Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_9', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31942
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:50:45.031793 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:50:45.037722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4151.44
 ---- batch: 020 ----
mean loss: 3989.23
 ---- batch: 030 ----
mean loss: 4041.19
train mean loss: 4056.08
epoch train time: 0:00:12.597872
elapsed time: 0:00:12.605243
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:50:57.637073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3986.55
 ---- batch: 020 ----
mean loss: 3909.54
 ---- batch: 030 ----
mean loss: 3892.89
train mean loss: 3925.12
epoch train time: 0:00:00.183312
elapsed time: 0:00:12.788680
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:50:57.820540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3816.34
 ---- batch: 020 ----
mean loss: 3745.00
 ---- batch: 030 ----
mean loss: 3692.99
train mean loss: 3729.21
epoch train time: 0:00:00.171031
elapsed time: 0:00:12.959866
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:50:57.991707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3535.96
 ---- batch: 020 ----
mean loss: 3427.47
 ---- batch: 030 ----
mean loss: 3379.38
train mean loss: 3434.75
epoch train time: 0:00:00.169045
elapsed time: 0:00:13.129091
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:50:58.160934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3185.39
 ---- batch: 020 ----
mean loss: 3098.43
 ---- batch: 030 ----
mean loss: 3139.00
train mean loss: 3122.28
epoch train time: 0:00:00.169074
elapsed time: 0:00:13.298304
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:50:58.330145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2944.23
 ---- batch: 020 ----
mean loss: 2857.12
 ---- batch: 030 ----
mean loss: 2823.78
train mean loss: 2860.06
epoch train time: 0:00:00.170925
elapsed time: 0:00:13.469362
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:50:58.501202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2730.94
 ---- batch: 020 ----
mean loss: 2641.32
 ---- batch: 030 ----
mean loss: 2576.93
train mean loss: 2635.62
epoch train time: 0:00:00.172052
elapsed time: 0:00:13.641548
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:50:58.673390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2503.77
 ---- batch: 020 ----
mean loss: 2471.99
 ---- batch: 030 ----
mean loss: 2374.80
train mean loss: 2438.35
epoch train time: 0:00:00.174294
elapsed time: 0:00:13.815988
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:50:58.847857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2320.22
 ---- batch: 020 ----
mean loss: 2274.71
 ---- batch: 030 ----
mean loss: 2235.97
train mean loss: 2265.67
epoch train time: 0:00:00.173603
elapsed time: 0:00:13.989770
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:50:59.021636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2177.69
 ---- batch: 020 ----
mean loss: 2112.45
 ---- batch: 030 ----
mean loss: 2081.13
train mean loss: 2109.90
epoch train time: 0:00:00.175110
elapsed time: 0:00:14.165043
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:50:59.196884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2005.70
 ---- batch: 020 ----
mean loss: 1993.01
 ---- batch: 030 ----
mean loss: 1933.76
train mean loss: 1966.83
epoch train time: 0:00:00.176833
elapsed time: 0:00:14.342023
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:50:59.373873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1876.64
 ---- batch: 020 ----
mean loss: 1858.96
 ---- batch: 030 ----
mean loss: 1790.86
train mean loss: 1839.80
epoch train time: 0:00:00.178762
elapsed time: 0:00:14.520945
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:50:59.552787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1769.65
 ---- batch: 020 ----
mean loss: 1716.54
 ---- batch: 030 ----
mean loss: 1714.81
train mean loss: 1720.17
epoch train time: 0:00:00.175078
elapsed time: 0:00:14.696160
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:50:59.728030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1678.02
 ---- batch: 020 ----
mean loss: 1626.32
 ---- batch: 030 ----
mean loss: 1569.32
train mean loss: 1610.96
epoch train time: 0:00:00.176112
elapsed time: 0:00:14.872437
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:50:59.904293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1541.19
 ---- batch: 020 ----
mean loss: 1521.05
 ---- batch: 030 ----
mean loss: 1492.55
train mean loss: 1511.77
epoch train time: 0:00:00.171053
elapsed time: 0:00:15.043664
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:51:00.075505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1444.24
 ---- batch: 020 ----
mean loss: 1419.20
 ---- batch: 030 ----
mean loss: 1407.74
train mean loss: 1418.54
epoch train time: 0:00:00.170809
elapsed time: 0:00:15.214620
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:51:00.246462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1367.01
 ---- batch: 020 ----
mean loss: 1332.06
 ---- batch: 030 ----
mean loss: 1315.15
train mean loss: 1332.49
epoch train time: 0:00:00.171195
elapsed time: 0:00:15.385979
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:51:00.417834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1283.48
 ---- batch: 020 ----
mean loss: 1262.05
 ---- batch: 030 ----
mean loss: 1220.60
train mean loss: 1254.57
epoch train time: 0:00:00.169352
elapsed time: 0:00:15.555490
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:51:00.587375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1211.57
 ---- batch: 020 ----
mean loss: 1190.59
 ---- batch: 030 ----
mean loss: 1159.18
train mean loss: 1181.42
epoch train time: 0:00:00.178009
elapsed time: 0:00:15.733714
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:51:00.765554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1141.76
 ---- batch: 020 ----
mean loss: 1115.17
 ---- batch: 030 ----
mean loss: 1097.41
train mean loss: 1113.97
epoch train time: 0:00:00.171670
elapsed time: 0:00:15.905565
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:51:00.937405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1063.12
 ---- batch: 020 ----
mean loss: 1055.99
 ---- batch: 030 ----
mean loss: 1038.43
train mean loss: 1053.02
epoch train time: 0:00:00.173270
elapsed time: 0:00:16.078970
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:51:01.110812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1008.69
 ---- batch: 020 ----
mean loss: 1018.75
 ---- batch: 030 ----
mean loss: 984.98
train mean loss: 995.82
epoch train time: 0:00:00.173674
elapsed time: 0:00:16.252792
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:51:01.284631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 970.04
 ---- batch: 020 ----
mean loss: 946.02
 ---- batch: 030 ----
mean loss: 936.31
train mean loss: 943.42
epoch train time: 0:00:00.170247
elapsed time: 0:00:16.423171
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:51:01.455026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.66
 ---- batch: 020 ----
mean loss: 911.32
 ---- batch: 030 ----
mean loss: 894.52
train mean loss: 894.71
epoch train time: 0:00:00.173146
elapsed time: 0:00:16.596511
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:51:01.628352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.06
 ---- batch: 020 ----
mean loss: 866.30
 ---- batch: 030 ----
mean loss: 834.63
train mean loss: 850.71
epoch train time: 0:00:00.174567
elapsed time: 0:00:16.771216
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:51:01.803057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 812.28
 ---- batch: 020 ----
mean loss: 827.75
 ---- batch: 030 ----
mean loss: 806.78
train mean loss: 810.96
epoch train time: 0:00:00.170284
elapsed time: 0:00:16.941650
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:51:01.973535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 789.13
 ---- batch: 020 ----
mean loss: 779.90
 ---- batch: 030 ----
mean loss: 751.98
train mean loss: 774.32
epoch train time: 0:00:00.167536
elapsed time: 0:00:17.109372
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:51:02.141230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 754.79
 ---- batch: 020 ----
mean loss: 742.04
 ---- batch: 030 ----
mean loss: 725.32
train mean loss: 739.22
epoch train time: 0:00:00.171122
elapsed time: 0:00:17.280646
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:51:02.312485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 712.52
 ---- batch: 020 ----
mean loss: 724.08
 ---- batch: 030 ----
mean loss: 700.76
train mean loss: 707.51
epoch train time: 0:00:00.168991
elapsed time: 0:00:17.449772
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:51:02.481635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 687.38
 ---- batch: 020 ----
mean loss: 678.50
 ---- batch: 030 ----
mean loss: 673.42
train mean loss: 679.69
epoch train time: 0:00:00.172245
elapsed time: 0:00:17.622803
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:51:02.654653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.61
 ---- batch: 020 ----
mean loss: 647.25
 ---- batch: 030 ----
mean loss: 661.23
train mean loss: 653.78
epoch train time: 0:00:00.184919
elapsed time: 0:00:17.807928
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:51:02.839843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 639.20
 ---- batch: 020 ----
mean loss: 635.61
 ---- batch: 030 ----
mean loss: 620.55
train mean loss: 628.88
epoch train time: 0:00:00.178215
elapsed time: 0:00:17.986400
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:51:03.018259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 618.35
 ---- batch: 020 ----
mean loss: 606.28
 ---- batch: 030 ----
mean loss: 594.23
train mean loss: 607.59
epoch train time: 0:00:00.175720
elapsed time: 0:00:18.162300
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:51:03.194338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.01
 ---- batch: 020 ----
mean loss: 595.52
 ---- batch: 030 ----
mean loss: 578.35
train mean loss: 587.88
epoch train time: 0:00:00.174168
elapsed time: 0:00:18.336831
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:51:03.368672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.55
 ---- batch: 020 ----
mean loss: 577.98
 ---- batch: 030 ----
mean loss: 563.36
train mean loss: 569.62
epoch train time: 0:00:00.178868
elapsed time: 0:00:18.515839
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:51:03.547695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.17
 ---- batch: 020 ----
mean loss: 550.56
 ---- batch: 030 ----
mean loss: 559.23
train mean loss: 553.53
epoch train time: 0:00:00.190324
elapsed time: 0:00:18.706322
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:51:03.738164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.63
 ---- batch: 020 ----
mean loss: 541.07
 ---- batch: 030 ----
mean loss: 535.08
train mean loss: 537.51
epoch train time: 0:00:00.175990
elapsed time: 0:00:18.882454
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:51:03.914297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.22
 ---- batch: 020 ----
mean loss: 520.59
 ---- batch: 030 ----
mean loss: 525.10
train mean loss: 523.97
epoch train time: 0:00:00.174001
elapsed time: 0:00:19.056619
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:51:04.088477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.44
 ---- batch: 020 ----
mean loss: 512.59
 ---- batch: 030 ----
mean loss: 506.32
train mean loss: 511.72
epoch train time: 0:00:00.172435
elapsed time: 0:00:19.229208
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:51:04.261050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.64
 ---- batch: 020 ----
mean loss: 500.03
 ---- batch: 030 ----
mean loss: 500.52
train mean loss: 500.03
epoch train time: 0:00:00.170259
elapsed time: 0:00:19.399602
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:51:04.431488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.48
 ---- batch: 020 ----
mean loss: 487.21
 ---- batch: 030 ----
mean loss: 487.93
train mean loss: 489.36
epoch train time: 0:00:00.174013
elapsed time: 0:00:19.573797
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:51:04.605664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.51
 ---- batch: 020 ----
mean loss: 473.29
 ---- batch: 030 ----
mean loss: 481.02
train mean loss: 479.46
epoch train time: 0:00:00.175026
elapsed time: 0:00:19.748993
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:51:04.780835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.57
 ---- batch: 020 ----
mean loss: 468.85
 ---- batch: 030 ----
mean loss: 467.20
train mean loss: 470.35
epoch train time: 0:00:00.174678
elapsed time: 0:00:19.923807
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:51:04.955647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.25
 ---- batch: 020 ----
mean loss: 471.15
 ---- batch: 030 ----
mean loss: 461.21
train mean loss: 462.51
epoch train time: 0:00:00.170710
elapsed time: 0:00:20.094698
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:51:05.126539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.59
 ---- batch: 020 ----
mean loss: 450.62
 ---- batch: 030 ----
mean loss: 457.91
train mean loss: 454.75
epoch train time: 0:00:00.167714
elapsed time: 0:00:20.262544
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:51:05.294382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.76
 ---- batch: 020 ----
mean loss: 446.42
 ---- batch: 030 ----
mean loss: 444.56
train mean loss: 448.10
epoch train time: 0:00:00.168159
elapsed time: 0:00:20.430835
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:51:05.462676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.43
 ---- batch: 020 ----
mean loss: 441.60
 ---- batch: 030 ----
mean loss: 440.90
train mean loss: 441.04
epoch train time: 0:00:00.170738
elapsed time: 0:00:20.601712
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:51:05.633553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.42
 ---- batch: 020 ----
mean loss: 440.32
 ---- batch: 030 ----
mean loss: 432.09
train mean loss: 435.12
epoch train time: 0:00:00.180545
elapsed time: 0:00:20.782398
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:51:05.814239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.35
 ---- batch: 020 ----
mean loss: 429.96
 ---- batch: 030 ----
mean loss: 423.48
train mean loss: 429.11
epoch train time: 0:00:00.175210
elapsed time: 0:00:20.957764
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:51:05.989628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.15
 ---- batch: 020 ----
mean loss: 423.71
 ---- batch: 030 ----
mean loss: 421.07
train mean loss: 424.36
epoch train time: 0:00:00.168667
elapsed time: 0:00:21.126589
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:51:06.158449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.99
 ---- batch: 020 ----
mean loss: 417.47
 ---- batch: 030 ----
mean loss: 421.62
train mean loss: 419.09
epoch train time: 0:00:00.169196
elapsed time: 0:00:21.295941
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:51:06.327782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.36
 ---- batch: 020 ----
mean loss: 406.99
 ---- batch: 030 ----
mean loss: 418.71
train mean loss: 414.59
epoch train time: 0:00:00.171741
elapsed time: 0:00:21.467822
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:51:06.499664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.63
 ---- batch: 020 ----
mean loss: 414.32
 ---- batch: 030 ----
mean loss: 401.75
train mean loss: 409.96
epoch train time: 0:00:00.174300
elapsed time: 0:00:21.642263
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:51:06.674105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.38
 ---- batch: 020 ----
mean loss: 404.24
 ---- batch: 030 ----
mean loss: 408.35
train mean loss: 406.08
epoch train time: 0:00:00.185713
elapsed time: 0:00:21.828121
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:51:06.859982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.20
 ---- batch: 020 ----
mean loss: 405.35
 ---- batch: 030 ----
mean loss: 396.40
train mean loss: 402.38
epoch train time: 0:00:00.179469
elapsed time: 0:00:22.007749
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:51:07.039591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.93
 ---- batch: 020 ----
mean loss: 398.05
 ---- batch: 030 ----
mean loss: 401.55
train mean loss: 399.03
epoch train time: 0:00:00.178986
elapsed time: 0:00:22.186874
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:51:07.218716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.62
 ---- batch: 020 ----
mean loss: 403.63
 ---- batch: 030 ----
mean loss: 392.83
train mean loss: 396.46
epoch train time: 0:00:00.186847
elapsed time: 0:00:22.373884
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:51:07.405728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.53
 ---- batch: 020 ----
mean loss: 393.60
 ---- batch: 030 ----
mean loss: 393.85
train mean loss: 393.50
epoch train time: 0:00:00.179833
elapsed time: 0:00:22.553880
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:51:07.585726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.83
 ---- batch: 020 ----
mean loss: 400.76
 ---- batch: 030 ----
mean loss: 391.59
train mean loss: 391.14
epoch train time: 0:00:00.181770
elapsed time: 0:00:22.735842
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:51:07.767685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.07
 ---- batch: 020 ----
mean loss: 392.00
 ---- batch: 030 ----
mean loss: 384.12
train mean loss: 389.16
epoch train time: 0:00:00.185648
elapsed time: 0:00:22.921639
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:51:07.953484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.92
 ---- batch: 020 ----
mean loss: 380.53
 ---- batch: 030 ----
mean loss: 384.21
train mean loss: 387.78
epoch train time: 0:00:00.177469
elapsed time: 0:00:23.099247
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:51:08.131087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.91
 ---- batch: 020 ----
mean loss: 384.39
 ---- batch: 030 ----
mean loss: 392.05
train mean loss: 385.80
epoch train time: 0:00:00.176711
elapsed time: 0:00:23.276091
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:51:08.307930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.66
 ---- batch: 020 ----
mean loss: 387.63
 ---- batch: 030 ----
mean loss: 380.71
train mean loss: 384.33
epoch train time: 0:00:00.177454
elapsed time: 0:00:23.453679
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:51:08.485535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.43
 ---- batch: 020 ----
mean loss: 384.71
 ---- batch: 030 ----
mean loss: 381.48
train mean loss: 382.65
epoch train time: 0:00:00.180889
elapsed time: 0:00:23.634735
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:51:08.666576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.18
 ---- batch: 020 ----
mean loss: 377.91
 ---- batch: 030 ----
mean loss: 381.02
train mean loss: 381.74
epoch train time: 0:00:00.183867
elapsed time: 0:00:23.818754
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:51:08.850602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.03
 ---- batch: 020 ----
mean loss: 376.14
 ---- batch: 030 ----
mean loss: 378.98
train mean loss: 380.57
epoch train time: 0:00:00.178577
elapsed time: 0:00:23.997488
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:51:09.029359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.78
 ---- batch: 020 ----
mean loss: 376.46
 ---- batch: 030 ----
mean loss: 376.41
train mean loss: 379.27
epoch train time: 0:00:00.174409
elapsed time: 0:00:24.172074
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:51:09.203934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.17
 ---- batch: 020 ----
mean loss: 375.16
 ---- batch: 030 ----
mean loss: 379.75
train mean loss: 378.61
epoch train time: 0:00:00.172936
elapsed time: 0:00:24.345196
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:51:09.377050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.42
 ---- batch: 020 ----
mean loss: 380.90
 ---- batch: 030 ----
mean loss: 377.05
train mean loss: 377.25
epoch train time: 0:00:00.183252
elapsed time: 0:00:24.528623
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:51:09.560492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.69
 ---- batch: 020 ----
mean loss: 377.00
 ---- batch: 030 ----
mean loss: 376.44
train mean loss: 376.81
epoch train time: 0:00:00.180033
elapsed time: 0:00:24.708820
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:51:09.740658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.00
 ---- batch: 020 ----
mean loss: 367.65
 ---- batch: 030 ----
mean loss: 380.97
train mean loss: 375.71
epoch train time: 0:00:00.181057
elapsed time: 0:00:24.890016
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:51:09.921858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.50
 ---- batch: 020 ----
mean loss: 373.77
 ---- batch: 030 ----
mean loss: 369.45
train mean loss: 375.39
epoch train time: 0:00:00.181618
elapsed time: 0:00:25.071774
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:51:10.103617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.24
 ---- batch: 020 ----
mean loss: 375.66
 ---- batch: 030 ----
mean loss: 371.31
train mean loss: 374.40
epoch train time: 0:00:00.177550
elapsed time: 0:00:25.249466
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:51:10.281307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.80
 ---- batch: 020 ----
mean loss: 381.87
 ---- batch: 030 ----
mean loss: 370.33
train mean loss: 373.82
epoch train time: 0:00:00.176130
elapsed time: 0:00:25.425733
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:51:10.457575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.82
 ---- batch: 020 ----
mean loss: 369.60
 ---- batch: 030 ----
mean loss: 376.30
train mean loss: 373.60
epoch train time: 0:00:00.180114
elapsed time: 0:00:25.605988
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:51:10.637829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.52
 ---- batch: 020 ----
mean loss: 376.88
 ---- batch: 030 ----
mean loss: 373.83
train mean loss: 373.07
epoch train time: 0:00:00.186331
elapsed time: 0:00:25.792476
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:51:10.824319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.97
 ---- batch: 020 ----
mean loss: 367.92
 ---- batch: 030 ----
mean loss: 373.08
train mean loss: 372.80
epoch train time: 0:00:00.181061
elapsed time: 0:00:25.973679
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:51:11.005520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.57
 ---- batch: 020 ----
mean loss: 375.31
 ---- batch: 030 ----
mean loss: 375.43
train mean loss: 371.99
epoch train time: 0:00:00.179098
elapsed time: 0:00:26.152923
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:51:11.184771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.32
 ---- batch: 020 ----
mean loss: 365.73
 ---- batch: 030 ----
mean loss: 367.78
train mean loss: 372.02
epoch train time: 0:00:00.177281
elapsed time: 0:00:26.330352
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:51:11.362193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.39
 ---- batch: 020 ----
mean loss: 371.42
 ---- batch: 030 ----
mean loss: 374.54
train mean loss: 371.63
epoch train time: 0:00:00.176777
elapsed time: 0:00:26.507267
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:51:11.539108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.61
 ---- batch: 020 ----
mean loss: 370.02
 ---- batch: 030 ----
mean loss: 382.29
train mean loss: 370.72
epoch train time: 0:00:00.179752
elapsed time: 0:00:26.687174
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:51:11.719062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.64
 ---- batch: 020 ----
mean loss: 372.60
 ---- batch: 030 ----
mean loss: 370.24
train mean loss: 370.64
epoch train time: 0:00:00.181513
elapsed time: 0:00:26.868875
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:51:11.900718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.70
 ---- batch: 020 ----
mean loss: 364.66
 ---- batch: 030 ----
mean loss: 371.69
train mean loss: 370.45
epoch train time: 0:00:00.175519
elapsed time: 0:00:27.044601
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:51:12.076457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.19
 ---- batch: 020 ----
mean loss: 373.51
 ---- batch: 030 ----
mean loss: 368.09
train mean loss: 369.91
epoch train time: 0:00:00.175423
elapsed time: 0:00:27.220174
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:51:12.252014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.01
 ---- batch: 020 ----
mean loss: 365.53
 ---- batch: 030 ----
mean loss: 369.77
train mean loss: 369.95
epoch train time: 0:00:00.171807
elapsed time: 0:00:27.392126
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:51:12.423968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.10
 ---- batch: 020 ----
mean loss: 374.87
 ---- batch: 030 ----
mean loss: 368.21
train mean loss: 369.24
epoch train time: 0:00:00.173658
elapsed time: 0:00:27.565925
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:51:12.597767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.46
 ---- batch: 020 ----
mean loss: 370.09
 ---- batch: 030 ----
mean loss: 365.72
train mean loss: 369.00
epoch train time: 0:00:00.175615
elapsed time: 0:00:27.741705
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:51:12.773549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.83
 ---- batch: 020 ----
mean loss: 379.03
 ---- batch: 030 ----
mean loss: 360.74
train mean loss: 368.70
epoch train time: 0:00:00.174986
elapsed time: 0:00:27.916832
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:51:12.948673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.23
 ---- batch: 020 ----
mean loss: 364.28
 ---- batch: 030 ----
mean loss: 363.61
train mean loss: 368.63
epoch train time: 0:00:00.171836
elapsed time: 0:00:28.088810
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:51:13.120651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.90
 ---- batch: 020 ----
mean loss: 364.65
 ---- batch: 030 ----
mean loss: 372.14
train mean loss: 368.07
epoch train time: 0:00:00.171935
elapsed time: 0:00:28.260882
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:51:13.292736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.83
 ---- batch: 020 ----
mean loss: 370.73
 ---- batch: 030 ----
mean loss: 371.21
train mean loss: 368.05
epoch train time: 0:00:00.170362
elapsed time: 0:00:28.431409
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:51:13.463247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.38
 ---- batch: 020 ----
mean loss: 367.53
 ---- batch: 030 ----
mean loss: 370.70
train mean loss: 367.46
epoch train time: 0:00:00.174019
elapsed time: 0:00:28.605562
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:51:13.637402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.65
 ---- batch: 020 ----
mean loss: 367.76
 ---- batch: 030 ----
mean loss: 364.36
train mean loss: 367.41
epoch train time: 0:00:00.179078
elapsed time: 0:00:28.784786
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:51:13.816637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.10
 ---- batch: 020 ----
mean loss: 365.69
 ---- batch: 030 ----
mean loss: 368.22
train mean loss: 367.29
epoch train time: 0:00:00.175808
elapsed time: 0:00:28.960743
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:51:13.992602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.73
 ---- batch: 020 ----
mean loss: 361.27
 ---- batch: 030 ----
mean loss: 374.72
train mean loss: 367.30
epoch train time: 0:00:00.171367
elapsed time: 0:00:29.132264
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:51:14.164104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.82
 ---- batch: 020 ----
mean loss: 364.13
 ---- batch: 030 ----
mean loss: 372.04
train mean loss: 366.51
epoch train time: 0:00:00.175701
elapsed time: 0:00:29.308097
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:51:14.339937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.27
 ---- batch: 020 ----
mean loss: 365.89
 ---- batch: 030 ----
mean loss: 371.09
train mean loss: 366.43
epoch train time: 0:00:00.172421
elapsed time: 0:00:29.480663
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:51:14.512506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.68
 ---- batch: 020 ----
mean loss: 364.00
 ---- batch: 030 ----
mean loss: 362.80
train mean loss: 366.40
epoch train time: 0:00:00.172550
elapsed time: 0:00:29.653377
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:51:14.685217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.26
 ---- batch: 020 ----
mean loss: 364.70
 ---- batch: 030 ----
mean loss: 363.46
train mean loss: 366.42
epoch train time: 0:00:00.177493
elapsed time: 0:00:29.831010
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:51:14.862853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.00
 ---- batch: 020 ----
mean loss: 362.37
 ---- batch: 030 ----
mean loss: 367.87
train mean loss: 365.69
epoch train time: 0:00:00.173581
elapsed time: 0:00:30.004730
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:51:15.036571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.35
 ---- batch: 020 ----
mean loss: 368.52
 ---- batch: 030 ----
mean loss: 360.30
train mean loss: 366.04
epoch train time: 0:00:00.179686
elapsed time: 0:00:30.184551
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:51:15.216401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.64
 ---- batch: 020 ----
mean loss: 370.37
 ---- batch: 030 ----
mean loss: 365.82
train mean loss: 365.53
epoch train time: 0:00:00.170643
elapsed time: 0:00:30.355381
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:51:15.387220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.73
 ---- batch: 020 ----
mean loss: 366.00
 ---- batch: 030 ----
mean loss: 365.69
train mean loss: 364.96
epoch train time: 0:00:00.172685
elapsed time: 0:00:30.528217
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:51:15.560058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.14
 ---- batch: 020 ----
mean loss: 370.74
 ---- batch: 030 ----
mean loss: 365.69
train mean loss: 365.19
epoch train time: 0:00:00.172806
elapsed time: 0:00:30.701172
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:51:15.733011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.16
 ---- batch: 020 ----
mean loss: 371.41
 ---- batch: 030 ----
mean loss: 366.88
train mean loss: 364.74
epoch train time: 0:00:00.173436
elapsed time: 0:00:30.874804
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:51:15.906658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.65
 ---- batch: 020 ----
mean loss: 365.28
 ---- batch: 030 ----
mean loss: 370.79
train mean loss: 364.73
epoch train time: 0:00:00.175024
elapsed time: 0:00:31.049979
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:51:16.081820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.19
 ---- batch: 020 ----
mean loss: 367.16
 ---- batch: 030 ----
mean loss: 366.60
train mean loss: 364.54
epoch train time: 0:00:00.172115
elapsed time: 0:00:31.222246
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:51:16.254079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.54
 ---- batch: 020 ----
mean loss: 356.78
 ---- batch: 030 ----
mean loss: 368.25
train mean loss: 364.30
epoch train time: 0:00:00.168325
elapsed time: 0:00:31.390707
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:51:16.422580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.11
 ---- batch: 020 ----
mean loss: 371.18
 ---- batch: 030 ----
mean loss: 359.44
train mean loss: 364.37
epoch train time: 0:00:00.168414
elapsed time: 0:00:31.559317
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:51:16.591156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.18
 ---- batch: 020 ----
mean loss: 358.07
 ---- batch: 030 ----
mean loss: 366.32
train mean loss: 364.15
epoch train time: 0:00:00.176162
elapsed time: 0:00:31.735623
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:51:16.767465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.35
 ---- batch: 020 ----
mean loss: 363.06
 ---- batch: 030 ----
mean loss: 364.53
train mean loss: 363.38
epoch train time: 0:00:00.174417
elapsed time: 0:00:31.910182
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:51:16.942068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.97
 ---- batch: 020 ----
mean loss: 362.65
 ---- batch: 030 ----
mean loss: 366.41
train mean loss: 363.42
epoch train time: 0:00:00.175635
elapsed time: 0:00:32.086035
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:51:17.117886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.51
 ---- batch: 020 ----
mean loss: 360.28
 ---- batch: 030 ----
mean loss: 361.99
train mean loss: 363.89
epoch train time: 0:00:00.170146
elapsed time: 0:00:32.256335
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:51:17.288174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.55
 ---- batch: 020 ----
mean loss: 362.62
 ---- batch: 030 ----
mean loss: 366.91
train mean loss: 363.07
epoch train time: 0:00:00.169900
elapsed time: 0:00:32.426370
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:51:17.458214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.70
 ---- batch: 020 ----
mean loss: 365.70
 ---- batch: 030 ----
mean loss: 371.07
train mean loss: 363.02
epoch train time: 0:00:00.171517
elapsed time: 0:00:32.598040
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:51:17.629878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.78
 ---- batch: 020 ----
mean loss: 363.95
 ---- batch: 030 ----
mean loss: 361.56
train mean loss: 363.32
epoch train time: 0:00:00.181788
elapsed time: 0:00:32.779963
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:51:17.811806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.43
 ---- batch: 020 ----
mean loss: 363.71
 ---- batch: 030 ----
mean loss: 364.26
train mean loss: 362.44
epoch train time: 0:00:00.177088
elapsed time: 0:00:32.957200
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:51:17.989043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.12
 ---- batch: 020 ----
mean loss: 365.38
 ---- batch: 030 ----
mean loss: 360.96
train mean loss: 361.97
epoch train time: 0:00:00.174772
elapsed time: 0:00:33.132110
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:51:18.163965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.45
 ---- batch: 020 ----
mean loss: 365.97
 ---- batch: 030 ----
mean loss: 361.43
train mean loss: 362.45
epoch train time: 0:00:00.169834
elapsed time: 0:00:33.302095
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:51:18.333935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.51
 ---- batch: 020 ----
mean loss: 359.61
 ---- batch: 030 ----
mean loss: 360.69
train mean loss: 362.14
epoch train time: 0:00:00.174419
elapsed time: 0:00:33.476656
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:51:18.508507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.59
 ---- batch: 020 ----
mean loss: 358.79
 ---- batch: 030 ----
mean loss: 359.77
train mean loss: 362.09
epoch train time: 0:00:00.174073
elapsed time: 0:00:33.650877
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:51:18.682736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.52
 ---- batch: 020 ----
mean loss: 359.62
 ---- batch: 030 ----
mean loss: 368.70
train mean loss: 361.61
epoch train time: 0:00:00.182738
elapsed time: 0:00:33.833777
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:51:18.865652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.11
 ---- batch: 020 ----
mean loss: 359.97
 ---- batch: 030 ----
mean loss: 365.49
train mean loss: 361.74
epoch train time: 0:00:00.176222
elapsed time: 0:00:34.010174
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:51:19.042014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.78
 ---- batch: 020 ----
mean loss: 363.40
 ---- batch: 030 ----
mean loss: 360.19
train mean loss: 361.31
epoch train time: 0:00:00.173289
elapsed time: 0:00:34.183597
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:51:19.215450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.48
 ---- batch: 020 ----
mean loss: 355.79
 ---- batch: 030 ----
mean loss: 364.23
train mean loss: 361.02
epoch train time: 0:00:00.171841
elapsed time: 0:00:34.355586
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:51:19.387439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.87
 ---- batch: 020 ----
mean loss: 359.08
 ---- batch: 030 ----
mean loss: 352.83
train mean loss: 361.41
epoch train time: 0:00:00.174824
elapsed time: 0:00:34.530571
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:51:19.562411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.95
 ---- batch: 020 ----
mean loss: 366.96
 ---- batch: 030 ----
mean loss: 361.57
train mean loss: 360.87
epoch train time: 0:00:00.173971
elapsed time: 0:00:34.704704
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:51:19.736537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.92
 ---- batch: 020 ----
mean loss: 359.73
 ---- batch: 030 ----
mean loss: 365.30
train mean loss: 360.81
epoch train time: 0:00:00.178718
elapsed time: 0:00:34.883556
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:51:19.915397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.78
 ---- batch: 020 ----
mean loss: 363.59
 ---- batch: 030 ----
mean loss: 361.79
train mean loss: 360.40
epoch train time: 0:00:00.175713
elapsed time: 0:00:35.059405
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:51:20.091245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.11
 ---- batch: 020 ----
mean loss: 362.13
 ---- batch: 030 ----
mean loss: 362.38
train mean loss: 360.24
epoch train time: 0:00:00.172443
elapsed time: 0:00:35.231984
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:51:20.263841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.06
 ---- batch: 020 ----
mean loss: 360.06
 ---- batch: 030 ----
mean loss: 370.81
train mean loss: 360.55
epoch train time: 0:00:00.172540
elapsed time: 0:00:35.404678
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:51:20.436518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.32
 ---- batch: 020 ----
mean loss: 372.60
 ---- batch: 030 ----
mean loss: 353.38
train mean loss: 360.14
epoch train time: 0:00:00.173080
elapsed time: 0:00:35.577920
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:51:20.609776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.57
 ---- batch: 020 ----
mean loss: 364.29
 ---- batch: 030 ----
mean loss: 367.46
train mean loss: 360.00
epoch train time: 0:00:00.172973
elapsed time: 0:00:35.751052
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:51:20.782894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.02
 ---- batch: 020 ----
mean loss: 357.94
 ---- batch: 030 ----
mean loss: 353.17
train mean loss: 359.78
epoch train time: 0:00:00.182607
elapsed time: 0:00:35.933807
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:51:20.965700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.77
 ---- batch: 020 ----
mean loss: 356.48
 ---- batch: 030 ----
mean loss: 366.08
train mean loss: 359.48
epoch train time: 0:00:00.175399
elapsed time: 0:00:36.109399
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:51:21.141267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.82
 ---- batch: 020 ----
mean loss: 361.78
 ---- batch: 030 ----
mean loss: 366.98
train mean loss: 359.65
epoch train time: 0:00:00.175732
elapsed time: 0:00:36.285303
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:51:21.317150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.03
 ---- batch: 020 ----
mean loss: 355.25
 ---- batch: 030 ----
mean loss: 355.31
train mean loss: 359.48
epoch train time: 0:00:00.175087
elapsed time: 0:00:36.460535
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:51:21.492376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.49
 ---- batch: 020 ----
mean loss: 357.80
 ---- batch: 030 ----
mean loss: 361.03
train mean loss: 359.24
epoch train time: 0:00:00.177961
elapsed time: 0:00:36.638644
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:51:21.670511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.47
 ---- batch: 020 ----
mean loss: 355.94
 ---- batch: 030 ----
mean loss: 362.16
train mean loss: 358.70
epoch train time: 0:00:00.177961
elapsed time: 0:00:36.816777
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:51:21.848639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.91
 ---- batch: 020 ----
mean loss: 355.65
 ---- batch: 030 ----
mean loss: 351.98
train mean loss: 358.99
epoch train time: 0:00:00.173965
elapsed time: 0:00:36.990901
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:51:22.022741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.19
 ---- batch: 020 ----
mean loss: 361.03
 ---- batch: 030 ----
mean loss: 353.81
train mean loss: 359.02
epoch train time: 0:00:00.168159
elapsed time: 0:00:37.159204
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:51:22.191049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.86
 ---- batch: 020 ----
mean loss: 357.22
 ---- batch: 030 ----
mean loss: 358.17
train mean loss: 358.86
epoch train time: 0:00:00.172355
elapsed time: 0:00:37.331699
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:51:22.363538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.27
 ---- batch: 020 ----
mean loss: 363.42
 ---- batch: 030 ----
mean loss: 357.93
train mean loss: 358.45
epoch train time: 0:00:00.171469
elapsed time: 0:00:37.503299
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:51:22.535204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.76
 ---- batch: 020 ----
mean loss: 365.48
 ---- batch: 030 ----
mean loss: 354.11
train mean loss: 358.21
epoch train time: 0:00:00.172003
elapsed time: 0:00:37.675502
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:51:22.707342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.98
 ---- batch: 020 ----
mean loss: 358.14
 ---- batch: 030 ----
mean loss: 358.02
train mean loss: 357.95
epoch train time: 0:00:00.172362
elapsed time: 0:00:37.848010
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:51:22.879849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.43
 ---- batch: 020 ----
mean loss: 359.76
 ---- batch: 030 ----
mean loss: 355.63
train mean loss: 358.00
epoch train time: 0:00:00.169017
elapsed time: 0:00:38.017162
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:51:23.049002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.84
 ---- batch: 020 ----
mean loss: 358.73
 ---- batch: 030 ----
mean loss: 359.01
train mean loss: 357.66
epoch train time: 0:00:00.167344
elapsed time: 0:00:38.184641
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:51:23.216481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.66
 ---- batch: 020 ----
mean loss: 353.40
 ---- batch: 030 ----
mean loss: 357.51
train mean loss: 357.40
epoch train time: 0:00:00.172086
elapsed time: 0:00:38.356860
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:51:23.388721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.60
 ---- batch: 020 ----
mean loss: 354.19
 ---- batch: 030 ----
mean loss: 361.51
train mean loss: 357.36
epoch train time: 0:00:00.171850
elapsed time: 0:00:38.528954
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:51:23.560784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.46
 ---- batch: 020 ----
mean loss: 357.58
 ---- batch: 030 ----
mean loss: 363.99
train mean loss: 357.19
epoch train time: 0:00:00.172336
elapsed time: 0:00:38.701416
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:51:23.733259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.71
 ---- batch: 020 ----
mean loss: 362.37
 ---- batch: 030 ----
mean loss: 351.35
train mean loss: 357.11
epoch train time: 0:00:00.181232
elapsed time: 0:00:38.882813
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:51:23.914656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.81
 ---- batch: 020 ----
mean loss: 363.89
 ---- batch: 030 ----
mean loss: 353.48
train mean loss: 356.97
epoch train time: 0:00:00.174819
elapsed time: 0:00:39.057772
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:51:24.089644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.39
 ---- batch: 020 ----
mean loss: 355.19
 ---- batch: 030 ----
mean loss: 353.98
train mean loss: 357.23
epoch train time: 0:00:00.173031
elapsed time: 0:00:39.230970
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:51:24.262811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.38
 ---- batch: 020 ----
mean loss: 355.59
 ---- batch: 030 ----
mean loss: 361.21
train mean loss: 356.61
epoch train time: 0:00:00.173647
elapsed time: 0:00:39.404785
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:51:24.436626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.16
 ---- batch: 020 ----
mean loss: 359.00
 ---- batch: 030 ----
mean loss: 360.46
train mean loss: 356.42
epoch train time: 0:00:00.173189
elapsed time: 0:00:39.578113
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:51:24.609958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.34
 ---- batch: 020 ----
mean loss: 353.82
 ---- batch: 030 ----
mean loss: 354.11
train mean loss: 356.67
epoch train time: 0:00:00.176126
elapsed time: 0:00:39.754383
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:51:24.786225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.57
 ---- batch: 020 ----
mean loss: 356.80
 ---- batch: 030 ----
mean loss: 352.45
train mean loss: 356.20
epoch train time: 0:00:00.176142
elapsed time: 0:00:39.930674
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:51:24.962551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.39
 ---- batch: 020 ----
mean loss: 360.66
 ---- batch: 030 ----
mean loss: 358.11
train mean loss: 356.20
epoch train time: 0:00:00.172635
elapsed time: 0:00:40.103484
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:51:25.135325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.61
 ---- batch: 020 ----
mean loss: 351.42
 ---- batch: 030 ----
mean loss: 356.38
train mean loss: 355.85
epoch train time: 0:00:00.172690
elapsed time: 0:00:40.276311
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:51:25.308153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.57
 ---- batch: 020 ----
mean loss: 359.12
 ---- batch: 030 ----
mean loss: 352.52
train mean loss: 355.91
epoch train time: 0:00:00.174602
elapsed time: 0:00:40.451050
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:51:25.482900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.60
 ---- batch: 020 ----
mean loss: 365.86
 ---- batch: 030 ----
mean loss: 348.52
train mean loss: 355.13
epoch train time: 0:00:00.174464
elapsed time: 0:00:40.625667
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:51:25.657510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.81
 ---- batch: 020 ----
mean loss: 351.95
 ---- batch: 030 ----
mean loss: 359.95
train mean loss: 355.33
epoch train time: 0:00:00.173025
elapsed time: 0:00:40.798857
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:51:25.830731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.11
 ---- batch: 020 ----
mean loss: 358.77
 ---- batch: 030 ----
mean loss: 358.63
train mean loss: 355.48
epoch train time: 0:00:00.179968
elapsed time: 0:00:40.979043
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:51:26.010899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.03
 ---- batch: 020 ----
mean loss: 357.63
 ---- batch: 030 ----
mean loss: 354.64
train mean loss: 355.33
epoch train time: 0:00:00.174681
elapsed time: 0:00:41.153916
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:51:26.185773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.51
 ---- batch: 020 ----
mean loss: 361.37
 ---- batch: 030 ----
mean loss: 353.02
train mean loss: 354.97
epoch train time: 0:00:00.176014
elapsed time: 0:00:41.330090
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:51:26.361933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.35
 ---- batch: 020 ----
mean loss: 360.47
 ---- batch: 030 ----
mean loss: 349.51
train mean loss: 354.42
epoch train time: 0:00:00.174594
elapsed time: 0:00:41.504827
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:51:26.536668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.50
 ---- batch: 020 ----
mean loss: 354.19
 ---- batch: 030 ----
mean loss: 355.69
train mean loss: 354.61
epoch train time: 0:00:00.177382
elapsed time: 0:00:41.682351
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:51:26.714194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.34
 ---- batch: 020 ----
mean loss: 347.28
 ---- batch: 030 ----
mean loss: 363.12
train mean loss: 354.97
epoch train time: 0:00:00.175915
elapsed time: 0:00:41.858413
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:51:26.890256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.24
 ---- batch: 020 ----
mean loss: 352.79
 ---- batch: 030 ----
mean loss: 354.80
train mean loss: 354.53
epoch train time: 0:00:00.178388
elapsed time: 0:00:42.036983
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:51:27.068824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.27
 ---- batch: 020 ----
mean loss: 357.49
 ---- batch: 030 ----
mean loss: 351.30
train mean loss: 354.24
epoch train time: 0:00:00.172658
elapsed time: 0:00:42.209783
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:51:27.241650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.58
 ---- batch: 020 ----
mean loss: 353.67
 ---- batch: 030 ----
mean loss: 352.38
train mean loss: 354.01
epoch train time: 0:00:00.179019
elapsed time: 0:00:42.388998
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:51:27.420840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.76
 ---- batch: 020 ----
mean loss: 360.55
 ---- batch: 030 ----
mean loss: 361.29
train mean loss: 353.85
epoch train time: 0:00:00.176285
elapsed time: 0:00:42.565422
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:51:27.597265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.19
 ---- batch: 020 ----
mean loss: 348.73
 ---- batch: 030 ----
mean loss: 355.68
train mean loss: 354.23
epoch train time: 0:00:00.174702
elapsed time: 0:00:42.740995
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:51:27.772846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.90
 ---- batch: 020 ----
mean loss: 357.09
 ---- batch: 030 ----
mean loss: 350.34
train mean loss: 353.84
epoch train time: 0:00:00.171028
elapsed time: 0:00:42.912193
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:51:27.944025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.80
 ---- batch: 020 ----
mean loss: 356.81
 ---- batch: 030 ----
mean loss: 354.43
train mean loss: 353.44
epoch train time: 0:00:00.171212
elapsed time: 0:00:43.083548
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:51:28.115386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.88
 ---- batch: 020 ----
mean loss: 355.70
 ---- batch: 030 ----
mean loss: 350.88
train mean loss: 353.35
epoch train time: 0:00:00.170156
elapsed time: 0:00:43.253875
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:51:28.285735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.43
 ---- batch: 020 ----
mean loss: 348.06
 ---- batch: 030 ----
mean loss: 359.32
train mean loss: 353.10
epoch train time: 0:00:00.170257
elapsed time: 0:00:43.424286
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:51:28.456126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.29
 ---- batch: 020 ----
mean loss: 351.79
 ---- batch: 030 ----
mean loss: 346.87
train mean loss: 352.91
epoch train time: 0:00:00.169319
elapsed time: 0:00:43.593739
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:51:28.625578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.69
 ---- batch: 020 ----
mean loss: 359.38
 ---- batch: 030 ----
mean loss: 346.67
train mean loss: 353.11
epoch train time: 0:00:00.183959
elapsed time: 0:00:43.777858
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:51:28.809704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.27
 ---- batch: 020 ----
mean loss: 352.59
 ---- batch: 030 ----
mean loss: 356.69
train mean loss: 352.63
epoch train time: 0:00:00.183572
elapsed time: 0:00:43.961586
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:51:28.993441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.21
 ---- batch: 020 ----
mean loss: 357.20
 ---- batch: 030 ----
mean loss: 347.20
train mean loss: 352.92
epoch train time: 0:00:00.170777
elapsed time: 0:00:44.132512
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:51:29.164353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.84
 ---- batch: 020 ----
mean loss: 350.86
 ---- batch: 030 ----
mean loss: 357.14
train mean loss: 352.33
epoch train time: 0:00:00.170192
elapsed time: 0:00:44.302838
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:51:29.334678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.30
 ---- batch: 020 ----
mean loss: 349.47
 ---- batch: 030 ----
mean loss: 353.08
train mean loss: 352.49
epoch train time: 0:00:00.170695
elapsed time: 0:00:44.473669
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:51:29.505519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.28
 ---- batch: 020 ----
mean loss: 352.15
 ---- batch: 030 ----
mean loss: 354.28
train mean loss: 352.36
epoch train time: 0:00:00.172771
elapsed time: 0:00:44.646586
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:51:29.678445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.11
 ---- batch: 020 ----
mean loss: 347.34
 ---- batch: 030 ----
mean loss: 351.67
train mean loss: 352.13
epoch train time: 0:00:00.176579
elapsed time: 0:00:44.823322
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:51:29.855210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.78
 ---- batch: 020 ----
mean loss: 357.76
 ---- batch: 030 ----
mean loss: 343.08
train mean loss: 352.13
epoch train time: 0:00:00.172347
elapsed time: 0:00:44.995853
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:51:30.027694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.97
 ---- batch: 020 ----
mean loss: 356.47
 ---- batch: 030 ----
mean loss: 347.68
train mean loss: 351.75
epoch train time: 0:00:00.171167
elapsed time: 0:00:45.167157
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:51:30.199011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.47
 ---- batch: 020 ----
mean loss: 358.37
 ---- batch: 030 ----
mean loss: 351.36
train mean loss: 351.70
epoch train time: 0:00:00.171725
elapsed time: 0:00:45.339034
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:51:30.370876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.07
 ---- batch: 020 ----
mean loss: 345.36
 ---- batch: 030 ----
mean loss: 356.85
train mean loss: 351.36
epoch train time: 0:00:00.173724
elapsed time: 0:00:45.512894
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:51:30.544752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.77
 ---- batch: 020 ----
mean loss: 354.38
 ---- batch: 030 ----
mean loss: 355.27
train mean loss: 351.07
epoch train time: 0:00:00.172743
elapsed time: 0:00:45.685796
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:51:30.717662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.82
 ---- batch: 020 ----
mean loss: 354.71
 ---- batch: 030 ----
mean loss: 351.39
train mean loss: 351.12
epoch train time: 0:00:00.173103
elapsed time: 0:00:45.859066
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:51:30.890908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.88
 ---- batch: 020 ----
mean loss: 356.46
 ---- batch: 030 ----
mean loss: 347.90
train mean loss: 350.72
epoch train time: 0:00:00.178431
elapsed time: 0:00:46.037635
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:51:31.069492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.15
 ---- batch: 020 ----
mean loss: 354.88
 ---- batch: 030 ----
mean loss: 345.63
train mean loss: 350.62
epoch train time: 0:00:00.175137
elapsed time: 0:00:46.212927
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:51:31.244769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.51
 ---- batch: 020 ----
mean loss: 350.13
 ---- batch: 030 ----
mean loss: 348.55
train mean loss: 350.66
epoch train time: 0:00:00.173011
elapsed time: 0:00:46.386199
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:51:31.418072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.86
 ---- batch: 020 ----
mean loss: 353.72
 ---- batch: 030 ----
mean loss: 349.66
train mean loss: 350.46
epoch train time: 0:00:00.175870
elapsed time: 0:00:46.562251
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:51:31.594096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.96
 ---- batch: 020 ----
mean loss: 345.34
 ---- batch: 030 ----
mean loss: 349.95
train mean loss: 350.17
epoch train time: 0:00:00.175608
elapsed time: 0:00:46.738002
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:51:31.769843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.54
 ---- batch: 020 ----
mean loss: 349.92
 ---- batch: 030 ----
mean loss: 347.13
train mean loss: 349.76
epoch train time: 0:00:00.181465
elapsed time: 0:00:46.919615
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:51:31.951459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.98
 ---- batch: 020 ----
mean loss: 353.98
 ---- batch: 030 ----
mean loss: 355.90
train mean loss: 349.90
epoch train time: 0:00:00.177057
elapsed time: 0:00:47.096825
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:51:32.128671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.75
 ---- batch: 020 ----
mean loss: 348.50
 ---- batch: 030 ----
mean loss: 349.19
train mean loss: 349.77
epoch train time: 0:00:00.175202
elapsed time: 0:00:47.272170
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:51:32.304011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.40
 ---- batch: 020 ----
mean loss: 350.22
 ---- batch: 030 ----
mean loss: 352.00
train mean loss: 349.11
epoch train time: 0:00:00.175071
elapsed time: 0:00:47.447380
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:51:32.479222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.86
 ---- batch: 020 ----
mean loss: 357.56
 ---- batch: 030 ----
mean loss: 345.47
train mean loss: 349.62
epoch train time: 0:00:00.175582
elapsed time: 0:00:47.623122
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:51:32.654967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.10
 ---- batch: 020 ----
mean loss: 345.48
 ---- batch: 030 ----
mean loss: 351.69
train mean loss: 349.32
epoch train time: 0:00:00.187229
elapsed time: 0:00:47.810521
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:51:32.842366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.17
 ---- batch: 020 ----
mean loss: 350.59
 ---- batch: 030 ----
mean loss: 354.35
train mean loss: 349.39
epoch train time: 0:00:00.174435
elapsed time: 0:00:47.985099
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:51:33.016957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.73
 ---- batch: 020 ----
mean loss: 352.03
 ---- batch: 030 ----
mean loss: 342.75
train mean loss: 349.48
epoch train time: 0:00:00.173793
elapsed time: 0:00:48.159044
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:51:33.190887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.68
 ---- batch: 020 ----
mean loss: 345.16
 ---- batch: 030 ----
mean loss: 356.43
train mean loss: 349.29
epoch train time: 0:00:00.172782
elapsed time: 0:00:48.331965
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:51:33.363806
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.91
 ---- batch: 020 ----
mean loss: 348.51
 ---- batch: 030 ----
mean loss: 352.52
train mean loss: 348.99
epoch train time: 0:00:00.173517
elapsed time: 0:00:48.505622
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:51:33.537464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.33
 ---- batch: 020 ----
mean loss: 348.13
 ---- batch: 030 ----
mean loss: 346.75
train mean loss: 349.28
epoch train time: 0:00:00.176246
elapsed time: 0:00:48.682011
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:51:33.713854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.29
 ---- batch: 020 ----
mean loss: 350.07
 ---- batch: 030 ----
mean loss: 355.38
train mean loss: 349.05
epoch train time: 0:00:00.174324
elapsed time: 0:00:48.856475
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:51:33.888362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.67
 ---- batch: 020 ----
mean loss: 342.79
 ---- batch: 030 ----
mean loss: 350.46
train mean loss: 349.41
epoch train time: 0:00:00.176483
elapsed time: 0:00:49.033144
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:51:34.064986
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.70
 ---- batch: 020 ----
mean loss: 356.55
 ---- batch: 030 ----
mean loss: 350.54
train mean loss: 348.98
epoch train time: 0:00:00.172503
elapsed time: 0:00:49.205791
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:51:34.237658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.57
 ---- batch: 020 ----
mean loss: 346.72
 ---- batch: 030 ----
mean loss: 352.53
train mean loss: 349.73
epoch train time: 0:00:00.174379
elapsed time: 0:00:49.380374
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:51:34.412236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.89
 ---- batch: 020 ----
mean loss: 348.65
 ---- batch: 030 ----
mean loss: 355.67
train mean loss: 348.86
epoch train time: 0:00:00.175469
elapsed time: 0:00:49.555998
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:51:34.587838
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.97
 ---- batch: 020 ----
mean loss: 356.17
 ---- batch: 030 ----
mean loss: 346.24
train mean loss: 348.75
epoch train time: 0:00:00.181041
elapsed time: 0:00:49.737178
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:51:34.769020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.43
 ---- batch: 020 ----
mean loss: 347.05
 ---- batch: 030 ----
mean loss: 345.14
train mean loss: 348.65
epoch train time: 0:00:00.174749
elapsed time: 0:00:49.912064
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:51:34.943905
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.79
 ---- batch: 020 ----
mean loss: 345.59
 ---- batch: 030 ----
mean loss: 350.31
train mean loss: 349.50
epoch train time: 0:00:00.174202
elapsed time: 0:00:50.086407
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:51:35.118249
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.09
 ---- batch: 020 ----
mean loss: 344.57
 ---- batch: 030 ----
mean loss: 349.92
train mean loss: 348.94
epoch train time: 0:00:00.173821
elapsed time: 0:00:50.260368
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:51:35.292227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.00
 ---- batch: 020 ----
mean loss: 350.46
 ---- batch: 030 ----
mean loss: 346.42
train mean loss: 348.67
epoch train time: 0:00:00.172603
elapsed time: 0:00:50.433134
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:51:35.464975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.10
 ---- batch: 020 ----
mean loss: 346.43
 ---- batch: 030 ----
mean loss: 351.64
train mean loss: 349.37
epoch train time: 0:00:00.175857
elapsed time: 0:00:50.609133
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:51:35.640975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.55
 ---- batch: 020 ----
mean loss: 348.89
 ---- batch: 030 ----
mean loss: 345.80
train mean loss: 349.19
epoch train time: 0:00:00.180790
elapsed time: 0:00:50.790078
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:51:35.821940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.83
 ---- batch: 020 ----
mean loss: 351.71
 ---- batch: 030 ----
mean loss: 343.96
train mean loss: 348.94
epoch train time: 0:00:00.174307
elapsed time: 0:00:50.964545
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:51:35.996386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.46
 ---- batch: 020 ----
mean loss: 344.91
 ---- batch: 030 ----
mean loss: 347.31
train mean loss: 348.83
epoch train time: 0:00:00.171678
elapsed time: 0:00:51.136360
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:51:36.168200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.29
 ---- batch: 020 ----
mean loss: 347.32
 ---- batch: 030 ----
mean loss: 345.60
train mean loss: 348.95
epoch train time: 0:00:00.169453
elapsed time: 0:00:51.305970
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:51:36.337827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.21
 ---- batch: 020 ----
mean loss: 336.13
 ---- batch: 030 ----
mean loss: 352.23
train mean loss: 348.70
epoch train time: 0:00:00.172653
elapsed time: 0:00:51.478796
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:51:36.510637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.54
 ---- batch: 020 ----
mean loss: 346.73
 ---- batch: 030 ----
mean loss: 349.87
train mean loss: 348.80
epoch train time: 0:00:00.172159
elapsed time: 0:00:51.651111
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:51:36.683008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.23
 ---- batch: 020 ----
mean loss: 349.41
 ---- batch: 030 ----
mean loss: 351.25
train mean loss: 348.99
epoch train time: 0:00:00.187413
elapsed time: 0:00:51.838726
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:51:36.870569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.60
 ---- batch: 020 ----
mean loss: 347.71
 ---- batch: 030 ----
mean loss: 345.31
train mean loss: 348.93
epoch train time: 0:00:00.175177
elapsed time: 0:00:52.014042
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:51:37.045881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.40
 ---- batch: 020 ----
mean loss: 344.37
 ---- batch: 030 ----
mean loss: 346.85
train mean loss: 348.64
epoch train time: 0:00:00.172214
elapsed time: 0:00:52.186394
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:51:37.218235
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.68
 ---- batch: 020 ----
mean loss: 342.42
 ---- batch: 030 ----
mean loss: 352.27
train mean loss: 348.65
epoch train time: 0:00:00.170861
elapsed time: 0:00:52.357391
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:51:37.389233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.72
 ---- batch: 020 ----
mean loss: 357.98
 ---- batch: 030 ----
mean loss: 350.07
train mean loss: 348.64
epoch train time: 0:00:00.172047
elapsed time: 0:00:52.529591
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:51:37.561449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.00
 ---- batch: 020 ----
mean loss: 351.26
 ---- batch: 030 ----
mean loss: 348.99
train mean loss: 349.31
epoch train time: 0:00:00.172539
elapsed time: 0:00:52.702287
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:51:37.734128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.63
 ---- batch: 020 ----
mean loss: 355.94
 ---- batch: 030 ----
mean loss: 340.77
train mean loss: 348.30
epoch train time: 0:00:00.180698
elapsed time: 0:00:52.883123
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:51:37.914978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.82
 ---- batch: 020 ----
mean loss: 339.24
 ---- batch: 030 ----
mean loss: 356.56
train mean loss: 348.56
epoch train time: 0:00:00.171607
elapsed time: 0:00:53.054892
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:51:38.086747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.15
 ---- batch: 020 ----
mean loss: 342.63
 ---- batch: 030 ----
mean loss: 356.16
train mean loss: 348.81
epoch train time: 0:00:00.170159
elapsed time: 0:00:53.225202
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:51:38.257042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.23
 ---- batch: 020 ----
mean loss: 352.29
 ---- batch: 030 ----
mean loss: 351.75
train mean loss: 348.56
epoch train time: 0:00:00.171802
elapsed time: 0:00:53.397151
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:51:38.428999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.47
 ---- batch: 020 ----
mean loss: 348.35
 ---- batch: 030 ----
mean loss: 350.46
train mean loss: 348.91
epoch train time: 0:00:00.175082
elapsed time: 0:00:53.572375
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:51:38.604214
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.51
 ---- batch: 020 ----
mean loss: 347.70
 ---- batch: 030 ----
mean loss: 352.77
train mean loss: 348.38
epoch train time: 0:00:00.181976
elapsed time: 0:00:53.754513
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:51:38.786355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.98
 ---- batch: 020 ----
mean loss: 351.39
 ---- batch: 030 ----
mean loss: 343.16
train mean loss: 348.74
epoch train time: 0:00:00.178612
elapsed time: 0:00:53.933268
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:51:38.965112
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.56
 ---- batch: 020 ----
mean loss: 354.22
 ---- batch: 030 ----
mean loss: 347.32
train mean loss: 348.62
epoch train time: 0:00:00.176051
elapsed time: 0:00:54.109472
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:51:39.141334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.00
 ---- batch: 020 ----
mean loss: 354.99
 ---- batch: 030 ----
mean loss: 337.57
train mean loss: 348.68
epoch train time: 0:00:00.176486
elapsed time: 0:00:54.286122
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:51:39.317974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.50
 ---- batch: 020 ----
mean loss: 354.06
 ---- batch: 030 ----
mean loss: 346.91
train mean loss: 348.64
epoch train time: 0:00:00.175274
elapsed time: 0:00:54.461562
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:51:39.493405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.38
 ---- batch: 020 ----
mean loss: 343.57
 ---- batch: 030 ----
mean loss: 346.63
train mean loss: 348.12
epoch train time: 0:00:00.178693
elapsed time: 0:00:54.640402
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:51:39.672244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.95
 ---- batch: 020 ----
mean loss: 347.94
 ---- batch: 030 ----
mean loss: 347.67
train mean loss: 348.38
epoch train time: 0:00:00.173689
elapsed time: 0:00:54.814237
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:51:39.846116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.01
 ---- batch: 020 ----
mean loss: 344.42
 ---- batch: 030 ----
mean loss: 348.69
train mean loss: 348.85
epoch train time: 0:00:00.175810
elapsed time: 0:00:54.990257
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:51:40.022116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.04
 ---- batch: 020 ----
mean loss: 354.10
 ---- batch: 030 ----
mean loss: 346.51
train mean loss: 348.20
epoch train time: 0:00:00.173330
elapsed time: 0:00:55.163759
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:51:40.195600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.82
 ---- batch: 020 ----
mean loss: 350.70
 ---- batch: 030 ----
mean loss: 341.43
train mean loss: 348.65
epoch train time: 0:00:00.170975
elapsed time: 0:00:55.334871
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:51:40.366719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.12
 ---- batch: 020 ----
mean loss: 345.15
 ---- batch: 030 ----
mean loss: 348.59
train mean loss: 348.57
epoch train time: 0:00:00.172282
elapsed time: 0:00:55.507298
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:51:40.539141
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.57
 ---- batch: 020 ----
mean loss: 359.79
 ---- batch: 030 ----
mean loss: 343.66
train mean loss: 348.62
epoch train time: 0:00:00.178415
elapsed time: 0:00:55.685889
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:51:40.717744
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.81
 ---- batch: 020 ----
mean loss: 349.93
 ---- batch: 030 ----
mean loss: 342.87
train mean loss: 348.62
epoch train time: 0:00:00.176544
elapsed time: 0:00:55.862587
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:51:40.894429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.98
 ---- batch: 020 ----
mean loss: 346.50
 ---- batch: 030 ----
mean loss: 354.23
train mean loss: 348.42
epoch train time: 0:00:00.171503
elapsed time: 0:00:56.034232
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:51:41.066073
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.80
 ---- batch: 020 ----
mean loss: 343.16
 ---- batch: 030 ----
mean loss: 350.18
train mean loss: 348.19
epoch train time: 0:00:00.169921
elapsed time: 0:00:56.206446
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_9/checkpoint.pth.tar
**** end time: 2019-09-27 16:51:41.238254 ****
