Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17116
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 01:33:07.415429 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:33:07.420734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4959.60
 ---- batch: 020 ----
mean loss: 4806.97
 ---- batch: 030 ----
mean loss: 4658.27
 ---- batch: 040 ----
mean loss: 4479.85
 ---- batch: 050 ----
mean loss: 4310.91
 ---- batch: 060 ----
mean loss: 4085.95
 ---- batch: 070 ----
mean loss: 3918.47
 ---- batch: 080 ----
mean loss: 3704.14
 ---- batch: 090 ----
mean loss: 3487.39
 ---- batch: 100 ----
mean loss: 3298.99
 ---- batch: 110 ----
mean loss: 3099.76
train mean loss: 4043.51
epoch train time: 0:00:33.653947
elapsed time: 0:00:33.660539
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:33:41.076008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2803.35
 ---- batch: 020 ----
mean loss: 2585.82
 ---- batch: 030 ----
mean loss: 2419.37
 ---- batch: 040 ----
mean loss: 2245.01
 ---- batch: 050 ----
mean loss: 2104.19
 ---- batch: 060 ----
mean loss: 1944.95
 ---- batch: 070 ----
mean loss: 1807.03
 ---- batch: 080 ----
mean loss: 1702.53
 ---- batch: 090 ----
mean loss: 1590.10
 ---- batch: 100 ----
mean loss: 1483.68
 ---- batch: 110 ----
mean loss: 1394.84
train mean loss: 1990.58
epoch train time: 0:00:00.732785
elapsed time: 0:00:34.393453
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:33:41.808944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1307.35
 ---- batch: 020 ----
mean loss: 1255.01
 ---- batch: 030 ----
mean loss: 1200.54
 ---- batch: 040 ----
mean loss: 1152.86
 ---- batch: 050 ----
mean loss: 1095.81
 ---- batch: 060 ----
mean loss: 1051.21
 ---- batch: 070 ----
mean loss: 1047.49
 ---- batch: 080 ----
mean loss: 1005.71
 ---- batch: 090 ----
mean loss: 982.00
 ---- batch: 100 ----
mean loss: 959.98
 ---- batch: 110 ----
mean loss: 943.73
train mean loss: 1086.57
epoch train time: 0:00:00.726410
elapsed time: 0:00:35.120019
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:33:42.535498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.26
 ---- batch: 020 ----
mean loss: 914.81
 ---- batch: 030 ----
mean loss: 912.27
 ---- batch: 040 ----
mean loss: 887.77
 ---- batch: 050 ----
mean loss: 870.49
 ---- batch: 060 ----
mean loss: 872.34
 ---- batch: 070 ----
mean loss: 873.45
 ---- batch: 080 ----
mean loss: 846.98
 ---- batch: 090 ----
mean loss: 870.69
 ---- batch: 100 ----
mean loss: 872.88
 ---- batch: 110 ----
mean loss: 852.45
train mean loss: 881.63
epoch train time: 0:00:00.728406
elapsed time: 0:00:35.848562
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:33:43.264039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.46
 ---- batch: 020 ----
mean loss: 848.08
 ---- batch: 030 ----
mean loss: 854.30
 ---- batch: 040 ----
mean loss: 860.40
 ---- batch: 050 ----
mean loss: 853.67
 ---- batch: 060 ----
mean loss: 839.30
 ---- batch: 070 ----
mean loss: 842.64
 ---- batch: 080 ----
mean loss: 839.44
 ---- batch: 090 ----
mean loss: 840.48
 ---- batch: 100 ----
mean loss: 855.56
 ---- batch: 110 ----
mean loss: 853.91
train mean loss: 848.62
epoch train time: 0:00:00.724824
elapsed time: 0:00:36.573526
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:33:43.989004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.11
 ---- batch: 020 ----
mean loss: 842.69
 ---- batch: 030 ----
mean loss: 847.69
 ---- batch: 040 ----
mean loss: 822.45
 ---- batch: 050 ----
mean loss: 836.62
 ---- batch: 060 ----
mean loss: 857.68
 ---- batch: 070 ----
mean loss: 835.82
 ---- batch: 080 ----
mean loss: 856.98
 ---- batch: 090 ----
mean loss: 836.77
 ---- batch: 100 ----
mean loss: 844.24
 ---- batch: 110 ----
mean loss: 842.29
train mean loss: 841.91
epoch train time: 0:00:00.725921
elapsed time: 0:00:37.299585
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:33:44.715062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.42
 ---- batch: 020 ----
mean loss: 824.83
 ---- batch: 030 ----
mean loss: 828.21
 ---- batch: 040 ----
mean loss: 838.05
 ---- batch: 050 ----
mean loss: 811.03
 ---- batch: 060 ----
mean loss: 839.28
 ---- batch: 070 ----
mean loss: 829.88
 ---- batch: 080 ----
mean loss: 853.26
 ---- batch: 090 ----
mean loss: 848.63
 ---- batch: 100 ----
mean loss: 852.94
 ---- batch: 110 ----
mean loss: 840.37
train mean loss: 837.87
epoch train time: 0:00:00.739833
elapsed time: 0:00:38.039561
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:33:45.455063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.12
 ---- batch: 020 ----
mean loss: 820.14
 ---- batch: 030 ----
mean loss: 854.82
 ---- batch: 040 ----
mean loss: 835.59
 ---- batch: 050 ----
mean loss: 836.51
 ---- batch: 060 ----
mean loss: 841.61
 ---- batch: 070 ----
mean loss: 812.63
 ---- batch: 080 ----
mean loss: 831.83
 ---- batch: 090 ----
mean loss: 835.40
 ---- batch: 100 ----
mean loss: 842.34
 ---- batch: 110 ----
mean loss: 837.30
train mean loss: 833.75
epoch train time: 0:00:00.735624
elapsed time: 0:00:38.775358
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:33:46.190835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.74
 ---- batch: 020 ----
mean loss: 835.92
 ---- batch: 030 ----
mean loss: 816.87
 ---- batch: 040 ----
mean loss: 816.47
 ---- batch: 050 ----
mean loss: 833.45
 ---- batch: 060 ----
mean loss: 816.95
 ---- batch: 070 ----
mean loss: 838.92
 ---- batch: 080 ----
mean loss: 823.76
 ---- batch: 090 ----
mean loss: 822.42
 ---- batch: 100 ----
mean loss: 847.36
 ---- batch: 110 ----
mean loss: 835.43
train mean loss: 829.38
epoch train time: 0:00:00.735386
elapsed time: 0:00:39.510883
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:33:46.926376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.18
 ---- batch: 020 ----
mean loss: 834.51
 ---- batch: 030 ----
mean loss: 806.16
 ---- batch: 040 ----
mean loss: 838.69
 ---- batch: 050 ----
mean loss: 819.61
 ---- batch: 060 ----
mean loss: 826.50
 ---- batch: 070 ----
mean loss: 819.78
 ---- batch: 080 ----
mean loss: 854.35
 ---- batch: 090 ----
mean loss: 815.53
 ---- batch: 100 ----
mean loss: 809.45
 ---- batch: 110 ----
mean loss: 830.85
train mean loss: 824.54
epoch train time: 0:00:00.725608
elapsed time: 0:00:40.236646
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:33:47.652123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.68
 ---- batch: 020 ----
mean loss: 805.63
 ---- batch: 030 ----
mean loss: 821.80
 ---- batch: 040 ----
mean loss: 838.33
 ---- batch: 050 ----
mean loss: 812.63
 ---- batch: 060 ----
mean loss: 816.56
 ---- batch: 070 ----
mean loss: 821.64
 ---- batch: 080 ----
mean loss: 814.41
 ---- batch: 090 ----
mean loss: 829.74
 ---- batch: 100 ----
mean loss: 813.67
 ---- batch: 110 ----
mean loss: 816.12
train mean loss: 819.75
epoch train time: 0:00:00.728029
elapsed time: 0:00:40.964827
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:33:48.380319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.38
 ---- batch: 020 ----
mean loss: 796.76
 ---- batch: 030 ----
mean loss: 831.81
 ---- batch: 040 ----
mean loss: 825.03
 ---- batch: 050 ----
mean loss: 815.73
 ---- batch: 060 ----
mean loss: 808.55
 ---- batch: 070 ----
mean loss: 813.01
 ---- batch: 080 ----
mean loss: 805.58
 ---- batch: 090 ----
mean loss: 820.92
 ---- batch: 100 ----
mean loss: 810.50
 ---- batch: 110 ----
mean loss: 783.37
train mean loss: 814.65
epoch train time: 0:00:00.731179
elapsed time: 0:00:41.696156
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:33:49.111642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.50
 ---- batch: 020 ----
mean loss: 818.71
 ---- batch: 030 ----
mean loss: 816.22
 ---- batch: 040 ----
mean loss: 804.95
 ---- batch: 050 ----
mean loss: 808.00
 ---- batch: 060 ----
mean loss: 803.45
 ---- batch: 070 ----
mean loss: 819.36
 ---- batch: 080 ----
mean loss: 784.85
 ---- batch: 090 ----
mean loss: 806.06
 ---- batch: 100 ----
mean loss: 818.89
 ---- batch: 110 ----
mean loss: 791.22
train mean loss: 809.57
epoch train time: 0:00:00.726372
elapsed time: 0:00:42.422680
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:33:49.838176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 793.72
 ---- batch: 020 ----
mean loss: 808.42
 ---- batch: 030 ----
mean loss: 799.93
 ---- batch: 040 ----
mean loss: 789.20
 ---- batch: 050 ----
mean loss: 795.38
 ---- batch: 060 ----
mean loss: 810.43
 ---- batch: 070 ----
mean loss: 808.09
 ---- batch: 080 ----
mean loss: 810.51
 ---- batch: 090 ----
mean loss: 795.95
 ---- batch: 100 ----
mean loss: 812.53
 ---- batch: 110 ----
mean loss: 814.94
train mean loss: 804.29
epoch train time: 0:00:00.727788
elapsed time: 0:00:43.150648
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:33:50.566129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.78
 ---- batch: 020 ----
mean loss: 794.04
 ---- batch: 030 ----
mean loss: 810.19
 ---- batch: 040 ----
mean loss: 811.02
 ---- batch: 050 ----
mean loss: 802.65
 ---- batch: 060 ----
mean loss: 791.14
 ---- batch: 070 ----
mean loss: 795.63
 ---- batch: 080 ----
mean loss: 786.44
 ---- batch: 090 ----
mean loss: 795.41
 ---- batch: 100 ----
mean loss: 795.22
 ---- batch: 110 ----
mean loss: 814.22
train mean loss: 798.96
epoch train time: 0:00:00.728557
elapsed time: 0:00:43.879393
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:33:51.294870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 799.73
 ---- batch: 020 ----
mean loss: 802.16
 ---- batch: 030 ----
mean loss: 791.33
 ---- batch: 040 ----
mean loss: 779.38
 ---- batch: 050 ----
mean loss: 791.44
 ---- batch: 060 ----
mean loss: 801.96
 ---- batch: 070 ----
mean loss: 804.57
 ---- batch: 080 ----
mean loss: 781.20
 ---- batch: 090 ----
mean loss: 783.79
 ---- batch: 100 ----
mean loss: 804.58
 ---- batch: 110 ----
mean loss: 780.75
train mean loss: 793.72
epoch train time: 0:00:00.729314
elapsed time: 0:00:44.608861
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:33:52.024339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.06
 ---- batch: 020 ----
mean loss: 760.03
 ---- batch: 030 ----
mean loss: 786.52
 ---- batch: 040 ----
mean loss: 809.40
 ---- batch: 050 ----
mean loss: 807.01
 ---- batch: 060 ----
mean loss: 803.57
 ---- batch: 070 ----
mean loss: 800.04
 ---- batch: 080 ----
mean loss: 787.17
 ---- batch: 090 ----
mean loss: 773.58
 ---- batch: 100 ----
mean loss: 781.37
 ---- batch: 110 ----
mean loss: 779.02
train mean loss: 788.47
epoch train time: 0:00:00.726467
elapsed time: 0:00:45.335467
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:33:52.750952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 765.79
 ---- batch: 020 ----
mean loss: 790.67
 ---- batch: 030 ----
mean loss: 771.01
 ---- batch: 040 ----
mean loss: 789.31
 ---- batch: 050 ----
mean loss: 799.40
 ---- batch: 060 ----
mean loss: 766.68
 ---- batch: 070 ----
mean loss: 793.86
 ---- batch: 080 ----
mean loss: 777.88
 ---- batch: 090 ----
mean loss: 778.26
 ---- batch: 100 ----
mean loss: 795.10
 ---- batch: 110 ----
mean loss: 790.64
train mean loss: 783.05
epoch train time: 0:00:00.723937
elapsed time: 0:00:46.059564
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:33:53.475041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 779.47
 ---- batch: 020 ----
mean loss: 791.27
 ---- batch: 030 ----
mean loss: 778.77
 ---- batch: 040 ----
mean loss: 759.98
 ---- batch: 050 ----
mean loss: 768.69
 ---- batch: 060 ----
mean loss: 783.95
 ---- batch: 070 ----
mean loss: 776.76
 ---- batch: 080 ----
mean loss: 777.10
 ---- batch: 090 ----
mean loss: 782.43
 ---- batch: 100 ----
mean loss: 772.03
 ---- batch: 110 ----
mean loss: 794.96
train mean loss: 777.76
epoch train time: 0:00:00.731305
elapsed time: 0:00:46.791004
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:33:54.206480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 773.24
 ---- batch: 020 ----
mean loss: 781.01
 ---- batch: 030 ----
mean loss: 784.57
 ---- batch: 040 ----
mean loss: 767.75
 ---- batch: 050 ----
mean loss: 759.78
 ---- batch: 060 ----
mean loss: 780.41
 ---- batch: 070 ----
mean loss: 774.27
 ---- batch: 080 ----
mean loss: 776.40
 ---- batch: 090 ----
mean loss: 766.28
 ---- batch: 100 ----
mean loss: 772.14
 ---- batch: 110 ----
mean loss: 774.18
train mean loss: 772.68
epoch train time: 0:00:00.731246
elapsed time: 0:00:47.522386
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:33:54.937884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.83
 ---- batch: 020 ----
mean loss: 805.23
 ---- batch: 030 ----
mean loss: 770.39
 ---- batch: 040 ----
mean loss: 790.86
 ---- batch: 050 ----
mean loss: 773.74
 ---- batch: 060 ----
mean loss: 775.12
 ---- batch: 070 ----
mean loss: 764.27
 ---- batch: 080 ----
mean loss: 774.39
 ---- batch: 090 ----
mean loss: 755.70
 ---- batch: 100 ----
mean loss: 752.62
 ---- batch: 110 ----
mean loss: 747.62
train mean loss: 767.67
epoch train time: 0:00:00.728082
elapsed time: 0:00:48.250652
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:33:55.666135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 756.73
 ---- batch: 020 ----
mean loss: 778.41
 ---- batch: 030 ----
mean loss: 754.24
 ---- batch: 040 ----
mean loss: 779.68
 ---- batch: 050 ----
mean loss: 766.25
 ---- batch: 060 ----
mean loss: 759.31
 ---- batch: 070 ----
mean loss: 771.16
 ---- batch: 080 ----
mean loss: 757.75
 ---- batch: 090 ----
mean loss: 753.94
 ---- batch: 100 ----
mean loss: 751.61
 ---- batch: 110 ----
mean loss: 766.10
train mean loss: 762.68
epoch train time: 0:00:00.746654
elapsed time: 0:00:48.997453
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:33:56.412933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.92
 ---- batch: 020 ----
mean loss: 746.02
 ---- batch: 030 ----
mean loss: 737.97
 ---- batch: 040 ----
mean loss: 763.20
 ---- batch: 050 ----
mean loss: 777.10
 ---- batch: 060 ----
mean loss: 753.82
 ---- batch: 070 ----
mean loss: 777.22
 ---- batch: 080 ----
mean loss: 747.19
 ---- batch: 090 ----
mean loss: 751.77
 ---- batch: 100 ----
mean loss: 779.43
 ---- batch: 110 ----
mean loss: 750.09
train mean loss: 757.58
epoch train time: 0:00:00.738466
elapsed time: 0:00:49.736063
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:33:57.151541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.86
 ---- batch: 020 ----
mean loss: 753.94
 ---- batch: 030 ----
mean loss: 767.93
 ---- batch: 040 ----
mean loss: 748.61
 ---- batch: 050 ----
mean loss: 766.94
 ---- batch: 060 ----
mean loss: 746.09
 ---- batch: 070 ----
mean loss: 748.47
 ---- batch: 080 ----
mean loss: 761.81
 ---- batch: 090 ----
mean loss: 748.05
 ---- batch: 100 ----
mean loss: 757.73
 ---- batch: 110 ----
mean loss: 737.04
train mean loss: 752.29
epoch train time: 0:00:00.744538
elapsed time: 0:00:50.480744
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:33:57.896224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 746.94
 ---- batch: 020 ----
mean loss: 748.34
 ---- batch: 030 ----
mean loss: 735.12
 ---- batch: 040 ----
mean loss: 754.29
 ---- batch: 050 ----
mean loss: 748.14
 ---- batch: 060 ----
mean loss: 756.71
 ---- batch: 070 ----
mean loss: 729.09
 ---- batch: 080 ----
mean loss: 748.72
 ---- batch: 090 ----
mean loss: 755.65
 ---- batch: 100 ----
mean loss: 734.51
 ---- batch: 110 ----
mean loss: 754.56
train mean loss: 746.87
epoch train time: 0:00:00.747213
elapsed time: 0:00:51.228093
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:33:58.643570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 746.40
 ---- batch: 020 ----
mean loss: 742.69
 ---- batch: 030 ----
mean loss: 735.56
 ---- batch: 040 ----
mean loss: 742.08
 ---- batch: 050 ----
mean loss: 741.11
 ---- batch: 060 ----
mean loss: 748.86
 ---- batch: 070 ----
mean loss: 754.62
 ---- batch: 080 ----
mean loss: 727.33
 ---- batch: 090 ----
mean loss: 749.01
 ---- batch: 100 ----
mean loss: 735.06
 ---- batch: 110 ----
mean loss: 737.45
train mean loss: 741.14
epoch train time: 0:00:00.750015
elapsed time: 0:00:51.978288
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:33:59.393782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 741.74
 ---- batch: 020 ----
mean loss: 742.27
 ---- batch: 030 ----
mean loss: 747.91
 ---- batch: 040 ----
mean loss: 741.35
 ---- batch: 050 ----
mean loss: 731.00
 ---- batch: 060 ----
mean loss: 720.58
 ---- batch: 070 ----
mean loss: 718.67
 ---- batch: 080 ----
mean loss: 753.11
 ---- batch: 090 ----
mean loss: 747.64
 ---- batch: 100 ----
mean loss: 722.26
 ---- batch: 110 ----
mean loss: 726.94
train mean loss: 735.29
epoch train time: 0:00:00.752471
elapsed time: 0:00:52.730918
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:34:00.146413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 729.73
 ---- batch: 020 ----
mean loss: 720.97
 ---- batch: 030 ----
mean loss: 746.64
 ---- batch: 040 ----
mean loss: 747.15
 ---- batch: 050 ----
mean loss: 726.22
 ---- batch: 060 ----
mean loss: 726.41
 ---- batch: 070 ----
mean loss: 723.56
 ---- batch: 080 ----
mean loss: 725.80
 ---- batch: 090 ----
mean loss: 734.25
 ---- batch: 100 ----
mean loss: 719.08
 ---- batch: 110 ----
mean loss: 727.97
train mean loss: 728.97
epoch train time: 0:00:00.766992
elapsed time: 0:00:53.498104
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:34:00.913583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.11
 ---- batch: 020 ----
mean loss: 721.77
 ---- batch: 030 ----
mean loss: 733.83
 ---- batch: 040 ----
mean loss: 743.24
 ---- batch: 050 ----
mean loss: 742.64
 ---- batch: 060 ----
mean loss: 714.43
 ---- batch: 070 ----
mean loss: 723.37
 ---- batch: 080 ----
mean loss: 722.78
 ---- batch: 090 ----
mean loss: 707.62
 ---- batch: 100 ----
mean loss: 726.23
 ---- batch: 110 ----
mean loss: 717.39
train mean loss: 722.54
epoch train time: 0:00:00.749513
elapsed time: 0:00:54.247757
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:34:01.663234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.06
 ---- batch: 020 ----
mean loss: 701.93
 ---- batch: 030 ----
mean loss: 712.30
 ---- batch: 040 ----
mean loss: 712.09
 ---- batch: 050 ----
mean loss: 724.10
 ---- batch: 060 ----
mean loss: 713.05
 ---- batch: 070 ----
mean loss: 714.91
 ---- batch: 080 ----
mean loss: 725.09
 ---- batch: 090 ----
mean loss: 715.56
 ---- batch: 100 ----
mean loss: 721.48
 ---- batch: 110 ----
mean loss: 714.34
train mean loss: 715.67
epoch train time: 0:00:00.753052
elapsed time: 0:00:55.000953
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:34:02.416442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.08
 ---- batch: 020 ----
mean loss: 704.28
 ---- batch: 030 ----
mean loss: 714.51
 ---- batch: 040 ----
mean loss: 715.52
 ---- batch: 050 ----
mean loss: 711.09
 ---- batch: 060 ----
mean loss: 710.15
 ---- batch: 070 ----
mean loss: 676.37
 ---- batch: 080 ----
mean loss: 717.98
 ---- batch: 090 ----
mean loss: 708.92
 ---- batch: 100 ----
mean loss: 710.69
 ---- batch: 110 ----
mean loss: 713.65
train mean loss: 708.56
epoch train time: 0:00:00.738453
elapsed time: 0:00:55.739570
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:34:03.155048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.65
 ---- batch: 020 ----
mean loss: 690.05
 ---- batch: 030 ----
mean loss: 717.08
 ---- batch: 040 ----
mean loss: 721.38
 ---- batch: 050 ----
mean loss: 684.30
 ---- batch: 060 ----
mean loss: 704.24
 ---- batch: 070 ----
mean loss: 702.49
 ---- batch: 080 ----
mean loss: 701.59
 ---- batch: 090 ----
mean loss: 696.44
 ---- batch: 100 ----
mean loss: 700.39
 ---- batch: 110 ----
mean loss: 687.72
train mean loss: 701.17
epoch train time: 0:00:00.741694
elapsed time: 0:00:56.481411
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:34:03.896906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.71
 ---- batch: 020 ----
mean loss: 687.50
 ---- batch: 030 ----
mean loss: 705.21
 ---- batch: 040 ----
mean loss: 707.50
 ---- batch: 050 ----
mean loss: 696.99
 ---- batch: 060 ----
mean loss: 702.32
 ---- batch: 070 ----
mean loss: 669.57
 ---- batch: 080 ----
mean loss: 700.96
 ---- batch: 090 ----
mean loss: 689.73
 ---- batch: 100 ----
mean loss: 698.29
 ---- batch: 110 ----
mean loss: 682.64
train mean loss: 693.06
epoch train time: 0:00:00.735808
elapsed time: 0:00:57.217379
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:34:04.632877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.21
 ---- batch: 020 ----
mean loss: 689.58
 ---- batch: 030 ----
mean loss: 679.98
 ---- batch: 040 ----
mean loss: 686.40
 ---- batch: 050 ----
mean loss: 670.57
 ---- batch: 060 ----
mean loss: 678.46
 ---- batch: 070 ----
mean loss: 692.01
 ---- batch: 080 ----
mean loss: 692.86
 ---- batch: 090 ----
mean loss: 691.34
 ---- batch: 100 ----
mean loss: 689.90
 ---- batch: 110 ----
mean loss: 686.50
train mean loss: 684.83
epoch train time: 0:00:00.732717
elapsed time: 0:00:57.950313
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:34:05.365798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 676.93
 ---- batch: 020 ----
mean loss: 652.70
 ---- batch: 030 ----
mean loss: 673.29
 ---- batch: 040 ----
mean loss: 690.07
 ---- batch: 050 ----
mean loss: 677.50
 ---- batch: 060 ----
mean loss: 685.36
 ---- batch: 070 ----
mean loss: 661.00
 ---- batch: 080 ----
mean loss: 683.96
 ---- batch: 090 ----
mean loss: 683.20
 ---- batch: 100 ----
mean loss: 682.69
 ---- batch: 110 ----
mean loss: 677.94
train mean loss: 676.22
epoch train time: 0:00:00.742716
elapsed time: 0:00:58.693174
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:34:06.108665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 697.00
 ---- batch: 020 ----
mean loss: 681.22
 ---- batch: 030 ----
mean loss: 666.35
 ---- batch: 040 ----
mean loss: 666.38
 ---- batch: 050 ----
mean loss: 667.47
 ---- batch: 060 ----
mean loss: 660.38
 ---- batch: 070 ----
mean loss: 663.37
 ---- batch: 080 ----
mean loss: 666.18
 ---- batch: 090 ----
mean loss: 651.85
 ---- batch: 100 ----
mean loss: 664.56
 ---- batch: 110 ----
mean loss: 656.44
train mean loss: 667.29
epoch train time: 0:00:00.745378
elapsed time: 0:00:59.438763
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:34:06.854244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.17
 ---- batch: 020 ----
mean loss: 659.29
 ---- batch: 030 ----
mean loss: 666.50
 ---- batch: 040 ----
mean loss: 659.80
 ---- batch: 050 ----
mean loss: 650.24
 ---- batch: 060 ----
mean loss: 665.89
 ---- batch: 070 ----
mean loss: 650.17
 ---- batch: 080 ----
mean loss: 646.79
 ---- batch: 090 ----
mean loss: 666.44
 ---- batch: 100 ----
mean loss: 660.04
 ---- batch: 110 ----
mean loss: 639.37
train mean loss: 657.91
epoch train time: 0:00:00.740479
elapsed time: 0:01:00.179388
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:34:07.594885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 655.41
 ---- batch: 020 ----
mean loss: 658.19
 ---- batch: 030 ----
mean loss: 652.81
 ---- batch: 040 ----
mean loss: 649.32
 ---- batch: 050 ----
mean loss: 666.35
 ---- batch: 060 ----
mean loss: 635.35
 ---- batch: 070 ----
mean loss: 640.46
 ---- batch: 080 ----
mean loss: 638.98
 ---- batch: 090 ----
mean loss: 634.02
 ---- batch: 100 ----
mean loss: 646.65
 ---- batch: 110 ----
mean loss: 657.82
train mean loss: 648.35
epoch train time: 0:00:00.743563
elapsed time: 0:01:00.923126
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:34:08.338605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.78
 ---- batch: 020 ----
mean loss: 644.06
 ---- batch: 030 ----
mean loss: 647.40
 ---- batch: 040 ----
mean loss: 642.93
 ---- batch: 050 ----
mean loss: 620.70
 ---- batch: 060 ----
mean loss: 632.32
 ---- batch: 070 ----
mean loss: 636.38
 ---- batch: 080 ----
mean loss: 646.83
 ---- batch: 090 ----
mean loss: 630.12
 ---- batch: 100 ----
mean loss: 630.48
 ---- batch: 110 ----
mean loss: 638.20
train mean loss: 638.41
epoch train time: 0:00:00.741267
elapsed time: 0:01:01.664547
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:34:09.080023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.81
 ---- batch: 020 ----
mean loss: 633.46
 ---- batch: 030 ----
mean loss: 620.43
 ---- batch: 040 ----
mean loss: 630.34
 ---- batch: 050 ----
mean loss: 630.47
 ---- batch: 060 ----
mean loss: 622.10
 ---- batch: 070 ----
mean loss: 626.24
 ---- batch: 080 ----
mean loss: 636.06
 ---- batch: 090 ----
mean loss: 625.91
 ---- batch: 100 ----
mean loss: 631.73
 ---- batch: 110 ----
mean loss: 628.50
train mean loss: 627.94
epoch train time: 0:00:00.733545
elapsed time: 0:01:02.398245
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:34:09.813744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 615.57
 ---- batch: 020 ----
mean loss: 630.59
 ---- batch: 030 ----
mean loss: 608.98
 ---- batch: 040 ----
mean loss: 626.91
 ---- batch: 050 ----
mean loss: 623.59
 ---- batch: 060 ----
mean loss: 619.52
 ---- batch: 070 ----
mean loss: 616.52
 ---- batch: 080 ----
mean loss: 619.58
 ---- batch: 090 ----
mean loss: 603.71
 ---- batch: 100 ----
mean loss: 612.67
 ---- batch: 110 ----
mean loss: 616.46
train mean loss: 617.27
epoch train time: 0:00:00.739527
elapsed time: 0:01:03.137972
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:34:10.553460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 617.13
 ---- batch: 020 ----
mean loss: 600.70
 ---- batch: 030 ----
mean loss: 611.03
 ---- batch: 040 ----
mean loss: 609.89
 ---- batch: 050 ----
mean loss: 599.34
 ---- batch: 060 ----
mean loss: 599.23
 ---- batch: 070 ----
mean loss: 618.26
 ---- batch: 080 ----
mean loss: 620.29
 ---- batch: 090 ----
mean loss: 607.82
 ---- batch: 100 ----
mean loss: 602.08
 ---- batch: 110 ----
mean loss: 590.76
train mean loss: 606.10
epoch train time: 0:00:00.728208
elapsed time: 0:01:03.866367
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:34:11.281866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 599.65
 ---- batch: 020 ----
mean loss: 593.92
 ---- batch: 030 ----
mean loss: 598.97
 ---- batch: 040 ----
mean loss: 601.37
 ---- batch: 050 ----
mean loss: 601.62
 ---- batch: 060 ----
mean loss: 596.26
 ---- batch: 070 ----
mean loss: 596.89
 ---- batch: 080 ----
mean loss: 592.99
 ---- batch: 090 ----
mean loss: 590.81
 ---- batch: 100 ----
mean loss: 578.98
 ---- batch: 110 ----
mean loss: 591.03
train mean loss: 594.78
epoch train time: 0:00:00.731360
elapsed time: 0:01:04.597887
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:34:12.013371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 582.81
 ---- batch: 020 ----
mean loss: 581.09
 ---- batch: 030 ----
mean loss: 596.93
 ---- batch: 040 ----
mean loss: 587.03
 ---- batch: 050 ----
mean loss: 584.88
 ---- batch: 060 ----
mean loss: 583.02
 ---- batch: 070 ----
mean loss: 568.52
 ---- batch: 080 ----
mean loss: 589.30
 ---- batch: 090 ----
mean loss: 574.86
 ---- batch: 100 ----
mean loss: 583.62
 ---- batch: 110 ----
mean loss: 576.54
train mean loss: 582.98
epoch train time: 0:00:00.741722
elapsed time: 0:01:05.339772
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:34:12.755252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.52
 ---- batch: 020 ----
mean loss: 572.78
 ---- batch: 030 ----
mean loss: 568.97
 ---- batch: 040 ----
mean loss: 578.65
 ---- batch: 050 ----
mean loss: 571.06
 ---- batch: 060 ----
mean loss: 598.15
 ---- batch: 070 ----
mean loss: 575.29
 ---- batch: 080 ----
mean loss: 562.79
 ---- batch: 090 ----
mean loss: 564.21
 ---- batch: 100 ----
mean loss: 552.57
 ---- batch: 110 ----
mean loss: 565.10
train mean loss: 570.94
epoch train time: 0:00:00.739239
elapsed time: 0:01:06.079206
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:34:13.494684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.34
 ---- batch: 020 ----
mean loss: 581.29
 ---- batch: 030 ----
mean loss: 569.72
 ---- batch: 040 ----
mean loss: 566.18
 ---- batch: 050 ----
mean loss: 553.05
 ---- batch: 060 ----
mean loss: 554.54
 ---- batch: 070 ----
mean loss: 552.87
 ---- batch: 080 ----
mean loss: 557.41
 ---- batch: 090 ----
mean loss: 547.67
 ---- batch: 100 ----
mean loss: 558.96
 ---- batch: 110 ----
mean loss: 557.34
train mean loss: 558.60
epoch train time: 0:00:00.730644
elapsed time: 0:01:06.810024
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:34:14.225526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.02
 ---- batch: 020 ----
mean loss: 551.23
 ---- batch: 030 ----
mean loss: 554.23
 ---- batch: 040 ----
mean loss: 546.11
 ---- batch: 050 ----
mean loss: 546.51
 ---- batch: 060 ----
mean loss: 531.40
 ---- batch: 070 ----
mean loss: 559.04
 ---- batch: 080 ----
mean loss: 541.18
 ---- batch: 090 ----
mean loss: 546.36
 ---- batch: 100 ----
mean loss: 535.80
 ---- batch: 110 ----
mean loss: 548.05
train mean loss: 546.34
epoch train time: 0:00:00.730828
elapsed time: 0:01:07.541016
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:34:14.956495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.26
 ---- batch: 020 ----
mean loss: 544.71
 ---- batch: 030 ----
mean loss: 531.60
 ---- batch: 040 ----
mean loss: 524.51
 ---- batch: 050 ----
mean loss: 542.09
 ---- batch: 060 ----
mean loss: 530.59
 ---- batch: 070 ----
mean loss: 546.98
 ---- batch: 080 ----
mean loss: 530.81
 ---- batch: 090 ----
mean loss: 540.61
 ---- batch: 100 ----
mean loss: 514.30
 ---- batch: 110 ----
mean loss: 529.08
train mean loss: 533.72
epoch train time: 0:00:00.731880
elapsed time: 0:01:08.273036
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:34:15.688517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.86
 ---- batch: 020 ----
mean loss: 525.22
 ---- batch: 030 ----
mean loss: 527.27
 ---- batch: 040 ----
mean loss: 514.28
 ---- batch: 050 ----
mean loss: 514.79
 ---- batch: 060 ----
mean loss: 529.14
 ---- batch: 070 ----
mean loss: 530.51
 ---- batch: 080 ----
mean loss: 521.23
 ---- batch: 090 ----
mean loss: 515.59
 ---- batch: 100 ----
mean loss: 517.36
 ---- batch: 110 ----
mean loss: 508.30
train mean loss: 521.32
epoch train time: 0:00:00.739730
elapsed time: 0:01:09.012923
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:34:16.428401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.33
 ---- batch: 020 ----
mean loss: 519.35
 ---- batch: 030 ----
mean loss: 513.92
 ---- batch: 040 ----
mean loss: 512.61
 ---- batch: 050 ----
mean loss: 502.43
 ---- batch: 060 ----
mean loss: 506.59
 ---- batch: 070 ----
mean loss: 501.78
 ---- batch: 080 ----
mean loss: 509.55
 ---- batch: 090 ----
mean loss: 512.09
 ---- batch: 100 ----
mean loss: 499.03
 ---- batch: 110 ----
mean loss: 498.08
train mean loss: 508.71
epoch train time: 0:00:00.730926
elapsed time: 0:01:09.743986
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:34:17.159472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.65
 ---- batch: 020 ----
mean loss: 501.41
 ---- batch: 030 ----
mean loss: 498.54
 ---- batch: 040 ----
mean loss: 501.63
 ---- batch: 050 ----
mean loss: 499.17
 ---- batch: 060 ----
mean loss: 495.89
 ---- batch: 070 ----
mean loss: 501.77
 ---- batch: 080 ----
mean loss: 495.83
 ---- batch: 090 ----
mean loss: 488.58
 ---- batch: 100 ----
mean loss: 485.34
 ---- batch: 110 ----
mean loss: 489.24
train mean loss: 496.43
epoch train time: 0:00:00.729493
elapsed time: 0:01:10.473624
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:34:17.889145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.09
 ---- batch: 020 ----
mean loss: 495.28
 ---- batch: 030 ----
mean loss: 501.10
 ---- batch: 040 ----
mean loss: 481.31
 ---- batch: 050 ----
mean loss: 477.32
 ---- batch: 060 ----
mean loss: 488.63
 ---- batch: 070 ----
mean loss: 481.41
 ---- batch: 080 ----
mean loss: 472.94
 ---- batch: 090 ----
mean loss: 474.64
 ---- batch: 100 ----
mean loss: 485.46
 ---- batch: 110 ----
mean loss: 480.22
train mean loss: 484.08
epoch train time: 0:00:00.728191
elapsed time: 0:01:11.201996
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:34:18.617473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.97
 ---- batch: 020 ----
mean loss: 484.33
 ---- batch: 030 ----
mean loss: 471.06
 ---- batch: 040 ----
mean loss: 466.59
 ---- batch: 050 ----
mean loss: 461.13
 ---- batch: 060 ----
mean loss: 476.64
 ---- batch: 070 ----
mean loss: 470.11
 ---- batch: 080 ----
mean loss: 472.61
 ---- batch: 090 ----
mean loss: 467.69
 ---- batch: 100 ----
mean loss: 461.86
 ---- batch: 110 ----
mean loss: 476.44
train mean loss: 471.95
epoch train time: 0:00:00.727637
elapsed time: 0:01:11.929770
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:34:19.345267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.59
 ---- batch: 020 ----
mean loss: 469.07
 ---- batch: 030 ----
mean loss: 467.50
 ---- batch: 040 ----
mean loss: 457.96
 ---- batch: 050 ----
mean loss: 450.43
 ---- batch: 060 ----
mean loss: 463.65
 ---- batch: 070 ----
mean loss: 463.04
 ---- batch: 080 ----
mean loss: 461.34
 ---- batch: 090 ----
mean loss: 446.71
 ---- batch: 100 ----
mean loss: 460.12
 ---- batch: 110 ----
mean loss: 453.60
train mean loss: 460.04
epoch train time: 0:00:00.728167
elapsed time: 0:01:12.658090
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:34:20.073567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.73
 ---- batch: 020 ----
mean loss: 452.77
 ---- batch: 030 ----
mean loss: 458.40
 ---- batch: 040 ----
mean loss: 454.88
 ---- batch: 050 ----
mean loss: 456.26
 ---- batch: 060 ----
mean loss: 431.98
 ---- batch: 070 ----
mean loss: 440.38
 ---- batch: 080 ----
mean loss: 441.14
 ---- batch: 090 ----
mean loss: 436.89
 ---- batch: 100 ----
mean loss: 455.81
 ---- batch: 110 ----
mean loss: 446.09
train mean loss: 448.14
epoch train time: 0:00:00.732780
elapsed time: 0:01:13.391034
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:34:20.806516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.58
 ---- batch: 020 ----
mean loss: 440.50
 ---- batch: 030 ----
mean loss: 451.96
 ---- batch: 040 ----
mean loss: 434.78
 ---- batch: 050 ----
mean loss: 433.85
 ---- batch: 060 ----
mean loss: 426.92
 ---- batch: 070 ----
mean loss: 429.19
 ---- batch: 080 ----
mean loss: 431.94
 ---- batch: 090 ----
mean loss: 438.87
 ---- batch: 100 ----
mean loss: 425.01
 ---- batch: 110 ----
mean loss: 433.10
train mean loss: 436.44
epoch train time: 0:00:00.739957
elapsed time: 0:01:14.131138
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:34:21.546617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.72
 ---- batch: 020 ----
mean loss: 430.61
 ---- batch: 030 ----
mean loss: 430.91
 ---- batch: 040 ----
mean loss: 426.99
 ---- batch: 050 ----
mean loss: 433.54
 ---- batch: 060 ----
mean loss: 436.14
 ---- batch: 070 ----
mean loss: 408.69
 ---- batch: 080 ----
mean loss: 411.94
 ---- batch: 090 ----
mean loss: 415.96
 ---- batch: 100 ----
mean loss: 414.65
 ---- batch: 110 ----
mean loss: 416.72
train mean loss: 424.05
epoch train time: 0:00:00.730696
elapsed time: 0:01:14.861977
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:34:22.277459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.78
 ---- batch: 020 ----
mean loss: 419.40
 ---- batch: 030 ----
mean loss: 404.29
 ---- batch: 040 ----
mean loss: 405.71
 ---- batch: 050 ----
mean loss: 405.09
 ---- batch: 060 ----
mean loss: 402.64
 ---- batch: 070 ----
mean loss: 402.84
 ---- batch: 080 ----
mean loss: 386.22
 ---- batch: 090 ----
mean loss: 407.59
 ---- batch: 100 ----
mean loss: 400.15
 ---- batch: 110 ----
mean loss: 401.41
train mean loss: 404.97
epoch train time: 0:00:00.745754
elapsed time: 0:01:15.607874
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:34:23.023353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.31
 ---- batch: 020 ----
mean loss: 389.02
 ---- batch: 030 ----
mean loss: 390.41
 ---- batch: 040 ----
mean loss: 382.01
 ---- batch: 050 ----
mean loss: 375.87
 ---- batch: 060 ----
mean loss: 359.36
 ---- batch: 070 ----
mean loss: 371.05
 ---- batch: 080 ----
mean loss: 375.77
 ---- batch: 090 ----
mean loss: 364.31
 ---- batch: 100 ----
mean loss: 369.53
 ---- batch: 110 ----
mean loss: 366.51
train mean loss: 374.85
epoch train time: 0:00:00.727763
elapsed time: 0:01:16.335779
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:34:23.751286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.89
 ---- batch: 020 ----
mean loss: 359.74
 ---- batch: 030 ----
mean loss: 364.60
 ---- batch: 040 ----
mean loss: 349.68
 ---- batch: 050 ----
mean loss: 346.50
 ---- batch: 060 ----
mean loss: 348.55
 ---- batch: 070 ----
mean loss: 332.34
 ---- batch: 080 ----
mean loss: 344.79
 ---- batch: 090 ----
mean loss: 356.77
 ---- batch: 100 ----
mean loss: 346.83
 ---- batch: 110 ----
mean loss: 346.69
train mean loss: 350.84
epoch train time: 0:00:00.724932
elapsed time: 0:01:17.060878
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:34:24.476356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.29
 ---- batch: 020 ----
mean loss: 333.12
 ---- batch: 030 ----
mean loss: 344.09
 ---- batch: 040 ----
mean loss: 335.24
 ---- batch: 050 ----
mean loss: 342.80
 ---- batch: 060 ----
mean loss: 325.94
 ---- batch: 070 ----
mean loss: 325.18
 ---- batch: 080 ----
mean loss: 330.87
 ---- batch: 090 ----
mean loss: 329.68
 ---- batch: 100 ----
mean loss: 331.51
 ---- batch: 110 ----
mean loss: 327.46
train mean loss: 333.74
epoch train time: 0:00:00.730104
elapsed time: 0:01:17.791121
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:34:25.206600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.67
 ---- batch: 020 ----
mean loss: 327.95
 ---- batch: 030 ----
mean loss: 323.85
 ---- batch: 040 ----
mean loss: 317.52
 ---- batch: 050 ----
mean loss: 322.61
 ---- batch: 060 ----
mean loss: 323.19
 ---- batch: 070 ----
mean loss: 325.45
 ---- batch: 080 ----
mean loss: 312.87
 ---- batch: 090 ----
mean loss: 323.66
 ---- batch: 100 ----
mean loss: 308.09
 ---- batch: 110 ----
mean loss: 322.56
train mean loss: 319.78
epoch train time: 0:00:00.740688
elapsed time: 0:01:18.531964
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:34:25.947455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.79
 ---- batch: 020 ----
mean loss: 306.08
 ---- batch: 030 ----
mean loss: 305.47
 ---- batch: 040 ----
mean loss: 316.08
 ---- batch: 050 ----
mean loss: 308.30
 ---- batch: 060 ----
mean loss: 307.73
 ---- batch: 070 ----
mean loss: 293.38
 ---- batch: 080 ----
mean loss: 310.65
 ---- batch: 090 ----
mean loss: 309.28
 ---- batch: 100 ----
mean loss: 309.91
 ---- batch: 110 ----
mean loss: 307.77
train mean loss: 308.21
epoch train time: 0:00:00.736304
elapsed time: 0:01:19.268420
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:34:26.683937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.17
 ---- batch: 020 ----
mean loss: 296.27
 ---- batch: 030 ----
mean loss: 300.39
 ---- batch: 040 ----
mean loss: 303.36
 ---- batch: 050 ----
mean loss: 292.65
 ---- batch: 060 ----
mean loss: 294.68
 ---- batch: 070 ----
mean loss: 307.56
 ---- batch: 080 ----
mean loss: 293.11
 ---- batch: 090 ----
mean loss: 295.20
 ---- batch: 100 ----
mean loss: 299.92
 ---- batch: 110 ----
mean loss: 311.05
train mean loss: 298.95
epoch train time: 0:00:00.732107
elapsed time: 0:01:20.000707
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:34:27.416184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.83
 ---- batch: 020 ----
mean loss: 296.89
 ---- batch: 030 ----
mean loss: 293.69
 ---- batch: 040 ----
mean loss: 275.20
 ---- batch: 050 ----
mean loss: 288.64
 ---- batch: 060 ----
mean loss: 292.01
 ---- batch: 070 ----
mean loss: 295.24
 ---- batch: 080 ----
mean loss: 291.80
 ---- batch: 090 ----
mean loss: 298.54
 ---- batch: 100 ----
mean loss: 287.10
 ---- batch: 110 ----
mean loss: 287.10
train mean loss: 291.04
epoch train time: 0:00:00.727202
elapsed time: 0:01:20.728080
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:34:28.143575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.99
 ---- batch: 020 ----
mean loss: 278.56
 ---- batch: 030 ----
mean loss: 283.62
 ---- batch: 040 ----
mean loss: 289.20
 ---- batch: 050 ----
mean loss: 294.35
 ---- batch: 060 ----
mean loss: 279.65
 ---- batch: 070 ----
mean loss: 287.10
 ---- batch: 080 ----
mean loss: 278.36
 ---- batch: 090 ----
mean loss: 278.84
 ---- batch: 100 ----
mean loss: 273.88
 ---- batch: 110 ----
mean loss: 285.61
train mean loss: 284.13
epoch train time: 0:00:00.725363
elapsed time: 0:01:21.453617
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:34:28.869125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.92
 ---- batch: 020 ----
mean loss: 281.79
 ---- batch: 030 ----
mean loss: 278.48
 ---- batch: 040 ----
mean loss: 279.45
 ---- batch: 050 ----
mean loss: 271.94
 ---- batch: 060 ----
mean loss: 287.33
 ---- batch: 070 ----
mean loss: 276.90
 ---- batch: 080 ----
mean loss: 281.93
 ---- batch: 090 ----
mean loss: 274.77
 ---- batch: 100 ----
mean loss: 278.56
 ---- batch: 110 ----
mean loss: 270.28
train mean loss: 278.64
epoch train time: 0:00:00.730944
elapsed time: 0:01:22.184727
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:34:29.600234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.69
 ---- batch: 020 ----
mean loss: 275.98
 ---- batch: 030 ----
mean loss: 266.97
 ---- batch: 040 ----
mean loss: 275.53
 ---- batch: 050 ----
mean loss: 274.98
 ---- batch: 060 ----
mean loss: 269.23
 ---- batch: 070 ----
mean loss: 268.43
 ---- batch: 080 ----
mean loss: 272.28
 ---- batch: 090 ----
mean loss: 273.72
 ---- batch: 100 ----
mean loss: 272.90
 ---- batch: 110 ----
mean loss: 280.89
train mean loss: 273.75
epoch train time: 0:00:00.737951
elapsed time: 0:01:22.922864
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:34:30.338358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.68
 ---- batch: 020 ----
mean loss: 276.48
 ---- batch: 030 ----
mean loss: 264.53
 ---- batch: 040 ----
mean loss: 274.17
 ---- batch: 050 ----
mean loss: 271.96
 ---- batch: 060 ----
mean loss: 277.38
 ---- batch: 070 ----
mean loss: 272.02
 ---- batch: 080 ----
mean loss: 261.65
 ---- batch: 090 ----
mean loss: 267.64
 ---- batch: 100 ----
mean loss: 255.80
 ---- batch: 110 ----
mean loss: 273.44
train mean loss: 269.41
epoch train time: 0:00:00.745770
elapsed time: 0:01:23.668821
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:34:31.084333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.52
 ---- batch: 020 ----
mean loss: 262.93
 ---- batch: 030 ----
mean loss: 266.77
 ---- batch: 040 ----
mean loss: 257.67
 ---- batch: 050 ----
mean loss: 264.43
 ---- batch: 060 ----
mean loss: 273.93
 ---- batch: 070 ----
mean loss: 267.42
 ---- batch: 080 ----
mean loss: 262.88
 ---- batch: 090 ----
mean loss: 264.67
 ---- batch: 100 ----
mean loss: 257.14
 ---- batch: 110 ----
mean loss: 269.29
train mean loss: 265.82
epoch train time: 0:00:00.745856
elapsed time: 0:01:24.414865
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:34:31.830344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.19
 ---- batch: 020 ----
mean loss: 264.38
 ---- batch: 030 ----
mean loss: 267.74
 ---- batch: 040 ----
mean loss: 268.68
 ---- batch: 050 ----
mean loss: 257.17
 ---- batch: 060 ----
mean loss: 256.32
 ---- batch: 070 ----
mean loss: 261.59
 ---- batch: 080 ----
mean loss: 262.96
 ---- batch: 090 ----
mean loss: 267.15
 ---- batch: 100 ----
mean loss: 263.34
 ---- batch: 110 ----
mean loss: 264.11
train mean loss: 262.60
epoch train time: 0:00:00.732801
elapsed time: 0:01:25.147816
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:34:32.563293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.25
 ---- batch: 020 ----
mean loss: 256.30
 ---- batch: 030 ----
mean loss: 264.20
 ---- batch: 040 ----
mean loss: 257.64
 ---- batch: 050 ----
mean loss: 242.65
 ---- batch: 060 ----
mean loss: 260.30
 ---- batch: 070 ----
mean loss: 266.91
 ---- batch: 080 ----
mean loss: 268.84
 ---- batch: 090 ----
mean loss: 257.28
 ---- batch: 100 ----
mean loss: 264.59
 ---- batch: 110 ----
mean loss: 258.52
train mean loss: 259.85
epoch train time: 0:00:00.730586
elapsed time: 0:01:25.878545
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:34:33.294044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.41
 ---- batch: 020 ----
mean loss: 255.56
 ---- batch: 030 ----
mean loss: 259.80
 ---- batch: 040 ----
mean loss: 262.66
 ---- batch: 050 ----
mean loss: 263.74
 ---- batch: 060 ----
mean loss: 253.48
 ---- batch: 070 ----
mean loss: 256.08
 ---- batch: 080 ----
mean loss: 255.43
 ---- batch: 090 ----
mean loss: 261.51
 ---- batch: 100 ----
mean loss: 259.83
 ---- batch: 110 ----
mean loss: 255.35
train mean loss: 257.30
epoch train time: 0:00:00.739130
elapsed time: 0:01:26.617841
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:34:34.033322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.07
 ---- batch: 020 ----
mean loss: 260.65
 ---- batch: 030 ----
mean loss: 252.26
 ---- batch: 040 ----
mean loss: 251.72
 ---- batch: 050 ----
mean loss: 248.93
 ---- batch: 060 ----
mean loss: 256.50
 ---- batch: 070 ----
mean loss: 254.19
 ---- batch: 080 ----
mean loss: 260.51
 ---- batch: 090 ----
mean loss: 257.23
 ---- batch: 100 ----
mean loss: 253.71
 ---- batch: 110 ----
mean loss: 253.46
train mean loss: 255.05
epoch train time: 0:00:00.730715
elapsed time: 0:01:27.348741
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:34:34.764234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.20
 ---- batch: 020 ----
mean loss: 248.96
 ---- batch: 030 ----
mean loss: 249.73
 ---- batch: 040 ----
mean loss: 249.99
 ---- batch: 050 ----
mean loss: 246.82
 ---- batch: 060 ----
mean loss: 263.97
 ---- batch: 070 ----
mean loss: 257.61
 ---- batch: 080 ----
mean loss: 253.88
 ---- batch: 090 ----
mean loss: 249.84
 ---- batch: 100 ----
mean loss: 255.04
 ---- batch: 110 ----
mean loss: 248.61
train mean loss: 253.07
epoch train time: 0:00:00.733497
elapsed time: 0:01:28.082399
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:34:35.497881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.06
 ---- batch: 020 ----
mean loss: 252.97
 ---- batch: 030 ----
mean loss: 260.62
 ---- batch: 040 ----
mean loss: 256.73
 ---- batch: 050 ----
mean loss: 258.27
 ---- batch: 060 ----
mean loss: 244.37
 ---- batch: 070 ----
mean loss: 241.95
 ---- batch: 080 ----
mean loss: 247.96
 ---- batch: 090 ----
mean loss: 251.62
 ---- batch: 100 ----
mean loss: 244.15
 ---- batch: 110 ----
mean loss: 252.96
train mean loss: 250.88
epoch train time: 0:00:00.733847
elapsed time: 0:01:28.816404
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:34:36.231884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.61
 ---- batch: 020 ----
mean loss: 247.66
 ---- batch: 030 ----
mean loss: 253.69
 ---- batch: 040 ----
mean loss: 253.32
 ---- batch: 050 ----
mean loss: 249.92
 ---- batch: 060 ----
mean loss: 251.11
 ---- batch: 070 ----
mean loss: 249.71
 ---- batch: 080 ----
mean loss: 247.39
 ---- batch: 090 ----
mean loss: 248.42
 ---- batch: 100 ----
mean loss: 242.41
 ---- batch: 110 ----
mean loss: 248.75
train mean loss: 248.81
epoch train time: 0:00:00.739651
elapsed time: 0:01:29.556194
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:34:36.971672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.69
 ---- batch: 020 ----
mean loss: 252.88
 ---- batch: 030 ----
mean loss: 241.78
 ---- batch: 040 ----
mean loss: 248.35
 ---- batch: 050 ----
mean loss: 254.12
 ---- batch: 060 ----
mean loss: 250.82
 ---- batch: 070 ----
mean loss: 240.55
 ---- batch: 080 ----
mean loss: 242.91
 ---- batch: 090 ----
mean loss: 253.42
 ---- batch: 100 ----
mean loss: 245.20
 ---- batch: 110 ----
mean loss: 239.83
train mean loss: 247.09
epoch train time: 0:00:00.727365
elapsed time: 0:01:30.283701
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:34:37.699181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.99
 ---- batch: 020 ----
mean loss: 244.92
 ---- batch: 030 ----
mean loss: 238.49
 ---- batch: 040 ----
mean loss: 247.04
 ---- batch: 050 ----
mean loss: 247.33
 ---- batch: 060 ----
mean loss: 252.64
 ---- batch: 070 ----
mean loss: 249.87
 ---- batch: 080 ----
mean loss: 249.90
 ---- batch: 090 ----
mean loss: 242.04
 ---- batch: 100 ----
mean loss: 240.71
 ---- batch: 110 ----
mean loss: 242.93
train mean loss: 245.38
epoch train time: 0:00:00.728229
elapsed time: 0:01:31.012076
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:34:38.427574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.28
 ---- batch: 020 ----
mean loss: 243.26
 ---- batch: 030 ----
mean loss: 244.73
 ---- batch: 040 ----
mean loss: 249.60
 ---- batch: 050 ----
mean loss: 245.90
 ---- batch: 060 ----
mean loss: 242.10
 ---- batch: 070 ----
mean loss: 245.69
 ---- batch: 080 ----
mean loss: 240.75
 ---- batch: 090 ----
mean loss: 251.70
 ---- batch: 100 ----
mean loss: 228.03
 ---- batch: 110 ----
mean loss: 251.52
train mean loss: 243.73
epoch train time: 0:00:00.733525
elapsed time: 0:01:31.745763
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:34:39.161241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.05
 ---- batch: 020 ----
mean loss: 248.62
 ---- batch: 030 ----
mean loss: 236.48
 ---- batch: 040 ----
mean loss: 238.74
 ---- batch: 050 ----
mean loss: 245.82
 ---- batch: 060 ----
mean loss: 242.79
 ---- batch: 070 ----
mean loss: 251.27
 ---- batch: 080 ----
mean loss: 237.89
 ---- batch: 090 ----
mean loss: 242.85
 ---- batch: 100 ----
mean loss: 244.95
 ---- batch: 110 ----
mean loss: 235.75
train mean loss: 242.20
epoch train time: 0:00:00.739157
elapsed time: 0:01:32.485056
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:34:39.900535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.39
 ---- batch: 020 ----
mean loss: 244.02
 ---- batch: 030 ----
mean loss: 236.56
 ---- batch: 040 ----
mean loss: 236.60
 ---- batch: 050 ----
mean loss: 240.36
 ---- batch: 060 ----
mean loss: 239.12
 ---- batch: 070 ----
mean loss: 238.17
 ---- batch: 080 ----
mean loss: 249.44
 ---- batch: 090 ----
mean loss: 245.37
 ---- batch: 100 ----
mean loss: 232.06
 ---- batch: 110 ----
mean loss: 248.17
train mean loss: 240.50
epoch train time: 0:00:00.730273
elapsed time: 0:01:33.215470
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:34:40.630950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.16
 ---- batch: 020 ----
mean loss: 247.00
 ---- batch: 030 ----
mean loss: 244.67
 ---- batch: 040 ----
mean loss: 246.35
 ---- batch: 050 ----
mean loss: 235.07
 ---- batch: 060 ----
mean loss: 239.46
 ---- batch: 070 ----
mean loss: 238.86
 ---- batch: 080 ----
mean loss: 238.55
 ---- batch: 090 ----
mean loss: 233.11
 ---- batch: 100 ----
mean loss: 238.17
 ---- batch: 110 ----
mean loss: 238.30
train mean loss: 239.32
epoch train time: 0:00:00.739089
elapsed time: 0:01:33.954701
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:34:41.370197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.25
 ---- batch: 020 ----
mean loss: 243.33
 ---- batch: 030 ----
mean loss: 231.19
 ---- batch: 040 ----
mean loss: 234.46
 ---- batch: 050 ----
mean loss: 240.41
 ---- batch: 060 ----
mean loss: 236.28
 ---- batch: 070 ----
mean loss: 226.33
 ---- batch: 080 ----
mean loss: 237.53
 ---- batch: 090 ----
mean loss: 236.54
 ---- batch: 100 ----
mean loss: 242.85
 ---- batch: 110 ----
mean loss: 239.37
train mean loss: 237.43
epoch train time: 0:00:00.732218
elapsed time: 0:01:34.687073
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:34:42.102556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.41
 ---- batch: 020 ----
mean loss: 239.24
 ---- batch: 030 ----
mean loss: 236.86
 ---- batch: 040 ----
mean loss: 230.94
 ---- batch: 050 ----
mean loss: 229.44
 ---- batch: 060 ----
mean loss: 236.21
 ---- batch: 070 ----
mean loss: 244.55
 ---- batch: 080 ----
mean loss: 244.51
 ---- batch: 090 ----
mean loss: 236.11
 ---- batch: 100 ----
mean loss: 236.31
 ---- batch: 110 ----
mean loss: 232.33
train mean loss: 236.06
epoch train time: 0:00:00.739152
elapsed time: 0:01:35.426403
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:34:42.841887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.77
 ---- batch: 020 ----
mean loss: 245.38
 ---- batch: 030 ----
mean loss: 224.97
 ---- batch: 040 ----
mean loss: 239.54
 ---- batch: 050 ----
mean loss: 236.75
 ---- batch: 060 ----
mean loss: 233.30
 ---- batch: 070 ----
mean loss: 235.95
 ---- batch: 080 ----
mean loss: 239.56
 ---- batch: 090 ----
mean loss: 235.01
 ---- batch: 100 ----
mean loss: 231.71
 ---- batch: 110 ----
mean loss: 229.66
train mean loss: 234.35
epoch train time: 0:00:00.731900
elapsed time: 0:01:36.158446
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:34:43.573955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.67
 ---- batch: 020 ----
mean loss: 243.48
 ---- batch: 030 ----
mean loss: 226.39
 ---- batch: 040 ----
mean loss: 232.53
 ---- batch: 050 ----
mean loss: 236.23
 ---- batch: 060 ----
mean loss: 227.42
 ---- batch: 070 ----
mean loss: 226.73
 ---- batch: 080 ----
mean loss: 227.11
 ---- batch: 090 ----
mean loss: 236.26
 ---- batch: 100 ----
mean loss: 236.05
 ---- batch: 110 ----
mean loss: 234.20
train mean loss: 232.85
epoch train time: 0:00:00.734439
elapsed time: 0:01:36.893055
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:34:44.308532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.72
 ---- batch: 020 ----
mean loss: 234.53
 ---- batch: 030 ----
mean loss: 227.84
 ---- batch: 040 ----
mean loss: 226.57
 ---- batch: 050 ----
mean loss: 232.34
 ---- batch: 060 ----
mean loss: 227.62
 ---- batch: 070 ----
mean loss: 230.92
 ---- batch: 080 ----
mean loss: 239.35
 ---- batch: 090 ----
mean loss: 228.56
 ---- batch: 100 ----
mean loss: 225.32
 ---- batch: 110 ----
mean loss: 231.17
train mean loss: 231.36
epoch train time: 0:00:00.728867
elapsed time: 0:01:37.622059
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:34:45.037540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.50
 ---- batch: 020 ----
mean loss: 227.85
 ---- batch: 030 ----
mean loss: 234.55
 ---- batch: 040 ----
mean loss: 229.38
 ---- batch: 050 ----
mean loss: 237.64
 ---- batch: 060 ----
mean loss: 223.58
 ---- batch: 070 ----
mean loss: 228.86
 ---- batch: 080 ----
mean loss: 233.56
 ---- batch: 090 ----
mean loss: 232.09
 ---- batch: 100 ----
mean loss: 232.32
 ---- batch: 110 ----
mean loss: 228.69
train mean loss: 229.76
epoch train time: 0:00:00.729347
elapsed time: 0:01:38.351554
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:34:45.767036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.36
 ---- batch: 020 ----
mean loss: 230.85
 ---- batch: 030 ----
mean loss: 232.45
 ---- batch: 040 ----
mean loss: 234.59
 ---- batch: 050 ----
mean loss: 223.16
 ---- batch: 060 ----
mean loss: 225.94
 ---- batch: 070 ----
mean loss: 229.52
 ---- batch: 080 ----
mean loss: 229.54
 ---- batch: 090 ----
mean loss: 227.66
 ---- batch: 100 ----
mean loss: 225.05
 ---- batch: 110 ----
mean loss: 223.19
train mean loss: 228.75
epoch train time: 0:00:00.728631
elapsed time: 0:01:39.080357
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:34:46.495866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.52
 ---- batch: 020 ----
mean loss: 224.82
 ---- batch: 030 ----
mean loss: 219.74
 ---- batch: 040 ----
mean loss: 226.41
 ---- batch: 050 ----
mean loss: 224.72
 ---- batch: 060 ----
mean loss: 234.56
 ---- batch: 070 ----
mean loss: 234.21
 ---- batch: 080 ----
mean loss: 231.65
 ---- batch: 090 ----
mean loss: 225.87
 ---- batch: 100 ----
mean loss: 227.74
 ---- batch: 110 ----
mean loss: 227.58
train mean loss: 227.53
epoch train time: 0:00:00.739636
elapsed time: 0:01:39.820166
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:34:47.235643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.96
 ---- batch: 020 ----
mean loss: 233.18
 ---- batch: 030 ----
mean loss: 235.05
 ---- batch: 040 ----
mean loss: 225.10
 ---- batch: 050 ----
mean loss: 228.08
 ---- batch: 060 ----
mean loss: 229.53
 ---- batch: 070 ----
mean loss: 225.02
 ---- batch: 080 ----
mean loss: 221.92
 ---- batch: 090 ----
mean loss: 225.90
 ---- batch: 100 ----
mean loss: 215.64
 ---- batch: 110 ----
mean loss: 228.87
train mean loss: 226.66
epoch train time: 0:00:00.726846
elapsed time: 0:01:40.547150
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:34:47.962654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.27
 ---- batch: 020 ----
mean loss: 231.39
 ---- batch: 030 ----
mean loss: 221.05
 ---- batch: 040 ----
mean loss: 231.99
 ---- batch: 050 ----
mean loss: 215.53
 ---- batch: 060 ----
mean loss: 224.35
 ---- batch: 070 ----
mean loss: 230.32
 ---- batch: 080 ----
mean loss: 221.78
 ---- batch: 090 ----
mean loss: 221.40
 ---- batch: 100 ----
mean loss: 221.59
 ---- batch: 110 ----
mean loss: 231.69
train mean loss: 225.76
epoch train time: 0:00:00.728700
elapsed time: 0:01:41.276028
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:34:48.691515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.63
 ---- batch: 020 ----
mean loss: 228.76
 ---- batch: 030 ----
mean loss: 226.44
 ---- batch: 040 ----
mean loss: 231.46
 ---- batch: 050 ----
mean loss: 221.44
 ---- batch: 060 ----
mean loss: 215.28
 ---- batch: 070 ----
mean loss: 227.52
 ---- batch: 080 ----
mean loss: 217.37
 ---- batch: 090 ----
mean loss: 231.02
 ---- batch: 100 ----
mean loss: 224.72
 ---- batch: 110 ----
mean loss: 225.94
train mean loss: 224.94
epoch train time: 0:00:00.732060
elapsed time: 0:01:42.008233
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:34:49.423709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.30
 ---- batch: 020 ----
mean loss: 232.86
 ---- batch: 030 ----
mean loss: 219.22
 ---- batch: 040 ----
mean loss: 223.03
 ---- batch: 050 ----
mean loss: 220.64
 ---- batch: 060 ----
mean loss: 221.26
 ---- batch: 070 ----
mean loss: 228.38
 ---- batch: 080 ----
mean loss: 231.18
 ---- batch: 090 ----
mean loss: 223.67
 ---- batch: 100 ----
mean loss: 224.41
 ---- batch: 110 ----
mean loss: 221.12
train mean loss: 224.28
epoch train time: 0:00:00.733946
elapsed time: 0:01:42.742316
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:34:50.157793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.19
 ---- batch: 020 ----
mean loss: 223.78
 ---- batch: 030 ----
mean loss: 229.77
 ---- batch: 040 ----
mean loss: 223.76
 ---- batch: 050 ----
mean loss: 226.43
 ---- batch: 060 ----
mean loss: 225.66
 ---- batch: 070 ----
mean loss: 226.21
 ---- batch: 080 ----
mean loss: 224.90
 ---- batch: 090 ----
mean loss: 227.31
 ---- batch: 100 ----
mean loss: 218.76
 ---- batch: 110 ----
mean loss: 215.82
train mean loss: 223.57
epoch train time: 0:00:00.731516
elapsed time: 0:01:43.473965
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:34:50.889457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.23
 ---- batch: 020 ----
mean loss: 225.80
 ---- batch: 030 ----
mean loss: 226.76
 ---- batch: 040 ----
mean loss: 226.13
 ---- batch: 050 ----
mean loss: 228.91
 ---- batch: 060 ----
mean loss: 218.91
 ---- batch: 070 ----
mean loss: 215.57
 ---- batch: 080 ----
mean loss: 217.77
 ---- batch: 090 ----
mean loss: 223.26
 ---- batch: 100 ----
mean loss: 220.82
 ---- batch: 110 ----
mean loss: 221.23
train mean loss: 223.03
epoch train time: 0:00:00.729953
elapsed time: 0:01:44.204067
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:34:51.619544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.45
 ---- batch: 020 ----
mean loss: 215.64
 ---- batch: 030 ----
mean loss: 203.81
 ---- batch: 040 ----
mean loss: 225.34
 ---- batch: 050 ----
mean loss: 237.63
 ---- batch: 060 ----
mean loss: 227.56
 ---- batch: 070 ----
mean loss: 222.98
 ---- batch: 080 ----
mean loss: 220.40
 ---- batch: 090 ----
mean loss: 218.45
 ---- batch: 100 ----
mean loss: 224.08
 ---- batch: 110 ----
mean loss: 228.76
train mean loss: 222.24
epoch train time: 0:00:00.736454
elapsed time: 0:01:44.940663
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:34:52.356142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.86
 ---- batch: 020 ----
mean loss: 211.69
 ---- batch: 030 ----
mean loss: 221.28
 ---- batch: 040 ----
mean loss: 221.41
 ---- batch: 050 ----
mean loss: 231.99
 ---- batch: 060 ----
mean loss: 218.25
 ---- batch: 070 ----
mean loss: 220.10
 ---- batch: 080 ----
mean loss: 232.06
 ---- batch: 090 ----
mean loss: 223.39
 ---- batch: 100 ----
mean loss: 226.11
 ---- batch: 110 ----
mean loss: 220.20
train mean loss: 221.60
epoch train time: 0:00:00.732977
elapsed time: 0:01:45.673777
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:34:53.089277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.76
 ---- batch: 020 ----
mean loss: 221.40
 ---- batch: 030 ----
mean loss: 221.65
 ---- batch: 040 ----
mean loss: 213.32
 ---- batch: 050 ----
mean loss: 219.47
 ---- batch: 060 ----
mean loss: 226.86
 ---- batch: 070 ----
mean loss: 221.83
 ---- batch: 080 ----
mean loss: 233.24
 ---- batch: 090 ----
mean loss: 221.63
 ---- batch: 100 ----
mean loss: 220.04
 ---- batch: 110 ----
mean loss: 213.89
train mean loss: 220.91
epoch train time: 0:00:00.733939
elapsed time: 0:01:46.407906
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:34:53.823450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.31
 ---- batch: 020 ----
mean loss: 227.53
 ---- batch: 030 ----
mean loss: 211.28
 ---- batch: 040 ----
mean loss: 226.56
 ---- batch: 050 ----
mean loss: 212.39
 ---- batch: 060 ----
mean loss: 223.67
 ---- batch: 070 ----
mean loss: 212.83
 ---- batch: 080 ----
mean loss: 211.29
 ---- batch: 090 ----
mean loss: 219.23
 ---- batch: 100 ----
mean loss: 221.79
 ---- batch: 110 ----
mean loss: 231.29
train mean loss: 220.32
epoch train time: 0:00:00.742390
elapsed time: 0:01:47.150511
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:34:54.565991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.51
 ---- batch: 020 ----
mean loss: 220.40
 ---- batch: 030 ----
mean loss: 228.66
 ---- batch: 040 ----
mean loss: 219.01
 ---- batch: 050 ----
mean loss: 223.26
 ---- batch: 060 ----
mean loss: 220.40
 ---- batch: 070 ----
mean loss: 219.36
 ---- batch: 080 ----
mean loss: 217.90
 ---- batch: 090 ----
mean loss: 208.92
 ---- batch: 100 ----
mean loss: 226.17
 ---- batch: 110 ----
mean loss: 217.36
train mean loss: 219.73
epoch train time: 0:00:00.727649
elapsed time: 0:01:47.878299
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:34:55.293778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.75
 ---- batch: 020 ----
mean loss: 221.66
 ---- batch: 030 ----
mean loss: 218.68
 ---- batch: 040 ----
mean loss: 216.51
 ---- batch: 050 ----
mean loss: 224.97
 ---- batch: 060 ----
mean loss: 212.97
 ---- batch: 070 ----
mean loss: 218.82
 ---- batch: 080 ----
mean loss: 223.77
 ---- batch: 090 ----
mean loss: 219.67
 ---- batch: 100 ----
mean loss: 211.39
 ---- batch: 110 ----
mean loss: 219.29
train mean loss: 219.18
epoch train time: 0:00:00.742674
elapsed time: 0:01:48.621113
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:34:56.036595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.66
 ---- batch: 020 ----
mean loss: 220.02
 ---- batch: 030 ----
mean loss: 223.24
 ---- batch: 040 ----
mean loss: 222.18
 ---- batch: 050 ----
mean loss: 216.64
 ---- batch: 060 ----
mean loss: 221.12
 ---- batch: 070 ----
mean loss: 215.25
 ---- batch: 080 ----
mean loss: 223.11
 ---- batch: 090 ----
mean loss: 211.38
 ---- batch: 100 ----
mean loss: 216.20
 ---- batch: 110 ----
mean loss: 214.18
train mean loss: 218.67
epoch train time: 0:00:00.733788
elapsed time: 0:01:49.355043
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:34:56.770518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.07
 ---- batch: 020 ----
mean loss: 216.21
 ---- batch: 030 ----
mean loss: 221.07
 ---- batch: 040 ----
mean loss: 217.52
 ---- batch: 050 ----
mean loss: 210.47
 ---- batch: 060 ----
mean loss: 219.82
 ---- batch: 070 ----
mean loss: 214.95
 ---- batch: 080 ----
mean loss: 212.09
 ---- batch: 090 ----
mean loss: 214.05
 ---- batch: 100 ----
mean loss: 222.94
 ---- batch: 110 ----
mean loss: 223.76
train mean loss: 218.26
epoch train time: 0:00:00.729817
elapsed time: 0:01:50.084994
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:34:57.500488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.29
 ---- batch: 020 ----
mean loss: 219.23
 ---- batch: 030 ----
mean loss: 216.61
 ---- batch: 040 ----
mean loss: 216.10
 ---- batch: 050 ----
mean loss: 219.13
 ---- batch: 060 ----
mean loss: 212.90
 ---- batch: 070 ----
mean loss: 222.96
 ---- batch: 080 ----
mean loss: 214.63
 ---- batch: 090 ----
mean loss: 216.76
 ---- batch: 100 ----
mean loss: 220.78
 ---- batch: 110 ----
mean loss: 224.72
train mean loss: 217.71
epoch train time: 0:00:00.740472
elapsed time: 0:01:50.825657
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:34:58.241144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.52
 ---- batch: 020 ----
mean loss: 216.10
 ---- batch: 030 ----
mean loss: 214.86
 ---- batch: 040 ----
mean loss: 211.48
 ---- batch: 050 ----
mean loss: 210.64
 ---- batch: 060 ----
mean loss: 222.35
 ---- batch: 070 ----
mean loss: 218.26
 ---- batch: 080 ----
mean loss: 212.62
 ---- batch: 090 ----
mean loss: 217.33
 ---- batch: 100 ----
mean loss: 227.28
 ---- batch: 110 ----
mean loss: 213.95
train mean loss: 217.15
epoch train time: 0:00:00.729240
elapsed time: 0:01:51.555070
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:34:58.970540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.82
 ---- batch: 020 ----
mean loss: 227.46
 ---- batch: 030 ----
mean loss: 209.69
 ---- batch: 040 ----
mean loss: 215.87
 ---- batch: 050 ----
mean loss: 220.47
 ---- batch: 060 ----
mean loss: 214.31
 ---- batch: 070 ----
mean loss: 211.87
 ---- batch: 080 ----
mean loss: 220.38
 ---- batch: 090 ----
mean loss: 224.84
 ---- batch: 100 ----
mean loss: 207.40
 ---- batch: 110 ----
mean loss: 218.72
train mean loss: 216.91
epoch train time: 0:00:00.724976
elapsed time: 0:01:52.280193
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:34:59.695671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.33
 ---- batch: 020 ----
mean loss: 218.23
 ---- batch: 030 ----
mean loss: 222.36
 ---- batch: 040 ----
mean loss: 213.42
 ---- batch: 050 ----
mean loss: 222.50
 ---- batch: 060 ----
mean loss: 208.81
 ---- batch: 070 ----
mean loss: 225.11
 ---- batch: 080 ----
mean loss: 214.85
 ---- batch: 090 ----
mean loss: 221.82
 ---- batch: 100 ----
mean loss: 204.53
 ---- batch: 110 ----
mean loss: 208.92
train mean loss: 216.26
epoch train time: 0:00:00.727308
elapsed time: 0:01:53.007670
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:35:00.423148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.83
 ---- batch: 020 ----
mean loss: 223.21
 ---- batch: 030 ----
mean loss: 204.40
 ---- batch: 040 ----
mean loss: 212.39
 ---- batch: 050 ----
mean loss: 216.68
 ---- batch: 060 ----
mean loss: 213.76
 ---- batch: 070 ----
mean loss: 216.78
 ---- batch: 080 ----
mean loss: 213.64
 ---- batch: 090 ----
mean loss: 219.18
 ---- batch: 100 ----
mean loss: 221.59
 ---- batch: 110 ----
mean loss: 218.18
train mean loss: 215.82
epoch train time: 0:00:00.729159
elapsed time: 0:01:53.736967
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:35:01.152454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.41
 ---- batch: 020 ----
mean loss: 210.54
 ---- batch: 030 ----
mean loss: 221.41
 ---- batch: 040 ----
mean loss: 210.21
 ---- batch: 050 ----
mean loss: 217.44
 ---- batch: 060 ----
mean loss: 209.67
 ---- batch: 070 ----
mean loss: 222.12
 ---- batch: 080 ----
mean loss: 215.72
 ---- batch: 090 ----
mean loss: 208.47
 ---- batch: 100 ----
mean loss: 217.80
 ---- batch: 110 ----
mean loss: 220.74
train mean loss: 215.48
epoch train time: 0:00:00.740395
elapsed time: 0:01:54.477510
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:35:01.892997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.69
 ---- batch: 020 ----
mean loss: 216.54
 ---- batch: 030 ----
mean loss: 215.51
 ---- batch: 040 ----
mean loss: 204.09
 ---- batch: 050 ----
mean loss: 220.53
 ---- batch: 060 ----
mean loss: 213.14
 ---- batch: 070 ----
mean loss: 216.71
 ---- batch: 080 ----
mean loss: 223.79
 ---- batch: 090 ----
mean loss: 212.58
 ---- batch: 100 ----
mean loss: 208.35
 ---- batch: 110 ----
mean loss: 220.83
train mean loss: 215.22
epoch train time: 0:00:00.731195
elapsed time: 0:01:55.208851
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:35:02.624347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.61
 ---- batch: 020 ----
mean loss: 210.47
 ---- batch: 030 ----
mean loss: 218.56
 ---- batch: 040 ----
mean loss: 213.21
 ---- batch: 050 ----
mean loss: 207.83
 ---- batch: 060 ----
mean loss: 221.34
 ---- batch: 070 ----
mean loss: 214.91
 ---- batch: 080 ----
mean loss: 220.06
 ---- batch: 090 ----
mean loss: 221.99
 ---- batch: 100 ----
mean loss: 204.16
 ---- batch: 110 ----
mean loss: 213.16
train mean loss: 214.76
epoch train time: 0:00:00.741412
elapsed time: 0:01:55.950424
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:35:03.365918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.55
 ---- batch: 020 ----
mean loss: 207.03
 ---- batch: 030 ----
mean loss: 217.38
 ---- batch: 040 ----
mean loss: 202.71
 ---- batch: 050 ----
mean loss: 213.54
 ---- batch: 060 ----
mean loss: 218.18
 ---- batch: 070 ----
mean loss: 213.95
 ---- batch: 080 ----
mean loss: 219.43
 ---- batch: 090 ----
mean loss: 215.83
 ---- batch: 100 ----
mean loss: 215.55
 ---- batch: 110 ----
mean loss: 220.13
train mean loss: 214.35
epoch train time: 0:00:00.736886
elapsed time: 0:01:56.687464
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:35:04.102942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.66
 ---- batch: 020 ----
mean loss: 225.03
 ---- batch: 030 ----
mean loss: 207.25
 ---- batch: 040 ----
mean loss: 210.90
 ---- batch: 050 ----
mean loss: 222.75
 ---- batch: 060 ----
mean loss: 229.25
 ---- batch: 070 ----
mean loss: 214.43
 ---- batch: 080 ----
mean loss: 204.47
 ---- batch: 090 ----
mean loss: 209.23
 ---- batch: 100 ----
mean loss: 214.40
 ---- batch: 110 ----
mean loss: 212.00
train mean loss: 214.41
epoch train time: 0:00:00.732855
elapsed time: 0:01:57.420462
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:35:04.835939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.34
 ---- batch: 020 ----
mean loss: 226.34
 ---- batch: 030 ----
mean loss: 216.11
 ---- batch: 040 ----
mean loss: 207.77
 ---- batch: 050 ----
mean loss: 214.60
 ---- batch: 060 ----
mean loss: 209.23
 ---- batch: 070 ----
mean loss: 210.03
 ---- batch: 080 ----
mean loss: 218.01
 ---- batch: 090 ----
mean loss: 216.24
 ---- batch: 100 ----
mean loss: 202.96
 ---- batch: 110 ----
mean loss: 208.55
train mean loss: 213.80
epoch train time: 0:00:00.731169
elapsed time: 0:01:58.151789
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:35:05.567280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.55
 ---- batch: 020 ----
mean loss: 216.76
 ---- batch: 030 ----
mean loss: 210.70
 ---- batch: 040 ----
mean loss: 209.71
 ---- batch: 050 ----
mean loss: 211.49
 ---- batch: 060 ----
mean loss: 212.77
 ---- batch: 070 ----
mean loss: 221.45
 ---- batch: 080 ----
mean loss: 214.50
 ---- batch: 090 ----
mean loss: 213.84
 ---- batch: 100 ----
mean loss: 220.08
 ---- batch: 110 ----
mean loss: 207.32
train mean loss: 213.54
epoch train time: 0:00:00.727707
elapsed time: 0:01:58.879646
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:35:06.295121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.11
 ---- batch: 020 ----
mean loss: 217.71
 ---- batch: 030 ----
mean loss: 213.93
 ---- batch: 040 ----
mean loss: 210.96
 ---- batch: 050 ----
mean loss: 210.09
 ---- batch: 060 ----
mean loss: 213.22
 ---- batch: 070 ----
mean loss: 208.97
 ---- batch: 080 ----
mean loss: 221.42
 ---- batch: 090 ----
mean loss: 211.94
 ---- batch: 100 ----
mean loss: 212.09
 ---- batch: 110 ----
mean loss: 204.92
train mean loss: 213.01
epoch train time: 0:00:00.727876
elapsed time: 0:01:59.607655
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:35:07.023129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.58
 ---- batch: 020 ----
mean loss: 212.17
 ---- batch: 030 ----
mean loss: 215.52
 ---- batch: 040 ----
mean loss: 210.53
 ---- batch: 050 ----
mean loss: 213.22
 ---- batch: 060 ----
mean loss: 208.33
 ---- batch: 070 ----
mean loss: 211.00
 ---- batch: 080 ----
mean loss: 218.58
 ---- batch: 090 ----
mean loss: 218.96
 ---- batch: 100 ----
mean loss: 212.81
 ---- batch: 110 ----
mean loss: 210.61
train mean loss: 212.63
epoch train time: 0:00:00.722692
elapsed time: 0:02:00.330477
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:35:07.745968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.86
 ---- batch: 020 ----
mean loss: 204.59
 ---- batch: 030 ----
mean loss: 211.29
 ---- batch: 040 ----
mean loss: 212.45
 ---- batch: 050 ----
mean loss: 215.27
 ---- batch: 060 ----
mean loss: 221.95
 ---- batch: 070 ----
mean loss: 212.91
 ---- batch: 080 ----
mean loss: 213.08
 ---- batch: 090 ----
mean loss: 217.72
 ---- batch: 100 ----
mean loss: 209.28
 ---- batch: 110 ----
mean loss: 210.76
train mean loss: 212.53
epoch train time: 0:00:00.726966
elapsed time: 0:02:01.057591
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:35:08.473065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.45
 ---- batch: 020 ----
mean loss: 199.99
 ---- batch: 030 ----
mean loss: 215.92
 ---- batch: 040 ----
mean loss: 215.35
 ---- batch: 050 ----
mean loss: 210.57
 ---- batch: 060 ----
mean loss: 210.73
 ---- batch: 070 ----
mean loss: 213.24
 ---- batch: 080 ----
mean loss: 200.90
 ---- batch: 090 ----
mean loss: 218.71
 ---- batch: 100 ----
mean loss: 211.59
 ---- batch: 110 ----
mean loss: 214.85
train mean loss: 212.17
epoch train time: 0:00:00.725390
elapsed time: 0:02:01.783116
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:35:09.198593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.96
 ---- batch: 020 ----
mean loss: 207.88
 ---- batch: 030 ----
mean loss: 210.59
 ---- batch: 040 ----
mean loss: 213.97
 ---- batch: 050 ----
mean loss: 207.56
 ---- batch: 060 ----
mean loss: 212.12
 ---- batch: 070 ----
mean loss: 214.38
 ---- batch: 080 ----
mean loss: 210.91
 ---- batch: 090 ----
mean loss: 212.32
 ---- batch: 100 ----
mean loss: 216.77
 ---- batch: 110 ----
mean loss: 210.27
train mean loss: 211.64
epoch train time: 0:00:00.722711
elapsed time: 0:02:02.505979
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:35:09.921475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.51
 ---- batch: 020 ----
mean loss: 218.30
 ---- batch: 030 ----
mean loss: 207.56
 ---- batch: 040 ----
mean loss: 212.34
 ---- batch: 050 ----
mean loss: 219.62
 ---- batch: 060 ----
mean loss: 209.45
 ---- batch: 070 ----
mean loss: 216.80
 ---- batch: 080 ----
mean loss: 217.22
 ---- batch: 090 ----
mean loss: 196.95
 ---- batch: 100 ----
mean loss: 217.10
 ---- batch: 110 ----
mean loss: 200.44
train mean loss: 211.58
epoch train time: 0:00:00.734160
elapsed time: 0:02:03.240317
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:35:10.655798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.75
 ---- batch: 020 ----
mean loss: 202.93
 ---- batch: 030 ----
mean loss: 208.34
 ---- batch: 040 ----
mean loss: 215.19
 ---- batch: 050 ----
mean loss: 210.69
 ---- batch: 060 ----
mean loss: 213.11
 ---- batch: 070 ----
mean loss: 213.32
 ---- batch: 080 ----
mean loss: 211.01
 ---- batch: 090 ----
mean loss: 220.25
 ---- batch: 100 ----
mean loss: 209.06
 ---- batch: 110 ----
mean loss: 205.07
train mean loss: 211.03
epoch train time: 0:00:00.744567
elapsed time: 0:02:03.985026
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:35:11.400504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.91
 ---- batch: 020 ----
mean loss: 206.94
 ---- batch: 030 ----
mean loss: 212.46
 ---- batch: 040 ----
mean loss: 216.72
 ---- batch: 050 ----
mean loss: 219.31
 ---- batch: 060 ----
mean loss: 213.29
 ---- batch: 070 ----
mean loss: 214.65
 ---- batch: 080 ----
mean loss: 206.06
 ---- batch: 090 ----
mean loss: 209.53
 ---- batch: 100 ----
mean loss: 201.49
 ---- batch: 110 ----
mean loss: 205.97
train mean loss: 210.84
epoch train time: 0:00:00.733044
elapsed time: 0:02:04.718256
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:35:12.133754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.88
 ---- batch: 020 ----
mean loss: 204.88
 ---- batch: 030 ----
mean loss: 216.35
 ---- batch: 040 ----
mean loss: 220.28
 ---- batch: 050 ----
mean loss: 204.50
 ---- batch: 060 ----
mean loss: 210.07
 ---- batch: 070 ----
mean loss: 215.91
 ---- batch: 080 ----
mean loss: 216.10
 ---- batch: 090 ----
mean loss: 202.98
 ---- batch: 100 ----
mean loss: 203.03
 ---- batch: 110 ----
mean loss: 212.05
train mean loss: 210.54
epoch train time: 0:00:00.727300
elapsed time: 0:02:05.445729
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:35:12.861208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.57
 ---- batch: 020 ----
mean loss: 208.30
 ---- batch: 030 ----
mean loss: 206.95
 ---- batch: 040 ----
mean loss: 214.60
 ---- batch: 050 ----
mean loss: 211.12
 ---- batch: 060 ----
mean loss: 207.33
 ---- batch: 070 ----
mean loss: 213.69
 ---- batch: 080 ----
mean loss: 212.53
 ---- batch: 090 ----
mean loss: 208.00
 ---- batch: 100 ----
mean loss: 210.13
 ---- batch: 110 ----
mean loss: 208.36
train mean loss: 210.43
epoch train time: 0:00:00.728506
elapsed time: 0:02:06.174386
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:35:13.589853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.90
 ---- batch: 020 ----
mean loss: 206.86
 ---- batch: 030 ----
mean loss: 211.27
 ---- batch: 040 ----
mean loss: 194.98
 ---- batch: 050 ----
mean loss: 222.39
 ---- batch: 060 ----
mean loss: 210.24
 ---- batch: 070 ----
mean loss: 206.07
 ---- batch: 080 ----
mean loss: 209.35
 ---- batch: 090 ----
mean loss: 214.30
 ---- batch: 100 ----
mean loss: 210.63
 ---- batch: 110 ----
mean loss: 210.73
train mean loss: 210.03
epoch train time: 0:00:00.725853
elapsed time: 0:02:06.900363
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:35:14.315839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.76
 ---- batch: 020 ----
mean loss: 203.21
 ---- batch: 030 ----
mean loss: 209.79
 ---- batch: 040 ----
mean loss: 208.12
 ---- batch: 050 ----
mean loss: 217.16
 ---- batch: 060 ----
mean loss: 211.58
 ---- batch: 070 ----
mean loss: 212.95
 ---- batch: 080 ----
mean loss: 210.24
 ---- batch: 090 ----
mean loss: 201.26
 ---- batch: 100 ----
mean loss: 216.87
 ---- batch: 110 ----
mean loss: 202.17
train mean loss: 209.88
epoch train time: 0:00:00.728401
elapsed time: 0:02:07.628916
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:35:15.044411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.16
 ---- batch: 020 ----
mean loss: 215.07
 ---- batch: 030 ----
mean loss: 210.04
 ---- batch: 040 ----
mean loss: 210.91
 ---- batch: 050 ----
mean loss: 212.99
 ---- batch: 060 ----
mean loss: 210.12
 ---- batch: 070 ----
mean loss: 199.35
 ---- batch: 080 ----
mean loss: 211.51
 ---- batch: 090 ----
mean loss: 208.39
 ---- batch: 100 ----
mean loss: 206.47
 ---- batch: 110 ----
mean loss: 214.01
train mean loss: 209.57
epoch train time: 0:00:00.734080
elapsed time: 0:02:08.363165
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:35:15.778648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.16
 ---- batch: 020 ----
mean loss: 206.03
 ---- batch: 030 ----
mean loss: 209.82
 ---- batch: 040 ----
mean loss: 218.22
 ---- batch: 050 ----
mean loss: 202.68
 ---- batch: 060 ----
mean loss: 206.09
 ---- batch: 070 ----
mean loss: 217.27
 ---- batch: 080 ----
mean loss: 212.61
 ---- batch: 090 ----
mean loss: 209.62
 ---- batch: 100 ----
mean loss: 207.13
 ---- batch: 110 ----
mean loss: 205.65
train mean loss: 209.15
epoch train time: 0:00:00.736942
elapsed time: 0:02:09.100250
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:35:16.515726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.91
 ---- batch: 020 ----
mean loss: 206.58
 ---- batch: 030 ----
mean loss: 215.44
 ---- batch: 040 ----
mean loss: 209.30
 ---- batch: 050 ----
mean loss: 210.83
 ---- batch: 060 ----
mean loss: 206.27
 ---- batch: 070 ----
mean loss: 211.53
 ---- batch: 080 ----
mean loss: 206.99
 ---- batch: 090 ----
mean loss: 203.61
 ---- batch: 100 ----
mean loss: 213.59
 ---- batch: 110 ----
mean loss: 208.65
train mean loss: 209.31
epoch train time: 0:00:00.729670
elapsed time: 0:02:09.830075
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:35:17.245572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.94
 ---- batch: 020 ----
mean loss: 212.91
 ---- batch: 030 ----
mean loss: 214.06
 ---- batch: 040 ----
mean loss: 210.61
 ---- batch: 050 ----
mean loss: 207.66
 ---- batch: 060 ----
mean loss: 209.08
 ---- batch: 070 ----
mean loss: 215.14
 ---- batch: 080 ----
mean loss: 203.55
 ---- batch: 090 ----
mean loss: 198.79
 ---- batch: 100 ----
mean loss: 207.74
 ---- batch: 110 ----
mean loss: 204.99
train mean loss: 208.64
epoch train time: 0:00:00.725491
elapsed time: 0:02:10.555725
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:35:17.971201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.83
 ---- batch: 020 ----
mean loss: 206.00
 ---- batch: 030 ----
mean loss: 209.32
 ---- batch: 040 ----
mean loss: 201.71
 ---- batch: 050 ----
mean loss: 206.70
 ---- batch: 060 ----
mean loss: 202.63
 ---- batch: 070 ----
mean loss: 209.24
 ---- batch: 080 ----
mean loss: 206.81
 ---- batch: 090 ----
mean loss: 217.30
 ---- batch: 100 ----
mean loss: 197.95
 ---- batch: 110 ----
mean loss: 213.70
train mean loss: 208.46
epoch train time: 0:00:00.729742
elapsed time: 0:02:11.285599
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:35:18.701076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.13
 ---- batch: 020 ----
mean loss: 209.92
 ---- batch: 030 ----
mean loss: 217.34
 ---- batch: 040 ----
mean loss: 209.52
 ---- batch: 050 ----
mean loss: 210.25
 ---- batch: 060 ----
mean loss: 213.36
 ---- batch: 070 ----
mean loss: 205.94
 ---- batch: 080 ----
mean loss: 203.74
 ---- batch: 090 ----
mean loss: 202.90
 ---- batch: 100 ----
mean loss: 208.85
 ---- batch: 110 ----
mean loss: 203.57
train mean loss: 208.40
epoch train time: 0:00:00.734226
elapsed time: 0:02:12.019964
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:35:19.435456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.16
 ---- batch: 020 ----
mean loss: 212.82
 ---- batch: 030 ----
mean loss: 213.21
 ---- batch: 040 ----
mean loss: 208.42
 ---- batch: 050 ----
mean loss: 204.96
 ---- batch: 060 ----
mean loss: 208.79
 ---- batch: 070 ----
mean loss: 205.40
 ---- batch: 080 ----
mean loss: 206.56
 ---- batch: 090 ----
mean loss: 213.93
 ---- batch: 100 ----
mean loss: 205.27
 ---- batch: 110 ----
mean loss: 205.65
train mean loss: 208.15
epoch train time: 0:00:00.728308
elapsed time: 0:02:12.748430
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:35:20.163909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.82
 ---- batch: 020 ----
mean loss: 212.63
 ---- batch: 030 ----
mean loss: 210.30
 ---- batch: 040 ----
mean loss: 207.77
 ---- batch: 050 ----
mean loss: 204.35
 ---- batch: 060 ----
mean loss: 201.02
 ---- batch: 070 ----
mean loss: 210.16
 ---- batch: 080 ----
mean loss: 205.42
 ---- batch: 090 ----
mean loss: 212.06
 ---- batch: 100 ----
mean loss: 199.63
 ---- batch: 110 ----
mean loss: 199.85
train mean loss: 207.86
epoch train time: 0:00:00.731851
elapsed time: 0:02:13.480421
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:35:20.895902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.50
 ---- batch: 020 ----
mean loss: 210.33
 ---- batch: 030 ----
mean loss: 205.38
 ---- batch: 040 ----
mean loss: 209.20
 ---- batch: 050 ----
mean loss: 210.69
 ---- batch: 060 ----
mean loss: 213.33
 ---- batch: 070 ----
mean loss: 202.30
 ---- batch: 080 ----
mean loss: 209.86
 ---- batch: 090 ----
mean loss: 198.59
 ---- batch: 100 ----
mean loss: 217.14
 ---- batch: 110 ----
mean loss: 211.23
train mean loss: 207.48
epoch train time: 0:00:00.729028
elapsed time: 0:02:14.209592
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:35:21.625093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.45
 ---- batch: 020 ----
mean loss: 212.90
 ---- batch: 030 ----
mean loss: 203.21
 ---- batch: 040 ----
mean loss: 213.00
 ---- batch: 050 ----
mean loss: 198.20
 ---- batch: 060 ----
mean loss: 212.60
 ---- batch: 070 ----
mean loss: 196.58
 ---- batch: 080 ----
mean loss: 208.00
 ---- batch: 090 ----
mean loss: 200.67
 ---- batch: 100 ----
mean loss: 213.11
 ---- batch: 110 ----
mean loss: 204.06
train mean loss: 207.22
epoch train time: 0:00:00.724890
elapsed time: 0:02:14.934644
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:35:22.350121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.47
 ---- batch: 020 ----
mean loss: 217.03
 ---- batch: 030 ----
mean loss: 207.94
 ---- batch: 040 ----
mean loss: 212.51
 ---- batch: 050 ----
mean loss: 209.65
 ---- batch: 060 ----
mean loss: 211.00
 ---- batch: 070 ----
mean loss: 199.05
 ---- batch: 080 ----
mean loss: 202.96
 ---- batch: 090 ----
mean loss: 206.42
 ---- batch: 100 ----
mean loss: 196.24
 ---- batch: 110 ----
mean loss: 211.84
train mean loss: 207.10
epoch train time: 0:00:00.728073
elapsed time: 0:02:15.662853
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:35:23.078329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.83
 ---- batch: 020 ----
mean loss: 207.27
 ---- batch: 030 ----
mean loss: 217.39
 ---- batch: 040 ----
mean loss: 212.58
 ---- batch: 050 ----
mean loss: 214.20
 ---- batch: 060 ----
mean loss: 201.36
 ---- batch: 070 ----
mean loss: 208.69
 ---- batch: 080 ----
mean loss: 205.95
 ---- batch: 090 ----
mean loss: 215.99
 ---- batch: 100 ----
mean loss: 204.06
 ---- batch: 110 ----
mean loss: 197.25
train mean loss: 207.10
epoch train time: 0:00:00.724836
elapsed time: 0:02:16.387840
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:35:23.803320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.44
 ---- batch: 020 ----
mean loss: 211.32
 ---- batch: 030 ----
mean loss: 206.78
 ---- batch: 040 ----
mean loss: 203.44
 ---- batch: 050 ----
mean loss: 200.38
 ---- batch: 060 ----
mean loss: 209.13
 ---- batch: 070 ----
mean loss: 206.45
 ---- batch: 080 ----
mean loss: 210.78
 ---- batch: 090 ----
mean loss: 207.01
 ---- batch: 100 ----
mean loss: 203.47
 ---- batch: 110 ----
mean loss: 200.17
train mean loss: 206.57
epoch train time: 0:00:00.737056
elapsed time: 0:02:17.125035
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:35:24.540513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.34
 ---- batch: 020 ----
mean loss: 217.84
 ---- batch: 030 ----
mean loss: 203.48
 ---- batch: 040 ----
mean loss: 207.35
 ---- batch: 050 ----
mean loss: 201.37
 ---- batch: 060 ----
mean loss: 203.67
 ---- batch: 070 ----
mean loss: 202.29
 ---- batch: 080 ----
mean loss: 204.83
 ---- batch: 090 ----
mean loss: 198.45
 ---- batch: 100 ----
mean loss: 210.99
 ---- batch: 110 ----
mean loss: 217.48
train mean loss: 206.46
epoch train time: 0:00:00.729985
elapsed time: 0:02:17.855161
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:35:25.270640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.13
 ---- batch: 020 ----
mean loss: 201.26
 ---- batch: 030 ----
mean loss: 197.81
 ---- batch: 040 ----
mean loss: 204.45
 ---- batch: 050 ----
mean loss: 215.15
 ---- batch: 060 ----
mean loss: 198.16
 ---- batch: 070 ----
mean loss: 207.76
 ---- batch: 080 ----
mean loss: 217.25
 ---- batch: 090 ----
mean loss: 211.21
 ---- batch: 100 ----
mean loss: 209.59
 ---- batch: 110 ----
mean loss: 197.46
train mean loss: 206.30
epoch train time: 0:00:00.737776
elapsed time: 0:02:18.593080
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:35:26.008560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.60
 ---- batch: 020 ----
mean loss: 205.58
 ---- batch: 030 ----
mean loss: 208.88
 ---- batch: 040 ----
mean loss: 212.00
 ---- batch: 050 ----
mean loss: 202.15
 ---- batch: 060 ----
mean loss: 209.29
 ---- batch: 070 ----
mean loss: 198.82
 ---- batch: 080 ----
mean loss: 213.32
 ---- batch: 090 ----
mean loss: 203.13
 ---- batch: 100 ----
mean loss: 206.20
 ---- batch: 110 ----
mean loss: 205.79
train mean loss: 206.07
epoch train time: 0:00:00.724859
elapsed time: 0:02:19.318090
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:35:26.733585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.40
 ---- batch: 020 ----
mean loss: 209.38
 ---- batch: 030 ----
mean loss: 211.51
 ---- batch: 040 ----
mean loss: 201.52
 ---- batch: 050 ----
mean loss: 212.69
 ---- batch: 060 ----
mean loss: 211.27
 ---- batch: 070 ----
mean loss: 204.94
 ---- batch: 080 ----
mean loss: 206.14
 ---- batch: 090 ----
mean loss: 203.55
 ---- batch: 100 ----
mean loss: 202.47
 ---- batch: 110 ----
mean loss: 206.64
train mean loss: 205.81
epoch train time: 0:00:00.731465
elapsed time: 0:02:20.049709
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:35:27.465184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.32
 ---- batch: 020 ----
mean loss: 210.03
 ---- batch: 030 ----
mean loss: 199.14
 ---- batch: 040 ----
mean loss: 208.95
 ---- batch: 050 ----
mean loss: 201.87
 ---- batch: 060 ----
mean loss: 207.31
 ---- batch: 070 ----
mean loss: 207.82
 ---- batch: 080 ----
mean loss: 200.39
 ---- batch: 090 ----
mean loss: 207.62
 ---- batch: 100 ----
mean loss: 198.74
 ---- batch: 110 ----
mean loss: 210.27
train mean loss: 205.84
epoch train time: 0:00:00.735311
elapsed time: 0:02:20.785161
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:35:28.200670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.00
 ---- batch: 020 ----
mean loss: 207.45
 ---- batch: 030 ----
mean loss: 205.11
 ---- batch: 040 ----
mean loss: 201.78
 ---- batch: 050 ----
mean loss: 211.91
 ---- batch: 060 ----
mean loss: 202.15
 ---- batch: 070 ----
mean loss: 206.42
 ---- batch: 080 ----
mean loss: 208.50
 ---- batch: 090 ----
mean loss: 206.34
 ---- batch: 100 ----
mean loss: 212.60
 ---- batch: 110 ----
mean loss: 194.01
train mean loss: 205.35
epoch train time: 0:00:00.729774
elapsed time: 0:02:21.515106
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:35:28.930584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.41
 ---- batch: 020 ----
mean loss: 208.26
 ---- batch: 030 ----
mean loss: 211.73
 ---- batch: 040 ----
mean loss: 205.63
 ---- batch: 050 ----
mean loss: 207.15
 ---- batch: 060 ----
mean loss: 193.56
 ---- batch: 070 ----
mean loss: 202.32
 ---- batch: 080 ----
mean loss: 213.12
 ---- batch: 090 ----
mean loss: 208.67
 ---- batch: 100 ----
mean loss: 209.41
 ---- batch: 110 ----
mean loss: 197.52
train mean loss: 205.14
epoch train time: 0:00:00.727689
elapsed time: 0:02:22.242953
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:35:29.658423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.95
 ---- batch: 020 ----
mean loss: 203.28
 ---- batch: 030 ----
mean loss: 195.44
 ---- batch: 040 ----
mean loss: 192.56
 ---- batch: 050 ----
mean loss: 201.89
 ---- batch: 060 ----
mean loss: 204.36
 ---- batch: 070 ----
mean loss: 211.71
 ---- batch: 080 ----
mean loss: 208.53
 ---- batch: 090 ----
mean loss: 208.31
 ---- batch: 100 ----
mean loss: 209.92
 ---- batch: 110 ----
mean loss: 204.26
train mean loss: 204.92
epoch train time: 0:00:00.730924
elapsed time: 0:02:22.974013
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:35:30.389491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.80
 ---- batch: 020 ----
mean loss: 201.27
 ---- batch: 030 ----
mean loss: 209.46
 ---- batch: 040 ----
mean loss: 200.97
 ---- batch: 050 ----
mean loss: 203.36
 ---- batch: 060 ----
mean loss: 212.00
 ---- batch: 070 ----
mean loss: 199.21
 ---- batch: 080 ----
mean loss: 210.04
 ---- batch: 090 ----
mean loss: 209.14
 ---- batch: 100 ----
mean loss: 200.07
 ---- batch: 110 ----
mean loss: 202.43
train mean loss: 204.92
epoch train time: 0:00:00.731660
elapsed time: 0:02:23.705812
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:35:31.121324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.85
 ---- batch: 020 ----
mean loss: 198.20
 ---- batch: 030 ----
mean loss: 207.98
 ---- batch: 040 ----
mean loss: 205.69
 ---- batch: 050 ----
mean loss: 203.40
 ---- batch: 060 ----
mean loss: 207.79
 ---- batch: 070 ----
mean loss: 205.10
 ---- batch: 080 ----
mean loss: 202.58
 ---- batch: 090 ----
mean loss: 205.07
 ---- batch: 100 ----
mean loss: 211.85
 ---- batch: 110 ----
mean loss: 197.14
train mean loss: 204.76
epoch train time: 0:00:00.732963
elapsed time: 0:02:24.438947
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:35:31.854423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.94
 ---- batch: 020 ----
mean loss: 201.79
 ---- batch: 030 ----
mean loss: 202.53
 ---- batch: 040 ----
mean loss: 197.62
 ---- batch: 050 ----
mean loss: 205.88
 ---- batch: 060 ----
mean loss: 200.13
 ---- batch: 070 ----
mean loss: 199.23
 ---- batch: 080 ----
mean loss: 210.61
 ---- batch: 090 ----
mean loss: 209.81
 ---- batch: 100 ----
mean loss: 202.77
 ---- batch: 110 ----
mean loss: 202.09
train mean loss: 204.28
epoch train time: 0:00:00.737001
elapsed time: 0:02:25.176164
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:35:32.591641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.46
 ---- batch: 020 ----
mean loss: 195.71
 ---- batch: 030 ----
mean loss: 200.77
 ---- batch: 040 ----
mean loss: 203.22
 ---- batch: 050 ----
mean loss: 212.47
 ---- batch: 060 ----
mean loss: 207.37
 ---- batch: 070 ----
mean loss: 206.15
 ---- batch: 080 ----
mean loss: 202.74
 ---- batch: 090 ----
mean loss: 200.25
 ---- batch: 100 ----
mean loss: 202.30
 ---- batch: 110 ----
mean loss: 206.21
train mean loss: 204.20
epoch train time: 0:00:00.731994
elapsed time: 0:02:25.908323
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:35:33.323799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.37
 ---- batch: 020 ----
mean loss: 202.92
 ---- batch: 030 ----
mean loss: 206.06
 ---- batch: 040 ----
mean loss: 202.97
 ---- batch: 050 ----
mean loss: 204.67
 ---- batch: 060 ----
mean loss: 199.82
 ---- batch: 070 ----
mean loss: 204.34
 ---- batch: 080 ----
mean loss: 202.00
 ---- batch: 090 ----
mean loss: 210.11
 ---- batch: 100 ----
mean loss: 199.36
 ---- batch: 110 ----
mean loss: 208.87
train mean loss: 203.83
epoch train time: 0:00:00.737339
elapsed time: 0:02:26.645792
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:35:34.061269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.83
 ---- batch: 020 ----
mean loss: 201.32
 ---- batch: 030 ----
mean loss: 200.39
 ---- batch: 040 ----
mean loss: 207.85
 ---- batch: 050 ----
mean loss: 199.04
 ---- batch: 060 ----
mean loss: 201.58
 ---- batch: 070 ----
mean loss: 210.11
 ---- batch: 080 ----
mean loss: 203.69
 ---- batch: 090 ----
mean loss: 207.76
 ---- batch: 100 ----
mean loss: 202.17
 ---- batch: 110 ----
mean loss: 206.64
train mean loss: 203.94
epoch train time: 0:00:00.729848
elapsed time: 0:02:27.375790
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:35:34.791266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.95
 ---- batch: 020 ----
mean loss: 200.16
 ---- batch: 030 ----
mean loss: 209.31
 ---- batch: 040 ----
mean loss: 216.93
 ---- batch: 050 ----
mean loss: 200.11
 ---- batch: 060 ----
mean loss: 203.68
 ---- batch: 070 ----
mean loss: 204.59
 ---- batch: 080 ----
mean loss: 199.16
 ---- batch: 090 ----
mean loss: 195.29
 ---- batch: 100 ----
mean loss: 207.72
 ---- batch: 110 ----
mean loss: 199.15
train mean loss: 203.76
epoch train time: 0:00:00.727748
elapsed time: 0:02:28.103679
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:35:35.519166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.51
 ---- batch: 020 ----
mean loss: 203.76
 ---- batch: 030 ----
mean loss: 206.34
 ---- batch: 040 ----
mean loss: 202.55
 ---- batch: 050 ----
mean loss: 193.77
 ---- batch: 060 ----
mean loss: 209.94
 ---- batch: 070 ----
mean loss: 198.12
 ---- batch: 080 ----
mean loss: 203.00
 ---- batch: 090 ----
mean loss: 207.33
 ---- batch: 100 ----
mean loss: 208.12
 ---- batch: 110 ----
mean loss: 208.70
train mean loss: 203.65
epoch train time: 0:00:00.740213
elapsed time: 0:02:28.844044
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:35:36.259546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.57
 ---- batch: 020 ----
mean loss: 198.99
 ---- batch: 030 ----
mean loss: 205.36
 ---- batch: 040 ----
mean loss: 198.65
 ---- batch: 050 ----
mean loss: 200.99
 ---- batch: 060 ----
mean loss: 199.82
 ---- batch: 070 ----
mean loss: 206.97
 ---- batch: 080 ----
mean loss: 200.02
 ---- batch: 090 ----
mean loss: 194.50
 ---- batch: 100 ----
mean loss: 207.03
 ---- batch: 110 ----
mean loss: 212.44
train mean loss: 203.19
epoch train time: 0:00:00.731652
elapsed time: 0:02:29.575857
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:35:36.991335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.79
 ---- batch: 020 ----
mean loss: 199.74
 ---- batch: 030 ----
mean loss: 198.97
 ---- batch: 040 ----
mean loss: 205.43
 ---- batch: 050 ----
mean loss: 205.63
 ---- batch: 060 ----
mean loss: 196.74
 ---- batch: 070 ----
mean loss: 208.86
 ---- batch: 080 ----
mean loss: 203.26
 ---- batch: 090 ----
mean loss: 198.65
 ---- batch: 100 ----
mean loss: 213.11
 ---- batch: 110 ----
mean loss: 198.61
train mean loss: 203.24
epoch train time: 0:00:00.729025
elapsed time: 0:02:30.305018
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:35:37.720495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.25
 ---- batch: 020 ----
mean loss: 198.89
 ---- batch: 030 ----
mean loss: 198.31
 ---- batch: 040 ----
mean loss: 205.64
 ---- batch: 050 ----
mean loss: 222.31
 ---- batch: 060 ----
mean loss: 200.44
 ---- batch: 070 ----
mean loss: 198.14
 ---- batch: 080 ----
mean loss: 209.69
 ---- batch: 090 ----
mean loss: 207.45
 ---- batch: 100 ----
mean loss: 194.01
 ---- batch: 110 ----
mean loss: 195.38
train mean loss: 202.80
epoch train time: 0:00:00.736633
elapsed time: 0:02:31.041789
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:35:38.457268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.29
 ---- batch: 020 ----
mean loss: 200.27
 ---- batch: 030 ----
mean loss: 203.38
 ---- batch: 040 ----
mean loss: 205.53
 ---- batch: 050 ----
mean loss: 213.39
 ---- batch: 060 ----
mean loss: 200.93
 ---- batch: 070 ----
mean loss: 199.41
 ---- batch: 080 ----
mean loss: 201.61
 ---- batch: 090 ----
mean loss: 203.50
 ---- batch: 100 ----
mean loss: 209.21
 ---- batch: 110 ----
mean loss: 202.11
train mean loss: 202.67
epoch train time: 0:00:00.743739
elapsed time: 0:02:31.785663
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:35:39.201138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.63
 ---- batch: 020 ----
mean loss: 207.56
 ---- batch: 030 ----
mean loss: 208.50
 ---- batch: 040 ----
mean loss: 196.10
 ---- batch: 050 ----
mean loss: 214.73
 ---- batch: 060 ----
mean loss: 204.09
 ---- batch: 070 ----
mean loss: 196.74
 ---- batch: 080 ----
mean loss: 185.30
 ---- batch: 090 ----
mean loss: 196.52
 ---- batch: 100 ----
mean loss: 205.96
 ---- batch: 110 ----
mean loss: 209.13
train mean loss: 202.86
epoch train time: 0:00:00.737583
elapsed time: 0:02:32.523379
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:35:39.938856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.05
 ---- batch: 020 ----
mean loss: 201.43
 ---- batch: 030 ----
mean loss: 209.36
 ---- batch: 040 ----
mean loss: 210.18
 ---- batch: 050 ----
mean loss: 205.56
 ---- batch: 060 ----
mean loss: 198.03
 ---- batch: 070 ----
mean loss: 202.38
 ---- batch: 080 ----
mean loss: 201.75
 ---- batch: 090 ----
mean loss: 194.47
 ---- batch: 100 ----
mean loss: 195.68
 ---- batch: 110 ----
mean loss: 203.13
train mean loss: 202.34
epoch train time: 0:00:00.723824
elapsed time: 0:02:33.247339
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:35:40.662814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.17
 ---- batch: 020 ----
mean loss: 205.82
 ---- batch: 030 ----
mean loss: 202.21
 ---- batch: 040 ----
mean loss: 201.74
 ---- batch: 050 ----
mean loss: 207.19
 ---- batch: 060 ----
mean loss: 196.59
 ---- batch: 070 ----
mean loss: 217.57
 ---- batch: 080 ----
mean loss: 199.13
 ---- batch: 090 ----
mean loss: 193.73
 ---- batch: 100 ----
mean loss: 203.82
 ---- batch: 110 ----
mean loss: 199.27
train mean loss: 202.33
epoch train time: 0:00:00.725827
elapsed time: 0:02:33.973296
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:35:41.388810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.14
 ---- batch: 020 ----
mean loss: 193.65
 ---- batch: 030 ----
mean loss: 205.77
 ---- batch: 040 ----
mean loss: 203.28
 ---- batch: 050 ----
mean loss: 214.23
 ---- batch: 060 ----
mean loss: 202.53
 ---- batch: 070 ----
mean loss: 201.95
 ---- batch: 080 ----
mean loss: 202.19
 ---- batch: 090 ----
mean loss: 204.09
 ---- batch: 100 ----
mean loss: 198.24
 ---- batch: 110 ----
mean loss: 193.03
train mean loss: 202.10
epoch train time: 0:00:00.730741
elapsed time: 0:02:34.704206
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:35:42.119699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.97
 ---- batch: 020 ----
mean loss: 202.77
 ---- batch: 030 ----
mean loss: 198.30
 ---- batch: 040 ----
mean loss: 198.42
 ---- batch: 050 ----
mean loss: 193.29
 ---- batch: 060 ----
mean loss: 200.29
 ---- batch: 070 ----
mean loss: 192.37
 ---- batch: 080 ----
mean loss: 215.41
 ---- batch: 090 ----
mean loss: 206.06
 ---- batch: 100 ----
mean loss: 218.24
 ---- batch: 110 ----
mean loss: 195.43
train mean loss: 202.03
epoch train time: 0:00:00.732403
elapsed time: 0:02:35.436761
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:35:42.852238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.62
 ---- batch: 020 ----
mean loss: 199.35
 ---- batch: 030 ----
mean loss: 197.47
 ---- batch: 040 ----
mean loss: 196.44
 ---- batch: 050 ----
mean loss: 205.77
 ---- batch: 060 ----
mean loss: 197.63
 ---- batch: 070 ----
mean loss: 202.06
 ---- batch: 080 ----
mean loss: 195.46
 ---- batch: 090 ----
mean loss: 208.19
 ---- batch: 100 ----
mean loss: 205.58
 ---- batch: 110 ----
mean loss: 206.48
train mean loss: 201.65
epoch train time: 0:00:00.727154
elapsed time: 0:02:36.164050
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:35:43.579525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.97
 ---- batch: 020 ----
mean loss: 208.85
 ---- batch: 030 ----
mean loss: 203.47
 ---- batch: 040 ----
mean loss: 211.36
 ---- batch: 050 ----
mean loss: 209.93
 ---- batch: 060 ----
mean loss: 199.05
 ---- batch: 070 ----
mean loss: 192.54
 ---- batch: 080 ----
mean loss: 200.46
 ---- batch: 090 ----
mean loss: 196.42
 ---- batch: 100 ----
mean loss: 196.26
 ---- batch: 110 ----
mean loss: 196.67
train mean loss: 201.66
epoch train time: 0:00:00.732856
elapsed time: 0:02:36.897040
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:35:44.312519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.29
 ---- batch: 020 ----
mean loss: 204.38
 ---- batch: 030 ----
mean loss: 190.90
 ---- batch: 040 ----
mean loss: 211.01
 ---- batch: 050 ----
mean loss: 207.35
 ---- batch: 060 ----
mean loss: 203.48
 ---- batch: 070 ----
mean loss: 202.01
 ---- batch: 080 ----
mean loss: 205.70
 ---- batch: 090 ----
mean loss: 194.08
 ---- batch: 100 ----
mean loss: 204.16
 ---- batch: 110 ----
mean loss: 196.12
train mean loss: 201.62
epoch train time: 0:00:00.728423
elapsed time: 0:02:37.625602
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:35:45.041081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.25
 ---- batch: 020 ----
mean loss: 208.28
 ---- batch: 030 ----
mean loss: 205.93
 ---- batch: 040 ----
mean loss: 197.19
 ---- batch: 050 ----
mean loss: 193.89
 ---- batch: 060 ----
mean loss: 206.57
 ---- batch: 070 ----
mean loss: 196.12
 ---- batch: 080 ----
mean loss: 204.58
 ---- batch: 090 ----
mean loss: 206.58
 ---- batch: 100 ----
mean loss: 195.18
 ---- batch: 110 ----
mean loss: 194.86
train mean loss: 201.31
epoch train time: 0:00:00.724331
elapsed time: 0:02:38.350087
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:35:45.765586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.30
 ---- batch: 020 ----
mean loss: 212.22
 ---- batch: 030 ----
mean loss: 207.30
 ---- batch: 040 ----
mean loss: 198.41
 ---- batch: 050 ----
mean loss: 201.03
 ---- batch: 060 ----
mean loss: 192.96
 ---- batch: 070 ----
mean loss: 207.76
 ---- batch: 080 ----
mean loss: 197.84
 ---- batch: 090 ----
mean loss: 202.56
 ---- batch: 100 ----
mean loss: 201.13
 ---- batch: 110 ----
mean loss: 200.22
train mean loss: 201.12
epoch train time: 0:00:00.730758
elapsed time: 0:02:39.081021
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:35:46.496502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.42
 ---- batch: 020 ----
mean loss: 207.58
 ---- batch: 030 ----
mean loss: 200.27
 ---- batch: 040 ----
mean loss: 191.98
 ---- batch: 050 ----
mean loss: 199.95
 ---- batch: 060 ----
mean loss: 200.53
 ---- batch: 070 ----
mean loss: 191.55
 ---- batch: 080 ----
mean loss: 205.41
 ---- batch: 090 ----
mean loss: 211.21
 ---- batch: 100 ----
mean loss: 195.60
 ---- batch: 110 ----
mean loss: 204.83
train mean loss: 200.99
epoch train time: 0:00:00.732224
elapsed time: 0:02:39.813386
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:35:47.228867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.93
 ---- batch: 020 ----
mean loss: 201.07
 ---- batch: 030 ----
mean loss: 205.08
 ---- batch: 040 ----
mean loss: 202.35
 ---- batch: 050 ----
mean loss: 194.54
 ---- batch: 060 ----
mean loss: 200.64
 ---- batch: 070 ----
mean loss: 202.39
 ---- batch: 080 ----
mean loss: 198.44
 ---- batch: 090 ----
mean loss: 209.55
 ---- batch: 100 ----
mean loss: 191.19
 ---- batch: 110 ----
mean loss: 204.06
train mean loss: 200.94
epoch train time: 0:00:00.722888
elapsed time: 0:02:40.536432
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:35:47.951904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.31
 ---- batch: 020 ----
mean loss: 207.79
 ---- batch: 030 ----
mean loss: 201.95
 ---- batch: 040 ----
mean loss: 200.20
 ---- batch: 050 ----
mean loss: 208.10
 ---- batch: 060 ----
mean loss: 198.15
 ---- batch: 070 ----
mean loss: 198.48
 ---- batch: 080 ----
mean loss: 193.72
 ---- batch: 090 ----
mean loss: 197.56
 ---- batch: 100 ----
mean loss: 202.11
 ---- batch: 110 ----
mean loss: 198.55
train mean loss: 200.58
epoch train time: 0:00:00.739089
elapsed time: 0:02:41.275655
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:35:48.691133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.25
 ---- batch: 020 ----
mean loss: 199.84
 ---- batch: 030 ----
mean loss: 196.88
 ---- batch: 040 ----
mean loss: 197.33
 ---- batch: 050 ----
mean loss: 193.31
 ---- batch: 060 ----
mean loss: 200.32
 ---- batch: 070 ----
mean loss: 203.68
 ---- batch: 080 ----
mean loss: 198.12
 ---- batch: 090 ----
mean loss: 208.15
 ---- batch: 100 ----
mean loss: 203.99
 ---- batch: 110 ----
mean loss: 197.95
train mean loss: 200.61
epoch train time: 0:00:00.745378
elapsed time: 0:02:42.021171
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:35:49.436649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.12
 ---- batch: 020 ----
mean loss: 203.35
 ---- batch: 030 ----
mean loss: 189.63
 ---- batch: 040 ----
mean loss: 211.35
 ---- batch: 050 ----
mean loss: 205.64
 ---- batch: 060 ----
mean loss: 201.31
 ---- batch: 070 ----
mean loss: 197.51
 ---- batch: 080 ----
mean loss: 203.48
 ---- batch: 090 ----
mean loss: 194.85
 ---- batch: 100 ----
mean loss: 199.45
 ---- batch: 110 ----
mean loss: 195.61
train mean loss: 200.34
epoch train time: 0:00:00.725911
elapsed time: 0:02:42.747220
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:35:50.162699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.73
 ---- batch: 020 ----
mean loss: 210.81
 ---- batch: 030 ----
mean loss: 204.27
 ---- batch: 040 ----
mean loss: 191.41
 ---- batch: 050 ----
mean loss: 200.63
 ---- batch: 060 ----
mean loss: 196.84
 ---- batch: 070 ----
mean loss: 192.05
 ---- batch: 080 ----
mean loss: 199.51
 ---- batch: 090 ----
mean loss: 203.29
 ---- batch: 100 ----
mean loss: 199.43
 ---- batch: 110 ----
mean loss: 192.92
train mean loss: 200.25
epoch train time: 0:00:00.725121
elapsed time: 0:02:43.472504
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:35:50.888003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.79
 ---- batch: 020 ----
mean loss: 207.50
 ---- batch: 030 ----
mean loss: 201.51
 ---- batch: 040 ----
mean loss: 194.24
 ---- batch: 050 ----
mean loss: 195.15
 ---- batch: 060 ----
mean loss: 205.60
 ---- batch: 070 ----
mean loss: 199.66
 ---- batch: 080 ----
mean loss: 194.47
 ---- batch: 090 ----
mean loss: 203.05
 ---- batch: 100 ----
mean loss: 198.73
 ---- batch: 110 ----
mean loss: 204.99
train mean loss: 200.18
epoch train time: 0:00:00.734705
elapsed time: 0:02:44.207386
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:35:51.622880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.10
 ---- batch: 020 ----
mean loss: 195.28
 ---- batch: 030 ----
mean loss: 201.54
 ---- batch: 040 ----
mean loss: 205.66
 ---- batch: 050 ----
mean loss: 203.63
 ---- batch: 060 ----
mean loss: 193.38
 ---- batch: 070 ----
mean loss: 205.83
 ---- batch: 080 ----
mean loss: 200.83
 ---- batch: 090 ----
mean loss: 197.56
 ---- batch: 100 ----
mean loss: 195.70
 ---- batch: 110 ----
mean loss: 208.52
train mean loss: 200.13
epoch train time: 0:00:00.737518
elapsed time: 0:02:44.945059
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:35:52.360540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.71
 ---- batch: 020 ----
mean loss: 207.57
 ---- batch: 030 ----
mean loss: 195.39
 ---- batch: 040 ----
mean loss: 192.82
 ---- batch: 050 ----
mean loss: 208.10
 ---- batch: 060 ----
mean loss: 203.82
 ---- batch: 070 ----
mean loss: 198.95
 ---- batch: 080 ----
mean loss: 200.88
 ---- batch: 090 ----
mean loss: 194.21
 ---- batch: 100 ----
mean loss: 200.25
 ---- batch: 110 ----
mean loss: 202.17
train mean loss: 199.84
epoch train time: 0:00:00.747733
elapsed time: 0:02:45.692946
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:35:53.108433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.78
 ---- batch: 020 ----
mean loss: 199.63
 ---- batch: 030 ----
mean loss: 212.55
 ---- batch: 040 ----
mean loss: 189.76
 ---- batch: 050 ----
mean loss: 197.47
 ---- batch: 060 ----
mean loss: 191.77
 ---- batch: 070 ----
mean loss: 196.80
 ---- batch: 080 ----
mean loss: 199.04
 ---- batch: 090 ----
mean loss: 203.24
 ---- batch: 100 ----
mean loss: 204.98
 ---- batch: 110 ----
mean loss: 200.96
train mean loss: 199.79
epoch train time: 0:00:00.752421
elapsed time: 0:02:46.445623
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:35:53.861143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.27
 ---- batch: 020 ----
mean loss: 205.67
 ---- batch: 030 ----
mean loss: 201.59
 ---- batch: 040 ----
mean loss: 202.55
 ---- batch: 050 ----
mean loss: 197.15
 ---- batch: 060 ----
mean loss: 203.82
 ---- batch: 070 ----
mean loss: 196.71
 ---- batch: 080 ----
mean loss: 196.36
 ---- batch: 090 ----
mean loss: 201.92
 ---- batch: 100 ----
mean loss: 197.43
 ---- batch: 110 ----
mean loss: 190.87
train mean loss: 199.70
epoch train time: 0:00:00.755823
elapsed time: 0:02:47.201628
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:35:54.617112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.83
 ---- batch: 020 ----
mean loss: 201.67
 ---- batch: 030 ----
mean loss: 189.45
 ---- batch: 040 ----
mean loss: 202.51
 ---- batch: 050 ----
mean loss: 189.71
 ---- batch: 060 ----
mean loss: 199.86
 ---- batch: 070 ----
mean loss: 199.70
 ---- batch: 080 ----
mean loss: 202.62
 ---- batch: 090 ----
mean loss: 209.23
 ---- batch: 100 ----
mean loss: 203.38
 ---- batch: 110 ----
mean loss: 197.93
train mean loss: 199.31
epoch train time: 0:00:00.749705
elapsed time: 0:02:47.951485
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:35:55.366987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.54
 ---- batch: 020 ----
mean loss: 201.38
 ---- batch: 030 ----
mean loss: 203.39
 ---- batch: 040 ----
mean loss: 197.94
 ---- batch: 050 ----
mean loss: 202.53
 ---- batch: 060 ----
mean loss: 204.36
 ---- batch: 070 ----
mean loss: 197.94
 ---- batch: 080 ----
mean loss: 199.42
 ---- batch: 090 ----
mean loss: 190.96
 ---- batch: 100 ----
mean loss: 195.86
 ---- batch: 110 ----
mean loss: 205.46
train mean loss: 199.19
epoch train time: 0:00:00.753516
elapsed time: 0:02:48.705185
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:35:56.120713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.65
 ---- batch: 020 ----
mean loss: 189.85
 ---- batch: 030 ----
mean loss: 206.80
 ---- batch: 040 ----
mean loss: 199.53
 ---- batch: 050 ----
mean loss: 202.60
 ---- batch: 060 ----
mean loss: 198.22
 ---- batch: 070 ----
mean loss: 199.79
 ---- batch: 080 ----
mean loss: 192.52
 ---- batch: 090 ----
mean loss: 196.44
 ---- batch: 100 ----
mean loss: 203.00
 ---- batch: 110 ----
mean loss: 201.86
train mean loss: 199.32
epoch train time: 0:00:00.748386
elapsed time: 0:02:49.453762
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:35:56.869242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.11
 ---- batch: 020 ----
mean loss: 211.40
 ---- batch: 030 ----
mean loss: 201.44
 ---- batch: 040 ----
mean loss: 196.50
 ---- batch: 050 ----
mean loss: 199.73
 ---- batch: 060 ----
mean loss: 198.12
 ---- batch: 070 ----
mean loss: 197.45
 ---- batch: 080 ----
mean loss: 196.91
 ---- batch: 090 ----
mean loss: 192.91
 ---- batch: 100 ----
mean loss: 191.99
 ---- batch: 110 ----
mean loss: 197.84
train mean loss: 199.01
epoch train time: 0:00:00.748022
elapsed time: 0:02:50.201932
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:35:57.617412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.77
 ---- batch: 020 ----
mean loss: 194.06
 ---- batch: 030 ----
mean loss: 199.09
 ---- batch: 040 ----
mean loss: 203.93
 ---- batch: 050 ----
mean loss: 201.86
 ---- batch: 060 ----
mean loss: 205.13
 ---- batch: 070 ----
mean loss: 195.51
 ---- batch: 080 ----
mean loss: 193.04
 ---- batch: 090 ----
mean loss: 206.14
 ---- batch: 100 ----
mean loss: 198.82
 ---- batch: 110 ----
mean loss: 197.46
train mean loss: 199.03
epoch train time: 0:00:00.747166
elapsed time: 0:02:50.949267
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:35:58.364744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.79
 ---- batch: 020 ----
mean loss: 200.29
 ---- batch: 030 ----
mean loss: 208.96
 ---- batch: 040 ----
mean loss: 191.39
 ---- batch: 050 ----
mean loss: 200.70
 ---- batch: 060 ----
mean loss: 196.29
 ---- batch: 070 ----
mean loss: 200.61
 ---- batch: 080 ----
mean loss: 192.31
 ---- batch: 090 ----
mean loss: 198.95
 ---- batch: 100 ----
mean loss: 198.91
 ---- batch: 110 ----
mean loss: 197.67
train mean loss: 198.76
epoch train time: 0:00:00.752562
elapsed time: 0:02:51.701973
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:35:59.117461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.53
 ---- batch: 020 ----
mean loss: 202.31
 ---- batch: 030 ----
mean loss: 208.12
 ---- batch: 040 ----
mean loss: 197.09
 ---- batch: 050 ----
mean loss: 199.46
 ---- batch: 060 ----
mean loss: 197.69
 ---- batch: 070 ----
mean loss: 195.25
 ---- batch: 080 ----
mean loss: 203.70
 ---- batch: 090 ----
mean loss: 198.99
 ---- batch: 100 ----
mean loss: 189.43
 ---- batch: 110 ----
mean loss: 193.47
train mean loss: 198.69
epoch train time: 0:00:00.761213
elapsed time: 0:02:52.463340
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:35:59.878819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.89
 ---- batch: 020 ----
mean loss: 194.96
 ---- batch: 030 ----
mean loss: 205.55
 ---- batch: 040 ----
mean loss: 197.50
 ---- batch: 050 ----
mean loss: 191.48
 ---- batch: 060 ----
mean loss: 205.62
 ---- batch: 070 ----
mean loss: 204.78
 ---- batch: 080 ----
mean loss: 196.40
 ---- batch: 090 ----
mean loss: 192.90
 ---- batch: 100 ----
mean loss: 199.55
 ---- batch: 110 ----
mean loss: 207.21
train mean loss: 198.75
epoch train time: 0:00:00.752954
elapsed time: 0:02:53.216434
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:36:00.631940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.68
 ---- batch: 020 ----
mean loss: 192.85
 ---- batch: 030 ----
mean loss: 197.48
 ---- batch: 040 ----
mean loss: 198.13
 ---- batch: 050 ----
mean loss: 207.29
 ---- batch: 060 ----
mean loss: 203.55
 ---- batch: 070 ----
mean loss: 195.48
 ---- batch: 080 ----
mean loss: 191.39
 ---- batch: 090 ----
mean loss: 202.63
 ---- batch: 100 ----
mean loss: 199.20
 ---- batch: 110 ----
mean loss: 196.28
train mean loss: 198.41
epoch train time: 0:00:00.757101
elapsed time: 0:02:53.973699
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:36:01.389177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.40
 ---- batch: 020 ----
mean loss: 193.66
 ---- batch: 030 ----
mean loss: 197.15
 ---- batch: 040 ----
mean loss: 199.90
 ---- batch: 050 ----
mean loss: 204.42
 ---- batch: 060 ----
mean loss: 206.89
 ---- batch: 070 ----
mean loss: 194.36
 ---- batch: 080 ----
mean loss: 196.83
 ---- batch: 090 ----
mean loss: 192.74
 ---- batch: 100 ----
mean loss: 192.01
 ---- batch: 110 ----
mean loss: 197.66
train mean loss: 198.31
epoch train time: 0:00:00.749102
elapsed time: 0:02:54.722943
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:36:02.138435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.96
 ---- batch: 020 ----
mean loss: 200.49
 ---- batch: 030 ----
mean loss: 198.39
 ---- batch: 040 ----
mean loss: 200.60
 ---- batch: 050 ----
mean loss: 203.23
 ---- batch: 060 ----
mean loss: 190.55
 ---- batch: 070 ----
mean loss: 190.58
 ---- batch: 080 ----
mean loss: 196.76
 ---- batch: 090 ----
mean loss: 190.60
 ---- batch: 100 ----
mean loss: 203.41
 ---- batch: 110 ----
mean loss: 206.55
train mean loss: 198.17
epoch train time: 0:00:00.743213
elapsed time: 0:02:55.466313
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:36:02.881806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.51
 ---- batch: 020 ----
mean loss: 209.59
 ---- batch: 030 ----
mean loss: 200.77
 ---- batch: 040 ----
mean loss: 196.15
 ---- batch: 050 ----
mean loss: 190.05
 ---- batch: 060 ----
mean loss: 195.48
 ---- batch: 070 ----
mean loss: 207.68
 ---- batch: 080 ----
mean loss: 192.95
 ---- batch: 090 ----
mean loss: 200.57
 ---- batch: 100 ----
mean loss: 203.05
 ---- batch: 110 ----
mean loss: 188.85
train mean loss: 198.02
epoch train time: 0:00:00.728767
elapsed time: 0:02:56.195248
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:36:03.610724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.85
 ---- batch: 020 ----
mean loss: 205.43
 ---- batch: 030 ----
mean loss: 206.04
 ---- batch: 040 ----
mean loss: 199.93
 ---- batch: 050 ----
mean loss: 187.87
 ---- batch: 060 ----
mean loss: 192.90
 ---- batch: 070 ----
mean loss: 198.69
 ---- batch: 080 ----
mean loss: 194.71
 ---- batch: 090 ----
mean loss: 202.92
 ---- batch: 100 ----
mean loss: 196.39
 ---- batch: 110 ----
mean loss: 202.90
train mean loss: 197.92
epoch train time: 0:00:00.734787
elapsed time: 0:02:56.930252
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:36:04.345748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.81
 ---- batch: 020 ----
mean loss: 202.07
 ---- batch: 030 ----
mean loss: 191.54
 ---- batch: 040 ----
mean loss: 207.40
 ---- batch: 050 ----
mean loss: 193.34
 ---- batch: 060 ----
mean loss: 192.74
 ---- batch: 070 ----
mean loss: 198.29
 ---- batch: 080 ----
mean loss: 199.22
 ---- batch: 090 ----
mean loss: 196.49
 ---- batch: 100 ----
mean loss: 187.39
 ---- batch: 110 ----
mean loss: 206.53
train mean loss: 197.67
epoch train time: 0:00:00.724029
elapsed time: 0:02:57.654437
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:36:05.069938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.44
 ---- batch: 020 ----
mean loss: 197.25
 ---- batch: 030 ----
mean loss: 200.11
 ---- batch: 040 ----
mean loss: 190.17
 ---- batch: 050 ----
mean loss: 202.62
 ---- batch: 060 ----
mean loss: 195.44
 ---- batch: 070 ----
mean loss: 211.51
 ---- batch: 080 ----
mean loss: 204.20
 ---- batch: 090 ----
mean loss: 191.94
 ---- batch: 100 ----
mean loss: 195.79
 ---- batch: 110 ----
mean loss: 191.19
train mean loss: 197.54
epoch train time: 0:00:00.726890
elapsed time: 0:02:58.381489
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:36:05.796965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.15
 ---- batch: 020 ----
mean loss: 195.40
 ---- batch: 030 ----
mean loss: 197.63
 ---- batch: 040 ----
mean loss: 197.53
 ---- batch: 050 ----
mean loss: 196.44
 ---- batch: 060 ----
mean loss: 203.15
 ---- batch: 070 ----
mean loss: 190.65
 ---- batch: 080 ----
mean loss: 191.95
 ---- batch: 090 ----
mean loss: 196.71
 ---- batch: 100 ----
mean loss: 190.02
 ---- batch: 110 ----
mean loss: 204.56
train mean loss: 197.63
epoch train time: 0:00:00.727954
elapsed time: 0:02:59.109581
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:36:06.525058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.17
 ---- batch: 020 ----
mean loss: 187.95
 ---- batch: 030 ----
mean loss: 208.09
 ---- batch: 040 ----
mean loss: 189.72
 ---- batch: 050 ----
mean loss: 201.18
 ---- batch: 060 ----
mean loss: 205.87
 ---- batch: 070 ----
mean loss: 198.70
 ---- batch: 080 ----
mean loss: 198.28
 ---- batch: 090 ----
mean loss: 190.44
 ---- batch: 100 ----
mean loss: 191.81
 ---- batch: 110 ----
mean loss: 199.78
train mean loss: 197.43
epoch train time: 0:00:00.725796
elapsed time: 0:02:59.835520
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:36:07.251021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.96
 ---- batch: 020 ----
mean loss: 194.76
 ---- batch: 030 ----
mean loss: 196.73
 ---- batch: 040 ----
mean loss: 190.84
 ---- batch: 050 ----
mean loss: 198.57
 ---- batch: 060 ----
mean loss: 201.27
 ---- batch: 070 ----
mean loss: 199.77
 ---- batch: 080 ----
mean loss: 187.93
 ---- batch: 090 ----
mean loss: 195.27
 ---- batch: 100 ----
mean loss: 198.08
 ---- batch: 110 ----
mean loss: 209.34
train mean loss: 197.45
epoch train time: 0:00:00.733440
elapsed time: 0:03:00.569130
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:36:07.984628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.90
 ---- batch: 020 ----
mean loss: 192.66
 ---- batch: 030 ----
mean loss: 206.78
 ---- batch: 040 ----
mean loss: 197.22
 ---- batch: 050 ----
mean loss: 191.94
 ---- batch: 060 ----
mean loss: 199.52
 ---- batch: 070 ----
mean loss: 192.17
 ---- batch: 080 ----
mean loss: 195.90
 ---- batch: 090 ----
mean loss: 197.12
 ---- batch: 100 ----
mean loss: 195.54
 ---- batch: 110 ----
mean loss: 195.25
train mean loss: 196.80
epoch train time: 0:00:00.736844
elapsed time: 0:03:01.306140
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:36:08.721625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.99
 ---- batch: 020 ----
mean loss: 192.71
 ---- batch: 030 ----
mean loss: 191.43
 ---- batch: 040 ----
mean loss: 200.38
 ---- batch: 050 ----
mean loss: 197.11
 ---- batch: 060 ----
mean loss: 198.45
 ---- batch: 070 ----
mean loss: 193.37
 ---- batch: 080 ----
mean loss: 203.26
 ---- batch: 090 ----
mean loss: 201.94
 ---- batch: 100 ----
mean loss: 191.92
 ---- batch: 110 ----
mean loss: 191.59
train mean loss: 196.76
epoch train time: 0:00:00.727643
elapsed time: 0:03:02.033935
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:36:09.449421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.55
 ---- batch: 020 ----
mean loss: 198.38
 ---- batch: 030 ----
mean loss: 202.27
 ---- batch: 040 ----
mean loss: 197.65
 ---- batch: 050 ----
mean loss: 199.91
 ---- batch: 060 ----
mean loss: 196.29
 ---- batch: 070 ----
mean loss: 189.72
 ---- batch: 080 ----
mean loss: 203.65
 ---- batch: 090 ----
mean loss: 197.18
 ---- batch: 100 ----
mean loss: 193.62
 ---- batch: 110 ----
mean loss: 190.61
train mean loss: 196.68
epoch train time: 0:00:00.730761
elapsed time: 0:03:02.764843
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:36:10.180338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.74
 ---- batch: 020 ----
mean loss: 199.24
 ---- batch: 030 ----
mean loss: 193.57
 ---- batch: 040 ----
mean loss: 192.91
 ---- batch: 050 ----
mean loss: 197.15
 ---- batch: 060 ----
mean loss: 197.35
 ---- batch: 070 ----
mean loss: 203.61
 ---- batch: 080 ----
mean loss: 199.62
 ---- batch: 090 ----
mean loss: 196.30
 ---- batch: 100 ----
mean loss: 191.03
 ---- batch: 110 ----
mean loss: 194.36
train mean loss: 196.66
epoch train time: 0:00:00.736330
elapsed time: 0:03:03.501326
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:36:10.916802
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.23
 ---- batch: 020 ----
mean loss: 191.47
 ---- batch: 030 ----
mean loss: 196.02
 ---- batch: 040 ----
mean loss: 192.36
 ---- batch: 050 ----
mean loss: 199.86
 ---- batch: 060 ----
mean loss: 200.55
 ---- batch: 070 ----
mean loss: 198.60
 ---- batch: 080 ----
mean loss: 200.96
 ---- batch: 090 ----
mean loss: 191.85
 ---- batch: 100 ----
mean loss: 191.46
 ---- batch: 110 ----
mean loss: 203.07
train mean loss: 196.69
epoch train time: 0:00:00.726191
elapsed time: 0:03:04.227654
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:36:11.643133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.23
 ---- batch: 020 ----
mean loss: 195.45
 ---- batch: 030 ----
mean loss: 196.62
 ---- batch: 040 ----
mean loss: 197.16
 ---- batch: 050 ----
mean loss: 194.36
 ---- batch: 060 ----
mean loss: 198.51
 ---- batch: 070 ----
mean loss: 197.39
 ---- batch: 080 ----
mean loss: 199.52
 ---- batch: 090 ----
mean loss: 195.40
 ---- batch: 100 ----
mean loss: 187.59
 ---- batch: 110 ----
mean loss: 202.74
train mean loss: 196.73
epoch train time: 0:00:00.731158
elapsed time: 0:03:04.958952
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:36:12.374442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.27
 ---- batch: 020 ----
mean loss: 198.25
 ---- batch: 030 ----
mean loss: 199.06
 ---- batch: 040 ----
mean loss: 207.47
 ---- batch: 050 ----
mean loss: 187.26
 ---- batch: 060 ----
mean loss: 199.89
 ---- batch: 070 ----
mean loss: 187.89
 ---- batch: 080 ----
mean loss: 197.58
 ---- batch: 090 ----
mean loss: 191.38
 ---- batch: 100 ----
mean loss: 207.45
 ---- batch: 110 ----
mean loss: 192.88
train mean loss: 196.70
epoch train time: 0:00:00.740574
elapsed time: 0:03:05.699709
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:36:13.115193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.93
 ---- batch: 020 ----
mean loss: 195.98
 ---- batch: 030 ----
mean loss: 190.48
 ---- batch: 040 ----
mean loss: 192.72
 ---- batch: 050 ----
mean loss: 198.79
 ---- batch: 060 ----
mean loss: 205.84
 ---- batch: 070 ----
mean loss: 201.68
 ---- batch: 080 ----
mean loss: 202.56
 ---- batch: 090 ----
mean loss: 193.62
 ---- batch: 100 ----
mean loss: 192.70
 ---- batch: 110 ----
mean loss: 199.03
train mean loss: 196.74
epoch train time: 0:00:00.744468
elapsed time: 0:03:06.444350
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:36:13.859827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.96
 ---- batch: 020 ----
mean loss: 182.34
 ---- batch: 030 ----
mean loss: 199.13
 ---- batch: 040 ----
mean loss: 192.24
 ---- batch: 050 ----
mean loss: 190.97
 ---- batch: 060 ----
mean loss: 202.58
 ---- batch: 070 ----
mean loss: 197.62
 ---- batch: 080 ----
mean loss: 193.81
 ---- batch: 090 ----
mean loss: 197.56
 ---- batch: 100 ----
mean loss: 196.03
 ---- batch: 110 ----
mean loss: 214.47
train mean loss: 196.63
epoch train time: 0:00:00.732550
elapsed time: 0:03:07.177033
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:36:14.592506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.38
 ---- batch: 020 ----
mean loss: 194.19
 ---- batch: 030 ----
mean loss: 196.28
 ---- batch: 040 ----
mean loss: 201.37
 ---- batch: 050 ----
mean loss: 200.89
 ---- batch: 060 ----
mean loss: 201.12
 ---- batch: 070 ----
mean loss: 192.72
 ---- batch: 080 ----
mean loss: 189.84
 ---- batch: 090 ----
mean loss: 187.73
 ---- batch: 100 ----
mean loss: 190.17
 ---- batch: 110 ----
mean loss: 198.68
train mean loss: 196.65
epoch train time: 0:00:00.725451
elapsed time: 0:03:07.902632
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:36:15.318143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.11
 ---- batch: 020 ----
mean loss: 203.32
 ---- batch: 030 ----
mean loss: 197.13
 ---- batch: 040 ----
mean loss: 199.66
 ---- batch: 050 ----
mean loss: 203.18
 ---- batch: 060 ----
mean loss: 198.05
 ---- batch: 070 ----
mean loss: 188.81
 ---- batch: 080 ----
mean loss: 188.78
 ---- batch: 090 ----
mean loss: 203.31
 ---- batch: 100 ----
mean loss: 188.82
 ---- batch: 110 ----
mean loss: 197.40
train mean loss: 196.62
epoch train time: 0:00:00.727831
elapsed time: 0:03:08.630649
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:36:16.046152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.27
 ---- batch: 020 ----
mean loss: 193.05
 ---- batch: 030 ----
mean loss: 197.60
 ---- batch: 040 ----
mean loss: 199.76
 ---- batch: 050 ----
mean loss: 198.73
 ---- batch: 060 ----
mean loss: 199.06
 ---- batch: 070 ----
mean loss: 200.30
 ---- batch: 080 ----
mean loss: 193.95
 ---- batch: 090 ----
mean loss: 194.89
 ---- batch: 100 ----
mean loss: 196.68
 ---- batch: 110 ----
mean loss: 197.76
train mean loss: 196.56
epoch train time: 0:00:00.729530
elapsed time: 0:03:09.360347
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:36:16.775825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.03
 ---- batch: 020 ----
mean loss: 194.16
 ---- batch: 030 ----
mean loss: 187.23
 ---- batch: 040 ----
mean loss: 201.65
 ---- batch: 050 ----
mean loss: 199.98
 ---- batch: 060 ----
mean loss: 204.91
 ---- batch: 070 ----
mean loss: 190.37
 ---- batch: 080 ----
mean loss: 194.87
 ---- batch: 090 ----
mean loss: 184.48
 ---- batch: 100 ----
mean loss: 203.80
 ---- batch: 110 ----
mean loss: 204.73
train mean loss: 196.54
epoch train time: 0:00:00.738908
elapsed time: 0:03:10.099396
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:36:17.514872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.80
 ---- batch: 020 ----
mean loss: 198.22
 ---- batch: 030 ----
mean loss: 197.19
 ---- batch: 040 ----
mean loss: 187.79
 ---- batch: 050 ----
mean loss: 201.85
 ---- batch: 060 ----
mean loss: 192.74
 ---- batch: 070 ----
mean loss: 203.79
 ---- batch: 080 ----
mean loss: 196.85
 ---- batch: 090 ----
mean loss: 191.74
 ---- batch: 100 ----
mean loss: 197.97
 ---- batch: 110 ----
mean loss: 198.43
train mean loss: 196.65
epoch train time: 0:00:00.735669
elapsed time: 0:03:10.835200
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:36:18.250676
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.75
 ---- batch: 020 ----
mean loss: 202.35
 ---- batch: 030 ----
mean loss: 196.87
 ---- batch: 040 ----
mean loss: 203.98
 ---- batch: 050 ----
mean loss: 194.51
 ---- batch: 060 ----
mean loss: 189.88
 ---- batch: 070 ----
mean loss: 205.55
 ---- batch: 080 ----
mean loss: 188.41
 ---- batch: 090 ----
mean loss: 182.81
 ---- batch: 100 ----
mean loss: 194.85
 ---- batch: 110 ----
mean loss: 197.59
train mean loss: 196.56
epoch train time: 0:00:00.733854
elapsed time: 0:03:11.569190
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:36:18.984666
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.90
 ---- batch: 020 ----
mean loss: 196.90
 ---- batch: 030 ----
mean loss: 195.58
 ---- batch: 040 ----
mean loss: 198.44
 ---- batch: 050 ----
mean loss: 194.54
 ---- batch: 060 ----
mean loss: 195.32
 ---- batch: 070 ----
mean loss: 191.15
 ---- batch: 080 ----
mean loss: 199.85
 ---- batch: 090 ----
mean loss: 194.20
 ---- batch: 100 ----
mean loss: 204.06
 ---- batch: 110 ----
mean loss: 193.94
train mean loss: 196.60
epoch train time: 0:00:00.722166
elapsed time: 0:03:12.291505
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:36:19.707004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.16
 ---- batch: 020 ----
mean loss: 191.14
 ---- batch: 030 ----
mean loss: 198.41
 ---- batch: 040 ----
mean loss: 198.01
 ---- batch: 050 ----
mean loss: 193.08
 ---- batch: 060 ----
mean loss: 199.12
 ---- batch: 070 ----
mean loss: 192.90
 ---- batch: 080 ----
mean loss: 201.20
 ---- batch: 090 ----
mean loss: 194.67
 ---- batch: 100 ----
mean loss: 204.55
 ---- batch: 110 ----
mean loss: 199.17
train mean loss: 196.60
epoch train time: 0:00:00.725480
elapsed time: 0:03:13.017186
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:36:20.432674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.95
 ---- batch: 020 ----
mean loss: 201.70
 ---- batch: 030 ----
mean loss: 204.76
 ---- batch: 040 ----
mean loss: 197.67
 ---- batch: 050 ----
mean loss: 193.77
 ---- batch: 060 ----
mean loss: 198.46
 ---- batch: 070 ----
mean loss: 196.08
 ---- batch: 080 ----
mean loss: 187.32
 ---- batch: 090 ----
mean loss: 193.04
 ---- batch: 100 ----
mean loss: 200.20
 ---- batch: 110 ----
mean loss: 194.40
train mean loss: 196.52
epoch train time: 0:00:00.735101
elapsed time: 0:03:13.752432
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:36:21.167917
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.34
 ---- batch: 020 ----
mean loss: 201.15
 ---- batch: 030 ----
mean loss: 191.75
 ---- batch: 040 ----
mean loss: 194.08
 ---- batch: 050 ----
mean loss: 196.80
 ---- batch: 060 ----
mean loss: 189.16
 ---- batch: 070 ----
mean loss: 193.63
 ---- batch: 080 ----
mean loss: 204.97
 ---- batch: 090 ----
mean loss: 199.43
 ---- batch: 100 ----
mean loss: 196.62
 ---- batch: 110 ----
mean loss: 199.52
train mean loss: 196.54
epoch train time: 0:00:00.728396
elapsed time: 0:03:14.480981
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:36:21.896467
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.40
 ---- batch: 020 ----
mean loss: 199.13
 ---- batch: 030 ----
mean loss: 195.94
 ---- batch: 040 ----
mean loss: 196.81
 ---- batch: 050 ----
mean loss: 202.96
 ---- batch: 060 ----
mean loss: 191.04
 ---- batch: 070 ----
mean loss: 194.62
 ---- batch: 080 ----
mean loss: 198.62
 ---- batch: 090 ----
mean loss: 198.98
 ---- batch: 100 ----
mean loss: 199.56
 ---- batch: 110 ----
mean loss: 196.71
train mean loss: 196.49
epoch train time: 0:00:00.729891
elapsed time: 0:03:15.211047
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:36:22.626541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.31
 ---- batch: 020 ----
mean loss: 192.14
 ---- batch: 030 ----
mean loss: 202.19
 ---- batch: 040 ----
mean loss: 208.03
 ---- batch: 050 ----
mean loss: 190.49
 ---- batch: 060 ----
mean loss: 189.80
 ---- batch: 070 ----
mean loss: 199.26
 ---- batch: 080 ----
mean loss: 202.03
 ---- batch: 090 ----
mean loss: 190.66
 ---- batch: 100 ----
mean loss: 194.58
 ---- batch: 110 ----
mean loss: 200.04
train mean loss: 196.45
epoch train time: 0:00:00.725594
elapsed time: 0:03:15.936795
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:36:23.352271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.43
 ---- batch: 020 ----
mean loss: 201.69
 ---- batch: 030 ----
mean loss: 207.28
 ---- batch: 040 ----
mean loss: 192.90
 ---- batch: 050 ----
mean loss: 194.28
 ---- batch: 060 ----
mean loss: 195.70
 ---- batch: 070 ----
mean loss: 196.96
 ---- batch: 080 ----
mean loss: 189.65
 ---- batch: 090 ----
mean loss: 191.24
 ---- batch: 100 ----
mean loss: 198.56
 ---- batch: 110 ----
mean loss: 193.09
train mean loss: 196.52
epoch train time: 0:00:00.726939
elapsed time: 0:03:16.663866
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:36:24.079341
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.90
 ---- batch: 020 ----
mean loss: 194.39
 ---- batch: 030 ----
mean loss: 192.32
 ---- batch: 040 ----
mean loss: 199.23
 ---- batch: 050 ----
mean loss: 205.18
 ---- batch: 060 ----
mean loss: 192.66
 ---- batch: 070 ----
mean loss: 191.91
 ---- batch: 080 ----
mean loss: 197.13
 ---- batch: 090 ----
mean loss: 200.43
 ---- batch: 100 ----
mean loss: 197.36
 ---- batch: 110 ----
mean loss: 196.46
train mean loss: 196.47
epoch train time: 0:00:00.728432
elapsed time: 0:03:17.392435
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:36:24.807962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.11
 ---- batch: 020 ----
mean loss: 199.20
 ---- batch: 030 ----
mean loss: 196.85
 ---- batch: 040 ----
mean loss: 199.50
 ---- batch: 050 ----
mean loss: 191.93
 ---- batch: 060 ----
mean loss: 207.90
 ---- batch: 070 ----
mean loss: 196.79
 ---- batch: 080 ----
mean loss: 198.56
 ---- batch: 090 ----
mean loss: 201.04
 ---- batch: 100 ----
mean loss: 182.00
 ---- batch: 110 ----
mean loss: 193.84
train mean loss: 196.50
epoch train time: 0:00:00.727256
elapsed time: 0:03:18.119877
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:36:25.535355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.47
 ---- batch: 020 ----
mean loss: 192.87
 ---- batch: 030 ----
mean loss: 201.85
 ---- batch: 040 ----
mean loss: 199.91
 ---- batch: 050 ----
mean loss: 186.63
 ---- batch: 060 ----
mean loss: 199.81
 ---- batch: 070 ----
mean loss: 192.49
 ---- batch: 080 ----
mean loss: 190.51
 ---- batch: 090 ----
mean loss: 200.84
 ---- batch: 100 ----
mean loss: 203.92
 ---- batch: 110 ----
mean loss: 193.89
train mean loss: 196.50
epoch train time: 0:00:00.733966
elapsed time: 0:03:18.853981
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:36:26.269461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.24
 ---- batch: 020 ----
mean loss: 202.57
 ---- batch: 030 ----
mean loss: 196.96
 ---- batch: 040 ----
mean loss: 193.94
 ---- batch: 050 ----
mean loss: 197.64
 ---- batch: 060 ----
mean loss: 185.53
 ---- batch: 070 ----
mean loss: 203.92
 ---- batch: 080 ----
mean loss: 190.44
 ---- batch: 090 ----
mean loss: 201.29
 ---- batch: 100 ----
mean loss: 201.15
 ---- batch: 110 ----
mean loss: 199.27
train mean loss: 196.37
epoch train time: 0:00:00.730927
elapsed time: 0:03:19.585062
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:36:27.000556
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.94
 ---- batch: 020 ----
mean loss: 194.84
 ---- batch: 030 ----
mean loss: 197.89
 ---- batch: 040 ----
mean loss: 192.41
 ---- batch: 050 ----
mean loss: 191.59
 ---- batch: 060 ----
mean loss: 202.02
 ---- batch: 070 ----
mean loss: 203.34
 ---- batch: 080 ----
mean loss: 203.90
 ---- batch: 090 ----
mean loss: 200.35
 ---- batch: 100 ----
mean loss: 199.03
 ---- batch: 110 ----
mean loss: 191.01
train mean loss: 196.38
epoch train time: 0:00:00.727113
elapsed time: 0:03:20.312329
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:36:27.727821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.42
 ---- batch: 020 ----
mean loss: 192.85
 ---- batch: 030 ----
mean loss: 191.66
 ---- batch: 040 ----
mean loss: 200.54
 ---- batch: 050 ----
mean loss: 200.58
 ---- batch: 060 ----
mean loss: 201.65
 ---- batch: 070 ----
mean loss: 202.96
 ---- batch: 080 ----
mean loss: 194.65
 ---- batch: 090 ----
mean loss: 190.14
 ---- batch: 100 ----
mean loss: 195.66
 ---- batch: 110 ----
mean loss: 193.45
train mean loss: 196.46
epoch train time: 0:00:00.736295
elapsed time: 0:03:21.048777
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:36:28.464257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.72
 ---- batch: 020 ----
mean loss: 201.19
 ---- batch: 030 ----
mean loss: 201.78
 ---- batch: 040 ----
mean loss: 198.58
 ---- batch: 050 ----
mean loss: 195.07
 ---- batch: 060 ----
mean loss: 198.66
 ---- batch: 070 ----
mean loss: 193.52
 ---- batch: 080 ----
mean loss: 185.47
 ---- batch: 090 ----
mean loss: 201.00
 ---- batch: 100 ----
mean loss: 201.91
 ---- batch: 110 ----
mean loss: 194.55
train mean loss: 196.37
epoch train time: 0:00:00.733660
elapsed time: 0:03:21.782579
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:36:29.198090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.12
 ---- batch: 020 ----
mean loss: 190.76
 ---- batch: 030 ----
mean loss: 190.67
 ---- batch: 040 ----
mean loss: 208.85
 ---- batch: 050 ----
mean loss: 191.12
 ---- batch: 060 ----
mean loss: 198.76
 ---- batch: 070 ----
mean loss: 198.69
 ---- batch: 080 ----
mean loss: 199.11
 ---- batch: 090 ----
mean loss: 190.93
 ---- batch: 100 ----
mean loss: 188.91
 ---- batch: 110 ----
mean loss: 199.31
train mean loss: 196.46
epoch train time: 0:00:00.725932
elapsed time: 0:03:22.508680
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:36:29.924156
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.98
 ---- batch: 020 ----
mean loss: 186.44
 ---- batch: 030 ----
mean loss: 186.69
 ---- batch: 040 ----
mean loss: 205.91
 ---- batch: 050 ----
mean loss: 200.95
 ---- batch: 060 ----
mean loss: 189.88
 ---- batch: 070 ----
mean loss: 197.79
 ---- batch: 080 ----
mean loss: 204.16
 ---- batch: 090 ----
mean loss: 193.60
 ---- batch: 100 ----
mean loss: 204.59
 ---- batch: 110 ----
mean loss: 198.89
train mean loss: 196.35
epoch train time: 0:00:00.729526
elapsed time: 0:03:23.238350
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:36:30.653829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.84
 ---- batch: 020 ----
mean loss: 191.55
 ---- batch: 030 ----
mean loss: 200.62
 ---- batch: 040 ----
mean loss: 200.85
 ---- batch: 050 ----
mean loss: 186.32
 ---- batch: 060 ----
mean loss: 203.02
 ---- batch: 070 ----
mean loss: 198.54
 ---- batch: 080 ----
mean loss: 202.07
 ---- batch: 090 ----
mean loss: 199.09
 ---- batch: 100 ----
mean loss: 191.69
 ---- batch: 110 ----
mean loss: 193.65
train mean loss: 196.28
epoch train time: 0:00:00.732669
elapsed time: 0:03:23.971158
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:36:31.386654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.12
 ---- batch: 020 ----
mean loss: 190.32
 ---- batch: 030 ----
mean loss: 191.12
 ---- batch: 040 ----
mean loss: 200.99
 ---- batch: 050 ----
mean loss: 197.69
 ---- batch: 060 ----
mean loss: 203.11
 ---- batch: 070 ----
mean loss: 200.22
 ---- batch: 080 ----
mean loss: 196.46
 ---- batch: 090 ----
mean loss: 195.84
 ---- batch: 100 ----
mean loss: 197.78
 ---- batch: 110 ----
mean loss: 194.59
train mean loss: 196.32
epoch train time: 0:00:00.731177
elapsed time: 0:03:24.702506
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:36:32.117975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.29
 ---- batch: 020 ----
mean loss: 192.42
 ---- batch: 030 ----
mean loss: 185.65
 ---- batch: 040 ----
mean loss: 198.14
 ---- batch: 050 ----
mean loss: 191.81
 ---- batch: 060 ----
mean loss: 197.54
 ---- batch: 070 ----
mean loss: 196.50
 ---- batch: 080 ----
mean loss: 204.53
 ---- batch: 090 ----
mean loss: 202.78
 ---- batch: 100 ----
mean loss: 198.48
 ---- batch: 110 ----
mean loss: 193.40
train mean loss: 196.31
epoch train time: 0:00:00.745895
elapsed time: 0:03:25.448527
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:36:32.864004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.47
 ---- batch: 020 ----
mean loss: 190.66
 ---- batch: 030 ----
mean loss: 191.46
 ---- batch: 040 ----
mean loss: 205.96
 ---- batch: 050 ----
mean loss: 192.07
 ---- batch: 060 ----
mean loss: 202.92
 ---- batch: 070 ----
mean loss: 200.23
 ---- batch: 080 ----
mean loss: 188.80
 ---- batch: 090 ----
mean loss: 192.21
 ---- batch: 100 ----
mean loss: 199.87
 ---- batch: 110 ----
mean loss: 192.65
train mean loss: 196.32
epoch train time: 0:00:00.722985
elapsed time: 0:03:26.171655
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:36:33.587132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.26
 ---- batch: 020 ----
mean loss: 200.30
 ---- batch: 030 ----
mean loss: 190.94
 ---- batch: 040 ----
mean loss: 198.19
 ---- batch: 050 ----
mean loss: 198.24
 ---- batch: 060 ----
mean loss: 206.61
 ---- batch: 070 ----
mean loss: 198.83
 ---- batch: 080 ----
mean loss: 191.14
 ---- batch: 090 ----
mean loss: 194.19
 ---- batch: 100 ----
mean loss: 192.00
 ---- batch: 110 ----
mean loss: 194.40
train mean loss: 196.33
epoch train time: 0:00:00.722246
elapsed time: 0:03:26.894035
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:36:34.309548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.18
 ---- batch: 020 ----
mean loss: 205.57
 ---- batch: 030 ----
mean loss: 195.52
 ---- batch: 040 ----
mean loss: 194.63
 ---- batch: 050 ----
mean loss: 195.05
 ---- batch: 060 ----
mean loss: 197.18
 ---- batch: 070 ----
mean loss: 187.41
 ---- batch: 080 ----
mean loss: 193.62
 ---- batch: 090 ----
mean loss: 201.95
 ---- batch: 100 ----
mean loss: 198.56
 ---- batch: 110 ----
mean loss: 190.74
train mean loss: 196.29
epoch train time: 0:00:00.736629
elapsed time: 0:03:27.630845
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:36:35.046356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.25
 ---- batch: 020 ----
mean loss: 201.33
 ---- batch: 030 ----
mean loss: 193.93
 ---- batch: 040 ----
mean loss: 196.22
 ---- batch: 050 ----
mean loss: 193.50
 ---- batch: 060 ----
mean loss: 195.87
 ---- batch: 070 ----
mean loss: 208.26
 ---- batch: 080 ----
mean loss: 191.36
 ---- batch: 090 ----
mean loss: 188.23
 ---- batch: 100 ----
mean loss: 206.23
 ---- batch: 110 ----
mean loss: 191.80
train mean loss: 196.25
epoch train time: 0:00:00.743714
elapsed time: 0:03:28.374746
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:36:35.790222
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.70
 ---- batch: 020 ----
mean loss: 198.64
 ---- batch: 030 ----
mean loss: 198.44
 ---- batch: 040 ----
mean loss: 202.55
 ---- batch: 050 ----
mean loss: 192.08
 ---- batch: 060 ----
mean loss: 203.19
 ---- batch: 070 ----
mean loss: 198.07
 ---- batch: 080 ----
mean loss: 203.46
 ---- batch: 090 ----
mean loss: 189.94
 ---- batch: 100 ----
mean loss: 196.75
 ---- batch: 110 ----
mean loss: 188.01
train mean loss: 196.28
epoch train time: 0:00:00.726887
elapsed time: 0:03:29.101768
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:36:36.517246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.91
 ---- batch: 020 ----
mean loss: 193.99
 ---- batch: 030 ----
mean loss: 198.26
 ---- batch: 040 ----
mean loss: 188.03
 ---- batch: 050 ----
mean loss: 195.13
 ---- batch: 060 ----
mean loss: 198.24
 ---- batch: 070 ----
mean loss: 195.39
 ---- batch: 080 ----
mean loss: 193.52
 ---- batch: 090 ----
mean loss: 209.22
 ---- batch: 100 ----
mean loss: 194.86
 ---- batch: 110 ----
mean loss: 199.59
train mean loss: 196.30
epoch train time: 0:00:00.734764
elapsed time: 0:03:29.836685
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:36:37.252180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.46
 ---- batch: 020 ----
mean loss: 196.99
 ---- batch: 030 ----
mean loss: 194.82
 ---- batch: 040 ----
mean loss: 195.02
 ---- batch: 050 ----
mean loss: 196.25
 ---- batch: 060 ----
mean loss: 202.85
 ---- batch: 070 ----
mean loss: 196.49
 ---- batch: 080 ----
mean loss: 194.38
 ---- batch: 090 ----
mean loss: 199.51
 ---- batch: 100 ----
mean loss: 205.77
 ---- batch: 110 ----
mean loss: 191.21
train mean loss: 196.23
epoch train time: 0:00:00.731881
elapsed time: 0:03:30.568718
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:36:37.984227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.54
 ---- batch: 020 ----
mean loss: 212.94
 ---- batch: 030 ----
mean loss: 201.86
 ---- batch: 040 ----
mean loss: 188.23
 ---- batch: 050 ----
mean loss: 190.85
 ---- batch: 060 ----
mean loss: 198.13
 ---- batch: 070 ----
mean loss: 199.36
 ---- batch: 080 ----
mean loss: 182.16
 ---- batch: 090 ----
mean loss: 192.55
 ---- batch: 100 ----
mean loss: 200.33
 ---- batch: 110 ----
mean loss: 195.54
train mean loss: 196.19
epoch train time: 0:00:00.724327
elapsed time: 0:03:31.293220
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:36:38.708720
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.62
 ---- batch: 020 ----
mean loss: 203.26
 ---- batch: 030 ----
mean loss: 189.57
 ---- batch: 040 ----
mean loss: 204.96
 ---- batch: 050 ----
mean loss: 204.88
 ---- batch: 060 ----
mean loss: 191.51
 ---- batch: 070 ----
mean loss: 200.85
 ---- batch: 080 ----
mean loss: 189.29
 ---- batch: 090 ----
mean loss: 189.66
 ---- batch: 100 ----
mean loss: 199.35
 ---- batch: 110 ----
mean loss: 194.14
train mean loss: 196.20
epoch train time: 0:00:00.729109
elapsed time: 0:03:32.022518
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:36:39.437996
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.48
 ---- batch: 020 ----
mean loss: 198.84
 ---- batch: 030 ----
mean loss: 192.54
 ---- batch: 040 ----
mean loss: 193.80
 ---- batch: 050 ----
mean loss: 197.48
 ---- batch: 060 ----
mean loss: 189.32
 ---- batch: 070 ----
mean loss: 201.55
 ---- batch: 080 ----
mean loss: 204.70
 ---- batch: 090 ----
mean loss: 195.90
 ---- batch: 100 ----
mean loss: 196.04
 ---- batch: 110 ----
mean loss: 201.87
train mean loss: 196.17
epoch train time: 0:00:00.731156
elapsed time: 0:03:32.753825
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:36:40.169315
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.77
 ---- batch: 020 ----
mean loss: 195.59
 ---- batch: 030 ----
mean loss: 199.23
 ---- batch: 040 ----
mean loss: 194.49
 ---- batch: 050 ----
mean loss: 191.54
 ---- batch: 060 ----
mean loss: 199.39
 ---- batch: 070 ----
mean loss: 200.06
 ---- batch: 080 ----
mean loss: 193.37
 ---- batch: 090 ----
mean loss: 194.12
 ---- batch: 100 ----
mean loss: 200.73
 ---- batch: 110 ----
mean loss: 200.64
train mean loss: 196.19
epoch train time: 0:00:00.730513
elapsed time: 0:03:33.484490
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:36:40.899967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.97
 ---- batch: 020 ----
mean loss: 195.41
 ---- batch: 030 ----
mean loss: 195.31
 ---- batch: 040 ----
mean loss: 193.48
 ---- batch: 050 ----
mean loss: 196.50
 ---- batch: 060 ----
mean loss: 197.75
 ---- batch: 070 ----
mean loss: 195.66
 ---- batch: 080 ----
mean loss: 194.81
 ---- batch: 090 ----
mean loss: 193.06
 ---- batch: 100 ----
mean loss: 194.92
 ---- batch: 110 ----
mean loss: 196.79
train mean loss: 196.20
epoch train time: 0:00:00.729912
elapsed time: 0:03:34.214536
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:36:41.630014
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.39
 ---- batch: 020 ----
mean loss: 202.22
 ---- batch: 030 ----
mean loss: 199.46
 ---- batch: 040 ----
mean loss: 196.04
 ---- batch: 050 ----
mean loss: 193.63
 ---- batch: 060 ----
mean loss: 198.21
 ---- batch: 070 ----
mean loss: 196.53
 ---- batch: 080 ----
mean loss: 193.62
 ---- batch: 090 ----
mean loss: 195.50
 ---- batch: 100 ----
mean loss: 202.65
 ---- batch: 110 ----
mean loss: 202.19
train mean loss: 196.14
epoch train time: 0:00:00.731388
elapsed time: 0:03:34.946071
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:36:42.361548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.69
 ---- batch: 020 ----
mean loss: 197.40
 ---- batch: 030 ----
mean loss: 195.19
 ---- batch: 040 ----
mean loss: 193.28
 ---- batch: 050 ----
mean loss: 191.87
 ---- batch: 060 ----
mean loss: 192.07
 ---- batch: 070 ----
mean loss: 193.08
 ---- batch: 080 ----
mean loss: 198.14
 ---- batch: 090 ----
mean loss: 199.05
 ---- batch: 100 ----
mean loss: 203.60
 ---- batch: 110 ----
mean loss: 198.49
train mean loss: 196.06
epoch train time: 0:00:00.724967
elapsed time: 0:03:35.671194
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:36:43.086670
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.01
 ---- batch: 020 ----
mean loss: 198.30
 ---- batch: 030 ----
mean loss: 192.79
 ---- batch: 040 ----
mean loss: 195.03
 ---- batch: 050 ----
mean loss: 191.25
 ---- batch: 060 ----
mean loss: 202.79
 ---- batch: 070 ----
mean loss: 193.14
 ---- batch: 080 ----
mean loss: 198.45
 ---- batch: 090 ----
mean loss: 197.55
 ---- batch: 100 ----
mean loss: 203.50
 ---- batch: 110 ----
mean loss: 194.34
train mean loss: 196.09
epoch train time: 0:00:00.722631
elapsed time: 0:03:36.396248
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_2/checkpoint.pth.tar
**** end time: 2019-09-27 01:36:43.811693 ****
