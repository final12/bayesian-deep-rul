Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 13855
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 12:13:21.807376 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 12:13:21.825963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2362.81
 ---- batch: 020 ----
mean loss: 1479.96
 ---- batch: 030 ----
mean loss: 1256.65
 ---- batch: 040 ----
mean loss: 1159.28
train mean loss: 1491.74
epoch train time: 0:00:23.531245
elapsed time: 0:00:23.558270
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 12:13:45.365702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1083.06
 ---- batch: 020 ----
mean loss: 1075.30
 ---- batch: 030 ----
mean loss: 1076.71
 ---- batch: 040 ----
mean loss: 1007.15
train mean loss: 1051.23
epoch train time: 0:00:08.416893
elapsed time: 0:00:31.976121
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 12:13:53.783795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.21
 ---- batch: 020 ----
mean loss: 1029.14
 ---- batch: 030 ----
mean loss: 1016.66
 ---- batch: 040 ----
mean loss: 1001.05
train mean loss: 1014.87
epoch train time: 0:00:08.433071
elapsed time: 0:00:40.410434
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 12:14:02.218155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.80
 ---- batch: 020 ----
mean loss: 978.94
 ---- batch: 030 ----
mean loss: 953.60
 ---- batch: 040 ----
mean loss: 971.80
train mean loss: 972.39
epoch train time: 0:00:08.396501
elapsed time: 0:00:48.808245
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 12:14:10.615958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 996.74
 ---- batch: 020 ----
mean loss: 967.45
 ---- batch: 030 ----
mean loss: 955.60
 ---- batch: 040 ----
mean loss: 972.72
train mean loss: 972.26
epoch train time: 0:00:08.409012
elapsed time: 0:00:57.218512
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 12:14:19.026196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.34
 ---- batch: 020 ----
mean loss: 958.71
 ---- batch: 030 ----
mean loss: 977.56
 ---- batch: 040 ----
mean loss: 951.42
train mean loss: 962.66
epoch train time: 0:00:08.364773
elapsed time: 0:01:05.584450
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 12:14:27.392161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.23
 ---- batch: 020 ----
mean loss: 960.88
 ---- batch: 030 ----
mean loss: 957.77
 ---- batch: 040 ----
mean loss: 942.22
train mean loss: 952.70
epoch train time: 0:00:08.381715
elapsed time: 0:01:13.967330
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 12:14:35.775025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.70
 ---- batch: 020 ----
mean loss: 945.06
 ---- batch: 030 ----
mean loss: 947.17
 ---- batch: 040 ----
mean loss: 963.23
train mean loss: 948.38
epoch train time: 0:00:08.391160
elapsed time: 0:01:22.359709
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 12:14:44.167397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.67
 ---- batch: 020 ----
mean loss: 954.77
 ---- batch: 030 ----
mean loss: 942.86
 ---- batch: 040 ----
mean loss: 952.22
train mean loss: 947.81
epoch train time: 0:00:08.404635
elapsed time: 0:01:30.765737
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 12:14:52.573486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.00
 ---- batch: 020 ----
mean loss: 953.56
 ---- batch: 030 ----
mean loss: 932.34
 ---- batch: 040 ----
mean loss: 933.54
train mean loss: 942.02
epoch train time: 0:00:08.403434
elapsed time: 0:01:39.170421
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 12:15:00.978113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.30
 ---- batch: 020 ----
mean loss: 938.73
 ---- batch: 030 ----
mean loss: 941.13
 ---- batch: 040 ----
mean loss: 941.60
train mean loss: 938.38
epoch train time: 0:00:08.379860
elapsed time: 0:01:47.551476
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 12:15:09.359209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.37
 ---- batch: 020 ----
mean loss: 947.09
 ---- batch: 030 ----
mean loss: 929.81
 ---- batch: 040 ----
mean loss: 939.69
train mean loss: 930.56
epoch train time: 0:00:08.398355
elapsed time: 0:01:55.951059
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 12:15:17.758698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.26
 ---- batch: 020 ----
mean loss: 926.18
 ---- batch: 030 ----
mean loss: 918.56
 ---- batch: 040 ----
mean loss: 939.32
train mean loss: 932.33
epoch train time: 0:00:08.425283
elapsed time: 0:02:04.377738
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 12:15:26.185447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.69
 ---- batch: 020 ----
mean loss: 924.35
 ---- batch: 030 ----
mean loss: 940.50
 ---- batch: 040 ----
mean loss: 922.32
train mean loss: 929.83
epoch train time: 0:00:08.408787
elapsed time: 0:02:12.787740
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 12:15:34.595427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.38
 ---- batch: 020 ----
mean loss: 918.73
 ---- batch: 030 ----
mean loss: 929.01
 ---- batch: 040 ----
mean loss: 912.01
train mean loss: 921.55
epoch train time: 0:00:08.402085
elapsed time: 0:02:21.191113
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 12:15:42.998823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.73
 ---- batch: 020 ----
mean loss: 921.41
 ---- batch: 030 ----
mean loss: 923.50
 ---- batch: 040 ----
mean loss: 913.45
train mean loss: 915.16
epoch train time: 0:00:08.445700
elapsed time: 0:02:29.638146
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 12:15:51.445850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.55
 ---- batch: 020 ----
mean loss: 916.41
 ---- batch: 030 ----
mean loss: 910.15
 ---- batch: 040 ----
mean loss: 919.98
train mean loss: 909.79
epoch train time: 0:00:08.448911
elapsed time: 0:02:38.088327
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 12:15:59.896020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.15
 ---- batch: 020 ----
mean loss: 915.12
 ---- batch: 030 ----
mean loss: 892.67
 ---- batch: 040 ----
mean loss: 899.39
train mean loss: 901.20
epoch train time: 0:00:08.384755
elapsed time: 0:02:46.474246
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 12:16:08.281985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.87
 ---- batch: 020 ----
mean loss: 902.81
 ---- batch: 030 ----
mean loss: 887.79
 ---- batch: 040 ----
mean loss: 898.18
train mean loss: 894.61
epoch train time: 0:00:08.410284
elapsed time: 0:02:54.885824
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 12:16:16.693587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.14
 ---- batch: 020 ----
mean loss: 881.28
 ---- batch: 030 ----
mean loss: 896.39
 ---- batch: 040 ----
mean loss: 913.68
train mean loss: 892.80
epoch train time: 0:00:08.353665
elapsed time: 0:03:03.240813
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 12:16:25.048467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.24
 ---- batch: 020 ----
mean loss: 909.29
 ---- batch: 030 ----
mean loss: 870.92
 ---- batch: 040 ----
mean loss: 883.03
train mean loss: 883.69
epoch train time: 0:00:08.356138
elapsed time: 0:03:11.598103
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 12:16:33.405812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.03
 ---- batch: 020 ----
mean loss: 865.26
 ---- batch: 030 ----
mean loss: 864.26
 ---- batch: 040 ----
mean loss: 868.35
train mean loss: 869.52
epoch train time: 0:00:08.359993
elapsed time: 0:03:19.959311
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 12:16:41.766905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.91
 ---- batch: 020 ----
mean loss: 848.04
 ---- batch: 030 ----
mean loss: 830.94
 ---- batch: 040 ----
mean loss: 820.62
train mean loss: 834.45
epoch train time: 0:00:08.281355
elapsed time: 0:03:28.241839
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 12:16:50.049551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 799.56
 ---- batch: 020 ----
mean loss: 789.06
 ---- batch: 030 ----
mean loss: 766.05
 ---- batch: 040 ----
mean loss: 755.67
train mean loss: 775.07
epoch train time: 0:00:08.398753
elapsed time: 0:03:36.641847
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 12:16:58.449523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.86
 ---- batch: 020 ----
mean loss: 715.25
 ---- batch: 030 ----
mean loss: 703.52
 ---- batch: 040 ----
mean loss: 718.04
train mean loss: 715.14
epoch train time: 0:00:08.363428
elapsed time: 0:03:45.006452
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 12:17:06.814145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 704.03
 ---- batch: 020 ----
mean loss: 677.81
 ---- batch: 030 ----
mean loss: 694.04
 ---- batch: 040 ----
mean loss: 660.69
train mean loss: 681.49
epoch train time: 0:00:08.344247
elapsed time: 0:03:53.352030
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 12:17:15.159698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.58
 ---- batch: 020 ----
mean loss: 662.15
 ---- batch: 030 ----
mean loss: 653.33
 ---- batch: 040 ----
mean loss: 630.32
train mean loss: 653.16
epoch train time: 0:00:08.366071
elapsed time: 0:04:01.719211
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 12:17:23.526919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 628.47
 ---- batch: 020 ----
mean loss: 631.64
 ---- batch: 030 ----
mean loss: 604.96
 ---- batch: 040 ----
mean loss: 618.74
train mean loss: 621.30
epoch train time: 0:00:08.346167
elapsed time: 0:04:10.066669
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 12:17:31.874349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.67
 ---- batch: 020 ----
mean loss: 592.61
 ---- batch: 030 ----
mean loss: 587.74
 ---- batch: 040 ----
mean loss: 580.81
train mean loss: 588.21
epoch train time: 0:00:08.339665
elapsed time: 0:04:18.407507
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 12:17:40.215338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.03
 ---- batch: 020 ----
mean loss: 564.75
 ---- batch: 030 ----
mean loss: 561.28
 ---- batch: 040 ----
mean loss: 562.68
train mean loss: 564.61
epoch train time: 0:00:08.256576
elapsed time: 0:04:26.665484
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 12:17:48.473198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.10
 ---- batch: 020 ----
mean loss: 541.32
 ---- batch: 030 ----
mean loss: 535.52
 ---- batch: 040 ----
mean loss: 527.49
train mean loss: 532.77
epoch train time: 0:00:08.322247
elapsed time: 0:04:34.988925
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 12:17:56.796601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.04
 ---- batch: 020 ----
mean loss: 511.02
 ---- batch: 030 ----
mean loss: 509.09
 ---- batch: 040 ----
mean loss: 507.06
train mean loss: 513.33
epoch train time: 0:00:08.346854
elapsed time: 0:04:43.336991
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 12:18:05.144666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.63
 ---- batch: 020 ----
mean loss: 490.98
 ---- batch: 030 ----
mean loss: 499.06
 ---- batch: 040 ----
mean loss: 498.05
train mean loss: 497.86
epoch train time: 0:00:08.278267
elapsed time: 0:04:51.616371
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 12:18:13.424031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.52
 ---- batch: 020 ----
mean loss: 493.97
 ---- batch: 030 ----
mean loss: 476.06
 ---- batch: 040 ----
mean loss: 463.22
train mean loss: 477.97
epoch train time: 0:00:08.319340
elapsed time: 0:04:59.936825
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 12:18:21.744506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.65
 ---- batch: 020 ----
mean loss: 470.62
 ---- batch: 030 ----
mean loss: 462.10
 ---- batch: 040 ----
mean loss: 472.32
train mean loss: 466.50
epoch train time: 0:00:08.330336
elapsed time: 0:05:08.268367
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 12:18:30.076072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.41
 ---- batch: 020 ----
mean loss: 458.94
 ---- batch: 030 ----
mean loss: 467.34
 ---- batch: 040 ----
mean loss: 447.97
train mean loss: 454.62
epoch train time: 0:00:08.264311
elapsed time: 0:05:16.533921
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 12:18:38.341676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.31
 ---- batch: 020 ----
mean loss: 436.32
 ---- batch: 030 ----
mean loss: 443.21
 ---- batch: 040 ----
mean loss: 448.41
train mean loss: 441.78
epoch train time: 0:00:08.335924
elapsed time: 0:05:24.871171
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 12:18:46.678917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.97
 ---- batch: 020 ----
mean loss: 433.66
 ---- batch: 030 ----
mean loss: 433.53
 ---- batch: 040 ----
mean loss: 429.40
train mean loss: 434.42
epoch train time: 0:00:08.349513
elapsed time: 0:05:33.221925
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 12:18:55.029580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.77
 ---- batch: 020 ----
mean loss: 415.15
 ---- batch: 030 ----
mean loss: 415.15
 ---- batch: 040 ----
mean loss: 421.16
train mean loss: 421.37
epoch train time: 0:00:08.301941
elapsed time: 0:05:41.525155
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 12:19:03.332876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.19
 ---- batch: 020 ----
mean loss: 416.71
 ---- batch: 030 ----
mean loss: 414.52
 ---- batch: 040 ----
mean loss: 416.62
train mean loss: 417.63
epoch train time: 0:00:08.380772
elapsed time: 0:05:49.907143
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 12:19:11.714840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.02
 ---- batch: 020 ----
mean loss: 402.95
 ---- batch: 030 ----
mean loss: 399.56
 ---- batch: 040 ----
mean loss: 407.43
train mean loss: 406.26
epoch train time: 0:00:08.364289
elapsed time: 0:05:58.272675
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 12:19:20.080336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.89
 ---- batch: 020 ----
mean loss: 415.03
 ---- batch: 030 ----
mean loss: 401.50
 ---- batch: 040 ----
mean loss: 396.48
train mean loss: 402.63
epoch train time: 0:00:08.295555
elapsed time: 0:06:06.569400
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 12:19:28.377112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.91
 ---- batch: 020 ----
mean loss: 400.29
 ---- batch: 030 ----
mean loss: 400.02
 ---- batch: 040 ----
mean loss: 403.27
train mean loss: 400.11
epoch train time: 0:00:08.309419
elapsed time: 0:06:14.880195
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 12:19:36.687887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.22
 ---- batch: 020 ----
mean loss: 395.97
 ---- batch: 030 ----
mean loss: 378.50
 ---- batch: 040 ----
mean loss: 399.98
train mean loss: 389.26
epoch train time: 0:00:08.354408
elapsed time: 0:06:23.235934
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 12:19:45.043544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.56
 ---- batch: 020 ----
mean loss: 382.48
 ---- batch: 030 ----
mean loss: 381.65
 ---- batch: 040 ----
mean loss: 388.89
train mean loss: 385.56
epoch train time: 0:00:08.315580
elapsed time: 0:06:31.552623
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 12:19:53.360301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.75
 ---- batch: 020 ----
mean loss: 386.29
 ---- batch: 030 ----
mean loss: 377.53
 ---- batch: 040 ----
mean loss: 370.00
train mean loss: 378.60
epoch train time: 0:00:08.268787
elapsed time: 0:06:39.822596
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 12:20:01.630377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.04
 ---- batch: 020 ----
mean loss: 378.33
 ---- batch: 030 ----
mean loss: 373.66
 ---- batch: 040 ----
mean loss: 384.17
train mean loss: 376.93
epoch train time: 0:00:08.279976
elapsed time: 0:06:48.103864
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 12:20:09.911535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.74
 ---- batch: 020 ----
mean loss: 369.93
 ---- batch: 030 ----
mean loss: 378.45
 ---- batch: 040 ----
mean loss: 374.52
train mean loss: 372.49
epoch train time: 0:00:08.307875
elapsed time: 0:06:56.412876
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 12:20:18.220561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.11
 ---- batch: 020 ----
mean loss: 362.58
 ---- batch: 030 ----
mean loss: 358.79
 ---- batch: 040 ----
mean loss: 365.87
train mean loss: 365.18
epoch train time: 0:00:08.315205
elapsed time: 0:07:04.729278
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 12:20:26.536992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.03
 ---- batch: 020 ----
mean loss: 364.03
 ---- batch: 030 ----
mean loss: 365.70
 ---- batch: 040 ----
mean loss: 361.22
train mean loss: 357.98
epoch train time: 0:00:08.306185
elapsed time: 0:07:13.036634
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 12:20:34.844279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.20
 ---- batch: 020 ----
mean loss: 356.51
 ---- batch: 030 ----
mean loss: 363.99
 ---- batch: 040 ----
mean loss: 358.40
train mean loss: 355.05
epoch train time: 0:00:08.283852
elapsed time: 0:07:21.321728
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 12:20:43.129461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.67
 ---- batch: 020 ----
mean loss: 358.11
 ---- batch: 030 ----
mean loss: 343.28
 ---- batch: 040 ----
mean loss: 355.23
train mean loss: 349.94
epoch train time: 0:00:08.276093
elapsed time: 0:07:29.599235
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 12:20:51.407015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.33
 ---- batch: 020 ----
mean loss: 347.79
 ---- batch: 030 ----
mean loss: 345.12
 ---- batch: 040 ----
mean loss: 345.28
train mean loss: 344.96
epoch train time: 0:00:08.250727
elapsed time: 0:07:37.851318
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 12:20:59.658994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.31
 ---- batch: 020 ----
mean loss: 337.60
 ---- batch: 030 ----
mean loss: 342.04
 ---- batch: 040 ----
mean loss: 337.11
train mean loss: 340.94
epoch train time: 0:00:08.319598
elapsed time: 0:07:46.172072
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 12:21:07.979751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.95
 ---- batch: 020 ----
mean loss: 341.55
 ---- batch: 030 ----
mean loss: 343.81
 ---- batch: 040 ----
mean loss: 344.56
train mean loss: 342.87
epoch train time: 0:00:08.312681
elapsed time: 0:07:54.485899
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 12:21:16.293560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.92
 ---- batch: 020 ----
mean loss: 327.71
 ---- batch: 030 ----
mean loss: 337.15
 ---- batch: 040 ----
mean loss: 345.05
train mean loss: 335.31
epoch train time: 0:00:08.270187
elapsed time: 0:08:02.757155
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 12:21:24.564839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.07
 ---- batch: 020 ----
mean loss: 335.75
 ---- batch: 030 ----
mean loss: 331.28
 ---- batch: 040 ----
mean loss: 330.02
train mean loss: 332.22
epoch train time: 0:00:08.334412
elapsed time: 0:08:11.092784
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 12:21:32.900471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.14
 ---- batch: 020 ----
mean loss: 332.35
 ---- batch: 030 ----
mean loss: 325.64
 ---- batch: 040 ----
mean loss: 338.29
train mean loss: 331.59
epoch train time: 0:00:08.288359
elapsed time: 0:08:19.382339
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 12:21:41.190043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.28
 ---- batch: 020 ----
mean loss: 322.55
 ---- batch: 030 ----
mean loss: 327.86
 ---- batch: 040 ----
mean loss: 329.17
train mean loss: 327.02
epoch train time: 0:00:08.307813
elapsed time: 0:08:27.691590
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 12:21:49.499266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.27
 ---- batch: 020 ----
mean loss: 327.88
 ---- batch: 030 ----
mean loss: 332.12
 ---- batch: 040 ----
mean loss: 324.57
train mean loss: 326.15
epoch train time: 0:00:08.428045
elapsed time: 0:08:36.120937
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 12:21:57.928612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.69
 ---- batch: 020 ----
mean loss: 329.81
 ---- batch: 030 ----
mean loss: 314.29
 ---- batch: 040 ----
mean loss: 324.73
train mean loss: 320.22
epoch train time: 0:00:08.390554
elapsed time: 0:08:44.512857
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 12:22:06.320560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.86
 ---- batch: 020 ----
mean loss: 314.74
 ---- batch: 030 ----
mean loss: 324.88
 ---- batch: 040 ----
mean loss: 315.23
train mean loss: 317.46
epoch train time: 0:00:08.271570
elapsed time: 0:08:52.785639
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 12:22:14.593326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.65
 ---- batch: 020 ----
mean loss: 319.00
 ---- batch: 030 ----
mean loss: 317.40
 ---- batch: 040 ----
mean loss: 311.62
train mean loss: 314.44
epoch train time: 0:00:08.234420
elapsed time: 0:09:01.021282
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 12:22:22.829034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.46
 ---- batch: 020 ----
mean loss: 307.68
 ---- batch: 030 ----
mean loss: 318.55
 ---- batch: 040 ----
mean loss: 323.73
train mean loss: 312.70
epoch train time: 0:00:08.331714
elapsed time: 0:09:09.354398
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 12:22:31.162097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.85
 ---- batch: 020 ----
mean loss: 309.82
 ---- batch: 030 ----
mean loss: 299.98
 ---- batch: 040 ----
mean loss: 308.08
train mean loss: 309.35
epoch train time: 0:00:08.303675
elapsed time: 0:09:17.659312
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 12:22:39.467029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.70
 ---- batch: 020 ----
mean loss: 305.98
 ---- batch: 030 ----
mean loss: 299.76
 ---- batch: 040 ----
mean loss: 309.72
train mean loss: 305.82
epoch train time: 0:00:08.285781
elapsed time: 0:09:25.946321
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 12:22:47.754037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.63
 ---- batch: 020 ----
mean loss: 311.74
 ---- batch: 030 ----
mean loss: 298.17
 ---- batch: 040 ----
mean loss: 310.43
train mean loss: 305.67
epoch train time: 0:00:08.213980
elapsed time: 0:09:34.161454
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 12:22:55.969131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.54
 ---- batch: 020 ----
mean loss: 309.93
 ---- batch: 030 ----
mean loss: 307.31
 ---- batch: 040 ----
mean loss: 306.96
train mean loss: 304.48
epoch train time: 0:00:08.201234
elapsed time: 0:09:42.363865
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 12:23:04.171555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.94
 ---- batch: 020 ----
mean loss: 308.22
 ---- batch: 030 ----
mean loss: 297.55
 ---- batch: 040 ----
mean loss: 298.14
train mean loss: 299.51
epoch train time: 0:00:08.162789
elapsed time: 0:09:50.527741
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 12:23:12.335422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.42
 ---- batch: 020 ----
mean loss: 307.95
 ---- batch: 030 ----
mean loss: 306.21
 ---- batch: 040 ----
mean loss: 299.92
train mean loss: 301.02
epoch train time: 0:00:08.333134
elapsed time: 0:09:58.862036
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 12:23:20.669750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.69
 ---- batch: 020 ----
mean loss: 294.38
 ---- batch: 030 ----
mean loss: 289.49
 ---- batch: 040 ----
mean loss: 298.00
train mean loss: 292.87
epoch train time: 0:00:08.318970
elapsed time: 0:10:07.182306
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 12:23:28.989984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.01
 ---- batch: 020 ----
mean loss: 290.73
 ---- batch: 030 ----
mean loss: 308.80
 ---- batch: 040 ----
mean loss: 294.57
train mean loss: 293.75
epoch train time: 0:00:08.445922
elapsed time: 0:10:15.629410
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 12:23:37.437115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.11
 ---- batch: 020 ----
mean loss: 293.51
 ---- batch: 030 ----
mean loss: 290.93
 ---- batch: 040 ----
mean loss: 285.68
train mean loss: 292.24
epoch train time: 0:00:08.365372
elapsed time: 0:10:23.996033
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 12:23:45.803698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.59
 ---- batch: 020 ----
mean loss: 286.96
 ---- batch: 030 ----
mean loss: 292.73
 ---- batch: 040 ----
mean loss: 292.02
train mean loss: 291.05
epoch train time: 0:00:08.364689
elapsed time: 0:10:32.361886
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 12:23:54.169594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.84
 ---- batch: 020 ----
mean loss: 291.97
 ---- batch: 030 ----
mean loss: 292.85
 ---- batch: 040 ----
mean loss: 289.73
train mean loss: 291.09
epoch train time: 0:00:08.361855
elapsed time: 0:10:40.724966
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 12:24:02.532730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.27
 ---- batch: 020 ----
mean loss: 281.90
 ---- batch: 030 ----
mean loss: 284.68
 ---- batch: 040 ----
mean loss: 288.91
train mean loss: 286.00
epoch train time: 0:00:08.400869
elapsed time: 0:10:49.127130
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 12:24:10.934823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.39
 ---- batch: 020 ----
mean loss: 277.47
 ---- batch: 030 ----
mean loss: 285.52
 ---- batch: 040 ----
mean loss: 289.04
train mean loss: 286.34
epoch train time: 0:00:08.451155
elapsed time: 0:10:57.579559
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 12:24:19.387279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.09
 ---- batch: 020 ----
mean loss: 284.63
 ---- batch: 030 ----
mean loss: 292.23
 ---- batch: 040 ----
mean loss: 271.20
train mean loss: 280.94
epoch train time: 0:00:08.444211
elapsed time: 0:11:06.024982
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 12:24:27.832749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.11
 ---- batch: 020 ----
mean loss: 289.86
 ---- batch: 030 ----
mean loss: 287.75
 ---- batch: 040 ----
mean loss: 276.37
train mean loss: 283.53
epoch train time: 0:00:08.315631
elapsed time: 0:11:14.341851
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 12:24:36.149554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.04
 ---- batch: 020 ----
mean loss: 281.88
 ---- batch: 030 ----
mean loss: 274.67
 ---- batch: 040 ----
mean loss: 284.25
train mean loss: 278.89
epoch train time: 0:00:08.219569
elapsed time: 0:11:22.562615
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 12:24:44.370318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.69
 ---- batch: 020 ----
mean loss: 279.71
 ---- batch: 030 ----
mean loss: 278.74
 ---- batch: 040 ----
mean loss: 277.45
train mean loss: 280.80
epoch train time: 0:00:08.263229
elapsed time: 0:11:30.827096
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 12:24:52.634771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.75
 ---- batch: 020 ----
mean loss: 276.72
 ---- batch: 030 ----
mean loss: 265.12
 ---- batch: 040 ----
mean loss: 285.24
train mean loss: 276.67
epoch train time: 0:00:08.372704
elapsed time: 0:11:39.201172
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 12:25:01.008905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.54
 ---- batch: 020 ----
mean loss: 277.25
 ---- batch: 030 ----
mean loss: 280.95
 ---- batch: 040 ----
mean loss: 274.69
train mean loss: 276.30
epoch train time: 0:00:08.392254
elapsed time: 0:11:47.594804
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 12:25:09.402485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.90
 ---- batch: 020 ----
mean loss: 278.06
 ---- batch: 030 ----
mean loss: 292.38
 ---- batch: 040 ----
mean loss: 274.81
train mean loss: 281.47
epoch train time: 0:00:08.305881
elapsed time: 0:11:55.901833
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 12:25:17.709530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.82
 ---- batch: 020 ----
mean loss: 273.50
 ---- batch: 030 ----
mean loss: 273.57
 ---- batch: 040 ----
mean loss: 273.33
train mean loss: 271.48
epoch train time: 0:00:08.257829
elapsed time: 0:12:04.160891
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 12:25:25.968570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.33
 ---- batch: 020 ----
mean loss: 269.98
 ---- batch: 030 ----
mean loss: 275.13
 ---- batch: 040 ----
mean loss: 273.00
train mean loss: 273.65
epoch train time: 0:00:08.308829
elapsed time: 0:12:12.470932
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 12:25:34.278598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.25
 ---- batch: 020 ----
mean loss: 272.78
 ---- batch: 030 ----
mean loss: 273.42
 ---- batch: 040 ----
mean loss: 263.07
train mean loss: 270.64
epoch train time: 0:00:08.278967
elapsed time: 0:12:20.751071
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 12:25:42.558755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.22
 ---- batch: 020 ----
mean loss: 267.68
 ---- batch: 030 ----
mean loss: 264.71
 ---- batch: 040 ----
mean loss: 262.32
train mean loss: 268.31
epoch train time: 0:00:08.254082
elapsed time: 0:12:29.006344
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 12:25:50.814042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.65
 ---- batch: 020 ----
mean loss: 268.06
 ---- batch: 030 ----
mean loss: 271.41
 ---- batch: 040 ----
mean loss: 261.03
train mean loss: 267.91
epoch train time: 0:00:08.272016
elapsed time: 0:12:37.279651
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 12:25:59.087389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.78
 ---- batch: 020 ----
mean loss: 260.03
 ---- batch: 030 ----
mean loss: 265.88
 ---- batch: 040 ----
mean loss: 266.44
train mean loss: 265.55
epoch train time: 0:00:08.236917
elapsed time: 0:12:45.517752
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 12:26:07.325437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.01
 ---- batch: 020 ----
mean loss: 265.36
 ---- batch: 030 ----
mean loss: 254.82
 ---- batch: 040 ----
mean loss: 271.23
train mean loss: 265.07
epoch train time: 0:00:08.256061
elapsed time: 0:12:53.774948
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 12:26:15.582654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.22
 ---- batch: 020 ----
mean loss: 271.01
 ---- batch: 030 ----
mean loss: 267.89
 ---- batch: 040 ----
mean loss: 265.41
train mean loss: 264.14
epoch train time: 0:00:08.246389
elapsed time: 0:13:02.022623
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 12:26:23.830283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.06
 ---- batch: 020 ----
mean loss: 257.09
 ---- batch: 030 ----
mean loss: 272.62
 ---- batch: 040 ----
mean loss: 265.20
train mean loss: 264.46
epoch train time: 0:00:08.269925
elapsed time: 0:13:10.293697
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 12:26:32.101389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.61
 ---- batch: 020 ----
mean loss: 262.56
 ---- batch: 030 ----
mean loss: 263.21
 ---- batch: 040 ----
mean loss: 265.12
train mean loss: 262.62
epoch train time: 0:00:08.239079
elapsed time: 0:13:18.534076
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 12:26:40.341771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.46
 ---- batch: 020 ----
mean loss: 267.21
 ---- batch: 030 ----
mean loss: 248.21
 ---- batch: 040 ----
mean loss: 261.51
train mean loss: 260.18
epoch train time: 0:00:08.364908
elapsed time: 0:13:26.900355
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 12:26:48.708045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.68
 ---- batch: 020 ----
mean loss: 254.77
 ---- batch: 030 ----
mean loss: 262.11
 ---- batch: 040 ----
mean loss: 253.52
train mean loss: 259.77
epoch train time: 0:00:08.415223
elapsed time: 0:13:35.316821
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 12:26:57.124624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.44
 ---- batch: 020 ----
mean loss: 253.14
 ---- batch: 030 ----
mean loss: 258.26
 ---- batch: 040 ----
mean loss: 261.09
train mean loss: 260.35
epoch train time: 0:00:08.340144
elapsed time: 0:13:43.658260
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 12:27:05.465934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.33
 ---- batch: 020 ----
mean loss: 263.76
 ---- batch: 030 ----
mean loss: 258.42
 ---- batch: 040 ----
mean loss: 261.02
train mean loss: 259.00
epoch train time: 0:00:08.374383
elapsed time: 0:13:52.033818
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 12:27:13.841518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.66
 ---- batch: 020 ----
mean loss: 254.26
 ---- batch: 030 ----
mean loss: 258.65
 ---- batch: 040 ----
mean loss: 251.36
train mean loss: 254.66
epoch train time: 0:00:08.350549
elapsed time: 0:14:00.385542
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 12:27:22.193253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.90
 ---- batch: 020 ----
mean loss: 254.71
 ---- batch: 030 ----
mean loss: 262.53
 ---- batch: 040 ----
mean loss: 259.58
train mean loss: 254.87
epoch train time: 0:00:08.407205
elapsed time: 0:14:08.793957
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 12:27:30.601644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.29
 ---- batch: 020 ----
mean loss: 251.12
 ---- batch: 030 ----
mean loss: 249.86
 ---- batch: 040 ----
mean loss: 259.09
train mean loss: 253.32
epoch train time: 0:00:08.379450
elapsed time: 0:14:17.174795
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 12:27:38.982486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.74
 ---- batch: 020 ----
mean loss: 263.54
 ---- batch: 030 ----
mean loss: 251.24
 ---- batch: 040 ----
mean loss: 244.38
train mean loss: 253.44
epoch train time: 0:00:08.400128
elapsed time: 0:14:25.576140
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 12:27:47.383841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.09
 ---- batch: 020 ----
mean loss: 255.77
 ---- batch: 030 ----
mean loss: 252.93
 ---- batch: 040 ----
mean loss: 252.46
train mean loss: 254.19
epoch train time: 0:00:08.581375
elapsed time: 0:14:34.158811
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 12:27:55.966487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.74
 ---- batch: 020 ----
mean loss: 257.28
 ---- batch: 030 ----
mean loss: 245.15
 ---- batch: 040 ----
mean loss: 250.75
train mean loss: 250.13
epoch train time: 0:00:08.368312
elapsed time: 0:14:42.528255
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 12:28:04.336018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.74
 ---- batch: 020 ----
mean loss: 255.30
 ---- batch: 030 ----
mean loss: 253.95
 ---- batch: 040 ----
mean loss: 257.77
train mean loss: 253.07
epoch train time: 0:00:08.364661
elapsed time: 0:14:50.894215
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 12:28:12.701947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.46
 ---- batch: 020 ----
mean loss: 253.12
 ---- batch: 030 ----
mean loss: 250.72
 ---- batch: 040 ----
mean loss: 249.13
train mean loss: 251.32
epoch train time: 0:00:08.365545
elapsed time: 0:14:59.260980
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 12:28:21.068726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.26
 ---- batch: 020 ----
mean loss: 252.49
 ---- batch: 030 ----
mean loss: 249.51
 ---- batch: 040 ----
mean loss: 245.73
train mean loss: 248.43
epoch train time: 0:00:08.362495
elapsed time: 0:15:07.625035
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 12:28:29.432555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.81
 ---- batch: 020 ----
mean loss: 243.12
 ---- batch: 030 ----
mean loss: 249.24
 ---- batch: 040 ----
mean loss: 241.22
train mean loss: 246.43
epoch train time: 0:00:08.385406
elapsed time: 0:15:16.011502
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 12:28:37.819204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.77
 ---- batch: 020 ----
mean loss: 243.92
 ---- batch: 030 ----
mean loss: 245.17
 ---- batch: 040 ----
mean loss: 251.43
train mean loss: 246.68
epoch train time: 0:00:08.362587
elapsed time: 0:15:24.375285
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 12:28:46.182973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.72
 ---- batch: 020 ----
mean loss: 244.62
 ---- batch: 030 ----
mean loss: 237.90
 ---- batch: 040 ----
mean loss: 247.20
train mean loss: 245.50
epoch train time: 0:00:08.352976
elapsed time: 0:15:32.729474
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 12:28:54.537165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.57
 ---- batch: 020 ----
mean loss: 250.26
 ---- batch: 030 ----
mean loss: 247.87
 ---- batch: 040 ----
mean loss: 249.72
train mean loss: 246.58
epoch train time: 0:00:08.366819
elapsed time: 0:15:41.097621
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 12:29:02.905318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.63
 ---- batch: 020 ----
mean loss: 241.34
 ---- batch: 030 ----
mean loss: 245.38
 ---- batch: 040 ----
mean loss: 245.02
train mean loss: 245.73
epoch train time: 0:00:08.359098
elapsed time: 0:15:49.457928
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 12:29:11.265640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.35
 ---- batch: 020 ----
mean loss: 246.88
 ---- batch: 030 ----
mean loss: 245.74
 ---- batch: 040 ----
mean loss: 242.17
train mean loss: 245.38
epoch train time: 0:00:08.395556
elapsed time: 0:15:57.854870
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 12:29:19.662643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.61
 ---- batch: 020 ----
mean loss: 237.17
 ---- batch: 030 ----
mean loss: 241.91
 ---- batch: 040 ----
mean loss: 254.07
train mean loss: 243.11
epoch train time: 0:00:08.447186
elapsed time: 0:16:06.303315
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 12:29:28.111016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.78
 ---- batch: 020 ----
mean loss: 246.07
 ---- batch: 030 ----
mean loss: 237.80
 ---- batch: 040 ----
mean loss: 244.51
train mean loss: 241.92
epoch train time: 0:00:08.483862
elapsed time: 0:16:14.788435
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 12:29:36.596122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.17
 ---- batch: 020 ----
mean loss: 251.96
 ---- batch: 030 ----
mean loss: 249.52
 ---- batch: 040 ----
mean loss: 246.52
train mean loss: 247.07
epoch train time: 0:00:08.400155
elapsed time: 0:16:23.189785
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 12:29:44.997463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.55
 ---- batch: 020 ----
mean loss: 241.27
 ---- batch: 030 ----
mean loss: 239.77
 ---- batch: 040 ----
mean loss: 248.75
train mean loss: 245.38
epoch train time: 0:00:08.384840
elapsed time: 0:16:31.575801
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 12:29:53.383535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.99
 ---- batch: 020 ----
mean loss: 237.70
 ---- batch: 030 ----
mean loss: 241.31
 ---- batch: 040 ----
mean loss: 246.91
train mean loss: 243.20
epoch train time: 0:00:08.401222
elapsed time: 0:16:39.978505
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 12:30:01.786197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.41
 ---- batch: 020 ----
mean loss: 244.24
 ---- batch: 030 ----
mean loss: 241.85
 ---- batch: 040 ----
mean loss: 230.81
train mean loss: 238.18
epoch train time: 0:00:08.395291
elapsed time: 0:16:48.375085
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 12:30:10.182797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.56
 ---- batch: 020 ----
mean loss: 239.71
 ---- batch: 030 ----
mean loss: 240.23
 ---- batch: 040 ----
mean loss: 246.25
train mean loss: 240.48
epoch train time: 0:00:08.400773
elapsed time: 0:16:56.777162
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 12:30:18.584830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.94
 ---- batch: 020 ----
mean loss: 236.51
 ---- batch: 030 ----
mean loss: 246.23
 ---- batch: 040 ----
mean loss: 245.87
train mean loss: 241.30
epoch train time: 0:00:08.404661
elapsed time: 0:17:05.183165
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 12:30:26.990848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.72
 ---- batch: 020 ----
mean loss: 235.16
 ---- batch: 030 ----
mean loss: 238.45
 ---- batch: 040 ----
mean loss: 239.88
train mean loss: 237.33
epoch train time: 0:00:08.387134
elapsed time: 0:17:13.571511
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 12:30:35.379193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.09
 ---- batch: 020 ----
mean loss: 240.54
 ---- batch: 030 ----
mean loss: 241.07
 ---- batch: 040 ----
mean loss: 232.01
train mean loss: 237.90
epoch train time: 0:00:08.446618
elapsed time: 0:17:22.019441
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 12:30:43.827153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.03
 ---- batch: 020 ----
mean loss: 230.07
 ---- batch: 030 ----
mean loss: 238.39
 ---- batch: 040 ----
mean loss: 239.59
train mean loss: 236.22
epoch train time: 0:00:08.386232
elapsed time: 0:17:30.406906
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 12:30:52.214599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.05
 ---- batch: 020 ----
mean loss: 237.66
 ---- batch: 030 ----
mean loss: 229.21
 ---- batch: 040 ----
mean loss: 241.97
train mean loss: 234.91
epoch train time: 0:00:08.410070
elapsed time: 0:17:38.818163
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 12:31:00.625877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.61
 ---- batch: 020 ----
mean loss: 230.34
 ---- batch: 030 ----
mean loss: 233.59
 ---- batch: 040 ----
mean loss: 242.47
train mean loss: 235.80
epoch train time: 0:00:08.412903
elapsed time: 0:17:47.232335
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 12:31:09.040025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.86
 ---- batch: 020 ----
mean loss: 235.34
 ---- batch: 030 ----
mean loss: 242.69
 ---- batch: 040 ----
mean loss: 225.24
train mean loss: 234.38
epoch train time: 0:00:08.416901
elapsed time: 0:17:55.650657
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 12:31:17.458123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.06
 ---- batch: 020 ----
mean loss: 244.69
 ---- batch: 030 ----
mean loss: 232.76
 ---- batch: 040 ----
mean loss: 239.01
train mean loss: 238.42
epoch train time: 0:00:08.403140
elapsed time: 0:18:04.054803
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 12:31:25.862456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.43
 ---- batch: 020 ----
mean loss: 232.78
 ---- batch: 030 ----
mean loss: 228.42
 ---- batch: 040 ----
mean loss: 235.17
train mean loss: 234.41
epoch train time: 0:00:08.413966
elapsed time: 0:18:12.469926
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 12:31:34.277639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.67
 ---- batch: 020 ----
mean loss: 235.86
 ---- batch: 030 ----
mean loss: 227.43
 ---- batch: 040 ----
mean loss: 233.07
train mean loss: 232.52
epoch train time: 0:00:08.416127
elapsed time: 0:18:20.887306
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 12:31:42.694972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.46
 ---- batch: 020 ----
mean loss: 230.71
 ---- batch: 030 ----
mean loss: 229.54
 ---- batch: 040 ----
mean loss: 232.73
train mean loss: 233.89
epoch train time: 0:00:08.429951
elapsed time: 0:18:29.318488
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 12:31:51.126214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.41
 ---- batch: 020 ----
mean loss: 229.29
 ---- batch: 030 ----
mean loss: 238.11
 ---- batch: 040 ----
mean loss: 225.56
train mean loss: 231.72
epoch train time: 0:00:08.440981
elapsed time: 0:18:37.760702
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 12:31:59.568415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.44
 ---- batch: 020 ----
mean loss: 232.66
 ---- batch: 030 ----
mean loss: 228.94
 ---- batch: 040 ----
mean loss: 239.43
train mean loss: 231.09
epoch train time: 0:00:08.420198
elapsed time: 0:18:46.182208
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 12:32:07.989928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.81
 ---- batch: 020 ----
mean loss: 226.98
 ---- batch: 030 ----
mean loss: 225.33
 ---- batch: 040 ----
mean loss: 228.75
train mean loss: 230.36
epoch train time: 0:00:08.413873
elapsed time: 0:18:54.597444
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 12:32:16.405163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.90
 ---- batch: 020 ----
mean loss: 229.48
 ---- batch: 030 ----
mean loss: 222.99
 ---- batch: 040 ----
mean loss: 234.00
train mean loss: 230.34
epoch train time: 0:00:08.421943
elapsed time: 0:19:03.020637
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 12:32:24.828361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.98
 ---- batch: 020 ----
mean loss: 226.08
 ---- batch: 030 ----
mean loss: 237.34
 ---- batch: 040 ----
mean loss: 232.53
train mean loss: 229.14
epoch train time: 0:00:08.424048
elapsed time: 0:19:11.445900
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 12:32:33.253599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.50
 ---- batch: 020 ----
mean loss: 231.90
 ---- batch: 030 ----
mean loss: 224.99
 ---- batch: 040 ----
mean loss: 225.85
train mean loss: 228.22
epoch train time: 0:00:08.412038
elapsed time: 0:19:19.859167
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 12:32:41.666899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.48
 ---- batch: 020 ----
mean loss: 238.32
 ---- batch: 030 ----
mean loss: 233.95
 ---- batch: 040 ----
mean loss: 226.88
train mean loss: 231.43
epoch train time: 0:00:08.404993
elapsed time: 0:19:28.265408
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 12:32:50.073087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.62
 ---- batch: 020 ----
mean loss: 229.35
 ---- batch: 030 ----
mean loss: 232.61
 ---- batch: 040 ----
mean loss: 227.37
train mean loss: 228.57
epoch train time: 0:00:08.394280
elapsed time: 0:19:36.660908
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 12:32:58.468625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.43
 ---- batch: 020 ----
mean loss: 230.35
 ---- batch: 030 ----
mean loss: 224.63
 ---- batch: 040 ----
mean loss: 236.73
train mean loss: 230.17
epoch train time: 0:00:08.441077
elapsed time: 0:19:45.103357
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 12:33:06.911078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.07
 ---- batch: 020 ----
mean loss: 226.18
 ---- batch: 030 ----
mean loss: 225.02
 ---- batch: 040 ----
mean loss: 232.94
train mean loss: 228.87
epoch train time: 0:00:08.444637
elapsed time: 0:19:53.549231
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 12:33:15.356952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.23
 ---- batch: 020 ----
mean loss: 222.51
 ---- batch: 030 ----
mean loss: 223.60
 ---- batch: 040 ----
mean loss: 228.53
train mean loss: 226.85
epoch train time: 0:00:08.430761
elapsed time: 0:20:01.981264
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 12:33:23.788973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.72
 ---- batch: 020 ----
mean loss: 229.46
 ---- batch: 030 ----
mean loss: 226.05
 ---- batch: 040 ----
mean loss: 232.76
train mean loss: 228.30
epoch train time: 0:00:08.440196
elapsed time: 0:20:10.422782
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 12:33:32.230467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.82
 ---- batch: 020 ----
mean loss: 218.67
 ---- batch: 030 ----
mean loss: 226.69
 ---- batch: 040 ----
mean loss: 225.35
train mean loss: 224.09
epoch train time: 0:00:08.454434
elapsed time: 0:20:18.878497
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 12:33:40.686202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.50
 ---- batch: 020 ----
mean loss: 222.31
 ---- batch: 030 ----
mean loss: 221.01
 ---- batch: 040 ----
mean loss: 235.38
train mean loss: 225.90
epoch train time: 0:00:08.410002
elapsed time: 0:20:27.289691
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 12:33:49.097356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.27
 ---- batch: 020 ----
mean loss: 217.02
 ---- batch: 030 ----
mean loss: 232.86
 ---- batch: 040 ----
mean loss: 222.32
train mean loss: 226.00
epoch train time: 0:00:08.402649
elapsed time: 0:20:35.693525
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 12:33:57.501220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.81
 ---- batch: 020 ----
mean loss: 234.04
 ---- batch: 030 ----
mean loss: 226.65
 ---- batch: 040 ----
mean loss: 217.17
train mean loss: 225.94
epoch train time: 0:00:08.400933
elapsed time: 0:20:44.095663
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 12:34:05.903449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.22
 ---- batch: 020 ----
mean loss: 226.12
 ---- batch: 030 ----
mean loss: 222.93
 ---- batch: 040 ----
mean loss: 233.41
train mean loss: 224.44
epoch train time: 0:00:08.408014
elapsed time: 0:20:52.505029
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 12:34:14.312700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.73
 ---- batch: 020 ----
mean loss: 216.99
 ---- batch: 030 ----
mean loss: 225.46
 ---- batch: 040 ----
mean loss: 223.48
train mean loss: 223.60
epoch train time: 0:00:08.433867
elapsed time: 0:21:00.940436
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 12:34:22.747904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.84
 ---- batch: 020 ----
mean loss: 225.39
 ---- batch: 030 ----
mean loss: 216.89
 ---- batch: 040 ----
mean loss: 226.59
train mean loss: 223.17
epoch train time: 0:00:08.463699
elapsed time: 0:21:09.405089
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 12:34:31.212825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.16
 ---- batch: 020 ----
mean loss: 229.87
 ---- batch: 030 ----
mean loss: 225.01
 ---- batch: 040 ----
mean loss: 216.73
train mean loss: 223.53
epoch train time: 0:00:08.491026
elapsed time: 0:21:17.897451
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 12:34:39.705149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.69
 ---- batch: 020 ----
mean loss: 217.87
 ---- batch: 030 ----
mean loss: 221.70
 ---- batch: 040 ----
mean loss: 224.24
train mean loss: 225.48
epoch train time: 0:00:08.443040
elapsed time: 0:21:26.341720
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 12:34:48.149425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.75
 ---- batch: 020 ----
mean loss: 225.97
 ---- batch: 030 ----
mean loss: 220.55
 ---- batch: 040 ----
mean loss: 227.50
train mean loss: 223.55
epoch train time: 0:00:08.430806
elapsed time: 0:21:34.773745
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 12:34:56.581441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.60
 ---- batch: 020 ----
mean loss: 224.51
 ---- batch: 030 ----
mean loss: 223.95
 ---- batch: 040 ----
mean loss: 214.75
train mean loss: 222.45
epoch train time: 0:00:08.449929
elapsed time: 0:21:43.224895
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 12:35:05.032648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.66
 ---- batch: 020 ----
mean loss: 233.19
 ---- batch: 030 ----
mean loss: 220.63
 ---- batch: 040 ----
mean loss: 212.59
train mean loss: 221.68
epoch train time: 0:00:08.434330
elapsed time: 0:21:51.660612
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 12:35:13.468373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.73
 ---- batch: 020 ----
mean loss: 220.24
 ---- batch: 030 ----
mean loss: 218.62
 ---- batch: 040 ----
mean loss: 226.64
train mean loss: 220.62
epoch train time: 0:00:08.435727
elapsed time: 0:22:00.097649
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 12:35:21.905336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.66
 ---- batch: 020 ----
mean loss: 217.20
 ---- batch: 030 ----
mean loss: 218.62
 ---- batch: 040 ----
mean loss: 216.37
train mean loss: 219.22
epoch train time: 0:00:08.441312
elapsed time: 0:22:08.540202
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 12:35:30.347951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.21
 ---- batch: 020 ----
mean loss: 217.60
 ---- batch: 030 ----
mean loss: 216.36
 ---- batch: 040 ----
mean loss: 223.01
train mean loss: 221.26
epoch train time: 0:00:08.448363
elapsed time: 0:22:16.989878
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 12:35:38.797594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.22
 ---- batch: 020 ----
mean loss: 215.86
 ---- batch: 030 ----
mean loss: 212.67
 ---- batch: 040 ----
mean loss: 223.65
train mean loss: 217.97
epoch train time: 0:00:08.454036
elapsed time: 0:22:25.445188
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 12:35:47.252912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.07
 ---- batch: 020 ----
mean loss: 221.66
 ---- batch: 030 ----
mean loss: 220.54
 ---- batch: 040 ----
mean loss: 212.59
train mean loss: 219.04
epoch train time: 0:00:08.431647
elapsed time: 0:22:33.878105
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 12:35:55.685904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.69
 ---- batch: 020 ----
mean loss: 221.07
 ---- batch: 030 ----
mean loss: 219.05
 ---- batch: 040 ----
mean loss: 219.06
train mean loss: 220.67
epoch train time: 0:00:08.461150
elapsed time: 0:22:42.340549
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 12:36:04.148244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.64
 ---- batch: 020 ----
mean loss: 217.64
 ---- batch: 030 ----
mean loss: 220.94
 ---- batch: 040 ----
mean loss: 221.22
train mean loss: 218.70
epoch train time: 0:00:08.431646
elapsed time: 0:22:50.773436
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 12:36:12.581135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.74
 ---- batch: 020 ----
mean loss: 210.27
 ---- batch: 030 ----
mean loss: 220.66
 ---- batch: 040 ----
mean loss: 222.52
train mean loss: 217.29
epoch train time: 0:00:08.435579
elapsed time: 0:22:59.210278
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 12:36:21.017973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.46
 ---- batch: 020 ----
mean loss: 220.51
 ---- batch: 030 ----
mean loss: 224.53
 ---- batch: 040 ----
mean loss: 223.52
train mean loss: 221.41
epoch train time: 0:00:08.424189
elapsed time: 0:23:07.635754
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 12:36:29.443509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.93
 ---- batch: 020 ----
mean loss: 222.79
 ---- batch: 030 ----
mean loss: 215.91
 ---- batch: 040 ----
mean loss: 220.04
train mean loss: 219.34
epoch train time: 0:00:08.434485
elapsed time: 0:23:16.071602
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 12:36:37.879378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.73
 ---- batch: 020 ----
mean loss: 210.92
 ---- batch: 030 ----
mean loss: 215.19
 ---- batch: 040 ----
mean loss: 224.96
train mean loss: 216.95
epoch train time: 0:00:08.449972
elapsed time: 0:23:24.522873
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 12:36:46.330565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.15
 ---- batch: 020 ----
mean loss: 217.64
 ---- batch: 030 ----
mean loss: 217.82
 ---- batch: 040 ----
mean loss: 214.07
train mean loss: 216.49
epoch train time: 0:00:08.447301
elapsed time: 0:23:32.971402
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 12:36:54.779080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.56
 ---- batch: 020 ----
mean loss: 218.96
 ---- batch: 030 ----
mean loss: 210.02
 ---- batch: 040 ----
mean loss: 214.79
train mean loss: 215.99
epoch train time: 0:00:08.450825
elapsed time: 0:23:41.423472
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 12:37:03.231194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.59
 ---- batch: 020 ----
mean loss: 218.46
 ---- batch: 030 ----
mean loss: 224.23
 ---- batch: 040 ----
mean loss: 211.66
train mean loss: 218.57
epoch train time: 0:00:08.459856
elapsed time: 0:23:49.884663
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 12:37:11.692349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.26
 ---- batch: 020 ----
mean loss: 210.45
 ---- batch: 030 ----
mean loss: 221.04
 ---- batch: 040 ----
mean loss: 219.85
train mean loss: 216.82
epoch train time: 0:00:08.439396
elapsed time: 0:23:58.325258
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 12:37:20.132946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.37
 ---- batch: 020 ----
mean loss: 216.19
 ---- batch: 030 ----
mean loss: 205.75
 ---- batch: 040 ----
mean loss: 225.45
train mean loss: 216.80
epoch train time: 0:00:08.469461
elapsed time: 0:24:06.795929
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 12:37:28.603640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.80
 ---- batch: 020 ----
mean loss: 211.17
 ---- batch: 030 ----
mean loss: 213.96
 ---- batch: 040 ----
mean loss: 219.62
train mean loss: 214.11
epoch train time: 0:00:08.470380
elapsed time: 0:24:15.267629
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 12:37:37.075367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.52
 ---- batch: 020 ----
mean loss: 214.19
 ---- batch: 030 ----
mean loss: 219.19
 ---- batch: 040 ----
mean loss: 218.08
train mean loss: 215.73
epoch train time: 0:00:08.449605
elapsed time: 0:24:23.718526
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 12:37:45.526294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.50
 ---- batch: 020 ----
mean loss: 217.87
 ---- batch: 030 ----
mean loss: 211.97
 ---- batch: 040 ----
mean loss: 211.92
train mean loss: 216.12
epoch train time: 0:00:08.462203
elapsed time: 0:24:32.182337
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 12:37:53.989812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.78
 ---- batch: 020 ----
mean loss: 221.36
 ---- batch: 030 ----
mean loss: 212.55
 ---- batch: 040 ----
mean loss: 221.52
train mean loss: 216.18
epoch train time: 0:00:08.488887
elapsed time: 0:24:40.672236
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 12:38:02.479908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.61
 ---- batch: 020 ----
mean loss: 208.91
 ---- batch: 030 ----
mean loss: 216.70
 ---- batch: 040 ----
mean loss: 223.20
train mean loss: 214.36
epoch train time: 0:00:08.449873
elapsed time: 0:24:49.123261
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 12:38:10.930948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.55
 ---- batch: 020 ----
mean loss: 214.40
 ---- batch: 030 ----
mean loss: 212.99
 ---- batch: 040 ----
mean loss: 213.76
train mean loss: 214.37
epoch train time: 0:00:08.455742
elapsed time: 0:24:57.580242
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 12:38:19.387921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.84
 ---- batch: 020 ----
mean loss: 211.35
 ---- batch: 030 ----
mean loss: 207.31
 ---- batch: 040 ----
mean loss: 215.91
train mean loss: 213.52
epoch train time: 0:00:08.419305
elapsed time: 0:25:06.000822
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 12:38:27.808546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.55
 ---- batch: 020 ----
mean loss: 211.52
 ---- batch: 030 ----
mean loss: 212.65
 ---- batch: 040 ----
mean loss: 209.85
train mean loss: 213.27
epoch train time: 0:00:08.455233
elapsed time: 0:25:14.457422
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 12:38:36.265108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.04
 ---- batch: 020 ----
mean loss: 208.19
 ---- batch: 030 ----
mean loss: 214.45
 ---- batch: 040 ----
mean loss: 218.06
train mean loss: 212.66
epoch train time: 0:00:08.455228
elapsed time: 0:25:22.913922
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 12:38:44.721658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.31
 ---- batch: 020 ----
mean loss: 217.96
 ---- batch: 030 ----
mean loss: 218.14
 ---- batch: 040 ----
mean loss: 207.52
train mean loss: 213.47
epoch train time: 0:00:08.468386
elapsed time: 0:25:31.383509
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 12:38:53.191112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.51
 ---- batch: 020 ----
mean loss: 207.48
 ---- batch: 030 ----
mean loss: 213.84
 ---- batch: 040 ----
mean loss: 211.88
train mean loss: 211.57
epoch train time: 0:00:08.462222
elapsed time: 0:25:39.846893
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 12:39:01.654603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.02
 ---- batch: 020 ----
mean loss: 210.53
 ---- batch: 030 ----
mean loss: 209.13
 ---- batch: 040 ----
mean loss: 211.49
train mean loss: 211.55
epoch train time: 0:00:08.464640
elapsed time: 0:25:48.312736
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 12:39:10.120457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.68
 ---- batch: 020 ----
mean loss: 215.61
 ---- batch: 030 ----
mean loss: 214.14
 ---- batch: 040 ----
mean loss: 205.47
train mean loss: 209.79
epoch train time: 0:00:08.445301
elapsed time: 0:25:56.759340
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 12:39:18.567031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.95
 ---- batch: 020 ----
mean loss: 218.35
 ---- batch: 030 ----
mean loss: 211.80
 ---- batch: 040 ----
mean loss: 212.79
train mean loss: 212.79
epoch train time: 0:00:08.440269
elapsed time: 0:26:05.200803
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 12:39:27.008498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.11
 ---- batch: 020 ----
mean loss: 213.51
 ---- batch: 030 ----
mean loss: 212.79
 ---- batch: 040 ----
mean loss: 208.75
train mean loss: 212.79
epoch train time: 0:00:08.468283
elapsed time: 0:26:13.670388
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 12:39:35.478087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.77
 ---- batch: 020 ----
mean loss: 225.11
 ---- batch: 030 ----
mean loss: 212.61
 ---- batch: 040 ----
mean loss: 214.17
train mean loss: 214.52
epoch train time: 0:00:08.449097
elapsed time: 0:26:22.120781
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 12:39:43.928557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.57
 ---- batch: 020 ----
mean loss: 211.39
 ---- batch: 030 ----
mean loss: 213.46
 ---- batch: 040 ----
mean loss: 204.74
train mean loss: 210.17
epoch train time: 0:00:08.526197
elapsed time: 0:26:30.648328
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 12:39:52.456159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.09
 ---- batch: 020 ----
mean loss: 211.40
 ---- batch: 030 ----
mean loss: 208.66
 ---- batch: 040 ----
mean loss: 209.13
train mean loss: 210.27
epoch train time: 0:00:08.511105
elapsed time: 0:26:39.160772
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 12:40:00.968495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.83
 ---- batch: 020 ----
mean loss: 215.57
 ---- batch: 030 ----
mean loss: 213.18
 ---- batch: 040 ----
mean loss: 211.05
train mean loss: 209.57
epoch train time: 0:00:08.478509
elapsed time: 0:26:47.640515
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 12:40:09.448242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.49
 ---- batch: 020 ----
mean loss: 213.10
 ---- batch: 030 ----
mean loss: 207.85
 ---- batch: 040 ----
mean loss: 204.77
train mean loss: 210.15
epoch train time: 0:00:08.486680
elapsed time: 0:26:56.128508
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 12:40:17.936201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.42
 ---- batch: 020 ----
mean loss: 213.39
 ---- batch: 030 ----
mean loss: 213.70
 ---- batch: 040 ----
mean loss: 208.02
train mean loss: 210.34
epoch train time: 0:00:08.482733
elapsed time: 0:27:04.612592
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 12:40:26.420268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.09
 ---- batch: 020 ----
mean loss: 213.33
 ---- batch: 030 ----
mean loss: 213.22
 ---- batch: 040 ----
mean loss: 206.02
train mean loss: 209.96
epoch train time: 0:00:08.471923
elapsed time: 0:27:13.085831
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 12:40:34.893553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.10
 ---- batch: 020 ----
mean loss: 204.79
 ---- batch: 030 ----
mean loss: 205.55
 ---- batch: 040 ----
mean loss: 211.52
train mean loss: 210.20
epoch train time: 0:00:08.461654
elapsed time: 0:27:21.548842
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 12:40:43.356486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.74
 ---- batch: 020 ----
mean loss: 213.37
 ---- batch: 030 ----
mean loss: 212.22
 ---- batch: 040 ----
mean loss: 207.85
train mean loss: 210.82
epoch train time: 0:00:08.534372
elapsed time: 0:27:30.084536
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 12:40:51.892307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.83
 ---- batch: 020 ----
mean loss: 202.86
 ---- batch: 030 ----
mean loss: 207.81
 ---- batch: 040 ----
mean loss: 216.82
train mean loss: 209.86
epoch train time: 0:00:08.476652
elapsed time: 0:27:38.562499
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 12:41:00.370230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.64
 ---- batch: 020 ----
mean loss: 210.32
 ---- batch: 030 ----
mean loss: 206.47
 ---- batch: 040 ----
mean loss: 207.83
train mean loss: 209.17
epoch train time: 0:00:08.480458
elapsed time: 0:27:47.044243
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 12:41:08.851945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.04
 ---- batch: 020 ----
mean loss: 209.66
 ---- batch: 030 ----
mean loss: 207.07
 ---- batch: 040 ----
mean loss: 216.97
train mean loss: 209.47
epoch train time: 0:00:08.472422
elapsed time: 0:27:55.518080
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 12:41:17.325819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.47
 ---- batch: 020 ----
mean loss: 217.43
 ---- batch: 030 ----
mean loss: 205.18
 ---- batch: 040 ----
mean loss: 207.58
train mean loss: 208.26
epoch train time: 0:00:08.475484
elapsed time: 0:28:03.994905
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 12:41:25.802586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.90
 ---- batch: 020 ----
mean loss: 206.11
 ---- batch: 030 ----
mean loss: 212.09
 ---- batch: 040 ----
mean loss: 208.73
train mean loss: 208.45
epoch train time: 0:00:08.479576
elapsed time: 0:28:12.475831
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 12:41:34.283524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.53
 ---- batch: 020 ----
mean loss: 218.48
 ---- batch: 030 ----
mean loss: 205.98
 ---- batch: 040 ----
mean loss: 213.55
train mean loss: 209.85
epoch train time: 0:00:08.486967
elapsed time: 0:28:20.964049
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 12:41:42.771771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.08
 ---- batch: 020 ----
mean loss: 205.85
 ---- batch: 030 ----
mean loss: 207.58
 ---- batch: 040 ----
mean loss: 210.21
train mean loss: 206.19
epoch train time: 0:00:08.510531
elapsed time: 0:28:29.476217
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 12:41:51.283647
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.93
 ---- batch: 020 ----
mean loss: 200.12
 ---- batch: 030 ----
mean loss: 207.79
 ---- batch: 040 ----
mean loss: 206.59
train mean loss: 205.33
epoch train time: 0:00:08.525120
elapsed time: 0:28:38.002342
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 12:41:59.810103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.92
 ---- batch: 020 ----
mean loss: 202.77
 ---- batch: 030 ----
mean loss: 208.62
 ---- batch: 040 ----
mean loss: 197.45
train mean loss: 205.91
epoch train time: 0:00:08.526454
elapsed time: 0:28:46.530029
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 12:42:08.337740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.15
 ---- batch: 020 ----
mean loss: 203.15
 ---- batch: 030 ----
mean loss: 208.68
 ---- batch: 040 ----
mean loss: 209.38
train mean loss: 205.63
epoch train time: 0:00:08.492364
elapsed time: 0:28:55.023840
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 12:42:16.831528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.95
 ---- batch: 020 ----
mean loss: 205.64
 ---- batch: 030 ----
mean loss: 200.13
 ---- batch: 040 ----
mean loss: 206.45
train mean loss: 205.75
epoch train time: 0:00:08.489008
elapsed time: 0:29:03.514169
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 12:42:25.321921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.77
 ---- batch: 020 ----
mean loss: 201.96
 ---- batch: 030 ----
mean loss: 203.77
 ---- batch: 040 ----
mean loss: 204.69
train mean loss: 205.31
epoch train time: 0:00:08.497161
elapsed time: 0:29:12.012634
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 12:42:33.820347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.89
 ---- batch: 020 ----
mean loss: 199.28
 ---- batch: 030 ----
mean loss: 202.25
 ---- batch: 040 ----
mean loss: 205.36
train mean loss: 204.53
epoch train time: 0:00:08.487188
elapsed time: 0:29:20.501033
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 12:42:42.308706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.70
 ---- batch: 020 ----
mean loss: 209.71
 ---- batch: 030 ----
mean loss: 204.55
 ---- batch: 040 ----
mean loss: 205.04
train mean loss: 205.29
epoch train time: 0:00:08.473769
elapsed time: 0:29:28.976006
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 12:42:50.783725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.98
 ---- batch: 020 ----
mean loss: 206.00
 ---- batch: 030 ----
mean loss: 203.95
 ---- batch: 040 ----
mean loss: 210.90
train mean loss: 205.80
epoch train time: 0:00:08.477157
elapsed time: 0:29:37.454435
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 12:42:59.262122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.14
 ---- batch: 020 ----
mean loss: 212.51
 ---- batch: 030 ----
mean loss: 206.90
 ---- batch: 040 ----
mean loss: 201.68
train mean loss: 205.79
epoch train time: 0:00:08.482839
elapsed time: 0:29:45.938750
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 12:43:07.746562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.91
 ---- batch: 020 ----
mean loss: 209.42
 ---- batch: 030 ----
mean loss: 207.78
 ---- batch: 040 ----
mean loss: 196.24
train mean loss: 204.79
epoch train time: 0:00:08.474413
elapsed time: 0:29:54.414490
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 12:43:16.222186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.94
 ---- batch: 020 ----
mean loss: 197.92
 ---- batch: 030 ----
mean loss: 207.11
 ---- batch: 040 ----
mean loss: 201.84
train mean loss: 205.17
epoch train time: 0:00:08.482469
elapsed time: 0:30:02.898196
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 12:43:24.705902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.47
 ---- batch: 020 ----
mean loss: 204.94
 ---- batch: 030 ----
mean loss: 202.79
 ---- batch: 040 ----
mean loss: 203.14
train mean loss: 205.26
epoch train time: 0:00:08.483588
elapsed time: 0:30:11.383073
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 12:43:33.190737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.31
 ---- batch: 020 ----
mean loss: 206.50
 ---- batch: 030 ----
mean loss: 201.16
 ---- batch: 040 ----
mean loss: 210.11
train mean loss: 205.62
epoch train time: 0:00:08.469292
elapsed time: 0:30:19.853653
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 12:43:41.661398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.88
 ---- batch: 020 ----
mean loss: 200.54
 ---- batch: 030 ----
mean loss: 206.20
 ---- batch: 040 ----
mean loss: 207.11
train mean loss: 205.01
epoch train time: 0:00:08.487984
elapsed time: 0:30:28.343054
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 12:43:50.150767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.16
 ---- batch: 020 ----
mean loss: 200.80
 ---- batch: 030 ----
mean loss: 204.37
 ---- batch: 040 ----
mean loss: 202.87
train mean loss: 205.00
epoch train time: 0:00:08.486049
elapsed time: 0:30:36.830418
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 12:43:58.638097
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.83
 ---- batch: 020 ----
mean loss: 201.69
 ---- batch: 030 ----
mean loss: 205.46
 ---- batch: 040 ----
mean loss: 209.96
train mean loss: 205.49
epoch train time: 0:00:08.492305
elapsed time: 0:30:45.323929
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 12:44:07.131632
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.45
 ---- batch: 020 ----
mean loss: 205.77
 ---- batch: 030 ----
mean loss: 208.14
 ---- batch: 040 ----
mean loss: 204.44
train mean loss: 205.44
epoch train time: 0:00:08.496552
elapsed time: 0:30:53.821713
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 12:44:15.629427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.57
 ---- batch: 020 ----
mean loss: 211.75
 ---- batch: 030 ----
mean loss: 208.17
 ---- batch: 040 ----
mean loss: 204.54
train mean loss: 205.31
epoch train time: 0:00:08.484384
elapsed time: 0:31:02.307466
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 12:44:24.115173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.12
 ---- batch: 020 ----
mean loss: 204.02
 ---- batch: 030 ----
mean loss: 200.94
 ---- batch: 040 ----
mean loss: 210.55
train mean loss: 204.54
epoch train time: 0:00:08.506331
elapsed time: 0:31:10.815044
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 12:44:32.622740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.08
 ---- batch: 020 ----
mean loss: 201.87
 ---- batch: 030 ----
mean loss: 205.45
 ---- batch: 040 ----
mean loss: 204.00
train mean loss: 205.15
epoch train time: 0:00:08.468092
elapsed time: 0:31:19.284389
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 12:44:41.092072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.35
 ---- batch: 020 ----
mean loss: 200.85
 ---- batch: 030 ----
mean loss: 197.27
 ---- batch: 040 ----
mean loss: 210.91
train mean loss: 205.27
epoch train time: 0:00:08.457149
elapsed time: 0:31:27.742724
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 12:44:49.550417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.06
 ---- batch: 020 ----
mean loss: 204.81
 ---- batch: 030 ----
mean loss: 205.60
 ---- batch: 040 ----
mean loss: 203.55
train mean loss: 204.75
epoch train time: 0:00:08.469482
elapsed time: 0:31:36.213446
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 12:44:58.021128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.29
 ---- batch: 020 ----
mean loss: 204.60
 ---- batch: 030 ----
mean loss: 203.54
 ---- batch: 040 ----
mean loss: 202.15
train mean loss: 204.44
epoch train time: 0:00:08.547792
elapsed time: 0:31:44.762460
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 12:45:06.570156
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.41
 ---- batch: 020 ----
mean loss: 212.09
 ---- batch: 030 ----
mean loss: 204.95
 ---- batch: 040 ----
mean loss: 197.02
train mean loss: 205.38
epoch train time: 0:00:08.492078
elapsed time: 0:31:53.255738
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 12:45:15.063415
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.13
 ---- batch: 020 ----
mean loss: 201.74
 ---- batch: 030 ----
mean loss: 207.30
 ---- batch: 040 ----
mean loss: 208.36
train mean loss: 204.65
epoch train time: 0:00:08.510748
elapsed time: 0:32:01.767719
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 12:45:23.575441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.48
 ---- batch: 020 ----
mean loss: 207.62
 ---- batch: 030 ----
mean loss: 202.74
 ---- batch: 040 ----
mean loss: 205.24
train mean loss: 204.92
epoch train time: 0:00:08.496973
elapsed time: 0:32:10.265964
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 12:45:32.073665
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.61
 ---- batch: 020 ----
mean loss: 208.62
 ---- batch: 030 ----
mean loss: 206.22
 ---- batch: 040 ----
mean loss: 202.63
train mean loss: 204.83
epoch train time: 0:00:08.488719
elapsed time: 0:32:18.755917
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 12:45:40.563614
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.14
 ---- batch: 020 ----
mean loss: 203.59
 ---- batch: 030 ----
mean loss: 214.03
 ---- batch: 040 ----
mean loss: 201.16
train mean loss: 204.61
epoch train time: 0:00:08.464377
elapsed time: 0:32:27.221512
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 12:45:49.029211
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.23
 ---- batch: 020 ----
mean loss: 199.13
 ---- batch: 030 ----
mean loss: 206.47
 ---- batch: 040 ----
mean loss: 206.25
train mean loss: 204.92
epoch train time: 0:00:08.552559
elapsed time: 0:32:35.775281
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 12:45:57.582959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.97
 ---- batch: 020 ----
mean loss: 204.59
 ---- batch: 030 ----
mean loss: 202.11
 ---- batch: 040 ----
mean loss: 204.44
train mean loss: 206.03
epoch train time: 0:00:08.491175
elapsed time: 0:32:44.267736
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 12:46:06.075461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.26
 ---- batch: 020 ----
mean loss: 207.12
 ---- batch: 030 ----
mean loss: 201.24
 ---- batch: 040 ----
mean loss: 209.02
train mean loss: 204.23
epoch train time: 0:00:08.456052
elapsed time: 0:32:52.725023
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 12:46:14.532705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.46
 ---- batch: 020 ----
mean loss: 202.88
 ---- batch: 030 ----
mean loss: 205.12
 ---- batch: 040 ----
mean loss: 201.58
train mean loss: 204.16
epoch train time: 0:00:08.470725
elapsed time: 0:33:01.197126
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 12:46:23.004588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.12
 ---- batch: 020 ----
mean loss: 209.45
 ---- batch: 030 ----
mean loss: 203.43
 ---- batch: 040 ----
mean loss: 206.57
train mean loss: 204.98
epoch train time: 0:00:08.480692
elapsed time: 0:33:09.678943
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 12:46:31.486652
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.16
 ---- batch: 020 ----
mean loss: 204.55
 ---- batch: 030 ----
mean loss: 205.41
 ---- batch: 040 ----
mean loss: 197.65
train mean loss: 204.33
epoch train time: 0:00:08.482742
elapsed time: 0:33:18.162948
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 12:46:39.970657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.64
 ---- batch: 020 ----
mean loss: 199.84
 ---- batch: 030 ----
mean loss: 207.59
 ---- batch: 040 ----
mean loss: 202.98
train mean loss: 204.59
epoch train time: 0:00:08.465553
elapsed time: 0:33:26.629800
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 12:46:48.437554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.16
 ---- batch: 020 ----
mean loss: 210.94
 ---- batch: 030 ----
mean loss: 202.85
 ---- batch: 040 ----
mean loss: 197.24
train mean loss: 204.70
epoch train time: 0:00:08.482466
elapsed time: 0:33:35.113537
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 12:46:56.921243
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.33
 ---- batch: 020 ----
mean loss: 205.95
 ---- batch: 030 ----
mean loss: 203.30
 ---- batch: 040 ----
mean loss: 208.40
train mean loss: 204.18
epoch train time: 0:00:08.491568
elapsed time: 0:33:43.606396
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 12:47:05.414084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.13
 ---- batch: 020 ----
mean loss: 204.65
 ---- batch: 030 ----
mean loss: 202.64
 ---- batch: 040 ----
mean loss: 207.30
train mean loss: 203.82
epoch train time: 0:00:08.497075
elapsed time: 0:33:52.104796
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 12:47:13.912513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.58
 ---- batch: 020 ----
mean loss: 207.48
 ---- batch: 030 ----
mean loss: 201.73
 ---- batch: 040 ----
mean loss: 197.56
train mean loss: 204.15
epoch train time: 0:00:08.475962
elapsed time: 0:34:00.581986
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 12:47:22.389721
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.73
 ---- batch: 020 ----
mean loss: 207.20
 ---- batch: 030 ----
mean loss: 205.95
 ---- batch: 040 ----
mean loss: 201.82
train mean loss: 205.05
epoch train time: 0:00:08.467260
elapsed time: 0:34:09.050567
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 12:47:30.858295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.99
 ---- batch: 020 ----
mean loss: 213.20
 ---- batch: 030 ----
mean loss: 204.18
 ---- batch: 040 ----
mean loss: 200.93
train mean loss: 204.61
epoch train time: 0:00:08.489499
elapsed time: 0:34:17.541326
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 12:47:39.349024
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.01
 ---- batch: 020 ----
mean loss: 205.46
 ---- batch: 030 ----
mean loss: 202.18
 ---- batch: 040 ----
mean loss: 212.95
train mean loss: 205.08
epoch train time: 0:00:08.464162
elapsed time: 0:34:26.006826
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 12:47:47.814556
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.18
 ---- batch: 020 ----
mean loss: 205.97
 ---- batch: 030 ----
mean loss: 203.60
 ---- batch: 040 ----
mean loss: 211.62
train mean loss: 203.86
epoch train time: 0:00:08.476845
elapsed time: 0:34:34.484964
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 12:47:56.292691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.63
 ---- batch: 020 ----
mean loss: 199.91
 ---- batch: 030 ----
mean loss: 204.87
 ---- batch: 040 ----
mean loss: 203.81
train mean loss: 204.34
epoch train time: 0:00:08.476217
elapsed time: 0:34:42.962501
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 12:48:04.770188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.81
 ---- batch: 020 ----
mean loss: 206.94
 ---- batch: 030 ----
mean loss: 207.98
 ---- batch: 040 ----
mean loss: 203.46
train mean loss: 204.53
epoch train time: 0:00:08.467395
elapsed time: 0:34:51.431086
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 12:48:13.238793
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.38
 ---- batch: 020 ----
mean loss: 201.38
 ---- batch: 030 ----
mean loss: 201.49
 ---- batch: 040 ----
mean loss: 204.07
train mean loss: 204.56
epoch train time: 0:00:08.470848
elapsed time: 0:34:59.903207
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 12:48:21.710921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.73
 ---- batch: 020 ----
mean loss: 205.06
 ---- batch: 030 ----
mean loss: 202.07
 ---- batch: 040 ----
mean loss: 205.95
train mean loss: 205.17
epoch train time: 0:00:08.488742
elapsed time: 0:35:08.393423
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 12:48:30.201038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.18
 ---- batch: 020 ----
mean loss: 206.22
 ---- batch: 030 ----
mean loss: 200.44
 ---- batch: 040 ----
mean loss: 201.00
train mean loss: 204.36
epoch train time: 0:00:08.462075
elapsed time: 0:35:16.866954
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_9/checkpoint.pth.tar
**** end time: 2019-09-25 12:48:38.674349 ****
