Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/bayesian_dense3/bayesian_dense3_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 29590
use_cuda: True
Dataset: CMAPSS/FD001
Building BayesianDense3...
Done.
**** start time: 2019-09-27 15:08:51.220552 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 15:08:51.230332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4000.30
 ---- batch: 020 ----
mean loss: 3707.21
 ---- batch: 030 ----
mean loss: 3584.49
train mean loss: 3714.93
epoch train time: 0:00:12.979262
elapsed time: 0:00:12.996006
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 15:09:04.216609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3259.56
 ---- batch: 020 ----
mean loss: 3037.85
 ---- batch: 030 ----
mean loss: 2902.90
train mean loss: 3034.11
epoch train time: 0:00:00.621139
elapsed time: 0:00:13.617499
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 15:09:04.838201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2691.86
 ---- batch: 020 ----
mean loss: 2568.67
 ---- batch: 030 ----
mean loss: 2489.63
train mean loss: 2554.83
epoch train time: 0:00:00.596272
elapsed time: 0:00:14.214174
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 15:09:05.434831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2340.04
 ---- batch: 020 ----
mean loss: 2239.34
 ---- batch: 030 ----
mean loss: 2214.76
train mean loss: 2254.10
epoch train time: 0:00:00.611833
elapsed time: 0:00:14.826432
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 15:09:06.047099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2090.19
 ---- batch: 020 ----
mean loss: 2050.06
 ---- batch: 030 ----
mean loss: 2086.30
train mean loss: 2067.16
epoch train time: 0:00:00.606885
elapsed time: 0:00:15.433703
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 15:09:06.654364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1975.81
 ---- batch: 020 ----
mean loss: 1943.92
 ---- batch: 030 ----
mean loss: 1925.85
train mean loss: 1944.20
epoch train time: 0:00:00.610066
elapsed time: 0:00:16.044124
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 15:09:07.264791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1889.76
 ---- batch: 020 ----
mean loss: 1841.60
 ---- batch: 030 ----
mean loss: 1809.23
train mean loss: 1837.90
epoch train time: 0:00:00.610177
elapsed time: 0:00:16.654704
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 15:09:07.875367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1780.25
 ---- batch: 020 ----
mean loss: 1769.47
 ---- batch: 030 ----
mean loss: 1702.98
train mean loss: 1744.05
epoch train time: 0:00:00.595735
elapsed time: 0:00:17.250775
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 15:09:08.471453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1682.61
 ---- batch: 020 ----
mean loss: 1666.57
 ---- batch: 030 ----
mean loss: 1640.84
train mean loss: 1656.48
epoch train time: 0:00:00.629055
elapsed time: 0:00:17.880294
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 15:09:09.101045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1629.33
 ---- batch: 020 ----
mean loss: 1572.04
 ---- batch: 030 ----
mean loss: 1555.87
train mean loss: 1576.53
epoch train time: 0:00:00.608100
elapsed time: 0:00:18.488875
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 15:09:09.709873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1498.01
 ---- batch: 020 ----
mean loss: 1513.78
 ---- batch: 030 ----
mean loss: 1473.36
train mean loss: 1490.77
epoch train time: 0:00:00.612970
elapsed time: 0:00:19.102587
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 15:09:10.323245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1449.60
 ---- batch: 020 ----
mean loss: 1439.03
 ---- batch: 030 ----
mean loss: 1403.79
train mean loss: 1431.04
epoch train time: 0:00:00.611092
elapsed time: 0:00:19.714106
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 15:09:10.934787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1385.77
 ---- batch: 020 ----
mean loss: 1361.79
 ---- batch: 030 ----
mean loss: 1356.86
train mean loss: 1357.76
epoch train time: 0:00:00.603175
elapsed time: 0:00:20.317677
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 15:09:11.538343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1343.55
 ---- batch: 020 ----
mean loss: 1307.02
 ---- batch: 030 ----
mean loss: 1259.36
train mean loss: 1293.95
epoch train time: 0:00:00.597131
elapsed time: 0:00:20.915167
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 15:09:12.135824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1244.83
 ---- batch: 020 ----
mean loss: 1231.18
 ---- batch: 030 ----
mean loss: 1221.91
train mean loss: 1228.05
epoch train time: 0:00:00.614766
elapsed time: 0:00:21.530426
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 15:09:12.751078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1183.78
 ---- batch: 020 ----
mean loss: 1170.71
 ---- batch: 030 ----
mean loss: 1159.90
train mean loss: 1168.19
epoch train time: 0:00:00.616230
elapsed time: 0:00:22.147059
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 15:09:13.367728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1136.11
 ---- batch: 020 ----
mean loss: 1106.22
 ---- batch: 030 ----
mean loss: 1094.41
train mean loss: 1107.09
epoch train time: 0:00:00.611342
elapsed time: 0:00:22.758860
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 15:09:13.979536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1067.81
 ---- batch: 020 ----
mean loss: 1049.59
 ---- batch: 030 ----
mean loss: 1021.48
train mean loss: 1044.65
epoch train time: 0:00:00.611524
elapsed time: 0:00:23.370765
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 15:09:14.591421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1011.77
 ---- batch: 020 ----
mean loss: 991.03
 ---- batch: 030 ----
mean loss: 980.03
train mean loss: 989.70
epoch train time: 0:00:00.626969
elapsed time: 0:00:23.998129
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 15:09:15.218832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 966.65
 ---- batch: 020 ----
mean loss: 934.13
 ---- batch: 030 ----
mean loss: 927.53
train mean loss: 938.87
epoch train time: 0:00:00.604019
elapsed time: 0:00:24.602569
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 15:09:15.823243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.24
 ---- batch: 020 ----
mean loss: 897.74
 ---- batch: 030 ----
mean loss: 876.52
train mean loss: 890.62
epoch train time: 0:00:00.606269
elapsed time: 0:00:25.209313
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 15:09:16.430005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.78
 ---- batch: 020 ----
mean loss: 863.21
 ---- batch: 030 ----
mean loss: 831.35
train mean loss: 838.88
epoch train time: 0:00:00.630599
elapsed time: 0:00:25.840338
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 15:09:17.061003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.96
 ---- batch: 020 ----
mean loss: 802.24
 ---- batch: 030 ----
mean loss: 793.61
train mean loss: 798.65
epoch train time: 0:00:00.609749
elapsed time: 0:00:26.450548
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 15:09:17.671219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.73
 ---- batch: 020 ----
mean loss: 776.18
 ---- batch: 030 ----
mean loss: 758.72
train mean loss: 757.63
epoch train time: 0:00:00.602066
elapsed time: 0:00:27.053000
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 15:09:18.273681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 741.08
 ---- batch: 020 ----
mean loss: 728.95
 ---- batch: 030 ----
mean loss: 702.38
train mean loss: 718.27
epoch train time: 0:00:00.613642
elapsed time: 0:00:27.667047
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 15:09:18.887702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.02
 ---- batch: 020 ----
mean loss: 698.66
 ---- batch: 030 ----
mean loss: 669.94
train mean loss: 679.11
epoch train time: 0:00:00.587137
elapsed time: 0:00:28.254540
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 15:09:19.475219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.70
 ---- batch: 020 ----
mean loss: 656.56
 ---- batch: 030 ----
mean loss: 625.55
train mean loss: 647.74
epoch train time: 0:00:00.605560
elapsed time: 0:00:28.860583
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 15:09:20.081245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 626.99
 ---- batch: 020 ----
mean loss: 613.74
 ---- batch: 030 ----
mean loss: 605.61
train mean loss: 613.17
epoch train time: 0:00:00.610745
elapsed time: 0:00:29.471716
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 15:09:20.692379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 586.20
 ---- batch: 020 ----
mean loss: 600.11
 ---- batch: 030 ----
mean loss: 565.05
train mean loss: 579.78
epoch train time: 0:00:00.605660
elapsed time: 0:00:30.077778
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 15:09:21.298473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.99
 ---- batch: 020 ----
mean loss: 552.83
 ---- batch: 030 ----
mean loss: 547.55
train mean loss: 551.28
epoch train time: 0:00:00.606695
elapsed time: 0:00:30.684901
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 15:09:21.905563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 537.06
 ---- batch: 020 ----
mean loss: 514.87
 ---- batch: 030 ----
mean loss: 538.86
train mean loss: 527.99
epoch train time: 0:00:00.597434
elapsed time: 0:00:31.282695
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 15:09:22.503354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.58
 ---- batch: 020 ----
mean loss: 505.46
 ---- batch: 030 ----
mean loss: 496.18
train mean loss: 499.39
epoch train time: 0:00:00.599631
elapsed time: 0:00:31.882671
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 15:09:23.103333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.25
 ---- batch: 020 ----
mean loss: 466.03
 ---- batch: 030 ----
mean loss: 459.86
train mean loss: 473.05
epoch train time: 0:00:00.608736
elapsed time: 0:00:32.491790
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 15:09:23.712452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.95
 ---- batch: 020 ----
mean loss: 463.31
 ---- batch: 030 ----
mean loss: 450.42
train mean loss: 454.79
epoch train time: 0:00:00.615006
elapsed time: 0:00:33.107186
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 15:09:24.327847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.05
 ---- batch: 020 ----
mean loss: 433.01
 ---- batch: 030 ----
mean loss: 427.47
train mean loss: 429.21
epoch train time: 0:00:00.603631
elapsed time: 0:00:33.711154
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 15:09:24.931866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.16
 ---- batch: 020 ----
mean loss: 410.55
 ---- batch: 030 ----
mean loss: 410.39
train mean loss: 411.15
epoch train time: 0:00:00.580400
elapsed time: 0:00:34.291994
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 15:09:25.512648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.01
 ---- batch: 020 ----
mean loss: 393.62
 ---- batch: 030 ----
mean loss: 388.00
train mean loss: 389.58
epoch train time: 0:00:00.597207
elapsed time: 0:00:34.889573
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 15:09:26.110235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.70
 ---- batch: 020 ----
mean loss: 372.20
 ---- batch: 030 ----
mean loss: 366.89
train mean loss: 372.72
epoch train time: 0:00:00.594674
elapsed time: 0:00:35.484639
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 15:09:26.705290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.93
 ---- batch: 020 ----
mean loss: 363.94
 ---- batch: 030 ----
mean loss: 350.44
train mean loss: 358.40
epoch train time: 0:00:00.601153
elapsed time: 0:00:36.086153
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 15:09:27.306817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.84
 ---- batch: 020 ----
mean loss: 342.17
 ---- batch: 030 ----
mean loss: 339.55
train mean loss: 342.80
epoch train time: 0:00:00.607938
elapsed time: 0:00:36.694474
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 15:09:27.915134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.98
 ---- batch: 020 ----
mean loss: 317.52
 ---- batch: 030 ----
mean loss: 324.78
train mean loss: 327.67
epoch train time: 0:00:00.593395
elapsed time: 0:00:37.288199
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 15:09:28.508869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.71
 ---- batch: 020 ----
mean loss: 310.36
 ---- batch: 030 ----
mean loss: 315.75
train mean loss: 310.42
epoch train time: 0:00:00.601343
elapsed time: 0:00:37.889899
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 15:09:29.110578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.75
 ---- batch: 020 ----
mean loss: 299.39
 ---- batch: 030 ----
mean loss: 289.18
train mean loss: 298.73
epoch train time: 0:00:00.599151
elapsed time: 0:00:38.489452
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 15:09:29.710115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.45
 ---- batch: 020 ----
mean loss: 283.64
 ---- batch: 030 ----
mean loss: 282.01
train mean loss: 285.59
epoch train time: 0:00:00.589244
elapsed time: 0:00:39.079068
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 15:09:30.299739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.17
 ---- batch: 020 ----
mean loss: 272.11
 ---- batch: 030 ----
mean loss: 274.53
train mean loss: 273.93
epoch train time: 0:00:00.597517
elapsed time: 0:00:39.676965
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 15:09:30.897670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.63
 ---- batch: 020 ----
mean loss: 259.17
 ---- batch: 030 ----
mean loss: 258.46
train mean loss: 262.39
epoch train time: 0:00:00.588880
elapsed time: 0:00:40.266246
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 15:09:31.486899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.01
 ---- batch: 020 ----
mean loss: 259.90
 ---- batch: 030 ----
mean loss: 246.68
train mean loss: 250.25
epoch train time: 0:00:00.591451
elapsed time: 0:00:40.858036
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 15:09:32.078695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.24
 ---- batch: 020 ----
mean loss: 241.07
 ---- batch: 030 ----
mean loss: 247.02
train mean loss: 243.52
epoch train time: 0:00:00.576510
elapsed time: 0:00:41.434864
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 15:09:32.655525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.90
 ---- batch: 020 ----
mean loss: 234.13
 ---- batch: 030 ----
mean loss: 232.73
train mean loss: 232.15
epoch train time: 0:00:00.597726
elapsed time: 0:00:42.032928
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 15:09:33.253587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.81
 ---- batch: 020 ----
mean loss: 223.25
 ---- batch: 030 ----
mean loss: 221.74
train mean loss: 223.15
epoch train time: 0:00:00.599439
elapsed time: 0:00:42.632723
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 15:09:33.853384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.83
 ---- batch: 020 ----
mean loss: 212.06
 ---- batch: 030 ----
mean loss: 214.73
train mean loss: 214.50
epoch train time: 0:00:00.580316
elapsed time: 0:00:43.213379
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 15:09:34.434048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.81
 ---- batch: 020 ----
mean loss: 206.64
 ---- batch: 030 ----
mean loss: 204.14
train mean loss: 206.93
epoch train time: 0:00:00.583618
elapsed time: 0:00:43.797335
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 15:09:35.018008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.75
 ---- batch: 020 ----
mean loss: 200.98
 ---- batch: 030 ----
mean loss: 197.86
train mean loss: 200.47
epoch train time: 0:00:00.598729
elapsed time: 0:00:44.396469
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 15:09:35.617164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.26
 ---- batch: 020 ----
mean loss: 199.91
 ---- batch: 030 ----
mean loss: 187.20
train mean loss: 193.61
epoch train time: 0:00:00.569060
elapsed time: 0:00:44.965933
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 15:09:36.186602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.15
 ---- batch: 020 ----
mean loss: 182.63
 ---- batch: 030 ----
mean loss: 185.52
train mean loss: 185.94
epoch train time: 0:00:00.595451
elapsed time: 0:00:45.561721
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 15:09:36.782379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.88
 ---- batch: 020 ----
mean loss: 182.99
 ---- batch: 030 ----
mean loss: 177.48
train mean loss: 182.61
epoch train time: 0:00:00.580327
elapsed time: 0:00:46.142456
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 15:09:37.363142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.53
 ---- batch: 020 ----
mean loss: 178.98
 ---- batch: 030 ----
mean loss: 172.53
train mean loss: 176.25
epoch train time: 0:00:00.599726
elapsed time: 0:00:46.742580
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 15:09:37.963243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.56
 ---- batch: 020 ----
mean loss: 171.00
 ---- batch: 030 ----
mean loss: 170.64
train mean loss: 170.24
epoch train time: 0:00:00.592668
elapsed time: 0:00:47.335599
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 15:09:38.556256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.23
 ---- batch: 020 ----
mean loss: 166.98
 ---- batch: 030 ----
mean loss: 165.23
train mean loss: 164.24
epoch train time: 0:00:00.598414
elapsed time: 0:00:47.934415
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 15:09:39.155077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.35
 ---- batch: 020 ----
mean loss: 158.09
 ---- batch: 030 ----
mean loss: 160.01
train mean loss: 161.10
epoch train time: 0:00:00.613473
elapsed time: 0:00:48.548359
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 15:09:39.769014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.15
 ---- batch: 020 ----
mean loss: 156.00
 ---- batch: 030 ----
mean loss: 159.09
train mean loss: 157.49
epoch train time: 0:00:00.584203
elapsed time: 0:00:49.132964
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 15:09:40.353678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.38
 ---- batch: 020 ----
mean loss: 148.35
 ---- batch: 030 ----
mean loss: 156.34
train mean loss: 150.99
epoch train time: 0:00:00.593190
elapsed time: 0:00:49.726577
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 15:09:40.947239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.66
 ---- batch: 020 ----
mean loss: 148.22
 ---- batch: 030 ----
mean loss: 144.70
train mean loss: 147.08
epoch train time: 0:00:00.593936
elapsed time: 0:00:50.320926
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 15:09:41.541581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.26
 ---- batch: 020 ----
mean loss: 147.10
 ---- batch: 030 ----
mean loss: 145.48
train mean loss: 144.54
epoch train time: 0:00:00.588200
elapsed time: 0:00:50.909483
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 15:09:42.130142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.45
 ---- batch: 020 ----
mean loss: 140.52
 ---- batch: 030 ----
mean loss: 140.89
train mean loss: 141.11
epoch train time: 0:00:00.602886
elapsed time: 0:00:51.512699
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 15:09:42.733361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.77
 ---- batch: 020 ----
mean loss: 140.68
 ---- batch: 030 ----
mean loss: 138.42
train mean loss: 137.73
epoch train time: 0:00:00.601503
elapsed time: 0:00:52.114642
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 15:09:43.335341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.35
 ---- batch: 020 ----
mean loss: 136.32
 ---- batch: 030 ----
mean loss: 134.73
train mean loss: 135.73
epoch train time: 0:00:00.591717
elapsed time: 0:00:52.706748
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 15:09:43.927423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.52
 ---- batch: 020 ----
mean loss: 133.17
 ---- batch: 030 ----
mean loss: 135.67
train mean loss: 133.56
epoch train time: 0:00:00.589538
elapsed time: 0:00:53.296668
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 15:09:44.517331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.73
 ---- batch: 020 ----
mean loss: 127.48
 ---- batch: 030 ----
mean loss: 125.78
train mean loss: 130.15
epoch train time: 0:00:00.607351
elapsed time: 0:00:53.904388
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 15:09:45.125067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.04
 ---- batch: 020 ----
mean loss: 126.17
 ---- batch: 030 ----
mean loss: 126.93
train mean loss: 127.51
epoch train time: 0:00:00.595269
elapsed time: 0:00:54.500032
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 15:09:45.720705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.59
 ---- batch: 020 ----
mean loss: 125.12
 ---- batch: 030 ----
mean loss: 125.45
train mean loss: 125.17
epoch train time: 0:00:00.595852
elapsed time: 0:00:55.096237
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 15:09:46.316906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.12
 ---- batch: 020 ----
mean loss: 122.63
 ---- batch: 030 ----
mean loss: 118.53
train mean loss: 121.22
epoch train time: 0:00:00.591865
elapsed time: 0:00:55.688463
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 15:09:46.909119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.04
 ---- batch: 020 ----
mean loss: 120.63
 ---- batch: 030 ----
mean loss: 121.57
train mean loss: 119.82
epoch train time: 0:00:00.587431
elapsed time: 0:00:56.276259
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 15:09:47.496924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.06
 ---- batch: 020 ----
mean loss: 119.69
 ---- batch: 030 ----
mean loss: 118.96
train mean loss: 118.84
epoch train time: 0:00:00.588114
elapsed time: 0:00:56.864718
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 15:09:48.085380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.43
 ---- batch: 020 ----
mean loss: 114.79
 ---- batch: 030 ----
mean loss: 115.55
train mean loss: 114.04
epoch train time: 0:00:00.589365
elapsed time: 0:00:57.454452
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 15:09:48.675112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.42
 ---- batch: 020 ----
mean loss: 112.47
 ---- batch: 030 ----
mean loss: 113.79
train mean loss: 112.37
epoch train time: 0:00:00.591950
elapsed time: 0:00:58.046823
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 15:09:49.267510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.51
 ---- batch: 020 ----
mean loss: 111.25
 ---- batch: 030 ----
mean loss: 110.08
train mean loss: 112.82
epoch train time: 0:00:00.603315
elapsed time: 0:00:58.650590
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 15:09:49.871253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.46
 ---- batch: 020 ----
mean loss: 110.37
 ---- batch: 030 ----
mean loss: 109.16
train mean loss: 110.56
epoch train time: 0:00:00.597732
elapsed time: 0:00:59.248700
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 15:09:50.469363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.99
 ---- batch: 020 ----
mean loss: 107.10
 ---- batch: 030 ----
mean loss: 109.01
train mean loss: 108.21
epoch train time: 0:00:00.595726
elapsed time: 0:00:59.844790
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 15:09:51.065467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.26
 ---- batch: 020 ----
mean loss: 107.48
 ---- batch: 030 ----
mean loss: 108.94
train mean loss: 107.51
epoch train time: 0:00:00.589127
elapsed time: 0:01:00.434316
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 15:09:51.654989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.81
 ---- batch: 020 ----
mean loss: 104.09
 ---- batch: 030 ----
mean loss: 104.51
train mean loss: 105.36
epoch train time: 0:00:00.583600
elapsed time: 0:01:01.018263
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 15:09:52.238920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.75
 ---- batch: 020 ----
mean loss: 103.61
 ---- batch: 030 ----
mean loss: 103.87
train mean loss: 103.32
epoch train time: 0:00:00.591053
elapsed time: 0:01:01.609685
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 15:09:52.830361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.93
 ---- batch: 020 ----
mean loss: 105.62
 ---- batch: 030 ----
mean loss: 98.89
train mean loss: 102.47
epoch train time: 0:00:00.596052
elapsed time: 0:01:02.206125
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 15:09:53.426796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.86
 ---- batch: 020 ----
mean loss: 99.55
 ---- batch: 030 ----
mean loss: 101.36
train mean loss: 101.60
epoch train time: 0:00:00.596001
elapsed time: 0:01:02.802516
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 15:09:54.023177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.17
 ---- batch: 020 ----
mean loss: 103.61
 ---- batch: 030 ----
mean loss: 100.20
train mean loss: 99.91
epoch train time: 0:00:00.590039
elapsed time: 0:01:03.392959
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 15:09:54.613642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.60
 ---- batch: 020 ----
mean loss: 99.83
 ---- batch: 030 ----
mean loss: 98.88
train mean loss: 98.32
epoch train time: 0:00:00.617499
elapsed time: 0:01:04.010823
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 15:09:55.231485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.84
 ---- batch: 020 ----
mean loss: 99.37
 ---- batch: 030 ----
mean loss: 98.96
train mean loss: 98.65
epoch train time: 0:00:00.601434
elapsed time: 0:01:04.612609
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 15:09:55.833264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.74
 ---- batch: 020 ----
mean loss: 97.09
 ---- batch: 030 ----
mean loss: 91.75
train mean loss: 95.96
epoch train time: 0:00:00.588374
elapsed time: 0:01:05.201307
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 15:09:56.421974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.49
 ---- batch: 020 ----
mean loss: 93.34
 ---- batch: 030 ----
mean loss: 97.32
train mean loss: 95.76
epoch train time: 0:00:00.592213
elapsed time: 0:01:05.793905
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 15:09:57.014566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.50
 ---- batch: 020 ----
mean loss: 92.05
 ---- batch: 030 ----
mean loss: 96.58
train mean loss: 94.95
epoch train time: 0:00:00.600984
elapsed time: 0:01:06.395326
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 15:09:57.615986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.82
 ---- batch: 020 ----
mean loss: 92.19
 ---- batch: 030 ----
mean loss: 95.37
train mean loss: 93.95
epoch train time: 0:00:00.601378
elapsed time: 0:01:06.997031
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 15:09:58.217710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.81
 ---- batch: 020 ----
mean loss: 94.19
 ---- batch: 030 ----
mean loss: 94.18
train mean loss: 93.09
epoch train time: 0:00:00.592415
elapsed time: 0:01:07.589814
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 15:09:58.810481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.84
 ---- batch: 020 ----
mean loss: 89.98
 ---- batch: 030 ----
mean loss: 90.81
train mean loss: 91.15
epoch train time: 0:00:00.584433
elapsed time: 0:01:08.174571
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 15:09:59.395253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.09
 ---- batch: 020 ----
mean loss: 90.86
 ---- batch: 030 ----
mean loss: 92.41
train mean loss: 91.46
epoch train time: 0:00:00.600969
elapsed time: 0:01:08.776199
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 15:09:59.996867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.49
 ---- batch: 020 ----
mean loss: 91.06
 ---- batch: 030 ----
mean loss: 91.30
train mean loss: 90.75
epoch train time: 0:00:00.596660
elapsed time: 0:01:09.373241
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 15:10:00.593901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.23
 ---- batch: 020 ----
mean loss: 85.14
 ---- batch: 030 ----
mean loss: 92.86
train mean loss: 89.74
epoch train time: 0:00:00.585351
elapsed time: 0:01:09.958922
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 15:10:01.179575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.29
 ---- batch: 020 ----
mean loss: 86.46
 ---- batch: 030 ----
mean loss: 89.18
train mean loss: 87.17
epoch train time: 0:00:00.591403
elapsed time: 0:01:10.550672
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 15:10:01.771350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.86
 ---- batch: 020 ----
mean loss: 89.39
 ---- batch: 030 ----
mean loss: 88.14
train mean loss: 88.98
epoch train time: 0:00:00.586801
elapsed time: 0:01:11.137853
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 15:10:02.358514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.98
 ---- batch: 020 ----
mean loss: 86.24
 ---- batch: 030 ----
mean loss: 89.38
train mean loss: 87.99
epoch train time: 0:00:00.587593
elapsed time: 0:01:11.725824
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 15:10:02.946503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.20
 ---- batch: 020 ----
mean loss: 87.44
 ---- batch: 030 ----
mean loss: 83.73
train mean loss: 86.35
epoch train time: 0:00:00.576572
elapsed time: 0:01:12.302753
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 15:10:03.523419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.57
 ---- batch: 020 ----
mean loss: 86.57
 ---- batch: 030 ----
mean loss: 80.63
train mean loss: 85.25
epoch train time: 0:00:00.613286
elapsed time: 0:01:12.916502
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 15:10:04.137180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.78
 ---- batch: 020 ----
mean loss: 83.17
 ---- batch: 030 ----
mean loss: 84.56
train mean loss: 84.91
epoch train time: 0:00:00.604219
elapsed time: 0:01:13.521185
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 15:10:04.741850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.66
 ---- batch: 020 ----
mean loss: 89.07
 ---- batch: 030 ----
mean loss: 84.05
train mean loss: 84.84
epoch train time: 0:00:00.590658
elapsed time: 0:01:14.112212
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 15:10:05.332900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.57
 ---- batch: 020 ----
mean loss: 86.84
 ---- batch: 030 ----
mean loss: 82.43
train mean loss: 83.96
epoch train time: 0:00:00.588932
elapsed time: 0:01:14.701553
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 15:10:05.922215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.54
 ---- batch: 020 ----
mean loss: 83.54
 ---- batch: 030 ----
mean loss: 82.00
train mean loss: 84.08
epoch train time: 0:00:00.593523
elapsed time: 0:01:15.295450
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 15:10:06.516116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.33
 ---- batch: 020 ----
mean loss: 82.37
 ---- batch: 030 ----
mean loss: 83.70
train mean loss: 82.76
epoch train time: 0:00:00.596013
elapsed time: 0:01:15.891821
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 15:10:07.112503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.03
 ---- batch: 020 ----
mean loss: 81.57
 ---- batch: 030 ----
mean loss: 78.78
train mean loss: 81.37
epoch train time: 0:00:00.586927
elapsed time: 0:01:16.479243
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 15:10:07.699864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.19
 ---- batch: 020 ----
mean loss: 80.06
 ---- batch: 030 ----
mean loss: 82.90
train mean loss: 80.70
epoch train time: 0:00:00.580280
elapsed time: 0:01:17.059882
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 15:10:08.280542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.76
 ---- batch: 020 ----
mean loss: 79.63
 ---- batch: 030 ----
mean loss: 78.16
train mean loss: 79.25
epoch train time: 0:00:00.617054
elapsed time: 0:01:17.677349
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 15:10:08.898010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.50
 ---- batch: 020 ----
mean loss: 80.95
 ---- batch: 030 ----
mean loss: 79.45
train mean loss: 80.73
epoch train time: 0:00:00.598367
elapsed time: 0:01:18.276071
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 15:10:09.496752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.28
 ---- batch: 020 ----
mean loss: 78.05
 ---- batch: 030 ----
mean loss: 82.84
train mean loss: 79.45
epoch train time: 0:00:00.596535
elapsed time: 0:01:18.873014
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 15:10:10.093693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.18
 ---- batch: 020 ----
mean loss: 77.15
 ---- batch: 030 ----
mean loss: 77.36
train mean loss: 78.96
epoch train time: 0:00:00.604535
elapsed time: 0:01:19.478047
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 15:10:10.698768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.32
 ---- batch: 020 ----
mean loss: 79.06
 ---- batch: 030 ----
mean loss: 77.31
train mean loss: 78.04
epoch train time: 0:00:00.609575
elapsed time: 0:01:20.088025
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 15:10:11.308687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.96
 ---- batch: 020 ----
mean loss: 74.57
 ---- batch: 030 ----
mean loss: 78.76
train mean loss: 77.47
epoch train time: 0:00:00.599213
elapsed time: 0:01:20.687589
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 15:10:11.908276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.46
 ---- batch: 020 ----
mean loss: 74.32
 ---- batch: 030 ----
mean loss: 79.97
train mean loss: 77.79
epoch train time: 0:00:00.583854
elapsed time: 0:01:21.271880
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 15:10:12.492540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.71
 ---- batch: 020 ----
mean loss: 79.38
 ---- batch: 030 ----
mean loss: 77.17
train mean loss: 77.86
epoch train time: 0:00:00.603026
elapsed time: 0:01:21.875270
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 15:10:13.095935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.64
 ---- batch: 020 ----
mean loss: 75.96
 ---- batch: 030 ----
mean loss: 77.33
train mean loss: 75.73
epoch train time: 0:00:00.595695
elapsed time: 0:01:22.471414
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 15:10:13.692101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.78
 ---- batch: 020 ----
mean loss: 76.44
 ---- batch: 030 ----
mean loss: 77.80
train mean loss: 76.93
epoch train time: 0:00:00.608567
elapsed time: 0:01:23.080510
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 15:10:14.301171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.66
 ---- batch: 020 ----
mean loss: 73.98
 ---- batch: 030 ----
mean loss: 75.15
train mean loss: 75.30
epoch train time: 0:00:00.613399
elapsed time: 0:01:23.694270
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 15:10:14.914942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.75
 ---- batch: 020 ----
mean loss: 73.67
 ---- batch: 030 ----
mean loss: 76.07
train mean loss: 74.89
epoch train time: 0:00:00.598430
elapsed time: 0:01:24.293098
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 15:10:15.513761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.62
 ---- batch: 020 ----
mean loss: 74.16
 ---- batch: 030 ----
mean loss: 73.86
train mean loss: 75.11
epoch train time: 0:00:00.598432
elapsed time: 0:01:24.891984
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 15:10:16.112688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.79
 ---- batch: 020 ----
mean loss: 74.17
 ---- batch: 030 ----
mean loss: 76.92
train mean loss: 74.39
epoch train time: 0:00:00.603933
elapsed time: 0:01:25.496355
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 15:10:16.717049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.69
 ---- batch: 020 ----
mean loss: 75.32
 ---- batch: 030 ----
mean loss: 73.69
train mean loss: 73.82
epoch train time: 0:00:00.590980
elapsed time: 0:01:26.087738
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 15:10:17.308418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.28
 ---- batch: 020 ----
mean loss: 72.93
 ---- batch: 030 ----
mean loss: 72.66
train mean loss: 73.00
epoch train time: 0:00:00.612682
elapsed time: 0:01:26.700801
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 15:10:17.921484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.50
 ---- batch: 020 ----
mean loss: 70.49
 ---- batch: 030 ----
mean loss: 73.87
train mean loss: 72.74
epoch train time: 0:00:00.603139
elapsed time: 0:01:27.304320
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 15:10:18.524983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.67
 ---- batch: 020 ----
mean loss: 72.02
 ---- batch: 030 ----
mean loss: 70.30
train mean loss: 72.07
epoch train time: 0:00:00.607986
elapsed time: 0:01:27.912652
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 15:10:19.133317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.45
 ---- batch: 020 ----
mean loss: 71.02
 ---- batch: 030 ----
mean loss: 71.18
train mean loss: 71.20
epoch train time: 0:00:00.597301
elapsed time: 0:01:28.510417
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 15:10:19.731035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.39
 ---- batch: 020 ----
mean loss: 71.43
 ---- batch: 030 ----
mean loss: 69.83
train mean loss: 70.72
epoch train time: 0:00:00.597577
elapsed time: 0:01:29.108511
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 15:10:20.329190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.13
 ---- batch: 020 ----
mean loss: 70.74
 ---- batch: 030 ----
mean loss: 70.41
train mean loss: 70.85
epoch train time: 0:00:00.598923
elapsed time: 0:01:29.707848
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 15:10:20.928530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.69
 ---- batch: 020 ----
mean loss: 71.38
 ---- batch: 030 ----
mean loss: 70.91
train mean loss: 69.59
epoch train time: 0:00:00.590100
elapsed time: 0:01:30.298322
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 15:10:21.518994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.79
 ---- batch: 020 ----
mean loss: 67.93
 ---- batch: 030 ----
mean loss: 67.84
train mean loss: 68.77
epoch train time: 0:00:00.601123
elapsed time: 0:01:30.899795
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 15:10:22.120464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.18
 ---- batch: 020 ----
mean loss: 67.26
 ---- batch: 030 ----
mean loss: 69.75
train mean loss: 68.68
epoch train time: 0:00:00.595321
elapsed time: 0:01:31.495492
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 15:10:22.716158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.31
 ---- batch: 020 ----
mean loss: 71.25
 ---- batch: 030 ----
mean loss: 67.81
train mean loss: 69.19
epoch train time: 0:00:00.606499
elapsed time: 0:01:32.102400
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 15:10:23.323077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.61
 ---- batch: 020 ----
mean loss: 67.29
 ---- batch: 030 ----
mean loss: 70.26
train mean loss: 68.64
epoch train time: 0:00:00.618738
elapsed time: 0:01:32.721508
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 15:10:23.942169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.85
 ---- batch: 020 ----
mean loss: 69.07
 ---- batch: 030 ----
mean loss: 67.49
train mean loss: 68.00
epoch train time: 0:00:00.607351
elapsed time: 0:01:33.329400
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 15:10:24.550097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.93
 ---- batch: 020 ----
mean loss: 66.50
 ---- batch: 030 ----
mean loss: 66.07
train mean loss: 66.45
epoch train time: 0:00:00.616254
elapsed time: 0:01:33.946109
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 15:10:25.166787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.06
 ---- batch: 020 ----
mean loss: 67.17
 ---- batch: 030 ----
mean loss: 65.80
train mean loss: 66.96
epoch train time: 0:00:00.598358
elapsed time: 0:01:34.544825
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 15:10:25.765496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.78
 ---- batch: 020 ----
mean loss: 68.09
 ---- batch: 030 ----
mean loss: 62.56
train mean loss: 65.84
epoch train time: 0:00:00.601664
elapsed time: 0:01:35.146855
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 15:10:26.367539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.53
 ---- batch: 020 ----
mean loss: 66.13
 ---- batch: 030 ----
mean loss: 65.88
train mean loss: 65.71
epoch train time: 0:00:00.600460
elapsed time: 0:01:35.747762
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 15:10:26.968435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.43
 ---- batch: 020 ----
mean loss: 67.60
 ---- batch: 030 ----
mean loss: 65.53
train mean loss: 66.05
epoch train time: 0:00:00.591442
elapsed time: 0:01:36.339554
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 15:10:27.560212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.15
 ---- batch: 020 ----
mean loss: 61.90
 ---- batch: 030 ----
mean loss: 67.01
train mean loss: 64.46
epoch train time: 0:00:00.607507
elapsed time: 0:01:36.947582
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 15:10:28.168276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.19
 ---- batch: 020 ----
mean loss: 63.97
 ---- batch: 030 ----
mean loss: 65.15
train mean loss: 64.29
epoch train time: 0:00:00.601850
elapsed time: 0:01:37.549875
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 15:10:28.770568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.20
 ---- batch: 020 ----
mean loss: 63.70
 ---- batch: 030 ----
mean loss: 65.52
train mean loss: 63.49
epoch train time: 0:00:00.608939
elapsed time: 0:01:38.159187
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 15:10:29.379861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.55
 ---- batch: 020 ----
mean loss: 62.62
 ---- batch: 030 ----
mean loss: 64.58
train mean loss: 63.95
epoch train time: 0:00:00.599065
elapsed time: 0:01:38.758595
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 15:10:29.979252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.25
 ---- batch: 020 ----
mean loss: 62.88
 ---- batch: 030 ----
mean loss: 62.08
train mean loss: 62.68
epoch train time: 0:00:00.584118
elapsed time: 0:01:39.343091
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 15:10:30.563759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.22
 ---- batch: 020 ----
mean loss: 63.26
 ---- batch: 030 ----
mean loss: 60.09
train mean loss: 62.80
epoch train time: 0:00:00.600948
elapsed time: 0:01:39.944439
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 15:10:31.165100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.69
 ---- batch: 020 ----
mean loss: 63.54
 ---- batch: 030 ----
mean loss: 59.84
train mean loss: 61.85
epoch train time: 0:00:00.597602
elapsed time: 0:01:40.542476
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 15:10:31.763155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.92
 ---- batch: 020 ----
mean loss: 61.11
 ---- batch: 030 ----
mean loss: 60.72
train mean loss: 60.85
epoch train time: 0:00:00.602527
elapsed time: 0:01:41.145383
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 15:10:32.366045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.02
 ---- batch: 020 ----
mean loss: 61.06
 ---- batch: 030 ----
mean loss: 59.52
train mean loss: 60.50
epoch train time: 0:00:00.594517
elapsed time: 0:01:41.740292
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 15:10:32.960888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.32
 ---- batch: 020 ----
mean loss: 60.65
 ---- batch: 030 ----
mean loss: 58.73
train mean loss: 60.57
epoch train time: 0:00:00.590000
elapsed time: 0:01:42.330576
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 15:10:33.551237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.68
 ---- batch: 020 ----
mean loss: 59.58
 ---- batch: 030 ----
mean loss: 58.83
train mean loss: 59.82
epoch train time: 0:00:00.593053
elapsed time: 0:01:42.923994
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 15:10:34.144671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.87
 ---- batch: 020 ----
mean loss: 58.72
 ---- batch: 030 ----
mean loss: 61.23
train mean loss: 59.29
epoch train time: 0:00:00.581595
elapsed time: 0:01:43.506028
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 15:10:34.726699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.40
 ---- batch: 020 ----
mean loss: 63.43
 ---- batch: 030 ----
mean loss: 57.25
train mean loss: 59.37
epoch train time: 0:00:00.589109
elapsed time: 0:01:44.095526
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 15:10:35.316189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.87
 ---- batch: 020 ----
mean loss: 58.68
 ---- batch: 030 ----
mean loss: 60.22
train mean loss: 58.39
epoch train time: 0:00:00.599071
elapsed time: 0:01:44.694966
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 15:10:35.915622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.30
 ---- batch: 020 ----
mean loss: 59.22
 ---- batch: 030 ----
mean loss: 57.56
train mean loss: 58.35
epoch train time: 0:00:00.588160
elapsed time: 0:01:45.283458
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 15:10:36.504112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.37
 ---- batch: 020 ----
mean loss: 57.07
 ---- batch: 030 ----
mean loss: 59.35
train mean loss: 57.04
epoch train time: 0:00:00.597870
elapsed time: 0:01:45.881675
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 15:10:37.102374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.79
 ---- batch: 020 ----
mean loss: 55.08
 ---- batch: 030 ----
mean loss: 56.78
train mean loss: 56.62
epoch train time: 0:00:00.609033
elapsed time: 0:01:46.491092
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 15:10:37.711786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.92
 ---- batch: 020 ----
mean loss: 56.93
 ---- batch: 030 ----
mean loss: 58.11
train mean loss: 57.44
epoch train time: 0:00:00.592030
elapsed time: 0:01:47.083493
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 15:10:38.304171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.80
 ---- batch: 020 ----
mean loss: 57.25
 ---- batch: 030 ----
mean loss: 55.55
train mean loss: 56.10
epoch train time: 0:00:00.613566
elapsed time: 0:01:47.697486
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 15:10:38.918143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.08
 ---- batch: 020 ----
mean loss: 54.28
 ---- batch: 030 ----
mean loss: 54.44
train mean loss: 55.79
epoch train time: 0:00:00.592371
elapsed time: 0:01:48.290197
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 15:10:39.510854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.08
 ---- batch: 020 ----
mean loss: 52.25
 ---- batch: 030 ----
mean loss: 57.73
train mean loss: 54.79
epoch train time: 0:00:00.588163
elapsed time: 0:01:48.878684
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 15:10:40.099351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.37
 ---- batch: 020 ----
mean loss: 54.79
 ---- batch: 030 ----
mean loss: 52.62
train mean loss: 54.60
epoch train time: 0:00:00.581440
elapsed time: 0:01:49.460558
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 15:10:40.681220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.76
 ---- batch: 020 ----
mean loss: 53.79
 ---- batch: 030 ----
mean loss: 55.15
train mean loss: 54.11
epoch train time: 0:00:00.601648
elapsed time: 0:01:50.062542
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 15:10:41.283203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.72
 ---- batch: 020 ----
mean loss: 53.28
 ---- batch: 030 ----
mean loss: 52.38
train mean loss: 53.54
epoch train time: 0:00:00.597415
elapsed time: 0:01:50.660306
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 15:10:41.881031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.32
 ---- batch: 020 ----
mean loss: 52.09
 ---- batch: 030 ----
mean loss: 52.56
train mean loss: 53.00
epoch train time: 0:00:00.606614
elapsed time: 0:01:51.267326
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 15:10:42.488007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.90
 ---- batch: 020 ----
mean loss: 51.35
 ---- batch: 030 ----
mean loss: 54.46
train mean loss: 52.95
epoch train time: 0:00:00.597868
elapsed time: 0:01:51.865563
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 15:10:43.086229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.83
 ---- batch: 020 ----
mean loss: 52.11
 ---- batch: 030 ----
mean loss: 51.61
train mean loss: 52.47
epoch train time: 0:00:00.601260
elapsed time: 0:01:52.467176
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 15:10:43.687836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.92
 ---- batch: 020 ----
mean loss: 50.80
 ---- batch: 030 ----
mean loss: 52.19
train mean loss: 52.80
epoch train time: 0:00:00.605377
elapsed time: 0:01:53.072907
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 15:10:44.293569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.67
 ---- batch: 020 ----
mean loss: 50.27
 ---- batch: 030 ----
mean loss: 50.68
train mean loss: 51.06
epoch train time: 0:00:00.593625
elapsed time: 0:01:53.666904
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 15:10:44.887647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.20
 ---- batch: 020 ----
mean loss: 51.88
 ---- batch: 030 ----
mean loss: 50.00
train mean loss: 50.44
epoch train time: 0:00:00.602317
elapsed time: 0:01:54.269659
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 15:10:45.490351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.64
 ---- batch: 020 ----
mean loss: 51.49
 ---- batch: 030 ----
mean loss: 48.74
train mean loss: 50.01
epoch train time: 0:00:00.600419
elapsed time: 0:01:54.870449
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 15:10:46.091108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.90
 ---- batch: 020 ----
mean loss: 49.74
 ---- batch: 030 ----
mean loss: 49.43
train mean loss: 49.71
epoch train time: 0:00:00.585148
elapsed time: 0:01:55.455953
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 15:10:46.676614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.50
 ---- batch: 020 ----
mean loss: 49.62
 ---- batch: 030 ----
mean loss: 49.36
train mean loss: 49.38
epoch train time: 0:00:00.610526
elapsed time: 0:01:56.066834
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 15:10:47.287499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.97
 ---- batch: 020 ----
mean loss: 48.76
 ---- batch: 030 ----
mean loss: 47.92
train mean loss: 47.98
epoch train time: 0:00:00.587964
elapsed time: 0:01:56.655237
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 15:10:47.875839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.29
 ---- batch: 020 ----
mean loss: 49.07
 ---- batch: 030 ----
mean loss: 50.00
train mean loss: 48.37
epoch train time: 0:00:00.601709
elapsed time: 0:01:57.257266
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 15:10:48.477940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.64
 ---- batch: 020 ----
mean loss: 46.69
 ---- batch: 030 ----
mean loss: 48.34
train mean loss: 47.57
epoch train time: 0:00:00.593565
elapsed time: 0:01:57.851181
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 15:10:49.071840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.32
 ---- batch: 020 ----
mean loss: 49.05
 ---- batch: 030 ----
mean loss: 47.85
train mean loss: 47.76
epoch train time: 0:00:00.590424
elapsed time: 0:01:58.441941
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 15:10:49.662601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.13
 ---- batch: 020 ----
mean loss: 47.65
 ---- batch: 030 ----
mean loss: 48.36
train mean loss: 46.96
epoch train time: 0:00:00.610258
elapsed time: 0:01:59.052541
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 15:10:50.273203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.78
 ---- batch: 020 ----
mean loss: 45.88
 ---- batch: 030 ----
mean loss: 47.32
train mean loss: 47.04
epoch train time: 0:00:00.597376
elapsed time: 0:01:59.650290
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 15:10:50.870998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.48
 ---- batch: 020 ----
mean loss: 44.74
 ---- batch: 030 ----
mean loss: 46.55
train mean loss: 45.80
epoch train time: 0:00:00.598410
elapsed time: 0:02:00.249140
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 15:10:51.469800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.15
 ---- batch: 020 ----
mean loss: 45.25
 ---- batch: 030 ----
mean loss: 45.74
train mean loss: 45.62
epoch train time: 0:00:00.590316
elapsed time: 0:02:00.839808
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 15:10:52.060503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.77
 ---- batch: 020 ----
mean loss: 48.08
 ---- batch: 030 ----
mean loss: 45.47
train mean loss: 45.89
epoch train time: 0:00:00.611509
elapsed time: 0:02:01.451740
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 15:10:52.672426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.95
 ---- batch: 020 ----
mean loss: 46.39
 ---- batch: 030 ----
mean loss: 42.01
train mean loss: 44.00
epoch train time: 0:00:00.610117
elapsed time: 0:02:02.062251
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 15:10:53.282902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.51
 ---- batch: 020 ----
mean loss: 43.76
 ---- batch: 030 ----
mean loss: 45.54
train mean loss: 43.88
epoch train time: 0:00:00.599462
elapsed time: 0:02:02.662048
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 15:10:53.882710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.16
 ---- batch: 020 ----
mean loss: 44.56
 ---- batch: 030 ----
mean loss: 43.84
train mean loss: 43.54
epoch train time: 0:00:00.593660
elapsed time: 0:02:03.256136
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 15:10:54.476828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.45
 ---- batch: 020 ----
mean loss: 43.13
 ---- batch: 030 ----
mean loss: 43.22
train mean loss: 43.58
epoch train time: 0:00:00.622294
elapsed time: 0:02:03.878822
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 15:10:55.099483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.21
 ---- batch: 020 ----
mean loss: 44.37
 ---- batch: 030 ----
mean loss: 43.76
train mean loss: 44.80
epoch train time: 0:00:00.599901
elapsed time: 0:02:04.479121
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 15:10:55.699796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.32
 ---- batch: 020 ----
mean loss: 41.32
 ---- batch: 030 ----
mean loss: 46.35
train mean loss: 43.13
epoch train time: 0:00:00.607924
elapsed time: 0:02:05.087388
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 15:10:56.308052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.11
 ---- batch: 020 ----
mean loss: 42.42
 ---- batch: 030 ----
mean loss: 41.94
train mean loss: 42.57
epoch train time: 0:00:00.596552
elapsed time: 0:02:05.684288
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 15:10:56.904975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.97
 ---- batch: 020 ----
mean loss: 41.83
 ---- batch: 030 ----
mean loss: 43.45
train mean loss: 42.39
epoch train time: 0:00:00.594721
elapsed time: 0:02:06.279377
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 15:10:57.500036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.44
 ---- batch: 020 ----
mean loss: 40.11
 ---- batch: 030 ----
mean loss: 41.92
train mean loss: 41.57
epoch train time: 0:00:00.593810
elapsed time: 0:02:06.873542
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 15:10:58.094202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.85
 ---- batch: 020 ----
mean loss: 40.74
 ---- batch: 030 ----
mean loss: 41.95
train mean loss: 41.33
epoch train time: 0:00:00.586924
elapsed time: 0:02:07.460883
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 15:10:58.681557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.43
 ---- batch: 020 ----
mean loss: 39.04
 ---- batch: 030 ----
mean loss: 42.38
train mean loss: 40.39
epoch train time: 0:00:00.599642
elapsed time: 0:02:08.060945
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 15:10:59.281625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.49
 ---- batch: 020 ----
mean loss: 40.68
 ---- batch: 030 ----
mean loss: 40.32
train mean loss: 40.15
epoch train time: 0:00:00.609947
elapsed time: 0:02:08.671267
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 15:10:59.891949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.06
 ---- batch: 020 ----
mean loss: 39.80
 ---- batch: 030 ----
mean loss: 39.81
train mean loss: 40.09
epoch train time: 0:00:00.600311
elapsed time: 0:02:09.271979
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 15:11:00.492649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.66
 ---- batch: 020 ----
mean loss: 40.29
 ---- batch: 030 ----
mean loss: 39.47
train mean loss: 40.19
epoch train time: 0:00:00.599451
elapsed time: 0:02:09.871774
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 15:11:01.092445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.03
 ---- batch: 020 ----
mean loss: 39.40
 ---- batch: 030 ----
mean loss: 37.90
train mean loss: 38.56
epoch train time: 0:00:00.589091
elapsed time: 0:02:10.461282
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 15:11:01.681962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.30
 ---- batch: 020 ----
mean loss: 39.10
 ---- batch: 030 ----
mean loss: 38.73
train mean loss: 38.53
epoch train time: 0:00:00.590206
elapsed time: 0:02:11.051850
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 15:11:02.272521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.63
 ---- batch: 020 ----
mean loss: 38.91
 ---- batch: 030 ----
mean loss: 37.69
train mean loss: 37.96
epoch train time: 0:00:00.596006
elapsed time: 0:02:11.648230
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 15:11:02.868914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.02
 ---- batch: 020 ----
mean loss: 36.72
 ---- batch: 030 ----
mean loss: 39.85
train mean loss: 38.51
epoch train time: 0:00:00.604602
elapsed time: 0:02:12.253225
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 15:11:03.473886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.71
 ---- batch: 020 ----
mean loss: 36.39
 ---- batch: 030 ----
mean loss: 37.89
train mean loss: 37.89
epoch train time: 0:00:00.612457
elapsed time: 0:02:12.866080
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 15:11:04.086762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.72
 ---- batch: 020 ----
mean loss: 37.75
 ---- batch: 030 ----
mean loss: 38.40
train mean loss: 36.95
epoch train time: 0:00:00.596603
elapsed time: 0:02:13.463142
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 15:11:04.683746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.28
 ---- batch: 020 ----
mean loss: 37.74
 ---- batch: 030 ----
mean loss: 36.40
train mean loss: 36.31
epoch train time: 0:00:00.598551
elapsed time: 0:02:14.061965
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 15:11:05.282622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.18
 ---- batch: 020 ----
mean loss: 35.36
 ---- batch: 030 ----
mean loss: 36.56
train mean loss: 36.33
epoch train time: 0:00:00.607578
elapsed time: 0:02:14.669915
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 15:11:05.890580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.73
 ---- batch: 020 ----
mean loss: 35.29
 ---- batch: 030 ----
mean loss: 38.41
train mean loss: 36.38
epoch train time: 0:00:00.600755
elapsed time: 0:02:15.271039
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 15:11:06.491744
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.46
 ---- batch: 020 ----
mean loss: 37.21
 ---- batch: 030 ----
mean loss: 36.22
train mean loss: 37.00
epoch train time: 0:00:00.597225
elapsed time: 0:02:15.868670
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 15:11:07.089353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.75
 ---- batch: 020 ----
mean loss: 37.53
 ---- batch: 030 ----
mean loss: 36.53
train mean loss: 36.44
epoch train time: 0:00:00.583437
elapsed time: 0:02:16.452493
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 15:11:07.673162
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.13
 ---- batch: 020 ----
mean loss: 35.82
 ---- batch: 030 ----
mean loss: 36.73
train mean loss: 36.06
epoch train time: 0:00:00.600756
elapsed time: 0:02:17.053606
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 15:11:08.274264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.97
 ---- batch: 020 ----
mean loss: 36.48
 ---- batch: 030 ----
mean loss: 35.86
train mean loss: 36.28
epoch train time: 0:00:00.604046
elapsed time: 0:02:17.658004
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 15:11:08.878673
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.53
 ---- batch: 020 ----
mean loss: 36.24
 ---- batch: 030 ----
mean loss: 34.91
train mean loss: 35.81
epoch train time: 0:00:00.587668
elapsed time: 0:02:18.246006
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 15:11:09.466663
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.95
 ---- batch: 020 ----
mean loss: 36.37
 ---- batch: 030 ----
mean loss: 37.42
train mean loss: 36.53
epoch train time: 0:00:00.599424
elapsed time: 0:02:18.845761
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 15:11:10.066489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.31
 ---- batch: 020 ----
mean loss: 35.63
 ---- batch: 030 ----
mean loss: 34.96
train mean loss: 36.10
epoch train time: 0:00:00.595685
elapsed time: 0:02:19.441885
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 15:11:10.662547
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.09
 ---- batch: 020 ----
mean loss: 37.01
 ---- batch: 030 ----
mean loss: 36.37
train mean loss: 36.06
epoch train time: 0:00:00.596146
elapsed time: 0:02:20.038379
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 15:11:11.259041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.43
 ---- batch: 020 ----
mean loss: 34.58
 ---- batch: 030 ----
mean loss: 36.96
train mean loss: 36.17
epoch train time: 0:00:00.607930
elapsed time: 0:02:20.646661
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 15:11:11.867323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.76
 ---- batch: 020 ----
mean loss: 36.83
 ---- batch: 030 ----
mean loss: 36.16
train mean loss: 36.43
epoch train time: 0:00:00.595276
elapsed time: 0:02:21.242268
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 15:11:12.462925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.83
 ---- batch: 020 ----
mean loss: 36.22
 ---- batch: 030 ----
mean loss: 37.04
train mean loss: 35.73
epoch train time: 0:00:00.601994
elapsed time: 0:02:21.844585
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 15:11:13.065245
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.32
 ---- batch: 020 ----
mean loss: 37.33
 ---- batch: 030 ----
mean loss: 34.18
train mean loss: 36.27
epoch train time: 0:00:00.589672
elapsed time: 0:02:22.434659
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 15:11:13.655323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.78
 ---- batch: 020 ----
mean loss: 35.89
 ---- batch: 030 ----
mean loss: 37.57
train mean loss: 36.04
epoch train time: 0:00:00.597119
elapsed time: 0:02:23.032126
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 15:11:14.252787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.18
 ---- batch: 020 ----
mean loss: 36.42
 ---- batch: 030 ----
mean loss: 34.21
train mean loss: 36.06
epoch train time: 0:00:00.598623
elapsed time: 0:02:23.631126
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 15:11:14.851788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.03
 ---- batch: 020 ----
mean loss: 36.16
 ---- batch: 030 ----
mean loss: 35.68
train mean loss: 35.62
epoch train time: 0:00:00.597685
elapsed time: 0:02:24.229157
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 15:11:15.449836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.90
 ---- batch: 020 ----
mean loss: 35.87
 ---- batch: 030 ----
mean loss: 35.96
train mean loss: 35.77
epoch train time: 0:00:00.603118
elapsed time: 0:02:24.832618
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 15:11:16.053292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.66
 ---- batch: 020 ----
mean loss: 34.99
 ---- batch: 030 ----
mean loss: 36.39
train mean loss: 35.44
epoch train time: 0:00:00.591862
elapsed time: 0:02:25.424871
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 15:11:16.645536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.01
 ---- batch: 020 ----
mean loss: 35.58
 ---- batch: 030 ----
mean loss: 35.79
train mean loss: 35.82
epoch train time: 0:00:00.614372
elapsed time: 0:02:26.039624
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 15:11:17.260290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.45
 ---- batch: 020 ----
mean loss: 34.68
 ---- batch: 030 ----
mean loss: 38.18
train mean loss: 36.03
epoch train time: 0:00:00.607448
elapsed time: 0:02:26.647433
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 15:11:17.868103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.69
 ---- batch: 020 ----
mean loss: 34.70
 ---- batch: 030 ----
mean loss: 36.55
train mean loss: 35.90
epoch train time: 0:00:00.586807
elapsed time: 0:02:27.234604
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 15:11:18.455299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.74
 ---- batch: 020 ----
mean loss: 34.38
 ---- batch: 030 ----
mean loss: 36.01
train mean loss: 35.70
epoch train time: 0:00:00.617329
elapsed time: 0:02:27.852341
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 15:11:19.073007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.26
 ---- batch: 020 ----
mean loss: 35.69
 ---- batch: 030 ----
mean loss: 34.26
train mean loss: 35.62
epoch train time: 0:00:00.588680
elapsed time: 0:02:28.441383
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 15:11:19.662049
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.85
 ---- batch: 020 ----
mean loss: 36.32
 ---- batch: 030 ----
mean loss: 34.58
train mean loss: 35.70
epoch train time: 0:00:00.602915
elapsed time: 0:02:29.044699
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 15:11:20.265394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.54
 ---- batch: 020 ----
mean loss: 35.14
 ---- batch: 030 ----
mean loss: 36.75
train mean loss: 35.73
epoch train time: 0:00:00.617203
elapsed time: 0:02:29.662295
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 15:11:20.882973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.20
 ---- batch: 020 ----
mean loss: 35.50
 ---- batch: 030 ----
mean loss: 37.30
train mean loss: 35.79
epoch train time: 0:00:00.602681
elapsed time: 0:02:30.265331
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 15:11:21.486001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.88
 ---- batch: 020 ----
mean loss: 36.80
 ---- batch: 030 ----
mean loss: 35.11
train mean loss: 35.55
epoch train time: 0:00:00.611269
elapsed time: 0:02:30.876970
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 15:11:22.097653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.22
 ---- batch: 020 ----
mean loss: 35.19
 ---- batch: 030 ----
mean loss: 34.86
train mean loss: 35.52
epoch train time: 0:00:00.602039
elapsed time: 0:02:31.479486
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 15:11:22.700196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.32
 ---- batch: 020 ----
mean loss: 35.41
 ---- batch: 030 ----
mean loss: 34.32
train mean loss: 34.87
epoch train time: 0:00:00.584308
elapsed time: 0:02:32.064196
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 15:11:23.284886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.88
 ---- batch: 020 ----
mean loss: 35.35
 ---- batch: 030 ----
mean loss: 34.42
train mean loss: 35.60
epoch train time: 0:00:00.590446
elapsed time: 0:02:32.655106
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 15:11:23.875703
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.68
 ---- batch: 020 ----
mean loss: 36.25
 ---- batch: 030 ----
mean loss: 34.67
train mean loss: 35.61
epoch train time: 0:00:00.598626
elapsed time: 0:02:33.254020
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 15:11:24.474683
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.69
 ---- batch: 020 ----
mean loss: 35.41
 ---- batch: 030 ----
mean loss: 35.68
train mean loss: 35.41
epoch train time: 0:00:00.591424
elapsed time: 0:02:33.845789
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 15:11:25.066442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.37
 ---- batch: 020 ----
mean loss: 34.97
 ---- batch: 030 ----
mean loss: 35.07
train mean loss: 34.88
epoch train time: 0:00:00.586184
elapsed time: 0:02:34.432310
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 15:11:25.652982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.02
 ---- batch: 020 ----
mean loss: 34.52
 ---- batch: 030 ----
mean loss: 34.05
train mean loss: 35.09
epoch train time: 0:00:00.605273
elapsed time: 0:02:35.038126
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 15:11:26.258802
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.93
 ---- batch: 020 ----
mean loss: 35.34
 ---- batch: 030 ----
mean loss: 34.45
train mean loss: 35.25
epoch train time: 0:00:00.603119
elapsed time: 0:02:35.641645
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 15:11:26.862328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.54
 ---- batch: 020 ----
mean loss: 34.82
 ---- batch: 030 ----
mean loss: 33.93
train mean loss: 35.10
epoch train time: 0:00:00.617454
elapsed time: 0:02:36.259589
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 15:11:27.480318
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.50
 ---- batch: 020 ----
mean loss: 35.05
 ---- batch: 030 ----
mean loss: 34.35
train mean loss: 35.29
epoch train time: 0:00:00.613140
elapsed time: 0:02:36.873173
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 15:11:28.093830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.60
 ---- batch: 020 ----
mean loss: 35.08
 ---- batch: 030 ----
mean loss: 36.10
train mean loss: 35.22
epoch train time: 0:00:00.598686
elapsed time: 0:02:37.472248
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 15:11:28.692909
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.15
 ---- batch: 020 ----
mean loss: 33.49
 ---- batch: 030 ----
mean loss: 34.31
train mean loss: 34.56
epoch train time: 0:00:00.618595
elapsed time: 0:02:38.091182
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 15:11:29.311838
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.76
 ---- batch: 020 ----
mean loss: 35.66
 ---- batch: 030 ----
mean loss: 34.26
train mean loss: 35.12
epoch train time: 0:00:00.614101
elapsed time: 0:02:38.705661
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 15:11:29.926325
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.07
 ---- batch: 020 ----
mean loss: 35.08
 ---- batch: 030 ----
mean loss: 37.46
train mean loss: 35.34
epoch train time: 0:00:00.592546
elapsed time: 0:02:39.298612
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 15:11:30.519271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.56
 ---- batch: 020 ----
mean loss: 35.09
 ---- batch: 030 ----
mean loss: 35.03
train mean loss: 34.77
epoch train time: 0:00:00.603528
elapsed time: 0:02:39.902530
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 15:11:31.123194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.48
 ---- batch: 020 ----
mean loss: 33.98
 ---- batch: 030 ----
mean loss: 35.60
train mean loss: 34.66
epoch train time: 0:00:00.615403
elapsed time: 0:02:40.518327
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 15:11:31.739028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.82
 ---- batch: 020 ----
mean loss: 33.91
 ---- batch: 030 ----
mean loss: 34.85
train mean loss: 34.71
epoch train time: 0:00:00.614449
elapsed time: 0:02:41.133174
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 15:11:32.353836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.47
 ---- batch: 020 ----
mean loss: 35.36
 ---- batch: 030 ----
mean loss: 32.96
train mean loss: 35.17
epoch train time: 0:00:00.601667
elapsed time: 0:02:41.735223
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 15:11:32.955892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.98
 ---- batch: 020 ----
mean loss: 35.68
 ---- batch: 030 ----
mean loss: 35.64
train mean loss: 35.02
epoch train time: 0:00:00.607449
elapsed time: 0:02:42.350830
checkpoint saved in file: log/CMAPSS/FD001/min-max/bayesian_dense3/bayesian_dense3_1/checkpoint.pth.tar
**** end time: 2019-09-27 15:11:33.571406 ****
