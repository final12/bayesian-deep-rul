Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31727
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:45:56.277638 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:45:56.282767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4169.08
 ---- batch: 020 ----
mean loss: 4001.14
 ---- batch: 030 ----
mean loss: 4048.24
train mean loss: 4067.01
epoch train time: 0:00:12.336464
elapsed time: 0:00:12.342898
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:46:08.620575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3985.81
 ---- batch: 020 ----
mean loss: 3902.07
 ---- batch: 030 ----
mean loss: 3877.26
train mean loss: 3915.13
epoch train time: 0:00:00.180977
elapsed time: 0:00:12.524039
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:46:08.801771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3788.73
 ---- batch: 020 ----
mean loss: 3713.05
 ---- batch: 030 ----
mean loss: 3662.25
train mean loss: 3699.81
epoch train time: 0:00:00.177691
elapsed time: 0:00:12.701928
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:46:08.979652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3519.54
 ---- batch: 020 ----
mean loss: 3424.94
 ---- batch: 030 ----
mean loss: 3391.24
train mean loss: 3435.86
epoch train time: 0:00:00.173285
elapsed time: 0:00:12.875403
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:46:09.153103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3214.85
 ---- batch: 020 ----
mean loss: 3136.50
 ---- batch: 030 ----
mean loss: 3185.48
train mean loss: 3161.96
epoch train time: 0:00:00.169022
elapsed time: 0:00:13.044583
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:46:09.322270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2997.33
 ---- batch: 020 ----
mean loss: 2913.38
 ---- batch: 030 ----
mean loss: 2883.79
train mean loss: 2917.21
epoch train time: 0:00:00.173927
elapsed time: 0:00:13.218645
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:46:09.496330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2794.05
 ---- batch: 020 ----
mean loss: 2705.85
 ---- batch: 030 ----
mean loss: 2642.39
train mean loss: 2700.29
epoch train time: 0:00:00.169675
elapsed time: 0:00:13.388474
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:46:09.666160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2570.95
 ---- batch: 020 ----
mean loss: 2540.20
 ---- batch: 030 ----
mean loss: 2442.79
train mean loss: 2506.32
epoch train time: 0:00:00.174863
elapsed time: 0:00:13.563477
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:46:09.841190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2389.00
 ---- batch: 020 ----
mean loss: 2343.88
 ---- batch: 030 ----
mean loss: 2305.12
train mean loss: 2334.76
epoch train time: 0:00:00.176763
elapsed time: 0:00:13.740426
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:46:10.018114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2246.69
 ---- batch: 020 ----
mean loss: 2181.03
 ---- batch: 030 ----
mean loss: 2150.33
train mean loss: 2178.67
epoch train time: 0:00:00.174875
elapsed time: 0:00:13.915456
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:46:10.193141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2073.46
 ---- batch: 020 ----
mean loss: 2060.76
 ---- batch: 030 ----
mean loss: 2000.92
train mean loss: 2034.25
epoch train time: 0:00:00.172762
elapsed time: 0:00:14.088357
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:46:10.366043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1942.82
 ---- batch: 020 ----
mean loss: 1925.16
 ---- batch: 030 ----
mean loss: 1855.33
train mean loss: 1905.37
epoch train time: 0:00:00.173650
elapsed time: 0:00:14.262144
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:46:10.539831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1834.12
 ---- batch: 020 ----
mean loss: 1779.59
 ---- batch: 030 ----
mean loss: 1778.14
train mean loss: 1783.41
epoch train time: 0:00:00.175605
elapsed time: 0:00:14.437891
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:46:10.715579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1740.76
 ---- batch: 020 ----
mean loss: 1687.36
 ---- batch: 030 ----
mean loss: 1628.91
train mean loss: 1671.67
epoch train time: 0:00:00.176353
elapsed time: 0:00:14.614384
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:46:10.892071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1600.26
 ---- batch: 020 ----
mean loss: 1579.47
 ---- batch: 030 ----
mean loss: 1550.08
train mean loss: 1569.90
epoch train time: 0:00:00.177743
elapsed time: 0:00:14.792282
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:46:11.069990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1500.12
 ---- batch: 020 ----
mean loss: 1474.64
 ---- batch: 030 ----
mean loss: 1462.97
train mean loss: 1473.98
epoch train time: 0:00:00.174703
elapsed time: 0:00:14.967158
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:46:11.244846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1420.27
 ---- batch: 020 ----
mean loss: 1384.86
 ---- batch: 030 ----
mean loss: 1367.28
train mean loss: 1385.18
epoch train time: 0:00:00.172306
elapsed time: 0:00:15.139615
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:46:11.417301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1334.93
 ---- batch: 020 ----
mean loss: 1312.07
 ---- batch: 030 ----
mean loss: 1268.76
train mean loss: 1304.56
epoch train time: 0:00:00.173515
elapsed time: 0:00:15.313274
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:46:11.590964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1259.97
 ---- batch: 020 ----
mean loss: 1238.38
 ---- batch: 030 ----
mean loss: 1205.48
train mean loss: 1228.69
epoch train time: 0:00:00.171768
elapsed time: 0:00:15.485198
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:46:11.762884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1187.81
 ---- batch: 020 ----
mean loss: 1159.76
 ---- batch: 030 ----
mean loss: 1141.24
train mean loss: 1158.56
epoch train time: 0:00:00.177591
elapsed time: 0:00:15.662959
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:46:11.940661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1105.09
 ---- batch: 020 ----
mean loss: 1098.13
 ---- batch: 030 ----
mean loss: 1079.99
train mean loss: 1095.06
epoch train time: 0:00:00.175590
elapsed time: 0:00:15.838701
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:46:12.116388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1048.36
 ---- batch: 020 ----
mean loss: 1059.73
 ---- batch: 030 ----
mean loss: 1023.93
train mean loss: 1035.30
epoch train time: 0:00:00.174106
elapsed time: 0:00:16.012962
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:46:12.290665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1008.39
 ---- batch: 020 ----
mean loss: 983.25
 ---- batch: 030 ----
mean loss: 972.73
train mean loss: 980.42
epoch train time: 0:00:00.171933
elapsed time: 0:00:16.185048
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:46:12.462733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.22
 ---- batch: 020 ----
mean loss: 946.42
 ---- batch: 030 ----
mean loss: 929.85
train mean loss: 929.30
epoch train time: 0:00:00.174500
elapsed time: 0:00:16.359694
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:46:12.637392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.90
 ---- batch: 020 ----
mean loss: 899.14
 ---- batch: 030 ----
mean loss: 865.94
train mean loss: 882.95
epoch train time: 0:00:00.175088
elapsed time: 0:00:16.534936
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:46:12.812626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.86
 ---- batch: 020 ----
mean loss: 858.39
 ---- batch: 030 ----
mean loss: 836.35
train mean loss: 841.00
epoch train time: 0:00:00.176705
elapsed time: 0:00:16.711814
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:46:12.989528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.92
 ---- batch: 020 ----
mean loss: 808.33
 ---- batch: 030 ----
mean loss: 778.45
train mean loss: 802.22
epoch train time: 0:00:00.174896
elapsed time: 0:00:16.886892
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:46:13.164578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 781.48
 ---- batch: 020 ----
mean loss: 768.45
 ---- batch: 030 ----
mean loss: 750.06
train mean loss: 764.96
epoch train time: 0:00:00.175301
elapsed time: 0:00:17.062331
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:46:13.340030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.39
 ---- batch: 020 ----
mean loss: 748.85
 ---- batch: 030 ----
mean loss: 723.82
train mean loss: 731.23
epoch train time: 0:00:00.174742
elapsed time: 0:00:17.237224
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:46:13.514924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 709.67
 ---- batch: 020 ----
mean loss: 700.51
 ---- batch: 030 ----
mean loss: 694.76
train mean loss: 701.46
epoch train time: 0:00:00.173987
elapsed time: 0:00:17.411386
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:46:13.689083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 678.96
 ---- batch: 020 ----
mean loss: 666.57
 ---- batch: 030 ----
mean loss: 681.98
train mean loss: 673.73
epoch train time: 0:00:00.178437
elapsed time: 0:00:17.589988
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:46:13.867678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 657.97
 ---- batch: 020 ----
mean loss: 653.96
 ---- batch: 030 ----
mean loss: 638.31
train mean loss: 646.98
epoch train time: 0:00:00.179441
elapsed time: 0:00:17.769574
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:46:14.047261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 635.39
 ---- batch: 020 ----
mean loss: 622.48
 ---- batch: 030 ----
mean loss: 609.95
train mean loss: 623.98
epoch train time: 0:00:00.178722
elapsed time: 0:00:17.948451
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:46:14.226141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.31
 ---- batch: 020 ----
mean loss: 611.30
 ---- batch: 030 ----
mean loss: 592.26
train mean loss: 602.62
epoch train time: 0:00:00.175125
elapsed time: 0:00:18.123721
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:46:14.401409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 584.25
 ---- batch: 020 ----
mean loss: 591.04
 ---- batch: 030 ----
mean loss: 576.41
train mean loss: 582.79
epoch train time: 0:00:00.176428
elapsed time: 0:00:18.300301
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:46:14.577987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 567.19
 ---- batch: 020 ----
mean loss: 562.34
 ---- batch: 030 ----
mean loss: 571.08
train mean loss: 565.24
epoch train time: 0:00:00.184077
elapsed time: 0:00:18.484530
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:46:14.762225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 554.46
 ---- batch: 020 ----
mean loss: 551.16
 ---- batch: 030 ----
mean loss: 544.88
train mean loss: 547.75
epoch train time: 0:00:00.178302
elapsed time: 0:00:18.662979
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:46:14.940665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.63
 ---- batch: 020 ----
mean loss: 529.37
 ---- batch: 030 ----
mean loss: 533.69
train mean loss: 532.85
epoch train time: 0:00:00.173821
elapsed time: 0:00:18.836937
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:46:15.114624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.03
 ---- batch: 020 ----
mean loss: 520.22
 ---- batch: 030 ----
mean loss: 513.77
train mean loss: 519.33
epoch train time: 0:00:00.173666
elapsed time: 0:00:19.010743
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:46:15.288428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.81
 ---- batch: 020 ----
mean loss: 505.85
 ---- batch: 030 ----
mean loss: 506.60
train mean loss: 506.44
epoch train time: 0:00:00.172336
elapsed time: 0:00:19.183216
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:46:15.460911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.29
 ---- batch: 020 ----
mean loss: 491.94
 ---- batch: 030 ----
mean loss: 493.15
train mean loss: 494.72
epoch train time: 0:00:00.172601
elapsed time: 0:00:19.356014
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:46:15.633734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.98
 ---- batch: 020 ----
mean loss: 477.53
 ---- batch: 030 ----
mean loss: 485.37
train mean loss: 483.79
epoch train time: 0:00:00.182152
elapsed time: 0:00:19.538340
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:46:15.816043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.14
 ---- batch: 020 ----
mean loss: 472.26
 ---- batch: 030 ----
mean loss: 469.50
train mean loss: 473.75
epoch train time: 0:00:00.176622
elapsed time: 0:00:19.715117
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:46:15.992805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.06
 ---- batch: 020 ----
mean loss: 473.02
 ---- batch: 030 ----
mean loss: 463.78
train mean loss: 465.03
epoch train time: 0:00:00.172190
elapsed time: 0:00:19.887445
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:46:16.165130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.77
 ---- batch: 020 ----
mean loss: 452.11
 ---- batch: 030 ----
mean loss: 459.36
train mean loss: 456.40
epoch train time: 0:00:00.172968
elapsed time: 0:00:20.060552
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:46:16.338240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.02
 ---- batch: 020 ----
mean loss: 447.03
 ---- batch: 030 ----
mean loss: 445.06
train mean loss: 448.96
epoch train time: 0:00:00.175842
elapsed time: 0:00:20.236538
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:46:16.514225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.39
 ---- batch: 020 ----
mean loss: 442.36
 ---- batch: 030 ----
mean loss: 440.85
train mean loss: 441.17
epoch train time: 0:00:00.174985
elapsed time: 0:00:20.411678
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:46:16.689381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.79
 ---- batch: 020 ----
mean loss: 439.80
 ---- batch: 030 ----
mean loss: 431.96
train mean loss: 434.60
epoch train time: 0:00:00.174523
elapsed time: 0:00:20.586372
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:46:16.864067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.25
 ---- batch: 020 ----
mean loss: 428.44
 ---- batch: 030 ----
mean loss: 422.60
train mean loss: 428.04
epoch train time: 0:00:00.193454
elapsed time: 0:00:20.780023
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:46:17.057727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.78
 ---- batch: 020 ----
mean loss: 422.56
 ---- batch: 030 ----
mean loss: 418.94
train mean loss: 422.80
epoch train time: 0:00:00.171407
elapsed time: 0:00:20.951587
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:46:17.229274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.20
 ---- batch: 020 ----
mean loss: 416.02
 ---- batch: 030 ----
mean loss: 419.21
train mean loss: 417.24
epoch train time: 0:00:00.172547
elapsed time: 0:00:21.124275
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:46:17.401960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.65
 ---- batch: 020 ----
mean loss: 404.56
 ---- batch: 030 ----
mean loss: 416.64
train mean loss: 412.60
epoch train time: 0:00:00.173609
elapsed time: 0:00:21.298032
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:46:17.575718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.04
 ---- batch: 020 ----
mean loss: 411.69
 ---- batch: 030 ----
mean loss: 400.32
train mean loss: 407.86
epoch train time: 0:00:00.179717
elapsed time: 0:00:21.477889
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:46:17.755577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.42
 ---- batch: 020 ----
mean loss: 401.63
 ---- batch: 030 ----
mean loss: 406.38
train mean loss: 404.07
epoch train time: 0:00:00.174750
elapsed time: 0:00:21.652779
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:46:17.930465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.62
 ---- batch: 020 ----
mean loss: 403.63
 ---- batch: 030 ----
mean loss: 394.07
train mean loss: 400.45
epoch train time: 0:00:00.173191
elapsed time: 0:00:21.826107
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:46:18.103810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.25
 ---- batch: 020 ----
mean loss: 396.10
 ---- batch: 030 ----
mean loss: 399.35
train mean loss: 397.18
epoch train time: 0:00:00.171159
elapsed time: 0:00:21.997484
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:46:18.275199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.90
 ---- batch: 020 ----
mean loss: 401.81
 ---- batch: 030 ----
mean loss: 391.18
train mean loss: 394.66
epoch train time: 0:00:00.171597
elapsed time: 0:00:22.169247
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:46:18.446931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.61
 ---- batch: 020 ----
mean loss: 391.75
 ---- batch: 030 ----
mean loss: 392.31
train mean loss: 391.73
epoch train time: 0:00:00.169778
elapsed time: 0:00:22.339163
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:46:18.616849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.47
 ---- batch: 020 ----
mean loss: 398.83
 ---- batch: 030 ----
mean loss: 389.65
train mean loss: 389.39
epoch train time: 0:00:00.182024
elapsed time: 0:00:22.521360
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:46:18.799047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.54
 ---- batch: 020 ----
mean loss: 390.23
 ---- batch: 030 ----
mean loss: 382.13
train mean loss: 387.42
epoch train time: 0:00:00.181295
elapsed time: 0:00:22.702807
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:46:18.980496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.28
 ---- batch: 020 ----
mean loss: 378.87
 ---- batch: 030 ----
mean loss: 382.47
train mean loss: 386.05
epoch train time: 0:00:00.175301
elapsed time: 0:00:22.878245
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:46:19.155947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.19
 ---- batch: 020 ----
mean loss: 382.68
 ---- batch: 030 ----
mean loss: 390.20
train mean loss: 384.09
epoch train time: 0:00:00.170517
elapsed time: 0:00:23.048913
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:46:19.326598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.99
 ---- batch: 020 ----
mean loss: 385.93
 ---- batch: 030 ----
mean loss: 378.87
train mean loss: 382.63
epoch train time: 0:00:00.171128
elapsed time: 0:00:23.220179
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:46:19.497865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.72
 ---- batch: 020 ----
mean loss: 382.98
 ---- batch: 030 ----
mean loss: 379.95
train mean loss: 380.95
epoch train time: 0:00:00.170865
elapsed time: 0:00:23.391183
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:46:19.668870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.42
 ---- batch: 020 ----
mean loss: 376.28
 ---- batch: 030 ----
mean loss: 379.35
train mean loss: 380.06
epoch train time: 0:00:00.184814
elapsed time: 0:00:23.576146
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:46:19.853834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.26
 ---- batch: 020 ----
mean loss: 374.45
 ---- batch: 030 ----
mean loss: 377.40
train mean loss: 378.90
epoch train time: 0:00:00.175191
elapsed time: 0:00:23.751492
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:46:20.029193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.20
 ---- batch: 020 ----
mean loss: 374.71
 ---- batch: 030 ----
mean loss: 374.75
train mean loss: 377.62
epoch train time: 0:00:00.172052
elapsed time: 0:00:23.923698
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:46:20.201384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.70
 ---- batch: 020 ----
mean loss: 373.38
 ---- batch: 030 ----
mean loss: 378.07
train mean loss: 376.97
epoch train time: 0:00:00.175387
elapsed time: 0:00:24.099245
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:46:20.376936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.76
 ---- batch: 020 ----
mean loss: 379.19
 ---- batch: 030 ----
mean loss: 375.53
train mean loss: 375.63
epoch train time: 0:00:00.179323
elapsed time: 0:00:24.278751
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:46:20.556492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.14
 ---- batch: 020 ----
mean loss: 375.19
 ---- batch: 030 ----
mean loss: 374.83
train mean loss: 375.20
epoch train time: 0:00:00.172694
elapsed time: 0:00:24.451637
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:46:20.729322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.33
 ---- batch: 020 ----
mean loss: 366.16
 ---- batch: 030 ----
mean loss: 379.39
train mean loss: 374.11
epoch train time: 0:00:00.173869
elapsed time: 0:00:24.625652
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:46:20.903348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.99
 ---- batch: 020 ----
mean loss: 372.15
 ---- batch: 030 ----
mean loss: 367.78
train mean loss: 373.83
epoch train time: 0:00:00.175270
elapsed time: 0:00:24.801071
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:46:21.078759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.68
 ---- batch: 020 ----
mean loss: 374.18
 ---- batch: 030 ----
mean loss: 369.62
train mean loss: 372.84
epoch train time: 0:00:00.174168
elapsed time: 0:00:24.975379
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:46:21.253065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.30
 ---- batch: 020 ----
mean loss: 380.22
 ---- batch: 030 ----
mean loss: 368.79
train mean loss: 372.28
epoch train time: 0:00:00.171431
elapsed time: 0:00:25.146949
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:46:21.424636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.34
 ---- batch: 020 ----
mean loss: 368.12
 ---- batch: 030 ----
mean loss: 374.70
train mean loss: 372.07
epoch train time: 0:00:00.171328
elapsed time: 0:00:25.318414
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:46:21.596101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.03
 ---- batch: 020 ----
mean loss: 375.39
 ---- batch: 030 ----
mean loss: 372.32
train mean loss: 371.56
epoch train time: 0:00:00.169899
elapsed time: 0:00:25.488474
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:46:21.766160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.45
 ---- batch: 020 ----
mean loss: 366.56
 ---- batch: 030 ----
mean loss: 371.47
train mean loss: 371.30
epoch train time: 0:00:00.173812
elapsed time: 0:00:25.662422
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:46:21.940123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.08
 ---- batch: 020 ----
mean loss: 373.80
 ---- batch: 030 ----
mean loss: 374.00
train mean loss: 370.52
epoch train time: 0:00:00.174040
elapsed time: 0:00:25.836617
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:46:22.114305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.88
 ---- batch: 020 ----
mean loss: 364.35
 ---- batch: 030 ----
mean loss: 366.34
train mean loss: 370.57
epoch train time: 0:00:00.170104
elapsed time: 0:00:26.006894
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:46:22.284606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.89
 ---- batch: 020 ----
mean loss: 370.13
 ---- batch: 030 ----
mean loss: 373.07
train mean loss: 370.19
epoch train time: 0:00:00.173934
elapsed time: 0:00:26.181000
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:46:22.458687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.18
 ---- batch: 020 ----
mean loss: 368.71
 ---- batch: 030 ----
mean loss: 380.78
train mean loss: 369.31
epoch train time: 0:00:00.171585
elapsed time: 0:00:26.352723
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:46:22.630407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.18
 ---- batch: 020 ----
mean loss: 371.13
 ---- batch: 030 ----
mean loss: 369.01
train mean loss: 369.24
epoch train time: 0:00:00.175410
elapsed time: 0:00:26.528274
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:46:22.805991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.35
 ---- batch: 020 ----
mean loss: 363.17
 ---- batch: 030 ----
mean loss: 370.36
train mean loss: 369.07
epoch train time: 0:00:00.177732
elapsed time: 0:00:26.706174
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:46:22.983864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.91
 ---- batch: 020 ----
mean loss: 372.00
 ---- batch: 030 ----
mean loss: 366.76
train mean loss: 368.56
epoch train time: 0:00:00.173639
elapsed time: 0:00:26.879996
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:46:23.157692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.55
 ---- batch: 020 ----
mean loss: 364.24
 ---- batch: 030 ----
mean loss: 368.50
train mean loss: 368.61
epoch train time: 0:00:00.170889
elapsed time: 0:00:27.051031
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:46:23.328716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.80
 ---- batch: 020 ----
mean loss: 373.58
 ---- batch: 030 ----
mean loss: 366.79
train mean loss: 367.92
epoch train time: 0:00:00.172921
elapsed time: 0:00:27.224106
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:46:23.501792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.15
 ---- batch: 020 ----
mean loss: 368.74
 ---- batch: 030 ----
mean loss: 364.50
train mean loss: 367.71
epoch train time: 0:00:00.171545
elapsed time: 0:00:27.395805
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:46:23.673517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.49
 ---- batch: 020 ----
mean loss: 377.80
 ---- batch: 030 ----
mean loss: 359.39
train mean loss: 367.43
epoch train time: 0:00:00.176918
elapsed time: 0:00:27.572898
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:46:23.850595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.96
 ---- batch: 020 ----
mean loss: 363.03
 ---- batch: 030 ----
mean loss: 362.36
train mean loss: 367.36
epoch train time: 0:00:00.175908
elapsed time: 0:00:27.748957
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:46:24.026643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.69
 ---- batch: 020 ----
mean loss: 363.46
 ---- batch: 030 ----
mean loss: 370.86
train mean loss: 366.83
epoch train time: 0:00:00.171971
elapsed time: 0:00:27.921080
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:46:24.198767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.53
 ---- batch: 020 ----
mean loss: 369.40
 ---- batch: 030 ----
mean loss: 370.12
train mean loss: 366.83
epoch train time: 0:00:00.171106
elapsed time: 0:00:28.092336
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:46:24.370023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.24
 ---- batch: 020 ----
mean loss: 366.30
 ---- batch: 030 ----
mean loss: 369.43
train mean loss: 366.26
epoch train time: 0:00:00.177490
elapsed time: 0:00:28.269994
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:46:24.547694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.43
 ---- batch: 020 ----
mean loss: 366.60
 ---- batch: 030 ----
mean loss: 363.10
train mean loss: 366.22
epoch train time: 0:00:00.179855
elapsed time: 0:00:28.450012
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:46:24.727706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.93
 ---- batch: 020 ----
mean loss: 364.59
 ---- batch: 030 ----
mean loss: 367.01
train mean loss: 366.12
epoch train time: 0:00:00.177279
elapsed time: 0:00:28.627489
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:46:24.905218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.56
 ---- batch: 020 ----
mean loss: 360.10
 ---- batch: 030 ----
mean loss: 373.48
train mean loss: 366.15
epoch train time: 0:00:00.181347
elapsed time: 0:00:28.809019
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:46:25.086704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.69
 ---- batch: 020 ----
mean loss: 362.93
 ---- batch: 030 ----
mean loss: 370.96
train mean loss: 365.38
epoch train time: 0:00:00.174551
elapsed time: 0:00:28.983704
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:46:25.261416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.21
 ---- batch: 020 ----
mean loss: 364.78
 ---- batch: 030 ----
mean loss: 369.93
train mean loss: 365.30
epoch train time: 0:00:00.176555
elapsed time: 0:00:29.160458
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:46:25.438187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.50
 ---- batch: 020 ----
mean loss: 362.89
 ---- batch: 030 ----
mean loss: 361.75
train mean loss: 365.30
epoch train time: 0:00:00.173203
elapsed time: 0:00:29.333840
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:46:25.611526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.16
 ---- batch: 020 ----
mean loss: 363.83
 ---- batch: 030 ----
mean loss: 362.24
train mean loss: 365.33
epoch train time: 0:00:00.176418
elapsed time: 0:00:29.510395
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:46:25.788078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.88
 ---- batch: 020 ----
mean loss: 361.27
 ---- batch: 030 ----
mean loss: 366.84
train mean loss: 364.61
epoch train time: 0:00:00.180006
elapsed time: 0:00:29.690589
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:46:25.968277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.32
 ---- batch: 020 ----
mean loss: 367.48
 ---- batch: 030 ----
mean loss: 359.26
train mean loss: 364.99
epoch train time: 0:00:00.179514
elapsed time: 0:00:29.870248
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:46:26.147935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.52
 ---- batch: 020 ----
mean loss: 369.37
 ---- batch: 030 ----
mean loss: 364.81
train mean loss: 364.48
epoch train time: 0:00:00.177032
elapsed time: 0:00:30.047433
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:46:26.325118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.60
 ---- batch: 020 ----
mean loss: 365.04
 ---- batch: 030 ----
mean loss: 364.67
train mean loss: 363.93
epoch train time: 0:00:00.177747
elapsed time: 0:00:30.225336
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:46:26.503022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.09
 ---- batch: 020 ----
mean loss: 369.78
 ---- batch: 030 ----
mean loss: 364.60
train mean loss: 364.19
epoch train time: 0:00:00.173492
elapsed time: 0:00:30.398981
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:46:26.676666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.16
 ---- batch: 020 ----
mean loss: 370.37
 ---- batch: 030 ----
mean loss: 365.95
train mean loss: 363.74
epoch train time: 0:00:00.176985
elapsed time: 0:00:30.576119
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:46:26.853807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.77
 ---- batch: 020 ----
mean loss: 364.25
 ---- batch: 030 ----
mean loss: 369.79
train mean loss: 363.74
epoch train time: 0:00:00.174643
elapsed time: 0:00:30.750908
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:46:27.028594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.24
 ---- batch: 020 ----
mean loss: 366.19
 ---- batch: 030 ----
mean loss: 365.69
train mean loss: 363.58
epoch train time: 0:00:00.175194
elapsed time: 0:00:30.926250
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:46:27.203928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.54
 ---- batch: 020 ----
mean loss: 355.86
 ---- batch: 030 ----
mean loss: 367.27
train mean loss: 363.35
epoch train time: 0:00:00.171841
elapsed time: 0:00:31.098237
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:46:27.375955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.19
 ---- batch: 020 ----
mean loss: 370.17
 ---- batch: 030 ----
mean loss: 358.58
train mean loss: 363.42
epoch train time: 0:00:00.173141
elapsed time: 0:00:31.271549
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:46:27.549251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.24
 ---- batch: 020 ----
mean loss: 357.16
 ---- batch: 030 ----
mean loss: 365.39
train mean loss: 363.22
epoch train time: 0:00:00.173687
elapsed time: 0:00:31.445406
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:46:27.723101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.40
 ---- batch: 020 ----
mean loss: 362.15
 ---- batch: 030 ----
mean loss: 363.65
train mean loss: 362.47
epoch train time: 0:00:00.179948
elapsed time: 0:00:31.625515
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:46:27.903201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.09
 ---- batch: 020 ----
mean loss: 361.77
 ---- batch: 030 ----
mean loss: 365.43
train mean loss: 362.53
epoch train time: 0:00:00.177149
elapsed time: 0:00:31.802807
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:46:28.080496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.56
 ---- batch: 020 ----
mean loss: 359.48
 ---- batch: 030 ----
mean loss: 361.02
train mean loss: 363.00
epoch train time: 0:00:00.176076
elapsed time: 0:00:31.979023
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:46:28.256710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.72
 ---- batch: 020 ----
mean loss: 361.74
 ---- batch: 030 ----
mean loss: 366.01
train mean loss: 362.20
epoch train time: 0:00:00.171642
elapsed time: 0:00:32.150802
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:46:28.428502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.88
 ---- batch: 020 ----
mean loss: 364.85
 ---- batch: 030 ----
mean loss: 370.19
train mean loss: 362.16
epoch train time: 0:00:00.174117
elapsed time: 0:00:32.325078
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:46:28.602788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.95
 ---- batch: 020 ----
mean loss: 363.10
 ---- batch: 030 ----
mean loss: 360.64
train mean loss: 362.47
epoch train time: 0:00:00.180886
elapsed time: 0:00:32.506125
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:46:28.783810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.61
 ---- batch: 020 ----
mean loss: 362.89
 ---- batch: 030 ----
mean loss: 363.41
train mean loss: 361.61
epoch train time: 0:00:00.178343
elapsed time: 0:00:32.684606
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:46:28.962291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.31
 ---- batch: 020 ----
mean loss: 364.52
 ---- batch: 030 ----
mean loss: 360.15
train mean loss: 361.14
epoch train time: 0:00:00.175108
elapsed time: 0:00:32.859872
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:46:29.137559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.60
 ---- batch: 020 ----
mean loss: 365.21
 ---- batch: 030 ----
mean loss: 360.60
train mean loss: 361.64
epoch train time: 0:00:00.176843
elapsed time: 0:00:33.036879
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:46:29.314564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.71
 ---- batch: 020 ----
mean loss: 358.74
 ---- batch: 030 ----
mean loss: 359.95
train mean loss: 361.35
epoch train time: 0:00:00.173970
elapsed time: 0:00:33.211042
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:46:29.488741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.78
 ---- batch: 020 ----
mean loss: 358.05
 ---- batch: 030 ----
mean loss: 358.96
train mean loss: 361.31
epoch train time: 0:00:00.176332
elapsed time: 0:00:33.387541
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:46:29.665228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.76
 ---- batch: 020 ----
mean loss: 358.81
 ---- batch: 030 ----
mean loss: 367.94
train mean loss: 360.84
epoch train time: 0:00:00.176897
elapsed time: 0:00:33.564606
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:46:29.842306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.35
 ---- batch: 020 ----
mean loss: 359.19
 ---- batch: 030 ----
mean loss: 364.74
train mean loss: 360.98
epoch train time: 0:00:00.179915
elapsed time: 0:00:33.744675
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:46:30.022379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.14
 ---- batch: 020 ----
mean loss: 362.61
 ---- batch: 030 ----
mean loss: 359.36
train mean loss: 360.57
epoch train time: 0:00:00.177730
elapsed time: 0:00:33.922568
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:46:30.200255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.72
 ---- batch: 020 ----
mean loss: 355.05
 ---- batch: 030 ----
mean loss: 363.51
train mean loss: 360.29
epoch train time: 0:00:00.174581
elapsed time: 0:00:34.097295
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:46:30.374981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.15
 ---- batch: 020 ----
mean loss: 358.37
 ---- batch: 030 ----
mean loss: 352.10
train mean loss: 360.69
epoch train time: 0:00:00.174211
elapsed time: 0:00:34.271645
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:46:30.549332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.17
 ---- batch: 020 ----
mean loss: 366.25
 ---- batch: 030 ----
mean loss: 360.92
train mean loss: 360.16
epoch train time: 0:00:00.173612
elapsed time: 0:00:34.445474
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:46:30.723165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.21
 ---- batch: 020 ----
mean loss: 359.02
 ---- batch: 030 ----
mean loss: 364.61
train mean loss: 360.11
epoch train time: 0:00:00.178072
elapsed time: 0:00:34.623692
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:46:30.901377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.09
 ---- batch: 020 ----
mean loss: 362.91
 ---- batch: 030 ----
mean loss: 361.10
train mean loss: 359.72
epoch train time: 0:00:00.174153
elapsed time: 0:00:34.797984
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:46:31.075670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.39
 ---- batch: 020 ----
mean loss: 361.45
 ---- batch: 030 ----
mean loss: 361.72
train mean loss: 359.57
epoch train time: 0:00:00.172570
elapsed time: 0:00:34.970691
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:46:31.248394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.40
 ---- batch: 020 ----
mean loss: 359.43
 ---- batch: 030 ----
mean loss: 370.08
train mean loss: 359.89
epoch train time: 0:00:00.171941
elapsed time: 0:00:35.142787
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:46:31.420473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.74
 ---- batch: 020 ----
mean loss: 371.94
 ---- batch: 030 ----
mean loss: 352.70
train mean loss: 359.49
epoch train time: 0:00:00.177386
elapsed time: 0:00:35.320359
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:46:31.598043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.89
 ---- batch: 020 ----
mean loss: 363.59
 ---- batch: 030 ----
mean loss: 366.91
train mean loss: 359.37
epoch train time: 0:00:00.177973
elapsed time: 0:00:35.498467
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:46:31.776153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.41
 ---- batch: 020 ----
mean loss: 357.30
 ---- batch: 030 ----
mean loss: 352.59
train mean loss: 359.15
epoch train time: 0:00:00.176846
elapsed time: 0:00:35.675460
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:46:31.953146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.07
 ---- batch: 020 ----
mean loss: 355.91
 ---- batch: 030 ----
mean loss: 365.50
train mean loss: 358.87
epoch train time: 0:00:00.176225
elapsed time: 0:00:35.851822
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:46:32.129508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.15
 ---- batch: 020 ----
mean loss: 361.31
 ---- batch: 030 ----
mean loss: 366.35
train mean loss: 359.05
epoch train time: 0:00:00.174463
elapsed time: 0:00:36.026420
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:46:32.304104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.44
 ---- batch: 020 ----
mean loss: 354.69
 ---- batch: 030 ----
mean loss: 354.68
train mean loss: 358.89
epoch train time: 0:00:00.169158
elapsed time: 0:00:36.195741
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:46:32.473460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.94
 ---- batch: 020 ----
mean loss: 357.17
 ---- batch: 030 ----
mean loss: 360.44
train mean loss: 358.66
epoch train time: 0:00:00.172132
elapsed time: 0:00:36.368062
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:46:32.645747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.86
 ---- batch: 020 ----
mean loss: 355.39
 ---- batch: 030 ----
mean loss: 361.68
train mean loss: 358.13
epoch train time: 0:00:00.192041
elapsed time: 0:00:36.560246
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:46:32.837951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.31
 ---- batch: 020 ----
mean loss: 355.14
 ---- batch: 030 ----
mean loss: 351.45
train mean loss: 358.44
epoch train time: 0:00:00.177431
elapsed time: 0:00:36.737834
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:46:33.015523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.59
 ---- batch: 020 ----
mean loss: 360.55
 ---- batch: 030 ----
mean loss: 353.28
train mean loss: 358.48
epoch train time: 0:00:00.170594
elapsed time: 0:00:36.908585
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:46:33.186271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.34
 ---- batch: 020 ----
mean loss: 356.73
 ---- batch: 030 ----
mean loss: 357.63
train mean loss: 358.33
epoch train time: 0:00:00.168146
elapsed time: 0:00:37.076866
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:46:33.354551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.74
 ---- batch: 020 ----
mean loss: 362.93
 ---- batch: 030 ----
mean loss: 357.39
train mean loss: 357.93
epoch train time: 0:00:00.168403
elapsed time: 0:00:37.245421
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:46:33.523105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.21
 ---- batch: 020 ----
mean loss: 365.00
 ---- batch: 030 ----
mean loss: 353.63
train mean loss: 357.70
epoch train time: 0:00:00.174135
elapsed time: 0:00:37.419702
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:46:33.697407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.49
 ---- batch: 020 ----
mean loss: 357.62
 ---- batch: 030 ----
mean loss: 357.55
train mean loss: 357.45
epoch train time: 0:00:00.186433
elapsed time: 0:00:37.606296
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:46:33.883982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.96
 ---- batch: 020 ----
mean loss: 359.25
 ---- batch: 030 ----
mean loss: 355.14
train mean loss: 357.51
epoch train time: 0:00:00.177350
elapsed time: 0:00:37.783794
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:46:34.061481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.39
 ---- batch: 020 ----
mean loss: 358.27
 ---- batch: 030 ----
mean loss: 358.47
train mean loss: 357.19
epoch train time: 0:00:00.174516
elapsed time: 0:00:37.958448
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:46:34.236133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.25
 ---- batch: 020 ----
mean loss: 352.89
 ---- batch: 030 ----
mean loss: 357.03
train mean loss: 356.94
epoch train time: 0:00:00.175692
elapsed time: 0:00:38.134286
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:46:34.411976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.14
 ---- batch: 020 ----
mean loss: 353.76
 ---- batch: 030 ----
mean loss: 361.06
train mean loss: 356.90
epoch train time: 0:00:00.177605
elapsed time: 0:00:38.312086
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:46:34.589767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.09
 ---- batch: 020 ----
mean loss: 357.20
 ---- batch: 030 ----
mean loss: 363.46
train mean loss: 356.75
epoch train time: 0:00:00.179860
elapsed time: 0:00:38.492081
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:46:34.769847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.22
 ---- batch: 020 ----
mean loss: 362.00
 ---- batch: 030 ----
mean loss: 350.89
train mean loss: 356.68
epoch train time: 0:00:00.179342
elapsed time: 0:00:38.671648
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:46:34.949335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.41
 ---- batch: 020 ----
mean loss: 363.47
 ---- batch: 030 ----
mean loss: 353.04
train mean loss: 356.56
epoch train time: 0:00:00.175149
elapsed time: 0:00:38.846937
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:46:35.124623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.90
 ---- batch: 020 ----
mean loss: 354.87
 ---- batch: 030 ----
mean loss: 353.64
train mean loss: 356.82
epoch train time: 0:00:00.174045
elapsed time: 0:00:39.021121
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:46:35.298807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.91
 ---- batch: 020 ----
mean loss: 355.25
 ---- batch: 030 ----
mean loss: 360.82
train mean loss: 356.21
epoch train time: 0:00:00.175162
elapsed time: 0:00:39.196425
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:46:35.474111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.74
 ---- batch: 020 ----
mean loss: 358.64
 ---- batch: 030 ----
mean loss: 360.02
train mean loss: 356.04
epoch train time: 0:00:00.174581
elapsed time: 0:00:39.371145
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:46:35.648831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.04
 ---- batch: 020 ----
mean loss: 353.41
 ---- batch: 030 ----
mean loss: 353.75
train mean loss: 356.30
epoch train time: 0:00:00.174941
elapsed time: 0:00:39.546253
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:46:35.823947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.25
 ---- batch: 020 ----
mean loss: 356.38
 ---- batch: 030 ----
mean loss: 352.07
train mean loss: 355.83
epoch train time: 0:00:00.172666
elapsed time: 0:00:39.719061
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:46:35.996761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.06
 ---- batch: 020 ----
mean loss: 360.33
 ---- batch: 030 ----
mean loss: 357.67
train mean loss: 355.84
epoch train time: 0:00:00.168246
elapsed time: 0:00:39.887453
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:46:36.165155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.31
 ---- batch: 020 ----
mean loss: 351.11
 ---- batch: 030 ----
mean loss: 355.94
train mean loss: 355.50
epoch train time: 0:00:00.170809
elapsed time: 0:00:40.058427
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:46:36.336111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.20
 ---- batch: 020 ----
mean loss: 358.83
 ---- batch: 030 ----
mean loss: 352.13
train mean loss: 355.57
epoch train time: 0:00:00.172169
elapsed time: 0:00:40.230729
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:46:36.508413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.29
 ---- batch: 020 ----
mean loss: 365.51
 ---- batch: 030 ----
mean loss: 348.24
train mean loss: 354.81
epoch train time: 0:00:00.173022
elapsed time: 0:00:40.403896
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:46:36.681592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.51
 ---- batch: 020 ----
mean loss: 351.63
 ---- batch: 030 ----
mean loss: 359.69
train mean loss: 355.01
epoch train time: 0:00:00.181060
elapsed time: 0:00:40.585139
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:46:36.862826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.79
 ---- batch: 020 ----
mean loss: 358.42
 ---- batch: 030 ----
mean loss: 358.38
train mean loss: 355.17
epoch train time: 0:00:00.176829
elapsed time: 0:00:40.762106
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:46:37.039790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.73
 ---- batch: 020 ----
mean loss: 357.31
 ---- batch: 030 ----
mean loss: 354.35
train mean loss: 355.03
epoch train time: 0:00:00.176284
elapsed time: 0:00:40.938543
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:46:37.216246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.21
 ---- batch: 020 ----
mean loss: 361.12
 ---- batch: 030 ----
mean loss: 352.69
train mean loss: 354.69
epoch train time: 0:00:00.174162
elapsed time: 0:00:41.112867
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:46:37.390552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.01
 ---- batch: 020 ----
mean loss: 360.27
 ---- batch: 030 ----
mean loss: 349.22
train mean loss: 354.14
epoch train time: 0:00:00.174776
elapsed time: 0:00:41.287782
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:46:37.565466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.23
 ---- batch: 020 ----
mean loss: 353.95
 ---- batch: 030 ----
mean loss: 355.37
train mean loss: 354.34
epoch train time: 0:00:00.170341
elapsed time: 0:00:41.458258
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:46:37.735959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.06
 ---- batch: 020 ----
mean loss: 347.01
 ---- batch: 030 ----
mean loss: 362.91
train mean loss: 354.71
epoch train time: 0:00:00.178937
elapsed time: 0:00:41.637412
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:46:37.915126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.95
 ---- batch: 020 ----
mean loss: 352.56
 ---- batch: 030 ----
mean loss: 354.56
train mean loss: 354.28
epoch train time: 0:00:00.175637
elapsed time: 0:00:41.813217
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:46:38.090911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.92
 ---- batch: 020 ----
mean loss: 357.32
 ---- batch: 030 ----
mean loss: 351.11
train mean loss: 354.00
epoch train time: 0:00:00.169060
elapsed time: 0:00:41.982421
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:46:38.260106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.28
 ---- batch: 020 ----
mean loss: 353.53
 ---- batch: 030 ----
mean loss: 352.10
train mean loss: 353.79
epoch train time: 0:00:00.171612
elapsed time: 0:00:42.154184
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:46:38.431869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.58
 ---- batch: 020 ----
mean loss: 360.33
 ---- batch: 030 ----
mean loss: 361.04
train mean loss: 353.63
epoch train time: 0:00:00.173625
elapsed time: 0:00:42.327946
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:46:38.605655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.03
 ---- batch: 020 ----
mean loss: 348.48
 ---- batch: 030 ----
mean loss: 355.44
train mean loss: 354.02
epoch train time: 0:00:00.180747
elapsed time: 0:00:42.508854
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:46:38.786550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.70
 ---- batch: 020 ----
mean loss: 356.89
 ---- batch: 030 ----
mean loss: 350.12
train mean loss: 353.64
epoch train time: 0:00:00.174363
elapsed time: 0:00:42.683377
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:46:38.961052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.61
 ---- batch: 020 ----
mean loss: 356.60
 ---- batch: 030 ----
mean loss: 354.22
train mean loss: 353.25
epoch train time: 0:00:00.173685
elapsed time: 0:00:42.857219
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:46:39.134920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.67
 ---- batch: 020 ----
mean loss: 355.57
 ---- batch: 030 ----
mean loss: 350.67
train mean loss: 353.16
epoch train time: 0:00:00.169579
elapsed time: 0:00:43.026949
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:46:39.304633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.36
 ---- batch: 020 ----
mean loss: 347.88
 ---- batch: 030 ----
mean loss: 359.07
train mean loss: 352.92
epoch train time: 0:00:00.170377
elapsed time: 0:00:43.197461
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:46:39.475179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.12
 ---- batch: 020 ----
mean loss: 351.56
 ---- batch: 030 ----
mean loss: 346.72
train mean loss: 352.74
epoch train time: 0:00:00.171032
elapsed time: 0:00:43.368663
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:46:39.646348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.42
 ---- batch: 020 ----
mean loss: 359.24
 ---- batch: 030 ----
mean loss: 346.53
train mean loss: 352.95
epoch train time: 0:00:00.178835
elapsed time: 0:00:43.547638
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:46:39.825324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.17
 ---- batch: 020 ----
mean loss: 352.44
 ---- batch: 030 ----
mean loss: 356.48
train mean loss: 352.47
epoch train time: 0:00:00.173076
elapsed time: 0:00:43.720888
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:46:39.998575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.06
 ---- batch: 020 ----
mean loss: 357.04
 ---- batch: 030 ----
mean loss: 347.07
train mean loss: 352.77
epoch train time: 0:00:00.170447
elapsed time: 0:00:43.891472
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:46:40.169171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.74
 ---- batch: 020 ----
mean loss: 350.71
 ---- batch: 030 ----
mean loss: 356.97
train mean loss: 352.19
epoch train time: 0:00:00.168084
elapsed time: 0:00:44.059703
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:46:40.337388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.08
 ---- batch: 020 ----
mean loss: 349.36
 ---- batch: 030 ----
mean loss: 353.03
train mean loss: 352.36
epoch train time: 0:00:00.168171
elapsed time: 0:00:44.228056
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:46:40.505740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.17
 ---- batch: 020 ----
mean loss: 352.10
 ---- batch: 030 ----
mean loss: 354.09
train mean loss: 352.23
epoch train time: 0:00:00.167943
elapsed time: 0:00:44.396136
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:46:40.673839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.00
 ---- batch: 020 ----
mean loss: 347.20
 ---- batch: 030 ----
mean loss: 351.56
train mean loss: 352.01
epoch train time: 0:00:00.181132
elapsed time: 0:00:44.577422
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:46:40.855108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.78
 ---- batch: 020 ----
mean loss: 357.65
 ---- batch: 030 ----
mean loss: 342.90
train mean loss: 352.01
epoch train time: 0:00:00.175544
elapsed time: 0:00:44.753104
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:46:41.030790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.81
 ---- batch: 020 ----
mean loss: 356.34
 ---- batch: 030 ----
mean loss: 347.69
train mean loss: 351.64
epoch train time: 0:00:00.168593
elapsed time: 0:00:44.921834
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:46:41.199518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.32
 ---- batch: 020 ----
mean loss: 358.26
 ---- batch: 030 ----
mean loss: 351.25
train mean loss: 351.60
epoch train time: 0:00:00.170281
elapsed time: 0:00:45.092280
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:46:41.369980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.94
 ---- batch: 020 ----
mean loss: 345.29
 ---- batch: 030 ----
mean loss: 356.78
train mean loss: 351.27
epoch train time: 0:00:00.169342
elapsed time: 0:00:45.261772
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:46:41.539456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.70
 ---- batch: 020 ----
mean loss: 354.30
 ---- batch: 030 ----
mean loss: 355.17
train mean loss: 350.98
epoch train time: 0:00:00.175946
elapsed time: 0:00:45.437862
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:46:41.715550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.75
 ---- batch: 020 ----
mean loss: 354.62
 ---- batch: 030 ----
mean loss: 351.35
train mean loss: 351.04
epoch train time: 0:00:00.176843
elapsed time: 0:00:45.614843
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:46:41.892546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.78
 ---- batch: 020 ----
mean loss: 356.42
 ---- batch: 030 ----
mean loss: 347.82
train mean loss: 350.64
epoch train time: 0:00:00.177592
elapsed time: 0:00:45.792607
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:46:42.070292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.04
 ---- batch: 020 ----
mean loss: 354.83
 ---- batch: 030 ----
mean loss: 345.62
train mean loss: 350.54
epoch train time: 0:00:00.172361
elapsed time: 0:00:45.965103
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:46:42.242789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.49
 ---- batch: 020 ----
mean loss: 350.05
 ---- batch: 030 ----
mean loss: 348.48
train mean loss: 350.60
epoch train time: 0:00:00.171490
elapsed time: 0:00:46.136729
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:46:42.414413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.81
 ---- batch: 020 ----
mean loss: 353.75
 ---- batch: 030 ----
mean loss: 349.51
train mean loss: 350.39
epoch train time: 0:00:00.169442
elapsed time: 0:00:46.306304
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:46:42.583988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.89
 ---- batch: 020 ----
mean loss: 345.23
 ---- batch: 030 ----
mean loss: 349.91
train mean loss: 350.11
epoch train time: 0:00:00.169378
elapsed time: 0:00:46.475817
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:46:42.753503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.46
 ---- batch: 020 ----
mean loss: 349.90
 ---- batch: 030 ----
mean loss: 347.08
train mean loss: 349.71
epoch train time: 0:00:00.181805
elapsed time: 0:00:46.657807
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:46:42.935509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.90
 ---- batch: 020 ----
mean loss: 353.95
 ---- batch: 030 ----
mean loss: 355.89
train mean loss: 349.85
epoch train time: 0:00:00.174756
elapsed time: 0:00:46.832719
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:46:43.110405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.74
 ---- batch: 020 ----
mean loss: 348.49
 ---- batch: 030 ----
mean loss: 349.13
train mean loss: 349.73
epoch train time: 0:00:00.170796
elapsed time: 0:00:47.003648
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:46:43.281332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.30
 ---- batch: 020 ----
mean loss: 350.22
 ---- batch: 030 ----
mean loss: 351.92
train mean loss: 349.08
epoch train time: 0:00:00.173727
elapsed time: 0:00:47.177524
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:46:43.455213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.84
 ---- batch: 020 ----
mean loss: 357.51
 ---- batch: 030 ----
mean loss: 345.43
train mean loss: 349.59
epoch train time: 0:00:00.174899
elapsed time: 0:00:47.352565
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:46:43.630253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.00
 ---- batch: 020 ----
mean loss: 345.47
 ---- batch: 030 ----
mean loss: 351.76
train mean loss: 349.29
epoch train time: 0:00:00.175238
elapsed time: 0:00:47.528001
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:46:43.805680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.08
 ---- batch: 020 ----
mean loss: 350.55
 ---- batch: 030 ----
mean loss: 354.34
train mean loss: 349.36
epoch train time: 0:00:00.175153
elapsed time: 0:00:47.703284
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:46:43.980970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.67
 ---- batch: 020 ----
mean loss: 352.02
 ---- batch: 030 ----
mean loss: 342.70
train mean loss: 349.45
epoch train time: 0:00:00.174204
elapsed time: 0:00:47.877627
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:46:44.155332
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.61
 ---- batch: 020 ----
mean loss: 345.14
 ---- batch: 030 ----
mean loss: 356.43
train mean loss: 349.26
epoch train time: 0:00:00.171460
elapsed time: 0:00:48.049241
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:46:44.326932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.84
 ---- batch: 020 ----
mean loss: 348.51
 ---- batch: 030 ----
mean loss: 352.48
train mean loss: 348.96
epoch train time: 0:00:00.173918
elapsed time: 0:00:48.223325
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:46:44.501013
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.27
 ---- batch: 020 ----
mean loss: 348.06
 ---- batch: 030 ----
mean loss: 346.76
train mean loss: 349.25
epoch train time: 0:00:00.172553
elapsed time: 0:00:48.396040
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:46:44.673728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.31
 ---- batch: 020 ----
mean loss: 350.00
 ---- batch: 030 ----
mean loss: 355.36
train mean loss: 349.02
epoch train time: 0:00:00.179738
elapsed time: 0:00:48.575932
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:46:44.853641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.55
 ---- batch: 020 ----
mean loss: 342.78
 ---- batch: 030 ----
mean loss: 350.47
train mean loss: 349.38
epoch train time: 0:00:00.174561
elapsed time: 0:00:48.750651
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:46:45.028336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.67
 ---- batch: 020 ----
mean loss: 356.47
 ---- batch: 030 ----
mean loss: 350.61
train mean loss: 348.95
epoch train time: 0:00:00.170875
elapsed time: 0:00:48.921662
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:46:45.199347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.48
 ---- batch: 020 ----
mean loss: 346.72
 ---- batch: 030 ----
mean loss: 352.49
train mean loss: 349.70
epoch train time: 0:00:00.169427
elapsed time: 0:00:49.091238
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:46:45.368985
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.83
 ---- batch: 020 ----
mean loss: 348.64
 ---- batch: 030 ----
mean loss: 355.65
train mean loss: 348.83
epoch train time: 0:00:00.171120
elapsed time: 0:00:49.262554
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:46:45.540255
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.89
 ---- batch: 020 ----
mean loss: 356.13
 ---- batch: 030 ----
mean loss: 346.22
train mean loss: 348.72
epoch train time: 0:00:00.173161
elapsed time: 0:00:49.435869
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:46:45.713557
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.36
 ---- batch: 020 ----
mean loss: 347.02
 ---- batch: 030 ----
mean loss: 345.18
train mean loss: 348.62
epoch train time: 0:00:00.176505
elapsed time: 0:00:49.612523
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:46:45.890223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.71
 ---- batch: 020 ----
mean loss: 345.55
 ---- batch: 030 ----
mean loss: 350.31
train mean loss: 349.47
epoch train time: 0:00:00.177517
elapsed time: 0:00:49.790238
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:46:46.067938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.09
 ---- batch: 020 ----
mean loss: 344.61
 ---- batch: 030 ----
mean loss: 349.83
train mean loss: 348.92
epoch train time: 0:00:00.172875
elapsed time: 0:00:49.963293
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:46:46.241003
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.95
 ---- batch: 020 ----
mean loss: 350.37
 ---- batch: 030 ----
mean loss: 346.44
train mean loss: 348.65
epoch train time: 0:00:00.173044
elapsed time: 0:00:50.136504
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:46:46.414190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.99
 ---- batch: 020 ----
mean loss: 346.42
 ---- batch: 030 ----
mean loss: 351.71
train mean loss: 349.34
epoch train time: 0:00:00.177333
elapsed time: 0:00:50.314016
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:46:46.591716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.55
 ---- batch: 020 ----
mean loss: 348.87
 ---- batch: 030 ----
mean loss: 345.74
train mean loss: 349.16
epoch train time: 0:00:00.173522
elapsed time: 0:00:50.487691
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:46:46.765378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.86
 ---- batch: 020 ----
mean loss: 351.65
 ---- batch: 030 ----
mean loss: 343.89
train mean loss: 348.92
epoch train time: 0:00:00.175390
elapsed time: 0:00:50.663219
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:46:46.940905
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.43
 ---- batch: 020 ----
mean loss: 344.78
 ---- batch: 030 ----
mean loss: 347.41
train mean loss: 348.80
epoch train time: 0:00:00.175916
elapsed time: 0:00:50.839277
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:46:47.116965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.24
 ---- batch: 020 ----
mean loss: 347.29
 ---- batch: 030 ----
mean loss: 345.58
train mean loss: 348.92
epoch train time: 0:00:00.174664
elapsed time: 0:00:51.014081
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:46:47.291769
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.22
 ---- batch: 020 ----
mean loss: 336.09
 ---- batch: 030 ----
mean loss: 352.13
train mean loss: 348.68
epoch train time: 0:00:00.172973
elapsed time: 0:00:51.187204
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:46:47.464893
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.61
 ---- batch: 020 ----
mean loss: 346.69
 ---- batch: 030 ----
mean loss: 349.72
train mean loss: 348.77
epoch train time: 0:00:00.174072
elapsed time: 0:00:51.361431
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:46:47.639119
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.21
 ---- batch: 020 ----
mean loss: 349.35
 ---- batch: 030 ----
mean loss: 351.25
train mean loss: 348.97
epoch train time: 0:00:00.177845
elapsed time: 0:00:51.539419
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:46:47.817106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.59
 ---- batch: 020 ----
mean loss: 347.71
 ---- batch: 030 ----
mean loss: 345.26
train mean loss: 348.91
epoch train time: 0:00:00.177926
elapsed time: 0:00:51.717527
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:46:47.995217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.48
 ---- batch: 020 ----
mean loss: 344.27
 ---- batch: 030 ----
mean loss: 346.80
train mean loss: 348.62
epoch train time: 0:00:00.172956
elapsed time: 0:00:51.890623
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:46:48.168309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.61
 ---- batch: 020 ----
mean loss: 342.37
 ---- batch: 030 ----
mean loss: 352.29
train mean loss: 348.63
epoch train time: 0:00:00.172578
elapsed time: 0:00:52.063348
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:46:48.341043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.66
 ---- batch: 020 ----
mean loss: 357.95
 ---- batch: 030 ----
mean loss: 350.13
train mean loss: 348.61
epoch train time: 0:00:00.174632
elapsed time: 0:00:52.238130
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:46:48.515817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.93
 ---- batch: 020 ----
mean loss: 351.18
 ---- batch: 030 ----
mean loss: 349.10
train mean loss: 349.29
epoch train time: 0:00:00.179618
elapsed time: 0:00:52.417954
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:46:48.695652
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.59
 ---- batch: 020 ----
mean loss: 355.86
 ---- batch: 030 ----
mean loss: 340.76
train mean loss: 348.27
epoch train time: 0:00:00.181299
elapsed time: 0:00:52.599425
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:46:48.877140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.77
 ---- batch: 020 ----
mean loss: 339.31
 ---- batch: 030 ----
mean loss: 356.53
train mean loss: 348.54
epoch train time: 0:00:00.176524
elapsed time: 0:00:52.776122
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:46:49.053809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.11
 ---- batch: 020 ----
mean loss: 342.59
 ---- batch: 030 ----
mean loss: 356.21
train mean loss: 348.79
epoch train time: 0:00:00.173354
elapsed time: 0:00:52.949623
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:46:49.227317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.15
 ---- batch: 020 ----
mean loss: 352.29
 ---- batch: 030 ----
mean loss: 351.75
train mean loss: 348.54
epoch train time: 0:00:00.170504
elapsed time: 0:00:53.120329
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:46:49.398006
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.36
 ---- batch: 020 ----
mean loss: 348.40
 ---- batch: 030 ----
mean loss: 350.42
train mean loss: 348.88
epoch train time: 0:00:00.175910
elapsed time: 0:00:53.296385
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:46:49.574071
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.50
 ---- batch: 020 ----
mean loss: 347.62
 ---- batch: 030 ----
mean loss: 352.79
train mean loss: 348.36
epoch train time: 0:00:00.181041
elapsed time: 0:00:53.477592
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:46:49.755278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.02
 ---- batch: 020 ----
mean loss: 351.32
 ---- batch: 030 ----
mean loss: 343.14
train mean loss: 348.72
epoch train time: 0:00:00.174579
elapsed time: 0:00:53.652328
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:46:49.930013
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.57
 ---- batch: 020 ----
mean loss: 354.26
 ---- batch: 030 ----
mean loss: 347.26
train mean loss: 348.61
epoch train time: 0:00:00.173985
elapsed time: 0:00:53.826454
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:46:50.104158
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.93
 ---- batch: 020 ----
mean loss: 354.97
 ---- batch: 030 ----
mean loss: 337.61
train mean loss: 348.66
epoch train time: 0:00:00.176153
elapsed time: 0:00:54.002761
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:46:50.280446
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.49
 ---- batch: 020 ----
mean loss: 354.01
 ---- batch: 030 ----
mean loss: 346.93
train mean loss: 348.62
epoch train time: 0:00:00.173399
elapsed time: 0:00:54.176315
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:46:50.454001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.32
 ---- batch: 020 ----
mean loss: 343.46
 ---- batch: 030 ----
mean loss: 346.72
train mean loss: 348.10
epoch train time: 0:00:00.174875
elapsed time: 0:00:54.351352
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:46:50.629038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.90
 ---- batch: 020 ----
mean loss: 348.00
 ---- batch: 030 ----
mean loss: 347.62
train mean loss: 348.36
epoch train time: 0:00:00.174598
elapsed time: 0:00:54.526121
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:46:50.803838
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.92
 ---- batch: 020 ----
mean loss: 344.37
 ---- batch: 030 ----
mean loss: 348.69
train mean loss: 348.84
epoch train time: 0:00:00.174283
elapsed time: 0:00:54.700577
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:46:50.978264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.02
 ---- batch: 020 ----
mean loss: 354.08
 ---- batch: 030 ----
mean loss: 346.53
train mean loss: 348.19
epoch train time: 0:00:00.175703
elapsed time: 0:00:54.876420
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:46:51.154106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.78
 ---- batch: 020 ----
mean loss: 350.72
 ---- batch: 030 ----
mean loss: 341.36
train mean loss: 348.63
epoch train time: 0:00:00.172421
elapsed time: 0:00:55.049030
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:46:51.326731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.09
 ---- batch: 020 ----
mean loss: 345.07
 ---- batch: 030 ----
mean loss: 348.59
train mean loss: 348.56
epoch train time: 0:00:00.176758
elapsed time: 0:00:55.225943
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:46:51.503630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.56
 ---- batch: 020 ----
mean loss: 359.72
 ---- batch: 030 ----
mean loss: 343.69
train mean loss: 348.60
epoch train time: 0:00:00.174184
elapsed time: 0:00:55.400284
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:46:51.677983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.78
 ---- batch: 020 ----
mean loss: 350.03
 ---- batch: 030 ----
mean loss: 342.76
train mean loss: 348.61
epoch train time: 0:00:00.175860
elapsed time: 0:00:55.576330
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:46:51.854018
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.93
 ---- batch: 020 ----
mean loss: 346.56
 ---- batch: 030 ----
mean loss: 354.17
train mean loss: 348.41
epoch train time: 0:00:00.172810
elapsed time: 0:00:55.749277
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:46:52.026972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.76
 ---- batch: 020 ----
mean loss: 343.21
 ---- batch: 030 ----
mean loss: 350.11
train mean loss: 348.18
epoch train time: 0:00:00.176207
elapsed time: 0:00:55.927574
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_5/checkpoint.pth.tar
**** end time: 2019-09-27 16:46:52.205228 ****
