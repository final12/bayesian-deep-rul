Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 20209
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 19:38:15.769651 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 19:38:15.779095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4119.46
 ---- batch: 020 ----
mean loss: 3877.42
 ---- batch: 030 ----
mean loss: 3753.22
 ---- batch: 040 ----
mean loss: 3519.42
 ---- batch: 050 ----
mean loss: 3261.67
 ---- batch: 060 ----
mean loss: 3195.21
 ---- batch: 070 ----
mean loss: 2977.60
 ---- batch: 080 ----
mean loss: 2891.69
 ---- batch: 090 ----
mean loss: 2768.40
train mean loss: 3330.13
epoch train time: 0:00:34.879446
elapsed time: 0:00:34.895654
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 19:38:50.665346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2551.81
 ---- batch: 020 ----
mean loss: 2531.35
 ---- batch: 030 ----
mean loss: 2438.68
 ---- batch: 040 ----
mean loss: 2353.77
 ---- batch: 050 ----
mean loss: 2245.32
 ---- batch: 060 ----
mean loss: 2224.89
 ---- batch: 070 ----
mean loss: 2160.45
 ---- batch: 080 ----
mean loss: 2095.73
 ---- batch: 090 ----
mean loss: 2073.28
train mean loss: 2280.00
epoch train time: 0:00:01.616582
elapsed time: 0:00:36.512586
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 19:38:52.282594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1959.18
 ---- batch: 020 ----
mean loss: 1917.77
 ---- batch: 030 ----
mean loss: 1907.48
 ---- batch: 040 ----
mean loss: 1880.90
 ---- batch: 050 ----
mean loss: 1852.68
 ---- batch: 060 ----
mean loss: 1814.93
 ---- batch: 070 ----
mean loss: 1808.02
 ---- batch: 080 ----
mean loss: 1754.38
 ---- batch: 090 ----
mean loss: 1705.58
train mean loss: 1836.91
epoch train time: 0:00:01.609907
elapsed time: 0:00:38.123188
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 19:38:53.893138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1685.63
 ---- batch: 020 ----
mean loss: 1630.69
 ---- batch: 030 ----
mean loss: 1613.74
 ---- batch: 040 ----
mean loss: 1624.92
 ---- batch: 050 ----
mean loss: 1579.13
 ---- batch: 060 ----
mean loss: 1586.10
 ---- batch: 070 ----
mean loss: 1518.93
 ---- batch: 080 ----
mean loss: 1520.12
 ---- batch: 090 ----
mean loss: 1517.96
train mean loss: 1581.00
epoch train time: 0:00:01.579830
elapsed time: 0:00:39.703655
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 19:38:55.473647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1463.99
 ---- batch: 020 ----
mean loss: 1452.62
 ---- batch: 030 ----
mean loss: 1440.31
 ---- batch: 040 ----
mean loss: 1411.45
 ---- batch: 050 ----
mean loss: 1410.62
 ---- batch: 060 ----
mean loss: 1371.89
 ---- batch: 070 ----
mean loss: 1378.42
 ---- batch: 080 ----
mean loss: 1381.29
 ---- batch: 090 ----
mean loss: 1336.41
train mean loss: 1399.76
epoch train time: 0:00:01.622111
elapsed time: 0:00:41.326441
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 19:38:57.096388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1307.04
 ---- batch: 020 ----
mean loss: 1290.07
 ---- batch: 030 ----
mean loss: 1308.67
 ---- batch: 040 ----
mean loss: 1274.92
 ---- batch: 050 ----
mean loss: 1310.23
 ---- batch: 060 ----
mean loss: 1233.71
 ---- batch: 070 ----
mean loss: 1263.23
 ---- batch: 080 ----
mean loss: 1264.26
 ---- batch: 090 ----
mean loss: 1209.68
train mean loss: 1269.73
epoch train time: 0:00:01.603461
elapsed time: 0:00:42.930544
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 19:38:58.700509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1200.22
 ---- batch: 020 ----
mean loss: 1212.54
 ---- batch: 030 ----
mean loss: 1167.97
 ---- batch: 040 ----
mean loss: 1190.54
 ---- batch: 050 ----
mean loss: 1176.33
 ---- batch: 060 ----
mean loss: 1181.93
 ---- batch: 070 ----
mean loss: 1149.28
 ---- batch: 080 ----
mean loss: 1142.74
 ---- batch: 090 ----
mean loss: 1138.52
train mean loss: 1170.09
epoch train time: 0:00:01.630254
elapsed time: 0:00:44.561441
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 19:39:00.331418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1111.00
 ---- batch: 020 ----
mean loss: 1120.73
 ---- batch: 030 ----
mean loss: 1142.26
 ---- batch: 040 ----
mean loss: 1091.45
 ---- batch: 050 ----
mean loss: 1111.22
 ---- batch: 060 ----
mean loss: 1101.74
 ---- batch: 070 ----
mean loss: 1075.49
 ---- batch: 080 ----
mean loss: 1086.60
 ---- batch: 090 ----
mean loss: 1067.57
train mean loss: 1096.38
epoch train time: 0:00:01.636950
elapsed time: 0:00:46.199086
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 19:39:01.969039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1072.17
 ---- batch: 020 ----
mean loss: 1036.80
 ---- batch: 030 ----
mean loss: 1045.62
 ---- batch: 040 ----
mean loss: 1047.52
 ---- batch: 050 ----
mean loss: 1052.66
 ---- batch: 060 ----
mean loss: 1020.35
 ---- batch: 070 ----
mean loss: 1034.53
 ---- batch: 080 ----
mean loss: 1004.82
 ---- batch: 090 ----
mean loss: 1032.40
train mean loss: 1038.07
epoch train time: 0:00:01.617467
elapsed time: 0:00:47.817223
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 19:39:03.587181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1027.23
 ---- batch: 020 ----
mean loss: 1006.16
 ---- batch: 030 ----
mean loss: 997.59
 ---- batch: 040 ----
mean loss: 981.41
 ---- batch: 050 ----
mean loss: 1005.21
 ---- batch: 060 ----
mean loss: 1002.92
 ---- batch: 070 ----
mean loss: 1005.10
 ---- batch: 080 ----
mean loss: 964.55
 ---- batch: 090 ----
mean loss: 979.95
train mean loss: 995.03
epoch train time: 0:00:01.601281
elapsed time: 0:00:49.419126
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 19:39:05.189076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.09
 ---- batch: 020 ----
mean loss: 987.13
 ---- batch: 030 ----
mean loss: 976.62
 ---- batch: 040 ----
mean loss: 954.65
 ---- batch: 050 ----
mean loss: 963.43
 ---- batch: 060 ----
mean loss: 962.56
 ---- batch: 070 ----
mean loss: 953.00
 ---- batch: 080 ----
mean loss: 948.45
 ---- batch: 090 ----
mean loss: 966.75
train mean loss: 962.66
epoch train time: 0:00:01.585687
elapsed time: 0:00:51.005508
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 19:39:06.775491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.36
 ---- batch: 020 ----
mean loss: 944.99
 ---- batch: 030 ----
mean loss: 951.29
 ---- batch: 040 ----
mean loss: 960.19
 ---- batch: 050 ----
mean loss: 951.72
 ---- batch: 060 ----
mean loss: 951.39
 ---- batch: 070 ----
mean loss: 951.67
 ---- batch: 080 ----
mean loss: 931.32
 ---- batch: 090 ----
mean loss: 919.93
train mean loss: 941.12
epoch train time: 0:00:01.622934
elapsed time: 0:00:52.629111
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 19:39:08.398953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.53
 ---- batch: 020 ----
mean loss: 923.75
 ---- batch: 030 ----
mean loss: 933.60
 ---- batch: 040 ----
mean loss: 908.50
 ---- batch: 050 ----
mean loss: 941.11
 ---- batch: 060 ----
mean loss: 922.10
 ---- batch: 070 ----
mean loss: 908.45
 ---- batch: 080 ----
mean loss: 925.26
 ---- batch: 090 ----
mean loss: 928.94
train mean loss: 925.68
epoch train time: 0:00:01.581702
elapsed time: 0:00:54.211355
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 19:39:09.981318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.99
 ---- batch: 020 ----
mean loss: 915.08
 ---- batch: 030 ----
mean loss: 909.66
 ---- batch: 040 ----
mean loss: 895.34
 ---- batch: 050 ----
mean loss: 913.87
 ---- batch: 060 ----
mean loss: 907.31
 ---- batch: 070 ----
mean loss: 933.40
 ---- batch: 080 ----
mean loss: 911.94
 ---- batch: 090 ----
mean loss: 913.42
train mean loss: 912.92
epoch train time: 0:00:01.598063
elapsed time: 0:00:55.810062
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 19:39:11.580020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.34
 ---- batch: 020 ----
mean loss: 910.99
 ---- batch: 030 ----
mean loss: 910.55
 ---- batch: 040 ----
mean loss: 900.17
 ---- batch: 050 ----
mean loss: 895.69
 ---- batch: 060 ----
mean loss: 887.28
 ---- batch: 070 ----
mean loss: 895.49
 ---- batch: 080 ----
mean loss: 915.60
 ---- batch: 090 ----
mean loss: 907.24
train mean loss: 905.65
epoch train time: 0:00:01.630122
elapsed time: 0:00:57.440858
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 19:39:13.210816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.07
 ---- batch: 020 ----
mean loss: 904.54
 ---- batch: 030 ----
mean loss: 896.47
 ---- batch: 040 ----
mean loss: 909.76
 ---- batch: 050 ----
mean loss: 901.24
 ---- batch: 060 ----
mean loss: 895.30
 ---- batch: 070 ----
mean loss: 879.68
 ---- batch: 080 ----
mean loss: 892.94
 ---- batch: 090 ----
mean loss: 908.18
train mean loss: 899.47
epoch train time: 0:00:01.602300
elapsed time: 0:00:59.043776
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 19:39:14.813746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.94
 ---- batch: 020 ----
mean loss: 871.09
 ---- batch: 030 ----
mean loss: 893.60
 ---- batch: 040 ----
mean loss: 907.99
 ---- batch: 050 ----
mean loss: 882.67
 ---- batch: 060 ----
mean loss: 898.06
 ---- batch: 070 ----
mean loss: 903.00
 ---- batch: 080 ----
mean loss: 909.50
 ---- batch: 090 ----
mean loss: 881.00
train mean loss: 894.51
epoch train time: 0:00:01.613280
elapsed time: 0:01:00.657704
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 19:39:16.427660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.13
 ---- batch: 020 ----
mean loss: 886.67
 ---- batch: 030 ----
mean loss: 912.89
 ---- batch: 040 ----
mean loss: 903.63
 ---- batch: 050 ----
mean loss: 880.05
 ---- batch: 060 ----
mean loss: 880.46
 ---- batch: 070 ----
mean loss: 889.54
 ---- batch: 080 ----
mean loss: 891.99
 ---- batch: 090 ----
mean loss: 887.14
train mean loss: 893.93
epoch train time: 0:00:01.611830
elapsed time: 0:01:02.270172
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 19:39:18.040116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.28
 ---- batch: 020 ----
mean loss: 900.23
 ---- batch: 030 ----
mean loss: 872.11
 ---- batch: 040 ----
mean loss: 894.93
 ---- batch: 050 ----
mean loss: 893.67
 ---- batch: 060 ----
mean loss: 888.03
 ---- batch: 070 ----
mean loss: 866.52
 ---- batch: 080 ----
mean loss: 905.88
 ---- batch: 090 ----
mean loss: 895.60
train mean loss: 889.38
epoch train time: 0:00:01.644376
elapsed time: 0:01:03.915240
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 19:39:19.685208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.62
 ---- batch: 020 ----
mean loss: 903.47
 ---- batch: 030 ----
mean loss: 893.25
 ---- batch: 040 ----
mean loss: 897.09
 ---- batch: 050 ----
mean loss: 877.38
 ---- batch: 060 ----
mean loss: 889.91
 ---- batch: 070 ----
mean loss: 894.54
 ---- batch: 080 ----
mean loss: 874.22
 ---- batch: 090 ----
mean loss: 887.09
train mean loss: 889.56
epoch train time: 0:00:01.658886
elapsed time: 0:01:05.574813
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 19:39:21.344778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.01
 ---- batch: 020 ----
mean loss: 883.24
 ---- batch: 030 ----
mean loss: 884.01
 ---- batch: 040 ----
mean loss: 888.30
 ---- batch: 050 ----
mean loss: 901.20
 ---- batch: 060 ----
mean loss: 883.97
 ---- batch: 070 ----
mean loss: 868.22
 ---- batch: 080 ----
mean loss: 904.94
 ---- batch: 090 ----
mean loss: 889.92
train mean loss: 887.80
epoch train time: 0:00:01.623286
elapsed time: 0:01:07.198755
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 19:39:22.968734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.35
 ---- batch: 020 ----
mean loss: 895.78
 ---- batch: 030 ----
mean loss: 886.45
 ---- batch: 040 ----
mean loss: 871.19
 ---- batch: 050 ----
mean loss: 881.03
 ---- batch: 060 ----
mean loss: 907.80
 ---- batch: 070 ----
mean loss: 893.76
 ---- batch: 080 ----
mean loss: 884.32
 ---- batch: 090 ----
mean loss: 884.49
train mean loss: 888.48
epoch train time: 0:00:01.646512
elapsed time: 0:01:08.845954
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 19:39:24.615732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.51
 ---- batch: 020 ----
mean loss: 885.38
 ---- batch: 030 ----
mean loss: 872.48
 ---- batch: 040 ----
mean loss: 877.96
 ---- batch: 050 ----
mean loss: 894.13
 ---- batch: 060 ----
mean loss: 893.85
 ---- batch: 070 ----
mean loss: 887.86
 ---- batch: 080 ----
mean loss: 889.99
 ---- batch: 090 ----
mean loss: 897.85
train mean loss: 887.62
epoch train time: 0:00:01.616528
elapsed time: 0:01:10.462930
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 19:39:26.232876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.89
 ---- batch: 020 ----
mean loss: 879.73
 ---- batch: 030 ----
mean loss: 891.27
 ---- batch: 040 ----
mean loss: 871.67
 ---- batch: 050 ----
mean loss: 881.27
 ---- batch: 060 ----
mean loss: 879.59
 ---- batch: 070 ----
mean loss: 895.48
 ---- batch: 080 ----
mean loss: 892.27
 ---- batch: 090 ----
mean loss: 884.21
train mean loss: 887.67
epoch train time: 0:00:01.637609
elapsed time: 0:01:12.101165
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 19:39:27.871197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.92
 ---- batch: 020 ----
mean loss: 898.42
 ---- batch: 030 ----
mean loss: 888.85
 ---- batch: 040 ----
mean loss: 885.79
 ---- batch: 050 ----
mean loss: 894.07
 ---- batch: 060 ----
mean loss: 890.29
 ---- batch: 070 ----
mean loss: 884.95
 ---- batch: 080 ----
mean loss: 876.80
 ---- batch: 090 ----
mean loss: 895.79
train mean loss: 885.79
epoch train time: 0:00:01.614980
elapsed time: 0:01:13.716828
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 19:39:29.486834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.32
 ---- batch: 020 ----
mean loss: 881.04
 ---- batch: 030 ----
mean loss: 872.58
 ---- batch: 040 ----
mean loss: 875.83
 ---- batch: 050 ----
mean loss: 874.91
 ---- batch: 060 ----
mean loss: 909.14
 ---- batch: 070 ----
mean loss: 892.34
 ---- batch: 080 ----
mean loss: 891.27
 ---- batch: 090 ----
mean loss: 879.01
train mean loss: 884.61
epoch train time: 0:00:01.621515
elapsed time: 0:01:15.338980
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 19:39:31.108998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.27
 ---- batch: 020 ----
mean loss: 879.80
 ---- batch: 030 ----
mean loss: 883.18
 ---- batch: 040 ----
mean loss: 882.82
 ---- batch: 050 ----
mean loss: 875.94
 ---- batch: 060 ----
mean loss: 876.19
 ---- batch: 070 ----
mean loss: 886.69
 ---- batch: 080 ----
mean loss: 898.31
 ---- batch: 090 ----
mean loss: 893.39
train mean loss: 883.57
epoch train time: 0:00:01.583955
elapsed time: 0:01:16.923732
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 19:39:32.693751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.77
 ---- batch: 020 ----
mean loss: 879.51
 ---- batch: 030 ----
mean loss: 892.13
 ---- batch: 040 ----
mean loss: 892.52
 ---- batch: 050 ----
mean loss: 876.00
 ---- batch: 060 ----
mean loss: 875.88
 ---- batch: 070 ----
mean loss: 870.13
 ---- batch: 080 ----
mean loss: 900.72
 ---- batch: 090 ----
mean loss: 868.68
train mean loss: 883.28
epoch train time: 0:00:01.612903
elapsed time: 0:01:18.537379
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 19:39:34.307343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.45
 ---- batch: 020 ----
mean loss: 879.24
 ---- batch: 030 ----
mean loss: 875.39
 ---- batch: 040 ----
mean loss: 881.78
 ---- batch: 050 ----
mean loss: 891.59
 ---- batch: 060 ----
mean loss: 892.53
 ---- batch: 070 ----
mean loss: 898.63
 ---- batch: 080 ----
mean loss: 874.35
 ---- batch: 090 ----
mean loss: 867.55
train mean loss: 884.47
epoch train time: 0:00:01.583363
elapsed time: 0:01:20.121403
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 19:39:35.891369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.40
 ---- batch: 020 ----
mean loss: 895.45
 ---- batch: 030 ----
mean loss: 874.82
 ---- batch: 040 ----
mean loss: 880.92
 ---- batch: 050 ----
mean loss: 884.01
 ---- batch: 060 ----
mean loss: 882.84
 ---- batch: 070 ----
mean loss: 879.84
 ---- batch: 080 ----
mean loss: 877.32
 ---- batch: 090 ----
mean loss: 860.23
train mean loss: 881.09
epoch train time: 0:00:01.641000
elapsed time: 0:01:21.763044
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 19:39:37.533117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.09
 ---- batch: 020 ----
mean loss: 879.08
 ---- batch: 030 ----
mean loss: 874.74
 ---- batch: 040 ----
mean loss: 886.49
 ---- batch: 050 ----
mean loss: 904.21
 ---- batch: 060 ----
mean loss: 874.37
 ---- batch: 070 ----
mean loss: 895.68
 ---- batch: 080 ----
mean loss: 872.08
 ---- batch: 090 ----
mean loss: 866.76
train mean loss: 882.10
epoch train time: 0:00:01.631470
elapsed time: 0:01:23.395258
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 19:39:39.165239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.62
 ---- batch: 020 ----
mean loss: 890.48
 ---- batch: 030 ----
mean loss: 883.74
 ---- batch: 040 ----
mean loss: 875.53
 ---- batch: 050 ----
mean loss: 889.17
 ---- batch: 060 ----
mean loss: 892.32
 ---- batch: 070 ----
mean loss: 869.04
 ---- batch: 080 ----
mean loss: 890.85
 ---- batch: 090 ----
mean loss: 877.93
train mean loss: 883.40
epoch train time: 0:00:01.627338
elapsed time: 0:01:25.023310
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 19:39:40.793358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.60
 ---- batch: 020 ----
mean loss: 884.68
 ---- batch: 030 ----
mean loss: 884.68
 ---- batch: 040 ----
mean loss: 879.37
 ---- batch: 050 ----
mean loss: 875.87
 ---- batch: 060 ----
mean loss: 874.02
 ---- batch: 070 ----
mean loss: 879.02
 ---- batch: 080 ----
mean loss: 884.52
 ---- batch: 090 ----
mean loss: 883.72
train mean loss: 883.08
epoch train time: 0:00:01.638372
elapsed time: 0:01:26.662500
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 19:39:42.432519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.42
 ---- batch: 020 ----
mean loss: 886.90
 ---- batch: 030 ----
mean loss: 876.27
 ---- batch: 040 ----
mean loss: 881.93
 ---- batch: 050 ----
mean loss: 890.93
 ---- batch: 060 ----
mean loss: 884.29
 ---- batch: 070 ----
mean loss: 889.48
 ---- batch: 080 ----
mean loss: 864.44
 ---- batch: 090 ----
mean loss: 880.56
train mean loss: 880.65
epoch train time: 0:00:01.643439
elapsed time: 0:01:28.306732
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 19:39:44.076739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.40
 ---- batch: 020 ----
mean loss: 880.57
 ---- batch: 030 ----
mean loss: 864.28
 ---- batch: 040 ----
mean loss: 888.49
 ---- batch: 050 ----
mean loss: 877.63
 ---- batch: 060 ----
mean loss: 895.45
 ---- batch: 070 ----
mean loss: 886.73
 ---- batch: 080 ----
mean loss: 883.47
 ---- batch: 090 ----
mean loss: 894.12
train mean loss: 882.99
epoch train time: 0:00:01.612690
elapsed time: 0:01:29.920179
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 19:39:45.690253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.77
 ---- batch: 020 ----
mean loss: 875.73
 ---- batch: 030 ----
mean loss: 883.74
 ---- batch: 040 ----
mean loss: 894.47
 ---- batch: 050 ----
mean loss: 896.00
 ---- batch: 060 ----
mean loss: 861.44
 ---- batch: 070 ----
mean loss: 866.64
 ---- batch: 080 ----
mean loss: 871.53
 ---- batch: 090 ----
mean loss: 911.94
train mean loss: 881.13
epoch train time: 0:00:01.611620
elapsed time: 0:01:31.532535
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 19:39:47.302506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.76
 ---- batch: 020 ----
mean loss: 872.37
 ---- batch: 030 ----
mean loss: 889.09
 ---- batch: 040 ----
mean loss: 900.48
 ---- batch: 050 ----
mean loss: 905.21
 ---- batch: 060 ----
mean loss: 877.66
 ---- batch: 070 ----
mean loss: 877.06
 ---- batch: 080 ----
mean loss: 858.94
 ---- batch: 090 ----
mean loss: 873.98
train mean loss: 880.77
epoch train time: 0:00:01.590628
elapsed time: 0:01:33.123864
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 19:39:48.893863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.19
 ---- batch: 020 ----
mean loss: 887.62
 ---- batch: 030 ----
mean loss: 888.94
 ---- batch: 040 ----
mean loss: 888.75
 ---- batch: 050 ----
mean loss: 872.96
 ---- batch: 060 ----
mean loss: 885.73
 ---- batch: 070 ----
mean loss: 877.30
 ---- batch: 080 ----
mean loss: 888.36
 ---- batch: 090 ----
mean loss: 863.89
train mean loss: 882.11
epoch train time: 0:00:01.627959
elapsed time: 0:01:34.752530
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 19:39:50.522492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.04
 ---- batch: 020 ----
mean loss: 878.63
 ---- batch: 030 ----
mean loss: 874.26
 ---- batch: 040 ----
mean loss: 879.22
 ---- batch: 050 ----
mean loss: 864.62
 ---- batch: 060 ----
mean loss: 888.57
 ---- batch: 070 ----
mean loss: 887.74
 ---- batch: 080 ----
mean loss: 867.92
 ---- batch: 090 ----
mean loss: 889.06
train mean loss: 879.89
epoch train time: 0:00:01.629567
elapsed time: 0:01:36.382753
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 19:39:52.152717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.42
 ---- batch: 020 ----
mean loss: 868.41
 ---- batch: 030 ----
mean loss: 875.22
 ---- batch: 040 ----
mean loss: 878.67
 ---- batch: 050 ----
mean loss: 879.29
 ---- batch: 060 ----
mean loss: 877.40
 ---- batch: 070 ----
mean loss: 879.82
 ---- batch: 080 ----
mean loss: 878.24
 ---- batch: 090 ----
mean loss: 885.50
train mean loss: 880.31
epoch train time: 0:00:01.596579
elapsed time: 0:01:37.980044
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 19:39:53.750007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.85
 ---- batch: 020 ----
mean loss: 891.61
 ---- batch: 030 ----
mean loss: 877.73
 ---- batch: 040 ----
mean loss: 868.05
 ---- batch: 050 ----
mean loss: 890.78
 ---- batch: 060 ----
mean loss: 884.17
 ---- batch: 070 ----
mean loss: 877.61
 ---- batch: 080 ----
mean loss: 870.40
 ---- batch: 090 ----
mean loss: 872.15
train mean loss: 879.53
epoch train time: 0:00:01.623854
elapsed time: 0:01:39.604614
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 19:39:55.374595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.33
 ---- batch: 020 ----
mean loss: 886.11
 ---- batch: 030 ----
mean loss: 882.90
 ---- batch: 040 ----
mean loss: 874.16
 ---- batch: 050 ----
mean loss: 867.89
 ---- batch: 060 ----
mean loss: 888.79
 ---- batch: 070 ----
mean loss: 884.52
 ---- batch: 080 ----
mean loss: 878.68
 ---- batch: 090 ----
mean loss: 887.49
train mean loss: 880.39
epoch train time: 0:00:01.632593
elapsed time: 0:01:41.237984
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 19:39:57.007979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.18
 ---- batch: 020 ----
mean loss: 866.96
 ---- batch: 030 ----
mean loss: 884.81
 ---- batch: 040 ----
mean loss: 850.37
 ---- batch: 050 ----
mean loss: 855.18
 ---- batch: 060 ----
mean loss: 876.52
 ---- batch: 070 ----
mean loss: 897.44
 ---- batch: 080 ----
mean loss: 894.66
 ---- batch: 090 ----
mean loss: 903.77
train mean loss: 879.71
epoch train time: 0:00:01.618569
elapsed time: 0:01:42.857266
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 19:39:58.627272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.22
 ---- batch: 020 ----
mean loss: 862.26
 ---- batch: 030 ----
mean loss: 882.45
 ---- batch: 040 ----
mean loss: 895.11
 ---- batch: 050 ----
mean loss: 870.38
 ---- batch: 060 ----
mean loss: 882.36
 ---- batch: 070 ----
mean loss: 875.13
 ---- batch: 080 ----
mean loss: 892.43
 ---- batch: 090 ----
mean loss: 890.32
train mean loss: 879.22
epoch train time: 0:00:01.650233
elapsed time: 0:01:44.508205
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 19:40:00.278013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.06
 ---- batch: 020 ----
mean loss: 900.75
 ---- batch: 030 ----
mean loss: 900.50
 ---- batch: 040 ----
mean loss: 881.42
 ---- batch: 050 ----
mean loss: 857.93
 ---- batch: 060 ----
mean loss: 888.03
 ---- batch: 070 ----
mean loss: 875.35
 ---- batch: 080 ----
mean loss: 878.85
 ---- batch: 090 ----
mean loss: 874.43
train mean loss: 879.72
epoch train time: 0:00:01.636130
elapsed time: 0:01:46.144834
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 19:40:01.914846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.83
 ---- batch: 020 ----
mean loss: 875.65
 ---- batch: 030 ----
mean loss: 881.54
 ---- batch: 040 ----
mean loss: 881.24
 ---- batch: 050 ----
mean loss: 880.40
 ---- batch: 060 ----
mean loss: 872.97
 ---- batch: 070 ----
mean loss: 884.04
 ---- batch: 080 ----
mean loss: 877.97
 ---- batch: 090 ----
mean loss: 867.21
train mean loss: 878.66
epoch train time: 0:00:01.625073
elapsed time: 0:01:47.770671
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 19:40:03.540646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.38
 ---- batch: 020 ----
mean loss: 876.31
 ---- batch: 030 ----
mean loss: 885.10
 ---- batch: 040 ----
mean loss: 869.27
 ---- batch: 050 ----
mean loss: 878.07
 ---- batch: 060 ----
mean loss: 865.81
 ---- batch: 070 ----
mean loss: 870.25
 ---- batch: 080 ----
mean loss: 824.22
 ---- batch: 090 ----
mean loss: 798.56
train mean loss: 851.85
epoch train time: 0:00:01.621297
elapsed time: 0:01:49.392617
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 19:40:05.162584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.27
 ---- batch: 020 ----
mean loss: 627.20
 ---- batch: 030 ----
mean loss: 568.33
 ---- batch: 040 ----
mean loss: 525.43
 ---- batch: 050 ----
mean loss: 523.15
 ---- batch: 060 ----
mean loss: 471.19
 ---- batch: 070 ----
mean loss: 472.29
 ---- batch: 080 ----
mean loss: 452.03
 ---- batch: 090 ----
mean loss: 444.16
train mean loss: 526.13
epoch train time: 0:00:01.626744
elapsed time: 0:01:51.020070
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 19:40:06.790049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.72
 ---- batch: 020 ----
mean loss: 412.04
 ---- batch: 030 ----
mean loss: 413.94
 ---- batch: 040 ----
mean loss: 400.33
 ---- batch: 050 ----
mean loss: 394.24
 ---- batch: 060 ----
mean loss: 395.34
 ---- batch: 070 ----
mean loss: 383.28
 ---- batch: 080 ----
mean loss: 385.87
 ---- batch: 090 ----
mean loss: 372.26
train mean loss: 396.33
epoch train time: 0:00:01.609096
elapsed time: 0:01:52.629818
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 19:40:08.399769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.58
 ---- batch: 020 ----
mean loss: 358.31
 ---- batch: 030 ----
mean loss: 365.02
 ---- batch: 040 ----
mean loss: 353.11
 ---- batch: 050 ----
mean loss: 351.00
 ---- batch: 060 ----
mean loss: 362.63
 ---- batch: 070 ----
mean loss: 340.14
 ---- batch: 080 ----
mean loss: 351.65
 ---- batch: 090 ----
mean loss: 338.14
train mean loss: 353.29
epoch train time: 0:00:01.575225
elapsed time: 0:01:54.205683
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 19:40:09.975693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.21
 ---- batch: 020 ----
mean loss: 329.40
 ---- batch: 030 ----
mean loss: 335.20
 ---- batch: 040 ----
mean loss: 320.63
 ---- batch: 050 ----
mean loss: 333.20
 ---- batch: 060 ----
mean loss: 323.47
 ---- batch: 070 ----
mean loss: 332.02
 ---- batch: 080 ----
mean loss: 314.58
 ---- batch: 090 ----
mean loss: 324.29
train mean loss: 327.09
epoch train time: 0:00:01.590761
elapsed time: 0:01:55.797138
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 19:40:11.567167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.65
 ---- batch: 020 ----
mean loss: 307.47
 ---- batch: 030 ----
mean loss: 310.28
 ---- batch: 040 ----
mean loss: 317.08
 ---- batch: 050 ----
mean loss: 304.57
 ---- batch: 060 ----
mean loss: 317.89
 ---- batch: 070 ----
mean loss: 315.11
 ---- batch: 080 ----
mean loss: 310.59
 ---- batch: 090 ----
mean loss: 300.79
train mean loss: 311.45
epoch train time: 0:00:01.665897
elapsed time: 0:01:57.463783
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 19:40:13.233846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.91
 ---- batch: 020 ----
mean loss: 294.72
 ---- batch: 030 ----
mean loss: 298.95
 ---- batch: 040 ----
mean loss: 297.24
 ---- batch: 050 ----
mean loss: 299.86
 ---- batch: 060 ----
mean loss: 301.34
 ---- batch: 070 ----
mean loss: 286.22
 ---- batch: 080 ----
mean loss: 296.26
 ---- batch: 090 ----
mean loss: 296.57
train mean loss: 297.72
epoch train time: 0:00:01.612074
elapsed time: 0:01:59.076635
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 19:40:14.846595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.62
 ---- batch: 020 ----
mean loss: 291.27
 ---- batch: 030 ----
mean loss: 302.73
 ---- batch: 040 ----
mean loss: 296.33
 ---- batch: 050 ----
mean loss: 284.36
 ---- batch: 060 ----
mean loss: 288.51
 ---- batch: 070 ----
mean loss: 279.94
 ---- batch: 080 ----
mean loss: 294.74
 ---- batch: 090 ----
mean loss: 284.26
train mean loss: 289.65
epoch train time: 0:00:01.640879
elapsed time: 0:02:00.718120
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 19:40:16.488075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.70
 ---- batch: 020 ----
mean loss: 284.12
 ---- batch: 030 ----
mean loss: 283.90
 ---- batch: 040 ----
mean loss: 277.46
 ---- batch: 050 ----
mean loss: 283.52
 ---- batch: 060 ----
mean loss: 283.82
 ---- batch: 070 ----
mean loss: 288.51
 ---- batch: 080 ----
mean loss: 278.52
 ---- batch: 090 ----
mean loss: 281.50
train mean loss: 283.18
epoch train time: 0:00:01.653190
elapsed time: 0:02:02.372030
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 19:40:18.141982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.64
 ---- batch: 020 ----
mean loss: 277.79
 ---- batch: 030 ----
mean loss: 278.56
 ---- batch: 040 ----
mean loss: 277.68
 ---- batch: 050 ----
mean loss: 280.18
 ---- batch: 060 ----
mean loss: 280.79
 ---- batch: 070 ----
mean loss: 288.43
 ---- batch: 080 ----
mean loss: 265.52
 ---- batch: 090 ----
mean loss: 262.51
train mean loss: 276.35
epoch train time: 0:00:01.633603
elapsed time: 0:02:04.006355
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 19:40:19.776330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.43
 ---- batch: 020 ----
mean loss: 269.81
 ---- batch: 030 ----
mean loss: 271.62
 ---- batch: 040 ----
mean loss: 279.07
 ---- batch: 050 ----
mean loss: 269.79
 ---- batch: 060 ----
mean loss: 265.99
 ---- batch: 070 ----
mean loss: 271.57
 ---- batch: 080 ----
mean loss: 270.50
 ---- batch: 090 ----
mean loss: 268.10
train mean loss: 270.78
epoch train time: 0:00:01.622593
elapsed time: 0:02:05.629573
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 19:40:21.399557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.71
 ---- batch: 020 ----
mean loss: 263.48
 ---- batch: 030 ----
mean loss: 261.89
 ---- batch: 040 ----
mean loss: 263.07
 ---- batch: 050 ----
mean loss: 271.00
 ---- batch: 060 ----
mean loss: 266.53
 ---- batch: 070 ----
mean loss: 261.16
 ---- batch: 080 ----
mean loss: 264.83
 ---- batch: 090 ----
mean loss: 262.09
train mean loss: 263.52
epoch train time: 0:00:01.605633
elapsed time: 0:02:07.235872
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 19:40:23.005860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.42
 ---- batch: 020 ----
mean loss: 255.32
 ---- batch: 030 ----
mean loss: 259.89
 ---- batch: 040 ----
mean loss: 264.02
 ---- batch: 050 ----
mean loss: 262.71
 ---- batch: 060 ----
mean loss: 258.47
 ---- batch: 070 ----
mean loss: 256.01
 ---- batch: 080 ----
mean loss: 262.53
 ---- batch: 090 ----
mean loss: 259.93
train mean loss: 260.76
epoch train time: 0:00:01.624818
elapsed time: 0:02:08.861375
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 19:40:24.631346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.88
 ---- batch: 020 ----
mean loss: 261.01
 ---- batch: 030 ----
mean loss: 244.83
 ---- batch: 040 ----
mean loss: 250.60
 ---- batch: 050 ----
mean loss: 255.94
 ---- batch: 060 ----
mean loss: 259.53
 ---- batch: 070 ----
mean loss: 250.80
 ---- batch: 080 ----
mean loss: 249.89
 ---- batch: 090 ----
mean loss: 252.83
train mean loss: 253.66
epoch train time: 0:00:01.648992
elapsed time: 0:02:10.511021
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 19:40:26.280985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.14
 ---- batch: 020 ----
mean loss: 251.05
 ---- batch: 030 ----
mean loss: 245.33
 ---- batch: 040 ----
mean loss: 247.26
 ---- batch: 050 ----
mean loss: 245.47
 ---- batch: 060 ----
mean loss: 246.95
 ---- batch: 070 ----
mean loss: 258.64
 ---- batch: 080 ----
mean loss: 262.67
 ---- batch: 090 ----
mean loss: 250.63
train mean loss: 250.96
epoch train time: 0:00:01.641460
elapsed time: 0:02:12.153110
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 19:40:27.923064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.83
 ---- batch: 020 ----
mean loss: 242.12
 ---- batch: 030 ----
mean loss: 242.49
 ---- batch: 040 ----
mean loss: 241.48
 ---- batch: 050 ----
mean loss: 250.96
 ---- batch: 060 ----
mean loss: 254.01
 ---- batch: 070 ----
mean loss: 241.16
 ---- batch: 080 ----
mean loss: 247.99
 ---- batch: 090 ----
mean loss: 255.02
train mean loss: 246.98
epoch train time: 0:00:01.645435
elapsed time: 0:02:13.799181
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 19:40:29.569150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.87
 ---- batch: 020 ----
mean loss: 236.39
 ---- batch: 030 ----
mean loss: 248.97
 ---- batch: 040 ----
mean loss: 234.53
 ---- batch: 050 ----
mean loss: 241.05
 ---- batch: 060 ----
mean loss: 242.94
 ---- batch: 070 ----
mean loss: 243.71
 ---- batch: 080 ----
mean loss: 248.05
 ---- batch: 090 ----
mean loss: 242.68
train mean loss: 242.40
epoch train time: 0:00:01.650814
elapsed time: 0:02:15.450652
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 19:40:31.220605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.31
 ---- batch: 020 ----
mean loss: 243.60
 ---- batch: 030 ----
mean loss: 237.67
 ---- batch: 040 ----
mean loss: 239.64
 ---- batch: 050 ----
mean loss: 252.74
 ---- batch: 060 ----
mean loss: 229.78
 ---- batch: 070 ----
mean loss: 235.06
 ---- batch: 080 ----
mean loss: 236.99
 ---- batch: 090 ----
mean loss: 234.18
train mean loss: 238.87
epoch train time: 0:00:01.643851
elapsed time: 0:02:17.095193
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 19:40:32.865162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.75
 ---- batch: 020 ----
mean loss: 231.80
 ---- batch: 030 ----
mean loss: 243.31
 ---- batch: 040 ----
mean loss: 245.79
 ---- batch: 050 ----
mean loss: 242.80
 ---- batch: 060 ----
mean loss: 234.98
 ---- batch: 070 ----
mean loss: 225.37
 ---- batch: 080 ----
mean loss: 225.41
 ---- batch: 090 ----
mean loss: 236.14
train mean loss: 237.32
epoch train time: 0:00:01.640571
elapsed time: 0:02:18.736479
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 19:40:34.506516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.83
 ---- batch: 020 ----
mean loss: 231.24
 ---- batch: 030 ----
mean loss: 233.20
 ---- batch: 040 ----
mean loss: 231.49
 ---- batch: 050 ----
mean loss: 235.21
 ---- batch: 060 ----
mean loss: 233.88
 ---- batch: 070 ----
mean loss: 234.83
 ---- batch: 080 ----
mean loss: 242.45
 ---- batch: 090 ----
mean loss: 229.63
train mean loss: 233.17
epoch train time: 0:00:01.627906
elapsed time: 0:02:20.365220
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 19:40:36.135266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.56
 ---- batch: 020 ----
mean loss: 225.56
 ---- batch: 030 ----
mean loss: 241.15
 ---- batch: 040 ----
mean loss: 233.84
 ---- batch: 050 ----
mean loss: 235.89
 ---- batch: 060 ----
mean loss: 242.47
 ---- batch: 070 ----
mean loss: 226.72
 ---- batch: 080 ----
mean loss: 225.11
 ---- batch: 090 ----
mean loss: 221.20
train mean loss: 230.51
epoch train time: 0:00:01.579431
elapsed time: 0:02:21.945405
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 19:40:37.715373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.38
 ---- batch: 020 ----
mean loss: 231.82
 ---- batch: 030 ----
mean loss: 223.27
 ---- batch: 040 ----
mean loss: 221.65
 ---- batch: 050 ----
mean loss: 222.07
 ---- batch: 060 ----
mean loss: 223.70
 ---- batch: 070 ----
mean loss: 226.59
 ---- batch: 080 ----
mean loss: 234.95
 ---- batch: 090 ----
mean loss: 225.57
train mean loss: 226.79
epoch train time: 0:00:01.634061
elapsed time: 0:02:23.580267
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 19:40:39.350302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.39
 ---- batch: 020 ----
mean loss: 226.97
 ---- batch: 030 ----
mean loss: 215.50
 ---- batch: 040 ----
mean loss: 224.65
 ---- batch: 050 ----
mean loss: 223.03
 ---- batch: 060 ----
mean loss: 224.30
 ---- batch: 070 ----
mean loss: 231.31
 ---- batch: 080 ----
mean loss: 224.16
 ---- batch: 090 ----
mean loss: 227.14
train mean loss: 225.12
epoch train time: 0:00:01.652538
elapsed time: 0:02:25.233561
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 19:40:41.003521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.93
 ---- batch: 020 ----
mean loss: 223.13
 ---- batch: 030 ----
mean loss: 221.20
 ---- batch: 040 ----
mean loss: 219.29
 ---- batch: 050 ----
mean loss: 222.37
 ---- batch: 060 ----
mean loss: 212.60
 ---- batch: 070 ----
mean loss: 211.02
 ---- batch: 080 ----
mean loss: 226.55
 ---- batch: 090 ----
mean loss: 232.03
train mean loss: 222.18
epoch train time: 0:00:01.647412
elapsed time: 0:02:26.881622
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 19:40:42.651577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.52
 ---- batch: 020 ----
mean loss: 210.15
 ---- batch: 030 ----
mean loss: 221.48
 ---- batch: 040 ----
mean loss: 225.95
 ---- batch: 050 ----
mean loss: 220.30
 ---- batch: 060 ----
mean loss: 213.80
 ---- batch: 070 ----
mean loss: 219.65
 ---- batch: 080 ----
mean loss: 220.02
 ---- batch: 090 ----
mean loss: 223.45
train mean loss: 218.69
epoch train time: 0:00:01.602169
elapsed time: 0:02:28.484405
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 19:40:44.254356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.02
 ---- batch: 020 ----
mean loss: 215.56
 ---- batch: 030 ----
mean loss: 216.77
 ---- batch: 040 ----
mean loss: 221.55
 ---- batch: 050 ----
mean loss: 221.99
 ---- batch: 060 ----
mean loss: 223.23
 ---- batch: 070 ----
mean loss: 209.63
 ---- batch: 080 ----
mean loss: 219.62
 ---- batch: 090 ----
mean loss: 217.96
train mean loss: 217.14
epoch train time: 0:00:01.606424
elapsed time: 0:02:30.091486
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 19:40:45.861488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.29
 ---- batch: 020 ----
mean loss: 202.26
 ---- batch: 030 ----
mean loss: 215.74
 ---- batch: 040 ----
mean loss: 208.65
 ---- batch: 050 ----
mean loss: 223.73
 ---- batch: 060 ----
mean loss: 214.50
 ---- batch: 070 ----
mean loss: 219.32
 ---- batch: 080 ----
mean loss: 224.28
 ---- batch: 090 ----
mean loss: 217.80
train mean loss: 213.70
epoch train time: 0:00:01.617346
elapsed time: 0:02:31.709502
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 19:40:47.479445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.90
 ---- batch: 020 ----
mean loss: 212.45
 ---- batch: 030 ----
mean loss: 219.95
 ---- batch: 040 ----
mean loss: 206.03
 ---- batch: 050 ----
mean loss: 209.13
 ---- batch: 060 ----
mean loss: 208.89
 ---- batch: 070 ----
mean loss: 209.58
 ---- batch: 080 ----
mean loss: 214.13
 ---- batch: 090 ----
mean loss: 214.64
train mean loss: 211.76
epoch train time: 0:00:01.636480
elapsed time: 0:02:33.346613
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 19:40:49.116611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.11
 ---- batch: 020 ----
mean loss: 211.78
 ---- batch: 030 ----
mean loss: 207.85
 ---- batch: 040 ----
mean loss: 211.40
 ---- batch: 050 ----
mean loss: 210.22
 ---- batch: 060 ----
mean loss: 210.99
 ---- batch: 070 ----
mean loss: 202.91
 ---- batch: 080 ----
mean loss: 207.25
 ---- batch: 090 ----
mean loss: 213.25
train mean loss: 209.31
epoch train time: 0:00:01.634655
elapsed time: 0:02:34.981977
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 19:40:50.751930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.28
 ---- batch: 020 ----
mean loss: 206.01
 ---- batch: 030 ----
mean loss: 209.65
 ---- batch: 040 ----
mean loss: 213.13
 ---- batch: 050 ----
mean loss: 201.17
 ---- batch: 060 ----
mean loss: 204.19
 ---- batch: 070 ----
mean loss: 211.70
 ---- batch: 080 ----
mean loss: 210.65
 ---- batch: 090 ----
mean loss: 207.75
train mean loss: 207.59
epoch train time: 0:00:01.627670
elapsed time: 0:02:36.610244
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 19:40:52.380199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.59
 ---- batch: 020 ----
mean loss: 203.72
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 200.49
 ---- batch: 050 ----
mean loss: 205.87
 ---- batch: 060 ----
mean loss: 205.31
 ---- batch: 070 ----
mean loss: 211.82
 ---- batch: 080 ----
mean loss: 210.53
 ---- batch: 090 ----
mean loss: 204.56
train mean loss: 205.13
epoch train time: 0:00:01.632005
elapsed time: 0:02:38.243073
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 19:40:54.013066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.94
 ---- batch: 020 ----
mean loss: 202.39
 ---- batch: 030 ----
mean loss: 198.21
 ---- batch: 040 ----
mean loss: 209.97
 ---- batch: 050 ----
mean loss: 194.47
 ---- batch: 060 ----
mean loss: 207.66
 ---- batch: 070 ----
mean loss: 206.00
 ---- batch: 080 ----
mean loss: 208.08
 ---- batch: 090 ----
mean loss: 208.27
train mean loss: 203.56
epoch train time: 0:00:01.626940
elapsed time: 0:02:39.870714
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 19:40:55.640678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.75
 ---- batch: 020 ----
mean loss: 199.70
 ---- batch: 030 ----
mean loss: 200.95
 ---- batch: 040 ----
mean loss: 212.51
 ---- batch: 050 ----
mean loss: 204.02
 ---- batch: 060 ----
mean loss: 200.60
 ---- batch: 070 ----
mean loss: 209.76
 ---- batch: 080 ----
mean loss: 201.59
 ---- batch: 090 ----
mean loss: 209.91
train mean loss: 202.64
epoch train time: 0:00:01.610392
elapsed time: 0:02:41.481988
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 19:40:57.251966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.26
 ---- batch: 020 ----
mean loss: 197.66
 ---- batch: 030 ----
mean loss: 197.65
 ---- batch: 040 ----
mean loss: 209.57
 ---- batch: 050 ----
mean loss: 198.37
 ---- batch: 060 ----
mean loss: 206.60
 ---- batch: 070 ----
mean loss: 192.81
 ---- batch: 080 ----
mean loss: 201.39
 ---- batch: 090 ----
mean loss: 200.55
train mean loss: 199.63
epoch train time: 0:00:01.648405
elapsed time: 0:02:43.131066
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 19:40:58.901038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.20
 ---- batch: 020 ----
mean loss: 187.63
 ---- batch: 030 ----
mean loss: 197.12
 ---- batch: 040 ----
mean loss: 199.19
 ---- batch: 050 ----
mean loss: 200.26
 ---- batch: 060 ----
mean loss: 197.43
 ---- batch: 070 ----
mean loss: 197.27
 ---- batch: 080 ----
mean loss: 196.07
 ---- batch: 090 ----
mean loss: 209.14
train mean loss: 197.88
epoch train time: 0:00:01.626820
elapsed time: 0:02:44.758601
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 19:41:00.528586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.52
 ---- batch: 020 ----
mean loss: 197.37
 ---- batch: 030 ----
mean loss: 201.43
 ---- batch: 040 ----
mean loss: 195.21
 ---- batch: 050 ----
mean loss: 199.11
 ---- batch: 060 ----
mean loss: 196.08
 ---- batch: 070 ----
mean loss: 193.00
 ---- batch: 080 ----
mean loss: 193.84
 ---- batch: 090 ----
mean loss: 198.27
train mean loss: 195.69
epoch train time: 0:00:01.575124
elapsed time: 0:02:46.334416
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 19:41:02.104366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.28
 ---- batch: 020 ----
mean loss: 191.69
 ---- batch: 030 ----
mean loss: 186.95
 ---- batch: 040 ----
mean loss: 190.53
 ---- batch: 050 ----
mean loss: 202.10
 ---- batch: 060 ----
mean loss: 193.42
 ---- batch: 070 ----
mean loss: 191.86
 ---- batch: 080 ----
mean loss: 203.53
 ---- batch: 090 ----
mean loss: 195.48
train mean loss: 194.31
epoch train time: 0:00:01.585283
elapsed time: 0:02:47.920375
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 19:41:03.690347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.53
 ---- batch: 020 ----
mean loss: 177.22
 ---- batch: 030 ----
mean loss: 194.71
 ---- batch: 040 ----
mean loss: 189.89
 ---- batch: 050 ----
mean loss: 197.42
 ---- batch: 060 ----
mean loss: 192.69
 ---- batch: 070 ----
mean loss: 188.33
 ---- batch: 080 ----
mean loss: 193.97
 ---- batch: 090 ----
mean loss: 199.53
train mean loss: 191.92
epoch train time: 0:00:01.629997
elapsed time: 0:02:49.551122
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 19:41:05.321160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.55
 ---- batch: 020 ----
mean loss: 185.09
 ---- batch: 030 ----
mean loss: 188.57
 ---- batch: 040 ----
mean loss: 185.88
 ---- batch: 050 ----
mean loss: 197.51
 ---- batch: 060 ----
mean loss: 197.12
 ---- batch: 070 ----
mean loss: 180.58
 ---- batch: 080 ----
mean loss: 187.69
 ---- batch: 090 ----
mean loss: 202.95
train mean loss: 190.91
epoch train time: 0:00:01.616502
elapsed time: 0:02:51.168370
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 19:41:06.938330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.89
 ---- batch: 020 ----
mean loss: 192.51
 ---- batch: 030 ----
mean loss: 183.16
 ---- batch: 040 ----
mean loss: 194.50
 ---- batch: 050 ----
mean loss: 187.96
 ---- batch: 060 ----
mean loss: 179.64
 ---- batch: 070 ----
mean loss: 179.87
 ---- batch: 080 ----
mean loss: 194.79
 ---- batch: 090 ----
mean loss: 196.47
train mean loss: 189.02
epoch train time: 0:00:01.637261
elapsed time: 0:02:52.806371
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 19:41:08.576151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.22
 ---- batch: 020 ----
mean loss: 185.07
 ---- batch: 030 ----
mean loss: 188.58
 ---- batch: 040 ----
mean loss: 190.55
 ---- batch: 050 ----
mean loss: 183.34
 ---- batch: 060 ----
mean loss: 183.44
 ---- batch: 070 ----
mean loss: 180.92
 ---- batch: 080 ----
mean loss: 191.46
 ---- batch: 090 ----
mean loss: 180.33
train mean loss: 185.27
epoch train time: 0:00:01.622954
elapsed time: 0:02:54.429806
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 19:41:10.199771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.14
 ---- batch: 020 ----
mean loss: 181.40
 ---- batch: 030 ----
mean loss: 180.91
 ---- batch: 040 ----
mean loss: 188.19
 ---- batch: 050 ----
mean loss: 174.69
 ---- batch: 060 ----
mean loss: 185.68
 ---- batch: 070 ----
mean loss: 188.95
 ---- batch: 080 ----
mean loss: 191.74
 ---- batch: 090 ----
mean loss: 191.40
train mean loss: 185.76
epoch train time: 0:00:01.613188
elapsed time: 0:02:56.043693
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 19:41:11.813710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.96
 ---- batch: 020 ----
mean loss: 181.58
 ---- batch: 030 ----
mean loss: 179.99
 ---- batch: 040 ----
mean loss: 188.34
 ---- batch: 050 ----
mean loss: 179.62
 ---- batch: 060 ----
mean loss: 188.25
 ---- batch: 070 ----
mean loss: 181.35
 ---- batch: 080 ----
mean loss: 186.34
 ---- batch: 090 ----
mean loss: 184.43
train mean loss: 183.96
epoch train time: 0:00:01.581072
elapsed time: 0:02:57.625497
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 19:41:13.395484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.76
 ---- batch: 020 ----
mean loss: 181.64
 ---- batch: 030 ----
mean loss: 180.97
 ---- batch: 040 ----
mean loss: 179.24
 ---- batch: 050 ----
mean loss: 184.73
 ---- batch: 060 ----
mean loss: 180.20
 ---- batch: 070 ----
mean loss: 183.73
 ---- batch: 080 ----
mean loss: 187.55
 ---- batch: 090 ----
mean loss: 184.95
train mean loss: 181.95
epoch train time: 0:00:01.550065
elapsed time: 0:02:59.176264
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 19:41:14.946228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.11
 ---- batch: 020 ----
mean loss: 180.53
 ---- batch: 030 ----
mean loss: 172.02
 ---- batch: 040 ----
mean loss: 180.22
 ---- batch: 050 ----
mean loss: 186.79
 ---- batch: 060 ----
mean loss: 192.61
 ---- batch: 070 ----
mean loss: 177.56
 ---- batch: 080 ----
mean loss: 189.20
 ---- batch: 090 ----
mean loss: 180.64
train mean loss: 181.56
epoch train time: 0:00:01.565433
elapsed time: 0:03:00.742349
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 19:41:16.512296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.86
 ---- batch: 020 ----
mean loss: 184.79
 ---- batch: 030 ----
mean loss: 174.90
 ---- batch: 040 ----
mean loss: 178.51
 ---- batch: 050 ----
mean loss: 182.78
 ---- batch: 060 ----
mean loss: 182.44
 ---- batch: 070 ----
mean loss: 179.39
 ---- batch: 080 ----
mean loss: 182.35
 ---- batch: 090 ----
mean loss: 175.21
train mean loss: 180.02
epoch train time: 0:00:01.598646
elapsed time: 0:03:02.341603
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 19:41:18.111576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.33
 ---- batch: 020 ----
mean loss: 179.31
 ---- batch: 030 ----
mean loss: 174.91
 ---- batch: 040 ----
mean loss: 178.79
 ---- batch: 050 ----
mean loss: 170.99
 ---- batch: 060 ----
mean loss: 183.24
 ---- batch: 070 ----
mean loss: 178.21
 ---- batch: 080 ----
mean loss: 187.84
 ---- batch: 090 ----
mean loss: 177.41
train mean loss: 178.60
epoch train time: 0:00:01.663612
elapsed time: 0:03:04.005879
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 19:41:19.775830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.89
 ---- batch: 020 ----
mean loss: 179.47
 ---- batch: 030 ----
mean loss: 173.72
 ---- batch: 040 ----
mean loss: 176.90
 ---- batch: 050 ----
mean loss: 172.36
 ---- batch: 060 ----
mean loss: 181.35
 ---- batch: 070 ----
mean loss: 180.33
 ---- batch: 080 ----
mean loss: 181.23
 ---- batch: 090 ----
mean loss: 176.17
train mean loss: 176.52
epoch train time: 0:00:01.629915
elapsed time: 0:03:05.636433
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 19:41:21.406391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.54
 ---- batch: 020 ----
mean loss: 168.28
 ---- batch: 030 ----
mean loss: 183.38
 ---- batch: 040 ----
mean loss: 168.41
 ---- batch: 050 ----
mean loss: 172.53
 ---- batch: 060 ----
mean loss: 180.99
 ---- batch: 070 ----
mean loss: 175.99
 ---- batch: 080 ----
mean loss: 172.78
 ---- batch: 090 ----
mean loss: 184.30
train mean loss: 175.50
epoch train time: 0:00:01.653820
elapsed time: 0:03:07.290884
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 19:41:23.060842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.69
 ---- batch: 020 ----
mean loss: 168.69
 ---- batch: 030 ----
mean loss: 171.46
 ---- batch: 040 ----
mean loss: 171.01
 ---- batch: 050 ----
mean loss: 173.88
 ---- batch: 060 ----
mean loss: 180.85
 ---- batch: 070 ----
mean loss: 173.73
 ---- batch: 080 ----
mean loss: 173.87
 ---- batch: 090 ----
mean loss: 177.00
train mean loss: 174.14
epoch train time: 0:00:01.646078
elapsed time: 0:03:08.937637
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 19:41:24.707619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.08
 ---- batch: 020 ----
mean loss: 165.24
 ---- batch: 030 ----
mean loss: 172.03
 ---- batch: 040 ----
mean loss: 173.40
 ---- batch: 050 ----
mean loss: 172.78
 ---- batch: 060 ----
mean loss: 175.66
 ---- batch: 070 ----
mean loss: 178.53
 ---- batch: 080 ----
mean loss: 179.58
 ---- batch: 090 ----
mean loss: 168.59
train mean loss: 172.54
epoch train time: 0:00:01.629889
elapsed time: 0:03:10.568218
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 19:41:26.338183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.98
 ---- batch: 020 ----
mean loss: 171.17
 ---- batch: 030 ----
mean loss: 167.52
 ---- batch: 040 ----
mean loss: 168.77
 ---- batch: 050 ----
mean loss: 171.88
 ---- batch: 060 ----
mean loss: 170.47
 ---- batch: 070 ----
mean loss: 177.45
 ---- batch: 080 ----
mean loss: 172.01
 ---- batch: 090 ----
mean loss: 177.26
train mean loss: 171.62
epoch train time: 0:00:01.603515
elapsed time: 0:03:12.172429
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 19:41:27.942437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.88
 ---- batch: 020 ----
mean loss: 170.60
 ---- batch: 030 ----
mean loss: 165.88
 ---- batch: 040 ----
mean loss: 167.26
 ---- batch: 050 ----
mean loss: 168.30
 ---- batch: 060 ----
mean loss: 178.10
 ---- batch: 070 ----
mean loss: 173.43
 ---- batch: 080 ----
mean loss: 174.21
 ---- batch: 090 ----
mean loss: 170.06
train mean loss: 170.49
epoch train time: 0:00:01.600426
elapsed time: 0:03:13.773565
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 19:41:29.543518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.10
 ---- batch: 020 ----
mean loss: 161.41
 ---- batch: 030 ----
mean loss: 165.38
 ---- batch: 040 ----
mean loss: 163.71
 ---- batch: 050 ----
mean loss: 175.72
 ---- batch: 060 ----
mean loss: 170.13
 ---- batch: 070 ----
mean loss: 167.31
 ---- batch: 080 ----
mean loss: 178.29
 ---- batch: 090 ----
mean loss: 173.75
train mean loss: 169.84
epoch train time: 0:00:01.618829
elapsed time: 0:03:15.393052
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 19:41:31.163025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.35
 ---- batch: 020 ----
mean loss: 161.35
 ---- batch: 030 ----
mean loss: 162.34
 ---- batch: 040 ----
mean loss: 167.75
 ---- batch: 050 ----
mean loss: 173.33
 ---- batch: 060 ----
mean loss: 161.85
 ---- batch: 070 ----
mean loss: 165.96
 ---- batch: 080 ----
mean loss: 172.78
 ---- batch: 090 ----
mean loss: 174.46
train mean loss: 168.09
epoch train time: 0:00:01.587899
elapsed time: 0:03:16.981653
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 19:41:32.751615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.29
 ---- batch: 020 ----
mean loss: 164.04
 ---- batch: 030 ----
mean loss: 165.21
 ---- batch: 040 ----
mean loss: 164.46
 ---- batch: 050 ----
mean loss: 166.94
 ---- batch: 060 ----
mean loss: 174.21
 ---- batch: 070 ----
mean loss: 164.02
 ---- batch: 080 ----
mean loss: 166.24
 ---- batch: 090 ----
mean loss: 165.29
train mean loss: 167.11
epoch train time: 0:00:01.601928
elapsed time: 0:03:18.584253
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 19:41:34.354243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.90
 ---- batch: 020 ----
mean loss: 158.77
 ---- batch: 030 ----
mean loss: 163.41
 ---- batch: 040 ----
mean loss: 166.27
 ---- batch: 050 ----
mean loss: 168.89
 ---- batch: 060 ----
mean loss: 165.56
 ---- batch: 070 ----
mean loss: 163.13
 ---- batch: 080 ----
mean loss: 165.77
 ---- batch: 090 ----
mean loss: 170.48
train mean loss: 165.54
epoch train time: 0:00:01.642874
elapsed time: 0:03:20.227849
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 19:41:35.997831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.38
 ---- batch: 020 ----
mean loss: 164.09
 ---- batch: 030 ----
mean loss: 157.22
 ---- batch: 040 ----
mean loss: 160.73
 ---- batch: 050 ----
mean loss: 163.00
 ---- batch: 060 ----
mean loss: 167.63
 ---- batch: 070 ----
mean loss: 170.44
 ---- batch: 080 ----
mean loss: 161.48
 ---- batch: 090 ----
mean loss: 160.95
train mean loss: 164.27
epoch train time: 0:00:01.608698
elapsed time: 0:03:21.837213
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 19:41:37.607199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.13
 ---- batch: 020 ----
mean loss: 169.18
 ---- batch: 030 ----
mean loss: 160.29
 ---- batch: 040 ----
mean loss: 166.26
 ---- batch: 050 ----
mean loss: 161.29
 ---- batch: 060 ----
mean loss: 167.09
 ---- batch: 070 ----
mean loss: 160.21
 ---- batch: 080 ----
mean loss: 162.93
 ---- batch: 090 ----
mean loss: 163.51
train mean loss: 164.38
epoch train time: 0:00:01.614820
elapsed time: 0:03:23.452717
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 19:41:39.222678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.53
 ---- batch: 020 ----
mean loss: 158.50
 ---- batch: 030 ----
mean loss: 169.39
 ---- batch: 040 ----
mean loss: 156.16
 ---- batch: 050 ----
mean loss: 157.47
 ---- batch: 060 ----
mean loss: 163.44
 ---- batch: 070 ----
mean loss: 166.51
 ---- batch: 080 ----
mean loss: 167.45
 ---- batch: 090 ----
mean loss: 165.70
train mean loss: 162.93
epoch train time: 0:00:01.633874
elapsed time: 0:03:25.087367
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 19:41:40.857320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.81
 ---- batch: 020 ----
mean loss: 162.65
 ---- batch: 030 ----
mean loss: 158.18
 ---- batch: 040 ----
mean loss: 161.12
 ---- batch: 050 ----
mean loss: 165.20
 ---- batch: 060 ----
mean loss: 162.24
 ---- batch: 070 ----
mean loss: 161.10
 ---- batch: 080 ----
mean loss: 160.89
 ---- batch: 090 ----
mean loss: 161.26
train mean loss: 161.67
epoch train time: 0:00:01.589443
elapsed time: 0:03:26.677590
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 19:41:42.447376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.28
 ---- batch: 020 ----
mean loss: 158.19
 ---- batch: 030 ----
mean loss: 157.80
 ---- batch: 040 ----
mean loss: 164.88
 ---- batch: 050 ----
mean loss: 159.37
 ---- batch: 060 ----
mean loss: 159.61
 ---- batch: 070 ----
mean loss: 170.99
 ---- batch: 080 ----
mean loss: 164.02
 ---- batch: 090 ----
mean loss: 161.64
train mean loss: 160.28
epoch train time: 0:00:01.605411
elapsed time: 0:03:28.283421
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 19:41:44.053363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.22
 ---- batch: 020 ----
mean loss: 161.55
 ---- batch: 030 ----
mean loss: 157.65
 ---- batch: 040 ----
mean loss: 159.47
 ---- batch: 050 ----
mean loss: 158.38
 ---- batch: 060 ----
mean loss: 159.50
 ---- batch: 070 ----
mean loss: 160.55
 ---- batch: 080 ----
mean loss: 161.31
 ---- batch: 090 ----
mean loss: 161.81
train mean loss: 160.47
epoch train time: 0:00:01.588846
elapsed time: 0:03:29.872887
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 19:41:45.642836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.59
 ---- batch: 020 ----
mean loss: 154.38
 ---- batch: 030 ----
mean loss: 154.81
 ---- batch: 040 ----
mean loss: 151.79
 ---- batch: 050 ----
mean loss: 155.53
 ---- batch: 060 ----
mean loss: 162.54
 ---- batch: 070 ----
mean loss: 164.18
 ---- batch: 080 ----
mean loss: 162.13
 ---- batch: 090 ----
mean loss: 159.64
train mean loss: 158.87
epoch train time: 0:00:01.580915
elapsed time: 0:03:31.454490
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 19:41:47.224438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.52
 ---- batch: 020 ----
mean loss: 157.86
 ---- batch: 030 ----
mean loss: 157.41
 ---- batch: 040 ----
mean loss: 152.61
 ---- batch: 050 ----
mean loss: 157.73
 ---- batch: 060 ----
mean loss: 159.18
 ---- batch: 070 ----
mean loss: 156.28
 ---- batch: 080 ----
mean loss: 155.79
 ---- batch: 090 ----
mean loss: 163.60
train mean loss: 157.61
epoch train time: 0:00:01.631474
elapsed time: 0:03:33.086606
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 19:41:48.856571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.61
 ---- batch: 020 ----
mean loss: 146.26
 ---- batch: 030 ----
mean loss: 162.29
 ---- batch: 040 ----
mean loss: 161.25
 ---- batch: 050 ----
mean loss: 161.87
 ---- batch: 060 ----
mean loss: 159.13
 ---- batch: 070 ----
mean loss: 162.39
 ---- batch: 080 ----
mean loss: 154.59
 ---- batch: 090 ----
mean loss: 157.24
train mean loss: 157.20
epoch train time: 0:00:01.596783
elapsed time: 0:03:34.684055
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 19:41:50.454035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.49
 ---- batch: 020 ----
mean loss: 154.72
 ---- batch: 030 ----
mean loss: 154.31
 ---- batch: 040 ----
mean loss: 156.72
 ---- batch: 050 ----
mean loss: 154.13
 ---- batch: 060 ----
mean loss: 164.00
 ---- batch: 070 ----
mean loss: 160.69
 ---- batch: 080 ----
mean loss: 154.53
 ---- batch: 090 ----
mean loss: 155.41
train mean loss: 156.43
epoch train time: 0:00:01.596264
elapsed time: 0:03:36.281002
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 19:41:52.050954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.01
 ---- batch: 020 ----
mean loss: 150.89
 ---- batch: 030 ----
mean loss: 152.65
 ---- batch: 040 ----
mean loss: 153.50
 ---- batch: 050 ----
mean loss: 158.14
 ---- batch: 060 ----
mean loss: 160.97
 ---- batch: 070 ----
mean loss: 148.79
 ---- batch: 080 ----
mean loss: 157.97
 ---- batch: 090 ----
mean loss: 158.70
train mean loss: 155.85
epoch train time: 0:00:01.581232
elapsed time: 0:03:37.862865
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 19:41:53.632819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.89
 ---- batch: 020 ----
mean loss: 154.22
 ---- batch: 030 ----
mean loss: 151.46
 ---- batch: 040 ----
mean loss: 156.34
 ---- batch: 050 ----
mean loss: 153.92
 ---- batch: 060 ----
mean loss: 160.58
 ---- batch: 070 ----
mean loss: 161.28
 ---- batch: 080 ----
mean loss: 144.91
 ---- batch: 090 ----
mean loss: 159.98
train mean loss: 155.05
epoch train time: 0:00:01.593357
elapsed time: 0:03:39.456821
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 19:41:55.226759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.39
 ---- batch: 020 ----
mean loss: 151.29
 ---- batch: 030 ----
mean loss: 150.36
 ---- batch: 040 ----
mean loss: 147.17
 ---- batch: 050 ----
mean loss: 160.20
 ---- batch: 060 ----
mean loss: 151.15
 ---- batch: 070 ----
mean loss: 157.36
 ---- batch: 080 ----
mean loss: 160.66
 ---- batch: 090 ----
mean loss: 160.08
train mean loss: 154.01
epoch train time: 0:00:01.622223
elapsed time: 0:03:41.079690
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 19:41:56.849799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.38
 ---- batch: 020 ----
mean loss: 143.85
 ---- batch: 030 ----
mean loss: 149.80
 ---- batch: 040 ----
mean loss: 153.30
 ---- batch: 050 ----
mean loss: 155.11
 ---- batch: 060 ----
mean loss: 154.60
 ---- batch: 070 ----
mean loss: 155.25
 ---- batch: 080 ----
mean loss: 157.80
 ---- batch: 090 ----
mean loss: 162.33
train mean loss: 152.57
epoch train time: 0:00:01.641381
elapsed time: 0:03:42.721846
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 19:41:58.491802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.91
 ---- batch: 020 ----
mean loss: 141.66
 ---- batch: 030 ----
mean loss: 150.48
 ---- batch: 040 ----
mean loss: 149.50
 ---- batch: 050 ----
mean loss: 152.85
 ---- batch: 060 ----
mean loss: 153.83
 ---- batch: 070 ----
mean loss: 154.06
 ---- batch: 080 ----
mean loss: 154.33
 ---- batch: 090 ----
mean loss: 162.33
train mean loss: 152.00
epoch train time: 0:00:01.613694
elapsed time: 0:03:44.336132
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 19:42:00.106084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.86
 ---- batch: 020 ----
mean loss: 154.89
 ---- batch: 030 ----
mean loss: 150.72
 ---- batch: 040 ----
mean loss: 154.12
 ---- batch: 050 ----
mean loss: 151.99
 ---- batch: 060 ----
mean loss: 149.26
 ---- batch: 070 ----
mean loss: 154.10
 ---- batch: 080 ----
mean loss: 154.39
 ---- batch: 090 ----
mean loss: 143.31
train mean loss: 151.38
epoch train time: 0:00:01.567466
elapsed time: 0:03:45.904278
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 19:42:01.674253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.67
 ---- batch: 020 ----
mean loss: 150.62
 ---- batch: 030 ----
mean loss: 148.14
 ---- batch: 040 ----
mean loss: 150.17
 ---- batch: 050 ----
mean loss: 153.54
 ---- batch: 060 ----
mean loss: 153.62
 ---- batch: 070 ----
mean loss: 151.92
 ---- batch: 080 ----
mean loss: 156.78
 ---- batch: 090 ----
mean loss: 149.98
train mean loss: 151.67
epoch train time: 0:00:01.627868
elapsed time: 0:03:47.532803
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 19:42:03.302756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.53
 ---- batch: 020 ----
mean loss: 144.51
 ---- batch: 030 ----
mean loss: 146.32
 ---- batch: 040 ----
mean loss: 151.65
 ---- batch: 050 ----
mean loss: 155.21
 ---- batch: 060 ----
mean loss: 152.08
 ---- batch: 070 ----
mean loss: 147.26
 ---- batch: 080 ----
mean loss: 151.76
 ---- batch: 090 ----
mean loss: 152.90
train mean loss: 149.94
epoch train time: 0:00:01.590156
elapsed time: 0:03:49.123643
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 19:42:04.893586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.90
 ---- batch: 020 ----
mean loss: 143.55
 ---- batch: 030 ----
mean loss: 150.27
 ---- batch: 040 ----
mean loss: 148.21
 ---- batch: 050 ----
mean loss: 151.00
 ---- batch: 060 ----
mean loss: 148.56
 ---- batch: 070 ----
mean loss: 152.94
 ---- batch: 080 ----
mean loss: 156.73
 ---- batch: 090 ----
mean loss: 150.63
train mean loss: 149.47
epoch train time: 0:00:01.588762
elapsed time: 0:03:50.713025
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 19:42:06.482991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.15
 ---- batch: 020 ----
mean loss: 149.54
 ---- batch: 030 ----
mean loss: 145.22
 ---- batch: 040 ----
mean loss: 146.98
 ---- batch: 050 ----
mean loss: 151.36
 ---- batch: 060 ----
mean loss: 148.29
 ---- batch: 070 ----
mean loss: 145.70
 ---- batch: 080 ----
mean loss: 151.39
 ---- batch: 090 ----
mean loss: 145.15
train mean loss: 148.63
epoch train time: 0:00:01.639167
elapsed time: 0:03:52.352814
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 19:42:08.122774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.47
 ---- batch: 020 ----
mean loss: 141.42
 ---- batch: 030 ----
mean loss: 142.43
 ---- batch: 040 ----
mean loss: 150.45
 ---- batch: 050 ----
mean loss: 152.26
 ---- batch: 060 ----
mean loss: 144.82
 ---- batch: 070 ----
mean loss: 150.03
 ---- batch: 080 ----
mean loss: 150.54
 ---- batch: 090 ----
mean loss: 153.31
train mean loss: 147.11
epoch train time: 0:00:01.613704
elapsed time: 0:03:53.967177
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 19:42:09.737146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.62
 ---- batch: 020 ----
mean loss: 145.45
 ---- batch: 030 ----
mean loss: 140.26
 ---- batch: 040 ----
mean loss: 149.31
 ---- batch: 050 ----
mean loss: 148.10
 ---- batch: 060 ----
mean loss: 151.87
 ---- batch: 070 ----
mean loss: 149.89
 ---- batch: 080 ----
mean loss: 146.26
 ---- batch: 090 ----
mean loss: 146.87
train mean loss: 146.86
epoch train time: 0:00:01.636425
elapsed time: 0:03:55.604253
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 19:42:11.374209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.53
 ---- batch: 020 ----
mean loss: 141.41
 ---- batch: 030 ----
mean loss: 140.33
 ---- batch: 040 ----
mean loss: 142.35
 ---- batch: 050 ----
mean loss: 144.35
 ---- batch: 060 ----
mean loss: 144.74
 ---- batch: 070 ----
mean loss: 146.05
 ---- batch: 080 ----
mean loss: 150.33
 ---- batch: 090 ----
mean loss: 159.21
train mean loss: 145.44
epoch train time: 0:00:01.634776
elapsed time: 0:03:57.239738
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 19:42:13.009771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.74
 ---- batch: 020 ----
mean loss: 144.66
 ---- batch: 030 ----
mean loss: 142.82
 ---- batch: 040 ----
mean loss: 149.27
 ---- batch: 050 ----
mean loss: 138.60
 ---- batch: 060 ----
mean loss: 148.70
 ---- batch: 070 ----
mean loss: 148.67
 ---- batch: 080 ----
mean loss: 150.31
 ---- batch: 090 ----
mean loss: 149.81
train mean loss: 145.21
epoch train time: 0:00:01.630015
elapsed time: 0:03:58.870684
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 19:42:14.640501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.65
 ---- batch: 020 ----
mean loss: 144.96
 ---- batch: 030 ----
mean loss: 139.18
 ---- batch: 040 ----
mean loss: 144.42
 ---- batch: 050 ----
mean loss: 150.55
 ---- batch: 060 ----
mean loss: 145.51
 ---- batch: 070 ----
mean loss: 145.56
 ---- batch: 080 ----
mean loss: 144.21
 ---- batch: 090 ----
mean loss: 154.07
train mean loss: 145.08
epoch train time: 0:00:01.586261
elapsed time: 0:04:00.457444
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 19:42:16.227386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.23
 ---- batch: 020 ----
mean loss: 136.74
 ---- batch: 030 ----
mean loss: 140.24
 ---- batch: 040 ----
mean loss: 145.56
 ---- batch: 050 ----
mean loss: 143.99
 ---- batch: 060 ----
mean loss: 140.56
 ---- batch: 070 ----
mean loss: 146.10
 ---- batch: 080 ----
mean loss: 151.61
 ---- batch: 090 ----
mean loss: 148.02
train mean loss: 143.81
epoch train time: 0:00:01.586810
elapsed time: 0:04:02.044892
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 19:42:17.814848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.50
 ---- batch: 020 ----
mean loss: 141.58
 ---- batch: 030 ----
mean loss: 137.85
 ---- batch: 040 ----
mean loss: 142.23
 ---- batch: 050 ----
mean loss: 143.90
 ---- batch: 060 ----
mean loss: 140.58
 ---- batch: 070 ----
mean loss: 145.16
 ---- batch: 080 ----
mean loss: 153.18
 ---- batch: 090 ----
mean loss: 148.80
train mean loss: 144.18
epoch train time: 0:00:01.603029
elapsed time: 0:04:03.648610
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 19:42:19.418579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.36
 ---- batch: 020 ----
mean loss: 143.27
 ---- batch: 030 ----
mean loss: 139.52
 ---- batch: 040 ----
mean loss: 140.91
 ---- batch: 050 ----
mean loss: 146.30
 ---- batch: 060 ----
mean loss: 139.18
 ---- batch: 070 ----
mean loss: 144.82
 ---- batch: 080 ----
mean loss: 138.46
 ---- batch: 090 ----
mean loss: 148.90
train mean loss: 142.22
epoch train time: 0:00:01.615100
elapsed time: 0:04:05.264385
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 19:42:21.034342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.04
 ---- batch: 020 ----
mean loss: 137.38
 ---- batch: 030 ----
mean loss: 147.72
 ---- batch: 040 ----
mean loss: 139.40
 ---- batch: 050 ----
mean loss: 140.42
 ---- batch: 060 ----
mean loss: 134.85
 ---- batch: 070 ----
mean loss: 142.27
 ---- batch: 080 ----
mean loss: 150.97
 ---- batch: 090 ----
mean loss: 146.52
train mean loss: 142.82
epoch train time: 0:00:01.608920
elapsed time: 0:04:06.873949
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 19:42:22.643898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.62
 ---- batch: 020 ----
mean loss: 139.45
 ---- batch: 030 ----
mean loss: 140.36
 ---- batch: 040 ----
mean loss: 139.36
 ---- batch: 050 ----
mean loss: 135.75
 ---- batch: 060 ----
mean loss: 140.94
 ---- batch: 070 ----
mean loss: 141.71
 ---- batch: 080 ----
mean loss: 142.96
 ---- batch: 090 ----
mean loss: 143.65
train mean loss: 140.97
epoch train time: 0:00:01.605204
elapsed time: 0:04:08.479825
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 19:42:24.249809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.74
 ---- batch: 020 ----
mean loss: 130.47
 ---- batch: 030 ----
mean loss: 140.18
 ---- batch: 040 ----
mean loss: 139.58
 ---- batch: 050 ----
mean loss: 137.47
 ---- batch: 060 ----
mean loss: 140.44
 ---- batch: 070 ----
mean loss: 147.60
 ---- batch: 080 ----
mean loss: 147.22
 ---- batch: 090 ----
mean loss: 145.86
train mean loss: 141.24
epoch train time: 0:00:01.569715
elapsed time: 0:04:10.050219
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 19:42:25.820182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.19
 ---- batch: 020 ----
mean loss: 143.28
 ---- batch: 030 ----
mean loss: 138.96
 ---- batch: 040 ----
mean loss: 137.70
 ---- batch: 050 ----
mean loss: 140.23
 ---- batch: 060 ----
mean loss: 141.16
 ---- batch: 070 ----
mean loss: 144.39
 ---- batch: 080 ----
mean loss: 144.91
 ---- batch: 090 ----
mean loss: 135.95
train mean loss: 140.99
epoch train time: 0:00:01.646975
elapsed time: 0:04:11.698037
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 19:42:27.468005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.95
 ---- batch: 020 ----
mean loss: 134.07
 ---- batch: 030 ----
mean loss: 135.77
 ---- batch: 040 ----
mean loss: 137.95
 ---- batch: 050 ----
mean loss: 137.18
 ---- batch: 060 ----
mean loss: 132.88
 ---- batch: 070 ----
mean loss: 144.74
 ---- batch: 080 ----
mean loss: 147.30
 ---- batch: 090 ----
mean loss: 148.11
train mean loss: 140.00
epoch train time: 0:00:01.634893
elapsed time: 0:04:13.333603
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 19:42:29.103564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.28
 ---- batch: 020 ----
mean loss: 133.71
 ---- batch: 030 ----
mean loss: 130.28
 ---- batch: 040 ----
mean loss: 140.26
 ---- batch: 050 ----
mean loss: 141.00
 ---- batch: 060 ----
mean loss: 140.52
 ---- batch: 070 ----
mean loss: 141.99
 ---- batch: 080 ----
mean loss: 138.98
 ---- batch: 090 ----
mean loss: 144.77
train mean loss: 138.87
epoch train time: 0:00:01.615890
elapsed time: 0:04:14.950152
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 19:42:30.720120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.07
 ---- batch: 020 ----
mean loss: 134.53
 ---- batch: 030 ----
mean loss: 131.50
 ---- batch: 040 ----
mean loss: 142.56
 ---- batch: 050 ----
mean loss: 134.92
 ---- batch: 060 ----
mean loss: 143.06
 ---- batch: 070 ----
mean loss: 139.25
 ---- batch: 080 ----
mean loss: 139.07
 ---- batch: 090 ----
mean loss: 142.17
train mean loss: 138.02
epoch train time: 0:00:01.611137
elapsed time: 0:04:16.562022
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 19:42:32.331982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.85
 ---- batch: 020 ----
mean loss: 134.72
 ---- batch: 030 ----
mean loss: 136.06
 ---- batch: 040 ----
mean loss: 139.42
 ---- batch: 050 ----
mean loss: 137.87
 ---- batch: 060 ----
mean loss: 137.23
 ---- batch: 070 ----
mean loss: 136.94
 ---- batch: 080 ----
mean loss: 142.98
 ---- batch: 090 ----
mean loss: 142.06
train mean loss: 137.99
epoch train time: 0:00:01.579046
elapsed time: 0:04:18.141685
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 19:42:33.911643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.67
 ---- batch: 020 ----
mean loss: 132.26
 ---- batch: 030 ----
mean loss: 136.93
 ---- batch: 040 ----
mean loss: 133.42
 ---- batch: 050 ----
mean loss: 136.41
 ---- batch: 060 ----
mean loss: 140.19
 ---- batch: 070 ----
mean loss: 139.44
 ---- batch: 080 ----
mean loss: 145.17
 ---- batch: 090 ----
mean loss: 141.46
train mean loss: 136.92
epoch train time: 0:00:01.578474
elapsed time: 0:04:19.720818
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 19:42:35.490772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.11
 ---- batch: 020 ----
mean loss: 135.07
 ---- batch: 030 ----
mean loss: 136.40
 ---- batch: 040 ----
mean loss: 138.67
 ---- batch: 050 ----
mean loss: 138.81
 ---- batch: 060 ----
mean loss: 134.20
 ---- batch: 070 ----
mean loss: 133.10
 ---- batch: 080 ----
mean loss: 140.34
 ---- batch: 090 ----
mean loss: 142.96
train mean loss: 136.86
epoch train time: 0:00:01.606171
elapsed time: 0:04:21.327627
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 19:42:37.097598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.48
 ---- batch: 020 ----
mean loss: 134.05
 ---- batch: 030 ----
mean loss: 139.38
 ---- batch: 040 ----
mean loss: 129.26
 ---- batch: 050 ----
mean loss: 141.66
 ---- batch: 060 ----
mean loss: 130.80
 ---- batch: 070 ----
mean loss: 142.01
 ---- batch: 080 ----
mean loss: 136.20
 ---- batch: 090 ----
mean loss: 141.25
train mean loss: 136.81
epoch train time: 0:00:01.622530
elapsed time: 0:04:22.950868
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 19:42:38.720848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.74
 ---- batch: 020 ----
mean loss: 130.55
 ---- batch: 030 ----
mean loss: 129.89
 ---- batch: 040 ----
mean loss: 135.15
 ---- batch: 050 ----
mean loss: 142.06
 ---- batch: 060 ----
mean loss: 141.47
 ---- batch: 070 ----
mean loss: 135.04
 ---- batch: 080 ----
mean loss: 135.54
 ---- batch: 090 ----
mean loss: 136.65
train mean loss: 135.40
epoch train time: 0:00:01.642414
elapsed time: 0:04:24.593956
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 19:42:40.363967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.21
 ---- batch: 020 ----
mean loss: 135.12
 ---- batch: 030 ----
mean loss: 128.18
 ---- batch: 040 ----
mean loss: 138.13
 ---- batch: 050 ----
mean loss: 134.32
 ---- batch: 060 ----
mean loss: 131.10
 ---- batch: 070 ----
mean loss: 139.82
 ---- batch: 080 ----
mean loss: 132.37
 ---- batch: 090 ----
mean loss: 131.66
train mean loss: 134.30
epoch train time: 0:00:01.647797
elapsed time: 0:04:26.242519
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 19:42:42.012475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.09
 ---- batch: 020 ----
mean loss: 133.98
 ---- batch: 030 ----
mean loss: 133.07
 ---- batch: 040 ----
mean loss: 140.44
 ---- batch: 050 ----
mean loss: 128.64
 ---- batch: 060 ----
mean loss: 136.51
 ---- batch: 070 ----
mean loss: 136.98
 ---- batch: 080 ----
mean loss: 134.10
 ---- batch: 090 ----
mean loss: 134.19
train mean loss: 134.20
epoch train time: 0:00:01.609350
elapsed time: 0:04:27.852544
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 19:42:43.622502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.07
 ---- batch: 020 ----
mean loss: 130.19
 ---- batch: 030 ----
mean loss: 126.87
 ---- batch: 040 ----
mean loss: 134.57
 ---- batch: 050 ----
mean loss: 141.71
 ---- batch: 060 ----
mean loss: 135.14
 ---- batch: 070 ----
mean loss: 133.00
 ---- batch: 080 ----
mean loss: 141.41
 ---- batch: 090 ----
mean loss: 132.37
train mean loss: 133.90
epoch train time: 0:00:01.636028
elapsed time: 0:04:29.489199
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 19:42:45.259201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.13
 ---- batch: 020 ----
mean loss: 129.49
 ---- batch: 030 ----
mean loss: 132.59
 ---- batch: 040 ----
mean loss: 133.21
 ---- batch: 050 ----
mean loss: 130.44
 ---- batch: 060 ----
mean loss: 136.48
 ---- batch: 070 ----
mean loss: 129.68
 ---- batch: 080 ----
mean loss: 133.32
 ---- batch: 090 ----
mean loss: 135.84
train mean loss: 133.31
epoch train time: 0:00:01.614480
elapsed time: 0:04:31.104464
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 19:42:46.874448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.33
 ---- batch: 020 ----
mean loss: 132.98
 ---- batch: 030 ----
mean loss: 129.14
 ---- batch: 040 ----
mean loss: 134.23
 ---- batch: 050 ----
mean loss: 127.42
 ---- batch: 060 ----
mean loss: 136.73
 ---- batch: 070 ----
mean loss: 132.22
 ---- batch: 080 ----
mean loss: 138.20
 ---- batch: 090 ----
mean loss: 137.22
train mean loss: 132.57
epoch train time: 0:00:01.640707
elapsed time: 0:04:32.745806
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 19:42:48.515754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.29
 ---- batch: 020 ----
mean loss: 128.41
 ---- batch: 030 ----
mean loss: 129.59
 ---- batch: 040 ----
mean loss: 134.49
 ---- batch: 050 ----
mean loss: 131.82
 ---- batch: 060 ----
mean loss: 126.07
 ---- batch: 070 ----
mean loss: 137.95
 ---- batch: 080 ----
mean loss: 134.11
 ---- batch: 090 ----
mean loss: 134.44
train mean loss: 132.18
epoch train time: 0:00:01.622113
elapsed time: 0:04:34.368702
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 19:42:50.138495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.66
 ---- batch: 020 ----
mean loss: 128.72
 ---- batch: 030 ----
mean loss: 127.05
 ---- batch: 040 ----
mean loss: 135.78
 ---- batch: 050 ----
mean loss: 132.31
 ---- batch: 060 ----
mean loss: 138.71
 ---- batch: 070 ----
mean loss: 136.77
 ---- batch: 080 ----
mean loss: 130.37
 ---- batch: 090 ----
mean loss: 132.29
train mean loss: 131.82
epoch train time: 0:00:01.590411
elapsed time: 0:04:35.959636
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 19:42:51.729588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.69
 ---- batch: 020 ----
mean loss: 132.96
 ---- batch: 030 ----
mean loss: 126.55
 ---- batch: 040 ----
mean loss: 135.65
 ---- batch: 050 ----
mean loss: 125.28
 ---- batch: 060 ----
mean loss: 134.64
 ---- batch: 070 ----
mean loss: 127.18
 ---- batch: 080 ----
mean loss: 137.57
 ---- batch: 090 ----
mean loss: 127.66
train mean loss: 131.34
epoch train time: 0:00:01.595208
elapsed time: 0:04:37.555497
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 19:42:53.325441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.13
 ---- batch: 020 ----
mean loss: 129.12
 ---- batch: 030 ----
mean loss: 127.10
 ---- batch: 040 ----
mean loss: 130.06
 ---- batch: 050 ----
mean loss: 131.49
 ---- batch: 060 ----
mean loss: 132.15
 ---- batch: 070 ----
mean loss: 131.38
 ---- batch: 080 ----
mean loss: 129.42
 ---- batch: 090 ----
mean loss: 135.98
train mean loss: 130.95
epoch train time: 0:00:01.620106
elapsed time: 0:04:39.176241
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 19:42:54.946283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.63
 ---- batch: 020 ----
mean loss: 127.99
 ---- batch: 030 ----
mean loss: 128.79
 ---- batch: 040 ----
mean loss: 129.65
 ---- batch: 050 ----
mean loss: 127.08
 ---- batch: 060 ----
mean loss: 128.96
 ---- batch: 070 ----
mean loss: 131.24
 ---- batch: 080 ----
mean loss: 134.11
 ---- batch: 090 ----
mean loss: 137.85
train mean loss: 129.61
epoch train time: 0:00:01.620570
elapsed time: 0:04:40.797529
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 19:42:56.567488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.71
 ---- batch: 020 ----
mean loss: 127.12
 ---- batch: 030 ----
mean loss: 125.17
 ---- batch: 040 ----
mean loss: 129.58
 ---- batch: 050 ----
mean loss: 128.82
 ---- batch: 060 ----
mean loss: 130.29
 ---- batch: 070 ----
mean loss: 130.42
 ---- batch: 080 ----
mean loss: 129.48
 ---- batch: 090 ----
mean loss: 134.96
train mean loss: 129.40
epoch train time: 0:00:01.628879
elapsed time: 0:04:42.427055
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 19:42:58.197026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.82
 ---- batch: 020 ----
mean loss: 126.73
 ---- batch: 030 ----
mean loss: 124.11
 ---- batch: 040 ----
mean loss: 127.64
 ---- batch: 050 ----
mean loss: 133.08
 ---- batch: 060 ----
mean loss: 136.87
 ---- batch: 070 ----
mean loss: 128.98
 ---- batch: 080 ----
mean loss: 128.12
 ---- batch: 090 ----
mean loss: 129.32
train mean loss: 128.76
epoch train time: 0:00:01.625455
elapsed time: 0:04:44.053217
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 19:42:59.823196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.83
 ---- batch: 020 ----
mean loss: 125.63
 ---- batch: 030 ----
mean loss: 124.65
 ---- batch: 040 ----
mean loss: 123.36
 ---- batch: 050 ----
mean loss: 129.80
 ---- batch: 060 ----
mean loss: 133.87
 ---- batch: 070 ----
mean loss: 132.71
 ---- batch: 080 ----
mean loss: 128.94
 ---- batch: 090 ----
mean loss: 127.61
train mean loss: 127.86
epoch train time: 0:00:01.624250
elapsed time: 0:04:45.678146
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 19:43:01.448115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.18
 ---- batch: 020 ----
mean loss: 120.71
 ---- batch: 030 ----
mean loss: 134.65
 ---- batch: 040 ----
mean loss: 125.40
 ---- batch: 050 ----
mean loss: 124.82
 ---- batch: 060 ----
mean loss: 127.81
 ---- batch: 070 ----
mean loss: 133.67
 ---- batch: 080 ----
mean loss: 136.84
 ---- batch: 090 ----
mean loss: 126.09
train mean loss: 128.20
epoch train time: 0:00:01.645244
elapsed time: 0:04:47.324035
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 19:43:03.094005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.49
 ---- batch: 020 ----
mean loss: 127.66
 ---- batch: 030 ----
mean loss: 127.98
 ---- batch: 040 ----
mean loss: 125.98
 ---- batch: 050 ----
mean loss: 127.53
 ---- batch: 060 ----
mean loss: 126.00
 ---- batch: 070 ----
mean loss: 130.41
 ---- batch: 080 ----
mean loss: 127.07
 ---- batch: 090 ----
mean loss: 129.11
train mean loss: 127.65
epoch train time: 0:00:01.616508
elapsed time: 0:04:48.941252
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 19:43:04.711341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.37
 ---- batch: 020 ----
mean loss: 122.00
 ---- batch: 030 ----
mean loss: 128.35
 ---- batch: 040 ----
mean loss: 127.20
 ---- batch: 050 ----
mean loss: 126.44
 ---- batch: 060 ----
mean loss: 125.04
 ---- batch: 070 ----
mean loss: 128.05
 ---- batch: 080 ----
mean loss: 130.25
 ---- batch: 090 ----
mean loss: 127.84
train mean loss: 127.06
epoch train time: 0:00:01.619093
elapsed time: 0:04:50.561105
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 19:43:06.331056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.41
 ---- batch: 020 ----
mean loss: 113.93
 ---- batch: 030 ----
mean loss: 121.04
 ---- batch: 040 ----
mean loss: 131.05
 ---- batch: 050 ----
mean loss: 125.05
 ---- batch: 060 ----
mean loss: 129.16
 ---- batch: 070 ----
mean loss: 130.59
 ---- batch: 080 ----
mean loss: 128.01
 ---- batch: 090 ----
mean loss: 131.50
train mean loss: 125.76
epoch train time: 0:00:01.616109
elapsed time: 0:04:52.177863
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 19:43:07.947823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.51
 ---- batch: 020 ----
mean loss: 118.09
 ---- batch: 030 ----
mean loss: 122.80
 ---- batch: 040 ----
mean loss: 123.65
 ---- batch: 050 ----
mean loss: 122.01
 ---- batch: 060 ----
mean loss: 126.64
 ---- batch: 070 ----
mean loss: 134.71
 ---- batch: 080 ----
mean loss: 132.84
 ---- batch: 090 ----
mean loss: 127.75
train mean loss: 125.65
epoch train time: 0:00:01.632602
elapsed time: 0:04:53.811071
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 19:43:09.581034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.39
 ---- batch: 020 ----
mean loss: 125.23
 ---- batch: 030 ----
mean loss: 123.98
 ---- batch: 040 ----
mean loss: 122.86
 ---- batch: 050 ----
mean loss: 118.20
 ---- batch: 060 ----
mean loss: 125.72
 ---- batch: 070 ----
mean loss: 130.64
 ---- batch: 080 ----
mean loss: 120.79
 ---- batch: 090 ----
mean loss: 129.64
train mean loss: 124.98
epoch train time: 0:00:01.643229
elapsed time: 0:04:55.454928
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 19:43:11.224948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.92
 ---- batch: 020 ----
mean loss: 124.51
 ---- batch: 030 ----
mean loss: 126.16
 ---- batch: 040 ----
mean loss: 128.24
 ---- batch: 050 ----
mean loss: 123.39
 ---- batch: 060 ----
mean loss: 127.71
 ---- batch: 070 ----
mean loss: 128.75
 ---- batch: 080 ----
mean loss: 126.80
 ---- batch: 090 ----
mean loss: 124.35
train mean loss: 124.65
epoch train time: 0:00:01.598412
elapsed time: 0:04:57.054096
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 19:43:12.824081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.61
 ---- batch: 020 ----
mean loss: 120.88
 ---- batch: 030 ----
mean loss: 115.72
 ---- batch: 040 ----
mean loss: 126.13
 ---- batch: 050 ----
mean loss: 123.87
 ---- batch: 060 ----
mean loss: 129.99
 ---- batch: 070 ----
mean loss: 129.73
 ---- batch: 080 ----
mean loss: 124.43
 ---- batch: 090 ----
mean loss: 126.46
train mean loss: 124.48
epoch train time: 0:00:01.582021
elapsed time: 0:04:58.636791
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 19:43:14.406790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.51
 ---- batch: 020 ----
mean loss: 124.50
 ---- batch: 030 ----
mean loss: 118.50
 ---- batch: 040 ----
mean loss: 116.13
 ---- batch: 050 ----
mean loss: 126.69
 ---- batch: 060 ----
mean loss: 125.65
 ---- batch: 070 ----
mean loss: 127.89
 ---- batch: 080 ----
mean loss: 124.89
 ---- batch: 090 ----
mean loss: 123.95
train mean loss: 123.97
epoch train time: 0:00:01.641316
elapsed time: 0:05:00.278770
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 19:43:16.048720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.04
 ---- batch: 020 ----
mean loss: 122.46
 ---- batch: 030 ----
mean loss: 119.78
 ---- batch: 040 ----
mean loss: 128.61
 ---- batch: 050 ----
mean loss: 122.19
 ---- batch: 060 ----
mean loss: 126.34
 ---- batch: 070 ----
mean loss: 125.13
 ---- batch: 080 ----
mean loss: 122.62
 ---- batch: 090 ----
mean loss: 124.01
train mean loss: 123.54
epoch train time: 0:00:01.637206
elapsed time: 0:05:01.916703
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 19:43:17.686727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.12
 ---- batch: 020 ----
mean loss: 126.75
 ---- batch: 030 ----
mean loss: 124.72
 ---- batch: 040 ----
mean loss: 120.51
 ---- batch: 050 ----
mean loss: 121.16
 ---- batch: 060 ----
mean loss: 123.07
 ---- batch: 070 ----
mean loss: 120.30
 ---- batch: 080 ----
mean loss: 125.53
 ---- batch: 090 ----
mean loss: 122.93
train mean loss: 123.46
epoch train time: 0:00:01.691964
elapsed time: 0:05:03.609397
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 19:43:19.379352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.24
 ---- batch: 020 ----
mean loss: 118.53
 ---- batch: 030 ----
mean loss: 127.30
 ---- batch: 040 ----
mean loss: 117.29
 ---- batch: 050 ----
mean loss: 124.90
 ---- batch: 060 ----
mean loss: 116.48
 ---- batch: 070 ----
mean loss: 125.31
 ---- batch: 080 ----
mean loss: 126.86
 ---- batch: 090 ----
mean loss: 126.03
train mean loss: 123.22
epoch train time: 0:00:01.650944
elapsed time: 0:05:05.261079
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 19:43:21.031025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.98
 ---- batch: 020 ----
mean loss: 124.20
 ---- batch: 030 ----
mean loss: 118.97
 ---- batch: 040 ----
mean loss: 118.37
 ---- batch: 050 ----
mean loss: 123.86
 ---- batch: 060 ----
mean loss: 129.01
 ---- batch: 070 ----
mean loss: 123.63
 ---- batch: 080 ----
mean loss: 123.30
 ---- batch: 090 ----
mean loss: 122.18
train mean loss: 122.58
epoch train time: 0:00:01.642431
elapsed time: 0:05:06.904150
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 19:43:22.674111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.88
 ---- batch: 020 ----
mean loss: 118.35
 ---- batch: 030 ----
mean loss: 115.24
 ---- batch: 040 ----
mean loss: 124.36
 ---- batch: 050 ----
mean loss: 116.87
 ---- batch: 060 ----
mean loss: 125.06
 ---- batch: 070 ----
mean loss: 122.71
 ---- batch: 080 ----
mean loss: 124.04
 ---- batch: 090 ----
mean loss: 124.05
train mean loss: 121.29
epoch train time: 0:00:01.644182
elapsed time: 0:05:08.548926
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 19:43:24.318920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.62
 ---- batch: 020 ----
mean loss: 120.00
 ---- batch: 030 ----
mean loss: 117.15
 ---- batch: 040 ----
mean loss: 121.10
 ---- batch: 050 ----
mean loss: 127.92
 ---- batch: 060 ----
mean loss: 117.76
 ---- batch: 070 ----
mean loss: 122.81
 ---- batch: 080 ----
mean loss: 124.30
 ---- batch: 090 ----
mean loss: 122.56
train mean loss: 121.18
epoch train time: 0:00:01.625387
elapsed time: 0:05:10.175021
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 19:43:25.944985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.18
 ---- batch: 020 ----
mean loss: 116.84
 ---- batch: 030 ----
mean loss: 121.15
 ---- batch: 040 ----
mean loss: 117.06
 ---- batch: 050 ----
mean loss: 120.39
 ---- batch: 060 ----
mean loss: 122.10
 ---- batch: 070 ----
mean loss: 119.41
 ---- batch: 080 ----
mean loss: 118.20
 ---- batch: 090 ----
mean loss: 127.81
train mean loss: 120.25
epoch train time: 0:00:01.658424
elapsed time: 0:05:11.834147
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 19:43:27.604137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.83
 ---- batch: 020 ----
mean loss: 120.96
 ---- batch: 030 ----
mean loss: 124.42
 ---- batch: 040 ----
mean loss: 118.46
 ---- batch: 050 ----
mean loss: 118.53
 ---- batch: 060 ----
mean loss: 118.09
 ---- batch: 070 ----
mean loss: 117.95
 ---- batch: 080 ----
mean loss: 123.19
 ---- batch: 090 ----
mean loss: 123.30
train mean loss: 120.30
epoch train time: 0:00:01.612293
elapsed time: 0:05:13.447108
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 19:43:29.217075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.78
 ---- batch: 020 ----
mean loss: 113.61
 ---- batch: 030 ----
mean loss: 118.63
 ---- batch: 040 ----
mean loss: 120.28
 ---- batch: 050 ----
mean loss: 118.98
 ---- batch: 060 ----
mean loss: 124.73
 ---- batch: 070 ----
mean loss: 126.28
 ---- batch: 080 ----
mean loss: 118.49
 ---- batch: 090 ----
mean loss: 123.70
train mean loss: 119.46
epoch train time: 0:00:01.622378
elapsed time: 0:05:15.070441
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 19:43:30.840157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.67
 ---- batch: 020 ----
mean loss: 117.23
 ---- batch: 030 ----
mean loss: 126.64
 ---- batch: 040 ----
mean loss: 116.59
 ---- batch: 050 ----
mean loss: 115.95
 ---- batch: 060 ----
mean loss: 118.61
 ---- batch: 070 ----
mean loss: 116.87
 ---- batch: 080 ----
mean loss: 119.08
 ---- batch: 090 ----
mean loss: 121.49
train mean loss: 119.20
epoch train time: 0:00:01.616417
elapsed time: 0:05:16.687235
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 19:43:32.457220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.04
 ---- batch: 020 ----
mean loss: 114.13
 ---- batch: 030 ----
mean loss: 117.43
 ---- batch: 040 ----
mean loss: 118.96
 ---- batch: 050 ----
mean loss: 120.05
 ---- batch: 060 ----
mean loss: 119.17
 ---- batch: 070 ----
mean loss: 121.73
 ---- batch: 080 ----
mean loss: 123.27
 ---- batch: 090 ----
mean loss: 119.08
train mean loss: 118.89
epoch train time: 0:00:01.653191
elapsed time: 0:05:18.341057
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 19:43:34.110998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.37
 ---- batch: 020 ----
mean loss: 117.72
 ---- batch: 030 ----
mean loss: 113.90
 ---- batch: 040 ----
mean loss: 119.59
 ---- batch: 050 ----
mean loss: 125.50
 ---- batch: 060 ----
mean loss: 123.87
 ---- batch: 070 ----
mean loss: 118.26
 ---- batch: 080 ----
mean loss: 117.01
 ---- batch: 090 ----
mean loss: 127.36
train mean loss: 119.80
epoch train time: 0:00:01.629761
elapsed time: 0:05:19.971419
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 19:43:35.741413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.81
 ---- batch: 020 ----
mean loss: 112.98
 ---- batch: 030 ----
mean loss: 114.48
 ---- batch: 040 ----
mean loss: 119.12
 ---- batch: 050 ----
mean loss: 114.86
 ---- batch: 060 ----
mean loss: 122.47
 ---- batch: 070 ----
mean loss: 122.15
 ---- batch: 080 ----
mean loss: 118.24
 ---- batch: 090 ----
mean loss: 115.65
train mean loss: 117.73
epoch train time: 0:00:01.616661
elapsed time: 0:05:21.588740
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 19:43:37.358698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.93
 ---- batch: 020 ----
mean loss: 119.69
 ---- batch: 030 ----
mean loss: 113.42
 ---- batch: 040 ----
mean loss: 114.77
 ---- batch: 050 ----
mean loss: 118.79
 ---- batch: 060 ----
mean loss: 119.30
 ---- batch: 070 ----
mean loss: 115.84
 ---- batch: 080 ----
mean loss: 115.67
 ---- batch: 090 ----
mean loss: 122.08
train mean loss: 116.89
epoch train time: 0:00:01.652479
elapsed time: 0:05:23.241909
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 19:43:39.011919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.39
 ---- batch: 020 ----
mean loss: 115.16
 ---- batch: 030 ----
mean loss: 118.61
 ---- batch: 040 ----
mean loss: 111.83
 ---- batch: 050 ----
mean loss: 114.73
 ---- batch: 060 ----
mean loss: 112.91
 ---- batch: 070 ----
mean loss: 119.66
 ---- batch: 080 ----
mean loss: 118.25
 ---- batch: 090 ----
mean loss: 121.49
train mean loss: 116.40
epoch train time: 0:00:01.649834
elapsed time: 0:05:24.892489
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 19:43:40.662443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.88
 ---- batch: 020 ----
mean loss: 117.10
 ---- batch: 030 ----
mean loss: 117.12
 ---- batch: 040 ----
mean loss: 115.03
 ---- batch: 050 ----
mean loss: 115.26
 ---- batch: 060 ----
mean loss: 110.46
 ---- batch: 070 ----
mean loss: 116.68
 ---- batch: 080 ----
mean loss: 120.78
 ---- batch: 090 ----
mean loss: 124.66
train mean loss: 116.79
epoch train time: 0:00:01.620908
elapsed time: 0:05:26.514070
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 19:43:42.284075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.58
 ---- batch: 020 ----
mean loss: 115.49
 ---- batch: 030 ----
mean loss: 113.46
 ---- batch: 040 ----
mean loss: 118.90
 ---- batch: 050 ----
mean loss: 111.39
 ---- batch: 060 ----
mean loss: 118.81
 ---- batch: 070 ----
mean loss: 115.81
 ---- batch: 080 ----
mean loss: 123.56
 ---- batch: 090 ----
mean loss: 118.92
train mean loss: 116.53
epoch train time: 0:00:01.625761
elapsed time: 0:05:28.140591
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 19:43:43.910561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.29
 ---- batch: 020 ----
mean loss: 113.22
 ---- batch: 030 ----
mean loss: 114.43
 ---- batch: 040 ----
mean loss: 113.68
 ---- batch: 050 ----
mean loss: 118.69
 ---- batch: 060 ----
mean loss: 118.61
 ---- batch: 070 ----
mean loss: 112.52
 ---- batch: 080 ----
mean loss: 114.89
 ---- batch: 090 ----
mean loss: 116.90
train mean loss: 115.54
epoch train time: 0:00:01.622418
elapsed time: 0:05:29.763680
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 19:43:45.533660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.65
 ---- batch: 020 ----
mean loss: 114.38
 ---- batch: 030 ----
mean loss: 112.30
 ---- batch: 040 ----
mean loss: 118.49
 ---- batch: 050 ----
mean loss: 114.76
 ---- batch: 060 ----
mean loss: 114.80
 ---- batch: 070 ----
mean loss: 115.43
 ---- batch: 080 ----
mean loss: 113.01
 ---- batch: 090 ----
mean loss: 113.81
train mean loss: 115.38
epoch train time: 0:00:01.626630
elapsed time: 0:05:31.390951
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 19:43:47.160924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.09
 ---- batch: 020 ----
mean loss: 117.74
 ---- batch: 030 ----
mean loss: 109.55
 ---- batch: 040 ----
mean loss: 119.95
 ---- batch: 050 ----
mean loss: 116.76
 ---- batch: 060 ----
mean loss: 118.07
 ---- batch: 070 ----
mean loss: 111.86
 ---- batch: 080 ----
mean loss: 114.52
 ---- batch: 090 ----
mean loss: 116.65
train mean loss: 115.21
epoch train time: 0:00:01.678451
elapsed time: 0:05:33.070069
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 19:43:48.840028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.42
 ---- batch: 020 ----
mean loss: 112.49
 ---- batch: 030 ----
mean loss: 112.42
 ---- batch: 040 ----
mean loss: 112.33
 ---- batch: 050 ----
mean loss: 117.23
 ---- batch: 060 ----
mean loss: 114.65
 ---- batch: 070 ----
mean loss: 117.45
 ---- batch: 080 ----
mean loss: 115.67
 ---- batch: 090 ----
mean loss: 115.61
train mean loss: 114.58
epoch train time: 0:00:01.635689
elapsed time: 0:05:34.706749
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 19:43:50.476729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.53
 ---- batch: 020 ----
mean loss: 111.00
 ---- batch: 030 ----
mean loss: 113.60
 ---- batch: 040 ----
mean loss: 117.42
 ---- batch: 050 ----
mean loss: 113.33
 ---- batch: 060 ----
mean loss: 115.67
 ---- batch: 070 ----
mean loss: 121.52
 ---- batch: 080 ----
mean loss: 113.94
 ---- batch: 090 ----
mean loss: 120.35
train mean loss: 115.07
epoch train time: 0:00:01.663124
elapsed time: 0:05:36.370656
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 19:43:52.140658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.05
 ---- batch: 020 ----
mean loss: 111.78
 ---- batch: 030 ----
mean loss: 113.79
 ---- batch: 040 ----
mean loss: 110.53
 ---- batch: 050 ----
mean loss: 115.78
 ---- batch: 060 ----
mean loss: 118.29
 ---- batch: 070 ----
mean loss: 114.22
 ---- batch: 080 ----
mean loss: 115.43
 ---- batch: 090 ----
mean loss: 116.22
train mean loss: 114.36
epoch train time: 0:00:01.637748
elapsed time: 0:05:38.009071
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 19:43:53.779075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.27
 ---- batch: 020 ----
mean loss: 108.50
 ---- batch: 030 ----
mean loss: 111.95
 ---- batch: 040 ----
mean loss: 113.55
 ---- batch: 050 ----
mean loss: 107.66
 ---- batch: 060 ----
mean loss: 115.77
 ---- batch: 070 ----
mean loss: 112.21
 ---- batch: 080 ----
mean loss: 120.46
 ---- batch: 090 ----
mean loss: 118.09
train mean loss: 113.19
epoch train time: 0:00:01.644558
elapsed time: 0:05:39.654484
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 19:43:55.424474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.29
 ---- batch: 020 ----
mean loss: 109.64
 ---- batch: 030 ----
mean loss: 112.48
 ---- batch: 040 ----
mean loss: 113.12
 ---- batch: 050 ----
mean loss: 110.57
 ---- batch: 060 ----
mean loss: 112.97
 ---- batch: 070 ----
mean loss: 113.15
 ---- batch: 080 ----
mean loss: 119.38
 ---- batch: 090 ----
mean loss: 115.21
train mean loss: 113.00
epoch train time: 0:00:01.621623
elapsed time: 0:05:41.276818
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 19:43:57.046803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.48
 ---- batch: 020 ----
mean loss: 109.07
 ---- batch: 030 ----
mean loss: 117.05
 ---- batch: 040 ----
mean loss: 115.13
 ---- batch: 050 ----
mean loss: 112.65
 ---- batch: 060 ----
mean loss: 112.69
 ---- batch: 070 ----
mean loss: 115.50
 ---- batch: 080 ----
mean loss: 116.50
 ---- batch: 090 ----
mean loss: 110.31
train mean loss: 112.86
epoch train time: 0:00:01.630283
elapsed time: 0:05:42.907776
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 19:43:58.677764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.98
 ---- batch: 020 ----
mean loss: 105.58
 ---- batch: 030 ----
mean loss: 111.87
 ---- batch: 040 ----
mean loss: 107.73
 ---- batch: 050 ----
mean loss: 112.51
 ---- batch: 060 ----
mean loss: 113.58
 ---- batch: 070 ----
mean loss: 120.54
 ---- batch: 080 ----
mean loss: 116.06
 ---- batch: 090 ----
mean loss: 112.47
train mean loss: 112.12
epoch train time: 0:00:01.610589
elapsed time: 0:05:44.519045
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 19:44:00.289001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.03
 ---- batch: 020 ----
mean loss: 108.05
 ---- batch: 030 ----
mean loss: 109.18
 ---- batch: 040 ----
mean loss: 113.12
 ---- batch: 050 ----
mean loss: 112.42
 ---- batch: 060 ----
mean loss: 113.01
 ---- batch: 070 ----
mean loss: 109.50
 ---- batch: 080 ----
mean loss: 109.04
 ---- batch: 090 ----
mean loss: 117.37
train mean loss: 111.47
epoch train time: 0:00:01.637196
elapsed time: 0:05:46.156895
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 19:44:01.926856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.29
 ---- batch: 020 ----
mean loss: 108.81
 ---- batch: 030 ----
mean loss: 109.30
 ---- batch: 040 ----
mean loss: 109.58
 ---- batch: 050 ----
mean loss: 112.33
 ---- batch: 060 ----
mean loss: 106.01
 ---- batch: 070 ----
mean loss: 115.48
 ---- batch: 080 ----
mean loss: 116.78
 ---- batch: 090 ----
mean loss: 119.23
train mean loss: 111.68
epoch train time: 0:00:01.612922
elapsed time: 0:05:47.770472
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 19:44:03.540416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.80
 ---- batch: 020 ----
mean loss: 107.19
 ---- batch: 030 ----
mean loss: 108.20
 ---- batch: 040 ----
mean loss: 112.10
 ---- batch: 050 ----
mean loss: 106.44
 ---- batch: 060 ----
mean loss: 109.72
 ---- batch: 070 ----
mean loss: 107.71
 ---- batch: 080 ----
mean loss: 117.10
 ---- batch: 090 ----
mean loss: 118.62
train mean loss: 111.40
epoch train time: 0:00:01.652769
elapsed time: 0:05:49.423832
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 19:44:05.193838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.41
 ---- batch: 020 ----
mean loss: 114.56
 ---- batch: 030 ----
mean loss: 112.55
 ---- batch: 040 ----
mean loss: 105.53
 ---- batch: 050 ----
mean loss: 107.59
 ---- batch: 060 ----
mean loss: 110.39
 ---- batch: 070 ----
mean loss: 111.57
 ---- batch: 080 ----
mean loss: 119.03
 ---- batch: 090 ----
mean loss: 114.75
train mean loss: 111.52
epoch train time: 0:00:01.626981
elapsed time: 0:05:51.051567
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 19:44:06.821523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.81
 ---- batch: 020 ----
mean loss: 113.45
 ---- batch: 030 ----
mean loss: 109.24
 ---- batch: 040 ----
mean loss: 109.23
 ---- batch: 050 ----
mean loss: 103.84
 ---- batch: 060 ----
mean loss: 111.01
 ---- batch: 070 ----
mean loss: 113.41
 ---- batch: 080 ----
mean loss: 106.47
 ---- batch: 090 ----
mean loss: 117.02
train mean loss: 110.86
epoch train time: 0:00:01.619800
elapsed time: 0:05:52.672033
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 19:44:08.441998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.88
 ---- batch: 020 ----
mean loss: 107.69
 ---- batch: 030 ----
mean loss: 108.76
 ---- batch: 040 ----
mean loss: 109.40
 ---- batch: 050 ----
mean loss: 110.14
 ---- batch: 060 ----
mean loss: 112.92
 ---- batch: 070 ----
mean loss: 110.14
 ---- batch: 080 ----
mean loss: 110.93
 ---- batch: 090 ----
mean loss: 116.31
train mean loss: 110.57
epoch train time: 0:00:01.634021
elapsed time: 0:05:54.306703
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 19:44:10.076681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.99
 ---- batch: 020 ----
mean loss: 105.82
 ---- batch: 030 ----
mean loss: 105.11
 ---- batch: 040 ----
mean loss: 110.06
 ---- batch: 050 ----
mean loss: 110.94
 ---- batch: 060 ----
mean loss: 112.16
 ---- batch: 070 ----
mean loss: 108.65
 ---- batch: 080 ----
mean loss: 114.95
 ---- batch: 090 ----
mean loss: 114.98
train mean loss: 109.88
epoch train time: 0:00:01.636512
elapsed time: 0:05:55.943881
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 19:44:11.713872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.78
 ---- batch: 020 ----
mean loss: 109.15
 ---- batch: 030 ----
mean loss: 107.45
 ---- batch: 040 ----
mean loss: 108.95
 ---- batch: 050 ----
mean loss: 109.90
 ---- batch: 060 ----
mean loss: 113.50
 ---- batch: 070 ----
mean loss: 110.89
 ---- batch: 080 ----
mean loss: 110.47
 ---- batch: 090 ----
mean loss: 112.95
train mean loss: 109.78
epoch train time: 0:00:01.653237
elapsed time: 0:05:57.597758
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 19:44:13.367713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.36
 ---- batch: 020 ----
mean loss: 112.35
 ---- batch: 030 ----
mean loss: 103.50
 ---- batch: 040 ----
mean loss: 105.69
 ---- batch: 050 ----
mean loss: 106.87
 ---- batch: 060 ----
mean loss: 112.95
 ---- batch: 070 ----
mean loss: 109.58
 ---- batch: 080 ----
mean loss: 118.49
 ---- batch: 090 ----
mean loss: 110.75
train mean loss: 109.34
epoch train time: 0:00:01.601528
elapsed time: 0:05:59.199995
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 19:44:14.969971
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.88
 ---- batch: 020 ----
mean loss: 105.55
 ---- batch: 030 ----
mean loss: 97.80
 ---- batch: 040 ----
mean loss: 108.72
 ---- batch: 050 ----
mean loss: 104.65
 ---- batch: 060 ----
mean loss: 101.50
 ---- batch: 070 ----
mean loss: 99.37
 ---- batch: 080 ----
mean loss: 104.41
 ---- batch: 090 ----
mean loss: 105.83
train mean loss: 104.25
epoch train time: 0:00:01.620868
elapsed time: 0:06:00.821835
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 19:44:16.591553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.48
 ---- batch: 020 ----
mean loss: 99.14
 ---- batch: 030 ----
mean loss: 105.48
 ---- batch: 040 ----
mean loss: 100.79
 ---- batch: 050 ----
mean loss: 103.49
 ---- batch: 060 ----
mean loss: 106.20
 ---- batch: 070 ----
mean loss: 104.23
 ---- batch: 080 ----
mean loss: 101.21
 ---- batch: 090 ----
mean loss: 98.09
train mean loss: 103.13
epoch train time: 0:00:01.608417
elapsed time: 0:06:02.430658
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 19:44:18.200615
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.11
 ---- batch: 020 ----
mean loss: 103.62
 ---- batch: 030 ----
mean loss: 98.37
 ---- batch: 040 ----
mean loss: 102.51
 ---- batch: 050 ----
mean loss: 102.93
 ---- batch: 060 ----
mean loss: 102.04
 ---- batch: 070 ----
mean loss: 104.61
 ---- batch: 080 ----
mean loss: 106.35
 ---- batch: 090 ----
mean loss: 101.95
train mean loss: 102.73
epoch train time: 0:00:01.617847
elapsed time: 0:06:04.049148
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 19:44:19.819135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.59
 ---- batch: 020 ----
mean loss: 101.50
 ---- batch: 030 ----
mean loss: 104.79
 ---- batch: 040 ----
mean loss: 102.27
 ---- batch: 050 ----
mean loss: 101.45
 ---- batch: 060 ----
mean loss: 101.78
 ---- batch: 070 ----
mean loss: 100.39
 ---- batch: 080 ----
mean loss: 109.33
 ---- batch: 090 ----
mean loss: 100.47
train mean loss: 102.63
epoch train time: 0:00:01.628145
elapsed time: 0:06:05.677991
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 19:44:21.447925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.31
 ---- batch: 020 ----
mean loss: 101.68
 ---- batch: 030 ----
mean loss: 105.39
 ---- batch: 040 ----
mean loss: 96.64
 ---- batch: 050 ----
mean loss: 101.31
 ---- batch: 060 ----
mean loss: 106.71
 ---- batch: 070 ----
mean loss: 98.81
 ---- batch: 080 ----
mean loss: 108.24
 ---- batch: 090 ----
mean loss: 103.49
train mean loss: 102.76
epoch train time: 0:00:01.647371
elapsed time: 0:06:07.325975
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 19:44:23.095969
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.63
 ---- batch: 020 ----
mean loss: 101.08
 ---- batch: 030 ----
mean loss: 104.20
 ---- batch: 040 ----
mean loss: 98.90
 ---- batch: 050 ----
mean loss: 103.24
 ---- batch: 060 ----
mean loss: 101.01
 ---- batch: 070 ----
mean loss: 103.56
 ---- batch: 080 ----
mean loss: 104.82
 ---- batch: 090 ----
mean loss: 106.05
train mean loss: 102.41
epoch train time: 0:00:01.608227
elapsed time: 0:06:08.934935
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 19:44:24.704902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.72
 ---- batch: 020 ----
mean loss: 101.16
 ---- batch: 030 ----
mean loss: 103.90
 ---- batch: 040 ----
mean loss: 104.84
 ---- batch: 050 ----
mean loss: 100.79
 ---- batch: 060 ----
mean loss: 99.68
 ---- batch: 070 ----
mean loss: 100.57
 ---- batch: 080 ----
mean loss: 102.88
 ---- batch: 090 ----
mean loss: 100.75
train mean loss: 102.47
epoch train time: 0:00:01.670983
elapsed time: 0:06:10.606631
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 19:44:26.376604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.21
 ---- batch: 020 ----
mean loss: 107.09
 ---- batch: 030 ----
mean loss: 104.95
 ---- batch: 040 ----
mean loss: 101.91
 ---- batch: 050 ----
mean loss: 105.31
 ---- batch: 060 ----
mean loss: 98.42
 ---- batch: 070 ----
mean loss: 100.56
 ---- batch: 080 ----
mean loss: 102.30
 ---- batch: 090 ----
mean loss: 97.13
train mean loss: 102.31
epoch train time: 0:00:01.623661
elapsed time: 0:06:12.231009
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 19:44:28.000972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.63
 ---- batch: 020 ----
mean loss: 105.53
 ---- batch: 030 ----
mean loss: 99.34
 ---- batch: 040 ----
mean loss: 99.64
 ---- batch: 050 ----
mean loss: 103.09
 ---- batch: 060 ----
mean loss: 103.71
 ---- batch: 070 ----
mean loss: 102.37
 ---- batch: 080 ----
mean loss: 100.90
 ---- batch: 090 ----
mean loss: 101.83
train mean loss: 102.55
epoch train time: 0:00:01.621611
elapsed time: 0:06:13.853341
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 19:44:29.623324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.64
 ---- batch: 020 ----
mean loss: 101.00
 ---- batch: 030 ----
mean loss: 105.58
 ---- batch: 040 ----
mean loss: 103.46
 ---- batch: 050 ----
mean loss: 103.02
 ---- batch: 060 ----
mean loss: 102.74
 ---- batch: 070 ----
mean loss: 102.96
 ---- batch: 080 ----
mean loss: 104.49
 ---- batch: 090 ----
mean loss: 101.42
train mean loss: 102.33
epoch train time: 0:00:01.628187
elapsed time: 0:06:15.482284
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 19:44:31.252338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.81
 ---- batch: 020 ----
mean loss: 94.82
 ---- batch: 030 ----
mean loss: 100.79
 ---- batch: 040 ----
mean loss: 103.54
 ---- batch: 050 ----
mean loss: 100.63
 ---- batch: 060 ----
mean loss: 101.35
 ---- batch: 070 ----
mean loss: 106.28
 ---- batch: 080 ----
mean loss: 101.09
 ---- batch: 090 ----
mean loss: 102.40
train mean loss: 102.07
epoch train time: 0:00:01.662885
elapsed time: 0:06:17.145908
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 19:44:32.915851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.16
 ---- batch: 020 ----
mean loss: 98.64
 ---- batch: 030 ----
mean loss: 101.91
 ---- batch: 040 ----
mean loss: 108.06
 ---- batch: 050 ----
mean loss: 102.47
 ---- batch: 060 ----
mean loss: 99.21
 ---- batch: 070 ----
mean loss: 103.77
 ---- batch: 080 ----
mean loss: 100.92
 ---- batch: 090 ----
mean loss: 101.63
train mean loss: 102.47
epoch train time: 0:00:01.625457
elapsed time: 0:06:18.772017
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 19:44:34.541970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.45
 ---- batch: 020 ----
mean loss: 102.98
 ---- batch: 030 ----
mean loss: 97.68
 ---- batch: 040 ----
mean loss: 104.33
 ---- batch: 050 ----
mean loss: 106.55
 ---- batch: 060 ----
mean loss: 99.71
 ---- batch: 070 ----
mean loss: 102.24
 ---- batch: 080 ----
mean loss: 101.42
 ---- batch: 090 ----
mean loss: 101.70
train mean loss: 102.22
epoch train time: 0:00:01.649107
elapsed time: 0:06:20.422017
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 19:44:36.192000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.50
 ---- batch: 020 ----
mean loss: 92.44
 ---- batch: 030 ----
mean loss: 98.34
 ---- batch: 040 ----
mean loss: 100.04
 ---- batch: 050 ----
mean loss: 101.74
 ---- batch: 060 ----
mean loss: 103.07
 ---- batch: 070 ----
mean loss: 104.79
 ---- batch: 080 ----
mean loss: 108.67
 ---- batch: 090 ----
mean loss: 107.07
train mean loss: 102.16
epoch train time: 0:00:01.646055
elapsed time: 0:06:22.068758
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 19:44:37.838725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.44
 ---- batch: 020 ----
mean loss: 104.66
 ---- batch: 030 ----
mean loss: 101.06
 ---- batch: 040 ----
mean loss: 99.21
 ---- batch: 050 ----
mean loss: 97.84
 ---- batch: 060 ----
mean loss: 102.69
 ---- batch: 070 ----
mean loss: 101.86
 ---- batch: 080 ----
mean loss: 104.57
 ---- batch: 090 ----
mean loss: 101.94
train mean loss: 102.11
epoch train time: 0:00:01.651188
elapsed time: 0:06:23.720608
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 19:44:39.490629
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.00
 ---- batch: 020 ----
mean loss: 98.61
 ---- batch: 030 ----
mean loss: 104.20
 ---- batch: 040 ----
mean loss: 100.94
 ---- batch: 050 ----
mean loss: 100.27
 ---- batch: 060 ----
mean loss: 100.31
 ---- batch: 070 ----
mean loss: 104.24
 ---- batch: 080 ----
mean loss: 105.70
 ---- batch: 090 ----
mean loss: 97.71
train mean loss: 102.06
epoch train time: 0:00:01.641418
elapsed time: 0:06:25.362700
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 19:44:41.132680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.93
 ---- batch: 020 ----
mean loss: 104.06
 ---- batch: 030 ----
mean loss: 104.19
 ---- batch: 040 ----
mean loss: 101.98
 ---- batch: 050 ----
mean loss: 101.36
 ---- batch: 060 ----
mean loss: 96.84
 ---- batch: 070 ----
mean loss: 101.06
 ---- batch: 080 ----
mean loss: 103.80
 ---- batch: 090 ----
mean loss: 105.35
train mean loss: 102.14
epoch train time: 0:00:01.622056
elapsed time: 0:06:26.985510
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 19:44:42.755425
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.19
 ---- batch: 020 ----
mean loss: 101.60
 ---- batch: 030 ----
mean loss: 101.95
 ---- batch: 040 ----
mean loss: 100.66
 ---- batch: 050 ----
mean loss: 99.33
 ---- batch: 060 ----
mean loss: 106.13
 ---- batch: 070 ----
mean loss: 97.42
 ---- batch: 080 ----
mean loss: 102.61
 ---- batch: 090 ----
mean loss: 110.97
train mean loss: 101.99
epoch train time: 0:00:01.584824
elapsed time: 0:06:28.570900
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 19:44:44.340841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.63
 ---- batch: 020 ----
mean loss: 102.27
 ---- batch: 030 ----
mean loss: 105.38
 ---- batch: 040 ----
mean loss: 105.37
 ---- batch: 050 ----
mean loss: 104.48
 ---- batch: 060 ----
mean loss: 95.76
 ---- batch: 070 ----
mean loss: 103.09
 ---- batch: 080 ----
mean loss: 102.12
 ---- batch: 090 ----
mean loss: 98.62
train mean loss: 101.89
epoch train time: 0:00:01.616617
elapsed time: 0:06:30.188110
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 19:44:45.958074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.44
 ---- batch: 020 ----
mean loss: 103.34
 ---- batch: 030 ----
mean loss: 101.58
 ---- batch: 040 ----
mean loss: 99.51
 ---- batch: 050 ----
mean loss: 99.61
 ---- batch: 060 ----
mean loss: 107.63
 ---- batch: 070 ----
mean loss: 99.58
 ---- batch: 080 ----
mean loss: 107.12
 ---- batch: 090 ----
mean loss: 98.54
train mean loss: 101.71
epoch train time: 0:00:01.606821
elapsed time: 0:06:31.795583
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 19:44:47.565548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.87
 ---- batch: 020 ----
mean loss: 99.18
 ---- batch: 030 ----
mean loss: 101.15
 ---- batch: 040 ----
mean loss: 101.46
 ---- batch: 050 ----
mean loss: 104.19
 ---- batch: 060 ----
mean loss: 105.08
 ---- batch: 070 ----
mean loss: 100.08
 ---- batch: 080 ----
mean loss: 107.00
 ---- batch: 090 ----
mean loss: 99.01
train mean loss: 101.82
epoch train time: 0:00:01.626271
elapsed time: 0:06:33.422504
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 19:44:49.192450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.98
 ---- batch: 020 ----
mean loss: 102.13
 ---- batch: 030 ----
mean loss: 101.03
 ---- batch: 040 ----
mean loss: 98.10
 ---- batch: 050 ----
mean loss: 96.65
 ---- batch: 060 ----
mean loss: 105.01
 ---- batch: 070 ----
mean loss: 106.49
 ---- batch: 080 ----
mean loss: 105.65
 ---- batch: 090 ----
mean loss: 99.28
train mean loss: 101.84
epoch train time: 0:00:01.659793
elapsed time: 0:06:35.082910
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 19:44:50.852685
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.18
 ---- batch: 020 ----
mean loss: 100.14
 ---- batch: 030 ----
mean loss: 102.57
 ---- batch: 040 ----
mean loss: 104.62
 ---- batch: 050 ----
mean loss: 103.81
 ---- batch: 060 ----
mean loss: 96.64
 ---- batch: 070 ----
mean loss: 101.93
 ---- batch: 080 ----
mean loss: 102.69
 ---- batch: 090 ----
mean loss: 107.27
train mean loss: 101.65
epoch train time: 0:00:01.621130
elapsed time: 0:06:36.704483
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 19:44:52.474530
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.54
 ---- batch: 020 ----
mean loss: 93.42
 ---- batch: 030 ----
mean loss: 102.47
 ---- batch: 040 ----
mean loss: 103.48
 ---- batch: 050 ----
mean loss: 97.35
 ---- batch: 060 ----
mean loss: 101.37
 ---- batch: 070 ----
mean loss: 100.56
 ---- batch: 080 ----
mean loss: 108.85
 ---- batch: 090 ----
mean loss: 104.60
train mean loss: 101.64
epoch train time: 0:00:01.647153
elapsed time: 0:06:38.352388
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 19:44:54.122378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.98
 ---- batch: 020 ----
mean loss: 106.10
 ---- batch: 030 ----
mean loss: 98.35
 ---- batch: 040 ----
mean loss: 104.03
 ---- batch: 050 ----
mean loss: 103.96
 ---- batch: 060 ----
mean loss: 99.26
 ---- batch: 070 ----
mean loss: 98.65
 ---- batch: 080 ----
mean loss: 103.11
 ---- batch: 090 ----
mean loss: 103.91
train mean loss: 101.72
epoch train time: 0:00:01.626658
elapsed time: 0:06:39.979772
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 19:44:55.749755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.40
 ---- batch: 020 ----
mean loss: 105.34
 ---- batch: 030 ----
mean loss: 101.75
 ---- batch: 040 ----
mean loss: 98.46
 ---- batch: 050 ----
mean loss: 99.05
 ---- batch: 060 ----
mean loss: 105.12
 ---- batch: 070 ----
mean loss: 97.39
 ---- batch: 080 ----
mean loss: 101.92
 ---- batch: 090 ----
mean loss: 103.86
train mean loss: 101.57
epoch train time: 0:00:01.612407
elapsed time: 0:06:41.592908
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 19:44:57.362865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.71
 ---- batch: 020 ----
mean loss: 102.99
 ---- batch: 030 ----
mean loss: 104.47
 ---- batch: 040 ----
mean loss: 105.17
 ---- batch: 050 ----
mean loss: 103.98
 ---- batch: 060 ----
mean loss: 98.97
 ---- batch: 070 ----
mean loss: 102.79
 ---- batch: 080 ----
mean loss: 105.04
 ---- batch: 090 ----
mean loss: 94.39
train mean loss: 101.63
epoch train time: 0:00:01.619867
elapsed time: 0:06:43.213403
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 19:44:58.983373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.74
 ---- batch: 020 ----
mean loss: 100.45
 ---- batch: 030 ----
mean loss: 104.85
 ---- batch: 040 ----
mean loss: 102.99
 ---- batch: 050 ----
mean loss: 102.31
 ---- batch: 060 ----
mean loss: 98.04
 ---- batch: 070 ----
mean loss: 104.16
 ---- batch: 080 ----
mean loss: 98.12
 ---- batch: 090 ----
mean loss: 105.25
train mean loss: 101.36
epoch train time: 0:00:01.628001
elapsed time: 0:06:44.842032
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 19:45:00.611971
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.60
 ---- batch: 020 ----
mean loss: 99.76
 ---- batch: 030 ----
mean loss: 100.54
 ---- batch: 040 ----
mean loss: 99.57
 ---- batch: 050 ----
mean loss: 102.53
 ---- batch: 060 ----
mean loss: 101.25
 ---- batch: 070 ----
mean loss: 102.87
 ---- batch: 080 ----
mean loss: 100.66
 ---- batch: 090 ----
mean loss: 106.47
train mean loss: 101.54
epoch train time: 0:00:01.610092
elapsed time: 0:06:46.452715
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 19:45:02.222687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.01
 ---- batch: 020 ----
mean loss: 100.76
 ---- batch: 030 ----
mean loss: 102.52
 ---- batch: 040 ----
mean loss: 94.85
 ---- batch: 050 ----
mean loss: 102.09
 ---- batch: 060 ----
mean loss: 102.74
 ---- batch: 070 ----
mean loss: 107.77
 ---- batch: 080 ----
mean loss: 106.19
 ---- batch: 090 ----
mean loss: 94.66
train mean loss: 101.54
epoch train time: 0:00:01.607562
elapsed time: 0:06:48.060968
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 19:45:03.830924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.93
 ---- batch: 020 ----
mean loss: 101.84
 ---- batch: 030 ----
mean loss: 103.52
 ---- batch: 040 ----
mean loss: 101.25
 ---- batch: 050 ----
mean loss: 99.71
 ---- batch: 060 ----
mean loss: 97.98
 ---- batch: 070 ----
mean loss: 98.84
 ---- batch: 080 ----
mean loss: 105.19
 ---- batch: 090 ----
mean loss: 97.83
train mean loss: 101.42
epoch train time: 0:00:01.632283
elapsed time: 0:06:49.693891
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 19:45:05.463886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.81
 ---- batch: 020 ----
mean loss: 96.09
 ---- batch: 030 ----
mean loss: 100.99
 ---- batch: 040 ----
mean loss: 97.95
 ---- batch: 050 ----
mean loss: 102.79
 ---- batch: 060 ----
mean loss: 102.35
 ---- batch: 070 ----
mean loss: 106.57
 ---- batch: 080 ----
mean loss: 98.76
 ---- batch: 090 ----
mean loss: 104.48
train mean loss: 101.24
epoch train time: 0:00:01.646941
elapsed time: 0:06:51.341501
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 19:45:07.111442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.02
 ---- batch: 020 ----
mean loss: 98.91
 ---- batch: 030 ----
mean loss: 101.63
 ---- batch: 040 ----
mean loss: 102.72
 ---- batch: 050 ----
mean loss: 99.37
 ---- batch: 060 ----
mean loss: 99.75
 ---- batch: 070 ----
mean loss: 95.86
 ---- batch: 080 ----
mean loss: 105.93
 ---- batch: 090 ----
mean loss: 104.10
train mean loss: 101.58
epoch train time: 0:00:01.616231
elapsed time: 0:06:52.958628
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 19:45:08.728327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.32
 ---- batch: 020 ----
mean loss: 102.17
 ---- batch: 030 ----
mean loss: 97.34
 ---- batch: 040 ----
mean loss: 106.77
 ---- batch: 050 ----
mean loss: 104.31
 ---- batch: 060 ----
mean loss: 101.55
 ---- batch: 070 ----
mean loss: 101.03
 ---- batch: 080 ----
mean loss: 103.97
 ---- batch: 090 ----
mean loss: 100.42
train mean loss: 101.29
epoch train time: 0:00:01.620763
elapsed time: 0:06:54.579732
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 19:45:10.349708
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.79
 ---- batch: 020 ----
mean loss: 101.67
 ---- batch: 030 ----
mean loss: 99.39
 ---- batch: 040 ----
mean loss: 98.86
 ---- batch: 050 ----
mean loss: 101.20
 ---- batch: 060 ----
mean loss: 105.52
 ---- batch: 070 ----
mean loss: 104.41
 ---- batch: 080 ----
mean loss: 100.32
 ---- batch: 090 ----
mean loss: 100.40
train mean loss: 101.28
epoch train time: 0:00:01.633481
elapsed time: 0:06:56.213871
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 19:45:11.983828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.70
 ---- batch: 020 ----
mean loss: 103.52
 ---- batch: 030 ----
mean loss: 105.43
 ---- batch: 040 ----
mean loss: 102.25
 ---- batch: 050 ----
mean loss: 95.83
 ---- batch: 060 ----
mean loss: 97.84
 ---- batch: 070 ----
mean loss: 96.44
 ---- batch: 080 ----
mean loss: 99.98
 ---- batch: 090 ----
mean loss: 104.85
train mean loss: 101.07
epoch train time: 0:00:01.611334
elapsed time: 0:06:57.825948
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 19:45:13.595896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.26
 ---- batch: 020 ----
mean loss: 99.98
 ---- batch: 030 ----
mean loss: 107.57
 ---- batch: 040 ----
mean loss: 95.87
 ---- batch: 050 ----
mean loss: 99.02
 ---- batch: 060 ----
mean loss: 94.20
 ---- batch: 070 ----
mean loss: 103.44
 ---- batch: 080 ----
mean loss: 103.62
 ---- batch: 090 ----
mean loss: 101.77
train mean loss: 101.32
epoch train time: 0:00:01.629256
elapsed time: 0:06:59.455798
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 19:45:15.225763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.47
 ---- batch: 020 ----
mean loss: 99.40
 ---- batch: 030 ----
mean loss: 100.35
 ---- batch: 040 ----
mean loss: 97.55
 ---- batch: 050 ----
mean loss: 105.24
 ---- batch: 060 ----
mean loss: 101.71
 ---- batch: 070 ----
mean loss: 106.56
 ---- batch: 080 ----
mean loss: 99.97
 ---- batch: 090 ----
mean loss: 98.41
train mean loss: 101.13
epoch train time: 0:00:01.584234
elapsed time: 0:07:01.040661
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 19:45:16.810603
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.69
 ---- batch: 020 ----
mean loss: 102.02
 ---- batch: 030 ----
mean loss: 97.50
 ---- batch: 040 ----
mean loss: 98.57
 ---- batch: 050 ----
mean loss: 104.17
 ---- batch: 060 ----
mean loss: 98.10
 ---- batch: 070 ----
mean loss: 100.10
 ---- batch: 080 ----
mean loss: 105.02
 ---- batch: 090 ----
mean loss: 102.26
train mean loss: 100.91
epoch train time: 0:00:01.607678
elapsed time: 0:07:02.648953
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 19:45:18.418915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.20
 ---- batch: 020 ----
mean loss: 102.24
 ---- batch: 030 ----
mean loss: 98.47
 ---- batch: 040 ----
mean loss: 97.97
 ---- batch: 050 ----
mean loss: 100.38
 ---- batch: 060 ----
mean loss: 100.27
 ---- batch: 070 ----
mean loss: 102.90
 ---- batch: 080 ----
mean loss: 99.96
 ---- batch: 090 ----
mean loss: 100.68
train mean loss: 100.95
epoch train time: 0:00:01.648556
elapsed time: 0:07:04.298117
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 19:45:20.068054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.66
 ---- batch: 020 ----
mean loss: 97.08
 ---- batch: 030 ----
mean loss: 104.12
 ---- batch: 040 ----
mean loss: 100.83
 ---- batch: 050 ----
mean loss: 103.59
 ---- batch: 060 ----
mean loss: 93.33
 ---- batch: 070 ----
mean loss: 102.58
 ---- batch: 080 ----
mean loss: 99.20
 ---- batch: 090 ----
mean loss: 102.11
train mean loss: 101.04
epoch train time: 0:00:01.618788
elapsed time: 0:07:05.917548
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 19:45:21.687513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.58
 ---- batch: 020 ----
mean loss: 96.30
 ---- batch: 030 ----
mean loss: 103.20
 ---- batch: 040 ----
mean loss: 102.53
 ---- batch: 050 ----
mean loss: 96.16
 ---- batch: 060 ----
mean loss: 98.81
 ---- batch: 070 ----
mean loss: 104.27
 ---- batch: 080 ----
mean loss: 102.35
 ---- batch: 090 ----
mean loss: 99.43
train mean loss: 100.83
epoch train time: 0:00:01.655908
elapsed time: 0:07:07.574101
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 19:45:23.344050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.28
 ---- batch: 020 ----
mean loss: 102.59
 ---- batch: 030 ----
mean loss: 97.31
 ---- batch: 040 ----
mean loss: 96.42
 ---- batch: 050 ----
mean loss: 97.89
 ---- batch: 060 ----
mean loss: 104.03
 ---- batch: 070 ----
mean loss: 100.28
 ---- batch: 080 ----
mean loss: 98.37
 ---- batch: 090 ----
mean loss: 111.11
train mean loss: 100.97
epoch train time: 0:00:01.638292
elapsed time: 0:07:09.213027
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 19:45:24.982989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 93.40
 ---- batch: 020 ----
mean loss: 105.31
 ---- batch: 030 ----
mean loss: 100.06
 ---- batch: 040 ----
mean loss: 99.39
 ---- batch: 050 ----
mean loss: 100.99
 ---- batch: 060 ----
mean loss: 97.16
 ---- batch: 070 ----
mean loss: 100.82
 ---- batch: 080 ----
mean loss: 100.09
 ---- batch: 090 ----
mean loss: 105.95
train mean loss: 100.95
epoch train time: 0:00:01.618869
elapsed time: 0:07:10.832521
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 19:45:26.602503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.76
 ---- batch: 020 ----
mean loss: 101.87
 ---- batch: 030 ----
mean loss: 96.70
 ---- batch: 040 ----
mean loss: 104.11
 ---- batch: 050 ----
mean loss: 103.78
 ---- batch: 060 ----
mean loss: 102.87
 ---- batch: 070 ----
mean loss: 100.02
 ---- batch: 080 ----
mean loss: 101.48
 ---- batch: 090 ----
mean loss: 102.06
train mean loss: 100.67
epoch train time: 0:00:01.636094
elapsed time: 0:07:12.469270
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 19:45:28.239351
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.02
 ---- batch: 020 ----
mean loss: 97.93
 ---- batch: 030 ----
mean loss: 96.72
 ---- batch: 040 ----
mean loss: 103.34
 ---- batch: 050 ----
mean loss: 98.92
 ---- batch: 060 ----
mean loss: 102.25
 ---- batch: 070 ----
mean loss: 99.68
 ---- batch: 080 ----
mean loss: 99.32
 ---- batch: 090 ----
mean loss: 104.26
train mean loss: 100.83
epoch train time: 0:00:01.636174
elapsed time: 0:07:14.106217
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 19:45:29.876200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.32
 ---- batch: 020 ----
mean loss: 98.15
 ---- batch: 030 ----
mean loss: 96.70
 ---- batch: 040 ----
mean loss: 103.27
 ---- batch: 050 ----
mean loss: 103.69
 ---- batch: 060 ----
mean loss: 101.44
 ---- batch: 070 ----
mean loss: 99.97
 ---- batch: 080 ----
mean loss: 100.11
 ---- batch: 090 ----
mean loss: 99.43
train mean loss: 100.66
epoch train time: 0:00:01.623322
elapsed time: 0:07:15.730220
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 19:45:31.500254
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.48
 ---- batch: 020 ----
mean loss: 98.02
 ---- batch: 030 ----
mean loss: 101.66
 ---- batch: 040 ----
mean loss: 99.76
 ---- batch: 050 ----
mean loss: 102.32
 ---- batch: 060 ----
mean loss: 105.82
 ---- batch: 070 ----
mean loss: 102.96
 ---- batch: 080 ----
mean loss: 98.83
 ---- batch: 090 ----
mean loss: 102.15
train mean loss: 100.73
epoch train time: 0:00:01.608668
elapsed time: 0:07:17.339688
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 19:45:33.109677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.94
 ---- batch: 020 ----
mean loss: 103.21
 ---- batch: 030 ----
mean loss: 98.35
 ---- batch: 040 ----
mean loss: 92.76
 ---- batch: 050 ----
mean loss: 100.37
 ---- batch: 060 ----
mean loss: 100.23
 ---- batch: 070 ----
mean loss: 101.49
 ---- batch: 080 ----
mean loss: 106.36
 ---- batch: 090 ----
mean loss: 105.10
train mean loss: 100.45
epoch train time: 0:00:01.618395
elapsed time: 0:07:18.967117
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_2/checkpoint.pth.tar
**** end time: 2019-09-25 19:45:34.736786 ****
