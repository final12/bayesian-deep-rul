Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5339
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-20 21:24:35.139838 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1             [-1, 8, 26, 1]           1,120
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
    BayesianConv2d-4            [-1, 14, 12, 1]             448
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
    BayesianLinear-8                    [-1, 1]             168
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 1,736
Trainable params: 1,736
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:24:35.149085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4315.00
 ---- batch: 020 ----
mean loss: 4245.33
 ---- batch: 030 ----
mean loss: 4228.67
 ---- batch: 040 ----
mean loss: 4087.94
train mean loss: 4204.37
epoch train time: 0:00:14.916413
elapsed time: 0:00:14.928367
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:24:50.068241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4017.83
 ---- batch: 020 ----
mean loss: 3870.86
 ---- batch: 030 ----
mean loss: 3763.67
 ---- batch: 040 ----
mean loss: 3769.45
train mean loss: 3840.75
epoch train time: 0:00:00.587617
elapsed time: 0:00:15.516136
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:24:50.656040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3568.35
 ---- batch: 020 ----
mean loss: 3509.99
 ---- batch: 030 ----
mean loss: 3489.78
 ---- batch: 040 ----
mean loss: 3369.38
train mean loss: 3467.86
epoch train time: 0:00:00.575388
elapsed time: 0:00:16.091758
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:24:51.231660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3253.63
 ---- batch: 020 ----
mean loss: 3283.29
 ---- batch: 030 ----
mean loss: 3042.68
 ---- batch: 040 ----
mean loss: 3084.03
train mean loss: 3164.70
epoch train time: 0:00:00.579316
elapsed time: 0:00:16.671319
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:24:51.811233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3022.21
 ---- batch: 020 ----
mean loss: 2929.24
 ---- batch: 030 ----
mean loss: 2917.55
 ---- batch: 040 ----
mean loss: 2825.17
train mean loss: 2916.41
epoch train time: 0:00:00.628653
elapsed time: 0:00:17.300197
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:24:52.440111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2760.23
 ---- batch: 020 ----
mean loss: 2777.22
 ---- batch: 030 ----
mean loss: 2680.94
 ---- batch: 040 ----
mean loss: 2614.43
train mean loss: 2704.97
epoch train time: 0:00:00.622196
elapsed time: 0:00:17.922647
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:24:53.062556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2561.71
 ---- batch: 020 ----
mean loss: 2592.63
 ---- batch: 030 ----
mean loss: 2520.92
 ---- batch: 040 ----
mean loss: 2410.91
train mean loss: 2517.46
epoch train time: 0:00:00.629593
elapsed time: 0:00:18.552484
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:24:53.692440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2435.15
 ---- batch: 020 ----
mean loss: 2337.98
 ---- batch: 030 ----
mean loss: 2379.04
 ---- batch: 040 ----
mean loss: 2305.24
train mean loss: 2360.08
epoch train time: 0:00:00.597241
elapsed time: 0:00:19.149992
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:24:54.289928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2327.81
 ---- batch: 020 ----
mean loss: 2216.65
 ---- batch: 030 ----
mean loss: 2184.55
 ---- batch: 040 ----
mean loss: 2148.16
train mean loss: 2214.71
epoch train time: 0:00:00.600320
elapsed time: 0:00:19.750537
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:24:54.890440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2123.37
 ---- batch: 020 ----
mean loss: 2086.15
 ---- batch: 030 ----
mean loss: 2062.02
 ---- batch: 040 ----
mean loss: 2065.46
train mean loss: 2078.22
epoch train time: 0:00:00.589536
elapsed time: 0:00:20.340275
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:24:55.480179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2000.41
 ---- batch: 020 ----
mean loss: 1983.90
 ---- batch: 030 ----
mean loss: 1959.20
 ---- batch: 040 ----
mean loss: 1873.86
train mean loss: 1954.45
epoch train time: 0:00:00.611918
elapsed time: 0:00:20.952442
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:24:56.092402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1888.09
 ---- batch: 020 ----
mean loss: 1822.12
 ---- batch: 030 ----
mean loss: 1777.52
 ---- batch: 040 ----
mean loss: 1770.10
train mean loss: 1811.16
epoch train time: 0:00:00.609153
elapsed time: 0:00:21.561878
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:24:56.701786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1727.22
 ---- batch: 020 ----
mean loss: 1708.72
 ---- batch: 030 ----
mean loss: 1656.41
 ---- batch: 040 ----
mean loss: 1666.79
train mean loss: 1685.20
epoch train time: 0:00:00.636598
elapsed time: 0:00:22.198678
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:24:57.338586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1615.56
 ---- batch: 020 ----
mean loss: 1593.26
 ---- batch: 030 ----
mean loss: 1565.83
 ---- batch: 040 ----
mean loss: 1548.81
train mean loss: 1579.04
epoch train time: 0:00:00.628593
elapsed time: 0:00:22.827465
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:24:57.967367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1523.49
 ---- batch: 020 ----
mean loss: 1483.12
 ---- batch: 030 ----
mean loss: 1472.27
 ---- batch: 040 ----
mean loss: 1461.44
train mean loss: 1481.94
epoch train time: 0:00:00.581035
elapsed time: 0:00:23.408727
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:24:58.548631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1401.11
 ---- batch: 020 ----
mean loss: 1419.50
 ---- batch: 030 ----
mean loss: 1394.08
 ---- batch: 040 ----
mean loss: 1372.01
train mean loss: 1393.92
epoch train time: 0:00:00.587792
elapsed time: 0:00:23.996702
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:24:59.136602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1332.97
 ---- batch: 020 ----
mean loss: 1320.83
 ---- batch: 030 ----
mean loss: 1319.73
 ---- batch: 040 ----
mean loss: 1313.91
train mean loss: 1322.41
epoch train time: 0:00:00.605595
elapsed time: 0:00:24.602530
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:24:59.742445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1272.37
 ---- batch: 020 ----
mean loss: 1263.95
 ---- batch: 030 ----
mean loss: 1250.11
 ---- batch: 040 ----
mean loss: 1229.14
train mean loss: 1253.11
epoch train time: 0:00:00.648537
elapsed time: 0:00:25.251307
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:25:00.391214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1222.25
 ---- batch: 020 ----
mean loss: 1171.41
 ---- batch: 030 ----
mean loss: 1189.21
 ---- batch: 040 ----
mean loss: 1172.89
train mean loss: 1185.52
epoch train time: 0:00:00.636519
elapsed time: 0:00:25.888048
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:25:01.027954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1141.86
 ---- batch: 020 ----
mean loss: 1147.68
 ---- batch: 030 ----
mean loss: 1144.81
 ---- batch: 040 ----
mean loss: 1103.34
train mean loss: 1134.31
epoch train time: 0:00:00.598682
elapsed time: 0:00:26.486914
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:25:01.626812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1104.24
 ---- batch: 020 ----
mean loss: 1095.29
 ---- batch: 030 ----
mean loss: 1081.98
 ---- batch: 040 ----
mean loss: 1066.90
train mean loss: 1085.56
epoch train time: 0:00:00.583764
elapsed time: 0:00:27.070873
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:25:02.210810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1056.43
 ---- batch: 020 ----
mean loss: 1032.20
 ---- batch: 030 ----
mean loss: 1025.77
 ---- batch: 040 ----
mean loss: 1028.23
train mean loss: 1034.15
epoch train time: 0:00:00.584714
elapsed time: 0:00:27.655813
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:25:02.795731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1016.29
 ---- batch: 020 ----
mean loss: 1003.81
 ---- batch: 030 ----
mean loss: 987.64
 ---- batch: 040 ----
mean loss: 970.50
train mean loss: 993.33
epoch train time: 0:00:00.614985
elapsed time: 0:00:28.271057
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:25:03.410987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.73
 ---- batch: 020 ----
mean loss: 971.36
 ---- batch: 030 ----
mean loss: 950.16
 ---- batch: 040 ----
mean loss: 936.76
train mean loss: 957.38
epoch train time: 0:00:00.631095
elapsed time: 0:00:28.902392
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:25:04.042302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.72
 ---- batch: 020 ----
mean loss: 930.23
 ---- batch: 030 ----
mean loss: 924.33
 ---- batch: 040 ----
mean loss: 900.41
train mean loss: 923.23
epoch train time: 0:00:00.631722
elapsed time: 0:00:29.534339
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:25:04.674315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.40
 ---- batch: 020 ----
mean loss: 904.91
 ---- batch: 030 ----
mean loss: 888.39
 ---- batch: 040 ----
mean loss: 876.18
train mean loss: 890.19
epoch train time: 0:00:00.592400
elapsed time: 0:00:30.127017
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:25:05.266918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.30
 ---- batch: 020 ----
mean loss: 861.03
 ---- batch: 030 ----
mean loss: 866.73
 ---- batch: 040 ----
mean loss: 861.78
train mean loss: 868.43
epoch train time: 0:00:00.588639
elapsed time: 0:00:30.715844
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:25:05.855747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.29
 ---- batch: 020 ----
mean loss: 849.89
 ---- batch: 030 ----
mean loss: 836.65
 ---- batch: 040 ----
mean loss: 841.33
train mean loss: 841.10
epoch train time: 0:00:00.593344
elapsed time: 0:00:31.309407
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:25:06.449329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.82
 ---- batch: 020 ----
mean loss: 812.31
 ---- batch: 030 ----
mean loss: 811.41
 ---- batch: 040 ----
mean loss: 817.70
train mean loss: 820.59
epoch train time: 0:00:00.613056
elapsed time: 0:00:31.922727
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:25:07.062630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.65
 ---- batch: 020 ----
mean loss: 788.66
 ---- batch: 030 ----
mean loss: 811.63
 ---- batch: 040 ----
mean loss: 795.85
train mean loss: 802.03
epoch train time: 0:00:00.614771
elapsed time: 0:00:32.537749
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:25:07.677662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 783.33
 ---- batch: 020 ----
mean loss: 784.32
 ---- batch: 030 ----
mean loss: 796.61
 ---- batch: 040 ----
mean loss: 780.67
train mean loss: 785.00
epoch train time: 0:00:00.615346
elapsed time: 0:00:33.153330
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:25:08.293250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 788.01
 ---- batch: 020 ----
mean loss: 757.34
 ---- batch: 030 ----
mean loss: 778.85
 ---- batch: 040 ----
mean loss: 758.73
train mean loss: 769.70
epoch train time: 0:00:00.615155
elapsed time: 0:00:33.768708
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:25:08.908617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 782.06
 ---- batch: 020 ----
mean loss: 744.80
 ---- batch: 030 ----
mean loss: 748.19
 ---- batch: 040 ----
mean loss: 771.29
train mean loss: 759.14
epoch train time: 0:00:00.596376
elapsed time: 0:00:34.365304
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:25:09.505205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 747.92
 ---- batch: 020 ----
mean loss: 747.73
 ---- batch: 030 ----
mean loss: 754.87
 ---- batch: 040 ----
mean loss: 745.35
train mean loss: 748.12
epoch train time: 0:00:00.579510
elapsed time: 0:00:34.945006
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:25:10.084909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 739.70
 ---- batch: 020 ----
mean loss: 743.07
 ---- batch: 030 ----
mean loss: 731.41
 ---- batch: 040 ----
mean loss: 740.29
train mean loss: 740.47
epoch train time: 0:00:00.586812
elapsed time: 0:00:35.532022
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:25:10.671941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 732.14
 ---- batch: 020 ----
mean loss: 721.95
 ---- batch: 030 ----
mean loss: 722.95
 ---- batch: 040 ----
mean loss: 742.24
train mean loss: 728.08
epoch train time: 0:00:00.618360
elapsed time: 0:00:36.150623
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:25:11.290530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.07
 ---- batch: 020 ----
mean loss: 727.75
 ---- batch: 030 ----
mean loss: 724.25
 ---- batch: 040 ----
mean loss: 723.50
train mean loss: 721.30
epoch train time: 0:00:00.613782
elapsed time: 0:00:36.764623
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:25:11.904559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.40
 ---- batch: 020 ----
mean loss: 708.24
 ---- batch: 030 ----
mean loss: 704.52
 ---- batch: 040 ----
mean loss: 727.81
train mean loss: 713.14
epoch train time: 0:00:00.605309
elapsed time: 0:00:37.370199
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:25:12.510106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.91
 ---- batch: 020 ----
mean loss: 699.04
 ---- batch: 030 ----
mean loss: 725.72
 ---- batch: 040 ----
mean loss: 703.54
train mean loss: 708.61
epoch train time: 0:00:00.605178
elapsed time: 0:00:37.975559
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:25:13.115459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 715.82
 ---- batch: 020 ----
mean loss: 706.59
 ---- batch: 030 ----
mean loss: 704.64
 ---- batch: 040 ----
mean loss: 693.28
train mean loss: 703.18
epoch train time: 0:00:00.578351
elapsed time: 0:00:38.554145
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:25:13.694047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.07
 ---- batch: 020 ----
mean loss: 720.65
 ---- batch: 030 ----
mean loss: 698.47
 ---- batch: 040 ----
mean loss: 694.49
train mean loss: 700.02
epoch train time: 0:00:00.589852
elapsed time: 0:00:39.144184
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:25:14.284087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 712.59
 ---- batch: 020 ----
mean loss: 697.00
 ---- batch: 030 ----
mean loss: 688.40
 ---- batch: 040 ----
mean loss: 686.11
train mean loss: 696.60
epoch train time: 0:00:00.611421
elapsed time: 0:00:39.755839
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:25:14.895747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.59
 ---- batch: 020 ----
mean loss: 695.27
 ---- batch: 030 ----
mean loss: 697.66
 ---- batch: 040 ----
mean loss: 692.63
train mean loss: 693.38
epoch train time: 0:00:00.615553
elapsed time: 0:00:40.371614
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:25:15.511523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 710.15
 ---- batch: 020 ----
mean loss: 679.92
 ---- batch: 030 ----
mean loss: 687.42
 ---- batch: 040 ----
mean loss: 681.14
train mean loss: 689.71
epoch train time: 0:00:00.611821
elapsed time: 0:00:40.983654
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:25:16.123561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.79
 ---- batch: 020 ----
mean loss: 662.88
 ---- batch: 030 ----
mean loss: 695.37
 ---- batch: 040 ----
mean loss: 699.42
train mean loss: 687.09
epoch train time: 0:00:00.612158
elapsed time: 0:00:41.596012
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:25:16.735915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 703.14
 ---- batch: 020 ----
mean loss: 669.39
 ---- batch: 030 ----
mean loss: 685.27
 ---- batch: 040 ----
mean loss: 688.96
train mean loss: 686.87
epoch train time: 0:00:00.582849
elapsed time: 0:00:42.179113
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:25:17.319034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.61
 ---- batch: 020 ----
mean loss: 688.11
 ---- batch: 030 ----
mean loss: 676.09
 ---- batch: 040 ----
mean loss: 694.23
train mean loss: 680.24
epoch train time: 0:00:00.579415
elapsed time: 0:00:42.758749
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:25:17.898660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.31
 ---- batch: 020 ----
mean loss: 698.28
 ---- batch: 030 ----
mean loss: 675.12
 ---- batch: 040 ----
mean loss: 687.73
train mean loss: 682.72
epoch train time: 0:00:00.581150
elapsed time: 0:00:43.340142
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:25:18.480046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.59
 ---- batch: 020 ----
mean loss: 693.99
 ---- batch: 030 ----
mean loss: 669.94
 ---- batch: 040 ----
mean loss: 683.04
train mean loss: 681.05
epoch train time: 0:00:00.616472
elapsed time: 0:00:43.956831
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:25:19.096741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.67
 ---- batch: 020 ----
mean loss: 674.12
 ---- batch: 030 ----
mean loss: 693.93
 ---- batch: 040 ----
mean loss: 687.35
train mean loss: 679.37
epoch train time: 0:00:00.627414
elapsed time: 0:00:44.584497
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:25:19.724427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 678.85
 ---- batch: 020 ----
mean loss: 691.23
 ---- batch: 030 ----
mean loss: 670.83
 ---- batch: 040 ----
mean loss: 677.39
train mean loss: 680.07
epoch train time: 0:00:00.608163
elapsed time: 0:00:45.192908
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:25:20.332812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.73
 ---- batch: 020 ----
mean loss: 673.20
 ---- batch: 030 ----
mean loss: 682.92
 ---- batch: 040 ----
mean loss: 688.32
train mean loss: 678.70
epoch train time: 0:00:00.605899
elapsed time: 0:00:45.798992
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:25:20.938901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.95
 ---- batch: 020 ----
mean loss: 689.30
 ---- batch: 030 ----
mean loss: 655.70
 ---- batch: 040 ----
mean loss: 682.27
train mean loss: 675.73
epoch train time: 0:00:00.577206
elapsed time: 0:00:46.376389
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:25:21.516292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.13
 ---- batch: 020 ----
mean loss: 665.19
 ---- batch: 030 ----
mean loss: 678.84
 ---- batch: 040 ----
mean loss: 680.93
train mean loss: 676.65
epoch train time: 0:00:00.602471
elapsed time: 0:00:46.979048
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:25:22.118948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.30
 ---- batch: 020 ----
mean loss: 661.39
 ---- batch: 030 ----
mean loss: 681.17
 ---- batch: 040 ----
mean loss: 671.01
train mean loss: 672.59
epoch train time: 0:00:00.580255
elapsed time: 0:00:47.559605
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:25:22.699571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.13
 ---- batch: 020 ----
mean loss: 657.13
 ---- batch: 030 ----
mean loss: 657.73
 ---- batch: 040 ----
mean loss: 687.85
train mean loss: 670.04
epoch train time: 0:00:00.630403
elapsed time: 0:00:48.190338
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:25:23.330265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.99
 ---- batch: 020 ----
mean loss: 670.94
 ---- batch: 030 ----
mean loss: 677.12
 ---- batch: 040 ----
mean loss: 656.96
train mean loss: 670.73
epoch train time: 0:00:00.620353
elapsed time: 0:00:48.810967
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:25:23.950878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.18
 ---- batch: 020 ----
mean loss: 672.77
 ---- batch: 030 ----
mean loss: 673.73
 ---- batch: 040 ----
mean loss: 660.02
train mean loss: 672.19
epoch train time: 0:00:00.617618
elapsed time: 0:00:49.428801
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:25:24.568709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.31
 ---- batch: 020 ----
mean loss: 684.83
 ---- batch: 030 ----
mean loss: 661.91
 ---- batch: 040 ----
mean loss: 663.69
train mean loss: 670.51
epoch train time: 0:00:00.624298
elapsed time: 0:00:50.053320
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:25:25.193238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.48
 ---- batch: 020 ----
mean loss: 683.43
 ---- batch: 030 ----
mean loss: 661.19
 ---- batch: 040 ----
mean loss: 685.46
train mean loss: 669.06
epoch train time: 0:00:00.590036
elapsed time: 0:00:50.643609
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:25:25.783526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.99
 ---- batch: 020 ----
mean loss: 668.45
 ---- batch: 030 ----
mean loss: 671.12
 ---- batch: 040 ----
mean loss: 655.34
train mean loss: 667.40
epoch train time: 0:00:00.606250
elapsed time: 0:00:51.250063
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:25:26.389982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.59
 ---- batch: 020 ----
mean loss: 672.48
 ---- batch: 030 ----
mean loss: 676.31
 ---- batch: 040 ----
mean loss: 666.64
train mean loss: 665.68
epoch train time: 0:00:00.615645
elapsed time: 0:00:51.866005
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:25:27.005946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 664.28
 ---- batch: 020 ----
mean loss: 676.15
 ---- batch: 030 ----
mean loss: 664.56
 ---- batch: 040 ----
mean loss: 649.13
train mean loss: 662.46
epoch train time: 0:00:00.628130
elapsed time: 0:00:52.494389
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:25:27.634301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.00
 ---- batch: 020 ----
mean loss: 652.19
 ---- batch: 030 ----
mean loss: 648.75
 ---- batch: 040 ----
mean loss: 662.02
train mean loss: 655.96
epoch train time: 0:00:00.632027
elapsed time: 0:00:53.126630
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:25:28.266565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.08
 ---- batch: 020 ----
mean loss: 637.44
 ---- batch: 030 ----
mean loss: 641.84
 ---- batch: 040 ----
mean loss: 622.86
train mean loss: 636.82
epoch train time: 0:00:00.625759
elapsed time: 0:00:53.752692
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:25:28.892606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 611.62
 ---- batch: 020 ----
mean loss: 617.60
 ---- batch: 030 ----
mean loss: 588.40
 ---- batch: 040 ----
mean loss: 568.01
train mean loss: 594.20
epoch train time: 0:00:00.595862
elapsed time: 0:00:54.348750
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:25:29.488651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.24
 ---- batch: 020 ----
mean loss: 502.79
 ---- batch: 030 ----
mean loss: 487.28
 ---- batch: 040 ----
mean loss: 465.34
train mean loss: 497.73
epoch train time: 0:00:00.604215
elapsed time: 0:00:54.953155
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:25:30.093061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.14
 ---- batch: 020 ----
mean loss: 429.65
 ---- batch: 030 ----
mean loss: 410.54
 ---- batch: 040 ----
mean loss: 405.40
train mean loss: 420.35
epoch train time: 0:00:00.593511
elapsed time: 0:00:55.546858
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:25:30.686763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.88
 ---- batch: 020 ----
mean loss: 381.04
 ---- batch: 030 ----
mean loss: 391.26
 ---- batch: 040 ----
mean loss: 378.88
train mean loss: 387.33
epoch train time: 0:00:00.632124
elapsed time: 0:00:56.179203
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:25:31.319109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.22
 ---- batch: 020 ----
mean loss: 375.31
 ---- batch: 030 ----
mean loss: 363.56
 ---- batch: 040 ----
mean loss: 364.09
train mean loss: 371.08
epoch train time: 0:00:00.634001
elapsed time: 0:00:56.813430
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:25:31.953348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.46
 ---- batch: 020 ----
mean loss: 362.22
 ---- batch: 030 ----
mean loss: 365.68
 ---- batch: 040 ----
mean loss: 349.89
train mean loss: 360.17
epoch train time: 0:00:00.631962
elapsed time: 0:00:57.445675
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:25:32.585626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.82
 ---- batch: 020 ----
mean loss: 353.97
 ---- batch: 030 ----
mean loss: 352.31
 ---- batch: 040 ----
mean loss: 346.27
train mean loss: 351.14
epoch train time: 0:00:00.624764
elapsed time: 0:00:58.070669
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:25:33.210570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.93
 ---- batch: 020 ----
mean loss: 344.23
 ---- batch: 030 ----
mean loss: 339.02
 ---- batch: 040 ----
mean loss: 346.34
train mean loss: 343.86
epoch train time: 0:00:00.605258
elapsed time: 0:00:58.676110
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:25:33.816009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.78
 ---- batch: 020 ----
mean loss: 328.65
 ---- batch: 030 ----
mean loss: 337.46
 ---- batch: 040 ----
mean loss: 332.00
train mean loss: 336.31
epoch train time: 0:00:00.583926
elapsed time: 0:00:59.260233
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:25:34.400131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.84
 ---- batch: 020 ----
mean loss: 326.04
 ---- batch: 030 ----
mean loss: 322.01
 ---- batch: 040 ----
mean loss: 333.20
train mean loss: 329.90
epoch train time: 0:00:00.594648
elapsed time: 0:00:59.855095
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:25:34.995037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.72
 ---- batch: 020 ----
mean loss: 324.10
 ---- batch: 030 ----
mean loss: 318.35
 ---- batch: 040 ----
mean loss: 332.28
train mean loss: 323.00
epoch train time: 0:00:00.640462
elapsed time: 0:01:00.495850
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:25:35.635760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.31
 ---- batch: 020 ----
mean loss: 316.04
 ---- batch: 030 ----
mean loss: 313.86
 ---- batch: 040 ----
mean loss: 318.41
train mean loss: 316.88
epoch train time: 0:00:00.632693
elapsed time: 0:01:01.128771
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:25:36.268679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.95
 ---- batch: 020 ----
mean loss: 321.42
 ---- batch: 030 ----
mean loss: 306.25
 ---- batch: 040 ----
mean loss: 311.45
train mean loss: 312.77
epoch train time: 0:00:00.636641
elapsed time: 0:01:01.765633
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:25:36.905539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.19
 ---- batch: 020 ----
mean loss: 308.79
 ---- batch: 030 ----
mean loss: 301.51
 ---- batch: 040 ----
mean loss: 306.97
train mean loss: 307.40
epoch train time: 0:00:00.607990
elapsed time: 0:01:02.373827
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:25:37.513728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.42
 ---- batch: 020 ----
mean loss: 302.43
 ---- batch: 030 ----
mean loss: 314.82
 ---- batch: 040 ----
mean loss: 291.63
train mean loss: 304.07
epoch train time: 0:00:00.611176
elapsed time: 0:01:02.985247
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:25:38.125156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.56
 ---- batch: 020 ----
mean loss: 301.72
 ---- batch: 030 ----
mean loss: 304.70
 ---- batch: 040 ----
mean loss: 288.26
train mean loss: 299.10
epoch train time: 0:00:00.579864
elapsed time: 0:01:03.565339
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:25:38.705241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.43
 ---- batch: 020 ----
mean loss: 299.19
 ---- batch: 030 ----
mean loss: 293.78
 ---- batch: 040 ----
mean loss: 292.63
train mean loss: 295.79
epoch train time: 0:00:00.612653
elapsed time: 0:01:04.178226
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:25:39.318135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.75
 ---- batch: 020 ----
mean loss: 302.23
 ---- batch: 030 ----
mean loss: 292.47
 ---- batch: 040 ----
mean loss: 292.28
train mean loss: 292.40
epoch train time: 0:00:00.613852
elapsed time: 0:01:04.792280
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:25:39.932195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.23
 ---- batch: 020 ----
mean loss: 294.08
 ---- batch: 030 ----
mean loss: 280.36
 ---- batch: 040 ----
mean loss: 294.49
train mean loss: 287.91
epoch train time: 0:00:00.612629
elapsed time: 0:01:05.405132
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:25:40.545039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.63
 ---- batch: 020 ----
mean loss: 289.20
 ---- batch: 030 ----
mean loss: 281.95
 ---- batch: 040 ----
mean loss: 284.25
train mean loss: 285.93
epoch train time: 0:00:00.614172
elapsed time: 0:01:06.019507
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:25:41.159411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.48
 ---- batch: 020 ----
mean loss: 281.39
 ---- batch: 030 ----
mean loss: 284.40
 ---- batch: 040 ----
mean loss: 288.18
train mean loss: 282.07
epoch train time: 0:00:00.587571
elapsed time: 0:01:06.607262
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:25:41.747162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.31
 ---- batch: 020 ----
mean loss: 283.02
 ---- batch: 030 ----
mean loss: 272.80
 ---- batch: 040 ----
mean loss: 281.81
train mean loss: 280.34
epoch train time: 0:00:00.606085
elapsed time: 0:01:07.213538
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:25:42.353444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.96
 ---- batch: 020 ----
mean loss: 270.76
 ---- batch: 030 ----
mean loss: 283.99
 ---- batch: 040 ----
mean loss: 277.34
train mean loss: 277.29
epoch train time: 0:00:00.597465
elapsed time: 0:01:07.811201
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:25:42.951122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.81
 ---- batch: 020 ----
mean loss: 276.79
 ---- batch: 030 ----
mean loss: 278.19
 ---- batch: 040 ----
mean loss: 267.81
train mean loss: 273.94
epoch train time: 0:00:00.606655
elapsed time: 0:01:08.418069
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:25:43.557974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.57
 ---- batch: 020 ----
mean loss: 262.46
 ---- batch: 030 ----
mean loss: 276.31
 ---- batch: 040 ----
mean loss: 272.86
train mean loss: 271.67
epoch train time: 0:00:00.614614
elapsed time: 0:01:09.032906
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:25:44.172811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.74
 ---- batch: 020 ----
mean loss: 269.50
 ---- batch: 030 ----
mean loss: 267.00
 ---- batch: 040 ----
mean loss: 268.87
train mean loss: 268.69
epoch train time: 0:00:00.614425
elapsed time: 0:01:09.647543
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:25:44.787448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.87
 ---- batch: 020 ----
mean loss: 268.20
 ---- batch: 030 ----
mean loss: 265.18
 ---- batch: 040 ----
mean loss: 263.50
train mean loss: 265.48
epoch train time: 0:00:00.599850
elapsed time: 0:01:10.247600
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:25:45.387503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.76
 ---- batch: 020 ----
mean loss: 267.27
 ---- batch: 030 ----
mean loss: 262.51
 ---- batch: 040 ----
mean loss: 265.93
train mean loss: 264.32
epoch train time: 0:00:00.598276
elapsed time: 0:01:10.846060
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:25:45.985978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.41
 ---- batch: 020 ----
mean loss: 259.44
 ---- batch: 030 ----
mean loss: 254.73
 ---- batch: 040 ----
mean loss: 266.14
train mean loss: 261.89
epoch train time: 0:00:00.587549
elapsed time: 0:01:11.433812
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:25:46.573747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.46
 ---- batch: 020 ----
mean loss: 264.60
 ---- batch: 030 ----
mean loss: 263.02
 ---- batch: 040 ----
mean loss: 257.95
train mean loss: 259.61
epoch train time: 0:00:00.608205
elapsed time: 0:01:12.042279
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:25:47.182207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.97
 ---- batch: 020 ----
mean loss: 261.49
 ---- batch: 030 ----
mean loss: 247.06
 ---- batch: 040 ----
mean loss: 254.28
train mean loss: 257.10
epoch train time: 0:00:00.630708
elapsed time: 0:01:12.673244
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:25:47.813147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.42
 ---- batch: 020 ----
mean loss: 254.31
 ---- batch: 030 ----
mean loss: 249.43
 ---- batch: 040 ----
mean loss: 261.56
train mean loss: 256.28
epoch train time: 0:00:00.611561
elapsed time: 0:01:13.285024
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:25:48.424962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.31
 ---- batch: 020 ----
mean loss: 253.49
 ---- batch: 030 ----
mean loss: 253.22
 ---- batch: 040 ----
mean loss: 252.62
train mean loss: 253.59
epoch train time: 0:00:00.622150
elapsed time: 0:01:13.907431
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:25:49.047386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.40
 ---- batch: 020 ----
mean loss: 252.35
 ---- batch: 030 ----
mean loss: 257.69
 ---- batch: 040 ----
mean loss: 247.64
train mean loss: 251.95
epoch train time: 0:00:00.594430
elapsed time: 0:01:14.502104
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:25:49.642049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.92
 ---- batch: 020 ----
mean loss: 250.46
 ---- batch: 030 ----
mean loss: 244.48
 ---- batch: 040 ----
mean loss: 252.07
train mean loss: 249.27
epoch train time: 0:00:00.598497
elapsed time: 0:01:15.100971
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:25:50.240893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.12
 ---- batch: 020 ----
mean loss: 248.83
 ---- batch: 030 ----
mean loss: 254.37
 ---- batch: 040 ----
mean loss: 248.48
train mean loss: 248.24
epoch train time: 0:00:00.591317
elapsed time: 0:01:15.692501
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:25:50.832442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.28
 ---- batch: 020 ----
mean loss: 247.47
 ---- batch: 030 ----
mean loss: 248.84
 ---- batch: 040 ----
mean loss: 241.65
train mean loss: 246.23
epoch train time: 0:00:00.608766
elapsed time: 0:01:16.301539
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:25:51.441479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.35
 ---- batch: 020 ----
mean loss: 247.91
 ---- batch: 030 ----
mean loss: 246.38
 ---- batch: 040 ----
mean loss: 240.77
train mean loss: 244.52
epoch train time: 0:00:00.629333
elapsed time: 0:01:16.931116
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:25:52.071042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.93
 ---- batch: 020 ----
mean loss: 247.81
 ---- batch: 030 ----
mean loss: 242.66
 ---- batch: 040 ----
mean loss: 240.43
train mean loss: 243.54
epoch train time: 0:00:00.626347
elapsed time: 0:01:17.557714
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:25:52.697621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.37
 ---- batch: 020 ----
mean loss: 244.18
 ---- batch: 030 ----
mean loss: 231.34
 ---- batch: 040 ----
mean loss: 244.15
train mean loss: 241.39
epoch train time: 0:00:00.620268
elapsed time: 0:01:18.178219
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:25:53.318150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.00
 ---- batch: 020 ----
mean loss: 243.02
 ---- batch: 030 ----
mean loss: 237.11
 ---- batch: 040 ----
mean loss: 236.61
train mean loss: 240.09
epoch train time: 0:00:00.594792
elapsed time: 0:01:18.773256
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:25:53.913176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.94
 ---- batch: 020 ----
mean loss: 242.19
 ---- batch: 030 ----
mean loss: 231.71
 ---- batch: 040 ----
mean loss: 234.46
train mean loss: 237.82
epoch train time: 0:00:00.591811
elapsed time: 0:01:19.365318
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:25:54.505213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.17
 ---- batch: 020 ----
mean loss: 248.61
 ---- batch: 030 ----
mean loss: 233.89
 ---- batch: 040 ----
mean loss: 228.22
train mean loss: 235.99
epoch train time: 0:00:00.602095
elapsed time: 0:01:19.967609
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:25:55.107518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.09
 ---- batch: 020 ----
mean loss: 237.58
 ---- batch: 030 ----
mean loss: 228.61
 ---- batch: 040 ----
mean loss: 234.39
train mean loss: 235.00
epoch train time: 0:00:00.627883
elapsed time: 0:01:20.595735
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:25:55.735681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.54
 ---- batch: 020 ----
mean loss: 237.26
 ---- batch: 030 ----
mean loss: 237.08
 ---- batch: 040 ----
mean loss: 225.11
train mean loss: 233.10
epoch train time: 0:00:00.614370
elapsed time: 0:01:21.210349
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:25:56.350257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.29
 ---- batch: 020 ----
mean loss: 222.27
 ---- batch: 030 ----
mean loss: 237.30
 ---- batch: 040 ----
mean loss: 236.37
train mean loss: 231.21
epoch train time: 0:00:00.620273
elapsed time: 0:01:21.830857
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:25:56.970799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.35
 ---- batch: 020 ----
mean loss: 230.36
 ---- batch: 030 ----
mean loss: 230.34
 ---- batch: 040 ----
mean loss: 231.37
train mean loss: 230.79
epoch train time: 0:00:00.616225
elapsed time: 0:01:22.447327
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:25:57.587230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.72
 ---- batch: 020 ----
mean loss: 225.17
 ---- batch: 030 ----
mean loss: 231.99
 ---- batch: 040 ----
mean loss: 228.04
train mean loss: 229.71
epoch train time: 0:00:00.605509
elapsed time: 0:01:23.053035
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:25:58.192936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.86
 ---- batch: 020 ----
mean loss: 231.14
 ---- batch: 030 ----
mean loss: 229.28
 ---- batch: 040 ----
mean loss: 225.70
train mean loss: 227.73
epoch train time: 0:00:00.590549
elapsed time: 0:01:23.643838
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:25:58.783746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.49
 ---- batch: 020 ----
mean loss: 226.07
 ---- batch: 030 ----
mean loss: 221.62
 ---- batch: 040 ----
mean loss: 222.64
train mean loss: 227.40
epoch train time: 0:00:00.613374
elapsed time: 0:01:24.257474
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:25:59.397387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.12
 ---- batch: 020 ----
mean loss: 222.41
 ---- batch: 030 ----
mean loss: 228.03
 ---- batch: 040 ----
mean loss: 225.05
train mean loss: 226.15
epoch train time: 0:00:00.636229
elapsed time: 0:01:24.893980
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:26:00.033890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.75
 ---- batch: 020 ----
mean loss: 223.04
 ---- batch: 030 ----
mean loss: 220.60
 ---- batch: 040 ----
mean loss: 218.46
train mean loss: 223.41
epoch train time: 0:00:00.622778
elapsed time: 0:01:25.516981
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:26:00.656893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.11
 ---- batch: 020 ----
mean loss: 232.53
 ---- batch: 030 ----
mean loss: 221.57
 ---- batch: 040 ----
mean loss: 220.24
train mean loss: 222.69
epoch train time: 0:00:00.616129
elapsed time: 0:01:26.133315
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:26:01.273220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.68
 ---- batch: 020 ----
mean loss: 217.56
 ---- batch: 030 ----
mean loss: 223.76
 ---- batch: 040 ----
mean loss: 220.22
train mean loss: 222.15
epoch train time: 0:00:00.603968
elapsed time: 0:01:26.737483
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:26:01.877402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.67
 ---- batch: 020 ----
mean loss: 220.23
 ---- batch: 030 ----
mean loss: 217.03
 ---- batch: 040 ----
mean loss: 221.74
train mean loss: 221.21
epoch train time: 0:00:00.576285
elapsed time: 0:01:27.313986
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:26:02.453890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.54
 ---- batch: 020 ----
mean loss: 221.54
 ---- batch: 030 ----
mean loss: 221.19
 ---- batch: 040 ----
mean loss: 216.84
train mean loss: 219.17
epoch train time: 0:00:00.592369
elapsed time: 0:01:27.906554
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:26:03.046457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.23
 ---- batch: 020 ----
mean loss: 223.81
 ---- batch: 030 ----
mean loss: 213.09
 ---- batch: 040 ----
mean loss: 220.85
train mean loss: 219.13
epoch train time: 0:00:00.604150
elapsed time: 0:01:28.510940
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:26:03.650860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.65
 ---- batch: 020 ----
mean loss: 215.16
 ---- batch: 030 ----
mean loss: 220.09
 ---- batch: 040 ----
mean loss: 223.32
train mean loss: 217.67
epoch train time: 0:00:00.610120
elapsed time: 0:01:29.121293
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:26:04.261197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.93
 ---- batch: 020 ----
mean loss: 210.97
 ---- batch: 030 ----
mean loss: 218.28
 ---- batch: 040 ----
mean loss: 217.84
train mean loss: 216.61
epoch train time: 0:00:00.616147
elapsed time: 0:01:29.737662
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:26:04.877590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.89
 ---- batch: 020 ----
mean loss: 215.07
 ---- batch: 030 ----
mean loss: 214.07
 ---- batch: 040 ----
mean loss: 212.78
train mean loss: 215.34
epoch train time: 0:00:00.616255
elapsed time: 0:01:30.354173
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:26:05.494081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.43
 ---- batch: 020 ----
mean loss: 215.07
 ---- batch: 030 ----
mean loss: 219.71
 ---- batch: 040 ----
mean loss: 219.19
train mean loss: 214.22
epoch train time: 0:00:00.597625
elapsed time: 0:01:30.952013
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:26:06.091918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.19
 ---- batch: 020 ----
mean loss: 218.82
 ---- batch: 030 ----
mean loss: 219.99
 ---- batch: 040 ----
mean loss: 202.88
train mean loss: 213.64
epoch train time: 0:00:00.589897
elapsed time: 0:01:31.542124
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:26:06.681998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.42
 ---- batch: 020 ----
mean loss: 208.83
 ---- batch: 030 ----
mean loss: 215.53
 ---- batch: 040 ----
mean loss: 212.76
train mean loss: 213.29
epoch train time: 0:00:00.600494
elapsed time: 0:01:32.142812
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:26:07.282718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.20
 ---- batch: 020 ----
mean loss: 209.50
 ---- batch: 030 ----
mean loss: 202.38
 ---- batch: 040 ----
mean loss: 217.84
train mean loss: 211.31
epoch train time: 0:00:00.619581
elapsed time: 0:01:32.762611
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:26:07.902518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.77
 ---- batch: 020 ----
mean loss: 210.25
 ---- batch: 030 ----
mean loss: 212.74
 ---- batch: 040 ----
mean loss: 212.30
train mean loss: 210.31
epoch train time: 0:00:00.608935
elapsed time: 0:01:33.371758
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:26:08.511668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.76
 ---- batch: 020 ----
mean loss: 207.70
 ---- batch: 030 ----
mean loss: 210.13
 ---- batch: 040 ----
mean loss: 206.60
train mean loss: 209.18
epoch train time: 0:00:00.617155
elapsed time: 0:01:33.989143
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:26:09.129090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.31
 ---- batch: 020 ----
mean loss: 207.98
 ---- batch: 030 ----
mean loss: 202.80
 ---- batch: 040 ----
mean loss: 211.53
train mean loss: 208.58
epoch train time: 0:00:00.603007
elapsed time: 0:01:34.592391
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:26:09.732293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.59
 ---- batch: 020 ----
mean loss: 205.37
 ---- batch: 030 ----
mean loss: 213.07
 ---- batch: 040 ----
mean loss: 208.29
train mean loss: 207.94
epoch train time: 0:00:00.579516
elapsed time: 0:01:35.172172
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:26:10.312113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.95
 ---- batch: 020 ----
mean loss: 207.20
 ---- batch: 030 ----
mean loss: 212.57
 ---- batch: 040 ----
mean loss: 199.58
train mean loss: 206.99
epoch train time: 0:00:00.591193
elapsed time: 0:01:35.763596
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:26:10.903503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.85
 ---- batch: 020 ----
mean loss: 212.11
 ---- batch: 030 ----
mean loss: 199.79
 ---- batch: 040 ----
mean loss: 208.14
train mean loss: 205.80
epoch train time: 0:00:00.595351
elapsed time: 0:01:36.359202
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:26:11.499111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.61
 ---- batch: 020 ----
mean loss: 208.85
 ---- batch: 030 ----
mean loss: 200.49
 ---- batch: 040 ----
mean loss: 207.91
train mean loss: 204.67
epoch train time: 0:00:00.644471
elapsed time: 0:01:37.003892
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:26:12.143805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.75
 ---- batch: 020 ----
mean loss: 202.95
 ---- batch: 030 ----
mean loss: 200.98
 ---- batch: 040 ----
mean loss: 205.79
train mean loss: 203.80
epoch train time: 0:00:00.625771
elapsed time: 0:01:37.629890
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:26:12.769799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.56
 ---- batch: 020 ----
mean loss: 202.07
 ---- batch: 030 ----
mean loss: 201.99
 ---- batch: 040 ----
mean loss: 204.62
train mean loss: 203.37
epoch train time: 0:00:00.610952
elapsed time: 0:01:38.241050
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:26:13.380959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.41
 ---- batch: 020 ----
mean loss: 205.71
 ---- batch: 030 ----
mean loss: 206.96
 ---- batch: 040 ----
mean loss: 198.11
train mean loss: 202.61
epoch train time: 0:00:00.604952
elapsed time: 0:01:38.846195
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:26:13.986097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.73
 ---- batch: 020 ----
mean loss: 195.77
 ---- batch: 030 ----
mean loss: 202.62
 ---- batch: 040 ----
mean loss: 196.87
train mean loss: 201.35
epoch train time: 0:00:00.592336
elapsed time: 0:01:39.438718
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:26:14.578620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.51
 ---- batch: 020 ----
mean loss: 198.96
 ---- batch: 030 ----
mean loss: 197.45
 ---- batch: 040 ----
mean loss: 209.30
train mean loss: 201.14
epoch train time: 0:00:00.606494
elapsed time: 0:01:40.045419
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:26:15.185324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.07
 ---- batch: 020 ----
mean loss: 198.71
 ---- batch: 030 ----
mean loss: 200.23
 ---- batch: 040 ----
mean loss: 205.72
train mean loss: 199.81
epoch train time: 0:00:00.632997
elapsed time: 0:01:40.678631
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:26:15.818540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.99
 ---- batch: 020 ----
mean loss: 199.99
 ---- batch: 030 ----
mean loss: 196.50
 ---- batch: 040 ----
mean loss: 195.58
train mean loss: 198.63
epoch train time: 0:00:00.619970
elapsed time: 0:01:41.298833
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:26:16.438800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.60
 ---- batch: 020 ----
mean loss: 196.04
 ---- batch: 030 ----
mean loss: 201.26
 ---- batch: 040 ----
mean loss: 196.35
train mean loss: 198.53
epoch train time: 0:00:00.620208
elapsed time: 0:01:41.919301
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:26:17.059204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.59
 ---- batch: 020 ----
mean loss: 200.94
 ---- batch: 030 ----
mean loss: 196.97
 ---- batch: 040 ----
mean loss: 192.79
train mean loss: 197.80
epoch train time: 0:00:00.574575
elapsed time: 0:01:42.494053
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:26:17.633949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.21
 ---- batch: 020 ----
mean loss: 198.79
 ---- batch: 030 ----
mean loss: 194.42
 ---- batch: 040 ----
mean loss: 194.20
train mean loss: 197.08
epoch train time: 0:00:00.599861
elapsed time: 0:01:43.094126
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:26:18.234043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.80
 ---- batch: 020 ----
mean loss: 200.20
 ---- batch: 030 ----
mean loss: 193.97
 ---- batch: 040 ----
mean loss: 198.03
train mean loss: 196.04
epoch train time: 0:00:00.600180
elapsed time: 0:01:43.694574
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:26:18.834478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.97
 ---- batch: 020 ----
mean loss: 192.29
 ---- batch: 030 ----
mean loss: 197.37
 ---- batch: 040 ----
mean loss: 196.74
train mean loss: 196.29
epoch train time: 0:00:00.626778
elapsed time: 0:01:44.321565
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:26:19.461474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.63
 ---- batch: 020 ----
mean loss: 189.24
 ---- batch: 030 ----
mean loss: 198.58
 ---- batch: 040 ----
mean loss: 191.42
train mean loss: 194.68
epoch train time: 0:00:00.637928
elapsed time: 0:01:44.959769
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:26:20.099646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.20
 ---- batch: 020 ----
mean loss: 192.88
 ---- batch: 030 ----
mean loss: 193.50
 ---- batch: 040 ----
mean loss: 193.22
train mean loss: 194.22
epoch train time: 0:00:00.626551
elapsed time: 0:01:45.586512
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:26:20.726420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.76
 ---- batch: 020 ----
mean loss: 191.27
 ---- batch: 030 ----
mean loss: 201.40
 ---- batch: 040 ----
mean loss: 191.38
train mean loss: 194.14
epoch train time: 0:00:00.593839
elapsed time: 0:01:46.180535
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:26:21.320453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.26
 ---- batch: 020 ----
mean loss: 188.28
 ---- batch: 030 ----
mean loss: 195.84
 ---- batch: 040 ----
mean loss: 192.68
train mean loss: 192.76
epoch train time: 0:00:00.604691
elapsed time: 0:01:46.785430
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:26:21.925332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.86
 ---- batch: 020 ----
mean loss: 188.23
 ---- batch: 030 ----
mean loss: 185.13
 ---- batch: 040 ----
mean loss: 198.41
train mean loss: 191.99
epoch train time: 0:00:00.598377
elapsed time: 0:01:47.383997
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:26:22.523903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.94
 ---- batch: 020 ----
mean loss: 190.56
 ---- batch: 030 ----
mean loss: 187.44
 ---- batch: 040 ----
mean loss: 193.08
train mean loss: 192.02
epoch train time: 0:00:00.613510
elapsed time: 0:01:47.997786
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:26:23.137691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.20
 ---- batch: 020 ----
mean loss: 197.13
 ---- batch: 030 ----
mean loss: 189.32
 ---- batch: 040 ----
mean loss: 189.54
train mean loss: 190.92
epoch train time: 0:00:00.615975
elapsed time: 0:01:48.613992
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:26:23.753923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.08
 ---- batch: 020 ----
mean loss: 188.23
 ---- batch: 030 ----
mean loss: 193.34
 ---- batch: 040 ----
mean loss: 190.12
train mean loss: 190.05
epoch train time: 0:00:00.627797
elapsed time: 0:01:49.242025
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:26:24.381933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.04
 ---- batch: 020 ----
mean loss: 195.87
 ---- batch: 030 ----
mean loss: 188.89
 ---- batch: 040 ----
mean loss: 192.24
train mean loss: 189.45
epoch train time: 0:00:00.607873
elapsed time: 0:01:49.850096
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:26:24.989998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.13
 ---- batch: 020 ----
mean loss: 192.23
 ---- batch: 030 ----
mean loss: 193.21
 ---- batch: 040 ----
mean loss: 178.32
train mean loss: 188.72
epoch train time: 0:00:00.594242
elapsed time: 0:01:50.444594
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:26:25.584515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.83
 ---- batch: 020 ----
mean loss: 188.10
 ---- batch: 030 ----
mean loss: 192.87
 ---- batch: 040 ----
mean loss: 183.46
train mean loss: 189.11
epoch train time: 0:00:00.588320
elapsed time: 0:01:51.033117
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:26:26.173017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.36
 ---- batch: 020 ----
mean loss: 181.11
 ---- batch: 030 ----
mean loss: 184.13
 ---- batch: 040 ----
mean loss: 199.37
train mean loss: 187.92
epoch train time: 0:00:00.608869
elapsed time: 0:01:51.642249
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:26:26.782166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.07
 ---- batch: 020 ----
mean loss: 186.52
 ---- batch: 030 ----
mean loss: 182.24
 ---- batch: 040 ----
mean loss: 190.02
train mean loss: 187.00
epoch train time: 0:00:00.619535
elapsed time: 0:01:52.262007
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:26:27.401927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.38
 ---- batch: 020 ----
mean loss: 185.99
 ---- batch: 030 ----
mean loss: 181.86
 ---- batch: 040 ----
mean loss: 185.67
train mean loss: 186.49
epoch train time: 0:00:00.611772
elapsed time: 0:01:52.873990
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:26:28.013898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.36
 ---- batch: 020 ----
mean loss: 182.82
 ---- batch: 030 ----
mean loss: 188.61
 ---- batch: 040 ----
mean loss: 185.67
train mean loss: 185.77
epoch train time: 0:00:00.590728
elapsed time: 0:01:53.464911
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:26:28.604851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.03
 ---- batch: 020 ----
mean loss: 184.67
 ---- batch: 030 ----
mean loss: 188.44
 ---- batch: 040 ----
mean loss: 183.55
train mean loss: 184.76
epoch train time: 0:00:00.586143
elapsed time: 0:01:54.051299
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:26:29.191200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.29
 ---- batch: 020 ----
mean loss: 186.21
 ---- batch: 030 ----
mean loss: 188.07
 ---- batch: 040 ----
mean loss: 184.66
train mean loss: 184.50
epoch train time: 0:00:00.586926
elapsed time: 0:01:54.638427
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:26:29.778344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.67
 ---- batch: 020 ----
mean loss: 179.88
 ---- batch: 030 ----
mean loss: 184.45
 ---- batch: 040 ----
mean loss: 185.94
train mean loss: 183.95
epoch train time: 0:00:00.613551
elapsed time: 0:01:55.252207
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:26:30.392146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.52
 ---- batch: 020 ----
mean loss: 184.88
 ---- batch: 030 ----
mean loss: 188.63
 ---- batch: 040 ----
mean loss: 183.60
train mean loss: 183.30
epoch train time: 0:00:00.633383
elapsed time: 0:01:55.885877
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:26:31.025797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.63
 ---- batch: 020 ----
mean loss: 181.40
 ---- batch: 030 ----
mean loss: 185.27
 ---- batch: 040 ----
mean loss: 182.72
train mean loss: 182.64
epoch train time: 0:00:00.630000
elapsed time: 0:01:56.516142
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:26:31.656048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.48
 ---- batch: 020 ----
mean loss: 180.23
 ---- batch: 030 ----
mean loss: 182.46
 ---- batch: 040 ----
mean loss: 184.35
train mean loss: 181.94
epoch train time: 0:00:00.604902
elapsed time: 0:01:57.121282
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:26:32.261264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.19
 ---- batch: 020 ----
mean loss: 179.35
 ---- batch: 030 ----
mean loss: 188.28
 ---- batch: 040 ----
mean loss: 177.10
train mean loss: 181.55
epoch train time: 0:00:00.607615
elapsed time: 0:01:57.729175
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:26:32.869081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.98
 ---- batch: 020 ----
mean loss: 173.81
 ---- batch: 030 ----
mean loss: 181.75
 ---- batch: 040 ----
mean loss: 182.09
train mean loss: 182.02
epoch train time: 0:00:00.610805
elapsed time: 0:01:58.340180
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:26:33.480088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.05
 ---- batch: 020 ----
mean loss: 177.16
 ---- batch: 030 ----
mean loss: 186.74
 ---- batch: 040 ----
mean loss: 180.37
train mean loss: 180.84
epoch train time: 0:00:00.615217
elapsed time: 0:01:58.955638
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:26:34.095548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.01
 ---- batch: 020 ----
mean loss: 179.12
 ---- batch: 030 ----
mean loss: 174.74
 ---- batch: 040 ----
mean loss: 185.63
train mean loss: 179.79
epoch train time: 0:00:00.634600
elapsed time: 0:01:59.590473
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:26:34.730399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.55
 ---- batch: 020 ----
mean loss: 184.86
 ---- batch: 030 ----
mean loss: 175.84
 ---- batch: 040 ----
mean loss: 178.15
train mean loss: 179.27
epoch train time: 0:00:00.626231
elapsed time: 0:02:00.216967
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:26:35.356845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.02
 ---- batch: 020 ----
mean loss: 178.11
 ---- batch: 030 ----
mean loss: 181.27
 ---- batch: 040 ----
mean loss: 169.49
train mean loss: 179.32
epoch train time: 0:00:00.607727
elapsed time: 0:02:00.824857
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:26:35.964762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.48
 ---- batch: 020 ----
mean loss: 172.50
 ---- batch: 030 ----
mean loss: 177.94
 ---- batch: 040 ----
mean loss: 185.91
train mean loss: 178.43
epoch train time: 0:00:00.601541
elapsed time: 0:02:01.426586
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:26:36.566489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.13
 ---- batch: 020 ----
mean loss: 180.91
 ---- batch: 030 ----
mean loss: 176.04
 ---- batch: 040 ----
mean loss: 174.98
train mean loss: 177.99
epoch train time: 0:00:00.589562
elapsed time: 0:02:02.016407
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:26:37.156356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.41
 ---- batch: 020 ----
mean loss: 171.45
 ---- batch: 030 ----
mean loss: 174.13
 ---- batch: 040 ----
mean loss: 181.98
train mean loss: 177.82
epoch train time: 0:00:00.591071
elapsed time: 0:02:02.607761
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:26:37.747719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.78
 ---- batch: 020 ----
mean loss: 178.34
 ---- batch: 030 ----
mean loss: 175.34
 ---- batch: 040 ----
mean loss: 175.18
train mean loss: 177.45
epoch train time: 0:00:00.614348
elapsed time: 0:02:03.222370
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:26:38.362277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.89
 ---- batch: 020 ----
mean loss: 172.63
 ---- batch: 030 ----
mean loss: 173.34
 ---- batch: 040 ----
mean loss: 183.57
train mean loss: 175.96
epoch train time: 0:00:00.629093
elapsed time: 0:02:03.851702
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:26:38.991611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.41
 ---- batch: 020 ----
mean loss: 177.06
 ---- batch: 030 ----
mean loss: 176.40
 ---- batch: 040 ----
mean loss: 176.28
train mean loss: 175.83
epoch train time: 0:00:00.607924
elapsed time: 0:02:04.459824
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:26:39.599726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.90
 ---- batch: 020 ----
mean loss: 173.91
 ---- batch: 030 ----
mean loss: 174.74
 ---- batch: 040 ----
mean loss: 180.61
train mean loss: 175.27
epoch train time: 0:00:00.601049
elapsed time: 0:02:05.061082
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:26:40.200991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.61
 ---- batch: 020 ----
mean loss: 176.05
 ---- batch: 030 ----
mean loss: 174.08
 ---- batch: 040 ----
mean loss: 175.63
train mean loss: 175.07
epoch train time: 0:00:00.597870
elapsed time: 0:02:05.659147
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:26:40.799053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.94
 ---- batch: 020 ----
mean loss: 173.29
 ---- batch: 030 ----
mean loss: 170.43
 ---- batch: 040 ----
mean loss: 179.01
train mean loss: 174.64
epoch train time: 0:00:00.598948
elapsed time: 0:02:06.258322
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:26:41.398226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.48
 ---- batch: 020 ----
mean loss: 169.19
 ---- batch: 030 ----
mean loss: 174.05
 ---- batch: 040 ----
mean loss: 179.89
train mean loss: 173.85
epoch train time: 0:00:00.626731
elapsed time: 0:02:06.885267
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:26:42.025175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.74
 ---- batch: 020 ----
mean loss: 166.17
 ---- batch: 030 ----
mean loss: 174.00
 ---- batch: 040 ----
mean loss: 174.98
train mean loss: 172.62
epoch train time: 0:00:00.621883
elapsed time: 0:02:07.507374
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:26:42.647318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.04
 ---- batch: 020 ----
mean loss: 172.85
 ---- batch: 030 ----
mean loss: 170.92
 ---- batch: 040 ----
mean loss: 169.97
train mean loss: 171.15
epoch train time: 0:00:00.616972
elapsed time: 0:02:08.124611
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:26:43.264516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.76
 ---- batch: 020 ----
mean loss: 165.33
 ---- batch: 030 ----
mean loss: 172.89
 ---- batch: 040 ----
mean loss: 173.96
train mean loss: 169.92
epoch train time: 0:00:00.596585
elapsed time: 0:02:08.721387
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:26:43.861296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.78
 ---- batch: 020 ----
mean loss: 167.80
 ---- batch: 030 ----
mean loss: 167.62
 ---- batch: 040 ----
mean loss: 169.90
train mean loss: 169.02
epoch train time: 0:00:00.587347
elapsed time: 0:02:09.308962
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:26:44.448893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.45
 ---- batch: 020 ----
mean loss: 178.48
 ---- batch: 030 ----
mean loss: 164.83
 ---- batch: 040 ----
mean loss: 168.50
train mean loss: 168.43
epoch train time: 0:00:00.606391
elapsed time: 0:02:09.915648
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:26:45.055567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.68
 ---- batch: 020 ----
mean loss: 165.20
 ---- batch: 030 ----
mean loss: 174.68
 ---- batch: 040 ----
mean loss: 163.72
train mean loss: 167.29
epoch train time: 0:00:00.613531
elapsed time: 0:02:10.529460
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:26:45.669376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.82
 ---- batch: 020 ----
mean loss: 172.07
 ---- batch: 030 ----
mean loss: 170.63
 ---- batch: 040 ----
mean loss: 162.69
train mean loss: 166.76
epoch train time: 0:00:00.637841
elapsed time: 0:02:11.167542
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:26:46.307456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.33
 ---- batch: 020 ----
mean loss: 164.01
 ---- batch: 030 ----
mean loss: 164.00
 ---- batch: 040 ----
mean loss: 172.36
train mean loss: 166.00
epoch train time: 0:00:00.612758
elapsed time: 0:02:11.780507
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:26:46.920431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.26
 ---- batch: 020 ----
mean loss: 168.09
 ---- batch: 030 ----
mean loss: 165.00
 ---- batch: 040 ----
mean loss: 168.42
train mean loss: 165.80
epoch train time: 0:00:00.587396
elapsed time: 0:02:12.368150
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:26:47.508053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.18
 ---- batch: 020 ----
mean loss: 174.19
 ---- batch: 030 ----
mean loss: 163.22
 ---- batch: 040 ----
mean loss: 162.56
train mean loss: 165.53
epoch train time: 0:00:00.596787
elapsed time: 0:02:12.965120
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:26:48.105020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.14
 ---- batch: 020 ----
mean loss: 169.72
 ---- batch: 030 ----
mean loss: 165.35
 ---- batch: 040 ----
mean loss: 162.59
train mean loss: 164.34
epoch train time: 0:00:00.596640
elapsed time: 0:02:13.561996
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:26:48.701909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.79
 ---- batch: 020 ----
mean loss: 165.42
 ---- batch: 030 ----
mean loss: 159.51
 ---- batch: 040 ----
mean loss: 163.74
train mean loss: 164.30
epoch train time: 0:00:00.658168
elapsed time: 0:02:14.220443
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:26:49.360356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.12
 ---- batch: 020 ----
mean loss: 162.71
 ---- batch: 030 ----
mean loss: 164.55
 ---- batch: 040 ----
mean loss: 161.01
train mean loss: 163.61
epoch train time: 0:00:00.618305
elapsed time: 0:02:14.838985
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:26:49.978887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.15
 ---- batch: 020 ----
mean loss: 161.96
 ---- batch: 030 ----
mean loss: 164.40
 ---- batch: 040 ----
mean loss: 158.09
train mean loss: 163.04
epoch train time: 0:00:00.595501
elapsed time: 0:02:15.434705
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:26:50.574605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.04
 ---- batch: 020 ----
mean loss: 170.45
 ---- batch: 030 ----
mean loss: 159.90
 ---- batch: 040 ----
mean loss: 167.81
train mean loss: 162.61
epoch train time: 0:00:00.590555
elapsed time: 0:02:16.025436
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:26:51.165342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.61
 ---- batch: 020 ----
mean loss: 162.32
 ---- batch: 030 ----
mean loss: 158.66
 ---- batch: 040 ----
mean loss: 161.37
train mean loss: 162.59
epoch train time: 0:00:00.590391
elapsed time: 0:02:16.616041
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:26:51.755954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 159.28
 ---- batch: 020 ----
mean loss: 164.40
 ---- batch: 030 ----
mean loss: 164.06
 ---- batch: 040 ----
mean loss: 160.61
train mean loss: 162.05
epoch train time: 0:00:00.620955
elapsed time: 0:02:17.237251
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:26:52.377160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 158.68
 ---- batch: 020 ----
mean loss: 167.96
 ---- batch: 030 ----
mean loss: 160.29
 ---- batch: 040 ----
mean loss: 158.00
train mean loss: 161.59
epoch train time: 0:00:00.634435
elapsed time: 0:02:17.871902
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:26:53.011827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.06
 ---- batch: 020 ----
mean loss: 166.13
 ---- batch: 030 ----
mean loss: 164.59
 ---- batch: 040 ----
mean loss: 155.96
train mean loss: 161.81
epoch train time: 0:00:00.620376
elapsed time: 0:02:18.492512
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:26:53.632455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 156.66
 ---- batch: 020 ----
mean loss: 160.21
 ---- batch: 030 ----
mean loss: 161.34
 ---- batch: 040 ----
mean loss: 166.05
train mean loss: 161.39
epoch train time: 0:00:00.627349
elapsed time: 0:02:19.120099
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:26:54.260001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.91
 ---- batch: 020 ----
mean loss: 166.35
 ---- batch: 030 ----
mean loss: 161.81
 ---- batch: 040 ----
mean loss: 156.09
train mean loss: 161.61
epoch train time: 0:00:00.593379
elapsed time: 0:02:19.713676
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:26:54.853621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.35
 ---- batch: 020 ----
mean loss: 160.69
 ---- batch: 030 ----
mean loss: 164.01
 ---- batch: 040 ----
mean loss: 161.18
train mean loss: 161.46
epoch train time: 0:00:00.593273
elapsed time: 0:02:20.307188
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:26:55.447095
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 161.85
 ---- batch: 020 ----
mean loss: 165.58
 ---- batch: 030 ----
mean loss: 162.62
 ---- batch: 040 ----
mean loss: 156.70
train mean loss: 161.67
epoch train time: 0:00:00.581319
elapsed time: 0:02:20.888713
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:26:56.028618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.64
 ---- batch: 020 ----
mean loss: 165.81
 ---- batch: 030 ----
mean loss: 157.14
 ---- batch: 040 ----
mean loss: 160.89
train mean loss: 161.53
epoch train time: 0:00:00.604235
elapsed time: 0:02:21.493155
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:26:56.633085
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 159.06
 ---- batch: 020 ----
mean loss: 164.68
 ---- batch: 030 ----
mean loss: 162.66
 ---- batch: 040 ----
mean loss: 164.70
train mean loss: 161.74
epoch train time: 0:00:00.621807
elapsed time: 0:02:22.115195
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:26:57.255100
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 156.43
 ---- batch: 020 ----
mean loss: 163.21
 ---- batch: 030 ----
mean loss: 163.29
 ---- batch: 040 ----
mean loss: 161.45
train mean loss: 161.63
epoch train time: 0:00:00.613429
elapsed time: 0:02:22.728806
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:26:57.868704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 161.48
 ---- batch: 020 ----
mean loss: 164.27
 ---- batch: 030 ----
mean loss: 159.47
 ---- batch: 040 ----
mean loss: 163.33
train mean loss: 161.53
epoch train time: 0:00:00.574018
elapsed time: 0:02:23.303036
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:26:58.442950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.41
 ---- batch: 020 ----
mean loss: 163.26
 ---- batch: 030 ----
mean loss: 159.13
 ---- batch: 040 ----
mean loss: 157.26
train mean loss: 161.23
epoch train time: 0:00:00.591085
elapsed time: 0:02:23.894335
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:26:59.034239
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.62
 ---- batch: 020 ----
mean loss: 158.71
 ---- batch: 030 ----
mean loss: 161.22
 ---- batch: 040 ----
mean loss: 162.27
train mean loss: 161.19
epoch train time: 0:00:00.596763
elapsed time: 0:02:24.491285
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:26:59.631205
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 156.80
 ---- batch: 020 ----
mean loss: 165.44
 ---- batch: 030 ----
mean loss: 160.83
 ---- batch: 040 ----
mean loss: 161.97
train mean loss: 161.47
epoch train time: 0:00:00.616053
elapsed time: 0:02:25.107593
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:27:00.247503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 158.97
 ---- batch: 020 ----
mean loss: 161.93
 ---- batch: 030 ----
mean loss: 158.74
 ---- batch: 040 ----
mean loss: 163.37
train mean loss: 161.35
epoch train time: 0:00:00.618662
elapsed time: 0:02:25.726521
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:27:00.866481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.51
 ---- batch: 020 ----
mean loss: 159.10
 ---- batch: 030 ----
mean loss: 159.77
 ---- batch: 040 ----
mean loss: 157.98
train mean loss: 160.95
epoch train time: 0:00:00.619534
elapsed time: 0:02:26.346299
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:27:01.486197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.35
 ---- batch: 020 ----
mean loss: 156.90
 ---- batch: 030 ----
mean loss: 160.06
 ---- batch: 040 ----
mean loss: 162.45
train mean loss: 161.37
epoch train time: 0:00:00.599161
elapsed time: 0:02:26.945683
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:27:02.085601
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.50
 ---- batch: 020 ----
mean loss: 165.35
 ---- batch: 030 ----
mean loss: 158.88
 ---- batch: 040 ----
mean loss: 156.22
train mean loss: 161.38
epoch train time: 0:00:00.587057
elapsed time: 0:02:27.532973
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:27:02.672884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.00
 ---- batch: 020 ----
mean loss: 157.42
 ---- batch: 030 ----
mean loss: 160.41
 ---- batch: 040 ----
mean loss: 159.60
train mean loss: 160.87
epoch train time: 0:00:00.606700
elapsed time: 0:02:28.139970
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:27:03.279921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 160.89
 ---- batch: 020 ----
mean loss: 164.53
 ---- batch: 030 ----
mean loss: 166.31
 ---- batch: 040 ----
mean loss: 155.38
train mean loss: 160.84
epoch train time: 0:00:00.613233
elapsed time: 0:02:28.753469
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:27:03.893374
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.56
 ---- batch: 020 ----
mean loss: 167.54
 ---- batch: 030 ----
mean loss: 150.90
 ---- batch: 040 ----
mean loss: 161.00
train mean loss: 161.53
epoch train time: 0:00:00.622169
elapsed time: 0:02:29.375856
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:27:04.515825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 154.71
 ---- batch: 020 ----
mean loss: 164.04
 ---- batch: 030 ----
mean loss: 165.44
 ---- batch: 040 ----
mean loss: 159.57
train mean loss: 161.06
epoch train time: 0:00:00.611436
elapsed time: 0:02:29.987606
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:27:05.127513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.77
 ---- batch: 020 ----
mean loss: 163.52
 ---- batch: 030 ----
mean loss: 158.44
 ---- batch: 040 ----
mean loss: 156.92
train mean loss: 161.00
epoch train time: 0:00:00.585208
elapsed time: 0:02:30.573020
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:27:05.712939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.81
 ---- batch: 020 ----
mean loss: 162.91
 ---- batch: 030 ----
mean loss: 156.94
 ---- batch: 040 ----
mean loss: 158.75
train mean loss: 161.08
epoch train time: 0:00:00.597699
elapsed time: 0:02:31.170960
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:27:06.310873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 159.36
 ---- batch: 020 ----
mean loss: 160.66
 ---- batch: 030 ----
mean loss: 160.19
 ---- batch: 040 ----
mean loss: 162.69
train mean loss: 160.99
epoch train time: 0:00:00.609386
elapsed time: 0:02:31.780568
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:27:06.920493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.56
 ---- batch: 020 ----
mean loss: 157.19
 ---- batch: 030 ----
mean loss: 159.55
 ---- batch: 040 ----
mean loss: 159.97
train mean loss: 160.94
epoch train time: 0:00:00.616484
elapsed time: 0:02:32.397306
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:27:07.537247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.18
 ---- batch: 020 ----
mean loss: 163.46
 ---- batch: 030 ----
mean loss: 161.35
 ---- batch: 040 ----
mean loss: 156.92
train mean loss: 160.73
epoch train time: 0:00:00.626428
elapsed time: 0:02:33.024007
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:27:08.163915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.40
 ---- batch: 020 ----
mean loss: 158.12
 ---- batch: 030 ----
mean loss: 163.65
 ---- batch: 040 ----
mean loss: 160.80
train mean loss: 160.71
epoch train time: 0:00:00.619009
elapsed time: 0:02:33.643242
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:27:08.783149
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 159.40
 ---- batch: 020 ----
mean loss: 163.81
 ---- batch: 030 ----
mean loss: 153.64
 ---- batch: 040 ----
mean loss: 165.91
train mean loss: 160.86
epoch train time: 0:00:00.583268
elapsed time: 0:02:34.226717
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:27:09.366624
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 157.31
 ---- batch: 020 ----
mean loss: 158.59
 ---- batch: 030 ----
mean loss: 164.51
 ---- batch: 040 ----
mean loss: 164.92
train mean loss: 160.34
epoch train time: 0:00:00.600388
elapsed time: 0:02:34.827324
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:27:09.967226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 161.71
 ---- batch: 020 ----
mean loss: 153.02
 ---- batch: 030 ----
mean loss: 166.02
 ---- batch: 040 ----
mean loss: 159.28
train mean loss: 160.67
epoch train time: 0:00:00.592534
elapsed time: 0:02:35.420052
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:27:10.559957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.78
 ---- batch: 020 ----
mean loss: 159.28
 ---- batch: 030 ----
mean loss: 165.75
 ---- batch: 040 ----
mean loss: 157.78
train mean loss: 160.45
epoch train time: 0:00:00.608599
elapsed time: 0:02:36.028892
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:27:11.168841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 157.62
 ---- batch: 020 ----
mean loss: 164.32
 ---- batch: 030 ----
mean loss: 162.70
 ---- batch: 040 ----
mean loss: 157.46
train mean loss: 160.27
epoch train time: 0:00:00.617500
elapsed time: 0:02:36.646712
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:27:11.786623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 158.78
 ---- batch: 020 ----
mean loss: 164.86
 ---- batch: 030 ----
mean loss: 160.11
 ---- batch: 040 ----
mean loss: 157.40
train mean loss: 160.34
epoch train time: 0:00:00.609153
elapsed time: 0:02:37.256141
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:27:12.396048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 157.05
 ---- batch: 020 ----
mean loss: 158.72
 ---- batch: 030 ----
mean loss: 157.69
 ---- batch: 040 ----
mean loss: 166.10
train mean loss: 160.16
epoch train time: 0:00:00.601009
elapsed time: 0:02:37.857359
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:27:12.997263
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.37
 ---- batch: 020 ----
mean loss: 154.21
 ---- batch: 030 ----
mean loss: 157.51
 ---- batch: 040 ----
mean loss: 159.74
train mean loss: 160.71
epoch train time: 0:00:00.589721
elapsed time: 0:02:38.447270
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:27:13.587173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.52
 ---- batch: 020 ----
mean loss: 158.53
 ---- batch: 030 ----
mean loss: 159.98
 ---- batch: 040 ----
mean loss: 160.59
train mean loss: 160.32
epoch train time: 0:00:00.583267
elapsed time: 0:02:39.030720
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:27:14.170634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.40
 ---- batch: 020 ----
mean loss: 154.95
 ---- batch: 030 ----
mean loss: 161.19
 ---- batch: 040 ----
mean loss: 160.84
train mean loss: 159.86
epoch train time: 0:00:00.598627
elapsed time: 0:02:39.629572
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:27:14.769477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 158.93
 ---- batch: 020 ----
mean loss: 151.99
 ---- batch: 030 ----
mean loss: 163.55
 ---- batch: 040 ----
mean loss: 166.03
train mean loss: 160.05
epoch train time: 0:00:00.640530
elapsed time: 0:02:40.270311
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:27:15.410220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 159.66
 ---- batch: 020 ----
mean loss: 167.05
 ---- batch: 030 ----
mean loss: 153.84
 ---- batch: 040 ----
mean loss: 163.57
train mean loss: 160.21
epoch train time: 0:00:00.624969
elapsed time: 0:02:40.895512
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:27:16.035418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 153.71
 ---- batch: 020 ----
mean loss: 163.67
 ---- batch: 030 ----
mean loss: 162.61
 ---- batch: 040 ----
mean loss: 160.28
train mean loss: 160.33
epoch train time: 0:00:00.618192
elapsed time: 0:02:41.513906
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:27:16.653827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.31
 ---- batch: 020 ----
mean loss: 162.06
 ---- batch: 030 ----
mean loss: 161.68
 ---- batch: 040 ----
mean loss: 158.19
train mean loss: 160.03
epoch train time: 0:00:00.592236
elapsed time: 0:02:42.106338
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:27:17.246238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 158.25
 ---- batch: 020 ----
mean loss: 160.04
 ---- batch: 030 ----
mean loss: 158.72
 ---- batch: 040 ----
mean loss: 163.86
train mean loss: 159.96
epoch train time: 0:00:00.588167
elapsed time: 0:02:42.694702
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:27:17.834607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 159.38
 ---- batch: 020 ----
mean loss: 162.24
 ---- batch: 030 ----
mean loss: 161.47
 ---- batch: 040 ----
mean loss: 157.91
train mean loss: 159.67
epoch train time: 0:00:00.603177
elapsed time: 0:02:43.298104
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:27:18.438010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 153.28
 ---- batch: 020 ----
mean loss: 163.19
 ---- batch: 030 ----
mean loss: 160.56
 ---- batch: 040 ----
mean loss: 160.87
train mean loss: 159.63
epoch train time: 0:00:00.625722
elapsed time: 0:02:43.924112
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:27:19.064030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 157.34
 ---- batch: 020 ----
mean loss: 159.37
 ---- batch: 030 ----
mean loss: 166.21
 ---- batch: 040 ----
mean loss: 156.71
train mean loss: 160.21
epoch train time: 0:00:00.612475
elapsed time: 0:02:44.536874
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:27:19.676783
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.07
 ---- batch: 020 ----
mean loss: 161.57
 ---- batch: 030 ----
mean loss: 154.89
 ---- batch: 040 ----
mean loss: 157.47
train mean loss: 160.25
epoch train time: 0:00:00.610324
elapsed time: 0:02:45.147393
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:27:20.287293
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 158.76
 ---- batch: 020 ----
mean loss: 158.33
 ---- batch: 030 ----
mean loss: 169.61
 ---- batch: 040 ----
mean loss: 153.18
train mean loss: 159.84
epoch train time: 0:00:00.591582
elapsed time: 0:02:45.739157
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:27:20.879061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 161.83
 ---- batch: 020 ----
mean loss: 163.24
 ---- batch: 030 ----
mean loss: 157.61
 ---- batch: 040 ----
mean loss: 157.30
train mean loss: 159.95
epoch train time: 0:00:00.571732
elapsed time: 0:02:46.314355
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_7/checkpoint.pth.tar
**** end time: 2019-09-20 21:27:21.454207 ****
