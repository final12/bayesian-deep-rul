Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 10586
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 18:39:30.586353 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 18:39:30.603354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1715.79
 ---- batch: 020 ----
mean loss: 1393.15
train mean loss: 1470.76
epoch train time: 0:00:11.556043
elapsed time: 0:00:11.581938
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 18:39:42.168339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1156.35
 ---- batch: 020 ----
mean loss: 1120.37
train mean loss: 1122.32
epoch train time: 0:00:04.015934
elapsed time: 0:00:15.599016
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 18:39:46.185618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1044.18
 ---- batch: 020 ----
mean loss: 1044.09
train mean loss: 1044.67
epoch train time: 0:00:04.041942
elapsed time: 0:00:19.642297
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 18:39:50.228913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 999.63
 ---- batch: 020 ----
mean loss: 998.24
train mean loss: 1008.31
epoch train time: 0:00:04.012800
elapsed time: 0:00:23.656415
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 18:39:54.242998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 975.01
 ---- batch: 020 ----
mean loss: 1036.00
train mean loss: 989.87
epoch train time: 0:00:04.010497
elapsed time: 0:00:27.668541
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 18:39:58.255146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 976.04
 ---- batch: 020 ----
mean loss: 972.37
train mean loss: 974.29
epoch train time: 0:00:04.006728
elapsed time: 0:00:31.676648
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 18:40:02.263276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 950.09
 ---- batch: 020 ----
mean loss: 979.27
train mean loss: 959.21
epoch train time: 0:00:03.995674
elapsed time: 0:00:35.673659
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 18:40:06.260316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.63
 ---- batch: 020 ----
mean loss: 969.59
train mean loss: 956.05
epoch train time: 0:00:03.998662
elapsed time: 0:00:39.673638
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 18:40:10.260217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.90
 ---- batch: 020 ----
mean loss: 929.95
train mean loss: 948.67
epoch train time: 0:00:03.997506
elapsed time: 0:00:43.672440
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 18:40:14.259048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.86
 ---- batch: 020 ----
mean loss: 944.03
train mean loss: 947.98
epoch train time: 0:00:03.991892
elapsed time: 0:00:47.665985
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 18:40:18.252587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.08
 ---- batch: 020 ----
mean loss: 953.99
train mean loss: 940.77
epoch train time: 0:00:04.005260
elapsed time: 0:00:51.672516
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 18:40:22.259086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.81
 ---- batch: 020 ----
mean loss: 925.25
train mean loss: 933.68
epoch train time: 0:00:04.011735
elapsed time: 0:00:55.685590
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 18:40:26.272214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.51
 ---- batch: 020 ----
mean loss: 941.29
train mean loss: 922.55
epoch train time: 0:00:03.999174
elapsed time: 0:00:59.686068
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 18:40:30.272657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.52
 ---- batch: 020 ----
mean loss: 924.79
train mean loss: 920.00
epoch train time: 0:00:04.018918
elapsed time: 0:01:03.706240
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 18:40:34.292810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.15
 ---- batch: 020 ----
mean loss: 909.08
train mean loss: 918.57
epoch train time: 0:00:04.014718
elapsed time: 0:01:07.722210
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 18:40:38.308792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.65
 ---- batch: 020 ----
mean loss: 916.70
train mean loss: 926.33
epoch train time: 0:00:04.020391
elapsed time: 0:01:11.743921
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 18:40:42.330527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.38
 ---- batch: 020 ----
mean loss: 910.90
train mean loss: 917.65
epoch train time: 0:00:04.019018
elapsed time: 0:01:15.764311
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 18:40:46.350930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.84
 ---- batch: 020 ----
mean loss: 907.25
train mean loss: 917.77
epoch train time: 0:00:04.011619
elapsed time: 0:01:19.777265
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 18:40:50.363890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.34
 ---- batch: 020 ----
mean loss: 922.63
train mean loss: 921.14
epoch train time: 0:00:04.020857
elapsed time: 0:01:23.799472
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 18:40:54.386063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.63
 ---- batch: 020 ----
mean loss: 912.09
train mean loss: 907.89
epoch train time: 0:00:04.005192
elapsed time: 0:01:27.805984
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 18:40:58.392571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.44
 ---- batch: 020 ----
mean loss: 896.53
train mean loss: 909.83
epoch train time: 0:00:04.012581
elapsed time: 0:01:31.819881
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 18:41:02.406469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.49
 ---- batch: 020 ----
mean loss: 912.68
train mean loss: 918.28
epoch train time: 0:00:04.007295
elapsed time: 0:01:35.828423
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 18:41:06.415024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.66
 ---- batch: 020 ----
mean loss: 909.50
train mean loss: 909.47
epoch train time: 0:00:04.017548
elapsed time: 0:01:39.847215
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 18:41:10.433812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.48
 ---- batch: 020 ----
mean loss: 898.37
train mean loss: 903.68
epoch train time: 0:00:04.013410
elapsed time: 0:01:43.861858
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 18:41:14.448440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.35
 ---- batch: 020 ----
mean loss: 871.49
train mean loss: 900.33
epoch train time: 0:00:04.041214
elapsed time: 0:01:47.904566
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 18:41:18.491154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.05
 ---- batch: 020 ----
mean loss: 913.53
train mean loss: 897.89
epoch train time: 0:00:04.077589
elapsed time: 0:01:51.983465
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 18:41:22.570088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.78
 ---- batch: 020 ----
mean loss: 879.81
train mean loss: 900.21
epoch train time: 0:00:04.012689
elapsed time: 0:01:55.997865
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 18:41:26.584467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.86
 ---- batch: 020 ----
mean loss: 889.39
train mean loss: 900.94
epoch train time: 0:00:04.014609
elapsed time: 0:02:00.013848
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 18:41:30.600461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.80
 ---- batch: 020 ----
mean loss: 900.46
train mean loss: 891.68
epoch train time: 0:00:04.022689
elapsed time: 0:02:04.038396
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 18:41:34.624983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.60
 ---- batch: 020 ----
mean loss: 914.35
train mean loss: 905.87
epoch train time: 0:00:04.078311
elapsed time: 0:02:08.118098
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 18:41:38.704752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.31
 ---- batch: 020 ----
mean loss: 878.14
train mean loss: 891.12
epoch train time: 0:00:04.070661
elapsed time: 0:02:12.190298
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 18:41:42.776904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.10
 ---- batch: 020 ----
mean loss: 909.48
train mean loss: 900.93
epoch train time: 0:00:04.042624
elapsed time: 0:02:16.234327
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 18:41:46.820959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.26
 ---- batch: 020 ----
mean loss: 880.68
train mean loss: 890.41
epoch train time: 0:00:04.003614
elapsed time: 0:02:20.239497
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 18:41:50.826130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.78
 ---- batch: 020 ----
mean loss: 882.78
train mean loss: 883.64
epoch train time: 0:00:03.999695
elapsed time: 0:02:24.240503
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 18:41:54.827077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.44
 ---- batch: 020 ----
mean loss: 891.05
train mean loss: 889.63
epoch train time: 0:00:04.012401
elapsed time: 0:02:28.254122
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 18:41:58.840716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.28
 ---- batch: 020 ----
mean loss: 905.72
train mean loss: 890.45
epoch train time: 0:00:03.999001
elapsed time: 0:02:32.254448
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 18:42:02.841054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.11
 ---- batch: 020 ----
mean loss: 898.43
train mean loss: 896.45
epoch train time: 0:00:04.003090
elapsed time: 0:02:36.258991
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 18:42:06.845580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.63
 ---- batch: 020 ----
mean loss: 882.36
train mean loss: 886.25
epoch train time: 0:00:03.999834
elapsed time: 0:02:40.260141
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 18:42:10.846728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.51
 ---- batch: 020 ----
mean loss: 874.99
train mean loss: 881.12
epoch train time: 0:00:04.003388
elapsed time: 0:02:44.264994
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 18:42:14.851640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.37
 ---- batch: 020 ----
mean loss: 884.67
train mean loss: 884.85
epoch train time: 0:00:04.001750
elapsed time: 0:02:48.268095
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 18:42:18.854737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.55
 ---- batch: 020 ----
mean loss: 885.14
train mean loss: 888.05
epoch train time: 0:00:03.994399
elapsed time: 0:02:52.263947
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 18:42:22.850596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.49
 ---- batch: 020 ----
mean loss: 898.28
train mean loss: 884.51
epoch train time: 0:00:04.006099
elapsed time: 0:02:56.271377
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 18:42:26.857977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.73
 ---- batch: 020 ----
mean loss: 888.80
train mean loss: 889.04
epoch train time: 0:00:03.997032
elapsed time: 0:03:00.269701
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 18:42:30.856288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.90
 ---- batch: 020 ----
mean loss: 878.09
train mean loss: 878.43
epoch train time: 0:00:04.003111
elapsed time: 0:03:04.274043
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 18:42:34.860638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.86
 ---- batch: 020 ----
mean loss: 901.46
train mean loss: 875.32
epoch train time: 0:00:03.975409
elapsed time: 0:03:08.250806
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 18:42:38.837388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.98
 ---- batch: 020 ----
mean loss: 855.20
train mean loss: 873.92
epoch train time: 0:00:03.974163
elapsed time: 0:03:12.226301
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 18:42:42.812900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.57
 ---- batch: 020 ----
mean loss: 846.83
train mean loss: 872.74
epoch train time: 0:00:03.970154
elapsed time: 0:03:16.197741
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 18:42:46.784334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.06
 ---- batch: 020 ----
mean loss: 859.25
train mean loss: 871.58
epoch train time: 0:00:04.000664
elapsed time: 0:03:20.199856
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 18:42:50.786566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.25
 ---- batch: 020 ----
mean loss: 894.23
train mean loss: 873.24
epoch train time: 0:00:04.008837
elapsed time: 0:03:24.210229
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 18:42:54.796849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.37
 ---- batch: 020 ----
mean loss: 868.35
train mean loss: 869.06
epoch train time: 0:00:04.007920
elapsed time: 0:03:28.219479
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 18:42:58.806084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.26
 ---- batch: 020 ----
mean loss: 873.93
train mean loss: 870.41
epoch train time: 0:00:04.012681
elapsed time: 0:03:32.233441
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 18:43:02.820027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.95
 ---- batch: 020 ----
mean loss: 848.91
train mean loss: 865.75
epoch train time: 0:00:03.993719
elapsed time: 0:03:36.228717
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 18:43:06.815333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.99
 ---- batch: 020 ----
mean loss: 846.94
train mean loss: 863.09
epoch train time: 0:00:04.002734
elapsed time: 0:03:40.232798
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 18:43:10.819393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.18
 ---- batch: 020 ----
mean loss: 854.94
train mean loss: 861.47
epoch train time: 0:00:04.008462
elapsed time: 0:03:44.242502
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 18:43:14.829074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.25
 ---- batch: 020 ----
mean loss: 853.51
train mean loss: 852.33
epoch train time: 0:00:03.996948
elapsed time: 0:03:48.240748
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 18:43:18.827345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.16
 ---- batch: 020 ----
mean loss: 862.49
train mean loss: 854.50
epoch train time: 0:00:03.998550
elapsed time: 0:03:52.240559
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 18:43:22.827180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.10
 ---- batch: 020 ----
mean loss: 849.45
train mean loss: 842.30
epoch train time: 0:00:03.997364
elapsed time: 0:03:56.239361
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 18:43:26.825999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.62
 ---- batch: 020 ----
mean loss: 853.90
train mean loss: 836.43
epoch train time: 0:00:03.997921
elapsed time: 0:04:00.238677
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 18:43:30.825289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.12
 ---- batch: 020 ----
mean loss: 828.20
train mean loss: 834.42
epoch train time: 0:00:04.008492
elapsed time: 0:04:04.248685
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 18:43:34.835316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 823.80
 ---- batch: 020 ----
mean loss: 808.97
train mean loss: 820.27
epoch train time: 0:00:04.012121
elapsed time: 0:04:08.262123
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 18:43:38.848702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.82
 ---- batch: 020 ----
mean loss: 815.86
train mean loss: 808.69
epoch train time: 0:00:04.001904
elapsed time: 0:04:12.265382
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 18:43:42.851963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.08
 ---- batch: 020 ----
mean loss: 800.49
train mean loss: 792.66
epoch train time: 0:00:03.998146
elapsed time: 0:04:16.264915
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 18:43:46.851514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.07
 ---- batch: 020 ----
mean loss: 770.96
train mean loss: 763.32
epoch train time: 0:00:03.997135
elapsed time: 0:04:20.263412
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 18:43:50.850048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 754.20
 ---- batch: 020 ----
mean loss: 744.10
train mean loss: 744.03
epoch train time: 0:00:03.998620
elapsed time: 0:04:24.263415
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 18:43:54.850030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 750.36
 ---- batch: 020 ----
mean loss: 719.57
train mean loss: 741.82
epoch train time: 0:00:04.008316
elapsed time: 0:04:28.273402
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 18:43:58.860069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.82
 ---- batch: 020 ----
mean loss: 729.33
train mean loss: 727.30
epoch train time: 0:00:03.989957
elapsed time: 0:04:32.264867
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 18:44:02.851454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 745.93
 ---- batch: 020 ----
mean loss: 702.77
train mean loss: 716.65
epoch train time: 0:00:04.000089
elapsed time: 0:04:36.266182
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 18:44:06.852785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.80
 ---- batch: 020 ----
mean loss: 695.72
train mean loss: 695.94
epoch train time: 0:00:04.009879
elapsed time: 0:04:40.277377
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 18:44:10.863973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.05
 ---- batch: 020 ----
mean loss: 683.26
train mean loss: 678.50
epoch train time: 0:00:04.000743
elapsed time: 0:04:44.279453
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 18:44:14.866069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.05
 ---- batch: 020 ----
mean loss: 673.63
train mean loss: 673.50
epoch train time: 0:00:04.008847
elapsed time: 0:04:48.289654
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 18:44:18.876266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.58
 ---- batch: 020 ----
mean loss: 661.90
train mean loss: 660.89
epoch train time: 0:00:04.008926
elapsed time: 0:04:52.299996
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 18:44:22.886615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 653.25
 ---- batch: 020 ----
mean loss: 634.52
train mean loss: 648.39
epoch train time: 0:00:03.986927
elapsed time: 0:04:56.288318
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 18:44:26.874933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 629.76
 ---- batch: 020 ----
mean loss: 661.92
train mean loss: 643.63
epoch train time: 0:00:04.019583
elapsed time: 0:05:00.309548
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 18:44:30.896167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 633.41
 ---- batch: 020 ----
mean loss: 644.50
train mean loss: 632.97
epoch train time: 0:00:04.021604
elapsed time: 0:05:04.332624
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 18:44:34.919204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 617.77
 ---- batch: 020 ----
mean loss: 618.91
train mean loss: 619.42
epoch train time: 0:00:03.997420
elapsed time: 0:05:08.331341
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 18:44:38.917932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 615.67
 ---- batch: 020 ----
mean loss: 606.92
train mean loss: 608.76
epoch train time: 0:00:04.023898
elapsed time: 0:05:12.356588
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 18:44:42.943172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.09
 ---- batch: 020 ----
mean loss: 600.49
train mean loss: 600.50
epoch train time: 0:00:04.019031
elapsed time: 0:05:16.377180
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 18:44:46.963778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 582.43
 ---- batch: 020 ----
mean loss: 614.64
train mean loss: 598.73
epoch train time: 0:00:03.995064
elapsed time: 0:05:20.373614
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 18:44:50.960202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.14
 ---- batch: 020 ----
mean loss: 594.79
train mean loss: 587.20
epoch train time: 0:00:04.003930
elapsed time: 0:05:24.378793
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 18:44:54.965421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.05
 ---- batch: 020 ----
mean loss: 593.46
train mean loss: 583.29
epoch train time: 0:00:04.020408
elapsed time: 0:05:28.400498
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 18:44:58.987083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 580.94
 ---- batch: 020 ----
mean loss: 577.23
train mean loss: 571.55
epoch train time: 0:00:04.016301
elapsed time: 0:05:32.418158
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 18:45:03.004774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.65
 ---- batch: 020 ----
mean loss: 571.38
train mean loss: 560.77
epoch train time: 0:00:04.000591
elapsed time: 0:05:36.420069
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 18:45:07.006661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 552.35
 ---- batch: 020 ----
mean loss: 544.43
train mean loss: 548.61
epoch train time: 0:00:03.995608
elapsed time: 0:05:40.417015
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 18:45:11.003605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.92
 ---- batch: 020 ----
mean loss: 550.70
train mean loss: 547.41
epoch train time: 0:00:04.012673
elapsed time: 0:05:44.430944
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 18:45:15.017537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 553.56
 ---- batch: 020 ----
mean loss: 541.43
train mean loss: 541.90
epoch train time: 0:00:04.011213
elapsed time: 0:05:48.443408
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 18:45:19.030037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.65
 ---- batch: 020 ----
mean loss: 531.80
train mean loss: 531.23
epoch train time: 0:00:04.007822
elapsed time: 0:05:52.452487
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 18:45:23.039066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.30
 ---- batch: 020 ----
mean loss: 526.87
train mean loss: 525.11
epoch train time: 0:00:04.021967
elapsed time: 0:05:56.475725
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 18:45:27.062302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.93
 ---- batch: 020 ----
mean loss: 515.41
train mean loss: 515.44
epoch train time: 0:00:04.024978
elapsed time: 0:06:00.501946
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 18:45:31.088552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 532.60
 ---- batch: 020 ----
mean loss: 516.55
train mean loss: 521.09
epoch train time: 0:00:04.013239
elapsed time: 0:06:04.516552
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 18:45:35.103188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.04
 ---- batch: 020 ----
mean loss: 512.70
train mean loss: 515.34
epoch train time: 0:00:04.006310
elapsed time: 0:06:08.524304
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 18:45:39.110904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.69
 ---- batch: 020 ----
mean loss: 498.30
train mean loss: 499.10
epoch train time: 0:00:04.014587
elapsed time: 0:06:12.540171
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 18:45:43.126745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.39
 ---- batch: 020 ----
mean loss: 480.03
train mean loss: 494.72
epoch train time: 0:00:04.022325
elapsed time: 0:06:16.563849
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 18:45:47.150428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.27
 ---- batch: 020 ----
mean loss: 490.05
train mean loss: 488.61
epoch train time: 0:00:04.014551
elapsed time: 0:06:20.580068
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 18:45:51.166721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.96
 ---- batch: 020 ----
mean loss: 477.99
train mean loss: 483.20
epoch train time: 0:00:04.013252
elapsed time: 0:06:24.594791
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 18:45:55.181427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.44
 ---- batch: 020 ----
mean loss: 465.73
train mean loss: 482.02
epoch train time: 0:00:04.014622
elapsed time: 0:06:28.610694
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 18:45:59.197273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.64
 ---- batch: 020 ----
mean loss: 464.50
train mean loss: 474.17
epoch train time: 0:00:04.012935
elapsed time: 0:06:32.624895
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 18:46:03.211481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.96
 ---- batch: 020 ----
mean loss: 460.05
train mean loss: 464.32
epoch train time: 0:00:04.005213
elapsed time: 0:06:36.631449
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 18:46:07.218058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.15
 ---- batch: 020 ----
mean loss: 457.10
train mean loss: 459.79
epoch train time: 0:00:03.982546
elapsed time: 0:06:40.615443
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 18:46:11.202058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.23
 ---- batch: 020 ----
mean loss: 468.57
train mean loss: 451.59
epoch train time: 0:00:04.010193
elapsed time: 0:06:44.626930
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 18:46:15.213536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.18
 ---- batch: 020 ----
mean loss: 446.63
train mean loss: 446.12
epoch train time: 0:00:04.021147
elapsed time: 0:06:48.649397
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 18:46:19.235978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.44
 ---- batch: 020 ----
mean loss: 439.85
train mean loss: 439.57
epoch train time: 0:00:04.032890
elapsed time: 0:06:52.683904
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 18:46:23.270511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.49
 ---- batch: 020 ----
mean loss: 438.64
train mean loss: 444.93
epoch train time: 0:00:04.077358
elapsed time: 0:06:56.762675
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 18:46:27.349253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.86
 ---- batch: 020 ----
mean loss: 426.92
train mean loss: 434.96
epoch train time: 0:00:04.027478
elapsed time: 0:07:00.791395
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 18:46:31.378040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.61
 ---- batch: 020 ----
mean loss: 439.97
train mean loss: 436.71
epoch train time: 0:00:04.017356
elapsed time: 0:07:04.810070
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 18:46:35.396658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.19
 ---- batch: 020 ----
mean loss: 421.05
train mean loss: 428.05
epoch train time: 0:00:04.007454
elapsed time: 0:07:08.818824
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 18:46:39.405407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.31
 ---- batch: 020 ----
mean loss: 419.79
train mean loss: 427.28
epoch train time: 0:00:03.997373
elapsed time: 0:07:12.817487
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 18:46:43.404135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.75
 ---- batch: 020 ----
mean loss: 430.71
train mean loss: 423.87
epoch train time: 0:00:04.048402
elapsed time: 0:07:16.867498
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 18:46:47.453911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.74
 ---- batch: 020 ----
mean loss: 419.91
train mean loss: 409.92
epoch train time: 0:00:04.065445
elapsed time: 0:07:20.934168
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 18:46:51.520821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.59
 ---- batch: 020 ----
mean loss: 417.06
train mean loss: 411.47
epoch train time: 0:00:04.054448
elapsed time: 0:07:24.990056
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 18:46:55.576651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.17
 ---- batch: 020 ----
mean loss: 410.08
train mean loss: 409.53
epoch train time: 0:00:03.991485
elapsed time: 0:07:28.982827
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 18:46:59.569443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.35
 ---- batch: 020 ----
mean loss: 402.49
train mean loss: 404.79
epoch train time: 0:00:03.978565
elapsed time: 0:07:32.962774
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 18:47:03.549401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.28
 ---- batch: 020 ----
mean loss: 403.89
train mean loss: 401.64
epoch train time: 0:00:03.992987
elapsed time: 0:07:36.957150
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 18:47:07.543719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.57
 ---- batch: 020 ----
mean loss: 400.06
train mean loss: 397.32
epoch train time: 0:00:03.996859
elapsed time: 0:07:40.955381
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 18:47:11.541971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.10
 ---- batch: 020 ----
mean loss: 397.73
train mean loss: 392.43
epoch train time: 0:00:03.992255
elapsed time: 0:07:44.948947
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 18:47:15.535579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.72
 ---- batch: 020 ----
mean loss: 376.80
train mean loss: 384.44
epoch train time: 0:00:03.994389
elapsed time: 0:07:48.944873
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 18:47:19.531460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.15
 ---- batch: 020 ----
mean loss: 387.68
train mean loss: 381.95
epoch train time: 0:00:04.015761
elapsed time: 0:07:52.961984
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 18:47:23.548580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.66
 ---- batch: 020 ----
mean loss: 387.06
train mean loss: 381.77
epoch train time: 0:00:04.004156
elapsed time: 0:07:56.967520
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 18:47:27.554148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.33
 ---- batch: 020 ----
mean loss: 372.28
train mean loss: 373.87
epoch train time: 0:00:04.006442
elapsed time: 0:08:00.975317
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 18:47:31.561896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.43
 ---- batch: 020 ----
mean loss: 387.56
train mean loss: 377.53
epoch train time: 0:00:04.012540
elapsed time: 0:08:04.989244
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 18:47:35.575844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.66
 ---- batch: 020 ----
mean loss: 378.97
train mean loss: 374.77
epoch train time: 0:00:04.001931
elapsed time: 0:08:08.992583
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 18:47:39.579171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.86
 ---- batch: 020 ----
mean loss: 377.77
train mean loss: 368.36
epoch train time: 0:00:04.010450
elapsed time: 0:08:13.004276
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 18:47:43.590867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.28
 ---- batch: 020 ----
mean loss: 383.26
train mean loss: 367.03
epoch train time: 0:00:04.003784
elapsed time: 0:08:17.009300
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 18:47:47.595899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.42
 ---- batch: 020 ----
mean loss: 369.87
train mean loss: 364.47
epoch train time: 0:00:04.010763
elapsed time: 0:08:21.021423
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 18:47:51.608001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.71
 ---- batch: 020 ----
mean loss: 371.23
train mean loss: 360.74
epoch train time: 0:00:04.012787
elapsed time: 0:08:25.035542
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 18:47:55.622197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.31
 ---- batch: 020 ----
mean loss: 360.42
train mean loss: 352.34
epoch train time: 0:00:04.016332
elapsed time: 0:08:29.053590
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 18:47:59.640178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.68
 ---- batch: 020 ----
mean loss: 359.23
train mean loss: 358.84
epoch train time: 0:00:04.006464
elapsed time: 0:08:33.061295
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 18:48:03.647920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.17
 ---- batch: 020 ----
mean loss: 363.31
train mean loss: 354.11
epoch train time: 0:00:04.000420
elapsed time: 0:08:37.063301
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 18:48:07.649733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.03
 ---- batch: 020 ----
mean loss: 346.37
train mean loss: 353.18
epoch train time: 0:00:03.990861
elapsed time: 0:08:41.055305
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 18:48:11.641901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.44
 ---- batch: 020 ----
mean loss: 350.70
train mean loss: 349.85
epoch train time: 0:00:04.002467
elapsed time: 0:08:45.059024
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 18:48:15.645627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.98
 ---- batch: 020 ----
mean loss: 344.67
train mean loss: 345.79
epoch train time: 0:00:03.971578
elapsed time: 0:08:49.031970
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 18:48:19.618548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.23
 ---- batch: 020 ----
mean loss: 332.87
train mean loss: 344.06
epoch train time: 0:00:04.009736
elapsed time: 0:08:53.042954
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 18:48:23.629537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.31
 ---- batch: 020 ----
mean loss: 351.71
train mean loss: 345.29
epoch train time: 0:00:04.008578
elapsed time: 0:08:57.052829
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 18:48:27.639428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.33
 ---- batch: 020 ----
mean loss: 341.22
train mean loss: 338.51
epoch train time: 0:00:03.998795
elapsed time: 0:09:01.052939
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 18:48:31.639531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.83
 ---- batch: 020 ----
mean loss: 342.62
train mean loss: 342.98
epoch train time: 0:00:04.004302
elapsed time: 0:09:05.058725
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 18:48:35.645319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.66
 ---- batch: 020 ----
mean loss: 331.67
train mean loss: 341.23
epoch train time: 0:00:04.005560
elapsed time: 0:09:09.065565
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 18:48:39.652166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.15
 ---- batch: 020 ----
mean loss: 329.66
train mean loss: 338.70
epoch train time: 0:00:03.993024
elapsed time: 0:09:13.059898
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 18:48:43.646533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.60
 ---- batch: 020 ----
mean loss: 335.98
train mean loss: 332.80
epoch train time: 0:00:03.998648
elapsed time: 0:09:17.060104
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 18:48:47.646723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.02
 ---- batch: 020 ----
mean loss: 333.41
train mean loss: 337.29
epoch train time: 0:00:03.996188
elapsed time: 0:09:21.057677
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 18:48:51.644269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.14
 ---- batch: 020 ----
mean loss: 335.98
train mean loss: 333.50
epoch train time: 0:00:03.994803
elapsed time: 0:09:25.053784
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 18:48:55.640396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.74
 ---- batch: 020 ----
mean loss: 330.75
train mean loss: 330.90
epoch train time: 0:00:04.013245
elapsed time: 0:09:29.068305
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 18:48:59.654906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.25
 ---- batch: 020 ----
mean loss: 333.96
train mean loss: 328.39
epoch train time: 0:00:03.998192
elapsed time: 0:09:33.068017
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 18:49:03.654684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.66
 ---- batch: 020 ----
mean loss: 330.21
train mean loss: 325.87
epoch train time: 0:00:04.004453
elapsed time: 0:09:37.073789
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 18:49:07.660369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.44
 ---- batch: 020 ----
mean loss: 317.18
train mean loss: 321.77
epoch train time: 0:00:04.008335
elapsed time: 0:09:41.083443
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 18:49:11.670162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.01
 ---- batch: 020 ----
mean loss: 319.34
train mean loss: 323.23
epoch train time: 0:00:03.988232
elapsed time: 0:09:45.073151
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 18:49:15.659764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.70
 ---- batch: 020 ----
mean loss: 314.68
train mean loss: 320.90
epoch train time: 0:00:03.999127
elapsed time: 0:09:49.073595
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 18:49:19.660233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.39
 ---- batch: 020 ----
mean loss: 319.00
train mean loss: 317.41
epoch train time: 0:00:03.994036
elapsed time: 0:09:53.068970
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 18:49:23.655603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.23
 ---- batch: 020 ----
mean loss: 322.01
train mean loss: 315.18
epoch train time: 0:00:04.003785
elapsed time: 0:09:57.074163
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 18:49:27.660730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.78
 ---- batch: 020 ----
mean loss: 323.33
train mean loss: 313.60
epoch train time: 0:00:04.002062
elapsed time: 0:10:01.077503
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 18:49:31.664121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.79
 ---- batch: 020 ----
mean loss: 319.37
train mean loss: 316.22
epoch train time: 0:00:03.997359
elapsed time: 0:10:05.076399
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 18:49:35.662810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.07
 ---- batch: 020 ----
mean loss: 303.34
train mean loss: 311.42
epoch train time: 0:00:04.011187
elapsed time: 0:10:09.088758
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 18:49:39.675412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.39
 ---- batch: 020 ----
mean loss: 301.86
train mean loss: 308.52
epoch train time: 0:00:04.004716
elapsed time: 0:10:13.094800
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 18:49:43.681382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.27
 ---- batch: 020 ----
mean loss: 315.07
train mean loss: 304.63
epoch train time: 0:00:03.998207
elapsed time: 0:10:17.094635
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 18:49:47.681235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.06
 ---- batch: 020 ----
mean loss: 308.98
train mean loss: 307.73
epoch train time: 0:00:03.978262
elapsed time: 0:10:21.074271
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 18:49:51.660892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.08
 ---- batch: 020 ----
mean loss: 298.21
train mean loss: 306.66
epoch train time: 0:00:03.993187
elapsed time: 0:10:25.068792
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 18:49:55.655394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.26
 ---- batch: 020 ----
mean loss: 300.38
train mean loss: 306.93
epoch train time: 0:00:04.001334
elapsed time: 0:10:29.071522
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 18:49:59.658125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.89
 ---- batch: 020 ----
mean loss: 307.89
train mean loss: 302.85
epoch train time: 0:00:03.978634
elapsed time: 0:10:33.051452
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 18:50:03.638047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.28
 ---- batch: 020 ----
mean loss: 313.98
train mean loss: 305.01
epoch train time: 0:00:04.001512
elapsed time: 0:10:37.054352
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 18:50:07.640977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.20
 ---- batch: 020 ----
mean loss: 301.39
train mean loss: 302.52
epoch train time: 0:00:03.983407
elapsed time: 0:10:41.039014
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 18:50:11.625598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.45
 ---- batch: 020 ----
mean loss: 312.24
train mean loss: 302.63
epoch train time: 0:00:03.973745
elapsed time: 0:10:45.014113
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 18:50:15.600704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.69
 ---- batch: 020 ----
mean loss: 294.31
train mean loss: 299.65
epoch train time: 0:00:04.008316
elapsed time: 0:10:49.023955
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 18:50:19.610559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.12
 ---- batch: 020 ----
mean loss: 292.56
train mean loss: 298.33
epoch train time: 0:00:04.003038
elapsed time: 0:10:53.028448
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 18:50:23.615094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.52
 ---- batch: 020 ----
mean loss: 289.87
train mean loss: 295.45
epoch train time: 0:00:04.006554
elapsed time: 0:10:57.036293
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 18:50:27.622875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.41
 ---- batch: 020 ----
mean loss: 301.55
train mean loss: 292.56
epoch train time: 0:00:04.005481
elapsed time: 0:11:01.042996
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 18:50:31.629570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.90
 ---- batch: 020 ----
mean loss: 295.91
train mean loss: 297.30
epoch train time: 0:00:03.999568
elapsed time: 0:11:05.043830
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 18:50:35.630419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.80
 ---- batch: 020 ----
mean loss: 281.83
train mean loss: 294.00
epoch train time: 0:00:04.001742
elapsed time: 0:11:09.046945
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 18:50:39.633542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.21
 ---- batch: 020 ----
mean loss: 291.61
train mean loss: 297.54
epoch train time: 0:00:03.996803
elapsed time: 0:11:13.045071
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 18:50:43.631721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.06
 ---- batch: 020 ----
mean loss: 291.73
train mean loss: 291.49
epoch train time: 0:00:03.996892
elapsed time: 0:11:17.043393
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 18:50:47.630064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.59
 ---- batch: 020 ----
mean loss: 299.68
train mean loss: 290.79
epoch train time: 0:00:04.000146
elapsed time: 0:11:21.044843
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 18:50:51.631419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.75
 ---- batch: 020 ----
mean loss: 285.51
train mean loss: 291.09
epoch train time: 0:00:04.010380
elapsed time: 0:11:25.056670
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 18:50:55.643282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.07
 ---- batch: 020 ----
mean loss: 290.81
train mean loss: 293.16
epoch train time: 0:00:04.021027
elapsed time: 0:11:29.078962
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 18:50:59.665540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.96
 ---- batch: 020 ----
mean loss: 279.21
train mean loss: 287.41
epoch train time: 0:00:04.007954
elapsed time: 0:11:33.088173
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 18:51:03.674749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.43
 ---- batch: 020 ----
mean loss: 285.72
train mean loss: 286.91
epoch train time: 0:00:04.005434
elapsed time: 0:11:37.094889
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 18:51:07.681493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.15
 ---- batch: 020 ----
mean loss: 285.58
train mean loss: 282.89
epoch train time: 0:00:04.004475
elapsed time: 0:11:41.100815
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 18:51:11.687441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.01
 ---- batch: 020 ----
mean loss: 288.02
train mean loss: 285.85
epoch train time: 0:00:03.977754
elapsed time: 0:11:45.080068
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 18:51:15.666478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.74
 ---- batch: 020 ----
mean loss: 290.95
train mean loss: 281.53
epoch train time: 0:00:03.978606
elapsed time: 0:11:49.059789
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 18:51:19.646369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.36
 ---- batch: 020 ----
mean loss: 284.93
train mean loss: 285.20
epoch train time: 0:00:03.991142
elapsed time: 0:11:53.052180
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 18:51:23.638778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.45
 ---- batch: 020 ----
mean loss: 289.62
train mean loss: 287.12
epoch train time: 0:00:04.001906
elapsed time: 0:11:57.055528
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 18:51:27.642132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.30
 ---- batch: 020 ----
mean loss: 282.88
train mean loss: 283.57
epoch train time: 0:00:04.034250
elapsed time: 0:12:01.091053
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 18:51:31.677692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.48
 ---- batch: 020 ----
mean loss: 281.74
train mean loss: 280.35
epoch train time: 0:00:04.018186
elapsed time: 0:12:05.110610
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 18:51:35.697203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.62
 ---- batch: 020 ----
mean loss: 268.51
train mean loss: 279.82
epoch train time: 0:00:03.995251
elapsed time: 0:12:09.107371
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 18:51:39.693943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.13
 ---- batch: 020 ----
mean loss: 274.82
train mean loss: 276.64
epoch train time: 0:00:04.002847
elapsed time: 0:12:13.111606
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 18:51:43.698194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.37
 ---- batch: 020 ----
mean loss: 287.62
train mean loss: 277.71
epoch train time: 0:00:04.020282
elapsed time: 0:12:17.133182
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 18:51:47.719763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.77
 ---- batch: 020 ----
mean loss: 278.91
train mean loss: 275.34
epoch train time: 0:00:03.987723
elapsed time: 0:12:21.122153
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 18:51:51.708744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.01
 ---- batch: 020 ----
mean loss: 275.68
train mean loss: 279.08
epoch train time: 0:00:04.001335
elapsed time: 0:12:25.124702
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 18:51:55.711295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.77
 ---- batch: 020 ----
mean loss: 284.06
train mean loss: 275.09
epoch train time: 0:00:04.069961
elapsed time: 0:12:29.196735
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 18:51:59.783363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.94
 ---- batch: 020 ----
mean loss: 277.67
train mean loss: 273.67
epoch train time: 0:00:04.063655
elapsed time: 0:12:33.261883
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 18:52:03.848491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.24
 ---- batch: 020 ----
mean loss: 268.35
train mean loss: 272.30
epoch train time: 0:00:04.020441
elapsed time: 0:12:37.283596
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 18:52:07.870171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.80
 ---- batch: 020 ----
mean loss: 277.61
train mean loss: 270.79
epoch train time: 0:00:03.989115
elapsed time: 0:12:41.273947
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 18:52:11.860579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.23
 ---- batch: 020 ----
mean loss: 278.50
train mean loss: 271.89
epoch train time: 0:00:03.988406
elapsed time: 0:12:45.263991
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 18:52:15.850597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.90
 ---- batch: 020 ----
mean loss: 267.24
train mean loss: 271.18
epoch train time: 0:00:03.969460
elapsed time: 0:12:49.234698
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 18:52:19.821267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.19
 ---- batch: 020 ----
mean loss: 268.56
train mean loss: 265.37
epoch train time: 0:00:03.980831
elapsed time: 0:12:53.216807
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 18:52:23.803377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.59
 ---- batch: 020 ----
mean loss: 266.60
train mean loss: 266.28
epoch train time: 0:00:04.013676
elapsed time: 0:12:57.231730
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 18:52:27.818314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.25
 ---- batch: 020 ----
mean loss: 265.61
train mean loss: 266.36
epoch train time: 0:00:04.001275
elapsed time: 0:13:01.234356
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 18:52:31.820944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.51
 ---- batch: 020 ----
mean loss: 258.12
train mean loss: 263.53
epoch train time: 0:00:03.988908
elapsed time: 0:13:05.224622
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 18:52:35.811197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.61
 ---- batch: 020 ----
mean loss: 263.33
train mean loss: 263.94
epoch train time: 0:00:03.980183
elapsed time: 0:13:09.206066
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 18:52:39.792678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.26
 ---- batch: 020 ----
mean loss: 266.23
train mean loss: 266.82
epoch train time: 0:00:03.987378
elapsed time: 0:13:13.194775
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 18:52:43.781378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.05
 ---- batch: 020 ----
mean loss: 274.08
train mean loss: 270.17
epoch train time: 0:00:03.984104
elapsed time: 0:13:17.180285
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 18:52:47.766893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.57
 ---- batch: 020 ----
mean loss: 267.26
train mean loss: 265.24
epoch train time: 0:00:03.991474
elapsed time: 0:13:21.173052
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 18:52:51.759650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.03
 ---- batch: 020 ----
mean loss: 260.06
train mean loss: 261.40
epoch train time: 0:00:03.989535
elapsed time: 0:13:25.163908
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 18:52:55.750477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.18
 ---- batch: 020 ----
mean loss: 263.18
train mean loss: 264.11
epoch train time: 0:00:03.996434
elapsed time: 0:13:29.161613
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 18:52:59.748189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.55
 ---- batch: 020 ----
mean loss: 257.89
train mean loss: 262.78
epoch train time: 0:00:03.982856
elapsed time: 0:13:33.145827
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 18:53:03.732461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.40
 ---- batch: 020 ----
mean loss: 263.44
train mean loss: 258.46
epoch train time: 0:00:03.980215
elapsed time: 0:13:37.127686
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 18:53:07.714102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.03
 ---- batch: 020 ----
mean loss: 260.95
train mean loss: 256.74
epoch train time: 0:00:03.972147
elapsed time: 0:13:41.100998
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 18:53:11.687593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.43
 ---- batch: 020 ----
mean loss: 268.40
train mean loss: 258.86
epoch train time: 0:00:03.968510
elapsed time: 0:13:45.070958
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 18:53:15.657621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.41
 ---- batch: 020 ----
mean loss: 252.54
train mean loss: 257.43
epoch train time: 0:00:03.967769
elapsed time: 0:13:49.040134
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 18:53:19.626725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.44
 ---- batch: 020 ----
mean loss: 255.29
train mean loss: 257.18
epoch train time: 0:00:03.997185
elapsed time: 0:13:53.038760
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 18:53:23.625390
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.85
 ---- batch: 020 ----
mean loss: 261.17
train mean loss: 257.67
epoch train time: 0:00:03.998749
elapsed time: 0:13:57.038806
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 18:53:27.625390
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.60
 ---- batch: 020 ----
mean loss: 255.56
train mean loss: 261.28
epoch train time: 0:00:04.010976
elapsed time: 0:14:01.051093
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 18:53:31.637704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.65
 ---- batch: 020 ----
mean loss: 266.66
train mean loss: 260.18
epoch train time: 0:00:03.978963
elapsed time: 0:14:05.031430
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 18:53:35.618034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.34
 ---- batch: 020 ----
mean loss: 253.42
train mean loss: 259.74
epoch train time: 0:00:03.977711
elapsed time: 0:14:09.010467
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 18:53:39.597043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.32
 ---- batch: 020 ----
mean loss: 262.11
train mean loss: 258.25
epoch train time: 0:00:03.989646
elapsed time: 0:14:13.001479
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 18:53:43.588058
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.36
 ---- batch: 020 ----
mean loss: 257.13
train mean loss: 258.22
epoch train time: 0:00:03.975472
elapsed time: 0:14:16.978167
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 18:53:47.564767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.79
 ---- batch: 020 ----
mean loss: 259.62
train mean loss: 259.74
epoch train time: 0:00:03.998762
elapsed time: 0:14:20.978271
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 18:53:51.564870
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.01
 ---- batch: 020 ----
mean loss: 255.65
train mean loss: 259.08
epoch train time: 0:00:03.980162
elapsed time: 0:14:24.959683
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 18:53:55.546273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.98
 ---- batch: 020 ----
mean loss: 256.17
train mean loss: 255.96
epoch train time: 0:00:04.013966
elapsed time: 0:14:28.975000
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 18:53:59.561587
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 268.87
 ---- batch: 020 ----
mean loss: 260.63
train mean loss: 261.23
epoch train time: 0:00:03.990638
elapsed time: 0:14:32.966886
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 18:54:03.553513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 263.07
 ---- batch: 020 ----
mean loss: 255.43
train mean loss: 256.87
epoch train time: 0:00:03.987264
elapsed time: 0:14:36.955569
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 18:54:07.542172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.12
 ---- batch: 020 ----
mean loss: 261.95
train mean loss: 255.89
epoch train time: 0:00:03.979643
elapsed time: 0:14:40.936565
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 18:54:11.523153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.71
 ---- batch: 020 ----
mean loss: 253.30
train mean loss: 253.28
epoch train time: 0:00:03.983790
elapsed time: 0:14:44.921565
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 18:54:15.508186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.05
 ---- batch: 020 ----
mean loss: 258.18
train mean loss: 256.03
epoch train time: 0:00:04.004087
elapsed time: 0:14:48.927048
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 18:54:19.513698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.59
 ---- batch: 020 ----
mean loss: 259.74
train mean loss: 257.57
epoch train time: 0:00:03.995322
elapsed time: 0:14:52.923894
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 18:54:23.510490
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.67
 ---- batch: 020 ----
mean loss: 266.34
train mean loss: 254.93
epoch train time: 0:00:03.992523
elapsed time: 0:14:56.917675
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 18:54:27.504287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.51
 ---- batch: 020 ----
mean loss: 250.99
train mean loss: 257.28
epoch train time: 0:00:03.991091
elapsed time: 0:15:00.910068
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 18:54:31.496718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.99
 ---- batch: 020 ----
mean loss: 256.04
train mean loss: 255.06
epoch train time: 0:00:03.989189
elapsed time: 0:15:04.900591
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 18:54:35.487244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.75
 ---- batch: 020 ----
mean loss: 259.72
train mean loss: 260.58
epoch train time: 0:00:03.995162
elapsed time: 0:15:08.897111
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 18:54:39.483700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.10
 ---- batch: 020 ----
mean loss: 257.32
train mean loss: 255.36
epoch train time: 0:00:03.991888
elapsed time: 0:15:12.890276
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 18:54:43.476931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.93
 ---- batch: 020 ----
mean loss: 255.47
train mean loss: 253.63
epoch train time: 0:00:04.000763
elapsed time: 0:15:16.892380
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 18:54:47.478964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.31
 ---- batch: 020 ----
mean loss: 255.63
train mean loss: 255.47
epoch train time: 0:00:03.995030
elapsed time: 0:15:20.888820
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 18:54:51.475397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.37
 ---- batch: 020 ----
mean loss: 258.65
train mean loss: 256.03
epoch train time: 0:00:03.992724
elapsed time: 0:15:24.882777
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 18:54:55.469361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.00
 ---- batch: 020 ----
mean loss: 261.43
train mean loss: 257.11
epoch train time: 0:00:03.992986
elapsed time: 0:15:28.877068
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 18:54:59.463638
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.72
 ---- batch: 020 ----
mean loss: 255.58
train mean loss: 254.35
epoch train time: 0:00:03.982572
elapsed time: 0:15:32.860899
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 18:55:03.447495
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.27
 ---- batch: 020 ----
mean loss: 253.27
train mean loss: 256.91
epoch train time: 0:00:03.976970
elapsed time: 0:15:36.839352
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 18:55:07.425935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.48
 ---- batch: 020 ----
mean loss: 252.38
train mean loss: 257.74
epoch train time: 0:00:03.984807
elapsed time: 0:15:40.825565
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 18:55:11.412227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 245.85
 ---- batch: 020 ----
mean loss: 257.28
train mean loss: 254.90
epoch train time: 0:00:04.009468
elapsed time: 0:15:44.836674
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 18:55:15.423089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.02
 ---- batch: 020 ----
mean loss: 260.61
train mean loss: 255.42
epoch train time: 0:00:04.025645
elapsed time: 0:15:48.863504
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 18:55:19.450164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.59
 ---- batch: 020 ----
mean loss: 263.71
train mean loss: 254.70
epoch train time: 0:00:04.000313
elapsed time: 0:15:52.865150
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 18:55:23.451725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 266.54
 ---- batch: 020 ----
mean loss: 252.90
train mean loss: 257.19
epoch train time: 0:00:04.006912
elapsed time: 0:15:56.873393
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 18:55:27.460046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.14
 ---- batch: 020 ----
mean loss: 261.81
train mean loss: 257.50
epoch train time: 0:00:04.035392
elapsed time: 0:16:00.910313
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 18:55:31.496968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.20
 ---- batch: 020 ----
mean loss: 255.06
train mean loss: 256.36
epoch train time: 0:00:04.002731
elapsed time: 0:16:04.914454
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 18:55:35.501033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 243.39
 ---- batch: 020 ----
mean loss: 261.21
train mean loss: 255.49
epoch train time: 0:00:03.993490
elapsed time: 0:16:08.909379
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 18:55:39.495966
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 260.73
 ---- batch: 020 ----
mean loss: 251.56
train mean loss: 255.27
epoch train time: 0:00:03.990844
elapsed time: 0:16:12.901570
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 18:55:43.488170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.52
 ---- batch: 020 ----
mean loss: 264.43
train mean loss: 257.96
epoch train time: 0:00:04.008500
elapsed time: 0:16:16.911318
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 18:55:47.497937
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.02
 ---- batch: 020 ----
mean loss: 248.11
train mean loss: 255.98
epoch train time: 0:00:03.986939
elapsed time: 0:16:20.899639
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 18:55:51.486220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 261.80
 ---- batch: 020 ----
mean loss: 256.72
train mean loss: 256.82
epoch train time: 0:00:04.014014
elapsed time: 0:16:24.914896
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 18:55:55.501513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.61
 ---- batch: 020 ----
mean loss: 252.32
train mean loss: 255.78
epoch train time: 0:00:04.022773
elapsed time: 0:16:28.939463
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 18:55:59.526063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.97
 ---- batch: 020 ----
mean loss: 254.57
train mean loss: 255.89
epoch train time: 0:00:03.993437
elapsed time: 0:16:32.934183
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 18:56:03.520764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.40
 ---- batch: 020 ----
mean loss: 245.69
train mean loss: 254.08
epoch train time: 0:00:04.002022
elapsed time: 0:16:36.937669
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 18:56:07.524260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.53
 ---- batch: 020 ----
mean loss: 246.92
train mean loss: 254.70
epoch train time: 0:00:03.998833
elapsed time: 0:16:40.937773
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 18:56:11.524364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.89
 ---- batch: 020 ----
mean loss: 250.21
train mean loss: 253.00
epoch train time: 0:00:03.993893
elapsed time: 0:16:44.932991
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 18:56:15.519580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.12
 ---- batch: 020 ----
mean loss: 258.55
train mean loss: 252.65
epoch train time: 0:00:04.021753
elapsed time: 0:16:48.964734
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_9/checkpoint.pth.tar
**** end time: 2019-09-26 18:56:19.551104 ****
