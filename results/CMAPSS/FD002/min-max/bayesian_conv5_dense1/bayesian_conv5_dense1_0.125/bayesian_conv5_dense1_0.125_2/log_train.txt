Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.125/bayesian_conv5_dense1_0.125_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.125, resume=False, step_size=200, visualize_step=50)
pid: 17054
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 16:08:26.394904 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 16:08:26.412593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1686.12
train mean loss: 1662.17
epoch train time: 0:00:05.998505
elapsed time: 0:00:06.024397
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 16:08:32.419359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1274.02
train mean loss: 1273.03
epoch train time: 0:00:02.134009
elapsed time: 0:00:08.158658
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 16:08:34.553696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1202.24
train mean loss: 1200.47
epoch train time: 0:00:02.136450
elapsed time: 0:00:10.295497
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 16:08:36.690484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1139.59
train mean loss: 1132.94
epoch train time: 0:00:02.130753
elapsed time: 0:00:12.426533
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 16:08:38.821509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1094.82
train mean loss: 1098.58
epoch train time: 0:00:02.125776
elapsed time: 0:00:14.552592
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 16:08:40.947572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1069.28
train mean loss: 1063.06
epoch train time: 0:00:02.126509
elapsed time: 0:00:16.679367
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 16:08:43.074347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1070.71
train mean loss: 1058.17
epoch train time: 0:00:02.136597
elapsed time: 0:00:18.816239
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 16:08:45.211219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1068.68
train mean loss: 1060.52
epoch train time: 0:00:02.132903
elapsed time: 0:00:20.949412
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 16:08:47.344395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1038.42
train mean loss: 1033.58
epoch train time: 0:00:02.131582
elapsed time: 0:00:23.081311
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 16:08:49.476294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1021.48
train mean loss: 1028.43
epoch train time: 0:00:02.134699
elapsed time: 0:00:25.216277
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 16:08:51.611272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1008.68
train mean loss: 1012.71
epoch train time: 0:00:02.127236
elapsed time: 0:00:27.343833
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 16:08:53.738840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1012.82
train mean loss: 1014.44
epoch train time: 0:00:02.094482
elapsed time: 0:00:29.438678
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 16:08:55.833693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 996.04
train mean loss: 986.56
epoch train time: 0:00:02.130469
elapsed time: 0:00:31.569455
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 16:08:57.964450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.19
train mean loss: 991.45
epoch train time: 0:00:02.127602
elapsed time: 0:00:33.697343
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 16:09:00.092328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 992.24
train mean loss: 999.15
epoch train time: 0:00:02.135555
elapsed time: 0:00:35.833193
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 16:09:02.228174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 993.31
train mean loss: 987.22
epoch train time: 0:00:02.124160
elapsed time: 0:00:37.957672
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 16:09:04.352689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 976.76
train mean loss: 977.96
epoch train time: 0:00:02.134141
elapsed time: 0:00:40.092118
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 16:09:06.487112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.02
train mean loss: 973.55
epoch train time: 0:00:02.145879
elapsed time: 0:00:42.238302
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 16:09:08.633286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 984.51
train mean loss: 975.62
epoch train time: 0:00:02.140480
elapsed time: 0:00:44.379090
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 16:09:10.774067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.17
train mean loss: 984.90
epoch train time: 0:00:02.137555
elapsed time: 0:00:46.516958
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 16:09:12.911947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 970.69
train mean loss: 972.31
epoch train time: 0:00:02.150247
elapsed time: 0:00:48.667508
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 16:09:15.062488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.66
train mean loss: 986.02
epoch train time: 0:00:02.125020
elapsed time: 0:00:50.792833
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 16:09:17.187839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.42
train mean loss: 959.72
epoch train time: 0:00:02.150677
elapsed time: 0:00:52.943819
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 16:09:19.338804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 967.33
train mean loss: 972.60
epoch train time: 0:00:02.142548
elapsed time: 0:00:55.086655
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 16:09:21.481655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.41
train mean loss: 977.03
epoch train time: 0:00:02.138695
elapsed time: 0:00:57.225693
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 16:09:23.620706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 965.12
train mean loss: 962.73
epoch train time: 0:00:02.134002
elapsed time: 0:00:59.360001
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 16:09:25.754978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.97
train mean loss: 953.48
epoch train time: 0:00:02.135216
elapsed time: 0:01:01.495483
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 16:09:27.890463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 962.34
train mean loss: 961.88
epoch train time: 0:00:02.141972
elapsed time: 0:01:03.637740
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 16:09:30.032718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.71
train mean loss: 971.80
epoch train time: 0:00:02.137476
elapsed time: 0:01:05.775464
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 16:09:32.170459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 950.38
train mean loss: 957.87
epoch train time: 0:00:02.140775
elapsed time: 0:01:07.916521
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 16:09:34.311500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.03
train mean loss: 965.10
epoch train time: 0:00:02.133706
elapsed time: 0:01:10.050494
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 16:09:36.445485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.15
train mean loss: 955.20
epoch train time: 0:00:02.128302
elapsed time: 0:01:12.179085
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 16:09:38.574072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.64
train mean loss: 949.21
epoch train time: 0:00:02.138882
elapsed time: 0:01:14.318257
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 16:09:40.713237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 971.97
train mean loss: 967.25
epoch train time: 0:00:02.129297
elapsed time: 0:01:16.447838
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 16:09:42.842815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.31
train mean loss: 966.09
epoch train time: 0:00:02.133860
elapsed time: 0:01:18.581976
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 16:09:44.976953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.29
train mean loss: 955.84
epoch train time: 0:00:02.162327
elapsed time: 0:01:20.744656
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 16:09:47.139710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.03
train mean loss: 956.13
epoch train time: 0:00:02.141471
elapsed time: 0:01:22.886459
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 16:09:49.281435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.44
train mean loss: 951.65
epoch train time: 0:00:02.136980
elapsed time: 0:01:25.023718
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 16:09:51.418699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.33
train mean loss: 953.64
epoch train time: 0:00:02.133945
elapsed time: 0:01:27.157938
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 16:09:53.552915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.05
train mean loss: 944.16
epoch train time: 0:00:02.132970
elapsed time: 0:01:29.291187
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 16:09:55.686169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.15
train mean loss: 934.80
epoch train time: 0:00:02.138501
elapsed time: 0:01:31.430001
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 16:09:57.824997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.34
train mean loss: 944.68
epoch train time: 0:00:02.132488
elapsed time: 0:01:33.562884
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 16:09:59.957876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.19
train mean loss: 957.75
epoch train time: 0:00:02.141124
elapsed time: 0:01:35.704295
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 16:10:02.099269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.08
train mean loss: 945.76
epoch train time: 0:00:02.129754
elapsed time: 0:01:37.834309
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 16:10:04.229312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.50
train mean loss: 938.45
epoch train time: 0:00:02.128440
elapsed time: 0:01:39.963040
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 16:10:06.358018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.70
train mean loss: 948.51
epoch train time: 0:00:02.128052
elapsed time: 0:01:42.091378
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 16:10:08.486379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.67
train mean loss: 946.58
epoch train time: 0:00:02.128173
elapsed time: 0:01:44.219865
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 16:10:10.614862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.46
train mean loss: 942.78
epoch train time: 0:00:02.140005
elapsed time: 0:01:46.360180
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 16:10:12.755174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.61
train mean loss: 950.34
epoch train time: 0:00:02.123153
elapsed time: 0:01:48.483630
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 16:10:14.878608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.54
train mean loss: 943.42
epoch train time: 0:00:02.139714
elapsed time: 0:01:50.623640
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 16:10:17.018622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.62
train mean loss: 931.34
epoch train time: 0:00:02.130642
elapsed time: 0:01:52.754551
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 16:10:19.149544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.39
train mean loss: 937.85
epoch train time: 0:00:02.122756
elapsed time: 0:01:54.877620
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 16:10:21.272649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.79
train mean loss: 939.90
epoch train time: 0:00:02.140081
elapsed time: 0:01:57.018060
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 16:10:23.413040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.15
train mean loss: 940.77
epoch train time: 0:00:02.135041
elapsed time: 0:01:59.153352
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 16:10:25.548328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.18
train mean loss: 944.13
epoch train time: 0:00:02.134673
elapsed time: 0:02:01.288305
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 16:10:27.683282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.14
train mean loss: 936.16
epoch train time: 0:00:02.133208
elapsed time: 0:02:03.421831
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 16:10:29.816822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.60
train mean loss: 936.62
epoch train time: 0:00:02.137277
elapsed time: 0:02:05.559392
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 16:10:31.954375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.99
train mean loss: 940.64
epoch train time: 0:00:02.128288
elapsed time: 0:02:07.687962
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 16:10:34.082940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.12
train mean loss: 938.06
epoch train time: 0:00:02.129659
elapsed time: 0:02:09.817914
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 16:10:36.212907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.19
train mean loss: 931.37
epoch train time: 0:00:02.137471
elapsed time: 0:02:11.955660
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 16:10:38.350641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.07
train mean loss: 938.64
epoch train time: 0:00:02.139260
elapsed time: 0:02:14.095217
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 16:10:40.490209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.74
train mean loss: 933.99
epoch train time: 0:00:02.139668
elapsed time: 0:02:16.235181
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 16:10:42.630162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.08
train mean loss: 931.22
epoch train time: 0:00:02.136901
elapsed time: 0:02:18.372358
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 16:10:44.767338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.97
train mean loss: 930.82
epoch train time: 0:00:02.131579
elapsed time: 0:02:20.504196
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 16:10:46.899170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.87
train mean loss: 930.54
epoch train time: 0:00:02.131135
elapsed time: 0:02:22.635636
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 16:10:49.030642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.43
train mean loss: 917.11
epoch train time: 0:00:02.135090
elapsed time: 0:02:24.771026
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 16:10:51.166039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.16
train mean loss: 932.31
epoch train time: 0:00:02.136845
elapsed time: 0:02:26.908233
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 16:10:53.303214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.82
train mean loss: 932.13
epoch train time: 0:00:02.125770
elapsed time: 0:02:29.034270
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 16:10:55.429259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.65
train mean loss: 930.47
epoch train time: 0:00:02.115672
elapsed time: 0:02:31.150258
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 16:10:57.545241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.62
train mean loss: 923.48
epoch train time: 0:00:02.108233
elapsed time: 0:02:33.258781
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 16:10:59.653761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.08
train mean loss: 931.80
epoch train time: 0:00:02.113512
elapsed time: 0:02:35.372595
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 16:11:01.767578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.76
train mean loss: 935.91
epoch train time: 0:00:02.120519
elapsed time: 0:02:37.493403
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 16:11:03.888384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.86
train mean loss: 922.48
epoch train time: 0:00:02.114695
elapsed time: 0:02:39.608379
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 16:11:06.003363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.98
train mean loss: 929.78
epoch train time: 0:00:02.122948
elapsed time: 0:02:41.731667
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 16:11:08.126647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.12
train mean loss: 923.85
epoch train time: 0:00:02.118555
elapsed time: 0:02:43.850513
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 16:11:10.245491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.87
train mean loss: 929.02
epoch train time: 0:00:02.121072
elapsed time: 0:02:45.971906
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 16:11:12.366893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.17
train mean loss: 926.95
epoch train time: 0:00:02.123825
elapsed time: 0:02:48.096037
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 16:11:14.491014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.46
train mean loss: 926.23
epoch train time: 0:00:02.121980
elapsed time: 0:02:50.218270
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 16:11:16.613245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.21
train mean loss: 929.57
epoch train time: 0:00:02.140440
elapsed time: 0:02:52.358977
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 16:11:18.753954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.25
train mean loss: 923.13
epoch train time: 0:00:02.130590
elapsed time: 0:02:54.489824
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 16:11:20.884802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.73
train mean loss: 923.87
epoch train time: 0:00:02.137034
elapsed time: 0:02:56.627124
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 16:11:23.022115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.62
train mean loss: 924.14
epoch train time: 0:00:02.122879
elapsed time: 0:02:58.750289
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 16:11:25.145274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.39
train mean loss: 913.97
epoch train time: 0:00:02.122644
elapsed time: 0:03:00.873199
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 16:11:27.268232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.41
train mean loss: 922.82
epoch train time: 0:00:02.117441
elapsed time: 0:03:02.991003
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 16:11:29.386008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.32
train mean loss: 922.21
epoch train time: 0:00:02.113818
elapsed time: 0:03:05.105118
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 16:11:31.500095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.03
train mean loss: 922.38
epoch train time: 0:00:02.130036
elapsed time: 0:03:07.235446
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 16:11:33.630438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.17
train mean loss: 921.38
epoch train time: 0:00:02.121062
elapsed time: 0:03:09.356794
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 16:11:35.751805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 930.49
train mean loss: 918.69
epoch train time: 0:00:02.121729
elapsed time: 0:03:11.478865
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 16:11:37.873886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.10
train mean loss: 917.77
epoch train time: 0:00:02.119952
elapsed time: 0:03:13.599192
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 16:11:39.994172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.20
train mean loss: 917.78
epoch train time: 0:00:02.127323
elapsed time: 0:03:15.726825
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 16:11:42.121859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.94
train mean loss: 921.26
epoch train time: 0:00:02.121800
elapsed time: 0:03:17.848987
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 16:11:44.243997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.78
train mean loss: 927.00
epoch train time: 0:00:02.114638
elapsed time: 0:03:19.963920
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 16:11:46.358893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.58
train mean loss: 910.79
epoch train time: 0:00:02.115312
elapsed time: 0:03:22.079568
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 16:11:48.474552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.74
train mean loss: 914.77
epoch train time: 0:00:02.123664
elapsed time: 0:03:24.203523
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 16:11:50.598501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.66
train mean loss: 907.11
epoch train time: 0:00:02.121604
elapsed time: 0:03:26.325417
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 16:11:52.720395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.89
train mean loss: 914.16
epoch train time: 0:00:02.125485
elapsed time: 0:03:28.451183
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 16:11:54.846163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.50
train mean loss: 909.06
epoch train time: 0:00:02.122216
elapsed time: 0:03:30.573697
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 16:11:56.968679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.58
train mean loss: 917.64
epoch train time: 0:00:02.125148
elapsed time: 0:03:32.699153
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 16:11:59.094131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.39
train mean loss: 914.33
epoch train time: 0:00:02.128687
elapsed time: 0:03:34.828166
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 16:12:01.223153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.40
train mean loss: 916.01
epoch train time: 0:00:02.123510
elapsed time: 0:03:36.951954
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 16:12:03.346961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.76
train mean loss: 909.30
epoch train time: 0:00:02.120270
elapsed time: 0:03:39.072540
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 16:12:05.467544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.21
train mean loss: 908.33
epoch train time: 0:00:02.111784
elapsed time: 0:03:41.184617
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 16:12:07.579617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.78
train mean loss: 911.82
epoch train time: 0:00:02.143162
elapsed time: 0:03:43.328070
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 16:12:09.723048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.53
train mean loss: 907.79
epoch train time: 0:00:02.146326
elapsed time: 0:03:45.474691
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 16:12:11.869687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.16
train mean loss: 894.00
epoch train time: 0:00:02.148065
elapsed time: 0:03:47.623065
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 16:12:14.018084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.81
train mean loss: 900.52
epoch train time: 0:00:02.150194
elapsed time: 0:03:49.773572
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 16:12:16.168555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.01
train mean loss: 900.97
epoch train time: 0:00:02.127932
elapsed time: 0:03:51.901801
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 16:12:18.296773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.91
train mean loss: 911.46
epoch train time: 0:00:02.121816
elapsed time: 0:03:54.023872
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 16:12:20.418852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 897.72
train mean loss: 901.79
epoch train time: 0:00:02.114869
elapsed time: 0:03:56.139067
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 16:12:22.534058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.39
train mean loss: 885.82
epoch train time: 0:00:02.116826
elapsed time: 0:03:58.256178
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 16:12:24.651158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.32
train mean loss: 890.42
epoch train time: 0:00:02.117480
elapsed time: 0:04:00.373943
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 16:12:26.768922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.87
train mean loss: 896.51
epoch train time: 0:00:02.115605
elapsed time: 0:04:02.489833
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 16:12:28.884818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.19
train mean loss: 890.96
epoch train time: 0:00:02.121289
elapsed time: 0:04:04.611456
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 16:12:31.006437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.02
train mean loss: 882.12
epoch train time: 0:00:02.128279
elapsed time: 0:04:06.740012
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 16:12:33.134996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.92
train mean loss: 884.23
epoch train time: 0:00:02.111481
elapsed time: 0:04:08.851769
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 16:12:35.246744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.23
train mean loss: 870.75
epoch train time: 0:00:02.111501
elapsed time: 0:04:10.963558
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 16:12:37.358562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.92
train mean loss: 862.47
epoch train time: 0:00:02.115681
elapsed time: 0:04:13.079545
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 16:12:39.474528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.76
train mean loss: 854.37
epoch train time: 0:00:02.116731
elapsed time: 0:04:15.196562
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 16:12:41.591540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.27
train mean loss: 845.48
epoch train time: 0:00:02.114982
elapsed time: 0:04:17.311842
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 16:12:43.706822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.76
train mean loss: 844.42
epoch train time: 0:00:02.115508
elapsed time: 0:04:19.427623
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 16:12:45.822621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 804.25
train mean loss: 811.85
epoch train time: 0:00:02.115606
elapsed time: 0:04:21.543508
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 16:12:47.938481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 791.99
train mean loss: 786.18
epoch train time: 0:00:02.108655
elapsed time: 0:04:23.652438
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 16:12:50.047433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 783.14
train mean loss: 776.89
epoch train time: 0:00:02.122595
elapsed time: 0:04:25.775348
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 16:12:52.170331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.43
train mean loss: 744.34
epoch train time: 0:00:02.113911
elapsed time: 0:04:27.889526
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 16:12:54.284504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 729.60
train mean loss: 736.01
epoch train time: 0:00:02.119525
elapsed time: 0:04:30.009322
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 16:12:56.404317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.75
train mean loss: 720.34
epoch train time: 0:00:02.112224
elapsed time: 0:04:32.121841
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 16:12:58.516828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.75
train mean loss: 706.80
epoch train time: 0:00:02.115746
elapsed time: 0:04:34.237901
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 16:13:00.632848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.21
train mean loss: 705.44
epoch train time: 0:00:02.115662
elapsed time: 0:04:36.353802
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 16:13:02.748777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.02
train mean loss: 687.19
epoch train time: 0:00:02.119269
elapsed time: 0:04:38.473356
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 16:13:04.868337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.14
train mean loss: 671.48
epoch train time: 0:00:02.126006
elapsed time: 0:04:40.599673
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 16:13:06.994653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 678.31
train mean loss: 671.13
epoch train time: 0:00:02.121176
elapsed time: 0:04:42.721124
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 16:13:09.116113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.48
train mean loss: 666.28
epoch train time: 0:00:02.116563
elapsed time: 0:04:44.837967
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 16:13:11.232974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.22
train mean loss: 669.11
epoch train time: 0:00:02.124739
elapsed time: 0:04:46.962988
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 16:13:13.357965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 648.92
train mean loss: 653.87
epoch train time: 0:00:02.120483
elapsed time: 0:04:49.083737
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 16:13:15.478719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.07
train mean loss: 656.44
epoch train time: 0:00:02.119363
elapsed time: 0:04:51.203356
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 16:13:17.598335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 639.51
train mean loss: 642.90
epoch train time: 0:00:02.124203
elapsed time: 0:04:53.327993
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 16:13:19.723003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 638.36
train mean loss: 639.96
epoch train time: 0:00:02.120864
elapsed time: 0:04:55.449167
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 16:13:21.844156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.54
train mean loss: 625.98
epoch train time: 0:00:02.117840
elapsed time: 0:04:57.567308
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 16:13:23.962317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 622.08
train mean loss: 625.84
epoch train time: 0:00:02.115573
elapsed time: 0:04:59.683193
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 16:13:26.078201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.33
train mean loss: 615.11
epoch train time: 0:00:02.115174
elapsed time: 0:05:01.798681
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 16:13:28.193686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.21
train mean loss: 620.00
epoch train time: 0:00:02.127006
elapsed time: 0:05:03.925972
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 16:13:30.320974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 620.37
train mean loss: 617.14
epoch train time: 0:00:02.123919
elapsed time: 0:05:06.050214
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 16:13:32.445189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 611.92
train mean loss: 612.15
epoch train time: 0:00:02.113830
elapsed time: 0:05:08.164313
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 16:13:34.559312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.60
train mean loss: 603.77
epoch train time: 0:00:02.117190
elapsed time: 0:05:10.281839
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 16:13:36.676819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 593.50
train mean loss: 588.98
epoch train time: 0:00:02.117795
elapsed time: 0:05:12.399941
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 16:13:38.794936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 581.75
train mean loss: 585.31
epoch train time: 0:00:02.125657
elapsed time: 0:05:14.525900
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 16:13:40.920915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.95
train mean loss: 582.13
epoch train time: 0:00:02.118491
elapsed time: 0:05:16.644731
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 16:13:43.039713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 577.22
train mean loss: 575.51
epoch train time: 0:00:02.121779
elapsed time: 0:05:18.766836
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 16:13:45.161830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.60
train mean loss: 574.19
epoch train time: 0:00:02.116395
elapsed time: 0:05:20.883560
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 16:13:47.278510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.09
train mean loss: 565.25
epoch train time: 0:00:02.114428
elapsed time: 0:05:22.998233
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 16:13:49.393214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.75
train mean loss: 557.42
epoch train time: 0:00:02.114423
elapsed time: 0:05:25.112952
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 16:13:51.507933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.73
train mean loss: 552.92
epoch train time: 0:00:02.119857
elapsed time: 0:05:27.233094
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 16:13:53.628103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 560.04
train mean loss: 551.50
epoch train time: 0:00:02.120045
elapsed time: 0:05:29.353466
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 16:13:55.748445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.96
train mean loss: 540.51
epoch train time: 0:00:02.117335
elapsed time: 0:05:31.471080
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 16:13:57.866057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.41
train mean loss: 528.05
epoch train time: 0:00:02.122001
elapsed time: 0:05:33.593365
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 16:13:59.988338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.37
train mean loss: 543.77
epoch train time: 0:00:02.120372
elapsed time: 0:05:35.714000
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 16:14:02.108995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.49
train mean loss: 528.20
epoch train time: 0:00:02.117308
elapsed time: 0:05:37.831586
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 16:14:04.226593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.88
train mean loss: 522.64
epoch train time: 0:00:02.119823
elapsed time: 0:05:39.951713
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 16:14:06.346720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.89
train mean loss: 527.86
epoch train time: 0:00:02.116761
elapsed time: 0:05:42.068768
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 16:14:08.463746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.74
train mean loss: 520.79
epoch train time: 0:00:02.109767
elapsed time: 0:05:44.178821
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 16:14:10.573800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.03
train mean loss: 515.50
epoch train time: 0:00:02.121609
elapsed time: 0:05:46.300708
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 16:14:12.695686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.67
train mean loss: 502.99
epoch train time: 0:00:02.118694
elapsed time: 0:05:48.419670
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 16:14:14.814653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.07
train mean loss: 502.51
epoch train time: 0:00:02.109702
elapsed time: 0:05:50.529692
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 16:14:16.924669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.65
train mean loss: 486.39
epoch train time: 0:00:02.103571
elapsed time: 0:05:52.633539
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 16:14:19.028517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.81
train mean loss: 488.53
epoch train time: 0:00:02.112880
elapsed time: 0:05:54.746691
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 16:14:21.141698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.49
train mean loss: 485.85
epoch train time: 0:00:02.114066
elapsed time: 0:05:56.861045
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 16:14:23.256024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.89
train mean loss: 479.13
epoch train time: 0:00:02.114177
elapsed time: 0:05:58.975477
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 16:14:25.370452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.65
train mean loss: 472.37
epoch train time: 0:00:02.116144
elapsed time: 0:06:01.091871
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 16:14:27.486848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.79
train mean loss: 466.79
epoch train time: 0:00:02.108961
elapsed time: 0:06:03.201108
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 16:14:29.596088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.17
train mean loss: 463.71
epoch train time: 0:00:02.120291
elapsed time: 0:06:05.321713
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 16:14:31.716695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.13
train mean loss: 467.20
epoch train time: 0:00:02.115453
elapsed time: 0:06:07.437447
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 16:14:33.832447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.89
train mean loss: 457.88
epoch train time: 0:00:02.109068
elapsed time: 0:06:09.546820
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 16:14:35.941796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.17
train mean loss: 459.70
epoch train time: 0:00:02.108154
elapsed time: 0:06:11.655247
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 16:14:38.050236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.71
train mean loss: 450.37
epoch train time: 0:00:02.111986
elapsed time: 0:06:13.767580
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 16:14:40.162530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.90
train mean loss: 448.23
epoch train time: 0:00:02.120918
elapsed time: 0:06:15.888753
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 16:14:42.283746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.05
train mean loss: 451.64
epoch train time: 0:00:02.109376
elapsed time: 0:06:17.998410
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 16:14:44.393407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.05
train mean loss: 446.58
epoch train time: 0:00:02.110550
elapsed time: 0:06:20.109254
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 16:14:46.504247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.43
train mean loss: 449.61
epoch train time: 0:00:02.110602
elapsed time: 0:06:22.220158
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 16:14:48.615153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.36
train mean loss: 434.43
epoch train time: 0:00:02.117935
elapsed time: 0:06:24.338388
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 16:14:50.733365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.15
train mean loss: 435.76
epoch train time: 0:00:02.136056
elapsed time: 0:06:26.474736
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 16:14:52.869720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.73
train mean loss: 430.14
epoch train time: 0:00:02.127273
elapsed time: 0:06:28.602330
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 16:14:54.997311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.55
train mean loss: 438.87
epoch train time: 0:00:02.113062
elapsed time: 0:06:30.715659
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 16:14:57.110635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.64
train mean loss: 436.24
epoch train time: 0:00:02.108012
elapsed time: 0:06:32.823971
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 16:14:59.218953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.03
train mean loss: 432.51
epoch train time: 0:00:02.120661
elapsed time: 0:06:34.944888
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 16:15:01.339868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.50
train mean loss: 429.51
epoch train time: 0:00:02.104192
elapsed time: 0:06:37.049327
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 16:15:03.444305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.27
train mean loss: 429.05
epoch train time: 0:00:02.110989
elapsed time: 0:06:39.160579
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 16:15:05.555559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.70
train mean loss: 414.25
epoch train time: 0:00:02.117791
elapsed time: 0:06:41.278652
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 16:15:07.673652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.81
train mean loss: 419.60
epoch train time: 0:00:02.116546
elapsed time: 0:06:43.395495
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 16:15:09.790479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.56
train mean loss: 421.92
epoch train time: 0:00:02.117113
elapsed time: 0:06:45.512904
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 16:15:11.907890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.96
train mean loss: 410.58
epoch train time: 0:00:02.115151
elapsed time: 0:06:47.628355
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 16:15:14.023341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.66
train mean loss: 410.60
epoch train time: 0:00:02.128228
elapsed time: 0:06:49.756878
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 16:15:16.151858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.07
train mean loss: 409.04
epoch train time: 0:00:02.111408
elapsed time: 0:06:51.868578
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 16:15:18.263561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.05
train mean loss: 408.54
epoch train time: 0:00:02.116334
elapsed time: 0:06:53.985200
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 16:15:20.380206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.68
train mean loss: 415.47
epoch train time: 0:00:02.119244
elapsed time: 0:06:56.104764
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 16:15:22.499770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.41
train mean loss: 416.49
epoch train time: 0:00:02.115259
elapsed time: 0:06:58.220334
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 16:15:24.615315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.48
train mean loss: 409.12
epoch train time: 0:00:02.117799
elapsed time: 0:07:00.338400
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 16:15:26.733378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.76
train mean loss: 408.07
epoch train time: 0:00:02.123057
elapsed time: 0:07:02.461724
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 16:15:28.856700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.96
train mean loss: 407.41
epoch train time: 0:00:02.125980
elapsed time: 0:07:04.588018
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 16:15:30.982998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.99
train mean loss: 402.04
epoch train time: 0:00:02.119454
elapsed time: 0:07:06.707744
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 16:15:33.102723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.54
train mean loss: 401.00
epoch train time: 0:00:02.113245
elapsed time: 0:07:08.821253
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 16:15:35.216227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.99
train mean loss: 402.42
epoch train time: 0:00:02.113710
elapsed time: 0:07:10.935229
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 16:15:37.330210
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.59
train mean loss: 397.44
epoch train time: 0:00:02.126345
elapsed time: 0:07:13.061934
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 16:15:39.456913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 397.35
train mean loss: 396.79
epoch train time: 0:00:02.115123
elapsed time: 0:07:15.177349
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 16:15:41.572327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 393.61
train mean loss: 393.52
epoch train time: 0:00:02.114010
elapsed time: 0:07:17.291650
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 16:15:43.686638
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 384.33
train mean loss: 385.19
epoch train time: 0:00:02.116415
elapsed time: 0:07:19.408365
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 16:15:45.803341
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 394.49
train mean loss: 390.53
epoch train time: 0:00:02.117967
elapsed time: 0:07:21.526620
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 16:15:47.921594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 389.85
train mean loss: 389.31
epoch train time: 0:00:02.094871
elapsed time: 0:07:23.621763
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 16:15:50.016751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 397.21
train mean loss: 395.62
epoch train time: 0:00:02.122776
elapsed time: 0:07:25.744828
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 16:15:52.139811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 387.33
train mean loss: 389.55
epoch train time: 0:00:02.128861
elapsed time: 0:07:27.873964
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 16:15:54.268962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 393.23
train mean loss: 398.36
epoch train time: 0:00:02.125071
elapsed time: 0:07:29.999317
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 16:15:56.394298
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.02
train mean loss: 391.92
epoch train time: 0:00:02.119255
elapsed time: 0:07:32.118857
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 16:15:58.513835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 392.32
train mean loss: 389.27
epoch train time: 0:00:02.117692
elapsed time: 0:07:34.236881
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 16:16:00.631893
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.35
train mean loss: 394.58
epoch train time: 0:00:02.119680
elapsed time: 0:07:36.356870
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 16:16:02.751848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 403.53
train mean loss: 398.37
epoch train time: 0:00:02.111954
elapsed time: 0:07:38.469091
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 16:16:04.864069
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 392.51
train mean loss: 390.80
epoch train time: 0:00:02.108141
elapsed time: 0:07:40.577499
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 16:16:06.972470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.57
train mean loss: 395.89
epoch train time: 0:00:02.109181
elapsed time: 0:07:42.686959
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 16:16:09.081944
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 386.76
train mean loss: 384.54
epoch train time: 0:00:02.114315
elapsed time: 0:07:44.801550
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 16:16:11.196553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 387.22
train mean loss: 389.64
epoch train time: 0:00:02.112435
elapsed time: 0:07:46.914331
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 16:16:13.309309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.16
train mean loss: 391.81
epoch train time: 0:00:02.113899
elapsed time: 0:07:49.028518
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 16:16:15.423494
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.46
train mean loss: 395.69
epoch train time: 0:00:02.112989
elapsed time: 0:07:51.141785
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 16:16:17.536767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 384.11
train mean loss: 392.50
epoch train time: 0:00:02.117748
elapsed time: 0:07:53.259850
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 16:16:19.654881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 393.10
train mean loss: 392.82
epoch train time: 0:00:02.121461
elapsed time: 0:07:55.381666
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 16:16:21.776655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.98
train mean loss: 396.09
epoch train time: 0:00:02.121333
elapsed time: 0:07:57.503291
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 16:16:23.898271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 393.57
train mean loss: 391.87
epoch train time: 0:00:02.114078
elapsed time: 0:07:59.617654
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 16:16:26.012636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 388.92
train mean loss: 389.31
epoch train time: 0:00:02.112017
elapsed time: 0:08:01.729940
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 16:16:28.124944
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.46
train mean loss: 389.80
epoch train time: 0:00:02.107279
elapsed time: 0:08:03.837552
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 16:16:30.232540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 395.64
train mean loss: 393.21
epoch train time: 0:00:02.121425
elapsed time: 0:08:05.959269
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 16:16:32.354253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.53
train mean loss: 387.69
epoch train time: 0:00:02.111558
elapsed time: 0:08:08.071212
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 16:16:34.466208
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 397.04
train mean loss: 393.23
epoch train time: 0:00:02.113389
elapsed time: 0:08:10.184903
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 16:16:36.579895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.99
train mean loss: 391.62
epoch train time: 0:00:02.106825
elapsed time: 0:08:12.292021
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 16:16:38.686999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 389.13
train mean loss: 389.96
epoch train time: 0:00:02.108025
elapsed time: 0:08:14.400357
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 16:16:40.795355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 391.23
train mean loss: 391.36
epoch train time: 0:00:02.115701
elapsed time: 0:08:16.516398
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 16:16:42.911376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 382.69
train mean loss: 385.74
epoch train time: 0:00:02.116619
elapsed time: 0:08:18.633329
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 16:16:45.028319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 393.58
train mean loss: 392.71
epoch train time: 0:00:02.119331
elapsed time: 0:08:20.752982
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 16:16:47.147932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 392.77
train mean loss: 394.20
epoch train time: 0:00:02.112891
elapsed time: 0:08:22.866265
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 16:16:49.261285
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 387.04
train mean loss: 390.16
epoch train time: 0:00:02.114361
elapsed time: 0:08:24.980923
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 16:16:51.375899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 385.02
train mean loss: 383.36
epoch train time: 0:00:02.113730
elapsed time: 0:08:27.094938
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 16:16:53.489893
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 386.77
train mean loss: 384.01
epoch train time: 0:00:02.111848
elapsed time: 0:08:29.207050
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 16:16:55.602029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 388.35
train mean loss: 391.56
epoch train time: 0:00:02.108096
elapsed time: 0:08:31.315463
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 16:16:57.710458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 385.30
train mean loss: 387.77
epoch train time: 0:00:02.134721
elapsed time: 0:08:33.450492
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 16:16:59.845479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 387.79
train mean loss: 383.43
epoch train time: 0:00:02.119104
elapsed time: 0:08:35.569890
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 16:17:01.964872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 381.82
train mean loss: 383.98
epoch train time: 0:00:02.119090
elapsed time: 0:08:37.689255
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 16:17:04.084244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 387.70
train mean loss: 391.33
epoch train time: 0:00:02.110572
elapsed time: 0:08:39.800111
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 16:17:06.195108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 390.02
train mean loss: 391.97
epoch train time: 0:00:02.111411
elapsed time: 0:08:41.911813
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 16:17:08.306792
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 388.21
train mean loss: 393.04
epoch train time: 0:00:02.115245
elapsed time: 0:08:44.027332
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 16:17:10.422311
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 389.07
train mean loss: 387.13
epoch train time: 0:00:02.120176
elapsed time: 0:08:46.147833
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 16:17:12.542845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 390.30
train mean loss: 388.80
epoch train time: 0:00:02.119782
elapsed time: 0:08:48.267970
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 16:17:14.662953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 396.35
train mean loss: 393.29
epoch train time: 0:00:02.120902
elapsed time: 0:08:50.389204
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 16:17:16.784184
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 382.75
train mean loss: 384.98
epoch train time: 0:00:02.126944
elapsed time: 0:08:52.516454
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 16:17:18.911465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 393.49
train mean loss: 388.79
epoch train time: 0:00:02.143194
elapsed time: 0:08:54.669567
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.125/bayesian_conv5_dense1_0.125_2/checkpoint.pth.tar
**** end time: 2019-09-25 16:17:21.064488 ****
