Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 22508
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:23:01.332296 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:23:01.342254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4659.95
 ---- batch: 020 ----
mean loss: 4463.41
train mean loss: 4556.77
epoch train time: 0:00:07.647838
elapsed time: 0:00:07.663760
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:23:08.996091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4272.82
 ---- batch: 020 ----
mean loss: 4036.91
train mean loss: 4132.36
epoch train time: 0:00:00.352910
elapsed time: 0:00:08.016836
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:23:09.349198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3745.85
 ---- batch: 020 ----
mean loss: 3676.59
train mean loss: 3704.15
epoch train time: 0:00:00.362106
elapsed time: 0:00:08.379128
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:23:09.711484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3431.12
 ---- batch: 020 ----
mean loss: 3282.64
train mean loss: 3332.96
epoch train time: 0:00:00.363565
elapsed time: 0:00:08.742887
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:23:10.075267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3127.93
 ---- batch: 020 ----
mean loss: 3001.53
train mean loss: 3065.92
epoch train time: 0:00:00.363236
elapsed time: 0:00:09.106325
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:23:10.438681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2934.62
 ---- batch: 020 ----
mean loss: 2834.35
train mean loss: 2870.93
epoch train time: 0:00:00.356942
elapsed time: 0:00:09.463450
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:23:10.795838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2766.56
 ---- batch: 020 ----
mean loss: 2676.85
train mean loss: 2716.62
epoch train time: 0:00:00.363751
elapsed time: 0:00:09.827468
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:23:11.159871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2646.47
 ---- batch: 020 ----
mean loss: 2566.96
train mean loss: 2606.62
epoch train time: 0:00:00.352503
elapsed time: 0:00:10.180192
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:23:11.512552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2551.17
 ---- batch: 020 ----
mean loss: 2472.66
train mean loss: 2504.82
epoch train time: 0:00:00.345622
elapsed time: 0:00:10.526028
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:23:11.858386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2441.51
 ---- batch: 020 ----
mean loss: 2416.17
train mean loss: 2410.73
epoch train time: 0:00:00.346443
elapsed time: 0:00:10.872648
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:23:12.205000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2340.86
 ---- batch: 020 ----
mean loss: 2355.23
train mean loss: 2351.31
epoch train time: 0:00:00.348436
elapsed time: 0:00:11.221257
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:23:12.553629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2308.68
 ---- batch: 020 ----
mean loss: 2218.69
train mean loss: 2268.52
epoch train time: 0:00:00.355214
elapsed time: 0:00:11.576685
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:23:12.909044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2195.05
 ---- batch: 020 ----
mean loss: 2204.89
train mean loss: 2192.09
epoch train time: 0:00:00.361609
elapsed time: 0:00:11.938475
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:23:13.270832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2145.37
 ---- batch: 020 ----
mean loss: 2104.84
train mean loss: 2120.39
epoch train time: 0:00:00.356418
elapsed time: 0:00:12.295085
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:23:13.627441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2084.97
 ---- batch: 020 ----
mean loss: 2036.93
train mean loss: 2060.92
epoch train time: 0:00:00.369295
elapsed time: 0:00:12.664564
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:23:13.996920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2036.90
 ---- batch: 020 ----
mean loss: 1953.65
train mean loss: 1993.27
epoch train time: 0:00:00.351105
elapsed time: 0:00:13.015844
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:23:14.348195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1958.04
 ---- batch: 020 ----
mean loss: 1914.66
train mean loss: 1929.54
epoch train time: 0:00:00.351628
elapsed time: 0:00:13.367668
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:23:14.700069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1898.73
 ---- batch: 020 ----
mean loss: 1836.26
train mean loss: 1870.93
epoch train time: 0:00:00.364682
elapsed time: 0:00:13.733071
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:23:15.065450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1821.03
 ---- batch: 020 ----
mean loss: 1813.58
train mean loss: 1815.47
epoch train time: 0:00:00.363207
elapsed time: 0:00:14.096485
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:23:15.428864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1767.53
 ---- batch: 020 ----
mean loss: 1787.40
train mean loss: 1768.97
epoch train time: 0:00:00.376050
elapsed time: 0:00:14.472753
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:23:15.805109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1726.92
 ---- batch: 020 ----
mean loss: 1685.88
train mean loss: 1700.43
epoch train time: 0:00:00.365518
elapsed time: 0:00:14.838450
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:23:16.170809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1673.55
 ---- batch: 020 ----
mean loss: 1644.92
train mean loss: 1652.88
epoch train time: 0:00:00.358574
elapsed time: 0:00:15.197209
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:23:16.529563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1624.38
 ---- batch: 020 ----
mean loss: 1602.07
train mean loss: 1600.52
epoch train time: 0:00:00.366327
elapsed time: 0:00:15.563708
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:23:16.896061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1587.45
 ---- batch: 020 ----
mean loss: 1572.17
train mean loss: 1575.08
epoch train time: 0:00:00.350400
elapsed time: 0:00:15.914297
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:23:17.246665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1535.37
 ---- batch: 020 ----
mean loss: 1514.27
train mean loss: 1533.45
epoch train time: 0:00:00.358993
elapsed time: 0:00:16.273525
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:23:17.605884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1508.92
 ---- batch: 020 ----
mean loss: 1478.21
train mean loss: 1482.50
epoch train time: 0:00:00.363667
elapsed time: 0:00:16.637402
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:23:17.969783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1459.52
 ---- batch: 020 ----
mean loss: 1444.88
train mean loss: 1440.04
epoch train time: 0:00:00.369595
elapsed time: 0:00:17.007209
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:23:18.339571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1416.22
 ---- batch: 020 ----
mean loss: 1372.15
train mean loss: 1391.51
epoch train time: 0:00:00.355933
elapsed time: 0:00:17.363327
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:23:18.695682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1337.78
 ---- batch: 020 ----
mean loss: 1291.39
train mean loss: 1311.02
epoch train time: 0:00:00.348161
elapsed time: 0:00:17.711660
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:23:19.044015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1282.17
 ---- batch: 020 ----
mean loss: 1257.50
train mean loss: 1271.72
epoch train time: 0:00:00.340021
elapsed time: 0:00:18.051853
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:23:19.384209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1228.41
 ---- batch: 020 ----
mean loss: 1223.53
train mean loss: 1228.40
epoch train time: 0:00:00.351557
elapsed time: 0:00:18.403633
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:23:19.735986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1192.51
 ---- batch: 020 ----
mean loss: 1184.33
train mean loss: 1185.28
epoch train time: 0:00:00.350124
elapsed time: 0:00:18.754558
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:23:20.086921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1172.39
 ---- batch: 020 ----
mean loss: 1150.05
train mean loss: 1162.86
epoch train time: 0:00:00.346314
elapsed time: 0:00:19.101093
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:23:20.433450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1110.29
 ---- batch: 020 ----
mean loss: 1124.10
train mean loss: 1115.92
epoch train time: 0:00:00.347990
elapsed time: 0:00:19.449275
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:23:20.781656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1068.59
 ---- batch: 020 ----
mean loss: 1071.28
train mean loss: 1071.37
epoch train time: 0:00:00.360743
elapsed time: 0:00:19.810227
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:23:21.142583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1064.87
 ---- batch: 020 ----
mean loss: 1035.53
train mean loss: 1044.81
epoch train time: 0:00:00.352070
elapsed time: 0:00:20.162488
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:23:21.494844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1017.71
 ---- batch: 020 ----
mean loss: 999.70
train mean loss: 1009.18
epoch train time: 0:00:00.357711
elapsed time: 0:00:20.520445
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:23:21.852812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 999.40
 ---- batch: 020 ----
mean loss: 967.16
train mean loss: 980.04
epoch train time: 0:00:00.345003
elapsed time: 0:00:20.865630
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:23:22.197983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 958.39
 ---- batch: 020 ----
mean loss: 936.69
train mean loss: 950.82
epoch train time: 0:00:00.340467
elapsed time: 0:00:21.206273
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:23:22.538623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.18
 ---- batch: 020 ----
mean loss: 914.53
train mean loss: 919.29
epoch train time: 0:00:00.347972
elapsed time: 0:00:21.554411
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:23:22.886767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.57
 ---- batch: 020 ----
mean loss: 886.85
train mean loss: 893.90
epoch train time: 0:00:00.347471
elapsed time: 0:00:21.902087
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:23:23.234441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.41
 ---- batch: 020 ----
mean loss: 864.87
train mean loss: 867.82
epoch train time: 0:00:00.347415
elapsed time: 0:00:22.249677
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:23:23.582033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.87
 ---- batch: 020 ----
mean loss: 821.19
train mean loss: 834.17
epoch train time: 0:00:00.363060
elapsed time: 0:00:22.613020
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:23:23.945383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.66
 ---- batch: 020 ----
mean loss: 800.42
train mean loss: 811.26
epoch train time: 0:00:00.352037
elapsed time: 0:00:22.965244
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:23:24.297604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.40
 ---- batch: 020 ----
mean loss: 785.09
train mean loss: 790.05
epoch train time: 0:00:00.351223
elapsed time: 0:00:23.316688
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:23:24.649050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.11
 ---- batch: 020 ----
mean loss: 753.48
train mean loss: 766.01
epoch train time: 0:00:00.356312
elapsed time: 0:00:23.673192
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:23:25.005555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 744.69
 ---- batch: 020 ----
mean loss: 729.21
train mean loss: 731.70
epoch train time: 0:00:00.358200
elapsed time: 0:00:24.031581
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:23:25.363964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 722.94
 ---- batch: 020 ----
mean loss: 700.46
train mean loss: 713.06
epoch train time: 0:00:00.377388
elapsed time: 0:00:24.409178
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:23:25.741572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.70
 ---- batch: 020 ----
mean loss: 699.23
train mean loss: 697.36
epoch train time: 0:00:00.364995
elapsed time: 0:00:24.774435
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:23:26.106805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.64
 ---- batch: 020 ----
mean loss: 674.70
train mean loss: 671.95
epoch train time: 0:00:00.365853
elapsed time: 0:00:25.140481
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:23:26.472863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 659.08
 ---- batch: 020 ----
mean loss: 645.33
train mean loss: 653.23
epoch train time: 0:00:00.360871
elapsed time: 0:00:25.501575
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:23:26.833926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 641.24
 ---- batch: 020 ----
mean loss: 617.10
train mean loss: 631.27
epoch train time: 0:00:00.356781
elapsed time: 0:00:25.858555
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:23:27.190915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 620.42
 ---- batch: 020 ----
mean loss: 596.65
train mean loss: 606.60
epoch train time: 0:00:00.357635
elapsed time: 0:00:26.216417
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:23:27.548775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 599.87
 ---- batch: 020 ----
mean loss: 580.71
train mean loss: 589.00
epoch train time: 0:00:00.358149
elapsed time: 0:00:26.574745
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:23:27.907105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.47
 ---- batch: 020 ----
mean loss: 575.54
train mean loss: 578.56
epoch train time: 0:00:00.351349
elapsed time: 0:00:26.926275
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:23:28.258629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.68
 ---- batch: 020 ----
mean loss: 538.86
train mean loss: 550.99
epoch train time: 0:00:00.344586
elapsed time: 0:00:27.271079
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:23:28.603433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 551.63
 ---- batch: 020 ----
mean loss: 534.00
train mean loss: 542.78
epoch train time: 0:00:00.366119
elapsed time: 0:00:27.637370
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:23:28.969750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.53
 ---- batch: 020 ----
mean loss: 520.71
train mean loss: 522.38
epoch train time: 0:00:00.347890
elapsed time: 0:00:27.985482
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:23:29.317843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.83
 ---- batch: 020 ----
mean loss: 495.72
train mean loss: 503.39
epoch train time: 0:00:00.347664
elapsed time: 0:00:28.333327
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:23:29.665690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.15
 ---- batch: 020 ----
mean loss: 495.03
train mean loss: 486.35
epoch train time: 0:00:00.352289
elapsed time: 0:00:28.685800
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:23:30.018155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.13
 ---- batch: 020 ----
mean loss: 474.23
train mean loss: 474.99
epoch train time: 0:00:00.349478
elapsed time: 0:00:29.035466
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:23:30.367891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.12
 ---- batch: 020 ----
mean loss: 458.41
train mean loss: 463.11
epoch train time: 0:00:00.347457
elapsed time: 0:00:29.383176
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:23:30.715530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.53
 ---- batch: 020 ----
mean loss: 430.07
train mean loss: 445.38
epoch train time: 0:00:00.346980
elapsed time: 0:00:29.730329
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:23:31.062684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.87
 ---- batch: 020 ----
mean loss: 435.31
train mean loss: 432.37
epoch train time: 0:00:00.357597
elapsed time: 0:00:30.088118
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:23:31.420485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.45
 ---- batch: 020 ----
mean loss: 428.40
train mean loss: 421.97
epoch train time: 0:00:00.355866
elapsed time: 0:00:30.444179
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:23:31.776555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.79
 ---- batch: 020 ----
mean loss: 413.23
train mean loss: 410.41
epoch train time: 0:00:00.360579
elapsed time: 0:00:30.804961
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:23:32.137337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.61
 ---- batch: 020 ----
mean loss: 399.09
train mean loss: 397.49
epoch train time: 0:00:00.360492
elapsed time: 0:00:31.165655
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:23:32.498038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.66
 ---- batch: 020 ----
mean loss: 383.14
train mean loss: 390.14
epoch train time: 0:00:00.362697
elapsed time: 0:00:31.528563
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:23:32.860926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.49
 ---- batch: 020 ----
mean loss: 373.54
train mean loss: 373.66
epoch train time: 0:00:00.354369
elapsed time: 0:00:31.883119
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:23:33.215476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.72
 ---- batch: 020 ----
mean loss: 357.44
train mean loss: 363.38
epoch train time: 0:00:00.350309
elapsed time: 0:00:32.233617
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:23:33.565974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.26
 ---- batch: 020 ----
mean loss: 345.72
train mean loss: 352.21
epoch train time: 0:00:00.349702
elapsed time: 0:00:32.583521
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:23:33.915873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.60
 ---- batch: 020 ----
mean loss: 336.11
train mean loss: 341.73
epoch train time: 0:00:00.349774
elapsed time: 0:00:32.933488
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:23:34.265845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.43
 ---- batch: 020 ----
mean loss: 329.90
train mean loss: 334.04
epoch train time: 0:00:00.349468
elapsed time: 0:00:33.283159
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:23:34.615508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.02
 ---- batch: 020 ----
mean loss: 330.30
train mean loss: 328.32
epoch train time: 0:00:00.351517
elapsed time: 0:00:33.634860
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:23:34.967215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.81
 ---- batch: 020 ----
mean loss: 307.36
train mean loss: 312.07
epoch train time: 0:00:00.352335
elapsed time: 0:00:33.987369
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:23:35.319723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.34
 ---- batch: 020 ----
mean loss: 302.86
train mean loss: 308.48
epoch train time: 0:00:00.343299
elapsed time: 0:00:34.330844
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:23:35.663198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.62
 ---- batch: 020 ----
mean loss: 296.12
train mean loss: 294.37
epoch train time: 0:00:00.355242
elapsed time: 0:00:34.686266
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:23:36.018621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.35
 ---- batch: 020 ----
mean loss: 293.00
train mean loss: 291.53
epoch train time: 0:00:00.358130
elapsed time: 0:00:35.044574
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:23:36.376934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.28
 ---- batch: 020 ----
mean loss: 288.77
train mean loss: 284.31
epoch train time: 0:00:00.359833
elapsed time: 0:00:35.404607
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:23:36.736965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.44
 ---- batch: 020 ----
mean loss: 274.00
train mean loss: 273.83
epoch train time: 0:00:00.362244
elapsed time: 0:00:35.767031
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:23:37.099389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.91
 ---- batch: 020 ----
mean loss: 272.90
train mean loss: 270.46
epoch train time: 0:00:00.354627
elapsed time: 0:00:36.121835
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:23:37.454205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.31
 ---- batch: 020 ----
mean loss: 259.00
train mean loss: 262.30
epoch train time: 0:00:00.356029
elapsed time: 0:00:36.478058
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:23:37.810425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.50
 ---- batch: 020 ----
mean loss: 250.18
train mean loss: 253.06
epoch train time: 0:00:00.348040
elapsed time: 0:00:36.826281
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:23:38.158636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.17
 ---- batch: 020 ----
mean loss: 247.90
train mean loss: 250.22
epoch train time: 0:00:00.341752
elapsed time: 0:00:37.168268
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:23:38.500659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.67
 ---- batch: 020 ----
mean loss: 232.43
train mean loss: 239.50
epoch train time: 0:00:00.350608
elapsed time: 0:00:37.519133
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:23:38.851494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.42
 ---- batch: 020 ----
mean loss: 233.46
train mean loss: 236.66
epoch train time: 0:00:00.356806
elapsed time: 0:00:37.876141
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:23:39.208498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.29
 ---- batch: 020 ----
mean loss: 230.73
train mean loss: 229.18
epoch train time: 0:00:00.349279
elapsed time: 0:00:38.225594
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:23:39.557960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.93
 ---- batch: 020 ----
mean loss: 221.81
train mean loss: 222.19
epoch train time: 0:00:00.348764
elapsed time: 0:00:38.574544
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:23:39.906900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.81
 ---- batch: 020 ----
mean loss: 213.95
train mean loss: 214.92
epoch train time: 0:00:00.346877
elapsed time: 0:00:38.921604
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:23:40.253958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.35
 ---- batch: 020 ----
mean loss: 210.06
train mean loss: 210.78
epoch train time: 0:00:00.356523
elapsed time: 0:00:39.278316
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:23:40.610685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.21
 ---- batch: 020 ----
mean loss: 205.61
train mean loss: 205.12
epoch train time: 0:00:00.367506
elapsed time: 0:00:39.646024
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:23:40.978386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.02
 ---- batch: 020 ----
mean loss: 200.23
train mean loss: 202.08
epoch train time: 0:00:00.365913
elapsed time: 0:00:40.012127
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:23:41.344495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.01
 ---- batch: 020 ----
mean loss: 195.21
train mean loss: 198.85
epoch train time: 0:00:00.368160
elapsed time: 0:00:40.380494
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:23:41.712858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.50
 ---- batch: 020 ----
mean loss: 189.46
train mean loss: 193.69
epoch train time: 0:00:00.371187
elapsed time: 0:00:40.751865
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:23:42.084233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.24
 ---- batch: 020 ----
mean loss: 185.27
train mean loss: 189.29
epoch train time: 0:00:00.348093
elapsed time: 0:00:41.100146
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:23:42.432502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.53
 ---- batch: 020 ----
mean loss: 181.91
train mean loss: 186.47
epoch train time: 0:00:00.357520
elapsed time: 0:00:41.457842
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:23:42.790198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.39
 ---- batch: 020 ----
mean loss: 180.30
train mean loss: 179.68
epoch train time: 0:00:00.355126
elapsed time: 0:00:41.813140
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:23:43.145545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.03
 ---- batch: 020 ----
mean loss: 180.81
train mean loss: 180.53
epoch train time: 0:00:00.355751
elapsed time: 0:00:42.169118
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:23:43.501476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.36
 ---- batch: 020 ----
mean loss: 171.21
train mean loss: 172.46
epoch train time: 0:00:00.354083
elapsed time: 0:00:42.523376
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:23:43.855730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.35
 ---- batch: 020 ----
mean loss: 168.55
train mean loss: 168.27
epoch train time: 0:00:00.340625
elapsed time: 0:00:42.864181
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:23:44.196534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.65
 ---- batch: 020 ----
mean loss: 168.18
train mean loss: 166.69
epoch train time: 0:00:00.346310
elapsed time: 0:00:43.210681
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:23:44.543050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.64
 ---- batch: 020 ----
mean loss: 161.03
train mean loss: 165.15
epoch train time: 0:00:00.368159
elapsed time: 0:00:43.579035
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:23:44.911393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.15
 ---- batch: 020 ----
mean loss: 162.06
train mean loss: 160.45
epoch train time: 0:00:00.360016
elapsed time: 0:00:43.939251
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:23:45.271610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.69
 ---- batch: 020 ----
mean loss: 155.26
train mean loss: 156.69
epoch train time: 0:00:00.361183
elapsed time: 0:00:44.300632
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:23:45.632986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.20
 ---- batch: 020 ----
mean loss: 156.63
train mean loss: 153.64
epoch train time: 0:00:00.354414
elapsed time: 0:00:44.655233
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:23:45.987587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.70
 ---- batch: 020 ----
mean loss: 150.41
train mean loss: 150.23
epoch train time: 0:00:00.358369
elapsed time: 0:00:45.013802
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:23:46.346155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.55
 ---- batch: 020 ----
mean loss: 149.06
train mean loss: 148.34
epoch train time: 0:00:00.350161
elapsed time: 0:00:45.364171
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:23:46.696520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.87
 ---- batch: 020 ----
mean loss: 147.06
train mean loss: 144.75
epoch train time: 0:00:00.351380
elapsed time: 0:00:45.715719
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:23:47.048072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.93
 ---- batch: 020 ----
mean loss: 143.51
train mean loss: 145.06
epoch train time: 0:00:00.362399
elapsed time: 0:00:46.078306
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:23:47.410643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.58
 ---- batch: 020 ----
mean loss: 143.08
train mean loss: 141.74
epoch train time: 0:00:00.357193
elapsed time: 0:00:46.435704
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:23:47.768066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.49
 ---- batch: 020 ----
mean loss: 135.16
train mean loss: 136.61
epoch train time: 0:00:00.362064
elapsed time: 0:00:46.797957
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:23:48.130311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.41
 ---- batch: 020 ----
mean loss: 131.83
train mean loss: 135.14
epoch train time: 0:00:00.355357
elapsed time: 0:00:47.153531
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:23:48.485907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.51
 ---- batch: 020 ----
mean loss: 134.64
train mean loss: 135.89
epoch train time: 0:00:00.394501
elapsed time: 0:00:47.548235
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:23:48.880591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.91
 ---- batch: 020 ----
mean loss: 135.22
train mean loss: 131.39
epoch train time: 0:00:00.349121
elapsed time: 0:00:47.897597
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:23:49.229949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.86
 ---- batch: 020 ----
mean loss: 128.28
train mean loss: 128.57
epoch train time: 0:00:00.349663
elapsed time: 0:00:48.247487
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:23:49.579856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.12
 ---- batch: 020 ----
mean loss: 129.34
train mean loss: 127.80
epoch train time: 0:00:00.356151
elapsed time: 0:00:48.603836
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:23:49.936190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.84
 ---- batch: 020 ----
mean loss: 119.61
train mean loss: 126.07
epoch train time: 0:00:00.368593
elapsed time: 0:00:48.972625
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:23:50.304985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.50
 ---- batch: 020 ----
mean loss: 126.07
train mean loss: 125.07
epoch train time: 0:00:00.372708
elapsed time: 0:00:49.345565
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:23:50.677929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.60
 ---- batch: 020 ----
mean loss: 122.36
train mean loss: 122.47
epoch train time: 0:00:00.371137
elapsed time: 0:00:49.716903
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:23:51.049266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.96
 ---- batch: 020 ----
mean loss: 122.86
train mean loss: 121.65
epoch train time: 0:00:00.360163
elapsed time: 0:00:50.077251
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:23:51.409609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.33
 ---- batch: 020 ----
mean loss: 121.52
train mean loss: 118.95
epoch train time: 0:00:00.347179
elapsed time: 0:00:50.424606
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:23:51.756991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.62
 ---- batch: 020 ----
mean loss: 117.25
train mean loss: 116.29
epoch train time: 0:00:00.357136
elapsed time: 0:00:50.781945
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:23:52.114297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.70
 ---- batch: 020 ----
mean loss: 118.70
train mean loss: 115.43
epoch train time: 0:00:00.359413
elapsed time: 0:00:51.141552
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:23:52.473918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.84
 ---- batch: 020 ----
mean loss: 113.17
train mean loss: 116.17
epoch train time: 0:00:00.363021
elapsed time: 0:00:51.504762
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:23:52.837118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.03
 ---- batch: 020 ----
mean loss: 113.74
train mean loss: 112.21
epoch train time: 0:00:00.348940
elapsed time: 0:00:51.853880
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:23:53.186249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.19
 ---- batch: 020 ----
mean loss: 108.40
train mean loss: 108.96
epoch train time: 0:00:00.345132
elapsed time: 0:00:52.199193
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:23:53.531544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.45
 ---- batch: 020 ----
mean loss: 108.37
train mean loss: 108.88
epoch train time: 0:00:00.349342
elapsed time: 0:00:52.548778
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:23:53.881127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.85
 ---- batch: 020 ----
mean loss: 106.39
train mean loss: 107.75
epoch train time: 0:00:00.359939
elapsed time: 0:00:52.908889
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:23:54.241294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.34
 ---- batch: 020 ----
mean loss: 106.36
train mean loss: 108.63
epoch train time: 0:00:00.361250
elapsed time: 0:00:53.270370
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:23:54.602735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.01
 ---- batch: 020 ----
mean loss: 107.34
train mean loss: 105.82
epoch train time: 0:00:00.360270
elapsed time: 0:00:53.630827
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:23:54.963183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.54
 ---- batch: 020 ----
mean loss: 106.76
train mean loss: 105.88
epoch train time: 0:00:00.359540
elapsed time: 0:00:53.990547
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:23:55.322906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.90
 ---- batch: 020 ----
mean loss: 101.98
train mean loss: 101.20
epoch train time: 0:00:00.358413
elapsed time: 0:00:54.349142
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:23:55.681497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.72
 ---- batch: 020 ----
mean loss: 106.68
train mean loss: 102.21
epoch train time: 0:00:00.363276
elapsed time: 0:00:54.712614
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:23:56.045005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.22
 ---- batch: 020 ----
mean loss: 101.37
train mean loss: 100.18
epoch train time: 0:00:00.357700
elapsed time: 0:00:55.070539
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:23:56.402913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.77
 ---- batch: 020 ----
mean loss: 98.77
train mean loss: 99.31
epoch train time: 0:00:00.359698
elapsed time: 0:00:55.430434
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:23:56.762790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.35
 ---- batch: 020 ----
mean loss: 103.33
train mean loss: 98.72
epoch train time: 0:00:00.355839
elapsed time: 0:00:55.786449
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:23:57.118805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.72
 ---- batch: 020 ----
mean loss: 98.59
train mean loss: 96.72
epoch train time: 0:00:00.354979
elapsed time: 0:00:56.141604
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:23:57.473959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.74
 ---- batch: 020 ----
mean loss: 99.02
train mean loss: 97.89
epoch train time: 0:00:00.346348
elapsed time: 0:00:56.488194
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:23:57.820551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.25
 ---- batch: 020 ----
mean loss: 91.31
train mean loss: 93.62
epoch train time: 0:00:00.362240
elapsed time: 0:00:56.850619
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:23:58.182977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.50
 ---- batch: 020 ----
mean loss: 92.57
train mean loss: 93.76
epoch train time: 0:00:00.365997
elapsed time: 0:00:57.216813
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:23:58.549172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.88
 ---- batch: 020 ----
mean loss: 96.25
train mean loss: 93.84
epoch train time: 0:00:00.362816
elapsed time: 0:00:57.579815
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:23:58.912174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.26
 ---- batch: 020 ----
mean loss: 88.54
train mean loss: 93.33
epoch train time: 0:00:00.362413
elapsed time: 0:00:57.942414
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:23:59.274777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.49
 ---- batch: 020 ----
mean loss: 92.11
train mean loss: 93.92
epoch train time: 0:00:00.355323
elapsed time: 0:00:58.297959
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:23:59.630348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.62
 ---- batch: 020 ----
mean loss: 90.57
train mean loss: 91.35
epoch train time: 0:00:00.363538
elapsed time: 0:00:58.661711
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:23:59.994071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.34
 ---- batch: 020 ----
mean loss: 93.13
train mean loss: 91.54
epoch train time: 0:00:00.353371
elapsed time: 0:00:59.015307
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:24:00.347684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.75
 ---- batch: 020 ----
mean loss: 88.24
train mean loss: 90.16
epoch train time: 0:00:00.354361
elapsed time: 0:00:59.369907
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:24:00.702268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.66
 ---- batch: 020 ----
mean loss: 88.53
train mean loss: 88.05
epoch train time: 0:00:00.362213
elapsed time: 0:00:59.732303
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:24:01.064659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.91
 ---- batch: 020 ----
mean loss: 87.52
train mean loss: 90.39
epoch train time: 0:00:00.352902
elapsed time: 0:01:00.085421
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:24:01.417826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.82
 ---- batch: 020 ----
mean loss: 88.16
train mean loss: 88.53
epoch train time: 0:00:00.351827
elapsed time: 0:01:00.437523
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:24:01.769875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.53
 ---- batch: 020 ----
mean loss: 84.35
train mean loss: 85.11
epoch train time: 0:00:00.360928
elapsed time: 0:01:00.798631
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:24:02.130992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.88
 ---- batch: 020 ----
mean loss: 86.44
train mean loss: 86.43
epoch train time: 0:00:00.352632
elapsed time: 0:01:01.151446
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:24:02.483804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.52
 ---- batch: 020 ----
mean loss: 84.51
train mean loss: 85.49
epoch train time: 0:00:00.356745
elapsed time: 0:01:01.508390
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:24:02.840752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.11
 ---- batch: 020 ----
mean loss: 86.43
train mean loss: 84.45
epoch train time: 0:00:00.353496
elapsed time: 0:01:01.862072
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:24:03.194431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.78
 ---- batch: 020 ----
mean loss: 81.64
train mean loss: 82.89
epoch train time: 0:00:00.356089
elapsed time: 0:01:02.218346
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:24:03.550707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.85
 ---- batch: 020 ----
mean loss: 81.16
train mean loss: 82.98
epoch train time: 0:00:00.360574
elapsed time: 0:01:02.579103
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:24:03.911457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.38
 ---- batch: 020 ----
mean loss: 84.88
train mean loss: 83.94
epoch train time: 0:00:00.360514
elapsed time: 0:01:02.939796
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:24:04.272155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.30
 ---- batch: 020 ----
mean loss: 81.67
train mean loss: 80.33
epoch train time: 0:00:00.361687
elapsed time: 0:01:03.301704
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:24:04.634063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.20
 ---- batch: 020 ----
mean loss: 78.97
train mean loss: 81.20
epoch train time: 0:00:00.353718
elapsed time: 0:01:03.655599
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:24:04.987969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.25
 ---- batch: 020 ----
mean loss: 79.17
train mean loss: 80.65
epoch train time: 0:00:00.345952
elapsed time: 0:01:04.001742
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:24:05.334113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.98
 ---- batch: 020 ----
mean loss: 81.82
train mean loss: 81.27
epoch train time: 0:00:00.346799
elapsed time: 0:01:04.348743
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:24:05.681132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.93
 ---- batch: 020 ----
mean loss: 79.46
train mean loss: 79.07
epoch train time: 0:00:00.360028
elapsed time: 0:01:04.709044
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:24:06.041434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.72
 ---- batch: 020 ----
mean loss: 82.23
train mean loss: 79.76
epoch train time: 0:00:00.358894
elapsed time: 0:01:05.068163
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:24:06.400520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.89
 ---- batch: 020 ----
mean loss: 82.70
train mean loss: 79.28
epoch train time: 0:00:00.372625
elapsed time: 0:01:05.440983
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:24:06.773342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.28
 ---- batch: 020 ----
mean loss: 76.54
train mean loss: 77.70
epoch train time: 0:00:00.377802
elapsed time: 0:01:05.818962
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:24:07.151319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.05
 ---- batch: 020 ----
mean loss: 79.04
train mean loss: 78.82
epoch train time: 0:00:00.342533
elapsed time: 0:01:06.161700
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:24:07.494078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.61
 ---- batch: 020 ----
mean loss: 77.09
train mean loss: 78.16
epoch train time: 0:00:00.347068
elapsed time: 0:01:06.509034
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:24:07.841401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.14
 ---- batch: 020 ----
mean loss: 73.96
train mean loss: 76.75
epoch train time: 0:00:00.346680
elapsed time: 0:01:06.855899
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:24:08.188260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.41
 ---- batch: 020 ----
mean loss: 76.30
train mean loss: 78.23
epoch train time: 0:00:00.346408
elapsed time: 0:01:07.202496
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:24:08.534880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.64
 ---- batch: 020 ----
mean loss: 76.88
train mean loss: 75.86
epoch train time: 0:00:00.363672
elapsed time: 0:01:07.566411
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:24:08.898779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.31
 ---- batch: 020 ----
mean loss: 75.03
train mean loss: 76.73
epoch train time: 0:00:00.343589
elapsed time: 0:01:07.910187
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:24:09.242544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.97
 ---- batch: 020 ----
mean loss: 77.05
train mean loss: 75.96
epoch train time: 0:00:00.344183
elapsed time: 0:01:08.254588
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:24:09.586940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.41
 ---- batch: 020 ----
mean loss: 75.25
train mean loss: 73.79
epoch train time: 0:00:00.349612
elapsed time: 0:01:08.604417
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:24:09.936792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.90
 ---- batch: 020 ----
mean loss: 73.23
train mean loss: 73.78
epoch train time: 0:00:00.349198
elapsed time: 0:01:08.953808
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:24:10.286163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.76
 ---- batch: 020 ----
mean loss: 74.16
train mean loss: 73.48
epoch train time: 0:00:00.350791
elapsed time: 0:01:09.304805
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:24:10.637141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.81
 ---- batch: 020 ----
mean loss: 73.42
train mean loss: 73.04
epoch train time: 0:00:00.381890
elapsed time: 0:01:09.686850
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:24:11.019228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.47
 ---- batch: 020 ----
mean loss: 75.46
train mean loss: 74.15
epoch train time: 0:00:00.361734
elapsed time: 0:01:10.048797
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:24:11.381150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.75
 ---- batch: 020 ----
mean loss: 73.98
train mean loss: 73.03
epoch train time: 0:00:00.348806
elapsed time: 0:01:10.397785
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:24:11.730138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.39
 ---- batch: 020 ----
mean loss: 74.11
train mean loss: 72.95
epoch train time: 0:00:00.363821
elapsed time: 0:01:10.761778
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:24:12.094128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.07
 ---- batch: 020 ----
mean loss: 75.43
train mean loss: 72.50
epoch train time: 0:00:00.346251
elapsed time: 0:01:11.108194
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:24:12.440549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.48
 ---- batch: 020 ----
mean loss: 71.19
train mean loss: 70.88
epoch train time: 0:00:00.360260
elapsed time: 0:01:11.468646
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:24:12.801020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.84
 ---- batch: 020 ----
mean loss: 73.49
train mean loss: 71.98
epoch train time: 0:00:00.353034
elapsed time: 0:01:11.821889
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:24:13.154270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.37
 ---- batch: 020 ----
mean loss: 69.91
train mean loss: 71.11
epoch train time: 0:00:00.343172
elapsed time: 0:01:12.165255
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:24:13.497602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.76
 ---- batch: 020 ----
mean loss: 70.78
train mean loss: 70.03
epoch train time: 0:00:00.347215
elapsed time: 0:01:12.512634
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:24:13.844985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.79
 ---- batch: 020 ----
mean loss: 68.39
train mean loss: 69.41
epoch train time: 0:00:00.342997
elapsed time: 0:01:12.855797
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:24:14.188149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.87
 ---- batch: 020 ----
mean loss: 68.12
train mean loss: 69.96
epoch train time: 0:00:00.347894
elapsed time: 0:01:13.203866
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:24:14.536220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.98
 ---- batch: 020 ----
mean loss: 67.35
train mean loss: 67.70
epoch train time: 0:00:00.366460
elapsed time: 0:01:13.570512
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:24:14.902864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.09
 ---- batch: 020 ----
mean loss: 70.18
train mean loss: 69.14
epoch train time: 0:00:00.344317
elapsed time: 0:01:13.915010
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:24:15.247372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.47
 ---- batch: 020 ----
mean loss: 70.17
train mean loss: 69.54
epoch train time: 0:00:00.342713
elapsed time: 0:01:14.257933
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:24:15.590285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.42
 ---- batch: 020 ----
mean loss: 68.91
train mean loss: 67.92
epoch train time: 0:00:00.351830
elapsed time: 0:01:14.609929
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:24:15.942283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.92
 ---- batch: 020 ----
mean loss: 67.27
train mean loss: 68.36
epoch train time: 0:00:00.346754
elapsed time: 0:01:14.956861
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:24:16.289216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.22
 ---- batch: 020 ----
mean loss: 67.65
train mean loss: 66.74
epoch train time: 0:00:00.348252
elapsed time: 0:01:15.305314
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:24:16.637683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.67
 ---- batch: 020 ----
mean loss: 68.03
train mean loss: 68.00
epoch train time: 0:00:00.351935
elapsed time: 0:01:15.657432
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:24:16.989803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.24
 ---- batch: 020 ----
mean loss: 63.90
train mean loss: 66.72
epoch train time: 0:00:00.339916
elapsed time: 0:01:15.997555
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:24:17.329918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.66
 ---- batch: 020 ----
mean loss: 66.42
train mean loss: 66.35
epoch train time: 0:00:00.343139
elapsed time: 0:01:16.340897
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:24:17.673255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.85
 ---- batch: 020 ----
mean loss: 65.55
train mean loss: 65.12
epoch train time: 0:00:00.368395
elapsed time: 0:01:16.709484
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:24:18.041849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.03
 ---- batch: 020 ----
mean loss: 66.44
train mean loss: 66.45
epoch train time: 0:00:00.346467
elapsed time: 0:01:17.056140
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:24:18.388489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.49
 ---- batch: 020 ----
mean loss: 68.08
train mean loss: 64.66
epoch train time: 0:00:00.350941
elapsed time: 0:01:17.407251
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:24:18.739630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.05
 ---- batch: 020 ----
mean loss: 65.37
train mean loss: 65.92
epoch train time: 0:00:00.345395
elapsed time: 0:01:17.752842
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:24:19.085197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.22
 ---- batch: 020 ----
mean loss: 64.80
train mean loss: 64.68
epoch train time: 0:00:00.343852
elapsed time: 0:01:18.097000
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:24:19.429354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.38
 ---- batch: 020 ----
mean loss: 66.51
train mean loss: 64.74
epoch train time: 0:00:00.347417
elapsed time: 0:01:18.444591
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:24:19.776948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.78
 ---- batch: 020 ----
mean loss: 65.53
train mean loss: 63.75
epoch train time: 0:00:00.345978
elapsed time: 0:01:18.790742
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:24:20.123101
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.22
 ---- batch: 020 ----
mean loss: 62.76
train mean loss: 63.91
epoch train time: 0:00:00.343672
elapsed time: 0:01:19.134642
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:24:20.466976
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.57
 ---- batch: 020 ----
mean loss: 63.31
train mean loss: 62.84
epoch train time: 0:00:00.347286
elapsed time: 0:01:19.482103
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:24:20.814488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.40
 ---- batch: 020 ----
mean loss: 61.76
train mean loss: 62.62
epoch train time: 0:00:00.342956
elapsed time: 0:01:19.825262
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:24:21.157617
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.94
 ---- batch: 020 ----
mean loss: 62.67
train mean loss: 62.65
epoch train time: 0:00:00.342175
elapsed time: 0:01:20.167652
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:24:21.500022
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.20
 ---- batch: 020 ----
mean loss: 61.90
train mean loss: 62.03
epoch train time: 0:00:00.364110
elapsed time: 0:01:20.531965
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:24:21.864339
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.08
 ---- batch: 020 ----
mean loss: 65.24
train mean loss: 63.57
epoch train time: 0:00:00.344328
elapsed time: 0:01:20.876485
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:24:22.208841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.22
 ---- batch: 020 ----
mean loss: 62.44
train mean loss: 62.02
epoch train time: 0:00:00.342723
elapsed time: 0:01:21.219397
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:24:22.551737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.52
 ---- batch: 020 ----
mean loss: 62.64
train mean loss: 62.55
epoch train time: 0:00:00.360273
elapsed time: 0:01:21.579827
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:24:22.912183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.58
 ---- batch: 020 ----
mean loss: 61.77
train mean loss: 63.40
epoch train time: 0:00:00.346960
elapsed time: 0:01:21.926959
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:24:23.259311
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.48
 ---- batch: 020 ----
mean loss: 63.99
train mean loss: 62.48
epoch train time: 0:00:00.353676
elapsed time: 0:01:22.280810
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:24:23.613181
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.76
 ---- batch: 020 ----
mean loss: 61.22
train mean loss: 62.18
epoch train time: 0:00:00.353884
elapsed time: 0:01:22.634887
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:24:23.967243
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.94
 ---- batch: 020 ----
mean loss: 62.58
train mean loss: 63.68
epoch train time: 0:00:00.361001
elapsed time: 0:01:22.996071
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:24:24.328435
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.78
 ---- batch: 020 ----
mean loss: 65.67
train mean loss: 63.19
epoch train time: 0:00:00.359159
elapsed time: 0:01:23.355424
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:24:24.687783
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.95
 ---- batch: 020 ----
mean loss: 62.82
train mean loss: 62.20
epoch train time: 0:00:00.363953
elapsed time: 0:01:23.719572
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:24:25.051934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.55
 ---- batch: 020 ----
mean loss: 61.43
train mean loss: 61.40
epoch train time: 0:00:00.346755
elapsed time: 0:01:24.066509
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:24:25.398865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.72
 ---- batch: 020 ----
mean loss: 63.19
train mean loss: 62.29
epoch train time: 0:00:00.350039
elapsed time: 0:01:24.416727
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:24:25.749080
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.69
 ---- batch: 020 ----
mean loss: 62.89
train mean loss: 62.68
epoch train time: 0:00:00.349330
elapsed time: 0:01:24.766230
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:24:26.098581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.67
 ---- batch: 020 ----
mean loss: 62.98
train mean loss: 62.43
epoch train time: 0:00:00.346243
elapsed time: 0:01:25.112647
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:24:26.445002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.85
 ---- batch: 020 ----
mean loss: 63.42
train mean loss: 61.88
epoch train time: 0:00:00.353806
elapsed time: 0:01:25.466641
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:24:26.798997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.94
 ---- batch: 020 ----
mean loss: 60.47
train mean loss: 62.10
epoch train time: 0:00:00.348565
elapsed time: 0:01:25.815377
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:24:27.147733
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.00
 ---- batch: 020 ----
mean loss: 61.43
train mean loss: 61.45
epoch train time: 0:00:00.338694
elapsed time: 0:01:26.154242
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:24:27.486613
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.41
 ---- batch: 020 ----
mean loss: 62.74
train mean loss: 62.55
epoch train time: 0:00:00.359304
elapsed time: 0:01:26.513763
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:24:27.846117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.93
 ---- batch: 020 ----
mean loss: 63.39
train mean loss: 62.60
epoch train time: 0:00:00.343781
elapsed time: 0:01:26.857720
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:24:28.190072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.09
 ---- batch: 020 ----
mean loss: 63.27
train mean loss: 63.10
epoch train time: 0:00:00.343959
elapsed time: 0:01:27.201850
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:24:28.534201
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.05
 ---- batch: 020 ----
mean loss: 64.78
train mean loss: 62.28
epoch train time: 0:00:00.367965
elapsed time: 0:01:27.569986
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:24:28.902361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.88
 ---- batch: 020 ----
mean loss: 64.09
train mean loss: 62.16
epoch train time: 0:00:00.344659
elapsed time: 0:01:27.914837
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:24:29.247191
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.54
 ---- batch: 020 ----
mean loss: 61.44
train mean loss: 61.80
epoch train time: 0:00:00.344769
elapsed time: 0:01:28.259787
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:24:29.592146
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.20
 ---- batch: 020 ----
mean loss: 64.69
train mean loss: 63.12
epoch train time: 0:00:00.350545
elapsed time: 0:01:28.610517
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:24:29.942879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.59
 ---- batch: 020 ----
mean loss: 65.00
train mean loss: 62.77
epoch train time: 0:00:00.357635
elapsed time: 0:01:28.968340
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:24:30.300696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.98
 ---- batch: 020 ----
mean loss: 62.80
train mean loss: 61.95
epoch train time: 0:00:00.357955
elapsed time: 0:01:29.326470
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:24:30.658827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.96
 ---- batch: 020 ----
mean loss: 63.31
train mean loss: 61.19
epoch train time: 0:00:00.371467
elapsed time: 0:01:29.698155
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:24:31.030514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.94
 ---- batch: 020 ----
mean loss: 60.58
train mean loss: 62.20
epoch train time: 0:00:00.362090
elapsed time: 0:01:30.060431
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:24:31.392797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.71
 ---- batch: 020 ----
mean loss: 59.81
train mean loss: 61.55
epoch train time: 0:00:00.360506
elapsed time: 0:01:30.421179
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:24:31.753514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.13
 ---- batch: 020 ----
mean loss: 61.69
train mean loss: 61.83
epoch train time: 0:00:00.365871
elapsed time: 0:01:30.787203
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:24:32.119554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.13
 ---- batch: 020 ----
mean loss: 61.50
train mean loss: 61.69
epoch train time: 0:00:00.356936
elapsed time: 0:01:31.144312
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:24:32.476668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.08
 ---- batch: 020 ----
mean loss: 61.75
train mean loss: 61.68
epoch train time: 0:00:00.350501
elapsed time: 0:01:31.495003
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:24:32.827360
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.65
 ---- batch: 020 ----
mean loss: 62.84
train mean loss: 62.34
epoch train time: 0:00:00.370545
elapsed time: 0:01:31.865749
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:24:33.198123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.31
 ---- batch: 020 ----
mean loss: 59.62
train mean loss: 62.15
epoch train time: 0:00:00.353021
elapsed time: 0:01:32.218966
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:24:33.551321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.03
 ---- batch: 020 ----
mean loss: 62.52
train mean loss: 60.82
epoch train time: 0:00:00.349261
elapsed time: 0:01:32.568401
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:24:33.900778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.54
 ---- batch: 020 ----
mean loss: 61.68
train mean loss: 62.28
epoch train time: 0:00:00.351491
elapsed time: 0:01:32.920093
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:24:34.252450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.38
 ---- batch: 020 ----
mean loss: 62.66
train mean loss: 62.00
epoch train time: 0:00:00.347060
elapsed time: 0:01:33.267343
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:24:34.599695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.94
 ---- batch: 020 ----
mean loss: 63.34
train mean loss: 61.21
epoch train time: 0:00:00.379167
elapsed time: 0:01:33.646684
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:24:34.979044
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.19
 ---- batch: 020 ----
mean loss: 61.79
train mean loss: 61.81
epoch train time: 0:00:00.355454
elapsed time: 0:01:34.002344
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:24:35.334704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.54
 ---- batch: 020 ----
mean loss: 62.07
train mean loss: 60.28
epoch train time: 0:00:00.354749
elapsed time: 0:01:34.357288
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:24:35.689645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.37
 ---- batch: 020 ----
mean loss: 62.54
train mean loss: 61.20
epoch train time: 0:00:00.381708
elapsed time: 0:01:34.739175
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:24:36.071533
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.80
 ---- batch: 020 ----
mean loss: 59.12
train mean loss: 61.40
epoch train time: 0:00:00.355722
elapsed time: 0:01:35.095092
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:24:36.427502
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.19
 ---- batch: 020 ----
mean loss: 59.59
train mean loss: 61.31
epoch train time: 0:00:00.361026
elapsed time: 0:01:35.456353
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:24:36.788714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.74
 ---- batch: 020 ----
mean loss: 61.46
train mean loss: 61.28
epoch train time: 0:00:00.371172
elapsed time: 0:01:35.827779
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:24:37.160157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.70
 ---- batch: 020 ----
mean loss: 62.61
train mean loss: 61.58
epoch train time: 0:00:00.355650
elapsed time: 0:01:36.191653
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_1/checkpoint.pth.tar
**** end time: 2019-10-01 14:24:37.523968 ****
