Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24323
use_cuda: True
Dataset: CMAPSS/FD001
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-27 10:05:20.725730 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 10:05:20.743487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2561.02
 ---- batch: 020 ----
mean loss: 1295.20
 ---- batch: 030 ----
mean loss: 1182.71
train mean loss: 1608.57
epoch train time: 0:00:17.117899
elapsed time: 0:00:17.143762
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 10:05:37.869549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1127.24
 ---- batch: 020 ----
mean loss: 1055.85
 ---- batch: 030 ----
mean loss: 1090.70
train mean loss: 1087.71
epoch train time: 0:00:07.024144
elapsed time: 0:00:24.168597
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 10:05:44.894491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1054.58
 ---- batch: 020 ----
mean loss: 1024.53
 ---- batch: 030 ----
mean loss: 1032.44
train mean loss: 1032.80
epoch train time: 0:00:06.981133
elapsed time: 0:00:31.150537
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 10:05:51.876427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1040.34
 ---- batch: 020 ----
mean loss: 999.50
 ---- batch: 030 ----
mean loss: 981.07
train mean loss: 1001.17
epoch train time: 0:00:06.942229
elapsed time: 0:00:38.093585
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 10:05:58.819465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1011.49
 ---- batch: 020 ----
mean loss: 977.84
 ---- batch: 030 ----
mean loss: 964.50
train mean loss: 982.58
epoch train time: 0:00:06.998889
elapsed time: 0:00:45.093243
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 10:06:05.819129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 966.10
 ---- batch: 020 ----
mean loss: 986.80
 ---- batch: 030 ----
mean loss: 960.43
train mean loss: 971.84
epoch train time: 0:00:07.033742
elapsed time: 0:00:52.127831
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 10:06:12.853748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.82
 ---- batch: 020 ----
mean loss: 925.63
 ---- batch: 030 ----
mean loss: 935.86
train mean loss: 938.08
epoch train time: 0:00:06.925362
elapsed time: 0:00:59.053977
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 10:06:19.779848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.01
 ---- batch: 020 ----
mean loss: 862.07
 ---- batch: 030 ----
mean loss: 806.70
train mean loss: 842.01
epoch train time: 0:00:07.008831
elapsed time: 0:01:06.063784
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 10:06:26.789721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.70
 ---- batch: 020 ----
mean loss: 649.57
 ---- batch: 030 ----
mean loss: 607.79
train mean loss: 643.32
epoch train time: 0:00:06.983678
elapsed time: 0:01:13.048295
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 10:06:33.774173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.73
 ---- batch: 020 ----
mean loss: 457.07
 ---- batch: 030 ----
mean loss: 428.83
train mean loss: 460.07
epoch train time: 0:00:06.990777
elapsed time: 0:01:20.039857
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 10:06:40.765783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.68
 ---- batch: 020 ----
mean loss: 404.87
 ---- batch: 030 ----
mean loss: 392.18
train mean loss: 401.90
epoch train time: 0:00:07.046581
elapsed time: 0:01:27.087304
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 10:06:47.813188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.40
 ---- batch: 020 ----
mean loss: 389.40
 ---- batch: 030 ----
mean loss: 382.47
train mean loss: 386.32
epoch train time: 0:00:06.933979
elapsed time: 0:01:34.022056
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 10:06:54.747934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.68
 ---- batch: 020 ----
mean loss: 374.99
 ---- batch: 030 ----
mean loss: 366.13
train mean loss: 371.16
epoch train time: 0:00:06.950550
elapsed time: 0:01:40.973350
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 10:07:01.699222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.09
 ---- batch: 020 ----
mean loss: 359.49
 ---- batch: 030 ----
mean loss: 352.97
train mean loss: 361.08
epoch train time: 0:00:06.919023
elapsed time: 0:01:47.893193
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 10:07:08.619091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.57
 ---- batch: 020 ----
mean loss: 356.40
 ---- batch: 030 ----
mean loss: 359.54
train mean loss: 356.89
epoch train time: 0:00:06.985303
elapsed time: 0:01:54.879269
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 10:07:15.605147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.90
 ---- batch: 020 ----
mean loss: 345.55
 ---- batch: 030 ----
mean loss: 347.22
train mean loss: 348.60
epoch train time: 0:00:06.942288
elapsed time: 0:02:01.822317
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 10:07:22.548201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.90
 ---- batch: 020 ----
mean loss: 343.23
 ---- batch: 030 ----
mean loss: 337.07
train mean loss: 335.57
epoch train time: 0:00:06.893574
elapsed time: 0:02:08.716636
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 10:07:29.442532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.29
 ---- batch: 020 ----
mean loss: 331.49
 ---- batch: 030 ----
mean loss: 321.11
train mean loss: 329.43
epoch train time: 0:00:06.853126
elapsed time: 0:02:15.570532
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 10:07:36.296444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.85
 ---- batch: 020 ----
mean loss: 328.60
 ---- batch: 030 ----
mean loss: 323.08
train mean loss: 325.04
epoch train time: 0:00:06.773492
elapsed time: 0:02:22.344902
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 10:07:43.070794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.96
 ---- batch: 020 ----
mean loss: 311.02
 ---- batch: 030 ----
mean loss: 310.83
train mean loss: 313.35
epoch train time: 0:00:07.010380
elapsed time: 0:02:29.356120
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 10:07:50.082011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.76
 ---- batch: 020 ----
mean loss: 312.13
 ---- batch: 030 ----
mean loss: 294.76
train mean loss: 304.12
epoch train time: 0:00:06.846724
elapsed time: 0:02:36.203848
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 10:07:56.929754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.34
 ---- batch: 020 ----
mean loss: 305.73
 ---- batch: 030 ----
mean loss: 297.32
train mean loss: 298.53
epoch train time: 0:00:06.915727
elapsed time: 0:02:43.120416
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 10:08:03.846338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.69
 ---- batch: 020 ----
mean loss: 290.90
 ---- batch: 030 ----
mean loss: 287.58
train mean loss: 288.59
epoch train time: 0:00:06.964676
elapsed time: 0:02:50.085896
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 10:08:10.811775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.81
 ---- batch: 020 ----
mean loss: 289.45
 ---- batch: 030 ----
mean loss: 283.99
train mean loss: 284.99
epoch train time: 0:00:06.897989
elapsed time: 0:02:56.984588
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 10:08:17.710478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.04
 ---- batch: 020 ----
mean loss: 264.05
 ---- batch: 030 ----
mean loss: 267.63
train mean loss: 268.39
epoch train time: 0:00:06.809573
elapsed time: 0:03:03.794994
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 10:08:24.520917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.09
 ---- batch: 020 ----
mean loss: 252.87
 ---- batch: 030 ----
mean loss: 254.15
train mean loss: 254.34
epoch train time: 0:00:07.015259
elapsed time: 0:03:10.811109
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 10:08:31.537019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.87
 ---- batch: 020 ----
mean loss: 249.60
 ---- batch: 030 ----
mean loss: 241.29
train mean loss: 246.45
epoch train time: 0:00:06.985067
elapsed time: 0:03:17.797076
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 10:08:38.522997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.71
 ---- batch: 020 ----
mean loss: 231.70
 ---- batch: 030 ----
mean loss: 228.09
train mean loss: 234.24
epoch train time: 0:00:07.037488
elapsed time: 0:03:24.835418
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 10:08:45.561299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.30
 ---- batch: 020 ----
mean loss: 217.55
 ---- batch: 030 ----
mean loss: 223.76
train mean loss: 223.27
epoch train time: 0:00:07.070712
elapsed time: 0:03:31.906812
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 10:08:52.632747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.27
 ---- batch: 020 ----
mean loss: 208.46
 ---- batch: 030 ----
mean loss: 208.91
train mean loss: 212.25
epoch train time: 0:00:06.906854
elapsed time: 0:03:38.814462
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 10:08:59.540335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.16
 ---- batch: 020 ----
mean loss: 204.33
 ---- batch: 030 ----
mean loss: 206.97
train mean loss: 205.62
epoch train time: 0:00:07.004771
elapsed time: 0:03:45.820051
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 10:09:06.545950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.37
 ---- batch: 020 ----
mean loss: 205.07
 ---- batch: 030 ----
mean loss: 200.62
train mean loss: 202.66
epoch train time: 0:00:06.959155
elapsed time: 0:03:52.780097
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 10:09:13.505981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.70
 ---- batch: 020 ----
mean loss: 193.75
 ---- batch: 030 ----
mean loss: 198.94
train mean loss: 196.25
epoch train time: 0:00:06.941310
elapsed time: 0:03:59.722307
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 10:09:20.448210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.70
 ---- batch: 020 ----
mean loss: 194.07
 ---- batch: 030 ----
mean loss: 196.03
train mean loss: 194.78
epoch train time: 0:00:06.969343
elapsed time: 0:04:06.692564
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 10:09:27.418487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.16
 ---- batch: 020 ----
mean loss: 190.61
 ---- batch: 030 ----
mean loss: 191.34
train mean loss: 190.96
epoch train time: 0:00:06.947340
elapsed time: 0:04:13.640749
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 10:09:34.366638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.15
 ---- batch: 020 ----
mean loss: 179.49
 ---- batch: 030 ----
mean loss: 187.07
train mean loss: 184.30
epoch train time: 0:00:06.890374
elapsed time: 0:04:20.531960
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 10:09:41.257859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.46
 ---- batch: 020 ----
mean loss: 184.26
 ---- batch: 030 ----
mean loss: 174.31
train mean loss: 181.15
epoch train time: 0:00:07.008215
elapsed time: 0:04:27.541043
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 10:09:48.266916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.37
 ---- batch: 020 ----
mean loss: 177.82
 ---- batch: 030 ----
mean loss: 176.49
train mean loss: 176.81
epoch train time: 0:00:06.899450
elapsed time: 0:04:34.441270
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 10:09:55.167159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.54
 ---- batch: 020 ----
mean loss: 172.73
 ---- batch: 030 ----
mean loss: 176.00
train mean loss: 174.30
epoch train time: 0:00:06.991752
elapsed time: 0:04:41.433804
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 10:10:02.159686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.86
 ---- batch: 020 ----
mean loss: 167.33
 ---- batch: 030 ----
mean loss: 171.15
train mean loss: 169.41
epoch train time: 0:00:06.930463
elapsed time: 0:04:48.365084
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 10:10:09.090969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.11
 ---- batch: 020 ----
mean loss: 162.03
 ---- batch: 030 ----
mean loss: 163.29
train mean loss: 164.92
epoch train time: 0:00:07.000416
elapsed time: 0:04:55.366342
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 10:10:16.092238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.95
 ---- batch: 020 ----
mean loss: 165.94
 ---- batch: 030 ----
mean loss: 170.46
train mean loss: 167.48
epoch train time: 0:00:06.888980
elapsed time: 0:05:02.256121
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 10:10:22.982029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.03
 ---- batch: 020 ----
mean loss: 157.42
 ---- batch: 030 ----
mean loss: 159.78
train mean loss: 160.94
epoch train time: 0:00:07.045557
elapsed time: 0:05:09.302484
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 10:10:30.028365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.47
 ---- batch: 020 ----
mean loss: 155.43
 ---- batch: 030 ----
mean loss: 159.22
train mean loss: 159.60
epoch train time: 0:00:06.959954
elapsed time: 0:05:16.263320
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 10:10:36.989275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.82
 ---- batch: 020 ----
mean loss: 158.14
 ---- batch: 030 ----
mean loss: 160.50
train mean loss: 159.91
epoch train time: 0:00:07.035867
elapsed time: 0:05:23.300220
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 10:10:44.026190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.27
 ---- batch: 020 ----
mean loss: 157.46
 ---- batch: 030 ----
mean loss: 158.09
train mean loss: 157.74
epoch train time: 0:00:06.989005
elapsed time: 0:05:30.290048
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 10:10:51.015933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.89
 ---- batch: 020 ----
mean loss: 159.98
 ---- batch: 030 ----
mean loss: 156.38
train mean loss: 156.35
epoch train time: 0:00:06.910519
elapsed time: 0:05:37.201360
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 10:10:57.927250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.99
 ---- batch: 020 ----
mean loss: 152.58
 ---- batch: 030 ----
mean loss: 156.57
train mean loss: 152.88
epoch train time: 0:00:07.021394
elapsed time: 0:05:44.223614
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 10:11:04.949540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.95
 ---- batch: 020 ----
mean loss: 154.40
 ---- batch: 030 ----
mean loss: 153.96
train mean loss: 152.31
epoch train time: 0:00:07.022240
elapsed time: 0:05:51.246663
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 10:11:11.972570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.71
 ---- batch: 020 ----
mean loss: 147.78
 ---- batch: 030 ----
mean loss: 148.39
train mean loss: 147.91
epoch train time: 0:00:06.969813
elapsed time: 0:05:58.217265
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 10:11:18.943144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.73
 ---- batch: 020 ----
mean loss: 150.21
 ---- batch: 030 ----
mean loss: 147.65
train mean loss: 148.54
epoch train time: 0:00:07.066566
elapsed time: 0:06:05.284659
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 10:11:26.010549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.65
 ---- batch: 020 ----
mean loss: 149.92
 ---- batch: 030 ----
mean loss: 148.03
train mean loss: 145.60
epoch train time: 0:00:06.944057
elapsed time: 0:06:12.229619
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 10:11:32.955518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.18
 ---- batch: 020 ----
mean loss: 141.40
 ---- batch: 030 ----
mean loss: 141.21
train mean loss: 144.20
epoch train time: 0:00:07.021416
elapsed time: 0:06:19.251828
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 10:11:39.977734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.91
 ---- batch: 020 ----
mean loss: 144.18
 ---- batch: 030 ----
mean loss: 140.22
train mean loss: 143.81
epoch train time: 0:00:07.021247
elapsed time: 0:06:26.273866
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 10:11:46.999745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.18
 ---- batch: 020 ----
mean loss: 140.95
 ---- batch: 030 ----
mean loss: 144.74
train mean loss: 142.73
epoch train time: 0:00:06.966444
elapsed time: 0:06:33.241083
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 10:11:53.966957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.43
 ---- batch: 020 ----
mean loss: 142.60
 ---- batch: 030 ----
mean loss: 139.57
train mean loss: 141.71
epoch train time: 0:00:06.957656
elapsed time: 0:06:40.199564
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 10:12:00.925471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.76
 ---- batch: 020 ----
mean loss: 140.36
 ---- batch: 030 ----
mean loss: 141.04
train mean loss: 140.49
epoch train time: 0:00:07.015476
elapsed time: 0:06:47.215805
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 10:12:07.941703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.36
 ---- batch: 020 ----
mean loss: 145.54
 ---- batch: 030 ----
mean loss: 138.74
train mean loss: 137.68
epoch train time: 0:00:06.978783
elapsed time: 0:06:54.195375
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 10:12:14.921290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.80
 ---- batch: 020 ----
mean loss: 136.65
 ---- batch: 030 ----
mean loss: 137.63
train mean loss: 136.17
epoch train time: 0:00:07.020954
elapsed time: 0:07:01.217112
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 10:12:21.943003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.82
 ---- batch: 020 ----
mean loss: 133.55
 ---- batch: 030 ----
mean loss: 135.97
train mean loss: 136.05
epoch train time: 0:00:06.991378
elapsed time: 0:07:08.209270
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 10:12:28.935169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.83
 ---- batch: 020 ----
mean loss: 132.59
 ---- batch: 030 ----
mean loss: 135.42
train mean loss: 133.72
epoch train time: 0:00:06.944551
elapsed time: 0:07:15.154614
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 10:12:35.880526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.30
 ---- batch: 020 ----
mean loss: 130.73
 ---- batch: 030 ----
mean loss: 134.23
train mean loss: 132.75
epoch train time: 0:00:07.027463
elapsed time: 0:07:22.182919
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 10:12:42.908814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.23
 ---- batch: 020 ----
mean loss: 133.63
 ---- batch: 030 ----
mean loss: 135.27
train mean loss: 133.90
epoch train time: 0:00:06.934770
elapsed time: 0:07:29.118455
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 10:12:49.844338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.37
 ---- batch: 020 ----
mean loss: 135.86
 ---- batch: 030 ----
mean loss: 136.50
train mean loss: 133.70
epoch train time: 0:00:07.002790
elapsed time: 0:07:36.122104
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 10:12:56.847989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.17
 ---- batch: 020 ----
mean loss: 131.04
 ---- batch: 030 ----
mean loss: 126.72
train mean loss: 130.74
epoch train time: 0:00:06.912726
elapsed time: 0:07:43.035574
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 10:13:03.761471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.49
 ---- batch: 020 ----
mean loss: 127.78
 ---- batch: 030 ----
mean loss: 131.26
train mean loss: 129.94
epoch train time: 0:00:06.972408
elapsed time: 0:07:50.008844
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 10:13:10.734728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.73
 ---- batch: 020 ----
mean loss: 132.66
 ---- batch: 030 ----
mean loss: 131.26
train mean loss: 132.20
epoch train time: 0:00:06.884099
elapsed time: 0:07:56.893736
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 10:13:17.619657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.23
 ---- batch: 020 ----
mean loss: 128.18
 ---- batch: 030 ----
mean loss: 131.68
train mean loss: 128.09
epoch train time: 0:00:06.963232
elapsed time: 0:08:03.857799
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 10:13:24.583685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.09
 ---- batch: 020 ----
mean loss: 127.45
 ---- batch: 030 ----
mean loss: 122.20
train mean loss: 128.29
epoch train time: 0:00:06.977041
elapsed time: 0:08:10.835635
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 10:13:31.561524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.15
 ---- batch: 020 ----
mean loss: 126.96
 ---- batch: 030 ----
mean loss: 128.64
train mean loss: 128.18
epoch train time: 0:00:06.984235
elapsed time: 0:08:17.820636
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 10:13:38.546522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.64
 ---- batch: 020 ----
mean loss: 128.96
 ---- batch: 030 ----
mean loss: 127.37
train mean loss: 128.34
epoch train time: 0:00:06.969148
elapsed time: 0:08:24.790532
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 10:13:45.516414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.72
 ---- batch: 020 ----
mean loss: 126.49
 ---- batch: 030 ----
mean loss: 124.35
train mean loss: 126.98
epoch train time: 0:00:06.960624
elapsed time: 0:08:31.751989
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 10:13:52.477880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.93
 ---- batch: 020 ----
mean loss: 126.28
 ---- batch: 030 ----
mean loss: 123.53
train mean loss: 125.58
epoch train time: 0:00:06.983001
elapsed time: 0:08:38.735794
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 10:13:59.461721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.78
 ---- batch: 020 ----
mean loss: 129.90
 ---- batch: 030 ----
mean loss: 125.68
train mean loss: 125.57
epoch train time: 0:00:06.984182
elapsed time: 0:08:45.720880
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 10:14:06.446776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.44
 ---- batch: 020 ----
mean loss: 125.45
 ---- batch: 030 ----
mean loss: 125.52
train mean loss: 123.52
epoch train time: 0:00:06.966468
elapsed time: 0:08:52.688125
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 10:14:13.414023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.17
 ---- batch: 020 ----
mean loss: 123.44
 ---- batch: 030 ----
mean loss: 123.12
train mean loss: 124.19
epoch train time: 0:00:06.815659
elapsed time: 0:08:59.504547
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 10:14:20.230462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.44
 ---- batch: 020 ----
mean loss: 123.07
 ---- batch: 030 ----
mean loss: 121.05
train mean loss: 123.59
epoch train time: 0:00:06.823752
elapsed time: 0:09:06.329077
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 10:14:27.054974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.07
 ---- batch: 020 ----
mean loss: 119.24
 ---- batch: 030 ----
mean loss: 126.66
train mean loss: 122.89
epoch train time: 0:00:07.018821
elapsed time: 0:09:13.348668
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 10:14:34.074556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.29
 ---- batch: 020 ----
mean loss: 118.44
 ---- batch: 030 ----
mean loss: 121.89
train mean loss: 122.61
epoch train time: 0:00:06.973290
elapsed time: 0:09:20.322771
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 10:14:41.048664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.32
 ---- batch: 020 ----
mean loss: 122.71
 ---- batch: 030 ----
mean loss: 122.84
train mean loss: 121.73
epoch train time: 0:00:06.929860
elapsed time: 0:09:27.253476
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 10:14:47.979355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.94
 ---- batch: 020 ----
mean loss: 119.49
 ---- batch: 030 ----
mean loss: 119.01
train mean loss: 122.05
epoch train time: 0:00:07.011194
elapsed time: 0:09:34.265456
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 10:14:54.991344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.21
 ---- batch: 020 ----
mean loss: 121.81
 ---- batch: 030 ----
mean loss: 123.94
train mean loss: 121.28
epoch train time: 0:00:06.887809
elapsed time: 0:09:41.154159
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 10:15:01.880048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.32
 ---- batch: 020 ----
mean loss: 120.21
 ---- batch: 030 ----
mean loss: 119.43
train mean loss: 119.78
epoch train time: 0:00:06.970597
elapsed time: 0:09:48.125531
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 10:15:08.851410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.85
 ---- batch: 020 ----
mean loss: 121.33
 ---- batch: 030 ----
mean loss: 119.56
train mean loss: 119.05
epoch train time: 0:00:06.956917
elapsed time: 0:09:55.083303
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 10:15:15.809223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.83
 ---- batch: 020 ----
mean loss: 122.21
 ---- batch: 030 ----
mean loss: 121.50
train mean loss: 119.72
epoch train time: 0:00:06.937791
elapsed time: 0:10:02.021941
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 10:15:22.747885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.50
 ---- batch: 020 ----
mean loss: 120.88
 ---- batch: 030 ----
mean loss: 119.41
train mean loss: 119.35
epoch train time: 0:00:06.904879
elapsed time: 0:10:08.927640
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 10:15:29.653522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.33
 ---- batch: 020 ----
mean loss: 117.67
 ---- batch: 030 ----
mean loss: 123.34
train mean loss: 118.30
epoch train time: 0:00:06.884329
elapsed time: 0:10:15.812732
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 10:15:36.538600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.94
 ---- batch: 020 ----
mean loss: 118.88
 ---- batch: 030 ----
mean loss: 111.49
train mean loss: 117.47
epoch train time: 0:00:06.911229
elapsed time: 0:10:22.724946
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 10:15:43.450905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.44
 ---- batch: 020 ----
mean loss: 119.44
 ---- batch: 030 ----
mean loss: 113.87
train mean loss: 118.05
epoch train time: 0:00:06.838291
elapsed time: 0:10:29.564099
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 10:15:50.290025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.46
 ---- batch: 020 ----
mean loss: 111.22
 ---- batch: 030 ----
mean loss: 121.28
train mean loss: 118.32
epoch train time: 0:00:06.792104
elapsed time: 0:10:36.357021
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 10:15:57.082912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.74
 ---- batch: 020 ----
mean loss: 116.18
 ---- batch: 030 ----
mean loss: 116.06
train mean loss: 116.41
epoch train time: 0:00:06.808030
elapsed time: 0:10:43.165852
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 10:16:03.891731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.98
 ---- batch: 020 ----
mean loss: 114.33
 ---- batch: 030 ----
mean loss: 117.44
train mean loss: 114.98
epoch train time: 0:00:06.798657
elapsed time: 0:10:49.965268
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 10:16:10.691156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.41
 ---- batch: 020 ----
mean loss: 118.05
 ---- batch: 030 ----
mean loss: 113.70
train mean loss: 116.20
epoch train time: 0:00:06.867647
elapsed time: 0:10:56.833678
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 10:16:17.559563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.40
 ---- batch: 020 ----
mean loss: 111.11
 ---- batch: 030 ----
mean loss: 116.31
train mean loss: 115.01
epoch train time: 0:00:06.785546
elapsed time: 0:11:03.620014
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 10:16:24.345907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.85
 ---- batch: 020 ----
mean loss: 114.17
 ---- batch: 030 ----
mean loss: 115.74
train mean loss: 114.33
epoch train time: 0:00:06.825964
elapsed time: 0:11:10.446845
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 10:16:31.172735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.54
 ---- batch: 020 ----
mean loss: 110.68
 ---- batch: 030 ----
mean loss: 119.32
train mean loss: 113.98
epoch train time: 0:00:06.758249
elapsed time: 0:11:17.205858
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 10:16:37.931734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.50
 ---- batch: 020 ----
mean loss: 109.94
 ---- batch: 030 ----
mean loss: 118.12
train mean loss: 112.92
epoch train time: 0:00:06.911687
elapsed time: 0:11:24.118522
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 10:16:44.844479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.18
 ---- batch: 020 ----
mean loss: 113.85
 ---- batch: 030 ----
mean loss: 114.32
train mean loss: 113.59
epoch train time: 0:00:06.785429
elapsed time: 0:11:30.904787
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 10:16:51.630662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.54
 ---- batch: 020 ----
mean loss: 116.18
 ---- batch: 030 ----
mean loss: 111.66
train mean loss: 113.27
epoch train time: 0:00:06.886490
elapsed time: 0:11:37.792115
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 10:16:58.518041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.03
 ---- batch: 020 ----
mean loss: 111.36
 ---- batch: 030 ----
mean loss: 109.30
train mean loss: 111.73
epoch train time: 0:00:06.816343
elapsed time: 0:11:44.609314
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 10:17:05.335231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.74
 ---- batch: 020 ----
mean loss: 110.03
 ---- batch: 030 ----
mean loss: 107.05
train mean loss: 110.40
epoch train time: 0:00:06.885651
elapsed time: 0:11:51.495748
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 10:17:12.221679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.49
 ---- batch: 020 ----
mean loss: 109.19
 ---- batch: 030 ----
mean loss: 111.62
train mean loss: 111.05
epoch train time: 0:00:06.805305
elapsed time: 0:11:58.301865
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 10:17:19.027753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.99
 ---- batch: 020 ----
mean loss: 111.61
 ---- batch: 030 ----
mean loss: 109.17
train mean loss: 109.66
epoch train time: 0:00:06.681127
elapsed time: 0:12:04.983774
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 10:17:25.709705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.38
 ---- batch: 020 ----
mean loss: 110.73
 ---- batch: 030 ----
mean loss: 110.97
train mean loss: 110.01
epoch train time: 0:00:06.800206
elapsed time: 0:12:11.784777
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 10:17:32.510696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.95
 ---- batch: 020 ----
mean loss: 109.22
 ---- batch: 030 ----
mean loss: 109.09
train mean loss: 110.27
epoch train time: 0:00:06.956093
elapsed time: 0:12:18.741687
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 10:17:39.467569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.97
 ---- batch: 020 ----
mean loss: 109.49
 ---- batch: 030 ----
mean loss: 115.08
train mean loss: 111.45
epoch train time: 0:00:06.772934
elapsed time: 0:12:25.515464
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 10:17:46.241352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.90
 ---- batch: 020 ----
mean loss: 111.49
 ---- batch: 030 ----
mean loss: 110.18
train mean loss: 111.16
epoch train time: 0:00:06.838341
elapsed time: 0:12:32.354733
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 10:17:53.080515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.12
 ---- batch: 020 ----
mean loss: 109.57
 ---- batch: 030 ----
mean loss: 109.76
train mean loss: 108.84
epoch train time: 0:00:06.865729
elapsed time: 0:12:39.221116
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 10:17:59.947003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.10
 ---- batch: 020 ----
mean loss: 109.06
 ---- batch: 030 ----
mean loss: 103.47
train mean loss: 108.33
epoch train time: 0:00:06.739351
elapsed time: 0:12:45.961218
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 10:18:06.687099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.32
 ---- batch: 020 ----
mean loss: 108.62
 ---- batch: 030 ----
mean loss: 106.53
train mean loss: 107.60
epoch train time: 0:00:06.879049
elapsed time: 0:12:52.841051
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 10:18:13.566932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.52
 ---- batch: 020 ----
mean loss: 108.54
 ---- batch: 030 ----
mean loss: 111.27
train mean loss: 109.18
epoch train time: 0:00:06.765879
elapsed time: 0:12:59.607680
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 10:18:20.333678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.18
 ---- batch: 020 ----
mean loss: 107.61
 ---- batch: 030 ----
mean loss: 105.27
train mean loss: 107.84
epoch train time: 0:00:06.865022
elapsed time: 0:13:06.473611
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 10:18:27.199502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.83
 ---- batch: 020 ----
mean loss: 108.53
 ---- batch: 030 ----
mean loss: 108.70
train mean loss: 109.09
epoch train time: 0:00:06.804822
elapsed time: 0:13:13.279227
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 10:18:34.005141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.76
 ---- batch: 020 ----
mean loss: 106.16
 ---- batch: 030 ----
mean loss: 107.01
train mean loss: 107.34
epoch train time: 0:00:06.832494
elapsed time: 0:13:20.112524
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 10:18:40.838406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.63
 ---- batch: 020 ----
mean loss: 105.17
 ---- batch: 030 ----
mean loss: 107.28
train mean loss: 106.53
epoch train time: 0:00:06.864697
elapsed time: 0:13:26.977956
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 10:18:47.703857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.83
 ---- batch: 020 ----
mean loss: 107.81
 ---- batch: 030 ----
mean loss: 108.10
train mean loss: 107.71
epoch train time: 0:00:06.786051
elapsed time: 0:13:33.764791
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 10:18:54.490690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.51
 ---- batch: 020 ----
mean loss: 106.20
 ---- batch: 030 ----
mean loss: 108.54
train mean loss: 106.93
epoch train time: 0:00:06.861497
elapsed time: 0:13:40.627054
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 10:19:01.353000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.01
 ---- batch: 020 ----
mean loss: 107.98
 ---- batch: 030 ----
mean loss: 108.41
train mean loss: 107.85
epoch train time: 0:00:06.775894
elapsed time: 0:13:47.403754
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 10:19:08.129683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.70
 ---- batch: 020 ----
mean loss: 106.86
 ---- batch: 030 ----
mean loss: 103.89
train mean loss: 105.96
epoch train time: 0:00:06.855995
elapsed time: 0:13:54.260647
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 10:19:14.986546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.77
 ---- batch: 020 ----
mean loss: 106.87
 ---- batch: 030 ----
mean loss: 105.51
train mean loss: 107.34
epoch train time: 0:00:06.731451
elapsed time: 0:14:00.992889
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 10:19:21.718774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.33
 ---- batch: 020 ----
mean loss: 107.99
 ---- batch: 030 ----
mean loss: 103.58
train mean loss: 106.25
epoch train time: 0:00:06.837085
elapsed time: 0:14:07.830763
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 10:19:28.556679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.49
 ---- batch: 020 ----
mean loss: 102.55
 ---- batch: 030 ----
mean loss: 108.16
train mean loss: 104.50
epoch train time: 0:00:06.817514
elapsed time: 0:14:14.649096
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 10:19:35.374992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.11
 ---- batch: 020 ----
mean loss: 105.77
 ---- batch: 030 ----
mean loss: 105.62
train mean loss: 104.52
epoch train time: 0:00:06.773306
elapsed time: 0:14:21.423246
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 10:19:42.149116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.94
 ---- batch: 020 ----
mean loss: 102.22
 ---- batch: 030 ----
mean loss: 104.05
train mean loss: 103.44
epoch train time: 0:00:06.865096
elapsed time: 0:14:28.289187
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 10:19:49.015068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.73
 ---- batch: 020 ----
mean loss: 103.12
 ---- batch: 030 ----
mean loss: 106.88
train mean loss: 104.94
epoch train time: 0:00:06.741500
elapsed time: 0:14:35.031423
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 10:19:55.757304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.64
 ---- batch: 020 ----
mean loss: 104.71
 ---- batch: 030 ----
mean loss: 100.26
train mean loss: 105.13
epoch train time: 0:00:06.796990
elapsed time: 0:14:41.829186
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 10:20:02.555066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.82
 ---- batch: 020 ----
mean loss: 103.62
 ---- batch: 030 ----
mean loss: 103.11
train mean loss: 103.52
epoch train time: 0:00:06.825657
elapsed time: 0:14:48.655674
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 10:20:09.381455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.38
 ---- batch: 020 ----
mean loss: 102.59
 ---- batch: 030 ----
mean loss: 109.22
train mean loss: 104.29
epoch train time: 0:00:06.695010
elapsed time: 0:14:55.351407
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 10:20:16.077312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.48
 ---- batch: 020 ----
mean loss: 102.96
 ---- batch: 030 ----
mean loss: 101.45
train mean loss: 102.85
epoch train time: 0:00:06.872111
elapsed time: 0:15:02.224336
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 10:20:22.950227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.36
 ---- batch: 020 ----
mean loss: 102.25
 ---- batch: 030 ----
mean loss: 101.87
train mean loss: 103.21
epoch train time: 0:00:06.800253
elapsed time: 0:15:09.025342
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 10:20:29.751261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.08
 ---- batch: 020 ----
mean loss: 102.21
 ---- batch: 030 ----
mean loss: 102.85
train mean loss: 102.70
epoch train time: 0:00:06.743439
elapsed time: 0:15:15.769560
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 10:20:36.495445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.14
 ---- batch: 020 ----
mean loss: 100.33
 ---- batch: 030 ----
mean loss: 102.39
train mean loss: 100.62
epoch train time: 0:00:06.880843
elapsed time: 0:15:22.651237
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 10:20:43.377123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.54
 ---- batch: 020 ----
mean loss: 105.39
 ---- batch: 030 ----
mean loss: 98.47
train mean loss: 102.72
epoch train time: 0:00:06.981657
elapsed time: 0:15:29.633592
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 10:20:50.359464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.78
 ---- batch: 020 ----
mean loss: 102.34
 ---- batch: 030 ----
mean loss: 101.99
train mean loss: 101.47
epoch train time: 0:00:06.795318
elapsed time: 0:15:36.429737
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 10:20:57.155611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.19
 ---- batch: 020 ----
mean loss: 101.46
 ---- batch: 030 ----
mean loss: 99.04
train mean loss: 101.19
epoch train time: 0:00:06.925570
elapsed time: 0:15:43.356088
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 10:21:04.081976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.24
 ---- batch: 020 ----
mean loss: 97.04
 ---- batch: 030 ----
mean loss: 104.29
train mean loss: 100.97
epoch train time: 0:00:06.849051
elapsed time: 0:15:50.205922
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 10:21:10.931797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.27
 ---- batch: 020 ----
mean loss: 102.07
 ---- batch: 030 ----
mean loss: 99.75
train mean loss: 101.47
epoch train time: 0:00:06.875844
elapsed time: 0:15:57.082541
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 10:21:17.808441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.60
 ---- batch: 020 ----
mean loss: 103.61
 ---- batch: 030 ----
mean loss: 98.01
train mean loss: 100.86
epoch train time: 0:00:06.868069
elapsed time: 0:16:03.951403
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 10:21:24.677281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.52
 ---- batch: 020 ----
mean loss: 104.69
 ---- batch: 030 ----
mean loss: 100.19
train mean loss: 102.35
epoch train time: 0:00:06.851017
elapsed time: 0:16:10.803190
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 10:21:31.529065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.92
 ---- batch: 020 ----
mean loss: 101.69
 ---- batch: 030 ----
mean loss: 98.30
train mean loss: 99.82
epoch train time: 0:00:06.929019
elapsed time: 0:16:17.733015
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 10:21:38.458895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.41
 ---- batch: 020 ----
mean loss: 100.32
 ---- batch: 030 ----
mean loss: 103.33
train mean loss: 101.44
epoch train time: 0:00:06.865253
elapsed time: 0:16:24.599061
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 10:21:45.324941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.11
 ---- batch: 020 ----
mean loss: 104.12
 ---- batch: 030 ----
mean loss: 105.31
train mean loss: 104.54
epoch train time: 0:00:06.899057
elapsed time: 0:16:31.498891
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 10:21:52.224811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.43
 ---- batch: 020 ----
mean loss: 98.38
 ---- batch: 030 ----
mean loss: 102.19
train mean loss: 98.88
epoch train time: 0:00:06.829845
elapsed time: 0:16:38.329564
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 10:21:59.055485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.94
 ---- batch: 020 ----
mean loss: 99.22
 ---- batch: 030 ----
mean loss: 100.57
train mean loss: 99.15
epoch train time: 0:00:06.894641
elapsed time: 0:16:45.225066
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 10:22:05.950944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.31
 ---- batch: 020 ----
mean loss: 98.13
 ---- batch: 030 ----
mean loss: 102.06
train mean loss: 99.07
epoch train time: 0:00:06.899379
elapsed time: 0:16:52.125209
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 10:22:12.851134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.29
 ---- batch: 020 ----
mean loss: 101.61
 ---- batch: 030 ----
mean loss: 95.68
train mean loss: 98.83
epoch train time: 0:00:06.833322
elapsed time: 0:16:58.959331
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 10:22:19.685221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.35
 ---- batch: 020 ----
mean loss: 100.55
 ---- batch: 030 ----
mean loss: 98.20
train mean loss: 98.68
epoch train time: 0:00:06.887215
elapsed time: 0:17:05.847377
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 10:22:26.573403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.58
 ---- batch: 020 ----
mean loss: 97.11
 ---- batch: 030 ----
mean loss: 97.68
train mean loss: 97.81
epoch train time: 0:00:06.862137
elapsed time: 0:17:12.710497
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 10:22:33.436404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.95
 ---- batch: 020 ----
mean loss: 99.23
 ---- batch: 030 ----
mean loss: 97.29
train mean loss: 98.32
epoch train time: 0:00:06.915529
elapsed time: 0:17:19.626929
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 10:22:40.352719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.22
 ---- batch: 020 ----
mean loss: 96.46
 ---- batch: 030 ----
mean loss: 98.22
train mean loss: 97.72
epoch train time: 0:00:06.893175
elapsed time: 0:17:26.520810
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 10:22:47.246692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.42
 ---- batch: 020 ----
mean loss: 99.77
 ---- batch: 030 ----
mean loss: 99.80
train mean loss: 98.83
epoch train time: 0:00:06.781578
elapsed time: 0:17:33.303143
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 10:22:54.029020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.83
 ---- batch: 020 ----
mean loss: 98.78
 ---- batch: 030 ----
mean loss: 104.02
train mean loss: 100.48
epoch train time: 0:00:06.861836
elapsed time: 0:17:40.165769
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 10:23:00.891693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.32
 ---- batch: 020 ----
mean loss: 100.39
 ---- batch: 030 ----
mean loss: 93.49
train mean loss: 98.24
epoch train time: 0:00:06.791395
elapsed time: 0:17:46.958038
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 10:23:07.683928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.94
 ---- batch: 020 ----
mean loss: 97.00
 ---- batch: 030 ----
mean loss: 100.32
train mean loss: 97.88
epoch train time: 0:00:06.803838
elapsed time: 0:17:53.762638
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 10:23:14.488539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.65
 ---- batch: 020 ----
mean loss: 98.87
 ---- batch: 030 ----
mean loss: 96.29
train mean loss: 97.91
epoch train time: 0:00:06.882402
elapsed time: 0:18:00.645819
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 10:23:21.371706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.12
 ---- batch: 020 ----
mean loss: 95.25
 ---- batch: 030 ----
mean loss: 98.97
train mean loss: 96.24
epoch train time: 0:00:06.946970
elapsed time: 0:18:07.593623
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 10:23:28.319512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.26
 ---- batch: 020 ----
mean loss: 95.65
 ---- batch: 030 ----
mean loss: 96.06
train mean loss: 96.48
epoch train time: 0:00:06.838449
elapsed time: 0:18:14.432844
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 10:23:35.158739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.01
 ---- batch: 020 ----
mean loss: 96.08
 ---- batch: 030 ----
mean loss: 98.78
train mean loss: 97.57
epoch train time: 0:00:06.699662
elapsed time: 0:18:21.133467
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 10:23:41.859374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.10
 ---- batch: 020 ----
mean loss: 97.63
 ---- batch: 030 ----
mean loss: 97.06
train mean loss: 96.07
epoch train time: 0:00:06.880095
elapsed time: 0:18:28.014474
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 10:23:48.740371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.26
 ---- batch: 020 ----
mean loss: 93.13
 ---- batch: 030 ----
mean loss: 93.90
train mean loss: 95.66
epoch train time: 0:00:06.880253
elapsed time: 0:18:34.895459
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 10:23:55.621340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.57
 ---- batch: 020 ----
mean loss: 95.45
 ---- batch: 030 ----
mean loss: 96.95
train mean loss: 97.03
epoch train time: 0:00:06.780432
elapsed time: 0:18:41.676654
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 10:24:02.402530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.05
 ---- batch: 020 ----
mean loss: 95.11
 ---- batch: 030 ----
mean loss: 93.96
train mean loss: 96.47
epoch train time: 0:00:06.873716
elapsed time: 0:18:48.551112
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 10:24:09.277010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.24
 ---- batch: 020 ----
mean loss: 98.81
 ---- batch: 030 ----
mean loss: 94.95
train mean loss: 96.34
epoch train time: 0:00:06.834951
elapsed time: 0:18:55.386822
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 10:24:16.112702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.28
 ---- batch: 020 ----
mean loss: 95.64
 ---- batch: 030 ----
mean loss: 92.59
train mean loss: 94.91
epoch train time: 0:00:06.859559
elapsed time: 0:19:02.247144
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 10:24:22.973048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.29
 ---- batch: 020 ----
mean loss: 95.30
 ---- batch: 030 ----
mean loss: 94.40
train mean loss: 96.18
epoch train time: 0:00:06.885235
elapsed time: 0:19:09.133251
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 10:24:29.859165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.29
 ---- batch: 020 ----
mean loss: 93.63
 ---- batch: 030 ----
mean loss: 99.46
train mean loss: 95.44
epoch train time: 0:00:06.892872
elapsed time: 0:19:16.026880
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 10:24:36.752795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.21
 ---- batch: 020 ----
mean loss: 96.52
 ---- batch: 030 ----
mean loss: 94.89
train mean loss: 95.88
epoch train time: 0:00:06.770213
elapsed time: 0:19:22.797964
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 10:24:43.523848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.12
 ---- batch: 020 ----
mean loss: 93.21
 ---- batch: 030 ----
mean loss: 96.96
train mean loss: 96.11
epoch train time: 0:00:06.854129
elapsed time: 0:19:29.652835
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 10:24:50.378704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.85
 ---- batch: 020 ----
mean loss: 91.52
 ---- batch: 030 ----
mean loss: 93.92
train mean loss: 94.56
epoch train time: 0:00:06.761283
elapsed time: 0:19:36.414841
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 10:24:57.140722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.14
 ---- batch: 020 ----
mean loss: 94.62
 ---- batch: 030 ----
mean loss: 92.26
train mean loss: 94.09
epoch train time: 0:00:06.815276
elapsed time: 0:19:43.230835
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 10:25:03.956723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.19
 ---- batch: 020 ----
mean loss: 94.96
 ---- batch: 030 ----
mean loss: 93.34
train mean loss: 94.31
epoch train time: 0:00:06.801584
elapsed time: 0:19:50.033154
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 10:25:10.759079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.48
 ---- batch: 020 ----
mean loss: 92.28
 ---- batch: 030 ----
mean loss: 92.90
train mean loss: 94.38
epoch train time: 0:00:06.725612
elapsed time: 0:19:56.759584
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 10:25:17.485473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.07
 ---- batch: 020 ----
mean loss: 90.89
 ---- batch: 030 ----
mean loss: 95.28
train mean loss: 93.63
epoch train time: 0:00:06.818509
elapsed time: 0:20:03.578837
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 10:25:24.304739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.73
 ---- batch: 020 ----
mean loss: 96.81
 ---- batch: 030 ----
mean loss: 92.93
train mean loss: 94.60
epoch train time: 0:00:06.764455
elapsed time: 0:20:10.344177
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 10:25:31.069965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.28
 ---- batch: 020 ----
mean loss: 94.44
 ---- batch: 030 ----
mean loss: 94.74
train mean loss: 93.50
epoch train time: 0:00:06.857345
elapsed time: 0:20:17.202238
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 10:25:37.928124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.14
 ---- batch: 020 ----
mean loss: 93.62
 ---- batch: 030 ----
mean loss: 94.59
train mean loss: 94.29
epoch train time: 0:00:06.736983
elapsed time: 0:20:23.940152
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 10:25:44.666042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.54
 ---- batch: 020 ----
mean loss: 93.35
 ---- batch: 030 ----
mean loss: 93.77
train mean loss: 93.07
epoch train time: 0:00:06.777426
elapsed time: 0:20:30.718370
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 10:25:51.444252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.02
 ---- batch: 020 ----
mean loss: 90.81
 ---- batch: 030 ----
mean loss: 93.68
train mean loss: 92.42
epoch train time: 0:00:06.862447
elapsed time: 0:20:37.581585
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 10:25:58.307475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.09
 ---- batch: 020 ----
mean loss: 94.83
 ---- batch: 030 ----
mean loss: 92.95
train mean loss: 93.41
epoch train time: 0:00:06.729682
elapsed time: 0:20:44.312070
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 10:26:05.037967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.97
 ---- batch: 020 ----
mean loss: 90.88
 ---- batch: 030 ----
mean loss: 92.94
train mean loss: 92.74
epoch train time: 0:00:06.844901
elapsed time: 0:20:51.157860
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 10:26:11.883812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.48
 ---- batch: 020 ----
mean loss: 96.78
 ---- batch: 030 ----
mean loss: 92.32
train mean loss: 92.69
epoch train time: 0:00:06.799332
elapsed time: 0:20:57.957971
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 10:26:18.683852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.40
 ---- batch: 020 ----
mean loss: 94.82
 ---- batch: 030 ----
mean loss: 94.23
train mean loss: 93.08
epoch train time: 0:00:06.726829
elapsed time: 0:21:04.685553
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 10:26:25.411468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.38
 ---- batch: 020 ----
mean loss: 92.23
 ---- batch: 030 ----
mean loss: 92.23
train mean loss: 92.24
epoch train time: 0:00:06.935537
elapsed time: 0:21:11.621894
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 10:26:32.347775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.62
 ---- batch: 020 ----
mean loss: 92.81
 ---- batch: 030 ----
mean loss: 93.47
train mean loss: 92.78
epoch train time: 0:00:06.925417
elapsed time: 0:21:18.548580
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 10:26:39.274487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.77
 ---- batch: 020 ----
mean loss: 92.51
 ---- batch: 030 ----
mean loss: 92.37
train mean loss: 92.07
epoch train time: 0:00:06.912241
elapsed time: 0:21:25.461698
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 10:26:46.187571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.35
 ---- batch: 020 ----
mean loss: 95.77
 ---- batch: 030 ----
mean loss: 88.25
train mean loss: 93.43
epoch train time: 0:00:06.787613
elapsed time: 0:21:32.250168
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 10:26:52.976043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.22
 ---- batch: 020 ----
mean loss: 94.93
 ---- batch: 030 ----
mean loss: 92.31
train mean loss: 93.45
epoch train time: 0:00:06.865854
elapsed time: 0:21:39.116797
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 10:26:59.842685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.76
 ---- batch: 020 ----
mean loss: 91.78
 ---- batch: 030 ----
mean loss: 92.41
train mean loss: 90.66
epoch train time: 0:00:06.701303
elapsed time: 0:21:45.818852
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 10:27:06.544744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.49
 ---- batch: 020 ----
mean loss: 89.45
 ---- batch: 030 ----
mean loss: 95.25
train mean loss: 91.22
epoch train time: 0:00:06.720835
elapsed time: 0:21:52.540445
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 10:27:13.266321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.09
 ---- batch: 020 ----
mean loss: 91.02
 ---- batch: 030 ----
mean loss: 92.91
train mean loss: 90.77
epoch train time: 0:00:06.918235
elapsed time: 0:21:59.459425
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 10:27:20.185302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.82
 ---- batch: 020 ----
mean loss: 89.07
 ---- batch: 030 ----
mean loss: 90.11
train mean loss: 90.85
epoch train time: 0:00:06.856866
elapsed time: 0:22:06.317084
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 10:27:27.042981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.92
 ---- batch: 020 ----
mean loss: 90.51
 ---- batch: 030 ----
mean loss: 90.01
train mean loss: 90.83
epoch train time: 0:00:06.829384
elapsed time: 0:22:13.147314
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 10:27:33.873220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.64
 ---- batch: 020 ----
mean loss: 89.53
 ---- batch: 030 ----
mean loss: 92.05
train mean loss: 90.94
epoch train time: 0:00:06.866619
elapsed time: 0:22:20.014786
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 10:27:40.740694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.53
 ---- batch: 020 ----
mean loss: 92.87
 ---- batch: 030 ----
mean loss: 89.18
train mean loss: 91.10
epoch train time: 0:00:06.878493
elapsed time: 0:22:26.894149
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 10:27:47.620034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.50
 ---- batch: 020 ----
mean loss: 92.10
 ---- batch: 030 ----
mean loss: 89.35
train mean loss: 90.50
epoch train time: 0:00:06.792407
elapsed time: 0:22:33.687324
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 10:27:54.413205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.95
 ---- batch: 020 ----
mean loss: 93.30
 ---- batch: 030 ----
mean loss: 89.17
train mean loss: 91.12
epoch train time: 0:00:06.881825
elapsed time: 0:22:40.570067
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 10:28:01.295953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.47
 ---- batch: 020 ----
mean loss: 92.41
 ---- batch: 030 ----
mean loss: 87.30
train mean loss: 90.48
epoch train time: 0:00:06.905144
elapsed time: 0:22:47.476031
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 10:28:08.201947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.04
 ---- batch: 020 ----
mean loss: 91.04
 ---- batch: 030 ----
mean loss: 89.20
train mean loss: 89.72
epoch train time: 0:00:06.802354
elapsed time: 0:22:54.279209
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 10:28:15.005139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.40
 ---- batch: 020 ----
mean loss: 89.53
 ---- batch: 030 ----
mean loss: 91.66
train mean loss: 89.64
epoch train time: 0:00:06.871750
elapsed time: 0:23:01.151762
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 10:28:21.877681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.02
 ---- batch: 020 ----
mean loss: 90.57
 ---- batch: 030 ----
mean loss: 92.21
train mean loss: 90.79
epoch train time: 0:00:06.819780
elapsed time: 0:23:07.972363
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 10:28:28.698264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.63
 ---- batch: 020 ----
mean loss: 91.75
 ---- batch: 030 ----
mean loss: 92.34
train mean loss: 90.90
epoch train time: 0:00:06.869980
elapsed time: 0:23:14.843309
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 10:28:35.569237
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.12
 ---- batch: 020 ----
mean loss: 88.68
 ---- batch: 030 ----
mean loss: 90.92
train mean loss: 88.89
epoch train time: 0:00:06.841037
elapsed time: 0:23:21.685372
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 10:28:42.411173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.32
 ---- batch: 020 ----
mean loss: 87.91
 ---- batch: 030 ----
mean loss: 89.14
train mean loss: 88.53
epoch train time: 0:00:06.843530
elapsed time: 0:23:28.529640
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 10:28:49.255534
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.11
 ---- batch: 020 ----
mean loss: 89.40
 ---- batch: 030 ----
mean loss: 89.06
train mean loss: 89.00
epoch train time: 0:00:06.839834
elapsed time: 0:23:35.370324
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 10:28:56.096221
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.50
 ---- batch: 020 ----
mean loss: 86.21
 ---- batch: 030 ----
mean loss: 87.94
train mean loss: 88.30
epoch train time: 0:00:06.890512
elapsed time: 0:23:42.261680
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 10:29:02.987566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.54
 ---- batch: 020 ----
mean loss: 87.74
 ---- batch: 030 ----
mean loss: 86.78
train mean loss: 88.22
epoch train time: 0:00:06.900169
elapsed time: 0:23:49.162642
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 10:29:09.888532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.70
 ---- batch: 020 ----
mean loss: 89.83
 ---- batch: 030 ----
mean loss: 86.03
train mean loss: 88.09
epoch train time: 0:00:06.813661
elapsed time: 0:23:55.977062
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 10:29:16.702960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.85
 ---- batch: 020 ----
mean loss: 89.11
 ---- batch: 030 ----
mean loss: 88.27
train mean loss: 88.91
epoch train time: 0:00:06.889418
elapsed time: 0:24:02.867296
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 10:29:23.593233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.28
 ---- batch: 020 ----
mean loss: 84.78
 ---- batch: 030 ----
mean loss: 89.22
train mean loss: 88.90
epoch train time: 0:00:06.958505
elapsed time: 0:24:09.826629
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 10:29:30.552551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.93
 ---- batch: 020 ----
mean loss: 89.43
 ---- batch: 030 ----
mean loss: 88.03
train mean loss: 89.11
epoch train time: 0:00:06.779399
elapsed time: 0:24:16.606814
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 10:29:37.332720
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.83
 ---- batch: 020 ----
mean loss: 87.25
 ---- batch: 030 ----
mean loss: 88.37
train mean loss: 88.79
epoch train time: 0:00:06.863896
elapsed time: 0:24:23.471481
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 10:29:44.197364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.09
 ---- batch: 020 ----
mean loss: 88.79
 ---- batch: 030 ----
mean loss: 87.37
train mean loss: 88.76
epoch train time: 0:00:06.920784
elapsed time: 0:24:30.393008
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 10:29:51.118904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 87.70
 ---- batch: 020 ----
mean loss: 91.41
 ---- batch: 030 ----
mean loss: 88.27
train mean loss: 88.49
epoch train time: 0:00:06.817616
elapsed time: 0:24:37.211392
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 10:29:57.937270
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.47
 ---- batch: 020 ----
mean loss: 86.42
 ---- batch: 030 ----
mean loss: 88.54
train mean loss: 88.24
epoch train time: 0:00:06.871629
elapsed time: 0:24:44.083736
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 10:30:04.809633
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.80
 ---- batch: 020 ----
mean loss: 90.28
 ---- batch: 030 ----
mean loss: 85.58
train mean loss: 88.86
epoch train time: 0:00:06.779177
elapsed time: 0:24:50.863695
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 10:30:11.589568
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.46
 ---- batch: 020 ----
mean loss: 84.69
 ---- batch: 030 ----
mean loss: 92.46
train mean loss: 88.45
epoch train time: 0:00:06.878421
elapsed time: 0:24:57.742889
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 10:30:18.468779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.01
 ---- batch: 020 ----
mean loss: 91.52
 ---- batch: 030 ----
mean loss: 87.57
train mean loss: 89.08
epoch train time: 0:00:06.811927
elapsed time: 0:25:04.555555
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 10:30:25.281430
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 86.41
 ---- batch: 020 ----
mean loss: 89.21
 ---- batch: 030 ----
mean loss: 90.34
train mean loss: 88.26
epoch train time: 0:00:06.707669
elapsed time: 0:25:11.263962
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 10:30:31.989846
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.62
 ---- batch: 020 ----
mean loss: 87.91
 ---- batch: 030 ----
mean loss: 88.01
train mean loss: 88.67
epoch train time: 0:00:06.800106
elapsed time: 0:25:18.064861
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 10:30:38.790794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 86.94
 ---- batch: 020 ----
mean loss: 89.92
 ---- batch: 030 ----
mean loss: 88.51
train mean loss: 88.42
epoch train time: 0:00:06.985768
elapsed time: 0:25:25.051553
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 10:30:45.777477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.08
 ---- batch: 020 ----
mean loss: 87.87
 ---- batch: 030 ----
mean loss: 88.90
train mean loss: 88.28
epoch train time: 0:00:06.766349
elapsed time: 0:25:31.818754
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 10:30:52.544639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.02
 ---- batch: 020 ----
mean loss: 90.48
 ---- batch: 030 ----
mean loss: 88.21
train mean loss: 88.59
epoch train time: 0:00:06.853872
elapsed time: 0:25:38.673400
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 10:30:59.399304
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.16
 ---- batch: 020 ----
mean loss: 87.47
 ---- batch: 030 ----
mean loss: 84.44
train mean loss: 87.92
epoch train time: 0:00:06.834169
elapsed time: 0:25:45.508448
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 10:31:06.234330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.86
 ---- batch: 020 ----
mean loss: 87.37
 ---- batch: 030 ----
mean loss: 90.69
train mean loss: 88.82
epoch train time: 0:00:06.773990
elapsed time: 0:25:52.283194
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 10:31:13.009078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.32
 ---- batch: 020 ----
mean loss: 86.18
 ---- batch: 030 ----
mean loss: 86.32
train mean loss: 87.83
epoch train time: 0:00:06.823871
elapsed time: 0:25:59.107837
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 10:31:19.833761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 86.66
 ---- batch: 020 ----
mean loss: 86.83
 ---- batch: 030 ----
mean loss: 90.76
train mean loss: 88.28
epoch train time: 0:00:06.877998
elapsed time: 0:26:05.986621
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 10:31:26.712509
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 86.65
 ---- batch: 020 ----
mean loss: 89.76
 ---- batch: 030 ----
mean loss: 88.89
train mean loss: 88.21
epoch train time: 0:00:06.745611
elapsed time: 0:26:12.733014
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 10:31:33.458895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 87.32
 ---- batch: 020 ----
mean loss: 91.32
 ---- batch: 030 ----
mean loss: 87.12
train mean loss: 88.74
epoch train time: 0:00:06.819557
elapsed time: 0:26:19.553340
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 10:31:40.279242
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 86.60
 ---- batch: 020 ----
mean loss: 87.74
 ---- batch: 030 ----
mean loss: 88.33
train mean loss: 88.10
epoch train time: 0:00:06.840003
elapsed time: 0:26:26.394060
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 10:31:47.119953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.59
 ---- batch: 020 ----
mean loss: 87.84
 ---- batch: 030 ----
mean loss: 86.70
train mean loss: 88.37
epoch train time: 0:00:06.708699
elapsed time: 0:26:33.103522
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 10:31:53.829398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.52
 ---- batch: 020 ----
mean loss: 89.46
 ---- batch: 030 ----
mean loss: 87.33
train mean loss: 88.17
epoch train time: 0:00:06.844665
elapsed time: 0:26:39.948940
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 10:32:00.674825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.76
 ---- batch: 020 ----
mean loss: 88.13
 ---- batch: 030 ----
mean loss: 88.88
train mean loss: 88.66
epoch train time: 0:00:06.844436
elapsed time: 0:26:46.794207
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 10:32:07.520115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.57
 ---- batch: 020 ----
mean loss: 89.18
 ---- batch: 030 ----
mean loss: 88.47
train mean loss: 88.52
epoch train time: 0:00:06.880053
elapsed time: 0:26:53.675107
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 10:32:14.401004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 87.71
 ---- batch: 020 ----
mean loss: 89.91
 ---- batch: 030 ----
mean loss: 86.93
train mean loss: 88.71
epoch train time: 0:00:06.841011
elapsed time: 0:27:00.516984
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 10:32:21.242764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 91.18
 ---- batch: 020 ----
mean loss: 86.78
 ---- batch: 030 ----
mean loss: 87.56
train mean loss: 87.84
epoch train time: 0:00:06.872003
elapsed time: 0:27:07.389753
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 10:32:28.115654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.04
 ---- batch: 020 ----
mean loss: 87.86
 ---- batch: 030 ----
mean loss: 87.58
train mean loss: 88.24
epoch train time: 0:00:06.892632
elapsed time: 0:27:14.283099
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 10:32:35.008993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.60
 ---- batch: 020 ----
mean loss: 86.81
 ---- batch: 030 ----
mean loss: 87.44
train mean loss: 88.14
epoch train time: 0:00:06.801854
elapsed time: 0:27:21.085750
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 10:32:41.811633
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.75
 ---- batch: 020 ----
mean loss: 87.24
 ---- batch: 030 ----
mean loss: 87.43
train mean loss: 87.77
epoch train time: 0:00:06.897163
elapsed time: 0:27:27.983715
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 10:32:48.709589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.58
 ---- batch: 020 ----
mean loss: 87.97
 ---- batch: 030 ----
mean loss: 86.58
train mean loss: 87.88
epoch train time: 0:00:06.783637
elapsed time: 0:27:34.768240
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 10:32:55.494149
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 87.29
 ---- batch: 020 ----
mean loss: 87.14
 ---- batch: 030 ----
mean loss: 86.56
train mean loss: 88.22
epoch train time: 0:00:06.818332
elapsed time: 0:27:41.587407
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 10:33:02.313285
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 87.74
 ---- batch: 020 ----
mean loss: 89.45
 ---- batch: 030 ----
mean loss: 90.48
train mean loss: 88.94
epoch train time: 0:00:06.846072
elapsed time: 0:27:48.434221
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 10:33:09.160124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.42
 ---- batch: 020 ----
mean loss: 86.34
 ---- batch: 030 ----
mean loss: 89.79
train mean loss: 88.19
epoch train time: 0:00:06.844091
elapsed time: 0:27:55.279173
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 10:33:16.005097
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 90.31
 ---- batch: 020 ----
mean loss: 87.81
 ---- batch: 030 ----
mean loss: 85.39
train mean loss: 87.81
epoch train time: 0:00:06.763896
elapsed time: 0:28:02.043932
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 10:33:22.769829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.92
 ---- batch: 020 ----
mean loss: 89.02
 ---- batch: 030 ----
mean loss: 87.02
train mean loss: 88.27
epoch train time: 0:00:06.811121
elapsed time: 0:28:08.855821
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 10:33:29.581721
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.65
 ---- batch: 020 ----
mean loss: 87.01
 ---- batch: 030 ----
mean loss: 85.20
train mean loss: 87.55
epoch train time: 0:00:06.857455
elapsed time: 0:28:15.714003
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 10:33:36.439897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.29
 ---- batch: 020 ----
mean loss: 86.14
 ---- batch: 030 ----
mean loss: 86.43
train mean loss: 87.39
epoch train time: 0:00:06.738130
elapsed time: 0:28:22.452913
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 10:33:43.178797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.91
 ---- batch: 020 ----
mean loss: 89.76
 ---- batch: 030 ----
mean loss: 86.78
train mean loss: 88.38
epoch train time: 0:00:06.848347
elapsed time: 0:28:29.302158
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 10:33:50.028054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 89.56
 ---- batch: 020 ----
mean loss: 86.81
 ---- batch: 030 ----
mean loss: 87.69
train mean loss: 88.19
epoch train time: 0:00:06.693241
elapsed time: 0:28:35.996283
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 10:33:56.722244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 88.62
 ---- batch: 020 ----
mean loss: 88.90
 ---- batch: 030 ----
mean loss: 85.11
train mean loss: 87.97
epoch train time: 0:00:06.682097
elapsed time: 0:28:42.679245
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 10:34:03.405146
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 83.92
 ---- batch: 020 ----
mean loss: 91.05
 ---- batch: 030 ----
mean loss: 90.35
train mean loss: 88.59
epoch train time: 0:00:06.945077
elapsed time: 0:28:49.634643
checkpoint saved in file: log/CMAPSS/FD001/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_9/checkpoint.pth.tar
**** end time: 2019-09-27 10:34:10.360404 ****
