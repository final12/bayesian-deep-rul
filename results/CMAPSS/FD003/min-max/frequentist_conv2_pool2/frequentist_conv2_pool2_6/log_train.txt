Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_6', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8173
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-20 23:32:10.015328 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:32:10.020230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4781.56
 ---- batch: 020 ----
mean loss: 4743.46
 ---- batch: 030 ----
mean loss: 4767.78
 ---- batch: 040 ----
mean loss: 4660.13
train mean loss: 4727.90
epoch train time: 0:00:15.199778
elapsed time: 0:00:15.206040
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:32:25.221406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4628.11
 ---- batch: 020 ----
mean loss: 4503.82
 ---- batch: 030 ----
mean loss: 4417.40
 ---- batch: 040 ----
mean loss: 4446.63
train mean loss: 4486.01
epoch train time: 0:00:00.217225
elapsed time: 0:00:15.423393
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:32:25.438766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4243.99
 ---- batch: 020 ----
mean loss: 4198.03
 ---- batch: 030 ----
mean loss: 4177.93
 ---- batch: 040 ----
mean loss: 4027.70
train mean loss: 4143.17
epoch train time: 0:00:00.213212
elapsed time: 0:00:15.636731
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:32:25.652104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3882.57
 ---- batch: 020 ----
mean loss: 3913.54
 ---- batch: 030 ----
mean loss: 3630.01
 ---- batch: 040 ----
mean loss: 3685.48
train mean loss: 3775.01
epoch train time: 0:00:00.212338
elapsed time: 0:00:15.849201
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:32:25.864569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3583.59
 ---- batch: 020 ----
mean loss: 3468.23
 ---- batch: 030 ----
mean loss: 3431.98
 ---- batch: 040 ----
mean loss: 3333.23
train mean loss: 3443.49
epoch train time: 0:00:00.198271
elapsed time: 0:00:16.047588
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:32:26.062957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3236.85
 ---- batch: 020 ----
mean loss: 3240.20
 ---- batch: 030 ----
mean loss: 3130.03
 ---- batch: 040 ----
mean loss: 3054.29
train mean loss: 3159.00
epoch train time: 0:00:00.194757
elapsed time: 0:00:16.242468
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:32:26.257853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2975.74
 ---- batch: 020 ----
mean loss: 2989.17
 ---- batch: 030 ----
mean loss: 2907.61
 ---- batch: 040 ----
mean loss: 2778.10
train mean loss: 2906.33
epoch train time: 0:00:00.199111
elapsed time: 0:00:16.441792
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:32:26.457163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2778.22
 ---- batch: 020 ----
mean loss: 2665.78
 ---- batch: 030 ----
mean loss: 2691.75
 ---- batch: 040 ----
mean loss: 2606.43
train mean loss: 2680.78
epoch train time: 0:00:00.198879
elapsed time: 0:00:16.640788
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:32:26.656172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2610.37
 ---- batch: 020 ----
mean loss: 2483.84
 ---- batch: 030 ----
mean loss: 2445.40
 ---- batch: 040 ----
mean loss: 2394.27
train mean loss: 2477.34
epoch train time: 0:00:00.209800
elapsed time: 0:00:16.850733
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:32:26.866101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2352.70
 ---- batch: 020 ----
mean loss: 2303.58
 ---- batch: 030 ----
mean loss: 2277.62
 ---- batch: 040 ----
mean loss: 2269.95
train mean loss: 2293.36
epoch train time: 0:00:00.207231
elapsed time: 0:00:17.058085
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:32:27.073487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2177.65
 ---- batch: 020 ----
mean loss: 2156.08
 ---- batch: 030 ----
mean loss: 2127.70
 ---- batch: 040 ----
mean loss: 2043.34
train mean loss: 2127.12
epoch train time: 0:00:00.210252
elapsed time: 0:00:17.268531
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:32:27.283903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2065.30
 ---- batch: 020 ----
mean loss: 1973.59
 ---- batch: 030 ----
mean loss: 1943.97
 ---- batch: 040 ----
mean loss: 1930.13
train mean loss: 1974.92
epoch train time: 0:00:00.210263
elapsed time: 0:00:17.478958
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:32:27.494331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1876.32
 ---- batch: 020 ----
mean loss: 1859.00
 ---- batch: 030 ----
mean loss: 1812.82
 ---- batch: 040 ----
mean loss: 1813.04
train mean loss: 1836.17
epoch train time: 0:00:00.218016
elapsed time: 0:00:17.697126
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:32:27.712509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1750.56
 ---- batch: 020 ----
mean loss: 1727.45
 ---- batch: 030 ----
mean loss: 1695.02
 ---- batch: 040 ----
mean loss: 1681.40
train mean loss: 1712.01
epoch train time: 0:00:00.213673
elapsed time: 0:00:17.910937
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:32:27.926309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1649.52
 ---- batch: 020 ----
mean loss: 1605.01
 ---- batch: 030 ----
mean loss: 1578.00
 ---- batch: 040 ----
mean loss: 1575.69
train mean loss: 1596.47
epoch train time: 0:00:00.209532
elapsed time: 0:00:18.120609
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:32:28.135978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1511.86
 ---- batch: 020 ----
mean loss: 1523.25
 ---- batch: 030 ----
mean loss: 1492.79
 ---- batch: 040 ----
mean loss: 1458.62
train mean loss: 1493.09
epoch train time: 0:00:00.209012
elapsed time: 0:00:18.329761
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:32:28.345133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1417.16
 ---- batch: 020 ----
mean loss: 1406.61
 ---- batch: 030 ----
mean loss: 1396.78
 ---- batch: 040 ----
mean loss: 1381.16
train mean loss: 1400.27
epoch train time: 0:00:00.211630
elapsed time: 0:00:18.541527
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:32:28.556898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1337.39
 ---- batch: 020 ----
mean loss: 1327.35
 ---- batch: 030 ----
mean loss: 1308.01
 ---- batch: 040 ----
mean loss: 1292.61
train mean loss: 1314.77
epoch train time: 0:00:00.208632
elapsed time: 0:00:18.750294
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:32:28.765669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1279.58
 ---- batch: 020 ----
mean loss: 1225.36
 ---- batch: 030 ----
mean loss: 1245.82
 ---- batch: 040 ----
mean loss: 1214.62
train mean loss: 1237.90
epoch train time: 0:00:00.205153
elapsed time: 0:00:18.955568
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:32:28.970935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1181.48
 ---- batch: 020 ----
mean loss: 1184.66
 ---- batch: 030 ----
mean loss: 1180.37
 ---- batch: 040 ----
mean loss: 1135.79
train mean loss: 1170.58
epoch train time: 0:00:00.200533
elapsed time: 0:00:19.156223
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:32:29.171591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1126.60
 ---- batch: 020 ----
mean loss: 1119.54
 ---- batch: 030 ----
mean loss: 1103.21
 ---- batch: 040 ----
mean loss: 1089.61
train mean loss: 1108.10
epoch train time: 0:00:00.201026
elapsed time: 0:00:19.357378
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:32:29.372746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1077.20
 ---- batch: 020 ----
mean loss: 1048.83
 ---- batch: 030 ----
mean loss: 1046.88
 ---- batch: 040 ----
mean loss: 1045.05
train mean loss: 1052.25
epoch train time: 0:00:00.202394
elapsed time: 0:00:19.559909
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:32:29.575292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1024.58
 ---- batch: 020 ----
mean loss: 1015.80
 ---- batch: 030 ----
mean loss: 997.63
 ---- batch: 040 ----
mean loss: 978.26
train mean loss: 1003.09
epoch train time: 0:00:00.215044
elapsed time: 0:00:19.775083
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:32:29.790452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 986.68
 ---- batch: 020 ----
mean loss: 973.49
 ---- batch: 030 ----
mean loss: 952.77
 ---- batch: 040 ----
mean loss: 936.47
train mean loss: 959.13
epoch train time: 0:00:00.203328
elapsed time: 0:00:19.978527
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:32:29.993906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 945.84
 ---- batch: 020 ----
mean loss: 926.18
 ---- batch: 030 ----
mean loss: 921.16
 ---- batch: 040 ----
mean loss: 893.93
train mean loss: 920.39
epoch train time: 0:00:00.203446
elapsed time: 0:00:20.182098
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:32:30.197466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.90
 ---- batch: 020 ----
mean loss: 904.19
 ---- batch: 030 ----
mean loss: 882.57
 ---- batch: 040 ----
mean loss: 870.63
train mean loss: 885.36
epoch train time: 0:00:00.203816
elapsed time: 0:00:20.386028
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:32:30.401395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.24
 ---- batch: 020 ----
mean loss: 847.08
 ---- batch: 030 ----
mean loss: 852.29
 ---- batch: 040 ----
mean loss: 850.27
train mean loss: 854.89
epoch train time: 0:00:00.205400
elapsed time: 0:00:20.591587
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:32:30.606971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.88
 ---- batch: 020 ----
mean loss: 833.88
 ---- batch: 030 ----
mean loss: 824.84
 ---- batch: 040 ----
mean loss: 824.38
train mean loss: 828.44
epoch train time: 0:00:00.210041
elapsed time: 0:00:20.801792
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:32:30.817174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.28
 ---- batch: 020 ----
mean loss: 796.57
 ---- batch: 030 ----
mean loss: 801.60
 ---- batch: 040 ----
mean loss: 798.35
train mean loss: 805.08
epoch train time: 0:00:00.215095
elapsed time: 0:00:21.017022
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:32:31.032417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 792.08
 ---- batch: 020 ----
mean loss: 773.68
 ---- batch: 030 ----
mean loss: 795.29
 ---- batch: 040 ----
mean loss: 776.13
train mean loss: 785.02
epoch train time: 0:00:00.216601
elapsed time: 0:00:21.233786
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:32:31.249156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.77
 ---- batch: 020 ----
mean loss: 763.42
 ---- batch: 030 ----
mean loss: 771.59
 ---- batch: 040 ----
mean loss: 764.06
train mean loss: 766.64
epoch train time: 0:00:00.210267
elapsed time: 0:00:21.444204
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:32:31.459589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 768.87
 ---- batch: 020 ----
mean loss: 743.93
 ---- batch: 030 ----
mean loss: 758.89
 ---- batch: 040 ----
mean loss: 739.58
train mean loss: 751.19
epoch train time: 0:00:00.212473
elapsed time: 0:00:21.656810
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:32:31.672179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.91
 ---- batch: 020 ----
mean loss: 723.67
 ---- batch: 030 ----
mean loss: 724.93
 ---- batch: 040 ----
mean loss: 747.83
train mean loss: 737.90
epoch train time: 0:00:00.213763
elapsed time: 0:00:21.870695
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:32:31.886067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 728.67
 ---- batch: 020 ----
mean loss: 728.22
 ---- batch: 030 ----
mean loss: 731.88
 ---- batch: 040 ----
mean loss: 721.41
train mean loss: 726.38
epoch train time: 0:00:00.208092
elapsed time: 0:00:22.078906
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:32:32.094298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 717.24
 ---- batch: 020 ----
mean loss: 719.25
 ---- batch: 030 ----
mean loss: 704.92
 ---- batch: 040 ----
mean loss: 719.15
train mean loss: 717.05
epoch train time: 0:00:00.205934
elapsed time: 0:00:22.284987
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:32:32.300359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.18
 ---- batch: 020 ----
mean loss: 699.59
 ---- batch: 030 ----
mean loss: 702.40
 ---- batch: 040 ----
mean loss: 724.09
train mean loss: 707.91
epoch train time: 0:00:00.204174
elapsed time: 0:00:22.489292
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:32:32.504658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.94
 ---- batch: 020 ----
mean loss: 708.45
 ---- batch: 030 ----
mean loss: 702.60
 ---- batch: 040 ----
mean loss: 703.58
train mean loss: 701.26
epoch train time: 0:00:00.204397
elapsed time: 0:00:22.693803
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:32:32.709172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.34
 ---- batch: 020 ----
mean loss: 687.56
 ---- batch: 030 ----
mean loss: 686.64
 ---- batch: 040 ----
mean loss: 712.85
train mean loss: 695.34
epoch train time: 0:00:00.212313
elapsed time: 0:00:22.906244
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:32:32.921622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.51
 ---- batch: 020 ----
mean loss: 677.93
 ---- batch: 030 ----
mean loss: 709.07
 ---- batch: 040 ----
mean loss: 686.82
train mean loss: 689.92
epoch train time: 0:00:00.205547
elapsed time: 0:00:23.111931
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:32:33.127303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 699.18
 ---- batch: 020 ----
mean loss: 687.26
 ---- batch: 030 ----
mean loss: 684.95
 ---- batch: 040 ----
mean loss: 677.70
train mean loss: 685.24
epoch train time: 0:00:00.201017
elapsed time: 0:00:23.313087
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:32:33.328460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.99
 ---- batch: 020 ----
mean loss: 698.85
 ---- batch: 030 ----
mean loss: 684.43
 ---- batch: 040 ----
mean loss: 675.06
train mean loss: 681.99
epoch train time: 0:00:00.201713
elapsed time: 0:00:23.514919
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:32:33.530287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.87
 ---- batch: 020 ----
mean loss: 680.20
 ---- batch: 030 ----
mean loss: 673.12
 ---- batch: 040 ----
mean loss: 666.01
train mean loss: 679.08
epoch train time: 0:00:00.204382
elapsed time: 0:00:23.719433
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:32:33.734803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 676.84
 ---- batch: 020 ----
mean loss: 677.05
 ---- batch: 030 ----
mean loss: 679.90
 ---- batch: 040 ----
mean loss: 678.46
train mean loss: 676.53
epoch train time: 0:00:00.205190
elapsed time: 0:00:23.924750
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:32:33.940134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.64
 ---- batch: 020 ----
mean loss: 663.93
 ---- batch: 030 ----
mean loss: 674.79
 ---- batch: 040 ----
mean loss: 668.16
train mean loss: 674.97
epoch train time: 0:00:00.203893
elapsed time: 0:00:24.128785
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:32:34.144156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.09
 ---- batch: 020 ----
mean loss: 647.64
 ---- batch: 030 ----
mean loss: 677.05
 ---- batch: 040 ----
mean loss: 687.27
train mean loss: 672.51
epoch train time: 0:00:00.204623
elapsed time: 0:00:24.333530
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:32:34.348899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.20
 ---- batch: 020 ----
mean loss: 653.93
 ---- batch: 030 ----
mean loss: 670.84
 ---- batch: 040 ----
mean loss: 676.19
train mean loss: 670.98
epoch train time: 0:00:00.208583
elapsed time: 0:00:24.542235
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:32:34.557620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.63
 ---- batch: 020 ----
mean loss: 677.67
 ---- batch: 030 ----
mean loss: 657.95
 ---- batch: 040 ----
mean loss: 681.64
train mean loss: 667.65
epoch train time: 0:00:00.213070
elapsed time: 0:00:24.755502
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:32:34.770878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 659.28
 ---- batch: 020 ----
mean loss: 667.66
 ---- batch: 030 ----
mean loss: 644.36
 ---- batch: 040 ----
mean loss: 641.41
train mean loss: 649.46
epoch train time: 0:00:00.214969
elapsed time: 0:00:24.970615
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:32:34.985985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 622.23
 ---- batch: 020 ----
mean loss: 613.94
 ---- batch: 030 ----
mean loss: 563.39
 ---- batch: 040 ----
mean loss: 521.06
train mean loss: 572.22
epoch train time: 0:00:00.205518
elapsed time: 0:00:25.176250
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:32:35.191620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.95
 ---- batch: 020 ----
mean loss: 440.48
 ---- batch: 030 ----
mean loss: 439.90
 ---- batch: 040 ----
mean loss: 430.67
train mean loss: 440.58
epoch train time: 0:00:00.207394
elapsed time: 0:00:25.383772
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:32:35.399149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.30
 ---- batch: 020 ----
mean loss: 423.85
 ---- batch: 030 ----
mean loss: 410.42
 ---- batch: 040 ----
mean loss: 421.50
train mean loss: 421.23
epoch train time: 0:00:00.212346
elapsed time: 0:00:25.596271
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:32:35.611642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.71
 ---- batch: 020 ----
mean loss: 406.91
 ---- batch: 030 ----
mean loss: 409.66
 ---- batch: 040 ----
mean loss: 407.05
train mean loss: 408.68
epoch train time: 0:00:00.211643
elapsed time: 0:00:25.808193
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:32:35.823568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.87
 ---- batch: 020 ----
mean loss: 402.82
 ---- batch: 030 ----
mean loss: 395.87
 ---- batch: 040 ----
mean loss: 389.14
train mean loss: 399.64
epoch train time: 0:00:00.221606
elapsed time: 0:00:26.029928
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:32:36.045314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.07
 ---- batch: 020 ----
mean loss: 393.87
 ---- batch: 030 ----
mean loss: 390.90
 ---- batch: 040 ----
mean loss: 387.20
train mean loss: 390.41
epoch train time: 0:00:00.213477
elapsed time: 0:00:26.243573
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:32:36.258943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.25
 ---- batch: 020 ----
mean loss: 386.10
 ---- batch: 030 ----
mean loss: 384.87
 ---- batch: 040 ----
mean loss: 378.94
train mean loss: 383.08
epoch train time: 0:00:00.203833
elapsed time: 0:00:26.447538
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:32:36.462935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.38
 ---- batch: 020 ----
mean loss: 370.42
 ---- batch: 030 ----
mean loss: 376.70
 ---- batch: 040 ----
mean loss: 372.69
train mean loss: 376.38
epoch train time: 0:00:00.203063
elapsed time: 0:00:26.650766
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:32:36.666134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.58
 ---- batch: 020 ----
mean loss: 364.25
 ---- batch: 030 ----
mean loss: 371.24
 ---- batch: 040 ----
mean loss: 373.51
train mean loss: 370.78
epoch train time: 0:00:00.201809
elapsed time: 0:00:26.852689
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:32:36.868057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.85
 ---- batch: 020 ----
mean loss: 364.68
 ---- batch: 030 ----
mean loss: 364.82
 ---- batch: 040 ----
mean loss: 361.60
train mean loss: 365.54
epoch train time: 0:00:00.212521
elapsed time: 0:00:27.065329
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:32:37.080714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.72
 ---- batch: 020 ----
mean loss: 366.49
 ---- batch: 030 ----
mean loss: 360.75
 ---- batch: 040 ----
mean loss: 360.30
train mean loss: 361.09
epoch train time: 0:00:00.200878
elapsed time: 0:00:27.266337
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:32:37.281704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.43
 ---- batch: 020 ----
mean loss: 361.33
 ---- batch: 030 ----
mean loss: 359.00
 ---- batch: 040 ----
mean loss: 356.61
train mean loss: 356.68
epoch train time: 0:00:00.200095
elapsed time: 0:00:27.466557
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:32:37.481926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.80
 ---- batch: 020 ----
mean loss: 351.10
 ---- batch: 030 ----
mean loss: 358.83
 ---- batch: 040 ----
mean loss: 347.49
train mean loss: 353.31
epoch train time: 0:00:00.200205
elapsed time: 0:00:27.666887
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:32:37.682260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.14
 ---- batch: 020 ----
mean loss: 354.22
 ---- batch: 030 ----
mean loss: 345.54
 ---- batch: 040 ----
mean loss: 354.42
train mean loss: 350.16
epoch train time: 0:00:00.204688
elapsed time: 0:00:27.871696
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:32:37.887064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.19
 ---- batch: 020 ----
mean loss: 349.51
 ---- batch: 030 ----
mean loss: 345.24
 ---- batch: 040 ----
mean loss: 345.04
train mean loss: 347.17
epoch train time: 0:00:00.205838
elapsed time: 0:00:28.077673
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:32:38.093061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.63
 ---- batch: 020 ----
mean loss: 348.86
 ---- batch: 030 ----
mean loss: 348.33
 ---- batch: 040 ----
mean loss: 342.82
train mean loss: 344.56
epoch train time: 0:00:00.214267
elapsed time: 0:00:28.292082
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:32:38.307453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.42
 ---- batch: 020 ----
mean loss: 344.60
 ---- batch: 030 ----
mean loss: 346.04
 ---- batch: 040 ----
mean loss: 330.43
train mean loss: 342.19
epoch train time: 0:00:00.216022
elapsed time: 0:00:28.508230
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:32:38.523605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.66
 ---- batch: 020 ----
mean loss: 341.68
 ---- batch: 030 ----
mean loss: 341.88
 ---- batch: 040 ----
mean loss: 337.39
train mean loss: 340.64
epoch train time: 0:00:00.213299
elapsed time: 0:00:28.721676
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:32:38.737047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.28
 ---- batch: 020 ----
mean loss: 323.21
 ---- batch: 030 ----
mean loss: 342.45
 ---- batch: 040 ----
mean loss: 341.55
train mean loss: 338.59
epoch train time: 0:00:00.213782
elapsed time: 0:00:28.935594
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:32:38.950993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.95
 ---- batch: 020 ----
mean loss: 338.26
 ---- batch: 030 ----
mean loss: 332.91
 ---- batch: 040 ----
mean loss: 337.99
train mean loss: 336.88
epoch train time: 0:00:00.210298
elapsed time: 0:00:29.146042
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:32:39.161427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.73
 ---- batch: 020 ----
mean loss: 323.92
 ---- batch: 030 ----
mean loss: 338.35
 ---- batch: 040 ----
mean loss: 341.71
train mean loss: 335.43
epoch train time: 0:00:00.209979
elapsed time: 0:00:29.356156
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:32:39.371527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.87
 ---- batch: 020 ----
mean loss: 332.31
 ---- batch: 030 ----
mean loss: 325.49
 ---- batch: 040 ----
mean loss: 330.47
train mean loss: 334.29
epoch train time: 0:00:00.212567
elapsed time: 0:00:29.568845
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:32:39.584246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.74
 ---- batch: 020 ----
mean loss: 336.39
 ---- batch: 030 ----
mean loss: 337.52
 ---- batch: 040 ----
mean loss: 326.60
train mean loss: 332.97
epoch train time: 0:00:00.209054
elapsed time: 0:00:29.778049
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:32:39.793417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.35
 ---- batch: 020 ----
mean loss: 335.86
 ---- batch: 030 ----
mean loss: 333.96
 ---- batch: 040 ----
mean loss: 329.13
train mean loss: 332.06
epoch train time: 0:00:00.218257
elapsed time: 0:00:29.996442
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:32:40.011814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.55
 ---- batch: 020 ----
mean loss: 334.28
 ---- batch: 030 ----
mean loss: 329.67
 ---- batch: 040 ----
mean loss: 333.30
train mean loss: 330.74
epoch train time: 0:00:00.205743
elapsed time: 0:00:30.202309
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:32:40.217678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.15
 ---- batch: 020 ----
mean loss: 321.34
 ---- batch: 030 ----
mean loss: 329.78
 ---- batch: 040 ----
mean loss: 330.82
train mean loss: 330.04
epoch train time: 0:00:00.203724
elapsed time: 0:00:30.406153
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:32:40.421522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.94
 ---- batch: 020 ----
mean loss: 328.87
 ---- batch: 030 ----
mean loss: 318.43
 ---- batch: 040 ----
mean loss: 335.91
train mean loss: 329.91
epoch train time: 0:00:00.200362
elapsed time: 0:00:30.606663
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:32:40.622036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.06
 ---- batch: 020 ----
mean loss: 330.14
 ---- batch: 030 ----
mean loss: 323.72
 ---- batch: 040 ----
mean loss: 337.37
train mean loss: 328.35
epoch train time: 0:00:00.206542
elapsed time: 0:00:30.813362
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:32:40.828753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.49
 ---- batch: 020 ----
mean loss: 328.00
 ---- batch: 030 ----
mean loss: 328.23
 ---- batch: 040 ----
mean loss: 329.75
train mean loss: 328.07
epoch train time: 0:00:00.204571
elapsed time: 0:00:31.018072
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:32:41.033440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.31
 ---- batch: 020 ----
mean loss: 332.14
 ---- batch: 030 ----
mean loss: 321.77
 ---- batch: 040 ----
mean loss: 328.38
train mean loss: 327.38
epoch train time: 0:00:00.203982
elapsed time: 0:00:31.222176
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:32:41.237558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.56
 ---- batch: 020 ----
mean loss: 327.24
 ---- batch: 030 ----
mean loss: 323.26
 ---- batch: 040 ----
mean loss: 328.87
train mean loss: 326.72
epoch train time: 0:00:00.207986
elapsed time: 0:00:31.430335
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:32:41.445709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.05
 ---- batch: 020 ----
mean loss: 324.07
 ---- batch: 030 ----
mean loss: 334.48
 ---- batch: 040 ----
mean loss: 317.41
train mean loss: 326.32
epoch train time: 0:00:00.206804
elapsed time: 0:00:31.637277
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:32:41.652644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.31
 ---- batch: 020 ----
mean loss: 327.63
 ---- batch: 030 ----
mean loss: 334.16
 ---- batch: 040 ----
mean loss: 314.64
train mean loss: 325.78
epoch train time: 0:00:00.205552
elapsed time: 0:00:31.842945
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:32:41.858313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.66
 ---- batch: 020 ----
mean loss: 321.03
 ---- batch: 030 ----
mean loss: 321.59
 ---- batch: 040 ----
mean loss: 327.96
train mean loss: 324.92
epoch train time: 0:00:00.208131
elapsed time: 0:00:32.051198
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:32:42.066590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.48
 ---- batch: 020 ----
mean loss: 332.90
 ---- batch: 030 ----
mean loss: 320.53
 ---- batch: 040 ----
mean loss: 329.23
train mean loss: 325.05
epoch train time: 0:00:00.211456
elapsed time: 0:00:32.262801
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:32:42.278171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.35
 ---- batch: 020 ----
mean loss: 328.15
 ---- batch: 030 ----
mean loss: 315.52
 ---- batch: 040 ----
mean loss: 330.99
train mean loss: 324.19
epoch train time: 0:00:00.210384
elapsed time: 0:00:32.473311
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:32:42.488684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.89
 ---- batch: 020 ----
mean loss: 325.52
 ---- batch: 030 ----
mean loss: 321.21
 ---- batch: 040 ----
mean loss: 326.23
train mean loss: 323.98
epoch train time: 0:00:00.214352
elapsed time: 0:00:32.687791
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:32:42.703161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.34
 ---- batch: 020 ----
mean loss: 323.50
 ---- batch: 030 ----
mean loss: 327.41
 ---- batch: 040 ----
mean loss: 330.10
train mean loss: 323.80
epoch train time: 0:00:00.213188
elapsed time: 0:00:32.901135
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:32:42.916527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.07
 ---- batch: 020 ----
mean loss: 327.29
 ---- batch: 030 ----
mean loss: 315.22
 ---- batch: 040 ----
mean loss: 325.63
train mean loss: 323.68
epoch train time: 0:00:00.206744
elapsed time: 0:00:33.108065
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:32:43.123435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.07
 ---- batch: 020 ----
mean loss: 314.48
 ---- batch: 030 ----
mean loss: 332.32
 ---- batch: 040 ----
mean loss: 323.63
train mean loss: 323.25
epoch train time: 0:00:00.212774
elapsed time: 0:00:33.320969
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:32:43.336341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.47
 ---- batch: 020 ----
mean loss: 325.79
 ---- batch: 030 ----
mean loss: 326.28
 ---- batch: 040 ----
mean loss: 318.89
train mean loss: 322.49
epoch train time: 0:00:00.210289
elapsed time: 0:00:33.531388
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:32:43.546760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.40
 ---- batch: 020 ----
mean loss: 317.56
 ---- batch: 030 ----
mean loss: 323.48
 ---- batch: 040 ----
mean loss: 323.87
train mean loss: 322.33
epoch train time: 0:00:00.206978
elapsed time: 0:00:33.738486
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:32:43.753855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.59
 ---- batch: 020 ----
mean loss: 320.11
 ---- batch: 030 ----
mean loss: 321.98
 ---- batch: 040 ----
mean loss: 325.10
train mean loss: 321.84
epoch train time: 0:00:00.204739
elapsed time: 0:00:33.943353
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:32:43.958722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.13
 ---- batch: 020 ----
mean loss: 326.42
 ---- batch: 030 ----
mean loss: 321.89
 ---- batch: 040 ----
mean loss: 322.78
train mean loss: 321.62
epoch train time: 0:00:00.200072
elapsed time: 0:00:34.143539
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:32:44.158907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.90
 ---- batch: 020 ----
mean loss: 327.14
 ---- batch: 030 ----
mean loss: 320.74
 ---- batch: 040 ----
mean loss: 319.91
train mean loss: 321.76
epoch train time: 0:00:00.199077
elapsed time: 0:00:34.342734
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:32:44.358102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.98
 ---- batch: 020 ----
mean loss: 320.68
 ---- batch: 030 ----
mean loss: 310.46
 ---- batch: 040 ----
mean loss: 327.93
train mean loss: 321.08
epoch train time: 0:00:00.201235
elapsed time: 0:00:34.544091
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:32:44.559473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.32
 ---- batch: 020 ----
mean loss: 326.70
 ---- batch: 030 ----
mean loss: 325.88
 ---- batch: 040 ----
mean loss: 315.69
train mean loss: 320.81
epoch train time: 0:00:00.201309
elapsed time: 0:00:34.745531
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:32:44.760901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.89
 ---- batch: 020 ----
mean loss: 322.95
 ---- batch: 030 ----
mean loss: 310.07
 ---- batch: 040 ----
mean loss: 321.42
train mean loss: 320.65
epoch train time: 0:00:00.205999
elapsed time: 0:00:34.951647
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:32:44.967029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.34
 ---- batch: 020 ----
mean loss: 319.66
 ---- batch: 030 ----
mean loss: 316.35
 ---- batch: 040 ----
mean loss: 328.16
train mean loss: 320.13
epoch train time: 0:00:00.194621
elapsed time: 0:00:35.146393
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:32:45.161770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.75
 ---- batch: 020 ----
mean loss: 320.39
 ---- batch: 030 ----
mean loss: 322.27
 ---- batch: 040 ----
mean loss: 320.31
train mean loss: 319.76
epoch train time: 0:00:00.194263
elapsed time: 0:00:35.340776
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:32:45.356140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.75
 ---- batch: 020 ----
mean loss: 319.98
 ---- batch: 030 ----
mean loss: 325.93
 ---- batch: 040 ----
mean loss: 315.19
train mean loss: 319.58
epoch train time: 0:00:00.195960
elapsed time: 0:00:35.536842
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:32:45.552208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.61
 ---- batch: 020 ----
mean loss: 321.43
 ---- batch: 030 ----
mean loss: 314.44
 ---- batch: 040 ----
mean loss: 326.32
train mean loss: 319.27
epoch train time: 0:00:00.196551
elapsed time: 0:00:35.733530
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:32:45.748902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.30
 ---- batch: 020 ----
mean loss: 321.75
 ---- batch: 030 ----
mean loss: 325.37
 ---- batch: 040 ----
mean loss: 315.80
train mean loss: 319.49
epoch train time: 0:00:00.214062
elapsed time: 0:00:35.947727
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:32:45.963100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.20
 ---- batch: 020 ----
mean loss: 320.74
 ---- batch: 030 ----
mean loss: 321.12
 ---- batch: 040 ----
mean loss: 312.57
train mean loss: 319.23
epoch train time: 0:00:00.212160
elapsed time: 0:00:36.160011
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:32:46.175379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.24
 ---- batch: 020 ----
mean loss: 327.40
 ---- batch: 030 ----
mean loss: 324.37
 ---- batch: 040 ----
mean loss: 307.04
train mean loss: 318.55
epoch train time: 0:00:00.216525
elapsed time: 0:00:36.376698
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:32:46.392069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.84
 ---- batch: 020 ----
mean loss: 325.26
 ---- batch: 030 ----
mean loss: 315.96
 ---- batch: 040 ----
mean loss: 315.56
train mean loss: 318.75
epoch train time: 0:00:00.216626
elapsed time: 0:00:36.593451
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:32:46.608822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.61
 ---- batch: 020 ----
mean loss: 319.99
 ---- batch: 030 ----
mean loss: 308.24
 ---- batch: 040 ----
mean loss: 317.06
train mean loss: 318.56
epoch train time: 0:00:00.215217
elapsed time: 0:00:36.808789
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:32:46.824158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.52
 ---- batch: 020 ----
mean loss: 323.37
 ---- batch: 030 ----
mean loss: 317.51
 ---- batch: 040 ----
mean loss: 315.16
train mean loss: 318.23
epoch train time: 0:00:00.210470
elapsed time: 0:00:37.019378
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:32:47.034764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.67
 ---- batch: 020 ----
mean loss: 322.23
 ---- batch: 030 ----
mean loss: 311.11
 ---- batch: 040 ----
mean loss: 315.96
train mean loss: 317.73
epoch train time: 0:00:00.211149
elapsed time: 0:00:37.230689
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:32:47.246052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.13
 ---- batch: 020 ----
mean loss: 331.11
 ---- batch: 030 ----
mean loss: 314.85
 ---- batch: 040 ----
mean loss: 309.56
train mean loss: 317.39
epoch train time: 0:00:00.208628
elapsed time: 0:00:37.439428
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:32:47.454811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.08
 ---- batch: 020 ----
mean loss: 324.08
 ---- batch: 030 ----
mean loss: 310.31
 ---- batch: 040 ----
mean loss: 311.57
train mean loss: 317.55
epoch train time: 0:00:00.206183
elapsed time: 0:00:37.645739
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:32:47.661120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.97
 ---- batch: 020 ----
mean loss: 324.04
 ---- batch: 030 ----
mean loss: 321.41
 ---- batch: 040 ----
mean loss: 311.66
train mean loss: 317.63
epoch train time: 0:00:00.206709
elapsed time: 0:00:37.852576
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:32:47.867945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.84
 ---- batch: 020 ----
mean loss: 303.19
 ---- batch: 030 ----
mean loss: 325.21
 ---- batch: 040 ----
mean loss: 325.46
train mean loss: 316.60
epoch train time: 0:00:00.203193
elapsed time: 0:00:38.055885
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:32:48.071265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.12
 ---- batch: 020 ----
mean loss: 309.80
 ---- batch: 030 ----
mean loss: 318.05
 ---- batch: 040 ----
mean loss: 318.15
train mean loss: 316.64
epoch train time: 0:00:00.202841
elapsed time: 0:00:38.258858
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:32:48.274229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.21
 ---- batch: 020 ----
mean loss: 312.00
 ---- batch: 030 ----
mean loss: 316.84
 ---- batch: 040 ----
mean loss: 321.04
train mean loss: 316.65
epoch train time: 0:00:00.204738
elapsed time: 0:00:38.463715
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:32:48.479083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.74
 ---- batch: 020 ----
mean loss: 324.77
 ---- batch: 030 ----
mean loss: 317.67
 ---- batch: 040 ----
mean loss: 312.38
train mean loss: 316.18
epoch train time: 0:00:00.200911
elapsed time: 0:00:38.664752
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:32:48.680133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.71
 ---- batch: 020 ----
mean loss: 316.62
 ---- batch: 030 ----
mean loss: 312.30
 ---- batch: 040 ----
mean loss: 312.01
train mean loss: 316.51
epoch train time: 0:00:00.204798
elapsed time: 0:00:38.869683
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:32:48.885053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.66
 ---- batch: 020 ----
mean loss: 313.40
 ---- batch: 030 ----
mean loss: 318.64
 ---- batch: 040 ----
mean loss: 314.84
train mean loss: 315.99
epoch train time: 0:00:00.207302
elapsed time: 0:00:39.077128
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:32:49.092501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.70
 ---- batch: 020 ----
mean loss: 316.41
 ---- batch: 030 ----
mean loss: 310.90
 ---- batch: 040 ----
mean loss: 312.05
train mean loss: 315.93
epoch train time: 0:00:00.207416
elapsed time: 0:00:39.284664
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:32:49.300032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.19
 ---- batch: 020 ----
mean loss: 324.44
 ---- batch: 030 ----
mean loss: 315.59
 ---- batch: 040 ----
mean loss: 311.97
train mean loss: 315.45
epoch train time: 0:00:00.208447
elapsed time: 0:00:39.493239
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:32:49.508639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.64
 ---- batch: 020 ----
mean loss: 308.57
 ---- batch: 030 ----
mean loss: 314.63
 ---- batch: 040 ----
mean loss: 315.09
train mean loss: 315.73
epoch train time: 0:00:00.217307
elapsed time: 0:00:39.710699
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:32:49.726071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.38
 ---- batch: 020 ----
mean loss: 314.96
 ---- batch: 030 ----
mean loss: 312.42
 ---- batch: 040 ----
mean loss: 319.29
train mean loss: 315.07
epoch train time: 0:00:00.215049
elapsed time: 0:00:39.925905
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:32:49.941278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.34
 ---- batch: 020 ----
mean loss: 315.86
 ---- batch: 030 ----
mean loss: 314.37
 ---- batch: 040 ----
mean loss: 312.90
train mean loss: 315.04
epoch train time: 0:00:00.214213
elapsed time: 0:00:40.140257
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:32:50.155633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.14
 ---- batch: 020 ----
mean loss: 322.87
 ---- batch: 030 ----
mean loss: 314.24
 ---- batch: 040 ----
mean loss: 311.08
train mean loss: 314.71
epoch train time: 0:00:00.214689
elapsed time: 0:00:40.355078
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:32:50.370451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.74
 ---- batch: 020 ----
mean loss: 311.11
 ---- batch: 030 ----
mean loss: 316.45
 ---- batch: 040 ----
mean loss: 323.73
train mean loss: 314.52
epoch train time: 0:00:00.217173
elapsed time: 0:00:40.572378
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:32:50.587761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.06
 ---- batch: 020 ----
mean loss: 309.99
 ---- batch: 030 ----
mean loss: 312.73
 ---- batch: 040 ----
mean loss: 315.50
train mean loss: 314.48
epoch train time: 0:00:00.214951
elapsed time: 0:00:40.787910
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:32:50.803304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.27
 ---- batch: 020 ----
mean loss: 311.79
 ---- batch: 030 ----
mean loss: 313.87
 ---- batch: 040 ----
mean loss: 312.12
train mean loss: 314.33
epoch train time: 0:00:00.213315
elapsed time: 0:00:41.001375
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:32:51.016765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.71
 ---- batch: 020 ----
mean loss: 319.23
 ---- batch: 030 ----
mean loss: 311.92
 ---- batch: 040 ----
mean loss: 329.25
train mean loss: 314.17
epoch train time: 0:00:00.212405
elapsed time: 0:00:41.213919
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:32:51.229302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.35
 ---- batch: 020 ----
mean loss: 308.60
 ---- batch: 030 ----
mean loss: 319.64
 ---- batch: 040 ----
mean loss: 307.93
train mean loss: 313.94
epoch train time: 0:00:00.209056
elapsed time: 0:00:41.423116
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:32:51.438483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.12
 ---- batch: 020 ----
mean loss: 306.92
 ---- batch: 030 ----
mean loss: 317.89
 ---- batch: 040 ----
mean loss: 315.50
train mean loss: 314.35
epoch train time: 0:00:00.200669
elapsed time: 0:00:41.623899
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:32:51.639284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.03
 ---- batch: 020 ----
mean loss: 309.90
 ---- batch: 030 ----
mean loss: 306.35
 ---- batch: 040 ----
mean loss: 315.77
train mean loss: 313.56
epoch train time: 0:00:00.203446
elapsed time: 0:00:41.827475
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:32:51.842843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.74
 ---- batch: 020 ----
mean loss: 314.56
 ---- batch: 030 ----
mean loss: 321.41
 ---- batch: 040 ----
mean loss: 315.94
train mean loss: 313.39
epoch train time: 0:00:00.199480
elapsed time: 0:00:42.027068
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:32:52.042449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.85
 ---- batch: 020 ----
mean loss: 307.73
 ---- batch: 030 ----
mean loss: 316.07
 ---- batch: 040 ----
mean loss: 308.99
train mean loss: 313.39
epoch train time: 0:00:00.198064
elapsed time: 0:00:42.225262
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:32:52.240629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.76
 ---- batch: 020 ----
mean loss: 313.22
 ---- batch: 030 ----
mean loss: 308.19
 ---- batch: 040 ----
mean loss: 315.80
train mean loss: 313.01
epoch train time: 0:00:00.203415
elapsed time: 0:00:42.428790
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:32:52.444156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.01
 ---- batch: 020 ----
mean loss: 312.55
 ---- batch: 030 ----
mean loss: 319.39
 ---- batch: 040 ----
mean loss: 317.54
train mean loss: 313.13
epoch train time: 0:00:00.197181
elapsed time: 0:00:42.626100
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:32:52.641470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.42
 ---- batch: 020 ----
mean loss: 311.65
 ---- batch: 030 ----
mean loss: 319.12
 ---- batch: 040 ----
mean loss: 305.10
train mean loss: 312.81
epoch train time: 0:00:00.205916
elapsed time: 0:00:42.832139
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:32:52.847509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.10
 ---- batch: 020 ----
mean loss: 316.26
 ---- batch: 030 ----
mean loss: 308.67
 ---- batch: 040 ----
mean loss: 314.09
train mean loss: 312.72
epoch train time: 0:00:00.201159
elapsed time: 0:00:43.033418
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:32:53.048787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.71
 ---- batch: 020 ----
mean loss: 313.33
 ---- batch: 030 ----
mean loss: 309.52
 ---- batch: 040 ----
mean loss: 316.80
train mean loss: 312.65
epoch train time: 0:00:00.200385
elapsed time: 0:00:43.233916
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:32:53.249281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.97
 ---- batch: 020 ----
mean loss: 309.12
 ---- batch: 030 ----
mean loss: 312.27
 ---- batch: 040 ----
mean loss: 314.09
train mean loss: 312.29
epoch train time: 0:00:00.209117
elapsed time: 0:00:43.443163
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:32:53.458533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.93
 ---- batch: 020 ----
mean loss: 318.29
 ---- batch: 030 ----
mean loss: 313.36
 ---- batch: 040 ----
mean loss: 307.81
train mean loss: 312.37
epoch train time: 0:00:00.209324
elapsed time: 0:00:43.652610
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:32:53.667981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.94
 ---- batch: 020 ----
mean loss: 308.96
 ---- batch: 030 ----
mean loss: 316.35
 ---- batch: 040 ----
mean loss: 306.62
train mean loss: 311.79
epoch train time: 0:00:00.214488
elapsed time: 0:00:43.867221
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:32:53.882592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.98
 ---- batch: 020 ----
mean loss: 307.53
 ---- batch: 030 ----
mean loss: 313.93
 ---- batch: 040 ----
mean loss: 309.58
train mean loss: 311.74
epoch train time: 0:00:00.210960
elapsed time: 0:00:44.078303
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:32:54.093673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.81
 ---- batch: 020 ----
mean loss: 312.32
 ---- batch: 030 ----
mean loss: 309.55
 ---- batch: 040 ----
mean loss: 318.36
train mean loss: 311.44
epoch train time: 0:00:00.210341
elapsed time: 0:00:44.288768
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:32:54.304140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.27
 ---- batch: 020 ----
mean loss: 310.38
 ---- batch: 030 ----
mean loss: 313.07
 ---- batch: 040 ----
mean loss: 315.00
train mean loss: 311.74
epoch train time: 0:00:00.209564
elapsed time: 0:00:44.498458
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:32:54.513844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.03
 ---- batch: 020 ----
mean loss: 314.42
 ---- batch: 030 ----
mean loss: 305.99
 ---- batch: 040 ----
mean loss: 306.05
train mean loss: 311.44
epoch train time: 0:00:00.208645
elapsed time: 0:00:44.707240
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:32:54.722611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.14
 ---- batch: 020 ----
mean loss: 307.26
 ---- batch: 030 ----
mean loss: 311.87
 ---- batch: 040 ----
mean loss: 315.97
train mean loss: 311.40
epoch train time: 0:00:00.213314
elapsed time: 0:00:44.920673
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:32:54.936042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.79
 ---- batch: 020 ----
mean loss: 311.61
 ---- batch: 030 ----
mean loss: 310.76
 ---- batch: 040 ----
mean loss: 309.43
train mean loss: 310.92
epoch train time: 0:00:00.203438
elapsed time: 0:00:45.124230
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:32:55.139598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.85
 ---- batch: 020 ----
mean loss: 318.89
 ---- batch: 030 ----
mean loss: 309.68
 ---- batch: 040 ----
mean loss: 305.82
train mean loss: 310.90
epoch train time: 0:00:00.197788
elapsed time: 0:00:45.322129
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:32:55.337495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.77
 ---- batch: 020 ----
mean loss: 316.71
 ---- batch: 030 ----
mean loss: 307.83
 ---- batch: 040 ----
mean loss: 313.86
train mean loss: 310.67
epoch train time: 0:00:00.196217
elapsed time: 0:00:45.518457
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:32:55.533823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.11
 ---- batch: 020 ----
mean loss: 311.30
 ---- batch: 030 ----
mean loss: 310.53
 ---- batch: 040 ----
mean loss: 308.40
train mean loss: 311.15
epoch train time: 0:00:00.195640
elapsed time: 0:00:45.714206
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:32:55.729574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.76
 ---- batch: 020 ----
mean loss: 297.31
 ---- batch: 030 ----
mean loss: 317.27
 ---- batch: 040 ----
mean loss: 311.63
train mean loss: 310.53
epoch train time: 0:00:00.202874
elapsed time: 0:00:45.917207
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:32:55.932569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.88
 ---- batch: 020 ----
mean loss: 308.13
 ---- batch: 030 ----
mean loss: 312.67
 ---- batch: 040 ----
mean loss: 309.64
train mean loss: 310.51
epoch train time: 0:00:00.201613
elapsed time: 0:00:46.118957
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:32:56.134328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.37
 ---- batch: 020 ----
mean loss: 306.71
 ---- batch: 030 ----
mean loss: 318.75
 ---- batch: 040 ----
mean loss: 307.63
train mean loss: 310.27
epoch train time: 0:00:00.203247
elapsed time: 0:00:46.322367
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:32:56.337749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.55
 ---- batch: 020 ----
mean loss: 301.48
 ---- batch: 030 ----
mean loss: 318.45
 ---- batch: 040 ----
mean loss: 310.15
train mean loss: 309.93
epoch train time: 0:00:00.199877
elapsed time: 0:00:46.522377
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:32:56.537759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.26
 ---- batch: 020 ----
mean loss: 307.04
 ---- batch: 030 ----
mean loss: 306.49
 ---- batch: 040 ----
mean loss: 309.15
train mean loss: 310.12
epoch train time: 0:00:00.201367
elapsed time: 0:00:46.723873
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:32:56.739240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.59
 ---- batch: 020 ----
mean loss: 310.23
 ---- batch: 030 ----
mean loss: 302.68
 ---- batch: 040 ----
mean loss: 314.28
train mean loss: 309.97
epoch train time: 0:00:00.203527
elapsed time: 0:00:46.927515
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:32:56.942896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.16
 ---- batch: 020 ----
mean loss: 312.93
 ---- batch: 030 ----
mean loss: 303.97
 ---- batch: 040 ----
mean loss: 317.79
train mean loss: 309.57
epoch train time: 0:00:00.204950
elapsed time: 0:00:47.132602
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:32:57.147973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.66
 ---- batch: 020 ----
mean loss: 305.81
 ---- batch: 030 ----
mean loss: 316.57
 ---- batch: 040 ----
mean loss: 308.99
train mean loss: 309.35
epoch train time: 0:00:00.211029
elapsed time: 0:00:47.343754
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:32:57.359125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.19
 ---- batch: 020 ----
mean loss: 319.02
 ---- batch: 030 ----
mean loss: 307.24
 ---- batch: 040 ----
mean loss: 308.45
train mean loss: 308.78
epoch train time: 0:00:00.210377
elapsed time: 0:00:47.554254
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:32:57.569624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.25
 ---- batch: 020 ----
mean loss: 312.69
 ---- batch: 030 ----
mean loss: 312.03
 ---- batch: 040 ----
mean loss: 297.33
train mean loss: 309.34
epoch train time: 0:00:00.217886
elapsed time: 0:00:47.772262
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:32:57.787647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.29
 ---- batch: 020 ----
mean loss: 312.78
 ---- batch: 030 ----
mean loss: 306.20
 ---- batch: 040 ----
mean loss: 307.37
train mean loss: 308.83
epoch train time: 0:00:00.212803
elapsed time: 0:00:47.985223
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:32:58.000594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.90
 ---- batch: 020 ----
mean loss: 298.01
 ---- batch: 030 ----
mean loss: 306.19
 ---- batch: 040 ----
mean loss: 323.99
train mean loss: 308.76
epoch train time: 0:00:00.211007
elapsed time: 0:00:48.196350
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:32:58.211730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.24
 ---- batch: 020 ----
mean loss: 311.19
 ---- batch: 030 ----
mean loss: 305.56
 ---- batch: 040 ----
mean loss: 306.39
train mean loss: 308.28
epoch train time: 0:00:00.210658
elapsed time: 0:00:48.407144
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:32:58.422515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.45
 ---- batch: 020 ----
mean loss: 307.56
 ---- batch: 030 ----
mean loss: 303.33
 ---- batch: 040 ----
mean loss: 306.70
train mean loss: 308.20
epoch train time: 0:00:00.212765
elapsed time: 0:00:48.620035
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:32:58.635405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.51
 ---- batch: 020 ----
mean loss: 300.07
 ---- batch: 030 ----
mean loss: 309.91
 ---- batch: 040 ----
mean loss: 309.08
train mean loss: 308.39
epoch train time: 0:00:00.214131
elapsed time: 0:00:48.834285
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:32:58.849654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.80
 ---- batch: 020 ----
mean loss: 308.46
 ---- batch: 030 ----
mean loss: 307.98
 ---- batch: 040 ----
mean loss: 310.81
train mean loss: 307.78
epoch train time: 0:00:00.205636
elapsed time: 0:00:49.040043
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:32:59.055414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.51
 ---- batch: 020 ----
mean loss: 310.02
 ---- batch: 030 ----
mean loss: 315.15
 ---- batch: 040 ----
mean loss: 307.86
train mean loss: 307.79
epoch train time: 0:00:00.202399
elapsed time: 0:00:49.242557
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:32:59.257922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.30
 ---- batch: 020 ----
mean loss: 307.78
 ---- batch: 030 ----
mean loss: 304.39
 ---- batch: 040 ----
mean loss: 312.91
train mean loss: 308.05
epoch train time: 0:00:00.203607
elapsed time: 0:00:49.446298
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:32:59.461681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.64
 ---- batch: 020 ----
mean loss: 300.93
 ---- batch: 030 ----
mean loss: 316.04
 ---- batch: 040 ----
mean loss: 309.01
train mean loss: 307.72
epoch train time: 0:00:00.205935
elapsed time: 0:00:49.652428
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:32:59.667799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.98
 ---- batch: 020 ----
mean loss: 312.70
 ---- batch: 030 ----
mean loss: 312.36
 ---- batch: 040 ----
mean loss: 304.49
train mean loss: 306.76
epoch train time: 0:00:00.211390
elapsed time: 0:00:49.863948
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:32:59.879319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.61
 ---- batch: 020 ----
mean loss: 306.89
 ---- batch: 030 ----
mean loss: 306.79
 ---- batch: 040 ----
mean loss: 311.75
train mean loss: 307.35
epoch train time: 0:00:00.206340
elapsed time: 0:00:50.070419
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:33:00.085791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.58
 ---- batch: 020 ----
mean loss: 310.46
 ---- batch: 030 ----
mean loss: 312.22
 ---- batch: 040 ----
mean loss: 297.55
train mean loss: 307.52
epoch train time: 0:00:00.209716
elapsed time: 0:00:50.280283
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:33:00.295655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.67
 ---- batch: 020 ----
mean loss: 304.72
 ---- batch: 030 ----
mean loss: 307.51
 ---- batch: 040 ----
mean loss: 302.37
train mean loss: 307.47
epoch train time: 0:00:00.211930
elapsed time: 0:00:50.492375
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:33:00.507786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.35
 ---- batch: 020 ----
mean loss: 302.93
 ---- batch: 030 ----
mean loss: 309.04
 ---- batch: 040 ----
mean loss: 308.81
train mean loss: 307.09
epoch train time: 0:00:00.213521
elapsed time: 0:00:50.706054
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:33:00.721421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.03
 ---- batch: 020 ----
mean loss: 306.50
 ---- batch: 030 ----
mean loss: 308.02
 ---- batch: 040 ----
mean loss: 312.99
train mean loss: 306.38
epoch train time: 0:00:00.214816
elapsed time: 0:00:50.921016
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:33:00.936463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.94
 ---- batch: 020 ----
mean loss: 307.80
 ---- batch: 030 ----
mean loss: 304.17
 ---- batch: 040 ----
mean loss: 305.90
train mean loss: 306.70
epoch train time: 0:00:00.209195
elapsed time: 0:00:51.130432
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:33:01.145797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.88
 ---- batch: 020 ----
mean loss: 309.22
 ---- batch: 030 ----
mean loss: 310.62
 ---- batch: 040 ----
mean loss: 295.46
train mean loss: 306.60
epoch train time: 0:00:00.211176
elapsed time: 0:00:51.341732
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:33:01.357104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.37
 ---- batch: 020 ----
mean loss: 296.27
 ---- batch: 030 ----
mean loss: 311.67
 ---- batch: 040 ----
mean loss: 317.01
train mean loss: 306.41
epoch train time: 0:00:00.210655
elapsed time: 0:00:51.552511
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:33:01.567883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.38
 ---- batch: 020 ----
mean loss: 311.84
 ---- batch: 030 ----
mean loss: 305.69
 ---- batch: 040 ----
mean loss: 300.26
train mean loss: 306.42
epoch train time: 0:00:00.208347
elapsed time: 0:00:51.760983
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:33:01.776369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.05
 ---- batch: 020 ----
mean loss: 296.67
 ---- batch: 030 ----
mean loss: 306.24
 ---- batch: 040 ----
mean loss: 311.32
train mean loss: 305.96
epoch train time: 0:00:00.214366
elapsed time: 0:00:51.975510
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:33:01.990882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.85
 ---- batch: 020 ----
mean loss: 309.08
 ---- batch: 030 ----
mean loss: 301.72
 ---- batch: 040 ----
mean loss: 303.82
train mean loss: 305.97
epoch train time: 0:00:00.207046
elapsed time: 0:00:52.182699
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:33:02.198083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.42
 ---- batch: 020 ----
mean loss: 298.34
 ---- batch: 030 ----
mean loss: 303.08
 ---- batch: 040 ----
mean loss: 312.53
train mean loss: 305.09
epoch train time: 0:00:00.209397
elapsed time: 0:00:52.392231
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:33:02.407601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.07
 ---- batch: 020 ----
mean loss: 302.57
 ---- batch: 030 ----
mean loss: 306.91
 ---- batch: 040 ----
mean loss: 310.02
train mean loss: 305.02
epoch train time: 0:00:00.203555
elapsed time: 0:00:52.595923
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:33:02.611293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.75
 ---- batch: 020 ----
mean loss: 306.95
 ---- batch: 030 ----
mean loss: 299.93
 ---- batch: 040 ----
mean loss: 306.78
train mean loss: 305.19
epoch train time: 0:00:00.204866
elapsed time: 0:00:52.800928
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:33:02.816299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.95
 ---- batch: 020 ----
mean loss: 311.29
 ---- batch: 030 ----
mean loss: 300.37
 ---- batch: 040 ----
mean loss: 302.99
train mean loss: 305.14
epoch train time: 0:00:00.202699
elapsed time: 0:00:53.003771
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:33:03.019139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.65
 ---- batch: 020 ----
mean loss: 310.85
 ---- batch: 030 ----
mean loss: 298.81
 ---- batch: 040 ----
mean loss: 304.22
train mean loss: 304.90
epoch train time: 0:00:00.202924
elapsed time: 0:00:53.206817
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:33:03.222190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.09
 ---- batch: 020 ----
mean loss: 299.60
 ---- batch: 030 ----
mean loss: 307.13
 ---- batch: 040 ----
mean loss: 310.05
train mean loss: 304.74
epoch train time: 0:00:00.198635
elapsed time: 0:00:53.405587
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:33:03.420953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.60
 ---- batch: 020 ----
mean loss: 302.74
 ---- batch: 030 ----
mean loss: 296.87
 ---- batch: 040 ----
mean loss: 315.52
train mean loss: 304.59
epoch train time: 0:00:00.197986
elapsed time: 0:00:53.603687
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:33:03.619055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.02
 ---- batch: 020 ----
mean loss: 309.31
 ---- batch: 030 ----
mean loss: 307.24
 ---- batch: 040 ----
mean loss: 300.67
train mean loss: 304.68
epoch train time: 0:00:00.197552
elapsed time: 0:00:53.801353
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:33:03.816735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.42
 ---- batch: 020 ----
mean loss: 298.23
 ---- batch: 030 ----
mean loss: 309.48
 ---- batch: 040 ----
mean loss: 308.11
train mean loss: 304.61
epoch train time: 0:00:00.201339
elapsed time: 0:00:54.002822
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:33:04.018191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.72
 ---- batch: 020 ----
mean loss: 304.21
 ---- batch: 030 ----
mean loss: 305.82
 ---- batch: 040 ----
mean loss: 299.05
train mean loss: 304.08
epoch train time: 0:00:00.202527
elapsed time: 0:00:54.205467
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:33:04.220851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.65
 ---- batch: 020 ----
mean loss: 314.63
 ---- batch: 030 ----
mean loss: 296.57
 ---- batch: 040 ----
mean loss: 308.89
train mean loss: 304.43
epoch train time: 0:00:00.202745
elapsed time: 0:00:54.408346
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:33:04.423739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.60
 ---- batch: 020 ----
mean loss: 308.18
 ---- batch: 030 ----
mean loss: 308.04
 ---- batch: 040 ----
mean loss: 293.08
train mean loss: 303.67
epoch train time: 0:00:00.201211
elapsed time: 0:00:54.609702
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:33:04.625073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.58
 ---- batch: 020 ----
mean loss: 302.27
 ---- batch: 030 ----
mean loss: 302.89
 ---- batch: 040 ----
mean loss: 308.08
train mean loss: 303.53
epoch train time: 0:00:00.211223
elapsed time: 0:00:54.821045
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:33:04.836451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.02
 ---- batch: 020 ----
mean loss: 301.18
 ---- batch: 030 ----
mean loss: 298.30
 ---- batch: 040 ----
mean loss: 312.42
train mean loss: 303.80
epoch train time: 0:00:00.216659
elapsed time: 0:00:55.037859
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:33:05.053227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.58
 ---- batch: 020 ----
mean loss: 308.88
 ---- batch: 030 ----
mean loss: 304.71
 ---- batch: 040 ----
mean loss: 308.84
train mean loss: 303.68
epoch train time: 0:00:00.206760
elapsed time: 0:00:55.244767
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:33:05.260137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.43
 ---- batch: 020 ----
mean loss: 315.61
 ---- batch: 030 ----
mean loss: 301.57
 ---- batch: 040 ----
mean loss: 298.75
train mean loss: 302.93
epoch train time: 0:00:00.206791
elapsed time: 0:00:55.451677
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:33:05.467045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.62
 ---- batch: 020 ----
mean loss: 303.43
 ---- batch: 030 ----
mean loss: 302.55
 ---- batch: 040 ----
mean loss: 304.50
train mean loss: 302.85
epoch train time: 0:00:00.204483
elapsed time: 0:00:55.656306
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:33:05.671686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.96
 ---- batch: 020 ----
mean loss: 302.27
 ---- batch: 030 ----
mean loss: 298.63
 ---- batch: 040 ----
mean loss: 305.86
train mean loss: 302.81
epoch train time: 0:00:00.220135
elapsed time: 0:00:55.876570
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:33:05.891939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.33
 ---- batch: 020 ----
mean loss: 304.24
 ---- batch: 030 ----
mean loss: 307.80
 ---- batch: 040 ----
mean loss: 301.93
train mean loss: 302.74
epoch train time: 0:00:00.208364
elapsed time: 0:00:56.085051
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:33:06.100445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.88
 ---- batch: 020 ----
mean loss: 299.84
 ---- batch: 030 ----
mean loss: 298.67
 ---- batch: 040 ----
mean loss: 300.57
train mean loss: 302.38
epoch train time: 0:00:00.206727
elapsed time: 0:00:56.291933
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:33:06.307306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.38
 ---- batch: 020 ----
mean loss: 312.92
 ---- batch: 030 ----
mean loss: 297.15
 ---- batch: 040 ----
mean loss: 306.90
train mean loss: 302.12
epoch train time: 0:00:00.204782
elapsed time: 0:00:56.496853
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:33:06.512239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.13
 ---- batch: 020 ----
mean loss: 298.82
 ---- batch: 030 ----
mean loss: 299.82
 ---- batch: 040 ----
mean loss: 305.46
train mean loss: 302.28
epoch train time: 0:00:00.199236
elapsed time: 0:00:56.696219
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:33:06.711600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.28
 ---- batch: 020 ----
mean loss: 304.82
 ---- batch: 030 ----
mean loss: 300.02
 ---- batch: 040 ----
mean loss: 303.19
train mean loss: 302.21
epoch train time: 0:00:00.202337
elapsed time: 0:00:56.898714
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:33:06.914078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.75
 ---- batch: 020 ----
mean loss: 307.46
 ---- batch: 030 ----
mean loss: 300.89
 ---- batch: 040 ----
mean loss: 303.47
train mean loss: 301.81
epoch train time: 0:00:00.203772
elapsed time: 0:00:57.102603
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:33:07.117972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.43
 ---- batch: 020 ----
mean loss: 310.67
 ---- batch: 030 ----
mean loss: 305.90
 ---- batch: 040 ----
mean loss: 299.17
train mean loss: 302.01
epoch train time: 0:00:00.201069
elapsed time: 0:00:57.303789
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:33:07.319172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.97
 ---- batch: 020 ----
mean loss: 303.48
 ---- batch: 030 ----
mean loss: 295.13
 ---- batch: 040 ----
mean loss: 310.38
train mean loss: 302.21
epoch train time: 0:00:00.202556
elapsed time: 0:00:57.506491
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:33:07.521861
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.79
 ---- batch: 020 ----
mean loss: 305.12
 ---- batch: 030 ----
mean loss: 303.49
 ---- batch: 040 ----
mean loss: 294.52
train mean loss: 301.89
epoch train time: 0:00:00.203880
elapsed time: 0:00:57.710501
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:33:07.725881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.17
 ---- batch: 020 ----
mean loss: 300.02
 ---- batch: 030 ----
mean loss: 308.63
 ---- batch: 040 ----
mean loss: 300.26
train mean loss: 301.78
epoch train time: 0:00:00.209673
elapsed time: 0:00:57.920308
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:33:07.935679
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.98
 ---- batch: 020 ----
mean loss: 298.87
 ---- batch: 030 ----
mean loss: 307.18
 ---- batch: 040 ----
mean loss: 297.31
train mean loss: 302.24
epoch train time: 0:00:00.203276
elapsed time: 0:00:58.123704
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:33:08.139074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.55
 ---- batch: 020 ----
mean loss: 309.68
 ---- batch: 030 ----
mean loss: 297.59
 ---- batch: 040 ----
mean loss: 303.05
train mean loss: 301.78
epoch train time: 0:00:00.204173
elapsed time: 0:00:58.328058
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:33:08.343461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.07
 ---- batch: 020 ----
mean loss: 307.12
 ---- batch: 030 ----
mean loss: 299.00
 ---- batch: 040 ----
mean loss: 305.64
train mean loss: 302.10
epoch train time: 0:00:00.210088
elapsed time: 0:00:58.538301
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:33:08.553671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.32
 ---- batch: 020 ----
mean loss: 300.16
 ---- batch: 030 ----
mean loss: 296.70
 ---- batch: 040 ----
mean loss: 310.20
train mean loss: 301.94
epoch train time: 0:00:00.210120
elapsed time: 0:00:58.748545
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:33:08.763915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.90
 ---- batch: 020 ----
mean loss: 301.41
 ---- batch: 030 ----
mean loss: 302.59
 ---- batch: 040 ----
mean loss: 306.78
train mean loss: 301.79
epoch train time: 0:00:00.210405
elapsed time: 0:00:58.959071
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:33:08.974471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.87
 ---- batch: 020 ----
mean loss: 299.77
 ---- batch: 030 ----
mean loss: 299.46
 ---- batch: 040 ----
mean loss: 306.47
train mean loss: 302.25
epoch train time: 0:00:00.209086
elapsed time: 0:00:59.168313
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:33:09.183709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.30
 ---- batch: 020 ----
mean loss: 303.61
 ---- batch: 030 ----
mean loss: 300.99
 ---- batch: 040 ----
mean loss: 301.69
train mean loss: 301.76
epoch train time: 0:00:00.209262
elapsed time: 0:00:59.377726
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:33:09.393098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.67
 ---- batch: 020 ----
mean loss: 295.91
 ---- batch: 030 ----
mean loss: 302.11
 ---- batch: 040 ----
mean loss: 307.71
train mean loss: 301.94
epoch train time: 0:00:00.210757
elapsed time: 0:00:59.588621
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:33:09.603990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.55
 ---- batch: 020 ----
mean loss: 309.19
 ---- batch: 030 ----
mean loss: 295.55
 ---- batch: 040 ----
mean loss: 299.53
train mean loss: 301.95
epoch train time: 0:00:00.209812
elapsed time: 0:00:59.798553
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:33:09.813940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.26
 ---- batch: 020 ----
mean loss: 297.03
 ---- batch: 030 ----
mean loss: 303.49
 ---- batch: 040 ----
mean loss: 299.26
train mean loss: 301.80
epoch train time: 0:00:00.212453
elapsed time: 0:01:00.011144
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:33:10.026515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.40
 ---- batch: 020 ----
mean loss: 302.57
 ---- batch: 030 ----
mean loss: 295.41
 ---- batch: 040 ----
mean loss: 305.76
train mean loss: 302.10
epoch train time: 0:00:00.209589
elapsed time: 0:01:00.220850
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:33:10.236218
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.59
 ---- batch: 020 ----
mean loss: 308.13
 ---- batch: 030 ----
mean loss: 300.74
 ---- batch: 040 ----
mean loss: 295.42
train mean loss: 301.69
epoch train time: 0:00:00.203284
elapsed time: 0:01:00.424250
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:33:10.439619
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.14
 ---- batch: 020 ----
mean loss: 297.35
 ---- batch: 030 ----
mean loss: 296.05
 ---- batch: 040 ----
mean loss: 302.11
train mean loss: 301.88
epoch train time: 0:00:00.203576
elapsed time: 0:01:00.627957
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:33:10.643324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.71
 ---- batch: 020 ----
mean loss: 302.58
 ---- batch: 030 ----
mean loss: 301.32
 ---- batch: 040 ----
mean loss: 304.62
train mean loss: 301.74
epoch train time: 0:00:00.203245
elapsed time: 0:01:00.831330
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:33:10.846712
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.25
 ---- batch: 020 ----
mean loss: 307.98
 ---- batch: 030 ----
mean loss: 296.44
 ---- batch: 040 ----
mean loss: 288.50
train mean loss: 302.33
epoch train time: 0:00:00.213323
elapsed time: 0:01:01.044811
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:33:11.060180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.32
 ---- batch: 020 ----
mean loss: 300.89
 ---- batch: 030 ----
mean loss: 306.85
 ---- batch: 040 ----
mean loss: 300.90
train mean loss: 302.03
epoch train time: 0:00:00.197644
elapsed time: 0:01:01.242567
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:33:11.257933
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.90
 ---- batch: 020 ----
mean loss: 305.34
 ---- batch: 030 ----
mean loss: 294.66
 ---- batch: 040 ----
mean loss: 301.74
train mean loss: 301.73
epoch train time: 0:00:00.196153
elapsed time: 0:01:01.438832
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:33:11.454215
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.12
 ---- batch: 020 ----
mean loss: 303.47
 ---- batch: 030 ----
mean loss: 302.47
 ---- batch: 040 ----
mean loss: 299.18
train mean loss: 301.87
epoch train time: 0:00:00.200727
elapsed time: 0:01:01.639701
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:33:11.655079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.39
 ---- batch: 020 ----
mean loss: 290.25
 ---- batch: 030 ----
mean loss: 302.62
 ---- batch: 040 ----
mean loss: 306.59
train mean loss: 301.77
epoch train time: 0:00:00.197259
elapsed time: 0:01:01.837099
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:33:11.852468
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.32
 ---- batch: 020 ----
mean loss: 297.15
 ---- batch: 030 ----
mean loss: 306.10
 ---- batch: 040 ----
mean loss: 299.55
train mean loss: 301.49
epoch train time: 0:00:00.197718
elapsed time: 0:01:02.034936
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:33:12.050306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.87
 ---- batch: 020 ----
mean loss: 302.31
 ---- batch: 030 ----
mean loss: 298.99
 ---- batch: 040 ----
mean loss: 298.37
train mean loss: 301.52
epoch train time: 0:00:00.204904
elapsed time: 0:01:02.239975
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:33:12.255345
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.94
 ---- batch: 020 ----
mean loss: 296.62
 ---- batch: 030 ----
mean loss: 304.33
 ---- batch: 040 ----
mean loss: 303.31
train mean loss: 301.55
epoch train time: 0:00:00.210322
elapsed time: 0:01:02.450431
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:33:12.465805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.37
 ---- batch: 020 ----
mean loss: 308.15
 ---- batch: 030 ----
mean loss: 294.33
 ---- batch: 040 ----
mean loss: 302.38
train mean loss: 301.51
epoch train time: 0:00:00.210904
elapsed time: 0:01:02.661462
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:33:12.676833
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.84
 ---- batch: 020 ----
mean loss: 304.75
 ---- batch: 030 ----
mean loss: 304.95
 ---- batch: 040 ----
mean loss: 306.97
train mean loss: 301.11
epoch train time: 0:00:00.213061
elapsed time: 0:01:02.874659
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:33:12.890030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.90
 ---- batch: 020 ----
mean loss: 297.38
 ---- batch: 030 ----
mean loss: 307.38
 ---- batch: 040 ----
mean loss: 302.99
train mean loss: 301.50
epoch train time: 0:00:00.209447
elapsed time: 0:01:03.084226
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:33:13.099595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.39
 ---- batch: 020 ----
mean loss: 302.40
 ---- batch: 030 ----
mean loss: 305.90
 ---- batch: 040 ----
mean loss: 304.56
train mean loss: 301.63
epoch train time: 0:00:00.203849
elapsed time: 0:01:03.288198
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:33:13.303571
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.15
 ---- batch: 020 ----
mean loss: 307.96
 ---- batch: 030 ----
mean loss: 300.81
 ---- batch: 040 ----
mean loss: 299.70
train mean loss: 301.58
epoch train time: 0:00:00.201509
elapsed time: 0:01:03.489835
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:33:13.505209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.59
 ---- batch: 020 ----
mean loss: 306.77
 ---- batch: 030 ----
mean loss: 305.40
 ---- batch: 040 ----
mean loss: 294.24
train mean loss: 301.48
epoch train time: 0:00:00.206387
elapsed time: 0:01:03.696361
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:33:13.711729
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.77
 ---- batch: 020 ----
mean loss: 303.51
 ---- batch: 030 ----
mean loss: 300.90
 ---- batch: 040 ----
mean loss: 300.90
train mean loss: 301.18
epoch train time: 0:00:00.205774
elapsed time: 0:01:03.902256
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:33:13.917627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.21
 ---- batch: 020 ----
mean loss: 293.84
 ---- batch: 030 ----
mean loss: 304.63
 ---- batch: 040 ----
mean loss: 295.15
train mean loss: 301.58
epoch train time: 0:00:00.203903
elapsed time: 0:01:04.106281
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:33:14.121664
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.65
 ---- batch: 020 ----
mean loss: 296.83
 ---- batch: 030 ----
mean loss: 302.74
 ---- batch: 040 ----
mean loss: 299.26
train mean loss: 301.22
epoch train time: 0:00:00.202973
elapsed time: 0:01:04.309411
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:33:14.324783
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.15
 ---- batch: 020 ----
mean loss: 297.95
 ---- batch: 030 ----
mean loss: 298.84
 ---- batch: 040 ----
mean loss: 296.49
train mean loss: 301.50
epoch train time: 0:00:00.202671
elapsed time: 0:01:04.512205
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:33:14.527576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.93
 ---- batch: 020 ----
mean loss: 302.20
 ---- batch: 030 ----
mean loss: 298.17
 ---- batch: 040 ----
mean loss: 307.92
train mean loss: 301.20
epoch train time: 0:00:00.202669
elapsed time: 0:01:04.714994
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:33:14.730363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.39
 ---- batch: 020 ----
mean loss: 305.37
 ---- batch: 030 ----
mean loss: 302.80
 ---- batch: 040 ----
mean loss: 303.56
train mean loss: 301.17
epoch train time: 0:00:00.209930
elapsed time: 0:01:04.925044
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:33:14.940440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.12
 ---- batch: 020 ----
mean loss: 304.17
 ---- batch: 030 ----
mean loss: 305.21
 ---- batch: 040 ----
mean loss: 299.48
train mean loss: 301.60
epoch train time: 0:00:00.200795
elapsed time: 0:01:05.126017
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:33:15.141418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.84
 ---- batch: 020 ----
mean loss: 304.70
 ---- batch: 030 ----
mean loss: 304.96
 ---- batch: 040 ----
mean loss: 296.00
train mean loss: 301.42
epoch train time: 0:00:00.203722
elapsed time: 0:01:05.329905
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:33:15.345275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.62
 ---- batch: 020 ----
mean loss: 300.74
 ---- batch: 030 ----
mean loss: 299.32
 ---- batch: 040 ----
mean loss: 304.76
train mean loss: 301.22
epoch train time: 0:00:00.200717
elapsed time: 0:01:05.530738
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:33:15.546106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.73
 ---- batch: 020 ----
mean loss: 303.29
 ---- batch: 030 ----
mean loss: 305.19
 ---- batch: 040 ----
mean loss: 303.40
train mean loss: 301.18
epoch train time: 0:00:00.210347
elapsed time: 0:01:05.741208
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:33:15.756579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.25
 ---- batch: 020 ----
mean loss: 301.15
 ---- batch: 030 ----
mean loss: 300.45
 ---- batch: 040 ----
mean loss: 302.41
train mean loss: 301.11
epoch train time: 0:00:00.209081
elapsed time: 0:01:05.950414
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:33:15.965802
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.70
 ---- batch: 020 ----
mean loss: 301.53
 ---- batch: 030 ----
mean loss: 306.85
 ---- batch: 040 ----
mean loss: 294.92
train mean loss: 301.62
epoch train time: 0:00:00.205321
elapsed time: 0:01:06.155876
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:33:16.171279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.51
 ---- batch: 020 ----
mean loss: 306.87
 ---- batch: 030 ----
mean loss: 299.91
 ---- batch: 040 ----
mean loss: 297.02
train mean loss: 301.82
epoch train time: 0:00:00.207742
elapsed time: 0:01:06.363773
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:33:16.379144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.64
 ---- batch: 020 ----
mean loss: 295.18
 ---- batch: 030 ----
mean loss: 309.92
 ---- batch: 040 ----
mean loss: 294.78
train mean loss: 301.18
epoch train time: 0:00:00.208219
elapsed time: 0:01:06.572115
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:33:16.587501
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.20
 ---- batch: 020 ----
mean loss: 307.66
 ---- batch: 030 ----
mean loss: 299.59
 ---- batch: 040 ----
mean loss: 295.97
train mean loss: 301.16
epoch train time: 0:00:00.207421
elapsed time: 0:01:06.781709
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_6/checkpoint.pth.tar
**** end time: 2019-09-20 23:33:16.797054 ****
