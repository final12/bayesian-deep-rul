Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_4', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31651
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:44:43.549986 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:44:43.555259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4157.54
 ---- batch: 020 ----
mean loss: 3988.69
 ---- batch: 030 ----
mean loss: 4033.34
train mean loss: 4053.42
epoch train time: 0:00:12.425149
elapsed time: 0:00:12.431840
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:44:55.981865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3965.48
 ---- batch: 020 ----
mean loss: 3877.13
 ---- batch: 030 ----
mean loss: 3846.82
train mean loss: 3888.50
epoch train time: 0:00:00.182358
elapsed time: 0:00:12.614380
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:44:56.164442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3749.79
 ---- batch: 020 ----
mean loss: 3669.69
 ---- batch: 030 ----
mean loss: 3616.34
train mean loss: 3656.64
epoch train time: 0:00:00.176767
elapsed time: 0:00:12.791313
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:44:56.341353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3475.05
 ---- batch: 020 ----
mean loss: 3383.69
 ---- batch: 030 ----
mean loss: 3354.01
train mean loss: 3395.90
epoch train time: 0:00:00.169368
elapsed time: 0:00:12.960825
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:44:56.510863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3185.63
 ---- batch: 020 ----
mean loss: 3111.87
 ---- batch: 030 ----
mean loss: 3164.45
train mean loss: 3138.01
epoch train time: 0:00:00.177747
elapsed time: 0:00:13.138724
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:44:56.688763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2981.79
 ---- batch: 020 ----
mean loss: 2900.77
 ---- batch: 030 ----
mean loss: 2873.39
train mean loss: 2904.95
epoch train time: 0:00:00.179311
elapsed time: 0:00:13.318180
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:44:56.868215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2786.67
 ---- batch: 020 ----
mean loss: 2700.16
 ---- batch: 030 ----
mean loss: 2638.20
train mean loss: 2694.90
epoch train time: 0:00:00.168704
elapsed time: 0:00:13.487030
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:44:57.037066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2568.68
 ---- batch: 020 ----
mean loss: 2539.03
 ---- batch: 030 ----
mean loss: 2442.66
train mean loss: 2505.38
epoch train time: 0:00:00.177442
elapsed time: 0:00:13.664623
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:44:57.214676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2390.18
 ---- batch: 020 ----
mean loss: 2345.85
 ---- batch: 030 ----
mean loss: 2307.76
train mean loss: 2336.87
epoch train time: 0:00:00.177083
elapsed time: 0:00:13.841878
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:44:57.391925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2250.27
 ---- batch: 020 ----
mean loss: 2185.14
 ---- batch: 030 ----
mean loss: 2155.03
train mean loss: 2182.92
epoch train time: 0:00:00.170575
elapsed time: 0:00:14.012604
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:44:57.562641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2078.70
 ---- batch: 020 ----
mean loss: 2066.50
 ---- batch: 030 ----
mean loss: 2006.99
train mean loss: 2040.03
epoch train time: 0:00:00.175898
elapsed time: 0:00:14.188644
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:44:57.738682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1949.36
 ---- batch: 020 ----
mean loss: 1932.04
 ---- batch: 030 ----
mean loss: 1862.36
train mean loss: 1912.26
epoch train time: 0:00:00.172686
elapsed time: 0:00:14.361475
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:44:57.911532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1841.63
 ---- batch: 020 ----
mean loss: 1787.21
 ---- batch: 030 ----
mean loss: 1786.04
train mean loss: 1791.11
epoch train time: 0:00:00.173361
elapsed time: 0:00:14.535003
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:44:58.085049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1749.02
 ---- batch: 020 ----
mean loss: 1695.61
 ---- batch: 030 ----
mean loss: 1637.24
train mean loss: 1679.95
epoch train time: 0:00:00.179582
elapsed time: 0:00:14.714738
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:44:58.264795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1608.77
 ---- batch: 020 ----
mean loss: 1588.13
 ---- batch: 030 ----
mean loss: 1558.86
train mean loss: 1578.57
epoch train time: 0:00:00.175629
elapsed time: 0:00:14.890552
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:44:58.440590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1508.88
 ---- batch: 020 ----
mean loss: 1483.50
 ---- batch: 030 ----
mean loss: 1471.96
train mean loss: 1482.88
epoch train time: 0:00:00.174683
elapsed time: 0:00:15.065401
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:44:58.615441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1429.19
 ---- batch: 020 ----
mean loss: 1393.83
 ---- batch: 030 ----
mean loss: 1376.32
train mean loss: 1394.19
epoch train time: 0:00:00.180949
elapsed time: 0:00:15.246509
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:44:58.796547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1344.04
 ---- batch: 020 ----
mean loss: 1321.08
 ---- batch: 030 ----
mean loss: 1277.56
train mean loss: 1313.59
epoch train time: 0:00:00.182491
elapsed time: 0:00:15.429145
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:44:58.979182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1269.00
 ---- batch: 020 ----
mean loss: 1247.40
 ---- batch: 030 ----
mean loss: 1214.33
train mean loss: 1237.65
epoch train time: 0:00:00.179223
elapsed time: 0:00:15.608511
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:44:59.158548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1196.76
 ---- batch: 020 ----
mean loss: 1168.57
 ---- batch: 030 ----
mean loss: 1150.01
train mean loss: 1167.38
epoch train time: 0:00:00.182245
elapsed time: 0:00:15.791638
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:44:59.341692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1113.66
 ---- batch: 020 ----
mean loss: 1106.75
 ---- batch: 030 ----
mean loss: 1088.62
train mean loss: 1103.70
epoch train time: 0:00:00.174055
elapsed time: 0:00:15.965849
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:44:59.515885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1056.74
 ---- batch: 020 ----
mean loss: 1068.39
 ---- batch: 030 ----
mean loss: 1032.30
train mean loss: 1043.71
epoch train time: 0:00:00.177512
elapsed time: 0:00:16.143519
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:44:59.693557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1016.73
 ---- batch: 020 ----
mean loss: 991.46
 ---- batch: 030 ----
mean loss: 980.84
train mean loss: 988.57
epoch train time: 0:00:00.181763
elapsed time: 0:00:16.325443
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:44:59.875489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.04
 ---- batch: 020 ----
mean loss: 954.42
 ---- batch: 030 ----
mean loss: 937.87
train mean loss: 937.16
epoch train time: 0:00:00.202340
elapsed time: 0:00:16.527939
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:45:00.077994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.71
 ---- batch: 020 ----
mean loss: 906.86
 ---- batch: 030 ----
mean loss: 873.30
train mean loss: 890.50
epoch train time: 0:00:00.173489
elapsed time: 0:00:16.701603
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:45:00.251666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.21
 ---- batch: 020 ----
mean loss: 865.71
 ---- batch: 030 ----
mean loss: 843.52
train mean loss: 848.24
epoch train time: 0:00:00.175255
elapsed time: 0:00:16.877032
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:45:00.427072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.95
 ---- batch: 020 ----
mean loss: 815.35
 ---- batch: 030 ----
mean loss: 785.08
train mean loss: 809.14
epoch train time: 0:00:00.175965
elapsed time: 0:00:17.053154
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:45:00.603193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 788.24
 ---- batch: 020 ----
mean loss: 775.09
 ---- batch: 030 ----
mean loss: 756.46
train mean loss: 771.51
epoch train time: 0:00:00.178949
elapsed time: 0:00:17.232263
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:45:00.782309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.69
 ---- batch: 020 ----
mean loss: 755.31
 ---- batch: 030 ----
mean loss: 729.89
train mean loss: 737.44
epoch train time: 0:00:00.182461
elapsed time: 0:00:17.414874
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:45:00.964922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 715.67
 ---- batch: 020 ----
mean loss: 706.40
 ---- batch: 030 ----
mean loss: 700.56
train mean loss: 707.34
epoch train time: 0:00:00.181398
elapsed time: 0:00:17.596436
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:45:01.146476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.64
 ---- batch: 020 ----
mean loss: 671.94
 ---- batch: 030 ----
mean loss: 687.70
train mean loss: 679.30
epoch train time: 0:00:00.178854
elapsed time: 0:00:17.775435
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:45:01.325471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.31
 ---- batch: 020 ----
mean loss: 659.28
 ---- batch: 030 ----
mean loss: 643.44
train mean loss: 652.21
epoch train time: 0:00:00.177019
elapsed time: 0:00:17.952595
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:45:01.502632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.42
 ---- batch: 020 ----
mean loss: 627.44
 ---- batch: 030 ----
mean loss: 614.69
train mean loss: 628.91
epoch train time: 0:00:00.178618
elapsed time: 0:00:18.131386
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:45:01.681424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 615.08
 ---- batch: 020 ----
mean loss: 616.17
 ---- batch: 030 ----
mean loss: 596.67
train mean loss: 607.25
epoch train time: 0:00:00.182264
elapsed time: 0:00:18.313795
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:45:01.863833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 588.69
 ---- batch: 020 ----
mean loss: 595.38
 ---- batch: 030 ----
mean loss: 580.72
train mean loss: 587.14
epoch train time: 0:00:00.178759
elapsed time: 0:00:18.492711
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:45:02.042747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.32
 ---- batch: 020 ----
mean loss: 566.33
 ---- batch: 030 ----
mean loss: 575.29
train mean loss: 569.32
epoch train time: 0:00:00.179303
elapsed time: 0:00:18.672161
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:45:02.222198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.38
 ---- batch: 020 ----
mean loss: 554.98
 ---- batch: 030 ----
mean loss: 548.66
train mean loss: 551.56
epoch train time: 0:00:00.177519
elapsed time: 0:00:18.849820
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:45:02.399856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.28
 ---- batch: 020 ----
mean loss: 532.95
 ---- batch: 030 ----
mean loss: 537.16
train mean loss: 536.42
epoch train time: 0:00:00.177112
elapsed time: 0:00:19.027089
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:45:02.577126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.56
 ---- batch: 020 ----
mean loss: 523.60
 ---- batch: 030 ----
mean loss: 517.01
train mean loss: 522.66
epoch train time: 0:00:00.182947
elapsed time: 0:00:19.210191
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:45:02.760244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.96
 ---- batch: 020 ----
mean loss: 508.83
 ---- batch: 030 ----
mean loss: 509.79
train mean loss: 509.54
epoch train time: 0:00:00.180706
elapsed time: 0:00:19.391054
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:45:02.941090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.23
 ---- batch: 020 ----
mean loss: 494.67
 ---- batch: 030 ----
mean loss: 496.09
train mean loss: 497.61
epoch train time: 0:00:00.178573
elapsed time: 0:00:19.569769
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:45:03.119806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.67
 ---- batch: 020 ----
mean loss: 480.23
 ---- batch: 030 ----
mean loss: 488.09
train mean loss: 486.46
epoch train time: 0:00:00.181586
elapsed time: 0:00:19.751507
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:45:03.301544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.75
 ---- batch: 020 ----
mean loss: 474.78
 ---- batch: 030 ----
mean loss: 471.84
train mean loss: 476.24
epoch train time: 0:00:00.174648
elapsed time: 0:00:19.926308
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:45:03.476343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.41
 ---- batch: 020 ----
mean loss: 475.45
 ---- batch: 030 ----
mean loss: 465.95
train mean loss: 467.34
epoch train time: 0:00:00.176654
elapsed time: 0:00:20.103101
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:45:03.653136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.07
 ---- batch: 020 ----
mean loss: 454.20
 ---- batch: 030 ----
mean loss: 461.41
train mean loss: 458.54
epoch train time: 0:00:00.176564
elapsed time: 0:00:20.279807
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:45:03.829844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.13
 ---- batch: 020 ----
mean loss: 448.93
 ---- batch: 030 ----
mean loss: 447.00
train mean loss: 450.94
epoch train time: 0:00:00.174523
elapsed time: 0:00:20.454492
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:45:04.004530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.16
 ---- batch: 020 ----
mean loss: 444.26
 ---- batch: 030 ----
mean loss: 442.68
train mean loss: 443.00
epoch train time: 0:00:00.175880
elapsed time: 0:00:20.630523
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:45:04.180562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.61
 ---- batch: 020 ----
mean loss: 441.50
 ---- batch: 030 ----
mean loss: 433.73
train mean loss: 436.28
epoch train time: 0:00:00.176753
elapsed time: 0:00:20.807456
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:45:04.357496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.89
 ---- batch: 020 ----
mean loss: 430.01
 ---- batch: 030 ----
mean loss: 424.10
train mean loss: 429.60
epoch train time: 0:00:00.174399
elapsed time: 0:00:20.982022
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:45:04.532059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.23
 ---- batch: 020 ----
mean loss: 424.02
 ---- batch: 030 ----
mean loss: 420.32
train mean loss: 424.21
epoch train time: 0:00:00.179012
elapsed time: 0:00:21.161179
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:45:04.711217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.55
 ---- batch: 020 ----
mean loss: 417.20
 ---- batch: 030 ----
mean loss: 420.56
train mean loss: 418.51
epoch train time: 0:00:00.177914
elapsed time: 0:00:21.339237
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:45:04.889274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.77
 ---- batch: 020 ----
mean loss: 405.78
 ---- batch: 030 ----
mean loss: 417.76
train mean loss: 413.71
epoch train time: 0:00:00.174660
elapsed time: 0:00:21.514040
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:45:05.064106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.11
 ---- batch: 020 ----
mean loss: 412.63
 ---- batch: 030 ----
mean loss: 401.28
train mean loss: 408.83
epoch train time: 0:00:00.173346
elapsed time: 0:00:21.687556
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:45:05.237621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.24
 ---- batch: 020 ----
mean loss: 402.60
 ---- batch: 030 ----
mean loss: 407.13
train mean loss: 404.90
epoch train time: 0:00:00.173935
elapsed time: 0:00:21.861673
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:45:05.411711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.38
 ---- batch: 020 ----
mean loss: 404.25
 ---- batch: 030 ----
mean loss: 394.81
train mean loss: 401.16
epoch train time: 0:00:00.174749
elapsed time: 0:00:22.036564
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:45:05.586601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.90
 ---- batch: 020 ----
mean loss: 396.73
 ---- batch: 030 ----
mean loss: 399.87
train mean loss: 397.78
epoch train time: 0:00:00.182865
elapsed time: 0:00:22.219572
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:45:05.769632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.43
 ---- batch: 020 ----
mean loss: 402.37
 ---- batch: 030 ----
mean loss: 391.67
train mean loss: 395.16
epoch train time: 0:00:00.176640
elapsed time: 0:00:22.396379
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:45:05.946417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.07
 ---- batch: 020 ----
mean loss: 392.07
 ---- batch: 030 ----
mean loss: 392.78
train mean loss: 392.14
epoch train time: 0:00:00.174262
elapsed time: 0:00:22.570797
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:45:06.120834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.91
 ---- batch: 020 ----
mean loss: 399.08
 ---- batch: 030 ----
mean loss: 389.97
train mean loss: 389.73
epoch train time: 0:00:00.182387
elapsed time: 0:00:22.753328
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:45:06.303366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.79
 ---- batch: 020 ----
mean loss: 390.50
 ---- batch: 030 ----
mean loss: 382.36
train mean loss: 387.68
epoch train time: 0:00:00.177872
elapsed time: 0:00:22.931341
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:45:06.481378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.44
 ---- batch: 020 ----
mean loss: 379.09
 ---- batch: 030 ----
mean loss: 382.69
train mean loss: 386.24
epoch train time: 0:00:00.175697
elapsed time: 0:00:23.107180
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:45:06.657218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.35
 ---- batch: 020 ----
mean loss: 382.79
 ---- batch: 030 ----
mean loss: 390.36
train mean loss: 384.21
epoch train time: 0:00:00.182324
elapsed time: 0:00:23.289649
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:45:06.839687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.09
 ---- batch: 020 ----
mean loss: 386.00
 ---- batch: 030 ----
mean loss: 378.96
train mean loss: 382.70
epoch train time: 0:00:00.181295
elapsed time: 0:00:23.471101
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:45:07.021137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.73
 ---- batch: 020 ----
mean loss: 383.01
 ---- batch: 030 ----
mean loss: 379.99
train mean loss: 380.97
epoch train time: 0:00:00.178180
elapsed time: 0:00:23.649432
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:45:07.199469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.40
 ---- batch: 020 ----
mean loss: 376.26
 ---- batch: 030 ----
mean loss: 379.33
train mean loss: 380.04
epoch train time: 0:00:00.179572
elapsed time: 0:00:23.829146
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:45:07.379183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.18
 ---- batch: 020 ----
mean loss: 374.39
 ---- batch: 030 ----
mean loss: 377.38
train mean loss: 378.83
epoch train time: 0:00:00.169863
elapsed time: 0:00:23.999147
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:45:07.549200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.11
 ---- batch: 020 ----
mean loss: 374.63
 ---- batch: 030 ----
mean loss: 374.71
train mean loss: 377.52
epoch train time: 0:00:00.172628
elapsed time: 0:00:24.171943
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:45:07.721983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.62
 ---- batch: 020 ----
mean loss: 373.25
 ---- batch: 030 ----
mean loss: 377.87
train mean loss: 376.84
epoch train time: 0:00:00.181489
elapsed time: 0:00:24.353577
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:45:07.903613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.63
 ---- batch: 020 ----
mean loss: 379.00
 ---- batch: 030 ----
mean loss: 375.36
train mean loss: 375.47
epoch train time: 0:00:00.170569
elapsed time: 0:00:24.524287
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:45:08.074323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.94
 ---- batch: 020 ----
mean loss: 375.05
 ---- batch: 030 ----
mean loss: 374.63
train mean loss: 375.01
epoch train time: 0:00:00.181671
elapsed time: 0:00:24.706104
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:45:08.256143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.09
 ---- batch: 020 ----
mean loss: 366.01
 ---- batch: 030 ----
mean loss: 379.11
train mean loss: 373.89
epoch train time: 0:00:00.191233
elapsed time: 0:00:24.897495
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:45:08.447533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.75
 ---- batch: 020 ----
mean loss: 371.92
 ---- batch: 030 ----
mean loss: 367.57
train mean loss: 373.58
epoch train time: 0:00:00.173659
elapsed time: 0:00:25.071296
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:45:08.621334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.42
 ---- batch: 020 ----
mean loss: 373.94
 ---- batch: 030 ----
mean loss: 369.29
train mean loss: 372.57
epoch train time: 0:00:00.175732
elapsed time: 0:00:25.247233
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:45:08.797344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.02
 ---- batch: 020 ----
mean loss: 379.91
 ---- batch: 030 ----
mean loss: 368.51
train mean loss: 371.99
epoch train time: 0:00:00.183843
elapsed time: 0:00:25.431295
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:45:08.981333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.02
 ---- batch: 020 ----
mean loss: 367.87
 ---- batch: 030 ----
mean loss: 374.40
train mean loss: 371.76
epoch train time: 0:00:00.181704
elapsed time: 0:00:25.613145
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:45:09.163182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.78
 ---- batch: 020 ----
mean loss: 375.08
 ---- batch: 030 ----
mean loss: 371.94
train mean loss: 371.24
epoch train time: 0:00:00.177045
elapsed time: 0:00:25.790353
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:45:09.340393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.13
 ---- batch: 020 ----
mean loss: 366.21
 ---- batch: 030 ----
mean loss: 371.14
train mean loss: 370.97
epoch train time: 0:00:00.178885
elapsed time: 0:00:25.969414
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:45:09.519496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.73
 ---- batch: 020 ----
mean loss: 373.52
 ---- batch: 030 ----
mean loss: 373.62
train mean loss: 370.18
epoch train time: 0:00:00.181200
elapsed time: 0:00:26.150806
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:45:09.700848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.51
 ---- batch: 020 ----
mean loss: 364.00
 ---- batch: 030 ----
mean loss: 365.99
train mean loss: 370.21
epoch train time: 0:00:00.179103
elapsed time: 0:00:26.330057
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:45:09.880112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.51
 ---- batch: 020 ----
mean loss: 369.74
 ---- batch: 030 ----
mean loss: 372.73
train mean loss: 369.82
epoch train time: 0:00:00.174963
elapsed time: 0:00:26.505768
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:45:10.055813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.88
 ---- batch: 020 ----
mean loss: 368.26
 ---- batch: 030 ----
mean loss: 380.39
train mean loss: 368.93
epoch train time: 0:00:00.175197
elapsed time: 0:00:26.681115
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:45:10.231150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.86
 ---- batch: 020 ----
mean loss: 370.72
 ---- batch: 030 ----
mean loss: 368.61
train mean loss: 368.85
epoch train time: 0:00:00.176955
elapsed time: 0:00:26.858204
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:45:10.408238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.96
 ---- batch: 020 ----
mean loss: 362.80
 ---- batch: 030 ----
mean loss: 369.96
train mean loss: 368.67
epoch train time: 0:00:00.174608
elapsed time: 0:00:27.032959
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:45:10.582997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.54
 ---- batch: 020 ----
mean loss: 371.59
 ---- batch: 030 ----
mean loss: 366.37
train mean loss: 368.16
epoch train time: 0:00:00.173562
elapsed time: 0:00:27.206666
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:45:10.756702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.16
 ---- batch: 020 ----
mean loss: 363.86
 ---- batch: 030 ----
mean loss: 368.05
train mean loss: 368.19
epoch train time: 0:00:00.178100
elapsed time: 0:00:27.384906
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:45:10.934953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.45
 ---- batch: 020 ----
mean loss: 373.11
 ---- batch: 030 ----
mean loss: 366.37
train mean loss: 367.50
epoch train time: 0:00:00.177981
elapsed time: 0:00:27.563062
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:45:11.113091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.76
 ---- batch: 020 ----
mean loss: 368.27
 ---- batch: 030 ----
mean loss: 364.09
train mean loss: 367.28
epoch train time: 0:00:00.172800
elapsed time: 0:00:27.736011
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:45:11.286047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.09
 ---- batch: 020 ----
mean loss: 377.33
 ---- batch: 030 ----
mean loss: 358.99
train mean loss: 367.00
epoch train time: 0:00:00.171860
elapsed time: 0:00:27.908019
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:45:11.458087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.49
 ---- batch: 020 ----
mean loss: 362.60
 ---- batch: 030 ----
mean loss: 361.99
train mean loss: 366.92
epoch train time: 0:00:00.176409
elapsed time: 0:00:28.084644
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:45:11.634679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.25
 ---- batch: 020 ----
mean loss: 363.02
 ---- batch: 030 ----
mean loss: 370.39
train mean loss: 366.39
epoch train time: 0:00:00.181815
elapsed time: 0:00:28.266604
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:45:11.816644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.10
 ---- batch: 020 ----
mean loss: 369.00
 ---- batch: 030 ----
mean loss: 369.60
train mean loss: 366.38
epoch train time: 0:00:00.180826
elapsed time: 0:00:28.447605
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:45:11.997682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.82
 ---- batch: 020 ----
mean loss: 365.82
 ---- batch: 030 ----
mean loss: 368.96
train mean loss: 365.81
epoch train time: 0:00:00.181389
elapsed time: 0:00:28.629175
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:45:12.179212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.94
 ---- batch: 020 ----
mean loss: 366.14
 ---- batch: 030 ----
mean loss: 362.68
train mean loss: 365.77
epoch train time: 0:00:00.173835
elapsed time: 0:00:28.803152
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:45:12.353212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.47
 ---- batch: 020 ----
mean loss: 364.11
 ---- batch: 030 ----
mean loss: 366.57
train mean loss: 365.66
epoch train time: 0:00:00.175943
elapsed time: 0:00:28.979258
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:45:12.529294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.15
 ---- batch: 020 ----
mean loss: 359.58
 ---- batch: 030 ----
mean loss: 373.06
train mean loss: 365.69
epoch train time: 0:00:00.176227
elapsed time: 0:00:29.155675
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:45:12.705716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.21
 ---- batch: 020 ----
mean loss: 362.49
 ---- batch: 030 ----
mean loss: 370.49
train mean loss: 364.91
epoch train time: 0:00:00.177761
elapsed time: 0:00:29.333582
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:45:12.883617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.80
 ---- batch: 020 ----
mean loss: 364.31
 ---- batch: 030 ----
mean loss: 369.45
train mean loss: 364.84
epoch train time: 0:00:00.177661
elapsed time: 0:00:29.511384
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:45:13.061420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.99
 ---- batch: 020 ----
mean loss: 362.47
 ---- batch: 030 ----
mean loss: 361.26
train mean loss: 364.83
epoch train time: 0:00:00.176152
elapsed time: 0:00:29.687695
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:45:13.237736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.73
 ---- batch: 020 ----
mean loss: 363.33
 ---- batch: 030 ----
mean loss: 361.78
train mean loss: 364.86
epoch train time: 0:00:00.175582
elapsed time: 0:00:29.863423
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:45:13.413460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.37
 ---- batch: 020 ----
mean loss: 360.82
 ---- batch: 030 ----
mean loss: 366.34
train mean loss: 364.14
epoch train time: 0:00:00.178010
elapsed time: 0:00:30.041577
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:45:13.591612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.87
 ---- batch: 020 ----
mean loss: 367.01
 ---- batch: 030 ----
mean loss: 358.79
train mean loss: 364.51
epoch train time: 0:00:00.176657
elapsed time: 0:00:30.218373
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:45:13.768407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.10
 ---- batch: 020 ----
mean loss: 368.84
 ---- batch: 030 ----
mean loss: 364.33
train mean loss: 364.00
epoch train time: 0:00:00.179444
elapsed time: 0:00:30.397968
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:45:13.948008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.09
 ---- batch: 020 ----
mean loss: 364.52
 ---- batch: 030 ----
mean loss: 364.24
train mean loss: 363.45
epoch train time: 0:00:00.178076
elapsed time: 0:00:30.576208
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:45:14.126244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.64
 ---- batch: 020 ----
mean loss: 369.24
 ---- batch: 030 ----
mean loss: 364.14
train mean loss: 363.70
epoch train time: 0:00:00.178508
elapsed time: 0:00:30.754858
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:45:14.304897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.72
 ---- batch: 020 ----
mean loss: 369.83
 ---- batch: 030 ----
mean loss: 365.42
train mean loss: 363.26
epoch train time: 0:00:00.175404
elapsed time: 0:00:30.930413
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:45:14.480456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.31
 ---- batch: 020 ----
mean loss: 363.77
 ---- batch: 030 ----
mean loss: 369.26
train mean loss: 363.25
epoch train time: 0:00:00.170007
elapsed time: 0:00:31.100571
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:45:14.650640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.75
 ---- batch: 020 ----
mean loss: 365.71
 ---- batch: 030 ----
mean loss: 365.17
train mean loss: 363.09
epoch train time: 0:00:00.174771
elapsed time: 0:00:31.275541
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:45:14.825580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.06
 ---- batch: 020 ----
mean loss: 355.38
 ---- batch: 030 ----
mean loss: 366.77
train mean loss: 362.86
epoch train time: 0:00:00.176454
elapsed time: 0:00:31.452142
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:45:15.002179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.73
 ---- batch: 020 ----
mean loss: 369.64
 ---- batch: 030 ----
mean loss: 358.11
train mean loss: 362.93
epoch train time: 0:00:00.174341
elapsed time: 0:00:31.626624
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:45:15.176663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.74
 ---- batch: 020 ----
mean loss: 356.69
 ---- batch: 030 ----
mean loss: 364.87
train mean loss: 362.73
epoch train time: 0:00:00.217367
elapsed time: 0:00:31.844216
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:45:15.394263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.88
 ---- batch: 020 ----
mean loss: 361.71
 ---- batch: 030 ----
mean loss: 363.11
train mean loss: 361.98
epoch train time: 0:00:00.279214
elapsed time: 0:00:32.123681
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:45:15.673729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.63
 ---- batch: 020 ----
mean loss: 361.26
 ---- batch: 030 ----
mean loss: 364.94
train mean loss: 362.03
epoch train time: 0:00:00.258446
elapsed time: 0:00:32.382333
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:45:15.932387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.05
 ---- batch: 020 ----
mean loss: 358.96
 ---- batch: 030 ----
mean loss: 360.55
train mean loss: 362.50
epoch train time: 0:00:00.184230
elapsed time: 0:00:32.566728
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:45:16.116771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.22
 ---- batch: 020 ----
mean loss: 361.25
 ---- batch: 030 ----
mean loss: 365.50
train mean loss: 361.70
epoch train time: 0:00:00.185145
elapsed time: 0:00:32.752027
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:45:16.302065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.39
 ---- batch: 020 ----
mean loss: 364.34
 ---- batch: 030 ----
mean loss: 369.67
train mean loss: 361.66
epoch train time: 0:00:00.177508
elapsed time: 0:00:32.929682
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:45:16.479722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.46
 ---- batch: 020 ----
mean loss: 362.58
 ---- batch: 030 ----
mean loss: 360.15
train mean loss: 361.97
epoch train time: 0:00:00.173924
elapsed time: 0:00:33.103756
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:45:16.653794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.07
 ---- batch: 020 ----
mean loss: 362.35
 ---- batch: 030 ----
mean loss: 362.93
train mean loss: 361.10
epoch train time: 0:00:00.175106
elapsed time: 0:00:33.279017
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:45:16.829063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.80
 ---- batch: 020 ----
mean loss: 364.02
 ---- batch: 030 ----
mean loss: 359.60
train mean loss: 360.64
epoch train time: 0:00:00.175121
elapsed time: 0:00:33.454290
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:45:17.004326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.10
 ---- batch: 020 ----
mean loss: 364.70
 ---- batch: 030 ----
mean loss: 360.10
train mean loss: 361.13
epoch train time: 0:00:00.175964
elapsed time: 0:00:33.630393
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:45:17.180429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.19
 ---- batch: 020 ----
mean loss: 358.26
 ---- batch: 030 ----
mean loss: 359.45
train mean loss: 360.84
epoch train time: 0:00:00.168470
elapsed time: 0:00:33.799012
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:45:17.349070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.27
 ---- batch: 020 ----
mean loss: 357.54
 ---- batch: 030 ----
mean loss: 358.43
train mean loss: 360.80
epoch train time: 0:00:00.184849
elapsed time: 0:00:33.984037
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:45:17.534074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.23
 ---- batch: 020 ----
mean loss: 358.34
 ---- batch: 030 ----
mean loss: 367.40
train mean loss: 360.33
epoch train time: 0:00:00.191051
elapsed time: 0:00:34.175253
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:45:17.725299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.80
 ---- batch: 020 ----
mean loss: 358.66
 ---- batch: 030 ----
mean loss: 364.23
train mean loss: 360.47
epoch train time: 0:00:00.180720
elapsed time: 0:00:34.356149
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:45:17.906206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.63
 ---- batch: 020 ----
mean loss: 362.09
 ---- batch: 030 ----
mean loss: 358.85
train mean loss: 360.06
epoch train time: 0:00:00.179364
elapsed time: 0:00:34.535699
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:45:18.085739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.21
 ---- batch: 020 ----
mean loss: 354.54
 ---- batch: 030 ----
mean loss: 362.99
train mean loss: 359.78
epoch train time: 0:00:00.174583
elapsed time: 0:00:34.710430
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:45:18.260469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.62
 ---- batch: 020 ----
mean loss: 357.85
 ---- batch: 030 ----
mean loss: 351.62
train mean loss: 360.17
epoch train time: 0:00:00.177236
elapsed time: 0:00:34.887813
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:45:18.437851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.67
 ---- batch: 020 ----
mean loss: 365.72
 ---- batch: 030 ----
mean loss: 360.40
train mean loss: 359.65
epoch train time: 0:00:00.171907
elapsed time: 0:00:35.059888
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:45:18.609923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.69
 ---- batch: 020 ----
mean loss: 358.51
 ---- batch: 030 ----
mean loss: 364.11
train mean loss: 359.59
epoch train time: 0:00:00.176393
elapsed time: 0:00:35.236421
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:45:18.786457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.61
 ---- batch: 020 ----
mean loss: 362.38
 ---- batch: 030 ----
mean loss: 360.62
train mean loss: 359.20
epoch train time: 0:00:00.179065
elapsed time: 0:00:35.415650
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:45:18.965689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.85
 ---- batch: 020 ----
mean loss: 360.94
 ---- batch: 030 ----
mean loss: 361.22
train mean loss: 359.05
epoch train time: 0:00:00.171599
elapsed time: 0:00:35.587400
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:45:19.137454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.91
 ---- batch: 020 ----
mean loss: 358.92
 ---- batch: 030 ----
mean loss: 369.53
train mean loss: 359.37
epoch train time: 0:00:00.172326
elapsed time: 0:00:35.759888
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:45:19.309927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.24
 ---- batch: 020 ----
mean loss: 371.41
 ---- batch: 030 ----
mean loss: 352.17
train mean loss: 358.97
epoch train time: 0:00:00.174826
elapsed time: 0:00:35.934859
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:45:19.484936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.40
 ---- batch: 020 ----
mean loss: 363.10
 ---- batch: 030 ----
mean loss: 366.35
train mean loss: 358.85
epoch train time: 0:00:00.170923
elapsed time: 0:00:36.105967
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:45:19.656003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.86
 ---- batch: 020 ----
mean loss: 356.77
 ---- batch: 030 ----
mean loss: 352.10
train mean loss: 358.64
epoch train time: 0:00:00.175881
elapsed time: 0:00:36.282038
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:45:19.832097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.56
 ---- batch: 020 ----
mean loss: 355.39
 ---- batch: 030 ----
mean loss: 364.96
train mean loss: 358.35
epoch train time: 0:00:00.174901
elapsed time: 0:00:36.457104
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:45:20.007140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.69
 ---- batch: 020 ----
mean loss: 360.77
 ---- batch: 030 ----
mean loss: 365.83
train mean loss: 358.53
epoch train time: 0:00:00.172581
elapsed time: 0:00:36.629828
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:45:20.179867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.92
 ---- batch: 020 ----
mean loss: 354.18
 ---- batch: 030 ----
mean loss: 354.17
train mean loss: 358.37
epoch train time: 0:00:00.180810
elapsed time: 0:00:36.810787
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:45:20.360826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.45
 ---- batch: 020 ----
mean loss: 356.68
 ---- batch: 030 ----
mean loss: 359.88
train mean loss: 358.15
epoch train time: 0:00:00.178510
elapsed time: 0:00:36.989443
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:45:20.539481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.36
 ---- batch: 020 ----
mean loss: 354.84
 ---- batch: 030 ----
mean loss: 361.14
train mean loss: 357.62
epoch train time: 0:00:00.180065
elapsed time: 0:00:37.169662
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:45:20.719700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.81
 ---- batch: 020 ----
mean loss: 354.62
 ---- batch: 030 ----
mean loss: 350.92
train mean loss: 357.92
epoch train time: 0:00:00.176259
elapsed time: 0:00:37.346067
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:45:20.896106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.11
 ---- batch: 020 ----
mean loss: 359.98
 ---- batch: 030 ----
mean loss: 352.79
train mean loss: 357.96
epoch train time: 0:00:00.174748
elapsed time: 0:00:37.520967
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:45:21.071003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.79
 ---- batch: 020 ----
mean loss: 356.20
 ---- batch: 030 ----
mean loss: 357.13
train mean loss: 357.81
epoch train time: 0:00:00.177146
elapsed time: 0:00:37.698255
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:45:21.248291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.27
 ---- batch: 020 ----
mean loss: 362.40
 ---- batch: 030 ----
mean loss: 356.83
train mean loss: 357.41
epoch train time: 0:00:00.169288
elapsed time: 0:00:37.867713
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:45:21.417750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.72
 ---- batch: 020 ----
mean loss: 364.49
 ---- batch: 030 ----
mean loss: 353.10
train mean loss: 357.18
epoch train time: 0:00:00.171524
elapsed time: 0:00:38.039380
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:45:21.589415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.97
 ---- batch: 020 ----
mean loss: 357.11
 ---- batch: 030 ----
mean loss: 356.98
train mean loss: 356.93
epoch train time: 0:00:00.174793
elapsed time: 0:00:38.214315
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:45:21.764353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.49
 ---- batch: 020 ----
mean loss: 358.72
 ---- batch: 030 ----
mean loss: 354.60
train mean loss: 356.99
epoch train time: 0:00:00.175449
elapsed time: 0:00:38.389909
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:45:21.939946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.90
 ---- batch: 020 ----
mean loss: 357.73
 ---- batch: 030 ----
mean loss: 357.93
train mean loss: 356.67
epoch train time: 0:00:00.172222
elapsed time: 0:00:38.562272
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:45:22.112327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.74
 ---- batch: 020 ----
mean loss: 352.33
 ---- batch: 030 ----
mean loss: 356.55
train mean loss: 356.42
epoch train time: 0:00:00.169984
elapsed time: 0:00:38.732416
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:45:22.282453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.62
 ---- batch: 020 ----
mean loss: 353.25
 ---- batch: 030 ----
mean loss: 360.50
train mean loss: 356.38
epoch train time: 0:00:00.169257
elapsed time: 0:00:38.901829
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:45:22.451853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.54
 ---- batch: 020 ----
mean loss: 356.68
 ---- batch: 030 ----
mean loss: 362.97
train mean loss: 356.22
epoch train time: 0:00:00.169781
elapsed time: 0:00:39.071744
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:45:22.621778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.72
 ---- batch: 020 ----
mean loss: 361.45
 ---- batch: 030 ----
mean loss: 350.39
train mean loss: 356.16
epoch train time: 0:00:00.171020
elapsed time: 0:00:39.242901
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:45:22.792936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.90
 ---- batch: 020 ----
mean loss: 362.92
 ---- batch: 030 ----
mean loss: 352.55
train mean loss: 356.04
epoch train time: 0:00:00.171852
elapsed time: 0:00:39.414895
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:45:22.964931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.37
 ---- batch: 020 ----
mean loss: 354.35
 ---- batch: 030 ----
mean loss: 353.15
train mean loss: 356.31
epoch train time: 0:00:00.174347
elapsed time: 0:00:39.589386
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:45:23.139422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.44
 ---- batch: 020 ----
mean loss: 354.69
 ---- batch: 030 ----
mean loss: 360.27
train mean loss: 355.69
epoch train time: 0:00:00.174311
elapsed time: 0:00:39.763843
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:45:23.313878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.27
 ---- batch: 020 ----
mean loss: 358.13
 ---- batch: 030 ----
mean loss: 359.45
train mean loss: 355.52
epoch train time: 0:00:00.174301
elapsed time: 0:00:39.938286
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:45:23.488322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.54
 ---- batch: 020 ----
mean loss: 352.91
 ---- batch: 030 ----
mean loss: 353.19
train mean loss: 355.77
epoch train time: 0:00:00.173397
elapsed time: 0:00:40.111831
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:45:23.661868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.77
 ---- batch: 020 ----
mean loss: 355.86
 ---- batch: 030 ----
mean loss: 351.53
train mean loss: 355.31
epoch train time: 0:00:00.174053
elapsed time: 0:00:40.286029
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:45:23.836084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.53
 ---- batch: 020 ----
mean loss: 359.79
 ---- batch: 030 ----
mean loss: 357.18
train mean loss: 355.32
epoch train time: 0:00:00.173027
elapsed time: 0:00:40.459218
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:45:24.009256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.77
 ---- batch: 020 ----
mean loss: 350.60
 ---- batch: 030 ----
mean loss: 355.43
train mean loss: 354.98
epoch train time: 0:00:00.177971
elapsed time: 0:00:40.637332
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:45:24.187368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.69
 ---- batch: 020 ----
mean loss: 358.27
 ---- batch: 030 ----
mean loss: 351.63
train mean loss: 355.06
epoch train time: 0:00:00.175451
elapsed time: 0:00:40.812919
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:45:24.362982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.80
 ---- batch: 020 ----
mean loss: 364.99
 ---- batch: 030 ----
mean loss: 347.72
train mean loss: 354.28
epoch train time: 0:00:00.171326
elapsed time: 0:00:40.984420
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:45:24.534477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.01
 ---- batch: 020 ----
mean loss: 351.12
 ---- batch: 030 ----
mean loss: 359.14
train mean loss: 354.50
epoch train time: 0:00:00.171842
elapsed time: 0:00:41.156430
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:45:24.706466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.31
 ---- batch: 020 ----
mean loss: 357.89
 ---- batch: 030 ----
mean loss: 357.85
train mean loss: 354.65
epoch train time: 0:00:00.179221
elapsed time: 0:00:41.335800
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:45:24.885835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.23
 ---- batch: 020 ----
mean loss: 356.77
 ---- batch: 030 ----
mean loss: 353.83
train mean loss: 354.51
epoch train time: 0:00:00.170393
elapsed time: 0:00:41.506338
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:45:25.056382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.69
 ---- batch: 020 ----
mean loss: 360.59
 ---- batch: 030 ----
mean loss: 352.19
train mean loss: 354.17
epoch train time: 0:00:00.167999
elapsed time: 0:00:41.674488
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:45:25.224526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.49
 ---- batch: 020 ----
mean loss: 359.75
 ---- batch: 030 ----
mean loss: 348.70
train mean loss: 353.63
epoch train time: 0:00:00.173501
elapsed time: 0:00:41.848135
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:45:25.398171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.74
 ---- batch: 020 ----
mean loss: 353.44
 ---- batch: 030 ----
mean loss: 354.82
train mean loss: 353.82
epoch train time: 0:00:00.173194
elapsed time: 0:00:42.021470
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:45:25.571505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.59
 ---- batch: 020 ----
mean loss: 346.50
 ---- batch: 030 ----
mean loss: 362.38
train mean loss: 354.19
epoch train time: 0:00:00.175830
elapsed time: 0:00:42.197447
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:45:25.747485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.43
 ---- batch: 020 ----
mean loss: 352.05
 ---- batch: 030 ----
mean loss: 354.02
train mean loss: 353.76
epoch train time: 0:00:00.178068
elapsed time: 0:00:42.375680
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:45:25.925719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.40
 ---- batch: 020 ----
mean loss: 356.73
 ---- batch: 030 ----
mean loss: 350.64
train mean loss: 353.48
epoch train time: 0:00:00.178737
elapsed time: 0:00:42.554572
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:45:26.104620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.79
 ---- batch: 020 ----
mean loss: 353.04
 ---- batch: 030 ----
mean loss: 351.56
train mean loss: 353.27
epoch train time: 0:00:00.177701
elapsed time: 0:00:42.732427
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:45:26.282463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.12
 ---- batch: 020 ----
mean loss: 359.81
 ---- batch: 030 ----
mean loss: 360.46
train mean loss: 353.12
epoch train time: 0:00:00.176323
elapsed time: 0:00:42.908904
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:45:26.458938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.52
 ---- batch: 020 ----
mean loss: 348.00
 ---- batch: 030 ----
mean loss: 354.90
train mean loss: 353.50
epoch train time: 0:00:00.173733
elapsed time: 0:00:43.082788
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:45:26.632843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.19
 ---- batch: 020 ----
mean loss: 356.37
 ---- batch: 030 ----
mean loss: 349.64
train mean loss: 353.13
epoch train time: 0:00:00.178326
elapsed time: 0:00:43.261296
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:45:26.811350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.11
 ---- batch: 020 ----
mean loss: 356.05
 ---- batch: 030 ----
mean loss: 353.72
train mean loss: 352.74
epoch train time: 0:00:00.177976
elapsed time: 0:00:43.439429
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:45:26.989465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.12
 ---- batch: 020 ----
mean loss: 355.07
 ---- batch: 030 ----
mean loss: 350.17
train mean loss: 352.65
epoch train time: 0:00:00.175906
elapsed time: 0:00:43.615480
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:45:27.165516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.84
 ---- batch: 020 ----
mean loss: 347.34
 ---- batch: 030 ----
mean loss: 358.62
train mean loss: 352.41
epoch train time: 0:00:00.174165
elapsed time: 0:00:43.789801
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:45:27.339867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.64
 ---- batch: 020 ----
mean loss: 351.02
 ---- batch: 030 ----
mean loss: 346.22
train mean loss: 352.23
epoch train time: 0:00:00.175889
elapsed time: 0:00:43.965873
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:45:27.515908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.95
 ---- batch: 020 ----
mean loss: 358.67
 ---- batch: 030 ----
mean loss: 346.04
train mean loss: 352.44
epoch train time: 0:00:00.173649
elapsed time: 0:00:44.139685
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:45:27.689723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.68
 ---- batch: 020 ----
mean loss: 351.93
 ---- batch: 030 ----
mean loss: 355.94
train mean loss: 351.96
epoch train time: 0:00:00.180182
elapsed time: 0:00:44.320010
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:45:27.870045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.57
 ---- batch: 020 ----
mean loss: 356.54
 ---- batch: 030 ----
mean loss: 346.54
train mean loss: 352.26
epoch train time: 0:00:00.175597
elapsed time: 0:00:44.495748
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:45:28.045788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.28
 ---- batch: 020 ----
mean loss: 350.21
 ---- batch: 030 ----
mean loss: 356.45
train mean loss: 351.68
epoch train time: 0:00:00.173165
elapsed time: 0:00:44.669117
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:45:28.219151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.60
 ---- batch: 020 ----
mean loss: 348.88
 ---- batch: 030 ----
mean loss: 352.50
train mean loss: 351.84
epoch train time: 0:00:00.173144
elapsed time: 0:00:44.842397
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:45:28.392432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.63
 ---- batch: 020 ----
mean loss: 351.58
 ---- batch: 030 ----
mean loss: 353.60
train mean loss: 351.72
epoch train time: 0:00:00.173548
elapsed time: 0:00:45.016129
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:45:28.566243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.51
 ---- batch: 020 ----
mean loss: 346.71
 ---- batch: 030 ----
mean loss: 351.02
train mean loss: 351.50
epoch train time: 0:00:00.170818
elapsed time: 0:00:45.187168
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:45:28.737205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.28
 ---- batch: 020 ----
mean loss: 357.18
 ---- batch: 030 ----
mean loss: 342.38
train mean loss: 351.50
epoch train time: 0:00:00.188083
elapsed time: 0:00:45.375438
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:45:28.925504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.31
 ---- batch: 020 ----
mean loss: 355.81
 ---- batch: 030 ----
mean loss: 347.19
train mean loss: 351.13
epoch train time: 0:00:00.178056
elapsed time: 0:00:45.553679
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:45:29.103714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.80
 ---- batch: 020 ----
mean loss: 357.76
 ---- batch: 030 ----
mean loss: 350.76
train mean loss: 351.10
epoch train time: 0:00:00.175597
elapsed time: 0:00:45.729414
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:45:29.279473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.48
 ---- batch: 020 ----
mean loss: 344.78
 ---- batch: 030 ----
mean loss: 356.26
train mean loss: 350.76
epoch train time: 0:00:00.175542
elapsed time: 0:00:45.905119
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:45:29.455169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.18
 ---- batch: 020 ----
mean loss: 353.82
 ---- batch: 030 ----
mean loss: 354.65
train mean loss: 350.48
epoch train time: 0:00:00.176807
elapsed time: 0:00:46.082081
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:45:29.632117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.24
 ---- batch: 020 ----
mean loss: 354.12
 ---- batch: 030 ----
mean loss: 350.86
train mean loss: 350.54
epoch train time: 0:00:00.180891
elapsed time: 0:00:46.263142
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:45:29.813177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.29
 ---- batch: 020 ----
mean loss: 355.95
 ---- batch: 030 ----
mean loss: 347.28
train mean loss: 350.15
epoch train time: 0:00:00.181059
elapsed time: 0:00:46.444339
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:45:29.994408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.54
 ---- batch: 020 ----
mean loss: 354.31
 ---- batch: 030 ----
mean loss: 345.16
train mean loss: 350.04
epoch train time: 0:00:00.177448
elapsed time: 0:00:46.621960
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:45:30.171996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.96
 ---- batch: 020 ----
mean loss: 349.55
 ---- batch: 030 ----
mean loss: 348.03
train mean loss: 350.10
epoch train time: 0:00:00.175702
elapsed time: 0:00:46.797804
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:45:30.347842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.30
 ---- batch: 020 ----
mean loss: 353.26
 ---- batch: 030 ----
mean loss: 349.03
train mean loss: 349.90
epoch train time: 0:00:00.178941
elapsed time: 0:00:46.976888
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:45:30.526922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.39
 ---- batch: 020 ----
mean loss: 344.73
 ---- batch: 030 ----
mean loss: 349.43
train mean loss: 349.62
epoch train time: 0:00:00.181824
elapsed time: 0:00:47.158852
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:45:30.708888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.95
 ---- batch: 020 ----
mean loss: 349.36
 ---- batch: 030 ----
mean loss: 346.67
train mean loss: 349.22
epoch train time: 0:00:00.179744
elapsed time: 0:00:47.338753
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:45:30.888790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.41
 ---- batch: 020 ----
mean loss: 353.44
 ---- batch: 030 ----
mean loss: 355.41
train mean loss: 349.36
epoch train time: 0:00:00.180516
elapsed time: 0:00:47.519442
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:45:31.069508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.24
 ---- batch: 020 ----
mean loss: 348.00
 ---- batch: 030 ----
mean loss: 348.64
train mean loss: 349.23
epoch train time: 0:00:00.177603
elapsed time: 0:00:47.697217
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:45:31.247253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.82
 ---- batch: 020 ----
mean loss: 349.72
 ---- batch: 030 ----
mean loss: 351.45
train mean loss: 348.59
epoch train time: 0:00:00.175355
elapsed time: 0:00:47.872713
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:45:31.422750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.36
 ---- batch: 020 ----
mean loss: 357.05
 ---- batch: 030 ----
mean loss: 344.93
train mean loss: 349.10
epoch train time: 0:00:00.172147
elapsed time: 0:00:48.045016
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:45:31.595070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.52
 ---- batch: 020 ----
mean loss: 344.98
 ---- batch: 030 ----
mean loss: 351.26
train mean loss: 348.80
epoch train time: 0:00:00.175713
elapsed time: 0:00:48.220911
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:45:31.770936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.62
 ---- batch: 020 ----
mean loss: 350.06
 ---- batch: 030 ----
mean loss: 353.87
train mean loss: 348.87
epoch train time: 0:00:00.176412
elapsed time: 0:00:48.397461
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:45:31.947507
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.21
 ---- batch: 020 ----
mean loss: 351.57
 ---- batch: 030 ----
mean loss: 342.19
train mean loss: 348.96
epoch train time: 0:00:00.169821
elapsed time: 0:00:48.567431
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:45:32.117466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.12
 ---- batch: 020 ----
mean loss: 344.69
 ---- batch: 030 ----
mean loss: 355.92
train mean loss: 348.77
epoch train time: 0:00:00.167725
elapsed time: 0:00:48.735295
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:45:32.285331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.35
 ---- batch: 020 ----
mean loss: 348.02
 ---- batch: 030 ----
mean loss: 352.03
train mean loss: 348.47
epoch train time: 0:00:00.172337
elapsed time: 0:00:48.907773
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:45:32.457809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.79
 ---- batch: 020 ----
mean loss: 347.58
 ---- batch: 030 ----
mean loss: 346.28
train mean loss: 348.77
epoch train time: 0:00:00.176612
elapsed time: 0:00:49.084527
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:45:32.634564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.76
 ---- batch: 020 ----
mean loss: 349.58
 ---- batch: 030 ----
mean loss: 354.92
train mean loss: 348.53
epoch train time: 0:00:00.170495
elapsed time: 0:00:49.255177
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:45:32.805222
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.03
 ---- batch: 020 ----
mean loss: 342.29
 ---- batch: 030 ----
mean loss: 349.99
train mean loss: 348.90
epoch train time: 0:00:00.178281
elapsed time: 0:00:49.433607
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:45:32.983643
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.21
 ---- batch: 020 ----
mean loss: 355.99
 ---- batch: 030 ----
mean loss: 350.08
train mean loss: 348.47
epoch train time: 0:00:00.176140
elapsed time: 0:00:49.609888
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:45:33.159924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.01
 ---- batch: 020 ----
mean loss: 346.19
 ---- batch: 030 ----
mean loss: 352.03
train mean loss: 349.22
epoch train time: 0:00:00.172712
elapsed time: 0:00:49.782740
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:45:33.332792
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.37
 ---- batch: 020 ----
mean loss: 348.13
 ---- batch: 030 ----
mean loss: 355.18
train mean loss: 348.34
epoch train time: 0:00:00.170318
elapsed time: 0:00:49.953213
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:45:33.503258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.43
 ---- batch: 020 ----
mean loss: 355.68
 ---- batch: 030 ----
mean loss: 345.67
train mean loss: 348.24
epoch train time: 0:00:00.171583
elapsed time: 0:00:50.124946
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:45:33.674982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.89
 ---- batch: 020 ----
mean loss: 346.54
 ---- batch: 030 ----
mean loss: 344.71
train mean loss: 348.13
epoch train time: 0:00:00.179988
elapsed time: 0:00:50.305074
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:45:33.855111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.27
 ---- batch: 020 ----
mean loss: 345.04
 ---- batch: 030 ----
mean loss: 349.82
train mean loss: 348.99
epoch train time: 0:00:00.170808
elapsed time: 0:00:50.476058
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:45:34.026141
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.59
 ---- batch: 020 ----
mean loss: 344.15
 ---- batch: 030 ----
mean loss: 349.33
train mean loss: 348.43
epoch train time: 0:00:00.170473
elapsed time: 0:00:50.646721
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:45:34.196757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.45
 ---- batch: 020 ----
mean loss: 349.89
 ---- batch: 030 ----
mean loss: 345.97
train mean loss: 348.17
epoch train time: 0:00:00.169509
elapsed time: 0:00:50.816370
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:45:34.366406
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.53
 ---- batch: 020 ----
mean loss: 346.00
 ---- batch: 030 ----
mean loss: 351.21
train mean loss: 348.86
epoch train time: 0:00:00.177435
elapsed time: 0:00:50.993957
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:45:34.543989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.08
 ---- batch: 020 ----
mean loss: 348.40
 ---- batch: 030 ----
mean loss: 345.23
train mean loss: 348.68
epoch train time: 0:00:00.181952
elapsed time: 0:00:51.176066
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:45:34.726101
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.41
 ---- batch: 020 ----
mean loss: 351.17
 ---- batch: 030 ----
mean loss: 343.35
train mean loss: 348.44
epoch train time: 0:00:00.179661
elapsed time: 0:00:51.355875
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:45:34.905922
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.94
 ---- batch: 020 ----
mean loss: 344.31
 ---- batch: 030 ----
mean loss: 346.93
train mean loss: 348.32
epoch train time: 0:00:00.183890
elapsed time: 0:00:51.539922
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:45:35.089958
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.77
 ---- batch: 020 ----
mean loss: 346.81
 ---- batch: 030 ----
mean loss: 345.08
train mean loss: 348.44
epoch train time: 0:00:00.178772
elapsed time: 0:00:51.718836
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:45:35.268878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.71
 ---- batch: 020 ----
mean loss: 335.58
 ---- batch: 030 ----
mean loss: 351.69
train mean loss: 348.20
epoch train time: 0:00:00.179151
elapsed time: 0:00:51.898148
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:45:35.448187
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.11
 ---- batch: 020 ----
mean loss: 346.24
 ---- batch: 030 ----
mean loss: 349.25
train mean loss: 348.29
epoch train time: 0:00:00.175371
elapsed time: 0:00:52.073674
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:45:35.623711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.72
 ---- batch: 020 ----
mean loss: 348.89
 ---- batch: 030 ----
mean loss: 350.77
train mean loss: 348.48
epoch train time: 0:00:00.176676
elapsed time: 0:00:52.250502
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:45:35.800539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.10
 ---- batch: 020 ----
mean loss: 347.20
 ---- batch: 030 ----
mean loss: 344.80
train mean loss: 348.43
epoch train time: 0:00:00.174132
elapsed time: 0:00:52.424774
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:45:35.974809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.98
 ---- batch: 020 ----
mean loss: 343.83
 ---- batch: 030 ----
mean loss: 346.30
train mean loss: 348.14
epoch train time: 0:00:00.175787
elapsed time: 0:00:52.600706
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:45:36.150743
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.12
 ---- batch: 020 ----
mean loss: 341.95
 ---- batch: 030 ----
mean loss: 351.75
train mean loss: 348.15
epoch train time: 0:00:00.173783
elapsed time: 0:00:52.774651
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:45:36.324706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.20
 ---- batch: 020 ----
mean loss: 357.46
 ---- batch: 030 ----
mean loss: 349.66
train mean loss: 348.13
epoch train time: 0:00:00.173668
elapsed time: 0:00:52.948477
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:45:36.498527
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.46
 ---- batch: 020 ----
mean loss: 350.71
 ---- batch: 030 ----
mean loss: 348.62
train mean loss: 348.81
epoch train time: 0:00:00.177469
elapsed time: 0:00:53.126111
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:45:36.676154
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.10
 ---- batch: 020 ----
mean loss: 355.37
 ---- batch: 030 ----
mean loss: 340.29
train mean loss: 347.80
epoch train time: 0:00:00.181892
elapsed time: 0:00:53.308152
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:45:36.858188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.32
 ---- batch: 020 ----
mean loss: 338.81
 ---- batch: 030 ----
mean loss: 356.00
train mean loss: 348.06
epoch train time: 0:00:00.174587
elapsed time: 0:00:53.482880
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:45:37.032915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.61
 ---- batch: 020 ----
mean loss: 342.09
 ---- batch: 030 ----
mean loss: 355.75
train mean loss: 348.31
epoch train time: 0:00:00.172875
elapsed time: 0:00:53.655911
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:45:37.205973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.66
 ---- batch: 020 ----
mean loss: 351.82
 ---- batch: 030 ----
mean loss: 351.29
train mean loss: 348.06
epoch train time: 0:00:00.177632
elapsed time: 0:00:53.833723
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:45:37.383747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.86
 ---- batch: 020 ----
mean loss: 347.97
 ---- batch: 030 ----
mean loss: 349.91
train mean loss: 348.40
epoch train time: 0:00:00.177080
elapsed time: 0:00:54.010934
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:45:37.560970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.02
 ---- batch: 020 ----
mean loss: 347.15
 ---- batch: 030 ----
mean loss: 352.31
train mean loss: 347.89
epoch train time: 0:00:00.176558
elapsed time: 0:00:54.187660
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:45:37.737702
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.48
 ---- batch: 020 ----
mean loss: 350.87
 ---- batch: 030 ----
mean loss: 342.68
train mean loss: 348.25
epoch train time: 0:00:00.178111
elapsed time: 0:00:54.365936
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:45:37.915974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.12
 ---- batch: 020 ----
mean loss: 353.72
 ---- batch: 030 ----
mean loss: 346.79
train mean loss: 348.13
epoch train time: 0:00:00.173339
elapsed time: 0:00:54.539419
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:45:38.089477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.45
 ---- batch: 020 ----
mean loss: 354.50
 ---- batch: 030 ----
mean loss: 337.11
train mean loss: 348.18
epoch train time: 0:00:00.173910
elapsed time: 0:00:54.713500
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:45:38.263538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.05
 ---- batch: 020 ----
mean loss: 353.54
 ---- batch: 030 ----
mean loss: 346.40
train mean loss: 348.15
epoch train time: 0:00:00.172989
elapsed time: 0:00:54.886628
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:45:38.436664
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.88
 ---- batch: 020 ----
mean loss: 342.92
 ---- batch: 030 ----
mean loss: 346.26
train mean loss: 347.63
epoch train time: 0:00:00.177245
elapsed time: 0:00:55.064023
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:45:38.614058
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.42
 ---- batch: 020 ----
mean loss: 347.54
 ---- batch: 030 ----
mean loss: 347.14
train mean loss: 347.89
epoch train time: 0:00:00.182576
elapsed time: 0:00:55.246750
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:45:38.796815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.45
 ---- batch: 020 ----
mean loss: 343.89
 ---- batch: 030 ----
mean loss: 348.20
train mean loss: 348.36
epoch train time: 0:00:00.180687
elapsed time: 0:00:55.427644
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:45:38.977682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.57
 ---- batch: 020 ----
mean loss: 353.58
 ---- batch: 030 ----
mean loss: 346.03
train mean loss: 347.71
epoch train time: 0:00:00.175440
elapsed time: 0:00:55.603243
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:45:39.153280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.31
 ---- batch: 020 ----
mean loss: 350.27
 ---- batch: 030 ----
mean loss: 340.86
train mean loss: 348.16
epoch train time: 0:00:00.173368
elapsed time: 0:00:55.776748
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:45:39.326783
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.59
 ---- batch: 020 ----
mean loss: 344.60
 ---- batch: 030 ----
mean loss: 348.10
train mean loss: 348.08
epoch train time: 0:00:00.172538
elapsed time: 0:00:55.949492
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:45:39.499563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.09
 ---- batch: 020 ----
mean loss: 359.23
 ---- batch: 030 ----
mean loss: 343.24
train mean loss: 348.12
epoch train time: 0:00:00.175687
elapsed time: 0:00:56.125355
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:45:39.675420
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.31
 ---- batch: 020 ----
mean loss: 349.55
 ---- batch: 030 ----
mean loss: 342.27
train mean loss: 348.13
epoch train time: 0:00:00.181799
elapsed time: 0:00:56.307330
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:45:39.857367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.46
 ---- batch: 020 ----
mean loss: 346.05
 ---- batch: 030 ----
mean loss: 353.68
train mean loss: 347.93
epoch train time: 0:00:00.178042
elapsed time: 0:00:56.485542
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:45:40.035579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.25
 ---- batch: 020 ----
mean loss: 342.73
 ---- batch: 030 ----
mean loss: 349.68
train mean loss: 347.71
epoch train time: 0:00:00.178121
elapsed time: 0:00:56.666450
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_4/checkpoint.pth.tar
**** end time: 2019-09-27 16:45:40.216454 ****
