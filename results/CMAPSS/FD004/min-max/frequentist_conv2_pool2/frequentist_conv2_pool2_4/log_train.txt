Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_4', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17276
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 01:40:51.569968 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:40:51.575281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4879.10
 ---- batch: 020 ----
mean loss: 4733.85
 ---- batch: 030 ----
mean loss: 4589.09
 ---- batch: 040 ----
mean loss: 4413.73
 ---- batch: 050 ----
mean loss: 4247.25
 ---- batch: 060 ----
mean loss: 4024.65
 ---- batch: 070 ----
mean loss: 3858.54
 ---- batch: 080 ----
mean loss: 3645.52
 ---- batch: 090 ----
mean loss: 3428.53
 ---- batch: 100 ----
mean loss: 3238.67
 ---- batch: 110 ----
mean loss: 3037.29
train mean loss: 3978.65
epoch train time: 0:00:33.991319
elapsed time: 0:00:33.997948
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:41:25.567957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2738.93
 ---- batch: 020 ----
mean loss: 2521.07
 ---- batch: 030 ----
mean loss: 2353.53
 ---- batch: 040 ----
mean loss: 2179.98
 ---- batch: 050 ----
mean loss: 2039.16
 ---- batch: 060 ----
mean loss: 1882.67
 ---- batch: 070 ----
mean loss: 1747.96
 ---- batch: 080 ----
mean loss: 1644.69
 ---- batch: 090 ----
mean loss: 1536.27
 ---- batch: 100 ----
mean loss: 1433.99
 ---- batch: 110 ----
mean loss: 1349.29
train mean loss: 1931.54
epoch train time: 0:00:00.730268
elapsed time: 0:00:34.728350
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:41:26.298371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1264.05
 ---- batch: 020 ----
mean loss: 1215.59
 ---- batch: 030 ----
mean loss: 1163.46
 ---- batch: 040 ----
mean loss: 1119.21
 ---- batch: 050 ----
mean loss: 1065.21
 ---- batch: 060 ----
mean loss: 1023.54
 ---- batch: 070 ----
mean loss: 1022.98
 ---- batch: 080 ----
mean loss: 984.36
 ---- batch: 090 ----
mean loss: 963.30
 ---- batch: 100 ----
mean loss: 942.24
 ---- batch: 110 ----
mean loss: 930.10
train mean loss: 1058.95
epoch train time: 0:00:00.731319
elapsed time: 0:00:35.459841
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:41:27.029892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.19
 ---- batch: 020 ----
mean loss: 904.97
 ---- batch: 030 ----
mean loss: 902.65
 ---- batch: 040 ----
mean loss: 879.00
 ---- batch: 050 ----
mean loss: 863.25
 ---- batch: 060 ----
mean loss: 866.03
 ---- batch: 070 ----
mean loss: 868.19
 ---- batch: 080 ----
mean loss: 841.44
 ---- batch: 090 ----
mean loss: 867.25
 ---- batch: 100 ----
mean loss: 869.77
 ---- batch: 110 ----
mean loss: 849.72
train mean loss: 875.04
epoch train time: 0:00:00.726334
elapsed time: 0:00:36.186347
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:41:27.756389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.69
 ---- batch: 020 ----
mean loss: 846.03
 ---- batch: 030 ----
mean loss: 852.94
 ---- batch: 040 ----
mean loss: 859.45
 ---- batch: 050 ----
mean loss: 852.52
 ---- batch: 060 ----
mean loss: 838.33
 ---- batch: 070 ----
mean loss: 841.71
 ---- batch: 080 ----
mean loss: 838.97
 ---- batch: 090 ----
mean loss: 839.92
 ---- batch: 100 ----
mean loss: 854.97
 ---- batch: 110 ----
mean loss: 853.19
train mean loss: 847.50
epoch train time: 0:00:00.735181
elapsed time: 0:00:36.921705
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:41:28.491745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.66
 ---- batch: 020 ----
mean loss: 842.26
 ---- batch: 030 ----
mean loss: 847.19
 ---- batch: 040 ----
mean loss: 821.84
 ---- batch: 050 ----
mean loss: 836.09
 ---- batch: 060 ----
mean loss: 857.18
 ---- batch: 070 ----
mean loss: 835.31
 ---- batch: 080 ----
mean loss: 856.76
 ---- batch: 090 ----
mean loss: 836.36
 ---- batch: 100 ----
mean loss: 844.12
 ---- batch: 110 ----
mean loss: 842.04
train mean loss: 841.50
epoch train time: 0:00:00.731924
elapsed time: 0:00:37.653808
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:41:29.223824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.07
 ---- batch: 020 ----
mean loss: 824.47
 ---- batch: 030 ----
mean loss: 827.85
 ---- batch: 040 ----
mean loss: 837.49
 ---- batch: 050 ----
mean loss: 810.46
 ---- batch: 060 ----
mean loss: 838.93
 ---- batch: 070 ----
mean loss: 829.39
 ---- batch: 080 ----
mean loss: 852.89
 ---- batch: 090 ----
mean loss: 848.33
 ---- batch: 100 ----
mean loss: 852.47
 ---- batch: 110 ----
mean loss: 839.80
train mean loss: 837.44
epoch train time: 0:00:00.726427
elapsed time: 0:00:38.380374
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:41:29.950391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.57
 ---- batch: 020 ----
mean loss: 819.67
 ---- batch: 030 ----
mean loss: 854.44
 ---- batch: 040 ----
mean loss: 835.39
 ---- batch: 050 ----
mean loss: 835.95
 ---- batch: 060 ----
mean loss: 841.25
 ---- batch: 070 ----
mean loss: 812.01
 ---- batch: 080 ----
mean loss: 831.51
 ---- batch: 090 ----
mean loss: 834.79
 ---- batch: 100 ----
mean loss: 842.00
 ---- batch: 110 ----
mean loss: 837.00
train mean loss: 833.33
epoch train time: 0:00:00.727833
elapsed time: 0:00:39.108382
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:41:30.678436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.44
 ---- batch: 020 ----
mean loss: 835.54
 ---- batch: 030 ----
mean loss: 816.59
 ---- batch: 040 ----
mean loss: 816.34
 ---- batch: 050 ----
mean loss: 833.24
 ---- batch: 060 ----
mean loss: 816.34
 ---- batch: 070 ----
mean loss: 838.51
 ---- batch: 080 ----
mean loss: 823.43
 ---- batch: 090 ----
mean loss: 822.16
 ---- batch: 100 ----
mean loss: 846.90
 ---- batch: 110 ----
mean loss: 835.14
train mean loss: 829.05
epoch train time: 0:00:00.729679
elapsed time: 0:00:39.838240
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:41:31.408259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.93
 ---- batch: 020 ----
mean loss: 834.13
 ---- batch: 030 ----
mean loss: 805.88
 ---- batch: 040 ----
mean loss: 838.44
 ---- batch: 050 ----
mean loss: 819.05
 ---- batch: 060 ----
mean loss: 825.86
 ---- batch: 070 ----
mean loss: 819.40
 ---- batch: 080 ----
mean loss: 853.79
 ---- batch: 090 ----
mean loss: 814.93
 ---- batch: 100 ----
mean loss: 808.79
 ---- batch: 110 ----
mean loss: 830.22
train mean loss: 824.06
epoch train time: 0:00:00.720851
elapsed time: 0:00:40.559263
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:41:32.129314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.93
 ---- batch: 020 ----
mean loss: 804.82
 ---- batch: 030 ----
mean loss: 821.01
 ---- batch: 040 ----
mean loss: 837.46
 ---- batch: 050 ----
mean loss: 811.85
 ---- batch: 060 ----
mean loss: 815.60
 ---- batch: 070 ----
mean loss: 820.70
 ---- batch: 080 ----
mean loss: 813.56
 ---- batch: 090 ----
mean loss: 828.73
 ---- batch: 100 ----
mean loss: 812.88
 ---- batch: 110 ----
mean loss: 814.89
train mean loss: 818.86
epoch train time: 0:00:00.729278
elapsed time: 0:00:41.288715
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:41:32.858749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.34
 ---- batch: 020 ----
mean loss: 795.77
 ---- batch: 030 ----
mean loss: 830.56
 ---- batch: 040 ----
mean loss: 824.02
 ---- batch: 050 ----
mean loss: 814.67
 ---- batch: 060 ----
mean loss: 807.48
 ---- batch: 070 ----
mean loss: 811.70
 ---- batch: 080 ----
mean loss: 804.45
 ---- batch: 090 ----
mean loss: 819.69
 ---- batch: 100 ----
mean loss: 809.04
 ---- batch: 110 ----
mean loss: 782.11
train mean loss: 813.48
epoch train time: 0:00:00.727541
elapsed time: 0:00:42.016413
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:41:33.586428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 817.17
 ---- batch: 030 ----
mean loss: 814.58
 ---- batch: 040 ----
mean loss: 803.82
 ---- batch: 050 ----
mean loss: 806.57
 ---- batch: 060 ----
mean loss: 802.41
 ---- batch: 070 ----
mean loss: 817.88
 ---- batch: 080 ----
mean loss: 783.56
 ---- batch: 090 ----
mean loss: 804.67
 ---- batch: 100 ----
mean loss: 817.27
 ---- batch: 110 ----
mean loss: 789.87
train mean loss: 808.18
epoch train time: 0:00:00.724008
elapsed time: 0:00:42.740563
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:41:34.310595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 792.36
 ---- batch: 020 ----
mean loss: 806.55
 ---- batch: 030 ----
mean loss: 798.30
 ---- batch: 040 ----
mean loss: 787.72
 ---- batch: 050 ----
mean loss: 793.56
 ---- batch: 060 ----
mean loss: 808.44
 ---- batch: 070 ----
mean loss: 806.70
 ---- batch: 080 ----
mean loss: 808.25
 ---- batch: 090 ----
mean loss: 793.82
 ---- batch: 100 ----
mean loss: 810.69
 ---- batch: 110 ----
mean loss: 813.31
train mean loss: 802.52
epoch train time: 0:00:00.727513
elapsed time: 0:00:43.468232
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:41:35.038248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 801.66
 ---- batch: 020 ----
mean loss: 791.50
 ---- batch: 030 ----
mean loss: 807.88
 ---- batch: 040 ----
mean loss: 809.13
 ---- batch: 050 ----
mean loss: 800.42
 ---- batch: 060 ----
mean loss: 788.95
 ---- batch: 070 ----
mean loss: 793.89
 ---- batch: 080 ----
mean loss: 784.39
 ---- batch: 090 ----
mean loss: 793.14
 ---- batch: 100 ----
mean loss: 793.23
 ---- batch: 110 ----
mean loss: 811.86
train mean loss: 796.79
epoch train time: 0:00:00.723411
elapsed time: 0:00:44.191783
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:41:35.761799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 797.15
 ---- batch: 020 ----
mean loss: 799.66
 ---- batch: 030 ----
mean loss: 788.99
 ---- batch: 040 ----
mean loss: 776.95
 ---- batch: 050 ----
mean loss: 789.33
 ---- batch: 060 ----
mean loss: 799.94
 ---- batch: 070 ----
mean loss: 801.97
 ---- batch: 080 ----
mean loss: 779.07
 ---- batch: 090 ----
mean loss: 781.40
 ---- batch: 100 ----
mean loss: 802.66
 ---- batch: 110 ----
mean loss: 778.66
train mean loss: 791.43
epoch train time: 0:00:00.724864
elapsed time: 0:00:44.916806
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:41:36.486828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 782.92
 ---- batch: 020 ----
mean loss: 757.57
 ---- batch: 030 ----
mean loss: 784.36
 ---- batch: 040 ----
mean loss: 807.35
 ---- batch: 050 ----
mean loss: 804.47
 ---- batch: 060 ----
mean loss: 801.29
 ---- batch: 070 ----
mean loss: 797.99
 ---- batch: 080 ----
mean loss: 784.66
 ---- batch: 090 ----
mean loss: 771.58
 ---- batch: 100 ----
mean loss: 779.29
 ---- batch: 110 ----
mean loss: 777.08
train mean loss: 786.27
epoch train time: 0:00:00.729975
elapsed time: 0:00:45.646928
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:41:37.216946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.73
 ---- batch: 020 ----
mean loss: 788.48
 ---- batch: 030 ----
mean loss: 768.64
 ---- batch: 040 ----
mean loss: 787.37
 ---- batch: 050 ----
mean loss: 796.96
 ---- batch: 060 ----
mean loss: 764.56
 ---- batch: 070 ----
mean loss: 791.36
 ---- batch: 080 ----
mean loss: 775.18
 ---- batch: 090 ----
mean loss: 775.89
 ---- batch: 100 ----
mean loss: 792.61
 ---- batch: 110 ----
mean loss: 787.84
train mean loss: 780.67
epoch train time: 0:00:00.734023
elapsed time: 0:00:46.381095
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:41:37.951133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 776.18
 ---- batch: 020 ----
mean loss: 788.37
 ---- batch: 030 ----
mean loss: 775.82
 ---- batch: 040 ----
mean loss: 757.89
 ---- batch: 050 ----
mean loss: 765.89
 ---- batch: 060 ----
mean loss: 780.87
 ---- batch: 070 ----
mean loss: 774.23
 ---- batch: 080 ----
mean loss: 774.68
 ---- batch: 090 ----
mean loss: 779.30
 ---- batch: 100 ----
mean loss: 769.20
 ---- batch: 110 ----
mean loss: 792.19
train mean loss: 774.97
epoch train time: 0:00:00.734617
elapsed time: 0:00:47.115877
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:41:38.685895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 770.08
 ---- batch: 020 ----
mean loss: 777.90
 ---- batch: 030 ----
mean loss: 781.63
 ---- batch: 040 ----
mean loss: 764.73
 ---- batch: 050 ----
mean loss: 756.92
 ---- batch: 060 ----
mean loss: 777.42
 ---- batch: 070 ----
mean loss: 771.27
 ---- batch: 080 ----
mean loss: 773.27
 ---- batch: 090 ----
mean loss: 763.34
 ---- batch: 100 ----
mean loss: 769.01
 ---- batch: 110 ----
mean loss: 770.74
train mean loss: 769.63
epoch train time: 0:00:00.735562
elapsed time: 0:00:47.851582
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:41:39.421599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.59
 ---- batch: 020 ----
mean loss: 801.98
 ---- batch: 030 ----
mean loss: 767.50
 ---- batch: 040 ----
mean loss: 787.42
 ---- batch: 050 ----
mean loss: 770.50
 ---- batch: 060 ----
mean loss: 771.17
 ---- batch: 070 ----
mean loss: 760.70
 ---- batch: 080 ----
mean loss: 770.31
 ---- batch: 090 ----
mean loss: 752.59
 ---- batch: 100 ----
mean loss: 749.44
 ---- batch: 110 ----
mean loss: 744.01
train mean loss: 764.30
epoch train time: 0:00:00.733333
elapsed time: 0:00:48.585056
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:41:40.155073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 752.54
 ---- batch: 020 ----
mean loss: 775.01
 ---- batch: 030 ----
mean loss: 751.03
 ---- batch: 040 ----
mean loss: 776.07
 ---- batch: 050 ----
mean loss: 762.34
 ---- batch: 060 ----
mean loss: 755.55
 ---- batch: 070 ----
mean loss: 767.47
 ---- batch: 080 ----
mean loss: 754.17
 ---- batch: 090 ----
mean loss: 750.29
 ---- batch: 100 ----
mean loss: 748.16
 ---- batch: 110 ----
mean loss: 761.96
train mean loss: 758.98
epoch train time: 0:00:00.730916
elapsed time: 0:00:49.316123
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:41:40.886165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 740.20
 ---- batch: 020 ----
mean loss: 741.91
 ---- batch: 030 ----
mean loss: 734.31
 ---- batch: 040 ----
mean loss: 759.50
 ---- batch: 050 ----
mean loss: 772.43
 ---- batch: 060 ----
mean loss: 750.20
 ---- batch: 070 ----
mean loss: 773.33
 ---- batch: 080 ----
mean loss: 743.36
 ---- batch: 090 ----
mean loss: 746.91
 ---- batch: 100 ----
mean loss: 776.03
 ---- batch: 110 ----
mean loss: 745.64
train mean loss: 753.60
epoch train time: 0:00:00.739997
elapsed time: 0:00:50.056289
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:41:41.626323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 732.00
 ---- batch: 020 ----
mean loss: 750.19
 ---- batch: 030 ----
mean loss: 763.92
 ---- batch: 040 ----
mean loss: 744.18
 ---- batch: 050 ----
mean loss: 762.67
 ---- batch: 060 ----
mean loss: 742.04
 ---- batch: 070 ----
mean loss: 744.84
 ---- batch: 080 ----
mean loss: 757.34
 ---- batch: 090 ----
mean loss: 744.03
 ---- batch: 100 ----
mean loss: 753.24
 ---- batch: 110 ----
mean loss: 733.01
train mean loss: 748.10
epoch train time: 0:00:00.732040
elapsed time: 0:00:50.788491
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:41:42.358535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.56
 ---- batch: 020 ----
mean loss: 744.04
 ---- batch: 030 ----
mean loss: 730.98
 ---- batch: 040 ----
mean loss: 749.96
 ---- batch: 050 ----
mean loss: 744.03
 ---- batch: 060 ----
mean loss: 751.95
 ---- batch: 070 ----
mean loss: 725.50
 ---- batch: 080 ----
mean loss: 744.43
 ---- batch: 090 ----
mean loss: 751.13
 ---- batch: 100 ----
mean loss: 729.95
 ---- batch: 110 ----
mean loss: 750.22
train mean loss: 742.56
epoch train time: 0:00:00.735593
elapsed time: 0:00:51.524253
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:41:43.094269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 741.68
 ---- batch: 020 ----
mean loss: 738.34
 ---- batch: 030 ----
mean loss: 730.49
 ---- batch: 040 ----
mean loss: 737.92
 ---- batch: 050 ----
mean loss: 737.56
 ---- batch: 060 ----
mean loss: 744.23
 ---- batch: 070 ----
mean loss: 750.08
 ---- batch: 080 ----
mean loss: 722.70
 ---- batch: 090 ----
mean loss: 745.01
 ---- batch: 100 ----
mean loss: 730.87
 ---- batch: 110 ----
mean loss: 733.23
train mean loss: 736.77
epoch train time: 0:00:00.730008
elapsed time: 0:00:52.254447
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:41:43.824472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.46
 ---- batch: 020 ----
mean loss: 737.81
 ---- batch: 030 ----
mean loss: 743.09
 ---- batch: 040 ----
mean loss: 737.18
 ---- batch: 050 ----
mean loss: 726.70
 ---- batch: 060 ----
mean loss: 716.78
 ---- batch: 070 ----
mean loss: 714.37
 ---- batch: 080 ----
mean loss: 748.69
 ---- batch: 090 ----
mean loss: 743.08
 ---- batch: 100 ----
mean loss: 717.59
 ---- batch: 110 ----
mean loss: 722.46
train mean loss: 730.94
epoch train time: 0:00:00.730859
elapsed time: 0:00:52.985451
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:41:44.555474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.76
 ---- batch: 020 ----
mean loss: 716.20
 ---- batch: 030 ----
mean loss: 742.65
 ---- batch: 040 ----
mean loss: 742.37
 ---- batch: 050 ----
mean loss: 722.02
 ---- batch: 060 ----
mean loss: 722.22
 ---- batch: 070 ----
mean loss: 719.55
 ---- batch: 080 ----
mean loss: 721.41
 ---- batch: 090 ----
mean loss: 730.01
 ---- batch: 100 ----
mean loss: 714.81
 ---- batch: 110 ----
mean loss: 723.53
train mean loss: 724.66
epoch train time: 0:00:00.732357
elapsed time: 0:00:53.717956
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:41:45.287975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 686.81
 ---- batch: 020 ----
mean loss: 717.74
 ---- batch: 030 ----
mean loss: 729.57
 ---- batch: 040 ----
mean loss: 739.10
 ---- batch: 050 ----
mean loss: 738.75
 ---- batch: 060 ----
mean loss: 709.79
 ---- batch: 070 ----
mean loss: 718.53
 ---- batch: 080 ----
mean loss: 718.68
 ---- batch: 090 ----
mean loss: 703.95
 ---- batch: 100 ----
mean loss: 721.71
 ---- batch: 110 ----
mean loss: 714.17
train mean loss: 718.36
epoch train time: 0:00:00.738009
elapsed time: 0:00:54.456110
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:41:46.026129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.27
 ---- batch: 020 ----
mean loss: 697.75
 ---- batch: 030 ----
mean loss: 708.87
 ---- batch: 040 ----
mean loss: 707.43
 ---- batch: 050 ----
mean loss: 721.02
 ---- batch: 060 ----
mean loss: 709.09
 ---- batch: 070 ----
mean loss: 710.48
 ---- batch: 080 ----
mean loss: 720.99
 ---- batch: 090 ----
mean loss: 711.88
 ---- batch: 100 ----
mean loss: 718.03
 ---- batch: 110 ----
mean loss: 710.76
train mean loss: 711.75
epoch train time: 0:00:00.738856
elapsed time: 0:00:55.195107
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:41:46.765124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.79
 ---- batch: 020 ----
mean loss: 700.33
 ---- batch: 030 ----
mean loss: 710.87
 ---- batch: 040 ----
mean loss: 711.25
 ---- batch: 050 ----
mean loss: 707.87
 ---- batch: 060 ----
mean loss: 706.96
 ---- batch: 070 ----
mean loss: 672.84
 ---- batch: 080 ----
mean loss: 714.45
 ---- batch: 090 ----
mean loss: 706.04
 ---- batch: 100 ----
mean loss: 706.79
 ---- batch: 110 ----
mean loss: 710.33
train mean loss: 705.06
epoch train time: 0:00:00.731774
elapsed time: 0:00:55.927020
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:41:47.497081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.37
 ---- batch: 020 ----
mean loss: 686.28
 ---- batch: 030 ----
mean loss: 714.12
 ---- batch: 040 ----
mean loss: 718.01
 ---- batch: 050 ----
mean loss: 681.93
 ---- batch: 060 ----
mean loss: 702.65
 ---- batch: 070 ----
mean loss: 699.25
 ---- batch: 080 ----
mean loss: 698.85
 ---- batch: 090 ----
mean loss: 693.26
 ---- batch: 100 ----
mean loss: 697.50
 ---- batch: 110 ----
mean loss: 685.16
train mean loss: 698.28
epoch train time: 0:00:00.731719
elapsed time: 0:00:56.658923
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:41:48.228939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.59
 ---- batch: 020 ----
mean loss: 684.79
 ---- batch: 030 ----
mean loss: 702.95
 ---- batch: 040 ----
mean loss: 704.93
 ---- batch: 050 ----
mean loss: 694.21
 ---- batch: 060 ----
mean loss: 700.46
 ---- batch: 070 ----
mean loss: 666.69
 ---- batch: 080 ----
mean loss: 699.28
 ---- batch: 090 ----
mean loss: 687.61
 ---- batch: 100 ----
mean loss: 697.17
 ---- batch: 110 ----
mean loss: 681.18
train mean loss: 690.95
epoch train time: 0:00:00.736567
elapsed time: 0:00:57.395629
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:41:48.965665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.54
 ---- batch: 020 ----
mean loss: 688.60
 ---- batch: 030 ----
mean loss: 678.36
 ---- batch: 040 ----
mean loss: 685.45
 ---- batch: 050 ----
mean loss: 669.80
 ---- batch: 060 ----
mean loss: 677.19
 ---- batch: 070 ----
mean loss: 691.88
 ---- batch: 080 ----
mean loss: 690.94
 ---- batch: 090 ----
mean loss: 690.35
 ---- batch: 100 ----
mean loss: 688.87
 ---- batch: 110 ----
mean loss: 684.46
train mean loss: 683.64
epoch train time: 0:00:00.729345
elapsed time: 0:00:58.125134
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:41:49.695186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.59
 ---- batch: 020 ----
mean loss: 652.14
 ---- batch: 030 ----
mean loss: 672.54
 ---- batch: 040 ----
mean loss: 690.22
 ---- batch: 050 ----
mean loss: 676.96
 ---- batch: 060 ----
mean loss: 685.86
 ---- batch: 070 ----
mean loss: 660.26
 ---- batch: 080 ----
mean loss: 683.96
 ---- batch: 090 ----
mean loss: 683.17
 ---- batch: 100 ----
mean loss: 683.54
 ---- batch: 110 ----
mean loss: 679.37
train mean loss: 676.09
epoch train time: 0:00:00.729674
elapsed time: 0:00:58.854989
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:41:50.425010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 697.66
 ---- batch: 020 ----
mean loss: 680.17
 ---- batch: 030 ----
mean loss: 667.10
 ---- batch: 040 ----
mean loss: 666.75
 ---- batch: 050 ----
mean loss: 668.51
 ---- batch: 060 ----
mean loss: 661.81
 ---- batch: 070 ----
mean loss: 664.77
 ---- batch: 080 ----
mean loss: 666.47
 ---- batch: 090 ----
mean loss: 653.72
 ---- batch: 100 ----
mean loss: 666.54
 ---- batch: 110 ----
mean loss: 659.35
train mean loss: 668.31
epoch train time: 0:00:00.732851
elapsed time: 0:00:59.587999
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:41:51.158015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.62
 ---- batch: 020 ----
mean loss: 660.47
 ---- batch: 030 ----
mean loss: 668.77
 ---- batch: 040 ----
mean loss: 662.05
 ---- batch: 050 ----
mean loss: 652.94
 ---- batch: 060 ----
mean loss: 668.34
 ---- batch: 070 ----
mean loss: 652.27
 ---- batch: 080 ----
mean loss: 650.10
 ---- batch: 090 ----
mean loss: 668.65
 ---- batch: 100 ----
mean loss: 662.23
 ---- batch: 110 ----
mean loss: 641.03
train mean loss: 660.12
epoch train time: 0:00:00.735697
elapsed time: 0:01:00.323868
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:41:51.893908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 657.32
 ---- batch: 020 ----
mean loss: 661.16
 ---- batch: 030 ----
mean loss: 655.87
 ---- batch: 040 ----
mean loss: 653.60
 ---- batch: 050 ----
mean loss: 669.52
 ---- batch: 060 ----
mean loss: 637.13
 ---- batch: 070 ----
mean loss: 643.42
 ---- batch: 080 ----
mean loss: 643.85
 ---- batch: 090 ----
mean loss: 637.94
 ---- batch: 100 ----
mean loss: 647.25
 ---- batch: 110 ----
mean loss: 661.35
train mean loss: 651.32
epoch train time: 0:00:00.729324
elapsed time: 0:01:01.053356
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:41:52.623396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 654.18
 ---- batch: 020 ----
mean loss: 648.55
 ---- batch: 030 ----
mean loss: 648.90
 ---- batch: 040 ----
mean loss: 644.20
 ---- batch: 050 ----
mean loss: 623.13
 ---- batch: 060 ----
mean loss: 636.06
 ---- batch: 070 ----
mean loss: 637.16
 ---- batch: 080 ----
mean loss: 648.80
 ---- batch: 090 ----
mean loss: 634.23
 ---- batch: 100 ----
mean loss: 631.26
 ---- batch: 110 ----
mean loss: 640.62
train mean loss: 640.88
epoch train time: 0:00:00.733495
elapsed time: 0:01:01.787032
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:41:53.357047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 632.02
 ---- batch: 020 ----
mean loss: 636.04
 ---- batch: 030 ----
mean loss: 618.37
 ---- batch: 040 ----
mean loss: 626.98
 ---- batch: 050 ----
mean loss: 629.51
 ---- batch: 060 ----
mean loss: 618.85
 ---- batch: 070 ----
mean loss: 620.31
 ---- batch: 080 ----
mean loss: 625.60
 ---- batch: 090 ----
mean loss: 612.67
 ---- batch: 100 ----
mean loss: 619.79
 ---- batch: 110 ----
mean loss: 614.10
train mean loss: 621.88
epoch train time: 0:00:00.727813
elapsed time: 0:01:02.514988
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:41:54.085008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.58
 ---- batch: 020 ----
mean loss: 612.15
 ---- batch: 030 ----
mean loss: 590.54
 ---- batch: 040 ----
mean loss: 603.36
 ---- batch: 050 ----
mean loss: 598.72
 ---- batch: 060 ----
mean loss: 595.42
 ---- batch: 070 ----
mean loss: 596.25
 ---- batch: 080 ----
mean loss: 595.26
 ---- batch: 090 ----
mean loss: 574.54
 ---- batch: 100 ----
mean loss: 583.51
 ---- batch: 110 ----
mean loss: 586.51
train mean loss: 593.33
epoch train time: 0:00:00.729331
elapsed time: 0:01:03.244469
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:41:54.814488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.78
 ---- batch: 020 ----
mean loss: 571.19
 ---- batch: 030 ----
mean loss: 578.67
 ---- batch: 040 ----
mean loss: 575.30
 ---- batch: 050 ----
mean loss: 564.17
 ---- batch: 060 ----
mean loss: 559.15
 ---- batch: 070 ----
mean loss: 583.62
 ---- batch: 080 ----
mean loss: 579.72
 ---- batch: 090 ----
mean loss: 567.98
 ---- batch: 100 ----
mean loss: 561.21
 ---- batch: 110 ----
mean loss: 552.88
train mean loss: 569.85
epoch train time: 0:00:00.731748
elapsed time: 0:01:03.976356
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:41:55.546371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 559.60
 ---- batch: 020 ----
mean loss: 550.17
 ---- batch: 030 ----
mean loss: 554.75
 ---- batch: 040 ----
mean loss: 560.98
 ---- batch: 050 ----
mean loss: 553.08
 ---- batch: 060 ----
mean loss: 551.06
 ---- batch: 070 ----
mean loss: 553.21
 ---- batch: 080 ----
mean loss: 541.42
 ---- batch: 090 ----
mean loss: 542.83
 ---- batch: 100 ----
mean loss: 526.93
 ---- batch: 110 ----
mean loss: 543.63
train mean loss: 548.54
epoch train time: 0:00:00.732345
elapsed time: 0:01:04.708842
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:41:56.278922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.30
 ---- batch: 020 ----
mean loss: 526.47
 ---- batch: 030 ----
mean loss: 543.69
 ---- batch: 040 ----
mean loss: 537.03
 ---- batch: 050 ----
mean loss: 531.62
 ---- batch: 060 ----
mean loss: 531.90
 ---- batch: 070 ----
mean loss: 519.63
 ---- batch: 080 ----
mean loss: 534.27
 ---- batch: 090 ----
mean loss: 521.11
 ---- batch: 100 ----
mean loss: 529.14
 ---- batch: 110 ----
mean loss: 516.73
train mean loss: 529.68
epoch train time: 0:00:00.733408
elapsed time: 0:01:05.442451
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:41:57.012497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.91
 ---- batch: 020 ----
mean loss: 519.57
 ---- batch: 030 ----
mean loss: 507.27
 ---- batch: 040 ----
mean loss: 516.33
 ---- batch: 050 ----
mean loss: 513.17
 ---- batch: 060 ----
mean loss: 541.27
 ---- batch: 070 ----
mean loss: 520.07
 ---- batch: 080 ----
mean loss: 503.91
 ---- batch: 090 ----
mean loss: 508.61
 ---- batch: 100 ----
mean loss: 493.64
 ---- batch: 110 ----
mean loss: 505.41
train mean loss: 512.27
epoch train time: 0:00:00.725190
elapsed time: 0:01:06.167821
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:41:57.737848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.23
 ---- batch: 020 ----
mean loss: 515.44
 ---- batch: 030 ----
mean loss: 508.74
 ---- batch: 040 ----
mean loss: 505.59
 ---- batch: 050 ----
mean loss: 494.08
 ---- batch: 060 ----
mean loss: 489.69
 ---- batch: 070 ----
mean loss: 491.55
 ---- batch: 080 ----
mean loss: 492.63
 ---- batch: 090 ----
mean loss: 482.63
 ---- batch: 100 ----
mean loss: 496.74
 ---- batch: 110 ----
mean loss: 494.45
train mean loss: 496.32
epoch train time: 0:00:00.727945
elapsed time: 0:01:06.895948
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:41:58.465987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.44
 ---- batch: 020 ----
mean loss: 493.79
 ---- batch: 030 ----
mean loss: 488.30
 ---- batch: 040 ----
mean loss: 480.44
 ---- batch: 050 ----
mean loss: 484.21
 ---- batch: 060 ----
mean loss: 467.97
 ---- batch: 070 ----
mean loss: 492.70
 ---- batch: 080 ----
mean loss: 479.70
 ---- batch: 090 ----
mean loss: 471.04
 ---- batch: 100 ----
mean loss: 472.17
 ---- batch: 110 ----
mean loss: 483.20
train mean loss: 481.72
epoch train time: 0:00:00.733013
elapsed time: 0:01:07.629125
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:41:59.199140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.97
 ---- batch: 020 ----
mean loss: 484.15
 ---- batch: 030 ----
mean loss: 468.50
 ---- batch: 040 ----
mean loss: 461.93
 ---- batch: 050 ----
mean loss: 474.89
 ---- batch: 060 ----
mean loss: 464.25
 ---- batch: 070 ----
mean loss: 482.83
 ---- batch: 080 ----
mean loss: 462.92
 ---- batch: 090 ----
mean loss: 472.68
 ---- batch: 100 ----
mean loss: 447.31
 ---- batch: 110 ----
mean loss: 461.51
train mean loss: 468.14
epoch train time: 0:00:00.730754
elapsed time: 0:01:08.360022
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:41:59.930042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.70
 ---- batch: 020 ----
mean loss: 456.46
 ---- batch: 030 ----
mean loss: 463.59
 ---- batch: 040 ----
mean loss: 444.75
 ---- batch: 050 ----
mean loss: 451.87
 ---- batch: 060 ----
mean loss: 461.31
 ---- batch: 070 ----
mean loss: 459.46
 ---- batch: 080 ----
mean loss: 463.89
 ---- batch: 090 ----
mean loss: 450.69
 ---- batch: 100 ----
mean loss: 449.72
 ---- batch: 110 ----
mean loss: 446.60
train mean loss: 455.78
epoch train time: 0:00:00.741018
elapsed time: 0:01:09.101197
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:42:00.671216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.80
 ---- batch: 020 ----
mean loss: 453.95
 ---- batch: 030 ----
mean loss: 442.21
 ---- batch: 040 ----
mean loss: 450.87
 ---- batch: 050 ----
mean loss: 437.90
 ---- batch: 060 ----
mean loss: 441.78
 ---- batch: 070 ----
mean loss: 440.33
 ---- batch: 080 ----
mean loss: 449.06
 ---- batch: 090 ----
mean loss: 447.37
 ---- batch: 100 ----
mean loss: 436.48
 ---- batch: 110 ----
mean loss: 423.48
train mean loss: 444.06
epoch train time: 0:00:00.734529
elapsed time: 0:01:09.835887
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:42:01.405915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.22
 ---- batch: 020 ----
mean loss: 436.42
 ---- batch: 030 ----
mean loss: 430.59
 ---- batch: 040 ----
mean loss: 438.86
 ---- batch: 050 ----
mean loss: 436.81
 ---- batch: 060 ----
mean loss: 431.70
 ---- batch: 070 ----
mean loss: 435.49
 ---- batch: 080 ----
mean loss: 428.16
 ---- batch: 090 ----
mean loss: 429.49
 ---- batch: 100 ----
mean loss: 429.98
 ---- batch: 110 ----
mean loss: 429.05
train mean loss: 433.29
epoch train time: 0:00:00.734072
elapsed time: 0:01:10.570111
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:42:02.140147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.76
 ---- batch: 020 ----
mean loss: 431.93
 ---- batch: 030 ----
mean loss: 440.91
 ---- batch: 040 ----
mean loss: 420.25
 ---- batch: 050 ----
mean loss: 414.15
 ---- batch: 060 ----
mean loss: 421.86
 ---- batch: 070 ----
mean loss: 420.22
 ---- batch: 080 ----
mean loss: 420.15
 ---- batch: 090 ----
mean loss: 413.87
 ---- batch: 100 ----
mean loss: 428.02
 ---- batch: 110 ----
mean loss: 421.64
train mean loss: 422.66
epoch train time: 0:00:00.727707
elapsed time: 0:01:11.297985
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:42:02.868003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.05
 ---- batch: 020 ----
mean loss: 426.20
 ---- batch: 030 ----
mean loss: 415.53
 ---- batch: 040 ----
mean loss: 404.86
 ---- batch: 050 ----
mean loss: 398.40
 ---- batch: 060 ----
mean loss: 411.40
 ---- batch: 070 ----
mean loss: 415.60
 ---- batch: 080 ----
mean loss: 418.18
 ---- batch: 090 ----
mean loss: 403.21
 ---- batch: 100 ----
mean loss: 405.39
 ---- batch: 110 ----
mean loss: 417.41
train mean loss: 412.44
epoch train time: 0:00:00.722710
elapsed time: 0:01:12.020836
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:42:03.590853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.65
 ---- batch: 020 ----
mean loss: 406.52
 ---- batch: 030 ----
mean loss: 411.44
 ---- batch: 040 ----
mean loss: 398.47
 ---- batch: 050 ----
mean loss: 398.82
 ---- batch: 060 ----
mean loss: 407.11
 ---- batch: 070 ----
mean loss: 403.45
 ---- batch: 080 ----
mean loss: 403.43
 ---- batch: 090 ----
mean loss: 390.05
 ---- batch: 100 ----
mean loss: 403.21
 ---- batch: 110 ----
mean loss: 397.10
train mean loss: 402.34
epoch train time: 0:00:00.734982
elapsed time: 0:01:12.755959
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:42:04.325977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.11
 ---- batch: 020 ----
mean loss: 391.19
 ---- batch: 030 ----
mean loss: 405.41
 ---- batch: 040 ----
mean loss: 401.10
 ---- batch: 050 ----
mean loss: 399.88
 ---- batch: 060 ----
mean loss: 377.35
 ---- batch: 070 ----
mean loss: 382.55
 ---- batch: 080 ----
mean loss: 384.85
 ---- batch: 090 ----
mean loss: 388.07
 ---- batch: 100 ----
mean loss: 402.58
 ---- batch: 110 ----
mean loss: 389.49
train mean loss: 392.28
epoch train time: 0:00:00.735212
elapsed time: 0:01:13.491311
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:42:05.061329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.75
 ---- batch: 020 ----
mean loss: 386.98
 ---- batch: 030 ----
mean loss: 397.07
 ---- batch: 040 ----
mean loss: 381.76
 ---- batch: 050 ----
mean loss: 383.03
 ---- batch: 060 ----
mean loss: 375.02
 ---- batch: 070 ----
mean loss: 370.14
 ---- batch: 080 ----
mean loss: 382.53
 ---- batch: 090 ----
mean loss: 386.04
 ---- batch: 100 ----
mean loss: 371.64
 ---- batch: 110 ----
mean loss: 377.15
train mean loss: 382.80
epoch train time: 0:00:00.724201
elapsed time: 0:01:14.215733
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:42:05.785808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.32
 ---- batch: 020 ----
mean loss: 376.45
 ---- batch: 030 ----
mean loss: 380.90
 ---- batch: 040 ----
mean loss: 372.68
 ---- batch: 050 ----
mean loss: 384.44
 ---- batch: 060 ----
mean loss: 382.44
 ---- batch: 070 ----
mean loss: 361.37
 ---- batch: 080 ----
mean loss: 361.57
 ---- batch: 090 ----
mean loss: 370.30
 ---- batch: 100 ----
mean loss: 367.86
 ---- batch: 110 ----
mean loss: 370.62
train mean loss: 373.61
epoch train time: 0:00:00.728816
elapsed time: 0:01:14.944745
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:42:06.514778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.50
 ---- batch: 020 ----
mean loss: 370.17
 ---- batch: 030 ----
mean loss: 357.13
 ---- batch: 040 ----
mean loss: 362.74
 ---- batch: 050 ----
mean loss: 360.14
 ---- batch: 060 ----
mean loss: 363.07
 ---- batch: 070 ----
mean loss: 364.81
 ---- batch: 080 ----
mean loss: 352.40
 ---- batch: 090 ----
mean loss: 371.66
 ---- batch: 100 ----
mean loss: 363.07
 ---- batch: 110 ----
mean loss: 368.18
train mean loss: 364.73
epoch train time: 0:00:00.730318
elapsed time: 0:01:15.675219
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:42:07.245238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.72
 ---- batch: 020 ----
mean loss: 362.96
 ---- batch: 030 ----
mean loss: 360.06
 ---- batch: 040 ----
mean loss: 358.92
 ---- batch: 050 ----
mean loss: 352.13
 ---- batch: 060 ----
mean loss: 338.44
 ---- batch: 070 ----
mean loss: 352.41
 ---- batch: 080 ----
mean loss: 361.98
 ---- batch: 090 ----
mean loss: 352.06
 ---- batch: 100 ----
mean loss: 358.24
 ---- batch: 110 ----
mean loss: 355.94
train mean loss: 355.73
epoch train time: 0:00:00.732654
elapsed time: 0:01:16.408021
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:42:07.978056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.65
 ---- batch: 020 ----
mean loss: 350.88
 ---- batch: 030 ----
mean loss: 359.29
 ---- batch: 040 ----
mean loss: 346.17
 ---- batch: 050 ----
mean loss: 337.96
 ---- batch: 060 ----
mean loss: 343.75
 ---- batch: 070 ----
mean loss: 331.37
 ---- batch: 080 ----
mean loss: 346.09
 ---- batch: 090 ----
mean loss: 358.24
 ---- batch: 100 ----
mean loss: 347.90
 ---- batch: 110 ----
mean loss: 343.06
train mean loss: 347.31
epoch train time: 0:00:00.724877
elapsed time: 0:01:17.133071
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:42:08.703092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.40
 ---- batch: 020 ----
mean loss: 334.05
 ---- batch: 030 ----
mean loss: 346.90
 ---- batch: 040 ----
mean loss: 338.89
 ---- batch: 050 ----
mean loss: 348.84
 ---- batch: 060 ----
mean loss: 332.66
 ---- batch: 070 ----
mean loss: 331.59
 ---- batch: 080 ----
mean loss: 336.47
 ---- batch: 090 ----
mean loss: 333.47
 ---- batch: 100 ----
mean loss: 340.52
 ---- batch: 110 ----
mean loss: 337.44
train mean loss: 338.93
epoch train time: 0:00:00.739175
elapsed time: 0:01:17.872406
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:42:09.442445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.78
 ---- batch: 020 ----
mean loss: 337.22
 ---- batch: 030 ----
mean loss: 334.05
 ---- batch: 040 ----
mean loss: 322.01
 ---- batch: 050 ----
mean loss: 331.25
 ---- batch: 060 ----
mean loss: 333.54
 ---- batch: 070 ----
mean loss: 338.71
 ---- batch: 080 ----
mean loss: 326.95
 ---- batch: 090 ----
mean loss: 336.96
 ---- batch: 100 ----
mean loss: 322.45
 ---- batch: 110 ----
mean loss: 337.14
train mean loss: 331.12
epoch train time: 0:00:00.728657
elapsed time: 0:01:18.601224
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:42:10.171239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.37
 ---- batch: 020 ----
mean loss: 315.58
 ---- batch: 030 ----
mean loss: 320.98
 ---- batch: 040 ----
mean loss: 332.84
 ---- batch: 050 ----
mean loss: 324.81
 ---- batch: 060 ----
mean loss: 321.54
 ---- batch: 070 ----
mean loss: 309.68
 ---- batch: 080 ----
mean loss: 329.60
 ---- batch: 090 ----
mean loss: 323.29
 ---- batch: 100 ----
mean loss: 321.03
 ---- batch: 110 ----
mean loss: 327.92
train mean loss: 323.75
epoch train time: 0:00:00.735085
elapsed time: 0:01:19.336449
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:42:10.906466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.27
 ---- batch: 020 ----
mean loss: 311.51
 ---- batch: 030 ----
mean loss: 317.18
 ---- batch: 040 ----
mean loss: 322.81
 ---- batch: 050 ----
mean loss: 307.02
 ---- batch: 060 ----
mean loss: 310.94
 ---- batch: 070 ----
mean loss: 326.62
 ---- batch: 080 ----
mean loss: 312.45
 ---- batch: 090 ----
mean loss: 311.95
 ---- batch: 100 ----
mean loss: 318.51
 ---- batch: 110 ----
mean loss: 331.48
train mean loss: 316.75
epoch train time: 0:00:00.737018
elapsed time: 0:01:20.073604
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:42:11.643654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.06
 ---- batch: 020 ----
mean loss: 315.50
 ---- batch: 030 ----
mean loss: 312.22
 ---- batch: 040 ----
mean loss: 295.56
 ---- batch: 050 ----
mean loss: 310.29
 ---- batch: 060 ----
mean loss: 311.98
 ---- batch: 070 ----
mean loss: 313.10
 ---- batch: 080 ----
mean loss: 312.65
 ---- batch: 090 ----
mean loss: 322.53
 ---- batch: 100 ----
mean loss: 305.15
 ---- batch: 110 ----
mean loss: 304.77
train mean loss: 310.57
epoch train time: 0:00:00.726669
elapsed time: 0:01:20.800448
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:42:12.370482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.67
 ---- batch: 020 ----
mean loss: 303.22
 ---- batch: 030 ----
mean loss: 307.44
 ---- batch: 040 ----
mean loss: 310.42
 ---- batch: 050 ----
mean loss: 315.10
 ---- batch: 060 ----
mean loss: 298.31
 ---- batch: 070 ----
mean loss: 307.26
 ---- batch: 080 ----
mean loss: 302.28
 ---- batch: 090 ----
mean loss: 295.92
 ---- batch: 100 ----
mean loss: 294.16
 ---- batch: 110 ----
mean loss: 305.12
train mean loss: 304.82
epoch train time: 0:00:00.733554
elapsed time: 0:01:21.534176
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:42:13.104193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.79
 ---- batch: 020 ----
mean loss: 305.08
 ---- batch: 030 ----
mean loss: 302.05
 ---- batch: 040 ----
mean loss: 298.80
 ---- batch: 050 ----
mean loss: 293.92
 ---- batch: 060 ----
mean loss: 305.95
 ---- batch: 070 ----
mean loss: 300.60
 ---- batch: 080 ----
mean loss: 305.13
 ---- batch: 090 ----
mean loss: 298.11
 ---- batch: 100 ----
mean loss: 298.48
 ---- batch: 110 ----
mean loss: 291.20
train mean loss: 300.04
epoch train time: 0:00:00.725633
elapsed time: 0:01:22.259984
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:42:13.830049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.94
 ---- batch: 020 ----
mean loss: 297.38
 ---- batch: 030 ----
mean loss: 289.81
 ---- batch: 040 ----
mean loss: 298.66
 ---- batch: 050 ----
mean loss: 298.29
 ---- batch: 060 ----
mean loss: 288.38
 ---- batch: 070 ----
mean loss: 289.88
 ---- batch: 080 ----
mean loss: 290.48
 ---- batch: 090 ----
mean loss: 297.29
 ---- batch: 100 ----
mean loss: 296.41
 ---- batch: 110 ----
mean loss: 300.53
train mean loss: 295.51
epoch train time: 0:00:00.750475
elapsed time: 0:01:23.010648
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:42:14.580666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.75
 ---- batch: 020 ----
mean loss: 296.95
 ---- batch: 030 ----
mean loss: 286.25
 ---- batch: 040 ----
mean loss: 295.52
 ---- batch: 050 ----
mean loss: 294.28
 ---- batch: 060 ----
mean loss: 300.74
 ---- batch: 070 ----
mean loss: 296.95
 ---- batch: 080 ----
mean loss: 284.49
 ---- batch: 090 ----
mean loss: 289.24
 ---- batch: 100 ----
mean loss: 282.04
 ---- batch: 110 ----
mean loss: 294.21
train mean loss: 291.38
epoch train time: 0:00:00.733999
elapsed time: 0:01:23.744822
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:42:15.314840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.53
 ---- batch: 020 ----
mean loss: 287.29
 ---- batch: 030 ----
mean loss: 289.27
 ---- batch: 040 ----
mean loss: 280.36
 ---- batch: 050 ----
mean loss: 285.45
 ---- batch: 060 ----
mean loss: 298.68
 ---- batch: 070 ----
mean loss: 289.86
 ---- batch: 080 ----
mean loss: 284.40
 ---- batch: 090 ----
mean loss: 288.04
 ---- batch: 100 ----
mean loss: 277.14
 ---- batch: 110 ----
mean loss: 292.45
train mean loss: 287.99
epoch train time: 0:00:00.731797
elapsed time: 0:01:24.476760
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:42:16.046807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.01
 ---- batch: 020 ----
mean loss: 287.36
 ---- batch: 030 ----
mean loss: 289.86
 ---- batch: 040 ----
mean loss: 287.05
 ---- batch: 050 ----
mean loss: 280.25
 ---- batch: 060 ----
mean loss: 275.08
 ---- batch: 070 ----
mean loss: 286.74
 ---- batch: 080 ----
mean loss: 283.56
 ---- batch: 090 ----
mean loss: 287.61
 ---- batch: 100 ----
mean loss: 286.79
 ---- batch: 110 ----
mean loss: 283.93
train mean loss: 284.81
epoch train time: 0:00:00.725912
elapsed time: 0:01:25.202861
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:42:16.772903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.34
 ---- batch: 020 ----
mean loss: 279.65
 ---- batch: 030 ----
mean loss: 281.95
 ---- batch: 040 ----
mean loss: 279.39
 ---- batch: 050 ----
mean loss: 264.76
 ---- batch: 060 ----
mean loss: 281.17
 ---- batch: 070 ----
mean loss: 287.62
 ---- batch: 080 ----
mean loss: 289.92
 ---- batch: 090 ----
mean loss: 280.83
 ---- batch: 100 ----
mean loss: 289.27
 ---- batch: 110 ----
mean loss: 279.55
train mean loss: 281.96
epoch train time: 0:00:00.731056
elapsed time: 0:01:25.934110
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:42:17.504130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.22
 ---- batch: 020 ----
mean loss: 276.86
 ---- batch: 030 ----
mean loss: 283.40
 ---- batch: 040 ----
mean loss: 283.70
 ---- batch: 050 ----
mean loss: 283.62
 ---- batch: 060 ----
mean loss: 279.65
 ---- batch: 070 ----
mean loss: 277.91
 ---- batch: 080 ----
mean loss: 281.29
 ---- batch: 090 ----
mean loss: 280.11
 ---- batch: 100 ----
mean loss: 280.91
 ---- batch: 110 ----
mean loss: 277.50
train mean loss: 279.40
epoch train time: 0:00:00.734951
elapsed time: 0:01:26.669207
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:42:18.239225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.86
 ---- batch: 020 ----
mean loss: 283.18
 ---- batch: 030 ----
mean loss: 273.27
 ---- batch: 040 ----
mean loss: 273.95
 ---- batch: 050 ----
mean loss: 269.74
 ---- batch: 060 ----
mean loss: 283.23
 ---- batch: 070 ----
mean loss: 276.52
 ---- batch: 080 ----
mean loss: 280.08
 ---- batch: 090 ----
mean loss: 275.55
 ---- batch: 100 ----
mean loss: 275.22
 ---- batch: 110 ----
mean loss: 276.03
train mean loss: 277.24
epoch train time: 0:00:00.729767
elapsed time: 0:01:27.399117
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:42:18.969134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.35
 ---- batch: 020 ----
mean loss: 271.90
 ---- batch: 030 ----
mean loss: 267.30
 ---- batch: 040 ----
mean loss: 273.47
 ---- batch: 050 ----
mean loss: 268.59
 ---- batch: 060 ----
mean loss: 287.14
 ---- batch: 070 ----
mean loss: 280.19
 ---- batch: 080 ----
mean loss: 276.72
 ---- batch: 090 ----
mean loss: 270.50
 ---- batch: 100 ----
mean loss: 278.13
 ---- batch: 110 ----
mean loss: 271.35
train mean loss: 275.32
epoch train time: 0:00:00.733085
elapsed time: 0:01:28.132355
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:42:19.702383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.67
 ---- batch: 020 ----
mean loss: 274.63
 ---- batch: 030 ----
mean loss: 283.87
 ---- batch: 040 ----
mean loss: 277.98
 ---- batch: 050 ----
mean loss: 282.97
 ---- batch: 060 ----
mean loss: 269.71
 ---- batch: 070 ----
mean loss: 265.85
 ---- batch: 080 ----
mean loss: 267.50
 ---- batch: 090 ----
mean loss: 270.67
 ---- batch: 100 ----
mean loss: 268.23
 ---- batch: 110 ----
mean loss: 269.11
train mean loss: 273.13
epoch train time: 0:00:00.746963
elapsed time: 0:01:28.879493
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:42:20.449517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.81
 ---- batch: 020 ----
mean loss: 267.96
 ---- batch: 030 ----
mean loss: 273.94
 ---- batch: 040 ----
mean loss: 274.26
 ---- batch: 050 ----
mean loss: 268.45
 ---- batch: 060 ----
mean loss: 275.22
 ---- batch: 070 ----
mean loss: 271.88
 ---- batch: 080 ----
mean loss: 271.88
 ---- batch: 090 ----
mean loss: 271.09
 ---- batch: 100 ----
mean loss: 267.69
 ---- batch: 110 ----
mean loss: 271.63
train mean loss: 271.19
epoch train time: 0:00:00.736418
elapsed time: 0:01:29.616058
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:42:21.186114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.72
 ---- batch: 020 ----
mean loss: 277.33
 ---- batch: 030 ----
mean loss: 262.72
 ---- batch: 040 ----
mean loss: 270.53
 ---- batch: 050 ----
mean loss: 275.20
 ---- batch: 060 ----
mean loss: 272.10
 ---- batch: 070 ----
mean loss: 263.96
 ---- batch: 080 ----
mean loss: 265.54
 ---- batch: 090 ----
mean loss: 276.78
 ---- batch: 100 ----
mean loss: 266.05
 ---- batch: 110 ----
mean loss: 264.82
train mean loss: 269.46
epoch train time: 0:00:00.731380
elapsed time: 0:01:30.347615
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:42:21.917656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.45
 ---- batch: 020 ----
mean loss: 266.31
 ---- batch: 030 ----
mean loss: 265.67
 ---- batch: 040 ----
mean loss: 266.40
 ---- batch: 050 ----
mean loss: 270.01
 ---- batch: 060 ----
mean loss: 275.49
 ---- batch: 070 ----
mean loss: 269.07
 ---- batch: 080 ----
mean loss: 277.43
 ---- batch: 090 ----
mean loss: 262.69
 ---- batch: 100 ----
mean loss: 263.64
 ---- batch: 110 ----
mean loss: 264.06
train mean loss: 267.70
epoch train time: 0:00:00.731860
elapsed time: 0:01:31.079664
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:42:22.649690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.41
 ---- batch: 020 ----
mean loss: 265.30
 ---- batch: 030 ----
mean loss: 264.25
 ---- batch: 040 ----
mean loss: 272.02
 ---- batch: 050 ----
mean loss: 269.38
 ---- batch: 060 ----
mean loss: 262.72
 ---- batch: 070 ----
mean loss: 265.48
 ---- batch: 080 ----
mean loss: 264.26
 ---- batch: 090 ----
mean loss: 278.31
 ---- batch: 100 ----
mean loss: 253.18
 ---- batch: 110 ----
mean loss: 271.95
train mean loss: 266.11
epoch train time: 0:00:00.738612
elapsed time: 0:01:31.818428
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:42:23.388448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.26
 ---- batch: 020 ----
mean loss: 270.11
 ---- batch: 030 ----
mean loss: 260.33
 ---- batch: 040 ----
mean loss: 261.84
 ---- batch: 050 ----
mean loss: 268.80
 ---- batch: 060 ----
mean loss: 261.84
 ---- batch: 070 ----
mean loss: 271.41
 ---- batch: 080 ----
mean loss: 261.64
 ---- batch: 090 ----
mean loss: 262.80
 ---- batch: 100 ----
mean loss: 270.10
 ---- batch: 110 ----
mean loss: 258.65
train mean loss: 264.50
epoch train time: 0:00:00.744681
elapsed time: 0:01:32.563267
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:42:24.133286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.09
 ---- batch: 020 ----
mean loss: 267.23
 ---- batch: 030 ----
mean loss: 257.20
 ---- batch: 040 ----
mean loss: 259.97
 ---- batch: 050 ----
mean loss: 262.68
 ---- batch: 060 ----
mean loss: 260.19
 ---- batch: 070 ----
mean loss: 259.72
 ---- batch: 080 ----
mean loss: 273.29
 ---- batch: 090 ----
mean loss: 267.64
 ---- batch: 100 ----
mean loss: 254.89
 ---- batch: 110 ----
mean loss: 272.28
train mean loss: 262.83
epoch train time: 0:00:00.731482
elapsed time: 0:01:33.294906
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:42:24.864924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.38
 ---- batch: 020 ----
mean loss: 267.93
 ---- batch: 030 ----
mean loss: 266.33
 ---- batch: 040 ----
mean loss: 268.01
 ---- batch: 050 ----
mean loss: 259.05
 ---- batch: 060 ----
mean loss: 261.11
 ---- batch: 070 ----
mean loss: 261.93
 ---- batch: 080 ----
mean loss: 257.92
 ---- batch: 090 ----
mean loss: 257.80
 ---- batch: 100 ----
mean loss: 261.33
 ---- batch: 110 ----
mean loss: 262.37
train mean loss: 261.49
epoch train time: 0:00:00.727631
elapsed time: 0:01:34.022676
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:42:25.592713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.30
 ---- batch: 020 ----
mean loss: 266.72
 ---- batch: 030 ----
mean loss: 256.74
 ---- batch: 040 ----
mean loss: 257.21
 ---- batch: 050 ----
mean loss: 260.83
 ---- batch: 060 ----
mean loss: 258.57
 ---- batch: 070 ----
mean loss: 247.85
 ---- batch: 080 ----
mean loss: 257.94
 ---- batch: 090 ----
mean loss: 261.60
 ---- batch: 100 ----
mean loss: 266.29
 ---- batch: 110 ----
mean loss: 258.04
train mean loss: 259.51
epoch train time: 0:00:00.725946
elapsed time: 0:01:34.748783
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:42:26.318832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.75
 ---- batch: 020 ----
mean loss: 259.91
 ---- batch: 030 ----
mean loss: 257.27
 ---- batch: 040 ----
mean loss: 254.43
 ---- batch: 050 ----
mean loss: 252.17
 ---- batch: 060 ----
mean loss: 259.52
 ---- batch: 070 ----
mean loss: 265.32
 ---- batch: 080 ----
mean loss: 264.65
 ---- batch: 090 ----
mean loss: 258.04
 ---- batch: 100 ----
mean loss: 259.15
 ---- batch: 110 ----
mean loss: 251.42
train mean loss: 257.79
epoch train time: 0:00:00.735198
elapsed time: 0:01:35.484170
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:42:27.054203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.02
 ---- batch: 020 ----
mean loss: 263.00
 ---- batch: 030 ----
mean loss: 247.36
 ---- batch: 040 ----
mean loss: 262.82
 ---- batch: 050 ----
mean loss: 258.12
 ---- batch: 060 ----
mean loss: 254.50
 ---- batch: 070 ----
mean loss: 259.42
 ---- batch: 080 ----
mean loss: 260.73
 ---- batch: 090 ----
mean loss: 256.79
 ---- batch: 100 ----
mean loss: 250.61
 ---- batch: 110 ----
mean loss: 253.18
train mean loss: 255.84
epoch train time: 0:00:00.724255
elapsed time: 0:01:36.208581
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:42:27.778600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.32
 ---- batch: 020 ----
mean loss: 266.00
 ---- batch: 030 ----
mean loss: 250.95
 ---- batch: 040 ----
mean loss: 252.51
 ---- batch: 050 ----
mean loss: 258.63
 ---- batch: 060 ----
mean loss: 249.32
 ---- batch: 070 ----
mean loss: 245.12
 ---- batch: 080 ----
mean loss: 247.98
 ---- batch: 090 ----
mean loss: 257.98
 ---- batch: 100 ----
mean loss: 253.73
 ---- batch: 110 ----
mean loss: 255.61
train mean loss: 253.87
epoch train time: 0:00:00.727543
elapsed time: 0:01:36.936268
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:42:28.506285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.84
 ---- batch: 020 ----
mean loss: 255.99
 ---- batch: 030 ----
mean loss: 251.46
 ---- batch: 040 ----
mean loss: 242.93
 ---- batch: 050 ----
mean loss: 254.57
 ---- batch: 060 ----
mean loss: 251.25
 ---- batch: 070 ----
mean loss: 249.36
 ---- batch: 080 ----
mean loss: 258.62
 ---- batch: 090 ----
mean loss: 250.98
 ---- batch: 100 ----
mean loss: 242.49
 ---- batch: 110 ----
mean loss: 251.21
train mean loss: 251.48
epoch train time: 0:00:00.723733
elapsed time: 0:01:37.660149
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:42:29.230169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.31
 ---- batch: 020 ----
mean loss: 248.02
 ---- batch: 030 ----
mean loss: 251.90
 ---- batch: 040 ----
mean loss: 251.88
 ---- batch: 050 ----
mean loss: 255.32
 ---- batch: 060 ----
mean loss: 240.90
 ---- batch: 070 ----
mean loss: 247.04
 ---- batch: 080 ----
mean loss: 252.29
 ---- batch: 090 ----
mean loss: 255.65
 ---- batch: 100 ----
mean loss: 254.17
 ---- batch: 110 ----
mean loss: 247.56
train mean loss: 249.00
epoch train time: 0:00:00.732668
elapsed time: 0:01:38.392965
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:42:29.962983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.62
 ---- batch: 020 ----
mean loss: 245.39
 ---- batch: 030 ----
mean loss: 249.20
 ---- batch: 040 ----
mean loss: 253.85
 ---- batch: 050 ----
mean loss: 242.75
 ---- batch: 060 ----
mean loss: 245.41
 ---- batch: 070 ----
mean loss: 248.10
 ---- batch: 080 ----
mean loss: 243.55
 ---- batch: 090 ----
mean loss: 247.49
 ---- batch: 100 ----
mean loss: 244.69
 ---- batch: 110 ----
mean loss: 241.81
train mean loss: 246.81
epoch train time: 0:00:00.719454
elapsed time: 0:01:39.112566
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:42:30.682608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.02
 ---- batch: 020 ----
mean loss: 243.80
 ---- batch: 030 ----
mean loss: 234.96
 ---- batch: 040 ----
mean loss: 245.75
 ---- batch: 050 ----
mean loss: 242.07
 ---- batch: 060 ----
mean loss: 251.86
 ---- batch: 070 ----
mean loss: 250.77
 ---- batch: 080 ----
mean loss: 249.75
 ---- batch: 090 ----
mean loss: 241.48
 ---- batch: 100 ----
mean loss: 243.17
 ---- batch: 110 ----
mean loss: 245.88
train mean loss: 244.49
epoch train time: 0:00:00.725195
elapsed time: 0:01:39.837926
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:42:31.407952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.73
 ---- batch: 020 ----
mean loss: 253.08
 ---- batch: 030 ----
mean loss: 249.38
 ---- batch: 040 ----
mean loss: 237.22
 ---- batch: 050 ----
mean loss: 241.20
 ---- batch: 060 ----
mean loss: 242.06
 ---- batch: 070 ----
mean loss: 241.86
 ---- batch: 080 ----
mean loss: 236.72
 ---- batch: 090 ----
mean loss: 240.41
 ---- batch: 100 ----
mean loss: 232.60
 ---- batch: 110 ----
mean loss: 247.50
train mean loss: 242.33
epoch train time: 0:00:00.725765
elapsed time: 0:01:40.563855
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:42:32.133912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.77
 ---- batch: 020 ----
mean loss: 245.58
 ---- batch: 030 ----
mean loss: 242.02
 ---- batch: 040 ----
mean loss: 245.40
 ---- batch: 050 ----
mean loss: 233.50
 ---- batch: 060 ----
mean loss: 237.39
 ---- batch: 070 ----
mean loss: 245.30
 ---- batch: 080 ----
mean loss: 238.11
 ---- batch: 090 ----
mean loss: 234.40
 ---- batch: 100 ----
mean loss: 233.41
 ---- batch: 110 ----
mean loss: 242.32
train mean loss: 240.29
epoch train time: 0:00:00.738389
elapsed time: 0:01:41.302459
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:42:32.872495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.83
 ---- batch: 020 ----
mean loss: 246.54
 ---- batch: 030 ----
mean loss: 241.07
 ---- batch: 040 ----
mean loss: 244.10
 ---- batch: 050 ----
mean loss: 235.12
 ---- batch: 060 ----
mean loss: 229.85
 ---- batch: 070 ----
mean loss: 237.33
 ---- batch: 080 ----
mean loss: 233.19
 ---- batch: 090 ----
mean loss: 245.21
 ---- batch: 100 ----
mean loss: 236.10
 ---- batch: 110 ----
mean loss: 233.16
train mean loss: 238.16
epoch train time: 0:00:00.731524
elapsed time: 0:01:42.034178
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:42:33.604197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.00
 ---- batch: 020 ----
mean loss: 247.41
 ---- batch: 030 ----
mean loss: 232.31
 ---- batch: 040 ----
mean loss: 234.64
 ---- batch: 050 ----
mean loss: 230.96
 ---- batch: 060 ----
mean loss: 235.35
 ---- batch: 070 ----
mean loss: 241.15
 ---- batch: 080 ----
mean loss: 243.23
 ---- batch: 090 ----
mean loss: 237.28
 ---- batch: 100 ----
mean loss: 233.75
 ---- batch: 110 ----
mean loss: 231.21
train mean loss: 236.35
epoch train time: 0:00:00.738863
elapsed time: 0:01:42.773199
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:42:34.343217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.59
 ---- batch: 020 ----
mean loss: 238.20
 ---- batch: 030 ----
mean loss: 237.94
 ---- batch: 040 ----
mean loss: 231.40
 ---- batch: 050 ----
mean loss: 238.53
 ---- batch: 060 ----
mean loss: 233.96
 ---- batch: 070 ----
mean loss: 240.44
 ---- batch: 080 ----
mean loss: 233.47
 ---- batch: 090 ----
mean loss: 236.45
 ---- batch: 100 ----
mean loss: 230.87
 ---- batch: 110 ----
mean loss: 228.42
train mean loss: 234.47
epoch train time: 0:00:00.733662
elapsed time: 0:01:43.507008
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:42:35.077026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.74
 ---- batch: 020 ----
mean loss: 236.93
 ---- batch: 030 ----
mean loss: 235.06
 ---- batch: 040 ----
mean loss: 237.31
 ---- batch: 050 ----
mean loss: 238.08
 ---- batch: 060 ----
mean loss: 228.60
 ---- batch: 070 ----
mean loss: 224.87
 ---- batch: 080 ----
mean loss: 227.54
 ---- batch: 090 ----
mean loss: 232.90
 ---- batch: 100 ----
mean loss: 231.62
 ---- batch: 110 ----
mean loss: 229.58
train mean loss: 232.87
epoch train time: 0:00:00.726070
elapsed time: 0:01:44.233224
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:42:35.803266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.10
 ---- batch: 020 ----
mean loss: 224.62
 ---- batch: 030 ----
mean loss: 211.15
 ---- batch: 040 ----
mean loss: 236.66
 ---- batch: 050 ----
mean loss: 244.63
 ---- batch: 060 ----
mean loss: 237.56
 ---- batch: 070 ----
mean loss: 231.19
 ---- batch: 080 ----
mean loss: 229.93
 ---- batch: 090 ----
mean loss: 230.09
 ---- batch: 100 ----
mean loss: 229.09
 ---- batch: 110 ----
mean loss: 238.68
train mean loss: 231.13
epoch train time: 0:00:00.730821
elapsed time: 0:01:44.964208
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:42:36.534223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.43
 ---- batch: 020 ----
mean loss: 221.40
 ---- batch: 030 ----
mean loss: 228.41
 ---- batch: 040 ----
mean loss: 230.10
 ---- batch: 050 ----
mean loss: 242.05
 ---- batch: 060 ----
mean loss: 225.53
 ---- batch: 070 ----
mean loss: 227.65
 ---- batch: 080 ----
mean loss: 237.22
 ---- batch: 090 ----
mean loss: 231.50
 ---- batch: 100 ----
mean loss: 232.77
 ---- batch: 110 ----
mean loss: 229.83
train mean loss: 229.69
epoch train time: 0:00:00.731113
elapsed time: 0:01:45.695452
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:42:37.265477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.70
 ---- batch: 020 ----
mean loss: 230.64
 ---- batch: 030 ----
mean loss: 229.13
 ---- batch: 040 ----
mean loss: 221.39
 ---- batch: 050 ----
mean loss: 227.54
 ---- batch: 060 ----
mean loss: 233.54
 ---- batch: 070 ----
mean loss: 228.46
 ---- batch: 080 ----
mean loss: 238.87
 ---- batch: 090 ----
mean loss: 229.29
 ---- batch: 100 ----
mean loss: 227.08
 ---- batch: 110 ----
mean loss: 223.50
train mean loss: 228.19
epoch train time: 0:00:00.737955
elapsed time: 0:01:46.433553
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:42:38.003567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.11
 ---- batch: 020 ----
mean loss: 234.11
 ---- batch: 030 ----
mean loss: 220.05
 ---- batch: 040 ----
mean loss: 233.45
 ---- batch: 050 ----
mean loss: 219.21
 ---- batch: 060 ----
mean loss: 229.57
 ---- batch: 070 ----
mean loss: 218.82
 ---- batch: 080 ----
mean loss: 217.22
 ---- batch: 090 ----
mean loss: 224.86
 ---- batch: 100 ----
mean loss: 228.26
 ---- batch: 110 ----
mean loss: 235.45
train mean loss: 226.85
epoch train time: 0:00:00.739756
elapsed time: 0:01:47.173454
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:42:38.743478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.71
 ---- batch: 020 ----
mean loss: 226.02
 ---- batch: 030 ----
mean loss: 236.53
 ---- batch: 040 ----
mean loss: 224.94
 ---- batch: 050 ----
mean loss: 226.00
 ---- batch: 060 ----
mean loss: 226.25
 ---- batch: 070 ----
mean loss: 225.36
 ---- batch: 080 ----
mean loss: 222.33
 ---- batch: 090 ----
mean loss: 220.81
 ---- batch: 100 ----
mean loss: 231.10
 ---- batch: 110 ----
mean loss: 225.56
train mean loss: 225.59
epoch train time: 0:00:00.749789
elapsed time: 0:01:47.923391
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:42:39.493408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.81
 ---- batch: 020 ----
mean loss: 226.31
 ---- batch: 030 ----
mean loss: 227.94
 ---- batch: 040 ----
mean loss: 221.60
 ---- batch: 050 ----
mean loss: 226.30
 ---- batch: 060 ----
mean loss: 218.94
 ---- batch: 070 ----
mean loss: 224.59
 ---- batch: 080 ----
mean loss: 227.72
 ---- batch: 090 ----
mean loss: 225.67
 ---- batch: 100 ----
mean loss: 216.46
 ---- batch: 110 ----
mean loss: 224.63
train mean loss: 224.43
epoch train time: 0:00:00.742566
elapsed time: 0:01:48.666101
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:42:40.236168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.44
 ---- batch: 020 ----
mean loss: 222.99
 ---- batch: 030 ----
mean loss: 229.99
 ---- batch: 040 ----
mean loss: 225.40
 ---- batch: 050 ----
mean loss: 222.39
 ---- batch: 060 ----
mean loss: 226.59
 ---- batch: 070 ----
mean loss: 216.22
 ---- batch: 080 ----
mean loss: 227.85
 ---- batch: 090 ----
mean loss: 218.34
 ---- batch: 100 ----
mean loss: 219.58
 ---- batch: 110 ----
mean loss: 219.42
train mean loss: 223.32
epoch train time: 0:00:00.743390
elapsed time: 0:01:49.409761
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:42:40.979807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.58
 ---- batch: 020 ----
mean loss: 219.77
 ---- batch: 030 ----
mean loss: 225.35
 ---- batch: 040 ----
mean loss: 219.85
 ---- batch: 050 ----
mean loss: 213.00
 ---- batch: 060 ----
mean loss: 224.68
 ---- batch: 070 ----
mean loss: 217.88
 ---- batch: 080 ----
mean loss: 220.66
 ---- batch: 090 ----
mean loss: 219.01
 ---- batch: 100 ----
mean loss: 224.59
 ---- batch: 110 ----
mean loss: 232.18
train mean loss: 222.37
epoch train time: 0:00:00.736513
elapsed time: 0:01:50.146453
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:42:41.716473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.49
 ---- batch: 020 ----
mean loss: 222.55
 ---- batch: 030 ----
mean loss: 219.86
 ---- batch: 040 ----
mean loss: 220.24
 ---- batch: 050 ----
mean loss: 219.92
 ---- batch: 060 ----
mean loss: 214.47
 ---- batch: 070 ----
mean loss: 226.07
 ---- batch: 080 ----
mean loss: 215.70
 ---- batch: 090 ----
mean loss: 221.47
 ---- batch: 100 ----
mean loss: 225.65
 ---- batch: 110 ----
mean loss: 232.59
train mean loss: 221.30
epoch train time: 0:00:00.744678
elapsed time: 0:01:50.891288
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:42:42.461306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.50
 ---- batch: 020 ----
mean loss: 219.47
 ---- batch: 030 ----
mean loss: 221.15
 ---- batch: 040 ----
mean loss: 214.29
 ---- batch: 050 ----
mean loss: 212.43
 ---- batch: 060 ----
mean loss: 226.44
 ---- batch: 070 ----
mean loss: 220.21
 ---- batch: 080 ----
mean loss: 217.66
 ---- batch: 090 ----
mean loss: 218.80
 ---- batch: 100 ----
mean loss: 230.19
 ---- batch: 110 ----
mean loss: 216.57
train mean loss: 220.40
epoch train time: 0:00:00.727785
elapsed time: 0:01:51.619225
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:42:43.189232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.76
 ---- batch: 020 ----
mean loss: 229.97
 ---- batch: 030 ----
mean loss: 213.19
 ---- batch: 040 ----
mean loss: 220.50
 ---- batch: 050 ----
mean loss: 222.07
 ---- batch: 060 ----
mean loss: 216.61
 ---- batch: 070 ----
mean loss: 215.37
 ---- batch: 080 ----
mean loss: 225.26
 ---- batch: 090 ----
mean loss: 224.80
 ---- batch: 100 ----
mean loss: 209.76
 ---- batch: 110 ----
mean loss: 224.42
train mean loss: 219.69
epoch train time: 0:00:00.734912
elapsed time: 0:01:52.354268
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:42:43.924285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.99
 ---- batch: 020 ----
mean loss: 220.18
 ---- batch: 030 ----
mean loss: 223.59
 ---- batch: 040 ----
mean loss: 215.82
 ---- batch: 050 ----
mean loss: 227.61
 ---- batch: 060 ----
mean loss: 213.59
 ---- batch: 070 ----
mean loss: 228.26
 ---- batch: 080 ----
mean loss: 219.56
 ---- batch: 090 ----
mean loss: 221.12
 ---- batch: 100 ----
mean loss: 204.93
 ---- batch: 110 ----
mean loss: 211.02
train mean loss: 218.74
epoch train time: 0:00:00.735227
elapsed time: 0:01:53.089632
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:42:44.659648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.60
 ---- batch: 020 ----
mean loss: 222.03
 ---- batch: 030 ----
mean loss: 209.08
 ---- batch: 040 ----
mean loss: 213.11
 ---- batch: 050 ----
mean loss: 221.42
 ---- batch: 060 ----
mean loss: 215.17
 ---- batch: 070 ----
mean loss: 219.00
 ---- batch: 080 ----
mean loss: 218.12
 ---- batch: 090 ----
mean loss: 221.08
 ---- batch: 100 ----
mean loss: 225.34
 ---- batch: 110 ----
mean loss: 219.27
train mean loss: 218.05
epoch train time: 0:00:00.742457
elapsed time: 0:01:53.832244
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:42:45.402277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.58
 ---- batch: 020 ----
mean loss: 209.28
 ---- batch: 030 ----
mean loss: 223.93
 ---- batch: 040 ----
mean loss: 211.06
 ---- batch: 050 ----
mean loss: 219.79
 ---- batch: 060 ----
mean loss: 214.23
 ---- batch: 070 ----
mean loss: 223.57
 ---- batch: 080 ----
mean loss: 218.76
 ---- batch: 090 ----
mean loss: 211.52
 ---- batch: 100 ----
mean loss: 219.61
 ---- batch: 110 ----
mean loss: 222.33
train mean loss: 217.39
epoch train time: 0:00:00.732864
elapsed time: 0:01:54.565298
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:42:46.135318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.50
 ---- batch: 020 ----
mean loss: 217.52
 ---- batch: 030 ----
mean loss: 211.88
 ---- batch: 040 ----
mean loss: 208.45
 ---- batch: 050 ----
mean loss: 224.16
 ---- batch: 060 ----
mean loss: 214.38
 ---- batch: 070 ----
mean loss: 217.01
 ---- batch: 080 ----
mean loss: 224.45
 ---- batch: 090 ----
mean loss: 214.67
 ---- batch: 100 ----
mean loss: 210.14
 ---- batch: 110 ----
mean loss: 221.02
train mean loss: 216.82
epoch train time: 0:00:00.741882
elapsed time: 0:01:55.307380
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:42:46.877413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.40
 ---- batch: 020 ----
mean loss: 213.72
 ---- batch: 030 ----
mean loss: 218.39
 ---- batch: 040 ----
mean loss: 217.35
 ---- batch: 050 ----
mean loss: 206.95
 ---- batch: 060 ----
mean loss: 218.72
 ---- batch: 070 ----
mean loss: 217.03
 ---- batch: 080 ----
mean loss: 220.50
 ---- batch: 090 ----
mean loss: 224.75
 ---- batch: 100 ----
mean loss: 206.29
 ---- batch: 110 ----
mean loss: 217.59
train mean loss: 216.17
epoch train time: 0:00:00.731141
elapsed time: 0:01:56.038674
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:42:47.608691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.54
 ---- batch: 020 ----
mean loss: 202.62
 ---- batch: 030 ----
mean loss: 217.24
 ---- batch: 040 ----
mean loss: 203.53
 ---- batch: 050 ----
mean loss: 215.33
 ---- batch: 060 ----
mean loss: 221.38
 ---- batch: 070 ----
mean loss: 215.80
 ---- batch: 080 ----
mean loss: 221.24
 ---- batch: 090 ----
mean loss: 217.57
 ---- batch: 100 ----
mean loss: 218.07
 ---- batch: 110 ----
mean loss: 222.20
train mean loss: 215.51
epoch train time: 0:00:00.726692
elapsed time: 0:01:56.765519
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:42:48.335535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.81
 ---- batch: 020 ----
mean loss: 223.93
 ---- batch: 030 ----
mean loss: 211.93
 ---- batch: 040 ----
mean loss: 215.62
 ---- batch: 050 ----
mean loss: 222.43
 ---- batch: 060 ----
mean loss: 230.46
 ---- batch: 070 ----
mean loss: 217.95
 ---- batch: 080 ----
mean loss: 204.18
 ---- batch: 090 ----
mean loss: 212.07
 ---- batch: 100 ----
mean loss: 213.70
 ---- batch: 110 ----
mean loss: 210.08
train mean loss: 215.44
epoch train time: 0:00:00.729252
elapsed time: 0:01:57.494915
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:42:49.064967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.96
 ---- batch: 020 ----
mean loss: 223.19
 ---- batch: 030 ----
mean loss: 215.69
 ---- batch: 040 ----
mean loss: 204.17
 ---- batch: 050 ----
mean loss: 213.47
 ---- batch: 060 ----
mean loss: 214.56
 ---- batch: 070 ----
mean loss: 213.35
 ---- batch: 080 ----
mean loss: 220.88
 ---- batch: 090 ----
mean loss: 219.42
 ---- batch: 100 ----
mean loss: 207.11
 ---- batch: 110 ----
mean loss: 207.01
train mean loss: 214.61
epoch train time: 0:00:00.749291
elapsed time: 0:01:58.244379
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:42:49.814428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.55
 ---- batch: 020 ----
mean loss: 214.43
 ---- batch: 030 ----
mean loss: 208.52
 ---- batch: 040 ----
mean loss: 211.59
 ---- batch: 050 ----
mean loss: 212.13
 ---- batch: 060 ----
mean loss: 214.18
 ---- batch: 070 ----
mean loss: 222.01
 ---- batch: 080 ----
mean loss: 217.64
 ---- batch: 090 ----
mean loss: 215.13
 ---- batch: 100 ----
mean loss: 217.63
 ---- batch: 110 ----
mean loss: 212.44
train mean loss: 214.10
epoch train time: 0:00:00.724851
elapsed time: 0:01:58.969403
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:42:50.539419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.83
 ---- batch: 020 ----
mean loss: 217.78
 ---- batch: 030 ----
mean loss: 212.20
 ---- batch: 040 ----
mean loss: 212.51
 ---- batch: 050 ----
mean loss: 208.83
 ---- batch: 060 ----
mean loss: 212.88
 ---- batch: 070 ----
mean loss: 212.93
 ---- batch: 080 ----
mean loss: 224.02
 ---- batch: 090 ----
mean loss: 212.00
 ---- batch: 100 ----
mean loss: 212.88
 ---- batch: 110 ----
mean loss: 207.64
train mean loss: 213.62
epoch train time: 0:00:00.732694
elapsed time: 0:01:59.702237
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:42:51.272254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.54
 ---- batch: 020 ----
mean loss: 212.05
 ---- batch: 030 ----
mean loss: 217.12
 ---- batch: 040 ----
mean loss: 208.69
 ---- batch: 050 ----
mean loss: 212.96
 ---- batch: 060 ----
mean loss: 208.64
 ---- batch: 070 ----
mean loss: 210.59
 ---- batch: 080 ----
mean loss: 218.23
 ---- batch: 090 ----
mean loss: 220.18
 ---- batch: 100 ----
mean loss: 214.70
 ---- batch: 110 ----
mean loss: 211.99
train mean loss: 213.00
epoch train time: 0:00:00.732108
elapsed time: 0:02:00.434492
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:42:52.004510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.78
 ---- batch: 020 ----
mean loss: 206.26
 ---- batch: 030 ----
mean loss: 210.83
 ---- batch: 040 ----
mean loss: 217.43
 ---- batch: 050 ----
mean loss: 215.31
 ---- batch: 060 ----
mean loss: 221.93
 ---- batch: 070 ----
mean loss: 211.03
 ---- batch: 080 ----
mean loss: 210.25
 ---- batch: 090 ----
mean loss: 222.29
 ---- batch: 100 ----
mean loss: 208.46
 ---- batch: 110 ----
mean loss: 212.81
train mean loss: 212.84
epoch train time: 0:00:00.731238
elapsed time: 0:02:01.165872
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:42:52.735890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.87
 ---- batch: 020 ----
mean loss: 200.66
 ---- batch: 030 ----
mean loss: 218.67
 ---- batch: 040 ----
mean loss: 212.64
 ---- batch: 050 ----
mean loss: 207.82
 ---- batch: 060 ----
mean loss: 211.58
 ---- batch: 070 ----
mean loss: 216.15
 ---- batch: 080 ----
mean loss: 200.80
 ---- batch: 090 ----
mean loss: 221.65
 ---- batch: 100 ----
mean loss: 212.23
 ---- batch: 110 ----
mean loss: 214.74
train mean loss: 212.41
epoch train time: 0:00:00.732674
elapsed time: 0:02:01.898686
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:42:53.468703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.98
 ---- batch: 020 ----
mean loss: 211.63
 ---- batch: 030 ----
mean loss: 210.86
 ---- batch: 040 ----
mean loss: 214.19
 ---- batch: 050 ----
mean loss: 204.02
 ---- batch: 060 ----
mean loss: 212.11
 ---- batch: 070 ----
mean loss: 211.39
 ---- batch: 080 ----
mean loss: 212.31
 ---- batch: 090 ----
mean loss: 211.81
 ---- batch: 100 ----
mean loss: 216.73
 ---- batch: 110 ----
mean loss: 212.86
train mean loss: 211.96
epoch train time: 0:00:00.737995
elapsed time: 0:02:02.636828
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:42:54.206847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.00
 ---- batch: 020 ----
mean loss: 218.03
 ---- batch: 030 ----
mean loss: 207.25
 ---- batch: 040 ----
mean loss: 214.87
 ---- batch: 050 ----
mean loss: 217.75
 ---- batch: 060 ----
mean loss: 211.81
 ---- batch: 070 ----
mean loss: 215.16
 ---- batch: 080 ----
mean loss: 214.43
 ---- batch: 090 ----
mean loss: 198.94
 ---- batch: 100 ----
mean loss: 216.43
 ---- batch: 110 ----
mean loss: 202.57
train mean loss: 211.68
epoch train time: 0:00:00.745961
elapsed time: 0:02:03.382940
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:42:54.952977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.63
 ---- batch: 020 ----
mean loss: 201.19
 ---- batch: 030 ----
mean loss: 209.85
 ---- batch: 040 ----
mean loss: 213.21
 ---- batch: 050 ----
mean loss: 211.86
 ---- batch: 060 ----
mean loss: 216.48
 ---- batch: 070 ----
mean loss: 211.28
 ---- batch: 080 ----
mean loss: 213.94
 ---- batch: 090 ----
mean loss: 218.12
 ---- batch: 100 ----
mean loss: 210.00
 ---- batch: 110 ----
mean loss: 202.72
train mean loss: 211.25
epoch train time: 0:00:00.733226
elapsed time: 0:02:04.116337
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:42:55.686363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.74
 ---- batch: 020 ----
mean loss: 208.43
 ---- batch: 030 ----
mean loss: 211.20
 ---- batch: 040 ----
mean loss: 218.00
 ---- batch: 050 ----
mean loss: 219.32
 ---- batch: 060 ----
mean loss: 215.49
 ---- batch: 070 ----
mean loss: 212.96
 ---- batch: 080 ----
mean loss: 207.19
 ---- batch: 090 ----
mean loss: 210.67
 ---- batch: 100 ----
mean loss: 199.94
 ---- batch: 110 ----
mean loss: 206.49
train mean loss: 210.90
epoch train time: 0:00:00.745145
elapsed time: 0:02:04.861648
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:42:56.431666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.74
 ---- batch: 020 ----
mean loss: 205.82
 ---- batch: 030 ----
mean loss: 214.14
 ---- batch: 040 ----
mean loss: 217.63
 ---- batch: 050 ----
mean loss: 203.19
 ---- batch: 060 ----
mean loss: 209.62
 ---- batch: 070 ----
mean loss: 221.60
 ---- batch: 080 ----
mean loss: 214.84
 ---- batch: 090 ----
mean loss: 206.58
 ---- batch: 100 ----
mean loss: 201.38
 ---- batch: 110 ----
mean loss: 213.63
train mean loss: 210.63
epoch train time: 0:00:00.732768
elapsed time: 0:02:05.594588
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:42:57.164605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.65
 ---- batch: 020 ----
mean loss: 205.42
 ---- batch: 030 ----
mean loss: 205.17
 ---- batch: 040 ----
mean loss: 215.96
 ---- batch: 050 ----
mean loss: 210.42
 ---- batch: 060 ----
mean loss: 206.92
 ---- batch: 070 ----
mean loss: 211.81
 ---- batch: 080 ----
mean loss: 214.15
 ---- batch: 090 ----
mean loss: 210.98
 ---- batch: 100 ----
mean loss: 210.05
 ---- batch: 110 ----
mean loss: 209.70
train mean loss: 210.50
epoch train time: 0:00:00.735762
elapsed time: 0:02:06.330509
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:42:57.900522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.74
 ---- batch: 020 ----
mean loss: 209.74
 ---- batch: 030 ----
mean loss: 213.36
 ---- batch: 040 ----
mean loss: 196.08
 ---- batch: 050 ----
mean loss: 222.46
 ---- batch: 060 ----
mean loss: 209.26
 ---- batch: 070 ----
mean loss: 207.71
 ---- batch: 080 ----
mean loss: 209.22
 ---- batch: 090 ----
mean loss: 210.65
 ---- batch: 100 ----
mean loss: 207.82
 ---- batch: 110 ----
mean loss: 211.35
train mean loss: 210.05
epoch train time: 0:00:00.730974
elapsed time: 0:02:07.061619
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:42:58.631635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.34
 ---- batch: 020 ----
mean loss: 204.97
 ---- batch: 030 ----
mean loss: 208.46
 ---- batch: 040 ----
mean loss: 208.34
 ---- batch: 050 ----
mean loss: 219.83
 ---- batch: 060 ----
mean loss: 207.74
 ---- batch: 070 ----
mean loss: 210.87
 ---- batch: 080 ----
mean loss: 210.20
 ---- batch: 090 ----
mean loss: 201.20
 ---- batch: 100 ----
mean loss: 218.07
 ---- batch: 110 ----
mean loss: 202.32
train mean loss: 209.82
epoch train time: 0:00:00.726624
elapsed time: 0:02:07.788427
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:42:59.358462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.40
 ---- batch: 020 ----
mean loss: 217.73
 ---- batch: 030 ----
mean loss: 208.17
 ---- batch: 040 ----
mean loss: 209.37
 ---- batch: 050 ----
mean loss: 213.31
 ---- batch: 060 ----
mean loss: 212.68
 ---- batch: 070 ----
mean loss: 200.40
 ---- batch: 080 ----
mean loss: 210.83
 ---- batch: 090 ----
mean loss: 209.10
 ---- batch: 100 ----
mean loss: 209.07
 ---- batch: 110 ----
mean loss: 210.54
train mean loss: 209.56
epoch train time: 0:00:00.728542
elapsed time: 0:02:08.517159
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:43:00.087196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.72
 ---- batch: 020 ----
mean loss: 208.48
 ---- batch: 030 ----
mean loss: 209.71
 ---- batch: 040 ----
mean loss: 219.10
 ---- batch: 050 ----
mean loss: 202.73
 ---- batch: 060 ----
mean loss: 207.82
 ---- batch: 070 ----
mean loss: 215.30
 ---- batch: 080 ----
mean loss: 212.08
 ---- batch: 090 ----
mean loss: 210.84
 ---- batch: 100 ----
mean loss: 205.63
 ---- batch: 110 ----
mean loss: 204.61
train mean loss: 209.16
epoch train time: 0:00:00.735076
elapsed time: 0:02:09.252452
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:43:00.822494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.25
 ---- batch: 020 ----
mean loss: 211.53
 ---- batch: 030 ----
mean loss: 213.32
 ---- batch: 040 ----
mean loss: 209.02
 ---- batch: 050 ----
mean loss: 210.72
 ---- batch: 060 ----
mean loss: 209.06
 ---- batch: 070 ----
mean loss: 210.30
 ---- batch: 080 ----
mean loss: 207.12
 ---- batch: 090 ----
mean loss: 202.89
 ---- batch: 100 ----
mean loss: 212.25
 ---- batch: 110 ----
mean loss: 205.67
train mean loss: 209.26
epoch train time: 0:00:00.742970
elapsed time: 0:02:09.995586
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:43:01.565629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.64
 ---- batch: 020 ----
mean loss: 216.67
 ---- batch: 030 ----
mean loss: 212.86
 ---- batch: 040 ----
mean loss: 211.83
 ---- batch: 050 ----
mean loss: 204.96
 ---- batch: 060 ----
mean loss: 208.80
 ---- batch: 070 ----
mean loss: 215.56
 ---- batch: 080 ----
mean loss: 204.03
 ---- batch: 090 ----
mean loss: 199.15
 ---- batch: 100 ----
mean loss: 208.46
 ---- batch: 110 ----
mean loss: 204.87
train mean loss: 208.70
epoch train time: 0:00:00.736463
elapsed time: 0:02:10.732213
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:43:02.302230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.93
 ---- batch: 020 ----
mean loss: 206.96
 ---- batch: 030 ----
mean loss: 209.41
 ---- batch: 040 ----
mean loss: 202.57
 ---- batch: 050 ----
mean loss: 208.94
 ---- batch: 060 ----
mean loss: 201.43
 ---- batch: 070 ----
mean loss: 205.43
 ---- batch: 080 ----
mean loss: 203.83
 ---- batch: 090 ----
mean loss: 217.54
 ---- batch: 100 ----
mean loss: 198.73
 ---- batch: 110 ----
mean loss: 213.93
train mean loss: 208.49
epoch train time: 0:00:00.735201
elapsed time: 0:02:11.467553
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:43:03.037572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.87
 ---- batch: 020 ----
mean loss: 212.83
 ---- batch: 030 ----
mean loss: 217.19
 ---- batch: 040 ----
mean loss: 209.11
 ---- batch: 050 ----
mean loss: 206.72
 ---- batch: 060 ----
mean loss: 210.87
 ---- batch: 070 ----
mean loss: 207.81
 ---- batch: 080 ----
mean loss: 203.50
 ---- batch: 090 ----
mean loss: 204.31
 ---- batch: 100 ----
mean loss: 208.85
 ---- batch: 110 ----
mean loss: 202.87
train mean loss: 208.37
epoch train time: 0:00:00.724440
elapsed time: 0:02:12.192153
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:43:03.762169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.27
 ---- batch: 020 ----
mean loss: 214.29
 ---- batch: 030 ----
mean loss: 214.93
 ---- batch: 040 ----
mean loss: 208.94
 ---- batch: 050 ----
mean loss: 203.31
 ---- batch: 060 ----
mean loss: 206.01
 ---- batch: 070 ----
mean loss: 202.39
 ---- batch: 080 ----
mean loss: 203.96
 ---- batch: 090 ----
mean loss: 212.97
 ---- batch: 100 ----
mean loss: 203.76
 ---- batch: 110 ----
mean loss: 210.07
train mean loss: 208.16
epoch train time: 0:00:00.727671
elapsed time: 0:02:12.919967
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:43:04.490016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.27
 ---- batch: 020 ----
mean loss: 213.53
 ---- batch: 030 ----
mean loss: 207.78
 ---- batch: 040 ----
mean loss: 208.75
 ---- batch: 050 ----
mean loss: 200.73
 ---- batch: 060 ----
mean loss: 200.94
 ---- batch: 070 ----
mean loss: 212.88
 ---- batch: 080 ----
mean loss: 203.69
 ---- batch: 090 ----
mean loss: 212.11
 ---- batch: 100 ----
mean loss: 199.52
 ---- batch: 110 ----
mean loss: 203.46
train mean loss: 207.85
epoch train time: 0:00:00.740211
elapsed time: 0:02:13.660353
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:43:05.230369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.90
 ---- batch: 020 ----
mean loss: 210.83
 ---- batch: 030 ----
mean loss: 210.79
 ---- batch: 040 ----
mean loss: 209.30
 ---- batch: 050 ----
mean loss: 214.90
 ---- batch: 060 ----
mean loss: 211.30
 ---- batch: 070 ----
mean loss: 197.94
 ---- batch: 080 ----
mean loss: 208.16
 ---- batch: 090 ----
mean loss: 197.64
 ---- batch: 100 ----
mean loss: 214.94
 ---- batch: 110 ----
mean loss: 210.30
train mean loss: 207.54
epoch train time: 0:00:00.726100
elapsed time: 0:02:14.386603
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:43:05.956629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.25
 ---- batch: 020 ----
mean loss: 213.88
 ---- batch: 030 ----
mean loss: 202.33
 ---- batch: 040 ----
mean loss: 215.40
 ---- batch: 050 ----
mean loss: 196.65
 ---- batch: 060 ----
mean loss: 212.22
 ---- batch: 070 ----
mean loss: 198.71
 ---- batch: 080 ----
mean loss: 208.67
 ---- batch: 090 ----
mean loss: 200.91
 ---- batch: 100 ----
mean loss: 214.71
 ---- batch: 110 ----
mean loss: 203.11
train mean loss: 207.29
epoch train time: 0:00:00.735941
elapsed time: 0:02:15.122738
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:43:06.692756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.18
 ---- batch: 020 ----
mean loss: 218.60
 ---- batch: 030 ----
mean loss: 208.17
 ---- batch: 040 ----
mean loss: 212.41
 ---- batch: 050 ----
mean loss: 211.84
 ---- batch: 060 ----
mean loss: 209.93
 ---- batch: 070 ----
mean loss: 201.49
 ---- batch: 080 ----
mean loss: 199.67
 ---- batch: 090 ----
mean loss: 208.02
 ---- batch: 100 ----
mean loss: 197.25
 ---- batch: 110 ----
mean loss: 208.48
train mean loss: 207.20
epoch train time: 0:00:00.729381
elapsed time: 0:02:15.852283
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:43:07.422302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.78
 ---- batch: 020 ----
mean loss: 204.28
 ---- batch: 030 ----
mean loss: 216.97
 ---- batch: 040 ----
mean loss: 212.47
 ---- batch: 050 ----
mean loss: 212.14
 ---- batch: 060 ----
mean loss: 200.08
 ---- batch: 070 ----
mean loss: 210.94
 ---- batch: 080 ----
mean loss: 207.26
 ---- batch: 090 ----
mean loss: 215.70
 ---- batch: 100 ----
mean loss: 203.95
 ---- batch: 110 ----
mean loss: 197.00
train mean loss: 207.11
epoch train time: 0:00:00.727591
elapsed time: 0:02:16.580024
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:43:08.150056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.69
 ---- batch: 020 ----
mean loss: 216.79
 ---- batch: 030 ----
mean loss: 206.01
 ---- batch: 040 ----
mean loss: 200.49
 ---- batch: 050 ----
mean loss: 201.73
 ---- batch: 060 ----
mean loss: 208.55
 ---- batch: 070 ----
mean loss: 206.55
 ---- batch: 080 ----
mean loss: 211.39
 ---- batch: 090 ----
mean loss: 206.53
 ---- batch: 100 ----
mean loss: 203.71
 ---- batch: 110 ----
mean loss: 202.88
train mean loss: 206.71
epoch train time: 0:00:00.754125
elapsed time: 0:02:17.334302
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:43:08.904319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.21
 ---- batch: 020 ----
mean loss: 215.87
 ---- batch: 030 ----
mean loss: 203.62
 ---- batch: 040 ----
mean loss: 205.94
 ---- batch: 050 ----
mean loss: 202.10
 ---- batch: 060 ----
mean loss: 201.68
 ---- batch: 070 ----
mean loss: 205.15
 ---- batch: 080 ----
mean loss: 205.34
 ---- batch: 090 ----
mean loss: 195.64
 ---- batch: 100 ----
mean loss: 212.18
 ---- batch: 110 ----
mean loss: 216.85
train mean loss: 206.65
epoch train time: 0:00:00.733889
elapsed time: 0:02:18.068337
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:43:09.638381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.99
 ---- batch: 020 ----
mean loss: 199.94
 ---- batch: 030 ----
mean loss: 198.12
 ---- batch: 040 ----
mean loss: 205.58
 ---- batch: 050 ----
mean loss: 215.43
 ---- batch: 060 ----
mean loss: 198.34
 ---- batch: 070 ----
mean loss: 207.43
 ---- batch: 080 ----
mean loss: 215.77
 ---- batch: 090 ----
mean loss: 210.87
 ---- batch: 100 ----
mean loss: 212.46
 ---- batch: 110 ----
mean loss: 198.66
train mean loss: 206.32
epoch train time: 0:00:00.735532
elapsed time: 0:02:18.804036
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:43:10.374051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.95
 ---- batch: 020 ----
mean loss: 205.75
 ---- batch: 030 ----
mean loss: 207.78
 ---- batch: 040 ----
mean loss: 211.66
 ---- batch: 050 ----
mean loss: 204.62
 ---- batch: 060 ----
mean loss: 208.63
 ---- batch: 070 ----
mean loss: 197.38
 ---- batch: 080 ----
mean loss: 215.26
 ---- batch: 090 ----
mean loss: 203.14
 ---- batch: 100 ----
mean loss: 209.37
 ---- batch: 110 ----
mean loss: 202.47
train mean loss: 206.12
epoch train time: 0:00:00.735311
elapsed time: 0:02:19.539490
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:43:11.109508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.96
 ---- batch: 020 ----
mean loss: 211.03
 ---- batch: 030 ----
mean loss: 214.89
 ---- batch: 040 ----
mean loss: 202.99
 ---- batch: 050 ----
mean loss: 212.36
 ---- batch: 060 ----
mean loss: 209.47
 ---- batch: 070 ----
mean loss: 203.89
 ---- batch: 080 ----
mean loss: 206.21
 ---- batch: 090 ----
mean loss: 203.61
 ---- batch: 100 ----
mean loss: 204.06
 ---- batch: 110 ----
mean loss: 205.16
train mean loss: 205.84
epoch train time: 0:00:00.751973
elapsed time: 0:02:20.291601
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:43:11.861637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.41
 ---- batch: 020 ----
mean loss: 208.86
 ---- batch: 030 ----
mean loss: 201.48
 ---- batch: 040 ----
mean loss: 208.24
 ---- batch: 050 ----
mean loss: 200.13
 ---- batch: 060 ----
mean loss: 204.82
 ---- batch: 070 ----
mean loss: 210.89
 ---- batch: 080 ----
mean loss: 200.97
 ---- batch: 090 ----
mean loss: 206.58
 ---- batch: 100 ----
mean loss: 200.05
 ---- batch: 110 ----
mean loss: 210.30
train mean loss: 205.73
epoch train time: 0:00:00.727043
elapsed time: 0:02:21.018803
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:43:12.588820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.85
 ---- batch: 020 ----
mean loss: 208.28
 ---- batch: 030 ----
mean loss: 203.62
 ---- batch: 040 ----
mean loss: 205.41
 ---- batch: 050 ----
mean loss: 213.67
 ---- batch: 060 ----
mean loss: 203.05
 ---- batch: 070 ----
mean loss: 204.92
 ---- batch: 080 ----
mean loss: 201.68
 ---- batch: 090 ----
mean loss: 206.30
 ---- batch: 100 ----
mean loss: 215.00
 ---- batch: 110 ----
mean loss: 194.81
train mean loss: 205.36
epoch train time: 0:00:00.731280
elapsed time: 0:02:21.750219
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:43:13.320235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.72
 ---- batch: 020 ----
mean loss: 208.56
 ---- batch: 030 ----
mean loss: 212.25
 ---- batch: 040 ----
mean loss: 201.28
 ---- batch: 050 ----
mean loss: 206.26
 ---- batch: 060 ----
mean loss: 195.32
 ---- batch: 070 ----
mean loss: 201.55
 ---- batch: 080 ----
mean loss: 214.87
 ---- batch: 090 ----
mean loss: 211.28
 ---- batch: 100 ----
mean loss: 209.42
 ---- batch: 110 ----
mean loss: 196.16
train mean loss: 205.18
epoch train time: 0:00:00.730027
elapsed time: 0:02:22.480448
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:43:14.050489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.31
 ---- batch: 020 ----
mean loss: 201.44
 ---- batch: 030 ----
mean loss: 198.35
 ---- batch: 040 ----
mean loss: 191.85
 ---- batch: 050 ----
mean loss: 202.40
 ---- batch: 060 ----
mean loss: 205.20
 ---- batch: 070 ----
mean loss: 210.52
 ---- batch: 080 ----
mean loss: 209.96
 ---- batch: 090 ----
mean loss: 208.85
 ---- batch: 100 ----
mean loss: 209.11
 ---- batch: 110 ----
mean loss: 202.72
train mean loss: 204.92
epoch train time: 0:00:00.732014
elapsed time: 0:02:23.212636
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:43:14.782653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.73
 ---- batch: 020 ----
mean loss: 199.70
 ---- batch: 030 ----
mean loss: 210.46
 ---- batch: 040 ----
mean loss: 198.74
 ---- batch: 050 ----
mean loss: 205.80
 ---- batch: 060 ----
mean loss: 213.37
 ---- batch: 070 ----
mean loss: 199.83
 ---- batch: 080 ----
mean loss: 209.90
 ---- batch: 090 ----
mean loss: 205.51
 ---- batch: 100 ----
mean loss: 199.00
 ---- batch: 110 ----
mean loss: 202.87
train mean loss: 204.93
epoch train time: 0:00:00.732113
elapsed time: 0:02:23.944888
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:43:15.514903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.01
 ---- batch: 020 ----
mean loss: 200.26
 ---- batch: 030 ----
mean loss: 205.90
 ---- batch: 040 ----
mean loss: 205.99
 ---- batch: 050 ----
mean loss: 202.53
 ---- batch: 060 ----
mean loss: 208.42
 ---- batch: 070 ----
mean loss: 203.71
 ---- batch: 080 ----
mean loss: 203.44
 ---- batch: 090 ----
mean loss: 205.13
 ---- batch: 100 ----
mean loss: 211.18
 ---- batch: 110 ----
mean loss: 196.82
train mean loss: 204.66
epoch train time: 0:00:00.737104
elapsed time: 0:02:24.682146
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:43:16.252167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.88
 ---- batch: 020 ----
mean loss: 200.73
 ---- batch: 030 ----
mean loss: 203.12
 ---- batch: 040 ----
mean loss: 196.47
 ---- batch: 050 ----
mean loss: 207.83
 ---- batch: 060 ----
mean loss: 200.31
 ---- batch: 070 ----
mean loss: 198.24
 ---- batch: 080 ----
mean loss: 213.01
 ---- batch: 090 ----
mean loss: 209.06
 ---- batch: 100 ----
mean loss: 203.45
 ---- batch: 110 ----
mean loss: 197.61
train mean loss: 204.27
epoch train time: 0:00:00.728764
elapsed time: 0:02:25.411070
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:43:16.981088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.44
 ---- batch: 020 ----
mean loss: 193.17
 ---- batch: 030 ----
mean loss: 206.50
 ---- batch: 040 ----
mean loss: 201.00
 ---- batch: 050 ----
mean loss: 209.69
 ---- batch: 060 ----
mean loss: 206.36
 ---- batch: 070 ----
mean loss: 207.81
 ---- batch: 080 ----
mean loss: 201.71
 ---- batch: 090 ----
mean loss: 200.55
 ---- batch: 100 ----
mean loss: 203.32
 ---- batch: 110 ----
mean loss: 206.90
train mean loss: 204.15
epoch train time: 0:00:00.727002
elapsed time: 0:02:26.138257
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:43:17.708272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.74
 ---- batch: 020 ----
mean loss: 200.22
 ---- batch: 030 ----
mean loss: 206.27
 ---- batch: 040 ----
mean loss: 200.58
 ---- batch: 050 ----
mean loss: 204.13
 ---- batch: 060 ----
mean loss: 199.93
 ---- batch: 070 ----
mean loss: 206.99
 ---- batch: 080 ----
mean loss: 201.81
 ---- batch: 090 ----
mean loss: 208.49
 ---- batch: 100 ----
mean loss: 198.88
 ---- batch: 110 ----
mean loss: 210.70
train mean loss: 203.79
epoch train time: 0:00:00.732014
elapsed time: 0:02:26.870407
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:43:18.440425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.90
 ---- batch: 020 ----
mean loss: 200.41
 ---- batch: 030 ----
mean loss: 199.97
 ---- batch: 040 ----
mean loss: 205.45
 ---- batch: 050 ----
mean loss: 199.04
 ---- batch: 060 ----
mean loss: 200.53
 ---- batch: 070 ----
mean loss: 210.26
 ---- batch: 080 ----
mean loss: 202.19
 ---- batch: 090 ----
mean loss: 207.22
 ---- batch: 100 ----
mean loss: 204.22
 ---- batch: 110 ----
mean loss: 208.78
train mean loss: 203.86
epoch train time: 0:00:00.731076
elapsed time: 0:02:27.601639
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:43:19.171676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.75
 ---- batch: 020 ----
mean loss: 197.87
 ---- batch: 030 ----
mean loss: 210.00
 ---- batch: 040 ----
mean loss: 216.56
 ---- batch: 050 ----
mean loss: 201.09
 ---- batch: 060 ----
mean loss: 203.96
 ---- batch: 070 ----
mean loss: 202.25
 ---- batch: 080 ----
mean loss: 199.20
 ---- batch: 090 ----
mean loss: 196.54
 ---- batch: 100 ----
mean loss: 205.54
 ---- batch: 110 ----
mean loss: 199.66
train mean loss: 203.70
epoch train time: 0:00:00.728822
elapsed time: 0:02:28.330620
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:43:19.900645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.33
 ---- batch: 020 ----
mean loss: 203.17
 ---- batch: 030 ----
mean loss: 205.33
 ---- batch: 040 ----
mean loss: 200.12
 ---- batch: 050 ----
mean loss: 193.04
 ---- batch: 060 ----
mean loss: 210.79
 ---- batch: 070 ----
mean loss: 195.86
 ---- batch: 080 ----
mean loss: 202.45
 ---- batch: 090 ----
mean loss: 206.38
 ---- batch: 100 ----
mean loss: 208.00
 ---- batch: 110 ----
mean loss: 211.01
train mean loss: 203.40
epoch train time: 0:00:00.722625
elapsed time: 0:02:29.053391
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:43:20.623406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.03
 ---- batch: 020 ----
mean loss: 201.38
 ---- batch: 030 ----
mean loss: 207.64
 ---- batch: 040 ----
mean loss: 196.22
 ---- batch: 050 ----
mean loss: 199.34
 ---- batch: 060 ----
mean loss: 200.76
 ---- batch: 070 ----
mean loss: 204.56
 ---- batch: 080 ----
mean loss: 197.68
 ---- batch: 090 ----
mean loss: 196.25
 ---- batch: 100 ----
mean loss: 207.72
 ---- batch: 110 ----
mean loss: 212.35
train mean loss: 203.05
epoch train time: 0:00:00.734012
elapsed time: 0:02:29.787547
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:43:21.357563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.50
 ---- batch: 020 ----
mean loss: 199.20
 ---- batch: 030 ----
mean loss: 199.51
 ---- batch: 040 ----
mean loss: 206.67
 ---- batch: 050 ----
mean loss: 205.02
 ---- batch: 060 ----
mean loss: 196.63
 ---- batch: 070 ----
mean loss: 209.78
 ---- batch: 080 ----
mean loss: 202.18
 ---- batch: 090 ----
mean loss: 197.95
 ---- batch: 100 ----
mean loss: 211.77
 ---- batch: 110 ----
mean loss: 200.09
train mean loss: 203.03
epoch train time: 0:00:00.734121
elapsed time: 0:02:30.521808
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:43:22.091826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.59
 ---- batch: 020 ----
mean loss: 200.83
 ---- batch: 030 ----
mean loss: 196.64
 ---- batch: 040 ----
mean loss: 204.62
 ---- batch: 050 ----
mean loss: 220.30
 ---- batch: 060 ----
mean loss: 195.63
 ---- batch: 070 ----
mean loss: 199.19
 ---- batch: 080 ----
mean loss: 209.58
 ---- batch: 090 ----
mean loss: 205.35
 ---- batch: 100 ----
mean loss: 193.40
 ---- batch: 110 ----
mean loss: 197.94
train mean loss: 202.60
epoch train time: 0:00:00.731962
elapsed time: 0:02:31.253910
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:43:22.823936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.32
 ---- batch: 020 ----
mean loss: 204.93
 ---- batch: 030 ----
mean loss: 204.22
 ---- batch: 040 ----
mean loss: 205.00
 ---- batch: 050 ----
mean loss: 213.13
 ---- batch: 060 ----
mean loss: 201.51
 ---- batch: 070 ----
mean loss: 200.02
 ---- batch: 080 ----
mean loss: 202.34
 ---- batch: 090 ----
mean loss: 201.03
 ---- batch: 100 ----
mean loss: 208.05
 ---- batch: 110 ----
mean loss: 200.57
train mean loss: 202.49
epoch train time: 0:00:00.728250
elapsed time: 0:02:31.982306
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:43:23.552329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.74
 ---- batch: 020 ----
mean loss: 207.09
 ---- batch: 030 ----
mean loss: 205.69
 ---- batch: 040 ----
mean loss: 193.50
 ---- batch: 050 ----
mean loss: 214.69
 ---- batch: 060 ----
mean loss: 204.84
 ---- batch: 070 ----
mean loss: 193.95
 ---- batch: 080 ----
mean loss: 185.94
 ---- batch: 090 ----
mean loss: 200.26
 ---- batch: 100 ----
mean loss: 205.94
 ---- batch: 110 ----
mean loss: 208.82
train mean loss: 202.49
epoch train time: 0:00:00.735519
elapsed time: 0:02:32.717970
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:43:24.287995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.11
 ---- batch: 020 ----
mean loss: 206.07
 ---- batch: 030 ----
mean loss: 209.84
 ---- batch: 040 ----
mean loss: 211.65
 ---- batch: 050 ----
mean loss: 202.44
 ---- batch: 060 ----
mean loss: 199.46
 ---- batch: 070 ----
mean loss: 201.50
 ---- batch: 080 ----
mean loss: 202.39
 ---- batch: 090 ----
mean loss: 192.05
 ---- batch: 100 ----
mean loss: 192.79
 ---- batch: 110 ----
mean loss: 199.88
train mean loss: 202.02
epoch train time: 0:00:00.732908
elapsed time: 0:02:33.451066
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:43:25.021113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.74
 ---- batch: 020 ----
mean loss: 206.62
 ---- batch: 030 ----
mean loss: 196.94
 ---- batch: 040 ----
mean loss: 201.02
 ---- batch: 050 ----
mean loss: 206.09
 ---- batch: 060 ----
mean loss: 198.44
 ---- batch: 070 ----
mean loss: 214.94
 ---- batch: 080 ----
mean loss: 200.07
 ---- batch: 090 ----
mean loss: 194.30
 ---- batch: 100 ----
mean loss: 202.53
 ---- batch: 110 ----
mean loss: 200.95
train mean loss: 201.93
epoch train time: 0:00:00.722688
elapsed time: 0:02:34.173921
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:43:25.743946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.92
 ---- batch: 020 ----
mean loss: 192.97
 ---- batch: 030 ----
mean loss: 205.90
 ---- batch: 040 ----
mean loss: 203.13
 ---- batch: 050 ----
mean loss: 214.07
 ---- batch: 060 ----
mean loss: 200.72
 ---- batch: 070 ----
mean loss: 199.69
 ---- batch: 080 ----
mean loss: 205.83
 ---- batch: 090 ----
mean loss: 203.52
 ---- batch: 100 ----
mean loss: 196.49
 ---- batch: 110 ----
mean loss: 192.18
train mean loss: 201.83
epoch train time: 0:00:00.735229
elapsed time: 0:02:34.909315
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:43:26.479347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.82
 ---- batch: 020 ----
mean loss: 202.19
 ---- batch: 030 ----
mean loss: 198.17
 ---- batch: 040 ----
mean loss: 197.90
 ---- batch: 050 ----
mean loss: 194.52
 ---- batch: 060 ----
mean loss: 204.50
 ---- batch: 070 ----
mean loss: 189.13
 ---- batch: 080 ----
mean loss: 213.43
 ---- batch: 090 ----
mean loss: 206.01
 ---- batch: 100 ----
mean loss: 214.58
 ---- batch: 110 ----
mean loss: 196.52
train mean loss: 201.68
epoch train time: 0:00:00.727496
elapsed time: 0:02:35.636965
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:43:27.206982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.18
 ---- batch: 020 ----
mean loss: 198.79
 ---- batch: 030 ----
mean loss: 199.16
 ---- batch: 040 ----
mean loss: 193.48
 ---- batch: 050 ----
mean loss: 203.66
 ---- batch: 060 ----
mean loss: 198.87
 ---- batch: 070 ----
mean loss: 199.50
 ---- batch: 080 ----
mean loss: 196.12
 ---- batch: 090 ----
mean loss: 208.94
 ---- batch: 100 ----
mean loss: 206.82
 ---- batch: 110 ----
mean loss: 207.11
train mean loss: 201.29
epoch train time: 0:00:00.724758
elapsed time: 0:02:36.361860
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:43:27.931876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.31
 ---- batch: 020 ----
mean loss: 208.89
 ---- batch: 030 ----
mean loss: 203.20
 ---- batch: 040 ----
mean loss: 211.09
 ---- batch: 050 ----
mean loss: 209.62
 ---- batch: 060 ----
mean loss: 196.80
 ---- batch: 070 ----
mean loss: 191.25
 ---- batch: 080 ----
mean loss: 199.86
 ---- batch: 090 ----
mean loss: 196.21
 ---- batch: 100 ----
mean loss: 196.04
 ---- batch: 110 ----
mean loss: 195.35
train mean loss: 201.30
epoch train time: 0:00:00.725821
elapsed time: 0:02:37.087819
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:43:28.657855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.73
 ---- batch: 020 ----
mean loss: 202.29
 ---- batch: 030 ----
mean loss: 188.23
 ---- batch: 040 ----
mean loss: 211.80
 ---- batch: 050 ----
mean loss: 206.15
 ---- batch: 060 ----
mean loss: 205.09
 ---- batch: 070 ----
mean loss: 201.43
 ---- batch: 080 ----
mean loss: 206.66
 ---- batch: 090 ----
mean loss: 196.38
 ---- batch: 100 ----
mean loss: 203.47
 ---- batch: 110 ----
mean loss: 194.31
train mean loss: 201.12
epoch train time: 0:00:00.734559
elapsed time: 0:02:37.822541
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:43:29.392571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.23
 ---- batch: 020 ----
mean loss: 206.42
 ---- batch: 030 ----
mean loss: 206.66
 ---- batch: 040 ----
mean loss: 196.75
 ---- batch: 050 ----
mean loss: 196.92
 ---- batch: 060 ----
mean loss: 204.40
 ---- batch: 070 ----
mean loss: 196.58
 ---- batch: 080 ----
mean loss: 203.83
 ---- batch: 090 ----
mean loss: 205.95
 ---- batch: 100 ----
mean loss: 194.53
 ---- batch: 110 ----
mean loss: 194.39
train mean loss: 200.84
epoch train time: 0:00:00.727539
elapsed time: 0:02:38.550244
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:43:30.120259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.55
 ---- batch: 020 ----
mean loss: 213.03
 ---- batch: 030 ----
mean loss: 205.34
 ---- batch: 040 ----
mean loss: 198.52
 ---- batch: 050 ----
mean loss: 200.96
 ---- batch: 060 ----
mean loss: 194.11
 ---- batch: 070 ----
mean loss: 209.88
 ---- batch: 080 ----
mean loss: 196.03
 ---- batch: 090 ----
mean loss: 200.96
 ---- batch: 100 ----
mean loss: 196.94
 ---- batch: 110 ----
mean loss: 202.29
train mean loss: 200.61
epoch train time: 0:00:00.726628
elapsed time: 0:02:39.277055
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:43:30.847070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.80
 ---- batch: 020 ----
mean loss: 205.92
 ---- batch: 030 ----
mean loss: 197.61
 ---- batch: 040 ----
mean loss: 192.66
 ---- batch: 050 ----
mean loss: 200.13
 ---- batch: 060 ----
mean loss: 200.75
 ---- batch: 070 ----
mean loss: 191.96
 ---- batch: 080 ----
mean loss: 203.17
 ---- batch: 090 ----
mean loss: 208.92
 ---- batch: 100 ----
mean loss: 199.66
 ---- batch: 110 ----
mean loss: 206.66
train mean loss: 200.37
epoch train time: 0:00:00.731025
elapsed time: 0:02:40.008220
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:43:31.578237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.92
 ---- batch: 020 ----
mean loss: 197.77
 ---- batch: 030 ----
mean loss: 207.33
 ---- batch: 040 ----
mean loss: 202.51
 ---- batch: 050 ----
mean loss: 193.43
 ---- batch: 060 ----
mean loss: 198.34
 ---- batch: 070 ----
mean loss: 199.64
 ---- batch: 080 ----
mean loss: 195.94
 ---- batch: 090 ----
mean loss: 208.20
 ---- batch: 100 ----
mean loss: 191.05
 ---- batch: 110 ----
mean loss: 205.15
train mean loss: 200.30
epoch train time: 0:00:00.746555
elapsed time: 0:02:40.754930
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:43:32.324943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.00
 ---- batch: 020 ----
mean loss: 208.39
 ---- batch: 030 ----
mean loss: 198.99
 ---- batch: 040 ----
mean loss: 200.27
 ---- batch: 050 ----
mean loss: 204.03
 ---- batch: 060 ----
mean loss: 200.58
 ---- batch: 070 ----
mean loss: 198.98
 ---- batch: 080 ----
mean loss: 192.33
 ---- batch: 090 ----
mean loss: 199.02
 ---- batch: 100 ----
mean loss: 201.77
 ---- batch: 110 ----
mean loss: 198.29
train mean loss: 199.97
epoch train time: 0:00:00.736490
elapsed time: 0:02:41.491556
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:43:33.061573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.90
 ---- batch: 020 ----
mean loss: 200.74
 ---- batch: 030 ----
mean loss: 195.46
 ---- batch: 040 ----
mean loss: 198.48
 ---- batch: 050 ----
mean loss: 194.99
 ---- batch: 060 ----
mean loss: 200.54
 ---- batch: 070 ----
mean loss: 202.51
 ---- batch: 080 ----
mean loss: 197.53
 ---- batch: 090 ----
mean loss: 205.83
 ---- batch: 100 ----
mean loss: 201.75
 ---- batch: 110 ----
mean loss: 193.68
train mean loss: 199.98
epoch train time: 0:00:00.738640
elapsed time: 0:02:42.230336
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:43:33.800377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.41
 ---- batch: 020 ----
mean loss: 199.36
 ---- batch: 030 ----
mean loss: 186.81
 ---- batch: 040 ----
mean loss: 209.92
 ---- batch: 050 ----
mean loss: 205.29
 ---- batch: 060 ----
mean loss: 202.01
 ---- batch: 070 ----
mean loss: 202.12
 ---- batch: 080 ----
mean loss: 200.51
 ---- batch: 090 ----
mean loss: 191.85
 ---- batch: 100 ----
mean loss: 199.10
 ---- batch: 110 ----
mean loss: 195.83
train mean loss: 199.70
epoch train time: 0:00:00.733944
elapsed time: 0:02:42.964447
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:43:34.534473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.09
 ---- batch: 020 ----
mean loss: 211.93
 ---- batch: 030 ----
mean loss: 203.64
 ---- batch: 040 ----
mean loss: 191.46
 ---- batch: 050 ----
mean loss: 199.43
 ---- batch: 060 ----
mean loss: 196.97
 ---- batch: 070 ----
mean loss: 189.88
 ---- batch: 080 ----
mean loss: 202.68
 ---- batch: 090 ----
mean loss: 200.48
 ---- batch: 100 ----
mean loss: 200.57
 ---- batch: 110 ----
mean loss: 189.98
train mean loss: 199.59
epoch train time: 0:00:00.727366
elapsed time: 0:02:43.691958
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:43:35.261974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.33
 ---- batch: 020 ----
mean loss: 208.71
 ---- batch: 030 ----
mean loss: 199.13
 ---- batch: 040 ----
mean loss: 194.73
 ---- batch: 050 ----
mean loss: 191.77
 ---- batch: 060 ----
mean loss: 201.92
 ---- batch: 070 ----
mean loss: 197.50
 ---- batch: 080 ----
mean loss: 194.17
 ---- batch: 090 ----
mean loss: 206.47
 ---- batch: 100 ----
mean loss: 197.36
 ---- batch: 110 ----
mean loss: 204.24
train mean loss: 199.59
epoch train time: 0:00:00.736356
elapsed time: 0:02:44.428470
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:43:35.998488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.32
 ---- batch: 020 ----
mean loss: 193.09
 ---- batch: 030 ----
mean loss: 204.22
 ---- batch: 040 ----
mean loss: 205.69
 ---- batch: 050 ----
mean loss: 206.11
 ---- batch: 060 ----
mean loss: 192.92
 ---- batch: 070 ----
mean loss: 203.78
 ---- batch: 080 ----
mean loss: 198.57
 ---- batch: 090 ----
mean loss: 195.81
 ---- batch: 100 ----
mean loss: 194.63
 ---- batch: 110 ----
mean loss: 207.94
train mean loss: 199.53
epoch train time: 0:00:00.731733
elapsed time: 0:02:45.160350
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:43:36.730368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.25
 ---- batch: 020 ----
mean loss: 206.79
 ---- batch: 030 ----
mean loss: 193.89
 ---- batch: 040 ----
mean loss: 190.97
 ---- batch: 050 ----
mean loss: 206.34
 ---- batch: 060 ----
mean loss: 202.75
 ---- batch: 070 ----
mean loss: 198.90
 ---- batch: 080 ----
mean loss: 202.91
 ---- batch: 090 ----
mean loss: 191.10
 ---- batch: 100 ----
mean loss: 202.21
 ---- batch: 110 ----
mean loss: 201.96
train mean loss: 199.08
epoch train time: 0:00:00.726505
elapsed time: 0:02:45.886999
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:43:37.457021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.14
 ---- batch: 020 ----
mean loss: 199.68
 ---- batch: 030 ----
mean loss: 214.12
 ---- batch: 040 ----
mean loss: 189.37
 ---- batch: 050 ----
mean loss: 196.46
 ---- batch: 060 ----
mean loss: 188.73
 ---- batch: 070 ----
mean loss: 192.77
 ---- batch: 080 ----
mean loss: 200.56
 ---- batch: 090 ----
mean loss: 201.56
 ---- batch: 100 ----
mean loss: 202.21
 ---- batch: 110 ----
mean loss: 201.11
train mean loss: 199.04
epoch train time: 0:00:00.729758
elapsed time: 0:02:46.616901
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:43:38.186936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.19
 ---- batch: 020 ----
mean loss: 204.38
 ---- batch: 030 ----
mean loss: 198.39
 ---- batch: 040 ----
mean loss: 203.96
 ---- batch: 050 ----
mean loss: 196.15
 ---- batch: 060 ----
mean loss: 204.87
 ---- batch: 070 ----
mean loss: 197.11
 ---- batch: 080 ----
mean loss: 193.69
 ---- batch: 090 ----
mean loss: 201.67
 ---- batch: 100 ----
mean loss: 192.87
 ---- batch: 110 ----
mean loss: 191.19
train mean loss: 198.88
epoch train time: 0:00:00.745373
elapsed time: 0:02:47.362433
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:43:38.932469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.73
 ---- batch: 020 ----
mean loss: 202.40
 ---- batch: 030 ----
mean loss: 192.27
 ---- batch: 040 ----
mean loss: 204.36
 ---- batch: 050 ----
mean loss: 188.66
 ---- batch: 060 ----
mean loss: 197.15
 ---- batch: 070 ----
mean loss: 198.32
 ---- batch: 080 ----
mean loss: 200.03
 ---- batch: 090 ----
mean loss: 210.47
 ---- batch: 100 ----
mean loss: 200.85
 ---- batch: 110 ----
mean loss: 192.42
train mean loss: 198.53
epoch train time: 0:00:00.733279
elapsed time: 0:02:48.095869
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:43:39.665885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.26
 ---- batch: 020 ----
mean loss: 200.42
 ---- batch: 030 ----
mean loss: 200.48
 ---- batch: 040 ----
mean loss: 194.52
 ---- batch: 050 ----
mean loss: 202.94
 ---- batch: 060 ----
mean loss: 204.24
 ---- batch: 070 ----
mean loss: 196.82
 ---- batch: 080 ----
mean loss: 199.07
 ---- batch: 090 ----
mean loss: 191.41
 ---- batch: 100 ----
mean loss: 196.02
 ---- batch: 110 ----
mean loss: 204.01
train mean loss: 198.35
epoch train time: 0:00:00.738672
elapsed time: 0:02:48.834678
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:43:40.404695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.49
 ---- batch: 020 ----
mean loss: 188.74
 ---- batch: 030 ----
mean loss: 205.94
 ---- batch: 040 ----
mean loss: 196.81
 ---- batch: 050 ----
mean loss: 202.48
 ---- batch: 060 ----
mean loss: 199.21
 ---- batch: 070 ----
mean loss: 199.28
 ---- batch: 080 ----
mean loss: 192.36
 ---- batch: 090 ----
mean loss: 195.73
 ---- batch: 100 ----
mean loss: 203.05
 ---- batch: 110 ----
mean loss: 203.46
train mean loss: 198.45
epoch train time: 0:00:00.732759
elapsed time: 0:02:49.567584
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:43:41.137634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.94
 ---- batch: 020 ----
mean loss: 207.65
 ---- batch: 030 ----
mean loss: 203.20
 ---- batch: 040 ----
mean loss: 192.36
 ---- batch: 050 ----
mean loss: 198.28
 ---- batch: 060 ----
mean loss: 197.13
 ---- batch: 070 ----
mean loss: 199.39
 ---- batch: 080 ----
mean loss: 196.40
 ---- batch: 090 ----
mean loss: 188.68
 ---- batch: 100 ----
mean loss: 191.45
 ---- batch: 110 ----
mean loss: 198.72
train mean loss: 198.17
epoch train time: 0:00:00.728890
elapsed time: 0:02:50.296650
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:43:41.866668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.89
 ---- batch: 020 ----
mean loss: 192.67
 ---- batch: 030 ----
mean loss: 195.94
 ---- batch: 040 ----
mean loss: 201.67
 ---- batch: 050 ----
mean loss: 201.65
 ---- batch: 060 ----
mean loss: 206.14
 ---- batch: 070 ----
mean loss: 195.10
 ---- batch: 080 ----
mean loss: 192.19
 ---- batch: 090 ----
mean loss: 206.35
 ---- batch: 100 ----
mean loss: 197.78
 ---- batch: 110 ----
mean loss: 198.97
train mean loss: 198.08
epoch train time: 0:00:00.727210
elapsed time: 0:02:51.024000
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:43:42.594017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.43
 ---- batch: 020 ----
mean loss: 200.09
 ---- batch: 030 ----
mean loss: 210.54
 ---- batch: 040 ----
mean loss: 189.10
 ---- batch: 050 ----
mean loss: 198.76
 ---- batch: 060 ----
mean loss: 195.46
 ---- batch: 070 ----
mean loss: 198.13
 ---- batch: 080 ----
mean loss: 194.66
 ---- batch: 090 ----
mean loss: 197.69
 ---- batch: 100 ----
mean loss: 198.87
 ---- batch: 110 ----
mean loss: 193.65
train mean loss: 197.83
epoch train time: 0:00:00.736411
elapsed time: 0:02:51.760556
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:43:43.330609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.53
 ---- batch: 020 ----
mean loss: 197.69
 ---- batch: 030 ----
mean loss: 209.02
 ---- batch: 040 ----
mean loss: 199.49
 ---- batch: 050 ----
mean loss: 199.17
 ---- batch: 060 ----
mean loss: 196.13
 ---- batch: 070 ----
mean loss: 194.91
 ---- batch: 080 ----
mean loss: 204.45
 ---- batch: 090 ----
mean loss: 198.75
 ---- batch: 100 ----
mean loss: 188.69
 ---- batch: 110 ----
mean loss: 190.91
train mean loss: 197.80
epoch train time: 0:00:00.737625
elapsed time: 0:02:52.498362
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:43:44.068383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.14
 ---- batch: 020 ----
mean loss: 195.09
 ---- batch: 030 ----
mean loss: 205.12
 ---- batch: 040 ----
mean loss: 195.32
 ---- batch: 050 ----
mean loss: 192.16
 ---- batch: 060 ----
mean loss: 204.01
 ---- batch: 070 ----
mean loss: 204.60
 ---- batch: 080 ----
mean loss: 195.51
 ---- batch: 090 ----
mean loss: 195.38
 ---- batch: 100 ----
mean loss: 199.94
 ---- batch: 110 ----
mean loss: 201.28
train mean loss: 197.70
epoch train time: 0:00:00.735277
elapsed time: 0:02:53.233785
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:43:44.803802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.49
 ---- batch: 020 ----
mean loss: 191.47
 ---- batch: 030 ----
mean loss: 197.60
 ---- batch: 040 ----
mean loss: 197.77
 ---- batch: 050 ----
mean loss: 206.52
 ---- batch: 060 ----
mean loss: 202.10
 ---- batch: 070 ----
mean loss: 191.73
 ---- batch: 080 ----
mean loss: 192.04
 ---- batch: 090 ----
mean loss: 200.72
 ---- batch: 100 ----
mean loss: 201.72
 ---- batch: 110 ----
mean loss: 193.21
train mean loss: 197.40
epoch train time: 0:00:00.742063
elapsed time: 0:02:53.975995
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:43:45.546012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.53
 ---- batch: 020 ----
mean loss: 194.14
 ---- batch: 030 ----
mean loss: 193.62
 ---- batch: 040 ----
mean loss: 195.52
 ---- batch: 050 ----
mean loss: 204.28
 ---- batch: 060 ----
mean loss: 208.89
 ---- batch: 070 ----
mean loss: 190.55
 ---- batch: 080 ----
mean loss: 198.73
 ---- batch: 090 ----
mean loss: 192.84
 ---- batch: 100 ----
mean loss: 190.29
 ---- batch: 110 ----
mean loss: 196.75
train mean loss: 197.30
epoch train time: 0:00:00.739780
elapsed time: 0:02:54.715916
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:43:46.285933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.36
 ---- batch: 020 ----
mean loss: 200.77
 ---- batch: 030 ----
mean loss: 196.67
 ---- batch: 040 ----
mean loss: 200.09
 ---- batch: 050 ----
mean loss: 202.82
 ---- batch: 060 ----
mean loss: 188.59
 ---- batch: 070 ----
mean loss: 189.60
 ---- batch: 080 ----
mean loss: 193.57
 ---- batch: 090 ----
mean loss: 189.37
 ---- batch: 100 ----
mean loss: 204.51
 ---- batch: 110 ----
mean loss: 203.87
train mean loss: 197.06
epoch train time: 0:00:00.737542
elapsed time: 0:02:55.453596
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:43:47.023613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.29
 ---- batch: 020 ----
mean loss: 205.51
 ---- batch: 030 ----
mean loss: 203.72
 ---- batch: 040 ----
mean loss: 194.71
 ---- batch: 050 ----
mean loss: 187.54
 ---- batch: 060 ----
mean loss: 192.69
 ---- batch: 070 ----
mean loss: 205.64
 ---- batch: 080 ----
mean loss: 195.78
 ---- batch: 090 ----
mean loss: 199.39
 ---- batch: 100 ----
mean loss: 201.73
 ---- batch: 110 ----
mean loss: 187.85
train mean loss: 196.88
epoch train time: 0:00:00.732036
elapsed time: 0:02:56.185787
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:43:47.755827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.92
 ---- batch: 020 ----
mean loss: 202.80
 ---- batch: 030 ----
mean loss: 206.20
 ---- batch: 040 ----
mean loss: 201.17
 ---- batch: 050 ----
mean loss: 188.93
 ---- batch: 060 ----
mean loss: 193.10
 ---- batch: 070 ----
mean loss: 196.75
 ---- batch: 080 ----
mean loss: 192.47
 ---- batch: 090 ----
mean loss: 201.47
 ---- batch: 100 ----
mean loss: 195.68
 ---- batch: 110 ----
mean loss: 199.93
train mean loss: 196.84
epoch train time: 0:00:00.736988
elapsed time: 0:02:56.922980
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:43:48.493012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.33
 ---- batch: 020 ----
mean loss: 199.52
 ---- batch: 030 ----
mean loss: 190.94
 ---- batch: 040 ----
mean loss: 207.05
 ---- batch: 050 ----
mean loss: 192.52
 ---- batch: 060 ----
mean loss: 195.67
 ---- batch: 070 ----
mean loss: 198.23
 ---- batch: 080 ----
mean loss: 196.05
 ---- batch: 090 ----
mean loss: 193.26
 ---- batch: 100 ----
mean loss: 184.84
 ---- batch: 110 ----
mean loss: 204.53
train mean loss: 196.64
epoch train time: 0:00:00.742993
elapsed time: 0:02:57.666132
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:43:49.236152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.10
 ---- batch: 020 ----
mean loss: 193.03
 ---- batch: 030 ----
mean loss: 202.65
 ---- batch: 040 ----
mean loss: 187.32
 ---- batch: 050 ----
mean loss: 204.08
 ---- batch: 060 ----
mean loss: 189.53
 ---- batch: 070 ----
mean loss: 210.99
 ---- batch: 080 ----
mean loss: 202.02
 ---- batch: 090 ----
mean loss: 194.61
 ---- batch: 100 ----
mean loss: 193.94
 ---- batch: 110 ----
mean loss: 192.36
train mean loss: 196.54
epoch train time: 0:00:00.746367
elapsed time: 0:02:58.412646
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:43:49.982664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.20
 ---- batch: 020 ----
mean loss: 193.55
 ---- batch: 030 ----
mean loss: 194.81
 ---- batch: 040 ----
mean loss: 196.24
 ---- batch: 050 ----
mean loss: 194.63
 ---- batch: 060 ----
mean loss: 205.80
 ---- batch: 070 ----
mean loss: 188.06
 ---- batch: 080 ----
mean loss: 191.94
 ---- batch: 090 ----
mean loss: 194.74
 ---- batch: 100 ----
mean loss: 185.83
 ---- batch: 110 ----
mean loss: 203.73
train mean loss: 196.38
epoch train time: 0:00:00.730847
elapsed time: 0:02:59.143653
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:43:50.713675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.17
 ---- batch: 020 ----
mean loss: 190.38
 ---- batch: 030 ----
mean loss: 207.03
 ---- batch: 040 ----
mean loss: 187.10
 ---- batch: 050 ----
mean loss: 197.30
 ---- batch: 060 ----
mean loss: 209.49
 ---- batch: 070 ----
mean loss: 196.10
 ---- batch: 080 ----
mean loss: 196.95
 ---- batch: 090 ----
mean loss: 186.30
 ---- batch: 100 ----
mean loss: 192.29
 ---- batch: 110 ----
mean loss: 198.70
train mean loss: 196.28
epoch train time: 0:00:00.724201
elapsed time: 0:02:59.868014
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:43:51.438028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.11
 ---- batch: 020 ----
mean loss: 190.98
 ---- batch: 030 ----
mean loss: 196.24
 ---- batch: 040 ----
mean loss: 192.67
 ---- batch: 050 ----
mean loss: 198.41
 ---- batch: 060 ----
mean loss: 200.59
 ---- batch: 070 ----
mean loss: 197.93
 ---- batch: 080 ----
mean loss: 186.38
 ---- batch: 090 ----
mean loss: 197.23
 ---- batch: 100 ----
mean loss: 197.73
 ---- batch: 110 ----
mean loss: 206.62
train mean loss: 196.29
epoch train time: 0:00:00.741081
elapsed time: 0:03:00.609235
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:43:52.179275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.81
 ---- batch: 020 ----
mean loss: 194.50
 ---- batch: 030 ----
mean loss: 205.36
 ---- batch: 040 ----
mean loss: 197.90
 ---- batch: 050 ----
mean loss: 192.66
 ---- batch: 060 ----
mean loss: 195.12
 ---- batch: 070 ----
mean loss: 192.45
 ---- batch: 080 ----
mean loss: 194.74
 ---- batch: 090 ----
mean loss: 195.45
 ---- batch: 100 ----
mean loss: 187.52
 ---- batch: 110 ----
mean loss: 196.71
train mean loss: 195.68
epoch train time: 0:00:00.735627
elapsed time: 0:03:01.345056
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:43:52.915084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.49
 ---- batch: 020 ----
mean loss: 189.38
 ---- batch: 030 ----
mean loss: 188.73
 ---- batch: 040 ----
mean loss: 201.56
 ---- batch: 050 ----
mean loss: 196.87
 ---- batch: 060 ----
mean loss: 196.13
 ---- batch: 070 ----
mean loss: 190.98
 ---- batch: 080 ----
mean loss: 204.03
 ---- batch: 090 ----
mean loss: 200.34
 ---- batch: 100 ----
mean loss: 195.07
 ---- batch: 110 ----
mean loss: 186.81
train mean loss: 195.63
epoch train time: 0:00:00.726778
elapsed time: 0:03:02.072006
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:43:53.642025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.15
 ---- batch: 020 ----
mean loss: 200.07
 ---- batch: 030 ----
mean loss: 200.00
 ---- batch: 040 ----
mean loss: 198.97
 ---- batch: 050 ----
mean loss: 199.05
 ---- batch: 060 ----
mean loss: 195.33
 ---- batch: 070 ----
mean loss: 188.50
 ---- batch: 080 ----
mean loss: 203.89
 ---- batch: 090 ----
mean loss: 193.86
 ---- batch: 100 ----
mean loss: 192.13
 ---- batch: 110 ----
mean loss: 188.99
train mean loss: 195.54
epoch train time: 0:00:00.732137
elapsed time: 0:03:02.804289
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:43:54.374307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.74
 ---- batch: 020 ----
mean loss: 198.02
 ---- batch: 030 ----
mean loss: 191.29
 ---- batch: 040 ----
mean loss: 192.32
 ---- batch: 050 ----
mean loss: 199.97
 ---- batch: 060 ----
mean loss: 195.13
 ---- batch: 070 ----
mean loss: 199.78
 ---- batch: 080 ----
mean loss: 201.09
 ---- batch: 090 ----
mean loss: 197.39
 ---- batch: 100 ----
mean loss: 190.51
 ---- batch: 110 ----
mean loss: 190.54
train mean loss: 195.53
epoch train time: 0:00:00.730381
elapsed time: 0:03:03.534809
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:43:55.104826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.34
 ---- batch: 020 ----
mean loss: 191.49
 ---- batch: 030 ----
mean loss: 195.30
 ---- batch: 040 ----
mean loss: 192.63
 ---- batch: 050 ----
mean loss: 197.59
 ---- batch: 060 ----
mean loss: 200.74
 ---- batch: 070 ----
mean loss: 200.14
 ---- batch: 080 ----
mean loss: 200.72
 ---- batch: 090 ----
mean loss: 190.12
 ---- batch: 100 ----
mean loss: 188.19
 ---- batch: 110 ----
mean loss: 202.20
train mean loss: 195.55
epoch train time: 0:00:00.729230
elapsed time: 0:03:04.264178
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:43:55.834211
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.49
 ---- batch: 020 ----
mean loss: 194.38
 ---- batch: 030 ----
mean loss: 195.33
 ---- batch: 040 ----
mean loss: 197.80
 ---- batch: 050 ----
mean loss: 187.86
 ---- batch: 060 ----
mean loss: 195.30
 ---- batch: 070 ----
mean loss: 198.56
 ---- batch: 080 ----
mean loss: 201.22
 ---- batch: 090 ----
mean loss: 194.44
 ---- batch: 100 ----
mean loss: 186.20
 ---- batch: 110 ----
mean loss: 201.85
train mean loss: 195.61
epoch train time: 0:00:00.727612
elapsed time: 0:03:04.991944
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:43:56.561975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.23
 ---- batch: 020 ----
mean loss: 197.04
 ---- batch: 030 ----
mean loss: 197.40
 ---- batch: 040 ----
mean loss: 206.50
 ---- batch: 050 ----
mean loss: 183.60
 ---- batch: 060 ----
mean loss: 196.49
 ---- batch: 070 ----
mean loss: 185.55
 ---- batch: 080 ----
mean loss: 200.39
 ---- batch: 090 ----
mean loss: 191.07
 ---- batch: 100 ----
mean loss: 205.86
 ---- batch: 110 ----
mean loss: 192.25
train mean loss: 195.62
epoch train time: 0:00:00.730424
elapsed time: 0:03:05.722538
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:43:57.292561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.62
 ---- batch: 020 ----
mean loss: 193.67
 ---- batch: 030 ----
mean loss: 186.57
 ---- batch: 040 ----
mean loss: 192.58
 ---- batch: 050 ----
mean loss: 198.93
 ---- batch: 060 ----
mean loss: 205.10
 ---- batch: 070 ----
mean loss: 201.43
 ---- batch: 080 ----
mean loss: 202.32
 ---- batch: 090 ----
mean loss: 193.47
 ---- batch: 100 ----
mean loss: 192.24
 ---- batch: 110 ----
mean loss: 198.81
train mean loss: 195.60
epoch train time: 0:00:00.726391
elapsed time: 0:03:06.449120
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:43:58.019135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.40
 ---- batch: 020 ----
mean loss: 176.87
 ---- batch: 030 ----
mean loss: 198.30
 ---- batch: 040 ----
mean loss: 191.29
 ---- batch: 050 ----
mean loss: 191.92
 ---- batch: 060 ----
mean loss: 205.16
 ---- batch: 070 ----
mean loss: 194.89
 ---- batch: 080 ----
mean loss: 194.12
 ---- batch: 090 ----
mean loss: 195.83
 ---- batch: 100 ----
mean loss: 197.05
 ---- batch: 110 ----
mean loss: 209.90
train mean loss: 195.50
epoch train time: 0:00:00.726797
elapsed time: 0:03:07.176058
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:43:58.746077
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.76
 ---- batch: 020 ----
mean loss: 190.62
 ---- batch: 030 ----
mean loss: 196.23
 ---- batch: 040 ----
mean loss: 201.62
 ---- batch: 050 ----
mean loss: 200.17
 ---- batch: 060 ----
mean loss: 200.93
 ---- batch: 070 ----
mean loss: 189.00
 ---- batch: 080 ----
mean loss: 189.08
 ---- batch: 090 ----
mean loss: 185.73
 ---- batch: 100 ----
mean loss: 191.29
 ---- batch: 110 ----
mean loss: 197.23
train mean loss: 195.49
epoch train time: 0:00:00.725179
elapsed time: 0:03:07.901380
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:43:59.471395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.90
 ---- batch: 020 ----
mean loss: 201.39
 ---- batch: 030 ----
mean loss: 196.90
 ---- batch: 040 ----
mean loss: 199.77
 ---- batch: 050 ----
mean loss: 202.22
 ---- batch: 060 ----
mean loss: 195.39
 ---- batch: 070 ----
mean loss: 188.04
 ---- batch: 080 ----
mean loss: 189.21
 ---- batch: 090 ----
mean loss: 203.93
 ---- batch: 100 ----
mean loss: 185.08
 ---- batch: 110 ----
mean loss: 192.11
train mean loss: 195.49
epoch train time: 0:00:00.728249
elapsed time: 0:03:08.629767
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:44:00.199782
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.90
 ---- batch: 020 ----
mean loss: 193.98
 ---- batch: 030 ----
mean loss: 196.22
 ---- batch: 040 ----
mean loss: 197.29
 ---- batch: 050 ----
mean loss: 200.93
 ---- batch: 060 ----
mean loss: 198.31
 ---- batch: 070 ----
mean loss: 200.12
 ---- batch: 080 ----
mean loss: 190.75
 ---- batch: 090 ----
mean loss: 195.84
 ---- batch: 100 ----
mean loss: 194.79
 ---- batch: 110 ----
mean loss: 193.41
train mean loss: 195.41
epoch train time: 0:00:00.727396
elapsed time: 0:03:09.357312
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:44:00.927330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.25
 ---- batch: 020 ----
mean loss: 192.72
 ---- batch: 030 ----
mean loss: 183.87
 ---- batch: 040 ----
mean loss: 201.88
 ---- batch: 050 ----
mean loss: 199.92
 ---- batch: 060 ----
mean loss: 199.49
 ---- batch: 070 ----
mean loss: 186.55
 ---- batch: 080 ----
mean loss: 194.55
 ---- batch: 090 ----
mean loss: 186.76
 ---- batch: 100 ----
mean loss: 200.94
 ---- batch: 110 ----
mean loss: 204.30
train mean loss: 195.41
epoch train time: 0:00:00.732938
elapsed time: 0:03:10.090388
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:44:01.660403
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.32
 ---- batch: 020 ----
mean loss: 194.13
 ---- batch: 030 ----
mean loss: 198.02
 ---- batch: 040 ----
mean loss: 185.07
 ---- batch: 050 ----
mean loss: 200.85
 ---- batch: 060 ----
mean loss: 191.91
 ---- batch: 070 ----
mean loss: 203.73
 ---- batch: 080 ----
mean loss: 196.34
 ---- batch: 090 ----
mean loss: 193.00
 ---- batch: 100 ----
mean loss: 195.88
 ---- batch: 110 ----
mean loss: 199.85
train mean loss: 195.48
epoch train time: 0:00:00.735944
elapsed time: 0:03:10.826471
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:44:02.396517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.76
 ---- batch: 020 ----
mean loss: 202.95
 ---- batch: 030 ----
mean loss: 195.11
 ---- batch: 040 ----
mean loss: 201.39
 ---- batch: 050 ----
mean loss: 192.22
 ---- batch: 060 ----
mean loss: 188.83
 ---- batch: 070 ----
mean loss: 203.14
 ---- batch: 080 ----
mean loss: 185.63
 ---- batch: 090 ----
mean loss: 183.18
 ---- batch: 100 ----
mean loss: 192.10
 ---- batch: 110 ----
mean loss: 197.62
train mean loss: 195.42
epoch train time: 0:00:00.729772
elapsed time: 0:03:11.556413
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:44:03.126431
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.08
 ---- batch: 020 ----
mean loss: 193.41
 ---- batch: 030 ----
mean loss: 196.81
 ---- batch: 040 ----
mean loss: 198.34
 ---- batch: 050 ----
mean loss: 194.88
 ---- batch: 060 ----
mean loss: 193.60
 ---- batch: 070 ----
mean loss: 190.90
 ---- batch: 080 ----
mean loss: 196.83
 ---- batch: 090 ----
mean loss: 195.94
 ---- batch: 100 ----
mean loss: 197.96
 ---- batch: 110 ----
mean loss: 194.17
train mean loss: 195.44
epoch train time: 0:00:00.738836
elapsed time: 0:03:12.295397
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:44:03.865418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.08
 ---- batch: 020 ----
mean loss: 193.33
 ---- batch: 030 ----
mean loss: 194.74
 ---- batch: 040 ----
mean loss: 195.63
 ---- batch: 050 ----
mean loss: 190.46
 ---- batch: 060 ----
mean loss: 198.89
 ---- batch: 070 ----
mean loss: 193.19
 ---- batch: 080 ----
mean loss: 196.56
 ---- batch: 090 ----
mean loss: 195.29
 ---- batch: 100 ----
mean loss: 204.14
 ---- batch: 110 ----
mean loss: 195.02
train mean loss: 195.46
epoch train time: 0:00:00.732323
elapsed time: 0:03:13.027866
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:44:04.597884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.45
 ---- batch: 020 ----
mean loss: 202.70
 ---- batch: 030 ----
mean loss: 203.32
 ---- batch: 040 ----
mean loss: 199.74
 ---- batch: 050 ----
mean loss: 194.40
 ---- batch: 060 ----
mean loss: 198.22
 ---- batch: 070 ----
mean loss: 195.60
 ---- batch: 080 ----
mean loss: 182.77
 ---- batch: 090 ----
mean loss: 191.23
 ---- batch: 100 ----
mean loss: 195.77
 ---- batch: 110 ----
mean loss: 191.60
train mean loss: 195.36
epoch train time: 0:00:00.730184
elapsed time: 0:03:13.758192
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:44:05.328239
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.88
 ---- batch: 020 ----
mean loss: 199.91
 ---- batch: 030 ----
mean loss: 188.73
 ---- batch: 040 ----
mean loss: 194.50
 ---- batch: 050 ----
mean loss: 195.76
 ---- batch: 060 ----
mean loss: 186.63
 ---- batch: 070 ----
mean loss: 192.85
 ---- batch: 080 ----
mean loss: 201.97
 ---- batch: 090 ----
mean loss: 196.57
 ---- batch: 100 ----
mean loss: 197.93
 ---- batch: 110 ----
mean loss: 198.30
train mean loss: 195.39
epoch train time: 0:00:00.727430
elapsed time: 0:03:14.485805
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:44:06.055825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.07
 ---- batch: 020 ----
mean loss: 199.53
 ---- batch: 030 ----
mean loss: 198.22
 ---- batch: 040 ----
mean loss: 195.97
 ---- batch: 050 ----
mean loss: 202.60
 ---- batch: 060 ----
mean loss: 190.95
 ---- batch: 070 ----
mean loss: 195.79
 ---- batch: 080 ----
mean loss: 196.54
 ---- batch: 090 ----
mean loss: 194.58
 ---- batch: 100 ----
mean loss: 199.25
 ---- batch: 110 ----
mean loss: 191.59
train mean loss: 195.34
epoch train time: 0:00:00.730969
elapsed time: 0:03:15.216921
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:44:06.786948
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.64
 ---- batch: 020 ----
mean loss: 190.93
 ---- batch: 030 ----
mean loss: 199.65
 ---- batch: 040 ----
mean loss: 204.82
 ---- batch: 050 ----
mean loss: 188.91
 ---- batch: 060 ----
mean loss: 192.94
 ---- batch: 070 ----
mean loss: 201.75
 ---- batch: 080 ----
mean loss: 199.30
 ---- batch: 090 ----
mean loss: 189.95
 ---- batch: 100 ----
mean loss: 193.94
 ---- batch: 110 ----
mean loss: 198.83
train mean loss: 195.29
epoch train time: 0:00:00.730307
elapsed time: 0:03:15.947391
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:44:07.517451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.78
 ---- batch: 020 ----
mean loss: 200.90
 ---- batch: 030 ----
mean loss: 208.26
 ---- batch: 040 ----
mean loss: 189.75
 ---- batch: 050 ----
mean loss: 193.31
 ---- batch: 060 ----
mean loss: 192.47
 ---- batch: 070 ----
mean loss: 195.37
 ---- batch: 080 ----
mean loss: 189.62
 ---- batch: 090 ----
mean loss: 191.81
 ---- batch: 100 ----
mean loss: 198.84
 ---- batch: 110 ----
mean loss: 189.93
train mean loss: 195.37
epoch train time: 0:00:00.731509
elapsed time: 0:03:16.679084
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:44:08.249105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.60
 ---- batch: 020 ----
mean loss: 194.99
 ---- batch: 030 ----
mean loss: 190.45
 ---- batch: 040 ----
mean loss: 199.51
 ---- batch: 050 ----
mean loss: 202.44
 ---- batch: 060 ----
mean loss: 191.97
 ---- batch: 070 ----
mean loss: 192.55
 ---- batch: 080 ----
mean loss: 190.88
 ---- batch: 090 ----
mean loss: 199.23
 ---- batch: 100 ----
mean loss: 197.02
 ---- batch: 110 ----
mean loss: 193.74
train mean loss: 195.33
epoch train time: 0:00:00.733263
elapsed time: 0:03:17.412496
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:44:08.982519
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.13
 ---- batch: 020 ----
mean loss: 195.81
 ---- batch: 030 ----
mean loss: 194.87
 ---- batch: 040 ----
mean loss: 197.10
 ---- batch: 050 ----
mean loss: 193.82
 ---- batch: 060 ----
mean loss: 204.43
 ---- batch: 070 ----
mean loss: 193.79
 ---- batch: 080 ----
mean loss: 199.42
 ---- batch: 090 ----
mean loss: 198.27
 ---- batch: 100 ----
mean loss: 183.35
 ---- batch: 110 ----
mean loss: 192.62
train mean loss: 195.33
epoch train time: 0:00:00.751675
elapsed time: 0:03:18.164323
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:44:09.734341
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.06
 ---- batch: 020 ----
mean loss: 192.14
 ---- batch: 030 ----
mean loss: 198.98
 ---- batch: 040 ----
mean loss: 202.06
 ---- batch: 050 ----
mean loss: 184.15
 ---- batch: 060 ----
mean loss: 197.78
 ---- batch: 070 ----
mean loss: 188.54
 ---- batch: 080 ----
mean loss: 191.03
 ---- batch: 090 ----
mean loss: 200.45
 ---- batch: 100 ----
mean loss: 200.79
 ---- batch: 110 ----
mean loss: 193.55
train mean loss: 195.35
epoch train time: 0:00:00.750624
elapsed time: 0:03:18.915104
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:44:10.485125
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.98
 ---- batch: 020 ----
mean loss: 201.25
 ---- batch: 030 ----
mean loss: 196.89
 ---- batch: 040 ----
mean loss: 191.80
 ---- batch: 050 ----
mean loss: 197.07
 ---- batch: 060 ----
mean loss: 182.94
 ---- batch: 070 ----
mean loss: 200.73
 ---- batch: 080 ----
mean loss: 193.06
 ---- batch: 090 ----
mean loss: 203.74
 ---- batch: 100 ----
mean loss: 199.96
 ---- batch: 110 ----
mean loss: 194.64
train mean loss: 195.24
epoch train time: 0:00:00.760061
elapsed time: 0:03:19.675338
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:44:11.245354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.03
 ---- batch: 020 ----
mean loss: 196.41
 ---- batch: 030 ----
mean loss: 196.54
 ---- batch: 040 ----
mean loss: 191.33
 ---- batch: 050 ----
mean loss: 191.80
 ---- batch: 060 ----
mean loss: 198.03
 ---- batch: 070 ----
mean loss: 203.44
 ---- batch: 080 ----
mean loss: 200.01
 ---- batch: 090 ----
mean loss: 197.97
 ---- batch: 100 ----
mean loss: 196.95
 ---- batch: 110 ----
mean loss: 190.46
train mean loss: 195.25
epoch train time: 0:00:00.758095
elapsed time: 0:03:20.433577
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:44:12.003616
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.97
 ---- batch: 020 ----
mean loss: 190.43
 ---- batch: 030 ----
mean loss: 191.49
 ---- batch: 040 ----
mean loss: 198.46
 ---- batch: 050 ----
mean loss: 197.80
 ---- batch: 060 ----
mean loss: 198.63
 ---- batch: 070 ----
mean loss: 202.24
 ---- batch: 080 ----
mean loss: 196.35
 ---- batch: 090 ----
mean loss: 191.14
 ---- batch: 100 ----
mean loss: 191.91
 ---- batch: 110 ----
mean loss: 193.35
train mean loss: 195.29
epoch train time: 0:00:00.760119
elapsed time: 0:03:21.193859
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:44:12.763877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.53
 ---- batch: 020 ----
mean loss: 200.33
 ---- batch: 030 ----
mean loss: 204.62
 ---- batch: 040 ----
mean loss: 197.82
 ---- batch: 050 ----
mean loss: 193.91
 ---- batch: 060 ----
mean loss: 193.36
 ---- batch: 070 ----
mean loss: 192.70
 ---- batch: 080 ----
mean loss: 183.49
 ---- batch: 090 ----
mean loss: 199.67
 ---- batch: 100 ----
mean loss: 200.25
 ---- batch: 110 ----
mean loss: 194.82
train mean loss: 195.22
epoch train time: 0:00:00.763260
elapsed time: 0:03:21.957265
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:44:13.527282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.30
 ---- batch: 020 ----
mean loss: 189.76
 ---- batch: 030 ----
mean loss: 188.82
 ---- batch: 040 ----
mean loss: 209.03
 ---- batch: 050 ----
mean loss: 189.06
 ---- batch: 060 ----
mean loss: 192.69
 ---- batch: 070 ----
mean loss: 197.33
 ---- batch: 080 ----
mean loss: 197.60
 ---- batch: 090 ----
mean loss: 193.39
 ---- batch: 100 ----
mean loss: 187.78
 ---- batch: 110 ----
mean loss: 200.32
train mean loss: 195.31
epoch train time: 0:00:00.756764
elapsed time: 0:03:22.714169
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:44:14.284191
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.86
 ---- batch: 020 ----
mean loss: 182.28
 ---- batch: 030 ----
mean loss: 188.48
 ---- batch: 040 ----
mean loss: 198.49
 ---- batch: 050 ----
mean loss: 198.49
 ---- batch: 060 ----
mean loss: 187.82
 ---- batch: 070 ----
mean loss: 196.44
 ---- batch: 080 ----
mean loss: 207.87
 ---- batch: 090 ----
mean loss: 192.54
 ---- batch: 100 ----
mean loss: 204.75
 ---- batch: 110 ----
mean loss: 198.17
train mean loss: 195.19
epoch train time: 0:00:00.740228
elapsed time: 0:03:23.454558
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:44:15.024578
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.30
 ---- batch: 020 ----
mean loss: 191.18
 ---- batch: 030 ----
mean loss: 199.96
 ---- batch: 040 ----
mean loss: 198.35
 ---- batch: 050 ----
mean loss: 183.96
 ---- batch: 060 ----
mean loss: 200.64
 ---- batch: 070 ----
mean loss: 196.29
 ---- batch: 080 ----
mean loss: 200.29
 ---- batch: 090 ----
mean loss: 194.71
 ---- batch: 100 ----
mean loss: 192.50
 ---- batch: 110 ----
mean loss: 195.20
train mean loss: 195.13
epoch train time: 0:00:00.733460
elapsed time: 0:03:24.188191
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:44:15.758226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.68
 ---- batch: 020 ----
mean loss: 186.63
 ---- batch: 030 ----
mean loss: 191.78
 ---- batch: 040 ----
mean loss: 197.22
 ---- batch: 050 ----
mean loss: 195.30
 ---- batch: 060 ----
mean loss: 206.61
 ---- batch: 070 ----
mean loss: 197.84
 ---- batch: 080 ----
mean loss: 197.39
 ---- batch: 090 ----
mean loss: 190.83
 ---- batch: 100 ----
mean loss: 200.22
 ---- batch: 110 ----
mean loss: 192.24
train mean loss: 195.16
epoch train time: 0:00:00.733595
elapsed time: 0:03:24.921964
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:44:16.491980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.65
 ---- batch: 020 ----
mean loss: 193.67
 ---- batch: 030 ----
mean loss: 185.10
 ---- batch: 040 ----
mean loss: 197.39
 ---- batch: 050 ----
mean loss: 189.54
 ---- batch: 060 ----
mean loss: 199.08
 ---- batch: 070 ----
mean loss: 190.89
 ---- batch: 080 ----
mean loss: 204.44
 ---- batch: 090 ----
mean loss: 200.48
 ---- batch: 100 ----
mean loss: 196.31
 ---- batch: 110 ----
mean loss: 192.09
train mean loss: 195.15
epoch train time: 0:00:00.730204
elapsed time: 0:03:25.652310
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:44:17.222345
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.22
 ---- batch: 020 ----
mean loss: 187.64
 ---- batch: 030 ----
mean loss: 191.23
 ---- batch: 040 ----
mean loss: 204.69
 ---- batch: 050 ----
mean loss: 190.48
 ---- batch: 060 ----
mean loss: 201.86
 ---- batch: 070 ----
mean loss: 199.41
 ---- batch: 080 ----
mean loss: 187.32
 ---- batch: 090 ----
mean loss: 192.52
 ---- batch: 100 ----
mean loss: 200.36
 ---- batch: 110 ----
mean loss: 191.67
train mean loss: 195.14
epoch train time: 0:00:00.737126
elapsed time: 0:03:26.389595
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:44:17.959612
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.60
 ---- batch: 020 ----
mean loss: 196.50
 ---- batch: 030 ----
mean loss: 189.95
 ---- batch: 040 ----
mean loss: 200.93
 ---- batch: 050 ----
mean loss: 193.65
 ---- batch: 060 ----
mean loss: 208.32
 ---- batch: 070 ----
mean loss: 197.76
 ---- batch: 080 ----
mean loss: 189.37
 ---- batch: 090 ----
mean loss: 192.29
 ---- batch: 100 ----
mean loss: 190.55
 ---- batch: 110 ----
mean loss: 193.49
train mean loss: 195.17
epoch train time: 0:00:00.736704
elapsed time: 0:03:27.126452
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:44:18.696477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.24
 ---- batch: 020 ----
mean loss: 208.78
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 188.72
 ---- batch: 050 ----
mean loss: 196.83
 ---- batch: 060 ----
mean loss: 197.49
 ---- batch: 070 ----
mean loss: 185.65
 ---- batch: 080 ----
mean loss: 190.88
 ---- batch: 090 ----
mean loss: 200.88
 ---- batch: 100 ----
mean loss: 197.10
 ---- batch: 110 ----
mean loss: 188.32
train mean loss: 195.11
epoch train time: 0:00:00.736826
elapsed time: 0:03:27.863428
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:44:19.433450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.85
 ---- batch: 020 ----
mean loss: 200.97
 ---- batch: 030 ----
mean loss: 195.48
 ---- batch: 040 ----
mean loss: 194.79
 ---- batch: 050 ----
mean loss: 191.27
 ---- batch: 060 ----
mean loss: 197.35
 ---- batch: 070 ----
mean loss: 204.61
 ---- batch: 080 ----
mean loss: 189.92
 ---- batch: 090 ----
mean loss: 186.57
 ---- batch: 100 ----
mean loss: 201.57
 ---- batch: 110 ----
mean loss: 190.91
train mean loss: 195.09
epoch train time: 0:00:00.741553
elapsed time: 0:03:28.605126
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:44:20.175157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.36
 ---- batch: 020 ----
mean loss: 199.03
 ---- batch: 030 ----
mean loss: 197.13
 ---- batch: 040 ----
mean loss: 202.65
 ---- batch: 050 ----
mean loss: 189.14
 ---- batch: 060 ----
mean loss: 201.82
 ---- batch: 070 ----
mean loss: 195.38
 ---- batch: 080 ----
mean loss: 202.65
 ---- batch: 090 ----
mean loss: 189.86
 ---- batch: 100 ----
mean loss: 195.19
 ---- batch: 110 ----
mean loss: 187.07
train mean loss: 195.12
epoch train time: 0:00:00.732862
elapsed time: 0:03:29.338155
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:44:20.908179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.61
 ---- batch: 020 ----
mean loss: 192.48
 ---- batch: 030 ----
mean loss: 199.50
 ---- batch: 040 ----
mean loss: 185.78
 ---- batch: 050 ----
mean loss: 193.63
 ---- batch: 060 ----
mean loss: 198.81
 ---- batch: 070 ----
mean loss: 197.00
 ---- batch: 080 ----
mean loss: 194.17
 ---- batch: 090 ----
mean loss: 208.64
 ---- batch: 100 ----
mean loss: 193.90
 ---- batch: 110 ----
mean loss: 195.83
train mean loss: 195.12
epoch train time: 0:00:00.741925
elapsed time: 0:03:30.080232
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:44:21.650275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.30
 ---- batch: 020 ----
mean loss: 193.58
 ---- batch: 030 ----
mean loss: 191.59
 ---- batch: 040 ----
mean loss: 194.41
 ---- batch: 050 ----
mean loss: 198.46
 ---- batch: 060 ----
mean loss: 200.45
 ---- batch: 070 ----
mean loss: 198.35
 ---- batch: 080 ----
mean loss: 195.56
 ---- batch: 090 ----
mean loss: 199.36
 ---- batch: 100 ----
mean loss: 201.62
 ---- batch: 110 ----
mean loss: 187.70
train mean loss: 195.07
epoch train time: 0:00:00.728467
elapsed time: 0:03:30.808865
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:44:22.378882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.17
 ---- batch: 020 ----
mean loss: 210.71
 ---- batch: 030 ----
mean loss: 202.02
 ---- batch: 040 ----
mean loss: 186.76
 ---- batch: 050 ----
mean loss: 190.28
 ---- batch: 060 ----
mean loss: 195.81
 ---- batch: 070 ----
mean loss: 197.54
 ---- batch: 080 ----
mean loss: 180.12
 ---- batch: 090 ----
mean loss: 192.39
 ---- batch: 100 ----
mean loss: 200.75
 ---- batch: 110 ----
mean loss: 194.11
train mean loss: 195.04
epoch train time: 0:00:00.733836
elapsed time: 0:03:31.542842
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:44:23.112858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.59
 ---- batch: 020 ----
mean loss: 201.25
 ---- batch: 030 ----
mean loss: 191.20
 ---- batch: 040 ----
mean loss: 203.65
 ---- batch: 050 ----
mean loss: 198.56
 ---- batch: 060 ----
mean loss: 190.94
 ---- batch: 070 ----
mean loss: 201.68
 ---- batch: 080 ----
mean loss: 189.22
 ---- batch: 090 ----
mean loss: 185.83
 ---- batch: 100 ----
mean loss: 200.22
 ---- batch: 110 ----
mean loss: 191.92
train mean loss: 195.04
epoch train time: 0:00:00.733258
elapsed time: 0:03:32.276293
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:44:23.846361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.88
 ---- batch: 020 ----
mean loss: 194.60
 ---- batch: 030 ----
mean loss: 196.70
 ---- batch: 040 ----
mean loss: 195.18
 ---- batch: 050 ----
mean loss: 194.79
 ---- batch: 060 ----
mean loss: 189.84
 ---- batch: 070 ----
mean loss: 197.51
 ---- batch: 080 ----
mean loss: 201.80
 ---- batch: 090 ----
mean loss: 193.40
 ---- batch: 100 ----
mean loss: 196.49
 ---- batch: 110 ----
mean loss: 200.26
train mean loss: 194.99
epoch train time: 0:00:00.737641
elapsed time: 0:03:33.014124
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:44:24.584141
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.01
 ---- batch: 020 ----
mean loss: 192.09
 ---- batch: 030 ----
mean loss: 197.65
 ---- batch: 040 ----
mean loss: 194.00
 ---- batch: 050 ----
mean loss: 191.18
 ---- batch: 060 ----
mean loss: 197.44
 ---- batch: 070 ----
mean loss: 198.55
 ---- batch: 080 ----
mean loss: 191.64
 ---- batch: 090 ----
mean loss: 194.37
 ---- batch: 100 ----
mean loss: 199.93
 ---- batch: 110 ----
mean loss: 199.37
train mean loss: 195.02
epoch train time: 0:00:00.728563
elapsed time: 0:03:33.742826
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:44:25.312858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.60
 ---- batch: 020 ----
mean loss: 194.46
 ---- batch: 030 ----
mean loss: 194.45
 ---- batch: 040 ----
mean loss: 194.41
 ---- batch: 050 ----
mean loss: 198.14
 ---- batch: 060 ----
mean loss: 190.85
 ---- batch: 070 ----
mean loss: 197.19
 ---- batch: 080 ----
mean loss: 192.24
 ---- batch: 090 ----
mean loss: 193.34
 ---- batch: 100 ----
mean loss: 194.21
 ---- batch: 110 ----
mean loss: 195.15
train mean loss: 195.03
epoch train time: 0:00:00.724298
elapsed time: 0:03:34.467278
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:44:26.037295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.70
 ---- batch: 020 ----
mean loss: 201.59
 ---- batch: 030 ----
mean loss: 196.54
 ---- batch: 040 ----
mean loss: 198.12
 ---- batch: 050 ----
mean loss: 192.26
 ---- batch: 060 ----
mean loss: 196.88
 ---- batch: 070 ----
mean loss: 197.53
 ---- batch: 080 ----
mean loss: 188.04
 ---- batch: 090 ----
mean loss: 195.47
 ---- batch: 100 ----
mean loss: 200.98
 ---- batch: 110 ----
mean loss: 201.69
train mean loss: 194.96
epoch train time: 0:00:00.734975
elapsed time: 0:03:35.202393
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:44:26.772437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.95
 ---- batch: 020 ----
mean loss: 193.68
 ---- batch: 030 ----
mean loss: 194.02
 ---- batch: 040 ----
mean loss: 194.37
 ---- batch: 050 ----
mean loss: 192.46
 ---- batch: 060 ----
mean loss: 191.26
 ---- batch: 070 ----
mean loss: 190.26
 ---- batch: 080 ----
mean loss: 194.65
 ---- batch: 090 ----
mean loss: 198.02
 ---- batch: 100 ----
mean loss: 202.19
 ---- batch: 110 ----
mean loss: 197.75
train mean loss: 194.90
epoch train time: 0:00:00.733123
elapsed time: 0:03:35.935708
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:44:27.505740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.02
 ---- batch: 020 ----
mean loss: 197.16
 ---- batch: 030 ----
mean loss: 190.44
 ---- batch: 040 ----
mean loss: 195.13
 ---- batch: 050 ----
mean loss: 191.06
 ---- batch: 060 ----
mean loss: 201.05
 ---- batch: 070 ----
mean loss: 193.66
 ---- batch: 080 ----
mean loss: 195.67
 ---- batch: 090 ----
mean loss: 195.45
 ---- batch: 100 ----
mean loss: 199.10
 ---- batch: 110 ----
mean loss: 196.00
train mean loss: 194.93
epoch train time: 0:00:00.732327
elapsed time: 0:03:36.670416
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_4/checkpoint.pth.tar
**** end time: 2019-09-27 01:44:28.240400 ****
