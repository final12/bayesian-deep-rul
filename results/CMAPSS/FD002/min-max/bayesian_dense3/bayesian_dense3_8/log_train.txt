Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 20983
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 20:24:31.155640 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 20:24:31.165589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4076.23
 ---- batch: 020 ----
mean loss: 3799.48
 ---- batch: 030 ----
mean loss: 3645.05
 ---- batch: 040 ----
mean loss: 3395.02
 ---- batch: 050 ----
mean loss: 3152.33
 ---- batch: 060 ----
mean loss: 3070.48
 ---- batch: 070 ----
mean loss: 2877.94
 ---- batch: 080 ----
mean loss: 2797.13
 ---- batch: 090 ----
mean loss: 2694.95
train mean loss: 3235.30
epoch train time: 0:00:35.441082
elapsed time: 0:00:35.457858
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 20:25:06.613538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2481.11
 ---- batch: 020 ----
mean loss: 2472.12
 ---- batch: 030 ----
mean loss: 2387.10
 ---- batch: 040 ----
mean loss: 2316.80
 ---- batch: 050 ----
mean loss: 2209.98
 ---- batch: 060 ----
mean loss: 2197.21
 ---- batch: 070 ----
mean loss: 2142.39
 ---- batch: 080 ----
mean loss: 2082.16
 ---- batch: 090 ----
mean loss: 2062.28
train mean loss: 2246.04
epoch train time: 0:00:01.643110
elapsed time: 0:00:37.101316
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 20:25:08.257243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1979.74
 ---- batch: 020 ----
mean loss: 1928.47
 ---- batch: 030 ----
mean loss: 1924.10
 ---- batch: 040 ----
mean loss: 1904.26
 ---- batch: 050 ----
mean loss: 1878.97
 ---- batch: 060 ----
mean loss: 1840.84
 ---- batch: 070 ----
mean loss: 1841.94
 ---- batch: 080 ----
mean loss: 1790.28
 ---- batch: 090 ----
mean loss: 1752.27
train mean loss: 1865.26
epoch train time: 0:00:01.648244
elapsed time: 0:00:38.750278
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 20:25:09.906216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1738.19
 ---- batch: 020 ----
mean loss: 1680.17
 ---- batch: 030 ----
mean loss: 1658.97
 ---- batch: 040 ----
mean loss: 1678.36
 ---- batch: 050 ----
mean loss: 1636.82
 ---- batch: 060 ----
mean loss: 1632.79
 ---- batch: 070 ----
mean loss: 1589.36
 ---- batch: 080 ----
mean loss: 1579.02
 ---- batch: 090 ----
mean loss: 1577.48
train mean loss: 1636.25
epoch train time: 0:00:01.638484
elapsed time: 0:00:40.389399
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 20:25:11.545325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1521.07
 ---- batch: 020 ----
mean loss: 1517.28
 ---- batch: 030 ----
mean loss: 1486.58
 ---- batch: 040 ----
mean loss: 1469.39
 ---- batch: 050 ----
mean loss: 1480.98
 ---- batch: 060 ----
mean loss: 1434.96
 ---- batch: 070 ----
mean loss: 1441.66
 ---- batch: 080 ----
mean loss: 1445.52
 ---- batch: 090 ----
mean loss: 1400.29
train mean loss: 1461.15
epoch train time: 0:00:01.648404
elapsed time: 0:00:42.038396
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 20:25:13.194387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1362.49
 ---- batch: 020 ----
mean loss: 1351.09
 ---- batch: 030 ----
mean loss: 1370.06
 ---- batch: 040 ----
mean loss: 1329.67
 ---- batch: 050 ----
mean loss: 1365.81
 ---- batch: 060 ----
mean loss: 1294.77
 ---- batch: 070 ----
mean loss: 1317.42
 ---- batch: 080 ----
mean loss: 1318.54
 ---- batch: 090 ----
mean loss: 1270.67
train mean loss: 1326.27
epoch train time: 0:00:01.630097
elapsed time: 0:00:43.669212
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 20:25:14.825153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1258.29
 ---- batch: 020 ----
mean loss: 1263.41
 ---- batch: 030 ----
mean loss: 1217.34
 ---- batch: 040 ----
mean loss: 1242.47
 ---- batch: 050 ----
mean loss: 1228.27
 ---- batch: 060 ----
mean loss: 1222.35
 ---- batch: 070 ----
mean loss: 1196.64
 ---- batch: 080 ----
mean loss: 1197.43
 ---- batch: 090 ----
mean loss: 1194.79
train mean loss: 1219.78
epoch train time: 0:00:01.618687
elapsed time: 0:00:45.288572
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 20:25:16.444507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1148.75
 ---- batch: 020 ----
mean loss: 1165.80
 ---- batch: 030 ----
mean loss: 1177.64
 ---- batch: 040 ----
mean loss: 1125.32
 ---- batch: 050 ----
mean loss: 1141.32
 ---- batch: 060 ----
mean loss: 1129.04
 ---- batch: 070 ----
mean loss: 1111.15
 ---- batch: 080 ----
mean loss: 1119.61
 ---- batch: 090 ----
mean loss: 1093.51
train mean loss: 1131.47
epoch train time: 0:00:01.638524
elapsed time: 0:00:46.927774
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 20:25:18.083717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1099.45
 ---- batch: 020 ----
mean loss: 1066.38
 ---- batch: 030 ----
mean loss: 1072.44
 ---- batch: 040 ----
mean loss: 1079.41
 ---- batch: 050 ----
mean loss: 1087.20
 ---- batch: 060 ----
mean loss: 1052.44
 ---- batch: 070 ----
mean loss: 1062.65
 ---- batch: 080 ----
mean loss: 1033.68
 ---- batch: 090 ----
mean loss: 1059.67
train mean loss: 1067.43
epoch train time: 0:00:01.660725
elapsed time: 0:00:48.589172
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 20:25:19.745115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1044.00
 ---- batch: 020 ----
mean loss: 1026.56
 ---- batch: 030 ----
mean loss: 1021.05
 ---- batch: 040 ----
mean loss: 1001.42
 ---- batch: 050 ----
mean loss: 1027.30
 ---- batch: 060 ----
mean loss: 1015.90
 ---- batch: 070 ----
mean loss: 1018.83
 ---- batch: 080 ----
mean loss: 991.36
 ---- batch: 090 ----
mean loss: 1002.68
train mean loss: 1013.85
epoch train time: 0:00:01.631692
elapsed time: 0:00:50.221467
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 20:25:21.377402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.38
 ---- batch: 020 ----
mean loss: 996.83
 ---- batch: 030 ----
mean loss: 990.93
 ---- batch: 040 ----
mean loss: 965.39
 ---- batch: 050 ----
mean loss: 965.00
 ---- batch: 060 ----
mean loss: 979.02
 ---- batch: 070 ----
mean loss: 972.00
 ---- batch: 080 ----
mean loss: 944.58
 ---- batch: 090 ----
mean loss: 969.42
train mean loss: 971.55
epoch train time: 0:00:01.646501
elapsed time: 0:00:51.868599
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 20:25:23.024538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.69
 ---- batch: 020 ----
mean loss: 949.60
 ---- batch: 030 ----
mean loss: 956.41
 ---- batch: 040 ----
mean loss: 959.99
 ---- batch: 050 ----
mean loss: 956.26
 ---- batch: 060 ----
mean loss: 947.52
 ---- batch: 070 ----
mean loss: 952.49
 ---- batch: 080 ----
mean loss: 924.99
 ---- batch: 090 ----
mean loss: 918.83
train mean loss: 942.43
epoch train time: 0:00:01.629830
elapsed time: 0:00:53.499082
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 20:25:24.654914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.62
 ---- batch: 020 ----
mean loss: 923.91
 ---- batch: 030 ----
mean loss: 934.52
 ---- batch: 040 ----
mean loss: 918.26
 ---- batch: 050 ----
mean loss: 937.85
 ---- batch: 060 ----
mean loss: 925.21
 ---- batch: 070 ----
mean loss: 906.57
 ---- batch: 080 ----
mean loss: 912.21
 ---- batch: 090 ----
mean loss: 920.07
train mean loss: 921.99
epoch train time: 0:00:01.650086
elapsed time: 0:00:55.149633
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 20:25:26.305624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.09
 ---- batch: 020 ----
mean loss: 911.76
 ---- batch: 030 ----
mean loss: 909.60
 ---- batch: 040 ----
mean loss: 887.43
 ---- batch: 050 ----
mean loss: 915.26
 ---- batch: 060 ----
mean loss: 910.24
 ---- batch: 070 ----
mean loss: 924.06
 ---- batch: 080 ----
mean loss: 908.97
 ---- batch: 090 ----
mean loss: 908.99
train mean loss: 909.83
epoch train time: 0:00:01.631429
elapsed time: 0:00:56.781742
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 20:25:27.937752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.02
 ---- batch: 020 ----
mean loss: 902.91
 ---- batch: 030 ----
mean loss: 899.05
 ---- batch: 040 ----
mean loss: 889.05
 ---- batch: 050 ----
mean loss: 892.53
 ---- batch: 060 ----
mean loss: 888.28
 ---- batch: 070 ----
mean loss: 894.89
 ---- batch: 080 ----
mean loss: 912.72
 ---- batch: 090 ----
mean loss: 900.46
train mean loss: 900.08
epoch train time: 0:00:01.633547
elapsed time: 0:00:58.415974
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 20:25:29.571919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.51
 ---- batch: 020 ----
mean loss: 899.73
 ---- batch: 030 ----
mean loss: 892.11
 ---- batch: 040 ----
mean loss: 904.27
 ---- batch: 050 ----
mean loss: 894.72
 ---- batch: 060 ----
mean loss: 888.77
 ---- batch: 070 ----
mean loss: 869.95
 ---- batch: 080 ----
mean loss: 894.06
 ---- batch: 090 ----
mean loss: 897.72
train mean loss: 893.67
epoch train time: 0:00:01.654790
elapsed time: 0:01:00.071407
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 20:25:31.227381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.95
 ---- batch: 020 ----
mean loss: 872.30
 ---- batch: 030 ----
mean loss: 889.80
 ---- batch: 040 ----
mean loss: 895.97
 ---- batch: 050 ----
mean loss: 869.28
 ---- batch: 060 ----
mean loss: 896.86
 ---- batch: 070 ----
mean loss: 905.16
 ---- batch: 080 ----
mean loss: 907.40
 ---- batch: 090 ----
mean loss: 877.98
train mean loss: 889.98
epoch train time: 0:00:01.629232
elapsed time: 0:01:01.701354
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 20:25:32.857301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.57
 ---- batch: 020 ----
mean loss: 883.34
 ---- batch: 030 ----
mean loss: 899.83
 ---- batch: 040 ----
mean loss: 893.23
 ---- batch: 050 ----
mean loss: 876.88
 ---- batch: 060 ----
mean loss: 875.72
 ---- batch: 070 ----
mean loss: 888.22
 ---- batch: 080 ----
mean loss: 880.02
 ---- batch: 090 ----
mean loss: 876.48
train mean loss: 886.81
epoch train time: 0:00:01.648875
elapsed time: 0:01:03.350952
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 20:25:34.506929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.59
 ---- batch: 020 ----
mean loss: 893.55
 ---- batch: 030 ----
mean loss: 870.09
 ---- batch: 040 ----
mean loss: 889.35
 ---- batch: 050 ----
mean loss: 887.04
 ---- batch: 060 ----
mean loss: 878.24
 ---- batch: 070 ----
mean loss: 869.22
 ---- batch: 080 ----
mean loss: 900.73
 ---- batch: 090 ----
mean loss: 891.47
train mean loss: 884.76
epoch train time: 0:00:01.615597
elapsed time: 0:01:04.967172
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 20:25:36.123105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.13
 ---- batch: 020 ----
mean loss: 897.13
 ---- batch: 030 ----
mean loss: 882.24
 ---- batch: 040 ----
mean loss: 889.59
 ---- batch: 050 ----
mean loss: 875.13
 ---- batch: 060 ----
mean loss: 881.57
 ---- batch: 070 ----
mean loss: 895.99
 ---- batch: 080 ----
mean loss: 872.38
 ---- batch: 090 ----
mean loss: 878.49
train mean loss: 883.89
epoch train time: 0:00:01.617347
elapsed time: 0:01:06.585090
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 20:25:37.741084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.88
 ---- batch: 020 ----
mean loss: 873.65
 ---- batch: 030 ----
mean loss: 877.65
 ---- batch: 040 ----
mean loss: 880.17
 ---- batch: 050 ----
mean loss: 901.34
 ---- batch: 060 ----
mean loss: 877.16
 ---- batch: 070 ----
mean loss: 865.61
 ---- batch: 080 ----
mean loss: 898.27
 ---- batch: 090 ----
mean loss: 884.10
train mean loss: 881.69
epoch train time: 0:00:01.626742
elapsed time: 0:01:08.212452
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 20:25:39.368377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.89
 ---- batch: 020 ----
mean loss: 893.18
 ---- batch: 030 ----
mean loss: 872.61
 ---- batch: 040 ----
mean loss: 863.20
 ---- batch: 050 ----
mean loss: 876.45
 ---- batch: 060 ----
mean loss: 897.41
 ---- batch: 070 ----
mean loss: 890.28
 ---- batch: 080 ----
mean loss: 877.99
 ---- batch: 090 ----
mean loss: 887.32
train mean loss: 882.36
epoch train time: 0:00:01.636978
elapsed time: 0:01:09.850072
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 20:25:41.005853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.61
 ---- batch: 020 ----
mean loss: 875.09
 ---- batch: 030 ----
mean loss: 871.69
 ---- batch: 040 ----
mean loss: 874.45
 ---- batch: 050 ----
mean loss: 885.29
 ---- batch: 060 ----
mean loss: 887.83
 ---- batch: 070 ----
mean loss: 884.46
 ---- batch: 080 ----
mean loss: 883.15
 ---- batch: 090 ----
mean loss: 890.38
train mean loss: 882.13
epoch train time: 0:00:01.615877
elapsed time: 0:01:11.466499
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 20:25:42.622479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.24
 ---- batch: 020 ----
mean loss: 876.88
 ---- batch: 030 ----
mean loss: 885.09
 ---- batch: 040 ----
mean loss: 863.16
 ---- batch: 050 ----
mean loss: 877.75
 ---- batch: 060 ----
mean loss: 871.31
 ---- batch: 070 ----
mean loss: 886.61
 ---- batch: 080 ----
mean loss: 885.48
 ---- batch: 090 ----
mean loss: 879.17
train mean loss: 881.46
epoch train time: 0:00:01.612720
elapsed time: 0:01:13.079923
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 20:25:44.235881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.18
 ---- batch: 020 ----
mean loss: 893.62
 ---- batch: 030 ----
mean loss: 888.76
 ---- batch: 040 ----
mean loss: 882.26
 ---- batch: 050 ----
mean loss: 886.30
 ---- batch: 060 ----
mean loss: 882.09
 ---- batch: 070 ----
mean loss: 879.09
 ---- batch: 080 ----
mean loss: 873.08
 ---- batch: 090 ----
mean loss: 892.08
train mean loss: 880.79
epoch train time: 0:00:01.648411
elapsed time: 0:01:14.728977
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 20:25:45.884947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.50
 ---- batch: 020 ----
mean loss: 874.03
 ---- batch: 030 ----
mean loss: 870.94
 ---- batch: 040 ----
mean loss: 875.96
 ---- batch: 050 ----
mean loss: 868.67
 ---- batch: 060 ----
mean loss: 909.99
 ---- batch: 070 ----
mean loss: 889.05
 ---- batch: 080 ----
mean loss: 882.69
 ---- batch: 090 ----
mean loss: 875.38
train mean loss: 880.80
epoch train time: 0:00:01.620914
elapsed time: 0:01:16.350514
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 20:25:47.506452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.91
 ---- batch: 020 ----
mean loss: 874.74
 ---- batch: 030 ----
mean loss: 874.90
 ---- batch: 040 ----
mean loss: 875.10
 ---- batch: 050 ----
mean loss: 866.89
 ---- batch: 060 ----
mean loss: 877.48
 ---- batch: 070 ----
mean loss: 884.51
 ---- batch: 080 ----
mean loss: 892.36
 ---- batch: 090 ----
mean loss: 896.02
train mean loss: 879.77
epoch train time: 0:00:01.669294
elapsed time: 0:01:18.020392
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 20:25:49.176341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.47
 ---- batch: 020 ----
mean loss: 875.03
 ---- batch: 030 ----
mean loss: 883.42
 ---- batch: 040 ----
mean loss: 891.66
 ---- batch: 050 ----
mean loss: 876.11
 ---- batch: 060 ----
mean loss: 877.35
 ---- batch: 070 ----
mean loss: 867.33
 ---- batch: 080 ----
mean loss: 895.73
 ---- batch: 090 ----
mean loss: 867.41
train mean loss: 879.39
epoch train time: 0:00:01.628727
elapsed time: 0:01:19.649818
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 20:25:50.805810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.19
 ---- batch: 020 ----
mean loss: 878.62
 ---- batch: 030 ----
mean loss: 867.67
 ---- batch: 040 ----
mean loss: 881.51
 ---- batch: 050 ----
mean loss: 888.21
 ---- batch: 060 ----
mean loss: 888.44
 ---- batch: 070 ----
mean loss: 899.59
 ---- batch: 080 ----
mean loss: 867.26
 ---- batch: 090 ----
mean loss: 867.48
train mean loss: 881.10
epoch train time: 0:00:01.588313
elapsed time: 0:01:21.238742
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 20:25:52.394666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.97
 ---- batch: 020 ----
mean loss: 889.53
 ---- batch: 030 ----
mean loss: 878.08
 ---- batch: 040 ----
mean loss: 881.23
 ---- batch: 050 ----
mean loss: 878.84
 ---- batch: 060 ----
mean loss: 884.12
 ---- batch: 070 ----
mean loss: 875.35
 ---- batch: 080 ----
mean loss: 877.10
 ---- batch: 090 ----
mean loss: 861.63
train mean loss: 879.84
epoch train time: 0:00:01.642335
elapsed time: 0:01:22.881719
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 20:25:54.037694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.02
 ---- batch: 020 ----
mean loss: 875.40
 ---- batch: 030 ----
mean loss: 873.47
 ---- batch: 040 ----
mean loss: 882.92
 ---- batch: 050 ----
mean loss: 894.44
 ---- batch: 060 ----
mean loss: 869.99
 ---- batch: 070 ----
mean loss: 897.32
 ---- batch: 080 ----
mean loss: 872.73
 ---- batch: 090 ----
mean loss: 863.24
train mean loss: 878.74
epoch train time: 0:00:01.634394
elapsed time: 0:01:24.516848
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 20:25:55.672852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.02
 ---- batch: 020 ----
mean loss: 884.11
 ---- batch: 030 ----
mean loss: 876.77
 ---- batch: 040 ----
mean loss: 873.72
 ---- batch: 050 ----
mean loss: 879.72
 ---- batch: 060 ----
mean loss: 890.08
 ---- batch: 070 ----
mean loss: 865.15
 ---- batch: 080 ----
mean loss: 887.21
 ---- batch: 090 ----
mean loss: 871.66
train mean loss: 878.53
epoch train time: 0:00:01.657961
elapsed time: 0:01:26.175475
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 20:25:57.331429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.13
 ---- batch: 020 ----
mean loss: 879.41
 ---- batch: 030 ----
mean loss: 880.83
 ---- batch: 040 ----
mean loss: 874.65
 ---- batch: 050 ----
mean loss: 874.81
 ---- batch: 060 ----
mean loss: 872.32
 ---- batch: 070 ----
mean loss: 868.23
 ---- batch: 080 ----
mean loss: 882.01
 ---- batch: 090 ----
mean loss: 882.12
train mean loss: 879.75
epoch train time: 0:00:01.693888
elapsed time: 0:01:27.870093
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 20:25:59.026052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.94
 ---- batch: 020 ----
mean loss: 885.89
 ---- batch: 030 ----
mean loss: 867.92
 ---- batch: 040 ----
mean loss: 882.72
 ---- batch: 050 ----
mean loss: 893.15
 ---- batch: 060 ----
mean loss: 885.00
 ---- batch: 070 ----
mean loss: 887.63
 ---- batch: 080 ----
mean loss: 862.83
 ---- batch: 090 ----
mean loss: 875.99
train mean loss: 878.52
epoch train time: 0:00:01.662276
elapsed time: 0:01:29.533018
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 20:26:00.688981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.79
 ---- batch: 020 ----
mean loss: 872.08
 ---- batch: 030 ----
mean loss: 860.51
 ---- batch: 040 ----
mean loss: 883.79
 ---- batch: 050 ----
mean loss: 873.17
 ---- batch: 060 ----
mean loss: 893.36
 ---- batch: 070 ----
mean loss: 882.26
 ---- batch: 080 ----
mean loss: 882.48
 ---- batch: 090 ----
mean loss: 891.54
train mean loss: 879.24
epoch train time: 0:00:01.636268
elapsed time: 0:01:31.169959
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 20:26:02.325960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.58
 ---- batch: 020 ----
mean loss: 871.77
 ---- batch: 030 ----
mean loss: 877.25
 ---- batch: 040 ----
mean loss: 890.63
 ---- batch: 050 ----
mean loss: 891.60
 ---- batch: 060 ----
mean loss: 863.61
 ---- batch: 070 ----
mean loss: 866.69
 ---- batch: 080 ----
mean loss: 867.88
 ---- batch: 090 ----
mean loss: 912.63
train mean loss: 879.12
epoch train time: 0:00:01.650905
elapsed time: 0:01:32.821589
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 20:26:03.977536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.74
 ---- batch: 020 ----
mean loss: 871.55
 ---- batch: 030 ----
mean loss: 883.90
 ---- batch: 040 ----
mean loss: 900.38
 ---- batch: 050 ----
mean loss: 898.60
 ---- batch: 060 ----
mean loss: 876.90
 ---- batch: 070 ----
mean loss: 874.79
 ---- batch: 080 ----
mean loss: 854.50
 ---- batch: 090 ----
mean loss: 871.75
train mean loss: 878.28
epoch train time: 0:00:01.593939
elapsed time: 0:01:34.416134
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 20:26:05.572093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.92
 ---- batch: 020 ----
mean loss: 886.36
 ---- batch: 030 ----
mean loss: 880.94
 ---- batch: 040 ----
mean loss: 885.60
 ---- batch: 050 ----
mean loss: 866.46
 ---- batch: 060 ----
mean loss: 885.49
 ---- batch: 070 ----
mean loss: 874.43
 ---- batch: 080 ----
mean loss: 884.92
 ---- batch: 090 ----
mean loss: 859.85
train mean loss: 878.19
epoch train time: 0:00:01.625916
elapsed time: 0:01:36.042702
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 20:26:07.198719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.83
 ---- batch: 020 ----
mean loss: 877.13
 ---- batch: 030 ----
mean loss: 875.13
 ---- batch: 040 ----
mean loss: 872.78
 ---- batch: 050 ----
mean loss: 861.81
 ---- batch: 060 ----
mean loss: 886.03
 ---- batch: 070 ----
mean loss: 884.97
 ---- batch: 080 ----
mean loss: 870.38
 ---- batch: 090 ----
mean loss: 887.97
train mean loss: 878.43
epoch train time: 0:00:01.606870
elapsed time: 0:01:37.650303
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 20:26:08.806279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.03
 ---- batch: 020 ----
mean loss: 866.56
 ---- batch: 030 ----
mean loss: 870.49
 ---- batch: 040 ----
mean loss: 875.27
 ---- batch: 050 ----
mean loss: 877.17
 ---- batch: 060 ----
mean loss: 877.87
 ---- batch: 070 ----
mean loss: 879.34
 ---- batch: 080 ----
mean loss: 874.91
 ---- batch: 090 ----
mean loss: 884.06
train mean loss: 877.92
epoch train time: 0:00:01.624966
elapsed time: 0:01:39.275919
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 20:26:10.431848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.63
 ---- batch: 020 ----
mean loss: 892.42
 ---- batch: 030 ----
mean loss: 878.70
 ---- batch: 040 ----
mean loss: 867.25
 ---- batch: 050 ----
mean loss: 890.15
 ---- batch: 060 ----
mean loss: 881.36
 ---- batch: 070 ----
mean loss: 874.82
 ---- batch: 080 ----
mean loss: 865.47
 ---- batch: 090 ----
mean loss: 872.11
train mean loss: 877.63
epoch train time: 0:00:01.584935
elapsed time: 0:01:40.861503
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 20:26:12.017466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.22
 ---- batch: 020 ----
mean loss: 880.88
 ---- batch: 030 ----
mean loss: 878.67
 ---- batch: 040 ----
mean loss: 871.51
 ---- batch: 050 ----
mean loss: 861.63
 ---- batch: 060 ----
mean loss: 885.49
 ---- batch: 070 ----
mean loss: 878.09
 ---- batch: 080 ----
mean loss: 878.19
 ---- batch: 090 ----
mean loss: 883.65
train mean loss: 876.97
epoch train time: 0:00:01.618070
elapsed time: 0:01:42.480189
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 20:26:13.636158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.06
 ---- batch: 020 ----
mean loss: 863.00
 ---- batch: 030 ----
mean loss: 879.74
 ---- batch: 040 ----
mean loss: 852.19
 ---- batch: 050 ----
mean loss: 855.85
 ---- batch: 060 ----
mean loss: 881.07
 ---- batch: 070 ----
mean loss: 891.19
 ---- batch: 080 ----
mean loss: 896.04
 ---- batch: 090 ----
mean loss: 902.72
train mean loss: 878.33
epoch train time: 0:00:01.627832
elapsed time: 0:01:44.108672
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 20:26:15.264632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.22
 ---- batch: 020 ----
mean loss: 862.10
 ---- batch: 030 ----
mean loss: 882.71
 ---- batch: 040 ----
mean loss: 892.47
 ---- batch: 050 ----
mean loss: 868.53
 ---- batch: 060 ----
mean loss: 881.81
 ---- batch: 070 ----
mean loss: 873.26
 ---- batch: 080 ----
mean loss: 886.78
 ---- batch: 090 ----
mean loss: 886.29
train mean loss: 877.04
epoch train time: 0:00:01.629702
elapsed time: 0:01:45.739021
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 20:26:16.894798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.66
 ---- batch: 020 ----
mean loss: 898.35
 ---- batch: 030 ----
mean loss: 897.13
 ---- batch: 040 ----
mean loss: 879.03
 ---- batch: 050 ----
mean loss: 855.48
 ---- batch: 060 ----
mean loss: 888.96
 ---- batch: 070 ----
mean loss: 874.68
 ---- batch: 080 ----
mean loss: 877.19
 ---- batch: 090 ----
mean loss: 870.23
train mean loss: 877.69
epoch train time: 0:00:01.609503
elapsed time: 0:01:47.348943
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 20:26:18.504887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.22
 ---- batch: 020 ----
mean loss: 873.34
 ---- batch: 030 ----
mean loss: 878.91
 ---- batch: 040 ----
mean loss: 878.75
 ---- batch: 050 ----
mean loss: 879.13
 ---- batch: 060 ----
mean loss: 869.42
 ---- batch: 070 ----
mean loss: 882.52
 ---- batch: 080 ----
mean loss: 878.91
 ---- batch: 090 ----
mean loss: 866.00
train mean loss: 876.84
epoch train time: 0:00:01.621095
elapsed time: 0:01:48.970654
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 20:26:20.126599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.27
 ---- batch: 020 ----
mean loss: 874.96
 ---- batch: 030 ----
mean loss: 887.59
 ---- batch: 040 ----
mean loss: 873.93
 ---- batch: 050 ----
mean loss: 886.67
 ---- batch: 060 ----
mean loss: 880.86
 ---- batch: 070 ----
mean loss: 895.52
 ---- batch: 080 ----
mean loss: 868.21
 ---- batch: 090 ----
mean loss: 873.91
train mean loss: 877.24
epoch train time: 0:00:01.627046
elapsed time: 0:01:50.598341
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 20:26:21.754314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.59
 ---- batch: 020 ----
mean loss: 870.15
 ---- batch: 030 ----
mean loss: 870.80
 ---- batch: 040 ----
mean loss: 883.02
 ---- batch: 050 ----
mean loss: 875.44
 ---- batch: 060 ----
mean loss: 879.84
 ---- batch: 070 ----
mean loss: 874.75
 ---- batch: 080 ----
mean loss: 877.30
 ---- batch: 090 ----
mean loss: 897.81
train mean loss: 876.61
epoch train time: 0:00:01.638557
elapsed time: 0:01:52.237507
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 20:26:23.393453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.23
 ---- batch: 020 ----
mean loss: 876.26
 ---- batch: 030 ----
mean loss: 842.43
 ---- batch: 040 ----
mean loss: 871.68
 ---- batch: 050 ----
mean loss: 881.45
 ---- batch: 060 ----
mean loss: 872.10
 ---- batch: 070 ----
mean loss: 895.64
 ---- batch: 080 ----
mean loss: 876.67
 ---- batch: 090 ----
mean loss: 902.22
train mean loss: 876.23
epoch train time: 0:00:01.657957
elapsed time: 0:01:53.896106
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 20:26:25.052057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.91
 ---- batch: 020 ----
mean loss: 888.38
 ---- batch: 030 ----
mean loss: 871.50
 ---- batch: 040 ----
mean loss: 883.52
 ---- batch: 050 ----
mean loss: 867.13
 ---- batch: 060 ----
mean loss: 870.26
 ---- batch: 070 ----
mean loss: 866.60
 ---- batch: 080 ----
mean loss: 887.75
 ---- batch: 090 ----
mean loss: 881.48
train mean loss: 876.99
epoch train time: 0:00:01.640690
elapsed time: 0:01:55.537476
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 20:26:26.693457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.63
 ---- batch: 020 ----
mean loss: 894.96
 ---- batch: 030 ----
mean loss: 872.66
 ---- batch: 040 ----
mean loss: 885.68
 ---- batch: 050 ----
mean loss: 859.18
 ---- batch: 060 ----
mean loss: 860.19
 ---- batch: 070 ----
mean loss: 847.08
 ---- batch: 080 ----
mean loss: 822.08
 ---- batch: 090 ----
mean loss: 802.71
train mean loss: 851.29
epoch train time: 0:00:01.645182
elapsed time: 0:01:57.183385
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 20:26:28.339471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.19
 ---- batch: 020 ----
mean loss: 618.76
 ---- batch: 030 ----
mean loss: 570.01
 ---- batch: 040 ----
mean loss: 523.11
 ---- batch: 050 ----
mean loss: 479.75
 ---- batch: 060 ----
mean loss: 474.44
 ---- batch: 070 ----
mean loss: 467.01
 ---- batch: 080 ----
mean loss: 457.03
 ---- batch: 090 ----
mean loss: 440.93
train mean loss: 519.72
epoch train time: 0:00:01.680113
elapsed time: 0:01:58.864316
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 20:26:30.020279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.05
 ---- batch: 020 ----
mean loss: 417.58
 ---- batch: 030 ----
mean loss: 411.86
 ---- batch: 040 ----
mean loss: 396.33
 ---- batch: 050 ----
mean loss: 397.45
 ---- batch: 060 ----
mean loss: 405.75
 ---- batch: 070 ----
mean loss: 372.32
 ---- batch: 080 ----
mean loss: 379.99
 ---- batch: 090 ----
mean loss: 376.35
train mean loss: 396.78
epoch train time: 0:00:01.655728
elapsed time: 0:02:00.520711
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 20:26:31.676650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.65
 ---- batch: 020 ----
mean loss: 358.01
 ---- batch: 030 ----
mean loss: 377.02
 ---- batch: 040 ----
mean loss: 354.01
 ---- batch: 050 ----
mean loss: 357.22
 ---- batch: 060 ----
mean loss: 343.22
 ---- batch: 070 ----
mean loss: 332.86
 ---- batch: 080 ----
mean loss: 350.97
 ---- batch: 090 ----
mean loss: 329.17
train mean loss: 351.30
epoch train time: 0:00:01.638728
elapsed time: 0:02:02.160064
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 20:26:33.316043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.21
 ---- batch: 020 ----
mean loss: 335.09
 ---- batch: 030 ----
mean loss: 343.25
 ---- batch: 040 ----
mean loss: 326.75
 ---- batch: 050 ----
mean loss: 317.61
 ---- batch: 060 ----
mean loss: 332.45
 ---- batch: 070 ----
mean loss: 331.24
 ---- batch: 080 ----
mean loss: 314.49
 ---- batch: 090 ----
mean loss: 317.78
train mean loss: 327.56
epoch train time: 0:00:01.635435
elapsed time: 0:02:03.796159
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 20:26:34.952133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.55
 ---- batch: 020 ----
mean loss: 311.23
 ---- batch: 030 ----
mean loss: 304.76
 ---- batch: 040 ----
mean loss: 319.82
 ---- batch: 050 ----
mean loss: 310.02
 ---- batch: 060 ----
mean loss: 313.97
 ---- batch: 070 ----
mean loss: 316.47
 ---- batch: 080 ----
mean loss: 290.38
 ---- batch: 090 ----
mean loss: 301.56
train mean loss: 309.90
epoch train time: 0:00:01.621058
elapsed time: 0:02:05.417856
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 20:26:36.573814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.73
 ---- batch: 020 ----
mean loss: 296.60
 ---- batch: 030 ----
mean loss: 302.10
 ---- batch: 040 ----
mean loss: 300.68
 ---- batch: 050 ----
mean loss: 297.06
 ---- batch: 060 ----
mean loss: 290.97
 ---- batch: 070 ----
mean loss: 295.57
 ---- batch: 080 ----
mean loss: 297.40
 ---- batch: 090 ----
mean loss: 292.86
train mean loss: 296.95
epoch train time: 0:00:01.645851
elapsed time: 0:02:07.064304
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 20:26:38.220252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.99
 ---- batch: 020 ----
mean loss: 290.39
 ---- batch: 030 ----
mean loss: 303.28
 ---- batch: 040 ----
mean loss: 281.38
 ---- batch: 050 ----
mean loss: 298.20
 ---- batch: 060 ----
mean loss: 292.33
 ---- batch: 070 ----
mean loss: 281.71
 ---- batch: 080 ----
mean loss: 288.46
 ---- batch: 090 ----
mean loss: 288.69
train mean loss: 289.95
epoch train time: 0:00:01.615593
elapsed time: 0:02:08.680545
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 20:26:39.836509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.41
 ---- batch: 020 ----
mean loss: 276.80
 ---- batch: 030 ----
mean loss: 281.43
 ---- batch: 040 ----
mean loss: 282.73
 ---- batch: 050 ----
mean loss: 287.85
 ---- batch: 060 ----
mean loss: 275.68
 ---- batch: 070 ----
mean loss: 275.51
 ---- batch: 080 ----
mean loss: 282.84
 ---- batch: 090 ----
mean loss: 277.88
train mean loss: 280.82
epoch train time: 0:00:01.605602
elapsed time: 0:02:10.286791
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 20:26:41.442725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.22
 ---- batch: 020 ----
mean loss: 285.54
 ---- batch: 030 ----
mean loss: 268.29
 ---- batch: 040 ----
mean loss: 275.16
 ---- batch: 050 ----
mean loss: 272.70
 ---- batch: 060 ----
mean loss: 280.94
 ---- batch: 070 ----
mean loss: 274.75
 ---- batch: 080 ----
mean loss: 269.33
 ---- batch: 090 ----
mean loss: 270.07
train mean loss: 275.25
epoch train time: 0:00:01.637120
elapsed time: 0:02:11.924509
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 20:26:43.080456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.43
 ---- batch: 020 ----
mean loss: 270.36
 ---- batch: 030 ----
mean loss: 263.98
 ---- batch: 040 ----
mean loss: 269.56
 ---- batch: 050 ----
mean loss: 257.57
 ---- batch: 060 ----
mean loss: 265.05
 ---- batch: 070 ----
mean loss: 272.22
 ---- batch: 080 ----
mean loss: 276.03
 ---- batch: 090 ----
mean loss: 264.38
train mean loss: 267.58
epoch train time: 0:00:01.655719
elapsed time: 0:02:13.580907
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 20:26:44.736873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.60
 ---- batch: 020 ----
mean loss: 255.02
 ---- batch: 030 ----
mean loss: 259.92
 ---- batch: 040 ----
mean loss: 254.76
 ---- batch: 050 ----
mean loss: 265.49
 ---- batch: 060 ----
mean loss: 272.36
 ---- batch: 070 ----
mean loss: 258.25
 ---- batch: 080 ----
mean loss: 260.29
 ---- batch: 090 ----
mean loss: 275.72
train mean loss: 262.63
epoch train time: 0:00:01.632858
elapsed time: 0:02:15.214368
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 20:26:46.370300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.95
 ---- batch: 020 ----
mean loss: 252.18
 ---- batch: 030 ----
mean loss: 261.56
 ---- batch: 040 ----
mean loss: 249.57
 ---- batch: 050 ----
mean loss: 255.57
 ---- batch: 060 ----
mean loss: 259.55
 ---- batch: 070 ----
mean loss: 264.49
 ---- batch: 080 ----
mean loss: 260.31
 ---- batch: 090 ----
mean loss: 254.04
train mean loss: 257.17
epoch train time: 0:00:01.641096
elapsed time: 0:02:16.856182
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 20:26:48.012137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.39
 ---- batch: 020 ----
mean loss: 256.71
 ---- batch: 030 ----
mean loss: 245.77
 ---- batch: 040 ----
mean loss: 252.85
 ---- batch: 050 ----
mean loss: 264.03
 ---- batch: 060 ----
mean loss: 248.10
 ---- batch: 070 ----
mean loss: 254.13
 ---- batch: 080 ----
mean loss: 250.15
 ---- batch: 090 ----
mean loss: 248.28
train mean loss: 252.43
epoch train time: 0:00:01.642543
elapsed time: 0:02:18.499410
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 20:26:49.655440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.10
 ---- batch: 020 ----
mean loss: 243.86
 ---- batch: 030 ----
mean loss: 252.02
 ---- batch: 040 ----
mean loss: 251.27
 ---- batch: 050 ----
mean loss: 252.52
 ---- batch: 060 ----
mean loss: 253.19
 ---- batch: 070 ----
mean loss: 235.50
 ---- batch: 080 ----
mean loss: 244.18
 ---- batch: 090 ----
mean loss: 247.56
train mean loss: 248.83
epoch train time: 0:00:01.626674
elapsed time: 0:02:20.126748
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 20:26:51.282670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.57
 ---- batch: 020 ----
mean loss: 245.11
 ---- batch: 030 ----
mean loss: 244.53
 ---- batch: 040 ----
mean loss: 242.60
 ---- batch: 050 ----
mean loss: 251.80
 ---- batch: 060 ----
mean loss: 247.05
 ---- batch: 070 ----
mean loss: 244.41
 ---- batch: 080 ----
mean loss: 251.34
 ---- batch: 090 ----
mean loss: 238.21
train mean loss: 244.61
epoch train time: 0:00:01.607975
elapsed time: 0:02:21.735340
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 20:26:52.891343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.05
 ---- batch: 020 ----
mean loss: 233.89
 ---- batch: 030 ----
mean loss: 250.97
 ---- batch: 040 ----
mean loss: 249.21
 ---- batch: 050 ----
mean loss: 246.60
 ---- batch: 060 ----
mean loss: 249.20
 ---- batch: 070 ----
mean loss: 233.52
 ---- batch: 080 ----
mean loss: 238.93
 ---- batch: 090 ----
mean loss: 234.51
train mean loss: 241.72
epoch train time: 0:00:01.616757
elapsed time: 0:02:23.352818
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 20:26:54.508772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.50
 ---- batch: 020 ----
mean loss: 235.79
 ---- batch: 030 ----
mean loss: 232.56
 ---- batch: 040 ----
mean loss: 229.17
 ---- batch: 050 ----
mean loss: 232.67
 ---- batch: 060 ----
mean loss: 232.88
 ---- batch: 070 ----
mean loss: 237.89
 ---- batch: 080 ----
mean loss: 242.72
 ---- batch: 090 ----
mean loss: 238.48
train mean loss: 235.86
epoch train time: 0:00:01.625744
elapsed time: 0:02:24.979221
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 20:26:56.135185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.81
 ---- batch: 020 ----
mean loss: 238.99
 ---- batch: 030 ----
mean loss: 227.09
 ---- batch: 040 ----
mean loss: 237.81
 ---- batch: 050 ----
mean loss: 229.83
 ---- batch: 060 ----
mean loss: 230.86
 ---- batch: 070 ----
mean loss: 244.64
 ---- batch: 080 ----
mean loss: 232.25
 ---- batch: 090 ----
mean loss: 237.10
train mean loss: 235.46
epoch train time: 0:00:01.612708
elapsed time: 0:02:26.592659
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 20:26:57.748592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.48
 ---- batch: 020 ----
mean loss: 223.85
 ---- batch: 030 ----
mean loss: 230.89
 ---- batch: 040 ----
mean loss: 233.49
 ---- batch: 050 ----
mean loss: 233.72
 ---- batch: 060 ----
mean loss: 220.57
 ---- batch: 070 ----
mean loss: 220.58
 ---- batch: 080 ----
mean loss: 235.32
 ---- batch: 090 ----
mean loss: 237.28
train mean loss: 230.51
epoch train time: 0:00:01.612320
elapsed time: 0:02:28.205600
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 20:26:59.361566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.86
 ---- batch: 020 ----
mean loss: 223.13
 ---- batch: 030 ----
mean loss: 231.62
 ---- batch: 040 ----
mean loss: 236.44
 ---- batch: 050 ----
mean loss: 227.11
 ---- batch: 060 ----
mean loss: 220.66
 ---- batch: 070 ----
mean loss: 228.49
 ---- batch: 080 ----
mean loss: 227.24
 ---- batch: 090 ----
mean loss: 230.68
train mean loss: 228.15
epoch train time: 0:00:01.614542
elapsed time: 0:02:29.820799
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 20:27:00.976747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.80
 ---- batch: 020 ----
mean loss: 225.94
 ---- batch: 030 ----
mean loss: 227.70
 ---- batch: 040 ----
mean loss: 225.05
 ---- batch: 050 ----
mean loss: 230.26
 ---- batch: 060 ----
mean loss: 232.19
 ---- batch: 070 ----
mean loss: 218.21
 ---- batch: 080 ----
mean loss: 222.51
 ---- batch: 090 ----
mean loss: 229.86
train mean loss: 225.91
epoch train time: 0:00:01.611293
elapsed time: 0:02:31.432759
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 20:27:02.588727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.02
 ---- batch: 020 ----
mean loss: 207.60
 ---- batch: 030 ----
mean loss: 225.35
 ---- batch: 040 ----
mean loss: 215.94
 ---- batch: 050 ----
mean loss: 230.78
 ---- batch: 060 ----
mean loss: 220.92
 ---- batch: 070 ----
mean loss: 230.99
 ---- batch: 080 ----
mean loss: 233.10
 ---- batch: 090 ----
mean loss: 227.15
train mean loss: 222.00
epoch train time: 0:00:01.607767
elapsed time: 0:02:33.041175
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 20:27:04.197177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.06
 ---- batch: 020 ----
mean loss: 221.46
 ---- batch: 030 ----
mean loss: 227.60
 ---- batch: 040 ----
mean loss: 214.47
 ---- batch: 050 ----
mean loss: 220.76
 ---- batch: 060 ----
mean loss: 221.32
 ---- batch: 070 ----
mean loss: 218.36
 ---- batch: 080 ----
mean loss: 221.88
 ---- batch: 090 ----
mean loss: 222.31
train mean loss: 220.26
epoch train time: 0:00:01.643464
elapsed time: 0:02:34.685357
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 20:27:05.841375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.55
 ---- batch: 020 ----
mean loss: 217.08
 ---- batch: 030 ----
mean loss: 216.78
 ---- batch: 040 ----
mean loss: 222.33
 ---- batch: 050 ----
mean loss: 216.44
 ---- batch: 060 ----
mean loss: 218.38
 ---- batch: 070 ----
mean loss: 212.49
 ---- batch: 080 ----
mean loss: 217.97
 ---- batch: 090 ----
mean loss: 219.07
train mean loss: 217.46
epoch train time: 0:00:01.647966
elapsed time: 0:02:36.334029
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 20:27:07.489977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.26
 ---- batch: 020 ----
mean loss: 213.13
 ---- batch: 030 ----
mean loss: 215.42
 ---- batch: 040 ----
mean loss: 222.57
 ---- batch: 050 ----
mean loss: 206.40
 ---- batch: 060 ----
mean loss: 211.79
 ---- batch: 070 ----
mean loss: 219.57
 ---- batch: 080 ----
mean loss: 217.51
 ---- batch: 090 ----
mean loss: 215.27
train mean loss: 214.94
epoch train time: 0:00:01.644696
elapsed time: 0:02:37.979329
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 20:27:09.135346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.71
 ---- batch: 020 ----
mean loss: 212.66
 ---- batch: 030 ----
mean loss: 201.82
 ---- batch: 040 ----
mean loss: 207.81
 ---- batch: 050 ----
mean loss: 217.72
 ---- batch: 060 ----
mean loss: 213.70
 ---- batch: 070 ----
mean loss: 219.41
 ---- batch: 080 ----
mean loss: 219.45
 ---- batch: 090 ----
mean loss: 216.47
train mean loss: 213.41
epoch train time: 0:00:01.656209
elapsed time: 0:02:39.636286
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 20:27:10.792231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.74
 ---- batch: 020 ----
mean loss: 213.45
 ---- batch: 030 ----
mean loss: 206.82
 ---- batch: 040 ----
mean loss: 214.19
 ---- batch: 050 ----
mean loss: 204.22
 ---- batch: 060 ----
mean loss: 207.82
 ---- batch: 070 ----
mean loss: 215.68
 ---- batch: 080 ----
mean loss: 214.34
 ---- batch: 090 ----
mean loss: 212.91
train mean loss: 210.39
epoch train time: 0:00:01.621056
elapsed time: 0:02:41.257949
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 20:27:12.413945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.61
 ---- batch: 020 ----
mean loss: 204.20
 ---- batch: 030 ----
mean loss: 206.79
 ---- batch: 040 ----
mean loss: 217.61
 ---- batch: 050 ----
mean loss: 207.86
 ---- batch: 060 ----
mean loss: 204.68
 ---- batch: 070 ----
mean loss: 221.67
 ---- batch: 080 ----
mean loss: 208.63
 ---- batch: 090 ----
mean loss: 211.05
train mean loss: 208.69
epoch train time: 0:00:01.640136
elapsed time: 0:02:42.898735
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 20:27:14.054742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.26
 ---- batch: 020 ----
mean loss: 196.48
 ---- batch: 030 ----
mean loss: 202.26
 ---- batch: 040 ----
mean loss: 217.75
 ---- batch: 050 ----
mean loss: 209.61
 ---- batch: 060 ----
mean loss: 211.55
 ---- batch: 070 ----
mean loss: 201.69
 ---- batch: 080 ----
mean loss: 211.66
 ---- batch: 090 ----
mean loss: 207.88
train mean loss: 206.83
epoch train time: 0:00:01.634707
elapsed time: 0:02:44.534118
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 20:27:15.690124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.72
 ---- batch: 020 ----
mean loss: 197.21
 ---- batch: 030 ----
mean loss: 201.87
 ---- batch: 040 ----
mean loss: 202.12
 ---- batch: 050 ----
mean loss: 205.46
 ---- batch: 060 ----
mean loss: 202.71
 ---- batch: 070 ----
mean loss: 202.37
 ---- batch: 080 ----
mean loss: 203.79
 ---- batch: 090 ----
mean loss: 211.93
train mean loss: 203.90
epoch train time: 0:00:01.629131
elapsed time: 0:02:46.163911
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 20:27:17.319874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.33
 ---- batch: 020 ----
mean loss: 203.03
 ---- batch: 030 ----
mean loss: 207.05
 ---- batch: 040 ----
mean loss: 200.10
 ---- batch: 050 ----
mean loss: 209.79
 ---- batch: 060 ----
mean loss: 198.00
 ---- batch: 070 ----
mean loss: 198.51
 ---- batch: 080 ----
mean loss: 196.58
 ---- batch: 090 ----
mean loss: 201.79
train mean loss: 201.47
epoch train time: 0:00:01.640639
elapsed time: 0:02:47.805200
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 20:27:18.961157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.15
 ---- batch: 020 ----
mean loss: 200.31
 ---- batch: 030 ----
mean loss: 191.92
 ---- batch: 040 ----
mean loss: 196.25
 ---- batch: 050 ----
mean loss: 207.52
 ---- batch: 060 ----
mean loss: 201.31
 ---- batch: 070 ----
mean loss: 195.22
 ---- batch: 080 ----
mean loss: 207.29
 ---- batch: 090 ----
mean loss: 196.57
train mean loss: 199.39
epoch train time: 0:00:01.632098
elapsed time: 0:02:49.437950
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 20:27:20.593814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.98
 ---- batch: 020 ----
mean loss: 191.17
 ---- batch: 030 ----
mean loss: 194.51
 ---- batch: 040 ----
mean loss: 191.70
 ---- batch: 050 ----
mean loss: 204.22
 ---- batch: 060 ----
mean loss: 202.30
 ---- batch: 070 ----
mean loss: 195.54
 ---- batch: 080 ----
mean loss: 205.14
 ---- batch: 090 ----
mean loss: 204.29
train mean loss: 198.30
epoch train time: 0:00:01.651286
elapsed time: 0:02:51.089739
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 20:27:22.245705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.63
 ---- batch: 020 ----
mean loss: 197.20
 ---- batch: 030 ----
mean loss: 193.04
 ---- batch: 040 ----
mean loss: 191.65
 ---- batch: 050 ----
mean loss: 198.65
 ---- batch: 060 ----
mean loss: 199.29
 ---- batch: 070 ----
mean loss: 189.53
 ---- batch: 080 ----
mean loss: 195.61
 ---- batch: 090 ----
mean loss: 205.94
train mean loss: 196.72
epoch train time: 0:00:01.642350
elapsed time: 0:02:52.732802
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 20:27:23.888750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.72
 ---- batch: 020 ----
mean loss: 197.54
 ---- batch: 030 ----
mean loss: 190.96
 ---- batch: 040 ----
mean loss: 199.15
 ---- batch: 050 ----
mean loss: 193.96
 ---- batch: 060 ----
mean loss: 188.00
 ---- batch: 070 ----
mean loss: 186.01
 ---- batch: 080 ----
mean loss: 195.02
 ---- batch: 090 ----
mean loss: 194.64
train mean loss: 194.48
epoch train time: 0:00:01.626424
elapsed time: 0:02:54.359858
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 20:27:25.515624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.56
 ---- batch: 020 ----
mean loss: 187.49
 ---- batch: 030 ----
mean loss: 198.98
 ---- batch: 040 ----
mean loss: 197.26
 ---- batch: 050 ----
mean loss: 190.09
 ---- batch: 060 ----
mean loss: 187.40
 ---- batch: 070 ----
mean loss: 186.09
 ---- batch: 080 ----
mean loss: 194.94
 ---- batch: 090 ----
mean loss: 185.95
train mean loss: 191.32
epoch train time: 0:00:01.614507
elapsed time: 0:02:55.974791
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 20:27:27.130806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.63
 ---- batch: 020 ----
mean loss: 182.85
 ---- batch: 030 ----
mean loss: 186.95
 ---- batch: 040 ----
mean loss: 188.33
 ---- batch: 050 ----
mean loss: 180.03
 ---- batch: 060 ----
mean loss: 195.20
 ---- batch: 070 ----
mean loss: 195.57
 ---- batch: 080 ----
mean loss: 194.51
 ---- batch: 090 ----
mean loss: 198.38
train mean loss: 190.91
epoch train time: 0:00:01.627672
elapsed time: 0:02:57.603117
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 20:27:28.759091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.18
 ---- batch: 020 ----
mean loss: 188.75
 ---- batch: 030 ----
mean loss: 183.29
 ---- batch: 040 ----
mean loss: 190.48
 ---- batch: 050 ----
mean loss: 183.95
 ---- batch: 060 ----
mean loss: 192.93
 ---- batch: 070 ----
mean loss: 189.95
 ---- batch: 080 ----
mean loss: 188.11
 ---- batch: 090 ----
mean loss: 190.12
train mean loss: 188.43
epoch train time: 0:00:01.627750
elapsed time: 0:02:59.231469
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 20:27:30.387404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.95
 ---- batch: 020 ----
mean loss: 188.05
 ---- batch: 030 ----
mean loss: 185.00
 ---- batch: 040 ----
mean loss: 186.73
 ---- batch: 050 ----
mean loss: 190.38
 ---- batch: 060 ----
mean loss: 188.16
 ---- batch: 070 ----
mean loss: 189.63
 ---- batch: 080 ----
mean loss: 186.67
 ---- batch: 090 ----
mean loss: 193.01
train mean loss: 186.60
epoch train time: 0:00:01.667970
elapsed time: 0:03:00.900095
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 20:27:32.056064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.53
 ---- batch: 020 ----
mean loss: 184.80
 ---- batch: 030 ----
mean loss: 179.11
 ---- batch: 040 ----
mean loss: 177.96
 ---- batch: 050 ----
mean loss: 189.21
 ---- batch: 060 ----
mean loss: 201.01
 ---- batch: 070 ----
mean loss: 181.88
 ---- batch: 080 ----
mean loss: 192.45
 ---- batch: 090 ----
mean loss: 184.36
train mean loss: 185.62
epoch train time: 0:00:01.631224
elapsed time: 0:03:02.531960
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 20:27:33.687896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.26
 ---- batch: 020 ----
mean loss: 189.43
 ---- batch: 030 ----
mean loss: 179.59
 ---- batch: 040 ----
mean loss: 182.67
 ---- batch: 050 ----
mean loss: 186.95
 ---- batch: 060 ----
mean loss: 191.05
 ---- batch: 070 ----
mean loss: 182.94
 ---- batch: 080 ----
mean loss: 189.36
 ---- batch: 090 ----
mean loss: 181.17
train mean loss: 185.37
epoch train time: 0:00:01.617207
elapsed time: 0:03:04.149736
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 20:27:35.305710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.56
 ---- batch: 020 ----
mean loss: 185.31
 ---- batch: 030 ----
mean loss: 180.10
 ---- batch: 040 ----
mean loss: 186.35
 ---- batch: 050 ----
mean loss: 176.37
 ---- batch: 060 ----
mean loss: 190.05
 ---- batch: 070 ----
mean loss: 180.45
 ---- batch: 080 ----
mean loss: 189.45
 ---- batch: 090 ----
mean loss: 178.02
train mean loss: 182.85
epoch train time: 0:00:01.637748
elapsed time: 0:03:05.788207
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 20:27:36.944159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.87
 ---- batch: 020 ----
mean loss: 183.58
 ---- batch: 030 ----
mean loss: 177.36
 ---- batch: 040 ----
mean loss: 185.27
 ---- batch: 050 ----
mean loss: 180.65
 ---- batch: 060 ----
mean loss: 193.24
 ---- batch: 070 ----
mean loss: 185.88
 ---- batch: 080 ----
mean loss: 183.40
 ---- batch: 090 ----
mean loss: 176.99
train mean loss: 181.82
epoch train time: 0:00:01.609498
elapsed time: 0:03:07.398308
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 20:27:38.554261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.06
 ---- batch: 020 ----
mean loss: 170.60
 ---- batch: 030 ----
mean loss: 181.65
 ---- batch: 040 ----
mean loss: 169.44
 ---- batch: 050 ----
mean loss: 173.72
 ---- batch: 060 ----
mean loss: 187.96
 ---- batch: 070 ----
mean loss: 181.78
 ---- batch: 080 ----
mean loss: 182.19
 ---- batch: 090 ----
mean loss: 189.67
train mean loss: 179.13
epoch train time: 0:00:01.656315
elapsed time: 0:03:09.055203
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 20:27:40.211154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.78
 ---- batch: 020 ----
mean loss: 172.73
 ---- batch: 030 ----
mean loss: 175.60
 ---- batch: 040 ----
mean loss: 176.27
 ---- batch: 050 ----
mean loss: 182.59
 ---- batch: 060 ----
mean loss: 180.55
 ---- batch: 070 ----
mean loss: 174.53
 ---- batch: 080 ----
mean loss: 180.90
 ---- batch: 090 ----
mean loss: 179.13
train mean loss: 178.72
epoch train time: 0:00:01.618703
elapsed time: 0:03:10.674532
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 20:27:41.830540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.49
 ---- batch: 020 ----
mean loss: 166.34
 ---- batch: 030 ----
mean loss: 175.27
 ---- batch: 040 ----
mean loss: 177.91
 ---- batch: 050 ----
mean loss: 180.85
 ---- batch: 060 ----
mean loss: 178.20
 ---- batch: 070 ----
mean loss: 185.35
 ---- batch: 080 ----
mean loss: 179.34
 ---- batch: 090 ----
mean loss: 172.94
train mean loss: 176.80
epoch train time: 0:00:01.625345
elapsed time: 0:03:12.300525
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 20:27:43.456477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.05
 ---- batch: 020 ----
mean loss: 168.83
 ---- batch: 030 ----
mean loss: 172.00
 ---- batch: 040 ----
mean loss: 175.47
 ---- batch: 050 ----
mean loss: 169.97
 ---- batch: 060 ----
mean loss: 176.22
 ---- batch: 070 ----
mean loss: 180.12
 ---- batch: 080 ----
mean loss: 179.38
 ---- batch: 090 ----
mean loss: 181.79
train mean loss: 174.84
epoch train time: 0:00:01.672950
elapsed time: 0:03:13.974075
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 20:27:45.130018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.92
 ---- batch: 020 ----
mean loss: 173.03
 ---- batch: 030 ----
mean loss: 166.39
 ---- batch: 040 ----
mean loss: 172.80
 ---- batch: 050 ----
mean loss: 172.25
 ---- batch: 060 ----
mean loss: 178.44
 ---- batch: 070 ----
mean loss: 179.29
 ---- batch: 080 ----
mean loss: 180.37
 ---- batch: 090 ----
mean loss: 176.38
train mean loss: 174.19
epoch train time: 0:00:01.633934
elapsed time: 0:03:15.608666
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 20:27:46.764625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.71
 ---- batch: 020 ----
mean loss: 163.52
 ---- batch: 030 ----
mean loss: 168.40
 ---- batch: 040 ----
mean loss: 165.32
 ---- batch: 050 ----
mean loss: 181.60
 ---- batch: 060 ----
mean loss: 176.72
 ---- batch: 070 ----
mean loss: 170.50
 ---- batch: 080 ----
mean loss: 179.88
 ---- batch: 090 ----
mean loss: 175.51
train mean loss: 173.70
epoch train time: 0:00:01.653240
elapsed time: 0:03:17.262535
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 20:27:48.418494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.58
 ---- batch: 020 ----
mean loss: 166.27
 ---- batch: 030 ----
mean loss: 169.41
 ---- batch: 040 ----
mean loss: 169.64
 ---- batch: 050 ----
mean loss: 178.30
 ---- batch: 060 ----
mean loss: 164.92
 ---- batch: 070 ----
mean loss: 170.04
 ---- batch: 080 ----
mean loss: 171.81
 ---- batch: 090 ----
mean loss: 176.63
train mean loss: 171.04
epoch train time: 0:00:01.642339
elapsed time: 0:03:18.905490
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 20:27:50.061453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.54
 ---- batch: 020 ----
mean loss: 169.74
 ---- batch: 030 ----
mean loss: 166.31
 ---- batch: 040 ----
mean loss: 167.15
 ---- batch: 050 ----
mean loss: 171.25
 ---- batch: 060 ----
mean loss: 180.78
 ---- batch: 070 ----
mean loss: 168.47
 ---- batch: 080 ----
mean loss: 166.58
 ---- batch: 090 ----
mean loss: 173.40
train mean loss: 170.09
epoch train time: 0:00:01.619114
elapsed time: 0:03:20.525301
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 20:27:51.681251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.17
 ---- batch: 020 ----
mean loss: 166.23
 ---- batch: 030 ----
mean loss: 170.56
 ---- batch: 040 ----
mean loss: 167.07
 ---- batch: 050 ----
mean loss: 168.35
 ---- batch: 060 ----
mean loss: 172.53
 ---- batch: 070 ----
mean loss: 169.01
 ---- batch: 080 ----
mean loss: 165.91
 ---- batch: 090 ----
mean loss: 173.75
train mean loss: 169.62
epoch train time: 0:00:01.635491
elapsed time: 0:03:22.161421
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 20:27:53.317402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.99
 ---- batch: 020 ----
mean loss: 170.93
 ---- batch: 030 ----
mean loss: 162.86
 ---- batch: 040 ----
mean loss: 162.29
 ---- batch: 050 ----
mean loss: 164.43
 ---- batch: 060 ----
mean loss: 170.71
 ---- batch: 070 ----
mean loss: 173.28
 ---- batch: 080 ----
mean loss: 162.73
 ---- batch: 090 ----
mean loss: 166.92
train mean loss: 167.71
epoch train time: 0:00:01.646823
elapsed time: 0:03:23.808967
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 20:27:54.964937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.55
 ---- batch: 020 ----
mean loss: 168.38
 ---- batch: 030 ----
mean loss: 158.35
 ---- batch: 040 ----
mean loss: 168.82
 ---- batch: 050 ----
mean loss: 166.06
 ---- batch: 060 ----
mean loss: 171.61
 ---- batch: 070 ----
mean loss: 164.69
 ---- batch: 080 ----
mean loss: 163.25
 ---- batch: 090 ----
mean loss: 172.61
train mean loss: 167.10
epoch train time: 0:00:01.626242
elapsed time: 0:03:25.435860
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 20:27:56.591794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.71
 ---- batch: 020 ----
mean loss: 159.18
 ---- batch: 030 ----
mean loss: 170.82
 ---- batch: 040 ----
mean loss: 162.39
 ---- batch: 050 ----
mean loss: 155.63
 ---- batch: 060 ----
mean loss: 171.86
 ---- batch: 070 ----
mean loss: 168.68
 ---- batch: 080 ----
mean loss: 169.08
 ---- batch: 090 ----
mean loss: 169.75
train mean loss: 165.54
epoch train time: 0:00:01.631189
elapsed time: 0:03:27.067613
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 20:27:58.223645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.83
 ---- batch: 020 ----
mean loss: 165.49
 ---- batch: 030 ----
mean loss: 156.30
 ---- batch: 040 ----
mean loss: 161.06
 ---- batch: 050 ----
mean loss: 167.63
 ---- batch: 060 ----
mean loss: 165.99
 ---- batch: 070 ----
mean loss: 163.21
 ---- batch: 080 ----
mean loss: 165.31
 ---- batch: 090 ----
mean loss: 168.75
train mean loss: 164.37
epoch train time: 0:00:01.634254
elapsed time: 0:03:28.702763
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 20:27:59.858549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.58
 ---- batch: 020 ----
mean loss: 160.43
 ---- batch: 030 ----
mean loss: 159.63
 ---- batch: 040 ----
mean loss: 169.35
 ---- batch: 050 ----
mean loss: 166.05
 ---- batch: 060 ----
mean loss: 162.58
 ---- batch: 070 ----
mean loss: 167.58
 ---- batch: 080 ----
mean loss: 166.52
 ---- batch: 090 ----
mean loss: 166.18
train mean loss: 163.97
epoch train time: 0:00:01.633438
elapsed time: 0:03:30.336658
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 20:28:01.492582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.00
 ---- batch: 020 ----
mean loss: 167.57
 ---- batch: 030 ----
mean loss: 155.42
 ---- batch: 040 ----
mean loss: 159.82
 ---- batch: 050 ----
mean loss: 161.50
 ---- batch: 060 ----
mean loss: 164.49
 ---- batch: 070 ----
mean loss: 164.26
 ---- batch: 080 ----
mean loss: 163.65
 ---- batch: 090 ----
mean loss: 160.12
train mean loss: 161.87
epoch train time: 0:00:01.618670
elapsed time: 0:03:31.955919
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 20:28:03.111856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.90
 ---- batch: 020 ----
mean loss: 158.29
 ---- batch: 030 ----
mean loss: 164.30
 ---- batch: 040 ----
mean loss: 154.07
 ---- batch: 050 ----
mean loss: 157.92
 ---- batch: 060 ----
mean loss: 166.01
 ---- batch: 070 ----
mean loss: 164.27
 ---- batch: 080 ----
mean loss: 164.26
 ---- batch: 090 ----
mean loss: 157.86
train mean loss: 161.78
epoch train time: 0:00:01.622844
elapsed time: 0:03:33.579471
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 20:28:04.735448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.85
 ---- batch: 020 ----
mean loss: 161.34
 ---- batch: 030 ----
mean loss: 162.51
 ---- batch: 040 ----
mean loss: 156.45
 ---- batch: 050 ----
mean loss: 160.04
 ---- batch: 060 ----
mean loss: 166.51
 ---- batch: 070 ----
mean loss: 161.63
 ---- batch: 080 ----
mean loss: 161.81
 ---- batch: 090 ----
mean loss: 160.87
train mean loss: 161.46
epoch train time: 0:00:01.651656
elapsed time: 0:03:35.231739
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 20:28:06.387713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.60
 ---- batch: 020 ----
mean loss: 152.90
 ---- batch: 030 ----
mean loss: 161.59
 ---- batch: 040 ----
mean loss: 167.29
 ---- batch: 050 ----
mean loss: 158.92
 ---- batch: 060 ----
mean loss: 164.86
 ---- batch: 070 ----
mean loss: 162.89
 ---- batch: 080 ----
mean loss: 155.25
 ---- batch: 090 ----
mean loss: 162.81
train mean loss: 159.33
epoch train time: 0:00:01.640364
elapsed time: 0:03:36.872862
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 20:28:08.028800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.74
 ---- batch: 020 ----
mean loss: 156.59
 ---- batch: 030 ----
mean loss: 152.59
 ---- batch: 040 ----
mean loss: 155.29
 ---- batch: 050 ----
mean loss: 160.10
 ---- batch: 060 ----
mean loss: 164.83
 ---- batch: 070 ----
mean loss: 166.64
 ---- batch: 080 ----
mean loss: 158.62
 ---- batch: 090 ----
mean loss: 155.90
train mean loss: 158.74
epoch train time: 0:00:01.635820
elapsed time: 0:03:38.509314
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 20:28:09.665262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.48
 ---- batch: 020 ----
mean loss: 156.77
 ---- batch: 030 ----
mean loss: 153.31
 ---- batch: 040 ----
mean loss: 158.67
 ---- batch: 050 ----
mean loss: 157.33
 ---- batch: 060 ----
mean loss: 161.87
 ---- batch: 070 ----
mean loss: 149.31
 ---- batch: 080 ----
mean loss: 157.75
 ---- batch: 090 ----
mean loss: 159.76
train mean loss: 157.61
epoch train time: 0:00:01.705526
elapsed time: 0:03:40.215603
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 20:28:11.371561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.82
 ---- batch: 020 ----
mean loss: 153.69
 ---- batch: 030 ----
mean loss: 153.68
 ---- batch: 040 ----
mean loss: 157.10
 ---- batch: 050 ----
mean loss: 154.51
 ---- batch: 060 ----
mean loss: 163.95
 ---- batch: 070 ----
mean loss: 163.52
 ---- batch: 080 ----
mean loss: 154.21
 ---- batch: 090 ----
mean loss: 166.48
train mean loss: 157.80
epoch train time: 0:00:01.664544
elapsed time: 0:03:41.880836
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 20:28:13.036771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.28
 ---- batch: 020 ----
mean loss: 157.29
 ---- batch: 030 ----
mean loss: 150.49
 ---- batch: 040 ----
mean loss: 147.61
 ---- batch: 050 ----
mean loss: 164.07
 ---- batch: 060 ----
mean loss: 157.51
 ---- batch: 070 ----
mean loss: 158.27
 ---- batch: 080 ----
mean loss: 156.24
 ---- batch: 090 ----
mean loss: 162.94
train mean loss: 155.90
epoch train time: 0:00:01.596297
elapsed time: 0:03:43.477782
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 20:28:14.633749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.93
 ---- batch: 020 ----
mean loss: 148.72
 ---- batch: 030 ----
mean loss: 152.72
 ---- batch: 040 ----
mean loss: 150.38
 ---- batch: 050 ----
mean loss: 159.08
 ---- batch: 060 ----
mean loss: 159.26
 ---- batch: 070 ----
mean loss: 162.60
 ---- batch: 080 ----
mean loss: 160.64
 ---- batch: 090 ----
mean loss: 161.74
train mean loss: 155.01
epoch train time: 0:00:01.635934
elapsed time: 0:03:45.114348
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 20:28:16.270299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.20
 ---- batch: 020 ----
mean loss: 144.73
 ---- batch: 030 ----
mean loss: 152.77
 ---- batch: 040 ----
mean loss: 149.08
 ---- batch: 050 ----
mean loss: 156.76
 ---- batch: 060 ----
mean loss: 153.49
 ---- batch: 070 ----
mean loss: 152.91
 ---- batch: 080 ----
mean loss: 160.12
 ---- batch: 090 ----
mean loss: 162.70
train mean loss: 153.58
epoch train time: 0:00:01.627008
elapsed time: 0:03:46.742060
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 20:28:17.898033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.30
 ---- batch: 020 ----
mean loss: 152.34
 ---- batch: 030 ----
mean loss: 149.83
 ---- batch: 040 ----
mean loss: 154.02
 ---- batch: 050 ----
mean loss: 155.61
 ---- batch: 060 ----
mean loss: 154.81
 ---- batch: 070 ----
mean loss: 152.72
 ---- batch: 080 ----
mean loss: 156.59
 ---- batch: 090 ----
mean loss: 151.03
train mean loss: 152.74
epoch train time: 0:00:01.643450
elapsed time: 0:03:48.386277
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 20:28:19.542150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.38
 ---- batch: 020 ----
mean loss: 148.71
 ---- batch: 030 ----
mean loss: 149.59
 ---- batch: 040 ----
mean loss: 149.50
 ---- batch: 050 ----
mean loss: 153.59
 ---- batch: 060 ----
mean loss: 151.71
 ---- batch: 070 ----
mean loss: 153.92
 ---- batch: 080 ----
mean loss: 157.92
 ---- batch: 090 ----
mean loss: 154.34
train mean loss: 152.36
epoch train time: 0:00:01.645033
elapsed time: 0:03:50.031841
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 20:28:21.187836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.40
 ---- batch: 020 ----
mean loss: 146.02
 ---- batch: 030 ----
mean loss: 148.14
 ---- batch: 040 ----
mean loss: 148.28
 ---- batch: 050 ----
mean loss: 152.93
 ---- batch: 060 ----
mean loss: 154.83
 ---- batch: 070 ----
mean loss: 150.18
 ---- batch: 080 ----
mean loss: 156.81
 ---- batch: 090 ----
mean loss: 150.69
train mean loss: 151.41
epoch train time: 0:00:01.629218
elapsed time: 0:03:51.661739
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 20:28:22.817729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.00
 ---- batch: 020 ----
mean loss: 147.17
 ---- batch: 030 ----
mean loss: 150.68
 ---- batch: 040 ----
mean loss: 149.36
 ---- batch: 050 ----
mean loss: 150.96
 ---- batch: 060 ----
mean loss: 153.01
 ---- batch: 070 ----
mean loss: 152.70
 ---- batch: 080 ----
mean loss: 158.08
 ---- batch: 090 ----
mean loss: 151.96
train mean loss: 150.68
epoch train time: 0:00:01.659534
elapsed time: 0:03:53.321926
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 20:28:24.477938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.57
 ---- batch: 020 ----
mean loss: 149.86
 ---- batch: 030 ----
mean loss: 149.64
 ---- batch: 040 ----
mean loss: 151.07
 ---- batch: 050 ----
mean loss: 151.35
 ---- batch: 060 ----
mean loss: 150.09
 ---- batch: 070 ----
mean loss: 148.33
 ---- batch: 080 ----
mean loss: 148.13
 ---- batch: 090 ----
mean loss: 145.52
train mean loss: 149.32
epoch train time: 0:00:01.652051
elapsed time: 0:03:54.974677
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 20:28:26.130615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.75
 ---- batch: 020 ----
mean loss: 140.33
 ---- batch: 030 ----
mean loss: 143.59
 ---- batch: 040 ----
mean loss: 149.61
 ---- batch: 050 ----
mean loss: 157.02
 ---- batch: 060 ----
mean loss: 149.38
 ---- batch: 070 ----
mean loss: 150.64
 ---- batch: 080 ----
mean loss: 156.50
 ---- batch: 090 ----
mean loss: 150.36
train mean loss: 148.81
epoch train time: 0:00:01.660807
elapsed time: 0:03:56.636228
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 20:28:27.792185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.10
 ---- batch: 020 ----
mean loss: 147.18
 ---- batch: 030 ----
mean loss: 141.64
 ---- batch: 040 ----
mean loss: 143.95
 ---- batch: 050 ----
mean loss: 149.23
 ---- batch: 060 ----
mean loss: 154.20
 ---- batch: 070 ----
mean loss: 148.20
 ---- batch: 080 ----
mean loss: 145.47
 ---- batch: 090 ----
mean loss: 149.14
train mean loss: 147.48
epoch train time: 0:00:01.655436
elapsed time: 0:03:58.292329
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 20:28:29.448262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.00
 ---- batch: 020 ----
mean loss: 147.54
 ---- batch: 030 ----
mean loss: 142.52
 ---- batch: 040 ----
mean loss: 143.22
 ---- batch: 050 ----
mean loss: 147.93
 ---- batch: 060 ----
mean loss: 149.54
 ---- batch: 070 ----
mean loss: 147.81
 ---- batch: 080 ----
mean loss: 147.72
 ---- batch: 090 ----
mean loss: 157.75
train mean loss: 147.36
epoch train time: 0:00:01.744266
elapsed time: 0:04:00.037259
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 20:28:31.193243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.72
 ---- batch: 020 ----
mean loss: 143.77
 ---- batch: 030 ----
mean loss: 146.21
 ---- batch: 040 ----
mean loss: 144.95
 ---- batch: 050 ----
mean loss: 141.62
 ---- batch: 060 ----
mean loss: 149.37
 ---- batch: 070 ----
mean loss: 148.49
 ---- batch: 080 ----
mean loss: 153.22
 ---- batch: 090 ----
mean loss: 154.16
train mean loss: 146.84
epoch train time: 0:00:01.666894
elapsed time: 0:04:01.705093
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 20:28:32.860863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.97
 ---- batch: 020 ----
mean loss: 142.80
 ---- batch: 030 ----
mean loss: 146.15
 ---- batch: 040 ----
mean loss: 144.29
 ---- batch: 050 ----
mean loss: 151.64
 ---- batch: 060 ----
mean loss: 146.46
 ---- batch: 070 ----
mean loss: 145.34
 ---- batch: 080 ----
mean loss: 143.66
 ---- batch: 090 ----
mean loss: 154.80
train mean loss: 146.03
epoch train time: 0:00:01.609831
elapsed time: 0:04:03.315364
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 20:28:34.471296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.67
 ---- batch: 020 ----
mean loss: 138.07
 ---- batch: 030 ----
mean loss: 142.56
 ---- batch: 040 ----
mean loss: 147.61
 ---- batch: 050 ----
mean loss: 143.70
 ---- batch: 060 ----
mean loss: 144.60
 ---- batch: 070 ----
mean loss: 153.03
 ---- batch: 080 ----
mean loss: 149.13
 ---- batch: 090 ----
mean loss: 146.36
train mean loss: 145.44
epoch train time: 0:00:01.644366
elapsed time: 0:04:04.960361
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 20:28:36.116310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.04
 ---- batch: 020 ----
mean loss: 141.63
 ---- batch: 030 ----
mean loss: 139.56
 ---- batch: 040 ----
mean loss: 142.59
 ---- batch: 050 ----
mean loss: 143.94
 ---- batch: 060 ----
mean loss: 138.27
 ---- batch: 070 ----
mean loss: 142.31
 ---- batch: 080 ----
mean loss: 154.52
 ---- batch: 090 ----
mean loss: 147.90
train mean loss: 144.09
epoch train time: 0:00:01.616335
elapsed time: 0:04:06.577367
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 20:28:37.733325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.65
 ---- batch: 020 ----
mean loss: 140.81
 ---- batch: 030 ----
mean loss: 141.08
 ---- batch: 040 ----
mean loss: 144.81
 ---- batch: 050 ----
mean loss: 143.68
 ---- batch: 060 ----
mean loss: 142.28
 ---- batch: 070 ----
mean loss: 146.16
 ---- batch: 080 ----
mean loss: 142.44
 ---- batch: 090 ----
mean loss: 148.41
train mean loss: 143.08
epoch train time: 0:00:01.623288
elapsed time: 0:04:08.201362
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 20:28:39.357305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.56
 ---- batch: 020 ----
mean loss: 140.79
 ---- batch: 030 ----
mean loss: 142.80
 ---- batch: 040 ----
mean loss: 137.79
 ---- batch: 050 ----
mean loss: 141.02
 ---- batch: 060 ----
mean loss: 143.69
 ---- batch: 070 ----
mean loss: 143.75
 ---- batch: 080 ----
mean loss: 148.65
 ---- batch: 090 ----
mean loss: 143.22
train mean loss: 143.11
epoch train time: 0:00:01.659890
elapsed time: 0:04:09.861921
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 20:28:41.017874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.61
 ---- batch: 020 ----
mean loss: 137.98
 ---- batch: 030 ----
mean loss: 143.84
 ---- batch: 040 ----
mean loss: 137.82
 ---- batch: 050 ----
mean loss: 136.97
 ---- batch: 060 ----
mean loss: 147.64
 ---- batch: 070 ----
mean loss: 143.15
 ---- batch: 080 ----
mean loss: 142.28
 ---- batch: 090 ----
mean loss: 143.83
train mean loss: 142.70
epoch train time: 0:00:01.621416
elapsed time: 0:04:11.483987
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 20:28:42.639990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.63
 ---- batch: 020 ----
mean loss: 132.80
 ---- batch: 030 ----
mean loss: 140.03
 ---- batch: 040 ----
mean loss: 136.04
 ---- batch: 050 ----
mean loss: 143.60
 ---- batch: 060 ----
mean loss: 141.29
 ---- batch: 070 ----
mean loss: 146.96
 ---- batch: 080 ----
mean loss: 145.99
 ---- batch: 090 ----
mean loss: 141.91
train mean loss: 141.28
epoch train time: 0:00:01.653858
elapsed time: 0:04:13.138553
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 20:28:44.294507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.52
 ---- batch: 020 ----
mean loss: 145.19
 ---- batch: 030 ----
mean loss: 139.84
 ---- batch: 040 ----
mean loss: 138.17
 ---- batch: 050 ----
mean loss: 136.51
 ---- batch: 060 ----
mean loss: 142.78
 ---- batch: 070 ----
mean loss: 141.97
 ---- batch: 080 ----
mean loss: 143.81
 ---- batch: 090 ----
mean loss: 136.29
train mean loss: 141.30
epoch train time: 0:00:01.646787
elapsed time: 0:04:14.786069
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 20:28:45.942023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.08
 ---- batch: 020 ----
mean loss: 134.36
 ---- batch: 030 ----
mean loss: 139.71
 ---- batch: 040 ----
mean loss: 137.28
 ---- batch: 050 ----
mean loss: 136.69
 ---- batch: 060 ----
mean loss: 135.25
 ---- batch: 070 ----
mean loss: 142.01
 ---- batch: 080 ----
mean loss: 145.86
 ---- batch: 090 ----
mean loss: 147.42
train mean loss: 140.21
epoch train time: 0:00:01.659252
elapsed time: 0:04:16.446110
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 20:28:47.602086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.71
 ---- batch: 020 ----
mean loss: 134.49
 ---- batch: 030 ----
mean loss: 133.32
 ---- batch: 040 ----
mean loss: 140.65
 ---- batch: 050 ----
mean loss: 138.85
 ---- batch: 060 ----
mean loss: 141.33
 ---- batch: 070 ----
mean loss: 142.18
 ---- batch: 080 ----
mean loss: 142.60
 ---- batch: 090 ----
mean loss: 142.59
train mean loss: 139.37
epoch train time: 0:00:01.663311
elapsed time: 0:04:18.110158
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 20:28:49.266111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.28
 ---- batch: 020 ----
mean loss: 139.16
 ---- batch: 030 ----
mean loss: 134.65
 ---- batch: 040 ----
mean loss: 139.91
 ---- batch: 050 ----
mean loss: 139.29
 ---- batch: 060 ----
mean loss: 145.83
 ---- batch: 070 ----
mean loss: 138.24
 ---- batch: 080 ----
mean loss: 144.02
 ---- batch: 090 ----
mean loss: 138.32
train mean loss: 139.05
epoch train time: 0:00:01.652569
elapsed time: 0:04:19.763415
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 20:28:50.919354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.78
 ---- batch: 020 ----
mean loss: 136.58
 ---- batch: 030 ----
mean loss: 140.31
 ---- batch: 040 ----
mean loss: 138.97
 ---- batch: 050 ----
mean loss: 137.73
 ---- batch: 060 ----
mean loss: 137.16
 ---- batch: 070 ----
mean loss: 136.42
 ---- batch: 080 ----
mean loss: 137.23
 ---- batch: 090 ----
mean loss: 146.90
train mean loss: 137.71
epoch train time: 0:00:01.623843
elapsed time: 0:04:21.387886
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 20:28:52.543840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.05
 ---- batch: 020 ----
mean loss: 134.93
 ---- batch: 030 ----
mean loss: 138.64
 ---- batch: 040 ----
mean loss: 136.70
 ---- batch: 050 ----
mean loss: 133.11
 ---- batch: 060 ----
mean loss: 139.37
 ---- batch: 070 ----
mean loss: 141.38
 ---- batch: 080 ----
mean loss: 143.09
 ---- batch: 090 ----
mean loss: 139.42
train mean loss: 137.16
epoch train time: 0:00:01.672811
elapsed time: 0:04:23.061355
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 20:28:54.217306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.43
 ---- batch: 020 ----
mean loss: 136.02
 ---- batch: 030 ----
mean loss: 135.26
 ---- batch: 040 ----
mean loss: 137.67
 ---- batch: 050 ----
mean loss: 138.28
 ---- batch: 060 ----
mean loss: 136.35
 ---- batch: 070 ----
mean loss: 136.42
 ---- batch: 080 ----
mean loss: 143.81
 ---- batch: 090 ----
mean loss: 139.16
train mean loss: 137.04
epoch train time: 0:00:01.666073
elapsed time: 0:04:24.728107
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 20:28:55.884089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.46
 ---- batch: 020 ----
mean loss: 133.62
 ---- batch: 030 ----
mean loss: 140.85
 ---- batch: 040 ----
mean loss: 125.59
 ---- batch: 050 ----
mean loss: 140.54
 ---- batch: 060 ----
mean loss: 133.85
 ---- batch: 070 ----
mean loss: 142.78
 ---- batch: 080 ----
mean loss: 133.77
 ---- batch: 090 ----
mean loss: 148.61
train mean loss: 137.23
epoch train time: 0:00:01.635242
elapsed time: 0:04:26.364061
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 20:28:57.520018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.45
 ---- batch: 020 ----
mean loss: 134.39
 ---- batch: 030 ----
mean loss: 133.91
 ---- batch: 040 ----
mean loss: 136.12
 ---- batch: 050 ----
mean loss: 137.58
 ---- batch: 060 ----
mean loss: 135.28
 ---- batch: 070 ----
mean loss: 141.26
 ---- batch: 080 ----
mean loss: 138.42
 ---- batch: 090 ----
mean loss: 131.30
train mean loss: 136.15
epoch train time: 0:00:01.642548
elapsed time: 0:04:28.007291
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 20:28:59.163242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.43
 ---- batch: 020 ----
mean loss: 137.68
 ---- batch: 030 ----
mean loss: 127.52
 ---- batch: 040 ----
mean loss: 142.75
 ---- batch: 050 ----
mean loss: 133.60
 ---- batch: 060 ----
mean loss: 131.20
 ---- batch: 070 ----
mean loss: 138.78
 ---- batch: 080 ----
mean loss: 131.78
 ---- batch: 090 ----
mean loss: 129.60
train mean loss: 134.71
epoch train time: 0:00:01.621148
elapsed time: 0:04:29.629190
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 20:29:00.785152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.14
 ---- batch: 020 ----
mean loss: 133.89
 ---- batch: 030 ----
mean loss: 133.59
 ---- batch: 040 ----
mean loss: 135.18
 ---- batch: 050 ----
mean loss: 134.84
 ---- batch: 060 ----
mean loss: 142.42
 ---- batch: 070 ----
mean loss: 133.18
 ---- batch: 080 ----
mean loss: 133.32
 ---- batch: 090 ----
mean loss: 136.42
train mean loss: 134.45
epoch train time: 0:00:01.678228
elapsed time: 0:04:31.308288
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 20:29:02.464368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.04
 ---- batch: 020 ----
mean loss: 133.31
 ---- batch: 030 ----
mean loss: 128.77
 ---- batch: 040 ----
mean loss: 134.49
 ---- batch: 050 ----
mean loss: 138.12
 ---- batch: 060 ----
mean loss: 132.99
 ---- batch: 070 ----
mean loss: 129.37
 ---- batch: 080 ----
mean loss: 142.98
 ---- batch: 090 ----
mean loss: 141.76
train mean loss: 133.96
epoch train time: 0:00:01.658845
elapsed time: 0:04:32.967939
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 20:29:04.123880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.30
 ---- batch: 020 ----
mean loss: 129.81
 ---- batch: 030 ----
mean loss: 132.66
 ---- batch: 040 ----
mean loss: 134.08
 ---- batch: 050 ----
mean loss: 129.93
 ---- batch: 060 ----
mean loss: 137.47
 ---- batch: 070 ----
mean loss: 133.84
 ---- batch: 080 ----
mean loss: 132.08
 ---- batch: 090 ----
mean loss: 136.21
train mean loss: 133.20
epoch train time: 0:00:01.644765
elapsed time: 0:04:34.613396
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 20:29:05.769353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.86
 ---- batch: 020 ----
mean loss: 131.84
 ---- batch: 030 ----
mean loss: 134.03
 ---- batch: 040 ----
mean loss: 130.49
 ---- batch: 050 ----
mean loss: 130.70
 ---- batch: 060 ----
mean loss: 134.52
 ---- batch: 070 ----
mean loss: 128.82
 ---- batch: 080 ----
mean loss: 141.95
 ---- batch: 090 ----
mean loss: 135.62
train mean loss: 132.67
epoch train time: 0:00:01.633690
elapsed time: 0:04:36.247773
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 20:29:07.403728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.69
 ---- batch: 020 ----
mean loss: 134.03
 ---- batch: 030 ----
mean loss: 128.32
 ---- batch: 040 ----
mean loss: 129.02
 ---- batch: 050 ----
mean loss: 130.16
 ---- batch: 060 ----
mean loss: 132.37
 ---- batch: 070 ----
mean loss: 138.92
 ---- batch: 080 ----
mean loss: 135.76
 ---- batch: 090 ----
mean loss: 136.01
train mean loss: 133.03
epoch train time: 0:00:01.615155
elapsed time: 0:04:37.863823
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 20:29:09.019603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.38
 ---- batch: 020 ----
mean loss: 126.83
 ---- batch: 030 ----
mean loss: 129.84
 ---- batch: 040 ----
mean loss: 130.83
 ---- batch: 050 ----
mean loss: 133.66
 ---- batch: 060 ----
mean loss: 140.00
 ---- batch: 070 ----
mean loss: 136.13
 ---- batch: 080 ----
mean loss: 132.16
 ---- batch: 090 ----
mean loss: 135.04
train mean loss: 132.19
epoch train time: 0:00:01.610320
elapsed time: 0:04:39.474656
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 20:29:10.630601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.45
 ---- batch: 020 ----
mean loss: 128.91
 ---- batch: 030 ----
mean loss: 130.15
 ---- batch: 040 ----
mean loss: 139.06
 ---- batch: 050 ----
mean loss: 129.70
 ---- batch: 060 ----
mean loss: 133.63
 ---- batch: 070 ----
mean loss: 124.30
 ---- batch: 080 ----
mean loss: 132.86
 ---- batch: 090 ----
mean loss: 132.23
train mean loss: 130.98
epoch train time: 0:00:01.623651
elapsed time: 0:04:41.098976
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 20:29:12.254925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.74
 ---- batch: 020 ----
mean loss: 131.87
 ---- batch: 030 ----
mean loss: 131.02
 ---- batch: 040 ----
mean loss: 130.08
 ---- batch: 050 ----
mean loss: 131.39
 ---- batch: 060 ----
mean loss: 133.59
 ---- batch: 070 ----
mean loss: 127.06
 ---- batch: 080 ----
mean loss: 130.94
 ---- batch: 090 ----
mean loss: 137.61
train mean loss: 131.34
epoch train time: 0:00:01.644426
elapsed time: 0:04:42.744137
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 20:29:13.900097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.57
 ---- batch: 020 ----
mean loss: 126.79
 ---- batch: 030 ----
mean loss: 132.47
 ---- batch: 040 ----
mean loss: 130.04
 ---- batch: 050 ----
mean loss: 125.39
 ---- batch: 060 ----
mean loss: 129.74
 ---- batch: 070 ----
mean loss: 131.43
 ---- batch: 080 ----
mean loss: 134.66
 ---- batch: 090 ----
mean loss: 136.35
train mean loss: 129.76
epoch train time: 0:00:01.653870
elapsed time: 0:04:44.398713
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 20:29:15.554667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.76
 ---- batch: 020 ----
mean loss: 121.56
 ---- batch: 030 ----
mean loss: 133.44
 ---- batch: 040 ----
mean loss: 129.26
 ---- batch: 050 ----
mean loss: 132.19
 ---- batch: 060 ----
mean loss: 133.76
 ---- batch: 070 ----
mean loss: 131.55
 ---- batch: 080 ----
mean loss: 130.51
 ---- batch: 090 ----
mean loss: 131.33
train mean loss: 129.75
epoch train time: 0:00:01.640318
elapsed time: 0:04:46.039743
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 20:29:17.195707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.08
 ---- batch: 020 ----
mean loss: 128.20
 ---- batch: 030 ----
mean loss: 121.59
 ---- batch: 040 ----
mean loss: 127.64
 ---- batch: 050 ----
mean loss: 133.23
 ---- batch: 060 ----
mean loss: 129.23
 ---- batch: 070 ----
mean loss: 132.38
 ---- batch: 080 ----
mean loss: 131.90
 ---- batch: 090 ----
mean loss: 131.11
train mean loss: 128.81
epoch train time: 0:00:01.672992
elapsed time: 0:04:47.713538
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 20:29:18.869541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.76
 ---- batch: 020 ----
mean loss: 126.23
 ---- batch: 030 ----
mean loss: 125.49
 ---- batch: 040 ----
mean loss: 128.73
 ---- batch: 050 ----
mean loss: 129.81
 ---- batch: 060 ----
mean loss: 132.58
 ---- batch: 070 ----
mean loss: 131.36
 ---- batch: 080 ----
mean loss: 127.06
 ---- batch: 090 ----
mean loss: 131.44
train mean loss: 128.38
epoch train time: 0:00:01.656310
elapsed time: 0:04:49.370587
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 20:29:20.526561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.89
 ---- batch: 020 ----
mean loss: 122.12
 ---- batch: 030 ----
mean loss: 133.37
 ---- batch: 040 ----
mean loss: 127.75
 ---- batch: 050 ----
mean loss: 126.55
 ---- batch: 060 ----
mean loss: 128.36
 ---- batch: 070 ----
mean loss: 130.39
 ---- batch: 080 ----
mean loss: 135.92
 ---- batch: 090 ----
mean loss: 130.12
train mean loss: 128.79
epoch train time: 0:00:01.668135
elapsed time: 0:04:51.039407
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 20:29:22.195381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.02
 ---- batch: 020 ----
mean loss: 128.62
 ---- batch: 030 ----
mean loss: 124.73
 ---- batch: 040 ----
mean loss: 129.42
 ---- batch: 050 ----
mean loss: 128.65
 ---- batch: 060 ----
mean loss: 128.46
 ---- batch: 070 ----
mean loss: 126.31
 ---- batch: 080 ----
mean loss: 130.76
 ---- batch: 090 ----
mean loss: 125.70
train mean loss: 127.86
epoch train time: 0:00:01.674724
elapsed time: 0:04:52.714857
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 20:29:23.870802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.10
 ---- batch: 020 ----
mean loss: 119.44
 ---- batch: 030 ----
mean loss: 128.56
 ---- batch: 040 ----
mean loss: 127.54
 ---- batch: 050 ----
mean loss: 127.58
 ---- batch: 060 ----
mean loss: 120.52
 ---- batch: 070 ----
mean loss: 125.80
 ---- batch: 080 ----
mean loss: 137.95
 ---- batch: 090 ----
mean loss: 126.74
train mean loss: 127.32
epoch train time: 0:00:01.635148
elapsed time: 0:04:54.350670
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 20:29:25.506619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.86
 ---- batch: 020 ----
mean loss: 123.03
 ---- batch: 030 ----
mean loss: 127.69
 ---- batch: 040 ----
mean loss: 129.39
 ---- batch: 050 ----
mean loss: 128.88
 ---- batch: 060 ----
mean loss: 126.25
 ---- batch: 070 ----
mean loss: 129.52
 ---- batch: 080 ----
mean loss: 126.87
 ---- batch: 090 ----
mean loss: 128.29
train mean loss: 126.67
epoch train time: 0:00:01.656456
elapsed time: 0:04:56.007773
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 20:29:27.163647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.71
 ---- batch: 020 ----
mean loss: 123.31
 ---- batch: 030 ----
mean loss: 126.50
 ---- batch: 040 ----
mean loss: 123.02
 ---- batch: 050 ----
mean loss: 122.88
 ---- batch: 060 ----
mean loss: 129.56
 ---- batch: 070 ----
mean loss: 132.53
 ---- batch: 080 ----
mean loss: 130.99
 ---- batch: 090 ----
mean loss: 124.69
train mean loss: 126.00
epoch train time: 0:00:01.621518
elapsed time: 0:04:57.629789
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 20:29:28.785763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.30
 ---- batch: 020 ----
mean loss: 122.60
 ---- batch: 030 ----
mean loss: 121.41
 ---- batch: 040 ----
mean loss: 123.87
 ---- batch: 050 ----
mean loss: 119.53
 ---- batch: 060 ----
mean loss: 126.46
 ---- batch: 070 ----
mean loss: 130.93
 ---- batch: 080 ----
mean loss: 126.98
 ---- batch: 090 ----
mean loss: 131.54
train mean loss: 125.49
epoch train time: 0:00:01.659570
elapsed time: 0:04:59.290040
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 20:29:30.445994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.58
 ---- batch: 020 ----
mean loss: 127.50
 ---- batch: 030 ----
mean loss: 125.57
 ---- batch: 040 ----
mean loss: 129.59
 ---- batch: 050 ----
mean loss: 123.95
 ---- batch: 060 ----
mean loss: 126.01
 ---- batch: 070 ----
mean loss: 131.08
 ---- batch: 080 ----
mean loss: 130.11
 ---- batch: 090 ----
mean loss: 123.25
train mean loss: 125.90
epoch train time: 0:00:01.682500
elapsed time: 0:05:00.973159
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 20:29:32.129157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.99
 ---- batch: 020 ----
mean loss: 124.65
 ---- batch: 030 ----
mean loss: 122.13
 ---- batch: 040 ----
mean loss: 122.33
 ---- batch: 050 ----
mean loss: 121.69
 ---- batch: 060 ----
mean loss: 129.51
 ---- batch: 070 ----
mean loss: 123.28
 ---- batch: 080 ----
mean loss: 122.38
 ---- batch: 090 ----
mean loss: 131.98
train mean loss: 124.74
epoch train time: 0:00:01.644434
elapsed time: 0:05:02.618325
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 20:29:33.774287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.13
 ---- batch: 020 ----
mean loss: 119.30
 ---- batch: 030 ----
mean loss: 120.06
 ---- batch: 040 ----
mean loss: 120.66
 ---- batch: 050 ----
mean loss: 126.03
 ---- batch: 060 ----
mean loss: 127.82
 ---- batch: 070 ----
mean loss: 126.46
 ---- batch: 080 ----
mean loss: 119.41
 ---- batch: 090 ----
mean loss: 129.11
train mean loss: 124.33
epoch train time: 0:00:01.666037
elapsed time: 0:05:04.284943
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 20:29:35.440864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.86
 ---- batch: 020 ----
mean loss: 125.75
 ---- batch: 030 ----
mean loss: 119.59
 ---- batch: 040 ----
mean loss: 125.46
 ---- batch: 050 ----
mean loss: 127.64
 ---- batch: 060 ----
mean loss: 127.54
 ---- batch: 070 ----
mean loss: 123.25
 ---- batch: 080 ----
mean loss: 123.57
 ---- batch: 090 ----
mean loss: 121.96
train mean loss: 124.16
epoch train time: 0:00:01.687146
elapsed time: 0:05:05.972703
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 20:29:37.128629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.14
 ---- batch: 020 ----
mean loss: 123.77
 ---- batch: 030 ----
mean loss: 123.16
 ---- batch: 040 ----
mean loss: 124.84
 ---- batch: 050 ----
mean loss: 118.96
 ---- batch: 060 ----
mean loss: 122.74
 ---- batch: 070 ----
mean loss: 120.40
 ---- batch: 080 ----
mean loss: 124.60
 ---- batch: 090 ----
mean loss: 127.18
train mean loss: 123.97
epoch train time: 0:00:01.680908
elapsed time: 0:05:07.654228
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 20:29:38.810185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.48
 ---- batch: 020 ----
mean loss: 123.76
 ---- batch: 030 ----
mean loss: 124.55
 ---- batch: 040 ----
mean loss: 115.54
 ---- batch: 050 ----
mean loss: 124.63
 ---- batch: 060 ----
mean loss: 120.04
 ---- batch: 070 ----
mean loss: 131.73
 ---- batch: 080 ----
mean loss: 121.65
 ---- batch: 090 ----
mean loss: 123.69
train mean loss: 123.20
epoch train time: 0:00:01.680780
elapsed time: 0:05:09.335612
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 20:29:40.491532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.10
 ---- batch: 020 ----
mean loss: 123.40
 ---- batch: 030 ----
mean loss: 117.31
 ---- batch: 040 ----
mean loss: 114.63
 ---- batch: 050 ----
mean loss: 122.15
 ---- batch: 060 ----
mean loss: 128.95
 ---- batch: 070 ----
mean loss: 124.23
 ---- batch: 080 ----
mean loss: 124.68
 ---- batch: 090 ----
mean loss: 125.46
train mean loss: 122.60
epoch train time: 0:00:01.642728
elapsed time: 0:05:10.978969
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 20:29:42.134933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.48
 ---- batch: 020 ----
mean loss: 117.29
 ---- batch: 030 ----
mean loss: 115.77
 ---- batch: 040 ----
mean loss: 124.64
 ---- batch: 050 ----
mean loss: 116.90
 ---- batch: 060 ----
mean loss: 122.99
 ---- batch: 070 ----
mean loss: 124.61
 ---- batch: 080 ----
mean loss: 127.20
 ---- batch: 090 ----
mean loss: 122.39
train mean loss: 121.64
epoch train time: 0:00:01.634300
elapsed time: 0:05:12.614029
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 20:29:43.769962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.11
 ---- batch: 020 ----
mean loss: 119.43
 ---- batch: 030 ----
mean loss: 121.14
 ---- batch: 040 ----
mean loss: 118.18
 ---- batch: 050 ----
mean loss: 126.55
 ---- batch: 060 ----
mean loss: 120.40
 ---- batch: 070 ----
mean loss: 129.91
 ---- batch: 080 ----
mean loss: 120.50
 ---- batch: 090 ----
mean loss: 124.65
train mean loss: 121.78
epoch train time: 0:00:01.639817
elapsed time: 0:05:14.254439
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 20:29:45.410387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.51
 ---- batch: 020 ----
mean loss: 115.08
 ---- batch: 030 ----
mean loss: 121.81
 ---- batch: 040 ----
mean loss: 116.96
 ---- batch: 050 ----
mean loss: 119.77
 ---- batch: 060 ----
mean loss: 119.48
 ---- batch: 070 ----
mean loss: 121.53
 ---- batch: 080 ----
mean loss: 122.19
 ---- batch: 090 ----
mean loss: 126.37
train mean loss: 120.74
epoch train time: 0:00:01.642165
elapsed time: 0:05:15.897296
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 20:29:47.053252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.91
 ---- batch: 020 ----
mean loss: 118.54
 ---- batch: 030 ----
mean loss: 120.04
 ---- batch: 040 ----
mean loss: 118.49
 ---- batch: 050 ----
mean loss: 119.20
 ---- batch: 060 ----
mean loss: 119.77
 ---- batch: 070 ----
mean loss: 120.41
 ---- batch: 080 ----
mean loss: 122.18
 ---- batch: 090 ----
mean loss: 124.67
train mean loss: 121.11
epoch train time: 0:00:01.654520
elapsed time: 0:05:17.552486
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 20:29:48.708433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.45
 ---- batch: 020 ----
mean loss: 114.92
 ---- batch: 030 ----
mean loss: 120.43
 ---- batch: 040 ----
mean loss: 122.93
 ---- batch: 050 ----
mean loss: 118.21
 ---- batch: 060 ----
mean loss: 128.47
 ---- batch: 070 ----
mean loss: 123.83
 ---- batch: 080 ----
mean loss: 123.27
 ---- batch: 090 ----
mean loss: 122.88
train mean loss: 120.91
epoch train time: 0:00:01.638885
elapsed time: 0:05:19.192250
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 20:29:50.347944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.23
 ---- batch: 020 ----
mean loss: 118.43
 ---- batch: 030 ----
mean loss: 123.96
 ---- batch: 040 ----
mean loss: 118.67
 ---- batch: 050 ----
mean loss: 116.72
 ---- batch: 060 ----
mean loss: 122.20
 ---- batch: 070 ----
mean loss: 122.04
 ---- batch: 080 ----
mean loss: 117.09
 ---- batch: 090 ----
mean loss: 122.43
train mean loss: 119.82
epoch train time: 0:00:01.647657
elapsed time: 0:05:20.840313
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 20:29:51.996249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.62
 ---- batch: 020 ----
mean loss: 115.27
 ---- batch: 030 ----
mean loss: 122.12
 ---- batch: 040 ----
mean loss: 117.50
 ---- batch: 050 ----
mean loss: 122.13
 ---- batch: 060 ----
mean loss: 125.04
 ---- batch: 070 ----
mean loss: 124.74
 ---- batch: 080 ----
mean loss: 119.72
 ---- batch: 090 ----
mean loss: 118.05
train mean loss: 119.98
epoch train time: 0:00:01.636522
elapsed time: 0:05:22.477409
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 20:29:53.633335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.62
 ---- batch: 020 ----
mean loss: 114.74
 ---- batch: 030 ----
mean loss: 119.13
 ---- batch: 040 ----
mean loss: 120.53
 ---- batch: 050 ----
mean loss: 121.75
 ---- batch: 060 ----
mean loss: 120.97
 ---- batch: 070 ----
mean loss: 120.13
 ---- batch: 080 ----
mean loss: 116.76
 ---- batch: 090 ----
mean loss: 123.18
train mean loss: 118.94
epoch train time: 0:00:01.657668
elapsed time: 0:05:24.135644
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 20:29:55.291571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.62
 ---- batch: 020 ----
mean loss: 113.28
 ---- batch: 030 ----
mean loss: 119.42
 ---- batch: 040 ----
mean loss: 114.33
 ---- batch: 050 ----
mean loss: 118.77
 ---- batch: 060 ----
mean loss: 121.51
 ---- batch: 070 ----
mean loss: 122.09
 ---- batch: 080 ----
mean loss: 117.23
 ---- batch: 090 ----
mean loss: 114.39
train mean loss: 117.82
epoch train time: 0:00:01.636094
elapsed time: 0:05:25.772407
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 20:29:56.928359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.48
 ---- batch: 020 ----
mean loss: 112.66
 ---- batch: 030 ----
mean loss: 114.85
 ---- batch: 040 ----
mean loss: 117.67
 ---- batch: 050 ----
mean loss: 118.74
 ---- batch: 060 ----
mean loss: 120.58
 ---- batch: 070 ----
mean loss: 117.09
 ---- batch: 080 ----
mean loss: 121.53
 ---- batch: 090 ----
mean loss: 122.59
train mean loss: 118.25
epoch train time: 0:00:01.640887
elapsed time: 0:05:27.413947
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 20:29:58.569859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.75
 ---- batch: 020 ----
mean loss: 116.85
 ---- batch: 030 ----
mean loss: 116.83
 ---- batch: 040 ----
mean loss: 115.64
 ---- batch: 050 ----
mean loss: 113.37
 ---- batch: 060 ----
mean loss: 118.70
 ---- batch: 070 ----
mean loss: 115.29
 ---- batch: 080 ----
mean loss: 120.62
 ---- batch: 090 ----
mean loss: 121.29
train mean loss: 117.58
epoch train time: 0:00:01.679659
elapsed time: 0:05:29.094196
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 20:30:00.250149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.31
 ---- batch: 020 ----
mean loss: 114.35
 ---- batch: 030 ----
mean loss: 115.40
 ---- batch: 040 ----
mean loss: 117.72
 ---- batch: 050 ----
mean loss: 118.31
 ---- batch: 060 ----
mean loss: 110.14
 ---- batch: 070 ----
mean loss: 119.58
 ---- batch: 080 ----
mean loss: 120.87
 ---- batch: 090 ----
mean loss: 120.26
train mean loss: 117.17
epoch train time: 0:00:01.657589
elapsed time: 0:05:30.752450
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 20:30:01.908387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.13
 ---- batch: 020 ----
mean loss: 114.75
 ---- batch: 030 ----
mean loss: 113.76
 ---- batch: 040 ----
mean loss: 115.36
 ---- batch: 050 ----
mean loss: 115.48
 ---- batch: 060 ----
mean loss: 122.17
 ---- batch: 070 ----
mean loss: 116.07
 ---- batch: 080 ----
mean loss: 119.14
 ---- batch: 090 ----
mean loss: 118.06
train mean loss: 116.85
epoch train time: 0:00:01.640297
elapsed time: 0:05:32.393348
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 20:30:03.549296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.98
 ---- batch: 020 ----
mean loss: 113.92
 ---- batch: 030 ----
mean loss: 116.50
 ---- batch: 040 ----
mean loss: 118.75
 ---- batch: 050 ----
mean loss: 118.09
 ---- batch: 060 ----
mean loss: 117.71
 ---- batch: 070 ----
mean loss: 119.08
 ---- batch: 080 ----
mean loss: 115.61
 ---- batch: 090 ----
mean loss: 113.78
train mean loss: 116.31
epoch train time: 0:00:01.667132
elapsed time: 0:05:34.061106
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 20:30:05.217091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.04
 ---- batch: 020 ----
mean loss: 112.50
 ---- batch: 030 ----
mean loss: 109.47
 ---- batch: 040 ----
mean loss: 119.54
 ---- batch: 050 ----
mean loss: 117.07
 ---- batch: 060 ----
mean loss: 115.29
 ---- batch: 070 ----
mean loss: 114.32
 ---- batch: 080 ----
mean loss: 122.17
 ---- batch: 090 ----
mean loss: 114.74
train mean loss: 115.64
epoch train time: 0:00:01.693249
elapsed time: 0:05:35.755034
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 20:30:06.910987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.83
 ---- batch: 020 ----
mean loss: 113.59
 ---- batch: 030 ----
mean loss: 109.68
 ---- batch: 040 ----
mean loss: 114.15
 ---- batch: 050 ----
mean loss: 119.96
 ---- batch: 060 ----
mean loss: 119.19
 ---- batch: 070 ----
mean loss: 111.47
 ---- batch: 080 ----
mean loss: 116.42
 ---- batch: 090 ----
mean loss: 119.02
train mean loss: 115.56
epoch train time: 0:00:01.653385
elapsed time: 0:05:37.409025
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 20:30:08.564976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.61
 ---- batch: 020 ----
mean loss: 108.83
 ---- batch: 030 ----
mean loss: 109.43
 ---- batch: 040 ----
mean loss: 117.19
 ---- batch: 050 ----
mean loss: 116.27
 ---- batch: 060 ----
mean loss: 114.09
 ---- batch: 070 ----
mean loss: 118.80
 ---- batch: 080 ----
mean loss: 115.81
 ---- batch: 090 ----
mean loss: 121.61
train mean loss: 114.90
epoch train time: 0:00:01.626447
elapsed time: 0:05:39.036058
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 20:30:10.192037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.36
 ---- batch: 020 ----
mean loss: 109.03
 ---- batch: 030 ----
mean loss: 114.58
 ---- batch: 040 ----
mean loss: 115.95
 ---- batch: 050 ----
mean loss: 113.34
 ---- batch: 060 ----
mean loss: 111.98
 ---- batch: 070 ----
mean loss: 119.61
 ---- batch: 080 ----
mean loss: 112.51
 ---- batch: 090 ----
mean loss: 117.74
train mean loss: 114.76
epoch train time: 0:00:01.641077
elapsed time: 0:05:40.677840
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 20:30:11.833823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.85
 ---- batch: 020 ----
mean loss: 115.43
 ---- batch: 030 ----
mean loss: 110.07
 ---- batch: 040 ----
mean loss: 114.01
 ---- batch: 050 ----
mean loss: 114.52
 ---- batch: 060 ----
mean loss: 119.95
 ---- batch: 070 ----
mean loss: 110.46
 ---- batch: 080 ----
mean loss: 111.99
 ---- batch: 090 ----
mean loss: 120.35
train mean loss: 114.61
epoch train time: 0:00:01.635572
elapsed time: 0:05:42.314093
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 20:30:13.470048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.49
 ---- batch: 020 ----
mean loss: 113.17
 ---- batch: 030 ----
mean loss: 109.81
 ---- batch: 040 ----
mean loss: 110.91
 ---- batch: 050 ----
mean loss: 116.63
 ---- batch: 060 ----
mean loss: 119.80
 ---- batch: 070 ----
mean loss: 113.35
 ---- batch: 080 ----
mean loss: 121.01
 ---- batch: 090 ----
mean loss: 114.88
train mean loss: 113.90
epoch train time: 0:00:01.634198
elapsed time: 0:05:43.948882
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 20:30:15.104807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.10
 ---- batch: 020 ----
mean loss: 112.10
 ---- batch: 030 ----
mean loss: 115.26
 ---- batch: 040 ----
mean loss: 112.36
 ---- batch: 050 ----
mean loss: 111.16
 ---- batch: 060 ----
mean loss: 110.49
 ---- batch: 070 ----
mean loss: 114.62
 ---- batch: 080 ----
mean loss: 118.58
 ---- batch: 090 ----
mean loss: 117.92
train mean loss: 113.45
epoch train time: 0:00:01.649443
elapsed time: 0:05:45.599140
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 20:30:16.755055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.09
 ---- batch: 020 ----
mean loss: 117.20
 ---- batch: 030 ----
mean loss: 110.03
 ---- batch: 040 ----
mean loss: 115.00
 ---- batch: 050 ----
mean loss: 114.54
 ---- batch: 060 ----
mean loss: 115.37
 ---- batch: 070 ----
mean loss: 114.60
 ---- batch: 080 ----
mean loss: 118.07
 ---- batch: 090 ----
mean loss: 109.04
train mean loss: 113.63
epoch train time: 0:00:01.649537
elapsed time: 0:05:47.249270
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 20:30:18.405213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.53
 ---- batch: 020 ----
mean loss: 109.37
 ---- batch: 030 ----
mean loss: 110.35
 ---- batch: 040 ----
mean loss: 110.31
 ---- batch: 050 ----
mean loss: 116.21
 ---- batch: 060 ----
mean loss: 114.92
 ---- batch: 070 ----
mean loss: 119.84
 ---- batch: 080 ----
mean loss: 113.41
 ---- batch: 090 ----
mean loss: 115.43
train mean loss: 113.28
epoch train time: 0:00:01.648311
elapsed time: 0:05:48.898289
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 20:30:20.054240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.71
 ---- batch: 020 ----
mean loss: 110.53
 ---- batch: 030 ----
mean loss: 116.91
 ---- batch: 040 ----
mean loss: 111.14
 ---- batch: 050 ----
mean loss: 113.21
 ---- batch: 060 ----
mean loss: 114.99
 ---- batch: 070 ----
mean loss: 113.31
 ---- batch: 080 ----
mean loss: 106.57
 ---- batch: 090 ----
mean loss: 113.33
train mean loss: 112.65
epoch train time: 0:00:01.648819
elapsed time: 0:05:50.547716
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 20:30:21.703674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.97
 ---- batch: 020 ----
mean loss: 105.22
 ---- batch: 030 ----
mean loss: 116.02
 ---- batch: 040 ----
mean loss: 107.56
 ---- batch: 050 ----
mean loss: 111.20
 ---- batch: 060 ----
mean loss: 108.57
 ---- batch: 070 ----
mean loss: 111.80
 ---- batch: 080 ----
mean loss: 116.01
 ---- batch: 090 ----
mean loss: 121.78
train mean loss: 112.16
epoch train time: 0:00:01.627041
elapsed time: 0:05:52.175414
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 20:30:23.331436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.04
 ---- batch: 020 ----
mean loss: 108.33
 ---- batch: 030 ----
mean loss: 110.96
 ---- batch: 040 ----
mean loss: 109.56
 ---- batch: 050 ----
mean loss: 109.88
 ---- batch: 060 ----
mean loss: 115.94
 ---- batch: 070 ----
mean loss: 112.19
 ---- batch: 080 ----
mean loss: 115.93
 ---- batch: 090 ----
mean loss: 115.09
train mean loss: 111.71
epoch train time: 0:00:01.675436
elapsed time: 0:05:53.851574
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 20:30:25.007535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.78
 ---- batch: 020 ----
mean loss: 109.07
 ---- batch: 030 ----
mean loss: 113.66
 ---- batch: 040 ----
mean loss: 112.04
 ---- batch: 050 ----
mean loss: 109.44
 ---- batch: 060 ----
mean loss: 112.78
 ---- batch: 070 ----
mean loss: 116.28
 ---- batch: 080 ----
mean loss: 115.05
 ---- batch: 090 ----
mean loss: 112.84
train mean loss: 111.67
epoch train time: 0:00:01.653663
elapsed time: 0:05:55.505911
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 20:30:26.661867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.17
 ---- batch: 020 ----
mean loss: 106.88
 ---- batch: 030 ----
mean loss: 112.00
 ---- batch: 040 ----
mean loss: 108.88
 ---- batch: 050 ----
mean loss: 103.27
 ---- batch: 060 ----
mean loss: 114.02
 ---- batch: 070 ----
mean loss: 111.78
 ---- batch: 080 ----
mean loss: 108.29
 ---- batch: 090 ----
mean loss: 118.24
train mean loss: 110.90
epoch train time: 0:00:01.665556
elapsed time: 0:05:57.172099
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 20:30:28.328059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.78
 ---- batch: 020 ----
mean loss: 113.30
 ---- batch: 030 ----
mean loss: 108.31
 ---- batch: 040 ----
mean loss: 107.94
 ---- batch: 050 ----
mean loss: 109.53
 ---- batch: 060 ----
mean loss: 110.86
 ---- batch: 070 ----
mean loss: 109.90
 ---- batch: 080 ----
mean loss: 109.34
 ---- batch: 090 ----
mean loss: 119.06
train mean loss: 110.57
epoch train time: 0:00:01.637340
elapsed time: 0:05:58.810113
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 20:30:29.966061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.15
 ---- batch: 020 ----
mean loss: 105.17
 ---- batch: 030 ----
mean loss: 109.57
 ---- batch: 040 ----
mean loss: 109.24
 ---- batch: 050 ----
mean loss: 110.58
 ---- batch: 060 ----
mean loss: 114.70
 ---- batch: 070 ----
mean loss: 109.94
 ---- batch: 080 ----
mean loss: 116.79
 ---- batch: 090 ----
mean loss: 113.76
train mean loss: 110.11
epoch train time: 0:00:01.656225
elapsed time: 0:06:00.466958
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 20:30:31.622930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.43
 ---- batch: 020 ----
mean loss: 110.80
 ---- batch: 030 ----
mean loss: 105.83
 ---- batch: 040 ----
mean loss: 104.72
 ---- batch: 050 ----
mean loss: 109.52
 ---- batch: 060 ----
mean loss: 113.20
 ---- batch: 070 ----
mean loss: 110.91
 ---- batch: 080 ----
mean loss: 114.55
 ---- batch: 090 ----
mean loss: 115.31
train mean loss: 110.39
epoch train time: 0:00:01.669980
elapsed time: 0:06:02.137630
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 20:30:33.293577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.92
 ---- batch: 020 ----
mean loss: 108.76
 ---- batch: 030 ----
mean loss: 104.13
 ---- batch: 040 ----
mean loss: 109.29
 ---- batch: 050 ----
mean loss: 104.03
 ---- batch: 060 ----
mean loss: 111.17
 ---- batch: 070 ----
mean loss: 113.39
 ---- batch: 080 ----
mean loss: 113.16
 ---- batch: 090 ----
mean loss: 115.97
train mean loss: 109.59
epoch train time: 0:00:01.638744
elapsed time: 0:06:03.777039
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 20:30:34.933031
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.43
 ---- batch: 020 ----
mean loss: 105.89
 ---- batch: 030 ----
mean loss: 103.87
 ---- batch: 040 ----
mean loss: 105.91
 ---- batch: 050 ----
mean loss: 99.87
 ---- batch: 060 ----
mean loss: 103.95
 ---- batch: 070 ----
mean loss: 105.64
 ---- batch: 080 ----
mean loss: 100.56
 ---- batch: 090 ----
mean loss: 105.99
train mean loss: 104.25
epoch train time: 0:00:01.644480
elapsed time: 0:06:05.422466
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 20:30:36.578157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.88
 ---- batch: 020 ----
mean loss: 99.11
 ---- batch: 030 ----
mean loss: 101.58
 ---- batch: 040 ----
mean loss: 101.09
 ---- batch: 050 ----
mean loss: 103.56
 ---- batch: 060 ----
mean loss: 106.51
 ---- batch: 070 ----
mean loss: 106.29
 ---- batch: 080 ----
mean loss: 101.89
 ---- batch: 090 ----
mean loss: 100.71
train mean loss: 103.45
epoch train time: 0:00:01.644136
elapsed time: 0:06:07.066963
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 20:30:38.222887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.15
 ---- batch: 020 ----
mean loss: 104.71
 ---- batch: 030 ----
mean loss: 100.84
 ---- batch: 040 ----
mean loss: 105.60
 ---- batch: 050 ----
mean loss: 105.84
 ---- batch: 060 ----
mean loss: 98.79
 ---- batch: 070 ----
mean loss: 104.22
 ---- batch: 080 ----
mean loss: 104.34
 ---- batch: 090 ----
mean loss: 101.45
train mean loss: 103.24
epoch train time: 0:00:01.637237
elapsed time: 0:06:08.704808
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 20:30:39.860753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.68
 ---- batch: 020 ----
mean loss: 100.92
 ---- batch: 030 ----
mean loss: 106.81
 ---- batch: 040 ----
mean loss: 97.78
 ---- batch: 050 ----
mean loss: 105.95
 ---- batch: 060 ----
mean loss: 103.94
 ---- batch: 070 ----
mean loss: 98.68
 ---- batch: 080 ----
mean loss: 108.42
 ---- batch: 090 ----
mean loss: 104.38
train mean loss: 102.78
epoch train time: 0:00:01.636258
elapsed time: 0:06:10.341668
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 20:30:41.497644
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.02
 ---- batch: 020 ----
mean loss: 104.77
 ---- batch: 030 ----
mean loss: 102.44
 ---- batch: 040 ----
mean loss: 100.44
 ---- batch: 050 ----
mean loss: 104.84
 ---- batch: 060 ----
mean loss: 103.13
 ---- batch: 070 ----
mean loss: 104.12
 ---- batch: 080 ----
mean loss: 106.41
 ---- batch: 090 ----
mean loss: 101.33
train mean loss: 102.78
epoch train time: 0:00:01.660173
elapsed time: 0:06:12.002554
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 20:30:43.158533
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.05
 ---- batch: 020 ----
mean loss: 97.61
 ---- batch: 030 ----
mean loss: 103.04
 ---- batch: 040 ----
mean loss: 103.52
 ---- batch: 050 ----
mean loss: 105.97
 ---- batch: 060 ----
mean loss: 107.21
 ---- batch: 070 ----
mean loss: 103.29
 ---- batch: 080 ----
mean loss: 103.92
 ---- batch: 090 ----
mean loss: 101.13
train mean loss: 102.81
epoch train time: 0:00:01.680255
elapsed time: 0:06:13.683509
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 20:30:44.839448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.45
 ---- batch: 020 ----
mean loss: 102.77
 ---- batch: 030 ----
mean loss: 99.89
 ---- batch: 040 ----
mean loss: 103.64
 ---- batch: 050 ----
mean loss: 104.68
 ---- batch: 060 ----
mean loss: 95.12
 ---- batch: 070 ----
mean loss: 101.83
 ---- batch: 080 ----
mean loss: 105.56
 ---- batch: 090 ----
mean loss: 104.74
train mean loss: 102.74
epoch train time: 0:00:01.648501
elapsed time: 0:06:15.332637
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 20:30:46.488598
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.91
 ---- batch: 020 ----
mean loss: 106.10
 ---- batch: 030 ----
mean loss: 99.31
 ---- batch: 040 ----
mean loss: 103.77
 ---- batch: 050 ----
mean loss: 104.83
 ---- batch: 060 ----
mean loss: 101.08
 ---- batch: 070 ----
mean loss: 100.09
 ---- batch: 080 ----
mean loss: 105.97
 ---- batch: 090 ----
mean loss: 100.37
train mean loss: 102.55
epoch train time: 0:00:01.652847
elapsed time: 0:06:16.986166
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 20:30:48.142183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.42
 ---- batch: 020 ----
mean loss: 103.41
 ---- batch: 030 ----
mean loss: 101.80
 ---- batch: 040 ----
mean loss: 100.59
 ---- batch: 050 ----
mean loss: 105.74
 ---- batch: 060 ----
mean loss: 106.16
 ---- batch: 070 ----
mean loss: 101.07
 ---- batch: 080 ----
mean loss: 98.37
 ---- batch: 090 ----
mean loss: 102.60
train mean loss: 102.55
epoch train time: 0:00:01.625373
elapsed time: 0:06:18.612286
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 20:30:49.768267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.46
 ---- batch: 020 ----
mean loss: 101.01
 ---- batch: 030 ----
mean loss: 104.05
 ---- batch: 040 ----
mean loss: 103.90
 ---- batch: 050 ----
mean loss: 100.65
 ---- batch: 060 ----
mean loss: 105.10
 ---- batch: 070 ----
mean loss: 104.42
 ---- batch: 080 ----
mean loss: 103.19
 ---- batch: 090 ----
mean loss: 102.23
train mean loss: 102.55
epoch train time: 0:00:01.647636
elapsed time: 0:06:20.260571
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 20:30:51.416533
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.42
 ---- batch: 020 ----
mean loss: 99.13
 ---- batch: 030 ----
mean loss: 101.46
 ---- batch: 040 ----
mean loss: 102.20
 ---- batch: 050 ----
mean loss: 101.84
 ---- batch: 060 ----
mean loss: 103.38
 ---- batch: 070 ----
mean loss: 105.44
 ---- batch: 080 ----
mean loss: 99.18
 ---- batch: 090 ----
mean loss: 102.17
train mean loss: 102.53
epoch train time: 0:00:01.652921
elapsed time: 0:06:21.914164
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 20:30:53.070108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.90
 ---- batch: 020 ----
mean loss: 101.00
 ---- batch: 030 ----
mean loss: 99.29
 ---- batch: 040 ----
mean loss: 104.95
 ---- batch: 050 ----
mean loss: 104.35
 ---- batch: 060 ----
mean loss: 101.99
 ---- batch: 070 ----
mean loss: 101.79
 ---- batch: 080 ----
mean loss: 101.74
 ---- batch: 090 ----
mean loss: 99.01
train mean loss: 102.48
epoch train time: 0:00:01.634570
elapsed time: 0:06:23.549400
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 20:30:54.705378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.44
 ---- batch: 020 ----
mean loss: 101.85
 ---- batch: 030 ----
mean loss: 99.67
 ---- batch: 040 ----
mean loss: 102.80
 ---- batch: 050 ----
mean loss: 105.86
 ---- batch: 060 ----
mean loss: 104.37
 ---- batch: 070 ----
mean loss: 103.00
 ---- batch: 080 ----
mean loss: 100.96
 ---- batch: 090 ----
mean loss: 100.65
train mean loss: 102.38
epoch train time: 0:00:01.642584
elapsed time: 0:06:25.192627
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 20:30:56.348562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.86
 ---- batch: 020 ----
mean loss: 94.34
 ---- batch: 030 ----
mean loss: 100.03
 ---- batch: 040 ----
mean loss: 104.89
 ---- batch: 050 ----
mean loss: 101.80
 ---- batch: 060 ----
mean loss: 100.68
 ---- batch: 070 ----
mean loss: 102.08
 ---- batch: 080 ----
mean loss: 108.29
 ---- batch: 090 ----
mean loss: 106.45
train mean loss: 102.47
epoch train time: 0:00:01.672848
elapsed time: 0:06:26.866124
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 20:30:58.022157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.15
 ---- batch: 020 ----
mean loss: 100.52
 ---- batch: 030 ----
mean loss: 103.08
 ---- batch: 040 ----
mean loss: 97.46
 ---- batch: 050 ----
mean loss: 101.12
 ---- batch: 060 ----
mean loss: 102.91
 ---- batch: 070 ----
mean loss: 99.80
 ---- batch: 080 ----
mean loss: 108.47
 ---- batch: 090 ----
mean loss: 100.63
train mean loss: 102.44
epoch train time: 0:00:01.684193
elapsed time: 0:06:28.551017
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 20:30:59.706982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.84
 ---- batch: 020 ----
mean loss: 99.73
 ---- batch: 030 ----
mean loss: 104.16
 ---- batch: 040 ----
mean loss: 105.54
 ---- batch: 050 ----
mean loss: 101.72
 ---- batch: 060 ----
mean loss: 99.48
 ---- batch: 070 ----
mean loss: 104.63
 ---- batch: 080 ----
mean loss: 102.23
 ---- batch: 090 ----
mean loss: 103.67
train mean loss: 102.08
epoch train time: 0:00:01.646504
elapsed time: 0:06:30.198192
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 20:31:01.354137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.17
 ---- batch: 020 ----
mean loss: 103.57
 ---- batch: 030 ----
mean loss: 101.49
 ---- batch: 040 ----
mean loss: 99.47
 ---- batch: 050 ----
mean loss: 100.60
 ---- batch: 060 ----
mean loss: 101.32
 ---- batch: 070 ----
mean loss: 103.33
 ---- batch: 080 ----
mean loss: 106.88
 ---- batch: 090 ----
mean loss: 104.27
train mean loss: 102.27
epoch train time: 0:00:01.680101
elapsed time: 0:06:31.878975
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 20:31:03.034915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.13
 ---- batch: 020 ----
mean loss: 98.22
 ---- batch: 030 ----
mean loss: 108.35
 ---- batch: 040 ----
mean loss: 98.59
 ---- batch: 050 ----
mean loss: 96.83
 ---- batch: 060 ----
mean loss: 104.39
 ---- batch: 070 ----
mean loss: 99.60
 ---- batch: 080 ----
mean loss: 102.66
 ---- batch: 090 ----
mean loss: 109.68
train mean loss: 102.11
epoch train time: 0:00:01.626878
elapsed time: 0:06:33.506501
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 20:31:04.662442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.21
 ---- batch: 020 ----
mean loss: 102.16
 ---- batch: 030 ----
mean loss: 103.01
 ---- batch: 040 ----
mean loss: 108.45
 ---- batch: 050 ----
mean loss: 104.88
 ---- batch: 060 ----
mean loss: 93.29
 ---- batch: 070 ----
mean loss: 100.35
 ---- batch: 080 ----
mean loss: 100.84
 ---- batch: 090 ----
mean loss: 103.17
train mean loss: 102.04
epoch train time: 0:00:01.678191
elapsed time: 0:06:35.185301
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 20:31:06.341242
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.24
 ---- batch: 020 ----
mean loss: 99.97
 ---- batch: 030 ----
mean loss: 99.48
 ---- batch: 040 ----
mean loss: 103.55
 ---- batch: 050 ----
mean loss: 101.31
 ---- batch: 060 ----
mean loss: 104.25
 ---- batch: 070 ----
mean loss: 98.58
 ---- batch: 080 ----
mean loss: 106.70
 ---- batch: 090 ----
mean loss: 100.17
train mean loss: 101.89
epoch train time: 0:00:01.681327
elapsed time: 0:06:36.867294
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 20:31:08.023257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.12
 ---- batch: 020 ----
mean loss: 99.48
 ---- batch: 030 ----
mean loss: 100.73
 ---- batch: 040 ----
mean loss: 100.50
 ---- batch: 050 ----
mean loss: 102.49
 ---- batch: 060 ----
mean loss: 108.61
 ---- batch: 070 ----
mean loss: 102.64
 ---- batch: 080 ----
mean loss: 101.85
 ---- batch: 090 ----
mean loss: 100.36
train mean loss: 101.90
epoch train time: 0:00:01.679606
elapsed time: 0:06:38.547636
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 20:31:09.703576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.20
 ---- batch: 020 ----
mean loss: 103.65
 ---- batch: 030 ----
mean loss: 97.50
 ---- batch: 040 ----
mean loss: 100.35
 ---- batch: 050 ----
mean loss: 99.38
 ---- batch: 060 ----
mean loss: 105.89
 ---- batch: 070 ----
mean loss: 98.67
 ---- batch: 080 ----
mean loss: 107.33
 ---- batch: 090 ----
mean loss: 102.59
train mean loss: 101.96
epoch train time: 0:00:01.701704
elapsed time: 0:06:40.249999
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 20:31:11.405769
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.57
 ---- batch: 020 ----
mean loss: 103.89
 ---- batch: 030 ----
mean loss: 102.16
 ---- batch: 040 ----
mean loss: 100.21
 ---- batch: 050 ----
mean loss: 101.99
 ---- batch: 060 ----
mean loss: 97.19
 ---- batch: 070 ----
mean loss: 104.36
 ---- batch: 080 ----
mean loss: 103.35
 ---- batch: 090 ----
mean loss: 104.05
train mean loss: 101.93
epoch train time: 0:00:01.672285
elapsed time: 0:06:41.922741
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 20:31:13.078765
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.79
 ---- batch: 020 ----
mean loss: 93.95
 ---- batch: 030 ----
mean loss: 102.40
 ---- batch: 040 ----
mean loss: 98.20
 ---- batch: 050 ----
mean loss: 97.93
 ---- batch: 060 ----
mean loss: 104.68
 ---- batch: 070 ----
mean loss: 100.33
 ---- batch: 080 ----
mean loss: 106.88
 ---- batch: 090 ----
mean loss: 106.14
train mean loss: 101.95
epoch train time: 0:00:01.688568
elapsed time: 0:06:43.612051
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 20:31:14.768008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.67
 ---- batch: 020 ----
mean loss: 105.02
 ---- batch: 030 ----
mean loss: 97.99
 ---- batch: 040 ----
mean loss: 103.80
 ---- batch: 050 ----
mean loss: 105.34
 ---- batch: 060 ----
mean loss: 100.00
 ---- batch: 070 ----
mean loss: 96.90
 ---- batch: 080 ----
mean loss: 99.92
 ---- batch: 090 ----
mean loss: 103.10
train mean loss: 101.91
epoch train time: 0:00:01.644108
elapsed time: 0:06:45.256814
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 20:31:16.412771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.00
 ---- batch: 020 ----
mean loss: 106.46
 ---- batch: 030 ----
mean loss: 100.86
 ---- batch: 040 ----
mean loss: 96.28
 ---- batch: 050 ----
mean loss: 99.82
 ---- batch: 060 ----
mean loss: 104.13
 ---- batch: 070 ----
mean loss: 103.96
 ---- batch: 080 ----
mean loss: 103.50
 ---- batch: 090 ----
mean loss: 103.87
train mean loss: 101.72
epoch train time: 0:00:01.629150
elapsed time: 0:06:46.886628
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 20:31:18.042591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.76
 ---- batch: 020 ----
mean loss: 103.93
 ---- batch: 030 ----
mean loss: 103.03
 ---- batch: 040 ----
mean loss: 105.62
 ---- batch: 050 ----
mean loss: 106.09
 ---- batch: 060 ----
mean loss: 102.11
 ---- batch: 070 ----
mean loss: 103.56
 ---- batch: 080 ----
mean loss: 100.28
 ---- batch: 090 ----
mean loss: 96.02
train mean loss: 101.75
epoch train time: 0:00:01.647403
elapsed time: 0:06:48.534685
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 20:31:19.690637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.79
 ---- batch: 020 ----
mean loss: 99.57
 ---- batch: 030 ----
mean loss: 103.94
 ---- batch: 040 ----
mean loss: 104.11
 ---- batch: 050 ----
mean loss: 103.77
 ---- batch: 060 ----
mean loss: 100.91
 ---- batch: 070 ----
mean loss: 104.01
 ---- batch: 080 ----
mean loss: 99.52
 ---- batch: 090 ----
mean loss: 104.07
train mean loss: 101.62
epoch train time: 0:00:01.625191
elapsed time: 0:06:50.160488
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 20:31:21.316420
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.45
 ---- batch: 020 ----
mean loss: 102.94
 ---- batch: 030 ----
mean loss: 98.60
 ---- batch: 040 ----
mean loss: 99.91
 ---- batch: 050 ----
mean loss: 106.13
 ---- batch: 060 ----
mean loss: 97.41
 ---- batch: 070 ----
mean loss: 103.56
 ---- batch: 080 ----
mean loss: 98.24
 ---- batch: 090 ----
mean loss: 104.96
train mean loss: 101.76
epoch train time: 0:00:01.669123
elapsed time: 0:06:51.830251
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 20:31:22.986223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.71
 ---- batch: 020 ----
mean loss: 103.73
 ---- batch: 030 ----
mean loss: 100.86
 ---- batch: 040 ----
mean loss: 94.73
 ---- batch: 050 ----
mean loss: 97.82
 ---- batch: 060 ----
mean loss: 99.81
 ---- batch: 070 ----
mean loss: 105.18
 ---- batch: 080 ----
mean loss: 108.21
 ---- batch: 090 ----
mean loss: 99.33
train mean loss: 101.46
epoch train time: 0:00:01.656772
elapsed time: 0:06:53.487699
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 20:31:24.643663
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.72
 ---- batch: 020 ----
mean loss: 100.42
 ---- batch: 030 ----
mean loss: 102.31
 ---- batch: 040 ----
mean loss: 101.65
 ---- batch: 050 ----
mean loss: 101.10
 ---- batch: 060 ----
mean loss: 97.17
 ---- batch: 070 ----
mean loss: 101.96
 ---- batch: 080 ----
mean loss: 106.99
 ---- batch: 090 ----
mean loss: 100.54
train mean loss: 101.36
epoch train time: 0:00:01.662056
elapsed time: 0:06:55.150386
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 20:31:26.306374
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.10
 ---- batch: 020 ----
mean loss: 96.89
 ---- batch: 030 ----
mean loss: 102.78
 ---- batch: 040 ----
mean loss: 98.39
 ---- batch: 050 ----
mean loss: 101.36
 ---- batch: 060 ----
mean loss: 99.68
 ---- batch: 070 ----
mean loss: 107.87
 ---- batch: 080 ----
mean loss: 102.63
 ---- batch: 090 ----
mean loss: 100.22
train mean loss: 101.34
epoch train time: 0:00:01.625556
elapsed time: 0:06:56.776670
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 20:31:27.932639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.61
 ---- batch: 020 ----
mean loss: 98.20
 ---- batch: 030 ----
mean loss: 100.59
 ---- batch: 040 ----
mean loss: 96.84
 ---- batch: 050 ----
mean loss: 100.07
 ---- batch: 060 ----
mean loss: 103.31
 ---- batch: 070 ----
mean loss: 94.05
 ---- batch: 080 ----
mean loss: 109.16
 ---- batch: 090 ----
mean loss: 105.10
train mean loss: 101.45
epoch train time: 0:00:01.675329
elapsed time: 0:06:58.452941
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 20:31:29.608648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.07
 ---- batch: 020 ----
mean loss: 103.02
 ---- batch: 030 ----
mean loss: 98.89
 ---- batch: 040 ----
mean loss: 105.19
 ---- batch: 050 ----
mean loss: 101.29
 ---- batch: 060 ----
mean loss: 102.51
 ---- batch: 070 ----
mean loss: 100.55
 ---- batch: 080 ----
mean loss: 102.32
 ---- batch: 090 ----
mean loss: 99.48
train mean loss: 101.37
epoch train time: 0:00:01.640547
elapsed time: 0:07:00.093887
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 20:31:31.249858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.48
 ---- batch: 020 ----
mean loss: 100.02
 ---- batch: 030 ----
mean loss: 100.48
 ---- batch: 040 ----
mean loss: 98.03
 ---- batch: 050 ----
mean loss: 104.02
 ---- batch: 060 ----
mean loss: 102.82
 ---- batch: 070 ----
mean loss: 101.32
 ---- batch: 080 ----
mean loss: 101.27
 ---- batch: 090 ----
mean loss: 102.64
train mean loss: 101.30
epoch train time: 0:00:01.627041
elapsed time: 0:07:01.721655
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 20:31:32.877594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.56
 ---- batch: 020 ----
mean loss: 102.85
 ---- batch: 030 ----
mean loss: 104.95
 ---- batch: 040 ----
mean loss: 101.75
 ---- batch: 050 ----
mean loss: 97.91
 ---- batch: 060 ----
mean loss: 97.03
 ---- batch: 070 ----
mean loss: 99.01
 ---- batch: 080 ----
mean loss: 103.17
 ---- batch: 090 ----
mean loss: 100.72
train mean loss: 101.19
epoch train time: 0:00:01.643238
elapsed time: 0:07:03.365499
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 20:31:34.521486
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.90
 ---- batch: 020 ----
mean loss: 99.56
 ---- batch: 030 ----
mean loss: 108.95
 ---- batch: 040 ----
mean loss: 96.56
 ---- batch: 050 ----
mean loss: 96.77
 ---- batch: 060 ----
mean loss: 98.57
 ---- batch: 070 ----
mean loss: 103.03
 ---- batch: 080 ----
mean loss: 103.60
 ---- batch: 090 ----
mean loss: 103.81
train mean loss: 101.21
epoch train time: 0:00:01.669536
elapsed time: 0:07:05.035835
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 20:31:36.191789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.06
 ---- batch: 020 ----
mean loss: 104.21
 ---- batch: 030 ----
mean loss: 101.57
 ---- batch: 040 ----
mean loss: 98.88
 ---- batch: 050 ----
mean loss: 101.96
 ---- batch: 060 ----
mean loss: 100.87
 ---- batch: 070 ----
mean loss: 103.42
 ---- batch: 080 ----
mean loss: 102.17
 ---- batch: 090 ----
mean loss: 98.13
train mean loss: 101.26
epoch train time: 0:00:01.645244
elapsed time: 0:07:06.681748
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 20:31:37.837778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.29
 ---- batch: 020 ----
mean loss: 104.74
 ---- batch: 030 ----
mean loss: 106.62
 ---- batch: 040 ----
mean loss: 98.99
 ---- batch: 050 ----
mean loss: 103.10
 ---- batch: 060 ----
mean loss: 96.10
 ---- batch: 070 ----
mean loss: 96.09
 ---- batch: 080 ----
mean loss: 102.29
 ---- batch: 090 ----
mean loss: 103.91
train mean loss: 101.23
epoch train time: 0:00:01.636358
elapsed time: 0:07:08.318753
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 20:31:39.474686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.47
 ---- batch: 020 ----
mean loss: 101.20
 ---- batch: 030 ----
mean loss: 102.49
 ---- batch: 040 ----
mean loss: 96.63
 ---- batch: 050 ----
mean loss: 102.08
 ---- batch: 060 ----
mean loss: 99.01
 ---- batch: 070 ----
mean loss: 98.89
 ---- batch: 080 ----
mean loss: 100.92
 ---- batch: 090 ----
mean loss: 102.19
train mean loss: 101.14
epoch train time: 0:00:01.627690
elapsed time: 0:07:09.947046
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 20:31:41.102990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.39
 ---- batch: 020 ----
mean loss: 99.38
 ---- batch: 030 ----
mean loss: 105.10
 ---- batch: 040 ----
mean loss: 99.52
 ---- batch: 050 ----
mean loss: 100.71
 ---- batch: 060 ----
mean loss: 95.41
 ---- batch: 070 ----
mean loss: 106.03
 ---- batch: 080 ----
mean loss: 101.33
 ---- batch: 090 ----
mean loss: 96.73
train mean loss: 101.32
epoch train time: 0:00:01.654369
elapsed time: 0:07:11.602108
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 20:31:42.758089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.73
 ---- batch: 020 ----
mean loss: 97.83
 ---- batch: 030 ----
mean loss: 99.64
 ---- batch: 040 ----
mean loss: 101.47
 ---- batch: 050 ----
mean loss: 95.99
 ---- batch: 060 ----
mean loss: 98.50
 ---- batch: 070 ----
mean loss: 97.12
 ---- batch: 080 ----
mean loss: 101.66
 ---- batch: 090 ----
mean loss: 109.01
train mean loss: 100.96
epoch train time: 0:00:01.695638
elapsed time: 0:07:13.298448
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 20:31:44.454397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.33
 ---- batch: 020 ----
mean loss: 101.97
 ---- batch: 030 ----
mean loss: 98.70
 ---- batch: 040 ----
mean loss: 103.13
 ---- batch: 050 ----
mean loss: 96.56
 ---- batch: 060 ----
mean loss: 103.62
 ---- batch: 070 ----
mean loss: 102.15
 ---- batch: 080 ----
mean loss: 97.14
 ---- batch: 090 ----
mean loss: 103.61
train mean loss: 101.01
epoch train time: 0:00:01.666396
elapsed time: 0:07:14.965498
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 20:31:46.121439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.33
 ---- batch: 020 ----
mean loss: 105.69
 ---- batch: 030 ----
mean loss: 100.13
 ---- batch: 040 ----
mean loss: 95.21
 ---- batch: 050 ----
mean loss: 99.53
 ---- batch: 060 ----
mean loss: 100.75
 ---- batch: 070 ----
mean loss: 102.35
 ---- batch: 080 ----
mean loss: 98.18
 ---- batch: 090 ----
mean loss: 105.12
train mean loss: 100.92
epoch train time: 0:00:01.632557
elapsed time: 0:07:16.598671
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 20:31:47.754616
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.30
 ---- batch: 020 ----
mean loss: 100.30
 ---- batch: 030 ----
mean loss: 99.48
 ---- batch: 040 ----
mean loss: 104.39
 ---- batch: 050 ----
mean loss: 103.04
 ---- batch: 060 ----
mean loss: 102.06
 ---- batch: 070 ----
mean loss: 98.94
 ---- batch: 080 ----
mean loss: 102.08
 ---- batch: 090 ----
mean loss: 103.79
train mean loss: 100.86
epoch train time: 0:00:01.646640
elapsed time: 0:07:18.245958
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 20:31:49.402031
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.54
 ---- batch: 020 ----
mean loss: 100.45
 ---- batch: 030 ----
mean loss: 94.71
 ---- batch: 040 ----
mean loss: 105.06
 ---- batch: 050 ----
mean loss: 99.07
 ---- batch: 060 ----
mean loss: 102.63
 ---- batch: 070 ----
mean loss: 97.90
 ---- batch: 080 ----
mean loss: 96.35
 ---- batch: 090 ----
mean loss: 102.97
train mean loss: 100.99
epoch train time: 0:00:01.636055
elapsed time: 0:07:19.882803
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 20:31:51.038763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.79
 ---- batch: 020 ----
mean loss: 100.56
 ---- batch: 030 ----
mean loss: 102.59
 ---- batch: 040 ----
mean loss: 101.57
 ---- batch: 050 ----
mean loss: 103.08
 ---- batch: 060 ----
mean loss: 96.83
 ---- batch: 070 ----
mean loss: 102.94
 ---- batch: 080 ----
mean loss: 105.21
 ---- batch: 090 ----
mean loss: 99.94
train mean loss: 100.73
epoch train time: 0:00:01.630402
elapsed time: 0:07:21.513873
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 20:31:52.669914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.25
 ---- batch: 020 ----
mean loss: 97.96
 ---- batch: 030 ----
mean loss: 99.33
 ---- batch: 040 ----
mean loss: 97.10
 ---- batch: 050 ----
mean loss: 103.60
 ---- batch: 060 ----
mean loss: 106.04
 ---- batch: 070 ----
mean loss: 103.66
 ---- batch: 080 ----
mean loss: 97.53
 ---- batch: 090 ----
mean loss: 99.55
train mean loss: 100.67
epoch train time: 0:00:01.631535
elapsed time: 0:07:23.146112
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 20:31:54.302050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.74
 ---- batch: 020 ----
mean loss: 104.89
 ---- batch: 030 ----
mean loss: 98.49
 ---- batch: 040 ----
mean loss: 97.98
 ---- batch: 050 ----
mean loss: 100.78
 ---- batch: 060 ----
mean loss: 100.60
 ---- batch: 070 ----
mean loss: 98.37
 ---- batch: 080 ----
mean loss: 103.76
 ---- batch: 090 ----
mean loss: 105.22
train mean loss: 100.77
epoch train time: 0:00:01.671111
elapsed time: 0:07:24.825930
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_8/checkpoint.pth.tar
**** end time: 2019-09-25 20:31:55.981587 ****
