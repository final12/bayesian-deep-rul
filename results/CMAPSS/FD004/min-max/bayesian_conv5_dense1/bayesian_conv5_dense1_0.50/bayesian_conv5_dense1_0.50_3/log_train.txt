Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_3', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 5374
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 12:13:45.308524 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 12:13:45.326169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1995.28
 ---- batch: 020 ----
mean loss: 1445.41
 ---- batch: 030 ----
mean loss: 1271.51
 ---- batch: 040 ----
mean loss: 1161.73
 ---- batch: 050 ----
mean loss: 1168.62
train mean loss: 1366.50
epoch train time: 0:00:23.087431
elapsed time: 0:00:23.113588
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 12:14:08.422161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1093.12
 ---- batch: 020 ----
mean loss: 1076.44
 ---- batch: 030 ----
mean loss: 1019.18
 ---- batch: 040 ----
mean loss: 1024.00
 ---- batch: 050 ----
mean loss: 1017.63
train mean loss: 1041.33
epoch train time: 0:00:07.921771
elapsed time: 0:00:31.036165
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 12:14:16.345035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 991.53
 ---- batch: 020 ----
mean loss: 1021.80
 ---- batch: 030 ----
mean loss: 992.55
 ---- batch: 040 ----
mean loss: 982.93
 ---- batch: 050 ----
mean loss: 951.82
train mean loss: 983.89
epoch train time: 0:00:07.942689
elapsed time: 0:00:38.979996
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 12:14:24.288886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.15
 ---- batch: 020 ----
mean loss: 939.42
 ---- batch: 030 ----
mean loss: 942.98
 ---- batch: 040 ----
mean loss: 951.50
 ---- batch: 050 ----
mean loss: 948.65
train mean loss: 950.58
epoch train time: 0:00:07.945723
elapsed time: 0:00:46.926869
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 12:14:32.235745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.53
 ---- batch: 020 ----
mean loss: 965.49
 ---- batch: 030 ----
mean loss: 920.95
 ---- batch: 040 ----
mean loss: 941.00
 ---- batch: 050 ----
mean loss: 911.07
train mean loss: 938.92
epoch train time: 0:00:07.907385
elapsed time: 0:00:54.835489
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 12:14:40.144401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.57
 ---- batch: 020 ----
mean loss: 930.45
 ---- batch: 030 ----
mean loss: 916.03
 ---- batch: 040 ----
mean loss: 918.07
 ---- batch: 050 ----
mean loss: 917.76
train mean loss: 927.07
epoch train time: 0:00:07.915257
elapsed time: 0:01:02.751917
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 12:14:48.060850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.03
 ---- batch: 020 ----
mean loss: 929.16
 ---- batch: 030 ----
mean loss: 917.27
 ---- batch: 040 ----
mean loss: 937.79
 ---- batch: 050 ----
mean loss: 889.46
train mean loss: 923.37
epoch train time: 0:00:07.936974
elapsed time: 0:01:10.690178
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 12:14:55.999081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 918.76
 ---- batch: 020 ----
mean loss: 916.65
 ---- batch: 030 ----
mean loss: 924.33
 ---- batch: 040 ----
mean loss: 901.73
 ---- batch: 050 ----
mean loss: 913.30
train mean loss: 911.60
epoch train time: 0:00:07.933914
elapsed time: 0:01:18.625265
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 12:15:03.934169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.48
 ---- batch: 020 ----
mean loss: 896.05
 ---- batch: 030 ----
mean loss: 909.43
 ---- batch: 040 ----
mean loss: 903.31
 ---- batch: 050 ----
mean loss: 906.17
train mean loss: 910.04
epoch train time: 0:00:07.947293
elapsed time: 0:01:26.573782
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 12:15:11.882650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.29
 ---- batch: 020 ----
mean loss: 905.58
 ---- batch: 030 ----
mean loss: 900.26
 ---- batch: 040 ----
mean loss: 922.11
 ---- batch: 050 ----
mean loss: 902.06
train mean loss: 907.14
epoch train time: 0:00:07.963898
elapsed time: 0:01:34.538812
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 12:15:19.847726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.07
 ---- batch: 020 ----
mean loss: 912.06
 ---- batch: 030 ----
mean loss: 920.79
 ---- batch: 040 ----
mean loss: 901.22
 ---- batch: 050 ----
mean loss: 907.92
train mean loss: 908.10
epoch train time: 0:00:07.939505
elapsed time: 0:01:42.479538
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 12:15:27.788439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.89
 ---- batch: 020 ----
mean loss: 888.46
 ---- batch: 030 ----
mean loss: 890.35
 ---- batch: 040 ----
mean loss: 899.85
 ---- batch: 050 ----
mean loss: 913.46
train mean loss: 897.27
epoch train time: 0:00:07.933986
elapsed time: 0:01:50.414803
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 12:15:35.723666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.11
 ---- batch: 020 ----
mean loss: 906.96
 ---- batch: 030 ----
mean loss: 914.54
 ---- batch: 040 ----
mean loss: 891.24
 ---- batch: 050 ----
mean loss: 883.05
train mean loss: 897.63
epoch train time: 0:00:07.941985
elapsed time: 0:01:58.357963
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 12:15:43.666832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.50
 ---- batch: 020 ----
mean loss: 884.16
 ---- batch: 030 ----
mean loss: 881.78
 ---- batch: 040 ----
mean loss: 909.07
 ---- batch: 050 ----
mean loss: 876.78
train mean loss: 892.23
epoch train time: 0:00:07.966903
elapsed time: 0:02:06.326029
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 12:15:51.634907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.48
 ---- batch: 020 ----
mean loss: 891.09
 ---- batch: 030 ----
mean loss: 901.28
 ---- batch: 040 ----
mean loss: 868.74
 ---- batch: 050 ----
mean loss: 888.23
train mean loss: 890.62
epoch train time: 0:00:07.934352
elapsed time: 0:02:14.261642
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 12:15:59.570585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.24
 ---- batch: 020 ----
mean loss: 873.15
 ---- batch: 030 ----
mean loss: 882.07
 ---- batch: 040 ----
mean loss: 898.14
 ---- batch: 050 ----
mean loss: 875.28
train mean loss: 885.90
epoch train time: 0:00:07.922438
elapsed time: 0:02:22.185344
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 12:16:07.494240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.15
 ---- batch: 020 ----
mean loss: 867.10
 ---- batch: 030 ----
mean loss: 884.54
 ---- batch: 040 ----
mean loss: 887.98
 ---- batch: 050 ----
mean loss: 865.77
train mean loss: 879.05
epoch train time: 0:00:07.943133
elapsed time: 0:02:30.129704
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 12:16:15.438607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.09
 ---- batch: 020 ----
mean loss: 888.69
 ---- batch: 030 ----
mean loss: 892.42
 ---- batch: 040 ----
mean loss: 863.36
 ---- batch: 050 ----
mean loss: 875.32
train mean loss: 883.34
epoch train time: 0:00:07.935406
elapsed time: 0:02:38.066292
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 12:16:23.375181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.80
 ---- batch: 020 ----
mean loss: 879.45
 ---- batch: 030 ----
mean loss: 888.63
 ---- batch: 040 ----
mean loss: 904.05
 ---- batch: 050 ----
mean loss: 860.67
train mean loss: 879.77
epoch train time: 0:00:07.957247
elapsed time: 0:02:46.024866
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 12:16:31.333879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.89
 ---- batch: 020 ----
mean loss: 895.50
 ---- batch: 030 ----
mean loss: 887.36
 ---- batch: 040 ----
mean loss: 846.02
 ---- batch: 050 ----
mean loss: 880.45
train mean loss: 875.13
epoch train time: 0:00:07.938086
elapsed time: 0:02:53.964339
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 12:16:39.273262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.64
 ---- batch: 020 ----
mean loss: 895.61
 ---- batch: 030 ----
mean loss: 846.37
 ---- batch: 040 ----
mean loss: 896.20
 ---- batch: 050 ----
mean loss: 851.11
train mean loss: 870.63
epoch train time: 0:00:07.950027
elapsed time: 0:03:01.915595
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 12:16:47.224487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.05
 ---- batch: 020 ----
mean loss: 861.47
 ---- batch: 030 ----
mean loss: 869.01
 ---- batch: 040 ----
mean loss: 855.13
 ---- batch: 050 ----
mean loss: 863.79
train mean loss: 864.48
epoch train time: 0:00:07.941838
elapsed time: 0:03:09.858561
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 12:16:55.167282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.04
 ---- batch: 020 ----
mean loss: 864.23
 ---- batch: 030 ----
mean loss: 853.38
 ---- batch: 040 ----
mean loss: 871.52
 ---- batch: 050 ----
mean loss: 854.79
train mean loss: 863.82
epoch train time: 0:00:07.936738
elapsed time: 0:03:17.796570
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 12:17:03.105569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.99
 ---- batch: 020 ----
mean loss: 860.31
 ---- batch: 030 ----
mean loss: 870.76
 ---- batch: 040 ----
mean loss: 835.88
 ---- batch: 050 ----
mean loss: 858.51
train mean loss: 859.36
epoch train time: 0:00:07.944522
elapsed time: 0:03:25.742340
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 12:17:11.051250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.35
 ---- batch: 020 ----
mean loss: 856.78
 ---- batch: 030 ----
mean loss: 856.55
 ---- batch: 040 ----
mean loss: 862.66
 ---- batch: 050 ----
mean loss: 858.08
train mean loss: 856.67
epoch train time: 0:00:07.937466
elapsed time: 0:03:33.681106
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 12:17:18.990018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.25
 ---- batch: 020 ----
mean loss: 831.39
 ---- batch: 030 ----
mean loss: 848.19
 ---- batch: 040 ----
mean loss: 866.14
 ---- batch: 050 ----
mean loss: 842.63
train mean loss: 846.04
epoch train time: 0:00:07.919406
elapsed time: 0:03:41.601759
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 12:17:26.910649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.72
 ---- batch: 020 ----
mean loss: 839.44
 ---- batch: 030 ----
mean loss: 827.64
 ---- batch: 040 ----
mean loss: 840.12
 ---- batch: 050 ----
mean loss: 849.58
train mean loss: 838.85
epoch train time: 0:00:07.929468
elapsed time: 0:03:49.532454
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 12:17:34.841328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.57
 ---- batch: 020 ----
mean loss: 825.41
 ---- batch: 030 ----
mean loss: 828.59
 ---- batch: 040 ----
mean loss: 811.37
 ---- batch: 050 ----
mean loss: 852.34
train mean loss: 829.92
epoch train time: 0:00:07.931662
elapsed time: 0:03:57.465390
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 12:17:42.774298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.84
 ---- batch: 020 ----
mean loss: 817.51
 ---- batch: 030 ----
mean loss: 810.29
 ---- batch: 040 ----
mean loss: 814.59
 ---- batch: 050 ----
mean loss: 795.69
train mean loss: 812.06
epoch train time: 0:00:07.925144
elapsed time: 0:04:05.391702
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 12:17:50.700692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.20
 ---- batch: 020 ----
mean loss: 801.39
 ---- batch: 030 ----
mean loss: 789.62
 ---- batch: 040 ----
mean loss: 752.47
 ---- batch: 050 ----
mean loss: 754.36
train mean loss: 777.38
epoch train time: 0:00:07.942657
elapsed time: 0:04:13.335678
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 12:17:58.644568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 756.71
 ---- batch: 020 ----
mean loss: 751.63
 ---- batch: 030 ----
mean loss: 733.79
 ---- batch: 040 ----
mean loss: 703.15
 ---- batch: 050 ----
mean loss: 738.04
train mean loss: 735.55
epoch train time: 0:00:07.937850
elapsed time: 0:04:21.274685
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 12:18:06.583582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 715.38
 ---- batch: 020 ----
mean loss: 722.39
 ---- batch: 030 ----
mean loss: 721.53
 ---- batch: 040 ----
mean loss: 708.96
 ---- batch: 050 ----
mean loss: 672.35
train mean loss: 709.84
epoch train time: 0:00:07.936975
elapsed time: 0:04:29.212876
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 12:18:14.521820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.32
 ---- batch: 020 ----
mean loss: 683.91
 ---- batch: 030 ----
mean loss: 699.27
 ---- batch: 040 ----
mean loss: 700.12
 ---- batch: 050 ----
mean loss: 690.12
train mean loss: 689.22
epoch train time: 0:00:08.004578
elapsed time: 0:04:37.218816
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 12:18:22.527740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.97
 ---- batch: 020 ----
mean loss: 691.91
 ---- batch: 030 ----
mean loss: 674.55
 ---- batch: 040 ----
mean loss: 655.02
 ---- batch: 050 ----
mean loss: 661.78
train mean loss: 665.27
epoch train time: 0:00:07.942840
elapsed time: 0:04:45.162910
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 12:18:30.471815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 659.89
 ---- batch: 020 ----
mean loss: 657.50
 ---- batch: 030 ----
mean loss: 653.70
 ---- batch: 040 ----
mean loss: 636.85
 ---- batch: 050 ----
mean loss: 622.13
train mean loss: 645.65
epoch train time: 0:00:07.912845
elapsed time: 0:04:53.076917
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 12:18:38.385929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.72
 ---- batch: 020 ----
mean loss: 628.84
 ---- batch: 030 ----
mean loss: 634.57
 ---- batch: 040 ----
mean loss: 617.42
 ---- batch: 050 ----
mean loss: 618.32
train mean loss: 629.53
epoch train time: 0:00:07.926004
elapsed time: 0:05:01.004229
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 12:18:46.313155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.99
 ---- batch: 020 ----
mean loss: 604.86
 ---- batch: 030 ----
mean loss: 614.08
 ---- batch: 040 ----
mean loss: 615.12
 ---- batch: 050 ----
mean loss: 607.16
train mean loss: 607.42
epoch train time: 0:00:07.932864
elapsed time: 0:05:08.938334
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 12:18:54.247241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 583.52
 ---- batch: 020 ----
mean loss: 609.48
 ---- batch: 030 ----
mean loss: 609.70
 ---- batch: 040 ----
mean loss: 574.73
 ---- batch: 050 ----
mean loss: 587.52
train mean loss: 593.18
epoch train time: 0:00:07.943972
elapsed time: 0:05:16.883523
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 12:19:02.192432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 600.85
 ---- batch: 020 ----
mean loss: 578.25
 ---- batch: 030 ----
mean loss: 574.47
 ---- batch: 040 ----
mean loss: 563.57
 ---- batch: 050 ----
mean loss: 566.57
train mean loss: 574.77
epoch train time: 0:00:07.965814
elapsed time: 0:05:24.850616
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 12:19:10.159570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 563.15
 ---- batch: 020 ----
mean loss: 563.11
 ---- batch: 030 ----
mean loss: 550.19
 ---- batch: 040 ----
mean loss: 568.13
 ---- batch: 050 ----
mean loss: 551.30
train mean loss: 559.38
epoch train time: 0:00:07.997456
elapsed time: 0:05:32.849334
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 12:19:18.158222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.47
 ---- batch: 020 ----
mean loss: 549.56
 ---- batch: 030 ----
mean loss: 535.90
 ---- batch: 040 ----
mean loss: 543.17
 ---- batch: 050 ----
mean loss: 523.13
train mean loss: 545.81
epoch train time: 0:00:07.979228
elapsed time: 0:05:40.829993
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 12:19:26.138875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.80
 ---- batch: 020 ----
mean loss: 553.38
 ---- batch: 030 ----
mean loss: 526.64
 ---- batch: 040 ----
mean loss: 521.25
 ---- batch: 050 ----
mean loss: 521.30
train mean loss: 531.87
epoch train time: 0:00:07.974263
elapsed time: 0:05:48.805487
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 12:19:34.114394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.50
 ---- batch: 020 ----
mean loss: 513.00
 ---- batch: 030 ----
mean loss: 507.90
 ---- batch: 040 ----
mean loss: 523.34
 ---- batch: 050 ----
mean loss: 527.06
train mean loss: 516.36
epoch train time: 0:00:07.941567
elapsed time: 0:05:56.748211
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 12:19:42.057135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.40
 ---- batch: 020 ----
mean loss: 506.37
 ---- batch: 030 ----
mean loss: 492.20
 ---- batch: 040 ----
mean loss: 491.75
 ---- batch: 050 ----
mean loss: 514.42
train mean loss: 502.99
epoch train time: 0:00:07.955477
elapsed time: 0:06:04.704856
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 12:19:50.013583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.43
 ---- batch: 020 ----
mean loss: 494.71
 ---- batch: 030 ----
mean loss: 500.58
 ---- batch: 040 ----
mean loss: 494.41
 ---- batch: 050 ----
mean loss: 477.17
train mean loss: 488.86
epoch train time: 0:00:07.958505
elapsed time: 0:06:12.664354
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 12:19:57.973296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.95
 ---- batch: 020 ----
mean loss: 487.21
 ---- batch: 030 ----
mean loss: 480.44
 ---- batch: 040 ----
mean loss: 486.30
 ---- batch: 050 ----
mean loss: 483.20
train mean loss: 483.80
epoch train time: 0:00:07.944720
elapsed time: 0:06:20.610270
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 12:20:05.919173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.40
 ---- batch: 020 ----
mean loss: 461.65
 ---- batch: 030 ----
mean loss: 481.97
 ---- batch: 040 ----
mean loss: 473.42
 ---- batch: 050 ----
mean loss: 458.78
train mean loss: 469.43
epoch train time: 0:00:07.952370
elapsed time: 0:06:28.563918
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 12:20:13.872882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.39
 ---- batch: 020 ----
mean loss: 455.94
 ---- batch: 030 ----
mean loss: 465.45
 ---- batch: 040 ----
mean loss: 455.89
 ---- batch: 050 ----
mean loss: 464.69
train mean loss: 458.16
epoch train time: 0:00:07.837176
elapsed time: 0:06:36.402288
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 12:20:21.711171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.26
 ---- batch: 020 ----
mean loss: 445.91
 ---- batch: 030 ----
mean loss: 448.80
 ---- batch: 040 ----
mean loss: 436.34
 ---- batch: 050 ----
mean loss: 437.35
train mean loss: 447.44
epoch train time: 0:00:07.822641
elapsed time: 0:06:44.226094
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 12:20:29.534975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.77
 ---- batch: 020 ----
mean loss: 448.28
 ---- batch: 030 ----
mean loss: 446.22
 ---- batch: 040 ----
mean loss: 445.12
 ---- batch: 050 ----
mean loss: 436.05
train mean loss: 442.27
epoch train time: 0:00:07.848684
elapsed time: 0:06:52.075954
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 12:20:37.384865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.33
 ---- batch: 020 ----
mean loss: 436.32
 ---- batch: 030 ----
mean loss: 437.84
 ---- batch: 040 ----
mean loss: 427.21
 ---- batch: 050 ----
mean loss: 425.68
train mean loss: 433.27
epoch train time: 0:00:07.859912
elapsed time: 0:06:59.937006
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 12:20:45.245932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.00
 ---- batch: 020 ----
mean loss: 425.82
 ---- batch: 030 ----
mean loss: 417.50
 ---- batch: 040 ----
mean loss: 408.70
 ---- batch: 050 ----
mean loss: 423.63
train mean loss: 421.77
epoch train time: 0:00:07.830617
elapsed time: 0:07:07.768810
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 12:20:53.077788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.69
 ---- batch: 020 ----
mean loss: 420.47
 ---- batch: 030 ----
mean loss: 417.39
 ---- batch: 040 ----
mean loss: 416.25
 ---- batch: 050 ----
mean loss: 422.29
train mean loss: 417.23
epoch train time: 0:00:07.814579
elapsed time: 0:07:15.584588
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 12:21:00.893561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.49
 ---- batch: 020 ----
mean loss: 407.82
 ---- batch: 030 ----
mean loss: 413.18
 ---- batch: 040 ----
mean loss: 406.51
 ---- batch: 050 ----
mean loss: 405.45
train mean loss: 406.96
epoch train time: 0:00:07.791203
elapsed time: 0:07:23.377044
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 12:21:08.685965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.00
 ---- batch: 020 ----
mean loss: 409.04
 ---- batch: 030 ----
mean loss: 400.54
 ---- batch: 040 ----
mean loss: 402.53
 ---- batch: 050 ----
mean loss: 385.30
train mean loss: 400.20
epoch train time: 0:00:07.826941
elapsed time: 0:07:31.205268
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 12:21:16.514161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.79
 ---- batch: 020 ----
mean loss: 391.61
 ---- batch: 030 ----
mean loss: 401.06
 ---- batch: 040 ----
mean loss: 386.30
 ---- batch: 050 ----
mean loss: 389.89
train mean loss: 390.42
epoch train time: 0:00:07.828323
elapsed time: 0:07:39.034728
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 12:21:24.343638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.45
 ---- batch: 020 ----
mean loss: 383.88
 ---- batch: 030 ----
mean loss: 374.57
 ---- batch: 040 ----
mean loss: 392.47
 ---- batch: 050 ----
mean loss: 390.65
train mean loss: 386.93
epoch train time: 0:00:07.817346
elapsed time: 0:07:46.853238
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 12:21:32.162154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.80
 ---- batch: 020 ----
mean loss: 381.29
 ---- batch: 030 ----
mean loss: 385.48
 ---- batch: 040 ----
mean loss: 382.54
 ---- batch: 050 ----
mean loss: 390.92
train mean loss: 383.70
epoch train time: 0:00:07.813411
elapsed time: 0:07:54.667832
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 12:21:39.976728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.52
 ---- batch: 020 ----
mean loss: 374.68
 ---- batch: 030 ----
mean loss: 383.14
 ---- batch: 040 ----
mean loss: 368.36
 ---- batch: 050 ----
mean loss: 379.41
train mean loss: 375.39
epoch train time: 0:00:07.829595
elapsed time: 0:08:02.498625
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 12:21:47.807509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.95
 ---- batch: 020 ----
mean loss: 364.22
 ---- batch: 030 ----
mean loss: 381.41
 ---- batch: 040 ----
mean loss: 371.01
 ---- batch: 050 ----
mean loss: 364.72
train mean loss: 371.44
epoch train time: 0:00:07.835512
elapsed time: 0:08:10.335325
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 12:21:55.644275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.05
 ---- batch: 020 ----
mean loss: 375.00
 ---- batch: 030 ----
mean loss: 366.77
 ---- batch: 040 ----
mean loss: 359.31
 ---- batch: 050 ----
mean loss: 362.76
train mean loss: 364.77
epoch train time: 0:00:07.834968
elapsed time: 0:08:18.171485
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 12:22:03.480368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.15
 ---- batch: 020 ----
mean loss: 362.99
 ---- batch: 030 ----
mean loss: 368.50
 ---- batch: 040 ----
mean loss: 370.99
 ---- batch: 050 ----
mean loss: 364.54
train mean loss: 365.71
epoch train time: 0:00:07.832240
elapsed time: 0:08:26.004821
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 12:22:11.313734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.96
 ---- batch: 020 ----
mean loss: 356.01
 ---- batch: 030 ----
mean loss: 351.88
 ---- batch: 040 ----
mean loss: 355.69
 ---- batch: 050 ----
mean loss: 371.89
train mean loss: 358.79
epoch train time: 0:00:07.811691
elapsed time: 0:08:33.817666
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 12:22:19.126599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.04
 ---- batch: 020 ----
mean loss: 361.45
 ---- batch: 030 ----
mean loss: 357.73
 ---- batch: 040 ----
mean loss: 358.01
 ---- batch: 050 ----
mean loss: 343.70
train mean loss: 355.06
epoch train time: 0:00:07.823521
elapsed time: 0:08:41.642395
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 12:22:26.951295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.53
 ---- batch: 020 ----
mean loss: 345.12
 ---- batch: 030 ----
mean loss: 361.74
 ---- batch: 040 ----
mean loss: 350.43
 ---- batch: 050 ----
mean loss: 359.76
train mean loss: 351.60
epoch train time: 0:00:07.836516
elapsed time: 0:08:49.480169
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 12:22:34.789104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.81
 ---- batch: 020 ----
mean loss: 353.19
 ---- batch: 030 ----
mean loss: 338.28
 ---- batch: 040 ----
mean loss: 346.37
 ---- batch: 050 ----
mean loss: 346.84
train mean loss: 344.42
epoch train time: 0:00:07.800124
elapsed time: 0:08:57.281474
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 12:22:42.590380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.31
 ---- batch: 020 ----
mean loss: 346.32
 ---- batch: 030 ----
mean loss: 344.18
 ---- batch: 040 ----
mean loss: 347.39
 ---- batch: 050 ----
mean loss: 353.95
train mean loss: 344.63
epoch train time: 0:00:07.791892
elapsed time: 0:09:05.074552
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 12:22:50.383441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.17
 ---- batch: 020 ----
mean loss: 337.27
 ---- batch: 030 ----
mean loss: 335.92
 ---- batch: 040 ----
mean loss: 338.50
 ---- batch: 050 ----
mean loss: 343.24
train mean loss: 339.56
epoch train time: 0:00:07.804932
elapsed time: 0:09:12.880621
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 12:22:58.189509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.75
 ---- batch: 020 ----
mean loss: 338.61
 ---- batch: 030 ----
mean loss: 343.56
 ---- batch: 040 ----
mean loss: 339.94
 ---- batch: 050 ----
mean loss: 336.99
train mean loss: 336.84
epoch train time: 0:00:07.807691
elapsed time: 0:09:20.689481
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 12:23:05.998375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.96
 ---- batch: 020 ----
mean loss: 340.36
 ---- batch: 030 ----
mean loss: 336.81
 ---- batch: 040 ----
mean loss: 337.32
 ---- batch: 050 ----
mean loss: 338.15
train mean loss: 334.87
epoch train time: 0:00:07.809371
elapsed time: 0:09:28.499991
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 12:23:13.808888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.08
 ---- batch: 020 ----
mean loss: 326.32
 ---- batch: 030 ----
mean loss: 325.06
 ---- batch: 040 ----
mean loss: 335.59
 ---- batch: 050 ----
mean loss: 329.49
train mean loss: 330.96
epoch train time: 0:00:07.866770
elapsed time: 0:09:36.367934
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 12:23:21.676883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.93
 ---- batch: 020 ----
mean loss: 334.00
 ---- batch: 030 ----
mean loss: 325.99
 ---- batch: 040 ----
mean loss: 328.96
 ---- batch: 050 ----
mean loss: 323.81
train mean loss: 327.46
epoch train time: 0:00:07.861327
elapsed time: 0:09:44.230471
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 12:23:29.539357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.11
 ---- batch: 020 ----
mean loss: 323.88
 ---- batch: 030 ----
mean loss: 320.67
 ---- batch: 040 ----
mean loss: 319.68
 ---- batch: 050 ----
mean loss: 329.19
train mean loss: 324.22
epoch train time: 0:00:07.908465
elapsed time: 0:09:52.140065
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 12:23:37.448979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.49
 ---- batch: 020 ----
mean loss: 321.82
 ---- batch: 030 ----
mean loss: 324.22
 ---- batch: 040 ----
mean loss: 322.80
 ---- batch: 050 ----
mean loss: 321.82
train mean loss: 323.08
epoch train time: 0:00:07.821640
elapsed time: 0:09:59.962858
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 12:23:45.271755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.21
 ---- batch: 020 ----
mean loss: 319.74
 ---- batch: 030 ----
mean loss: 323.46
 ---- batch: 040 ----
mean loss: 327.97
 ---- batch: 050 ----
mean loss: 312.15
train mean loss: 321.49
epoch train time: 0:00:07.827924
elapsed time: 0:10:07.791916
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 12:23:53.100833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.24
 ---- batch: 020 ----
mean loss: 317.12
 ---- batch: 030 ----
mean loss: 309.58
 ---- batch: 040 ----
mean loss: 323.28
 ---- batch: 050 ----
mean loss: 316.36
train mean loss: 316.19
epoch train time: 0:00:07.823659
elapsed time: 0:10:15.616754
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 12:24:00.925673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.66
 ---- batch: 020 ----
mean loss: 323.58
 ---- batch: 030 ----
mean loss: 325.07
 ---- batch: 040 ----
mean loss: 315.02
 ---- batch: 050 ----
mean loss: 317.43
train mean loss: 315.57
epoch train time: 0:00:07.815441
elapsed time: 0:10:23.433434
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 12:24:08.742394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.75
 ---- batch: 020 ----
mean loss: 310.70
 ---- batch: 030 ----
mean loss: 315.42
 ---- batch: 040 ----
mean loss: 315.30
 ---- batch: 050 ----
mean loss: 310.29
train mean loss: 310.18
epoch train time: 0:00:07.845403
elapsed time: 0:10:31.280089
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 12:24:16.589017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.54
 ---- batch: 020 ----
mean loss: 311.49
 ---- batch: 030 ----
mean loss: 301.43
 ---- batch: 040 ----
mean loss: 308.83
 ---- batch: 050 ----
mean loss: 312.80
train mean loss: 308.54
epoch train time: 0:00:07.831277
elapsed time: 0:10:39.112544
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 12:24:24.421467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.88
 ---- batch: 020 ----
mean loss: 305.88
 ---- batch: 030 ----
mean loss: 313.77
 ---- batch: 040 ----
mean loss: 319.15
 ---- batch: 050 ----
mean loss: 308.06
train mean loss: 310.75
epoch train time: 0:00:07.855673
elapsed time: 0:10:46.969354
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 12:24:32.278247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.18
 ---- batch: 020 ----
mean loss: 306.17
 ---- batch: 030 ----
mean loss: 306.27
 ---- batch: 040 ----
mean loss: 312.42
 ---- batch: 050 ----
mean loss: 317.57
train mean loss: 311.52
epoch train time: 0:00:07.856295
elapsed time: 0:10:54.826790
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 12:24:40.135760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.96
 ---- batch: 020 ----
mean loss: 297.53
 ---- batch: 030 ----
mean loss: 295.57
 ---- batch: 040 ----
mean loss: 307.05
 ---- batch: 050 ----
mean loss: 315.45
train mean loss: 303.84
epoch train time: 0:00:07.853788
elapsed time: 0:11:02.681887
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 12:24:47.990800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.89
 ---- batch: 020 ----
mean loss: 298.22
 ---- batch: 030 ----
mean loss: 293.98
 ---- batch: 040 ----
mean loss: 300.21
 ---- batch: 050 ----
mean loss: 308.55
train mean loss: 300.47
epoch train time: 0:00:07.857004
elapsed time: 0:11:10.540080
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 12:24:55.848980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.28
 ---- batch: 020 ----
mean loss: 295.38
 ---- batch: 030 ----
mean loss: 296.34
 ---- batch: 040 ----
mean loss: 306.52
 ---- batch: 050 ----
mean loss: 299.61
train mean loss: 299.59
epoch train time: 0:00:07.851838
elapsed time: 0:11:18.393174
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 12:25:03.702076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.19
 ---- batch: 020 ----
mean loss: 290.97
 ---- batch: 030 ----
mean loss: 295.63
 ---- batch: 040 ----
mean loss: 296.91
 ---- batch: 050 ----
mean loss: 296.05
train mean loss: 296.58
epoch train time: 0:00:07.834599
elapsed time: 0:11:26.228902
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 12:25:11.537890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.53
 ---- batch: 020 ----
mean loss: 308.58
 ---- batch: 030 ----
mean loss: 283.89
 ---- batch: 040 ----
mean loss: 295.59
 ---- batch: 050 ----
mean loss: 294.85
train mean loss: 296.11
epoch train time: 0:00:07.840518
elapsed time: 0:11:34.070746
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 12:25:19.379497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.46
 ---- batch: 020 ----
mean loss: 293.93
 ---- batch: 030 ----
mean loss: 291.45
 ---- batch: 040 ----
mean loss: 297.97
 ---- batch: 050 ----
mean loss: 291.13
train mean loss: 295.38
epoch train time: 0:00:07.855210
elapsed time: 0:11:41.926974
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 12:25:27.235863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.68
 ---- batch: 020 ----
mean loss: 290.47
 ---- batch: 030 ----
mean loss: 297.29
 ---- batch: 040 ----
mean loss: 278.82
 ---- batch: 050 ----
mean loss: 291.07
train mean loss: 290.88
epoch train time: 0:00:07.849535
elapsed time: 0:11:49.777760
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 12:25:35.086679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.59
 ---- batch: 020 ----
mean loss: 283.02
 ---- batch: 030 ----
mean loss: 294.22
 ---- batch: 040 ----
mean loss: 290.71
 ---- batch: 050 ----
mean loss: 291.93
train mean loss: 288.85
epoch train time: 0:00:07.811450
elapsed time: 0:11:57.590420
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 12:25:42.899303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.60
 ---- batch: 020 ----
mean loss: 298.85
 ---- batch: 030 ----
mean loss: 293.17
 ---- batch: 040 ----
mean loss: 288.97
 ---- batch: 050 ----
mean loss: 284.71
train mean loss: 288.58
epoch train time: 0:00:07.814542
elapsed time: 0:12:05.406252
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 12:25:50.715196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.13
 ---- batch: 020 ----
mean loss: 294.73
 ---- batch: 030 ----
mean loss: 283.50
 ---- batch: 040 ----
mean loss: 281.16
 ---- batch: 050 ----
mean loss: 291.52
train mean loss: 287.01
epoch train time: 0:00:07.853846
elapsed time: 0:12:13.261302
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 12:25:58.570189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.59
 ---- batch: 020 ----
mean loss: 275.42
 ---- batch: 030 ----
mean loss: 286.99
 ---- batch: 040 ----
mean loss: 287.94
 ---- batch: 050 ----
mean loss: 285.17
train mean loss: 283.67
epoch train time: 0:00:07.849101
elapsed time: 0:12:21.111486
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 12:26:06.420393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.23
 ---- batch: 020 ----
mean loss: 284.96
 ---- batch: 030 ----
mean loss: 287.98
 ---- batch: 040 ----
mean loss: 278.54
 ---- batch: 050 ----
mean loss: 285.68
train mean loss: 284.10
epoch train time: 0:00:07.837461
elapsed time: 0:12:28.950127
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 12:26:14.259051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.99
 ---- batch: 020 ----
mean loss: 278.86
 ---- batch: 030 ----
mean loss: 281.51
 ---- batch: 040 ----
mean loss: 283.43
 ---- batch: 050 ----
mean loss: 283.87
train mean loss: 282.05
epoch train time: 0:00:07.867020
elapsed time: 0:12:36.818382
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 12:26:22.127381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.54
 ---- batch: 020 ----
mean loss: 281.23
 ---- batch: 030 ----
mean loss: 281.44
 ---- batch: 040 ----
mean loss: 278.41
 ---- batch: 050 ----
mean loss: 275.39
train mean loss: 278.26
epoch train time: 0:00:07.842373
elapsed time: 0:12:44.662071
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 12:26:29.970991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.68
 ---- batch: 020 ----
mean loss: 280.20
 ---- batch: 030 ----
mean loss: 278.44
 ---- batch: 040 ----
mean loss: 280.29
 ---- batch: 050 ----
mean loss: 286.59
train mean loss: 278.32
epoch train time: 0:00:07.860973
elapsed time: 0:12:52.524202
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 12:26:37.833107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.78
 ---- batch: 020 ----
mean loss: 265.69
 ---- batch: 030 ----
mean loss: 278.83
 ---- batch: 040 ----
mean loss: 277.48
 ---- batch: 050 ----
mean loss: 281.36
train mean loss: 274.82
epoch train time: 0:00:07.851779
elapsed time: 0:13:00.377338
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 12:26:45.686428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.53
 ---- batch: 020 ----
mean loss: 282.31
 ---- batch: 030 ----
mean loss: 267.52
 ---- batch: 040 ----
mean loss: 269.99
 ---- batch: 050 ----
mean loss: 278.67
train mean loss: 273.45
epoch train time: 0:00:07.848717
elapsed time: 0:13:08.227460
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 12:26:53.536355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.08
 ---- batch: 020 ----
mean loss: 283.53
 ---- batch: 030 ----
mean loss: 269.08
 ---- batch: 040 ----
mean loss: 273.40
 ---- batch: 050 ----
mean loss: 268.65
train mean loss: 273.93
epoch train time: 0:00:07.871310
elapsed time: 0:13:16.099914
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 12:27:01.408804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.87
 ---- batch: 020 ----
mean loss: 276.77
 ---- batch: 030 ----
mean loss: 269.00
 ---- batch: 040 ----
mean loss: 280.65
 ---- batch: 050 ----
mean loss: 262.90
train mean loss: 272.34
epoch train time: 0:00:07.870965
elapsed time: 0:13:23.972027
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 12:27:09.280946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.01
 ---- batch: 020 ----
mean loss: 283.56
 ---- batch: 030 ----
mean loss: 272.91
 ---- batch: 040 ----
mean loss: 265.87
 ---- batch: 050 ----
mean loss: 267.30
train mean loss: 270.58
epoch train time: 0:00:07.865354
elapsed time: 0:13:31.838575
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 12:27:17.147461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.20
 ---- batch: 020 ----
mean loss: 274.37
 ---- batch: 030 ----
mean loss: 273.48
 ---- batch: 040 ----
mean loss: 267.19
 ---- batch: 050 ----
mean loss: 259.91
train mean loss: 268.37
epoch train time: 0:00:07.838185
elapsed time: 0:13:39.677896
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 12:27:24.986793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.68
 ---- batch: 020 ----
mean loss: 268.50
 ---- batch: 030 ----
mean loss: 269.55
 ---- batch: 040 ----
mean loss: 257.72
 ---- batch: 050 ----
mean loss: 268.49
train mean loss: 268.31
epoch train time: 0:00:07.868070
elapsed time: 0:13:47.547184
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 12:27:32.856062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.09
 ---- batch: 020 ----
mean loss: 256.40
 ---- batch: 030 ----
mean loss: 265.12
 ---- batch: 040 ----
mean loss: 264.87
 ---- batch: 050 ----
mean loss: 269.14
train mean loss: 264.96
epoch train time: 0:00:07.833287
elapsed time: 0:13:55.381605
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 12:27:40.690474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.89
 ---- batch: 020 ----
mean loss: 264.17
 ---- batch: 030 ----
mean loss: 270.16
 ---- batch: 040 ----
mean loss: 264.16
 ---- batch: 050 ----
mean loss: 273.70
train mean loss: 267.69
epoch train time: 0:00:07.870343
elapsed time: 0:14:03.253059
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 12:27:48.562006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.97
 ---- batch: 020 ----
mean loss: 262.47
 ---- batch: 030 ----
mean loss: 256.33
 ---- batch: 040 ----
mean loss: 264.22
 ---- batch: 050 ----
mean loss: 273.66
train mean loss: 262.06
epoch train time: 0:00:07.904846
elapsed time: 0:14:11.159097
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 12:27:56.468009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.99
 ---- batch: 020 ----
mean loss: 259.88
 ---- batch: 030 ----
mean loss: 258.37
 ---- batch: 040 ----
mean loss: 261.38
 ---- batch: 050 ----
mean loss: 256.92
train mean loss: 261.46
epoch train time: 0:00:07.838540
elapsed time: 0:14:18.998996
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 12:28:04.307666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.42
 ---- batch: 020 ----
mean loss: 253.45
 ---- batch: 030 ----
mean loss: 269.37
 ---- batch: 040 ----
mean loss: 255.44
 ---- batch: 050 ----
mean loss: 263.72
train mean loss: 260.56
epoch train time: 0:00:07.821753
elapsed time: 0:14:26.821787
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 12:28:12.130660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.86
 ---- batch: 020 ----
mean loss: 258.70
 ---- batch: 030 ----
mean loss: 251.11
 ---- batch: 040 ----
mean loss: 264.54
 ---- batch: 050 ----
mean loss: 254.65
train mean loss: 258.07
epoch train time: 0:00:07.859016
elapsed time: 0:14:34.682052
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 12:28:19.990994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.44
 ---- batch: 020 ----
mean loss: 254.08
 ---- batch: 030 ----
mean loss: 244.69
 ---- batch: 040 ----
mean loss: 270.35
 ---- batch: 050 ----
mean loss: 264.89
train mean loss: 258.93
epoch train time: 0:00:07.828128
elapsed time: 0:14:42.511320
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 12:28:27.820276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.68
 ---- batch: 020 ----
mean loss: 260.96
 ---- batch: 030 ----
mean loss: 257.26
 ---- batch: 040 ----
mean loss: 268.57
 ---- batch: 050 ----
mean loss: 261.26
train mean loss: 259.19
epoch train time: 0:00:07.801732
elapsed time: 0:14:50.314309
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 12:28:35.623213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.96
 ---- batch: 020 ----
mean loss: 250.39
 ---- batch: 030 ----
mean loss: 262.83
 ---- batch: 040 ----
mean loss: 257.53
 ---- batch: 050 ----
mean loss: 255.45
train mean loss: 256.98
epoch train time: 0:00:07.862683
elapsed time: 0:14:58.178151
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 12:28:43.487062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.29
 ---- batch: 020 ----
mean loss: 245.39
 ---- batch: 030 ----
mean loss: 256.39
 ---- batch: 040 ----
mean loss: 247.53
 ---- batch: 050 ----
mean loss: 255.45
train mean loss: 253.80
epoch train time: 0:00:07.853715
elapsed time: 0:15:06.032970
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 12:28:51.341910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.07
 ---- batch: 020 ----
mean loss: 257.64
 ---- batch: 030 ----
mean loss: 255.90
 ---- batch: 040 ----
mean loss: 251.92
 ---- batch: 050 ----
mean loss: 247.50
train mean loss: 252.79
epoch train time: 0:00:07.805585
elapsed time: 0:15:13.839754
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 12:28:59.148657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.65
 ---- batch: 020 ----
mean loss: 261.86
 ---- batch: 030 ----
mean loss: 255.16
 ---- batch: 040 ----
mean loss: 255.03
 ---- batch: 050 ----
mean loss: 252.50
train mean loss: 253.62
epoch train time: 0:00:07.819324
elapsed time: 0:15:21.660238
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 12:29:06.969119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.51
 ---- batch: 020 ----
mean loss: 248.85
 ---- batch: 030 ----
mean loss: 255.11
 ---- batch: 040 ----
mean loss: 258.59
 ---- batch: 050 ----
mean loss: 242.55
train mean loss: 249.62
epoch train time: 0:00:07.827382
elapsed time: 0:15:29.488784
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 12:29:14.797769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.84
 ---- batch: 020 ----
mean loss: 249.08
 ---- batch: 030 ----
mean loss: 249.93
 ---- batch: 040 ----
mean loss: 257.03
 ---- batch: 050 ----
mean loss: 249.90
train mean loss: 250.47
epoch train time: 0:00:07.856093
elapsed time: 0:15:37.346181
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 12:29:22.655118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.13
 ---- batch: 020 ----
mean loss: 252.69
 ---- batch: 030 ----
mean loss: 243.27
 ---- batch: 040 ----
mean loss: 242.27
 ---- batch: 050 ----
mean loss: 249.46
train mean loss: 248.76
epoch train time: 0:00:07.812448
elapsed time: 0:15:45.159777
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 12:29:30.468721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.31
 ---- batch: 020 ----
mean loss: 243.31
 ---- batch: 030 ----
mean loss: 247.70
 ---- batch: 040 ----
mean loss: 249.01
 ---- batch: 050 ----
mean loss: 255.08
train mean loss: 250.12
epoch train time: 0:00:07.801511
elapsed time: 0:15:52.962603
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 12:29:38.271505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.04
 ---- batch: 020 ----
mean loss: 251.21
 ---- batch: 030 ----
mean loss: 241.72
 ---- batch: 040 ----
mean loss: 253.47
 ---- batch: 050 ----
mean loss: 239.00
train mean loss: 247.46
epoch train time: 0:00:07.838481
elapsed time: 0:16:00.802227
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 12:29:46.111123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.27
 ---- batch: 020 ----
mean loss: 250.96
 ---- batch: 030 ----
mean loss: 252.00
 ---- batch: 040 ----
mean loss: 241.65
 ---- batch: 050 ----
mean loss: 258.97
train mean loss: 250.35
epoch train time: 0:00:07.846315
elapsed time: 0:16:08.649792
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 12:29:53.958693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.53
 ---- batch: 020 ----
mean loss: 246.46
 ---- batch: 030 ----
mean loss: 245.28
 ---- batch: 040 ----
mean loss: 242.82
 ---- batch: 050 ----
mean loss: 235.14
train mean loss: 244.57
epoch train time: 0:00:07.847461
elapsed time: 0:16:16.498452
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 12:30:01.807354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.47
 ---- batch: 020 ----
mean loss: 243.25
 ---- batch: 030 ----
mean loss: 251.29
 ---- batch: 040 ----
mean loss: 251.55
 ---- batch: 050 ----
mean loss: 238.60
train mean loss: 245.88
epoch train time: 0:00:07.796727
elapsed time: 0:16:24.296439
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 12:30:09.605394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.51
 ---- batch: 020 ----
mean loss: 243.45
 ---- batch: 030 ----
mean loss: 238.50
 ---- batch: 040 ----
mean loss: 243.83
 ---- batch: 050 ----
mean loss: 238.78
train mean loss: 241.18
epoch train time: 0:00:07.808898
elapsed time: 0:16:32.106520
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 12:30:17.415427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.46
 ---- batch: 020 ----
mean loss: 246.26
 ---- batch: 030 ----
mean loss: 238.12
 ---- batch: 040 ----
mean loss: 244.73
 ---- batch: 050 ----
mean loss: 243.29
train mean loss: 242.52
epoch train time: 0:00:07.797760
elapsed time: 0:16:39.905447
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 12:30:25.214348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.58
 ---- batch: 020 ----
mean loss: 239.83
 ---- batch: 030 ----
mean loss: 233.06
 ---- batch: 040 ----
mean loss: 243.11
 ---- batch: 050 ----
mean loss: 236.13
train mean loss: 239.74
epoch train time: 0:00:07.812982
elapsed time: 0:16:47.719610
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 12:30:33.028521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.96
 ---- batch: 020 ----
mean loss: 238.41
 ---- batch: 030 ----
mean loss: 243.13
 ---- batch: 040 ----
mean loss: 241.32
 ---- batch: 050 ----
mean loss: 244.43
train mean loss: 240.06
epoch train time: 0:00:07.800062
elapsed time: 0:16:55.521311
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 12:30:40.829930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.24
 ---- batch: 020 ----
mean loss: 239.85
 ---- batch: 030 ----
mean loss: 240.81
 ---- batch: 040 ----
mean loss: 230.04
 ---- batch: 050 ----
mean loss: 239.05
train mean loss: 238.61
epoch train time: 0:00:07.781495
elapsed time: 0:17:03.303643
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 12:30:48.612583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.16
 ---- batch: 020 ----
mean loss: 231.34
 ---- batch: 030 ----
mean loss: 232.54
 ---- batch: 040 ----
mean loss: 242.35
 ---- batch: 050 ----
mean loss: 238.34
train mean loss: 237.64
epoch train time: 0:00:07.800222
elapsed time: 0:17:11.105104
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 12:30:56.414005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.92
 ---- batch: 020 ----
mean loss: 237.00
 ---- batch: 030 ----
mean loss: 234.58
 ---- batch: 040 ----
mean loss: 234.45
 ---- batch: 050 ----
mean loss: 230.93
train mean loss: 235.90
epoch train time: 0:00:07.777287
elapsed time: 0:17:18.883702
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 12:31:04.192610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.18
 ---- batch: 020 ----
mean loss: 240.45
 ---- batch: 030 ----
mean loss: 238.92
 ---- batch: 040 ----
mean loss: 232.21
 ---- batch: 050 ----
mean loss: 235.26
train mean loss: 234.46
epoch train time: 0:00:07.689635
elapsed time: 0:17:26.574519
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 12:31:11.883414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.69
 ---- batch: 020 ----
mean loss: 232.08
 ---- batch: 030 ----
mean loss: 234.57
 ---- batch: 040 ----
mean loss: 240.28
 ---- batch: 050 ----
mean loss: 233.23
train mean loss: 235.33
epoch train time: 0:00:07.712581
elapsed time: 0:17:34.288230
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 12:31:19.597133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.86
 ---- batch: 020 ----
mean loss: 227.96
 ---- batch: 030 ----
mean loss: 232.40
 ---- batch: 040 ----
mean loss: 238.83
 ---- batch: 050 ----
mean loss: 234.17
train mean loss: 233.54
epoch train time: 0:00:07.708771
elapsed time: 0:17:41.998345
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 12:31:27.307244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.56
 ---- batch: 020 ----
mean loss: 234.58
 ---- batch: 030 ----
mean loss: 238.55
 ---- batch: 040 ----
mean loss: 237.79
 ---- batch: 050 ----
mean loss: 230.13
train mean loss: 233.23
epoch train time: 0:00:07.707303
elapsed time: 0:17:49.706809
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 12:31:35.015712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.92
 ---- batch: 020 ----
mean loss: 238.20
 ---- batch: 030 ----
mean loss: 228.52
 ---- batch: 040 ----
mean loss: 227.04
 ---- batch: 050 ----
mean loss: 224.29
train mean loss: 231.67
epoch train time: 0:00:07.688267
elapsed time: 0:17:57.396241
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 12:31:42.705112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.84
 ---- batch: 020 ----
mean loss: 228.93
 ---- batch: 030 ----
mean loss: 227.18
 ---- batch: 040 ----
mean loss: 231.77
 ---- batch: 050 ----
mean loss: 229.50
train mean loss: 230.02
epoch train time: 0:00:07.691262
elapsed time: 0:18:05.088570
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 12:31:50.397448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.88
 ---- batch: 020 ----
mean loss: 229.68
 ---- batch: 030 ----
mean loss: 235.09
 ---- batch: 040 ----
mean loss: 236.66
 ---- batch: 050 ----
mean loss: 233.04
train mean loss: 231.16
epoch train time: 0:00:07.758469
elapsed time: 0:18:12.848239
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 12:31:58.157130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.32
 ---- batch: 020 ----
mean loss: 239.04
 ---- batch: 030 ----
mean loss: 223.54
 ---- batch: 040 ----
mean loss: 240.49
 ---- batch: 050 ----
mean loss: 233.91
train mean loss: 230.93
epoch train time: 0:00:07.742451
elapsed time: 0:18:20.591892
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 12:32:05.900810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.70
 ---- batch: 020 ----
mean loss: 239.28
 ---- batch: 030 ----
mean loss: 223.80
 ---- batch: 040 ----
mean loss: 230.26
 ---- batch: 050 ----
mean loss: 232.29
train mean loss: 230.99
epoch train time: 0:00:07.672147
elapsed time: 0:18:28.265174
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 12:32:13.574082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.71
 ---- batch: 020 ----
mean loss: 228.37
 ---- batch: 030 ----
mean loss: 229.92
 ---- batch: 040 ----
mean loss: 234.39
 ---- batch: 050 ----
mean loss: 227.59
train mean loss: 230.31
epoch train time: 0:00:07.665963
elapsed time: 0:18:35.932228
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 12:32:21.241142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.64
 ---- batch: 020 ----
mean loss: 229.93
 ---- batch: 030 ----
mean loss: 231.52
 ---- batch: 040 ----
mean loss: 221.29
 ---- batch: 050 ----
mean loss: 240.22
train mean loss: 229.29
epoch train time: 0:00:07.678940
elapsed time: 0:18:43.612321
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 12:32:28.921198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.50
 ---- batch: 020 ----
mean loss: 234.92
 ---- batch: 030 ----
mean loss: 224.12
 ---- batch: 040 ----
mean loss: 227.16
 ---- batch: 050 ----
mean loss: 218.61
train mean loss: 225.62
epoch train time: 0:00:07.666543
elapsed time: 0:18:51.279911
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 12:32:36.588817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.76
 ---- batch: 020 ----
mean loss: 221.72
 ---- batch: 030 ----
mean loss: 225.81
 ---- batch: 040 ----
mean loss: 221.69
 ---- batch: 050 ----
mean loss: 226.15
train mean loss: 225.20
epoch train time: 0:00:07.660601
elapsed time: 0:18:58.941759
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 12:32:44.250665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.61
 ---- batch: 020 ----
mean loss: 223.49
 ---- batch: 030 ----
mean loss: 221.57
 ---- batch: 040 ----
mean loss: 217.02
 ---- batch: 050 ----
mean loss: 223.04
train mean loss: 222.82
epoch train time: 0:00:07.623242
elapsed time: 0:19:06.566207
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 12:32:51.875080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.28
 ---- batch: 020 ----
mean loss: 221.14
 ---- batch: 030 ----
mean loss: 221.68
 ---- batch: 040 ----
mean loss: 217.01
 ---- batch: 050 ----
mean loss: 233.72
train mean loss: 222.55
epoch train time: 0:00:07.587008
elapsed time: 0:19:14.154285
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 12:32:59.463120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.12
 ---- batch: 020 ----
mean loss: 228.43
 ---- batch: 030 ----
mean loss: 226.11
 ---- batch: 040 ----
mean loss: 233.27
 ---- batch: 050 ----
mean loss: 215.60
train mean loss: 226.53
epoch train time: 0:00:07.681180
elapsed time: 0:19:21.836518
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 12:33:07.145405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.40
 ---- batch: 020 ----
mean loss: 217.12
 ---- batch: 030 ----
mean loss: 216.41
 ---- batch: 040 ----
mean loss: 227.90
 ---- batch: 050 ----
mean loss: 224.74
train mean loss: 221.56
epoch train time: 0:00:07.629818
elapsed time: 0:19:29.467498
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 12:33:14.776410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.19
 ---- batch: 020 ----
mean loss: 209.13
 ---- batch: 030 ----
mean loss: 225.18
 ---- batch: 040 ----
mean loss: 233.05
 ---- batch: 050 ----
mean loss: 220.56
train mean loss: 221.48
epoch train time: 0:00:07.787210
elapsed time: 0:19:37.256367
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 12:33:22.565344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.31
 ---- batch: 020 ----
mean loss: 212.10
 ---- batch: 030 ----
mean loss: 221.13
 ---- batch: 040 ----
mean loss: 221.28
 ---- batch: 050 ----
mean loss: 220.18
train mean loss: 220.01
epoch train time: 0:00:07.808325
elapsed time: 0:19:45.066184
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 12:33:30.374822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.69
 ---- batch: 020 ----
mean loss: 220.11
 ---- batch: 030 ----
mean loss: 222.52
 ---- batch: 040 ----
mean loss: 208.69
 ---- batch: 050 ----
mean loss: 226.89
train mean loss: 218.51
epoch train time: 0:00:07.803489
elapsed time: 0:19:52.870549
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 12:33:38.179489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.54
 ---- batch: 020 ----
mean loss: 220.40
 ---- batch: 030 ----
mean loss: 228.51
 ---- batch: 040 ----
mean loss: 217.59
 ---- batch: 050 ----
mean loss: 220.73
train mean loss: 221.63
epoch train time: 0:00:07.826802
elapsed time: 0:20:00.698547
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 12:33:46.007427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.68
 ---- batch: 020 ----
mean loss: 218.28
 ---- batch: 030 ----
mean loss: 220.37
 ---- batch: 040 ----
mean loss: 213.23
 ---- batch: 050 ----
mean loss: 216.27
train mean loss: 219.01
epoch train time: 0:00:07.891652
elapsed time: 0:20:08.591323
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 12:33:53.900203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.78
 ---- batch: 020 ----
mean loss: 222.58
 ---- batch: 030 ----
mean loss: 216.21
 ---- batch: 040 ----
mean loss: 219.86
 ---- batch: 050 ----
mean loss: 223.08
train mean loss: 218.90
epoch train time: 0:00:07.786941
elapsed time: 0:20:16.379331
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 12:34:01.688214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.81
 ---- batch: 020 ----
mean loss: 223.99
 ---- batch: 030 ----
mean loss: 224.14
 ---- batch: 040 ----
mean loss: 209.61
 ---- batch: 050 ----
mean loss: 216.19
train mean loss: 218.91
epoch train time: 0:00:07.612454
elapsed time: 0:20:23.992939
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 12:34:09.301849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.12
 ---- batch: 020 ----
mean loss: 224.50
 ---- batch: 030 ----
mean loss: 210.96
 ---- batch: 040 ----
mean loss: 224.37
 ---- batch: 050 ----
mean loss: 214.83
train mean loss: 218.32
epoch train time: 0:00:07.692166
elapsed time: 0:20:31.686207
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 12:34:16.995057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.94
 ---- batch: 020 ----
mean loss: 208.35
 ---- batch: 030 ----
mean loss: 228.35
 ---- batch: 040 ----
mean loss: 213.64
 ---- batch: 050 ----
mean loss: 223.03
train mean loss: 216.97
epoch train time: 0:00:07.645802
elapsed time: 0:20:39.333137
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 12:34:24.642026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.65
 ---- batch: 020 ----
mean loss: 216.00
 ---- batch: 030 ----
mean loss: 209.75
 ---- batch: 040 ----
mean loss: 220.55
 ---- batch: 050 ----
mean loss: 214.64
train mean loss: 217.36
epoch train time: 0:00:07.856963
elapsed time: 0:20:47.191234
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 12:34:32.500146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.58
 ---- batch: 020 ----
mean loss: 219.34
 ---- batch: 030 ----
mean loss: 217.34
 ---- batch: 040 ----
mean loss: 206.64
 ---- batch: 050 ----
mean loss: 216.83
train mean loss: 214.30
epoch train time: 0:00:07.880055
elapsed time: 0:20:55.072440
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 12:34:40.381415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.61
 ---- batch: 020 ----
mean loss: 207.96
 ---- batch: 030 ----
mean loss: 220.04
 ---- batch: 040 ----
mean loss: 212.74
 ---- batch: 050 ----
mean loss: 210.11
train mean loss: 213.84
epoch train time: 0:00:07.774849
elapsed time: 0:21:02.848675
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 12:34:48.157571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.23
 ---- batch: 020 ----
mean loss: 216.39
 ---- batch: 030 ----
mean loss: 204.80
 ---- batch: 040 ----
mean loss: 213.05
 ---- batch: 050 ----
mean loss: 214.90
train mean loss: 213.09
epoch train time: 0:00:07.718412
elapsed time: 0:21:10.568309
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 12:34:55.877199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.94
 ---- batch: 020 ----
mean loss: 211.38
 ---- batch: 030 ----
mean loss: 211.33
 ---- batch: 040 ----
mean loss: 220.67
 ---- batch: 050 ----
mean loss: 207.63
train mean loss: 215.07
epoch train time: 0:00:07.794774
elapsed time: 0:21:18.364242
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 12:35:03.673223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.38
 ---- batch: 020 ----
mean loss: 226.97
 ---- batch: 030 ----
mean loss: 214.52
 ---- batch: 040 ----
mean loss: 208.59
 ---- batch: 050 ----
mean loss: 217.32
train mean loss: 216.01
epoch train time: 0:00:07.627759
elapsed time: 0:21:25.993259
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 12:35:11.302130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.28
 ---- batch: 020 ----
mean loss: 210.51
 ---- batch: 030 ----
mean loss: 214.86
 ---- batch: 040 ----
mean loss: 210.65
 ---- batch: 050 ----
mean loss: 213.71
train mean loss: 212.07
epoch train time: 0:00:07.659569
elapsed time: 0:21:33.653917
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 12:35:18.962837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.60
 ---- batch: 020 ----
mean loss: 214.42
 ---- batch: 030 ----
mean loss: 209.96
 ---- batch: 040 ----
mean loss: 213.81
 ---- batch: 050 ----
mean loss: 209.64
train mean loss: 212.24
epoch train time: 0:00:07.636651
elapsed time: 0:21:41.291693
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 12:35:26.600607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.83
 ---- batch: 020 ----
mean loss: 212.18
 ---- batch: 030 ----
mean loss: 221.93
 ---- batch: 040 ----
mean loss: 214.44
 ---- batch: 050 ----
mean loss: 208.50
train mean loss: 213.21
epoch train time: 0:00:07.645453
elapsed time: 0:21:48.938244
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 12:35:34.247110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.90
 ---- batch: 020 ----
mean loss: 214.46
 ---- batch: 030 ----
mean loss: 220.18
 ---- batch: 040 ----
mean loss: 203.06
 ---- batch: 050 ----
mean loss: 207.00
train mean loss: 210.83
epoch train time: 0:00:07.599526
elapsed time: 0:21:56.538884
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 12:35:41.847810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.04
 ---- batch: 020 ----
mean loss: 202.26
 ---- batch: 030 ----
mean loss: 214.52
 ---- batch: 040 ----
mean loss: 211.09
 ---- batch: 050 ----
mean loss: 209.64
train mean loss: 209.33
epoch train time: 0:00:07.631229
elapsed time: 0:22:04.171286
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 12:35:49.480138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.67
 ---- batch: 020 ----
mean loss: 211.05
 ---- batch: 030 ----
mean loss: 216.50
 ---- batch: 040 ----
mean loss: 206.57
 ---- batch: 050 ----
mean loss: 201.38
train mean loss: 209.32
epoch train time: 0:00:07.595883
elapsed time: 0:22:11.768300
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 12:35:57.077263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.93
 ---- batch: 020 ----
mean loss: 215.80
 ---- batch: 030 ----
mean loss: 208.60
 ---- batch: 040 ----
mean loss: 209.30
 ---- batch: 050 ----
mean loss: 202.14
train mean loss: 207.45
epoch train time: 0:00:07.802619
elapsed time: 0:22:19.572129
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 12:36:04.881016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.90
 ---- batch: 020 ----
mean loss: 212.80
 ---- batch: 030 ----
mean loss: 207.76
 ---- batch: 040 ----
mean loss: 214.93
 ---- batch: 050 ----
mean loss: 207.35
train mean loss: 210.57
epoch train time: 0:00:07.861985
elapsed time: 0:22:27.435287
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 12:36:12.744187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.19
 ---- batch: 020 ----
mean loss: 215.65
 ---- batch: 030 ----
mean loss: 217.35
 ---- batch: 040 ----
mean loss: 214.42
 ---- batch: 050 ----
mean loss: 204.78
train mean loss: 214.46
epoch train time: 0:00:07.794331
elapsed time: 0:22:35.230781
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 12:36:20.539677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.28
 ---- batch: 020 ----
mean loss: 207.71
 ---- batch: 030 ----
mean loss: 208.80
 ---- batch: 040 ----
mean loss: 220.87
 ---- batch: 050 ----
mean loss: 204.41
train mean loss: 208.90
epoch train time: 0:00:07.813716
elapsed time: 0:22:43.045706
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 12:36:28.354725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.08
 ---- batch: 020 ----
mean loss: 209.02
 ---- batch: 030 ----
mean loss: 208.26
 ---- batch: 040 ----
mean loss: 212.04
 ---- batch: 050 ----
mean loss: 207.22
train mean loss: 208.65
epoch train time: 0:00:07.871393
elapsed time: 0:22:50.918360
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 12:36:36.227253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.07
 ---- batch: 020 ----
mean loss: 204.71
 ---- batch: 030 ----
mean loss: 212.62
 ---- batch: 040 ----
mean loss: 206.26
 ---- batch: 050 ----
mean loss: 207.90
train mean loss: 206.14
epoch train time: 0:00:07.815717
elapsed time: 0:22:58.735525
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 12:36:44.044122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.24
 ---- batch: 020 ----
mean loss: 204.02
 ---- batch: 030 ----
mean loss: 209.27
 ---- batch: 040 ----
mean loss: 205.26
 ---- batch: 050 ----
mean loss: 212.77
train mean loss: 207.28
epoch train time: 0:00:07.815703
elapsed time: 0:23:06.552248
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 12:36:51.861169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.47
 ---- batch: 020 ----
mean loss: 205.31
 ---- batch: 030 ----
mean loss: 204.26
 ---- batch: 040 ----
mean loss: 200.58
 ---- batch: 050 ----
mean loss: 204.12
train mean loss: 205.87
epoch train time: 0:00:07.826658
elapsed time: 0:23:14.380112
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 12:36:59.688995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.86
 ---- batch: 020 ----
mean loss: 204.69
 ---- batch: 030 ----
mean loss: 212.43
 ---- batch: 040 ----
mean loss: 212.13
 ---- batch: 050 ----
mean loss: 208.19
train mean loss: 205.83
epoch train time: 0:00:07.845503
elapsed time: 0:23:22.226725
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 12:37:07.535614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.75
 ---- batch: 020 ----
mean loss: 211.18
 ---- batch: 030 ----
mean loss: 209.57
 ---- batch: 040 ----
mean loss: 201.11
 ---- batch: 050 ----
mean loss: 200.89
train mean loss: 205.26
epoch train time: 0:00:07.826777
elapsed time: 0:23:30.054651
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 12:37:15.363557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.80
 ---- batch: 020 ----
mean loss: 198.31
 ---- batch: 030 ----
mean loss: 205.34
 ---- batch: 040 ----
mean loss: 208.36
 ---- batch: 050 ----
mean loss: 206.34
train mean loss: 203.32
epoch train time: 0:00:07.841359
elapsed time: 0:23:37.897348
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 12:37:23.206261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.20
 ---- batch: 020 ----
mean loss: 201.08
 ---- batch: 030 ----
mean loss: 201.57
 ---- batch: 040 ----
mean loss: 209.67
 ---- batch: 050 ----
mean loss: 202.16
train mean loss: 203.51
epoch train time: 0:00:07.974932
elapsed time: 0:23:45.873473
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 12:37:31.182371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.69
 ---- batch: 020 ----
mean loss: 206.98
 ---- batch: 030 ----
mean loss: 208.21
 ---- batch: 040 ----
mean loss: 203.96
 ---- batch: 050 ----
mean loss: 199.50
train mean loss: 203.41
epoch train time: 0:00:08.066543
elapsed time: 0:23:53.941252
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 12:37:39.250162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.47
 ---- batch: 020 ----
mean loss: 206.45
 ---- batch: 030 ----
mean loss: 199.77
 ---- batch: 040 ----
mean loss: 205.28
 ---- batch: 050 ----
mean loss: 215.13
train mean loss: 205.02
epoch train time: 0:00:08.026283
elapsed time: 0:24:01.968738
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 12:37:47.277696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.59
 ---- batch: 020 ----
mean loss: 207.11
 ---- batch: 030 ----
mean loss: 202.31
 ---- batch: 040 ----
mean loss: 207.04
 ---- batch: 050 ----
mean loss: 210.03
train mean loss: 205.11
epoch train time: 0:00:07.908505
elapsed time: 0:24:09.878444
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 12:37:55.187316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.87
 ---- batch: 020 ----
mean loss: 199.40
 ---- batch: 030 ----
mean loss: 207.95
 ---- batch: 040 ----
mean loss: 201.90
 ---- batch: 050 ----
mean loss: 204.21
train mean loss: 203.41
epoch train time: 0:00:07.815730
elapsed time: 0:24:17.695323
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 12:38:03.004209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.93
 ---- batch: 020 ----
mean loss: 203.86
 ---- batch: 030 ----
mean loss: 197.08
 ---- batch: 040 ----
mean loss: 206.23
 ---- batch: 050 ----
mean loss: 196.13
train mean loss: 200.80
epoch train time: 0:00:07.746505
elapsed time: 0:24:25.443001
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 12:38:10.751929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.38
 ---- batch: 020 ----
mean loss: 213.15
 ---- batch: 030 ----
mean loss: 198.04
 ---- batch: 040 ----
mean loss: 195.94
 ---- batch: 050 ----
mean loss: 206.59
train mean loss: 202.32
epoch train time: 0:00:07.774080
elapsed time: 0:24:33.218300
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 12:38:18.527200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.87
 ---- batch: 020 ----
mean loss: 194.22
 ---- batch: 030 ----
mean loss: 191.85
 ---- batch: 040 ----
mean loss: 207.72
 ---- batch: 050 ----
mean loss: 208.83
train mean loss: 200.50
epoch train time: 0:00:07.862835
elapsed time: 0:24:41.082354
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 12:38:26.391301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.18
 ---- batch: 020 ----
mean loss: 199.78
 ---- batch: 030 ----
mean loss: 196.18
 ---- batch: 040 ----
mean loss: 202.19
 ---- batch: 050 ----
mean loss: 202.91
train mean loss: 202.37
epoch train time: 0:00:07.855365
elapsed time: 0:24:48.938974
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 12:38:34.247881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.13
 ---- batch: 020 ----
mean loss: 199.11
 ---- batch: 030 ----
mean loss: 198.74
 ---- batch: 040 ----
mean loss: 194.98
 ---- batch: 050 ----
mean loss: 204.17
train mean loss: 200.23
epoch train time: 0:00:07.795453
elapsed time: 0:24:56.735586
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 12:38:42.044488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.51
 ---- batch: 020 ----
mean loss: 198.44
 ---- batch: 030 ----
mean loss: 206.05
 ---- batch: 040 ----
mean loss: 198.47
 ---- batch: 050 ----
mean loss: 195.55
train mean loss: 199.77
epoch train time: 0:00:07.783008
elapsed time: 0:25:04.519724
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 12:38:49.828637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.80
 ---- batch: 020 ----
mean loss: 202.26
 ---- batch: 030 ----
mean loss: 191.17
 ---- batch: 040 ----
mean loss: 201.46
 ---- batch: 050 ----
mean loss: 196.22
train mean loss: 200.49
epoch train time: 0:00:07.707048
elapsed time: 0:25:12.227912
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 12:38:57.536781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.21
 ---- batch: 020 ----
mean loss: 202.78
 ---- batch: 030 ----
mean loss: 193.50
 ---- batch: 040 ----
mean loss: 204.15
 ---- batch: 050 ----
mean loss: 204.22
train mean loss: 199.64
epoch train time: 0:00:07.725647
elapsed time: 0:25:19.954751
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 12:39:05.263674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.98
 ---- batch: 020 ----
mean loss: 197.43
 ---- batch: 030 ----
mean loss: 201.57
 ---- batch: 040 ----
mean loss: 200.28
 ---- batch: 050 ----
mean loss: 189.62
train mean loss: 198.13
epoch train time: 0:00:07.764534
elapsed time: 0:25:27.720469
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 12:39:13.029383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.59
 ---- batch: 020 ----
mean loss: 197.69
 ---- batch: 030 ----
mean loss: 196.71
 ---- batch: 040 ----
mean loss: 210.37
 ---- batch: 050 ----
mean loss: 204.97
train mean loss: 198.90
epoch train time: 0:00:07.867829
elapsed time: 0:25:35.589577
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 12:39:20.898467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.98
 ---- batch: 020 ----
mean loss: 201.87
 ---- batch: 030 ----
mean loss: 194.12
 ---- batch: 040 ----
mean loss: 200.39
 ---- batch: 050 ----
mean loss: 199.89
train mean loss: 199.81
epoch train time: 0:00:07.874621
elapsed time: 0:25:43.465415
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 12:39:28.774411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.76
 ---- batch: 020 ----
mean loss: 201.96
 ---- batch: 030 ----
mean loss: 197.39
 ---- batch: 040 ----
mean loss: 198.48
 ---- batch: 050 ----
mean loss: 198.85
train mean loss: 199.48
epoch train time: 0:00:07.893318
elapsed time: 0:25:51.360036
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 12:39:36.668963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.01
 ---- batch: 020 ----
mean loss: 199.53
 ---- batch: 030 ----
mean loss: 195.23
 ---- batch: 040 ----
mean loss: 195.17
 ---- batch: 050 ----
mean loss: 197.60
train mean loss: 197.73
epoch train time: 0:00:07.880076
elapsed time: 0:25:59.241320
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 12:39:44.550261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.62
 ---- batch: 020 ----
mean loss: 191.35
 ---- batch: 030 ----
mean loss: 204.69
 ---- batch: 040 ----
mean loss: 199.42
 ---- batch: 050 ----
mean loss: 199.44
train mean loss: 199.03
epoch train time: 0:00:07.877866
elapsed time: 0:26:07.120421
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 12:39:52.429354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.32
 ---- batch: 020 ----
mean loss: 200.62
 ---- batch: 030 ----
mean loss: 206.46
 ---- batch: 040 ----
mean loss: 196.30
 ---- batch: 050 ----
mean loss: 196.66
train mean loss: 199.44
epoch train time: 0:00:07.826465
elapsed time: 0:26:14.948012
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 12:40:00.256918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.25
 ---- batch: 020 ----
mean loss: 196.68
 ---- batch: 030 ----
mean loss: 202.40
 ---- batch: 040 ----
mean loss: 205.36
 ---- batch: 050 ----
mean loss: 192.39
train mean loss: 198.33
epoch train time: 0:00:07.817511
elapsed time: 0:26:22.766714
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 12:40:08.075621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.33
 ---- batch: 020 ----
mean loss: 202.44
 ---- batch: 030 ----
mean loss: 192.82
 ---- batch: 040 ----
mean loss: 195.55
 ---- batch: 050 ----
mean loss: 198.19
train mean loss: 197.21
epoch train time: 0:00:07.792872
elapsed time: 0:26:30.560672
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 12:40:15.869563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.26
 ---- batch: 020 ----
mean loss: 192.97
 ---- batch: 030 ----
mean loss: 191.55
 ---- batch: 040 ----
mean loss: 192.67
 ---- batch: 050 ----
mean loss: 197.63
train mean loss: 194.31
epoch train time: 0:00:07.813221
elapsed time: 0:26:38.375443
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 12:40:23.684026
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.89
 ---- batch: 020 ----
mean loss: 195.45
 ---- batch: 030 ----
mean loss: 191.42
 ---- batch: 040 ----
mean loss: 186.34
 ---- batch: 050 ----
mean loss: 196.43
train mean loss: 195.03
epoch train time: 0:00:07.796961
elapsed time: 0:26:46.173201
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 12:40:31.482086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.20
 ---- batch: 020 ----
mean loss: 194.22
 ---- batch: 030 ----
mean loss: 187.62
 ---- batch: 040 ----
mean loss: 192.98
 ---- batch: 050 ----
mean loss: 199.96
train mean loss: 193.99
epoch train time: 0:00:07.796402
elapsed time: 0:26:53.970739
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 12:40:39.279705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.14
 ---- batch: 020 ----
mean loss: 192.17
 ---- batch: 030 ----
mean loss: 192.89
 ---- batch: 040 ----
mean loss: 194.46
 ---- batch: 050 ----
mean loss: 192.32
train mean loss: 194.36
epoch train time: 0:00:07.805362
elapsed time: 0:27:01.777421
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 12:40:47.086302
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.26
 ---- batch: 020 ----
mean loss: 200.33
 ---- batch: 030 ----
mean loss: 191.88
 ---- batch: 040 ----
mean loss: 195.11
 ---- batch: 050 ----
mean loss: 198.47
train mean loss: 194.07
epoch train time: 0:00:07.798935
elapsed time: 0:27:09.577442
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 12:40:54.886309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.81
 ---- batch: 020 ----
mean loss: 198.13
 ---- batch: 030 ----
mean loss: 198.24
 ---- batch: 040 ----
mean loss: 199.30
 ---- batch: 050 ----
mean loss: 195.70
train mean loss: 193.99
epoch train time: 0:00:07.809946
elapsed time: 0:27:17.388508
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 12:41:02.697399
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.66
 ---- batch: 020 ----
mean loss: 193.40
 ---- batch: 030 ----
mean loss: 192.90
 ---- batch: 040 ----
mean loss: 200.21
 ---- batch: 050 ----
mean loss: 189.94
train mean loss: 194.33
epoch train time: 0:00:07.832859
elapsed time: 0:27:25.222468
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 12:41:10.531363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.04
 ---- batch: 020 ----
mean loss: 199.00
 ---- batch: 030 ----
mean loss: 199.13
 ---- batch: 040 ----
mean loss: 182.63
 ---- batch: 050 ----
mean loss: 195.24
train mean loss: 193.50
epoch train time: 0:00:07.834227
elapsed time: 0:27:33.057771
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 12:41:18.366668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.46
 ---- batch: 020 ----
mean loss: 194.94
 ---- batch: 030 ----
mean loss: 189.42
 ---- batch: 040 ----
mean loss: 197.64
 ---- batch: 050 ----
mean loss: 193.79
train mean loss: 193.95
epoch train time: 0:00:07.841054
elapsed time: 0:27:40.899913
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 12:41:26.208820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.48
 ---- batch: 020 ----
mean loss: 198.99
 ---- batch: 030 ----
mean loss: 192.38
 ---- batch: 040 ----
mean loss: 194.83
 ---- batch: 050 ----
mean loss: 193.34
train mean loss: 193.58
epoch train time: 0:00:07.827345
elapsed time: 0:27:48.728507
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 12:41:34.037406
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.63
 ---- batch: 020 ----
mean loss: 200.72
 ---- batch: 030 ----
mean loss: 196.20
 ---- batch: 040 ----
mean loss: 185.88
 ---- batch: 050 ----
mean loss: 196.86
train mean loss: 194.68
epoch train time: 0:00:07.830268
elapsed time: 0:27:56.559908
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 12:41:41.868782
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.67
 ---- batch: 020 ----
mean loss: 193.30
 ---- batch: 030 ----
mean loss: 197.43
 ---- batch: 040 ----
mean loss: 189.33
 ---- batch: 050 ----
mean loss: 193.76
train mean loss: 193.99
epoch train time: 0:00:07.838019
elapsed time: 0:28:04.399047
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 12:41:49.707946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.41
 ---- batch: 020 ----
mean loss: 191.38
 ---- batch: 030 ----
mean loss: 199.01
 ---- batch: 040 ----
mean loss: 185.66
 ---- batch: 050 ----
mean loss: 196.09
train mean loss: 194.64
epoch train time: 0:00:07.834302
elapsed time: 0:28:12.234545
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 12:41:57.543464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.41
 ---- batch: 020 ----
mean loss: 193.08
 ---- batch: 030 ----
mean loss: 190.97
 ---- batch: 040 ----
mean loss: 189.29
 ---- batch: 050 ----
mean loss: 191.16
train mean loss: 192.91
epoch train time: 0:00:07.805639
elapsed time: 0:28:20.041335
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 12:42:05.350061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.53
 ---- batch: 020 ----
mean loss: 189.83
 ---- batch: 030 ----
mean loss: 187.15
 ---- batch: 040 ----
mean loss: 208.01
 ---- batch: 050 ----
mean loss: 196.31
train mean loss: 194.18
epoch train time: 0:00:07.820181
elapsed time: 0:28:27.862537
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 12:42:13.171447
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.11
 ---- batch: 020 ----
mean loss: 197.92
 ---- batch: 030 ----
mean loss: 197.70
 ---- batch: 040 ----
mean loss: 196.21
 ---- batch: 050 ----
mean loss: 189.33
train mean loss: 194.24
epoch train time: 0:00:07.819616
elapsed time: 0:28:35.683365
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 12:42:20.992258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.24
 ---- batch: 020 ----
mean loss: 192.22
 ---- batch: 030 ----
mean loss: 195.83
 ---- batch: 040 ----
mean loss: 195.88
 ---- batch: 050 ----
mean loss: 187.78
train mean loss: 194.34
epoch train time: 0:00:07.835373
elapsed time: 0:28:43.519908
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 12:42:28.828784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.45
 ---- batch: 020 ----
mean loss: 186.43
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 184.63
 ---- batch: 050 ----
mean loss: 194.85
train mean loss: 193.73
epoch train time: 0:00:07.814770
elapsed time: 0:28:51.335771
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 12:42:36.644695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.29
 ---- batch: 020 ----
mean loss: 188.46
 ---- batch: 030 ----
mean loss: 190.16
 ---- batch: 040 ----
mean loss: 194.05
 ---- batch: 050 ----
mean loss: 194.13
train mean loss: 193.83
epoch train time: 0:00:07.827671
elapsed time: 0:28:59.164569
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 12:42:44.473468
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.80
 ---- batch: 020 ----
mean loss: 200.23
 ---- batch: 030 ----
mean loss: 187.71
 ---- batch: 040 ----
mean loss: 201.91
 ---- batch: 050 ----
mean loss: 188.40
train mean loss: 193.48
epoch train time: 0:00:07.831541
elapsed time: 0:29:06.997293
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 12:42:52.306210
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.50
 ---- batch: 020 ----
mean loss: 198.61
 ---- batch: 030 ----
mean loss: 195.40
 ---- batch: 040 ----
mean loss: 196.81
 ---- batch: 050 ----
mean loss: 202.61
train mean loss: 193.45
epoch train time: 0:00:07.836267
elapsed time: 0:29:14.834691
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 12:43:00.143585
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.19
 ---- batch: 020 ----
mean loss: 196.37
 ---- batch: 030 ----
mean loss: 189.57
 ---- batch: 040 ----
mean loss: 192.72
 ---- batch: 050 ----
mean loss: 199.03
train mean loss: 193.19
epoch train time: 0:00:07.821457
elapsed time: 0:29:22.657364
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 12:43:07.966287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.57
 ---- batch: 020 ----
mean loss: 188.82
 ---- batch: 030 ----
mean loss: 193.06
 ---- batch: 040 ----
mean loss: 198.23
 ---- batch: 050 ----
mean loss: 192.80
train mean loss: 193.26
epoch train time: 0:00:07.821191
elapsed time: 0:29:30.479690
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 12:43:15.788575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.71
 ---- batch: 020 ----
mean loss: 202.27
 ---- batch: 030 ----
mean loss: 198.00
 ---- batch: 040 ----
mean loss: 188.99
 ---- batch: 050 ----
mean loss: 193.03
train mean loss: 193.63
epoch train time: 0:00:07.838658
elapsed time: 0:29:38.319457
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 12:43:23.628411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.88
 ---- batch: 020 ----
mean loss: 194.45
 ---- batch: 030 ----
mean loss: 192.12
 ---- batch: 040 ----
mean loss: 195.74
 ---- batch: 050 ----
mean loss: 185.54
train mean loss: 192.21
epoch train time: 0:00:07.847651
elapsed time: 0:29:46.168245
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 12:43:31.477143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.02
 ---- batch: 020 ----
mean loss: 191.93
 ---- batch: 030 ----
mean loss: 196.50
 ---- batch: 040 ----
mean loss: 189.27
 ---- batch: 050 ----
mean loss: 194.61
train mean loss: 192.93
epoch train time: 0:00:07.771793
elapsed time: 0:29:53.941120
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 12:43:39.249983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.89
 ---- batch: 020 ----
mean loss: 185.36
 ---- batch: 030 ----
mean loss: 198.09
 ---- batch: 040 ----
mean loss: 196.85
 ---- batch: 050 ----
mean loss: 187.97
train mean loss: 193.71
epoch train time: 0:00:07.646144
elapsed time: 0:30:01.588474
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 12:43:46.897337
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.51
 ---- batch: 020 ----
mean loss: 196.00
 ---- batch: 030 ----
mean loss: 191.36
 ---- batch: 040 ----
mean loss: 193.65
 ---- batch: 050 ----
mean loss: 193.37
train mean loss: 193.78
epoch train time: 0:00:07.656355
elapsed time: 0:30:09.245805
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 12:43:54.554681
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.12
 ---- batch: 020 ----
mean loss: 197.00
 ---- batch: 030 ----
mean loss: 194.71
 ---- batch: 040 ----
mean loss: 181.92
 ---- batch: 050 ----
mean loss: 198.45
train mean loss: 193.67
epoch train time: 0:00:07.699805
elapsed time: 0:30:16.946734
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 12:44:02.255670
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.03
 ---- batch: 020 ----
mean loss: 191.63
 ---- batch: 030 ----
mean loss: 188.13
 ---- batch: 040 ----
mean loss: 194.74
 ---- batch: 050 ----
mean loss: 202.23
train mean loss: 193.22
epoch train time: 0:00:07.673772
elapsed time: 0:30:24.621667
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 12:44:09.930555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.55
 ---- batch: 020 ----
mean loss: 192.92
 ---- batch: 030 ----
mean loss: 195.69
 ---- batch: 040 ----
mean loss: 187.37
 ---- batch: 050 ----
mean loss: 197.23
train mean loss: 193.54
epoch train time: 0:00:07.716883
elapsed time: 0:30:32.339704
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 12:44:17.648602
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.73
 ---- batch: 020 ----
mean loss: 192.22
 ---- batch: 030 ----
mean loss: 201.66
 ---- batch: 040 ----
mean loss: 191.33
 ---- batch: 050 ----
mean loss: 186.22
train mean loss: 193.22
epoch train time: 0:00:07.671906
elapsed time: 0:30:40.012704
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 12:44:25.321581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.11
 ---- batch: 020 ----
mean loss: 190.09
 ---- batch: 030 ----
mean loss: 187.93
 ---- batch: 040 ----
mean loss: 195.61
 ---- batch: 050 ----
mean loss: 192.50
train mean loss: 193.63
epoch train time: 0:00:07.644092
elapsed time: 0:30:47.658244
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 12:44:32.966831
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.13
 ---- batch: 020 ----
mean loss: 188.97
 ---- batch: 030 ----
mean loss: 196.19
 ---- batch: 040 ----
mean loss: 191.62
 ---- batch: 050 ----
mean loss: 196.48
train mean loss: 192.74
epoch train time: 0:00:07.711980
elapsed time: 0:30:55.371202
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 12:44:40.680150
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.24
 ---- batch: 020 ----
mean loss: 196.01
 ---- batch: 030 ----
mean loss: 194.86
 ---- batch: 040 ----
mean loss: 190.96
 ---- batch: 050 ----
mean loss: 186.58
train mean loss: 192.96
epoch train time: 0:00:07.618515
elapsed time: 0:31:02.991016
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 12:44:48.300017
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.74
 ---- batch: 020 ----
mean loss: 189.04
 ---- batch: 030 ----
mean loss: 186.67
 ---- batch: 040 ----
mean loss: 198.74
 ---- batch: 050 ----
mean loss: 199.07
train mean loss: 192.95
epoch train time: 0:00:07.605719
elapsed time: 0:31:10.597909
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 12:44:55.906819
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.46
 ---- batch: 020 ----
mean loss: 198.85
 ---- batch: 030 ----
mean loss: 195.11
 ---- batch: 040 ----
mean loss: 196.48
 ---- batch: 050 ----
mean loss: 182.52
train mean loss: 193.10
epoch train time: 0:00:07.611272
elapsed time: 0:31:18.210278
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 12:45:03.519124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.26
 ---- batch: 020 ----
mean loss: 190.09
 ---- batch: 030 ----
mean loss: 199.48
 ---- batch: 040 ----
mean loss: 192.92
 ---- batch: 050 ----
mean loss: 193.59
train mean loss: 192.76
epoch train time: 0:00:07.616987
elapsed time: 0:31:25.828503
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 12:45:11.137366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.52
 ---- batch: 020 ----
mean loss: 195.01
 ---- batch: 030 ----
mean loss: 194.00
 ---- batch: 040 ----
mean loss: 187.65
 ---- batch: 050 ----
mean loss: 198.99
train mean loss: 192.92
epoch train time: 0:00:07.616601
elapsed time: 0:31:33.446119
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 12:45:18.754992
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.79
 ---- batch: 020 ----
mean loss: 185.45
 ---- batch: 030 ----
mean loss: 188.20
 ---- batch: 040 ----
mean loss: 197.20
 ---- batch: 050 ----
mean loss: 194.67
train mean loss: 193.30
epoch train time: 0:00:07.619352
elapsed time: 0:31:41.066490
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 12:45:26.375403
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.86
 ---- batch: 020 ----
mean loss: 196.21
 ---- batch: 030 ----
mean loss: 196.17
 ---- batch: 040 ----
mean loss: 205.63
 ---- batch: 050 ----
mean loss: 186.36
train mean loss: 193.55
epoch train time: 0:00:07.641769
elapsed time: 0:31:48.709483
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 12:45:34.018352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.70
 ---- batch: 020 ----
mean loss: 194.01
 ---- batch: 030 ----
mean loss: 188.60
 ---- batch: 040 ----
mean loss: 194.25
 ---- batch: 050 ----
mean loss: 192.89
train mean loss: 193.45
epoch train time: 0:00:07.640116
elapsed time: 0:31:56.350725
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 12:45:41.659622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.96
 ---- batch: 020 ----
mean loss: 197.38
 ---- batch: 030 ----
mean loss: 188.21
 ---- batch: 040 ----
mean loss: 189.33
 ---- batch: 050 ----
mean loss: 190.97
train mean loss: 192.79
epoch train time: 0:00:07.638579
elapsed time: 0:32:03.990358
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 12:45:49.299247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.43
 ---- batch: 020 ----
mean loss: 190.20
 ---- batch: 030 ----
mean loss: 187.92
 ---- batch: 040 ----
mean loss: 193.76
 ---- batch: 050 ----
mean loss: 202.04
train mean loss: 193.38
epoch train time: 0:00:07.642960
elapsed time: 0:32:11.634479
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 12:45:56.943369
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.65
 ---- batch: 020 ----
mean loss: 193.48
 ---- batch: 030 ----
mean loss: 190.44
 ---- batch: 040 ----
mean loss: 193.78
 ---- batch: 050 ----
mean loss: 197.23
train mean loss: 192.05
epoch train time: 0:00:07.626822
elapsed time: 0:32:19.262344
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 12:46:04.571241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.11
 ---- batch: 020 ----
mean loss: 196.24
 ---- batch: 030 ----
mean loss: 194.30
 ---- batch: 040 ----
mean loss: 193.16
 ---- batch: 050 ----
mean loss: 189.78
train mean loss: 192.11
epoch train time: 0:00:07.612018
elapsed time: 0:32:26.875524
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 12:46:12.184403
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.33
 ---- batch: 020 ----
mean loss: 186.61
 ---- batch: 030 ----
mean loss: 192.31
 ---- batch: 040 ----
mean loss: 195.11
 ---- batch: 050 ----
mean loss: 202.80
train mean loss: 193.13
epoch train time: 0:00:07.620481
elapsed time: 0:32:34.497101
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 12:46:19.805987
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.72
 ---- batch: 020 ----
mean loss: 194.13
 ---- batch: 030 ----
mean loss: 194.32
 ---- batch: 040 ----
mean loss: 194.63
 ---- batch: 050 ----
mean loss: 186.97
train mean loss: 192.49
epoch train time: 0:00:07.654251
elapsed time: 0:32:42.152481
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 12:46:27.461380
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.58
 ---- batch: 020 ----
mean loss: 190.61
 ---- batch: 030 ----
mean loss: 198.24
 ---- batch: 040 ----
mean loss: 188.39
 ---- batch: 050 ----
mean loss: 193.64
train mean loss: 192.46
epoch train time: 0:00:07.650685
elapsed time: 0:32:49.813219
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_3/checkpoint.pth.tar
**** end time: 2019-09-26 12:46:35.121760 ****
