Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_1', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17038
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 01:29:16.329010 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 11, 11]             560
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
            Conv2d-4            [-1, 14, 4, 11]             224
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
            Linear-8                    [-1, 1]             308
================================================================
Total params: 1,092
Trainable params: 1,092
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:29:16.334154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4936.03
 ---- batch: 020 ----
mean loss: 4787.85
 ---- batch: 030 ----
mean loss: 4641.79
 ---- batch: 040 ----
mean loss: 4464.74
 ---- batch: 050 ----
mean loss: 4296.45
 ---- batch: 060 ----
mean loss: 4071.61
 ---- batch: 070 ----
mean loss: 3903.50
 ---- batch: 080 ----
mean loss: 3688.23
 ---- batch: 090 ----
mean loss: 3469.73
 ---- batch: 100 ----
mean loss: 3279.23
 ---- batch: 110 ----
mean loss: 3077.86
train mean loss: 4025.80
epoch train time: 0:00:33.745999
elapsed time: 0:00:33.752426
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:29:50.081485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2779.22
 ---- batch: 020 ----
mean loss: 2560.89
 ---- batch: 030 ----
mean loss: 2393.38
 ---- batch: 040 ----
mean loss: 2219.14
 ---- batch: 050 ----
mean loss: 2078.23
 ---- batch: 060 ----
mean loss: 1919.86
 ---- batch: 070 ----
mean loss: 1783.11
 ---- batch: 080 ----
mean loss: 1678.93
 ---- batch: 090 ----
mean loss: 1568.04
 ---- batch: 100 ----
mean loss: 1463.16
 ---- batch: 110 ----
mean loss: 1375.98
train mean loss: 1966.98
epoch train time: 0:00:00.710466
elapsed time: 0:00:34.463034
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:29:50.792095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1289.38
 ---- batch: 020 ----
mean loss: 1238.50
 ---- batch: 030 ----
mean loss: 1185.00
 ---- batch: 040 ----
mean loss: 1138.69
 ---- batch: 050 ----
mean loss: 1082.91
 ---- batch: 060 ----
mean loss: 1039.51
 ---- batch: 070 ----
mean loss: 1037.03
 ---- batch: 080 ----
mean loss: 996.64
 ---- batch: 090 ----
mean loss: 974.02
 ---- batch: 100 ----
mean loss: 952.41
 ---- batch: 110 ----
mean loss: 937.82
train mean loss: 1074.92
epoch train time: 0:00:00.721625
elapsed time: 0:00:35.184803
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:29:51.513865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.08
 ---- batch: 020 ----
mean loss: 910.63
 ---- batch: 030 ----
mean loss: 908.20
 ---- batch: 040 ----
mean loss: 884.07
 ---- batch: 050 ----
mean loss: 867.44
 ---- batch: 060 ----
mean loss: 869.62
 ---- batch: 070 ----
mean loss: 871.32
 ---- batch: 080 ----
mean loss: 844.69
 ---- batch: 090 ----
mean loss: 869.38
 ---- batch: 100 ----
mean loss: 871.71
 ---- batch: 110 ----
mean loss: 851.50
train mean loss: 878.90
epoch train time: 0:00:00.708809
elapsed time: 0:00:35.893786
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:29:52.222847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.59
 ---- batch: 020 ----
mean loss: 847.54
 ---- batch: 030 ----
mean loss: 854.16
 ---- batch: 040 ----
mean loss: 860.34
 ---- batch: 050 ----
mean loss: 853.69
 ---- batch: 060 ----
mean loss: 839.41
 ---- batch: 070 ----
mean loss: 842.91
 ---- batch: 080 ----
mean loss: 839.94
 ---- batch: 090 ----
mean loss: 840.92
 ---- batch: 100 ----
mean loss: 856.02
 ---- batch: 110 ----
mean loss: 854.43
train mean loss: 848.70
epoch train time: 0:00:00.714483
elapsed time: 0:00:36.608448
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:29:52.937508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.65
 ---- batch: 020 ----
mean loss: 843.38
 ---- batch: 030 ----
mean loss: 848.42
 ---- batch: 040 ----
mean loss: 822.90
 ---- batch: 050 ----
mean loss: 837.34
 ---- batch: 060 ----
mean loss: 858.62
 ---- batch: 070 ----
mean loss: 836.64
 ---- batch: 080 ----
mean loss: 857.75
 ---- batch: 090 ----
mean loss: 837.45
 ---- batch: 100 ----
mean loss: 844.93
 ---- batch: 110 ----
mean loss: 843.03
train mean loss: 842.62
epoch train time: 0:00:00.714450
elapsed time: 0:00:37.323068
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:29:53.652128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.89
 ---- batch: 020 ----
mean loss: 825.60
 ---- batch: 030 ----
mean loss: 828.83
 ---- batch: 040 ----
mean loss: 838.64
 ---- batch: 050 ----
mean loss: 811.35
 ---- batch: 060 ----
mean loss: 839.82
 ---- batch: 070 ----
mean loss: 830.25
 ---- batch: 080 ----
mean loss: 853.75
 ---- batch: 090 ----
mean loss: 849.23
 ---- batch: 100 ----
mean loss: 853.27
 ---- batch: 110 ----
mean loss: 840.43
train mean loss: 838.34
epoch train time: 0:00:00.714896
elapsed time: 0:00:38.038104
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:29:54.367163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.32
 ---- batch: 020 ----
mean loss: 820.34
 ---- batch: 030 ----
mean loss: 855.09
 ---- batch: 040 ----
mean loss: 836.10
 ---- batch: 050 ----
mean loss: 836.51
 ---- batch: 060 ----
mean loss: 841.82
 ---- batch: 070 ----
mean loss: 812.38
 ---- batch: 080 ----
mean loss: 831.97
 ---- batch: 090 ----
mean loss: 835.17
 ---- batch: 100 ----
mean loss: 842.43
 ---- batch: 110 ----
mean loss: 837.35
train mean loss: 833.86
epoch train time: 0:00:00.716929
elapsed time: 0:00:38.755172
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:29:55.084264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.73
 ---- batch: 020 ----
mean loss: 835.80
 ---- batch: 030 ----
mean loss: 816.91
 ---- batch: 040 ----
mean loss: 816.40
 ---- batch: 050 ----
mean loss: 833.38
 ---- batch: 060 ----
mean loss: 816.45
 ---- batch: 070 ----
mean loss: 838.79
 ---- batch: 080 ----
mean loss: 823.57
 ---- batch: 090 ----
mean loss: 822.40
 ---- batch: 100 ----
mean loss: 846.99
 ---- batch: 110 ----
mean loss: 835.44
train mean loss: 829.26
epoch train time: 0:00:00.715752
elapsed time: 0:00:39.471097
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:29:55.800156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.10
 ---- batch: 020 ----
mean loss: 834.28
 ---- batch: 030 ----
mean loss: 805.99
 ---- batch: 040 ----
mean loss: 838.72
 ---- batch: 050 ----
mean loss: 819.40
 ---- batch: 060 ----
mean loss: 826.16
 ---- batch: 070 ----
mean loss: 819.87
 ---- batch: 080 ----
mean loss: 854.22
 ---- batch: 090 ----
mean loss: 815.17
 ---- batch: 100 ----
mean loss: 809.13
 ---- batch: 110 ----
mean loss: 830.76
train mean loss: 824.37
epoch train time: 0:00:00.711595
elapsed time: 0:00:40.182830
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:29:56.511889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.65
 ---- batch: 020 ----
mean loss: 805.63
 ---- batch: 030 ----
mean loss: 821.35
 ---- batch: 040 ----
mean loss: 838.17
 ---- batch: 050 ----
mean loss: 812.45
 ---- batch: 060 ----
mean loss: 816.69
 ---- batch: 070 ----
mean loss: 821.69
 ---- batch: 080 ----
mean loss: 814.22
 ---- batch: 090 ----
mean loss: 829.60
 ---- batch: 100 ----
mean loss: 813.66
 ---- batch: 110 ----
mean loss: 816.25
train mean loss: 819.65
epoch train time: 0:00:00.710741
elapsed time: 0:00:40.893710
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:29:57.222770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.21
 ---- batch: 020 ----
mean loss: 797.00
 ---- batch: 030 ----
mean loss: 831.84
 ---- batch: 040 ----
mean loss: 824.91
 ---- batch: 050 ----
mean loss: 815.59
 ---- batch: 060 ----
mean loss: 808.50
 ---- batch: 070 ----
mean loss: 812.86
 ---- batch: 080 ----
mean loss: 805.92
 ---- batch: 090 ----
mean loss: 821.07
 ---- batch: 100 ----
mean loss: 810.47
 ---- batch: 110 ----
mean loss: 783.41
train mean loss: 814.66
epoch train time: 0:00:00.715014
elapsed time: 0:00:41.608875
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:29:57.937935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.30
 ---- batch: 020 ----
mean loss: 818.80
 ---- batch: 030 ----
mean loss: 816.19
 ---- batch: 040 ----
mean loss: 804.79
 ---- batch: 050 ----
mean loss: 807.81
 ---- batch: 060 ----
mean loss: 803.19
 ---- batch: 070 ----
mean loss: 819.26
 ---- batch: 080 ----
mean loss: 784.80
 ---- batch: 090 ----
mean loss: 805.81
 ---- batch: 100 ----
mean loss: 818.64
 ---- batch: 110 ----
mean loss: 791.05
train mean loss: 809.43
epoch train time: 0:00:00.728626
elapsed time: 0:00:42.337652
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:29:58.666723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 793.28
 ---- batch: 020 ----
mean loss: 807.92
 ---- batch: 030 ----
mean loss: 799.46
 ---- batch: 040 ----
mean loss: 788.65
 ---- batch: 050 ----
mean loss: 794.89
 ---- batch: 060 ----
mean loss: 809.87
 ---- batch: 070 ----
mean loss: 807.29
 ---- batch: 080 ----
mean loss: 809.77
 ---- batch: 090 ----
mean loss: 795.11
 ---- batch: 100 ----
mean loss: 811.55
 ---- batch: 110 ----
mean loss: 813.75
train mean loss: 803.60
epoch train time: 0:00:00.716397
elapsed time: 0:00:43.054212
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:29:59.383298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 802.88
 ---- batch: 020 ----
mean loss: 793.29
 ---- batch: 030 ----
mean loss: 809.26
 ---- batch: 040 ----
mean loss: 810.05
 ---- batch: 050 ----
mean loss: 801.82
 ---- batch: 060 ----
mean loss: 790.15
 ---- batch: 070 ----
mean loss: 794.40
 ---- batch: 080 ----
mean loss: 785.48
 ---- batch: 090 ----
mean loss: 794.42
 ---- batch: 100 ----
mean loss: 794.00
 ---- batch: 110 ----
mean loss: 813.47
train mean loss: 798.01
epoch train time: 0:00:00.716340
elapsed time: 0:00:43.770716
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:30:00.099774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 798.76
 ---- batch: 020 ----
mean loss: 801.13
 ---- batch: 030 ----
mean loss: 790.52
 ---- batch: 040 ----
mean loss: 778.40
 ---- batch: 050 ----
mean loss: 790.70
 ---- batch: 060 ----
mean loss: 801.13
 ---- batch: 070 ----
mean loss: 803.73
 ---- batch: 080 ----
mean loss: 780.21
 ---- batch: 090 ----
mean loss: 783.13
 ---- batch: 100 ----
mean loss: 803.76
 ---- batch: 110 ----
mean loss: 779.94
train mean loss: 792.86
epoch train time: 0:00:00.711014
elapsed time: 0:00:44.481908
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:30:00.810982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 784.33
 ---- batch: 020 ----
mean loss: 759.30
 ---- batch: 030 ----
mean loss: 785.80
 ---- batch: 040 ----
mean loss: 808.53
 ---- batch: 050 ----
mean loss: 806.56
 ---- batch: 060 ----
mean loss: 802.80
 ---- batch: 070 ----
mean loss: 799.26
 ---- batch: 080 ----
mean loss: 786.66
 ---- batch: 090 ----
mean loss: 772.64
 ---- batch: 100 ----
mean loss: 780.88
 ---- batch: 110 ----
mean loss: 778.16
train mean loss: 787.75
epoch train time: 0:00:00.719455
elapsed time: 0:00:45.201520
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:30:01.530578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 764.95
 ---- batch: 020 ----
mean loss: 789.92
 ---- batch: 030 ----
mean loss: 770.55
 ---- batch: 040 ----
mean loss: 788.40
 ---- batch: 050 ----
mean loss: 798.95
 ---- batch: 060 ----
mean loss: 765.78
 ---- batch: 070 ----
mean loss: 793.44
 ---- batch: 080 ----
mean loss: 777.24
 ---- batch: 090 ----
mean loss: 777.38
 ---- batch: 100 ----
mean loss: 794.43
 ---- batch: 110 ----
mean loss: 790.27
train mean loss: 782.38
epoch train time: 0:00:00.707070
elapsed time: 0:00:45.908728
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:30:02.237786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 779.21
 ---- batch: 020 ----
mean loss: 790.82
 ---- batch: 030 ----
mean loss: 778.03
 ---- batch: 040 ----
mean loss: 758.89
 ---- batch: 050 ----
mean loss: 767.88
 ---- batch: 060 ----
mean loss: 783.22
 ---- batch: 070 ----
mean loss: 775.87
 ---- batch: 080 ----
mean loss: 776.02
 ---- batch: 090 ----
mean loss: 781.72
 ---- batch: 100 ----
mean loss: 771.40
 ---- batch: 110 ----
mean loss: 793.90
train mean loss: 777.00
epoch train time: 0:00:00.711290
elapsed time: 0:00:46.620154
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:30:02.949212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 772.51
 ---- batch: 020 ----
mean loss: 780.32
 ---- batch: 030 ----
mean loss: 783.71
 ---- batch: 040 ----
mean loss: 766.73
 ---- batch: 050 ----
mean loss: 758.77
 ---- batch: 060 ----
mean loss: 779.27
 ---- batch: 070 ----
mean loss: 773.05
 ---- batch: 080 ----
mean loss: 775.32
 ---- batch: 090 ----
mean loss: 765.26
 ---- batch: 100 ----
mean loss: 771.15
 ---- batch: 110 ----
mean loss: 773.17
train mean loss: 771.68
epoch train time: 0:00:00.701911
elapsed time: 0:00:47.322199
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:30:03.651255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 735.77
 ---- batch: 020 ----
mean loss: 803.62
 ---- batch: 030 ----
mean loss: 769.03
 ---- batch: 040 ----
mean loss: 789.48
 ---- batch: 050 ----
mean loss: 772.56
 ---- batch: 060 ----
mean loss: 773.94
 ---- batch: 070 ----
mean loss: 763.47
 ---- batch: 080 ----
mean loss: 773.23
 ---- batch: 090 ----
mean loss: 754.42
 ---- batch: 100 ----
mean loss: 750.75
 ---- batch: 110 ----
mean loss: 746.07
train mean loss: 766.34
epoch train time: 0:00:00.721439
elapsed time: 0:00:48.043773
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:30:04.372847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.50
 ---- batch: 020 ----
mean loss: 776.78
 ---- batch: 030 ----
mean loss: 752.51
 ---- batch: 040 ----
mean loss: 777.88
 ---- batch: 050 ----
mean loss: 764.79
 ---- batch: 060 ----
mean loss: 757.17
 ---- batch: 070 ----
mean loss: 769.32
 ---- batch: 080 ----
mean loss: 755.83
 ---- batch: 090 ----
mean loss: 751.91
 ---- batch: 100 ----
mean loss: 749.71
 ---- batch: 110 ----
mean loss: 764.55
train mean loss: 760.93
epoch train time: 0:00:00.714462
elapsed time: 0:00:48.758410
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:30:05.087470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.22
 ---- batch: 020 ----
mean loss: 743.89
 ---- batch: 030 ----
mean loss: 736.18
 ---- batch: 040 ----
mean loss: 760.60
 ---- batch: 050 ----
mean loss: 774.82
 ---- batch: 060 ----
mean loss: 751.64
 ---- batch: 070 ----
mean loss: 775.00
 ---- batch: 080 ----
mean loss: 744.98
 ---- batch: 090 ----
mean loss: 749.67
 ---- batch: 100 ----
mean loss: 776.93
 ---- batch: 110 ----
mean loss: 747.57
train mean loss: 755.36
epoch train time: 0:00:00.713888
elapsed time: 0:00:49.472471
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:30:05.801532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 734.22
 ---- batch: 020 ----
mean loss: 751.29
 ---- batch: 030 ----
mean loss: 765.23
 ---- batch: 040 ----
mean loss: 746.54
 ---- batch: 050 ----
mean loss: 764.22
 ---- batch: 060 ----
mean loss: 743.50
 ---- batch: 070 ----
mean loss: 745.75
 ---- batch: 080 ----
mean loss: 759.13
 ---- batch: 090 ----
mean loss: 744.97
 ---- batch: 100 ----
mean loss: 754.75
 ---- batch: 110 ----
mean loss: 734.39
train mean loss: 749.59
epoch train time: 0:00:00.711902
elapsed time: 0:00:50.184516
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:30:06.513575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.89
 ---- batch: 020 ----
mean loss: 745.30
 ---- batch: 030 ----
mean loss: 732.14
 ---- batch: 040 ----
mean loss: 751.27
 ---- batch: 050 ----
mean loss: 745.15
 ---- batch: 060 ----
mean loss: 753.44
 ---- batch: 070 ----
mean loss: 726.28
 ---- batch: 080 ----
mean loss: 745.54
 ---- batch: 090 ----
mean loss: 752.24
 ---- batch: 100 ----
mean loss: 731.28
 ---- batch: 110 ----
mean loss: 751.38
train mean loss: 743.74
epoch train time: 0:00:00.714603
elapsed time: 0:00:50.899290
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:30:07.228361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.03
 ---- batch: 020 ----
mean loss: 739.37
 ---- batch: 030 ----
mean loss: 732.06
 ---- batch: 040 ----
mean loss: 738.91
 ---- batch: 050 ----
mean loss: 737.89
 ---- batch: 060 ----
mean loss: 745.29
 ---- batch: 070 ----
mean loss: 750.87
 ---- batch: 080 ----
mean loss: 723.69
 ---- batch: 090 ----
mean loss: 745.39
 ---- batch: 100 ----
mean loss: 731.75
 ---- batch: 110 ----
mean loss: 733.61
train mean loss: 737.65
epoch train time: 0:00:00.713069
elapsed time: 0:00:51.612512
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:30:07.941572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.12
 ---- batch: 020 ----
mean loss: 738.49
 ---- batch: 030 ----
mean loss: 744.34
 ---- batch: 040 ----
mean loss: 737.59
 ---- batch: 050 ----
mean loss: 727.21
 ---- batch: 060 ----
mean loss: 716.93
 ---- batch: 070 ----
mean loss: 714.87
 ---- batch: 080 ----
mean loss: 749.09
 ---- batch: 090 ----
mean loss: 743.60
 ---- batch: 100 ----
mean loss: 718.42
 ---- batch: 110 ----
mean loss: 723.33
train mean loss: 731.53
epoch train time: 0:00:00.705302
elapsed time: 0:00:52.318034
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:30:08.647095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 726.14
 ---- batch: 020 ----
mean loss: 717.18
 ---- batch: 030 ----
mean loss: 742.61
 ---- batch: 040 ----
mean loss: 743.01
 ---- batch: 050 ----
mean loss: 722.26
 ---- batch: 060 ----
mean loss: 722.59
 ---- batch: 070 ----
mean loss: 719.25
 ---- batch: 080 ----
mean loss: 721.89
 ---- batch: 090 ----
mean loss: 730.45
 ---- batch: 100 ----
mean loss: 715.20
 ---- batch: 110 ----
mean loss: 723.68
train mean loss: 725.00
epoch train time: 0:00:00.705361
elapsed time: 0:00:53.023547
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:30:09.352653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 687.18
 ---- batch: 020 ----
mean loss: 718.12
 ---- batch: 030 ----
mean loss: 729.44
 ---- batch: 040 ----
mean loss: 738.97
 ---- batch: 050 ----
mean loss: 738.60
 ---- batch: 060 ----
mean loss: 710.15
 ---- batch: 070 ----
mean loss: 719.03
 ---- batch: 080 ----
mean loss: 718.70
 ---- batch: 090 ----
mean loss: 703.37
 ---- batch: 100 ----
mean loss: 722.07
 ---- batch: 110 ----
mean loss: 713.78
train mean loss: 718.45
epoch train time: 0:00:00.714907
elapsed time: 0:00:53.738639
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:30:10.067697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.59
 ---- batch: 020 ----
mean loss: 697.78
 ---- batch: 030 ----
mean loss: 708.06
 ---- batch: 040 ----
mean loss: 707.96
 ---- batch: 050 ----
mean loss: 720.19
 ---- batch: 060 ----
mean loss: 709.02
 ---- batch: 070 ----
mean loss: 710.67
 ---- batch: 080 ----
mean loss: 720.93
 ---- batch: 090 ----
mean loss: 711.50
 ---- batch: 100 ----
mean loss: 717.38
 ---- batch: 110 ----
mean loss: 710.23
train mean loss: 711.55
epoch train time: 0:00:00.719439
elapsed time: 0:00:54.458217
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:30:10.787278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.19
 ---- batch: 020 ----
mean loss: 700.07
 ---- batch: 030 ----
mean loss: 710.53
 ---- batch: 040 ----
mean loss: 711.12
 ---- batch: 050 ----
mean loss: 706.93
 ---- batch: 060 ----
mean loss: 705.84
 ---- batch: 070 ----
mean loss: 672.44
 ---- batch: 080 ----
mean loss: 713.82
 ---- batch: 090 ----
mean loss: 705.29
 ---- batch: 100 ----
mean loss: 706.91
 ---- batch: 110 ----
mean loss: 709.30
train mean loss: 704.49
epoch train time: 0:00:00.723837
elapsed time: 0:00:55.182200
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:30:11.511262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 704.61
 ---- batch: 020 ----
mean loss: 686.34
 ---- batch: 030 ----
mean loss: 712.94
 ---- batch: 040 ----
mean loss: 717.34
 ---- batch: 050 ----
mean loss: 680.43
 ---- batch: 060 ----
mean loss: 700.47
 ---- batch: 070 ----
mean loss: 698.74
 ---- batch: 080 ----
mean loss: 697.60
 ---- batch: 090 ----
mean loss: 692.69
 ---- batch: 100 ----
mean loss: 696.06
 ---- batch: 110 ----
mean loss: 683.77
train mean loss: 697.23
epoch train time: 0:00:00.717293
elapsed time: 0:00:55.899636
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:30:12.228690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.01
 ---- batch: 020 ----
mean loss: 683.81
 ---- batch: 030 ----
mean loss: 701.55
 ---- batch: 040 ----
mean loss: 703.70
 ---- batch: 050 ----
mean loss: 693.05
 ---- batch: 060 ----
mean loss: 698.69
 ---- batch: 070 ----
mean loss: 665.69
 ---- batch: 080 ----
mean loss: 697.35
 ---- batch: 090 ----
mean loss: 685.88
 ---- batch: 100 ----
mean loss: 694.56
 ---- batch: 110 ----
mean loss: 679.20
train mean loss: 689.35
epoch train time: 0:00:00.707924
elapsed time: 0:00:56.607692
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:30:12.936751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 678.63
 ---- batch: 020 ----
mean loss: 686.00
 ---- batch: 030 ----
mean loss: 676.63
 ---- batch: 040 ----
mean loss: 682.87
 ---- batch: 050 ----
mean loss: 667.39
 ---- batch: 060 ----
mean loss: 674.58
 ---- batch: 070 ----
mean loss: 689.02
 ---- batch: 080 ----
mean loss: 689.47
 ---- batch: 090 ----
mean loss: 688.11
 ---- batch: 100 ----
mean loss: 686.42
 ---- batch: 110 ----
mean loss: 683.03
train mean loss: 681.41
epoch train time: 0:00:00.706996
elapsed time: 0:00:57.314826
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:30:13.643905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.64
 ---- batch: 020 ----
mean loss: 649.93
 ---- batch: 030 ----
mean loss: 670.38
 ---- batch: 040 ----
mean loss: 687.24
 ---- batch: 050 ----
mean loss: 674.41
 ---- batch: 060 ----
mean loss: 682.33
 ---- batch: 070 ----
mean loss: 657.53
 ---- batch: 080 ----
mean loss: 680.66
 ---- batch: 090 ----
mean loss: 680.31
 ---- batch: 100 ----
mean loss: 679.71
 ---- batch: 110 ----
mean loss: 675.21
train mean loss: 673.16
epoch train time: 0:00:00.720992
elapsed time: 0:00:58.036005
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:30:14.365083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.97
 ---- batch: 020 ----
mean loss: 678.25
 ---- batch: 030 ----
mean loss: 663.37
 ---- batch: 040 ----
mean loss: 663.77
 ---- batch: 050 ----
mean loss: 664.74
 ---- batch: 060 ----
mean loss: 658.08
 ---- batch: 070 ----
mean loss: 660.74
 ---- batch: 080 ----
mean loss: 663.31
 ---- batch: 090 ----
mean loss: 649.13
 ---- batch: 100 ----
mean loss: 662.49
 ---- batch: 110 ----
mean loss: 654.18
train mean loss: 664.63
epoch train time: 0:00:00.715641
elapsed time: 0:00:58.751842
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:30:15.080900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.30
 ---- batch: 020 ----
mean loss: 656.96
 ---- batch: 030 ----
mean loss: 664.37
 ---- batch: 040 ----
mean loss: 657.35
 ---- batch: 050 ----
mean loss: 648.18
 ---- batch: 060 ----
mean loss: 663.99
 ---- batch: 070 ----
mean loss: 647.67
 ---- batch: 080 ----
mean loss: 644.58
 ---- batch: 090 ----
mean loss: 663.99
 ---- batch: 100 ----
mean loss: 657.95
 ---- batch: 110 ----
mean loss: 637.61
train mean loss: 655.66
epoch train time: 0:00:00.721058
elapsed time: 0:00:59.473072
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:30:15.802152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 653.02
 ---- batch: 020 ----
mean loss: 655.88
 ---- batch: 030 ----
mean loss: 651.16
 ---- batch: 040 ----
mean loss: 647.71
 ---- batch: 050 ----
mean loss: 664.36
 ---- batch: 060 ----
mean loss: 633.73
 ---- batch: 070 ----
mean loss: 638.86
 ---- batch: 080 ----
mean loss: 637.07
 ---- batch: 090 ----
mean loss: 631.62
 ---- batch: 100 ----
mean loss: 645.33
 ---- batch: 110 ----
mean loss: 655.75
train mean loss: 646.48
epoch train time: 0:00:00.707942
elapsed time: 0:01:00.181175
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:30:16.510235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 647.72
 ---- batch: 020 ----
mean loss: 642.75
 ---- batch: 030 ----
mean loss: 645.71
 ---- batch: 040 ----
mean loss: 641.41
 ---- batch: 050 ----
mean loss: 618.88
 ---- batch: 060 ----
mean loss: 630.91
 ---- batch: 070 ----
mean loss: 634.07
 ---- batch: 080 ----
mean loss: 646.08
 ---- batch: 090 ----
mean loss: 628.58
 ---- batch: 100 ----
mean loss: 629.17
 ---- batch: 110 ----
mean loss: 636.97
train mean loss: 636.87
epoch train time: 0:00:00.736545
elapsed time: 0:01:00.917959
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:30:17.247021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 630.63
 ---- batch: 020 ----
mean loss: 631.79
 ---- batch: 030 ----
mean loss: 619.14
 ---- batch: 040 ----
mean loss: 628.88
 ---- batch: 050 ----
mean loss: 628.85
 ---- batch: 060 ----
mean loss: 620.91
 ---- batch: 070 ----
mean loss: 625.03
 ---- batch: 080 ----
mean loss: 635.17
 ---- batch: 090 ----
mean loss: 624.65
 ---- batch: 100 ----
mean loss: 630.76
 ---- batch: 110 ----
mean loss: 627.49
train mean loss: 626.71
epoch train time: 0:00:00.723793
elapsed time: 0:01:01.641891
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:30:17.970950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 614.05
 ---- batch: 020 ----
mean loss: 629.59
 ---- batch: 030 ----
mean loss: 608.17
 ---- batch: 040 ----
mean loss: 626.26
 ---- batch: 050 ----
mean loss: 622.56
 ---- batch: 060 ----
mean loss: 618.10
 ---- batch: 070 ----
mean loss: 615.36
 ---- batch: 080 ----
mean loss: 618.80
 ---- batch: 090 ----
mean loss: 602.85
 ---- batch: 100 ----
mean loss: 612.31
 ---- batch: 110 ----
mean loss: 615.32
train mean loss: 616.31
epoch train time: 0:00:00.702516
elapsed time: 0:01:02.344544
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:30:18.673639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 616.18
 ---- batch: 020 ----
mean loss: 599.98
 ---- batch: 030 ----
mean loss: 610.27
 ---- batch: 040 ----
mean loss: 609.18
 ---- batch: 050 ----
mean loss: 598.70
 ---- batch: 060 ----
mean loss: 598.73
 ---- batch: 070 ----
mean loss: 617.28
 ---- batch: 080 ----
mean loss: 619.40
 ---- batch: 090 ----
mean loss: 607.17
 ---- batch: 100 ----
mean loss: 601.55
 ---- batch: 110 ----
mean loss: 590.34
train mean loss: 605.41
epoch train time: 0:00:00.716640
elapsed time: 0:01:03.061363
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:30:19.390432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 599.30
 ---- batch: 020 ----
mean loss: 593.06
 ---- batch: 030 ----
mean loss: 598.62
 ---- batch: 040 ----
mean loss: 600.96
 ---- batch: 050 ----
mean loss: 601.06
 ---- batch: 060 ----
mean loss: 595.80
 ---- batch: 070 ----
mean loss: 595.89
 ---- batch: 080 ----
mean loss: 592.81
 ---- batch: 090 ----
mean loss: 591.06
 ---- batch: 100 ----
mean loss: 578.64
 ---- batch: 110 ----
mean loss: 590.45
train mean loss: 594.35
epoch train time: 0:00:00.708372
elapsed time: 0:01:03.769881
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:30:20.098946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 582.75
 ---- batch: 020 ----
mean loss: 580.25
 ---- batch: 030 ----
mean loss: 596.53
 ---- batch: 040 ----
mean loss: 586.79
 ---- batch: 050 ----
mean loss: 584.74
 ---- batch: 060 ----
mean loss: 583.07
 ---- batch: 070 ----
mean loss: 568.85
 ---- batch: 080 ----
mean loss: 589.26
 ---- batch: 090 ----
mean loss: 574.51
 ---- batch: 100 ----
mean loss: 583.66
 ---- batch: 110 ----
mean loss: 576.54
train mean loss: 582.82
epoch train time: 0:00:00.713113
elapsed time: 0:01:04.483137
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:30:20.812211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.24
 ---- batch: 020 ----
mean loss: 573.16
 ---- batch: 030 ----
mean loss: 568.14
 ---- batch: 040 ----
mean loss: 578.98
 ---- batch: 050 ----
mean loss: 572.04
 ---- batch: 060 ----
mean loss: 597.90
 ---- batch: 070 ----
mean loss: 575.61
 ---- batch: 080 ----
mean loss: 562.58
 ---- batch: 090 ----
mean loss: 564.52
 ---- batch: 100 ----
mean loss: 552.93
 ---- batch: 110 ----
mean loss: 565.00
train mean loss: 571.05
epoch train time: 0:00:00.716503
elapsed time: 0:01:05.199794
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:30:21.528858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.37
 ---- batch: 020 ----
mean loss: 581.82
 ---- batch: 030 ----
mean loss: 569.62
 ---- batch: 040 ----
mean loss: 567.44
 ---- batch: 050 ----
mean loss: 552.77
 ---- batch: 060 ----
mean loss: 554.83
 ---- batch: 070 ----
mean loss: 553.16
 ---- batch: 080 ----
mean loss: 558.26
 ---- batch: 090 ----
mean loss: 547.60
 ---- batch: 100 ----
mean loss: 559.85
 ---- batch: 110 ----
mean loss: 557.57
train mean loss: 558.95
epoch train time: 0:00:00.717883
elapsed time: 0:01:05.917826
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:30:22.246888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.08
 ---- batch: 020 ----
mean loss: 552.13
 ---- batch: 030 ----
mean loss: 554.84
 ---- batch: 040 ----
mean loss: 546.09
 ---- batch: 050 ----
mean loss: 547.67
 ---- batch: 060 ----
mean loss: 531.95
 ---- batch: 070 ----
mean loss: 560.07
 ---- batch: 080 ----
mean loss: 542.51
 ---- batch: 090 ----
mean loss: 546.27
 ---- batch: 100 ----
mean loss: 536.29
 ---- batch: 110 ----
mean loss: 548.50
train mean loss: 546.96
epoch train time: 0:00:00.712659
elapsed time: 0:01:06.630630
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:30:22.959714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 537.13
 ---- batch: 020 ----
mean loss: 545.48
 ---- batch: 030 ----
mean loss: 532.59
 ---- batch: 040 ----
mean loss: 524.95
 ---- batch: 050 ----
mean loss: 543.61
 ---- batch: 060 ----
mean loss: 531.32
 ---- batch: 070 ----
mean loss: 548.13
 ---- batch: 080 ----
mean loss: 531.78
 ---- batch: 090 ----
mean loss: 541.24
 ---- batch: 100 ----
mean loss: 515.08
 ---- batch: 110 ----
mean loss: 530.06
train mean loss: 534.59
epoch train time: 0:00:00.710501
elapsed time: 0:01:07.341297
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:30:23.670358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 528.88
 ---- batch: 020 ----
mean loss: 525.87
 ---- batch: 030 ----
mean loss: 527.83
 ---- batch: 040 ----
mean loss: 515.01
 ---- batch: 050 ----
mean loss: 515.66
 ---- batch: 060 ----
mean loss: 530.76
 ---- batch: 070 ----
mean loss: 532.60
 ---- batch: 080 ----
mean loss: 522.44
 ---- batch: 090 ----
mean loss: 516.75
 ---- batch: 100 ----
mean loss: 518.97
 ---- batch: 110 ----
mean loss: 509.67
train mean loss: 522.52
epoch train time: 0:00:00.720315
elapsed time: 0:01:08.061757
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:30:24.390817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.40
 ---- batch: 020 ----
mean loss: 521.34
 ---- batch: 030 ----
mean loss: 515.06
 ---- batch: 040 ----
mean loss: 513.24
 ---- batch: 050 ----
mean loss: 503.65
 ---- batch: 060 ----
mean loss: 508.15
 ---- batch: 070 ----
mean loss: 503.28
 ---- batch: 080 ----
mean loss: 511.49
 ---- batch: 090 ----
mean loss: 513.90
 ---- batch: 100 ----
mean loss: 501.17
 ---- batch: 110 ----
mean loss: 499.89
train mean loss: 510.27
epoch train time: 0:00:00.727227
elapsed time: 0:01:08.789126
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:30:25.118188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.91
 ---- batch: 020 ----
mean loss: 502.63
 ---- batch: 030 ----
mean loss: 500.93
 ---- batch: 040 ----
mean loss: 503.77
 ---- batch: 050 ----
mean loss: 500.25
 ---- batch: 060 ----
mean loss: 497.94
 ---- batch: 070 ----
mean loss: 504.36
 ---- batch: 080 ----
mean loss: 497.66
 ---- batch: 090 ----
mean loss: 490.51
 ---- batch: 100 ----
mean loss: 487.09
 ---- batch: 110 ----
mean loss: 491.48
train mean loss: 498.37
epoch train time: 0:00:00.721159
elapsed time: 0:01:09.510427
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:30:25.839487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.05
 ---- batch: 020 ----
mean loss: 497.60
 ---- batch: 030 ----
mean loss: 504.12
 ---- batch: 040 ----
mean loss: 483.52
 ---- batch: 050 ----
mean loss: 479.42
 ---- batch: 060 ----
mean loss: 490.67
 ---- batch: 070 ----
mean loss: 483.61
 ---- batch: 080 ----
mean loss: 475.34
 ---- batch: 090 ----
mean loss: 477.19
 ---- batch: 100 ----
mean loss: 487.49
 ---- batch: 110 ----
mean loss: 483.89
train mean loss: 486.47
epoch train time: 0:00:00.714583
elapsed time: 0:01:10.225146
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:30:26.554203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.56
 ---- batch: 020 ----
mean loss: 487.32
 ---- batch: 030 ----
mean loss: 473.30
 ---- batch: 040 ----
mean loss: 468.58
 ---- batch: 050 ----
mean loss: 463.11
 ---- batch: 060 ----
mean loss: 479.39
 ---- batch: 070 ----
mean loss: 473.62
 ---- batch: 080 ----
mean loss: 475.36
 ---- batch: 090 ----
mean loss: 471.61
 ---- batch: 100 ----
mean loss: 465.20
 ---- batch: 110 ----
mean loss: 479.29
train mean loss: 474.78
epoch train time: 0:00:00.719238
elapsed time: 0:01:10.944520
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:30:27.273581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.60
 ---- batch: 020 ----
mean loss: 471.38
 ---- batch: 030 ----
mean loss: 471.14
 ---- batch: 040 ----
mean loss: 461.24
 ---- batch: 050 ----
mean loss: 453.58
 ---- batch: 060 ----
mean loss: 466.57
 ---- batch: 070 ----
mean loss: 466.24
 ---- batch: 080 ----
mean loss: 465.20
 ---- batch: 090 ----
mean loss: 449.17
 ---- batch: 100 ----
mean loss: 463.96
 ---- batch: 110 ----
mean loss: 456.91
train mean loss: 463.27
epoch train time: 0:00:00.713514
elapsed time: 0:01:11.658233
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:30:27.987343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.78
 ---- batch: 020 ----
mean loss: 456.06
 ---- batch: 030 ----
mean loss: 461.75
 ---- batch: 040 ----
mean loss: 457.99
 ---- batch: 050 ----
mean loss: 459.74
 ---- batch: 060 ----
mean loss: 434.92
 ---- batch: 070 ----
mean loss: 443.71
 ---- batch: 080 ----
mean loss: 444.89
 ---- batch: 090 ----
mean loss: 440.41
 ---- batch: 100 ----
mean loss: 459.75
 ---- batch: 110 ----
mean loss: 449.73
train mean loss: 451.62
epoch train time: 0:00:00.732337
elapsed time: 0:01:12.390765
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:30:28.719825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.00
 ---- batch: 020 ----
mean loss: 444.56
 ---- batch: 030 ----
mean loss: 455.75
 ---- batch: 040 ----
mean loss: 437.64
 ---- batch: 050 ----
mean loss: 436.97
 ---- batch: 060 ----
mean loss: 430.26
 ---- batch: 070 ----
mean loss: 432.22
 ---- batch: 080 ----
mean loss: 435.33
 ---- batch: 090 ----
mean loss: 442.43
 ---- batch: 100 ----
mean loss: 428.24
 ---- batch: 110 ----
mean loss: 436.19
train mean loss: 439.76
epoch train time: 0:00:00.714142
elapsed time: 0:01:13.105046
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:30:29.434119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.66
 ---- batch: 020 ----
mean loss: 433.62
 ---- batch: 030 ----
mean loss: 433.21
 ---- batch: 040 ----
mean loss: 428.69
 ---- batch: 050 ----
mean loss: 434.45
 ---- batch: 060 ----
mean loss: 437.53
 ---- batch: 070 ----
mean loss: 408.43
 ---- batch: 080 ----
mean loss: 410.64
 ---- batch: 090 ----
mean loss: 415.53
 ---- batch: 100 ----
mean loss: 412.76
 ---- batch: 110 ----
mean loss: 414.50
train mean loss: 424.53
epoch train time: 0:00:00.712342
elapsed time: 0:01:13.817545
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:30:30.146605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.12
 ---- batch: 020 ----
mean loss: 414.00
 ---- batch: 030 ----
mean loss: 399.28
 ---- batch: 040 ----
mean loss: 399.05
 ---- batch: 050 ----
mean loss: 398.29
 ---- batch: 060 ----
mean loss: 395.98
 ---- batch: 070 ----
mean loss: 394.94
 ---- batch: 080 ----
mean loss: 378.50
 ---- batch: 090 ----
mean loss: 398.36
 ---- batch: 100 ----
mean loss: 390.65
 ---- batch: 110 ----
mean loss: 393.82
train mean loss: 398.00
epoch train time: 0:00:00.712788
elapsed time: 0:01:14.530471
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:30:30.859532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.36
 ---- batch: 020 ----
mean loss: 382.16
 ---- batch: 030 ----
mean loss: 383.03
 ---- batch: 040 ----
mean loss: 374.33
 ---- batch: 050 ----
mean loss: 367.87
 ---- batch: 060 ----
mean loss: 353.16
 ---- batch: 070 ----
mean loss: 364.69
 ---- batch: 080 ----
mean loss: 368.96
 ---- batch: 090 ----
mean loss: 358.81
 ---- batch: 100 ----
mean loss: 363.89
 ---- batch: 110 ----
mean loss: 362.45
train mean loss: 368.38
epoch train time: 0:00:00.718308
elapsed time: 0:01:15.248979
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:30:31.578044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.81
 ---- batch: 020 ----
mean loss: 353.89
 ---- batch: 030 ----
mean loss: 360.63
 ---- batch: 040 ----
mean loss: 345.13
 ---- batch: 050 ----
mean loss: 341.68
 ---- batch: 060 ----
mean loss: 341.63
 ---- batch: 070 ----
mean loss: 328.49
 ---- batch: 080 ----
mean loss: 339.45
 ---- batch: 090 ----
mean loss: 351.11
 ---- batch: 100 ----
mean loss: 341.11
 ---- batch: 110 ----
mean loss: 340.57
train mean loss: 345.60
epoch train time: 0:00:00.713044
elapsed time: 0:01:15.962164
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:30:32.291251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.91
 ---- batch: 020 ----
mean loss: 328.05
 ---- batch: 030 ----
mean loss: 339.03
 ---- batch: 040 ----
mean loss: 331.01
 ---- batch: 050 ----
mean loss: 338.88
 ---- batch: 060 ----
mean loss: 321.02
 ---- batch: 070 ----
mean loss: 317.45
 ---- batch: 080 ----
mean loss: 325.29
 ---- batch: 090 ----
mean loss: 324.68
 ---- batch: 100 ----
mean loss: 326.63
 ---- batch: 110 ----
mean loss: 319.84
train mean loss: 328.26
epoch train time: 0:00:00.713614
elapsed time: 0:01:16.675974
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:30:33.005049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.49
 ---- batch: 020 ----
mean loss: 322.30
 ---- batch: 030 ----
mean loss: 316.50
 ---- batch: 040 ----
mean loss: 311.02
 ---- batch: 050 ----
mean loss: 317.80
 ---- batch: 060 ----
mean loss: 313.93
 ---- batch: 070 ----
mean loss: 321.33
 ---- batch: 080 ----
mean loss: 307.90
 ---- batch: 090 ----
mean loss: 317.57
 ---- batch: 100 ----
mean loss: 302.62
 ---- batch: 110 ----
mean loss: 318.80
train mean loss: 314.17
epoch train time: 0:00:00.712872
elapsed time: 0:01:17.389011
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:30:33.718079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.02
 ---- batch: 020 ----
mean loss: 297.98
 ---- batch: 030 ----
mean loss: 298.79
 ---- batch: 040 ----
mean loss: 309.00
 ---- batch: 050 ----
mean loss: 300.82
 ---- batch: 060 ----
mean loss: 301.42
 ---- batch: 070 ----
mean loss: 287.09
 ---- batch: 080 ----
mean loss: 302.33
 ---- batch: 090 ----
mean loss: 303.50
 ---- batch: 100 ----
mean loss: 302.56
 ---- batch: 110 ----
mean loss: 297.80
train mean loss: 301.13
epoch train time: 0:00:00.719014
elapsed time: 0:01:18.108172
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:30:34.437233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.65
 ---- batch: 020 ----
mean loss: 289.07
 ---- batch: 030 ----
mean loss: 292.77
 ---- batch: 040 ----
mean loss: 296.56
 ---- batch: 050 ----
mean loss: 285.44
 ---- batch: 060 ----
mean loss: 286.24
 ---- batch: 070 ----
mean loss: 298.37
 ---- batch: 080 ----
mean loss: 284.98
 ---- batch: 090 ----
mean loss: 285.91
 ---- batch: 100 ----
mean loss: 290.43
 ---- batch: 110 ----
mean loss: 303.17
train mean loss: 290.88
epoch train time: 0:00:00.714278
elapsed time: 0:01:18.822600
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:30:35.151657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.00
 ---- batch: 020 ----
mean loss: 287.59
 ---- batch: 030 ----
mean loss: 285.69
 ---- batch: 040 ----
mean loss: 268.07
 ---- batch: 050 ----
mean loss: 281.48
 ---- batch: 060 ----
mean loss: 280.07
 ---- batch: 070 ----
mean loss: 288.19
 ---- batch: 080 ----
mean loss: 285.68
 ---- batch: 090 ----
mean loss: 292.26
 ---- batch: 100 ----
mean loss: 278.83
 ---- batch: 110 ----
mean loss: 278.78
train mean loss: 283.27
epoch train time: 0:00:00.710558
elapsed time: 0:01:19.533297
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:30:35.862391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.53
 ---- batch: 020 ----
mean loss: 270.57
 ---- batch: 030 ----
mean loss: 276.34
 ---- batch: 040 ----
mean loss: 281.88
 ---- batch: 050 ----
mean loss: 286.87
 ---- batch: 060 ----
mean loss: 270.84
 ---- batch: 070 ----
mean loss: 280.41
 ---- batch: 080 ----
mean loss: 271.17
 ---- batch: 090 ----
mean loss: 272.23
 ---- batch: 100 ----
mean loss: 267.06
 ---- batch: 110 ----
mean loss: 278.40
train mean loss: 276.63
epoch train time: 0:00:00.727682
elapsed time: 0:01:20.261187
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:30:36.590246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.49
 ---- batch: 020 ----
mean loss: 274.10
 ---- batch: 030 ----
mean loss: 268.50
 ---- batch: 040 ----
mean loss: 273.76
 ---- batch: 050 ----
mean loss: 262.66
 ---- batch: 060 ----
mean loss: 280.13
 ---- batch: 070 ----
mean loss: 268.17
 ---- batch: 080 ----
mean loss: 274.96
 ---- batch: 090 ----
mean loss: 268.55
 ---- batch: 100 ----
mean loss: 271.23
 ---- batch: 110 ----
mean loss: 263.15
train mean loss: 270.77
epoch train time: 0:00:00.740368
elapsed time: 0:01:21.001697
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:30:37.330776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.61
 ---- batch: 020 ----
mean loss: 267.56
 ---- batch: 030 ----
mean loss: 258.80
 ---- batch: 040 ----
mean loss: 267.36
 ---- batch: 050 ----
mean loss: 267.13
 ---- batch: 060 ----
mean loss: 259.88
 ---- batch: 070 ----
mean loss: 260.09
 ---- batch: 080 ----
mean loss: 262.74
 ---- batch: 090 ----
mean loss: 265.17
 ---- batch: 100 ----
mean loss: 265.82
 ---- batch: 110 ----
mean loss: 274.62
train mean loss: 265.64
epoch train time: 0:00:00.717343
elapsed time: 0:01:21.719209
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:30:38.048269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.91
 ---- batch: 020 ----
mean loss: 267.66
 ---- batch: 030 ----
mean loss: 257.25
 ---- batch: 040 ----
mean loss: 266.33
 ---- batch: 050 ----
mean loss: 264.11
 ---- batch: 060 ----
mean loss: 268.69
 ---- batch: 070 ----
mean loss: 265.18
 ---- batch: 080 ----
mean loss: 252.91
 ---- batch: 090 ----
mean loss: 257.11
 ---- batch: 100 ----
mean loss: 249.08
 ---- batch: 110 ----
mean loss: 267.18
train mean loss: 261.45
epoch train time: 0:00:00.714570
elapsed time: 0:01:22.433928
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:30:38.762989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.35
 ---- batch: 020 ----
mean loss: 258.25
 ---- batch: 030 ----
mean loss: 260.77
 ---- batch: 040 ----
mean loss: 246.88
 ---- batch: 050 ----
mean loss: 258.63
 ---- batch: 060 ----
mean loss: 264.38
 ---- batch: 070 ----
mean loss: 258.91
 ---- batch: 080 ----
mean loss: 255.39
 ---- batch: 090 ----
mean loss: 255.76
 ---- batch: 100 ----
mean loss: 249.70
 ---- batch: 110 ----
mean loss: 262.97
train mean loss: 257.92
epoch train time: 0:00:00.720388
elapsed time: 0:01:23.154468
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:30:39.483528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.13
 ---- batch: 020 ----
mean loss: 254.66
 ---- batch: 030 ----
mean loss: 259.61
 ---- batch: 040 ----
mean loss: 261.52
 ---- batch: 050 ----
mean loss: 249.69
 ---- batch: 060 ----
mean loss: 246.56
 ---- batch: 070 ----
mean loss: 255.11
 ---- batch: 080 ----
mean loss: 253.60
 ---- batch: 090 ----
mean loss: 257.88
 ---- batch: 100 ----
mean loss: 255.28
 ---- batch: 110 ----
mean loss: 258.15
train mean loss: 254.79
epoch train time: 0:00:00.717783
elapsed time: 0:01:23.872424
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:30:40.201485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.29
 ---- batch: 020 ----
mean loss: 248.40
 ---- batch: 030 ----
mean loss: 257.27
 ---- batch: 040 ----
mean loss: 250.58
 ---- batch: 050 ----
mean loss: 235.99
 ---- batch: 060 ----
mean loss: 254.02
 ---- batch: 070 ----
mean loss: 257.48
 ---- batch: 080 ----
mean loss: 260.29
 ---- batch: 090 ----
mean loss: 247.84
 ---- batch: 100 ----
mean loss: 256.70
 ---- batch: 110 ----
mean loss: 250.78
train mean loss: 252.03
epoch train time: 0:00:00.718754
elapsed time: 0:01:24.591346
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:30:40.920426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.21
 ---- batch: 020 ----
mean loss: 248.52
 ---- batch: 030 ----
mean loss: 252.72
 ---- batch: 040 ----
mean loss: 256.86
 ---- batch: 050 ----
mean loss: 258.19
 ---- batch: 060 ----
mean loss: 242.32
 ---- batch: 070 ----
mean loss: 249.32
 ---- batch: 080 ----
mean loss: 248.13
 ---- batch: 090 ----
mean loss: 248.99
 ---- batch: 100 ----
mean loss: 252.50
 ---- batch: 110 ----
mean loss: 247.42
train mean loss: 249.52
epoch train time: 0:00:00.722526
elapsed time: 0:01:25.314038
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:30:41.643099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.38
 ---- batch: 020 ----
mean loss: 253.06
 ---- batch: 030 ----
mean loss: 244.32
 ---- batch: 040 ----
mean loss: 243.16
 ---- batch: 050 ----
mean loss: 242.17
 ---- batch: 060 ----
mean loss: 247.70
 ---- batch: 070 ----
mean loss: 245.93
 ---- batch: 080 ----
mean loss: 252.39
 ---- batch: 090 ----
mean loss: 249.60
 ---- batch: 100 ----
mean loss: 244.54
 ---- batch: 110 ----
mean loss: 246.96
train mean loss: 247.37
epoch train time: 0:00:00.738821
elapsed time: 0:01:26.053001
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:30:42.382138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.07
 ---- batch: 020 ----
mean loss: 242.96
 ---- batch: 030 ----
mean loss: 242.69
 ---- batch: 040 ----
mean loss: 242.60
 ---- batch: 050 ----
mean loss: 237.28
 ---- batch: 060 ----
mean loss: 255.61
 ---- batch: 070 ----
mean loss: 248.38
 ---- batch: 080 ----
mean loss: 248.69
 ---- batch: 090 ----
mean loss: 240.55
 ---- batch: 100 ----
mean loss: 246.39
 ---- batch: 110 ----
mean loss: 241.05
train mean loss: 245.37
epoch train time: 0:00:00.738154
elapsed time: 0:01:26.791375
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:30:43.120437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.23
 ---- batch: 020 ----
mean loss: 245.93
 ---- batch: 030 ----
mean loss: 252.78
 ---- batch: 040 ----
mean loss: 244.97
 ---- batch: 050 ----
mean loss: 251.04
 ---- batch: 060 ----
mean loss: 237.87
 ---- batch: 070 ----
mean loss: 238.33
 ---- batch: 080 ----
mean loss: 240.91
 ---- batch: 090 ----
mean loss: 242.74
 ---- batch: 100 ----
mean loss: 236.99
 ---- batch: 110 ----
mean loss: 243.17
train mean loss: 243.20
epoch train time: 0:00:00.737206
elapsed time: 0:01:27.528733
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:30:43.857796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.28
 ---- batch: 020 ----
mean loss: 241.19
 ---- batch: 030 ----
mean loss: 246.67
 ---- batch: 040 ----
mean loss: 245.02
 ---- batch: 050 ----
mean loss: 241.85
 ---- batch: 060 ----
mean loss: 241.71
 ---- batch: 070 ----
mean loss: 240.11
 ---- batch: 080 ----
mean loss: 239.20
 ---- batch: 090 ----
mean loss: 240.94
 ---- batch: 100 ----
mean loss: 234.94
 ---- batch: 110 ----
mean loss: 241.13
train mean loss: 241.09
epoch train time: 0:00:00.725560
elapsed time: 0:01:28.254443
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:30:44.583509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.92
 ---- batch: 020 ----
mean loss: 244.17
 ---- batch: 030 ----
mean loss: 233.92
 ---- batch: 040 ----
mean loss: 240.46
 ---- batch: 050 ----
mean loss: 247.62
 ---- batch: 060 ----
mean loss: 242.73
 ---- batch: 070 ----
mean loss: 236.61
 ---- batch: 080 ----
mean loss: 234.26
 ---- batch: 090 ----
mean loss: 245.78
 ---- batch: 100 ----
mean loss: 236.36
 ---- batch: 110 ----
mean loss: 232.12
train mean loss: 239.27
epoch train time: 0:00:00.726406
elapsed time: 0:01:28.981001
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:30:45.310063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.58
 ---- batch: 020 ----
mean loss: 237.83
 ---- batch: 030 ----
mean loss: 228.72
 ---- batch: 040 ----
mean loss: 237.14
 ---- batch: 050 ----
mean loss: 239.05
 ---- batch: 060 ----
mean loss: 244.04
 ---- batch: 070 ----
mean loss: 241.85
 ---- batch: 080 ----
mean loss: 243.94
 ---- batch: 090 ----
mean loss: 236.75
 ---- batch: 100 ----
mean loss: 231.57
 ---- batch: 110 ----
mean loss: 236.62
train mean loss: 237.45
epoch train time: 0:00:00.732003
elapsed time: 0:01:29.713152
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:30:46.042214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.39
 ---- batch: 020 ----
mean loss: 239.08
 ---- batch: 030 ----
mean loss: 235.35
 ---- batch: 040 ----
mean loss: 241.68
 ---- batch: 050 ----
mean loss: 238.14
 ---- batch: 060 ----
mean loss: 233.79
 ---- batch: 070 ----
mean loss: 237.01
 ---- batch: 080 ----
mean loss: 231.03
 ---- batch: 090 ----
mean loss: 242.08
 ---- batch: 100 ----
mean loss: 221.12
 ---- batch: 110 ----
mean loss: 243.05
train mean loss: 235.62
epoch train time: 0:00:00.749862
elapsed time: 0:01:30.463161
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:30:46.792241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.93
 ---- batch: 020 ----
mean loss: 239.12
 ---- batch: 030 ----
mean loss: 227.13
 ---- batch: 040 ----
mean loss: 230.34
 ---- batch: 050 ----
mean loss: 238.29
 ---- batch: 060 ----
mean loss: 234.71
 ---- batch: 070 ----
mean loss: 243.49
 ---- batch: 080 ----
mean loss: 229.59
 ---- batch: 090 ----
mean loss: 235.92
 ---- batch: 100 ----
mean loss: 235.72
 ---- batch: 110 ----
mean loss: 226.17
train mean loss: 233.96
epoch train time: 0:00:00.734021
elapsed time: 0:01:31.197345
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:30:47.526406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.94
 ---- batch: 020 ----
mean loss: 235.43
 ---- batch: 030 ----
mean loss: 228.64
 ---- batch: 040 ----
mean loss: 230.01
 ---- batch: 050 ----
mean loss: 231.31
 ---- batch: 060 ----
mean loss: 229.43
 ---- batch: 070 ----
mean loss: 230.83
 ---- batch: 080 ----
mean loss: 242.85
 ---- batch: 090 ----
mean loss: 234.91
 ---- batch: 100 ----
mean loss: 222.36
 ---- batch: 110 ----
mean loss: 241.30
train mean loss: 232.34
epoch train time: 0:00:00.743937
elapsed time: 0:01:31.941434
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:30:48.270497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.99
 ---- batch: 020 ----
mean loss: 237.21
 ---- batch: 030 ----
mean loss: 236.45
 ---- batch: 040 ----
mean loss: 238.47
 ---- batch: 050 ----
mean loss: 228.52
 ---- batch: 060 ----
mean loss: 231.46
 ---- batch: 070 ----
mean loss: 232.25
 ---- batch: 080 ----
mean loss: 228.86
 ---- batch: 090 ----
mean loss: 226.66
 ---- batch: 100 ----
mean loss: 229.78
 ---- batch: 110 ----
mean loss: 230.82
train mean loss: 231.39
epoch train time: 0:00:00.732748
elapsed time: 0:01:32.674369
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:30:49.003449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.87
 ---- batch: 020 ----
mean loss: 238.12
 ---- batch: 030 ----
mean loss: 225.05
 ---- batch: 040 ----
mean loss: 224.44
 ---- batch: 050 ----
mean loss: 233.65
 ---- batch: 060 ----
mean loss: 228.22
 ---- batch: 070 ----
mean loss: 220.34
 ---- batch: 080 ----
mean loss: 230.00
 ---- batch: 090 ----
mean loss: 229.21
 ---- batch: 100 ----
mean loss: 234.28
 ---- batch: 110 ----
mean loss: 230.11
train mean loss: 229.70
epoch train time: 0:00:00.740893
elapsed time: 0:01:33.415433
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:30:49.744499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.39
 ---- batch: 020 ----
mean loss: 229.87
 ---- batch: 030 ----
mean loss: 229.76
 ---- batch: 040 ----
mean loss: 225.51
 ---- batch: 050 ----
mean loss: 224.60
 ---- batch: 060 ----
mean loss: 227.28
 ---- batch: 070 ----
mean loss: 235.77
 ---- batch: 080 ----
mean loss: 238.65
 ---- batch: 090 ----
mean loss: 229.22
 ---- batch: 100 ----
mean loss: 228.16
 ---- batch: 110 ----
mean loss: 224.55
train mean loss: 228.82
epoch train time: 0:00:00.744846
elapsed time: 0:01:34.160430
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:30:50.489493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.99
 ---- batch: 020 ----
mean loss: 235.60
 ---- batch: 030 ----
mean loss: 218.53
 ---- batch: 040 ----
mean loss: 233.68
 ---- batch: 050 ----
mean loss: 230.39
 ---- batch: 060 ----
mean loss: 224.52
 ---- batch: 070 ----
mean loss: 227.57
 ---- batch: 080 ----
mean loss: 232.55
 ---- batch: 090 ----
mean loss: 228.05
 ---- batch: 100 ----
mean loss: 227.55
 ---- batch: 110 ----
mean loss: 223.60
train mean loss: 227.58
epoch train time: 0:00:00.739410
elapsed time: 0:01:34.899983
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:30:51.229045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.80
 ---- batch: 020 ----
mean loss: 238.80
 ---- batch: 030 ----
mean loss: 221.44
 ---- batch: 040 ----
mean loss: 225.49
 ---- batch: 050 ----
mean loss: 231.52
 ---- batch: 060 ----
mean loss: 222.89
 ---- batch: 070 ----
mean loss: 220.19
 ---- batch: 080 ----
mean loss: 222.21
 ---- batch: 090 ----
mean loss: 228.75
 ---- batch: 100 ----
mean loss: 226.96
 ---- batch: 110 ----
mean loss: 227.86
train mean loss: 226.66
epoch train time: 0:00:00.749783
elapsed time: 0:01:35.649916
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:30:51.979001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.85
 ---- batch: 020 ----
mean loss: 229.80
 ---- batch: 030 ----
mean loss: 223.95
 ---- batch: 040 ----
mean loss: 218.71
 ---- batch: 050 ----
mean loss: 225.65
 ---- batch: 060 ----
mean loss: 223.24
 ---- batch: 070 ----
mean loss: 225.35
 ---- batch: 080 ----
mean loss: 233.99
 ---- batch: 090 ----
mean loss: 223.40
 ---- batch: 100 ----
mean loss: 218.66
 ---- batch: 110 ----
mean loss: 225.71
train mean loss: 225.71
epoch train time: 0:00:00.708572
elapsed time: 0:01:36.358660
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:30:52.687727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.18
 ---- batch: 020 ----
mean loss: 222.40
 ---- batch: 030 ----
mean loss: 230.45
 ---- batch: 040 ----
mean loss: 225.55
 ---- batch: 050 ----
mean loss: 232.18
 ---- batch: 060 ----
mean loss: 217.03
 ---- batch: 070 ----
mean loss: 223.75
 ---- batch: 080 ----
mean loss: 227.16
 ---- batch: 090 ----
mean loss: 228.23
 ---- batch: 100 ----
mean loss: 225.71
 ---- batch: 110 ----
mean loss: 224.83
train mean loss: 224.78
epoch train time: 0:00:00.719872
elapsed time: 0:01:37.078688
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:30:53.407753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.06
 ---- batch: 020 ----
mean loss: 226.99
 ---- batch: 030 ----
mean loss: 227.82
 ---- batch: 040 ----
mean loss: 231.04
 ---- batch: 050 ----
mean loss: 217.75
 ---- batch: 060 ----
mean loss: 221.26
 ---- batch: 070 ----
mean loss: 224.59
 ---- batch: 080 ----
mean loss: 223.19
 ---- batch: 090 ----
mean loss: 224.06
 ---- batch: 100 ----
mean loss: 221.27
 ---- batch: 110 ----
mean loss: 220.21
train mean loss: 224.21
epoch train time: 0:00:00.714753
elapsed time: 0:01:37.793589
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:30:54.122649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.74
 ---- batch: 020 ----
mean loss: 221.33
 ---- batch: 030 ----
mean loss: 216.15
 ---- batch: 040 ----
mean loss: 223.24
 ---- batch: 050 ----
mean loss: 220.33
 ---- batch: 060 ----
mean loss: 230.06
 ---- batch: 070 ----
mean loss: 228.58
 ---- batch: 080 ----
mean loss: 230.57
 ---- batch: 090 ----
mean loss: 221.09
 ---- batch: 100 ----
mean loss: 226.02
 ---- batch: 110 ----
mean loss: 223.08
train mean loss: 223.29
epoch train time: 0:00:00.729002
elapsed time: 0:01:38.522751
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:30:54.851811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.72
 ---- batch: 020 ----
mean loss: 232.11
 ---- batch: 030 ----
mean loss: 230.20
 ---- batch: 040 ----
mean loss: 220.10
 ---- batch: 050 ----
mean loss: 224.41
 ---- batch: 060 ----
mean loss: 224.23
 ---- batch: 070 ----
mean loss: 222.79
 ---- batch: 080 ----
mean loss: 215.35
 ---- batch: 090 ----
mean loss: 220.73
 ---- batch: 100 ----
mean loss: 212.62
 ---- batch: 110 ----
mean loss: 224.74
train mean loss: 222.69
epoch train time: 0:00:00.721846
elapsed time: 0:01:39.244736
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:30:55.573795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.25
 ---- batch: 020 ----
mean loss: 227.32
 ---- batch: 030 ----
mean loss: 218.27
 ---- batch: 040 ----
mean loss: 227.33
 ---- batch: 050 ----
mean loss: 213.52
 ---- batch: 060 ----
mean loss: 220.50
 ---- batch: 070 ----
mean loss: 227.34
 ---- batch: 080 ----
mean loss: 217.86
 ---- batch: 090 ----
mean loss: 217.50
 ---- batch: 100 ----
mean loss: 217.83
 ---- batch: 110 ----
mean loss: 227.79
train mean loss: 222.03
epoch train time: 0:00:00.724984
elapsed time: 0:01:39.969932
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:30:56.299020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.51
 ---- batch: 020 ----
mean loss: 225.85
 ---- batch: 030 ----
mean loss: 222.81
 ---- batch: 040 ----
mean loss: 227.26
 ---- batch: 050 ----
mean loss: 217.62
 ---- batch: 060 ----
mean loss: 213.05
 ---- batch: 070 ----
mean loss: 224.43
 ---- batch: 080 ----
mean loss: 215.16
 ---- batch: 090 ----
mean loss: 229.19
 ---- batch: 100 ----
mean loss: 219.75
 ---- batch: 110 ----
mean loss: 219.28
train mean loss: 221.27
epoch train time: 0:00:00.714409
elapsed time: 0:01:40.684512
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:30:57.013573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.35
 ---- batch: 020 ----
mean loss: 228.76
 ---- batch: 030 ----
mean loss: 214.67
 ---- batch: 040 ----
mean loss: 217.90
 ---- batch: 050 ----
mean loss: 219.48
 ---- batch: 060 ----
mean loss: 219.07
 ---- batch: 070 ----
mean loss: 225.61
 ---- batch: 080 ----
mean loss: 229.58
 ---- batch: 090 ----
mean loss: 220.55
 ---- batch: 100 ----
mean loss: 220.32
 ---- batch: 110 ----
mean loss: 216.81
train mean loss: 220.86
epoch train time: 0:00:00.704815
elapsed time: 0:01:41.389472
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:30:57.718533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.98
 ---- batch: 020 ----
mean loss: 219.71
 ---- batch: 030 ----
mean loss: 226.50
 ---- batch: 040 ----
mean loss: 221.12
 ---- batch: 050 ----
mean loss: 222.82
 ---- batch: 060 ----
mean loss: 218.37
 ---- batch: 070 ----
mean loss: 223.47
 ---- batch: 080 ----
mean loss: 221.41
 ---- batch: 090 ----
mean loss: 223.08
 ---- batch: 100 ----
mean loss: 215.36
 ---- batch: 110 ----
mean loss: 215.41
train mean loss: 220.10
epoch train time: 0:00:00.726244
elapsed time: 0:01:42.115866
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:30:58.444926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.44
 ---- batch: 020 ----
mean loss: 222.87
 ---- batch: 030 ----
mean loss: 222.09
 ---- batch: 040 ----
mean loss: 224.30
 ---- batch: 050 ----
mean loss: 225.41
 ---- batch: 060 ----
mean loss: 217.01
 ---- batch: 070 ----
mean loss: 213.12
 ---- batch: 080 ----
mean loss: 213.81
 ---- batch: 090 ----
mean loss: 219.71
 ---- batch: 100 ----
mean loss: 217.09
 ---- batch: 110 ----
mean loss: 218.62
train mean loss: 219.69
epoch train time: 0:00:00.743950
elapsed time: 0:01:42.859965
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:30:59.189032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.96
 ---- batch: 020 ----
mean loss: 211.34
 ---- batch: 030 ----
mean loss: 202.01
 ---- batch: 040 ----
mean loss: 221.22
 ---- batch: 050 ----
mean loss: 233.92
 ---- batch: 060 ----
mean loss: 225.68
 ---- batch: 070 ----
mean loss: 218.06
 ---- batch: 080 ----
mean loss: 219.03
 ---- batch: 090 ----
mean loss: 214.99
 ---- batch: 100 ----
mean loss: 220.53
 ---- batch: 110 ----
mean loss: 226.55
train mean loss: 219.04
epoch train time: 0:00:00.728295
elapsed time: 0:01:43.588409
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:30:59.917471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.32
 ---- batch: 020 ----
mean loss: 210.15
 ---- batch: 030 ----
mean loss: 218.07
 ---- batch: 040 ----
mean loss: 218.89
 ---- batch: 050 ----
mean loss: 229.56
 ---- batch: 060 ----
mean loss: 215.30
 ---- batch: 070 ----
mean loss: 215.96
 ---- batch: 080 ----
mean loss: 227.32
 ---- batch: 090 ----
mean loss: 219.23
 ---- batch: 100 ----
mean loss: 223.79
 ---- batch: 110 ----
mean loss: 217.39
train mean loss: 218.58
epoch train time: 0:00:00.712941
elapsed time: 0:01:44.301498
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:31:00.630559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.71
 ---- batch: 020 ----
mean loss: 218.57
 ---- batch: 030 ----
mean loss: 218.34
 ---- batch: 040 ----
mean loss: 210.07
 ---- batch: 050 ----
mean loss: 216.84
 ---- batch: 060 ----
mean loss: 224.59
 ---- batch: 070 ----
mean loss: 215.64
 ---- batch: 080 ----
mean loss: 230.81
 ---- batch: 090 ----
mean loss: 218.60
 ---- batch: 100 ----
mean loss: 217.29
 ---- batch: 110 ----
mean loss: 212.72
train mean loss: 217.92
epoch train time: 0:00:00.720380
elapsed time: 0:01:45.022021
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:31:01.351108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.87
 ---- batch: 020 ----
mean loss: 224.41
 ---- batch: 030 ----
mean loss: 208.40
 ---- batch: 040 ----
mean loss: 223.54
 ---- batch: 050 ----
mean loss: 208.62
 ---- batch: 060 ----
mean loss: 219.29
 ---- batch: 070 ----
mean loss: 209.04
 ---- batch: 080 ----
mean loss: 210.23
 ---- batch: 090 ----
mean loss: 216.36
 ---- batch: 100 ----
mean loss: 221.15
 ---- batch: 110 ----
mean loss: 226.82
train mean loss: 217.38
epoch train time: 0:00:00.725839
elapsed time: 0:01:45.748028
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:31:02.077087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.93
 ---- batch: 020 ----
mean loss: 216.95
 ---- batch: 030 ----
mean loss: 225.20
 ---- batch: 040 ----
mean loss: 218.25
 ---- batch: 050 ----
mean loss: 219.88
 ---- batch: 060 ----
mean loss: 218.39
 ---- batch: 070 ----
mean loss: 216.29
 ---- batch: 080 ----
mean loss: 213.61
 ---- batch: 090 ----
mean loss: 208.14
 ---- batch: 100 ----
mean loss: 224.15
 ---- batch: 110 ----
mean loss: 215.02
train mean loss: 216.93
epoch train time: 0:00:00.713319
elapsed time: 0:01:46.461488
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:31:02.790549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.26
 ---- batch: 020 ----
mean loss: 215.43
 ---- batch: 030 ----
mean loss: 214.42
 ---- batch: 040 ----
mean loss: 212.94
 ---- batch: 050 ----
mean loss: 222.09
 ---- batch: 060 ----
mean loss: 209.93
 ---- batch: 070 ----
mean loss: 217.23
 ---- batch: 080 ----
mean loss: 224.66
 ---- batch: 090 ----
mean loss: 218.61
 ---- batch: 100 ----
mean loss: 209.36
 ---- batch: 110 ----
mean loss: 215.56
train mean loss: 216.53
epoch train time: 0:00:00.715412
elapsed time: 0:01:47.177041
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:31:03.506126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.85
 ---- batch: 020 ----
mean loss: 215.86
 ---- batch: 030 ----
mean loss: 221.97
 ---- batch: 040 ----
mean loss: 220.21
 ---- batch: 050 ----
mean loss: 215.34
 ---- batch: 060 ----
mean loss: 218.14
 ---- batch: 070 ----
mean loss: 212.92
 ---- batch: 080 ----
mean loss: 218.71
 ---- batch: 090 ----
mean loss: 209.16
 ---- batch: 100 ----
mean loss: 212.75
 ---- batch: 110 ----
mean loss: 210.59
train mean loss: 216.07
epoch train time: 0:00:00.727833
elapsed time: 0:01:47.905086
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:31:04.234143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.29
 ---- batch: 020 ----
mean loss: 211.99
 ---- batch: 030 ----
mean loss: 218.96
 ---- batch: 040 ----
mean loss: 213.33
 ---- batch: 050 ----
mean loss: 209.73
 ---- batch: 060 ----
mean loss: 218.83
 ---- batch: 070 ----
mean loss: 212.75
 ---- batch: 080 ----
mean loss: 210.79
 ---- batch: 090 ----
mean loss: 211.20
 ---- batch: 100 ----
mean loss: 219.67
 ---- batch: 110 ----
mean loss: 220.61
train mean loss: 215.63
epoch train time: 0:00:00.719867
elapsed time: 0:01:48.625092
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:31:04.954155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.92
 ---- batch: 020 ----
mean loss: 218.28
 ---- batch: 030 ----
mean loss: 214.02
 ---- batch: 040 ----
mean loss: 211.97
 ---- batch: 050 ----
mean loss: 215.18
 ---- batch: 060 ----
mean loss: 211.34
 ---- batch: 070 ----
mean loss: 220.15
 ---- batch: 080 ----
mean loss: 210.80
 ---- batch: 090 ----
mean loss: 216.67
 ---- batch: 100 ----
mean loss: 216.79
 ---- batch: 110 ----
mean loss: 223.54
train mean loss: 215.14
epoch train time: 0:00:00.715980
elapsed time: 0:01:49.341236
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:31:05.670331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.58
 ---- batch: 020 ----
mean loss: 212.89
 ---- batch: 030 ----
mean loss: 213.57
 ---- batch: 040 ----
mean loss: 208.44
 ---- batch: 050 ----
mean loss: 207.51
 ---- batch: 060 ----
mean loss: 218.87
 ---- batch: 070 ----
mean loss: 216.98
 ---- batch: 080 ----
mean loss: 211.29
 ---- batch: 090 ----
mean loss: 215.13
 ---- batch: 100 ----
mean loss: 225.13
 ---- batch: 110 ----
mean loss: 209.54
train mean loss: 214.69
epoch train time: 0:00:00.727343
elapsed time: 0:01:50.068784
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:31:06.397838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.47
 ---- batch: 020 ----
mean loss: 224.71
 ---- batch: 030 ----
mean loss: 207.34
 ---- batch: 040 ----
mean loss: 213.74
 ---- batch: 050 ----
mean loss: 218.07
 ---- batch: 060 ----
mean loss: 211.03
 ---- batch: 070 ----
mean loss: 208.98
 ---- batch: 080 ----
mean loss: 220.39
 ---- batch: 090 ----
mean loss: 221.54
 ---- batch: 100 ----
mean loss: 204.51
 ---- batch: 110 ----
mean loss: 217.48
train mean loss: 214.42
epoch train time: 0:00:00.708614
elapsed time: 0:01:50.777539
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:31:07.106601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.78
 ---- batch: 020 ----
mean loss: 215.90
 ---- batch: 030 ----
mean loss: 219.00
 ---- batch: 040 ----
mean loss: 212.54
 ---- batch: 050 ----
mean loss: 217.74
 ---- batch: 060 ----
mean loss: 204.16
 ---- batch: 070 ----
mean loss: 222.18
 ---- batch: 080 ----
mean loss: 213.24
 ---- batch: 090 ----
mean loss: 222.91
 ---- batch: 100 ----
mean loss: 199.86
 ---- batch: 110 ----
mean loss: 208.60
train mean loss: 213.80
epoch train time: 0:00:00.711815
elapsed time: 0:01:51.489508
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:31:07.818569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.76
 ---- batch: 020 ----
mean loss: 218.57
 ---- batch: 030 ----
mean loss: 200.96
 ---- batch: 040 ----
mean loss: 210.81
 ---- batch: 050 ----
mean loss: 214.29
 ---- batch: 060 ----
mean loss: 212.58
 ---- batch: 070 ----
mean loss: 215.66
 ---- batch: 080 ----
mean loss: 211.22
 ---- batch: 090 ----
mean loss: 216.65
 ---- batch: 100 ----
mean loss: 219.01
 ---- batch: 110 ----
mean loss: 215.67
train mean loss: 213.44
epoch train time: 0:00:00.710338
elapsed time: 0:01:52.199990
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:31:08.529050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.87
 ---- batch: 020 ----
mean loss: 208.51
 ---- batch: 030 ----
mean loss: 219.02
 ---- batch: 040 ----
mean loss: 206.28
 ---- batch: 050 ----
mean loss: 214.60
 ---- batch: 060 ----
mean loss: 207.22
 ---- batch: 070 ----
mean loss: 222.23
 ---- batch: 080 ----
mean loss: 213.57
 ---- batch: 090 ----
mean loss: 207.31
 ---- batch: 100 ----
mean loss: 215.59
 ---- batch: 110 ----
mean loss: 217.13
train mean loss: 213.04
epoch train time: 0:00:00.718987
elapsed time: 0:01:52.919118
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:31:09.248181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.69
 ---- batch: 020 ----
mean loss: 213.91
 ---- batch: 030 ----
mean loss: 211.71
 ---- batch: 040 ----
mean loss: 201.24
 ---- batch: 050 ----
mean loss: 218.73
 ---- batch: 060 ----
mean loss: 209.46
 ---- batch: 070 ----
mean loss: 213.86
 ---- batch: 080 ----
mean loss: 221.54
 ---- batch: 090 ----
mean loss: 211.69
 ---- batch: 100 ----
mean loss: 208.24
 ---- batch: 110 ----
mean loss: 217.94
train mean loss: 212.72
epoch train time: 0:00:00.711268
elapsed time: 0:01:53.630532
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:31:09.959596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.22
 ---- batch: 020 ----
mean loss: 209.25
 ---- batch: 030 ----
mean loss: 218.19
 ---- batch: 040 ----
mean loss: 209.40
 ---- batch: 050 ----
mean loss: 206.68
 ---- batch: 060 ----
mean loss: 217.24
 ---- batch: 070 ----
mean loss: 210.94
 ---- batch: 080 ----
mean loss: 218.26
 ---- batch: 090 ----
mean loss: 220.33
 ---- batch: 100 ----
mean loss: 201.27
 ---- batch: 110 ----
mean loss: 209.49
train mean loss: 212.27
epoch train time: 0:00:00.717715
elapsed time: 0:01:54.348397
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:31:10.677481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.29
 ---- batch: 020 ----
mean loss: 204.55
 ---- batch: 030 ----
mean loss: 213.54
 ---- batch: 040 ----
mean loss: 200.19
 ---- batch: 050 ----
mean loss: 212.43
 ---- batch: 060 ----
mean loss: 216.26
 ---- batch: 070 ----
mean loss: 210.59
 ---- batch: 080 ----
mean loss: 216.96
 ---- batch: 090 ----
mean loss: 213.01
 ---- batch: 100 ----
mean loss: 214.31
 ---- batch: 110 ----
mean loss: 217.29
train mean loss: 211.89
epoch train time: 0:00:00.721569
elapsed time: 0:01:55.070136
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:31:11.399198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.10
 ---- batch: 020 ----
mean loss: 223.54
 ---- batch: 030 ----
mean loss: 205.08
 ---- batch: 040 ----
mean loss: 210.33
 ---- batch: 050 ----
mean loss: 220.36
 ---- batch: 060 ----
mean loss: 225.68
 ---- batch: 070 ----
mean loss: 212.00
 ---- batch: 080 ----
mean loss: 201.97
 ---- batch: 090 ----
mean loss: 207.80
 ---- batch: 100 ----
mean loss: 209.62
 ---- batch: 110 ----
mean loss: 207.77
train mean loss: 211.81
epoch train time: 0:00:00.708432
elapsed time: 0:01:55.778710
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:31:12.107772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.87
 ---- batch: 020 ----
mean loss: 218.92
 ---- batch: 030 ----
mean loss: 213.09
 ---- batch: 040 ----
mean loss: 205.91
 ---- batch: 050 ----
mean loss: 210.99
 ---- batch: 060 ----
mean loss: 208.78
 ---- batch: 070 ----
mean loss: 207.90
 ---- batch: 080 ----
mean loss: 216.08
 ---- batch: 090 ----
mean loss: 213.76
 ---- batch: 100 ----
mean loss: 201.86
 ---- batch: 110 ----
mean loss: 205.94
train mean loss: 211.15
epoch train time: 0:00:00.707753
elapsed time: 0:01:56.486606
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:31:12.815667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.51
 ---- batch: 020 ----
mean loss: 212.50
 ---- batch: 030 ----
mean loss: 207.39
 ---- batch: 040 ----
mean loss: 210.19
 ---- batch: 050 ----
mean loss: 208.50
 ---- batch: 060 ----
mean loss: 210.59
 ---- batch: 070 ----
mean loss: 218.67
 ---- batch: 080 ----
mean loss: 212.10
 ---- batch: 090 ----
mean loss: 210.43
 ---- batch: 100 ----
mean loss: 218.58
 ---- batch: 110 ----
mean loss: 205.07
train mean loss: 210.88
epoch train time: 0:00:00.721077
elapsed time: 0:01:57.207821
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:31:13.536881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.98
 ---- batch: 020 ----
mean loss: 214.24
 ---- batch: 030 ----
mean loss: 212.35
 ---- batch: 040 ----
mean loss: 209.23
 ---- batch: 050 ----
mean loss: 205.75
 ---- batch: 060 ----
mean loss: 209.45
 ---- batch: 070 ----
mean loss: 206.42
 ---- batch: 080 ----
mean loss: 218.74
 ---- batch: 090 ----
mean loss: 209.23
 ---- batch: 100 ----
mean loss: 211.29
 ---- batch: 110 ----
mean loss: 202.31
train mean loss: 210.33
epoch train time: 0:00:00.710073
elapsed time: 0:01:57.918040
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:31:14.247103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.46
 ---- batch: 020 ----
mean loss: 208.98
 ---- batch: 030 ----
mean loss: 212.35
 ---- batch: 040 ----
mean loss: 207.97
 ---- batch: 050 ----
mean loss: 210.89
 ---- batch: 060 ----
mean loss: 205.39
 ---- batch: 070 ----
mean loss: 207.19
 ---- batch: 080 ----
mean loss: 215.36
 ---- batch: 090 ----
mean loss: 214.39
 ---- batch: 100 ----
mean loss: 210.00
 ---- batch: 110 ----
mean loss: 209.29
train mean loss: 209.87
epoch train time: 0:00:00.709479
elapsed time: 0:01:58.627664
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:31:14.956727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.93
 ---- batch: 020 ----
mean loss: 203.93
 ---- batch: 030 ----
mean loss: 206.85
 ---- batch: 040 ----
mean loss: 209.70
 ---- batch: 050 ----
mean loss: 211.67
 ---- batch: 060 ----
mean loss: 219.87
 ---- batch: 070 ----
mean loss: 206.18
 ---- batch: 080 ----
mean loss: 209.81
 ---- batch: 090 ----
mean loss: 217.12
 ---- batch: 100 ----
mean loss: 206.74
 ---- batch: 110 ----
mean loss: 210.32
train mean loss: 209.79
epoch train time: 0:00:00.709370
elapsed time: 0:01:59.337186
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:31:15.666266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.57
 ---- batch: 020 ----
mean loss: 196.92
 ---- batch: 030 ----
mean loss: 213.67
 ---- batch: 040 ----
mean loss: 212.64
 ---- batch: 050 ----
mean loss: 208.98
 ---- batch: 060 ----
mean loss: 207.73
 ---- batch: 070 ----
mean loss: 210.65
 ---- batch: 080 ----
mean loss: 198.56
 ---- batch: 090 ----
mean loss: 215.98
 ---- batch: 100 ----
mean loss: 209.22
 ---- batch: 110 ----
mean loss: 212.70
train mean loss: 209.25
epoch train time: 0:00:00.722905
elapsed time: 0:02:00.060255
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:31:16.389317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.89
 ---- batch: 020 ----
mean loss: 204.76
 ---- batch: 030 ----
mean loss: 206.98
 ---- batch: 040 ----
mean loss: 212.51
 ---- batch: 050 ----
mean loss: 205.48
 ---- batch: 060 ----
mean loss: 207.37
 ---- batch: 070 ----
mean loss: 210.68
 ---- batch: 080 ----
mean loss: 208.70
 ---- batch: 090 ----
mean loss: 210.07
 ---- batch: 100 ----
mean loss: 213.57
 ---- batch: 110 ----
mean loss: 208.38
train mean loss: 208.82
epoch train time: 0:00:00.733409
elapsed time: 0:02:00.793811
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:31:17.122874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.77
 ---- batch: 020 ----
mean loss: 213.54
 ---- batch: 030 ----
mean loss: 202.59
 ---- batch: 040 ----
mean loss: 209.48
 ---- batch: 050 ----
mean loss: 216.22
 ---- batch: 060 ----
mean loss: 207.02
 ---- batch: 070 ----
mean loss: 214.04
 ---- batch: 080 ----
mean loss: 213.91
 ---- batch: 090 ----
mean loss: 195.92
 ---- batch: 100 ----
mean loss: 212.82
 ---- batch: 110 ----
mean loss: 199.18
train mean loss: 208.54
epoch train time: 0:00:00.701566
elapsed time: 0:02:01.495553
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:31:17.824620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.19
 ---- batch: 020 ----
mean loss: 201.18
 ---- batch: 030 ----
mean loss: 205.94
 ---- batch: 040 ----
mean loss: 211.44
 ---- batch: 050 ----
mean loss: 209.86
 ---- batch: 060 ----
mean loss: 209.04
 ---- batch: 070 ----
mean loss: 209.99
 ---- batch: 080 ----
mean loss: 207.27
 ---- batch: 090 ----
mean loss: 217.13
 ---- batch: 100 ----
mean loss: 205.61
 ---- batch: 110 ----
mean loss: 200.86
train mean loss: 207.97
epoch train time: 0:00:00.712099
elapsed time: 0:02:02.207814
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:31:18.536891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.55
 ---- batch: 020 ----
mean loss: 203.58
 ---- batch: 030 ----
mean loss: 210.32
 ---- batch: 040 ----
mean loss: 214.59
 ---- batch: 050 ----
mean loss: 214.15
 ---- batch: 060 ----
mean loss: 211.21
 ---- batch: 070 ----
mean loss: 210.94
 ---- batch: 080 ----
mean loss: 202.50
 ---- batch: 090 ----
mean loss: 209.01
 ---- batch: 100 ----
mean loss: 197.40
 ---- batch: 110 ----
mean loss: 205.37
train mean loss: 207.71
epoch train time: 0:00:00.722532
elapsed time: 0:02:02.930504
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:31:19.259566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.15
 ---- batch: 020 ----
mean loss: 202.99
 ---- batch: 030 ----
mean loss: 210.39
 ---- batch: 040 ----
mean loss: 216.73
 ---- batch: 050 ----
mean loss: 200.28
 ---- batch: 060 ----
mean loss: 207.11
 ---- batch: 070 ----
mean loss: 213.69
 ---- batch: 080 ----
mean loss: 213.58
 ---- batch: 090 ----
mean loss: 199.98
 ---- batch: 100 ----
mean loss: 201.33
 ---- batch: 110 ----
mean loss: 207.91
train mean loss: 207.37
epoch train time: 0:00:00.722215
elapsed time: 0:02:03.652888
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:31:19.981985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.48
 ---- batch: 020 ----
mean loss: 203.89
 ---- batch: 030 ----
mean loss: 204.37
 ---- batch: 040 ----
mean loss: 212.02
 ---- batch: 050 ----
mean loss: 205.32
 ---- batch: 060 ----
mean loss: 204.06
 ---- batch: 070 ----
mean loss: 207.62
 ---- batch: 080 ----
mean loss: 211.87
 ---- batch: 090 ----
mean loss: 205.20
 ---- batch: 100 ----
mean loss: 207.47
 ---- batch: 110 ----
mean loss: 205.10
train mean loss: 207.04
epoch train time: 0:00:00.728516
elapsed time: 0:02:04.381595
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:31:20.710646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.70
 ---- batch: 020 ----
mean loss: 202.13
 ---- batch: 030 ----
mean loss: 206.05
 ---- batch: 040 ----
mean loss: 192.73
 ---- batch: 050 ----
mean loss: 216.69
 ---- batch: 060 ----
mean loss: 208.60
 ---- batch: 070 ----
mean loss: 205.63
 ---- batch: 080 ----
mean loss: 206.27
 ---- batch: 090 ----
mean loss: 208.28
 ---- batch: 100 ----
mean loss: 208.69
 ---- batch: 110 ----
mean loss: 208.50
train mean loss: 206.66
epoch train time: 0:00:00.728426
elapsed time: 0:02:05.110154
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:31:21.439215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.07
 ---- batch: 020 ----
mean loss: 200.09
 ---- batch: 030 ----
mean loss: 206.96
 ---- batch: 040 ----
mean loss: 206.36
 ---- batch: 050 ----
mean loss: 214.02
 ---- batch: 060 ----
mean loss: 207.42
 ---- batch: 070 ----
mean loss: 209.77
 ---- batch: 080 ----
mean loss: 207.70
 ---- batch: 090 ----
mean loss: 198.09
 ---- batch: 100 ----
mean loss: 212.27
 ---- batch: 110 ----
mean loss: 196.26
train mean loss: 206.46
epoch train time: 0:00:00.713803
elapsed time: 0:02:05.824110
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:31:22.153197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.11
 ---- batch: 020 ----
mean loss: 212.36
 ---- batch: 030 ----
mean loss: 205.34
 ---- batch: 040 ----
mean loss: 206.03
 ---- batch: 050 ----
mean loss: 210.16
 ---- batch: 060 ----
mean loss: 207.98
 ---- batch: 070 ----
mean loss: 194.65
 ---- batch: 080 ----
mean loss: 206.59
 ---- batch: 090 ----
mean loss: 206.64
 ---- batch: 100 ----
mean loss: 203.03
 ---- batch: 110 ----
mean loss: 210.82
train mean loss: 206.01
epoch train time: 0:00:00.719903
elapsed time: 0:02:06.544183
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:31:22.873247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.95
 ---- batch: 020 ----
mean loss: 203.86
 ---- batch: 030 ----
mean loss: 205.08
 ---- batch: 040 ----
mean loss: 213.51
 ---- batch: 050 ----
mean loss: 199.57
 ---- batch: 060 ----
mean loss: 204.64
 ---- batch: 070 ----
mean loss: 210.80
 ---- batch: 080 ----
mean loss: 209.81
 ---- batch: 090 ----
mean loss: 207.51
 ---- batch: 100 ----
mean loss: 202.85
 ---- batch: 110 ----
mean loss: 202.84
train mean loss: 205.64
epoch train time: 0:00:00.726829
elapsed time: 0:02:07.271159
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:31:23.600234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.32
 ---- batch: 020 ----
mean loss: 203.64
 ---- batch: 030 ----
mean loss: 212.03
 ---- batch: 040 ----
mean loss: 204.62
 ---- batch: 050 ----
mean loss: 206.17
 ---- batch: 060 ----
mean loss: 202.53
 ---- batch: 070 ----
mean loss: 208.14
 ---- batch: 080 ----
mean loss: 206.45
 ---- batch: 090 ----
mean loss: 199.55
 ---- batch: 100 ----
mean loss: 209.35
 ---- batch: 110 ----
mean loss: 202.48
train mean loss: 205.56
epoch train time: 0:00:00.706368
elapsed time: 0:02:07.977687
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:31:24.306750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.10
 ---- batch: 020 ----
mean loss: 209.47
 ---- batch: 030 ----
mean loss: 208.23
 ---- batch: 040 ----
mean loss: 208.27
 ---- batch: 050 ----
mean loss: 203.27
 ---- batch: 060 ----
mean loss: 203.95
 ---- batch: 070 ----
mean loss: 211.61
 ---- batch: 080 ----
mean loss: 201.05
 ---- batch: 090 ----
mean loss: 194.42
 ---- batch: 100 ----
mean loss: 204.25
 ---- batch: 110 ----
mean loss: 201.72
train mean loss: 204.90
epoch train time: 0:00:00.721193
elapsed time: 0:02:08.699024
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:31:25.028109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.15
 ---- batch: 020 ----
mean loss: 201.28
 ---- batch: 030 ----
mean loss: 207.37
 ---- batch: 040 ----
mean loss: 197.49
 ---- batch: 050 ----
mean loss: 200.47
 ---- batch: 060 ----
mean loss: 199.44
 ---- batch: 070 ----
mean loss: 202.50
 ---- batch: 080 ----
mean loss: 202.83
 ---- batch: 090 ----
mean loss: 212.35
 ---- batch: 100 ----
mean loss: 197.60
 ---- batch: 110 ----
mean loss: 212.15
train mean loss: 204.67
epoch train time: 0:00:00.705640
elapsed time: 0:02:09.404828
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:31:25.733890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.32
 ---- batch: 020 ----
mean loss: 205.07
 ---- batch: 030 ----
mean loss: 213.87
 ---- batch: 040 ----
mean loss: 207.29
 ---- batch: 050 ----
mean loss: 203.95
 ---- batch: 060 ----
mean loss: 206.11
 ---- batch: 070 ----
mean loss: 203.06
 ---- batch: 080 ----
mean loss: 196.22
 ---- batch: 090 ----
mean loss: 198.75
 ---- batch: 100 ----
mean loss: 209.32
 ---- batch: 110 ----
mean loss: 199.62
train mean loss: 204.52
epoch train time: 0:00:00.728806
elapsed time: 0:02:10.133777
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:31:26.462837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.04
 ---- batch: 020 ----
mean loss: 211.25
 ---- batch: 030 ----
mean loss: 209.42
 ---- batch: 040 ----
mean loss: 203.96
 ---- batch: 050 ----
mean loss: 202.31
 ---- batch: 060 ----
mean loss: 205.72
 ---- batch: 070 ----
mean loss: 200.94
 ---- batch: 080 ----
mean loss: 202.97
 ---- batch: 090 ----
mean loss: 208.97
 ---- batch: 100 ----
mean loss: 199.13
 ---- batch: 110 ----
mean loss: 202.94
train mean loss: 204.22
epoch train time: 0:00:00.724541
elapsed time: 0:02:10.858459
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:31:27.187519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.89
 ---- batch: 020 ----
mean loss: 211.55
 ---- batch: 030 ----
mean loss: 206.49
 ---- batch: 040 ----
mean loss: 203.27
 ---- batch: 050 ----
mean loss: 197.88
 ---- batch: 060 ----
mean loss: 200.02
 ---- batch: 070 ----
mean loss: 206.62
 ---- batch: 080 ----
mean loss: 201.74
 ---- batch: 090 ----
mean loss: 206.54
 ---- batch: 100 ----
mean loss: 196.06
 ---- batch: 110 ----
mean loss: 195.05
train mean loss: 203.88
epoch train time: 0:00:00.722826
elapsed time: 0:02:11.581431
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:31:27.910492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.92
 ---- batch: 020 ----
mean loss: 205.61
 ---- batch: 030 ----
mean loss: 203.00
 ---- batch: 040 ----
mean loss: 206.63
 ---- batch: 050 ----
mean loss: 205.13
 ---- batch: 060 ----
mean loss: 208.45
 ---- batch: 070 ----
mean loss: 196.72
 ---- batch: 080 ----
mean loss: 205.13
 ---- batch: 090 ----
mean loss: 194.15
 ---- batch: 100 ----
mean loss: 212.00
 ---- batch: 110 ----
mean loss: 207.64
train mean loss: 203.46
epoch train time: 0:00:00.716403
elapsed time: 0:02:12.297982
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:31:28.627040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.07
 ---- batch: 020 ----
mean loss: 208.83
 ---- batch: 030 ----
mean loss: 197.76
 ---- batch: 040 ----
mean loss: 209.61
 ---- batch: 050 ----
mean loss: 195.48
 ---- batch: 060 ----
mean loss: 209.37
 ---- batch: 070 ----
mean loss: 193.05
 ---- batch: 080 ----
mean loss: 203.28
 ---- batch: 090 ----
mean loss: 197.57
 ---- batch: 100 ----
mean loss: 206.86
 ---- batch: 110 ----
mean loss: 198.37
train mean loss: 203.15
epoch train time: 0:00:00.720552
elapsed time: 0:02:13.018672
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:31:29.347750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.23
 ---- batch: 020 ----
mean loss: 212.55
 ---- batch: 030 ----
mean loss: 203.99
 ---- batch: 040 ----
mean loss: 208.44
 ---- batch: 050 ----
mean loss: 204.44
 ---- batch: 060 ----
mean loss: 205.26
 ---- batch: 070 ----
mean loss: 195.27
 ---- batch: 080 ----
mean loss: 198.22
 ---- batch: 090 ----
mean loss: 203.25
 ---- batch: 100 ----
mean loss: 194.77
 ---- batch: 110 ----
mean loss: 206.14
train mean loss: 202.99
epoch train time: 0:00:00.724023
elapsed time: 0:02:13.742865
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:31:30.071925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.90
 ---- batch: 020 ----
mean loss: 203.53
 ---- batch: 030 ----
mean loss: 214.00
 ---- batch: 040 ----
mean loss: 208.43
 ---- batch: 050 ----
mean loss: 207.91
 ---- batch: 060 ----
mean loss: 196.29
 ---- batch: 070 ----
mean loss: 204.63
 ---- batch: 080 ----
mean loss: 204.22
 ---- batch: 090 ----
mean loss: 209.07
 ---- batch: 100 ----
mean loss: 199.75
 ---- batch: 110 ----
mean loss: 193.93
train mean loss: 202.92
epoch train time: 0:00:00.727219
elapsed time: 0:02:14.470233
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:31:30.799296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.29
 ---- batch: 020 ----
mean loss: 206.78
 ---- batch: 030 ----
mean loss: 201.29
 ---- batch: 040 ----
mean loss: 200.59
 ---- batch: 050 ----
mean loss: 199.61
 ---- batch: 060 ----
mean loss: 203.19
 ---- batch: 070 ----
mean loss: 201.41
 ---- batch: 080 ----
mean loss: 207.45
 ---- batch: 090 ----
mean loss: 203.63
 ---- batch: 100 ----
mean loss: 197.26
 ---- batch: 110 ----
mean loss: 197.80
train mean loss: 202.47
epoch train time: 0:00:00.721995
elapsed time: 0:02:15.192370
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:31:31.521450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.72
 ---- batch: 020 ----
mean loss: 213.18
 ---- batch: 030 ----
mean loss: 198.70
 ---- batch: 040 ----
mean loss: 200.21
 ---- batch: 050 ----
mean loss: 197.99
 ---- batch: 060 ----
mean loss: 200.47
 ---- batch: 070 ----
mean loss: 198.87
 ---- batch: 080 ----
mean loss: 201.70
 ---- batch: 090 ----
mean loss: 193.93
 ---- batch: 100 ----
mean loss: 207.95
 ---- batch: 110 ----
mean loss: 212.15
train mean loss: 202.32
epoch train time: 0:00:00.720920
elapsed time: 0:02:15.913449
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:31:32.242508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.17
 ---- batch: 020 ----
mean loss: 198.70
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 200.16
 ---- batch: 050 ----
mean loss: 210.34
 ---- batch: 060 ----
mean loss: 194.51
 ---- batch: 070 ----
mean loss: 204.07
 ---- batch: 080 ----
mean loss: 211.30
 ---- batch: 090 ----
mean loss: 206.59
 ---- batch: 100 ----
mean loss: 203.85
 ---- batch: 110 ----
mean loss: 192.74
train mean loss: 201.99
epoch train time: 0:00:00.720648
elapsed time: 0:02:16.634237
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:31:32.963311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.16
 ---- batch: 020 ----
mean loss: 201.43
 ---- batch: 030 ----
mean loss: 203.99
 ---- batch: 040 ----
mean loss: 206.34
 ---- batch: 050 ----
mean loss: 198.30
 ---- batch: 060 ----
mean loss: 202.55
 ---- batch: 070 ----
mean loss: 196.07
 ---- batch: 080 ----
mean loss: 207.73
 ---- batch: 090 ----
mean loss: 200.93
 ---- batch: 100 ----
mean loss: 202.92
 ---- batch: 110 ----
mean loss: 200.87
train mean loss: 201.76
epoch train time: 0:00:00.720331
elapsed time: 0:02:17.354721
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:31:33.683780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.88
 ---- batch: 020 ----
mean loss: 203.38
 ---- batch: 030 ----
mean loss: 208.74
 ---- batch: 040 ----
mean loss: 199.22
 ---- batch: 050 ----
mean loss: 210.57
 ---- batch: 060 ----
mean loss: 206.05
 ---- batch: 070 ----
mean loss: 199.00
 ---- batch: 080 ----
mean loss: 202.91
 ---- batch: 090 ----
mean loss: 196.57
 ---- batch: 100 ----
mean loss: 197.40
 ---- batch: 110 ----
mean loss: 200.93
train mean loss: 201.45
epoch train time: 0:00:00.719562
elapsed time: 0:02:18.074452
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:31:34.403530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.35
 ---- batch: 020 ----
mean loss: 205.54
 ---- batch: 030 ----
mean loss: 196.07
 ---- batch: 040 ----
mean loss: 204.01
 ---- batch: 050 ----
mean loss: 194.25
 ---- batch: 060 ----
mean loss: 201.61
 ---- batch: 070 ----
mean loss: 203.78
 ---- batch: 080 ----
mean loss: 197.19
 ---- batch: 090 ----
mean loss: 203.86
 ---- batch: 100 ----
mean loss: 198.31
 ---- batch: 110 ----
mean loss: 204.32
train mean loss: 201.42
epoch train time: 0:00:00.717972
elapsed time: 0:02:18.792582
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:31:35.121665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.42
 ---- batch: 020 ----
mean loss: 202.37
 ---- batch: 030 ----
mean loss: 198.09
 ---- batch: 040 ----
mean loss: 197.47
 ---- batch: 050 ----
mean loss: 208.83
 ---- batch: 060 ----
mean loss: 197.69
 ---- batch: 070 ----
mean loss: 200.57
 ---- batch: 080 ----
mean loss: 204.46
 ---- batch: 090 ----
mean loss: 200.22
 ---- batch: 100 ----
mean loss: 207.97
 ---- batch: 110 ----
mean loss: 190.87
train mean loss: 200.92
epoch train time: 0:00:00.717149
elapsed time: 0:02:19.509936
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:31:35.838999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.26
 ---- batch: 020 ----
mean loss: 202.94
 ---- batch: 030 ----
mean loss: 205.78
 ---- batch: 040 ----
mean loss: 198.55
 ---- batch: 050 ----
mean loss: 202.29
 ---- batch: 060 ----
mean loss: 189.67
 ---- batch: 070 ----
mean loss: 198.88
 ---- batch: 080 ----
mean loss: 210.04
 ---- batch: 090 ----
mean loss: 202.37
 ---- batch: 100 ----
mean loss: 206.92
 ---- batch: 110 ----
mean loss: 193.21
train mean loss: 200.81
epoch train time: 0:00:00.729901
elapsed time: 0:02:20.239995
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:31:36.569046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.05
 ---- batch: 020 ----
mean loss: 198.56
 ---- batch: 030 ----
mean loss: 190.76
 ---- batch: 040 ----
mean loss: 192.78
 ---- batch: 050 ----
mean loss: 200.14
 ---- batch: 060 ----
mean loss: 200.73
 ---- batch: 070 ----
mean loss: 203.46
 ---- batch: 080 ----
mean loss: 204.39
 ---- batch: 090 ----
mean loss: 202.78
 ---- batch: 100 ----
mean loss: 203.42
 ---- batch: 110 ----
mean loss: 199.23
train mean loss: 200.51
epoch train time: 0:00:00.721203
elapsed time: 0:02:20.961360
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:31:37.290441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.49
 ---- batch: 020 ----
mean loss: 197.32
 ---- batch: 030 ----
mean loss: 204.75
 ---- batch: 040 ----
mean loss: 194.82
 ---- batch: 050 ----
mean loss: 199.63
 ---- batch: 060 ----
mean loss: 206.98
 ---- batch: 070 ----
mean loss: 196.37
 ---- batch: 080 ----
mean loss: 205.79
 ---- batch: 090 ----
mean loss: 202.42
 ---- batch: 100 ----
mean loss: 194.45
 ---- batch: 110 ----
mean loss: 199.34
train mean loss: 200.51
epoch train time: 0:00:00.725352
elapsed time: 0:02:21.686890
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:31:38.015950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.11
 ---- batch: 020 ----
mean loss: 195.07
 ---- batch: 030 ----
mean loss: 203.26
 ---- batch: 040 ----
mean loss: 201.97
 ---- batch: 050 ----
mean loss: 198.35
 ---- batch: 060 ----
mean loss: 203.75
 ---- batch: 070 ----
mean loss: 199.32
 ---- batch: 080 ----
mean loss: 199.36
 ---- batch: 090 ----
mean loss: 199.60
 ---- batch: 100 ----
mean loss: 207.90
 ---- batch: 110 ----
mean loss: 191.74
train mean loss: 200.18
epoch train time: 0:00:00.715625
elapsed time: 0:02:22.402669
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:31:38.731747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.77
 ---- batch: 020 ----
mean loss: 195.96
 ---- batch: 030 ----
mean loss: 197.27
 ---- batch: 040 ----
mean loss: 193.49
 ---- batch: 050 ----
mean loss: 201.23
 ---- batch: 060 ----
mean loss: 195.58
 ---- batch: 070 ----
mean loss: 194.82
 ---- batch: 080 ----
mean loss: 206.09
 ---- batch: 090 ----
mean loss: 204.96
 ---- batch: 100 ----
mean loss: 199.31
 ---- batch: 110 ----
mean loss: 198.63
train mean loss: 199.89
epoch train time: 0:00:00.717218
elapsed time: 0:02:23.120050
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:31:39.449117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.94
 ---- batch: 020 ----
mean loss: 189.46
 ---- batch: 030 ----
mean loss: 199.07
 ---- batch: 040 ----
mean loss: 197.72
 ---- batch: 050 ----
mean loss: 206.62
 ---- batch: 060 ----
mean loss: 203.50
 ---- batch: 070 ----
mean loss: 203.26
 ---- batch: 080 ----
mean loss: 199.02
 ---- batch: 090 ----
mean loss: 194.29
 ---- batch: 100 ----
mean loss: 198.92
 ---- batch: 110 ----
mean loss: 202.74
train mean loss: 199.73
epoch train time: 0:00:00.725672
elapsed time: 0:02:23.845883
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:31:40.174956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.27
 ---- batch: 020 ----
mean loss: 197.41
 ---- batch: 030 ----
mean loss: 199.97
 ---- batch: 040 ----
mean loss: 197.32
 ---- batch: 050 ----
mean loss: 201.19
 ---- batch: 060 ----
mean loss: 195.06
 ---- batch: 070 ----
mean loss: 199.75
 ---- batch: 080 ----
mean loss: 200.00
 ---- batch: 090 ----
mean loss: 206.55
 ---- batch: 100 ----
mean loss: 194.80
 ---- batch: 110 ----
mean loss: 204.69
train mean loss: 199.39
epoch train time: 0:00:00.717299
elapsed time: 0:02:24.563340
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:31:40.892402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.37
 ---- batch: 020 ----
mean loss: 198.37
 ---- batch: 030 ----
mean loss: 197.19
 ---- batch: 040 ----
mean loss: 201.05
 ---- batch: 050 ----
mean loss: 196.39
 ---- batch: 060 ----
mean loss: 197.32
 ---- batch: 070 ----
mean loss: 204.49
 ---- batch: 080 ----
mean loss: 197.00
 ---- batch: 090 ----
mean loss: 203.91
 ---- batch: 100 ----
mean loss: 197.98
 ---- batch: 110 ----
mean loss: 200.22
train mean loss: 199.40
epoch train time: 0:00:00.718595
elapsed time: 0:02:25.282087
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:31:41.611149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.66
 ---- batch: 020 ----
mean loss: 195.10
 ---- batch: 030 ----
mean loss: 206.21
 ---- batch: 040 ----
mean loss: 211.85
 ---- batch: 050 ----
mean loss: 193.91
 ---- batch: 060 ----
mean loss: 198.31
 ---- batch: 070 ----
mean loss: 203.21
 ---- batch: 080 ----
mean loss: 194.29
 ---- batch: 090 ----
mean loss: 190.76
 ---- batch: 100 ----
mean loss: 201.16
 ---- batch: 110 ----
mean loss: 196.53
train mean loss: 199.27
epoch train time: 0:00:00.733418
elapsed time: 0:02:26.015647
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:31:42.344706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.77
 ---- batch: 020 ----
mean loss: 197.90
 ---- batch: 030 ----
mean loss: 201.68
 ---- batch: 040 ----
mean loss: 198.00
 ---- batch: 050 ----
mean loss: 189.00
 ---- batch: 060 ----
mean loss: 206.99
 ---- batch: 070 ----
mean loss: 193.73
 ---- batch: 080 ----
mean loss: 197.72
 ---- batch: 090 ----
mean loss: 204.63
 ---- batch: 100 ----
mean loss: 200.47
 ---- batch: 110 ----
mean loss: 205.50
train mean loss: 199.11
epoch train time: 0:00:00.717268
elapsed time: 0:02:26.733053
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:31:43.062110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.97
 ---- batch: 020 ----
mean loss: 196.14
 ---- batch: 030 ----
mean loss: 200.57
 ---- batch: 040 ----
mean loss: 193.74
 ---- batch: 050 ----
mean loss: 198.06
 ---- batch: 060 ----
mean loss: 195.19
 ---- batch: 070 ----
mean loss: 200.52
 ---- batch: 080 ----
mean loss: 196.53
 ---- batch: 090 ----
mean loss: 188.95
 ---- batch: 100 ----
mean loss: 203.55
 ---- batch: 110 ----
mean loss: 207.23
train mean loss: 198.67
epoch train time: 0:00:00.705045
elapsed time: 0:02:27.438232
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:31:43.767313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.22
 ---- batch: 020 ----
mean loss: 197.12
 ---- batch: 030 ----
mean loss: 195.81
 ---- batch: 040 ----
mean loss: 201.79
 ---- batch: 050 ----
mean loss: 201.52
 ---- batch: 060 ----
mean loss: 192.66
 ---- batch: 070 ----
mean loss: 202.29
 ---- batch: 080 ----
mean loss: 197.78
 ---- batch: 090 ----
mean loss: 194.15
 ---- batch: 100 ----
mean loss: 208.18
 ---- batch: 110 ----
mean loss: 194.13
train mean loss: 198.81
epoch train time: 0:00:00.722227
elapsed time: 0:02:28.160822
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:31:44.489893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.31
 ---- batch: 020 ----
mean loss: 196.85
 ---- batch: 030 ----
mean loss: 197.54
 ---- batch: 040 ----
mean loss: 201.04
 ---- batch: 050 ----
mean loss: 213.28
 ---- batch: 060 ----
mean loss: 193.83
 ---- batch: 070 ----
mean loss: 191.97
 ---- batch: 080 ----
mean loss: 206.52
 ---- batch: 090 ----
mean loss: 203.14
 ---- batch: 100 ----
mean loss: 186.10
 ---- batch: 110 ----
mean loss: 190.71
train mean loss: 198.29
epoch train time: 0:00:00.730136
elapsed time: 0:02:28.891137
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:31:45.220230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.62
 ---- batch: 020 ----
mean loss: 199.07
 ---- batch: 030 ----
mean loss: 196.30
 ---- batch: 040 ----
mean loss: 200.72
 ---- batch: 050 ----
mean loss: 206.79
 ---- batch: 060 ----
mean loss: 193.55
 ---- batch: 070 ----
mean loss: 195.91
 ---- batch: 080 ----
mean loss: 198.61
 ---- batch: 090 ----
mean loss: 198.71
 ---- batch: 100 ----
mean loss: 206.90
 ---- batch: 110 ----
mean loss: 197.92
train mean loss: 198.18
epoch train time: 0:00:00.708312
elapsed time: 0:02:29.599622
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:31:45.928685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.79
 ---- batch: 020 ----
mean loss: 203.38
 ---- batch: 030 ----
mean loss: 202.60
 ---- batch: 040 ----
mean loss: 192.74
 ---- batch: 050 ----
mean loss: 208.17
 ---- batch: 060 ----
mean loss: 198.75
 ---- batch: 070 ----
mean loss: 194.36
 ---- batch: 080 ----
mean loss: 179.81
 ---- batch: 090 ----
mean loss: 192.93
 ---- batch: 100 ----
mean loss: 202.72
 ---- batch: 110 ----
mean loss: 203.97
train mean loss: 198.32
epoch train time: 0:00:00.715603
elapsed time: 0:02:30.315367
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:31:46.644427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.76
 ---- batch: 020 ----
mean loss: 199.31
 ---- batch: 030 ----
mean loss: 205.71
 ---- batch: 040 ----
mean loss: 207.85
 ---- batch: 050 ----
mean loss: 199.36
 ---- batch: 060 ----
mean loss: 195.91
 ---- batch: 070 ----
mean loss: 196.44
 ---- batch: 080 ----
mean loss: 196.84
 ---- batch: 090 ----
mean loss: 190.11
 ---- batch: 100 ----
mean loss: 189.39
 ---- batch: 110 ----
mean loss: 197.06
train mean loss: 197.82
epoch train time: 0:00:00.722569
elapsed time: 0:02:31.038078
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:31:47.367137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.94
 ---- batch: 020 ----
mean loss: 204.21
 ---- batch: 030 ----
mean loss: 198.57
 ---- batch: 040 ----
mean loss: 199.11
 ---- batch: 050 ----
mean loss: 201.77
 ---- batch: 060 ----
mean loss: 189.79
 ---- batch: 070 ----
mean loss: 211.19
 ---- batch: 080 ----
mean loss: 193.95
 ---- batch: 090 ----
mean loss: 187.74
 ---- batch: 100 ----
mean loss: 196.75
 ---- batch: 110 ----
mean loss: 197.02
train mean loss: 197.73
epoch train time: 0:00:00.730102
elapsed time: 0:02:31.768321
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:31:48.097383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.07
 ---- batch: 020 ----
mean loss: 189.49
 ---- batch: 030 ----
mean loss: 202.26
 ---- batch: 040 ----
mean loss: 197.89
 ---- batch: 050 ----
mean loss: 212.97
 ---- batch: 060 ----
mean loss: 195.88
 ---- batch: 070 ----
mean loss: 195.64
 ---- batch: 080 ----
mean loss: 198.97
 ---- batch: 090 ----
mean loss: 198.80
 ---- batch: 100 ----
mean loss: 191.54
 ---- batch: 110 ----
mean loss: 188.00
train mean loss: 197.63
epoch train time: 0:00:00.730091
elapsed time: 0:02:32.498553
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:31:48.827631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.53
 ---- batch: 020 ----
mean loss: 198.31
 ---- batch: 030 ----
mean loss: 192.66
 ---- batch: 040 ----
mean loss: 192.18
 ---- batch: 050 ----
mean loss: 191.45
 ---- batch: 060 ----
mean loss: 196.85
 ---- batch: 070 ----
mean loss: 187.38
 ---- batch: 080 ----
mean loss: 209.63
 ---- batch: 090 ----
mean loss: 201.36
 ---- batch: 100 ----
mean loss: 213.26
 ---- batch: 110 ----
mean loss: 191.32
train mean loss: 197.66
epoch train time: 0:00:00.731446
elapsed time: 0:02:33.230159
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:31:49.559220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.08
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 192.21
 ---- batch: 040 ----
mean loss: 191.11
 ---- batch: 050 ----
mean loss: 200.92
 ---- batch: 060 ----
mean loss: 197.73
 ---- batch: 070 ----
mean loss: 195.87
 ---- batch: 080 ----
mean loss: 191.20
 ---- batch: 090 ----
mean loss: 202.45
 ---- batch: 100 ----
mean loss: 201.35
 ---- batch: 110 ----
mean loss: 202.57
train mean loss: 197.23
epoch train time: 0:00:00.738937
elapsed time: 0:02:33.969259
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:31:50.298323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.53
 ---- batch: 020 ----
mean loss: 201.56
 ---- batch: 030 ----
mean loss: 201.21
 ---- batch: 040 ----
mean loss: 206.18
 ---- batch: 050 ----
mean loss: 206.64
 ---- batch: 060 ----
mean loss: 190.84
 ---- batch: 070 ----
mean loss: 189.48
 ---- batch: 080 ----
mean loss: 196.23
 ---- batch: 090 ----
mean loss: 191.59
 ---- batch: 100 ----
mean loss: 191.43
 ---- batch: 110 ----
mean loss: 194.05
train mean loss: 197.13
epoch train time: 0:00:00.742618
elapsed time: 0:02:34.712029
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:31:51.041092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.81
 ---- batch: 020 ----
mean loss: 198.95
 ---- batch: 030 ----
mean loss: 189.47
 ---- batch: 040 ----
mean loss: 208.26
 ---- batch: 050 ----
mean loss: 201.67
 ---- batch: 060 ----
mean loss: 198.72
 ---- batch: 070 ----
mean loss: 198.06
 ---- batch: 080 ----
mean loss: 200.67
 ---- batch: 090 ----
mean loss: 192.65
 ---- batch: 100 ----
mean loss: 198.73
 ---- batch: 110 ----
mean loss: 191.02
train mean loss: 197.20
epoch train time: 0:00:00.748037
elapsed time: 0:02:35.460216
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:31:51.789292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.84
 ---- batch: 020 ----
mean loss: 202.50
 ---- batch: 030 ----
mean loss: 198.42
 ---- batch: 040 ----
mean loss: 195.10
 ---- batch: 050 ----
mean loss: 191.58
 ---- batch: 060 ----
mean loss: 203.11
 ---- batch: 070 ----
mean loss: 190.67
 ---- batch: 080 ----
mean loss: 198.87
 ---- batch: 090 ----
mean loss: 201.50
 ---- batch: 100 ----
mean loss: 191.20
 ---- batch: 110 ----
mean loss: 191.17
train mean loss: 196.88
epoch train time: 0:00:00.728132
elapsed time: 0:02:36.188507
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:31:52.517570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.52
 ---- batch: 020 ----
mean loss: 207.41
 ---- batch: 030 ----
mean loss: 201.85
 ---- batch: 040 ----
mean loss: 195.49
 ---- batch: 050 ----
mean loss: 195.09
 ---- batch: 060 ----
mean loss: 190.37
 ---- batch: 070 ----
mean loss: 204.29
 ---- batch: 080 ----
mean loss: 191.86
 ---- batch: 090 ----
mean loss: 199.39
 ---- batch: 100 ----
mean loss: 196.04
 ---- batch: 110 ----
mean loss: 195.66
train mean loss: 196.74
epoch train time: 0:00:00.738442
elapsed time: 0:02:36.927101
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:31:53.256184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.52
 ---- batch: 020 ----
mean loss: 203.61
 ---- batch: 030 ----
mean loss: 196.68
 ---- batch: 040 ----
mean loss: 190.12
 ---- batch: 050 ----
mean loss: 195.41
 ---- batch: 060 ----
mean loss: 195.45
 ---- batch: 070 ----
mean loss: 188.78
 ---- batch: 080 ----
mean loss: 199.76
 ---- batch: 090 ----
mean loss: 206.71
 ---- batch: 100 ----
mean loss: 191.76
 ---- batch: 110 ----
mean loss: 198.27
train mean loss: 196.54
epoch train time: 0:00:00.741229
elapsed time: 0:02:37.668497
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:31:53.997581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.36
 ---- batch: 020 ----
mean loss: 196.99
 ---- batch: 030 ----
mean loss: 202.95
 ---- batch: 040 ----
mean loss: 199.89
 ---- batch: 050 ----
mean loss: 190.45
 ---- batch: 060 ----
mean loss: 194.32
 ---- batch: 070 ----
mean loss: 196.37
 ---- batch: 080 ----
mean loss: 192.67
 ---- batch: 090 ----
mean loss: 202.50
 ---- batch: 100 ----
mean loss: 188.42
 ---- batch: 110 ----
mean loss: 199.25
train mean loss: 196.44
epoch train time: 0:00:00.727342
elapsed time: 0:02:38.396017
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:31:54.725068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.79
 ---- batch: 020 ----
mean loss: 204.75
 ---- batch: 030 ----
mean loss: 198.09
 ---- batch: 040 ----
mean loss: 195.96
 ---- batch: 050 ----
mean loss: 202.67
 ---- batch: 060 ----
mean loss: 194.76
 ---- batch: 070 ----
mean loss: 195.46
 ---- batch: 080 ----
mean loss: 188.45
 ---- batch: 090 ----
mean loss: 194.87
 ---- batch: 100 ----
mean loss: 194.30
 ---- batch: 110 ----
mean loss: 195.15
train mean loss: 196.19
epoch train time: 0:00:00.729173
elapsed time: 0:02:39.125326
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:31:55.454391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.30
 ---- batch: 020 ----
mean loss: 193.49
 ---- batch: 030 ----
mean loss: 195.03
 ---- batch: 040 ----
mean loss: 193.71
 ---- batch: 050 ----
mean loss: 189.41
 ---- batch: 060 ----
mean loss: 193.96
 ---- batch: 070 ----
mean loss: 200.71
 ---- batch: 080 ----
mean loss: 197.30
 ---- batch: 090 ----
mean loss: 200.53
 ---- batch: 100 ----
mean loss: 198.48
 ---- batch: 110 ----
mean loss: 191.06
train mean loss: 196.21
epoch train time: 0:00:00.726555
elapsed time: 0:02:39.852026
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:31:56.181087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.25
 ---- batch: 020 ----
mean loss: 195.38
 ---- batch: 030 ----
mean loss: 187.48
 ---- batch: 040 ----
mean loss: 206.16
 ---- batch: 050 ----
mean loss: 201.13
 ---- batch: 060 ----
mean loss: 196.73
 ---- batch: 070 ----
mean loss: 193.11
 ---- batch: 080 ----
mean loss: 202.23
 ---- batch: 090 ----
mean loss: 189.38
 ---- batch: 100 ----
mean loss: 195.56
 ---- batch: 110 ----
mean loss: 193.60
train mean loss: 195.98
epoch train time: 0:00:00.714011
elapsed time: 0:02:40.566195
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:31:56.895258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.18
 ---- batch: 020 ----
mean loss: 205.63
 ---- batch: 030 ----
mean loss: 200.12
 ---- batch: 040 ----
mean loss: 187.13
 ---- batch: 050 ----
mean loss: 195.36
 ---- batch: 060 ----
mean loss: 196.76
 ---- batch: 070 ----
mean loss: 185.86
 ---- batch: 080 ----
mean loss: 196.84
 ---- batch: 090 ----
mean loss: 201.14
 ---- batch: 100 ----
mean loss: 195.70
 ---- batch: 110 ----
mean loss: 187.95
train mean loss: 195.90
epoch train time: 0:00:00.713927
elapsed time: 0:02:41.280264
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:31:57.609322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.97
 ---- batch: 020 ----
mean loss: 202.23
 ---- batch: 030 ----
mean loss: 197.95
 ---- batch: 040 ----
mean loss: 190.48
 ---- batch: 050 ----
mean loss: 189.47
 ---- batch: 060 ----
mean loss: 198.18
 ---- batch: 070 ----
mean loss: 194.74
 ---- batch: 080 ----
mean loss: 194.00
 ---- batch: 090 ----
mean loss: 198.43
 ---- batch: 100 ----
mean loss: 192.18
 ---- batch: 110 ----
mean loss: 201.54
train mean loss: 195.79
epoch train time: 0:00:00.717437
elapsed time: 0:02:41.997841
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:31:58.326923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.36
 ---- batch: 020 ----
mean loss: 191.25
 ---- batch: 030 ----
mean loss: 197.71
 ---- batch: 040 ----
mean loss: 201.33
 ---- batch: 050 ----
mean loss: 199.92
 ---- batch: 060 ----
mean loss: 188.93
 ---- batch: 070 ----
mean loss: 199.78
 ---- batch: 080 ----
mean loss: 198.74
 ---- batch: 090 ----
mean loss: 192.78
 ---- batch: 100 ----
mean loss: 190.15
 ---- batch: 110 ----
mean loss: 201.14
train mean loss: 195.93
epoch train time: 0:00:00.717630
elapsed time: 0:02:42.715637
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:31:59.044700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.63
 ---- batch: 020 ----
mean loss: 201.34
 ---- batch: 030 ----
mean loss: 193.54
 ---- batch: 040 ----
mean loss: 188.73
 ---- batch: 050 ----
mean loss: 201.47
 ---- batch: 060 ----
mean loss: 199.58
 ---- batch: 070 ----
mean loss: 196.22
 ---- batch: 080 ----
mean loss: 197.44
 ---- batch: 090 ----
mean loss: 188.17
 ---- batch: 100 ----
mean loss: 196.72
 ---- batch: 110 ----
mean loss: 197.20
train mean loss: 195.46
epoch train time: 0:00:00.737402
elapsed time: 0:02:43.453182
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:31:59.782241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.69
 ---- batch: 020 ----
mean loss: 195.87
 ---- batch: 030 ----
mean loss: 204.99
 ---- batch: 040 ----
mean loss: 186.89
 ---- batch: 050 ----
mean loss: 194.17
 ---- batch: 060 ----
mean loss: 186.89
 ---- batch: 070 ----
mean loss: 190.29
 ---- batch: 080 ----
mean loss: 195.62
 ---- batch: 090 ----
mean loss: 199.87
 ---- batch: 100 ----
mean loss: 198.75
 ---- batch: 110 ----
mean loss: 199.48
train mean loss: 195.50
epoch train time: 0:00:00.719292
elapsed time: 0:02:44.172664
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:32:00.501759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.98
 ---- batch: 020 ----
mean loss: 200.00
 ---- batch: 030 ----
mean loss: 196.61
 ---- batch: 040 ----
mean loss: 201.41
 ---- batch: 050 ----
mean loss: 191.36
 ---- batch: 060 ----
mean loss: 197.27
 ---- batch: 070 ----
mean loss: 193.25
 ---- batch: 080 ----
mean loss: 191.13
 ---- batch: 090 ----
mean loss: 198.12
 ---- batch: 100 ----
mean loss: 191.41
 ---- batch: 110 ----
mean loss: 186.36
train mean loss: 195.26
epoch train time: 0:00:00.718400
elapsed time: 0:02:44.891240
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:32:01.220298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.84
 ---- batch: 020 ----
mean loss: 195.97
 ---- batch: 030 ----
mean loss: 186.23
 ---- batch: 040 ----
mean loss: 200.10
 ---- batch: 050 ----
mean loss: 183.65
 ---- batch: 060 ----
mean loss: 195.92
 ---- batch: 070 ----
mean loss: 194.37
 ---- batch: 080 ----
mean loss: 199.44
 ---- batch: 090 ----
mean loss: 203.66
 ---- batch: 100 ----
mean loss: 198.93
 ---- batch: 110 ----
mean loss: 192.80
train mean loss: 195.06
epoch train time: 0:00:00.732690
elapsed time: 0:02:45.624074
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:32:01.953135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.07
 ---- batch: 020 ----
mean loss: 197.14
 ---- batch: 030 ----
mean loss: 199.31
 ---- batch: 040 ----
mean loss: 194.57
 ---- batch: 050 ----
mean loss: 196.16
 ---- batch: 060 ----
mean loss: 198.76
 ---- batch: 070 ----
mean loss: 193.62
 ---- batch: 080 ----
mean loss: 195.91
 ---- batch: 090 ----
mean loss: 185.39
 ---- batch: 100 ----
mean loss: 192.88
 ---- batch: 110 ----
mean loss: 200.61
train mean loss: 194.87
epoch train time: 0:00:00.726112
elapsed time: 0:02:46.350344
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:32:02.679437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.18
 ---- batch: 020 ----
mean loss: 183.73
 ---- batch: 030 ----
mean loss: 203.33
 ---- batch: 040 ----
mean loss: 196.47
 ---- batch: 050 ----
mean loss: 200.92
 ---- batch: 060 ----
mean loss: 194.52
 ---- batch: 070 ----
mean loss: 199.33
 ---- batch: 080 ----
mean loss: 187.77
 ---- batch: 090 ----
mean loss: 190.94
 ---- batch: 100 ----
mean loss: 199.04
 ---- batch: 110 ----
mean loss: 195.75
train mean loss: 195.01
epoch train time: 0:00:00.736136
elapsed time: 0:02:47.086662
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:32:03.415725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.27
 ---- batch: 020 ----
mean loss: 205.43
 ---- batch: 030 ----
mean loss: 199.26
 ---- batch: 040 ----
mean loss: 191.14
 ---- batch: 050 ----
mean loss: 193.29
 ---- batch: 060 ----
mean loss: 193.25
 ---- batch: 070 ----
mean loss: 193.41
 ---- batch: 080 ----
mean loss: 193.92
 ---- batch: 090 ----
mean loss: 187.61
 ---- batch: 100 ----
mean loss: 189.35
 ---- batch: 110 ----
mean loss: 195.35
train mean loss: 194.70
epoch train time: 0:00:00.735514
elapsed time: 0:02:47.822358
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:32:04.151417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.14
 ---- batch: 020 ----
mean loss: 191.75
 ---- batch: 030 ----
mean loss: 195.69
 ---- batch: 040 ----
mean loss: 197.62
 ---- batch: 050 ----
mean loss: 196.69
 ---- batch: 060 ----
mean loss: 201.07
 ---- batch: 070 ----
mean loss: 190.55
 ---- batch: 080 ----
mean loss: 190.04
 ---- batch: 090 ----
mean loss: 199.43
 ---- batch: 100 ----
mean loss: 195.91
 ---- batch: 110 ----
mean loss: 194.90
train mean loss: 194.72
epoch train time: 0:00:00.750690
elapsed time: 0:02:48.573188
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:32:04.902248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.41
 ---- batch: 020 ----
mean loss: 197.48
 ---- batch: 030 ----
mean loss: 202.76
 ---- batch: 040 ----
mean loss: 184.40
 ---- batch: 050 ----
mean loss: 197.39
 ---- batch: 060 ----
mean loss: 190.08
 ---- batch: 070 ----
mean loss: 196.84
 ---- batch: 080 ----
mean loss: 190.36
 ---- batch: 090 ----
mean loss: 196.02
 ---- batch: 100 ----
mean loss: 195.09
 ---- batch: 110 ----
mean loss: 193.75
train mean loss: 194.46
epoch train time: 0:00:00.725935
elapsed time: 0:02:49.299274
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:32:05.628353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.20
 ---- batch: 020 ----
mean loss: 197.11
 ---- batch: 030 ----
mean loss: 205.66
 ---- batch: 040 ----
mean loss: 193.47
 ---- batch: 050 ----
mean loss: 195.57
 ---- batch: 060 ----
mean loss: 193.44
 ---- batch: 070 ----
mean loss: 186.31
 ---- batch: 080 ----
mean loss: 198.60
 ---- batch: 090 ----
mean loss: 193.93
 ---- batch: 100 ----
mean loss: 186.14
 ---- batch: 110 ----
mean loss: 190.98
train mean loss: 194.41
epoch train time: 0:00:00.735383
elapsed time: 0:02:50.034817
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:32:06.363877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.36
 ---- batch: 020 ----
mean loss: 192.50
 ---- batch: 030 ----
mean loss: 201.30
 ---- batch: 040 ----
mean loss: 191.22
 ---- batch: 050 ----
mean loss: 186.71
 ---- batch: 060 ----
mean loss: 201.45
 ---- batch: 070 ----
mean loss: 201.85
 ---- batch: 080 ----
mean loss: 191.33
 ---- batch: 090 ----
mean loss: 189.84
 ---- batch: 100 ----
mean loss: 195.23
 ---- batch: 110 ----
mean loss: 200.60
train mean loss: 194.44
epoch train time: 0:00:00.729892
elapsed time: 0:02:50.764851
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:32:07.093911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.28
 ---- batch: 020 ----
mean loss: 188.93
 ---- batch: 030 ----
mean loss: 192.65
 ---- batch: 040 ----
mean loss: 194.92
 ---- batch: 050 ----
mean loss: 202.96
 ---- batch: 060 ----
mean loss: 197.43
 ---- batch: 070 ----
mean loss: 188.74
 ---- batch: 080 ----
mean loss: 187.36
 ---- batch: 090 ----
mean loss: 200.50
 ---- batch: 100 ----
mean loss: 198.57
 ---- batch: 110 ----
mean loss: 190.62
train mean loss: 194.16
epoch train time: 0:00:00.731412
elapsed time: 0:02:51.496418
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:32:07.825499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.00
 ---- batch: 020 ----
mean loss: 191.42
 ---- batch: 030 ----
mean loss: 190.39
 ---- batch: 040 ----
mean loss: 193.16
 ---- batch: 050 ----
mean loss: 199.20
 ---- batch: 060 ----
mean loss: 201.93
 ---- batch: 070 ----
mean loss: 188.23
 ---- batch: 080 ----
mean loss: 194.47
 ---- batch: 090 ----
mean loss: 188.80
 ---- batch: 100 ----
mean loss: 191.43
 ---- batch: 110 ----
mean loss: 194.98
train mean loss: 194.10
epoch train time: 0:00:00.732523
elapsed time: 0:02:52.229105
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:32:08.558168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.48
 ---- batch: 020 ----
mean loss: 196.23
 ---- batch: 030 ----
mean loss: 192.72
 ---- batch: 040 ----
mean loss: 197.06
 ---- batch: 050 ----
mean loss: 198.03
 ---- batch: 060 ----
mean loss: 186.96
 ---- batch: 070 ----
mean loss: 187.32
 ---- batch: 080 ----
mean loss: 192.27
 ---- batch: 090 ----
mean loss: 186.98
 ---- batch: 100 ----
mean loss: 199.91
 ---- batch: 110 ----
mean loss: 202.70
train mean loss: 193.93
epoch train time: 0:00:00.737742
elapsed time: 0:02:52.966996
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:32:09.296059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.10
 ---- batch: 020 ----
mean loss: 202.78
 ---- batch: 030 ----
mean loss: 196.43
 ---- batch: 040 ----
mean loss: 191.21
 ---- batch: 050 ----
mean loss: 186.62
 ---- batch: 060 ----
mean loss: 190.56
 ---- batch: 070 ----
mean loss: 203.34
 ---- batch: 080 ----
mean loss: 190.47
 ---- batch: 090 ----
mean loss: 195.48
 ---- batch: 100 ----
mean loss: 196.66
 ---- batch: 110 ----
mean loss: 186.67
train mean loss: 193.78
epoch train time: 0:00:00.740522
elapsed time: 0:02:53.707660
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:32:10.036720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.86
 ---- batch: 020 ----
mean loss: 199.51
 ---- batch: 030 ----
mean loss: 202.13
 ---- batch: 040 ----
mean loss: 193.36
 ---- batch: 050 ----
mean loss: 186.14
 ---- batch: 060 ----
mean loss: 188.17
 ---- batch: 070 ----
mean loss: 199.42
 ---- batch: 080 ----
mean loss: 191.41
 ---- batch: 090 ----
mean loss: 195.52
 ---- batch: 100 ----
mean loss: 191.43
 ---- batch: 110 ----
mean loss: 200.12
train mean loss: 193.76
epoch train time: 0:00:00.734244
elapsed time: 0:02:54.442062
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:32:10.771124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.56
 ---- batch: 020 ----
mean loss: 196.60
 ---- batch: 030 ----
mean loss: 188.84
 ---- batch: 040 ----
mean loss: 204.50
 ---- batch: 050 ----
mean loss: 190.61
 ---- batch: 060 ----
mean loss: 189.56
 ---- batch: 070 ----
mean loss: 193.35
 ---- batch: 080 ----
mean loss: 192.12
 ---- batch: 090 ----
mean loss: 192.51
 ---- batch: 100 ----
mean loss: 180.73
 ---- batch: 110 ----
mean loss: 203.38
train mean loss: 193.52
epoch train time: 0:00:00.727313
elapsed time: 0:02:55.169520
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:32:11.498578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.15
 ---- batch: 020 ----
mean loss: 193.78
 ---- batch: 030 ----
mean loss: 196.15
 ---- batch: 040 ----
mean loss: 188.68
 ---- batch: 050 ----
mean loss: 195.97
 ---- batch: 060 ----
mean loss: 188.13
 ---- batch: 070 ----
mean loss: 207.91
 ---- batch: 080 ----
mean loss: 199.03
 ---- batch: 090 ----
mean loss: 191.60
 ---- batch: 100 ----
mean loss: 192.91
 ---- batch: 110 ----
mean loss: 186.35
train mean loss: 193.54
epoch train time: 0:00:00.723170
elapsed time: 0:02:55.892842
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:32:12.221932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.67
 ---- batch: 020 ----
mean loss: 189.36
 ---- batch: 030 ----
mean loss: 195.51
 ---- batch: 040 ----
mean loss: 194.60
 ---- batch: 050 ----
mean loss: 191.66
 ---- batch: 060 ----
mean loss: 198.76
 ---- batch: 070 ----
mean loss: 186.74
 ---- batch: 080 ----
mean loss: 189.00
 ---- batch: 090 ----
mean loss: 191.05
 ---- batch: 100 ----
mean loss: 185.17
 ---- batch: 110 ----
mean loss: 200.51
train mean loss: 193.47
epoch train time: 0:00:00.726515
elapsed time: 0:02:56.619529
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:32:12.948589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.79
 ---- batch: 020 ----
mean loss: 182.15
 ---- batch: 030 ----
mean loss: 202.50
 ---- batch: 040 ----
mean loss: 183.76
 ---- batch: 050 ----
mean loss: 196.07
 ---- batch: 060 ----
mean loss: 203.89
 ---- batch: 070 ----
mean loss: 194.10
 ---- batch: 080 ----
mean loss: 195.59
 ---- batch: 090 ----
mean loss: 183.03
 ---- batch: 100 ----
mean loss: 191.82
 ---- batch: 110 ----
mean loss: 196.79
train mean loss: 193.29
epoch train time: 0:00:00.733485
elapsed time: 0:02:57.353156
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:32:13.682215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.56
 ---- batch: 020 ----
mean loss: 187.29
 ---- batch: 030 ----
mean loss: 190.50
 ---- batch: 040 ----
mean loss: 186.28
 ---- batch: 050 ----
mean loss: 195.31
 ---- batch: 060 ----
mean loss: 196.35
 ---- batch: 070 ----
mean loss: 196.17
 ---- batch: 080 ----
mean loss: 184.26
 ---- batch: 090 ----
mean loss: 192.10
 ---- batch: 100 ----
mean loss: 194.66
 ---- batch: 110 ----
mean loss: 206.21
train mean loss: 193.25
epoch train time: 0:00:00.725292
elapsed time: 0:02:58.078587
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:32:14.407648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.58
 ---- batch: 020 ----
mean loss: 189.99
 ---- batch: 030 ----
mean loss: 201.24
 ---- batch: 040 ----
mean loss: 194.24
 ---- batch: 050 ----
mean loss: 187.62
 ---- batch: 060 ----
mean loss: 194.40
 ---- batch: 070 ----
mean loss: 187.61
 ---- batch: 080 ----
mean loss: 193.57
 ---- batch: 090 ----
mean loss: 194.40
 ---- batch: 100 ----
mean loss: 190.42
 ---- batch: 110 ----
mean loss: 188.96
train mean loss: 192.69
epoch train time: 0:00:00.733835
elapsed time: 0:02:58.812580
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:32:15.141656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.34
 ---- batch: 020 ----
mean loss: 189.18
 ---- batch: 030 ----
mean loss: 183.82
 ---- batch: 040 ----
mean loss: 195.57
 ---- batch: 050 ----
mean loss: 191.24
 ---- batch: 060 ----
mean loss: 195.91
 ---- batch: 070 ----
mean loss: 186.71
 ---- batch: 080 ----
mean loss: 201.12
 ---- batch: 090 ----
mean loss: 199.21
 ---- batch: 100 ----
mean loss: 187.46
 ---- batch: 110 ----
mean loss: 185.81
train mean loss: 192.69
epoch train time: 0:00:00.729301
elapsed time: 0:02:59.542047
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:32:15.871108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.17
 ---- batch: 020 ----
mean loss: 196.60
 ---- batch: 030 ----
mean loss: 198.39
 ---- batch: 040 ----
mean loss: 192.29
 ---- batch: 050 ----
mean loss: 195.35
 ---- batch: 060 ----
mean loss: 191.70
 ---- batch: 070 ----
mean loss: 186.75
 ---- batch: 080 ----
mean loss: 197.75
 ---- batch: 090 ----
mean loss: 192.40
 ---- batch: 100 ----
mean loss: 188.63
 ---- batch: 110 ----
mean loss: 187.94
train mean loss: 192.61
epoch train time: 0:00:00.738975
elapsed time: 0:03:00.281168
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:32:16.610230
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.16
 ---- batch: 020 ----
mean loss: 191.53
 ---- batch: 030 ----
mean loss: 191.64
 ---- batch: 040 ----
mean loss: 192.20
 ---- batch: 050 ----
mean loss: 196.27
 ---- batch: 060 ----
mean loss: 191.49
 ---- batch: 070 ----
mean loss: 197.35
 ---- batch: 080 ----
mean loss: 195.18
 ---- batch: 090 ----
mean loss: 193.65
 ---- batch: 100 ----
mean loss: 186.27
 ---- batch: 110 ----
mean loss: 191.50
train mean loss: 192.58
epoch train time: 0:00:00.734875
elapsed time: 0:03:01.016187
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:32:17.345268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.75
 ---- batch: 020 ----
mean loss: 187.73
 ---- batch: 030 ----
mean loss: 190.47
 ---- batch: 040 ----
mean loss: 189.72
 ---- batch: 050 ----
mean loss: 194.84
 ---- batch: 060 ----
mean loss: 197.91
 ---- batch: 070 ----
mean loss: 194.45
 ---- batch: 080 ----
mean loss: 196.17
 ---- batch: 090 ----
mean loss: 187.41
 ---- batch: 100 ----
mean loss: 185.99
 ---- batch: 110 ----
mean loss: 199.36
train mean loss: 192.60
epoch train time: 0:00:00.731334
elapsed time: 0:03:01.747689
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:32:18.076750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.32
 ---- batch: 020 ----
mean loss: 191.80
 ---- batch: 030 ----
mean loss: 193.34
 ---- batch: 040 ----
mean loss: 198.19
 ---- batch: 050 ----
mean loss: 187.67
 ---- batch: 060 ----
mean loss: 196.25
 ---- batch: 070 ----
mean loss: 192.26
 ---- batch: 080 ----
mean loss: 193.98
 ---- batch: 090 ----
mean loss: 192.88
 ---- batch: 100 ----
mean loss: 181.26
 ---- batch: 110 ----
mean loss: 199.14
train mean loss: 192.64
epoch train time: 0:00:00.732376
elapsed time: 0:03:02.480209
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:32:18.809268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.99
 ---- batch: 020 ----
mean loss: 194.70
 ---- batch: 030 ----
mean loss: 194.04
 ---- batch: 040 ----
mean loss: 205.20
 ---- batch: 050 ----
mean loss: 181.78
 ---- batch: 060 ----
mean loss: 196.30
 ---- batch: 070 ----
mean loss: 183.55
 ---- batch: 080 ----
mean loss: 195.17
 ---- batch: 090 ----
mean loss: 189.57
 ---- batch: 100 ----
mean loss: 203.24
 ---- batch: 110 ----
mean loss: 187.57
train mean loss: 192.63
epoch train time: 0:00:00.732162
elapsed time: 0:03:03.212529
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:32:19.541591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.51
 ---- batch: 020 ----
mean loss: 190.97
 ---- batch: 030 ----
mean loss: 187.53
 ---- batch: 040 ----
mean loss: 190.38
 ---- batch: 050 ----
mean loss: 194.02
 ---- batch: 060 ----
mean loss: 201.20
 ---- batch: 070 ----
mean loss: 197.81
 ---- batch: 080 ----
mean loss: 201.12
 ---- batch: 090 ----
mean loss: 188.30
 ---- batch: 100 ----
mean loss: 187.45
 ---- batch: 110 ----
mean loss: 195.19
train mean loss: 192.66
epoch train time: 0:00:00.737177
elapsed time: 0:03:03.949851
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:32:20.278913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.15
 ---- batch: 020 ----
mean loss: 178.97
 ---- batch: 030 ----
mean loss: 195.12
 ---- batch: 040 ----
mean loss: 189.72
 ---- batch: 050 ----
mean loss: 187.48
 ---- batch: 060 ----
mean loss: 196.51
 ---- batch: 070 ----
mean loss: 195.24
 ---- batch: 080 ----
mean loss: 188.08
 ---- batch: 090 ----
mean loss: 192.54
 ---- batch: 100 ----
mean loss: 191.46
 ---- batch: 110 ----
mean loss: 209.14
train mean loss: 192.57
epoch train time: 0:00:00.728018
elapsed time: 0:03:04.678020
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:32:21.007080
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.24
 ---- batch: 020 ----
mean loss: 190.73
 ---- batch: 030 ----
mean loss: 194.10
 ---- batch: 040 ----
mean loss: 198.83
 ---- batch: 050 ----
mean loss: 196.58
 ---- batch: 060 ----
mean loss: 195.59
 ---- batch: 070 ----
mean loss: 187.65
 ---- batch: 080 ----
mean loss: 186.58
 ---- batch: 090 ----
mean loss: 185.07
 ---- batch: 100 ----
mean loss: 184.10
 ---- batch: 110 ----
mean loss: 191.64
train mean loss: 192.56
epoch train time: 0:00:00.725096
elapsed time: 0:03:05.403261
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:32:21.732325
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.71
 ---- batch: 020 ----
mean loss: 199.87
 ---- batch: 030 ----
mean loss: 193.31
 ---- batch: 040 ----
mean loss: 198.26
 ---- batch: 050 ----
mean loss: 196.94
 ---- batch: 060 ----
mean loss: 192.73
 ---- batch: 070 ----
mean loss: 185.17
 ---- batch: 080 ----
mean loss: 187.46
 ---- batch: 090 ----
mean loss: 197.90
 ---- batch: 100 ----
mean loss: 183.35
 ---- batch: 110 ----
mean loss: 192.51
train mean loss: 192.55
epoch train time: 0:00:00.738256
elapsed time: 0:03:06.141679
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:32:22.470761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.61
 ---- batch: 020 ----
mean loss: 189.13
 ---- batch: 030 ----
mean loss: 191.85
 ---- batch: 040 ----
mean loss: 196.26
 ---- batch: 050 ----
mean loss: 195.62
 ---- batch: 060 ----
mean loss: 195.09
 ---- batch: 070 ----
mean loss: 193.12
 ---- batch: 080 ----
mean loss: 188.48
 ---- batch: 090 ----
mean loss: 192.00
 ---- batch: 100 ----
mean loss: 194.59
 ---- batch: 110 ----
mean loss: 193.03
train mean loss: 192.50
epoch train time: 0:00:00.736606
elapsed time: 0:03:06.878459
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:32:23.207521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.20
 ---- batch: 020 ----
mean loss: 190.77
 ---- batch: 030 ----
mean loss: 183.51
 ---- batch: 040 ----
mean loss: 199.18
 ---- batch: 050 ----
mean loss: 194.03
 ---- batch: 060 ----
mean loss: 199.98
 ---- batch: 070 ----
mean loss: 186.95
 ---- batch: 080 ----
mean loss: 191.18
 ---- batch: 090 ----
mean loss: 181.89
 ---- batch: 100 ----
mean loss: 198.76
 ---- batch: 110 ----
mean loss: 198.43
train mean loss: 192.48
epoch train time: 0:00:00.733590
elapsed time: 0:03:07.612207
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:32:23.941269
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.08
 ---- batch: 020 ----
mean loss: 191.78
 ---- batch: 030 ----
mean loss: 192.92
 ---- batch: 040 ----
mean loss: 185.12
 ---- batch: 050 ----
mean loss: 197.26
 ---- batch: 060 ----
mean loss: 189.93
 ---- batch: 070 ----
mean loss: 198.77
 ---- batch: 080 ----
mean loss: 193.51
 ---- batch: 090 ----
mean loss: 189.36
 ---- batch: 100 ----
mean loss: 192.39
 ---- batch: 110 ----
mean loss: 195.49
train mean loss: 192.57
epoch train time: 0:00:00.742671
elapsed time: 0:03:08.355023
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:32:24.684089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.48
 ---- batch: 020 ----
mean loss: 195.17
 ---- batch: 030 ----
mean loss: 192.92
 ---- batch: 040 ----
mean loss: 199.72
 ---- batch: 050 ----
mean loss: 189.97
 ---- batch: 060 ----
mean loss: 186.33
 ---- batch: 070 ----
mean loss: 199.73
 ---- batch: 080 ----
mean loss: 185.76
 ---- batch: 090 ----
mean loss: 179.91
 ---- batch: 100 ----
mean loss: 189.86
 ---- batch: 110 ----
mean loss: 195.16
train mean loss: 192.51
epoch train time: 0:00:00.730393
elapsed time: 0:03:09.085571
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:32:25.414634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.81
 ---- batch: 020 ----
mean loss: 193.73
 ---- batch: 030 ----
mean loss: 195.22
 ---- batch: 040 ----
mean loss: 192.72
 ---- batch: 050 ----
mean loss: 187.57
 ---- batch: 060 ----
mean loss: 189.19
 ---- batch: 070 ----
mean loss: 185.26
 ---- batch: 080 ----
mean loss: 196.26
 ---- batch: 090 ----
mean loss: 192.92
 ---- batch: 100 ----
mean loss: 198.41
 ---- batch: 110 ----
mean loss: 193.61
train mean loss: 192.52
epoch train time: 0:00:00.725472
elapsed time: 0:03:09.811200
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:32:26.140276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.10
 ---- batch: 020 ----
mean loss: 188.36
 ---- batch: 030 ----
mean loss: 192.20
 ---- batch: 040 ----
mean loss: 194.97
 ---- batch: 050 ----
mean loss: 190.35
 ---- batch: 060 ----
mean loss: 194.36
 ---- batch: 070 ----
mean loss: 191.91
 ---- batch: 080 ----
mean loss: 197.92
 ---- batch: 090 ----
mean loss: 191.02
 ---- batch: 100 ----
mean loss: 198.71
 ---- batch: 110 ----
mean loss: 192.09
train mean loss: 192.53
epoch train time: 0:00:00.726078
elapsed time: 0:03:10.537439
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:32:26.866523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.70
 ---- batch: 020 ----
mean loss: 198.16
 ---- batch: 030 ----
mean loss: 200.52
 ---- batch: 040 ----
mean loss: 194.43
 ---- batch: 050 ----
mean loss: 191.13
 ---- batch: 060 ----
mean loss: 194.75
 ---- batch: 070 ----
mean loss: 190.64
 ---- batch: 080 ----
mean loss: 182.04
 ---- batch: 090 ----
mean loss: 190.30
 ---- batch: 100 ----
mean loss: 194.30
 ---- batch: 110 ----
mean loss: 190.90
train mean loss: 192.45
epoch train time: 0:00:00.736823
elapsed time: 0:03:11.274463
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:32:27.603555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.95
 ---- batch: 020 ----
mean loss: 195.50
 ---- batch: 030 ----
mean loss: 191.02
 ---- batch: 040 ----
mean loss: 190.06
 ---- batch: 050 ----
mean loss: 190.44
 ---- batch: 060 ----
mean loss: 186.96
 ---- batch: 070 ----
mean loss: 191.03
 ---- batch: 080 ----
mean loss: 199.94
 ---- batch: 090 ----
mean loss: 195.82
 ---- batch: 100 ----
mean loss: 193.81
 ---- batch: 110 ----
mean loss: 194.18
train mean loss: 192.47
epoch train time: 0:00:00.727790
elapsed time: 0:03:12.002443
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:32:28.331504
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.68
 ---- batch: 020 ----
mean loss: 195.96
 ---- batch: 030 ----
mean loss: 195.83
 ---- batch: 040 ----
mean loss: 192.93
 ---- batch: 050 ----
mean loss: 201.17
 ---- batch: 060 ----
mean loss: 187.54
 ---- batch: 070 ----
mean loss: 190.13
 ---- batch: 080 ----
mean loss: 194.87
 ---- batch: 090 ----
mean loss: 191.04
 ---- batch: 100 ----
mean loss: 194.46
 ---- batch: 110 ----
mean loss: 191.02
train mean loss: 192.42
epoch train time: 0:00:00.726393
elapsed time: 0:03:12.728986
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:32:29.058045
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.72
 ---- batch: 020 ----
mean loss: 188.11
 ---- batch: 030 ----
mean loss: 195.08
 ---- batch: 040 ----
mean loss: 200.78
 ---- batch: 050 ----
mean loss: 185.78
 ---- batch: 060 ----
mean loss: 188.25
 ---- batch: 070 ----
mean loss: 197.95
 ---- batch: 080 ----
mean loss: 198.80
 ---- batch: 090 ----
mean loss: 190.23
 ---- batch: 100 ----
mean loss: 190.44
 ---- batch: 110 ----
mean loss: 195.52
train mean loss: 192.41
epoch train time: 0:00:00.726296
elapsed time: 0:03:13.455431
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:32:29.784492
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.69
 ---- batch: 020 ----
mean loss: 197.24
 ---- batch: 030 ----
mean loss: 201.77
 ---- batch: 040 ----
mean loss: 187.72
 ---- batch: 050 ----
mean loss: 191.15
 ---- batch: 060 ----
mean loss: 188.69
 ---- batch: 070 ----
mean loss: 194.94
 ---- batch: 080 ----
mean loss: 187.50
 ---- batch: 090 ----
mean loss: 185.51
 ---- batch: 100 ----
mean loss: 193.87
 ---- batch: 110 ----
mean loss: 188.23
train mean loss: 192.47
epoch train time: 0:00:00.849310
elapsed time: 0:03:14.304911
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:32:30.633977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.47
 ---- batch: 020 ----
mean loss: 190.59
 ---- batch: 030 ----
mean loss: 186.75
 ---- batch: 040 ----
mean loss: 194.66
 ---- batch: 050 ----
mean loss: 196.37
 ---- batch: 060 ----
mean loss: 191.53
 ---- batch: 070 ----
mean loss: 189.39
 ---- batch: 080 ----
mean loss: 193.35
 ---- batch: 090 ----
mean loss: 194.63
 ---- batch: 100 ----
mean loss: 195.74
 ---- batch: 110 ----
mean loss: 191.76
train mean loss: 192.41
epoch train time: 0:00:00.763611
elapsed time: 0:03:15.068694
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:32:31.397754
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.05
 ---- batch: 020 ----
mean loss: 193.06
 ---- batch: 030 ----
mean loss: 193.01
 ---- batch: 040 ----
mean loss: 194.64
 ---- batch: 050 ----
mean loss: 191.66
 ---- batch: 060 ----
mean loss: 202.79
 ---- batch: 070 ----
mean loss: 191.63
 ---- batch: 080 ----
mean loss: 195.72
 ---- batch: 090 ----
mean loss: 196.29
 ---- batch: 100 ----
mean loss: 179.92
 ---- batch: 110 ----
mean loss: 189.41
train mean loss: 192.43
epoch train time: 0:00:00.730552
elapsed time: 0:03:15.799385
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:32:32.128463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.38
 ---- batch: 020 ----
mean loss: 185.84
 ---- batch: 030 ----
mean loss: 196.68
 ---- batch: 040 ----
mean loss: 199.09
 ---- batch: 050 ----
mean loss: 182.26
 ---- batch: 060 ----
mean loss: 195.19
 ---- batch: 070 ----
mean loss: 188.45
 ---- batch: 080 ----
mean loss: 186.94
 ---- batch: 090 ----
mean loss: 196.86
 ---- batch: 100 ----
mean loss: 198.88
 ---- batch: 110 ----
mean loss: 190.37
train mean loss: 192.45
epoch train time: 0:00:00.730562
elapsed time: 0:03:16.530107
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:32:32.859172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.03
 ---- batch: 020 ----
mean loss: 197.50
 ---- batch: 030 ----
mean loss: 192.99
 ---- batch: 040 ----
mean loss: 189.85
 ---- batch: 050 ----
mean loss: 191.37
 ---- batch: 060 ----
mean loss: 180.17
 ---- batch: 070 ----
mean loss: 198.46
 ---- batch: 080 ----
mean loss: 186.29
 ---- batch: 090 ----
mean loss: 199.76
 ---- batch: 100 ----
mean loss: 196.03
 ---- batch: 110 ----
mean loss: 197.11
train mean loss: 192.35
epoch train time: 0:00:00.735644
elapsed time: 0:03:17.265907
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:32:33.594969
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.44
 ---- batch: 020 ----
mean loss: 192.72
 ---- batch: 030 ----
mean loss: 192.31
 ---- batch: 040 ----
mean loss: 189.89
 ---- batch: 050 ----
mean loss: 188.52
 ---- batch: 060 ----
mean loss: 195.66
 ---- batch: 070 ----
mean loss: 200.62
 ---- batch: 080 ----
mean loss: 200.12
 ---- batch: 090 ----
mean loss: 194.57
 ---- batch: 100 ----
mean loss: 194.10
 ---- batch: 110 ----
mean loss: 188.35
train mean loss: 192.36
epoch train time: 0:00:00.727779
elapsed time: 0:03:17.993831
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:32:34.322891
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.38
 ---- batch: 020 ----
mean loss: 188.71
 ---- batch: 030 ----
mean loss: 188.60
 ---- batch: 040 ----
mean loss: 195.61
 ---- batch: 050 ----
mean loss: 195.64
 ---- batch: 060 ----
mean loss: 194.89
 ---- batch: 070 ----
mean loss: 201.06
 ---- batch: 080 ----
mean loss: 190.28
 ---- batch: 090 ----
mean loss: 184.73
 ---- batch: 100 ----
mean loss: 190.54
 ---- batch: 110 ----
mean loss: 193.71
train mean loss: 192.39
epoch train time: 0:00:00.734432
elapsed time: 0:03:18.728405
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:32:35.057481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.04
 ---- batch: 020 ----
mean loss: 199.87
 ---- batch: 030 ----
mean loss: 197.07
 ---- batch: 040 ----
mean loss: 195.18
 ---- batch: 050 ----
mean loss: 190.53
 ---- batch: 060 ----
mean loss: 194.34
 ---- batch: 070 ----
mean loss: 192.62
 ---- batch: 080 ----
mean loss: 182.54
 ---- batch: 090 ----
mean loss: 195.72
 ---- batch: 100 ----
mean loss: 197.64
 ---- batch: 110 ----
mean loss: 188.57
train mean loss: 192.32
epoch train time: 0:00:00.742490
elapsed time: 0:03:19.471067
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:32:35.800135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.53
 ---- batch: 020 ----
mean loss: 188.70
 ---- batch: 030 ----
mean loss: 188.53
 ---- batch: 040 ----
mean loss: 205.18
 ---- batch: 050 ----
mean loss: 185.88
 ---- batch: 060 ----
mean loss: 192.99
 ---- batch: 070 ----
mean loss: 193.40
 ---- batch: 080 ----
mean loss: 194.38
 ---- batch: 090 ----
mean loss: 187.44
 ---- batch: 100 ----
mean loss: 187.98
 ---- batch: 110 ----
mean loss: 195.69
train mean loss: 192.41
epoch train time: 0:00:00.730951
elapsed time: 0:03:20.202169
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:32:36.531247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.52
 ---- batch: 020 ----
mean loss: 182.59
 ---- batch: 030 ----
mean loss: 185.19
 ---- batch: 040 ----
mean loss: 201.92
 ---- batch: 050 ----
mean loss: 197.84
 ---- batch: 060 ----
mean loss: 184.55
 ---- batch: 070 ----
mean loss: 191.92
 ---- batch: 080 ----
mean loss: 201.19
 ---- batch: 090 ----
mean loss: 191.03
 ---- batch: 100 ----
mean loss: 199.49
 ---- batch: 110 ----
mean loss: 192.89
train mean loss: 192.31
epoch train time: 0:00:00.735618
elapsed time: 0:03:20.937948
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:32:37.267022
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.33
 ---- batch: 020 ----
mean loss: 188.18
 ---- batch: 030 ----
mean loss: 199.10
 ---- batch: 040 ----
mean loss: 196.82
 ---- batch: 050 ----
mean loss: 181.23
 ---- batch: 060 ----
mean loss: 198.68
 ---- batch: 070 ----
mean loss: 194.37
 ---- batch: 080 ----
mean loss: 196.02
 ---- batch: 090 ----
mean loss: 192.83
 ---- batch: 100 ----
mean loss: 187.05
 ---- batch: 110 ----
mean loss: 191.94
train mean loss: 192.27
epoch train time: 0:00:00.743132
elapsed time: 0:03:21.681251
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:32:38.010328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.97
 ---- batch: 020 ----
mean loss: 188.82
 ---- batch: 030 ----
mean loss: 188.83
 ---- batch: 040 ----
mean loss: 195.51
 ---- batch: 050 ----
mean loss: 193.48
 ---- batch: 060 ----
mean loss: 199.48
 ---- batch: 070 ----
mean loss: 194.44
 ---- batch: 080 ----
mean loss: 191.13
 ---- batch: 090 ----
mean loss: 189.26
 ---- batch: 100 ----
mean loss: 192.66
 ---- batch: 110 ----
mean loss: 190.60
train mean loss: 192.26
epoch train time: 0:00:00.730726
elapsed time: 0:03:22.412148
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:32:38.741204
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.10
 ---- batch: 020 ----
mean loss: 188.62
 ---- batch: 030 ----
mean loss: 183.58
 ---- batch: 040 ----
mean loss: 194.58
 ---- batch: 050 ----
mean loss: 187.45
 ---- batch: 060 ----
mean loss: 194.00
 ---- batch: 070 ----
mean loss: 192.64
 ---- batch: 080 ----
mean loss: 202.98
 ---- batch: 090 ----
mean loss: 196.57
 ---- batch: 100 ----
mean loss: 192.28
 ---- batch: 110 ----
mean loss: 188.02
train mean loss: 192.28
epoch train time: 0:00:00.730103
elapsed time: 0:03:23.142386
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:32:39.471448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.95
 ---- batch: 020 ----
mean loss: 185.89
 ---- batch: 030 ----
mean loss: 188.61
 ---- batch: 040 ----
mean loss: 199.84
 ---- batch: 050 ----
mean loss: 189.10
 ---- batch: 060 ----
mean loss: 198.31
 ---- batch: 070 ----
mean loss: 196.00
 ---- batch: 080 ----
mean loss: 184.45
 ---- batch: 090 ----
mean loss: 190.89
 ---- batch: 100 ----
mean loss: 193.68
 ---- batch: 110 ----
mean loss: 188.87
train mean loss: 192.28
epoch train time: 0:00:00.731254
elapsed time: 0:03:23.873787
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:32:40.202849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.53
 ---- batch: 020 ----
mean loss: 194.48
 ---- batch: 030 ----
mean loss: 192.07
 ---- batch: 040 ----
mean loss: 193.47
 ---- batch: 050 ----
mean loss: 191.92
 ---- batch: 060 ----
mean loss: 201.29
 ---- batch: 070 ----
mean loss: 197.64
 ---- batch: 080 ----
mean loss: 188.82
 ---- batch: 090 ----
mean loss: 189.14
 ---- batch: 100 ----
mean loss: 188.35
 ---- batch: 110 ----
mean loss: 186.19
train mean loss: 192.27
epoch train time: 0:00:00.741561
elapsed time: 0:03:24.615492
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:32:40.944552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.33
 ---- batch: 020 ----
mean loss: 203.27
 ---- batch: 030 ----
mean loss: 190.70
 ---- batch: 040 ----
mean loss: 191.10
 ---- batch: 050 ----
mean loss: 191.63
 ---- batch: 060 ----
mean loss: 193.98
 ---- batch: 070 ----
mean loss: 181.90
 ---- batch: 080 ----
mean loss: 189.18
 ---- batch: 090 ----
mean loss: 197.49
 ---- batch: 100 ----
mean loss: 194.88
 ---- batch: 110 ----
mean loss: 185.64
train mean loss: 192.24
epoch train time: 0:00:00.730610
elapsed time: 0:03:25.346240
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:32:41.675336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.15
 ---- batch: 020 ----
mean loss: 194.84
 ---- batch: 030 ----
mean loss: 189.41
 ---- batch: 040 ----
mean loss: 192.72
 ---- batch: 050 ----
mean loss: 191.95
 ---- batch: 060 ----
mean loss: 193.27
 ---- batch: 070 ----
mean loss: 204.45
 ---- batch: 080 ----
mean loss: 186.90
 ---- batch: 090 ----
mean loss: 183.16
 ---- batch: 100 ----
mean loss: 199.72
 ---- batch: 110 ----
mean loss: 187.83
train mean loss: 192.22
epoch train time: 0:00:00.736594
elapsed time: 0:03:26.083011
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:32:42.412071
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.40
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 193.95
 ---- batch: 040 ----
mean loss: 195.49
 ---- batch: 050 ----
mean loss: 186.65
 ---- batch: 060 ----
mean loss: 200.38
 ---- batch: 070 ----
mean loss: 195.94
 ---- batch: 080 ----
mean loss: 199.25
 ---- batch: 090 ----
mean loss: 185.46
 ---- batch: 100 ----
mean loss: 190.58
 ---- batch: 110 ----
mean loss: 185.58
train mean loss: 192.24
epoch train time: 0:00:00.731092
elapsed time: 0:03:26.814248
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:32:43.143309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.81
 ---- batch: 020 ----
mean loss: 189.42
 ---- batch: 030 ----
mean loss: 196.66
 ---- batch: 040 ----
mean loss: 180.98
 ---- batch: 050 ----
mean loss: 190.82
 ---- batch: 060 ----
mean loss: 193.09
 ---- batch: 070 ----
mean loss: 194.80
 ---- batch: 080 ----
mean loss: 189.30
 ---- batch: 090 ----
mean loss: 203.17
 ---- batch: 100 ----
mean loss: 190.42
 ---- batch: 110 ----
mean loss: 197.12
train mean loss: 192.26
epoch train time: 0:00:00.746568
elapsed time: 0:03:27.560963
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:32:43.890023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.54
 ---- batch: 020 ----
mean loss: 191.44
 ---- batch: 030 ----
mean loss: 187.94
 ---- batch: 040 ----
mean loss: 190.45
 ---- batch: 050 ----
mean loss: 193.69
 ---- batch: 060 ----
mean loss: 199.15
 ---- batch: 070 ----
mean loss: 192.30
 ---- batch: 080 ----
mean loss: 193.58
 ---- batch: 090 ----
mean loss: 195.81
 ---- batch: 100 ----
mean loss: 201.15
 ---- batch: 110 ----
mean loss: 186.03
train mean loss: 192.19
epoch train time: 0:00:00.724594
elapsed time: 0:03:28.285701
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:32:44.614776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.34
 ---- batch: 020 ----
mean loss: 208.48
 ---- batch: 030 ----
mean loss: 199.19
 ---- batch: 040 ----
mean loss: 183.48
 ---- batch: 050 ----
mean loss: 189.15
 ---- batch: 060 ----
mean loss: 192.58
 ---- batch: 070 ----
mean loss: 193.65
 ---- batch: 080 ----
mean loss: 179.00
 ---- batch: 090 ----
mean loss: 189.56
 ---- batch: 100 ----
mean loss: 194.80
 ---- batch: 110 ----
mean loss: 191.67
train mean loss: 192.16
epoch train time: 0:00:00.738425
elapsed time: 0:03:29.024285
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:32:45.353348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.67
 ---- batch: 020 ----
mean loss: 200.56
 ---- batch: 030 ----
mean loss: 184.88
 ---- batch: 040 ----
mean loss: 202.33
 ---- batch: 050 ----
mean loss: 197.90
 ---- batch: 060 ----
mean loss: 188.52
 ---- batch: 070 ----
mean loss: 199.13
 ---- batch: 080 ----
mean loss: 182.60
 ---- batch: 090 ----
mean loss: 187.67
 ---- batch: 100 ----
mean loss: 197.73
 ---- batch: 110 ----
mean loss: 186.59
train mean loss: 192.16
epoch train time: 0:00:00.743824
elapsed time: 0:03:29.768256
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:32:46.097334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.10
 ---- batch: 020 ----
mean loss: 191.78
 ---- batch: 030 ----
mean loss: 190.93
 ---- batch: 040 ----
mean loss: 189.01
 ---- batch: 050 ----
mean loss: 191.44
 ---- batch: 060 ----
mean loss: 186.42
 ---- batch: 070 ----
mean loss: 196.32
 ---- batch: 080 ----
mean loss: 201.35
 ---- batch: 090 ----
mean loss: 192.23
 ---- batch: 100 ----
mean loss: 194.11
 ---- batch: 110 ----
mean loss: 198.53
train mean loss: 192.15
epoch train time: 0:00:00.729843
elapsed time: 0:03:30.498259
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:32:46.827351
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.38
 ---- batch: 020 ----
mean loss: 194.02
 ---- batch: 030 ----
mean loss: 194.84
 ---- batch: 040 ----
mean loss: 189.45
 ---- batch: 050 ----
mean loss: 188.32
 ---- batch: 060 ----
mean loss: 194.12
 ---- batch: 070 ----
mean loss: 198.58
 ---- batch: 080 ----
mean loss: 187.72
 ---- batch: 090 ----
mean loss: 190.93
 ---- batch: 100 ----
mean loss: 196.47
 ---- batch: 110 ----
mean loss: 195.18
train mean loss: 192.14
epoch train time: 0:00:00.731731
elapsed time: 0:03:31.230169
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:32:47.559231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.08
 ---- batch: 020 ----
mean loss: 190.93
 ---- batch: 030 ----
mean loss: 190.66
 ---- batch: 040 ----
mean loss: 188.00
 ---- batch: 050 ----
mean loss: 194.88
 ---- batch: 060 ----
mean loss: 194.02
 ---- batch: 070 ----
mean loss: 191.09
 ---- batch: 080 ----
mean loss: 189.21
 ---- batch: 090 ----
mean loss: 190.00
 ---- batch: 100 ----
mean loss: 192.00
 ---- batch: 110 ----
mean loss: 193.82
train mean loss: 192.20
epoch train time: 0:00:00.728951
elapsed time: 0:03:31.959268
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:32:48.288328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.40
 ---- batch: 020 ----
mean loss: 198.75
 ---- batch: 030 ----
mean loss: 196.09
 ---- batch: 040 ----
mean loss: 193.50
 ---- batch: 050 ----
mean loss: 190.09
 ---- batch: 060 ----
mean loss: 192.78
 ---- batch: 070 ----
mean loss: 191.74
 ---- batch: 080 ----
mean loss: 188.32
 ---- batch: 090 ----
mean loss: 193.17
 ---- batch: 100 ----
mean loss: 195.65
 ---- batch: 110 ----
mean loss: 198.85
train mean loss: 192.11
epoch train time: 0:00:00.730999
elapsed time: 0:03:32.690408
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:32:49.019469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.71
 ---- batch: 020 ----
mean loss: 191.98
 ---- batch: 030 ----
mean loss: 188.88
 ---- batch: 040 ----
mean loss: 191.99
 ---- batch: 050 ----
mean loss: 188.56
 ---- batch: 060 ----
mean loss: 189.78
 ---- batch: 070 ----
mean loss: 189.63
 ---- batch: 080 ----
mean loss: 193.43
 ---- batch: 090 ----
mean loss: 193.16
 ---- batch: 100 ----
mean loss: 201.15
 ---- batch: 110 ----
mean loss: 195.31
train mean loss: 192.06
epoch train time: 0:00:00.724389
elapsed time: 0:03:33.414934
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:32:49.743992
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.66
 ---- batch: 020 ----
mean loss: 192.14
 ---- batch: 030 ----
mean loss: 191.10
 ---- batch: 040 ----
mean loss: 193.31
 ---- batch: 050 ----
mean loss: 188.22
 ---- batch: 060 ----
mean loss: 196.21
 ---- batch: 070 ----
mean loss: 191.29
 ---- batch: 080 ----
mean loss: 193.21
 ---- batch: 090 ----
mean loss: 194.60
 ---- batch: 100 ----
mean loss: 197.25
 ---- batch: 110 ----
mean loss: 189.69
train mean loss: 192.07
epoch train time: 0:00:00.729956
elapsed time: 0:03:34.147181
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_1/checkpoint.pth.tar
**** end time: 2019-09-27 01:32:50.476208 ****
