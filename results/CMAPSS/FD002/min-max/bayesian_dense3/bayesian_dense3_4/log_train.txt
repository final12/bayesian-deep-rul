Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 20473
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 19:53:40.839850 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 19:53:40.849441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4296.09
 ---- batch: 020 ----
mean loss: 4102.86
 ---- batch: 030 ----
mean loss: 3949.33
 ---- batch: 040 ----
mean loss: 3670.32
 ---- batch: 050 ----
mean loss: 3378.73
 ---- batch: 060 ----
mean loss: 3277.84
 ---- batch: 070 ----
mean loss: 3056.22
 ---- batch: 080 ----
mean loss: 2960.19
 ---- batch: 090 ----
mean loss: 2843.19
train mean loss: 3455.97
epoch train time: 0:00:35.006561
elapsed time: 0:00:35.023061
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 19:54:15.862969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2633.88
 ---- batch: 020 ----
mean loss: 2623.06
 ---- batch: 030 ----
mean loss: 2540.68
 ---- batch: 040 ----
mean loss: 2493.22
 ---- batch: 050 ----
mean loss: 2391.75
 ---- batch: 060 ----
mean loss: 2397.72
 ---- batch: 070 ----
mean loss: 2354.18
 ---- batch: 080 ----
mean loss: 2292.85
 ---- batch: 090 ----
mean loss: 2285.91
train mean loss: 2433.45
epoch train time: 0:00:01.648998
elapsed time: 0:00:36.672454
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 19:54:17.512601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2197.81
 ---- batch: 020 ----
mean loss: 2158.33
 ---- batch: 030 ----
mean loss: 2164.05
 ---- batch: 040 ----
mean loss: 2142.53
 ---- batch: 050 ----
mean loss: 2124.98
 ---- batch: 060 ----
mean loss: 2087.64
 ---- batch: 070 ----
mean loss: 2090.69
 ---- batch: 080 ----
mean loss: 2046.25
 ---- batch: 090 ----
mean loss: 2003.81
train mean loss: 2108.59
epoch train time: 0:00:01.629320
elapsed time: 0:00:38.302489
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 19:54:19.142669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1987.40
 ---- batch: 020 ----
mean loss: 1937.12
 ---- batch: 030 ----
mean loss: 1912.79
 ---- batch: 040 ----
mean loss: 1939.72
 ---- batch: 050 ----
mean loss: 1896.77
 ---- batch: 060 ----
mean loss: 1888.40
 ---- batch: 070 ----
mean loss: 1839.22
 ---- batch: 080 ----
mean loss: 1822.60
 ---- batch: 090 ----
mean loss: 1831.00
train mean loss: 1888.32
epoch train time: 0:00:01.614435
elapsed time: 0:00:39.917595
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 19:54:20.757755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1769.57
 ---- batch: 020 ----
mean loss: 1753.97
 ---- batch: 030 ----
mean loss: 1730.64
 ---- batch: 040 ----
mean loss: 1705.63
 ---- batch: 050 ----
mean loss: 1724.97
 ---- batch: 060 ----
mean loss: 1665.30
 ---- batch: 070 ----
mean loss: 1665.14
 ---- batch: 080 ----
mean loss: 1680.36
 ---- batch: 090 ----
mean loss: 1633.32
train mean loss: 1697.95
epoch train time: 0:00:01.617628
elapsed time: 0:00:41.535940
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 19:54:22.376041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1590.97
 ---- batch: 020 ----
mean loss: 1565.80
 ---- batch: 030 ----
mean loss: 1587.54
 ---- batch: 040 ----
mean loss: 1556.69
 ---- batch: 050 ----
mean loss: 1585.75
 ---- batch: 060 ----
mean loss: 1499.07
 ---- batch: 070 ----
mean loss: 1528.94
 ---- batch: 080 ----
mean loss: 1541.83
 ---- batch: 090 ----
mean loss: 1480.56
train mean loss: 1542.94
epoch train time: 0:00:01.613215
elapsed time: 0:00:43.149838
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 19:54:23.989993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1462.55
 ---- batch: 020 ----
mean loss: 1468.80
 ---- batch: 030 ----
mean loss: 1406.95
 ---- batch: 040 ----
mean loss: 1438.84
 ---- batch: 050 ----
mean loss: 1412.93
 ---- batch: 060 ----
mean loss: 1412.05
 ---- batch: 070 ----
mean loss: 1387.33
 ---- batch: 080 ----
mean loss: 1377.50
 ---- batch: 090 ----
mean loss: 1380.08
train mean loss: 1410.57
epoch train time: 0:00:01.673195
elapsed time: 0:00:44.823692
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 19:54:25.663845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1326.74
 ---- batch: 020 ----
mean loss: 1343.68
 ---- batch: 030 ----
mean loss: 1349.83
 ---- batch: 040 ----
mean loss: 1287.70
 ---- batch: 050 ----
mean loss: 1320.03
 ---- batch: 060 ----
mean loss: 1306.94
 ---- batch: 070 ----
mean loss: 1269.61
 ---- batch: 080 ----
mean loss: 1280.90
 ---- batch: 090 ----
mean loss: 1253.03
train mean loss: 1299.72
epoch train time: 0:00:01.632884
elapsed time: 0:00:46.457205
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 19:54:27.297362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1253.97
 ---- batch: 020 ----
mean loss: 1220.10
 ---- batch: 030 ----
mean loss: 1226.39
 ---- batch: 040 ----
mean loss: 1214.04
 ---- batch: 050 ----
mean loss: 1235.37
 ---- batch: 060 ----
mean loss: 1190.02
 ---- batch: 070 ----
mean loss: 1206.86
 ---- batch: 080 ----
mean loss: 1172.01
 ---- batch: 090 ----
mean loss: 1201.45
train mean loss: 1211.84
epoch train time: 0:00:01.614706
elapsed time: 0:00:48.072598
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 19:54:28.912740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1174.90
 ---- batch: 020 ----
mean loss: 1154.96
 ---- batch: 030 ----
mean loss: 1162.23
 ---- batch: 040 ----
mean loss: 1118.94
 ---- batch: 050 ----
mean loss: 1150.32
 ---- batch: 060 ----
mean loss: 1135.21
 ---- batch: 070 ----
mean loss: 1147.16
 ---- batch: 080 ----
mean loss: 1107.69
 ---- batch: 090 ----
mean loss: 1120.26
train mean loss: 1138.24
epoch train time: 0:00:01.670140
elapsed time: 0:00:49.743346
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 19:54:30.583532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1079.21
 ---- batch: 020 ----
mean loss: 1103.13
 ---- batch: 030 ----
mean loss: 1100.98
 ---- batch: 040 ----
mean loss: 1075.14
 ---- batch: 050 ----
mean loss: 1070.23
 ---- batch: 060 ----
mean loss: 1086.38
 ---- batch: 070 ----
mean loss: 1070.99
 ---- batch: 080 ----
mean loss: 1045.68
 ---- batch: 090 ----
mean loss: 1069.57
train mean loss: 1076.58
epoch train time: 0:00:01.622825
elapsed time: 0:00:51.366901
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 19:54:32.207052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.93
 ---- batch: 020 ----
mean loss: 1044.76
 ---- batch: 030 ----
mean loss: 1054.01
 ---- batch: 040 ----
mean loss: 1048.46
 ---- batch: 050 ----
mean loss: 1037.62
 ---- batch: 060 ----
mean loss: 1033.99
 ---- batch: 070 ----
mean loss: 1032.97
 ---- batch: 080 ----
mean loss: 1011.26
 ---- batch: 090 ----
mean loss: 1000.31
train mean loss: 1029.38
epoch train time: 0:00:01.630971
elapsed time: 0:00:52.998564
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 19:54:33.838606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 997.04
 ---- batch: 020 ----
mean loss: 987.63
 ---- batch: 030 ----
mean loss: 1005.62
 ---- batch: 040 ----
mean loss: 985.74
 ---- batch: 050 ----
mean loss: 1017.23
 ---- batch: 060 ----
mean loss: 989.01
 ---- batch: 070 ----
mean loss: 973.97
 ---- batch: 080 ----
mean loss: 986.75
 ---- batch: 090 ----
mean loss: 985.37
train mean loss: 992.19
epoch train time: 0:00:01.654149
elapsed time: 0:00:54.653254
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 19:54:35.493409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.44
 ---- batch: 020 ----
mean loss: 969.03
 ---- batch: 030 ----
mean loss: 967.74
 ---- batch: 040 ----
mean loss: 943.64
 ---- batch: 050 ----
mean loss: 958.41
 ---- batch: 060 ----
mean loss: 958.71
 ---- batch: 070 ----
mean loss: 972.56
 ---- batch: 080 ----
mean loss: 957.98
 ---- batch: 090 ----
mean loss: 957.21
train mean loss: 961.48
epoch train time: 0:00:01.645824
elapsed time: 0:00:56.299721
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 19:54:37.139893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.08
 ---- batch: 020 ----
mean loss: 946.45
 ---- batch: 030 ----
mean loss: 943.98
 ---- batch: 040 ----
mean loss: 926.82
 ---- batch: 050 ----
mean loss: 938.02
 ---- batch: 060 ----
mean loss: 930.68
 ---- batch: 070 ----
mean loss: 929.71
 ---- batch: 080 ----
mean loss: 944.47
 ---- batch: 090 ----
mean loss: 939.71
train mean loss: 939.90
epoch train time: 0:00:01.661924
elapsed time: 0:00:57.962350
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 19:54:38.802510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.72
 ---- batch: 020 ----
mean loss: 927.16
 ---- batch: 030 ----
mean loss: 928.75
 ---- batch: 040 ----
mean loss: 935.20
 ---- batch: 050 ----
mean loss: 921.90
 ---- batch: 060 ----
mean loss: 918.08
 ---- batch: 070 ----
mean loss: 900.07
 ---- batch: 080 ----
mean loss: 920.52
 ---- batch: 090 ----
mean loss: 912.91
train mean loss: 922.54
epoch train time: 0:00:01.656351
elapsed time: 0:00:59.619411
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 19:54:40.459595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.04
 ---- batch: 020 ----
mean loss: 896.44
 ---- batch: 030 ----
mean loss: 916.43
 ---- batch: 040 ----
mean loss: 917.49
 ---- batch: 050 ----
mean loss: 895.03
 ---- batch: 060 ----
mean loss: 918.19
 ---- batch: 070 ----
mean loss: 924.16
 ---- batch: 080 ----
mean loss: 923.23
 ---- batch: 090 ----
mean loss: 892.63
train mean loss: 911.70
epoch train time: 0:00:01.647626
elapsed time: 0:01:01.267769
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 19:54:42.107998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.69
 ---- batch: 020 ----
mean loss: 896.24
 ---- batch: 030 ----
mean loss: 918.84
 ---- batch: 040 ----
mean loss: 913.57
 ---- batch: 050 ----
mean loss: 894.41
 ---- batch: 060 ----
mean loss: 888.15
 ---- batch: 070 ----
mean loss: 895.58
 ---- batch: 080 ----
mean loss: 887.66
 ---- batch: 090 ----
mean loss: 892.40
train mean loss: 900.70
epoch train time: 0:00:01.659212
elapsed time: 0:01:02.927687
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 19:54:43.767841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.94
 ---- batch: 020 ----
mean loss: 899.09
 ---- batch: 030 ----
mean loss: 891.36
 ---- batch: 040 ----
mean loss: 897.66
 ---- batch: 050 ----
mean loss: 895.68
 ---- batch: 060 ----
mean loss: 887.99
 ---- batch: 070 ----
mean loss: 876.86
 ---- batch: 080 ----
mean loss: 909.22
 ---- batch: 090 ----
mean loss: 900.05
train mean loss: 894.78
epoch train time: 0:00:01.663175
elapsed time: 0:01:04.591484
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 19:54:45.431634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.57
 ---- batch: 020 ----
mean loss: 906.81
 ---- batch: 030 ----
mean loss: 888.75
 ---- batch: 040 ----
mean loss: 894.53
 ---- batch: 050 ----
mean loss: 877.45
 ---- batch: 060 ----
mean loss: 891.79
 ---- batch: 070 ----
mean loss: 900.28
 ---- batch: 080 ----
mean loss: 878.61
 ---- batch: 090 ----
mean loss: 886.16
train mean loss: 890.62
epoch train time: 0:00:01.648676
elapsed time: 0:01:06.240846
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 19:54:47.081075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.05
 ---- batch: 020 ----
mean loss: 881.48
 ---- batch: 030 ----
mean loss: 879.06
 ---- batch: 040 ----
mean loss: 889.05
 ---- batch: 050 ----
mean loss: 908.86
 ---- batch: 060 ----
mean loss: 883.58
 ---- batch: 070 ----
mean loss: 872.30
 ---- batch: 080 ----
mean loss: 900.21
 ---- batch: 090 ----
mean loss: 886.68
train mean loss: 887.25
epoch train time: 0:00:01.664497
elapsed time: 0:01:07.906080
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 19:54:48.746236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.18
 ---- batch: 020 ----
mean loss: 898.01
 ---- batch: 030 ----
mean loss: 879.62
 ---- batch: 040 ----
mean loss: 864.22
 ---- batch: 050 ----
mean loss: 874.97
 ---- batch: 060 ----
mean loss: 898.11
 ---- batch: 070 ----
mean loss: 888.56
 ---- batch: 080 ----
mean loss: 879.07
 ---- batch: 090 ----
mean loss: 883.19
train mean loss: 884.21
epoch train time: 0:00:01.658502
elapsed time: 0:01:09.565279
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 19:54:50.405257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.74
 ---- batch: 020 ----
mean loss: 877.72
 ---- batch: 030 ----
mean loss: 868.76
 ---- batch: 040 ----
mean loss: 873.56
 ---- batch: 050 ----
mean loss: 889.62
 ---- batch: 060 ----
mean loss: 885.50
 ---- batch: 070 ----
mean loss: 882.25
 ---- batch: 080 ----
mean loss: 888.88
 ---- batch: 090 ----
mean loss: 895.91
train mean loss: 882.69
epoch train time: 0:00:01.656085
elapsed time: 0:01:11.221928
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 19:54:52.062120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.37
 ---- batch: 020 ----
mean loss: 879.05
 ---- batch: 030 ----
mean loss: 885.30
 ---- batch: 040 ----
mean loss: 867.52
 ---- batch: 050 ----
mean loss: 877.11
 ---- batch: 060 ----
mean loss: 870.09
 ---- batch: 070 ----
mean loss: 891.00
 ---- batch: 080 ----
mean loss: 889.22
 ---- batch: 090 ----
mean loss: 879.71
train mean loss: 883.05
epoch train time: 0:00:01.690791
elapsed time: 0:01:12.913452
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 19:54:53.753599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.45
 ---- batch: 020 ----
mean loss: 893.29
 ---- batch: 030 ----
mean loss: 892.02
 ---- batch: 040 ----
mean loss: 880.55
 ---- batch: 050 ----
mean loss: 884.21
 ---- batch: 060 ----
mean loss: 882.72
 ---- batch: 070 ----
mean loss: 878.60
 ---- batch: 080 ----
mean loss: 871.70
 ---- batch: 090 ----
mean loss: 890.32
train mean loss: 880.84
epoch train time: 0:00:01.692049
elapsed time: 0:01:14.606168
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 19:54:55.446330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.74
 ---- batch: 020 ----
mean loss: 879.05
 ---- batch: 030 ----
mean loss: 869.62
 ---- batch: 040 ----
mean loss: 870.97
 ---- batch: 050 ----
mean loss: 868.98
 ---- batch: 060 ----
mean loss: 906.41
 ---- batch: 070 ----
mean loss: 889.52
 ---- batch: 080 ----
mean loss: 882.44
 ---- batch: 090 ----
mean loss: 876.02
train mean loss: 880.59
epoch train time: 0:00:01.658659
elapsed time: 0:01:16.265466
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 19:54:57.105650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.42
 ---- batch: 020 ----
mean loss: 876.72
 ---- batch: 030 ----
mean loss: 877.24
 ---- batch: 040 ----
mean loss: 878.41
 ---- batch: 050 ----
mean loss: 870.79
 ---- batch: 060 ----
mean loss: 871.35
 ---- batch: 070 ----
mean loss: 886.03
 ---- batch: 080 ----
mean loss: 893.89
 ---- batch: 090 ----
mean loss: 896.63
train mean loss: 880.83
epoch train time: 0:00:01.679867
elapsed time: 0:01:17.946093
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 19:54:58.786254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.31
 ---- batch: 020 ----
mean loss: 873.38
 ---- batch: 030 ----
mean loss: 884.42
 ---- batch: 040 ----
mean loss: 888.99
 ---- batch: 050 ----
mean loss: 874.55
 ---- batch: 060 ----
mean loss: 874.99
 ---- batch: 070 ----
mean loss: 867.70
 ---- batch: 080 ----
mean loss: 896.93
 ---- batch: 090 ----
mean loss: 866.77
train mean loss: 879.54
epoch train time: 0:00:01.669428
elapsed time: 0:01:19.616115
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 19:55:00.456263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.67
 ---- batch: 020 ----
mean loss: 881.88
 ---- batch: 030 ----
mean loss: 869.46
 ---- batch: 040 ----
mean loss: 878.54
 ---- batch: 050 ----
mean loss: 890.20
 ---- batch: 060 ----
mean loss: 888.26
 ---- batch: 070 ----
mean loss: 892.78
 ---- batch: 080 ----
mean loss: 866.65
 ---- batch: 090 ----
mean loss: 867.78
train mean loss: 880.64
epoch train time: 0:00:01.658763
elapsed time: 0:01:21.275555
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 19:55:02.115714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.03
 ---- batch: 020 ----
mean loss: 888.72
 ---- batch: 030 ----
mean loss: 874.78
 ---- batch: 040 ----
mean loss: 878.85
 ---- batch: 050 ----
mean loss: 879.49
 ---- batch: 060 ----
mean loss: 884.22
 ---- batch: 070 ----
mean loss: 874.91
 ---- batch: 080 ----
mean loss: 872.39
 ---- batch: 090 ----
mean loss: 862.99
train mean loss: 878.42
epoch train time: 0:00:01.687061
elapsed time: 0:01:22.963278
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 19:55:03.803448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.46
 ---- batch: 020 ----
mean loss: 874.92
 ---- batch: 030 ----
mean loss: 873.11
 ---- batch: 040 ----
mean loss: 883.31
 ---- batch: 050 ----
mean loss: 899.52
 ---- batch: 060 ----
mean loss: 869.12
 ---- batch: 070 ----
mean loss: 897.14
 ---- batch: 080 ----
mean loss: 872.22
 ---- batch: 090 ----
mean loss: 865.21
train mean loss: 879.68
epoch train time: 0:00:01.635031
elapsed time: 0:01:24.598996
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 19:55:05.439161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.09
 ---- batch: 020 ----
mean loss: 884.12
 ---- batch: 030 ----
mean loss: 877.78
 ---- batch: 040 ----
mean loss: 871.87
 ---- batch: 050 ----
mean loss: 882.40
 ---- batch: 060 ----
mean loss: 891.53
 ---- batch: 070 ----
mean loss: 863.10
 ---- batch: 080 ----
mean loss: 885.51
 ---- batch: 090 ----
mean loss: 873.60
train mean loss: 878.83
epoch train time: 0:00:01.670691
elapsed time: 0:01:26.270343
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 19:55:07.110511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.80
 ---- batch: 020 ----
mean loss: 878.51
 ---- batch: 030 ----
mean loss: 883.65
 ---- batch: 040 ----
mean loss: 873.37
 ---- batch: 050 ----
mean loss: 873.80
 ---- batch: 060 ----
mean loss: 870.94
 ---- batch: 070 ----
mean loss: 871.16
 ---- batch: 080 ----
mean loss: 880.04
 ---- batch: 090 ----
mean loss: 877.23
train mean loss: 878.63
epoch train time: 0:00:01.646907
elapsed time: 0:01:27.917913
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 19:55:08.758076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.64
 ---- batch: 020 ----
mean loss: 888.53
 ---- batch: 030 ----
mean loss: 871.56
 ---- batch: 040 ----
mean loss: 880.92
 ---- batch: 050 ----
mean loss: 892.07
 ---- batch: 060 ----
mean loss: 882.46
 ---- batch: 070 ----
mean loss: 885.69
 ---- batch: 080 ----
mean loss: 860.40
 ---- batch: 090 ----
mean loss: 877.76
train mean loss: 878.65
epoch train time: 0:00:01.643951
elapsed time: 0:01:29.562554
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 19:55:10.402783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.22
 ---- batch: 020 ----
mean loss: 878.64
 ---- batch: 030 ----
mean loss: 865.21
 ---- batch: 040 ----
mean loss: 883.85
 ---- batch: 050 ----
mean loss: 872.71
 ---- batch: 060 ----
mean loss: 891.67
 ---- batch: 070 ----
mean loss: 880.30
 ---- batch: 080 ----
mean loss: 879.06
 ---- batch: 090 ----
mean loss: 890.14
train mean loss: 879.28
epoch train time: 0:00:01.662000
elapsed time: 0:01:31.225291
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 19:55:12.065492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.44
 ---- batch: 020 ----
mean loss: 874.38
 ---- batch: 030 ----
mean loss: 880.16
 ---- batch: 040 ----
mean loss: 890.56
 ---- batch: 050 ----
mean loss: 892.05
 ---- batch: 060 ----
mean loss: 857.99
 ---- batch: 070 ----
mean loss: 864.65
 ---- batch: 080 ----
mean loss: 867.67
 ---- batch: 090 ----
mean loss: 911.57
train mean loss: 878.92
epoch train time: 0:00:01.649476
elapsed time: 0:01:32.875513
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 19:55:13.715676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.82
 ---- batch: 020 ----
mean loss: 870.43
 ---- batch: 030 ----
mean loss: 886.39
 ---- batch: 040 ----
mean loss: 899.56
 ---- batch: 050 ----
mean loss: 897.70
 ---- batch: 060 ----
mean loss: 876.56
 ---- batch: 070 ----
mean loss: 876.21
 ---- batch: 080 ----
mean loss: 854.49
 ---- batch: 090 ----
mean loss: 868.63
train mean loss: 878.03
epoch train time: 0:00:01.667106
elapsed time: 0:01:34.543313
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 19:55:15.383462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.17
 ---- batch: 020 ----
mean loss: 884.89
 ---- batch: 030 ----
mean loss: 882.38
 ---- batch: 040 ----
mean loss: 884.28
 ---- batch: 050 ----
mean loss: 866.80
 ---- batch: 060 ----
mean loss: 883.75
 ---- batch: 070 ----
mean loss: 871.93
 ---- batch: 080 ----
mean loss: 885.88
 ---- batch: 090 ----
mean loss: 859.79
train mean loss: 877.60
epoch train time: 0:00:01.667914
elapsed time: 0:01:36.211870
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 19:55:17.052132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.01
 ---- batch: 020 ----
mean loss: 877.10
 ---- batch: 030 ----
mean loss: 872.58
 ---- batch: 040 ----
mean loss: 873.42
 ---- batch: 050 ----
mean loss: 863.05
 ---- batch: 060 ----
mean loss: 884.95
 ---- batch: 070 ----
mean loss: 885.79
 ---- batch: 080 ----
mean loss: 872.38
 ---- batch: 090 ----
mean loss: 889.01
train mean loss: 878.62
epoch train time: 0:00:01.635068
elapsed time: 0:01:37.847690
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 19:55:18.687848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.60
 ---- batch: 020 ----
mean loss: 865.84
 ---- batch: 030 ----
mean loss: 872.66
 ---- batch: 040 ----
mean loss: 875.19
 ---- batch: 050 ----
mean loss: 877.25
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 876.87
 ---- batch: 080 ----
mean loss: 873.61
 ---- batch: 090 ----
mean loss: 881.72
train mean loss: 877.57
epoch train time: 0:00:01.636844
elapsed time: 0:01:39.485182
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 19:55:20.325339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.98
 ---- batch: 020 ----
mean loss: 888.27
 ---- batch: 030 ----
mean loss: 881.04
 ---- batch: 040 ----
mean loss: 868.42
 ---- batch: 050 ----
mean loss: 888.97
 ---- batch: 060 ----
mean loss: 880.96
 ---- batch: 070 ----
mean loss: 872.18
 ---- batch: 080 ----
mean loss: 866.51
 ---- batch: 090 ----
mean loss: 867.99
train mean loss: 876.66
epoch train time: 0:00:01.648777
elapsed time: 0:01:41.134614
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 19:55:21.974788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.96
 ---- batch: 020 ----
mean loss: 882.06
 ---- batch: 030 ----
mean loss: 882.30
 ---- batch: 040 ----
mean loss: 871.32
 ---- batch: 050 ----
mean loss: 864.24
 ---- batch: 060 ----
mean loss: 887.88
 ---- batch: 070 ----
mean loss: 878.11
 ---- batch: 080 ----
mean loss: 875.07
 ---- batch: 090 ----
mean loss: 886.04
train mean loss: 878.02
epoch train time: 0:00:01.625684
elapsed time: 0:01:42.760989
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 19:55:23.601146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.92
 ---- batch: 020 ----
mean loss: 865.30
 ---- batch: 030 ----
mean loss: 884.03
 ---- batch: 040 ----
mean loss: 850.93
 ---- batch: 050 ----
mean loss: 854.16
 ---- batch: 060 ----
mean loss: 875.87
 ---- batch: 070 ----
mean loss: 891.00
 ---- batch: 080 ----
mean loss: 897.95
 ---- batch: 090 ----
mean loss: 901.20
train mean loss: 878.23
epoch train time: 0:00:01.657280
elapsed time: 0:01:44.418896
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 19:55:25.259053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.04
 ---- batch: 020 ----
mean loss: 860.69
 ---- batch: 030 ----
mean loss: 882.76
 ---- batch: 040 ----
mean loss: 893.30
 ---- batch: 050 ----
mean loss: 873.89
 ---- batch: 060 ----
mean loss: 878.78
 ---- batch: 070 ----
mean loss: 875.71
 ---- batch: 080 ----
mean loss: 889.03
 ---- batch: 090 ----
mean loss: 888.86
train mean loss: 878.13
epoch train time: 0:00:01.636451
elapsed time: 0:01:46.056130
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 19:55:26.896110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.91
 ---- batch: 020 ----
mean loss: 897.71
 ---- batch: 030 ----
mean loss: 898.68
 ---- batch: 040 ----
mean loss: 878.65
 ---- batch: 050 ----
mean loss: 855.85
 ---- batch: 060 ----
mean loss: 886.91
 ---- batch: 070 ----
mean loss: 875.38
 ---- batch: 080 ----
mean loss: 878.14
 ---- batch: 090 ----
mean loss: 873.02
train mean loss: 878.24
epoch train time: 0:00:01.607362
elapsed time: 0:01:47.663929
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 19:55:28.504066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.99
 ---- batch: 020 ----
mean loss: 872.32
 ---- batch: 030 ----
mean loss: 878.45
 ---- batch: 040 ----
mean loss: 878.45
 ---- batch: 050 ----
mean loss: 880.97
 ---- batch: 060 ----
mean loss: 869.45
 ---- batch: 070 ----
mean loss: 882.04
 ---- batch: 080 ----
mean loss: 877.04
 ---- batch: 090 ----
mean loss: 866.11
train mean loss: 876.48
epoch train time: 0:00:01.658169
elapsed time: 0:01:49.322685
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 19:55:30.162850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.09
 ---- batch: 020 ----
mean loss: 875.74
 ---- batch: 030 ----
mean loss: 887.00
 ---- batch: 040 ----
mean loss: 873.06
 ---- batch: 050 ----
mean loss: 888.75
 ---- batch: 060 ----
mean loss: 880.96
 ---- batch: 070 ----
mean loss: 898.03
 ---- batch: 080 ----
mean loss: 866.41
 ---- batch: 090 ----
mean loss: 873.24
train mean loss: 877.31
epoch train time: 0:00:01.659249
elapsed time: 0:01:50.982629
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 19:55:31.822794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.10
 ---- batch: 020 ----
mean loss: 870.41
 ---- batch: 030 ----
mean loss: 871.66
 ---- batch: 040 ----
mean loss: 883.14
 ---- batch: 050 ----
mean loss: 875.76
 ---- batch: 060 ----
mean loss: 878.51
 ---- batch: 070 ----
mean loss: 873.53
 ---- batch: 080 ----
mean loss: 880.02
 ---- batch: 090 ----
mean loss: 898.82
train mean loss: 877.05
epoch train time: 0:00:01.630216
elapsed time: 0:01:52.613477
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 19:55:33.453706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.39
 ---- batch: 020 ----
mean loss: 875.10
 ---- batch: 030 ----
mean loss: 844.73
 ---- batch: 040 ----
mean loss: 872.94
 ---- batch: 050 ----
mean loss: 880.53
 ---- batch: 060 ----
mean loss: 871.66
 ---- batch: 070 ----
mean loss: 898.38
 ---- batch: 080 ----
mean loss: 877.21
 ---- batch: 090 ----
mean loss: 900.64
train mean loss: 876.57
epoch train time: 0:00:01.658930
elapsed time: 0:01:54.273119
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 19:55:35.113295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.35
 ---- batch: 020 ----
mean loss: 887.65
 ---- batch: 030 ----
mean loss: 870.81
 ---- batch: 040 ----
mean loss: 883.20
 ---- batch: 050 ----
mean loss: 868.34
 ---- batch: 060 ----
mean loss: 871.20
 ---- batch: 070 ----
mean loss: 867.22
 ---- batch: 080 ----
mean loss: 885.80
 ---- batch: 090 ----
mean loss: 881.67
train mean loss: 876.52
epoch train time: 0:00:01.642658
elapsed time: 0:01:55.916435
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 19:55:36.756689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.13
 ---- batch: 020 ----
mean loss: 894.63
 ---- batch: 030 ----
mean loss: 875.83
 ---- batch: 040 ----
mean loss: 889.87
 ---- batch: 050 ----
mean loss: 869.57
 ---- batch: 060 ----
mean loss: 870.62
 ---- batch: 070 ----
mean loss: 874.55
 ---- batch: 080 ----
mean loss: 860.55
 ---- batch: 090 ----
mean loss: 877.27
train mean loss: 877.12
epoch train time: 0:00:01.641877
elapsed time: 0:01:57.559098
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 19:55:38.399276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.60
 ---- batch: 020 ----
mean loss: 875.65
 ---- batch: 030 ----
mean loss: 873.56
 ---- batch: 040 ----
mean loss: 864.80
 ---- batch: 050 ----
mean loss: 871.27
 ---- batch: 060 ----
mean loss: 876.68
 ---- batch: 070 ----
mean loss: 900.92
 ---- batch: 080 ----
mean loss: 875.86
 ---- batch: 090 ----
mean loss: 887.89
train mean loss: 877.18
epoch train time: 0:00:01.632071
elapsed time: 0:01:59.191868
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 19:55:40.032028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.13
 ---- batch: 020 ----
mean loss: 876.65
 ---- batch: 030 ----
mean loss: 870.34
 ---- batch: 040 ----
mean loss: 876.40
 ---- batch: 050 ----
mean loss: 871.29
 ---- batch: 060 ----
mean loss: 876.49
 ---- batch: 070 ----
mean loss: 877.98
 ---- batch: 080 ----
mean loss: 890.52
 ---- batch: 090 ----
mean loss: 868.94
train mean loss: 876.07
epoch train time: 0:00:01.643480
elapsed time: 0:02:00.836023
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 19:55:41.676187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.78
 ---- batch: 020 ----
mean loss: 864.30
 ---- batch: 030 ----
mean loss: 885.84
 ---- batch: 040 ----
mean loss: 878.61
 ---- batch: 050 ----
mean loss: 877.97
 ---- batch: 060 ----
mean loss: 873.30
 ---- batch: 070 ----
mean loss: 874.86
 ---- batch: 080 ----
mean loss: 875.32
 ---- batch: 090 ----
mean loss: 884.11
train mean loss: 876.22
epoch train time: 0:00:01.674009
elapsed time: 0:02:02.510716
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 19:55:43.350851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.78
 ---- batch: 020 ----
mean loss: 856.27
 ---- batch: 030 ----
mean loss: 880.45
 ---- batch: 040 ----
mean loss: 882.01
 ---- batch: 050 ----
mean loss: 877.31
 ---- batch: 060 ----
mean loss: 861.15
 ---- batch: 070 ----
mean loss: 882.61
 ---- batch: 080 ----
mean loss: 887.15
 ---- batch: 090 ----
mean loss: 864.04
train mean loss: 877.35
epoch train time: 0:00:01.660538
elapsed time: 0:02:04.171854
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 19:55:45.012010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.48
 ---- batch: 020 ----
mean loss: 856.62
 ---- batch: 030 ----
mean loss: 866.63
 ---- batch: 040 ----
mean loss: 868.92
 ---- batch: 050 ----
mean loss: 893.64
 ---- batch: 060 ----
mean loss: 874.43
 ---- batch: 070 ----
mean loss: 914.96
 ---- batch: 080 ----
mean loss: 860.00
 ---- batch: 090 ----
mean loss: 883.49
train mean loss: 876.96
epoch train time: 0:00:01.651438
elapsed time: 0:02:05.823881
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 19:55:46.664012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.56
 ---- batch: 020 ----
mean loss: 871.38
 ---- batch: 030 ----
mean loss: 882.09
 ---- batch: 040 ----
mean loss: 867.82
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 875.13
 ---- batch: 070 ----
mean loss: 881.24
 ---- batch: 080 ----
mean loss: 872.44
 ---- batch: 090 ----
mean loss: 871.37
train mean loss: 876.85
epoch train time: 0:00:01.646250
elapsed time: 0:02:07.470851
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 19:55:48.311030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.56
 ---- batch: 020 ----
mean loss: 884.07
 ---- batch: 030 ----
mean loss: 878.07
 ---- batch: 040 ----
mean loss: 875.90
 ---- batch: 050 ----
mean loss: 887.63
 ---- batch: 060 ----
mean loss: 875.45
 ---- batch: 070 ----
mean loss: 878.84
 ---- batch: 080 ----
mean loss: 889.47
 ---- batch: 090 ----
mean loss: 868.07
train mean loss: 875.72
epoch train time: 0:00:01.673781
elapsed time: 0:02:09.145301
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 19:55:49.985447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.33
 ---- batch: 020 ----
mean loss: 871.75
 ---- batch: 030 ----
mean loss: 891.87
 ---- batch: 040 ----
mean loss: 879.13
 ---- batch: 050 ----
mean loss: 876.11
 ---- batch: 060 ----
mean loss: 869.84
 ---- batch: 070 ----
mean loss: 879.19
 ---- batch: 080 ----
mean loss: 872.81
 ---- batch: 090 ----
mean loss: 884.45
train mean loss: 876.48
epoch train time: 0:00:01.666591
elapsed time: 0:02:10.812514
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 19:55:51.652670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.38
 ---- batch: 020 ----
mean loss: 872.37
 ---- batch: 030 ----
mean loss: 871.92
 ---- batch: 040 ----
mean loss: 891.11
 ---- batch: 050 ----
mean loss: 876.87
 ---- batch: 060 ----
mean loss: 883.35
 ---- batch: 070 ----
mean loss: 871.23
 ---- batch: 080 ----
mean loss: 875.44
 ---- batch: 090 ----
mean loss: 876.45
train mean loss: 875.82
epoch train time: 0:00:01.647457
elapsed time: 0:02:12.460619
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 19:55:53.300794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.95
 ---- batch: 020 ----
mean loss: 872.86
 ---- batch: 030 ----
mean loss: 879.35
 ---- batch: 040 ----
mean loss: 892.89
 ---- batch: 050 ----
mean loss: 864.27
 ---- batch: 060 ----
mean loss: 863.89
 ---- batch: 070 ----
mean loss: 849.32
 ---- batch: 080 ----
mean loss: 839.11
 ---- batch: 090 ----
mean loss: 806.40
train mean loss: 853.59
epoch train time: 0:00:01.675809
elapsed time: 0:02:14.137125
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 19:55:54.977341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 724.16
 ---- batch: 020 ----
mean loss: 646.88
 ---- batch: 030 ----
mean loss: 599.34
 ---- batch: 040 ----
mean loss: 550.01
 ---- batch: 050 ----
mean loss: 504.13
 ---- batch: 060 ----
mean loss: 499.82
 ---- batch: 070 ----
mean loss: 475.73
 ---- batch: 080 ----
mean loss: 467.97
 ---- batch: 090 ----
mean loss: 463.24
train mean loss: 541.04
epoch train time: 0:00:01.644119
elapsed time: 0:02:15.781972
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 19:55:56.622172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.99
 ---- batch: 020 ----
mean loss: 433.54
 ---- batch: 030 ----
mean loss: 417.46
 ---- batch: 040 ----
mean loss: 404.41
 ---- batch: 050 ----
mean loss: 410.87
 ---- batch: 060 ----
mean loss: 399.57
 ---- batch: 070 ----
mean loss: 389.61
 ---- batch: 080 ----
mean loss: 400.67
 ---- batch: 090 ----
mean loss: 390.08
train mean loss: 407.61
epoch train time: 0:00:01.658166
elapsed time: 0:02:17.440807
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 19:55:58.281000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.47
 ---- batch: 020 ----
mean loss: 377.38
 ---- batch: 030 ----
mean loss: 365.10
 ---- batch: 040 ----
mean loss: 366.64
 ---- batch: 050 ----
mean loss: 374.34
 ---- batch: 060 ----
mean loss: 354.60
 ---- batch: 070 ----
mean loss: 354.00
 ---- batch: 080 ----
mean loss: 357.75
 ---- batch: 090 ----
mean loss: 346.76
train mean loss: 361.43
epoch train time: 0:00:01.652903
elapsed time: 0:02:19.094402
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 19:55:59.934562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.39
 ---- batch: 020 ----
mean loss: 346.81
 ---- batch: 030 ----
mean loss: 343.11
 ---- batch: 040 ----
mean loss: 347.74
 ---- batch: 050 ----
mean loss: 340.40
 ---- batch: 060 ----
mean loss: 344.44
 ---- batch: 070 ----
mean loss: 327.29
 ---- batch: 080 ----
mean loss: 325.91
 ---- batch: 090 ----
mean loss: 324.95
train mean loss: 339.66
epoch train time: 0:00:01.637647
elapsed time: 0:02:20.732814
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 19:56:01.572998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.67
 ---- batch: 020 ----
mean loss: 321.94
 ---- batch: 030 ----
mean loss: 326.63
 ---- batch: 040 ----
mean loss: 321.04
 ---- batch: 050 ----
mean loss: 323.51
 ---- batch: 060 ----
mean loss: 318.53
 ---- batch: 070 ----
mean loss: 310.75
 ---- batch: 080 ----
mean loss: 317.56
 ---- batch: 090 ----
mean loss: 311.46
train mean loss: 318.44
epoch train time: 0:00:01.635596
elapsed time: 0:02:22.369098
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 19:56:03.209299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.74
 ---- batch: 020 ----
mean loss: 300.85
 ---- batch: 030 ----
mean loss: 327.02
 ---- batch: 040 ----
mean loss: 313.83
 ---- batch: 050 ----
mean loss: 308.22
 ---- batch: 060 ----
mean loss: 309.77
 ---- batch: 070 ----
mean loss: 303.37
 ---- batch: 080 ----
mean loss: 302.41
 ---- batch: 090 ----
mean loss: 293.41
train mean loss: 306.78
epoch train time: 0:00:01.649716
elapsed time: 0:02:24.019520
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 19:56:04.859683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.49
 ---- batch: 020 ----
mean loss: 301.06
 ---- batch: 030 ----
mean loss: 291.88
 ---- batch: 040 ----
mean loss: 294.11
 ---- batch: 050 ----
mean loss: 295.13
 ---- batch: 060 ----
mean loss: 295.91
 ---- batch: 070 ----
mean loss: 299.00
 ---- batch: 080 ----
mean loss: 297.34
 ---- batch: 090 ----
mean loss: 294.47
train mean loss: 296.78
epoch train time: 0:00:01.660518
elapsed time: 0:02:25.680638
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 19:56:06.520799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.41
 ---- batch: 020 ----
mean loss: 297.98
 ---- batch: 030 ----
mean loss: 280.77
 ---- batch: 040 ----
mean loss: 293.27
 ---- batch: 050 ----
mean loss: 286.85
 ---- batch: 060 ----
mean loss: 287.90
 ---- batch: 070 ----
mean loss: 295.16
 ---- batch: 080 ----
mean loss: 286.71
 ---- batch: 090 ----
mean loss: 288.79
train mean loss: 290.35
epoch train time: 0:00:01.658938
elapsed time: 0:02:27.340314
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 19:56:08.180574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.35
 ---- batch: 020 ----
mean loss: 282.08
 ---- batch: 030 ----
mean loss: 288.02
 ---- batch: 040 ----
mean loss: 278.78
 ---- batch: 050 ----
mean loss: 284.50
 ---- batch: 060 ----
mean loss: 271.57
 ---- batch: 070 ----
mean loss: 262.52
 ---- batch: 080 ----
mean loss: 292.47
 ---- batch: 090 ----
mean loss: 285.77
train mean loss: 281.95
epoch train time: 0:00:01.623166
elapsed time: 0:02:28.964273
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 19:56:09.804414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.22
 ---- batch: 020 ----
mean loss: 270.08
 ---- batch: 030 ----
mean loss: 279.55
 ---- batch: 040 ----
mean loss: 286.36
 ---- batch: 050 ----
mean loss: 275.21
 ---- batch: 060 ----
mean loss: 269.74
 ---- batch: 070 ----
mean loss: 279.67
 ---- batch: 080 ----
mean loss: 276.03
 ---- batch: 090 ----
mean loss: 281.38
train mean loss: 275.79
epoch train time: 0:00:01.620655
elapsed time: 0:02:30.585498
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 19:56:11.425660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.53
 ---- batch: 020 ----
mean loss: 267.51
 ---- batch: 030 ----
mean loss: 273.65
 ---- batch: 040 ----
mean loss: 267.83
 ---- batch: 050 ----
mean loss: 277.38
 ---- batch: 060 ----
mean loss: 278.20
 ---- batch: 070 ----
mean loss: 265.86
 ---- batch: 080 ----
mean loss: 270.88
 ---- batch: 090 ----
mean loss: 272.31
train mean loss: 271.66
epoch train time: 0:00:01.652708
elapsed time: 0:02:32.238869
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 19:56:13.079031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.03
 ---- batch: 020 ----
mean loss: 251.09
 ---- batch: 030 ----
mean loss: 268.62
 ---- batch: 040 ----
mean loss: 274.12
 ---- batch: 050 ----
mean loss: 268.80
 ---- batch: 060 ----
mean loss: 268.13
 ---- batch: 070 ----
mean loss: 268.20
 ---- batch: 080 ----
mean loss: 271.37
 ---- batch: 090 ----
mean loss: 270.92
train mean loss: 266.77
epoch train time: 0:00:01.668719
elapsed time: 0:02:33.908276
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 19:56:14.748485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.41
 ---- batch: 020 ----
mean loss: 265.68
 ---- batch: 030 ----
mean loss: 273.43
 ---- batch: 040 ----
mean loss: 263.59
 ---- batch: 050 ----
mean loss: 260.69
 ---- batch: 060 ----
mean loss: 259.56
 ---- batch: 070 ----
mean loss: 260.42
 ---- batch: 080 ----
mean loss: 267.65
 ---- batch: 090 ----
mean loss: 265.07
train mean loss: 262.74
epoch train time: 0:00:01.675823
elapsed time: 0:02:35.584847
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 19:56:16.425001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.29
 ---- batch: 020 ----
mean loss: 256.65
 ---- batch: 030 ----
mean loss: 265.23
 ---- batch: 040 ----
mean loss: 258.64
 ---- batch: 050 ----
mean loss: 261.58
 ---- batch: 060 ----
mean loss: 262.00
 ---- batch: 070 ----
mean loss: 250.06
 ---- batch: 080 ----
mean loss: 258.76
 ---- batch: 090 ----
mean loss: 256.11
train mean loss: 257.57
epoch train time: 0:00:01.643139
elapsed time: 0:02:37.228733
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 19:56:18.068939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.22
 ---- batch: 020 ----
mean loss: 251.41
 ---- batch: 030 ----
mean loss: 255.89
 ---- batch: 040 ----
mean loss: 255.15
 ---- batch: 050 ----
mean loss: 241.30
 ---- batch: 060 ----
mean loss: 248.81
 ---- batch: 070 ----
mean loss: 253.05
 ---- batch: 080 ----
mean loss: 257.87
 ---- batch: 090 ----
mean loss: 250.78
train mean loss: 251.80
epoch train time: 0:00:01.659595
elapsed time: 0:02:38.889052
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 19:56:19.729246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.64
 ---- batch: 020 ----
mean loss: 246.38
 ---- batch: 030 ----
mean loss: 236.05
 ---- batch: 040 ----
mean loss: 249.63
 ---- batch: 050 ----
mean loss: 253.94
 ---- batch: 060 ----
mean loss: 245.70
 ---- batch: 070 ----
mean loss: 257.38
 ---- batch: 080 ----
mean loss: 253.88
 ---- batch: 090 ----
mean loss: 249.17
train mean loss: 249.97
epoch train time: 0:00:01.631139
elapsed time: 0:02:40.520879
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 19:56:21.361035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.81
 ---- batch: 020 ----
mean loss: 248.86
 ---- batch: 030 ----
mean loss: 248.00
 ---- batch: 040 ----
mean loss: 251.91
 ---- batch: 050 ----
mean loss: 241.52
 ---- batch: 060 ----
mean loss: 245.90
 ---- batch: 070 ----
mean loss: 251.66
 ---- batch: 080 ----
mean loss: 251.33
 ---- batch: 090 ----
mean loss: 244.38
train mean loss: 247.13
epoch train time: 0:00:01.653397
elapsed time: 0:02:42.174991
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 19:56:23.015141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.14
 ---- batch: 020 ----
mean loss: 237.43
 ---- batch: 030 ----
mean loss: 247.34
 ---- batch: 040 ----
mean loss: 257.36
 ---- batch: 050 ----
mean loss: 243.74
 ---- batch: 060 ----
mean loss: 237.56
 ---- batch: 070 ----
mean loss: 252.10
 ---- batch: 080 ----
mean loss: 242.44
 ---- batch: 090 ----
mean loss: 243.44
train mean loss: 243.08
epoch train time: 0:00:01.654909
elapsed time: 0:02:43.830551
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 19:56:24.670712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.20
 ---- batch: 020 ----
mean loss: 236.40
 ---- batch: 030 ----
mean loss: 231.74
 ---- batch: 040 ----
mean loss: 245.07
 ---- batch: 050 ----
mean loss: 240.52
 ---- batch: 060 ----
mean loss: 247.95
 ---- batch: 070 ----
mean loss: 236.47
 ---- batch: 080 ----
mean loss: 241.77
 ---- batch: 090 ----
mean loss: 238.61
train mean loss: 238.94
epoch train time: 0:00:01.654516
elapsed time: 0:02:45.485707
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 19:56:26.325948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.57
 ---- batch: 020 ----
mean loss: 234.76
 ---- batch: 030 ----
mean loss: 234.44
 ---- batch: 040 ----
mean loss: 234.94
 ---- batch: 050 ----
mean loss: 237.32
 ---- batch: 060 ----
mean loss: 238.13
 ---- batch: 070 ----
mean loss: 237.27
 ---- batch: 080 ----
mean loss: 238.01
 ---- batch: 090 ----
mean loss: 245.73
train mean loss: 237.45
epoch train time: 0:00:01.652995
elapsed time: 0:02:47.139407
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 19:56:27.979559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.63
 ---- batch: 020 ----
mean loss: 237.50
 ---- batch: 030 ----
mean loss: 236.36
 ---- batch: 040 ----
mean loss: 230.18
 ---- batch: 050 ----
mean loss: 244.58
 ---- batch: 060 ----
mean loss: 235.94
 ---- batch: 070 ----
mean loss: 232.18
 ---- batch: 080 ----
mean loss: 226.77
 ---- batch: 090 ----
mean loss: 233.92
train mean loss: 233.90
epoch train time: 0:00:01.629602
elapsed time: 0:02:48.769602
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 19:56:29.609764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.82
 ---- batch: 020 ----
mean loss: 228.88
 ---- batch: 030 ----
mean loss: 222.91
 ---- batch: 040 ----
mean loss: 229.03
 ---- batch: 050 ----
mean loss: 234.07
 ---- batch: 060 ----
mean loss: 233.79
 ---- batch: 070 ----
mean loss: 225.85
 ---- batch: 080 ----
mean loss: 241.19
 ---- batch: 090 ----
mean loss: 228.12
train mean loss: 230.34
epoch train time: 0:00:01.632510
elapsed time: 0:02:50.402779
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 19:56:31.242941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.21
 ---- batch: 020 ----
mean loss: 220.04
 ---- batch: 030 ----
mean loss: 223.76
 ---- batch: 040 ----
mean loss: 227.94
 ---- batch: 050 ----
mean loss: 237.33
 ---- batch: 060 ----
mean loss: 231.60
 ---- batch: 070 ----
mean loss: 225.69
 ---- batch: 080 ----
mean loss: 225.40
 ---- batch: 090 ----
mean loss: 230.99
train mean loss: 229.03
epoch train time: 0:00:01.649921
elapsed time: 0:02:52.053390
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 19:56:32.893595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.17
 ---- batch: 020 ----
mean loss: 225.44
 ---- batch: 030 ----
mean loss: 229.11
 ---- batch: 040 ----
mean loss: 221.61
 ---- batch: 050 ----
mean loss: 229.02
 ---- batch: 060 ----
mean loss: 226.23
 ---- batch: 070 ----
mean loss: 216.37
 ---- batch: 080 ----
mean loss: 224.49
 ---- batch: 090 ----
mean loss: 234.76
train mean loss: 226.09
epoch train time: 0:00:01.648720
elapsed time: 0:02:53.702766
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 19:56:34.542924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.97
 ---- batch: 020 ----
mean loss: 235.80
 ---- batch: 030 ----
mean loss: 223.91
 ---- batch: 040 ----
mean loss: 236.62
 ---- batch: 050 ----
mean loss: 221.21
 ---- batch: 060 ----
mean loss: 215.07
 ---- batch: 070 ----
mean loss: 214.85
 ---- batch: 080 ----
mean loss: 226.32
 ---- batch: 090 ----
mean loss: 236.51
train mean loss: 226.95
epoch train time: 0:00:01.664299
elapsed time: 0:02:55.367757
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 19:56:36.207752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.27
 ---- batch: 020 ----
mean loss: 215.82
 ---- batch: 030 ----
mean loss: 223.95
 ---- batch: 040 ----
mean loss: 227.15
 ---- batch: 050 ----
mean loss: 217.86
 ---- batch: 060 ----
mean loss: 222.62
 ---- batch: 070 ----
mean loss: 217.50
 ---- batch: 080 ----
mean loss: 226.46
 ---- batch: 090 ----
mean loss: 216.60
train mean loss: 222.40
epoch train time: 0:00:01.643893
elapsed time: 0:02:57.012234
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 19:56:37.852473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.09
 ---- batch: 020 ----
mean loss: 214.09
 ---- batch: 030 ----
mean loss: 216.92
 ---- batch: 040 ----
mean loss: 217.98
 ---- batch: 050 ----
mean loss: 202.86
 ---- batch: 060 ----
mean loss: 221.49
 ---- batch: 070 ----
mean loss: 225.39
 ---- batch: 080 ----
mean loss: 221.76
 ---- batch: 090 ----
mean loss: 224.42
train mean loss: 218.63
epoch train time: 0:00:01.677408
elapsed time: 0:02:58.690361
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 19:56:39.530555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.32
 ---- batch: 020 ----
mean loss: 220.77
 ---- batch: 030 ----
mean loss: 212.12
 ---- batch: 040 ----
mean loss: 219.25
 ---- batch: 050 ----
mean loss: 211.55
 ---- batch: 060 ----
mean loss: 223.94
 ---- batch: 070 ----
mean loss: 215.35
 ---- batch: 080 ----
mean loss: 218.83
 ---- batch: 090 ----
mean loss: 216.33
train mean loss: 217.23
epoch train time: 0:00:01.672658
elapsed time: 0:03:00.363696
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 19:56:41.203849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.95
 ---- batch: 020 ----
mean loss: 216.79
 ---- batch: 030 ----
mean loss: 216.71
 ---- batch: 040 ----
mean loss: 215.11
 ---- batch: 050 ----
mean loss: 221.43
 ---- batch: 060 ----
mean loss: 211.87
 ---- batch: 070 ----
mean loss: 215.24
 ---- batch: 080 ----
mean loss: 216.28
 ---- batch: 090 ----
mean loss: 217.60
train mean loss: 215.29
epoch train time: 0:00:01.648861
elapsed time: 0:03:02.013232
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 19:56:42.853385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.76
 ---- batch: 020 ----
mean loss: 210.28
 ---- batch: 030 ----
mean loss: 200.14
 ---- batch: 040 ----
mean loss: 215.07
 ---- batch: 050 ----
mean loss: 219.21
 ---- batch: 060 ----
mean loss: 223.94
 ---- batch: 070 ----
mean loss: 210.95
 ---- batch: 080 ----
mean loss: 220.61
 ---- batch: 090 ----
mean loss: 210.80
train mean loss: 213.32
epoch train time: 0:00:01.653097
elapsed time: 0:03:03.666971
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 19:56:44.507173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.25
 ---- batch: 020 ----
mean loss: 216.83
 ---- batch: 030 ----
mean loss: 208.53
 ---- batch: 040 ----
mean loss: 209.23
 ---- batch: 050 ----
mean loss: 215.86
 ---- batch: 060 ----
mean loss: 218.86
 ---- batch: 070 ----
mean loss: 205.41
 ---- batch: 080 ----
mean loss: 220.19
 ---- batch: 090 ----
mean loss: 201.41
train mean loss: 211.88
epoch train time: 0:00:01.611081
elapsed time: 0:03:05.278734
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 19:56:46.118898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.13
 ---- batch: 020 ----
mean loss: 212.21
 ---- batch: 030 ----
mean loss: 208.90
 ---- batch: 040 ----
mean loss: 209.02
 ---- batch: 050 ----
mean loss: 203.41
 ---- batch: 060 ----
mean loss: 215.08
 ---- batch: 070 ----
mean loss: 206.03
 ---- batch: 080 ----
mean loss: 213.84
 ---- batch: 090 ----
mean loss: 210.20
train mean loss: 209.37
epoch train time: 0:00:01.682734
elapsed time: 0:03:06.962184
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 19:56:47.802349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.44
 ---- batch: 020 ----
mean loss: 217.39
 ---- batch: 030 ----
mean loss: 206.66
 ---- batch: 040 ----
mean loss: 206.26
 ---- batch: 050 ----
mean loss: 208.21
 ---- batch: 060 ----
mean loss: 212.43
 ---- batch: 070 ----
mean loss: 215.91
 ---- batch: 080 ----
mean loss: 211.65
 ---- batch: 090 ----
mean loss: 203.01
train mean loss: 207.83
epoch train time: 0:00:01.677240
elapsed time: 0:03:08.640070
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 19:56:49.480246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.28
 ---- batch: 020 ----
mean loss: 195.94
 ---- batch: 030 ----
mean loss: 217.45
 ---- batch: 040 ----
mean loss: 200.33
 ---- batch: 050 ----
mean loss: 196.98
 ---- batch: 060 ----
mean loss: 212.29
 ---- batch: 070 ----
mean loss: 211.52
 ---- batch: 080 ----
mean loss: 205.61
 ---- batch: 090 ----
mean loss: 212.85
train mean loss: 206.01
epoch train time: 0:00:01.658262
elapsed time: 0:03:10.299012
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 19:56:51.139201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.83
 ---- batch: 020 ----
mean loss: 204.58
 ---- batch: 030 ----
mean loss: 205.57
 ---- batch: 040 ----
mean loss: 203.35
 ---- batch: 050 ----
mean loss: 204.60
 ---- batch: 060 ----
mean loss: 206.06
 ---- batch: 070 ----
mean loss: 196.72
 ---- batch: 080 ----
mean loss: 203.58
 ---- batch: 090 ----
mean loss: 202.74
train mean loss: 204.14
epoch train time: 0:00:01.662937
elapsed time: 0:03:11.962662
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 19:56:52.802818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.35
 ---- batch: 020 ----
mean loss: 195.62
 ---- batch: 030 ----
mean loss: 202.15
 ---- batch: 040 ----
mean loss: 202.36
 ---- batch: 050 ----
mean loss: 199.74
 ---- batch: 060 ----
mean loss: 201.03
 ---- batch: 070 ----
mean loss: 210.28
 ---- batch: 080 ----
mean loss: 205.52
 ---- batch: 090 ----
mean loss: 198.52
train mean loss: 202.17
epoch train time: 0:00:01.632739
elapsed time: 0:03:13.595998
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 19:56:54.436141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.94
 ---- batch: 020 ----
mean loss: 195.30
 ---- batch: 030 ----
mean loss: 196.87
 ---- batch: 040 ----
mean loss: 203.28
 ---- batch: 050 ----
mean loss: 197.93
 ---- batch: 060 ----
mean loss: 196.64
 ---- batch: 070 ----
mean loss: 206.70
 ---- batch: 080 ----
mean loss: 199.55
 ---- batch: 090 ----
mean loss: 203.72
train mean loss: 200.09
epoch train time: 0:00:01.654394
elapsed time: 0:03:15.251082
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 19:56:56.091248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.93
 ---- batch: 020 ----
mean loss: 197.54
 ---- batch: 030 ----
mean loss: 192.19
 ---- batch: 040 ----
mean loss: 191.57
 ---- batch: 050 ----
mean loss: 201.06
 ---- batch: 060 ----
mean loss: 202.34
 ---- batch: 070 ----
mean loss: 204.62
 ---- batch: 080 ----
mean loss: 197.81
 ---- batch: 090 ----
mean loss: 200.80
train mean loss: 198.57
epoch train time: 0:00:01.673928
elapsed time: 0:03:16.925660
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 19:56:57.765928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.07
 ---- batch: 020 ----
mean loss: 190.01
 ---- batch: 030 ----
mean loss: 194.42
 ---- batch: 040 ----
mean loss: 189.21
 ---- batch: 050 ----
mean loss: 202.84
 ---- batch: 060 ----
mean loss: 200.84
 ---- batch: 070 ----
mean loss: 194.21
 ---- batch: 080 ----
mean loss: 202.38
 ---- batch: 090 ----
mean loss: 196.05
train mean loss: 196.91
epoch train time: 0:00:01.663645
elapsed time: 0:03:18.590030
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 19:56:59.430226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.28
 ---- batch: 020 ----
mean loss: 187.47
 ---- batch: 030 ----
mean loss: 190.60
 ---- batch: 040 ----
mean loss: 194.92
 ---- batch: 050 ----
mean loss: 198.19
 ---- batch: 060 ----
mean loss: 187.95
 ---- batch: 070 ----
mean loss: 193.29
 ---- batch: 080 ----
mean loss: 199.34
 ---- batch: 090 ----
mean loss: 198.59
train mean loss: 194.43
epoch train time: 0:00:01.636207
elapsed time: 0:03:20.226941
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 19:57:01.067122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.72
 ---- batch: 020 ----
mean loss: 191.64
 ---- batch: 030 ----
mean loss: 190.04
 ---- batch: 040 ----
mean loss: 193.32
 ---- batch: 050 ----
mean loss: 200.51
 ---- batch: 060 ----
mean loss: 202.51
 ---- batch: 070 ----
mean loss: 191.23
 ---- batch: 080 ----
mean loss: 190.38
 ---- batch: 090 ----
mean loss: 195.63
train mean loss: 194.46
epoch train time: 0:00:01.652498
elapsed time: 0:03:21.880201
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 19:57:02.720388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.01
 ---- batch: 020 ----
mean loss: 190.58
 ---- batch: 030 ----
mean loss: 189.28
 ---- batch: 040 ----
mean loss: 196.01
 ---- batch: 050 ----
mean loss: 191.22
 ---- batch: 060 ----
mean loss: 193.15
 ---- batch: 070 ----
mean loss: 189.56
 ---- batch: 080 ----
mean loss: 188.98
 ---- batch: 090 ----
mean loss: 196.19
train mean loss: 191.83
epoch train time: 0:00:01.666005
elapsed time: 0:03:23.546919
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 19:57:04.387104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.17
 ---- batch: 020 ----
mean loss: 193.27
 ---- batch: 030 ----
mean loss: 182.60
 ---- batch: 040 ----
mean loss: 187.49
 ---- batch: 050 ----
mean loss: 188.03
 ---- batch: 060 ----
mean loss: 193.62
 ---- batch: 070 ----
mean loss: 196.21
 ---- batch: 080 ----
mean loss: 179.73
 ---- batch: 090 ----
mean loss: 187.88
train mean loss: 189.48
epoch train time: 0:00:01.646697
elapsed time: 0:03:25.194366
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 19:57:06.034615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.81
 ---- batch: 020 ----
mean loss: 192.85
 ---- batch: 030 ----
mean loss: 181.44
 ---- batch: 040 ----
mean loss: 194.93
 ---- batch: 050 ----
mean loss: 188.11
 ---- batch: 060 ----
mean loss: 192.40
 ---- batch: 070 ----
mean loss: 183.02
 ---- batch: 080 ----
mean loss: 188.32
 ---- batch: 090 ----
mean loss: 187.30
train mean loss: 188.76
epoch train time: 0:00:01.648117
elapsed time: 0:03:26.843277
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 19:57:07.683445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.90
 ---- batch: 020 ----
mean loss: 183.22
 ---- batch: 030 ----
mean loss: 196.81
 ---- batch: 040 ----
mean loss: 182.30
 ---- batch: 050 ----
mean loss: 180.36
 ---- batch: 060 ----
mean loss: 192.66
 ---- batch: 070 ----
mean loss: 189.11
 ---- batch: 080 ----
mean loss: 192.25
 ---- batch: 090 ----
mean loss: 191.26
train mean loss: 187.72
epoch train time: 0:00:01.646076
elapsed time: 0:03:28.490136
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 19:57:09.330265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.45
 ---- batch: 020 ----
mean loss: 189.75
 ---- batch: 030 ----
mean loss: 176.62
 ---- batch: 040 ----
mean loss: 181.42
 ---- batch: 050 ----
mean loss: 187.98
 ---- batch: 060 ----
mean loss: 190.12
 ---- batch: 070 ----
mean loss: 186.06
 ---- batch: 080 ----
mean loss: 188.54
 ---- batch: 090 ----
mean loss: 188.68
train mean loss: 185.74
epoch train time: 0:00:01.674118
elapsed time: 0:03:30.165071
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 19:57:11.005070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.50
 ---- batch: 020 ----
mean loss: 182.57
 ---- batch: 030 ----
mean loss: 182.02
 ---- batch: 040 ----
mean loss: 190.26
 ---- batch: 050 ----
mean loss: 184.07
 ---- batch: 060 ----
mean loss: 186.95
 ---- batch: 070 ----
mean loss: 188.18
 ---- batch: 080 ----
mean loss: 191.07
 ---- batch: 090 ----
mean loss: 188.01
train mean loss: 185.07
epoch train time: 0:00:01.688391
elapsed time: 0:03:31.854119
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 19:57:12.694278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.38
 ---- batch: 020 ----
mean loss: 184.51
 ---- batch: 030 ----
mean loss: 178.07
 ---- batch: 040 ----
mean loss: 185.53
 ---- batch: 050 ----
mean loss: 182.31
 ---- batch: 060 ----
mean loss: 185.61
 ---- batch: 070 ----
mean loss: 185.44
 ---- batch: 080 ----
mean loss: 187.74
 ---- batch: 090 ----
mean loss: 184.17
train mean loss: 183.90
epoch train time: 0:00:01.661304
elapsed time: 0:03:33.516049
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 19:57:14.356202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.75
 ---- batch: 020 ----
mean loss: 181.13
 ---- batch: 030 ----
mean loss: 182.58
 ---- batch: 040 ----
mean loss: 173.14
 ---- batch: 050 ----
mean loss: 175.41
 ---- batch: 060 ----
mean loss: 189.31
 ---- batch: 070 ----
mean loss: 182.98
 ---- batch: 080 ----
mean loss: 187.55
 ---- batch: 090 ----
mean loss: 184.48
train mean loss: 182.75
epoch train time: 0:00:01.648020
elapsed time: 0:03:35.164678
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 19:57:16.004819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.17
 ---- batch: 020 ----
mean loss: 179.62
 ---- batch: 030 ----
mean loss: 180.48
 ---- batch: 040 ----
mean loss: 182.29
 ---- batch: 050 ----
mean loss: 180.46
 ---- batch: 060 ----
mean loss: 185.23
 ---- batch: 070 ----
mean loss: 186.19
 ---- batch: 080 ----
mean loss: 177.65
 ---- batch: 090 ----
mean loss: 180.82
train mean loss: 181.25
epoch train time: 0:00:01.639699
elapsed time: 0:03:36.805011
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 19:57:17.645165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.58
 ---- batch: 020 ----
mean loss: 173.10
 ---- batch: 030 ----
mean loss: 183.98
 ---- batch: 040 ----
mean loss: 184.20
 ---- batch: 050 ----
mean loss: 183.34
 ---- batch: 060 ----
mean loss: 178.73
 ---- batch: 070 ----
mean loss: 180.80
 ---- batch: 080 ----
mean loss: 177.08
 ---- batch: 090 ----
mean loss: 177.61
train mean loss: 179.25
epoch train time: 0:00:01.640251
elapsed time: 0:03:38.445953
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 19:57:19.286106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.30
 ---- batch: 020 ----
mean loss: 181.21
 ---- batch: 030 ----
mean loss: 178.89
 ---- batch: 040 ----
mean loss: 171.14
 ---- batch: 050 ----
mean loss: 177.43
 ---- batch: 060 ----
mean loss: 182.62
 ---- batch: 070 ----
mean loss: 185.53
 ---- batch: 080 ----
mean loss: 180.52
 ---- batch: 090 ----
mean loss: 173.01
train mean loss: 178.60
epoch train time: 0:00:01.653541
elapsed time: 0:03:40.100186
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 19:57:20.940318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.24
 ---- batch: 020 ----
mean loss: 174.33
 ---- batch: 030 ----
mean loss: 174.14
 ---- batch: 040 ----
mean loss: 177.99
 ---- batch: 050 ----
mean loss: 175.86
 ---- batch: 060 ----
mean loss: 179.68
 ---- batch: 070 ----
mean loss: 173.03
 ---- batch: 080 ----
mean loss: 181.42
 ---- batch: 090 ----
mean loss: 177.75
train mean loss: 177.49
epoch train time: 0:00:01.634024
elapsed time: 0:03:41.734881
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 19:57:22.575044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.49
 ---- batch: 020 ----
mean loss: 173.84
 ---- batch: 030 ----
mean loss: 177.49
 ---- batch: 040 ----
mean loss: 174.99
 ---- batch: 050 ----
mean loss: 172.76
 ---- batch: 060 ----
mean loss: 180.70
 ---- batch: 070 ----
mean loss: 177.66
 ---- batch: 080 ----
mean loss: 167.23
 ---- batch: 090 ----
mean loss: 180.06
train mean loss: 176.05
epoch train time: 0:00:01.684747
elapsed time: 0:03:43.420360
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 19:57:24.260525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.49
 ---- batch: 020 ----
mean loss: 176.57
 ---- batch: 030 ----
mean loss: 172.45
 ---- batch: 040 ----
mean loss: 165.98
 ---- batch: 050 ----
mean loss: 180.97
 ---- batch: 060 ----
mean loss: 170.59
 ---- batch: 070 ----
mean loss: 176.12
 ---- batch: 080 ----
mean loss: 173.98
 ---- batch: 090 ----
mean loss: 183.34
train mean loss: 174.80
epoch train time: 0:00:01.667147
elapsed time: 0:03:45.088184
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 19:57:25.928333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.42
 ---- batch: 020 ----
mean loss: 162.85
 ---- batch: 030 ----
mean loss: 171.78
 ---- batch: 040 ----
mean loss: 171.35
 ---- batch: 050 ----
mean loss: 175.26
 ---- batch: 060 ----
mean loss: 178.88
 ---- batch: 070 ----
mean loss: 177.94
 ---- batch: 080 ----
mean loss: 184.26
 ---- batch: 090 ----
mean loss: 183.39
train mean loss: 173.83
epoch train time: 0:00:01.641602
elapsed time: 0:03:46.730457
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 19:57:27.570611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.14
 ---- batch: 020 ----
mean loss: 158.93
 ---- batch: 030 ----
mean loss: 170.16
 ---- batch: 040 ----
mean loss: 171.11
 ---- batch: 050 ----
mean loss: 178.01
 ---- batch: 060 ----
mean loss: 172.13
 ---- batch: 070 ----
mean loss: 176.34
 ---- batch: 080 ----
mean loss: 176.30
 ---- batch: 090 ----
mean loss: 181.23
train mean loss: 172.76
epoch train time: 0:00:01.648733
elapsed time: 0:03:48.379823
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 19:57:29.219965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.45
 ---- batch: 020 ----
mean loss: 168.43
 ---- batch: 030 ----
mean loss: 173.33
 ---- batch: 040 ----
mean loss: 177.67
 ---- batch: 050 ----
mean loss: 174.88
 ---- batch: 060 ----
mean loss: 172.05
 ---- batch: 070 ----
mean loss: 172.76
 ---- batch: 080 ----
mean loss: 174.63
 ---- batch: 090 ----
mean loss: 167.47
train mean loss: 171.61
epoch train time: 0:00:01.643610
elapsed time: 0:03:50.024103
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 19:57:30.864254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.26
 ---- batch: 020 ----
mean loss: 168.15
 ---- batch: 030 ----
mean loss: 167.87
 ---- batch: 040 ----
mean loss: 168.86
 ---- batch: 050 ----
mean loss: 167.74
 ---- batch: 060 ----
mean loss: 173.93
 ---- batch: 070 ----
mean loss: 178.45
 ---- batch: 080 ----
mean loss: 175.15
 ---- batch: 090 ----
mean loss: 169.01
train mean loss: 171.04
epoch train time: 0:00:01.617650
elapsed time: 0:03:51.642394
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 19:57:32.482533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.04
 ---- batch: 020 ----
mean loss: 165.06
 ---- batch: 030 ----
mean loss: 168.36
 ---- batch: 040 ----
mean loss: 173.53
 ---- batch: 050 ----
mean loss: 174.95
 ---- batch: 060 ----
mean loss: 169.13
 ---- batch: 070 ----
mean loss: 166.97
 ---- batch: 080 ----
mean loss: 173.48
 ---- batch: 090 ----
mean loss: 175.48
train mean loss: 170.48
epoch train time: 0:00:01.687181
elapsed time: 0:03:53.330185
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 19:57:34.170365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.02
 ---- batch: 020 ----
mean loss: 167.53
 ---- batch: 030 ----
mean loss: 167.38
 ---- batch: 040 ----
mean loss: 165.59
 ---- batch: 050 ----
mean loss: 174.71
 ---- batch: 060 ----
mean loss: 167.73
 ---- batch: 070 ----
mean loss: 168.66
 ---- batch: 080 ----
mean loss: 173.15
 ---- batch: 090 ----
mean loss: 174.39
train mean loss: 168.85
epoch train time: 0:00:01.663729
elapsed time: 0:03:54.994654
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 19:57:35.834810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.58
 ---- batch: 020 ----
mean loss: 167.07
 ---- batch: 030 ----
mean loss: 165.73
 ---- batch: 040 ----
mean loss: 168.07
 ---- batch: 050 ----
mean loss: 170.81
 ---- batch: 060 ----
mean loss: 165.83
 ---- batch: 070 ----
mean loss: 166.21
 ---- batch: 080 ----
mean loss: 169.76
 ---- batch: 090 ----
mean loss: 164.77
train mean loss: 167.57
epoch train time: 0:00:01.680376
elapsed time: 0:03:56.675685
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 19:57:37.515858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.47
 ---- batch: 020 ----
mean loss: 158.64
 ---- batch: 030 ----
mean loss: 161.25
 ---- batch: 040 ----
mean loss: 167.99
 ---- batch: 050 ----
mean loss: 173.88
 ---- batch: 060 ----
mean loss: 169.32
 ---- batch: 070 ----
mean loss: 169.75
 ---- batch: 080 ----
mean loss: 170.74
 ---- batch: 090 ----
mean loss: 170.79
train mean loss: 167.14
epoch train time: 0:00:01.672702
elapsed time: 0:03:58.349151
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 19:57:39.189285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.65
 ---- batch: 020 ----
mean loss: 164.95
 ---- batch: 030 ----
mean loss: 158.61
 ---- batch: 040 ----
mean loss: 165.00
 ---- batch: 050 ----
mean loss: 161.62
 ---- batch: 060 ----
mean loss: 173.68
 ---- batch: 070 ----
mean loss: 166.02
 ---- batch: 080 ----
mean loss: 168.04
 ---- batch: 090 ----
mean loss: 164.46
train mean loss: 165.56
epoch train time: 0:00:01.678469
elapsed time: 0:04:00.028306
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 19:57:40.868514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.98
 ---- batch: 020 ----
mean loss: 162.43
 ---- batch: 030 ----
mean loss: 159.92
 ---- batch: 040 ----
mean loss: 159.66
 ---- batch: 050 ----
mean loss: 166.32
 ---- batch: 060 ----
mean loss: 164.62
 ---- batch: 070 ----
mean loss: 163.58
 ---- batch: 080 ----
mean loss: 169.44
 ---- batch: 090 ----
mean loss: 172.04
train mean loss: 164.21
epoch train time: 0:00:01.662680
elapsed time: 0:04:01.691930
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 19:57:42.532142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.85
 ---- batch: 020 ----
mean loss: 161.80
 ---- batch: 030 ----
mean loss: 159.77
 ---- batch: 040 ----
mean loss: 165.94
 ---- batch: 050 ----
mean loss: 158.78
 ---- batch: 060 ----
mean loss: 166.54
 ---- batch: 070 ----
mean loss: 170.63
 ---- batch: 080 ----
mean loss: 167.92
 ---- batch: 090 ----
mean loss: 171.67
train mean loss: 163.94
epoch train time: 0:00:01.678389
elapsed time: 0:04:03.371225
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 19:57:44.211212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.06
 ---- batch: 020 ----
mean loss: 163.73
 ---- batch: 030 ----
mean loss: 158.94
 ---- batch: 040 ----
mean loss: 159.77
 ---- batch: 050 ----
mean loss: 171.12
 ---- batch: 060 ----
mean loss: 162.79
 ---- batch: 070 ----
mean loss: 162.30
 ---- batch: 080 ----
mean loss: 160.39
 ---- batch: 090 ----
mean loss: 171.65
train mean loss: 162.84
epoch train time: 0:00:01.687393
elapsed time: 0:04:05.059152
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 19:57:45.899305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.46
 ---- batch: 020 ----
mean loss: 152.99
 ---- batch: 030 ----
mean loss: 156.57
 ---- batch: 040 ----
mean loss: 163.92
 ---- batch: 050 ----
mean loss: 160.29
 ---- batch: 060 ----
mean loss: 160.36
 ---- batch: 070 ----
mean loss: 170.79
 ---- batch: 080 ----
mean loss: 170.38
 ---- batch: 090 ----
mean loss: 166.13
train mean loss: 162.11
epoch train time: 0:00:01.660630
elapsed time: 0:04:06.720454
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 19:57:47.560640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.41
 ---- batch: 020 ----
mean loss: 158.17
 ---- batch: 030 ----
mean loss: 158.60
 ---- batch: 040 ----
mean loss: 158.63
 ---- batch: 050 ----
mean loss: 155.98
 ---- batch: 060 ----
mean loss: 158.14
 ---- batch: 070 ----
mean loss: 159.21
 ---- batch: 080 ----
mean loss: 176.47
 ---- batch: 090 ----
mean loss: 164.77
train mean loss: 161.80
epoch train time: 0:00:01.645544
elapsed time: 0:04:08.366649
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 19:57:49.206801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.73
 ---- batch: 020 ----
mean loss: 158.65
 ---- batch: 030 ----
mean loss: 160.14
 ---- batch: 040 ----
mean loss: 157.45
 ---- batch: 050 ----
mean loss: 159.51
 ---- batch: 060 ----
mean loss: 158.31
 ---- batch: 070 ----
mean loss: 160.18
 ---- batch: 080 ----
mean loss: 164.34
 ---- batch: 090 ----
mean loss: 167.12
train mean loss: 160.03
epoch train time: 0:00:01.609900
elapsed time: 0:04:09.977225
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 19:57:50.817448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.42
 ---- batch: 020 ----
mean loss: 155.76
 ---- batch: 030 ----
mean loss: 162.61
 ---- batch: 040 ----
mean loss: 160.54
 ---- batch: 050 ----
mean loss: 156.71
 ---- batch: 060 ----
mean loss: 155.63
 ---- batch: 070 ----
mean loss: 165.76
 ---- batch: 080 ----
mean loss: 162.66
 ---- batch: 090 ----
mean loss: 160.83
train mean loss: 160.43
epoch train time: 0:00:01.637660
elapsed time: 0:04:11.615727
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 19:57:52.455882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.13
 ---- batch: 020 ----
mean loss: 156.49
 ---- batch: 030 ----
mean loss: 158.70
 ---- batch: 040 ----
mean loss: 155.79
 ---- batch: 050 ----
mean loss: 151.22
 ---- batch: 060 ----
mean loss: 162.92
 ---- batch: 070 ----
mean loss: 159.17
 ---- batch: 080 ----
mean loss: 163.66
 ---- batch: 090 ----
mean loss: 163.13
train mean loss: 158.97
epoch train time: 0:00:01.666237
elapsed time: 0:04:13.282601
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 19:57:54.122754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.38
 ---- batch: 020 ----
mean loss: 148.82
 ---- batch: 030 ----
mean loss: 152.77
 ---- batch: 040 ----
mean loss: 153.77
 ---- batch: 050 ----
mean loss: 162.08
 ---- batch: 060 ----
mean loss: 153.54
 ---- batch: 070 ----
mean loss: 165.52
 ---- batch: 080 ----
mean loss: 162.73
 ---- batch: 090 ----
mean loss: 163.20
train mean loss: 158.16
epoch train time: 0:00:01.646934
elapsed time: 0:04:14.930176
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 19:57:55.770349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.23
 ---- batch: 020 ----
mean loss: 158.64
 ---- batch: 030 ----
mean loss: 159.70
 ---- batch: 040 ----
mean loss: 156.95
 ---- batch: 050 ----
mean loss: 155.34
 ---- batch: 060 ----
mean loss: 161.14
 ---- batch: 070 ----
mean loss: 167.49
 ---- batch: 080 ----
mean loss: 158.99
 ---- batch: 090 ----
mean loss: 151.63
train mean loss: 158.00
epoch train time: 0:00:01.629093
elapsed time: 0:04:16.559965
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 19:57:57.400135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.37
 ---- batch: 020 ----
mean loss: 150.51
 ---- batch: 030 ----
mean loss: 154.62
 ---- batch: 040 ----
mean loss: 158.60
 ---- batch: 050 ----
mean loss: 152.72
 ---- batch: 060 ----
mean loss: 151.21
 ---- batch: 070 ----
mean loss: 156.45
 ---- batch: 080 ----
mean loss: 167.92
 ---- batch: 090 ----
mean loss: 162.41
train mean loss: 156.81
epoch train time: 0:00:01.657190
elapsed time: 0:04:18.217819
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 19:57:59.058017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.40
 ---- batch: 020 ----
mean loss: 153.95
 ---- batch: 030 ----
mean loss: 151.01
 ---- batch: 040 ----
mean loss: 157.31
 ---- batch: 050 ----
mean loss: 154.18
 ---- batch: 060 ----
mean loss: 155.80
 ---- batch: 070 ----
mean loss: 156.16
 ---- batch: 080 ----
mean loss: 158.98
 ---- batch: 090 ----
mean loss: 158.04
train mean loss: 155.93
epoch train time: 0:00:01.642646
elapsed time: 0:04:19.861268
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 19:58:00.701575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.98
 ---- batch: 020 ----
mean loss: 153.53
 ---- batch: 030 ----
mean loss: 150.97
 ---- batch: 040 ----
mean loss: 157.22
 ---- batch: 050 ----
mean loss: 153.34
 ---- batch: 060 ----
mean loss: 160.16
 ---- batch: 070 ----
mean loss: 155.25
 ---- batch: 080 ----
mean loss: 158.29
 ---- batch: 090 ----
mean loss: 159.24
train mean loss: 155.73
epoch train time: 0:00:01.634878
elapsed time: 0:04:21.496910
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 19:58:02.337094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.23
 ---- batch: 020 ----
mean loss: 152.38
 ---- batch: 030 ----
mean loss: 153.57
 ---- batch: 040 ----
mean loss: 149.54
 ---- batch: 050 ----
mean loss: 154.84
 ---- batch: 060 ----
mean loss: 152.90
 ---- batch: 070 ----
mean loss: 150.95
 ---- batch: 080 ----
mean loss: 161.10
 ---- batch: 090 ----
mean loss: 165.31
train mean loss: 154.22
epoch train time: 0:00:01.646340
elapsed time: 0:04:23.143902
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 19:58:03.984099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.92
 ---- batch: 020 ----
mean loss: 148.16
 ---- batch: 030 ----
mean loss: 162.11
 ---- batch: 040 ----
mean loss: 153.53
 ---- batch: 050 ----
mean loss: 151.13
 ---- batch: 060 ----
mean loss: 155.05
 ---- batch: 070 ----
mean loss: 154.97
 ---- batch: 080 ----
mean loss: 160.85
 ---- batch: 090 ----
mean loss: 154.46
train mean loss: 153.54
epoch train time: 0:00:01.645085
elapsed time: 0:04:24.789701
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 19:58:05.629871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.61
 ---- batch: 020 ----
mean loss: 157.42
 ---- batch: 030 ----
mean loss: 150.94
 ---- batch: 040 ----
mean loss: 151.51
 ---- batch: 050 ----
mean loss: 151.08
 ---- batch: 060 ----
mean loss: 151.21
 ---- batch: 070 ----
mean loss: 149.89
 ---- batch: 080 ----
mean loss: 160.49
 ---- batch: 090 ----
mean loss: 157.92
train mean loss: 153.53
epoch train time: 0:00:01.630326
elapsed time: 0:04:26.420688
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 19:58:07.260824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.30
 ---- batch: 020 ----
mean loss: 151.74
 ---- batch: 030 ----
mean loss: 152.77
 ---- batch: 040 ----
mean loss: 143.61
 ---- batch: 050 ----
mean loss: 159.35
 ---- batch: 060 ----
mean loss: 150.44
 ---- batch: 070 ----
mean loss: 158.60
 ---- batch: 080 ----
mean loss: 151.14
 ---- batch: 090 ----
mean loss: 159.99
train mean loss: 152.79
epoch train time: 0:00:01.631944
elapsed time: 0:04:28.053260
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 19:58:08.893424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.26
 ---- batch: 020 ----
mean loss: 148.44
 ---- batch: 030 ----
mean loss: 143.33
 ---- batch: 040 ----
mean loss: 150.81
 ---- batch: 050 ----
mean loss: 158.80
 ---- batch: 060 ----
mean loss: 159.54
 ---- batch: 070 ----
mean loss: 149.13
 ---- batch: 080 ----
mean loss: 152.45
 ---- batch: 090 ----
mean loss: 149.25
train mean loss: 151.65
epoch train time: 0:00:01.678560
elapsed time: 0:04:29.732470
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 19:58:10.572640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.07
 ---- batch: 020 ----
mean loss: 154.01
 ---- batch: 030 ----
mean loss: 144.34
 ---- batch: 040 ----
mean loss: 155.79
 ---- batch: 050 ----
mean loss: 147.14
 ---- batch: 060 ----
mean loss: 147.79
 ---- batch: 070 ----
mean loss: 153.36
 ---- batch: 080 ----
mean loss: 150.73
 ---- batch: 090 ----
mean loss: 148.17
train mean loss: 151.33
epoch train time: 0:00:01.667510
elapsed time: 0:04:31.400590
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 19:58:12.240759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.04
 ---- batch: 020 ----
mean loss: 149.83
 ---- batch: 030 ----
mean loss: 150.84
 ---- batch: 040 ----
mean loss: 151.43
 ---- batch: 050 ----
mean loss: 146.62
 ---- batch: 060 ----
mean loss: 154.85
 ---- batch: 070 ----
mean loss: 152.71
 ---- batch: 080 ----
mean loss: 152.94
 ---- batch: 090 ----
mean loss: 151.43
train mean loss: 150.31
epoch train time: 0:00:01.661569
elapsed time: 0:04:33.062866
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 19:58:13.903025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.00
 ---- batch: 020 ----
mean loss: 149.99
 ---- batch: 030 ----
mean loss: 143.73
 ---- batch: 040 ----
mean loss: 151.47
 ---- batch: 050 ----
mean loss: 154.23
 ---- batch: 060 ----
mean loss: 151.75
 ---- batch: 070 ----
mean loss: 147.89
 ---- batch: 080 ----
mean loss: 150.78
 ---- batch: 090 ----
mean loss: 154.76
train mean loss: 149.58
epoch train time: 0:00:01.650888
elapsed time: 0:04:34.714446
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 19:58:15.554611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.56
 ---- batch: 020 ----
mean loss: 144.92
 ---- batch: 030 ----
mean loss: 147.80
 ---- batch: 040 ----
mean loss: 149.69
 ---- batch: 050 ----
mean loss: 151.36
 ---- batch: 060 ----
mean loss: 149.54
 ---- batch: 070 ----
mean loss: 148.02
 ---- batch: 080 ----
mean loss: 144.71
 ---- batch: 090 ----
mean loss: 152.24
train mean loss: 149.20
epoch train time: 0:00:01.639440
elapsed time: 0:04:36.354514
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 19:58:17.194659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.31
 ---- batch: 020 ----
mean loss: 150.53
 ---- batch: 030 ----
mean loss: 144.22
 ---- batch: 040 ----
mean loss: 144.48
 ---- batch: 050 ----
mean loss: 149.70
 ---- batch: 060 ----
mean loss: 148.40
 ---- batch: 070 ----
mean loss: 144.83
 ---- batch: 080 ----
mean loss: 156.82
 ---- batch: 090 ----
mean loss: 154.30
train mean loss: 148.47
epoch train time: 0:00:01.648105
elapsed time: 0:04:38.003317
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 19:58:18.843490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.02
 ---- batch: 020 ----
mean loss: 146.64
 ---- batch: 030 ----
mean loss: 146.69
 ---- batch: 040 ----
mean loss: 148.75
 ---- batch: 050 ----
mean loss: 139.76
 ---- batch: 060 ----
mean loss: 144.18
 ---- batch: 070 ----
mean loss: 153.04
 ---- batch: 080 ----
mean loss: 153.96
 ---- batch: 090 ----
mean loss: 150.31
train mean loss: 147.82
epoch train time: 0:00:01.641520
elapsed time: 0:04:39.645705
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 19:58:20.485720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.69
 ---- batch: 020 ----
mean loss: 140.58
 ---- batch: 030 ----
mean loss: 145.71
 ---- batch: 040 ----
mean loss: 150.78
 ---- batch: 050 ----
mean loss: 147.16
 ---- batch: 060 ----
mean loss: 154.15
 ---- batch: 070 ----
mean loss: 149.94
 ---- batch: 080 ----
mean loss: 143.88
 ---- batch: 090 ----
mean loss: 146.66
train mean loss: 147.06
epoch train time: 0:00:01.672563
elapsed time: 0:04:41.318782
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 19:58:22.158973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.56
 ---- batch: 020 ----
mean loss: 149.81
 ---- batch: 030 ----
mean loss: 143.51
 ---- batch: 040 ----
mean loss: 149.79
 ---- batch: 050 ----
mean loss: 142.60
 ---- batch: 060 ----
mean loss: 148.09
 ---- batch: 070 ----
mean loss: 141.84
 ---- batch: 080 ----
mean loss: 154.21
 ---- batch: 090 ----
mean loss: 145.74
train mean loss: 146.82
epoch train time: 0:00:01.653747
elapsed time: 0:04:42.973296
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 19:58:23.813462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.82
 ---- batch: 020 ----
mean loss: 148.54
 ---- batch: 030 ----
mean loss: 143.68
 ---- batch: 040 ----
mean loss: 146.87
 ---- batch: 050 ----
mean loss: 147.58
 ---- batch: 060 ----
mean loss: 146.73
 ---- batch: 070 ----
mean loss: 145.52
 ---- batch: 080 ----
mean loss: 149.50
 ---- batch: 090 ----
mean loss: 153.24
train mean loss: 146.67
epoch train time: 0:00:01.651428
elapsed time: 0:04:44.625330
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 19:58:25.465559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.34
 ---- batch: 020 ----
mean loss: 148.32
 ---- batch: 030 ----
mean loss: 147.60
 ---- batch: 040 ----
mean loss: 140.95
 ---- batch: 050 ----
mean loss: 145.57
 ---- batch: 060 ----
mean loss: 144.67
 ---- batch: 070 ----
mean loss: 146.17
 ---- batch: 080 ----
mean loss: 150.05
 ---- batch: 090 ----
mean loss: 148.43
train mean loss: 145.42
epoch train time: 0:00:01.658334
elapsed time: 0:04:46.284434
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 19:58:27.124581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.65
 ---- batch: 020 ----
mean loss: 136.70
 ---- batch: 030 ----
mean loss: 145.25
 ---- batch: 040 ----
mean loss: 148.12
 ---- batch: 050 ----
mean loss: 146.07
 ---- batch: 060 ----
mean loss: 142.69
 ---- batch: 070 ----
mean loss: 146.99
 ---- batch: 080 ----
mean loss: 142.90
 ---- batch: 090 ----
mean loss: 148.85
train mean loss: 145.43
epoch train time: 0:00:01.628954
elapsed time: 0:04:47.914052
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 19:58:28.754204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.81
 ---- batch: 020 ----
mean loss: 145.63
 ---- batch: 030 ----
mean loss: 136.39
 ---- batch: 040 ----
mean loss: 144.97
 ---- batch: 050 ----
mean loss: 149.11
 ---- batch: 060 ----
mean loss: 147.31
 ---- batch: 070 ----
mean loss: 142.05
 ---- batch: 080 ----
mean loss: 146.30
 ---- batch: 090 ----
mean loss: 148.67
train mean loss: 144.19
epoch train time: 0:00:01.636625
elapsed time: 0:04:49.551317
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 19:58:30.391485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.97
 ---- batch: 020 ----
mean loss: 140.25
 ---- batch: 030 ----
mean loss: 143.92
 ---- batch: 040 ----
mean loss: 140.79
 ---- batch: 050 ----
mean loss: 148.00
 ---- batch: 060 ----
mean loss: 144.78
 ---- batch: 070 ----
mean loss: 145.25
 ---- batch: 080 ----
mean loss: 144.37
 ---- batch: 090 ----
mean loss: 141.91
train mean loss: 143.53
epoch train time: 0:00:01.644362
elapsed time: 0:04:51.196389
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 19:58:32.036567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.86
 ---- batch: 020 ----
mean loss: 141.72
 ---- batch: 030 ----
mean loss: 149.71
 ---- batch: 040 ----
mean loss: 140.75
 ---- batch: 050 ----
mean loss: 140.10
 ---- batch: 060 ----
mean loss: 144.42
 ---- batch: 070 ----
mean loss: 145.79
 ---- batch: 080 ----
mean loss: 153.46
 ---- batch: 090 ----
mean loss: 144.39
train mean loss: 144.05
epoch train time: 0:00:01.646595
elapsed time: 0:04:52.843684
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 19:58:33.683856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.12
 ---- batch: 020 ----
mean loss: 141.86
 ---- batch: 030 ----
mean loss: 138.42
 ---- batch: 040 ----
mean loss: 137.78
 ---- batch: 050 ----
mean loss: 145.37
 ---- batch: 060 ----
mean loss: 150.01
 ---- batch: 070 ----
mean loss: 137.16
 ---- batch: 080 ----
mean loss: 142.44
 ---- batch: 090 ----
mean loss: 142.46
train mean loss: 142.38
epoch train time: 0:00:01.680109
elapsed time: 0:04:54.524487
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 19:58:35.364669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.12
 ---- batch: 020 ----
mean loss: 134.39
 ---- batch: 030 ----
mean loss: 142.87
 ---- batch: 040 ----
mean loss: 140.72
 ---- batch: 050 ----
mean loss: 142.01
 ---- batch: 060 ----
mean loss: 136.82
 ---- batch: 070 ----
mean loss: 146.35
 ---- batch: 080 ----
mean loss: 145.41
 ---- batch: 090 ----
mean loss: 143.14
train mean loss: 142.06
epoch train time: 0:00:01.677477
elapsed time: 0:04:56.202618
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 19:58:37.042779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.24
 ---- batch: 020 ----
mean loss: 135.62
 ---- batch: 030 ----
mean loss: 139.06
 ---- batch: 040 ----
mean loss: 147.58
 ---- batch: 050 ----
mean loss: 146.42
 ---- batch: 060 ----
mean loss: 140.30
 ---- batch: 070 ----
mean loss: 141.87
 ---- batch: 080 ----
mean loss: 138.39
 ---- batch: 090 ----
mean loss: 145.86
train mean loss: 141.51
epoch train time: 0:00:01.670949
elapsed time: 0:04:57.874288
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 19:58:38.714452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.12
 ---- batch: 020 ----
mean loss: 136.21
 ---- batch: 030 ----
mean loss: 136.51
 ---- batch: 040 ----
mean loss: 140.78
 ---- batch: 050 ----
mean loss: 139.26
 ---- batch: 060 ----
mean loss: 145.84
 ---- batch: 070 ----
mean loss: 144.23
 ---- batch: 080 ----
mean loss: 148.86
 ---- batch: 090 ----
mean loss: 139.96
train mean loss: 140.76
epoch train time: 0:00:01.679601
elapsed time: 0:04:59.554599
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 19:58:40.394754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.88
 ---- batch: 020 ----
mean loss: 137.45
 ---- batch: 030 ----
mean loss: 141.06
 ---- batch: 040 ----
mean loss: 136.31
 ---- batch: 050 ----
mean loss: 131.00
 ---- batch: 060 ----
mean loss: 137.31
 ---- batch: 070 ----
mean loss: 148.61
 ---- batch: 080 ----
mean loss: 142.28
 ---- batch: 090 ----
mean loss: 145.90
train mean loss: 140.19
epoch train time: 0:00:01.678069
elapsed time: 0:05:01.233311
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 19:58:42.073469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.65
 ---- batch: 020 ----
mean loss: 142.36
 ---- batch: 030 ----
mean loss: 144.77
 ---- batch: 040 ----
mean loss: 141.25
 ---- batch: 050 ----
mean loss: 136.88
 ---- batch: 060 ----
mean loss: 143.66
 ---- batch: 070 ----
mean loss: 142.65
 ---- batch: 080 ----
mean loss: 141.91
 ---- batch: 090 ----
mean loss: 135.88
train mean loss: 139.82
epoch train time: 0:00:01.644344
elapsed time: 0:05:02.878317
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 19:58:43.718461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.74
 ---- batch: 020 ----
mean loss: 135.12
 ---- batch: 030 ----
mean loss: 136.75
 ---- batch: 040 ----
mean loss: 139.68
 ---- batch: 050 ----
mean loss: 139.59
 ---- batch: 060 ----
mean loss: 144.19
 ---- batch: 070 ----
mean loss: 142.66
 ---- batch: 080 ----
mean loss: 139.73
 ---- batch: 090 ----
mean loss: 146.26
train mean loss: 140.26
epoch train time: 0:00:01.637563
elapsed time: 0:05:04.516549
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 19:58:45.356717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.52
 ---- batch: 020 ----
mean loss: 135.16
 ---- batch: 030 ----
mean loss: 139.11
 ---- batch: 040 ----
mean loss: 136.34
 ---- batch: 050 ----
mean loss: 140.63
 ---- batch: 060 ----
mean loss: 140.44
 ---- batch: 070 ----
mean loss: 141.02
 ---- batch: 080 ----
mean loss: 134.68
 ---- batch: 090 ----
mean loss: 142.21
train mean loss: 138.92
epoch train time: 0:00:01.649669
elapsed time: 0:05:06.166883
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 19:58:47.007030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.64
 ---- batch: 020 ----
mean loss: 135.88
 ---- batch: 030 ----
mean loss: 131.35
 ---- batch: 040 ----
mean loss: 142.46
 ---- batch: 050 ----
mean loss: 143.07
 ---- batch: 060 ----
mean loss: 140.36
 ---- batch: 070 ----
mean loss: 138.19
 ---- batch: 080 ----
mean loss: 139.84
 ---- batch: 090 ----
mean loss: 138.66
train mean loss: 138.40
epoch train time: 0:00:01.663136
elapsed time: 0:05:07.830642
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 19:58:48.670797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.37
 ---- batch: 020 ----
mean loss: 142.09
 ---- batch: 030 ----
mean loss: 142.05
 ---- batch: 040 ----
mean loss: 135.96
 ---- batch: 050 ----
mean loss: 135.74
 ---- batch: 060 ----
mean loss: 134.22
 ---- batch: 070 ----
mean loss: 132.71
 ---- batch: 080 ----
mean loss: 139.59
 ---- batch: 090 ----
mean loss: 142.63
train mean loss: 138.42
epoch train time: 0:00:01.645753
elapsed time: 0:05:09.477012
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 19:58:50.317175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.15
 ---- batch: 020 ----
mean loss: 135.20
 ---- batch: 030 ----
mean loss: 142.82
 ---- batch: 040 ----
mean loss: 133.59
 ---- batch: 050 ----
mean loss: 137.00
 ---- batch: 060 ----
mean loss: 134.52
 ---- batch: 070 ----
mean loss: 144.12
 ---- batch: 080 ----
mean loss: 133.95
 ---- batch: 090 ----
mean loss: 138.93
train mean loss: 137.73
epoch train time: 0:00:01.642091
elapsed time: 0:05:11.119773
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 19:58:51.959923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.05
 ---- batch: 020 ----
mean loss: 137.41
 ---- batch: 030 ----
mean loss: 129.04
 ---- batch: 040 ----
mean loss: 133.54
 ---- batch: 050 ----
mean loss: 135.59
 ---- batch: 060 ----
mean loss: 146.12
 ---- batch: 070 ----
mean loss: 142.20
 ---- batch: 080 ----
mean loss: 144.39
 ---- batch: 090 ----
mean loss: 135.97
train mean loss: 137.53
epoch train time: 0:00:01.663498
elapsed time: 0:05:12.784047
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 19:58:53.624226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.71
 ---- batch: 020 ----
mean loss: 134.64
 ---- batch: 030 ----
mean loss: 133.41
 ---- batch: 040 ----
mean loss: 135.51
 ---- batch: 050 ----
mean loss: 130.86
 ---- batch: 060 ----
mean loss: 141.87
 ---- batch: 070 ----
mean loss: 136.83
 ---- batch: 080 ----
mean loss: 135.65
 ---- batch: 090 ----
mean loss: 141.12
train mean loss: 136.50
epoch train time: 0:00:01.665101
elapsed time: 0:05:14.449840
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 19:58:55.290014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.94
 ---- batch: 020 ----
mean loss: 139.09
 ---- batch: 030 ----
mean loss: 133.10
 ---- batch: 040 ----
mean loss: 133.43
 ---- batch: 050 ----
mean loss: 142.65
 ---- batch: 060 ----
mean loss: 134.98
 ---- batch: 070 ----
mean loss: 138.78
 ---- batch: 080 ----
mean loss: 137.32
 ---- batch: 090 ----
mean loss: 135.13
train mean loss: 136.19
epoch train time: 0:00:01.636652
elapsed time: 0:05:16.087189
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 19:58:56.927348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.14
 ---- batch: 020 ----
mean loss: 127.50
 ---- batch: 030 ----
mean loss: 134.59
 ---- batch: 040 ----
mean loss: 133.37
 ---- batch: 050 ----
mean loss: 133.52
 ---- batch: 060 ----
mean loss: 136.49
 ---- batch: 070 ----
mean loss: 137.51
 ---- batch: 080 ----
mean loss: 136.60
 ---- batch: 090 ----
mean loss: 146.41
train mean loss: 135.44
epoch train time: 0:00:01.647711
elapsed time: 0:05:17.735546
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 19:58:58.575693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.42
 ---- batch: 020 ----
mean loss: 130.43
 ---- batch: 030 ----
mean loss: 137.66
 ---- batch: 040 ----
mean loss: 136.93
 ---- batch: 050 ----
mean loss: 134.03
 ---- batch: 060 ----
mean loss: 130.17
 ---- batch: 070 ----
mean loss: 137.99
 ---- batch: 080 ----
mean loss: 136.25
 ---- batch: 090 ----
mean loss: 135.89
train mean loss: 135.38
epoch train time: 0:00:01.652546
elapsed time: 0:05:19.388720
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 19:59:00.228925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.06
 ---- batch: 020 ----
mean loss: 127.83
 ---- batch: 030 ----
mean loss: 136.22
 ---- batch: 040 ----
mean loss: 133.24
 ---- batch: 050 ----
mean loss: 131.37
 ---- batch: 060 ----
mean loss: 143.04
 ---- batch: 070 ----
mean loss: 136.04
 ---- batch: 080 ----
mean loss: 134.46
 ---- batch: 090 ----
mean loss: 136.77
train mean loss: 134.41
epoch train time: 0:00:01.671959
elapsed time: 0:05:21.061683
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 19:59:01.901598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.79
 ---- batch: 020 ----
mean loss: 133.91
 ---- batch: 030 ----
mean loss: 136.73
 ---- batch: 040 ----
mean loss: 128.54
 ---- batch: 050 ----
mean loss: 130.75
 ---- batch: 060 ----
mean loss: 135.61
 ---- batch: 070 ----
mean loss: 128.67
 ---- batch: 080 ----
mean loss: 133.39
 ---- batch: 090 ----
mean loss: 138.57
train mean loss: 133.57
epoch train time: 0:00:01.671456
elapsed time: 0:05:22.733520
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 19:59:03.573681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.14
 ---- batch: 020 ----
mean loss: 127.27
 ---- batch: 030 ----
mean loss: 134.50
 ---- batch: 040 ----
mean loss: 134.70
 ---- batch: 050 ----
mean loss: 133.48
 ---- batch: 060 ----
mean loss: 135.40
 ---- batch: 070 ----
mean loss: 139.38
 ---- batch: 080 ----
mean loss: 135.53
 ---- batch: 090 ----
mean loss: 132.71
train mean loss: 133.73
epoch train time: 0:00:01.702520
elapsed time: 0:05:24.436807
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 19:59:05.276982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.95
 ---- batch: 020 ----
mean loss: 131.52
 ---- batch: 030 ----
mean loss: 130.09
 ---- batch: 040 ----
mean loss: 132.17
 ---- batch: 050 ----
mean loss: 139.85
 ---- batch: 060 ----
mean loss: 136.79
 ---- batch: 070 ----
mean loss: 132.40
 ---- batch: 080 ----
mean loss: 131.20
 ---- batch: 090 ----
mean loss: 140.34
train mean loss: 134.08
epoch train time: 0:00:01.720529
elapsed time: 0:05:26.158084
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 19:59:06.998243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.52
 ---- batch: 020 ----
mean loss: 127.87
 ---- batch: 030 ----
mean loss: 132.04
 ---- batch: 040 ----
mean loss: 128.12
 ---- batch: 050 ----
mean loss: 130.77
 ---- batch: 060 ----
mean loss: 133.08
 ---- batch: 070 ----
mean loss: 137.82
 ---- batch: 080 ----
mean loss: 135.84
 ---- batch: 090 ----
mean loss: 131.48
train mean loss: 132.10
epoch train time: 0:00:01.635757
elapsed time: 0:05:27.794526
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 19:59:08.634698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.64
 ---- batch: 020 ----
mean loss: 132.02
 ---- batch: 030 ----
mean loss: 128.74
 ---- batch: 040 ----
mean loss: 130.49
 ---- batch: 050 ----
mean loss: 135.39
 ---- batch: 060 ----
mean loss: 133.22
 ---- batch: 070 ----
mean loss: 133.34
 ---- batch: 080 ----
mean loss: 132.70
 ---- batch: 090 ----
mean loss: 135.11
train mean loss: 132.11
epoch train time: 0:00:01.628796
elapsed time: 0:05:29.424058
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 19:59:10.264202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.04
 ---- batch: 020 ----
mean loss: 127.25
 ---- batch: 030 ----
mean loss: 131.47
 ---- batch: 040 ----
mean loss: 130.83
 ---- batch: 050 ----
mean loss: 132.96
 ---- batch: 060 ----
mean loss: 135.53
 ---- batch: 070 ----
mean loss: 131.26
 ---- batch: 080 ----
mean loss: 135.52
 ---- batch: 090 ----
mean loss: 135.02
train mean loss: 132.27
epoch train time: 0:00:01.639401
elapsed time: 0:05:31.064210
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 19:59:11.904386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.14
 ---- batch: 020 ----
mean loss: 125.98
 ---- batch: 030 ----
mean loss: 128.77
 ---- batch: 040 ----
mean loss: 130.36
 ---- batch: 050 ----
mean loss: 135.22
 ---- batch: 060 ----
mean loss: 127.61
 ---- batch: 070 ----
mean loss: 129.21
 ---- batch: 080 ----
mean loss: 133.61
 ---- batch: 090 ----
mean loss: 134.81
train mean loss: 131.43
epoch train time: 0:00:01.677099
elapsed time: 0:05:32.741958
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 19:59:13.582115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.61
 ---- batch: 020 ----
mean loss: 126.59
 ---- batch: 030 ----
mean loss: 128.38
 ---- batch: 040 ----
mean loss: 132.54
 ---- batch: 050 ----
mean loss: 132.57
 ---- batch: 060 ----
mean loss: 137.48
 ---- batch: 070 ----
mean loss: 136.57
 ---- batch: 080 ----
mean loss: 137.75
 ---- batch: 090 ----
mean loss: 130.31
train mean loss: 131.80
epoch train time: 0:00:01.621842
elapsed time: 0:05:34.364434
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 19:59:15.204637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.09
 ---- batch: 020 ----
mean loss: 127.04
 ---- batch: 030 ----
mean loss: 132.72
 ---- batch: 040 ----
mean loss: 134.05
 ---- batch: 050 ----
mean loss: 130.47
 ---- batch: 060 ----
mean loss: 130.10
 ---- batch: 070 ----
mean loss: 132.19
 ---- batch: 080 ----
mean loss: 135.15
 ---- batch: 090 ----
mean loss: 129.67
train mean loss: 130.81
epoch train time: 0:00:01.642695
elapsed time: 0:05:36.007871
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 19:59:16.848032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.07
 ---- batch: 020 ----
mean loss: 125.96
 ---- batch: 030 ----
mean loss: 127.13
 ---- batch: 040 ----
mean loss: 131.01
 ---- batch: 050 ----
mean loss: 131.97
 ---- batch: 060 ----
mean loss: 130.71
 ---- batch: 070 ----
mean loss: 127.03
 ---- batch: 080 ----
mean loss: 134.54
 ---- batch: 090 ----
mean loss: 129.49
train mean loss: 130.64
epoch train time: 0:00:01.634693
elapsed time: 0:05:37.643208
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 19:59:18.483374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.99
 ---- batch: 020 ----
mean loss: 129.04
 ---- batch: 030 ----
mean loss: 127.03
 ---- batch: 040 ----
mean loss: 135.38
 ---- batch: 050 ----
mean loss: 131.42
 ---- batch: 060 ----
mean loss: 132.80
 ---- batch: 070 ----
mean loss: 125.58
 ---- batch: 080 ----
mean loss: 127.61
 ---- batch: 090 ----
mean loss: 129.65
train mean loss: 129.80
epoch train time: 0:00:01.675651
elapsed time: 0:05:39.319526
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 19:59:20.159699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.72
 ---- batch: 020 ----
mean loss: 122.71
 ---- batch: 030 ----
mean loss: 121.30
 ---- batch: 040 ----
mean loss: 130.69
 ---- batch: 050 ----
mean loss: 138.64
 ---- batch: 060 ----
mean loss: 132.00
 ---- batch: 070 ----
mean loss: 134.04
 ---- batch: 080 ----
mean loss: 130.09
 ---- batch: 090 ----
mean loss: 130.31
train mean loss: 129.71
epoch train time: 0:00:01.664282
elapsed time: 0:05:40.984506
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 19:59:21.824678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.93
 ---- batch: 020 ----
mean loss: 123.74
 ---- batch: 030 ----
mean loss: 129.97
 ---- batch: 040 ----
mean loss: 132.56
 ---- batch: 050 ----
mean loss: 122.06
 ---- batch: 060 ----
mean loss: 131.01
 ---- batch: 070 ----
mean loss: 136.95
 ---- batch: 080 ----
mean loss: 124.07
 ---- batch: 090 ----
mean loss: 137.62
train mean loss: 128.95
epoch train time: 0:00:01.641988
elapsed time: 0:05:42.627135
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 19:59:23.467289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.75
 ---- batch: 020 ----
mean loss: 128.47
 ---- batch: 030 ----
mean loss: 127.56
 ---- batch: 040 ----
mean loss: 126.11
 ---- batch: 050 ----
mean loss: 131.18
 ---- batch: 060 ----
mean loss: 127.51
 ---- batch: 070 ----
mean loss: 123.99
 ---- batch: 080 ----
mean loss: 133.97
 ---- batch: 090 ----
mean loss: 130.13
train mean loss: 128.39
epoch train time: 0:00:01.659192
elapsed time: 0:05:44.287084
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 19:59:25.127243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.71
 ---- batch: 020 ----
mean loss: 125.62
 ---- batch: 030 ----
mean loss: 119.35
 ---- batch: 040 ----
mean loss: 126.19
 ---- batch: 050 ----
mean loss: 130.38
 ---- batch: 060 ----
mean loss: 134.58
 ---- batch: 070 ----
mean loss: 128.82
 ---- batch: 080 ----
mean loss: 129.88
 ---- batch: 090 ----
mean loss: 131.61
train mean loss: 128.37
epoch train time: 0:00:01.666231
elapsed time: 0:05:45.954071
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 19:59:26.794296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.10
 ---- batch: 020 ----
mean loss: 124.48
 ---- batch: 030 ----
mean loss: 129.58
 ---- batch: 040 ----
mean loss: 126.82
 ---- batch: 050 ----
mean loss: 128.20
 ---- batch: 060 ----
mean loss: 125.65
 ---- batch: 070 ----
mean loss: 130.37
 ---- batch: 080 ----
mean loss: 137.49
 ---- batch: 090 ----
mean loss: 136.20
train mean loss: 128.73
epoch train time: 0:00:01.637386
elapsed time: 0:05:47.592321
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 19:59:28.432446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.48
 ---- batch: 020 ----
mean loss: 126.43
 ---- batch: 030 ----
mean loss: 126.45
 ---- batch: 040 ----
mean loss: 130.43
 ---- batch: 050 ----
mean loss: 130.73
 ---- batch: 060 ----
mean loss: 128.79
 ---- batch: 070 ----
mean loss: 127.10
 ---- batch: 080 ----
mean loss: 129.95
 ---- batch: 090 ----
mean loss: 122.78
train mean loss: 127.30
epoch train time: 0:00:01.656564
elapsed time: 0:05:49.249640
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 19:59:30.089862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.32
 ---- batch: 020 ----
mean loss: 121.29
 ---- batch: 030 ----
mean loss: 127.31
 ---- batch: 040 ----
mean loss: 124.96
 ---- batch: 050 ----
mean loss: 127.05
 ---- batch: 060 ----
mean loss: 125.72
 ---- batch: 070 ----
mean loss: 130.39
 ---- batch: 080 ----
mean loss: 135.08
 ---- batch: 090 ----
mean loss: 129.73
train mean loss: 126.93
epoch train time: 0:00:01.650439
elapsed time: 0:05:50.900850
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 19:59:31.740998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.86
 ---- batch: 020 ----
mean loss: 126.64
 ---- batch: 030 ----
mean loss: 125.98
 ---- batch: 040 ----
mean loss: 126.89
 ---- batch: 050 ----
mean loss: 128.40
 ---- batch: 060 ----
mean loss: 125.73
 ---- batch: 070 ----
mean loss: 125.68
 ---- batch: 080 ----
mean loss: 125.29
 ---- batch: 090 ----
mean loss: 130.33
train mean loss: 126.66
epoch train time: 0:00:01.663971
elapsed time: 0:05:52.565497
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 19:59:33.405677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.56
 ---- batch: 020 ----
mean loss: 124.83
 ---- batch: 030 ----
mean loss: 127.50
 ---- batch: 040 ----
mean loss: 121.28
 ---- batch: 050 ----
mean loss: 129.48
 ---- batch: 060 ----
mean loss: 117.68
 ---- batch: 070 ----
mean loss: 129.13
 ---- batch: 080 ----
mean loss: 131.46
 ---- batch: 090 ----
mean loss: 133.76
train mean loss: 126.96
epoch train time: 0:00:01.657974
elapsed time: 0:05:54.224159
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 19:59:35.064322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.20
 ---- batch: 020 ----
mean loss: 127.79
 ---- batch: 030 ----
mean loss: 124.21
 ---- batch: 040 ----
mean loss: 123.11
 ---- batch: 050 ----
mean loss: 124.14
 ---- batch: 060 ----
mean loss: 127.00
 ---- batch: 070 ----
mean loss: 126.49
 ---- batch: 080 ----
mean loss: 129.43
 ---- batch: 090 ----
mean loss: 126.07
train mean loss: 126.14
epoch train time: 0:00:01.632283
elapsed time: 0:05:55.857155
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 19:59:36.697323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.81
 ---- batch: 020 ----
mean loss: 124.82
 ---- batch: 030 ----
mean loss: 125.20
 ---- batch: 040 ----
mean loss: 124.97
 ---- batch: 050 ----
mean loss: 120.68
 ---- batch: 060 ----
mean loss: 125.85
 ---- batch: 070 ----
mean loss: 129.34
 ---- batch: 080 ----
mean loss: 126.14
 ---- batch: 090 ----
mean loss: 129.28
train mean loss: 125.65
epoch train time: 0:00:01.655619
elapsed time: 0:05:57.513427
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 19:59:38.353572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.25
 ---- batch: 020 ----
mean loss: 120.50
 ---- batch: 030 ----
mean loss: 121.62
 ---- batch: 040 ----
mean loss: 121.74
 ---- batch: 050 ----
mean loss: 116.73
 ---- batch: 060 ----
mean loss: 129.26
 ---- batch: 070 ----
mean loss: 128.59
 ---- batch: 080 ----
mean loss: 123.61
 ---- batch: 090 ----
mean loss: 134.10
train mean loss: 125.14
epoch train time: 0:00:01.667182
elapsed time: 0:05:59.181316
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 19:59:40.021489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.12
 ---- batch: 020 ----
mean loss: 125.30
 ---- batch: 030 ----
mean loss: 123.25
 ---- batch: 040 ----
mean loss: 119.47
 ---- batch: 050 ----
mean loss: 119.77
 ---- batch: 060 ----
mean loss: 128.31
 ---- batch: 070 ----
mean loss: 124.62
 ---- batch: 080 ----
mean loss: 127.54
 ---- batch: 090 ----
mean loss: 131.72
train mean loss: 124.90
epoch train time: 0:00:01.653598
elapsed time: 0:06:00.835630
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 19:59:41.675778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.50
 ---- batch: 020 ----
mean loss: 115.36
 ---- batch: 030 ----
mean loss: 122.49
 ---- batch: 040 ----
mean loss: 123.64
 ---- batch: 050 ----
mean loss: 129.22
 ---- batch: 060 ----
mean loss: 125.97
 ---- batch: 070 ----
mean loss: 129.12
 ---- batch: 080 ----
mean loss: 135.81
 ---- batch: 090 ----
mean loss: 126.37
train mean loss: 124.87
epoch train time: 0:00:01.673444
elapsed time: 0:06:02.509659
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 19:59:43.349853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.25
 ---- batch: 020 ----
mean loss: 118.85
 ---- batch: 030 ----
mean loss: 120.06
 ---- batch: 040 ----
mean loss: 123.37
 ---- batch: 050 ----
mean loss: 124.41
 ---- batch: 060 ----
mean loss: 128.65
 ---- batch: 070 ----
mean loss: 128.76
 ---- batch: 080 ----
mean loss: 130.93
 ---- batch: 090 ----
mean loss: 124.61
train mean loss: 124.38
epoch train time: 0:00:01.641045
elapsed time: 0:06:04.151402
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 19:59:44.991547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.55
 ---- batch: 020 ----
mean loss: 124.70
 ---- batch: 030 ----
mean loss: 111.95
 ---- batch: 040 ----
mean loss: 118.34
 ---- batch: 050 ----
mean loss: 123.77
 ---- batch: 060 ----
mean loss: 125.18
 ---- batch: 070 ----
mean loss: 130.16
 ---- batch: 080 ----
mean loss: 130.12
 ---- batch: 090 ----
mean loss: 130.00
train mean loss: 123.68
epoch train time: 0:00:01.641260
elapsed time: 0:06:05.793307
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 19:59:46.633468
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.42
 ---- batch: 020 ----
mean loss: 118.04
 ---- batch: 030 ----
mean loss: 114.41
 ---- batch: 040 ----
mean loss: 125.36
 ---- batch: 050 ----
mean loss: 116.90
 ---- batch: 060 ----
mean loss: 116.57
 ---- batch: 070 ----
mean loss: 116.72
 ---- batch: 080 ----
mean loss: 119.94
 ---- batch: 090 ----
mean loss: 117.02
train mean loss: 118.96
epoch train time: 0:00:01.657118
elapsed time: 0:06:07.451294
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 19:59:48.291216
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.58
 ---- batch: 020 ----
mean loss: 114.54
 ---- batch: 030 ----
mean loss: 116.74
 ---- batch: 040 ----
mean loss: 112.18
 ---- batch: 050 ----
mean loss: 120.87
 ---- batch: 060 ----
mean loss: 123.86
 ---- batch: 070 ----
mean loss: 114.79
 ---- batch: 080 ----
mean loss: 119.62
 ---- batch: 090 ----
mean loss: 111.74
train mean loss: 117.61
epoch train time: 0:00:01.640720
elapsed time: 0:06:09.092444
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 19:59:49.932607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.15
 ---- batch: 020 ----
mean loss: 115.98
 ---- batch: 030 ----
mean loss: 113.57
 ---- batch: 040 ----
mean loss: 121.74
 ---- batch: 050 ----
mean loss: 119.62
 ---- batch: 060 ----
mean loss: 113.87
 ---- batch: 070 ----
mean loss: 116.49
 ---- batch: 080 ----
mean loss: 121.13
 ---- batch: 090 ----
mean loss: 112.63
train mean loss: 117.10
epoch train time: 0:00:01.635899
elapsed time: 0:06:10.729066
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 19:59:51.569286
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.83
 ---- batch: 020 ----
mean loss: 116.31
 ---- batch: 030 ----
mean loss: 122.55
 ---- batch: 040 ----
mean loss: 110.17
 ---- batch: 050 ----
mean loss: 116.14
 ---- batch: 060 ----
mean loss: 115.96
 ---- batch: 070 ----
mean loss: 115.27
 ---- batch: 080 ----
mean loss: 125.61
 ---- batch: 090 ----
mean loss: 116.51
train mean loss: 116.94
epoch train time: 0:00:01.629630
elapsed time: 0:06:12.359373
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 19:59:53.199524
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.98
 ---- batch: 020 ----
mean loss: 124.94
 ---- batch: 030 ----
mean loss: 113.96
 ---- batch: 040 ----
mean loss: 109.98
 ---- batch: 050 ----
mean loss: 119.19
 ---- batch: 060 ----
mean loss: 117.80
 ---- batch: 070 ----
mean loss: 114.88
 ---- batch: 080 ----
mean loss: 122.48
 ---- batch: 090 ----
mean loss: 114.61
train mean loss: 116.99
epoch train time: 0:00:01.655812
elapsed time: 0:06:14.015858
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 19:59:54.856007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.83
 ---- batch: 020 ----
mean loss: 113.14
 ---- batch: 030 ----
mean loss: 114.26
 ---- batch: 040 ----
mean loss: 119.96
 ---- batch: 050 ----
mean loss: 116.57
 ---- batch: 060 ----
mean loss: 120.78
 ---- batch: 070 ----
mean loss: 117.82
 ---- batch: 080 ----
mean loss: 114.51
 ---- batch: 090 ----
mean loss: 119.27
train mean loss: 116.76
epoch train time: 0:00:01.650145
elapsed time: 0:06:15.666709
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 19:59:56.506897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.36
 ---- batch: 020 ----
mean loss: 110.81
 ---- batch: 030 ----
mean loss: 111.53
 ---- batch: 040 ----
mean loss: 120.67
 ---- batch: 050 ----
mean loss: 118.08
 ---- batch: 060 ----
mean loss: 115.72
 ---- batch: 070 ----
mean loss: 114.48
 ---- batch: 080 ----
mean loss: 121.60
 ---- batch: 090 ----
mean loss: 118.08
train mean loss: 116.81
epoch train time: 0:00:01.662861
elapsed time: 0:06:17.330240
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 19:59:58.170449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.35
 ---- batch: 020 ----
mean loss: 120.03
 ---- batch: 030 ----
mean loss: 119.78
 ---- batch: 040 ----
mean loss: 115.96
 ---- batch: 050 ----
mean loss: 118.70
 ---- batch: 060 ----
mean loss: 112.88
 ---- batch: 070 ----
mean loss: 113.48
 ---- batch: 080 ----
mean loss: 118.46
 ---- batch: 090 ----
mean loss: 112.89
train mean loss: 116.62
epoch train time: 0:00:01.644436
elapsed time: 0:06:18.975438
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 19:59:59.815612
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.61
 ---- batch: 020 ----
mean loss: 118.34
 ---- batch: 030 ----
mean loss: 112.90
 ---- batch: 040 ----
mean loss: 115.50
 ---- batch: 050 ----
mean loss: 119.52
 ---- batch: 060 ----
mean loss: 116.33
 ---- batch: 070 ----
mean loss: 113.79
 ---- batch: 080 ----
mean loss: 113.73
 ---- batch: 090 ----
mean loss: 116.45
train mean loss: 116.56
epoch train time: 0:00:01.656043
elapsed time: 0:06:20.632154
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 20:00:01.472313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.74
 ---- batch: 020 ----
mean loss: 115.25
 ---- batch: 030 ----
mean loss: 121.22
 ---- batch: 040 ----
mean loss: 116.71
 ---- batch: 050 ----
mean loss: 112.03
 ---- batch: 060 ----
mean loss: 121.32
 ---- batch: 070 ----
mean loss: 114.73
 ---- batch: 080 ----
mean loss: 121.35
 ---- batch: 090 ----
mean loss: 113.66
train mean loss: 116.52
epoch train time: 0:00:01.678990
elapsed time: 0:06:22.311830
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 20:00:03.152009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.66
 ---- batch: 020 ----
mean loss: 109.23
 ---- batch: 030 ----
mean loss: 118.31
 ---- batch: 040 ----
mean loss: 118.46
 ---- batch: 050 ----
mean loss: 115.31
 ---- batch: 060 ----
mean loss: 118.26
 ---- batch: 070 ----
mean loss: 117.08
 ---- batch: 080 ----
mean loss: 116.98
 ---- batch: 090 ----
mean loss: 117.77
train mean loss: 116.31
epoch train time: 0:00:01.670724
elapsed time: 0:06:23.983377
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 20:00:04.823535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.74
 ---- batch: 020 ----
mean loss: 108.82
 ---- batch: 030 ----
mean loss: 119.24
 ---- batch: 040 ----
mean loss: 118.35
 ---- batch: 050 ----
mean loss: 117.04
 ---- batch: 060 ----
mean loss: 114.45
 ---- batch: 070 ----
mean loss: 118.14
 ---- batch: 080 ----
mean loss: 115.48
 ---- batch: 090 ----
mean loss: 115.83
train mean loss: 116.50
epoch train time: 0:00:01.677740
elapsed time: 0:06:25.661852
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 20:00:06.501976
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.72
 ---- batch: 020 ----
mean loss: 114.74
 ---- batch: 030 ----
mean loss: 111.23
 ---- batch: 040 ----
mean loss: 119.38
 ---- batch: 050 ----
mean loss: 123.16
 ---- batch: 060 ----
mean loss: 118.03
 ---- batch: 070 ----
mean loss: 113.47
 ---- batch: 080 ----
mean loss: 117.17
 ---- batch: 090 ----
mean loss: 115.57
train mean loss: 116.45
epoch train time: 0:00:01.663064
elapsed time: 0:06:27.325517
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 20:00:08.165686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.01
 ---- batch: 020 ----
mean loss: 107.03
 ---- batch: 030 ----
mean loss: 116.15
 ---- batch: 040 ----
mean loss: 114.05
 ---- batch: 050 ----
mean loss: 115.33
 ---- batch: 060 ----
mean loss: 118.34
 ---- batch: 070 ----
mean loss: 118.35
 ---- batch: 080 ----
mean loss: 121.26
 ---- batch: 090 ----
mean loss: 118.23
train mean loss: 116.28
epoch train time: 0:00:01.684314
elapsed time: 0:06:29.010513
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 20:00:09.850671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.89
 ---- batch: 020 ----
mean loss: 117.06
 ---- batch: 030 ----
mean loss: 115.56
 ---- batch: 040 ----
mean loss: 113.56
 ---- batch: 050 ----
mean loss: 117.15
 ---- batch: 060 ----
mean loss: 117.57
 ---- batch: 070 ----
mean loss: 111.48
 ---- batch: 080 ----
mean loss: 119.87
 ---- batch: 090 ----
mean loss: 111.32
train mean loss: 116.31
epoch train time: 0:00:01.681044
elapsed time: 0:06:30.692250
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 20:00:11.532386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.78
 ---- batch: 020 ----
mean loss: 113.38
 ---- batch: 030 ----
mean loss: 118.94
 ---- batch: 040 ----
mean loss: 115.03
 ---- batch: 050 ----
mean loss: 116.15
 ---- batch: 060 ----
mean loss: 114.58
 ---- batch: 070 ----
mean loss: 121.15
 ---- batch: 080 ----
mean loss: 116.01
 ---- batch: 090 ----
mean loss: 117.18
train mean loss: 116.17
epoch train time: 0:00:01.676853
elapsed time: 0:06:32.369802
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 20:00:13.209966
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.55
 ---- batch: 020 ----
mean loss: 117.15
 ---- batch: 030 ----
mean loss: 123.67
 ---- batch: 040 ----
mean loss: 112.34
 ---- batch: 050 ----
mean loss: 114.97
 ---- batch: 060 ----
mean loss: 113.60
 ---- batch: 070 ----
mean loss: 115.57
 ---- batch: 080 ----
mean loss: 117.80
 ---- batch: 090 ----
mean loss: 121.81
train mean loss: 116.13
epoch train time: 0:00:01.662346
elapsed time: 0:06:34.032829
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 20:00:14.873021
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.29
 ---- batch: 020 ----
mean loss: 113.92
 ---- batch: 030 ----
mean loss: 119.76
 ---- batch: 040 ----
mean loss: 113.71
 ---- batch: 050 ----
mean loss: 115.20
 ---- batch: 060 ----
mean loss: 116.41
 ---- batch: 070 ----
mean loss: 110.87
 ---- batch: 080 ----
mean loss: 119.07
 ---- batch: 090 ----
mean loss: 122.05
train mean loss: 116.19
epoch train time: 0:00:01.634083
elapsed time: 0:06:35.667608
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 20:00:16.507786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.44
 ---- batch: 020 ----
mean loss: 114.86
 ---- batch: 030 ----
mean loss: 113.21
 ---- batch: 040 ----
mean loss: 120.20
 ---- batch: 050 ----
mean loss: 122.44
 ---- batch: 060 ----
mean loss: 110.85
 ---- batch: 070 ----
mean loss: 114.40
 ---- batch: 080 ----
mean loss: 116.97
 ---- batch: 090 ----
mean loss: 117.82
train mean loss: 116.17
epoch train time: 0:00:01.630751
elapsed time: 0:06:37.299035
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 20:00:18.139175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.05
 ---- batch: 020 ----
mean loss: 112.21
 ---- batch: 030 ----
mean loss: 115.32
 ---- batch: 040 ----
mean loss: 114.13
 ---- batch: 050 ----
mean loss: 115.95
 ---- batch: 060 ----
mean loss: 118.49
 ---- batch: 070 ----
mean loss: 115.82
 ---- batch: 080 ----
mean loss: 121.58
 ---- batch: 090 ----
mean loss: 113.81
train mean loss: 115.87
epoch train time: 0:00:01.614271
elapsed time: 0:06:38.913960
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 20:00:19.754170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.22
 ---- batch: 020 ----
mean loss: 115.11
 ---- batch: 030 ----
mean loss: 117.23
 ---- batch: 040 ----
mean loss: 116.45
 ---- batch: 050 ----
mean loss: 115.93
 ---- batch: 060 ----
mean loss: 120.47
 ---- batch: 070 ----
mean loss: 113.43
 ---- batch: 080 ----
mean loss: 118.40
 ---- batch: 090 ----
mean loss: 113.72
train mean loss: 115.87
epoch train time: 0:00:01.679266
elapsed time: 0:06:40.593968
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 20:00:21.434131
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.52
 ---- batch: 020 ----
mean loss: 116.33
 ---- batch: 030 ----
mean loss: 115.91
 ---- batch: 040 ----
mean loss: 110.48
 ---- batch: 050 ----
mean loss: 111.62
 ---- batch: 060 ----
mean loss: 119.01
 ---- batch: 070 ----
mean loss: 116.68
 ---- batch: 080 ----
mean loss: 119.40
 ---- batch: 090 ----
mean loss: 116.66
train mean loss: 116.02
epoch train time: 0:00:01.660455
elapsed time: 0:06:42.255133
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 20:00:23.095190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.21
 ---- batch: 020 ----
mean loss: 117.58
 ---- batch: 030 ----
mean loss: 119.93
 ---- batch: 040 ----
mean loss: 112.07
 ---- batch: 050 ----
mean loss: 117.18
 ---- batch: 060 ----
mean loss: 115.64
 ---- batch: 070 ----
mean loss: 114.98
 ---- batch: 080 ----
mean loss: 115.03
 ---- batch: 090 ----
mean loss: 119.31
train mean loss: 115.71
epoch train time: 0:00:01.674091
elapsed time: 0:06:43.929856
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 20:00:24.770083
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.38
 ---- batch: 020 ----
mean loss: 109.79
 ---- batch: 030 ----
mean loss: 119.07
 ---- batch: 040 ----
mean loss: 111.70
 ---- batch: 050 ----
mean loss: 111.64
 ---- batch: 060 ----
mean loss: 111.02
 ---- batch: 070 ----
mean loss: 117.69
 ---- batch: 080 ----
mean loss: 126.46
 ---- batch: 090 ----
mean loss: 120.17
train mean loss: 115.78
epoch train time: 0:00:01.614491
elapsed time: 0:06:45.545119
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 20:00:26.385265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.32
 ---- batch: 020 ----
mean loss: 120.67
 ---- batch: 030 ----
mean loss: 112.73
 ---- batch: 040 ----
mean loss: 119.27
 ---- batch: 050 ----
mean loss: 118.02
 ---- batch: 060 ----
mean loss: 115.95
 ---- batch: 070 ----
mean loss: 111.81
 ---- batch: 080 ----
mean loss: 115.36
 ---- batch: 090 ----
mean loss: 114.31
train mean loss: 115.77
epoch train time: 0:00:01.659838
elapsed time: 0:06:47.205622
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 20:00:28.045798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.64
 ---- batch: 020 ----
mean loss: 115.69
 ---- batch: 030 ----
mean loss: 120.93
 ---- batch: 040 ----
mean loss: 116.40
 ---- batch: 050 ----
mean loss: 118.43
 ---- batch: 060 ----
mean loss: 114.81
 ---- batch: 070 ----
mean loss: 110.89
 ---- batch: 080 ----
mean loss: 118.05
 ---- batch: 090 ----
mean loss: 117.38
train mean loss: 115.73
epoch train time: 0:00:01.678935
elapsed time: 0:06:48.885340
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 20:00:29.725552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.31
 ---- batch: 020 ----
mean loss: 120.97
 ---- batch: 030 ----
mean loss: 118.84
 ---- batch: 040 ----
mean loss: 120.93
 ---- batch: 050 ----
mean loss: 118.92
 ---- batch: 060 ----
mean loss: 110.94
 ---- batch: 070 ----
mean loss: 114.19
 ---- batch: 080 ----
mean loss: 113.17
 ---- batch: 090 ----
mean loss: 111.77
train mean loss: 115.72
epoch train time: 0:00:01.662975
elapsed time: 0:06:50.549010
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 20:00:31.389177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.66
 ---- batch: 020 ----
mean loss: 114.23
 ---- batch: 030 ----
mean loss: 117.48
 ---- batch: 040 ----
mean loss: 120.40
 ---- batch: 050 ----
mean loss: 120.00
 ---- batch: 060 ----
mean loss: 113.56
 ---- batch: 070 ----
mean loss: 120.01
 ---- batch: 080 ----
mean loss: 109.53
 ---- batch: 090 ----
mean loss: 117.68
train mean loss: 115.49
epoch train time: 0:00:01.660210
elapsed time: 0:06:52.209932
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 20:00:33.050089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.92
 ---- batch: 020 ----
mean loss: 113.15
 ---- batch: 030 ----
mean loss: 113.38
 ---- batch: 040 ----
mean loss: 115.85
 ---- batch: 050 ----
mean loss: 117.73
 ---- batch: 060 ----
mean loss: 116.17
 ---- batch: 070 ----
mean loss: 114.80
 ---- batch: 080 ----
mean loss: 108.68
 ---- batch: 090 ----
mean loss: 119.98
train mean loss: 115.45
epoch train time: 0:00:01.661113
elapsed time: 0:06:53.871750
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 20:00:34.711897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.95
 ---- batch: 020 ----
mean loss: 115.98
 ---- batch: 030 ----
mean loss: 119.89
 ---- batch: 040 ----
mean loss: 110.56
 ---- batch: 050 ----
mean loss: 109.56
 ---- batch: 060 ----
mean loss: 116.17
 ---- batch: 070 ----
mean loss: 116.41
 ---- batch: 080 ----
mean loss: 119.41
 ---- batch: 090 ----
mean loss: 111.73
train mean loss: 115.53
epoch train time: 0:00:01.640191
elapsed time: 0:06:55.512540
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 20:00:36.352680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.99
 ---- batch: 020 ----
mean loss: 117.69
 ---- batch: 030 ----
mean loss: 119.08
 ---- batch: 040 ----
mean loss: 115.71
 ---- batch: 050 ----
mean loss: 113.64
 ---- batch: 060 ----
mean loss: 109.59
 ---- batch: 070 ----
mean loss: 115.45
 ---- batch: 080 ----
mean loss: 114.57
 ---- batch: 090 ----
mean loss: 115.02
train mean loss: 115.41
epoch train time: 0:00:01.656265
elapsed time: 0:06:57.169474
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 20:00:38.009641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.53
 ---- batch: 020 ----
mean loss: 113.23
 ---- batch: 030 ----
mean loss: 114.43
 ---- batch: 040 ----
mean loss: 113.69
 ---- batch: 050 ----
mean loss: 114.75
 ---- batch: 060 ----
mean loss: 117.60
 ---- batch: 070 ----
mean loss: 119.01
 ---- batch: 080 ----
mean loss: 116.49
 ---- batch: 090 ----
mean loss: 114.91
train mean loss: 115.24
epoch train time: 0:00:01.631292
elapsed time: 0:06:58.801473
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 20:00:39.641635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.58
 ---- batch: 020 ----
mean loss: 110.21
 ---- batch: 030 ----
mean loss: 111.00
 ---- batch: 040 ----
mean loss: 113.82
 ---- batch: 050 ----
mean loss: 111.74
 ---- batch: 060 ----
mean loss: 114.05
 ---- batch: 070 ----
mean loss: 116.61
 ---- batch: 080 ----
mean loss: 121.88
 ---- batch: 090 ----
mean loss: 116.83
train mean loss: 115.58
epoch train time: 0:00:01.630067
elapsed time: 0:07:00.432486
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 20:00:41.272406
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.13
 ---- batch: 020 ----
mean loss: 115.49
 ---- batch: 030 ----
mean loss: 109.94
 ---- batch: 040 ----
mean loss: 117.76
 ---- batch: 050 ----
mean loss: 117.66
 ---- batch: 060 ----
mean loss: 114.66
 ---- batch: 070 ----
mean loss: 114.42
 ---- batch: 080 ----
mean loss: 117.18
 ---- batch: 090 ----
mean loss: 115.42
train mean loss: 115.41
epoch train time: 0:00:01.658865
elapsed time: 0:07:02.091745
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 20:00:42.931923
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.34
 ---- batch: 020 ----
mean loss: 111.32
 ---- batch: 030 ----
mean loss: 114.61
 ---- batch: 040 ----
mean loss: 110.63
 ---- batch: 050 ----
mean loss: 120.28
 ---- batch: 060 ----
mean loss: 114.76
 ---- batch: 070 ----
mean loss: 117.26
 ---- batch: 080 ----
mean loss: 115.37
 ---- batch: 090 ----
mean loss: 116.34
train mean loss: 115.04
epoch train time: 0:00:01.644817
elapsed time: 0:07:03.737253
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 20:00:44.577399
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.99
 ---- batch: 020 ----
mean loss: 117.48
 ---- batch: 030 ----
mean loss: 115.44
 ---- batch: 040 ----
mean loss: 114.98
 ---- batch: 050 ----
mean loss: 112.09
 ---- batch: 060 ----
mean loss: 111.20
 ---- batch: 070 ----
mean loss: 112.76
 ---- batch: 080 ----
mean loss: 116.71
 ---- batch: 090 ----
mean loss: 120.38
train mean loss: 115.17
epoch train time: 0:00:01.655401
elapsed time: 0:07:05.393351
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 20:00:46.233520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.12
 ---- batch: 020 ----
mean loss: 112.07
 ---- batch: 030 ----
mean loss: 123.10
 ---- batch: 040 ----
mean loss: 107.38
 ---- batch: 050 ----
mean loss: 115.53
 ---- batch: 060 ----
mean loss: 110.03
 ---- batch: 070 ----
mean loss: 114.36
 ---- batch: 080 ----
mean loss: 119.73
 ---- batch: 090 ----
mean loss: 115.05
train mean loss: 115.36
epoch train time: 0:00:01.669788
elapsed time: 0:07:07.063785
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 20:00:47.903943
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.80
 ---- batch: 020 ----
mean loss: 113.13
 ---- batch: 030 ----
mean loss: 119.19
 ---- batch: 040 ----
mean loss: 112.18
 ---- batch: 050 ----
mean loss: 114.87
 ---- batch: 060 ----
mean loss: 114.80
 ---- batch: 070 ----
mean loss: 120.56
 ---- batch: 080 ----
mean loss: 114.28
 ---- batch: 090 ----
mean loss: 110.75
train mean loss: 115.17
epoch train time: 0:00:01.674108
elapsed time: 0:07:08.738563
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 20:00:49.578716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.53
 ---- batch: 020 ----
mean loss: 117.77
 ---- batch: 030 ----
mean loss: 115.62
 ---- batch: 040 ----
mean loss: 114.11
 ---- batch: 050 ----
mean loss: 118.18
 ---- batch: 060 ----
mean loss: 111.88
 ---- batch: 070 ----
mean loss: 109.74
 ---- batch: 080 ----
mean loss: 120.89
 ---- batch: 090 ----
mean loss: 116.47
train mean loss: 115.11
epoch train time: 0:00:01.642585
elapsed time: 0:07:10.381839
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 20:00:51.222004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.76
 ---- batch: 020 ----
mean loss: 117.23
 ---- batch: 030 ----
mean loss: 111.87
 ---- batch: 040 ----
mean loss: 112.74
 ---- batch: 050 ----
mean loss: 113.52
 ---- batch: 060 ----
mean loss: 109.57
 ---- batch: 070 ----
mean loss: 122.30
 ---- batch: 080 ----
mean loss: 116.23
 ---- batch: 090 ----
mean loss: 110.02
train mean loss: 115.04
epoch train time: 0:00:01.654812
elapsed time: 0:07:12.037336
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 20:00:52.877562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.53
 ---- batch: 020 ----
mean loss: 110.30
 ---- batch: 030 ----
mean loss: 115.80
 ---- batch: 040 ----
mean loss: 112.86
 ---- batch: 050 ----
mean loss: 117.54
 ---- batch: 060 ----
mean loss: 111.42
 ---- batch: 070 ----
mean loss: 119.62
 ---- batch: 080 ----
mean loss: 113.61
 ---- batch: 090 ----
mean loss: 111.47
train mean loss: 115.10
epoch train time: 0:00:01.686104
elapsed time: 0:07:13.724143
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 20:00:54.564304
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.84
 ---- batch: 020 ----
mean loss: 114.16
 ---- batch: 030 ----
mean loss: 112.60
 ---- batch: 040 ----
mean loss: 111.71
 ---- batch: 050 ----
mean loss: 111.99
 ---- batch: 060 ----
mean loss: 117.24
 ---- batch: 070 ----
mean loss: 117.69
 ---- batch: 080 ----
mean loss: 116.80
 ---- batch: 090 ----
mean loss: 115.53
train mean loss: 114.84
epoch train time: 0:00:01.632645
elapsed time: 0:07:15.357458
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 20:00:56.197645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.78
 ---- batch: 020 ----
mean loss: 112.28
 ---- batch: 030 ----
mean loss: 113.50
 ---- batch: 040 ----
mean loss: 110.94
 ---- batch: 050 ----
mean loss: 111.28
 ---- batch: 060 ----
mean loss: 119.85
 ---- batch: 070 ----
mean loss: 120.19
 ---- batch: 080 ----
mean loss: 111.67
 ---- batch: 090 ----
mean loss: 121.44
train mean loss: 114.95
epoch train time: 0:00:01.679143
elapsed time: 0:07:17.037380
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 20:00:57.877546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.94
 ---- batch: 020 ----
mean loss: 121.82
 ---- batch: 030 ----
mean loss: 114.43
 ---- batch: 040 ----
mean loss: 110.34
 ---- batch: 050 ----
mean loss: 115.03
 ---- batch: 060 ----
mean loss: 112.56
 ---- batch: 070 ----
mean loss: 115.81
 ---- batch: 080 ----
mean loss: 112.15
 ---- batch: 090 ----
mean loss: 117.57
train mean loss: 114.80
epoch train time: 0:00:01.654211
elapsed time: 0:07:18.692303
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 20:00:59.532468
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.02
 ---- batch: 020 ----
mean loss: 112.75
 ---- batch: 030 ----
mean loss: 113.06
 ---- batch: 040 ----
mean loss: 113.85
 ---- batch: 050 ----
mean loss: 114.38
 ---- batch: 060 ----
mean loss: 117.94
 ---- batch: 070 ----
mean loss: 116.59
 ---- batch: 080 ----
mean loss: 120.68
 ---- batch: 090 ----
mean loss: 117.53
train mean loss: 114.56
epoch train time: 0:00:01.644403
elapsed time: 0:07:20.337382
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 20:01:01.177536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.51
 ---- batch: 020 ----
mean loss: 114.19
 ---- batch: 030 ----
mean loss: 112.97
 ---- batch: 040 ----
mean loss: 118.66
 ---- batch: 050 ----
mean loss: 111.88
 ---- batch: 060 ----
mean loss: 116.14
 ---- batch: 070 ----
mean loss: 113.89
 ---- batch: 080 ----
mean loss: 109.66
 ---- batch: 090 ----
mean loss: 117.68
train mean loss: 114.78
epoch train time: 0:00:01.659879
elapsed time: 0:07:21.997916
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 20:01:02.838161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.94
 ---- batch: 020 ----
mean loss: 113.41
 ---- batch: 030 ----
mean loss: 111.83
 ---- batch: 040 ----
mean loss: 117.02
 ---- batch: 050 ----
mean loss: 116.94
 ---- batch: 060 ----
mean loss: 115.82
 ---- batch: 070 ----
mean loss: 112.22
 ---- batch: 080 ----
mean loss: 117.34
 ---- batch: 090 ----
mean loss: 114.78
train mean loss: 114.59
epoch train time: 0:00:01.643101
elapsed time: 0:07:23.641777
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 20:01:04.481941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.24
 ---- batch: 020 ----
mean loss: 112.90
 ---- batch: 030 ----
mean loss: 113.18
 ---- batch: 040 ----
mean loss: 115.49
 ---- batch: 050 ----
mean loss: 116.43
 ---- batch: 060 ----
mean loss: 119.42
 ---- batch: 070 ----
mean loss: 119.38
 ---- batch: 080 ----
mean loss: 111.03
 ---- batch: 090 ----
mean loss: 111.62
train mean loss: 114.56
epoch train time: 0:00:01.654355
elapsed time: 0:07:25.296829
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 20:01:06.137000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.92
 ---- batch: 020 ----
mean loss: 114.16
 ---- batch: 030 ----
mean loss: 116.12
 ---- batch: 040 ----
mean loss: 106.89
 ---- batch: 050 ----
mean loss: 113.50
 ---- batch: 060 ----
mean loss: 117.50
 ---- batch: 070 ----
mean loss: 114.71
 ---- batch: 080 ----
mean loss: 117.07
 ---- batch: 090 ----
mean loss: 117.24
train mean loss: 114.55
epoch train time: 0:00:01.653395
elapsed time: 0:07:26.959298
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_4/checkpoint.pth.tar
**** end time: 2019-09-25 20:01:07.799166 ****
