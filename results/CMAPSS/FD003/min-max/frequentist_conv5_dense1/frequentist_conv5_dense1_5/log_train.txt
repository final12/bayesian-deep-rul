Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7106
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 22:45:32.380567 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 22:45:32.388191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4060.70
 ---- batch: 020 ----
mean loss: 1523.21
 ---- batch: 030 ----
mean loss: 419.85
 ---- batch: 040 ----
mean loss: 444.63
train mean loss: 1526.57
epoch train time: 0:00:15.630798
elapsed time: 0:00:15.640864
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 22:45:48.021476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.51
 ---- batch: 020 ----
mean loss: 346.28
 ---- batch: 030 ----
mean loss: 315.02
 ---- batch: 040 ----
mean loss: 308.46
train mean loss: 328.19
epoch train time: 0:00:01.839115
elapsed time: 0:00:17.480120
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 22:45:49.860755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.80
 ---- batch: 020 ----
mean loss: 299.44
 ---- batch: 030 ----
mean loss: 285.46
 ---- batch: 040 ----
mean loss: 280.22
train mean loss: 289.37
epoch train time: 0:00:01.720958
elapsed time: 0:00:19.201237
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 22:45:51.581854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.11
 ---- batch: 020 ----
mean loss: 251.75
 ---- batch: 030 ----
mean loss: 248.20
 ---- batch: 040 ----
mean loss: 239.08
train mean loss: 249.98
epoch train time: 0:00:01.736835
elapsed time: 0:00:20.938222
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 22:45:53.318839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.63
 ---- batch: 020 ----
mean loss: 237.99
 ---- batch: 030 ----
mean loss: 223.21
 ---- batch: 040 ----
mean loss: 216.22
train mean loss: 227.18
epoch train time: 0:00:01.672407
elapsed time: 0:00:22.610828
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 22:45:54.991447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.89
 ---- batch: 020 ----
mean loss: 209.74
 ---- batch: 030 ----
mean loss: 213.64
 ---- batch: 040 ----
mean loss: 210.15
train mean loss: 214.07
epoch train time: 0:00:01.777559
elapsed time: 0:00:24.388539
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 22:45:56.769155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.49
 ---- batch: 020 ----
mean loss: 221.01
 ---- batch: 030 ----
mean loss: 217.24
 ---- batch: 040 ----
mean loss: 209.57
train mean loss: 213.47
epoch train time: 0:00:01.671528
elapsed time: 0:00:26.060215
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 22:45:58.440832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.29
 ---- batch: 020 ----
mean loss: 215.98
 ---- batch: 030 ----
mean loss: 205.17
 ---- batch: 040 ----
mean loss: 198.09
train mean loss: 208.92
epoch train time: 0:00:01.773413
elapsed time: 0:00:27.833780
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 22:46:00.214396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.70
 ---- batch: 020 ----
mean loss: 202.49
 ---- batch: 030 ----
mean loss: 195.71
 ---- batch: 040 ----
mean loss: 210.21
train mean loss: 204.29
epoch train time: 0:00:01.672187
elapsed time: 0:00:29.506121
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 22:46:01.886737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.40
 ---- batch: 020 ----
mean loss: 199.05
 ---- batch: 030 ----
mean loss: 197.33
 ---- batch: 040 ----
mean loss: 209.57
train mean loss: 203.92
epoch train time: 0:00:01.662136
elapsed time: 0:00:31.168404
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 22:46:03.549022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.15
 ---- batch: 020 ----
mean loss: 203.27
 ---- batch: 030 ----
mean loss: 198.57
 ---- batch: 040 ----
mean loss: 199.96
train mean loss: 201.77
epoch train time: 0:00:01.784664
elapsed time: 0:00:32.953205
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 22:46:05.333820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.97
 ---- batch: 020 ----
mean loss: 206.68
 ---- batch: 030 ----
mean loss: 195.79
 ---- batch: 040 ----
mean loss: 200.94
train mean loss: 200.84
epoch train time: 0:00:01.671692
elapsed time: 0:00:34.625043
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 22:46:07.005673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.75
 ---- batch: 020 ----
mean loss: 201.26
 ---- batch: 030 ----
mean loss: 193.39
 ---- batch: 040 ----
mean loss: 195.26
train mean loss: 199.19
epoch train time: 0:00:01.779992
elapsed time: 0:00:36.405190
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 22:46:08.785838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.59
 ---- batch: 020 ----
mean loss: 196.06
 ---- batch: 030 ----
mean loss: 197.71
 ---- batch: 040 ----
mean loss: 201.83
train mean loss: 201.50
epoch train time: 0:00:01.675837
elapsed time: 0:00:38.081201
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 22:46:10.461818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.29
 ---- batch: 020 ----
mean loss: 200.95
 ---- batch: 030 ----
mean loss: 191.71
 ---- batch: 040 ----
mean loss: 198.93
train mean loss: 197.69
epoch train time: 0:00:01.741532
elapsed time: 0:00:39.822892
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 22:46:12.203507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.85
 ---- batch: 020 ----
mean loss: 190.78
 ---- batch: 030 ----
mean loss: 202.44
 ---- batch: 040 ----
mean loss: 201.55
train mean loss: 199.07
epoch train time: 0:00:01.701439
elapsed time: 0:00:41.524476
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 22:46:13.905097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.18
 ---- batch: 020 ----
mean loss: 199.23
 ---- batch: 030 ----
mean loss: 200.33
 ---- batch: 040 ----
mean loss: 191.54
train mean loss: 197.78
epoch train time: 0:00:01.673313
elapsed time: 0:00:43.197996
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 22:46:15.578640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.27
 ---- batch: 020 ----
mean loss: 195.42
 ---- batch: 030 ----
mean loss: 200.17
 ---- batch: 040 ----
mean loss: 200.99
train mean loss: 198.17
epoch train time: 0:00:01.788423
elapsed time: 0:00:44.986589
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 22:46:17.367208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.89
 ---- batch: 020 ----
mean loss: 205.16
 ---- batch: 030 ----
mean loss: 201.48
 ---- batch: 040 ----
mean loss: 191.03
train mean loss: 197.54
epoch train time: 0:00:01.678086
elapsed time: 0:00:46.664824
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 22:46:19.045444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.09
 ---- batch: 020 ----
mean loss: 199.59
 ---- batch: 030 ----
mean loss: 200.55
 ---- batch: 040 ----
mean loss: 200.35
train mean loss: 198.94
epoch train time: 0:00:01.780803
elapsed time: 0:00:48.445775
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 22:46:20.826387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.34
 ---- batch: 020 ----
mean loss: 199.35
 ---- batch: 030 ----
mean loss: 192.94
 ---- batch: 040 ----
mean loss: 190.83
train mean loss: 194.40
epoch train time: 0:00:01.687939
elapsed time: 0:00:50.133859
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 22:46:22.514487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.86
 ---- batch: 020 ----
mean loss: 204.20
 ---- batch: 030 ----
mean loss: 188.97
 ---- batch: 040 ----
mean loss: 191.75
train mean loss: 196.05
epoch train time: 0:00:01.665598
elapsed time: 0:00:51.799612
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 22:46:24.180230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.94
 ---- batch: 020 ----
mean loss: 198.79
 ---- batch: 030 ----
mean loss: 193.74
 ---- batch: 040 ----
mean loss: 191.37
train mean loss: 194.18
epoch train time: 0:00:01.782280
elapsed time: 0:00:53.582037
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 22:46:25.962651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.27
 ---- batch: 020 ----
mean loss: 190.19
 ---- batch: 030 ----
mean loss: 199.21
 ---- batch: 040 ----
mean loss: 194.97
train mean loss: 193.26
epoch train time: 0:00:01.668440
elapsed time: 0:00:55.250616
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 22:46:27.631244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.02
 ---- batch: 020 ----
mean loss: 197.87
 ---- batch: 030 ----
mean loss: 196.96
 ---- batch: 040 ----
mean loss: 197.07
train mean loss: 194.71
epoch train time: 0:00:01.772690
elapsed time: 0:00:57.023445
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 22:46:29.404075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.44
 ---- batch: 020 ----
mean loss: 198.92
 ---- batch: 030 ----
mean loss: 190.93
 ---- batch: 040 ----
mean loss: 204.29
train mean loss: 196.10
epoch train time: 0:00:01.680504
elapsed time: 0:00:58.704130
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 22:46:31.084764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.72
 ---- batch: 020 ----
mean loss: 191.55
 ---- batch: 030 ----
mean loss: 196.48
 ---- batch: 040 ----
mean loss: 189.65
train mean loss: 192.36
epoch train time: 0:00:01.751225
elapsed time: 0:01:00.455533
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 22:46:32.836153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.11
 ---- batch: 020 ----
mean loss: 194.84
 ---- batch: 030 ----
mean loss: 194.36
 ---- batch: 040 ----
mean loss: 188.60
train mean loss: 193.94
epoch train time: 0:00:01.697291
elapsed time: 0:01:02.152969
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 22:46:34.533583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.85
 ---- batch: 020 ----
mean loss: 188.76
 ---- batch: 030 ----
mean loss: 187.93
 ---- batch: 040 ----
mean loss: 198.97
train mean loss: 192.48
epoch train time: 0:00:01.668158
elapsed time: 0:01:03.821282
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 22:46:36.201900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.26
 ---- batch: 020 ----
mean loss: 201.56
 ---- batch: 030 ----
mean loss: 189.57
 ---- batch: 040 ----
mean loss: 188.55
train mean loss: 192.80
epoch train time: 0:00:01.794320
elapsed time: 0:01:05.615744
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 22:46:37.996361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.04
 ---- batch: 020 ----
mean loss: 185.15
 ---- batch: 030 ----
mean loss: 190.65
 ---- batch: 040 ----
mean loss: 188.83
train mean loss: 190.55
epoch train time: 0:00:01.666895
elapsed time: 0:01:07.282789
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 22:46:39.663420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.69
 ---- batch: 020 ----
mean loss: 191.16
 ---- batch: 030 ----
mean loss: 189.48
 ---- batch: 040 ----
mean loss: 186.89
train mean loss: 192.26
epoch train time: 0:00:01.777960
elapsed time: 0:01:09.060914
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 22:46:41.441530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.29
 ---- batch: 020 ----
mean loss: 184.80
 ---- batch: 030 ----
mean loss: 188.85
 ---- batch: 040 ----
mean loss: 185.19
train mean loss: 191.10
epoch train time: 0:00:01.672509
elapsed time: 0:01:10.733568
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 22:46:43.114199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.28
 ---- batch: 020 ----
mean loss: 196.02
 ---- batch: 030 ----
mean loss: 187.99
 ---- batch: 040 ----
mean loss: 184.58
train mean loss: 190.68
epoch train time: 0:00:01.729261
elapsed time: 0:01:12.463002
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 22:46:44.843618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.71
 ---- batch: 020 ----
mean loss: 194.84
 ---- batch: 030 ----
mean loss: 192.96
 ---- batch: 040 ----
mean loss: 188.59
train mean loss: 190.61
epoch train time: 0:00:01.724030
elapsed time: 0:01:14.187188
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 22:46:46.567801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.90
 ---- batch: 020 ----
mean loss: 188.75
 ---- batch: 030 ----
mean loss: 185.27
 ---- batch: 040 ----
mean loss: 191.77
train mean loss: 190.10
epoch train time: 0:00:01.671839
elapsed time: 0:01:15.859172
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 22:46:48.239787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.48
 ---- batch: 020 ----
mean loss: 192.92
 ---- batch: 030 ----
mean loss: 191.93
 ---- batch: 040 ----
mean loss: 191.59
train mean loss: 192.49
epoch train time: 0:00:01.782753
elapsed time: 0:01:17.642060
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 22:46:50.022691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.55
 ---- batch: 020 ----
mean loss: 183.22
 ---- batch: 030 ----
mean loss: 187.84
 ---- batch: 040 ----
mean loss: 189.75
train mean loss: 188.77
epoch train time: 0:00:01.680155
elapsed time: 0:01:19.322365
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 22:46:51.702979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.92
 ---- batch: 020 ----
mean loss: 186.01
 ---- batch: 030 ----
mean loss: 184.61
 ---- batch: 040 ----
mean loss: 188.13
train mean loss: 188.70
epoch train time: 0:00:01.791057
elapsed time: 0:01:21.113547
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 22:46:53.494173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.04
 ---- batch: 020 ----
mean loss: 187.02
 ---- batch: 030 ----
mean loss: 187.42
 ---- batch: 040 ----
mean loss: 186.37
train mean loss: 187.29
epoch train time: 0:00:01.678616
elapsed time: 0:01:22.792316
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 22:46:55.172966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.03
 ---- batch: 020 ----
mean loss: 192.93
 ---- batch: 030 ----
mean loss: 198.34
 ---- batch: 040 ----
mean loss: 199.39
train mean loss: 194.55
epoch train time: 0:00:01.763640
elapsed time: 0:01:24.556135
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 22:46:56.936752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.52
 ---- batch: 020 ----
mean loss: 187.58
 ---- batch: 030 ----
mean loss: 196.46
 ---- batch: 040 ----
mean loss: 182.37
train mean loss: 190.99
epoch train time: 0:00:01.671543
elapsed time: 0:01:26.227850
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 22:46:58.608469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.16
 ---- batch: 020 ----
mean loss: 185.84
 ---- batch: 030 ----
mean loss: 192.08
 ---- batch: 040 ----
mean loss: 190.96
train mean loss: 188.39
epoch train time: 0:00:01.762887
elapsed time: 0:01:27.990887
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 22:47:00.371505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.65
 ---- batch: 020 ----
mean loss: 189.98
 ---- batch: 030 ----
mean loss: 184.35
 ---- batch: 040 ----
mean loss: 193.85
train mean loss: 191.32
epoch train time: 0:00:01.672102
elapsed time: 0:01:29.663146
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 22:47:02.043764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.54
 ---- batch: 020 ----
mean loss: 189.66
 ---- batch: 030 ----
mean loss: 188.71
 ---- batch: 040 ----
mean loss: 185.51
train mean loss: 188.12
epoch train time: 0:00:01.766801
elapsed time: 0:01:31.430094
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 22:47:03.810712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.36
 ---- batch: 020 ----
mean loss: 186.78
 ---- batch: 030 ----
mean loss: 189.77
 ---- batch: 040 ----
mean loss: 188.06
train mean loss: 187.75
epoch train time: 0:00:01.676372
elapsed time: 0:01:33.106626
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 22:47:05.487250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.49
 ---- batch: 020 ----
mean loss: 181.52
 ---- batch: 030 ----
mean loss: 191.92
 ---- batch: 040 ----
mean loss: 186.15
train mean loss: 186.36
epoch train time: 0:00:01.661297
elapsed time: 0:01:34.768105
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 22:47:07.148721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.41
 ---- batch: 020 ----
mean loss: 185.20
 ---- batch: 030 ----
mean loss: 189.13
 ---- batch: 040 ----
mean loss: 184.62
train mean loss: 186.68
epoch train time: 0:00:01.798040
elapsed time: 0:01:36.566317
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 22:47:08.946936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.71
 ---- batch: 020 ----
mean loss: 191.20
 ---- batch: 030 ----
mean loss: 182.13
 ---- batch: 040 ----
mean loss: 189.13
train mean loss: 186.42
epoch train time: 0:00:01.667205
elapsed time: 0:01:38.233666
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 22:47:10.614304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.57
 ---- batch: 020 ----
mean loss: 184.70
 ---- batch: 030 ----
mean loss: 181.68
 ---- batch: 040 ----
mean loss: 186.88
train mean loss: 185.13
epoch train time: 0:00:01.778693
elapsed time: 0:01:40.012522
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 22:47:12.393140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.31
 ---- batch: 020 ----
mean loss: 186.25
 ---- batch: 030 ----
mean loss: 183.51
 ---- batch: 040 ----
mean loss: 190.17
train mean loss: 187.99
epoch train time: 0:00:01.667886
elapsed time: 0:01:41.680552
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 22:47:14.061171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.13
 ---- batch: 020 ----
mean loss: 185.18
 ---- batch: 030 ----
mean loss: 192.11
 ---- batch: 040 ----
mean loss: 180.19
train mean loss: 185.59
epoch train time: 0:00:01.776554
elapsed time: 0:01:43.457255
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 22:47:15.837873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.67
 ---- batch: 020 ----
mean loss: 177.22
 ---- batch: 030 ----
mean loss: 185.58
 ---- batch: 040 ----
mean loss: 181.91
train mean loss: 184.99
epoch train time: 0:00:01.665195
elapsed time: 0:01:45.122609
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 22:47:17.503227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.90
 ---- batch: 020 ----
mean loss: 182.27
 ---- batch: 030 ----
mean loss: 179.79
 ---- batch: 040 ----
mean loss: 186.68
train mean loss: 183.13
epoch train time: 0:00:01.774306
elapsed time: 0:01:46.897056
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 22:47:19.277671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.33
 ---- batch: 020 ----
mean loss: 187.59
 ---- batch: 030 ----
mean loss: 185.56
 ---- batch: 040 ----
mean loss: 183.04
train mean loss: 184.92
epoch train time: 0:00:01.669986
elapsed time: 0:01:48.567193
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 22:47:20.947812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.55
 ---- batch: 020 ----
mean loss: 193.40
 ---- batch: 030 ----
mean loss: 180.62
 ---- batch: 040 ----
mean loss: 182.68
train mean loss: 190.02
epoch train time: 0:00:01.764701
elapsed time: 0:01:50.332040
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 22:47:22.712655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.68
 ---- batch: 020 ----
mean loss: 185.17
 ---- batch: 030 ----
mean loss: 181.33
 ---- batch: 040 ----
mean loss: 189.83
train mean loss: 185.83
epoch train time: 0:00:01.675307
elapsed time: 0:01:52.007501
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 22:47:24.388118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.41
 ---- batch: 020 ----
mean loss: 187.84
 ---- batch: 030 ----
mean loss: 185.68
 ---- batch: 040 ----
mean loss: 189.98
train mean loss: 186.90
epoch train time: 0:00:01.658897
elapsed time: 0:01:53.666549
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 22:47:26.047166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.51
 ---- batch: 020 ----
mean loss: 186.57
 ---- batch: 030 ----
mean loss: 185.76
 ---- batch: 040 ----
mean loss: 183.06
train mean loss: 182.72
epoch train time: 0:00:01.799102
elapsed time: 0:01:55.465798
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 22:47:27.846413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.48
 ---- batch: 020 ----
mean loss: 185.64
 ---- batch: 030 ----
mean loss: 178.16
 ---- batch: 040 ----
mean loss: 180.92
train mean loss: 181.81
epoch train time: 0:00:01.667658
elapsed time: 0:01:57.133596
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 22:47:29.514212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.11
 ---- batch: 020 ----
mean loss: 177.12
 ---- batch: 030 ----
mean loss: 186.87
 ---- batch: 040 ----
mean loss: 183.01
train mean loss: 183.11
epoch train time: 0:00:01.790198
elapsed time: 0:01:58.923943
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 22:47:31.304577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.85
 ---- batch: 020 ----
mean loss: 181.54
 ---- batch: 030 ----
mean loss: 170.48
 ---- batch: 040 ----
mean loss: 183.03
train mean loss: 178.31
epoch train time: 0:00:01.665394
elapsed time: 0:02:00.589499
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 22:47:32.970115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.01
 ---- batch: 020 ----
mean loss: 178.89
 ---- batch: 030 ----
mean loss: 178.95
 ---- batch: 040 ----
mean loss: 181.63
train mean loss: 179.75
epoch train time: 0:00:01.771561
elapsed time: 0:02:02.361194
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 22:47:34.741808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.36
 ---- batch: 020 ----
mean loss: 182.46
 ---- batch: 030 ----
mean loss: 181.93
 ---- batch: 040 ----
mean loss: 172.07
train mean loss: 178.53
epoch train time: 0:00:01.669197
elapsed time: 0:02:04.030538
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 22:47:36.411157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.21
 ---- batch: 020 ----
mean loss: 178.44
 ---- batch: 030 ----
mean loss: 181.28
 ---- batch: 040 ----
mean loss: 164.41
train mean loss: 177.31
epoch train time: 0:00:01.771894
elapsed time: 0:02:05.802577
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 22:47:38.183190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.45
 ---- batch: 020 ----
mean loss: 179.25
 ---- batch: 030 ----
mean loss: 170.31
 ---- batch: 040 ----
mean loss: 179.64
train mean loss: 176.30
epoch train time: 0:00:01.676800
elapsed time: 0:02:07.479530
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 22:47:39.860150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.82
 ---- batch: 020 ----
mean loss: 169.55
 ---- batch: 030 ----
mean loss: 179.40
 ---- batch: 040 ----
mean loss: 184.54
train mean loss: 176.37
epoch train time: 0:00:01.766575
elapsed time: 0:02:09.246252
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 22:47:41.626867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.86
 ---- batch: 020 ----
mean loss: 173.64
 ---- batch: 030 ----
mean loss: 172.16
 ---- batch: 040 ----
mean loss: 167.94
train mean loss: 170.58
epoch train time: 0:00:01.681041
elapsed time: 0:02:10.927439
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 22:47:43.308057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.37
 ---- batch: 020 ----
mean loss: 167.53
 ---- batch: 030 ----
mean loss: 165.55
 ---- batch: 040 ----
mean loss: 176.12
train mean loss: 170.85
epoch train time: 0:00:01.756957
elapsed time: 0:02:12.684542
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 22:47:45.065161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.06
 ---- batch: 020 ----
mean loss: 170.92
 ---- batch: 030 ----
mean loss: 165.69
 ---- batch: 040 ----
mean loss: 170.78
train mean loss: 171.64
epoch train time: 0:00:01.663610
elapsed time: 0:02:14.348318
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 22:47:46.728941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.63
 ---- batch: 020 ----
mean loss: 171.65
 ---- batch: 030 ----
mean loss: 169.29
 ---- batch: 040 ----
mean loss: 170.59
train mean loss: 171.49
epoch train time: 0:00:01.660074
elapsed time: 0:02:16.008550
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 22:47:48.389172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.39
 ---- batch: 020 ----
mean loss: 172.96
 ---- batch: 030 ----
mean loss: 172.72
 ---- batch: 040 ----
mean loss: 169.85
train mean loss: 171.50
epoch train time: 0:00:01.765724
elapsed time: 0:02:17.774427
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 22:47:50.155046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.21
 ---- batch: 020 ----
mean loss: 177.87
 ---- batch: 030 ----
mean loss: 167.66
 ---- batch: 040 ----
mean loss: 168.87
train mean loss: 172.04
epoch train time: 0:00:01.662790
elapsed time: 0:02:19.437371
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 22:47:51.818005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.36
 ---- batch: 020 ----
mean loss: 162.29
 ---- batch: 030 ----
mean loss: 168.20
 ---- batch: 040 ----
mean loss: 163.55
train mean loss: 166.71
epoch train time: 0:00:01.778259
elapsed time: 0:02:21.215795
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 22:47:53.596448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.29
 ---- batch: 020 ----
mean loss: 168.55
 ---- batch: 030 ----
mean loss: 163.36
 ---- batch: 040 ----
mean loss: 167.71
train mean loss: 167.88
epoch train time: 0:00:01.660296
elapsed time: 0:02:22.876285
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 22:47:55.256905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.32
 ---- batch: 020 ----
mean loss: 164.89
 ---- batch: 030 ----
mean loss: 165.28
 ---- batch: 040 ----
mean loss: 164.69
train mean loss: 164.00
epoch train time: 0:00:01.768352
elapsed time: 0:02:24.644838
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 22:47:57.025476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.52
 ---- batch: 020 ----
mean loss: 162.90
 ---- batch: 030 ----
mean loss: 159.04
 ---- batch: 040 ----
mean loss: 162.40
train mean loss: 161.25
epoch train time: 0:00:01.666126
elapsed time: 0:02:26.311140
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 22:47:58.691759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.73
 ---- batch: 020 ----
mean loss: 165.83
 ---- batch: 030 ----
mean loss: 165.02
 ---- batch: 040 ----
mean loss: 159.22
train mean loss: 162.00
epoch train time: 0:00:01.772104
elapsed time: 0:02:28.083386
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 22:48:00.464020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.48
 ---- batch: 020 ----
mean loss: 162.44
 ---- batch: 030 ----
mean loss: 161.53
 ---- batch: 040 ----
mean loss: 164.85
train mean loss: 163.32
epoch train time: 0:00:01.667646
elapsed time: 0:02:29.751198
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 22:48:02.131825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.53
 ---- batch: 020 ----
mean loss: 159.84
 ---- batch: 030 ----
mean loss: 168.13
 ---- batch: 040 ----
mean loss: 160.99
train mean loss: 161.66
epoch train time: 0:00:01.763874
elapsed time: 0:02:31.515238
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 22:48:03.895855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.12
 ---- batch: 020 ----
mean loss: 159.18
 ---- batch: 030 ----
mean loss: 158.35
 ---- batch: 040 ----
mean loss: 162.64
train mean loss: 159.97
epoch train time: 0:00:01.677460
elapsed time: 0:02:33.192839
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 22:48:05.573454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.84
 ---- batch: 020 ----
mean loss: 164.90
 ---- batch: 030 ----
mean loss: 167.26
 ---- batch: 040 ----
mean loss: 161.71
train mean loss: 163.01
epoch train time: 0:00:01.661148
elapsed time: 0:02:34.854120
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 22:48:07.234736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.38
 ---- batch: 020 ----
mean loss: 161.27
 ---- batch: 030 ----
mean loss: 156.01
 ---- batch: 040 ----
mean loss: 161.07
train mean loss: 159.86
epoch train time: 0:00:01.777169
elapsed time: 0:02:36.631437
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 22:48:09.012052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.74
 ---- batch: 020 ----
mean loss: 165.60
 ---- batch: 030 ----
mean loss: 159.16
 ---- batch: 040 ----
mean loss: 162.32
train mean loss: 160.20
epoch train time: 0:00:01.666957
elapsed time: 0:02:38.298544
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 22:48:10.679169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.88
 ---- batch: 020 ----
mean loss: 162.34
 ---- batch: 030 ----
mean loss: 155.20
 ---- batch: 040 ----
mean loss: 157.48
train mean loss: 157.97
epoch train time: 0:00:01.779116
elapsed time: 0:02:40.077803
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 22:48:12.458434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.93
 ---- batch: 020 ----
mean loss: 154.47
 ---- batch: 030 ----
mean loss: 155.62
 ---- batch: 040 ----
mean loss: 161.03
train mean loss: 156.28
epoch train time: 0:00:01.669300
elapsed time: 0:02:41.747295
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 22:48:14.127923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.48
 ---- batch: 020 ----
mean loss: 160.42
 ---- batch: 030 ----
mean loss: 154.22
 ---- batch: 040 ----
mean loss: 156.00
train mean loss: 157.03
epoch train time: 0:00:01.778323
elapsed time: 0:02:43.525784
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 22:48:15.906397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.11
 ---- batch: 020 ----
mean loss: 154.19
 ---- batch: 030 ----
mean loss: 156.26
 ---- batch: 040 ----
mean loss: 151.48
train mean loss: 153.61
epoch train time: 0:00:01.672069
elapsed time: 0:02:45.198003
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 22:48:17.578622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.31
 ---- batch: 020 ----
mean loss: 156.41
 ---- batch: 030 ----
mean loss: 151.98
 ---- batch: 040 ----
mean loss: 146.93
train mean loss: 152.59
epoch train time: 0:00:01.785182
elapsed time: 0:02:46.983336
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 22:48:19.363952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.82
 ---- batch: 020 ----
mean loss: 151.23
 ---- batch: 030 ----
mean loss: 151.85
 ---- batch: 040 ----
mean loss: 152.44
train mean loss: 152.59
epoch train time: 0:00:01.681395
elapsed time: 0:02:48.664886
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 22:48:21.045520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.49
 ---- batch: 020 ----
mean loss: 161.99
 ---- batch: 030 ----
mean loss: 147.22
 ---- batch: 040 ----
mean loss: 155.26
train mean loss: 153.02
epoch train time: 0:00:01.785957
elapsed time: 0:02:50.451013
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 22:48:22.831638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.64
 ---- batch: 020 ----
mean loss: 150.84
 ---- batch: 030 ----
mean loss: 148.49
 ---- batch: 040 ----
mean loss: 144.39
train mean loss: 148.92
epoch train time: 0:00:01.687896
elapsed time: 0:02:52.139083
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 22:48:24.519704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.86
 ---- batch: 020 ----
mean loss: 147.66
 ---- batch: 030 ----
mean loss: 148.85
 ---- batch: 040 ----
mean loss: 148.74
train mean loss: 148.41
epoch train time: 0:00:01.773158
elapsed time: 0:02:53.912414
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 22:48:26.293049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.49
 ---- batch: 020 ----
mean loss: 152.68
 ---- batch: 030 ----
mean loss: 145.74
 ---- batch: 040 ----
mean loss: 151.01
train mean loss: 150.06
epoch train time: 0:00:01.689000
elapsed time: 0:02:55.601586
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 22:48:27.982206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.31
 ---- batch: 020 ----
mean loss: 150.21
 ---- batch: 030 ----
mean loss: 146.66
 ---- batch: 040 ----
mean loss: 152.16
train mean loss: 149.23
epoch train time: 0:00:01.672615
elapsed time: 0:02:57.274349
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 22:48:29.654966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.60
 ---- batch: 020 ----
mean loss: 149.90
 ---- batch: 030 ----
mean loss: 146.10
 ---- batch: 040 ----
mean loss: 147.33
train mean loss: 148.67
epoch train time: 0:00:01.790154
elapsed time: 0:02:59.064685
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 22:48:31.445301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.34
 ---- batch: 020 ----
mean loss: 150.56
 ---- batch: 030 ----
mean loss: 143.56
 ---- batch: 040 ----
mean loss: 142.49
train mean loss: 146.36
epoch train time: 0:00:01.666422
elapsed time: 0:03:00.731267
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 22:48:33.111909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.66
 ---- batch: 020 ----
mean loss: 143.43
 ---- batch: 030 ----
mean loss: 146.32
 ---- batch: 040 ----
mean loss: 146.22
train mean loss: 145.61
epoch train time: 0:00:01.777893
elapsed time: 0:03:02.509329
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 22:48:34.889945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.95
 ---- batch: 020 ----
mean loss: 144.50
 ---- batch: 030 ----
mean loss: 148.83
 ---- batch: 040 ----
mean loss: 148.20
train mean loss: 145.24
epoch train time: 0:00:01.671798
elapsed time: 0:03:04.181277
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 22:48:36.561894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.55
 ---- batch: 020 ----
mean loss: 152.46
 ---- batch: 030 ----
mean loss: 147.31
 ---- batch: 040 ----
mean loss: 143.80
train mean loss: 146.20
epoch train time: 0:00:01.789445
elapsed time: 0:03:05.970865
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 22:48:38.351481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.59
 ---- batch: 020 ----
mean loss: 141.64
 ---- batch: 030 ----
mean loss: 140.76
 ---- batch: 040 ----
mean loss: 143.39
train mean loss: 140.94
epoch train time: 0:00:01.675729
elapsed time: 0:03:07.646757
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 22:48:40.027388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.11
 ---- batch: 020 ----
mean loss: 143.46
 ---- batch: 030 ----
mean loss: 145.84
 ---- batch: 040 ----
mean loss: 136.71
train mean loss: 142.91
epoch train time: 0:00:01.788764
elapsed time: 0:03:09.435720
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 22:48:41.816353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.79
 ---- batch: 020 ----
mean loss: 135.27
 ---- batch: 030 ----
mean loss: 144.38
 ---- batch: 040 ----
mean loss: 142.20
train mean loss: 141.21
epoch train time: 0:00:01.686506
elapsed time: 0:03:11.122398
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 22:48:43.503016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.36
 ---- batch: 020 ----
mean loss: 136.15
 ---- batch: 030 ----
mean loss: 140.68
 ---- batch: 040 ----
mean loss: 142.36
train mean loss: 140.00
epoch train time: 0:00:01.781574
elapsed time: 0:03:12.904139
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 22:48:45.284789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.17
 ---- batch: 020 ----
mean loss: 139.29
 ---- batch: 030 ----
mean loss: 135.26
 ---- batch: 040 ----
mean loss: 140.37
train mean loss: 138.23
epoch train time: 0:00:01.686465
elapsed time: 0:03:14.590801
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 22:48:46.971420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.29
 ---- batch: 020 ----
mean loss: 139.83
 ---- batch: 030 ----
mean loss: 141.61
 ---- batch: 040 ----
mean loss: 143.63
train mean loss: 140.12
epoch train time: 0:00:01.780796
elapsed time: 0:03:16.371752
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 22:48:48.752370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.92
 ---- batch: 020 ----
mean loss: 135.82
 ---- batch: 030 ----
mean loss: 134.70
 ---- batch: 040 ----
mean loss: 135.63
train mean loss: 136.42
epoch train time: 0:00:01.688059
elapsed time: 0:03:18.059997
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 22:48:50.440620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.28
 ---- batch: 020 ----
mean loss: 136.83
 ---- batch: 030 ----
mean loss: 146.21
 ---- batch: 040 ----
mean loss: 141.68
train mean loss: 141.54
epoch train time: 0:00:01.683880
elapsed time: 0:03:19.744020
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 22:48:52.124649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.61
 ---- batch: 020 ----
mean loss: 133.21
 ---- batch: 030 ----
mean loss: 129.77
 ---- batch: 040 ----
mean loss: 143.82
train mean loss: 137.54
epoch train time: 0:00:01.775733
elapsed time: 0:03:21.519939
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 22:48:53.900558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.87
 ---- batch: 020 ----
mean loss: 133.55
 ---- batch: 030 ----
mean loss: 138.45
 ---- batch: 040 ----
mean loss: 128.68
train mean loss: 135.98
epoch train time: 0:00:01.677982
elapsed time: 0:03:23.198075
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 22:48:55.578693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.44
 ---- batch: 020 ----
mean loss: 132.38
 ---- batch: 030 ----
mean loss: 139.66
 ---- batch: 040 ----
mean loss: 141.53
train mean loss: 137.98
epoch train time: 0:00:01.779913
elapsed time: 0:03:24.978129
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 22:48:57.358748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.03
 ---- batch: 020 ----
mean loss: 135.56
 ---- batch: 030 ----
mean loss: 134.28
 ---- batch: 040 ----
mean loss: 129.30
train mean loss: 135.22
epoch train time: 0:00:01.684174
elapsed time: 0:03:26.662448
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 22:48:59.043065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.38
 ---- batch: 020 ----
mean loss: 132.95
 ---- batch: 030 ----
mean loss: 140.85
 ---- batch: 040 ----
mean loss: 138.87
train mean loss: 136.59
epoch train time: 0:00:01.778350
elapsed time: 0:03:28.440936
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 22:49:00.821570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.24
 ---- batch: 020 ----
mean loss: 138.76
 ---- batch: 030 ----
mean loss: 134.62
 ---- batch: 040 ----
mean loss: 133.30
train mean loss: 137.80
epoch train time: 0:00:01.682885
elapsed time: 0:03:30.123995
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 22:49:02.504609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.42
 ---- batch: 020 ----
mean loss: 128.03
 ---- batch: 030 ----
mean loss: 137.26
 ---- batch: 040 ----
mean loss: 135.56
train mean loss: 133.99
epoch train time: 0:00:01.796184
elapsed time: 0:03:31.920323
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 22:49:04.300935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.17
 ---- batch: 020 ----
mean loss: 134.04
 ---- batch: 030 ----
mean loss: 136.77
 ---- batch: 040 ----
mean loss: 130.82
train mean loss: 132.82
epoch train time: 0:00:01.674126
elapsed time: 0:03:33.594586
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 22:49:05.975203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.14
 ---- batch: 020 ----
mean loss: 129.53
 ---- batch: 030 ----
mean loss: 129.60
 ---- batch: 040 ----
mean loss: 134.57
train mean loss: 132.36
epoch train time: 0:00:01.777972
elapsed time: 0:03:35.372688
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 22:49:07.753297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.84
 ---- batch: 020 ----
mean loss: 138.28
 ---- batch: 030 ----
mean loss: 130.47
 ---- batch: 040 ----
mean loss: 131.82
train mean loss: 133.76
epoch train time: 0:00:01.678992
elapsed time: 0:03:37.051913
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 22:49:09.432559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.20
 ---- batch: 020 ----
mean loss: 133.24
 ---- batch: 030 ----
mean loss: 131.55
 ---- batch: 040 ----
mean loss: 132.13
train mean loss: 132.76
epoch train time: 0:00:01.774040
elapsed time: 0:03:38.826127
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 22:49:11.206746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.13
 ---- batch: 020 ----
mean loss: 131.51
 ---- batch: 030 ----
mean loss: 128.44
 ---- batch: 040 ----
mean loss: 129.33
train mean loss: 131.53
epoch train time: 0:00:01.689921
elapsed time: 0:03:40.516198
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 22:49:12.896830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.73
 ---- batch: 020 ----
mean loss: 129.31
 ---- batch: 030 ----
mean loss: 131.91
 ---- batch: 040 ----
mean loss: 133.67
train mean loss: 130.79
epoch train time: 0:00:01.709518
elapsed time: 0:03:42.225862
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 22:49:14.606474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.36
 ---- batch: 020 ----
mean loss: 132.12
 ---- batch: 030 ----
mean loss: 122.78
 ---- batch: 040 ----
mean loss: 134.05
train mean loss: 129.71
epoch train time: 0:00:01.736720
elapsed time: 0:03:43.962728
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 22:49:16.343350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.85
 ---- batch: 020 ----
mean loss: 133.34
 ---- batch: 030 ----
mean loss: 137.81
 ---- batch: 040 ----
mean loss: 128.62
train mean loss: 131.34
epoch train time: 0:00:01.673523
elapsed time: 0:03:45.636393
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 22:49:18.017008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.31
 ---- batch: 020 ----
mean loss: 129.14
 ---- batch: 030 ----
mean loss: 130.83
 ---- batch: 040 ----
mean loss: 125.04
train mean loss: 129.64
epoch train time: 0:00:01.704703
elapsed time: 0:03:47.341238
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 22:49:19.721853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.84
 ---- batch: 020 ----
mean loss: 129.79
 ---- batch: 030 ----
mean loss: 125.44
 ---- batch: 040 ----
mean loss: 128.43
train mean loss: 127.16
epoch train time: 0:00:01.681378
elapsed time: 0:03:49.022771
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 22:49:21.403389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.19
 ---- batch: 020 ----
mean loss: 135.04
 ---- batch: 030 ----
mean loss: 132.10
 ---- batch: 040 ----
mean loss: 135.48
train mean loss: 135.48
epoch train time: 0:00:01.777086
elapsed time: 0:03:50.799994
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 22:49:23.180610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.19
 ---- batch: 020 ----
mean loss: 131.51
 ---- batch: 030 ----
mean loss: 136.44
 ---- batch: 040 ----
mean loss: 130.35
train mean loss: 131.26
epoch train time: 0:00:01.680852
elapsed time: 0:03:52.481008
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 22:49:24.861615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.53
 ---- batch: 020 ----
mean loss: 134.24
 ---- batch: 030 ----
mean loss: 132.42
 ---- batch: 040 ----
mean loss: 136.60
train mean loss: 134.56
epoch train time: 0:00:01.784675
elapsed time: 0:03:54.265815
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 22:49:26.646427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.53
 ---- batch: 020 ----
mean loss: 127.60
 ---- batch: 030 ----
mean loss: 130.64
 ---- batch: 040 ----
mean loss: 133.82
train mean loss: 131.09
epoch train time: 0:00:01.689398
elapsed time: 0:03:55.955348
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 22:49:28.335967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.88
 ---- batch: 020 ----
mean loss: 128.24
 ---- batch: 030 ----
mean loss: 129.90
 ---- batch: 040 ----
mean loss: 127.59
train mean loss: 129.13
epoch train time: 0:00:01.781232
elapsed time: 0:03:57.736717
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 22:49:30.117357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.47
 ---- batch: 020 ----
mean loss: 132.88
 ---- batch: 030 ----
mean loss: 128.54
 ---- batch: 040 ----
mean loss: 128.88
train mean loss: 129.33
epoch train time: 0:00:01.689581
elapsed time: 0:03:59.426467
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 22:49:31.807100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.78
 ---- batch: 020 ----
mean loss: 128.85
 ---- batch: 030 ----
mean loss: 127.19
 ---- batch: 040 ----
mean loss: 125.30
train mean loss: 127.77
epoch train time: 0:00:01.707118
elapsed time: 0:04:01.133733
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 22:49:33.514358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.85
 ---- batch: 020 ----
mean loss: 125.55
 ---- batch: 030 ----
mean loss: 129.16
 ---- batch: 040 ----
mean loss: 125.47
train mean loss: 126.74
epoch train time: 0:00:01.748767
elapsed time: 0:04:02.882657
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 22:49:35.263270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.80
 ---- batch: 020 ----
mean loss: 128.32
 ---- batch: 030 ----
mean loss: 128.18
 ---- batch: 040 ----
mean loss: 125.52
train mean loss: 127.12
epoch train time: 0:00:01.675106
elapsed time: 0:04:04.557893
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 22:49:36.938521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.67
 ---- batch: 020 ----
mean loss: 125.60
 ---- batch: 030 ----
mean loss: 128.15
 ---- batch: 040 ----
mean loss: 123.18
train mean loss: 126.80
epoch train time: 0:00:01.787869
elapsed time: 0:04:06.345907
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 22:49:38.726519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.15
 ---- batch: 020 ----
mean loss: 123.76
 ---- batch: 030 ----
mean loss: 127.86
 ---- batch: 040 ----
mean loss: 126.75
train mean loss: 127.61
epoch train time: 0:00:01.678217
elapsed time: 0:04:08.024264
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 22:49:40.404879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.87
 ---- batch: 020 ----
mean loss: 125.60
 ---- batch: 030 ----
mean loss: 119.87
 ---- batch: 040 ----
mean loss: 123.18
train mean loss: 124.07
epoch train time: 0:00:01.787020
elapsed time: 0:04:09.811413
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 22:49:42.192025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.99
 ---- batch: 020 ----
mean loss: 124.32
 ---- batch: 030 ----
mean loss: 130.68
 ---- batch: 040 ----
mean loss: 127.23
train mean loss: 126.87
epoch train time: 0:00:01.685676
elapsed time: 0:04:11.497231
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 22:49:43.877847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.03
 ---- batch: 020 ----
mean loss: 132.53
 ---- batch: 030 ----
mean loss: 129.18
 ---- batch: 040 ----
mean loss: 126.47
train mean loss: 129.30
epoch train time: 0:00:01.785593
elapsed time: 0:04:13.282956
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 22:49:45.663570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.96
 ---- batch: 020 ----
mean loss: 125.90
 ---- batch: 030 ----
mean loss: 127.38
 ---- batch: 040 ----
mean loss: 125.08
train mean loss: 127.05
epoch train time: 0:00:01.680561
elapsed time: 0:04:14.963657
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 22:49:47.344272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.47
 ---- batch: 020 ----
mean loss: 125.86
 ---- batch: 030 ----
mean loss: 129.05
 ---- batch: 040 ----
mean loss: 135.15
train mean loss: 128.75
epoch train time: 0:00:01.782637
elapsed time: 0:04:16.746426
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 22:49:49.127038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.16
 ---- batch: 020 ----
mean loss: 129.71
 ---- batch: 030 ----
mean loss: 125.02
 ---- batch: 040 ----
mean loss: 127.88
train mean loss: 126.22
epoch train time: 0:00:01.684227
elapsed time: 0:04:18.430819
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 22:49:50.811447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.47
 ---- batch: 020 ----
mean loss: 126.33
 ---- batch: 030 ----
mean loss: 128.65
 ---- batch: 040 ----
mean loss: 119.22
train mean loss: 124.17
epoch train time: 0:00:01.775171
elapsed time: 0:04:20.206144
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 22:49:52.586757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.93
 ---- batch: 020 ----
mean loss: 120.72
 ---- batch: 030 ----
mean loss: 130.42
 ---- batch: 040 ----
mean loss: 126.34
train mean loss: 124.95
epoch train time: 0:00:01.683253
elapsed time: 0:04:21.889555
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 22:49:54.270189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.00
 ---- batch: 020 ----
mean loss: 129.29
 ---- batch: 030 ----
mean loss: 132.88
 ---- batch: 040 ----
mean loss: 128.42
train mean loss: 128.97
epoch train time: 0:00:01.668771
elapsed time: 0:04:23.558522
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 22:49:55.939142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.74
 ---- batch: 020 ----
mean loss: 121.80
 ---- batch: 030 ----
mean loss: 125.67
 ---- batch: 040 ----
mean loss: 123.39
train mean loss: 125.50
epoch train time: 0:00:01.789177
elapsed time: 0:04:25.347863
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 22:49:57.728483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.85
 ---- batch: 020 ----
mean loss: 125.85
 ---- batch: 030 ----
mean loss: 120.00
 ---- batch: 040 ----
mean loss: 127.59
train mean loss: 123.55
epoch train time: 0:00:01.673006
elapsed time: 0:04:27.021025
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 22:49:59.401645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.85
 ---- batch: 020 ----
mean loss: 122.73
 ---- batch: 030 ----
mean loss: 128.92
 ---- batch: 040 ----
mean loss: 124.31
train mean loss: 126.53
epoch train time: 0:00:01.788419
elapsed time: 0:04:28.809604
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 22:50:01.190223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.05
 ---- batch: 020 ----
mean loss: 124.34
 ---- batch: 030 ----
mean loss: 121.39
 ---- batch: 040 ----
mean loss: 120.77
train mean loss: 123.31
epoch train time: 0:00:01.678720
elapsed time: 0:04:30.488483
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 22:50:02.869090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.07
 ---- batch: 020 ----
mean loss: 130.80
 ---- batch: 030 ----
mean loss: 120.04
 ---- batch: 040 ----
mean loss: 120.15
train mean loss: 125.66
epoch train time: 0:00:01.779391
elapsed time: 0:04:32.268053
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 22:50:04.648663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.29
 ---- batch: 020 ----
mean loss: 121.09
 ---- batch: 030 ----
mean loss: 125.27
 ---- batch: 040 ----
mean loss: 122.99
train mean loss: 124.11
epoch train time: 0:00:01.677286
elapsed time: 0:04:33.945470
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 22:50:06.326084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.01
 ---- batch: 020 ----
mean loss: 121.78
 ---- batch: 030 ----
mean loss: 128.53
 ---- batch: 040 ----
mean loss: 134.01
train mean loss: 127.19
epoch train time: 0:00:01.778694
elapsed time: 0:04:35.724298
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 22:50:08.104911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.94
 ---- batch: 020 ----
mean loss: 125.86
 ---- batch: 030 ----
mean loss: 122.19
 ---- batch: 040 ----
mean loss: 123.15
train mean loss: 123.55
epoch train time: 0:00:01.682600
elapsed time: 0:04:37.407070
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 22:50:09.787689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.53
 ---- batch: 020 ----
mean loss: 119.58
 ---- batch: 030 ----
mean loss: 119.30
 ---- batch: 040 ----
mean loss: 130.88
train mean loss: 123.64
epoch train time: 0:00:01.785155
elapsed time: 0:04:39.192374
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 22:50:11.573017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.75
 ---- batch: 020 ----
mean loss: 125.28
 ---- batch: 030 ----
mean loss: 121.88
 ---- batch: 040 ----
mean loss: 124.87
train mean loss: 123.83
epoch train time: 0:00:01.686212
elapsed time: 0:04:40.878751
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 22:50:13.259366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.73
 ---- batch: 020 ----
mean loss: 126.81
 ---- batch: 030 ----
mean loss: 125.73
 ---- batch: 040 ----
mean loss: 121.29
train mean loss: 123.41
epoch train time: 0:00:01.779053
elapsed time: 0:04:42.657933
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 22:50:15.038560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.98
 ---- batch: 020 ----
mean loss: 130.22
 ---- batch: 030 ----
mean loss: 122.90
 ---- batch: 040 ----
mean loss: 130.95
train mean loss: 125.53
epoch train time: 0:00:01.684088
elapsed time: 0:04:44.342192
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 22:50:16.722816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.27
 ---- batch: 020 ----
mean loss: 121.89
 ---- batch: 030 ----
mean loss: 120.31
 ---- batch: 040 ----
mean loss: 122.24
train mean loss: 121.07
epoch train time: 0:00:01.671279
elapsed time: 0:04:46.013611
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 22:50:18.394226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.91
 ---- batch: 020 ----
mean loss: 123.97
 ---- batch: 030 ----
mean loss: 121.87
 ---- batch: 040 ----
mean loss: 119.75
train mean loss: 120.98
epoch train time: 0:00:01.786267
elapsed time: 0:04:47.800024
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 22:50:20.180640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.03
 ---- batch: 020 ----
mean loss: 123.97
 ---- batch: 030 ----
mean loss: 121.95
 ---- batch: 040 ----
mean loss: 124.87
train mean loss: 123.12
epoch train time: 0:00:01.673680
elapsed time: 0:04:49.473855
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 22:50:21.854470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.14
 ---- batch: 020 ----
mean loss: 119.44
 ---- batch: 030 ----
mean loss: 123.89
 ---- batch: 040 ----
mean loss: 125.08
train mean loss: 122.06
epoch train time: 0:00:01.792739
elapsed time: 0:04:51.266785
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 22:50:23.647404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.87
 ---- batch: 020 ----
mean loss: 121.36
 ---- batch: 030 ----
mean loss: 124.98
 ---- batch: 040 ----
mean loss: 122.07
train mean loss: 122.37
epoch train time: 0:00:01.679091
elapsed time: 0:04:52.946023
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 22:50:25.326639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.45
 ---- batch: 020 ----
mean loss: 118.19
 ---- batch: 030 ----
mean loss: 122.43
 ---- batch: 040 ----
mean loss: 124.77
train mean loss: 120.78
epoch train time: 0:00:01.791476
elapsed time: 0:04:54.737624
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 22:50:27.118234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.15
 ---- batch: 020 ----
mean loss: 121.08
 ---- batch: 030 ----
mean loss: 125.29
 ---- batch: 040 ----
mean loss: 119.34
train mean loss: 120.51
epoch train time: 0:00:01.682139
elapsed time: 0:04:56.419946
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 22:50:28.800585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.59
 ---- batch: 020 ----
mean loss: 124.57
 ---- batch: 030 ----
mean loss: 122.89
 ---- batch: 040 ----
mean loss: 123.37
train mean loss: 123.55
epoch train time: 0:00:01.791559
elapsed time: 0:04:58.211655
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 22:50:30.592276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.47
 ---- batch: 020 ----
mean loss: 131.62
 ---- batch: 030 ----
mean loss: 123.60
 ---- batch: 040 ----
mean loss: 121.47
train mean loss: 126.09
epoch train time: 0:00:01.675753
elapsed time: 0:04:59.887561
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 22:50:32.268179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.66
 ---- batch: 020 ----
mean loss: 121.64
 ---- batch: 030 ----
mean loss: 120.40
 ---- batch: 040 ----
mean loss: 123.54
train mean loss: 121.57
epoch train time: 0:00:01.777587
elapsed time: 0:05:01.665279
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 22:50:34.045901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.02
 ---- batch: 020 ----
mean loss: 120.97
 ---- batch: 030 ----
mean loss: 120.57
 ---- batch: 040 ----
mean loss: 120.72
train mean loss: 120.65
epoch train time: 0:00:01.689845
elapsed time: 0:05:03.355281
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 22:50:35.735917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.65
 ---- batch: 020 ----
mean loss: 119.95
 ---- batch: 030 ----
mean loss: 125.91
 ---- batch: 040 ----
mean loss: 132.27
train mean loss: 125.38
epoch train time: 0:00:01.670082
elapsed time: 0:05:05.025522
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 22:50:37.406138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.63
 ---- batch: 020 ----
mean loss: 125.45
 ---- batch: 030 ----
mean loss: 126.13
 ---- batch: 040 ----
mean loss: 120.83
train mean loss: 124.86
epoch train time: 0:00:01.792302
elapsed time: 0:05:06.817949
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 22:50:39.198563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.67
 ---- batch: 020 ----
mean loss: 120.81
 ---- batch: 030 ----
mean loss: 122.23
 ---- batch: 040 ----
mean loss: 117.12
train mean loss: 119.88
epoch train time: 0:00:01.683060
elapsed time: 0:05:08.501149
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 22:50:40.881792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.16
 ---- batch: 020 ----
mean loss: 120.85
 ---- batch: 030 ----
mean loss: 123.93
 ---- batch: 040 ----
mean loss: 118.18
train mean loss: 121.23
epoch train time: 0:00:01.772649
elapsed time: 0:05:10.273951
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 22:50:42.654560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.61
 ---- batch: 020 ----
mean loss: 123.37
 ---- batch: 030 ----
mean loss: 117.76
 ---- batch: 040 ----
mean loss: 127.83
train mean loss: 122.31
epoch train time: 0:00:01.682057
elapsed time: 0:05:11.956156
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 22:50:44.336782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.40
 ---- batch: 020 ----
mean loss: 127.09
 ---- batch: 030 ----
mean loss: 119.85
 ---- batch: 040 ----
mean loss: 121.33
train mean loss: 121.27
epoch train time: 0:00:01.774271
elapsed time: 0:05:13.730592
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 22:50:46.111196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.19
 ---- batch: 020 ----
mean loss: 123.32
 ---- batch: 030 ----
mean loss: 121.92
 ---- batch: 040 ----
mean loss: 122.56
train mean loss: 122.28
epoch train time: 0:00:01.682693
elapsed time: 0:05:15.413429
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 22:50:47.794047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.89
 ---- batch: 020 ----
mean loss: 117.70
 ---- batch: 030 ----
mean loss: 117.87
 ---- batch: 040 ----
mean loss: 123.33
train mean loss: 120.08
epoch train time: 0:00:01.778141
elapsed time: 0:05:17.191704
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 22:50:49.572336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.31
 ---- batch: 020 ----
mean loss: 119.34
 ---- batch: 030 ----
mean loss: 121.04
 ---- batch: 040 ----
mean loss: 117.28
train mean loss: 120.31
epoch train time: 0:00:01.686051
elapsed time: 0:05:18.877911
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 22:50:51.258527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.89
 ---- batch: 020 ----
mean loss: 118.99
 ---- batch: 030 ----
mean loss: 112.64
 ---- batch: 040 ----
mean loss: 120.53
train mean loss: 117.75
epoch train time: 0:00:01.669949
elapsed time: 0:05:20.547995
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 22:50:52.928608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.22
 ---- batch: 020 ----
mean loss: 119.35
 ---- batch: 030 ----
mean loss: 115.81
 ---- batch: 040 ----
mean loss: 116.68
train mean loss: 117.24
epoch train time: 0:00:01.727325
elapsed time: 0:05:22.275462
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 22:50:54.656079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.99
 ---- batch: 020 ----
mean loss: 119.46
 ---- batch: 030 ----
mean loss: 120.60
 ---- batch: 040 ----
mean loss: 116.09
train mean loss: 118.73
epoch train time: 0:00:01.672199
elapsed time: 0:05:23.947805
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 22:50:56.328455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.14
 ---- batch: 020 ----
mean loss: 123.60
 ---- batch: 030 ----
mean loss: 114.97
 ---- batch: 040 ----
mean loss: 121.42
train mean loss: 120.62
epoch train time: 0:00:01.796424
elapsed time: 0:05:25.744395
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 22:50:58.125009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.65
 ---- batch: 020 ----
mean loss: 118.32
 ---- batch: 030 ----
mean loss: 115.62
 ---- batch: 040 ----
mean loss: 120.69
train mean loss: 119.69
epoch train time: 0:00:01.675937
elapsed time: 0:05:27.420494
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 22:50:59.801115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.49
 ---- batch: 020 ----
mean loss: 119.69
 ---- batch: 030 ----
mean loss: 117.05
 ---- batch: 040 ----
mean loss: 122.49
train mean loss: 121.05
epoch train time: 0:00:01.731229
elapsed time: 0:05:29.151875
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 22:51:01.532507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.25
 ---- batch: 020 ----
mean loss: 120.49
 ---- batch: 030 ----
mean loss: 124.16
 ---- batch: 040 ----
mean loss: 119.51
train mean loss: 119.47
epoch train time: 0:00:01.676459
elapsed time: 0:05:30.828500
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 22:51:03.209119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.07
 ---- batch: 020 ----
mean loss: 119.90
 ---- batch: 030 ----
mean loss: 118.06
 ---- batch: 040 ----
mean loss: 118.50
train mean loss: 119.17
epoch train time: 0:00:01.779989
elapsed time: 0:05:32.608641
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 22:51:04.989257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.83
 ---- batch: 020 ----
mean loss: 111.96
 ---- batch: 030 ----
mean loss: 116.91
 ---- batch: 040 ----
mean loss: 117.29
train mean loss: 115.42
epoch train time: 0:00:01.680423
elapsed time: 0:05:34.289219
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 22:51:06.669834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.32
 ---- batch: 020 ----
mean loss: 124.40
 ---- batch: 030 ----
mean loss: 119.08
 ---- batch: 040 ----
mean loss: 116.19
train mean loss: 118.96
epoch train time: 0:00:01.780778
elapsed time: 0:05:36.070129
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 22:51:08.450758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.16
 ---- batch: 020 ----
mean loss: 113.38
 ---- batch: 030 ----
mean loss: 115.59
 ---- batch: 040 ----
mean loss: 117.07
train mean loss: 115.99
epoch train time: 0:00:01.686637
elapsed time: 0:05:37.756947
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 22:51:10.137584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.17
 ---- batch: 020 ----
mean loss: 118.68
 ---- batch: 030 ----
mean loss: 120.27
 ---- batch: 040 ----
mean loss: 116.68
train mean loss: 118.72
epoch train time: 0:00:01.776494
elapsed time: 0:05:39.533603
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 22:51:11.914224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.61
 ---- batch: 020 ----
mean loss: 116.69
 ---- batch: 030 ----
mean loss: 115.91
 ---- batch: 040 ----
mean loss: 120.50
train mean loss: 117.46
epoch train time: 0:00:01.686609
elapsed time: 0:05:41.220367
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 22:51:13.600984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.61
 ---- batch: 020 ----
mean loss: 119.87
 ---- batch: 030 ----
mean loss: 119.97
 ---- batch: 040 ----
mean loss: 122.74
train mean loss: 120.87
epoch train time: 0:00:01.677757
elapsed time: 0:05:42.898297
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 22:51:15.278912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.36
 ---- batch: 020 ----
mean loss: 122.00
 ---- batch: 030 ----
mean loss: 118.55
 ---- batch: 040 ----
mean loss: 122.47
train mean loss: 120.09
epoch train time: 0:00:01.798387
elapsed time: 0:05:44.696823
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 22:51:17.077454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.06
 ---- batch: 020 ----
mean loss: 117.80
 ---- batch: 030 ----
mean loss: 117.86
 ---- batch: 040 ----
mean loss: 119.28
train mean loss: 118.20
epoch train time: 0:00:01.675948
elapsed time: 0:05:46.372949
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 22:51:18.753563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.45
 ---- batch: 020 ----
mean loss: 117.65
 ---- batch: 030 ----
mean loss: 118.18
 ---- batch: 040 ----
mean loss: 118.77
train mean loss: 117.57
epoch train time: 0:00:01.791968
elapsed time: 0:05:48.165041
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 22:51:20.545655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.18
 ---- batch: 020 ----
mean loss: 122.63
 ---- batch: 030 ----
mean loss: 116.70
 ---- batch: 040 ----
mean loss: 115.85
train mean loss: 116.57
epoch train time: 0:00:01.679350
elapsed time: 0:05:49.844542
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 22:51:22.225159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.32
 ---- batch: 020 ----
mean loss: 120.67
 ---- batch: 030 ----
mean loss: 118.95
 ---- batch: 040 ----
mean loss: 119.89
train mean loss: 117.25
epoch train time: 0:00:01.789588
elapsed time: 0:05:51.634289
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 22:51:24.014900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.85
 ---- batch: 020 ----
mean loss: 123.20
 ---- batch: 030 ----
mean loss: 117.36
 ---- batch: 040 ----
mean loss: 116.41
train mean loss: 118.70
epoch train time: 0:00:01.684914
elapsed time: 0:05:53.319353
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 22:51:25.699968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.10
 ---- batch: 020 ----
mean loss: 115.33
 ---- batch: 030 ----
mean loss: 114.92
 ---- batch: 040 ----
mean loss: 118.23
train mean loss: 116.14
epoch train time: 0:00:01.779988
elapsed time: 0:05:55.099472
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 22:51:27.480101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.77
 ---- batch: 020 ----
mean loss: 116.57
 ---- batch: 030 ----
mean loss: 116.33
 ---- batch: 040 ----
mean loss: 114.65
train mean loss: 114.72
epoch train time: 0:00:01.685223
elapsed time: 0:05:56.784847
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 22:51:29.165508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.05
 ---- batch: 020 ----
mean loss: 120.37
 ---- batch: 030 ----
mean loss: 118.81
 ---- batch: 040 ----
mean loss: 115.15
train mean loss: 116.86
epoch train time: 0:00:01.772867
elapsed time: 0:05:58.557898
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 22:51:30.938513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.21
 ---- batch: 020 ----
mean loss: 114.16
 ---- batch: 030 ----
mean loss: 116.00
 ---- batch: 040 ----
mean loss: 113.52
train mean loss: 115.29
epoch train time: 0:00:01.675724
elapsed time: 0:06:00.233778
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 22:51:32.614396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.49
 ---- batch: 020 ----
mean loss: 112.38
 ---- batch: 030 ----
mean loss: 109.46
 ---- batch: 040 ----
mean loss: 112.77
train mean loss: 112.58
epoch train time: 0:00:01.726495
elapsed time: 0:06:01.960426
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 22:51:34.341030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.35
 ---- batch: 020 ----
mean loss: 114.87
 ---- batch: 030 ----
mean loss: 111.81
 ---- batch: 040 ----
mean loss: 110.78
train mean loss: 113.16
epoch train time: 0:00:01.722634
elapsed time: 0:06:03.683204
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 22:51:36.063821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.34
 ---- batch: 020 ----
mean loss: 115.86
 ---- batch: 030 ----
mean loss: 113.12
 ---- batch: 040 ----
mean loss: 113.50
train mean loss: 114.20
epoch train time: 0:00:01.668936
elapsed time: 0:06:05.352294
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 22:51:37.732938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.82
 ---- batch: 020 ----
mean loss: 111.08
 ---- batch: 030 ----
mean loss: 112.93
 ---- batch: 040 ----
mean loss: 115.00
train mean loss: 112.40
epoch train time: 0:00:01.791869
elapsed time: 0:06:07.144385
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 22:51:39.525002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.04
 ---- batch: 020 ----
mean loss: 113.86
 ---- batch: 030 ----
mean loss: 111.78
 ---- batch: 040 ----
mean loss: 113.11
train mean loss: 113.23
epoch train time: 0:00:01.671458
elapsed time: 0:06:08.815984
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 22:51:41.196602
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.31
 ---- batch: 020 ----
mean loss: 115.65
 ---- batch: 030 ----
mean loss: 114.78
 ---- batch: 040 ----
mean loss: 109.02
train mean loss: 113.72
epoch train time: 0:00:01.771290
elapsed time: 0:06:10.587440
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 22:51:42.968054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.78
 ---- batch: 020 ----
mean loss: 115.32
 ---- batch: 030 ----
mean loss: 111.34
 ---- batch: 040 ----
mean loss: 115.75
train mean loss: 114.06
epoch train time: 0:00:01.682363
elapsed time: 0:06:12.269946
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 22:51:44.650563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.56
 ---- batch: 020 ----
mean loss: 117.24
 ---- batch: 030 ----
mean loss: 110.04
 ---- batch: 040 ----
mean loss: 112.97
train mean loss: 113.44
epoch train time: 0:00:01.672101
elapsed time: 0:06:13.942192
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 22:51:46.322826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.38
 ---- batch: 020 ----
mean loss: 113.60
 ---- batch: 030 ----
mean loss: 109.72
 ---- batch: 040 ----
mean loss: 115.90
train mean loss: 112.51
epoch train time: 0:00:01.780174
elapsed time: 0:06:15.722594
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 22:51:48.103235
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.26
 ---- batch: 020 ----
mean loss: 113.03
 ---- batch: 030 ----
mean loss: 111.84
 ---- batch: 040 ----
mean loss: 111.03
train mean loss: 112.44
epoch train time: 0:00:01.677331
elapsed time: 0:06:17.400120
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 22:51:49.780746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.81
 ---- batch: 020 ----
mean loss: 112.03
 ---- batch: 030 ----
mean loss: 109.82
 ---- batch: 040 ----
mean loss: 113.93
train mean loss: 112.23
epoch train time: 0:00:01.778165
elapsed time: 0:06:19.178459
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 22:51:51.559076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.70
 ---- batch: 020 ----
mean loss: 111.53
 ---- batch: 030 ----
mean loss: 114.39
 ---- batch: 040 ----
mean loss: 116.61
train mean loss: 113.84
epoch train time: 0:00:01.677399
elapsed time: 0:06:20.855993
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 22:51:53.236607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.21
 ---- batch: 020 ----
mean loss: 107.07
 ---- batch: 030 ----
mean loss: 113.82
 ---- batch: 040 ----
mean loss: 114.51
train mean loss: 112.89
epoch train time: 0:00:01.775641
elapsed time: 0:06:22.631760
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 22:51:55.012372
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.55
 ---- batch: 020 ----
mean loss: 115.09
 ---- batch: 030 ----
mean loss: 114.34
 ---- batch: 040 ----
mean loss: 113.87
train mean loss: 113.43
epoch train time: 0:00:01.684448
elapsed time: 0:06:24.316360
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 22:51:56.696978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.84
 ---- batch: 020 ----
mean loss: 112.58
 ---- batch: 030 ----
mean loss: 114.10
 ---- batch: 040 ----
mean loss: 109.23
train mean loss: 112.32
epoch train time: 0:00:01.674102
elapsed time: 0:06:25.990617
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 22:51:58.371281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.67
 ---- batch: 020 ----
mean loss: 114.88
 ---- batch: 030 ----
mean loss: 113.48
 ---- batch: 040 ----
mean loss: 110.34
train mean loss: 112.22
epoch train time: 0:00:01.780219
elapsed time: 0:06:27.771010
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 22:52:00.151621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.44
 ---- batch: 020 ----
mean loss: 109.62
 ---- batch: 030 ----
mean loss: 113.35
 ---- batch: 040 ----
mean loss: 112.77
train mean loss: 111.84
epoch train time: 0:00:01.679615
elapsed time: 0:06:29.450782
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 22:52:01.831413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.38
 ---- batch: 020 ----
mean loss: 113.63
 ---- batch: 030 ----
mean loss: 112.06
 ---- batch: 040 ----
mean loss: 113.08
train mean loss: 113.88
epoch train time: 0:00:01.777398
elapsed time: 0:06:31.228333
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 22:52:03.608959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.55
 ---- batch: 020 ----
mean loss: 111.57
 ---- batch: 030 ----
mean loss: 115.29
 ---- batch: 040 ----
mean loss: 116.94
train mean loss: 113.45
epoch train time: 0:00:01.678133
elapsed time: 0:06:32.906621
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 22:52:05.287321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.40
 ---- batch: 020 ----
mean loss: 112.41
 ---- batch: 030 ----
mean loss: 112.09
 ---- batch: 040 ----
mean loss: 115.20
train mean loss: 113.27
epoch train time: 0:00:01.663943
elapsed time: 0:06:34.570794
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 22:52:06.951409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.75
 ---- batch: 020 ----
mean loss: 113.08
 ---- batch: 030 ----
mean loss: 112.63
 ---- batch: 040 ----
mean loss: 111.72
train mean loss: 113.36
epoch train time: 0:00:01.782637
elapsed time: 0:06:36.353588
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 22:52:08.734202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.16
 ---- batch: 020 ----
mean loss: 115.87
 ---- batch: 030 ----
mean loss: 111.95
 ---- batch: 040 ----
mean loss: 113.29
train mean loss: 113.89
epoch train time: 0:00:01.678157
elapsed time: 0:06:38.031919
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 22:52:10.412552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.57
 ---- batch: 020 ----
mean loss: 113.77
 ---- batch: 030 ----
mean loss: 116.10
 ---- batch: 040 ----
mean loss: 109.39
train mean loss: 112.93
epoch train time: 0:00:01.771959
elapsed time: 0:06:39.804038
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 22:52:12.184656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.72
 ---- batch: 020 ----
mean loss: 115.36
 ---- batch: 030 ----
mean loss: 111.36
 ---- batch: 040 ----
mean loss: 112.67
train mean loss: 113.60
epoch train time: 0:00:01.684489
elapsed time: 0:06:41.488675
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 22:52:13.869293
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.83
 ---- batch: 020 ----
mean loss: 112.89
 ---- batch: 030 ----
mean loss: 112.30
 ---- batch: 040 ----
mean loss: 115.91
train mean loss: 112.98
epoch train time: 0:00:01.781845
elapsed time: 0:06:43.270655
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 22:52:15.651269
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.92
 ---- batch: 020 ----
mean loss: 113.40
 ---- batch: 030 ----
mean loss: 112.17
 ---- batch: 040 ----
mean loss: 113.49
train mean loss: 113.60
epoch train time: 0:00:01.681222
elapsed time: 0:06:44.952034
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 22:52:17.332665
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.35
 ---- batch: 020 ----
mean loss: 113.20
 ---- batch: 030 ----
mean loss: 118.81
 ---- batch: 040 ----
mean loss: 109.66
train mean loss: 113.77
epoch train time: 0:00:01.674293
elapsed time: 0:06:46.626483
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 22:52:19.007116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.91
 ---- batch: 020 ----
mean loss: 108.39
 ---- batch: 030 ----
mean loss: 113.48
 ---- batch: 040 ----
mean loss: 114.18
train mean loss: 112.30
epoch train time: 0:00:01.794919
elapsed time: 0:06:48.421566
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 22:52:20.802185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.70
 ---- batch: 020 ----
mean loss: 115.23
 ---- batch: 030 ----
mean loss: 109.97
 ---- batch: 040 ----
mean loss: 112.78
train mean loss: 112.38
epoch train time: 0:00:01.674369
elapsed time: 0:06:50.096091
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 22:52:22.476711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.61
 ---- batch: 020 ----
mean loss: 116.05
 ---- batch: 030 ----
mean loss: 113.00
 ---- batch: 040 ----
mean loss: 112.11
train mean loss: 112.73
epoch train time: 0:00:01.783637
elapsed time: 0:06:51.879890
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 22:52:24.260508
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.46
 ---- batch: 020 ----
mean loss: 110.31
 ---- batch: 030 ----
mean loss: 116.58
 ---- batch: 040 ----
mean loss: 113.76
train mean loss: 113.95
epoch train time: 0:00:01.685327
elapsed time: 0:06:53.565364
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 22:52:25.945979
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.61
 ---- batch: 020 ----
mean loss: 115.87
 ---- batch: 030 ----
mean loss: 112.02
 ---- batch: 040 ----
mean loss: 111.08
train mean loss: 113.27
epoch train time: 0:00:01.776322
elapsed time: 0:06:55.341830
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 22:52:27.722451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.92
 ---- batch: 020 ----
mean loss: 114.07
 ---- batch: 030 ----
mean loss: 111.17
 ---- batch: 040 ----
mean loss: 111.55
train mean loss: 112.31
epoch train time: 0:00:01.677787
elapsed time: 0:06:57.019792
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 22:52:29.400419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.41
 ---- batch: 020 ----
mean loss: 113.16
 ---- batch: 030 ----
mean loss: 111.85
 ---- batch: 040 ----
mean loss: 110.25
train mean loss: 112.36
epoch train time: 0:00:01.668110
elapsed time: 0:06:58.688054
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 22:52:31.068668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.84
 ---- batch: 020 ----
mean loss: 113.46
 ---- batch: 030 ----
mean loss: 112.41
 ---- batch: 040 ----
mean loss: 112.92
train mean loss: 112.68
epoch train time: 0:00:01.775590
elapsed time: 0:07:00.463777
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 22:52:32.844419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.76
 ---- batch: 020 ----
mean loss: 110.57
 ---- batch: 030 ----
mean loss: 109.86
 ---- batch: 040 ----
mean loss: 112.47
train mean loss: 112.08
epoch train time: 0:00:01.672899
elapsed time: 0:07:02.136863
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 22:52:34.517482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.45
 ---- batch: 020 ----
mean loss: 109.13
 ---- batch: 030 ----
mean loss: 111.76
 ---- batch: 040 ----
mean loss: 113.44
train mean loss: 113.57
epoch train time: 0:00:01.780453
elapsed time: 0:07:03.917456
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 22:52:36.298072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.33
 ---- batch: 020 ----
mean loss: 111.92
 ---- batch: 030 ----
mean loss: 113.74
 ---- batch: 040 ----
mean loss: 113.60
train mean loss: 112.67
epoch train time: 0:00:01.686655
elapsed time: 0:07:05.604269
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 22:52:37.984890
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.03
 ---- batch: 020 ----
mean loss: 110.48
 ---- batch: 030 ----
mean loss: 112.54
 ---- batch: 040 ----
mean loss: 111.64
train mean loss: 111.94
epoch train time: 0:00:01.670987
elapsed time: 0:07:07.275435
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 22:52:39.656082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.95
 ---- batch: 020 ----
mean loss: 113.40
 ---- batch: 030 ----
mean loss: 108.07
 ---- batch: 040 ----
mean loss: 114.72
train mean loss: 113.12
epoch train time: 0:00:01.794496
elapsed time: 0:07:09.070111
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 22:52:41.450727
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.15
 ---- batch: 020 ----
mean loss: 114.29
 ---- batch: 030 ----
mean loss: 115.92
 ---- batch: 040 ----
mean loss: 112.24
train mean loss: 112.49
epoch train time: 0:00:01.677061
elapsed time: 0:07:10.747324
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 22:52:43.127941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.94
 ---- batch: 020 ----
mean loss: 112.50
 ---- batch: 030 ----
mean loss: 114.81
 ---- batch: 040 ----
mean loss: 111.60
train mean loss: 112.29
epoch train time: 0:00:01.789242
elapsed time: 0:07:12.536710
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 22:52:44.917328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.06
 ---- batch: 020 ----
mean loss: 111.43
 ---- batch: 030 ----
mean loss: 112.58
 ---- batch: 040 ----
mean loss: 112.73
train mean loss: 112.59
epoch train time: 0:00:01.679643
elapsed time: 0:07:14.216514
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 22:52:46.597130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.77
 ---- batch: 020 ----
mean loss: 110.48
 ---- batch: 030 ----
mean loss: 113.12
 ---- batch: 040 ----
mean loss: 112.34
train mean loss: 112.76
epoch train time: 0:00:01.787873
elapsed time: 0:07:16.004541
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 22:52:48.385173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.57
 ---- batch: 020 ----
mean loss: 118.00
 ---- batch: 030 ----
mean loss: 114.21
 ---- batch: 040 ----
mean loss: 108.27
train mean loss: 112.30
epoch train time: 0:00:01.689717
elapsed time: 0:07:17.694430
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 22:52:50.075045
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.20
 ---- batch: 020 ----
mean loss: 113.98
 ---- batch: 030 ----
mean loss: 115.37
 ---- batch: 040 ----
mean loss: 114.32
train mean loss: 113.70
epoch train time: 0:00:01.671866
elapsed time: 0:07:19.366432
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 22:52:51.747045
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.93
 ---- batch: 020 ----
mean loss: 110.56
 ---- batch: 030 ----
mean loss: 107.90
 ---- batch: 040 ----
mean loss: 113.03
train mean loss: 111.35
epoch train time: 0:00:01.787331
elapsed time: 0:07:21.153904
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 22:52:53.534536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.16
 ---- batch: 020 ----
mean loss: 113.07
 ---- batch: 030 ----
mean loss: 112.50
 ---- batch: 040 ----
mean loss: 107.00
train mean loss: 111.76
epoch train time: 0:00:01.684867
elapsed time: 0:07:22.838938
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 22:52:55.219572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.11
 ---- batch: 020 ----
mean loss: 111.53
 ---- batch: 030 ----
mean loss: 113.71
 ---- batch: 040 ----
mean loss: 112.42
train mean loss: 113.93
epoch train time: 0:00:01.772861
elapsed time: 0:07:24.615047
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_5/checkpoint.pth.tar
**** end time: 2019-09-20 22:52:56.995629 ****
