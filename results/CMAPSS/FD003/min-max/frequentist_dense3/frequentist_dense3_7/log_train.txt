Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_7', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8805
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistDense3...
Done.
**** start time: 2019-09-20 23:47:08.320240 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
            Linear-2                  [-1, 100]          42,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 62,100
Trainable params: 62,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:47:08.323355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4769.36
 ---- batch: 020 ----
mean loss: 4667.58
 ---- batch: 030 ----
mean loss: 4630.68
 ---- batch: 040 ----
mean loss: 4488.62
train mean loss: 4622.91
epoch train time: 0:00:15.274490
elapsed time: 0:00:15.279631
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:47:23.599908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4438.40
 ---- batch: 020 ----
mean loss: 4315.67
 ---- batch: 030 ----
mean loss: 4239.16
 ---- batch: 040 ----
mean loss: 4280.22
train mean loss: 4307.46
epoch train time: 0:00:00.201025
elapsed time: 0:00:15.480769
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:47:23.801075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4108.15
 ---- batch: 020 ----
mean loss: 4088.16
 ---- batch: 030 ----
mean loss: 4098.48
 ---- batch: 040 ----
mean loss: 3981.53
train mean loss: 4055.00
epoch train time: 0:00:00.194762
elapsed time: 0:00:15.675670
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:47:23.995952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3875.06
 ---- batch: 020 ----
mean loss: 3933.34
 ---- batch: 030 ----
mean loss: 3673.78
 ---- batch: 040 ----
mean loss: 3755.75
train mean loss: 3810.44
epoch train time: 0:00:00.197968
elapsed time: 0:00:15.873790
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:47:24.194088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3682.41
 ---- batch: 020 ----
mean loss: 3586.13
 ---- batch: 030 ----
mean loss: 3569.29
 ---- batch: 040 ----
mean loss: 3488.15
train mean loss: 3573.40
epoch train time: 0:00:00.195159
elapsed time: 0:00:16.069081
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:47:24.389362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3412.76
 ---- batch: 020 ----
mean loss: 3433.54
 ---- batch: 030 ----
mean loss: 3333.45
 ---- batch: 040 ----
mean loss: 3268.88
train mean loss: 3357.88
epoch train time: 0:00:00.197658
elapsed time: 0:00:16.266854
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:47:24.587135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3205.05
 ---- batch: 020 ----
mean loss: 3232.59
 ---- batch: 030 ----
mean loss: 3158.82
 ---- batch: 040 ----
mean loss: 3027.59
train mean loss: 3150.69
epoch train time: 0:00:00.194538
elapsed time: 0:00:16.461499
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:47:24.781786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3036.38
 ---- batch: 020 ----
mean loss: 2922.35
 ---- batch: 030 ----
mean loss: 2956.14
 ---- batch: 040 ----
mean loss: 2872.71
train mean loss: 2942.90
epoch train time: 0:00:00.199646
elapsed time: 0:00:16.661287
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:47:24.981613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2886.57
 ---- batch: 020 ----
mean loss: 2756.57
 ---- batch: 030 ----
mean loss: 2723.84
 ---- batch: 040 ----
mean loss: 2674.60
train mean loss: 2754.80
epoch train time: 0:00:00.200175
elapsed time: 0:00:16.861629
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:47:25.181910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2637.23
 ---- batch: 020 ----
mean loss: 2590.24
 ---- batch: 030 ----
mean loss: 2566.07
 ---- batch: 040 ----
mean loss: 2562.69
train mean loss: 2581.60
epoch train time: 0:00:00.199379
elapsed time: 0:00:17.061120
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:47:25.381402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2468.89
 ---- batch: 020 ----
mean loss: 2447.83
 ---- batch: 030 ----
mean loss: 2422.83
 ---- batch: 040 ----
mean loss: 2332.93
train mean loss: 2420.12
epoch train time: 0:00:00.198172
elapsed time: 0:00:17.259411
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:47:25.579695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2363.84
 ---- batch: 020 ----
mean loss: 2264.09
 ---- batch: 030 ----
mean loss: 2231.13
 ---- batch: 040 ----
mean loss: 2219.32
train mean loss: 2266.60
epoch train time: 0:00:00.213553
elapsed time: 0:00:17.473083
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:47:25.793365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2165.89
 ---- batch: 020 ----
mean loss: 2151.44
 ---- batch: 030 ----
mean loss: 2097.76
 ---- batch: 040 ----
mean loss: 2098.66
train mean loss: 2124.02
epoch train time: 0:00:00.218236
elapsed time: 0:00:17.691457
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:47:26.011766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2033.11
 ---- batch: 020 ----
mean loss: 2006.70
 ---- batch: 030 ----
mean loss: 1969.62
 ---- batch: 040 ----
mean loss: 1959.46
train mean loss: 1990.87
epoch train time: 0:00:00.208527
elapsed time: 0:00:17.900136
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:47:26.220447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1922.80
 ---- batch: 020 ----
mean loss: 1873.15
 ---- batch: 030 ----
mean loss: 1843.11
 ---- batch: 040 ----
mean loss: 1842.99
train mean loss: 1863.89
epoch train time: 0:00:00.208729
elapsed time: 0:00:18.109029
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:47:26.429315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1769.64
 ---- batch: 020 ----
mean loss: 1781.64
 ---- batch: 030 ----
mean loss: 1746.95
 ---- batch: 040 ----
mean loss: 1706.16
train mean loss: 1746.74
epoch train time: 0:00:00.206649
elapsed time: 0:00:18.315850
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:47:26.636136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1657.42
 ---- batch: 020 ----
mean loss: 1648.05
 ---- batch: 030 ----
mean loss: 1636.20
 ---- batch: 040 ----
mean loss: 1621.90
train mean loss: 1640.91
epoch train time: 0:00:00.207426
elapsed time: 0:00:18.523402
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:47:26.843686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1568.30
 ---- batch: 020 ----
mean loss: 1558.09
 ---- batch: 030 ----
mean loss: 1536.36
 ---- batch: 040 ----
mean loss: 1515.79
train mean loss: 1542.11
epoch train time: 0:00:00.207443
elapsed time: 0:00:18.730966
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:47:27.051249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1501.23
 ---- batch: 020 ----
mean loss: 1437.13
 ---- batch: 030 ----
mean loss: 1463.82
 ---- batch: 040 ----
mean loss: 1423.38
train mean loss: 1452.17
epoch train time: 0:00:00.206927
elapsed time: 0:00:18.938049
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:47:27.258334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1385.80
 ---- batch: 020 ----
mean loss: 1389.61
 ---- batch: 030 ----
mean loss: 1384.25
 ---- batch: 040 ----
mean loss: 1327.38
train mean loss: 1371.68
epoch train time: 0:00:00.205604
elapsed time: 0:00:19.143778
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:47:27.464063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1315.60
 ---- batch: 020 ----
mean loss: 1312.49
 ---- batch: 030 ----
mean loss: 1288.74
 ---- batch: 040 ----
mean loss: 1274.03
train mean loss: 1295.77
epoch train time: 0:00:00.207595
elapsed time: 0:00:19.351621
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:47:27.671932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1255.94
 ---- batch: 020 ----
mean loss: 1221.12
 ---- batch: 030 ----
mean loss: 1222.71
 ---- batch: 040 ----
mean loss: 1217.86
train mean loss: 1226.67
epoch train time: 0:00:00.212659
elapsed time: 0:00:19.564438
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:47:27.884717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1192.04
 ---- batch: 020 ----
mean loss: 1170.69
 ---- batch: 030 ----
mean loss: 1103.92
 ---- batch: 040 ----
mean loss: 1057.74
train mean loss: 1126.28
epoch train time: 0:00:00.197730
elapsed time: 0:00:19.762280
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:47:28.082560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1046.76
 ---- batch: 020 ----
mean loss: 1042.55
 ---- batch: 030 ----
mean loss: 1006.35
 ---- batch: 040 ----
mean loss: 982.39
train mean loss: 1015.02
epoch train time: 0:00:00.197215
elapsed time: 0:00:19.959602
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:47:28.279881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 986.18
 ---- batch: 020 ----
mean loss: 948.40
 ---- batch: 030 ----
mean loss: 939.52
 ---- batch: 040 ----
mean loss: 920.74
train mean loss: 945.20
epoch train time: 0:00:00.193589
elapsed time: 0:00:20.153314
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:47:28.473608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.23
 ---- batch: 020 ----
mean loss: 907.81
 ---- batch: 030 ----
mean loss: 876.25
 ---- batch: 040 ----
mean loss: 854.24
train mean loss: 882.20
epoch train time: 0:00:00.194195
elapsed time: 0:00:20.347634
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:47:28.667912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.05
 ---- batch: 020 ----
mean loss: 832.60
 ---- batch: 030 ----
mean loss: 806.23
 ---- batch: 040 ----
mean loss: 806.07
train mean loss: 823.14
epoch train time: 0:00:00.197909
elapsed time: 0:00:20.545684
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:47:28.865964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 793.47
 ---- batch: 020 ----
mean loss: 773.82
 ---- batch: 030 ----
mean loss: 771.39
 ---- batch: 040 ----
mean loss: 750.42
train mean loss: 769.15
epoch train time: 0:00:00.199550
elapsed time: 0:00:20.745352
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:47:29.065639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 747.64
 ---- batch: 020 ----
mean loss: 723.94
 ---- batch: 030 ----
mean loss: 710.12
 ---- batch: 040 ----
mean loss: 702.26
train mean loss: 718.83
epoch train time: 0:00:00.205012
elapsed time: 0:00:20.950483
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:47:29.270765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.81
 ---- batch: 020 ----
mean loss: 667.13
 ---- batch: 030 ----
mean loss: 677.84
 ---- batch: 040 ----
mean loss: 660.88
train mean loss: 672.49
epoch train time: 0:00:00.203938
elapsed time: 0:00:21.154542
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:47:29.474820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 634.48
 ---- batch: 020 ----
mean loss: 641.48
 ---- batch: 030 ----
mean loss: 631.35
 ---- batch: 040 ----
mean loss: 618.49
train mean loss: 628.45
epoch train time: 0:00:00.204422
elapsed time: 0:00:21.359129
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:47:29.679438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 604.50
 ---- batch: 020 ----
mean loss: 595.93
 ---- batch: 030 ----
mean loss: 579.64
 ---- batch: 040 ----
mean loss: 567.99
train mean loss: 583.81
epoch train time: 0:00:00.208532
elapsed time: 0:00:21.567825
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:47:29.888111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 552.67
 ---- batch: 020 ----
mean loss: 552.73
 ---- batch: 030 ----
mean loss: 531.10
 ---- batch: 040 ----
mean loss: 542.23
train mean loss: 541.74
epoch train time: 0:00:00.206882
elapsed time: 0:00:21.774833
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:47:30.095118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.80
 ---- batch: 020 ----
mean loss: 502.98
 ---- batch: 030 ----
mean loss: 497.28
 ---- batch: 040 ----
mean loss: 491.19
train mean loss: 501.42
epoch train time: 0:00:00.208335
elapsed time: 0:00:21.983303
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:47:30.303588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.77
 ---- batch: 020 ----
mean loss: 467.74
 ---- batch: 030 ----
mean loss: 459.72
 ---- batch: 040 ----
mean loss: 444.87
train mean loss: 462.95
epoch train time: 0:00:00.208139
elapsed time: 0:00:22.191567
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:47:30.511854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.45
 ---- batch: 020 ----
mean loss: 430.35
 ---- batch: 030 ----
mean loss: 424.22
 ---- batch: 040 ----
mean loss: 424.37
train mean loss: 427.33
epoch train time: 0:00:00.206625
elapsed time: 0:00:22.398323
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:47:30.718616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.00
 ---- batch: 020 ----
mean loss: 402.38
 ---- batch: 030 ----
mean loss: 394.27
 ---- batch: 040 ----
mean loss: 384.59
train mean loss: 395.16
epoch train time: 0:00:00.210365
elapsed time: 0:00:22.608855
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:47:30.929152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.23
 ---- batch: 020 ----
mean loss: 369.18
 ---- batch: 030 ----
mean loss: 362.42
 ---- batch: 040 ----
mean loss: 357.79
train mean loss: 365.35
epoch train time: 0:00:00.206854
elapsed time: 0:00:22.815844
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:47:31.136144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.95
 ---- batch: 020 ----
mean loss: 343.60
 ---- batch: 030 ----
mean loss: 332.86
 ---- batch: 040 ----
mean loss: 332.39
train mean loss: 339.11
epoch train time: 0:00:00.212842
elapsed time: 0:00:23.028827
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:47:31.349111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.30
 ---- batch: 020 ----
mean loss: 316.79
 ---- batch: 030 ----
mean loss: 311.70
 ---- batch: 040 ----
mean loss: 305.41
train mean loss: 314.62
epoch train time: 0:00:00.207899
elapsed time: 0:00:23.236848
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:47:31.557130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.68
 ---- batch: 020 ----
mean loss: 294.15
 ---- batch: 030 ----
mean loss: 291.97
 ---- batch: 040 ----
mean loss: 284.56
train mean loss: 292.53
epoch train time: 0:00:00.205324
elapsed time: 0:00:23.442302
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:47:31.762641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.84
 ---- batch: 020 ----
mean loss: 266.65
 ---- batch: 030 ----
mean loss: 275.62
 ---- batch: 040 ----
mean loss: 266.92
train mean loss: 271.53
epoch train time: 0:00:00.195664
elapsed time: 0:00:23.638138
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:47:31.958420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.21
 ---- batch: 020 ----
mean loss: 257.33
 ---- batch: 030 ----
mean loss: 253.07
 ---- batch: 040 ----
mean loss: 245.26
train mean loss: 253.17
epoch train time: 0:00:00.194398
elapsed time: 0:00:23.832650
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:47:32.152929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.07
 ---- batch: 020 ----
mean loss: 245.86
 ---- batch: 030 ----
mean loss: 230.28
 ---- batch: 040 ----
mean loss: 225.97
train mean loss: 235.41
epoch train time: 0:00:00.198826
elapsed time: 0:00:24.031649
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:47:32.351928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.90
 ---- batch: 020 ----
mean loss: 224.22
 ---- batch: 030 ----
mean loss: 218.16
 ---- batch: 040 ----
mean loss: 216.72
train mean loss: 220.65
epoch train time: 0:00:00.196375
elapsed time: 0:00:24.228134
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:47:32.548433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.36
 ---- batch: 020 ----
mean loss: 207.74
 ---- batch: 030 ----
mean loss: 204.54
 ---- batch: 040 ----
mean loss: 200.59
train mean loss: 205.52
epoch train time: 0:00:00.195484
elapsed time: 0:00:24.423753
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:47:32.744035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.65
 ---- batch: 020 ----
mean loss: 190.85
 ---- batch: 030 ----
mean loss: 195.37
 ---- batch: 040 ----
mean loss: 188.29
train mean loss: 191.69
epoch train time: 0:00:00.199261
elapsed time: 0:00:24.623143
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:47:32.943422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.92
 ---- batch: 020 ----
mean loss: 182.15
 ---- batch: 030 ----
mean loss: 175.19
 ---- batch: 040 ----
mean loss: 178.60
train mean loss: 179.62
epoch train time: 0:00:00.197445
elapsed time: 0:00:24.820700
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:47:33.140980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.04
 ---- batch: 020 ----
mean loss: 168.73
 ---- batch: 030 ----
mean loss: 166.12
 ---- batch: 040 ----
mean loss: 168.94
train mean loss: 168.23
epoch train time: 0:00:00.199577
elapsed time: 0:00:25.020390
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:47:33.340684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.04
 ---- batch: 020 ----
mean loss: 159.95
 ---- batch: 030 ----
mean loss: 155.91
 ---- batch: 040 ----
mean loss: 156.00
train mean loss: 158.67
epoch train time: 0:00:00.198102
elapsed time: 0:00:25.218620
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:47:33.538900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.89
 ---- batch: 020 ----
mean loss: 151.27
 ---- batch: 030 ----
mean loss: 147.36
 ---- batch: 040 ----
mean loss: 147.49
train mean loss: 149.29
epoch train time: 0:00:00.201969
elapsed time: 0:00:25.420712
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:47:33.740998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.19
 ---- batch: 020 ----
mean loss: 139.73
 ---- batch: 030 ----
mean loss: 138.91
 ---- batch: 040 ----
mean loss: 138.74
train mean loss: 140.33
epoch train time: 0:00:00.207695
elapsed time: 0:00:25.628536
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:47:33.948822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.70
 ---- batch: 020 ----
mean loss: 132.87
 ---- batch: 030 ----
mean loss: 133.96
 ---- batch: 040 ----
mean loss: 129.20
train mean loss: 133.21
epoch train time: 0:00:00.208597
elapsed time: 0:00:25.837927
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:47:34.158234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.70
 ---- batch: 020 ----
mean loss: 126.14
 ---- batch: 030 ----
mean loss: 124.15
 ---- batch: 040 ----
mean loss: 124.56
train mean loss: 125.63
epoch train time: 0:00:00.209576
elapsed time: 0:00:26.047649
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:47:34.367959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.28
 ---- batch: 020 ----
mean loss: 117.26
 ---- batch: 030 ----
mean loss: 119.70
 ---- batch: 040 ----
mean loss: 117.50
train mean loss: 119.34
epoch train time: 0:00:00.210253
elapsed time: 0:00:26.258053
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:47:34.578354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.87
 ---- batch: 020 ----
mean loss: 117.10
 ---- batch: 030 ----
mean loss: 116.68
 ---- batch: 040 ----
mean loss: 110.06
train mean loss: 114.90
epoch train time: 0:00:00.209003
elapsed time: 0:00:26.467198
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:47:34.787483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.91
 ---- batch: 020 ----
mean loss: 109.02
 ---- batch: 030 ----
mean loss: 105.84
 ---- batch: 040 ----
mean loss: 107.59
train mean loss: 108.91
epoch train time: 0:00:00.205460
elapsed time: 0:00:26.672786
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:47:34.993072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.69
 ---- batch: 020 ----
mean loss: 103.50
 ---- batch: 030 ----
mean loss: 104.52
 ---- batch: 040 ----
mean loss: 101.58
train mean loss: 103.86
epoch train time: 0:00:00.205910
elapsed time: 0:00:26.878822
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:47:35.199106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.89
 ---- batch: 020 ----
mean loss: 99.14
 ---- batch: 030 ----
mean loss: 98.09
 ---- batch: 040 ----
mean loss: 100.30
train mean loss: 99.62
epoch train time: 0:00:00.207362
elapsed time: 0:00:27.086304
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:47:35.406601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.32
 ---- batch: 020 ----
mean loss: 96.33
 ---- batch: 030 ----
mean loss: 95.04
 ---- batch: 040 ----
mean loss: 93.91
train mean loss: 95.83
epoch train time: 0:00:00.203110
elapsed time: 0:00:27.289545
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:47:35.609827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.80
 ---- batch: 020 ----
mean loss: 92.16
 ---- batch: 030 ----
mean loss: 93.74
 ---- batch: 040 ----
mean loss: 90.50
train mean loss: 92.11
epoch train time: 0:00:00.201901
elapsed time: 0:00:27.491579
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:47:35.811918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.84
 ---- batch: 020 ----
mean loss: 89.97
 ---- batch: 030 ----
mean loss: 87.43
 ---- batch: 040 ----
mean loss: 89.50
train mean loss: 89.49
epoch train time: 0:00:00.196636
elapsed time: 0:00:27.688394
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:47:36.008677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.58
 ---- batch: 020 ----
mean loss: 87.65
 ---- batch: 030 ----
mean loss: 86.34
 ---- batch: 040 ----
mean loss: 87.54
train mean loss: 86.36
epoch train time: 0:00:00.196375
elapsed time: 0:00:27.884890
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:47:36.205173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.83
 ---- batch: 020 ----
mean loss: 83.37
 ---- batch: 030 ----
mean loss: 86.39
 ---- batch: 040 ----
mean loss: 80.83
train mean loss: 83.69
epoch train time: 0:00:00.198159
elapsed time: 0:00:28.083164
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:47:36.403445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.90
 ---- batch: 020 ----
mean loss: 79.21
 ---- batch: 030 ----
mean loss: 81.34
 ---- batch: 040 ----
mean loss: 82.40
train mean loss: 80.75
epoch train time: 0:00:00.199902
elapsed time: 0:00:28.283184
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:47:36.603467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.69
 ---- batch: 020 ----
mean loss: 80.17
 ---- batch: 030 ----
mean loss: 77.18
 ---- batch: 040 ----
mean loss: 78.47
train mean loss: 78.96
epoch train time: 0:00:00.196320
elapsed time: 0:00:28.479617
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:47:36.799896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.27
 ---- batch: 020 ----
mean loss: 76.50
 ---- batch: 030 ----
mean loss: 76.90
 ---- batch: 040 ----
mean loss: 75.12
train mean loss: 76.50
epoch train time: 0:00:00.193117
elapsed time: 0:00:28.672844
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:47:36.993123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.31
 ---- batch: 020 ----
mean loss: 78.75
 ---- batch: 030 ----
mean loss: 77.35
 ---- batch: 040 ----
mean loss: 72.81
train mean loss: 76.14
epoch train time: 0:00:00.196428
elapsed time: 0:00:28.869384
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:47:37.189665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.33
 ---- batch: 020 ----
mean loss: 73.74
 ---- batch: 030 ----
mean loss: 75.47
 ---- batch: 040 ----
mean loss: 74.80
train mean loss: 75.19
epoch train time: 0:00:00.196975
elapsed time: 0:00:29.066476
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:47:37.386759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.20
 ---- batch: 020 ----
mean loss: 70.87
 ---- batch: 030 ----
mean loss: 69.13
 ---- batch: 040 ----
mean loss: 73.88
train mean loss: 71.83
epoch train time: 0:00:00.200048
elapsed time: 0:00:29.266643
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:47:37.586924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.80
 ---- batch: 020 ----
mean loss: 68.82
 ---- batch: 030 ----
mean loss: 69.97
 ---- batch: 040 ----
mean loss: 71.26
train mean loss: 70.55
epoch train time: 0:00:00.208934
elapsed time: 0:00:29.475698
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:47:37.795981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.79
 ---- batch: 020 ----
mean loss: 70.57
 ---- batch: 030 ----
mean loss: 70.04
 ---- batch: 040 ----
mean loss: 67.13
train mean loss: 68.91
epoch train time: 0:00:00.208222
elapsed time: 0:00:29.684050
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:47:38.004334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.23
 ---- batch: 020 ----
mean loss: 73.01
 ---- batch: 030 ----
mean loss: 68.37
 ---- batch: 040 ----
mean loss: 68.22
train mean loss: 68.54
epoch train time: 0:00:00.209713
elapsed time: 0:00:29.893892
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:47:38.214208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.47
 ---- batch: 020 ----
mean loss: 64.84
 ---- batch: 030 ----
mean loss: 67.68
 ---- batch: 040 ----
mean loss: 68.30
train mean loss: 67.08
epoch train time: 0:00:00.209154
elapsed time: 0:00:30.103199
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:47:38.423497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.10
 ---- batch: 020 ----
mean loss: 67.24
 ---- batch: 030 ----
mean loss: 67.57
 ---- batch: 040 ----
mean loss: 65.48
train mean loss: 66.72
epoch train time: 0:00:00.206792
elapsed time: 0:00:30.310128
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:47:38.630410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.28
 ---- batch: 020 ----
mean loss: 65.89
 ---- batch: 030 ----
mean loss: 65.31
 ---- batch: 040 ----
mean loss: 62.01
train mean loss: 65.05
epoch train time: 0:00:00.204941
elapsed time: 0:00:30.515205
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:47:38.835489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.78
 ---- batch: 020 ----
mean loss: 60.47
 ---- batch: 030 ----
mean loss: 66.33
 ---- batch: 040 ----
mean loss: 66.31
train mean loss: 64.44
epoch train time: 0:00:00.206229
elapsed time: 0:00:30.721566
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:47:39.041850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.34
 ---- batch: 020 ----
mean loss: 64.58
 ---- batch: 030 ----
mean loss: 63.90
 ---- batch: 040 ----
mean loss: 62.43
train mean loss: 63.27
epoch train time: 0:00:00.207229
elapsed time: 0:00:30.928916
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:47:39.249200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.35
 ---- batch: 020 ----
mean loss: 59.29
 ---- batch: 030 ----
mean loss: 64.75
 ---- batch: 040 ----
mean loss: 63.35
train mean loss: 62.89
epoch train time: 0:00:00.200199
elapsed time: 0:00:31.129245
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:47:39.449524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.68
 ---- batch: 020 ----
mean loss: 60.69
 ---- batch: 030 ----
mean loss: 62.25
 ---- batch: 040 ----
mean loss: 62.65
train mean loss: 62.11
epoch train time: 0:00:00.199359
elapsed time: 0:00:31.328718
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:47:39.648999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.91
 ---- batch: 020 ----
mean loss: 58.82
 ---- batch: 030 ----
mean loss: 61.33
 ---- batch: 040 ----
mean loss: 60.71
train mean loss: 61.01
epoch train time: 0:00:00.196021
elapsed time: 0:00:31.524859
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:47:39.845143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.68
 ---- batch: 020 ----
mean loss: 61.54
 ---- batch: 030 ----
mean loss: 62.96
 ---- batch: 040 ----
mean loss: 62.95
train mean loss: 61.35
epoch train time: 0:00:00.193817
elapsed time: 0:00:31.718793
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:47:40.039073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.07
 ---- batch: 020 ----
mean loss: 58.94
 ---- batch: 030 ----
mean loss: 59.65
 ---- batch: 040 ----
mean loss: 61.24
train mean loss: 60.57
epoch train time: 0:00:00.195411
elapsed time: 0:00:31.914315
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:47:40.234595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.80
 ---- batch: 020 ----
mean loss: 60.97
 ---- batch: 030 ----
mean loss: 59.56
 ---- batch: 040 ----
mean loss: 61.75
train mean loss: 60.34
epoch train time: 0:00:00.197024
elapsed time: 0:00:32.111466
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:47:40.431772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.46
 ---- batch: 020 ----
mean loss: 57.27
 ---- batch: 030 ----
mean loss: 60.23
 ---- batch: 040 ----
mean loss: 57.68
train mean loss: 59.51
epoch train time: 0:00:00.199228
elapsed time: 0:00:32.310835
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:47:40.631131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.01
 ---- batch: 020 ----
mean loss: 58.86
 ---- batch: 030 ----
mean loss: 57.55
 ---- batch: 040 ----
mean loss: 59.95
train mean loss: 58.88
epoch train time: 0:00:00.195485
elapsed time: 0:00:32.506447
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:47:40.826727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.40
 ---- batch: 020 ----
mean loss: 58.10
 ---- batch: 030 ----
mean loss: 57.72
 ---- batch: 040 ----
mean loss: 60.92
train mean loss: 58.36
epoch train time: 0:00:00.195327
elapsed time: 0:00:32.701892
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:47:41.022176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.67
 ---- batch: 020 ----
mean loss: 58.49
 ---- batch: 030 ----
mean loss: 57.34
 ---- batch: 040 ----
mean loss: 55.70
train mean loss: 57.86
epoch train time: 0:00:00.196402
elapsed time: 0:00:32.898419
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:47:41.218703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.14
 ---- batch: 020 ----
mean loss: 56.57
 ---- batch: 030 ----
mean loss: 55.09
 ---- batch: 040 ----
mean loss: 56.62
train mean loss: 57.13
epoch train time: 0:00:00.203424
elapsed time: 0:00:33.101982
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:47:41.422274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.56
 ---- batch: 020 ----
mean loss: 54.84
 ---- batch: 030 ----
mean loss: 56.85
 ---- batch: 040 ----
mean loss: 59.12
train mean loss: 56.73
epoch train time: 0:00:00.202941
elapsed time: 0:00:33.305072
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:47:41.625354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.87
 ---- batch: 020 ----
mean loss: 59.02
 ---- batch: 030 ----
mean loss: 54.75
 ---- batch: 040 ----
mean loss: 57.58
train mean loss: 56.82
epoch train time: 0:00:00.206559
elapsed time: 0:00:33.511752
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:47:41.832034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.13
 ---- batch: 020 ----
mean loss: 57.60
 ---- batch: 030 ----
mean loss: 55.02
 ---- batch: 040 ----
mean loss: 54.43
train mean loss: 55.89
epoch train time: 0:00:00.201941
elapsed time: 0:00:33.713812
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:47:42.034095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.07
 ---- batch: 020 ----
mean loss: 55.68
 ---- batch: 030 ----
mean loss: 56.82
 ---- batch: 040 ----
mean loss: 55.33
train mean loss: 55.58
epoch train time: 0:00:00.202513
elapsed time: 0:00:33.916448
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:47:42.236731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.30
 ---- batch: 020 ----
mean loss: 57.12
 ---- batch: 030 ----
mean loss: 53.84
 ---- batch: 040 ----
mean loss: 54.68
train mean loss: 55.12
epoch train time: 0:00:00.202967
elapsed time: 0:00:34.119540
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:47:42.439822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.79
 ---- batch: 020 ----
mean loss: 56.96
 ---- batch: 030 ----
mean loss: 53.36
 ---- batch: 040 ----
mean loss: 55.86
train mean loss: 55.04
epoch train time: 0:00:00.200853
elapsed time: 0:00:34.320513
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:47:42.640795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.32
 ---- batch: 020 ----
mean loss: 57.26
 ---- batch: 030 ----
mean loss: 56.42
 ---- batch: 040 ----
mean loss: 55.61
train mean loss: 56.76
epoch train time: 0:00:00.208662
elapsed time: 0:00:34.529296
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:47:42.849582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.36
 ---- batch: 020 ----
mean loss: 57.20
 ---- batch: 030 ----
mean loss: 51.07
 ---- batch: 040 ----
mean loss: 54.20
train mean loss: 54.43
epoch train time: 0:00:00.205978
elapsed time: 0:00:34.735397
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:47:43.055695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.12
 ---- batch: 020 ----
mean loss: 53.55
 ---- batch: 030 ----
mean loss: 51.15
 ---- batch: 040 ----
mean loss: 54.86
train mean loss: 53.95
epoch train time: 0:00:00.202450
elapsed time: 0:00:34.937979
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:47:43.258261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.59
 ---- batch: 020 ----
mean loss: 53.65
 ---- batch: 030 ----
mean loss: 53.28
 ---- batch: 040 ----
mean loss: 54.77
train mean loss: 53.63
epoch train time: 0:00:00.198495
elapsed time: 0:00:35.136592
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:47:43.456873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.69
 ---- batch: 020 ----
mean loss: 57.20
 ---- batch: 030 ----
mean loss: 55.96
 ---- batch: 040 ----
mean loss: 51.72
train mean loss: 54.94
epoch train time: 0:00:00.198373
elapsed time: 0:00:35.335100
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:47:43.655382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.92
 ---- batch: 020 ----
mean loss: 52.46
 ---- batch: 030 ----
mean loss: 55.06
 ---- batch: 040 ----
mean loss: 53.42
train mean loss: 53.23
epoch train time: 0:00:00.199755
elapsed time: 0:00:35.534977
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:47:43.855261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.71
 ---- batch: 020 ----
mean loss: 52.14
 ---- batch: 030 ----
mean loss: 52.96
 ---- batch: 040 ----
mean loss: 54.82
train mean loss: 53.50
epoch train time: 0:00:00.198912
elapsed time: 0:00:35.734024
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:47:44.054323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.14
 ---- batch: 020 ----
mean loss: 50.64
 ---- batch: 030 ----
mean loss: 54.68
 ---- batch: 040 ----
mean loss: 54.60
train mean loss: 52.83
epoch train time: 0:00:00.200322
elapsed time: 0:00:35.934485
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:47:44.254769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.63
 ---- batch: 020 ----
mean loss: 54.15
 ---- batch: 030 ----
mean loss: 52.85
 ---- batch: 040 ----
mean loss: 50.51
train mean loss: 52.71
epoch train time: 0:00:00.201952
elapsed time: 0:00:36.136563
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:47:44.456848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.25
 ---- batch: 020 ----
mean loss: 53.00
 ---- batch: 030 ----
mean loss: 49.66
 ---- batch: 040 ----
mean loss: 53.90
train mean loss: 52.27
epoch train time: 0:00:00.202930
elapsed time: 0:00:36.339612
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:47:44.659893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.54
 ---- batch: 020 ----
mean loss: 52.92
 ---- batch: 030 ----
mean loss: 52.18
 ---- batch: 040 ----
mean loss: 56.13
train mean loss: 52.77
epoch train time: 0:00:00.202994
elapsed time: 0:00:36.542739
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:47:44.863023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.18
 ---- batch: 020 ----
mean loss: 49.88
 ---- batch: 030 ----
mean loss: 52.63
 ---- batch: 040 ----
mean loss: 52.51
train mean loss: 51.86
epoch train time: 0:00:00.193437
elapsed time: 0:00:36.736308
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:47:45.056583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.50
 ---- batch: 020 ----
mean loss: 52.44
 ---- batch: 030 ----
mean loss: 50.87
 ---- batch: 040 ----
mean loss: 51.68
train mean loss: 52.10
epoch train time: 0:00:00.202496
elapsed time: 0:00:36.938915
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:47:45.259211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.05
 ---- batch: 020 ----
mean loss: 49.59
 ---- batch: 030 ----
mean loss: 50.22
 ---- batch: 040 ----
mean loss: 55.94
train mean loss: 51.80
epoch train time: 0:00:00.206413
elapsed time: 0:00:37.145463
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:47:45.465746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.64
 ---- batch: 020 ----
mean loss: 49.19
 ---- batch: 030 ----
mean loss: 53.11
 ---- batch: 040 ----
mean loss: 48.36
train mean loss: 51.16
epoch train time: 0:00:00.206540
elapsed time: 0:00:37.352168
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:47:45.672468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.49
 ---- batch: 020 ----
mean loss: 49.81
 ---- batch: 030 ----
mean loss: 52.28
 ---- batch: 040 ----
mean loss: 50.24
train mean loss: 50.64
epoch train time: 0:00:00.208602
elapsed time: 0:00:37.560910
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:47:45.881194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.52
 ---- batch: 020 ----
mean loss: 52.09
 ---- batch: 030 ----
mean loss: 49.74
 ---- batch: 040 ----
mean loss: 50.07
train mean loss: 51.07
epoch train time: 0:00:00.206995
elapsed time: 0:00:37.768037
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:47:46.088350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.84
 ---- batch: 020 ----
mean loss: 52.42
 ---- batch: 030 ----
mean loss: 50.21
 ---- batch: 040 ----
mean loss: 49.65
train mean loss: 51.26
epoch train time: 0:00:00.206962
elapsed time: 0:00:37.975155
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:47:46.295442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.88
 ---- batch: 020 ----
mean loss: 51.71
 ---- batch: 030 ----
mean loss: 50.14
 ---- batch: 040 ----
mean loss: 48.19
train mean loss: 50.26
epoch train time: 0:00:00.206734
elapsed time: 0:00:38.182016
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:47:46.502330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.68
 ---- batch: 020 ----
mean loss: 48.42
 ---- batch: 030 ----
mean loss: 52.46
 ---- batch: 040 ----
mean loss: 50.05
train mean loss: 50.29
epoch train time: 0:00:00.206516
elapsed time: 0:00:38.388687
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:47:46.708984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.07
 ---- batch: 020 ----
mean loss: 53.01
 ---- batch: 030 ----
mean loss: 49.50
 ---- batch: 040 ----
mean loss: 49.47
train mean loss: 50.96
epoch train time: 0:00:00.206802
elapsed time: 0:00:38.595626
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:47:46.915908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.93
 ---- batch: 020 ----
mean loss: 49.69
 ---- batch: 030 ----
mean loss: 49.48
 ---- batch: 040 ----
mean loss: 54.49
train mean loss: 50.45
epoch train time: 0:00:00.198914
elapsed time: 0:00:38.794657
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:47:47.114964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.42
 ---- batch: 020 ----
mean loss: 52.96
 ---- batch: 030 ----
mean loss: 49.57
 ---- batch: 040 ----
mean loss: 48.06
train mean loss: 51.11
epoch train time: 0:00:00.198528
elapsed time: 0:00:38.993338
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:47:47.313619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.36
 ---- batch: 020 ----
mean loss: 48.28
 ---- batch: 030 ----
mean loss: 50.06
 ---- batch: 040 ----
mean loss: 50.52
train mean loss: 49.57
epoch train time: 0:00:00.198486
elapsed time: 0:00:39.191942
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:47:47.512225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.06
 ---- batch: 020 ----
mean loss: 50.31
 ---- batch: 030 ----
mean loss: 52.32
 ---- batch: 040 ----
mean loss: 48.00
train mean loss: 50.38
epoch train time: 0:00:00.199975
elapsed time: 0:00:39.392041
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:47:47.712322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.53
 ---- batch: 020 ----
mean loss: 50.77
 ---- batch: 030 ----
mean loss: 50.27
 ---- batch: 040 ----
mean loss: 49.91
train mean loss: 49.69
epoch train time: 0:00:00.198047
elapsed time: 0:00:39.590216
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:47:47.910496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.03
 ---- batch: 020 ----
mean loss: 50.58
 ---- batch: 030 ----
mean loss: 46.08
 ---- batch: 040 ----
mean loss: 52.91
train mean loss: 49.25
epoch train time: 0:00:00.200886
elapsed time: 0:00:39.791219
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:47:48.111517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.71
 ---- batch: 020 ----
mean loss: 50.25
 ---- batch: 030 ----
mean loss: 50.41
 ---- batch: 040 ----
mean loss: 49.98
train mean loss: 49.70
epoch train time: 0:00:00.202919
elapsed time: 0:00:39.994285
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:47:48.314567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.80
 ---- batch: 020 ----
mean loss: 49.14
 ---- batch: 030 ----
mean loss: 49.06
 ---- batch: 040 ----
mean loss: 47.69
train mean loss: 48.95
epoch train time: 0:00:00.196464
elapsed time: 0:00:40.190866
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:47:48.511146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.49
 ---- batch: 020 ----
mean loss: 46.98
 ---- batch: 030 ----
mean loss: 46.81
 ---- batch: 040 ----
mean loss: 48.29
train mean loss: 48.04
epoch train time: 0:00:00.200953
elapsed time: 0:00:40.391933
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:47:48.712214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.44
 ---- batch: 020 ----
mean loss: 47.84
 ---- batch: 030 ----
mean loss: 51.26
 ---- batch: 040 ----
mean loss: 47.61
train mean loss: 48.17
epoch train time: 0:00:00.208326
elapsed time: 0:00:40.600395
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:47:48.920680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.18
 ---- batch: 020 ----
mean loss: 48.53
 ---- batch: 030 ----
mean loss: 47.62
 ---- batch: 040 ----
mean loss: 47.54
train mean loss: 48.44
epoch train time: 0:00:00.206198
elapsed time: 0:00:40.806727
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:47:49.127005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.65
 ---- batch: 020 ----
mean loss: 46.29
 ---- batch: 030 ----
mean loss: 47.41
 ---- batch: 040 ----
mean loss: 48.89
train mean loss: 48.23
epoch train time: 0:00:00.207811
elapsed time: 0:00:41.014656
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:47:49.334942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.49
 ---- batch: 020 ----
mean loss: 47.89
 ---- batch: 030 ----
mean loss: 47.15
 ---- batch: 040 ----
mean loss: 48.67
train mean loss: 47.55
epoch train time: 0:00:00.205591
elapsed time: 0:00:41.220410
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:47:49.540712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.52
 ---- batch: 020 ----
mean loss: 46.98
 ---- batch: 030 ----
mean loss: 48.94
 ---- batch: 040 ----
mean loss: 46.05
train mean loss: 47.92
epoch train time: 0:00:00.217367
elapsed time: 0:00:41.437927
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:47:49.758228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.17
 ---- batch: 020 ----
mean loss: 48.57
 ---- batch: 030 ----
mean loss: 45.53
 ---- batch: 040 ----
mean loss: 48.73
train mean loss: 47.60
epoch train time: 0:00:00.210694
elapsed time: 0:00:41.648762
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:47:49.969045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.05
 ---- batch: 020 ----
mean loss: 47.79
 ---- batch: 030 ----
mean loss: 45.87
 ---- batch: 040 ----
mean loss: 47.03
train mean loss: 47.95
epoch train time: 0:00:00.201725
elapsed time: 0:00:41.850603
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:47:50.170914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.51
 ---- batch: 020 ----
mean loss: 49.15
 ---- batch: 030 ----
mean loss: 46.99
 ---- batch: 040 ----
mean loss: 44.86
train mean loss: 47.82
epoch train time: 0:00:00.201605
elapsed time: 0:00:42.052361
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:47:50.372644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.96
 ---- batch: 020 ----
mean loss: 47.54
 ---- batch: 030 ----
mean loss: 48.03
 ---- batch: 040 ----
mean loss: 46.56
train mean loss: 46.69
epoch train time: 0:00:00.200512
elapsed time: 0:00:42.252990
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:47:50.573287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.23
 ---- batch: 020 ----
mean loss: 47.57
 ---- batch: 030 ----
mean loss: 47.10
 ---- batch: 040 ----
mean loss: 46.19
train mean loss: 46.88
epoch train time: 0:00:00.201985
elapsed time: 0:00:42.455124
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:47:50.775420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.20
 ---- batch: 020 ----
mean loss: 46.91
 ---- batch: 030 ----
mean loss: 45.61
 ---- batch: 040 ----
mean loss: 47.94
train mean loss: 46.54
epoch train time: 0:00:00.197959
elapsed time: 0:00:42.653226
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:47:50.973505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.52
 ---- batch: 020 ----
mean loss: 47.93
 ---- batch: 030 ----
mean loss: 45.75
 ---- batch: 040 ----
mean loss: 45.66
train mean loss: 46.28
epoch train time: 0:00:00.197297
elapsed time: 0:00:42.850635
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:47:51.170915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.03
 ---- batch: 020 ----
mean loss: 46.78
 ---- batch: 030 ----
mean loss: 47.22
 ---- batch: 040 ----
mean loss: 46.34
train mean loss: 46.59
epoch train time: 0:00:00.197898
elapsed time: 0:00:43.048653
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:47:51.368934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.20
 ---- batch: 020 ----
mean loss: 51.30
 ---- batch: 030 ----
mean loss: 46.33
 ---- batch: 040 ----
mean loss: 45.05
train mean loss: 47.16
epoch train time: 0:00:00.196262
elapsed time: 0:00:43.245050
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:47:51.565330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.88
 ---- batch: 020 ----
mean loss: 45.02
 ---- batch: 030 ----
mean loss: 46.12
 ---- batch: 040 ----
mean loss: 46.16
train mean loss: 46.02
epoch train time: 0:00:00.210347
elapsed time: 0:00:43.455544
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:47:51.775827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.13
 ---- batch: 020 ----
mean loss: 43.61
 ---- batch: 030 ----
mean loss: 46.08
 ---- batch: 040 ----
mean loss: 46.97
train mean loss: 45.91
epoch train time: 0:00:00.207065
elapsed time: 0:00:43.662723
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:47:51.983003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.52
 ---- batch: 020 ----
mean loss: 46.76
 ---- batch: 030 ----
mean loss: 46.90
 ---- batch: 040 ----
mean loss: 46.19
train mean loss: 45.79
epoch train time: 0:00:00.202904
elapsed time: 0:00:43.865744
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:47:52.186029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.09
 ---- batch: 020 ----
mean loss: 48.20
 ---- batch: 030 ----
mean loss: 46.50
 ---- batch: 040 ----
mean loss: 44.38
train mean loss: 45.46
epoch train time: 0:00:00.203832
elapsed time: 0:00:44.069715
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:47:52.389996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.92
 ---- batch: 020 ----
mean loss: 46.59
 ---- batch: 030 ----
mean loss: 46.37
 ---- batch: 040 ----
mean loss: 46.43
train mean loss: 45.98
epoch train time: 0:00:00.205451
elapsed time: 0:00:44.275300
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:47:52.595585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.16
 ---- batch: 020 ----
mean loss: 47.68
 ---- batch: 030 ----
mean loss: 45.23
 ---- batch: 040 ----
mean loss: 43.88
train mean loss: 45.58
epoch train time: 0:00:00.221474
elapsed time: 0:00:44.496899
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:47:52.817183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.62
 ---- batch: 020 ----
mean loss: 42.97
 ---- batch: 030 ----
mean loss: 45.46
 ---- batch: 040 ----
mean loss: 47.07
train mean loss: 45.51
epoch train time: 0:00:00.207884
elapsed time: 0:00:44.704907
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:47:53.025207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.69
 ---- batch: 020 ----
mean loss: 46.06
 ---- batch: 030 ----
mean loss: 44.23
 ---- batch: 040 ----
mean loss: 46.09
train mean loss: 45.34
epoch train time: 0:00:00.207569
elapsed time: 0:00:44.912617
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:47:53.232902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.73
 ---- batch: 020 ----
mean loss: 43.89
 ---- batch: 030 ----
mean loss: 45.96
 ---- batch: 040 ----
mean loss: 43.64
train mean loss: 45.18
epoch train time: 0:00:00.206372
elapsed time: 0:00:45.119107
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:47:53.439390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.94
 ---- batch: 020 ----
mean loss: 47.47
 ---- batch: 030 ----
mean loss: 43.68
 ---- batch: 040 ----
mean loss: 42.94
train mean loss: 44.41
epoch train time: 0:00:00.207670
elapsed time: 0:00:45.326941
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:47:53.647219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.39
 ---- batch: 020 ----
mean loss: 43.74
 ---- batch: 030 ----
mean loss: 45.34
 ---- batch: 040 ----
mean loss: 44.46
train mean loss: 44.51
epoch train time: 0:00:00.212058
elapsed time: 0:00:45.539139
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:47:53.859431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.65
 ---- batch: 020 ----
mean loss: 44.15
 ---- batch: 030 ----
mean loss: 45.60
 ---- batch: 040 ----
mean loss: 46.62
train mean loss: 45.11
epoch train time: 0:00:00.205699
elapsed time: 0:00:45.744968
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:47:54.065259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.11
 ---- batch: 020 ----
mean loss: 44.91
 ---- batch: 030 ----
mean loss: 43.69
 ---- batch: 040 ----
mean loss: 44.03
train mean loss: 44.09
epoch train time: 0:00:00.202882
elapsed time: 0:00:45.947978
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:47:54.268274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.55
 ---- batch: 020 ----
mean loss: 44.29
 ---- batch: 030 ----
mean loss: 44.26
 ---- batch: 040 ----
mean loss: 44.14
train mean loss: 44.22
epoch train time: 0:00:00.200045
elapsed time: 0:00:46.148203
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:47:54.468488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.03
 ---- batch: 020 ----
mean loss: 44.61
 ---- batch: 030 ----
mean loss: 41.40
 ---- batch: 040 ----
mean loss: 43.47
train mean loss: 43.72
epoch train time: 0:00:00.198868
elapsed time: 0:00:46.347195
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:47:54.667478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.58
 ---- batch: 020 ----
mean loss: 44.25
 ---- batch: 030 ----
mean loss: 43.26
 ---- batch: 040 ----
mean loss: 44.10
train mean loss: 43.92
epoch train time: 0:00:00.200409
elapsed time: 0:00:46.547724
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:47:54.868007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.05
 ---- batch: 020 ----
mean loss: 43.21
 ---- batch: 030 ----
mean loss: 45.41
 ---- batch: 040 ----
mean loss: 42.82
train mean loss: 43.40
epoch train time: 0:00:00.202333
elapsed time: 0:00:46.750190
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:47:55.070471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.52
 ---- batch: 020 ----
mean loss: 43.60
 ---- batch: 030 ----
mean loss: 43.80
 ---- batch: 040 ----
mean loss: 43.95
train mean loss: 43.48
epoch train time: 0:00:00.203958
elapsed time: 0:00:46.954261
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:47:55.274542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.77
 ---- batch: 020 ----
mean loss: 45.11
 ---- batch: 030 ----
mean loss: 44.47
 ---- batch: 040 ----
mean loss: 43.50
train mean loss: 43.94
epoch train time: 0:00:00.206057
elapsed time: 0:00:47.160435
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:47:55.480731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.22
 ---- batch: 020 ----
mean loss: 42.75
 ---- batch: 030 ----
mean loss: 43.42
 ---- batch: 040 ----
mean loss: 42.32
train mean loss: 42.87
epoch train time: 0:00:00.205308
elapsed time: 0:00:47.365890
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:47:55.686182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.67
 ---- batch: 020 ----
mean loss: 41.57
 ---- batch: 030 ----
mean loss: 42.99
 ---- batch: 040 ----
mean loss: 45.89
train mean loss: 43.04
epoch train time: 0:00:00.204447
elapsed time: 0:00:47.570464
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:47:55.890771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.80
 ---- batch: 020 ----
mean loss: 42.46
 ---- batch: 030 ----
mean loss: 42.70
 ---- batch: 040 ----
mean loss: 45.02
train mean loss: 43.17
epoch train time: 0:00:00.205360
elapsed time: 0:00:47.775966
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:47:56.096262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.67
 ---- batch: 020 ----
mean loss: 41.21
 ---- batch: 030 ----
mean loss: 43.75
 ---- batch: 040 ----
mean loss: 43.49
train mean loss: 42.85
epoch train time: 0:00:00.206466
elapsed time: 0:00:47.982560
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:47:56.302840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.16
 ---- batch: 020 ----
mean loss: 43.63
 ---- batch: 030 ----
mean loss: 40.78
 ---- batch: 040 ----
mean loss: 44.57
train mean loss: 42.52
epoch train time: 0:00:00.209708
elapsed time: 0:00:48.192390
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:47:56.512676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.40
 ---- batch: 020 ----
mean loss: 41.23
 ---- batch: 030 ----
mean loss: 42.97
 ---- batch: 040 ----
mean loss: 43.81
train mean loss: 42.27
epoch train time: 0:00:00.213968
elapsed time: 0:00:48.406483
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:47:56.726777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.51
 ---- batch: 020 ----
mean loss: 43.06
 ---- batch: 030 ----
mean loss: 43.58
 ---- batch: 040 ----
mean loss: 41.95
train mean loss: 42.47
epoch train time: 0:00:00.207185
elapsed time: 0:00:48.613800
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:47:56.934083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.84
 ---- batch: 020 ----
mean loss: 41.89
 ---- batch: 030 ----
mean loss: 42.20
 ---- batch: 040 ----
mean loss: 43.26
train mean loss: 42.76
epoch train time: 0:00:00.206390
elapsed time: 0:00:48.820322
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:47:57.140634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.29
 ---- batch: 020 ----
mean loss: 41.59
 ---- batch: 030 ----
mean loss: 42.46
 ---- batch: 040 ----
mean loss: 40.07
train mean loss: 41.81
epoch train time: 0:00:00.202113
elapsed time: 0:00:49.022580
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:47:57.342861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.35
 ---- batch: 020 ----
mean loss: 42.55
 ---- batch: 030 ----
mean loss: 40.78
 ---- batch: 040 ----
mean loss: 40.85
train mean loss: 41.64
epoch train time: 0:00:00.203446
elapsed time: 0:00:49.226150
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:47:57.546436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.59
 ---- batch: 020 ----
mean loss: 42.03
 ---- batch: 030 ----
mean loss: 44.96
 ---- batch: 040 ----
mean loss: 39.66
train mean loss: 42.08
epoch train time: 0:00:00.216458
elapsed time: 0:00:49.442734
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:47:57.763018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.54
 ---- batch: 020 ----
mean loss: 39.79
 ---- batch: 030 ----
mean loss: 43.68
 ---- batch: 040 ----
mean loss: 42.63
train mean loss: 41.83
epoch train time: 0:00:00.204200
elapsed time: 0:00:49.647060
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:47:57.967358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.36
 ---- batch: 020 ----
mean loss: 41.00
 ---- batch: 030 ----
mean loss: 42.06
 ---- batch: 040 ----
mean loss: 41.62
train mean loss: 41.41
epoch train time: 0:00:00.201500
elapsed time: 0:00:49.848698
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:47:58.168981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.74
 ---- batch: 020 ----
mean loss: 39.86
 ---- batch: 030 ----
mean loss: 43.02
 ---- batch: 040 ----
mean loss: 41.83
train mean loss: 41.29
epoch train time: 0:00:00.194550
elapsed time: 0:00:50.043380
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:47:58.363660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.34
 ---- batch: 020 ----
mean loss: 42.49
 ---- batch: 030 ----
mean loss: 41.47
 ---- batch: 040 ----
mean loss: 44.35
train mean loss: 42.27
epoch train time: 0:00:00.193905
elapsed time: 0:00:50.237399
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:47:58.557682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.06
 ---- batch: 020 ----
mean loss: 44.32
 ---- batch: 030 ----
mean loss: 43.76
 ---- batch: 040 ----
mean loss: 40.75
train mean loss: 42.15
epoch train time: 0:00:00.195174
elapsed time: 0:00:50.432709
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:47:58.752983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.15
 ---- batch: 020 ----
mean loss: 41.04
 ---- batch: 030 ----
mean loss: 41.34
 ---- batch: 040 ----
mean loss: 40.03
train mean loss: 40.96
epoch train time: 0:00:00.196487
elapsed time: 0:00:50.629299
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:47:58.949577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.07
 ---- batch: 020 ----
mean loss: 41.33
 ---- batch: 030 ----
mean loss: 41.52
 ---- batch: 040 ----
mean loss: 41.26
train mean loss: 41.46
epoch train time: 0:00:00.194278
elapsed time: 0:00:50.823685
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:47:59.143962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.54
 ---- batch: 020 ----
mean loss: 42.60
 ---- batch: 030 ----
mean loss: 40.95
 ---- batch: 040 ----
mean loss: 43.69
train mean loss: 41.91
epoch train time: 0:00:00.203072
elapsed time: 0:00:51.026869
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:47:59.347151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.08
 ---- batch: 020 ----
mean loss: 39.52
 ---- batch: 030 ----
mean loss: 38.39
 ---- batch: 040 ----
mean loss: 41.93
train mean loss: 40.41
epoch train time: 0:00:00.200060
elapsed time: 0:00:51.227059
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:47:59.547342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.95
 ---- batch: 020 ----
mean loss: 40.32
 ---- batch: 030 ----
mean loss: 38.62
 ---- batch: 040 ----
mean loss: 40.44
train mean loss: 40.22
epoch train time: 0:00:00.201003
elapsed time: 0:00:51.428199
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:47:59.748498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.46
 ---- batch: 020 ----
mean loss: 44.12
 ---- batch: 030 ----
mean loss: 41.18
 ---- batch: 040 ----
mean loss: 40.48
train mean loss: 41.90
epoch train time: 0:00:00.197597
elapsed time: 0:00:51.625940
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:47:59.946220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.42
 ---- batch: 020 ----
mean loss: 42.47
 ---- batch: 030 ----
mean loss: 38.85
 ---- batch: 040 ----
mean loss: 39.20
train mean loss: 39.96
epoch train time: 0:00:00.205702
elapsed time: 0:00:51.831773
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:48:00.152085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.46
 ---- batch: 020 ----
mean loss: 39.04
 ---- batch: 030 ----
mean loss: 38.95
 ---- batch: 040 ----
mean loss: 40.97
train mean loss: 39.96
epoch train time: 0:00:00.209090
elapsed time: 0:00:52.041055
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:48:00.361360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.42
 ---- batch: 020 ----
mean loss: 39.58
 ---- batch: 030 ----
mean loss: 37.53
 ---- batch: 040 ----
mean loss: 41.44
train mean loss: 39.79
epoch train time: 0:00:00.204821
elapsed time: 0:00:52.246018
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:48:00.566304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.76
 ---- batch: 020 ----
mean loss: 40.59
 ---- batch: 030 ----
mean loss: 42.62
 ---- batch: 040 ----
mean loss: 39.93
train mean loss: 40.70
epoch train time: 0:00:00.211230
elapsed time: 0:00:52.457375
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:48:00.777663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.48
 ---- batch: 020 ----
mean loss: 41.88
 ---- batch: 030 ----
mean loss: 37.31
 ---- batch: 040 ----
mean loss: 42.10
train mean loss: 39.92
epoch train time: 0:00:00.212974
elapsed time: 0:00:52.670474
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:48:00.990758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.16
 ---- batch: 020 ----
mean loss: 37.08
 ---- batch: 030 ----
mean loss: 42.21
 ---- batch: 040 ----
mean loss: 39.50
train mean loss: 39.82
epoch train time: 0:00:00.206807
elapsed time: 0:00:52.877404
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:48:01.197689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.32
 ---- batch: 020 ----
mean loss: 41.52
 ---- batch: 030 ----
mean loss: 40.16
 ---- batch: 040 ----
mean loss: 35.06
train mean loss: 39.64
epoch train time: 0:00:00.204258
elapsed time: 0:00:53.081816
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:48:01.402102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.68
 ---- batch: 020 ----
mean loss: 41.93
 ---- batch: 030 ----
mean loss: 40.42
 ---- batch: 040 ----
mean loss: 41.58
train mean loss: 40.61
epoch train time: 0:00:00.206379
elapsed time: 0:00:53.288325
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:48:01.608612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.42
 ---- batch: 020 ----
mean loss: 39.68
 ---- batch: 030 ----
mean loss: 40.19
 ---- batch: 040 ----
mean loss: 41.23
train mean loss: 39.72
epoch train time: 0:00:00.209360
elapsed time: 0:00:53.497813
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:48:01.818098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.09
 ---- batch: 020 ----
mean loss: 39.27
 ---- batch: 030 ----
mean loss: 41.34
 ---- batch: 040 ----
mean loss: 41.33
train mean loss: 39.72
epoch train time: 0:00:00.203814
elapsed time: 0:00:53.701773
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:48:02.022068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.47
 ---- batch: 020 ----
mean loss: 40.08
 ---- batch: 030 ----
mean loss: 37.17
 ---- batch: 040 ----
mean loss: 39.66
train mean loss: 38.65
epoch train time: 0:00:00.200430
elapsed time: 0:00:53.902348
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:48:02.222632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.63
 ---- batch: 020 ----
mean loss: 37.75
 ---- batch: 030 ----
mean loss: 40.75
 ---- batch: 040 ----
mean loss: 37.61
train mean loss: 38.44
epoch train time: 0:00:00.198627
elapsed time: 0:00:54.101111
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:48:02.421395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.22
 ---- batch: 020 ----
mean loss: 37.96
 ---- batch: 030 ----
mean loss: 41.23
 ---- batch: 040 ----
mean loss: 37.65
train mean loss: 39.20
epoch train time: 0:00:00.198602
elapsed time: 0:00:54.299838
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:48:02.620123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.79
 ---- batch: 020 ----
mean loss: 37.32
 ---- batch: 030 ----
mean loss: 39.55
 ---- batch: 040 ----
mean loss: 38.09
train mean loss: 38.48
epoch train time: 0:00:00.203910
elapsed time: 0:00:54.503870
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:48:02.824167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.27
 ---- batch: 020 ----
mean loss: 41.46
 ---- batch: 030 ----
mean loss: 37.53
 ---- batch: 040 ----
mean loss: 37.39
train mean loss: 38.86
epoch train time: 0:00:00.199105
elapsed time: 0:00:54.703106
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:48:03.023387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.95
 ---- batch: 020 ----
mean loss: 38.07
 ---- batch: 030 ----
mean loss: 39.60
 ---- batch: 040 ----
mean loss: 39.35
train mean loss: 38.78
epoch train time: 0:00:00.197907
elapsed time: 0:00:54.901130
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:48:03.221412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.09
 ---- batch: 020 ----
mean loss: 40.11
 ---- batch: 030 ----
mean loss: 38.12
 ---- batch: 040 ----
mean loss: 39.30
train mean loss: 38.74
epoch train time: 0:00:00.197835
elapsed time: 0:00:55.099082
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:48:03.419364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.57
 ---- batch: 020 ----
mean loss: 38.55
 ---- batch: 030 ----
mean loss: 38.36
 ---- batch: 040 ----
mean loss: 37.74
train mean loss: 38.39
epoch train time: 0:00:00.199085
elapsed time: 0:00:55.298282
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:48:03.618561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.80
 ---- batch: 020 ----
mean loss: 37.79
 ---- batch: 030 ----
mean loss: 39.20
 ---- batch: 040 ----
mean loss: 38.16
train mean loss: 38.00
epoch train time: 0:00:00.201525
elapsed time: 0:00:55.499940
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:48:03.820235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.07
 ---- batch: 020 ----
mean loss: 37.45
 ---- batch: 030 ----
mean loss: 37.63
 ---- batch: 040 ----
mean loss: 38.80
train mean loss: 37.97
epoch train time: 0:00:00.207479
elapsed time: 0:00:55.707563
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:48:04.027851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.46
 ---- batch: 020 ----
mean loss: 37.57
 ---- batch: 030 ----
mean loss: 39.28
 ---- batch: 040 ----
mean loss: 39.06
train mean loss: 38.62
epoch train time: 0:00:00.208803
elapsed time: 0:00:55.916503
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:48:04.236790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.90
 ---- batch: 020 ----
mean loss: 36.16
 ---- batch: 030 ----
mean loss: 36.67
 ---- batch: 040 ----
mean loss: 38.36
train mean loss: 36.68
epoch train time: 0:00:00.208388
elapsed time: 0:00:56.125037
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:48:04.445317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.27
 ---- batch: 020 ----
mean loss: 37.63
 ---- batch: 030 ----
mean loss: 36.97
 ---- batch: 040 ----
mean loss: 36.27
train mean loss: 36.55
epoch train time: 0:00:00.207329
elapsed time: 0:00:56.332493
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:48:04.652779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.37
 ---- batch: 020 ----
mean loss: 37.19
 ---- batch: 030 ----
mean loss: 35.85
 ---- batch: 040 ----
mean loss: 35.48
train mean loss: 36.50
epoch train time: 0:00:00.206169
elapsed time: 0:00:56.538785
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:48:04.859069
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.60
 ---- batch: 020 ----
mean loss: 37.01
 ---- batch: 030 ----
mean loss: 36.38
 ---- batch: 040 ----
mean loss: 37.62
train mean loss: 36.49
epoch train time: 0:00:00.201781
elapsed time: 0:00:56.740693
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:48:05.060975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.65
 ---- batch: 020 ----
mean loss: 36.17
 ---- batch: 030 ----
mean loss: 37.24
 ---- batch: 040 ----
mean loss: 36.26
train mean loss: 36.50
epoch train time: 0:00:00.210751
elapsed time: 0:00:56.951566
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:48:05.271865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.30
 ---- batch: 020 ----
mean loss: 35.65
 ---- batch: 030 ----
mean loss: 37.04
 ---- batch: 040 ----
mean loss: 35.72
train mean loss: 36.51
epoch train time: 0:00:00.207135
elapsed time: 0:00:57.158835
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:48:05.479117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.09
 ---- batch: 020 ----
mean loss: 35.09
 ---- batch: 030 ----
mean loss: 35.85
 ---- batch: 040 ----
mean loss: 36.85
train mean loss: 36.60
epoch train time: 0:00:00.205084
elapsed time: 0:00:57.364056
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:48:05.684354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.14
 ---- batch: 020 ----
mean loss: 36.41
 ---- batch: 030 ----
mean loss: 35.16
 ---- batch: 040 ----
mean loss: 35.93
train mean loss: 36.40
epoch train time: 0:00:00.199637
elapsed time: 0:00:57.563825
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:48:05.884137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.52
 ---- batch: 020 ----
mean loss: 36.23
 ---- batch: 030 ----
mean loss: 35.23
 ---- batch: 040 ----
mean loss: 39.21
train mean loss: 36.47
epoch train time: 0:00:00.202007
elapsed time: 0:00:57.765982
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:48:06.086265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.08
 ---- batch: 020 ----
mean loss: 37.60
 ---- batch: 030 ----
mean loss: 36.81
 ---- batch: 040 ----
mean loss: 34.62
train mean loss: 36.45
epoch train time: 0:00:00.197825
elapsed time: 0:00:57.963952
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:48:06.284241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.37
 ---- batch: 020 ----
mean loss: 36.03
 ---- batch: 030 ----
mean loss: 35.59
 ---- batch: 040 ----
mean loss: 35.59
train mean loss: 36.42
epoch train time: 0:00:00.197531
elapsed time: 0:00:58.161625
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:48:06.481906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.83
 ---- batch: 020 ----
mean loss: 35.32
 ---- batch: 030 ----
mean loss: 35.74
 ---- batch: 040 ----
mean loss: 36.75
train mean loss: 36.37
epoch train time: 0:00:00.198063
elapsed time: 0:00:58.359847
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:48:06.680127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.29
 ---- batch: 020 ----
mean loss: 34.19
 ---- batch: 030 ----
mean loss: 35.45
 ---- batch: 040 ----
mean loss: 37.96
train mean loss: 36.43
epoch train time: 0:00:00.203826
elapsed time: 0:00:58.563792
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:48:06.884073
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.70
 ---- batch: 020 ----
mean loss: 36.49
 ---- batch: 030 ----
mean loss: 36.48
 ---- batch: 040 ----
mean loss: 36.86
train mean loss: 36.40
epoch train time: 0:00:00.200017
elapsed time: 0:00:58.763924
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:48:07.084205
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.33
 ---- batch: 020 ----
mean loss: 35.52
 ---- batch: 030 ----
mean loss: 36.74
 ---- batch: 040 ----
mean loss: 37.12
train mean loss: 36.30
epoch train time: 0:00:00.199226
elapsed time: 0:00:58.963297
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:48:07.283580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 39.12
 ---- batch: 020 ----
mean loss: 36.67
 ---- batch: 030 ----
mean loss: 35.93
 ---- batch: 040 ----
mean loss: 33.38
train mean loss: 36.33
epoch train time: 0:00:00.194946
elapsed time: 0:00:59.158392
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:48:07.478690
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.83
 ---- batch: 020 ----
mean loss: 36.04
 ---- batch: 030 ----
mean loss: 36.87
 ---- batch: 040 ----
mean loss: 35.46
train mean loss: 36.37
epoch train time: 0:00:00.205759
elapsed time: 0:00:59.364285
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:48:07.684569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.20
 ---- batch: 020 ----
mean loss: 38.86
 ---- batch: 030 ----
mean loss: 35.75
 ---- batch: 040 ----
mean loss: 37.13
train mean loss: 36.28
epoch train time: 0:00:00.206918
elapsed time: 0:00:59.571322
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:48:07.891619
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.25
 ---- batch: 020 ----
mean loss: 37.42
 ---- batch: 030 ----
mean loss: 35.91
 ---- batch: 040 ----
mean loss: 34.91
train mean loss: 36.35
epoch train time: 0:00:00.208059
elapsed time: 0:00:59.779533
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:48:08.099824
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.24
 ---- batch: 020 ----
mean loss: 36.40
 ---- batch: 030 ----
mean loss: 37.84
 ---- batch: 040 ----
mean loss: 35.68
train mean loss: 36.26
epoch train time: 0:00:00.206847
elapsed time: 0:00:59.986508
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:48:08.306791
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.47
 ---- batch: 020 ----
mean loss: 36.13
 ---- batch: 030 ----
mean loss: 35.67
 ---- batch: 040 ----
mean loss: 37.58
train mean loss: 36.34
epoch train time: 0:00:00.206896
elapsed time: 0:01:00.193534
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:48:08.513819
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.57
 ---- batch: 020 ----
mean loss: 37.12
 ---- batch: 030 ----
mean loss: 35.11
 ---- batch: 040 ----
mean loss: 36.91
train mean loss: 36.32
epoch train time: 0:00:00.207945
elapsed time: 0:01:00.401602
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:48:08.721886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.15
 ---- batch: 020 ----
mean loss: 35.75
 ---- batch: 030 ----
mean loss: 36.02
 ---- batch: 040 ----
mean loss: 36.81
train mean loss: 36.22
epoch train time: 0:00:00.207540
elapsed time: 0:01:00.609282
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:48:08.929581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.85
 ---- batch: 020 ----
mean loss: 37.23
 ---- batch: 030 ----
mean loss: 35.55
 ---- batch: 040 ----
mean loss: 36.44
train mean loss: 36.30
epoch train time: 0:00:00.206339
elapsed time: 0:01:00.815770
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:48:09.136058
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.06
 ---- batch: 020 ----
mean loss: 36.46
 ---- batch: 030 ----
mean loss: 36.27
 ---- batch: 040 ----
mean loss: 36.14
train mean loss: 36.31
epoch train time: 0:00:00.206873
elapsed time: 0:01:01.022764
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:48:09.343044
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.36
 ---- batch: 020 ----
mean loss: 36.45
 ---- batch: 030 ----
mean loss: 36.54
 ---- batch: 040 ----
mean loss: 36.55
train mean loss: 36.18
epoch train time: 0:00:00.198609
elapsed time: 0:01:01.221489
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:48:09.541784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.13
 ---- batch: 020 ----
mean loss: 36.44
 ---- batch: 030 ----
mean loss: 37.21
 ---- batch: 040 ----
mean loss: 35.42
train mean loss: 36.12
epoch train time: 0:00:00.207039
elapsed time: 0:01:01.428661
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:48:09.748942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.71
 ---- batch: 020 ----
mean loss: 34.04
 ---- batch: 030 ----
mean loss: 37.70
 ---- batch: 040 ----
mean loss: 36.85
train mean loss: 36.15
epoch train time: 0:00:00.199133
elapsed time: 0:01:01.627905
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:48:09.948185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.20
 ---- batch: 020 ----
mean loss: 36.68
 ---- batch: 030 ----
mean loss: 36.40
 ---- batch: 040 ----
mean loss: 37.37
train mean loss: 36.12
epoch train time: 0:00:00.200451
elapsed time: 0:01:01.828485
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:48:10.148768
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.41
 ---- batch: 020 ----
mean loss: 36.82
 ---- batch: 030 ----
mean loss: 36.21
 ---- batch: 040 ----
mean loss: 35.01
train mean loss: 36.19
epoch train time: 0:00:00.200188
elapsed time: 0:01:02.028795
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:48:10.349092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.07
 ---- batch: 020 ----
mean loss: 33.95
 ---- batch: 030 ----
mean loss: 36.22
 ---- batch: 040 ----
mean loss: 37.06
train mean loss: 36.25
epoch train time: 0:00:00.201635
elapsed time: 0:01:02.230565
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:48:10.550847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.12
 ---- batch: 020 ----
mean loss: 36.57
 ---- batch: 030 ----
mean loss: 36.98
 ---- batch: 040 ----
mean loss: 34.45
train mean loss: 36.12
epoch train time: 0:00:00.199798
elapsed time: 0:01:02.430478
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:48:10.750761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.95
 ---- batch: 020 ----
mean loss: 38.07
 ---- batch: 030 ----
mean loss: 34.83
 ---- batch: 040 ----
mean loss: 34.94
train mean loss: 36.31
epoch train time: 0:00:00.205983
elapsed time: 0:01:02.636592
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:48:10.956866
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.03
 ---- batch: 020 ----
mean loss: 36.01
 ---- batch: 030 ----
mean loss: 36.38
 ---- batch: 040 ----
mean loss: 36.36
train mean loss: 36.15
epoch train time: 0:00:00.200084
elapsed time: 0:01:02.836786
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:48:11.157067
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.63
 ---- batch: 020 ----
mean loss: 35.36
 ---- batch: 030 ----
mean loss: 34.22
 ---- batch: 040 ----
mean loss: 37.10
train mean loss: 36.11
epoch train time: 0:00:00.202366
elapsed time: 0:01:03.039271
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:48:11.359554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.94
 ---- batch: 020 ----
mean loss: 35.96
 ---- batch: 030 ----
mean loss: 36.56
 ---- batch: 040 ----
mean loss: 35.79
train mean loss: 35.97
epoch train time: 0:00:00.204278
elapsed time: 0:01:03.243669
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:48:11.563951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.49
 ---- batch: 020 ----
mean loss: 33.65
 ---- batch: 030 ----
mean loss: 36.53
 ---- batch: 040 ----
mean loss: 35.46
train mean loss: 36.01
epoch train time: 0:00:00.219175
elapsed time: 0:01:03.462994
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:48:11.783288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.27
 ---- batch: 020 ----
mean loss: 36.93
 ---- batch: 030 ----
mean loss: 35.54
 ---- batch: 040 ----
mean loss: 36.53
train mean loss: 36.09
epoch train time: 0:00:00.201980
elapsed time: 0:01:03.665121
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:48:11.985403
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.51
 ---- batch: 020 ----
mean loss: 36.48
 ---- batch: 030 ----
mean loss: 37.15
 ---- batch: 040 ----
mean loss: 36.44
train mean loss: 36.00
epoch train time: 0:00:00.204052
elapsed time: 0:01:03.869294
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:48:12.189575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.03
 ---- batch: 020 ----
mean loss: 37.13
 ---- batch: 030 ----
mean loss: 33.87
 ---- batch: 040 ----
mean loss: 36.40
train mean loss: 36.00
epoch train time: 0:00:00.202227
elapsed time: 0:01:04.071667
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:48:12.391949
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.01
 ---- batch: 020 ----
mean loss: 35.98
 ---- batch: 030 ----
mean loss: 36.16
 ---- batch: 040 ----
mean loss: 36.68
train mean loss: 36.00
epoch train time: 0:00:00.201416
elapsed time: 0:01:04.273201
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:48:12.593480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.53
 ---- batch: 020 ----
mean loss: 35.74
 ---- batch: 030 ----
mean loss: 35.85
 ---- batch: 040 ----
mean loss: 35.65
train mean loss: 35.88
epoch train time: 0:00:00.204865
elapsed time: 0:01:04.478187
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:48:12.798487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.24
 ---- batch: 020 ----
mean loss: 34.47
 ---- batch: 030 ----
mean loss: 36.63
 ---- batch: 040 ----
mean loss: 36.71
train mean loss: 35.95
epoch train time: 0:00:00.202749
elapsed time: 0:01:04.681083
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:48:13.001365
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.91
 ---- batch: 020 ----
mean loss: 35.89
 ---- batch: 030 ----
mean loss: 36.73
 ---- batch: 040 ----
mean loss: 35.41
train mean loss: 35.88
epoch train time: 0:00:00.199057
elapsed time: 0:01:04.880259
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:48:13.200543
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.18
 ---- batch: 020 ----
mean loss: 37.35
 ---- batch: 030 ----
mean loss: 36.42
 ---- batch: 040 ----
mean loss: 36.04
train mean loss: 35.93
epoch train time: 0:00:00.194910
elapsed time: 0:01:05.075309
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:48:13.395590
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.18
 ---- batch: 020 ----
mean loss: 34.28
 ---- batch: 030 ----
mean loss: 37.55
 ---- batch: 040 ----
mean loss: 37.85
train mean loss: 35.92
epoch train time: 0:00:00.193867
elapsed time: 0:01:05.269287
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:48:13.589567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.70
 ---- batch: 020 ----
mean loss: 36.86
 ---- batch: 030 ----
mean loss: 34.93
 ---- batch: 040 ----
mean loss: 36.04
train mean loss: 35.96
epoch train time: 0:00:00.208443
elapsed time: 0:01:05.477847
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:48:13.798144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.11
 ---- batch: 020 ----
mean loss: 35.25
 ---- batch: 030 ----
mean loss: 36.90
 ---- batch: 040 ----
mean loss: 34.74
train mean loss: 35.96
epoch train time: 0:00:00.199732
elapsed time: 0:01:05.677710
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:48:13.998008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.72
 ---- batch: 020 ----
mean loss: 36.49
 ---- batch: 030 ----
mean loss: 35.41
 ---- batch: 040 ----
mean loss: 35.24
train mean loss: 35.87
epoch train time: 0:00:00.201616
elapsed time: 0:01:05.882630
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_7/checkpoint.pth.tar
**** end time: 2019-09-20 23:48:14.202886 ****
