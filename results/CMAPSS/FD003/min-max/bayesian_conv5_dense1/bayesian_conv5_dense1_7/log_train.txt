Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 30816
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-20 14:35:54.802990 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 14:35:54.819389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2162.08
 ---- batch: 020 ----
mean loss: 1348.92
 ---- batch: 030 ----
mean loss: 1186.02
 ---- batch: 040 ----
mean loss: 1108.31
train mean loss: 1425.75
epoch train time: 0:00:20.558888
elapsed time: 0:00:20.582858
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 14:36:15.385888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1033.28
 ---- batch: 020 ----
mean loss: 1022.44
 ---- batch: 030 ----
mean loss: 1059.94
 ---- batch: 040 ----
mean loss: 985.29
train mean loss: 1024.58
epoch train time: 0:00:08.190121
elapsed time: 0:00:28.773309
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 14:36:23.576513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1003.12
 ---- batch: 020 ----
mean loss: 993.64
 ---- batch: 030 ----
mean loss: 966.86
 ---- batch: 040 ----
mean loss: 999.18
train mean loss: 988.25
epoch train time: 0:00:08.271603
elapsed time: 0:00:37.045525
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 14:36:31.848693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.43
 ---- batch: 020 ----
mean loss: 938.99
 ---- batch: 030 ----
mean loss: 962.55
 ---- batch: 040 ----
mean loss: 928.96
train mean loss: 949.68
epoch train time: 0:00:08.167853
elapsed time: 0:00:45.213851
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 14:36:40.017061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.63
 ---- batch: 020 ----
mean loss: 833.20
 ---- batch: 030 ----
mean loss: 786.67
 ---- batch: 040 ----
mean loss: 725.45
train mean loss: 798.98
epoch train time: 0:00:08.190807
elapsed time: 0:00:53.405135
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 14:36:48.208266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 660.83
 ---- batch: 020 ----
mean loss: 580.69
 ---- batch: 030 ----
mean loss: 515.83
 ---- batch: 040 ----
mean loss: 450.58
train mean loss: 541.87
epoch train time: 0:00:08.237281
elapsed time: 0:01:01.642873
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 14:36:56.446047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.90
 ---- batch: 020 ----
mean loss: 409.47
 ---- batch: 030 ----
mean loss: 381.81
 ---- batch: 040 ----
mean loss: 385.10
train mean loss: 397.19
epoch train time: 0:00:08.245517
elapsed time: 0:01:09.888851
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 14:37:04.691989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.73
 ---- batch: 020 ----
mean loss: 376.88
 ---- batch: 030 ----
mean loss: 342.66
 ---- batch: 040 ----
mean loss: 336.19
train mean loss: 357.53
epoch train time: 0:00:08.205803
elapsed time: 0:01:18.095077
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 14:37:12.898240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.87
 ---- batch: 020 ----
mean loss: 330.27
 ---- batch: 030 ----
mean loss: 331.34
 ---- batch: 040 ----
mean loss: 333.35
train mean loss: 332.53
epoch train time: 0:00:08.229617
elapsed time: 0:01:26.325155
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 14:37:21.128301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.14
 ---- batch: 020 ----
mean loss: 314.66
 ---- batch: 030 ----
mean loss: 316.25
 ---- batch: 040 ----
mean loss: 306.80
train mean loss: 313.31
epoch train time: 0:00:08.211015
elapsed time: 0:01:34.536606
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 14:37:29.339762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.91
 ---- batch: 020 ----
mean loss: 309.31
 ---- batch: 030 ----
mean loss: 294.58
 ---- batch: 040 ----
mean loss: 287.96
train mean loss: 299.51
epoch train time: 0:00:08.012202
elapsed time: 0:01:42.549318
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 14:37:37.352541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.98
 ---- batch: 020 ----
mean loss: 299.23
 ---- batch: 030 ----
mean loss: 283.68
 ---- batch: 040 ----
mean loss: 288.99
train mean loss: 291.94
epoch train time: 0:00:08.009464
elapsed time: 0:01:50.559297
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 14:37:45.362427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.62
 ---- batch: 020 ----
mean loss: 278.22
 ---- batch: 030 ----
mean loss: 275.88
 ---- batch: 040 ----
mean loss: 273.92
train mean loss: 279.83
epoch train time: 0:00:08.077407
elapsed time: 0:01:58.637175
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 14:37:53.440318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.55
 ---- batch: 020 ----
mean loss: 276.72
 ---- batch: 030 ----
mean loss: 274.92
 ---- batch: 040 ----
mean loss: 279.20
train mean loss: 279.34
epoch train time: 0:00:08.173619
elapsed time: 0:02:06.811267
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 14:38:01.614452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.64
 ---- batch: 020 ----
mean loss: 267.21
 ---- batch: 030 ----
mean loss: 262.50
 ---- batch: 040 ----
mean loss: 262.41
train mean loss: 268.76
epoch train time: 0:00:08.159634
elapsed time: 0:02:14.971401
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 14:38:09.774559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.99
 ---- batch: 020 ----
mean loss: 263.27
 ---- batch: 030 ----
mean loss: 258.65
 ---- batch: 040 ----
mean loss: 264.22
train mean loss: 262.78
epoch train time: 0:00:08.172074
elapsed time: 0:02:23.143965
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 14:38:17.947110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.08
 ---- batch: 020 ----
mean loss: 257.37
 ---- batch: 030 ----
mean loss: 257.34
 ---- batch: 040 ----
mean loss: 255.10
train mean loss: 256.84
epoch train time: 0:00:08.015575
elapsed time: 0:02:31.160027
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 14:38:25.963167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.68
 ---- batch: 020 ----
mean loss: 256.71
 ---- batch: 030 ----
mean loss: 256.66
 ---- batch: 040 ----
mean loss: 243.96
train mean loss: 251.12
epoch train time: 0:00:07.980306
elapsed time: 0:02:39.140759
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 14:38:33.943923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.20
 ---- batch: 020 ----
mean loss: 253.85
 ---- batch: 030 ----
mean loss: 242.43
 ---- batch: 040 ----
mean loss: 238.12
train mean loss: 244.77
epoch train time: 0:00:07.984288
elapsed time: 0:02:47.125547
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 14:38:41.928696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.58
 ---- batch: 020 ----
mean loss: 244.76
 ---- batch: 030 ----
mean loss: 237.59
 ---- batch: 040 ----
mean loss: 246.81
train mean loss: 242.00
epoch train time: 0:00:07.976675
elapsed time: 0:02:55.102770
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 14:38:49.905995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.08
 ---- batch: 020 ----
mean loss: 242.96
 ---- batch: 030 ----
mean loss: 243.23
 ---- batch: 040 ----
mean loss: 234.87
train mean loss: 238.56
epoch train time: 0:00:08.050034
elapsed time: 0:03:03.153309
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 14:38:57.956473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.28
 ---- batch: 020 ----
mean loss: 233.91
 ---- batch: 030 ----
mean loss: 227.42
 ---- batch: 040 ----
mean loss: 231.34
train mean loss: 232.14
epoch train time: 0:00:08.078248
elapsed time: 0:03:11.232019
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 14:39:06.035131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.82
 ---- batch: 020 ----
mean loss: 229.28
 ---- batch: 030 ----
mean loss: 227.51
 ---- batch: 040 ----
mean loss: 223.42
train mean loss: 228.31
epoch train time: 0:00:07.985014
elapsed time: 0:03:19.217484
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 14:39:14.020630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.53
 ---- batch: 020 ----
mean loss: 228.24
 ---- batch: 030 ----
mean loss: 222.37
 ---- batch: 040 ----
mean loss: 227.64
train mean loss: 224.73
epoch train time: 0:00:07.929314
elapsed time: 0:03:27.147239
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 14:39:21.950403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.12
 ---- batch: 020 ----
mean loss: 222.04
 ---- batch: 030 ----
mean loss: 221.87
 ---- batch: 040 ----
mean loss: 224.20
train mean loss: 221.85
epoch train time: 0:00:07.884371
elapsed time: 0:03:35.032095
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 14:39:29.835235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.94
 ---- batch: 020 ----
mean loss: 220.86
 ---- batch: 030 ----
mean loss: 212.81
 ---- batch: 040 ----
mean loss: 215.47
train mean loss: 217.90
epoch train time: 0:00:07.883869
elapsed time: 0:03:42.916455
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 14:39:37.719639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.28
 ---- batch: 020 ----
mean loss: 213.92
 ---- batch: 030 ----
mean loss: 221.28
 ---- batch: 040 ----
mean loss: 209.93
train mean loss: 214.76
epoch train time: 0:00:07.872664
elapsed time: 0:03:50.789612
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 14:39:45.592798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.52
 ---- batch: 020 ----
mean loss: 216.91
 ---- batch: 030 ----
mean loss: 215.21
 ---- batch: 040 ----
mean loss: 203.58
train mean loss: 212.26
epoch train time: 0:00:07.754595
elapsed time: 0:03:58.544710
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 14:39:53.347861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.78
 ---- batch: 020 ----
mean loss: 204.49
 ---- batch: 030 ----
mean loss: 212.51
 ---- batch: 040 ----
mean loss: 210.13
train mean loss: 207.15
epoch train time: 0:00:07.797145
elapsed time: 0:04:06.342319
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 14:40:01.145497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.89
 ---- batch: 020 ----
mean loss: 208.77
 ---- batch: 030 ----
mean loss: 194.43
 ---- batch: 040 ----
mean loss: 196.18
train mean loss: 200.28
epoch train time: 0:00:07.837528
elapsed time: 0:04:14.180350
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 14:40:08.983517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.60
 ---- batch: 020 ----
mean loss: 201.66
 ---- batch: 030 ----
mean loss: 198.94
 ---- batch: 040 ----
mean loss: 206.73
train mean loss: 202.66
epoch train time: 0:00:07.797497
elapsed time: 0:04:21.978334
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 14:40:16.781477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.27
 ---- batch: 020 ----
mean loss: 197.78
 ---- batch: 030 ----
mean loss: 199.01
 ---- batch: 040 ----
mean loss: 193.75
train mean loss: 199.64
epoch train time: 0:00:07.872927
elapsed time: 0:04:29.851796
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 14:40:24.654995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.73
 ---- batch: 020 ----
mean loss: 193.61
 ---- batch: 030 ----
mean loss: 193.02
 ---- batch: 040 ----
mean loss: 195.80
train mean loss: 197.34
epoch train time: 0:00:07.892661
elapsed time: 0:04:37.744991
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 14:40:32.548140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.82
 ---- batch: 020 ----
mean loss: 194.37
 ---- batch: 030 ----
mean loss: 186.42
 ---- batch: 040 ----
mean loss: 192.03
train mean loss: 193.86
epoch train time: 0:00:07.900558
elapsed time: 0:04:45.646022
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 14:40:40.449167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.20
 ---- batch: 020 ----
mean loss: 198.55
 ---- batch: 030 ----
mean loss: 190.34
 ---- batch: 040 ----
mean loss: 188.17
train mean loss: 190.93
epoch train time: 0:00:07.879185
elapsed time: 0:04:53.525651
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 14:40:48.328792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.67
 ---- batch: 020 ----
mean loss: 195.21
 ---- batch: 030 ----
mean loss: 192.27
 ---- batch: 040 ----
mean loss: 190.69
train mean loss: 191.91
epoch train time: 0:00:07.885958
elapsed time: 0:05:01.412112
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 14:40:56.215261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.48
 ---- batch: 020 ----
mean loss: 195.69
 ---- batch: 030 ----
mean loss: 190.80
 ---- batch: 040 ----
mean loss: 188.08
train mean loss: 188.27
epoch train time: 0:00:07.910539
elapsed time: 0:05:09.323115
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 14:41:04.126267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.26
 ---- batch: 020 ----
mean loss: 181.04
 ---- batch: 030 ----
mean loss: 184.39
 ---- batch: 040 ----
mean loss: 186.79
train mean loss: 186.31
epoch train time: 0:00:07.894442
elapsed time: 0:05:17.218006
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 14:41:12.021165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.78
 ---- batch: 020 ----
mean loss: 184.11
 ---- batch: 030 ----
mean loss: 185.94
 ---- batch: 040 ----
mean loss: 184.56
train mean loss: 184.50
epoch train time: 0:00:08.019491
elapsed time: 0:05:25.237941
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 14:41:20.041106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.30
 ---- batch: 020 ----
mean loss: 186.69
 ---- batch: 030 ----
mean loss: 184.43
 ---- batch: 040 ----
mean loss: 181.14
train mean loss: 184.25
epoch train time: 0:00:08.105074
elapsed time: 0:05:33.343476
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 14:41:28.146624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.53
 ---- batch: 020 ----
mean loss: 178.23
 ---- batch: 030 ----
mean loss: 182.53
 ---- batch: 040 ----
mean loss: 177.38
train mean loss: 179.80
epoch train time: 0:00:07.924873
elapsed time: 0:05:41.268884
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 14:41:36.072038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.21
 ---- batch: 020 ----
mean loss: 173.43
 ---- batch: 030 ----
mean loss: 190.93
 ---- batch: 040 ----
mean loss: 174.64
train mean loss: 177.80
epoch train time: 0:00:07.934183
elapsed time: 0:05:49.203518
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 14:41:44.006662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.92
 ---- batch: 020 ----
mean loss: 173.41
 ---- batch: 030 ----
mean loss: 178.55
 ---- batch: 040 ----
mean loss: 173.56
train mean loss: 176.08
epoch train time: 0:00:07.923185
elapsed time: 0:05:57.127134
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 14:41:51.930272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.73
 ---- batch: 020 ----
mean loss: 170.35
 ---- batch: 030 ----
mean loss: 165.68
 ---- batch: 040 ----
mean loss: 173.57
train mean loss: 172.87
epoch train time: 0:00:07.961755
elapsed time: 0:06:05.089291
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 14:41:59.892379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.76
 ---- batch: 020 ----
mean loss: 172.63
 ---- batch: 030 ----
mean loss: 171.19
 ---- batch: 040 ----
mean loss: 172.38
train mean loss: 172.10
epoch train time: 0:00:07.892379
elapsed time: 0:06:12.982115
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 14:42:07.785283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.95
 ---- batch: 020 ----
mean loss: 172.44
 ---- batch: 030 ----
mean loss: 169.46
 ---- batch: 040 ----
mean loss: 172.29
train mean loss: 170.63
epoch train time: 0:00:07.977480
elapsed time: 0:06:20.960144
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 14:42:15.763343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.96
 ---- batch: 020 ----
mean loss: 166.12
 ---- batch: 030 ----
mean loss: 169.48
 ---- batch: 040 ----
mean loss: 164.15
train mean loss: 167.26
epoch train time: 0:00:07.992625
elapsed time: 0:06:28.953289
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 14:42:23.756453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.63
 ---- batch: 020 ----
mean loss: 163.43
 ---- batch: 030 ----
mean loss: 171.86
 ---- batch: 040 ----
mean loss: 164.00
train mean loss: 166.54
epoch train time: 0:00:07.946002
elapsed time: 0:06:36.899778
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 14:42:31.702921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.22
 ---- batch: 020 ----
mean loss: 165.65
 ---- batch: 030 ----
mean loss: 158.95
 ---- batch: 040 ----
mean loss: 170.71
train mean loss: 164.32
epoch train time: 0:00:07.932005
elapsed time: 0:06:44.832210
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 14:42:39.635341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.51
 ---- batch: 020 ----
mean loss: 161.39
 ---- batch: 030 ----
mean loss: 159.29
 ---- batch: 040 ----
mean loss: 162.69
train mean loss: 162.23
epoch train time: 0:00:07.974807
elapsed time: 0:06:52.807491
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 14:42:47.610629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.81
 ---- batch: 020 ----
mean loss: 159.72
 ---- batch: 030 ----
mean loss: 158.21
 ---- batch: 040 ----
mean loss: 164.88
train mean loss: 160.58
epoch train time: 0:00:07.949637
elapsed time: 0:07:00.757586
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 14:42:55.560761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.91
 ---- batch: 020 ----
mean loss: 164.41
 ---- batch: 030 ----
mean loss: 160.38
 ---- batch: 040 ----
mean loss: 155.15
train mean loss: 159.63
epoch train time: 0:00:07.922554
elapsed time: 0:07:08.680601
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 14:43:03.483786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.34
 ---- batch: 020 ----
mean loss: 152.05
 ---- batch: 030 ----
mean loss: 155.31
 ---- batch: 040 ----
mean loss: 152.98
train mean loss: 156.82
epoch train time: 0:00:07.944180
elapsed time: 0:07:16.625248
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 14:43:11.428406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.13
 ---- batch: 020 ----
mean loss: 153.69
 ---- batch: 030 ----
mean loss: 155.55
 ---- batch: 040 ----
mean loss: 160.18
train mean loss: 154.96
epoch train time: 0:00:08.035618
elapsed time: 0:07:24.661310
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 14:43:19.464466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.07
 ---- batch: 020 ----
mean loss: 159.62
 ---- batch: 030 ----
mean loss: 157.59
 ---- batch: 040 ----
mean loss: 148.83
train mean loss: 155.89
epoch train time: 0:00:08.041643
elapsed time: 0:07:32.703394
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 14:43:27.506532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.48
 ---- batch: 020 ----
mean loss: 157.06
 ---- batch: 030 ----
mean loss: 154.46
 ---- batch: 040 ----
mean loss: 147.32
train mean loss: 154.68
epoch train time: 0:00:08.045476
elapsed time: 0:07:40.749292
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 14:43:35.552452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.82
 ---- batch: 020 ----
mean loss: 152.81
 ---- batch: 030 ----
mean loss: 149.78
 ---- batch: 040 ----
mean loss: 154.98
train mean loss: 152.88
epoch train time: 0:00:08.121892
elapsed time: 0:07:48.871648
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 14:43:43.674838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.91
 ---- batch: 020 ----
mean loss: 145.54
 ---- batch: 030 ----
mean loss: 148.05
 ---- batch: 040 ----
mean loss: 150.34
train mean loss: 149.29
epoch train time: 0:00:08.076114
elapsed time: 0:07:56.948288
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 14:43:51.751425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.92
 ---- batch: 020 ----
mean loss: 148.90
 ---- batch: 030 ----
mean loss: 148.71
 ---- batch: 040 ----
mean loss: 152.06
train mean loss: 148.64
epoch train time: 0:00:08.088833
elapsed time: 0:08:05.037581
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 14:43:59.840726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.07
 ---- batch: 020 ----
mean loss: 153.67
 ---- batch: 030 ----
mean loss: 143.01
 ---- batch: 040 ----
mean loss: 144.68
train mean loss: 146.59
epoch train time: 0:00:07.948367
elapsed time: 0:08:12.986410
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 14:44:07.789557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.56
 ---- batch: 020 ----
mean loss: 141.50
 ---- batch: 030 ----
mean loss: 147.63
 ---- batch: 040 ----
mean loss: 150.31
train mean loss: 145.86
epoch train time: 0:00:07.940077
elapsed time: 0:08:20.927019
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 14:44:15.730168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.30
 ---- batch: 020 ----
mean loss: 148.09
 ---- batch: 030 ----
mean loss: 139.12
 ---- batch: 040 ----
mean loss: 143.82
train mean loss: 143.77
epoch train time: 0:00:07.904476
elapsed time: 0:08:28.831903
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 14:44:23.635063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.96
 ---- batch: 020 ----
mean loss: 143.52
 ---- batch: 030 ----
mean loss: 143.79
 ---- batch: 040 ----
mean loss: 145.38
train mean loss: 143.58
epoch train time: 0:00:07.733200
elapsed time: 0:08:36.565586
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 14:44:31.368779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.28
 ---- batch: 020 ----
mean loss: 141.94
 ---- batch: 030 ----
mean loss: 141.68
 ---- batch: 040 ----
mean loss: 139.46
train mean loss: 140.01
epoch train time: 0:00:07.785083
elapsed time: 0:08:44.351158
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 14:44:39.154263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.52
 ---- batch: 020 ----
mean loss: 139.04
 ---- batch: 030 ----
mean loss: 143.17
 ---- batch: 040 ----
mean loss: 134.72
train mean loss: 140.94
epoch train time: 0:00:07.868007
elapsed time: 0:08:52.219561
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 14:44:47.022662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.50
 ---- batch: 020 ----
mean loss: 138.70
 ---- batch: 030 ----
mean loss: 133.98
 ---- batch: 040 ----
mean loss: 142.79
train mean loss: 137.98
epoch train time: 0:00:07.755131
elapsed time: 0:08:59.975094
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 14:44:54.778233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.74
 ---- batch: 020 ----
mean loss: 132.24
 ---- batch: 030 ----
mean loss: 138.74
 ---- batch: 040 ----
mean loss: 145.36
train mean loss: 138.40
epoch train time: 0:00:07.659227
elapsed time: 0:09:07.634830
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 14:45:02.437987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.90
 ---- batch: 020 ----
mean loss: 135.69
 ---- batch: 030 ----
mean loss: 140.17
 ---- batch: 040 ----
mean loss: 137.04
train mean loss: 136.58
epoch train time: 0:00:07.881879
elapsed time: 0:09:15.517129
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 14:45:10.320267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.39
 ---- batch: 020 ----
mean loss: 134.77
 ---- batch: 030 ----
mean loss: 133.61
 ---- batch: 040 ----
mean loss: 135.16
train mean loss: 135.44
epoch train time: 0:00:07.842137
elapsed time: 0:09:23.359666
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 14:45:18.162798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.62
 ---- batch: 020 ----
mean loss: 131.96
 ---- batch: 030 ----
mean loss: 132.34
 ---- batch: 040 ----
mean loss: 135.65
train mean loss: 133.40
epoch train time: 0:00:07.797081
elapsed time: 0:09:31.157178
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 14:45:25.960329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.87
 ---- batch: 020 ----
mean loss: 129.64
 ---- batch: 030 ----
mean loss: 138.90
 ---- batch: 040 ----
mean loss: 131.52
train mean loss: 132.65
epoch train time: 0:00:07.679070
elapsed time: 0:09:38.836724
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 14:45:33.639871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.39
 ---- batch: 020 ----
mean loss: 135.75
 ---- batch: 030 ----
mean loss: 132.25
 ---- batch: 040 ----
mean loss: 127.56
train mean loss: 131.33
epoch train time: 0:00:07.778580
elapsed time: 0:09:46.615752
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 14:45:41.418860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.21
 ---- batch: 020 ----
mean loss: 136.80
 ---- batch: 030 ----
mean loss: 126.33
 ---- batch: 040 ----
mean loss: 128.37
train mean loss: 129.15
epoch train time: 0:00:07.676510
elapsed time: 0:09:54.292665
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 14:45:49.095820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.33
 ---- batch: 020 ----
mean loss: 129.68
 ---- batch: 030 ----
mean loss: 129.84
 ---- batch: 040 ----
mean loss: 128.60
train mean loss: 130.24
epoch train time: 0:00:07.778700
elapsed time: 0:10:02.071843
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 14:45:56.874997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.33
 ---- batch: 020 ----
mean loss: 126.32
 ---- batch: 030 ----
mean loss: 130.94
 ---- batch: 040 ----
mean loss: 127.29
train mean loss: 128.84
epoch train time: 0:00:08.078429
elapsed time: 0:10:10.150741
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 14:46:04.953939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.05
 ---- batch: 020 ----
mean loss: 130.65
 ---- batch: 030 ----
mean loss: 125.59
 ---- batch: 040 ----
mean loss: 128.36
train mean loss: 127.53
epoch train time: 0:00:08.026608
elapsed time: 0:10:18.177863
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 14:46:12.981035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.31
 ---- batch: 020 ----
mean loss: 124.69
 ---- batch: 030 ----
mean loss: 124.90
 ---- batch: 040 ----
mean loss: 128.17
train mean loss: 125.55
epoch train time: 0:00:08.041973
elapsed time: 0:10:26.220311
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 14:46:21.023413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.56
 ---- batch: 020 ----
mean loss: 125.90
 ---- batch: 030 ----
mean loss: 126.13
 ---- batch: 040 ----
mean loss: 120.99
train mean loss: 124.01
epoch train time: 0:00:08.071343
elapsed time: 0:10:34.292164
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 14:46:29.095373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.46
 ---- batch: 020 ----
mean loss: 129.72
 ---- batch: 030 ----
mean loss: 123.97
 ---- batch: 040 ----
mean loss: 122.04
train mean loss: 126.84
epoch train time: 0:00:08.128204
elapsed time: 0:10:42.420918
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 14:46:37.224008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.24
 ---- batch: 020 ----
mean loss: 121.63
 ---- batch: 030 ----
mean loss: 124.46
 ---- batch: 040 ----
mean loss: 123.11
train mean loss: 123.68
epoch train time: 0:00:08.085738
elapsed time: 0:10:50.507047
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 14:46:45.310202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.62
 ---- batch: 020 ----
mean loss: 122.49
 ---- batch: 030 ----
mean loss: 121.15
 ---- batch: 040 ----
mean loss: 122.77
train mean loss: 123.17
epoch train time: 0:00:08.118257
elapsed time: 0:10:58.625764
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 14:46:53.428891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.98
 ---- batch: 020 ----
mean loss: 125.13
 ---- batch: 030 ----
mean loss: 115.66
 ---- batch: 040 ----
mean loss: 118.11
train mean loss: 119.46
epoch train time: 0:00:08.136046
elapsed time: 0:11:06.762225
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 14:47:01.565366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.71
 ---- batch: 020 ----
mean loss: 120.31
 ---- batch: 030 ----
mean loss: 119.34
 ---- batch: 040 ----
mean loss: 121.91
train mean loss: 120.49
epoch train time: 0:00:08.108997
elapsed time: 0:11:14.871817
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 14:47:09.675048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.12
 ---- batch: 020 ----
mean loss: 123.71
 ---- batch: 030 ----
mean loss: 118.44
 ---- batch: 040 ----
mean loss: 122.59
train mean loss: 119.28
epoch train time: 0:00:07.973012
elapsed time: 0:11:22.845363
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 14:47:17.648536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.62
 ---- batch: 020 ----
mean loss: 119.96
 ---- batch: 030 ----
mean loss: 116.24
 ---- batch: 040 ----
mean loss: 116.55
train mean loss: 118.27
epoch train time: 0:00:08.022267
elapsed time: 0:11:30.868117
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 14:47:25.671244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.42
 ---- batch: 020 ----
mean loss: 118.94
 ---- batch: 030 ----
mean loss: 124.42
 ---- batch: 040 ----
mean loss: 118.32
train mean loss: 118.79
epoch train time: 0:00:07.980016
elapsed time: 0:11:38.848579
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 14:47:33.651736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.89
 ---- batch: 020 ----
mean loss: 117.30
 ---- batch: 030 ----
mean loss: 114.44
 ---- batch: 040 ----
mean loss: 116.24
train mean loss: 116.00
epoch train time: 0:00:07.888437
elapsed time: 0:11:46.737494
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 14:47:41.540674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.07
 ---- batch: 020 ----
mean loss: 112.85
 ---- batch: 030 ----
mean loss: 118.24
 ---- batch: 040 ----
mean loss: 114.41
train mean loss: 115.10
epoch train time: 0:00:07.845224
elapsed time: 0:11:54.583219
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 14:47:49.386348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.25
 ---- batch: 020 ----
mean loss: 116.31
 ---- batch: 030 ----
mean loss: 110.42
 ---- batch: 040 ----
mean loss: 111.93
train mean loss: 113.84
epoch train time: 0:00:08.041728
elapsed time: 0:12:02.625383
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 14:47:57.428550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.15
 ---- batch: 020 ----
mean loss: 111.18
 ---- batch: 030 ----
mean loss: 118.95
 ---- batch: 040 ----
mean loss: 116.71
train mean loss: 115.56
epoch train time: 0:00:08.099101
elapsed time: 0:12:10.724926
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 14:48:05.528105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.53
 ---- batch: 020 ----
mean loss: 117.98
 ---- batch: 030 ----
mean loss: 109.94
 ---- batch: 040 ----
mean loss: 114.59
train mean loss: 113.33
epoch train time: 0:00:08.057570
elapsed time: 0:12:18.783005
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 14:48:13.586155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.87
 ---- batch: 020 ----
mean loss: 116.86
 ---- batch: 030 ----
mean loss: 109.51
 ---- batch: 040 ----
mean loss: 109.32
train mean loss: 113.05
epoch train time: 0:00:08.102545
elapsed time: 0:12:26.885997
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 14:48:21.689150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.09
 ---- batch: 020 ----
mean loss: 109.06
 ---- batch: 030 ----
mean loss: 110.18
 ---- batch: 040 ----
mean loss: 110.74
train mean loss: 110.11
epoch train time: 0:00:08.050937
elapsed time: 0:12:34.937379
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 14:48:29.740555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.32
 ---- batch: 020 ----
mean loss: 112.64
 ---- batch: 030 ----
mean loss: 109.58
 ---- batch: 040 ----
mean loss: 111.25
train mean loss: 111.04
epoch train time: 0:00:08.072390
elapsed time: 0:12:43.010273
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 14:48:37.813429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.71
 ---- batch: 020 ----
mean loss: 113.99
 ---- batch: 030 ----
mean loss: 106.99
 ---- batch: 040 ----
mean loss: 110.68
train mean loss: 109.15
epoch train time: 0:00:08.104345
elapsed time: 0:12:51.115122
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 14:48:45.918275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.22
 ---- batch: 020 ----
mean loss: 112.16
 ---- batch: 030 ----
mean loss: 102.91
 ---- batch: 040 ----
mean loss: 105.66
train mean loss: 108.11
epoch train time: 0:00:08.062987
elapsed time: 0:12:59.178566
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 14:48:53.981702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.41
 ---- batch: 020 ----
mean loss: 112.28
 ---- batch: 030 ----
mean loss: 104.51
 ---- batch: 040 ----
mean loss: 109.16
train mean loss: 108.96
epoch train time: 0:00:08.044458
elapsed time: 0:13:07.223454
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 14:49:02.026593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.22
 ---- batch: 020 ----
mean loss: 102.63
 ---- batch: 030 ----
mean loss: 104.96
 ---- batch: 040 ----
mean loss: 104.53
train mean loss: 105.44
epoch train time: 0:00:08.089500
elapsed time: 0:13:15.313423
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 14:49:10.116588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.96
 ---- batch: 020 ----
mean loss: 106.82
 ---- batch: 030 ----
mean loss: 104.26
 ---- batch: 040 ----
mean loss: 107.17
train mean loss: 106.37
epoch train time: 0:00:08.060754
elapsed time: 0:13:23.374641
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 14:49:18.177784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.60
 ---- batch: 020 ----
mean loss: 107.73
 ---- batch: 030 ----
mean loss: 106.66
 ---- batch: 040 ----
mean loss: 103.59
train mean loss: 105.32
epoch train time: 0:00:08.029196
elapsed time: 0:13:31.404269
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 14:49:26.207448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.67
 ---- batch: 020 ----
mean loss: 106.82
 ---- batch: 030 ----
mean loss: 99.96
 ---- batch: 040 ----
mean loss: 104.21
train mean loss: 102.63
epoch train time: 0:00:08.051247
elapsed time: 0:13:39.455971
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 14:49:34.259110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.09
 ---- batch: 020 ----
mean loss: 104.04
 ---- batch: 030 ----
mean loss: 104.24
 ---- batch: 040 ----
mean loss: 106.26
train mean loss: 105.03
epoch train time: 0:00:08.113572
elapsed time: 0:13:47.569987
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 14:49:42.373115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.28
 ---- batch: 020 ----
mean loss: 98.86
 ---- batch: 030 ----
mean loss: 103.31
 ---- batch: 040 ----
mean loss: 105.83
train mean loss: 102.17
epoch train time: 0:00:07.866922
elapsed time: 0:13:55.437315
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 14:49:50.240485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.30
 ---- batch: 020 ----
mean loss: 102.74
 ---- batch: 030 ----
mean loss: 103.33
 ---- batch: 040 ----
mean loss: 97.90
train mean loss: 102.03
epoch train time: 0:00:07.836108
elapsed time: 0:14:03.273892
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 14:49:58.077026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.94
 ---- batch: 020 ----
mean loss: 101.98
 ---- batch: 030 ----
mean loss: 100.48
 ---- batch: 040 ----
mean loss: 105.22
train mean loss: 101.93
epoch train time: 0:00:07.953659
elapsed time: 0:14:11.228024
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 14:50:06.031174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.54
 ---- batch: 020 ----
mean loss: 100.43
 ---- batch: 030 ----
mean loss: 100.84
 ---- batch: 040 ----
mean loss: 101.01
train mean loss: 100.80
epoch train time: 0:00:08.034569
elapsed time: 0:14:19.263064
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 14:50:14.066204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.35
 ---- batch: 020 ----
mean loss: 102.03
 ---- batch: 030 ----
mean loss: 106.14
 ---- batch: 040 ----
mean loss: 97.44
train mean loss: 101.53
epoch train time: 0:00:08.066447
elapsed time: 0:14:27.330028
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 14:50:22.133104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.66
 ---- batch: 020 ----
mean loss: 98.38
 ---- batch: 030 ----
mean loss: 96.46
 ---- batch: 040 ----
mean loss: 97.34
train mean loss: 97.86
epoch train time: 0:00:08.052753
elapsed time: 0:14:35.383130
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 14:50:30.186268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.04
 ---- batch: 020 ----
mean loss: 100.67
 ---- batch: 030 ----
mean loss: 94.41
 ---- batch: 040 ----
mean loss: 101.51
train mean loss: 99.23
epoch train time: 0:00:08.119860
elapsed time: 0:14:43.503551
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 14:50:38.306715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.15
 ---- batch: 020 ----
mean loss: 98.30
 ---- batch: 030 ----
mean loss: 100.41
 ---- batch: 040 ----
mean loss: 93.18
train mean loss: 98.39
epoch train time: 0:00:08.015220
elapsed time: 0:14:51.519229
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 14:50:46.322383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.31
 ---- batch: 020 ----
mean loss: 95.47
 ---- batch: 030 ----
mean loss: 100.07
 ---- batch: 040 ----
mean loss: 93.97
train mean loss: 96.95
epoch train time: 0:00:07.974911
elapsed time: 0:14:59.494579
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 14:50:54.297724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.59
 ---- batch: 020 ----
mean loss: 100.28
 ---- batch: 030 ----
mean loss: 98.00
 ---- batch: 040 ----
mean loss: 94.54
train mean loss: 98.29
epoch train time: 0:00:07.987916
elapsed time: 0:15:07.482932
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 14:51:02.286085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.57
 ---- batch: 020 ----
mean loss: 97.03
 ---- batch: 030 ----
mean loss: 96.89
 ---- batch: 040 ----
mean loss: 94.19
train mean loss: 96.03
epoch train time: 0:00:07.844138
elapsed time: 0:15:15.327500
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 14:51:10.130647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.69
 ---- batch: 020 ----
mean loss: 93.62
 ---- batch: 030 ----
mean loss: 96.72
 ---- batch: 040 ----
mean loss: 95.65
train mean loss: 95.66
epoch train time: 0:00:07.976726
elapsed time: 0:15:23.304648
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 14:51:18.107786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.69
 ---- batch: 020 ----
mean loss: 95.56
 ---- batch: 030 ----
mean loss: 97.12
 ---- batch: 040 ----
mean loss: 93.20
train mean loss: 95.48
epoch train time: 0:00:08.039408
elapsed time: 0:15:31.344546
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 14:51:26.147687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.95
 ---- batch: 020 ----
mean loss: 94.35
 ---- batch: 030 ----
mean loss: 97.93
 ---- batch: 040 ----
mean loss: 90.94
train mean loss: 94.84
epoch train time: 0:00:07.999791
elapsed time: 0:15:39.344752
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 14:51:34.147918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.82
 ---- batch: 020 ----
mean loss: 93.87
 ---- batch: 030 ----
mean loss: 96.54
 ---- batch: 040 ----
mean loss: 99.23
train mean loss: 96.66
epoch train time: 0:00:07.859831
elapsed time: 0:15:47.205058
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 14:51:42.008219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.11
 ---- batch: 020 ----
mean loss: 99.24
 ---- batch: 030 ----
mean loss: 92.23
 ---- batch: 040 ----
mean loss: 91.60
train mean loss: 94.93
epoch train time: 0:00:07.807723
elapsed time: 0:15:55.013356
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 14:51:49.816601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.48
 ---- batch: 020 ----
mean loss: 95.02
 ---- batch: 030 ----
mean loss: 94.53
 ---- batch: 040 ----
mean loss: 92.45
train mean loss: 94.20
epoch train time: 0:00:07.931060
elapsed time: 0:16:02.944949
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 14:51:57.748067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.48
 ---- batch: 020 ----
mean loss: 93.57
 ---- batch: 030 ----
mean loss: 94.39
 ---- batch: 040 ----
mean loss: 89.61
train mean loss: 92.42
epoch train time: 0:00:07.807824
elapsed time: 0:16:10.753185
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 14:52:05.556276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.37
 ---- batch: 020 ----
mean loss: 93.74
 ---- batch: 030 ----
mean loss: 100.18
 ---- batch: 040 ----
mean loss: 96.98
train mean loss: 95.46
epoch train time: 0:00:07.780436
elapsed time: 0:16:18.534021
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 14:52:13.337168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.52
 ---- batch: 020 ----
mean loss: 94.29
 ---- batch: 030 ----
mean loss: 90.66
 ---- batch: 040 ----
mean loss: 94.47
train mean loss: 92.78
epoch train time: 0:00:07.755644
elapsed time: 0:16:26.290098
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 14:52:21.093244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.60
 ---- batch: 020 ----
mean loss: 92.42
 ---- batch: 030 ----
mean loss: 93.78
 ---- batch: 040 ----
mean loss: 88.88
train mean loss: 91.46
epoch train time: 0:00:07.680578
elapsed time: 0:16:33.971128
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 14:52:28.774290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.65
 ---- batch: 020 ----
mean loss: 90.72
 ---- batch: 030 ----
mean loss: 93.77
 ---- batch: 040 ----
mean loss: 88.40
train mean loss: 91.73
epoch train time: 0:00:07.684448
elapsed time: 0:16:41.656032
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 14:52:36.459160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.94
 ---- batch: 020 ----
mean loss: 91.36
 ---- batch: 030 ----
mean loss: 89.78
 ---- batch: 040 ----
mean loss: 89.49
train mean loss: 91.08
epoch train time: 0:00:07.799088
elapsed time: 0:16:49.455530
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 14:52:44.258665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.52
 ---- batch: 020 ----
mean loss: 90.97
 ---- batch: 030 ----
mean loss: 94.29
 ---- batch: 040 ----
mean loss: 91.89
train mean loss: 90.80
epoch train time: 0:00:08.100521
elapsed time: 0:16:57.556473
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 14:52:52.359617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.93
 ---- batch: 020 ----
mean loss: 91.04
 ---- batch: 030 ----
mean loss: 90.06
 ---- batch: 040 ----
mean loss: 87.02
train mean loss: 89.04
epoch train time: 0:00:08.068265
elapsed time: 0:17:05.625249
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 14:53:00.428291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.25
 ---- batch: 020 ----
mean loss: 89.54
 ---- batch: 030 ----
mean loss: 90.51
 ---- batch: 040 ----
mean loss: 92.53
train mean loss: 91.20
epoch train time: 0:00:07.917675
elapsed time: 0:17:13.543257
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 14:53:08.346416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.32
 ---- batch: 020 ----
mean loss: 90.01
 ---- batch: 030 ----
mean loss: 92.44
 ---- batch: 040 ----
mean loss: 91.19
train mean loss: 90.07
epoch train time: 0:00:07.969643
elapsed time: 0:17:21.513326
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 14:53:16.316510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.27
 ---- batch: 020 ----
mean loss: 90.40
 ---- batch: 030 ----
mean loss: 88.71
 ---- batch: 040 ----
mean loss: 89.80
train mean loss: 90.66
epoch train time: 0:00:07.890114
elapsed time: 0:17:29.403946
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 14:53:24.207082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.40
 ---- batch: 020 ----
mean loss: 91.70
 ---- batch: 030 ----
mean loss: 88.10
 ---- batch: 040 ----
mean loss: 87.94
train mean loss: 88.78
epoch train time: 0:00:07.908161
elapsed time: 0:17:37.312562
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 14:53:32.115724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.72
 ---- batch: 020 ----
mean loss: 87.47
 ---- batch: 030 ----
mean loss: 86.60
 ---- batch: 040 ----
mean loss: 89.43
train mean loss: 88.56
epoch train time: 0:00:07.955163
elapsed time: 0:17:45.268199
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 14:53:40.071339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.96
 ---- batch: 020 ----
mean loss: 89.13
 ---- batch: 030 ----
mean loss: 88.74
 ---- batch: 040 ----
mean loss: 87.66
train mean loss: 89.23
epoch train time: 0:00:07.945634
elapsed time: 0:17:53.214256
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 14:53:48.017425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.58
 ---- batch: 020 ----
mean loss: 87.47
 ---- batch: 030 ----
mean loss: 89.48
 ---- batch: 040 ----
mean loss: 86.76
train mean loss: 87.10
epoch train time: 0:00:07.797444
elapsed time: 0:18:01.012498
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 14:53:55.815661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.31
 ---- batch: 020 ----
mean loss: 91.04
 ---- batch: 030 ----
mean loss: 90.35
 ---- batch: 040 ----
mean loss: 86.85
train mean loss: 89.43
epoch train time: 0:00:07.822107
elapsed time: 0:18:08.835090
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 14:54:03.638246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.49
 ---- batch: 020 ----
mean loss: 85.14
 ---- batch: 030 ----
mean loss: 84.61
 ---- batch: 040 ----
mean loss: 91.45
train mean loss: 86.76
epoch train time: 0:00:07.840241
elapsed time: 0:18:16.675840
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 14:54:11.478960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.38
 ---- batch: 020 ----
mean loss: 88.99
 ---- batch: 030 ----
mean loss: 84.88
 ---- batch: 040 ----
mean loss: 83.77
train mean loss: 86.14
epoch train time: 0:00:07.698085
elapsed time: 0:18:24.374356
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 14:54:19.177469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.09
 ---- batch: 020 ----
mean loss: 86.20
 ---- batch: 030 ----
mean loss: 88.90
 ---- batch: 040 ----
mean loss: 91.66
train mean loss: 87.88
epoch train time: 0:00:07.803482
elapsed time: 0:18:32.178251
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 14:54:26.981389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.67
 ---- batch: 020 ----
mean loss: 92.19
 ---- batch: 030 ----
mean loss: 85.09
 ---- batch: 040 ----
mean loss: 83.36
train mean loss: 86.87
epoch train time: 0:00:07.918203
elapsed time: 0:18:40.096891
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 14:54:34.900039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.95
 ---- batch: 020 ----
mean loss: 85.11
 ---- batch: 030 ----
mean loss: 86.20
 ---- batch: 040 ----
mean loss: 86.36
train mean loss: 85.96
epoch train time: 0:00:07.978439
elapsed time: 0:18:48.075781
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 14:54:42.878933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.24
 ---- batch: 020 ----
mean loss: 84.06
 ---- batch: 030 ----
mean loss: 86.85
 ---- batch: 040 ----
mean loss: 86.77
train mean loss: 86.24
epoch train time: 0:00:08.101272
elapsed time: 0:18:56.177510
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 14:54:50.980663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.64
 ---- batch: 020 ----
mean loss: 86.21
 ---- batch: 030 ----
mean loss: 84.87
 ---- batch: 040 ----
mean loss: 85.78
train mean loss: 84.70
epoch train time: 0:00:08.068406
elapsed time: 0:19:04.246368
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 14:54:59.049514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.32
 ---- batch: 020 ----
mean loss: 88.08
 ---- batch: 030 ----
mean loss: 85.00
 ---- batch: 040 ----
mean loss: 81.47
train mean loss: 84.42
epoch train time: 0:00:08.080627
elapsed time: 0:19:12.327481
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 14:55:07.130671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.79
 ---- batch: 020 ----
mean loss: 84.91
 ---- batch: 030 ----
mean loss: 82.71
 ---- batch: 040 ----
mean loss: 84.65
train mean loss: 84.10
epoch train time: 0:00:07.901953
elapsed time: 0:19:20.229927
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 14:55:15.033075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.48
 ---- batch: 020 ----
mean loss: 83.50
 ---- batch: 030 ----
mean loss: 86.15
 ---- batch: 040 ----
mean loss: 81.01
train mean loss: 84.21
epoch train time: 0:00:07.884720
elapsed time: 0:19:28.115103
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 14:55:22.918253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.39
 ---- batch: 020 ----
mean loss: 81.12
 ---- batch: 030 ----
mean loss: 84.46
 ---- batch: 040 ----
mean loss: 83.98
train mean loss: 83.71
epoch train time: 0:00:07.918102
elapsed time: 0:19:36.033694
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 14:55:30.836889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.27
 ---- batch: 020 ----
mean loss: 86.68
 ---- batch: 030 ----
mean loss: 81.92
 ---- batch: 040 ----
mean loss: 86.26
train mean loss: 84.21
epoch train time: 0:00:07.856507
elapsed time: 0:19:43.890716
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 14:55:38.693891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.63
 ---- batch: 020 ----
mean loss: 81.79
 ---- batch: 030 ----
mean loss: 83.86
 ---- batch: 040 ----
mean loss: 83.52
train mean loss: 83.83
epoch train time: 0:00:07.835376
elapsed time: 0:19:51.726578
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 14:55:46.529717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.46
 ---- batch: 020 ----
mean loss: 86.37
 ---- batch: 030 ----
mean loss: 82.22
 ---- batch: 040 ----
mean loss: 78.73
train mean loss: 82.51
epoch train time: 0:00:07.737985
elapsed time: 0:19:59.465062
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 14:55:54.268108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.95
 ---- batch: 020 ----
mean loss: 81.04
 ---- batch: 030 ----
mean loss: 82.81
 ---- batch: 040 ----
mean loss: 80.01
train mean loss: 82.31
epoch train time: 0:00:07.766791
elapsed time: 0:20:07.232187
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 14:56:02.035346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.35
 ---- batch: 020 ----
mean loss: 82.09
 ---- batch: 030 ----
mean loss: 83.20
 ---- batch: 040 ----
mean loss: 82.45
train mean loss: 82.93
epoch train time: 0:00:07.925892
elapsed time: 0:20:15.158524
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 14:56:09.961664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.02
 ---- batch: 020 ----
mean loss: 81.87
 ---- batch: 030 ----
mean loss: 82.48
 ---- batch: 040 ----
mean loss: 83.65
train mean loss: 82.90
epoch train time: 0:00:07.753156
elapsed time: 0:20:22.912079
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 14:56:17.715204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.63
 ---- batch: 020 ----
mean loss: 78.56
 ---- batch: 030 ----
mean loss: 80.51
 ---- batch: 040 ----
mean loss: 84.80
train mean loss: 82.19
epoch train time: 0:00:07.680280
elapsed time: 0:20:30.592766
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 14:56:25.395912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.04
 ---- batch: 020 ----
mean loss: 81.94
 ---- batch: 030 ----
mean loss: 78.70
 ---- batch: 040 ----
mean loss: 81.56
train mean loss: 80.86
epoch train time: 0:00:07.520462
elapsed time: 0:20:38.113693
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 14:56:32.916846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.57
 ---- batch: 020 ----
mean loss: 82.02
 ---- batch: 030 ----
mean loss: 80.30
 ---- batch: 040 ----
mean loss: 83.48
train mean loss: 81.52
epoch train time: 0:00:07.445988
elapsed time: 0:20:45.560125
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 14:56:40.363268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.50
 ---- batch: 020 ----
mean loss: 80.76
 ---- batch: 030 ----
mean loss: 86.35
 ---- batch: 040 ----
mean loss: 79.44
train mean loss: 81.34
epoch train time: 0:00:07.524887
elapsed time: 0:20:53.085493
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 14:56:47.888625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.31
 ---- batch: 020 ----
mean loss: 84.36
 ---- batch: 030 ----
mean loss: 82.30
 ---- batch: 040 ----
mean loss: 85.28
train mean loss: 82.66
epoch train time: 0:00:07.681036
elapsed time: 0:21:00.766961
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 14:56:55.570101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.25
 ---- batch: 020 ----
mean loss: 82.02
 ---- batch: 030 ----
mean loss: 81.04
 ---- batch: 040 ----
mean loss: 79.49
train mean loss: 80.35
epoch train time: 0:00:07.608082
elapsed time: 0:21:08.375469
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 14:57:03.178610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.17
 ---- batch: 020 ----
mean loss: 80.33
 ---- batch: 030 ----
mean loss: 82.39
 ---- batch: 040 ----
mean loss: 79.93
train mean loss: 80.58
epoch train time: 0:00:07.647283
elapsed time: 0:21:16.023166
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 14:57:10.826343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.20
 ---- batch: 020 ----
mean loss: 78.50
 ---- batch: 030 ----
mean loss: 78.69
 ---- batch: 040 ----
mean loss: 83.07
train mean loss: 80.00
epoch train time: 0:00:07.620480
elapsed time: 0:21:23.644076
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 14:57:18.447204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.16
 ---- batch: 020 ----
mean loss: 78.81
 ---- batch: 030 ----
mean loss: 77.80
 ---- batch: 040 ----
mean loss: 81.54
train mean loss: 79.13
epoch train time: 0:00:07.577947
elapsed time: 0:21:31.222453
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 14:57:26.025589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.23
 ---- batch: 020 ----
mean loss: 78.39
 ---- batch: 030 ----
mean loss: 79.53
 ---- batch: 040 ----
mean loss: 80.27
train mean loss: 79.29
epoch train time: 0:00:07.644335
elapsed time: 0:21:38.867197
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 14:57:33.670331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.50
 ---- batch: 020 ----
mean loss: 78.72
 ---- batch: 030 ----
mean loss: 77.22
 ---- batch: 040 ----
mean loss: 79.97
train mean loss: 78.39
epoch train time: 0:00:07.784636
elapsed time: 0:21:46.652209
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 14:57:41.455329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.24
 ---- batch: 020 ----
mean loss: 76.67
 ---- batch: 030 ----
mean loss: 82.19
 ---- batch: 040 ----
mean loss: 80.80
train mean loss: 79.74
epoch train time: 0:00:07.613299
elapsed time: 0:21:54.265953
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 14:57:49.069086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.83
 ---- batch: 020 ----
mean loss: 78.62
 ---- batch: 030 ----
mean loss: 78.82
 ---- batch: 040 ----
mean loss: 79.61
train mean loss: 77.93
epoch train time: 0:00:07.555470
elapsed time: 0:22:01.821834
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 14:57:56.624974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.80
 ---- batch: 020 ----
mean loss: 76.60
 ---- batch: 030 ----
mean loss: 78.65
 ---- batch: 040 ----
mean loss: 79.60
train mean loss: 79.23
epoch train time: 0:00:07.602503
elapsed time: 0:22:09.424765
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 14:58:04.227905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.06
 ---- batch: 020 ----
mean loss: 77.99
 ---- batch: 030 ----
mean loss: 77.83
 ---- batch: 040 ----
mean loss: 76.91
train mean loss: 77.73
epoch train time: 0:00:07.587519
elapsed time: 0:22:17.012753
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 14:58:11.815925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.83
 ---- batch: 020 ----
mean loss: 75.84
 ---- batch: 030 ----
mean loss: 79.12
 ---- batch: 040 ----
mean loss: 79.62
train mean loss: 77.92
epoch train time: 0:00:07.577698
elapsed time: 0:22:24.590894
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 14:58:19.394025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.81
 ---- batch: 020 ----
mean loss: 75.25
 ---- batch: 030 ----
mean loss: 79.52
 ---- batch: 040 ----
mean loss: 82.68
train mean loss: 78.87
epoch train time: 0:00:07.704512
elapsed time: 0:22:32.295814
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 14:58:27.098966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.15
 ---- batch: 020 ----
mean loss: 76.67
 ---- batch: 030 ----
mean loss: 77.54
 ---- batch: 040 ----
mean loss: 75.84
train mean loss: 77.27
epoch train time: 0:00:07.731996
elapsed time: 0:22:40.028290
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 14:58:34.831425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.04
 ---- batch: 020 ----
mean loss: 77.52
 ---- batch: 030 ----
mean loss: 76.60
 ---- batch: 040 ----
mean loss: 76.64
train mean loss: 77.58
epoch train time: 0:00:07.728744
elapsed time: 0:22:47.757495
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 14:58:42.560664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.08
 ---- batch: 020 ----
mean loss: 74.77
 ---- batch: 030 ----
mean loss: 80.15
 ---- batch: 040 ----
mean loss: 75.87
train mean loss: 76.56
epoch train time: 0:00:07.714010
elapsed time: 0:22:55.471973
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 14:58:50.275096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.26
 ---- batch: 020 ----
mean loss: 80.67
 ---- batch: 030 ----
mean loss: 76.03
 ---- batch: 040 ----
mean loss: 79.85
train mean loss: 79.05
epoch train time: 0:00:07.775313
elapsed time: 0:23:03.247734
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 14:58:58.050884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.84
 ---- batch: 020 ----
mean loss: 79.52
 ---- batch: 030 ----
mean loss: 76.93
 ---- batch: 040 ----
mean loss: 75.48
train mean loss: 76.49
epoch train time: 0:00:07.633955
elapsed time: 0:23:10.882186
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 14:59:05.685254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.24
 ---- batch: 020 ----
mean loss: 76.02
 ---- batch: 030 ----
mean loss: 75.84
 ---- batch: 040 ----
mean loss: 73.17
train mean loss: 75.91
epoch train time: 0:00:07.635437
elapsed time: 0:23:18.517944
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 14:59:13.321071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.44
 ---- batch: 020 ----
mean loss: 74.94
 ---- batch: 030 ----
mean loss: 77.61
 ---- batch: 040 ----
mean loss: 74.41
train mean loss: 75.69
epoch train time: 0:00:07.493745
elapsed time: 0:23:26.012183
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 14:59:20.815334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.99
 ---- batch: 020 ----
mean loss: 75.16
 ---- batch: 030 ----
mean loss: 75.95
 ---- batch: 040 ----
mean loss: 74.58
train mean loss: 75.18
epoch train time: 0:00:07.468468
elapsed time: 0:23:33.481046
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 14:59:28.284199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.32
 ---- batch: 020 ----
mean loss: 73.34
 ---- batch: 030 ----
mean loss: 76.86
 ---- batch: 040 ----
mean loss: 77.09
train mean loss: 76.53
epoch train time: 0:00:07.510501
elapsed time: 0:23:40.991978
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 14:59:35.795114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.16
 ---- batch: 020 ----
mean loss: 73.57
 ---- batch: 030 ----
mean loss: 73.08
 ---- batch: 040 ----
mean loss: 74.77
train mean loss: 75.07
epoch train time: 0:00:07.542571
elapsed time: 0:23:48.535046
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 14:59:43.338201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.09
 ---- batch: 020 ----
mean loss: 76.92
 ---- batch: 030 ----
mean loss: 74.90
 ---- batch: 040 ----
mean loss: 73.88
train mean loss: 75.62
epoch train time: 0:00:07.618400
elapsed time: 0:23:56.153858
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 14:59:50.956944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.61
 ---- batch: 020 ----
mean loss: 76.54
 ---- batch: 030 ----
mean loss: 72.42
 ---- batch: 040 ----
mean loss: 74.15
train mean loss: 75.58
epoch train time: 0:00:07.621810
elapsed time: 0:24:03.776004
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 14:59:58.579134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.77
 ---- batch: 020 ----
mean loss: 71.91
 ---- batch: 030 ----
mean loss: 73.37
 ---- batch: 040 ----
mean loss: 73.54
train mean loss: 73.70
epoch train time: 0:00:07.666449
elapsed time: 0:24:11.442855
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 15:00:06.245982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.14
 ---- batch: 020 ----
mean loss: 78.24
 ---- batch: 030 ----
mean loss: 73.17
 ---- batch: 040 ----
mean loss: 75.61
train mean loss: 75.00
epoch train time: 0:00:07.587069
elapsed time: 0:24:19.030355
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 15:00:13.833523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.51
 ---- batch: 020 ----
mean loss: 75.42
 ---- batch: 030 ----
mean loss: 75.76
 ---- batch: 040 ----
mean loss: 75.37
train mean loss: 74.80
epoch train time: 0:00:07.607205
elapsed time: 0:24:26.638014
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 15:00:21.441140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.02
 ---- batch: 020 ----
mean loss: 75.51
 ---- batch: 030 ----
mean loss: 72.11
 ---- batch: 040 ----
mean loss: 76.83
train mean loss: 74.07
epoch train time: 0:00:07.666790
elapsed time: 0:24:34.305257
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 15:00:29.108411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.81
 ---- batch: 020 ----
mean loss: 67.93
 ---- batch: 030 ----
mean loss: 74.20
 ---- batch: 040 ----
mean loss: 74.59
train mean loss: 73.01
epoch train time: 0:00:07.785278
elapsed time: 0:24:42.090965
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 15:00:36.894111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.83
 ---- batch: 020 ----
mean loss: 76.00
 ---- batch: 030 ----
mean loss: 72.32
 ---- batch: 040 ----
mean loss: 68.81
train mean loss: 72.96
epoch train time: 0:00:07.793150
elapsed time: 0:24:49.884578
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 15:00:44.687689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.87
 ---- batch: 020 ----
mean loss: 70.99
 ---- batch: 030 ----
mean loss: 72.79
 ---- batch: 040 ----
mean loss: 74.08
train mean loss: 72.40
epoch train time: 0:00:07.784615
elapsed time: 0:24:57.669594
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 15:00:52.472769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.53
 ---- batch: 020 ----
mean loss: 73.62
 ---- batch: 030 ----
mean loss: 74.47
 ---- batch: 040 ----
mean loss: 75.57
train mean loss: 73.60
epoch train time: 0:00:07.768097
elapsed time: 0:25:05.438186
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 15:01:00.241318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.88
 ---- batch: 020 ----
mean loss: 73.46
 ---- batch: 030 ----
mean loss: 71.87
 ---- batch: 040 ----
mean loss: 77.61
train mean loss: 74.65
epoch train time: 0:00:07.665387
elapsed time: 0:25:13.104045
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 15:01:07.907216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.12
 ---- batch: 020 ----
mean loss: 73.24
 ---- batch: 030 ----
mean loss: 70.19
 ---- batch: 040 ----
mean loss: 71.48
train mean loss: 72.41
epoch train time: 0:00:07.670851
elapsed time: 0:25:20.775362
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 15:01:15.578506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.55
 ---- batch: 020 ----
mean loss: 70.91
 ---- batch: 030 ----
mean loss: 74.11
 ---- batch: 040 ----
mean loss: 70.42
train mean loss: 71.99
epoch train time: 0:00:07.758629
elapsed time: 0:25:28.534459
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 15:01:23.337677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.22
 ---- batch: 020 ----
mean loss: 71.87
 ---- batch: 030 ----
mean loss: 75.47
 ---- batch: 040 ----
mean loss: 72.87
train mean loss: 72.84
epoch train time: 0:00:07.595475
elapsed time: 0:25:36.130431
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 15:01:30.933567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.67
 ---- batch: 020 ----
mean loss: 72.18
 ---- batch: 030 ----
mean loss: 73.63
 ---- batch: 040 ----
mean loss: 70.75
train mean loss: 71.21
epoch train time: 0:00:07.660544
elapsed time: 0:25:43.791372
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 15:01:38.594530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.63
 ---- batch: 020 ----
mean loss: 72.42
 ---- batch: 030 ----
mean loss: 71.81
 ---- batch: 040 ----
mean loss: 73.32
train mean loss: 72.38
epoch train time: 0:00:07.415805
elapsed time: 0:25:51.207639
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 15:01:46.010839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.47
 ---- batch: 020 ----
mean loss: 70.47
 ---- batch: 030 ----
mean loss: 74.06
 ---- batch: 040 ----
mean loss: 71.52
train mean loss: 71.34
epoch train time: 0:00:07.441154
elapsed time: 0:25:58.649295
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 15:01:53.452452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.52
 ---- batch: 020 ----
mean loss: 72.54
 ---- batch: 030 ----
mean loss: 70.57
 ---- batch: 040 ----
mean loss: 69.81
train mean loss: 70.87
epoch train time: 0:00:07.460731
elapsed time: 0:26:06.110657
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 15:02:00.913994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.38
 ---- batch: 020 ----
mean loss: 75.19
 ---- batch: 030 ----
mean loss: 73.02
 ---- batch: 040 ----
mean loss: 71.22
train mean loss: 72.85
epoch train time: 0:00:07.712852
elapsed time: 0:26:13.824119
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 15:02:08.627254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.03
 ---- batch: 020 ----
mean loss: 71.14
 ---- batch: 030 ----
mean loss: 72.88
 ---- batch: 040 ----
mean loss: 74.29
train mean loss: 72.21
epoch train time: 0:00:07.790509
elapsed time: 0:26:21.615103
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 15:02:16.418274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.11
 ---- batch: 020 ----
mean loss: 72.55
 ---- batch: 030 ----
mean loss: 71.56
 ---- batch: 040 ----
mean loss: 70.09
train mean loss: 70.82
epoch train time: 0:00:07.808958
elapsed time: 0:26:29.424619
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 15:02:24.227816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.48
 ---- batch: 020 ----
mean loss: 69.46
 ---- batch: 030 ----
mean loss: 71.41
 ---- batch: 040 ----
mean loss: 69.70
train mean loss: 70.25
epoch train time: 0:00:07.745708
elapsed time: 0:26:37.170836
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 15:02:31.973980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.50
 ---- batch: 020 ----
mean loss: 68.74
 ---- batch: 030 ----
mean loss: 69.18
 ---- batch: 040 ----
mean loss: 69.86
train mean loss: 68.82
epoch train time: 0:00:07.795888
elapsed time: 0:26:44.967297
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 15:02:39.770334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.92
 ---- batch: 020 ----
mean loss: 71.52
 ---- batch: 030 ----
mean loss: 68.63
 ---- batch: 040 ----
mean loss: 66.53
train mean loss: 68.88
epoch train time: 0:00:07.646770
elapsed time: 0:26:52.614391
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 15:02:47.417534
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.76
 ---- batch: 020 ----
mean loss: 71.89
 ---- batch: 030 ----
mean loss: 68.93
 ---- batch: 040 ----
mean loss: 66.46
train mean loss: 69.01
epoch train time: 0:00:07.679912
elapsed time: 0:27:00.294753
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 15:02:55.097935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.57
 ---- batch: 020 ----
mean loss: 68.34
 ---- batch: 030 ----
mean loss: 67.75
 ---- batch: 040 ----
mean loss: 71.52
train mean loss: 68.46
epoch train time: 0:00:07.751061
elapsed time: 0:27:08.046278
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 15:03:02.849430
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.28
 ---- batch: 020 ----
mean loss: 68.39
 ---- batch: 030 ----
mean loss: 68.10
 ---- batch: 040 ----
mean loss: 68.78
train mean loss: 68.94
epoch train time: 0:00:07.750248
elapsed time: 0:27:15.796966
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 15:03:10.600114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.11
 ---- batch: 020 ----
mean loss: 68.17
 ---- batch: 030 ----
mean loss: 70.57
 ---- batch: 040 ----
mean loss: 65.63
train mean loss: 68.30
epoch train time: 0:00:07.581329
elapsed time: 0:27:23.378682
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 15:03:18.181826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.11
 ---- batch: 020 ----
mean loss: 67.22
 ---- batch: 030 ----
mean loss: 67.69
 ---- batch: 040 ----
mean loss: 67.03
train mean loss: 68.30
epoch train time: 0:00:07.565336
elapsed time: 0:27:30.944480
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 15:03:25.747632
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.60
 ---- batch: 020 ----
mean loss: 71.23
 ---- batch: 030 ----
mean loss: 65.95
 ---- batch: 040 ----
mean loss: 69.47
train mean loss: 68.47
epoch train time: 0:00:07.578462
elapsed time: 0:27:38.523382
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 15:03:33.326525
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.83
 ---- batch: 020 ----
mean loss: 68.82
 ---- batch: 030 ----
mean loss: 66.15
 ---- batch: 040 ----
mean loss: 70.42
train mean loss: 68.28
epoch train time: 0:00:07.681268
elapsed time: 0:27:46.205073
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 15:03:41.008165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.01
 ---- batch: 020 ----
mean loss: 71.80
 ---- batch: 030 ----
mean loss: 68.23
 ---- batch: 040 ----
mean loss: 65.02
train mean loss: 68.21
epoch train time: 0:00:07.770274
elapsed time: 0:27:53.975741
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 15:03:48.778870
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.05
 ---- batch: 020 ----
mean loss: 68.73
 ---- batch: 030 ----
mean loss: 66.44
 ---- batch: 040 ----
mean loss: 68.77
train mean loss: 68.64
epoch train time: 0:00:07.765214
elapsed time: 0:28:01.741336
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 15:03:56.544489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.06
 ---- batch: 020 ----
mean loss: 67.85
 ---- batch: 030 ----
mean loss: 68.19
 ---- batch: 040 ----
mean loss: 69.60
train mean loss: 68.67
epoch train time: 0:00:07.686939
elapsed time: 0:28:09.428740
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 15:04:04.231881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.39
 ---- batch: 020 ----
mean loss: 65.94
 ---- batch: 030 ----
mean loss: 66.77
 ---- batch: 040 ----
mean loss: 71.94
train mean loss: 68.59
epoch train time: 0:00:07.517519
elapsed time: 0:28:16.946720
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 15:04:11.749870
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.69
 ---- batch: 020 ----
mean loss: 68.67
 ---- batch: 030 ----
mean loss: 66.48
 ---- batch: 040 ----
mean loss: 68.31
train mean loss: 68.02
epoch train time: 0:00:07.509806
elapsed time: 0:28:24.457024
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 15:04:19.260161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.19
 ---- batch: 020 ----
mean loss: 68.78
 ---- batch: 030 ----
mean loss: 69.33
 ---- batch: 040 ----
mean loss: 67.73
train mean loss: 68.53
epoch train time: 0:00:07.581265
elapsed time: 0:28:32.038735
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 15:04:26.841887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.56
 ---- batch: 020 ----
mean loss: 68.20
 ---- batch: 030 ----
mean loss: 69.01
 ---- batch: 040 ----
mean loss: 63.58
train mean loss: 68.36
epoch train time: 0:00:07.543094
elapsed time: 0:28:39.582265
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 15:04:34.385412
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.52
 ---- batch: 020 ----
mean loss: 66.83
 ---- batch: 030 ----
mean loss: 70.64
 ---- batch: 040 ----
mean loss: 68.70
train mean loss: 68.83
epoch train time: 0:00:07.437098
elapsed time: 0:28:47.019822
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 15:04:41.822967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.01
 ---- batch: 020 ----
mean loss: 71.01
 ---- batch: 030 ----
mean loss: 68.41
 ---- batch: 040 ----
mean loss: 68.27
train mean loss: 68.31
epoch train time: 0:00:07.410526
elapsed time: 0:28:54.430787
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 15:04:49.233952
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.30
 ---- batch: 020 ----
mean loss: 69.62
 ---- batch: 030 ----
mean loss: 68.77
 ---- batch: 040 ----
mean loss: 68.52
train mean loss: 68.87
epoch train time: 0:00:07.406662
elapsed time: 0:29:01.837881
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 15:04:56.641053
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.76
 ---- batch: 020 ----
mean loss: 69.71
 ---- batch: 030 ----
mean loss: 69.53
 ---- batch: 040 ----
mean loss: 66.40
train mean loss: 67.82
epoch train time: 0:00:07.401590
elapsed time: 0:29:09.239948
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 15:05:04.043103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.33
 ---- batch: 020 ----
mean loss: 68.18
 ---- batch: 030 ----
mean loss: 66.68
 ---- batch: 040 ----
mean loss: 70.18
train mean loss: 68.37
epoch train time: 0:00:07.471407
elapsed time: 0:29:16.711806
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 15:05:11.514964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.69
 ---- batch: 020 ----
mean loss: 70.48
 ---- batch: 030 ----
mean loss: 69.09
 ---- batch: 040 ----
mean loss: 67.21
train mean loss: 68.63
epoch train time: 0:00:07.619302
elapsed time: 0:29:24.331561
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 15:05:19.134696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.00
 ---- batch: 020 ----
mean loss: 68.55
 ---- batch: 030 ----
mean loss: 67.92
 ---- batch: 040 ----
mean loss: 67.50
train mean loss: 68.19
epoch train time: 0:00:07.455170
elapsed time: 0:29:31.787190
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 15:05:26.590341
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.70
 ---- batch: 020 ----
mean loss: 68.43
 ---- batch: 030 ----
mean loss: 66.51
 ---- batch: 040 ----
mean loss: 69.34
train mean loss: 68.10
epoch train time: 0:00:07.470866
elapsed time: 0:29:39.258617
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 15:05:34.061762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.58
 ---- batch: 020 ----
mean loss: 66.38
 ---- batch: 030 ----
mean loss: 68.50
 ---- batch: 040 ----
mean loss: 69.95
train mean loss: 68.08
epoch train time: 0:00:07.422403
elapsed time: 0:29:46.681455
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 15:05:41.484589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.32
 ---- batch: 020 ----
mean loss: 67.20
 ---- batch: 030 ----
mean loss: 66.13
 ---- batch: 040 ----
mean loss: 68.98
train mean loss: 67.94
epoch train time: 0:00:07.353940
elapsed time: 0:29:54.035870
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 15:05:48.839029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.75
 ---- batch: 020 ----
mean loss: 68.96
 ---- batch: 030 ----
mean loss: 71.15
 ---- batch: 040 ----
mean loss: 65.51
train mean loss: 68.08
epoch train time: 0:00:07.407263
elapsed time: 0:30:01.443563
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 15:05:56.246697
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.99
 ---- batch: 020 ----
mean loss: 67.35
 ---- batch: 030 ----
mean loss: 71.54
 ---- batch: 040 ----
mean loss: 66.37
train mean loss: 68.56
epoch train time: 0:00:07.644547
elapsed time: 0:30:09.088590
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 15:06:03.891707
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.76
 ---- batch: 020 ----
mean loss: 68.15
 ---- batch: 030 ----
mean loss: 68.03
 ---- batch: 040 ----
mean loss: 68.07
train mean loss: 68.05
epoch train time: 0:00:07.491823
elapsed time: 0:30:16.580783
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 15:06:11.383920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.65
 ---- batch: 020 ----
mean loss: 68.87
 ---- batch: 030 ----
mean loss: 69.67
 ---- batch: 040 ----
mean loss: 65.18
train mean loss: 68.56
epoch train time: 0:00:07.485535
elapsed time: 0:30:24.066740
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 15:06:18.869867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.33
 ---- batch: 020 ----
mean loss: 65.99
 ---- batch: 030 ----
mean loss: 67.01
 ---- batch: 040 ----
mean loss: 69.72
train mean loss: 67.92
epoch train time: 0:00:07.442562
elapsed time: 0:30:31.509692
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 15:06:26.312854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.72
 ---- batch: 020 ----
mean loss: 69.29
 ---- batch: 030 ----
mean loss: 69.02
 ---- batch: 040 ----
mean loss: 66.83
train mean loss: 68.02
epoch train time: 0:00:07.478319
elapsed time: 0:30:38.988525
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 15:06:33.791630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.23
 ---- batch: 020 ----
mean loss: 69.50
 ---- batch: 030 ----
mean loss: 67.43
 ---- batch: 040 ----
mean loss: 66.90
train mean loss: 68.36
epoch train time: 0:00:07.501665
elapsed time: 0:30:46.490682
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 15:06:41.293716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.98
 ---- batch: 020 ----
mean loss: 68.22
 ---- batch: 030 ----
mean loss: 68.61
 ---- batch: 040 ----
mean loss: 67.42
train mean loss: 68.12
epoch train time: 0:00:07.627621
elapsed time: 0:30:54.118609
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 15:06:48.921748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.55
 ---- batch: 020 ----
mean loss: 68.42
 ---- batch: 030 ----
mean loss: 66.62
 ---- batch: 040 ----
mean loss: 69.67
train mean loss: 68.33
epoch train time: 0:00:07.676595
elapsed time: 0:31:01.795611
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 15:06:56.598721
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.52
 ---- batch: 020 ----
mean loss: 68.42
 ---- batch: 030 ----
mean loss: 69.01
 ---- batch: 040 ----
mean loss: 68.05
train mean loss: 67.75
epoch train time: 0:00:07.714845
elapsed time: 0:31:09.510842
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 15:07:04.313993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.70
 ---- batch: 020 ----
mean loss: 65.21
 ---- batch: 030 ----
mean loss: 67.42
 ---- batch: 040 ----
mean loss: 68.04
train mean loss: 67.55
epoch train time: 0:00:07.677423
elapsed time: 0:31:17.188693
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 15:07:11.991856
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.18
 ---- batch: 020 ----
mean loss: 65.46
 ---- batch: 030 ----
mean loss: 68.92
 ---- batch: 040 ----
mean loss: 67.77
train mean loss: 67.37
epoch train time: 0:00:07.677477
elapsed time: 0:31:24.866600
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 15:07:19.669743
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.18
 ---- batch: 020 ----
mean loss: 66.54
 ---- batch: 030 ----
mean loss: 69.59
 ---- batch: 040 ----
mean loss: 69.30
train mean loss: 67.87
epoch train time: 0:00:07.632816
elapsed time: 0:31:32.499841
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 15:07:27.303060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.17
 ---- batch: 020 ----
mean loss: 68.21
 ---- batch: 030 ----
mean loss: 64.82
 ---- batch: 040 ----
mean loss: 69.56
train mean loss: 67.95
epoch train time: 0:00:07.729731
elapsed time: 0:31:40.230067
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 15:07:35.033199
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.30
 ---- batch: 020 ----
mean loss: 67.46
 ---- batch: 030 ----
mean loss: 67.75
 ---- batch: 040 ----
mean loss: 67.10
train mean loss: 67.11
epoch train time: 0:00:07.776160
elapsed time: 0:31:48.006671
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 15:07:42.809834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.11
 ---- batch: 020 ----
mean loss: 66.05
 ---- batch: 030 ----
mean loss: 67.34
 ---- batch: 040 ----
mean loss: 68.51
train mean loss: 67.67
epoch train time: 0:00:07.783324
elapsed time: 0:31:55.790491
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 15:07:50.593623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.44
 ---- batch: 020 ----
mean loss: 68.03
 ---- batch: 030 ----
mean loss: 67.39
 ---- batch: 040 ----
mean loss: 68.56
train mean loss: 67.80
epoch train time: 0:00:07.805542
elapsed time: 0:32:03.596441
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 15:07:58.399635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.16
 ---- batch: 020 ----
mean loss: 68.22
 ---- batch: 030 ----
mean loss: 67.64
 ---- batch: 040 ----
mean loss: 67.70
train mean loss: 67.50
epoch train time: 0:00:07.815706
elapsed time: 0:32:11.412674
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 15:08:06.215832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.47
 ---- batch: 020 ----
mean loss: 71.50
 ---- batch: 030 ----
mean loss: 67.23
 ---- batch: 040 ----
mean loss: 66.91
train mean loss: 68.08
epoch train time: 0:00:07.741960
elapsed time: 0:32:19.155154
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 15:08:13.958296
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.95
 ---- batch: 020 ----
mean loss: 65.12
 ---- batch: 030 ----
mean loss: 69.73
 ---- batch: 040 ----
mean loss: 67.21
train mean loss: 67.17
epoch train time: 0:00:07.632233
elapsed time: 0:32:26.787823
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 15:08:21.590979
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.39
 ---- batch: 020 ----
mean loss: 68.10
 ---- batch: 030 ----
mean loss: 64.86
 ---- batch: 040 ----
mean loss: 67.54
train mean loss: 67.48
epoch train time: 0:00:07.727261
elapsed time: 0:32:34.515498
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 15:08:29.318594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.23
 ---- batch: 020 ----
mean loss: 66.28
 ---- batch: 030 ----
mean loss: 71.55
 ---- batch: 040 ----
mean loss: 64.49
train mean loss: 67.57
epoch train time: 0:00:07.760892
elapsed time: 0:32:42.276770
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 15:08:37.079918
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.28
 ---- batch: 020 ----
mean loss: 66.93
 ---- batch: 030 ----
mean loss: 66.94
 ---- batch: 040 ----
mean loss: 66.47
train mean loss: 67.37
epoch train time: 0:00:07.525222
elapsed time: 0:32:49.810945
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_7/checkpoint.pth.tar
**** end time: 2019-09-20 15:08:44.613952 ****
