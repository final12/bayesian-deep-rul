Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7535
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 23:08:34.025859 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:08:34.033344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4572.85
 ---- batch: 020 ----
mean loss: 2996.89
 ---- batch: 030 ----
mean loss: 984.24
 ---- batch: 040 ----
mean loss: 387.26
train mean loss: 2107.95
epoch train time: 0:00:15.377363
elapsed time: 0:00:15.387303
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:08:49.413198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.29
 ---- batch: 020 ----
mean loss: 320.21
 ---- batch: 030 ----
mean loss: 285.93
 ---- batch: 040 ----
mean loss: 275.23
train mean loss: 305.42
epoch train time: 0:00:01.830944
elapsed time: 0:00:17.218372
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:08:51.244289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.38
 ---- batch: 020 ----
mean loss: 267.14
 ---- batch: 030 ----
mean loss: 254.23
 ---- batch: 040 ----
mean loss: 252.76
train mean loss: 259.02
epoch train time: 0:00:01.705756
elapsed time: 0:00:18.924261
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:08:52.950162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.04
 ---- batch: 020 ----
mean loss: 237.69
 ---- batch: 030 ----
mean loss: 228.79
 ---- batch: 040 ----
mean loss: 232.78
train mean loss: 234.00
epoch train time: 0:00:01.670732
elapsed time: 0:00:20.595193
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:08:54.621129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.84
 ---- batch: 020 ----
mean loss: 232.85
 ---- batch: 030 ----
mean loss: 220.17
 ---- batch: 040 ----
mean loss: 210.72
train mean loss: 223.41
epoch train time: 0:00:01.661308
elapsed time: 0:00:22.256730
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:08:56.282639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.53
 ---- batch: 020 ----
mean loss: 211.05
 ---- batch: 030 ----
mean loss: 221.83
 ---- batch: 040 ----
mean loss: 213.39
train mean loss: 216.09
epoch train time: 0:00:01.777414
elapsed time: 0:00:24.034275
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:08:58.060180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.79
 ---- batch: 020 ----
mean loss: 219.78
 ---- batch: 030 ----
mean loss: 212.37
 ---- batch: 040 ----
mean loss: 211.81
train mean loss: 213.28
epoch train time: 0:00:01.668014
elapsed time: 0:00:25.702428
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:08:59.728337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.66
 ---- batch: 020 ----
mean loss: 214.70
 ---- batch: 030 ----
mean loss: 209.41
 ---- batch: 040 ----
mean loss: 202.52
train mean loss: 211.21
epoch train time: 0:00:01.778433
elapsed time: 0:00:27.480999
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:09:01.506912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.16
 ---- batch: 020 ----
mean loss: 210.58
 ---- batch: 030 ----
mean loss: 200.89
 ---- batch: 040 ----
mean loss: 215.65
train mean loss: 209.76
epoch train time: 0:00:01.670885
elapsed time: 0:00:29.152033
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:09:03.177988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.16
 ---- batch: 020 ----
mean loss: 203.86
 ---- batch: 030 ----
mean loss: 205.35
 ---- batch: 040 ----
mean loss: 211.05
train mean loss: 207.44
epoch train time: 0:00:01.762955
elapsed time: 0:00:30.915173
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:09:04.941081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.92
 ---- batch: 020 ----
mean loss: 204.47
 ---- batch: 030 ----
mean loss: 199.13
 ---- batch: 040 ----
mean loss: 200.29
train mean loss: 203.09
epoch train time: 0:00:01.689500
elapsed time: 0:00:32.604826
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:09:06.630755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.88
 ---- batch: 020 ----
mean loss: 208.94
 ---- batch: 030 ----
mean loss: 196.01
 ---- batch: 040 ----
mean loss: 199.84
train mean loss: 201.70
epoch train time: 0:00:01.665826
elapsed time: 0:00:34.270826
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:09:08.296736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.40
 ---- batch: 020 ----
mean loss: 199.95
 ---- batch: 030 ----
mean loss: 197.95
 ---- batch: 040 ----
mean loss: 197.57
train mean loss: 200.51
epoch train time: 0:00:01.780021
elapsed time: 0:00:36.050988
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:09:10.076912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.61
 ---- batch: 020 ----
mean loss: 199.26
 ---- batch: 030 ----
mean loss: 198.29
 ---- batch: 040 ----
mean loss: 198.97
train mean loss: 200.46
epoch train time: 0:00:01.670065
elapsed time: 0:00:37.721208
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:09:11.747116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.68
 ---- batch: 020 ----
mean loss: 195.45
 ---- batch: 030 ----
mean loss: 194.79
 ---- batch: 040 ----
mean loss: 192.38
train mean loss: 195.78
epoch train time: 0:00:01.682682
elapsed time: 0:00:39.404024
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:09:13.429929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.94
 ---- batch: 020 ----
mean loss: 197.71
 ---- batch: 030 ----
mean loss: 195.84
 ---- batch: 040 ----
mean loss: 199.97
train mean loss: 197.70
epoch train time: 0:00:01.694587
elapsed time: 0:00:41.098790
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:09:15.124715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.04
 ---- batch: 020 ----
mean loss: 195.63
 ---- batch: 030 ----
mean loss: 196.90
 ---- batch: 040 ----
mean loss: 195.43
train mean loss: 196.94
epoch train time: 0:00:01.667350
elapsed time: 0:00:42.766327
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:09:16.792270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.67
 ---- batch: 020 ----
mean loss: 194.32
 ---- batch: 030 ----
mean loss: 199.97
 ---- batch: 040 ----
mean loss: 197.31
train mean loss: 195.25
epoch train time: 0:00:01.782073
elapsed time: 0:00:44.548570
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:09:18.574477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.42
 ---- batch: 020 ----
mean loss: 202.70
 ---- batch: 030 ----
mean loss: 202.28
 ---- batch: 040 ----
mean loss: 189.11
train mean loss: 197.00
epoch train time: 0:00:01.675787
elapsed time: 0:00:46.224522
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:09:20.250432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.00
 ---- batch: 020 ----
mean loss: 199.07
 ---- batch: 030 ----
mean loss: 194.21
 ---- batch: 040 ----
mean loss: 199.21
train mean loss: 196.25
epoch train time: 0:00:01.766581
elapsed time: 0:00:47.991290
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:09:22.017213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.61
 ---- batch: 020 ----
mean loss: 197.67
 ---- batch: 030 ----
mean loss: 195.53
 ---- batch: 040 ----
mean loss: 195.01
train mean loss: 195.04
epoch train time: 0:00:01.676603
elapsed time: 0:00:49.668048
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:09:23.693952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.05
 ---- batch: 020 ----
mean loss: 198.22
 ---- batch: 030 ----
mean loss: 188.17
 ---- batch: 040 ----
mean loss: 188.66
train mean loss: 192.30
epoch train time: 0:00:01.693261
elapsed time: 0:00:51.361433
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:09:25.387352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.51
 ---- batch: 020 ----
mean loss: 188.43
 ---- batch: 030 ----
mean loss: 188.89
 ---- batch: 040 ----
mean loss: 187.79
train mean loss: 191.10
epoch train time: 0:00:01.762957
elapsed time: 0:00:53.124542
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:09:27.150473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.48
 ---- batch: 020 ----
mean loss: 190.00
 ---- batch: 030 ----
mean loss: 195.31
 ---- batch: 040 ----
mean loss: 195.75
train mean loss: 192.55
epoch train time: 0:00:01.670816
elapsed time: 0:00:54.795537
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:09:28.821445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.03
 ---- batch: 020 ----
mean loss: 196.59
 ---- batch: 030 ----
mean loss: 196.49
 ---- batch: 040 ----
mean loss: 196.62
train mean loss: 194.15
epoch train time: 0:00:01.779972
elapsed time: 0:00:56.575662
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:09:30.601579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.74
 ---- batch: 020 ----
mean loss: 195.79
 ---- batch: 030 ----
mean loss: 186.82
 ---- batch: 040 ----
mean loss: 196.58
train mean loss: 191.45
epoch train time: 0:00:01.677084
elapsed time: 0:00:58.252919
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:09:32.278835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.71
 ---- batch: 020 ----
mean loss: 188.38
 ---- batch: 030 ----
mean loss: 199.95
 ---- batch: 040 ----
mean loss: 191.92
train mean loss: 191.21
epoch train time: 0:00:01.783975
elapsed time: 0:01:00.037034
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:09:34.062939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.17
 ---- batch: 020 ----
mean loss: 198.94
 ---- batch: 030 ----
mean loss: 193.91
 ---- batch: 040 ----
mean loss: 190.64
train mean loss: 193.16
epoch train time: 0:00:01.688893
elapsed time: 0:01:01.726075
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:09:35.751986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.65
 ---- batch: 020 ----
mean loss: 186.56
 ---- batch: 030 ----
mean loss: 194.48
 ---- batch: 040 ----
mean loss: 202.49
train mean loss: 193.46
epoch train time: 0:00:01.674318
elapsed time: 0:01:03.400535
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:09:37.426444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.86
 ---- batch: 020 ----
mean loss: 206.92
 ---- batch: 030 ----
mean loss: 187.12
 ---- batch: 040 ----
mean loss: 189.46
train mean loss: 194.45
epoch train time: 0:00:01.788984
elapsed time: 0:01:05.189653
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:09:39.215556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.12
 ---- batch: 020 ----
mean loss: 188.52
 ---- batch: 030 ----
mean loss: 192.54
 ---- batch: 040 ----
mean loss: 192.41
train mean loss: 193.28
epoch train time: 0:00:01.667521
elapsed time: 0:01:06.857318
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:09:40.883229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.18
 ---- batch: 020 ----
mean loss: 189.12
 ---- batch: 030 ----
mean loss: 194.36
 ---- batch: 040 ----
mean loss: 188.99
train mean loss: 193.34
epoch train time: 0:00:01.775828
elapsed time: 0:01:08.633285
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:09:42.659207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.69
 ---- batch: 020 ----
mean loss: 181.98
 ---- batch: 030 ----
mean loss: 192.34
 ---- batch: 040 ----
mean loss: 191.21
train mean loss: 191.88
epoch train time: 0:00:01.675063
elapsed time: 0:01:10.308515
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:09:44.334423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.92
 ---- batch: 020 ----
mean loss: 195.08
 ---- batch: 030 ----
mean loss: 190.50
 ---- batch: 040 ----
mean loss: 184.26
train mean loss: 190.33
epoch train time: 0:00:01.677002
elapsed time: 0:01:11.985674
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:09:46.011585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.43
 ---- batch: 020 ----
mean loss: 196.15
 ---- batch: 030 ----
mean loss: 189.70
 ---- batch: 040 ----
mean loss: 186.40
train mean loss: 189.95
epoch train time: 0:00:01.793908
elapsed time: 0:01:13.779741
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:09:47.805666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.92
 ---- batch: 020 ----
mean loss: 187.63
 ---- batch: 030 ----
mean loss: 188.81
 ---- batch: 040 ----
mean loss: 192.27
train mean loss: 189.51
epoch train time: 0:00:01.666462
elapsed time: 0:01:15.446370
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:09:49.472277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.97
 ---- batch: 020 ----
mean loss: 193.31
 ---- batch: 030 ----
mean loss: 187.73
 ---- batch: 040 ----
mean loss: 188.20
train mean loss: 187.69
epoch train time: 0:00:01.784538
elapsed time: 0:01:17.231071
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:09:51.256978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.04
 ---- batch: 020 ----
mean loss: 183.47
 ---- batch: 030 ----
mean loss: 184.19
 ---- batch: 040 ----
mean loss: 189.25
train mean loss: 187.25
epoch train time: 0:00:01.683081
elapsed time: 0:01:18.914307
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:09:52.940216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.12
 ---- batch: 020 ----
mean loss: 177.54
 ---- batch: 030 ----
mean loss: 189.62
 ---- batch: 040 ----
mean loss: 192.54
train mean loss: 187.32
epoch train time: 0:00:01.785591
elapsed time: 0:01:20.700047
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:09:54.725956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.55
 ---- batch: 020 ----
mean loss: 190.09
 ---- batch: 030 ----
mean loss: 185.79
 ---- batch: 040 ----
mean loss: 186.98
train mean loss: 188.50
epoch train time: 0:00:01.687848
elapsed time: 0:01:22.388054
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:09:56.413966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.02
 ---- batch: 020 ----
mean loss: 187.20
 ---- batch: 030 ----
mean loss: 188.01
 ---- batch: 040 ----
mean loss: 192.68
train mean loss: 189.35
epoch train time: 0:00:01.668923
elapsed time: 0:01:24.057123
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:09:58.083038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.59
 ---- batch: 020 ----
mean loss: 181.94
 ---- batch: 030 ----
mean loss: 194.01
 ---- batch: 040 ----
mean loss: 180.39
train mean loss: 186.79
epoch train time: 0:00:01.798962
elapsed time: 0:01:25.856241
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:09:59.882149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.12
 ---- batch: 020 ----
mean loss: 183.27
 ---- batch: 030 ----
mean loss: 192.34
 ---- batch: 040 ----
mean loss: 183.50
train mean loss: 186.52
epoch train time: 0:00:01.674731
elapsed time: 0:01:27.531133
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:10:01.557044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.71
 ---- batch: 020 ----
mean loss: 189.01
 ---- batch: 030 ----
mean loss: 180.71
 ---- batch: 040 ----
mean loss: 187.98
train mean loss: 188.83
epoch train time: 0:00:01.788315
elapsed time: 0:01:29.319593
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:10:03.345499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.73
 ---- batch: 020 ----
mean loss: 192.51
 ---- batch: 030 ----
mean loss: 188.54
 ---- batch: 040 ----
mean loss: 178.47
train mean loss: 186.82
epoch train time: 0:00:01.677057
elapsed time: 0:01:30.996804
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:10:05.022714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.57
 ---- batch: 020 ----
mean loss: 181.46
 ---- batch: 030 ----
mean loss: 176.57
 ---- batch: 040 ----
mean loss: 189.26
train mean loss: 183.68
epoch train time: 0:00:01.781861
elapsed time: 0:01:32.778821
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:10:06.804735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.69
 ---- batch: 020 ----
mean loss: 182.98
 ---- batch: 030 ----
mean loss: 187.18
 ---- batch: 040 ----
mean loss: 179.84
train mean loss: 182.49
epoch train time: 0:00:01.680331
elapsed time: 0:01:34.459380
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:10:08.485319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.22
 ---- batch: 020 ----
mean loss: 178.18
 ---- batch: 030 ----
mean loss: 182.33
 ---- batch: 040 ----
mean loss: 173.99
train mean loss: 179.40
epoch train time: 0:00:01.773381
elapsed time: 0:01:36.232947
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:10:10.258864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.67
 ---- batch: 020 ----
mean loss: 184.70
 ---- batch: 030 ----
mean loss: 181.98
 ---- batch: 040 ----
mean loss: 188.60
train mean loss: 183.91
epoch train time: 0:00:01.693356
elapsed time: 0:01:37.926470
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:10:11.952381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.90
 ---- batch: 020 ----
mean loss: 183.43
 ---- batch: 030 ----
mean loss: 176.39
 ---- batch: 040 ----
mean loss: 182.30
train mean loss: 180.92
epoch train time: 0:00:01.724589
elapsed time: 0:01:39.651231
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:10:13.677141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.87
 ---- batch: 020 ----
mean loss: 174.12
 ---- batch: 030 ----
mean loss: 173.14
 ---- batch: 040 ----
mean loss: 179.70
train mean loss: 175.04
epoch train time: 0:00:01.732319
elapsed time: 0:01:41.383701
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:10:15.409609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.90
 ---- batch: 020 ----
mean loss: 168.73
 ---- batch: 030 ----
mean loss: 180.63
 ---- batch: 040 ----
mean loss: 169.63
train mean loss: 173.44
epoch train time: 0:00:01.671731
elapsed time: 0:01:43.055575
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:10:17.081484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.39
 ---- batch: 020 ----
mean loss: 170.45
 ---- batch: 030 ----
mean loss: 173.08
 ---- batch: 040 ----
mean loss: 170.85
train mean loss: 174.20
epoch train time: 0:00:01.792296
elapsed time: 0:01:44.848040
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:10:18.873949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.14
 ---- batch: 020 ----
mean loss: 168.76
 ---- batch: 030 ----
mean loss: 165.74
 ---- batch: 040 ----
mean loss: 171.63
train mean loss: 167.31
epoch train time: 0:00:01.674823
elapsed time: 0:01:46.523022
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:10:20.548930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.52
 ---- batch: 020 ----
mean loss: 175.19
 ---- batch: 030 ----
mean loss: 181.64
 ---- batch: 040 ----
mean loss: 165.56
train mean loss: 174.13
epoch train time: 0:00:01.790042
elapsed time: 0:01:48.313212
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:10:22.339122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.12
 ---- batch: 020 ----
mean loss: 167.16
 ---- batch: 030 ----
mean loss: 163.79
 ---- batch: 040 ----
mean loss: 163.14
train mean loss: 167.11
epoch train time: 0:00:01.670903
elapsed time: 0:01:49.984268
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:10:24.010176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.41
 ---- batch: 020 ----
mean loss: 154.37
 ---- batch: 030 ----
mean loss: 161.95
 ---- batch: 040 ----
mean loss: 164.79
train mean loss: 162.59
epoch train time: 0:00:01.786479
elapsed time: 0:01:51.770915
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:10:25.796825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.67
 ---- batch: 020 ----
mean loss: 154.35
 ---- batch: 030 ----
mean loss: 162.24
 ---- batch: 040 ----
mean loss: 161.01
train mean loss: 160.02
epoch train time: 0:00:01.684929
elapsed time: 0:01:53.456007
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:10:27.481917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.22
 ---- batch: 020 ----
mean loss: 158.95
 ---- batch: 030 ----
mean loss: 162.75
 ---- batch: 040 ----
mean loss: 162.70
train mean loss: 159.98
epoch train time: 0:00:01.783376
elapsed time: 0:01:55.239533
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:10:29.265469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.03
 ---- batch: 020 ----
mean loss: 167.20
 ---- batch: 030 ----
mean loss: 159.58
 ---- batch: 040 ----
mean loss: 160.84
train mean loss: 161.02
epoch train time: 0:00:01.685670
elapsed time: 0:01:56.925380
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:10:30.951289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.52
 ---- batch: 020 ----
mean loss: 160.10
 ---- batch: 030 ----
mean loss: 154.90
 ---- batch: 040 ----
mean loss: 155.20
train mean loss: 156.89
epoch train time: 0:00:01.787536
elapsed time: 0:01:58.713049
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:10:32.738953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.44
 ---- batch: 020 ----
mean loss: 155.61
 ---- batch: 030 ----
mean loss: 150.27
 ---- batch: 040 ----
mean loss: 154.89
train mean loss: 152.38
epoch train time: 0:00:01.690969
elapsed time: 0:02:00.404161
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:10:34.430070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.33
 ---- batch: 020 ----
mean loss: 157.48
 ---- batch: 030 ----
mean loss: 150.78
 ---- batch: 040 ----
mean loss: 155.68
train mean loss: 153.77
epoch train time: 0:00:01.676156
elapsed time: 0:02:02.080448
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:10:36.106362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.70
 ---- batch: 020 ----
mean loss: 151.30
 ---- batch: 030 ----
mean loss: 153.61
 ---- batch: 040 ----
mean loss: 148.38
train mean loss: 150.85
epoch train time: 0:00:01.777560
elapsed time: 0:02:03.858215
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:10:37.884134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.15
 ---- batch: 020 ----
mean loss: 151.04
 ---- batch: 030 ----
mean loss: 155.43
 ---- batch: 040 ----
mean loss: 144.53
train mean loss: 151.70
epoch train time: 0:00:01.677998
elapsed time: 0:02:05.536363
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:10:39.562288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.62
 ---- batch: 020 ----
mean loss: 150.65
 ---- batch: 030 ----
mean loss: 146.55
 ---- batch: 040 ----
mean loss: 148.29
train mean loss: 148.20
epoch train time: 0:00:01.782581
elapsed time: 0:02:07.319094
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:10:41.344998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.43
 ---- batch: 020 ----
mean loss: 142.47
 ---- batch: 030 ----
mean loss: 146.06
 ---- batch: 040 ----
mean loss: 146.55
train mean loss: 145.89
epoch train time: 0:00:01.682018
elapsed time: 0:02:09.001270
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:10:43.027183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.23
 ---- batch: 020 ----
mean loss: 150.67
 ---- batch: 030 ----
mean loss: 147.94
 ---- batch: 040 ----
mean loss: 145.43
train mean loss: 147.60
epoch train time: 0:00:01.782739
elapsed time: 0:02:10.784174
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:10:44.810085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.88
 ---- batch: 020 ----
mean loss: 144.33
 ---- batch: 030 ----
mean loss: 149.65
 ---- batch: 040 ----
mean loss: 147.21
train mean loss: 148.43
epoch train time: 0:00:01.686804
elapsed time: 0:02:12.471127
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:10:46.497050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.90
 ---- batch: 020 ----
mean loss: 144.72
 ---- batch: 030 ----
mean loss: 140.59
 ---- batch: 040 ----
mean loss: 145.69
train mean loss: 145.36
epoch train time: 0:00:01.797589
elapsed time: 0:02:14.268859
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:10:48.294763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.22
 ---- batch: 020 ----
mean loss: 145.02
 ---- batch: 030 ----
mean loss: 145.53
 ---- batch: 040 ----
mean loss: 146.61
train mean loss: 145.31
epoch train time: 0:00:01.678225
elapsed time: 0:02:15.947222
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:10:49.973144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.57
 ---- batch: 020 ----
mean loss: 143.73
 ---- batch: 030 ----
mean loss: 148.74
 ---- batch: 040 ----
mean loss: 142.56
train mean loss: 143.45
epoch train time: 0:00:01.783843
elapsed time: 0:02:17.731223
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:10:51.757125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.92
 ---- batch: 020 ----
mean loss: 146.17
 ---- batch: 030 ----
mean loss: 137.79
 ---- batch: 040 ----
mean loss: 144.71
train mean loss: 143.12
epoch train time: 0:00:01.682930
elapsed time: 0:02:19.414291
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:10:53.440201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.47
 ---- batch: 020 ----
mean loss: 139.52
 ---- batch: 030 ----
mean loss: 147.55
 ---- batch: 040 ----
mean loss: 144.48
train mean loss: 144.09
epoch train time: 0:00:01.777309
elapsed time: 0:02:21.191739
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:10:55.217649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.16
 ---- batch: 020 ----
mean loss: 141.78
 ---- batch: 030 ----
mean loss: 141.47
 ---- batch: 040 ----
mean loss: 146.12
train mean loss: 143.40
epoch train time: 0:00:01.695299
elapsed time: 0:02:22.887221
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:10:56.913130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.88
 ---- batch: 020 ----
mean loss: 145.02
 ---- batch: 030 ----
mean loss: 143.80
 ---- batch: 040 ----
mean loss: 144.60
train mean loss: 144.78
epoch train time: 0:00:01.684169
elapsed time: 0:02:24.571517
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:10:58.597420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.54
 ---- batch: 020 ----
mean loss: 144.12
 ---- batch: 030 ----
mean loss: 143.17
 ---- batch: 040 ----
mean loss: 141.44
train mean loss: 143.53
epoch train time: 0:00:01.787321
elapsed time: 0:02:26.359013
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:11:00.384947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.87
 ---- batch: 020 ----
mean loss: 148.65
 ---- batch: 030 ----
mean loss: 142.58
 ---- batch: 040 ----
mean loss: 134.53
train mean loss: 141.17
epoch train time: 0:00:01.681327
elapsed time: 0:02:28.040514
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:11:02.066450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.90
 ---- batch: 020 ----
mean loss: 140.57
 ---- batch: 030 ----
mean loss: 137.99
 ---- batch: 040 ----
mean loss: 145.16
train mean loss: 142.56
epoch train time: 0:00:01.794741
elapsed time: 0:02:29.835436
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:11:03.861354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.33
 ---- batch: 020 ----
mean loss: 138.78
 ---- batch: 030 ----
mean loss: 146.00
 ---- batch: 040 ----
mean loss: 140.05
train mean loss: 142.28
epoch train time: 0:00:01.683338
elapsed time: 0:02:31.518941
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:11:05.544855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.08
 ---- batch: 020 ----
mean loss: 146.44
 ---- batch: 030 ----
mean loss: 141.87
 ---- batch: 040 ----
mean loss: 138.84
train mean loss: 143.83
epoch train time: 0:00:01.791372
elapsed time: 0:02:33.310485
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:11:07.336409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.05
 ---- batch: 020 ----
mean loss: 150.65
 ---- batch: 030 ----
mean loss: 139.91
 ---- batch: 040 ----
mean loss: 140.73
train mean loss: 143.83
epoch train time: 0:00:01.685496
elapsed time: 0:02:34.996161
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:11:09.022076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.37
 ---- batch: 020 ----
mean loss: 137.35
 ---- batch: 030 ----
mean loss: 139.54
 ---- batch: 040 ----
mean loss: 138.69
train mean loss: 139.82
epoch train time: 0:00:01.795196
elapsed time: 0:02:36.791495
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:11:10.817414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.83
 ---- batch: 020 ----
mean loss: 140.20
 ---- batch: 030 ----
mean loss: 140.12
 ---- batch: 040 ----
mean loss: 141.42
train mean loss: 139.39
epoch train time: 0:00:01.689679
elapsed time: 0:02:38.481326
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:11:12.507233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.73
 ---- batch: 020 ----
mean loss: 139.71
 ---- batch: 030 ----
mean loss: 139.33
 ---- batch: 040 ----
mean loss: 140.61
train mean loss: 139.83
epoch train time: 0:00:01.786860
elapsed time: 0:02:40.268320
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:11:14.294227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.08
 ---- batch: 020 ----
mean loss: 146.01
 ---- batch: 030 ----
mean loss: 143.02
 ---- batch: 040 ----
mean loss: 142.45
train mean loss: 141.70
epoch train time: 0:00:01.687043
elapsed time: 0:02:41.955526
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:11:15.981435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.19
 ---- batch: 020 ----
mean loss: 140.79
 ---- batch: 030 ----
mean loss: 136.29
 ---- batch: 040 ----
mean loss: 137.81
train mean loss: 139.14
epoch train time: 0:00:01.777730
elapsed time: 0:02:43.733420
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:11:17.759324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.96
 ---- batch: 020 ----
mean loss: 135.07
 ---- batch: 030 ----
mean loss: 140.40
 ---- batch: 040 ----
mean loss: 134.76
train mean loss: 137.09
epoch train time: 0:00:01.690692
elapsed time: 0:02:45.424248
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:11:19.450157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.33
 ---- batch: 020 ----
mean loss: 142.49
 ---- batch: 030 ----
mean loss: 139.36
 ---- batch: 040 ----
mean loss: 133.17
train mean loss: 138.14
epoch train time: 0:00:01.676299
elapsed time: 0:02:47.100689
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:11:21.126598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.31
 ---- batch: 020 ----
mean loss: 145.62
 ---- batch: 030 ----
mean loss: 141.05
 ---- batch: 040 ----
mean loss: 141.85
train mean loss: 142.60
epoch train time: 0:00:01.789905
elapsed time: 0:02:48.890738
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:11:22.916665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.52
 ---- batch: 020 ----
mean loss: 139.89
 ---- batch: 030 ----
mean loss: 137.01
 ---- batch: 040 ----
mean loss: 138.63
train mean loss: 137.68
epoch train time: 0:00:01.680934
elapsed time: 0:02:50.571837
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:11:24.597748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.53
 ---- batch: 020 ----
mean loss: 143.31
 ---- batch: 030 ----
mean loss: 140.30
 ---- batch: 040 ----
mean loss: 132.72
train mean loss: 138.31
epoch train time: 0:00:01.810575
elapsed time: 0:02:52.382586
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:11:26.408490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.69
 ---- batch: 020 ----
mean loss: 136.79
 ---- batch: 030 ----
mean loss: 136.14
 ---- batch: 040 ----
mean loss: 139.61
train mean loss: 136.86
epoch train time: 0:00:01.692613
elapsed time: 0:02:54.075338
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:11:28.101276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.42
 ---- batch: 020 ----
mean loss: 138.35
 ---- batch: 030 ----
mean loss: 135.57
 ---- batch: 040 ----
mean loss: 136.53
train mean loss: 137.79
epoch train time: 0:00:01.782362
elapsed time: 0:02:55.857864
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:11:29.883796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.29
 ---- batch: 020 ----
mean loss: 149.40
 ---- batch: 030 ----
mean loss: 139.45
 ---- batch: 040 ----
mean loss: 139.11
train mean loss: 139.68
epoch train time: 0:00:01.680105
elapsed time: 0:02:57.538148
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:11:31.564075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.38
 ---- batch: 020 ----
mean loss: 135.11
 ---- batch: 030 ----
mean loss: 133.88
 ---- batch: 040 ----
mean loss: 133.64
train mean loss: 135.00
epoch train time: 0:00:01.776091
elapsed time: 0:02:59.314395
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:11:33.340303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.02
 ---- batch: 020 ----
mean loss: 133.44
 ---- batch: 030 ----
mean loss: 131.26
 ---- batch: 040 ----
mean loss: 134.82
train mean loss: 136.03
epoch train time: 0:00:01.664027
elapsed time: 0:03:00.978586
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:11:35.004500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.44
 ---- batch: 020 ----
mean loss: 135.91
 ---- batch: 030 ----
mean loss: 139.67
 ---- batch: 040 ----
mean loss: 140.19
train mean loss: 138.22
epoch train time: 0:00:01.763801
elapsed time: 0:03:02.742555
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:11:36.768467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.00
 ---- batch: 020 ----
mean loss: 147.89
 ---- batch: 030 ----
mean loss: 137.39
 ---- batch: 040 ----
mean loss: 137.03
train mean loss: 142.50
epoch train time: 0:00:01.677981
elapsed time: 0:03:04.420689
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:11:38.446599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.04
 ---- batch: 020 ----
mean loss: 131.41
 ---- batch: 030 ----
mean loss: 134.59
 ---- batch: 040 ----
mean loss: 135.84
train mean loss: 135.08
epoch train time: 0:00:01.671574
elapsed time: 0:03:06.092410
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:11:40.118321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.68
 ---- batch: 020 ----
mean loss: 139.22
 ---- batch: 030 ----
mean loss: 134.86
 ---- batch: 040 ----
mean loss: 135.30
train mean loss: 135.41
epoch train time: 0:00:01.782355
elapsed time: 0:03:07.874922
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:11:41.900865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.40
 ---- batch: 020 ----
mean loss: 138.37
 ---- batch: 030 ----
mean loss: 133.53
 ---- batch: 040 ----
mean loss: 134.23
train mean loss: 136.92
epoch train time: 0:00:01.664262
elapsed time: 0:03:09.539386
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:11:43.565319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.88
 ---- batch: 020 ----
mean loss: 135.95
 ---- batch: 030 ----
mean loss: 138.55
 ---- batch: 040 ----
mean loss: 132.84
train mean loss: 135.77
epoch train time: 0:00:01.779328
elapsed time: 0:03:11.318886
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:11:45.344798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.70
 ---- batch: 020 ----
mean loss: 136.12
 ---- batch: 030 ----
mean loss: 136.08
 ---- batch: 040 ----
mean loss: 132.58
train mean loss: 135.09
epoch train time: 0:00:01.676823
elapsed time: 0:03:12.995860
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:11:47.021780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.38
 ---- batch: 020 ----
mean loss: 134.89
 ---- batch: 030 ----
mean loss: 131.13
 ---- batch: 040 ----
mean loss: 137.84
train mean loss: 133.46
epoch train time: 0:00:01.783656
elapsed time: 0:03:14.779694
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:11:48.805604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.55
 ---- batch: 020 ----
mean loss: 135.14
 ---- batch: 030 ----
mean loss: 130.15
 ---- batch: 040 ----
mean loss: 128.11
train mean loss: 132.21
epoch train time: 0:00:01.663207
elapsed time: 0:03:16.443069
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:11:50.468998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.37
 ---- batch: 020 ----
mean loss: 134.71
 ---- batch: 030 ----
mean loss: 136.84
 ---- batch: 040 ----
mean loss: 133.67
train mean loss: 135.69
epoch train time: 0:00:01.789863
elapsed time: 0:03:18.233267
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:11:52.259182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.29
 ---- batch: 020 ----
mean loss: 135.77
 ---- batch: 030 ----
mean loss: 133.98
 ---- batch: 040 ----
mean loss: 130.34
train mean loss: 134.34
epoch train time: 0:00:01.680315
elapsed time: 0:03:19.913732
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:11:53.939640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.05
 ---- batch: 020 ----
mean loss: 133.76
 ---- batch: 030 ----
mean loss: 134.12
 ---- batch: 040 ----
mean loss: 134.30
train mean loss: 132.97
epoch train time: 0:00:01.657477
elapsed time: 0:03:21.571374
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:11:55.597291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.76
 ---- batch: 020 ----
mean loss: 127.84
 ---- batch: 030 ----
mean loss: 135.91
 ---- batch: 040 ----
mean loss: 125.50
train mean loss: 130.85
epoch train time: 0:00:01.768869
elapsed time: 0:03:23.340392
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:11:57.366309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.64
 ---- batch: 020 ----
mean loss: 132.49
 ---- batch: 030 ----
mean loss: 134.47
 ---- batch: 040 ----
mean loss: 128.53
train mean loss: 132.92
epoch train time: 0:00:01.660986
elapsed time: 0:03:25.001544
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:11:59.027452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.11
 ---- batch: 020 ----
mean loss: 135.76
 ---- batch: 030 ----
mean loss: 131.59
 ---- batch: 040 ----
mean loss: 133.75
train mean loss: 133.83
epoch train time: 0:00:01.770097
elapsed time: 0:03:26.771785
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:12:00.797695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.16
 ---- batch: 020 ----
mean loss: 135.27
 ---- batch: 030 ----
mean loss: 132.10
 ---- batch: 040 ----
mean loss: 129.16
train mean loss: 133.08
epoch train time: 0:00:01.686958
elapsed time: 0:03:28.458921
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:12:02.484827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.47
 ---- batch: 020 ----
mean loss: 126.76
 ---- batch: 030 ----
mean loss: 133.23
 ---- batch: 040 ----
mean loss: 131.04
train mean loss: 131.55
epoch train time: 0:00:01.670230
elapsed time: 0:03:30.129304
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:12:04.155211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.01
 ---- batch: 020 ----
mean loss: 130.03
 ---- batch: 030 ----
mean loss: 134.97
 ---- batch: 040 ----
mean loss: 130.43
train mean loss: 131.13
epoch train time: 0:00:01.782748
elapsed time: 0:03:31.912186
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:12:05.938094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.35
 ---- batch: 020 ----
mean loss: 131.54
 ---- batch: 030 ----
mean loss: 132.83
 ---- batch: 040 ----
mean loss: 130.66
train mean loss: 131.82
epoch train time: 0:00:01.676977
elapsed time: 0:03:33.589307
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:12:07.615215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.17
 ---- batch: 020 ----
mean loss: 131.10
 ---- batch: 030 ----
mean loss: 131.53
 ---- batch: 040 ----
mean loss: 135.30
train mean loss: 132.28
epoch train time: 0:00:01.797362
elapsed time: 0:03:35.386815
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:12:09.412723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.49
 ---- batch: 020 ----
mean loss: 130.64
 ---- batch: 030 ----
mean loss: 129.92
 ---- batch: 040 ----
mean loss: 132.62
train mean loss: 130.68
epoch train time: 0:00:01.670956
elapsed time: 0:03:37.057925
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:12:11.083853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.54
 ---- batch: 020 ----
mean loss: 130.58
 ---- batch: 030 ----
mean loss: 132.18
 ---- batch: 040 ----
mean loss: 128.58
train mean loss: 129.79
epoch train time: 0:00:01.767551
elapsed time: 0:03:38.825633
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:12:12.851541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.44
 ---- batch: 020 ----
mean loss: 130.04
 ---- batch: 030 ----
mean loss: 132.61
 ---- batch: 040 ----
mean loss: 130.75
train mean loss: 130.00
epoch train time: 0:00:01.683373
elapsed time: 0:03:40.509175
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:12:14.535084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.59
 ---- batch: 020 ----
mean loss: 130.33
 ---- batch: 030 ----
mean loss: 131.01
 ---- batch: 040 ----
mean loss: 133.76
train mean loss: 130.73
epoch train time: 0:00:01.780119
elapsed time: 0:03:42.289440
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:12:16.315362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.71
 ---- batch: 020 ----
mean loss: 132.37
 ---- batch: 030 ----
mean loss: 122.16
 ---- batch: 040 ----
mean loss: 134.15
train mean loss: 131.34
epoch train time: 0:00:01.679615
elapsed time: 0:03:43.969217
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:12:17.995163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.11
 ---- batch: 020 ----
mean loss: 135.51
 ---- batch: 030 ----
mean loss: 135.23
 ---- batch: 040 ----
mean loss: 127.53
train mean loss: 132.43
epoch train time: 0:00:01.668896
elapsed time: 0:03:45.638308
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:12:19.664235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.23
 ---- batch: 020 ----
mean loss: 131.40
 ---- batch: 030 ----
mean loss: 133.53
 ---- batch: 040 ----
mean loss: 134.62
train mean loss: 134.05
epoch train time: 0:00:01.785560
elapsed time: 0:03:47.424025
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:12:21.449936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.98
 ---- batch: 020 ----
mean loss: 132.03
 ---- batch: 030 ----
mean loss: 129.32
 ---- batch: 040 ----
mean loss: 128.77
train mean loss: 133.89
epoch train time: 0:00:01.665933
elapsed time: 0:03:49.090104
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:12:23.116012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.42
 ---- batch: 020 ----
mean loss: 126.60
 ---- batch: 030 ----
mean loss: 133.39
 ---- batch: 040 ----
mean loss: 134.43
train mean loss: 131.75
epoch train time: 0:00:01.785074
elapsed time: 0:03:50.875316
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:12:24.901220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.49
 ---- batch: 020 ----
mean loss: 136.20
 ---- batch: 030 ----
mean loss: 139.52
 ---- batch: 040 ----
mean loss: 129.79
train mean loss: 133.33
epoch train time: 0:00:01.676549
elapsed time: 0:03:52.552027
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:12:26.577929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.80
 ---- batch: 020 ----
mean loss: 132.02
 ---- batch: 030 ----
mean loss: 130.90
 ---- batch: 040 ----
mean loss: 137.67
train mean loss: 133.38
epoch train time: 0:00:01.785536
elapsed time: 0:03:54.337698
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:12:28.363606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.40
 ---- batch: 020 ----
mean loss: 128.09
 ---- batch: 030 ----
mean loss: 133.51
 ---- batch: 040 ----
mean loss: 130.75
train mean loss: 130.18
epoch train time: 0:00:01.675746
elapsed time: 0:03:56.013600
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:12:30.039515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.14
 ---- batch: 020 ----
mean loss: 131.26
 ---- batch: 030 ----
mean loss: 129.37
 ---- batch: 040 ----
mean loss: 130.89
train mean loss: 131.36
epoch train time: 0:00:01.779266
elapsed time: 0:03:57.793045
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:12:31.818971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.18
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 127.75
 ---- batch: 040 ----
mean loss: 128.45
train mean loss: 131.09
epoch train time: 0:00:01.678050
elapsed time: 0:03:59.471265
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:12:33.497175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.34
 ---- batch: 020 ----
mean loss: 132.40
 ---- batch: 030 ----
mean loss: 128.48
 ---- batch: 040 ----
mean loss: 127.75
train mean loss: 129.58
epoch train time: 0:00:01.779660
elapsed time: 0:04:01.251087
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:12:35.277007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.77
 ---- batch: 020 ----
mean loss: 129.32
 ---- batch: 030 ----
mean loss: 136.54
 ---- batch: 040 ----
mean loss: 128.00
train mean loss: 131.34
epoch train time: 0:00:01.692731
elapsed time: 0:04:02.943981
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:12:36.969894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.35
 ---- batch: 020 ----
mean loss: 132.51
 ---- batch: 030 ----
mean loss: 135.09
 ---- batch: 040 ----
mean loss: 127.83
train mean loss: 131.46
epoch train time: 0:00:01.745273
elapsed time: 0:04:04.689404
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:12:38.715312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.36
 ---- batch: 020 ----
mean loss: 131.22
 ---- batch: 030 ----
mean loss: 135.95
 ---- batch: 040 ----
mean loss: 132.60
train mean loss: 131.61
epoch train time: 0:00:01.721582
elapsed time: 0:04:06.411159
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:12:40.437070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.14
 ---- batch: 020 ----
mean loss: 125.88
 ---- batch: 030 ----
mean loss: 129.59
 ---- batch: 040 ----
mean loss: 132.72
train mean loss: 128.92
epoch train time: 0:00:01.668593
elapsed time: 0:04:08.079926
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:12:42.105833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.09
 ---- batch: 020 ----
mean loss: 130.42
 ---- batch: 030 ----
mean loss: 130.91
 ---- batch: 040 ----
mean loss: 127.26
train mean loss: 130.26
epoch train time: 0:00:01.774822
elapsed time: 0:04:09.854876
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:12:43.880780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.75
 ---- batch: 020 ----
mean loss: 127.45
 ---- batch: 030 ----
mean loss: 138.02
 ---- batch: 040 ----
mean loss: 131.10
train mean loss: 131.88
epoch train time: 0:00:01.686419
elapsed time: 0:04:11.541439
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:12:45.567346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.03
 ---- batch: 020 ----
mean loss: 135.54
 ---- batch: 030 ----
mean loss: 126.54
 ---- batch: 040 ----
mean loss: 128.70
train mean loss: 129.96
epoch train time: 0:00:01.787215
elapsed time: 0:04:13.328789
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:12:47.354699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.14
 ---- batch: 020 ----
mean loss: 128.84
 ---- batch: 030 ----
mean loss: 130.44
 ---- batch: 040 ----
mean loss: 129.11
train mean loss: 129.32
epoch train time: 0:00:01.680488
elapsed time: 0:04:15.009433
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:12:49.035368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.93
 ---- batch: 020 ----
mean loss: 127.84
 ---- batch: 030 ----
mean loss: 127.16
 ---- batch: 040 ----
mean loss: 135.07
train mean loss: 131.23
epoch train time: 0:00:01.778139
elapsed time: 0:04:16.787742
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:12:50.813651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.31
 ---- batch: 020 ----
mean loss: 129.91
 ---- batch: 030 ----
mean loss: 124.66
 ---- batch: 040 ----
mean loss: 130.44
train mean loss: 128.68
epoch train time: 0:00:01.685626
elapsed time: 0:04:18.473534
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:12:52.499464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.85
 ---- batch: 020 ----
mean loss: 133.49
 ---- batch: 030 ----
mean loss: 132.38
 ---- batch: 040 ----
mean loss: 123.32
train mean loss: 128.91
epoch train time: 0:00:01.787453
elapsed time: 0:04:20.261142
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:12:54.287046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.48
 ---- batch: 020 ----
mean loss: 131.90
 ---- batch: 030 ----
mean loss: 130.01
 ---- batch: 040 ----
mean loss: 129.27
train mean loss: 130.27
epoch train time: 0:00:01.687860
elapsed time: 0:04:21.949188
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:12:55.975105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.41
 ---- batch: 020 ----
mean loss: 131.07
 ---- batch: 030 ----
mean loss: 130.36
 ---- batch: 040 ----
mean loss: 125.23
train mean loss: 128.77
epoch train time: 0:00:01.779955
elapsed time: 0:04:23.729287
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:12:57.755201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.55
 ---- batch: 020 ----
mean loss: 128.19
 ---- batch: 030 ----
mean loss: 124.50
 ---- batch: 040 ----
mean loss: 121.81
train mean loss: 126.23
epoch train time: 0:00:01.689071
elapsed time: 0:04:25.418593
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:12:59.444507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.81
 ---- batch: 020 ----
mean loss: 136.00
 ---- batch: 030 ----
mean loss: 130.90
 ---- batch: 040 ----
mean loss: 131.40
train mean loss: 131.38
epoch train time: 0:00:01.698644
elapsed time: 0:04:27.117386
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:13:01.143304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.33
 ---- batch: 020 ----
mean loss: 126.41
 ---- batch: 030 ----
mean loss: 130.83
 ---- batch: 040 ----
mean loss: 132.33
train mean loss: 130.86
epoch train time: 0:00:01.760663
elapsed time: 0:04:28.878239
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:13:02.904187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.06
 ---- batch: 020 ----
mean loss: 131.44
 ---- batch: 030 ----
mean loss: 127.53
 ---- batch: 040 ----
mean loss: 130.22
train mean loss: 129.90
epoch train time: 0:00:01.679591
elapsed time: 0:04:30.558041
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:13:04.583970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.40
 ---- batch: 020 ----
mean loss: 126.48
 ---- batch: 030 ----
mean loss: 126.18
 ---- batch: 040 ----
mean loss: 125.22
train mean loss: 126.93
epoch train time: 0:00:01.783406
elapsed time: 0:04:32.341628
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:13:06.367547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.91
 ---- batch: 020 ----
mean loss: 126.90
 ---- batch: 030 ----
mean loss: 131.07
 ---- batch: 040 ----
mean loss: 128.01
train mean loss: 128.59
epoch train time: 0:00:01.680381
elapsed time: 0:04:34.022192
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:13:08.048103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.39
 ---- batch: 020 ----
mean loss: 125.01
 ---- batch: 030 ----
mean loss: 127.86
 ---- batch: 040 ----
mean loss: 131.27
train mean loss: 128.81
epoch train time: 0:00:01.798964
elapsed time: 0:04:35.821300
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:13:09.847204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.56
 ---- batch: 020 ----
mean loss: 129.65
 ---- batch: 030 ----
mean loss: 125.94
 ---- batch: 040 ----
mean loss: 127.61
train mean loss: 127.78
epoch train time: 0:00:01.683231
elapsed time: 0:04:37.504672
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:13:11.530591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.58
 ---- batch: 020 ----
mean loss: 127.23
 ---- batch: 030 ----
mean loss: 125.84
 ---- batch: 040 ----
mean loss: 130.10
train mean loss: 129.26
epoch train time: 0:00:01.783065
elapsed time: 0:04:39.287884
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:13:13.313849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.92
 ---- batch: 020 ----
mean loss: 133.68
 ---- batch: 030 ----
mean loss: 124.36
 ---- batch: 040 ----
mean loss: 131.79
train mean loss: 128.38
epoch train time: 0:00:01.675800
elapsed time: 0:04:40.963892
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:13:14.989801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.02
 ---- batch: 020 ----
mean loss: 130.74
 ---- batch: 030 ----
mean loss: 135.21
 ---- batch: 040 ----
mean loss: 125.45
train mean loss: 130.38
epoch train time: 0:00:01.776004
elapsed time: 0:04:42.740027
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:13:16.765930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.60
 ---- batch: 020 ----
mean loss: 131.04
 ---- batch: 030 ----
mean loss: 127.75
 ---- batch: 040 ----
mean loss: 133.44
train mean loss: 129.46
epoch train time: 0:00:01.683819
elapsed time: 0:04:44.423981
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:13:18.449889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.28
 ---- batch: 020 ----
mean loss: 125.94
 ---- batch: 030 ----
mean loss: 122.78
 ---- batch: 040 ----
mean loss: 132.82
train mean loss: 125.24
epoch train time: 0:00:01.781246
elapsed time: 0:04:46.205357
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:13:20.231261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.55
 ---- batch: 020 ----
mean loss: 130.28
 ---- batch: 030 ----
mean loss: 134.44
 ---- batch: 040 ----
mean loss: 133.90
train mean loss: 131.42
epoch train time: 0:00:01.686503
elapsed time: 0:04:47.892019
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:13:21.917928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.91
 ---- batch: 020 ----
mean loss: 122.05
 ---- batch: 030 ----
mean loss: 125.29
 ---- batch: 040 ----
mean loss: 128.37
train mean loss: 126.90
epoch train time: 0:00:01.673982
elapsed time: 0:04:49.566128
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:13:23.592046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.86
 ---- batch: 020 ----
mean loss: 125.19
 ---- batch: 030 ----
mean loss: 127.90
 ---- batch: 040 ----
mean loss: 132.39
train mean loss: 126.81
epoch train time: 0:00:01.783851
elapsed time: 0:04:51.350121
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:13:25.376072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.55
 ---- batch: 020 ----
mean loss: 125.84
 ---- batch: 030 ----
mean loss: 128.08
 ---- batch: 040 ----
mean loss: 125.00
train mean loss: 127.09
epoch train time: 0:00:01.677379
elapsed time: 0:04:53.027688
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:13:27.053597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.18
 ---- batch: 020 ----
mean loss: 124.16
 ---- batch: 030 ----
mean loss: 127.03
 ---- batch: 040 ----
mean loss: 125.89
train mean loss: 125.67
epoch train time: 0:00:01.786454
elapsed time: 0:04:54.814281
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:13:28.840191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.43
 ---- batch: 020 ----
mean loss: 124.76
 ---- batch: 030 ----
mean loss: 123.82
 ---- batch: 040 ----
mean loss: 119.57
train mean loss: 123.60
epoch train time: 0:00:01.681023
elapsed time: 0:04:56.495464
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:13:30.521373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.34
 ---- batch: 020 ----
mean loss: 127.85
 ---- batch: 030 ----
mean loss: 124.90
 ---- batch: 040 ----
mean loss: 130.26
train mean loss: 127.29
epoch train time: 0:00:01.794115
elapsed time: 0:04:58.289718
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:13:32.315624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.29
 ---- batch: 020 ----
mean loss: 123.20
 ---- batch: 030 ----
mean loss: 123.95
 ---- batch: 040 ----
mean loss: 122.03
train mean loss: 125.08
epoch train time: 0:00:01.689079
elapsed time: 0:04:59.978948
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:13:34.004877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.13
 ---- batch: 020 ----
mean loss: 125.48
 ---- batch: 030 ----
mean loss: 129.36
 ---- batch: 040 ----
mean loss: 123.75
train mean loss: 125.46
epoch train time: 0:00:01.787690
elapsed time: 0:05:01.766916
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:13:35.792858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.73
 ---- batch: 020 ----
mean loss: 134.79
 ---- batch: 030 ----
mean loss: 129.34
 ---- batch: 040 ----
mean loss: 128.08
train mean loss: 130.77
epoch train time: 0:00:01.680576
elapsed time: 0:05:03.447665
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:13:37.473572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.21
 ---- batch: 020 ----
mean loss: 119.31
 ---- batch: 030 ----
mean loss: 127.87
 ---- batch: 040 ----
mean loss: 127.99
train mean loss: 125.42
epoch train time: 0:00:01.777903
elapsed time: 0:05:05.225699
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:13:39.251606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.19
 ---- batch: 020 ----
mean loss: 122.67
 ---- batch: 030 ----
mean loss: 128.96
 ---- batch: 040 ----
mean loss: 128.45
train mean loss: 126.23
epoch train time: 0:00:01.678158
elapsed time: 0:05:06.904046
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:13:40.929955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.50
 ---- batch: 020 ----
mean loss: 131.57
 ---- batch: 030 ----
mean loss: 131.12
 ---- batch: 040 ----
mean loss: 132.25
train mean loss: 129.97
epoch train time: 0:00:01.776535
elapsed time: 0:05:08.680729
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:13:42.706646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.03
 ---- batch: 020 ----
mean loss: 129.58
 ---- batch: 030 ----
mean loss: 126.99
 ---- batch: 040 ----
mean loss: 127.53
train mean loss: 128.73
epoch train time: 0:00:01.692620
elapsed time: 0:05:10.373518
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:13:44.399428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.35
 ---- batch: 020 ----
mean loss: 128.75
 ---- batch: 030 ----
mean loss: 124.06
 ---- batch: 040 ----
mean loss: 128.37
train mean loss: 127.22
epoch train time: 0:00:01.677991
elapsed time: 0:05:12.051639
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:13:46.077546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.80
 ---- batch: 020 ----
mean loss: 126.39
 ---- batch: 030 ----
mean loss: 127.29
 ---- batch: 040 ----
mean loss: 128.85
train mean loss: 126.77
epoch train time: 0:00:01.781941
elapsed time: 0:05:13.833732
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:13:47.859634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.04
 ---- batch: 020 ----
mean loss: 127.92
 ---- batch: 030 ----
mean loss: 130.09
 ---- batch: 040 ----
mean loss: 124.47
train mean loss: 126.81
epoch train time: 0:00:01.680472
elapsed time: 0:05:15.514338
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:13:49.540256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.26
 ---- batch: 020 ----
mean loss: 126.60
 ---- batch: 030 ----
mean loss: 125.82
 ---- batch: 040 ----
mean loss: 125.86
train mean loss: 125.36
epoch train time: 0:00:01.785470
elapsed time: 0:05:17.299995
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:13:51.325899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.14
 ---- batch: 020 ----
mean loss: 123.10
 ---- batch: 030 ----
mean loss: 127.45
 ---- batch: 040 ----
mean loss: 127.97
train mean loss: 125.77
epoch train time: 0:00:01.677851
elapsed time: 0:05:18.977982
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:13:53.003905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.52
 ---- batch: 020 ----
mean loss: 124.03
 ---- batch: 030 ----
mean loss: 120.40
 ---- batch: 040 ----
mean loss: 126.31
train mean loss: 124.79
epoch train time: 0:00:01.784969
elapsed time: 0:05:20.763125
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:13:54.789048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.65
 ---- batch: 020 ----
mean loss: 123.03
 ---- batch: 030 ----
mean loss: 124.70
 ---- batch: 040 ----
mean loss: 123.54
train mean loss: 124.70
epoch train time: 0:00:01.674448
elapsed time: 0:05:22.437723
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:13:56.463627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.90
 ---- batch: 020 ----
mean loss: 125.49
 ---- batch: 030 ----
mean loss: 127.37
 ---- batch: 040 ----
mean loss: 121.44
train mean loss: 123.51
epoch train time: 0:00:01.667519
elapsed time: 0:05:24.105368
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:13:58.131295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.53
 ---- batch: 020 ----
mean loss: 122.62
 ---- batch: 030 ----
mean loss: 127.72
 ---- batch: 040 ----
mean loss: 126.86
train mean loss: 124.40
epoch train time: 0:00:01.784915
elapsed time: 0:05:25.890430
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:13:59.916379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.21
 ---- batch: 020 ----
mean loss: 122.61
 ---- batch: 030 ----
mean loss: 121.77
 ---- batch: 040 ----
mean loss: 125.55
train mean loss: 123.10
epoch train time: 0:00:01.677993
elapsed time: 0:05:27.568604
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:14:01.594510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.96
 ---- batch: 020 ----
mean loss: 127.17
 ---- batch: 030 ----
mean loss: 120.66
 ---- batch: 040 ----
mean loss: 129.05
train mean loss: 125.65
epoch train time: 0:00:01.800366
elapsed time: 0:05:29.369109
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:14:03.395014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.06
 ---- batch: 020 ----
mean loss: 128.11
 ---- batch: 030 ----
mean loss: 125.21
 ---- batch: 040 ----
mean loss: 125.23
train mean loss: 124.71
epoch train time: 0:00:01.676007
elapsed time: 0:05:31.045272
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:14:05.071214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.98
 ---- batch: 020 ----
mean loss: 120.53
 ---- batch: 030 ----
mean loss: 123.83
 ---- batch: 040 ----
mean loss: 122.84
train mean loss: 124.59
epoch train time: 0:00:01.778203
elapsed time: 0:05:32.823652
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:14:06.849559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.71
 ---- batch: 020 ----
mean loss: 120.09
 ---- batch: 030 ----
mean loss: 125.10
 ---- batch: 040 ----
mean loss: 126.07
train mean loss: 124.48
epoch train time: 0:00:01.685015
elapsed time: 0:05:34.508826
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:14:08.534734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.06
 ---- batch: 020 ----
mean loss: 125.73
 ---- batch: 030 ----
mean loss: 123.54
 ---- batch: 040 ----
mean loss: 120.55
train mean loss: 121.92
epoch train time: 0:00:01.676664
elapsed time: 0:05:36.185629
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:14:10.211540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.33
 ---- batch: 020 ----
mean loss: 120.15
 ---- batch: 030 ----
mean loss: 129.04
 ---- batch: 040 ----
mean loss: 129.55
train mean loss: 125.13
epoch train time: 0:00:01.784458
elapsed time: 0:05:37.970246
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:14:11.996155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.94
 ---- batch: 020 ----
mean loss: 129.53
 ---- batch: 030 ----
mean loss: 122.99
 ---- batch: 040 ----
mean loss: 129.44
train mean loss: 126.66
epoch train time: 0:00:01.686546
elapsed time: 0:05:39.656944
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:14:13.682853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.21
 ---- batch: 020 ----
mean loss: 126.23
 ---- batch: 030 ----
mean loss: 121.99
 ---- batch: 040 ----
mean loss: 127.66
train mean loss: 126.32
epoch train time: 0:00:01.794100
elapsed time: 0:05:41.451177
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:14:15.477097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.86
 ---- batch: 020 ----
mean loss: 125.27
 ---- batch: 030 ----
mean loss: 122.23
 ---- batch: 040 ----
mean loss: 121.56
train mean loss: 122.83
epoch train time: 0:00:01.678449
elapsed time: 0:05:43.129773
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:14:17.155678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.57
 ---- batch: 020 ----
mean loss: 126.40
 ---- batch: 030 ----
mean loss: 125.60
 ---- batch: 040 ----
mean loss: 126.82
train mean loss: 125.64
epoch train time: 0:00:01.682039
elapsed time: 0:05:44.811965
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:14:18.837871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.33
 ---- batch: 020 ----
mean loss: 125.24
 ---- batch: 030 ----
mean loss: 122.78
 ---- batch: 040 ----
mean loss: 126.47
train mean loss: 123.75
epoch train time: 0:00:01.781926
elapsed time: 0:05:46.594022
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:14:20.619928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.94
 ---- batch: 020 ----
mean loss: 126.83
 ---- batch: 030 ----
mean loss: 120.42
 ---- batch: 040 ----
mean loss: 123.30
train mean loss: 123.25
epoch train time: 0:00:01.680867
elapsed time: 0:05:48.275054
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:14:22.300962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.31
 ---- batch: 020 ----
mean loss: 129.15
 ---- batch: 030 ----
mean loss: 127.51
 ---- batch: 040 ----
mean loss: 124.04
train mean loss: 125.51
epoch train time: 0:00:01.780458
elapsed time: 0:05:50.055647
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:14:24.081552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.53
 ---- batch: 020 ----
mean loss: 121.45
 ---- batch: 030 ----
mean loss: 121.60
 ---- batch: 040 ----
mean loss: 120.95
train mean loss: 121.44
epoch train time: 0:00:01.680167
elapsed time: 0:05:51.735954
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:14:25.761863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.98
 ---- batch: 020 ----
mean loss: 126.09
 ---- batch: 030 ----
mean loss: 120.26
 ---- batch: 040 ----
mean loss: 121.89
train mean loss: 122.64
epoch train time: 0:00:01.756252
elapsed time: 0:05:53.492343
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:14:27.518249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.00
 ---- batch: 020 ----
mean loss: 121.53
 ---- batch: 030 ----
mean loss: 125.28
 ---- batch: 040 ----
mean loss: 125.55
train mean loss: 124.47
epoch train time: 0:00:01.711732
elapsed time: 0:05:55.204228
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:14:29.230136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.17
 ---- batch: 020 ----
mean loss: 122.39
 ---- batch: 030 ----
mean loss: 119.70
 ---- batch: 040 ----
mean loss: 118.89
train mean loss: 121.64
epoch train time: 0:00:01.680904
elapsed time: 0:05:56.885270
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:14:30.911177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.01
 ---- batch: 020 ----
mean loss: 131.77
 ---- batch: 030 ----
mean loss: 129.76
 ---- batch: 040 ----
mean loss: 125.72
train mean loss: 127.26
epoch train time: 0:00:01.788405
elapsed time: 0:05:58.673812
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:14:32.699718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.28
 ---- batch: 020 ----
mean loss: 123.81
 ---- batch: 030 ----
mean loss: 130.03
 ---- batch: 040 ----
mean loss: 124.67
train mean loss: 125.90
epoch train time: 0:00:01.682667
elapsed time: 0:06:00.356630
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:14:34.382551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.86
 ---- batch: 020 ----
mean loss: 121.29
 ---- batch: 030 ----
mean loss: 118.35
 ---- batch: 040 ----
mean loss: 121.13
train mean loss: 120.83
epoch train time: 0:00:01.780563
elapsed time: 0:06:02.137345
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:14:36.163253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.57
 ---- batch: 020 ----
mean loss: 125.44
 ---- batch: 030 ----
mean loss: 116.18
 ---- batch: 040 ----
mean loss: 118.45
train mean loss: 119.84
epoch train time: 0:00:01.685710
elapsed time: 0:06:03.823195
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:14:37.849101
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.02
 ---- batch: 020 ----
mean loss: 123.84
 ---- batch: 030 ----
mean loss: 120.91
 ---- batch: 040 ----
mean loss: 120.60
train mean loss: 121.18
epoch train time: 0:00:01.670804
elapsed time: 0:06:05.494135
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:14:39.520043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.76
 ---- batch: 020 ----
mean loss: 120.28
 ---- batch: 030 ----
mean loss: 118.10
 ---- batch: 040 ----
mean loss: 121.46
train mean loss: 119.56
epoch train time: 0:00:01.795119
elapsed time: 0:06:07.289383
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:14:41.315289
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.75
 ---- batch: 020 ----
mean loss: 119.75
 ---- batch: 030 ----
mean loss: 118.64
 ---- batch: 040 ----
mean loss: 124.42
train mean loss: 120.34
epoch train time: 0:00:01.682979
elapsed time: 0:06:08.972501
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:14:42.998409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.00
 ---- batch: 020 ----
mean loss: 116.83
 ---- batch: 030 ----
mean loss: 118.87
 ---- batch: 040 ----
mean loss: 117.43
train mean loss: 118.93
epoch train time: 0:00:01.781628
elapsed time: 0:06:10.754267
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:14:44.780173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.61
 ---- batch: 020 ----
mean loss: 119.60
 ---- batch: 030 ----
mean loss: 118.74
 ---- batch: 040 ----
mean loss: 118.60
train mean loss: 119.87
epoch train time: 0:00:01.679452
elapsed time: 0:06:12.433863
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:14:46.459776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.50
 ---- batch: 020 ----
mean loss: 122.77
 ---- batch: 030 ----
mean loss: 120.06
 ---- batch: 040 ----
mean loss: 120.82
train mean loss: 121.18
epoch train time: 0:00:01.776951
elapsed time: 0:06:14.210963
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:14:48.236889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.84
 ---- batch: 020 ----
mean loss: 122.12
 ---- batch: 030 ----
mean loss: 118.46
 ---- batch: 040 ----
mean loss: 125.23
train mean loss: 120.78
epoch train time: 0:00:01.686633
elapsed time: 0:06:15.897761
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:14:49.923670
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.78
 ---- batch: 020 ----
mean loss: 118.83
 ---- batch: 030 ----
mean loss: 119.45
 ---- batch: 040 ----
mean loss: 117.78
train mean loss: 119.48
epoch train time: 0:00:01.668372
elapsed time: 0:06:17.566273
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:14:51.592182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.21
 ---- batch: 020 ----
mean loss: 121.69
 ---- batch: 030 ----
mean loss: 119.22
 ---- batch: 040 ----
mean loss: 121.00
train mean loss: 120.05
epoch train time: 0:00:01.795058
elapsed time: 0:06:19.361469
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:14:53.387377
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.03
 ---- batch: 020 ----
mean loss: 120.24
 ---- batch: 030 ----
mean loss: 120.30
 ---- batch: 040 ----
mean loss: 117.65
train mean loss: 120.68
epoch train time: 0:00:01.679080
elapsed time: 0:06:21.040708
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:14:55.066618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.36
 ---- batch: 020 ----
mean loss: 118.35
 ---- batch: 030 ----
mean loss: 124.03
 ---- batch: 040 ----
mean loss: 123.39
train mean loss: 121.26
epoch train time: 0:00:01.772247
elapsed time: 0:06:22.813095
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:14:56.839018
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.83
 ---- batch: 020 ----
mean loss: 120.49
 ---- batch: 030 ----
mean loss: 124.36
 ---- batch: 040 ----
mean loss: 119.88
train mean loss: 120.63
epoch train time: 0:00:01.684778
elapsed time: 0:06:24.498087
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:14:58.524012
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.08
 ---- batch: 020 ----
mean loss: 122.23
 ---- batch: 030 ----
mean loss: 119.02
 ---- batch: 040 ----
mean loss: 117.99
train mean loss: 119.93
epoch train time: 0:00:01.671571
elapsed time: 0:06:26.169821
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:15:00.195771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.20
 ---- batch: 020 ----
mean loss: 119.03
 ---- batch: 030 ----
mean loss: 117.98
 ---- batch: 040 ----
mean loss: 115.65
train mean loss: 118.81
epoch train time: 0:00:01.790639
elapsed time: 0:06:27.960648
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:15:01.986560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.73
 ---- batch: 020 ----
mean loss: 119.63
 ---- batch: 030 ----
mean loss: 119.82
 ---- batch: 040 ----
mean loss: 120.89
train mean loss: 120.20
epoch train time: 0:00:01.674926
elapsed time: 0:06:29.635726
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:15:03.661634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.28
 ---- batch: 020 ----
mean loss: 123.66
 ---- batch: 030 ----
mean loss: 120.21
 ---- batch: 040 ----
mean loss: 118.57
train mean loss: 120.65
epoch train time: 0:00:01.784225
elapsed time: 0:06:31.420081
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:15:05.445986
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.21
 ---- batch: 020 ----
mean loss: 119.18
 ---- batch: 030 ----
mean loss: 117.32
 ---- batch: 040 ----
mean loss: 121.29
train mean loss: 120.02
epoch train time: 0:00:01.678856
elapsed time: 0:06:33.099084
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:15:07.124993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.02
 ---- batch: 020 ----
mean loss: 118.15
 ---- batch: 030 ----
mean loss: 122.18
 ---- batch: 040 ----
mean loss: 120.02
train mean loss: 119.07
epoch train time: 0:00:01.774532
elapsed time: 0:06:34.873755
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:15:08.899662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.82
 ---- batch: 020 ----
mean loss: 123.75
 ---- batch: 030 ----
mean loss: 114.82
 ---- batch: 040 ----
mean loss: 118.99
train mean loss: 119.42
epoch train time: 0:00:01.691769
elapsed time: 0:06:36.565659
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:15:10.591565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.92
 ---- batch: 020 ----
mean loss: 118.32
 ---- batch: 030 ----
mean loss: 119.45
 ---- batch: 040 ----
mean loss: 115.56
train mean loss: 118.77
epoch train time: 0:00:01.676237
elapsed time: 0:06:38.242032
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:15:12.267941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.55
 ---- batch: 020 ----
mean loss: 118.75
 ---- batch: 030 ----
mean loss: 119.87
 ---- batch: 040 ----
mean loss: 118.50
train mean loss: 119.07
epoch train time: 0:00:01.792165
elapsed time: 0:06:40.034337
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:15:14.060246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.81
 ---- batch: 020 ----
mean loss: 122.32
 ---- batch: 030 ----
mean loss: 118.72
 ---- batch: 040 ----
mean loss: 118.59
train mean loss: 120.17
epoch train time: 0:00:01.686088
elapsed time: 0:06:41.720569
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:15:15.746479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.64
 ---- batch: 020 ----
mean loss: 119.46
 ---- batch: 030 ----
mean loss: 119.27
 ---- batch: 040 ----
mean loss: 122.42
train mean loss: 119.70
epoch train time: 0:00:01.780494
elapsed time: 0:06:43.501197
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:15:17.527117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.73
 ---- batch: 020 ----
mean loss: 120.59
 ---- batch: 030 ----
mean loss: 115.63
 ---- batch: 040 ----
mean loss: 123.87
train mean loss: 120.73
epoch train time: 0:00:01.672476
elapsed time: 0:06:45.173825
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:15:19.199732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.38
 ---- batch: 020 ----
mean loss: 120.91
 ---- batch: 030 ----
mean loss: 122.77
 ---- batch: 040 ----
mean loss: 116.10
train mean loss: 119.78
epoch train time: 0:00:01.667652
elapsed time: 0:06:46.841614
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:15:20.867520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.77
 ---- batch: 020 ----
mean loss: 115.80
 ---- batch: 030 ----
mean loss: 118.28
 ---- batch: 040 ----
mean loss: 121.32
train mean loss: 119.00
epoch train time: 0:00:01.781517
elapsed time: 0:06:48.623267
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:15:22.649180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.62
 ---- batch: 020 ----
mean loss: 121.05
 ---- batch: 030 ----
mean loss: 116.21
 ---- batch: 040 ----
mean loss: 122.71
train mean loss: 120.18
epoch train time: 0:00:01.675898
elapsed time: 0:06:50.299311
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:15:24.325235
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.99
 ---- batch: 020 ----
mean loss: 119.07
 ---- batch: 030 ----
mean loss: 119.51
 ---- batch: 040 ----
mean loss: 117.40
train mean loss: 117.53
epoch train time: 0:00:01.785253
elapsed time: 0:06:52.084726
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:15:26.110632
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.36
 ---- batch: 020 ----
mean loss: 120.59
 ---- batch: 030 ----
mean loss: 121.27
 ---- batch: 040 ----
mean loss: 120.19
train mean loss: 119.49
epoch train time: 0:00:01.683061
elapsed time: 0:06:53.767977
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:15:27.793904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.79
 ---- batch: 020 ----
mean loss: 124.07
 ---- batch: 030 ----
mean loss: 119.05
 ---- batch: 040 ----
mean loss: 117.33
train mean loss: 119.26
epoch train time: 0:00:01.776769
elapsed time: 0:06:55.544922
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:15:29.570829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.04
 ---- batch: 020 ----
mean loss: 119.19
 ---- batch: 030 ----
mean loss: 116.49
 ---- batch: 040 ----
mean loss: 121.67
train mean loss: 119.16
epoch train time: 0:00:01.677490
elapsed time: 0:06:57.222592
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:15:31.248491
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.92
 ---- batch: 020 ----
mean loss: 120.27
 ---- batch: 030 ----
mean loss: 118.14
 ---- batch: 040 ----
mean loss: 119.09
train mean loss: 119.34
epoch train time: 0:00:01.674371
elapsed time: 0:06:58.897100
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:15:32.923011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.33
 ---- batch: 020 ----
mean loss: 118.53
 ---- batch: 030 ----
mean loss: 115.93
 ---- batch: 040 ----
mean loss: 119.74
train mean loss: 118.49
epoch train time: 0:00:01.790011
elapsed time: 0:07:00.687252
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:15:34.713157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.00
 ---- batch: 020 ----
mean loss: 117.66
 ---- batch: 030 ----
mean loss: 120.36
 ---- batch: 040 ----
mean loss: 118.84
train mean loss: 119.29
epoch train time: 0:00:01.683158
elapsed time: 0:07:02.370575
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:15:36.396488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.99
 ---- batch: 020 ----
mean loss: 118.24
 ---- batch: 030 ----
mean loss: 118.53
 ---- batch: 040 ----
mean loss: 120.64
train mean loss: 121.07
epoch train time: 0:00:01.781123
elapsed time: 0:07:04.151853
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:15:38.177810
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.05
 ---- batch: 020 ----
mean loss: 118.11
 ---- batch: 030 ----
mean loss: 116.88
 ---- batch: 040 ----
mean loss: 120.06
train mean loss: 118.39
epoch train time: 0:00:01.683901
elapsed time: 0:07:05.835958
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:15:39.861864
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.86
 ---- batch: 020 ----
mean loss: 119.59
 ---- batch: 030 ----
mean loss: 120.13
 ---- batch: 040 ----
mean loss: 122.72
train mean loss: 120.56
epoch train time: 0:00:01.665651
elapsed time: 0:07:07.501744
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:15:41.527804
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.65
 ---- batch: 020 ----
mean loss: 119.85
 ---- batch: 030 ----
mean loss: 115.60
 ---- batch: 040 ----
mean loss: 121.12
train mean loss: 119.97
epoch train time: 0:00:01.786967
elapsed time: 0:07:09.288997
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:15:43.314906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.16
 ---- batch: 020 ----
mean loss: 120.39
 ---- batch: 030 ----
mean loss: 119.91
 ---- batch: 040 ----
mean loss: 121.56
train mean loss: 119.85
epoch train time: 0:00:01.678843
elapsed time: 0:07:10.967994
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:15:44.993903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.90
 ---- batch: 020 ----
mean loss: 121.21
 ---- batch: 030 ----
mean loss: 122.92
 ---- batch: 040 ----
mean loss: 119.84
train mean loss: 119.74
epoch train time: 0:00:01.781757
elapsed time: 0:07:12.749882
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:15:46.775787
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.40
 ---- batch: 020 ----
mean loss: 118.58
 ---- batch: 030 ----
mean loss: 121.14
 ---- batch: 040 ----
mean loss: 117.67
train mean loss: 119.03
epoch train time: 0:00:01.683037
elapsed time: 0:07:14.433055
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:15:48.458965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.98
 ---- batch: 020 ----
mean loss: 118.49
 ---- batch: 030 ----
mean loss: 120.15
 ---- batch: 040 ----
mean loss: 120.22
train mean loss: 118.87
epoch train time: 0:00:01.773570
elapsed time: 0:07:16.206790
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:15:50.232695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.60
 ---- batch: 020 ----
mean loss: 120.41
 ---- batch: 030 ----
mean loss: 119.70
 ---- batch: 040 ----
mean loss: 114.99
train mean loss: 117.59
epoch train time: 0:00:01.687122
elapsed time: 0:07:17.894073
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:15:51.919980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.72
 ---- batch: 020 ----
mean loss: 117.69
 ---- batch: 030 ----
mean loss: 121.05
 ---- batch: 040 ----
mean loss: 118.88
train mean loss: 119.18
epoch train time: 0:00:01.674438
elapsed time: 0:07:19.568637
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:15:53.594540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.62
 ---- batch: 020 ----
mean loss: 120.02
 ---- batch: 030 ----
mean loss: 120.84
 ---- batch: 040 ----
mean loss: 119.57
train mean loss: 120.15
epoch train time: 0:00:01.795706
elapsed time: 0:07:21.364473
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:15:55.390381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.58
 ---- batch: 020 ----
mean loss: 121.47
 ---- batch: 030 ----
mean loss: 122.66
 ---- batch: 040 ----
mean loss: 117.27
train mean loss: 120.55
epoch train time: 0:00:01.679522
elapsed time: 0:07:23.044150
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:15:57.070060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.53
 ---- batch: 020 ----
mean loss: 118.82
 ---- batch: 030 ----
mean loss: 120.21
 ---- batch: 040 ----
mean loss: 117.69
train mean loss: 119.98
epoch train time: 0:00:01.780305
elapsed time: 0:07:24.827566
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_8/checkpoint.pth.tar
**** end time: 2019-09-20 23:15:58.853440 ****
