Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 21104
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 20:32:13.441056 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 20:32:13.451798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4110.28
 ---- batch: 020 ----
mean loss: 3776.34
 ---- batch: 030 ----
mean loss: 3566.64
 ---- batch: 040 ----
mean loss: 3285.60
 ---- batch: 050 ----
mean loss: 3022.01
 ---- batch: 060 ----
mean loss: 2936.16
 ---- batch: 070 ----
mean loss: 2732.93
 ---- batch: 080 ----
mean loss: 2655.06
 ---- batch: 090 ----
mean loss: 2549.56
train mean loss: 3137.27
epoch train time: 0:00:35.435612
elapsed time: 0:00:35.453552
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 20:32:48.894649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2374.58
 ---- batch: 020 ----
mean loss: 2381.44
 ---- batch: 030 ----
mean loss: 2301.43
 ---- batch: 040 ----
mean loss: 2253.03
 ---- batch: 050 ----
mean loss: 2160.09
 ---- batch: 060 ----
mean loss: 2154.83
 ---- batch: 070 ----
mean loss: 2118.42
 ---- batch: 080 ----
mean loss: 2065.50
 ---- batch: 090 ----
mean loss: 2054.39
train mean loss: 2195.35
epoch train time: 0:00:01.677587
elapsed time: 0:00:37.131568
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 20:32:50.572923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1975.86
 ---- batch: 020 ----
mean loss: 1937.70
 ---- batch: 030 ----
mean loss: 1932.60
 ---- batch: 040 ----
mean loss: 1910.66
 ---- batch: 050 ----
mean loss: 1894.82
 ---- batch: 060 ----
mean loss: 1855.91
 ---- batch: 070 ----
mean loss: 1858.47
 ---- batch: 080 ----
mean loss: 1810.10
 ---- batch: 090 ----
mean loss: 1767.82
train mean loss: 1877.35
epoch train time: 0:00:01.665395
elapsed time: 0:00:38.797581
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 20:32:52.238964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1750.32
 ---- batch: 020 ----
mean loss: 1699.92
 ---- batch: 030 ----
mean loss: 1673.35
 ---- batch: 040 ----
mean loss: 1702.52
 ---- batch: 050 ----
mean loss: 1654.49
 ---- batch: 060 ----
mean loss: 1650.51
 ---- batch: 070 ----
mean loss: 1609.11
 ---- batch: 080 ----
mean loss: 1596.23
 ---- batch: 090 ----
mean loss: 1594.35
train mean loss: 1653.08
epoch train time: 0:00:01.683084
elapsed time: 0:00:40.481313
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 20:32:53.922686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1531.27
 ---- batch: 020 ----
mean loss: 1525.36
 ---- batch: 030 ----
mean loss: 1499.87
 ---- batch: 040 ----
mean loss: 1484.51
 ---- batch: 050 ----
mean loss: 1493.01
 ---- batch: 060 ----
mean loss: 1441.11
 ---- batch: 070 ----
mean loss: 1438.88
 ---- batch: 080 ----
mean loss: 1455.77
 ---- batch: 090 ----
mean loss: 1413.97
train mean loss: 1470.20
epoch train time: 0:00:01.699868
elapsed time: 0:00:42.181813
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 20:32:55.623149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1370.56
 ---- batch: 020 ----
mean loss: 1358.73
 ---- batch: 030 ----
mean loss: 1374.19
 ---- batch: 040 ----
mean loss: 1337.35
 ---- batch: 050 ----
mean loss: 1363.28
 ---- batch: 060 ----
mean loss: 1292.09
 ---- batch: 070 ----
mean loss: 1320.14
 ---- batch: 080 ----
mean loss: 1322.63
 ---- batch: 090 ----
mean loss: 1271.24
train mean loss: 1329.81
epoch train time: 0:00:01.691833
elapsed time: 0:00:43.874261
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 20:32:57.315620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1258.43
 ---- batch: 020 ----
mean loss: 1268.56
 ---- batch: 030 ----
mean loss: 1216.17
 ---- batch: 040 ----
mean loss: 1245.80
 ---- batch: 050 ----
mean loss: 1228.87
 ---- batch: 060 ----
mean loss: 1220.42
 ---- batch: 070 ----
mean loss: 1204.22
 ---- batch: 080 ----
mean loss: 1201.61
 ---- batch: 090 ----
mean loss: 1184.43
train mean loss: 1220.77
epoch train time: 0:00:01.690017
elapsed time: 0:00:45.564891
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 20:32:59.006269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1152.04
 ---- batch: 020 ----
mean loss: 1167.62
 ---- batch: 030 ----
mean loss: 1178.04
 ---- batch: 040 ----
mean loss: 1122.06
 ---- batch: 050 ----
mean loss: 1138.92
 ---- batch: 060 ----
mean loss: 1133.87
 ---- batch: 070 ----
mean loss: 1101.62
 ---- batch: 080 ----
mean loss: 1116.45
 ---- batch: 090 ----
mean loss: 1091.79
train mean loss: 1129.66
epoch train time: 0:00:01.677865
elapsed time: 0:00:47.243451
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 20:33:00.684816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1091.01
 ---- batch: 020 ----
mean loss: 1064.83
 ---- batch: 030 ----
mean loss: 1068.79
 ---- batch: 040 ----
mean loss: 1075.91
 ---- batch: 050 ----
mean loss: 1071.38
 ---- batch: 060 ----
mean loss: 1037.47
 ---- batch: 070 ----
mean loss: 1055.08
 ---- batch: 080 ----
mean loss: 1027.58
 ---- batch: 090 ----
mean loss: 1056.78
train mean loss: 1059.83
epoch train time: 0:00:01.710895
elapsed time: 0:00:48.954958
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 20:33:02.396323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.08
 ---- batch: 020 ----
mean loss: 1023.83
 ---- batch: 030 ----
mean loss: 1017.68
 ---- batch: 040 ----
mean loss: 996.00
 ---- batch: 050 ----
mean loss: 1022.10
 ---- batch: 060 ----
mean loss: 1010.21
 ---- batch: 070 ----
mean loss: 1011.97
 ---- batch: 080 ----
mean loss: 988.80
 ---- batch: 090 ----
mean loss: 994.58
train mean loss: 1008.10
epoch train time: 0:00:01.692645
elapsed time: 0:00:50.648304
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 20:33:04.089685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.96
 ---- batch: 020 ----
mean loss: 989.53
 ---- batch: 030 ----
mean loss: 983.20
 ---- batch: 040 ----
mean loss: 972.37
 ---- batch: 050 ----
mean loss: 959.39
 ---- batch: 060 ----
mean loss: 977.14
 ---- batch: 070 ----
mean loss: 967.02
 ---- batch: 080 ----
mean loss: 941.25
 ---- batch: 090 ----
mean loss: 968.05
train mean loss: 968.68
epoch train time: 0:00:01.681665
elapsed time: 0:00:52.330628
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 20:33:05.772011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.64
 ---- batch: 020 ----
mean loss: 946.55
 ---- batch: 030 ----
mean loss: 958.51
 ---- batch: 040 ----
mean loss: 955.83
 ---- batch: 050 ----
mean loss: 950.86
 ---- batch: 060 ----
mean loss: 943.30
 ---- batch: 070 ----
mean loss: 955.00
 ---- batch: 080 ----
mean loss: 928.41
 ---- batch: 090 ----
mean loss: 919.49
train mean loss: 940.33
epoch train time: 0:00:01.673033
elapsed time: 0:00:54.004282
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 20:33:07.445524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.45
 ---- batch: 020 ----
mean loss: 919.93
 ---- batch: 030 ----
mean loss: 934.54
 ---- batch: 040 ----
mean loss: 912.99
 ---- batch: 050 ----
mean loss: 937.51
 ---- batch: 060 ----
mean loss: 921.58
 ---- batch: 070 ----
mean loss: 901.67
 ---- batch: 080 ----
mean loss: 915.41
 ---- batch: 090 ----
mean loss: 921.37
train mean loss: 921.05
epoch train time: 0:00:01.702294
elapsed time: 0:00:55.707109
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 20:33:09.148471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 917.31
 ---- batch: 020 ----
mean loss: 911.79
 ---- batch: 030 ----
mean loss: 905.26
 ---- batch: 040 ----
mean loss: 888.72
 ---- batch: 050 ----
mean loss: 907.91
 ---- batch: 060 ----
mean loss: 905.10
 ---- batch: 070 ----
mean loss: 927.12
 ---- batch: 080 ----
mean loss: 903.26
 ---- batch: 090 ----
mean loss: 898.44
train mean loss: 906.68
epoch train time: 0:00:01.691179
elapsed time: 0:00:57.398926
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 20:33:10.840271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.86
 ---- batch: 020 ----
mean loss: 903.60
 ---- batch: 030 ----
mean loss: 902.89
 ---- batch: 040 ----
mean loss: 887.77
 ---- batch: 050 ----
mean loss: 893.09
 ---- batch: 060 ----
mean loss: 885.70
 ---- batch: 070 ----
mean loss: 894.28
 ---- batch: 080 ----
mean loss: 908.10
 ---- batch: 090 ----
mean loss: 896.29
train mean loss: 899.64
epoch train time: 0:00:01.687320
elapsed time: 0:00:59.086832
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 20:33:12.528191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.35
 ---- batch: 020 ----
mean loss: 894.77
 ---- batch: 030 ----
mean loss: 891.77
 ---- batch: 040 ----
mean loss: 898.06
 ---- batch: 050 ----
mean loss: 893.16
 ---- batch: 060 ----
mean loss: 883.57
 ---- batch: 070 ----
mean loss: 869.72
 ---- batch: 080 ----
mean loss: 892.64
 ---- batch: 090 ----
mean loss: 896.61
train mean loss: 890.65
epoch train time: 0:00:01.672393
elapsed time: 0:01:00.759932
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 20:33:14.201308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 897.44
 ---- batch: 020 ----
mean loss: 871.17
 ---- batch: 030 ----
mean loss: 887.66
 ---- batch: 040 ----
mean loss: 890.38
 ---- batch: 050 ----
mean loss: 867.05
 ---- batch: 060 ----
mean loss: 899.07
 ---- batch: 070 ----
mean loss: 902.58
 ---- batch: 080 ----
mean loss: 905.18
 ---- batch: 090 ----
mean loss: 873.56
train mean loss: 889.03
epoch train time: 0:00:01.677105
elapsed time: 0:01:02.437760
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 20:33:15.879104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.53
 ---- batch: 020 ----
mean loss: 880.17
 ---- batch: 030 ----
mean loss: 901.12
 ---- batch: 040 ----
mean loss: 899.49
 ---- batch: 050 ----
mean loss: 870.50
 ---- batch: 060 ----
mean loss: 870.52
 ---- batch: 070 ----
mean loss: 888.11
 ---- batch: 080 ----
mean loss: 879.86
 ---- batch: 090 ----
mean loss: 882.86
train mean loss: 885.92
epoch train time: 0:00:01.663054
elapsed time: 0:01:04.101469
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 20:33:17.542854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.04
 ---- batch: 020 ----
mean loss: 890.92
 ---- batch: 030 ----
mean loss: 874.95
 ---- batch: 040 ----
mean loss: 891.14
 ---- batch: 050 ----
mean loss: 887.28
 ---- batch: 060 ----
mean loss: 880.41
 ---- batch: 070 ----
mean loss: 864.43
 ---- batch: 080 ----
mean loss: 896.33
 ---- batch: 090 ----
mean loss: 891.28
train mean loss: 884.64
epoch train time: 0:00:01.714863
elapsed time: 0:01:05.816997
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 20:33:19.258362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.40
 ---- batch: 020 ----
mean loss: 893.77
 ---- batch: 030 ----
mean loss: 885.78
 ---- batch: 040 ----
mean loss: 890.21
 ---- batch: 050 ----
mean loss: 871.98
 ---- batch: 060 ----
mean loss: 888.21
 ---- batch: 070 ----
mean loss: 895.08
 ---- batch: 080 ----
mean loss: 870.98
 ---- batch: 090 ----
mean loss: 879.53
train mean loss: 884.25
epoch train time: 0:00:01.709864
elapsed time: 0:01:07.527546
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 20:33:20.968911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.90
 ---- batch: 020 ----
mean loss: 877.48
 ---- batch: 030 ----
mean loss: 872.90
 ---- batch: 040 ----
mean loss: 882.02
 ---- batch: 050 ----
mean loss: 896.12
 ---- batch: 060 ----
mean loss: 877.72
 ---- batch: 070 ----
mean loss: 864.71
 ---- batch: 080 ----
mean loss: 895.83
 ---- batch: 090 ----
mean loss: 886.55
train mean loss: 881.22
epoch train time: 0:00:01.682422
elapsed time: 0:01:09.210583
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 20:33:22.651946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.45
 ---- batch: 020 ----
mean loss: 895.48
 ---- batch: 030 ----
mean loss: 875.87
 ---- batch: 040 ----
mean loss: 863.41
 ---- batch: 050 ----
mean loss: 872.78
 ---- batch: 060 ----
mean loss: 898.28
 ---- batch: 070 ----
mean loss: 890.22
 ---- batch: 080 ----
mean loss: 875.73
 ---- batch: 090 ----
mean loss: 882.25
train mean loss: 882.90
epoch train time: 0:00:01.683947
elapsed time: 0:01:10.895156
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 20:33:24.336336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.04
 ---- batch: 020 ----
mean loss: 874.87
 ---- batch: 030 ----
mean loss: 868.18
 ---- batch: 040 ----
mean loss: 874.71
 ---- batch: 050 ----
mean loss: 885.47
 ---- batch: 060 ----
mean loss: 883.24
 ---- batch: 070 ----
mean loss: 881.01
 ---- batch: 080 ----
mean loss: 883.71
 ---- batch: 090 ----
mean loss: 890.98
train mean loss: 880.46
epoch train time: 0:00:01.719652
elapsed time: 0:01:12.615252
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 20:33:26.056613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.65
 ---- batch: 020 ----
mean loss: 875.99
 ---- batch: 030 ----
mean loss: 881.98
 ---- batch: 040 ----
mean loss: 863.68
 ---- batch: 050 ----
mean loss: 878.53
 ---- batch: 060 ----
mean loss: 872.68
 ---- batch: 070 ----
mean loss: 886.65
 ---- batch: 080 ----
mean loss: 883.96
 ---- batch: 090 ----
mean loss: 874.72
train mean loss: 880.90
epoch train time: 0:00:01.701499
elapsed time: 0:01:14.317437
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 20:33:27.758826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.60
 ---- batch: 020 ----
mean loss: 892.06
 ---- batch: 030 ----
mean loss: 890.36
 ---- batch: 040 ----
mean loss: 882.59
 ---- batch: 050 ----
mean loss: 886.57
 ---- batch: 060 ----
mean loss: 882.58
 ---- batch: 070 ----
mean loss: 879.58
 ---- batch: 080 ----
mean loss: 871.79
 ---- batch: 090 ----
mean loss: 888.41
train mean loss: 880.52
epoch train time: 0:00:01.683708
elapsed time: 0:01:16.001838
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 20:33:29.443227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.61
 ---- batch: 020 ----
mean loss: 874.59
 ---- batch: 030 ----
mean loss: 866.79
 ---- batch: 040 ----
mean loss: 870.55
 ---- batch: 050 ----
mean loss: 867.73
 ---- batch: 060 ----
mean loss: 907.38
 ---- batch: 070 ----
mean loss: 885.16
 ---- batch: 080 ----
mean loss: 882.19
 ---- batch: 090 ----
mean loss: 870.49
train mean loss: 879.03
epoch train time: 0:00:01.695120
elapsed time: 0:01:17.697657
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 20:33:31.139061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.11
 ---- batch: 020 ----
mean loss: 879.17
 ---- batch: 030 ----
mean loss: 875.46
 ---- batch: 040 ----
mean loss: 876.01
 ---- batch: 050 ----
mean loss: 869.92
 ---- batch: 060 ----
mean loss: 874.40
 ---- batch: 070 ----
mean loss: 883.98
 ---- batch: 080 ----
mean loss: 888.50
 ---- batch: 090 ----
mean loss: 893.21
train mean loss: 879.34
epoch train time: 0:00:01.693753
elapsed time: 0:01:19.392051
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 20:33:32.833401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.95
 ---- batch: 020 ----
mean loss: 873.17
 ---- batch: 030 ----
mean loss: 881.89
 ---- batch: 040 ----
mean loss: 887.83
 ---- batch: 050 ----
mean loss: 876.63
 ---- batch: 060 ----
mean loss: 874.00
 ---- batch: 070 ----
mean loss: 870.00
 ---- batch: 080 ----
mean loss: 897.65
 ---- batch: 090 ----
mean loss: 867.95
train mean loss: 879.32
epoch train time: 0:00:01.687135
elapsed time: 0:01:21.079835
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 20:33:34.521201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.38
 ---- batch: 020 ----
mean loss: 879.73
 ---- batch: 030 ----
mean loss: 866.42
 ---- batch: 040 ----
mean loss: 876.51
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 889.40
 ---- batch: 070 ----
mean loss: 897.40
 ---- batch: 080 ----
mean loss: 867.29
 ---- batch: 090 ----
mean loss: 863.60
train mean loss: 880.88
epoch train time: 0:00:01.702101
elapsed time: 0:01:22.782573
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 20:33:36.223923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.31
 ---- batch: 020 ----
mean loss: 890.64
 ---- batch: 030 ----
mean loss: 874.63
 ---- batch: 040 ----
mean loss: 880.16
 ---- batch: 050 ----
mean loss: 881.64
 ---- batch: 060 ----
mean loss: 882.02
 ---- batch: 070 ----
mean loss: 877.34
 ---- batch: 080 ----
mean loss: 874.85
 ---- batch: 090 ----
mean loss: 861.51
train mean loss: 879.28
epoch train time: 0:00:01.671997
elapsed time: 0:01:24.455247
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 20:33:37.896612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.16
 ---- batch: 020 ----
mean loss: 875.98
 ---- batch: 030 ----
mean loss: 873.07
 ---- batch: 040 ----
mean loss: 882.52
 ---- batch: 050 ----
mean loss: 896.85
 ---- batch: 060 ----
mean loss: 870.87
 ---- batch: 070 ----
mean loss: 893.33
 ---- batch: 080 ----
mean loss: 873.39
 ---- batch: 090 ----
mean loss: 865.52
train mean loss: 879.46
epoch train time: 0:00:01.684028
elapsed time: 0:01:26.139950
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 20:33:39.581326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.41
 ---- batch: 020 ----
mean loss: 882.12
 ---- batch: 030 ----
mean loss: 878.23
 ---- batch: 040 ----
mean loss: 874.49
 ---- batch: 050 ----
mean loss: 881.46
 ---- batch: 060 ----
mean loss: 892.20
 ---- batch: 070 ----
mean loss: 865.23
 ---- batch: 080 ----
mean loss: 886.33
 ---- batch: 090 ----
mean loss: 876.37
train mean loss: 879.26
epoch train time: 0:00:01.655381
elapsed time: 0:01:27.795972
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 20:33:41.237334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.90
 ---- batch: 020 ----
mean loss: 878.47
 ---- batch: 030 ----
mean loss: 882.16
 ---- batch: 040 ----
mean loss: 874.33
 ---- batch: 050 ----
mean loss: 872.33
 ---- batch: 060 ----
mean loss: 871.84
 ---- batch: 070 ----
mean loss: 870.67
 ---- batch: 080 ----
mean loss: 878.70
 ---- batch: 090 ----
mean loss: 880.86
train mean loss: 878.98
epoch train time: 0:00:01.680157
elapsed time: 0:01:29.476919
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 20:33:42.918377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.90
 ---- batch: 020 ----
mean loss: 887.59
 ---- batch: 030 ----
mean loss: 868.91
 ---- batch: 040 ----
mean loss: 880.01
 ---- batch: 050 ----
mean loss: 893.86
 ---- batch: 060 ----
mean loss: 882.52
 ---- batch: 070 ----
mean loss: 885.78
 ---- batch: 080 ----
mean loss: 862.20
 ---- batch: 090 ----
mean loss: 875.49
train mean loss: 878.22
epoch train time: 0:00:01.693322
elapsed time: 0:01:31.170942
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 20:33:44.612304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.91
 ---- batch: 020 ----
mean loss: 876.86
 ---- batch: 030 ----
mean loss: 860.59
 ---- batch: 040 ----
mean loss: 881.45
 ---- batch: 050 ----
mean loss: 874.97
 ---- batch: 060 ----
mean loss: 893.61
 ---- batch: 070 ----
mean loss: 884.05
 ---- batch: 080 ----
mean loss: 880.46
 ---- batch: 090 ----
mean loss: 887.95
train mean loss: 879.25
epoch train time: 0:00:01.680286
elapsed time: 0:01:32.851843
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 20:33:46.293196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.54
 ---- batch: 020 ----
mean loss: 873.46
 ---- batch: 030 ----
mean loss: 878.15
 ---- batch: 040 ----
mean loss: 890.47
 ---- batch: 050 ----
mean loss: 889.31
 ---- batch: 060 ----
mean loss: 860.42
 ---- batch: 070 ----
mean loss: 863.66
 ---- batch: 080 ----
mean loss: 868.18
 ---- batch: 090 ----
mean loss: 908.82
train mean loss: 877.84
epoch train time: 0:00:01.677672
elapsed time: 0:01:34.530177
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 20:33:47.971562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.70
 ---- batch: 020 ----
mean loss: 871.50
 ---- batch: 030 ----
mean loss: 884.11
 ---- batch: 040 ----
mean loss: 897.87
 ---- batch: 050 ----
mean loss: 899.85
 ---- batch: 060 ----
mean loss: 874.52
 ---- batch: 070 ----
mean loss: 876.41
 ---- batch: 080 ----
mean loss: 851.75
 ---- batch: 090 ----
mean loss: 871.40
train mean loss: 877.57
epoch train time: 0:00:01.687196
elapsed time: 0:01:36.218024
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 20:33:49.659380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.83
 ---- batch: 020 ----
mean loss: 887.28
 ---- batch: 030 ----
mean loss: 883.13
 ---- batch: 040 ----
mean loss: 885.49
 ---- batch: 050 ----
mean loss: 868.14
 ---- batch: 060 ----
mean loss: 885.16
 ---- batch: 070 ----
mean loss: 870.29
 ---- batch: 080 ----
mean loss: 884.87
 ---- batch: 090 ----
mean loss: 861.99
train mean loss: 878.60
epoch train time: 0:00:01.681647
elapsed time: 0:01:37.900271
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 20:33:51.341653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.49
 ---- batch: 020 ----
mean loss: 879.97
 ---- batch: 030 ----
mean loss: 869.48
 ---- batch: 040 ----
mean loss: 874.62
 ---- batch: 050 ----
mean loss: 864.42
 ---- batch: 060 ----
mean loss: 886.16
 ---- batch: 070 ----
mean loss: 887.70
 ---- batch: 080 ----
mean loss: 869.91
 ---- batch: 090 ----
mean loss: 888.40
train mean loss: 878.97
epoch train time: 0:00:01.656071
elapsed time: 0:01:39.556996
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 20:33:52.998354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.77
 ---- batch: 020 ----
mean loss: 865.78
 ---- batch: 030 ----
mean loss: 871.01
 ---- batch: 040 ----
mean loss: 874.65
 ---- batch: 050 ----
mean loss: 877.36
 ---- batch: 060 ----
mean loss: 875.49
 ---- batch: 070 ----
mean loss: 879.36
 ---- batch: 080 ----
mean loss: 876.29
 ---- batch: 090 ----
mean loss: 883.26
train mean loss: 878.18
epoch train time: 0:00:01.653359
elapsed time: 0:01:41.211009
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 20:33:54.652370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.62
 ---- batch: 020 ----
mean loss: 891.52
 ---- batch: 030 ----
mean loss: 877.07
 ---- batch: 040 ----
mean loss: 868.05
 ---- batch: 050 ----
mean loss: 890.07
 ---- batch: 060 ----
mean loss: 877.85
 ---- batch: 070 ----
mean loss: 877.38
 ---- batch: 080 ----
mean loss: 863.49
 ---- batch: 090 ----
mean loss: 869.83
train mean loss: 876.72
epoch train time: 0:00:01.671590
elapsed time: 0:01:42.883199
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 20:33:56.324624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.47
 ---- batch: 020 ----
mean loss: 883.95
 ---- batch: 030 ----
mean loss: 882.34
 ---- batch: 040 ----
mean loss: 871.54
 ---- batch: 050 ----
mean loss: 861.53
 ---- batch: 060 ----
mean loss: 888.75
 ---- batch: 070 ----
mean loss: 878.79
 ---- batch: 080 ----
mean loss: 875.43
 ---- batch: 090 ----
mean loss: 885.37
train mean loss: 877.44
epoch train time: 0:00:01.684804
elapsed time: 0:01:44.568776
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 20:33:58.010272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.07
 ---- batch: 020 ----
mean loss: 865.83
 ---- batch: 030 ----
mean loss: 878.86
 ---- batch: 040 ----
mean loss: 853.21
 ---- batch: 050 ----
mean loss: 853.01
 ---- batch: 060 ----
mean loss: 877.08
 ---- batch: 070 ----
mean loss: 891.82
 ---- batch: 080 ----
mean loss: 896.15
 ---- batch: 090 ----
mean loss: 902.57
train mean loss: 877.50
epoch train time: 0:00:01.689108
elapsed time: 0:01:46.258784
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 20:33:59.700156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.23
 ---- batch: 020 ----
mean loss: 861.51
 ---- batch: 030 ----
mean loss: 882.85
 ---- batch: 040 ----
mean loss: 890.00
 ---- batch: 050 ----
mean loss: 871.39
 ---- batch: 060 ----
mean loss: 878.76
 ---- batch: 070 ----
mean loss: 874.58
 ---- batch: 080 ----
mean loss: 888.43
 ---- batch: 090 ----
mean loss: 888.49
train mean loss: 877.23
epoch train time: 0:00:01.679406
elapsed time: 0:01:47.938778
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 20:34:01.379951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.65
 ---- batch: 020 ----
mean loss: 894.94
 ---- batch: 030 ----
mean loss: 896.25
 ---- batch: 040 ----
mean loss: 878.45
 ---- batch: 050 ----
mean loss: 856.58
 ---- batch: 060 ----
mean loss: 886.23
 ---- batch: 070 ----
mean loss: 874.25
 ---- batch: 080 ----
mean loss: 875.64
 ---- batch: 090 ----
mean loss: 871.37
train mean loss: 876.53
epoch train time: 0:00:01.664330
elapsed time: 0:01:49.603589
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 20:34:03.044961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.93
 ---- batch: 020 ----
mean loss: 874.10
 ---- batch: 030 ----
mean loss: 874.56
 ---- batch: 040 ----
mean loss: 878.31
 ---- batch: 050 ----
mean loss: 875.54
 ---- batch: 060 ----
mean loss: 863.60
 ---- batch: 070 ----
mean loss: 871.64
 ---- batch: 080 ----
mean loss: 861.46
 ---- batch: 090 ----
mean loss: 830.78
train mean loss: 865.55
epoch train time: 0:00:01.696171
elapsed time: 0:01:51.300412
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 20:34:04.741803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.36
 ---- batch: 020 ----
mean loss: 730.60
 ---- batch: 030 ----
mean loss: 666.74
 ---- batch: 040 ----
mean loss: 596.81
 ---- batch: 050 ----
mean loss: 557.89
 ---- batch: 060 ----
mean loss: 531.05
 ---- batch: 070 ----
mean loss: 482.74
 ---- batch: 080 ----
mean loss: 470.37
 ---- batch: 090 ----
mean loss: 461.28
train mean loss: 577.78
epoch train time: 0:00:01.677755
elapsed time: 0:01:52.978774
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 20:34:06.420234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.30
 ---- batch: 020 ----
mean loss: 427.04
 ---- batch: 030 ----
mean loss: 418.32
 ---- batch: 040 ----
mean loss: 411.06
 ---- batch: 050 ----
mean loss: 420.95
 ---- batch: 060 ----
mean loss: 397.41
 ---- batch: 070 ----
mean loss: 395.01
 ---- batch: 080 ----
mean loss: 383.85
 ---- batch: 090 ----
mean loss: 386.30
train mean loss: 408.67
epoch train time: 0:00:01.673578
elapsed time: 0:01:54.653077
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 20:34:08.094493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.49
 ---- batch: 020 ----
mean loss: 369.96
 ---- batch: 030 ----
mean loss: 371.31
 ---- batch: 040 ----
mean loss: 365.36
 ---- batch: 050 ----
mean loss: 355.07
 ---- batch: 060 ----
mean loss: 365.51
 ---- batch: 070 ----
mean loss: 361.22
 ---- batch: 080 ----
mean loss: 359.08
 ---- batch: 090 ----
mean loss: 340.64
train mean loss: 361.33
epoch train time: 0:00:01.664214
elapsed time: 0:01:56.318029
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 20:34:09.759422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.24
 ---- batch: 020 ----
mean loss: 334.30
 ---- batch: 030 ----
mean loss: 353.20
 ---- batch: 040 ----
mean loss: 337.54
 ---- batch: 050 ----
mean loss: 330.93
 ---- batch: 060 ----
mean loss: 339.71
 ---- batch: 070 ----
mean loss: 317.45
 ---- batch: 080 ----
mean loss: 339.35
 ---- batch: 090 ----
mean loss: 329.88
train mean loss: 335.59
epoch train time: 0:00:01.666587
elapsed time: 0:01:57.985236
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 20:34:11.426591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.87
 ---- batch: 020 ----
mean loss: 317.68
 ---- batch: 030 ----
mean loss: 322.68
 ---- batch: 040 ----
mean loss: 307.54
 ---- batch: 050 ----
mean loss: 309.31
 ---- batch: 060 ----
mean loss: 313.35
 ---- batch: 070 ----
mean loss: 317.95
 ---- batch: 080 ----
mean loss: 307.15
 ---- batch: 090 ----
mean loss: 309.86
train mean loss: 314.88
epoch train time: 0:00:01.696050
elapsed time: 0:01:59.681900
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 20:34:13.123280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.83
 ---- batch: 020 ----
mean loss: 300.55
 ---- batch: 030 ----
mean loss: 306.68
 ---- batch: 040 ----
mean loss: 308.04
 ---- batch: 050 ----
mean loss: 294.04
 ---- batch: 060 ----
mean loss: 302.14
 ---- batch: 070 ----
mean loss: 296.08
 ---- batch: 080 ----
mean loss: 296.91
 ---- batch: 090 ----
mean loss: 304.87
train mean loss: 301.43
epoch train time: 0:00:01.682512
elapsed time: 0:02:01.365077
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 20:34:14.806421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.76
 ---- batch: 020 ----
mean loss: 284.80
 ---- batch: 030 ----
mean loss: 297.01
 ---- batch: 040 ----
mean loss: 289.22
 ---- batch: 050 ----
mean loss: 298.34
 ---- batch: 060 ----
mean loss: 298.36
 ---- batch: 070 ----
mean loss: 285.94
 ---- batch: 080 ----
mean loss: 289.89
 ---- batch: 090 ----
mean loss: 290.97
train mean loss: 292.17
epoch train time: 0:00:01.694044
elapsed time: 0:02:03.059721
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 20:34:16.501132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.90
 ---- batch: 020 ----
mean loss: 279.47
 ---- batch: 030 ----
mean loss: 293.49
 ---- batch: 040 ----
mean loss: 282.28
 ---- batch: 050 ----
mean loss: 281.18
 ---- batch: 060 ----
mean loss: 281.06
 ---- batch: 070 ----
mean loss: 270.76
 ---- batch: 080 ----
mean loss: 291.40
 ---- batch: 090 ----
mean loss: 274.62
train mean loss: 282.27
epoch train time: 0:00:01.687548
elapsed time: 0:02:04.747966
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 20:34:18.189332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.18
 ---- batch: 020 ----
mean loss: 279.14
 ---- batch: 030 ----
mean loss: 286.10
 ---- batch: 040 ----
mean loss: 277.04
 ---- batch: 050 ----
mean loss: 273.34
 ---- batch: 060 ----
mean loss: 283.11
 ---- batch: 070 ----
mean loss: 278.52
 ---- batch: 080 ----
mean loss: 260.05
 ---- batch: 090 ----
mean loss: 274.73
train mean loss: 276.18
epoch train time: 0:00:01.685543
elapsed time: 0:02:06.434198
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 20:34:19.875557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.78
 ---- batch: 020 ----
mean loss: 268.95
 ---- batch: 030 ----
mean loss: 272.46
 ---- batch: 040 ----
mean loss: 272.46
 ---- batch: 050 ----
mean loss: 267.09
 ---- batch: 060 ----
mean loss: 270.50
 ---- batch: 070 ----
mean loss: 279.23
 ---- batch: 080 ----
mean loss: 263.84
 ---- batch: 090 ----
mean loss: 257.64
train mean loss: 270.42
epoch train time: 0:00:01.671364
elapsed time: 0:02:08.106163
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 20:34:21.547544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.24
 ---- batch: 020 ----
mean loss: 257.84
 ---- batch: 030 ----
mean loss: 271.69
 ---- batch: 040 ----
mean loss: 270.14
 ---- batch: 050 ----
mean loss: 268.02
 ---- batch: 060 ----
mean loss: 258.50
 ---- batch: 070 ----
mean loss: 264.35
 ---- batch: 080 ----
mean loss: 264.67
 ---- batch: 090 ----
mean loss: 267.56
train mean loss: 264.86
epoch train time: 0:00:01.685346
elapsed time: 0:02:09.792175
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 20:34:23.233534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.24
 ---- batch: 020 ----
mean loss: 262.59
 ---- batch: 030 ----
mean loss: 260.23
 ---- batch: 040 ----
mean loss: 254.62
 ---- batch: 050 ----
mean loss: 266.35
 ---- batch: 060 ----
mean loss: 262.06
 ---- batch: 070 ----
mean loss: 255.63
 ---- batch: 080 ----
mean loss: 253.50
 ---- batch: 090 ----
mean loss: 258.80
train mean loss: 258.70
epoch train time: 0:00:01.662575
elapsed time: 0:02:11.455372
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 20:34:24.896729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.91
 ---- batch: 020 ----
mean loss: 249.57
 ---- batch: 030 ----
mean loss: 256.04
 ---- batch: 040 ----
mean loss: 251.41
 ---- batch: 050 ----
mean loss: 254.73
 ---- batch: 060 ----
mean loss: 251.13
 ---- batch: 070 ----
mean loss: 253.74
 ---- batch: 080 ----
mean loss: 262.64
 ---- batch: 090 ----
mean loss: 255.71
train mean loss: 254.39
epoch train time: 0:00:01.671076
elapsed time: 0:02:13.127062
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 20:34:26.568435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.93
 ---- batch: 020 ----
mean loss: 256.37
 ---- batch: 030 ----
mean loss: 245.49
 ---- batch: 040 ----
mean loss: 249.81
 ---- batch: 050 ----
mean loss: 257.03
 ---- batch: 060 ----
mean loss: 252.45
 ---- batch: 070 ----
mean loss: 250.26
 ---- batch: 080 ----
mean loss: 250.35
 ---- batch: 090 ----
mean loss: 245.69
train mean loss: 250.29
epoch train time: 0:00:01.693882
elapsed time: 0:02:14.821605
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 20:34:28.262964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.35
 ---- batch: 020 ----
mean loss: 241.35
 ---- batch: 030 ----
mean loss: 240.86
 ---- batch: 040 ----
mean loss: 247.26
 ---- batch: 050 ----
mean loss: 243.08
 ---- batch: 060 ----
mean loss: 247.04
 ---- batch: 070 ----
mean loss: 251.75
 ---- batch: 080 ----
mean loss: 253.14
 ---- batch: 090 ----
mean loss: 249.41
train mean loss: 246.05
epoch train time: 0:00:01.657217
elapsed time: 0:02:16.479438
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 20:34:29.920789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.36
 ---- batch: 020 ----
mean loss: 238.09
 ---- batch: 030 ----
mean loss: 239.89
 ---- batch: 040 ----
mean loss: 236.14
 ---- batch: 050 ----
mean loss: 238.91
 ---- batch: 060 ----
mean loss: 254.73
 ---- batch: 070 ----
mean loss: 234.95
 ---- batch: 080 ----
mean loss: 240.53
 ---- batch: 090 ----
mean loss: 255.07
train mean loss: 242.46
epoch train time: 0:00:01.672834
elapsed time: 0:02:18.152853
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 20:34:31.594225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.46
 ---- batch: 020 ----
mean loss: 233.66
 ---- batch: 030 ----
mean loss: 245.74
 ---- batch: 040 ----
mean loss: 227.92
 ---- batch: 050 ----
mean loss: 239.89
 ---- batch: 060 ----
mean loss: 240.55
 ---- batch: 070 ----
mean loss: 243.27
 ---- batch: 080 ----
mean loss: 241.64
 ---- batch: 090 ----
mean loss: 243.86
train mean loss: 239.47
epoch train time: 0:00:01.676236
elapsed time: 0:02:19.829727
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 20:34:33.271125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.30
 ---- batch: 020 ----
mean loss: 237.78
 ---- batch: 030 ----
mean loss: 228.28
 ---- batch: 040 ----
mean loss: 237.84
 ---- batch: 050 ----
mean loss: 240.97
 ---- batch: 060 ----
mean loss: 229.65
 ---- batch: 070 ----
mean loss: 231.51
 ---- batch: 080 ----
mean loss: 233.23
 ---- batch: 090 ----
mean loss: 230.81
train mean loss: 234.85
epoch train time: 0:00:01.676016
elapsed time: 0:02:21.506461
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 20:34:34.947826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.99
 ---- batch: 020 ----
mean loss: 231.29
 ---- batch: 030 ----
mean loss: 239.05
 ---- batch: 040 ----
mean loss: 239.83
 ---- batch: 050 ----
mean loss: 235.07
 ---- batch: 060 ----
mean loss: 233.51
 ---- batch: 070 ----
mean loss: 225.35
 ---- batch: 080 ----
mean loss: 224.96
 ---- batch: 090 ----
mean loss: 227.56
train mean loss: 232.70
epoch train time: 0:00:01.701125
elapsed time: 0:02:23.208215
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 20:34:36.649627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.36
 ---- batch: 020 ----
mean loss: 226.55
 ---- batch: 030 ----
mean loss: 223.74
 ---- batch: 040 ----
mean loss: 226.49
 ---- batch: 050 ----
mean loss: 233.28
 ---- batch: 060 ----
mean loss: 236.00
 ---- batch: 070 ----
mean loss: 223.87
 ---- batch: 080 ----
mean loss: 236.82
 ---- batch: 090 ----
mean loss: 221.79
train mean loss: 228.22
epoch train time: 0:00:01.684916
elapsed time: 0:02:24.893806
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 20:34:38.335187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.51
 ---- batch: 020 ----
mean loss: 221.79
 ---- batch: 030 ----
mean loss: 237.34
 ---- batch: 040 ----
mean loss: 230.30
 ---- batch: 050 ----
mean loss: 227.73
 ---- batch: 060 ----
mean loss: 235.24
 ---- batch: 070 ----
mean loss: 222.25
 ---- batch: 080 ----
mean loss: 223.55
 ---- batch: 090 ----
mean loss: 219.13
train mean loss: 226.28
epoch train time: 0:00:01.677197
elapsed time: 0:02:26.571655
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 20:34:40.013053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.49
 ---- batch: 020 ----
mean loss: 225.48
 ---- batch: 030 ----
mean loss: 222.36
 ---- batch: 040 ----
mean loss: 218.31
 ---- batch: 050 ----
mean loss: 221.37
 ---- batch: 060 ----
mean loss: 220.21
 ---- batch: 070 ----
mean loss: 224.99
 ---- batch: 080 ----
mean loss: 235.06
 ---- batch: 090 ----
mean loss: 219.81
train mean loss: 224.34
epoch train time: 0:00:01.702221
elapsed time: 0:02:28.274556
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 20:34:41.715914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.13
 ---- batch: 020 ----
mean loss: 225.09
 ---- batch: 030 ----
mean loss: 209.50
 ---- batch: 040 ----
mean loss: 221.57
 ---- batch: 050 ----
mean loss: 220.43
 ---- batch: 060 ----
mean loss: 217.92
 ---- batch: 070 ----
mean loss: 233.82
 ---- batch: 080 ----
mean loss: 219.03
 ---- batch: 090 ----
mean loss: 225.04
train mean loss: 222.25
epoch train time: 0:00:01.721318
elapsed time: 0:02:29.996492
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 20:34:43.437860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.32
 ---- batch: 020 ----
mean loss: 211.22
 ---- batch: 030 ----
mean loss: 215.38
 ---- batch: 040 ----
mean loss: 221.57
 ---- batch: 050 ----
mean loss: 218.72
 ---- batch: 060 ----
mean loss: 213.13
 ---- batch: 070 ----
mean loss: 207.26
 ---- batch: 080 ----
mean loss: 225.68
 ---- batch: 090 ----
mean loss: 226.24
train mean loss: 218.02
epoch train time: 0:00:01.713940
elapsed time: 0:02:31.711137
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 20:34:45.152518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.54
 ---- batch: 020 ----
mean loss: 203.04
 ---- batch: 030 ----
mean loss: 213.40
 ---- batch: 040 ----
mean loss: 222.03
 ---- batch: 050 ----
mean loss: 217.57
 ---- batch: 060 ----
mean loss: 213.57
 ---- batch: 070 ----
mean loss: 219.37
 ---- batch: 080 ----
mean loss: 213.89
 ---- batch: 090 ----
mean loss: 218.25
train mean loss: 214.75
epoch train time: 0:00:01.714377
elapsed time: 0:02:33.426159
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 20:34:46.867595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.77
 ---- batch: 020 ----
mean loss: 212.83
 ---- batch: 030 ----
mean loss: 213.25
 ---- batch: 040 ----
mean loss: 219.66
 ---- batch: 050 ----
mean loss: 216.52
 ---- batch: 060 ----
mean loss: 217.24
 ---- batch: 070 ----
mean loss: 209.04
 ---- batch: 080 ----
mean loss: 212.39
 ---- batch: 090 ----
mean loss: 214.08
train mean loss: 213.68
epoch train time: 0:00:01.664731
elapsed time: 0:02:35.091585
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 20:34:48.532943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.54
 ---- batch: 020 ----
mean loss: 196.55
 ---- batch: 030 ----
mean loss: 214.62
 ---- batch: 040 ----
mean loss: 209.97
 ---- batch: 050 ----
mean loss: 225.98
 ---- batch: 060 ----
mean loss: 211.77
 ---- batch: 070 ----
mean loss: 214.73
 ---- batch: 080 ----
mean loss: 217.25
 ---- batch: 090 ----
mean loss: 215.27
train mean loss: 211.41
epoch train time: 0:00:01.698289
elapsed time: 0:02:36.790519
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 20:34:50.231885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.43
 ---- batch: 020 ----
mean loss: 211.31
 ---- batch: 030 ----
mean loss: 213.38
 ---- batch: 040 ----
mean loss: 206.29
 ---- batch: 050 ----
mean loss: 206.52
 ---- batch: 060 ----
mean loss: 210.45
 ---- batch: 070 ----
mean loss: 208.38
 ---- batch: 080 ----
mean loss: 211.28
 ---- batch: 090 ----
mean loss: 215.68
train mean loss: 210.33
epoch train time: 0:00:01.680967
elapsed time: 0:02:38.472145
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 20:34:51.913503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.09
 ---- batch: 020 ----
mean loss: 209.48
 ---- batch: 030 ----
mean loss: 206.35
 ---- batch: 040 ----
mean loss: 208.34
 ---- batch: 050 ----
mean loss: 206.70
 ---- batch: 060 ----
mean loss: 211.61
 ---- batch: 070 ----
mean loss: 204.97
 ---- batch: 080 ----
mean loss: 208.58
 ---- batch: 090 ----
mean loss: 208.27
train mean loss: 207.69
epoch train time: 0:00:01.669864
elapsed time: 0:02:40.142639
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 20:34:53.584007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.09
 ---- batch: 020 ----
mean loss: 205.44
 ---- batch: 030 ----
mean loss: 206.43
 ---- batch: 040 ----
mean loss: 212.45
 ---- batch: 050 ----
mean loss: 198.09
 ---- batch: 060 ----
mean loss: 201.72
 ---- batch: 070 ----
mean loss: 211.52
 ---- batch: 080 ----
mean loss: 207.68
 ---- batch: 090 ----
mean loss: 202.53
train mean loss: 205.35
epoch train time: 0:00:01.680975
elapsed time: 0:02:41.824270
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 20:34:55.265672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.58
 ---- batch: 020 ----
mean loss: 202.66
 ---- batch: 030 ----
mean loss: 193.54
 ---- batch: 040 ----
mean loss: 194.30
 ---- batch: 050 ----
mean loss: 213.06
 ---- batch: 060 ----
mean loss: 202.09
 ---- batch: 070 ----
mean loss: 205.91
 ---- batch: 080 ----
mean loss: 208.82
 ---- batch: 090 ----
mean loss: 205.23
train mean loss: 203.57
epoch train time: 0:00:01.697351
elapsed time: 0:02:43.522275
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 20:34:56.963652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.18
 ---- batch: 020 ----
mean loss: 199.47
 ---- batch: 030 ----
mean loss: 197.41
 ---- batch: 040 ----
mean loss: 203.63
 ---- batch: 050 ----
mean loss: 194.82
 ---- batch: 060 ----
mean loss: 199.58
 ---- batch: 070 ----
mean loss: 210.06
 ---- batch: 080 ----
mean loss: 205.08
 ---- batch: 090 ----
mean loss: 204.46
train mean loss: 200.90
epoch train time: 0:00:01.693910
elapsed time: 0:02:45.216887
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 20:34:58.658274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.22
 ---- batch: 020 ----
mean loss: 196.96
 ---- batch: 030 ----
mean loss: 197.29
 ---- batch: 040 ----
mean loss: 209.57
 ---- batch: 050 ----
mean loss: 200.17
 ---- batch: 060 ----
mean loss: 197.56
 ---- batch: 070 ----
mean loss: 210.50
 ---- batch: 080 ----
mean loss: 203.33
 ---- batch: 090 ----
mean loss: 200.34
train mean loss: 199.86
epoch train time: 0:00:01.705357
elapsed time: 0:02:46.922928
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 20:35:00.364275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.41
 ---- batch: 020 ----
mean loss: 190.70
 ---- batch: 030 ----
mean loss: 192.28
 ---- batch: 040 ----
mean loss: 204.13
 ---- batch: 050 ----
mean loss: 200.22
 ---- batch: 060 ----
mean loss: 205.49
 ---- batch: 070 ----
mean loss: 190.61
 ---- batch: 080 ----
mean loss: 203.58
 ---- batch: 090 ----
mean loss: 198.42
train mean loss: 196.86
epoch train time: 0:00:01.695778
elapsed time: 0:02:48.619354
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 20:35:02.060727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.75
 ---- batch: 020 ----
mean loss: 186.13
 ---- batch: 030 ----
mean loss: 193.34
 ---- batch: 040 ----
mean loss: 196.04
 ---- batch: 050 ----
mean loss: 197.38
 ---- batch: 060 ----
mean loss: 192.59
 ---- batch: 070 ----
mean loss: 199.10
 ---- batch: 080 ----
mean loss: 197.72
 ---- batch: 090 ----
mean loss: 202.80
train mean loss: 195.83
epoch train time: 0:00:01.675124
elapsed time: 0:02:50.295099
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 20:35:03.736462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.63
 ---- batch: 020 ----
mean loss: 196.73
 ---- batch: 030 ----
mean loss: 197.52
 ---- batch: 040 ----
mean loss: 193.74
 ---- batch: 050 ----
mean loss: 198.26
 ---- batch: 060 ----
mean loss: 194.36
 ---- batch: 070 ----
mean loss: 191.69
 ---- batch: 080 ----
mean loss: 193.68
 ---- batch: 090 ----
mean loss: 192.65
train mean loss: 194.20
epoch train time: 0:00:01.695584
elapsed time: 0:02:51.991264
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 20:35:05.432617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.31
 ---- batch: 020 ----
mean loss: 188.20
 ---- batch: 030 ----
mean loss: 187.19
 ---- batch: 040 ----
mean loss: 192.66
 ---- batch: 050 ----
mean loss: 195.41
 ---- batch: 060 ----
mean loss: 202.33
 ---- batch: 070 ----
mean loss: 190.82
 ---- batch: 080 ----
mean loss: 202.28
 ---- batch: 090 ----
mean loss: 189.63
train mean loss: 192.59
epoch train time: 0:00:01.689621
elapsed time: 0:02:53.681459
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 20:35:07.122819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.26
 ---- batch: 020 ----
mean loss: 180.31
 ---- batch: 030 ----
mean loss: 186.31
 ---- batch: 040 ----
mean loss: 188.57
 ---- batch: 050 ----
mean loss: 194.28
 ---- batch: 060 ----
mean loss: 196.93
 ---- batch: 070 ----
mean loss: 184.27
 ---- batch: 080 ----
mean loss: 191.86
 ---- batch: 090 ----
mean loss: 199.52
train mean loss: 189.99
epoch train time: 0:00:01.720290
elapsed time: 0:02:55.402391
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 20:35:08.843766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.78
 ---- batch: 020 ----
mean loss: 185.72
 ---- batch: 030 ----
mean loss: 183.94
 ---- batch: 040 ----
mean loss: 187.76
 ---- batch: 050 ----
mean loss: 189.44
 ---- batch: 060 ----
mean loss: 191.22
 ---- batch: 070 ----
mean loss: 181.34
 ---- batch: 080 ----
mean loss: 190.32
 ---- batch: 090 ----
mean loss: 196.20
train mean loss: 188.64
epoch train time: 0:00:01.678648
elapsed time: 0:02:57.081643
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 20:35:10.522991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.64
 ---- batch: 020 ----
mean loss: 193.87
 ---- batch: 030 ----
mean loss: 189.23
 ---- batch: 040 ----
mean loss: 195.19
 ---- batch: 050 ----
mean loss: 189.23
 ---- batch: 060 ----
mean loss: 178.63
 ---- batch: 070 ----
mean loss: 176.94
 ---- batch: 080 ----
mean loss: 198.17
 ---- batch: 090 ----
mean loss: 190.64
train mean loss: 188.88
epoch train time: 0:00:01.678970
elapsed time: 0:02:58.761228
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 20:35:12.202479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.93
 ---- batch: 020 ----
mean loss: 180.56
 ---- batch: 030 ----
mean loss: 190.17
 ---- batch: 040 ----
mean loss: 188.12
 ---- batch: 050 ----
mean loss: 179.67
 ---- batch: 060 ----
mean loss: 185.25
 ---- batch: 070 ----
mean loss: 181.14
 ---- batch: 080 ----
mean loss: 188.46
 ---- batch: 090 ----
mean loss: 179.45
train mean loss: 184.45
epoch train time: 0:00:01.681970
elapsed time: 0:03:00.443713
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 20:35:13.885071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.43
 ---- batch: 020 ----
mean loss: 177.85
 ---- batch: 030 ----
mean loss: 180.03
 ---- batch: 040 ----
mean loss: 184.14
 ---- batch: 050 ----
mean loss: 172.29
 ---- batch: 060 ----
mean loss: 185.05
 ---- batch: 070 ----
mean loss: 188.43
 ---- batch: 080 ----
mean loss: 187.27
 ---- batch: 090 ----
mean loss: 190.80
train mean loss: 183.62
epoch train time: 0:00:01.676969
elapsed time: 0:03:02.121396
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 20:35:15.562838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.02
 ---- batch: 020 ----
mean loss: 180.21
 ---- batch: 030 ----
mean loss: 178.66
 ---- batch: 040 ----
mean loss: 187.14
 ---- batch: 050 ----
mean loss: 179.86
 ---- batch: 060 ----
mean loss: 188.48
 ---- batch: 070 ----
mean loss: 183.59
 ---- batch: 080 ----
mean loss: 186.09
 ---- batch: 090 ----
mean loss: 182.59
train mean loss: 183.50
epoch train time: 0:00:01.766263
elapsed time: 0:03:03.888374
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 20:35:17.329802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.88
 ---- batch: 020 ----
mean loss: 179.87
 ---- batch: 030 ----
mean loss: 177.07
 ---- batch: 040 ----
mean loss: 182.22
 ---- batch: 050 ----
mean loss: 183.87
 ---- batch: 060 ----
mean loss: 182.39
 ---- batch: 070 ----
mean loss: 185.38
 ---- batch: 080 ----
mean loss: 188.14
 ---- batch: 090 ----
mean loss: 182.24
train mean loss: 180.88
epoch train time: 0:00:01.683786
elapsed time: 0:03:05.572945
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 20:35:19.014299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.15
 ---- batch: 020 ----
mean loss: 174.42
 ---- batch: 030 ----
mean loss: 173.61
 ---- batch: 040 ----
mean loss: 176.64
 ---- batch: 050 ----
mean loss: 183.25
 ---- batch: 060 ----
mean loss: 193.86
 ---- batch: 070 ----
mean loss: 177.41
 ---- batch: 080 ----
mean loss: 182.04
 ---- batch: 090 ----
mean loss: 178.91
train mean loss: 179.58
epoch train time: 0:00:01.657971
elapsed time: 0:03:07.231554
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 20:35:20.672834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.84
 ---- batch: 020 ----
mean loss: 182.49
 ---- batch: 030 ----
mean loss: 171.56
 ---- batch: 040 ----
mean loss: 176.39
 ---- batch: 050 ----
mean loss: 183.17
 ---- batch: 060 ----
mean loss: 184.41
 ---- batch: 070 ----
mean loss: 177.76
 ---- batch: 080 ----
mean loss: 181.70
 ---- batch: 090 ----
mean loss: 175.80
train mean loss: 179.20
epoch train time: 0:00:01.702755
elapsed time: 0:03:08.934830
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 20:35:22.376216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.20
 ---- batch: 020 ----
mean loss: 176.95
 ---- batch: 030 ----
mean loss: 177.79
 ---- batch: 040 ----
mean loss: 178.78
 ---- batch: 050 ----
mean loss: 170.16
 ---- batch: 060 ----
mean loss: 179.94
 ---- batch: 070 ----
mean loss: 174.85
 ---- batch: 080 ----
mean loss: 186.46
 ---- batch: 090 ----
mean loss: 179.26
train mean loss: 177.15
epoch train time: 0:00:01.686688
elapsed time: 0:03:10.622175
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 20:35:24.063551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.41
 ---- batch: 020 ----
mean loss: 177.09
 ---- batch: 030 ----
mean loss: 179.74
 ---- batch: 040 ----
mean loss: 177.40
 ---- batch: 050 ----
mean loss: 176.82
 ---- batch: 060 ----
mean loss: 183.00
 ---- batch: 070 ----
mean loss: 185.39
 ---- batch: 080 ----
mean loss: 176.35
 ---- batch: 090 ----
mean loss: 170.34
train mean loss: 176.58
epoch train time: 0:00:01.684625
elapsed time: 0:03:12.307483
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 20:35:25.748836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.26
 ---- batch: 020 ----
mean loss: 163.50
 ---- batch: 030 ----
mean loss: 182.20
 ---- batch: 040 ----
mean loss: 166.90
 ---- batch: 050 ----
mean loss: 174.52
 ---- batch: 060 ----
mean loss: 180.10
 ---- batch: 070 ----
mean loss: 174.70
 ---- batch: 080 ----
mean loss: 177.67
 ---- batch: 090 ----
mean loss: 177.91
train mean loss: 174.18
epoch train time: 0:00:01.688289
elapsed time: 0:03:13.996380
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 20:35:27.437808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.94
 ---- batch: 020 ----
mean loss: 171.08
 ---- batch: 030 ----
mean loss: 172.92
 ---- batch: 040 ----
mean loss: 170.97
 ---- batch: 050 ----
mean loss: 174.35
 ---- batch: 060 ----
mean loss: 177.76
 ---- batch: 070 ----
mean loss: 169.28
 ---- batch: 080 ----
mean loss: 172.96
 ---- batch: 090 ----
mean loss: 171.36
train mean loss: 173.33
epoch train time: 0:00:01.718320
elapsed time: 0:03:15.715404
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 20:35:29.156769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.74
 ---- batch: 020 ----
mean loss: 162.82
 ---- batch: 030 ----
mean loss: 172.98
 ---- batch: 040 ----
mean loss: 173.57
 ---- batch: 050 ----
mean loss: 175.09
 ---- batch: 060 ----
mean loss: 169.81
 ---- batch: 070 ----
mean loss: 181.67
 ---- batch: 080 ----
mean loss: 173.58
 ---- batch: 090 ----
mean loss: 169.72
train mean loss: 171.97
epoch train time: 0:00:01.684231
elapsed time: 0:03:17.400359
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 20:35:30.841755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.23
 ---- batch: 020 ----
mean loss: 162.33
 ---- batch: 030 ----
mean loss: 167.01
 ---- batch: 040 ----
mean loss: 175.22
 ---- batch: 050 ----
mean loss: 169.51
 ---- batch: 060 ----
mean loss: 165.91
 ---- batch: 070 ----
mean loss: 173.21
 ---- batch: 080 ----
mean loss: 174.37
 ---- batch: 090 ----
mean loss: 174.18
train mean loss: 170.22
epoch train time: 0:00:01.710033
elapsed time: 0:03:19.111049
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 20:35:32.552418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.80
 ---- batch: 020 ----
mean loss: 172.50
 ---- batch: 030 ----
mean loss: 167.54
 ---- batch: 040 ----
mean loss: 171.07
 ---- batch: 050 ----
mean loss: 170.75
 ---- batch: 060 ----
mean loss: 167.96
 ---- batch: 070 ----
mean loss: 171.35
 ---- batch: 080 ----
mean loss: 169.81
 ---- batch: 090 ----
mean loss: 167.73
train mean loss: 168.94
epoch train time: 0:00:01.716618
elapsed time: 0:03:20.828313
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 20:35:34.269710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.54
 ---- batch: 020 ----
mean loss: 156.30
 ---- batch: 030 ----
mean loss: 163.10
 ---- batch: 040 ----
mean loss: 160.48
 ---- batch: 050 ----
mean loss: 176.01
 ---- batch: 060 ----
mean loss: 171.82
 ---- batch: 070 ----
mean loss: 167.19
 ---- batch: 080 ----
mean loss: 173.07
 ---- batch: 090 ----
mean loss: 172.05
train mean loss: 168.30
epoch train time: 0:00:01.705558
elapsed time: 0:03:22.534554
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 20:35:35.975904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.54
 ---- batch: 020 ----
mean loss: 160.50
 ---- batch: 030 ----
mean loss: 165.51
 ---- batch: 040 ----
mean loss: 166.22
 ---- batch: 050 ----
mean loss: 167.79
 ---- batch: 060 ----
mean loss: 167.12
 ---- batch: 070 ----
mean loss: 166.80
 ---- batch: 080 ----
mean loss: 171.14
 ---- batch: 090 ----
mean loss: 173.19
train mean loss: 167.78
epoch train time: 0:00:01.677975
elapsed time: 0:03:24.213139
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 20:35:37.654564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.78
 ---- batch: 020 ----
mean loss: 159.06
 ---- batch: 030 ----
mean loss: 163.96
 ---- batch: 040 ----
mean loss: 164.55
 ---- batch: 050 ----
mean loss: 170.99
 ---- batch: 060 ----
mean loss: 173.67
 ---- batch: 070 ----
mean loss: 167.43
 ---- batch: 080 ----
mean loss: 163.98
 ---- batch: 090 ----
mean loss: 165.06
train mean loss: 166.11
epoch train time: 0:00:01.670996
elapsed time: 0:03:25.884838
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 20:35:39.326235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.86
 ---- batch: 020 ----
mean loss: 164.93
 ---- batch: 030 ----
mean loss: 167.25
 ---- batch: 040 ----
mean loss: 162.80
 ---- batch: 050 ----
mean loss: 166.25
 ---- batch: 060 ----
mean loss: 164.06
 ---- batch: 070 ----
mean loss: 170.02
 ---- batch: 080 ----
mean loss: 164.44
 ---- batch: 090 ----
mean loss: 166.96
train mean loss: 166.24
epoch train time: 0:00:01.660879
elapsed time: 0:03:27.546356
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 20:35:40.987708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.58
 ---- batch: 020 ----
mean loss: 162.84
 ---- batch: 030 ----
mean loss: 156.78
 ---- batch: 040 ----
mean loss: 163.16
 ---- batch: 050 ----
mean loss: 162.47
 ---- batch: 060 ----
mean loss: 164.31
 ---- batch: 070 ----
mean loss: 171.78
 ---- batch: 080 ----
mean loss: 160.43
 ---- batch: 090 ----
mean loss: 165.88
train mean loss: 163.98
epoch train time: 0:00:01.660432
elapsed time: 0:03:29.207362
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 20:35:42.648728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.45
 ---- batch: 020 ----
mean loss: 168.03
 ---- batch: 030 ----
mean loss: 158.95
 ---- batch: 040 ----
mean loss: 165.92
 ---- batch: 050 ----
mean loss: 162.28
 ---- batch: 060 ----
mean loss: 169.90
 ---- batch: 070 ----
mean loss: 155.94
 ---- batch: 080 ----
mean loss: 161.96
 ---- batch: 090 ----
mean loss: 161.27
train mean loss: 162.96
epoch train time: 0:00:01.701563
elapsed time: 0:03:30.909644
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 20:35:44.351041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.50
 ---- batch: 020 ----
mean loss: 161.34
 ---- batch: 030 ----
mean loss: 167.23
 ---- batch: 040 ----
mean loss: 154.17
 ---- batch: 050 ----
mean loss: 154.31
 ---- batch: 060 ----
mean loss: 162.78
 ---- batch: 070 ----
mean loss: 162.94
 ---- batch: 080 ----
mean loss: 167.23
 ---- batch: 090 ----
mean loss: 164.47
train mean loss: 161.30
epoch train time: 0:00:01.687166
elapsed time: 0:03:32.597498
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 20:35:46.038851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.48
 ---- batch: 020 ----
mean loss: 164.55
 ---- batch: 030 ----
mean loss: 153.98
 ---- batch: 040 ----
mean loss: 158.93
 ---- batch: 050 ----
mean loss: 164.44
 ---- batch: 060 ----
mean loss: 166.43
 ---- batch: 070 ----
mean loss: 160.62
 ---- batch: 080 ----
mean loss: 158.39
 ---- batch: 090 ----
mean loss: 165.73
train mean loss: 161.19
epoch train time: 0:00:01.674548
elapsed time: 0:03:34.272927
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 20:35:47.714127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.84
 ---- batch: 020 ----
mean loss: 157.84
 ---- batch: 030 ----
mean loss: 158.93
 ---- batch: 040 ----
mean loss: 157.97
 ---- batch: 050 ----
mean loss: 160.54
 ---- batch: 060 ----
mean loss: 160.62
 ---- batch: 070 ----
mean loss: 162.84
 ---- batch: 080 ----
mean loss: 164.73
 ---- batch: 090 ----
mean loss: 159.56
train mean loss: 159.35
epoch train time: 0:00:01.677828
elapsed time: 0:03:35.951243
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 20:35:49.392618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.09
 ---- batch: 020 ----
mean loss: 157.07
 ---- batch: 030 ----
mean loss: 152.92
 ---- batch: 040 ----
mean loss: 159.09
 ---- batch: 050 ----
mean loss: 154.93
 ---- batch: 060 ----
mean loss: 159.90
 ---- batch: 070 ----
mean loss: 162.53
 ---- batch: 080 ----
mean loss: 165.08
 ---- batch: 090 ----
mean loss: 157.01
train mean loss: 158.24
epoch train time: 0:00:01.664202
elapsed time: 0:03:37.616095
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 20:35:51.057443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.50
 ---- batch: 020 ----
mean loss: 155.72
 ---- batch: 030 ----
mean loss: 158.03
 ---- batch: 040 ----
mean loss: 156.26
 ---- batch: 050 ----
mean loss: 155.76
 ---- batch: 060 ----
mean loss: 163.90
 ---- batch: 070 ----
mean loss: 159.75
 ---- batch: 080 ----
mean loss: 159.98
 ---- batch: 090 ----
mean loss: 156.76
train mean loss: 158.59
epoch train time: 0:00:01.677153
elapsed time: 0:03:39.293901
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 20:35:52.735286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.10
 ---- batch: 020 ----
mean loss: 156.70
 ---- batch: 030 ----
mean loss: 157.97
 ---- batch: 040 ----
mean loss: 147.40
 ---- batch: 050 ----
mean loss: 157.24
 ---- batch: 060 ----
mean loss: 163.62
 ---- batch: 070 ----
mean loss: 159.55
 ---- batch: 080 ----
mean loss: 155.56
 ---- batch: 090 ----
mean loss: 154.02
train mean loss: 156.90
epoch train time: 0:00:01.696639
elapsed time: 0:03:40.991184
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 20:35:54.432545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.43
 ---- batch: 020 ----
mean loss: 151.30
 ---- batch: 030 ----
mean loss: 158.49
 ---- batch: 040 ----
mean loss: 162.27
 ---- batch: 050 ----
mean loss: 157.36
 ---- batch: 060 ----
mean loss: 160.02
 ---- batch: 070 ----
mean loss: 160.25
 ---- batch: 080 ----
mean loss: 149.55
 ---- batch: 090 ----
mean loss: 156.04
train mean loss: 155.20
epoch train time: 0:00:01.681573
elapsed time: 0:03:42.673404
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 20:35:56.114767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.47
 ---- batch: 020 ----
mean loss: 156.72
 ---- batch: 030 ----
mean loss: 149.67
 ---- batch: 040 ----
mean loss: 152.90
 ---- batch: 050 ----
mean loss: 156.68
 ---- batch: 060 ----
mean loss: 159.99
 ---- batch: 070 ----
mean loss: 162.44
 ---- batch: 080 ----
mean loss: 155.42
 ---- batch: 090 ----
mean loss: 155.13
train mean loss: 155.16
epoch train time: 0:00:01.667016
elapsed time: 0:03:44.341090
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 20:35:57.782500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.89
 ---- batch: 020 ----
mean loss: 145.94
 ---- batch: 030 ----
mean loss: 153.83
 ---- batch: 040 ----
mean loss: 155.41
 ---- batch: 050 ----
mean loss: 153.97
 ---- batch: 060 ----
mean loss: 163.26
 ---- batch: 070 ----
mean loss: 148.97
 ---- batch: 080 ----
mean loss: 154.09
 ---- batch: 090 ----
mean loss: 151.48
train mean loss: 153.82
epoch train time: 0:00:01.689357
elapsed time: 0:03:46.031116
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 20:35:59.472456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.58
 ---- batch: 020 ----
mean loss: 149.01
 ---- batch: 030 ----
mean loss: 153.82
 ---- batch: 040 ----
mean loss: 152.07
 ---- batch: 050 ----
mean loss: 148.74
 ---- batch: 060 ----
mean loss: 158.38
 ---- batch: 070 ----
mean loss: 159.66
 ---- batch: 080 ----
mean loss: 150.58
 ---- batch: 090 ----
mean loss: 157.01
train mean loss: 153.65
epoch train time: 0:00:01.666259
elapsed time: 0:03:47.698016
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 20:36:01.139395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.68
 ---- batch: 020 ----
mean loss: 148.75
 ---- batch: 030 ----
mean loss: 147.87
 ---- batch: 040 ----
mean loss: 143.93
 ---- batch: 050 ----
mean loss: 159.72
 ---- batch: 060 ----
mean loss: 153.23
 ---- batch: 070 ----
mean loss: 155.95
 ---- batch: 080 ----
mean loss: 155.26
 ---- batch: 090 ----
mean loss: 158.49
train mean loss: 152.15
epoch train time: 0:00:01.683792
elapsed time: 0:03:49.382599
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 20:36:02.824022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.91
 ---- batch: 020 ----
mean loss: 141.76
 ---- batch: 030 ----
mean loss: 152.13
 ---- batch: 040 ----
mean loss: 151.24
 ---- batch: 050 ----
mean loss: 153.08
 ---- batch: 060 ----
mean loss: 154.07
 ---- batch: 070 ----
mean loss: 153.25
 ---- batch: 080 ----
mean loss: 160.48
 ---- batch: 090 ----
mean loss: 157.93
train mean loss: 151.43
epoch train time: 0:00:01.704018
elapsed time: 0:03:51.087309
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 20:36:04.528689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.36
 ---- batch: 020 ----
mean loss: 140.67
 ---- batch: 030 ----
mean loss: 148.38
 ---- batch: 040 ----
mean loss: 151.40
 ---- batch: 050 ----
mean loss: 156.37
 ---- batch: 060 ----
mean loss: 147.44
 ---- batch: 070 ----
mean loss: 150.64
 ---- batch: 080 ----
mean loss: 154.35
 ---- batch: 090 ----
mean loss: 163.30
train mean loss: 151.14
epoch train time: 0:00:01.700125
elapsed time: 0:03:52.788044
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 20:36:06.229388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.96
 ---- batch: 020 ----
mean loss: 147.31
 ---- batch: 030 ----
mean loss: 152.32
 ---- batch: 040 ----
mean loss: 148.72
 ---- batch: 050 ----
mean loss: 155.12
 ---- batch: 060 ----
mean loss: 151.27
 ---- batch: 070 ----
mean loss: 150.70
 ---- batch: 080 ----
mean loss: 150.35
 ---- batch: 090 ----
mean loss: 146.18
train mean loss: 149.95
epoch train time: 0:00:01.707466
elapsed time: 0:03:54.496095
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 20:36:07.937497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.42
 ---- batch: 020 ----
mean loss: 148.12
 ---- batch: 030 ----
mean loss: 142.60
 ---- batch: 040 ----
mean loss: 146.43
 ---- batch: 050 ----
mean loss: 150.18
 ---- batch: 060 ----
mean loss: 148.00
 ---- batch: 070 ----
mean loss: 150.06
 ---- batch: 080 ----
mean loss: 156.22
 ---- batch: 090 ----
mean loss: 149.13
train mean loss: 148.66
epoch train time: 0:00:01.677369
elapsed time: 0:03:56.174128
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 20:36:09.615478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.52
 ---- batch: 020 ----
mean loss: 143.08
 ---- batch: 030 ----
mean loss: 142.19
 ---- batch: 040 ----
mean loss: 153.04
 ---- batch: 050 ----
mean loss: 154.21
 ---- batch: 060 ----
mean loss: 149.13
 ---- batch: 070 ----
mean loss: 144.72
 ---- batch: 080 ----
mean loss: 154.75
 ---- batch: 090 ----
mean loss: 148.33
train mean loss: 147.79
epoch train time: 0:00:01.693850
elapsed time: 0:03:57.868573
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 20:36:11.309938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.74
 ---- batch: 020 ----
mean loss: 146.68
 ---- batch: 030 ----
mean loss: 145.39
 ---- batch: 040 ----
mean loss: 143.98
 ---- batch: 050 ----
mean loss: 152.48
 ---- batch: 060 ----
mean loss: 143.78
 ---- batch: 070 ----
mean loss: 151.46
 ---- batch: 080 ----
mean loss: 154.43
 ---- batch: 090 ----
mean loss: 149.71
train mean loss: 147.40
epoch train time: 0:00:01.691228
elapsed time: 0:03:59.560424
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 20:36:13.001822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.26
 ---- batch: 020 ----
mean loss: 142.15
 ---- batch: 030 ----
mean loss: 145.20
 ---- batch: 040 ----
mean loss: 147.52
 ---- batch: 050 ----
mean loss: 149.06
 ---- batch: 060 ----
mean loss: 146.27
 ---- batch: 070 ----
mean loss: 147.37
 ---- batch: 080 ----
mean loss: 144.69
 ---- batch: 090 ----
mean loss: 146.97
train mean loss: 146.31
epoch train time: 0:00:01.658091
elapsed time: 0:04:01.219128
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 20:36:14.660502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.51
 ---- batch: 020 ----
mean loss: 140.44
 ---- batch: 030 ----
mean loss: 137.08
 ---- batch: 040 ----
mean loss: 148.50
 ---- batch: 050 ----
mean loss: 149.64
 ---- batch: 060 ----
mean loss: 145.50
 ---- batch: 070 ----
mean loss: 147.61
 ---- batch: 080 ----
mean loss: 151.00
 ---- batch: 090 ----
mean loss: 154.00
train mean loss: 146.04
epoch train time: 0:00:01.684534
elapsed time: 0:04:02.904254
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 20:36:16.345654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.95
 ---- batch: 020 ----
mean loss: 142.99
 ---- batch: 030 ----
mean loss: 138.70
 ---- batch: 040 ----
mean loss: 143.94
 ---- batch: 050 ----
mean loss: 140.22
 ---- batch: 060 ----
mean loss: 150.87
 ---- batch: 070 ----
mean loss: 146.26
 ---- batch: 080 ----
mean loss: 143.21
 ---- batch: 090 ----
mean loss: 149.19
train mean loss: 144.80
epoch train time: 0:00:01.720532
elapsed time: 0:04:04.625501
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 20:36:18.066862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.13
 ---- batch: 020 ----
mean loss: 143.57
 ---- batch: 030 ----
mean loss: 140.70
 ---- batch: 040 ----
mean loss: 140.61
 ---- batch: 050 ----
mean loss: 146.06
 ---- batch: 060 ----
mean loss: 145.85
 ---- batch: 070 ----
mean loss: 143.39
 ---- batch: 080 ----
mean loss: 143.81
 ---- batch: 090 ----
mean loss: 158.04
train mean loss: 144.28
epoch train time: 0:00:01.706981
elapsed time: 0:04:06.333130
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 20:36:19.774503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.34
 ---- batch: 020 ----
mean loss: 144.11
 ---- batch: 030 ----
mean loss: 142.24
 ---- batch: 040 ----
mean loss: 143.58
 ---- batch: 050 ----
mean loss: 141.03
 ---- batch: 060 ----
mean loss: 143.95
 ---- batch: 070 ----
mean loss: 144.02
 ---- batch: 080 ----
mean loss: 148.16
 ---- batch: 090 ----
mean loss: 147.30
train mean loss: 143.41
epoch train time: 0:00:01.705647
elapsed time: 0:04:08.039673
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 20:36:21.480892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.52
 ---- batch: 020 ----
mean loss: 142.71
 ---- batch: 030 ----
mean loss: 135.76
 ---- batch: 040 ----
mean loss: 144.40
 ---- batch: 050 ----
mean loss: 143.71
 ---- batch: 060 ----
mean loss: 145.14
 ---- batch: 070 ----
mean loss: 141.05
 ---- batch: 080 ----
mean loss: 141.44
 ---- batch: 090 ----
mean loss: 157.80
train mean loss: 143.35
epoch train time: 0:00:01.720572
elapsed time: 0:04:09.760785
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 20:36:23.202183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.54
 ---- batch: 020 ----
mean loss: 131.39
 ---- batch: 030 ----
mean loss: 139.39
 ---- batch: 040 ----
mean loss: 140.14
 ---- batch: 050 ----
mean loss: 139.37
 ---- batch: 060 ----
mean loss: 138.10
 ---- batch: 070 ----
mean loss: 147.81
 ---- batch: 080 ----
mean loss: 149.13
 ---- batch: 090 ----
mean loss: 149.71
train mean loss: 141.69
epoch train time: 0:00:01.722094
elapsed time: 0:04:11.483697
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 20:36:24.925122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.17
 ---- batch: 020 ----
mean loss: 140.17
 ---- batch: 030 ----
mean loss: 134.66
 ---- batch: 040 ----
mean loss: 138.27
 ---- batch: 050 ----
mean loss: 138.94
 ---- batch: 060 ----
mean loss: 139.00
 ---- batch: 070 ----
mean loss: 143.74
 ---- batch: 080 ----
mean loss: 145.87
 ---- batch: 090 ----
mean loss: 144.53
train mean loss: 141.33
epoch train time: 0:00:01.696232
elapsed time: 0:04:13.180645
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 20:36:26.622020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.62
 ---- batch: 020 ----
mean loss: 141.08
 ---- batch: 030 ----
mean loss: 135.57
 ---- batch: 040 ----
mean loss: 139.51
 ---- batch: 050 ----
mean loss: 137.57
 ---- batch: 060 ----
mean loss: 143.13
 ---- batch: 070 ----
mean loss: 140.39
 ---- batch: 080 ----
mean loss: 138.08
 ---- batch: 090 ----
mean loss: 148.60
train mean loss: 140.19
epoch train time: 0:00:01.687667
elapsed time: 0:04:14.868905
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 20:36:28.310267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.12
 ---- batch: 020 ----
mean loss: 136.18
 ---- batch: 030 ----
mean loss: 143.90
 ---- batch: 040 ----
mean loss: 137.43
 ---- batch: 050 ----
mean loss: 138.37
 ---- batch: 060 ----
mean loss: 136.75
 ---- batch: 070 ----
mean loss: 144.23
 ---- batch: 080 ----
mean loss: 144.35
 ---- batch: 090 ----
mean loss: 141.52
train mean loss: 139.98
epoch train time: 0:00:01.674203
elapsed time: 0:04:16.543798
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 20:36:29.985183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.95
 ---- batch: 020 ----
mean loss: 136.68
 ---- batch: 030 ----
mean loss: 137.92
 ---- batch: 040 ----
mean loss: 135.45
 ---- batch: 050 ----
mean loss: 136.08
 ---- batch: 060 ----
mean loss: 143.08
 ---- batch: 070 ----
mean loss: 139.03
 ---- batch: 080 ----
mean loss: 139.61
 ---- batch: 090 ----
mean loss: 137.70
train mean loss: 138.80
epoch train time: 0:00:01.674496
elapsed time: 0:04:18.219012
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 20:36:31.660465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.78
 ---- batch: 020 ----
mean loss: 131.22
 ---- batch: 030 ----
mean loss: 136.83
 ---- batch: 040 ----
mean loss: 137.63
 ---- batch: 050 ----
mean loss: 138.17
 ---- batch: 060 ----
mean loss: 134.90
 ---- batch: 070 ----
mean loss: 140.84
 ---- batch: 080 ----
mean loss: 140.52
 ---- batch: 090 ----
mean loss: 145.46
train mean loss: 138.49
epoch train time: 0:00:01.663644
elapsed time: 0:04:19.883353
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 20:36:33.324710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.12
 ---- batch: 020 ----
mean loss: 135.78
 ---- batch: 030 ----
mean loss: 136.11
 ---- batch: 040 ----
mean loss: 134.31
 ---- batch: 050 ----
mean loss: 134.20
 ---- batch: 060 ----
mean loss: 143.74
 ---- batch: 070 ----
mean loss: 143.01
 ---- batch: 080 ----
mean loss: 138.41
 ---- batch: 090 ----
mean loss: 134.16
train mean loss: 137.48
epoch train time: 0:00:01.683375
elapsed time: 0:04:21.567366
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 20:36:35.008720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.53
 ---- batch: 020 ----
mean loss: 133.90
 ---- batch: 030 ----
mean loss: 135.68
 ---- batch: 040 ----
mean loss: 137.63
 ---- batch: 050 ----
mean loss: 135.78
 ---- batch: 060 ----
mean loss: 135.46
 ---- batch: 070 ----
mean loss: 137.66
 ---- batch: 080 ----
mean loss: 142.64
 ---- batch: 090 ----
mean loss: 144.27
train mean loss: 137.74
epoch train time: 0:00:01.670330
elapsed time: 0:04:23.238341
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 20:36:36.679723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.55
 ---- batch: 020 ----
mean loss: 133.23
 ---- batch: 030 ----
mean loss: 132.87
 ---- batch: 040 ----
mean loss: 139.04
 ---- batch: 050 ----
mean loss: 131.20
 ---- batch: 060 ----
mean loss: 135.94
 ---- batch: 070 ----
mean loss: 140.14
 ---- batch: 080 ----
mean loss: 139.44
 ---- batch: 090 ----
mean loss: 139.39
train mean loss: 136.12
epoch train time: 0:00:01.685717
elapsed time: 0:04:24.924696
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 20:36:38.366116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.07
 ---- batch: 020 ----
mean loss: 131.57
 ---- batch: 030 ----
mean loss: 130.61
 ---- batch: 040 ----
mean loss: 141.08
 ---- batch: 050 ----
mean loss: 135.23
 ---- batch: 060 ----
mean loss: 136.85
 ---- batch: 070 ----
mean loss: 133.16
 ---- batch: 080 ----
mean loss: 134.36
 ---- batch: 090 ----
mean loss: 137.96
train mean loss: 135.52
epoch train time: 0:00:01.690351
elapsed time: 0:04:26.615751
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 20:36:40.057225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.23
 ---- batch: 020 ----
mean loss: 133.04
 ---- batch: 030 ----
mean loss: 136.16
 ---- batch: 040 ----
mean loss: 134.06
 ---- batch: 050 ----
mean loss: 132.85
 ---- batch: 060 ----
mean loss: 129.25
 ---- batch: 070 ----
mean loss: 131.87
 ---- batch: 080 ----
mean loss: 144.38
 ---- batch: 090 ----
mean loss: 144.45
train mean loss: 135.35
epoch train time: 0:00:01.672741
elapsed time: 0:04:28.289232
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 20:36:41.730607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.97
 ---- batch: 020 ----
mean loss: 128.87
 ---- batch: 030 ----
mean loss: 132.36
 ---- batch: 040 ----
mean loss: 131.34
 ---- batch: 050 ----
mean loss: 134.52
 ---- batch: 060 ----
mean loss: 137.92
 ---- batch: 070 ----
mean loss: 134.30
 ---- batch: 080 ----
mean loss: 143.90
 ---- batch: 090 ----
mean loss: 136.11
train mean loss: 134.37
epoch train time: 0:00:01.717059
elapsed time: 0:04:30.006915
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 20:36:43.448311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.76
 ---- batch: 020 ----
mean loss: 133.34
 ---- batch: 030 ----
mean loss: 131.42
 ---- batch: 040 ----
mean loss: 132.24
 ---- batch: 050 ----
mean loss: 134.40
 ---- batch: 060 ----
mean loss: 132.60
 ---- batch: 070 ----
mean loss: 131.95
 ---- batch: 080 ----
mean loss: 138.40
 ---- batch: 090 ----
mean loss: 133.51
train mean loss: 133.21
epoch train time: 0:00:01.706286
elapsed time: 0:04:31.713833
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 20:36:45.155282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.17
 ---- batch: 020 ----
mean loss: 127.85
 ---- batch: 030 ----
mean loss: 136.13
 ---- batch: 040 ----
mean loss: 125.83
 ---- batch: 050 ----
mean loss: 137.38
 ---- batch: 060 ----
mean loss: 131.30
 ---- batch: 070 ----
mean loss: 140.00
 ---- batch: 080 ----
mean loss: 131.51
 ---- batch: 090 ----
mean loss: 140.63
train mean loss: 133.44
epoch train time: 0:00:01.735175
elapsed time: 0:04:33.449715
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 20:36:46.891083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.39
 ---- batch: 020 ----
mean loss: 128.68
 ---- batch: 030 ----
mean loss: 129.09
 ---- batch: 040 ----
mean loss: 133.39
 ---- batch: 050 ----
mean loss: 136.63
 ---- batch: 060 ----
mean loss: 136.34
 ---- batch: 070 ----
mean loss: 133.55
 ---- batch: 080 ----
mean loss: 131.71
 ---- batch: 090 ----
mean loss: 132.44
train mean loss: 133.18
epoch train time: 0:00:01.681713
elapsed time: 0:04:35.132067
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 20:36:48.573435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.72
 ---- batch: 020 ----
mean loss: 130.14
 ---- batch: 030 ----
mean loss: 128.81
 ---- batch: 040 ----
mean loss: 135.81
 ---- batch: 050 ----
mean loss: 127.41
 ---- batch: 060 ----
mean loss: 128.84
 ---- batch: 070 ----
mean loss: 134.99
 ---- batch: 080 ----
mean loss: 131.51
 ---- batch: 090 ----
mean loss: 130.79
train mean loss: 131.58
epoch train time: 0:00:01.680318
elapsed time: 0:04:36.812992
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 20:36:50.254344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.17
 ---- batch: 020 ----
mean loss: 130.68
 ---- batch: 030 ----
mean loss: 129.72
 ---- batch: 040 ----
mean loss: 133.57
 ---- batch: 050 ----
mean loss: 127.80
 ---- batch: 060 ----
mean loss: 135.66
 ---- batch: 070 ----
mean loss: 133.88
 ---- batch: 080 ----
mean loss: 130.43
 ---- batch: 090 ----
mean loss: 135.10
train mean loss: 131.33
epoch train time: 0:00:01.682508
elapsed time: 0:04:38.496110
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 20:36:51.937495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.59
 ---- batch: 020 ----
mean loss: 130.32
 ---- batch: 030 ----
mean loss: 128.08
 ---- batch: 040 ----
mean loss: 131.29
 ---- batch: 050 ----
mean loss: 139.98
 ---- batch: 060 ----
mean loss: 133.62
 ---- batch: 070 ----
mean loss: 128.19
 ---- batch: 080 ----
mean loss: 133.76
 ---- batch: 090 ----
mean loss: 130.09
train mean loss: 131.19
epoch train time: 0:00:01.694972
elapsed time: 0:04:40.191759
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 20:36:53.633283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.43
 ---- batch: 020 ----
mean loss: 125.49
 ---- batch: 030 ----
mean loss: 130.83
 ---- batch: 040 ----
mean loss: 128.24
 ---- batch: 050 ----
mean loss: 128.89
 ---- batch: 060 ----
mean loss: 133.28
 ---- batch: 070 ----
mean loss: 127.60
 ---- batch: 080 ----
mean loss: 126.97
 ---- batch: 090 ----
mean loss: 134.43
train mean loss: 130.16
epoch train time: 0:00:01.695815
elapsed time: 0:04:41.888389
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 20:36:55.329798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.82
 ---- batch: 020 ----
mean loss: 133.63
 ---- batch: 030 ----
mean loss: 125.13
 ---- batch: 040 ----
mean loss: 129.05
 ---- batch: 050 ----
mean loss: 123.47
 ---- batch: 060 ----
mean loss: 134.39
 ---- batch: 070 ----
mean loss: 127.22
 ---- batch: 080 ----
mean loss: 137.73
 ---- batch: 090 ----
mean loss: 133.02
train mean loss: 129.69
epoch train time: 0:00:01.699220
elapsed time: 0:04:43.588282
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 20:36:57.029720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.28
 ---- batch: 020 ----
mean loss: 126.11
 ---- batch: 030 ----
mean loss: 126.07
 ---- batch: 040 ----
mean loss: 128.96
 ---- batch: 050 ----
mean loss: 125.33
 ---- batch: 060 ----
mean loss: 129.81
 ---- batch: 070 ----
mean loss: 130.64
 ---- batch: 080 ----
mean loss: 133.66
 ---- batch: 090 ----
mean loss: 134.44
train mean loss: 129.35
epoch train time: 0:00:01.711824
elapsed time: 0:04:45.301056
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 20:36:58.742243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.29
 ---- batch: 020 ----
mean loss: 124.10
 ---- batch: 030 ----
mean loss: 120.39
 ---- batch: 040 ----
mean loss: 128.40
 ---- batch: 050 ----
mean loss: 133.90
 ---- batch: 060 ----
mean loss: 132.13
 ---- batch: 070 ----
mean loss: 128.66
 ---- batch: 080 ----
mean loss: 130.00
 ---- batch: 090 ----
mean loss: 130.15
train mean loss: 128.53
epoch train time: 0:00:01.690601
elapsed time: 0:04:46.992195
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 20:37:00.433544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.82
 ---- batch: 020 ----
mean loss: 127.92
 ---- batch: 030 ----
mean loss: 128.09
 ---- batch: 040 ----
mean loss: 134.15
 ---- batch: 050 ----
mean loss: 120.40
 ---- batch: 060 ----
mean loss: 126.99
 ---- batch: 070 ----
mean loss: 126.35
 ---- batch: 080 ----
mean loss: 132.59
 ---- batch: 090 ----
mean loss: 129.64
train mean loss: 128.20
epoch train time: 0:00:01.692534
elapsed time: 0:04:48.685306
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 20:37:02.126670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.27
 ---- batch: 020 ----
mean loss: 128.10
 ---- batch: 030 ----
mean loss: 122.71
 ---- batch: 040 ----
mean loss: 126.32
 ---- batch: 050 ----
mean loss: 129.87
 ---- batch: 060 ----
mean loss: 129.56
 ---- batch: 070 ----
mean loss: 127.88
 ---- batch: 080 ----
mean loss: 128.10
 ---- batch: 090 ----
mean loss: 132.73
train mean loss: 128.04
epoch train time: 0:00:01.709176
elapsed time: 0:04:50.395138
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 20:37:03.836538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.03
 ---- batch: 020 ----
mean loss: 125.34
 ---- batch: 030 ----
mean loss: 127.53
 ---- batch: 040 ----
mean loss: 130.76
 ---- batch: 050 ----
mean loss: 122.70
 ---- batch: 060 ----
mean loss: 127.40
 ---- batch: 070 ----
mean loss: 126.75
 ---- batch: 080 ----
mean loss: 132.34
 ---- batch: 090 ----
mean loss: 130.66
train mean loss: 126.71
epoch train time: 0:00:01.709453
elapsed time: 0:04:52.105255
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 20:37:05.546619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.82
 ---- batch: 020 ----
mean loss: 119.53
 ---- batch: 030 ----
mean loss: 130.00
 ---- batch: 040 ----
mean loss: 126.00
 ---- batch: 050 ----
mean loss: 127.68
 ---- batch: 060 ----
mean loss: 127.24
 ---- batch: 070 ----
mean loss: 124.63
 ---- batch: 080 ----
mean loss: 129.38
 ---- batch: 090 ----
mean loss: 128.77
train mean loss: 126.56
epoch train time: 0:00:01.688276
elapsed time: 0:04:53.794151
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 20:37:07.235568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.59
 ---- batch: 020 ----
mean loss: 126.56
 ---- batch: 030 ----
mean loss: 117.59
 ---- batch: 040 ----
mean loss: 128.36
 ---- batch: 050 ----
mean loss: 130.23
 ---- batch: 060 ----
mean loss: 130.52
 ---- batch: 070 ----
mean loss: 126.67
 ---- batch: 080 ----
mean loss: 130.15
 ---- batch: 090 ----
mean loss: 123.02
train mean loss: 126.07
epoch train time: 0:00:01.717246
elapsed time: 0:04:55.512110
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 20:37:08.953479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.08
 ---- batch: 020 ----
mean loss: 121.78
 ---- batch: 030 ----
mean loss: 125.00
 ---- batch: 040 ----
mean loss: 121.85
 ---- batch: 050 ----
mean loss: 123.40
 ---- batch: 060 ----
mean loss: 130.06
 ---- batch: 070 ----
mean loss: 126.95
 ---- batch: 080 ----
mean loss: 128.10
 ---- batch: 090 ----
mean loss: 126.32
train mean loss: 125.21
epoch train time: 0:00:01.700656
elapsed time: 0:04:57.213395
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 20:37:10.654762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.64
 ---- batch: 020 ----
mean loss: 117.70
 ---- batch: 030 ----
mean loss: 127.07
 ---- batch: 040 ----
mean loss: 124.94
 ---- batch: 050 ----
mean loss: 120.14
 ---- batch: 060 ----
mean loss: 126.05
 ---- batch: 070 ----
mean loss: 132.17
 ---- batch: 080 ----
mean loss: 131.40
 ---- batch: 090 ----
mean loss: 126.03
train mean loss: 125.21
epoch train time: 0:00:01.688709
elapsed time: 0:04:58.902699
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 20:37:12.344059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.82
 ---- batch: 020 ----
mean loss: 126.68
 ---- batch: 030 ----
mean loss: 121.80
 ---- batch: 040 ----
mean loss: 122.94
 ---- batch: 050 ----
mean loss: 121.67
 ---- batch: 060 ----
mean loss: 124.93
 ---- batch: 070 ----
mean loss: 123.27
 ---- batch: 080 ----
mean loss: 127.28
 ---- batch: 090 ----
mean loss: 123.36
train mean loss: 124.55
epoch train time: 0:00:01.699669
elapsed time: 0:05:00.603057
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 20:37:14.044482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.56
 ---- batch: 020 ----
mean loss: 119.08
 ---- batch: 030 ----
mean loss: 127.35
 ---- batch: 040 ----
mean loss: 120.66
 ---- batch: 050 ----
mean loss: 126.75
 ---- batch: 060 ----
mean loss: 118.51
 ---- batch: 070 ----
mean loss: 121.43
 ---- batch: 080 ----
mean loss: 129.48
 ---- batch: 090 ----
mean loss: 129.31
train mean loss: 124.12
epoch train time: 0:00:01.679809
elapsed time: 0:05:02.283566
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 20:37:15.724919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.68
 ---- batch: 020 ----
mean loss: 116.99
 ---- batch: 030 ----
mean loss: 121.00
 ---- batch: 040 ----
mean loss: 127.43
 ---- batch: 050 ----
mean loss: 124.66
 ---- batch: 060 ----
mean loss: 126.22
 ---- batch: 070 ----
mean loss: 128.33
 ---- batch: 080 ----
mean loss: 124.86
 ---- batch: 090 ----
mean loss: 128.66
train mean loss: 123.81
epoch train time: 0:00:01.698744
elapsed time: 0:05:03.982918
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 20:37:17.424265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.43
 ---- batch: 020 ----
mean loss: 118.29
 ---- batch: 030 ----
mean loss: 116.93
 ---- batch: 040 ----
mean loss: 124.09
 ---- batch: 050 ----
mean loss: 122.51
 ---- batch: 060 ----
mean loss: 128.36
 ---- batch: 070 ----
mean loss: 129.96
 ---- batch: 080 ----
mean loss: 125.20
 ---- batch: 090 ----
mean loss: 119.76
train mean loss: 122.77
epoch train time: 0:00:01.691664
elapsed time: 0:05:05.675183
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 20:37:19.116544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.47
 ---- batch: 020 ----
mean loss: 116.75
 ---- batch: 030 ----
mean loss: 122.51
 ---- batch: 040 ----
mean loss: 123.95
 ---- batch: 050 ----
mean loss: 118.76
 ---- batch: 060 ----
mean loss: 121.24
 ---- batch: 070 ----
mean loss: 126.92
 ---- batch: 080 ----
mean loss: 123.70
 ---- batch: 090 ----
mean loss: 128.68
train mean loss: 123.10
epoch train time: 0:00:01.709655
elapsed time: 0:05:07.385478
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 20:37:20.826884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.06
 ---- batch: 020 ----
mean loss: 127.52
 ---- batch: 030 ----
mean loss: 124.14
 ---- batch: 040 ----
mean loss: 121.24
 ---- batch: 050 ----
mean loss: 116.04
 ---- batch: 060 ----
mean loss: 126.06
 ---- batch: 070 ----
mean loss: 124.46
 ---- batch: 080 ----
mean loss: 126.75
 ---- batch: 090 ----
mean loss: 119.20
train mean loss: 121.88
epoch train time: 0:00:01.681357
elapsed time: 0:05:09.067491
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 20:37:22.508860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.08
 ---- batch: 020 ----
mean loss: 118.95
 ---- batch: 030 ----
mean loss: 114.98
 ---- batch: 040 ----
mean loss: 119.14
 ---- batch: 050 ----
mean loss: 119.58
 ---- batch: 060 ----
mean loss: 125.46
 ---- batch: 070 ----
mean loss: 125.54
 ---- batch: 080 ----
mean loss: 119.59
 ---- batch: 090 ----
mean loss: 125.35
train mean loss: 121.42
epoch train time: 0:00:01.724957
elapsed time: 0:05:10.793056
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 20:37:24.234465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.49
 ---- batch: 020 ----
mean loss: 118.23
 ---- batch: 030 ----
mean loss: 115.90
 ---- batch: 040 ----
mean loss: 116.08
 ---- batch: 050 ----
mean loss: 122.34
 ---- batch: 060 ----
mean loss: 125.51
 ---- batch: 070 ----
mean loss: 124.03
 ---- batch: 080 ----
mean loss: 119.32
 ---- batch: 090 ----
mean loss: 120.46
train mean loss: 120.95
epoch train time: 0:00:01.698681
elapsed time: 0:05:12.492508
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 20:37:25.933902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.48
 ---- batch: 020 ----
mean loss: 119.77
 ---- batch: 030 ----
mean loss: 117.62
 ---- batch: 040 ----
mean loss: 123.52
 ---- batch: 050 ----
mean loss: 124.30
 ---- batch: 060 ----
mean loss: 121.48
 ---- batch: 070 ----
mean loss: 119.79
 ---- batch: 080 ----
mean loss: 118.87
 ---- batch: 090 ----
mean loss: 123.20
train mean loss: 120.63
epoch train time: 0:00:01.705415
elapsed time: 0:05:14.198596
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 20:37:27.639966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.73
 ---- batch: 020 ----
mean loss: 118.77
 ---- batch: 030 ----
mean loss: 119.13
 ---- batch: 040 ----
mean loss: 119.40
 ---- batch: 050 ----
mean loss: 116.66
 ---- batch: 060 ----
mean loss: 120.97
 ---- batch: 070 ----
mean loss: 114.37
 ---- batch: 080 ----
mean loss: 126.34
 ---- batch: 090 ----
mean loss: 123.22
train mean loss: 120.27
epoch train time: 0:00:01.723289
elapsed time: 0:05:15.922513
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 20:37:29.363865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.38
 ---- batch: 020 ----
mean loss: 116.39
 ---- batch: 030 ----
mean loss: 120.47
 ---- batch: 040 ----
mean loss: 117.45
 ---- batch: 050 ----
mean loss: 120.29
 ---- batch: 060 ----
mean loss: 117.53
 ---- batch: 070 ----
mean loss: 126.93
 ---- batch: 080 ----
mean loss: 119.10
 ---- batch: 090 ----
mean loss: 124.31
train mean loss: 120.07
epoch train time: 0:00:01.679837
elapsed time: 0:05:17.602960
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 20:37:31.044345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.95
 ---- batch: 020 ----
mean loss: 126.04
 ---- batch: 030 ----
mean loss: 116.28
 ---- batch: 040 ----
mean loss: 113.93
 ---- batch: 050 ----
mean loss: 121.14
 ---- batch: 060 ----
mean loss: 125.10
 ---- batch: 070 ----
mean loss: 118.32
 ---- batch: 080 ----
mean loss: 120.37
 ---- batch: 090 ----
mean loss: 124.94
train mean loss: 120.35
epoch train time: 0:00:01.691513
elapsed time: 0:05:19.295186
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 20:37:32.736572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.84
 ---- batch: 020 ----
mean loss: 113.43
 ---- batch: 030 ----
mean loss: 117.76
 ---- batch: 040 ----
mean loss: 122.54
 ---- batch: 050 ----
mean loss: 119.47
 ---- batch: 060 ----
mean loss: 121.40
 ---- batch: 070 ----
mean loss: 119.71
 ---- batch: 080 ----
mean loss: 122.51
 ---- batch: 090 ----
mean loss: 120.46
train mean loss: 119.51
epoch train time: 0:00:01.675418
elapsed time: 0:05:20.971302
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 20:37:34.412693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.95
 ---- batch: 020 ----
mean loss: 120.68
 ---- batch: 030 ----
mean loss: 117.15
 ---- batch: 040 ----
mean loss: 116.86
 ---- batch: 050 ----
mean loss: 123.64
 ---- batch: 060 ----
mean loss: 116.83
 ---- batch: 070 ----
mean loss: 122.86
 ---- batch: 080 ----
mean loss: 119.01
 ---- batch: 090 ----
mean loss: 118.67
train mean loss: 118.44
epoch train time: 0:00:01.668616
elapsed time: 0:05:22.640605
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 20:37:36.082031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.47
 ---- batch: 020 ----
mean loss: 115.95
 ---- batch: 030 ----
mean loss: 116.99
 ---- batch: 040 ----
mean loss: 112.54
 ---- batch: 050 ----
mean loss: 117.31
 ---- batch: 060 ----
mean loss: 120.32
 ---- batch: 070 ----
mean loss: 117.59
 ---- batch: 080 ----
mean loss: 118.42
 ---- batch: 090 ----
mean loss: 124.72
train mean loss: 117.81
epoch train time: 0:00:01.679660
elapsed time: 0:05:24.320981
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 20:37:37.762325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.77
 ---- batch: 020 ----
mean loss: 112.73
 ---- batch: 030 ----
mean loss: 117.52
 ---- batch: 040 ----
mean loss: 119.39
 ---- batch: 050 ----
mean loss: 116.51
 ---- batch: 060 ----
mean loss: 114.60
 ---- batch: 070 ----
mean loss: 119.66
 ---- batch: 080 ----
mean loss: 119.04
 ---- batch: 090 ----
mean loss: 116.99
train mean loss: 117.61
epoch train time: 0:00:01.711910
elapsed time: 0:05:26.033544
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 20:37:39.474904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.40
 ---- batch: 020 ----
mean loss: 112.24
 ---- batch: 030 ----
mean loss: 119.50
 ---- batch: 040 ----
mean loss: 111.23
 ---- batch: 050 ----
mean loss: 116.37
 ---- batch: 060 ----
mean loss: 122.69
 ---- batch: 070 ----
mean loss: 120.06
 ---- batch: 080 ----
mean loss: 121.19
 ---- batch: 090 ----
mean loss: 116.85
train mean loss: 117.10
epoch train time: 0:00:01.693586
elapsed time: 0:05:27.728088
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 20:37:41.169192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.63
 ---- batch: 020 ----
mean loss: 118.22
 ---- batch: 030 ----
mean loss: 117.53
 ---- batch: 040 ----
mean loss: 111.97
 ---- batch: 050 ----
mean loss: 112.82
 ---- batch: 060 ----
mean loss: 121.22
 ---- batch: 070 ----
mean loss: 116.95
 ---- batch: 080 ----
mean loss: 117.35
 ---- batch: 090 ----
mean loss: 120.06
train mean loss: 116.58
epoch train time: 0:00:01.676623
elapsed time: 0:05:29.405068
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 20:37:42.846434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.55
 ---- batch: 020 ----
mean loss: 110.43
 ---- batch: 030 ----
mean loss: 115.84
 ---- batch: 040 ----
mean loss: 116.79
 ---- batch: 050 ----
mean loss: 117.98
 ---- batch: 060 ----
mean loss: 120.54
 ---- batch: 070 ----
mean loss: 119.29
 ---- batch: 080 ----
mean loss: 120.56
 ---- batch: 090 ----
mean loss: 117.34
train mean loss: 117.45
epoch train time: 0:00:01.693415
elapsed time: 0:05:31.099135
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 20:37:44.540510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.84
 ---- batch: 020 ----
mean loss: 114.81
 ---- batch: 030 ----
mean loss: 110.16
 ---- batch: 040 ----
mean loss: 117.86
 ---- batch: 050 ----
mean loss: 119.01
 ---- batch: 060 ----
mean loss: 120.87
 ---- batch: 070 ----
mean loss: 117.13
 ---- batch: 080 ----
mean loss: 115.06
 ---- batch: 090 ----
mean loss: 123.11
train mean loss: 116.16
epoch train time: 0:00:01.681791
elapsed time: 0:05:32.781583
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 20:37:46.222994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.65
 ---- batch: 020 ----
mean loss: 107.27
 ---- batch: 030 ----
mean loss: 115.74
 ---- batch: 040 ----
mean loss: 113.68
 ---- batch: 050 ----
mean loss: 113.79
 ---- batch: 060 ----
mean loss: 118.41
 ---- batch: 070 ----
mean loss: 118.20
 ---- batch: 080 ----
mean loss: 120.37
 ---- batch: 090 ----
mean loss: 113.65
train mean loss: 115.19
epoch train time: 0:00:01.703796
elapsed time: 0:05:34.486126
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 20:37:47.927493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.74
 ---- batch: 020 ----
mean loss: 118.52
 ---- batch: 030 ----
mean loss: 112.10
 ---- batch: 040 ----
mean loss: 115.34
 ---- batch: 050 ----
mean loss: 115.08
 ---- batch: 060 ----
mean loss: 117.06
 ---- batch: 070 ----
mean loss: 115.06
 ---- batch: 080 ----
mean loss: 117.03
 ---- batch: 090 ----
mean loss: 112.93
train mean loss: 115.14
epoch train time: 0:00:01.714040
elapsed time: 0:05:36.200786
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 20:37:49.642144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.26
 ---- batch: 020 ----
mean loss: 111.20
 ---- batch: 030 ----
mean loss: 113.10
 ---- batch: 040 ----
mean loss: 116.10
 ---- batch: 050 ----
mean loss: 116.50
 ---- batch: 060 ----
mean loss: 113.46
 ---- batch: 070 ----
mean loss: 113.30
 ---- batch: 080 ----
mean loss: 115.34
 ---- batch: 090 ----
mean loss: 117.44
train mean loss: 114.45
epoch train time: 0:00:01.670170
elapsed time: 0:05:37.871566
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 20:37:51.312950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.20
 ---- batch: 020 ----
mean loss: 113.53
 ---- batch: 030 ----
mean loss: 112.16
 ---- batch: 040 ----
mean loss: 115.50
 ---- batch: 050 ----
mean loss: 110.40
 ---- batch: 060 ----
mean loss: 107.83
 ---- batch: 070 ----
mean loss: 114.96
 ---- batch: 080 ----
mean loss: 117.49
 ---- batch: 090 ----
mean loss: 117.42
train mean loss: 113.79
epoch train time: 0:00:01.695357
elapsed time: 0:05:39.567738
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 20:37:53.009101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.08
 ---- batch: 020 ----
mean loss: 112.14
 ---- batch: 030 ----
mean loss: 109.98
 ---- batch: 040 ----
mean loss: 115.82
 ---- batch: 050 ----
mean loss: 112.55
 ---- batch: 060 ----
mean loss: 122.26
 ---- batch: 070 ----
mean loss: 114.72
 ---- batch: 080 ----
mean loss: 116.10
 ---- batch: 090 ----
mean loss: 117.94
train mean loss: 114.49
epoch train time: 0:00:01.682749
elapsed time: 0:05:41.251151
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 20:37:54.692517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.64
 ---- batch: 020 ----
mean loss: 113.39
 ---- batch: 030 ----
mean loss: 113.48
 ---- batch: 040 ----
mean loss: 116.40
 ---- batch: 050 ----
mean loss: 112.44
 ---- batch: 060 ----
mean loss: 111.24
 ---- batch: 070 ----
mean loss: 113.69
 ---- batch: 080 ----
mean loss: 114.60
 ---- batch: 090 ----
mean loss: 114.38
train mean loss: 113.76
epoch train time: 0:00:01.685463
elapsed time: 0:05:42.937222
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 20:37:56.378666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.53
 ---- batch: 020 ----
mean loss: 110.97
 ---- batch: 030 ----
mean loss: 105.81
 ---- batch: 040 ----
mean loss: 109.65
 ---- batch: 050 ----
mean loss: 110.64
 ---- batch: 060 ----
mean loss: 111.89
 ---- batch: 070 ----
mean loss: 117.25
 ---- batch: 080 ----
mean loss: 116.21
 ---- batch: 090 ----
mean loss: 111.23
train mean loss: 112.66
epoch train time: 0:00:01.672198
elapsed time: 0:05:44.610204
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 20:37:58.051589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.54
 ---- batch: 020 ----
mean loss: 110.41
 ---- batch: 030 ----
mean loss: 109.21
 ---- batch: 040 ----
mean loss: 113.88
 ---- batch: 050 ----
mean loss: 110.99
 ---- batch: 060 ----
mean loss: 119.34
 ---- batch: 070 ----
mean loss: 112.79
 ---- batch: 080 ----
mean loss: 112.93
 ---- batch: 090 ----
mean loss: 112.93
train mean loss: 112.66
epoch train time: 0:00:01.698926
elapsed time: 0:05:46.309779
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 20:37:59.751166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.51
 ---- batch: 020 ----
mean loss: 106.47
 ---- batch: 030 ----
mean loss: 104.93
 ---- batch: 040 ----
mean loss: 116.80
 ---- batch: 050 ----
mean loss: 114.62
 ---- batch: 060 ----
mean loss: 115.50
 ---- batch: 070 ----
mean loss: 113.84
 ---- batch: 080 ----
mean loss: 113.98
 ---- batch: 090 ----
mean loss: 119.74
train mean loss: 112.20
epoch train time: 0:00:01.677123
elapsed time: 0:05:47.987506
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 20:38:01.428886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.77
 ---- batch: 020 ----
mean loss: 108.56
 ---- batch: 030 ----
mean loss: 109.08
 ---- batch: 040 ----
mean loss: 107.71
 ---- batch: 050 ----
mean loss: 110.70
 ---- batch: 060 ----
mean loss: 112.99
 ---- batch: 070 ----
mean loss: 116.97
 ---- batch: 080 ----
mean loss: 110.70
 ---- batch: 090 ----
mean loss: 118.06
train mean loss: 112.16
epoch train time: 0:00:01.682699
elapsed time: 0:05:49.670842
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 20:38:03.112324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.43
 ---- batch: 020 ----
mean loss: 113.71
 ---- batch: 030 ----
mean loss: 108.52
 ---- batch: 040 ----
mean loss: 110.20
 ---- batch: 050 ----
mean loss: 111.40
 ---- batch: 060 ----
mean loss: 114.03
 ---- batch: 070 ----
mean loss: 110.38
 ---- batch: 080 ----
mean loss: 114.29
 ---- batch: 090 ----
mean loss: 114.99
train mean loss: 112.18
epoch train time: 0:00:01.699547
elapsed time: 0:05:51.371158
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 20:38:04.812595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.37
 ---- batch: 020 ----
mean loss: 107.67
 ---- batch: 030 ----
mean loss: 107.12
 ---- batch: 040 ----
mean loss: 111.04
 ---- batch: 050 ----
mean loss: 108.59
 ---- batch: 060 ----
mean loss: 111.90
 ---- batch: 070 ----
mean loss: 113.56
 ---- batch: 080 ----
mean loss: 118.61
 ---- batch: 090 ----
mean loss: 114.93
train mean loss: 111.48
epoch train time: 0:00:01.660775
elapsed time: 0:05:53.032657
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 20:38:06.474116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.96
 ---- batch: 020 ----
mean loss: 109.74
 ---- batch: 030 ----
mean loss: 110.13
 ---- batch: 040 ----
mean loss: 107.78
 ---- batch: 050 ----
mean loss: 107.21
 ---- batch: 060 ----
mean loss: 106.37
 ---- batch: 070 ----
mean loss: 114.89
 ---- batch: 080 ----
mean loss: 119.26
 ---- batch: 090 ----
mean loss: 116.41
train mean loss: 110.37
epoch train time: 0:00:01.689270
elapsed time: 0:05:54.722672
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 20:38:08.164095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.57
 ---- batch: 020 ----
mean loss: 108.03
 ---- batch: 030 ----
mean loss: 110.85
 ---- batch: 040 ----
mean loss: 110.11
 ---- batch: 050 ----
mean loss: 113.18
 ---- batch: 060 ----
mean loss: 111.17
 ---- batch: 070 ----
mean loss: 113.47
 ---- batch: 080 ----
mean loss: 112.41
 ---- batch: 090 ----
mean loss: 109.42
train mean loss: 110.50
epoch train time: 0:00:01.675689
elapsed time: 0:05:56.399049
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 20:38:09.840432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.15
 ---- batch: 020 ----
mean loss: 104.80
 ---- batch: 030 ----
mean loss: 109.65
 ---- batch: 040 ----
mean loss: 109.59
 ---- batch: 050 ----
mean loss: 111.47
 ---- batch: 060 ----
mean loss: 111.63
 ---- batch: 070 ----
mean loss: 119.36
 ---- batch: 080 ----
mean loss: 115.76
 ---- batch: 090 ----
mean loss: 109.30
train mean loss: 110.31
epoch train time: 0:00:01.657900
elapsed time: 0:05:58.057602
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 20:38:11.498974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.90
 ---- batch: 020 ----
mean loss: 106.54
 ---- batch: 030 ----
mean loss: 113.88
 ---- batch: 040 ----
mean loss: 113.52
 ---- batch: 050 ----
mean loss: 112.31
 ---- batch: 060 ----
mean loss: 111.94
 ---- batch: 070 ----
mean loss: 108.78
 ---- batch: 080 ----
mean loss: 105.32
 ---- batch: 090 ----
mean loss: 114.15
train mean loss: 110.16
epoch train time: 0:00:01.684175
elapsed time: 0:05:59.742445
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 20:38:13.183804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.03
 ---- batch: 020 ----
mean loss: 106.43
 ---- batch: 030 ----
mean loss: 106.51
 ---- batch: 040 ----
mean loss: 108.61
 ---- batch: 050 ----
mean loss: 110.55
 ---- batch: 060 ----
mean loss: 102.13
 ---- batch: 070 ----
mean loss: 108.63
 ---- batch: 080 ----
mean loss: 113.19
 ---- batch: 090 ----
mean loss: 120.19
train mean loss: 109.64
epoch train time: 0:00:01.676702
elapsed time: 0:06:01.419794
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 20:38:14.861172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.94
 ---- batch: 020 ----
mean loss: 110.17
 ---- batch: 030 ----
mean loss: 105.06
 ---- batch: 040 ----
mean loss: 107.80
 ---- batch: 050 ----
mean loss: 103.61
 ---- batch: 060 ----
mean loss: 109.57
 ---- batch: 070 ----
mean loss: 109.10
 ---- batch: 080 ----
mean loss: 114.00
 ---- batch: 090 ----
mean loss: 109.14
train mean loss: 108.83
epoch train time: 0:00:01.680671
elapsed time: 0:06:03.101125
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 20:38:16.542487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.71
 ---- batch: 020 ----
mean loss: 105.62
 ---- batch: 030 ----
mean loss: 110.16
 ---- batch: 040 ----
mean loss: 106.28
 ---- batch: 050 ----
mean loss: 109.13
 ---- batch: 060 ----
mean loss: 111.02
 ---- batch: 070 ----
mean loss: 110.77
 ---- batch: 080 ----
mean loss: 111.35
 ---- batch: 090 ----
mean loss: 109.05
train mean loss: 108.58
epoch train time: 0:00:01.688473
elapsed time: 0:06:04.790171
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 20:38:18.231537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.07
 ---- batch: 020 ----
mean loss: 101.90
 ---- batch: 030 ----
mean loss: 104.68
 ---- batch: 040 ----
mean loss: 108.55
 ---- batch: 050 ----
mean loss: 104.94
 ---- batch: 060 ----
mean loss: 113.56
 ---- batch: 070 ----
mean loss: 109.10
 ---- batch: 080 ----
mean loss: 102.53
 ---- batch: 090 ----
mean loss: 115.96
train mean loss: 108.18
epoch train time: 0:00:01.695426
elapsed time: 0:06:06.486225
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 20:38:19.927642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.29
 ---- batch: 020 ----
mean loss: 108.40
 ---- batch: 030 ----
mean loss: 104.62
 ---- batch: 040 ----
mean loss: 101.50
 ---- batch: 050 ----
mean loss: 104.78
 ---- batch: 060 ----
mean loss: 111.58
 ---- batch: 070 ----
mean loss: 109.90
 ---- batch: 080 ----
mean loss: 109.64
 ---- batch: 090 ----
mean loss: 115.87
train mean loss: 108.24
epoch train time: 0:00:01.672367
elapsed time: 0:06:08.159263
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 20:38:21.600641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.82
 ---- batch: 020 ----
mean loss: 104.55
 ---- batch: 030 ----
mean loss: 103.49
 ---- batch: 040 ----
mean loss: 108.75
 ---- batch: 050 ----
mean loss: 109.98
 ---- batch: 060 ----
mean loss: 109.08
 ---- batch: 070 ----
mean loss: 107.46
 ---- batch: 080 ----
mean loss: 116.16
 ---- batch: 090 ----
mean loss: 108.35
train mean loss: 107.37
epoch train time: 0:00:01.660780
elapsed time: 0:06:09.820671
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 20:38:23.262085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.14
 ---- batch: 020 ----
mean loss: 100.42
 ---- batch: 030 ----
mean loss: 107.66
 ---- batch: 040 ----
mean loss: 109.44
 ---- batch: 050 ----
mean loss: 109.41
 ---- batch: 060 ----
mean loss: 108.84
 ---- batch: 070 ----
mean loss: 107.27
 ---- batch: 080 ----
mean loss: 111.70
 ---- batch: 090 ----
mean loss: 108.48
train mean loss: 107.38
epoch train time: 0:00:01.685746
elapsed time: 0:06:11.507079
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 20:38:24.948482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.14
 ---- batch: 020 ----
mean loss: 105.78
 ---- batch: 030 ----
mean loss: 100.70
 ---- batch: 040 ----
mean loss: 106.76
 ---- batch: 050 ----
mean loss: 102.95
 ---- batch: 060 ----
mean loss: 106.15
 ---- batch: 070 ----
mean loss: 112.50
 ---- batch: 080 ----
mean loss: 114.83
 ---- batch: 090 ----
mean loss: 109.61
train mean loss: 107.49
epoch train time: 0:00:01.704385
elapsed time: 0:06:13.212171
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 20:38:26.653562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.28
 ---- batch: 020 ----
mean loss: 103.07
 ---- batch: 030 ----
mean loss: 97.67
 ---- batch: 040 ----
mean loss: 103.58
 ---- batch: 050 ----
mean loss: 98.71
 ---- batch: 060 ----
mean loss: 99.38
 ---- batch: 070 ----
mean loss: 97.95
 ---- batch: 080 ----
mean loss: 103.40
 ---- batch: 090 ----
mean loss: 104.95
train mean loss: 101.76
epoch train time: 0:00:01.677379
elapsed time: 0:06:14.890516
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 20:38:28.331662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.15
 ---- batch: 020 ----
mean loss: 100.36
 ---- batch: 030 ----
mean loss: 99.69
 ---- batch: 040 ----
mean loss: 98.88
 ---- batch: 050 ----
mean loss: 102.42
 ---- batch: 060 ----
mean loss: 104.35
 ---- batch: 070 ----
mean loss: 99.25
 ---- batch: 080 ----
mean loss: 99.83
 ---- batch: 090 ----
mean loss: 98.73
train mean loss: 100.90
epoch train time: 0:00:01.710412
elapsed time: 0:06:16.601450
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 20:38:30.042818
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.53
 ---- batch: 020 ----
mean loss: 97.30
 ---- batch: 030 ----
mean loss: 100.55
 ---- batch: 040 ----
mean loss: 103.26
 ---- batch: 050 ----
mean loss: 102.14
 ---- batch: 060 ----
mean loss: 97.56
 ---- batch: 070 ----
mean loss: 96.10
 ---- batch: 080 ----
mean loss: 103.29
 ---- batch: 090 ----
mean loss: 98.78
train mean loss: 100.49
epoch train time: 0:00:01.695062
elapsed time: 0:06:18.297196
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 20:38:31.738557
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.18
 ---- batch: 020 ----
mean loss: 101.49
 ---- batch: 030 ----
mean loss: 102.57
 ---- batch: 040 ----
mean loss: 94.40
 ---- batch: 050 ----
mean loss: 100.96
 ---- batch: 060 ----
mean loss: 98.19
 ---- batch: 070 ----
mean loss: 97.20
 ---- batch: 080 ----
mean loss: 109.88
 ---- batch: 090 ----
mean loss: 100.38
train mean loss: 100.39
epoch train time: 0:00:01.678749
elapsed time: 0:06:19.976600
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 20:38:33.417994
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.16
 ---- batch: 020 ----
mean loss: 101.34
 ---- batch: 030 ----
mean loss: 98.64
 ---- batch: 040 ----
mean loss: 94.66
 ---- batch: 050 ----
mean loss: 103.20
 ---- batch: 060 ----
mean loss: 103.37
 ---- batch: 070 ----
mean loss: 100.88
 ---- batch: 080 ----
mean loss: 103.73
 ---- batch: 090 ----
mean loss: 95.78
train mean loss: 100.38
epoch train time: 0:00:01.669500
elapsed time: 0:06:21.646787
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 20:38:35.088166
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.00
 ---- batch: 020 ----
mean loss: 95.30
 ---- batch: 030 ----
mean loss: 100.47
 ---- batch: 040 ----
mean loss: 99.19
 ---- batch: 050 ----
mean loss: 101.28
 ---- batch: 060 ----
mean loss: 99.60
 ---- batch: 070 ----
mean loss: 102.71
 ---- batch: 080 ----
mean loss: 101.67
 ---- batch: 090 ----
mean loss: 103.08
train mean loss: 100.31
epoch train time: 0:00:01.674752
elapsed time: 0:06:23.322200
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 20:38:36.763576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.07
 ---- batch: 020 ----
mean loss: 101.02
 ---- batch: 030 ----
mean loss: 105.16
 ---- batch: 040 ----
mean loss: 104.83
 ---- batch: 050 ----
mean loss: 99.57
 ---- batch: 060 ----
mean loss: 93.64
 ---- batch: 070 ----
mean loss: 99.49
 ---- batch: 080 ----
mean loss: 99.88
 ---- batch: 090 ----
mean loss: 101.14
train mean loss: 100.37
epoch train time: 0:00:01.670367
elapsed time: 0:06:24.993221
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 20:38:38.434576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.17
 ---- batch: 020 ----
mean loss: 106.31
 ---- batch: 030 ----
mean loss: 97.11
 ---- batch: 040 ----
mean loss: 99.80
 ---- batch: 050 ----
mean loss: 101.99
 ---- batch: 060 ----
mean loss: 95.41
 ---- batch: 070 ----
mean loss: 98.55
 ---- batch: 080 ----
mean loss: 102.07
 ---- batch: 090 ----
mean loss: 97.24
train mean loss: 100.06
epoch train time: 0:00:01.701189
elapsed time: 0:06:26.694996
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 20:38:40.136424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.52
 ---- batch: 020 ----
mean loss: 98.24
 ---- batch: 030 ----
mean loss: 98.89
 ---- batch: 040 ----
mean loss: 98.91
 ---- batch: 050 ----
mean loss: 101.85
 ---- batch: 060 ----
mean loss: 102.26
 ---- batch: 070 ----
mean loss: 99.99
 ---- batch: 080 ----
mean loss: 100.68
 ---- batch: 090 ----
mean loss: 102.19
train mean loss: 100.13
epoch train time: 0:00:01.704889
elapsed time: 0:06:28.400633
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 20:38:41.841999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.50
 ---- batch: 020 ----
mean loss: 97.98
 ---- batch: 030 ----
mean loss: 104.44
 ---- batch: 040 ----
mean loss: 101.37
 ---- batch: 050 ----
mean loss: 99.34
 ---- batch: 060 ----
mean loss: 103.00
 ---- batch: 070 ----
mean loss: 95.20
 ---- batch: 080 ----
mean loss: 101.96
 ---- batch: 090 ----
mean loss: 100.98
train mean loss: 100.00
epoch train time: 0:00:01.678071
elapsed time: 0:06:30.079319
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 20:38:43.520690
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.60
 ---- batch: 020 ----
mean loss: 93.90
 ---- batch: 030 ----
mean loss: 100.21
 ---- batch: 040 ----
mean loss: 99.17
 ---- batch: 050 ----
mean loss: 100.90
 ---- batch: 060 ----
mean loss: 99.25
 ---- batch: 070 ----
mean loss: 102.81
 ---- batch: 080 ----
mean loss: 98.77
 ---- batch: 090 ----
mean loss: 101.84
train mean loss: 99.93
epoch train time: 0:00:01.688801
elapsed time: 0:06:31.768720
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 20:38:45.210074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.55
 ---- batch: 020 ----
mean loss: 95.00
 ---- batch: 030 ----
mean loss: 102.25
 ---- batch: 040 ----
mean loss: 106.70
 ---- batch: 050 ----
mean loss: 99.20
 ---- batch: 060 ----
mean loss: 97.64
 ---- batch: 070 ----
mean loss: 95.68
 ---- batch: 080 ----
mean loss: 97.93
 ---- batch: 090 ----
mean loss: 101.75
train mean loss: 99.96
epoch train time: 0:00:01.683897
elapsed time: 0:06:33.453228
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 20:38:46.894573
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.85
 ---- batch: 020 ----
mean loss: 96.32
 ---- batch: 030 ----
mean loss: 96.23
 ---- batch: 040 ----
mean loss: 101.83
 ---- batch: 050 ----
mean loss: 104.74
 ---- batch: 060 ----
mean loss: 99.78
 ---- batch: 070 ----
mean loss: 102.21
 ---- batch: 080 ----
mean loss: 98.63
 ---- batch: 090 ----
mean loss: 99.42
train mean loss: 99.92
epoch train time: 0:00:01.690459
elapsed time: 0:06:35.144280
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 20:38:48.585683
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.52
 ---- batch: 020 ----
mean loss: 95.19
 ---- batch: 030 ----
mean loss: 100.51
 ---- batch: 040 ----
mean loss: 94.68
 ---- batch: 050 ----
mean loss: 97.44
 ---- batch: 060 ----
mean loss: 101.05
 ---- batch: 070 ----
mean loss: 101.78
 ---- batch: 080 ----
mean loss: 102.69
 ---- batch: 090 ----
mean loss: 104.90
train mean loss: 100.05
epoch train time: 0:00:01.697866
elapsed time: 0:06:36.842803
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 20:38:50.284153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.66
 ---- batch: 020 ----
mean loss: 102.68
 ---- batch: 030 ----
mean loss: 98.18
 ---- batch: 040 ----
mean loss: 97.21
 ---- batch: 050 ----
mean loss: 98.04
 ---- batch: 060 ----
mean loss: 98.45
 ---- batch: 070 ----
mean loss: 98.53
 ---- batch: 080 ----
mean loss: 102.88
 ---- batch: 090 ----
mean loss: 99.80
train mean loss: 99.78
epoch train time: 0:00:01.707791
elapsed time: 0:06:38.551223
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 20:38:51.992570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.02
 ---- batch: 020 ----
mean loss: 100.12
 ---- batch: 030 ----
mean loss: 97.99
 ---- batch: 040 ----
mean loss: 103.25
 ---- batch: 050 ----
mean loss: 97.13
 ---- batch: 060 ----
mean loss: 99.21
 ---- batch: 070 ----
mean loss: 96.94
 ---- batch: 080 ----
mean loss: 101.09
 ---- batch: 090 ----
mean loss: 99.60
train mean loss: 99.54
epoch train time: 0:00:01.676691
elapsed time: 0:06:40.228536
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 20:38:53.669902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.76
 ---- batch: 020 ----
mean loss: 101.82
 ---- batch: 030 ----
mean loss: 99.87
 ---- batch: 040 ----
mean loss: 96.83
 ---- batch: 050 ----
mean loss: 100.74
 ---- batch: 060 ----
mean loss: 96.12
 ---- batch: 070 ----
mean loss: 98.18
 ---- batch: 080 ----
mean loss: 103.54
 ---- batch: 090 ----
mean loss: 101.65
train mean loss: 99.73
epoch train time: 0:00:01.687873
elapsed time: 0:06:41.917003
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 20:38:55.358350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.88
 ---- batch: 020 ----
mean loss: 99.30
 ---- batch: 030 ----
mean loss: 101.75
 ---- batch: 040 ----
mean loss: 100.22
 ---- batch: 050 ----
mean loss: 97.54
 ---- batch: 060 ----
mean loss: 102.48
 ---- batch: 070 ----
mean loss: 95.97
 ---- batch: 080 ----
mean loss: 101.65
 ---- batch: 090 ----
mean loss: 106.00
train mean loss: 99.64
epoch train time: 0:00:01.680899
elapsed time: 0:06:43.598551
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 20:38:57.039965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.06
 ---- batch: 020 ----
mean loss: 99.35
 ---- batch: 030 ----
mean loss: 98.55
 ---- batch: 040 ----
mean loss: 106.09
 ---- batch: 050 ----
mean loss: 101.93
 ---- batch: 060 ----
mean loss: 91.74
 ---- batch: 070 ----
mean loss: 97.81
 ---- batch: 080 ----
mean loss: 100.53
 ---- batch: 090 ----
mean loss: 97.76
train mean loss: 99.69
epoch train time: 0:00:01.713778
elapsed time: 0:06:45.313053
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 20:38:58.754400
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.13
 ---- batch: 020 ----
mean loss: 99.03
 ---- batch: 030 ----
mean loss: 99.16
 ---- batch: 040 ----
mean loss: 100.16
 ---- batch: 050 ----
mean loss: 100.57
 ---- batch: 060 ----
mean loss: 102.19
 ---- batch: 070 ----
mean loss: 95.64
 ---- batch: 080 ----
mean loss: 103.66
 ---- batch: 090 ----
mean loss: 95.74
train mean loss: 99.41
epoch train time: 0:00:01.690366
elapsed time: 0:06:47.004014
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 20:39:00.445388
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.85
 ---- batch: 020 ----
mean loss: 99.45
 ---- batch: 030 ----
mean loss: 95.85
 ---- batch: 040 ----
mean loss: 102.66
 ---- batch: 050 ----
mean loss: 101.32
 ---- batch: 060 ----
mean loss: 101.39
 ---- batch: 070 ----
mean loss: 98.69
 ---- batch: 080 ----
mean loss: 98.81
 ---- batch: 090 ----
mean loss: 95.56
train mean loss: 99.50
epoch train time: 0:00:01.703462
elapsed time: 0:06:48.708127
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 20:39:02.149513
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.42
 ---- batch: 020 ----
mean loss: 99.03
 ---- batch: 030 ----
mean loss: 102.10
 ---- batch: 040 ----
mean loss: 97.74
 ---- batch: 050 ----
mean loss: 95.69
 ---- batch: 060 ----
mean loss: 98.13
 ---- batch: 070 ----
mean loss: 97.13
 ---- batch: 080 ----
mean loss: 106.28
 ---- batch: 090 ----
mean loss: 98.44
train mean loss: 99.53
epoch train time: 0:00:01.711906
elapsed time: 0:06:50.420806
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 20:39:03.862035
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.97
 ---- batch: 020 ----
mean loss: 101.47
 ---- batch: 030 ----
mean loss: 102.61
 ---- batch: 040 ----
mean loss: 97.12
 ---- batch: 050 ----
mean loss: 100.80
 ---- batch: 060 ----
mean loss: 95.62
 ---- batch: 070 ----
mean loss: 101.43
 ---- batch: 080 ----
mean loss: 100.74
 ---- batch: 090 ----
mean loss: 99.85
train mean loss: 99.43
epoch train time: 0:00:01.716996
elapsed time: 0:06:52.138307
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 20:39:05.579681
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.65
 ---- batch: 020 ----
mean loss: 89.99
 ---- batch: 030 ----
mean loss: 101.29
 ---- batch: 040 ----
mean loss: 98.20
 ---- batch: 050 ----
mean loss: 98.27
 ---- batch: 060 ----
mean loss: 97.96
 ---- batch: 070 ----
mean loss: 100.46
 ---- batch: 080 ----
mean loss: 103.73
 ---- batch: 090 ----
mean loss: 105.34
train mean loss: 99.50
epoch train time: 0:00:01.711776
elapsed time: 0:06:53.850766
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 20:39:07.292113
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.29
 ---- batch: 020 ----
mean loss: 103.19
 ---- batch: 030 ----
mean loss: 95.99
 ---- batch: 040 ----
mean loss: 101.08
 ---- batch: 050 ----
mean loss: 100.23
 ---- batch: 060 ----
mean loss: 99.17
 ---- batch: 070 ----
mean loss: 97.10
 ---- batch: 080 ----
mean loss: 101.35
 ---- batch: 090 ----
mean loss: 99.03
train mean loss: 99.40
epoch train time: 0:00:01.717511
elapsed time: 0:06:55.568964
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 20:39:09.010365
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.75
 ---- batch: 020 ----
mean loss: 104.12
 ---- batch: 030 ----
mean loss: 97.96
 ---- batch: 040 ----
mean loss: 98.55
 ---- batch: 050 ----
mean loss: 98.63
 ---- batch: 060 ----
mean loss: 101.01
 ---- batch: 070 ----
mean loss: 95.35
 ---- batch: 080 ----
mean loss: 102.21
 ---- batch: 090 ----
mean loss: 101.05
train mean loss: 99.40
epoch train time: 0:00:01.674524
elapsed time: 0:06:57.244266
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 20:39:10.685689
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.17
 ---- batch: 020 ----
mean loss: 100.92
 ---- batch: 030 ----
mean loss: 100.25
 ---- batch: 040 ----
mean loss: 102.41
 ---- batch: 050 ----
mean loss: 100.46
 ---- batch: 060 ----
mean loss: 97.92
 ---- batch: 070 ----
mean loss: 98.96
 ---- batch: 080 ----
mean loss: 99.52
 ---- batch: 090 ----
mean loss: 96.01
train mean loss: 99.21
epoch train time: 0:00:01.704223
elapsed time: 0:06:58.949207
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 20:39:12.390608
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.25
 ---- batch: 020 ----
mean loss: 95.82
 ---- batch: 030 ----
mean loss: 105.70
 ---- batch: 040 ----
mean loss: 103.92
 ---- batch: 050 ----
mean loss: 100.82
 ---- batch: 060 ----
mean loss: 99.34
 ---- batch: 070 ----
mean loss: 102.10
 ---- batch: 080 ----
mean loss: 93.26
 ---- batch: 090 ----
mean loss: 98.27
train mean loss: 99.07
epoch train time: 0:00:01.702920
elapsed time: 0:07:00.652837
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 20:39:14.094226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.68
 ---- batch: 020 ----
mean loss: 97.25
 ---- batch: 030 ----
mean loss: 99.04
 ---- batch: 040 ----
mean loss: 101.86
 ---- batch: 050 ----
mean loss: 102.37
 ---- batch: 060 ----
mean loss: 95.86
 ---- batch: 070 ----
mean loss: 100.06
 ---- batch: 080 ----
mean loss: 96.19
 ---- batch: 090 ----
mean loss: 101.14
train mean loss: 99.19
epoch train time: 0:00:01.649661
elapsed time: 0:07:02.303188
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 20:39:15.744572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.62
 ---- batch: 020 ----
mean loss: 103.21
 ---- batch: 030 ----
mean loss: 98.34
 ---- batch: 040 ----
mean loss: 94.12
 ---- batch: 050 ----
mean loss: 97.93
 ---- batch: 060 ----
mean loss: 98.18
 ---- batch: 070 ----
mean loss: 98.68
 ---- batch: 080 ----
mean loss: 101.92
 ---- batch: 090 ----
mean loss: 96.35
train mean loss: 99.21
epoch train time: 0:00:01.693160
elapsed time: 0:07:03.996961
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 20:39:17.438310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.82
 ---- batch: 020 ----
mean loss: 99.75
 ---- batch: 030 ----
mean loss: 101.08
 ---- batch: 040 ----
mean loss: 100.48
 ---- batch: 050 ----
mean loss: 99.40
 ---- batch: 060 ----
mean loss: 96.46
 ---- batch: 070 ----
mean loss: 99.05
 ---- batch: 080 ----
mean loss: 97.91
 ---- batch: 090 ----
mean loss: 97.00
train mean loss: 99.04
epoch train time: 0:00:01.678272
elapsed time: 0:07:05.675831
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 20:39:19.117188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.04
 ---- batch: 020 ----
mean loss: 95.95
 ---- batch: 030 ----
mean loss: 100.91
 ---- batch: 040 ----
mean loss: 96.85
 ---- batch: 050 ----
mean loss: 98.14
 ---- batch: 060 ----
mean loss: 97.85
 ---- batch: 070 ----
mean loss: 105.56
 ---- batch: 080 ----
mean loss: 101.17
 ---- batch: 090 ----
mean loss: 95.91
train mean loss: 98.96
epoch train time: 0:00:01.684018
elapsed time: 0:07:07.360451
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 20:39:20.801861
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.23
 ---- batch: 020 ----
mean loss: 97.60
 ---- batch: 030 ----
mean loss: 96.03
 ---- batch: 040 ----
mean loss: 95.75
 ---- batch: 050 ----
mean loss: 101.29
 ---- batch: 060 ----
mean loss: 94.62
 ---- batch: 070 ----
mean loss: 97.95
 ---- batch: 080 ----
mean loss: 108.61
 ---- batch: 090 ----
mean loss: 98.75
train mean loss: 99.31
epoch train time: 0:00:01.699340
elapsed time: 0:07:09.060813
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 20:39:22.501932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.54
 ---- batch: 020 ----
mean loss: 99.02
 ---- batch: 030 ----
mean loss: 96.38
 ---- batch: 040 ----
mean loss: 101.97
 ---- batch: 050 ----
mean loss: 101.39
 ---- batch: 060 ----
mean loss: 98.52
 ---- batch: 070 ----
mean loss: 102.30
 ---- batch: 080 ----
mean loss: 101.57
 ---- batch: 090 ----
mean loss: 98.77
train mean loss: 98.96
epoch train time: 0:00:01.687854
elapsed time: 0:07:10.749017
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 20:39:24.190372
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.75
 ---- batch: 020 ----
mean loss: 100.82
 ---- batch: 030 ----
mean loss: 96.47
 ---- batch: 040 ----
mean loss: 94.84
 ---- batch: 050 ----
mean loss: 101.51
 ---- batch: 060 ----
mean loss: 101.62
 ---- batch: 070 ----
mean loss: 100.86
 ---- batch: 080 ----
mean loss: 98.75
 ---- batch: 090 ----
mean loss: 98.39
train mean loss: 98.79
epoch train time: 0:00:01.729268
elapsed time: 0:07:12.479001
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 20:39:25.920386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.17
 ---- batch: 020 ----
mean loss: 97.11
 ---- batch: 030 ----
mean loss: 102.55
 ---- batch: 040 ----
mean loss: 99.07
 ---- batch: 050 ----
mean loss: 93.61
 ---- batch: 060 ----
mean loss: 96.49
 ---- batch: 070 ----
mean loss: 97.11
 ---- batch: 080 ----
mean loss: 103.03
 ---- batch: 090 ----
mean loss: 99.56
train mean loss: 98.86
epoch train time: 0:00:01.690197
elapsed time: 0:07:14.169886
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 20:39:27.611275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.52
 ---- batch: 020 ----
mean loss: 94.31
 ---- batch: 030 ----
mean loss: 103.27
 ---- batch: 040 ----
mean loss: 96.49
 ---- batch: 050 ----
mean loss: 98.45
 ---- batch: 060 ----
mean loss: 95.96
 ---- batch: 070 ----
mean loss: 96.68
 ---- batch: 080 ----
mean loss: 103.37
 ---- batch: 090 ----
mean loss: 102.23
train mean loss: 99.10
epoch train time: 0:00:01.686375
elapsed time: 0:07:15.856909
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 20:39:29.298279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.53
 ---- batch: 020 ----
mean loss: 103.07
 ---- batch: 030 ----
mean loss: 96.33
 ---- batch: 040 ----
mean loss: 95.95
 ---- batch: 050 ----
mean loss: 98.98
 ---- batch: 060 ----
mean loss: 101.17
 ---- batch: 070 ----
mean loss: 98.84
 ---- batch: 080 ----
mean loss: 98.76
 ---- batch: 090 ----
mean loss: 97.88
train mean loss: 98.83
epoch train time: 0:00:01.702038
elapsed time: 0:07:17.559572
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 20:39:31.000960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.90
 ---- batch: 020 ----
mean loss: 97.88
 ---- batch: 030 ----
mean loss: 98.93
 ---- batch: 040 ----
mean loss: 94.26
 ---- batch: 050 ----
mean loss: 101.55
 ---- batch: 060 ----
mean loss: 98.15
 ---- batch: 070 ----
mean loss: 95.77
 ---- batch: 080 ----
mean loss: 101.94
 ---- batch: 090 ----
mean loss: 99.75
train mean loss: 98.90
epoch train time: 0:00:01.682114
elapsed time: 0:07:19.242377
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 20:39:32.683751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.43
 ---- batch: 020 ----
mean loss: 101.80
 ---- batch: 030 ----
mean loss: 95.25
 ---- batch: 040 ----
mean loss: 94.95
 ---- batch: 050 ----
mean loss: 100.36
 ---- batch: 060 ----
mean loss: 99.93
 ---- batch: 070 ----
mean loss: 98.01
 ---- batch: 080 ----
mean loss: 96.63
 ---- batch: 090 ----
mean loss: 98.84
train mean loss: 98.65
epoch train time: 0:00:01.679449
elapsed time: 0:07:20.922419
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 20:39:34.363772
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.02
 ---- batch: 020 ----
mean loss: 99.12
 ---- batch: 030 ----
mean loss: 103.01
 ---- batch: 040 ----
mean loss: 96.90
 ---- batch: 050 ----
mean loss: 100.03
 ---- batch: 060 ----
mean loss: 94.77
 ---- batch: 070 ----
mean loss: 100.89
 ---- batch: 080 ----
mean loss: 96.16
 ---- batch: 090 ----
mean loss: 93.87
train mean loss: 98.75
epoch train time: 0:00:01.676477
elapsed time: 0:07:22.599572
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 20:39:36.040927
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.88
 ---- batch: 020 ----
mean loss: 96.25
 ---- batch: 030 ----
mean loss: 95.12
 ---- batch: 040 ----
mean loss: 95.88
 ---- batch: 050 ----
mean loss: 92.82
 ---- batch: 060 ----
mean loss: 96.05
 ---- batch: 070 ----
mean loss: 101.99
 ---- batch: 080 ----
mean loss: 100.72
 ---- batch: 090 ----
mean loss: 100.44
train mean loss: 98.51
epoch train time: 0:00:01.668874
elapsed time: 0:07:24.269101
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 20:39:37.710456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.94
 ---- batch: 020 ----
mean loss: 100.93
 ---- batch: 030 ----
mean loss: 95.73
 ---- batch: 040 ----
mean loss: 98.20
 ---- batch: 050 ----
mean loss: 95.26
 ---- batch: 060 ----
mean loss: 102.53
 ---- batch: 070 ----
mean loss: 97.60
 ---- batch: 080 ----
mean loss: 95.64
 ---- batch: 090 ----
mean loss: 98.77
train mean loss: 98.61
epoch train time: 0:00:01.669201
elapsed time: 0:07:25.938886
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 20:39:39.380261
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.96
 ---- batch: 020 ----
mean loss: 101.39
 ---- batch: 030 ----
mean loss: 95.15
 ---- batch: 040 ----
mean loss: 97.60
 ---- batch: 050 ----
mean loss: 93.81
 ---- batch: 060 ----
mean loss: 97.97
 ---- batch: 070 ----
mean loss: 97.74
 ---- batch: 080 ----
mean loss: 98.79
 ---- batch: 090 ----
mean loss: 102.27
train mean loss: 98.72
epoch train time: 0:00:01.709810
elapsed time: 0:07:27.649328
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 20:39:41.090711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.12
 ---- batch: 020 ----
mean loss: 98.73
 ---- batch: 030 ----
mean loss: 93.76
 ---- batch: 040 ----
mean loss: 97.43
 ---- batch: 050 ----
mean loss: 103.24
 ---- batch: 060 ----
mean loss: 102.87
 ---- batch: 070 ----
mean loss: 97.27
 ---- batch: 080 ----
mean loss: 98.66
 ---- batch: 090 ----
mean loss: 100.04
train mean loss: 98.57
epoch train time: 0:00:01.687486
elapsed time: 0:07:29.337435
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 20:39:42.778828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.71
 ---- batch: 020 ----
mean loss: 91.94
 ---- batch: 030 ----
mean loss: 94.65
 ---- batch: 040 ----
mean loss: 100.22
 ---- batch: 050 ----
mean loss: 99.65
 ---- batch: 060 ----
mean loss: 101.13
 ---- batch: 070 ----
mean loss: 98.98
 ---- batch: 080 ----
mean loss: 95.64
 ---- batch: 090 ----
mean loss: 103.41
train mean loss: 98.52
epoch train time: 0:00:01.695570
elapsed time: 0:07:31.033643
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 20:39:44.474992
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.62
 ---- batch: 020 ----
mean loss: 95.02
 ---- batch: 030 ----
mean loss: 96.48
 ---- batch: 040 ----
mean loss: 104.14
 ---- batch: 050 ----
mean loss: 98.86
 ---- batch: 060 ----
mean loss: 102.75
 ---- batch: 070 ----
mean loss: 98.68
 ---- batch: 080 ----
mean loss: 93.27
 ---- batch: 090 ----
mean loss: 97.37
train mean loss: 98.49
epoch train time: 0:00:01.673639
elapsed time: 0:07:32.707877
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 20:39:46.149253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.15
 ---- batch: 020 ----
mean loss: 92.98
 ---- batch: 030 ----
mean loss: 95.58
 ---- batch: 040 ----
mean loss: 99.46
 ---- batch: 050 ----
mean loss: 99.24
 ---- batch: 060 ----
mean loss: 103.78
 ---- batch: 070 ----
mean loss: 99.37
 ---- batch: 080 ----
mean loss: 93.30
 ---- batch: 090 ----
mean loss: 98.86
train mean loss: 98.25
epoch train time: 0:00:01.712736
elapsed time: 0:07:34.421294
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 20:39:47.862666
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.70
 ---- batch: 020 ----
mean loss: 99.30
 ---- batch: 030 ----
mean loss: 96.70
 ---- batch: 040 ----
mean loss: 92.58
 ---- batch: 050 ----
mean loss: 97.61
 ---- batch: 060 ----
mean loss: 96.07
 ---- batch: 070 ----
mean loss: 97.91
 ---- batch: 080 ----
mean loss: 106.01
 ---- batch: 090 ----
mean loss: 103.91
train mean loss: 98.24
epoch train time: 0:00:01.720464
elapsed time: 0:07:36.150461
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_9/checkpoint.pth.tar
**** end time: 2019-09-25 20:39:49.591534 ****
