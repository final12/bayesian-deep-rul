Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6683
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 22:22:28.269732 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 22:22:28.277610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3947.62
 ---- batch: 020 ----
mean loss: 1593.60
 ---- batch: 030 ----
mean loss: 430.54
 ---- batch: 040 ----
mean loss: 370.22
train mean loss: 1497.84
epoch train time: 0:00:15.200283
elapsed time: 0:00:15.210443
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 22:22:43.480208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.37
 ---- batch: 020 ----
mean loss: 289.51
 ---- batch: 030 ----
mean loss: 271.00
 ---- batch: 040 ----
mean loss: 263.26
train mean loss: 283.79
epoch train time: 0:00:01.847237
elapsed time: 0:00:17.057804
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 22:22:45.327593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.11
 ---- batch: 020 ----
mean loss: 251.93
 ---- batch: 030 ----
mean loss: 243.67
 ---- batch: 040 ----
mean loss: 244.84
train mean loss: 247.27
epoch train time: 0:00:01.694506
elapsed time: 0:00:18.752456
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 22:22:47.022291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.66
 ---- batch: 020 ----
mean loss: 228.37
 ---- batch: 030 ----
mean loss: 224.61
 ---- batch: 040 ----
mean loss: 227.19
train mean loss: 226.72
epoch train time: 0:00:01.675419
elapsed time: 0:00:20.428080
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 22:22:48.697860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.33
 ---- batch: 020 ----
mean loss: 224.67
 ---- batch: 030 ----
mean loss: 216.51
 ---- batch: 040 ----
mean loss: 208.26
train mean loss: 218.39
epoch train time: 0:00:01.775256
elapsed time: 0:00:22.203478
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 22:22:50.473258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.82
 ---- batch: 020 ----
mean loss: 206.29
 ---- batch: 030 ----
mean loss: 217.82
 ---- batch: 040 ----
mean loss: 208.71
train mean loss: 211.72
epoch train time: 0:00:01.685075
elapsed time: 0:00:23.888724
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 22:22:52.158509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.96
 ---- batch: 020 ----
mean loss: 215.93
 ---- batch: 030 ----
mean loss: 208.28
 ---- batch: 040 ----
mean loss: 206.77
train mean loss: 208.34
epoch train time: 0:00:01.671182
elapsed time: 0:00:25.560059
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 22:22:53.829836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.75
 ---- batch: 020 ----
mean loss: 214.96
 ---- batch: 030 ----
mean loss: 202.53
 ---- batch: 040 ----
mean loss: 196.74
train mean loss: 206.64
epoch train time: 0:00:01.776369
elapsed time: 0:00:27.336557
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 22:22:55.606342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.84
 ---- batch: 020 ----
mean loss: 200.45
 ---- batch: 030 ----
mean loss: 196.14
 ---- batch: 040 ----
mean loss: 208.93
train mean loss: 203.25
epoch train time: 0:00:01.681947
elapsed time: 0:00:29.018693
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 22:22:57.288487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.83
 ---- batch: 020 ----
mean loss: 200.68
 ---- batch: 030 ----
mean loss: 200.89
 ---- batch: 040 ----
mean loss: 205.25
train mean loss: 202.87
epoch train time: 0:00:01.782078
elapsed time: 0:00:30.800911
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 22:22:59.070702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.77
 ---- batch: 020 ----
mean loss: 207.10
 ---- batch: 030 ----
mean loss: 197.23
 ---- batch: 040 ----
mean loss: 197.08
train mean loss: 202.04
epoch train time: 0:00:01.678014
elapsed time: 0:00:32.479074
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 22:23:00.748856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.04
 ---- batch: 020 ----
mean loss: 203.89
 ---- batch: 030 ----
mean loss: 199.37
 ---- batch: 040 ----
mean loss: 197.82
train mean loss: 199.14
epoch train time: 0:00:01.776117
elapsed time: 0:00:34.255353
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 22:23:02.525131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.06
 ---- batch: 020 ----
mean loss: 201.53
 ---- batch: 030 ----
mean loss: 196.94
 ---- batch: 040 ----
mean loss: 195.09
train mean loss: 199.10
epoch train time: 0:00:01.678675
elapsed time: 0:00:35.934207
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 22:23:04.203999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.10
 ---- batch: 020 ----
mean loss: 194.31
 ---- batch: 030 ----
mean loss: 194.32
 ---- batch: 040 ----
mean loss: 198.32
train mean loss: 197.58
epoch train time: 0:00:01.678841
elapsed time: 0:00:37.613202
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 22:23:05.882975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.35
 ---- batch: 020 ----
mean loss: 195.50
 ---- batch: 030 ----
mean loss: 193.44
 ---- batch: 040 ----
mean loss: 195.92
train mean loss: 195.68
epoch train time: 0:00:01.782404
elapsed time: 0:00:39.395754
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 22:23:07.665544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.92
 ---- batch: 020 ----
mean loss: 195.97
 ---- batch: 030 ----
mean loss: 195.61
 ---- batch: 040 ----
mean loss: 194.89
train mean loss: 194.45
epoch train time: 0:00:01.689817
elapsed time: 0:00:41.085714
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 22:23:09.355493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.34
 ---- batch: 020 ----
mean loss: 193.95
 ---- batch: 030 ----
mean loss: 196.80
 ---- batch: 040 ----
mean loss: 191.50
train mean loss: 194.47
epoch train time: 0:00:01.787797
elapsed time: 0:00:42.873654
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 22:23:11.143464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.63
 ---- batch: 020 ----
mean loss: 197.24
 ---- batch: 030 ----
mean loss: 194.88
 ---- batch: 040 ----
mean loss: 197.34
train mean loss: 194.32
epoch train time: 0:00:01.688672
elapsed time: 0:00:44.562496
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 22:23:12.832275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.00
 ---- batch: 020 ----
mean loss: 201.98
 ---- batch: 030 ----
mean loss: 200.01
 ---- batch: 040 ----
mean loss: 189.35
train mean loss: 194.88
epoch train time: 0:00:01.798809
elapsed time: 0:00:46.361441
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 22:23:14.631218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.70
 ---- batch: 020 ----
mean loss: 195.47
 ---- batch: 030 ----
mean loss: 194.92
 ---- batch: 040 ----
mean loss: 198.62
train mean loss: 195.35
epoch train time: 0:00:01.679812
elapsed time: 0:00:48.041396
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 22:23:16.311179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.31
 ---- batch: 020 ----
mean loss: 194.53
 ---- batch: 030 ----
mean loss: 203.20
 ---- batch: 040 ----
mean loss: 189.95
train mean loss: 195.10
epoch train time: 0:00:01.798253
elapsed time: 0:00:49.839795
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 22:23:18.109602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.70
 ---- batch: 020 ----
mean loss: 203.93
 ---- batch: 030 ----
mean loss: 182.82
 ---- batch: 040 ----
mean loss: 191.77
train mean loss: 193.19
epoch train time: 0:00:01.688031
elapsed time: 0:00:51.528009
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 22:23:19.797793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.87
 ---- batch: 020 ----
mean loss: 194.68
 ---- batch: 030 ----
mean loss: 188.94
 ---- batch: 040 ----
mean loss: 188.26
train mean loss: 190.88
epoch train time: 0:00:01.793347
elapsed time: 0:00:53.321490
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 22:23:21.591307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.64
 ---- batch: 020 ----
mean loss: 185.44
 ---- batch: 030 ----
mean loss: 199.35
 ---- batch: 040 ----
mean loss: 194.13
train mean loss: 192.32
epoch train time: 0:00:01.680452
elapsed time: 0:00:55.002148
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 22:23:23.271927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.65
 ---- batch: 020 ----
mean loss: 197.41
 ---- batch: 030 ----
mean loss: 196.34
 ---- batch: 040 ----
mean loss: 197.76
train mean loss: 195.19
epoch train time: 0:00:01.787733
elapsed time: 0:00:56.790063
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 22:23:25.059874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.40
 ---- batch: 020 ----
mean loss: 194.68
 ---- batch: 030 ----
mean loss: 188.04
 ---- batch: 040 ----
mean loss: 193.27
train mean loss: 191.82
epoch train time: 0:00:01.696645
elapsed time: 0:00:58.486951
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 22:23:26.756803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.70
 ---- batch: 020 ----
mean loss: 184.55
 ---- batch: 030 ----
mean loss: 195.28
 ---- batch: 040 ----
mean loss: 190.40
train mean loss: 189.13
epoch train time: 0:00:01.684631
elapsed time: 0:01:00.171783
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 22:23:28.441574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.06
 ---- batch: 020 ----
mean loss: 199.92
 ---- batch: 030 ----
mean loss: 197.94
 ---- batch: 040 ----
mean loss: 183.44
train mean loss: 192.30
epoch train time: 0:00:01.782572
elapsed time: 0:01:01.954540
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 22:23:30.224335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.14
 ---- batch: 020 ----
mean loss: 187.36
 ---- batch: 030 ----
mean loss: 188.94
 ---- batch: 040 ----
mean loss: 194.25
train mean loss: 190.16
epoch train time: 0:00:01.689537
elapsed time: 0:01:03.644226
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 22:23:31.914018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.94
 ---- batch: 020 ----
mean loss: 198.49
 ---- batch: 030 ----
mean loss: 181.77
 ---- batch: 040 ----
mean loss: 187.93
train mean loss: 190.86
epoch train time: 0:00:01.793700
elapsed time: 0:01:05.438069
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 22:23:33.707863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.48
 ---- batch: 020 ----
mean loss: 184.91
 ---- batch: 030 ----
mean loss: 188.20
 ---- batch: 040 ----
mean loss: 187.88
train mean loss: 189.73
epoch train time: 0:00:01.682016
elapsed time: 0:01:07.120232
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 22:23:35.390023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.07
 ---- batch: 020 ----
mean loss: 191.37
 ---- batch: 030 ----
mean loss: 193.37
 ---- batch: 040 ----
mean loss: 184.45
train mean loss: 190.58
epoch train time: 0:00:01.798595
elapsed time: 0:01:08.919016
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 22:23:37.188795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.40
 ---- batch: 020 ----
mean loss: 183.67
 ---- batch: 030 ----
mean loss: 189.57
 ---- batch: 040 ----
mean loss: 187.68
train mean loss: 189.41
epoch train time: 0:00:01.687648
elapsed time: 0:01:10.606807
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 22:23:38.876592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.08
 ---- batch: 020 ----
mean loss: 196.71
 ---- batch: 030 ----
mean loss: 184.21
 ---- batch: 040 ----
mean loss: 184.67
train mean loss: 191.11
epoch train time: 0:00:01.793617
elapsed time: 0:01:12.400592
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 22:23:40.670392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.03
 ---- batch: 020 ----
mean loss: 192.77
 ---- batch: 030 ----
mean loss: 192.18
 ---- batch: 040 ----
mean loss: 186.10
train mean loss: 189.24
epoch train time: 0:00:01.688902
elapsed time: 0:01:14.089645
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 22:23:42.359422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.02
 ---- batch: 020 ----
mean loss: 191.06
 ---- batch: 030 ----
mean loss: 187.17
 ---- batch: 040 ----
mean loss: 190.22
train mean loss: 190.47
epoch train time: 0:00:01.778620
elapsed time: 0:01:15.868434
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 22:23:44.138244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.08
 ---- batch: 020 ----
mean loss: 194.23
 ---- batch: 030 ----
mean loss: 189.95
 ---- batch: 040 ----
mean loss: 191.36
train mean loss: 191.57
epoch train time: 0:00:01.697693
elapsed time: 0:01:17.566307
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 22:23:45.836086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.83
 ---- batch: 020 ----
mean loss: 186.01
 ---- batch: 030 ----
mean loss: 188.09
 ---- batch: 040 ----
mean loss: 189.14
train mean loss: 189.78
epoch train time: 0:00:01.782869
elapsed time: 0:01:19.349314
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 22:23:47.619106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.43
 ---- batch: 020 ----
mean loss: 186.28
 ---- batch: 030 ----
mean loss: 190.50
 ---- batch: 040 ----
mean loss: 197.12
train mean loss: 192.51
epoch train time: 0:00:01.689861
elapsed time: 0:01:21.039348
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 22:23:49.309147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.54
 ---- batch: 020 ----
mean loss: 188.68
 ---- batch: 030 ----
mean loss: 192.54
 ---- batch: 040 ----
mean loss: 188.04
train mean loss: 190.45
epoch train time: 0:00:01.718200
elapsed time: 0:01:22.757696
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 22:23:51.027482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.08
 ---- batch: 020 ----
mean loss: 186.02
 ---- batch: 030 ----
mean loss: 190.55
 ---- batch: 040 ----
mean loss: 190.35
train mean loss: 189.09
epoch train time: 0:00:01.752583
elapsed time: 0:01:24.510428
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 22:23:52.780209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.27
 ---- batch: 020 ----
mean loss: 190.37
 ---- batch: 030 ----
mean loss: 200.29
 ---- batch: 040 ----
mean loss: 183.16
train mean loss: 190.54
epoch train time: 0:00:01.686871
elapsed time: 0:01:26.197425
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 22:23:54.467204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.99
 ---- batch: 020 ----
mean loss: 185.32
 ---- batch: 030 ----
mean loss: 192.79
 ---- batch: 040 ----
mean loss: 188.70
train mean loss: 188.40
epoch train time: 0:00:01.804410
elapsed time: 0:01:28.001959
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 22:23:56.271734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.93
 ---- batch: 020 ----
mean loss: 188.49
 ---- batch: 030 ----
mean loss: 186.11
 ---- batch: 040 ----
mean loss: 190.40
train mean loss: 191.35
epoch train time: 0:00:01.691196
elapsed time: 0:01:29.693294
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 22:23:57.963072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.96
 ---- batch: 020 ----
mean loss: 192.98
 ---- batch: 030 ----
mean loss: 192.68
 ---- batch: 040 ----
mean loss: 190.42
train mean loss: 190.34
epoch train time: 0:00:01.796371
elapsed time: 0:01:31.489828
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 22:23:59.759626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.01
 ---- batch: 020 ----
mean loss: 189.17
 ---- batch: 030 ----
mean loss: 184.53
 ---- batch: 040 ----
mean loss: 189.24
train mean loss: 187.08
epoch train time: 0:00:01.690702
elapsed time: 0:01:33.180688
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 22:24:01.450470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.71
 ---- batch: 020 ----
mean loss: 189.08
 ---- batch: 030 ----
mean loss: 190.72
 ---- batch: 040 ----
mean loss: 181.00
train mean loss: 187.79
epoch train time: 0:00:01.790268
elapsed time: 0:01:34.971128
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 22:24:03.240937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.23
 ---- batch: 020 ----
mean loss: 188.68
 ---- batch: 030 ----
mean loss: 191.20
 ---- batch: 040 ----
mean loss: 184.08
train mean loss: 187.86
epoch train time: 0:00:01.680637
elapsed time: 0:01:36.651932
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 22:24:04.921715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.14
 ---- batch: 020 ----
mean loss: 190.26
 ---- batch: 030 ----
mean loss: 183.35
 ---- batch: 040 ----
mean loss: 193.23
train mean loss: 189.20
epoch train time: 0:00:01.782524
elapsed time: 0:01:38.434588
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 22:24:06.704363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.21
 ---- batch: 020 ----
mean loss: 187.54
 ---- batch: 030 ----
mean loss: 184.54
 ---- batch: 040 ----
mean loss: 191.87
train mean loss: 188.82
epoch train time: 0:00:01.695998
elapsed time: 0:01:40.130748
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 22:24:08.400528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.54
 ---- batch: 020 ----
mean loss: 189.39
 ---- batch: 030 ----
mean loss: 188.06
 ---- batch: 040 ----
mean loss: 190.48
train mean loss: 189.02
epoch train time: 0:00:01.781376
elapsed time: 0:01:41.912280
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 22:24:10.182089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.14
 ---- batch: 020 ----
mean loss: 185.61
 ---- batch: 030 ----
mean loss: 191.91
 ---- batch: 040 ----
mean loss: 179.73
train mean loss: 185.67
epoch train time: 0:00:01.684909
elapsed time: 0:01:43.597357
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 22:24:11.867136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.75
 ---- batch: 020 ----
mean loss: 183.07
 ---- batch: 030 ----
mean loss: 190.47
 ---- batch: 040 ----
mean loss: 182.81
train mean loss: 188.16
epoch train time: 0:00:01.787065
elapsed time: 0:01:45.384558
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 22:24:13.654333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.72
 ---- batch: 020 ----
mean loss: 184.41
 ---- batch: 030 ----
mean loss: 180.48
 ---- batch: 040 ----
mean loss: 191.76
train mean loss: 185.66
epoch train time: 0:00:01.690527
elapsed time: 0:01:47.075216
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 22:24:15.344994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.61
 ---- batch: 020 ----
mean loss: 187.61
 ---- batch: 030 ----
mean loss: 191.85
 ---- batch: 040 ----
mean loss: 188.29
train mean loss: 189.02
epoch train time: 0:00:01.785250
elapsed time: 0:01:48.860630
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 22:24:17.130405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.60
 ---- batch: 020 ----
mean loss: 202.50
 ---- batch: 030 ----
mean loss: 192.47
 ---- batch: 040 ----
mean loss: 183.06
train mean loss: 194.57
epoch train time: 0:00:01.693325
elapsed time: 0:01:50.554108
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 22:24:18.823922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.69
 ---- batch: 020 ----
mean loss: 191.47
 ---- batch: 030 ----
mean loss: 181.75
 ---- batch: 040 ----
mean loss: 193.72
train mean loss: 189.64
epoch train time: 0:00:01.694913
elapsed time: 0:01:52.249179
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 22:24:20.518954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.76
 ---- batch: 020 ----
mean loss: 186.25
 ---- batch: 030 ----
mean loss: 188.70
 ---- batch: 040 ----
mean loss: 187.50
train mean loss: 186.35
epoch train time: 0:00:01.725185
elapsed time: 0:01:53.974511
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 22:24:22.244292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.54
 ---- batch: 020 ----
mean loss: 186.98
 ---- batch: 030 ----
mean loss: 188.01
 ---- batch: 040 ----
mean loss: 189.71
train mean loss: 186.51
epoch train time: 0:00:01.685262
elapsed time: 0:01:55.659901
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 22:24:23.929691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.32
 ---- batch: 020 ----
mean loss: 194.49
 ---- batch: 030 ----
mean loss: 188.63
 ---- batch: 040 ----
mean loss: 187.34
train mean loss: 189.05
epoch train time: 0:00:01.785697
elapsed time: 0:01:57.445740
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 22:24:25.715521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.18
 ---- batch: 020 ----
mean loss: 179.95
 ---- batch: 030 ----
mean loss: 187.67
 ---- batch: 040 ----
mean loss: 186.99
train mean loss: 184.20
epoch train time: 0:00:01.690680
elapsed time: 0:01:59.136557
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 22:24:27.406344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.91
 ---- batch: 020 ----
mean loss: 186.56
 ---- batch: 030 ----
mean loss: 178.45
 ---- batch: 040 ----
mean loss: 190.04
train mean loss: 184.45
epoch train time: 0:00:01.793722
elapsed time: 0:02:00.930414
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 22:24:29.200190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.41
 ---- batch: 020 ----
mean loss: 185.76
 ---- batch: 030 ----
mean loss: 186.04
 ---- batch: 040 ----
mean loss: 185.30
train mean loss: 185.01
epoch train time: 0:00:01.687127
elapsed time: 0:02:02.617688
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 22:24:30.887481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.86
 ---- batch: 020 ----
mean loss: 189.78
 ---- batch: 030 ----
mean loss: 188.10
 ---- batch: 040 ----
mean loss: 182.39
train mean loss: 184.31
epoch train time: 0:00:01.799061
elapsed time: 0:02:04.416915
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 22:24:32.686691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.72
 ---- batch: 020 ----
mean loss: 178.92
 ---- batch: 030 ----
mean loss: 187.85
 ---- batch: 040 ----
mean loss: 174.56
train mean loss: 181.88
epoch train time: 0:00:01.689521
elapsed time: 0:02:06.106564
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 22:24:34.376356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.23
 ---- batch: 020 ----
mean loss: 182.35
 ---- batch: 030 ----
mean loss: 174.70
 ---- batch: 040 ----
mean loss: 186.01
train mean loss: 181.97
epoch train time: 0:00:01.716471
elapsed time: 0:02:07.823178
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 22:24:36.093019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.83
 ---- batch: 020 ----
mean loss: 178.47
 ---- batch: 030 ----
mean loss: 185.02
 ---- batch: 040 ----
mean loss: 187.31
train mean loss: 183.14
epoch train time: 0:00:01.692461
elapsed time: 0:02:09.515887
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 22:24:37.785670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.52
 ---- batch: 020 ----
mean loss: 183.28
 ---- batch: 030 ----
mean loss: 184.48
 ---- batch: 040 ----
mean loss: 176.35
train mean loss: 180.86
epoch train time: 0:00:01.786327
elapsed time: 0:02:11.302350
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 22:24:39.572125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.00
 ---- batch: 020 ----
mean loss: 174.00
 ---- batch: 030 ----
mean loss: 175.56
 ---- batch: 040 ----
mean loss: 183.44
train mean loss: 178.57
epoch train time: 0:00:01.682520
elapsed time: 0:02:12.985009
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 22:24:41.254792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.02
 ---- batch: 020 ----
mean loss: 177.13
 ---- batch: 030 ----
mean loss: 173.66
 ---- batch: 040 ----
mean loss: 176.53
train mean loss: 177.92
epoch train time: 0:00:01.785545
elapsed time: 0:02:14.770749
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 22:24:43.040561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.73
 ---- batch: 020 ----
mean loss: 178.15
 ---- batch: 030 ----
mean loss: 188.00
 ---- batch: 040 ----
mean loss: 181.02
train mean loss: 182.14
epoch train time: 0:00:01.687093
elapsed time: 0:02:16.458046
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 22:24:44.727842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.38
 ---- batch: 020 ----
mean loss: 174.62
 ---- batch: 030 ----
mean loss: 175.28
 ---- batch: 040 ----
mean loss: 166.38
train mean loss: 172.97
epoch train time: 0:00:01.690726
elapsed time: 0:02:18.148926
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 22:24:46.418736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.32
 ---- batch: 020 ----
mean loss: 178.51
 ---- batch: 030 ----
mean loss: 171.51
 ---- batch: 040 ----
mean loss: 168.98
train mean loss: 171.81
epoch train time: 0:00:01.779174
elapsed time: 0:02:19.928274
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 22:24:48.198053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.62
 ---- batch: 020 ----
mean loss: 170.63
 ---- batch: 030 ----
mean loss: 167.05
 ---- batch: 040 ----
mean loss: 167.54
train mean loss: 171.36
epoch train time: 0:00:01.690839
elapsed time: 0:02:21.619267
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 22:24:49.889047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.42
 ---- batch: 020 ----
mean loss: 168.27
 ---- batch: 030 ----
mean loss: 180.31
 ---- batch: 040 ----
mean loss: 166.96
train mean loss: 172.46
epoch train time: 0:00:01.795895
elapsed time: 0:02:23.415316
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 22:24:51.685128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.09
 ---- batch: 020 ----
mean loss: 167.02
 ---- batch: 030 ----
mean loss: 163.27
 ---- batch: 040 ----
mean loss: 163.66
train mean loss: 165.70
epoch train time: 0:00:01.690930
elapsed time: 0:02:25.106422
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 22:24:53.376202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.64
 ---- batch: 020 ----
mean loss: 165.06
 ---- batch: 030 ----
mean loss: 162.37
 ---- batch: 040 ----
mean loss: 162.99
train mean loss: 164.37
epoch train time: 0:00:01.789621
elapsed time: 0:02:26.896231
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 22:24:55.166052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.55
 ---- batch: 020 ----
mean loss: 164.85
 ---- batch: 030 ----
mean loss: 159.23
 ---- batch: 040 ----
mean loss: 157.49
train mean loss: 161.51
epoch train time: 0:00:01.694983
elapsed time: 0:02:28.591407
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 22:24:56.861209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.10
 ---- batch: 020 ----
mean loss: 165.11
 ---- batch: 030 ----
mean loss: 162.65
 ---- batch: 040 ----
mean loss: 163.89
train mean loss: 166.58
epoch train time: 0:00:01.793354
elapsed time: 0:02:30.384915
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 22:24:58.654700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.80
 ---- batch: 020 ----
mean loss: 156.81
 ---- batch: 030 ----
mean loss: 162.29
 ---- batch: 040 ----
mean loss: 155.71
train mean loss: 158.47
epoch train time: 0:00:01.684575
elapsed time: 0:02:32.069638
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 22:25:00.339415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.43
 ---- batch: 020 ----
mean loss: 160.78
 ---- batch: 030 ----
mean loss: 155.73
 ---- batch: 040 ----
mean loss: 149.36
train mean loss: 156.14
epoch train time: 0:00:01.788739
elapsed time: 0:02:33.858502
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 22:25:02.128291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.90
 ---- batch: 020 ----
mean loss: 158.98
 ---- batch: 030 ----
mean loss: 159.13
 ---- batch: 040 ----
mean loss: 153.49
train mean loss: 157.94
epoch train time: 0:00:01.694903
elapsed time: 0:02:35.553603
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 22:25:03.823384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.50
 ---- batch: 020 ----
mean loss: 155.90
 ---- batch: 030 ----
mean loss: 152.33
 ---- batch: 040 ----
mean loss: 152.52
train mean loss: 153.77
epoch train time: 0:00:01.789204
elapsed time: 0:02:37.343024
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 22:25:05.612803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.11
 ---- batch: 020 ----
mean loss: 158.42
 ---- batch: 030 ----
mean loss: 152.27
 ---- batch: 040 ----
mean loss: 153.75
train mean loss: 153.71
epoch train time: 0:00:01.690959
elapsed time: 0:02:39.034139
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 22:25:07.303934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.85
 ---- batch: 020 ----
mean loss: 147.36
 ---- batch: 030 ----
mean loss: 150.58
 ---- batch: 040 ----
mean loss: 147.19
train mean loss: 149.21
epoch train time: 0:00:01.783095
elapsed time: 0:02:40.817379
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 22:25:09.087156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.71
 ---- batch: 020 ----
mean loss: 149.64
 ---- batch: 030 ----
mean loss: 144.42
 ---- batch: 040 ----
mean loss: 153.46
train mean loss: 148.48
epoch train time: 0:00:01.694770
elapsed time: 0:02:42.512309
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 22:25:10.782091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.77
 ---- batch: 020 ----
mean loss: 146.76
 ---- batch: 030 ----
mean loss: 143.91
 ---- batch: 040 ----
mean loss: 151.18
train mean loss: 149.90
epoch train time: 0:00:01.683668
elapsed time: 0:02:44.196141
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 22:25:12.465935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.83
 ---- batch: 020 ----
mean loss: 156.78
 ---- batch: 030 ----
mean loss: 152.58
 ---- batch: 040 ----
mean loss: 144.39
train mean loss: 152.27
epoch train time: 0:00:01.801453
elapsed time: 0:02:45.997764
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 22:25:14.267545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.04
 ---- batch: 020 ----
mean loss: 150.34
 ---- batch: 030 ----
mean loss: 150.11
 ---- batch: 040 ----
mean loss: 145.63
train mean loss: 148.84
epoch train time: 0:00:01.683287
elapsed time: 0:02:47.681187
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 22:25:15.951017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.98
 ---- batch: 020 ----
mean loss: 146.79
 ---- batch: 030 ----
mean loss: 152.00
 ---- batch: 040 ----
mean loss: 146.66
train mean loss: 147.87
epoch train time: 0:00:01.781053
elapsed time: 0:02:49.462434
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 22:25:17.732227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.63
 ---- batch: 020 ----
mean loss: 147.40
 ---- batch: 030 ----
mean loss: 141.26
 ---- batch: 040 ----
mean loss: 151.73
train mean loss: 146.35
epoch train time: 0:00:01.687048
elapsed time: 0:02:51.149629
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 22:25:19.419405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.12
 ---- batch: 020 ----
mean loss: 149.53
 ---- batch: 030 ----
mean loss: 149.21
 ---- batch: 040 ----
mean loss: 142.12
train mean loss: 147.80
epoch train time: 0:00:01.798108
elapsed time: 0:02:52.947872
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 22:25:21.217645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.18
 ---- batch: 020 ----
mean loss: 148.98
 ---- batch: 030 ----
mean loss: 144.03
 ---- batch: 040 ----
mean loss: 144.80
train mean loss: 145.14
epoch train time: 0:00:01.690013
elapsed time: 0:02:54.638024
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 22:25:22.907798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.47
 ---- batch: 020 ----
mean loss: 148.24
 ---- batch: 030 ----
mean loss: 142.10
 ---- batch: 040 ----
mean loss: 145.28
train mean loss: 145.22
epoch train time: 0:00:01.793544
elapsed time: 0:02:56.431692
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 22:25:24.701468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.08
 ---- batch: 020 ----
mean loss: 148.38
 ---- batch: 030 ----
mean loss: 143.96
 ---- batch: 040 ----
mean loss: 147.35
train mean loss: 145.36
epoch train time: 0:00:01.694784
elapsed time: 0:02:58.126620
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 22:25:26.396416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.59
 ---- batch: 020 ----
mean loss: 145.22
 ---- batch: 030 ----
mean loss: 138.32
 ---- batch: 040 ----
mean loss: 140.15
train mean loss: 143.00
epoch train time: 0:00:01.790479
elapsed time: 0:02:59.917267
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 22:25:28.187042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.05
 ---- batch: 020 ----
mean loss: 143.62
 ---- batch: 030 ----
mean loss: 141.81
 ---- batch: 040 ----
mean loss: 143.16
train mean loss: 145.65
epoch train time: 0:00:01.683568
elapsed time: 0:03:01.601011
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 22:25:29.870797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.49
 ---- batch: 020 ----
mean loss: 147.36
 ---- batch: 030 ----
mean loss: 147.85
 ---- batch: 040 ----
mean loss: 140.67
train mean loss: 145.08
epoch train time: 0:00:01.755877
elapsed time: 0:03:03.357038
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 22:25:31.626827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.14
 ---- batch: 020 ----
mean loss: 148.57
 ---- batch: 030 ----
mean loss: 141.03
 ---- batch: 040 ----
mean loss: 141.94
train mean loss: 144.17
epoch train time: 0:00:01.720907
elapsed time: 0:03:05.078090
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 22:25:33.347870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.63
 ---- batch: 020 ----
mean loss: 141.86
 ---- batch: 030 ----
mean loss: 148.40
 ---- batch: 040 ----
mean loss: 143.50
train mean loss: 143.34
epoch train time: 0:00:01.679538
elapsed time: 0:03:06.757765
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 22:25:35.027544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.75
 ---- batch: 020 ----
mean loss: 150.58
 ---- batch: 030 ----
mean loss: 145.46
 ---- batch: 040 ----
mean loss: 146.08
train mean loss: 146.73
epoch train time: 0:00:01.791541
elapsed time: 0:03:08.549438
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 22:25:36.819217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.61
 ---- batch: 020 ----
mean loss: 142.55
 ---- batch: 030 ----
mean loss: 137.45
 ---- batch: 040 ----
mean loss: 139.93
train mean loss: 142.12
epoch train time: 0:00:01.684409
elapsed time: 0:03:10.233980
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 22:25:38.503778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.15
 ---- batch: 020 ----
mean loss: 134.68
 ---- batch: 030 ----
mean loss: 140.79
 ---- batch: 040 ----
mean loss: 143.46
train mean loss: 142.35
epoch train time: 0:00:01.794104
elapsed time: 0:03:12.028241
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 22:25:40.298016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.11
 ---- batch: 020 ----
mean loss: 142.85
 ---- batch: 030 ----
mean loss: 137.68
 ---- batch: 040 ----
mean loss: 139.01
train mean loss: 140.24
epoch train time: 0:00:01.681140
elapsed time: 0:03:13.709510
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 22:25:41.979297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.38
 ---- batch: 020 ----
mean loss: 143.94
 ---- batch: 030 ----
mean loss: 135.95
 ---- batch: 040 ----
mean loss: 143.09
train mean loss: 141.69
epoch train time: 0:00:01.677774
elapsed time: 0:03:15.387416
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 22:25:43.657190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.86
 ---- batch: 020 ----
mean loss: 141.11
 ---- batch: 030 ----
mean loss: 138.45
 ---- batch: 040 ----
mean loss: 142.61
train mean loss: 140.96
epoch train time: 0:00:01.796729
elapsed time: 0:03:17.184299
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 22:25:45.454077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.52
 ---- batch: 020 ----
mean loss: 141.04
 ---- batch: 030 ----
mean loss: 141.00
 ---- batch: 040 ----
mean loss: 137.23
train mean loss: 140.45
epoch train time: 0:00:01.679736
elapsed time: 0:03:18.864223
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 22:25:47.133991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.52
 ---- batch: 020 ----
mean loss: 137.36
 ---- batch: 030 ----
mean loss: 136.99
 ---- batch: 040 ----
mean loss: 135.52
train mean loss: 138.69
epoch train time: 0:00:01.793540
elapsed time: 0:03:20.657878
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 22:25:48.927654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.11
 ---- batch: 020 ----
mean loss: 137.37
 ---- batch: 030 ----
mean loss: 136.61
 ---- batch: 040 ----
mean loss: 140.29
train mean loss: 137.76
epoch train time: 0:00:01.686158
elapsed time: 0:03:22.344167
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 22:25:50.613954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.26
 ---- batch: 020 ----
mean loss: 138.37
 ---- batch: 030 ----
mean loss: 153.70
 ---- batch: 040 ----
mean loss: 135.24
train mean loss: 142.13
epoch train time: 0:00:01.792759
elapsed time: 0:03:24.137105
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 22:25:52.406902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.16
 ---- batch: 020 ----
mean loss: 135.77
 ---- batch: 030 ----
mean loss: 143.53
 ---- batch: 040 ----
mean loss: 137.10
train mean loss: 138.36
epoch train time: 0:00:01.694672
elapsed time: 0:03:25.831944
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 22:25:54.101790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.77
 ---- batch: 020 ----
mean loss: 143.64
 ---- batch: 030 ----
mean loss: 142.33
 ---- batch: 040 ----
mean loss: 142.13
train mean loss: 142.36
epoch train time: 0:00:01.678981
elapsed time: 0:03:27.511151
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 22:25:55.780930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.99
 ---- batch: 020 ----
mean loss: 137.69
 ---- batch: 030 ----
mean loss: 138.26
 ---- batch: 040 ----
mean loss: 135.86
train mean loss: 137.08
epoch train time: 0:00:01.801646
elapsed time: 0:03:29.312952
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 22:25:57.582739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.86
 ---- batch: 020 ----
mean loss: 137.99
 ---- batch: 030 ----
mean loss: 140.03
 ---- batch: 040 ----
mean loss: 137.68
train mean loss: 137.81
epoch train time: 0:00:01.681245
elapsed time: 0:03:30.994418
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 22:25:59.264200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.95
 ---- batch: 020 ----
mean loss: 130.05
 ---- batch: 030 ----
mean loss: 139.99
 ---- batch: 040 ----
mean loss: 134.45
train mean loss: 136.02
epoch train time: 0:00:01.785860
elapsed time: 0:03:32.780409
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 22:26:01.050186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.47
 ---- batch: 020 ----
mean loss: 140.35
 ---- batch: 030 ----
mean loss: 138.05
 ---- batch: 040 ----
mean loss: 133.43
train mean loss: 137.43
epoch train time: 0:00:01.682226
elapsed time: 0:03:34.462785
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 22:26:02.732578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.91
 ---- batch: 020 ----
mean loss: 133.60
 ---- batch: 030 ----
mean loss: 134.07
 ---- batch: 040 ----
mean loss: 137.10
train mean loss: 135.56
epoch train time: 0:00:01.788264
elapsed time: 0:03:36.251206
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 22:26:04.520982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.94
 ---- batch: 020 ----
mean loss: 133.34
 ---- batch: 030 ----
mean loss: 132.61
 ---- batch: 040 ----
mean loss: 135.50
train mean loss: 133.65
epoch train time: 0:00:01.687028
elapsed time: 0:03:37.938364
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 22:26:06.208145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.43
 ---- batch: 020 ----
mean loss: 138.47
 ---- batch: 030 ----
mean loss: 136.67
 ---- batch: 040 ----
mean loss: 135.15
train mean loss: 135.96
epoch train time: 0:00:01.678616
elapsed time: 0:03:39.617111
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 22:26:07.886890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.25
 ---- batch: 020 ----
mean loss: 135.18
 ---- batch: 030 ----
mean loss: 135.14
 ---- batch: 040 ----
mean loss: 140.39
train mean loss: 137.04
epoch train time: 0:00:01.794782
elapsed time: 0:03:41.412079
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 22:26:09.681868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.62
 ---- batch: 020 ----
mean loss: 134.74
 ---- batch: 030 ----
mean loss: 138.16
 ---- batch: 040 ----
mean loss: 133.34
train mean loss: 136.21
epoch train time: 0:00:01.684862
elapsed time: 0:03:43.097081
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 22:26:11.366870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.33
 ---- batch: 020 ----
mean loss: 138.66
 ---- batch: 030 ----
mean loss: 129.43
 ---- batch: 040 ----
mean loss: 139.32
train mean loss: 135.79
epoch train time: 0:00:01.800905
elapsed time: 0:03:44.898135
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 22:26:13.167928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.11
 ---- batch: 020 ----
mean loss: 146.23
 ---- batch: 030 ----
mean loss: 137.21
 ---- batch: 040 ----
mean loss: 130.63
train mean loss: 137.05
epoch train time: 0:00:01.678846
elapsed time: 0:03:46.577132
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 22:26:14.846916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.36
 ---- batch: 020 ----
mean loss: 135.38
 ---- batch: 030 ----
mean loss: 137.03
 ---- batch: 040 ----
mean loss: 135.83
train mean loss: 138.47
epoch train time: 0:00:01.783055
elapsed time: 0:03:48.360317
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 22:26:16.630107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.85
 ---- batch: 020 ----
mean loss: 135.56
 ---- batch: 030 ----
mean loss: 138.22
 ---- batch: 040 ----
mean loss: 140.21
train mean loss: 138.59
epoch train time: 0:00:01.679114
elapsed time: 0:03:50.039600
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 22:26:18.309382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.37
 ---- batch: 020 ----
mean loss: 134.94
 ---- batch: 030 ----
mean loss: 135.71
 ---- batch: 040 ----
mean loss: 134.03
train mean loss: 135.15
epoch train time: 0:00:01.681010
elapsed time: 0:03:51.720753
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 22:26:19.990536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.83
 ---- batch: 020 ----
mean loss: 137.01
 ---- batch: 030 ----
mean loss: 143.00
 ---- batch: 040 ----
mean loss: 133.52
train mean loss: 136.98
epoch train time: 0:00:01.800007
elapsed time: 0:03:53.520915
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 22:26:21.790684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.90
 ---- batch: 020 ----
mean loss: 137.87
 ---- batch: 030 ----
mean loss: 131.64
 ---- batch: 040 ----
mean loss: 139.96
train mean loss: 136.90
epoch train time: 0:00:01.686691
elapsed time: 0:03:55.207730
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 22:26:23.477508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.49
 ---- batch: 020 ----
mean loss: 137.16
 ---- batch: 030 ----
mean loss: 136.65
 ---- batch: 040 ----
mean loss: 136.57
train mean loss: 135.22
epoch train time: 0:00:01.804896
elapsed time: 0:03:57.012750
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 22:26:25.282526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.83
 ---- batch: 020 ----
mean loss: 133.14
 ---- batch: 030 ----
mean loss: 134.46
 ---- batch: 040 ----
mean loss: 132.86
train mean loss: 133.70
epoch train time: 0:00:01.679823
elapsed time: 0:03:58.692735
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 22:26:26.962528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.04
 ---- batch: 020 ----
mean loss: 130.36
 ---- batch: 030 ----
mean loss: 134.75
 ---- batch: 040 ----
mean loss: 132.86
train mean loss: 133.91
epoch train time: 0:00:01.673559
elapsed time: 0:04:00.366445
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 22:26:28.636223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.07
 ---- batch: 020 ----
mean loss: 136.48
 ---- batch: 030 ----
mean loss: 136.54
 ---- batch: 040 ----
mean loss: 134.48
train mean loss: 135.00
epoch train time: 0:00:01.792587
elapsed time: 0:04:02.159162
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 22:26:30.428956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.98
 ---- batch: 020 ----
mean loss: 130.85
 ---- batch: 030 ----
mean loss: 133.27
 ---- batch: 040 ----
mean loss: 137.03
train mean loss: 133.59
epoch train time: 0:00:01.694726
elapsed time: 0:04:03.854040
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 22:26:32.123819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.29
 ---- batch: 020 ----
mean loss: 134.29
 ---- batch: 030 ----
mean loss: 133.48
 ---- batch: 040 ----
mean loss: 128.01
train mean loss: 131.08
epoch train time: 0:00:01.742221
elapsed time: 0:04:05.596407
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 22:26:33.866211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.07
 ---- batch: 020 ----
mean loss: 133.17
 ---- batch: 030 ----
mean loss: 138.57
 ---- batch: 040 ----
mean loss: 135.99
train mean loss: 135.42
epoch train time: 0:00:01.723205
elapsed time: 0:04:07.319775
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 22:26:35.589556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.50
 ---- batch: 020 ----
mean loss: 137.41
 ---- batch: 030 ----
mean loss: 135.29
 ---- batch: 040 ----
mean loss: 132.15
train mean loss: 135.37
epoch train time: 0:00:01.680871
elapsed time: 0:04:09.000804
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 22:26:37.270582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.87
 ---- batch: 020 ----
mean loss: 129.37
 ---- batch: 030 ----
mean loss: 131.76
 ---- batch: 040 ----
mean loss: 129.93
train mean loss: 131.31
epoch train time: 0:00:01.785648
elapsed time: 0:04:10.786583
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 22:26:39.056361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.66
 ---- batch: 020 ----
mean loss: 130.44
 ---- batch: 030 ----
mean loss: 137.53
 ---- batch: 040 ----
mean loss: 136.09
train mean loss: 134.40
epoch train time: 0:00:01.687588
elapsed time: 0:04:12.474305
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 22:26:40.744101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.88
 ---- batch: 020 ----
mean loss: 136.65
 ---- batch: 030 ----
mean loss: 130.70
 ---- batch: 040 ----
mean loss: 132.06
train mean loss: 132.71
epoch train time: 0:00:01.791468
elapsed time: 0:04:14.265913
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 22:26:42.535700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.60
 ---- batch: 020 ----
mean loss: 133.07
 ---- batch: 030 ----
mean loss: 132.06
 ---- batch: 040 ----
mean loss: 133.97
train mean loss: 134.52
epoch train time: 0:00:01.678429
elapsed time: 0:04:15.944494
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 22:26:44.214277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.40
 ---- batch: 020 ----
mean loss: 134.84
 ---- batch: 030 ----
mean loss: 131.58
 ---- batch: 040 ----
mean loss: 139.67
train mean loss: 135.19
epoch train time: 0:00:01.676044
elapsed time: 0:04:17.620694
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 22:26:45.890486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.94
 ---- batch: 020 ----
mean loss: 133.59
 ---- batch: 030 ----
mean loss: 130.47
 ---- batch: 040 ----
mean loss: 134.66
train mean loss: 132.32
epoch train time: 0:00:01.785891
elapsed time: 0:04:19.406844
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 22:26:47.676626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.78
 ---- batch: 020 ----
mean loss: 134.43
 ---- batch: 030 ----
mean loss: 136.65
 ---- batch: 040 ----
mean loss: 127.18
train mean loss: 132.91
epoch train time: 0:00:01.684276
elapsed time: 0:04:21.091265
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 22:26:49.361044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.59
 ---- batch: 020 ----
mean loss: 130.14
 ---- batch: 030 ----
mean loss: 132.94
 ---- batch: 040 ----
mean loss: 134.39
train mean loss: 132.46
epoch train time: 0:00:01.787794
elapsed time: 0:04:22.879183
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 22:26:51.148957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.85
 ---- batch: 020 ----
mean loss: 137.91
 ---- batch: 030 ----
mean loss: 133.79
 ---- batch: 040 ----
mean loss: 128.55
train mean loss: 135.78
epoch train time: 0:00:01.680078
elapsed time: 0:04:24.559404
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 22:26:52.829185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.20
 ---- batch: 020 ----
mean loss: 133.81
 ---- batch: 030 ----
mean loss: 130.99
 ---- batch: 040 ----
mean loss: 129.07
train mean loss: 133.61
epoch train time: 0:00:01.794793
elapsed time: 0:04:26.354328
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 22:26:54.624123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.01
 ---- batch: 020 ----
mean loss: 131.77
 ---- batch: 030 ----
mean loss: 132.28
 ---- batch: 040 ----
mean loss: 139.85
train mean loss: 133.33
epoch train time: 0:00:01.698803
elapsed time: 0:04:28.053283
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 22:26:56.323062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.25
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 137.46
 ---- batch: 040 ----
mean loss: 134.51
train mean loss: 134.86
epoch train time: 0:00:01.680847
elapsed time: 0:04:29.734278
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 22:26:58.004058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.36
 ---- batch: 020 ----
mean loss: 139.93
 ---- batch: 030 ----
mean loss: 128.54
 ---- batch: 040 ----
mean loss: 130.47
train mean loss: 133.69
epoch train time: 0:00:01.782481
elapsed time: 0:04:31.516932
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 22:26:59.786696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.54
 ---- batch: 020 ----
mean loss: 130.83
 ---- batch: 030 ----
mean loss: 133.68
 ---- batch: 040 ----
mean loss: 127.66
train mean loss: 131.03
epoch train time: 0:00:01.688712
elapsed time: 0:04:33.205771
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 22:27:01.475565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.08
 ---- batch: 020 ----
mean loss: 131.40
 ---- batch: 030 ----
mean loss: 132.19
 ---- batch: 040 ----
mean loss: 129.01
train mean loss: 130.65
epoch train time: 0:00:01.783725
elapsed time: 0:04:34.989638
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 22:27:03.259414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.36
 ---- batch: 020 ----
mean loss: 127.69
 ---- batch: 030 ----
mean loss: 131.92
 ---- batch: 040 ----
mean loss: 135.70
train mean loss: 131.87
epoch train time: 0:00:01.694967
elapsed time: 0:04:36.684757
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 22:27:04.954551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.52
 ---- batch: 020 ----
mean loss: 132.38
 ---- batch: 030 ----
mean loss: 125.08
 ---- batch: 040 ----
mean loss: 131.94
train mean loss: 130.12
epoch train time: 0:00:01.785754
elapsed time: 0:04:38.470717
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 22:27:06.740540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.15
 ---- batch: 020 ----
mean loss: 128.70
 ---- batch: 030 ----
mean loss: 127.30
 ---- batch: 040 ----
mean loss: 131.15
train mean loss: 130.71
epoch train time: 0:00:01.696391
elapsed time: 0:04:40.167321
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 22:27:08.437101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.78
 ---- batch: 020 ----
mean loss: 136.61
 ---- batch: 030 ----
mean loss: 128.49
 ---- batch: 040 ----
mean loss: 128.93
train mean loss: 130.30
epoch train time: 0:00:01.678225
elapsed time: 0:04:41.845675
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 22:27:10.115451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.77
 ---- batch: 020 ----
mean loss: 132.64
 ---- batch: 030 ----
mean loss: 135.32
 ---- batch: 040 ----
mean loss: 131.14
train mean loss: 131.07
epoch train time: 0:00:01.786882
elapsed time: 0:04:43.632692
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 22:27:11.902467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.21
 ---- batch: 020 ----
mean loss: 138.25
 ---- batch: 030 ----
mean loss: 134.50
 ---- batch: 040 ----
mean loss: 137.70
train mean loss: 134.42
epoch train time: 0:00:01.686281
elapsed time: 0:04:45.319108
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 22:27:13.588888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.41
 ---- batch: 020 ----
mean loss: 133.02
 ---- batch: 030 ----
mean loss: 132.23
 ---- batch: 040 ----
mean loss: 129.95
train mean loss: 132.13
epoch train time: 0:00:01.791471
elapsed time: 0:04:47.110760
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 22:27:15.380570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.66
 ---- batch: 020 ----
mean loss: 130.24
 ---- batch: 030 ----
mean loss: 132.07
 ---- batch: 040 ----
mean loss: 132.00
train mean loss: 131.56
epoch train time: 0:00:01.684653
elapsed time: 0:04:48.795612
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 22:27:17.065395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.29
 ---- batch: 020 ----
mean loss: 122.94
 ---- batch: 030 ----
mean loss: 129.77
 ---- batch: 040 ----
mean loss: 137.72
train mean loss: 129.68
epoch train time: 0:00:01.684188
elapsed time: 0:04:50.479945
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 22:27:18.749737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.95
 ---- batch: 020 ----
mean loss: 126.02
 ---- batch: 030 ----
mean loss: 129.61
 ---- batch: 040 ----
mean loss: 133.61
train mean loss: 128.87
epoch train time: 0:00:01.709001
elapsed time: 0:04:52.189098
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 22:27:20.458880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.87
 ---- batch: 020 ----
mean loss: 129.54
 ---- batch: 030 ----
mean loss: 133.10
 ---- batch: 040 ----
mean loss: 128.74
train mean loss: 129.62
epoch train time: 0:00:01.688661
elapsed time: 0:04:53.877914
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 22:27:22.147695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.40
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 126.86
 ---- batch: 040 ----
mean loss: 132.23
train mean loss: 129.62
epoch train time: 0:00:01.795971
elapsed time: 0:04:55.674025
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 22:27:23.943820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.96
 ---- batch: 020 ----
mean loss: 124.34
 ---- batch: 030 ----
mean loss: 132.14
 ---- batch: 040 ----
mean loss: 125.68
train mean loss: 129.28
epoch train time: 0:00:01.684766
elapsed time: 0:04:57.358950
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 22:27:25.628730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.23
 ---- batch: 020 ----
mean loss: 130.09
 ---- batch: 030 ----
mean loss: 129.61
 ---- batch: 040 ----
mean loss: 127.88
train mean loss: 128.20
epoch train time: 0:00:01.788307
elapsed time: 0:04:59.147395
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 22:27:27.417178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.05
 ---- batch: 020 ----
mean loss: 126.60
 ---- batch: 030 ----
mean loss: 126.46
 ---- batch: 040 ----
mean loss: 128.63
train mean loss: 129.20
epoch train time: 0:00:01.695047
elapsed time: 0:05:00.842602
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 22:27:29.112378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.50
 ---- batch: 020 ----
mean loss: 129.47
 ---- batch: 030 ----
mean loss: 129.26
 ---- batch: 040 ----
mean loss: 129.39
train mean loss: 129.01
epoch train time: 0:00:01.729257
elapsed time: 0:05:02.571989
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 22:27:30.841767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.75
 ---- batch: 020 ----
mean loss: 125.30
 ---- batch: 030 ----
mean loss: 129.02
 ---- batch: 040 ----
mean loss: 127.88
train mean loss: 128.35
epoch train time: 0:00:01.742101
elapsed time: 0:05:04.314222
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 22:27:32.584001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.51
 ---- batch: 020 ----
mean loss: 130.80
 ---- batch: 030 ----
mean loss: 132.35
 ---- batch: 040 ----
mean loss: 136.59
train mean loss: 133.04
epoch train time: 0:00:01.682397
elapsed time: 0:05:05.996768
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 22:27:34.266549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.16
 ---- batch: 020 ----
mean loss: 125.62
 ---- batch: 030 ----
mean loss: 134.17
 ---- batch: 040 ----
mean loss: 131.33
train mean loss: 131.04
epoch train time: 0:00:01.793112
elapsed time: 0:05:07.790004
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 22:27:36.059813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.57
 ---- batch: 020 ----
mean loss: 134.09
 ---- batch: 030 ----
mean loss: 127.47
 ---- batch: 040 ----
mean loss: 126.40
train mean loss: 131.03
epoch train time: 0:00:01.684708
elapsed time: 0:05:09.474881
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 22:27:37.744670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.87
 ---- batch: 020 ----
mean loss: 128.87
 ---- batch: 030 ----
mean loss: 132.40
 ---- batch: 040 ----
mean loss: 129.83
train mean loss: 130.01
epoch train time: 0:00:01.796164
elapsed time: 0:05:11.271199
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 22:27:39.541015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.27
 ---- batch: 020 ----
mean loss: 129.02
 ---- batch: 030 ----
mean loss: 127.16
 ---- batch: 040 ----
mean loss: 128.65
train mean loss: 128.41
epoch train time: 0:00:01.693439
elapsed time: 0:05:12.964813
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 22:27:41.234596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.24
 ---- batch: 020 ----
mean loss: 128.02
 ---- batch: 030 ----
mean loss: 129.39
 ---- batch: 040 ----
mean loss: 133.44
train mean loss: 128.70
epoch train time: 0:00:01.713750
elapsed time: 0:05:14.678743
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 22:27:42.948519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.69
 ---- batch: 020 ----
mean loss: 132.16
 ---- batch: 030 ----
mean loss: 128.44
 ---- batch: 040 ----
mean loss: 124.86
train mean loss: 130.23
epoch train time: 0:00:01.773985
elapsed time: 0:05:16.452892
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 22:27:44.722680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.31
 ---- batch: 020 ----
mean loss: 128.71
 ---- batch: 030 ----
mean loss: 128.36
 ---- batch: 040 ----
mean loss: 126.47
train mean loss: 127.80
epoch train time: 0:00:01.685159
elapsed time: 0:05:18.138207
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 22:27:46.407987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.80
 ---- batch: 020 ----
mean loss: 126.14
 ---- batch: 030 ----
mean loss: 126.57
 ---- batch: 040 ----
mean loss: 124.60
train mean loss: 125.80
epoch train time: 0:00:01.786347
elapsed time: 0:05:19.924694
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 22:27:48.194476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.44
 ---- batch: 020 ----
mean loss: 129.85
 ---- batch: 030 ----
mean loss: 127.92
 ---- batch: 040 ----
mean loss: 128.51
train mean loss: 128.81
epoch train time: 0:00:01.680110
elapsed time: 0:05:21.604946
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 22:27:49.874726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.45
 ---- batch: 020 ----
mean loss: 126.64
 ---- batch: 030 ----
mean loss: 123.22
 ---- batch: 040 ----
mean loss: 126.86
train mean loss: 126.05
epoch train time: 0:00:01.781734
elapsed time: 0:05:23.386819
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 22:27:51.656608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.10
 ---- batch: 020 ----
mean loss: 130.37
 ---- batch: 030 ----
mean loss: 130.96
 ---- batch: 040 ----
mean loss: 128.69
train mean loss: 128.96
epoch train time: 0:00:01.690929
elapsed time: 0:05:25.077895
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 22:27:53.347676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.31
 ---- batch: 020 ----
mean loss: 131.47
 ---- batch: 030 ----
mean loss: 127.90
 ---- batch: 040 ----
mean loss: 127.57
train mean loss: 129.50
epoch train time: 0:00:01.679444
elapsed time: 0:05:26.757463
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 22:27:55.027236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.16
 ---- batch: 020 ----
mean loss: 125.36
 ---- batch: 030 ----
mean loss: 124.97
 ---- batch: 040 ----
mean loss: 129.27
train mean loss: 127.30
epoch train time: 0:00:01.795221
elapsed time: 0:05:28.552840
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 22:27:56.822654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.69
 ---- batch: 020 ----
mean loss: 128.68
 ---- batch: 030 ----
mean loss: 124.34
 ---- batch: 040 ----
mean loss: 129.19
train mean loss: 127.59
epoch train time: 0:00:01.688234
elapsed time: 0:05:30.241250
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 22:27:58.511029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.25
 ---- batch: 020 ----
mean loss: 125.12
 ---- batch: 030 ----
mean loss: 136.55
 ---- batch: 040 ----
mean loss: 132.94
train mean loss: 129.16
epoch train time: 0:00:01.790521
elapsed time: 0:05:32.031894
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 22:28:00.301668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.61
 ---- batch: 020 ----
mean loss: 136.98
 ---- batch: 030 ----
mean loss: 131.47
 ---- batch: 040 ----
mean loss: 137.64
train mean loss: 133.12
epoch train time: 0:00:01.680466
elapsed time: 0:05:33.712496
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 22:28:01.982292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.76
 ---- batch: 020 ----
mean loss: 123.37
 ---- batch: 030 ----
mean loss: 128.67
 ---- batch: 040 ----
mean loss: 130.97
train mean loss: 129.17
epoch train time: 0:00:01.787805
elapsed time: 0:05:35.500490
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 22:28:03.770272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.36
 ---- batch: 020 ----
mean loss: 127.75
 ---- batch: 030 ----
mean loss: 124.94
 ---- batch: 040 ----
mean loss: 124.79
train mean loss: 127.05
epoch train time: 0:00:01.688944
elapsed time: 0:05:37.189580
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 22:28:05.459358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.49
 ---- batch: 020 ----
mean loss: 124.63
 ---- batch: 030 ----
mean loss: 122.66
 ---- batch: 040 ----
mean loss: 126.40
train mean loss: 123.91
epoch train time: 0:00:01.679043
elapsed time: 0:05:38.868775
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 22:28:07.138550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.00
 ---- batch: 020 ----
mean loss: 128.99
 ---- batch: 030 ----
mean loss: 128.06
 ---- batch: 040 ----
mean loss: 124.27
train mean loss: 126.35
epoch train time: 0:00:01.785954
elapsed time: 0:05:40.654859
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 22:28:08.924639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.76
 ---- batch: 020 ----
mean loss: 128.25
 ---- batch: 030 ----
mean loss: 120.11
 ---- batch: 040 ----
mean loss: 128.66
train mean loss: 127.07
epoch train time: 0:00:01.688698
elapsed time: 0:05:42.343722
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 22:28:10.613503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.26
 ---- batch: 020 ----
mean loss: 127.18
 ---- batch: 030 ----
mean loss: 128.11
 ---- batch: 040 ----
mean loss: 131.31
train mean loss: 127.34
epoch train time: 0:00:01.797656
elapsed time: 0:05:44.141513
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 22:28:12.411289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.10
 ---- batch: 020 ----
mean loss: 124.90
 ---- batch: 030 ----
mean loss: 123.93
 ---- batch: 040 ----
mean loss: 129.47
train mean loss: 125.85
epoch train time: 0:00:01.696958
elapsed time: 0:05:45.838615
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 22:28:14.108418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.91
 ---- batch: 020 ----
mean loss: 129.91
 ---- batch: 030 ----
mean loss: 125.93
 ---- batch: 040 ----
mean loss: 124.90
train mean loss: 126.61
epoch train time: 0:00:01.788575
elapsed time: 0:05:47.627350
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 22:28:15.897144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.44
 ---- batch: 020 ----
mean loss: 122.20
 ---- batch: 030 ----
mean loss: 126.90
 ---- batch: 040 ----
mean loss: 126.33
train mean loss: 124.93
epoch train time: 0:00:01.696695
elapsed time: 0:05:49.324189
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 22:28:17.593997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.11
 ---- batch: 020 ----
mean loss: 130.27
 ---- batch: 030 ----
mean loss: 125.99
 ---- batch: 040 ----
mean loss: 122.90
train mean loss: 125.36
epoch train time: 0:00:01.681779
elapsed time: 0:05:51.006127
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 22:28:19.275907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.01
 ---- batch: 020 ----
mean loss: 127.91
 ---- batch: 030 ----
mean loss: 124.96
 ---- batch: 040 ----
mean loss: 125.12
train mean loss: 124.59
epoch train time: 0:00:01.789106
elapsed time: 0:05:52.795362
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 22:28:21.065137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.68
 ---- batch: 020 ----
mean loss: 132.26
 ---- batch: 030 ----
mean loss: 127.26
 ---- batch: 040 ----
mean loss: 127.33
train mean loss: 127.83
epoch train time: 0:00:01.691915
elapsed time: 0:05:54.487415
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 22:28:22.757194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.12
 ---- batch: 020 ----
mean loss: 124.87
 ---- batch: 030 ----
mean loss: 127.28
 ---- batch: 040 ----
mean loss: 128.21
train mean loss: 126.38
epoch train time: 0:00:01.789490
elapsed time: 0:05:56.277088
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 22:28:24.546882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.97
 ---- batch: 020 ----
mean loss: 126.52
 ---- batch: 030 ----
mean loss: 125.46
 ---- batch: 040 ----
mean loss: 122.06
train mean loss: 125.06
epoch train time: 0:00:01.688753
elapsed time: 0:05:57.965988
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 22:28:26.235765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.68
 ---- batch: 020 ----
mean loss: 132.03
 ---- batch: 030 ----
mean loss: 129.13
 ---- batch: 040 ----
mean loss: 128.09
train mean loss: 127.52
epoch train time: 0:00:01.739813
elapsed time: 0:05:59.705920
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 22:28:27.975692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.10
 ---- batch: 020 ----
mean loss: 123.66
 ---- batch: 030 ----
mean loss: 125.53
 ---- batch: 040 ----
mean loss: 121.90
train mean loss: 125.41
epoch train time: 0:00:01.741932
elapsed time: 0:06:01.447986
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 22:28:29.717778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.76
 ---- batch: 020 ----
mean loss: 122.56
 ---- batch: 030 ----
mean loss: 122.19
 ---- batch: 040 ----
mean loss: 123.07
train mean loss: 121.56
epoch train time: 0:00:01.683887
elapsed time: 0:06:03.132050
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 22:28:31.401820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.50
 ---- batch: 020 ----
mean loss: 120.32
 ---- batch: 030 ----
mean loss: 125.76
 ---- batch: 040 ----
mean loss: 119.07
train mean loss: 122.30
epoch train time: 0:00:01.790405
elapsed time: 0:06:04.922570
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 22:28:33.192345
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.25
 ---- batch: 020 ----
mean loss: 126.36
 ---- batch: 030 ----
mean loss: 121.38
 ---- batch: 040 ----
mean loss: 124.77
train mean loss: 123.62
epoch train time: 0:00:01.682055
elapsed time: 0:06:06.604761
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 22:28:34.874542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.75
 ---- batch: 020 ----
mean loss: 122.83
 ---- batch: 030 ----
mean loss: 122.67
 ---- batch: 040 ----
mean loss: 125.95
train mean loss: 121.79
epoch train time: 0:00:01.787905
elapsed time: 0:06:08.392845
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 22:28:36.662626
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.25
 ---- batch: 020 ----
mean loss: 122.16
 ---- batch: 030 ----
mean loss: 122.77
 ---- batch: 040 ----
mean loss: 122.33
train mean loss: 121.51
epoch train time: 0:00:01.698640
elapsed time: 0:06:10.091642
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 22:28:38.361468
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.89
 ---- batch: 020 ----
mean loss: 123.94
 ---- batch: 030 ----
mean loss: 121.63
 ---- batch: 040 ----
mean loss: 119.62
train mean loss: 122.89
epoch train time: 0:00:01.684040
elapsed time: 0:06:11.775904
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 22:28:40.045710
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.07
 ---- batch: 020 ----
mean loss: 120.17
 ---- batch: 030 ----
mean loss: 119.64
 ---- batch: 040 ----
mean loss: 122.24
train mean loss: 121.58
epoch train time: 0:00:01.786949
elapsed time: 0:06:13.563014
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 22:28:41.832790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.06
 ---- batch: 020 ----
mean loss: 125.28
 ---- batch: 030 ----
mean loss: 116.77
 ---- batch: 040 ----
mean loss: 122.86
train mean loss: 122.08
epoch train time: 0:00:01.687902
elapsed time: 0:06:15.251058
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 22:28:43.520838
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.67
 ---- batch: 020 ----
mean loss: 122.76
 ---- batch: 030 ----
mean loss: 118.94
 ---- batch: 040 ----
mean loss: 128.08
train mean loss: 122.15
epoch train time: 0:00:01.784768
elapsed time: 0:06:17.035969
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 22:28:45.305760
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.76
 ---- batch: 020 ----
mean loss: 119.64
 ---- batch: 030 ----
mean loss: 123.59
 ---- batch: 040 ----
mean loss: 121.30
train mean loss: 121.88
epoch train time: 0:00:01.687104
elapsed time: 0:06:18.723226
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 22:28:46.993003
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.54
 ---- batch: 020 ----
mean loss: 122.17
 ---- batch: 030 ----
mean loss: 121.72
 ---- batch: 040 ----
mean loss: 121.77
train mean loss: 122.05
epoch train time: 0:00:01.782674
elapsed time: 0:06:20.506030
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 22:28:48.775815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.11
 ---- batch: 020 ----
mean loss: 122.43
 ---- batch: 030 ----
mean loss: 122.61
 ---- batch: 040 ----
mean loss: 123.78
train mean loss: 122.58
epoch train time: 0:00:01.696443
elapsed time: 0:06:22.202633
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 22:28:50.472432
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.47
 ---- batch: 020 ----
mean loss: 117.57
 ---- batch: 030 ----
mean loss: 124.98
 ---- batch: 040 ----
mean loss: 123.41
train mean loss: 121.41
epoch train time: 0:00:01.689790
elapsed time: 0:06:23.892597
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 22:28:52.162376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.53
 ---- batch: 020 ----
mean loss: 123.76
 ---- batch: 030 ----
mean loss: 121.57
 ---- batch: 040 ----
mean loss: 123.09
train mean loss: 122.54
epoch train time: 0:00:01.785018
elapsed time: 0:06:25.677737
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 22:28:53.947510
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.41
 ---- batch: 020 ----
mean loss: 121.47
 ---- batch: 030 ----
mean loss: 126.40
 ---- batch: 040 ----
mean loss: 121.51
train mean loss: 123.73
epoch train time: 0:00:01.688015
elapsed time: 0:06:27.365898
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 22:28:55.635681
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.09
 ---- batch: 020 ----
mean loss: 120.77
 ---- batch: 030 ----
mean loss: 120.28
 ---- batch: 040 ----
mean loss: 120.75
train mean loss: 120.73
epoch train time: 0:00:01.792355
elapsed time: 0:06:29.158405
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 22:28:57.428202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.69
 ---- batch: 020 ----
mean loss: 120.70
 ---- batch: 030 ----
mean loss: 127.48
 ---- batch: 040 ----
mean loss: 120.96
train mean loss: 123.39
epoch train time: 0:00:01.683199
elapsed time: 0:06:30.841768
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 22:28:59.111568
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.25
 ---- batch: 020 ----
mean loss: 123.46
 ---- batch: 030 ----
mean loss: 119.31
 ---- batch: 040 ----
mean loss: 122.04
train mean loss: 121.34
epoch train time: 0:00:01.782628
elapsed time: 0:06:32.624581
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 22:29:00.894362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.62
 ---- batch: 020 ----
mean loss: 124.15
 ---- batch: 030 ----
mean loss: 122.69
 ---- batch: 040 ----
mean loss: 120.34
train mean loss: 122.77
epoch train time: 0:00:01.704900
elapsed time: 0:06:34.329616
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 22:29:02.599393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.36
 ---- batch: 020 ----
mean loss: 122.59
 ---- batch: 030 ----
mean loss: 122.78
 ---- batch: 040 ----
mean loss: 123.41
train mean loss: 122.14
epoch train time: 0:00:01.685797
elapsed time: 0:06:36.015574
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 22:29:04.285353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.94
 ---- batch: 020 ----
mean loss: 123.47
 ---- batch: 030 ----
mean loss: 117.98
 ---- batch: 040 ----
mean loss: 126.08
train mean loss: 122.98
epoch train time: 0:00:01.802321
elapsed time: 0:06:37.818047
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 22:29:06.087824
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.33
 ---- batch: 020 ----
mean loss: 123.29
 ---- batch: 030 ----
mean loss: 121.83
 ---- batch: 040 ----
mean loss: 116.56
train mean loss: 121.02
epoch train time: 0:00:01.686580
elapsed time: 0:06:39.504767
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 22:29:07.774550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.33
 ---- batch: 020 ----
mean loss: 121.80
 ---- batch: 030 ----
mean loss: 129.22
 ---- batch: 040 ----
mean loss: 118.33
train mean loss: 122.78
epoch train time: 0:00:01.790479
elapsed time: 0:06:41.295393
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 22:29:09.565184
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.20
 ---- batch: 020 ----
mean loss: 125.63
 ---- batch: 030 ----
mean loss: 118.45
 ---- batch: 040 ----
mean loss: 122.32
train mean loss: 122.90
epoch train time: 0:00:01.687489
elapsed time: 0:06:42.983033
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 22:29:11.252830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.26
 ---- batch: 020 ----
mean loss: 118.09
 ---- batch: 030 ----
mean loss: 124.37
 ---- batch: 040 ----
mean loss: 125.02
train mean loss: 121.95
epoch train time: 0:00:01.678086
elapsed time: 0:06:44.661260
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 22:29:12.931033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.32
 ---- batch: 020 ----
mean loss: 120.95
 ---- batch: 030 ----
mean loss: 120.45
 ---- batch: 040 ----
mean loss: 121.24
train mean loss: 122.05
epoch train time: 0:00:01.787154
elapsed time: 0:06:46.448557
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 22:29:14.718340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.90
 ---- batch: 020 ----
mean loss: 123.55
 ---- batch: 030 ----
mean loss: 127.10
 ---- batch: 040 ----
mean loss: 114.71
train mean loss: 122.48
epoch train time: 0:00:01.692041
elapsed time: 0:06:48.140745
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 22:29:16.410528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.48
 ---- batch: 020 ----
mean loss: 117.57
 ---- batch: 030 ----
mean loss: 122.22
 ---- batch: 040 ----
mean loss: 122.12
train mean loss: 120.60
epoch train time: 0:00:01.799052
elapsed time: 0:06:49.939936
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 22:29:18.209717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.59
 ---- batch: 020 ----
mean loss: 119.80
 ---- batch: 030 ----
mean loss: 121.55
 ---- batch: 040 ----
mean loss: 125.84
train mean loss: 122.51
epoch train time: 0:00:01.691356
elapsed time: 0:06:51.631445
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 22:29:19.901250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.70
 ---- batch: 020 ----
mean loss: 124.98
 ---- batch: 030 ----
mean loss: 120.36
 ---- batch: 040 ----
mean loss: 118.72
train mean loss: 121.11
epoch train time: 0:00:01.783900
elapsed time: 0:06:53.415497
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 22:29:21.685312
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.60
 ---- batch: 020 ----
mean loss: 117.99
 ---- batch: 030 ----
mean loss: 123.80
 ---- batch: 040 ----
mean loss: 123.99
train mean loss: 121.43
epoch train time: 0:00:01.691929
elapsed time: 0:06:55.107623
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 22:29:23.377402
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.20
 ---- batch: 020 ----
mean loss: 125.74
 ---- batch: 030 ----
mean loss: 124.16
 ---- batch: 040 ----
mean loss: 121.11
train mean loss: 123.18
epoch train time: 0:00:01.682144
elapsed time: 0:06:56.789915
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 22:29:25.059725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.88
 ---- batch: 020 ----
mean loss: 126.80
 ---- batch: 030 ----
mean loss: 121.99
 ---- batch: 040 ----
mean loss: 120.83
train mean loss: 122.17
epoch train time: 0:00:01.804597
elapsed time: 0:06:58.594702
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 22:29:26.864472
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.94
 ---- batch: 020 ----
mean loss: 123.34
 ---- batch: 030 ----
mean loss: 123.66
 ---- batch: 040 ----
mean loss: 123.36
train mean loss: 122.66
epoch train time: 0:00:01.685975
elapsed time: 0:07:00.280811
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 22:29:28.550592
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.18
 ---- batch: 020 ----
mean loss: 122.68
 ---- batch: 030 ----
mean loss: 119.43
 ---- batch: 040 ----
mean loss: 122.58
train mean loss: 121.52
epoch train time: 0:00:01.796192
elapsed time: 0:07:02.077131
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 22:29:30.346905
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.58
 ---- batch: 020 ----
mean loss: 118.14
 ---- batch: 030 ----
mean loss: 122.40
 ---- batch: 040 ----
mean loss: 121.70
train mean loss: 120.71
epoch train time: 0:00:01.695933
elapsed time: 0:07:03.773210
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 22:29:32.042990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.40
 ---- batch: 020 ----
mean loss: 123.80
 ---- batch: 030 ----
mean loss: 119.24
 ---- batch: 040 ----
mean loss: 121.71
train mean loss: 121.76
epoch train time: 0:00:01.780464
elapsed time: 0:07:05.553810
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 22:29:33.823605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.52
 ---- batch: 020 ----
mean loss: 121.48
 ---- batch: 030 ----
mean loss: 125.02
 ---- batch: 040 ----
mean loss: 124.19
train mean loss: 122.75
epoch train time: 0:00:01.702419
elapsed time: 0:07:07.256382
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 22:29:35.526172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.76
 ---- batch: 020 ----
mean loss: 120.63
 ---- batch: 030 ----
mean loss: 123.98
 ---- batch: 040 ----
mean loss: 122.45
train mean loss: 122.10
epoch train time: 0:00:01.688270
elapsed time: 0:07:08.944804
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 22:29:37.214588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.35
 ---- batch: 020 ----
mean loss: 122.72
 ---- batch: 030 ----
mean loss: 118.72
 ---- batch: 040 ----
mean loss: 124.97
train mean loss: 121.62
epoch train time: 0:00:01.783453
elapsed time: 0:07:10.728435
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 22:29:38.998229
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.47
 ---- batch: 020 ----
mean loss: 121.57
 ---- batch: 030 ----
mean loss: 123.08
 ---- batch: 040 ----
mean loss: 120.74
train mean loss: 120.90
epoch train time: 0:00:01.684816
elapsed time: 0:07:12.413402
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 22:29:40.683197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.63
 ---- batch: 020 ----
mean loss: 121.76
 ---- batch: 030 ----
mean loss: 121.33
 ---- batch: 040 ----
mean loss: 120.40
train mean loss: 121.31
epoch train time: 0:00:01.795904
elapsed time: 0:07:14.209461
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 22:29:42.479257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.40
 ---- batch: 020 ----
mean loss: 118.65
 ---- batch: 030 ----
mean loss: 120.83
 ---- batch: 040 ----
mean loss: 121.24
train mean loss: 121.26
epoch train time: 0:00:01.692129
elapsed time: 0:07:15.901771
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 22:29:44.171560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.27
 ---- batch: 020 ----
mean loss: 121.15
 ---- batch: 030 ----
mean loss: 121.09
 ---- batch: 040 ----
mean loss: 119.91
train mean loss: 120.40
epoch train time: 0:00:01.678324
elapsed time: 0:07:17.580233
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 22:29:45.850009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.37
 ---- batch: 020 ----
mean loss: 125.53
 ---- batch: 030 ----
mean loss: 127.97
 ---- batch: 040 ----
mean loss: 117.94
train mean loss: 121.59
epoch train time: 0:00:01.782529
elapsed time: 0:07:19.362907
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 22:29:47.632702
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.36
 ---- batch: 020 ----
mean loss: 121.80
 ---- batch: 030 ----
mean loss: 126.12
 ---- batch: 040 ----
mean loss: 117.90
train mean loss: 121.89
epoch train time: 0:00:01.698218
elapsed time: 0:07:21.061291
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 22:29:49.331081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.10
 ---- batch: 020 ----
mean loss: 123.24
 ---- batch: 030 ----
mean loss: 119.49
 ---- batch: 040 ----
mean loss: 117.37
train mean loss: 120.91
epoch train time: 0:00:01.794439
elapsed time: 0:07:22.855873
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 22:29:51.125650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.42
 ---- batch: 020 ----
mean loss: 122.25
 ---- batch: 030 ----
mean loss: 120.16
 ---- batch: 040 ----
mean loss: 120.04
train mean loss: 121.95
epoch train time: 0:00:01.695445
elapsed time: 0:07:24.551460
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 22:29:52.821238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.92
 ---- batch: 020 ----
mean loss: 119.74
 ---- batch: 030 ----
mean loss: 123.39
 ---- batch: 040 ----
mean loss: 118.58
train mean loss: 121.72
epoch train time: 0:00:01.788136
elapsed time: 0:07:26.345028
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_2/checkpoint.pth.tar
**** end time: 2019-09-20 22:29:54.614777 ****
