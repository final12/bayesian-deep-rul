Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_7', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24657
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:22:31.514646 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:22:31.518097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4321.41
 ---- batch: 020 ----
mean loss: 4150.16
 ---- batch: 030 ----
mean loss: 4125.18
 ---- batch: 040 ----
mean loss: 4004.69
 ---- batch: 050 ----
mean loss: 3869.94
 ---- batch: 060 ----
mean loss: 3910.63
 ---- batch: 070 ----
mean loss: 3785.52
 ---- batch: 080 ----
mean loss: 3786.88
 ---- batch: 090 ----
mean loss: 3733.01
train mean loss: 3947.05
epoch train time: 0:00:33.522941
elapsed time: 0:00:33.528829
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:23:05.043532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3570.23
 ---- batch: 020 ----
mean loss: 3605.85
 ---- batch: 030 ----
mean loss: 3535.10
 ---- batch: 040 ----
mean loss: 3486.53
 ---- batch: 050 ----
mean loss: 3362.81
 ---- batch: 060 ----
mean loss: 3364.53
 ---- batch: 070 ----
mean loss: 3302.82
 ---- batch: 080 ----
mean loss: 3217.83
 ---- batch: 090 ----
mean loss: 3188.66
train mean loss: 3387.78
epoch train time: 0:00:00.506471
elapsed time: 0:00:34.035458
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:23:05.550158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3065.49
 ---- batch: 020 ----
mean loss: 3005.20
 ---- batch: 030 ----
mean loss: 2991.29
 ---- batch: 040 ----
mean loss: 2944.32
 ---- batch: 050 ----
mean loss: 2911.11
 ---- batch: 060 ----
mean loss: 2860.26
 ---- batch: 070 ----
mean loss: 2845.58
 ---- batch: 080 ----
mean loss: 2772.09
 ---- batch: 090 ----
mean loss: 2697.17
train mean loss: 2888.85
epoch train time: 0:00:00.498417
elapsed time: 0:00:34.534020
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:23:06.048742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2655.86
 ---- batch: 020 ----
mean loss: 2581.43
 ---- batch: 030 ----
mean loss: 2533.47
 ---- batch: 040 ----
mean loss: 2551.56
 ---- batch: 050 ----
mean loss: 2487.63
 ---- batch: 060 ----
mean loss: 2469.08
 ---- batch: 070 ----
mean loss: 2381.67
 ---- batch: 080 ----
mean loss: 2361.97
 ---- batch: 090 ----
mean loss: 2355.37
train mean loss: 2474.98
epoch train time: 0:00:00.497209
elapsed time: 0:00:35.031407
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:23:06.546143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2255.97
 ---- batch: 020 ----
mean loss: 2245.52
 ---- batch: 030 ----
mean loss: 2188.74
 ---- batch: 040 ----
mean loss: 2155.03
 ---- batch: 050 ----
mean loss: 2170.97
 ---- batch: 060 ----
mean loss: 2083.98
 ---- batch: 070 ----
mean loss: 2075.37
 ---- batch: 080 ----
mean loss: 2075.90
 ---- batch: 090 ----
mean loss: 2017.48
train mean loss: 2130.79
epoch train time: 0:00:00.502511
elapsed time: 0:00:35.534118
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:23:07.048817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1939.54
 ---- batch: 020 ----
mean loss: 1907.21
 ---- batch: 030 ----
mean loss: 1915.98
 ---- batch: 040 ----
mean loss: 1868.47
 ---- batch: 050 ----
mean loss: 1902.85
 ---- batch: 060 ----
mean loss: 1794.37
 ---- batch: 070 ----
mean loss: 1815.47
 ---- batch: 080 ----
mean loss: 1818.88
 ---- batch: 090 ----
mean loss: 1737.57
train mean loss: 1846.70
epoch train time: 0:00:00.497467
elapsed time: 0:00:36.031742
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:23:07.546442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1715.49
 ---- batch: 020 ----
mean loss: 1707.46
 ---- batch: 030 ----
mean loss: 1623.93
 ---- batch: 040 ----
mean loss: 1656.08
 ---- batch: 050 ----
mean loss: 1625.00
 ---- batch: 060 ----
mean loss: 1619.21
 ---- batch: 070 ----
mean loss: 1582.98
 ---- batch: 080 ----
mean loss: 1571.05
 ---- batch: 090 ----
mean loss: 1554.92
train mean loss: 1620.29
epoch train time: 0:00:00.509931
elapsed time: 0:00:36.541834
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:23:08.056536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1495.08
 ---- batch: 020 ----
mean loss: 1501.91
 ---- batch: 030 ----
mean loss: 1501.22
 ---- batch: 040 ----
mean loss: 1425.51
 ---- batch: 050 ----
mean loss: 1463.65
 ---- batch: 060 ----
mean loss: 1441.08
 ---- batch: 070 ----
mean loss: 1392.07
 ---- batch: 080 ----
mean loss: 1412.66
 ---- batch: 090 ----
mean loss: 1370.70
train mean loss: 1438.83
epoch train time: 0:00:00.496878
elapsed time: 0:00:37.038882
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:23:08.553580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1361.40
 ---- batch: 020 ----
mean loss: 1318.57
 ---- batch: 030 ----
mean loss: 1317.19
 ---- batch: 040 ----
mean loss: 1300.32
 ---- batch: 050 ----
mean loss: 1325.74
 ---- batch: 060 ----
mean loss: 1269.91
 ---- batch: 070 ----
mean loss: 1286.24
 ---- batch: 080 ----
mean loss: 1241.62
 ---- batch: 090 ----
mean loss: 1269.78
train mean loss: 1296.24
epoch train time: 0:00:00.494705
elapsed time: 0:00:37.533741
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:23:09.048439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1239.04
 ---- batch: 020 ----
mean loss: 1209.91
 ---- batch: 030 ----
mean loss: 1216.64
 ---- batch: 040 ----
mean loss: 1168.13
 ---- batch: 050 ----
mean loss: 1196.49
 ---- batch: 060 ----
mean loss: 1177.25
 ---- batch: 070 ----
mean loss: 1187.84
 ---- batch: 080 ----
mean loss: 1142.57
 ---- batch: 090 ----
mean loss: 1157.91
train mean loss: 1184.21
epoch train time: 0:00:00.492383
elapsed time: 0:00:38.026269
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:23:09.540986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1111.86
 ---- batch: 020 ----
mean loss: 1131.35
 ---- batch: 030 ----
mean loss: 1126.82
 ---- batch: 040 ----
mean loss: 1093.87
 ---- batch: 050 ----
mean loss: 1093.42
 ---- batch: 060 ----
mean loss: 1106.95
 ---- batch: 070 ----
mean loss: 1090.26
 ---- batch: 080 ----
mean loss: 1059.99
 ---- batch: 090 ----
mean loss: 1083.29
train mean loss: 1097.99
epoch train time: 0:00:00.513626
elapsed time: 0:00:38.540087
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:23:10.054784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1045.91
 ---- batch: 020 ----
mean loss: 1056.25
 ---- batch: 030 ----
mean loss: 1060.15
 ---- batch: 040 ----
mean loss: 1046.86
 ---- batch: 050 ----
mean loss: 1040.33
 ---- batch: 060 ----
mean loss: 1035.47
 ---- batch: 070 ----
mean loss: 1035.51
 ---- batch: 080 ----
mean loss: 1005.99
 ---- batch: 090 ----
mean loss: 995.99
train mean loss: 1032.61
epoch train time: 0:00:00.491579
elapsed time: 0:00:39.031814
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:23:10.546520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.16
 ---- batch: 020 ----
mean loss: 986.29
 ---- batch: 030 ----
mean loss: 1002.45
 ---- batch: 040 ----
mean loss: 979.98
 ---- batch: 050 ----
mean loss: 1008.76
 ---- batch: 060 ----
mean loss: 980.60
 ---- batch: 070 ----
mean loss: 966.97
 ---- batch: 080 ----
mean loss: 975.88
 ---- batch: 090 ----
mean loss: 973.26
train mean loss: 984.79
epoch train time: 0:00:00.497486
elapsed time: 0:00:39.529454
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:23:11.044152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 967.46
 ---- batch: 020 ----
mean loss: 957.77
 ---- batch: 030 ----
mean loss: 952.95
 ---- batch: 040 ----
mean loss: 934.09
 ---- batch: 050 ----
mean loss: 950.08
 ---- batch: 060 ----
mean loss: 946.01
 ---- batch: 070 ----
mean loss: 958.33
 ---- batch: 080 ----
mean loss: 944.40
 ---- batch: 090 ----
mean loss: 942.47
train mean loss: 949.11
epoch train time: 0:00:00.496869
elapsed time: 0:00:40.026471
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:23:11.541190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.76
 ---- batch: 020 ----
mean loss: 936.13
 ---- batch: 030 ----
mean loss: 927.53
 ---- batch: 040 ----
mean loss: 910.27
 ---- batch: 050 ----
mean loss: 922.09
 ---- batch: 060 ----
mean loss: 915.79
 ---- batch: 070 ----
mean loss: 916.12
 ---- batch: 080 ----
mean loss: 929.86
 ---- batch: 090 ----
mean loss: 921.97
train mean loss: 924.62
epoch train time: 0:00:00.497495
elapsed time: 0:00:40.524145
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:23:12.038833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.59
 ---- batch: 020 ----
mean loss: 910.42
 ---- batch: 030 ----
mean loss: 910.13
 ---- batch: 040 ----
mean loss: 916.44
 ---- batch: 050 ----
mean loss: 910.46
 ---- batch: 060 ----
mean loss: 899.12
 ---- batch: 070 ----
mean loss: 883.28
 ---- batch: 080 ----
mean loss: 904.91
 ---- batch: 090 ----
mean loss: 902.78
train mean loss: 906.84
epoch train time: 0:00:00.498497
elapsed time: 0:00:41.022780
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:23:12.537479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.85
 ---- batch: 020 ----
mean loss: 881.02
 ---- batch: 030 ----
mean loss: 894.77
 ---- batch: 040 ----
mean loss: 901.40
 ---- batch: 050 ----
mean loss: 878.49
 ---- batch: 060 ----
mean loss: 902.53
 ---- batch: 070 ----
mean loss: 907.48
 ---- batch: 080 ----
mean loss: 907.71
 ---- batch: 090 ----
mean loss: 878.97
train mean loss: 895.37
epoch train time: 0:00:00.502838
elapsed time: 0:00:41.525768
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:23:13.040466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.76
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 906.14
 ---- batch: 040 ----
mean loss: 896.04
 ---- batch: 050 ----
mean loss: 877.98
 ---- batch: 060 ----
mean loss: 873.21
 ---- batch: 070 ----
mean loss: 883.68
 ---- batch: 080 ----
mean loss: 881.02
 ---- batch: 090 ----
mean loss: 878.51
train mean loss: 887.30
epoch train time: 0:00:00.490056
elapsed time: 0:00:42.016026
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:23:13.530744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.05
 ---- batch: 020 ----
mean loss: 890.16
 ---- batch: 030 ----
mean loss: 871.87
 ---- batch: 040 ----
mean loss: 887.63
 ---- batch: 050 ----
mean loss: 885.10
 ---- batch: 060 ----
mean loss: 878.30
 ---- batch: 070 ----
mean loss: 865.12
 ---- batch: 080 ----
mean loss: 894.87
 ---- batch: 090 ----
mean loss: 888.90
train mean loss: 882.48
epoch train time: 0:00:00.505896
elapsed time: 0:00:42.522105
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:23:14.036816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.70
 ---- batch: 020 ----
mean loss: 892.43
 ---- batch: 030 ----
mean loss: 879.12
 ---- batch: 040 ----
mean loss: 886.15
 ---- batch: 050 ----
mean loss: 870.39
 ---- batch: 060 ----
mean loss: 880.12
 ---- batch: 070 ----
mean loss: 888.84
 ---- batch: 080 ----
mean loss: 867.43
 ---- batch: 090 ----
mean loss: 874.95
train mean loss: 879.63
epoch train time: 0:00:00.508513
elapsed time: 0:00:43.030786
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:23:14.545531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.79
 ---- batch: 020 ----
mean loss: 870.37
 ---- batch: 030 ----
mean loss: 870.03
 ---- batch: 040 ----
mean loss: 878.10
 ---- batch: 050 ----
mean loss: 895.01
 ---- batch: 060 ----
mean loss: 875.37
 ---- batch: 070 ----
mean loss: 860.17
 ---- batch: 080 ----
mean loss: 890.35
 ---- batch: 090 ----
mean loss: 880.73
train mean loss: 876.96
epoch train time: 0:00:00.500458
elapsed time: 0:00:43.531467
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:23:15.046163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.01
 ---- batch: 020 ----
mean loss: 889.00
 ---- batch: 030 ----
mean loss: 869.05
 ---- batch: 040 ----
mean loss: 857.54
 ---- batch: 050 ----
mean loss: 868.99
 ---- batch: 060 ----
mean loss: 892.06
 ---- batch: 070 ----
mean loss: 882.73
 ---- batch: 080 ----
mean loss: 872.21
 ---- batch: 090 ----
mean loss: 877.31
train mean loss: 876.87
epoch train time: 0:00:00.498633
elapsed time: 0:00:44.030243
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:23:15.544950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.43
 ---- batch: 020 ----
mean loss: 871.69
 ---- batch: 030 ----
mean loss: 862.14
 ---- batch: 040 ----
mean loss: 868.52
 ---- batch: 050 ----
mean loss: 879.06
 ---- batch: 060 ----
mean loss: 880.11
 ---- batch: 070 ----
mean loss: 876.29
 ---- batch: 080 ----
mean loss: 878.97
 ---- batch: 090 ----
mean loss: 885.02
train mean loss: 875.59
epoch train time: 0:00:00.496021
elapsed time: 0:00:44.526420
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:23:16.041117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.75
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 877.41
 ---- batch: 040 ----
mean loss: 858.82
 ---- batch: 050 ----
mean loss: 873.14
 ---- batch: 060 ----
mean loss: 866.36
 ---- batch: 070 ----
mean loss: 881.73
 ---- batch: 080 ----
mean loss: 881.42
 ---- batch: 090 ----
mean loss: 873.35
train mean loss: 876.50
epoch train time: 0:00:00.494338
elapsed time: 0:00:45.020913
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:23:16.535645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.01
 ---- batch: 020 ----
mean loss: 889.31
 ---- batch: 030 ----
mean loss: 883.26
 ---- batch: 040 ----
mean loss: 876.03
 ---- batch: 050 ----
mean loss: 881.09
 ---- batch: 060 ----
mean loss: 877.86
 ---- batch: 070 ----
mean loss: 873.92
 ---- batch: 080 ----
mean loss: 866.05
 ---- batch: 090 ----
mean loss: 883.83
train mean loss: 875.33
epoch train time: 0:00:00.495597
elapsed time: 0:00:45.516694
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:23:17.031410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.50
 ---- batch: 020 ----
mean loss: 869.82
 ---- batch: 030 ----
mean loss: 863.56
 ---- batch: 040 ----
mean loss: 866.55
 ---- batch: 050 ----
mean loss: 864.83
 ---- batch: 060 ----
mean loss: 901.92
 ---- batch: 070 ----
mean loss: 883.65
 ---- batch: 080 ----
mean loss: 878.31
 ---- batch: 090 ----
mean loss: 869.32
train mean loss: 874.96
epoch train time: 0:00:00.502818
elapsed time: 0:00:46.019680
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:23:17.534376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.23
 ---- batch: 020 ----
mean loss: 873.58
 ---- batch: 030 ----
mean loss: 871.86
 ---- batch: 040 ----
mean loss: 872.96
 ---- batch: 050 ----
mean loss: 864.77
 ---- batch: 060 ----
mean loss: 869.93
 ---- batch: 070 ----
mean loss: 879.67
 ---- batch: 080 ----
mean loss: 887.22
 ---- batch: 090 ----
mean loss: 887.97
train mean loss: 875.14
epoch train time: 0:00:00.507490
elapsed time: 0:00:46.527325
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:23:18.042039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.90
 ---- batch: 020 ----
mean loss: 869.19
 ---- batch: 030 ----
mean loss: 878.22
 ---- batch: 040 ----
mean loss: 885.29
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.66
 ---- batch: 080 ----
mean loss: 892.05
 ---- batch: 090 ----
mean loss: 862.47
train mean loss: 874.91
epoch train time: 0:00:00.495004
elapsed time: 0:00:47.022492
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:23:18.537191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.85
 ---- batch: 020 ----
mean loss: 874.99
 ---- batch: 030 ----
mean loss: 861.95
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.67
 ---- batch: 060 ----
mean loss: 883.31
 ---- batch: 070 ----
mean loss: 889.93
 ---- batch: 080 ----
mean loss: 863.92
 ---- batch: 090 ----
mean loss: 859.41
train mean loss: 875.84
epoch train time: 0:00:00.497506
elapsed time: 0:00:47.520148
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:23:19.034860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.40
 ---- batch: 030 ----
mean loss: 870.59
 ---- batch: 040 ----
mean loss: 876.62
 ---- batch: 050 ----
mean loss: 875.44
 ---- batch: 060 ----
mean loss: 878.09
 ---- batch: 070 ----
mean loss: 872.71
 ---- batch: 080 ----
mean loss: 870.19
 ---- batch: 090 ----
mean loss: 857.34
train mean loss: 874.97
epoch train time: 0:00:00.488570
elapsed time: 0:00:48.008879
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:23:19.523577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.87
 ---- batch: 020 ----
mean loss: 871.05
 ---- batch: 030 ----
mean loss: 868.38
 ---- batch: 040 ----
mean loss: 878.45
 ---- batch: 050 ----
mean loss: 892.62
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.12
train mean loss: 875.07
epoch train time: 0:00:00.505161
elapsed time: 0:00:48.514189
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:23:20.028886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.12
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.03
 ---- batch: 050 ----
mean loss: 876.38
 ---- batch: 060 ----
mean loss: 886.66
 ---- batch: 070 ----
mean loss: 861.70
 ---- batch: 080 ----
mean loss: 882.74
 ---- batch: 090 ----
mean loss: 870.73
train mean loss: 874.91
epoch train time: 0:00:00.503164
elapsed time: 0:00:49.017508
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:23:20.532201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.45
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.21
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.77
train mean loss: 876.11
epoch train time: 0:00:00.500365
elapsed time: 0:00:49.518025
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:23:21.032752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.87
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.28
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.77
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.498852
elapsed time: 0:00:50.017079
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:23:21.531770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.82
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.58
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.57
 ---- batch: 070 ----
mean loss: 878.82
 ---- batch: 080 ----
mean loss: 875.93
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.70
epoch train time: 0:00:00.497602
elapsed time: 0:00:50.514830
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:23:22.029531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.44
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.42
 ---- batch: 070 ----
mean loss: 861.52
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.20
train mean loss: 875.33
epoch train time: 0:00:00.497450
elapsed time: 0:00:51.012460
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:23:22.527159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.85
 ---- batch: 040 ----
mean loss: 896.13
 ---- batch: 050 ----
mean loss: 895.65
 ---- batch: 060 ----
mean loss: 872.60
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.88
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.509267
elapsed time: 0:00:51.521877
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:23:23.036576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.28
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 881.99
 ---- batch: 090 ----
mean loss: 857.68
train mean loss: 875.42
epoch train time: 0:00:00.498163
elapsed time: 0:00:52.020234
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:23:23.534954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.12
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.70
train mean loss: 875.94
epoch train time: 0:00:00.503791
elapsed time: 0:00:52.524198
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:23:24.038896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.24
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.78
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.57
train mean loss: 875.20
epoch train time: 0:00:00.491421
elapsed time: 0:00:53.015770
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:23:24.530492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.89
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.50
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.94
 ---- batch: 090 ----
mean loss: 867.29
train mean loss: 874.55
epoch train time: 0:00:00.496434
elapsed time: 0:00:53.512387
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:23:25.027084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.83
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.496667
elapsed time: 0:00:54.009214
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:23:25.523911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.13
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.49
 ---- batch: 060 ----
mean loss: 873.51
 ---- batch: 070 ----
mean loss: 889.66
 ---- batch: 080 ----
mean loss: 893.93
 ---- batch: 090 ----
mean loss: 900.17
train mean loss: 875.66
epoch train time: 0:00:00.508826
elapsed time: 0:00:54.518186
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:23:26.032884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.41
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.90
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.10
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.23
epoch train time: 0:00:00.494959
elapsed time: 0:00:55.013292
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:23:26.527992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.91
 ---- batch: 020 ----
mean loss: 896.01
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.44
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.65
epoch train time: 0:00:00.499922
elapsed time: 0:00:55.513380
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:23:27.028069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.23
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.62
 ---- batch: 080 ----
mean loss: 876.95
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.491212
elapsed time: 0:00:56.004742
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:23:27.519449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.46
 ---- batch: 070 ----
mean loss: 894.00
 ---- batch: 080 ----
mean loss: 865.67
 ---- batch: 090 ----
mean loss: 871.93
train mean loss: 875.10
epoch train time: 0:00:00.497221
elapsed time: 0:00:56.502121
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:23:28.016818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.70
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.70
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.65
 ---- batch: 060 ----
mean loss: 876.81
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.45
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.85
epoch train time: 0:00:00.494470
elapsed time: 0:00:56.996752
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:23:28.511495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.54
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.90
 ---- batch: 050 ----
mean loss: 879.94
 ---- batch: 060 ----
mean loss: 870.33
 ---- batch: 070 ----
mean loss: 893.30
 ---- batch: 080 ----
mean loss: 874.30
 ---- batch: 090 ----
mean loss: 901.59
train mean loss: 874.94
epoch train time: 0:00:00.499887
elapsed time: 0:00:57.496838
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:23:29.011538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.79
 ---- batch: 020 ----
mean loss: 885.95
 ---- batch: 030 ----
mean loss: 869.68
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 866.17
 ---- batch: 060 ----
mean loss: 869.62
 ---- batch: 070 ----
mean loss: 865.49
 ---- batch: 080 ----
mean loss: 885.12
 ---- batch: 090 ----
mean loss: 878.95
train mean loss: 874.96
epoch train time: 0:00:00.492697
elapsed time: 0:00:57.989700
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:23:29.504395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.61
 ---- batch: 020 ----
mean loss: 893.23
 ---- batch: 030 ----
mean loss: 872.98
 ---- batch: 040 ----
mean loss: 888.76
 ---- batch: 050 ----
mean loss: 867.77
 ---- batch: 060 ----
mean loss: 869.08
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 858.03
 ---- batch: 090 ----
mean loss: 875.46
train mean loss: 875.15
epoch train time: 0:00:00.501017
elapsed time: 0:00:58.490878
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:23:30.005577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.42
 ---- batch: 020 ----
mean loss: 873.35
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 862.72
 ---- batch: 050 ----
mean loss: 871.30
 ---- batch: 060 ----
mean loss: 875.63
 ---- batch: 070 ----
mean loss: 898.82
 ---- batch: 080 ----
mean loss: 874.46
 ---- batch: 090 ----
mean loss: 887.67
train mean loss: 875.69
epoch train time: 0:00:00.496748
elapsed time: 0:00:58.987784
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:23:30.502483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.20
 ---- batch: 020 ----
mean loss: 875.67
 ---- batch: 030 ----
mean loss: 868.29
 ---- batch: 040 ----
mean loss: 876.40
 ---- batch: 050 ----
mean loss: 869.82
 ---- batch: 060 ----
mean loss: 874.43
 ---- batch: 070 ----
mean loss: 877.11
 ---- batch: 080 ----
mean loss: 890.28
 ---- batch: 090 ----
mean loss: 870.38
train mean loss: 875.10
epoch train time: 0:00:00.497909
elapsed time: 0:00:59.485844
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:23:31.000544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.39
 ---- batch: 020 ----
mean loss: 863.20
 ---- batch: 030 ----
mean loss: 884.34
 ---- batch: 040 ----
mean loss: 878.62
 ---- batch: 050 ----
mean loss: 875.71
 ---- batch: 060 ----
mean loss: 870.58
 ---- batch: 070 ----
mean loss: 875.20
 ---- batch: 080 ----
mean loss: 873.82
 ---- batch: 090 ----
mean loss: 883.90
train mean loss: 874.97
epoch train time: 0:00:00.502591
elapsed time: 0:00:59.988623
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:23:31.503316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.10
 ---- batch: 020 ----
mean loss: 853.59
 ---- batch: 030 ----
mean loss: 878.74
 ---- batch: 040 ----
mean loss: 879.11
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 858.45
 ---- batch: 070 ----
mean loss: 882.02
 ---- batch: 080 ----
mean loss: 887.78
 ---- batch: 090 ----
mean loss: 862.44
train mean loss: 875.73
epoch train time: 0:00:00.503790
elapsed time: 0:01:00.492590
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:23:32.007324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.91
 ---- batch: 020 ----
mean loss: 854.59
 ---- batch: 030 ----
mean loss: 865.08
 ---- batch: 040 ----
mean loss: 868.00
 ---- batch: 050 ----
mean loss: 891.59
 ---- batch: 060 ----
mean loss: 872.24
 ---- batch: 070 ----
mean loss: 913.58
 ---- batch: 080 ----
mean loss: 859.98
 ---- batch: 090 ----
mean loss: 881.53
train mean loss: 875.42
epoch train time: 0:00:00.502526
elapsed time: 0:01:00.995321
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:23:32.510019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.25
 ---- batch: 020 ----
mean loss: 870.54
 ---- batch: 030 ----
mean loss: 879.46
 ---- batch: 040 ----
mean loss: 867.01
 ---- batch: 050 ----
mean loss: 886.26
 ---- batch: 060 ----
mean loss: 875.60
 ---- batch: 070 ----
mean loss: 878.36
 ---- batch: 080 ----
mean loss: 871.59
 ---- batch: 090 ----
mean loss: 870.12
train mean loss: 875.56
epoch train time: 0:00:00.509791
elapsed time: 0:01:01.505263
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:23:33.019970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.72
 ---- batch: 020 ----
mean loss: 880.71
 ---- batch: 030 ----
mean loss: 873.43
 ---- batch: 040 ----
mean loss: 855.98
 ---- batch: 050 ----
mean loss: 836.06
 ---- batch: 060 ----
mean loss: 802.60
 ---- batch: 070 ----
mean loss: 767.17
 ---- batch: 080 ----
mean loss: 741.74
 ---- batch: 090 ----
mean loss: 695.98
train mean loss: 801.92
epoch train time: 0:00:00.502130
elapsed time: 0:01:02.007555
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:23:33.522254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 623.85
 ---- batch: 020 ----
mean loss: 564.10
 ---- batch: 030 ----
mean loss: 503.75
 ---- batch: 040 ----
mean loss: 457.44
 ---- batch: 050 ----
mean loss: 428.91
 ---- batch: 060 ----
mean loss: 410.53
 ---- batch: 070 ----
mean loss: 399.89
 ---- batch: 080 ----
mean loss: 384.98
 ---- batch: 090 ----
mean loss: 379.58
train mean loss: 455.89
epoch train time: 0:00:00.502492
elapsed time: 0:01:02.510237
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:23:34.024947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.28
 ---- batch: 020 ----
mean loss: 359.24
 ---- batch: 030 ----
mean loss: 344.66
 ---- batch: 040 ----
mean loss: 333.40
 ---- batch: 050 ----
mean loss: 328.67
 ---- batch: 060 ----
mean loss: 326.45
 ---- batch: 070 ----
mean loss: 315.04
 ---- batch: 080 ----
mean loss: 305.20
 ---- batch: 090 ----
mean loss: 306.46
train mean loss: 329.84
epoch train time: 0:00:00.494698
elapsed time: 0:01:03.005094
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:23:34.519792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.61
 ---- batch: 020 ----
mean loss: 291.17
 ---- batch: 030 ----
mean loss: 284.27
 ---- batch: 040 ----
mean loss: 286.81
 ---- batch: 050 ----
mean loss: 278.06
 ---- batch: 060 ----
mean loss: 274.42
 ---- batch: 070 ----
mean loss: 282.70
 ---- batch: 080 ----
mean loss: 281.41
 ---- batch: 090 ----
mean loss: 278.33
train mean loss: 282.64
epoch train time: 0:00:00.506481
elapsed time: 0:01:03.511728
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:23:35.026428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.51
 ---- batch: 020 ----
mean loss: 260.18
 ---- batch: 030 ----
mean loss: 257.31
 ---- batch: 040 ----
mean loss: 253.79
 ---- batch: 050 ----
mean loss: 259.20
 ---- batch: 060 ----
mean loss: 266.88
 ---- batch: 070 ----
mean loss: 257.28
 ---- batch: 080 ----
mean loss: 253.04
 ---- batch: 090 ----
mean loss: 267.46
train mean loss: 260.28
epoch train time: 0:00:00.503980
elapsed time: 0:01:04.015860
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:23:35.530558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.79
 ---- batch: 020 ----
mean loss: 243.29
 ---- batch: 030 ----
mean loss: 254.35
 ---- batch: 040 ----
mean loss: 241.71
 ---- batch: 050 ----
mean loss: 248.50
 ---- batch: 060 ----
mean loss: 249.91
 ---- batch: 070 ----
mean loss: 248.42
 ---- batch: 080 ----
mean loss: 248.53
 ---- batch: 090 ----
mean loss: 244.05
train mean loss: 247.79
epoch train time: 0:00:00.498944
elapsed time: 0:01:04.514950
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:23:36.029687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.22
 ---- batch: 020 ----
mean loss: 248.30
 ---- batch: 030 ----
mean loss: 239.92
 ---- batch: 040 ----
mean loss: 239.38
 ---- batch: 050 ----
mean loss: 252.36
 ---- batch: 060 ----
mean loss: 235.12
 ---- batch: 070 ----
mean loss: 233.24
 ---- batch: 080 ----
mean loss: 236.26
 ---- batch: 090 ----
mean loss: 231.51
train mean loss: 239.38
epoch train time: 0:00:00.492106
elapsed time: 0:01:05.007258
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:23:36.521958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.53
 ---- batch: 020 ----
mean loss: 234.29
 ---- batch: 030 ----
mean loss: 240.90
 ---- batch: 040 ----
mean loss: 239.77
 ---- batch: 050 ----
mean loss: 239.35
 ---- batch: 060 ----
mean loss: 235.31
 ---- batch: 070 ----
mean loss: 225.04
 ---- batch: 080 ----
mean loss: 227.79
 ---- batch: 090 ----
mean loss: 230.81
train mean loss: 234.49
epoch train time: 0:00:00.501007
elapsed time: 0:01:05.508435
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:23:37.023132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.54
 ---- batch: 020 ----
mean loss: 226.13
 ---- batch: 030 ----
mean loss: 232.16
 ---- batch: 040 ----
mean loss: 223.83
 ---- batch: 050 ----
mean loss: 231.97
 ---- batch: 060 ----
mean loss: 232.04
 ---- batch: 070 ----
mean loss: 228.82
 ---- batch: 080 ----
mean loss: 235.44
 ---- batch: 090 ----
mean loss: 226.16
train mean loss: 228.98
epoch train time: 0:00:00.497485
elapsed time: 0:01:06.006107
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:23:37.520797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.29
 ---- batch: 020 ----
mean loss: 218.01
 ---- batch: 030 ----
mean loss: 240.15
 ---- batch: 040 ----
mean loss: 235.89
 ---- batch: 050 ----
mean loss: 230.54
 ---- batch: 060 ----
mean loss: 234.52
 ---- batch: 070 ----
mean loss: 221.25
 ---- batch: 080 ----
mean loss: 221.14
 ---- batch: 090 ----
mean loss: 215.93
train mean loss: 226.28
epoch train time: 0:00:00.502288
elapsed time: 0:01:06.508537
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:23:38.023235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.73
 ---- batch: 020 ----
mean loss: 225.24
 ---- batch: 030 ----
mean loss: 218.83
 ---- batch: 040 ----
mean loss: 215.40
 ---- batch: 050 ----
mean loss: 216.37
 ---- batch: 060 ----
mean loss: 219.37
 ---- batch: 070 ----
mean loss: 223.28
 ---- batch: 080 ----
mean loss: 222.77
 ---- batch: 090 ----
mean loss: 216.85
train mean loss: 220.76
epoch train time: 0:00:00.501022
elapsed time: 0:01:07.009717
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:23:38.524415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.31
 ---- batch: 020 ----
mean loss: 224.52
 ---- batch: 030 ----
mean loss: 207.79
 ---- batch: 040 ----
mean loss: 214.01
 ---- batch: 050 ----
mean loss: 211.09
 ---- batch: 060 ----
mean loss: 219.76
 ---- batch: 070 ----
mean loss: 226.92
 ---- batch: 080 ----
mean loss: 216.63
 ---- batch: 090 ----
mean loss: 222.75
train mean loss: 218.37
epoch train time: 0:00:00.499849
elapsed time: 0:01:07.509731
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:23:39.024429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.46
 ---- batch: 020 ----
mean loss: 210.96
 ---- batch: 030 ----
mean loss: 212.13
 ---- batch: 040 ----
mean loss: 212.78
 ---- batch: 050 ----
mean loss: 216.47
 ---- batch: 060 ----
mean loss: 209.33
 ---- batch: 070 ----
mean loss: 206.23
 ---- batch: 080 ----
mean loss: 221.16
 ---- batch: 090 ----
mean loss: 223.74
train mean loss: 215.85
epoch train time: 0:00:00.489566
elapsed time: 0:01:07.999502
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:23:39.514208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.90
 ---- batch: 020 ----
mean loss: 207.88
 ---- batch: 030 ----
mean loss: 213.12
 ---- batch: 040 ----
mean loss: 218.87
 ---- batch: 050 ----
mean loss: 215.90
 ---- batch: 060 ----
mean loss: 208.05
 ---- batch: 070 ----
mean loss: 212.76
 ---- batch: 080 ----
mean loss: 209.15
 ---- batch: 090 ----
mean loss: 212.74
train mean loss: 211.74
epoch train time: 0:00:00.498488
elapsed time: 0:01:08.498145
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:23:40.012853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.44
 ---- batch: 020 ----
mean loss: 206.96
 ---- batch: 030 ----
mean loss: 210.99
 ---- batch: 040 ----
mean loss: 213.74
 ---- batch: 050 ----
mean loss: 214.05
 ---- batch: 060 ----
mean loss: 213.53
 ---- batch: 070 ----
mean loss: 206.62
 ---- batch: 080 ----
mean loss: 213.03
 ---- batch: 090 ----
mean loss: 211.13
train mean loss: 210.91
epoch train time: 0:00:00.497485
elapsed time: 0:01:08.995787
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:23:40.510485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.43
 ---- batch: 020 ----
mean loss: 194.29
 ---- batch: 030 ----
mean loss: 210.28
 ---- batch: 040 ----
mean loss: 204.42
 ---- batch: 050 ----
mean loss: 216.11
 ---- batch: 060 ----
mean loss: 210.00
 ---- batch: 070 ----
mean loss: 210.24
 ---- batch: 080 ----
mean loss: 215.98
 ---- batch: 090 ----
mean loss: 209.40
train mean loss: 207.16
epoch train time: 0:00:00.492026
elapsed time: 0:01:09.487966
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:23:41.002665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.74
 ---- batch: 020 ----
mean loss: 203.64
 ---- batch: 030 ----
mean loss: 214.43
 ---- batch: 040 ----
mean loss: 202.01
 ---- batch: 050 ----
mean loss: 203.21
 ---- batch: 060 ----
mean loss: 210.10
 ---- batch: 070 ----
mean loss: 202.78
 ---- batch: 080 ----
mean loss: 209.63
 ---- batch: 090 ----
mean loss: 205.85
train mean loss: 205.89
epoch train time: 0:00:00.492103
elapsed time: 0:01:09.980217
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:23:41.494915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.64
 ---- batch: 020 ----
mean loss: 202.05
 ---- batch: 030 ----
mean loss: 200.16
 ---- batch: 040 ----
mean loss: 207.24
 ---- batch: 050 ----
mean loss: 203.35
 ---- batch: 060 ----
mean loss: 206.48
 ---- batch: 070 ----
mean loss: 201.46
 ---- batch: 080 ----
mean loss: 199.56
 ---- batch: 090 ----
mean loss: 207.09
train mean loss: 203.62
epoch train time: 0:00:00.499507
elapsed time: 0:01:10.479876
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:23:41.994576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.53
 ---- batch: 020 ----
mean loss: 201.50
 ---- batch: 030 ----
mean loss: 202.20
 ---- batch: 040 ----
mean loss: 207.17
 ---- batch: 050 ----
mean loss: 196.41
 ---- batch: 060 ----
mean loss: 197.28
 ---- batch: 070 ----
mean loss: 207.49
 ---- batch: 080 ----
mean loss: 203.20
 ---- batch: 090 ----
mean loss: 202.53
train mean loss: 201.51
epoch train time: 0:00:00.492518
elapsed time: 0:01:10.972540
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:23:42.487242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.99
 ---- batch: 020 ----
mean loss: 200.26
 ---- batch: 030 ----
mean loss: 187.22
 ---- batch: 040 ----
mean loss: 196.65
 ---- batch: 050 ----
mean loss: 202.61
 ---- batch: 060 ----
mean loss: 197.88
 ---- batch: 070 ----
mean loss: 202.99
 ---- batch: 080 ----
mean loss: 206.83
 ---- batch: 090 ----
mean loss: 200.78
train mean loss: 199.31
epoch train time: 0:00:00.499361
elapsed time: 0:01:11.472063
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:23:42.986759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.40
 ---- batch: 020 ----
mean loss: 194.80
 ---- batch: 030 ----
mean loss: 195.55
 ---- batch: 040 ----
mean loss: 203.73
 ---- batch: 050 ----
mean loss: 187.49
 ---- batch: 060 ----
mean loss: 196.84
 ---- batch: 070 ----
mean loss: 199.44
 ---- batch: 080 ----
mean loss: 199.28
 ---- batch: 090 ----
mean loss: 198.55
train mean loss: 196.24
epoch train time: 0:00:00.494061
elapsed time: 0:01:11.966271
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:23:43.481014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.60
 ---- batch: 020 ----
mean loss: 192.69
 ---- batch: 030 ----
mean loss: 191.84
 ---- batch: 040 ----
mean loss: 205.23
 ---- batch: 050 ----
mean loss: 193.79
 ---- batch: 060 ----
mean loss: 190.44
 ---- batch: 070 ----
mean loss: 204.85
 ---- batch: 080 ----
mean loss: 195.24
 ---- batch: 090 ----
mean loss: 197.92
train mean loss: 194.90
epoch train time: 0:00:00.503988
elapsed time: 0:01:12.470449
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:23:43.985146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.65
 ---- batch: 020 ----
mean loss: 188.45
 ---- batch: 030 ----
mean loss: 189.72
 ---- batch: 040 ----
mean loss: 205.80
 ---- batch: 050 ----
mean loss: 195.79
 ---- batch: 060 ----
mean loss: 200.13
 ---- batch: 070 ----
mean loss: 188.02
 ---- batch: 080 ----
mean loss: 196.14
 ---- batch: 090 ----
mean loss: 194.53
train mean loss: 193.43
epoch train time: 0:00:00.495698
elapsed time: 0:01:12.966316
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:23:44.481030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.57
 ---- batch: 020 ----
mean loss: 184.08
 ---- batch: 030 ----
mean loss: 184.70
 ---- batch: 040 ----
mean loss: 189.52
 ---- batch: 050 ----
mean loss: 192.59
 ---- batch: 060 ----
mean loss: 188.81
 ---- batch: 070 ----
mean loss: 193.00
 ---- batch: 080 ----
mean loss: 195.69
 ---- batch: 090 ----
mean loss: 197.80
train mean loss: 191.27
epoch train time: 0:00:00.507459
elapsed time: 0:01:13.473938
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:23:44.988635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.13
 ---- batch: 020 ----
mean loss: 189.04
 ---- batch: 030 ----
mean loss: 197.70
 ---- batch: 040 ----
mean loss: 187.05
 ---- batch: 050 ----
mean loss: 191.89
 ---- batch: 060 ----
mean loss: 187.80
 ---- batch: 070 ----
mean loss: 186.68
 ---- batch: 080 ----
mean loss: 188.80
 ---- batch: 090 ----
mean loss: 187.71
train mean loss: 189.23
epoch train time: 0:00:00.494057
elapsed time: 0:01:13.968142
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:23:45.482871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.52
 ---- batch: 020 ----
mean loss: 186.18
 ---- batch: 030 ----
mean loss: 180.71
 ---- batch: 040 ----
mean loss: 188.35
 ---- batch: 050 ----
mean loss: 196.36
 ---- batch: 060 ----
mean loss: 189.72
 ---- batch: 070 ----
mean loss: 185.97
 ---- batch: 080 ----
mean loss: 197.36
 ---- batch: 090 ----
mean loss: 189.21
train mean loss: 188.93
epoch train time: 0:00:00.496809
elapsed time: 0:01:14.465130
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:23:45.979829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.31
 ---- batch: 020 ----
mean loss: 178.58
 ---- batch: 030 ----
mean loss: 188.26
 ---- batch: 040 ----
mean loss: 181.19
 ---- batch: 050 ----
mean loss: 191.49
 ---- batch: 060 ----
mean loss: 194.12
 ---- batch: 070 ----
mean loss: 186.48
 ---- batch: 080 ----
mean loss: 190.84
 ---- batch: 090 ----
mean loss: 190.84
train mean loss: 187.72
epoch train time: 0:00:00.498395
elapsed time: 0:01:14.963675
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:23:46.478373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.64
 ---- batch: 020 ----
mean loss: 181.38
 ---- batch: 030 ----
mean loss: 185.16
 ---- batch: 040 ----
mean loss: 182.61
 ---- batch: 050 ----
mean loss: 189.34
 ---- batch: 060 ----
mean loss: 192.03
 ---- batch: 070 ----
mean loss: 182.05
 ---- batch: 080 ----
mean loss: 186.99
 ---- batch: 090 ----
mean loss: 197.73
train mean loss: 187.24
epoch train time: 0:00:00.505897
elapsed time: 0:01:15.469726
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:23:46.984415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.98
 ---- batch: 020 ----
mean loss: 191.38
 ---- batch: 030 ----
mean loss: 180.16
 ---- batch: 040 ----
mean loss: 187.04
 ---- batch: 050 ----
mean loss: 183.14
 ---- batch: 060 ----
mean loss: 180.89
 ---- batch: 070 ----
mean loss: 178.27
 ---- batch: 080 ----
mean loss: 185.88
 ---- batch: 090 ----
mean loss: 195.39
train mean loss: 186.04
epoch train time: 0:00:00.490267
elapsed time: 0:01:15.960137
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:23:47.474883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.15
 ---- batch: 020 ----
mean loss: 182.37
 ---- batch: 030 ----
mean loss: 186.34
 ---- batch: 040 ----
mean loss: 186.67
 ---- batch: 050 ----
mean loss: 182.82
 ---- batch: 060 ----
mean loss: 185.62
 ---- batch: 070 ----
mean loss: 181.50
 ---- batch: 080 ----
mean loss: 186.04
 ---- batch: 090 ----
mean loss: 172.53
train mean loss: 182.69
epoch train time: 0:00:00.493660
elapsed time: 0:01:16.453998
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:23:47.968697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.39
 ---- batch: 020 ----
mean loss: 180.86
 ---- batch: 030 ----
mean loss: 181.84
 ---- batch: 040 ----
mean loss: 182.91
 ---- batch: 050 ----
mean loss: 169.54
 ---- batch: 060 ----
mean loss: 181.93
 ---- batch: 070 ----
mean loss: 185.87
 ---- batch: 080 ----
mean loss: 185.08
 ---- batch: 090 ----
mean loss: 190.24
train mean loss: 182.26
epoch train time: 0:00:00.509907
elapsed time: 0:01:16.964053
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:23:48.478759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.03
 ---- batch: 020 ----
mean loss: 183.08
 ---- batch: 030 ----
mean loss: 179.75
 ---- batch: 040 ----
mean loss: 174.78
 ---- batch: 050 ----
mean loss: 176.38
 ---- batch: 060 ----
mean loss: 188.72
 ---- batch: 070 ----
mean loss: 182.14
 ---- batch: 080 ----
mean loss: 183.09
 ---- batch: 090 ----
mean loss: 178.86
train mean loss: 181.91
epoch train time: 0:00:00.505308
elapsed time: 0:01:17.469522
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:23:48.984221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.74
 ---- batch: 020 ----
mean loss: 183.14
 ---- batch: 030 ----
mean loss: 186.66
 ---- batch: 040 ----
mean loss: 185.33
 ---- batch: 050 ----
mean loss: 182.84
 ---- batch: 060 ----
mean loss: 183.88
 ---- batch: 070 ----
mean loss: 185.71
 ---- batch: 080 ----
mean loss: 182.55
 ---- batch: 090 ----
mean loss: 177.21
train mean loss: 182.14
epoch train time: 0:00:00.497568
elapsed time: 0:01:17.967269
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:23:49.481963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.62
 ---- batch: 020 ----
mean loss: 176.11
 ---- batch: 030 ----
mean loss: 169.29
 ---- batch: 040 ----
mean loss: 175.83
 ---- batch: 050 ----
mean loss: 185.90
 ---- batch: 060 ----
mean loss: 188.76
 ---- batch: 070 ----
mean loss: 176.46
 ---- batch: 080 ----
mean loss: 184.19
 ---- batch: 090 ----
mean loss: 177.56
train mean loss: 178.93
epoch train time: 0:00:00.497425
elapsed time: 0:01:18.464839
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:23:49.979537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.03
 ---- batch: 020 ----
mean loss: 182.67
 ---- batch: 030 ----
mean loss: 170.93
 ---- batch: 040 ----
mean loss: 172.14
 ---- batch: 050 ----
mean loss: 182.40
 ---- batch: 060 ----
mean loss: 181.52
 ---- batch: 070 ----
mean loss: 177.81
 ---- batch: 080 ----
mean loss: 183.23
 ---- batch: 090 ----
mean loss: 178.71
train mean loss: 178.55
epoch train time: 0:00:00.492351
elapsed time: 0:01:18.957346
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:23:50.472046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.76
 ---- batch: 020 ----
mean loss: 180.62
 ---- batch: 030 ----
mean loss: 177.14
 ---- batch: 040 ----
mean loss: 175.58
 ---- batch: 050 ----
mean loss: 168.38
 ---- batch: 060 ----
mean loss: 181.33
 ---- batch: 070 ----
mean loss: 175.06
 ---- batch: 080 ----
mean loss: 185.01
 ---- batch: 090 ----
mean loss: 174.89
train mean loss: 177.49
epoch train time: 0:00:00.508196
elapsed time: 0:01:19.465708
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:23:50.980446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.28
 ---- batch: 020 ----
mean loss: 179.83
 ---- batch: 030 ----
mean loss: 175.16
 ---- batch: 040 ----
mean loss: 178.90
 ---- batch: 050 ----
mean loss: 177.97
 ---- batch: 060 ----
mean loss: 185.23
 ---- batch: 070 ----
mean loss: 180.32
 ---- batch: 080 ----
mean loss: 175.76
 ---- batch: 090 ----
mean loss: 167.68
train mean loss: 175.82
epoch train time: 0:00:00.495891
elapsed time: 0:01:19.961786
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:23:51.476483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.21
 ---- batch: 020 ----
mean loss: 169.15
 ---- batch: 030 ----
mean loss: 178.26
 ---- batch: 040 ----
mean loss: 168.56
 ---- batch: 050 ----
mean loss: 171.10
 ---- batch: 060 ----
mean loss: 183.03
 ---- batch: 070 ----
mean loss: 174.28
 ---- batch: 080 ----
mean loss: 177.91
 ---- batch: 090 ----
mean loss: 182.77
train mean loss: 175.48
epoch train time: 0:00:00.501693
elapsed time: 0:01:20.463644
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:23:51.978342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.35
 ---- batch: 020 ----
mean loss: 172.81
 ---- batch: 030 ----
mean loss: 170.98
 ---- batch: 040 ----
mean loss: 171.02
 ---- batch: 050 ----
mean loss: 180.32
 ---- batch: 060 ----
mean loss: 179.61
 ---- batch: 070 ----
mean loss: 174.60
 ---- batch: 080 ----
mean loss: 177.47
 ---- batch: 090 ----
mean loss: 179.43
train mean loss: 175.80
epoch train time: 0:00:00.495128
elapsed time: 0:01:20.958920
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:23:52.473641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.38
 ---- batch: 020 ----
mean loss: 163.94
 ---- batch: 030 ----
mean loss: 175.73
 ---- batch: 040 ----
mean loss: 173.56
 ---- batch: 050 ----
mean loss: 175.52
 ---- batch: 060 ----
mean loss: 179.46
 ---- batch: 070 ----
mean loss: 182.80
 ---- batch: 080 ----
mean loss: 175.31
 ---- batch: 090 ----
mean loss: 172.99
train mean loss: 174.56
epoch train time: 0:00:00.508734
elapsed time: 0:01:21.467825
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:23:52.982524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.61
 ---- batch: 020 ----
mean loss: 172.01
 ---- batch: 030 ----
mean loss: 171.38
 ---- batch: 040 ----
mean loss: 175.94
 ---- batch: 050 ----
mean loss: 169.73
 ---- batch: 060 ----
mean loss: 169.72
 ---- batch: 070 ----
mean loss: 174.17
 ---- batch: 080 ----
mean loss: 175.55
 ---- batch: 090 ----
mean loss: 180.87
train mean loss: 172.86
epoch train time: 0:00:00.491818
elapsed time: 0:01:21.959793
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:23:53.474490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.19
 ---- batch: 020 ----
mean loss: 171.18
 ---- batch: 030 ----
mean loss: 167.54
 ---- batch: 040 ----
mean loss: 170.62
 ---- batch: 050 ----
mean loss: 175.28
 ---- batch: 060 ----
mean loss: 174.69
 ---- batch: 070 ----
mean loss: 178.05
 ---- batch: 080 ----
mean loss: 171.22
 ---- batch: 090 ----
mean loss: 169.22
train mean loss: 172.22
epoch train time: 0:00:00.497364
elapsed time: 0:01:22.457326
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:23:53.972026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.72
 ---- batch: 020 ----
mean loss: 165.31
 ---- batch: 030 ----
mean loss: 167.02
 ---- batch: 040 ----
mean loss: 167.02
 ---- batch: 050 ----
mean loss: 178.70
 ---- batch: 060 ----
mean loss: 174.19
 ---- batch: 070 ----
mean loss: 171.32
 ---- batch: 080 ----
mean loss: 177.70
 ---- batch: 090 ----
mean loss: 173.93
train mean loss: 172.12
epoch train time: 0:00:00.499997
elapsed time: 0:01:22.957487
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:23:54.472185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.95
 ---- batch: 020 ----
mean loss: 166.07
 ---- batch: 030 ----
mean loss: 166.99
 ---- batch: 040 ----
mean loss: 170.48
 ---- batch: 050 ----
mean loss: 174.57
 ---- batch: 060 ----
mean loss: 165.24
 ---- batch: 070 ----
mean loss: 169.93
 ---- batch: 080 ----
mean loss: 170.24
 ---- batch: 090 ----
mean loss: 177.24
train mean loss: 170.61
epoch train time: 0:00:00.515279
elapsed time: 0:01:23.472918
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:23:54.987640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.85
 ---- batch: 020 ----
mean loss: 169.16
 ---- batch: 030 ----
mean loss: 169.66
 ---- batch: 040 ----
mean loss: 172.17
 ---- batch: 050 ----
mean loss: 175.33
 ---- batch: 060 ----
mean loss: 172.99
 ---- batch: 070 ----
mean loss: 170.85
 ---- batch: 080 ----
mean loss: 166.70
 ---- batch: 090 ----
mean loss: 168.30
train mean loss: 170.80
epoch train time: 0:00:00.506274
elapsed time: 0:01:23.979378
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:23:55.494090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.38
 ---- batch: 020 ----
mean loss: 166.58
 ---- batch: 030 ----
mean loss: 168.93
 ---- batch: 040 ----
mean loss: 167.84
 ---- batch: 050 ----
mean loss: 169.93
 ---- batch: 060 ----
mean loss: 174.05
 ---- batch: 070 ----
mean loss: 170.07
 ---- batch: 080 ----
mean loss: 166.73
 ---- batch: 090 ----
mean loss: 170.91
train mean loss: 169.86
epoch train time: 0:00:00.506175
elapsed time: 0:01:24.485746
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:23:56.000443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.64
 ---- batch: 020 ----
mean loss: 170.38
 ---- batch: 030 ----
mean loss: 161.27
 ---- batch: 040 ----
mean loss: 169.48
 ---- batch: 050 ----
mean loss: 167.23
 ---- batch: 060 ----
mean loss: 166.20
 ---- batch: 070 ----
mean loss: 175.66
 ---- batch: 080 ----
mean loss: 161.97
 ---- batch: 090 ----
mean loss: 169.14
train mean loss: 168.22
epoch train time: 0:00:00.509400
elapsed time: 0:01:24.995297
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:23:56.510012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.06
 ---- batch: 020 ----
mean loss: 174.30
 ---- batch: 030 ----
mean loss: 161.51
 ---- batch: 040 ----
mean loss: 170.35
 ---- batch: 050 ----
mean loss: 166.43
 ---- batch: 060 ----
mean loss: 169.24
 ---- batch: 070 ----
mean loss: 162.79
 ---- batch: 080 ----
mean loss: 162.42
 ---- batch: 090 ----
mean loss: 163.69
train mean loss: 167.41
epoch train time: 0:00:00.508981
elapsed time: 0:01:25.504442
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:23:57.019149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.54
 ---- batch: 020 ----
mean loss: 160.64
 ---- batch: 030 ----
mean loss: 174.44
 ---- batch: 040 ----
mean loss: 164.98
 ---- batch: 050 ----
mean loss: 160.44
 ---- batch: 060 ----
mean loss: 169.98
 ---- batch: 070 ----
mean loss: 170.09
 ---- batch: 080 ----
mean loss: 171.08
 ---- batch: 090 ----
mean loss: 168.54
train mean loss: 166.81
epoch train time: 0:00:00.490550
elapsed time: 0:01:25.995199
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:23:57.509903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.23
 ---- batch: 020 ----
mean loss: 169.53
 ---- batch: 030 ----
mean loss: 162.88
 ---- batch: 040 ----
mean loss: 162.40
 ---- batch: 050 ----
mean loss: 172.83
 ---- batch: 060 ----
mean loss: 165.95
 ---- batch: 070 ----
mean loss: 169.13
 ---- batch: 080 ----
mean loss: 166.07
 ---- batch: 090 ----
mean loss: 170.02
train mean loss: 167.06
epoch train time: 0:00:00.496392
elapsed time: 0:01:26.491777
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:23:58.006466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.13
 ---- batch: 020 ----
mean loss: 170.10
 ---- batch: 030 ----
mean loss: 163.03
 ---- batch: 040 ----
mean loss: 169.47
 ---- batch: 050 ----
mean loss: 165.20
 ---- batch: 060 ----
mean loss: 163.81
 ---- batch: 070 ----
mean loss: 171.37
 ---- batch: 080 ----
mean loss: 168.66
 ---- batch: 090 ----
mean loss: 164.56
train mean loss: 165.96
epoch train time: 0:00:00.497271
elapsed time: 0:01:26.989189
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:23:58.503904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.82
 ---- batch: 020 ----
mean loss: 169.67
 ---- batch: 030 ----
mean loss: 159.44
 ---- batch: 040 ----
mean loss: 162.63
 ---- batch: 050 ----
mean loss: 158.51
 ---- batch: 060 ----
mean loss: 167.38
 ---- batch: 070 ----
mean loss: 165.56
 ---- batch: 080 ----
mean loss: 168.60
 ---- batch: 090 ----
mean loss: 162.43
train mean loss: 165.13
epoch train time: 0:00:00.495473
elapsed time: 0:01:27.484824
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:23:58.999520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.58
 ---- batch: 020 ----
mean loss: 162.11
 ---- batch: 030 ----
mean loss: 165.83
 ---- batch: 040 ----
mean loss: 158.28
 ---- batch: 050 ----
mean loss: 161.46
 ---- batch: 060 ----
mean loss: 168.26
 ---- batch: 070 ----
mean loss: 168.61
 ---- batch: 080 ----
mean loss: 166.15
 ---- batch: 090 ----
mean loss: 167.27
train mean loss: 164.98
epoch train time: 0:00:00.490727
elapsed time: 0:01:27.975694
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:23:59.490390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.64
 ---- batch: 020 ----
mean loss: 158.56
 ---- batch: 030 ----
mean loss: 163.85
 ---- batch: 040 ----
mean loss: 166.66
 ---- batch: 050 ----
mean loss: 163.28
 ---- batch: 060 ----
mean loss: 172.99
 ---- batch: 070 ----
mean loss: 166.16
 ---- batch: 080 ----
mean loss: 164.70
 ---- batch: 090 ----
mean loss: 168.09
train mean loss: 165.68
epoch train time: 0:00:00.498991
elapsed time: 0:01:28.474837
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:23:59.989535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.74
 ---- batch: 020 ----
mean loss: 159.11
 ---- batch: 030 ----
mean loss: 170.16
 ---- batch: 040 ----
mean loss: 166.34
 ---- batch: 050 ----
mean loss: 162.91
 ---- batch: 060 ----
mean loss: 166.70
 ---- batch: 070 ----
mean loss: 164.04
 ---- batch: 080 ----
mean loss: 160.18
 ---- batch: 090 ----
mean loss: 164.58
train mean loss: 163.12
epoch train time: 0:00:00.495208
elapsed time: 0:01:28.970206
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:24:00.484903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.55
 ---- batch: 020 ----
mean loss: 168.32
 ---- batch: 030 ----
mean loss: 167.03
 ---- batch: 040 ----
mean loss: 154.20
 ---- batch: 050 ----
mean loss: 160.49
 ---- batch: 060 ----
mean loss: 165.35
 ---- batch: 070 ----
mean loss: 164.79
 ---- batch: 080 ----
mean loss: 163.55
 ---- batch: 090 ----
mean loss: 166.65
train mean loss: 163.65
epoch train time: 0:00:00.503327
elapsed time: 0:01:29.473673
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:24:00.988367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.07
 ---- batch: 020 ----
mean loss: 161.08
 ---- batch: 030 ----
mean loss: 160.20
 ---- batch: 040 ----
mean loss: 167.35
 ---- batch: 050 ----
mean loss: 165.73
 ---- batch: 060 ----
mean loss: 167.58
 ---- batch: 070 ----
mean loss: 156.69
 ---- batch: 080 ----
mean loss: 165.90
 ---- batch: 090 ----
mean loss: 166.57
train mean loss: 163.99
epoch train time: 0:00:00.497212
elapsed time: 0:01:29.971057
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:24:01.485781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.60
 ---- batch: 020 ----
mean loss: 162.42
 ---- batch: 030 ----
mean loss: 159.37
 ---- batch: 040 ----
mean loss: 160.48
 ---- batch: 050 ----
mean loss: 159.78
 ---- batch: 060 ----
mean loss: 168.14
 ---- batch: 070 ----
mean loss: 168.41
 ---- batch: 080 ----
mean loss: 153.73
 ---- batch: 090 ----
mean loss: 169.93
train mean loss: 163.00
epoch train time: 0:00:00.496567
elapsed time: 0:01:30.467802
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:24:01.982500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.69
 ---- batch: 020 ----
mean loss: 159.36
 ---- batch: 030 ----
mean loss: 158.69
 ---- batch: 040 ----
mean loss: 151.37
 ---- batch: 050 ----
mean loss: 168.65
 ---- batch: 060 ----
mean loss: 164.55
 ---- batch: 070 ----
mean loss: 161.46
 ---- batch: 080 ----
mean loss: 161.54
 ---- batch: 090 ----
mean loss: 167.81
train mean loss: 161.63
epoch train time: 0:00:00.490117
elapsed time: 0:01:30.958081
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:24:02.472796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.66
 ---- batch: 020 ----
mean loss: 151.07
 ---- batch: 030 ----
mean loss: 160.13
 ---- batch: 040 ----
mean loss: 161.46
 ---- batch: 050 ----
mean loss: 163.09
 ---- batch: 060 ----
mean loss: 168.16
 ---- batch: 070 ----
mean loss: 164.85
 ---- batch: 080 ----
mean loss: 168.00
 ---- batch: 090 ----
mean loss: 172.93
train mean loss: 161.71
epoch train time: 0:00:00.493693
elapsed time: 0:01:31.451947
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:24:02.966645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.48
 ---- batch: 020 ----
mean loss: 149.51
 ---- batch: 030 ----
mean loss: 161.13
 ---- batch: 040 ----
mean loss: 159.69
 ---- batch: 050 ----
mean loss: 161.08
 ---- batch: 060 ----
mean loss: 156.34
 ---- batch: 070 ----
mean loss: 160.83
 ---- batch: 080 ----
mean loss: 166.73
 ---- batch: 090 ----
mean loss: 174.67
train mean loss: 161.30
epoch train time: 0:00:00.495343
elapsed time: 0:01:31.947435
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:24:03.462130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.98
 ---- batch: 020 ----
mean loss: 156.60
 ---- batch: 030 ----
mean loss: 161.45
 ---- batch: 040 ----
mean loss: 161.43
 ---- batch: 050 ----
mean loss: 166.84
 ---- batch: 060 ----
mean loss: 158.33
 ---- batch: 070 ----
mean loss: 161.78
 ---- batch: 080 ----
mean loss: 161.77
 ---- batch: 090 ----
mean loss: 155.95
train mean loss: 160.31
epoch train time: 0:00:00.501029
elapsed time: 0:01:32.448607
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:24:03.963305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.71
 ---- batch: 020 ----
mean loss: 153.20
 ---- batch: 030 ----
mean loss: 156.05
 ---- batch: 040 ----
mean loss: 160.81
 ---- batch: 050 ----
mean loss: 163.89
 ---- batch: 060 ----
mean loss: 160.56
 ---- batch: 070 ----
mean loss: 161.10
 ---- batch: 080 ----
mean loss: 166.21
 ---- batch: 090 ----
mean loss: 158.34
train mean loss: 160.32
epoch train time: 0:00:00.498284
elapsed time: 0:01:32.947078
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:24:04.461776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.36
 ---- batch: 020 ----
mean loss: 161.96
 ---- batch: 030 ----
mean loss: 157.26
 ---- batch: 040 ----
mean loss: 158.56
 ---- batch: 050 ----
mean loss: 163.20
 ---- batch: 060 ----
mean loss: 158.47
 ---- batch: 070 ----
mean loss: 152.59
 ---- batch: 080 ----
mean loss: 165.06
 ---- batch: 090 ----
mean loss: 157.44
train mean loss: 158.78
epoch train time: 0:00:00.491173
elapsed time: 0:01:33.438403
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:24:04.953114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.18
 ---- batch: 020 ----
mean loss: 153.93
 ---- batch: 030 ----
mean loss: 156.55
 ---- batch: 040 ----
mean loss: 157.21
 ---- batch: 050 ----
mean loss: 161.51
 ---- batch: 060 ----
mean loss: 158.68
 ---- batch: 070 ----
mean loss: 161.65
 ---- batch: 080 ----
mean loss: 166.79
 ---- batch: 090 ----
mean loss: 157.74
train mean loss: 158.89
epoch train time: 0:00:00.497990
elapsed time: 0:01:33.936552
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:24:05.451257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.59
 ---- batch: 020 ----
mean loss: 158.63
 ---- batch: 030 ----
mean loss: 156.49
 ---- batch: 040 ----
mean loss: 158.92
 ---- batch: 050 ----
mean loss: 160.12
 ---- batch: 060 ----
mean loss: 160.34
 ---- batch: 070 ----
mean loss: 161.03
 ---- batch: 080 ----
mean loss: 155.00
 ---- batch: 090 ----
mean loss: 159.67
train mean loss: 158.61
epoch train time: 0:00:00.506041
elapsed time: 0:01:34.442746
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:24:05.957490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.03
 ---- batch: 020 ----
mean loss: 149.30
 ---- batch: 030 ----
mean loss: 151.60
 ---- batch: 040 ----
mean loss: 158.86
 ---- batch: 050 ----
mean loss: 160.43
 ---- batch: 060 ----
mean loss: 157.12
 ---- batch: 070 ----
mean loss: 160.36
 ---- batch: 080 ----
mean loss: 162.58
 ---- batch: 090 ----
mean loss: 161.63
train mean loss: 157.19
epoch train time: 0:00:00.495545
elapsed time: 0:01:34.938480
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:24:06.453174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.01
 ---- batch: 020 ----
mean loss: 155.71
 ---- batch: 030 ----
mean loss: 151.65
 ---- batch: 040 ----
mean loss: 160.07
 ---- batch: 050 ----
mean loss: 152.65
 ---- batch: 060 ----
mean loss: 164.81
 ---- batch: 070 ----
mean loss: 157.37
 ---- batch: 080 ----
mean loss: 156.49
 ---- batch: 090 ----
mean loss: 159.08
train mean loss: 157.32
epoch train time: 0:00:00.498495
elapsed time: 0:01:35.437119
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:24:06.951823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.85
 ---- batch: 020 ----
mean loss: 157.43
 ---- batch: 030 ----
mean loss: 150.62
 ---- batch: 040 ----
mean loss: 153.82
 ---- batch: 050 ----
mean loss: 158.03
 ---- batch: 060 ----
mean loss: 155.15
 ---- batch: 070 ----
mean loss: 159.66
 ---- batch: 080 ----
mean loss: 157.79
 ---- batch: 090 ----
mean loss: 166.91
train mean loss: 157.09
epoch train time: 0:00:00.493416
elapsed time: 0:01:35.930702
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:24:07.445402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.70
 ---- batch: 020 ----
mean loss: 154.92
 ---- batch: 030 ----
mean loss: 156.42
 ---- batch: 040 ----
mean loss: 159.40
 ---- batch: 050 ----
mean loss: 153.88
 ---- batch: 060 ----
mean loss: 160.53
 ---- batch: 070 ----
mean loss: 155.91
 ---- batch: 080 ----
mean loss: 161.38
 ---- batch: 090 ----
mean loss: 163.49
train mean loss: 157.03
epoch train time: 0:00:00.499807
elapsed time: 0:01:36.430677
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:24:07.945363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.19
 ---- batch: 020 ----
mean loss: 153.33
 ---- batch: 030 ----
mean loss: 152.45
 ---- batch: 040 ----
mean loss: 164.14
 ---- batch: 050 ----
mean loss: 161.67
 ---- batch: 060 ----
mean loss: 155.22
 ---- batch: 070 ----
mean loss: 152.57
 ---- batch: 080 ----
mean loss: 153.52
 ---- batch: 090 ----
mean loss: 166.65
train mean loss: 156.87
epoch train time: 0:00:00.501706
elapsed time: 0:01:36.932519
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:24:08.447219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.94
 ---- batch: 020 ----
mean loss: 147.97
 ---- batch: 030 ----
mean loss: 150.70
 ---- batch: 040 ----
mean loss: 153.79
 ---- batch: 050 ----
mean loss: 152.47
 ---- batch: 060 ----
mean loss: 153.95
 ---- batch: 070 ----
mean loss: 162.13
 ---- batch: 080 ----
mean loss: 163.80
 ---- batch: 090 ----
mean loss: 162.41
train mean loss: 155.40
epoch train time: 0:00:00.498520
elapsed time: 0:01:37.431191
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:24:08.945888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.48
 ---- batch: 020 ----
mean loss: 148.30
 ---- batch: 030 ----
mean loss: 152.09
 ---- batch: 040 ----
mean loss: 154.35
 ---- batch: 050 ----
mean loss: 155.72
 ---- batch: 060 ----
mean loss: 148.95
 ---- batch: 070 ----
mean loss: 155.78
 ---- batch: 080 ----
mean loss: 164.11
 ---- batch: 090 ----
mean loss: 160.31
train mean loss: 154.93
epoch train time: 0:00:00.494014
elapsed time: 0:01:37.925379
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:24:09.440078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.45
 ---- batch: 020 ----
mean loss: 153.55
 ---- batch: 030 ----
mean loss: 155.84
 ---- batch: 040 ----
mean loss: 152.50
 ---- batch: 050 ----
mean loss: 154.37
 ---- batch: 060 ----
mean loss: 154.33
 ---- batch: 070 ----
mean loss: 156.11
 ---- batch: 080 ----
mean loss: 155.54
 ---- batch: 090 ----
mean loss: 159.87
train mean loss: 154.44
epoch train time: 0:00:00.510228
elapsed time: 0:01:38.435762
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:24:09.950485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.76
 ---- batch: 020 ----
mean loss: 147.20
 ---- batch: 030 ----
mean loss: 157.24
 ---- batch: 040 ----
mean loss: 149.75
 ---- batch: 050 ----
mean loss: 154.87
 ---- batch: 060 ----
mean loss: 154.29
 ---- batch: 070 ----
mean loss: 160.99
 ---- batch: 080 ----
mean loss: 165.07
 ---- batch: 090 ----
mean loss: 159.14
train mean loss: 155.85
epoch train time: 0:00:00.500329
elapsed time: 0:01:38.936267
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:24:10.450964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.57
 ---- batch: 020 ----
mean loss: 154.63
 ---- batch: 030 ----
mean loss: 158.74
 ---- batch: 040 ----
mean loss: 148.53
 ---- batch: 050 ----
mean loss: 144.83
 ---- batch: 060 ----
mean loss: 157.99
 ---- batch: 070 ----
mean loss: 155.80
 ---- batch: 080 ----
mean loss: 158.77
 ---- batch: 090 ----
mean loss: 155.36
train mean loss: 155.25
epoch train time: 0:00:00.502726
elapsed time: 0:01:39.439159
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:24:10.953858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.34
 ---- batch: 020 ----
mean loss: 140.80
 ---- batch: 030 ----
mean loss: 151.06
 ---- batch: 040 ----
mean loss: 152.33
 ---- batch: 050 ----
mean loss: 154.43
 ---- batch: 060 ----
mean loss: 150.52
 ---- batch: 070 ----
mean loss: 156.89
 ---- batch: 080 ----
mean loss: 163.06
 ---- batch: 090 ----
mean loss: 163.32
train mean loss: 154.26
epoch train time: 0:00:00.509729
elapsed time: 0:01:39.949037
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:24:11.463735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.04
 ---- batch: 020 ----
mean loss: 153.99
 ---- batch: 030 ----
mean loss: 150.49
 ---- batch: 040 ----
mean loss: 150.26
 ---- batch: 050 ----
mean loss: 151.05
 ---- batch: 060 ----
mean loss: 157.40
 ---- batch: 070 ----
mean loss: 162.14
 ---- batch: 080 ----
mean loss: 151.49
 ---- batch: 090 ----
mean loss: 147.52
train mean loss: 152.95
epoch train time: 0:00:00.500039
elapsed time: 0:01:40.449243
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:24:11.963943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.58
 ---- batch: 020 ----
mean loss: 150.25
 ---- batch: 030 ----
mean loss: 145.63
 ---- batch: 040 ----
mean loss: 153.63
 ---- batch: 050 ----
mean loss: 146.86
 ---- batch: 060 ----
mean loss: 151.00
 ---- batch: 070 ----
mean loss: 153.99
 ---- batch: 080 ----
mean loss: 158.15
 ---- batch: 090 ----
mean loss: 161.30
train mean loss: 153.43
epoch train time: 0:00:00.492050
elapsed time: 0:01:40.941466
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:24:12.456162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.21
 ---- batch: 020 ----
mean loss: 148.71
 ---- batch: 030 ----
mean loss: 149.69
 ---- batch: 040 ----
mean loss: 153.09
 ---- batch: 050 ----
mean loss: 150.54
 ---- batch: 060 ----
mean loss: 155.06
 ---- batch: 070 ----
mean loss: 153.57
 ---- batch: 080 ----
mean loss: 153.78
 ---- batch: 090 ----
mean loss: 155.07
train mean loss: 152.46
epoch train time: 0:00:00.496808
elapsed time: 0:01:41.438434
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:24:12.953130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.40
 ---- batch: 020 ----
mean loss: 152.29
 ---- batch: 030 ----
mean loss: 150.19
 ---- batch: 040 ----
mean loss: 156.64
 ---- batch: 050 ----
mean loss: 150.36
 ---- batch: 060 ----
mean loss: 158.48
 ---- batch: 070 ----
mean loss: 150.24
 ---- batch: 080 ----
mean loss: 150.05
 ---- batch: 090 ----
mean loss: 152.74
train mean loss: 151.92
epoch train time: 0:00:00.511798
elapsed time: 0:01:41.950386
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:24:13.465095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.13
 ---- batch: 020 ----
mean loss: 149.17
 ---- batch: 030 ----
mean loss: 149.29
 ---- batch: 040 ----
mean loss: 152.04
 ---- batch: 050 ----
mean loss: 157.19
 ---- batch: 060 ----
mean loss: 150.51
 ---- batch: 070 ----
mean loss: 151.49
 ---- batch: 080 ----
mean loss: 157.31
 ---- batch: 090 ----
mean loss: 160.86
train mean loss: 152.53
epoch train time: 0:00:00.506764
elapsed time: 0:01:42.457319
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:24:13.972058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.30
 ---- batch: 020 ----
mean loss: 148.19
 ---- batch: 030 ----
mean loss: 151.33
 ---- batch: 040 ----
mean loss: 151.04
 ---- batch: 050 ----
mean loss: 148.26
 ---- batch: 060 ----
mean loss: 156.84
 ---- batch: 070 ----
mean loss: 154.93
 ---- batch: 080 ----
mean loss: 158.46
 ---- batch: 090 ----
mean loss: 155.93
train mean loss: 152.30
epoch train time: 0:00:00.498399
elapsed time: 0:01:42.955927
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:24:14.470635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.36
 ---- batch: 020 ----
mean loss: 151.08
 ---- batch: 030 ----
mean loss: 149.99
 ---- batch: 040 ----
mean loss: 153.18
 ---- batch: 050 ----
mean loss: 155.68
 ---- batch: 060 ----
mean loss: 148.29
 ---- batch: 070 ----
mean loss: 149.60
 ---- batch: 080 ----
mean loss: 150.37
 ---- batch: 090 ----
mean loss: 155.83
train mean loss: 150.99
epoch train time: 0:00:00.497003
elapsed time: 0:01:43.453117
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:24:14.967815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.10
 ---- batch: 020 ----
mean loss: 146.61
 ---- batch: 030 ----
mean loss: 158.48
 ---- batch: 040 ----
mean loss: 138.99
 ---- batch: 050 ----
mean loss: 155.83
 ---- batch: 060 ----
mean loss: 144.45
 ---- batch: 070 ----
mean loss: 156.57
 ---- batch: 080 ----
mean loss: 148.47
 ---- batch: 090 ----
mean loss: 163.03
train mean loss: 150.99
epoch train time: 0:00:00.491964
elapsed time: 0:01:43.945227
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:24:15.459925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.71
 ---- batch: 020 ----
mean loss: 144.37
 ---- batch: 030 ----
mean loss: 144.98
 ---- batch: 040 ----
mean loss: 152.67
 ---- batch: 050 ----
mean loss: 158.43
 ---- batch: 060 ----
mean loss: 153.41
 ---- batch: 070 ----
mean loss: 157.17
 ---- batch: 080 ----
mean loss: 155.19
 ---- batch: 090 ----
mean loss: 150.82
train mean loss: 151.68
epoch train time: 0:00:00.499198
elapsed time: 0:01:44.444570
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:24:15.959276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.91
 ---- batch: 020 ----
mean loss: 153.14
 ---- batch: 030 ----
mean loss: 143.42
 ---- batch: 040 ----
mean loss: 152.85
 ---- batch: 050 ----
mean loss: 146.35
 ---- batch: 060 ----
mean loss: 149.10
 ---- batch: 070 ----
mean loss: 152.09
 ---- batch: 080 ----
mean loss: 149.58
 ---- batch: 090 ----
mean loss: 148.26
train mean loss: 150.69
epoch train time: 0:00:00.501007
elapsed time: 0:01:44.945730
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:24:16.460426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.17
 ---- batch: 020 ----
mean loss: 153.85
 ---- batch: 030 ----
mean loss: 151.99
 ---- batch: 040 ----
mean loss: 152.01
 ---- batch: 050 ----
mean loss: 144.61
 ---- batch: 060 ----
mean loss: 150.49
 ---- batch: 070 ----
mean loss: 150.48
 ---- batch: 080 ----
mean loss: 149.33
 ---- batch: 090 ----
mean loss: 149.73
train mean loss: 149.65
epoch train time: 0:00:00.497580
elapsed time: 0:01:45.443460
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:24:16.958161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.31
 ---- batch: 020 ----
mean loss: 149.43
 ---- batch: 030 ----
mean loss: 142.27
 ---- batch: 040 ----
mean loss: 149.50
 ---- batch: 050 ----
mean loss: 153.80
 ---- batch: 060 ----
mean loss: 153.45
 ---- batch: 070 ----
mean loss: 144.51
 ---- batch: 080 ----
mean loss: 159.45
 ---- batch: 090 ----
mean loss: 152.48
train mean loss: 149.86
epoch train time: 0:00:00.496230
elapsed time: 0:01:45.939846
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:24:17.454567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.18
 ---- batch: 020 ----
mean loss: 145.97
 ---- batch: 030 ----
mean loss: 145.69
 ---- batch: 040 ----
mean loss: 156.66
 ---- batch: 050 ----
mean loss: 150.35
 ---- batch: 060 ----
mean loss: 155.98
 ---- batch: 070 ----
mean loss: 143.75
 ---- batch: 080 ----
mean loss: 147.97
 ---- batch: 090 ----
mean loss: 148.70
train mean loss: 149.53
epoch train time: 0:00:00.498287
elapsed time: 0:01:46.438314
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:24:17.953012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.58
 ---- batch: 020 ----
mean loss: 150.09
 ---- batch: 030 ----
mean loss: 148.18
 ---- batch: 040 ----
mean loss: 147.16
 ---- batch: 050 ----
mean loss: 146.00
 ---- batch: 060 ----
mean loss: 151.00
 ---- batch: 070 ----
mean loss: 144.53
 ---- batch: 080 ----
mean loss: 156.87
 ---- batch: 090 ----
mean loss: 153.48
train mean loss: 149.17
epoch train time: 0:00:00.499010
elapsed time: 0:01:46.937488
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:24:18.452187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.19
 ---- batch: 020 ----
mean loss: 148.00
 ---- batch: 030 ----
mean loss: 144.24
 ---- batch: 040 ----
mean loss: 148.78
 ---- batch: 050 ----
mean loss: 143.31
 ---- batch: 060 ----
mean loss: 142.95
 ---- batch: 070 ----
mean loss: 153.74
 ---- batch: 080 ----
mean loss: 148.97
 ---- batch: 090 ----
mean loss: 153.71
train mean loss: 148.33
epoch train time: 0:00:00.496734
elapsed time: 0:01:47.434396
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:24:18.949084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.68
 ---- batch: 020 ----
mean loss: 141.93
 ---- batch: 030 ----
mean loss: 143.15
 ---- batch: 040 ----
mean loss: 153.78
 ---- batch: 050 ----
mean loss: 151.44
 ---- batch: 060 ----
mean loss: 152.72
 ---- batch: 070 ----
mean loss: 150.23
 ---- batch: 080 ----
mean loss: 141.34
 ---- batch: 090 ----
mean loss: 152.33
train mean loss: 148.08
epoch train time: 0:00:00.492261
elapsed time: 0:01:47.926796
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:24:19.441495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.07
 ---- batch: 020 ----
mean loss: 147.90
 ---- batch: 030 ----
mean loss: 147.61
 ---- batch: 040 ----
mean loss: 151.80
 ---- batch: 050 ----
mean loss: 144.75
 ---- batch: 060 ----
mean loss: 150.40
 ---- batch: 070 ----
mean loss: 141.28
 ---- batch: 080 ----
mean loss: 154.42
 ---- batch: 090 ----
mean loss: 148.66
train mean loss: 147.86
epoch train time: 0:00:00.515552
elapsed time: 0:01:48.442496
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:24:19.957196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.65
 ---- batch: 020 ----
mean loss: 148.75
 ---- batch: 030 ----
mean loss: 140.04
 ---- batch: 040 ----
mean loss: 146.92
 ---- batch: 050 ----
mean loss: 145.12
 ---- batch: 060 ----
mean loss: 152.62
 ---- batch: 070 ----
mean loss: 146.60
 ---- batch: 080 ----
mean loss: 147.88
 ---- batch: 090 ----
mean loss: 154.50
train mean loss: 147.69
epoch train time: 0:00:00.499163
elapsed time: 0:01:48.941822
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:24:20.456547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.12
 ---- batch: 020 ----
mean loss: 146.99
 ---- batch: 030 ----
mean loss: 148.51
 ---- batch: 040 ----
mean loss: 144.85
 ---- batch: 050 ----
mean loss: 143.85
 ---- batch: 060 ----
mean loss: 143.49
 ---- batch: 070 ----
mean loss: 148.03
 ---- batch: 080 ----
mean loss: 151.56
 ---- batch: 090 ----
mean loss: 154.31
train mean loss: 146.49
epoch train time: 0:00:00.503630
elapsed time: 0:01:49.445625
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:24:20.960320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.93
 ---- batch: 020 ----
mean loss: 141.66
 ---- batch: 030 ----
mean loss: 150.56
 ---- batch: 040 ----
mean loss: 148.08
 ---- batch: 050 ----
mean loss: 144.10
 ---- batch: 060 ----
mean loss: 144.63
 ---- batch: 070 ----
mean loss: 146.01
 ---- batch: 080 ----
mean loss: 143.79
 ---- batch: 090 ----
mean loss: 151.50
train mean loss: 146.74
epoch train time: 0:00:00.500954
elapsed time: 0:01:49.946725
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:24:21.461453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.72
 ---- batch: 020 ----
mean loss: 149.00
 ---- batch: 030 ----
mean loss: 139.69
 ---- batch: 040 ----
mean loss: 149.08
 ---- batch: 050 ----
mean loss: 152.52
 ---- batch: 060 ----
mean loss: 152.50
 ---- batch: 070 ----
mean loss: 156.57
 ---- batch: 080 ----
mean loss: 156.95
 ---- batch: 090 ----
mean loss: 149.07
train mean loss: 149.93
epoch train time: 0:00:00.501279
elapsed time: 0:01:50.448185
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:24:21.962890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.80
 ---- batch: 020 ----
mean loss: 151.17
 ---- batch: 030 ----
mean loss: 146.16
 ---- batch: 040 ----
mean loss: 143.26
 ---- batch: 050 ----
mean loss: 147.24
 ---- batch: 060 ----
mean loss: 153.29
 ---- batch: 070 ----
mean loss: 145.14
 ---- batch: 080 ----
mean loss: 145.12
 ---- batch: 090 ----
mean loss: 143.39
train mean loss: 146.81
epoch train time: 0:00:00.502255
elapsed time: 0:01:50.950603
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:24:22.465298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.47
 ---- batch: 020 ----
mean loss: 143.00
 ---- batch: 030 ----
mean loss: 153.90
 ---- batch: 040 ----
mean loss: 145.21
 ---- batch: 050 ----
mean loss: 139.81
 ---- batch: 060 ----
mean loss: 149.11
 ---- batch: 070 ----
mean loss: 149.20
 ---- batch: 080 ----
mean loss: 154.16
 ---- batch: 090 ----
mean loss: 152.90
train mean loss: 147.19
epoch train time: 0:00:00.505392
elapsed time: 0:01:51.456143
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:24:22.970841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.54
 ---- batch: 020 ----
mean loss: 150.05
 ---- batch: 030 ----
mean loss: 147.46
 ---- batch: 040 ----
mean loss: 144.90
 ---- batch: 050 ----
mean loss: 150.34
 ---- batch: 060 ----
mean loss: 144.62
 ---- batch: 070 ----
mean loss: 140.06
 ---- batch: 080 ----
mean loss: 147.48
 ---- batch: 090 ----
mean loss: 148.09
train mean loss: 146.21
epoch train time: 0:00:00.492831
elapsed time: 0:01:51.949119
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:24:23.463816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.54
 ---- batch: 020 ----
mean loss: 139.36
 ---- batch: 030 ----
mean loss: 144.31
 ---- batch: 040 ----
mean loss: 146.65
 ---- batch: 050 ----
mean loss: 142.23
 ---- batch: 060 ----
mean loss: 142.19
 ---- batch: 070 ----
mean loss: 146.68
 ---- batch: 080 ----
mean loss: 148.72
 ---- batch: 090 ----
mean loss: 146.68
train mean loss: 145.04
epoch train time: 0:00:00.506058
elapsed time: 0:01:52.455376
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:24:23.970082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.59
 ---- batch: 020 ----
mean loss: 137.74
 ---- batch: 030 ----
mean loss: 144.42
 ---- batch: 040 ----
mean loss: 147.56
 ---- batch: 050 ----
mean loss: 151.65
 ---- batch: 060 ----
mean loss: 144.89
 ---- batch: 070 ----
mean loss: 147.99
 ---- batch: 080 ----
mean loss: 145.93
 ---- batch: 090 ----
mean loss: 150.14
train mean loss: 144.95
epoch train time: 0:00:00.492312
elapsed time: 0:01:52.947877
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:24:24.462573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.46
 ---- batch: 020 ----
mean loss: 141.61
 ---- batch: 030 ----
mean loss: 143.36
 ---- batch: 040 ----
mean loss: 145.14
 ---- batch: 050 ----
mean loss: 140.72
 ---- batch: 060 ----
mean loss: 146.83
 ---- batch: 070 ----
mean loss: 152.31
 ---- batch: 080 ----
mean loss: 153.54
 ---- batch: 090 ----
mean loss: 143.07
train mean loss: 145.07
epoch train time: 0:00:00.506773
elapsed time: 0:01:53.454800
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:24:24.969533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.03
 ---- batch: 020 ----
mean loss: 137.78
 ---- batch: 030 ----
mean loss: 143.28
 ---- batch: 040 ----
mean loss: 145.19
 ---- batch: 050 ----
mean loss: 140.40
 ---- batch: 060 ----
mean loss: 145.43
 ---- batch: 070 ----
mean loss: 153.61
 ---- batch: 080 ----
mean loss: 146.60
 ---- batch: 090 ----
mean loss: 144.51
train mean loss: 144.53
epoch train time: 0:00:00.496872
elapsed time: 0:01:53.951858
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:24:25.466556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.37
 ---- batch: 020 ----
mean loss: 149.71
 ---- batch: 030 ----
mean loss: 148.54
 ---- batch: 040 ----
mean loss: 149.46
 ---- batch: 050 ----
mean loss: 135.47
 ---- batch: 060 ----
mean loss: 150.92
 ---- batch: 070 ----
mean loss: 149.72
 ---- batch: 080 ----
mean loss: 146.74
 ---- batch: 090 ----
mean loss: 139.20
train mean loss: 144.81
epoch train time: 0:00:00.500138
elapsed time: 0:01:54.452191
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:24:25.966921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.86
 ---- batch: 020 ----
mean loss: 140.04
 ---- batch: 030 ----
mean loss: 139.50
 ---- batch: 040 ----
mean loss: 143.72
 ---- batch: 050 ----
mean loss: 142.91
 ---- batch: 060 ----
mean loss: 149.31
 ---- batch: 070 ----
mean loss: 147.97
 ---- batch: 080 ----
mean loss: 141.77
 ---- batch: 090 ----
mean loss: 148.04
train mean loss: 144.24
epoch train time: 0:00:00.500811
elapsed time: 0:01:54.953185
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:24:26.467884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.78
 ---- batch: 020 ----
mean loss: 138.48
 ---- batch: 030 ----
mean loss: 139.69
 ---- batch: 040 ----
mean loss: 139.38
 ---- batch: 050 ----
mean loss: 147.76
 ---- batch: 060 ----
mean loss: 146.24
 ---- batch: 070 ----
mean loss: 148.40
 ---- batch: 080 ----
mean loss: 142.12
 ---- batch: 090 ----
mean loss: 147.75
train mean loss: 143.98
epoch train time: 0:00:00.509439
elapsed time: 0:01:55.462772
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:24:26.977472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.49
 ---- batch: 020 ----
mean loss: 143.87
 ---- batch: 030 ----
mean loss: 140.62
 ---- batch: 040 ----
mean loss: 149.36
 ---- batch: 050 ----
mean loss: 141.97
 ---- batch: 060 ----
mean loss: 149.91
 ---- batch: 070 ----
mean loss: 143.42
 ---- batch: 080 ----
mean loss: 140.16
 ---- batch: 090 ----
mean loss: 146.38
train mean loss: 143.88
epoch train time: 0:00:00.492990
elapsed time: 0:01:55.955914
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:24:27.470612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.78
 ---- batch: 020 ----
mean loss: 144.19
 ---- batch: 030 ----
mean loss: 146.06
 ---- batch: 040 ----
mean loss: 141.49
 ---- batch: 050 ----
mean loss: 139.54
 ---- batch: 060 ----
mean loss: 140.01
 ---- batch: 070 ----
mean loss: 138.52
 ---- batch: 080 ----
mean loss: 142.93
 ---- batch: 090 ----
mean loss: 148.20
train mean loss: 143.12
epoch train time: 0:00:00.503448
elapsed time: 0:01:56.459514
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:24:27.974212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.90
 ---- batch: 020 ----
mean loss: 141.11
 ---- batch: 030 ----
mean loss: 147.67
 ---- batch: 040 ----
mean loss: 138.72
 ---- batch: 050 ----
mean loss: 144.21
 ---- batch: 060 ----
mean loss: 141.90
 ---- batch: 070 ----
mean loss: 146.65
 ---- batch: 080 ----
mean loss: 143.85
 ---- batch: 090 ----
mean loss: 145.58
train mean loss: 143.42
epoch train time: 0:00:00.492229
elapsed time: 0:01:56.951891
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:24:28.466597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.74
 ---- batch: 020 ----
mean loss: 143.29
 ---- batch: 030 ----
mean loss: 138.07
 ---- batch: 040 ----
mean loss: 140.87
 ---- batch: 050 ----
mean loss: 148.80
 ---- batch: 060 ----
mean loss: 151.56
 ---- batch: 070 ----
mean loss: 143.34
 ---- batch: 080 ----
mean loss: 144.69
 ---- batch: 090 ----
mean loss: 144.50
train mean loss: 143.97
epoch train time: 0:00:00.497667
elapsed time: 0:01:57.449715
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:24:28.964439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.84
 ---- batch: 020 ----
mean loss: 138.67
 ---- batch: 030 ----
mean loss: 138.17
 ---- batch: 040 ----
mean loss: 145.82
 ---- batch: 050 ----
mean loss: 136.87
 ---- batch: 060 ----
mean loss: 141.59
 ---- batch: 070 ----
mean loss: 146.30
 ---- batch: 080 ----
mean loss: 144.02
 ---- batch: 090 ----
mean loss: 147.20
train mean loss: 142.45
epoch train time: 0:00:00.506192
elapsed time: 0:01:57.956083
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:24:29.470784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.01
 ---- batch: 020 ----
mean loss: 141.05
 ---- batch: 030 ----
mean loss: 139.38
 ---- batch: 040 ----
mean loss: 142.64
 ---- batch: 050 ----
mean loss: 149.87
 ---- batch: 060 ----
mean loss: 137.12
 ---- batch: 070 ----
mean loss: 146.45
 ---- batch: 080 ----
mean loss: 141.95
 ---- batch: 090 ----
mean loss: 143.10
train mean loss: 141.70
epoch train time: 0:00:00.503295
elapsed time: 0:01:58.459540
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:24:29.974229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.05
 ---- batch: 020 ----
mean loss: 139.42
 ---- batch: 030 ----
mean loss: 142.30
 ---- batch: 040 ----
mean loss: 140.35
 ---- batch: 050 ----
mean loss: 141.54
 ---- batch: 060 ----
mean loss: 143.10
 ---- batch: 070 ----
mean loss: 143.14
 ---- batch: 080 ----
mean loss: 138.22
 ---- batch: 090 ----
mean loss: 150.29
train mean loss: 141.78
epoch train time: 0:00:00.497373
elapsed time: 0:01:58.957049
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:24:30.471746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.53
 ---- batch: 020 ----
mean loss: 138.09
 ---- batch: 030 ----
mean loss: 136.51
 ---- batch: 040 ----
mean loss: 143.19
 ---- batch: 050 ----
mean loss: 143.84
 ---- batch: 060 ----
mean loss: 138.22
 ---- batch: 070 ----
mean loss: 143.45
 ---- batch: 080 ----
mean loss: 148.17
 ---- batch: 090 ----
mean loss: 140.83
train mean loss: 142.24
epoch train time: 0:00:00.497668
elapsed time: 0:01:59.454879
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:24:30.969570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.96
 ---- batch: 020 ----
mean loss: 137.09
 ---- batch: 030 ----
mean loss: 148.13
 ---- batch: 040 ----
mean loss: 134.58
 ---- batch: 050 ----
mean loss: 137.11
 ---- batch: 060 ----
mean loss: 141.23
 ---- batch: 070 ----
mean loss: 145.97
 ---- batch: 080 ----
mean loss: 141.07
 ---- batch: 090 ----
mean loss: 146.72
train mean loss: 140.89
epoch train time: 0:00:00.491541
elapsed time: 0:01:59.946573
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:24:31.461257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.30
 ---- batch: 020 ----
mean loss: 143.01
 ---- batch: 030 ----
mean loss: 143.68
 ---- batch: 040 ----
mean loss: 136.64
 ---- batch: 050 ----
mean loss: 144.34
 ---- batch: 060 ----
mean loss: 146.91
 ---- batch: 070 ----
mean loss: 143.42
 ---- batch: 080 ----
mean loss: 142.31
 ---- batch: 090 ----
mean loss: 142.82
train mean loss: 141.90
epoch train time: 0:00:00.509113
elapsed time: 0:02:00.455834
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:24:31.970541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.58
 ---- batch: 020 ----
mean loss: 137.68
 ---- batch: 030 ----
mean loss: 144.77
 ---- batch: 040 ----
mean loss: 144.38
 ---- batch: 050 ----
mean loss: 136.47
 ---- batch: 060 ----
mean loss: 147.48
 ---- batch: 070 ----
mean loss: 147.75
 ---- batch: 080 ----
mean loss: 138.89
 ---- batch: 090 ----
mean loss: 140.07
train mean loss: 141.56
epoch train time: 0:00:00.492099
elapsed time: 0:02:00.948095
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:24:32.462810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.33
 ---- batch: 020 ----
mean loss: 134.74
 ---- batch: 030 ----
mean loss: 136.21
 ---- batch: 040 ----
mean loss: 141.44
 ---- batch: 050 ----
mean loss: 144.29
 ---- batch: 060 ----
mean loss: 147.07
 ---- batch: 070 ----
mean loss: 138.31
 ---- batch: 080 ----
mean loss: 135.83
 ---- batch: 090 ----
mean loss: 150.19
train mean loss: 140.61
epoch train time: 0:00:00.499707
elapsed time: 0:02:01.447977
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:24:32.962666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.30
 ---- batch: 020 ----
mean loss: 137.59
 ---- batch: 030 ----
mean loss: 141.55
 ---- batch: 040 ----
mean loss: 134.95
 ---- batch: 050 ----
mean loss: 134.09
 ---- batch: 060 ----
mean loss: 144.35
 ---- batch: 070 ----
mean loss: 143.94
 ---- batch: 080 ----
mean loss: 139.87
 ---- batch: 090 ----
mean loss: 137.52
train mean loss: 139.93
epoch train time: 0:00:00.493582
elapsed time: 0:02:01.941706
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:24:33.456390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.76
 ---- batch: 020 ----
mean loss: 140.91
 ---- batch: 030 ----
mean loss: 139.54
 ---- batch: 040 ----
mean loss: 138.58
 ---- batch: 050 ----
mean loss: 142.47
 ---- batch: 060 ----
mean loss: 143.94
 ---- batch: 070 ----
mean loss: 137.15
 ---- batch: 080 ----
mean loss: 135.34
 ---- batch: 090 ----
mean loss: 141.54
train mean loss: 140.02
epoch train time: 0:00:00.498940
elapsed time: 0:02:02.440788
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:24:33.955490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.32
 ---- batch: 020 ----
mean loss: 137.22
 ---- batch: 030 ----
mean loss: 140.66
 ---- batch: 040 ----
mean loss: 138.66
 ---- batch: 050 ----
mean loss: 136.14
 ---- batch: 060 ----
mean loss: 140.48
 ---- batch: 070 ----
mean loss: 137.24
 ---- batch: 080 ----
mean loss: 141.11
 ---- batch: 090 ----
mean loss: 145.12
train mean loss: 139.15
epoch train time: 0:00:00.493772
elapsed time: 0:02:02.934712
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:24:34.449424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.53
 ---- batch: 020 ----
mean loss: 140.03
 ---- batch: 030 ----
mean loss: 137.11
 ---- batch: 040 ----
mean loss: 141.36
 ---- batch: 050 ----
mean loss: 140.29
 ---- batch: 060 ----
mean loss: 139.84
 ---- batch: 070 ----
mean loss: 141.43
 ---- batch: 080 ----
mean loss: 145.07
 ---- batch: 090 ----
mean loss: 138.64
train mean loss: 140.51
epoch train time: 0:00:00.504029
elapsed time: 0:02:03.438899
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:24:34.953637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.93
 ---- batch: 020 ----
mean loss: 137.99
 ---- batch: 030 ----
mean loss: 136.30
 ---- batch: 040 ----
mean loss: 139.25
 ---- batch: 050 ----
mean loss: 138.33
 ---- batch: 060 ----
mean loss: 140.89
 ---- batch: 070 ----
mean loss: 143.12
 ---- batch: 080 ----
mean loss: 140.87
 ---- batch: 090 ----
mean loss: 141.73
train mean loss: 139.35
epoch train time: 0:00:00.489860
elapsed time: 0:02:03.928951
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:24:35.443648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.06
 ---- batch: 020 ----
mean loss: 139.87
 ---- batch: 030 ----
mean loss: 139.68
 ---- batch: 040 ----
mean loss: 139.50
 ---- batch: 050 ----
mean loss: 139.84
 ---- batch: 060 ----
mean loss: 141.84
 ---- batch: 070 ----
mean loss: 135.24
 ---- batch: 080 ----
mean loss: 146.03
 ---- batch: 090 ----
mean loss: 132.93
train mean loss: 139.01
epoch train time: 0:00:00.503215
elapsed time: 0:02:04.432326
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:24:35.947026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.76
 ---- batch: 020 ----
mean loss: 138.85
 ---- batch: 030 ----
mean loss: 134.35
 ---- batch: 040 ----
mean loss: 140.17
 ---- batch: 050 ----
mean loss: 141.40
 ---- batch: 060 ----
mean loss: 142.37
 ---- batch: 070 ----
mean loss: 137.55
 ---- batch: 080 ----
mean loss: 137.55
 ---- batch: 090 ----
mean loss: 135.08
train mean loss: 139.16
epoch train time: 0:00:00.499730
elapsed time: 0:02:04.932229
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:24:36.446939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.91
 ---- batch: 020 ----
mean loss: 137.94
 ---- batch: 030 ----
mean loss: 135.64
 ---- batch: 040 ----
mean loss: 138.59
 ---- batch: 050 ----
mean loss: 144.29
 ---- batch: 060 ----
mean loss: 143.22
 ---- batch: 070 ----
mean loss: 136.21
 ---- batch: 080 ----
mean loss: 136.32
 ---- batch: 090 ----
mean loss: 139.47
train mean loss: 138.42
epoch train time: 0:00:00.519329
elapsed time: 0:02:05.451761
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:24:36.966470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.08
 ---- batch: 020 ----
mean loss: 134.69
 ---- batch: 030 ----
mean loss: 135.56
 ---- batch: 040 ----
mean loss: 140.78
 ---- batch: 050 ----
mean loss: 146.25
 ---- batch: 060 ----
mean loss: 140.22
 ---- batch: 070 ----
mean loss: 140.51
 ---- batch: 080 ----
mean loss: 135.76
 ---- batch: 090 ----
mean loss: 135.05
train mean loss: 138.85
epoch train time: 0:00:00.512148
elapsed time: 0:02:05.964072
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:24:37.478774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.06
 ---- batch: 020 ----
mean loss: 129.84
 ---- batch: 030 ----
mean loss: 138.96
 ---- batch: 040 ----
mean loss: 140.72
 ---- batch: 050 ----
mean loss: 137.74
 ---- batch: 060 ----
mean loss: 147.57
 ---- batch: 070 ----
mean loss: 145.82
 ---- batch: 080 ----
mean loss: 134.50
 ---- batch: 090 ----
mean loss: 145.42
train mean loss: 139.74
epoch train time: 0:00:00.519657
elapsed time: 0:02:06.483882
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:24:37.998580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.85
 ---- batch: 020 ----
mean loss: 135.43
 ---- batch: 030 ----
mean loss: 136.01
 ---- batch: 040 ----
mean loss: 137.76
 ---- batch: 050 ----
mean loss: 141.55
 ---- batch: 060 ----
mean loss: 138.69
 ---- batch: 070 ----
mean loss: 136.62
 ---- batch: 080 ----
mean loss: 137.18
 ---- batch: 090 ----
mean loss: 139.91
train mean loss: 138.04
epoch train time: 0:00:00.517812
elapsed time: 0:02:07.001850
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:24:38.516550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.43
 ---- batch: 020 ----
mean loss: 138.04
 ---- batch: 030 ----
mean loss: 136.14
 ---- batch: 040 ----
mean loss: 134.32
 ---- batch: 050 ----
mean loss: 139.32
 ---- batch: 060 ----
mean loss: 136.41
 ---- batch: 070 ----
mean loss: 135.48
 ---- batch: 080 ----
mean loss: 145.42
 ---- batch: 090 ----
mean loss: 139.68
train mean loss: 137.75
epoch train time: 0:00:00.517079
elapsed time: 0:02:07.519082
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:24:39.033781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.41
 ---- batch: 020 ----
mean loss: 135.03
 ---- batch: 030 ----
mean loss: 141.30
 ---- batch: 040 ----
mean loss: 134.12
 ---- batch: 050 ----
mean loss: 134.32
 ---- batch: 060 ----
mean loss: 134.19
 ---- batch: 070 ----
mean loss: 134.42
 ---- batch: 080 ----
mean loss: 141.97
 ---- batch: 090 ----
mean loss: 145.98
train mean loss: 137.65
epoch train time: 0:00:00.522101
elapsed time: 0:02:08.041335
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:24:39.556048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.56
 ---- batch: 020 ----
mean loss: 136.86
 ---- batch: 030 ----
mean loss: 135.12
 ---- batch: 040 ----
mean loss: 139.05
 ---- batch: 050 ----
mean loss: 137.75
 ---- batch: 060 ----
mean loss: 137.65
 ---- batch: 070 ----
mean loss: 139.19
 ---- batch: 080 ----
mean loss: 142.39
 ---- batch: 090 ----
mean loss: 130.63
train mean loss: 137.21
epoch train time: 0:00:00.516489
elapsed time: 0:02:08.557990
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:24:40.072710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.52
 ---- batch: 020 ----
mean loss: 134.54
 ---- batch: 030 ----
mean loss: 137.65
 ---- batch: 040 ----
mean loss: 133.24
 ---- batch: 050 ----
mean loss: 135.18
 ---- batch: 060 ----
mean loss: 137.05
 ---- batch: 070 ----
mean loss: 140.78
 ---- batch: 080 ----
mean loss: 141.59
 ---- batch: 090 ----
mean loss: 133.57
train mean loss: 136.79
epoch train time: 0:00:00.511481
elapsed time: 0:02:09.069641
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:24:40.584377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.71
 ---- batch: 020 ----
mean loss: 143.11
 ---- batch: 030 ----
mean loss: 139.24
 ---- batch: 040 ----
mean loss: 138.38
 ---- batch: 050 ----
mean loss: 140.32
 ---- batch: 060 ----
mean loss: 137.24
 ---- batch: 070 ----
mean loss: 132.09
 ---- batch: 080 ----
mean loss: 132.85
 ---- batch: 090 ----
mean loss: 138.08
train mean loss: 137.14
epoch train time: 0:00:00.526096
elapsed time: 0:02:09.595929
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:24:41.110628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.87
 ---- batch: 020 ----
mean loss: 130.89
 ---- batch: 030 ----
mean loss: 138.47
 ---- batch: 040 ----
mean loss: 135.09
 ---- batch: 050 ----
mean loss: 138.10
 ---- batch: 060 ----
mean loss: 133.31
 ---- batch: 070 ----
mean loss: 138.83
 ---- batch: 080 ----
mean loss: 137.19
 ---- batch: 090 ----
mean loss: 144.32
train mean loss: 136.50
epoch train time: 0:00:00.512203
elapsed time: 0:02:10.108298
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:24:41.622999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.69
 ---- batch: 020 ----
mean loss: 132.51
 ---- batch: 030 ----
mean loss: 130.93
 ---- batch: 040 ----
mean loss: 134.52
 ---- batch: 050 ----
mean loss: 137.69
 ---- batch: 060 ----
mean loss: 136.14
 ---- batch: 070 ----
mean loss: 135.16
 ---- batch: 080 ----
mean loss: 141.19
 ---- batch: 090 ----
mean loss: 139.23
train mean loss: 136.21
epoch train time: 0:00:00.516500
elapsed time: 0:02:10.624946
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:24:42.139643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.43
 ---- batch: 020 ----
mean loss: 139.79
 ---- batch: 030 ----
mean loss: 136.99
 ---- batch: 040 ----
mean loss: 129.90
 ---- batch: 050 ----
mean loss: 134.94
 ---- batch: 060 ----
mean loss: 137.92
 ---- batch: 070 ----
mean loss: 142.30
 ---- batch: 080 ----
mean loss: 139.80
 ---- batch: 090 ----
mean loss: 138.90
train mean loss: 136.78
epoch train time: 0:00:00.495391
elapsed time: 0:02:11.120484
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:24:42.635197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.11
 ---- batch: 020 ----
mean loss: 135.01
 ---- batch: 030 ----
mean loss: 137.48
 ---- batch: 040 ----
mean loss: 134.32
 ---- batch: 050 ----
mean loss: 133.09
 ---- batch: 060 ----
mean loss: 139.80
 ---- batch: 070 ----
mean loss: 137.92
 ---- batch: 080 ----
mean loss: 129.21
 ---- batch: 090 ----
mean loss: 139.19
train mean loss: 136.02
epoch train time: 0:00:00.499855
elapsed time: 0:02:11.620515
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:24:43.135216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.25
 ---- batch: 020 ----
mean loss: 129.05
 ---- batch: 030 ----
mean loss: 129.31
 ---- batch: 040 ----
mean loss: 132.41
 ---- batch: 050 ----
mean loss: 135.16
 ---- batch: 060 ----
mean loss: 139.79
 ---- batch: 070 ----
mean loss: 133.94
 ---- batch: 080 ----
mean loss: 135.45
 ---- batch: 090 ----
mean loss: 142.23
train mean loss: 135.49
epoch train time: 0:00:00.498120
elapsed time: 0:02:12.118785
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:24:43.633482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.08
 ---- batch: 020 ----
mean loss: 130.26
 ---- batch: 030 ----
mean loss: 135.46
 ---- batch: 040 ----
mean loss: 137.44
 ---- batch: 050 ----
mean loss: 137.65
 ---- batch: 060 ----
mean loss: 142.03
 ---- batch: 070 ----
mean loss: 137.21
 ---- batch: 080 ----
mean loss: 140.22
 ---- batch: 090 ----
mean loss: 136.67
train mean loss: 135.66
epoch train time: 0:00:00.503229
elapsed time: 0:02:12.622161
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:24:44.136893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.18
 ---- batch: 020 ----
mean loss: 133.52
 ---- batch: 030 ----
mean loss: 135.53
 ---- batch: 040 ----
mean loss: 134.32
 ---- batch: 050 ----
mean loss: 134.25
 ---- batch: 060 ----
mean loss: 136.89
 ---- batch: 070 ----
mean loss: 135.27
 ---- batch: 080 ----
mean loss: 142.48
 ---- batch: 090 ----
mean loss: 138.61
train mean loss: 136.23
epoch train time: 0:00:00.493717
elapsed time: 0:02:13.116078
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:24:44.630776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.15
 ---- batch: 020 ----
mean loss: 137.04
 ---- batch: 030 ----
mean loss: 126.28
 ---- batch: 040 ----
mean loss: 133.41
 ---- batch: 050 ----
mean loss: 132.58
 ---- batch: 060 ----
mean loss: 134.94
 ---- batch: 070 ----
mean loss: 141.64
 ---- batch: 080 ----
mean loss: 141.32
 ---- batch: 090 ----
mean loss: 140.45
train mean loss: 135.38
epoch train time: 0:00:00.497470
elapsed time: 0:02:13.613699
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:24:45.128398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.28
 ---- batch: 020 ----
mean loss: 134.37
 ---- batch: 030 ----
mean loss: 123.17
 ---- batch: 040 ----
mean loss: 132.81
 ---- batch: 050 ----
mean loss: 124.42
 ---- batch: 060 ----
mean loss: 122.11
 ---- batch: 070 ----
mean loss: 127.64
 ---- batch: 080 ----
mean loss: 132.27
 ---- batch: 090 ----
mean loss: 129.19
train mean loss: 129.46
epoch train time: 0:00:00.499358
elapsed time: 0:02:14.113223
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:24:45.627910
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.19
 ---- batch: 020 ----
mean loss: 125.48
 ---- batch: 030 ----
mean loss: 123.39
 ---- batch: 040 ----
mean loss: 125.55
 ---- batch: 050 ----
mean loss: 129.58
 ---- batch: 060 ----
mean loss: 128.52
 ---- batch: 070 ----
mean loss: 129.07
 ---- batch: 080 ----
mean loss: 132.31
 ---- batch: 090 ----
mean loss: 122.96
train mean loss: 127.75
epoch train time: 0:00:00.506171
elapsed time: 0:02:14.619537
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:24:46.134238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.52
 ---- batch: 020 ----
mean loss: 128.78
 ---- batch: 030 ----
mean loss: 119.62
 ---- batch: 040 ----
mean loss: 126.03
 ---- batch: 050 ----
mean loss: 129.58
 ---- batch: 060 ----
mean loss: 124.47
 ---- batch: 070 ----
mean loss: 127.68
 ---- batch: 080 ----
mean loss: 128.44
 ---- batch: 090 ----
mean loss: 131.61
train mean loss: 127.06
epoch train time: 0:00:00.501389
elapsed time: 0:02:15.121081
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:24:46.635779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.88
 ---- batch: 020 ----
mean loss: 123.60
 ---- batch: 030 ----
mean loss: 129.47
 ---- batch: 040 ----
mean loss: 124.99
 ---- batch: 050 ----
mean loss: 128.63
 ---- batch: 060 ----
mean loss: 129.44
 ---- batch: 070 ----
mean loss: 124.55
 ---- batch: 080 ----
mean loss: 131.61
 ---- batch: 090 ----
mean loss: 121.92
train mean loss: 127.03
epoch train time: 0:00:00.508330
elapsed time: 0:02:15.629563
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:24:47.144295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.65
 ---- batch: 020 ----
mean loss: 131.53
 ---- batch: 030 ----
mean loss: 131.09
 ---- batch: 040 ----
mean loss: 119.80
 ---- batch: 050 ----
mean loss: 128.10
 ---- batch: 060 ----
mean loss: 128.08
 ---- batch: 070 ----
mean loss: 121.87
 ---- batch: 080 ----
mean loss: 132.02
 ---- batch: 090 ----
mean loss: 125.12
train mean loss: 126.98
epoch train time: 0:00:00.500136
elapsed time: 0:02:16.129882
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:24:47.644599
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.72
 ---- batch: 020 ----
mean loss: 122.98
 ---- batch: 030 ----
mean loss: 127.85
 ---- batch: 040 ----
mean loss: 128.25
 ---- batch: 050 ----
mean loss: 130.87
 ---- batch: 060 ----
mean loss: 123.77
 ---- batch: 070 ----
mean loss: 127.85
 ---- batch: 080 ----
mean loss: 127.37
 ---- batch: 090 ----
mean loss: 128.32
train mean loss: 127.03
epoch train time: 0:00:00.504069
elapsed time: 0:02:16.634127
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:24:48.148818
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.04
 ---- batch: 020 ----
mean loss: 123.64
 ---- batch: 030 ----
mean loss: 127.22
 ---- batch: 040 ----
mean loss: 130.25
 ---- batch: 050 ----
mean loss: 128.97
 ---- batch: 060 ----
mean loss: 121.36
 ---- batch: 070 ----
mean loss: 125.30
 ---- batch: 080 ----
mean loss: 125.26
 ---- batch: 090 ----
mean loss: 129.95
train mean loss: 127.01
epoch train time: 0:00:00.501137
elapsed time: 0:02:17.135408
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:24:48.650105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.61
 ---- batch: 020 ----
mean loss: 128.89
 ---- batch: 030 ----
mean loss: 129.47
 ---- batch: 040 ----
mean loss: 129.78
 ---- batch: 050 ----
mean loss: 127.23
 ---- batch: 060 ----
mean loss: 125.22
 ---- batch: 070 ----
mean loss: 123.40
 ---- batch: 080 ----
mean loss: 127.18
 ---- batch: 090 ----
mean loss: 124.98
train mean loss: 126.74
epoch train time: 0:00:00.500424
elapsed time: 0:02:17.635982
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:24:49.150680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.07
 ---- batch: 020 ----
mean loss: 130.13
 ---- batch: 030 ----
mean loss: 124.27
 ---- batch: 040 ----
mean loss: 122.16
 ---- batch: 050 ----
mean loss: 130.90
 ---- batch: 060 ----
mean loss: 128.39
 ---- batch: 070 ----
mean loss: 125.77
 ---- batch: 080 ----
mean loss: 124.63
 ---- batch: 090 ----
mean loss: 127.84
train mean loss: 126.73
epoch train time: 0:00:00.497113
elapsed time: 0:02:18.133244
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:24:49.647951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.11
 ---- batch: 020 ----
mean loss: 126.44
 ---- batch: 030 ----
mean loss: 132.39
 ---- batch: 040 ----
mean loss: 129.27
 ---- batch: 050 ----
mean loss: 125.88
 ---- batch: 060 ----
mean loss: 129.29
 ---- batch: 070 ----
mean loss: 123.37
 ---- batch: 080 ----
mean loss: 127.90
 ---- batch: 090 ----
mean loss: 126.12
train mean loss: 126.75
epoch train time: 0:00:00.497743
elapsed time: 0:02:18.631150
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:24:50.145847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.51
 ---- batch: 020 ----
mean loss: 119.39
 ---- batch: 030 ----
mean loss: 129.16
 ---- batch: 040 ----
mean loss: 131.64
 ---- batch: 050 ----
mean loss: 124.48
 ---- batch: 060 ----
mean loss: 123.67
 ---- batch: 070 ----
mean loss: 130.39
 ---- batch: 080 ----
mean loss: 125.99
 ---- batch: 090 ----
mean loss: 126.79
train mean loss: 126.51
epoch train time: 0:00:00.495543
elapsed time: 0:02:19.126840
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:24:50.641539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.90
 ---- batch: 020 ----
mean loss: 121.58
 ---- batch: 030 ----
mean loss: 126.74
 ---- batch: 040 ----
mean loss: 130.74
 ---- batch: 050 ----
mean loss: 125.91
 ---- batch: 060 ----
mean loss: 125.22
 ---- batch: 070 ----
mean loss: 125.75
 ---- batch: 080 ----
mean loss: 128.87
 ---- batch: 090 ----
mean loss: 127.66
train mean loss: 126.75
epoch train time: 0:00:00.520044
elapsed time: 0:02:19.647059
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:24:51.161778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.49
 ---- batch: 020 ----
mean loss: 127.43
 ---- batch: 030 ----
mean loss: 121.34
 ---- batch: 040 ----
mean loss: 129.68
 ---- batch: 050 ----
mean loss: 130.60
 ---- batch: 060 ----
mean loss: 127.39
 ---- batch: 070 ----
mean loss: 130.05
 ---- batch: 080 ----
mean loss: 126.14
 ---- batch: 090 ----
mean loss: 127.76
train mean loss: 126.64
epoch train time: 0:00:00.507177
elapsed time: 0:02:20.154422
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:24:51.669150
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.02
 ---- batch: 020 ----
mean loss: 119.11
 ---- batch: 030 ----
mean loss: 124.53
 ---- batch: 040 ----
mean loss: 125.56
 ---- batch: 050 ----
mean loss: 123.84
 ---- batch: 060 ----
mean loss: 129.09
 ---- batch: 070 ----
mean loss: 129.15
 ---- batch: 080 ----
mean loss: 130.95
 ---- batch: 090 ----
mean loss: 131.15
train mean loss: 126.70
epoch train time: 0:00:00.503223
elapsed time: 0:02:20.657841
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:24:52.172535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.17
 ---- batch: 020 ----
mean loss: 123.47
 ---- batch: 030 ----
mean loss: 122.93
 ---- batch: 040 ----
mean loss: 124.15
 ---- batch: 050 ----
mean loss: 125.90
 ---- batch: 060 ----
mean loss: 130.42
 ---- batch: 070 ----
mean loss: 125.84
 ---- batch: 080 ----
mean loss: 133.72
 ---- batch: 090 ----
mean loss: 126.33
train mean loss: 126.60
epoch train time: 0:00:00.499433
elapsed time: 0:02:21.157448
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:24:52.672179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.39
 ---- batch: 020 ----
mean loss: 123.10
 ---- batch: 030 ----
mean loss: 127.91
 ---- batch: 040 ----
mean loss: 125.39
 ---- batch: 050 ----
mean loss: 126.63
 ---- batch: 060 ----
mean loss: 126.21
 ---- batch: 070 ----
mean loss: 129.03
 ---- batch: 080 ----
mean loss: 128.04
 ---- batch: 090 ----
mean loss: 124.96
train mean loss: 126.57
epoch train time: 0:00:00.499635
elapsed time: 0:02:21.657265
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:24:53.171970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.62
 ---- batch: 020 ----
mean loss: 131.50
 ---- batch: 030 ----
mean loss: 127.44
 ---- batch: 040 ----
mean loss: 123.15
 ---- batch: 050 ----
mean loss: 124.97
 ---- batch: 060 ----
mean loss: 127.31
 ---- batch: 070 ----
mean loss: 125.79
 ---- batch: 080 ----
mean loss: 130.61
 ---- batch: 090 ----
mean loss: 128.47
train mean loss: 126.64
epoch train time: 0:00:00.494649
elapsed time: 0:02:22.152071
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:24:53.666770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.17
 ---- batch: 020 ----
mean loss: 126.55
 ---- batch: 030 ----
mean loss: 125.05
 ---- batch: 040 ----
mean loss: 125.25
 ---- batch: 050 ----
mean loss: 123.23
 ---- batch: 060 ----
mean loss: 126.84
 ---- batch: 070 ----
mean loss: 123.93
 ---- batch: 080 ----
mean loss: 131.17
 ---- batch: 090 ----
mean loss: 134.57
train mean loss: 126.31
epoch train time: 0:00:00.497626
elapsed time: 0:02:22.649860
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:24:54.164566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.31
 ---- batch: 020 ----
mean loss: 124.23
 ---- batch: 030 ----
mean loss: 130.43
 ---- batch: 040 ----
mean loss: 128.80
 ---- batch: 050 ----
mean loss: 130.43
 ---- batch: 060 ----
mean loss: 119.26
 ---- batch: 070 ----
mean loss: 122.10
 ---- batch: 080 ----
mean loss: 130.60
 ---- batch: 090 ----
mean loss: 123.99
train mean loss: 126.30
epoch train time: 0:00:00.493905
elapsed time: 0:02:23.143942
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:24:54.658657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.12
 ---- batch: 020 ----
mean loss: 123.58
 ---- batch: 030 ----
mean loss: 121.88
 ---- batch: 040 ----
mean loss: 129.03
 ---- batch: 050 ----
mean loss: 125.59
 ---- batch: 060 ----
mean loss: 125.49
 ---- batch: 070 ----
mean loss: 126.96
 ---- batch: 080 ----
mean loss: 128.84
 ---- batch: 090 ----
mean loss: 126.03
train mean loss: 126.36
epoch train time: 0:00:00.497247
elapsed time: 0:02:23.641384
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:24:55.156100
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.64
 ---- batch: 020 ----
mean loss: 120.06
 ---- batch: 030 ----
mean loss: 124.95
 ---- batch: 040 ----
mean loss: 126.44
 ---- batch: 050 ----
mean loss: 125.34
 ---- batch: 060 ----
mean loss: 129.65
 ---- batch: 070 ----
mean loss: 126.38
 ---- batch: 080 ----
mean loss: 134.29
 ---- batch: 090 ----
mean loss: 122.66
train mean loss: 126.18
epoch train time: 0:00:00.492770
elapsed time: 0:02:24.134320
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:24:55.649023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.70
 ---- batch: 020 ----
mean loss: 126.35
 ---- batch: 030 ----
mean loss: 124.24
 ---- batch: 040 ----
mean loss: 120.34
 ---- batch: 050 ----
mean loss: 122.16
 ---- batch: 060 ----
mean loss: 129.67
 ---- batch: 070 ----
mean loss: 124.66
 ---- batch: 080 ----
mean loss: 136.47
 ---- batch: 090 ----
mean loss: 127.51
train mean loss: 126.27
epoch train time: 0:00:00.504490
elapsed time: 0:02:24.638985
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:24:56.153714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.70
 ---- batch: 020 ----
mean loss: 126.57
 ---- batch: 030 ----
mean loss: 127.96
 ---- batch: 040 ----
mean loss: 124.81
 ---- batch: 050 ----
mean loss: 125.55
 ---- batch: 060 ----
mean loss: 124.06
 ---- batch: 070 ----
mean loss: 127.89
 ---- batch: 080 ----
mean loss: 126.60
 ---- batch: 090 ----
mean loss: 132.15
train mean loss: 126.20
epoch train time: 0:00:00.505584
elapsed time: 0:02:25.144750
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:24:56.659451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.01
 ---- batch: 020 ----
mean loss: 119.82
 ---- batch: 030 ----
mean loss: 124.21
 ---- batch: 040 ----
mean loss: 122.09
 ---- batch: 050 ----
mean loss: 125.64
 ---- batch: 060 ----
mean loss: 125.93
 ---- batch: 070 ----
mean loss: 127.61
 ---- batch: 080 ----
mean loss: 137.05
 ---- batch: 090 ----
mean loss: 133.22
train mean loss: 126.17
epoch train time: 0:00:00.507325
elapsed time: 0:02:25.652259
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:24:57.166960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.61
 ---- batch: 020 ----
mean loss: 132.50
 ---- batch: 030 ----
mean loss: 128.07
 ---- batch: 040 ----
mean loss: 129.66
 ---- batch: 050 ----
mean loss: 125.63
 ---- batch: 060 ----
mean loss: 122.13
 ---- batch: 070 ----
mean loss: 123.55
 ---- batch: 080 ----
mean loss: 124.45
 ---- batch: 090 ----
mean loss: 126.74
train mean loss: 126.22
epoch train time: 0:00:00.507578
elapsed time: 0:02:26.159987
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:24:57.674685
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.29
 ---- batch: 020 ----
mean loss: 128.92
 ---- batch: 030 ----
mean loss: 127.48
 ---- batch: 040 ----
mean loss: 125.70
 ---- batch: 050 ----
mean loss: 123.26
 ---- batch: 060 ----
mean loss: 128.79
 ---- batch: 070 ----
mean loss: 119.38
 ---- batch: 080 ----
mean loss: 124.41
 ---- batch: 090 ----
mean loss: 132.11
train mean loss: 126.42
epoch train time: 0:00:00.517490
elapsed time: 0:02:26.677629
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:24:58.192329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.85
 ---- batch: 020 ----
mean loss: 134.18
 ---- batch: 030 ----
mean loss: 126.72
 ---- batch: 040 ----
mean loss: 130.95
 ---- batch: 050 ----
mean loss: 130.24
 ---- batch: 060 ----
mean loss: 125.37
 ---- batch: 070 ----
mean loss: 125.82
 ---- batch: 080 ----
mean loss: 127.52
 ---- batch: 090 ----
mean loss: 117.99
train mean loss: 126.13
epoch train time: 0:00:00.511175
elapsed time: 0:02:27.188966
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:24:58.703666
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.42
 ---- batch: 020 ----
mean loss: 122.96
 ---- batch: 030 ----
mean loss: 128.63
 ---- batch: 040 ----
mean loss: 126.98
 ---- batch: 050 ----
mean loss: 125.53
 ---- batch: 060 ----
mean loss: 128.08
 ---- batch: 070 ----
mean loss: 126.69
 ---- batch: 080 ----
mean loss: 124.84
 ---- batch: 090 ----
mean loss: 127.49
train mean loss: 126.06
epoch train time: 0:00:00.509407
elapsed time: 0:02:27.698522
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:24:59.213238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.84
 ---- batch: 020 ----
mean loss: 126.98
 ---- batch: 030 ----
mean loss: 127.62
 ---- batch: 040 ----
mean loss: 126.67
 ---- batch: 050 ----
mean loss: 128.40
 ---- batch: 060 ----
mean loss: 127.24
 ---- batch: 070 ----
mean loss: 124.90
 ---- batch: 080 ----
mean loss: 118.76
 ---- batch: 090 ----
mean loss: 131.20
train mean loss: 126.08
epoch train time: 0:00:00.501261
elapsed time: 0:02:28.199952
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:24:59.714650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.83
 ---- batch: 020 ----
mean loss: 124.48
 ---- batch: 030 ----
mean loss: 125.12
 ---- batch: 040 ----
mean loss: 122.99
 ---- batch: 050 ----
mean loss: 124.16
 ---- batch: 060 ----
mean loss: 126.19
 ---- batch: 070 ----
mean loss: 131.02
 ---- batch: 080 ----
mean loss: 129.62
 ---- batch: 090 ----
mean loss: 123.84
train mean loss: 126.13
epoch train time: 0:00:00.498218
elapsed time: 0:02:28.698318
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:25:00.213023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.03
 ---- batch: 020 ----
mean loss: 125.58
 ---- batch: 030 ----
mean loss: 127.72
 ---- batch: 040 ----
mean loss: 129.49
 ---- batch: 050 ----
mean loss: 122.58
 ---- batch: 060 ----
mean loss: 120.66
 ---- batch: 070 ----
mean loss: 128.18
 ---- batch: 080 ----
mean loss: 128.12
 ---- batch: 090 ----
mean loss: 124.27
train mean loss: 125.85
epoch train time: 0:00:00.497324
elapsed time: 0:02:29.195817
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:25:00.710516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.44
 ---- batch: 020 ----
mean loss: 120.70
 ---- batch: 030 ----
mean loss: 122.34
 ---- batch: 040 ----
mean loss: 125.80
 ---- batch: 050 ----
mean loss: 124.14
 ---- batch: 060 ----
mean loss: 129.30
 ---- batch: 070 ----
mean loss: 131.09
 ---- batch: 080 ----
mean loss: 127.20
 ---- batch: 090 ----
mean loss: 123.03
train mean loss: 125.67
epoch train time: 0:00:00.505352
elapsed time: 0:02:29.701333
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:25:01.216034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.21
 ---- batch: 020 ----
mean loss: 128.98
 ---- batch: 030 ----
mean loss: 125.68
 ---- batch: 040 ----
mean loss: 126.54
 ---- batch: 050 ----
mean loss: 125.36
 ---- batch: 060 ----
mean loss: 121.79
 ---- batch: 070 ----
mean loss: 120.70
 ---- batch: 080 ----
mean loss: 129.58
 ---- batch: 090 ----
mean loss: 125.13
train mean loss: 126.08
epoch train time: 0:00:00.501875
elapsed time: 0:02:30.203385
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:25:01.718070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.43
 ---- batch: 020 ----
mean loss: 130.63
 ---- batch: 030 ----
mean loss: 122.15
 ---- batch: 040 ----
mean loss: 126.00
 ---- batch: 050 ----
mean loss: 128.27
 ---- batch: 060 ----
mean loss: 126.55
 ---- batch: 070 ----
mean loss: 128.28
 ---- batch: 080 ----
mean loss: 125.04
 ---- batch: 090 ----
mean loss: 126.72
train mean loss: 125.82
epoch train time: 0:00:00.502636
elapsed time: 0:02:30.706158
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:25:02.220859
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.39
 ---- batch: 020 ----
mean loss: 127.89
 ---- batch: 030 ----
mean loss: 120.86
 ---- batch: 040 ----
mean loss: 120.34
 ---- batch: 050 ----
mean loss: 127.76
 ---- batch: 060 ----
mean loss: 132.73
 ---- batch: 070 ----
mean loss: 127.33
 ---- batch: 080 ----
mean loss: 127.73
 ---- batch: 090 ----
mean loss: 122.95
train mean loss: 125.74
epoch train time: 0:00:00.501400
elapsed time: 0:02:31.207710
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:25:02.722425
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.53
 ---- batch: 020 ----
mean loss: 124.62
 ---- batch: 030 ----
mean loss: 129.54
 ---- batch: 040 ----
mean loss: 128.23
 ---- batch: 050 ----
mean loss: 129.57
 ---- batch: 060 ----
mean loss: 119.56
 ---- batch: 070 ----
mean loss: 122.52
 ---- batch: 080 ----
mean loss: 124.05
 ---- batch: 090 ----
mean loss: 121.16
train mean loss: 125.66
epoch train time: 0:00:00.517685
elapsed time: 0:02:31.725560
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:25:03.240260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.67
 ---- batch: 020 ----
mean loss: 120.71
 ---- batch: 030 ----
mean loss: 132.66
 ---- batch: 040 ----
mean loss: 121.85
 ---- batch: 050 ----
mean loss: 126.24
 ---- batch: 060 ----
mean loss: 124.76
 ---- batch: 070 ----
mean loss: 128.42
 ---- batch: 080 ----
mean loss: 129.39
 ---- batch: 090 ----
mean loss: 126.80
train mean loss: 125.87
epoch train time: 0:00:00.502703
elapsed time: 0:02:32.228418
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:25:03.743115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.87
 ---- batch: 020 ----
mean loss: 124.33
 ---- batch: 030 ----
mean loss: 125.70
 ---- batch: 040 ----
mean loss: 123.82
 ---- batch: 050 ----
mean loss: 128.15
 ---- batch: 060 ----
mean loss: 124.61
 ---- batch: 070 ----
mean loss: 126.36
 ---- batch: 080 ----
mean loss: 126.08
 ---- batch: 090 ----
mean loss: 128.88
train mean loss: 125.75
epoch train time: 0:00:00.500844
elapsed time: 0:02:32.729408
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:25:04.244105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.57
 ---- batch: 020 ----
mean loss: 129.01
 ---- batch: 030 ----
mean loss: 126.55
 ---- batch: 040 ----
mean loss: 119.17
 ---- batch: 050 ----
mean loss: 128.98
 ---- batch: 060 ----
mean loss: 124.59
 ---- batch: 070 ----
mean loss: 122.58
 ---- batch: 080 ----
mean loss: 130.27
 ---- batch: 090 ----
mean loss: 127.80
train mean loss: 125.97
epoch train time: 0:00:00.505133
elapsed time: 0:02:33.234687
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:25:04.749384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.18
 ---- batch: 020 ----
mean loss: 131.01
 ---- batch: 030 ----
mean loss: 125.69
 ---- batch: 040 ----
mean loss: 116.90
 ---- batch: 050 ----
mean loss: 125.35
 ---- batch: 060 ----
mean loss: 122.59
 ---- batch: 070 ----
mean loss: 124.48
 ---- batch: 080 ----
mean loss: 128.24
 ---- batch: 090 ----
mean loss: 122.65
train mean loss: 125.74
epoch train time: 0:00:00.506595
elapsed time: 0:02:33.741445
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:25:05.256144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.40
 ---- batch: 020 ----
mean loss: 122.58
 ---- batch: 030 ----
mean loss: 130.86
 ---- batch: 040 ----
mean loss: 125.98
 ---- batch: 050 ----
mean loss: 125.32
 ---- batch: 060 ----
mean loss: 121.84
 ---- batch: 070 ----
mean loss: 129.00
 ---- batch: 080 ----
mean loss: 121.78
 ---- batch: 090 ----
mean loss: 124.03
train mean loss: 125.79
epoch train time: 0:00:00.505149
elapsed time: 0:02:34.246745
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:25:05.761446
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.17
 ---- batch: 020 ----
mean loss: 123.56
 ---- batch: 030 ----
mean loss: 126.74
 ---- batch: 040 ----
mean loss: 124.03
 ---- batch: 050 ----
mean loss: 120.09
 ---- batch: 060 ----
mean loss: 124.07
 ---- batch: 070 ----
mean loss: 124.89
 ---- batch: 080 ----
mean loss: 125.12
 ---- batch: 090 ----
mean loss: 130.06
train mean loss: 125.79
epoch train time: 0:00:00.500921
elapsed time: 0:02:34.747818
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:25:06.262525
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.55
 ---- batch: 020 ----
mean loss: 126.29
 ---- batch: 030 ----
mean loss: 124.97
 ---- batch: 040 ----
mean loss: 125.59
 ---- batch: 050 ----
mean loss: 120.09
 ---- batch: 060 ----
mean loss: 125.71
 ---- batch: 070 ----
mean loss: 130.97
 ---- batch: 080 ----
mean loss: 118.97
 ---- batch: 090 ----
mean loss: 130.09
train mean loss: 125.77
epoch train time: 0:00:00.492803
elapsed time: 0:02:35.240782
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:25:06.755514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.65
 ---- batch: 020 ----
mean loss: 134.95
 ---- batch: 030 ----
mean loss: 120.21
 ---- batch: 040 ----
mean loss: 125.02
 ---- batch: 050 ----
mean loss: 120.52
 ---- batch: 060 ----
mean loss: 125.98
 ---- batch: 070 ----
mean loss: 126.73
 ---- batch: 080 ----
mean loss: 120.32
 ---- batch: 090 ----
mean loss: 130.71
train mean loss: 125.61
epoch train time: 0:00:00.503486
elapsed time: 0:02:35.744459
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:25:07.259157
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.65
 ---- batch: 020 ----
mean loss: 128.29
 ---- batch: 030 ----
mean loss: 123.12
 ---- batch: 040 ----
mean loss: 124.31
 ---- batch: 050 ----
mean loss: 130.35
 ---- batch: 060 ----
mean loss: 123.14
 ---- batch: 070 ----
mean loss: 123.69
 ---- batch: 080 ----
mean loss: 127.31
 ---- batch: 090 ----
mean loss: 130.12
train mean loss: 125.61
epoch train time: 0:00:00.496065
elapsed time: 0:02:36.240693
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:25:07.755392
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.10
 ---- batch: 020 ----
mean loss: 129.29
 ---- batch: 030 ----
mean loss: 118.16
 ---- batch: 040 ----
mean loss: 127.30
 ---- batch: 050 ----
mean loss: 125.19
 ---- batch: 060 ----
mean loss: 125.74
 ---- batch: 070 ----
mean loss: 123.80
 ---- batch: 080 ----
mean loss: 121.64
 ---- batch: 090 ----
mean loss: 131.71
train mean loss: 125.44
epoch train time: 0:00:00.505233
elapsed time: 0:02:36.746077
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:25:08.260781
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.08
 ---- batch: 020 ----
mean loss: 125.99
 ---- batch: 030 ----
mean loss: 124.86
 ---- batch: 040 ----
mean loss: 130.37
 ---- batch: 050 ----
mean loss: 127.61
 ---- batch: 060 ----
mean loss: 125.03
 ---- batch: 070 ----
mean loss: 125.72
 ---- batch: 080 ----
mean loss: 126.53
 ---- batch: 090 ----
mean loss: 123.56
train mean loss: 125.61
epoch train time: 0:00:00.496813
elapsed time: 0:02:37.243075
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:25:08.757775
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.24
 ---- batch: 020 ----
mean loss: 120.48
 ---- batch: 030 ----
mean loss: 124.04
 ---- batch: 040 ----
mean loss: 123.91
 ---- batch: 050 ----
mean loss: 129.83
 ---- batch: 060 ----
mean loss: 127.04
 ---- batch: 070 ----
mean loss: 131.96
 ---- batch: 080 ----
mean loss: 126.35
 ---- batch: 090 ----
mean loss: 122.60
train mean loss: 125.48
epoch train time: 0:00:00.498476
elapsed time: 0:02:37.741704
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:25:09.256404
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.06
 ---- batch: 020 ----
mean loss: 126.25
 ---- batch: 030 ----
mean loss: 118.57
 ---- batch: 040 ----
mean loss: 117.09
 ---- batch: 050 ----
mean loss: 126.06
 ---- batch: 060 ----
mean loss: 124.83
 ---- batch: 070 ----
mean loss: 128.91
 ---- batch: 080 ----
mean loss: 130.45
 ---- batch: 090 ----
mean loss: 132.76
train mean loss: 125.30
epoch train time: 0:00:00.491164
elapsed time: 0:02:38.236464
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_7/checkpoint.pth.tar
**** end time: 2019-09-25 23:25:09.751128 ****
