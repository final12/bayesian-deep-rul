Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 15163
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 23:30:15.253225 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 23:30:15.263005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4640.70
 ---- batch: 020 ----
mean loss: 4464.80
 ---- batch: 030 ----
mean loss: 4269.65
 ---- batch: 040 ----
mean loss: 4042.28
 ---- batch: 050 ----
mean loss: 3861.93
 ---- batch: 060 ----
mean loss: 3666.11
 ---- batch: 070 ----
mean loss: 3530.87
 ---- batch: 080 ----
mean loss: 3369.52
 ---- batch: 090 ----
mean loss: 3231.75
 ---- batch: 100 ----
mean loss: 3147.91
 ---- batch: 110 ----
mean loss: 3069.08
train mean loss: 3733.08
epoch train time: 0:00:34.094506
elapsed time: 0:00:34.110569
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 23:30:49.363843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2916.51
 ---- batch: 020 ----
mean loss: 2820.14
 ---- batch: 030 ----
mean loss: 2746.97
 ---- batch: 040 ----
mean loss: 2695.28
 ---- batch: 050 ----
mean loss: 2645.96
 ---- batch: 060 ----
mean loss: 2565.08
 ---- batch: 070 ----
mean loss: 2503.15
 ---- batch: 080 ----
mean loss: 2475.90
 ---- batch: 090 ----
mean loss: 2425.47
 ---- batch: 100 ----
mean loss: 2339.23
 ---- batch: 110 ----
mean loss: 2297.00
train mean loss: 2578.35
epoch train time: 0:00:01.830637
elapsed time: 0:00:35.941430
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 23:30:51.195011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2286.18
 ---- batch: 020 ----
mean loss: 2240.17
 ---- batch: 030 ----
mean loss: 2225.53
 ---- batch: 040 ----
mean loss: 2182.84
 ---- batch: 050 ----
mean loss: 2143.02
 ---- batch: 060 ----
mean loss: 2104.87
 ---- batch: 070 ----
mean loss: 2096.18
 ---- batch: 080 ----
mean loss: 2040.66
 ---- batch: 090 ----
mean loss: 2001.48
 ---- batch: 100 ----
mean loss: 1998.63
 ---- batch: 110 ----
mean loss: 1916.48
train mean loss: 2109.32
epoch train time: 0:00:01.869637
elapsed time: 0:00:37.811655
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 23:30:53.065256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1929.13
 ---- batch: 020 ----
mean loss: 1865.63
 ---- batch: 030 ----
mean loss: 1900.63
 ---- batch: 040 ----
mean loss: 1864.83
 ---- batch: 050 ----
mean loss: 1805.95
 ---- batch: 060 ----
mean loss: 1822.05
 ---- batch: 070 ----
mean loss: 1787.61
 ---- batch: 080 ----
mean loss: 1775.12
 ---- batch: 090 ----
mean loss: 1732.56
 ---- batch: 100 ----
mean loss: 1722.55
 ---- batch: 110 ----
mean loss: 1701.41
train mean loss: 1805.88
epoch train time: 0:00:01.885376
elapsed time: 0:00:39.697628
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 23:30:54.951196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1694.21
 ---- batch: 020 ----
mean loss: 1650.87
 ---- batch: 030 ----
mean loss: 1629.02
 ---- batch: 040 ----
mean loss: 1597.34
 ---- batch: 050 ----
mean loss: 1585.63
 ---- batch: 060 ----
mean loss: 1563.49
 ---- batch: 070 ----
mean loss: 1524.35
 ---- batch: 080 ----
mean loss: 1511.17
 ---- batch: 090 ----
mean loss: 1504.61
 ---- batch: 100 ----
mean loss: 1485.68
 ---- batch: 110 ----
mean loss: 1491.37
train mean loss: 1564.05
epoch train time: 0:00:01.898023
elapsed time: 0:00:41.596194
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 23:30:56.849810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1472.02
 ---- batch: 020 ----
mean loss: 1443.71
 ---- batch: 030 ----
mean loss: 1418.39
 ---- batch: 040 ----
mean loss: 1411.21
 ---- batch: 050 ----
mean loss: 1382.87
 ---- batch: 060 ----
mean loss: 1368.32
 ---- batch: 070 ----
mean loss: 1345.34
 ---- batch: 080 ----
mean loss: 1331.04
 ---- batch: 090 ----
mean loss: 1325.20
 ---- batch: 100 ----
mean loss: 1329.90
 ---- batch: 110 ----
mean loss: 1307.21
train mean loss: 1373.67
epoch train time: 0:00:01.873921
elapsed time: 0:00:43.470726
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 23:30:58.724327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1299.42
 ---- batch: 020 ----
mean loss: 1267.75
 ---- batch: 030 ----
mean loss: 1248.66
 ---- batch: 040 ----
mean loss: 1252.70
 ---- batch: 050 ----
mean loss: 1254.27
 ---- batch: 060 ----
mean loss: 1209.95
 ---- batch: 070 ----
mean loss: 1199.38
 ---- batch: 080 ----
mean loss: 1201.30
 ---- batch: 090 ----
mean loss: 1192.46
 ---- batch: 100 ----
mean loss: 1179.34
 ---- batch: 110 ----
mean loss: 1185.52
train mean loss: 1225.97
epoch train time: 0:00:01.887539
elapsed time: 0:00:45.358860
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 23:31:00.612482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1155.59
 ---- batch: 020 ----
mean loss: 1162.66
 ---- batch: 030 ----
mean loss: 1143.31
 ---- batch: 040 ----
mean loss: 1116.90
 ---- batch: 050 ----
mean loss: 1128.94
 ---- batch: 060 ----
mean loss: 1114.18
 ---- batch: 070 ----
mean loss: 1101.27
 ---- batch: 080 ----
mean loss: 1105.80
 ---- batch: 090 ----
mean loss: 1116.50
 ---- batch: 100 ----
mean loss: 1106.17
 ---- batch: 110 ----
mean loss: 1061.67
train mean loss: 1118.17
epoch train time: 0:00:01.865333
elapsed time: 0:00:47.224792
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 23:31:02.478421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1062.25
 ---- batch: 020 ----
mean loss: 1063.71
 ---- batch: 030 ----
mean loss: 1051.30
 ---- batch: 040 ----
mean loss: 1027.06
 ---- batch: 050 ----
mean loss: 1040.85
 ---- batch: 060 ----
mean loss: 1041.41
 ---- batch: 070 ----
mean loss: 1030.20
 ---- batch: 080 ----
mean loss: 1021.07
 ---- batch: 090 ----
mean loss: 1010.17
 ---- batch: 100 ----
mean loss: 1023.99
 ---- batch: 110 ----
mean loss: 1021.89
train mean loss: 1034.42
epoch train time: 0:00:01.874679
elapsed time: 0:00:49.100054
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 23:31:04.353664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 988.84
 ---- batch: 020 ----
mean loss: 998.03
 ---- batch: 030 ----
mean loss: 968.48
 ---- batch: 040 ----
mean loss: 984.81
 ---- batch: 050 ----
mean loss: 978.48
 ---- batch: 060 ----
mean loss: 982.13
 ---- batch: 070 ----
mean loss: 973.73
 ---- batch: 080 ----
mean loss: 987.03
 ---- batch: 090 ----
mean loss: 962.18
 ---- batch: 100 ----
mean loss: 954.67
 ---- batch: 110 ----
mean loss: 973.63
train mean loss: 976.87
epoch train time: 0:00:01.867811
elapsed time: 0:00:50.968455
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 23:31:06.222051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 952.37
 ---- batch: 020 ----
mean loss: 932.01
 ---- batch: 030 ----
mean loss: 949.12
 ---- batch: 040 ----
mean loss: 960.55
 ---- batch: 050 ----
mean loss: 938.87
 ---- batch: 060 ----
mean loss: 945.46
 ---- batch: 070 ----
mean loss: 930.35
 ---- batch: 080 ----
mean loss: 929.44
 ---- batch: 090 ----
mean loss: 942.25
 ---- batch: 100 ----
mean loss: 930.79
 ---- batch: 110 ----
mean loss: 933.34
train mean loss: 939.57
epoch train time: 0:00:01.916691
elapsed time: 0:00:52.885702
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 23:31:08.139095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.21
 ---- batch: 020 ----
mean loss: 910.98
 ---- batch: 030 ----
mean loss: 924.66
 ---- batch: 040 ----
mean loss: 913.07
 ---- batch: 050 ----
mean loss: 911.96
 ---- batch: 060 ----
mean loss: 906.81
 ---- batch: 070 ----
mean loss: 906.89
 ---- batch: 080 ----
mean loss: 903.81
 ---- batch: 090 ----
mean loss: 911.31
 ---- batch: 100 ----
mean loss: 901.80
 ---- batch: 110 ----
mean loss: 892.15
train mean loss: 911.67
epoch train time: 0:00:01.878415
elapsed time: 0:00:54.764494
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 23:31:10.018165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.07
 ---- batch: 020 ----
mean loss: 906.16
 ---- batch: 030 ----
mean loss: 904.30
 ---- batch: 040 ----
mean loss: 889.20
 ---- batch: 050 ----
mean loss: 894.86
 ---- batch: 060 ----
mean loss: 887.71
 ---- batch: 070 ----
mean loss: 905.69
 ---- batch: 080 ----
mean loss: 877.00
 ---- batch: 090 ----
mean loss: 888.61
 ---- batch: 100 ----
mean loss: 897.32
 ---- batch: 110 ----
mean loss: 874.64
train mean loss: 895.18
epoch train time: 0:00:01.864421
elapsed time: 0:00:56.629531
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 23:31:11.883106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.05
 ---- batch: 020 ----
mean loss: 881.29
 ---- batch: 030 ----
mean loss: 877.72
 ---- batch: 040 ----
mean loss: 872.06
 ---- batch: 050 ----
mean loss: 870.49
 ---- batch: 060 ----
mean loss: 900.71
 ---- batch: 070 ----
mean loss: 885.07
 ---- batch: 080 ----
mean loss: 881.57
 ---- batch: 090 ----
mean loss: 873.00
 ---- batch: 100 ----
mean loss: 889.68
 ---- batch: 110 ----
mean loss: 890.12
train mean loss: 882.32
epoch train time: 0:00:01.873259
elapsed time: 0:00:58.503356
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 23:31:13.756959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.82
 ---- batch: 020 ----
mean loss: 878.12
 ---- batch: 030 ----
mean loss: 889.94
 ---- batch: 040 ----
mean loss: 888.10
 ---- batch: 050 ----
mean loss: 877.97
 ---- batch: 060 ----
mean loss: 863.20
 ---- batch: 070 ----
mean loss: 866.86
 ---- batch: 080 ----
mean loss: 864.26
 ---- batch: 090 ----
mean loss: 868.64
 ---- batch: 100 ----
mean loss: 867.77
 ---- batch: 110 ----
mean loss: 892.70
train mean loss: 874.82
epoch train time: 0:00:01.894582
elapsed time: 0:01:00.398533
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 23:31:15.652155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.43
 ---- batch: 020 ----
mean loss: 877.63
 ---- batch: 030 ----
mean loss: 871.55
 ---- batch: 040 ----
mean loss: 859.30
 ---- batch: 050 ----
mean loss: 862.58
 ---- batch: 060 ----
mean loss: 880.44
 ---- batch: 070 ----
mean loss: 878.86
 ---- batch: 080 ----
mean loss: 846.78
 ---- batch: 090 ----
mean loss: 864.71
 ---- batch: 100 ----
mean loss: 880.27
 ---- batch: 110 ----
mean loss: 863.89
train mean loss: 870.20
epoch train time: 0:00:01.882618
elapsed time: 0:01:02.281764
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 23:31:17.535429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.01
 ---- batch: 020 ----
mean loss: 848.28
 ---- batch: 030 ----
mean loss: 859.44
 ---- batch: 040 ----
mean loss: 883.48
 ---- batch: 050 ----
mean loss: 887.74
 ---- batch: 060 ----
mean loss: 880.91
 ---- batch: 070 ----
mean loss: 883.09
 ---- batch: 080 ----
mean loss: 861.82
 ---- batch: 090 ----
mean loss: 852.22
 ---- batch: 100 ----
mean loss: 857.84
 ---- batch: 110 ----
mean loss: 861.84
train mean loss: 867.00
epoch train time: 0:00:01.869165
elapsed time: 0:01:04.151578
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 23:31:19.405167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.01
 ---- batch: 020 ----
mean loss: 871.82
 ---- batch: 030 ----
mean loss: 854.08
 ---- batch: 040 ----
mean loss: 861.64
 ---- batch: 050 ----
mean loss: 879.77
 ---- batch: 060 ----
mean loss: 847.64
 ---- batch: 070 ----
mean loss: 877.26
 ---- batch: 080 ----
mean loss: 867.54
 ---- batch: 090 ----
mean loss: 856.19
 ---- batch: 100 ----
mean loss: 882.60
 ---- batch: 110 ----
mean loss: 882.05
train mean loss: 865.46
epoch train time: 0:00:01.874104
elapsed time: 0:01:06.026229
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 23:31:21.279802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.96
 ---- batch: 020 ----
mean loss: 872.60
 ---- batch: 030 ----
mean loss: 861.85
 ---- batch: 040 ----
mean loss: 840.48
 ---- batch: 050 ----
mean loss: 858.17
 ---- batch: 060 ----
mean loss: 872.90
 ---- batch: 070 ----
mean loss: 857.51
 ---- batch: 080 ----
mean loss: 865.20
 ---- batch: 090 ----
mean loss: 868.90
 ---- batch: 100 ----
mean loss: 854.16
 ---- batch: 110 ----
mean loss: 881.12
train mean loss: 862.70
epoch train time: 0:00:01.858577
elapsed time: 0:01:07.885319
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 23:31:23.138882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.26
 ---- batch: 020 ----
mean loss: 872.03
 ---- batch: 030 ----
mean loss: 872.19
 ---- batch: 040 ----
mean loss: 859.30
 ---- batch: 050 ----
mean loss: 841.25
 ---- batch: 060 ----
mean loss: 866.12
 ---- batch: 070 ----
mean loss: 857.34
 ---- batch: 080 ----
mean loss: 876.09
 ---- batch: 090 ----
mean loss: 853.91
 ---- batch: 100 ----
mean loss: 867.46
 ---- batch: 110 ----
mean loss: 866.59
train mean loss: 861.69
epoch train time: 0:00:01.853429
elapsed time: 0:01:09.739285
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 23:31:24.992884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 822.13
 ---- batch: 020 ----
mean loss: 894.50
 ---- batch: 030 ----
mean loss: 849.87
 ---- batch: 040 ----
mean loss: 888.96
 ---- batch: 050 ----
mean loss: 861.44
 ---- batch: 060 ----
mean loss: 875.17
 ---- batch: 070 ----
mean loss: 854.76
 ---- batch: 080 ----
mean loss: 872.71
 ---- batch: 090 ----
mean loss: 843.08
 ---- batch: 100 ----
mean loss: 843.25
 ---- batch: 110 ----
mean loss: 841.58
train mean loss: 858.29
epoch train time: 0:00:01.860209
elapsed time: 0:01:11.600142
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 23:31:26.853600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.32
 ---- batch: 020 ----
mean loss: 871.59
 ---- batch: 030 ----
mean loss: 845.01
 ---- batch: 040 ----
mean loss: 872.68
 ---- batch: 050 ----
mean loss: 868.01
 ---- batch: 060 ----
mean loss: 852.66
 ---- batch: 070 ----
mean loss: 866.75
 ---- batch: 080 ----
mean loss: 856.55
 ---- batch: 090 ----
mean loss: 846.77
 ---- batch: 100 ----
mean loss: 846.82
 ---- batch: 110 ----
mean loss: 871.37
train mean loss: 859.24
epoch train time: 0:00:01.881065
elapsed time: 0:01:13.481668
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 23:31:28.735284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.86
 ---- batch: 020 ----
mean loss: 850.00
 ---- batch: 030 ----
mean loss: 827.99
 ---- batch: 040 ----
mean loss: 855.70
 ---- batch: 050 ----
mean loss: 883.46
 ---- batch: 060 ----
mean loss: 850.40
 ---- batch: 070 ----
mean loss: 877.42
 ---- batch: 080 ----
mean loss: 848.25
 ---- batch: 090 ----
mean loss: 860.86
 ---- batch: 100 ----
mean loss: 878.53
 ---- batch: 110 ----
mean loss: 850.90
train mean loss: 857.39
epoch train time: 0:00:01.884828
elapsed time: 0:01:15.367068
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 23:31:30.620662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.70
 ---- batch: 020 ----
mean loss: 856.94
 ---- batch: 030 ----
mean loss: 864.63
 ---- batch: 040 ----
mean loss: 857.71
 ---- batch: 050 ----
mean loss: 876.20
 ---- batch: 060 ----
mean loss: 849.42
 ---- batch: 070 ----
mean loss: 847.68
 ---- batch: 080 ----
mean loss: 871.23
 ---- batch: 090 ----
mean loss: 860.40
 ---- batch: 100 ----
mean loss: 863.02
 ---- batch: 110 ----
mean loss: 836.34
train mean loss: 857.21
epoch train time: 0:00:01.876047
elapsed time: 0:01:17.243690
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 23:31:32.497268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.45
 ---- batch: 020 ----
mean loss: 851.81
 ---- batch: 030 ----
mean loss: 835.20
 ---- batch: 040 ----
mean loss: 866.87
 ---- batch: 050 ----
mean loss: 858.41
 ---- batch: 060 ----
mean loss: 868.46
 ---- batch: 070 ----
mean loss: 832.68
 ---- batch: 080 ----
mean loss: 859.01
 ---- batch: 090 ----
mean loss: 863.93
 ---- batch: 100 ----
mean loss: 849.52
 ---- batch: 110 ----
mean loss: 863.92
train mean loss: 855.53
epoch train time: 0:00:01.877164
elapsed time: 0:01:19.121384
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 23:31:34.375025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.93
 ---- batch: 020 ----
mean loss: 856.61
 ---- batch: 030 ----
mean loss: 859.12
 ---- batch: 040 ----
mean loss: 851.92
 ---- batch: 050 ----
mean loss: 846.89
 ---- batch: 060 ----
mean loss: 863.19
 ---- batch: 070 ----
mean loss: 879.95
 ---- batch: 080 ----
mean loss: 840.13
 ---- batch: 090 ----
mean loss: 857.05
 ---- batch: 100 ----
mean loss: 849.07
 ---- batch: 110 ----
mean loss: 850.85
train mean loss: 855.44
epoch train time: 0:00:01.913694
elapsed time: 0:01:21.035682
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 23:31:36.289319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.87
 ---- batch: 020 ----
mean loss: 860.43
 ---- batch: 030 ----
mean loss: 868.79
 ---- batch: 040 ----
mean loss: 856.33
 ---- batch: 050 ----
mean loss: 850.23
 ---- batch: 060 ----
mean loss: 834.28
 ---- batch: 070 ----
mean loss: 838.19
 ---- batch: 080 ----
mean loss: 866.32
 ---- batch: 090 ----
mean loss: 876.74
 ---- batch: 100 ----
mean loss: 846.71
 ---- batch: 110 ----
mean loss: 845.22
train mean loss: 853.87
epoch train time: 0:00:01.925455
elapsed time: 0:01:22.961755
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 23:31:38.215395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.84
 ---- batch: 020 ----
mean loss: 847.27
 ---- batch: 030 ----
mean loss: 864.87
 ---- batch: 040 ----
mean loss: 872.17
 ---- batch: 050 ----
mean loss: 848.54
 ---- batch: 060 ----
mean loss: 846.69
 ---- batch: 070 ----
mean loss: 841.42
 ---- batch: 080 ----
mean loss: 860.16
 ---- batch: 090 ----
mean loss: 862.05
 ---- batch: 100 ----
mean loss: 845.68
 ---- batch: 110 ----
mean loss: 853.26
train mean loss: 853.46
epoch train time: 0:00:01.908822
elapsed time: 0:01:24.871186
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 23:31:40.124775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.21
 ---- batch: 020 ----
mean loss: 847.46
 ---- batch: 030 ----
mean loss: 864.72
 ---- batch: 040 ----
mean loss: 874.25
 ---- batch: 050 ----
mean loss: 868.23
 ---- batch: 060 ----
mean loss: 846.64
 ---- batch: 070 ----
mean loss: 860.79
 ---- batch: 080 ----
mean loss: 852.18
 ---- batch: 090 ----
mean loss: 833.53
 ---- batch: 100 ----
mean loss: 863.06
 ---- batch: 110 ----
mean loss: 844.95
train mean loss: 852.89
epoch train time: 0:00:01.982840
elapsed time: 0:01:26.854597
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 23:31:42.108227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.40
 ---- batch: 020 ----
mean loss: 831.17
 ---- batch: 030 ----
mean loss: 846.78
 ---- batch: 040 ----
mean loss: 852.33
 ---- batch: 050 ----
mean loss: 852.97
 ---- batch: 060 ----
mean loss: 851.30
 ---- batch: 070 ----
mean loss: 852.27
 ---- batch: 080 ----
mean loss: 870.44
 ---- batch: 090 ----
mean loss: 851.82
 ---- batch: 100 ----
mean loss: 858.01
 ---- batch: 110 ----
mean loss: 851.32
train mean loss: 852.40
epoch train time: 0:00:01.951907
elapsed time: 0:01:28.807104
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 23:31:44.060711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.35
 ---- batch: 020 ----
mean loss: 845.38
 ---- batch: 030 ----
mean loss: 856.52
 ---- batch: 040 ----
mean loss: 867.49
 ---- batch: 050 ----
mean loss: 856.98
 ---- batch: 060 ----
mean loss: 852.01
 ---- batch: 070 ----
mean loss: 815.08
 ---- batch: 080 ----
mean loss: 858.10
 ---- batch: 090 ----
mean loss: 852.85
 ---- batch: 100 ----
mean loss: 862.41
 ---- batch: 110 ----
mean loss: 864.40
train mean loss: 852.45
epoch train time: 0:00:01.930974
elapsed time: 0:01:30.738648
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 23:31:45.992221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.77
 ---- batch: 020 ----
mean loss: 838.63
 ---- batch: 030 ----
mean loss: 870.98
 ---- batch: 040 ----
mean loss: 874.39
 ---- batch: 050 ----
mean loss: 826.32
 ---- batch: 060 ----
mean loss: 838.29
 ---- batch: 070 ----
mean loss: 861.92
 ---- batch: 080 ----
mean loss: 855.76
 ---- batch: 090 ----
mean loss: 850.20
 ---- batch: 100 ----
mean loss: 851.12
 ---- batch: 110 ----
mean loss: 836.61
train mean loss: 850.96
epoch train time: 0:00:01.882399
elapsed time: 0:01:32.621590
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 23:31:47.875191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.67
 ---- batch: 020 ----
mean loss: 839.14
 ---- batch: 030 ----
mean loss: 862.15
 ---- batch: 040 ----
mean loss: 872.30
 ---- batch: 050 ----
mean loss: 859.74
 ---- batch: 060 ----
mean loss: 861.85
 ---- batch: 070 ----
mean loss: 832.42
 ---- batch: 080 ----
mean loss: 857.47
 ---- batch: 090 ----
mean loss: 845.63
 ---- batch: 100 ----
mean loss: 860.34
 ---- batch: 110 ----
mean loss: 840.32
train mean loss: 851.46
epoch train time: 0:00:01.895337
elapsed time: 0:01:34.517474
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 23:31:49.771074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.35
 ---- batch: 020 ----
mean loss: 853.53
 ---- batch: 030 ----
mean loss: 846.54
 ---- batch: 040 ----
mean loss: 850.49
 ---- batch: 050 ----
mean loss: 833.17
 ---- batch: 060 ----
mean loss: 836.46
 ---- batch: 070 ----
mean loss: 851.24
 ---- batch: 080 ----
mean loss: 865.57
 ---- batch: 090 ----
mean loss: 863.95
 ---- batch: 100 ----
mean loss: 856.94
 ---- batch: 110 ----
mean loss: 860.29
train mean loss: 850.09
epoch train time: 0:00:01.844838
elapsed time: 0:01:36.362890
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 23:31:51.616267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.95
 ---- batch: 020 ----
mean loss: 819.31
 ---- batch: 030 ----
mean loss: 843.98
 ---- batch: 040 ----
mean loss: 856.94
 ---- batch: 050 ----
mean loss: 855.45
 ---- batch: 060 ----
mean loss: 860.96
 ---- batch: 070 ----
mean loss: 841.92
 ---- batch: 080 ----
mean loss: 856.57
 ---- batch: 090 ----
mean loss: 861.50
 ---- batch: 100 ----
mean loss: 858.85
 ---- batch: 110 ----
mean loss: 846.55
train mean loss: 850.12
epoch train time: 0:00:01.878355
elapsed time: 0:01:38.241603
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 23:31:53.495228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.63
 ---- batch: 020 ----
mean loss: 868.78
 ---- batch: 030 ----
mean loss: 840.32
 ---- batch: 040 ----
mean loss: 847.25
 ---- batch: 050 ----
mean loss: 840.14
 ---- batch: 060 ----
mean loss: 841.63
 ---- batch: 070 ----
mean loss: 848.91
 ---- batch: 080 ----
mean loss: 856.42
 ---- batch: 090 ----
mean loss: 838.02
 ---- batch: 100 ----
mean loss: 839.45
 ---- batch: 110 ----
mean loss: 843.00
train mean loss: 849.44
epoch train time: 0:00:01.890261
elapsed time: 0:01:40.132538
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 23:31:55.386143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.78
 ---- batch: 020 ----
mean loss: 848.41
 ---- batch: 030 ----
mean loss: 857.01
 ---- batch: 040 ----
mean loss: 852.76
 ---- batch: 050 ----
mean loss: 831.47
 ---- batch: 060 ----
mean loss: 855.90
 ---- batch: 070 ----
mean loss: 840.60
 ---- batch: 080 ----
mean loss: 836.61
 ---- batch: 090 ----
mean loss: 870.06
 ---- batch: 100 ----
mean loss: 859.55
 ---- batch: 110 ----
mean loss: 833.36
train mean loss: 850.21
epoch train time: 0:00:01.905652
elapsed time: 0:01:42.038752
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 23:31:57.292336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.95
 ---- batch: 020 ----
mean loss: 860.96
 ---- batch: 030 ----
mean loss: 835.68
 ---- batch: 040 ----
mean loss: 845.17
 ---- batch: 050 ----
mean loss: 873.34
 ---- batch: 060 ----
mean loss: 838.13
 ---- batch: 070 ----
mean loss: 843.95
 ---- batch: 080 ----
mean loss: 835.02
 ---- batch: 090 ----
mean loss: 834.29
 ---- batch: 100 ----
mean loss: 857.85
 ---- batch: 110 ----
mean loss: 863.34
train mean loss: 849.52
epoch train time: 0:00:01.904026
elapsed time: 0:01:43.943323
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 23:31:59.196932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.81
 ---- batch: 020 ----
mean loss: 844.93
 ---- batch: 030 ----
mean loss: 862.22
 ---- batch: 040 ----
mean loss: 863.36
 ---- batch: 050 ----
mean loss: 824.09
 ---- batch: 060 ----
mean loss: 835.98
 ---- batch: 070 ----
mean loss: 863.69
 ---- batch: 080 ----
mean loss: 861.71
 ---- batch: 090 ----
mean loss: 830.56
 ---- batch: 100 ----
mean loss: 845.84
 ---- batch: 110 ----
mean loss: 851.58
train mean loss: 850.10
epoch train time: 0:00:01.866434
elapsed time: 0:01:45.810352
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 23:32:01.064002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.57
 ---- batch: 020 ----
mean loss: 852.04
 ---- batch: 030 ----
mean loss: 848.10
 ---- batch: 040 ----
mean loss: 852.42
 ---- batch: 050 ----
mean loss: 837.67
 ---- batch: 060 ----
mean loss: 839.05
 ---- batch: 070 ----
mean loss: 846.34
 ---- batch: 080 ----
mean loss: 856.83
 ---- batch: 090 ----
mean loss: 851.36
 ---- batch: 100 ----
mean loss: 855.38
 ---- batch: 110 ----
mean loss: 852.36
train mean loss: 849.17
epoch train time: 0:00:01.922132
elapsed time: 0:01:47.733116
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 23:32:02.986769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.52
 ---- batch: 020 ----
mean loss: 860.18
 ---- batch: 030 ----
mean loss: 837.90
 ---- batch: 040 ----
mean loss: 862.34
 ---- batch: 050 ----
mean loss: 863.49
 ---- batch: 060 ----
mean loss: 849.83
 ---- batch: 070 ----
mean loss: 837.90
 ---- batch: 080 ----
mean loss: 853.73
 ---- batch: 090 ----
mean loss: 840.60
 ---- batch: 100 ----
mean loss: 843.53
 ---- batch: 110 ----
mean loss: 849.48
train mean loss: 849.29
epoch train time: 0:00:01.886668
elapsed time: 0:01:49.620447
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 23:32:04.874081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.54
 ---- batch: 020 ----
mean loss: 831.11
 ---- batch: 030 ----
mean loss: 845.33
 ---- batch: 040 ----
mean loss: 852.79
 ---- batch: 050 ----
mean loss: 839.12
 ---- batch: 060 ----
mean loss: 854.46
 ---- batch: 070 ----
mean loss: 846.34
 ---- batch: 080 ----
mean loss: 870.96
 ---- batch: 090 ----
mean loss: 858.65
 ---- batch: 100 ----
mean loss: 850.15
 ---- batch: 110 ----
mean loss: 829.20
train mean loss: 848.72
epoch train time: 0:00:01.842259
elapsed time: 0:01:51.463352
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 23:32:06.717042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.17
 ---- batch: 020 ----
mean loss: 838.23
 ---- batch: 030 ----
mean loss: 850.76
 ---- batch: 040 ----
mean loss: 839.38
 ---- batch: 050 ----
mean loss: 870.84
 ---- batch: 060 ----
mean loss: 837.34
 ---- batch: 070 ----
mean loss: 847.39
 ---- batch: 080 ----
mean loss: 859.97
 ---- batch: 090 ----
mean loss: 844.92
 ---- batch: 100 ----
mean loss: 846.21
 ---- batch: 110 ----
mean loss: 844.30
train mean loss: 847.61
epoch train time: 0:00:01.854693
elapsed time: 0:01:53.318804
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 23:32:08.572490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.40
 ---- batch: 020 ----
mean loss: 852.70
 ---- batch: 030 ----
mean loss: 859.51
 ---- batch: 040 ----
mean loss: 841.07
 ---- batch: 050 ----
mean loss: 849.28
 ---- batch: 060 ----
mean loss: 846.78
 ---- batch: 070 ----
mean loss: 813.73
 ---- batch: 080 ----
mean loss: 859.79
 ---- batch: 090 ----
mean loss: 847.91
 ---- batch: 100 ----
mean loss: 852.87
 ---- batch: 110 ----
mean loss: 856.45
train mean loss: 848.23
epoch train time: 0:00:01.865636
elapsed time: 0:01:55.185115
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 23:32:10.438722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.63
 ---- batch: 020 ----
mean loss: 842.16
 ---- batch: 030 ----
mean loss: 854.29
 ---- batch: 040 ----
mean loss: 862.31
 ---- batch: 050 ----
mean loss: 838.58
 ---- batch: 060 ----
mean loss: 870.23
 ---- batch: 070 ----
mean loss: 856.96
 ---- batch: 080 ----
mean loss: 841.90
 ---- batch: 090 ----
mean loss: 827.50
 ---- batch: 100 ----
mean loss: 836.32
 ---- batch: 110 ----
mean loss: 846.09
train mean loss: 849.07
epoch train time: 0:00:01.895775
elapsed time: 0:01:57.081499
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 23:32:12.335117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.69
 ---- batch: 020 ----
mean loss: 872.74
 ---- batch: 030 ----
mean loss: 863.66
 ---- batch: 040 ----
mean loss: 853.07
 ---- batch: 050 ----
mean loss: 836.62
 ---- batch: 060 ----
mean loss: 846.61
 ---- batch: 070 ----
mean loss: 839.11
 ---- batch: 080 ----
mean loss: 849.43
 ---- batch: 090 ----
mean loss: 843.55
 ---- batch: 100 ----
mean loss: 848.69
 ---- batch: 110 ----
mean loss: 858.08
train mean loss: 847.59
epoch train time: 0:00:01.868778
elapsed time: 0:01:58.950853
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 23:32:14.204455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.92
 ---- batch: 020 ----
mean loss: 827.47
 ---- batch: 030 ----
mean loss: 859.58
 ---- batch: 040 ----
mean loss: 860.83
 ---- batch: 050 ----
mean loss: 829.93
 ---- batch: 060 ----
mean loss: 827.77
 ---- batch: 070 ----
mean loss: 865.57
 ---- batch: 080 ----
mean loss: 825.85
 ---- batch: 090 ----
mean loss: 871.29
 ---- batch: 100 ----
mean loss: 842.54
 ---- batch: 110 ----
mean loss: 858.69
train mean loss: 847.14
epoch train time: 0:00:01.903839
elapsed time: 0:02:00.855254
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 23:32:16.108913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.03
 ---- batch: 020 ----
mean loss: 844.26
 ---- batch: 030 ----
mean loss: 835.72
 ---- batch: 040 ----
mean loss: 835.81
 ---- batch: 050 ----
mean loss: 849.23
 ---- batch: 060 ----
mean loss: 845.10
 ---- batch: 070 ----
mean loss: 869.15
 ---- batch: 080 ----
mean loss: 854.34
 ---- batch: 090 ----
mean loss: 867.00
 ---- batch: 100 ----
mean loss: 832.85
 ---- batch: 110 ----
mean loss: 846.93
train mean loss: 848.04
epoch train time: 0:00:01.880345
elapsed time: 0:02:02.736229
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 23:32:17.989856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.80
 ---- batch: 020 ----
mean loss: 847.10
 ---- batch: 030 ----
mean loss: 854.82
 ---- batch: 040 ----
mean loss: 864.77
 ---- batch: 050 ----
mean loss: 836.33
 ---- batch: 060 ----
mean loss: 854.91
 ---- batch: 070 ----
mean loss: 868.89
 ---- batch: 080 ----
mean loss: 835.53
 ---- batch: 090 ----
mean loss: 834.77
 ---- batch: 100 ----
mean loss: 854.83
 ---- batch: 110 ----
mean loss: 829.28
train mean loss: 847.39
epoch train time: 0:00:01.862376
elapsed time: 0:02:04.599225
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 23:32:19.852868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.40
 ---- batch: 020 ----
mean loss: 852.20
 ---- batch: 030 ----
mean loss: 859.95
 ---- batch: 040 ----
mean loss: 837.36
 ---- batch: 050 ----
mean loss: 837.78
 ---- batch: 060 ----
mean loss: 847.96
 ---- batch: 070 ----
mean loss: 833.21
 ---- batch: 080 ----
mean loss: 847.67
 ---- batch: 090 ----
mean loss: 851.57
 ---- batch: 100 ----
mean loss: 843.89
 ---- batch: 110 ----
mean loss: 847.43
train mean loss: 847.06
epoch train time: 0:00:01.893335
elapsed time: 0:02:06.493186
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 23:32:21.746794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.89
 ---- batch: 020 ----
mean loss: 849.19
 ---- batch: 030 ----
mean loss: 866.48
 ---- batch: 040 ----
mean loss: 854.53
 ---- batch: 050 ----
mean loss: 840.13
 ---- batch: 060 ----
mean loss: 843.46
 ---- batch: 070 ----
mean loss: 865.08
 ---- batch: 080 ----
mean loss: 857.78
 ---- batch: 090 ----
mean loss: 836.33
 ---- batch: 100 ----
mean loss: 818.39
 ---- batch: 110 ----
mean loss: 841.58
train mean loss: 847.00
epoch train time: 0:00:01.913910
elapsed time: 0:02:08.407815
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 23:32:23.661436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.67
 ---- batch: 020 ----
mean loss: 845.94
 ---- batch: 030 ----
mean loss: 845.82
 ---- batch: 040 ----
mean loss: 837.44
 ---- batch: 050 ----
mean loss: 848.04
 ---- batch: 060 ----
mean loss: 867.02
 ---- batch: 070 ----
mean loss: 844.56
 ---- batch: 080 ----
mean loss: 813.05
 ---- batch: 090 ----
mean loss: 835.88
 ---- batch: 100 ----
mean loss: 862.26
 ---- batch: 110 ----
mean loss: 852.22
train mean loss: 846.62
epoch train time: 0:00:01.897941
elapsed time: 0:02:10.306360
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 23:32:25.559965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.14
 ---- batch: 020 ----
mean loss: 839.33
 ---- batch: 030 ----
mean loss: 840.85
 ---- batch: 040 ----
mean loss: 826.14
 ---- batch: 050 ----
mean loss: 840.64
 ---- batch: 060 ----
mean loss: 860.97
 ---- batch: 070 ----
mean loss: 826.44
 ---- batch: 080 ----
mean loss: 859.31
 ---- batch: 090 ----
mean loss: 836.40
 ---- batch: 100 ----
mean loss: 852.84
 ---- batch: 110 ----
mean loss: 864.90
train mean loss: 846.43
epoch train time: 0:00:01.900686
elapsed time: 0:02:12.207615
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 23:32:27.461223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 861.39
 ---- batch: 030 ----
mean loss: 855.40
 ---- batch: 040 ----
mean loss: 843.78
 ---- batch: 050 ----
mean loss: 833.39
 ---- batch: 060 ----
mean loss: 842.22
 ---- batch: 070 ----
mean loss: 857.13
 ---- batch: 080 ----
mean loss: 852.18
 ---- batch: 090 ----
mean loss: 836.68
 ---- batch: 100 ----
mean loss: 842.76
 ---- batch: 110 ----
mean loss: 843.49
train mean loss: 846.70
epoch train time: 0:00:01.912915
elapsed time: 0:02:14.121113
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 23:32:29.374760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.43
 ---- batch: 020 ----
mean loss: 857.04
 ---- batch: 030 ----
mean loss: 848.12
 ---- batch: 040 ----
mean loss: 844.92
 ---- batch: 050 ----
mean loss: 859.64
 ---- batch: 060 ----
mean loss: 820.50
 ---- batch: 070 ----
mean loss: 864.38
 ---- batch: 080 ----
mean loss: 814.65
 ---- batch: 090 ----
mean loss: 836.22
 ---- batch: 100 ----
mean loss: 843.38
 ---- batch: 110 ----
mean loss: 859.40
train mean loss: 846.08
epoch train time: 0:00:01.907361
elapsed time: 0:02:16.029087
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 23:32:31.282722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.13
 ---- batch: 020 ----
mean loss: 839.34
 ---- batch: 030 ----
mean loss: 843.76
 ---- batch: 040 ----
mean loss: 856.44
 ---- batch: 050 ----
mean loss: 849.79
 ---- batch: 060 ----
mean loss: 840.49
 ---- batch: 070 ----
mean loss: 847.53
 ---- batch: 080 ----
mean loss: 830.90
 ---- batch: 090 ----
mean loss: 850.89
 ---- batch: 100 ----
mean loss: 844.54
 ---- batch: 110 ----
mean loss: 853.96
train mean loss: 846.72
epoch train time: 0:00:01.893605
elapsed time: 0:02:17.923277
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 23:32:33.176586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.63
 ---- batch: 020 ----
mean loss: 854.69
 ---- batch: 030 ----
mean loss: 835.31
 ---- batch: 040 ----
mean loss: 830.09
 ---- batch: 050 ----
mean loss: 842.78
 ---- batch: 060 ----
mean loss: 845.20
 ---- batch: 070 ----
mean loss: 846.54
 ---- batch: 080 ----
mean loss: 850.32
 ---- batch: 090 ----
mean loss: 836.77
 ---- batch: 100 ----
mean loss: 837.56
 ---- batch: 110 ----
mean loss: 850.32
train mean loss: 846.14
epoch train time: 0:00:01.900647
elapsed time: 0:02:19.824208
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 23:32:35.077886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.93
 ---- batch: 020 ----
mean loss: 855.08
 ---- batch: 030 ----
mean loss: 827.50
 ---- batch: 040 ----
mean loss: 842.19
 ---- batch: 050 ----
mean loss: 837.09
 ---- batch: 060 ----
mean loss: 843.29
 ---- batch: 070 ----
mean loss: 840.31
 ---- batch: 080 ----
mean loss: 816.23
 ---- batch: 090 ----
mean loss: 881.25
 ---- batch: 100 ----
mean loss: 871.79
 ---- batch: 110 ----
mean loss: 851.92
train mean loss: 846.48
epoch train time: 0:00:01.906229
elapsed time: 0:02:21.731090
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 23:32:36.984694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.48
 ---- batch: 020 ----
mean loss: 824.76
 ---- batch: 030 ----
mean loss: 846.57
 ---- batch: 040 ----
mean loss: 846.38
 ---- batch: 050 ----
mean loss: 843.00
 ---- batch: 060 ----
mean loss: 840.21
 ---- batch: 070 ----
mean loss: 867.70
 ---- batch: 080 ----
mean loss: 842.68
 ---- batch: 090 ----
mean loss: 839.13
 ---- batch: 100 ----
mean loss: 841.65
 ---- batch: 110 ----
mean loss: 862.56
train mean loss: 845.99
epoch train time: 0:00:01.905666
elapsed time: 0:02:23.637352
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 23:32:38.890984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.93
 ---- batch: 020 ----
mean loss: 854.09
 ---- batch: 030 ----
mean loss: 861.36
 ---- batch: 040 ----
mean loss: 835.73
 ---- batch: 050 ----
mean loss: 843.95
 ---- batch: 060 ----
mean loss: 840.85
 ---- batch: 070 ----
mean loss: 858.19
 ---- batch: 080 ----
mean loss: 828.86
 ---- batch: 090 ----
mean loss: 856.74
 ---- batch: 100 ----
mean loss: 839.98
 ---- batch: 110 ----
mean loss: 824.60
train mean loss: 845.89
epoch train time: 0:00:01.888995
elapsed time: 0:02:25.527125
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 23:32:40.780767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.54
 ---- batch: 020 ----
mean loss: 841.24
 ---- batch: 030 ----
mean loss: 858.57
 ---- batch: 040 ----
mean loss: 858.57
 ---- batch: 050 ----
mean loss: 837.78
 ---- batch: 060 ----
mean loss: 831.56
 ---- batch: 070 ----
mean loss: 833.77
 ---- batch: 080 ----
mean loss: 859.61
 ---- batch: 090 ----
mean loss: 828.63
 ---- batch: 100 ----
mean loss: 843.25
 ---- batch: 110 ----
mean loss: 846.31
train mean loss: 844.67
epoch train time: 0:00:01.882248
elapsed time: 0:02:27.409999
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 23:32:42.663610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.07
 ---- batch: 020 ----
mean loss: 824.59
 ---- batch: 030 ----
mean loss: 827.83
 ---- batch: 040 ----
mean loss: 820.97
 ---- batch: 050 ----
mean loss: 782.45
 ---- batch: 060 ----
mean loss: 765.66
 ---- batch: 070 ----
mean loss: 705.29
 ---- batch: 080 ----
mean loss: 654.46
 ---- batch: 090 ----
mean loss: 597.11
 ---- batch: 100 ----
mean loss: 531.95
 ---- batch: 110 ----
mean loss: 518.80
train mean loss: 707.42
epoch train time: 0:00:01.907077
elapsed time: 0:02:29.317653
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 23:32:44.571278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.88
 ---- batch: 020 ----
mean loss: 470.16
 ---- batch: 030 ----
mean loss: 459.96
 ---- batch: 040 ----
mean loss: 451.71
 ---- batch: 050 ----
mean loss: 449.16
 ---- batch: 060 ----
mean loss: 432.59
 ---- batch: 070 ----
mean loss: 407.44
 ---- batch: 080 ----
mean loss: 414.30
 ---- batch: 090 ----
mean loss: 416.46
 ---- batch: 100 ----
mean loss: 409.48
 ---- batch: 110 ----
mean loss: 413.38
train mean loss: 436.03
epoch train time: 0:00:01.873999
elapsed time: 0:02:31.192255
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 23:32:46.445875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.30
 ---- batch: 020 ----
mean loss: 376.41
 ---- batch: 030 ----
mean loss: 367.31
 ---- batch: 040 ----
mean loss: 389.44
 ---- batch: 050 ----
mean loss: 370.53
 ---- batch: 060 ----
mean loss: 374.20
 ---- batch: 070 ----
mean loss: 380.55
 ---- batch: 080 ----
mean loss: 362.26
 ---- batch: 090 ----
mean loss: 366.51
 ---- batch: 100 ----
mean loss: 371.41
 ---- batch: 110 ----
mean loss: 377.87
train mean loss: 375.97
epoch train time: 0:00:01.900755
elapsed time: 0:02:33.093611
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 23:32:48.347218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.52
 ---- batch: 020 ----
mean loss: 352.58
 ---- batch: 030 ----
mean loss: 347.93
 ---- batch: 040 ----
mean loss: 336.30
 ---- batch: 050 ----
mean loss: 347.76
 ---- batch: 060 ----
mean loss: 345.14
 ---- batch: 070 ----
mean loss: 348.94
 ---- batch: 080 ----
mean loss: 352.83
 ---- batch: 090 ----
mean loss: 358.60
 ---- batch: 100 ----
mean loss: 344.45
 ---- batch: 110 ----
mean loss: 336.03
train mean loss: 348.52
epoch train time: 0:00:01.934122
elapsed time: 0:02:35.028336
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 23:32:50.282045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.79
 ---- batch: 020 ----
mean loss: 326.67
 ---- batch: 030 ----
mean loss: 330.39
 ---- batch: 040 ----
mean loss: 331.31
 ---- batch: 050 ----
mean loss: 342.67
 ---- batch: 060 ----
mean loss: 325.26
 ---- batch: 070 ----
mean loss: 332.56
 ---- batch: 080 ----
mean loss: 324.17
 ---- batch: 090 ----
mean loss: 309.31
 ---- batch: 100 ----
mean loss: 320.74
 ---- batch: 110 ----
mean loss: 327.61
train mean loss: 328.31
epoch train time: 0:00:01.894053
elapsed time: 0:02:36.923081
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 23:32:52.176677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.70
 ---- batch: 020 ----
mean loss: 314.94
 ---- batch: 030 ----
mean loss: 318.81
 ---- batch: 040 ----
mean loss: 315.18
 ---- batch: 050 ----
mean loss: 309.65
 ---- batch: 060 ----
mean loss: 315.14
 ---- batch: 070 ----
mean loss: 310.02
 ---- batch: 080 ----
mean loss: 319.70
 ---- batch: 090 ----
mean loss: 309.46
 ---- batch: 100 ----
mean loss: 307.19
 ---- batch: 110 ----
mean loss: 301.24
train mean loss: 312.86
epoch train time: 0:00:01.902202
elapsed time: 0:02:38.825887
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 23:32:54.079498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.04
 ---- batch: 020 ----
mean loss: 300.24
 ---- batch: 030 ----
mean loss: 299.03
 ---- batch: 040 ----
mean loss: 295.84
 ---- batch: 050 ----
mean loss: 316.09
 ---- batch: 060 ----
mean loss: 293.35
 ---- batch: 070 ----
mean loss: 310.29
 ---- batch: 080 ----
mean loss: 299.62
 ---- batch: 090 ----
mean loss: 310.15
 ---- batch: 100 ----
mean loss: 302.64
 ---- batch: 110 ----
mean loss: 308.64
train mean loss: 305.15
epoch train time: 0:00:01.882403
elapsed time: 0:02:40.708881
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 23:32:55.962515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.68
 ---- batch: 020 ----
mean loss: 299.02
 ---- batch: 030 ----
mean loss: 291.58
 ---- batch: 040 ----
mean loss: 288.09
 ---- batch: 050 ----
mean loss: 299.77
 ---- batch: 060 ----
mean loss: 305.76
 ---- batch: 070 ----
mean loss: 302.05
 ---- batch: 080 ----
mean loss: 288.79
 ---- batch: 090 ----
mean loss: 293.74
 ---- batch: 100 ----
mean loss: 281.16
 ---- batch: 110 ----
mean loss: 298.73
train mean loss: 295.19
epoch train time: 0:00:01.906332
elapsed time: 0:02:42.615825
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 23:32:57.869405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.90
 ---- batch: 020 ----
mean loss: 293.09
 ---- batch: 030 ----
mean loss: 286.78
 ---- batch: 040 ----
mean loss: 282.48
 ---- batch: 050 ----
mean loss: 282.84
 ---- batch: 060 ----
mean loss: 286.83
 ---- batch: 070 ----
mean loss: 292.05
 ---- batch: 080 ----
mean loss: 276.94
 ---- batch: 090 ----
mean loss: 288.68
 ---- batch: 100 ----
mean loss: 279.34
 ---- batch: 110 ----
mean loss: 290.66
train mean loss: 286.74
epoch train time: 0:00:01.897823
elapsed time: 0:02:44.514205
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 23:32:59.767808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.76
 ---- batch: 020 ----
mean loss: 286.82
 ---- batch: 030 ----
mean loss: 292.43
 ---- batch: 040 ----
mean loss: 286.94
 ---- batch: 050 ----
mean loss: 279.58
 ---- batch: 060 ----
mean loss: 272.81
 ---- batch: 070 ----
mean loss: 279.90
 ---- batch: 080 ----
mean loss: 272.63
 ---- batch: 090 ----
mean loss: 276.97
 ---- batch: 100 ----
mean loss: 279.25
 ---- batch: 110 ----
mean loss: 283.39
train mean loss: 281.56
epoch train time: 0:00:01.877357
elapsed time: 0:02:46.392139
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 23:33:01.645753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.27
 ---- batch: 020 ----
mean loss: 269.00
 ---- batch: 030 ----
mean loss: 282.78
 ---- batch: 040 ----
mean loss: 275.11
 ---- batch: 050 ----
mean loss: 258.39
 ---- batch: 060 ----
mean loss: 270.00
 ---- batch: 070 ----
mean loss: 279.82
 ---- batch: 080 ----
mean loss: 284.86
 ---- batch: 090 ----
mean loss: 272.88
 ---- batch: 100 ----
mean loss: 275.53
 ---- batch: 110 ----
mean loss: 267.56
train mean loss: 273.60
epoch train time: 0:00:01.876490
elapsed time: 0:02:48.269210
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 23:33:03.522799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.16
 ---- batch: 020 ----
mean loss: 264.81
 ---- batch: 030 ----
mean loss: 267.49
 ---- batch: 040 ----
mean loss: 272.58
 ---- batch: 050 ----
mean loss: 277.39
 ---- batch: 060 ----
mean loss: 268.36
 ---- batch: 070 ----
mean loss: 266.65
 ---- batch: 080 ----
mean loss: 265.50
 ---- batch: 090 ----
mean loss: 271.63
 ---- batch: 100 ----
mean loss: 277.02
 ---- batch: 110 ----
mean loss: 262.03
train mean loss: 268.68
epoch train time: 0:00:01.890837
elapsed time: 0:02:50.160634
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 23:33:05.414231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.46
 ---- batch: 020 ----
mean loss: 266.35
 ---- batch: 030 ----
mean loss: 268.35
 ---- batch: 040 ----
mean loss: 262.46
 ---- batch: 050 ----
mean loss: 260.01
 ---- batch: 060 ----
mean loss: 269.70
 ---- batch: 070 ----
mean loss: 260.76
 ---- batch: 080 ----
mean loss: 272.13
 ---- batch: 090 ----
mean loss: 265.62
 ---- batch: 100 ----
mean loss: 261.65
 ---- batch: 110 ----
mean loss: 257.77
train mean loss: 264.19
epoch train time: 0:00:01.894153
elapsed time: 0:02:52.055350
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 23:33:07.309040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.87
 ---- batch: 020 ----
mean loss: 259.61
 ---- batch: 030 ----
mean loss: 257.25
 ---- batch: 040 ----
mean loss: 252.20
 ---- batch: 050 ----
mean loss: 252.56
 ---- batch: 060 ----
mean loss: 274.67
 ---- batch: 070 ----
mean loss: 263.81
 ---- batch: 080 ----
mean loss: 259.54
 ---- batch: 090 ----
mean loss: 253.78
 ---- batch: 100 ----
mean loss: 260.52
 ---- batch: 110 ----
mean loss: 252.92
train mean loss: 259.53
epoch train time: 0:00:01.888980
elapsed time: 0:02:53.945005
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 23:33:09.198594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.52
 ---- batch: 020 ----
mean loss: 251.19
 ---- batch: 030 ----
mean loss: 270.79
 ---- batch: 040 ----
mean loss: 255.16
 ---- batch: 050 ----
mean loss: 263.40
 ---- batch: 060 ----
mean loss: 251.80
 ---- batch: 070 ----
mean loss: 247.30
 ---- batch: 080 ----
mean loss: 251.50
 ---- batch: 090 ----
mean loss: 250.08
 ---- batch: 100 ----
mean loss: 253.60
 ---- batch: 110 ----
mean loss: 257.70
train mean loss: 254.63
epoch train time: 0:00:01.901352
elapsed time: 0:02:55.846915
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 23:33:11.100506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.14
 ---- batch: 020 ----
mean loss: 252.81
 ---- batch: 030 ----
mean loss: 254.73
 ---- batch: 040 ----
mean loss: 252.75
 ---- batch: 050 ----
mean loss: 254.73
 ---- batch: 060 ----
mean loss: 255.90
 ---- batch: 070 ----
mean loss: 254.80
 ---- batch: 080 ----
mean loss: 251.99
 ---- batch: 090 ----
mean loss: 247.61
 ---- batch: 100 ----
mean loss: 238.81
 ---- batch: 110 ----
mean loss: 249.87
train mean loss: 250.45
epoch train time: 0:00:01.909444
elapsed time: 0:02:57.756920
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 23:33:13.010536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.81
 ---- batch: 020 ----
mean loss: 255.06
 ---- batch: 030 ----
mean loss: 237.70
 ---- batch: 040 ----
mean loss: 245.53
 ---- batch: 050 ----
mean loss: 254.95
 ---- batch: 060 ----
mean loss: 250.57
 ---- batch: 070 ----
mean loss: 244.46
 ---- batch: 080 ----
mean loss: 241.89
 ---- batch: 090 ----
mean loss: 256.07
 ---- batch: 100 ----
mean loss: 239.52
 ---- batch: 110 ----
mean loss: 237.16
train mean loss: 245.74
epoch train time: 0:00:01.909137
elapsed time: 0:02:59.666663
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 23:33:14.920245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.69
 ---- batch: 020 ----
mean loss: 243.58
 ---- batch: 030 ----
mean loss: 232.12
 ---- batch: 040 ----
mean loss: 241.08
 ---- batch: 050 ----
mean loss: 238.62
 ---- batch: 060 ----
mean loss: 253.26
 ---- batch: 070 ----
mean loss: 248.05
 ---- batch: 080 ----
mean loss: 250.73
 ---- batch: 090 ----
mean loss: 237.61
 ---- batch: 100 ----
mean loss: 241.38
 ---- batch: 110 ----
mean loss: 244.29
train mean loss: 242.45
epoch train time: 0:00:01.852739
elapsed time: 0:03:01.519943
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 23:33:16.773532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.31
 ---- batch: 020 ----
mean loss: 241.81
 ---- batch: 030 ----
mean loss: 229.46
 ---- batch: 040 ----
mean loss: 250.17
 ---- batch: 050 ----
mean loss: 240.60
 ---- batch: 060 ----
mean loss: 238.70
 ---- batch: 070 ----
mean loss: 238.64
 ---- batch: 080 ----
mean loss: 235.44
 ---- batch: 090 ----
mean loss: 245.81
 ---- batch: 100 ----
mean loss: 225.89
 ---- batch: 110 ----
mean loss: 247.41
train mean loss: 238.86
epoch train time: 0:00:01.838589
elapsed time: 0:03:03.359087
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 23:33:18.612695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.16
 ---- batch: 020 ----
mean loss: 240.85
 ---- batch: 030 ----
mean loss: 226.01
 ---- batch: 040 ----
mean loss: 231.80
 ---- batch: 050 ----
mean loss: 243.51
 ---- batch: 060 ----
mean loss: 240.45
 ---- batch: 070 ----
mean loss: 242.30
 ---- batch: 080 ----
mean loss: 234.21
 ---- batch: 090 ----
mean loss: 236.98
 ---- batch: 100 ----
mean loss: 238.50
 ---- batch: 110 ----
mean loss: 229.01
train mean loss: 235.76
epoch train time: 0:00:01.850107
elapsed time: 0:03:05.209793
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 23:33:20.463443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.51
 ---- batch: 020 ----
mean loss: 235.61
 ---- batch: 030 ----
mean loss: 228.98
 ---- batch: 040 ----
mean loss: 231.69
 ---- batch: 050 ----
mean loss: 230.40
 ---- batch: 060 ----
mean loss: 222.49
 ---- batch: 070 ----
mean loss: 228.10
 ---- batch: 080 ----
mean loss: 247.54
 ---- batch: 090 ----
mean loss: 234.37
 ---- batch: 100 ----
mean loss: 223.37
 ---- batch: 110 ----
mean loss: 233.92
train mean loss: 231.05
epoch train time: 0:00:01.908295
elapsed time: 0:03:07.118736
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 23:33:22.372342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.57
 ---- batch: 020 ----
mean loss: 237.39
 ---- batch: 030 ----
mean loss: 234.71
 ---- batch: 040 ----
mean loss: 234.47
 ---- batch: 050 ----
mean loss: 233.67
 ---- batch: 060 ----
mean loss: 230.64
 ---- batch: 070 ----
mean loss: 231.33
 ---- batch: 080 ----
mean loss: 222.77
 ---- batch: 090 ----
mean loss: 226.16
 ---- batch: 100 ----
mean loss: 229.48
 ---- batch: 110 ----
mean loss: 225.53
train mean loss: 229.54
epoch train time: 0:00:01.912489
elapsed time: 0:03:09.031780
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 23:33:24.285363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.48
 ---- batch: 020 ----
mean loss: 226.57
 ---- batch: 030 ----
mean loss: 221.41
 ---- batch: 040 ----
mean loss: 221.43
 ---- batch: 050 ----
mean loss: 224.38
 ---- batch: 060 ----
mean loss: 224.24
 ---- batch: 070 ----
mean loss: 220.11
 ---- batch: 080 ----
mean loss: 217.00
 ---- batch: 090 ----
mean loss: 221.25
 ---- batch: 100 ----
mean loss: 226.61
 ---- batch: 110 ----
mean loss: 225.89
train mean loss: 222.87
epoch train time: 0:00:01.899626
elapsed time: 0:03:10.931980
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 23:33:26.185573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.47
 ---- batch: 020 ----
mean loss: 226.17
 ---- batch: 030 ----
mean loss: 221.85
 ---- batch: 040 ----
mean loss: 222.85
 ---- batch: 050 ----
mean loss: 215.52
 ---- batch: 060 ----
mean loss: 225.29
 ---- batch: 070 ----
mean loss: 228.08
 ---- batch: 080 ----
mean loss: 228.47
 ---- batch: 090 ----
mean loss: 218.19
 ---- batch: 100 ----
mean loss: 227.07
 ---- batch: 110 ----
mean loss: 219.11
train mean loss: 222.30
epoch train time: 0:00:01.852548
elapsed time: 0:03:12.785083
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 23:33:28.038655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.17
 ---- batch: 020 ----
mean loss: 225.80
 ---- batch: 030 ----
mean loss: 215.27
 ---- batch: 040 ----
mean loss: 225.77
 ---- batch: 050 ----
mean loss: 227.64
 ---- batch: 060 ----
mean loss: 223.88
 ---- batch: 070 ----
mean loss: 220.55
 ---- batch: 080 ----
mean loss: 226.28
 ---- batch: 090 ----
mean loss: 222.45
 ---- batch: 100 ----
mean loss: 221.96
 ---- batch: 110 ----
mean loss: 215.69
train mean loss: 221.27
epoch train time: 0:00:01.859447
elapsed time: 0:03:14.645085
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 23:33:29.898695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.48
 ---- batch: 020 ----
mean loss: 228.43
 ---- batch: 030 ----
mean loss: 210.77
 ---- batch: 040 ----
mean loss: 214.02
 ---- batch: 050 ----
mean loss: 219.54
 ---- batch: 060 ----
mean loss: 215.26
 ---- batch: 070 ----
mean loss: 210.25
 ---- batch: 080 ----
mean loss: 213.42
 ---- batch: 090 ----
mean loss: 220.53
 ---- batch: 100 ----
mean loss: 218.81
 ---- batch: 110 ----
mean loss: 219.64
train mean loss: 216.75
epoch train time: 0:00:01.874311
elapsed time: 0:03:16.519949
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 23:33:31.773525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.12
 ---- batch: 020 ----
mean loss: 216.90
 ---- batch: 030 ----
mean loss: 213.80
 ---- batch: 040 ----
mean loss: 205.09
 ---- batch: 050 ----
mean loss: 211.92
 ---- batch: 060 ----
mean loss: 210.22
 ---- batch: 070 ----
mean loss: 212.29
 ---- batch: 080 ----
mean loss: 219.72
 ---- batch: 090 ----
mean loss: 215.43
 ---- batch: 100 ----
mean loss: 205.86
 ---- batch: 110 ----
mean loss: 215.96
train mean loss: 213.41
epoch train time: 0:00:01.896296
elapsed time: 0:03:18.416805
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 23:33:33.670410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.87
 ---- batch: 020 ----
mean loss: 207.85
 ---- batch: 030 ----
mean loss: 212.67
 ---- batch: 040 ----
mean loss: 211.27
 ---- batch: 050 ----
mean loss: 215.16
 ---- batch: 060 ----
mean loss: 205.63
 ---- batch: 070 ----
mean loss: 213.53
 ---- batch: 080 ----
mean loss: 212.16
 ---- batch: 090 ----
mean loss: 216.96
 ---- batch: 100 ----
mean loss: 220.01
 ---- batch: 110 ----
mean loss: 208.16
train mean loss: 210.72
epoch train time: 0:00:01.884501
elapsed time: 0:03:20.301870
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 23:33:35.555483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.64
 ---- batch: 020 ----
mean loss: 201.80
 ---- batch: 030 ----
mean loss: 217.16
 ---- batch: 040 ----
mean loss: 215.03
 ---- batch: 050 ----
mean loss: 200.89
 ---- batch: 060 ----
mean loss: 209.93
 ---- batch: 070 ----
mean loss: 209.99
 ---- batch: 080 ----
mean loss: 211.59
 ---- batch: 090 ----
mean loss: 205.66
 ---- batch: 100 ----
mean loss: 207.11
 ---- batch: 110 ----
mean loss: 207.81
train mean loss: 208.96
epoch train time: 0:00:01.886962
elapsed time: 0:03:22.189405
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 23:33:37.442999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.34
 ---- batch: 020 ----
mean loss: 207.97
 ---- batch: 030 ----
mean loss: 194.96
 ---- batch: 040 ----
mean loss: 203.63
 ---- batch: 050 ----
mean loss: 196.15
 ---- batch: 060 ----
mean loss: 216.21
 ---- batch: 070 ----
mean loss: 212.28
 ---- batch: 080 ----
mean loss: 207.28
 ---- batch: 090 ----
mean loss: 200.69
 ---- batch: 100 ----
mean loss: 209.06
 ---- batch: 110 ----
mean loss: 205.96
train mean loss: 205.61
epoch train time: 0:00:01.852228
elapsed time: 0:03:24.042177
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 23:33:39.295753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.50
 ---- batch: 020 ----
mean loss: 211.28
 ---- batch: 030 ----
mean loss: 206.59
 ---- batch: 040 ----
mean loss: 199.22
 ---- batch: 050 ----
mean loss: 203.55
 ---- batch: 060 ----
mean loss: 206.76
 ---- batch: 070 ----
mean loss: 204.48
 ---- batch: 080 ----
mean loss: 200.86
 ---- batch: 090 ----
mean loss: 202.52
 ---- batch: 100 ----
mean loss: 199.61
 ---- batch: 110 ----
mean loss: 208.40
train mean loss: 204.04
epoch train time: 0:00:01.901500
elapsed time: 0:03:25.944214
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 23:33:41.197848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.19
 ---- batch: 020 ----
mean loss: 201.18
 ---- batch: 030 ----
mean loss: 199.12
 ---- batch: 040 ----
mean loss: 205.39
 ---- batch: 050 ----
mean loss: 196.76
 ---- batch: 060 ----
mean loss: 199.18
 ---- batch: 070 ----
mean loss: 205.12
 ---- batch: 080 ----
mean loss: 202.38
 ---- batch: 090 ----
mean loss: 194.97
 ---- batch: 100 ----
mean loss: 196.07
 ---- batch: 110 ----
mean loss: 206.29
train mean loss: 200.80
epoch train time: 0:00:01.919795
elapsed time: 0:03:27.864672
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 23:33:43.118252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.67
 ---- batch: 020 ----
mean loss: 207.63
 ---- batch: 030 ----
mean loss: 202.94
 ---- batch: 040 ----
mean loss: 205.21
 ---- batch: 050 ----
mean loss: 198.00
 ---- batch: 060 ----
mean loss: 185.31
 ---- batch: 070 ----
mean loss: 200.35
 ---- batch: 080 ----
mean loss: 191.51
 ---- batch: 090 ----
mean loss: 201.08
 ---- batch: 100 ----
mean loss: 203.99
 ---- batch: 110 ----
mean loss: 196.06
train mean loss: 199.51
epoch train time: 0:00:01.902365
elapsed time: 0:03:29.767601
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 23:33:45.021202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.62
 ---- batch: 020 ----
mean loss: 206.36
 ---- batch: 030 ----
mean loss: 200.32
 ---- batch: 040 ----
mean loss: 194.90
 ---- batch: 050 ----
mean loss: 195.53
 ---- batch: 060 ----
mean loss: 199.16
 ---- batch: 070 ----
mean loss: 206.54
 ---- batch: 080 ----
mean loss: 202.47
 ---- batch: 090 ----
mean loss: 185.18
 ---- batch: 100 ----
mean loss: 197.61
 ---- batch: 110 ----
mean loss: 192.25
train mean loss: 197.63
epoch train time: 0:00:01.899119
elapsed time: 0:03:31.667293
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 23:33:46.920893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.07
 ---- batch: 020 ----
mean loss: 196.69
 ---- batch: 030 ----
mean loss: 204.11
 ---- batch: 040 ----
mean loss: 193.81
 ---- batch: 050 ----
mean loss: 195.58
 ---- batch: 060 ----
mean loss: 193.01
 ---- batch: 070 ----
mean loss: 197.21
 ---- batch: 080 ----
mean loss: 192.37
 ---- batch: 090 ----
mean loss: 195.40
 ---- batch: 100 ----
mean loss: 193.87
 ---- batch: 110 ----
mean loss: 195.18
train mean loss: 195.63
epoch train time: 0:00:01.895520
elapsed time: 0:03:33.563379
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 23:33:48.816992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.20
 ---- batch: 020 ----
mean loss: 190.50
 ---- batch: 030 ----
mean loss: 190.40
 ---- batch: 040 ----
mean loss: 201.54
 ---- batch: 050 ----
mean loss: 201.43
 ---- batch: 060 ----
mean loss: 188.36
 ---- batch: 070 ----
mean loss: 186.78
 ---- batch: 080 ----
mean loss: 183.98
 ---- batch: 090 ----
mean loss: 195.47
 ---- batch: 100 ----
mean loss: 190.10
 ---- batch: 110 ----
mean loss: 191.46
train mean loss: 192.25
epoch train time: 0:00:01.884447
elapsed time: 0:03:35.448477
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 23:33:50.702109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.73
 ---- batch: 020 ----
mean loss: 180.22
 ---- batch: 030 ----
mean loss: 178.02
 ---- batch: 040 ----
mean loss: 195.13
 ---- batch: 050 ----
mean loss: 200.93
 ---- batch: 060 ----
mean loss: 194.50
 ---- batch: 070 ----
mean loss: 194.46
 ---- batch: 080 ----
mean loss: 186.19
 ---- batch: 090 ----
mean loss: 191.09
 ---- batch: 100 ----
mean loss: 186.46
 ---- batch: 110 ----
mean loss: 201.41
train mean loss: 190.92
epoch train time: 0:00:01.915563
elapsed time: 0:03:37.364635
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 23:33:52.617958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.00
 ---- batch: 020 ----
mean loss: 188.48
 ---- batch: 030 ----
mean loss: 188.91
 ---- batch: 040 ----
mean loss: 186.86
 ---- batch: 050 ----
mean loss: 202.67
 ---- batch: 060 ----
mean loss: 184.49
 ---- batch: 070 ----
mean loss: 187.48
 ---- batch: 080 ----
mean loss: 198.10
 ---- batch: 090 ----
mean loss: 194.09
 ---- batch: 100 ----
mean loss: 192.16
 ---- batch: 110 ----
mean loss: 184.70
train mean loss: 190.17
epoch train time: 0:00:01.912121
elapsed time: 0:03:39.277065
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 23:33:54.530648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.57
 ---- batch: 020 ----
mean loss: 187.86
 ---- batch: 030 ----
mean loss: 188.04
 ---- batch: 040 ----
mean loss: 183.58
 ---- batch: 050 ----
mean loss: 188.07
 ---- batch: 060 ----
mean loss: 188.92
 ---- batch: 070 ----
mean loss: 188.10
 ---- batch: 080 ----
mean loss: 197.83
 ---- batch: 090 ----
mean loss: 188.27
 ---- batch: 100 ----
mean loss: 189.07
 ---- batch: 110 ----
mean loss: 186.72
train mean loss: 187.82
epoch train time: 0:00:01.903852
elapsed time: 0:03:41.181490
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 23:33:56.435181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.47
 ---- batch: 020 ----
mean loss: 191.13
 ---- batch: 030 ----
mean loss: 174.06
 ---- batch: 040 ----
mean loss: 185.68
 ---- batch: 050 ----
mean loss: 176.66
 ---- batch: 060 ----
mean loss: 186.58
 ---- batch: 070 ----
mean loss: 178.73
 ---- batch: 080 ----
mean loss: 181.72
 ---- batch: 090 ----
mean loss: 186.35
 ---- batch: 100 ----
mean loss: 194.71
 ---- batch: 110 ----
mean loss: 193.65
train mean loss: 185.71
epoch train time: 0:00:01.895392
elapsed time: 0:03:43.077543
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 23:33:58.331150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.45
 ---- batch: 020 ----
mean loss: 184.50
 ---- batch: 030 ----
mean loss: 189.32
 ---- batch: 040 ----
mean loss: 182.79
 ---- batch: 050 ----
mean loss: 184.18
 ---- batch: 060 ----
mean loss: 190.74
 ---- batch: 070 ----
mean loss: 184.87
 ---- batch: 080 ----
mean loss: 186.63
 ---- batch: 090 ----
mean loss: 180.53
 ---- batch: 100 ----
mean loss: 188.55
 ---- batch: 110 ----
mean loss: 185.75
train mean loss: 184.90
epoch train time: 0:00:01.873427
elapsed time: 0:03:44.951567
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 23:34:00.205142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.61
 ---- batch: 020 ----
mean loss: 182.90
 ---- batch: 030 ----
mean loss: 182.51
 ---- batch: 040 ----
mean loss: 176.83
 ---- batch: 050 ----
mean loss: 191.93
 ---- batch: 060 ----
mean loss: 178.37
 ---- batch: 070 ----
mean loss: 186.23
 ---- batch: 080 ----
mean loss: 187.28
 ---- batch: 090 ----
mean loss: 185.24
 ---- batch: 100 ----
mean loss: 179.62
 ---- batch: 110 ----
mean loss: 186.02
train mean loss: 183.87
epoch train time: 0:00:01.889546
elapsed time: 0:03:46.841659
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 23:34:02.095317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.95
 ---- batch: 020 ----
mean loss: 187.09
 ---- batch: 030 ----
mean loss: 181.68
 ---- batch: 040 ----
mean loss: 182.37
 ---- batch: 050 ----
mean loss: 182.07
 ---- batch: 060 ----
mean loss: 186.41
 ---- batch: 070 ----
mean loss: 176.24
 ---- batch: 080 ----
mean loss: 179.03
 ---- batch: 090 ----
mean loss: 179.40
 ---- batch: 100 ----
mean loss: 180.77
 ---- batch: 110 ----
mean loss: 182.30
train mean loss: 182.44
epoch train time: 0:00:01.909917
elapsed time: 0:03:48.752202
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 23:34:04.005843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.37
 ---- batch: 020 ----
mean loss: 179.22
 ---- batch: 030 ----
mean loss: 182.88
 ---- batch: 040 ----
mean loss: 178.72
 ---- batch: 050 ----
mean loss: 173.49
 ---- batch: 060 ----
mean loss: 190.07
 ---- batch: 070 ----
mean loss: 180.96
 ---- batch: 080 ----
mean loss: 178.51
 ---- batch: 090 ----
mean loss: 174.90
 ---- batch: 100 ----
mean loss: 179.93
 ---- batch: 110 ----
mean loss: 183.48
train mean loss: 180.83
epoch train time: 0:00:01.896744
elapsed time: 0:03:50.649568
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 23:34:05.903187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.44
 ---- batch: 020 ----
mean loss: 180.23
 ---- batch: 030 ----
mean loss: 177.66
 ---- batch: 040 ----
mean loss: 179.65
 ---- batch: 050 ----
mean loss: 180.72
 ---- batch: 060 ----
mean loss: 174.58
 ---- batch: 070 ----
mean loss: 181.05
 ---- batch: 080 ----
mean loss: 173.63
 ---- batch: 090 ----
mean loss: 182.38
 ---- batch: 100 ----
mean loss: 182.03
 ---- batch: 110 ----
mean loss: 187.71
train mean loss: 178.61
epoch train time: 0:00:01.879533
elapsed time: 0:03:52.529725
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 23:34:07.783354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.08
 ---- batch: 020 ----
mean loss: 172.93
 ---- batch: 030 ----
mean loss: 177.25
 ---- batch: 040 ----
mean loss: 173.48
 ---- batch: 050 ----
mean loss: 173.81
 ---- batch: 060 ----
mean loss: 181.86
 ---- batch: 070 ----
mean loss: 178.93
 ---- batch: 080 ----
mean loss: 172.71
 ---- batch: 090 ----
mean loss: 179.15
 ---- batch: 100 ----
mean loss: 183.35
 ---- batch: 110 ----
mean loss: 175.39
train mean loss: 177.70
epoch train time: 0:00:01.877382
elapsed time: 0:03:54.407970
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 23:34:09.661365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.56
 ---- batch: 020 ----
mean loss: 180.92
 ---- batch: 030 ----
mean loss: 174.49
 ---- batch: 040 ----
mean loss: 172.71
 ---- batch: 050 ----
mean loss: 178.13
 ---- batch: 060 ----
mean loss: 176.50
 ---- batch: 070 ----
mean loss: 170.60
 ---- batch: 080 ----
mean loss: 182.19
 ---- batch: 090 ----
mean loss: 184.23
 ---- batch: 100 ----
mean loss: 172.42
 ---- batch: 110 ----
mean loss: 178.59
train mean loss: 176.66
epoch train time: 0:00:01.878374
elapsed time: 0:03:56.286713
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 23:34:11.540295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.84
 ---- batch: 020 ----
mean loss: 175.23
 ---- batch: 030 ----
mean loss: 171.83
 ---- batch: 040 ----
mean loss: 169.70
 ---- batch: 050 ----
mean loss: 179.28
 ---- batch: 060 ----
mean loss: 170.14
 ---- batch: 070 ----
mean loss: 186.65
 ---- batch: 080 ----
mean loss: 180.40
 ---- batch: 090 ----
mean loss: 180.71
 ---- batch: 100 ----
mean loss: 167.15
 ---- batch: 110 ----
mean loss: 174.77
train mean loss: 175.48
epoch train time: 0:00:01.900862
elapsed time: 0:03:58.188113
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 23:34:13.441746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.55
 ---- batch: 020 ----
mean loss: 181.92
 ---- batch: 030 ----
mean loss: 166.92
 ---- batch: 040 ----
mean loss: 166.66
 ---- batch: 050 ----
mean loss: 174.44
 ---- batch: 060 ----
mean loss: 177.11
 ---- batch: 070 ----
mean loss: 177.26
 ---- batch: 080 ----
mean loss: 173.16
 ---- batch: 090 ----
mean loss: 181.43
 ---- batch: 100 ----
mean loss: 179.73
 ---- batch: 110 ----
mean loss: 174.86
train mean loss: 175.35
epoch train time: 0:00:01.888529
elapsed time: 0:04:00.077236
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 23:34:15.330864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.09
 ---- batch: 020 ----
mean loss: 167.68
 ---- batch: 030 ----
mean loss: 173.76
 ---- batch: 040 ----
mean loss: 168.98
 ---- batch: 050 ----
mean loss: 174.70
 ---- batch: 060 ----
mean loss: 174.16
 ---- batch: 070 ----
mean loss: 180.78
 ---- batch: 080 ----
mean loss: 172.59
 ---- batch: 090 ----
mean loss: 171.21
 ---- batch: 100 ----
mean loss: 175.45
 ---- batch: 110 ----
mean loss: 181.65
train mean loss: 173.56
epoch train time: 0:00:01.909397
elapsed time: 0:04:01.987234
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 23:34:17.240875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.79
 ---- batch: 020 ----
mean loss: 170.33
 ---- batch: 030 ----
mean loss: 167.91
 ---- batch: 040 ----
mean loss: 165.22
 ---- batch: 050 ----
mean loss: 178.41
 ---- batch: 060 ----
mean loss: 170.82
 ---- batch: 070 ----
mean loss: 171.30
 ---- batch: 080 ----
mean loss: 180.53
 ---- batch: 090 ----
mean loss: 171.08
 ---- batch: 100 ----
mean loss: 174.07
 ---- batch: 110 ----
mean loss: 177.42
train mean loss: 172.82
epoch train time: 0:00:01.884171
elapsed time: 0:04:03.872002
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 23:34:19.125585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.44
 ---- batch: 020 ----
mean loss: 167.08
 ---- batch: 030 ----
mean loss: 167.70
 ---- batch: 040 ----
mean loss: 170.02
 ---- batch: 050 ----
mean loss: 168.53
 ---- batch: 060 ----
mean loss: 170.87
 ---- batch: 070 ----
mean loss: 173.94
 ---- batch: 080 ----
mean loss: 179.98
 ---- batch: 090 ----
mean loss: 177.60
 ---- batch: 100 ----
mean loss: 163.78
 ---- batch: 110 ----
mean loss: 170.75
train mean loss: 171.04
epoch train time: 0:00:01.863275
elapsed time: 0:04:05.735859
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 23:34:20.989501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.78
 ---- batch: 020 ----
mean loss: 162.97
 ---- batch: 030 ----
mean loss: 169.48
 ---- batch: 040 ----
mean loss: 160.31
 ---- batch: 050 ----
mean loss: 168.18
 ---- batch: 060 ----
mean loss: 170.96
 ---- batch: 070 ----
mean loss: 167.91
 ---- batch: 080 ----
mean loss: 177.33
 ---- batch: 090 ----
mean loss: 171.35
 ---- batch: 100 ----
mean loss: 173.50
 ---- batch: 110 ----
mean loss: 177.79
train mean loss: 170.29
epoch train time: 0:00:01.855706
elapsed time: 0:04:07.592197
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 23:34:22.845821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.28
 ---- batch: 020 ----
mean loss: 174.89
 ---- batch: 030 ----
mean loss: 167.16
 ---- batch: 040 ----
mean loss: 169.15
 ---- batch: 050 ----
mean loss: 175.85
 ---- batch: 060 ----
mean loss: 179.24
 ---- batch: 070 ----
mean loss: 168.51
 ---- batch: 080 ----
mean loss: 161.88
 ---- batch: 090 ----
mean loss: 168.13
 ---- batch: 100 ----
mean loss: 167.06
 ---- batch: 110 ----
mean loss: 163.96
train mean loss: 169.29
epoch train time: 0:00:01.873868
elapsed time: 0:04:09.466681
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 23:34:24.720286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.40
 ---- batch: 020 ----
mean loss: 176.60
 ---- batch: 030 ----
mean loss: 168.69
 ---- batch: 040 ----
mean loss: 168.91
 ---- batch: 050 ----
mean loss: 168.79
 ---- batch: 060 ----
mean loss: 166.81
 ---- batch: 070 ----
mean loss: 167.16
 ---- batch: 080 ----
mean loss: 172.46
 ---- batch: 090 ----
mean loss: 168.18
 ---- batch: 100 ----
mean loss: 159.29
 ---- batch: 110 ----
mean loss: 155.94
train mean loss: 168.11
epoch train time: 0:00:01.885398
elapsed time: 0:04:11.352643
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 23:34:26.606228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.13
 ---- batch: 020 ----
mean loss: 166.28
 ---- batch: 030 ----
mean loss: 166.34
 ---- batch: 040 ----
mean loss: 166.87
 ---- batch: 050 ----
mean loss: 167.45
 ---- batch: 060 ----
mean loss: 168.73
 ---- batch: 070 ----
mean loss: 174.83
 ---- batch: 080 ----
mean loss: 176.01
 ---- batch: 090 ----
mean loss: 161.49
 ---- batch: 100 ----
mean loss: 174.58
 ---- batch: 110 ----
mean loss: 163.84
train mean loss: 168.31
epoch train time: 0:00:01.861538
elapsed time: 0:04:13.214733
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 23:34:28.468332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.68
 ---- batch: 020 ----
mean loss: 163.01
 ---- batch: 030 ----
mean loss: 165.28
 ---- batch: 040 ----
mean loss: 163.64
 ---- batch: 050 ----
mean loss: 163.37
 ---- batch: 060 ----
mean loss: 170.21
 ---- batch: 070 ----
mean loss: 171.78
 ---- batch: 080 ----
mean loss: 169.23
 ---- batch: 090 ----
mean loss: 167.84
 ---- batch: 100 ----
mean loss: 168.60
 ---- batch: 110 ----
mean loss: 164.50
train mean loss: 167.39
epoch train time: 0:00:01.853883
elapsed time: 0:04:15.069211
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 23:34:30.322857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.29
 ---- batch: 020 ----
mean loss: 157.87
 ---- batch: 030 ----
mean loss: 162.73
 ---- batch: 040 ----
mean loss: 164.28
 ---- batch: 050 ----
mean loss: 167.00
 ---- batch: 060 ----
mean loss: 165.21
 ---- batch: 070 ----
mean loss: 169.14
 ---- batch: 080 ----
mean loss: 165.48
 ---- batch: 090 ----
mean loss: 169.11
 ---- batch: 100 ----
mean loss: 166.63
 ---- batch: 110 ----
mean loss: 169.71
train mean loss: 165.15
epoch train time: 0:00:01.912086
elapsed time: 0:04:16.981968
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 23:34:32.235679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.01
 ---- batch: 020 ----
mean loss: 164.02
 ---- batch: 030 ----
mean loss: 161.10
 ---- batch: 040 ----
mean loss: 162.17
 ---- batch: 050 ----
mean loss: 168.53
 ---- batch: 060 ----
mean loss: 170.35
 ---- batch: 070 ----
mean loss: 166.56
 ---- batch: 080 ----
mean loss: 164.63
 ---- batch: 090 ----
mean loss: 168.85
 ---- batch: 100 ----
mean loss: 163.47
 ---- batch: 110 ----
mean loss: 168.94
train mean loss: 165.75
epoch train time: 0:00:01.877228
elapsed time: 0:04:18.859850
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 23:34:34.113412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.61
 ---- batch: 020 ----
mean loss: 152.82
 ---- batch: 030 ----
mean loss: 169.18
 ---- batch: 040 ----
mean loss: 166.81
 ---- batch: 050 ----
mean loss: 159.51
 ---- batch: 060 ----
mean loss: 161.35
 ---- batch: 070 ----
mean loss: 170.97
 ---- batch: 080 ----
mean loss: 158.64
 ---- batch: 090 ----
mean loss: 169.57
 ---- batch: 100 ----
mean loss: 161.29
 ---- batch: 110 ----
mean loss: 167.15
train mean loss: 163.96
epoch train time: 0:00:01.839099
elapsed time: 0:04:20.699491
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 23:34:35.953136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.24
 ---- batch: 020 ----
mean loss: 159.26
 ---- batch: 030 ----
mean loss: 160.91
 ---- batch: 040 ----
mean loss: 163.06
 ---- batch: 050 ----
mean loss: 155.34
 ---- batch: 060 ----
mean loss: 161.13
 ---- batch: 070 ----
mean loss: 169.12
 ---- batch: 080 ----
mean loss: 167.95
 ---- batch: 090 ----
mean loss: 163.07
 ---- batch: 100 ----
mean loss: 169.85
 ---- batch: 110 ----
mean loss: 165.49
train mean loss: 163.35
epoch train time: 0:00:01.906846
elapsed time: 0:04:22.606975
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 23:34:37.860587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.19
 ---- batch: 020 ----
mean loss: 167.94
 ---- batch: 030 ----
mean loss: 158.12
 ---- batch: 040 ----
mean loss: 160.87
 ---- batch: 050 ----
mean loss: 168.12
 ---- batch: 060 ----
mean loss: 164.18
 ---- batch: 070 ----
mean loss: 169.09
 ---- batch: 080 ----
mean loss: 167.12
 ---- batch: 090 ----
mean loss: 159.80
 ---- batch: 100 ----
mean loss: 162.32
 ---- batch: 110 ----
mean loss: 153.11
train mean loss: 162.79
epoch train time: 0:00:01.882577
elapsed time: 0:04:24.490149
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 23:34:39.743742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.40
 ---- batch: 020 ----
mean loss: 154.25
 ---- batch: 030 ----
mean loss: 160.01
 ---- batch: 040 ----
mean loss: 164.00
 ---- batch: 050 ----
mean loss: 160.42
 ---- batch: 060 ----
mean loss: 163.17
 ---- batch: 070 ----
mean loss: 166.07
 ---- batch: 080 ----
mean loss: 165.31
 ---- batch: 090 ----
mean loss: 166.70
 ---- batch: 100 ----
mean loss: 168.28
 ---- batch: 110 ----
mean loss: 155.75
train mean loss: 161.92
epoch train time: 0:00:01.873704
elapsed time: 0:04:26.364432
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 23:34:41.618019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.52
 ---- batch: 020 ----
mean loss: 146.60
 ---- batch: 030 ----
mean loss: 162.34
 ---- batch: 040 ----
mean loss: 162.04
 ---- batch: 050 ----
mean loss: 165.79
 ---- batch: 060 ----
mean loss: 168.08
 ---- batch: 070 ----
mean loss: 170.53
 ---- batch: 080 ----
mean loss: 162.47
 ---- batch: 090 ----
mean loss: 164.80
 ---- batch: 100 ----
mean loss: 154.37
 ---- batch: 110 ----
mean loss: 158.43
train mean loss: 161.33
epoch train time: 0:00:01.871062
elapsed time: 0:04:28.236032
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 23:34:43.489669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.63
 ---- batch: 020 ----
mean loss: 153.88
 ---- batch: 030 ----
mean loss: 160.57
 ---- batch: 040 ----
mean loss: 166.02
 ---- batch: 050 ----
mean loss: 153.98
 ---- batch: 060 ----
mean loss: 162.76
 ---- batch: 070 ----
mean loss: 165.17
 ---- batch: 080 ----
mean loss: 165.55
 ---- batch: 090 ----
mean loss: 162.46
 ---- batch: 100 ----
mean loss: 158.38
 ---- batch: 110 ----
mean loss: 161.71
train mean loss: 160.38
epoch train time: 0:00:01.879189
elapsed time: 0:04:30.115827
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 23:34:45.369445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.41
 ---- batch: 020 ----
mean loss: 150.95
 ---- batch: 030 ----
mean loss: 156.77
 ---- batch: 040 ----
mean loss: 160.90
 ---- batch: 050 ----
mean loss: 159.33
 ---- batch: 060 ----
mean loss: 158.35
 ---- batch: 070 ----
mean loss: 156.37
 ---- batch: 080 ----
mean loss: 166.60
 ---- batch: 090 ----
mean loss: 161.96
 ---- batch: 100 ----
mean loss: 157.39
 ---- batch: 110 ----
mean loss: 157.41
train mean loss: 158.60
epoch train time: 0:00:01.858839
elapsed time: 0:04:31.975501
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 23:34:47.228871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.21
 ---- batch: 020 ----
mean loss: 158.45
 ---- batch: 030 ----
mean loss: 155.35
 ---- batch: 040 ----
mean loss: 140.94
 ---- batch: 050 ----
mean loss: 171.15
 ---- batch: 060 ----
mean loss: 161.28
 ---- batch: 070 ----
mean loss: 159.74
 ---- batch: 080 ----
mean loss: 162.56
 ---- batch: 090 ----
mean loss: 159.94
 ---- batch: 100 ----
mean loss: 158.90
 ---- batch: 110 ----
mean loss: 163.09
train mean loss: 158.97
epoch train time: 0:00:01.886276
elapsed time: 0:04:33.862123
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 23:34:49.115710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.15
 ---- batch: 020 ----
mean loss: 149.76
 ---- batch: 030 ----
mean loss: 160.27
 ---- batch: 040 ----
mean loss: 155.58
 ---- batch: 050 ----
mean loss: 159.30
 ---- batch: 060 ----
mean loss: 157.85
 ---- batch: 070 ----
mean loss: 158.90
 ---- batch: 080 ----
mean loss: 162.38
 ---- batch: 090 ----
mean loss: 152.82
 ---- batch: 100 ----
mean loss: 164.92
 ---- batch: 110 ----
mean loss: 149.85
train mean loss: 157.96
epoch train time: 0:00:01.864959
elapsed time: 0:04:35.727639
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 23:34:50.981231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.12
 ---- batch: 020 ----
mean loss: 158.97
 ---- batch: 030 ----
mean loss: 154.77
 ---- batch: 040 ----
mean loss: 158.49
 ---- batch: 050 ----
mean loss: 158.87
 ---- batch: 060 ----
mean loss: 159.88
 ---- batch: 070 ----
mean loss: 152.16
 ---- batch: 080 ----
mean loss: 156.38
 ---- batch: 090 ----
mean loss: 157.08
 ---- batch: 100 ----
mean loss: 157.97
 ---- batch: 110 ----
mean loss: 162.04
train mean loss: 157.84
epoch train time: 0:00:01.913569
elapsed time: 0:04:37.641766
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 23:34:52.895408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.93
 ---- batch: 020 ----
mean loss: 152.69
 ---- batch: 030 ----
mean loss: 153.33
 ---- batch: 040 ----
mean loss: 165.25
 ---- batch: 050 ----
mean loss: 153.13
 ---- batch: 060 ----
mean loss: 156.94
 ---- batch: 070 ----
mean loss: 160.89
 ---- batch: 080 ----
mean loss: 161.86
 ---- batch: 090 ----
mean loss: 161.39
 ---- batch: 100 ----
mean loss: 151.67
 ---- batch: 110 ----
mean loss: 155.72
train mean loss: 156.74
epoch train time: 0:00:01.852809
elapsed time: 0:04:39.495214
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 23:34:54.748838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.37
 ---- batch: 020 ----
mean loss: 157.35
 ---- batch: 030 ----
mean loss: 158.04
 ---- batch: 040 ----
mean loss: 159.07
 ---- batch: 050 ----
mean loss: 156.04
 ---- batch: 060 ----
mean loss: 155.54
 ---- batch: 070 ----
mean loss: 154.29
 ---- batch: 080 ----
mean loss: 151.02
 ---- batch: 090 ----
mean loss: 151.31
 ---- batch: 100 ----
mean loss: 160.77
 ---- batch: 110 ----
mean loss: 161.06
train mean loss: 156.34
epoch train time: 0:00:01.887104
elapsed time: 0:04:41.382897
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 23:34:56.636497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.59
 ---- batch: 020 ----
mean loss: 152.78
 ---- batch: 030 ----
mean loss: 153.19
 ---- batch: 040 ----
mean loss: 157.52
 ---- batch: 050 ----
mean loss: 157.20
 ---- batch: 060 ----
mean loss: 161.94
 ---- batch: 070 ----
mean loss: 165.26
 ---- batch: 080 ----
mean loss: 149.82
 ---- batch: 090 ----
mean loss: 150.67
 ---- batch: 100 ----
mean loss: 152.81
 ---- batch: 110 ----
mean loss: 153.87
train mean loss: 155.50
epoch train time: 0:00:01.891569
elapsed time: 0:04:43.275012
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 23:34:58.528598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.32
 ---- batch: 020 ----
mean loss: 150.73
 ---- batch: 030 ----
mean loss: 152.54
 ---- batch: 040 ----
mean loss: 147.84
 ---- batch: 050 ----
mean loss: 155.87
 ---- batch: 060 ----
mean loss: 149.49
 ---- batch: 070 ----
mean loss: 156.80
 ---- batch: 080 ----
mean loss: 151.11
 ---- batch: 090 ----
mean loss: 162.61
 ---- batch: 100 ----
mean loss: 144.58
 ---- batch: 110 ----
mean loss: 160.53
train mean loss: 154.31
epoch train time: 0:00:01.918446
elapsed time: 0:04:45.193987
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 23:35:00.447619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.67
 ---- batch: 020 ----
mean loss: 154.20
 ---- batch: 030 ----
mean loss: 158.26
 ---- batch: 040 ----
mean loss: 153.68
 ---- batch: 050 ----
mean loss: 157.97
 ---- batch: 060 ----
mean loss: 155.50
 ---- batch: 070 ----
mean loss: 152.26
 ---- batch: 080 ----
mean loss: 155.11
 ---- batch: 090 ----
mean loss: 152.83
 ---- batch: 100 ----
mean loss: 156.66
 ---- batch: 110 ----
mean loss: 156.91
train mean loss: 154.99
epoch train time: 0:00:01.884036
elapsed time: 0:04:47.078679
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 23:35:02.332313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.44
 ---- batch: 020 ----
mean loss: 155.15
 ---- batch: 030 ----
mean loss: 157.88
 ---- batch: 040 ----
mean loss: 159.45
 ---- batch: 050 ----
mean loss: 147.83
 ---- batch: 060 ----
mean loss: 152.33
 ---- batch: 070 ----
mean loss: 154.10
 ---- batch: 080 ----
mean loss: 155.13
 ---- batch: 090 ----
mean loss: 154.27
 ---- batch: 100 ----
mean loss: 148.45
 ---- batch: 110 ----
mean loss: 157.24
train mean loss: 154.26
epoch train time: 0:00:01.883183
elapsed time: 0:04:48.962479
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 23:35:04.216096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.86
 ---- batch: 020 ----
mean loss: 151.60
 ---- batch: 030 ----
mean loss: 151.60
 ---- batch: 040 ----
mean loss: 149.81
 ---- batch: 050 ----
mean loss: 148.85
 ---- batch: 060 ----
mean loss: 152.61
 ---- batch: 070 ----
mean loss: 154.45
 ---- batch: 080 ----
mean loss: 155.51
 ---- batch: 090 ----
mean loss: 156.21
 ---- batch: 100 ----
mean loss: 150.87
 ---- batch: 110 ----
mean loss: 149.35
train mean loss: 153.18
epoch train time: 0:00:01.901464
elapsed time: 0:04:50.864528
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 23:35:06.118153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.69
 ---- batch: 020 ----
mean loss: 152.34
 ---- batch: 030 ----
mean loss: 147.29
 ---- batch: 040 ----
mean loss: 150.34
 ---- batch: 050 ----
mean loss: 149.92
 ---- batch: 060 ----
mean loss: 158.60
 ---- batch: 070 ----
mean loss: 148.40
 ---- batch: 080 ----
mean loss: 149.20
 ---- batch: 090 ----
mean loss: 145.26
 ---- batch: 100 ----
mean loss: 165.00
 ---- batch: 110 ----
mean loss: 163.13
train mean loss: 152.03
epoch train time: 0:00:01.858032
elapsed time: 0:04:52.723192
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 23:35:07.976782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.06
 ---- batch: 020 ----
mean loss: 156.13
 ---- batch: 030 ----
mean loss: 143.24
 ---- batch: 040 ----
mean loss: 162.03
 ---- batch: 050 ----
mean loss: 145.03
 ---- batch: 060 ----
mean loss: 155.28
 ---- batch: 070 ----
mean loss: 142.10
 ---- batch: 080 ----
mean loss: 152.69
 ---- batch: 090 ----
mean loss: 145.79
 ---- batch: 100 ----
mean loss: 158.58
 ---- batch: 110 ----
mean loss: 150.94
train mean loss: 151.68
epoch train time: 0:00:01.898170
elapsed time: 0:04:54.621938
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 23:35:09.875558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.21
 ---- batch: 020 ----
mean loss: 154.56
 ---- batch: 030 ----
mean loss: 156.03
 ---- batch: 040 ----
mean loss: 156.06
 ---- batch: 050 ----
mean loss: 156.37
 ---- batch: 060 ----
mean loss: 157.99
 ---- batch: 070 ----
mean loss: 149.49
 ---- batch: 080 ----
mean loss: 147.97
 ---- batch: 090 ----
mean loss: 153.07
 ---- batch: 100 ----
mean loss: 142.31
 ---- batch: 110 ----
mean loss: 151.94
train mean loss: 151.85
epoch train time: 0:00:01.890624
elapsed time: 0:04:56.513153
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 23:35:11.766779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.39
 ---- batch: 020 ----
mean loss: 146.55
 ---- batch: 030 ----
mean loss: 153.66
 ---- batch: 040 ----
mean loss: 157.66
 ---- batch: 050 ----
mean loss: 150.67
 ---- batch: 060 ----
mean loss: 150.04
 ---- batch: 070 ----
mean loss: 150.99
 ---- batch: 080 ----
mean loss: 153.23
 ---- batch: 090 ----
mean loss: 157.60
 ---- batch: 100 ----
mean loss: 150.11
 ---- batch: 110 ----
mean loss: 148.53
train mean loss: 151.18
epoch train time: 0:00:01.914954
elapsed time: 0:04:58.428728
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 23:35:13.682374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.64
 ---- batch: 020 ----
mean loss: 153.28
 ---- batch: 030 ----
mean loss: 148.05
 ---- batch: 040 ----
mean loss: 143.70
 ---- batch: 050 ----
mean loss: 151.43
 ---- batch: 060 ----
mean loss: 150.27
 ---- batch: 070 ----
mean loss: 145.93
 ---- batch: 080 ----
mean loss: 154.76
 ---- batch: 090 ----
mean loss: 151.00
 ---- batch: 100 ----
mean loss: 151.10
 ---- batch: 110 ----
mean loss: 147.66
train mean loss: 150.37
epoch train time: 0:00:01.900734
elapsed time: 0:05:00.330075
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 23:35:15.583740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.49
 ---- batch: 020 ----
mean loss: 157.55
 ---- batch: 030 ----
mean loss: 147.57
 ---- batch: 040 ----
mean loss: 147.80
 ---- batch: 050 ----
mean loss: 144.57
 ---- batch: 060 ----
mean loss: 149.97
 ---- batch: 070 ----
mean loss: 144.42
 ---- batch: 080 ----
mean loss: 153.77
 ---- batch: 090 ----
mean loss: 149.60
 ---- batch: 100 ----
mean loss: 155.26
 ---- batch: 110 ----
mean loss: 155.55
train mean loss: 150.19
epoch train time: 0:00:01.895211
elapsed time: 0:05:02.225912
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 23:35:17.479503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.38
 ---- batch: 020 ----
mean loss: 141.90
 ---- batch: 030 ----
mean loss: 148.16
 ---- batch: 040 ----
mean loss: 146.49
 ---- batch: 050 ----
mean loss: 152.01
 ---- batch: 060 ----
mean loss: 144.24
 ---- batch: 070 ----
mean loss: 155.10
 ---- batch: 080 ----
mean loss: 156.03
 ---- batch: 090 ----
mean loss: 160.44
 ---- batch: 100 ----
mean loss: 152.91
 ---- batch: 110 ----
mean loss: 140.63
train mean loss: 149.62
epoch train time: 0:00:01.876888
elapsed time: 0:05:04.103352
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 23:35:19.357045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.22
 ---- batch: 020 ----
mean loss: 145.89
 ---- batch: 030 ----
mean loss: 153.80
 ---- batch: 040 ----
mean loss: 147.70
 ---- batch: 050 ----
mean loss: 146.84
 ---- batch: 060 ----
mean loss: 146.19
 ---- batch: 070 ----
mean loss: 143.61
 ---- batch: 080 ----
mean loss: 153.39
 ---- batch: 090 ----
mean loss: 151.18
 ---- batch: 100 ----
mean loss: 148.47
 ---- batch: 110 ----
mean loss: 151.68
train mean loss: 148.61
epoch train time: 0:00:01.885034
elapsed time: 0:05:05.989030
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 23:35:21.242683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.89
 ---- batch: 020 ----
mean loss: 153.74
 ---- batch: 030 ----
mean loss: 151.10
 ---- batch: 040 ----
mean loss: 144.22
 ---- batch: 050 ----
mean loss: 151.24
 ---- batch: 060 ----
mean loss: 146.79
 ---- batch: 070 ----
mean loss: 148.93
 ---- batch: 080 ----
mean loss: 151.88
 ---- batch: 090 ----
mean loss: 147.80
 ---- batch: 100 ----
mean loss: 147.52
 ---- batch: 110 ----
mean loss: 150.02
train mean loss: 148.21
epoch train time: 0:00:01.886079
elapsed time: 0:05:07.875720
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 23:35:23.129305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.20
 ---- batch: 020 ----
mean loss: 151.12
 ---- batch: 030 ----
mean loss: 139.75
 ---- batch: 040 ----
mean loss: 148.40
 ---- batch: 050 ----
mean loss: 147.50
 ---- batch: 060 ----
mean loss: 148.41
 ---- batch: 070 ----
mean loss: 148.32
 ---- batch: 080 ----
mean loss: 144.84
 ---- batch: 090 ----
mean loss: 148.09
 ---- batch: 100 ----
mean loss: 145.36
 ---- batch: 110 ----
mean loss: 148.79
train mean loss: 147.14
epoch train time: 0:00:01.885524
elapsed time: 0:05:09.761780
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 23:35:25.015418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.21
 ---- batch: 020 ----
mean loss: 149.09
 ---- batch: 030 ----
mean loss: 140.58
 ---- batch: 040 ----
mean loss: 142.70
 ---- batch: 050 ----
mean loss: 153.45
 ---- batch: 060 ----
mean loss: 147.55
 ---- batch: 070 ----
mean loss: 144.95
 ---- batch: 080 ----
mean loss: 150.21
 ---- batch: 090 ----
mean loss: 151.62
 ---- batch: 100 ----
mean loss: 154.99
 ---- batch: 110 ----
mean loss: 143.41
train mean loss: 147.64
epoch train time: 0:00:01.910606
elapsed time: 0:05:11.672991
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 23:35:26.926597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.70
 ---- batch: 020 ----
mean loss: 149.66
 ---- batch: 030 ----
mean loss: 153.12
 ---- batch: 040 ----
mean loss: 138.48
 ---- batch: 050 ----
mean loss: 146.78
 ---- batch: 060 ----
mean loss: 141.16
 ---- batch: 070 ----
mean loss: 151.46
 ---- batch: 080 ----
mean loss: 154.72
 ---- batch: 090 ----
mean loss: 152.17
 ---- batch: 100 ----
mean loss: 150.37
 ---- batch: 110 ----
mean loss: 143.29
train mean loss: 147.07
epoch train time: 0:00:01.885649
elapsed time: 0:05:13.559511
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 23:35:28.812860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.03
 ---- batch: 020 ----
mean loss: 142.38
 ---- batch: 030 ----
mean loss: 137.96
 ---- batch: 040 ----
mean loss: 135.63
 ---- batch: 050 ----
mean loss: 143.61
 ---- batch: 060 ----
mean loss: 145.62
 ---- batch: 070 ----
mean loss: 152.17
 ---- batch: 080 ----
mean loss: 153.17
 ---- batch: 090 ----
mean loss: 151.87
 ---- batch: 100 ----
mean loss: 153.36
 ---- batch: 110 ----
mean loss: 149.35
train mean loss: 146.90
epoch train time: 0:00:01.872332
elapsed time: 0:05:15.432185
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 23:35:30.685809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.43
 ---- batch: 020 ----
mean loss: 141.12
 ---- batch: 030 ----
mean loss: 145.71
 ---- batch: 040 ----
mean loss: 140.64
 ---- batch: 050 ----
mean loss: 148.42
 ---- batch: 060 ----
mean loss: 155.58
 ---- batch: 070 ----
mean loss: 142.43
 ---- batch: 080 ----
mean loss: 150.32
 ---- batch: 090 ----
mean loss: 147.74
 ---- batch: 100 ----
mean loss: 145.65
 ---- batch: 110 ----
mean loss: 148.35
train mean loss: 146.54
epoch train time: 0:00:01.859161
elapsed time: 0:05:17.291941
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 23:35:32.545253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.38
 ---- batch: 020 ----
mean loss: 142.96
 ---- batch: 030 ----
mean loss: 149.50
 ---- batch: 040 ----
mean loss: 139.69
 ---- batch: 050 ----
mean loss: 143.01
 ---- batch: 060 ----
mean loss: 148.10
 ---- batch: 070 ----
mean loss: 150.05
 ---- batch: 080 ----
mean loss: 150.29
 ---- batch: 090 ----
mean loss: 147.00
 ---- batch: 100 ----
mean loss: 151.17
 ---- batch: 110 ----
mean loss: 142.90
train mean loss: 146.94
epoch train time: 0:00:01.930728
elapsed time: 0:05:19.222938
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 23:35:34.476523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.18
 ---- batch: 020 ----
mean loss: 136.83
 ---- batch: 030 ----
mean loss: 141.56
 ---- batch: 040 ----
mean loss: 139.06
 ---- batch: 050 ----
mean loss: 147.35
 ---- batch: 060 ----
mean loss: 144.39
 ---- batch: 070 ----
mean loss: 146.76
 ---- batch: 080 ----
mean loss: 150.26
 ---- batch: 090 ----
mean loss: 149.76
 ---- batch: 100 ----
mean loss: 147.52
 ---- batch: 110 ----
mean loss: 141.28
train mean loss: 144.97
epoch train time: 0:00:01.900325
elapsed time: 0:05:21.123810
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 23:35:36.377403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.78
 ---- batch: 020 ----
mean loss: 134.37
 ---- batch: 030 ----
mean loss: 143.22
 ---- batch: 040 ----
mean loss: 140.10
 ---- batch: 050 ----
mean loss: 147.13
 ---- batch: 060 ----
mean loss: 147.32
 ---- batch: 070 ----
mean loss: 148.58
 ---- batch: 080 ----
mean loss: 145.23
 ---- batch: 090 ----
mean loss: 143.95
 ---- batch: 100 ----
mean loss: 147.34
 ---- batch: 110 ----
mean loss: 145.35
train mean loss: 144.62
epoch train time: 0:00:01.882759
elapsed time: 0:05:23.007142
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 23:35:38.260760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.11
 ---- batch: 020 ----
mean loss: 139.36
 ---- batch: 030 ----
mean loss: 139.23
 ---- batch: 040 ----
mean loss: 142.36
 ---- batch: 050 ----
mean loss: 143.58
 ---- batch: 060 ----
mean loss: 144.70
 ---- batch: 070 ----
mean loss: 145.08
 ---- batch: 080 ----
mean loss: 141.11
 ---- batch: 090 ----
mean loss: 152.44
 ---- batch: 100 ----
mean loss: 141.37
 ---- batch: 110 ----
mean loss: 151.75
train mean loss: 143.97
epoch train time: 0:00:01.923312
elapsed time: 0:05:24.931052
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 23:35:40.184659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.34
 ---- batch: 020 ----
mean loss: 137.61
 ---- batch: 030 ----
mean loss: 147.53
 ---- batch: 040 ----
mean loss: 141.45
 ---- batch: 050 ----
mean loss: 138.18
 ---- batch: 060 ----
mean loss: 138.85
 ---- batch: 070 ----
mean loss: 151.03
 ---- batch: 080 ----
mean loss: 146.13
 ---- batch: 090 ----
mean loss: 147.90
 ---- batch: 100 ----
mean loss: 150.05
 ---- batch: 110 ----
mean loss: 145.01
train mean loss: 144.14
epoch train time: 0:00:01.890503
elapsed time: 0:05:26.822140
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 23:35:42.075731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.59
 ---- batch: 020 ----
mean loss: 132.18
 ---- batch: 030 ----
mean loss: 152.59
 ---- batch: 040 ----
mean loss: 149.40
 ---- batch: 050 ----
mean loss: 137.44
 ---- batch: 060 ----
mean loss: 141.64
 ---- batch: 070 ----
mean loss: 145.03
 ---- batch: 080 ----
mean loss: 143.39
 ---- batch: 090 ----
mean loss: 137.50
 ---- batch: 100 ----
mean loss: 150.28
 ---- batch: 110 ----
mean loss: 145.45
train mean loss: 143.44
epoch train time: 0:00:01.898305
elapsed time: 0:05:28.721019
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 23:35:43.974669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.00
 ---- batch: 020 ----
mean loss: 138.24
 ---- batch: 030 ----
mean loss: 145.42
 ---- batch: 040 ----
mean loss: 139.80
 ---- batch: 050 ----
mean loss: 136.53
 ---- batch: 060 ----
mean loss: 147.68
 ---- batch: 070 ----
mean loss: 135.77
 ---- batch: 080 ----
mean loss: 138.73
 ---- batch: 090 ----
mean loss: 151.46
 ---- batch: 100 ----
mean loss: 147.14
 ---- batch: 110 ----
mean loss: 151.77
train mean loss: 142.80
epoch train time: 0:00:01.902049
elapsed time: 0:05:30.623806
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 23:35:45.877518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.98
 ---- batch: 020 ----
mean loss: 142.06
 ---- batch: 030 ----
mean loss: 138.71
 ---- batch: 040 ----
mean loss: 139.07
 ---- batch: 050 ----
mean loss: 143.71
 ---- batch: 060 ----
mean loss: 141.93
 ---- batch: 070 ----
mean loss: 144.89
 ---- batch: 080 ----
mean loss: 141.16
 ---- batch: 090 ----
mean loss: 134.13
 ---- batch: 100 ----
mean loss: 148.34
 ---- batch: 110 ----
mean loss: 149.90
train mean loss: 142.79
epoch train time: 0:00:01.898452
elapsed time: 0:05:32.522939
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 23:35:47.776538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.11
 ---- batch: 020 ----
mean loss: 140.84
 ---- batch: 030 ----
mean loss: 139.07
 ---- batch: 040 ----
mean loss: 147.71
 ---- batch: 050 ----
mean loss: 142.13
 ---- batch: 060 ----
mean loss: 135.93
 ---- batch: 070 ----
mean loss: 146.09
 ---- batch: 080 ----
mean loss: 144.71
 ---- batch: 090 ----
mean loss: 141.66
 ---- batch: 100 ----
mean loss: 147.65
 ---- batch: 110 ----
mean loss: 137.96
train mean loss: 142.52
epoch train time: 0:00:01.887264
elapsed time: 0:05:34.410848
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 23:35:49.664466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.39
 ---- batch: 020 ----
mean loss: 135.64
 ---- batch: 030 ----
mean loss: 136.21
 ---- batch: 040 ----
mean loss: 148.85
 ---- batch: 050 ----
mean loss: 152.00
 ---- batch: 060 ----
mean loss: 140.45
 ---- batch: 070 ----
mean loss: 134.08
 ---- batch: 080 ----
mean loss: 151.59
 ---- batch: 090 ----
mean loss: 150.10
 ---- batch: 100 ----
mean loss: 135.56
 ---- batch: 110 ----
mean loss: 137.26
train mean loss: 142.03
epoch train time: 0:00:01.881292
elapsed time: 0:05:36.292707
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 23:35:51.546319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.78
 ---- batch: 020 ----
mean loss: 141.84
 ---- batch: 030 ----
mean loss: 138.22
 ---- batch: 040 ----
mean loss: 143.48
 ---- batch: 050 ----
mean loss: 148.32
 ---- batch: 060 ----
mean loss: 143.72
 ---- batch: 070 ----
mean loss: 138.81
 ---- batch: 080 ----
mean loss: 141.32
 ---- batch: 090 ----
mean loss: 141.09
 ---- batch: 100 ----
mean loss: 147.90
 ---- batch: 110 ----
mean loss: 143.21
train mean loss: 141.58
epoch train time: 0:00:01.913072
elapsed time: 0:05:38.206352
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 23:35:53.459947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.30
 ---- batch: 020 ----
mean loss: 145.63
 ---- batch: 030 ----
mean loss: 143.64
 ---- batch: 040 ----
mean loss: 136.42
 ---- batch: 050 ----
mean loss: 150.83
 ---- batch: 060 ----
mean loss: 146.99
 ---- batch: 070 ----
mean loss: 136.62
 ---- batch: 080 ----
mean loss: 124.75
 ---- batch: 090 ----
mean loss: 140.59
 ---- batch: 100 ----
mean loss: 148.41
 ---- batch: 110 ----
mean loss: 144.63
train mean loss: 141.62
epoch train time: 0:00:01.905603
elapsed time: 0:05:40.112574
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 23:35:55.366337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.14
 ---- batch: 020 ----
mean loss: 138.20
 ---- batch: 030 ----
mean loss: 142.46
 ---- batch: 040 ----
mean loss: 141.35
 ---- batch: 050 ----
mean loss: 141.31
 ---- batch: 060 ----
mean loss: 138.05
 ---- batch: 070 ----
mean loss: 138.76
 ---- batch: 080 ----
mean loss: 139.42
 ---- batch: 090 ----
mean loss: 142.25
 ---- batch: 100 ----
mean loss: 132.98
 ---- batch: 110 ----
mean loss: 141.20
train mean loss: 140.06
epoch train time: 0:00:01.858239
elapsed time: 0:05:41.971617
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 23:35:57.226281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.05
 ---- batch: 020 ----
mean loss: 144.41
 ---- batch: 030 ----
mean loss: 139.24
 ---- batch: 040 ----
mean loss: 138.42
 ---- batch: 050 ----
mean loss: 140.64
 ---- batch: 060 ----
mean loss: 135.96
 ---- batch: 070 ----
mean loss: 151.20
 ---- batch: 080 ----
mean loss: 138.09
 ---- batch: 090 ----
mean loss: 131.02
 ---- batch: 100 ----
mean loss: 144.57
 ---- batch: 110 ----
mean loss: 141.95
train mean loss: 140.65
epoch train time: 0:00:01.893811
elapsed time: 0:05:43.867054
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 23:35:59.120706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.14
 ---- batch: 020 ----
mean loss: 135.00
 ---- batch: 030 ----
mean loss: 142.25
 ---- batch: 040 ----
mean loss: 140.24
 ---- batch: 050 ----
mean loss: 151.41
 ---- batch: 060 ----
mean loss: 134.71
 ---- batch: 070 ----
mean loss: 140.74
 ---- batch: 080 ----
mean loss: 142.64
 ---- batch: 090 ----
mean loss: 145.37
 ---- batch: 100 ----
mean loss: 138.17
 ---- batch: 110 ----
mean loss: 128.06
train mean loss: 140.16
epoch train time: 0:00:01.897287
elapsed time: 0:05:45.765005
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 23:36:01.018657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.69
 ---- batch: 020 ----
mean loss: 140.80
 ---- batch: 030 ----
mean loss: 137.17
 ---- batch: 040 ----
mean loss: 132.71
 ---- batch: 050 ----
mean loss: 136.37
 ---- batch: 060 ----
mean loss: 136.57
 ---- batch: 070 ----
mean loss: 129.80
 ---- batch: 080 ----
mean loss: 152.90
 ---- batch: 090 ----
mean loss: 142.73
 ---- batch: 100 ----
mean loss: 156.62
 ---- batch: 110 ----
mean loss: 135.20
train mean loss: 139.53
epoch train time: 0:00:01.902102
elapsed time: 0:05:47.667742
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 23:36:02.921341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.20
 ---- batch: 020 ----
mean loss: 139.49
 ---- batch: 030 ----
mean loss: 132.84
 ---- batch: 040 ----
mean loss: 132.59
 ---- batch: 050 ----
mean loss: 138.90
 ---- batch: 060 ----
mean loss: 134.09
 ---- batch: 070 ----
mean loss: 142.19
 ---- batch: 080 ----
mean loss: 137.90
 ---- batch: 090 ----
mean loss: 147.01
 ---- batch: 100 ----
mean loss: 143.75
 ---- batch: 110 ----
mean loss: 145.50
train mean loss: 139.23
epoch train time: 0:00:01.886117
elapsed time: 0:05:49.554490
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 23:36:04.808093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.33
 ---- batch: 020 ----
mean loss: 138.08
 ---- batch: 030 ----
mean loss: 131.36
 ---- batch: 040 ----
mean loss: 149.88
 ---- batch: 050 ----
mean loss: 147.41
 ---- batch: 060 ----
mean loss: 131.34
 ---- batch: 070 ----
mean loss: 128.39
 ---- batch: 080 ----
mean loss: 138.61
 ---- batch: 090 ----
mean loss: 138.66
 ---- batch: 100 ----
mean loss: 140.51
 ---- batch: 110 ----
mean loss: 138.62
train mean loss: 138.11
epoch train time: 0:00:01.910056
elapsed time: 0:05:51.465195
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 23:36:06.718811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.65
 ---- batch: 020 ----
mean loss: 137.36
 ---- batch: 030 ----
mean loss: 126.05
 ---- batch: 040 ----
mean loss: 142.53
 ---- batch: 050 ----
mean loss: 139.75
 ---- batch: 060 ----
mean loss: 140.55
 ---- batch: 070 ----
mean loss: 140.76
 ---- batch: 080 ----
mean loss: 144.36
 ---- batch: 090 ----
mean loss: 140.72
 ---- batch: 100 ----
mean loss: 144.53
 ---- batch: 110 ----
mean loss: 134.81
train mean loss: 138.89
epoch train time: 0:00:01.895929
elapsed time: 0:05:53.361706
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 23:36:08.615330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.29
 ---- batch: 020 ----
mean loss: 138.19
 ---- batch: 030 ----
mean loss: 137.07
 ---- batch: 040 ----
mean loss: 137.50
 ---- batch: 050 ----
mean loss: 139.18
 ---- batch: 060 ----
mean loss: 139.69
 ---- batch: 070 ----
mean loss: 134.62
 ---- batch: 080 ----
mean loss: 143.93
 ---- batch: 090 ----
mean loss: 145.37
 ---- batch: 100 ----
mean loss: 132.75
 ---- batch: 110 ----
mean loss: 135.47
train mean loss: 138.23
epoch train time: 0:00:01.923683
elapsed time: 0:05:55.285994
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 23:36:10.539601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.12
 ---- batch: 020 ----
mean loss: 145.64
 ---- batch: 030 ----
mean loss: 144.47
 ---- batch: 040 ----
mean loss: 136.42
 ---- batch: 050 ----
mean loss: 134.62
 ---- batch: 060 ----
mean loss: 129.75
 ---- batch: 070 ----
mean loss: 146.92
 ---- batch: 080 ----
mean loss: 136.39
 ---- batch: 090 ----
mean loss: 144.88
 ---- batch: 100 ----
mean loss: 138.44
 ---- batch: 110 ----
mean loss: 135.22
train mean loss: 138.55
epoch train time: 0:00:01.887043
elapsed time: 0:05:57.173616
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 23:36:12.427228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.65
 ---- batch: 020 ----
mean loss: 135.24
 ---- batch: 030 ----
mean loss: 135.58
 ---- batch: 040 ----
mean loss: 131.09
 ---- batch: 050 ----
mean loss: 136.62
 ---- batch: 060 ----
mean loss: 136.26
 ---- batch: 070 ----
mean loss: 134.63
 ---- batch: 080 ----
mean loss: 139.94
 ---- batch: 090 ----
mean loss: 145.31
 ---- batch: 100 ----
mean loss: 142.33
 ---- batch: 110 ----
mean loss: 145.40
train mean loss: 137.40
epoch train time: 0:00:01.864761
elapsed time: 0:05:59.038941
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 23:36:14.292539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.42
 ---- batch: 020 ----
mean loss: 132.32
 ---- batch: 030 ----
mean loss: 137.59
 ---- batch: 040 ----
mean loss: 135.55
 ---- batch: 050 ----
mean loss: 133.19
 ---- batch: 060 ----
mean loss: 131.20
 ---- batch: 070 ----
mean loss: 139.66
 ---- batch: 080 ----
mean loss: 130.31
 ---- batch: 090 ----
mean loss: 147.22
 ---- batch: 100 ----
mean loss: 134.56
 ---- batch: 110 ----
mean loss: 144.90
train mean loss: 137.33
epoch train time: 0:00:01.873839
elapsed time: 0:06:00.913612
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 23:36:16.166964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.45
 ---- batch: 020 ----
mean loss: 137.58
 ---- batch: 030 ----
mean loss: 134.74
 ---- batch: 040 ----
mean loss: 139.64
 ---- batch: 050 ----
mean loss: 139.90
 ---- batch: 060 ----
mean loss: 133.29
 ---- batch: 070 ----
mean loss: 139.48
 ---- batch: 080 ----
mean loss: 135.98
 ---- batch: 090 ----
mean loss: 136.55
 ---- batch: 100 ----
mean loss: 140.51
 ---- batch: 110 ----
mean loss: 137.52
train mean loss: 136.73
epoch train time: 0:00:01.884671
elapsed time: 0:06:02.798623
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 23:36:18.052233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.44
 ---- batch: 020 ----
mean loss: 135.32
 ---- batch: 030 ----
mean loss: 132.59
 ---- batch: 040 ----
mean loss: 136.13
 ---- batch: 050 ----
mean loss: 133.21
 ---- batch: 060 ----
mean loss: 136.67
 ---- batch: 070 ----
mean loss: 140.84
 ---- batch: 080 ----
mean loss: 135.66
 ---- batch: 090 ----
mean loss: 136.93
 ---- batch: 100 ----
mean loss: 135.27
 ---- batch: 110 ----
mean loss: 133.01
train mean loss: 136.16
epoch train time: 0:00:01.923864
elapsed time: 0:06:04.723082
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 23:36:19.976678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.86
 ---- batch: 020 ----
mean loss: 133.51
 ---- batch: 030 ----
mean loss: 127.78
 ---- batch: 040 ----
mean loss: 143.96
 ---- batch: 050 ----
mean loss: 139.84
 ---- batch: 060 ----
mean loss: 134.60
 ---- batch: 070 ----
mean loss: 134.85
 ---- batch: 080 ----
mean loss: 138.15
 ---- batch: 090 ----
mean loss: 130.95
 ---- batch: 100 ----
mean loss: 137.42
 ---- batch: 110 ----
mean loss: 137.81
train mean loss: 135.71
epoch train time: 0:00:01.859219
elapsed time: 0:06:06.582859
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 23:36:21.836463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.55
 ---- batch: 020 ----
mean loss: 140.60
 ---- batch: 030 ----
mean loss: 140.03
 ---- batch: 040 ----
mean loss: 132.18
 ---- batch: 050 ----
mean loss: 135.63
 ---- batch: 060 ----
mean loss: 135.37
 ---- batch: 070 ----
mean loss: 128.79
 ---- batch: 080 ----
mean loss: 134.69
 ---- batch: 090 ----
mean loss: 139.28
 ---- batch: 100 ----
mean loss: 141.99
 ---- batch: 110 ----
mean loss: 130.06
train mean loss: 135.72
epoch train time: 0:00:01.879872
elapsed time: 0:06:08.463340
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 23:36:23.716993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.75
 ---- batch: 020 ----
mean loss: 142.72
 ---- batch: 030 ----
mean loss: 138.35
 ---- batch: 040 ----
mean loss: 129.89
 ---- batch: 050 ----
mean loss: 130.86
 ---- batch: 060 ----
mean loss: 135.99
 ---- batch: 070 ----
mean loss: 129.00
 ---- batch: 080 ----
mean loss: 135.79
 ---- batch: 090 ----
mean loss: 137.55
 ---- batch: 100 ----
mean loss: 133.46
 ---- batch: 110 ----
mean loss: 141.13
train mean loss: 135.52
epoch train time: 0:00:01.887648
elapsed time: 0:06:10.351656
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 23:36:25.605262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.40
 ---- batch: 020 ----
mean loss: 127.97
 ---- batch: 030 ----
mean loss: 130.50
 ---- batch: 040 ----
mean loss: 142.05
 ---- batch: 050 ----
mean loss: 137.60
 ---- batch: 060 ----
mean loss: 132.23
 ---- batch: 070 ----
mean loss: 133.55
 ---- batch: 080 ----
mean loss: 136.25
 ---- batch: 090 ----
mean loss: 135.04
 ---- batch: 100 ----
mean loss: 135.45
 ---- batch: 110 ----
mean loss: 142.77
train mean loss: 134.91
epoch train time: 0:00:01.884478
elapsed time: 0:06:12.236714
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 23:36:27.490363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.58
 ---- batch: 020 ----
mean loss: 136.65
 ---- batch: 030 ----
mean loss: 126.31
 ---- batch: 040 ----
mean loss: 132.27
 ---- batch: 050 ----
mean loss: 144.85
 ---- batch: 060 ----
mean loss: 138.81
 ---- batch: 070 ----
mean loss: 141.35
 ---- batch: 080 ----
mean loss: 138.11
 ---- batch: 090 ----
mean loss: 130.22
 ---- batch: 100 ----
mean loss: 138.79
 ---- batch: 110 ----
mean loss: 136.59
train mean loss: 135.16
epoch train time: 0:00:01.858275
elapsed time: 0:06:14.095598
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 23:36:29.349194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.50
 ---- batch: 020 ----
mean loss: 132.90
 ---- batch: 030 ----
mean loss: 146.87
 ---- batch: 040 ----
mean loss: 123.18
 ---- batch: 050 ----
mean loss: 131.68
 ---- batch: 060 ----
mean loss: 125.12
 ---- batch: 070 ----
mean loss: 131.38
 ---- batch: 080 ----
mean loss: 137.96
 ---- batch: 090 ----
mean loss: 138.17
 ---- batch: 100 ----
mean loss: 140.14
 ---- batch: 110 ----
mean loss: 138.61
train mean loss: 134.16
epoch train time: 0:00:01.895624
elapsed time: 0:06:15.991800
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 23:36:31.245452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.36
 ---- batch: 020 ----
mean loss: 134.12
 ---- batch: 030 ----
mean loss: 129.73
 ---- batch: 040 ----
mean loss: 135.97
 ---- batch: 050 ----
mean loss: 132.80
 ---- batch: 060 ----
mean loss: 139.89
 ---- batch: 070 ----
mean loss: 135.36
 ---- batch: 080 ----
mean loss: 130.86
 ---- batch: 090 ----
mean loss: 136.08
 ---- batch: 100 ----
mean loss: 137.90
 ---- batch: 110 ----
mean loss: 130.15
train mean loss: 134.41
epoch train time: 0:00:01.888740
elapsed time: 0:06:17.881143
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 23:36:33.134736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.87
 ---- batch: 020 ----
mean loss: 136.77
 ---- batch: 030 ----
mean loss: 122.78
 ---- batch: 040 ----
mean loss: 132.64
 ---- batch: 050 ----
mean loss: 126.63
 ---- batch: 060 ----
mean loss: 129.53
 ---- batch: 070 ----
mean loss: 140.47
 ---- batch: 080 ----
mean loss: 134.85
 ---- batch: 090 ----
mean loss: 141.85
 ---- batch: 100 ----
mean loss: 140.57
 ---- batch: 110 ----
mean loss: 129.56
train mean loss: 133.49
epoch train time: 0:00:01.886207
elapsed time: 0:06:19.767923
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 23:36:35.021524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.34
 ---- batch: 020 ----
mean loss: 128.87
 ---- batch: 030 ----
mean loss: 135.46
 ---- batch: 040 ----
mean loss: 130.20
 ---- batch: 050 ----
mean loss: 136.03
 ---- batch: 060 ----
mean loss: 137.26
 ---- batch: 070 ----
mean loss: 133.16
 ---- batch: 080 ----
mean loss: 135.00
 ---- batch: 090 ----
mean loss: 123.50
 ---- batch: 100 ----
mean loss: 136.30
 ---- batch: 110 ----
mean loss: 140.38
train mean loss: 133.03
epoch train time: 0:00:01.885060
elapsed time: 0:06:21.653544
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 23:36:36.907148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.89
 ---- batch: 020 ----
mean loss: 127.98
 ---- batch: 030 ----
mean loss: 136.46
 ---- batch: 040 ----
mean loss: 132.62
 ---- batch: 050 ----
mean loss: 134.42
 ---- batch: 060 ----
mean loss: 134.37
 ---- batch: 070 ----
mean loss: 133.48
 ---- batch: 080 ----
mean loss: 123.46
 ---- batch: 090 ----
mean loss: 131.36
 ---- batch: 100 ----
mean loss: 137.35
 ---- batch: 110 ----
mean loss: 138.98
train mean loss: 132.79
epoch train time: 0:00:01.878892
elapsed time: 0:06:23.533043
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 23:36:38.786681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.59
 ---- batch: 020 ----
mean loss: 134.77
 ---- batch: 030 ----
mean loss: 140.47
 ---- batch: 040 ----
mean loss: 132.68
 ---- batch: 050 ----
mean loss: 133.18
 ---- batch: 060 ----
mean loss: 133.07
 ---- batch: 070 ----
mean loss: 129.14
 ---- batch: 080 ----
mean loss: 131.53
 ---- batch: 090 ----
mean loss: 128.80
 ---- batch: 100 ----
mean loss: 130.97
 ---- batch: 110 ----
mean loss: 134.18
train mean loss: 132.59
epoch train time: 0:00:01.907445
elapsed time: 0:06:25.441125
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 23:36:40.694772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.02
 ---- batch: 020 ----
mean loss: 126.62
 ---- batch: 030 ----
mean loss: 129.52
 ---- batch: 040 ----
mean loss: 136.90
 ---- batch: 050 ----
mean loss: 134.59
 ---- batch: 060 ----
mean loss: 141.24
 ---- batch: 070 ----
mean loss: 127.88
 ---- batch: 080 ----
mean loss: 124.86
 ---- batch: 090 ----
mean loss: 136.72
 ---- batch: 100 ----
mean loss: 135.84
 ---- batch: 110 ----
mean loss: 134.02
train mean loss: 131.99
epoch train time: 0:00:01.913789
elapsed time: 0:06:27.355568
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 23:36:42.609143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.49
 ---- batch: 020 ----
mean loss: 136.34
 ---- batch: 030 ----
mean loss: 137.79
 ---- batch: 040 ----
mean loss: 122.00
 ---- batch: 050 ----
mean loss: 128.10
 ---- batch: 060 ----
mean loss: 131.43
 ---- batch: 070 ----
mean loss: 129.96
 ---- batch: 080 ----
mean loss: 129.12
 ---- batch: 090 ----
mean loss: 137.54
 ---- batch: 100 ----
mean loss: 136.85
 ---- batch: 110 ----
mean loss: 130.27
train mean loss: 131.77
epoch train time: 0:00:01.883549
elapsed time: 0:06:29.239665
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 23:36:44.493243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.31
 ---- batch: 020 ----
mean loss: 126.61
 ---- batch: 030 ----
mean loss: 141.86
 ---- batch: 040 ----
mean loss: 130.01
 ---- batch: 050 ----
mean loss: 127.39
 ---- batch: 060 ----
mean loss: 132.11
 ---- batch: 070 ----
mean loss: 126.63
 ---- batch: 080 ----
mean loss: 136.94
 ---- batch: 090 ----
mean loss: 132.91
 ---- batch: 100 ----
mean loss: 127.38
 ---- batch: 110 ----
mean loss: 131.86
train mean loss: 131.36
epoch train time: 0:00:01.940173
elapsed time: 0:06:31.180423
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 23:36:46.434027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.67
 ---- batch: 020 ----
mean loss: 127.62
 ---- batch: 030 ----
mean loss: 133.13
 ---- batch: 040 ----
mean loss: 123.28
 ---- batch: 050 ----
mean loss: 130.01
 ---- batch: 060 ----
mean loss: 138.04
 ---- batch: 070 ----
mean loss: 141.64
 ---- batch: 080 ----
mean loss: 130.45
 ---- batch: 090 ----
mean loss: 129.16
 ---- batch: 100 ----
mean loss: 131.81
 ---- batch: 110 ----
mean loss: 138.18
train mean loss: 131.42
epoch train time: 0:00:01.980376
elapsed time: 0:06:33.161399
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 23:36:48.415003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.16
 ---- batch: 020 ----
mean loss: 122.56
 ---- batch: 030 ----
mean loss: 132.77
 ---- batch: 040 ----
mean loss: 129.83
 ---- batch: 050 ----
mean loss: 133.89
 ---- batch: 060 ----
mean loss: 137.39
 ---- batch: 070 ----
mean loss: 132.30
 ---- batch: 080 ----
mean loss: 126.86
 ---- batch: 090 ----
mean loss: 134.68
 ---- batch: 100 ----
mean loss: 134.68
 ---- batch: 110 ----
mean loss: 129.37
train mean loss: 131.02
epoch train time: 0:00:01.956477
elapsed time: 0:06:35.118447
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 23:36:50.372094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.11
 ---- batch: 020 ----
mean loss: 128.80
 ---- batch: 030 ----
mean loss: 132.74
 ---- batch: 040 ----
mean loss: 125.85
 ---- batch: 050 ----
mean loss: 133.36
 ---- batch: 060 ----
mean loss: 134.33
 ---- batch: 070 ----
mean loss: 123.68
 ---- batch: 080 ----
mean loss: 130.45
 ---- batch: 090 ----
mean loss: 129.32
 ---- batch: 100 ----
mean loss: 128.50
 ---- batch: 110 ----
mean loss: 138.22
train mean loss: 130.52
epoch train time: 0:00:01.947220
elapsed time: 0:06:37.066280
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 23:36:52.319873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.84
 ---- batch: 020 ----
mean loss: 130.43
 ---- batch: 030 ----
mean loss: 127.63
 ---- batch: 040 ----
mean loss: 133.72
 ---- batch: 050 ----
mean loss: 130.00
 ---- batch: 060 ----
mean loss: 123.37
 ---- batch: 070 ----
mean loss: 127.76
 ---- batch: 080 ----
mean loss: 130.88
 ---- batch: 090 ----
mean loss: 126.90
 ---- batch: 100 ----
mean loss: 141.95
 ---- batch: 110 ----
mean loss: 139.26
train mean loss: 130.92
epoch train time: 0:00:01.956893
elapsed time: 0:06:39.023751
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 23:36:54.277345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.46
 ---- batch: 020 ----
mean loss: 137.60
 ---- batch: 030 ----
mean loss: 132.21
 ---- batch: 040 ----
mean loss: 131.44
 ---- batch: 050 ----
mean loss: 124.25
 ---- batch: 060 ----
mean loss: 122.06
 ---- batch: 070 ----
mean loss: 133.11
 ---- batch: 080 ----
mean loss: 128.78
 ---- batch: 090 ----
mean loss: 131.11
 ---- batch: 100 ----
mean loss: 131.25
 ---- batch: 110 ----
mean loss: 130.92
train mean loss: 129.64
epoch train time: 0:00:01.927024
elapsed time: 0:06:40.951339
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 23:36:56.204986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.07
 ---- batch: 020 ----
mean loss: 131.75
 ---- batch: 030 ----
mean loss: 135.82
 ---- batch: 040 ----
mean loss: 130.81
 ---- batch: 050 ----
mean loss: 123.30
 ---- batch: 060 ----
mean loss: 127.32
 ---- batch: 070 ----
mean loss: 132.09
 ---- batch: 080 ----
mean loss: 127.13
 ---- batch: 090 ----
mean loss: 133.09
 ---- batch: 100 ----
mean loss: 130.85
 ---- batch: 110 ----
mean loss: 133.19
train mean loss: 129.66
epoch train time: 0:00:01.912318
elapsed time: 0:06:42.864286
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 23:36:58.117881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.95
 ---- batch: 020 ----
mean loss: 122.25
 ---- batch: 030 ----
mean loss: 123.90
 ---- batch: 040 ----
mean loss: 134.12
 ---- batch: 050 ----
mean loss: 120.92
 ---- batch: 060 ----
mean loss: 125.00
 ---- batch: 070 ----
mean loss: 134.91
 ---- batch: 080 ----
mean loss: 127.56
 ---- batch: 090 ----
mean loss: 131.02
 ---- batch: 100 ----
mean loss: 127.39
 ---- batch: 110 ----
mean loss: 139.24
train mean loss: 129.23
epoch train time: 0:00:01.891345
elapsed time: 0:06:44.756237
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 23:37:00.009927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.61
 ---- batch: 020 ----
mean loss: 127.85
 ---- batch: 030 ----
mean loss: 125.98
 ---- batch: 040 ----
mean loss: 127.35
 ---- batch: 050 ----
mean loss: 130.29
 ---- batch: 060 ----
mean loss: 120.53
 ---- batch: 070 ----
mean loss: 139.74
 ---- batch: 080 ----
mean loss: 132.36
 ---- batch: 090 ----
mean loss: 133.96
 ---- batch: 100 ----
mean loss: 130.11
 ---- batch: 110 ----
mean loss: 126.53
train mean loss: 129.21
epoch train time: 0:00:01.895390
elapsed time: 0:06:46.652320
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 23:37:01.905959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.23
 ---- batch: 020 ----
mean loss: 123.03
 ---- batch: 030 ----
mean loss: 128.44
 ---- batch: 040 ----
mean loss: 127.24
 ---- batch: 050 ----
mean loss: 129.02
 ---- batch: 060 ----
mean loss: 132.09
 ---- batch: 070 ----
mean loss: 123.63
 ---- batch: 080 ----
mean loss: 136.81
 ---- batch: 090 ----
mean loss: 127.83
 ---- batch: 100 ----
mean loss: 125.55
 ---- batch: 110 ----
mean loss: 135.98
train mean loss: 129.19
epoch train time: 0:00:01.915151
elapsed time: 0:06:48.568104
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 23:37:03.821741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.93
 ---- batch: 020 ----
mean loss: 123.53
 ---- batch: 030 ----
mean loss: 134.13
 ---- batch: 040 ----
mean loss: 121.32
 ---- batch: 050 ----
mean loss: 128.27
 ---- batch: 060 ----
mean loss: 140.76
 ---- batch: 070 ----
mean loss: 128.22
 ---- batch: 080 ----
mean loss: 127.84
 ---- batch: 090 ----
mean loss: 125.40
 ---- batch: 100 ----
mean loss: 126.98
 ---- batch: 110 ----
mean loss: 134.80
train mean loss: 129.06
epoch train time: 0:00:01.898153
elapsed time: 0:06:50.466870
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 23:37:05.720461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.15
 ---- batch: 020 ----
mean loss: 120.02
 ---- batch: 030 ----
mean loss: 124.35
 ---- batch: 040 ----
mean loss: 123.39
 ---- batch: 050 ----
mean loss: 129.90
 ---- batch: 060 ----
mean loss: 131.96
 ---- batch: 070 ----
mean loss: 127.13
 ---- batch: 080 ----
mean loss: 125.68
 ---- batch: 090 ----
mean loss: 132.60
 ---- batch: 100 ----
mean loss: 132.11
 ---- batch: 110 ----
mean loss: 138.61
train mean loss: 128.64
epoch train time: 0:00:01.916357
elapsed time: 0:06:52.383789
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 23:37:07.637391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.60
 ---- batch: 020 ----
mean loss: 122.42
 ---- batch: 030 ----
mean loss: 128.25
 ---- batch: 040 ----
mean loss: 123.51
 ---- batch: 050 ----
mean loss: 119.29
 ---- batch: 060 ----
mean loss: 122.20
 ---- batch: 070 ----
mean loss: 119.32
 ---- batch: 080 ----
mean loss: 127.33
 ---- batch: 090 ----
mean loss: 125.35
 ---- batch: 100 ----
mean loss: 128.90
 ---- batch: 110 ----
mean loss: 119.51
train mean loss: 123.42
epoch train time: 0:00:01.888007
elapsed time: 0:06:54.272746
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 23:37:09.526025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.70
 ---- batch: 020 ----
mean loss: 121.00
 ---- batch: 030 ----
mean loss: 113.18
 ---- batch: 040 ----
mean loss: 124.17
 ---- batch: 050 ----
mean loss: 121.53
 ---- batch: 060 ----
mean loss: 123.44
 ---- batch: 070 ----
mean loss: 114.54
 ---- batch: 080 ----
mean loss: 127.17
 ---- batch: 090 ----
mean loss: 130.66
 ---- batch: 100 ----
mean loss: 124.32
 ---- batch: 110 ----
mean loss: 116.40
train mean loss: 122.69
epoch train time: 0:00:01.899556
elapsed time: 0:06:56.172598
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 23:37:11.426227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.46
 ---- batch: 020 ----
mean loss: 121.41
 ---- batch: 030 ----
mean loss: 125.02
 ---- batch: 040 ----
mean loss: 120.23
 ---- batch: 050 ----
mean loss: 123.49
 ---- batch: 060 ----
mean loss: 123.22
 ---- batch: 070 ----
mean loss: 120.02
 ---- batch: 080 ----
mean loss: 128.12
 ---- batch: 090 ----
mean loss: 121.71
 ---- batch: 100 ----
mean loss: 123.86
 ---- batch: 110 ----
mean loss: 118.80
train mean loss: 122.39
epoch train time: 0:00:01.887278
elapsed time: 0:06:58.060507
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 23:37:13.313823
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.23
 ---- batch: 020 ----
mean loss: 119.81
 ---- batch: 030 ----
mean loss: 118.37
 ---- batch: 040 ----
mean loss: 120.04
 ---- batch: 050 ----
mean loss: 129.13
 ---- batch: 060 ----
mean loss: 122.78
 ---- batch: 070 ----
mean loss: 126.39
 ---- batch: 080 ----
mean loss: 122.14
 ---- batch: 090 ----
mean loss: 127.31
 ---- batch: 100 ----
mean loss: 115.75
 ---- batch: 110 ----
mean loss: 118.12
train mean loss: 122.33
epoch train time: 0:00:01.897215
elapsed time: 0:06:59.958038
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 23:37:15.211694
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.56
 ---- batch: 020 ----
mean loss: 118.22
 ---- batch: 030 ----
mean loss: 123.39
 ---- batch: 040 ----
mean loss: 116.37
 ---- batch: 050 ----
mean loss: 115.69
 ---- batch: 060 ----
mean loss: 128.80
 ---- batch: 070 ----
mean loss: 124.72
 ---- batch: 080 ----
mean loss: 130.46
 ---- batch: 090 ----
mean loss: 114.13
 ---- batch: 100 ----
mean loss: 120.66
 ---- batch: 110 ----
mean loss: 125.94
train mean loss: 122.25
epoch train time: 0:00:01.891478
elapsed time: 0:07:01.850126
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 23:37:17.103718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.19
 ---- batch: 020 ----
mean loss: 118.73
 ---- batch: 030 ----
mean loss: 122.91
 ---- batch: 040 ----
mean loss: 126.57
 ---- batch: 050 ----
mean loss: 110.50
 ---- batch: 060 ----
mean loss: 128.59
 ---- batch: 070 ----
mean loss: 124.04
 ---- batch: 080 ----
mean loss: 127.76
 ---- batch: 090 ----
mean loss: 123.69
 ---- batch: 100 ----
mean loss: 114.55
 ---- batch: 110 ----
mean loss: 123.80
train mean loss: 122.26
epoch train time: 0:00:01.861838
elapsed time: 0:07:03.712535
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 23:37:18.966196
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.54
 ---- batch: 020 ----
mean loss: 123.88
 ---- batch: 030 ----
mean loss: 120.58
 ---- batch: 040 ----
mean loss: 127.75
 ---- batch: 050 ----
mean loss: 116.71
 ---- batch: 060 ----
mean loss: 123.87
 ---- batch: 070 ----
mean loss: 117.40
 ---- batch: 080 ----
mean loss: 127.95
 ---- batch: 090 ----
mean loss: 118.71
 ---- batch: 100 ----
mean loss: 128.78
 ---- batch: 110 ----
mean loss: 120.15
train mean loss: 122.21
epoch train time: 0:00:01.885050
elapsed time: 0:07:05.598249
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 23:37:20.851888
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.36
 ---- batch: 020 ----
mean loss: 118.60
 ---- batch: 030 ----
mean loss: 116.35
 ---- batch: 040 ----
mean loss: 121.00
 ---- batch: 050 ----
mean loss: 127.18
 ---- batch: 060 ----
mean loss: 128.97
 ---- batch: 070 ----
mean loss: 127.02
 ---- batch: 080 ----
mean loss: 123.55
 ---- batch: 090 ----
mean loss: 123.80
 ---- batch: 100 ----
mean loss: 118.49
 ---- batch: 110 ----
mean loss: 119.27
train mean loss: 122.11
epoch train time: 0:00:01.902461
elapsed time: 0:07:07.501354
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 23:37:22.754985
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.37
 ---- batch: 020 ----
mean loss: 114.29
 ---- batch: 030 ----
mean loss: 126.20
 ---- batch: 040 ----
mean loss: 120.07
 ---- batch: 050 ----
mean loss: 117.29
 ---- batch: 060 ----
mean loss: 126.90
 ---- batch: 070 ----
mean loss: 125.12
 ---- batch: 080 ----
mean loss: 116.19
 ---- batch: 090 ----
mean loss: 125.59
 ---- batch: 100 ----
mean loss: 121.11
 ---- batch: 110 ----
mean loss: 130.68
train mean loss: 122.05
epoch train time: 0:00:01.899794
elapsed time: 0:07:09.401751
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 23:37:24.655399
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.82
 ---- batch: 020 ----
mean loss: 118.15
 ---- batch: 030 ----
mean loss: 121.94
 ---- batch: 040 ----
mean loss: 127.41
 ---- batch: 050 ----
mean loss: 120.75
 ---- batch: 060 ----
mean loss: 123.79
 ---- batch: 070 ----
mean loss: 119.58
 ---- batch: 080 ----
mean loss: 117.89
 ---- batch: 090 ----
mean loss: 118.96
 ---- batch: 100 ----
mean loss: 118.88
 ---- batch: 110 ----
mean loss: 123.73
train mean loss: 121.96
epoch train time: 0:00:01.878175
elapsed time: 0:07:11.280621
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 23:37:26.534271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.74
 ---- batch: 020 ----
mean loss: 120.04
 ---- batch: 030 ----
mean loss: 124.83
 ---- batch: 040 ----
mean loss: 120.19
 ---- batch: 050 ----
mean loss: 127.39
 ---- batch: 060 ----
mean loss: 122.04
 ---- batch: 070 ----
mean loss: 114.43
 ---- batch: 080 ----
mean loss: 120.03
 ---- batch: 090 ----
mean loss: 129.99
 ---- batch: 100 ----
mean loss: 115.91
 ---- batch: 110 ----
mean loss: 122.79
train mean loss: 121.97
epoch train time: 0:00:01.904272
elapsed time: 0:07:13.185547
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 23:37:28.439160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.62
 ---- batch: 020 ----
mean loss: 118.58
 ---- batch: 030 ----
mean loss: 125.01
 ---- batch: 040 ----
mean loss: 122.33
 ---- batch: 050 ----
mean loss: 122.05
 ---- batch: 060 ----
mean loss: 122.39
 ---- batch: 070 ----
mean loss: 123.06
 ---- batch: 080 ----
mean loss: 120.46
 ---- batch: 090 ----
mean loss: 121.67
 ---- batch: 100 ----
mean loss: 126.10
 ---- batch: 110 ----
mean loss: 119.47
train mean loss: 121.89
epoch train time: 0:00:01.877617
elapsed time: 0:07:15.063747
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 23:37:30.317350
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.24
 ---- batch: 020 ----
mean loss: 122.74
 ---- batch: 030 ----
mean loss: 112.12
 ---- batch: 040 ----
mean loss: 132.19
 ---- batch: 050 ----
mean loss: 122.36
 ---- batch: 060 ----
mean loss: 125.89
 ---- batch: 070 ----
mean loss: 119.97
 ---- batch: 080 ----
mean loss: 118.75
 ---- batch: 090 ----
mean loss: 111.70
 ---- batch: 100 ----
mean loss: 122.23
 ---- batch: 110 ----
mean loss: 126.48
train mean loss: 121.87
epoch train time: 0:00:01.888666
elapsed time: 0:07:16.953004
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 23:37:32.206604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.42
 ---- batch: 020 ----
mean loss: 119.50
 ---- batch: 030 ----
mean loss: 120.22
 ---- batch: 040 ----
mean loss: 118.43
 ---- batch: 050 ----
mean loss: 125.56
 ---- batch: 060 ----
mean loss: 119.01
 ---- batch: 070 ----
mean loss: 128.89
 ---- batch: 080 ----
mean loss: 120.51
 ---- batch: 090 ----
mean loss: 123.03
 ---- batch: 100 ----
mean loss: 122.62
 ---- batch: 110 ----
mean loss: 126.60
train mean loss: 121.84
epoch train time: 0:00:01.866863
elapsed time: 0:07:18.820482
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 23:37:34.074223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.37
 ---- batch: 020 ----
mean loss: 118.77
 ---- batch: 030 ----
mean loss: 123.20
 ---- batch: 040 ----
mean loss: 128.47
 ---- batch: 050 ----
mean loss: 116.35
 ---- batch: 060 ----
mean loss: 116.78
 ---- batch: 070 ----
mean loss: 127.39
 ---- batch: 080 ----
mean loss: 120.75
 ---- batch: 090 ----
mean loss: 113.31
 ---- batch: 100 ----
mean loss: 121.55
 ---- batch: 110 ----
mean loss: 123.81
train mean loss: 121.83
epoch train time: 0:00:01.887412
elapsed time: 0:07:20.708604
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 23:37:35.962208
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.27
 ---- batch: 020 ----
mean loss: 120.81
 ---- batch: 030 ----
mean loss: 118.45
 ---- batch: 040 ----
mean loss: 122.81
 ---- batch: 050 ----
mean loss: 118.28
 ---- batch: 060 ----
mean loss: 118.31
 ---- batch: 070 ----
mean loss: 120.59
 ---- batch: 080 ----
mean loss: 125.05
 ---- batch: 090 ----
mean loss: 125.61
 ---- batch: 100 ----
mean loss: 125.33
 ---- batch: 110 ----
mean loss: 118.13
train mean loss: 121.77
epoch train time: 0:00:01.888582
elapsed time: 0:07:22.597801
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 23:37:37.851381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.89
 ---- batch: 020 ----
mean loss: 117.70
 ---- batch: 030 ----
mean loss: 123.23
 ---- batch: 040 ----
mean loss: 116.94
 ---- batch: 050 ----
mean loss: 123.80
 ---- batch: 060 ----
mean loss: 125.37
 ---- batch: 070 ----
mean loss: 117.52
 ---- batch: 080 ----
mean loss: 124.89
 ---- batch: 090 ----
mean loss: 122.59
 ---- batch: 100 ----
mean loss: 128.10
 ---- batch: 110 ----
mean loss: 119.10
train mean loss: 121.61
epoch train time: 0:00:01.926123
elapsed time: 0:07:24.524536
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 23:37:39.778179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.81
 ---- batch: 020 ----
mean loss: 121.59
 ---- batch: 030 ----
mean loss: 124.60
 ---- batch: 040 ----
mean loss: 124.46
 ---- batch: 050 ----
mean loss: 118.32
 ---- batch: 060 ----
mean loss: 119.54
 ---- batch: 070 ----
mean loss: 122.89
 ---- batch: 080 ----
mean loss: 118.97
 ---- batch: 090 ----
mean loss: 123.71
 ---- batch: 100 ----
mean loss: 124.24
 ---- batch: 110 ----
mean loss: 115.08
train mean loss: 121.64
epoch train time: 0:00:01.874163
elapsed time: 0:07:26.399329
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 23:37:41.652957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.68
 ---- batch: 020 ----
mean loss: 121.44
 ---- batch: 030 ----
mean loss: 117.18
 ---- batch: 040 ----
mean loss: 117.32
 ---- batch: 050 ----
mean loss: 119.39
 ---- batch: 060 ----
mean loss: 115.52
 ---- batch: 070 ----
mean loss: 119.46
 ---- batch: 080 ----
mean loss: 129.46
 ---- batch: 090 ----
mean loss: 122.94
 ---- batch: 100 ----
mean loss: 124.80
 ---- batch: 110 ----
mean loss: 124.80
train mean loss: 121.61
epoch train time: 0:00:01.863200
elapsed time: 0:07:28.263123
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 23:37:43.516752
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.82
 ---- batch: 020 ----
mean loss: 123.72
 ---- batch: 030 ----
mean loss: 126.79
 ---- batch: 040 ----
mean loss: 120.86
 ---- batch: 050 ----
mean loss: 127.09
 ---- batch: 060 ----
mean loss: 115.70
 ---- batch: 070 ----
mean loss: 121.99
 ---- batch: 080 ----
mean loss: 121.61
 ---- batch: 090 ----
mean loss: 119.71
 ---- batch: 100 ----
mean loss: 125.52
 ---- batch: 110 ----
mean loss: 116.61
train mean loss: 121.57
epoch train time: 0:00:01.914759
elapsed time: 0:07:30.178497
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 23:37:45.432093
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.31
 ---- batch: 020 ----
mean loss: 115.77
 ---- batch: 030 ----
mean loss: 124.58
 ---- batch: 040 ----
mean loss: 121.60
 ---- batch: 050 ----
mean loss: 119.74
 ---- batch: 060 ----
mean loss: 122.46
 ---- batch: 070 ----
mean loss: 128.65
 ---- batch: 080 ----
mean loss: 127.06
 ---- batch: 090 ----
mean loss: 116.70
 ---- batch: 100 ----
mean loss: 117.98
 ---- batch: 110 ----
mean loss: 123.89
train mean loss: 121.52
epoch train time: 0:00:01.872367
elapsed time: 0:07:32.051504
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 23:37:47.305114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.75
 ---- batch: 020 ----
mean loss: 124.03
 ---- batch: 030 ----
mean loss: 127.31
 ---- batch: 040 ----
mean loss: 119.09
 ---- batch: 050 ----
mean loss: 120.75
 ---- batch: 060 ----
mean loss: 124.22
 ---- batch: 070 ----
mean loss: 121.62
 ---- batch: 080 ----
mean loss: 118.66
 ---- batch: 090 ----
mean loss: 118.00
 ---- batch: 100 ----
mean loss: 123.96
 ---- batch: 110 ----
mean loss: 119.35
train mean loss: 121.59
epoch train time: 0:00:01.870370
elapsed time: 0:07:33.922525
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 23:37:49.176268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.96
 ---- batch: 020 ----
mean loss: 120.29
 ---- batch: 030 ----
mean loss: 118.68
 ---- batch: 040 ----
mean loss: 119.41
 ---- batch: 050 ----
mean loss: 128.74
 ---- batch: 060 ----
mean loss: 121.03
 ---- batch: 070 ----
mean loss: 122.45
 ---- batch: 080 ----
mean loss: 122.68
 ---- batch: 090 ----
mean loss: 125.62
 ---- batch: 100 ----
mean loss: 124.18
 ---- batch: 110 ----
mean loss: 113.73
train mean loss: 121.41
epoch train time: 0:00:01.883785
elapsed time: 0:07:35.807022
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 23:37:51.060663
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.79
 ---- batch: 020 ----
mean loss: 121.29
 ---- batch: 030 ----
mean loss: 122.28
 ---- batch: 040 ----
mean loss: 118.28
 ---- batch: 050 ----
mean loss: 121.29
 ---- batch: 060 ----
mean loss: 120.83
 ---- batch: 070 ----
mean loss: 126.53
 ---- batch: 080 ----
mean loss: 129.08
 ---- batch: 090 ----
mean loss: 122.96
 ---- batch: 100 ----
mean loss: 113.14
 ---- batch: 110 ----
mean loss: 122.32
train mean loss: 121.44
epoch train time: 0:00:01.842162
elapsed time: 0:07:37.649872
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 23:37:52.903482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.77
 ---- batch: 020 ----
mean loss: 115.50
 ---- batch: 030 ----
mean loss: 124.76
 ---- batch: 040 ----
mean loss: 122.73
 ---- batch: 050 ----
mean loss: 117.09
 ---- batch: 060 ----
mean loss: 124.48
 ---- batch: 070 ----
mean loss: 114.17
 ---- batch: 080 ----
mean loss: 113.38
 ---- batch: 090 ----
mean loss: 127.68
 ---- batch: 100 ----
mean loss: 128.05
 ---- batch: 110 ----
mean loss: 123.32
train mean loss: 121.46
epoch train time: 0:00:01.920763
elapsed time: 0:07:39.571220
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 23:37:54.824817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.65
 ---- batch: 020 ----
mean loss: 132.16
 ---- batch: 030 ----
mean loss: 122.56
 ---- batch: 040 ----
mean loss: 117.95
 ---- batch: 050 ----
mean loss: 115.90
 ---- batch: 060 ----
mean loss: 110.32
 ---- batch: 070 ----
mean loss: 132.00
 ---- batch: 080 ----
mean loss: 116.98
 ---- batch: 090 ----
mean loss: 122.73
 ---- batch: 100 ----
mean loss: 122.99
 ---- batch: 110 ----
mean loss: 123.37
train mean loss: 121.38
epoch train time: 0:00:01.890657
elapsed time: 0:07:41.462484
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 23:37:56.716084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.35
 ---- batch: 020 ----
mean loss: 122.23
 ---- batch: 030 ----
mean loss: 122.49
 ---- batch: 040 ----
mean loss: 115.42
 ---- batch: 050 ----
mean loss: 116.10
 ---- batch: 060 ----
mean loss: 126.50
 ---- batch: 070 ----
mean loss: 127.59
 ---- batch: 080 ----
mean loss: 127.71
 ---- batch: 090 ----
mean loss: 122.07
 ---- batch: 100 ----
mean loss: 122.84
 ---- batch: 110 ----
mean loss: 122.81
train mean loss: 121.23
epoch train time: 0:00:01.874597
elapsed time: 0:07:43.337632
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 23:37:58.591218
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.95
 ---- batch: 020 ----
mean loss: 116.55
 ---- batch: 030 ----
mean loss: 120.42
 ---- batch: 040 ----
mean loss: 125.33
 ---- batch: 050 ----
mean loss: 123.04
 ---- batch: 060 ----
mean loss: 120.71
 ---- batch: 070 ----
mean loss: 127.44
 ---- batch: 080 ----
mean loss: 122.23
 ---- batch: 090 ----
mean loss: 117.64
 ---- batch: 100 ----
mean loss: 120.44
 ---- batch: 110 ----
mean loss: 119.20
train mean loss: 121.25
epoch train time: 0:00:01.907300
elapsed time: 0:07:45.245494
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 23:38:00.499083
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.58
 ---- batch: 020 ----
mean loss: 124.52
 ---- batch: 030 ----
mean loss: 132.53
 ---- batch: 040 ----
mean loss: 126.44
 ---- batch: 050 ----
mean loss: 120.26
 ---- batch: 060 ----
mean loss: 123.18
 ---- batch: 070 ----
mean loss: 118.00
 ---- batch: 080 ----
mean loss: 111.63
 ---- batch: 090 ----
mean loss: 121.29
 ---- batch: 100 ----
mean loss: 123.23
 ---- batch: 110 ----
mean loss: 120.09
train mean loss: 121.20
epoch train time: 0:00:01.851169
elapsed time: 0:07:47.097197
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 23:38:02.350778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.32
 ---- batch: 020 ----
mean loss: 115.01
 ---- batch: 030 ----
mean loss: 116.59
 ---- batch: 040 ----
mean loss: 132.08
 ---- batch: 050 ----
mean loss: 118.95
 ---- batch: 060 ----
mean loss: 122.71
 ---- batch: 070 ----
mean loss: 120.16
 ---- batch: 080 ----
mean loss: 122.53
 ---- batch: 090 ----
mean loss: 116.53
 ---- batch: 100 ----
mean loss: 115.27
 ---- batch: 110 ----
mean loss: 127.13
train mean loss: 121.23
epoch train time: 0:00:01.885030
elapsed time: 0:07:48.982778
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 23:38:04.236378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.21
 ---- batch: 020 ----
mean loss: 113.67
 ---- batch: 030 ----
mean loss: 116.36
 ---- batch: 040 ----
mean loss: 128.70
 ---- batch: 050 ----
mean loss: 121.84
 ---- batch: 060 ----
mean loss: 113.93
 ---- batch: 070 ----
mean loss: 121.55
 ---- batch: 080 ----
mean loss: 124.33
 ---- batch: 090 ----
mean loss: 122.08
 ---- batch: 100 ----
mean loss: 132.37
 ---- batch: 110 ----
mean loss: 125.14
train mean loss: 121.18
epoch train time: 0:00:01.917243
elapsed time: 0:07:50.900608
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 23:38:06.154190
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.37
 ---- batch: 020 ----
mean loss: 118.82
 ---- batch: 030 ----
mean loss: 124.84
 ---- batch: 040 ----
mean loss: 119.62
 ---- batch: 050 ----
mean loss: 114.70
 ---- batch: 060 ----
mean loss: 124.63
 ---- batch: 070 ----
mean loss: 126.59
 ---- batch: 080 ----
mean loss: 123.29
 ---- batch: 090 ----
mean loss: 121.99
 ---- batch: 100 ----
mean loss: 116.26
 ---- batch: 110 ----
mean loss: 117.20
train mean loss: 121.01
epoch train time: 0:00:01.892461
elapsed time: 0:07:52.793637
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 23:38:08.047233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.03
 ---- batch: 020 ----
mean loss: 110.89
 ---- batch: 030 ----
mean loss: 120.86
 ---- batch: 040 ----
mean loss: 120.76
 ---- batch: 050 ----
mean loss: 120.50
 ---- batch: 060 ----
mean loss: 125.02
 ---- batch: 070 ----
mean loss: 124.00
 ---- batch: 080 ----
mean loss: 127.50
 ---- batch: 090 ----
mean loss: 120.13
 ---- batch: 100 ----
mean loss: 124.57
 ---- batch: 110 ----
mean loss: 118.98
train mean loss: 121.00
epoch train time: 0:00:01.905950
elapsed time: 0:07:54.700465
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 23:38:09.953849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.34
 ---- batch: 020 ----
mean loss: 118.87
 ---- batch: 030 ----
mean loss: 118.54
 ---- batch: 040 ----
mean loss: 124.23
 ---- batch: 050 ----
mean loss: 120.12
 ---- batch: 060 ----
mean loss: 120.41
 ---- batch: 070 ----
mean loss: 120.92
 ---- batch: 080 ----
mean loss: 124.29
 ---- batch: 090 ----
mean loss: 122.93
 ---- batch: 100 ----
mean loss: 124.21
 ---- batch: 110 ----
mean loss: 119.00
train mean loss: 120.97
epoch train time: 0:00:01.881703
elapsed time: 0:07:56.582541
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 23:38:11.836126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.08
 ---- batch: 020 ----
mean loss: 113.02
 ---- batch: 030 ----
mean loss: 121.14
 ---- batch: 040 ----
mean loss: 131.20
 ---- batch: 050 ----
mean loss: 118.36
 ---- batch: 060 ----
mean loss: 124.04
 ---- batch: 070 ----
mean loss: 126.75
 ---- batch: 080 ----
mean loss: 115.86
 ---- batch: 090 ----
mean loss: 118.95
 ---- batch: 100 ----
mean loss: 123.99
 ---- batch: 110 ----
mean loss: 113.66
train mean loss: 120.93
epoch train time: 0:00:01.903326
elapsed time: 0:07:58.486421
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 23:38:13.740009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.62
 ---- batch: 020 ----
mean loss: 123.52
 ---- batch: 030 ----
mean loss: 124.81
 ---- batch: 040 ----
mean loss: 126.60
 ---- batch: 050 ----
mean loss: 119.43
 ---- batch: 060 ----
mean loss: 129.08
 ---- batch: 070 ----
mean loss: 121.43
 ---- batch: 080 ----
mean loss: 116.71
 ---- batch: 090 ----
mean loss: 119.12
 ---- batch: 100 ----
mean loss: 119.16
 ---- batch: 110 ----
mean loss: 112.00
train mean loss: 120.84
epoch train time: 0:00:01.857758
elapsed time: 0:08:00.344723
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 23:38:15.598334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.64
 ---- batch: 020 ----
mean loss: 128.73
 ---- batch: 030 ----
mean loss: 121.24
 ---- batch: 040 ----
mean loss: 118.42
 ---- batch: 050 ----
mean loss: 118.26
 ---- batch: 060 ----
mean loss: 125.34
 ---- batch: 070 ----
mean loss: 110.64
 ---- batch: 080 ----
mean loss: 121.95
 ---- batch: 090 ----
mean loss: 123.02
 ---- batch: 100 ----
mean loss: 119.14
 ---- batch: 110 ----
mean loss: 120.08
train mean loss: 120.98
epoch train time: 0:00:01.904831
elapsed time: 0:08:02.250138
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 23:38:17.503760
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.41
 ---- batch: 020 ----
mean loss: 123.12
 ---- batch: 030 ----
mean loss: 115.49
 ---- batch: 040 ----
mean loss: 122.61
 ---- batch: 050 ----
mean loss: 123.53
 ---- batch: 060 ----
mean loss: 122.79
 ---- batch: 070 ----
mean loss: 128.44
 ---- batch: 080 ----
mean loss: 115.34
 ---- batch: 090 ----
mean loss: 114.56
 ---- batch: 100 ----
mean loss: 122.94
 ---- batch: 110 ----
mean loss: 119.54
train mean loss: 120.78
epoch train time: 0:00:01.897275
elapsed time: 0:08:04.148010
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 23:38:19.401598
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.15
 ---- batch: 020 ----
mean loss: 124.70
 ---- batch: 030 ----
mean loss: 121.97
 ---- batch: 040 ----
mean loss: 123.38
 ---- batch: 050 ----
mean loss: 120.02
 ---- batch: 060 ----
mean loss: 130.73
 ---- batch: 070 ----
mean loss: 126.86
 ---- batch: 080 ----
mean loss: 124.00
 ---- batch: 090 ----
mean loss: 117.56
 ---- batch: 100 ----
mean loss: 116.34
 ---- batch: 110 ----
mean loss: 113.11
train mean loss: 120.90
epoch train time: 0:00:01.906816
elapsed time: 0:08:06.055409
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 23:38:21.309009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.26
 ---- batch: 020 ----
mean loss: 116.88
 ---- batch: 030 ----
mean loss: 119.08
 ---- batch: 040 ----
mean loss: 118.79
 ---- batch: 050 ----
mean loss: 118.97
 ---- batch: 060 ----
mean loss: 121.65
 ---- batch: 070 ----
mean loss: 121.54
 ---- batch: 080 ----
mean loss: 123.39
 ---- batch: 090 ----
mean loss: 131.63
 ---- batch: 100 ----
mean loss: 112.91
 ---- batch: 110 ----
mean loss: 126.72
train mean loss: 120.80
epoch train time: 0:00:01.892613
elapsed time: 0:08:07.948605
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 23:38:23.202200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.47
 ---- batch: 020 ----
mean loss: 119.04
 ---- batch: 030 ----
mean loss: 116.44
 ---- batch: 040 ----
mean loss: 118.86
 ---- batch: 050 ----
mean loss: 126.32
 ---- batch: 060 ----
mean loss: 121.74
 ---- batch: 070 ----
mean loss: 125.23
 ---- batch: 080 ----
mean loss: 120.07
 ---- batch: 090 ----
mean loss: 123.33
 ---- batch: 100 ----
mean loss: 126.03
 ---- batch: 110 ----
mean loss: 113.61
train mean loss: 120.76
epoch train time: 0:00:01.880239
elapsed time: 0:08:09.829435
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 23:38:25.083012
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.15
 ---- batch: 020 ----
mean loss: 126.04
 ---- batch: 030 ----
mean loss: 124.11
 ---- batch: 040 ----
mean loss: 113.88
 ---- batch: 050 ----
mean loss: 119.42
 ---- batch: 060 ----
mean loss: 123.86
 ---- batch: 070 ----
mean loss: 122.91
 ---- batch: 080 ----
mean loss: 112.39
 ---- batch: 090 ----
mean loss: 112.95
 ---- batch: 100 ----
mean loss: 127.56
 ---- batch: 110 ----
mean loss: 125.39
train mean loss: 120.80
epoch train time: 0:00:01.877421
elapsed time: 0:08:11.707455
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 23:38:26.961108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.14
 ---- batch: 020 ----
mean loss: 126.23
 ---- batch: 030 ----
mean loss: 115.63
 ---- batch: 040 ----
mean loss: 127.14
 ---- batch: 050 ----
mean loss: 123.09
 ---- batch: 060 ----
mean loss: 115.20
 ---- batch: 070 ----
mean loss: 128.36
 ---- batch: 080 ----
mean loss: 115.44
 ---- batch: 090 ----
mean loss: 114.31
 ---- batch: 100 ----
mean loss: 125.53
 ---- batch: 110 ----
mean loss: 119.84
train mean loss: 120.67
epoch train time: 0:00:01.906346
elapsed time: 0:08:13.614479
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 23:38:28.868088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.70
 ---- batch: 020 ----
mean loss: 116.81
 ---- batch: 030 ----
mean loss: 114.13
 ---- batch: 040 ----
mean loss: 122.53
 ---- batch: 050 ----
mean loss: 123.56
 ---- batch: 060 ----
mean loss: 121.31
 ---- batch: 070 ----
mean loss: 124.64
 ---- batch: 080 ----
mean loss: 126.24
 ---- batch: 090 ----
mean loss: 118.71
 ---- batch: 100 ----
mean loss: 126.60
 ---- batch: 110 ----
mean loss: 121.91
train mean loss: 120.60
epoch train time: 0:00:01.910641
elapsed time: 0:08:15.525734
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 23:38:30.779321
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.83
 ---- batch: 020 ----
mean loss: 118.75
 ---- batch: 030 ----
mean loss: 124.22
 ---- batch: 040 ----
mean loss: 120.06
 ---- batch: 050 ----
mean loss: 113.76
 ---- batch: 060 ----
mean loss: 124.78
 ---- batch: 070 ----
mean loss: 120.64
 ---- batch: 080 ----
mean loss: 117.81
 ---- batch: 090 ----
mean loss: 118.84
 ---- batch: 100 ----
mean loss: 126.37
 ---- batch: 110 ----
mean loss: 128.40
train mean loss: 120.57
epoch train time: 0:00:01.867885
elapsed time: 0:08:17.394186
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 23:38:32.647805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.12
 ---- batch: 020 ----
mean loss: 119.89
 ---- batch: 030 ----
mean loss: 120.22
 ---- batch: 040 ----
mean loss: 115.26
 ---- batch: 050 ----
mean loss: 125.78
 ---- batch: 060 ----
mean loss: 121.25
 ---- batch: 070 ----
mean loss: 122.56
 ---- batch: 080 ----
mean loss: 118.24
 ---- batch: 090 ----
mean loss: 117.25
 ---- batch: 100 ----
mean loss: 121.60
 ---- batch: 110 ----
mean loss: 118.32
train mean loss: 120.54
epoch train time: 0:00:01.876096
elapsed time: 0:08:19.270881
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 23:38:34.524522
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.28
 ---- batch: 020 ----
mean loss: 120.87
 ---- batch: 030 ----
mean loss: 118.62
 ---- batch: 040 ----
mean loss: 117.29
 ---- batch: 050 ----
mean loss: 121.73
 ---- batch: 060 ----
mean loss: 124.69
 ---- batch: 070 ----
mean loss: 121.85
 ---- batch: 080 ----
mean loss: 114.72
 ---- batch: 090 ----
mean loss: 122.41
 ---- batch: 100 ----
mean loss: 124.95
 ---- batch: 110 ----
mean loss: 125.45
train mean loss: 120.52
epoch train time: 0:00:01.874692
elapsed time: 0:08:21.146198
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 23:38:36.399827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.75
 ---- batch: 020 ----
mean loss: 119.06
 ---- batch: 030 ----
mean loss: 118.31
 ---- batch: 040 ----
mean loss: 117.93
 ---- batch: 050 ----
mean loss: 116.62
 ---- batch: 060 ----
mean loss: 120.96
 ---- batch: 070 ----
mean loss: 116.30
 ---- batch: 080 ----
mean loss: 121.82
 ---- batch: 090 ----
mean loss: 123.80
 ---- batch: 100 ----
mean loss: 124.81
 ---- batch: 110 ----
mean loss: 124.80
train mean loss: 120.49
epoch train time: 0:00:01.910245
elapsed time: 0:08:23.057098
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 23:38:38.310792
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.31
 ---- batch: 020 ----
mean loss: 119.59
 ---- batch: 030 ----
mean loss: 118.09
 ---- batch: 040 ----
mean loss: 118.53
 ---- batch: 050 ----
mean loss: 116.94
 ---- batch: 060 ----
mean loss: 126.24
 ---- batch: 070 ----
mean loss: 120.32
 ---- batch: 080 ----
mean loss: 121.13
 ---- batch: 090 ----
mean loss: 121.75
 ---- batch: 100 ----
mean loss: 123.64
 ---- batch: 110 ----
mean loss: 118.52
train mean loss: 120.50
epoch train time: 0:00:01.938287
elapsed time: 0:08:25.003533
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_8/checkpoint.pth.tar
**** end time: 2019-09-26 23:38:40.256774 ****
