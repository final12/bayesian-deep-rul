Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 2101
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-24 20:04:13.164798 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-24 20:04:13.183068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2309.77
 ---- batch: 020 ----
mean loss: 1393.91
 ---- batch: 030 ----
mean loss: 1209.08
 ---- batch: 040 ----
mean loss: 1143.48
 ---- batch: 050 ----
mean loss: 1152.44
 ---- batch: 060 ----
mean loss: 1085.32
 ---- batch: 070 ----
mean loss: 1077.87
 ---- batch: 080 ----
mean loss: 1061.47
 ---- batch: 090 ----
mean loss: 1035.97
train mean loss: 1257.69
epoch train time: 0:00:49.141111
elapsed time: 0:00:49.168853
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-24 20:05:02.333697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1041.25
 ---- batch: 020 ----
mean loss: 1032.49
 ---- batch: 030 ----
mean loss: 1010.55
 ---- batch: 040 ----
mean loss: 1001.62
 ---- batch: 050 ----
mean loss: 998.00
 ---- batch: 060 ----
mean loss: 1000.70
 ---- batch: 070 ----
mean loss: 991.93
 ---- batch: 080 ----
mean loss: 1013.31
 ---- batch: 090 ----
mean loss: 1020.98
train mean loss: 1009.78
epoch train time: 0:00:18.703529
elapsed time: 0:01:07.873169
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-24 20:05:21.038541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 976.06
 ---- batch: 020 ----
mean loss: 956.10
 ---- batch: 030 ----
mean loss: 974.45
 ---- batch: 040 ----
mean loss: 994.37
 ---- batch: 050 ----
mean loss: 986.89
 ---- batch: 060 ----
mean loss: 947.68
 ---- batch: 070 ----
mean loss: 973.21
 ---- batch: 080 ----
mean loss: 941.93
 ---- batch: 090 ----
mean loss: 979.51
train mean loss: 969.89
epoch train time: 0:00:18.694930
elapsed time: 0:01:26.569599
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-24 20:05:39.735395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 973.19
 ---- batch: 020 ----
mean loss: 948.00
 ---- batch: 030 ----
mean loss: 957.81
 ---- batch: 040 ----
mean loss: 956.60
 ---- batch: 050 ----
mean loss: 943.63
 ---- batch: 060 ----
mean loss: 936.03
 ---- batch: 070 ----
mean loss: 957.36
 ---- batch: 080 ----
mean loss: 959.11
 ---- batch: 090 ----
mean loss: 941.32
train mean loss: 953.60
epoch train time: 0:00:18.671010
elapsed time: 0:01:45.242294
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-24 20:05:58.407631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 970.97
 ---- batch: 020 ----
mean loss: 940.79
 ---- batch: 030 ----
mean loss: 954.62
 ---- batch: 040 ----
mean loss: 952.18
 ---- batch: 050 ----
mean loss: 932.00
 ---- batch: 060 ----
mean loss: 932.67
 ---- batch: 070 ----
mean loss: 945.14
 ---- batch: 080 ----
mean loss: 954.62
 ---- batch: 090 ----
mean loss: 938.55
train mean loss: 943.74
epoch train time: 0:00:18.671071
elapsed time: 0:02:03.914624
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-24 20:06:17.080023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.60
 ---- batch: 020 ----
mean loss: 930.60
 ---- batch: 030 ----
mean loss: 949.60
 ---- batch: 040 ----
mean loss: 938.73
 ---- batch: 050 ----
mean loss: 929.84
 ---- batch: 060 ----
mean loss: 920.58
 ---- batch: 070 ----
mean loss: 941.43
 ---- batch: 080 ----
mean loss: 935.84
 ---- batch: 090 ----
mean loss: 918.91
train mean loss: 935.61
epoch train time: 0:00:18.714678
elapsed time: 0:02:22.630754
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-24 20:06:35.796209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.28
 ---- batch: 020 ----
mean loss: 942.30
 ---- batch: 030 ----
mean loss: 932.70
 ---- batch: 040 ----
mean loss: 928.63
 ---- batch: 050 ----
mean loss: 939.72
 ---- batch: 060 ----
mean loss: 943.95
 ---- batch: 070 ----
mean loss: 911.03
 ---- batch: 080 ----
mean loss: 923.32
 ---- batch: 090 ----
mean loss: 923.42
train mean loss: 928.19
epoch train time: 0:00:18.690393
elapsed time: 0:02:41.322499
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-24 20:06:54.487845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.47
 ---- batch: 020 ----
mean loss: 921.39
 ---- batch: 030 ----
mean loss: 955.99
 ---- batch: 040 ----
mean loss: 934.19
 ---- batch: 050 ----
mean loss: 913.64
 ---- batch: 060 ----
mean loss: 923.24
 ---- batch: 070 ----
mean loss: 940.69
 ---- batch: 080 ----
mean loss: 916.98
 ---- batch: 090 ----
mean loss: 901.30
train mean loss: 921.79
epoch train time: 0:00:18.738348
elapsed time: 0:03:00.062112
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-24 20:07:13.227483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.35
 ---- batch: 020 ----
mean loss: 908.49
 ---- batch: 030 ----
mean loss: 927.37
 ---- batch: 040 ----
mean loss: 934.62
 ---- batch: 050 ----
mean loss: 907.33
 ---- batch: 060 ----
mean loss: 893.32
 ---- batch: 070 ----
mean loss: 912.86
 ---- batch: 080 ----
mean loss: 897.17
 ---- batch: 090 ----
mean loss: 928.16
train mean loss: 913.49
epoch train time: 0:00:18.704927
elapsed time: 0:03:18.768337
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-24 20:07:31.933727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.20
 ---- batch: 020 ----
mean loss: 914.42
 ---- batch: 030 ----
mean loss: 890.43
 ---- batch: 040 ----
mean loss: 906.34
 ---- batch: 050 ----
mean loss: 918.75
 ---- batch: 060 ----
mean loss: 926.22
 ---- batch: 070 ----
mean loss: 899.86
 ---- batch: 080 ----
mean loss: 902.96
 ---- batch: 090 ----
mean loss: 902.51
train mean loss: 907.34
epoch train time: 0:00:18.708873
elapsed time: 0:03:37.478532
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-24 20:07:50.643969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.29
 ---- batch: 020 ----
mean loss: 911.08
 ---- batch: 030 ----
mean loss: 898.81
 ---- batch: 040 ----
mean loss: 894.57
 ---- batch: 050 ----
mean loss: 883.83
 ---- batch: 060 ----
mean loss: 889.67
 ---- batch: 070 ----
mean loss: 897.22
 ---- batch: 080 ----
mean loss: 887.16
 ---- batch: 090 ----
mean loss: 900.20
train mean loss: 893.42
epoch train time: 0:00:18.718331
elapsed time: 0:03:56.198359
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-24 20:08:09.363749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.97
 ---- batch: 020 ----
mean loss: 875.21
 ---- batch: 030 ----
mean loss: 879.28
 ---- batch: 040 ----
mean loss: 900.95
 ---- batch: 050 ----
mean loss: 881.95
 ---- batch: 060 ----
mean loss: 872.58
 ---- batch: 070 ----
mean loss: 889.54
 ---- batch: 080 ----
mean loss: 865.09
 ---- batch: 090 ----
mean loss: 853.03
train mean loss: 871.05
epoch train time: 0:00:18.700446
elapsed time: 0:04:14.900250
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-24 20:08:28.065623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.16
 ---- batch: 020 ----
mean loss: 851.50
 ---- batch: 030 ----
mean loss: 841.72
 ---- batch: 040 ----
mean loss: 814.90
 ---- batch: 050 ----
mean loss: 827.02
 ---- batch: 060 ----
mean loss: 834.25
 ---- batch: 070 ----
mean loss: 798.45
 ---- batch: 080 ----
mean loss: 806.12
 ---- batch: 090 ----
mean loss: 805.24
train mean loss: 821.35
epoch train time: 0:00:18.673138
elapsed time: 0:04:33.574787
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-24 20:08:46.740278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 770.26
 ---- batch: 020 ----
mean loss: 765.26
 ---- batch: 030 ----
mean loss: 731.48
 ---- batch: 040 ----
mean loss: 717.23
 ---- batch: 050 ----
mean loss: 725.83
 ---- batch: 060 ----
mean loss: 701.31
 ---- batch: 070 ----
mean loss: 711.02
 ---- batch: 080 ----
mean loss: 688.40
 ---- batch: 090 ----
mean loss: 702.18
train mean loss: 721.43
epoch train time: 0:00:18.681859
elapsed time: 0:04:52.258095
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-24 20:09:05.423475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.33
 ---- batch: 020 ----
mean loss: 666.60
 ---- batch: 030 ----
mean loss: 683.26
 ---- batch: 040 ----
mean loss: 657.29
 ---- batch: 050 ----
mean loss: 659.88
 ---- batch: 060 ----
mean loss: 661.54
 ---- batch: 070 ----
mean loss: 655.96
 ---- batch: 080 ----
mean loss: 646.34
 ---- batch: 090 ----
mean loss: 634.33
train mean loss: 657.40
epoch train time: 0:00:18.589982
elapsed time: 0:05:10.849440
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-24 20:09:24.014807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.07
 ---- batch: 020 ----
mean loss: 623.25
 ---- batch: 030 ----
mean loss: 599.57
 ---- batch: 040 ----
mean loss: 604.71
 ---- batch: 050 ----
mean loss: 606.98
 ---- batch: 060 ----
mean loss: 595.75
 ---- batch: 070 ----
mean loss: 584.15
 ---- batch: 080 ----
mean loss: 584.26
 ---- batch: 090 ----
mean loss: 600.87
train mean loss: 600.19
epoch train time: 0:00:18.497130
elapsed time: 0:05:29.347875
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-24 20:09:42.513203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.77
 ---- batch: 020 ----
mean loss: 555.90
 ---- batch: 030 ----
mean loss: 561.39
 ---- batch: 040 ----
mean loss: 564.86
 ---- batch: 050 ----
mean loss: 555.12
 ---- batch: 060 ----
mean loss: 551.05
 ---- batch: 070 ----
mean loss: 561.72
 ---- batch: 080 ----
mean loss: 566.01
 ---- batch: 090 ----
mean loss: 528.08
train mean loss: 557.27
epoch train time: 0:00:18.626289
elapsed time: 0:05:47.975431
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-24 20:10:01.140791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.76
 ---- batch: 020 ----
mean loss: 522.68
 ---- batch: 030 ----
mean loss: 531.10
 ---- batch: 040 ----
mean loss: 525.71
 ---- batch: 050 ----
mean loss: 510.41
 ---- batch: 060 ----
mean loss: 508.80
 ---- batch: 070 ----
mean loss: 514.11
 ---- batch: 080 ----
mean loss: 507.29
 ---- batch: 090 ----
mean loss: 502.55
train mean loss: 515.10
epoch train time: 0:00:18.635739
elapsed time: 0:06:06.612496
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-24 20:10:19.777927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.11
 ---- batch: 020 ----
mean loss: 501.68
 ---- batch: 030 ----
mean loss: 486.14
 ---- batch: 040 ----
mean loss: 484.00
 ---- batch: 050 ----
mean loss: 494.46
 ---- batch: 060 ----
mean loss: 468.18
 ---- batch: 070 ----
mean loss: 473.40
 ---- batch: 080 ----
mean loss: 462.05
 ---- batch: 090 ----
mean loss: 480.35
train mean loss: 482.28
epoch train time: 0:00:18.711361
elapsed time: 0:06:25.325399
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-24 20:10:38.490841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.68
 ---- batch: 020 ----
mean loss: 455.92
 ---- batch: 030 ----
mean loss: 462.91
 ---- batch: 040 ----
mean loss: 464.23
 ---- batch: 050 ----
mean loss: 452.47
 ---- batch: 060 ----
mean loss: 434.23
 ---- batch: 070 ----
mean loss: 449.56
 ---- batch: 080 ----
mean loss: 447.22
 ---- batch: 090 ----
mean loss: 447.96
train mean loss: 453.54
epoch train time: 0:00:18.676951
elapsed time: 0:06:44.003886
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-24 20:10:57.169348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.47
 ---- batch: 020 ----
mean loss: 436.88
 ---- batch: 030 ----
mean loss: 435.18
 ---- batch: 040 ----
mean loss: 434.93
 ---- batch: 050 ----
mean loss: 438.09
 ---- batch: 060 ----
mean loss: 438.22
 ---- batch: 070 ----
mean loss: 437.22
 ---- batch: 080 ----
mean loss: 428.36
 ---- batch: 090 ----
mean loss: 433.46
train mean loss: 436.30
epoch train time: 0:00:18.623437
elapsed time: 0:07:02.628769
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-24 20:11:15.794144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.29
 ---- batch: 020 ----
mean loss: 425.92
 ---- batch: 030 ----
mean loss: 429.51
 ---- batch: 040 ----
mean loss: 429.95
 ---- batch: 050 ----
mean loss: 412.38
 ---- batch: 060 ----
mean loss: 423.75
 ---- batch: 070 ----
mean loss: 413.54
 ---- batch: 080 ----
mean loss: 414.08
 ---- batch: 090 ----
mean loss: 419.32
train mean loss: 421.37
epoch train time: 0:00:18.664124
elapsed time: 0:07:21.294303
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-24 20:11:34.459510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.57
 ---- batch: 020 ----
mean loss: 403.33
 ---- batch: 030 ----
mean loss: 411.63
 ---- batch: 040 ----
mean loss: 424.29
 ---- batch: 050 ----
mean loss: 410.61
 ---- batch: 060 ----
mean loss: 399.92
 ---- batch: 070 ----
mean loss: 415.42
 ---- batch: 080 ----
mean loss: 413.71
 ---- batch: 090 ----
mean loss: 393.11
train mean loss: 409.84
epoch train time: 0:00:18.718833
elapsed time: 0:07:40.014324
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-24 20:11:53.179758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.53
 ---- batch: 020 ----
mean loss: 403.46
 ---- batch: 030 ----
mean loss: 394.74
 ---- batch: 040 ----
mean loss: 391.42
 ---- batch: 050 ----
mean loss: 394.89
 ---- batch: 060 ----
mean loss: 393.11
 ---- batch: 070 ----
mean loss: 385.61
 ---- batch: 080 ----
mean loss: 389.16
 ---- batch: 090 ----
mean loss: 382.36
train mean loss: 392.91
epoch train time: 0:00:18.740968
elapsed time: 0:07:58.756713
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-24 20:12:11.922099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.62
 ---- batch: 020 ----
mean loss: 389.59
 ---- batch: 030 ----
mean loss: 388.20
 ---- batch: 040 ----
mean loss: 381.57
 ---- batch: 050 ----
mean loss: 383.77
 ---- batch: 060 ----
mean loss: 375.74
 ---- batch: 070 ----
mean loss: 370.44
 ---- batch: 080 ----
mean loss: 369.73
 ---- batch: 090 ----
mean loss: 371.21
train mean loss: 379.58
epoch train time: 0:00:18.816503
elapsed time: 0:08:17.574596
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-24 20:12:30.739974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.24
 ---- batch: 020 ----
mean loss: 377.76
 ---- batch: 030 ----
mean loss: 373.33
 ---- batch: 040 ----
mean loss: 368.29
 ---- batch: 050 ----
mean loss: 363.98
 ---- batch: 060 ----
mean loss: 372.80
 ---- batch: 070 ----
mean loss: 357.41
 ---- batch: 080 ----
mean loss: 377.97
 ---- batch: 090 ----
mean loss: 367.17
train mean loss: 369.43
epoch train time: 0:00:18.750163
elapsed time: 0:08:36.326071
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-24 20:12:49.491445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.44
 ---- batch: 020 ----
mean loss: 364.16
 ---- batch: 030 ----
mean loss: 377.99
 ---- batch: 040 ----
mean loss: 374.60
 ---- batch: 050 ----
mean loss: 362.66
 ---- batch: 060 ----
mean loss: 355.80
 ---- batch: 070 ----
mean loss: 357.06
 ---- batch: 080 ----
mean loss: 354.71
 ---- batch: 090 ----
mean loss: 349.93
train mean loss: 361.53
epoch train time: 0:00:18.779839
elapsed time: 0:08:55.107291
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-24 20:13:08.272697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.85
 ---- batch: 020 ----
mean loss: 353.85
 ---- batch: 030 ----
mean loss: 352.31
 ---- batch: 040 ----
mean loss: 352.24
 ---- batch: 050 ----
mean loss: 354.57
 ---- batch: 060 ----
mean loss: 342.83
 ---- batch: 070 ----
mean loss: 346.18
 ---- batch: 080 ----
mean loss: 350.39
 ---- batch: 090 ----
mean loss: 340.44
train mean loss: 350.93
epoch train time: 0:00:18.852274
elapsed time: 0:09:13.960942
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-24 20:13:27.126267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.68
 ---- batch: 020 ----
mean loss: 340.04
 ---- batch: 030 ----
mean loss: 339.21
 ---- batch: 040 ----
mean loss: 352.50
 ---- batch: 050 ----
mean loss: 340.76
 ---- batch: 060 ----
mean loss: 336.60
 ---- batch: 070 ----
mean loss: 347.95
 ---- batch: 080 ----
mean loss: 347.04
 ---- batch: 090 ----
mean loss: 333.12
train mean loss: 342.62
epoch train time: 0:00:18.750691
elapsed time: 0:09:32.712919
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-24 20:13:45.878366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.92
 ---- batch: 020 ----
mean loss: 335.26
 ---- batch: 030 ----
mean loss: 328.25
 ---- batch: 040 ----
mean loss: 326.59
 ---- batch: 050 ----
mean loss: 344.04
 ---- batch: 060 ----
mean loss: 349.29
 ---- batch: 070 ----
mean loss: 337.20
 ---- batch: 080 ----
mean loss: 332.09
 ---- batch: 090 ----
mean loss: 323.40
train mean loss: 335.60
epoch train time: 0:00:18.726624
elapsed time: 0:09:51.441033
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-24 20:14:04.606442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.46
 ---- batch: 020 ----
mean loss: 330.89
 ---- batch: 030 ----
mean loss: 331.35
 ---- batch: 040 ----
mean loss: 328.93
 ---- batch: 050 ----
mean loss: 333.76
 ---- batch: 060 ----
mean loss: 323.60
 ---- batch: 070 ----
mean loss: 319.90
 ---- batch: 080 ----
mean loss: 328.34
 ---- batch: 090 ----
mean loss: 321.40
train mean loss: 327.55
epoch train time: 0:00:18.794357
elapsed time: 0:10:10.236888
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-24 20:14:23.402395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.71
 ---- batch: 020 ----
mean loss: 324.31
 ---- batch: 030 ----
mean loss: 320.53
 ---- batch: 040 ----
mean loss: 313.31
 ---- batch: 050 ----
mean loss: 309.14
 ---- batch: 060 ----
mean loss: 333.07
 ---- batch: 070 ----
mean loss: 318.44
 ---- batch: 080 ----
mean loss: 325.85
 ---- batch: 090 ----
mean loss: 319.29
train mean loss: 321.27
epoch train time: 0:00:18.769368
elapsed time: 0:10:29.007828
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-24 20:14:42.173143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.77
 ---- batch: 020 ----
mean loss: 328.78
 ---- batch: 030 ----
mean loss: 321.30
 ---- batch: 040 ----
mean loss: 314.46
 ---- batch: 050 ----
mean loss: 316.79
 ---- batch: 060 ----
mean loss: 311.43
 ---- batch: 070 ----
mean loss: 325.92
 ---- batch: 080 ----
mean loss: 322.01
 ---- batch: 090 ----
mean loss: 308.85
train mean loss: 317.92
epoch train time: 0:00:18.723789
elapsed time: 0:10:47.732892
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-24 20:15:00.898348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.66
 ---- batch: 020 ----
mean loss: 314.69
 ---- batch: 030 ----
mean loss: 313.32
 ---- batch: 040 ----
mean loss: 302.86
 ---- batch: 050 ----
mean loss: 304.82
 ---- batch: 060 ----
mean loss: 305.56
 ---- batch: 070 ----
mean loss: 308.95
 ---- batch: 080 ----
mean loss: 313.77
 ---- batch: 090 ----
mean loss: 308.20
train mean loss: 310.26
epoch train time: 0:00:18.691677
elapsed time: 0:11:06.426023
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-24 20:15:19.591596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.29
 ---- batch: 020 ----
mean loss: 307.58
 ---- batch: 030 ----
mean loss: 304.75
 ---- batch: 040 ----
mean loss: 307.15
 ---- batch: 050 ----
mean loss: 305.03
 ---- batch: 060 ----
mean loss: 305.49
 ---- batch: 070 ----
mean loss: 308.97
 ---- batch: 080 ----
mean loss: 293.38
 ---- batch: 090 ----
mean loss: 303.06
train mean loss: 304.40
epoch train time: 0:00:18.758243
elapsed time: 0:11:25.185816
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-24 20:15:38.351151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.91
 ---- batch: 020 ----
mean loss: 304.50
 ---- batch: 030 ----
mean loss: 300.46
 ---- batch: 040 ----
mean loss: 305.11
 ---- batch: 050 ----
mean loss: 300.60
 ---- batch: 060 ----
mean loss: 297.03
 ---- batch: 070 ----
mean loss: 299.97
 ---- batch: 080 ----
mean loss: 301.61
 ---- batch: 090 ----
mean loss: 295.64
train mean loss: 302.87
epoch train time: 0:00:18.825266
elapsed time: 0:11:44.012691
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-24 20:15:57.178089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.26
 ---- batch: 020 ----
mean loss: 308.35
 ---- batch: 030 ----
mean loss: 290.85
 ---- batch: 040 ----
mean loss: 298.21
 ---- batch: 050 ----
mean loss: 296.31
 ---- batch: 060 ----
mean loss: 291.34
 ---- batch: 070 ----
mean loss: 293.68
 ---- batch: 080 ----
mean loss: 293.13
 ---- batch: 090 ----
mean loss: 287.74
train mean loss: 296.92
epoch train time: 0:00:18.691098
elapsed time: 0:12:02.705232
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-24 20:16:15.870649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.28
 ---- batch: 020 ----
mean loss: 292.85
 ---- batch: 030 ----
mean loss: 292.64
 ---- batch: 040 ----
mean loss: 305.69
 ---- batch: 050 ----
mean loss: 301.33
 ---- batch: 060 ----
mean loss: 292.08
 ---- batch: 070 ----
mean loss: 289.96
 ---- batch: 080 ----
mean loss: 294.01
 ---- batch: 090 ----
mean loss: 287.85
train mean loss: 294.88
epoch train time: 0:00:18.119484
elapsed time: 0:12:20.826189
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-24 20:16:33.991602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.73
 ---- batch: 020 ----
mean loss: 292.32
 ---- batch: 030 ----
mean loss: 290.02
 ---- batch: 040 ----
mean loss: 290.61
 ---- batch: 050 ----
mean loss: 290.81
 ---- batch: 060 ----
mean loss: 288.04
 ---- batch: 070 ----
mean loss: 291.09
 ---- batch: 080 ----
mean loss: 285.57
 ---- batch: 090 ----
mean loss: 283.22
train mean loss: 287.06
epoch train time: 0:00:18.369405
elapsed time: 0:12:39.197012
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-24 20:16:52.362441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.25
 ---- batch: 020 ----
mean loss: 280.80
 ---- batch: 030 ----
mean loss: 285.69
 ---- batch: 040 ----
mean loss: 285.49
 ---- batch: 050 ----
mean loss: 296.90
 ---- batch: 060 ----
mean loss: 280.48
 ---- batch: 070 ----
mean loss: 280.92
 ---- batch: 080 ----
mean loss: 281.44
 ---- batch: 090 ----
mean loss: 283.80
train mean loss: 284.78
epoch train time: 0:00:18.778578
elapsed time: 0:12:57.977128
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-24 20:17:11.142646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.65
 ---- batch: 020 ----
mean loss: 286.47
 ---- batch: 030 ----
mean loss: 287.42
 ---- batch: 040 ----
mean loss: 280.01
 ---- batch: 050 ----
mean loss: 278.53
 ---- batch: 060 ----
mean loss: 286.11
 ---- batch: 070 ----
mean loss: 282.91
 ---- batch: 080 ----
mean loss: 290.13
 ---- batch: 090 ----
mean loss: 277.39
train mean loss: 283.21
epoch train time: 0:00:18.816598
elapsed time: 0:13:16.795171
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-24 20:17:29.960573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.62
 ---- batch: 020 ----
mean loss: 274.89
 ---- batch: 030 ----
mean loss: 284.69
 ---- batch: 040 ----
mean loss: 287.64
 ---- batch: 050 ----
mean loss: 283.20
 ---- batch: 060 ----
mean loss: 285.47
 ---- batch: 070 ----
mean loss: 274.10
 ---- batch: 080 ----
mean loss: 279.58
 ---- batch: 090 ----
mean loss: 275.83
train mean loss: 278.89
epoch train time: 0:00:18.796116
elapsed time: 0:13:35.592632
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-24 20:17:48.758402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.78
 ---- batch: 020 ----
mean loss: 265.48
 ---- batch: 030 ----
mean loss: 275.78
 ---- batch: 040 ----
mean loss: 274.99
 ---- batch: 050 ----
mean loss: 285.47
 ---- batch: 060 ----
mean loss: 276.63
 ---- batch: 070 ----
mean loss: 276.86
 ---- batch: 080 ----
mean loss: 288.29
 ---- batch: 090 ----
mean loss: 277.50
train mean loss: 277.69
epoch train time: 0:00:18.802569
elapsed time: 0:13:54.397002
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-24 20:18:07.562401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.31
 ---- batch: 020 ----
mean loss: 286.89
 ---- batch: 030 ----
mean loss: 272.05
 ---- batch: 040 ----
mean loss: 272.61
 ---- batch: 050 ----
mean loss: 270.10
 ---- batch: 060 ----
mean loss: 270.06
 ---- batch: 070 ----
mean loss: 269.49
 ---- batch: 080 ----
mean loss: 272.10
 ---- batch: 090 ----
mean loss: 277.61
train mean loss: 272.96
epoch train time: 0:00:18.777590
elapsed time: 0:14:13.175962
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-24 20:18:26.341354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.63
 ---- batch: 020 ----
mean loss: 258.51
 ---- batch: 030 ----
mean loss: 281.65
 ---- batch: 040 ----
mean loss: 264.55
 ---- batch: 050 ----
mean loss: 267.05
 ---- batch: 060 ----
mean loss: 276.37
 ---- batch: 070 ----
mean loss: 281.72
 ---- batch: 080 ----
mean loss: 270.54
 ---- batch: 090 ----
mean loss: 270.50
train mean loss: 271.32
epoch train time: 0:00:18.796170
elapsed time: 0:14:31.973469
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-24 20:18:45.138849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.43
 ---- batch: 020 ----
mean loss: 276.06
 ---- batch: 030 ----
mean loss: 266.62
 ---- batch: 040 ----
mean loss: 268.44
 ---- batch: 050 ----
mean loss: 262.88
 ---- batch: 060 ----
mean loss: 263.18
 ---- batch: 070 ----
mean loss: 270.18
 ---- batch: 080 ----
mean loss: 263.97
 ---- batch: 090 ----
mean loss: 276.99
train mean loss: 268.45
epoch train time: 0:00:18.769639
elapsed time: 0:14:50.744427
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-24 20:19:03.909849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.97
 ---- batch: 020 ----
mean loss: 273.14
 ---- batch: 030 ----
mean loss: 270.04
 ---- batch: 040 ----
mean loss: 268.93
 ---- batch: 050 ----
mean loss: 265.19
 ---- batch: 060 ----
mean loss: 266.18
 ---- batch: 070 ----
mean loss: 263.84
 ---- batch: 080 ----
mean loss: 259.78
 ---- batch: 090 ----
mean loss: 265.30
train mean loss: 267.48
epoch train time: 0:00:18.822760
elapsed time: 0:15:09.568566
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-24 20:19:22.733976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.20
 ---- batch: 020 ----
mean loss: 264.16
 ---- batch: 030 ----
mean loss: 265.16
 ---- batch: 040 ----
mean loss: 256.18
 ---- batch: 050 ----
mean loss: 268.30
 ---- batch: 060 ----
mean loss: 272.44
 ---- batch: 070 ----
mean loss: 259.93
 ---- batch: 080 ----
mean loss: 256.54
 ---- batch: 090 ----
mean loss: 258.04
train mean loss: 263.02
epoch train time: 0:00:18.784567
elapsed time: 0:15:28.354458
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-24 20:19:41.519873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.52
 ---- batch: 020 ----
mean loss: 257.85
 ---- batch: 030 ----
mean loss: 265.28
 ---- batch: 040 ----
mean loss: 263.04
 ---- batch: 050 ----
mean loss: 258.67
 ---- batch: 060 ----
mean loss: 262.25
 ---- batch: 070 ----
mean loss: 271.91
 ---- batch: 080 ----
mean loss: 271.92
 ---- batch: 090 ----
mean loss: 262.10
train mean loss: 263.28
epoch train time: 0:00:18.696238
elapsed time: 0:15:47.052128
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-24 20:20:00.217522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.68
 ---- batch: 020 ----
mean loss: 251.52
 ---- batch: 030 ----
mean loss: 273.03
 ---- batch: 040 ----
mean loss: 257.45
 ---- batch: 050 ----
mean loss: 263.00
 ---- batch: 060 ----
mean loss: 267.31
 ---- batch: 070 ----
mean loss: 251.05
 ---- batch: 080 ----
mean loss: 257.99
 ---- batch: 090 ----
mean loss: 259.50
train mean loss: 259.50
epoch train time: 0:00:18.717478
elapsed time: 0:16:05.770938
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-24 20:20:18.936298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.84
 ---- batch: 020 ----
mean loss: 257.00
 ---- batch: 030 ----
mean loss: 260.32
 ---- batch: 040 ----
mean loss: 258.25
 ---- batch: 050 ----
mean loss: 261.91
 ---- batch: 060 ----
mean loss: 263.18
 ---- batch: 070 ----
mean loss: 258.63
 ---- batch: 080 ----
mean loss: 249.50
 ---- batch: 090 ----
mean loss: 252.92
train mean loss: 257.58
epoch train time: 0:00:18.668915
elapsed time: 0:16:24.441160
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-24 20:20:37.606523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.45
 ---- batch: 020 ----
mean loss: 253.11
 ---- batch: 030 ----
mean loss: 258.92
 ---- batch: 040 ----
mean loss: 261.74
 ---- batch: 050 ----
mean loss: 256.28
 ---- batch: 060 ----
mean loss: 257.45
 ---- batch: 070 ----
mean loss: 250.91
 ---- batch: 080 ----
mean loss: 254.10
 ---- batch: 090 ----
mean loss: 256.18
train mean loss: 256.45
epoch train time: 0:00:18.729656
elapsed time: 0:16:43.172176
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-24 20:20:56.337578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.07
 ---- batch: 020 ----
mean loss: 258.45
 ---- batch: 030 ----
mean loss: 260.51
 ---- batch: 040 ----
mean loss: 248.48
 ---- batch: 050 ----
mean loss: 255.88
 ---- batch: 060 ----
mean loss: 258.92
 ---- batch: 070 ----
mean loss: 246.17
 ---- batch: 080 ----
mean loss: 242.93
 ---- batch: 090 ----
mean loss: 258.34
train mean loss: 253.74
epoch train time: 0:00:18.763590
elapsed time: 0:17:01.937139
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-24 20:21:15.102525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.97
 ---- batch: 020 ----
mean loss: 249.09
 ---- batch: 030 ----
mean loss: 262.49
 ---- batch: 040 ----
mean loss: 252.82
 ---- batch: 050 ----
mean loss: 247.86
 ---- batch: 060 ----
mean loss: 250.37
 ---- batch: 070 ----
mean loss: 247.40
 ---- batch: 080 ----
mean loss: 265.55
 ---- batch: 090 ----
mean loss: 247.09
train mean loss: 252.61
epoch train time: 0:00:18.704689
elapsed time: 0:17:20.643181
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-24 20:21:33.808546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.66
 ---- batch: 020 ----
mean loss: 251.25
 ---- batch: 030 ----
mean loss: 252.60
 ---- batch: 040 ----
mean loss: 250.16
 ---- batch: 050 ----
mean loss: 252.53
 ---- batch: 060 ----
mean loss: 248.48
 ---- batch: 070 ----
mean loss: 250.84
 ---- batch: 080 ----
mean loss: 239.69
 ---- batch: 090 ----
mean loss: 254.42
train mean loss: 250.31
epoch train time: 0:00:18.684073
elapsed time: 0:17:39.328581
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-24 20:21:52.494005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.52
 ---- batch: 020 ----
mean loss: 244.44
 ---- batch: 030 ----
mean loss: 251.41
 ---- batch: 040 ----
mean loss: 250.90
 ---- batch: 050 ----
mean loss: 245.57
 ---- batch: 060 ----
mean loss: 249.87
 ---- batch: 070 ----
mean loss: 253.53
 ---- batch: 080 ----
mean loss: 241.33
 ---- batch: 090 ----
mean loss: 243.03
train mean loss: 248.50
epoch train time: 0:00:18.702907
elapsed time: 0:17:58.032903
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-24 20:22:11.198247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.94
 ---- batch: 020 ----
mean loss: 240.93
 ---- batch: 030 ----
mean loss: 251.63
 ---- batch: 040 ----
mean loss: 250.58
 ---- batch: 050 ----
mean loss: 244.94
 ---- batch: 060 ----
mean loss: 242.54
 ---- batch: 070 ----
mean loss: 250.89
 ---- batch: 080 ----
mean loss: 247.21
 ---- batch: 090 ----
mean loss: 252.18
train mean loss: 247.40
epoch train time: 0:00:18.674300
elapsed time: 0:18:16.708639
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-24 20:22:29.874051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.08
 ---- batch: 020 ----
mean loss: 248.44
 ---- batch: 030 ----
mean loss: 248.13
 ---- batch: 040 ----
mean loss: 243.84
 ---- batch: 050 ----
mean loss: 250.19
 ---- batch: 060 ----
mean loss: 253.46
 ---- batch: 070 ----
mean loss: 236.74
 ---- batch: 080 ----
mean loss: 246.37
 ---- batch: 090 ----
mean loss: 246.38
train mean loss: 246.37
epoch train time: 0:00:18.670281
elapsed time: 0:18:35.380304
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-24 20:22:48.545701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.20
 ---- batch: 020 ----
mean loss: 245.29
 ---- batch: 030 ----
mean loss: 239.03
 ---- batch: 040 ----
mean loss: 246.87
 ---- batch: 050 ----
mean loss: 242.69
 ---- batch: 060 ----
mean loss: 242.89
 ---- batch: 070 ----
mean loss: 244.11
 ---- batch: 080 ----
mean loss: 252.87
 ---- batch: 090 ----
mean loss: 245.84
train mean loss: 245.67
epoch train time: 0:00:18.687135
elapsed time: 0:18:54.068780
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-24 20:23:07.234355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.05
 ---- batch: 020 ----
mean loss: 247.77
 ---- batch: 030 ----
mean loss: 235.66
 ---- batch: 040 ----
mean loss: 244.19
 ---- batch: 050 ----
mean loss: 246.16
 ---- batch: 060 ----
mean loss: 244.17
 ---- batch: 070 ----
mean loss: 238.71
 ---- batch: 080 ----
mean loss: 239.06
 ---- batch: 090 ----
mean loss: 239.75
train mean loss: 241.73
epoch train time: 0:00:18.667559
elapsed time: 0:19:12.738030
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-24 20:23:25.903885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.29
 ---- batch: 020 ----
mean loss: 242.93
 ---- batch: 030 ----
mean loss: 236.41
 ---- batch: 040 ----
mean loss: 245.47
 ---- batch: 050 ----
mean loss: 237.27
 ---- batch: 060 ----
mean loss: 237.00
 ---- batch: 070 ----
mean loss: 245.03
 ---- batch: 080 ----
mean loss: 243.93
 ---- batch: 090 ----
mean loss: 242.92
train mean loss: 240.66
epoch train time: 0:00:18.628746
elapsed time: 0:19:31.368624
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-24 20:23:44.534000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.72
 ---- batch: 020 ----
mean loss: 235.46
 ---- batch: 030 ----
mean loss: 233.88
 ---- batch: 040 ----
mean loss: 232.06
 ---- batch: 050 ----
mean loss: 237.62
 ---- batch: 060 ----
mean loss: 250.25
 ---- batch: 070 ----
mean loss: 236.14
 ---- batch: 080 ----
mean loss: 237.70
 ---- batch: 090 ----
mean loss: 251.29
train mean loss: 239.92
epoch train time: 0:00:18.471057
elapsed time: 0:19:49.841098
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-24 20:24:03.006593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.41
 ---- batch: 020 ----
mean loss: 236.22
 ---- batch: 030 ----
mean loss: 242.20
 ---- batch: 040 ----
mean loss: 235.13
 ---- batch: 050 ----
mean loss: 245.59
 ---- batch: 060 ----
mean loss: 238.68
 ---- batch: 070 ----
mean loss: 237.87
 ---- batch: 080 ----
mean loss: 238.60
 ---- batch: 090 ----
mean loss: 240.27
train mean loss: 239.83
epoch train time: 0:00:17.696498
elapsed time: 0:20:07.538937
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-24 20:24:20.704301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.45
 ---- batch: 020 ----
mean loss: 239.32
 ---- batch: 030 ----
mean loss: 235.39
 ---- batch: 040 ----
mean loss: 240.45
 ---- batch: 050 ----
mean loss: 248.73
 ---- batch: 060 ----
mean loss: 237.95
 ---- batch: 070 ----
mean loss: 235.03
 ---- batch: 080 ----
mean loss: 232.73
 ---- batch: 090 ----
mean loss: 229.83
train mean loss: 237.39
epoch train time: 0:00:17.654338
elapsed time: 0:20:25.194549
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-24 20:24:38.359896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.92
 ---- batch: 020 ----
mean loss: 231.79
 ---- batch: 030 ----
mean loss: 237.04
 ---- batch: 040 ----
mean loss: 235.86
 ---- batch: 050 ----
mean loss: 240.12
 ---- batch: 060 ----
mean loss: 237.48
 ---- batch: 070 ----
mean loss: 228.62
 ---- batch: 080 ----
mean loss: 231.63
 ---- batch: 090 ----
mean loss: 233.64
train mean loss: 236.22
epoch train time: 0:00:17.624388
elapsed time: 0:20:42.820115
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-24 20:24:55.985449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.13
 ---- batch: 020 ----
mean loss: 237.10
 ---- batch: 030 ----
mean loss: 238.48
 ---- batch: 040 ----
mean loss: 236.09
 ---- batch: 050 ----
mean loss: 232.47
 ---- batch: 060 ----
mean loss: 235.28
 ---- batch: 070 ----
mean loss: 232.76
 ---- batch: 080 ----
mean loss: 234.43
 ---- batch: 090 ----
mean loss: 234.11
train mean loss: 234.88
epoch train time: 0:00:17.572080
elapsed time: 0:21:00.393450
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-24 20:25:13.558822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.10
 ---- batch: 020 ----
mean loss: 231.35
 ---- batch: 030 ----
mean loss: 238.74
 ---- batch: 040 ----
mean loss: 240.86
 ---- batch: 050 ----
mean loss: 238.99
 ---- batch: 060 ----
mean loss: 237.99
 ---- batch: 070 ----
mean loss: 227.88
 ---- batch: 080 ----
mean loss: 230.23
 ---- batch: 090 ----
mean loss: 224.81
train mean loss: 233.48
epoch train time: 0:00:17.610525
elapsed time: 0:21:18.005201
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-24 20:25:31.170560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.46
 ---- batch: 020 ----
mean loss: 235.10
 ---- batch: 030 ----
mean loss: 223.19
 ---- batch: 040 ----
mean loss: 229.31
 ---- batch: 050 ----
mean loss: 231.77
 ---- batch: 060 ----
mean loss: 234.35
 ---- batch: 070 ----
mean loss: 233.33
 ---- batch: 080 ----
mean loss: 233.00
 ---- batch: 090 ----
mean loss: 227.14
train mean loss: 230.95
epoch train time: 0:00:17.671089
elapsed time: 0:21:35.677601
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-24 20:25:48.843012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.89
 ---- batch: 020 ----
mean loss: 230.95
 ---- batch: 030 ----
mean loss: 224.01
 ---- batch: 040 ----
mean loss: 227.77
 ---- batch: 050 ----
mean loss: 228.98
 ---- batch: 060 ----
mean loss: 228.07
 ---- batch: 070 ----
mean loss: 240.47
 ---- batch: 080 ----
mean loss: 229.47
 ---- batch: 090 ----
mean loss: 233.21
train mean loss: 230.92
epoch train time: 0:00:17.737656
elapsed time: 0:21:53.416546
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-24 20:26:06.581932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.62
 ---- batch: 020 ----
mean loss: 229.18
 ---- batch: 030 ----
mean loss: 225.50
 ---- batch: 040 ----
mean loss: 229.60
 ---- batch: 050 ----
mean loss: 228.62
 ---- batch: 060 ----
mean loss: 223.09
 ---- batch: 070 ----
mean loss: 217.78
 ---- batch: 080 ----
mean loss: 237.62
 ---- batch: 090 ----
mean loss: 234.18
train mean loss: 229.98
epoch train time: 0:00:17.776893
elapsed time: 0:22:11.194903
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-24 20:26:24.360392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.71
 ---- batch: 020 ----
mean loss: 222.62
 ---- batch: 030 ----
mean loss: 225.83
 ---- batch: 040 ----
mean loss: 235.53
 ---- batch: 050 ----
mean loss: 237.15
 ---- batch: 060 ----
mean loss: 225.42
 ---- batch: 070 ----
mean loss: 233.41
 ---- batch: 080 ----
mean loss: 235.82
 ---- batch: 090 ----
mean loss: 228.83
train mean loss: 229.68
epoch train time: 0:00:17.716200
elapsed time: 0:22:28.912476
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-24 20:26:42.077851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.01
 ---- batch: 020 ----
mean loss: 227.79
 ---- batch: 030 ----
mean loss: 233.57
 ---- batch: 040 ----
mean loss: 229.41
 ---- batch: 050 ----
mean loss: 229.24
 ---- batch: 060 ----
mean loss: 232.56
 ---- batch: 070 ----
mean loss: 219.35
 ---- batch: 080 ----
mean loss: 226.35
 ---- batch: 090 ----
mean loss: 229.39
train mean loss: 228.11
epoch train time: 0:00:17.856884
elapsed time: 0:22:46.770699
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-24 20:26:59.936149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.41
 ---- batch: 020 ----
mean loss: 213.28
 ---- batch: 030 ----
mean loss: 226.07
 ---- batch: 040 ----
mean loss: 223.33
 ---- batch: 050 ----
mean loss: 234.94
 ---- batch: 060 ----
mean loss: 224.80
 ---- batch: 070 ----
mean loss: 228.64
 ---- batch: 080 ----
mean loss: 234.07
 ---- batch: 090 ----
mean loss: 230.57
train mean loss: 226.39
epoch train time: 0:00:17.842935
elapsed time: 0:23:04.614937
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-24 20:27:17.780384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.18
 ---- batch: 020 ----
mean loss: 235.32
 ---- batch: 030 ----
mean loss: 233.31
 ---- batch: 040 ----
mean loss: 222.83
 ---- batch: 050 ----
mean loss: 220.53
 ---- batch: 060 ----
mean loss: 225.62
 ---- batch: 070 ----
mean loss: 223.93
 ---- batch: 080 ----
mean loss: 226.47
 ---- batch: 090 ----
mean loss: 222.34
train mean loss: 226.05
epoch train time: 0:00:17.574382
elapsed time: 0:23:22.190675
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-24 20:27:35.356003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.35
 ---- batch: 020 ----
mean loss: 230.71
 ---- batch: 030 ----
mean loss: 231.67
 ---- batch: 040 ----
mean loss: 232.35
 ---- batch: 050 ----
mean loss: 223.96
 ---- batch: 060 ----
mean loss: 222.76
 ---- batch: 070 ----
mean loss: 222.23
 ---- batch: 080 ----
mean loss: 217.62
 ---- batch: 090 ----
mean loss: 224.03
train mean loss: 225.50
epoch train time: 0:00:17.572823
elapsed time: 0:23:39.764755
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-24 20:27:52.930155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.10
 ---- batch: 020 ----
mean loss: 222.37
 ---- batch: 030 ----
mean loss: 228.77
 ---- batch: 040 ----
mean loss: 231.65
 ---- batch: 050 ----
mean loss: 223.41
 ---- batch: 060 ----
mean loss: 220.39
 ---- batch: 070 ----
mean loss: 224.96
 ---- batch: 080 ----
mean loss: 223.08
 ---- batch: 090 ----
mean loss: 226.07
train mean loss: 224.54
epoch train time: 0:00:17.527065
elapsed time: 0:23:57.293081
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-24 20:28:10.458395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.15
 ---- batch: 020 ----
mean loss: 225.28
 ---- batch: 030 ----
mean loss: 220.54
 ---- batch: 040 ----
mean loss: 223.82
 ---- batch: 050 ----
mean loss: 233.93
 ---- batch: 060 ----
mean loss: 224.03
 ---- batch: 070 ----
mean loss: 229.12
 ---- batch: 080 ----
mean loss: 228.99
 ---- batch: 090 ----
mean loss: 226.55
train mean loss: 226.11
epoch train time: 0:00:17.509995
elapsed time: 0:24:14.804241
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-24 20:28:27.969633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.46
 ---- batch: 020 ----
mean loss: 223.49
 ---- batch: 030 ----
mean loss: 224.21
 ---- batch: 040 ----
mean loss: 239.20
 ---- batch: 050 ----
mean loss: 218.84
 ---- batch: 060 ----
mean loss: 221.30
 ---- batch: 070 ----
mean loss: 224.75
 ---- batch: 080 ----
mean loss: 224.18
 ---- batch: 090 ----
mean loss: 221.85
train mean loss: 224.11
epoch train time: 0:00:17.446129
elapsed time: 0:24:32.251627
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-24 20:28:45.417028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.76
 ---- batch: 020 ----
mean loss: 219.93
 ---- batch: 030 ----
mean loss: 220.62
 ---- batch: 040 ----
mean loss: 232.92
 ---- batch: 050 ----
mean loss: 217.35
 ---- batch: 060 ----
mean loss: 216.15
 ---- batch: 070 ----
mean loss: 233.99
 ---- batch: 080 ----
mean loss: 219.92
 ---- batch: 090 ----
mean loss: 222.61
train mean loss: 221.88
epoch train time: 0:00:17.461094
elapsed time: 0:24:49.713985
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-24 20:29:02.879184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.61
 ---- batch: 020 ----
mean loss: 218.46
 ---- batch: 030 ----
mean loss: 218.70
 ---- batch: 040 ----
mean loss: 228.74
 ---- batch: 050 ----
mean loss: 221.61
 ---- batch: 060 ----
mean loss: 228.37
 ---- batch: 070 ----
mean loss: 214.97
 ---- batch: 080 ----
mean loss: 221.89
 ---- batch: 090 ----
mean loss: 221.21
train mean loss: 220.49
epoch train time: 0:00:17.485803
elapsed time: 0:25:07.200898
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-24 20:29:20.366253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.77
 ---- batch: 020 ----
mean loss: 213.79
 ---- batch: 030 ----
mean loss: 218.19
 ---- batch: 040 ----
mean loss: 217.61
 ---- batch: 050 ----
mean loss: 217.83
 ---- batch: 060 ----
mean loss: 216.63
 ---- batch: 070 ----
mean loss: 220.91
 ---- batch: 080 ----
mean loss: 222.77
 ---- batch: 090 ----
mean loss: 226.68
train mean loss: 219.39
epoch train time: 0:00:17.511062
elapsed time: 0:25:24.713173
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-24 20:29:37.878585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.79
 ---- batch: 020 ----
mean loss: 220.73
 ---- batch: 030 ----
mean loss: 228.38
 ---- batch: 040 ----
mean loss: 218.67
 ---- batch: 050 ----
mean loss: 222.63
 ---- batch: 060 ----
mean loss: 218.55
 ---- batch: 070 ----
mean loss: 222.32
 ---- batch: 080 ----
mean loss: 214.63
 ---- batch: 090 ----
mean loss: 219.33
train mean loss: 220.30
epoch train time: 0:00:17.553541
elapsed time: 0:25:42.268086
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-24 20:29:55.433498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.32
 ---- batch: 020 ----
mean loss: 218.23
 ---- batch: 030 ----
mean loss: 217.75
 ---- batch: 040 ----
mean loss: 218.58
 ---- batch: 050 ----
mean loss: 222.59
 ---- batch: 060 ----
mean loss: 225.99
 ---- batch: 070 ----
mean loss: 212.39
 ---- batch: 080 ----
mean loss: 226.08
 ---- batch: 090 ----
mean loss: 215.99
train mean loss: 219.11
epoch train time: 0:00:17.608543
elapsed time: 0:25:59.877888
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-24 20:30:13.043248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.13
 ---- batch: 020 ----
mean loss: 214.07
 ---- batch: 030 ----
mean loss: 220.31
 ---- batch: 040 ----
mean loss: 215.18
 ---- batch: 050 ----
mean loss: 217.30
 ---- batch: 060 ----
mean loss: 216.52
 ---- batch: 070 ----
mean loss: 218.29
 ---- batch: 080 ----
mean loss: 215.19
 ---- batch: 090 ----
mean loss: 221.88
train mean loss: 217.79
epoch train time: 0:00:17.489785
elapsed time: 0:26:17.368915
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-24 20:30:30.534259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.04
 ---- batch: 020 ----
mean loss: 216.48
 ---- batch: 030 ----
mean loss: 215.78
 ---- batch: 040 ----
mean loss: 215.06
 ---- batch: 050 ----
mean loss: 221.06
 ---- batch: 060 ----
mean loss: 218.21
 ---- batch: 070 ----
mean loss: 209.44
 ---- batch: 080 ----
mean loss: 217.58
 ---- batch: 090 ----
mean loss: 222.51
train mean loss: 216.96
epoch train time: 0:00:18.072873
elapsed time: 0:26:35.443031
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-24 20:30:48.608414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.88
 ---- batch: 020 ----
mean loss: 226.99
 ---- batch: 030 ----
mean loss: 220.07
 ---- batch: 040 ----
mean loss: 224.38
 ---- batch: 050 ----
mean loss: 214.73
 ---- batch: 060 ----
mean loss: 208.68
 ---- batch: 070 ----
mean loss: 206.33
 ---- batch: 080 ----
mean loss: 215.00
 ---- batch: 090 ----
mean loss: 223.91
train mean loss: 218.28
epoch train time: 0:00:17.773851
elapsed time: 0:26:53.218162
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-24 20:31:06.383499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.90
 ---- batch: 020 ----
mean loss: 213.00
 ---- batch: 030 ----
mean loss: 222.45
 ---- batch: 040 ----
mean loss: 218.67
 ---- batch: 050 ----
mean loss: 215.31
 ---- batch: 060 ----
mean loss: 219.82
 ---- batch: 070 ----
mean loss: 212.49
 ---- batch: 080 ----
mean loss: 215.07
 ---- batch: 090 ----
mean loss: 209.36
train mean loss: 216.05
epoch train time: 0:00:17.930299
elapsed time: 0:27:11.149753
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-24 20:31:24.315139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.62
 ---- batch: 020 ----
mean loss: 215.84
 ---- batch: 030 ----
mean loss: 215.07
 ---- batch: 040 ----
mean loss: 214.38
 ---- batch: 050 ----
mean loss: 202.42
 ---- batch: 060 ----
mean loss: 218.22
 ---- batch: 070 ----
mean loss: 220.41
 ---- batch: 080 ----
mean loss: 213.20
 ---- batch: 090 ----
mean loss: 216.75
train mean loss: 214.93
epoch train time: 0:00:17.819480
elapsed time: 0:27:28.970592
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-24 20:31:42.135985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.46
 ---- batch: 020 ----
mean loss: 216.29
 ---- batch: 030 ----
mean loss: 216.17
 ---- batch: 040 ----
mean loss: 215.58
 ---- batch: 050 ----
mean loss: 209.21
 ---- batch: 060 ----
mean loss: 222.32
 ---- batch: 070 ----
mean loss: 214.67
 ---- batch: 080 ----
mean loss: 213.70
 ---- batch: 090 ----
mean loss: 211.75
train mean loss: 215.28
epoch train time: 0:00:17.802940
elapsed time: 0:27:46.774852
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-24 20:31:59.940228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.52
 ---- batch: 020 ----
mean loss: 219.51
 ---- batch: 030 ----
mean loss: 216.14
 ---- batch: 040 ----
mean loss: 213.18
 ---- batch: 050 ----
mean loss: 216.91
 ---- batch: 060 ----
mean loss: 212.38
 ---- batch: 070 ----
mean loss: 214.70
 ---- batch: 080 ----
mean loss: 212.76
 ---- batch: 090 ----
mean loss: 216.35
train mean loss: 215.34
epoch train time: 0:00:17.866854
elapsed time: 0:28:04.642926
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-24 20:32:17.808317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.68
 ---- batch: 020 ----
mean loss: 211.99
 ---- batch: 030 ----
mean loss: 204.55
 ---- batch: 040 ----
mean loss: 215.15
 ---- batch: 050 ----
mean loss: 219.55
 ---- batch: 060 ----
mean loss: 221.13
 ---- batch: 070 ----
mean loss: 213.86
 ---- batch: 080 ----
mean loss: 221.85
 ---- batch: 090 ----
mean loss: 214.27
train mean loss: 214.75
epoch train time: 0:00:17.948376
elapsed time: 0:28:22.592571
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-24 20:32:35.757950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.37
 ---- batch: 020 ----
mean loss: 220.87
 ---- batch: 030 ----
mean loss: 209.87
 ---- batch: 040 ----
mean loss: 217.44
 ---- batch: 050 ----
mean loss: 223.84
 ---- batch: 060 ----
mean loss: 221.09
 ---- batch: 070 ----
mean loss: 215.09
 ---- batch: 080 ----
mean loss: 214.47
 ---- batch: 090 ----
mean loss: 207.08
train mean loss: 215.64
epoch train time: 0:00:17.946025
elapsed time: 0:28:40.539807
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-24 20:32:53.705152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.45
 ---- batch: 020 ----
mean loss: 223.03
 ---- batch: 030 ----
mean loss: 212.63
 ---- batch: 040 ----
mean loss: 212.89
 ---- batch: 050 ----
mean loss: 209.15
 ---- batch: 060 ----
mean loss: 214.95
 ---- batch: 070 ----
mean loss: 210.63
 ---- batch: 080 ----
mean loss: 218.53
 ---- batch: 090 ----
mean loss: 207.28
train mean loss: 213.23
epoch train time: 0:00:17.925218
elapsed time: 0:28:58.466187
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-24 20:33:11.631544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.24
 ---- batch: 020 ----
mean loss: 218.69
 ---- batch: 030 ----
mean loss: 211.46
 ---- batch: 040 ----
mean loss: 216.60
 ---- batch: 050 ----
mean loss: 214.19
 ---- batch: 060 ----
mean loss: 219.27
 ---- batch: 070 ----
mean loss: 221.67
 ---- batch: 080 ----
mean loss: 213.14
 ---- batch: 090 ----
mean loss: 206.61
train mean loss: 214.46
epoch train time: 0:00:17.950256
elapsed time: 0:29:16.417821
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-24 20:33:29.583293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.23
 ---- batch: 020 ----
mean loss: 205.46
 ---- batch: 030 ----
mean loss: 220.77
 ---- batch: 040 ----
mean loss: 202.87
 ---- batch: 050 ----
mean loss: 208.58
 ---- batch: 060 ----
mean loss: 220.64
 ---- batch: 070 ----
mean loss: 212.86
 ---- batch: 080 ----
mean loss: 210.37
 ---- batch: 090 ----
mean loss: 214.04
train mean loss: 211.30
epoch train time: 0:00:17.939447
elapsed time: 0:29:34.358578
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-24 20:33:47.523978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.81
 ---- batch: 020 ----
mean loss: 212.77
 ---- batch: 030 ----
mean loss: 215.97
 ---- batch: 040 ----
mean loss: 207.86
 ---- batch: 050 ----
mean loss: 214.04
 ---- batch: 060 ----
mean loss: 211.27
 ---- batch: 070 ----
mean loss: 204.16
 ---- batch: 080 ----
mean loss: 213.32
 ---- batch: 090 ----
mean loss: 212.24
train mean loss: 211.67
epoch train time: 0:00:17.995492
elapsed time: 0:29:52.355272
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-24 20:34:05.520675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.89
 ---- batch: 020 ----
mean loss: 208.60
 ---- batch: 030 ----
mean loss: 208.41
 ---- batch: 040 ----
mean loss: 217.27
 ---- batch: 050 ----
mean loss: 211.75
 ---- batch: 060 ----
mean loss: 204.75
 ---- batch: 070 ----
mean loss: 218.53
 ---- batch: 080 ----
mean loss: 208.13
 ---- batch: 090 ----
mean loss: 212.78
train mean loss: 212.13
epoch train time: 0:00:17.944816
elapsed time: 0:30:10.301472
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-24 20:34:23.466873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.60
 ---- batch: 020 ----
mean loss: 207.06
 ---- batch: 030 ----
mean loss: 208.29
 ---- batch: 040 ----
mean loss: 213.43
 ---- batch: 050 ----
mean loss: 206.50
 ---- batch: 060 ----
mean loss: 209.83
 ---- batch: 070 ----
mean loss: 218.97
 ---- batch: 080 ----
mean loss: 216.17
 ---- batch: 090 ----
mean loss: 217.93
train mean loss: 211.55
epoch train time: 0:00:17.922728
elapsed time: 0:30:28.225444
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-24 20:34:41.390800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.55
 ---- batch: 020 ----
mean loss: 217.15
 ---- batch: 030 ----
mean loss: 205.86
 ---- batch: 040 ----
mean loss: 207.25
 ---- batch: 050 ----
mean loss: 203.01
 ---- batch: 060 ----
mean loss: 212.72
 ---- batch: 070 ----
mean loss: 213.91
 ---- batch: 080 ----
mean loss: 216.57
 ---- batch: 090 ----
mean loss: 206.82
train mean loss: 210.54
epoch train time: 0:00:18.020383
elapsed time: 0:30:46.247030
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-24 20:34:59.412377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.99
 ---- batch: 020 ----
mean loss: 202.97
 ---- batch: 030 ----
mean loss: 205.32
 ---- batch: 040 ----
mean loss: 203.63
 ---- batch: 050 ----
mean loss: 213.47
 ---- batch: 060 ----
mean loss: 212.31
 ---- batch: 070 ----
mean loss: 205.47
 ---- batch: 080 ----
mean loss: 212.72
 ---- batch: 090 ----
mean loss: 208.32
train mean loss: 209.13
epoch train time: 0:00:17.985958
elapsed time: 0:31:04.234169
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-24 20:35:17.399522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.58
 ---- batch: 020 ----
mean loss: 205.87
 ---- batch: 030 ----
mean loss: 207.76
 ---- batch: 040 ----
mean loss: 209.91
 ---- batch: 050 ----
mean loss: 217.46
 ---- batch: 060 ----
mean loss: 200.53
 ---- batch: 070 ----
mean loss: 200.90
 ---- batch: 080 ----
mean loss: 208.07
 ---- batch: 090 ----
mean loss: 209.67
train mean loss: 208.54
epoch train time: 0:00:17.987109
elapsed time: 0:31:22.222548
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-24 20:35:35.388198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.66
 ---- batch: 020 ----
mean loss: 207.62
 ---- batch: 030 ----
mean loss: 210.82
 ---- batch: 040 ----
mean loss: 205.86
 ---- batch: 050 ----
mean loss: 211.54
 ---- batch: 060 ----
mean loss: 210.35
 ---- batch: 070 ----
mean loss: 202.88
 ---- batch: 080 ----
mean loss: 203.42
 ---- batch: 090 ----
mean loss: 210.53
train mean loss: 208.19
epoch train time: 0:00:17.881154
elapsed time: 0:31:40.105451
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-24 20:35:53.270852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.81
 ---- batch: 020 ----
mean loss: 203.11
 ---- batch: 030 ----
mean loss: 207.36
 ---- batch: 040 ----
mean loss: 207.33
 ---- batch: 050 ----
mean loss: 203.71
 ---- batch: 060 ----
mean loss: 211.26
 ---- batch: 070 ----
mean loss: 208.65
 ---- batch: 080 ----
mean loss: 204.05
 ---- batch: 090 ----
mean loss: 212.06
train mean loss: 208.41
epoch train time: 0:00:17.916803
elapsed time: 0:31:58.023499
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-24 20:36:11.188847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.78
 ---- batch: 020 ----
mean loss: 206.43
 ---- batch: 030 ----
mean loss: 206.01
 ---- batch: 040 ----
mean loss: 205.39
 ---- batch: 050 ----
mean loss: 207.58
 ---- batch: 060 ----
mean loss: 206.38
 ---- batch: 070 ----
mean loss: 214.08
 ---- batch: 080 ----
mean loss: 201.78
 ---- batch: 090 ----
mean loss: 208.30
train mean loss: 207.93
epoch train time: 0:00:17.930747
elapsed time: 0:32:15.955565
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-24 20:36:29.121033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.06
 ---- batch: 020 ----
mean loss: 212.81
 ---- batch: 030 ----
mean loss: 205.08
 ---- batch: 040 ----
mean loss: 210.05
 ---- batch: 050 ----
mean loss: 210.10
 ---- batch: 060 ----
mean loss: 210.20
 ---- batch: 070 ----
mean loss: 198.55
 ---- batch: 080 ----
mean loss: 204.73
 ---- batch: 090 ----
mean loss: 200.92
train mean loss: 207.22
epoch train time: 0:00:17.910526
elapsed time: 0:32:33.867461
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-24 20:36:47.032822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.99
 ---- batch: 020 ----
mean loss: 202.88
 ---- batch: 030 ----
mean loss: 213.97
 ---- batch: 040 ----
mean loss: 208.82
 ---- batch: 050 ----
mean loss: 200.69
 ---- batch: 060 ----
mean loss: 206.12
 ---- batch: 070 ----
mean loss: 208.09
 ---- batch: 080 ----
mean loss: 208.61
 ---- batch: 090 ----
mean loss: 212.52
train mean loss: 207.37
epoch train time: 0:00:17.922161
elapsed time: 0:32:51.790868
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-24 20:37:04.956288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.65
 ---- batch: 020 ----
mean loss: 209.08
 ---- batch: 030 ----
mean loss: 205.66
 ---- batch: 040 ----
mean loss: 201.53
 ---- batch: 050 ----
mean loss: 209.18
 ---- batch: 060 ----
mean loss: 210.10
 ---- batch: 070 ----
mean loss: 202.33
 ---- batch: 080 ----
mean loss: 206.01
 ---- batch: 090 ----
mean loss: 205.35
train mean loss: 206.00
epoch train time: 0:00:17.949813
elapsed time: 0:33:09.742297
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-24 20:37:22.907575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.13
 ---- batch: 020 ----
mean loss: 208.21
 ---- batch: 030 ----
mean loss: 203.65
 ---- batch: 040 ----
mean loss: 207.36
 ---- batch: 050 ----
mean loss: 200.73
 ---- batch: 060 ----
mean loss: 209.07
 ---- batch: 070 ----
mean loss: 203.38
 ---- batch: 080 ----
mean loss: 213.74
 ---- batch: 090 ----
mean loss: 209.66
train mean loss: 206.27
epoch train time: 0:00:17.589016
elapsed time: 0:33:27.332424
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-24 20:37:40.497896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.88
 ---- batch: 020 ----
mean loss: 207.64
 ---- batch: 030 ----
mean loss: 199.34
 ---- batch: 040 ----
mean loss: 205.45
 ---- batch: 050 ----
mean loss: 199.99
 ---- batch: 060 ----
mean loss: 211.69
 ---- batch: 070 ----
mean loss: 210.52
 ---- batch: 080 ----
mean loss: 209.28
 ---- batch: 090 ----
mean loss: 205.96
train mean loss: 207.55
epoch train time: 0:00:17.633964
elapsed time: 0:33:44.967759
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-24 20:37:58.133204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.89
 ---- batch: 020 ----
mean loss: 206.02
 ---- batch: 030 ----
mean loss: 208.89
 ---- batch: 040 ----
mean loss: 200.88
 ---- batch: 050 ----
mean loss: 201.24
 ---- batch: 060 ----
mean loss: 209.27
 ---- batch: 070 ----
mean loss: 209.75
 ---- batch: 080 ----
mean loss: 206.62
 ---- batch: 090 ----
mean loss: 206.52
train mean loss: 207.04
epoch train time: 0:00:17.543267
elapsed time: 0:34:02.512396
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-24 20:38:15.677775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.53
 ---- batch: 020 ----
mean loss: 205.95
 ---- batch: 030 ----
mean loss: 208.94
 ---- batch: 040 ----
mean loss: 203.95
 ---- batch: 050 ----
mean loss: 204.56
 ---- batch: 060 ----
mean loss: 209.81
 ---- batch: 070 ----
mean loss: 201.24
 ---- batch: 080 ----
mean loss: 200.48
 ---- batch: 090 ----
mean loss: 204.39
train mean loss: 206.01
epoch train time: 0:00:17.508925
elapsed time: 0:34:20.022684
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-24 20:38:33.187959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.48
 ---- batch: 020 ----
mean loss: 203.26
 ---- batch: 030 ----
mean loss: 210.67
 ---- batch: 040 ----
mean loss: 209.92
 ---- batch: 050 ----
mean loss: 206.93
 ---- batch: 060 ----
mean loss: 204.63
 ---- batch: 070 ----
mean loss: 209.58
 ---- batch: 080 ----
mean loss: 198.32
 ---- batch: 090 ----
mean loss: 200.38
train mean loss: 203.93
epoch train time: 0:00:17.606670
elapsed time: 0:34:37.630520
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-24 20:38:50.795894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.81
 ---- batch: 020 ----
mean loss: 208.70
 ---- batch: 030 ----
mean loss: 200.25
 ---- batch: 040 ----
mean loss: 201.30
 ---- batch: 050 ----
mean loss: 199.10
 ---- batch: 060 ----
mean loss: 208.95
 ---- batch: 070 ----
mean loss: 214.30
 ---- batch: 080 ----
mean loss: 207.83
 ---- batch: 090 ----
mean loss: 209.13
train mean loss: 205.58
epoch train time: 0:00:17.596698
elapsed time: 0:34:55.228439
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-24 20:39:08.393816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.81
 ---- batch: 020 ----
mean loss: 208.85
 ---- batch: 030 ----
mean loss: 202.88
 ---- batch: 040 ----
mean loss: 210.69
 ---- batch: 050 ----
mean loss: 205.57
 ---- batch: 060 ----
mean loss: 211.90
 ---- batch: 070 ----
mean loss: 196.63
 ---- batch: 080 ----
mean loss: 202.34
 ---- batch: 090 ----
mean loss: 201.92
train mean loss: 205.56
epoch train time: 0:00:17.602096
elapsed time: 0:35:12.831803
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-24 20:39:25.997137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.15
 ---- batch: 020 ----
mean loss: 206.13
 ---- batch: 030 ----
mean loss: 199.82
 ---- batch: 040 ----
mean loss: 203.72
 ---- batch: 050 ----
mean loss: 205.71
 ---- batch: 060 ----
mean loss: 205.24
 ---- batch: 070 ----
mean loss: 205.95
 ---- batch: 080 ----
mean loss: 193.60
 ---- batch: 090 ----
mean loss: 206.84
train mean loss: 203.51
epoch train time: 0:00:17.599551
elapsed time: 0:35:30.432524
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-24 20:39:43.597961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.49
 ---- batch: 020 ----
mean loss: 218.39
 ---- batch: 030 ----
mean loss: 213.98
 ---- batch: 040 ----
mean loss: 204.93
 ---- batch: 050 ----
mean loss: 213.63
 ---- batch: 060 ----
mean loss: 203.83
 ---- batch: 070 ----
mean loss: 207.80
 ---- batch: 080 ----
mean loss: 200.76
 ---- batch: 090 ----
mean loss: 203.16
train mean loss: 207.78
epoch train time: 0:00:17.628962
elapsed time: 0:35:48.062772
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-24 20:40:01.228156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.57
 ---- batch: 020 ----
mean loss: 196.03
 ---- batch: 030 ----
mean loss: 201.42
 ---- batch: 040 ----
mean loss: 198.05
 ---- batch: 050 ----
mean loss: 204.21
 ---- batch: 060 ----
mean loss: 208.11
 ---- batch: 070 ----
mean loss: 211.30
 ---- batch: 080 ----
mean loss: 209.99
 ---- batch: 090 ----
mean loss: 207.32
train mean loss: 202.83
epoch train time: 0:00:17.635177
elapsed time: 0:36:05.699255
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-24 20:40:18.864604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.37
 ---- batch: 020 ----
mean loss: 199.13
 ---- batch: 030 ----
mean loss: 205.49
 ---- batch: 040 ----
mean loss: 202.89
 ---- batch: 050 ----
mean loss: 200.64
 ---- batch: 060 ----
mean loss: 200.59
 ---- batch: 070 ----
mean loss: 220.02
 ---- batch: 080 ----
mean loss: 211.40
 ---- batch: 090 ----
mean loss: 218.28
train mean loss: 206.83
epoch train time: 0:00:17.603327
elapsed time: 0:36:23.303985
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-24 20:40:36.469327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.11
 ---- batch: 020 ----
mean loss: 208.78
 ---- batch: 030 ----
mean loss: 209.05
 ---- batch: 040 ----
mean loss: 210.43
 ---- batch: 050 ----
mean loss: 207.71
 ---- batch: 060 ----
mean loss: 203.96
 ---- batch: 070 ----
mean loss: 202.82
 ---- batch: 080 ----
mean loss: 205.60
 ---- batch: 090 ----
mean loss: 195.33
train mean loss: 204.56
epoch train time: 0:00:17.535299
elapsed time: 0:36:40.840458
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-24 20:40:54.005889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.18
 ---- batch: 020 ----
mean loss: 201.55
 ---- batch: 030 ----
mean loss: 200.27
 ---- batch: 040 ----
mean loss: 206.76
 ---- batch: 050 ----
mean loss: 199.19
 ---- batch: 060 ----
mean loss: 206.94
 ---- batch: 070 ----
mean loss: 199.49
 ---- batch: 080 ----
mean loss: 205.01
 ---- batch: 090 ----
mean loss: 201.40
train mean loss: 203.30
epoch train time: 0:00:17.592254
elapsed time: 0:36:58.434027
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-24 20:41:11.599403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.41
 ---- batch: 020 ----
mean loss: 205.62
 ---- batch: 030 ----
mean loss: 199.56
 ---- batch: 040 ----
mean loss: 208.96
 ---- batch: 050 ----
mean loss: 206.01
 ---- batch: 060 ----
mean loss: 201.01
 ---- batch: 070 ----
mean loss: 193.05
 ---- batch: 080 ----
mean loss: 207.02
 ---- batch: 090 ----
mean loss: 198.07
train mean loss: 202.84
epoch train time: 0:00:17.592148
elapsed time: 0:37:16.027370
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-24 20:41:29.192567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.35
 ---- batch: 020 ----
mean loss: 202.60
 ---- batch: 030 ----
mean loss: 208.23
 ---- batch: 040 ----
mean loss: 205.47
 ---- batch: 050 ----
mean loss: 212.83
 ---- batch: 060 ----
mean loss: 204.03
 ---- batch: 070 ----
mean loss: 207.07
 ---- batch: 080 ----
mean loss: 208.34
 ---- batch: 090 ----
mean loss: 199.60
train mean loss: 204.22
epoch train time: 0:00:17.616807
elapsed time: 0:37:33.645278
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-24 20:41:46.810628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.55
 ---- batch: 020 ----
mean loss: 206.25
 ---- batch: 030 ----
mean loss: 200.05
 ---- batch: 040 ----
mean loss: 205.61
 ---- batch: 050 ----
mean loss: 201.27
 ---- batch: 060 ----
mean loss: 200.96
 ---- batch: 070 ----
mean loss: 195.68
 ---- batch: 080 ----
mean loss: 197.93
 ---- batch: 090 ----
mean loss: 196.07
train mean loss: 201.29
epoch train time: 0:00:17.518500
elapsed time: 0:37:51.164958
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-24 20:42:04.330313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.57
 ---- batch: 020 ----
mean loss: 191.61
 ---- batch: 030 ----
mean loss: 194.98
 ---- batch: 040 ----
mean loss: 205.08
 ---- batch: 050 ----
mean loss: 204.10
 ---- batch: 060 ----
mean loss: 200.60
 ---- batch: 070 ----
mean loss: 203.49
 ---- batch: 080 ----
mean loss: 203.14
 ---- batch: 090 ----
mean loss: 199.64
train mean loss: 200.54
epoch train time: 0:00:17.466769
elapsed time: 0:38:08.633052
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-24 20:42:21.798419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.34
 ---- batch: 020 ----
mean loss: 199.82
 ---- batch: 030 ----
mean loss: 194.90
 ---- batch: 040 ----
mean loss: 202.27
 ---- batch: 050 ----
mean loss: 200.90
 ---- batch: 060 ----
mean loss: 203.89
 ---- batch: 070 ----
mean loss: 200.99
 ---- batch: 080 ----
mean loss: 203.10
 ---- batch: 090 ----
mean loss: 202.30
train mean loss: 200.14
epoch train time: 0:00:17.475721
elapsed time: 0:38:26.110047
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-24 20:42:39.275410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.46
 ---- batch: 020 ----
mean loss: 199.49
 ---- batch: 030 ----
mean loss: 195.07
 ---- batch: 040 ----
mean loss: 196.28
 ---- batch: 050 ----
mean loss: 199.36
 ---- batch: 060 ----
mean loss: 197.54
 ---- batch: 070 ----
mean loss: 200.46
 ---- batch: 080 ----
mean loss: 204.99
 ---- batch: 090 ----
mean loss: 208.53
train mean loss: 199.80
epoch train time: 0:00:17.545539
elapsed time: 0:38:43.656890
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-24 20:42:56.822302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.80
 ---- batch: 020 ----
mean loss: 201.15
 ---- batch: 030 ----
mean loss: 205.06
 ---- batch: 040 ----
mean loss: 205.18
 ---- batch: 050 ----
mean loss: 199.48
 ---- batch: 060 ----
mean loss: 197.04
 ---- batch: 070 ----
mean loss: 197.75
 ---- batch: 080 ----
mean loss: 202.41
 ---- batch: 090 ----
mean loss: 204.14
train mean loss: 200.49
epoch train time: 0:00:17.475546
elapsed time: 0:39:01.134004
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-24 20:43:14.299223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.35
 ---- batch: 020 ----
mean loss: 200.29
 ---- batch: 030 ----
mean loss: 197.80
 ---- batch: 040 ----
mean loss: 201.78
 ---- batch: 050 ----
mean loss: 199.40
 ---- batch: 060 ----
mean loss: 199.33
 ---- batch: 070 ----
mean loss: 193.54
 ---- batch: 080 ----
mean loss: 198.01
 ---- batch: 090 ----
mean loss: 206.82
train mean loss: 199.52
epoch train time: 0:00:17.499776
elapsed time: 0:39:18.634839
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-24 20:43:31.800283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.11
 ---- batch: 020 ----
mean loss: 201.48
 ---- batch: 030 ----
mean loss: 205.52
 ---- batch: 040 ----
mean loss: 209.93
 ---- batch: 050 ----
mean loss: 199.49
 ---- batch: 060 ----
mean loss: 196.36
 ---- batch: 070 ----
mean loss: 202.28
 ---- batch: 080 ----
mean loss: 203.32
 ---- batch: 090 ----
mean loss: 207.79
train mean loss: 203.64
epoch train time: 0:00:17.474766
elapsed time: 0:39:36.110978
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-24 20:43:49.276400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.45
 ---- batch: 020 ----
mean loss: 197.28
 ---- batch: 030 ----
mean loss: 196.28
 ---- batch: 040 ----
mean loss: 203.10
 ---- batch: 050 ----
mean loss: 197.58
 ---- batch: 060 ----
mean loss: 193.84
 ---- batch: 070 ----
mean loss: 199.66
 ---- batch: 080 ----
mean loss: 204.39
 ---- batch: 090 ----
mean loss: 201.80
train mean loss: 199.61
epoch train time: 0:00:17.430602
elapsed time: 0:39:53.542880
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-24 20:44:06.708243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.54
 ---- batch: 020 ----
mean loss: 196.83
 ---- batch: 030 ----
mean loss: 196.29
 ---- batch: 040 ----
mean loss: 202.13
 ---- batch: 050 ----
mean loss: 197.02
 ---- batch: 060 ----
mean loss: 197.04
 ---- batch: 070 ----
mean loss: 200.11
 ---- batch: 080 ----
mean loss: 196.68
 ---- batch: 090 ----
mean loss: 205.24
train mean loss: 198.66
epoch train time: 0:00:17.440490
elapsed time: 0:40:10.984620
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-24 20:44:24.149992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.00
 ---- batch: 020 ----
mean loss: 196.78
 ---- batch: 030 ----
mean loss: 208.48
 ---- batch: 040 ----
mean loss: 208.92
 ---- batch: 050 ----
mean loss: 200.90
 ---- batch: 060 ----
mean loss: 189.46
 ---- batch: 070 ----
mean loss: 200.54
 ---- batch: 080 ----
mean loss: 206.22
 ---- batch: 090 ----
mean loss: 200.48
train mean loss: 201.67
epoch train time: 0:00:17.493747
elapsed time: 0:40:28.479597
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-24 20:44:41.644943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.36
 ---- batch: 020 ----
mean loss: 198.36
 ---- batch: 030 ----
mean loss: 195.52
 ---- batch: 040 ----
mean loss: 194.84
 ---- batch: 050 ----
mean loss: 200.20
 ---- batch: 060 ----
mean loss: 200.43
 ---- batch: 070 ----
mean loss: 196.92
 ---- batch: 080 ----
mean loss: 204.09
 ---- batch: 090 ----
mean loss: 194.49
train mean loss: 198.57
epoch train time: 0:00:17.456686
elapsed time: 0:40:45.937550
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-24 20:44:59.102890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.63
 ---- batch: 020 ----
mean loss: 189.96
 ---- batch: 030 ----
mean loss: 195.28
 ---- batch: 040 ----
mean loss: 199.54
 ---- batch: 050 ----
mean loss: 200.67
 ---- batch: 060 ----
mean loss: 194.29
 ---- batch: 070 ----
mean loss: 198.39
 ---- batch: 080 ----
mean loss: 200.77
 ---- batch: 090 ----
mean loss: 202.53
train mean loss: 199.00
epoch train time: 0:00:17.444217
elapsed time: 0:41:03.382896
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-24 20:45:16.548231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.24
 ---- batch: 020 ----
mean loss: 199.61
 ---- batch: 030 ----
mean loss: 194.91
 ---- batch: 040 ----
mean loss: 198.01
 ---- batch: 050 ----
mean loss: 199.57
 ---- batch: 060 ----
mean loss: 207.61
 ---- batch: 070 ----
mean loss: 196.77
 ---- batch: 080 ----
mean loss: 195.44
 ---- batch: 090 ----
mean loss: 194.72
train mean loss: 198.23
epoch train time: 0:00:17.519422
elapsed time: 0:41:20.903558
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-24 20:45:34.068931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.72
 ---- batch: 020 ----
mean loss: 202.46
 ---- batch: 030 ----
mean loss: 199.56
 ---- batch: 040 ----
mean loss: 202.21
 ---- batch: 050 ----
mean loss: 193.94
 ---- batch: 060 ----
mean loss: 195.08
 ---- batch: 070 ----
mean loss: 193.65
 ---- batch: 080 ----
mean loss: 198.75
 ---- batch: 090 ----
mean loss: 202.24
train mean loss: 199.90
epoch train time: 0:00:17.492981
elapsed time: 0:41:38.397732
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-24 20:45:51.563070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.03
 ---- batch: 020 ----
mean loss: 196.21
 ---- batch: 030 ----
mean loss: 201.96
 ---- batch: 040 ----
mean loss: 197.64
 ---- batch: 050 ----
mean loss: 194.99
 ---- batch: 060 ----
mean loss: 200.15
 ---- batch: 070 ----
mean loss: 197.97
 ---- batch: 080 ----
mean loss: 195.90
 ---- batch: 090 ----
mean loss: 197.64
train mean loss: 197.90
epoch train time: 0:00:17.454337
elapsed time: 0:41:55.853264
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-24 20:46:09.018711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.69
 ---- batch: 020 ----
mean loss: 197.64
 ---- batch: 030 ----
mean loss: 196.25
 ---- batch: 040 ----
mean loss: 199.19
 ---- batch: 050 ----
mean loss: 192.63
 ---- batch: 060 ----
mean loss: 198.92
 ---- batch: 070 ----
mean loss: 204.34
 ---- batch: 080 ----
mean loss: 201.62
 ---- batch: 090 ----
mean loss: 192.77
train mean loss: 198.21
epoch train time: 0:00:17.420488
elapsed time: 0:42:13.275018
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-24 20:46:26.440382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.34
 ---- batch: 020 ----
mean loss: 205.44
 ---- batch: 030 ----
mean loss: 205.07
 ---- batch: 040 ----
mean loss: 194.09
 ---- batch: 050 ----
mean loss: 201.30
 ---- batch: 060 ----
mean loss: 196.13
 ---- batch: 070 ----
mean loss: 194.01
 ---- batch: 080 ----
mean loss: 197.90
 ---- batch: 090 ----
mean loss: 204.51
train mean loss: 199.83
epoch train time: 0:00:17.370827
elapsed time: 0:42:30.647051
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-24 20:46:43.812363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.53
 ---- batch: 020 ----
mean loss: 194.06
 ---- batch: 030 ----
mean loss: 200.34
 ---- batch: 040 ----
mean loss: 194.61
 ---- batch: 050 ----
mean loss: 191.47
 ---- batch: 060 ----
mean loss: 200.44
 ---- batch: 070 ----
mean loss: 200.90
 ---- batch: 080 ----
mean loss: 203.37
 ---- batch: 090 ----
mean loss: 190.69
train mean loss: 196.23
epoch train time: 0:00:17.498731
elapsed time: 0:42:48.146962
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-24 20:47:01.312323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.29
 ---- batch: 020 ----
mean loss: 195.57
 ---- batch: 030 ----
mean loss: 193.98
 ---- batch: 040 ----
mean loss: 199.10
 ---- batch: 050 ----
mean loss: 198.02
 ---- batch: 060 ----
mean loss: 196.07
 ---- batch: 070 ----
mean loss: 195.82
 ---- batch: 080 ----
mean loss: 196.75
 ---- batch: 090 ----
mean loss: 200.97
train mean loss: 196.22
epoch train time: 0:00:17.354452
elapsed time: 0:43:05.502666
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-24 20:47:18.668114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.12
 ---- batch: 020 ----
mean loss: 197.57
 ---- batch: 030 ----
mean loss: 205.70
 ---- batch: 040 ----
mean loss: 182.93
 ---- batch: 050 ----
mean loss: 203.02
 ---- batch: 060 ----
mean loss: 194.64
 ---- batch: 070 ----
mean loss: 199.11
 ---- batch: 080 ----
mean loss: 192.17
 ---- batch: 090 ----
mean loss: 199.68
train mean loss: 196.25
epoch train time: 0:00:17.386447
elapsed time: 0:43:22.890523
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-24 20:47:36.056050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.64
 ---- batch: 020 ----
mean loss: 193.90
 ---- batch: 030 ----
mean loss: 200.56
 ---- batch: 040 ----
mean loss: 203.27
 ---- batch: 050 ----
mean loss: 206.24
 ---- batch: 060 ----
mean loss: 203.31
 ---- batch: 070 ----
mean loss: 201.06
 ---- batch: 080 ----
mean loss: 206.89
 ---- batch: 090 ----
mean loss: 198.33
train mean loss: 201.24
epoch train time: 0:00:17.330837
elapsed time: 0:43:40.222678
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-24 20:47:53.388101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.71
 ---- batch: 020 ----
mean loss: 202.64
 ---- batch: 030 ----
mean loss: 189.05
 ---- batch: 040 ----
mean loss: 199.66
 ---- batch: 050 ----
mean loss: 190.21
 ---- batch: 060 ----
mean loss: 187.84
 ---- batch: 070 ----
mean loss: 198.23
 ---- batch: 080 ----
mean loss: 189.59
 ---- batch: 090 ----
mean loss: 190.61
train mean loss: 195.40
epoch train time: 0:00:17.373024
elapsed time: 0:43:57.597069
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-24 20:48:10.762469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.11
 ---- batch: 020 ----
mean loss: 203.36
 ---- batch: 030 ----
mean loss: 197.03
 ---- batch: 040 ----
mean loss: 195.70
 ---- batch: 050 ----
mean loss: 185.59
 ---- batch: 060 ----
mean loss: 200.91
 ---- batch: 070 ----
mean loss: 195.71
 ---- batch: 080 ----
mean loss: 191.76
 ---- batch: 090 ----
mean loss: 196.04
train mean loss: 195.20
epoch train time: 0:00:17.230019
elapsed time: 0:44:14.828319
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-24 20:48:27.993669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.40
 ---- batch: 020 ----
mean loss: 198.55
 ---- batch: 030 ----
mean loss: 190.65
 ---- batch: 040 ----
mean loss: 200.68
 ---- batch: 050 ----
mean loss: 201.17
 ---- batch: 060 ----
mean loss: 198.13
 ---- batch: 070 ----
mean loss: 192.50
 ---- batch: 080 ----
mean loss: 200.01
 ---- batch: 090 ----
mean loss: 193.50
train mean loss: 195.30
epoch train time: 0:00:17.220776
elapsed time: 0:44:32.050289
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-24 20:48:45.215699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.53
 ---- batch: 020 ----
mean loss: 197.97
 ---- batch: 030 ----
mean loss: 201.52
 ---- batch: 040 ----
mean loss: 200.67
 ---- batch: 050 ----
mean loss: 194.81
 ---- batch: 060 ----
mean loss: 194.98
 ---- batch: 070 ----
mean loss: 195.98
 ---- batch: 080 ----
mean loss: 190.04
 ---- batch: 090 ----
mean loss: 192.84
train mean loss: 195.80
epoch train time: 0:00:17.260157
elapsed time: 0:44:49.311647
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-24 20:49:02.477105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.51
 ---- batch: 020 ----
mean loss: 199.43
 ---- batch: 030 ----
mean loss: 196.27
 ---- batch: 040 ----
mean loss: 193.09
 ---- batch: 050 ----
mean loss: 193.93
 ---- batch: 060 ----
mean loss: 198.26
 ---- batch: 070 ----
mean loss: 189.50
 ---- batch: 080 ----
mean loss: 198.71
 ---- batch: 090 ----
mean loss: 200.05
train mean loss: 195.30
epoch train time: 0:00:17.283621
elapsed time: 0:45:06.596572
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-24 20:49:19.762032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.31
 ---- batch: 020 ----
mean loss: 197.24
 ---- batch: 030 ----
mean loss: 196.99
 ---- batch: 040 ----
mean loss: 196.07
 ---- batch: 050 ----
mean loss: 187.78
 ---- batch: 060 ----
mean loss: 190.55
 ---- batch: 070 ----
mean loss: 196.34
 ---- batch: 080 ----
mean loss: 196.93
 ---- batch: 090 ----
mean loss: 195.13
train mean loss: 195.28
epoch train time: 0:00:17.309909
elapsed time: 0:45:23.908274
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-24 20:49:37.073254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.27
 ---- batch: 020 ----
mean loss: 198.87
 ---- batch: 030 ----
mean loss: 191.74
 ---- batch: 040 ----
mean loss: 194.25
 ---- batch: 050 ----
mean loss: 194.75
 ---- batch: 060 ----
mean loss: 198.92
 ---- batch: 070 ----
mean loss: 195.14
 ---- batch: 080 ----
mean loss: 190.96
 ---- batch: 090 ----
mean loss: 193.59
train mean loss: 194.39
epoch train time: 0:00:17.267283
elapsed time: 0:45:41.176346
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-24 20:49:54.341733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.50
 ---- batch: 020 ----
mean loss: 200.58
 ---- batch: 030 ----
mean loss: 189.47
 ---- batch: 040 ----
mean loss: 198.53
 ---- batch: 050 ----
mean loss: 195.66
 ---- batch: 060 ----
mean loss: 201.15
 ---- batch: 070 ----
mean loss: 188.71
 ---- batch: 080 ----
mean loss: 202.74
 ---- batch: 090 ----
mean loss: 197.24
train mean loss: 197.04
epoch train time: 0:00:17.333925
elapsed time: 0:45:58.511526
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-24 20:50:11.676954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.98
 ---- batch: 020 ----
mean loss: 194.84
 ---- batch: 030 ----
mean loss: 193.52
 ---- batch: 040 ----
mean loss: 192.95
 ---- batch: 050 ----
mean loss: 192.59
 ---- batch: 060 ----
mean loss: 210.57
 ---- batch: 070 ----
mean loss: 200.35
 ---- batch: 080 ----
mean loss: 197.28
 ---- batch: 090 ----
mean loss: 200.89
train mean loss: 196.89
epoch train time: 0:00:17.291016
elapsed time: 0:46:15.803765
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-24 20:50:28.969166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.24
 ---- batch: 020 ----
mean loss: 195.47
 ---- batch: 030 ----
mean loss: 197.82
 ---- batch: 040 ----
mean loss: 188.73
 ---- batch: 050 ----
mean loss: 191.54
 ---- batch: 060 ----
mean loss: 193.76
 ---- batch: 070 ----
mean loss: 197.33
 ---- batch: 080 ----
mean loss: 190.99
 ---- batch: 090 ----
mean loss: 194.46
train mean loss: 193.57
epoch train time: 0:00:17.234926
elapsed time: 0:46:33.039888
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-24 20:50:46.205218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.14
 ---- batch: 020 ----
mean loss: 189.38
 ---- batch: 030 ----
mean loss: 196.86
 ---- batch: 040 ----
mean loss: 199.59
 ---- batch: 050 ----
mean loss: 192.07
 ---- batch: 060 ----
mean loss: 189.49
 ---- batch: 070 ----
mean loss: 189.42
 ---- batch: 080 ----
mean loss: 189.07
 ---- batch: 090 ----
mean loss: 191.67
train mean loss: 192.89
epoch train time: 0:00:17.241576
elapsed time: 0:46:50.282621
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-24 20:51:03.447941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.39
 ---- batch: 020 ----
mean loss: 194.23
 ---- batch: 030 ----
mean loss: 188.97
 ---- batch: 040 ----
mean loss: 193.34
 ---- batch: 050 ----
mean loss: 194.72
 ---- batch: 060 ----
mean loss: 202.81
 ---- batch: 070 ----
mean loss: 199.10
 ---- batch: 080 ----
mean loss: 194.15
 ---- batch: 090 ----
mean loss: 191.71
train mean loss: 194.28
epoch train time: 0:00:17.325095
elapsed time: 0:47:07.608830
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-24 20:51:20.774150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.30
 ---- batch: 020 ----
mean loss: 195.01
 ---- batch: 030 ----
mean loss: 196.73
 ---- batch: 040 ----
mean loss: 189.38
 ---- batch: 050 ----
mean loss: 186.66
 ---- batch: 060 ----
mean loss: 200.48
 ---- batch: 070 ----
mean loss: 192.12
 ---- batch: 080 ----
mean loss: 188.87
 ---- batch: 090 ----
mean loss: 189.66
train mean loss: 192.62
epoch train time: 0:00:17.259412
elapsed time: 0:47:24.869420
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-24 20:51:38.034809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.01
 ---- batch: 020 ----
mean loss: 186.91
 ---- batch: 030 ----
mean loss: 197.55
 ---- batch: 040 ----
mean loss: 199.67
 ---- batch: 050 ----
mean loss: 183.02
 ---- batch: 060 ----
mean loss: 193.15
 ---- batch: 070 ----
mean loss: 197.93
 ---- batch: 080 ----
mean loss: 209.05
 ---- batch: 090 ----
mean loss: 194.49
train mean loss: 194.74
epoch train time: 0:00:17.426475
elapsed time: 0:47:42.297193
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-24 20:51:55.462600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.46
 ---- batch: 020 ----
mean loss: 203.46
 ---- batch: 030 ----
mean loss: 196.38
 ---- batch: 040 ----
mean loss: 195.28
 ---- batch: 050 ----
mean loss: 197.60
 ---- batch: 060 ----
mean loss: 197.31
 ---- batch: 070 ----
mean loss: 190.88
 ---- batch: 080 ----
mean loss: 188.77
 ---- batch: 090 ----
mean loss: 189.84
train mean loss: 195.06
epoch train time: 0:00:17.507820
elapsed time: 0:47:59.806235
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-24 20:52:12.971556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.12
 ---- batch: 020 ----
mean loss: 191.15
 ---- batch: 030 ----
mean loss: 190.00
 ---- batch: 040 ----
mean loss: 186.92
 ---- batch: 050 ----
mean loss: 187.78
 ---- batch: 060 ----
mean loss: 189.56
 ---- batch: 070 ----
mean loss: 194.65
 ---- batch: 080 ----
mean loss: 196.45
 ---- batch: 090 ----
mean loss: 192.46
train mean loss: 192.43
epoch train time: 0:00:17.390546
elapsed time: 0:48:17.197866
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-24 20:52:30.363191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.61
 ---- batch: 020 ----
mean loss: 182.73
 ---- batch: 030 ----
mean loss: 194.22
 ---- batch: 040 ----
mean loss: 194.29
 ---- batch: 050 ----
mean loss: 197.40
 ---- batch: 060 ----
mean loss: 192.84
 ---- batch: 070 ----
mean loss: 196.19
 ---- batch: 080 ----
mean loss: 193.65
 ---- batch: 090 ----
mean loss: 190.87
train mean loss: 192.18
epoch train time: 0:00:17.396419
elapsed time: 0:48:34.595521
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-24 20:52:47.760899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.21
 ---- batch: 020 ----
mean loss: 194.27
 ---- batch: 030 ----
mean loss: 187.57
 ---- batch: 040 ----
mean loss: 195.33
 ---- batch: 050 ----
mean loss: 192.77
 ---- batch: 060 ----
mean loss: 191.43
 ---- batch: 070 ----
mean loss: 202.58
 ---- batch: 080 ----
mean loss: 190.00
 ---- batch: 090 ----
mean loss: 188.90
train mean loss: 192.49
epoch train time: 0:00:17.444659
elapsed time: 0:48:52.041595
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-24 20:53:05.207010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.61
 ---- batch: 020 ----
mean loss: 186.87
 ---- batch: 030 ----
mean loss: 195.07
 ---- batch: 040 ----
mean loss: 191.40
 ---- batch: 050 ----
mean loss: 192.34
 ---- batch: 060 ----
mean loss: 197.93
 ---- batch: 070 ----
mean loss: 196.43
 ---- batch: 080 ----
mean loss: 191.79
 ---- batch: 090 ----
mean loss: 191.35
train mean loss: 192.52
epoch train time: 0:00:17.667115
elapsed time: 0:49:09.709985
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-24 20:53:22.875388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.19
 ---- batch: 020 ----
mean loss: 198.16
 ---- batch: 030 ----
mean loss: 201.24
 ---- batch: 040 ----
mean loss: 202.49
 ---- batch: 050 ----
mean loss: 188.58
 ---- batch: 060 ----
mean loss: 195.76
 ---- batch: 070 ----
mean loss: 191.64
 ---- batch: 080 ----
mean loss: 195.36
 ---- batch: 090 ----
mean loss: 186.10
train mean loss: 193.67
epoch train time: 0:00:17.719958
elapsed time: 0:49:27.431279
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-24 20:53:40.596645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.69
 ---- batch: 020 ----
mean loss: 183.54
 ---- batch: 030 ----
mean loss: 189.40
 ---- batch: 040 ----
mean loss: 194.45
 ---- batch: 050 ----
mean loss: 195.32
 ---- batch: 060 ----
mean loss: 193.26
 ---- batch: 070 ----
mean loss: 191.17
 ---- batch: 080 ----
mean loss: 186.14
 ---- batch: 090 ----
mean loss: 191.73
train mean loss: 191.47
epoch train time: 0:00:17.730567
elapsed time: 0:49:45.163050
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-24 20:53:58.328379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.65
 ---- batch: 020 ----
mean loss: 186.80
 ---- batch: 030 ----
mean loss: 188.99
 ---- batch: 040 ----
mean loss: 185.95
 ---- batch: 050 ----
mean loss: 196.40
 ---- batch: 060 ----
mean loss: 192.62
 ---- batch: 070 ----
mean loss: 193.01
 ---- batch: 080 ----
mean loss: 190.49
 ---- batch: 090 ----
mean loss: 197.50
train mean loss: 191.38
epoch train time: 0:00:17.763699
elapsed time: 0:50:02.928042
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-24 20:54:16.093376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.77
 ---- batch: 020 ----
mean loss: 191.70
 ---- batch: 030 ----
mean loss: 187.26
 ---- batch: 040 ----
mean loss: 200.29
 ---- batch: 050 ----
mean loss: 190.27
 ---- batch: 060 ----
mean loss: 198.56
 ---- batch: 070 ----
mean loss: 189.16
 ---- batch: 080 ----
mean loss: 188.14
 ---- batch: 090 ----
mean loss: 196.14
train mean loss: 192.20
epoch train time: 0:00:17.755211
elapsed time: 0:50:20.684545
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-24 20:54:33.849932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.06
 ---- batch: 020 ----
mean loss: 190.06
 ---- batch: 030 ----
mean loss: 194.62
 ---- batch: 040 ----
mean loss: 190.34
 ---- batch: 050 ----
mean loss: 190.94
 ---- batch: 060 ----
mean loss: 188.82
 ---- batch: 070 ----
mean loss: 181.78
 ---- batch: 080 ----
mean loss: 198.39
 ---- batch: 090 ----
mean loss: 204.67
train mean loss: 192.96
epoch train time: 0:00:17.754380
elapsed time: 0:50:38.440136
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-24 20:54:51.605459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.32
 ---- batch: 020 ----
mean loss: 188.90
 ---- batch: 030 ----
mean loss: 194.39
 ---- batch: 040 ----
mean loss: 185.15
 ---- batch: 050 ----
mean loss: 192.00
 ---- batch: 060 ----
mean loss: 188.13
 ---- batch: 070 ----
mean loss: 191.32
 ---- batch: 080 ----
mean loss: 196.44
 ---- batch: 090 ----
mean loss: 192.62
train mean loss: 191.71
epoch train time: 0:00:17.789711
elapsed time: 0:50:56.231121
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-24 20:55:09.396517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.70
 ---- batch: 020 ----
mean loss: 193.87
 ---- batch: 030 ----
mean loss: 186.49
 ---- batch: 040 ----
mean loss: 194.62
 ---- batch: 050 ----
mean loss: 194.13
 ---- batch: 060 ----
mean loss: 205.06
 ---- batch: 070 ----
mean loss: 184.83
 ---- batch: 080 ----
mean loss: 193.58
 ---- batch: 090 ----
mean loss: 191.58
train mean loss: 193.70
epoch train time: 0:00:17.746021
elapsed time: 0:51:13.978388
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-24 20:55:27.143734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.46
 ---- batch: 020 ----
mean loss: 187.54
 ---- batch: 030 ----
mean loss: 185.74
 ---- batch: 040 ----
mean loss: 188.52
 ---- batch: 050 ----
mean loss: 191.80
 ---- batch: 060 ----
mean loss: 197.78
 ---- batch: 070 ----
mean loss: 193.09
 ---- batch: 080 ----
mean loss: 192.55
 ---- batch: 090 ----
mean loss: 186.53
train mean loss: 190.61
epoch train time: 0:00:17.788396
elapsed time: 0:51:31.768137
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-24 20:55:44.933515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.95
 ---- batch: 020 ----
mean loss: 192.67
 ---- batch: 030 ----
mean loss: 189.43
 ---- batch: 040 ----
mean loss: 189.45
 ---- batch: 050 ----
mean loss: 202.50
 ---- batch: 060 ----
mean loss: 189.76
 ---- batch: 070 ----
mean loss: 190.96
 ---- batch: 080 ----
mean loss: 181.74
 ---- batch: 090 ----
mean loss: 194.93
train mean loss: 192.79
epoch train time: 0:00:17.789946
elapsed time: 0:51:49.559309
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-24 20:56:02.724646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.15
 ---- batch: 020 ----
mean loss: 188.60
 ---- batch: 030 ----
mean loss: 186.15
 ---- batch: 040 ----
mean loss: 191.10
 ---- batch: 050 ----
mean loss: 188.56
 ---- batch: 060 ----
mean loss: 192.50
 ---- batch: 070 ----
mean loss: 198.67
 ---- batch: 080 ----
mean loss: 188.36
 ---- batch: 090 ----
mean loss: 196.89
train mean loss: 191.17
epoch train time: 0:00:17.737749
elapsed time: 0:52:07.298222
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-24 20:56:20.463678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.58
 ---- batch: 020 ----
mean loss: 188.18
 ---- batch: 030 ----
mean loss: 190.06
 ---- batch: 040 ----
mean loss: 193.79
 ---- batch: 050 ----
mean loss: 189.31
 ---- batch: 060 ----
mean loss: 190.01
 ---- batch: 070 ----
mean loss: 192.91
 ---- batch: 080 ----
mean loss: 189.65
 ---- batch: 090 ----
mean loss: 186.53
train mean loss: 190.72
epoch train time: 0:00:17.766918
elapsed time: 0:52:25.066489
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-24 20:56:38.231998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.05
 ---- batch: 020 ----
mean loss: 183.91
 ---- batch: 030 ----
mean loss: 192.59
 ---- batch: 040 ----
mean loss: 183.76
 ---- batch: 050 ----
mean loss: 186.21
 ---- batch: 060 ----
mean loss: 194.42
 ---- batch: 070 ----
mean loss: 195.23
 ---- batch: 080 ----
mean loss: 202.29
 ---- batch: 090 ----
mean loss: 187.90
train mean loss: 189.99
epoch train time: 0:00:17.799157
elapsed time: 0:52:42.867260
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-24 20:56:56.032243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.32
 ---- batch: 020 ----
mean loss: 197.61
 ---- batch: 030 ----
mean loss: 197.04
 ---- batch: 040 ----
mean loss: 182.70
 ---- batch: 050 ----
mean loss: 187.93
 ---- batch: 060 ----
mean loss: 196.32
 ---- batch: 070 ----
mean loss: 188.89
 ---- batch: 080 ----
mean loss: 190.79
 ---- batch: 090 ----
mean loss: 191.17
train mean loss: 191.17
epoch train time: 0:00:17.842708
elapsed time: 0:53:00.710842
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-24 20:57:13.876167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.41
 ---- batch: 020 ----
mean loss: 186.68
 ---- batch: 030 ----
mean loss: 187.26
 ---- batch: 040 ----
mean loss: 191.31
 ---- batch: 050 ----
mean loss: 190.94
 ---- batch: 060 ----
mean loss: 192.09
 ---- batch: 070 ----
mean loss: 194.37
 ---- batch: 080 ----
mean loss: 184.52
 ---- batch: 090 ----
mean loss: 189.27
train mean loss: 190.02
epoch train time: 0:00:17.835366
elapsed time: 0:53:18.547522
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-24 20:57:31.712921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.31
 ---- batch: 020 ----
mean loss: 190.28
 ---- batch: 030 ----
mean loss: 187.57
 ---- batch: 040 ----
mean loss: 191.46
 ---- batch: 050 ----
mean loss: 194.08
 ---- batch: 060 ----
mean loss: 188.72
 ---- batch: 070 ----
mean loss: 180.09
 ---- batch: 080 ----
mean loss: 182.45
 ---- batch: 090 ----
mean loss: 193.90
train mean loss: 188.17
epoch train time: 0:00:17.760593
elapsed time: 0:53:36.309426
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-24 20:57:49.474755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.36
 ---- batch: 020 ----
mean loss: 189.24
 ---- batch: 030 ----
mean loss: 190.69
 ---- batch: 040 ----
mean loss: 185.01
 ---- batch: 050 ----
mean loss: 184.07
 ---- batch: 060 ----
mean loss: 190.26
 ---- batch: 070 ----
mean loss: 194.41
 ---- batch: 080 ----
mean loss: 184.33
 ---- batch: 090 ----
mean loss: 185.05
train mean loss: 188.28
epoch train time: 0:00:17.795755
elapsed time: 0:53:54.106379
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-24 20:58:07.271750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.46
 ---- batch: 020 ----
mean loss: 189.99
 ---- batch: 030 ----
mean loss: 191.38
 ---- batch: 040 ----
mean loss: 189.39
 ---- batch: 050 ----
mean loss: 186.03
 ---- batch: 060 ----
mean loss: 190.25
 ---- batch: 070 ----
mean loss: 190.92
 ---- batch: 080 ----
mean loss: 178.26
 ---- batch: 090 ----
mean loss: 187.87
train mean loss: 188.22
epoch train time: 0:00:17.835868
elapsed time: 0:54:11.943551
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-24 20:58:25.108977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.73
 ---- batch: 020 ----
mean loss: 188.57
 ---- batch: 030 ----
mean loss: 191.45
 ---- batch: 040 ----
mean loss: 184.12
 ---- batch: 050 ----
mean loss: 191.12
 ---- batch: 060 ----
mean loss: 188.76
 ---- batch: 070 ----
mean loss: 185.98
 ---- batch: 080 ----
mean loss: 188.71
 ---- batch: 090 ----
mean loss: 192.64
train mean loss: 189.09
epoch train time: 0:00:18.044409
elapsed time: 0:54:29.989363
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-24 20:58:43.154796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.79
 ---- batch: 020 ----
mean loss: 189.86
 ---- batch: 030 ----
mean loss: 188.94
 ---- batch: 040 ----
mean loss: 191.02
 ---- batch: 050 ----
mean loss: 189.88
 ---- batch: 060 ----
mean loss: 181.14
 ---- batch: 070 ----
mean loss: 183.53
 ---- batch: 080 ----
mean loss: 186.35
 ---- batch: 090 ----
mean loss: 185.57
train mean loss: 188.29
epoch train time: 0:00:17.812748
elapsed time: 0:54:47.803385
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-24 20:59:00.968724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.40
 ---- batch: 020 ----
mean loss: 190.71
 ---- batch: 030 ----
mean loss: 188.92
 ---- batch: 040 ----
mean loss: 190.00
 ---- batch: 050 ----
mean loss: 184.29
 ---- batch: 060 ----
mean loss: 189.32
 ---- batch: 070 ----
mean loss: 185.69
 ---- batch: 080 ----
mean loss: 186.98
 ---- batch: 090 ----
mean loss: 184.50
train mean loss: 187.80
epoch train time: 0:00:17.822367
elapsed time: 0:55:05.626994
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-24 20:59:18.792330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.94
 ---- batch: 020 ----
mean loss: 187.85
 ---- batch: 030 ----
mean loss: 195.41
 ---- batch: 040 ----
mean loss: 192.81
 ---- batch: 050 ----
mean loss: 183.82
 ---- batch: 060 ----
mean loss: 193.92
 ---- batch: 070 ----
mean loss: 185.77
 ---- batch: 080 ----
mean loss: 188.78
 ---- batch: 090 ----
mean loss: 181.87
train mean loss: 189.00
epoch train time: 0:00:17.815948
elapsed time: 0:55:23.444137
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-24 20:59:36.609528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.42
 ---- batch: 020 ----
mean loss: 187.51
 ---- batch: 030 ----
mean loss: 184.12
 ---- batch: 040 ----
mean loss: 191.93
 ---- batch: 050 ----
mean loss: 189.94
 ---- batch: 060 ----
mean loss: 188.16
 ---- batch: 070 ----
mean loss: 185.54
 ---- batch: 080 ----
mean loss: 191.13
 ---- batch: 090 ----
mean loss: 186.94
train mean loss: 188.41
epoch train time: 0:00:17.766014
elapsed time: 0:55:41.211392
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-24 20:59:54.376727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.99
 ---- batch: 020 ----
mean loss: 189.97
 ---- batch: 030 ----
mean loss: 183.99
 ---- batch: 040 ----
mean loss: 190.99
 ---- batch: 050 ----
mean loss: 191.15
 ---- batch: 060 ----
mean loss: 191.64
 ---- batch: 070 ----
mean loss: 185.81
 ---- batch: 080 ----
mean loss: 182.16
 ---- batch: 090 ----
mean loss: 188.93
train mean loss: 188.42
epoch train time: 0:00:17.816478
elapsed time: 0:55:59.029049
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-24 21:00:12.194411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.90
 ---- batch: 020 ----
mean loss: 182.59
 ---- batch: 030 ----
mean loss: 181.80
 ---- batch: 040 ----
mean loss: 194.65
 ---- batch: 050 ----
mean loss: 201.19
 ---- batch: 060 ----
mean loss: 190.65
 ---- batch: 070 ----
mean loss: 186.59
 ---- batch: 080 ----
mean loss: 185.62
 ---- batch: 090 ----
mean loss: 189.10
train mean loss: 188.84
epoch train time: 0:00:17.812343
elapsed time: 0:56:16.842651
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-24 21:00:30.008040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.85
 ---- batch: 020 ----
mean loss: 180.39
 ---- batch: 030 ----
mean loss: 191.42
 ---- batch: 040 ----
mean loss: 185.14
 ---- batch: 050 ----
mean loss: 184.62
 ---- batch: 060 ----
mean loss: 191.08
 ---- batch: 070 ----
mean loss: 193.50
 ---- batch: 080 ----
mean loss: 183.38
 ---- batch: 090 ----
mean loss: 194.28
train mean loss: 187.66
epoch train time: 0:00:17.798966
elapsed time: 0:56:34.642860
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-24 21:00:47.808213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.16
 ---- batch: 020 ----
mean loss: 195.45
 ---- batch: 030 ----
mean loss: 185.35
 ---- batch: 040 ----
mean loss: 184.49
 ---- batch: 050 ----
mean loss: 186.61
 ---- batch: 060 ----
mean loss: 185.99
 ---- batch: 070 ----
mean loss: 187.83
 ---- batch: 080 ----
mean loss: 189.30
 ---- batch: 090 ----
mean loss: 185.38
train mean loss: 187.47
epoch train time: 0:00:17.817600
elapsed time: 0:56:52.461764
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-24 21:01:05.627176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.36
 ---- batch: 020 ----
mean loss: 191.40
 ---- batch: 030 ----
mean loss: 183.50
 ---- batch: 040 ----
mean loss: 180.90
 ---- batch: 050 ----
mean loss: 186.82
 ---- batch: 060 ----
mean loss: 186.82
 ---- batch: 070 ----
mean loss: 189.49
 ---- batch: 080 ----
mean loss: 189.11
 ---- batch: 090 ----
mean loss: 186.82
train mean loss: 186.96
epoch train time: 0:00:17.870257
elapsed time: 0:57:10.333320
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-24 21:01:23.498734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.37
 ---- batch: 020 ----
mean loss: 188.17
 ---- batch: 030 ----
mean loss: 209.06
 ---- batch: 040 ----
mean loss: 192.50
 ---- batch: 050 ----
mean loss: 192.48
 ---- batch: 060 ----
mean loss: 182.34
 ---- batch: 070 ----
mean loss: 185.15
 ---- batch: 080 ----
mean loss: 193.10
 ---- batch: 090 ----
mean loss: 193.23
train mean loss: 190.79
epoch train time: 0:00:17.876223
elapsed time: 0:57:28.210915
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-24 21:01:41.376289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.47
 ---- batch: 020 ----
mean loss: 184.04
 ---- batch: 030 ----
mean loss: 190.62
 ---- batch: 040 ----
mean loss: 194.19
 ---- batch: 050 ----
mean loss: 194.28
 ---- batch: 060 ----
mean loss: 185.51
 ---- batch: 070 ----
mean loss: 188.23
 ---- batch: 080 ----
mean loss: 189.62
 ---- batch: 090 ----
mean loss: 180.68
train mean loss: 187.74
epoch train time: 0:00:17.831124
elapsed time: 0:57:46.043276
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-24 21:01:59.208647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.82
 ---- batch: 020 ----
mean loss: 190.97
 ---- batch: 030 ----
mean loss: 190.14
 ---- batch: 040 ----
mean loss: 181.10
 ---- batch: 050 ----
mean loss: 184.96
 ---- batch: 060 ----
mean loss: 190.31
 ---- batch: 070 ----
mean loss: 185.11
 ---- batch: 080 ----
mean loss: 185.42
 ---- batch: 090 ----
mean loss: 185.91
train mean loss: 186.73
epoch train time: 0:00:17.817851
elapsed time: 0:58:03.862392
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-24 21:02:17.027763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.29
 ---- batch: 020 ----
mean loss: 190.55
 ---- batch: 030 ----
mean loss: 182.83
 ---- batch: 040 ----
mean loss: 185.24
 ---- batch: 050 ----
mean loss: 188.47
 ---- batch: 060 ----
mean loss: 193.27
 ---- batch: 070 ----
mean loss: 179.97
 ---- batch: 080 ----
mean loss: 182.65
 ---- batch: 090 ----
mean loss: 185.52
train mean loss: 186.01
epoch train time: 0:00:17.881079
elapsed time: 0:58:21.744802
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-24 21:02:34.910158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.14
 ---- batch: 020 ----
mean loss: 180.14
 ---- batch: 030 ----
mean loss: 186.38
 ---- batch: 040 ----
mean loss: 186.05
 ---- batch: 050 ----
mean loss: 183.87
 ---- batch: 060 ----
mean loss: 177.21
 ---- batch: 070 ----
mean loss: 190.32
 ---- batch: 080 ----
mean loss: 191.53
 ---- batch: 090 ----
mean loss: 192.35
train mean loss: 185.89
epoch train time: 0:00:17.793658
elapsed time: 0:58:39.539882
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-24 21:02:52.705151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.42
 ---- batch: 020 ----
mean loss: 186.24
 ---- batch: 030 ----
mean loss: 188.68
 ---- batch: 040 ----
mean loss: 187.96
 ---- batch: 050 ----
mean loss: 179.00
 ---- batch: 060 ----
mean loss: 187.67
 ---- batch: 070 ----
mean loss: 187.97
 ---- batch: 080 ----
mean loss: 188.87
 ---- batch: 090 ----
mean loss: 181.25
train mean loss: 186.15
epoch train time: 0:00:17.815932
elapsed time: 0:58:57.356952
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-24 21:03:10.522338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.37
 ---- batch: 020 ----
mean loss: 186.04
 ---- batch: 030 ----
mean loss: 190.79
 ---- batch: 040 ----
mean loss: 181.99
 ---- batch: 050 ----
mean loss: 183.07
 ---- batch: 060 ----
mean loss: 187.93
 ---- batch: 070 ----
mean loss: 189.64
 ---- batch: 080 ----
mean loss: 184.53
 ---- batch: 090 ----
mean loss: 181.78
train mean loss: 185.24
epoch train time: 0:00:17.829689
elapsed time: 0:59:15.187873
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-24 21:03:28.353261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.97
 ---- batch: 020 ----
mean loss: 191.99
 ---- batch: 030 ----
mean loss: 189.18
 ---- batch: 040 ----
mean loss: 191.98
 ---- batch: 050 ----
mean loss: 190.12
 ---- batch: 060 ----
mean loss: 187.94
 ---- batch: 070 ----
mean loss: 185.36
 ---- batch: 080 ----
mean loss: 178.81
 ---- batch: 090 ----
mean loss: 187.98
train mean loss: 187.94
epoch train time: 0:00:17.820967
elapsed time: 0:59:33.010032
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-24 21:03:46.175391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.69
 ---- batch: 020 ----
mean loss: 184.60
 ---- batch: 030 ----
mean loss: 186.10
 ---- batch: 040 ----
mean loss: 182.25
 ---- batch: 050 ----
mean loss: 183.32
 ---- batch: 060 ----
mean loss: 193.43
 ---- batch: 070 ----
mean loss: 183.64
 ---- batch: 080 ----
mean loss: 187.66
 ---- batch: 090 ----
mean loss: 194.95
train mean loss: 186.40
epoch train time: 0:00:17.792424
elapsed time: 0:59:50.803660
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-24 21:04:03.969016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.94
 ---- batch: 020 ----
mean loss: 182.92
 ---- batch: 030 ----
mean loss: 184.65
 ---- batch: 040 ----
mean loss: 195.27
 ---- batch: 050 ----
mean loss: 194.49
 ---- batch: 060 ----
mean loss: 187.37
 ---- batch: 070 ----
mean loss: 186.41
 ---- batch: 080 ----
mean loss: 189.85
 ---- batch: 090 ----
mean loss: 181.12
train mean loss: 186.74
epoch train time: 0:00:17.807710
elapsed time: 1:00:08.612584
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-24 21:04:21.777938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.99
 ---- batch: 020 ----
mean loss: 177.65
 ---- batch: 030 ----
mean loss: 187.36
 ---- batch: 040 ----
mean loss: 183.73
 ---- batch: 050 ----
mean loss: 185.32
 ---- batch: 060 ----
mean loss: 186.07
 ---- batch: 070 ----
mean loss: 182.61
 ---- batch: 080 ----
mean loss: 186.57
 ---- batch: 090 ----
mean loss: 184.65
train mean loss: 183.86
epoch train time: 0:00:17.826083
elapsed time: 1:00:26.439885
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-24 21:04:39.605236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.22
 ---- batch: 020 ----
mean loss: 183.20
 ---- batch: 030 ----
mean loss: 175.34
 ---- batch: 040 ----
mean loss: 184.64
 ---- batch: 050 ----
mean loss: 188.19
 ---- batch: 060 ----
mean loss: 182.88
 ---- batch: 070 ----
mean loss: 189.96
 ---- batch: 080 ----
mean loss: 190.06
 ---- batch: 090 ----
mean loss: 192.93
train mean loss: 185.75
epoch train time: 0:00:17.863872
elapsed time: 1:00:44.304999
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-24 21:04:57.470478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.35
 ---- batch: 020 ----
mean loss: 180.72
 ---- batch: 030 ----
mean loss: 181.02
 ---- batch: 040 ----
mean loss: 188.03
 ---- batch: 050 ----
mean loss: 180.96
 ---- batch: 060 ----
mean loss: 176.86
 ---- batch: 070 ----
mean loss: 181.61
 ---- batch: 080 ----
mean loss: 184.25
 ---- batch: 090 ----
mean loss: 185.04
train mean loss: 183.16
epoch train time: 0:00:17.844659
elapsed time: 1:01:02.151584
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-24 21:05:15.316580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.28
 ---- batch: 020 ----
mean loss: 178.09
 ---- batch: 030 ----
mean loss: 181.92
 ---- batch: 040 ----
mean loss: 175.94
 ---- batch: 050 ----
mean loss: 182.62
 ---- batch: 060 ----
mean loss: 183.12
 ---- batch: 070 ----
mean loss: 179.00
 ---- batch: 080 ----
mean loss: 183.52
 ---- batch: 090 ----
mean loss: 179.01
train mean loss: 181.45
epoch train time: 0:00:17.910689
elapsed time: 1:01:20.063138
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-24 21:05:33.228478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.53
 ---- batch: 020 ----
mean loss: 184.81
 ---- batch: 030 ----
mean loss: 177.99
 ---- batch: 040 ----
mean loss: 183.58
 ---- batch: 050 ----
mean loss: 184.35
 ---- batch: 060 ----
mean loss: 174.39
 ---- batch: 070 ----
mean loss: 185.81
 ---- batch: 080 ----
mean loss: 182.12
 ---- batch: 090 ----
mean loss: 181.14
train mean loss: 181.18
epoch train time: 0:00:17.876297
elapsed time: 1:01:37.940647
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-24 21:05:51.106070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.10
 ---- batch: 020 ----
mean loss: 181.11
 ---- batch: 030 ----
mean loss: 187.20
 ---- batch: 040 ----
mean loss: 178.34
 ---- batch: 050 ----
mean loss: 177.09
 ---- batch: 060 ----
mean loss: 180.28
 ---- batch: 070 ----
mean loss: 179.49
 ---- batch: 080 ----
mean loss: 192.85
 ---- batch: 090 ----
mean loss: 177.46
train mean loss: 180.94
epoch train time: 0:00:17.877798
elapsed time: 1:01:55.819711
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-24 21:06:08.985063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.07
 ---- batch: 020 ----
mean loss: 182.70
 ---- batch: 030 ----
mean loss: 189.01
 ---- batch: 040 ----
mean loss: 171.11
 ---- batch: 050 ----
mean loss: 182.45
 ---- batch: 060 ----
mean loss: 181.22
 ---- batch: 070 ----
mean loss: 176.60
 ---- batch: 080 ----
mean loss: 190.62
 ---- batch: 090 ----
mean loss: 177.58
train mean loss: 180.96
epoch train time: 0:00:17.842179
elapsed time: 1:02:13.663269
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-24 21:06:26.828733
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.87
 ---- batch: 020 ----
mean loss: 177.66
 ---- batch: 030 ----
mean loss: 178.55
 ---- batch: 040 ----
mean loss: 187.16
 ---- batch: 050 ----
mean loss: 179.76
 ---- batch: 060 ----
mean loss: 181.02
 ---- batch: 070 ----
mean loss: 187.21
 ---- batch: 080 ----
mean loss: 180.34
 ---- batch: 090 ----
mean loss: 182.24
train mean loss: 181.28
epoch train time: 0:00:17.844032
elapsed time: 1:02:31.508685
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-24 21:06:44.674048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.81
 ---- batch: 020 ----
mean loss: 175.08
 ---- batch: 030 ----
mean loss: 184.69
 ---- batch: 040 ----
mean loss: 185.49
 ---- batch: 050 ----
mean loss: 181.50
 ---- batch: 060 ----
mean loss: 178.46
 ---- batch: 070 ----
mean loss: 180.51
 ---- batch: 080 ----
mean loss: 176.91
 ---- batch: 090 ----
mean loss: 180.37
train mean loss: 181.10
epoch train time: 0:00:17.848773
elapsed time: 1:02:49.358609
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-24 21:07:02.523942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.45
 ---- batch: 020 ----
mean loss: 181.14
 ---- batch: 030 ----
mean loss: 182.29
 ---- batch: 040 ----
mean loss: 190.25
 ---- batch: 050 ----
mean loss: 181.59
 ---- batch: 060 ----
mean loss: 177.53
 ---- batch: 070 ----
mean loss: 178.92
 ---- batch: 080 ----
mean loss: 184.05
 ---- batch: 090 ----
mean loss: 176.33
train mean loss: 181.16
epoch train time: 0:00:17.826954
elapsed time: 1:03:07.186737
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-24 21:07:20.352114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.07
 ---- batch: 020 ----
mean loss: 186.46
 ---- batch: 030 ----
mean loss: 182.34
 ---- batch: 040 ----
mean loss: 172.71
 ---- batch: 050 ----
mean loss: 187.03
 ---- batch: 060 ----
mean loss: 188.15
 ---- batch: 070 ----
mean loss: 178.27
 ---- batch: 080 ----
mean loss: 177.51
 ---- batch: 090 ----
mean loss: 179.04
train mean loss: 181.13
epoch train time: 0:00:17.973051
elapsed time: 1:03:25.161027
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-24 21:07:38.326383
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.01
 ---- batch: 020 ----
mean loss: 183.21
 ---- batch: 030 ----
mean loss: 183.81
 ---- batch: 040 ----
mean loss: 180.43
 ---- batch: 050 ----
mean loss: 174.97
 ---- batch: 060 ----
mean loss: 186.24
 ---- batch: 070 ----
mean loss: 179.98
 ---- batch: 080 ----
mean loss: 180.21
 ---- batch: 090 ----
mean loss: 178.39
train mean loss: 180.68
epoch train time: 0:00:17.884809
elapsed time: 1:03:43.047073
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-24 21:07:56.212416
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.57
 ---- batch: 020 ----
mean loss: 175.12
 ---- batch: 030 ----
mean loss: 182.36
 ---- batch: 040 ----
mean loss: 179.67
 ---- batch: 050 ----
mean loss: 177.97
 ---- batch: 060 ----
mean loss: 179.32
 ---- batch: 070 ----
mean loss: 185.05
 ---- batch: 080 ----
mean loss: 182.84
 ---- batch: 090 ----
mean loss: 185.33
train mean loss: 180.86
epoch train time: 0:00:17.899504
elapsed time: 1:04:00.947748
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-24 21:08:14.113158
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.75
 ---- batch: 020 ----
mean loss: 180.15
 ---- batch: 030 ----
mean loss: 178.56
 ---- batch: 040 ----
mean loss: 188.14
 ---- batch: 050 ----
mean loss: 179.63
 ---- batch: 060 ----
mean loss: 174.11
 ---- batch: 070 ----
mean loss: 183.90
 ---- batch: 080 ----
mean loss: 183.92
 ---- batch: 090 ----
mean loss: 179.00
train mean loss: 181.01
epoch train time: 0:00:17.917596
elapsed time: 1:04:18.866710
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-24 21:08:32.032084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.70
 ---- batch: 020 ----
mean loss: 184.26
 ---- batch: 030 ----
mean loss: 177.39
 ---- batch: 040 ----
mean loss: 184.46
 ---- batch: 050 ----
mean loss: 181.25
 ---- batch: 060 ----
mean loss: 183.71
 ---- batch: 070 ----
mean loss: 177.85
 ---- batch: 080 ----
mean loss: 180.74
 ---- batch: 090 ----
mean loss: 180.48
train mean loss: 180.68
epoch train time: 0:00:17.841646
elapsed time: 1:04:36.709566
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-24 21:08:49.874924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.29
 ---- batch: 020 ----
mean loss: 170.01
 ---- batch: 030 ----
mean loss: 177.14
 ---- batch: 040 ----
mean loss: 180.35
 ---- batch: 050 ----
mean loss: 181.09
 ---- batch: 060 ----
mean loss: 184.95
 ---- batch: 070 ----
mean loss: 188.80
 ---- batch: 080 ----
mean loss: 188.35
 ---- batch: 090 ----
mean loss: 181.86
train mean loss: 181.26
epoch train time: 0:00:17.853629
elapsed time: 1:04:54.564384
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-24 21:09:07.729774
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.64
 ---- batch: 020 ----
mean loss: 182.97
 ---- batch: 030 ----
mean loss: 178.76
 ---- batch: 040 ----
mean loss: 180.25
 ---- batch: 050 ----
mean loss: 178.60
 ---- batch: 060 ----
mean loss: 182.09
 ---- batch: 070 ----
mean loss: 179.88
 ---- batch: 080 ----
mean loss: 183.30
 ---- batch: 090 ----
mean loss: 178.94
train mean loss: 181.24
epoch train time: 0:00:17.902026
elapsed time: 1:05:12.467634
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-24 21:09:25.632970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.56
 ---- batch: 020 ----
mean loss: 176.50
 ---- batch: 030 ----
mean loss: 185.44
 ---- batch: 040 ----
mean loss: 180.72
 ---- batch: 050 ----
mean loss: 180.92
 ---- batch: 060 ----
mean loss: 183.12
 ---- batch: 070 ----
mean loss: 179.40
 ---- batch: 080 ----
mean loss: 184.59
 ---- batch: 090 ----
mean loss: 177.51
train mean loss: 181.05
epoch train time: 0:00:17.884721
elapsed time: 1:05:30.353683
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-24 21:09:43.519043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.11
 ---- batch: 020 ----
mean loss: 179.75
 ---- batch: 030 ----
mean loss: 183.62
 ---- batch: 040 ----
mean loss: 180.82
 ---- batch: 050 ----
mean loss: 185.47
 ---- batch: 060 ----
mean loss: 177.59
 ---- batch: 070 ----
mean loss: 176.61
 ---- batch: 080 ----
mean loss: 184.87
 ---- batch: 090 ----
mean loss: 181.12
train mean loss: 180.93
epoch train time: 0:00:17.931795
elapsed time: 1:05:48.286636
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-24 21:10:01.451972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.67
 ---- batch: 020 ----
mean loss: 181.39
 ---- batch: 030 ----
mean loss: 184.66
 ---- batch: 040 ----
mean loss: 182.22
 ---- batch: 050 ----
mean loss: 177.58
 ---- batch: 060 ----
mean loss: 182.42
 ---- batch: 070 ----
mean loss: 176.09
 ---- batch: 080 ----
mean loss: 184.25
 ---- batch: 090 ----
mean loss: 183.81
train mean loss: 180.68
epoch train time: 0:00:17.852173
elapsed time: 1:06:06.140005
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-24 21:10:19.305334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.16
 ---- batch: 020 ----
mean loss: 181.55
 ---- batch: 030 ----
mean loss: 183.28
 ---- batch: 040 ----
mean loss: 181.38
 ---- batch: 050 ----
mean loss: 185.71
 ---- batch: 060 ----
mean loss: 175.37
 ---- batch: 070 ----
mean loss: 175.54
 ---- batch: 080 ----
mean loss: 186.15
 ---- batch: 090 ----
mean loss: 176.88
train mean loss: 180.45
epoch train time: 0:00:17.868608
elapsed time: 1:06:24.009837
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-24 21:10:37.175206
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.28
 ---- batch: 020 ----
mean loss: 176.11
 ---- batch: 030 ----
mean loss: 179.74
 ---- batch: 040 ----
mean loss: 183.78
 ---- batch: 050 ----
mean loss: 185.29
 ---- batch: 060 ----
mean loss: 177.07
 ---- batch: 070 ----
mean loss: 179.21
 ---- batch: 080 ----
mean loss: 182.73
 ---- batch: 090 ----
mean loss: 181.68
train mean loss: 180.71
epoch train time: 0:00:17.915616
elapsed time: 1:06:41.926834
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-24 21:10:55.092247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.84
 ---- batch: 020 ----
mean loss: 180.32
 ---- batch: 030 ----
mean loss: 177.53
 ---- batch: 040 ----
mean loss: 181.36
 ---- batch: 050 ----
mean loss: 178.11
 ---- batch: 060 ----
mean loss: 188.81
 ---- batch: 070 ----
mean loss: 179.66
 ---- batch: 080 ----
mean loss: 185.05
 ---- batch: 090 ----
mean loss: 175.59
train mean loss: 180.58
epoch train time: 0:00:17.960291
elapsed time: 1:06:59.888413
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-24 21:11:13.053825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.88
 ---- batch: 020 ----
mean loss: 185.01
 ---- batch: 030 ----
mean loss: 179.48
 ---- batch: 040 ----
mean loss: 176.34
 ---- batch: 050 ----
mean loss: 176.35
 ---- batch: 060 ----
mean loss: 181.68
 ---- batch: 070 ----
mean loss: 180.54
 ---- batch: 080 ----
mean loss: 185.21
 ---- batch: 090 ----
mean loss: 182.62
train mean loss: 180.63
epoch train time: 0:00:17.918252
elapsed time: 1:07:17.808019
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-24 21:11:30.973367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.10
 ---- batch: 020 ----
mean loss: 180.25
 ---- batch: 030 ----
mean loss: 178.46
 ---- batch: 040 ----
mean loss: 180.13
 ---- batch: 050 ----
mean loss: 181.69
 ---- batch: 060 ----
mean loss: 182.37
 ---- batch: 070 ----
mean loss: 181.02
 ---- batch: 080 ----
mean loss: 174.26
 ---- batch: 090 ----
mean loss: 185.59
train mean loss: 180.46
epoch train time: 0:00:17.911492
elapsed time: 1:07:35.720912
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-24 21:11:48.886289
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.58
 ---- batch: 020 ----
mean loss: 176.49
 ---- batch: 030 ----
mean loss: 179.12
 ---- batch: 040 ----
mean loss: 178.26
 ---- batch: 050 ----
mean loss: 178.24
 ---- batch: 060 ----
mean loss: 176.26
 ---- batch: 070 ----
mean loss: 181.50
 ---- batch: 080 ----
mean loss: 194.57
 ---- batch: 090 ----
mean loss: 184.13
train mean loss: 180.99
epoch train time: 0:00:17.707296
elapsed time: 1:07:53.429450
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-24 21:12:06.594845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.39
 ---- batch: 020 ----
mean loss: 183.50
 ---- batch: 030 ----
mean loss: 178.22
 ---- batch: 040 ----
mean loss: 183.01
 ---- batch: 050 ----
mean loss: 180.51
 ---- batch: 060 ----
mean loss: 180.14
 ---- batch: 070 ----
mean loss: 176.32
 ---- batch: 080 ----
mean loss: 177.87
 ---- batch: 090 ----
mean loss: 181.47
train mean loss: 180.56
epoch train time: 0:00:17.621210
elapsed time: 1:08:11.051906
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-24 21:12:24.217278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.02
 ---- batch: 020 ----
mean loss: 186.14
 ---- batch: 030 ----
mean loss: 182.01
 ---- batch: 040 ----
mean loss: 176.89
 ---- batch: 050 ----
mean loss: 177.13
 ---- batch: 060 ----
mean loss: 180.72
 ---- batch: 070 ----
mean loss: 173.43
 ---- batch: 080 ----
mean loss: 183.30
 ---- batch: 090 ----
mean loss: 187.67
train mean loss: 180.54
epoch train time: 0:00:17.600182
elapsed time: 1:08:28.653370
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-24 21:12:41.818620
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.92
 ---- batch: 020 ----
mean loss: 182.35
 ---- batch: 030 ----
mean loss: 178.28
 ---- batch: 040 ----
mean loss: 185.03
 ---- batch: 050 ----
mean loss: 187.74
 ---- batch: 060 ----
mean loss: 178.78
 ---- batch: 070 ----
mean loss: 180.36
 ---- batch: 080 ----
mean loss: 178.45
 ---- batch: 090 ----
mean loss: 177.86
train mean loss: 180.51
epoch train time: 0:00:17.600390
elapsed time: 1:08:46.254894
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-24 21:12:59.420255
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.81
 ---- batch: 020 ----
mean loss: 173.39
 ---- batch: 030 ----
mean loss: 182.87
 ---- batch: 040 ----
mean loss: 181.93
 ---- batch: 050 ----
mean loss: 177.02
 ---- batch: 060 ----
mean loss: 181.24
 ---- batch: 070 ----
mean loss: 183.76
 ---- batch: 080 ----
mean loss: 177.31
 ---- batch: 090 ----
mean loss: 183.96
train mean loss: 180.40
epoch train time: 0:00:17.538273
elapsed time: 1:09:03.794430
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-24 21:13:16.959796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.24
 ---- batch: 020 ----
mean loss: 178.04
 ---- batch: 030 ----
mean loss: 181.03
 ---- batch: 040 ----
mean loss: 186.23
 ---- batch: 050 ----
mean loss: 186.24
 ---- batch: 060 ----
mean loss: 177.03
 ---- batch: 070 ----
mean loss: 180.55
 ---- batch: 080 ----
mean loss: 175.22
 ---- batch: 090 ----
mean loss: 183.68
train mean loss: 180.57
epoch train time: 0:00:17.520060
elapsed time: 1:09:21.315728
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-24 21:13:34.481166
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.79
 ---- batch: 020 ----
mean loss: 181.74
 ---- batch: 030 ----
mean loss: 182.15
 ---- batch: 040 ----
mean loss: 178.66
 ---- batch: 050 ----
mean loss: 178.25
 ---- batch: 060 ----
mean loss: 180.91
 ---- batch: 070 ----
mean loss: 181.27
 ---- batch: 080 ----
mean loss: 180.11
 ---- batch: 090 ----
mean loss: 179.82
train mean loss: 180.55
epoch train time: 0:00:17.512900
elapsed time: 1:09:38.829904
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-24 21:13:51.995294
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.78
 ---- batch: 020 ----
mean loss: 181.01
 ---- batch: 030 ----
mean loss: 180.25
 ---- batch: 040 ----
mean loss: 181.49
 ---- batch: 050 ----
mean loss: 177.67
 ---- batch: 060 ----
mean loss: 173.63
 ---- batch: 070 ----
mean loss: 177.95
 ---- batch: 080 ----
mean loss: 181.88
 ---- batch: 090 ----
mean loss: 184.48
train mean loss: 180.05
epoch train time: 0:00:17.525372
elapsed time: 1:09:56.356582
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-24 21:14:09.521962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.93
 ---- batch: 020 ----
mean loss: 178.65
 ---- batch: 030 ----
mean loss: 181.52
 ---- batch: 040 ----
mean loss: 176.15
 ---- batch: 050 ----
mean loss: 175.90
 ---- batch: 060 ----
mean loss: 183.05
 ---- batch: 070 ----
mean loss: 183.16
 ---- batch: 080 ----
mean loss: 178.58
 ---- batch: 090 ----
mean loss: 182.96
train mean loss: 180.18
epoch train time: 0:00:17.500058
elapsed time: 1:10:13.857855
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-24 21:14:27.023299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.94
 ---- batch: 020 ----
mean loss: 179.09
 ---- batch: 030 ----
mean loss: 177.79
 ---- batch: 040 ----
mean loss: 175.58
 ---- batch: 050 ----
mean loss: 182.98
 ---- batch: 060 ----
mean loss: 181.50
 ---- batch: 070 ----
mean loss: 173.74
 ---- batch: 080 ----
mean loss: 180.99
 ---- batch: 090 ----
mean loss: 181.62
train mean loss: 180.51
epoch train time: 0:00:17.494389
elapsed time: 1:10:31.354263
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-24 21:14:44.519130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.30
 ---- batch: 020 ----
mean loss: 183.35
 ---- batch: 030 ----
mean loss: 176.99
 ---- batch: 040 ----
mean loss: 184.15
 ---- batch: 050 ----
mean loss: 180.01
 ---- batch: 060 ----
mean loss: 180.32
 ---- batch: 070 ----
mean loss: 183.07
 ---- batch: 080 ----
mean loss: 180.26
 ---- batch: 090 ----
mean loss: 176.54
train mean loss: 179.86
epoch train time: 0:00:17.452861
elapsed time: 1:10:48.807802
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-24 21:15:01.973170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.27
 ---- batch: 020 ----
mean loss: 181.56
 ---- batch: 030 ----
mean loss: 175.48
 ---- batch: 040 ----
mean loss: 177.55
 ---- batch: 050 ----
mean loss: 184.05
 ---- batch: 060 ----
mean loss: 183.46
 ---- batch: 070 ----
mean loss: 179.64
 ---- batch: 080 ----
mean loss: 181.55
 ---- batch: 090 ----
mean loss: 179.08
train mean loss: 180.16
epoch train time: 0:00:17.525186
elapsed time: 1:11:06.334205
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-24 21:15:19.499569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.12
 ---- batch: 020 ----
mean loss: 180.14
 ---- batch: 030 ----
mean loss: 181.57
 ---- batch: 040 ----
mean loss: 184.14
 ---- batch: 050 ----
mean loss: 174.80
 ---- batch: 060 ----
mean loss: 174.48
 ---- batch: 070 ----
mean loss: 183.99
 ---- batch: 080 ----
mean loss: 181.86
 ---- batch: 090 ----
mean loss: 179.99
train mean loss: 179.84
epoch train time: 0:00:17.472039
elapsed time: 1:11:23.807487
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-24 21:15:36.972826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.61
 ---- batch: 020 ----
mean loss: 174.95
 ---- batch: 030 ----
mean loss: 189.29
 ---- batch: 040 ----
mean loss: 179.56
 ---- batch: 050 ----
mean loss: 178.67
 ---- batch: 060 ----
mean loss: 174.61
 ---- batch: 070 ----
mean loss: 180.38
 ---- batch: 080 ----
mean loss: 184.48
 ---- batch: 090 ----
mean loss: 181.47
train mean loss: 180.27
epoch train time: 0:00:17.471099
elapsed time: 1:11:41.279933
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-24 21:15:54.445475
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.64
 ---- batch: 020 ----
mean loss: 179.68
 ---- batch: 030 ----
mean loss: 181.11
 ---- batch: 040 ----
mean loss: 178.81
 ---- batch: 050 ----
mean loss: 182.81
 ---- batch: 060 ----
mean loss: 176.00
 ---- batch: 070 ----
mean loss: 180.86
 ---- batch: 080 ----
mean loss: 178.28
 ---- batch: 090 ----
mean loss: 180.57
train mean loss: 180.37
epoch train time: 0:00:17.462297
elapsed time: 1:11:58.743752
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-24 21:16:11.909109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.82
 ---- batch: 020 ----
mean loss: 183.16
 ---- batch: 030 ----
mean loss: 179.07
 ---- batch: 040 ----
mean loss: 174.30
 ---- batch: 050 ----
mean loss: 182.44
 ---- batch: 060 ----
mean loss: 180.72
 ---- batch: 070 ----
mean loss: 176.89
 ---- batch: 080 ----
mean loss: 178.13
 ---- batch: 090 ----
mean loss: 182.74
train mean loss: 180.09
epoch train time: 0:00:17.433143
elapsed time: 1:12:16.178192
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-24 21:16:29.343525
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.60
 ---- batch: 020 ----
mean loss: 184.62
 ---- batch: 030 ----
mean loss: 181.42
 ---- batch: 040 ----
mean loss: 172.26
 ---- batch: 050 ----
mean loss: 180.05
 ---- batch: 060 ----
mean loss: 177.66
 ---- batch: 070 ----
mean loss: 183.38
 ---- batch: 080 ----
mean loss: 180.58
 ---- batch: 090 ----
mean loss: 174.13
train mean loss: 180.00
epoch train time: 0:00:17.402553
elapsed time: 1:12:33.581921
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-24 21:16:46.747306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.42
 ---- batch: 020 ----
mean loss: 177.09
 ---- batch: 030 ----
mean loss: 181.04
 ---- batch: 040 ----
mean loss: 176.86
 ---- batch: 050 ----
mean loss: 179.89
 ---- batch: 060 ----
mean loss: 174.63
 ---- batch: 070 ----
mean loss: 181.53
 ---- batch: 080 ----
mean loss: 175.28
 ---- batch: 090 ----
mean loss: 185.16
train mean loss: 180.18
epoch train time: 0:00:17.414352
elapsed time: 1:12:50.997495
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-24 21:17:04.162823
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.54
 ---- batch: 020 ----
mean loss: 176.90
 ---- batch: 030 ----
mean loss: 183.29
 ---- batch: 040 ----
mean loss: 179.86
 ---- batch: 050 ----
mean loss: 174.89
 ---- batch: 060 ----
mean loss: 178.88
 ---- batch: 070 ----
mean loss: 176.86
 ---- batch: 080 ----
mean loss: 178.36
 ---- batch: 090 ----
mean loss: 185.70
train mean loss: 179.84
epoch train time: 0:00:17.361461
elapsed time: 1:13:08.360090
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-24 21:17:21.525470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.62
 ---- batch: 020 ----
mean loss: 180.61
 ---- batch: 030 ----
mean loss: 181.70
 ---- batch: 040 ----
mean loss: 176.66
 ---- batch: 050 ----
mean loss: 175.53
 ---- batch: 060 ----
mean loss: 182.76
 ---- batch: 070 ----
mean loss: 185.02
 ---- batch: 080 ----
mean loss: 173.62
 ---- batch: 090 ----
mean loss: 184.57
train mean loss: 180.27
epoch train time: 0:00:17.329454
elapsed time: 1:13:25.690860
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-24 21:17:38.856289
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.17
 ---- batch: 020 ----
mean loss: 189.31
 ---- batch: 030 ----
mean loss: 179.25
 ---- batch: 040 ----
mean loss: 171.82
 ---- batch: 050 ----
mean loss: 180.38
 ---- batch: 060 ----
mean loss: 178.70
 ---- batch: 070 ----
mean loss: 181.61
 ---- batch: 080 ----
mean loss: 176.04
 ---- batch: 090 ----
mean loss: 180.23
train mean loss: 180.04
epoch train time: 0:00:17.380042
elapsed time: 1:13:43.072339
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-24 21:17:56.237757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.63
 ---- batch: 020 ----
mean loss: 181.86
 ---- batch: 030 ----
mean loss: 175.62
 ---- batch: 040 ----
mean loss: 178.51
 ---- batch: 050 ----
mean loss: 185.59
 ---- batch: 060 ----
mean loss: 181.47
 ---- batch: 070 ----
mean loss: 177.28
 ---- batch: 080 ----
mean loss: 181.29
 ---- batch: 090 ----
mean loss: 183.70
train mean loss: 179.44
epoch train time: 0:00:17.370537
elapsed time: 1:14:00.444111
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-24 21:18:13.609420
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.41
 ---- batch: 020 ----
mean loss: 179.00
 ---- batch: 030 ----
mean loss: 176.27
 ---- batch: 040 ----
mean loss: 184.36
 ---- batch: 050 ----
mean loss: 177.07
 ---- batch: 060 ----
mean loss: 175.00
 ---- batch: 070 ----
mean loss: 179.63
 ---- batch: 080 ----
mean loss: 178.21
 ---- batch: 090 ----
mean loss: 181.35
train mean loss: 179.85
epoch train time: 0:00:17.287766
elapsed time: 1:14:17.733050
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-24 21:18:30.898421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.24
 ---- batch: 020 ----
mean loss: 178.10
 ---- batch: 030 ----
mean loss: 177.90
 ---- batch: 040 ----
mean loss: 184.70
 ---- batch: 050 ----
mean loss: 183.49
 ---- batch: 060 ----
mean loss: 184.11
 ---- batch: 070 ----
mean loss: 174.34
 ---- batch: 080 ----
mean loss: 172.91
 ---- batch: 090 ----
mean loss: 182.61
train mean loss: 179.42
epoch train time: 0:00:17.391244
elapsed time: 1:14:35.125476
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-24 21:18:48.290816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.49
 ---- batch: 020 ----
mean loss: 180.96
 ---- batch: 030 ----
mean loss: 175.24
 ---- batch: 040 ----
mean loss: 183.68
 ---- batch: 050 ----
mean loss: 174.86
 ---- batch: 060 ----
mean loss: 184.28
 ---- batch: 070 ----
mean loss: 191.61
 ---- batch: 080 ----
mean loss: 174.68
 ---- batch: 090 ----
mean loss: 177.36
train mean loss: 179.87
epoch train time: 0:00:17.339438
elapsed time: 1:14:52.466075
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-24 21:19:05.631422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.36
 ---- batch: 020 ----
mean loss: 180.88
 ---- batch: 030 ----
mean loss: 179.59
 ---- batch: 040 ----
mean loss: 171.56
 ---- batch: 050 ----
mean loss: 185.99
 ---- batch: 060 ----
mean loss: 181.94
 ---- batch: 070 ----
mean loss: 177.16
 ---- batch: 080 ----
mean loss: 181.89
 ---- batch: 090 ----
mean loss: 183.01
train mean loss: 179.46
epoch train time: 0:00:17.398337
elapsed time: 1:15:09.875448
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_1/checkpoint.pth.tar
**** end time: 2019-09-24 21:19:23.040263 ****
