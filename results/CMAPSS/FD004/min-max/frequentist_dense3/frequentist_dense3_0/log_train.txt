Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17776
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:03:54.112106 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:03:54.115529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4909.79
 ---- batch: 020 ----
mean loss: 4785.05
 ---- batch: 030 ----
mean loss: 4666.27
 ---- batch: 040 ----
mean loss: 4551.92
 ---- batch: 050 ----
mean loss: 4480.99
 ---- batch: 060 ----
mean loss: 4373.73
 ---- batch: 070 ----
mean loss: 4338.22
 ---- batch: 080 ----
mean loss: 4264.44
 ---- batch: 090 ----
mean loss: 4192.25
 ---- batch: 100 ----
mean loss: 4157.35
 ---- batch: 110 ----
mean loss: 4117.19
train mean loss: 4429.24
epoch train time: 0:00:32.123559
elapsed time: 0:00:32.129325
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:04:26.241473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3994.21
 ---- batch: 020 ----
mean loss: 3899.85
 ---- batch: 030 ----
mean loss: 3867.00
 ---- batch: 040 ----
mean loss: 3795.73
 ---- batch: 050 ----
mean loss: 3769.27
 ---- batch: 060 ----
mean loss: 3677.45
 ---- batch: 070 ----
mean loss: 3586.37
 ---- batch: 080 ----
mean loss: 3572.96
 ---- batch: 090 ----
mean loss: 3486.42
 ---- batch: 100 ----
mean loss: 3399.06
 ---- batch: 110 ----
mean loss: 3319.46
train mean loss: 3662.19
epoch train time: 0:00:00.563625
elapsed time: 0:00:32.693095
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:04:26.805253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3300.29
 ---- batch: 020 ----
mean loss: 3245.87
 ---- batch: 030 ----
mean loss: 3218.90
 ---- batch: 040 ----
mean loss: 3161.15
 ---- batch: 050 ----
mean loss: 3100.07
 ---- batch: 060 ----
mean loss: 3038.26
 ---- batch: 070 ----
mean loss: 3011.45
 ---- batch: 080 ----
mean loss: 2926.22
 ---- batch: 090 ----
mean loss: 2865.82
 ---- batch: 100 ----
mean loss: 2860.46
 ---- batch: 110 ----
mean loss: 2736.76
train mean loss: 3036.18
epoch train time: 0:00:00.559828
elapsed time: 0:00:33.253065
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:04:27.365239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2721.22
 ---- batch: 020 ----
mean loss: 2637.70
 ---- batch: 030 ----
mean loss: 2666.30
 ---- batch: 040 ----
mean loss: 2628.51
 ---- batch: 050 ----
mean loss: 2548.39
 ---- batch: 060 ----
mean loss: 2539.54
 ---- batch: 070 ----
mean loss: 2493.81
 ---- batch: 080 ----
mean loss: 2491.39
 ---- batch: 090 ----
mean loss: 2398.99
 ---- batch: 100 ----
mean loss: 2382.55
 ---- batch: 110 ----
mean loss: 2342.98
train mean loss: 2526.22
epoch train time: 0:00:00.568388
elapsed time: 0:00:33.821607
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:04:27.933761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2330.59
 ---- batch: 020 ----
mean loss: 2268.33
 ---- batch: 030 ----
mean loss: 2218.75
 ---- batch: 040 ----
mean loss: 2181.14
 ---- batch: 050 ----
mean loss: 2162.11
 ---- batch: 060 ----
mean loss: 2117.16
 ---- batch: 070 ----
mean loss: 2067.37
 ---- batch: 080 ----
mean loss: 2033.45
 ---- batch: 090 ----
mean loss: 2027.28
 ---- batch: 100 ----
mean loss: 2012.01
 ---- batch: 110 ----
mean loss: 1990.18
train mean loss: 2122.83
epoch train time: 0:00:00.546307
elapsed time: 0:00:34.368044
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:04:28.480214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1945.86
 ---- batch: 020 ----
mean loss: 1911.75
 ---- batch: 030 ----
mean loss: 1876.79
 ---- batch: 040 ----
mean loss: 1880.00
 ---- batch: 050 ----
mean loss: 1804.07
 ---- batch: 060 ----
mean loss: 1779.33
 ---- batch: 070 ----
mean loss: 1747.29
 ---- batch: 080 ----
mean loss: 1719.76
 ---- batch: 090 ----
mean loss: 1710.93
 ---- batch: 100 ----
mean loss: 1706.14
 ---- batch: 110 ----
mean loss: 1668.30
train mean loss: 1792.27
epoch train time: 0:00:00.578828
elapsed time: 0:00:34.947020
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:04:29.059186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1647.54
 ---- batch: 020 ----
mean loss: 1617.23
 ---- batch: 030 ----
mean loss: 1587.18
 ---- batch: 040 ----
mean loss: 1578.52
 ---- batch: 050 ----
mean loss: 1592.81
 ---- batch: 060 ----
mean loss: 1509.76
 ---- batch: 070 ----
mean loss: 1504.69
 ---- batch: 080 ----
mean loss: 1484.19
 ---- batch: 090 ----
mean loss: 1481.14
 ---- batch: 100 ----
mean loss: 1446.15
 ---- batch: 110 ----
mean loss: 1464.83
train mean loss: 1534.78
epoch train time: 0:00:00.550610
elapsed time: 0:00:35.497791
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:04:29.609942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1402.79
 ---- batch: 020 ----
mean loss: 1413.49
 ---- batch: 030 ----
mean loss: 1372.55
 ---- batch: 040 ----
mean loss: 1355.02
 ---- batch: 050 ----
mean loss: 1354.46
 ---- batch: 060 ----
mean loss: 1339.78
 ---- batch: 070 ----
mean loss: 1322.61
 ---- batch: 080 ----
mean loss: 1311.69
 ---- batch: 090 ----
mean loss: 1316.42
 ---- batch: 100 ----
mean loss: 1302.56
 ---- batch: 110 ----
mean loss: 1247.88
train mean loss: 1338.33
epoch train time: 0:00:00.557880
elapsed time: 0:00:36.055810
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:04:30.168000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1239.03
 ---- batch: 020 ----
mean loss: 1246.76
 ---- batch: 030 ----
mean loss: 1216.07
 ---- batch: 040 ----
mean loss: 1197.68
 ---- batch: 050 ----
mean loss: 1201.81
 ---- batch: 060 ----
mean loss: 1199.68
 ---- batch: 070 ----
mean loss: 1185.84
 ---- batch: 080 ----
mean loss: 1169.13
 ---- batch: 090 ----
mean loss: 1155.03
 ---- batch: 100 ----
mean loss: 1156.07
 ---- batch: 110 ----
mean loss: 1149.49
train mean loss: 1190.04
epoch train time: 0:00:00.555205
elapsed time: 0:00:36.611197
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:04:30.723354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1125.61
 ---- batch: 020 ----
mean loss: 1123.88
 ---- batch: 030 ----
mean loss: 1096.68
 ---- batch: 040 ----
mean loss: 1088.21
 ---- batch: 050 ----
mean loss: 1083.44
 ---- batch: 060 ----
mean loss: 1083.68
 ---- batch: 070 ----
mean loss: 1078.44
 ---- batch: 080 ----
mean loss: 1066.33
 ---- batch: 090 ----
mean loss: 1058.01
 ---- batch: 100 ----
mean loss: 1033.08
 ---- batch: 110 ----
mean loss: 1047.69
train mean loss: 1079.33
epoch train time: 0:00:00.552186
elapsed time: 0:00:37.163520
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:04:31.275673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1024.14
 ---- batch: 020 ----
mean loss: 1013.77
 ---- batch: 030 ----
mean loss: 1014.08
 ---- batch: 040 ----
mean loss: 1009.33
 ---- batch: 050 ----
mean loss: 998.50
 ---- batch: 060 ----
mean loss: 991.20
 ---- batch: 070 ----
mean loss: 974.34
 ---- batch: 080 ----
mean loss: 976.01
 ---- batch: 090 ----
mean loss: 979.31
 ---- batch: 100 ----
mean loss: 973.94
 ---- batch: 110 ----
mean loss: 956.10
train mean loss: 991.03
epoch train time: 0:00:00.569636
elapsed time: 0:00:37.733321
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:04:31.845509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.95
 ---- batch: 020 ----
mean loss: 944.73
 ---- batch: 030 ----
mean loss: 944.98
 ---- batch: 040 ----
mean loss: 934.20
 ---- batch: 050 ----
mean loss: 922.99
 ---- batch: 060 ----
mean loss: 928.54
 ---- batch: 070 ----
mean loss: 931.28
 ---- batch: 080 ----
mean loss: 920.50
 ---- batch: 090 ----
mean loss: 923.52
 ---- batch: 100 ----
mean loss: 917.06
 ---- batch: 110 ----
mean loss: 900.66
train mean loss: 930.25
epoch train time: 0:00:00.594292
elapsed time: 0:00:38.327805
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:04:32.439978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.47
 ---- batch: 020 ----
mean loss: 911.28
 ---- batch: 030 ----
mean loss: 904.70
 ---- batch: 040 ----
mean loss: 894.19
 ---- batch: 050 ----
mean loss: 888.20
 ---- batch: 060 ----
mean loss: 874.20
 ---- batch: 070 ----
mean loss: 898.15
 ---- batch: 080 ----
mean loss: 871.11
 ---- batch: 090 ----
mean loss: 879.85
 ---- batch: 100 ----
mean loss: 891.29
 ---- batch: 110 ----
mean loss: 865.99
train mean loss: 890.41
epoch train time: 0:00:00.572150
elapsed time: 0:00:38.900124
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:04:33.012279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.90
 ---- batch: 020 ----
mean loss: 869.98
 ---- batch: 030 ----
mean loss: 868.74
 ---- batch: 040 ----
mean loss: 860.52
 ---- batch: 050 ----
mean loss: 863.37
 ---- batch: 060 ----
mean loss: 876.31
 ---- batch: 070 ----
mean loss: 868.53
 ---- batch: 080 ----
mean loss: 870.82
 ---- batch: 090 ----
mean loss: 858.61
 ---- batch: 100 ----
mean loss: 868.36
 ---- batch: 110 ----
mean loss: 867.97
train mean loss: 867.55
epoch train time: 0:00:00.567417
elapsed time: 0:00:39.467675
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:04:33.579860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.48
 ---- batch: 020 ----
mean loss: 855.49
 ---- batch: 030 ----
mean loss: 872.01
 ---- batch: 040 ----
mean loss: 869.35
 ---- batch: 050 ----
mean loss: 856.60
 ---- batch: 060 ----
mean loss: 847.92
 ---- batch: 070 ----
mean loss: 848.25
 ---- batch: 080 ----
mean loss: 843.74
 ---- batch: 090 ----
mean loss: 852.67
 ---- batch: 100 ----
mean loss: 845.81
 ---- batch: 110 ----
mean loss: 866.68
train mean loss: 855.52
epoch train time: 0:00:00.564658
elapsed time: 0:00:40.032496
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:04:34.144649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.39
 ---- batch: 020 ----
mean loss: 857.07
 ---- batch: 030 ----
mean loss: 847.52
 ---- batch: 040 ----
mean loss: 835.52
 ---- batch: 050 ----
mean loss: 846.24
 ---- batch: 060 ----
mean loss: 853.48
 ---- batch: 070 ----
mean loss: 863.07
 ---- batch: 080 ----
mean loss: 837.88
 ---- batch: 090 ----
mean loss: 842.28
 ---- batch: 100 ----
mean loss: 857.25
 ---- batch: 110 ----
mean loss: 836.47
train mean loss: 849.51
epoch train time: 0:00:00.571017
elapsed time: 0:00:40.603649
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:04:34.715803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.90
 ---- batch: 020 ----
mean loss: 819.19
 ---- batch: 030 ----
mean loss: 845.27
 ---- batch: 040 ----
mean loss: 865.10
 ---- batch: 050 ----
mean loss: 866.10
 ---- batch: 060 ----
mean loss: 861.05
 ---- batch: 070 ----
mean loss: 856.13
 ---- batch: 080 ----
mean loss: 846.88
 ---- batch: 090 ----
mean loss: 832.57
 ---- batch: 100 ----
mean loss: 838.43
 ---- batch: 110 ----
mean loss: 838.60
train mean loss: 846.53
epoch train time: 0:00:00.578394
elapsed time: 0:00:41.182183
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:04:35.294352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.77
 ---- batch: 020 ----
mean loss: 849.22
 ---- batch: 030 ----
mean loss: 832.91
 ---- batch: 040 ----
mean loss: 846.15
 ---- batch: 050 ----
mean loss: 860.85
 ---- batch: 060 ----
mean loss: 824.54
 ---- batch: 070 ----
mean loss: 859.13
 ---- batch: 080 ----
mean loss: 841.21
 ---- batch: 090 ----
mean loss: 841.63
 ---- batch: 100 ----
mean loss: 860.43
 ---- batch: 110 ----
mean loss: 856.44
train mean loss: 844.96
epoch train time: 0:00:00.575994
elapsed time: 0:00:41.758332
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:04:35.870506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.58
 ---- batch: 020 ----
mean loss: 855.77
 ---- batch: 030 ----
mean loss: 843.09
 ---- batch: 040 ----
mean loss: 822.24
 ---- batch: 050 ----
mean loss: 835.31
 ---- batch: 060 ----
mean loss: 852.01
 ---- batch: 070 ----
mean loss: 841.35
 ---- batch: 080 ----
mean loss: 841.65
 ---- batch: 090 ----
mean loss: 852.47
 ---- batch: 100 ----
mean loss: 840.15
 ---- batch: 110 ----
mean loss: 862.99
train mean loss: 844.25
epoch train time: 0:00:00.586455
elapsed time: 0:00:42.344968
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:04:36.457121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.85
 ---- batch: 020 ----
mean loss: 852.75
 ---- batch: 030 ----
mean loss: 854.69
 ---- batch: 040 ----
mean loss: 838.49
 ---- batch: 050 ----
mean loss: 828.25
 ---- batch: 060 ----
mean loss: 851.59
 ---- batch: 070 ----
mean loss: 842.92
 ---- batch: 080 ----
mean loss: 848.97
 ---- batch: 090 ----
mean loss: 837.84
 ---- batch: 100 ----
mean loss: 845.93
 ---- batch: 110 ----
mean loss: 851.10
train mean loss: 844.06
epoch train time: 0:00:00.579630
elapsed time: 0:00:42.924739
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:04:37.036894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.61
 ---- batch: 020 ----
mean loss: 881.09
 ---- batch: 030 ----
mean loss: 843.22
 ---- batch: 040 ----
mean loss: 869.89
 ---- batch: 050 ----
mean loss: 847.83
 ---- batch: 060 ----
mean loss: 859.64
 ---- batch: 070 ----
mean loss: 840.66
 ---- batch: 080 ----
mean loss: 857.12
 ---- batch: 090 ----
mean loss: 829.91
 ---- batch: 100 ----
mean loss: 825.98
 ---- batch: 110 ----
mean loss: 827.58
train mean loss: 844.01
epoch train time: 0:00:00.556189
elapsed time: 0:00:43.481063
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:04:37.593215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.56
 ---- batch: 020 ----
mean loss: 856.30
 ---- batch: 030 ----
mean loss: 828.50
 ---- batch: 040 ----
mean loss: 859.07
 ---- batch: 050 ----
mean loss: 850.48
 ---- batch: 060 ----
mean loss: 841.68
 ---- batch: 070 ----
mean loss: 853.72
 ---- batch: 080 ----
mean loss: 837.50
 ---- batch: 090 ----
mean loss: 835.51
 ---- batch: 100 ----
mean loss: 831.81
 ---- batch: 110 ----
mean loss: 853.07
train mean loss: 843.98
epoch train time: 0:00:00.555376
elapsed time: 0:00:44.036623
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:04:38.148779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.43
 ---- batch: 020 ----
mean loss: 834.51
 ---- batch: 030 ----
mean loss: 818.87
 ---- batch: 040 ----
mean loss: 844.22
 ---- batch: 050 ----
mean loss: 871.53
 ---- batch: 060 ----
mean loss: 835.58
 ---- batch: 070 ----
mean loss: 864.81
 ---- batch: 080 ----
mean loss: 832.25
 ---- batch: 090 ----
mean loss: 846.16
 ---- batch: 100 ----
mean loss: 862.09
 ---- batch: 110 ----
mean loss: 844.15
train mean loss: 844.02
epoch train time: 0:00:00.547335
elapsed time: 0:00:44.584104
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:04:38.696259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.35
 ---- batch: 020 ----
mean loss: 841.59
 ---- batch: 030 ----
mean loss: 856.93
 ---- batch: 040 ----
mean loss: 844.29
 ---- batch: 050 ----
mean loss: 860.71
 ---- batch: 060 ----
mean loss: 834.28
 ---- batch: 070 ----
mean loss: 833.48
 ---- batch: 080 ----
mean loss: 858.26
 ---- batch: 090 ----
mean loss: 841.81
 ---- batch: 100 ----
mean loss: 851.81
 ---- batch: 110 ----
mean loss: 829.20
train mean loss: 843.99
epoch train time: 0:00:00.551464
elapsed time: 0:00:45.135706
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:04:39.247861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.90
 ---- batch: 020 ----
mean loss: 842.32
 ---- batch: 030 ----
mean loss: 827.28
 ---- batch: 040 ----
mean loss: 849.85
 ---- batch: 050 ----
mean loss: 846.89
 ---- batch: 060 ----
mean loss: 858.70
 ---- batch: 070 ----
mean loss: 818.87
 ---- batch: 080 ----
mean loss: 846.68
 ---- batch: 090 ----
mean loss: 854.56
 ---- batch: 100 ----
mean loss: 835.46
 ---- batch: 110 ----
mean loss: 854.72
train mean loss: 844.00
epoch train time: 0:00:00.551497
elapsed time: 0:00:45.687345
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:04:39.799517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.75
 ---- batch: 020 ----
mean loss: 845.49
 ---- batch: 030 ----
mean loss: 843.53
 ---- batch: 040 ----
mean loss: 840.50
 ---- batch: 050 ----
mean loss: 834.65
 ---- batch: 060 ----
mean loss: 852.36
 ---- batch: 070 ----
mean loss: 864.91
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.37
 ---- batch: 100 ----
mean loss: 838.74
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.96
epoch train time: 0:00:00.547438
elapsed time: 0:00:46.234959
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:04:40.347108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.07
 ---- batch: 020 ----
mean loss: 852.37
 ---- batch: 030 ----
mean loss: 858.97
 ---- batch: 040 ----
mean loss: 845.35
 ---- batch: 050 ----
mean loss: 839.65
 ---- batch: 060 ----
mean loss: 819.38
 ---- batch: 070 ----
mean loss: 828.47
 ---- batch: 080 ----
mean loss: 860.90
 ---- batch: 090 ----
mean loss: 863.14
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.562575
elapsed time: 0:00:46.797669
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:04:40.909824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.65
 ---- batch: 030 ----
mean loss: 854.14
 ---- batch: 040 ----
mean loss: 865.58
 ---- batch: 050 ----
mean loss: 838.80
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.95
 ---- batch: 080 ----
mean loss: 849.24
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.78
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.85
epoch train time: 0:00:00.549915
elapsed time: 0:00:47.347718
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:04:41.459870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.59
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.25
 ---- batch: 040 ----
mean loss: 861.75
 ---- batch: 050 ----
mean loss: 858.50
 ---- batch: 060 ----
mean loss: 838.93
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.46
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.553446
elapsed time: 0:00:47.901315
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:04:42.013498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.47
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.57
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.556206
elapsed time: 0:00:48.457686
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:04:42.569838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.44
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.62
 ---- batch: 090 ----
mean loss: 844.56
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.560545
elapsed time: 0:00:49.018367
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:04:43.130554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.90
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.11
 ---- batch: 050 ----
mean loss: 818.04
 ---- batch: 060 ----
mean loss: 833.13
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 843.00
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.554400
elapsed time: 0:00:49.572952
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:04:43.685106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.56
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.01
train mean loss: 843.87
epoch train time: 0:00:00.550108
elapsed time: 0:00:50.123201
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:04:44.235356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.59
 ---- batch: 020 ----
mean loss: 848.84
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.43
 ---- batch: 080 ----
mean loss: 858.19
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.550785
elapsed time: 0:00:50.674137
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:04:44.786289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.72
 ---- batch: 030 ----
mean loss: 838.89
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.73
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.555917
elapsed time: 0:00:51.230187
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:04:45.342340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.62
 ---- batch: 030 ----
mean loss: 838.12
 ---- batch: 040 ----
mean loss: 845.02
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.19
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.555308
elapsed time: 0:00:51.785640
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:04:45.897800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.13
 ---- batch: 090 ----
mean loss: 864.27
 ---- batch: 100 ----
mean loss: 852.66
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.562335
elapsed time: 0:00:52.348121
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:04:46.460284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.71
 ---- batch: 030 ----
mean loss: 830.99
 ---- batch: 040 ----
mean loss: 838.39
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.25
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.75
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.08
 ---- batch: 110 ----
mean loss: 855.25
train mean loss: 843.95
epoch train time: 0:00:00.558020
elapsed time: 0:00:52.906284
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:04:47.018476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.39
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.84
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.03
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.23
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.554595
elapsed time: 0:00:53.461058
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:04:47.573207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.51
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.67
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.95
train mean loss: 844.01
epoch train time: 0:00:00.550077
elapsed time: 0:00:54.011261
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:04:48.123412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.23
 ---- batch: 030 ----
mean loss: 833.33
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.66
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.546457
elapsed time: 0:00:54.557863
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:04:48.670014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.50
 ---- batch: 020 ----
mean loss: 825.59
 ---- batch: 030 ----
mean loss: 839.15
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.49
 ---- batch: 090 ----
mean loss: 853.98
 ---- batch: 100 ----
mean loss: 844.04
 ---- batch: 110 ----
mean loss: 822.73
train mean loss: 843.97
epoch train time: 0:00:00.549593
elapsed time: 0:00:55.107583
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:04:49.219733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.68
 ---- batch: 030 ----
mean loss: 846.89
 ---- batch: 040 ----
mean loss: 835.59
 ---- batch: 050 ----
mean loss: 868.31
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 842.00
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.546749
elapsed time: 0:00:55.654467
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:04:49.766621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.80
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.95
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.17
 ---- batch: 080 ----
mean loss: 856.45
 ---- batch: 090 ----
mean loss: 845.24
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.566338
elapsed time: 0:00:56.220937
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:04:50.333107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.81
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.11
 ---- batch: 050 ----
mean loss: 833.75
 ---- batch: 060 ----
mean loss: 865.06
 ---- batch: 070 ----
mean loss: 852.63
 ---- batch: 080 ----
mean loss: 833.14
 ---- batch: 090 ----
mean loss: 822.31
 ---- batch: 100 ----
mean loss: 830.73
 ---- batch: 110 ----
mean loss: 843.90
train mean loss: 844.04
epoch train time: 0:00:00.559553
elapsed time: 0:00:56.780637
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:04:50.892804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.98
 ---- batch: 020 ----
mean loss: 869.92
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.32
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.35
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.550708
elapsed time: 0:00:57.331495
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:04:51.443652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.24
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.09
 ---- batch: 080 ----
mean loss: 821.70
 ---- batch: 090 ----
mean loss: 868.28
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.557440
elapsed time: 0:00:57.889075
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:04:52.001227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.58
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.88
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.560069
elapsed time: 0:00:58.449277
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:04:52.561438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.80
 ---- batch: 030 ----
mean loss: 851.05
 ---- batch: 040 ----
mean loss: 856.67
 ---- batch: 050 ----
mean loss: 832.85
 ---- batch: 060 ----
mean loss: 851.56
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.22
 ---- batch: 090 ----
mean loss: 833.16
 ---- batch: 100 ----
mean loss: 851.11
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.03
epoch train time: 0:00:00.578564
elapsed time: 0:00:59.027989
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:04:53.140149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.59
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.98
 ---- batch: 060 ----
mean loss: 847.40
 ---- batch: 070 ----
mean loss: 828.85
 ---- batch: 080 ----
mean loss: 845.63
 ---- batch: 090 ----
mean loss: 847.92
 ---- batch: 100 ----
mean loss: 839.45
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.548271
elapsed time: 0:00:59.576400
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:04:53.688572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.51
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.97
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.37
 ---- batch: 070 ----
mean loss: 860.78
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.01
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.563464
elapsed time: 0:01:00.140018
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:04:54.252189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.82
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.15
 ---- batch: 050 ----
mean loss: 844.73
 ---- batch: 060 ----
mean loss: 863.77
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.25
 ---- batch: 090 ----
mean loss: 833.56
 ---- batch: 100 ----
mean loss: 859.02
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.550778
elapsed time: 0:01:00.690951
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:04:54.803104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.94
 ---- batch: 020 ----
mean loss: 835.17
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.17
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.76
 ---- batch: 070 ----
mean loss: 822.81
 ---- batch: 080 ----
mean loss: 857.45
 ---- batch: 090 ----
mean loss: 835.61
 ---- batch: 100 ----
mean loss: 851.21
 ---- batch: 110 ----
mean loss: 863.64
train mean loss: 844.00
epoch train time: 0:00:00.550189
elapsed time: 0:01:01.241269
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:04:55.353421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.58
 ---- batch: 020 ----
mean loss: 858.18
 ---- batch: 030 ----
mean loss: 852.87
 ---- batch: 040 ----
mean loss: 839.77
 ---- batch: 050 ----
mean loss: 830.20
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.69
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.72
 ---- batch: 110 ----
mean loss: 842.08
train mean loss: 843.93
epoch train time: 0:00:00.563624
elapsed time: 0:01:01.805025
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:04:55.917181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.39
 ---- batch: 020 ----
mean loss: 853.11
 ---- batch: 030 ----
mean loss: 847.67
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 858.99
 ---- batch: 060 ----
mean loss: 816.13
 ---- batch: 070 ----
mean loss: 859.41
 ---- batch: 080 ----
mean loss: 811.08
 ---- batch: 090 ----
mean loss: 824.42
 ---- batch: 100 ----
mean loss: 811.88
 ---- batch: 110 ----
mean loss: 804.09
train mean loss: 833.57
epoch train time: 0:00:00.549432
elapsed time: 0:01:02.354597
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:04:56.466750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.46
 ---- batch: 020 ----
mean loss: 751.14
 ---- batch: 030 ----
mean loss: 740.18
 ---- batch: 040 ----
mean loss: 722.64
 ---- batch: 050 ----
mean loss: 692.25
 ---- batch: 060 ----
mean loss: 645.87
 ---- batch: 070 ----
mean loss: 582.43
 ---- batch: 080 ----
mean loss: 497.26
 ---- batch: 090 ----
mean loss: 450.95
 ---- batch: 100 ----
mean loss: 408.05
 ---- batch: 110 ----
mean loss: 392.30
train mean loss: 600.05
epoch train time: 0:00:00.552875
elapsed time: 0:01:02.907624
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:04:57.019777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.41
 ---- batch: 020 ----
mean loss: 365.60
 ---- batch: 030 ----
mean loss: 367.55
 ---- batch: 040 ----
mean loss: 346.12
 ---- batch: 050 ----
mean loss: 346.07
 ---- batch: 060 ----
mean loss: 333.59
 ---- batch: 070 ----
mean loss: 317.81
 ---- batch: 080 ----
mean loss: 304.03
 ---- batch: 090 ----
mean loss: 311.90
 ---- batch: 100 ----
mean loss: 301.68
 ---- batch: 110 ----
mean loss: 301.22
train mean loss: 332.95
epoch train time: 0:00:00.554052
elapsed time: 0:01:03.461818
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:04:57.573992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.40
 ---- batch: 020 ----
mean loss: 281.51
 ---- batch: 030 ----
mean loss: 278.01
 ---- batch: 040 ----
mean loss: 270.26
 ---- batch: 050 ----
mean loss: 279.00
 ---- batch: 060 ----
mean loss: 272.20
 ---- batch: 070 ----
mean loss: 276.58
 ---- batch: 080 ----
mean loss: 268.24
 ---- batch: 090 ----
mean loss: 266.00
 ---- batch: 100 ----
mean loss: 261.62
 ---- batch: 110 ----
mean loss: 269.35
train mean loss: 274.23
epoch train time: 0:00:00.554606
elapsed time: 0:01:04.016576
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:04:58.128731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.24
 ---- batch: 020 ----
mean loss: 263.97
 ---- batch: 030 ----
mean loss: 256.42
 ---- batch: 040 ----
mean loss: 250.65
 ---- batch: 050 ----
mean loss: 243.73
 ---- batch: 060 ----
mean loss: 240.78
 ---- batch: 070 ----
mean loss: 241.94
 ---- batch: 080 ----
mean loss: 256.86
 ---- batch: 090 ----
mean loss: 243.88
 ---- batch: 100 ----
mean loss: 255.22
 ---- batch: 110 ----
mean loss: 240.95
train mean loss: 249.10
epoch train time: 0:00:00.550220
elapsed time: 0:01:04.566943
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:04:58.679109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.57
 ---- batch: 020 ----
mean loss: 233.97
 ---- batch: 030 ----
mean loss: 239.31
 ---- batch: 040 ----
mean loss: 240.94
 ---- batch: 050 ----
mean loss: 231.21
 ---- batch: 060 ----
mean loss: 232.54
 ---- batch: 070 ----
mean loss: 225.66
 ---- batch: 080 ----
mean loss: 236.11
 ---- batch: 090 ----
mean loss: 243.57
 ---- batch: 100 ----
mean loss: 241.35
 ---- batch: 110 ----
mean loss: 237.30
train mean loss: 236.03
epoch train time: 0:00:00.546605
elapsed time: 0:01:05.113693
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:04:59.225845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.23
 ---- batch: 020 ----
mean loss: 222.14
 ---- batch: 030 ----
mean loss: 235.86
 ---- batch: 040 ----
mean loss: 230.72
 ---- batch: 050 ----
mean loss: 229.81
 ---- batch: 060 ----
mean loss: 229.85
 ---- batch: 070 ----
mean loss: 224.80
 ---- batch: 080 ----
mean loss: 230.70
 ---- batch: 090 ----
mean loss: 230.90
 ---- batch: 100 ----
mean loss: 223.05
 ---- batch: 110 ----
mean loss: 219.04
train mean loss: 227.48
epoch train time: 0:00:00.545935
elapsed time: 0:01:05.659756
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:04:59.771905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.19
 ---- batch: 020 ----
mean loss: 228.75
 ---- batch: 030 ----
mean loss: 220.67
 ---- batch: 040 ----
mean loss: 210.56
 ---- batch: 050 ----
mean loss: 223.87
 ---- batch: 060 ----
mean loss: 218.51
 ---- batch: 070 ----
mean loss: 225.10
 ---- batch: 080 ----
mean loss: 211.97
 ---- batch: 090 ----
mean loss: 225.67
 ---- batch: 100 ----
mean loss: 211.46
 ---- batch: 110 ----
mean loss: 222.74
train mean loss: 219.29
epoch train time: 0:00:00.544359
elapsed time: 0:01:06.204242
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:05:00.316394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.44
 ---- batch: 020 ----
mean loss: 203.79
 ---- batch: 030 ----
mean loss: 213.26
 ---- batch: 040 ----
mean loss: 218.72
 ---- batch: 050 ----
mean loss: 202.09
 ---- batch: 060 ----
mean loss: 211.56
 ---- batch: 070 ----
mean loss: 198.87
 ---- batch: 080 ----
mean loss: 224.04
 ---- batch: 090 ----
mean loss: 209.26
 ---- batch: 100 ----
mean loss: 212.41
 ---- batch: 110 ----
mean loss: 210.57
train mean loss: 211.53
epoch train time: 0:00:00.546159
elapsed time: 0:01:06.750533
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:05:00.862690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.87
 ---- batch: 020 ----
mean loss: 201.57
 ---- batch: 030 ----
mean loss: 202.61
 ---- batch: 040 ----
mean loss: 209.60
 ---- batch: 050 ----
mean loss: 203.42
 ---- batch: 060 ----
mean loss: 203.60
 ---- batch: 070 ----
mean loss: 212.18
 ---- batch: 080 ----
mean loss: 204.01
 ---- batch: 090 ----
mean loss: 197.02
 ---- batch: 100 ----
mean loss: 204.31
 ---- batch: 110 ----
mean loss: 215.19
train mean loss: 205.20
epoch train time: 0:00:00.547141
elapsed time: 0:01:07.297810
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:05:01.409981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.38
 ---- batch: 020 ----
mean loss: 203.73
 ---- batch: 030 ----
mean loss: 195.56
 ---- batch: 040 ----
mean loss: 185.37
 ---- batch: 050 ----
mean loss: 202.96
 ---- batch: 060 ----
mean loss: 204.36
 ---- batch: 070 ----
mean loss: 206.20
 ---- batch: 080 ----
mean loss: 205.42
 ---- batch: 090 ----
mean loss: 211.54
 ---- batch: 100 ----
mean loss: 199.99
 ---- batch: 110 ----
mean loss: 194.83
train mean loss: 200.99
epoch train time: 0:00:00.547841
elapsed time: 0:01:07.845801
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:05:01.957985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.44
 ---- batch: 020 ----
mean loss: 190.11
 ---- batch: 030 ----
mean loss: 202.67
 ---- batch: 040 ----
mean loss: 202.83
 ---- batch: 050 ----
mean loss: 205.06
 ---- batch: 060 ----
mean loss: 199.67
 ---- batch: 070 ----
mean loss: 205.83
 ---- batch: 080 ----
mean loss: 197.19
 ---- batch: 090 ----
mean loss: 190.00
 ---- batch: 100 ----
mean loss: 194.27
 ---- batch: 110 ----
mean loss: 200.17
train mean loss: 199.04
epoch train time: 0:00:00.551253
elapsed time: 0:01:08.397216
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:05:02.509367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.00
 ---- batch: 020 ----
mean loss: 190.29
 ---- batch: 030 ----
mean loss: 191.05
 ---- batch: 040 ----
mean loss: 192.61
 ---- batch: 050 ----
mean loss: 185.08
 ---- batch: 060 ----
mean loss: 198.69
 ---- batch: 070 ----
mean loss: 193.95
 ---- batch: 080 ----
mean loss: 201.04
 ---- batch: 090 ----
mean loss: 192.58
 ---- batch: 100 ----
mean loss: 195.74
 ---- batch: 110 ----
mean loss: 196.57
train mean loss: 193.76
epoch train time: 0:00:00.551348
elapsed time: 0:01:08.948694
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:05:03.060840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.59
 ---- batch: 020 ----
mean loss: 188.80
 ---- batch: 030 ----
mean loss: 178.55
 ---- batch: 040 ----
mean loss: 192.54
 ---- batch: 050 ----
mean loss: 192.79
 ---- batch: 060 ----
mean loss: 186.22
 ---- batch: 070 ----
mean loss: 195.90
 ---- batch: 080 ----
mean loss: 193.55
 ---- batch: 090 ----
mean loss: 202.10
 ---- batch: 100 ----
mean loss: 193.55
 ---- batch: 110 ----
mean loss: 195.99
train mean loss: 192.07
epoch train time: 0:00:00.548498
elapsed time: 0:01:09.497319
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:05:03.609473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.16
 ---- batch: 020 ----
mean loss: 190.17
 ---- batch: 030 ----
mean loss: 180.94
 ---- batch: 040 ----
mean loss: 187.27
 ---- batch: 050 ----
mean loss: 192.77
 ---- batch: 060 ----
mean loss: 195.77
 ---- batch: 070 ----
mean loss: 195.63
 ---- batch: 080 ----
mean loss: 185.21
 ---- batch: 090 ----
mean loss: 196.39
 ---- batch: 100 ----
mean loss: 180.87
 ---- batch: 110 ----
mean loss: 195.89
train mean loss: 189.88
epoch train time: 0:00:00.549511
elapsed time: 0:01:10.046969
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:05:04.159125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.16
 ---- batch: 020 ----
mean loss: 191.86
 ---- batch: 030 ----
mean loss: 190.89
 ---- batch: 040 ----
mean loss: 180.86
 ---- batch: 050 ----
mean loss: 190.88
 ---- batch: 060 ----
mean loss: 191.19
 ---- batch: 070 ----
mean loss: 184.96
 ---- batch: 080 ----
mean loss: 184.55
 ---- batch: 090 ----
mean loss: 184.31
 ---- batch: 100 ----
mean loss: 184.39
 ---- batch: 110 ----
mean loss: 192.37
train mean loss: 188.12
epoch train time: 0:00:00.554287
elapsed time: 0:01:10.601390
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:05:04.713542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.48
 ---- batch: 020 ----
mean loss: 187.59
 ---- batch: 030 ----
mean loss: 195.95
 ---- batch: 040 ----
mean loss: 183.73
 ---- batch: 050 ----
mean loss: 183.55
 ---- batch: 060 ----
mean loss: 181.17
 ---- batch: 070 ----
mean loss: 189.63
 ---- batch: 080 ----
mean loss: 186.01
 ---- batch: 090 ----
mean loss: 190.76
 ---- batch: 100 ----
mean loss: 185.10
 ---- batch: 110 ----
mean loss: 187.05
train mean loss: 186.30
epoch train time: 0:00:00.548189
elapsed time: 0:01:11.149727
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:05:05.261900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.43
 ---- batch: 020 ----
mean loss: 176.21
 ---- batch: 030 ----
mean loss: 188.91
 ---- batch: 040 ----
mean loss: 187.04
 ---- batch: 050 ----
mean loss: 174.07
 ---- batch: 060 ----
mean loss: 187.16
 ---- batch: 070 ----
mean loss: 191.64
 ---- batch: 080 ----
mean loss: 195.27
 ---- batch: 090 ----
mean loss: 182.59
 ---- batch: 100 ----
mean loss: 183.34
 ---- batch: 110 ----
mean loss: 185.93
train mean loss: 185.01
epoch train time: 0:00:00.546172
elapsed time: 0:01:11.696049
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:05:05.808204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.24
 ---- batch: 020 ----
mean loss: 179.53
 ---- batch: 030 ----
mean loss: 184.13
 ---- batch: 040 ----
mean loss: 184.36
 ---- batch: 050 ----
mean loss: 187.79
 ---- batch: 060 ----
mean loss: 181.36
 ---- batch: 070 ----
mean loss: 184.65
 ---- batch: 080 ----
mean loss: 181.41
 ---- batch: 090 ----
mean loss: 180.95
 ---- batch: 100 ----
mean loss: 189.71
 ---- batch: 110 ----
mean loss: 181.27
train mean loss: 183.27
epoch train time: 0:00:00.542822
elapsed time: 0:01:12.239055
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:05:06.351207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.48
 ---- batch: 020 ----
mean loss: 183.42
 ---- batch: 030 ----
mean loss: 180.78
 ---- batch: 040 ----
mean loss: 177.96
 ---- batch: 050 ----
mean loss: 178.09
 ---- batch: 060 ----
mean loss: 189.78
 ---- batch: 070 ----
mean loss: 180.01
 ---- batch: 080 ----
mean loss: 189.07
 ---- batch: 090 ----
mean loss: 184.99
 ---- batch: 100 ----
mean loss: 185.77
 ---- batch: 110 ----
mean loss: 178.82
train mean loss: 182.78
epoch train time: 0:00:00.548652
elapsed time: 0:01:12.787839
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:05:06.899991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.03
 ---- batch: 020 ----
mean loss: 180.72
 ---- batch: 030 ----
mean loss: 177.89
 ---- batch: 040 ----
mean loss: 176.35
 ---- batch: 050 ----
mean loss: 179.97
 ---- batch: 060 ----
mean loss: 180.91
 ---- batch: 070 ----
mean loss: 185.96
 ---- batch: 080 ----
mean loss: 185.52
 ---- batch: 090 ----
mean loss: 178.80
 ---- batch: 100 ----
mean loss: 180.25
 ---- batch: 110 ----
mean loss: 184.37
train mean loss: 181.67
epoch train time: 0:00:00.539200
elapsed time: 0:01:13.327200
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:05:07.439352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.34
 ---- batch: 020 ----
mean loss: 176.20
 ---- batch: 030 ----
mean loss: 187.44
 ---- batch: 040 ----
mean loss: 182.28
 ---- batch: 050 ----
mean loss: 187.91
 ---- batch: 060 ----
mean loss: 175.10
 ---- batch: 070 ----
mean loss: 179.03
 ---- batch: 080 ----
mean loss: 183.22
 ---- batch: 090 ----
mean loss: 182.36
 ---- batch: 100 ----
mean loss: 176.13
 ---- batch: 110 ----
mean loss: 186.02
train mean loss: 180.99
epoch train time: 0:00:00.564853
elapsed time: 0:01:13.892186
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:05:08.004340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.98
 ---- batch: 020 ----
mean loss: 172.49
 ---- batch: 030 ----
mean loss: 181.43
 ---- batch: 040 ----
mean loss: 181.23
 ---- batch: 050 ----
mean loss: 183.83
 ---- batch: 060 ----
mean loss: 184.52
 ---- batch: 070 ----
mean loss: 186.45
 ---- batch: 080 ----
mean loss: 178.96
 ---- batch: 090 ----
mean loss: 181.55
 ---- batch: 100 ----
mean loss: 167.95
 ---- batch: 110 ----
mean loss: 183.29
train mean loss: 180.26
epoch train time: 0:00:00.551942
elapsed time: 0:01:14.444281
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:05:08.556473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.17
 ---- batch: 020 ----
mean loss: 182.32
 ---- batch: 030 ----
mean loss: 171.41
 ---- batch: 040 ----
mean loss: 176.83
 ---- batch: 050 ----
mean loss: 182.99
 ---- batch: 060 ----
mean loss: 178.43
 ---- batch: 070 ----
mean loss: 178.03
 ---- batch: 080 ----
mean loss: 179.43
 ---- batch: 090 ----
mean loss: 182.26
 ---- batch: 100 ----
mean loss: 182.38
 ---- batch: 110 ----
mean loss: 173.10
train mean loss: 178.81
epoch train time: 0:00:00.551283
elapsed time: 0:01:14.995736
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:05:09.107890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.94
 ---- batch: 020 ----
mean loss: 174.63
 ---- batch: 030 ----
mean loss: 169.76
 ---- batch: 040 ----
mean loss: 178.90
 ---- batch: 050 ----
mean loss: 180.05
 ---- batch: 060 ----
mean loss: 186.65
 ---- batch: 070 ----
mean loss: 176.19
 ---- batch: 080 ----
mean loss: 182.05
 ---- batch: 090 ----
mean loss: 178.64
 ---- batch: 100 ----
mean loss: 175.43
 ---- batch: 110 ----
mean loss: 177.41
train mean loss: 177.08
epoch train time: 0:00:00.550453
elapsed time: 0:01:15.546325
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:05:09.658478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.75
 ---- batch: 020 ----
mean loss: 175.42
 ---- batch: 030 ----
mean loss: 170.19
 ---- batch: 040 ----
mean loss: 181.06
 ---- batch: 050 ----
mean loss: 178.64
 ---- batch: 060 ----
mean loss: 180.88
 ---- batch: 070 ----
mean loss: 179.24
 ---- batch: 080 ----
mean loss: 173.82
 ---- batch: 090 ----
mean loss: 182.51
 ---- batch: 100 ----
mean loss: 169.66
 ---- batch: 110 ----
mean loss: 192.14
train mean loss: 177.30
epoch train time: 0:00:00.560168
elapsed time: 0:01:16.106636
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:05:10.218796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.91
 ---- batch: 020 ----
mean loss: 181.73
 ---- batch: 030 ----
mean loss: 173.60
 ---- batch: 040 ----
mean loss: 172.49
 ---- batch: 050 ----
mean loss: 175.45
 ---- batch: 060 ----
mean loss: 179.93
 ---- batch: 070 ----
mean loss: 184.15
 ---- batch: 080 ----
mean loss: 174.37
 ---- batch: 090 ----
mean loss: 179.63
 ---- batch: 100 ----
mean loss: 178.41
 ---- batch: 110 ----
mean loss: 172.83
train mean loss: 176.59
epoch train time: 0:00:00.553639
elapsed time: 0:01:16.660415
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:05:10.772570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.30
 ---- batch: 020 ----
mean loss: 171.57
 ---- batch: 030 ----
mean loss: 175.28
 ---- batch: 040 ----
mean loss: 173.71
 ---- batch: 050 ----
mean loss: 176.79
 ---- batch: 060 ----
mean loss: 169.42
 ---- batch: 070 ----
mean loss: 171.94
 ---- batch: 080 ----
mean loss: 192.88
 ---- batch: 090 ----
mean loss: 175.56
 ---- batch: 100 ----
mean loss: 165.83
 ---- batch: 110 ----
mean loss: 184.10
train mean loss: 175.38
epoch train time: 0:00:00.555404
elapsed time: 0:01:17.215955
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:05:11.328108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.79
 ---- batch: 020 ----
mean loss: 179.03
 ---- batch: 030 ----
mean loss: 179.46
 ---- batch: 040 ----
mean loss: 179.91
 ---- batch: 050 ----
mean loss: 166.92
 ---- batch: 060 ----
mean loss: 178.05
 ---- batch: 070 ----
mean loss: 185.59
 ---- batch: 080 ----
mean loss: 177.24
 ---- batch: 090 ----
mean loss: 176.27
 ---- batch: 100 ----
mean loss: 171.60
 ---- batch: 110 ----
mean loss: 174.99
train mean loss: 176.10
epoch train time: 0:00:00.551846
elapsed time: 0:01:17.767934
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:05:11.880086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.22
 ---- batch: 020 ----
mean loss: 172.57
 ---- batch: 030 ----
mean loss: 176.42
 ---- batch: 040 ----
mean loss: 169.69
 ---- batch: 050 ----
mean loss: 174.11
 ---- batch: 060 ----
mean loss: 173.30
 ---- batch: 070 ----
mean loss: 172.73
 ---- batch: 080 ----
mean loss: 172.90
 ---- batch: 090 ----
mean loss: 173.38
 ---- batch: 100 ----
mean loss: 174.77
 ---- batch: 110 ----
mean loss: 175.09
train mean loss: 173.75
epoch train time: 0:00:00.551654
elapsed time: 0:01:18.319766
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:05:12.431952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.20
 ---- batch: 020 ----
mean loss: 177.11
 ---- batch: 030 ----
mean loss: 172.13
 ---- batch: 040 ----
mean loss: 168.70
 ---- batch: 050 ----
mean loss: 167.11
 ---- batch: 060 ----
mean loss: 176.23
 ---- batch: 070 ----
mean loss: 180.62
 ---- batch: 080 ----
mean loss: 179.91
 ---- batch: 090 ----
mean loss: 171.65
 ---- batch: 100 ----
mean loss: 179.48
 ---- batch: 110 ----
mean loss: 172.65
train mean loss: 174.17
epoch train time: 0:00:00.562613
elapsed time: 0:01:18.882547
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:05:12.994702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.21
 ---- batch: 020 ----
mean loss: 180.14
 ---- batch: 030 ----
mean loss: 162.07
 ---- batch: 040 ----
mean loss: 175.37
 ---- batch: 050 ----
mean loss: 173.71
 ---- batch: 060 ----
mean loss: 173.86
 ---- batch: 070 ----
mean loss: 172.63
 ---- batch: 080 ----
mean loss: 179.09
 ---- batch: 090 ----
mean loss: 175.63
 ---- batch: 100 ----
mean loss: 176.41
 ---- batch: 110 ----
mean loss: 180.34
train mean loss: 173.75
epoch train time: 0:00:00.548563
elapsed time: 0:01:19.431242
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:05:13.543394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.83
 ---- batch: 020 ----
mean loss: 182.86
 ---- batch: 030 ----
mean loss: 163.03
 ---- batch: 040 ----
mean loss: 169.28
 ---- batch: 050 ----
mean loss: 179.37
 ---- batch: 060 ----
mean loss: 168.53
 ---- batch: 070 ----
mean loss: 168.80
 ---- batch: 080 ----
mean loss: 171.81
 ---- batch: 090 ----
mean loss: 177.72
 ---- batch: 100 ----
mean loss: 177.24
 ---- batch: 110 ----
mean loss: 178.38
train mean loss: 172.68
epoch train time: 0:00:00.556486
elapsed time: 0:01:19.987866
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:05:14.100031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.57
 ---- batch: 020 ----
mean loss: 173.22
 ---- batch: 030 ----
mean loss: 173.19
 ---- batch: 040 ----
mean loss: 164.12
 ---- batch: 050 ----
mean loss: 167.62
 ---- batch: 060 ----
mean loss: 170.96
 ---- batch: 070 ----
mean loss: 172.80
 ---- batch: 080 ----
mean loss: 179.71
 ---- batch: 090 ----
mean loss: 173.15
 ---- batch: 100 ----
mean loss: 166.06
 ---- batch: 110 ----
mean loss: 176.10
train mean loss: 171.46
epoch train time: 0:00:00.546005
elapsed time: 0:01:20.534023
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:05:14.646178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.30
 ---- batch: 020 ----
mean loss: 164.37
 ---- batch: 030 ----
mean loss: 173.87
 ---- batch: 040 ----
mean loss: 170.75
 ---- batch: 050 ----
mean loss: 173.60
 ---- batch: 060 ----
mean loss: 163.18
 ---- batch: 070 ----
mean loss: 178.06
 ---- batch: 080 ----
mean loss: 173.31
 ---- batch: 090 ----
mean loss: 170.68
 ---- batch: 100 ----
mean loss: 177.13
 ---- batch: 110 ----
mean loss: 170.93
train mean loss: 170.33
epoch train time: 0:00:00.547695
elapsed time: 0:01:21.081878
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:05:15.194063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.86
 ---- batch: 020 ----
mean loss: 169.57
 ---- batch: 030 ----
mean loss: 167.74
 ---- batch: 040 ----
mean loss: 171.42
 ---- batch: 050 ----
mean loss: 160.80
 ---- batch: 060 ----
mean loss: 169.31
 ---- batch: 070 ----
mean loss: 175.10
 ---- batch: 080 ----
mean loss: 176.13
 ---- batch: 090 ----
mean loss: 169.63
 ---- batch: 100 ----
mean loss: 170.20
 ---- batch: 110 ----
mean loss: 171.36
train mean loss: 169.68
epoch train time: 0:00:00.542909
elapsed time: 0:01:21.624952
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:05:15.737138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.74
 ---- batch: 020 ----
mean loss: 167.21
 ---- batch: 030 ----
mean loss: 161.16
 ---- batch: 040 ----
mean loss: 166.26
 ---- batch: 050 ----
mean loss: 161.94
 ---- batch: 060 ----
mean loss: 177.47
 ---- batch: 070 ----
mean loss: 179.87
 ---- batch: 080 ----
mean loss: 172.96
 ---- batch: 090 ----
mean loss: 165.58
 ---- batch: 100 ----
mean loss: 172.71
 ---- batch: 110 ----
mean loss: 172.08
train mean loss: 169.73
epoch train time: 0:00:00.553501
elapsed time: 0:01:22.178622
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:05:16.290775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.20
 ---- batch: 020 ----
mean loss: 168.22
 ---- batch: 030 ----
mean loss: 166.81
 ---- batch: 040 ----
mean loss: 161.36
 ---- batch: 050 ----
mean loss: 168.92
 ---- batch: 060 ----
mean loss: 172.74
 ---- batch: 070 ----
mean loss: 171.32
 ---- batch: 080 ----
mean loss: 171.27
 ---- batch: 090 ----
mean loss: 170.09
 ---- batch: 100 ----
mean loss: 172.11
 ---- batch: 110 ----
mean loss: 170.89
train mean loss: 169.08
epoch train time: 0:00:00.553539
elapsed time: 0:01:22.732295
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:05:16.844449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.49
 ---- batch: 020 ----
mean loss: 175.60
 ---- batch: 030 ----
mean loss: 159.81
 ---- batch: 040 ----
mean loss: 173.27
 ---- batch: 050 ----
mean loss: 158.48
 ---- batch: 060 ----
mean loss: 168.72
 ---- batch: 070 ----
mean loss: 174.48
 ---- batch: 080 ----
mean loss: 178.73
 ---- batch: 090 ----
mean loss: 165.40
 ---- batch: 100 ----
mean loss: 170.25
 ---- batch: 110 ----
mean loss: 170.23
train mean loss: 169.43
epoch train time: 0:00:00.547940
elapsed time: 0:01:23.280369
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:05:17.392538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.45
 ---- batch: 020 ----
mean loss: 173.85
 ---- batch: 030 ----
mean loss: 164.46
 ---- batch: 040 ----
mean loss: 173.18
 ---- batch: 050 ----
mean loss: 167.29
 ---- batch: 060 ----
mean loss: 155.10
 ---- batch: 070 ----
mean loss: 170.54
 ---- batch: 080 ----
mean loss: 158.31
 ---- batch: 090 ----
mean loss: 172.99
 ---- batch: 100 ----
mean loss: 172.80
 ---- batch: 110 ----
mean loss: 171.69
train mean loss: 167.85
epoch train time: 0:00:00.549165
elapsed time: 0:01:23.829685
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:05:17.941837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.59
 ---- batch: 020 ----
mean loss: 175.17
 ---- batch: 030 ----
mean loss: 169.06
 ---- batch: 040 ----
mean loss: 161.84
 ---- batch: 050 ----
mean loss: 164.07
 ---- batch: 060 ----
mean loss: 171.39
 ---- batch: 070 ----
mean loss: 173.03
 ---- batch: 080 ----
mean loss: 171.45
 ---- batch: 090 ----
mean loss: 163.07
 ---- batch: 100 ----
mean loss: 166.55
 ---- batch: 110 ----
mean loss: 164.88
train mean loss: 167.82
epoch train time: 0:00:00.545313
elapsed time: 0:01:24.375127
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:05:18.487278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.52
 ---- batch: 020 ----
mean loss: 165.84
 ---- batch: 030 ----
mean loss: 170.95
 ---- batch: 040 ----
mean loss: 162.26
 ---- batch: 050 ----
mean loss: 169.52
 ---- batch: 060 ----
mean loss: 166.56
 ---- batch: 070 ----
mean loss: 166.72
 ---- batch: 080 ----
mean loss: 169.92
 ---- batch: 090 ----
mean loss: 164.87
 ---- batch: 100 ----
mean loss: 163.48
 ---- batch: 110 ----
mean loss: 168.00
train mean loss: 166.99
epoch train time: 0:00:00.556432
elapsed time: 0:01:24.931712
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:05:19.043862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.31
 ---- batch: 020 ----
mean loss: 173.57
 ---- batch: 030 ----
mean loss: 162.69
 ---- batch: 040 ----
mean loss: 172.51
 ---- batch: 050 ----
mean loss: 174.39
 ---- batch: 060 ----
mean loss: 161.11
 ---- batch: 070 ----
mean loss: 161.84
 ---- batch: 080 ----
mean loss: 161.59
 ---- batch: 090 ----
mean loss: 171.46
 ---- batch: 100 ----
mean loss: 167.15
 ---- batch: 110 ----
mean loss: 166.99
train mean loss: 167.74
epoch train time: 0:00:00.549869
elapsed time: 0:01:25.481734
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:05:19.593903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.16
 ---- batch: 020 ----
mean loss: 154.50
 ---- batch: 030 ----
mean loss: 152.65
 ---- batch: 040 ----
mean loss: 170.00
 ---- batch: 050 ----
mean loss: 176.30
 ---- batch: 060 ----
mean loss: 171.39
 ---- batch: 070 ----
mean loss: 168.70
 ---- batch: 080 ----
mean loss: 164.46
 ---- batch: 090 ----
mean loss: 160.98
 ---- batch: 100 ----
mean loss: 167.74
 ---- batch: 110 ----
mean loss: 169.37
train mean loss: 166.02
epoch train time: 0:00:00.553688
elapsed time: 0:01:26.035588
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:05:20.147758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.24
 ---- batch: 020 ----
mean loss: 161.42
 ---- batch: 030 ----
mean loss: 162.89
 ---- batch: 040 ----
mean loss: 168.38
 ---- batch: 050 ----
mean loss: 174.85
 ---- batch: 060 ----
mean loss: 163.63
 ---- batch: 070 ----
mean loss: 159.65
 ---- batch: 080 ----
mean loss: 170.65
 ---- batch: 090 ----
mean loss: 173.11
 ---- batch: 100 ----
mean loss: 168.64
 ---- batch: 110 ----
mean loss: 160.84
train mean loss: 165.93
epoch train time: 0:00:00.552278
elapsed time: 0:01:26.588042
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:05:20.700210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.41
 ---- batch: 020 ----
mean loss: 164.23
 ---- batch: 030 ----
mean loss: 163.40
 ---- batch: 040 ----
mean loss: 161.52
 ---- batch: 050 ----
mean loss: 165.89
 ---- batch: 060 ----
mean loss: 170.09
 ---- batch: 070 ----
mean loss: 166.69
 ---- batch: 080 ----
mean loss: 176.31
 ---- batch: 090 ----
mean loss: 164.44
 ---- batch: 100 ----
mean loss: 165.16
 ---- batch: 110 ----
mean loss: 160.46
train mean loss: 164.77
epoch train time: 0:00:00.556526
elapsed time: 0:01:27.144744
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:05:21.256898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.10
 ---- batch: 020 ----
mean loss: 165.46
 ---- batch: 030 ----
mean loss: 155.61
 ---- batch: 040 ----
mean loss: 167.93
 ---- batch: 050 ----
mean loss: 164.37
 ---- batch: 060 ----
mean loss: 163.45
 ---- batch: 070 ----
mean loss: 158.10
 ---- batch: 080 ----
mean loss: 154.74
 ---- batch: 090 ----
mean loss: 165.51
 ---- batch: 100 ----
mean loss: 168.24
 ---- batch: 110 ----
mean loss: 173.06
train mean loss: 164.24
epoch train time: 0:00:00.548756
elapsed time: 0:01:27.693634
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:05:21.805788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.76
 ---- batch: 020 ----
mean loss: 160.19
 ---- batch: 030 ----
mean loss: 164.65
 ---- batch: 040 ----
mean loss: 160.37
 ---- batch: 050 ----
mean loss: 162.13
 ---- batch: 060 ----
mean loss: 170.37
 ---- batch: 070 ----
mean loss: 165.31
 ---- batch: 080 ----
mean loss: 163.08
 ---- batch: 090 ----
mean loss: 161.02
 ---- batch: 100 ----
mean loss: 168.52
 ---- batch: 110 ----
mean loss: 165.76
train mean loss: 163.39
epoch train time: 0:00:00.539924
elapsed time: 0:01:28.233695
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:05:22.345848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.64
 ---- batch: 020 ----
mean loss: 163.29
 ---- batch: 030 ----
mean loss: 163.06
 ---- batch: 040 ----
mean loss: 159.64
 ---- batch: 050 ----
mean loss: 169.46
 ---- batch: 060 ----
mean loss: 157.80
 ---- batch: 070 ----
mean loss: 162.61
 ---- batch: 080 ----
mean loss: 166.53
 ---- batch: 090 ----
mean loss: 163.01
 ---- batch: 100 ----
mean loss: 152.98
 ---- batch: 110 ----
mean loss: 167.70
train mean loss: 163.16
epoch train time: 0:00:00.549810
elapsed time: 0:01:28.783669
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:05:22.895842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.59
 ---- batch: 020 ----
mean loss: 159.53
 ---- batch: 030 ----
mean loss: 160.82
 ---- batch: 040 ----
mean loss: 166.83
 ---- batch: 050 ----
mean loss: 161.11
 ---- batch: 060 ----
mean loss: 166.89
 ---- batch: 070 ----
mean loss: 164.35
 ---- batch: 080 ----
mean loss: 172.19
 ---- batch: 090 ----
mean loss: 162.88
 ---- batch: 100 ----
mean loss: 165.03
 ---- batch: 110 ----
mean loss: 157.65
train mean loss: 164.12
epoch train time: 0:00:00.553223
elapsed time: 0:01:29.337077
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:05:23.449229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.43
 ---- batch: 020 ----
mean loss: 158.32
 ---- batch: 030 ----
mean loss: 161.10
 ---- batch: 040 ----
mean loss: 163.60
 ---- batch: 050 ----
mean loss: 159.61
 ---- batch: 060 ----
mean loss: 168.13
 ---- batch: 070 ----
mean loss: 161.78
 ---- batch: 080 ----
mean loss: 159.76
 ---- batch: 090 ----
mean loss: 155.85
 ---- batch: 100 ----
mean loss: 164.79
 ---- batch: 110 ----
mean loss: 168.55
train mean loss: 162.72
epoch train time: 0:00:00.559910
elapsed time: 0:01:29.897125
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:05:24.009280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.23
 ---- batch: 020 ----
mean loss: 162.92
 ---- batch: 030 ----
mean loss: 163.97
 ---- batch: 040 ----
mean loss: 160.61
 ---- batch: 050 ----
mean loss: 160.18
 ---- batch: 060 ----
mean loss: 159.14
 ---- batch: 070 ----
mean loss: 160.99
 ---- batch: 080 ----
mean loss: 158.87
 ---- batch: 090 ----
mean loss: 162.31
 ---- batch: 100 ----
mean loss: 167.13
 ---- batch: 110 ----
mean loss: 174.69
train mean loss: 162.00
epoch train time: 0:00:00.550405
elapsed time: 0:01:30.447664
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:05:24.559816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.50
 ---- batch: 020 ----
mean loss: 159.89
 ---- batch: 030 ----
mean loss: 158.82
 ---- batch: 040 ----
mean loss: 157.54
 ---- batch: 050 ----
mean loss: 156.75
 ---- batch: 060 ----
mean loss: 167.46
 ---- batch: 070 ----
mean loss: 163.95
 ---- batch: 080 ----
mean loss: 159.78
 ---- batch: 090 ----
mean loss: 163.00
 ---- batch: 100 ----
mean loss: 170.76
 ---- batch: 110 ----
mean loss: 158.88
train mean loss: 161.94
epoch train time: 0:00:00.554982
elapsed time: 0:01:31.002790
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:05:25.114943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.59
 ---- batch: 020 ----
mean loss: 165.62
 ---- batch: 030 ----
mean loss: 158.35
 ---- batch: 040 ----
mean loss: 157.79
 ---- batch: 050 ----
mean loss: 161.69
 ---- batch: 060 ----
mean loss: 161.53
 ---- batch: 070 ----
mean loss: 155.29
 ---- batch: 080 ----
mean loss: 166.88
 ---- batch: 090 ----
mean loss: 167.01
 ---- batch: 100 ----
mean loss: 159.10
 ---- batch: 110 ----
mean loss: 164.50
train mean loss: 161.23
epoch train time: 0:00:00.550453
elapsed time: 0:01:31.553399
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:05:25.665553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.89
 ---- batch: 020 ----
mean loss: 166.07
 ---- batch: 030 ----
mean loss: 161.72
 ---- batch: 040 ----
mean loss: 158.69
 ---- batch: 050 ----
mean loss: 168.01
 ---- batch: 060 ----
mean loss: 154.34
 ---- batch: 070 ----
mean loss: 171.03
 ---- batch: 080 ----
mean loss: 162.29
 ---- batch: 090 ----
mean loss: 164.63
 ---- batch: 100 ----
mean loss: 152.18
 ---- batch: 110 ----
mean loss: 162.11
train mean loss: 161.41
epoch train time: 0:00:00.552823
elapsed time: 0:01:32.106363
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:05:26.218549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.25
 ---- batch: 020 ----
mean loss: 161.99
 ---- batch: 030 ----
mean loss: 150.56
 ---- batch: 040 ----
mean loss: 158.93
 ---- batch: 050 ----
mean loss: 162.49
 ---- batch: 060 ----
mean loss: 157.05
 ---- batch: 070 ----
mean loss: 165.00
 ---- batch: 080 ----
mean loss: 162.77
 ---- batch: 090 ----
mean loss: 163.71
 ---- batch: 100 ----
mean loss: 165.73
 ---- batch: 110 ----
mean loss: 165.78
train mean loss: 160.70
epoch train time: 0:00:00.551819
elapsed time: 0:01:32.658362
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:05:26.770543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.61
 ---- batch: 020 ----
mean loss: 150.41
 ---- batch: 030 ----
mean loss: 163.05
 ---- batch: 040 ----
mean loss: 157.10
 ---- batch: 050 ----
mean loss: 162.94
 ---- batch: 060 ----
mean loss: 157.11
 ---- batch: 070 ----
mean loss: 166.21
 ---- batch: 080 ----
mean loss: 165.21
 ---- batch: 090 ----
mean loss: 158.17
 ---- batch: 100 ----
mean loss: 158.59
 ---- batch: 110 ----
mean loss: 166.74
train mean loss: 159.83
epoch train time: 0:00:00.551682
elapsed time: 0:01:33.210224
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:05:27.322379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.57
 ---- batch: 020 ----
mean loss: 157.36
 ---- batch: 030 ----
mean loss: 154.63
 ---- batch: 040 ----
mean loss: 152.56
 ---- batch: 050 ----
mean loss: 166.99
 ---- batch: 060 ----
mean loss: 159.44
 ---- batch: 070 ----
mean loss: 159.74
 ---- batch: 080 ----
mean loss: 162.28
 ---- batch: 090 ----
mean loss: 152.93
 ---- batch: 100 ----
mean loss: 155.84
 ---- batch: 110 ----
mean loss: 164.88
train mean loss: 159.09
epoch train time: 0:00:00.545345
elapsed time: 0:01:33.755703
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:05:27.867855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.33
 ---- batch: 020 ----
mean loss: 152.69
 ---- batch: 030 ----
mean loss: 157.85
 ---- batch: 040 ----
mean loss: 160.02
 ---- batch: 050 ----
mean loss: 153.92
 ---- batch: 060 ----
mean loss: 160.05
 ---- batch: 070 ----
mean loss: 161.59
 ---- batch: 080 ----
mean loss: 168.86
 ---- batch: 090 ----
mean loss: 168.86
 ---- batch: 100 ----
mean loss: 152.45
 ---- batch: 110 ----
mean loss: 162.84
train mean loss: 160.08
epoch train time: 0:00:00.545920
elapsed time: 0:01:34.301760
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:05:28.413912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.22
 ---- batch: 020 ----
mean loss: 152.43
 ---- batch: 030 ----
mean loss: 160.39
 ---- batch: 040 ----
mean loss: 147.98
 ---- batch: 050 ----
mean loss: 158.06
 ---- batch: 060 ----
mean loss: 158.57
 ---- batch: 070 ----
mean loss: 149.85
 ---- batch: 080 ----
mean loss: 166.37
 ---- batch: 090 ----
mean loss: 162.53
 ---- batch: 100 ----
mean loss: 159.24
 ---- batch: 110 ----
mean loss: 166.64
train mean loss: 158.63
epoch train time: 0:00:00.567265
elapsed time: 0:01:34.869155
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:05:28.981308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.62
 ---- batch: 020 ----
mean loss: 165.20
 ---- batch: 030 ----
mean loss: 156.79
 ---- batch: 040 ----
mean loss: 158.68
 ---- batch: 050 ----
mean loss: 163.89
 ---- batch: 060 ----
mean loss: 169.09
 ---- batch: 070 ----
mean loss: 154.46
 ---- batch: 080 ----
mean loss: 152.71
 ---- batch: 090 ----
mean loss: 159.02
 ---- batch: 100 ----
mean loss: 160.71
 ---- batch: 110 ----
mean loss: 156.40
train mean loss: 159.05
epoch train time: 0:00:00.545775
elapsed time: 0:01:35.415066
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:05:29.527219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.43
 ---- batch: 020 ----
mean loss: 165.59
 ---- batch: 030 ----
mean loss: 154.16
 ---- batch: 040 ----
mean loss: 149.68
 ---- batch: 050 ----
mean loss: 158.60
 ---- batch: 060 ----
mean loss: 156.51
 ---- batch: 070 ----
mean loss: 157.21
 ---- batch: 080 ----
mean loss: 162.61
 ---- batch: 090 ----
mean loss: 161.35
 ---- batch: 100 ----
mean loss: 147.35
 ---- batch: 110 ----
mean loss: 150.93
train mean loss: 157.02
epoch train time: 0:00:00.547483
elapsed time: 0:01:35.962680
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:05:30.074832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.69
 ---- batch: 020 ----
mean loss: 158.73
 ---- batch: 030 ----
mean loss: 155.22
 ---- batch: 040 ----
mean loss: 155.92
 ---- batch: 050 ----
mean loss: 157.23
 ---- batch: 060 ----
mean loss: 161.56
 ---- batch: 070 ----
mean loss: 168.95
 ---- batch: 080 ----
mean loss: 163.76
 ---- batch: 090 ----
mean loss: 155.53
 ---- batch: 100 ----
mean loss: 160.52
 ---- batch: 110 ----
mean loss: 155.05
train mean loss: 158.66
epoch train time: 0:00:00.542353
elapsed time: 0:01:36.505164
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:05:30.617350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.90
 ---- batch: 020 ----
mean loss: 158.32
 ---- batch: 030 ----
mean loss: 155.61
 ---- batch: 040 ----
mean loss: 153.41
 ---- batch: 050 ----
mean loss: 153.46
 ---- batch: 060 ----
mean loss: 158.87
 ---- batch: 070 ----
mean loss: 160.65
 ---- batch: 080 ----
mean loss: 160.12
 ---- batch: 090 ----
mean loss: 159.65
 ---- batch: 100 ----
mean loss: 161.39
 ---- batch: 110 ----
mean loss: 152.11
train mean loss: 157.78
epoch train time: 0:00:00.556753
elapsed time: 0:01:37.062127
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:05:31.174315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.25
 ---- batch: 020 ----
mean loss: 150.50
 ---- batch: 030 ----
mean loss: 153.42
 ---- batch: 040 ----
mean loss: 154.53
 ---- batch: 050 ----
mean loss: 158.14
 ---- batch: 060 ----
mean loss: 156.03
 ---- batch: 070 ----
mean loss: 156.94
 ---- batch: 080 ----
mean loss: 161.82
 ---- batch: 090 ----
mean loss: 156.39
 ---- batch: 100 ----
mean loss: 158.72
 ---- batch: 110 ----
mean loss: 162.92
train mean loss: 156.72
epoch train time: 0:00:00.555294
elapsed time: 0:01:37.617626
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:05:31.729780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.66
 ---- batch: 020 ----
mean loss: 154.96
 ---- batch: 030 ----
mean loss: 151.62
 ---- batch: 040 ----
mean loss: 157.55
 ---- batch: 050 ----
mean loss: 160.31
 ---- batch: 060 ----
mean loss: 160.36
 ---- batch: 070 ----
mean loss: 156.22
 ---- batch: 080 ----
mean loss: 155.60
 ---- batch: 090 ----
mean loss: 164.34
 ---- batch: 100 ----
mean loss: 156.94
 ---- batch: 110 ----
mean loss: 160.43
train mean loss: 157.34
epoch train time: 0:00:00.544618
elapsed time: 0:01:38.162384
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:05:32.274538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.10
 ---- batch: 020 ----
mean loss: 143.35
 ---- batch: 030 ----
mean loss: 161.81
 ---- batch: 040 ----
mean loss: 160.08
 ---- batch: 050 ----
mean loss: 154.37
 ---- batch: 060 ----
mean loss: 153.12
 ---- batch: 070 ----
mean loss: 163.10
 ---- batch: 080 ----
mean loss: 151.51
 ---- batch: 090 ----
mean loss: 162.82
 ---- batch: 100 ----
mean loss: 157.68
 ---- batch: 110 ----
mean loss: 159.41
train mean loss: 156.89
epoch train time: 0:00:00.564448
elapsed time: 0:01:38.726969
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:05:32.839155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.08
 ---- batch: 020 ----
mean loss: 148.53
 ---- batch: 030 ----
mean loss: 151.18
 ---- batch: 040 ----
mean loss: 152.58
 ---- batch: 050 ----
mean loss: 149.11
 ---- batch: 060 ----
mean loss: 151.82
 ---- batch: 070 ----
mean loss: 157.97
 ---- batch: 080 ----
mean loss: 161.13
 ---- batch: 090 ----
mean loss: 157.23
 ---- batch: 100 ----
mean loss: 164.93
 ---- batch: 110 ----
mean loss: 160.15
train mean loss: 155.66
epoch train time: 0:00:00.548030
elapsed time: 0:01:39.275166
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:05:33.387320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.84
 ---- batch: 020 ----
mean loss: 161.44
 ---- batch: 030 ----
mean loss: 152.40
 ---- batch: 040 ----
mean loss: 153.26
 ---- batch: 050 ----
mean loss: 160.90
 ---- batch: 060 ----
mean loss: 157.14
 ---- batch: 070 ----
mean loss: 161.43
 ---- batch: 080 ----
mean loss: 157.54
 ---- batch: 090 ----
mean loss: 153.56
 ---- batch: 100 ----
mean loss: 157.33
 ---- batch: 110 ----
mean loss: 152.65
train mean loss: 156.61
epoch train time: 0:00:00.552150
elapsed time: 0:01:39.827449
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:05:33.939629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.43
 ---- batch: 020 ----
mean loss: 152.07
 ---- batch: 030 ----
mean loss: 153.30
 ---- batch: 040 ----
mean loss: 155.45
 ---- batch: 050 ----
mean loss: 152.72
 ---- batch: 060 ----
mean loss: 153.02
 ---- batch: 070 ----
mean loss: 158.06
 ---- batch: 080 ----
mean loss: 160.74
 ---- batch: 090 ----
mean loss: 161.51
 ---- batch: 100 ----
mean loss: 158.42
 ---- batch: 110 ----
mean loss: 148.56
train mean loss: 155.18
epoch train time: 0:00:00.540881
elapsed time: 0:01:40.368541
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:05:34.480708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.17
 ---- batch: 020 ----
mean loss: 143.89
 ---- batch: 030 ----
mean loss: 157.43
 ---- batch: 040 ----
mean loss: 155.99
 ---- batch: 050 ----
mean loss: 160.00
 ---- batch: 060 ----
mean loss: 161.17
 ---- batch: 070 ----
mean loss: 163.25
 ---- batch: 080 ----
mean loss: 151.02
 ---- batch: 090 ----
mean loss: 154.34
 ---- batch: 100 ----
mean loss: 155.74
 ---- batch: 110 ----
mean loss: 150.58
train mean loss: 155.10
epoch train time: 0:00:00.557945
elapsed time: 0:01:40.926636
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:05:35.038790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.64
 ---- batch: 020 ----
mean loss: 147.28
 ---- batch: 030 ----
mean loss: 158.81
 ---- batch: 040 ----
mean loss: 158.69
 ---- batch: 050 ----
mean loss: 149.38
 ---- batch: 060 ----
mean loss: 156.44
 ---- batch: 070 ----
mean loss: 161.88
 ---- batch: 080 ----
mean loss: 160.51
 ---- batch: 090 ----
mean loss: 155.05
 ---- batch: 100 ----
mean loss: 153.31
 ---- batch: 110 ----
mean loss: 156.80
train mean loss: 155.36
epoch train time: 0:00:00.552535
elapsed time: 0:01:41.479314
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:05:35.591470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.67
 ---- batch: 020 ----
mean loss: 148.49
 ---- batch: 030 ----
mean loss: 154.55
 ---- batch: 040 ----
mean loss: 158.15
 ---- batch: 050 ----
mean loss: 155.92
 ---- batch: 060 ----
mean loss: 151.30
 ---- batch: 070 ----
mean loss: 151.71
 ---- batch: 080 ----
mean loss: 158.09
 ---- batch: 090 ----
mean loss: 151.84
 ---- batch: 100 ----
mean loss: 155.28
 ---- batch: 110 ----
mean loss: 155.74
train mean loss: 154.20
epoch train time: 0:00:00.550030
elapsed time: 0:01:42.029527
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:05:36.141696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.12
 ---- batch: 020 ----
mean loss: 150.72
 ---- batch: 030 ----
mean loss: 153.52
 ---- batch: 040 ----
mean loss: 137.38
 ---- batch: 050 ----
mean loss: 164.20
 ---- batch: 060 ----
mean loss: 155.61
 ---- batch: 070 ----
mean loss: 153.61
 ---- batch: 080 ----
mean loss: 156.23
 ---- batch: 090 ----
mean loss: 158.71
 ---- batch: 100 ----
mean loss: 153.42
 ---- batch: 110 ----
mean loss: 156.36
train mean loss: 154.24
epoch train time: 0:00:00.544487
elapsed time: 0:01:42.574186
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:05:36.686341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.10
 ---- batch: 020 ----
mean loss: 141.36
 ---- batch: 030 ----
mean loss: 153.78
 ---- batch: 040 ----
mean loss: 154.79
 ---- batch: 050 ----
mean loss: 154.75
 ---- batch: 060 ----
mean loss: 154.35
 ---- batch: 070 ----
mean loss: 154.88
 ---- batch: 080 ----
mean loss: 162.83
 ---- batch: 090 ----
mean loss: 150.06
 ---- batch: 100 ----
mean loss: 163.56
 ---- batch: 110 ----
mean loss: 145.43
train mean loss: 153.93
epoch train time: 0:00:00.554419
elapsed time: 0:01:43.128754
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:05:37.240906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.75
 ---- batch: 020 ----
mean loss: 159.14
 ---- batch: 030 ----
mean loss: 151.90
 ---- batch: 040 ----
mean loss: 152.23
 ---- batch: 050 ----
mean loss: 155.95
 ---- batch: 060 ----
mean loss: 152.20
 ---- batch: 070 ----
mean loss: 146.73
 ---- batch: 080 ----
mean loss: 152.86
 ---- batch: 090 ----
mean loss: 156.43
 ---- batch: 100 ----
mean loss: 154.12
 ---- batch: 110 ----
mean loss: 157.83
train mean loss: 153.64
epoch train time: 0:00:00.558299
elapsed time: 0:01:43.687187
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:05:37.799357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.40
 ---- batch: 020 ----
mean loss: 148.61
 ---- batch: 030 ----
mean loss: 152.07
 ---- batch: 040 ----
mean loss: 155.27
 ---- batch: 050 ----
mean loss: 148.71
 ---- batch: 060 ----
mean loss: 149.74
 ---- batch: 070 ----
mean loss: 161.57
 ---- batch: 080 ----
mean loss: 155.44
 ---- batch: 090 ----
mean loss: 158.58
 ---- batch: 100 ----
mean loss: 149.47
 ---- batch: 110 ----
mean loss: 153.28
train mean loss: 153.14
epoch train time: 0:00:00.557222
elapsed time: 0:01:44.244576
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:05:38.356745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.75
 ---- batch: 020 ----
mean loss: 150.03
 ---- batch: 030 ----
mean loss: 154.71
 ---- batch: 040 ----
mean loss: 156.79
 ---- batch: 050 ----
mean loss: 152.52
 ---- batch: 060 ----
mean loss: 153.57
 ---- batch: 070 ----
mean loss: 153.43
 ---- batch: 080 ----
mean loss: 152.21
 ---- batch: 090 ----
mean loss: 147.43
 ---- batch: 100 ----
mean loss: 155.52
 ---- batch: 110 ----
mean loss: 156.67
train mean loss: 153.03
epoch train time: 0:00:00.555030
elapsed time: 0:01:44.799815
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:05:38.911967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.50
 ---- batch: 020 ----
mean loss: 151.80
 ---- batch: 030 ----
mean loss: 150.21
 ---- batch: 040 ----
mean loss: 154.46
 ---- batch: 050 ----
mean loss: 153.54
 ---- batch: 060 ----
mean loss: 156.49
 ---- batch: 070 ----
mean loss: 156.06
 ---- batch: 080 ----
mean loss: 146.16
 ---- batch: 090 ----
mean loss: 145.00
 ---- batch: 100 ----
mean loss: 151.06
 ---- batch: 110 ----
mean loss: 153.17
train mean loss: 152.04
epoch train time: 0:00:00.548653
elapsed time: 0:01:45.348599
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:05:39.460750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.28
 ---- batch: 020 ----
mean loss: 146.61
 ---- batch: 030 ----
mean loss: 150.59
 ---- batch: 040 ----
mean loss: 146.93
 ---- batch: 050 ----
mean loss: 152.92
 ---- batch: 060 ----
mean loss: 146.20
 ---- batch: 070 ----
mean loss: 153.06
 ---- batch: 080 ----
mean loss: 150.29
 ---- batch: 090 ----
mean loss: 161.57
 ---- batch: 100 ----
mean loss: 149.53
 ---- batch: 110 ----
mean loss: 157.48
train mean loss: 152.73
epoch train time: 0:00:00.562333
elapsed time: 0:01:45.911077
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:05:40.023241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.56
 ---- batch: 020 ----
mean loss: 157.02
 ---- batch: 030 ----
mean loss: 160.95
 ---- batch: 040 ----
mean loss: 151.99
 ---- batch: 050 ----
mean loss: 152.59
 ---- batch: 060 ----
mean loss: 150.16
 ---- batch: 070 ----
mean loss: 155.41
 ---- batch: 080 ----
mean loss: 148.72
 ---- batch: 090 ----
mean loss: 148.39
 ---- batch: 100 ----
mean loss: 155.04
 ---- batch: 110 ----
mean loss: 152.46
train mean loss: 153.15
epoch train time: 0:00:00.550170
elapsed time: 0:01:46.461400
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:05:40.573555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.12
 ---- batch: 020 ----
mean loss: 155.18
 ---- batch: 030 ----
mean loss: 155.88
 ---- batch: 040 ----
mean loss: 158.25
 ---- batch: 050 ----
mean loss: 149.21
 ---- batch: 060 ----
mean loss: 154.18
 ---- batch: 070 ----
mean loss: 150.38
 ---- batch: 080 ----
mean loss: 151.28
 ---- batch: 090 ----
mean loss: 152.61
 ---- batch: 100 ----
mean loss: 145.00
 ---- batch: 110 ----
mean loss: 153.84
train mean loss: 152.50
epoch train time: 0:00:00.550497
elapsed time: 0:01:47.012057
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:05:41.124243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.57
 ---- batch: 020 ----
mean loss: 155.40
 ---- batch: 030 ----
mean loss: 149.56
 ---- batch: 040 ----
mean loss: 146.36
 ---- batch: 050 ----
mean loss: 145.46
 ---- batch: 060 ----
mean loss: 151.40
 ---- batch: 070 ----
mean loss: 159.67
 ---- batch: 080 ----
mean loss: 153.91
 ---- batch: 090 ----
mean loss: 156.06
 ---- batch: 100 ----
mean loss: 147.61
 ---- batch: 110 ----
mean loss: 149.20
train mean loss: 152.45
epoch train time: 0:00:00.545788
elapsed time: 0:01:47.558029
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:05:41.670184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.53
 ---- batch: 020 ----
mean loss: 148.09
 ---- batch: 030 ----
mean loss: 147.63
 ---- batch: 040 ----
mean loss: 151.41
 ---- batch: 050 ----
mean loss: 149.89
 ---- batch: 060 ----
mean loss: 155.39
 ---- batch: 070 ----
mean loss: 145.09
 ---- batch: 080 ----
mean loss: 152.44
 ---- batch: 090 ----
mean loss: 145.59
 ---- batch: 100 ----
mean loss: 161.01
 ---- batch: 110 ----
mean loss: 156.15
train mean loss: 150.38
epoch train time: 0:00:00.552162
elapsed time: 0:01:48.110329
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:05:42.222484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.18
 ---- batch: 020 ----
mean loss: 156.25
 ---- batch: 030 ----
mean loss: 141.36
 ---- batch: 040 ----
mean loss: 157.45
 ---- batch: 050 ----
mean loss: 144.28
 ---- batch: 060 ----
mean loss: 153.65
 ---- batch: 070 ----
mean loss: 149.30
 ---- batch: 080 ----
mean loss: 151.90
 ---- batch: 090 ----
mean loss: 149.78
 ---- batch: 100 ----
mean loss: 158.93
 ---- batch: 110 ----
mean loss: 147.82
train mean loss: 151.74
epoch train time: 0:00:00.553759
elapsed time: 0:01:48.664245
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:05:42.776405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.02
 ---- batch: 020 ----
mean loss: 149.92
 ---- batch: 030 ----
mean loss: 153.93
 ---- batch: 040 ----
mean loss: 161.15
 ---- batch: 050 ----
mean loss: 154.39
 ---- batch: 060 ----
mean loss: 157.85
 ---- batch: 070 ----
mean loss: 146.51
 ---- batch: 080 ----
mean loss: 145.27
 ---- batch: 090 ----
mean loss: 149.45
 ---- batch: 100 ----
mean loss: 142.28
 ---- batch: 110 ----
mean loss: 155.69
train mean loss: 150.66
epoch train time: 0:00:00.550749
elapsed time: 0:01:49.215131
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:05:43.327283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.73
 ---- batch: 020 ----
mean loss: 144.72
 ---- batch: 030 ----
mean loss: 151.73
 ---- batch: 040 ----
mean loss: 156.57
 ---- batch: 050 ----
mean loss: 156.53
 ---- batch: 060 ----
mean loss: 142.86
 ---- batch: 070 ----
mean loss: 150.55
 ---- batch: 080 ----
mean loss: 157.05
 ---- batch: 090 ----
mean loss: 156.84
 ---- batch: 100 ----
mean loss: 153.57
 ---- batch: 110 ----
mean loss: 143.67
train mean loss: 150.67
epoch train time: 0:00:00.564940
elapsed time: 0:01:49.780210
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:05:43.892366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.40
 ---- batch: 020 ----
mean loss: 152.60
 ---- batch: 030 ----
mean loss: 146.44
 ---- batch: 040 ----
mean loss: 145.59
 ---- batch: 050 ----
mean loss: 148.36
 ---- batch: 060 ----
mean loss: 150.34
 ---- batch: 070 ----
mean loss: 151.74
 ---- batch: 080 ----
mean loss: 152.17
 ---- batch: 090 ----
mean loss: 152.20
 ---- batch: 100 ----
mean loss: 147.82
 ---- batch: 110 ----
mean loss: 147.13
train mean loss: 149.99
epoch train time: 0:00:00.546769
elapsed time: 0:01:50.327119
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:05:44.439273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.03
 ---- batch: 020 ----
mean loss: 152.19
 ---- batch: 030 ----
mean loss: 144.68
 ---- batch: 040 ----
mean loss: 145.67
 ---- batch: 050 ----
mean loss: 145.25
 ---- batch: 060 ----
mean loss: 151.66
 ---- batch: 070 ----
mean loss: 148.60
 ---- batch: 080 ----
mean loss: 153.23
 ---- batch: 090 ----
mean loss: 145.33
 ---- batch: 100 ----
mean loss: 157.23
 ---- batch: 110 ----
mean loss: 159.29
train mean loss: 149.73
epoch train time: 0:00:00.549401
elapsed time: 0:01:50.876702
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:05:44.988902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.58
 ---- batch: 020 ----
mean loss: 142.01
 ---- batch: 030 ----
mean loss: 144.09
 ---- batch: 040 ----
mean loss: 148.23
 ---- batch: 050 ----
mean loss: 153.35
 ---- batch: 060 ----
mean loss: 143.67
 ---- batch: 070 ----
mean loss: 159.99
 ---- batch: 080 ----
mean loss: 158.14
 ---- batch: 090 ----
mean loss: 159.08
 ---- batch: 100 ----
mean loss: 152.54
 ---- batch: 110 ----
mean loss: 142.68
train mean loss: 150.41
epoch train time: 0:00:00.545177
elapsed time: 0:01:51.422059
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:05:45.534211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.17
 ---- batch: 020 ----
mean loss: 144.57
 ---- batch: 030 ----
mean loss: 153.90
 ---- batch: 040 ----
mean loss: 151.12
 ---- batch: 050 ----
mean loss: 146.67
 ---- batch: 060 ----
mean loss: 147.80
 ---- batch: 070 ----
mean loss: 146.34
 ---- batch: 080 ----
mean loss: 149.41
 ---- batch: 090 ----
mean loss: 150.91
 ---- batch: 100 ----
mean loss: 153.19
 ---- batch: 110 ----
mean loss: 152.59
train mean loss: 149.28
epoch train time: 0:00:00.571209
elapsed time: 0:01:51.993423
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:05:46.105588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.73
 ---- batch: 020 ----
mean loss: 154.50
 ---- batch: 030 ----
mean loss: 153.18
 ---- batch: 040 ----
mean loss: 146.39
 ---- batch: 050 ----
mean loss: 152.20
 ---- batch: 060 ----
mean loss: 149.97
 ---- batch: 070 ----
mean loss: 150.28
 ---- batch: 080 ----
mean loss: 153.35
 ---- batch: 090 ----
mean loss: 150.34
 ---- batch: 100 ----
mean loss: 146.33
 ---- batch: 110 ----
mean loss: 145.52
train mean loss: 149.08
epoch train time: 0:00:00.568099
elapsed time: 0:01:52.561705
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:05:46.673862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.47
 ---- batch: 020 ----
mean loss: 150.75
 ---- batch: 030 ----
mean loss: 142.72
 ---- batch: 040 ----
mean loss: 146.68
 ---- batch: 050 ----
mean loss: 147.08
 ---- batch: 060 ----
mean loss: 148.67
 ---- batch: 070 ----
mean loss: 150.04
 ---- batch: 080 ----
mean loss: 148.80
 ---- batch: 090 ----
mean loss: 148.83
 ---- batch: 100 ----
mean loss: 144.66
 ---- batch: 110 ----
mean loss: 151.89
train mean loss: 148.51
epoch train time: 0:00:00.557494
elapsed time: 0:01:53.119357
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:05:47.231524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.24
 ---- batch: 020 ----
mean loss: 151.08
 ---- batch: 030 ----
mean loss: 145.76
 ---- batch: 040 ----
mean loss: 147.17
 ---- batch: 050 ----
mean loss: 152.10
 ---- batch: 060 ----
mean loss: 146.48
 ---- batch: 070 ----
mean loss: 147.67
 ---- batch: 080 ----
mean loss: 151.49
 ---- batch: 090 ----
mean loss: 153.55
 ---- batch: 100 ----
mean loss: 156.68
 ---- batch: 110 ----
mean loss: 142.84
train mean loss: 149.00
epoch train time: 0:00:00.554950
elapsed time: 0:01:53.674456
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:05:47.786609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.76
 ---- batch: 020 ----
mean loss: 151.73
 ---- batch: 030 ----
mean loss: 152.32
 ---- batch: 040 ----
mean loss: 145.77
 ---- batch: 050 ----
mean loss: 144.84
 ---- batch: 060 ----
mean loss: 139.86
 ---- batch: 070 ----
mean loss: 150.76
 ---- batch: 080 ----
mean loss: 155.31
 ---- batch: 090 ----
mean loss: 152.11
 ---- batch: 100 ----
mean loss: 157.53
 ---- batch: 110 ----
mean loss: 149.94
train mean loss: 149.11
epoch train time: 0:00:00.565856
elapsed time: 0:01:54.240459
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:05:48.352622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.58
 ---- batch: 020 ----
mean loss: 144.07
 ---- batch: 030 ----
mean loss: 143.65
 ---- batch: 040 ----
mean loss: 136.24
 ---- batch: 050 ----
mean loss: 141.15
 ---- batch: 060 ----
mean loss: 152.07
 ---- batch: 070 ----
mean loss: 154.35
 ---- batch: 080 ----
mean loss: 152.03
 ---- batch: 090 ----
mean loss: 150.85
 ---- batch: 100 ----
mean loss: 156.58
 ---- batch: 110 ----
mean loss: 145.54
train mean loss: 148.45
epoch train time: 0:00:00.563096
elapsed time: 0:01:54.803700
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:05:48.915852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.45
 ---- batch: 020 ----
mean loss: 139.53
 ---- batch: 030 ----
mean loss: 149.20
 ---- batch: 040 ----
mean loss: 144.62
 ---- batch: 050 ----
mean loss: 149.99
 ---- batch: 060 ----
mean loss: 151.77
 ---- batch: 070 ----
mean loss: 147.81
 ---- batch: 080 ----
mean loss: 156.42
 ---- batch: 090 ----
mean loss: 152.02
 ---- batch: 100 ----
mean loss: 144.80
 ---- batch: 110 ----
mean loss: 148.38
train mean loss: 148.06
epoch train time: 0:00:00.551886
elapsed time: 0:01:55.355749
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:05:49.467904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.94
 ---- batch: 020 ----
mean loss: 143.05
 ---- batch: 030 ----
mean loss: 150.23
 ---- batch: 040 ----
mean loss: 146.10
 ---- batch: 050 ----
mean loss: 141.48
 ---- batch: 060 ----
mean loss: 152.26
 ---- batch: 070 ----
mean loss: 149.87
 ---- batch: 080 ----
mean loss: 149.49
 ---- batch: 090 ----
mean loss: 154.24
 ---- batch: 100 ----
mean loss: 153.30
 ---- batch: 110 ----
mean loss: 150.00
train mean loss: 149.25
epoch train time: 0:00:00.574300
elapsed time: 0:01:55.930197
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:05:50.042348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.81
 ---- batch: 020 ----
mean loss: 143.26
 ---- batch: 030 ----
mean loss: 143.97
 ---- batch: 040 ----
mean loss: 142.26
 ---- batch: 050 ----
mean loss: 146.09
 ---- batch: 060 ----
mean loss: 146.71
 ---- batch: 070 ----
mean loss: 145.98
 ---- batch: 080 ----
mean loss: 152.78
 ---- batch: 090 ----
mean loss: 148.85
 ---- batch: 100 ----
mean loss: 147.74
 ---- batch: 110 ----
mean loss: 146.58
train mean loss: 147.42
epoch train time: 0:00:00.541629
elapsed time: 0:01:56.471951
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:05:50.584100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.70
 ---- batch: 020 ----
mean loss: 132.83
 ---- batch: 030 ----
mean loss: 145.90
 ---- batch: 040 ----
mean loss: 148.01
 ---- batch: 050 ----
mean loss: 153.22
 ---- batch: 060 ----
mean loss: 155.48
 ---- batch: 070 ----
mean loss: 152.72
 ---- batch: 080 ----
mean loss: 145.81
 ---- batch: 090 ----
mean loss: 142.15
 ---- batch: 100 ----
mean loss: 145.90
 ---- batch: 110 ----
mean loss: 148.10
train mean loss: 147.37
epoch train time: 0:00:00.549231
elapsed time: 0:01:57.021304
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:05:51.133457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.51
 ---- batch: 020 ----
mean loss: 142.90
 ---- batch: 030 ----
mean loss: 145.02
 ---- batch: 040 ----
mean loss: 145.81
 ---- batch: 050 ----
mean loss: 143.97
 ---- batch: 060 ----
mean loss: 149.05
 ---- batch: 070 ----
mean loss: 144.97
 ---- batch: 080 ----
mean loss: 141.14
 ---- batch: 090 ----
mean loss: 151.27
 ---- batch: 100 ----
mean loss: 146.93
 ---- batch: 110 ----
mean loss: 152.40
train mean loss: 146.34
epoch train time: 0:00:00.550973
elapsed time: 0:01:57.572408
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:05:51.684559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.73
 ---- batch: 020 ----
mean loss: 143.51
 ---- batch: 030 ----
mean loss: 147.72
 ---- batch: 040 ----
mean loss: 144.88
 ---- batch: 050 ----
mean loss: 142.21
 ---- batch: 060 ----
mean loss: 142.59
 ---- batch: 070 ----
mean loss: 156.61
 ---- batch: 080 ----
mean loss: 146.83
 ---- batch: 090 ----
mean loss: 150.52
 ---- batch: 100 ----
mean loss: 145.88
 ---- batch: 110 ----
mean loss: 152.94
train mean loss: 147.17
epoch train time: 0:00:00.554982
elapsed time: 0:01:58.127527
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:05:52.239700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.88
 ---- batch: 020 ----
mean loss: 141.29
 ---- batch: 030 ----
mean loss: 151.85
 ---- batch: 040 ----
mean loss: 157.50
 ---- batch: 050 ----
mean loss: 145.77
 ---- batch: 060 ----
mean loss: 146.38
 ---- batch: 070 ----
mean loss: 149.67
 ---- batch: 080 ----
mean loss: 145.30
 ---- batch: 090 ----
mean loss: 141.49
 ---- batch: 100 ----
mean loss: 148.03
 ---- batch: 110 ----
mean loss: 146.40
train mean loss: 147.13
epoch train time: 0:00:00.568106
elapsed time: 0:01:58.695785
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:05:52.807944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.34
 ---- batch: 020 ----
mean loss: 141.84
 ---- batch: 030 ----
mean loss: 147.66
 ---- batch: 040 ----
mean loss: 143.07
 ---- batch: 050 ----
mean loss: 139.33
 ---- batch: 060 ----
mean loss: 154.37
 ---- batch: 070 ----
mean loss: 138.01
 ---- batch: 080 ----
mean loss: 145.70
 ---- batch: 090 ----
mean loss: 153.36
 ---- batch: 100 ----
mean loss: 147.30
 ---- batch: 110 ----
mean loss: 152.42
train mean loss: 145.86
epoch train time: 0:00:00.558839
elapsed time: 0:01:59.254783
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:05:53.366935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.14
 ---- batch: 020 ----
mean loss: 141.95
 ---- batch: 030 ----
mean loss: 147.58
 ---- batch: 040 ----
mean loss: 143.11
 ---- batch: 050 ----
mean loss: 144.81
 ---- batch: 060 ----
mean loss: 139.87
 ---- batch: 070 ----
mean loss: 145.31
 ---- batch: 080 ----
mean loss: 145.06
 ---- batch: 090 ----
mean loss: 139.63
 ---- batch: 100 ----
mean loss: 151.12
 ---- batch: 110 ----
mean loss: 152.75
train mean loss: 145.77
epoch train time: 0:00:00.552525
elapsed time: 0:01:59.807444
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:05:53.919599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.53
 ---- batch: 020 ----
mean loss: 145.65
 ---- batch: 030 ----
mean loss: 140.41
 ---- batch: 040 ----
mean loss: 153.50
 ---- batch: 050 ----
mean loss: 146.67
 ---- batch: 060 ----
mean loss: 137.87
 ---- batch: 070 ----
mean loss: 145.35
 ---- batch: 080 ----
mean loss: 145.59
 ---- batch: 090 ----
mean loss: 144.59
 ---- batch: 100 ----
mean loss: 156.81
 ---- batch: 110 ----
mean loss: 145.72
train mean loss: 146.76
epoch train time: 0:00:00.559934
elapsed time: 0:02:00.367512
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:05:54.479664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.57
 ---- batch: 020 ----
mean loss: 145.76
 ---- batch: 030 ----
mean loss: 139.12
 ---- batch: 040 ----
mean loss: 149.65
 ---- batch: 050 ----
mean loss: 154.89
 ---- batch: 060 ----
mean loss: 140.70
 ---- batch: 070 ----
mean loss: 139.66
 ---- batch: 080 ----
mean loss: 153.53
 ---- batch: 090 ----
mean loss: 151.69
 ---- batch: 100 ----
mean loss: 139.67
 ---- batch: 110 ----
mean loss: 138.78
train mean loss: 145.24
epoch train time: 0:00:00.556029
elapsed time: 0:02:00.923675
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:05:55.035827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.47
 ---- batch: 020 ----
mean loss: 144.18
 ---- batch: 030 ----
mean loss: 141.40
 ---- batch: 040 ----
mean loss: 148.20
 ---- batch: 050 ----
mean loss: 156.52
 ---- batch: 060 ----
mean loss: 146.69
 ---- batch: 070 ----
mean loss: 142.31
 ---- batch: 080 ----
mean loss: 142.52
 ---- batch: 090 ----
mean loss: 142.53
 ---- batch: 100 ----
mean loss: 149.53
 ---- batch: 110 ----
mean loss: 146.98
train mean loss: 145.39
epoch train time: 0:00:00.559406
elapsed time: 0:02:01.483216
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:05:55.595371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.76
 ---- batch: 020 ----
mean loss: 147.86
 ---- batch: 030 ----
mean loss: 143.37
 ---- batch: 040 ----
mean loss: 137.97
 ---- batch: 050 ----
mean loss: 152.67
 ---- batch: 060 ----
mean loss: 145.78
 ---- batch: 070 ----
mean loss: 140.55
 ---- batch: 080 ----
mean loss: 130.32
 ---- batch: 090 ----
mean loss: 143.83
 ---- batch: 100 ----
mean loss: 149.84
 ---- batch: 110 ----
mean loss: 149.36
train mean loss: 144.47
epoch train time: 0:00:00.560810
elapsed time: 0:02:02.044163
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:05:56.156374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.05
 ---- batch: 020 ----
mean loss: 146.77
 ---- batch: 030 ----
mean loss: 151.56
 ---- batch: 040 ----
mean loss: 148.66
 ---- batch: 050 ----
mean loss: 142.25
 ---- batch: 060 ----
mean loss: 146.54
 ---- batch: 070 ----
mean loss: 141.52
 ---- batch: 080 ----
mean loss: 144.99
 ---- batch: 090 ----
mean loss: 144.16
 ---- batch: 100 ----
mean loss: 139.51
 ---- batch: 110 ----
mean loss: 145.53
train mean loss: 145.25
epoch train time: 0:00:00.558134
elapsed time: 0:02:02.602498
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:05:56.714654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.93
 ---- batch: 020 ----
mean loss: 146.68
 ---- batch: 030 ----
mean loss: 142.65
 ---- batch: 040 ----
mean loss: 146.21
 ---- batch: 050 ----
mean loss: 146.46
 ---- batch: 060 ----
mean loss: 140.46
 ---- batch: 070 ----
mean loss: 150.27
 ---- batch: 080 ----
mean loss: 146.58
 ---- batch: 090 ----
mean loss: 136.34
 ---- batch: 100 ----
mean loss: 147.11
 ---- batch: 110 ----
mean loss: 144.66
train mean loss: 144.71
epoch train time: 0:00:00.552981
elapsed time: 0:02:03.155621
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:05:57.267776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.71
 ---- batch: 020 ----
mean loss: 137.87
 ---- batch: 030 ----
mean loss: 146.73
 ---- batch: 040 ----
mean loss: 141.58
 ---- batch: 050 ----
mean loss: 155.70
 ---- batch: 060 ----
mean loss: 140.40
 ---- batch: 070 ----
mean loss: 148.11
 ---- batch: 080 ----
mean loss: 149.06
 ---- batch: 090 ----
mean loss: 151.74
 ---- batch: 100 ----
mean loss: 139.82
 ---- batch: 110 ----
mean loss: 134.68
train mean loss: 144.82
epoch train time: 0:00:00.552729
elapsed time: 0:02:03.708484
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:05:57.820668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.48
 ---- batch: 020 ----
mean loss: 147.53
 ---- batch: 030 ----
mean loss: 137.45
 ---- batch: 040 ----
mean loss: 143.51
 ---- batch: 050 ----
mean loss: 142.28
 ---- batch: 060 ----
mean loss: 145.22
 ---- batch: 070 ----
mean loss: 134.35
 ---- batch: 080 ----
mean loss: 155.03
 ---- batch: 090 ----
mean loss: 145.37
 ---- batch: 100 ----
mean loss: 158.07
 ---- batch: 110 ----
mean loss: 141.98
train mean loss: 144.79
epoch train time: 0:00:00.563176
elapsed time: 0:02:04.271827
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:05:58.383982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.27
 ---- batch: 020 ----
mean loss: 141.31
 ---- batch: 030 ----
mean loss: 140.07
 ---- batch: 040 ----
mean loss: 138.99
 ---- batch: 050 ----
mean loss: 144.78
 ---- batch: 060 ----
mean loss: 140.78
 ---- batch: 070 ----
mean loss: 141.16
 ---- batch: 080 ----
mean loss: 140.69
 ---- batch: 090 ----
mean loss: 153.44
 ---- batch: 100 ----
mean loss: 148.74
 ---- batch: 110 ----
mean loss: 148.49
train mean loss: 143.94
epoch train time: 0:00:00.566697
elapsed time: 0:02:04.838668
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:05:58.950824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.61
 ---- batch: 020 ----
mean loss: 145.30
 ---- batch: 030 ----
mean loss: 139.23
 ---- batch: 040 ----
mean loss: 152.81
 ---- batch: 050 ----
mean loss: 152.01
 ---- batch: 060 ----
mean loss: 137.63
 ---- batch: 070 ----
mean loss: 138.55
 ---- batch: 080 ----
mean loss: 147.53
 ---- batch: 090 ----
mean loss: 144.92
 ---- batch: 100 ----
mean loss: 140.45
 ---- batch: 110 ----
mean loss: 144.07
train mean loss: 143.79
epoch train time: 0:00:00.564122
elapsed time: 0:02:05.402932
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:05:59.515106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.54
 ---- batch: 020 ----
mean loss: 142.58
 ---- batch: 030 ----
mean loss: 133.82
 ---- batch: 040 ----
mean loss: 151.28
 ---- batch: 050 ----
mean loss: 146.62
 ---- batch: 060 ----
mean loss: 143.33
 ---- batch: 070 ----
mean loss: 144.51
 ---- batch: 080 ----
mean loss: 148.71
 ---- batch: 090 ----
mean loss: 144.46
 ---- batch: 100 ----
mean loss: 148.73
 ---- batch: 110 ----
mean loss: 139.03
train mean loss: 143.64
epoch train time: 0:00:00.565051
elapsed time: 0:02:05.968135
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:06:00.080288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.19
 ---- batch: 020 ----
mean loss: 145.07
 ---- batch: 030 ----
mean loss: 142.09
 ---- batch: 040 ----
mean loss: 144.53
 ---- batch: 050 ----
mean loss: 141.61
 ---- batch: 060 ----
mean loss: 145.94
 ---- batch: 070 ----
mean loss: 139.29
 ---- batch: 080 ----
mean loss: 147.65
 ---- batch: 090 ----
mean loss: 151.49
 ---- batch: 100 ----
mean loss: 135.87
 ---- batch: 110 ----
mean loss: 143.01
train mean loss: 144.16
epoch train time: 0:00:00.561449
elapsed time: 0:02:06.529764
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:06:00.641944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.56
 ---- batch: 020 ----
mean loss: 146.46
 ---- batch: 030 ----
mean loss: 151.41
 ---- batch: 040 ----
mean loss: 139.67
 ---- batch: 050 ----
mean loss: 136.09
 ---- batch: 060 ----
mean loss: 137.94
 ---- batch: 070 ----
mean loss: 151.32
 ---- batch: 080 ----
mean loss: 140.35
 ---- batch: 090 ----
mean loss: 145.13
 ---- batch: 100 ----
mean loss: 147.01
 ---- batch: 110 ----
mean loss: 144.14
train mean loss: 143.16
epoch train time: 0:00:00.553818
elapsed time: 0:02:07.083762
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:06:01.195926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.82
 ---- batch: 020 ----
mean loss: 145.50
 ---- batch: 030 ----
mean loss: 143.37
 ---- batch: 040 ----
mean loss: 138.67
 ---- batch: 050 ----
mean loss: 141.31
 ---- batch: 060 ----
mean loss: 143.63
 ---- batch: 070 ----
mean loss: 139.04
 ---- batch: 080 ----
mean loss: 143.30
 ---- batch: 090 ----
mean loss: 147.87
 ---- batch: 100 ----
mean loss: 146.71
 ---- batch: 110 ----
mean loss: 149.31
train mean loss: 142.95
epoch train time: 0:00:00.548928
elapsed time: 0:02:07.632855
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:06:01.745011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.21
 ---- batch: 020 ----
mean loss: 137.94
 ---- batch: 030 ----
mean loss: 143.94
 ---- batch: 040 ----
mean loss: 142.04
 ---- batch: 050 ----
mean loss: 134.73
 ---- batch: 060 ----
mean loss: 139.73
 ---- batch: 070 ----
mean loss: 143.59
 ---- batch: 080 ----
mean loss: 134.59
 ---- batch: 090 ----
mean loss: 149.05
 ---- batch: 100 ----
mean loss: 137.72
 ---- batch: 110 ----
mean loss: 154.28
train mean loss: 142.06
epoch train time: 0:00:00.554375
elapsed time: 0:02:08.187378
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:06:02.299543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.62
 ---- batch: 020 ----
mean loss: 145.54
 ---- batch: 030 ----
mean loss: 140.47
 ---- batch: 040 ----
mean loss: 147.43
 ---- batch: 050 ----
mean loss: 144.83
 ---- batch: 060 ----
mean loss: 136.98
 ---- batch: 070 ----
mean loss: 140.35
 ---- batch: 080 ----
mean loss: 138.78
 ---- batch: 090 ----
mean loss: 146.44
 ---- batch: 100 ----
mean loss: 144.75
 ---- batch: 110 ----
mean loss: 146.76
train mean loss: 142.64
epoch train time: 0:00:00.549364
elapsed time: 0:02:08.736904
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:06:02.849056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.05
 ---- batch: 020 ----
mean loss: 140.76
 ---- batch: 030 ----
mean loss: 140.30
 ---- batch: 040 ----
mean loss: 140.97
 ---- batch: 050 ----
mean loss: 139.16
 ---- batch: 060 ----
mean loss: 144.42
 ---- batch: 070 ----
mean loss: 148.61
 ---- batch: 080 ----
mean loss: 142.37
 ---- batch: 090 ----
mean loss: 138.87
 ---- batch: 100 ----
mean loss: 144.23
 ---- batch: 110 ----
mean loss: 140.96
train mean loss: 142.56
epoch train time: 0:00:00.540867
elapsed time: 0:02:09.277902
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:06:03.390115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.39
 ---- batch: 020 ----
mean loss: 145.15
 ---- batch: 030 ----
mean loss: 135.71
 ---- batch: 040 ----
mean loss: 150.59
 ---- batch: 050 ----
mean loss: 147.06
 ---- batch: 060 ----
mean loss: 140.50
 ---- batch: 070 ----
mean loss: 141.61
 ---- batch: 080 ----
mean loss: 146.31
 ---- batch: 090 ----
mean loss: 138.40
 ---- batch: 100 ----
mean loss: 141.44
 ---- batch: 110 ----
mean loss: 139.85
train mean loss: 142.39
epoch train time: 0:00:00.566122
elapsed time: 0:02:09.844219
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:06:03.956370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.58
 ---- batch: 020 ----
mean loss: 143.16
 ---- batch: 030 ----
mean loss: 144.07
 ---- batch: 040 ----
mean loss: 139.36
 ---- batch: 050 ----
mean loss: 143.34
 ---- batch: 060 ----
mean loss: 143.14
 ---- batch: 070 ----
mean loss: 133.85
 ---- batch: 080 ----
mean loss: 142.43
 ---- batch: 090 ----
mean loss: 144.70
 ---- batch: 100 ----
mean loss: 146.89
 ---- batch: 110 ----
mean loss: 137.98
train mean loss: 142.13
epoch train time: 0:00:00.546955
elapsed time: 0:02:10.391321
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:06:04.503512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.27
 ---- batch: 020 ----
mean loss: 145.08
 ---- batch: 030 ----
mean loss: 148.43
 ---- batch: 040 ----
mean loss: 137.43
 ---- batch: 050 ----
mean loss: 137.05
 ---- batch: 060 ----
mean loss: 144.67
 ---- batch: 070 ----
mean loss: 138.85
 ---- batch: 080 ----
mean loss: 139.95
 ---- batch: 090 ----
mean loss: 144.76
 ---- batch: 100 ----
mean loss: 138.13
 ---- batch: 110 ----
mean loss: 145.47
train mean loss: 141.74
epoch train time: 0:00:00.554205
elapsed time: 0:02:10.945699
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:06:05.057852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.69
 ---- batch: 020 ----
mean loss: 137.80
 ---- batch: 030 ----
mean loss: 136.13
 ---- batch: 040 ----
mean loss: 144.72
 ---- batch: 050 ----
mean loss: 140.67
 ---- batch: 060 ----
mean loss: 133.86
 ---- batch: 070 ----
mean loss: 149.28
 ---- batch: 080 ----
mean loss: 145.80
 ---- batch: 090 ----
mean loss: 138.54
 ---- batch: 100 ----
mean loss: 143.81
 ---- batch: 110 ----
mean loss: 151.00
train mean loss: 141.81
epoch train time: 0:00:00.543958
elapsed time: 0:02:11.489791
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:06:05.601950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.13
 ---- batch: 020 ----
mean loss: 144.35
 ---- batch: 030 ----
mean loss: 136.07
 ---- batch: 040 ----
mean loss: 136.74
 ---- batch: 050 ----
mean loss: 146.94
 ---- batch: 060 ----
mean loss: 141.57
 ---- batch: 070 ----
mean loss: 144.25
 ---- batch: 080 ----
mean loss: 141.44
 ---- batch: 090 ----
mean loss: 136.86
 ---- batch: 100 ----
mean loss: 148.47
 ---- batch: 110 ----
mean loss: 141.80
train mean loss: 140.98
epoch train time: 0:00:00.560360
elapsed time: 0:02:12.050288
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:06:06.162442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.52
 ---- batch: 020 ----
mean loss: 140.38
 ---- batch: 030 ----
mean loss: 147.73
 ---- batch: 040 ----
mean loss: 130.37
 ---- batch: 050 ----
mean loss: 143.16
 ---- batch: 060 ----
mean loss: 132.32
 ---- batch: 070 ----
mean loss: 140.84
 ---- batch: 080 ----
mean loss: 143.69
 ---- batch: 090 ----
mean loss: 148.23
 ---- batch: 100 ----
mean loss: 146.53
 ---- batch: 110 ----
mean loss: 143.29
train mean loss: 141.08
epoch train time: 0:00:00.544072
elapsed time: 0:02:12.594492
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:06:06.706670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.18
 ---- batch: 020 ----
mean loss: 146.12
 ---- batch: 030 ----
mean loss: 146.56
 ---- batch: 040 ----
mean loss: 143.55
 ---- batch: 050 ----
mean loss: 143.03
 ---- batch: 060 ----
mean loss: 143.11
 ---- batch: 070 ----
mean loss: 143.03
 ---- batch: 080 ----
mean loss: 138.74
 ---- batch: 090 ----
mean loss: 142.13
 ---- batch: 100 ----
mean loss: 142.07
 ---- batch: 110 ----
mean loss: 140.20
train mean loss: 143.05
epoch train time: 0:00:00.541636
elapsed time: 0:02:13.136285
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:06:07.248439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.95
 ---- batch: 020 ----
mean loss: 140.75
 ---- batch: 030 ----
mean loss: 129.47
 ---- batch: 040 ----
mean loss: 139.25
 ---- batch: 050 ----
mean loss: 134.88
 ---- batch: 060 ----
mean loss: 133.38
 ---- batch: 070 ----
mean loss: 145.39
 ---- batch: 080 ----
mean loss: 143.12
 ---- batch: 090 ----
mean loss: 148.53
 ---- batch: 100 ----
mean loss: 143.18
 ---- batch: 110 ----
mean loss: 146.54
train mean loss: 140.70
epoch train time: 0:00:00.563314
elapsed time: 0:02:13.699735
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:06:07.811890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.36
 ---- batch: 020 ----
mean loss: 144.58
 ---- batch: 030 ----
mean loss: 143.71
 ---- batch: 040 ----
mean loss: 139.37
 ---- batch: 050 ----
mean loss: 144.21
 ---- batch: 060 ----
mean loss: 141.44
 ---- batch: 070 ----
mean loss: 139.15
 ---- batch: 080 ----
mean loss: 139.78
 ---- batch: 090 ----
mean loss: 132.46
 ---- batch: 100 ----
mean loss: 143.81
 ---- batch: 110 ----
mean loss: 150.03
train mean loss: 141.40
epoch train time: 0:00:00.559228
elapsed time: 0:02:14.259105
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:06:08.371258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.59
 ---- batch: 020 ----
mean loss: 138.91
 ---- batch: 030 ----
mean loss: 141.18
 ---- batch: 040 ----
mean loss: 140.34
 ---- batch: 050 ----
mean loss: 143.85
 ---- batch: 060 ----
mean loss: 143.13
 ---- batch: 070 ----
mean loss: 142.87
 ---- batch: 080 ----
mean loss: 136.67
 ---- batch: 090 ----
mean loss: 140.13
 ---- batch: 100 ----
mean loss: 144.18
 ---- batch: 110 ----
mean loss: 146.26
train mean loss: 141.20
epoch train time: 0:00:00.568466
elapsed time: 0:02:14.827707
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:06:08.939860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.88
 ---- batch: 020 ----
mean loss: 147.85
 ---- batch: 030 ----
mean loss: 150.35
 ---- batch: 040 ----
mean loss: 138.12
 ---- batch: 050 ----
mean loss: 140.69
 ---- batch: 060 ----
mean loss: 140.62
 ---- batch: 070 ----
mean loss: 140.26
 ---- batch: 080 ----
mean loss: 139.71
 ---- batch: 090 ----
mean loss: 137.74
 ---- batch: 100 ----
mean loss: 137.42
 ---- batch: 110 ----
mean loss: 140.59
train mean loss: 140.87
epoch train time: 0:00:00.550091
elapsed time: 0:02:15.377930
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:06:09.490082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.54
 ---- batch: 020 ----
mean loss: 137.51
 ---- batch: 030 ----
mean loss: 133.49
 ---- batch: 040 ----
mean loss: 142.92
 ---- batch: 050 ----
mean loss: 145.10
 ---- batch: 060 ----
mean loss: 151.99
 ---- batch: 070 ----
mean loss: 133.40
 ---- batch: 080 ----
mean loss: 139.12
 ---- batch: 090 ----
mean loss: 144.00
 ---- batch: 100 ----
mean loss: 145.75
 ---- batch: 110 ----
mean loss: 145.38
train mean loss: 141.30
epoch train time: 0:00:00.554076
elapsed time: 0:02:15.932138
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:06:10.044291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.05
 ---- batch: 020 ----
mean loss: 139.53
 ---- batch: 030 ----
mean loss: 147.67
 ---- batch: 040 ----
mean loss: 133.42
 ---- batch: 050 ----
mean loss: 143.71
 ---- batch: 060 ----
mean loss: 137.37
 ---- batch: 070 ----
mean loss: 139.90
 ---- batch: 080 ----
mean loss: 138.71
 ---- batch: 090 ----
mean loss: 145.18
 ---- batch: 100 ----
mean loss: 140.09
 ---- batch: 110 ----
mean loss: 136.70
train mean loss: 139.97
epoch train time: 0:00:00.543979
elapsed time: 0:02:16.476249
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:06:10.588427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.61
 ---- batch: 020 ----
mean loss: 139.80
 ---- batch: 030 ----
mean loss: 154.16
 ---- batch: 040 ----
mean loss: 140.32
 ---- batch: 050 ----
mean loss: 134.73
 ---- batch: 060 ----
mean loss: 142.27
 ---- batch: 070 ----
mean loss: 133.59
 ---- batch: 080 ----
mean loss: 145.64
 ---- batch: 090 ----
mean loss: 139.54
 ---- batch: 100 ----
mean loss: 134.09
 ---- batch: 110 ----
mean loss: 137.84
train mean loss: 140.12
epoch train time: 0:00:00.557570
elapsed time: 0:02:17.033978
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:06:11.146133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.41
 ---- batch: 020 ----
mean loss: 129.80
 ---- batch: 030 ----
mean loss: 141.06
 ---- batch: 040 ----
mean loss: 136.05
 ---- batch: 050 ----
mean loss: 134.09
 ---- batch: 060 ----
mean loss: 146.52
 ---- batch: 070 ----
mean loss: 151.62
 ---- batch: 080 ----
mean loss: 140.13
 ---- batch: 090 ----
mean loss: 138.21
 ---- batch: 100 ----
mean loss: 143.94
 ---- batch: 110 ----
mean loss: 146.90
train mean loss: 139.87
epoch train time: 0:00:00.547373
elapsed time: 0:02:17.581489
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:06:11.693686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.99
 ---- batch: 020 ----
mean loss: 132.98
 ---- batch: 030 ----
mean loss: 136.60
 ---- batch: 040 ----
mean loss: 138.98
 ---- batch: 050 ----
mean loss: 141.60
 ---- batch: 060 ----
mean loss: 144.86
 ---- batch: 070 ----
mean loss: 136.78
 ---- batch: 080 ----
mean loss: 136.30
 ---- batch: 090 ----
mean loss: 142.59
 ---- batch: 100 ----
mean loss: 142.10
 ---- batch: 110 ----
mean loss: 142.13
train mean loss: 139.23
epoch train time: 0:00:00.555345
elapsed time: 0:02:18.137026
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:06:12.249179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.68
 ---- batch: 020 ----
mean loss: 141.26
 ---- batch: 030 ----
mean loss: 142.10
 ---- batch: 040 ----
mean loss: 137.28
 ---- batch: 050 ----
mean loss: 145.76
 ---- batch: 060 ----
mean loss: 143.59
 ---- batch: 070 ----
mean loss: 132.97
 ---- batch: 080 ----
mean loss: 138.45
 ---- batch: 090 ----
mean loss: 137.67
 ---- batch: 100 ----
mean loss: 139.81
 ---- batch: 110 ----
mean loss: 138.44
train mean loss: 140.10
epoch train time: 0:00:00.547647
elapsed time: 0:02:18.684808
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:06:12.796971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.62
 ---- batch: 020 ----
mean loss: 137.72
 ---- batch: 030 ----
mean loss: 137.03
 ---- batch: 040 ----
mean loss: 143.35
 ---- batch: 050 ----
mean loss: 140.68
 ---- batch: 060 ----
mean loss: 135.27
 ---- batch: 070 ----
mean loss: 136.14
 ---- batch: 080 ----
mean loss: 140.14
 ---- batch: 090 ----
mean loss: 133.70
 ---- batch: 100 ----
mean loss: 148.36
 ---- batch: 110 ----
mean loss: 148.79
train mean loss: 139.92
epoch train time: 0:00:00.545728
elapsed time: 0:02:19.230680
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:06:13.342847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.85
 ---- batch: 020 ----
mean loss: 149.79
 ---- batch: 030 ----
mean loss: 138.88
 ---- batch: 040 ----
mean loss: 138.79
 ---- batch: 050 ----
mean loss: 134.25
 ---- batch: 060 ----
mean loss: 131.59
 ---- batch: 070 ----
mean loss: 144.88
 ---- batch: 080 ----
mean loss: 137.69
 ---- batch: 090 ----
mean loss: 140.40
 ---- batch: 100 ----
mean loss: 142.51
 ---- batch: 110 ----
mean loss: 135.97
train mean loss: 139.22
epoch train time: 0:00:00.547458
elapsed time: 0:02:19.778285
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:06:13.890463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.69
 ---- batch: 020 ----
mean loss: 148.73
 ---- batch: 030 ----
mean loss: 143.56
 ---- batch: 040 ----
mean loss: 137.80
 ---- batch: 050 ----
mean loss: 130.78
 ---- batch: 060 ----
mean loss: 135.30
 ---- batch: 070 ----
mean loss: 141.30
 ---- batch: 080 ----
mean loss: 137.21
 ---- batch: 090 ----
mean loss: 144.38
 ---- batch: 100 ----
mean loss: 138.35
 ---- batch: 110 ----
mean loss: 138.38
train mean loss: 139.12
epoch train time: 0:00:00.543288
elapsed time: 0:02:20.321734
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:06:14.433903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.82
 ---- batch: 020 ----
mean loss: 134.97
 ---- batch: 030 ----
mean loss: 136.13
 ---- batch: 040 ----
mean loss: 147.59
 ---- batch: 050 ----
mean loss: 131.49
 ---- batch: 060 ----
mean loss: 134.17
 ---- batch: 070 ----
mean loss: 142.34
 ---- batch: 080 ----
mean loss: 141.16
 ---- batch: 090 ----
mean loss: 140.38
 ---- batch: 100 ----
mean loss: 132.93
 ---- batch: 110 ----
mean loss: 145.65
train mean loss: 138.97
epoch train time: 0:00:00.555091
elapsed time: 0:02:20.876987
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:06:14.989169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.71
 ---- batch: 020 ----
mean loss: 136.12
 ---- batch: 030 ----
mean loss: 139.28
 ---- batch: 040 ----
mean loss: 130.33
 ---- batch: 050 ----
mean loss: 141.76
 ---- batch: 060 ----
mean loss: 130.23
 ---- batch: 070 ----
mean loss: 149.85
 ---- batch: 080 ----
mean loss: 146.66
 ---- batch: 090 ----
mean loss: 141.16
 ---- batch: 100 ----
mean loss: 143.02
 ---- batch: 110 ----
mean loss: 135.97
train mean loss: 139.24
epoch train time: 0:00:00.553577
elapsed time: 0:02:21.430726
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:06:15.542879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.49
 ---- batch: 020 ----
mean loss: 133.51
 ---- batch: 030 ----
mean loss: 136.86
 ---- batch: 040 ----
mean loss: 137.42
 ---- batch: 050 ----
mean loss: 137.60
 ---- batch: 060 ----
mean loss: 144.68
 ---- batch: 070 ----
mean loss: 132.05
 ---- batch: 080 ----
mean loss: 141.61
 ---- batch: 090 ----
mean loss: 137.93
 ---- batch: 100 ----
mean loss: 137.25
 ---- batch: 110 ----
mean loss: 143.09
train mean loss: 138.80
epoch train time: 0:00:00.548306
elapsed time: 0:02:21.979165
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:06:16.091321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.43
 ---- batch: 020 ----
mean loss: 133.49
 ---- batch: 030 ----
mean loss: 144.44
 ---- batch: 040 ----
mean loss: 129.26
 ---- batch: 050 ----
mean loss: 137.18
 ---- batch: 060 ----
mean loss: 148.05
 ---- batch: 070 ----
mean loss: 143.75
 ---- batch: 080 ----
mean loss: 140.99
 ---- batch: 090 ----
mean loss: 135.30
 ---- batch: 100 ----
mean loss: 136.77
 ---- batch: 110 ----
mean loss: 142.07
train mean loss: 139.19
epoch train time: 0:00:00.544070
elapsed time: 0:02:22.523379
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:06:16.635534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.28
 ---- batch: 020 ----
mean loss: 131.64
 ---- batch: 030 ----
mean loss: 134.79
 ---- batch: 040 ----
mean loss: 133.34
 ---- batch: 050 ----
mean loss: 136.48
 ---- batch: 060 ----
mean loss: 143.51
 ---- batch: 070 ----
mean loss: 137.19
 ---- batch: 080 ----
mean loss: 134.66
 ---- batch: 090 ----
mean loss: 145.65
 ---- batch: 100 ----
mean loss: 139.32
 ---- batch: 110 ----
mean loss: 150.84
train mean loss: 138.85
epoch train time: 0:00:00.551776
elapsed time: 0:02:23.075293
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:06:17.187451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.83
 ---- batch: 020 ----
mean loss: 134.33
 ---- batch: 030 ----
mean loss: 140.91
 ---- batch: 040 ----
mean loss: 130.88
 ---- batch: 050 ----
mean loss: 128.90
 ---- batch: 060 ----
mean loss: 132.89
 ---- batch: 070 ----
mean loss: 125.50
 ---- batch: 080 ----
mean loss: 133.01
 ---- batch: 090 ----
mean loss: 134.53
 ---- batch: 100 ----
mean loss: 134.43
 ---- batch: 110 ----
mean loss: 130.19
train mean loss: 132.59
epoch train time: 0:00:00.555130
elapsed time: 0:02:23.630581
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:06:17.742745
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.59
 ---- batch: 020 ----
mean loss: 128.93
 ---- batch: 030 ----
mean loss: 126.29
 ---- batch: 040 ----
mean loss: 131.92
 ---- batch: 050 ----
mean loss: 131.52
 ---- batch: 060 ----
mean loss: 134.93
 ---- batch: 070 ----
mean loss: 127.86
 ---- batch: 080 ----
mean loss: 138.59
 ---- batch: 090 ----
mean loss: 130.66
 ---- batch: 100 ----
mean loss: 132.39
 ---- batch: 110 ----
mean loss: 127.75
train mean loss: 131.68
epoch train time: 0:00:00.555717
elapsed time: 0:02:24.186444
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:06:18.298597
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.15
 ---- batch: 020 ----
mean loss: 133.21
 ---- batch: 030 ----
mean loss: 134.23
 ---- batch: 040 ----
mean loss: 127.16
 ---- batch: 050 ----
mean loss: 133.99
 ---- batch: 060 ----
mean loss: 135.00
 ---- batch: 070 ----
mean loss: 129.30
 ---- batch: 080 ----
mean loss: 134.70
 ---- batch: 090 ----
mean loss: 133.90
 ---- batch: 100 ----
mean loss: 128.76
 ---- batch: 110 ----
mean loss: 124.93
train mean loss: 131.35
epoch train time: 0:00:00.576393
elapsed time: 0:02:24.762970
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:06:18.875163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.14
 ---- batch: 020 ----
mean loss: 124.90
 ---- batch: 030 ----
mean loss: 128.11
 ---- batch: 040 ----
mean loss: 126.12
 ---- batch: 050 ----
mean loss: 135.51
 ---- batch: 060 ----
mean loss: 134.50
 ---- batch: 070 ----
mean loss: 140.44
 ---- batch: 080 ----
mean loss: 130.78
 ---- batch: 090 ----
mean loss: 137.09
 ---- batch: 100 ----
mean loss: 125.47
 ---- batch: 110 ----
mean loss: 128.95
train mean loss: 131.20
epoch train time: 0:00:00.548829
elapsed time: 0:02:25.311983
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:06:19.424139
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.22
 ---- batch: 020 ----
mean loss: 129.79
 ---- batch: 030 ----
mean loss: 132.50
 ---- batch: 040 ----
mean loss: 129.96
 ---- batch: 050 ----
mean loss: 128.76
 ---- batch: 060 ----
mean loss: 132.21
 ---- batch: 070 ----
mean loss: 133.74
 ---- batch: 080 ----
mean loss: 140.29
 ---- batch: 090 ----
mean loss: 123.78
 ---- batch: 100 ----
mean loss: 127.34
 ---- batch: 110 ----
mean loss: 134.28
train mean loss: 131.36
epoch train time: 0:00:00.552653
elapsed time: 0:02:25.864770
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:06:19.976921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.81
 ---- batch: 020 ----
mean loss: 128.19
 ---- batch: 030 ----
mean loss: 130.41
 ---- batch: 040 ----
mean loss: 135.68
 ---- batch: 050 ----
mean loss: 123.30
 ---- batch: 060 ----
mean loss: 137.93
 ---- batch: 070 ----
mean loss: 133.34
 ---- batch: 080 ----
mean loss: 134.16
 ---- batch: 090 ----
mean loss: 132.44
 ---- batch: 100 ----
mean loss: 121.40
 ---- batch: 110 ----
mean loss: 132.23
train mean loss: 131.14
epoch train time: 0:00:00.545153
elapsed time: 0:02:26.410053
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:06:20.522204
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.25
 ---- batch: 020 ----
mean loss: 133.81
 ---- batch: 030 ----
mean loss: 130.13
 ---- batch: 040 ----
mean loss: 139.64
 ---- batch: 050 ----
mean loss: 126.95
 ---- batch: 060 ----
mean loss: 134.01
 ---- batch: 070 ----
mean loss: 125.66
 ---- batch: 080 ----
mean loss: 135.55
 ---- batch: 090 ----
mean loss: 133.31
 ---- batch: 100 ----
mean loss: 132.21
 ---- batch: 110 ----
mean loss: 128.70
train mean loss: 131.16
epoch train time: 0:00:00.574064
elapsed time: 0:02:26.984247
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:06:21.096401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.64
 ---- batch: 020 ----
mean loss: 128.55
 ---- batch: 030 ----
mean loss: 129.51
 ---- batch: 040 ----
mean loss: 128.00
 ---- batch: 050 ----
mean loss: 133.22
 ---- batch: 060 ----
mean loss: 138.58
 ---- batch: 070 ----
mean loss: 137.61
 ---- batch: 080 ----
mean loss: 133.12
 ---- batch: 090 ----
mean loss: 130.24
 ---- batch: 100 ----
mean loss: 127.33
 ---- batch: 110 ----
mean loss: 130.02
train mean loss: 131.01
epoch train time: 0:00:00.567312
elapsed time: 0:02:27.551707
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:06:21.663883
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.33
 ---- batch: 020 ----
mean loss: 121.08
 ---- batch: 030 ----
mean loss: 134.41
 ---- batch: 040 ----
mean loss: 133.42
 ---- batch: 050 ----
mean loss: 127.73
 ---- batch: 060 ----
mean loss: 134.42
 ---- batch: 070 ----
mean loss: 134.60
 ---- batch: 080 ----
mean loss: 123.27
 ---- batch: 090 ----
mean loss: 132.60
 ---- batch: 100 ----
mean loss: 130.42
 ---- batch: 110 ----
mean loss: 143.25
train mean loss: 130.97
epoch train time: 0:00:00.568703
elapsed time: 0:02:28.120573
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:06:22.232727
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.83
 ---- batch: 020 ----
mean loss: 130.13
 ---- batch: 030 ----
mean loss: 129.54
 ---- batch: 040 ----
mean loss: 133.70
 ---- batch: 050 ----
mean loss: 132.30
 ---- batch: 060 ----
mean loss: 130.13
 ---- batch: 070 ----
mean loss: 128.66
 ---- batch: 080 ----
mean loss: 125.70
 ---- batch: 090 ----
mean loss: 129.93
 ---- batch: 100 ----
mean loss: 126.64
 ---- batch: 110 ----
mean loss: 128.74
train mean loss: 130.97
epoch train time: 0:00:00.553849
elapsed time: 0:02:28.674558
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:06:22.786711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.34
 ---- batch: 020 ----
mean loss: 137.49
 ---- batch: 030 ----
mean loss: 137.26
 ---- batch: 040 ----
mean loss: 133.92
 ---- batch: 050 ----
mean loss: 133.30
 ---- batch: 060 ----
mean loss: 127.29
 ---- batch: 070 ----
mean loss: 127.47
 ---- batch: 080 ----
mean loss: 126.94
 ---- batch: 090 ----
mean loss: 135.01
 ---- batch: 100 ----
mean loss: 120.60
 ---- batch: 110 ----
mean loss: 129.60
train mean loss: 131.00
epoch train time: 0:00:00.553956
elapsed time: 0:02:29.228645
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:06:23.340797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.95
 ---- batch: 020 ----
mean loss: 126.00
 ---- batch: 030 ----
mean loss: 131.89
 ---- batch: 040 ----
mean loss: 132.80
 ---- batch: 050 ----
mean loss: 131.79
 ---- batch: 060 ----
mean loss: 131.52
 ---- batch: 070 ----
mean loss: 131.78
 ---- batch: 080 ----
mean loss: 129.31
 ---- batch: 090 ----
mean loss: 130.20
 ---- batch: 100 ----
mean loss: 138.55
 ---- batch: 110 ----
mean loss: 131.41
train mean loss: 130.92
epoch train time: 0:00:00.553135
elapsed time: 0:02:29.781917
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:06:23.894072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.58
 ---- batch: 020 ----
mean loss: 129.01
 ---- batch: 030 ----
mean loss: 120.54
 ---- batch: 040 ----
mean loss: 134.96
 ---- batch: 050 ----
mean loss: 133.55
 ---- batch: 060 ----
mean loss: 137.47
 ---- batch: 070 ----
mean loss: 124.00
 ---- batch: 080 ----
mean loss: 134.37
 ---- batch: 090 ----
mean loss: 120.68
 ---- batch: 100 ----
mean loss: 135.86
 ---- batch: 110 ----
mean loss: 135.17
train mean loss: 130.86
epoch train time: 0:00:00.553828
elapsed time: 0:02:30.335885
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:06:24.448040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.37
 ---- batch: 020 ----
mean loss: 127.99
 ---- batch: 030 ----
mean loss: 128.51
 ---- batch: 040 ----
mean loss: 125.90
 ---- batch: 050 ----
mean loss: 132.42
 ---- batch: 060 ----
mean loss: 126.66
 ---- batch: 070 ----
mean loss: 139.31
 ---- batch: 080 ----
mean loss: 131.08
 ---- batch: 090 ----
mean loss: 130.31
 ---- batch: 100 ----
mean loss: 132.62
 ---- batch: 110 ----
mean loss: 135.72
train mean loss: 130.88
epoch train time: 0:00:00.569764
elapsed time: 0:02:30.905794
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:06:25.017968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.81
 ---- batch: 020 ----
mean loss: 128.32
 ---- batch: 030 ----
mean loss: 134.14
 ---- batch: 040 ----
mean loss: 134.75
 ---- batch: 050 ----
mean loss: 127.74
 ---- batch: 060 ----
mean loss: 129.48
 ---- batch: 070 ----
mean loss: 136.04
 ---- batch: 080 ----
mean loss: 126.07
 ---- batch: 090 ----
mean loss: 121.81
 ---- batch: 100 ----
mean loss: 131.51
 ---- batch: 110 ----
mean loss: 133.46
train mean loss: 130.91
epoch train time: 0:00:00.562394
elapsed time: 0:02:31.468345
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:06:25.580500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.44
 ---- batch: 020 ----
mean loss: 131.07
 ---- batch: 030 ----
mean loss: 128.88
 ---- batch: 040 ----
mean loss: 130.22
 ---- batch: 050 ----
mean loss: 129.23
 ---- batch: 060 ----
mean loss: 133.03
 ---- batch: 070 ----
mean loss: 126.53
 ---- batch: 080 ----
mean loss: 133.73
 ---- batch: 090 ----
mean loss: 131.18
 ---- batch: 100 ----
mean loss: 128.67
 ---- batch: 110 ----
mean loss: 130.31
train mean loss: 130.80
epoch train time: 0:00:00.559490
elapsed time: 0:02:32.027974
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:06:26.140132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.71
 ---- batch: 020 ----
mean loss: 131.19
 ---- batch: 030 ----
mean loss: 131.97
 ---- batch: 040 ----
mean loss: 129.70
 ---- batch: 050 ----
mean loss: 131.08
 ---- batch: 060 ----
mean loss: 132.98
 ---- batch: 070 ----
mean loss: 129.51
 ---- batch: 080 ----
mean loss: 133.52
 ---- batch: 090 ----
mean loss: 130.30
 ---- batch: 100 ----
mean loss: 134.90
 ---- batch: 110 ----
mean loss: 127.07
train mean loss: 130.90
epoch train time: 0:00:00.560669
elapsed time: 0:02:32.588809
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:06:26.700975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.69
 ---- batch: 020 ----
mean loss: 131.32
 ---- batch: 030 ----
mean loss: 134.08
 ---- batch: 040 ----
mean loss: 128.68
 ---- batch: 050 ----
mean loss: 127.06
 ---- batch: 060 ----
mean loss: 134.46
 ---- batch: 070 ----
mean loss: 128.92
 ---- batch: 080 ----
mean loss: 127.05
 ---- batch: 090 ----
mean loss: 127.55
 ---- batch: 100 ----
mean loss: 139.12
 ---- batch: 110 ----
mean loss: 128.51
train mean loss: 130.79
epoch train time: 0:00:00.561491
elapsed time: 0:02:33.150472
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:06:27.262629
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.74
 ---- batch: 020 ----
mean loss: 128.59
 ---- batch: 030 ----
mean loss: 127.59
 ---- batch: 040 ----
mean loss: 130.19
 ---- batch: 050 ----
mean loss: 129.86
 ---- batch: 060 ----
mean loss: 123.08
 ---- batch: 070 ----
mean loss: 127.86
 ---- batch: 080 ----
mean loss: 137.01
 ---- batch: 090 ----
mean loss: 134.37
 ---- batch: 100 ----
mean loss: 131.64
 ---- batch: 110 ----
mean loss: 135.06
train mean loss: 130.83
epoch train time: 0:00:00.557452
elapsed time: 0:02:33.708060
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:06:27.820212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.86
 ---- batch: 020 ----
mean loss: 131.72
 ---- batch: 030 ----
mean loss: 133.49
 ---- batch: 040 ----
mean loss: 132.50
 ---- batch: 050 ----
mean loss: 136.67
 ---- batch: 060 ----
mean loss: 127.37
 ---- batch: 070 ----
mean loss: 124.11
 ---- batch: 080 ----
mean loss: 127.68
 ---- batch: 090 ----
mean loss: 129.24
 ---- batch: 100 ----
mean loss: 135.93
 ---- batch: 110 ----
mean loss: 134.07
train mean loss: 130.73
epoch train time: 0:00:00.554967
elapsed time: 0:02:34.263181
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:06:28.375340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.57
 ---- batch: 020 ----
mean loss: 128.28
 ---- batch: 030 ----
mean loss: 133.46
 ---- batch: 040 ----
mean loss: 134.45
 ---- batch: 050 ----
mean loss: 127.86
 ---- batch: 060 ----
mean loss: 128.20
 ---- batch: 070 ----
mean loss: 133.17
 ---- batch: 080 ----
mean loss: 136.46
 ---- batch: 090 ----
mean loss: 130.34
 ---- batch: 100 ----
mean loss: 130.85
 ---- batch: 110 ----
mean loss: 132.65
train mean loss: 130.61
epoch train time: 0:00:00.556057
elapsed time: 0:02:34.819420
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:06:28.931611
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.26
 ---- batch: 020 ----
mean loss: 129.30
 ---- batch: 030 ----
mean loss: 140.79
 ---- batch: 040 ----
mean loss: 124.41
 ---- batch: 050 ----
mean loss: 127.74
 ---- batch: 060 ----
mean loss: 131.84
 ---- batch: 070 ----
mean loss: 132.37
 ---- batch: 080 ----
mean loss: 126.69
 ---- batch: 090 ----
mean loss: 128.47
 ---- batch: 100 ----
mean loss: 131.18
 ---- batch: 110 ----
mean loss: 131.46
train mean loss: 130.83
epoch train time: 0:00:00.560675
elapsed time: 0:02:35.380266
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:06:29.492419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.28
 ---- batch: 020 ----
mean loss: 130.85
 ---- batch: 030 ----
mean loss: 125.34
 ---- batch: 040 ----
mean loss: 128.55
 ---- batch: 050 ----
mean loss: 135.29
 ---- batch: 060 ----
mean loss: 135.32
 ---- batch: 070 ----
mean loss: 131.98
 ---- batch: 080 ----
mean loss: 133.43
 ---- batch: 090 ----
mean loss: 132.06
 ---- batch: 100 ----
mean loss: 128.52
 ---- batch: 110 ----
mean loss: 126.47
train mean loss: 130.72
epoch train time: 0:00:00.557478
elapsed time: 0:02:35.937884
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:06:30.050037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.78
 ---- batch: 020 ----
mean loss: 130.08
 ---- batch: 030 ----
mean loss: 129.96
 ---- batch: 040 ----
mean loss: 123.61
 ---- batch: 050 ----
mean loss: 131.11
 ---- batch: 060 ----
mean loss: 136.88
 ---- batch: 070 ----
mean loss: 134.36
 ---- batch: 080 ----
mean loss: 139.40
 ---- batch: 090 ----
mean loss: 134.33
 ---- batch: 100 ----
mean loss: 120.36
 ---- batch: 110 ----
mean loss: 131.67
train mean loss: 130.73
epoch train time: 0:00:00.553052
elapsed time: 0:02:36.491072
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:06:30.603224
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.76
 ---- batch: 020 ----
mean loss: 125.07
 ---- batch: 030 ----
mean loss: 132.13
 ---- batch: 040 ----
mean loss: 134.17
 ---- batch: 050 ----
mean loss: 123.52
 ---- batch: 060 ----
mean loss: 137.00
 ---- batch: 070 ----
mean loss: 126.90
 ---- batch: 080 ----
mean loss: 124.87
 ---- batch: 090 ----
mean loss: 131.94
 ---- batch: 100 ----
mean loss: 137.17
 ---- batch: 110 ----
mean loss: 130.85
train mean loss: 130.69
epoch train time: 0:00:00.555527
elapsed time: 0:02:37.046735
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:06:31.158890
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.11
 ---- batch: 020 ----
mean loss: 140.55
 ---- batch: 030 ----
mean loss: 133.45
 ---- batch: 040 ----
mean loss: 126.25
 ---- batch: 050 ----
mean loss: 127.76
 ---- batch: 060 ----
mean loss: 119.59
 ---- batch: 070 ----
mean loss: 139.78
 ---- batch: 080 ----
mean loss: 124.51
 ---- batch: 090 ----
mean loss: 135.93
 ---- batch: 100 ----
mean loss: 130.73
 ---- batch: 110 ----
mean loss: 132.19
train mean loss: 130.46
epoch train time: 0:00:00.565164
elapsed time: 0:02:37.612053
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:06:31.724223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.81
 ---- batch: 020 ----
mean loss: 130.57
 ---- batch: 030 ----
mean loss: 131.16
 ---- batch: 040 ----
mean loss: 125.21
 ---- batch: 050 ----
mean loss: 127.61
 ---- batch: 060 ----
mean loss: 132.54
 ---- batch: 070 ----
mean loss: 133.93
 ---- batch: 080 ----
mean loss: 137.33
 ---- batch: 090 ----
mean loss: 131.06
 ---- batch: 100 ----
mean loss: 132.62
 ---- batch: 110 ----
mean loss: 131.35
train mean loss: 130.50
epoch train time: 0:00:00.556064
elapsed time: 0:02:38.168286
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:06:32.280487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.98
 ---- batch: 020 ----
mean loss: 127.80
 ---- batch: 030 ----
mean loss: 129.16
 ---- batch: 040 ----
mean loss: 137.37
 ---- batch: 050 ----
mean loss: 133.18
 ---- batch: 060 ----
mean loss: 128.71
 ---- batch: 070 ----
mean loss: 133.54
 ---- batch: 080 ----
mean loss: 132.05
 ---- batch: 090 ----
mean loss: 129.56
 ---- batch: 100 ----
mean loss: 124.17
 ---- batch: 110 ----
mean loss: 129.46
train mean loss: 130.47
epoch train time: 0:00:00.555375
elapsed time: 0:02:38.723846
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:06:32.836006
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.81
 ---- batch: 020 ----
mean loss: 131.73
 ---- batch: 030 ----
mean loss: 139.67
 ---- batch: 040 ----
mean loss: 136.96
 ---- batch: 050 ----
mean loss: 131.79
 ---- batch: 060 ----
mean loss: 133.10
 ---- batch: 070 ----
mean loss: 127.68
 ---- batch: 080 ----
mean loss: 120.15
 ---- batch: 090 ----
mean loss: 130.94
 ---- batch: 100 ----
mean loss: 132.31
 ---- batch: 110 ----
mean loss: 129.29
train mean loss: 130.40
epoch train time: 0:00:00.549213
elapsed time: 0:02:39.273202
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:06:33.385357
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.64
 ---- batch: 020 ----
mean loss: 128.47
 ---- batch: 030 ----
mean loss: 127.43
 ---- batch: 040 ----
mean loss: 141.53
 ---- batch: 050 ----
mean loss: 128.61
 ---- batch: 060 ----
mean loss: 132.12
 ---- batch: 070 ----
mean loss: 122.11
 ---- batch: 080 ----
mean loss: 131.37
 ---- batch: 090 ----
mean loss: 129.92
 ---- batch: 100 ----
mean loss: 127.61
 ---- batch: 110 ----
mean loss: 131.13
train mean loss: 130.54
epoch train time: 0:00:00.558143
elapsed time: 0:02:39.831478
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:06:33.943632
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.63
 ---- batch: 020 ----
mean loss: 117.82
 ---- batch: 030 ----
mean loss: 128.47
 ---- batch: 040 ----
mean loss: 138.06
 ---- batch: 050 ----
mean loss: 133.93
 ---- batch: 060 ----
mean loss: 120.15
 ---- batch: 070 ----
mean loss: 133.77
 ---- batch: 080 ----
mean loss: 135.21
 ---- batch: 090 ----
mean loss: 128.69
 ---- batch: 100 ----
mean loss: 141.10
 ---- batch: 110 ----
mean loss: 137.21
train mean loss: 130.50
epoch train time: 0:00:00.551539
elapsed time: 0:02:40.383154
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:06:34.495310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.55
 ---- batch: 020 ----
mean loss: 127.74
 ---- batch: 030 ----
mean loss: 132.15
 ---- batch: 040 ----
mean loss: 130.80
 ---- batch: 050 ----
mean loss: 120.85
 ---- batch: 060 ----
mean loss: 140.16
 ---- batch: 070 ----
mean loss: 133.94
 ---- batch: 080 ----
mean loss: 130.15
 ---- batch: 090 ----
mean loss: 134.06
 ---- batch: 100 ----
mean loss: 128.30
 ---- batch: 110 ----
mean loss: 127.92
train mean loss: 130.58
epoch train time: 0:00:00.553803
elapsed time: 0:02:40.937094
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:06:35.049250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.79
 ---- batch: 020 ----
mean loss: 122.66
 ---- batch: 030 ----
mean loss: 129.25
 ---- batch: 040 ----
mean loss: 134.82
 ---- batch: 050 ----
mean loss: 127.06
 ---- batch: 060 ----
mean loss: 135.57
 ---- batch: 070 ----
mean loss: 135.30
 ---- batch: 080 ----
mean loss: 133.47
 ---- batch: 090 ----
mean loss: 129.88
 ---- batch: 100 ----
mean loss: 131.75
 ---- batch: 110 ----
mean loss: 129.79
train mean loss: 130.44
epoch train time: 0:00:00.555265
elapsed time: 0:02:41.492511
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:06:35.604659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.37
 ---- batch: 020 ----
mean loss: 129.21
 ---- batch: 030 ----
mean loss: 124.11
 ---- batch: 040 ----
mean loss: 131.32
 ---- batch: 050 ----
mean loss: 126.34
 ---- batch: 060 ----
mean loss: 132.85
 ---- batch: 070 ----
mean loss: 133.23
 ---- batch: 080 ----
mean loss: 132.51
 ---- batch: 090 ----
mean loss: 131.85
 ---- batch: 100 ----
mean loss: 134.35
 ---- batch: 110 ----
mean loss: 129.35
train mean loss: 130.32
epoch train time: 0:00:00.552305
elapsed time: 0:02:42.044942
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:06:36.157115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.64
 ---- batch: 020 ----
mean loss: 126.47
 ---- batch: 030 ----
mean loss: 124.83
 ---- batch: 040 ----
mean loss: 139.35
 ---- batch: 050 ----
mean loss: 126.38
 ---- batch: 060 ----
mean loss: 138.74
 ---- batch: 070 ----
mean loss: 133.49
 ---- batch: 080 ----
mean loss: 124.29
 ---- batch: 090 ----
mean loss: 131.21
 ---- batch: 100 ----
mean loss: 131.23
 ---- batch: 110 ----
mean loss: 126.35
train mean loss: 130.40
epoch train time: 0:00:00.556884
elapsed time: 0:02:42.601996
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:06:36.714165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.21
 ---- batch: 020 ----
mean loss: 130.60
 ---- batch: 030 ----
mean loss: 133.33
 ---- batch: 040 ----
mean loss: 134.70
 ---- batch: 050 ----
mean loss: 129.23
 ---- batch: 060 ----
mean loss: 143.47
 ---- batch: 070 ----
mean loss: 131.90
 ---- batch: 080 ----
mean loss: 126.38
 ---- batch: 090 ----
mean loss: 124.04
 ---- batch: 100 ----
mean loss: 129.42
 ---- batch: 110 ----
mean loss: 124.50
train mean loss: 130.26
epoch train time: 0:00:00.562446
elapsed time: 0:02:43.164594
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:06:37.276748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.21
 ---- batch: 020 ----
mean loss: 139.72
 ---- batch: 030 ----
mean loss: 126.05
 ---- batch: 040 ----
mean loss: 125.57
 ---- batch: 050 ----
mean loss: 128.59
 ---- batch: 060 ----
mean loss: 133.56
 ---- batch: 070 ----
mean loss: 125.15
 ---- batch: 080 ----
mean loss: 129.88
 ---- batch: 090 ----
mean loss: 130.68
 ---- batch: 100 ----
mean loss: 134.04
 ---- batch: 110 ----
mean loss: 125.77
train mean loss: 130.29
epoch train time: 0:00:00.585512
elapsed time: 0:02:43.750254
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:06:37.862411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.54
 ---- batch: 020 ----
mean loss: 137.02
 ---- batch: 030 ----
mean loss: 126.15
 ---- batch: 040 ----
mean loss: 133.46
 ---- batch: 050 ----
mean loss: 131.41
 ---- batch: 060 ----
mean loss: 130.62
 ---- batch: 070 ----
mean loss: 138.57
 ---- batch: 080 ----
mean loss: 121.84
 ---- batch: 090 ----
mean loss: 124.50
 ---- batch: 100 ----
mean loss: 132.13
 ---- batch: 110 ----
mean loss: 128.29
train mean loss: 130.26
epoch train time: 0:00:00.578435
elapsed time: 0:02:44.328834
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:06:38.441005
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.09
 ---- batch: 020 ----
mean loss: 132.74
 ---- batch: 030 ----
mean loss: 131.51
 ---- batch: 040 ----
mean loss: 135.90
 ---- batch: 050 ----
mean loss: 126.59
 ---- batch: 060 ----
mean loss: 136.42
 ---- batch: 070 ----
mean loss: 136.88
 ---- batch: 080 ----
mean loss: 133.87
 ---- batch: 090 ----
mean loss: 124.46
 ---- batch: 100 ----
mean loss: 131.25
 ---- batch: 110 ----
mean loss: 121.01
train mean loss: 130.31
epoch train time: 0:00:00.569042
elapsed time: 0:02:44.898032
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:06:39.010186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.57
 ---- batch: 020 ----
mean loss: 126.73
 ---- batch: 030 ----
mean loss: 131.19
 ---- batch: 040 ----
mean loss: 128.97
 ---- batch: 050 ----
mean loss: 128.25
 ---- batch: 060 ----
mean loss: 129.31
 ---- batch: 070 ----
mean loss: 130.44
 ---- batch: 080 ----
mean loss: 129.38
 ---- batch: 090 ----
mean loss: 144.58
 ---- batch: 100 ----
mean loss: 123.00
 ---- batch: 110 ----
mean loss: 133.62
train mean loss: 130.35
epoch train time: 0:00:00.555890
elapsed time: 0:02:45.454093
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:06:39.566262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.76
 ---- batch: 020 ----
mean loss: 126.48
 ---- batch: 030 ----
mean loss: 127.55
 ---- batch: 040 ----
mean loss: 126.46
 ---- batch: 050 ----
mean loss: 133.87
 ---- batch: 060 ----
mean loss: 136.05
 ---- batch: 070 ----
mean loss: 134.62
 ---- batch: 080 ----
mean loss: 127.99
 ---- batch: 090 ----
mean loss: 130.96
 ---- batch: 100 ----
mean loss: 136.51
 ---- batch: 110 ----
mean loss: 126.33
train mean loss: 130.07
epoch train time: 0:00:00.555016
elapsed time: 0:02:46.009260
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:06:40.121412
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.92
 ---- batch: 020 ----
mean loss: 134.06
 ---- batch: 030 ----
mean loss: 132.84
 ---- batch: 040 ----
mean loss: 124.11
 ---- batch: 050 ----
mean loss: 129.42
 ---- batch: 060 ----
mean loss: 134.65
 ---- batch: 070 ----
mean loss: 132.81
 ---- batch: 080 ----
mean loss: 117.59
 ---- batch: 090 ----
mean loss: 122.82
 ---- batch: 100 ----
mean loss: 135.63
 ---- batch: 110 ----
mean loss: 136.12
train mean loss: 130.28
epoch train time: 0:00:00.549475
elapsed time: 0:02:46.558869
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:06:40.671030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.70
 ---- batch: 020 ----
mean loss: 137.73
 ---- batch: 030 ----
mean loss: 127.82
 ---- batch: 040 ----
mean loss: 140.53
 ---- batch: 050 ----
mean loss: 132.27
 ---- batch: 060 ----
mean loss: 125.16
 ---- batch: 070 ----
mean loss: 136.83
 ---- batch: 080 ----
mean loss: 120.48
 ---- batch: 090 ----
mean loss: 125.89
 ---- batch: 100 ----
mean loss: 132.82
 ---- batch: 110 ----
mean loss: 130.67
train mean loss: 130.13
epoch train time: 0:00:00.560470
elapsed time: 0:02:47.119479
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:06:41.231633
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.92
 ---- batch: 020 ----
mean loss: 124.50
 ---- batch: 030 ----
mean loss: 129.38
 ---- batch: 040 ----
mean loss: 134.16
 ---- batch: 050 ----
mean loss: 131.29
 ---- batch: 060 ----
mean loss: 128.48
 ---- batch: 070 ----
mean loss: 134.41
 ---- batch: 080 ----
mean loss: 135.08
 ---- batch: 090 ----
mean loss: 128.95
 ---- batch: 100 ----
mean loss: 130.41
 ---- batch: 110 ----
mean loss: 130.97
train mean loss: 130.22
epoch train time: 0:00:00.572497
elapsed time: 0:02:47.692125
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:06:41.804278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.90
 ---- batch: 020 ----
mean loss: 125.87
 ---- batch: 030 ----
mean loss: 132.85
 ---- batch: 040 ----
mean loss: 127.93
 ---- batch: 050 ----
mean loss: 129.86
 ---- batch: 060 ----
mean loss: 129.95
 ---- batch: 070 ----
mean loss: 130.00
 ---- batch: 080 ----
mean loss: 125.69
 ---- batch: 090 ----
mean loss: 127.84
 ---- batch: 100 ----
mean loss: 136.00
 ---- batch: 110 ----
mean loss: 137.97
train mean loss: 130.17
epoch train time: 0:00:00.566661
elapsed time: 0:02:48.258923
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:06:42.371093
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.11
 ---- batch: 020 ----
mean loss: 134.35
 ---- batch: 030 ----
mean loss: 127.22
 ---- batch: 040 ----
mean loss: 124.68
 ---- batch: 050 ----
mean loss: 133.01
 ---- batch: 060 ----
mean loss: 129.40
 ---- batch: 070 ----
mean loss: 131.92
 ---- batch: 080 ----
mean loss: 130.47
 ---- batch: 090 ----
mean loss: 129.38
 ---- batch: 100 ----
mean loss: 131.67
 ---- batch: 110 ----
mean loss: 127.40
train mean loss: 130.06
epoch train time: 0:00:00.569070
elapsed time: 0:02:48.828155
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:06:42.940306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.07
 ---- batch: 020 ----
mean loss: 129.73
 ---- batch: 030 ----
mean loss: 130.43
 ---- batch: 040 ----
mean loss: 128.32
 ---- batch: 050 ----
mean loss: 127.41
 ---- batch: 060 ----
mean loss: 127.98
 ---- batch: 070 ----
mean loss: 132.41
 ---- batch: 080 ----
mean loss: 127.91
 ---- batch: 090 ----
mean loss: 130.28
 ---- batch: 100 ----
mean loss: 136.81
 ---- batch: 110 ----
mean loss: 135.11
train mean loss: 129.96
epoch train time: 0:00:00.562552
elapsed time: 0:02:49.390851
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:06:43.503028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.49
 ---- batch: 020 ----
mean loss: 124.65
 ---- batch: 030 ----
mean loss: 129.65
 ---- batch: 040 ----
mean loss: 130.86
 ---- batch: 050 ----
mean loss: 125.20
 ---- batch: 060 ----
mean loss: 132.50
 ---- batch: 070 ----
mean loss: 122.79
 ---- batch: 080 ----
mean loss: 130.16
 ---- batch: 090 ----
mean loss: 136.12
 ---- batch: 100 ----
mean loss: 135.02
 ---- batch: 110 ----
mean loss: 130.68
train mean loss: 130.03
epoch train time: 0:00:00.576734
elapsed time: 0:02:49.967788
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:06:44.079950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.25
 ---- batch: 020 ----
mean loss: 126.61
 ---- batch: 030 ----
mean loss: 125.99
 ---- batch: 040 ----
mean loss: 126.37
 ---- batch: 050 ----
mean loss: 127.48
 ---- batch: 060 ----
mean loss: 133.81
 ---- batch: 070 ----
mean loss: 129.60
 ---- batch: 080 ----
mean loss: 131.38
 ---- batch: 090 ----
mean loss: 135.34
 ---- batch: 100 ----
mean loss: 132.36
 ---- batch: 110 ----
mean loss: 131.39
train mean loss: 130.04
epoch train time: 0:00:00.564198
elapsed time: 0:02:50.535270
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_0/checkpoint.pth.tar
**** end time: 2019-09-27 02:06:44.647417 ****
