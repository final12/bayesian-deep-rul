Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_6', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 16346
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-27 00:46:29.310454 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 00:46:29.319959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4624.64
 ---- batch: 020 ----
mean loss: 2729.46
 ---- batch: 030 ----
mean loss: 1506.53
 ---- batch: 040 ----
mean loss: 1342.14
 ---- batch: 050 ----
mean loss: 1203.24
 ---- batch: 060 ----
mean loss: 1117.84
 ---- batch: 070 ----
mean loss: 1092.81
 ---- batch: 080 ----
mean loss: 1062.36
 ---- batch: 090 ----
mean loss: 995.45
 ---- batch: 100 ----
mean loss: 983.78
 ---- batch: 110 ----
mean loss: 945.46
train mean loss: 1583.24
epoch train time: 0:00:33.413383
elapsed time: 0:00:33.425547
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 00:47:02.736041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.32
 ---- batch: 020 ----
mean loss: 919.04
 ---- batch: 030 ----
mean loss: 853.89
 ---- batch: 040 ----
mean loss: 864.47
 ---- batch: 050 ----
mean loss: 827.75
 ---- batch: 060 ----
mean loss: 811.90
 ---- batch: 070 ----
mean loss: 815.81
 ---- batch: 080 ----
mean loss: 798.63
 ---- batch: 090 ----
mean loss: 800.29
 ---- batch: 100 ----
mean loss: 787.17
 ---- batch: 110 ----
mean loss: 794.15
train mean loss: 833.06
epoch train time: 0:00:02.232150
elapsed time: 0:00:35.657843
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 00:47:04.968355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.73
 ---- batch: 020 ----
mean loss: 760.71
 ---- batch: 030 ----
mean loss: 725.86
 ---- batch: 040 ----
mean loss: 734.91
 ---- batch: 050 ----
mean loss: 721.87
 ---- batch: 060 ----
mean loss: 718.69
 ---- batch: 070 ----
mean loss: 725.29
 ---- batch: 080 ----
mean loss: 712.31
 ---- batch: 090 ----
mean loss: 697.09
 ---- batch: 100 ----
mean loss: 699.50
 ---- batch: 110 ----
mean loss: 699.25
train mean loss: 720.36
epoch train time: 0:00:02.173305
elapsed time: 0:00:37.831315
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 00:47:07.141828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.11
 ---- batch: 020 ----
mean loss: 701.31
 ---- batch: 030 ----
mean loss: 692.70
 ---- batch: 040 ----
mean loss: 665.77
 ---- batch: 050 ----
mean loss: 671.93
 ---- batch: 060 ----
mean loss: 663.58
 ---- batch: 070 ----
mean loss: 664.24
 ---- batch: 080 ----
mean loss: 644.57
 ---- batch: 090 ----
mean loss: 663.89
 ---- batch: 100 ----
mean loss: 645.02
 ---- batch: 110 ----
mean loss: 636.97
train mean loss: 666.39
epoch train time: 0:00:02.167154
elapsed time: 0:00:39.998653
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 00:47:09.309178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.95
 ---- batch: 020 ----
mean loss: 621.50
 ---- batch: 030 ----
mean loss: 643.23
 ---- batch: 040 ----
mean loss: 647.51
 ---- batch: 050 ----
mean loss: 629.30
 ---- batch: 060 ----
mean loss: 630.87
 ---- batch: 070 ----
mean loss: 619.44
 ---- batch: 080 ----
mean loss: 644.59
 ---- batch: 090 ----
mean loss: 621.97
 ---- batch: 100 ----
mean loss: 628.91
 ---- batch: 110 ----
mean loss: 628.24
train mean loss: 631.49
epoch train time: 0:00:02.186762
elapsed time: 0:00:42.185609
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 00:47:11.496115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 619.49
 ---- batch: 020 ----
mean loss: 622.04
 ---- batch: 030 ----
mean loss: 613.45
 ---- batch: 040 ----
mean loss: 597.86
 ---- batch: 050 ----
mean loss: 621.12
 ---- batch: 060 ----
mean loss: 616.18
 ---- batch: 070 ----
mean loss: 603.46
 ---- batch: 080 ----
mean loss: 620.67
 ---- batch: 090 ----
mean loss: 604.25
 ---- batch: 100 ----
mean loss: 605.98
 ---- batch: 110 ----
mean loss: 602.23
train mean loss: 611.34
epoch train time: 0:00:02.174258
elapsed time: 0:00:44.360028
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 00:47:13.670536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 591.05
 ---- batch: 020 ----
mean loss: 602.55
 ---- batch: 030 ----
mean loss: 588.59
 ---- batch: 040 ----
mean loss: 606.60
 ---- batch: 050 ----
mean loss: 590.37
 ---- batch: 060 ----
mean loss: 586.90
 ---- batch: 070 ----
mean loss: 569.56
 ---- batch: 080 ----
mean loss: 606.73
 ---- batch: 090 ----
mean loss: 597.69
 ---- batch: 100 ----
mean loss: 604.11
 ---- batch: 110 ----
mean loss: 600.63
train mean loss: 595.30
epoch train time: 0:00:02.182193
elapsed time: 0:00:46.542396
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 00:47:15.852902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 591.05
 ---- batch: 020 ----
mean loss: 576.37
 ---- batch: 030 ----
mean loss: 613.94
 ---- batch: 040 ----
mean loss: 593.94
 ---- batch: 050 ----
mean loss: 590.34
 ---- batch: 060 ----
mean loss: 573.31
 ---- batch: 070 ----
mean loss: 593.38
 ---- batch: 080 ----
mean loss: 588.33
 ---- batch: 090 ----
mean loss: 598.17
 ---- batch: 100 ----
mean loss: 574.99
 ---- batch: 110 ----
mean loss: 589.11
train mean loss: 588.91
epoch train time: 0:00:02.181486
elapsed time: 0:00:48.724059
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 00:47:18.034566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 597.51
 ---- batch: 020 ----
mean loss: 565.36
 ---- batch: 030 ----
mean loss: 580.20
 ---- batch: 040 ----
mean loss: 568.05
 ---- batch: 050 ----
mean loss: 592.17
 ---- batch: 060 ----
mean loss: 592.79
 ---- batch: 070 ----
mean loss: 577.74
 ---- batch: 080 ----
mean loss: 581.92
 ---- batch: 090 ----
mean loss: 585.15
 ---- batch: 100 ----
mean loss: 591.75
 ---- batch: 110 ----
mean loss: 577.74
train mean loss: 582.13
epoch train time: 0:00:02.183376
elapsed time: 0:00:50.907623
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 00:47:20.218156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 568.00
 ---- batch: 020 ----
mean loss: 565.04
 ---- batch: 030 ----
mean loss: 566.30
 ---- batch: 040 ----
mean loss: 581.19
 ---- batch: 050 ----
mean loss: 580.26
 ---- batch: 060 ----
mean loss: 567.35
 ---- batch: 070 ----
mean loss: 565.10
 ---- batch: 080 ----
mean loss: 570.58
 ---- batch: 090 ----
mean loss: 559.09
 ---- batch: 100 ----
mean loss: 558.74
 ---- batch: 110 ----
mean loss: 589.40
train mean loss: 570.02
epoch train time: 0:00:02.180362
elapsed time: 0:00:53.088201
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 00:47:22.398731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 557.48
 ---- batch: 020 ----
mean loss: 550.32
 ---- batch: 030 ----
mean loss: 562.08
 ---- batch: 040 ----
mean loss: 586.73
 ---- batch: 050 ----
mean loss: 573.26
 ---- batch: 060 ----
mean loss: 569.57
 ---- batch: 070 ----
mean loss: 566.51
 ---- batch: 080 ----
mean loss: 568.17
 ---- batch: 090 ----
mean loss: 559.48
 ---- batch: 100 ----
mean loss: 559.36
 ---- batch: 110 ----
mean loss: 537.71
train mean loss: 562.59
epoch train time: 0:00:02.184088
elapsed time: 0:00:55.272509
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 00:47:24.583041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 552.26
 ---- batch: 020 ----
mean loss: 580.72
 ---- batch: 030 ----
mean loss: 560.88
 ---- batch: 040 ----
mean loss: 553.69
 ---- batch: 050 ----
mean loss: 552.36
 ---- batch: 060 ----
mean loss: 556.29
 ---- batch: 070 ----
mean loss: 528.27
 ---- batch: 080 ----
mean loss: 564.78
 ---- batch: 090 ----
mean loss: 559.94
 ---- batch: 100 ----
mean loss: 542.62
 ---- batch: 110 ----
mean loss: 534.94
train mean loss: 553.98
epoch train time: 0:00:02.182736
elapsed time: 0:00:57.455442
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 00:47:26.765950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.00
 ---- batch: 020 ----
mean loss: 545.95
 ---- batch: 030 ----
mean loss: 548.43
 ---- batch: 040 ----
mean loss: 544.67
 ---- batch: 050 ----
mean loss: 530.93
 ---- batch: 060 ----
mean loss: 553.67
 ---- batch: 070 ----
mean loss: 538.99
 ---- batch: 080 ----
mean loss: 534.25
 ---- batch: 090 ----
mean loss: 528.20
 ---- batch: 100 ----
mean loss: 562.29
 ---- batch: 110 ----
mean loss: 536.87
train mean loss: 543.52
epoch train time: 0:00:02.183384
elapsed time: 0:00:59.639005
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 00:47:28.949516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 537.84
 ---- batch: 020 ----
mean loss: 521.31
 ---- batch: 030 ----
mean loss: 527.23
 ---- batch: 040 ----
mean loss: 534.26
 ---- batch: 050 ----
mean loss: 532.23
 ---- batch: 060 ----
mean loss: 533.12
 ---- batch: 070 ----
mean loss: 536.55
 ---- batch: 080 ----
mean loss: 529.25
 ---- batch: 090 ----
mean loss: 535.97
 ---- batch: 100 ----
mean loss: 555.37
 ---- batch: 110 ----
mean loss: 526.95
train mean loss: 533.30
epoch train time: 0:00:02.180633
elapsed time: 0:01:01.819816
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 00:47:31.130324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.14
 ---- batch: 020 ----
mean loss: 541.52
 ---- batch: 030 ----
mean loss: 551.35
 ---- batch: 040 ----
mean loss: 536.58
 ---- batch: 050 ----
mean loss: 545.01
 ---- batch: 060 ----
mean loss: 530.43
 ---- batch: 070 ----
mean loss: 530.02
 ---- batch: 080 ----
mean loss: 552.24
 ---- batch: 090 ----
mean loss: 528.84
 ---- batch: 100 ----
mean loss: 512.38
 ---- batch: 110 ----
mean loss: 544.86
train mean loss: 536.89
epoch train time: 0:00:02.183904
elapsed time: 0:01:04.003907
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 00:47:33.314417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 535.93
 ---- batch: 020 ----
mean loss: 533.71
 ---- batch: 030 ----
mean loss: 524.53
 ---- batch: 040 ----
mean loss: 525.36
 ---- batch: 050 ----
mean loss: 525.17
 ---- batch: 060 ----
mean loss: 563.40
 ---- batch: 070 ----
mean loss: 518.50
 ---- batch: 080 ----
mean loss: 523.66
 ---- batch: 090 ----
mean loss: 535.33
 ---- batch: 100 ----
mean loss: 514.74
 ---- batch: 110 ----
mean loss: 526.55
train mean loss: 530.49
epoch train time: 0:00:02.184204
elapsed time: 0:01:06.188273
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 00:47:35.498781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 531.90
 ---- batch: 020 ----
mean loss: 514.16
 ---- batch: 030 ----
mean loss: 529.07
 ---- batch: 040 ----
mean loss: 531.59
 ---- batch: 050 ----
mean loss: 525.65
 ---- batch: 060 ----
mean loss: 523.32
 ---- batch: 070 ----
mean loss: 532.80
 ---- batch: 080 ----
mean loss: 519.25
 ---- batch: 090 ----
mean loss: 530.43
 ---- batch: 100 ----
mean loss: 532.78
 ---- batch: 110 ----
mean loss: 528.73
train mean loss: 527.17
epoch train time: 0:00:02.177318
elapsed time: 0:01:08.365771
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 00:47:37.676298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.76
 ---- batch: 020 ----
mean loss: 543.99
 ---- batch: 030 ----
mean loss: 515.25
 ---- batch: 040 ----
mean loss: 521.70
 ---- batch: 050 ----
mean loss: 533.63
 ---- batch: 060 ----
mean loss: 515.09
 ---- batch: 070 ----
mean loss: 524.58
 ---- batch: 080 ----
mean loss: 528.09
 ---- batch: 090 ----
mean loss: 508.24
 ---- batch: 100 ----
mean loss: 541.15
 ---- batch: 110 ----
mean loss: 523.56
train mean loss: 524.31
epoch train time: 0:00:02.172447
elapsed time: 0:01:10.538445
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 00:47:39.848971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.02
 ---- batch: 020 ----
mean loss: 523.44
 ---- batch: 030 ----
mean loss: 521.57
 ---- batch: 040 ----
mean loss: 526.95
 ---- batch: 050 ----
mean loss: 512.43
 ---- batch: 060 ----
mean loss: 515.88
 ---- batch: 070 ----
mean loss: 528.64
 ---- batch: 080 ----
mean loss: 525.60
 ---- batch: 090 ----
mean loss: 508.74
 ---- batch: 100 ----
mean loss: 524.76
 ---- batch: 110 ----
mean loss: 522.65
train mean loss: 519.46
epoch train time: 0:00:02.180302
elapsed time: 0:01:12.718970
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 00:47:42.029478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 528.95
 ---- batch: 020 ----
mean loss: 512.52
 ---- batch: 030 ----
mean loss: 514.98
 ---- batch: 040 ----
mean loss: 506.56
 ---- batch: 050 ----
mean loss: 523.70
 ---- batch: 060 ----
mean loss: 512.60
 ---- batch: 070 ----
mean loss: 509.12
 ---- batch: 080 ----
mean loss: 515.69
 ---- batch: 090 ----
mean loss: 516.59
 ---- batch: 100 ----
mean loss: 534.90
 ---- batch: 110 ----
mean loss: 510.21
train mean loss: 516.78
epoch train time: 0:00:02.178732
elapsed time: 0:01:14.897858
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 00:47:44.208368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.13
 ---- batch: 020 ----
mean loss: 516.82
 ---- batch: 030 ----
mean loss: 525.90
 ---- batch: 040 ----
mean loss: 529.52
 ---- batch: 050 ----
mean loss: 522.46
 ---- batch: 060 ----
mean loss: 497.94
 ---- batch: 070 ----
mean loss: 525.89
 ---- batch: 080 ----
mean loss: 510.16
 ---- batch: 090 ----
mean loss: 499.28
 ---- batch: 100 ----
mean loss: 512.21
 ---- batch: 110 ----
mean loss: 520.76
train mean loss: 514.43
epoch train time: 0:00:02.175277
elapsed time: 0:01:17.073305
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 00:47:46.383816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.86
 ---- batch: 020 ----
mean loss: 525.48
 ---- batch: 030 ----
mean loss: 507.57
 ---- batch: 040 ----
mean loss: 524.15
 ---- batch: 050 ----
mean loss: 515.40
 ---- batch: 060 ----
mean loss: 512.41
 ---- batch: 070 ----
mean loss: 505.91
 ---- batch: 080 ----
mean loss: 501.23
 ---- batch: 090 ----
mean loss: 517.46
 ---- batch: 100 ----
mean loss: 523.30
 ---- batch: 110 ----
mean loss: 523.49
train mean loss: 516.18
epoch train time: 0:00:02.186029
elapsed time: 0:01:19.259522
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 00:47:48.570054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.49
 ---- batch: 020 ----
mean loss: 513.06
 ---- batch: 030 ----
mean loss: 502.19
 ---- batch: 040 ----
mean loss: 506.21
 ---- batch: 050 ----
mean loss: 516.41
 ---- batch: 060 ----
mean loss: 509.52
 ---- batch: 070 ----
mean loss: 531.33
 ---- batch: 080 ----
mean loss: 497.13
 ---- batch: 090 ----
mean loss: 507.30
 ---- batch: 100 ----
mean loss: 508.13
 ---- batch: 110 ----
mean loss: 506.86
train mean loss: 509.43
epoch train time: 0:00:02.176759
elapsed time: 0:01:21.436478
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 00:47:50.746984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.91
 ---- batch: 020 ----
mean loss: 520.30
 ---- batch: 030 ----
mean loss: 515.13
 ---- batch: 040 ----
mean loss: 520.53
 ---- batch: 050 ----
mean loss: 525.98
 ---- batch: 060 ----
mean loss: 522.50
 ---- batch: 070 ----
mean loss: 524.10
 ---- batch: 080 ----
mean loss: 506.62
 ---- batch: 090 ----
mean loss: 509.03
 ---- batch: 100 ----
mean loss: 521.95
 ---- batch: 110 ----
mean loss: 505.90
train mean loss: 516.37
epoch train time: 0:00:02.178547
elapsed time: 0:01:23.615208
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 00:47:52.925740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.24
 ---- batch: 020 ----
mean loss: 510.42
 ---- batch: 030 ----
mean loss: 516.80
 ---- batch: 040 ----
mean loss: 509.41
 ---- batch: 050 ----
mean loss: 524.74
 ---- batch: 060 ----
mean loss: 514.18
 ---- batch: 070 ----
mean loss: 509.05
 ---- batch: 080 ----
mean loss: 513.21
 ---- batch: 090 ----
mean loss: 519.00
 ---- batch: 100 ----
mean loss: 503.36
 ---- batch: 110 ----
mean loss: 518.93
train mean loss: 512.87
epoch train time: 0:00:02.180491
elapsed time: 0:01:25.795898
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 00:47:55.106416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.45
 ---- batch: 020 ----
mean loss: 532.02
 ---- batch: 030 ----
mean loss: 508.43
 ---- batch: 040 ----
mean loss: 500.60
 ---- batch: 050 ----
mean loss: 517.52
 ---- batch: 060 ----
mean loss: 509.89
 ---- batch: 070 ----
mean loss: 523.28
 ---- batch: 080 ----
mean loss: 506.16
 ---- batch: 090 ----
mean loss: 494.85
 ---- batch: 100 ----
mean loss: 501.65
 ---- batch: 110 ----
mean loss: 500.88
train mean loss: 510.57
epoch train time: 0:00:02.181198
elapsed time: 0:01:27.977318
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 00:47:57.287832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.78
 ---- batch: 020 ----
mean loss: 507.01
 ---- batch: 030 ----
mean loss: 510.60
 ---- batch: 040 ----
mean loss: 527.90
 ---- batch: 050 ----
mean loss: 516.48
 ---- batch: 060 ----
mean loss: 504.38
 ---- batch: 070 ----
mean loss: 495.47
 ---- batch: 080 ----
mean loss: 517.47
 ---- batch: 090 ----
mean loss: 516.90
 ---- batch: 100 ----
mean loss: 513.82
 ---- batch: 110 ----
mean loss: 507.70
train mean loss: 512.36
epoch train time: 0:00:02.177280
elapsed time: 0:01:30.154780
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 00:47:59.465290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.75
 ---- batch: 020 ----
mean loss: 505.15
 ---- batch: 030 ----
mean loss: 526.84
 ---- batch: 040 ----
mean loss: 520.11
 ---- batch: 050 ----
mean loss: 504.17
 ---- batch: 060 ----
mean loss: 502.99
 ---- batch: 070 ----
mean loss: 500.72
 ---- batch: 080 ----
mean loss: 508.97
 ---- batch: 090 ----
mean loss: 507.82
 ---- batch: 100 ----
mean loss: 495.47
 ---- batch: 110 ----
mean loss: 512.10
train mean loss: 508.38
epoch train time: 0:00:02.181167
elapsed time: 0:01:32.336110
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 00:48:01.646615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.54
 ---- batch: 020 ----
mean loss: 513.89
 ---- batch: 030 ----
mean loss: 501.48
 ---- batch: 040 ----
mean loss: 504.24
 ---- batch: 050 ----
mean loss: 505.08
 ---- batch: 060 ----
mean loss: 505.33
 ---- batch: 070 ----
mean loss: 503.20
 ---- batch: 080 ----
mean loss: 508.74
 ---- batch: 090 ----
mean loss: 496.15
 ---- batch: 100 ----
mean loss: 501.14
 ---- batch: 110 ----
mean loss: 518.96
train mean loss: 505.11
epoch train time: 0:00:02.178394
elapsed time: 0:01:34.514675
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 00:48:03.825183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.02
 ---- batch: 020 ----
mean loss: 502.55
 ---- batch: 030 ----
mean loss: 499.63
 ---- batch: 040 ----
mean loss: 493.69
 ---- batch: 050 ----
mean loss: 500.85
 ---- batch: 060 ----
mean loss: 487.18
 ---- batch: 070 ----
mean loss: 506.77
 ---- batch: 080 ----
mean loss: 495.57
 ---- batch: 090 ----
mean loss: 512.72
 ---- batch: 100 ----
mean loss: 532.93
 ---- batch: 110 ----
mean loss: 501.15
train mean loss: 503.88
epoch train time: 0:00:02.180806
elapsed time: 0:01:36.695645
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 00:48:06.006171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.67
 ---- batch: 020 ----
mean loss: 486.18
 ---- batch: 030 ----
mean loss: 509.72
 ---- batch: 040 ----
mean loss: 502.56
 ---- batch: 050 ----
mean loss: 497.21
 ---- batch: 060 ----
mean loss: 501.51
 ---- batch: 070 ----
mean loss: 484.55
 ---- batch: 080 ----
mean loss: 517.96
 ---- batch: 090 ----
mean loss: 505.76
 ---- batch: 100 ----
mean loss: 519.11
 ---- batch: 110 ----
mean loss: 514.18
train mean loss: 502.51
epoch train time: 0:00:02.194363
elapsed time: 0:01:38.890199
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 00:48:08.200707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.08
 ---- batch: 020 ----
mean loss: 515.95
 ---- batch: 030 ----
mean loss: 512.85
 ---- batch: 040 ----
mean loss: 530.95
 ---- batch: 050 ----
mean loss: 504.59
 ---- batch: 060 ----
mean loss: 504.44
 ---- batch: 070 ----
mean loss: 489.69
 ---- batch: 080 ----
mean loss: 486.73
 ---- batch: 090 ----
mean loss: 508.91
 ---- batch: 100 ----
mean loss: 503.47
 ---- batch: 110 ----
mean loss: 514.56
train mean loss: 507.25
epoch train time: 0:00:02.196458
elapsed time: 0:01:41.086828
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 00:48:10.397351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.27
 ---- batch: 020 ----
mean loss: 499.82
 ---- batch: 030 ----
mean loss: 500.49
 ---- batch: 040 ----
mean loss: 483.70
 ---- batch: 050 ----
mean loss: 502.79
 ---- batch: 060 ----
mean loss: 516.83
 ---- batch: 070 ----
mean loss: 502.44
 ---- batch: 080 ----
mean loss: 514.23
 ---- batch: 090 ----
mean loss: 512.54
 ---- batch: 100 ----
mean loss: 505.36
 ---- batch: 110 ----
mean loss: 503.23
train mean loss: 503.27
epoch train time: 0:00:02.178139
elapsed time: 0:01:43.265165
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 00:48:12.575679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.45
 ---- batch: 020 ----
mean loss: 500.01
 ---- batch: 030 ----
mean loss: 508.71
 ---- batch: 040 ----
mean loss: 497.41
 ---- batch: 050 ----
mean loss: 492.36
 ---- batch: 060 ----
mean loss: 494.33
 ---- batch: 070 ----
mean loss: 524.02
 ---- batch: 080 ----
mean loss: 503.12
 ---- batch: 090 ----
mean loss: 493.70
 ---- batch: 100 ----
mean loss: 485.95
 ---- batch: 110 ----
mean loss: 498.02
train mean loss: 498.98
epoch train time: 0:00:02.181802
elapsed time: 0:01:45.447187
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 00:48:14.757703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.19
 ---- batch: 020 ----
mean loss: 494.85
 ---- batch: 030 ----
mean loss: 488.16
 ---- batch: 040 ----
mean loss: 506.92
 ---- batch: 050 ----
mean loss: 502.18
 ---- batch: 060 ----
mean loss: 506.66
 ---- batch: 070 ----
mean loss: 500.77
 ---- batch: 080 ----
mean loss: 521.52
 ---- batch: 090 ----
mean loss: 508.74
 ---- batch: 100 ----
mean loss: 501.46
 ---- batch: 110 ----
mean loss: 501.56
train mean loss: 502.20
epoch train time: 0:00:02.183965
elapsed time: 0:01:47.631330
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 00:48:16.941855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.51
 ---- batch: 020 ----
mean loss: 504.07
 ---- batch: 030 ----
mean loss: 505.82
 ---- batch: 040 ----
mean loss: 503.27
 ---- batch: 050 ----
mean loss: 497.06
 ---- batch: 060 ----
mean loss: 508.13
 ---- batch: 070 ----
mean loss: 503.34
 ---- batch: 080 ----
mean loss: 497.79
 ---- batch: 090 ----
mean loss: 484.90
 ---- batch: 100 ----
mean loss: 510.53
 ---- batch: 110 ----
mean loss: 486.53
train mean loss: 501.20
epoch train time: 0:00:02.180570
elapsed time: 0:01:49.812083
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 00:48:19.122593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.33
 ---- batch: 020 ----
mean loss: 497.26
 ---- batch: 030 ----
mean loss: 510.18
 ---- batch: 040 ----
mean loss: 485.32
 ---- batch: 050 ----
mean loss: 490.00
 ---- batch: 060 ----
mean loss: 494.26
 ---- batch: 070 ----
mean loss: 498.79
 ---- batch: 080 ----
mean loss: 497.42
 ---- batch: 090 ----
mean loss: 488.20
 ---- batch: 100 ----
mean loss: 499.36
 ---- batch: 110 ----
mean loss: 507.39
train mean loss: 498.45
epoch train time: 0:00:02.179900
elapsed time: 0:01:51.992170
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 00:48:21.302699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.72
 ---- batch: 020 ----
mean loss: 488.49
 ---- batch: 030 ----
mean loss: 496.55
 ---- batch: 040 ----
mean loss: 496.66
 ---- batch: 050 ----
mean loss: 502.38
 ---- batch: 060 ----
mean loss: 502.93
 ---- batch: 070 ----
mean loss: 486.19
 ---- batch: 080 ----
mean loss: 486.16
 ---- batch: 090 ----
mean loss: 491.24
 ---- batch: 100 ----
mean loss: 493.27
 ---- batch: 110 ----
mean loss: 503.42
train mean loss: 493.99
epoch train time: 0:00:02.178367
elapsed time: 0:01:54.170712
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 00:48:23.481217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.72
 ---- batch: 020 ----
mean loss: 503.14
 ---- batch: 030 ----
mean loss: 495.03
 ---- batch: 040 ----
mean loss: 506.83
 ---- batch: 050 ----
mean loss: 500.17
 ---- batch: 060 ----
mean loss: 491.47
 ---- batch: 070 ----
mean loss: 475.10
 ---- batch: 080 ----
mean loss: 509.71
 ---- batch: 090 ----
mean loss: 500.50
 ---- batch: 100 ----
mean loss: 492.00
 ---- batch: 110 ----
mean loss: 505.85
train mean loss: 497.59
epoch train time: 0:00:02.186116
elapsed time: 0:01:56.357004
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 00:48:25.667513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.03
 ---- batch: 020 ----
mean loss: 490.93
 ---- batch: 030 ----
mean loss: 510.52
 ---- batch: 040 ----
mean loss: 491.14
 ---- batch: 050 ----
mean loss: 498.30
 ---- batch: 060 ----
mean loss: 499.53
 ---- batch: 070 ----
mean loss: 496.00
 ---- batch: 080 ----
mean loss: 500.74
 ---- batch: 090 ----
mean loss: 502.74
 ---- batch: 100 ----
mean loss: 512.58
 ---- batch: 110 ----
mean loss: 490.94
train mean loss: 497.08
epoch train time: 0:00:02.178541
elapsed time: 0:01:58.535719
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 00:48:27.846227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.80
 ---- batch: 020 ----
mean loss: 491.96
 ---- batch: 030 ----
mean loss: 496.04
 ---- batch: 040 ----
mean loss: 488.94
 ---- batch: 050 ----
mean loss: 491.70
 ---- batch: 060 ----
mean loss: 499.35
 ---- batch: 070 ----
mean loss: 496.23
 ---- batch: 080 ----
mean loss: 488.49
 ---- batch: 090 ----
mean loss: 488.12
 ---- batch: 100 ----
mean loss: 493.93
 ---- batch: 110 ----
mean loss: 498.90
train mean loss: 492.86
epoch train time: 0:00:02.177953
elapsed time: 0:02:00.713869
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 00:48:30.024381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.93
 ---- batch: 020 ----
mean loss: 485.95
 ---- batch: 030 ----
mean loss: 499.13
 ---- batch: 040 ----
mean loss: 494.98
 ---- batch: 050 ----
mean loss: 492.29
 ---- batch: 060 ----
mean loss: 486.42
 ---- batch: 070 ----
mean loss: 499.32
 ---- batch: 080 ----
mean loss: 502.36
 ---- batch: 090 ----
mean loss: 486.00
 ---- batch: 100 ----
mean loss: 485.59
 ---- batch: 110 ----
mean loss: 488.67
train mean loss: 491.13
epoch train time: 0:00:02.179244
elapsed time: 0:02:02.893290
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 00:48:32.203799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.51
 ---- batch: 020 ----
mean loss: 487.16
 ---- batch: 030 ----
mean loss: 483.72
 ---- batch: 040 ----
mean loss: 497.60
 ---- batch: 050 ----
mean loss: 487.39
 ---- batch: 060 ----
mean loss: 493.69
 ---- batch: 070 ----
mean loss: 507.46
 ---- batch: 080 ----
mean loss: 504.57
 ---- batch: 090 ----
mean loss: 497.54
 ---- batch: 100 ----
mean loss: 475.89
 ---- batch: 110 ----
mean loss: 503.33
train mean loss: 496.16
epoch train time: 0:00:02.182251
elapsed time: 0:02:05.075722
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 00:48:34.386255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.88
 ---- batch: 020 ----
mean loss: 478.69
 ---- batch: 030 ----
mean loss: 495.74
 ---- batch: 040 ----
mean loss: 493.73
 ---- batch: 050 ----
mean loss: 486.12
 ---- batch: 060 ----
mean loss: 500.53
 ---- batch: 070 ----
mean loss: 494.01
 ---- batch: 080 ----
mean loss: 497.01
 ---- batch: 090 ----
mean loss: 485.48
 ---- batch: 100 ----
mean loss: 488.31
 ---- batch: 110 ----
mean loss: 491.99
train mean loss: 493.87
epoch train time: 0:00:02.181843
elapsed time: 0:02:07.257756
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 00:48:36.568278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.80
 ---- batch: 020 ----
mean loss: 484.09
 ---- batch: 030 ----
mean loss: 489.93
 ---- batch: 040 ----
mean loss: 494.35
 ---- batch: 050 ----
mean loss: 503.27
 ---- batch: 060 ----
mean loss: 520.19
 ---- batch: 070 ----
mean loss: 486.41
 ---- batch: 080 ----
mean loss: 484.97
 ---- batch: 090 ----
mean loss: 479.78
 ---- batch: 100 ----
mean loss: 499.53
 ---- batch: 110 ----
mean loss: 481.26
train mean loss: 492.14
epoch train time: 0:00:02.180540
elapsed time: 0:02:09.438502
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 00:48:38.749012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.29
 ---- batch: 020 ----
mean loss: 501.02
 ---- batch: 030 ----
mean loss: 502.20
 ---- batch: 040 ----
mean loss: 491.42
 ---- batch: 050 ----
mean loss: 494.65
 ---- batch: 060 ----
mean loss: 488.59
 ---- batch: 070 ----
mean loss: 497.12
 ---- batch: 080 ----
mean loss: 488.50
 ---- batch: 090 ----
mean loss: 476.65
 ---- batch: 100 ----
mean loss: 496.85
 ---- batch: 110 ----
mean loss: 495.23
train mean loss: 493.56
epoch train time: 0:00:02.180464
elapsed time: 0:02:11.619131
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 00:48:40.929662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.90
 ---- batch: 020 ----
mean loss: 491.39
 ---- batch: 030 ----
mean loss: 494.56
 ---- batch: 040 ----
mean loss: 495.34
 ---- batch: 050 ----
mean loss: 479.17
 ---- batch: 060 ----
mean loss: 488.66
 ---- batch: 070 ----
mean loss: 487.74
 ---- batch: 080 ----
mean loss: 488.59
 ---- batch: 090 ----
mean loss: 491.49
 ---- batch: 100 ----
mean loss: 494.17
 ---- batch: 110 ----
mean loss: 487.59
train mean loss: 489.99
epoch train time: 0:00:02.178930
elapsed time: 0:02:13.798271
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 00:48:43.108794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.92
 ---- batch: 020 ----
mean loss: 503.30
 ---- batch: 030 ----
mean loss: 486.98
 ---- batch: 040 ----
mean loss: 482.49
 ---- batch: 050 ----
mean loss: 482.04
 ---- batch: 060 ----
mean loss: 493.66
 ---- batch: 070 ----
mean loss: 497.51
 ---- batch: 080 ----
mean loss: 480.31
 ---- batch: 090 ----
mean loss: 494.12
 ---- batch: 100 ----
mean loss: 477.47
 ---- batch: 110 ----
mean loss: 476.10
train mean loss: 487.47
epoch train time: 0:00:02.180782
elapsed time: 0:02:15.979311
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 00:48:45.289828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.95
 ---- batch: 020 ----
mean loss: 492.73
 ---- batch: 030 ----
mean loss: 490.66
 ---- batch: 040 ----
mean loss: 485.25
 ---- batch: 050 ----
mean loss: 488.47
 ---- batch: 060 ----
mean loss: 497.29
 ---- batch: 070 ----
mean loss: 495.58
 ---- batch: 080 ----
mean loss: 496.95
 ---- batch: 090 ----
mean loss: 493.31
 ---- batch: 100 ----
mean loss: 481.50
 ---- batch: 110 ----
mean loss: 477.92
train mean loss: 489.92
epoch train time: 0:00:02.175659
elapsed time: 0:02:18.155187
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 00:48:47.465719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.57
 ---- batch: 020 ----
mean loss: 506.79
 ---- batch: 030 ----
mean loss: 484.16
 ---- batch: 040 ----
mean loss: 502.51
 ---- batch: 050 ----
mean loss: 503.69
 ---- batch: 060 ----
mean loss: 480.89
 ---- batch: 070 ----
mean loss: 482.70
 ---- batch: 080 ----
mean loss: 502.11
 ---- batch: 090 ----
mean loss: 502.55
 ---- batch: 100 ----
mean loss: 479.22
 ---- batch: 110 ----
mean loss: 477.81
train mean loss: 492.10
epoch train time: 0:00:02.174652
elapsed time: 0:02:20.330033
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 00:48:49.640569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.20
 ---- batch: 020 ----
mean loss: 490.11
 ---- batch: 030 ----
mean loss: 482.13
 ---- batch: 040 ----
mean loss: 486.21
 ---- batch: 050 ----
mean loss: 500.10
 ---- batch: 060 ----
mean loss: 480.23
 ---- batch: 070 ----
mean loss: 508.09
 ---- batch: 080 ----
mean loss: 515.00
 ---- batch: 090 ----
mean loss: 486.70
 ---- batch: 100 ----
mean loss: 495.56
 ---- batch: 110 ----
mean loss: 501.87
train mean loss: 494.10
epoch train time: 0:00:02.179820
elapsed time: 0:02:22.510054
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 00:48:51.820563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.12
 ---- batch: 020 ----
mean loss: 497.57
 ---- batch: 030 ----
mean loss: 488.15
 ---- batch: 040 ----
mean loss: 478.49
 ---- batch: 050 ----
mean loss: 482.34
 ---- batch: 060 ----
mean loss: 491.85
 ---- batch: 070 ----
mean loss: 474.78
 ---- batch: 080 ----
mean loss: 482.99
 ---- batch: 090 ----
mean loss: 475.97
 ---- batch: 100 ----
mean loss: 492.72
 ---- batch: 110 ----
mean loss: 484.80
train mean loss: 484.91
epoch train time: 0:00:02.182294
elapsed time: 0:02:24.692527
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 00:48:54.003037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.04
 ---- batch: 020 ----
mean loss: 480.62
 ---- batch: 030 ----
mean loss: 489.96
 ---- batch: 040 ----
mean loss: 483.01
 ---- batch: 050 ----
mean loss: 480.87
 ---- batch: 060 ----
mean loss: 482.25
 ---- batch: 070 ----
mean loss: 472.20
 ---- batch: 080 ----
mean loss: 482.88
 ---- batch: 090 ----
mean loss: 489.06
 ---- batch: 100 ----
mean loss: 480.74
 ---- batch: 110 ----
mean loss: 501.78
train mean loss: 486.95
epoch train time: 0:00:02.175517
elapsed time: 0:02:26.868218
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 00:48:56.178728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.15
 ---- batch: 020 ----
mean loss: 488.66
 ---- batch: 030 ----
mean loss: 494.56
 ---- batch: 040 ----
mean loss: 495.90
 ---- batch: 050 ----
mean loss: 481.90
 ---- batch: 060 ----
mean loss: 485.42
 ---- batch: 070 ----
mean loss: 496.62
 ---- batch: 080 ----
mean loss: 482.51
 ---- batch: 090 ----
mean loss: 475.84
 ---- batch: 100 ----
mean loss: 478.19
 ---- batch: 110 ----
mean loss: 473.99
train mean loss: 484.53
epoch train time: 0:00:02.180470
elapsed time: 0:02:29.048875
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 00:48:58.359388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.54
 ---- batch: 020 ----
mean loss: 485.58
 ---- batch: 030 ----
mean loss: 490.17
 ---- batch: 040 ----
mean loss: 485.95
 ---- batch: 050 ----
mean loss: 499.75
 ---- batch: 060 ----
mean loss: 479.01
 ---- batch: 070 ----
mean loss: 476.53
 ---- batch: 080 ----
mean loss: 480.88
 ---- batch: 090 ----
mean loss: 474.74
 ---- batch: 100 ----
mean loss: 481.46
 ---- batch: 110 ----
mean loss: 488.25
train mean loss: 484.95
epoch train time: 0:00:02.180062
elapsed time: 0:02:31.229162
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 00:49:00.539678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.87
 ---- batch: 020 ----
mean loss: 486.74
 ---- batch: 030 ----
mean loss: 491.51
 ---- batch: 040 ----
mean loss: 486.94
 ---- batch: 050 ----
mean loss: 478.70
 ---- batch: 060 ----
mean loss: 483.24
 ---- batch: 070 ----
mean loss: 487.60
 ---- batch: 080 ----
mean loss: 489.67
 ---- batch: 090 ----
mean loss: 492.02
 ---- batch: 100 ----
mean loss: 489.54
 ---- batch: 110 ----
mean loss: 499.12
train mean loss: 489.51
epoch train time: 0:00:02.179424
elapsed time: 0:02:33.408776
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 00:49:02.719314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.17
 ---- batch: 020 ----
mean loss: 488.32
 ---- batch: 030 ----
mean loss: 501.22
 ---- batch: 040 ----
mean loss: 485.33
 ---- batch: 050 ----
mean loss: 494.94
 ---- batch: 060 ----
mean loss: 478.68
 ---- batch: 070 ----
mean loss: 479.50
 ---- batch: 080 ----
mean loss: 472.70
 ---- batch: 090 ----
mean loss: 485.51
 ---- batch: 100 ----
mean loss: 476.66
 ---- batch: 110 ----
mean loss: 479.30
train mean loss: 484.81
epoch train time: 0:00:02.177821
elapsed time: 0:02:35.586803
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 00:49:04.897328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.67
 ---- batch: 020 ----
mean loss: 478.21
 ---- batch: 030 ----
mean loss: 487.34
 ---- batch: 040 ----
mean loss: 481.32
 ---- batch: 050 ----
mean loss: 491.42
 ---- batch: 060 ----
mean loss: 493.86
 ---- batch: 070 ----
mean loss: 487.53
 ---- batch: 080 ----
mean loss: 476.97
 ---- batch: 090 ----
mean loss: 498.04
 ---- batch: 100 ----
mean loss: 485.14
 ---- batch: 110 ----
mean loss: 486.75
train mean loss: 486.60
epoch train time: 0:00:02.177046
elapsed time: 0:02:37.764041
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 00:49:07.074554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.63
 ---- batch: 020 ----
mean loss: 479.07
 ---- batch: 030 ----
mean loss: 491.80
 ---- batch: 040 ----
mean loss: 477.46
 ---- batch: 050 ----
mean loss: 473.94
 ---- batch: 060 ----
mean loss: 471.58
 ---- batch: 070 ----
mean loss: 490.93
 ---- batch: 080 ----
mean loss: 479.34
 ---- batch: 090 ----
mean loss: 487.48
 ---- batch: 100 ----
mean loss: 485.54
 ---- batch: 110 ----
mean loss: 479.50
train mean loss: 482.63
epoch train time: 0:00:02.176484
elapsed time: 0:02:39.940720
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 00:49:09.251240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.30
 ---- batch: 020 ----
mean loss: 480.54
 ---- batch: 030 ----
mean loss: 482.61
 ---- batch: 040 ----
mean loss: 483.25
 ---- batch: 050 ----
mean loss: 464.27
 ---- batch: 060 ----
mean loss: 480.43
 ---- batch: 070 ----
mean loss: 475.47
 ---- batch: 080 ----
mean loss: 480.93
 ---- batch: 090 ----
mean loss: 479.77
 ---- batch: 100 ----
mean loss: 488.40
 ---- batch: 110 ----
mean loss: 499.15
train mean loss: 481.46
epoch train time: 0:00:02.176532
elapsed time: 0:02:42.117424
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 00:49:11.427935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.74
 ---- batch: 020 ----
mean loss: 477.68
 ---- batch: 030 ----
mean loss: 481.50
 ---- batch: 040 ----
mean loss: 479.46
 ---- batch: 050 ----
mean loss: 496.89
 ---- batch: 060 ----
mean loss: 489.43
 ---- batch: 070 ----
mean loss: 491.07
 ---- batch: 080 ----
mean loss: 492.44
 ---- batch: 090 ----
mean loss: 482.44
 ---- batch: 100 ----
mean loss: 481.88
 ---- batch: 110 ----
mean loss: 486.07
train mean loss: 485.91
epoch train time: 0:00:02.178083
elapsed time: 0:02:44.295673
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 00:49:13.606186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.62
 ---- batch: 020 ----
mean loss: 476.96
 ---- batch: 030 ----
mean loss: 481.09
 ---- batch: 040 ----
mean loss: 483.21
 ---- batch: 050 ----
mean loss: 475.74
 ---- batch: 060 ----
mean loss: 465.99
 ---- batch: 070 ----
mean loss: 489.01
 ---- batch: 080 ----
mean loss: 465.89
 ---- batch: 090 ----
mean loss: 490.38
 ---- batch: 100 ----
mean loss: 467.89
 ---- batch: 110 ----
mean loss: 504.08
train mean loss: 479.68
epoch train time: 0:00:02.178270
elapsed time: 0:02:46.474117
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 00:49:15.784628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.94
 ---- batch: 020 ----
mean loss: 478.55
 ---- batch: 030 ----
mean loss: 470.15
 ---- batch: 040 ----
mean loss: 479.80
 ---- batch: 050 ----
mean loss: 490.90
 ---- batch: 060 ----
mean loss: 480.20
 ---- batch: 070 ----
mean loss: 463.91
 ---- batch: 080 ----
mean loss: 482.24
 ---- batch: 090 ----
mean loss: 482.40
 ---- batch: 100 ----
mean loss: 482.05
 ---- batch: 110 ----
mean loss: 486.29
train mean loss: 480.00
epoch train time: 0:00:02.190330
elapsed time: 0:02:48.664627
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 00:49:17.975212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.18
 ---- batch: 020 ----
mean loss: 461.96
 ---- batch: 030 ----
mean loss: 474.98
 ---- batch: 040 ----
mean loss: 462.10
 ---- batch: 050 ----
mean loss: 470.47
 ---- batch: 060 ----
mean loss: 477.43
 ---- batch: 070 ----
mean loss: 497.56
 ---- batch: 080 ----
mean loss: 481.51
 ---- batch: 090 ----
mean loss: 470.12
 ---- batch: 100 ----
mean loss: 486.82
 ---- batch: 110 ----
mean loss: 495.13
train mean loss: 477.87
epoch train time: 0:00:02.189865
elapsed time: 0:02:50.854798
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 00:49:20.165313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.14
 ---- batch: 020 ----
mean loss: 500.85
 ---- batch: 030 ----
mean loss: 488.40
 ---- batch: 040 ----
mean loss: 473.69
 ---- batch: 050 ----
mean loss: 477.09
 ---- batch: 060 ----
mean loss: 480.28
 ---- batch: 070 ----
mean loss: 488.93
 ---- batch: 080 ----
mean loss: 482.68
 ---- batch: 090 ----
mean loss: 484.57
 ---- batch: 100 ----
mean loss: 496.75
 ---- batch: 110 ----
mean loss: 482.08
train mean loss: 485.64
epoch train time: 0:00:02.191109
elapsed time: 0:02:53.046077
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 00:49:22.356589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.81
 ---- batch: 020 ----
mean loss: 484.13
 ---- batch: 030 ----
mean loss: 473.15
 ---- batch: 040 ----
mean loss: 497.78
 ---- batch: 050 ----
mean loss: 493.78
 ---- batch: 060 ----
mean loss: 480.39
 ---- batch: 070 ----
mean loss: 488.28
 ---- batch: 080 ----
mean loss: 481.07
 ---- batch: 090 ----
mean loss: 467.05
 ---- batch: 100 ----
mean loss: 465.13
 ---- batch: 110 ----
mean loss: 486.09
train mean loss: 483.91
epoch train time: 0:00:02.192268
elapsed time: 0:02:55.238534
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 00:49:24.549042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.93
 ---- batch: 020 ----
mean loss: 501.07
 ---- batch: 030 ----
mean loss: 481.99
 ---- batch: 040 ----
mean loss: 481.15
 ---- batch: 050 ----
mean loss: 471.63
 ---- batch: 060 ----
mean loss: 493.73
 ---- batch: 070 ----
mean loss: 476.49
 ---- batch: 080 ----
mean loss: 491.69
 ---- batch: 090 ----
mean loss: 481.63
 ---- batch: 100 ----
mean loss: 465.76
 ---- batch: 110 ----
mean loss: 486.56
train mean loss: 481.92
epoch train time: 0:00:02.184869
elapsed time: 0:02:57.423636
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 00:49:26.734166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.92
 ---- batch: 020 ----
mean loss: 472.29
 ---- batch: 030 ----
mean loss: 470.36
 ---- batch: 040 ----
mean loss: 477.55
 ---- batch: 050 ----
mean loss: 497.19
 ---- batch: 060 ----
mean loss: 494.75
 ---- batch: 070 ----
mean loss: 489.88
 ---- batch: 080 ----
mean loss: 495.12
 ---- batch: 090 ----
mean loss: 488.35
 ---- batch: 100 ----
mean loss: 484.48
 ---- batch: 110 ----
mean loss: 489.41
train mean loss: 485.73
epoch train time: 0:00:02.178627
elapsed time: 0:02:59.602469
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 00:49:28.912978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.77
 ---- batch: 020 ----
mean loss: 465.72
 ---- batch: 030 ----
mean loss: 493.56
 ---- batch: 040 ----
mean loss: 469.51
 ---- batch: 050 ----
mean loss: 479.84
 ---- batch: 060 ----
mean loss: 491.05
 ---- batch: 070 ----
mean loss: 496.01
 ---- batch: 080 ----
mean loss: 481.66
 ---- batch: 090 ----
mean loss: 480.14
 ---- batch: 100 ----
mean loss: 471.46
 ---- batch: 110 ----
mean loss: 472.68
train mean loss: 479.74
epoch train time: 0:00:02.184847
elapsed time: 0:03:01.787490
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 00:49:31.098000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.06
 ---- batch: 020 ----
mean loss: 463.29
 ---- batch: 030 ----
mean loss: 483.41
 ---- batch: 040 ----
mean loss: 466.21
 ---- batch: 050 ----
mean loss: 478.30
 ---- batch: 060 ----
mean loss: 479.15
 ---- batch: 070 ----
mean loss: 477.23
 ---- batch: 080 ----
mean loss: 470.51
 ---- batch: 090 ----
mean loss: 484.07
 ---- batch: 100 ----
mean loss: 468.83
 ---- batch: 110 ----
mean loss: 478.59
train mean loss: 475.31
epoch train time: 0:00:02.179109
elapsed time: 0:03:03.966758
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 00:49:33.277296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.67
 ---- batch: 020 ----
mean loss: 473.50
 ---- batch: 030 ----
mean loss: 484.67
 ---- batch: 040 ----
mean loss: 474.88
 ---- batch: 050 ----
mean loss: 481.87
 ---- batch: 060 ----
mean loss: 483.26
 ---- batch: 070 ----
mean loss: 492.39
 ---- batch: 080 ----
mean loss: 474.90
 ---- batch: 090 ----
mean loss: 478.20
 ---- batch: 100 ----
mean loss: 486.83
 ---- batch: 110 ----
mean loss: 480.58
train mean loss: 481.65
epoch train time: 0:00:02.178061
elapsed time: 0:03:06.145016
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 00:49:35.455523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.82
 ---- batch: 020 ----
mean loss: 490.95
 ---- batch: 030 ----
mean loss: 490.68
 ---- batch: 040 ----
mean loss: 470.14
 ---- batch: 050 ----
mean loss: 473.30
 ---- batch: 060 ----
mean loss: 481.45
 ---- batch: 070 ----
mean loss: 475.63
 ---- batch: 080 ----
mean loss: 503.54
 ---- batch: 090 ----
mean loss: 477.57
 ---- batch: 100 ----
mean loss: 474.69
 ---- batch: 110 ----
mean loss: 486.94
train mean loss: 481.40
epoch train time: 0:00:02.182854
elapsed time: 0:03:08.328060
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 00:49:37.638571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.03
 ---- batch: 020 ----
mean loss: 469.13
 ---- batch: 030 ----
mean loss: 470.14
 ---- batch: 040 ----
mean loss: 482.41
 ---- batch: 050 ----
mean loss: 479.17
 ---- batch: 060 ----
mean loss: 475.54
 ---- batch: 070 ----
mean loss: 464.66
 ---- batch: 080 ----
mean loss: 492.07
 ---- batch: 090 ----
mean loss: 477.15
 ---- batch: 100 ----
mean loss: 477.63
 ---- batch: 110 ----
mean loss: 466.88
train mean loss: 475.01
epoch train time: 0:00:02.174075
elapsed time: 0:03:10.502322
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 00:49:39.812831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.37
 ---- batch: 020 ----
mean loss: 494.80
 ---- batch: 030 ----
mean loss: 479.11
 ---- batch: 040 ----
mean loss: 460.36
 ---- batch: 050 ----
mean loss: 469.22
 ---- batch: 060 ----
mean loss: 478.08
 ---- batch: 070 ----
mean loss: 468.41
 ---- batch: 080 ----
mean loss: 499.02
 ---- batch: 090 ----
mean loss: 491.97
 ---- batch: 100 ----
mean loss: 473.26
 ---- batch: 110 ----
mean loss: 478.85
train mean loss: 480.12
epoch train time: 0:00:02.153386
elapsed time: 0:03:12.655867
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 00:49:41.966374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.00
 ---- batch: 020 ----
mean loss: 471.86
 ---- batch: 030 ----
mean loss: 473.39
 ---- batch: 040 ----
mean loss: 465.60
 ---- batch: 050 ----
mean loss: 463.98
 ---- batch: 060 ----
mean loss: 483.30
 ---- batch: 070 ----
mean loss: 469.54
 ---- batch: 080 ----
mean loss: 483.06
 ---- batch: 090 ----
mean loss: 466.92
 ---- batch: 100 ----
mean loss: 476.69
 ---- batch: 110 ----
mean loss: 493.95
train mean loss: 474.45
epoch train time: 0:00:02.158603
elapsed time: 0:03:14.814631
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 00:49:44.125139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.92
 ---- batch: 020 ----
mean loss: 481.77
 ---- batch: 030 ----
mean loss: 489.03
 ---- batch: 040 ----
mean loss: 475.50
 ---- batch: 050 ----
mean loss: 488.14
 ---- batch: 060 ----
mean loss: 477.83
 ---- batch: 070 ----
mean loss: 471.01
 ---- batch: 080 ----
mean loss: 477.26
 ---- batch: 090 ----
mean loss: 492.01
 ---- batch: 100 ----
mean loss: 488.41
 ---- batch: 110 ----
mean loss: 470.76
train mean loss: 480.99
epoch train time: 0:00:02.155207
elapsed time: 0:03:16.969986
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 00:49:46.280508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.86
 ---- batch: 020 ----
mean loss: 463.99
 ---- batch: 030 ----
mean loss: 479.25
 ---- batch: 040 ----
mean loss: 486.53
 ---- batch: 050 ----
mean loss: 482.01
 ---- batch: 060 ----
mean loss: 475.44
 ---- batch: 070 ----
mean loss: 481.51
 ---- batch: 080 ----
mean loss: 468.00
 ---- batch: 090 ----
mean loss: 463.09
 ---- batch: 100 ----
mean loss: 493.00
 ---- batch: 110 ----
mean loss: 482.13
train mean loss: 476.55
epoch train time: 0:00:02.158113
elapsed time: 0:03:19.128276
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 00:49:48.438783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.57
 ---- batch: 020 ----
mean loss: 471.90
 ---- batch: 030 ----
mean loss: 462.51
 ---- batch: 040 ----
mean loss: 466.45
 ---- batch: 050 ----
mean loss: 497.89
 ---- batch: 060 ----
mean loss: 487.93
 ---- batch: 070 ----
mean loss: 467.61
 ---- batch: 080 ----
mean loss: 479.35
 ---- batch: 090 ----
mean loss: 482.68
 ---- batch: 100 ----
mean loss: 479.98
 ---- batch: 110 ----
mean loss: 473.65
train mean loss: 476.91
epoch train time: 0:00:02.154230
elapsed time: 0:03:21.282645
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 00:49:50.593147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.58
 ---- batch: 020 ----
mean loss: 474.50
 ---- batch: 030 ----
mean loss: 473.15
 ---- batch: 040 ----
mean loss: 476.04
 ---- batch: 050 ----
mean loss: 471.88
 ---- batch: 060 ----
mean loss: 480.48
 ---- batch: 070 ----
mean loss: 479.50
 ---- batch: 080 ----
mean loss: 483.97
 ---- batch: 090 ----
mean loss: 464.17
 ---- batch: 100 ----
mean loss: 474.02
 ---- batch: 110 ----
mean loss: 472.15
train mean loss: 475.23
epoch train time: 0:00:02.137572
elapsed time: 0:03:23.420370
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 00:49:52.730906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.31
 ---- batch: 020 ----
mean loss: 485.57
 ---- batch: 030 ----
mean loss: 475.52
 ---- batch: 040 ----
mean loss: 476.81
 ---- batch: 050 ----
mean loss: 481.22
 ---- batch: 060 ----
mean loss: 478.71
 ---- batch: 070 ----
mean loss: 471.56
 ---- batch: 080 ----
mean loss: 472.82
 ---- batch: 090 ----
mean loss: 488.59
 ---- batch: 100 ----
mean loss: 458.70
 ---- batch: 110 ----
mean loss: 494.27
train mean loss: 479.39
epoch train time: 0:00:02.151280
elapsed time: 0:03:25.571838
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 00:49:54.882344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.80
 ---- batch: 020 ----
mean loss: 500.24
 ---- batch: 030 ----
mean loss: 472.95
 ---- batch: 040 ----
mean loss: 471.19
 ---- batch: 050 ----
mean loss: 476.81
 ---- batch: 060 ----
mean loss: 473.35
 ---- batch: 070 ----
mean loss: 485.80
 ---- batch: 080 ----
mean loss: 480.24
 ---- batch: 090 ----
mean loss: 486.33
 ---- batch: 100 ----
mean loss: 496.24
 ---- batch: 110 ----
mean loss: 455.10
train mean loss: 478.71
epoch train time: 0:00:02.146676
elapsed time: 0:03:27.718656
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 00:49:57.029159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.47
 ---- batch: 020 ----
mean loss: 471.05
 ---- batch: 030 ----
mean loss: 464.89
 ---- batch: 040 ----
mean loss: 460.52
 ---- batch: 050 ----
mean loss: 477.34
 ---- batch: 060 ----
mean loss: 478.53
 ---- batch: 070 ----
mean loss: 469.59
 ---- batch: 080 ----
mean loss: 485.45
 ---- batch: 090 ----
mean loss: 477.38
 ---- batch: 100 ----
mean loss: 471.77
 ---- batch: 110 ----
mean loss: 475.53
train mean loss: 473.06
epoch train time: 0:00:02.141801
elapsed time: 0:03:29.860617
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 00:49:59.171119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.66
 ---- batch: 020 ----
mean loss: 483.26
 ---- batch: 030 ----
mean loss: 476.08
 ---- batch: 040 ----
mean loss: 484.66
 ---- batch: 050 ----
mean loss: 472.50
 ---- batch: 060 ----
mean loss: 487.59
 ---- batch: 070 ----
mean loss: 473.63
 ---- batch: 080 ----
mean loss: 466.66
 ---- batch: 090 ----
mean loss: 466.28
 ---- batch: 100 ----
mean loss: 476.60
 ---- batch: 110 ----
mean loss: 477.63
train mean loss: 476.79
epoch train time: 0:00:02.140775
elapsed time: 0:03:32.001551
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 00:50:01.312082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.97
 ---- batch: 020 ----
mean loss: 479.24
 ---- batch: 030 ----
mean loss: 477.89
 ---- batch: 040 ----
mean loss: 477.39
 ---- batch: 050 ----
mean loss: 490.53
 ---- batch: 060 ----
mean loss: 477.97
 ---- batch: 070 ----
mean loss: 464.05
 ---- batch: 080 ----
mean loss: 458.97
 ---- batch: 090 ----
mean loss: 469.56
 ---- batch: 100 ----
mean loss: 469.75
 ---- batch: 110 ----
mean loss: 474.62
train mean loss: 474.38
epoch train time: 0:00:02.140876
elapsed time: 0:03:34.142592
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 00:50:03.453092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.98
 ---- batch: 020 ----
mean loss: 475.33
 ---- batch: 030 ----
mean loss: 475.83
 ---- batch: 040 ----
mean loss: 460.72
 ---- batch: 050 ----
mean loss: 469.97
 ---- batch: 060 ----
mean loss: 478.18
 ---- batch: 070 ----
mean loss: 467.77
 ---- batch: 080 ----
mean loss: 484.20
 ---- batch: 090 ----
mean loss: 481.62
 ---- batch: 100 ----
mean loss: 476.37
 ---- batch: 110 ----
mean loss: 472.42
train mean loss: 475.42
epoch train time: 0:00:02.133783
elapsed time: 0:03:36.276532
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 00:50:05.587037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.48
 ---- batch: 020 ----
mean loss: 490.01
 ---- batch: 030 ----
mean loss: 471.88
 ---- batch: 040 ----
mean loss: 482.23
 ---- batch: 050 ----
mean loss: 483.72
 ---- batch: 060 ----
mean loss: 472.09
 ---- batch: 070 ----
mean loss: 474.04
 ---- batch: 080 ----
mean loss: 490.92
 ---- batch: 090 ----
mean loss: 471.68
 ---- batch: 100 ----
mean loss: 466.97
 ---- batch: 110 ----
mean loss: 477.29
train mean loss: 474.56
epoch train time: 0:00:02.128805
elapsed time: 0:03:38.405489
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 00:50:07.716002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.99
 ---- batch: 020 ----
mean loss: 486.82
 ---- batch: 030 ----
mean loss: 478.41
 ---- batch: 040 ----
mean loss: 459.55
 ---- batch: 050 ----
mean loss: 474.76
 ---- batch: 060 ----
mean loss: 481.24
 ---- batch: 070 ----
mean loss: 464.65
 ---- batch: 080 ----
mean loss: 461.26
 ---- batch: 090 ----
mean loss: 495.62
 ---- batch: 100 ----
mean loss: 497.86
 ---- batch: 110 ----
mean loss: 480.03
train mean loss: 477.19
epoch train time: 0:00:02.138251
elapsed time: 0:03:40.543896
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 00:50:09.854402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.56
 ---- batch: 020 ----
mean loss: 480.42
 ---- batch: 030 ----
mean loss: 483.39
 ---- batch: 040 ----
mean loss: 478.97
 ---- batch: 050 ----
mean loss: 485.23
 ---- batch: 060 ----
mean loss: 478.82
 ---- batch: 070 ----
mean loss: 462.66
 ---- batch: 080 ----
mean loss: 484.99
 ---- batch: 090 ----
mean loss: 481.37
 ---- batch: 100 ----
mean loss: 466.51
 ---- batch: 110 ----
mean loss: 468.58
train mean loss: 476.87
epoch train time: 0:00:02.143824
elapsed time: 0:03:42.687870
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 00:50:11.998375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.74
 ---- batch: 020 ----
mean loss: 474.81
 ---- batch: 030 ----
mean loss: 468.62
 ---- batch: 040 ----
mean loss: 481.81
 ---- batch: 050 ----
mean loss: 477.32
 ---- batch: 060 ----
mean loss: 476.04
 ---- batch: 070 ----
mean loss: 470.78
 ---- batch: 080 ----
mean loss: 478.36
 ---- batch: 090 ----
mean loss: 471.30
 ---- batch: 100 ----
mean loss: 471.07
 ---- batch: 110 ----
mean loss: 487.65
train mean loss: 475.07
epoch train time: 0:00:02.137787
elapsed time: 0:03:44.825804
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 00:50:14.136329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.76
 ---- batch: 020 ----
mean loss: 476.02
 ---- batch: 030 ----
mean loss: 472.32
 ---- batch: 040 ----
mean loss: 488.31
 ---- batch: 050 ----
mean loss: 469.78
 ---- batch: 060 ----
mean loss: 461.94
 ---- batch: 070 ----
mean loss: 476.24
 ---- batch: 080 ----
mean loss: 480.90
 ---- batch: 090 ----
mean loss: 479.92
 ---- batch: 100 ----
mean loss: 478.56
 ---- batch: 110 ----
mean loss: 471.72
train mean loss: 475.69
epoch train time: 0:00:02.150961
elapsed time: 0:03:46.976944
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 00:50:16.287477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.42
 ---- batch: 020 ----
mean loss: 476.00
 ---- batch: 030 ----
mean loss: 470.54
 ---- batch: 040 ----
mean loss: 486.40
 ---- batch: 050 ----
mean loss: 481.05
 ---- batch: 060 ----
mean loss: 481.25
 ---- batch: 070 ----
mean loss: 472.45
 ---- batch: 080 ----
mean loss: 473.26
 ---- batch: 090 ----
mean loss: 470.08
 ---- batch: 100 ----
mean loss: 485.19
 ---- batch: 110 ----
mean loss: 483.30
train mean loss: 475.25
epoch train time: 0:00:02.147392
elapsed time: 0:03:49.124528
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 00:50:18.435033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.57
 ---- batch: 020 ----
mean loss: 487.06
 ---- batch: 030 ----
mean loss: 494.64
 ---- batch: 040 ----
mean loss: 462.13
 ---- batch: 050 ----
mean loss: 476.80
 ---- batch: 060 ----
mean loss: 489.24
 ---- batch: 070 ----
mean loss: 468.79
 ---- batch: 080 ----
mean loss: 467.19
 ---- batch: 090 ----
mean loss: 467.09
 ---- batch: 100 ----
mean loss: 467.84
 ---- batch: 110 ----
mean loss: 478.31
train mean loss: 474.44
epoch train time: 0:00:02.148805
elapsed time: 0:03:51.273469
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 00:50:20.583977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.49
 ---- batch: 020 ----
mean loss: 484.59
 ---- batch: 030 ----
mean loss: 485.60
 ---- batch: 040 ----
mean loss: 485.35
 ---- batch: 050 ----
mean loss: 477.93
 ---- batch: 060 ----
mean loss: 470.62
 ---- batch: 070 ----
mean loss: 468.77
 ---- batch: 080 ----
mean loss: 462.97
 ---- batch: 090 ----
mean loss: 463.74
 ---- batch: 100 ----
mean loss: 459.39
 ---- batch: 110 ----
mean loss: 471.18
train mean loss: 474.87
epoch train time: 0:00:02.170844
elapsed time: 0:03:53.444471
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 00:50:22.754977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.04
 ---- batch: 020 ----
mean loss: 492.11
 ---- batch: 030 ----
mean loss: 486.52
 ---- batch: 040 ----
mean loss: 480.87
 ---- batch: 050 ----
mean loss: 469.42
 ---- batch: 060 ----
mean loss: 462.61
 ---- batch: 070 ----
mean loss: 473.40
 ---- batch: 080 ----
mean loss: 467.89
 ---- batch: 090 ----
mean loss: 464.98
 ---- batch: 100 ----
mean loss: 478.06
 ---- batch: 110 ----
mean loss: 470.47
train mean loss: 474.25
epoch train time: 0:00:02.178657
elapsed time: 0:03:55.623301
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 00:50:24.933813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.19
 ---- batch: 020 ----
mean loss: 472.80
 ---- batch: 030 ----
mean loss: 479.81
 ---- batch: 040 ----
mean loss: 467.22
 ---- batch: 050 ----
mean loss: 486.75
 ---- batch: 060 ----
mean loss: 471.03
 ---- batch: 070 ----
mean loss: 484.42
 ---- batch: 080 ----
mean loss: 466.51
 ---- batch: 090 ----
mean loss: 470.66
 ---- batch: 100 ----
mean loss: 476.92
 ---- batch: 110 ----
mean loss: 466.35
train mean loss: 474.86
epoch train time: 0:00:02.177628
elapsed time: 0:03:57.801098
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 00:50:27.111615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.76
 ---- batch: 020 ----
mean loss: 476.75
 ---- batch: 030 ----
mean loss: 465.59
 ---- batch: 040 ----
mean loss: 469.64
 ---- batch: 050 ----
mean loss: 476.02
 ---- batch: 060 ----
mean loss: 467.85
 ---- batch: 070 ----
mean loss: 482.15
 ---- batch: 080 ----
mean loss: 479.49
 ---- batch: 090 ----
mean loss: 467.86
 ---- batch: 100 ----
mean loss: 460.07
 ---- batch: 110 ----
mean loss: 465.07
train mean loss: 471.07
epoch train time: 0:00:02.166585
elapsed time: 0:03:59.967934
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 00:50:29.278475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.39
 ---- batch: 020 ----
mean loss: 486.85
 ---- batch: 030 ----
mean loss: 468.64
 ---- batch: 040 ----
mean loss: 464.96
 ---- batch: 050 ----
mean loss: 481.52
 ---- batch: 060 ----
mean loss: 479.87
 ---- batch: 070 ----
mean loss: 472.22
 ---- batch: 080 ----
mean loss: 486.12
 ---- batch: 090 ----
mean loss: 474.50
 ---- batch: 100 ----
mean loss: 471.53
 ---- batch: 110 ----
mean loss: 462.67
train mean loss: 475.48
epoch train time: 0:00:02.178297
elapsed time: 0:04:02.146446
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 00:50:31.456957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.25
 ---- batch: 020 ----
mean loss: 462.19
 ---- batch: 030 ----
mean loss: 443.74
 ---- batch: 040 ----
mean loss: 468.82
 ---- batch: 050 ----
mean loss: 484.26
 ---- batch: 060 ----
mean loss: 487.47
 ---- batch: 070 ----
mean loss: 477.03
 ---- batch: 080 ----
mean loss: 474.44
 ---- batch: 090 ----
mean loss: 471.71
 ---- batch: 100 ----
mean loss: 471.51
 ---- batch: 110 ----
mean loss: 475.13
train mean loss: 472.38
epoch train time: 0:00:02.183077
elapsed time: 0:04:04.329709
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 00:50:33.640218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.04
 ---- batch: 020 ----
mean loss: 481.84
 ---- batch: 030 ----
mean loss: 483.18
 ---- batch: 040 ----
mean loss: 476.47
 ---- batch: 050 ----
mean loss: 484.91
 ---- batch: 060 ----
mean loss: 465.95
 ---- batch: 070 ----
mean loss: 489.37
 ---- batch: 080 ----
mean loss: 480.75
 ---- batch: 090 ----
mean loss: 468.87
 ---- batch: 100 ----
mean loss: 477.97
 ---- batch: 110 ----
mean loss: 474.59
train mean loss: 477.06
epoch train time: 0:00:02.177879
elapsed time: 0:04:06.507764
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 00:50:35.818282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.92
 ---- batch: 020 ----
mean loss: 458.51
 ---- batch: 030 ----
mean loss: 464.79
 ---- batch: 040 ----
mean loss: 463.94
 ---- batch: 050 ----
mean loss: 461.32
 ---- batch: 060 ----
mean loss: 468.53
 ---- batch: 070 ----
mean loss: 473.96
 ---- batch: 080 ----
mean loss: 476.70
 ---- batch: 090 ----
mean loss: 468.55
 ---- batch: 100 ----
mean loss: 471.26
 ---- batch: 110 ----
mean loss: 477.97
train mean loss: 467.78
epoch train time: 0:00:02.184703
elapsed time: 0:04:08.692663
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 00:50:38.003172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.91
 ---- batch: 020 ----
mean loss: 462.89
 ---- batch: 030 ----
mean loss: 478.36
 ---- batch: 040 ----
mean loss: 465.74
 ---- batch: 050 ----
mean loss: 463.66
 ---- batch: 060 ----
mean loss: 464.55
 ---- batch: 070 ----
mean loss: 468.39
 ---- batch: 080 ----
mean loss: 469.27
 ---- batch: 090 ----
mean loss: 469.00
 ---- batch: 100 ----
mean loss: 480.17
 ---- batch: 110 ----
mean loss: 488.61
train mean loss: 470.34
epoch train time: 0:00:02.174410
elapsed time: 0:04:10.867341
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 00:50:40.178889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.32
 ---- batch: 020 ----
mean loss: 468.10
 ---- batch: 030 ----
mean loss: 484.52
 ---- batch: 040 ----
mean loss: 462.75
 ---- batch: 050 ----
mean loss: 469.49
 ---- batch: 060 ----
mean loss: 466.48
 ---- batch: 070 ----
mean loss: 467.26
 ---- batch: 080 ----
mean loss: 472.15
 ---- batch: 090 ----
mean loss: 463.52
 ---- batch: 100 ----
mean loss: 461.29
 ---- batch: 110 ----
mean loss: 474.10
train mean loss: 468.52
epoch train time: 0:00:02.179845
elapsed time: 0:04:13.048403
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 00:50:42.358909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.92
 ---- batch: 020 ----
mean loss: 464.94
 ---- batch: 030 ----
mean loss: 470.73
 ---- batch: 040 ----
mean loss: 468.20
 ---- batch: 050 ----
mean loss: 458.00
 ---- batch: 060 ----
mean loss: 463.96
 ---- batch: 070 ----
mean loss: 476.26
 ---- batch: 080 ----
mean loss: 469.32
 ---- batch: 090 ----
mean loss: 469.55
 ---- batch: 100 ----
mean loss: 470.69
 ---- batch: 110 ----
mean loss: 461.09
train mean loss: 467.94
epoch train time: 0:00:02.179260
elapsed time: 0:04:15.227852
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 00:50:44.538394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.26
 ---- batch: 020 ----
mean loss: 458.09
 ---- batch: 030 ----
mean loss: 473.41
 ---- batch: 040 ----
mean loss: 477.65
 ---- batch: 050 ----
mean loss: 468.17
 ---- batch: 060 ----
mean loss: 473.53
 ---- batch: 070 ----
mean loss: 465.18
 ---- batch: 080 ----
mean loss: 476.35
 ---- batch: 090 ----
mean loss: 470.77
 ---- batch: 100 ----
mean loss: 465.26
 ---- batch: 110 ----
mean loss: 456.44
train mean loss: 467.56
epoch train time: 0:00:02.178151
elapsed time: 0:04:17.406208
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 00:50:46.716733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.61
 ---- batch: 020 ----
mean loss: 479.50
 ---- batch: 030 ----
mean loss: 454.09
 ---- batch: 040 ----
mean loss: 480.95
 ---- batch: 050 ----
mean loss: 468.41
 ---- batch: 060 ----
mean loss: 479.42
 ---- batch: 070 ----
mean loss: 477.27
 ---- batch: 080 ----
mean loss: 461.33
 ---- batch: 090 ----
mean loss: 469.71
 ---- batch: 100 ----
mean loss: 468.08
 ---- batch: 110 ----
mean loss: 482.39
train mean loss: 470.70
epoch train time: 0:00:02.181877
elapsed time: 0:04:19.588273
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 00:50:48.898780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.29
 ---- batch: 020 ----
mean loss: 464.64
 ---- batch: 030 ----
mean loss: 470.88
 ---- batch: 040 ----
mean loss: 468.63
 ---- batch: 050 ----
mean loss: 471.57
 ---- batch: 060 ----
mean loss: 452.83
 ---- batch: 070 ----
mean loss: 472.65
 ---- batch: 080 ----
mean loss: 467.11
 ---- batch: 090 ----
mean loss: 465.39
 ---- batch: 100 ----
mean loss: 473.26
 ---- batch: 110 ----
mean loss: 467.27
train mean loss: 466.02
epoch train time: 0:00:02.179482
elapsed time: 0:04:21.767935
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 00:50:51.078446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.88
 ---- batch: 020 ----
mean loss: 473.34
 ---- batch: 030 ----
mean loss: 466.62
 ---- batch: 040 ----
mean loss: 463.24
 ---- batch: 050 ----
mean loss: 471.98
 ---- batch: 060 ----
mean loss: 467.02
 ---- batch: 070 ----
mean loss: 472.53
 ---- batch: 080 ----
mean loss: 486.27
 ---- batch: 090 ----
mean loss: 479.78
 ---- batch: 100 ----
mean loss: 484.87
 ---- batch: 110 ----
mean loss: 481.49
train mean loss: 473.99
epoch train time: 0:00:02.172289
elapsed time: 0:04:23.940395
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 00:50:53.250889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.88
 ---- batch: 020 ----
mean loss: 474.68
 ---- batch: 030 ----
mean loss: 469.54
 ---- batch: 040 ----
mean loss: 461.24
 ---- batch: 050 ----
mean loss: 460.23
 ---- batch: 060 ----
mean loss: 456.38
 ---- batch: 070 ----
mean loss: 457.13
 ---- batch: 080 ----
mean loss: 479.52
 ---- batch: 090 ----
mean loss: 479.70
 ---- batch: 100 ----
mean loss: 456.05
 ---- batch: 110 ----
mean loss: 476.34
train mean loss: 466.88
epoch train time: 0:00:02.178174
elapsed time: 0:04:26.118737
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 00:50:55.429251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.28
 ---- batch: 020 ----
mean loss: 474.20
 ---- batch: 030 ----
mean loss: 471.28
 ---- batch: 040 ----
mean loss: 467.33
 ---- batch: 050 ----
mean loss: 477.89
 ---- batch: 060 ----
mean loss: 479.96
 ---- batch: 070 ----
mean loss: 476.43
 ---- batch: 080 ----
mean loss: 473.75
 ---- batch: 090 ----
mean loss: 486.35
 ---- batch: 100 ----
mean loss: 465.14
 ---- batch: 110 ----
mean loss: 459.43
train mean loss: 471.72
epoch train time: 0:00:02.181364
elapsed time: 0:04:28.300285
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 00:50:57.610824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.25
 ---- batch: 020 ----
mean loss: 475.18
 ---- batch: 030 ----
mean loss: 468.09
 ---- batch: 040 ----
mean loss: 455.45
 ---- batch: 050 ----
mean loss: 464.46
 ---- batch: 060 ----
mean loss: 466.32
 ---- batch: 070 ----
mean loss: 477.80
 ---- batch: 080 ----
mean loss: 467.86
 ---- batch: 090 ----
mean loss: 476.86
 ---- batch: 100 ----
mean loss: 468.47
 ---- batch: 110 ----
mean loss: 472.18
train mean loss: 469.10
epoch train time: 0:00:02.180989
elapsed time: 0:04:30.481482
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 00:50:59.792005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.25
 ---- batch: 020 ----
mean loss: 452.35
 ---- batch: 030 ----
mean loss: 473.54
 ---- batch: 040 ----
mean loss: 480.86
 ---- batch: 050 ----
mean loss: 462.06
 ---- batch: 060 ----
mean loss: 453.74
 ---- batch: 070 ----
mean loss: 477.42
 ---- batch: 080 ----
mean loss: 469.50
 ---- batch: 090 ----
mean loss: 477.11
 ---- batch: 100 ----
mean loss: 470.36
 ---- batch: 110 ----
mean loss: 462.74
train mean loss: 467.75
epoch train time: 0:00:02.179593
elapsed time: 0:04:32.661246
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 00:51:01.971757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.48
 ---- batch: 020 ----
mean loss: 466.74
 ---- batch: 030 ----
mean loss: 467.28
 ---- batch: 040 ----
mean loss: 458.09
 ---- batch: 050 ----
mean loss: 466.96
 ---- batch: 060 ----
mean loss: 466.82
 ---- batch: 070 ----
mean loss: 462.35
 ---- batch: 080 ----
mean loss: 465.59
 ---- batch: 090 ----
mean loss: 466.37
 ---- batch: 100 ----
mean loss: 488.47
 ---- batch: 110 ----
mean loss: 475.17
train mean loss: 467.50
epoch train time: 0:00:02.182518
elapsed time: 0:04:34.843957
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 00:51:04.154468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.49
 ---- batch: 020 ----
mean loss: 476.65
 ---- batch: 030 ----
mean loss: 471.03
 ---- batch: 040 ----
mean loss: 469.08
 ---- batch: 050 ----
mean loss: 452.14
 ---- batch: 060 ----
mean loss: 465.13
 ---- batch: 070 ----
mean loss: 456.77
 ---- batch: 080 ----
mean loss: 453.11
 ---- batch: 090 ----
mean loss: 466.39
 ---- batch: 100 ----
mean loss: 496.26
 ---- batch: 110 ----
mean loss: 459.54
train mean loss: 466.69
epoch train time: 0:00:02.183552
elapsed time: 0:04:37.027691
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 00:51:06.338206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.32
 ---- batch: 020 ----
mean loss: 466.93
 ---- batch: 030 ----
mean loss: 475.31
 ---- batch: 040 ----
mean loss: 461.84
 ---- batch: 050 ----
mean loss: 466.18
 ---- batch: 060 ----
mean loss: 461.05
 ---- batch: 070 ----
mean loss: 463.41
 ---- batch: 080 ----
mean loss: 456.36
 ---- batch: 090 ----
mean loss: 469.33
 ---- batch: 100 ----
mean loss: 460.02
 ---- batch: 110 ----
mean loss: 472.31
train mean loss: 465.17
epoch train time: 0:00:02.180609
elapsed time: 0:04:39.208465
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 00:51:08.518972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.72
 ---- batch: 020 ----
mean loss: 481.03
 ---- batch: 030 ----
mean loss: 450.04
 ---- batch: 040 ----
mean loss: 475.76
 ---- batch: 050 ----
mean loss: 473.99
 ---- batch: 060 ----
mean loss: 470.96
 ---- batch: 070 ----
mean loss: 466.29
 ---- batch: 080 ----
mean loss: 456.12
 ---- batch: 090 ----
mean loss: 458.54
 ---- batch: 100 ----
mean loss: 475.48
 ---- batch: 110 ----
mean loss: 468.13
train mean loss: 468.48
epoch train time: 0:00:02.179220
elapsed time: 0:04:41.387849
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 00:51:10.698355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.24
 ---- batch: 020 ----
mean loss: 472.55
 ---- batch: 030 ----
mean loss: 472.84
 ---- batch: 040 ----
mean loss: 451.55
 ---- batch: 050 ----
mean loss: 459.83
 ---- batch: 060 ----
mean loss: 472.01
 ---- batch: 070 ----
mean loss: 457.58
 ---- batch: 080 ----
mean loss: 472.07
 ---- batch: 090 ----
mean loss: 471.52
 ---- batch: 100 ----
mean loss: 452.98
 ---- batch: 110 ----
mean loss: 467.33
train mean loss: 466.39
epoch train time: 0:00:02.180240
elapsed time: 0:04:43.568291
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 00:51:12.878828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.92
 ---- batch: 020 ----
mean loss: 466.22
 ---- batch: 030 ----
mean loss: 463.20
 ---- batch: 040 ----
mean loss: 468.11
 ---- batch: 050 ----
mean loss: 461.50
 ---- batch: 060 ----
mean loss: 483.48
 ---- batch: 070 ----
mean loss: 474.04
 ---- batch: 080 ----
mean loss: 465.05
 ---- batch: 090 ----
mean loss: 472.06
 ---- batch: 100 ----
mean loss: 467.70
 ---- batch: 110 ----
mean loss: 458.26
train mean loss: 467.54
epoch train time: 0:00:02.184705
elapsed time: 0:04:45.753225
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 00:51:15.063733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.48
 ---- batch: 020 ----
mean loss: 470.45
 ---- batch: 030 ----
mean loss: 476.41
 ---- batch: 040 ----
mean loss: 461.70
 ---- batch: 050 ----
mean loss: 456.24
 ---- batch: 060 ----
mean loss: 471.54
 ---- batch: 070 ----
mean loss: 454.52
 ---- batch: 080 ----
mean loss: 485.03
 ---- batch: 090 ----
mean loss: 456.62
 ---- batch: 100 ----
mean loss: 461.77
 ---- batch: 110 ----
mean loss: 464.22
train mean loss: 466.57
epoch train time: 0:00:02.175123
elapsed time: 0:04:47.928501
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 00:51:17.239007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.54
 ---- batch: 020 ----
mean loss: 465.72
 ---- batch: 030 ----
mean loss: 472.76
 ---- batch: 040 ----
mean loss: 469.42
 ---- batch: 050 ----
mean loss: 465.81
 ---- batch: 060 ----
mean loss: 463.40
 ---- batch: 070 ----
mean loss: 463.12
 ---- batch: 080 ----
mean loss: 474.94
 ---- batch: 090 ----
mean loss: 479.90
 ---- batch: 100 ----
mean loss: 479.99
 ---- batch: 110 ----
mean loss: 457.77
train mean loss: 467.82
epoch train time: 0:00:02.153332
elapsed time: 0:04:50.082017
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 00:51:19.392541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.34
 ---- batch: 020 ----
mean loss: 456.59
 ---- batch: 030 ----
mean loss: 468.55
 ---- batch: 040 ----
mean loss: 449.11
 ---- batch: 050 ----
mean loss: 478.30
 ---- batch: 060 ----
mean loss: 471.88
 ---- batch: 070 ----
mean loss: 469.08
 ---- batch: 080 ----
mean loss: 479.23
 ---- batch: 090 ----
mean loss: 478.10
 ---- batch: 100 ----
mean loss: 456.12
 ---- batch: 110 ----
mean loss: 473.55
train mean loss: 466.34
epoch train time: 0:00:02.165208
elapsed time: 0:04:52.247421
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 00:51:21.557944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.94
 ---- batch: 020 ----
mean loss: 450.41
 ---- batch: 030 ----
mean loss: 460.09
 ---- batch: 040 ----
mean loss: 474.56
 ---- batch: 050 ----
mean loss: 458.98
 ---- batch: 060 ----
mean loss: 458.43
 ---- batch: 070 ----
mean loss: 465.52
 ---- batch: 080 ----
mean loss: 468.66
 ---- batch: 090 ----
mean loss: 471.67
 ---- batch: 100 ----
mean loss: 465.30
 ---- batch: 110 ----
mean loss: 452.58
train mean loss: 462.90
epoch train time: 0:00:02.168854
elapsed time: 0:04:54.416509
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 00:51:23.727015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.95
 ---- batch: 020 ----
mean loss: 460.54
 ---- batch: 030 ----
mean loss: 467.08
 ---- batch: 040 ----
mean loss: 486.63
 ---- batch: 050 ----
mean loss: 455.44
 ---- batch: 060 ----
mean loss: 470.02
 ---- batch: 070 ----
mean loss: 469.85
 ---- batch: 080 ----
mean loss: 474.42
 ---- batch: 090 ----
mean loss: 472.41
 ---- batch: 100 ----
mean loss: 459.32
 ---- batch: 110 ----
mean loss: 450.36
train mean loss: 466.26
epoch train time: 0:00:02.158740
elapsed time: 0:04:56.575408
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 00:51:25.885942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.76
 ---- batch: 020 ----
mean loss: 473.12
 ---- batch: 030 ----
mean loss: 468.09
 ---- batch: 040 ----
mean loss: 480.34
 ---- batch: 050 ----
mean loss: 470.93
 ---- batch: 060 ----
mean loss: 455.72
 ---- batch: 070 ----
mean loss: 466.38
 ---- batch: 080 ----
mean loss: 460.45
 ---- batch: 090 ----
mean loss: 442.10
 ---- batch: 100 ----
mean loss: 467.13
 ---- batch: 110 ----
mean loss: 459.08
train mean loss: 465.22
epoch train time: 0:00:02.158685
elapsed time: 0:04:58.734293
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 00:51:28.044803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.62
 ---- batch: 020 ----
mean loss: 453.62
 ---- batch: 030 ----
mean loss: 459.92
 ---- batch: 040 ----
mean loss: 465.19
 ---- batch: 050 ----
mean loss: 462.23
 ---- batch: 060 ----
mean loss: 468.76
 ---- batch: 070 ----
mean loss: 462.15
 ---- batch: 080 ----
mean loss: 471.11
 ---- batch: 090 ----
mean loss: 475.07
 ---- batch: 100 ----
mean loss: 460.11
 ---- batch: 110 ----
mean loss: 458.40
train mean loss: 463.38
epoch train time: 0:00:02.155170
elapsed time: 0:05:00.889638
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 00:51:30.200146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.07
 ---- batch: 020 ----
mean loss: 458.39
 ---- batch: 030 ----
mean loss: 468.66
 ---- batch: 040 ----
mean loss: 462.14
 ---- batch: 050 ----
mean loss: 479.78
 ---- batch: 060 ----
mean loss: 469.71
 ---- batch: 070 ----
mean loss: 465.46
 ---- batch: 080 ----
mean loss: 465.94
 ---- batch: 090 ----
mean loss: 459.94
 ---- batch: 100 ----
mean loss: 458.44
 ---- batch: 110 ----
mean loss: 456.37
train mean loss: 465.21
epoch train time: 0:00:02.163055
elapsed time: 0:05:03.052855
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 00:51:32.363369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.09
 ---- batch: 020 ----
mean loss: 459.03
 ---- batch: 030 ----
mean loss: 479.68
 ---- batch: 040 ----
mean loss: 472.23
 ---- batch: 050 ----
mean loss: 468.26
 ---- batch: 060 ----
mean loss: 455.45
 ---- batch: 070 ----
mean loss: 466.14
 ---- batch: 080 ----
mean loss: 468.71
 ---- batch: 090 ----
mean loss: 470.49
 ---- batch: 100 ----
mean loss: 463.35
 ---- batch: 110 ----
mean loss: 462.24
train mean loss: 465.44
epoch train time: 0:00:02.157328
elapsed time: 0:05:05.210337
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 00:51:34.520844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.28
 ---- batch: 020 ----
mean loss: 481.15
 ---- batch: 030 ----
mean loss: 441.53
 ---- batch: 040 ----
mean loss: 461.98
 ---- batch: 050 ----
mean loss: 457.20
 ---- batch: 060 ----
mean loss: 454.23
 ---- batch: 070 ----
mean loss: 461.44
 ---- batch: 080 ----
mean loss: 465.37
 ---- batch: 090 ----
mean loss: 460.83
 ---- batch: 100 ----
mean loss: 470.02
 ---- batch: 110 ----
mean loss: 461.39
train mean loss: 463.52
epoch train time: 0:00:02.160296
elapsed time: 0:05:07.370834
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 00:51:36.681360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.52
 ---- batch: 020 ----
mean loss: 467.30
 ---- batch: 030 ----
mean loss: 468.45
 ---- batch: 040 ----
mean loss: 465.26
 ---- batch: 050 ----
mean loss: 465.10
 ---- batch: 060 ----
mean loss: 479.94
 ---- batch: 070 ----
mean loss: 453.05
 ---- batch: 080 ----
mean loss: 460.86
 ---- batch: 090 ----
mean loss: 471.12
 ---- batch: 100 ----
mean loss: 462.61
 ---- batch: 110 ----
mean loss: 463.43
train mean loss: 465.08
epoch train time: 0:00:02.156859
elapsed time: 0:05:09.527875
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 00:51:38.838397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.40
 ---- batch: 020 ----
mean loss: 463.90
 ---- batch: 030 ----
mean loss: 484.90
 ---- batch: 040 ----
mean loss: 468.68
 ---- batch: 050 ----
mean loss: 468.93
 ---- batch: 060 ----
mean loss: 463.22
 ---- batch: 070 ----
mean loss: 450.49
 ---- batch: 080 ----
mean loss: 462.26
 ---- batch: 090 ----
mean loss: 453.31
 ---- batch: 100 ----
mean loss: 472.16
 ---- batch: 110 ----
mean loss: 478.65
train mean loss: 465.47
epoch train time: 0:00:02.154256
elapsed time: 0:05:11.682298
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 00:51:40.992825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.97
 ---- batch: 020 ----
mean loss: 472.39
 ---- batch: 030 ----
mean loss: 454.12
 ---- batch: 040 ----
mean loss: 465.97
 ---- batch: 050 ----
mean loss: 456.33
 ---- batch: 060 ----
mean loss: 469.75
 ---- batch: 070 ----
mean loss: 446.61
 ---- batch: 080 ----
mean loss: 440.56
 ---- batch: 090 ----
mean loss: 451.41
 ---- batch: 100 ----
mean loss: 461.59
 ---- batch: 110 ----
mean loss: 467.37
train mean loss: 457.74
epoch train time: 0:00:02.151203
elapsed time: 0:05:13.833685
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 00:51:43.144194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.15
 ---- batch: 020 ----
mean loss: 455.48
 ---- batch: 030 ----
mean loss: 474.78
 ---- batch: 040 ----
mean loss: 457.84
 ---- batch: 050 ----
mean loss: 452.64
 ---- batch: 060 ----
mean loss: 452.44
 ---- batch: 070 ----
mean loss: 475.09
 ---- batch: 080 ----
mean loss: 457.23
 ---- batch: 090 ----
mean loss: 465.11
 ---- batch: 100 ----
mean loss: 450.85
 ---- batch: 110 ----
mean loss: 461.94
train mean loss: 459.97
epoch train time: 0:00:02.154194
elapsed time: 0:05:15.988064
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 00:51:45.298569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.08
 ---- batch: 020 ----
mean loss: 458.55
 ---- batch: 030 ----
mean loss: 479.26
 ---- batch: 040 ----
mean loss: 476.54
 ---- batch: 050 ----
mean loss: 468.73
 ---- batch: 060 ----
mean loss: 460.10
 ---- batch: 070 ----
mean loss: 462.79
 ---- batch: 080 ----
mean loss: 453.88
 ---- batch: 090 ----
mean loss: 461.00
 ---- batch: 100 ----
mean loss: 471.53
 ---- batch: 110 ----
mean loss: 466.47
train mean loss: 464.96
epoch train time: 0:00:02.157383
elapsed time: 0:05:18.145598
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 00:51:47.456103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.04
 ---- batch: 020 ----
mean loss: 471.22
 ---- batch: 030 ----
mean loss: 467.22
 ---- batch: 040 ----
mean loss: 465.46
 ---- batch: 050 ----
mean loss: 453.26
 ---- batch: 060 ----
mean loss: 480.71
 ---- batch: 070 ----
mean loss: 459.88
 ---- batch: 080 ----
mean loss: 457.01
 ---- batch: 090 ----
mean loss: 453.10
 ---- batch: 100 ----
mean loss: 464.44
 ---- batch: 110 ----
mean loss: 451.91
train mean loss: 462.44
epoch train time: 0:00:02.147070
elapsed time: 0:05:20.292819
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 00:51:49.603324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.62
 ---- batch: 020 ----
mean loss: 463.10
 ---- batch: 030 ----
mean loss: 457.94
 ---- batch: 040 ----
mean loss: 457.19
 ---- batch: 050 ----
mean loss: 447.08
 ---- batch: 060 ----
mean loss: 456.00
 ---- batch: 070 ----
mean loss: 460.26
 ---- batch: 080 ----
mean loss: 472.62
 ---- batch: 090 ----
mean loss: 460.52
 ---- batch: 100 ----
mean loss: 454.48
 ---- batch: 110 ----
mean loss: 471.04
train mean loss: 460.14
epoch train time: 0:00:02.163313
elapsed time: 0:05:22.456306
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 00:51:51.766813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.52
 ---- batch: 020 ----
mean loss: 477.58
 ---- batch: 030 ----
mean loss: 461.95
 ---- batch: 040 ----
mean loss: 451.18
 ---- batch: 050 ----
mean loss: 467.15
 ---- batch: 060 ----
mean loss: 476.63
 ---- batch: 070 ----
mean loss: 467.49
 ---- batch: 080 ----
mean loss: 463.98
 ---- batch: 090 ----
mean loss: 461.70
 ---- batch: 100 ----
mean loss: 453.42
 ---- batch: 110 ----
mean loss: 465.97
train mean loss: 464.89
epoch train time: 0:00:02.149789
elapsed time: 0:05:24.606253
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 00:51:53.916783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.92
 ---- batch: 020 ----
mean loss: 471.73
 ---- batch: 030 ----
mean loss: 465.94
 ---- batch: 040 ----
mean loss: 469.43
 ---- batch: 050 ----
mean loss: 453.99
 ---- batch: 060 ----
mean loss: 456.03
 ---- batch: 070 ----
mean loss: 443.08
 ---- batch: 080 ----
mean loss: 461.79
 ---- batch: 090 ----
mean loss: 456.47
 ---- batch: 100 ----
mean loss: 457.48
 ---- batch: 110 ----
mean loss: 473.48
train mean loss: 460.37
epoch train time: 0:00:02.158257
elapsed time: 0:05:26.764730
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 00:51:56.075259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.54
 ---- batch: 020 ----
mean loss: 453.88
 ---- batch: 030 ----
mean loss: 473.81
 ---- batch: 040 ----
mean loss: 460.31
 ---- batch: 050 ----
mean loss: 455.37
 ---- batch: 060 ----
mean loss: 467.93
 ---- batch: 070 ----
mean loss: 465.69
 ---- batch: 080 ----
mean loss: 464.98
 ---- batch: 090 ----
mean loss: 482.40
 ---- batch: 100 ----
mean loss: 456.57
 ---- batch: 110 ----
mean loss: 461.96
train mean loss: 465.61
epoch train time: 0:00:02.166376
elapsed time: 0:05:28.931281
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 00:51:58.241789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.66
 ---- batch: 020 ----
mean loss: 473.62
 ---- batch: 030 ----
mean loss: 451.05
 ---- batch: 040 ----
mean loss: 460.77
 ---- batch: 050 ----
mean loss: 461.58
 ---- batch: 060 ----
mean loss: 460.57
 ---- batch: 070 ----
mean loss: 459.42
 ---- batch: 080 ----
mean loss: 470.83
 ---- batch: 090 ----
mean loss: 447.56
 ---- batch: 100 ----
mean loss: 462.54
 ---- batch: 110 ----
mean loss: 468.82
train mean loss: 460.70
epoch train time: 0:00:02.169274
elapsed time: 0:05:31.100742
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 00:52:00.411250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.13
 ---- batch: 020 ----
mean loss: 462.39
 ---- batch: 030 ----
mean loss: 458.52
 ---- batch: 040 ----
mean loss: 441.84
 ---- batch: 050 ----
mean loss: 471.85
 ---- batch: 060 ----
mean loss: 461.55
 ---- batch: 070 ----
mean loss: 451.61
 ---- batch: 080 ----
mean loss: 464.49
 ---- batch: 090 ----
mean loss: 459.25
 ---- batch: 100 ----
mean loss: 471.35
 ---- batch: 110 ----
mean loss: 454.77
train mean loss: 460.18
epoch train time: 0:00:02.170438
elapsed time: 0:05:33.271347
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 00:52:02.581888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.87
 ---- batch: 020 ----
mean loss: 458.50
 ---- batch: 030 ----
mean loss: 453.90
 ---- batch: 040 ----
mean loss: 476.22
 ---- batch: 050 ----
mean loss: 454.92
 ---- batch: 060 ----
mean loss: 460.75
 ---- batch: 070 ----
mean loss: 451.08
 ---- batch: 080 ----
mean loss: 451.43
 ---- batch: 090 ----
mean loss: 458.28
 ---- batch: 100 ----
mean loss: 449.78
 ---- batch: 110 ----
mean loss: 465.41
train mean loss: 457.60
epoch train time: 0:00:02.160135
elapsed time: 0:05:35.431678
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 00:52:04.742186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.33
 ---- batch: 020 ----
mean loss: 461.86
 ---- batch: 030 ----
mean loss: 463.17
 ---- batch: 040 ----
mean loss: 469.31
 ---- batch: 050 ----
mean loss: 445.95
 ---- batch: 060 ----
mean loss: 448.25
 ---- batch: 070 ----
mean loss: 458.50
 ---- batch: 080 ----
mean loss: 454.78
 ---- batch: 090 ----
mean loss: 459.62
 ---- batch: 100 ----
mean loss: 459.67
 ---- batch: 110 ----
mean loss: 472.59
train mean loss: 459.38
epoch train time: 0:00:02.166564
elapsed time: 0:05:37.598398
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 00:52:06.908905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.99
 ---- batch: 020 ----
mean loss: 457.04
 ---- batch: 030 ----
mean loss: 461.52
 ---- batch: 040 ----
mean loss: 458.60
 ---- batch: 050 ----
mean loss: 457.27
 ---- batch: 060 ----
mean loss: 455.30
 ---- batch: 070 ----
mean loss: 460.74
 ---- batch: 080 ----
mean loss: 461.86
 ---- batch: 090 ----
mean loss: 462.13
 ---- batch: 100 ----
mean loss: 460.05
 ---- batch: 110 ----
mean loss: 459.32
train mean loss: 459.17
epoch train time: 0:00:02.163476
elapsed time: 0:05:39.762036
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 00:52:09.072578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.44
 ---- batch: 020 ----
mean loss: 473.98
 ---- batch: 030 ----
mean loss: 459.53
 ---- batch: 040 ----
mean loss: 448.95
 ---- batch: 050 ----
mean loss: 469.79
 ---- batch: 060 ----
mean loss: 465.64
 ---- batch: 070 ----
mean loss: 457.23
 ---- batch: 080 ----
mean loss: 448.97
 ---- batch: 090 ----
mean loss: 466.90
 ---- batch: 100 ----
mean loss: 470.44
 ---- batch: 110 ----
mean loss: 470.18
train mean loss: 462.26
epoch train time: 0:00:02.162368
elapsed time: 0:05:41.924606
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 00:52:11.235111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.93
 ---- batch: 020 ----
mean loss: 452.89
 ---- batch: 030 ----
mean loss: 463.10
 ---- batch: 040 ----
mean loss: 469.11
 ---- batch: 050 ----
mean loss: 467.21
 ---- batch: 060 ----
mean loss: 453.61
 ---- batch: 070 ----
mean loss: 464.89
 ---- batch: 080 ----
mean loss: 479.73
 ---- batch: 090 ----
mean loss: 450.35
 ---- batch: 100 ----
mean loss: 454.44
 ---- batch: 110 ----
mean loss: 463.35
train mean loss: 461.86
epoch train time: 0:00:02.164953
elapsed time: 0:05:44.089725
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 00:52:13.400235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.91
 ---- batch: 020 ----
mean loss: 471.49
 ---- batch: 030 ----
mean loss: 469.07
 ---- batch: 040 ----
mean loss: 468.72
 ---- batch: 050 ----
mean loss: 457.45
 ---- batch: 060 ----
mean loss: 469.64
 ---- batch: 070 ----
mean loss: 463.69
 ---- batch: 080 ----
mean loss: 463.48
 ---- batch: 090 ----
mean loss: 450.90
 ---- batch: 100 ----
mean loss: 459.55
 ---- batch: 110 ----
mean loss: 457.43
train mean loss: 462.86
epoch train time: 0:00:02.165265
elapsed time: 0:05:46.255178
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 00:52:15.565685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.09
 ---- batch: 020 ----
mean loss: 447.45
 ---- batch: 030 ----
mean loss: 470.02
 ---- batch: 040 ----
mean loss: 460.35
 ---- batch: 050 ----
mean loss: 472.56
 ---- batch: 060 ----
mean loss: 449.95
 ---- batch: 070 ----
mean loss: 441.11
 ---- batch: 080 ----
mean loss: 451.77
 ---- batch: 090 ----
mean loss: 457.39
 ---- batch: 100 ----
mean loss: 464.38
 ---- batch: 110 ----
mean loss: 451.14
train mean loss: 456.68
epoch train time: 0:00:02.168546
elapsed time: 0:05:48.423913
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 00:52:17.734421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.55
 ---- batch: 020 ----
mean loss: 466.42
 ---- batch: 030 ----
mean loss: 471.02
 ---- batch: 040 ----
mean loss: 453.61
 ---- batch: 050 ----
mean loss: 437.58
 ---- batch: 060 ----
mean loss: 454.83
 ---- batch: 070 ----
mean loss: 464.93
 ---- batch: 080 ----
mean loss: 451.91
 ---- batch: 090 ----
mean loss: 466.03
 ---- batch: 100 ----
mean loss: 450.76
 ---- batch: 110 ----
mean loss: 452.52
train mean loss: 457.61
epoch train time: 0:00:02.171698
elapsed time: 0:05:50.595786
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 00:52:19.906291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.46
 ---- batch: 020 ----
mean loss: 476.40
 ---- batch: 030 ----
mean loss: 455.33
 ---- batch: 040 ----
mean loss: 476.48
 ---- batch: 050 ----
mean loss: 480.12
 ---- batch: 060 ----
mean loss: 457.78
 ---- batch: 070 ----
mean loss: 467.55
 ---- batch: 080 ----
mean loss: 463.48
 ---- batch: 090 ----
mean loss: 455.26
 ---- batch: 100 ----
mean loss: 460.07
 ---- batch: 110 ----
mean loss: 452.64
train mean loss: 462.10
epoch train time: 0:00:02.177332
elapsed time: 0:05:52.773286
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 00:52:22.083804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.99
 ---- batch: 020 ----
mean loss: 456.77
 ---- batch: 030 ----
mean loss: 468.64
 ---- batch: 040 ----
mean loss: 460.52
 ---- batch: 050 ----
mean loss: 457.18
 ---- batch: 060 ----
mean loss: 433.49
 ---- batch: 070 ----
mean loss: 464.29
 ---- batch: 080 ----
mean loss: 478.87
 ---- batch: 090 ----
mean loss: 457.13
 ---- batch: 100 ----
mean loss: 445.57
 ---- batch: 110 ----
mean loss: 458.51
train mean loss: 457.43
epoch train time: 0:00:02.180130
elapsed time: 0:05:54.953604
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 00:52:24.264122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.53
 ---- batch: 020 ----
mean loss: 460.87
 ---- batch: 030 ----
mean loss: 451.05
 ---- batch: 040 ----
mean loss: 452.99
 ---- batch: 050 ----
mean loss: 458.83
 ---- batch: 060 ----
mean loss: 455.67
 ---- batch: 070 ----
mean loss: 458.86
 ---- batch: 080 ----
mean loss: 467.10
 ---- batch: 090 ----
mean loss: 448.35
 ---- batch: 100 ----
mean loss: 465.74
 ---- batch: 110 ----
mean loss: 452.07
train mean loss: 456.84
epoch train time: 0:00:02.178034
elapsed time: 0:05:57.131814
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 00:52:26.442319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.63
 ---- batch: 020 ----
mean loss: 450.35
 ---- batch: 030 ----
mean loss: 448.54
 ---- batch: 040 ----
mean loss: 469.57
 ---- batch: 050 ----
mean loss: 455.13
 ---- batch: 060 ----
mean loss: 456.19
 ---- batch: 070 ----
mean loss: 440.79
 ---- batch: 080 ----
mean loss: 463.76
 ---- batch: 090 ----
mean loss: 456.87
 ---- batch: 100 ----
mean loss: 451.53
 ---- batch: 110 ----
mean loss: 468.60
train mean loss: 456.05
epoch train time: 0:00:02.186124
elapsed time: 0:05:59.318097
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 00:52:28.628607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.78
 ---- batch: 020 ----
mean loss: 459.12
 ---- batch: 030 ----
mean loss: 456.18
 ---- batch: 040 ----
mean loss: 466.57
 ---- batch: 050 ----
mean loss: 456.21
 ---- batch: 060 ----
mean loss: 462.82
 ---- batch: 070 ----
mean loss: 458.19
 ---- batch: 080 ----
mean loss: 459.16
 ---- batch: 090 ----
mean loss: 461.64
 ---- batch: 100 ----
mean loss: 467.57
 ---- batch: 110 ----
mean loss: 446.61
train mean loss: 458.26
epoch train time: 0:00:02.189907
elapsed time: 0:06:01.508178
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 00:52:30.818688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.69
 ---- batch: 020 ----
mean loss: 445.87
 ---- batch: 030 ----
mean loss: 460.35
 ---- batch: 040 ----
mean loss: 460.95
 ---- batch: 050 ----
mean loss: 458.38
 ---- batch: 060 ----
mean loss: 456.59
 ---- batch: 070 ----
mean loss: 467.88
 ---- batch: 080 ----
mean loss: 454.01
 ---- batch: 090 ----
mean loss: 459.65
 ---- batch: 100 ----
mean loss: 448.37
 ---- batch: 110 ----
mean loss: 460.08
train mean loss: 457.43
epoch train time: 0:00:02.183248
elapsed time: 0:06:03.691597
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 00:52:33.002105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.44
 ---- batch: 020 ----
mean loss: 459.29
 ---- batch: 030 ----
mean loss: 465.68
 ---- batch: 040 ----
mean loss: 470.00
 ---- batch: 050 ----
mean loss: 464.93
 ---- batch: 060 ----
mean loss: 458.30
 ---- batch: 070 ----
mean loss: 466.97
 ---- batch: 080 ----
mean loss: 453.81
 ---- batch: 090 ----
mean loss: 454.75
 ---- batch: 100 ----
mean loss: 452.91
 ---- batch: 110 ----
mean loss: 459.42
train mean loss: 459.63
epoch train time: 0:00:02.192712
elapsed time: 0:06:05.884496
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 00:52:35.195003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.00
 ---- batch: 020 ----
mean loss: 447.03
 ---- batch: 030 ----
mean loss: 465.07
 ---- batch: 040 ----
mean loss: 450.60
 ---- batch: 050 ----
mean loss: 465.77
 ---- batch: 060 ----
mean loss: 456.67
 ---- batch: 070 ----
mean loss: 462.54
 ---- batch: 080 ----
mean loss: 460.09
 ---- batch: 090 ----
mean loss: 468.57
 ---- batch: 100 ----
mean loss: 452.85
 ---- batch: 110 ----
mean loss: 449.49
train mean loss: 457.89
epoch train time: 0:00:02.186989
elapsed time: 0:06:08.071644
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 00:52:37.382186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.79
 ---- batch: 020 ----
mean loss: 448.18
 ---- batch: 030 ----
mean loss: 464.49
 ---- batch: 040 ----
mean loss: 453.24
 ---- batch: 050 ----
mean loss: 459.66
 ---- batch: 060 ----
mean loss: 442.35
 ---- batch: 070 ----
mean loss: 459.39
 ---- batch: 080 ----
mean loss: 454.62
 ---- batch: 090 ----
mean loss: 452.47
 ---- batch: 100 ----
mean loss: 443.64
 ---- batch: 110 ----
mean loss: 468.82
train mean loss: 453.92
epoch train time: 0:00:02.186992
elapsed time: 0:06:10.258832
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 00:52:39.569339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.33
 ---- batch: 020 ----
mean loss: 450.20
 ---- batch: 030 ----
mean loss: 448.67
 ---- batch: 040 ----
mean loss: 454.95
 ---- batch: 050 ----
mean loss: 464.27
 ---- batch: 060 ----
mean loss: 465.05
 ---- batch: 070 ----
mean loss: 452.32
 ---- batch: 080 ----
mean loss: 465.43
 ---- batch: 090 ----
mean loss: 443.86
 ---- batch: 100 ----
mean loss: 473.65
 ---- batch: 110 ----
mean loss: 453.82
train mean loss: 456.42
epoch train time: 0:00:02.184180
elapsed time: 0:06:12.443208
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 00:52:41.753719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.17
 ---- batch: 020 ----
mean loss: 449.18
 ---- batch: 030 ----
mean loss: 450.50
 ---- batch: 040 ----
mean loss: 462.36
 ---- batch: 050 ----
mean loss: 456.27
 ---- batch: 060 ----
mean loss: 464.20
 ---- batch: 070 ----
mean loss: 446.79
 ---- batch: 080 ----
mean loss: 461.52
 ---- batch: 090 ----
mean loss: 464.42
 ---- batch: 100 ----
mean loss: 460.90
 ---- batch: 110 ----
mean loss: 453.06
train mean loss: 455.39
epoch train time: 0:00:02.186913
elapsed time: 0:06:14.630310
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 00:52:43.940834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.99
 ---- batch: 020 ----
mean loss: 471.65
 ---- batch: 030 ----
mean loss: 457.75
 ---- batch: 040 ----
mean loss: 456.69
 ---- batch: 050 ----
mean loss: 452.23
 ---- batch: 060 ----
mean loss: 465.94
 ---- batch: 070 ----
mean loss: 469.33
 ---- batch: 080 ----
mean loss: 469.04
 ---- batch: 090 ----
mean loss: 455.83
 ---- batch: 100 ----
mean loss: 446.70
 ---- batch: 110 ----
mean loss: 459.64
train mean loss: 460.46
epoch train time: 0:00:02.185603
elapsed time: 0:06:16.816113
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 00:52:46.126629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.78
 ---- batch: 020 ----
mean loss: 447.79
 ---- batch: 030 ----
mean loss: 462.95
 ---- batch: 040 ----
mean loss: 448.90
 ---- batch: 050 ----
mean loss: 450.00
 ---- batch: 060 ----
mean loss: 450.87
 ---- batch: 070 ----
mean loss: 452.11
 ---- batch: 080 ----
mean loss: 470.07
 ---- batch: 090 ----
mean loss: 438.74
 ---- batch: 100 ----
mean loss: 465.77
 ---- batch: 110 ----
mean loss: 469.79
train mean loss: 455.92
epoch train time: 0:00:02.186877
elapsed time: 0:06:19.003273
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 00:52:48.313800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.31
 ---- batch: 020 ----
mean loss: 454.66
 ---- batch: 030 ----
mean loss: 437.63
 ---- batch: 040 ----
mean loss: 443.83
 ---- batch: 050 ----
mean loss: 459.04
 ---- batch: 060 ----
mean loss: 459.59
 ---- batch: 070 ----
mean loss: 450.59
 ---- batch: 080 ----
mean loss: 472.70
 ---- batch: 090 ----
mean loss: 464.57
 ---- batch: 100 ----
mean loss: 440.68
 ---- batch: 110 ----
mean loss: 426.01
train mean loss: 452.53
epoch train time: 0:00:02.185032
elapsed time: 0:06:21.188507
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 00:52:50.499015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.34
 ---- batch: 020 ----
mean loss: 449.06
 ---- batch: 030 ----
mean loss: 456.72
 ---- batch: 040 ----
mean loss: 455.91
 ---- batch: 050 ----
mean loss: 457.72
 ---- batch: 060 ----
mean loss: 448.75
 ---- batch: 070 ----
mean loss: 451.75
 ---- batch: 080 ----
mean loss: 459.36
 ---- batch: 090 ----
mean loss: 463.21
 ---- batch: 100 ----
mean loss: 476.40
 ---- batch: 110 ----
mean loss: 467.47
train mean loss: 455.99
epoch train time: 0:00:02.190332
elapsed time: 0:06:23.379045
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 00:52:52.689581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.17
 ---- batch: 020 ----
mean loss: 458.51
 ---- batch: 030 ----
mean loss: 462.04
 ---- batch: 040 ----
mean loss: 456.70
 ---- batch: 050 ----
mean loss: 467.50
 ---- batch: 060 ----
mean loss: 435.40
 ---- batch: 070 ----
mean loss: 440.64
 ---- batch: 080 ----
mean loss: 436.64
 ---- batch: 090 ----
mean loss: 460.85
 ---- batch: 100 ----
mean loss: 444.07
 ---- batch: 110 ----
mean loss: 452.40
train mean loss: 451.03
epoch train time: 0:00:02.183547
elapsed time: 0:06:25.562821
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 00:52:54.873356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.65
 ---- batch: 020 ----
mean loss: 448.09
 ---- batch: 030 ----
mean loss: 464.52
 ---- batch: 040 ----
mean loss: 450.87
 ---- batch: 050 ----
mean loss: 453.18
 ---- batch: 060 ----
mean loss: 448.97
 ---- batch: 070 ----
mean loss: 439.61
 ---- batch: 080 ----
mean loss: 458.25
 ---- batch: 090 ----
mean loss: 449.22
 ---- batch: 100 ----
mean loss: 439.08
 ---- batch: 110 ----
mean loss: 458.59
train mean loss: 451.29
epoch train time: 0:00:02.189274
elapsed time: 0:06:27.752295
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 00:52:57.062807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.87
 ---- batch: 020 ----
mean loss: 457.71
 ---- batch: 030 ----
mean loss: 468.98
 ---- batch: 040 ----
mean loss: 462.33
 ---- batch: 050 ----
mean loss: 461.38
 ---- batch: 060 ----
mean loss: 446.13
 ---- batch: 070 ----
mean loss: 467.48
 ---- batch: 080 ----
mean loss: 448.68
 ---- batch: 090 ----
mean loss: 454.54
 ---- batch: 100 ----
mean loss: 457.49
 ---- batch: 110 ----
mean loss: 461.77
train mean loss: 458.07
epoch train time: 0:00:02.187670
elapsed time: 0:06:29.940146
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 00:52:59.250658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.87
 ---- batch: 020 ----
mean loss: 453.23
 ---- batch: 030 ----
mean loss: 454.01
 ---- batch: 040 ----
mean loss: 474.84
 ---- batch: 050 ----
mean loss: 464.73
 ---- batch: 060 ----
mean loss: 450.58
 ---- batch: 070 ----
mean loss: 450.29
 ---- batch: 080 ----
mean loss: 460.26
 ---- batch: 090 ----
mean loss: 460.76
 ---- batch: 100 ----
mean loss: 449.98
 ---- batch: 110 ----
mean loss: 470.16
train mean loss: 457.21
epoch train time: 0:00:02.187809
elapsed time: 0:06:32.128124
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 00:53:01.438631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.48
 ---- batch: 020 ----
mean loss: 442.29
 ---- batch: 030 ----
mean loss: 447.19
 ---- batch: 040 ----
mean loss: 453.19
 ---- batch: 050 ----
mean loss: 423.83
 ---- batch: 060 ----
mean loss: 456.64
 ---- batch: 070 ----
mean loss: 442.23
 ---- batch: 080 ----
mean loss: 468.91
 ---- batch: 090 ----
mean loss: 440.00
 ---- batch: 100 ----
mean loss: 452.21
 ---- batch: 110 ----
mean loss: 445.40
train mean loss: 448.08
epoch train time: 0:00:02.183626
elapsed time: 0:06:34.311924
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 00:53:03.622444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.18
 ---- batch: 020 ----
mean loss: 448.61
 ---- batch: 030 ----
mean loss: 449.54
 ---- batch: 040 ----
mean loss: 444.32
 ---- batch: 050 ----
mean loss: 455.44
 ---- batch: 060 ----
mean loss: 456.13
 ---- batch: 070 ----
mean loss: 459.25
 ---- batch: 080 ----
mean loss: 443.33
 ---- batch: 090 ----
mean loss: 446.42
 ---- batch: 100 ----
mean loss: 448.44
 ---- batch: 110 ----
mean loss: 457.26
train mean loss: 452.47
epoch train time: 0:00:02.186361
elapsed time: 0:06:36.498499
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 00:53:05.809025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.90
 ---- batch: 020 ----
mean loss: 451.20
 ---- batch: 030 ----
mean loss: 446.55
 ---- batch: 040 ----
mean loss: 450.41
 ---- batch: 050 ----
mean loss: 460.44
 ---- batch: 060 ----
mean loss: 445.24
 ---- batch: 070 ----
mean loss: 449.06
 ---- batch: 080 ----
mean loss: 458.28
 ---- batch: 090 ----
mean loss: 440.29
 ---- batch: 100 ----
mean loss: 459.79
 ---- batch: 110 ----
mean loss: 451.66
train mean loss: 450.91
epoch train time: 0:00:02.190150
elapsed time: 0:06:38.688866
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 00:53:07.999379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.85
 ---- batch: 020 ----
mean loss: 452.92
 ---- batch: 030 ----
mean loss: 432.74
 ---- batch: 040 ----
mean loss: 455.63
 ---- batch: 050 ----
mean loss: 453.76
 ---- batch: 060 ----
mean loss: 444.73
 ---- batch: 070 ----
mean loss: 455.16
 ---- batch: 080 ----
mean loss: 453.48
 ---- batch: 090 ----
mean loss: 441.22
 ---- batch: 100 ----
mean loss: 465.95
 ---- batch: 110 ----
mean loss: 452.77
train mean loss: 450.19
epoch train time: 0:00:02.190781
elapsed time: 0:06:40.879836
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 00:53:10.190348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.11
 ---- batch: 020 ----
mean loss: 452.89
 ---- batch: 030 ----
mean loss: 455.88
 ---- batch: 040 ----
mean loss: 444.13
 ---- batch: 050 ----
mean loss: 447.42
 ---- batch: 060 ----
mean loss: 467.46
 ---- batch: 070 ----
mean loss: 438.31
 ---- batch: 080 ----
mean loss: 460.48
 ---- batch: 090 ----
mean loss: 467.10
 ---- batch: 100 ----
mean loss: 450.13
 ---- batch: 110 ----
mean loss: 447.17
train mean loss: 452.99
epoch train time: 0:00:02.188867
elapsed time: 0:06:43.068899
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 00:53:12.379409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.29
 ---- batch: 020 ----
mean loss: 464.23
 ---- batch: 030 ----
mean loss: 443.96
 ---- batch: 040 ----
mean loss: 441.52
 ---- batch: 050 ----
mean loss: 456.69
 ---- batch: 060 ----
mean loss: 437.66
 ---- batch: 070 ----
mean loss: 468.53
 ---- batch: 080 ----
mean loss: 464.11
 ---- batch: 090 ----
mean loss: 457.77
 ---- batch: 100 ----
mean loss: 446.70
 ---- batch: 110 ----
mean loss: 452.30
train mean loss: 452.40
epoch train time: 0:00:02.198443
elapsed time: 0:06:45.267523
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 00:53:14.578034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.99
 ---- batch: 020 ----
mean loss: 456.65
 ---- batch: 030 ----
mean loss: 444.02
 ---- batch: 040 ----
mean loss: 445.10
 ---- batch: 050 ----
mean loss: 444.92
 ---- batch: 060 ----
mean loss: 450.82
 ---- batch: 070 ----
mean loss: 451.74
 ---- batch: 080 ----
mean loss: 458.89
 ---- batch: 090 ----
mean loss: 455.20
 ---- batch: 100 ----
mean loss: 455.93
 ---- batch: 110 ----
mean loss: 456.40
train mean loss: 451.58
epoch train time: 0:00:02.202341
elapsed time: 0:06:47.470036
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 00:53:16.780561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.57
 ---- batch: 020 ----
mean loss: 443.28
 ---- batch: 030 ----
mean loss: 458.96
 ---- batch: 040 ----
mean loss: 429.01
 ---- batch: 050 ----
mean loss: 448.55
 ---- batch: 060 ----
mean loss: 442.96
 ---- batch: 070 ----
mean loss: 452.90
 ---- batch: 080 ----
mean loss: 443.02
 ---- batch: 090 ----
mean loss: 451.15
 ---- batch: 100 ----
mean loss: 448.28
 ---- batch: 110 ----
mean loss: 458.85
train mean loss: 447.74
epoch train time: 0:00:02.182355
elapsed time: 0:06:49.652616
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 00:53:18.963114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.84
 ---- batch: 020 ----
mean loss: 452.62
 ---- batch: 030 ----
mean loss: 452.81
 ---- batch: 040 ----
mean loss: 443.76
 ---- batch: 050 ----
mean loss: 445.52
 ---- batch: 060 ----
mean loss: 455.77
 ---- batch: 070 ----
mean loss: 454.76
 ---- batch: 080 ----
mean loss: 453.09
 ---- batch: 090 ----
mean loss: 458.66
 ---- batch: 100 ----
mean loss: 452.92
 ---- batch: 110 ----
mean loss: 449.61
train mean loss: 451.80
epoch train time: 0:00:02.182584
elapsed time: 0:06:51.835378
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 00:53:21.145904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.84
 ---- batch: 020 ----
mean loss: 445.83
 ---- batch: 030 ----
mean loss: 462.00
 ---- batch: 040 ----
mean loss: 456.61
 ---- batch: 050 ----
mean loss: 452.82
 ---- batch: 060 ----
mean loss: 438.80
 ---- batch: 070 ----
mean loss: 443.94
 ---- batch: 080 ----
mean loss: 451.50
 ---- batch: 090 ----
mean loss: 461.23
 ---- batch: 100 ----
mean loss: 447.03
 ---- batch: 110 ----
mean loss: 454.42
train mean loss: 453.03
epoch train time: 0:00:02.182786
elapsed time: 0:06:54.018384
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 00:53:23.328897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.28
 ---- batch: 020 ----
mean loss: 470.55
 ---- batch: 030 ----
mean loss: 446.66
 ---- batch: 040 ----
mean loss: 460.27
 ---- batch: 050 ----
mean loss: 447.28
 ---- batch: 060 ----
mean loss: 450.63
 ---- batch: 070 ----
mean loss: 455.04
 ---- batch: 080 ----
mean loss: 444.67
 ---- batch: 090 ----
mean loss: 447.14
 ---- batch: 100 ----
mean loss: 453.94
 ---- batch: 110 ----
mean loss: 453.43
train mean loss: 452.48
epoch train time: 0:00:02.187903
elapsed time: 0:06:56.206465
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 00:53:25.516990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.01
 ---- batch: 020 ----
mean loss: 447.02
 ---- batch: 030 ----
mean loss: 448.81
 ---- batch: 040 ----
mean loss: 452.90
 ---- batch: 050 ----
mean loss: 441.58
 ---- batch: 060 ----
mean loss: 451.68
 ---- batch: 070 ----
mean loss: 441.26
 ---- batch: 080 ----
mean loss: 453.11
 ---- batch: 090 ----
mean loss: 464.11
 ---- batch: 100 ----
mean loss: 443.24
 ---- batch: 110 ----
mean loss: 444.37
train mean loss: 449.15
epoch train time: 0:00:02.186581
elapsed time: 0:06:58.393238
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 00:53:27.703746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.12
 ---- batch: 020 ----
mean loss: 450.61
 ---- batch: 030 ----
mean loss: 454.30
 ---- batch: 040 ----
mean loss: 455.01
 ---- batch: 050 ----
mean loss: 443.74
 ---- batch: 060 ----
mean loss: 446.92
 ---- batch: 070 ----
mean loss: 447.88
 ---- batch: 080 ----
mean loss: 438.71
 ---- batch: 090 ----
mean loss: 451.72
 ---- batch: 100 ----
mean loss: 447.22
 ---- batch: 110 ----
mean loss: 442.48
train mean loss: 448.46
epoch train time: 0:00:02.186390
elapsed time: 0:07:00.579817
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 00:53:29.890330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.02
 ---- batch: 020 ----
mean loss: 451.50
 ---- batch: 030 ----
mean loss: 447.05
 ---- batch: 040 ----
mean loss: 434.55
 ---- batch: 050 ----
mean loss: 463.00
 ---- batch: 060 ----
mean loss: 451.09
 ---- batch: 070 ----
mean loss: 445.98
 ---- batch: 080 ----
mean loss: 435.20
 ---- batch: 090 ----
mean loss: 459.55
 ---- batch: 100 ----
mean loss: 441.81
 ---- batch: 110 ----
mean loss: 449.55
train mean loss: 449.53
epoch train time: 0:00:02.175270
elapsed time: 0:07:02.755309
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 00:53:32.065815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.41
 ---- batch: 020 ----
mean loss: 443.96
 ---- batch: 030 ----
mean loss: 447.01
 ---- batch: 040 ----
mean loss: 438.11
 ---- batch: 050 ----
mean loss: 455.72
 ---- batch: 060 ----
mean loss: 459.43
 ---- batch: 070 ----
mean loss: 445.02
 ---- batch: 080 ----
mean loss: 441.94
 ---- batch: 090 ----
mean loss: 441.40
 ---- batch: 100 ----
mean loss: 444.12
 ---- batch: 110 ----
mean loss: 437.46
train mean loss: 446.22
epoch train time: 0:00:02.162965
elapsed time: 0:07:04.918443
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 00:53:34.228972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.84
 ---- batch: 020 ----
mean loss: 449.97
 ---- batch: 030 ----
mean loss: 459.07
 ---- batch: 040 ----
mean loss: 430.22
 ---- batch: 050 ----
mean loss: 448.74
 ---- batch: 060 ----
mean loss: 450.60
 ---- batch: 070 ----
mean loss: 448.72
 ---- batch: 080 ----
mean loss: 438.26
 ---- batch: 090 ----
mean loss: 464.86
 ---- batch: 100 ----
mean loss: 475.69
 ---- batch: 110 ----
mean loss: 460.87
train mean loss: 452.31
epoch train time: 0:00:02.167756
elapsed time: 0:07:07.086466
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 00:53:36.397029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.60
 ---- batch: 020 ----
mean loss: 455.04
 ---- batch: 030 ----
mean loss: 453.22
 ---- batch: 040 ----
mean loss: 442.07
 ---- batch: 050 ----
mean loss: 445.03
 ---- batch: 060 ----
mean loss: 452.66
 ---- batch: 070 ----
mean loss: 462.32
 ---- batch: 080 ----
mean loss: 440.62
 ---- batch: 090 ----
mean loss: 455.83
 ---- batch: 100 ----
mean loss: 441.61
 ---- batch: 110 ----
mean loss: 458.18
train mean loss: 450.53
epoch train time: 0:00:02.148830
elapsed time: 0:07:09.235490
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 00:53:38.545987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.97
 ---- batch: 020 ----
mean loss: 443.32
 ---- batch: 030 ----
mean loss: 435.28
 ---- batch: 040 ----
mean loss: 447.83
 ---- batch: 050 ----
mean loss: 453.74
 ---- batch: 060 ----
mean loss: 450.45
 ---- batch: 070 ----
mean loss: 433.56
 ---- batch: 080 ----
mean loss: 439.99
 ---- batch: 090 ----
mean loss: 449.87
 ---- batch: 100 ----
mean loss: 459.51
 ---- batch: 110 ----
mean loss: 444.29
train mean loss: 445.43
epoch train time: 0:00:02.145654
elapsed time: 0:07:11.381283
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 00:53:40.691838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.24
 ---- batch: 020 ----
mean loss: 447.60
 ---- batch: 030 ----
mean loss: 451.88
 ---- batch: 040 ----
mean loss: 447.15
 ---- batch: 050 ----
mean loss: 446.74
 ---- batch: 060 ----
mean loss: 443.19
 ---- batch: 070 ----
mean loss: 437.71
 ---- batch: 080 ----
mean loss: 441.70
 ---- batch: 090 ----
mean loss: 439.48
 ---- batch: 100 ----
mean loss: 448.53
 ---- batch: 110 ----
mean loss: 447.76
train mean loss: 444.14
epoch train time: 0:00:02.139104
elapsed time: 0:07:13.520590
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 00:53:42.831095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.78
 ---- batch: 020 ----
mean loss: 442.01
 ---- batch: 030 ----
mean loss: 433.61
 ---- batch: 040 ----
mean loss: 442.25
 ---- batch: 050 ----
mean loss: 438.78
 ---- batch: 060 ----
mean loss: 439.17
 ---- batch: 070 ----
mean loss: 447.20
 ---- batch: 080 ----
mean loss: 454.06
 ---- batch: 090 ----
mean loss: 450.75
 ---- batch: 100 ----
mean loss: 453.84
 ---- batch: 110 ----
mean loss: 439.61
train mean loss: 443.82
epoch train time: 0:00:02.135280
elapsed time: 0:07:15.656019
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 00:53:44.966524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.63
 ---- batch: 020 ----
mean loss: 443.99
 ---- batch: 030 ----
mean loss: 446.70
 ---- batch: 040 ----
mean loss: 448.01
 ---- batch: 050 ----
mean loss: 448.95
 ---- batch: 060 ----
mean loss: 452.47
 ---- batch: 070 ----
mean loss: 446.65
 ---- batch: 080 ----
mean loss: 433.01
 ---- batch: 090 ----
mean loss: 430.68
 ---- batch: 100 ----
mean loss: 452.31
 ---- batch: 110 ----
mean loss: 447.09
train mean loss: 445.57
epoch train time: 0:00:02.138359
elapsed time: 0:07:17.794529
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 00:53:47.105033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.62
 ---- batch: 020 ----
mean loss: 446.02
 ---- batch: 030 ----
mean loss: 452.18
 ---- batch: 040 ----
mean loss: 442.11
 ---- batch: 050 ----
mean loss: 447.38
 ---- batch: 060 ----
mean loss: 451.10
 ---- batch: 070 ----
mean loss: 435.50
 ---- batch: 080 ----
mean loss: 449.42
 ---- batch: 090 ----
mean loss: 455.82
 ---- batch: 100 ----
mean loss: 461.49
 ---- batch: 110 ----
mean loss: 452.78
train mean loss: 449.71
epoch train time: 0:00:02.139330
elapsed time: 0:07:19.934022
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 00:53:49.244604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.75
 ---- batch: 020 ----
mean loss: 446.61
 ---- batch: 030 ----
mean loss: 435.43
 ---- batch: 040 ----
mean loss: 434.55
 ---- batch: 050 ----
mean loss: 477.88
 ---- batch: 060 ----
mean loss: 433.46
 ---- batch: 070 ----
mean loss: 435.95
 ---- batch: 080 ----
mean loss: 452.32
 ---- batch: 090 ----
mean loss: 447.23
 ---- batch: 100 ----
mean loss: 437.60
 ---- batch: 110 ----
mean loss: 443.22
train mean loss: 444.09
epoch train time: 0:00:02.139552
elapsed time: 0:07:22.073796
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 00:53:51.384302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.70
 ---- batch: 020 ----
mean loss: 438.63
 ---- batch: 030 ----
mean loss: 451.97
 ---- batch: 040 ----
mean loss: 456.68
 ---- batch: 050 ----
mean loss: 456.99
 ---- batch: 060 ----
mean loss: 451.49
 ---- batch: 070 ----
mean loss: 441.15
 ---- batch: 080 ----
mean loss: 445.34
 ---- batch: 090 ----
mean loss: 450.93
 ---- batch: 100 ----
mean loss: 458.39
 ---- batch: 110 ----
mean loss: 449.57
train mean loss: 448.58
epoch train time: 0:00:02.140600
elapsed time: 0:07:24.214547
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 00:53:53.525050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.27
 ---- batch: 020 ----
mean loss: 434.48
 ---- batch: 030 ----
mean loss: 468.24
 ---- batch: 040 ----
mean loss: 454.35
 ---- batch: 050 ----
mean loss: 435.90
 ---- batch: 060 ----
mean loss: 452.26
 ---- batch: 070 ----
mean loss: 450.79
 ---- batch: 080 ----
mean loss: 431.74
 ---- batch: 090 ----
mean loss: 436.80
 ---- batch: 100 ----
mean loss: 450.94
 ---- batch: 110 ----
mean loss: 458.00
train mean loss: 447.78
epoch train time: 0:00:02.136987
elapsed time: 0:07:26.351685
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 00:53:55.662228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.05
 ---- batch: 020 ----
mean loss: 430.97
 ---- batch: 030 ----
mean loss: 436.25
 ---- batch: 040 ----
mean loss: 451.20
 ---- batch: 050 ----
mean loss: 442.02
 ---- batch: 060 ----
mean loss: 431.11
 ---- batch: 070 ----
mean loss: 456.78
 ---- batch: 080 ----
mean loss: 429.97
 ---- batch: 090 ----
mean loss: 455.14
 ---- batch: 100 ----
mean loss: 449.65
 ---- batch: 110 ----
mean loss: 450.58
train mean loss: 442.75
epoch train time: 0:00:02.138410
elapsed time: 0:07:28.490287
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 00:53:57.800794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.13
 ---- batch: 020 ----
mean loss: 449.54
 ---- batch: 030 ----
mean loss: 438.49
 ---- batch: 040 ----
mean loss: 435.33
 ---- batch: 050 ----
mean loss: 438.66
 ---- batch: 060 ----
mean loss: 451.83
 ---- batch: 070 ----
mean loss: 432.50
 ---- batch: 080 ----
mean loss: 457.13
 ---- batch: 090 ----
mean loss: 438.12
 ---- batch: 100 ----
mean loss: 448.25
 ---- batch: 110 ----
mean loss: 457.67
train mean loss: 444.54
epoch train time: 0:00:02.143632
elapsed time: 0:07:30.634080
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 00:53:59.944586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.53
 ---- batch: 020 ----
mean loss: 443.07
 ---- batch: 030 ----
mean loss: 450.93
 ---- batch: 040 ----
mean loss: 453.18
 ---- batch: 050 ----
mean loss: 454.11
 ---- batch: 060 ----
mean loss: 431.20
 ---- batch: 070 ----
mean loss: 448.81
 ---- batch: 080 ----
mean loss: 440.35
 ---- batch: 090 ----
mean loss: 439.13
 ---- batch: 100 ----
mean loss: 440.17
 ---- batch: 110 ----
mean loss: 450.44
train mean loss: 443.56
epoch train time: 0:00:02.136672
elapsed time: 0:07:32.770900
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 00:54:02.081411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.34
 ---- batch: 020 ----
mean loss: 446.09
 ---- batch: 030 ----
mean loss: 435.50
 ---- batch: 040 ----
mean loss: 433.21
 ---- batch: 050 ----
mean loss: 432.77
 ---- batch: 060 ----
mean loss: 452.38
 ---- batch: 070 ----
mean loss: 445.95
 ---- batch: 080 ----
mean loss: 426.17
 ---- batch: 090 ----
mean loss: 430.47
 ---- batch: 100 ----
mean loss: 453.22
 ---- batch: 110 ----
mean loss: 429.67
train mean loss: 440.07
epoch train time: 0:00:02.137333
elapsed time: 0:07:34.908387
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 00:54:04.218911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.77
 ---- batch: 020 ----
mean loss: 429.49
 ---- batch: 030 ----
mean loss: 452.05
 ---- batch: 040 ----
mean loss: 455.00
 ---- batch: 050 ----
mean loss: 446.97
 ---- batch: 060 ----
mean loss: 428.20
 ---- batch: 070 ----
mean loss: 427.76
 ---- batch: 080 ----
mean loss: 454.42
 ---- batch: 090 ----
mean loss: 446.23
 ---- batch: 100 ----
mean loss: 434.38
 ---- batch: 110 ----
mean loss: 450.05
train mean loss: 441.43
epoch train time: 0:00:02.136823
elapsed time: 0:07:37.045369
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 00:54:06.355871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.21
 ---- batch: 020 ----
mean loss: 441.90
 ---- batch: 030 ----
mean loss: 426.71
 ---- batch: 040 ----
mean loss: 443.47
 ---- batch: 050 ----
mean loss: 439.10
 ---- batch: 060 ----
mean loss: 439.81
 ---- batch: 070 ----
mean loss: 441.41
 ---- batch: 080 ----
mean loss: 442.38
 ---- batch: 090 ----
mean loss: 440.61
 ---- batch: 100 ----
mean loss: 447.59
 ---- batch: 110 ----
mean loss: 442.28
train mean loss: 440.12
epoch train time: 0:00:02.133828
elapsed time: 0:07:39.179333
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 00:54:08.489830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.82
 ---- batch: 020 ----
mean loss: 456.92
 ---- batch: 030 ----
mean loss: 444.42
 ---- batch: 040 ----
mean loss: 428.31
 ---- batch: 050 ----
mean loss: 457.26
 ---- batch: 060 ----
mean loss: 437.60
 ---- batch: 070 ----
mean loss: 447.77
 ---- batch: 080 ----
mean loss: 438.79
 ---- batch: 090 ----
mean loss: 437.26
 ---- batch: 100 ----
mean loss: 447.22
 ---- batch: 110 ----
mean loss: 436.38
train mean loss: 443.54
epoch train time: 0:00:02.130823
elapsed time: 0:07:41.310288
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 00:54:10.620790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.44
 ---- batch: 020 ----
mean loss: 447.17
 ---- batch: 030 ----
mean loss: 431.37
 ---- batch: 040 ----
mean loss: 430.91
 ---- batch: 050 ----
mean loss: 453.34
 ---- batch: 060 ----
mean loss: 450.44
 ---- batch: 070 ----
mean loss: 433.51
 ---- batch: 080 ----
mean loss: 433.38
 ---- batch: 090 ----
mean loss: 449.93
 ---- batch: 100 ----
mean loss: 433.77
 ---- batch: 110 ----
mean loss: 452.02
train mean loss: 441.00
epoch train time: 0:00:02.134104
elapsed time: 0:07:43.444541
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 00:54:12.755053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.74
 ---- batch: 020 ----
mean loss: 429.15
 ---- batch: 030 ----
mean loss: 459.49
 ---- batch: 040 ----
mean loss: 428.38
 ---- batch: 050 ----
mean loss: 449.76
 ---- batch: 060 ----
mean loss: 455.97
 ---- batch: 070 ----
mean loss: 437.04
 ---- batch: 080 ----
mean loss: 442.42
 ---- batch: 090 ----
mean loss: 421.65
 ---- batch: 100 ----
mean loss: 436.53
 ---- batch: 110 ----
mean loss: 433.52
train mean loss: 439.39
epoch train time: 0:00:02.136954
elapsed time: 0:07:45.581647
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 00:54:14.892156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.38
 ---- batch: 020 ----
mean loss: 434.16
 ---- batch: 030 ----
mean loss: 446.01
 ---- batch: 040 ----
mean loss: 447.59
 ---- batch: 050 ----
mean loss: 440.86
 ---- batch: 060 ----
mean loss: 441.21
 ---- batch: 070 ----
mean loss: 436.99
 ---- batch: 080 ----
mean loss: 443.56
 ---- batch: 090 ----
mean loss: 435.56
 ---- batch: 100 ----
mean loss: 434.92
 ---- batch: 110 ----
mean loss: 441.14
train mean loss: 440.80
epoch train time: 0:00:02.135406
elapsed time: 0:07:47.717209
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 00:54:17.027735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 419.58
 ---- batch: 020 ----
mean loss: 430.28
 ---- batch: 030 ----
mean loss: 439.56
 ---- batch: 040 ----
mean loss: 439.45
 ---- batch: 050 ----
mean loss: 435.23
 ---- batch: 060 ----
mean loss: 429.31
 ---- batch: 070 ----
mean loss: 417.18
 ---- batch: 080 ----
mean loss: 446.03
 ---- batch: 090 ----
mean loss: 433.89
 ---- batch: 100 ----
mean loss: 425.58
 ---- batch: 110 ----
mean loss: 431.18
train mean loss: 430.92
epoch train time: 0:00:02.132854
elapsed time: 0:07:49.850234
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 00:54:19.160724
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 426.01
 ---- batch: 020 ----
mean loss: 430.28
 ---- batch: 030 ----
mean loss: 431.76
 ---- batch: 040 ----
mean loss: 427.38
 ---- batch: 050 ----
mean loss: 425.72
 ---- batch: 060 ----
mean loss: 435.03
 ---- batch: 070 ----
mean loss: 441.48
 ---- batch: 080 ----
mean loss: 429.87
 ---- batch: 090 ----
mean loss: 445.45
 ---- batch: 100 ----
mean loss: 428.40
 ---- batch: 110 ----
mean loss: 423.35
train mean loss: 431.39
epoch train time: 0:00:02.134182
elapsed time: 0:07:51.984583
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 00:54:21.295087
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 445.76
 ---- batch: 020 ----
mean loss: 443.38
 ---- batch: 030 ----
mean loss: 439.99
 ---- batch: 040 ----
mean loss: 418.13
 ---- batch: 050 ----
mean loss: 441.85
 ---- batch: 060 ----
mean loss: 422.38
 ---- batch: 070 ----
mean loss: 424.40
 ---- batch: 080 ----
mean loss: 446.05
 ---- batch: 090 ----
mean loss: 438.68
 ---- batch: 100 ----
mean loss: 437.46
 ---- batch: 110 ----
mean loss: 414.35
train mean loss: 433.57
epoch train time: 0:00:02.135926
elapsed time: 0:07:54.120650
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 00:54:23.431149
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 443.46
 ---- batch: 020 ----
mean loss: 434.26
 ---- batch: 030 ----
mean loss: 429.21
 ---- batch: 040 ----
mean loss: 436.45
 ---- batch: 050 ----
mean loss: 433.20
 ---- batch: 060 ----
mean loss: 416.79
 ---- batch: 070 ----
mean loss: 431.33
 ---- batch: 080 ----
mean loss: 424.40
 ---- batch: 090 ----
mean loss: 440.44
 ---- batch: 100 ----
mean loss: 420.51
 ---- batch: 110 ----
mean loss: 431.33
train mean loss: 429.85
epoch train time: 0:00:02.132646
elapsed time: 0:07:56.253428
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 00:54:25.563931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 436.18
 ---- batch: 020 ----
mean loss: 411.16
 ---- batch: 030 ----
mean loss: 431.18
 ---- batch: 040 ----
mean loss: 428.31
 ---- batch: 050 ----
mean loss: 432.75
 ---- batch: 060 ----
mean loss: 430.34
 ---- batch: 070 ----
mean loss: 428.23
 ---- batch: 080 ----
mean loss: 423.23
 ---- batch: 090 ----
mean loss: 441.55
 ---- batch: 100 ----
mean loss: 428.00
 ---- batch: 110 ----
mean loss: 444.47
train mean loss: 430.47
epoch train time: 0:00:02.139799
elapsed time: 0:07:58.393379
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 00:54:27.703879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 430.37
 ---- batch: 020 ----
mean loss: 433.80
 ---- batch: 030 ----
mean loss: 425.68
 ---- batch: 040 ----
mean loss: 433.02
 ---- batch: 050 ----
mean loss: 424.91
 ---- batch: 060 ----
mean loss: 432.71
 ---- batch: 070 ----
mean loss: 425.11
 ---- batch: 080 ----
mean loss: 430.06
 ---- batch: 090 ----
mean loss: 424.39
 ---- batch: 100 ----
mean loss: 438.25
 ---- batch: 110 ----
mean loss: 425.13
train mean loss: 429.91
epoch train time: 0:00:02.137672
elapsed time: 0:08:00.531202
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 00:54:29.841711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 440.04
 ---- batch: 020 ----
mean loss: 442.10
 ---- batch: 030 ----
mean loss: 448.26
 ---- batch: 040 ----
mean loss: 440.69
 ---- batch: 050 ----
mean loss: 429.26
 ---- batch: 060 ----
mean loss: 436.12
 ---- batch: 070 ----
mean loss: 422.77
 ---- batch: 080 ----
mean loss: 428.26
 ---- batch: 090 ----
mean loss: 432.72
 ---- batch: 100 ----
mean loss: 436.25
 ---- batch: 110 ----
mean loss: 432.72
train mean loss: 434.71
epoch train time: 0:00:02.143843
elapsed time: 0:08:02.675227
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 00:54:31.985770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 419.94
 ---- batch: 020 ----
mean loss: 432.27
 ---- batch: 030 ----
mean loss: 424.67
 ---- batch: 040 ----
mean loss: 444.83
 ---- batch: 050 ----
mean loss: 430.51
 ---- batch: 060 ----
mean loss: 448.31
 ---- batch: 070 ----
mean loss: 426.12
 ---- batch: 080 ----
mean loss: 440.12
 ---- batch: 090 ----
mean loss: 418.26
 ---- batch: 100 ----
mean loss: 436.80
 ---- batch: 110 ----
mean loss: 428.16
train mean loss: 431.95
epoch train time: 0:00:02.142839
elapsed time: 0:08:04.818263
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 00:54:34.128766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 422.67
 ---- batch: 020 ----
mean loss: 434.90
 ---- batch: 030 ----
mean loss: 442.18
 ---- batch: 040 ----
mean loss: 426.12
 ---- batch: 050 ----
mean loss: 428.20
 ---- batch: 060 ----
mean loss: 437.11
 ---- batch: 070 ----
mean loss: 425.14
 ---- batch: 080 ----
mean loss: 427.55
 ---- batch: 090 ----
mean loss: 444.70
 ---- batch: 100 ----
mean loss: 431.41
 ---- batch: 110 ----
mean loss: 443.21
train mean loss: 432.28
epoch train time: 0:00:02.141584
elapsed time: 0:08:06.959989
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 00:54:36.270523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 432.11
 ---- batch: 020 ----
mean loss: 417.09
 ---- batch: 030 ----
mean loss: 445.00
 ---- batch: 040 ----
mean loss: 424.06
 ---- batch: 050 ----
mean loss: 444.17
 ---- batch: 060 ----
mean loss: 445.43
 ---- batch: 070 ----
mean loss: 417.68
 ---- batch: 080 ----
mean loss: 435.32
 ---- batch: 090 ----
mean loss: 425.40
 ---- batch: 100 ----
mean loss: 428.87
 ---- batch: 110 ----
mean loss: 431.51
train mean loss: 431.59
epoch train time: 0:00:02.136142
elapsed time: 0:08:09.096307
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 00:54:38.406809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 424.61
 ---- batch: 020 ----
mean loss: 430.20
 ---- batch: 030 ----
mean loss: 424.65
 ---- batch: 040 ----
mean loss: 433.06
 ---- batch: 050 ----
mean loss: 436.36
 ---- batch: 060 ----
mean loss: 425.73
 ---- batch: 070 ----
mean loss: 426.08
 ---- batch: 080 ----
mean loss: 430.13
 ---- batch: 090 ----
mean loss: 438.78
 ---- batch: 100 ----
mean loss: 434.79
 ---- batch: 110 ----
mean loss: 441.07
train mean loss: 431.55
epoch train time: 0:00:02.141406
elapsed time: 0:08:11.237885
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 00:54:40.548391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 439.02
 ---- batch: 020 ----
mean loss: 434.26
 ---- batch: 030 ----
mean loss: 435.22
 ---- batch: 040 ----
mean loss: 437.64
 ---- batch: 050 ----
mean loss: 443.55
 ---- batch: 060 ----
mean loss: 440.38
 ---- batch: 070 ----
mean loss: 435.79
 ---- batch: 080 ----
mean loss: 428.37
 ---- batch: 090 ----
mean loss: 409.33
 ---- batch: 100 ----
mean loss: 431.51
 ---- batch: 110 ----
mean loss: 433.87
train mean loss: 433.01
epoch train time: 0:00:02.133918
elapsed time: 0:08:13.372017
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 00:54:42.682531
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 430.51
 ---- batch: 020 ----
mean loss: 429.17
 ---- batch: 030 ----
mean loss: 424.82
 ---- batch: 040 ----
mean loss: 432.10
 ---- batch: 050 ----
mean loss: 437.49
 ---- batch: 060 ----
mean loss: 436.42
 ---- batch: 070 ----
mean loss: 426.14
 ---- batch: 080 ----
mean loss: 444.83
 ---- batch: 090 ----
mean loss: 415.67
 ---- batch: 100 ----
mean loss: 433.68
 ---- batch: 110 ----
mean loss: 413.83
train mean loss: 429.15
epoch train time: 0:00:02.136628
elapsed time: 0:08:15.508795
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 00:54:44.819305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 416.88
 ---- batch: 020 ----
mean loss: 443.10
 ---- batch: 030 ----
mean loss: 429.58
 ---- batch: 040 ----
mean loss: 427.39
 ---- batch: 050 ----
mean loss: 424.34
 ---- batch: 060 ----
mean loss: 429.24
 ---- batch: 070 ----
mean loss: 443.45
 ---- batch: 080 ----
mean loss: 425.97
 ---- batch: 090 ----
mean loss: 441.84
 ---- batch: 100 ----
mean loss: 425.12
 ---- batch: 110 ----
mean loss: 433.42
train mean loss: 430.89
epoch train time: 0:00:02.134024
elapsed time: 0:08:17.642961
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 00:54:46.953463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 434.29
 ---- batch: 020 ----
mean loss: 429.59
 ---- batch: 030 ----
mean loss: 436.57
 ---- batch: 040 ----
mean loss: 433.86
 ---- batch: 050 ----
mean loss: 423.18
 ---- batch: 060 ----
mean loss: 427.09
 ---- batch: 070 ----
mean loss: 432.52
 ---- batch: 080 ----
mean loss: 432.96
 ---- batch: 090 ----
mean loss: 421.84
 ---- batch: 100 ----
mean loss: 420.66
 ---- batch: 110 ----
mean loss: 439.68
train mean loss: 430.68
epoch train time: 0:00:02.134918
elapsed time: 0:08:19.778027
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 00:54:49.088572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 431.39
 ---- batch: 020 ----
mean loss: 428.16
 ---- batch: 030 ----
mean loss: 420.86
 ---- batch: 040 ----
mean loss: 431.73
 ---- batch: 050 ----
mean loss: 422.52
 ---- batch: 060 ----
mean loss: 429.89
 ---- batch: 070 ----
mean loss: 429.96
 ---- batch: 080 ----
mean loss: 435.63
 ---- batch: 090 ----
mean loss: 431.26
 ---- batch: 100 ----
mean loss: 425.22
 ---- batch: 110 ----
mean loss: 432.15
train mean loss: 428.93
epoch train time: 0:00:02.133183
elapsed time: 0:08:21.911411
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 00:54:51.221925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 430.90
 ---- batch: 020 ----
mean loss: 423.86
 ---- batch: 030 ----
mean loss: 438.67
 ---- batch: 040 ----
mean loss: 432.52
 ---- batch: 050 ----
mean loss: 447.50
 ---- batch: 060 ----
mean loss: 425.26
 ---- batch: 070 ----
mean loss: 419.07
 ---- batch: 080 ----
mean loss: 428.13
 ---- batch: 090 ----
mean loss: 426.69
 ---- batch: 100 ----
mean loss: 430.07
 ---- batch: 110 ----
mean loss: 432.88
train mean loss: 430.23
epoch train time: 0:00:02.134127
elapsed time: 0:08:24.045693
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 00:54:53.356212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 440.63
 ---- batch: 020 ----
mean loss: 432.28
 ---- batch: 030 ----
mean loss: 425.96
 ---- batch: 040 ----
mean loss: 431.89
 ---- batch: 050 ----
mean loss: 423.86
 ---- batch: 060 ----
mean loss: 448.15
 ---- batch: 070 ----
mean loss: 417.83
 ---- batch: 080 ----
mean loss: 416.77
 ---- batch: 090 ----
mean loss: 428.62
 ---- batch: 100 ----
mean loss: 430.01
 ---- batch: 110 ----
mean loss: 433.14
train mean loss: 429.88
epoch train time: 0:00:02.133767
elapsed time: 0:08:26.179620
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 00:54:55.490124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 432.69
 ---- batch: 020 ----
mean loss: 439.02
 ---- batch: 030 ----
mean loss: 427.24
 ---- batch: 040 ----
mean loss: 428.34
 ---- batch: 050 ----
mean loss: 429.24
 ---- batch: 060 ----
mean loss: 433.11
 ---- batch: 070 ----
mean loss: 415.09
 ---- batch: 080 ----
mean loss: 438.70
 ---- batch: 090 ----
mean loss: 418.82
 ---- batch: 100 ----
mean loss: 427.03
 ---- batch: 110 ----
mean loss: 420.63
train mean loss: 427.98
epoch train time: 0:00:02.134135
elapsed time: 0:08:28.313937
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 00:54:57.624439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 441.69
 ---- batch: 020 ----
mean loss: 422.19
 ---- batch: 030 ----
mean loss: 434.74
 ---- batch: 040 ----
mean loss: 425.91
 ---- batch: 050 ----
mean loss: 432.06
 ---- batch: 060 ----
mean loss: 427.66
 ---- batch: 070 ----
mean loss: 438.28
 ---- batch: 080 ----
mean loss: 424.17
 ---- batch: 090 ----
mean loss: 436.87
 ---- batch: 100 ----
mean loss: 438.39
 ---- batch: 110 ----
mean loss: 427.33
train mean loss: 431.03
epoch train time: 0:00:02.136032
elapsed time: 0:08:30.450133
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 00:54:59.760651
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 431.34
 ---- batch: 020 ----
mean loss: 430.51
 ---- batch: 030 ----
mean loss: 431.61
 ---- batch: 040 ----
mean loss: 439.02
 ---- batch: 050 ----
mean loss: 427.88
 ---- batch: 060 ----
mean loss: 420.11
 ---- batch: 070 ----
mean loss: 433.08
 ---- batch: 080 ----
mean loss: 438.06
 ---- batch: 090 ----
mean loss: 425.77
 ---- batch: 100 ----
mean loss: 437.84
 ---- batch: 110 ----
mean loss: 417.32
train mean loss: 429.62
epoch train time: 0:00:02.137757
elapsed time: 0:08:32.588042
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 00:55:01.898550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 439.95
 ---- batch: 020 ----
mean loss: 430.85
 ---- batch: 030 ----
mean loss: 436.52
 ---- batch: 040 ----
mean loss: 423.40
 ---- batch: 050 ----
mean loss: 419.75
 ---- batch: 060 ----
mean loss: 444.33
 ---- batch: 070 ----
mean loss: 440.10
 ---- batch: 080 ----
mean loss: 443.55
 ---- batch: 090 ----
mean loss: 423.78
 ---- batch: 100 ----
mean loss: 446.09
 ---- batch: 110 ----
mean loss: 422.43
train mean loss: 433.98
epoch train time: 0:00:02.138039
elapsed time: 0:08:34.726245
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 00:55:04.036749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 408.86
 ---- batch: 020 ----
mean loss: 434.52
 ---- batch: 030 ----
mean loss: 426.34
 ---- batch: 040 ----
mean loss: 434.10
 ---- batch: 050 ----
mean loss: 425.41
 ---- batch: 060 ----
mean loss: 421.74
 ---- batch: 070 ----
mean loss: 436.05
 ---- batch: 080 ----
mean loss: 425.28
 ---- batch: 090 ----
mean loss: 418.37
 ---- batch: 100 ----
mean loss: 432.22
 ---- batch: 110 ----
mean loss: 433.57
train mean loss: 426.25
epoch train time: 0:00:02.135483
elapsed time: 0:08:36.861871
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 00:55:06.172378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 421.92
 ---- batch: 020 ----
mean loss: 440.01
 ---- batch: 030 ----
mean loss: 424.03
 ---- batch: 040 ----
mean loss: 434.78
 ---- batch: 050 ----
mean loss: 428.44
 ---- batch: 060 ----
mean loss: 437.96
 ---- batch: 070 ----
mean loss: 422.91
 ---- batch: 080 ----
mean loss: 415.13
 ---- batch: 090 ----
mean loss: 450.03
 ---- batch: 100 ----
mean loss: 418.39
 ---- batch: 110 ----
mean loss: 425.17
train mean loss: 429.58
epoch train time: 0:00:02.129447
elapsed time: 0:08:38.991460
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 00:55:08.301961
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 436.11
 ---- batch: 020 ----
mean loss: 425.00
 ---- batch: 030 ----
mean loss: 448.36
 ---- batch: 040 ----
mean loss: 439.39
 ---- batch: 050 ----
mean loss: 431.41
 ---- batch: 060 ----
mean loss: 434.13
 ---- batch: 070 ----
mean loss: 432.70
 ---- batch: 080 ----
mean loss: 421.36
 ---- batch: 090 ----
mean loss: 433.27
 ---- batch: 100 ----
mean loss: 436.97
 ---- batch: 110 ----
mean loss: 425.73
train mean loss: 433.27
epoch train time: 0:00:02.132394
elapsed time: 0:08:41.124014
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 00:55:10.434519
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 430.00
 ---- batch: 020 ----
mean loss: 422.48
 ---- batch: 030 ----
mean loss: 440.00
 ---- batch: 040 ----
mean loss: 422.30
 ---- batch: 050 ----
mean loss: 435.23
 ---- batch: 060 ----
mean loss: 432.29
 ---- batch: 070 ----
mean loss: 426.08
 ---- batch: 080 ----
mean loss: 434.08
 ---- batch: 090 ----
mean loss: 436.62
 ---- batch: 100 ----
mean loss: 433.92
 ---- batch: 110 ----
mean loss: 419.15
train mean loss: 430.97
epoch train time: 0:00:02.131308
elapsed time: 0:08:43.255465
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 00:55:12.565991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 429.85
 ---- batch: 020 ----
mean loss: 432.37
 ---- batch: 030 ----
mean loss: 414.10
 ---- batch: 040 ----
mean loss: 425.68
 ---- batch: 050 ----
mean loss: 420.68
 ---- batch: 060 ----
mean loss: 425.79
 ---- batch: 070 ----
mean loss: 440.27
 ---- batch: 080 ----
mean loss: 443.31
 ---- batch: 090 ----
mean loss: 426.07
 ---- batch: 100 ----
mean loss: 427.54
 ---- batch: 110 ----
mean loss: 418.88
train mean loss: 427.60
epoch train time: 0:00:02.134991
elapsed time: 0:08:45.390626
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 00:55:14.701152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 416.95
 ---- batch: 020 ----
mean loss: 423.60
 ---- batch: 030 ----
mean loss: 413.43
 ---- batch: 040 ----
mean loss: 416.49
 ---- batch: 050 ----
mean loss: 427.83
 ---- batch: 060 ----
mean loss: 437.03
 ---- batch: 070 ----
mean loss: 430.80
 ---- batch: 080 ----
mean loss: 429.80
 ---- batch: 090 ----
mean loss: 433.35
 ---- batch: 100 ----
mean loss: 440.55
 ---- batch: 110 ----
mean loss: 426.08
train mean loss: 427.34
epoch train time: 0:00:02.134105
elapsed time: 0:08:47.524926
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 00:55:16.835473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 436.45
 ---- batch: 020 ----
mean loss: 437.75
 ---- batch: 030 ----
mean loss: 427.80
 ---- batch: 040 ----
mean loss: 427.48
 ---- batch: 050 ----
mean loss: 436.59
 ---- batch: 060 ----
mean loss: 432.74
 ---- batch: 070 ----
mean loss: 432.96
 ---- batch: 080 ----
mean loss: 413.90
 ---- batch: 090 ----
mean loss: 435.45
 ---- batch: 100 ----
mean loss: 426.78
 ---- batch: 110 ----
mean loss: 437.91
train mean loss: 431.29
epoch train time: 0:00:02.134723
elapsed time: 0:08:49.659831
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 00:55:18.970332
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 429.41
 ---- batch: 020 ----
mean loss: 422.36
 ---- batch: 030 ----
mean loss: 412.13
 ---- batch: 040 ----
mean loss: 446.12
 ---- batch: 050 ----
mean loss: 428.88
 ---- batch: 060 ----
mean loss: 437.35
 ---- batch: 070 ----
mean loss: 425.21
 ---- batch: 080 ----
mean loss: 418.64
 ---- batch: 090 ----
mean loss: 421.41
 ---- batch: 100 ----
mean loss: 415.46
 ---- batch: 110 ----
mean loss: 420.92
train mean loss: 426.01
epoch train time: 0:00:02.133436
elapsed time: 0:08:51.793405
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 00:55:21.103912
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 419.61
 ---- batch: 020 ----
mean loss: 429.77
 ---- batch: 030 ----
mean loss: 418.87
 ---- batch: 040 ----
mean loss: 424.01
 ---- batch: 050 ----
mean loss: 427.46
 ---- batch: 060 ----
mean loss: 421.27
 ---- batch: 070 ----
mean loss: 430.05
 ---- batch: 080 ----
mean loss: 435.03
 ---- batch: 090 ----
mean loss: 425.71
 ---- batch: 100 ----
mean loss: 419.31
 ---- batch: 110 ----
mean loss: 439.79
train mean loss: 426.84
epoch train time: 0:00:02.134957
elapsed time: 0:08:53.928521
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 00:55:23.239054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 432.56
 ---- batch: 020 ----
mean loss: 433.79
 ---- batch: 030 ----
mean loss: 425.93
 ---- batch: 040 ----
mean loss: 429.41
 ---- batch: 050 ----
mean loss: 419.81
 ---- batch: 060 ----
mean loss: 440.23
 ---- batch: 070 ----
mean loss: 424.38
 ---- batch: 080 ----
mean loss: 437.00
 ---- batch: 090 ----
mean loss: 428.54
 ---- batch: 100 ----
mean loss: 414.32
 ---- batch: 110 ----
mean loss: 414.11
train mean loss: 427.04
epoch train time: 0:00:02.134462
elapsed time: 0:08:56.063176
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 00:55:25.373686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 424.04
 ---- batch: 020 ----
mean loss: 422.37
 ---- batch: 030 ----
mean loss: 425.81
 ---- batch: 040 ----
mean loss: 421.46
 ---- batch: 050 ----
mean loss: 430.26
 ---- batch: 060 ----
mean loss: 444.14
 ---- batch: 070 ----
mean loss: 426.14
 ---- batch: 080 ----
mean loss: 426.19
 ---- batch: 090 ----
mean loss: 420.35
 ---- batch: 100 ----
mean loss: 433.99
 ---- batch: 110 ----
mean loss: 419.97
train mean loss: 426.98
epoch train time: 0:00:02.134303
elapsed time: 0:08:58.197642
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 00:55:27.508167
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 428.19
 ---- batch: 020 ----
mean loss: 422.27
 ---- batch: 030 ----
mean loss: 424.74
 ---- batch: 040 ----
mean loss: 427.67
 ---- batch: 050 ----
mean loss: 429.26
 ---- batch: 060 ----
mean loss: 427.86
 ---- batch: 070 ----
mean loss: 439.22
 ---- batch: 080 ----
mean loss: 432.50
 ---- batch: 090 ----
mean loss: 433.94
 ---- batch: 100 ----
mean loss: 427.81
 ---- batch: 110 ----
mean loss: 422.62
train mean loss: 429.13
epoch train time: 0:00:02.142288
elapsed time: 0:09:00.340093
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 00:55:29.650593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 434.07
 ---- batch: 020 ----
mean loss: 424.80
 ---- batch: 030 ----
mean loss: 436.38
 ---- batch: 040 ----
mean loss: 434.23
 ---- batch: 050 ----
mean loss: 427.45
 ---- batch: 060 ----
mean loss: 432.75
 ---- batch: 070 ----
mean loss: 440.65
 ---- batch: 080 ----
mean loss: 419.42
 ---- batch: 090 ----
mean loss: 421.10
 ---- batch: 100 ----
mean loss: 422.24
 ---- batch: 110 ----
mean loss: 420.52
train mean loss: 428.41
epoch train time: 0:00:02.132514
elapsed time: 0:09:02.472743
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 00:55:31.783246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 431.18
 ---- batch: 020 ----
mean loss: 443.55
 ---- batch: 030 ----
mean loss: 426.72
 ---- batch: 040 ----
mean loss: 420.63
 ---- batch: 050 ----
mean loss: 427.25
 ---- batch: 060 ----
mean loss: 437.65
 ---- batch: 070 ----
mean loss: 431.46
 ---- batch: 080 ----
mean loss: 423.40
 ---- batch: 090 ----
mean loss: 426.30
 ---- batch: 100 ----
mean loss: 410.76
 ---- batch: 110 ----
mean loss: 439.73
train mean loss: 428.48
epoch train time: 0:00:02.129462
elapsed time: 0:09:04.602347
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 00:55:33.912849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 428.57
 ---- batch: 020 ----
mean loss: 430.57
 ---- batch: 030 ----
mean loss: 427.05
 ---- batch: 040 ----
mean loss: 435.93
 ---- batch: 050 ----
mean loss: 427.61
 ---- batch: 060 ----
mean loss: 431.75
 ---- batch: 070 ----
mean loss: 416.33
 ---- batch: 080 ----
mean loss: 421.13
 ---- batch: 090 ----
mean loss: 427.66
 ---- batch: 100 ----
mean loss: 430.00
 ---- batch: 110 ----
mean loss: 428.94
train mean loss: 427.71
epoch train time: 0:00:02.135589
elapsed time: 0:09:06.738078
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 00:55:36.048584
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 429.44
 ---- batch: 020 ----
mean loss: 427.95
 ---- batch: 030 ----
mean loss: 427.83
 ---- batch: 040 ----
mean loss: 433.18
 ---- batch: 050 ----
mean loss: 433.89
 ---- batch: 060 ----
mean loss: 437.14
 ---- batch: 070 ----
mean loss: 429.18
 ---- batch: 080 ----
mean loss: 430.15
 ---- batch: 090 ----
mean loss: 428.16
 ---- batch: 100 ----
mean loss: 431.15
 ---- batch: 110 ----
mean loss: 415.77
train mean loss: 428.92
epoch train time: 0:00:02.135247
elapsed time: 0:09:08.873478
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 00:55:38.184008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 427.37
 ---- batch: 020 ----
mean loss: 428.17
 ---- batch: 030 ----
mean loss: 441.03
 ---- batch: 040 ----
mean loss: 426.20
 ---- batch: 050 ----
mean loss: 419.63
 ---- batch: 060 ----
mean loss: 428.16
 ---- batch: 070 ----
mean loss: 423.34
 ---- batch: 080 ----
mean loss: 436.25
 ---- batch: 090 ----
mean loss: 425.40
 ---- batch: 100 ----
mean loss: 428.89
 ---- batch: 110 ----
mean loss: 426.28
train mean loss: 427.83
epoch train time: 0:00:02.135172
elapsed time: 0:09:11.008813
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 00:55:40.319312
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 428.85
 ---- batch: 020 ----
mean loss: 429.02
 ---- batch: 030 ----
mean loss: 423.35
 ---- batch: 040 ----
mean loss: 424.06
 ---- batch: 050 ----
mean loss: 423.49
 ---- batch: 060 ----
mean loss: 420.70
 ---- batch: 070 ----
mean loss: 433.60
 ---- batch: 080 ----
mean loss: 421.96
 ---- batch: 090 ----
mean loss: 436.49
 ---- batch: 100 ----
mean loss: 426.13
 ---- batch: 110 ----
mean loss: 435.54
train mean loss: 427.42
epoch train time: 0:00:02.131726
elapsed time: 0:09:13.140678
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 00:55:42.451202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 429.23
 ---- batch: 020 ----
mean loss: 432.81
 ---- batch: 030 ----
mean loss: 426.37
 ---- batch: 040 ----
mean loss: 434.17
 ---- batch: 050 ----
mean loss: 424.56
 ---- batch: 060 ----
mean loss: 426.36
 ---- batch: 070 ----
mean loss: 430.52
 ---- batch: 080 ----
mean loss: 429.23
 ---- batch: 090 ----
mean loss: 432.92
 ---- batch: 100 ----
mean loss: 435.02
 ---- batch: 110 ----
mean loss: 435.53
train mean loss: 430.44
epoch train time: 0:00:02.136038
elapsed time: 0:09:15.276889
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 00:55:44.587394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 429.05
 ---- batch: 020 ----
mean loss: 441.65
 ---- batch: 030 ----
mean loss: 443.11
 ---- batch: 040 ----
mean loss: 422.22
 ---- batch: 050 ----
mean loss: 419.93
 ---- batch: 060 ----
mean loss: 438.54
 ---- batch: 070 ----
mean loss: 436.59
 ---- batch: 080 ----
mean loss: 426.62
 ---- batch: 090 ----
mean loss: 432.90
 ---- batch: 100 ----
mean loss: 430.43
 ---- batch: 110 ----
mean loss: 413.89
train mean loss: 430.26
epoch train time: 0:00:02.135332
elapsed time: 0:09:17.412363
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 00:55:46.722867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 416.60
 ---- batch: 020 ----
mean loss: 431.40
 ---- batch: 030 ----
mean loss: 437.67
 ---- batch: 040 ----
mean loss: 429.81
 ---- batch: 050 ----
mean loss: 433.69
 ---- batch: 060 ----
mean loss: 429.60
 ---- batch: 070 ----
mean loss: 437.15
 ---- batch: 080 ----
mean loss: 425.28
 ---- batch: 090 ----
mean loss: 428.90
 ---- batch: 100 ----
mean loss: 421.90
 ---- batch: 110 ----
mean loss: 424.11
train mean loss: 429.38
epoch train time: 0:00:02.130173
elapsed time: 0:09:19.542680
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 00:55:48.853182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 432.05
 ---- batch: 020 ----
mean loss: 423.86
 ---- batch: 030 ----
mean loss: 432.98
 ---- batch: 040 ----
mean loss: 423.76
 ---- batch: 050 ----
mean loss: 433.34
 ---- batch: 060 ----
mean loss: 426.38
 ---- batch: 070 ----
mean loss: 434.16
 ---- batch: 080 ----
mean loss: 429.71
 ---- batch: 090 ----
mean loss: 418.47
 ---- batch: 100 ----
mean loss: 411.75
 ---- batch: 110 ----
mean loss: 427.80
train mean loss: 426.15
epoch train time: 0:00:02.132666
elapsed time: 0:09:21.675527
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 00:55:50.986029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 428.88
 ---- batch: 020 ----
mean loss: 429.68
 ---- batch: 030 ----
mean loss: 424.66
 ---- batch: 040 ----
mean loss: 430.72
 ---- batch: 050 ----
mean loss: 425.19
 ---- batch: 060 ----
mean loss: 420.84
 ---- batch: 070 ----
mean loss: 434.43
 ---- batch: 080 ----
mean loss: 427.04
 ---- batch: 090 ----
mean loss: 425.15
 ---- batch: 100 ----
mean loss: 423.97
 ---- batch: 110 ----
mean loss: 432.85
train mean loss: 427.37
epoch train time: 0:00:02.130847
elapsed time: 0:09:23.806541
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 00:55:53.117045
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 429.70
 ---- batch: 020 ----
mean loss: 430.15
 ---- batch: 030 ----
mean loss: 427.51
 ---- batch: 040 ----
mean loss: 432.55
 ---- batch: 050 ----
mean loss: 424.56
 ---- batch: 060 ----
mean loss: 412.77
 ---- batch: 070 ----
mean loss: 419.93
 ---- batch: 080 ----
mean loss: 431.90
 ---- batch: 090 ----
mean loss: 425.90
 ---- batch: 100 ----
mean loss: 437.43
 ---- batch: 110 ----
mean loss: 431.31
train mean loss: 427.72
epoch train time: 0:00:02.133240
elapsed time: 0:09:25.939919
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 00:55:55.250439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 419.66
 ---- batch: 020 ----
mean loss: 433.42
 ---- batch: 030 ----
mean loss: 414.22
 ---- batch: 040 ----
mean loss: 441.44
 ---- batch: 050 ----
mean loss: 427.83
 ---- batch: 060 ----
mean loss: 436.30
 ---- batch: 070 ----
mean loss: 430.54
 ---- batch: 080 ----
mean loss: 433.29
 ---- batch: 090 ----
mean loss: 429.89
 ---- batch: 100 ----
mean loss: 427.84
 ---- batch: 110 ----
mean loss: 448.87
train mean loss: 430.47
epoch train time: 0:00:02.128845
elapsed time: 0:09:28.068916
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 00:55:57.379418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 422.29
 ---- batch: 020 ----
mean loss: 428.76
 ---- batch: 030 ----
mean loss: 423.66
 ---- batch: 040 ----
mean loss: 424.57
 ---- batch: 050 ----
mean loss: 431.69
 ---- batch: 060 ----
mean loss: 442.22
 ---- batch: 070 ----
mean loss: 417.58
 ---- batch: 080 ----
mean loss: 431.34
 ---- batch: 090 ----
mean loss: 426.38
 ---- batch: 100 ----
mean loss: 421.43
 ---- batch: 110 ----
mean loss: 434.66
train mean loss: 427.08
epoch train time: 0:00:02.137825
elapsed time: 0:09:30.206889
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 00:55:59.517394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 423.54
 ---- batch: 020 ----
mean loss: 416.53
 ---- batch: 030 ----
mean loss: 420.81
 ---- batch: 040 ----
mean loss: 425.63
 ---- batch: 050 ----
mean loss: 424.32
 ---- batch: 060 ----
mean loss: 423.13
 ---- batch: 070 ----
mean loss: 412.94
 ---- batch: 080 ----
mean loss: 431.76
 ---- batch: 090 ----
mean loss: 430.45
 ---- batch: 100 ----
mean loss: 439.74
 ---- batch: 110 ----
mean loss: 416.80
train mean loss: 423.87
epoch train time: 0:00:02.136731
elapsed time: 0:09:32.347045
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_6/checkpoint.pth.tar
**** end time: 2019-09-27 00:56:01.657513 ****
