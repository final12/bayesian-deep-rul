Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31558
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:42:17.664784 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:42:17.670715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4141.25
 ---- batch: 020 ----
mean loss: 3973.93
 ---- batch: 030 ----
mean loss: 4018.08
train mean loss: 4037.76
epoch train time: 0:00:12.628704
elapsed time: 0:00:12.636447
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:42:30.301271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3945.01
 ---- batch: 020 ----
mean loss: 3850.20
 ---- batch: 030 ----
mean loss: 3811.89
train mean loss: 3859.10
epoch train time: 0:00:00.175510
elapsed time: 0:00:12.812091
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:42:30.476935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3703.79
 ---- batch: 020 ----
mean loss: 3618.48
 ---- batch: 030 ----
mean loss: 3561.60
train mean loss: 3605.25
epoch train time: 0:00:00.174859
elapsed time: 0:00:12.987137
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:42:30.651972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3418.60
 ---- batch: 020 ----
mean loss: 3327.34
 ---- batch: 030 ----
mean loss: 3298.08
train mean loss: 3339.81
epoch train time: 0:00:00.174368
elapsed time: 0:00:13.161651
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:42:30.826501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3133.85
 ---- batch: 020 ----
mean loss: 3063.02
 ---- batch: 030 ----
mean loss: 3117.82
train mean loss: 3089.78
epoch train time: 0:00:00.186112
elapsed time: 0:00:13.347945
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:42:31.012780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2941.41
 ---- batch: 020 ----
mean loss: 2864.40
 ---- batch: 030 ----
mean loss: 2839.92
train mean loss: 2869.07
epoch train time: 0:00:00.173275
elapsed time: 0:00:13.521374
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:42:31.186284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2757.99
 ---- batch: 020 ----
mean loss: 2674.38
 ---- batch: 030 ----
mean loss: 2615.08
train mean loss: 2669.68
epoch train time: 0:00:00.173060
elapsed time: 0:00:13.694673
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:42:31.359522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2548.88
 ---- batch: 020 ----
mean loss: 2521.14
 ---- batch: 030 ----
mean loss: 2426.77
train mean loss: 2487.96
epoch train time: 0:00:00.173177
elapsed time: 0:00:13.868003
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:42:31.532852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2376.65
 ---- batch: 020 ----
mean loss: 2333.71
 ---- batch: 030 ----
mean loss: 2296.97
train mean loss: 2325.03
epoch train time: 0:00:00.176913
elapsed time: 0:00:14.045123
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:42:31.709959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2241.26
 ---- batch: 020 ----
mean loss: 2177.21
 ---- batch: 030 ----
mean loss: 2148.01
train mean loss: 2175.18
epoch train time: 0:00:00.177209
elapsed time: 0:00:14.222476
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:42:31.887319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2073.08
 ---- batch: 020 ----
mean loss: 2061.65
 ---- batch: 030 ----
mean loss: 2002.89
train mean loss: 2035.35
epoch train time: 0:00:00.176613
elapsed time: 0:00:14.399251
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:42:32.064116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1946.27
 ---- batch: 020 ----
mean loss: 1929.53
 ---- batch: 030 ----
mean loss: 1860.47
train mean loss: 1909.91
epoch train time: 0:00:00.171271
elapsed time: 0:00:14.570692
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:42:32.235526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1840.49
 ---- batch: 020 ----
mean loss: 1786.54
 ---- batch: 030 ----
mean loss: 1785.80
train mean loss: 1790.53
epoch train time: 0:00:00.171978
elapsed time: 0:00:14.742824
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:42:32.407672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1749.40
 ---- batch: 020 ----
mean loss: 1696.34
 ---- batch: 030 ----
mean loss: 1638.30
train mean loss: 1680.75
epoch train time: 0:00:00.171971
elapsed time: 0:00:14.914965
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:42:32.579796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1610.30
 ---- batch: 020 ----
mean loss: 1589.95
 ---- batch: 030 ----
mean loss: 1560.95
train mean loss: 1580.44
epoch train time: 0:00:00.182224
elapsed time: 0:00:15.097351
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:42:32.762193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1511.29
 ---- batch: 020 ----
mean loss: 1486.14
 ---- batch: 030 ----
mean loss: 1474.83
train mean loss: 1485.58
epoch train time: 0:00:00.180626
elapsed time: 0:00:15.278162
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:42:32.942998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1432.29
 ---- batch: 020 ----
mean loss: 1397.11
 ---- batch: 030 ----
mean loss: 1379.76
train mean loss: 1397.51
epoch train time: 0:00:00.176903
elapsed time: 0:00:15.455225
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:42:33.120059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1347.71
 ---- batch: 020 ----
mean loss: 1324.85
 ---- batch: 030 ----
mean loss: 1281.34
train mean loss: 1317.38
epoch train time: 0:00:00.178495
elapsed time: 0:00:15.633859
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:42:33.298693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1273.04
 ---- batch: 020 ----
mean loss: 1251.53
 ---- batch: 030 ----
mean loss: 1218.48
train mean loss: 1241.77
epoch train time: 0:00:00.177039
elapsed time: 0:00:15.811037
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:42:33.475886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1201.09
 ---- batch: 020 ----
mean loss: 1172.89
 ---- batch: 030 ----
mean loss: 1154.40
train mean loss: 1171.74
epoch train time: 0:00:00.174469
elapsed time: 0:00:15.985683
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:42:33.650527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1118.04
 ---- batch: 020 ----
mean loss: 1111.23
 ---- batch: 030 ----
mean loss: 1093.16
train mean loss: 1108.21
epoch train time: 0:00:00.182191
elapsed time: 0:00:16.168023
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:42:33.832868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1061.24
 ---- batch: 020 ----
mean loss: 1073.10
 ---- batch: 030 ----
mean loss: 1036.89
train mean loss: 1048.29
epoch train time: 0:00:00.175853
elapsed time: 0:00:16.344025
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:42:34.008858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1021.38
 ---- batch: 020 ----
mean loss: 996.07
 ---- batch: 030 ----
mean loss: 985.44
train mean loss: 993.17
epoch train time: 0:00:00.174284
elapsed time: 0:00:16.518456
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:42:34.183290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.54
 ---- batch: 020 ----
mean loss: 959.03
 ---- batch: 030 ----
mean loss: 942.56
train mean loss: 941.73
epoch train time: 0:00:00.181718
elapsed time: 0:00:16.700313
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:42:34.365160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.32
 ---- batch: 020 ----
mean loss: 911.43
 ---- batch: 030 ----
mean loss: 877.70
train mean loss: 894.99
epoch train time: 0:00:00.175375
elapsed time: 0:00:16.875840
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:42:34.540675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.61
 ---- batch: 020 ----
mean loss: 870.16
 ---- batch: 030 ----
mean loss: 847.90
train mean loss: 852.63
epoch train time: 0:00:00.181021
elapsed time: 0:00:17.057016
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:42:34.721858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.28
 ---- batch: 020 ----
mean loss: 819.69
 ---- batch: 030 ----
mean loss: 789.18
train mean loss: 813.41
epoch train time: 0:00:00.183223
elapsed time: 0:00:17.240385
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:42:34.905219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 792.44
 ---- batch: 020 ----
mean loss: 779.27
 ---- batch: 030 ----
mean loss: 760.49
train mean loss: 775.62
epoch train time: 0:00:00.171426
elapsed time: 0:00:17.411986
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:42:35.076819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 746.64
 ---- batch: 020 ----
mean loss: 759.40
 ---- batch: 030 ----
mean loss: 733.78
train mean loss: 741.39
epoch train time: 0:00:00.168692
elapsed time: 0:00:17.580851
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:42:35.245697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 719.51
 ---- batch: 020 ----
mean loss: 710.18
 ---- batch: 030 ----
mean loss: 704.30
train mean loss: 711.13
epoch train time: 0:00:00.171851
elapsed time: 0:00:17.752876
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:42:35.417714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 688.32
 ---- batch: 020 ----
mean loss: 675.43
 ---- batch: 030 ----
mean loss: 691.41
train mean loss: 682.91
epoch train time: 0:00:00.170955
elapsed time: 0:00:17.923970
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:42:35.588802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 666.79
 ---- batch: 020 ----
mean loss: 662.75
 ---- batch: 030 ----
mean loss: 646.82
train mean loss: 655.63
epoch train time: 0:00:00.183280
elapsed time: 0:00:18.107388
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:42:35.772222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 643.76
 ---- batch: 020 ----
mean loss: 630.64
 ---- batch: 030 ----
mean loss: 617.83
train mean loss: 632.15
epoch train time: 0:00:00.184416
elapsed time: 0:00:18.291949
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:42:35.956810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 618.21
 ---- batch: 020 ----
mean loss: 619.39
 ---- batch: 030 ----
mean loss: 599.60
train mean loss: 610.31
epoch train time: 0:00:00.175563
elapsed time: 0:00:18.467688
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:42:36.132521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 591.62
 ---- batch: 020 ----
mean loss: 598.26
 ---- batch: 030 ----
mean loss: 583.60
train mean loss: 590.02
epoch train time: 0:00:00.174807
elapsed time: 0:00:18.642632
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:42:36.307480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 574.05
 ---- batch: 020 ----
mean loss: 569.01
 ---- batch: 030 ----
mean loss: 578.07
train mean loss: 572.04
epoch train time: 0:00:00.173966
elapsed time: 0:00:18.816784
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:42:36.481666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 560.98
 ---- batch: 020 ----
mean loss: 557.49
 ---- batch: 030 ----
mean loss: 551.16
train mean loss: 554.09
epoch train time: 0:00:00.172369
elapsed time: 0:00:18.989343
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:42:36.654176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.70
 ---- batch: 020 ----
mean loss: 535.31
 ---- batch: 030 ----
mean loss: 539.49
train mean loss: 538.78
epoch train time: 0:00:00.174646
elapsed time: 0:00:19.164140
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:42:36.828975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 532.91
 ---- batch: 020 ----
mean loss: 525.83
 ---- batch: 030 ----
mean loss: 519.16
train mean loss: 524.86
epoch train time: 0:00:00.171580
elapsed time: 0:00:19.335859
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:42:37.000695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.06
 ---- batch: 020 ----
mean loss: 510.76
 ---- batch: 030 ----
mean loss: 511.86
train mean loss: 511.58
epoch train time: 0:00:00.171521
elapsed time: 0:00:19.507519
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:42:37.172351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.21
 ---- batch: 020 ----
mean loss: 496.45
 ---- batch: 030 ----
mean loss: 498.00
train mean loss: 499.51
epoch train time: 0:00:00.172430
elapsed time: 0:00:19.680084
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:42:37.344917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.43
 ---- batch: 020 ----
mean loss: 481.96
 ---- batch: 030 ----
mean loss: 489.93
train mean loss: 488.22
epoch train time: 0:00:00.172544
elapsed time: 0:00:19.852768
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:42:37.517600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.52
 ---- batch: 020 ----
mean loss: 476.44
 ---- batch: 030 ----
mean loss: 473.32
train mean loss: 477.87
epoch train time: 0:00:00.172420
elapsed time: 0:00:20.025326
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:42:37.690159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.93
 ---- batch: 020 ----
mean loss: 476.98
 ---- batch: 030 ----
mean loss: 467.40
train mean loss: 468.85
epoch train time: 0:00:00.170914
elapsed time: 0:00:20.196418
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:42:37.861263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 460.55
 ---- batch: 020 ----
mean loss: 455.56
 ---- batch: 030 ----
mean loss: 462.78
train mean loss: 459.93
epoch train time: 0:00:00.174174
elapsed time: 0:00:20.370744
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:42:38.035578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.49
 ---- batch: 020 ----
mean loss: 450.15
 ---- batch: 030 ----
mean loss: 448.29
train mean loss: 452.22
epoch train time: 0:00:00.173074
elapsed time: 0:00:20.543971
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:42:38.208816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.32
 ---- batch: 020 ----
mean loss: 445.50
 ---- batch: 030 ----
mean loss: 443.84
train mean loss: 444.18
epoch train time: 0:00:00.172411
elapsed time: 0:00:20.716533
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:42:38.381366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.76
 ---- batch: 020 ----
mean loss: 442.56
 ---- batch: 030 ----
mean loss: 434.89
train mean loss: 437.36
epoch train time: 0:00:00.172586
elapsed time: 0:00:20.889260
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:42:38.554093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.90
 ---- batch: 020 ----
mean loss: 431.00
 ---- batch: 030 ----
mean loss: 425.09
train mean loss: 430.59
epoch train time: 0:00:00.175548
elapsed time: 0:00:21.065008
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:42:38.729841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.16
 ---- batch: 020 ----
mean loss: 424.94
 ---- batch: 030 ----
mean loss: 421.16
train mean loss: 425.11
epoch train time: 0:00:00.175665
elapsed time: 0:00:21.240811
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:42:38.905672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.39
 ---- batch: 020 ----
mean loss: 418.01
 ---- batch: 030 ----
mean loss: 421.39
train mean loss: 419.32
epoch train time: 0:00:00.172314
elapsed time: 0:00:21.413290
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:42:39.078129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.57
 ---- batch: 020 ----
mean loss: 406.54
 ---- batch: 030 ----
mean loss: 418.45
train mean loss: 414.45
epoch train time: 0:00:00.171694
elapsed time: 0:00:21.585127
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:42:39.249960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.87
 ---- batch: 020 ----
mean loss: 413.25
 ---- batch: 030 ----
mean loss: 401.94
train mean loss: 409.49
epoch train time: 0:00:00.171676
elapsed time: 0:00:21.756941
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:42:39.421788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.85
 ---- batch: 020 ----
mean loss: 403.23
 ---- batch: 030 ----
mean loss: 407.69
train mean loss: 405.50
epoch train time: 0:00:00.175297
elapsed time: 0:00:21.932394
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:42:39.597225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.97
 ---- batch: 020 ----
mean loss: 404.76
 ---- batch: 030 ----
mean loss: 395.39
train mean loss: 401.71
epoch train time: 0:00:00.170777
elapsed time: 0:00:22.103315
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:42:39.768156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.41
 ---- batch: 020 ----
mean loss: 397.26
 ---- batch: 030 ----
mean loss: 400.30
train mean loss: 398.28
epoch train time: 0:00:00.175463
elapsed time: 0:00:22.278927
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:42:39.943759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.92
 ---- batch: 020 ----
mean loss: 402.86
 ---- batch: 030 ----
mean loss: 392.07
train mean loss: 395.61
epoch train time: 0:00:00.178475
elapsed time: 0:00:22.457539
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:42:40.122370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.51
 ---- batch: 020 ----
mean loss: 392.46
 ---- batch: 030 ----
mean loss: 393.20
train mean loss: 392.55
epoch train time: 0:00:00.174779
elapsed time: 0:00:22.632454
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:42:40.297306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.34
 ---- batch: 020 ----
mean loss: 399.43
 ---- batch: 030 ----
mean loss: 390.32
train mean loss: 390.10
epoch train time: 0:00:00.174684
elapsed time: 0:00:22.807297
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:42:40.472132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.16
 ---- batch: 020 ----
mean loss: 390.86
 ---- batch: 030 ----
mean loss: 382.65
train mean loss: 388.02
epoch train time: 0:00:00.174372
elapsed time: 0:00:22.981810
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:42:40.646673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.74
 ---- batch: 020 ----
mean loss: 379.40
 ---- batch: 030 ----
mean loss: 383.02
train mean loss: 386.54
epoch train time: 0:00:00.186237
elapsed time: 0:00:23.168216
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:42:40.833050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.61
 ---- batch: 020 ----
mean loss: 383.07
 ---- batch: 030 ----
mean loss: 390.66
train mean loss: 384.49
epoch train time: 0:00:00.176579
elapsed time: 0:00:23.344938
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:42:41.009772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.35
 ---- batch: 020 ----
mean loss: 386.23
 ---- batch: 030 ----
mean loss: 379.20
train mean loss: 382.95
epoch train time: 0:00:00.173819
elapsed time: 0:00:23.518914
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:42:41.183748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.96
 ---- batch: 020 ----
mean loss: 383.24
 ---- batch: 030 ----
mean loss: 380.23
train mean loss: 381.20
epoch train time: 0:00:00.175131
elapsed time: 0:00:23.694182
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:42:41.359031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.60
 ---- batch: 020 ----
mean loss: 376.47
 ---- batch: 030 ----
mean loss: 379.53
train mean loss: 380.24
epoch train time: 0:00:00.173846
elapsed time: 0:00:23.868200
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:42:41.533042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.34
 ---- batch: 020 ----
mean loss: 374.58
 ---- batch: 030 ----
mean loss: 377.59
train mean loss: 379.02
epoch train time: 0:00:00.181975
elapsed time: 0:00:24.050332
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:42:41.715181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.25
 ---- batch: 020 ----
mean loss: 374.81
 ---- batch: 030 ----
mean loss: 374.89
train mean loss: 377.69
epoch train time: 0:00:00.175054
elapsed time: 0:00:24.225566
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:42:41.890403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.83
 ---- batch: 020 ----
mean loss: 373.42
 ---- batch: 030 ----
mean loss: 377.99
train mean loss: 376.99
epoch train time: 0:00:00.177974
elapsed time: 0:00:24.403685
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:42:42.068529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.80
 ---- batch: 020 ----
mean loss: 379.13
 ---- batch: 030 ----
mean loss: 375.50
train mean loss: 375.60
epoch train time: 0:00:00.177798
elapsed time: 0:00:24.581636
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:42:42.246470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.06
 ---- batch: 020 ----
mean loss: 375.14
 ---- batch: 030 ----
mean loss: 374.78
train mean loss: 375.14
epoch train time: 0:00:00.180090
elapsed time: 0:00:24.761921
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:42:42.426755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.21
 ---- batch: 020 ----
mean loss: 366.14
 ---- batch: 030 ----
mean loss: 379.21
train mean loss: 374.01
epoch train time: 0:00:00.175552
elapsed time: 0:00:24.937612
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:42:42.602443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.85
 ---- batch: 020 ----
mean loss: 372.03
 ---- batch: 030 ----
mean loss: 367.67
train mean loss: 373.69
epoch train time: 0:00:00.177822
elapsed time: 0:00:25.115581
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:42:42.780417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.51
 ---- batch: 020 ----
mean loss: 374.05
 ---- batch: 030 ----
mean loss: 369.39
train mean loss: 372.67
epoch train time: 0:00:00.177247
elapsed time: 0:00:25.293002
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:42:42.957851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.14
 ---- batch: 020 ----
mean loss: 379.97
 ---- batch: 030 ----
mean loss: 368.61
train mean loss: 372.08
epoch train time: 0:00:00.177556
elapsed time: 0:00:25.470714
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:42:43.135548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.12
 ---- batch: 020 ----
mean loss: 367.96
 ---- batch: 030 ----
mean loss: 374.46
train mean loss: 371.85
epoch train time: 0:00:00.174170
elapsed time: 0:00:25.645025
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:42:43.309857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.87
 ---- batch: 020 ----
mean loss: 375.17
 ---- batch: 030 ----
mean loss: 371.99
train mean loss: 371.32
epoch train time: 0:00:00.174699
elapsed time: 0:00:25.819893
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:42:43.484742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.19
 ---- batch: 020 ----
mean loss: 366.32
 ---- batch: 030 ----
mean loss: 371.18
train mean loss: 371.04
epoch train time: 0:00:00.171119
elapsed time: 0:00:25.991167
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:42:43.656000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.83
 ---- batch: 020 ----
mean loss: 373.55
 ---- batch: 030 ----
mean loss: 373.69
train mean loss: 370.25
epoch train time: 0:00:00.176826
elapsed time: 0:00:26.168130
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:42:43.832962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.58
 ---- batch: 020 ----
mean loss: 364.06
 ---- batch: 030 ----
mean loss: 366.06
train mean loss: 370.28
epoch train time: 0:00:00.175191
elapsed time: 0:00:26.343490
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:42:44.008323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.57
 ---- batch: 020 ----
mean loss: 369.82
 ---- batch: 030 ----
mean loss: 372.77
train mean loss: 369.88
epoch train time: 0:00:00.176944
elapsed time: 0:00:26.520569
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:42:44.185411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.94
 ---- batch: 020 ----
mean loss: 368.33
 ---- batch: 030 ----
mean loss: 380.43
train mean loss: 368.99
epoch train time: 0:00:00.176738
elapsed time: 0:00:26.697452
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:42:44.362284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.92
 ---- batch: 020 ----
mean loss: 370.78
 ---- batch: 030 ----
mean loss: 368.66
train mean loss: 368.91
epoch train time: 0:00:00.175102
elapsed time: 0:00:26.872742
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:42:44.537575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.02
 ---- batch: 020 ----
mean loss: 362.86
 ---- batch: 030 ----
mean loss: 370.03
train mean loss: 368.72
epoch train time: 0:00:00.175858
elapsed time: 0:00:27.048746
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:42:44.713587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.63
 ---- batch: 020 ----
mean loss: 371.61
 ---- batch: 030 ----
mean loss: 366.42
train mean loss: 368.21
epoch train time: 0:00:00.175835
elapsed time: 0:00:27.224727
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:42:44.889591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.19
 ---- batch: 020 ----
mean loss: 363.92
 ---- batch: 030 ----
mean loss: 368.13
train mean loss: 368.25
epoch train time: 0:00:00.171827
elapsed time: 0:00:27.396722
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:42:45.061565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.50
 ---- batch: 020 ----
mean loss: 373.17
 ---- batch: 030 ----
mean loss: 366.41
train mean loss: 367.55
epoch train time: 0:00:00.175330
elapsed time: 0:00:27.572201
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:42:45.237051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.81
 ---- batch: 020 ----
mean loss: 368.33
 ---- batch: 030 ----
mean loss: 364.14
train mean loss: 367.33
epoch train time: 0:00:00.179419
elapsed time: 0:00:27.751777
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:42:45.416627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.13
 ---- batch: 020 ----
mean loss: 377.36
 ---- batch: 030 ----
mean loss: 359.04
train mean loss: 367.05
epoch train time: 0:00:00.176929
elapsed time: 0:00:27.928885
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:42:45.593742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.51
 ---- batch: 020 ----
mean loss: 362.66
 ---- batch: 030 ----
mean loss: 362.03
train mean loss: 366.97
epoch train time: 0:00:00.174581
elapsed time: 0:00:28.103649
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:42:45.768486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.29
 ---- batch: 020 ----
mean loss: 363.08
 ---- batch: 030 ----
mean loss: 370.45
train mean loss: 366.44
epoch train time: 0:00:00.175783
elapsed time: 0:00:28.279591
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:42:45.944437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.16
 ---- batch: 020 ----
mean loss: 369.01
 ---- batch: 030 ----
mean loss: 369.68
train mean loss: 366.43
epoch train time: 0:00:00.175365
elapsed time: 0:00:28.455106
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:42:46.119938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.86
 ---- batch: 020 ----
mean loss: 365.89
 ---- batch: 030 ----
mean loss: 369.01
train mean loss: 365.86
epoch train time: 0:00:00.173698
elapsed time: 0:00:28.628942
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:42:46.293778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.00
 ---- batch: 020 ----
mean loss: 366.19
 ---- batch: 030 ----
mean loss: 362.73
train mean loss: 365.81
epoch train time: 0:00:00.177926
elapsed time: 0:00:28.807009
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:42:46.471841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.52
 ---- batch: 020 ----
mean loss: 364.14
 ---- batch: 030 ----
mean loss: 366.62
train mean loss: 365.71
epoch train time: 0:00:00.173925
elapsed time: 0:00:28.981090
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:42:46.645955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.17
 ---- batch: 020 ----
mean loss: 359.64
 ---- batch: 030 ----
mean loss: 373.09
train mean loss: 365.73
epoch train time: 0:00:00.179761
elapsed time: 0:00:29.161072
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:42:46.825903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.26
 ---- batch: 020 ----
mean loss: 362.52
 ---- batch: 030 ----
mean loss: 370.55
train mean loss: 364.96
epoch train time: 0:00:00.173291
elapsed time: 0:00:29.334496
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:42:46.999328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.83
 ---- batch: 020 ----
mean loss: 364.36
 ---- batch: 030 ----
mean loss: 369.51
train mean loss: 364.88
epoch train time: 0:00:00.170546
elapsed time: 0:00:29.505181
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:42:47.170013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.03
 ---- batch: 020 ----
mean loss: 362.52
 ---- batch: 030 ----
mean loss: 361.30
train mean loss: 364.87
epoch train time: 0:00:00.173338
elapsed time: 0:00:29.678662
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:42:47.343503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.77
 ---- batch: 020 ----
mean loss: 363.39
 ---- batch: 030 ----
mean loss: 361.84
train mean loss: 364.90
epoch train time: 0:00:00.174647
elapsed time: 0:00:29.853454
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:42:47.518287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.43
 ---- batch: 020 ----
mean loss: 360.87
 ---- batch: 030 ----
mean loss: 366.39
train mean loss: 364.19
epoch train time: 0:00:00.170944
elapsed time: 0:00:30.024598
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:42:47.689460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.90
 ---- batch: 020 ----
mean loss: 367.08
 ---- batch: 030 ----
mean loss: 358.83
train mean loss: 364.56
epoch train time: 0:00:00.187358
elapsed time: 0:00:30.212147
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:42:47.876991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.13
 ---- batch: 020 ----
mean loss: 368.89
 ---- batch: 030 ----
mean loss: 364.39
train mean loss: 364.05
epoch train time: 0:00:00.176515
elapsed time: 0:00:30.388809
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:42:48.053722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.14
 ---- batch: 020 ----
mean loss: 364.60
 ---- batch: 030 ----
mean loss: 364.25
train mean loss: 363.50
epoch train time: 0:00:00.171760
elapsed time: 0:00:30.560817
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:42:48.225673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.65
 ---- batch: 020 ----
mean loss: 369.31
 ---- batch: 030 ----
mean loss: 364.19
train mean loss: 363.75
epoch train time: 0:00:00.173391
elapsed time: 0:00:30.734416
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:42:48.399250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.77
 ---- batch: 020 ----
mean loss: 369.88
 ---- batch: 030 ----
mean loss: 365.48
train mean loss: 363.30
epoch train time: 0:00:00.178326
elapsed time: 0:00:30.912917
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:42:48.577765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.36
 ---- batch: 020 ----
mean loss: 363.82
 ---- batch: 030 ----
mean loss: 369.32
train mean loss: 363.30
epoch train time: 0:00:00.179473
elapsed time: 0:00:31.092541
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:42:48.757375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.79
 ---- batch: 020 ----
mean loss: 365.76
 ---- batch: 030 ----
mean loss: 365.22
train mean loss: 363.14
epoch train time: 0:00:00.174053
elapsed time: 0:00:31.266744
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:42:48.931581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.09
 ---- batch: 020 ----
mean loss: 355.44
 ---- batch: 030 ----
mean loss: 366.81
train mean loss: 362.91
epoch train time: 0:00:00.176328
elapsed time: 0:00:31.443217
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:42:49.108052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.80
 ---- batch: 020 ----
mean loss: 369.66
 ---- batch: 030 ----
mean loss: 358.15
train mean loss: 362.98
epoch train time: 0:00:00.175892
elapsed time: 0:00:31.619254
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:42:49.284089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.77
 ---- batch: 020 ----
mean loss: 356.74
 ---- batch: 030 ----
mean loss: 364.93
train mean loss: 362.77
epoch train time: 0:00:00.179335
elapsed time: 0:00:31.798729
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:42:49.463562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.95
 ---- batch: 020 ----
mean loss: 361.73
 ---- batch: 030 ----
mean loss: 363.18
train mean loss: 362.02
epoch train time: 0:00:00.179700
elapsed time: 0:00:31.978581
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:42:49.643416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.66
 ---- batch: 020 ----
mean loss: 361.31
 ---- batch: 030 ----
mean loss: 364.98
train mean loss: 362.08
epoch train time: 0:00:00.180665
elapsed time: 0:00:32.159387
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:42:49.824220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.06
 ---- batch: 020 ----
mean loss: 359.03
 ---- batch: 030 ----
mean loss: 360.60
train mean loss: 362.55
epoch train time: 0:00:00.181002
elapsed time: 0:00:32.340535
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:42:50.005377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.29
 ---- batch: 020 ----
mean loss: 361.29
 ---- batch: 030 ----
mean loss: 365.54
train mean loss: 361.75
epoch train time: 0:00:00.179821
elapsed time: 0:00:32.520536
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:42:50.185388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.43
 ---- batch: 020 ----
mean loss: 364.40
 ---- batch: 030 ----
mean loss: 369.71
train mean loss: 361.71
epoch train time: 0:00:00.179825
elapsed time: 0:00:32.700523
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:42:50.365366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.51
 ---- batch: 020 ----
mean loss: 362.62
 ---- batch: 030 ----
mean loss: 360.19
train mean loss: 362.01
epoch train time: 0:00:00.182658
elapsed time: 0:00:32.883342
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:42:50.548179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.14
 ---- batch: 020 ----
mean loss: 362.41
 ---- batch: 030 ----
mean loss: 362.98
train mean loss: 361.15
epoch train time: 0:00:00.177108
elapsed time: 0:00:33.060609
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:42:50.725442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.87
 ---- batch: 020 ----
mean loss: 364.06
 ---- batch: 030 ----
mean loss: 359.66
train mean loss: 360.69
epoch train time: 0:00:00.175867
elapsed time: 0:00:33.236613
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:42:50.901445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.14
 ---- batch: 020 ----
mean loss: 364.75
 ---- batch: 030 ----
mean loss: 360.14
train mean loss: 361.18
epoch train time: 0:00:00.172701
elapsed time: 0:00:33.409459
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:42:51.074290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.25
 ---- batch: 020 ----
mean loss: 358.30
 ---- batch: 030 ----
mean loss: 359.48
train mean loss: 360.89
epoch train time: 0:00:00.167726
elapsed time: 0:00:33.577328
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:42:51.242161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.32
 ---- batch: 020 ----
mean loss: 357.60
 ---- batch: 030 ----
mean loss: 358.48
train mean loss: 360.85
epoch train time: 0:00:00.168315
elapsed time: 0:00:33.745776
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:42:51.410617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.31
 ---- batch: 020 ----
mean loss: 358.37
 ---- batch: 030 ----
mean loss: 367.45
train mean loss: 360.38
epoch train time: 0:00:00.171889
elapsed time: 0:00:33.917807
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:42:51.582654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.88
 ---- batch: 020 ----
mean loss: 358.70
 ---- batch: 030 ----
mean loss: 364.29
train mean loss: 360.52
epoch train time: 0:00:00.173060
elapsed time: 0:00:34.091021
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:42:51.755855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.70
 ---- batch: 020 ----
mean loss: 362.13
 ---- batch: 030 ----
mean loss: 358.88
train mean loss: 360.11
epoch train time: 0:00:00.175151
elapsed time: 0:00:34.266332
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:42:51.931175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.24
 ---- batch: 020 ----
mean loss: 354.59
 ---- batch: 030 ----
mean loss: 363.05
train mean loss: 359.83
epoch train time: 0:00:00.182552
elapsed time: 0:00:34.449033
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:42:52.113865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.66
 ---- batch: 020 ----
mean loss: 357.92
 ---- batch: 030 ----
mean loss: 351.66
train mean loss: 360.22
epoch train time: 0:00:00.173191
elapsed time: 0:00:34.622384
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:42:52.287216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.71
 ---- batch: 020 ----
mean loss: 365.77
 ---- batch: 030 ----
mean loss: 360.46
train mean loss: 359.69
epoch train time: 0:00:00.177521
elapsed time: 0:00:34.800054
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:42:52.464876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.74
 ---- batch: 020 ----
mean loss: 358.54
 ---- batch: 030 ----
mean loss: 364.17
train mean loss: 359.64
epoch train time: 0:00:00.177402
elapsed time: 0:00:34.977609
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:42:52.642458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.65
 ---- batch: 020 ----
mean loss: 362.43
 ---- batch: 030 ----
mean loss: 360.68
train mean loss: 359.25
epoch train time: 0:00:00.173940
elapsed time: 0:00:35.151714
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:42:52.816547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.91
 ---- batch: 020 ----
mean loss: 360.98
 ---- batch: 030 ----
mean loss: 361.26
train mean loss: 359.10
epoch train time: 0:00:00.175815
elapsed time: 0:00:35.327679
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:42:52.992544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.95
 ---- batch: 020 ----
mean loss: 358.98
 ---- batch: 030 ----
mean loss: 369.58
train mean loss: 359.42
epoch train time: 0:00:00.175212
elapsed time: 0:00:35.503078
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:42:53.167920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.30
 ---- batch: 020 ----
mean loss: 371.42
 ---- batch: 030 ----
mean loss: 352.23
train mean loss: 359.02
epoch train time: 0:00:00.172706
elapsed time: 0:00:35.675938
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:42:53.340772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.41
 ---- batch: 020 ----
mean loss: 363.14
 ---- batch: 030 ----
mean loss: 366.43
train mean loss: 358.90
epoch train time: 0:00:00.173350
elapsed time: 0:00:35.849432
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:42:53.514267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.93
 ---- batch: 020 ----
mean loss: 356.83
 ---- batch: 030 ----
mean loss: 352.16
train mean loss: 358.69
epoch train time: 0:00:00.175048
elapsed time: 0:00:36.024619
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:42:53.689479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.61
 ---- batch: 020 ----
mean loss: 355.44
 ---- batch: 030 ----
mean loss: 365.04
train mean loss: 358.40
epoch train time: 0:00:00.177017
elapsed time: 0:00:36.201806
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:42:53.866641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.74
 ---- batch: 020 ----
mean loss: 360.82
 ---- batch: 030 ----
mean loss: 365.88
train mean loss: 358.58
epoch train time: 0:00:00.175794
elapsed time: 0:00:36.377743
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:42:54.042588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.99
 ---- batch: 020 ----
mean loss: 354.22
 ---- batch: 030 ----
mean loss: 354.21
train mean loss: 358.42
epoch train time: 0:00:00.176908
elapsed time: 0:00:36.554815
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:42:54.219663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.50
 ---- batch: 020 ----
mean loss: 356.69
 ---- batch: 030 ----
mean loss: 359.97
train mean loss: 358.20
epoch train time: 0:00:00.178164
elapsed time: 0:00:36.733138
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:42:54.397972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.40
 ---- batch: 020 ----
mean loss: 354.89
 ---- batch: 030 ----
mean loss: 361.23
train mean loss: 357.67
epoch train time: 0:00:00.182391
elapsed time: 0:00:36.915675
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:42:54.580509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.84
 ---- batch: 020 ----
mean loss: 354.66
 ---- batch: 030 ----
mean loss: 351.00
train mean loss: 357.97
epoch train time: 0:00:00.186263
elapsed time: 0:00:37.102080
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:42:54.766915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.13
 ---- batch: 020 ----
mean loss: 360.05
 ---- batch: 030 ----
mean loss: 352.86
train mean loss: 358.01
epoch train time: 0:00:00.178216
elapsed time: 0:00:37.280436
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:42:54.945288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.84
 ---- batch: 020 ----
mean loss: 356.26
 ---- batch: 030 ----
mean loss: 357.19
train mean loss: 357.86
epoch train time: 0:00:00.174963
elapsed time: 0:00:37.455557
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:42:55.120390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.31
 ---- batch: 020 ----
mean loss: 362.48
 ---- batch: 030 ----
mean loss: 356.88
train mean loss: 357.46
epoch train time: 0:00:00.172805
elapsed time: 0:00:37.628500
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:42:55.293332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.74
 ---- batch: 020 ----
mean loss: 364.54
 ---- batch: 030 ----
mean loss: 353.16
train mean loss: 357.23
epoch train time: 0:00:00.177895
elapsed time: 0:00:37.806578
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:42:55.471412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.01
 ---- batch: 020 ----
mean loss: 357.17
 ---- batch: 030 ----
mean loss: 357.05
train mean loss: 356.99
epoch train time: 0:00:00.175741
elapsed time: 0:00:37.982460
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:42:55.647293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.54
 ---- batch: 020 ----
mean loss: 358.78
 ---- batch: 030 ----
mean loss: 354.64
train mean loss: 357.04
epoch train time: 0:00:00.173986
elapsed time: 0:00:38.156584
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:42:55.821417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.94
 ---- batch: 020 ----
mean loss: 357.79
 ---- batch: 030 ----
mean loss: 358.00
train mean loss: 356.72
epoch train time: 0:00:00.177960
elapsed time: 0:00:38.334699
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:42:55.999541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.77
 ---- batch: 020 ----
mean loss: 352.39
 ---- batch: 030 ----
mean loss: 356.60
train mean loss: 356.47
epoch train time: 0:00:00.176110
elapsed time: 0:00:38.510987
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:42:56.175819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.65
 ---- batch: 020 ----
mean loss: 353.31
 ---- batch: 030 ----
mean loss: 360.58
train mean loss: 356.44
epoch train time: 0:00:00.173130
elapsed time: 0:00:38.684266
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:42:56.349089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.64
 ---- batch: 020 ----
mean loss: 356.73
 ---- batch: 030 ----
mean loss: 362.97
train mean loss: 356.28
epoch train time: 0:00:00.173937
elapsed time: 0:00:38.858342
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:42:56.523175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.77
 ---- batch: 020 ----
mean loss: 361.50
 ---- batch: 030 ----
mean loss: 350.43
train mean loss: 356.21
epoch train time: 0:00:00.175157
elapsed time: 0:00:39.033691
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:42:56.698547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.98
 ---- batch: 020 ----
mean loss: 362.96
 ---- batch: 030 ----
mean loss: 352.59
train mean loss: 356.09
epoch train time: 0:00:00.184068
elapsed time: 0:00:39.217919
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:42:56.882751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.41
 ---- batch: 020 ----
mean loss: 354.41
 ---- batch: 030 ----
mean loss: 353.21
train mean loss: 356.36
epoch train time: 0:00:00.176569
elapsed time: 0:00:39.394682
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:42:57.059538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.47
 ---- batch: 020 ----
mean loss: 354.76
 ---- batch: 030 ----
mean loss: 360.36
train mean loss: 355.75
epoch train time: 0:00:00.177659
elapsed time: 0:00:39.572509
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:42:57.237340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.32
 ---- batch: 020 ----
mean loss: 358.19
 ---- batch: 030 ----
mean loss: 359.51
train mean loss: 355.57
epoch train time: 0:00:00.173315
elapsed time: 0:00:39.745993
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:42:57.410859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.59
 ---- batch: 020 ----
mean loss: 352.96
 ---- batch: 030 ----
mean loss: 353.28
train mean loss: 355.83
epoch train time: 0:00:00.174380
elapsed time: 0:00:39.920588
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:42:57.585419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.81
 ---- batch: 020 ----
mean loss: 355.91
 ---- batch: 030 ----
mean loss: 351.60
train mean loss: 355.37
epoch train time: 0:00:00.177458
elapsed time: 0:00:40.098185
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:42:57.763036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.60
 ---- batch: 020 ----
mean loss: 359.82
 ---- batch: 030 ----
mean loss: 357.25
train mean loss: 355.38
epoch train time: 0:00:00.179083
elapsed time: 0:00:40.277426
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:42:57.942268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.86
 ---- batch: 020 ----
mean loss: 350.66
 ---- batch: 030 ----
mean loss: 355.46
train mean loss: 355.04
epoch train time: 0:00:00.171154
elapsed time: 0:00:40.448750
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:42:58.113581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.74
 ---- batch: 020 ----
mean loss: 358.34
 ---- batch: 030 ----
mean loss: 351.69
train mean loss: 355.11
epoch train time: 0:00:00.166563
elapsed time: 0:00:40.615467
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:42:58.280297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.86
 ---- batch: 020 ----
mean loss: 365.05
 ---- batch: 030 ----
mean loss: 347.75
train mean loss: 354.34
epoch train time: 0:00:00.167621
elapsed time: 0:00:40.783219
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:42:58.448050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.05
 ---- batch: 020 ----
mean loss: 351.19
 ---- batch: 030 ----
mean loss: 359.20
train mean loss: 354.55
epoch train time: 0:00:00.169391
elapsed time: 0:00:40.952747
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:42:58.617579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.37
 ---- batch: 020 ----
mean loss: 357.95
 ---- batch: 030 ----
mean loss: 357.89
train mean loss: 354.71
epoch train time: 0:00:00.183033
elapsed time: 0:00:41.135917
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:42:58.800750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.25
 ---- batch: 020 ----
mean loss: 356.85
 ---- batch: 030 ----
mean loss: 353.91
train mean loss: 354.57
epoch train time: 0:00:00.177640
elapsed time: 0:00:41.313696
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:42:58.978529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.76
 ---- batch: 020 ----
mean loss: 360.63
 ---- batch: 030 ----
mean loss: 352.24
train mean loss: 354.23
epoch train time: 0:00:00.174135
elapsed time: 0:00:41.487968
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:42:59.152800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.54
 ---- batch: 020 ----
mean loss: 359.80
 ---- batch: 030 ----
mean loss: 348.78
train mean loss: 353.68
epoch train time: 0:00:00.182511
elapsed time: 0:00:41.670621
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:42:59.335468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.79
 ---- batch: 020 ----
mean loss: 353.48
 ---- batch: 030 ----
mean loss: 354.90
train mean loss: 353.88
epoch train time: 0:00:00.177336
elapsed time: 0:00:41.848111
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:42:59.512945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.61
 ---- batch: 020 ----
mean loss: 346.57
 ---- batch: 030 ----
mean loss: 362.43
train mean loss: 354.25
epoch train time: 0:00:00.178761
elapsed time: 0:00:42.027021
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:42:59.691877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.50
 ---- batch: 020 ----
mean loss: 352.10
 ---- batch: 030 ----
mean loss: 354.07
train mean loss: 353.82
epoch train time: 0:00:00.183196
elapsed time: 0:00:42.210406
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:42:59.875241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.43
 ---- batch: 020 ----
mean loss: 356.83
 ---- batch: 030 ----
mean loss: 350.69
train mean loss: 353.54
epoch train time: 0:00:00.174118
elapsed time: 0:00:42.384691
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:43:00.049522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.83
 ---- batch: 020 ----
mean loss: 353.10
 ---- batch: 030 ----
mean loss: 351.61
train mean loss: 353.33
epoch train time: 0:00:00.169783
elapsed time: 0:00:42.554610
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:43:00.219442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.16
 ---- batch: 020 ----
mean loss: 359.88
 ---- batch: 030 ----
mean loss: 360.53
train mean loss: 353.18
epoch train time: 0:00:00.172898
elapsed time: 0:00:42.727645
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:43:00.392476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.58
 ---- batch: 020 ----
mean loss: 348.03
 ---- batch: 030 ----
mean loss: 354.97
train mean loss: 353.56
epoch train time: 0:00:00.171229
elapsed time: 0:00:42.899006
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:43:00.563840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.23
 ---- batch: 020 ----
mean loss: 356.42
 ---- batch: 030 ----
mean loss: 349.69
train mean loss: 353.19
epoch train time: 0:00:00.171796
elapsed time: 0:00:43.070976
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:43:00.735807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.17
 ---- batch: 020 ----
mean loss: 356.13
 ---- batch: 030 ----
mean loss: 353.75
train mean loss: 352.80
epoch train time: 0:00:00.181025
elapsed time: 0:00:43.252138
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:43:00.916972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.20
 ---- batch: 020 ----
mean loss: 355.12
 ---- batch: 030 ----
mean loss: 350.22
train mean loss: 352.71
epoch train time: 0:00:00.177043
elapsed time: 0:00:43.429323
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:43:01.094188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.91
 ---- batch: 020 ----
mean loss: 347.42
 ---- batch: 030 ----
mean loss: 358.62
train mean loss: 352.47
epoch train time: 0:00:00.174324
elapsed time: 0:00:43.603814
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:43:01.268647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.68
 ---- batch: 020 ----
mean loss: 351.08
 ---- batch: 030 ----
mean loss: 346.30
train mean loss: 352.29
epoch train time: 0:00:00.173428
elapsed time: 0:00:43.777380
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:43:01.442244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.96
 ---- batch: 020 ----
mean loss: 358.78
 ---- batch: 030 ----
mean loss: 346.10
train mean loss: 352.50
epoch train time: 0:00:00.188937
elapsed time: 0:00:43.966487
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:43:01.631322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.76
 ---- batch: 020 ----
mean loss: 351.98
 ---- batch: 030 ----
mean loss: 356.01
train mean loss: 352.02
epoch train time: 0:00:00.177947
elapsed time: 0:00:44.144574
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:43:01.809408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.63
 ---- batch: 020 ----
mean loss: 356.59
 ---- batch: 030 ----
mean loss: 346.60
train mean loss: 352.32
epoch train time: 0:00:00.175265
elapsed time: 0:00:44.319991
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:43:01.984825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.32
 ---- batch: 020 ----
mean loss: 350.26
 ---- batch: 030 ----
mean loss: 356.52
train mean loss: 351.74
epoch train time: 0:00:00.171108
elapsed time: 0:00:44.491262
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:43:02.156131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.67
 ---- batch: 020 ----
mean loss: 348.92
 ---- batch: 030 ----
mean loss: 352.57
train mean loss: 351.90
epoch train time: 0:00:00.172164
elapsed time: 0:00:44.663613
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:43:02.328446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.69
 ---- batch: 020 ----
mean loss: 351.67
 ---- batch: 030 ----
mean loss: 353.64
train mean loss: 351.78
epoch train time: 0:00:00.172407
elapsed time: 0:00:44.836166
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:43:02.501014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.59
 ---- batch: 020 ----
mean loss: 346.75
 ---- batch: 030 ----
mean loss: 351.08
train mean loss: 351.56
epoch train time: 0:00:00.172045
elapsed time: 0:00:45.008374
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:43:02.673216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.37
 ---- batch: 020 ----
mean loss: 357.19
 ---- batch: 030 ----
mean loss: 342.45
train mean loss: 351.56
epoch train time: 0:00:00.183300
elapsed time: 0:00:45.191836
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:43:02.856668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.36
 ---- batch: 020 ----
mean loss: 355.88
 ---- batch: 030 ----
mean loss: 347.26
train mean loss: 351.19
epoch train time: 0:00:00.183819
elapsed time: 0:00:45.375794
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:43:03.040627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.88
 ---- batch: 020 ----
mean loss: 357.80
 ---- batch: 030 ----
mean loss: 350.82
train mean loss: 351.16
epoch train time: 0:00:00.174468
elapsed time: 0:00:45.550430
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:43:03.215266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.51
 ---- batch: 020 ----
mean loss: 344.84
 ---- batch: 030 ----
mean loss: 356.34
train mean loss: 350.82
epoch train time: 0:00:00.171635
elapsed time: 0:00:45.722216
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:43:03.387049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.25
 ---- batch: 020 ----
mean loss: 353.88
 ---- batch: 030 ----
mean loss: 354.70
train mean loss: 350.54
epoch train time: 0:00:00.175952
elapsed time: 0:00:45.898305
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:43:03.563138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.33
 ---- batch: 020 ----
mean loss: 354.16
 ---- batch: 030 ----
mean loss: 350.91
train mean loss: 350.59
epoch train time: 0:00:00.174525
elapsed time: 0:00:46.072979
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:43:03.737822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.32
 ---- batch: 020 ----
mean loss: 356.04
 ---- batch: 030 ----
mean loss: 347.35
train mean loss: 350.20
epoch train time: 0:00:00.194510
elapsed time: 0:00:46.267667
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:43:03.932527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.60
 ---- batch: 020 ----
mean loss: 354.37
 ---- batch: 030 ----
mean loss: 345.22
train mean loss: 350.10
epoch train time: 0:00:00.175328
elapsed time: 0:00:46.443176
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:43:04.108007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.04
 ---- batch: 020 ----
mean loss: 349.61
 ---- batch: 030 ----
mean loss: 348.04
train mean loss: 350.16
epoch train time: 0:00:00.175561
elapsed time: 0:00:46.618875
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:43:04.283707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.38
 ---- batch: 020 ----
mean loss: 353.30
 ---- batch: 030 ----
mean loss: 349.07
train mean loss: 349.95
epoch train time: 0:00:00.176430
elapsed time: 0:00:46.795449
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:43:04.460301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.44
 ---- batch: 020 ----
mean loss: 344.79
 ---- batch: 030 ----
mean loss: 349.46
train mean loss: 349.68
epoch train time: 0:00:00.176127
elapsed time: 0:00:46.971745
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:43:04.636578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.01
 ---- batch: 020 ----
mean loss: 349.45
 ---- batch: 030 ----
mean loss: 346.69
train mean loss: 349.27
epoch train time: 0:00:00.173407
elapsed time: 0:00:47.145292
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:43:04.810125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.44
 ---- batch: 020 ----
mean loss: 353.51
 ---- batch: 030 ----
mean loss: 355.46
train mean loss: 349.41
epoch train time: 0:00:00.178120
elapsed time: 0:00:47.323564
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:43:04.988396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.29
 ---- batch: 020 ----
mean loss: 348.06
 ---- batch: 030 ----
mean loss: 348.73
train mean loss: 349.29
epoch train time: 0:00:00.174695
elapsed time: 0:00:47.498404
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:43:05.163238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.87
 ---- batch: 020 ----
mean loss: 349.80
 ---- batch: 030 ----
mean loss: 351.50
train mean loss: 348.64
epoch train time: 0:00:00.174677
elapsed time: 0:00:47.673221
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:43:05.338054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.41
 ---- batch: 020 ----
mean loss: 357.10
 ---- batch: 030 ----
mean loss: 344.99
train mean loss: 349.15
epoch train time: 0:00:00.172152
elapsed time: 0:00:47.846253
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:43:05.511103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.57
 ---- batch: 020 ----
mean loss: 345.04
 ---- batch: 030 ----
mean loss: 351.33
train mean loss: 348.86
epoch train time: 0:00:00.172664
elapsed time: 0:00:48.019137
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:43:05.683985
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.66
 ---- batch: 020 ----
mean loss: 350.11
 ---- batch: 030 ----
mean loss: 353.92
train mean loss: 348.92
epoch train time: 0:00:00.181325
elapsed time: 0:00:48.200615
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:43:05.865447
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.25
 ---- batch: 020 ----
mean loss: 351.63
 ---- batch: 030 ----
mean loss: 342.25
train mean loss: 349.01
epoch train time: 0:00:00.175299
elapsed time: 0:00:48.376052
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:43:06.040884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.15
 ---- batch: 020 ----
mean loss: 344.74
 ---- batch: 030 ----
mean loss: 355.99
train mean loss: 348.83
epoch train time: 0:00:00.170965
elapsed time: 0:00:48.547152
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:43:06.212007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.41
 ---- batch: 020 ----
mean loss: 348.07
 ---- batch: 030 ----
mean loss: 352.08
train mean loss: 348.53
epoch train time: 0:00:00.171608
elapsed time: 0:00:48.718919
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:43:06.383750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.84
 ---- batch: 020 ----
mean loss: 347.62
 ---- batch: 030 ----
mean loss: 346.36
train mean loss: 348.82
epoch train time: 0:00:00.173561
elapsed time: 0:00:48.892617
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:43:06.557448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.84
 ---- batch: 020 ----
mean loss: 349.61
 ---- batch: 030 ----
mean loss: 354.95
train mean loss: 348.59
epoch train time: 0:00:00.172441
elapsed time: 0:00:49.065201
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:43:06.730033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.07
 ---- batch: 020 ----
mean loss: 342.34
 ---- batch: 030 ----
mean loss: 350.06
train mean loss: 348.95
epoch train time: 0:00:00.180483
elapsed time: 0:00:49.245835
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:43:06.910701
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.26
 ---- batch: 020 ----
mean loss: 356.03
 ---- batch: 030 ----
mean loss: 350.17
train mean loss: 348.52
epoch train time: 0:00:00.180664
elapsed time: 0:00:49.426672
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:43:07.091505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.04
 ---- batch: 020 ----
mean loss: 346.29
 ---- batch: 030 ----
mean loss: 352.05
train mean loss: 349.27
epoch train time: 0:00:00.176148
elapsed time: 0:00:49.602982
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:43:07.267831
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.41
 ---- batch: 020 ----
mean loss: 348.21
 ---- batch: 030 ----
mean loss: 355.22
train mean loss: 348.40
epoch train time: 0:00:00.178184
elapsed time: 0:00:49.781333
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:43:07.446170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.46
 ---- batch: 020 ----
mean loss: 355.74
 ---- batch: 030 ----
mean loss: 345.73
train mean loss: 348.29
epoch train time: 0:00:00.180423
elapsed time: 0:00:49.961899
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:43:07.626775
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.95
 ---- batch: 020 ----
mean loss: 346.59
 ---- batch: 030 ----
mean loss: 344.78
train mean loss: 348.19
epoch train time: 0:00:00.179352
elapsed time: 0:00:50.141435
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:43:07.806267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.32
 ---- batch: 020 ----
mean loss: 345.10
 ---- batch: 030 ----
mean loss: 349.88
train mean loss: 349.04
epoch train time: 0:00:00.179357
elapsed time: 0:00:50.320936
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:43:07.985775
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.69
 ---- batch: 020 ----
mean loss: 344.22
 ---- batch: 030 ----
mean loss: 349.35
train mean loss: 348.49
epoch train time: 0:00:00.172514
elapsed time: 0:00:50.493591
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:43:08.158431
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.49
 ---- batch: 020 ----
mean loss: 349.96
 ---- batch: 030 ----
mean loss: 346.05
train mean loss: 348.22
epoch train time: 0:00:00.170437
elapsed time: 0:00:50.664196
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:43:08.329030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.58
 ---- batch: 020 ----
mean loss: 346.04
 ---- batch: 030 ----
mean loss: 351.27
train mean loss: 348.92
epoch train time: 0:00:00.173968
elapsed time: 0:00:50.838306
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:43:08.503146
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.13
 ---- batch: 020 ----
mean loss: 348.45
 ---- batch: 030 ----
mean loss: 345.31
train mean loss: 348.73
epoch train time: 0:00:00.176996
elapsed time: 0:00:51.015462
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:43:08.680295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.45
 ---- batch: 020 ----
mean loss: 351.21
 ---- batch: 030 ----
mean loss: 343.44
train mean loss: 348.49
epoch train time: 0:00:00.175319
elapsed time: 0:00:51.190946
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:43:08.855778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.98
 ---- batch: 020 ----
mean loss: 344.37
 ---- batch: 030 ----
mean loss: 346.99
train mean loss: 348.38
epoch train time: 0:00:00.176795
elapsed time: 0:00:51.367885
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:43:09.032718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.83
 ---- batch: 020 ----
mean loss: 346.85
 ---- batch: 030 ----
mean loss: 345.15
train mean loss: 348.50
epoch train time: 0:00:00.173349
elapsed time: 0:00:51.541388
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:43:09.206336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.78
 ---- batch: 020 ----
mean loss: 335.65
 ---- batch: 030 ----
mean loss: 351.72
train mean loss: 348.25
epoch train time: 0:00:00.170383
elapsed time: 0:00:51.712044
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:43:09.376884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.20
 ---- batch: 020 ----
mean loss: 346.27
 ---- batch: 030 ----
mean loss: 349.29
train mean loss: 348.34
epoch train time: 0:00:00.173811
elapsed time: 0:00:51.886016
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:43:09.550859
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.79
 ---- batch: 020 ----
mean loss: 348.93
 ---- batch: 030 ----
mean loss: 350.80
train mean loss: 348.54
epoch train time: 0:00:00.173062
elapsed time: 0:00:52.059228
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:43:09.724061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.15
 ---- batch: 020 ----
mean loss: 347.27
 ---- batch: 030 ----
mean loss: 344.85
train mean loss: 348.48
epoch train time: 0:00:00.176290
elapsed time: 0:00:52.235663
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:43:09.900499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.06
 ---- batch: 020 ----
mean loss: 343.86
 ---- batch: 030 ----
mean loss: 346.37
train mean loss: 348.19
epoch train time: 0:00:00.180985
elapsed time: 0:00:52.416800
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:43:10.081661
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.16
 ---- batch: 020 ----
mean loss: 341.98
 ---- batch: 030 ----
mean loss: 351.86
train mean loss: 348.20
epoch train time: 0:00:00.179748
elapsed time: 0:00:52.596727
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:43:10.261558
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.25
 ---- batch: 020 ----
mean loss: 357.52
 ---- batch: 030 ----
mean loss: 349.71
train mean loss: 348.19
epoch train time: 0:00:00.173095
elapsed time: 0:00:52.769970
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:43:10.434803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.50
 ---- batch: 020 ----
mean loss: 350.76
 ---- batch: 030 ----
mean loss: 348.69
train mean loss: 348.86
epoch train time: 0:00:00.175061
elapsed time: 0:00:52.945181
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:43:10.610016
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.16
 ---- batch: 020 ----
mean loss: 355.41
 ---- batch: 030 ----
mean loss: 340.37
train mean loss: 347.85
epoch train time: 0:00:00.178368
elapsed time: 0:00:53.123709
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:43:10.788545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.33
 ---- batch: 020 ----
mean loss: 338.90
 ---- batch: 030 ----
mean loss: 356.08
train mean loss: 348.11
epoch train time: 0:00:00.180220
elapsed time: 0:00:53.304072
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:43:10.968915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.66
 ---- batch: 020 ----
mean loss: 342.16
 ---- batch: 030 ----
mean loss: 355.80
train mean loss: 348.37
epoch train time: 0:00:00.179330
elapsed time: 0:00:53.483548
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:43:11.148382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.71
 ---- batch: 020 ----
mean loss: 351.87
 ---- batch: 030 ----
mean loss: 351.33
train mean loss: 348.11
epoch train time: 0:00:00.173905
elapsed time: 0:00:53.657603
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:43:11.322424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.93
 ---- batch: 020 ----
mean loss: 348.02
 ---- batch: 030 ----
mean loss: 349.97
train mean loss: 348.46
epoch train time: 0:00:00.175412
elapsed time: 0:00:53.833149
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:43:11.497982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.07
 ---- batch: 020 ----
mean loss: 347.22
 ---- batch: 030 ----
mean loss: 352.36
train mean loss: 347.94
epoch train time: 0:00:00.177303
elapsed time: 0:00:54.010596
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:43:11.675449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.59
 ---- batch: 020 ----
mean loss: 350.90
 ---- batch: 030 ----
mean loss: 342.76
train mean loss: 348.30
epoch train time: 0:00:00.184124
elapsed time: 0:00:54.194879
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:43:11.859714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.15
 ---- batch: 020 ----
mean loss: 353.80
 ---- batch: 030 ----
mean loss: 346.85
train mean loss: 348.19
epoch train time: 0:00:00.187600
elapsed time: 0:00:54.382635
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:43:12.047488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.50
 ---- batch: 020 ----
mean loss: 354.57
 ---- batch: 030 ----
mean loss: 337.17
train mean loss: 348.23
epoch train time: 0:00:00.177648
elapsed time: 0:00:54.560442
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:43:12.225276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.13
 ---- batch: 020 ----
mean loss: 353.58
 ---- batch: 030 ----
mean loss: 346.47
train mean loss: 348.20
epoch train time: 0:00:00.172408
elapsed time: 0:00:54.732998
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:43:12.397830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.91
 ---- batch: 020 ----
mean loss: 342.99
 ---- batch: 030 ----
mean loss: 346.33
train mean loss: 347.68
epoch train time: 0:00:00.175312
elapsed time: 0:00:54.908448
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:43:12.573281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.46
 ---- batch: 020 ----
mean loss: 347.63
 ---- batch: 030 ----
mean loss: 347.17
train mean loss: 347.94
epoch train time: 0:00:00.175517
elapsed time: 0:00:55.084112
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:43:12.748947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.50
 ---- batch: 020 ----
mean loss: 343.94
 ---- batch: 030 ----
mean loss: 348.27
train mean loss: 348.41
epoch train time: 0:00:00.178942
elapsed time: 0:00:55.263239
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:43:12.928072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.60
 ---- batch: 020 ----
mean loss: 353.64
 ---- batch: 030 ----
mean loss: 346.12
train mean loss: 347.77
epoch train time: 0:00:00.176389
elapsed time: 0:00:55.439767
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:43:13.104601
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.36
 ---- batch: 020 ----
mean loss: 350.30
 ---- batch: 030 ----
mean loss: 340.91
train mean loss: 348.21
epoch train time: 0:00:00.174827
elapsed time: 0:00:55.614734
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:43:13.279568
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.65
 ---- batch: 020 ----
mean loss: 344.65
 ---- batch: 030 ----
mean loss: 348.15
train mean loss: 348.13
epoch train time: 0:00:00.172911
elapsed time: 0:00:55.787798
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:43:13.452631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.14
 ---- batch: 020 ----
mean loss: 359.29
 ---- batch: 030 ----
mean loss: 343.30
train mean loss: 348.18
epoch train time: 0:00:00.174927
elapsed time: 0:00:55.962863
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:43:13.627696
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.36
 ---- batch: 020 ----
mean loss: 349.62
 ---- batch: 030 ----
mean loss: 342.33
train mean loss: 348.19
epoch train time: 0:00:00.177827
elapsed time: 0:00:56.140865
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:43:13.805712
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.48
 ---- batch: 020 ----
mean loss: 346.14
 ---- batch: 030 ----
mean loss: 353.76
train mean loss: 347.99
epoch train time: 0:00:00.183094
elapsed time: 0:00:56.324121
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:43:13.988976
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.33
 ---- batch: 020 ----
mean loss: 342.80
 ---- batch: 030 ----
mean loss: 349.71
train mean loss: 347.76
epoch train time: 0:00:00.177162
elapsed time: 0:00:56.503676
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_2/checkpoint.pth.tar
**** end time: 2019-09-27 16:43:14.168491 ****
