Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 13519
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-26 21:45:21.862538 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 11, 11]           1,120
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
    BayesianConv2d-4            [-1, 14, 4, 11]             448
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
    BayesianLinear-8                    [-1, 1]             616
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,184
Trainable params: 2,184
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 21:45:21.874348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4786.75
 ---- batch: 020 ----
mean loss: 4634.00
 ---- batch: 030 ----
mean loss: 4474.65
 ---- batch: 040 ----
mean loss: 4290.61
 ---- batch: 050 ----
mean loss: 4109.86
 ---- batch: 060 ----
mean loss: 3884.81
 ---- batch: 070 ----
mean loss: 3728.29
 ---- batch: 080 ----
mean loss: 3545.78
 ---- batch: 090 ----
mean loss: 3357.93
 ---- batch: 100 ----
mean loss: 3209.60
 ---- batch: 110 ----
mean loss: 3045.29
train mean loss: 3888.36
epoch train time: 0:00:34.772681
elapsed time: 0:00:34.788017
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 21:45:56.650603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2789.20
 ---- batch: 020 ----
mean loss: 2594.52
 ---- batch: 030 ----
mean loss: 2454.89
 ---- batch: 040 ----
mean loss: 2295.98
 ---- batch: 050 ----
mean loss: 2160.40
 ---- batch: 060 ----
mean loss: 2016.81
 ---- batch: 070 ----
mean loss: 1870.09
 ---- batch: 080 ----
mean loss: 1771.01
 ---- batch: 090 ----
mean loss: 1655.69
 ---- batch: 100 ----
mean loss: 1542.39
 ---- batch: 110 ----
mean loss: 1446.57
train mean loss: 2037.85
epoch train time: 0:00:02.501547
elapsed time: 0:00:37.289786
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 21:45:59.152599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1353.53
 ---- batch: 020 ----
mean loss: 1298.44
 ---- batch: 030 ----
mean loss: 1241.60
 ---- batch: 040 ----
mean loss: 1181.57
 ---- batch: 050 ----
mean loss: 1126.51
 ---- batch: 060 ----
mean loss: 1071.42
 ---- batch: 070 ----
mean loss: 1071.13
 ---- batch: 080 ----
mean loss: 1022.96
 ---- batch: 090 ----
mean loss: 994.95
 ---- batch: 100 ----
mean loss: 972.93
 ---- batch: 110 ----
mean loss: 954.30
train mean loss: 1112.11
epoch train time: 0:00:02.513201
elapsed time: 0:00:39.803428
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 21:46:01.666190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.16
 ---- batch: 020 ----
mean loss: 919.11
 ---- batch: 030 ----
mean loss: 925.52
 ---- batch: 040 ----
mean loss: 899.20
 ---- batch: 050 ----
mean loss: 884.65
 ---- batch: 060 ----
mean loss: 882.02
 ---- batch: 070 ----
mean loss: 880.28
 ---- batch: 080 ----
mean loss: 856.36
 ---- batch: 090 ----
mean loss: 879.52
 ---- batch: 100 ----
mean loss: 879.60
 ---- batch: 110 ----
mean loss: 865.47
train mean loss: 892.17
epoch train time: 0:00:02.484124
elapsed time: 0:00:42.287981
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 21:46:04.150798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.67
 ---- batch: 020 ----
mean loss: 858.83
 ---- batch: 030 ----
mean loss: 872.41
 ---- batch: 040 ----
mean loss: 875.38
 ---- batch: 050 ----
mean loss: 866.95
 ---- batch: 060 ----
mean loss: 856.18
 ---- batch: 070 ----
mean loss: 862.51
 ---- batch: 080 ----
mean loss: 854.36
 ---- batch: 090 ----
mean loss: 854.71
 ---- batch: 100 ----
mean loss: 866.05
 ---- batch: 110 ----
mean loss: 866.52
train mean loss: 862.93
epoch train time: 0:00:02.490389
elapsed time: 0:00:44.778812
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 21:46:06.641577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.54
 ---- batch: 020 ----
mean loss: 858.98
 ---- batch: 030 ----
mean loss: 863.19
 ---- batch: 040 ----
mean loss: 839.12
 ---- batch: 050 ----
mean loss: 852.99
 ---- batch: 060 ----
mean loss: 874.43
 ---- batch: 070 ----
mean loss: 847.85
 ---- batch: 080 ----
mean loss: 872.51
 ---- batch: 090 ----
mean loss: 848.06
 ---- batch: 100 ----
mean loss: 858.57
 ---- batch: 110 ----
mean loss: 856.27
train mean loss: 856.91
epoch train time: 0:00:02.523931
elapsed time: 0:00:47.303161
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 21:46:09.165936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.73
 ---- batch: 020 ----
mean loss: 842.74
 ---- batch: 030 ----
mean loss: 847.30
 ---- batch: 040 ----
mean loss: 846.20
 ---- batch: 050 ----
mean loss: 823.67
 ---- batch: 060 ----
mean loss: 850.86
 ---- batch: 070 ----
mean loss: 843.64
 ---- batch: 080 ----
mean loss: 872.16
 ---- batch: 090 ----
mean loss: 863.64
 ---- batch: 100 ----
mean loss: 863.63
 ---- batch: 110 ----
mean loss: 858.18
train mean loss: 852.33
epoch train time: 0:00:02.503447
elapsed time: 0:00:49.807029
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 21:46:11.669805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.30
 ---- batch: 020 ----
mean loss: 834.00
 ---- batch: 030 ----
mean loss: 867.41
 ---- batch: 040 ----
mean loss: 848.19
 ---- batch: 050 ----
mean loss: 848.73
 ---- batch: 060 ----
mean loss: 857.47
 ---- batch: 070 ----
mean loss: 821.28
 ---- batch: 080 ----
mean loss: 847.42
 ---- batch: 090 ----
mean loss: 846.23
 ---- batch: 100 ----
mean loss: 856.18
 ---- batch: 110 ----
mean loss: 852.13
train mean loss: 846.94
epoch train time: 0:00:02.519335
elapsed time: 0:00:52.326771
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 21:46:14.189551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.58
 ---- batch: 020 ----
mean loss: 844.73
 ---- batch: 030 ----
mean loss: 829.16
 ---- batch: 040 ----
mean loss: 831.32
 ---- batch: 050 ----
mean loss: 846.09
 ---- batch: 060 ----
mean loss: 827.33
 ---- batch: 070 ----
mean loss: 848.47
 ---- batch: 080 ----
mean loss: 836.81
 ---- batch: 090 ----
mean loss: 837.15
 ---- batch: 100 ----
mean loss: 858.40
 ---- batch: 110 ----
mean loss: 848.53
train mean loss: 841.46
epoch train time: 0:00:02.509061
elapsed time: 0:00:54.836276
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 21:46:16.699056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 847.51
 ---- batch: 030 ----
mean loss: 820.22
 ---- batch: 040 ----
mean loss: 852.85
 ---- batch: 050 ----
mean loss: 827.98
 ---- batch: 060 ----
mean loss: 841.65
 ---- batch: 070 ----
mean loss: 832.50
 ---- batch: 080 ----
mean loss: 863.18
 ---- batch: 090 ----
mean loss: 828.52
 ---- batch: 100 ----
mean loss: 822.57
 ---- batch: 110 ----
mean loss: 844.21
train mean loss: 836.88
epoch train time: 0:00:02.504947
elapsed time: 0:00:57.341637
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 21:46:19.204407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.02
 ---- batch: 020 ----
mean loss: 816.75
 ---- batch: 030 ----
mean loss: 834.17
 ---- batch: 040 ----
mean loss: 851.03
 ---- batch: 050 ----
mean loss: 825.40
 ---- batch: 060 ----
mean loss: 827.26
 ---- batch: 070 ----
mean loss: 831.52
 ---- batch: 080 ----
mean loss: 827.40
 ---- batch: 090 ----
mean loss: 844.65
 ---- batch: 100 ----
mean loss: 825.24
 ---- batch: 110 ----
mean loss: 825.44
train mean loss: 831.70
epoch train time: 0:00:02.530015
elapsed time: 0:00:59.872109
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 21:46:21.734842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 810.86
 ---- batch: 030 ----
mean loss: 847.92
 ---- batch: 040 ----
mean loss: 836.38
 ---- batch: 050 ----
mean loss: 824.22
 ---- batch: 060 ----
mean loss: 818.73
 ---- batch: 070 ----
mean loss: 825.27
 ---- batch: 080 ----
mean loss: 817.58
 ---- batch: 090 ----
mean loss: 829.70
 ---- batch: 100 ----
mean loss: 818.66
 ---- batch: 110 ----
mean loss: 797.04
train mean loss: 826.21
epoch train time: 0:00:02.505595
elapsed time: 0:01:02.378142
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 21:46:24.240915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.08
 ---- batch: 020 ----
mean loss: 829.40
 ---- batch: 030 ----
mean loss: 832.45
 ---- batch: 040 ----
mean loss: 815.95
 ---- batch: 050 ----
mean loss: 817.22
 ---- batch: 060 ----
mean loss: 813.16
 ---- batch: 070 ----
mean loss: 831.59
 ---- batch: 080 ----
mean loss: 799.25
 ---- batch: 090 ----
mean loss: 819.29
 ---- batch: 100 ----
mean loss: 828.35
 ---- batch: 110 ----
mean loss: 803.16
train mean loss: 821.30
epoch train time: 0:00:02.527408
elapsed time: 0:01:04.905954
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 21:46:26.768751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 805.65
 ---- batch: 020 ----
mean loss: 821.60
 ---- batch: 030 ----
mean loss: 810.57
 ---- batch: 040 ----
mean loss: 801.83
 ---- batch: 050 ----
mean loss: 806.63
 ---- batch: 060 ----
mean loss: 824.52
 ---- batch: 070 ----
mean loss: 820.88
 ---- batch: 080 ----
mean loss: 815.58
 ---- batch: 090 ----
mean loss: 804.91
 ---- batch: 100 ----
mean loss: 828.96
 ---- batch: 110 ----
mean loss: 830.63
train mean loss: 816.19
epoch train time: 0:00:02.478750
elapsed time: 0:01:07.385191
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 21:46:29.247979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 815.32
 ---- batch: 020 ----
mean loss: 804.17
 ---- batch: 030 ----
mean loss: 821.00
 ---- batch: 040 ----
mean loss: 819.44
 ---- batch: 050 ----
mean loss: 813.44
 ---- batch: 060 ----
mean loss: 801.32
 ---- batch: 070 ----
mean loss: 802.83
 ---- batch: 080 ----
mean loss: 795.29
 ---- batch: 090 ----
mean loss: 806.44
 ---- batch: 100 ----
mean loss: 808.60
 ---- batch: 110 ----
mean loss: 826.70
train mean loss: 809.51
epoch train time: 0:00:02.513395
elapsed time: 0:01:09.899052
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 21:46:31.761855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.47
 ---- batch: 020 ----
mean loss: 812.50
 ---- batch: 030 ----
mean loss: 806.97
 ---- batch: 040 ----
mean loss: 787.77
 ---- batch: 050 ----
mean loss: 801.97
 ---- batch: 060 ----
mean loss: 813.80
 ---- batch: 070 ----
mean loss: 814.25
 ---- batch: 080 ----
mean loss: 794.86
 ---- batch: 090 ----
mean loss: 797.18
 ---- batch: 100 ----
mean loss: 820.08
 ---- batch: 110 ----
mean loss: 791.98
train mean loss: 805.85
epoch train time: 0:00:02.507020
elapsed time: 0:01:12.406531
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 21:46:34.269295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 796.10
 ---- batch: 020 ----
mean loss: 773.74
 ---- batch: 030 ----
mean loss: 797.03
 ---- batch: 040 ----
mean loss: 823.12
 ---- batch: 050 ----
mean loss: 819.05
 ---- batch: 060 ----
mean loss: 819.38
 ---- batch: 070 ----
mean loss: 811.56
 ---- batch: 080 ----
mean loss: 797.72
 ---- batch: 090 ----
mean loss: 791.70
 ---- batch: 100 ----
mean loss: 796.36
 ---- batch: 110 ----
mean loss: 792.43
train mean loss: 801.63
epoch train time: 0:00:02.494846
elapsed time: 0:01:14.901783
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 21:46:36.764551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 776.49
 ---- batch: 020 ----
mean loss: 806.35
 ---- batch: 030 ----
mean loss: 782.13
 ---- batch: 040 ----
mean loss: 807.65
 ---- batch: 050 ----
mean loss: 814.17
 ---- batch: 060 ----
mean loss: 784.96
 ---- batch: 070 ----
mean loss: 809.81
 ---- batch: 080 ----
mean loss: 790.11
 ---- batch: 090 ----
mean loss: 792.43
 ---- batch: 100 ----
mean loss: 811.18
 ---- batch: 110 ----
mean loss: 805.20
train mean loss: 797.89
epoch train time: 0:00:02.497708
elapsed time: 0:01:17.399922
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 21:46:39.262679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 793.72
 ---- batch: 020 ----
mean loss: 810.92
 ---- batch: 030 ----
mean loss: 790.42
 ---- batch: 040 ----
mean loss: 776.42
 ---- batch: 050 ----
mean loss: 782.01
 ---- batch: 060 ----
mean loss: 794.42
 ---- batch: 070 ----
mean loss: 792.36
 ---- batch: 080 ----
mean loss: 790.65
 ---- batch: 090 ----
mean loss: 800.08
 ---- batch: 100 ----
mean loss: 787.41
 ---- batch: 110 ----
mean loss: 811.30
train mean loss: 792.64
epoch train time: 0:00:02.486596
elapsed time: 0:01:19.886907
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 21:46:41.749656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 791.75
 ---- batch: 020 ----
mean loss: 797.45
 ---- batch: 030 ----
mean loss: 802.36
 ---- batch: 040 ----
mean loss: 789.12
 ---- batch: 050 ----
mean loss: 777.92
 ---- batch: 060 ----
mean loss: 800.30
 ---- batch: 070 ----
mean loss: 790.36
 ---- batch: 080 ----
mean loss: 793.70
 ---- batch: 090 ----
mean loss: 781.84
 ---- batch: 100 ----
mean loss: 789.10
 ---- batch: 110 ----
mean loss: 789.72
train mean loss: 790.31
epoch train time: 0:00:02.499552
elapsed time: 0:01:22.386825
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 21:46:44.249582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 751.67
 ---- batch: 020 ----
mean loss: 821.75
 ---- batch: 030 ----
mean loss: 790.82
 ---- batch: 040 ----
mean loss: 808.65
 ---- batch: 050 ----
mean loss: 791.46
 ---- batch: 060 ----
mean loss: 791.78
 ---- batch: 070 ----
mean loss: 782.50
 ---- batch: 080 ----
mean loss: 792.99
 ---- batch: 090 ----
mean loss: 775.71
 ---- batch: 100 ----
mean loss: 771.04
 ---- batch: 110 ----
mean loss: 767.24
train mean loss: 785.88
epoch train time: 0:00:02.500140
elapsed time: 0:01:24.887359
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 21:46:46.750038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 772.62
 ---- batch: 020 ----
mean loss: 796.69
 ---- batch: 030 ----
mean loss: 773.17
 ---- batch: 040 ----
mean loss: 798.78
 ---- batch: 050 ----
mean loss: 784.15
 ---- batch: 060 ----
mean loss: 778.24
 ---- batch: 070 ----
mean loss: 788.54
 ---- batch: 080 ----
mean loss: 781.32
 ---- batch: 090 ----
mean loss: 773.66
 ---- batch: 100 ----
mean loss: 770.07
 ---- batch: 110 ----
mean loss: 785.95
train mean loss: 781.50
epoch train time: 0:00:02.479636
elapsed time: 0:01:27.367366
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 21:46:49.230155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.80
 ---- batch: 020 ----
mean loss: 766.60
 ---- batch: 030 ----
mean loss: 755.13
 ---- batch: 040 ----
mean loss: 782.91
 ---- batch: 050 ----
mean loss: 796.24
 ---- batch: 060 ----
mean loss: 774.76
 ---- batch: 070 ----
mean loss: 799.67
 ---- batch: 080 ----
mean loss: 770.98
 ---- batch: 090 ----
mean loss: 769.82
 ---- batch: 100 ----
mean loss: 802.51
 ---- batch: 110 ----
mean loss: 770.68
train mean loss: 777.90
epoch train time: 0:00:02.499867
elapsed time: 0:01:29.867673
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 21:46:51.730470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.40
 ---- batch: 020 ----
mean loss: 774.21
 ---- batch: 030 ----
mean loss: 788.28
 ---- batch: 040 ----
mean loss: 768.02
 ---- batch: 050 ----
mean loss: 788.18
 ---- batch: 060 ----
mean loss: 765.14
 ---- batch: 070 ----
mean loss: 770.92
 ---- batch: 080 ----
mean loss: 782.30
 ---- batch: 090 ----
mean loss: 770.55
 ---- batch: 100 ----
mean loss: 781.64
 ---- batch: 110 ----
mean loss: 757.06
train mean loss: 773.04
epoch train time: 0:00:02.502718
elapsed time: 0:01:32.370812
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 21:46:54.233580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 769.99
 ---- batch: 020 ----
mean loss: 770.80
 ---- batch: 030 ----
mean loss: 756.76
 ---- batch: 040 ----
mean loss: 778.69
 ---- batch: 050 ----
mean loss: 772.63
 ---- batch: 060 ----
mean loss: 778.99
 ---- batch: 070 ----
mean loss: 749.24
 ---- batch: 080 ----
mean loss: 773.06
 ---- batch: 090 ----
mean loss: 779.10
 ---- batch: 100 ----
mean loss: 756.74
 ---- batch: 110 ----
mean loss: 775.57
train mean loss: 769.53
epoch train time: 0:00:02.511809
elapsed time: 0:01:34.883059
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 21:46:56.745901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 768.44
 ---- batch: 020 ----
mean loss: 765.19
 ---- batch: 030 ----
mean loss: 758.31
 ---- batch: 040 ----
mean loss: 765.54
 ---- batch: 050 ----
mean loss: 761.65
 ---- batch: 060 ----
mean loss: 771.98
 ---- batch: 070 ----
mean loss: 775.10
 ---- batch: 080 ----
mean loss: 750.29
 ---- batch: 090 ----
mean loss: 774.77
 ---- batch: 100 ----
mean loss: 758.47
 ---- batch: 110 ----
mean loss: 758.19
train mean loss: 763.55
epoch train time: 0:00:02.510970
elapsed time: 0:01:37.394538
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 21:46:59.257342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 767.81
 ---- batch: 020 ----
mean loss: 767.93
 ---- batch: 030 ----
mean loss: 773.52
 ---- batch: 040 ----
mean loss: 766.61
 ---- batch: 050 ----
mean loss: 755.09
 ---- batch: 060 ----
mean loss: 744.62
 ---- batch: 070 ----
mean loss: 744.13
 ---- batch: 080 ----
mean loss: 777.32
 ---- batch: 090 ----
mean loss: 772.71
 ---- batch: 100 ----
mean loss: 746.50
 ---- batch: 110 ----
mean loss: 752.83
train mean loss: 760.36
epoch train time: 0:00:02.485722
elapsed time: 0:01:39.880687
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 21:47:01.743440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.81
 ---- batch: 020 ----
mean loss: 743.63
 ---- batch: 030 ----
mean loss: 770.90
 ---- batch: 040 ----
mean loss: 774.97
 ---- batch: 050 ----
mean loss: 748.67
 ---- batch: 060 ----
mean loss: 752.79
 ---- batch: 070 ----
mean loss: 751.93
 ---- batch: 080 ----
mean loss: 751.24
 ---- batch: 090 ----
mean loss: 757.60
 ---- batch: 100 ----
mean loss: 745.27
 ---- batch: 110 ----
mean loss: 755.31
train mean loss: 754.62
epoch train time: 0:00:02.510625
elapsed time: 0:01:42.391708
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 21:47:04.254490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.89
 ---- batch: 020 ----
mean loss: 750.13
 ---- batch: 030 ----
mean loss: 759.27
 ---- batch: 040 ----
mean loss: 770.82
 ---- batch: 050 ----
mean loss: 771.08
 ---- batch: 060 ----
mean loss: 740.65
 ---- batch: 070 ----
mean loss: 752.07
 ---- batch: 080 ----
mean loss: 750.01
 ---- batch: 090 ----
mean loss: 734.87
 ---- batch: 100 ----
mean loss: 757.24
 ---- batch: 110 ----
mean loss: 747.50
train mean loss: 750.11
epoch train time: 0:00:02.494510
elapsed time: 0:01:44.886641
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 21:47:06.749445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 751.96
 ---- batch: 020 ----
mean loss: 728.52
 ---- batch: 030 ----
mean loss: 742.01
 ---- batch: 040 ----
mean loss: 739.64
 ---- batch: 050 ----
mean loss: 753.09
 ---- batch: 060 ----
mean loss: 742.17
 ---- batch: 070 ----
mean loss: 741.60
 ---- batch: 080 ----
mean loss: 755.42
 ---- batch: 090 ----
mean loss: 745.55
 ---- batch: 100 ----
mean loss: 752.96
 ---- batch: 110 ----
mean loss: 744.77
train mean loss: 744.56
epoch train time: 0:00:02.528770
elapsed time: 0:01:47.415864
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 21:47:09.278629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 742.83
 ---- batch: 020 ----
mean loss: 734.67
 ---- batch: 030 ----
mean loss: 746.18
 ---- batch: 040 ----
mean loss: 749.54
 ---- batch: 050 ----
mean loss: 743.48
 ---- batch: 060 ----
mean loss: 738.45
 ---- batch: 070 ----
mean loss: 706.87
 ---- batch: 080 ----
mean loss: 748.07
 ---- batch: 090 ----
mean loss: 739.85
 ---- batch: 100 ----
mean loss: 742.70
 ---- batch: 110 ----
mean loss: 746.64
train mean loss: 740.04
epoch train time: 0:00:02.537059
elapsed time: 0:01:49.953345
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 21:47:11.816135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 740.22
 ---- batch: 020 ----
mean loss: 720.12
 ---- batch: 030 ----
mean loss: 753.73
 ---- batch: 040 ----
mean loss: 755.25
 ---- batch: 050 ----
mean loss: 717.13
 ---- batch: 060 ----
mean loss: 737.37
 ---- batch: 070 ----
mean loss: 736.65
 ---- batch: 080 ----
mean loss: 734.87
 ---- batch: 090 ----
mean loss: 730.18
 ---- batch: 100 ----
mean loss: 733.05
 ---- batch: 110 ----
mean loss: 720.11
train mean loss: 734.34
epoch train time: 0:00:02.477564
elapsed time: 0:01:52.431356
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 21:47:14.294150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 719.98
 ---- batch: 020 ----
mean loss: 721.85
 ---- batch: 030 ----
mean loss: 742.63
 ---- batch: 040 ----
mean loss: 744.39
 ---- batch: 050 ----
mean loss: 729.68
 ---- batch: 060 ----
mean loss: 739.72
 ---- batch: 070 ----
mean loss: 703.41
 ---- batch: 080 ----
mean loss: 734.86
 ---- batch: 090 ----
mean loss: 725.48
 ---- batch: 100 ----
mean loss: 731.75
 ---- batch: 110 ----
mean loss: 717.41
train mean loss: 728.08
epoch train time: 0:00:02.514330
elapsed time: 0:01:54.946124
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 21:47:16.808896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 716.77
 ---- batch: 020 ----
mean loss: 725.35
 ---- batch: 030 ----
mean loss: 716.42
 ---- batch: 040 ----
mean loss: 723.21
 ---- batch: 050 ----
mean loss: 706.18
 ---- batch: 060 ----
mean loss: 716.37
 ---- batch: 070 ----
mean loss: 726.81
 ---- batch: 080 ----
mean loss: 729.92
 ---- batch: 090 ----
mean loss: 727.86
 ---- batch: 100 ----
mean loss: 730.38
 ---- batch: 110 ----
mean loss: 722.61
train mean loss: 721.36
epoch train time: 0:00:02.508390
elapsed time: 0:01:57.454931
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 21:47:19.317760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 717.05
 ---- batch: 020 ----
mean loss: 694.43
 ---- batch: 030 ----
mean loss: 713.23
 ---- batch: 040 ----
mean loss: 729.77
 ---- batch: 050 ----
mean loss: 717.75
 ---- batch: 060 ----
mean loss: 726.15
 ---- batch: 070 ----
mean loss: 702.63
 ---- batch: 080 ----
mean loss: 721.96
 ---- batch: 090 ----
mean loss: 722.80
 ---- batch: 100 ----
mean loss: 722.36
 ---- batch: 110 ----
mean loss: 718.08
train mean loss: 716.29
epoch train time: 0:00:02.504521
elapsed time: 0:01:59.959950
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 21:47:21.822645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.04
 ---- batch: 020 ----
mean loss: 720.29
 ---- batch: 030 ----
mean loss: 707.71
 ---- batch: 040 ----
mean loss: 706.91
 ---- batch: 050 ----
mean loss: 708.97
 ---- batch: 060 ----
mean loss: 701.02
 ---- batch: 070 ----
mean loss: 702.57
 ---- batch: 080 ----
mean loss: 709.39
 ---- batch: 090 ----
mean loss: 693.88
 ---- batch: 100 ----
mean loss: 705.74
 ---- batch: 110 ----
mean loss: 699.17
train mean loss: 708.34
epoch train time: 0:00:02.526203
elapsed time: 0:02:02.486496
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 21:47:24.349265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 716.56
 ---- batch: 020 ----
mean loss: 701.61
 ---- batch: 030 ----
mean loss: 709.17
 ---- batch: 040 ----
mean loss: 704.55
 ---- batch: 050 ----
mean loss: 691.88
 ---- batch: 060 ----
mean loss: 709.96
 ---- batch: 070 ----
mean loss: 693.89
 ---- batch: 080 ----
mean loss: 691.18
 ---- batch: 090 ----
mean loss: 711.17
 ---- batch: 100 ----
mean loss: 703.56
 ---- batch: 110 ----
mean loss: 682.41
train mean loss: 701.52
epoch train time: 0:00:02.517950
elapsed time: 0:02:05.004875
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 21:47:26.867641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.65
 ---- batch: 020 ----
mean loss: 704.67
 ---- batch: 030 ----
mean loss: 695.64
 ---- batch: 040 ----
mean loss: 696.04
 ---- batch: 050 ----
mean loss: 713.17
 ---- batch: 060 ----
mean loss: 680.39
 ---- batch: 070 ----
mean loss: 686.91
 ---- batch: 080 ----
mean loss: 683.77
 ---- batch: 090 ----
mean loss: 682.42
 ---- batch: 100 ----
mean loss: 692.71
 ---- batch: 110 ----
mean loss: 708.50
train mean loss: 694.65
epoch train time: 0:00:02.503331
elapsed time: 0:02:07.508690
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 21:47:29.371498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 699.41
 ---- batch: 020 ----
mean loss: 690.35
 ---- batch: 030 ----
mean loss: 691.96
 ---- batch: 040 ----
mean loss: 692.41
 ---- batch: 050 ----
mean loss: 667.15
 ---- batch: 060 ----
mean loss: 680.15
 ---- batch: 070 ----
mean loss: 684.39
 ---- batch: 080 ----
mean loss: 695.72
 ---- batch: 090 ----
mean loss: 678.59
 ---- batch: 100 ----
mean loss: 677.50
 ---- batch: 110 ----
mean loss: 688.20
train mean loss: 686.31
epoch train time: 0:00:02.490201
elapsed time: 0:02:09.999353
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 21:47:31.862168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.44
 ---- batch: 020 ----
mean loss: 684.23
 ---- batch: 030 ----
mean loss: 669.78
 ---- batch: 040 ----
mean loss: 681.04
 ---- batch: 050 ----
mean loss: 681.04
 ---- batch: 060 ----
mean loss: 673.59
 ---- batch: 070 ----
mean loss: 674.24
 ---- batch: 080 ----
mean loss: 685.55
 ---- batch: 090 ----
mean loss: 677.18
 ---- batch: 100 ----
mean loss: 683.20
 ---- batch: 110 ----
mean loss: 681.26
train mean loss: 678.56
epoch train time: 0:00:02.472978
elapsed time: 0:02:12.472781
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 21:47:34.335561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.60
 ---- batch: 020 ----
mean loss: 683.95
 ---- batch: 030 ----
mean loss: 662.73
 ---- batch: 040 ----
mean loss: 681.27
 ---- batch: 050 ----
mean loss: 677.82
 ---- batch: 060 ----
mean loss: 670.84
 ---- batch: 070 ----
mean loss: 670.41
 ---- batch: 080 ----
mean loss: 674.23
 ---- batch: 090 ----
mean loss: 655.07
 ---- batch: 100 ----
mean loss: 664.04
 ---- batch: 110 ----
mean loss: 666.51
train mean loss: 670.09
epoch train time: 0:00:02.532601
elapsed time: 0:02:15.005975
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 21:47:36.868883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.59
 ---- batch: 020 ----
mean loss: 653.89
 ---- batch: 030 ----
mean loss: 666.23
 ---- batch: 040 ----
mean loss: 662.83
 ---- batch: 050 ----
mean loss: 652.62
 ---- batch: 060 ----
mean loss: 651.66
 ---- batch: 070 ----
mean loss: 672.32
 ---- batch: 080 ----
mean loss: 675.04
 ---- batch: 090 ----
mean loss: 662.24
 ---- batch: 100 ----
mean loss: 656.77
 ---- batch: 110 ----
mean loss: 644.51
train mean loss: 660.10
epoch train time: 0:00:02.501571
elapsed time: 0:02:17.508136
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 21:47:39.370923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 654.65
 ---- batch: 020 ----
mean loss: 647.16
 ---- batch: 030 ----
mean loss: 653.94
 ---- batch: 040 ----
mean loss: 654.41
 ---- batch: 050 ----
mean loss: 656.47
 ---- batch: 060 ----
mean loss: 646.42
 ---- batch: 070 ----
mean loss: 648.68
 ---- batch: 080 ----
mean loss: 648.59
 ---- batch: 090 ----
mean loss: 640.59
 ---- batch: 100 ----
mean loss: 631.32
 ---- batch: 110 ----
mean loss: 643.70
train mean loss: 647.78
epoch train time: 0:00:02.494106
elapsed time: 0:02:20.002705
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 21:47:41.865471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 632.28
 ---- batch: 020 ----
mean loss: 629.80
 ---- batch: 030 ----
mean loss: 645.50
 ---- batch: 040 ----
mean loss: 635.13
 ---- batch: 050 ----
mean loss: 633.89
 ---- batch: 060 ----
mean loss: 630.26
 ---- batch: 070 ----
mean loss: 616.83
 ---- batch: 080 ----
mean loss: 636.37
 ---- batch: 090 ----
mean loss: 623.35
 ---- batch: 100 ----
mean loss: 629.34
 ---- batch: 110 ----
mean loss: 622.25
train mean loss: 630.81
epoch train time: 0:00:02.527112
elapsed time: 0:02:22.530229
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 21:47:44.392996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 615.71
 ---- batch: 020 ----
mean loss: 616.76
 ---- batch: 030 ----
mean loss: 612.17
 ---- batch: 040 ----
mean loss: 618.51
 ---- batch: 050 ----
mean loss: 610.74
 ---- batch: 060 ----
mean loss: 640.61
 ---- batch: 070 ----
mean loss: 618.71
 ---- batch: 080 ----
mean loss: 598.47
 ---- batch: 090 ----
mean loss: 603.70
 ---- batch: 100 ----
mean loss: 588.74
 ---- batch: 110 ----
mean loss: 602.53
train mean loss: 611.24
epoch train time: 0:00:02.478393
elapsed time: 0:02:25.009061
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 21:47:46.871869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.39
 ---- batch: 020 ----
mean loss: 615.78
 ---- batch: 030 ----
mean loss: 604.01
 ---- batch: 040 ----
mean loss: 599.93
 ---- batch: 050 ----
mean loss: 585.70
 ---- batch: 060 ----
mean loss: 583.53
 ---- batch: 070 ----
mean loss: 582.22
 ---- batch: 080 ----
mean loss: 586.17
 ---- batch: 090 ----
mean loss: 575.75
 ---- batch: 100 ----
mean loss: 587.95
 ---- batch: 110 ----
mean loss: 587.52
train mean loss: 589.73
epoch train time: 0:00:02.475362
elapsed time: 0:02:27.484857
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 21:47:49.347625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 572.88
 ---- batch: 020 ----
mean loss: 575.02
 ---- batch: 030 ----
mean loss: 578.43
 ---- batch: 040 ----
mean loss: 566.95
 ---- batch: 050 ----
mean loss: 567.96
 ---- batch: 060 ----
mean loss: 554.31
 ---- batch: 070 ----
mean loss: 582.79
 ---- batch: 080 ----
mean loss: 559.20
 ---- batch: 090 ----
mean loss: 562.76
 ---- batch: 100 ----
mean loss: 553.43
 ---- batch: 110 ----
mean loss: 561.97
train mean loss: 566.64
epoch train time: 0:00:02.498385
elapsed time: 0:02:29.983649
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 21:47:51.846412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 554.45
 ---- batch: 020 ----
mean loss: 561.99
 ---- batch: 030 ----
mean loss: 544.08
 ---- batch: 040 ----
mean loss: 533.22
 ---- batch: 050 ----
mean loss: 552.06
 ---- batch: 060 ----
mean loss: 538.00
 ---- batch: 070 ----
mean loss: 557.30
 ---- batch: 080 ----
mean loss: 537.69
 ---- batch: 090 ----
mean loss: 543.89
 ---- batch: 100 ----
mean loss: 518.18
 ---- batch: 110 ----
mean loss: 532.73
train mean loss: 542.60
epoch train time: 0:00:02.505530
elapsed time: 0:02:32.489573
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 21:47:54.352367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 531.58
 ---- batch: 020 ----
mean loss: 526.71
 ---- batch: 030 ----
mean loss: 534.24
 ---- batch: 040 ----
mean loss: 513.44
 ---- batch: 050 ----
mean loss: 513.91
 ---- batch: 060 ----
mean loss: 529.34
 ---- batch: 070 ----
mean loss: 528.55
 ---- batch: 080 ----
mean loss: 524.01
 ---- batch: 090 ----
mean loss: 510.49
 ---- batch: 100 ----
mean loss: 514.72
 ---- batch: 110 ----
mean loss: 506.91
train mean loss: 521.36
epoch train time: 0:00:02.538331
elapsed time: 0:02:35.028327
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 21:47:56.891107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.14
 ---- batch: 020 ----
mean loss: 514.56
 ---- batch: 030 ----
mean loss: 507.26
 ---- batch: 040 ----
mean loss: 510.95
 ---- batch: 050 ----
mean loss: 496.77
 ---- batch: 060 ----
mean loss: 500.98
 ---- batch: 070 ----
mean loss: 495.38
 ---- batch: 080 ----
mean loss: 503.05
 ---- batch: 090 ----
mean loss: 501.68
 ---- batch: 100 ----
mean loss: 488.35
 ---- batch: 110 ----
mean loss: 481.96
train mean loss: 501.57
epoch train time: 0:00:02.493181
elapsed time: 0:02:37.521933
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 21:47:59.384738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.27
 ---- batch: 020 ----
mean loss: 494.95
 ---- batch: 030 ----
mean loss: 479.40
 ---- batch: 040 ----
mean loss: 491.87
 ---- batch: 050 ----
mean loss: 489.69
 ---- batch: 060 ----
mean loss: 484.99
 ---- batch: 070 ----
mean loss: 484.84
 ---- batch: 080 ----
mean loss: 481.64
 ---- batch: 090 ----
mean loss: 477.98
 ---- batch: 100 ----
mean loss: 472.23
 ---- batch: 110 ----
mean loss: 473.55
train mean loss: 483.75
epoch train time: 0:00:02.544264
elapsed time: 0:02:40.066637
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 21:48:01.929429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.32
 ---- batch: 020 ----
mean loss: 478.27
 ---- batch: 030 ----
mean loss: 486.08
 ---- batch: 040 ----
mean loss: 465.33
 ---- batch: 050 ----
mean loss: 458.66
 ---- batch: 060 ----
mean loss: 466.94
 ---- batch: 070 ----
mean loss: 465.33
 ---- batch: 080 ----
mean loss: 461.25
 ---- batch: 090 ----
mean loss: 455.48
 ---- batch: 100 ----
mean loss: 468.41
 ---- batch: 110 ----
mean loss: 460.94
train mean loss: 466.78
epoch train time: 0:00:02.544815
elapsed time: 0:02:42.611877
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 21:48:04.474643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.57
 ---- batch: 020 ----
mean loss: 463.98
 ---- batch: 030 ----
mean loss: 454.89
 ---- batch: 040 ----
mean loss: 445.10
 ---- batch: 050 ----
mean loss: 439.72
 ---- batch: 060 ----
mean loss: 449.97
 ---- batch: 070 ----
mean loss: 454.44
 ---- batch: 080 ----
mean loss: 455.00
 ---- batch: 090 ----
mean loss: 442.70
 ---- batch: 100 ----
mean loss: 441.00
 ---- batch: 110 ----
mean loss: 454.62
train mean loss: 450.89
epoch train time: 0:00:02.551563
elapsed time: 0:02:45.163848
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 21:48:07.026645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.02
 ---- batch: 020 ----
mean loss: 442.18
 ---- batch: 030 ----
mean loss: 446.78
 ---- batch: 040 ----
mean loss: 431.59
 ---- batch: 050 ----
mean loss: 429.87
 ---- batch: 060 ----
mean loss: 441.12
 ---- batch: 070 ----
mean loss: 442.00
 ---- batch: 080 ----
mean loss: 437.56
 ---- batch: 090 ----
mean loss: 421.98
 ---- batch: 100 ----
mean loss: 437.02
 ---- batch: 110 ----
mean loss: 428.42
train mean loss: 436.49
epoch train time: 0:00:02.547608
elapsed time: 0:02:47.711900
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 21:48:09.574671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.98
 ---- batch: 020 ----
mean loss: 426.65
 ---- batch: 030 ----
mean loss: 434.92
 ---- batch: 040 ----
mean loss: 432.75
 ---- batch: 050 ----
mean loss: 429.72
 ---- batch: 060 ----
mean loss: 404.33
 ---- batch: 070 ----
mean loss: 412.79
 ---- batch: 080 ----
mean loss: 414.23
 ---- batch: 090 ----
mean loss: 416.99
 ---- batch: 100 ----
mean loss: 432.17
 ---- batch: 110 ----
mean loss: 418.07
train mean loss: 422.43
epoch train time: 0:00:02.522245
elapsed time: 0:02:50.234549
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 21:48:12.097339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.24
 ---- batch: 020 ----
mean loss: 416.18
 ---- batch: 030 ----
mean loss: 424.38
 ---- batch: 040 ----
mean loss: 407.99
 ---- batch: 050 ----
mean loss: 411.33
 ---- batch: 060 ----
mean loss: 402.93
 ---- batch: 070 ----
mean loss: 398.08
 ---- batch: 080 ----
mean loss: 407.36
 ---- batch: 090 ----
mean loss: 409.69
 ---- batch: 100 ----
mean loss: 394.74
 ---- batch: 110 ----
mean loss: 404.86
train mean loss: 409.68
epoch train time: 0:00:02.485595
elapsed time: 0:02:52.720579
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 21:48:14.583348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.76
 ---- batch: 020 ----
mean loss: 401.16
 ---- batch: 030 ----
mean loss: 405.22
 ---- batch: 040 ----
mean loss: 397.92
 ---- batch: 050 ----
mean loss: 406.27
 ---- batch: 060 ----
mean loss: 410.28
 ---- batch: 070 ----
mean loss: 384.93
 ---- batch: 080 ----
mean loss: 385.29
 ---- batch: 090 ----
mean loss: 392.11
 ---- batch: 100 ----
mean loss: 393.63
 ---- batch: 110 ----
mean loss: 388.56
train mean loss: 397.30
epoch train time: 0:00:02.492447
elapsed time: 0:02:55.213440
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 21:48:17.076062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.97
 ---- batch: 020 ----
mean loss: 393.95
 ---- batch: 030 ----
mean loss: 378.97
 ---- batch: 040 ----
mean loss: 380.99
 ---- batch: 050 ----
mean loss: 385.91
 ---- batch: 060 ----
mean loss: 380.20
 ---- batch: 070 ----
mean loss: 385.18
 ---- batch: 080 ----
mean loss: 373.26
 ---- batch: 090 ----
mean loss: 392.78
 ---- batch: 100 ----
mean loss: 383.60
 ---- batch: 110 ----
mean loss: 388.67
train mean loss: 385.65
epoch train time: 0:00:02.501581
elapsed time: 0:02:57.715288
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 21:48:19.578075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.35
 ---- batch: 020 ----
mean loss: 382.35
 ---- batch: 030 ----
mean loss: 377.19
 ---- batch: 040 ----
mean loss: 376.29
 ---- batch: 050 ----
mean loss: 373.62
 ---- batch: 060 ----
mean loss: 356.25
 ---- batch: 070 ----
mean loss: 372.20
 ---- batch: 080 ----
mean loss: 379.97
 ---- batch: 090 ----
mean loss: 367.99
 ---- batch: 100 ----
mean loss: 374.34
 ---- batch: 110 ----
mean loss: 373.50
train mean loss: 373.62
epoch train time: 0:00:02.505125
elapsed time: 0:03:00.220830
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 21:48:22.083600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.75
 ---- batch: 020 ----
mean loss: 369.33
 ---- batch: 030 ----
mean loss: 375.88
 ---- batch: 040 ----
mean loss: 363.63
 ---- batch: 050 ----
mean loss: 353.64
 ---- batch: 060 ----
mean loss: 361.00
 ---- batch: 070 ----
mean loss: 346.86
 ---- batch: 080 ----
mean loss: 360.61
 ---- batch: 090 ----
mean loss: 372.33
 ---- batch: 100 ----
mean loss: 361.43
 ---- batch: 110 ----
mean loss: 360.15
train mean loss: 363.19
epoch train time: 0:00:02.514592
elapsed time: 0:03:02.735835
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 21:48:24.598615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.93
 ---- batch: 020 ----
mean loss: 350.52
 ---- batch: 030 ----
mean loss: 363.01
 ---- batch: 040 ----
mean loss: 354.30
 ---- batch: 050 ----
mean loss: 363.44
 ---- batch: 060 ----
mean loss: 345.33
 ---- batch: 070 ----
mean loss: 344.43
 ---- batch: 080 ----
mean loss: 350.52
 ---- batch: 090 ----
mean loss: 346.72
 ---- batch: 100 ----
mean loss: 351.75
 ---- batch: 110 ----
mean loss: 349.16
train mean loss: 352.61
epoch train time: 0:00:02.533922
elapsed time: 0:03:05.270176
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 21:48:27.132984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.77
 ---- batch: 020 ----
mean loss: 346.68
 ---- batch: 030 ----
mean loss: 347.33
 ---- batch: 040 ----
mean loss: 336.16
 ---- batch: 050 ----
mean loss: 341.95
 ---- batch: 060 ----
mean loss: 345.79
 ---- batch: 070 ----
mean loss: 349.66
 ---- batch: 080 ----
mean loss: 336.72
 ---- batch: 090 ----
mean loss: 345.74
 ---- batch: 100 ----
mean loss: 331.76
 ---- batch: 110 ----
mean loss: 347.63
train mean loss: 341.93
epoch train time: 0:00:02.504234
elapsed time: 0:03:07.774883
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 21:48:29.637695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.89
 ---- batch: 020 ----
mean loss: 326.55
 ---- batch: 030 ----
mean loss: 330.34
 ---- batch: 040 ----
mean loss: 340.96
 ---- batch: 050 ----
mean loss: 334.59
 ---- batch: 060 ----
mean loss: 334.27
 ---- batch: 070 ----
mean loss: 318.89
 ---- batch: 080 ----
mean loss: 332.95
 ---- batch: 090 ----
mean loss: 331.04
 ---- batch: 100 ----
mean loss: 330.07
 ---- batch: 110 ----
mean loss: 335.45
train mean loss: 332.33
epoch train time: 0:00:02.512327
elapsed time: 0:03:10.287650
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 21:48:32.150427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.80
 ---- batch: 020 ----
mean loss: 318.87
 ---- batch: 030 ----
mean loss: 321.44
 ---- batch: 040 ----
mean loss: 326.79
 ---- batch: 050 ----
mean loss: 312.86
 ---- batch: 060 ----
mean loss: 322.85
 ---- batch: 070 ----
mean loss: 329.42
 ---- batch: 080 ----
mean loss: 318.25
 ---- batch: 090 ----
mean loss: 316.57
 ---- batch: 100 ----
mean loss: 325.16
 ---- batch: 110 ----
mean loss: 335.04
train mean loss: 322.62
epoch train time: 0:00:02.497146
elapsed time: 0:03:12.785235
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 21:48:34.648020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.66
 ---- batch: 020 ----
mean loss: 319.85
 ---- batch: 030 ----
mean loss: 315.24
 ---- batch: 040 ----
mean loss: 301.56
 ---- batch: 050 ----
mean loss: 314.10
 ---- batch: 060 ----
mean loss: 318.77
 ---- batch: 070 ----
mean loss: 314.21
 ---- batch: 080 ----
mean loss: 317.08
 ---- batch: 090 ----
mean loss: 323.69
 ---- batch: 100 ----
mean loss: 309.20
 ---- batch: 110 ----
mean loss: 305.55
train mean loss: 314.35
epoch train time: 0:00:02.500822
elapsed time: 0:03:15.286489
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 21:48:37.149275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.62
 ---- batch: 020 ----
mean loss: 304.54
 ---- batch: 030 ----
mean loss: 308.04
 ---- batch: 040 ----
mean loss: 311.54
 ---- batch: 050 ----
mean loss: 316.27
 ---- batch: 060 ----
mean loss: 300.95
 ---- batch: 070 ----
mean loss: 306.71
 ---- batch: 080 ----
mean loss: 305.52
 ---- batch: 090 ----
mean loss: 296.47
 ---- batch: 100 ----
mean loss: 297.26
 ---- batch: 110 ----
mean loss: 307.31
train mean loss: 306.55
epoch train time: 0:00:02.472540
elapsed time: 0:03:17.759468
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 21:48:39.622229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.27
 ---- batch: 020 ----
mean loss: 304.12
 ---- batch: 030 ----
mean loss: 305.50
 ---- batch: 040 ----
mean loss: 300.58
 ---- batch: 050 ----
mean loss: 299.03
 ---- batch: 060 ----
mean loss: 304.30
 ---- batch: 070 ----
mean loss: 297.39
 ---- batch: 080 ----
mean loss: 304.76
 ---- batch: 090 ----
mean loss: 295.93
 ---- batch: 100 ----
mean loss: 293.88
 ---- batch: 110 ----
mean loss: 290.45
train mean loss: 299.60
epoch train time: 0:00:02.497360
elapsed time: 0:03:20.257212
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 21:48:42.119984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.50
 ---- batch: 020 ----
mean loss: 293.71
 ---- batch: 030 ----
mean loss: 284.25
 ---- batch: 040 ----
mean loss: 298.96
 ---- batch: 050 ----
mean loss: 290.40
 ---- batch: 060 ----
mean loss: 284.81
 ---- batch: 070 ----
mean loss: 287.49
 ---- batch: 080 ----
mean loss: 286.85
 ---- batch: 090 ----
mean loss: 291.55
 ---- batch: 100 ----
mean loss: 290.92
 ---- batch: 110 ----
mean loss: 297.39
train mean loss: 291.97
epoch train time: 0:00:02.499173
elapsed time: 0:03:22.756779
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 21:48:44.619565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.52
 ---- batch: 020 ----
mean loss: 289.20
 ---- batch: 030 ----
mean loss: 280.57
 ---- batch: 040 ----
mean loss: 290.78
 ---- batch: 050 ----
mean loss: 287.37
 ---- batch: 060 ----
mean loss: 295.92
 ---- batch: 070 ----
mean loss: 289.57
 ---- batch: 080 ----
mean loss: 277.03
 ---- batch: 090 ----
mean loss: 283.89
 ---- batch: 100 ----
mean loss: 272.52
 ---- batch: 110 ----
mean loss: 287.41
train mean loss: 284.84
epoch train time: 0:00:02.508192
elapsed time: 0:03:25.265406
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 21:48:47.128166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.69
 ---- batch: 020 ----
mean loss: 280.71
 ---- batch: 030 ----
mean loss: 281.31
 ---- batch: 040 ----
mean loss: 273.91
 ---- batch: 050 ----
mean loss: 276.30
 ---- batch: 060 ----
mean loss: 284.99
 ---- batch: 070 ----
mean loss: 280.21
 ---- batch: 080 ----
mean loss: 271.38
 ---- batch: 090 ----
mean loss: 280.89
 ---- batch: 100 ----
mean loss: 271.01
 ---- batch: 110 ----
mean loss: 278.80
train mean loss: 278.65
epoch train time: 0:00:02.513237
elapsed time: 0:03:27.779017
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 21:48:49.641801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.16
 ---- batch: 020 ----
mean loss: 276.79
 ---- batch: 030 ----
mean loss: 278.18
 ---- batch: 040 ----
mean loss: 274.84
 ---- batch: 050 ----
mean loss: 264.68
 ---- batch: 060 ----
mean loss: 264.64
 ---- batch: 070 ----
mean loss: 276.75
 ---- batch: 080 ----
mean loss: 272.83
 ---- batch: 090 ----
mean loss: 273.11
 ---- batch: 100 ----
mean loss: 271.04
 ---- batch: 110 ----
mean loss: 270.99
train mean loss: 272.09
epoch train time: 0:00:02.499461
elapsed time: 0:03:30.278938
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 21:48:52.141785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.78
 ---- batch: 020 ----
mean loss: 266.35
 ---- batch: 030 ----
mean loss: 270.98
 ---- batch: 040 ----
mean loss: 266.57
 ---- batch: 050 ----
mean loss: 252.89
 ---- batch: 060 ----
mean loss: 264.71
 ---- batch: 070 ----
mean loss: 269.51
 ---- batch: 080 ----
mean loss: 275.34
 ---- batch: 090 ----
mean loss: 266.86
 ---- batch: 100 ----
mean loss: 270.64
 ---- batch: 110 ----
mean loss: 264.11
train mean loss: 266.73
epoch train time: 0:00:02.511742
elapsed time: 0:03:32.791211
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 21:48:54.653972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.92
 ---- batch: 020 ----
mean loss: 265.28
 ---- batch: 030 ----
mean loss: 261.49
 ---- batch: 040 ----
mean loss: 264.65
 ---- batch: 050 ----
mean loss: 268.81
 ---- batch: 060 ----
mean loss: 260.16
 ---- batch: 070 ----
mean loss: 262.21
 ---- batch: 080 ----
mean loss: 264.37
 ---- batch: 090 ----
mean loss: 264.42
 ---- batch: 100 ----
mean loss: 263.46
 ---- batch: 110 ----
mean loss: 258.58
train mean loss: 262.48
epoch train time: 0:00:02.526361
elapsed time: 0:03:35.317982
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 21:48:57.180750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.86
 ---- batch: 020 ----
mean loss: 265.68
 ---- batch: 030 ----
mean loss: 253.57
 ---- batch: 040 ----
mean loss: 254.45
 ---- batch: 050 ----
mean loss: 250.38
 ---- batch: 060 ----
mean loss: 266.17
 ---- batch: 070 ----
mean loss: 255.80
 ---- batch: 080 ----
mean loss: 262.21
 ---- batch: 090 ----
mean loss: 258.92
 ---- batch: 100 ----
mean loss: 255.22
 ---- batch: 110 ----
mean loss: 256.90
train mean loss: 258.46
epoch train time: 0:00:02.518344
elapsed time: 0:03:37.836731
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 21:48:59.699513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.75
 ---- batch: 020 ----
mean loss: 253.00
 ---- batch: 030 ----
mean loss: 250.74
 ---- batch: 040 ----
mean loss: 251.00
 ---- batch: 050 ----
mean loss: 250.52
 ---- batch: 060 ----
mean loss: 261.14
 ---- batch: 070 ----
mean loss: 261.71
 ---- batch: 080 ----
mean loss: 256.62
 ---- batch: 090 ----
mean loss: 251.60
 ---- batch: 100 ----
mean loss: 256.15
 ---- batch: 110 ----
mean loss: 251.51
train mean loss: 255.22
epoch train time: 0:00:02.514796
elapsed time: 0:03:40.351926
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 21:49:02.214725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.36
 ---- batch: 020 ----
mean loss: 254.38
 ---- batch: 030 ----
mean loss: 263.79
 ---- batch: 040 ----
mean loss: 258.81
 ---- batch: 050 ----
mean loss: 257.91
 ---- batch: 060 ----
mean loss: 245.65
 ---- batch: 070 ----
mean loss: 246.93
 ---- batch: 080 ----
mean loss: 246.07
 ---- batch: 090 ----
mean loss: 249.18
 ---- batch: 100 ----
mean loss: 249.54
 ---- batch: 110 ----
mean loss: 252.91
train mean loss: 252.48
epoch train time: 0:00:02.520757
elapsed time: 0:03:42.873124
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 21:49:04.735927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.98
 ---- batch: 020 ----
mean loss: 244.39
 ---- batch: 030 ----
mean loss: 254.49
 ---- batch: 040 ----
mean loss: 254.62
 ---- batch: 050 ----
mean loss: 249.44
 ---- batch: 060 ----
mean loss: 250.04
 ---- batch: 070 ----
mean loss: 249.78
 ---- batch: 080 ----
mean loss: 248.40
 ---- batch: 090 ----
mean loss: 250.56
 ---- batch: 100 ----
mean loss: 245.45
 ---- batch: 110 ----
mean loss: 251.18
train mean loss: 249.43
epoch train time: 0:00:02.501211
elapsed time: 0:03:45.374765
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 21:49:07.237539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.77
 ---- batch: 020 ----
mean loss: 254.76
 ---- batch: 030 ----
mean loss: 243.72
 ---- batch: 040 ----
mean loss: 248.40
 ---- batch: 050 ----
mean loss: 252.25
 ---- batch: 060 ----
mean loss: 250.73
 ---- batch: 070 ----
mean loss: 242.08
 ---- batch: 080 ----
mean loss: 242.90
 ---- batch: 090 ----
mean loss: 254.40
 ---- batch: 100 ----
mean loss: 244.27
 ---- batch: 110 ----
mean loss: 240.05
train mean loss: 247.57
epoch train time: 0:00:02.513182
elapsed time: 0:03:47.888405
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 21:49:09.751173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.91
 ---- batch: 020 ----
mean loss: 244.54
 ---- batch: 030 ----
mean loss: 238.92
 ---- batch: 040 ----
mean loss: 245.03
 ---- batch: 050 ----
mean loss: 249.23
 ---- batch: 060 ----
mean loss: 253.76
 ---- batch: 070 ----
mean loss: 247.31
 ---- batch: 080 ----
mean loss: 250.61
 ---- batch: 090 ----
mean loss: 241.42
 ---- batch: 100 ----
mean loss: 243.29
 ---- batch: 110 ----
mean loss: 244.04
train mean loss: 245.29
epoch train time: 0:00:02.509673
elapsed time: 0:03:50.398460
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 21:49:12.261207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.08
 ---- batch: 020 ----
mean loss: 247.83
 ---- batch: 030 ----
mean loss: 242.04
 ---- batch: 040 ----
mean loss: 251.36
 ---- batch: 050 ----
mean loss: 244.04
 ---- batch: 060 ----
mean loss: 239.26
 ---- batch: 070 ----
mean loss: 242.22
 ---- batch: 080 ----
mean loss: 239.08
 ---- batch: 090 ----
mean loss: 253.53
 ---- batch: 100 ----
mean loss: 230.95
 ---- batch: 110 ----
mean loss: 250.64
train mean loss: 243.61
epoch train time: 0:00:02.494793
elapsed time: 0:03:52.893649
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 21:49:14.756432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.02
 ---- batch: 020 ----
mean loss: 247.66
 ---- batch: 030 ----
mean loss: 233.89
 ---- batch: 040 ----
mean loss: 237.65
 ---- batch: 050 ----
mean loss: 247.63
 ---- batch: 060 ----
mean loss: 243.01
 ---- batch: 070 ----
mean loss: 252.56
 ---- batch: 080 ----
mean loss: 239.10
 ---- batch: 090 ----
mean loss: 242.86
 ---- batch: 100 ----
mean loss: 242.74
 ---- batch: 110 ----
mean loss: 234.62
train mean loss: 241.94
epoch train time: 0:00:02.506018
elapsed time: 0:03:55.400081
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 21:49:17.262883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.82
 ---- batch: 020 ----
mean loss: 242.79
 ---- batch: 030 ----
mean loss: 235.80
 ---- batch: 040 ----
mean loss: 238.55
 ---- batch: 050 ----
mean loss: 239.59
 ---- batch: 060 ----
mean loss: 239.61
 ---- batch: 070 ----
mean loss: 235.53
 ---- batch: 080 ----
mean loss: 252.49
 ---- batch: 090 ----
mean loss: 242.89
 ---- batch: 100 ----
mean loss: 231.62
 ---- batch: 110 ----
mean loss: 247.89
train mean loss: 240.40
epoch train time: 0:00:02.523548
elapsed time: 0:03:57.924078
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 21:49:19.786862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.66
 ---- batch: 020 ----
mean loss: 244.40
 ---- batch: 030 ----
mean loss: 244.53
 ---- batch: 040 ----
mean loss: 244.91
 ---- batch: 050 ----
mean loss: 234.00
 ---- batch: 060 ----
mean loss: 241.46
 ---- batch: 070 ----
mean loss: 239.65
 ---- batch: 080 ----
mean loss: 235.75
 ---- batch: 090 ----
mean loss: 236.72
 ---- batch: 100 ----
mean loss: 237.67
 ---- batch: 110 ----
mean loss: 237.69
train mean loss: 239.00
epoch train time: 0:00:02.495364
elapsed time: 0:04:00.419877
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 21:49:22.282695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.52
 ---- batch: 020 ----
mean loss: 241.85
 ---- batch: 030 ----
mean loss: 233.39
 ---- batch: 040 ----
mean loss: 233.33
 ---- batch: 050 ----
mean loss: 241.58
 ---- batch: 060 ----
mean loss: 235.85
 ---- batch: 070 ----
mean loss: 228.25
 ---- batch: 080 ----
mean loss: 236.96
 ---- batch: 090 ----
mean loss: 235.55
 ---- batch: 100 ----
mean loss: 241.19
 ---- batch: 110 ----
mean loss: 237.46
train mean loss: 237.07
epoch train time: 0:00:02.509723
elapsed time: 0:04:02.930061
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 21:49:24.792845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.76
 ---- batch: 020 ----
mean loss: 241.11
 ---- batch: 030 ----
mean loss: 234.68
 ---- batch: 040 ----
mean loss: 231.37
 ---- batch: 050 ----
mean loss: 229.00
 ---- batch: 060 ----
mean loss: 236.88
 ---- batch: 070 ----
mean loss: 244.54
 ---- batch: 080 ----
mean loss: 246.31
 ---- batch: 090 ----
mean loss: 238.09
 ---- batch: 100 ----
mean loss: 236.09
 ---- batch: 110 ----
mean loss: 229.76
train mean loss: 236.19
epoch train time: 0:00:02.511729
elapsed time: 0:04:05.442214
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 21:49:27.305031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.75
 ---- batch: 020 ----
mean loss: 242.90
 ---- batch: 030 ----
mean loss: 226.34
 ---- batch: 040 ----
mean loss: 241.75
 ---- batch: 050 ----
mean loss: 240.86
 ---- batch: 060 ----
mean loss: 232.42
 ---- batch: 070 ----
mean loss: 237.90
 ---- batch: 080 ----
mean loss: 238.11
 ---- batch: 090 ----
mean loss: 233.95
 ---- batch: 100 ----
mean loss: 235.21
 ---- batch: 110 ----
mean loss: 230.31
train mean loss: 234.71
epoch train time: 0:00:02.513038
elapsed time: 0:04:07.955731
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 21:49:29.818527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.21
 ---- batch: 020 ----
mean loss: 247.02
 ---- batch: 030 ----
mean loss: 227.41
 ---- batch: 040 ----
mean loss: 230.27
 ---- batch: 050 ----
mean loss: 234.57
 ---- batch: 060 ----
mean loss: 230.25
 ---- batch: 070 ----
mean loss: 227.78
 ---- batch: 080 ----
mean loss: 229.63
 ---- batch: 090 ----
mean loss: 236.96
 ---- batch: 100 ----
mean loss: 233.10
 ---- batch: 110 ----
mean loss: 238.50
train mean loss: 233.54
epoch train time: 0:00:02.485613
elapsed time: 0:04:10.441775
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 21:49:32.304569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.44
 ---- batch: 020 ----
mean loss: 232.69
 ---- batch: 030 ----
mean loss: 232.30
 ---- batch: 040 ----
mean loss: 224.43
 ---- batch: 050 ----
mean loss: 232.36
 ---- batch: 060 ----
mean loss: 230.62
 ---- batch: 070 ----
mean loss: 231.02
 ---- batch: 080 ----
mean loss: 242.56
 ---- batch: 090 ----
mean loss: 234.00
 ---- batch: 100 ----
mean loss: 223.31
 ---- batch: 110 ----
mean loss: 232.10
train mean loss: 232.33
epoch train time: 0:00:02.473900
elapsed time: 0:04:12.916158
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 21:49:34.778971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.90
 ---- batch: 020 ----
mean loss: 226.47
 ---- batch: 030 ----
mean loss: 236.52
 ---- batch: 040 ----
mean loss: 234.67
 ---- batch: 050 ----
mean loss: 236.20
 ---- batch: 060 ----
mean loss: 225.43
 ---- batch: 070 ----
mean loss: 229.12
 ---- batch: 080 ----
mean loss: 236.02
 ---- batch: 090 ----
mean loss: 235.20
 ---- batch: 100 ----
mean loss: 235.02
 ---- batch: 110 ----
mean loss: 231.07
train mean loss: 231.29
epoch train time: 0:00:02.487764
elapsed time: 0:04:15.404363
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 21:49:37.267140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.34
 ---- batch: 020 ----
mean loss: 230.52
 ---- batch: 030 ----
mean loss: 236.54
 ---- batch: 040 ----
mean loss: 238.35
 ---- batch: 050 ----
mean loss: 224.04
 ---- batch: 060 ----
mean loss: 225.46
 ---- batch: 070 ----
mean loss: 232.76
 ---- batch: 080 ----
mean loss: 230.35
 ---- batch: 090 ----
mean loss: 230.97
 ---- batch: 100 ----
mean loss: 227.44
 ---- batch: 110 ----
mean loss: 221.85
train mean loss: 230.36
epoch train time: 0:00:02.500041
elapsed time: 0:04:17.904827
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 21:49:39.767619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.82
 ---- batch: 020 ----
mean loss: 227.06
 ---- batch: 030 ----
mean loss: 222.72
 ---- batch: 040 ----
mean loss: 228.84
 ---- batch: 050 ----
mean loss: 223.20
 ---- batch: 060 ----
mean loss: 236.06
 ---- batch: 070 ----
mean loss: 236.41
 ---- batch: 080 ----
mean loss: 234.19
 ---- batch: 090 ----
mean loss: 224.79
 ---- batch: 100 ----
mean loss: 228.30
 ---- batch: 110 ----
mean loss: 230.72
train mean loss: 229.11
epoch train time: 0:00:02.483158
elapsed time: 0:04:20.388401
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 21:49:42.251170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.69
 ---- batch: 020 ----
mean loss: 235.98
 ---- batch: 030 ----
mean loss: 236.93
 ---- batch: 040 ----
mean loss: 224.32
 ---- batch: 050 ----
mean loss: 226.49
 ---- batch: 060 ----
mean loss: 229.12
 ---- batch: 070 ----
mean loss: 229.77
 ---- batch: 080 ----
mean loss: 223.53
 ---- batch: 090 ----
mean loss: 224.30
 ---- batch: 100 ----
mean loss: 216.65
 ---- batch: 110 ----
mean loss: 234.30
train mean loss: 228.11
epoch train time: 0:00:02.497742
elapsed time: 0:04:22.886555
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 21:49:44.749321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.61
 ---- batch: 020 ----
mean loss: 227.47
 ---- batch: 030 ----
mean loss: 226.37
 ---- batch: 040 ----
mean loss: 235.43
 ---- batch: 050 ----
mean loss: 220.23
 ---- batch: 060 ----
mean loss: 224.07
 ---- batch: 070 ----
mean loss: 230.29
 ---- batch: 080 ----
mean loss: 223.67
 ---- batch: 090 ----
mean loss: 221.71
 ---- batch: 100 ----
mean loss: 221.69
 ---- batch: 110 ----
mean loss: 233.58
train mean loss: 227.03
epoch train time: 0:00:02.520277
elapsed time: 0:04:25.407247
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 21:49:47.270022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.93
 ---- batch: 020 ----
mean loss: 234.15
 ---- batch: 030 ----
mean loss: 227.11
 ---- batch: 040 ----
mean loss: 230.30
 ---- batch: 050 ----
mean loss: 225.72
 ---- batch: 060 ----
mean loss: 215.40
 ---- batch: 070 ----
mean loss: 228.55
 ---- batch: 080 ----
mean loss: 217.18
 ---- batch: 090 ----
mean loss: 232.18
 ---- batch: 100 ----
mean loss: 225.67
 ---- batch: 110 ----
mean loss: 222.39
train mean loss: 225.93
epoch train time: 0:00:02.585310
elapsed time: 0:04:27.993032
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 21:49:49.855852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.41
 ---- batch: 020 ----
mean loss: 233.62
 ---- batch: 030 ----
mean loss: 221.87
 ---- batch: 040 ----
mean loss: 224.15
 ---- batch: 050 ----
mean loss: 221.75
 ---- batch: 060 ----
mean loss: 224.84
 ---- batch: 070 ----
mean loss: 228.88
 ---- batch: 080 ----
mean loss: 232.63
 ---- batch: 090 ----
mean loss: 223.18
 ---- batch: 100 ----
mean loss: 223.45
 ---- batch: 110 ----
mean loss: 220.92
train mean loss: 225.04
epoch train time: 0:00:02.519273
elapsed time: 0:04:30.512814
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 21:49:52.375587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.54
 ---- batch: 020 ----
mean loss: 225.09
 ---- batch: 030 ----
mean loss: 229.69
 ---- batch: 040 ----
mean loss: 221.37
 ---- batch: 050 ----
mean loss: 226.99
 ---- batch: 060 ----
mean loss: 226.41
 ---- batch: 070 ----
mean loss: 227.80
 ---- batch: 080 ----
mean loss: 222.30
 ---- batch: 090 ----
mean loss: 226.93
 ---- batch: 100 ----
mean loss: 221.04
 ---- batch: 110 ----
mean loss: 217.65
train mean loss: 224.07
epoch train time: 0:00:02.498326
elapsed time: 0:04:33.011586
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 21:49:54.874363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.65
 ---- batch: 020 ----
mean loss: 224.05
 ---- batch: 030 ----
mean loss: 224.73
 ---- batch: 040 ----
mean loss: 228.75
 ---- batch: 050 ----
mean loss: 232.20
 ---- batch: 060 ----
mean loss: 220.19
 ---- batch: 070 ----
mean loss: 212.86
 ---- batch: 080 ----
mean loss: 217.16
 ---- batch: 090 ----
mean loss: 223.02
 ---- batch: 100 ----
mean loss: 222.24
 ---- batch: 110 ----
mean loss: 222.62
train mean loss: 223.15
epoch train time: 0:00:02.505788
elapsed time: 0:04:35.517778
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 21:49:57.380589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.44
 ---- batch: 020 ----
mean loss: 215.25
 ---- batch: 030 ----
mean loss: 203.31
 ---- batch: 040 ----
mean loss: 226.57
 ---- batch: 050 ----
mean loss: 235.17
 ---- batch: 060 ----
mean loss: 227.72
 ---- batch: 070 ----
mean loss: 222.43
 ---- batch: 080 ----
mean loss: 224.58
 ---- batch: 090 ----
mean loss: 219.82
 ---- batch: 100 ----
mean loss: 221.60
 ---- batch: 110 ----
mean loss: 227.58
train mean loss: 222.30
epoch train time: 0:00:02.488783
elapsed time: 0:04:38.007027
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 21:49:59.869821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.37
 ---- batch: 020 ----
mean loss: 215.08
 ---- batch: 030 ----
mean loss: 221.88
 ---- batch: 040 ----
mean loss: 222.02
 ---- batch: 050 ----
mean loss: 232.39
 ---- batch: 060 ----
mean loss: 216.81
 ---- batch: 070 ----
mean loss: 220.44
 ---- batch: 080 ----
mean loss: 228.73
 ---- batch: 090 ----
mean loss: 222.88
 ---- batch: 100 ----
mean loss: 223.75
 ---- batch: 110 ----
mean loss: 219.57
train mean loss: 221.52
epoch train time: 0:00:02.499148
elapsed time: 0:04:40.506625
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 21:50:02.369292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.87
 ---- batch: 020 ----
mean loss: 221.15
 ---- batch: 030 ----
mean loss: 220.06
 ---- batch: 040 ----
mean loss: 212.17
 ---- batch: 050 ----
mean loss: 218.79
 ---- batch: 060 ----
mean loss: 224.17
 ---- batch: 070 ----
mean loss: 222.31
 ---- batch: 080 ----
mean loss: 231.12
 ---- batch: 090 ----
mean loss: 224.61
 ---- batch: 100 ----
mean loss: 217.71
 ---- batch: 110 ----
mean loss: 216.60
train mean loss: 220.44
epoch train time: 0:00:02.484602
elapsed time: 0:04:42.991549
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 21:50:04.854347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.58
 ---- batch: 020 ----
mean loss: 227.06
 ---- batch: 030 ----
mean loss: 211.56
 ---- batch: 040 ----
mean loss: 227.08
 ---- batch: 050 ----
mean loss: 214.10
 ---- batch: 060 ----
mean loss: 223.49
 ---- batch: 070 ----
mean loss: 211.54
 ---- batch: 080 ----
mean loss: 210.42
 ---- batch: 090 ----
mean loss: 217.30
 ---- batch: 100 ----
mean loss: 222.28
 ---- batch: 110 ----
mean loss: 227.49
train mean loss: 219.86
epoch train time: 0:00:02.487175
elapsed time: 0:04:45.479181
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 21:50:07.341969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.76
 ---- batch: 020 ----
mean loss: 220.15
 ---- batch: 030 ----
mean loss: 228.40
 ---- batch: 040 ----
mean loss: 215.34
 ---- batch: 050 ----
mean loss: 219.41
 ---- batch: 060 ----
mean loss: 219.80
 ---- batch: 070 ----
mean loss: 220.53
 ---- batch: 080 ----
mean loss: 219.31
 ---- batch: 090 ----
mean loss: 212.37
 ---- batch: 100 ----
mean loss: 225.60
 ---- batch: 110 ----
mean loss: 217.65
train mean loss: 219.24
epoch train time: 0:00:02.514902
elapsed time: 0:04:47.994540
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 21:50:09.857317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.05
 ---- batch: 020 ----
mean loss: 220.12
 ---- batch: 030 ----
mean loss: 219.93
 ---- batch: 040 ----
mean loss: 216.32
 ---- batch: 050 ----
mean loss: 220.92
 ---- batch: 060 ----
mean loss: 212.90
 ---- batch: 070 ----
mean loss: 215.98
 ---- batch: 080 ----
mean loss: 221.19
 ---- batch: 090 ----
mean loss: 220.00
 ---- batch: 100 ----
mean loss: 211.11
 ---- batch: 110 ----
mean loss: 219.33
train mean loss: 218.25
epoch train time: 0:00:02.476283
elapsed time: 0:04:50.471247
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 21:50:12.334028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.71
 ---- batch: 020 ----
mean loss: 217.21
 ---- batch: 030 ----
mean loss: 222.83
 ---- batch: 040 ----
mean loss: 217.68
 ---- batch: 050 ----
mean loss: 214.08
 ---- batch: 060 ----
mean loss: 222.15
 ---- batch: 070 ----
mean loss: 214.19
 ---- batch: 080 ----
mean loss: 220.69
 ---- batch: 090 ----
mean loss: 212.09
 ---- batch: 100 ----
mean loss: 213.82
 ---- batch: 110 ----
mean loss: 214.69
train mean loss: 217.50
epoch train time: 0:00:02.491449
elapsed time: 0:04:52.963142
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 21:50:14.825976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.85
 ---- batch: 020 ----
mean loss: 213.06
 ---- batch: 030 ----
mean loss: 217.24
 ---- batch: 040 ----
mean loss: 213.89
 ---- batch: 050 ----
mean loss: 210.37
 ---- batch: 060 ----
mean loss: 220.22
 ---- batch: 070 ----
mean loss: 215.31
 ---- batch: 080 ----
mean loss: 213.66
 ---- batch: 090 ----
mean loss: 211.33
 ---- batch: 100 ----
mean loss: 220.05
 ---- batch: 110 ----
mean loss: 225.90
train mean loss: 216.93
epoch train time: 0:00:02.462912
elapsed time: 0:04:55.426542
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 21:50:17.289322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.18
 ---- batch: 020 ----
mean loss: 217.86
 ---- batch: 030 ----
mean loss: 218.26
 ---- batch: 040 ----
mean loss: 215.08
 ---- batch: 050 ----
mean loss: 216.16
 ---- batch: 060 ----
mean loss: 210.02
 ---- batch: 070 ----
mean loss: 220.86
 ---- batch: 080 ----
mean loss: 210.94
 ---- batch: 090 ----
mean loss: 214.35
 ---- batch: 100 ----
mean loss: 219.83
 ---- batch: 110 ----
mean loss: 226.66
train mean loss: 216.41
epoch train time: 0:00:02.542660
elapsed time: 0:04:57.969641
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 21:50:19.832444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.72
 ---- batch: 020 ----
mean loss: 212.90
 ---- batch: 030 ----
mean loss: 214.37
 ---- batch: 040 ----
mean loss: 209.23
 ---- batch: 050 ----
mean loss: 207.97
 ---- batch: 060 ----
mean loss: 221.40
 ---- batch: 070 ----
mean loss: 214.36
 ---- batch: 080 ----
mean loss: 212.00
 ---- batch: 090 ----
mean loss: 214.99
 ---- batch: 100 ----
mean loss: 224.79
 ---- batch: 110 ----
mean loss: 213.54
train mean loss: 215.43
epoch train time: 0:00:02.480247
elapsed time: 0:05:00.450447
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 21:50:22.313209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.33
 ---- batch: 020 ----
mean loss: 225.64
 ---- batch: 030 ----
mean loss: 206.95
 ---- batch: 040 ----
mean loss: 216.18
 ---- batch: 050 ----
mean loss: 217.82
 ---- batch: 060 ----
mean loss: 211.95
 ---- batch: 070 ----
mean loss: 208.97
 ---- batch: 080 ----
mean loss: 220.71
 ---- batch: 090 ----
mean loss: 222.43
 ---- batch: 100 ----
mean loss: 207.08
 ---- batch: 110 ----
mean loss: 217.20
train mean loss: 214.83
epoch train time: 0:00:02.502390
elapsed time: 0:05:02.953268
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 21:50:24.816042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.24
 ---- batch: 020 ----
mean loss: 217.10
 ---- batch: 030 ----
mean loss: 219.49
 ---- batch: 040 ----
mean loss: 209.60
 ---- batch: 050 ----
mean loss: 223.44
 ---- batch: 060 ----
mean loss: 209.72
 ---- batch: 070 ----
mean loss: 221.34
 ---- batch: 080 ----
mean loss: 213.94
 ---- batch: 090 ----
mean loss: 216.99
 ---- batch: 100 ----
mean loss: 203.31
 ---- batch: 110 ----
mean loss: 209.23
train mean loss: 214.28
epoch train time: 0:00:02.485109
elapsed time: 0:05:05.438801
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 21:50:27.301597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.93
 ---- batch: 020 ----
mean loss: 220.32
 ---- batch: 030 ----
mean loss: 204.18
 ---- batch: 040 ----
mean loss: 208.44
 ---- batch: 050 ----
mean loss: 216.07
 ---- batch: 060 ----
mean loss: 210.72
 ---- batch: 070 ----
mean loss: 213.33
 ---- batch: 080 ----
mean loss: 215.56
 ---- batch: 090 ----
mean loss: 220.10
 ---- batch: 100 ----
mean loss: 219.75
 ---- batch: 110 ----
mean loss: 212.85
train mean loss: 213.77
epoch train time: 0:00:02.493057
elapsed time: 0:05:07.932336
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 21:50:29.795113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.10
 ---- batch: 020 ----
mean loss: 207.53
 ---- batch: 030 ----
mean loss: 218.16
 ---- batch: 040 ----
mean loss: 205.37
 ---- batch: 050 ----
mean loss: 217.57
 ---- batch: 060 ----
mean loss: 209.67
 ---- batch: 070 ----
mean loss: 219.43
 ---- batch: 080 ----
mean loss: 212.09
 ---- batch: 090 ----
mean loss: 208.60
 ---- batch: 100 ----
mean loss: 216.12
 ---- batch: 110 ----
mean loss: 219.69
train mean loss: 213.19
epoch train time: 0:00:02.495771
elapsed time: 0:05:10.428523
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 21:50:32.291273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.02
 ---- batch: 020 ----
mean loss: 212.65
 ---- batch: 030 ----
mean loss: 210.03
 ---- batch: 040 ----
mean loss: 203.54
 ---- batch: 050 ----
mean loss: 219.30
 ---- batch: 060 ----
mean loss: 212.92
 ---- batch: 070 ----
mean loss: 212.97
 ---- batch: 080 ----
mean loss: 220.78
 ---- batch: 090 ----
mean loss: 209.36
 ---- batch: 100 ----
mean loss: 206.17
 ---- batch: 110 ----
mean loss: 219.26
train mean loss: 212.75
epoch train time: 0:00:02.519797
elapsed time: 0:05:12.948714
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 21:50:34.811481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.33
 ---- batch: 020 ----
mean loss: 210.79
 ---- batch: 030 ----
mean loss: 213.07
 ---- batch: 040 ----
mean loss: 213.47
 ---- batch: 050 ----
mean loss: 203.33
 ---- batch: 060 ----
mean loss: 215.06
 ---- batch: 070 ----
mean loss: 213.16
 ---- batch: 080 ----
mean loss: 216.22
 ---- batch: 090 ----
mean loss: 220.76
 ---- batch: 100 ----
mean loss: 203.97
 ---- batch: 110 ----
mean loss: 212.17
train mean loss: 212.22
epoch train time: 0:00:02.514552
elapsed time: 0:05:15.463699
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 21:50:37.326510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.13
 ---- batch: 020 ----
mean loss: 200.29
 ---- batch: 030 ----
mean loss: 212.82
 ---- batch: 040 ----
mean loss: 199.64
 ---- batch: 050 ----
mean loss: 212.18
 ---- batch: 060 ----
mean loss: 216.94
 ---- batch: 070 ----
mean loss: 210.93
 ---- batch: 080 ----
mean loss: 214.91
 ---- batch: 090 ----
mean loss: 213.31
 ---- batch: 100 ----
mean loss: 214.91
 ---- batch: 110 ----
mean loss: 216.21
train mean loss: 211.61
epoch train time: 0:00:02.487646
elapsed time: 0:05:17.951824
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 21:50:39.814625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.64
 ---- batch: 020 ----
mean loss: 220.99
 ---- batch: 030 ----
mean loss: 207.05
 ---- batch: 040 ----
mean loss: 212.08
 ---- batch: 050 ----
mean loss: 217.53
 ---- batch: 060 ----
mean loss: 226.81
 ---- batch: 070 ----
mean loss: 213.21
 ---- batch: 080 ----
mean loss: 201.78
 ---- batch: 090 ----
mean loss: 209.13
 ---- batch: 100 ----
mean loss: 209.04
 ---- batch: 110 ----
mean loss: 206.00
train mean loss: 211.69
epoch train time: 0:00:02.496359
elapsed time: 0:05:20.448622
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 21:50:42.311394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.28
 ---- batch: 020 ----
mean loss: 219.34
 ---- batch: 030 ----
mean loss: 212.32
 ---- batch: 040 ----
mean loss: 203.10
 ---- batch: 050 ----
mean loss: 212.05
 ---- batch: 060 ----
mean loss: 208.99
 ---- batch: 070 ----
mean loss: 206.11
 ---- batch: 080 ----
mean loss: 216.04
 ---- batch: 090 ----
mean loss: 214.56
 ---- batch: 100 ----
mean loss: 203.02
 ---- batch: 110 ----
mean loss: 203.72
train mean loss: 210.66
epoch train time: 0:00:02.518415
elapsed time: 0:05:22.967480
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 21:50:44.830256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.58
 ---- batch: 020 ----
mean loss: 210.51
 ---- batch: 030 ----
mean loss: 205.55
 ---- batch: 040 ----
mean loss: 207.70
 ---- batch: 050 ----
mean loss: 208.67
 ---- batch: 060 ----
mean loss: 209.77
 ---- batch: 070 ----
mean loss: 216.75
 ---- batch: 080 ----
mean loss: 213.83
 ---- batch: 090 ----
mean loss: 209.24
 ---- batch: 100 ----
mean loss: 215.88
 ---- batch: 110 ----
mean loss: 207.78
train mean loss: 210.17
epoch train time: 0:00:02.516591
elapsed time: 0:05:25.484484
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 21:50:47.347259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.52
 ---- batch: 020 ----
mean loss: 212.14
 ---- batch: 030 ----
mean loss: 208.67
 ---- batch: 040 ----
mean loss: 206.19
 ---- batch: 050 ----
mean loss: 205.20
 ---- batch: 060 ----
mean loss: 209.69
 ---- batch: 070 ----
mean loss: 207.40
 ---- batch: 080 ----
mean loss: 221.74
 ---- batch: 090 ----
mean loss: 207.84
 ---- batch: 100 ----
mean loss: 211.00
 ---- batch: 110 ----
mean loss: 204.11
train mean loss: 209.78
epoch train time: 0:00:02.486239
elapsed time: 0:05:27.971200
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 21:50:49.833975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.87
 ---- batch: 020 ----
mean loss: 208.44
 ---- batch: 030 ----
mean loss: 213.50
 ---- batch: 040 ----
mean loss: 205.39
 ---- batch: 050 ----
mean loss: 207.22
 ---- batch: 060 ----
mean loss: 206.48
 ---- batch: 070 ----
mean loss: 207.74
 ---- batch: 080 ----
mean loss: 213.71
 ---- batch: 090 ----
mean loss: 214.64
 ---- batch: 100 ----
mean loss: 212.00
 ---- batch: 110 ----
mean loss: 208.03
train mean loss: 209.27
epoch train time: 0:00:02.489703
elapsed time: 0:05:30.461340
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 21:50:52.324126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.44
 ---- batch: 020 ----
mean loss: 201.23
 ---- batch: 030 ----
mean loss: 205.58
 ---- batch: 040 ----
mean loss: 213.57
 ---- batch: 050 ----
mean loss: 212.62
 ---- batch: 060 ----
mean loss: 218.34
 ---- batch: 070 ----
mean loss: 207.05
 ---- batch: 080 ----
mean loss: 207.92
 ---- batch: 090 ----
mean loss: 218.27
 ---- batch: 100 ----
mean loss: 203.86
 ---- batch: 110 ----
mean loss: 208.98
train mean loss: 209.09
epoch train time: 0:00:02.504160
elapsed time: 0:05:32.965950
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 21:50:54.828724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.49
 ---- batch: 020 ----
mean loss: 196.63
 ---- batch: 030 ----
mean loss: 212.85
 ---- batch: 040 ----
mean loss: 210.43
 ---- batch: 050 ----
mean loss: 205.35
 ---- batch: 060 ----
mean loss: 207.04
 ---- batch: 070 ----
mean loss: 212.08
 ---- batch: 080 ----
mean loss: 197.83
 ---- batch: 090 ----
mean loss: 218.72
 ---- batch: 100 ----
mean loss: 206.98
 ---- batch: 110 ----
mean loss: 210.57
train mean loss: 208.51
epoch train time: 0:00:02.485912
elapsed time: 0:05:35.452265
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 21:50:57.315125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.33
 ---- batch: 020 ----
mean loss: 205.39
 ---- batch: 030 ----
mean loss: 207.00
 ---- batch: 040 ----
mean loss: 211.43
 ---- batch: 050 ----
mean loss: 199.74
 ---- batch: 060 ----
mean loss: 207.51
 ---- batch: 070 ----
mean loss: 206.40
 ---- batch: 080 ----
mean loss: 210.33
 ---- batch: 090 ----
mean loss: 206.55
 ---- batch: 100 ----
mean loss: 214.63
 ---- batch: 110 ----
mean loss: 210.60
train mean loss: 208.02
epoch train time: 0:00:02.490126
elapsed time: 0:05:37.942888
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 21:50:59.805685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.09
 ---- batch: 020 ----
mean loss: 213.67
 ---- batch: 030 ----
mean loss: 202.13
 ---- batch: 040 ----
mean loss: 211.46
 ---- batch: 050 ----
mean loss: 213.69
 ---- batch: 060 ----
mean loss: 208.80
 ---- batch: 070 ----
mean loss: 211.22
 ---- batch: 080 ----
mean loss: 211.17
 ---- batch: 090 ----
mean loss: 192.64
 ---- batch: 100 ----
mean loss: 212.42
 ---- batch: 110 ----
mean loss: 197.27
train mean loss: 207.54
epoch train time: 0:00:02.482435
elapsed time: 0:05:40.425777
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 21:51:02.288558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.07
 ---- batch: 020 ----
mean loss: 200.40
 ---- batch: 030 ----
mean loss: 204.83
 ---- batch: 040 ----
mean loss: 208.69
 ---- batch: 050 ----
mean loss: 205.58
 ---- batch: 060 ----
mean loss: 209.62
 ---- batch: 070 ----
mean loss: 206.69
 ---- batch: 080 ----
mean loss: 211.75
 ---- batch: 090 ----
mean loss: 212.76
 ---- batch: 100 ----
mean loss: 205.62
 ---- batch: 110 ----
mean loss: 200.85
train mean loss: 206.98
epoch train time: 0:00:02.504319
elapsed time: 0:05:42.930533
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 21:51:04.793419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.08
 ---- batch: 020 ----
mean loss: 203.01
 ---- batch: 030 ----
mean loss: 207.58
 ---- batch: 040 ----
mean loss: 213.07
 ---- batch: 050 ----
mean loss: 215.98
 ---- batch: 060 ----
mean loss: 210.95
 ---- batch: 070 ----
mean loss: 210.50
 ---- batch: 080 ----
mean loss: 201.81
 ---- batch: 090 ----
mean loss: 205.54
 ---- batch: 100 ----
mean loss: 198.40
 ---- batch: 110 ----
mean loss: 201.33
train mean loss: 206.65
epoch train time: 0:00:02.487347
elapsed time: 0:05:45.418405
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 21:51:07.281173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.20
 ---- batch: 020 ----
mean loss: 201.93
 ---- batch: 030 ----
mean loss: 208.20
 ---- batch: 040 ----
mean loss: 213.90
 ---- batch: 050 ----
mean loss: 199.24
 ---- batch: 060 ----
mean loss: 207.60
 ---- batch: 070 ----
mean loss: 213.18
 ---- batch: 080 ----
mean loss: 213.08
 ---- batch: 090 ----
mean loss: 199.30
 ---- batch: 100 ----
mean loss: 198.83
 ---- batch: 110 ----
mean loss: 207.88
train mean loss: 206.19
epoch train time: 0:00:02.506789
elapsed time: 0:05:47.925614
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 21:51:09.788391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.76
 ---- batch: 020 ----
mean loss: 200.99
 ---- batch: 030 ----
mean loss: 201.40
 ---- batch: 040 ----
mean loss: 210.41
 ---- batch: 050 ----
mean loss: 207.73
 ---- batch: 060 ----
mean loss: 201.48
 ---- batch: 070 ----
mean loss: 206.43
 ---- batch: 080 ----
mean loss: 210.98
 ---- batch: 090 ----
mean loss: 206.61
 ---- batch: 100 ----
mean loss: 206.33
 ---- batch: 110 ----
mean loss: 203.45
train mean loss: 205.94
epoch train time: 0:00:02.502754
elapsed time: 0:05:50.428886
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 21:51:12.291571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.51
 ---- batch: 020 ----
mean loss: 205.01
 ---- batch: 030 ----
mean loss: 204.94
 ---- batch: 040 ----
mean loss: 190.80
 ---- batch: 050 ----
mean loss: 217.68
 ---- batch: 060 ----
mean loss: 206.68
 ---- batch: 070 ----
mean loss: 203.26
 ---- batch: 080 ----
mean loss: 205.38
 ---- batch: 090 ----
mean loss: 206.30
 ---- batch: 100 ----
mean loss: 204.20
 ---- batch: 110 ----
mean loss: 207.00
train mean loss: 205.44
epoch train time: 0:00:02.506125
elapsed time: 0:05:52.935358
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 21:51:14.798195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.05
 ---- batch: 020 ----
mean loss: 198.75
 ---- batch: 030 ----
mean loss: 205.71
 ---- batch: 040 ----
mean loss: 202.48
 ---- batch: 050 ----
mean loss: 211.97
 ---- batch: 060 ----
mean loss: 205.01
 ---- batch: 070 ----
mean loss: 206.06
 ---- batch: 080 ----
mean loss: 206.41
 ---- batch: 090 ----
mean loss: 197.56
 ---- batch: 100 ----
mean loss: 212.85
 ---- batch: 110 ----
mean loss: 196.41
train mean loss: 205.04
epoch train time: 0:00:02.478529
elapsed time: 0:05:55.414370
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 21:51:17.277131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.36
 ---- batch: 020 ----
mean loss: 215.67
 ---- batch: 030 ----
mean loss: 202.73
 ---- batch: 040 ----
mean loss: 204.77
 ---- batch: 050 ----
mean loss: 207.36
 ---- batch: 060 ----
mean loss: 207.82
 ---- batch: 070 ----
mean loss: 195.68
 ---- batch: 080 ----
mean loss: 205.64
 ---- batch: 090 ----
mean loss: 203.59
 ---- batch: 100 ----
mean loss: 205.44
 ---- batch: 110 ----
mean loss: 203.76
train mean loss: 204.59
epoch train time: 0:00:02.508654
elapsed time: 0:05:57.923432
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 21:51:19.786197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.85
 ---- batch: 020 ----
mean loss: 202.46
 ---- batch: 030 ----
mean loss: 202.77
 ---- batch: 040 ----
mean loss: 214.70
 ---- batch: 050 ----
mean loss: 198.25
 ---- batch: 060 ----
mean loss: 201.38
 ---- batch: 070 ----
mean loss: 210.45
 ---- batch: 080 ----
mean loss: 206.40
 ---- batch: 090 ----
mean loss: 206.68
 ---- batch: 100 ----
mean loss: 201.38
 ---- batch: 110 ----
mean loss: 199.56
train mean loss: 204.11
epoch train time: 0:00:02.485346
elapsed time: 0:06:00.409171
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 21:51:22.271933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.71
 ---- batch: 020 ----
mean loss: 204.26
 ---- batch: 030 ----
mean loss: 208.44
 ---- batch: 040 ----
mean loss: 203.83
 ---- batch: 050 ----
mean loss: 205.82
 ---- batch: 060 ----
mean loss: 201.46
 ---- batch: 070 ----
mean loss: 206.45
 ---- batch: 080 ----
mean loss: 203.04
 ---- batch: 090 ----
mean loss: 198.27
 ---- batch: 100 ----
mean loss: 207.53
 ---- batch: 110 ----
mean loss: 201.31
train mean loss: 204.12
epoch train time: 0:00:02.528711
elapsed time: 0:06:02.938292
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 21:51:24.801065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.03
 ---- batch: 020 ----
mean loss: 209.58
 ---- batch: 030 ----
mean loss: 207.36
 ---- batch: 040 ----
mean loss: 204.56
 ---- batch: 050 ----
mean loss: 202.66
 ---- batch: 060 ----
mean loss: 201.75
 ---- batch: 070 ----
mean loss: 209.56
 ---- batch: 080 ----
mean loss: 201.02
 ---- batch: 090 ----
mean loss: 195.11
 ---- batch: 100 ----
mean loss: 202.02
 ---- batch: 110 ----
mean loss: 199.44
train mean loss: 203.43
epoch train time: 0:00:02.503671
elapsed time: 0:06:05.442372
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 21:51:27.305195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.29
 ---- batch: 020 ----
mean loss: 202.77
 ---- batch: 030 ----
mean loss: 204.75
 ---- batch: 040 ----
mean loss: 196.99
 ---- batch: 050 ----
mean loss: 204.45
 ---- batch: 060 ----
mean loss: 194.68
 ---- batch: 070 ----
mean loss: 200.99
 ---- batch: 080 ----
mean loss: 200.44
 ---- batch: 090 ----
mean loss: 209.76
 ---- batch: 100 ----
mean loss: 192.14
 ---- batch: 110 ----
mean loss: 208.58
train mean loss: 203.13
epoch train time: 0:00:02.508948
elapsed time: 0:06:07.951809
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 21:51:29.814640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.79
 ---- batch: 020 ----
mean loss: 205.61
 ---- batch: 030 ----
mean loss: 209.92
 ---- batch: 040 ----
mean loss: 203.94
 ---- batch: 050 ----
mean loss: 201.90
 ---- batch: 060 ----
mean loss: 208.74
 ---- batch: 070 ----
mean loss: 201.65
 ---- batch: 080 ----
mean loss: 198.17
 ---- batch: 090 ----
mean loss: 198.88
 ---- batch: 100 ----
mean loss: 201.85
 ---- batch: 110 ----
mean loss: 198.04
train mean loss: 202.76
epoch train time: 0:00:02.529081
elapsed time: 0:06:10.481362
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 21:51:32.344118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.55
 ---- batch: 020 ----
mean loss: 207.55
 ---- batch: 030 ----
mean loss: 208.23
 ---- batch: 040 ----
mean loss: 203.40
 ---- batch: 050 ----
mean loss: 198.69
 ---- batch: 060 ----
mean loss: 199.95
 ---- batch: 070 ----
mean loss: 201.21
 ---- batch: 080 ----
mean loss: 199.45
 ---- batch: 090 ----
mean loss: 206.70
 ---- batch: 100 ----
mean loss: 199.07
 ---- batch: 110 ----
mean loss: 203.33
train mean loss: 202.56
epoch train time: 0:00:02.494341
elapsed time: 0:06:12.976126
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 21:51:34.838920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.85
 ---- batch: 020 ----
mean loss: 205.93
 ---- batch: 030 ----
mean loss: 203.63
 ---- batch: 040 ----
mean loss: 202.46
 ---- batch: 050 ----
mean loss: 193.88
 ---- batch: 060 ----
mean loss: 197.94
 ---- batch: 070 ----
mean loss: 207.28
 ---- batch: 080 ----
mean loss: 199.43
 ---- batch: 090 ----
mean loss: 205.06
 ---- batch: 100 ----
mean loss: 194.78
 ---- batch: 110 ----
mean loss: 196.42
train mean loss: 202.04
epoch train time: 0:00:02.493351
elapsed time: 0:06:15.469912
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 21:51:37.332695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.36
 ---- batch: 020 ----
mean loss: 204.21
 ---- batch: 030 ----
mean loss: 204.59
 ---- batch: 040 ----
mean loss: 202.89
 ---- batch: 050 ----
mean loss: 207.01
 ---- batch: 060 ----
mean loss: 206.23
 ---- batch: 070 ----
mean loss: 194.69
 ---- batch: 080 ----
mean loss: 202.59
 ---- batch: 090 ----
mean loss: 191.04
 ---- batch: 100 ----
mean loss: 209.72
 ---- batch: 110 ----
mean loss: 204.72
train mean loss: 201.55
epoch train time: 0:00:02.501592
elapsed time: 0:06:17.972001
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 21:51:39.834788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.11
 ---- batch: 020 ----
mean loss: 207.52
 ---- batch: 030 ----
mean loss: 196.55
 ---- batch: 040 ----
mean loss: 206.62
 ---- batch: 050 ----
mean loss: 191.69
 ---- batch: 060 ----
mean loss: 208.11
 ---- batch: 070 ----
mean loss: 191.48
 ---- batch: 080 ----
mean loss: 202.72
 ---- batch: 090 ----
mean loss: 195.25
 ---- batch: 100 ----
mean loss: 206.34
 ---- batch: 110 ----
mean loss: 198.07
train mean loss: 201.32
epoch train time: 0:00:02.508365
elapsed time: 0:06:20.480784
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 21:51:42.343563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.09
 ---- batch: 020 ----
mean loss: 212.11
 ---- batch: 030 ----
mean loss: 202.65
 ---- batch: 040 ----
mean loss: 208.13
 ---- batch: 050 ----
mean loss: 201.09
 ---- batch: 060 ----
mean loss: 202.61
 ---- batch: 070 ----
mean loss: 197.84
 ---- batch: 080 ----
mean loss: 193.96
 ---- batch: 090 ----
mean loss: 202.76
 ---- batch: 100 ----
mean loss: 189.51
 ---- batch: 110 ----
mean loss: 203.75
train mean loss: 201.14
epoch train time: 0:00:02.502717
elapsed time: 0:06:22.983919
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 21:51:44.846718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.05
 ---- batch: 020 ----
mean loss: 199.18
 ---- batch: 030 ----
mean loss: 209.16
 ---- batch: 040 ----
mean loss: 207.36
 ---- batch: 050 ----
mean loss: 205.29
 ---- batch: 060 ----
mean loss: 192.10
 ---- batch: 070 ----
mean loss: 205.51
 ---- batch: 080 ----
mean loss: 201.66
 ---- batch: 090 ----
mean loss: 209.33
 ---- batch: 100 ----
mean loss: 198.23
 ---- batch: 110 ----
mean loss: 192.36
train mean loss: 200.88
epoch train time: 0:00:02.470802
elapsed time: 0:06:25.455179
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 21:51:47.317943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.15
 ---- batch: 020 ----
mean loss: 210.11
 ---- batch: 030 ----
mean loss: 199.86
 ---- batch: 040 ----
mean loss: 195.47
 ---- batch: 050 ----
mean loss: 195.48
 ---- batch: 060 ----
mean loss: 202.09
 ---- batch: 070 ----
mean loss: 199.30
 ---- batch: 080 ----
mean loss: 204.13
 ---- batch: 090 ----
mean loss: 200.22
 ---- batch: 100 ----
mean loss: 198.12
 ---- batch: 110 ----
mean loss: 195.03
train mean loss: 200.37
epoch train time: 0:00:02.479914
elapsed time: 0:06:27.935513
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 21:51:49.798277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.15
 ---- batch: 020 ----
mean loss: 211.22
 ---- batch: 030 ----
mean loss: 197.03
 ---- batch: 040 ----
mean loss: 199.83
 ---- batch: 050 ----
mean loss: 195.46
 ---- batch: 060 ----
mean loss: 197.01
 ---- batch: 070 ----
mean loss: 198.24
 ---- batch: 080 ----
mean loss: 196.84
 ---- batch: 090 ----
mean loss: 191.76
 ---- batch: 100 ----
mean loss: 205.40
 ---- batch: 110 ----
mean loss: 208.70
train mean loss: 200.21
epoch train time: 0:00:02.528235
elapsed time: 0:06:30.464167
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 21:51:52.326947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.67
 ---- batch: 020 ----
mean loss: 192.01
 ---- batch: 030 ----
mean loss: 191.43
 ---- batch: 040 ----
mean loss: 199.62
 ---- batch: 050 ----
mean loss: 206.16
 ---- batch: 060 ----
mean loss: 192.88
 ---- batch: 070 ----
mean loss: 201.91
 ---- batch: 080 ----
mean loss: 209.63
 ---- batch: 090 ----
mean loss: 205.58
 ---- batch: 100 ----
mean loss: 204.53
 ---- batch: 110 ----
mean loss: 192.84
train mean loss: 199.76
epoch train time: 0:00:02.504673
elapsed time: 0:06:32.969265
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 21:51:54.832035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.20
 ---- batch: 020 ----
mean loss: 199.73
 ---- batch: 030 ----
mean loss: 201.91
 ---- batch: 040 ----
mean loss: 206.53
 ---- batch: 050 ----
mean loss: 195.58
 ---- batch: 060 ----
mean loss: 201.03
 ---- batch: 070 ----
mean loss: 193.73
 ---- batch: 080 ----
mean loss: 206.80
 ---- batch: 090 ----
mean loss: 195.22
 ---- batch: 100 ----
mean loss: 201.95
 ---- batch: 110 ----
mean loss: 196.34
train mean loss: 199.42
epoch train time: 0:00:02.516582
elapsed time: 0:06:35.486247
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 21:51:57.349022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.14
 ---- batch: 020 ----
mean loss: 204.41
 ---- batch: 030 ----
mean loss: 208.38
 ---- batch: 040 ----
mean loss: 193.99
 ---- batch: 050 ----
mean loss: 206.22
 ---- batch: 060 ----
mean loss: 205.09
 ---- batch: 070 ----
mean loss: 197.65
 ---- batch: 080 ----
mean loss: 198.81
 ---- batch: 090 ----
mean loss: 194.75
 ---- batch: 100 ----
mean loss: 196.03
 ---- batch: 110 ----
mean loss: 197.18
train mean loss: 198.89
epoch train time: 0:00:02.530314
elapsed time: 0:06:38.016988
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 21:51:59.879786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.60
 ---- batch: 020 ----
mean loss: 204.68
 ---- batch: 030 ----
mean loss: 192.97
 ---- batch: 040 ----
mean loss: 201.19
 ---- batch: 050 ----
mean loss: 194.15
 ---- batch: 060 ----
mean loss: 197.64
 ---- batch: 070 ----
mean loss: 201.93
 ---- batch: 080 ----
mean loss: 195.48
 ---- batch: 090 ----
mean loss: 200.80
 ---- batch: 100 ----
mean loss: 192.80
 ---- batch: 110 ----
mean loss: 199.25
train mean loss: 198.77
epoch train time: 0:00:02.515420
elapsed time: 0:06:40.532839
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 21:52:02.395643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.40
 ---- batch: 020 ----
mean loss: 200.79
 ---- batch: 030 ----
mean loss: 198.36
 ---- batch: 040 ----
mean loss: 196.97
 ---- batch: 050 ----
mean loss: 204.84
 ---- batch: 060 ----
mean loss: 195.44
 ---- batch: 070 ----
mean loss: 200.97
 ---- batch: 080 ----
mean loss: 196.62
 ---- batch: 090 ----
mean loss: 197.68
 ---- batch: 100 ----
mean loss: 206.68
 ---- batch: 110 ----
mean loss: 188.51
train mean loss: 198.32
epoch train time: 0:00:02.515038
elapsed time: 0:06:43.048330
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 21:52:04.911098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.33
 ---- batch: 020 ----
mean loss: 201.08
 ---- batch: 030 ----
mean loss: 203.24
 ---- batch: 040 ----
mean loss: 193.53
 ---- batch: 050 ----
mean loss: 200.44
 ---- batch: 060 ----
mean loss: 189.93
 ---- batch: 070 ----
mean loss: 193.79
 ---- batch: 080 ----
mean loss: 209.29
 ---- batch: 090 ----
mean loss: 203.92
 ---- batch: 100 ----
mean loss: 201.80
 ---- batch: 110 ----
mean loss: 189.88
train mean loss: 198.05
epoch train time: 0:00:02.483779
elapsed time: 0:06:45.532693
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 21:52:07.395370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.97
 ---- batch: 020 ----
mean loss: 193.50
 ---- batch: 030 ----
mean loss: 191.28
 ---- batch: 040 ----
mean loss: 183.87
 ---- batch: 050 ----
mean loss: 196.01
 ---- batch: 060 ----
mean loss: 199.84
 ---- batch: 070 ----
mean loss: 201.69
 ---- batch: 080 ----
mean loss: 201.75
 ---- batch: 090 ----
mean loss: 199.70
 ---- batch: 100 ----
mean loss: 203.87
 ---- batch: 110 ----
mean loss: 197.22
train mean loss: 197.66
epoch train time: 0:00:02.523544
elapsed time: 0:06:48.056562
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 21:52:09.919330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.27
 ---- batch: 020 ----
mean loss: 192.82
 ---- batch: 030 ----
mean loss: 202.73
 ---- batch: 040 ----
mean loss: 190.73
 ---- batch: 050 ----
mean loss: 198.47
 ---- batch: 060 ----
mean loss: 205.00
 ---- batch: 070 ----
mean loss: 192.55
 ---- batch: 080 ----
mean loss: 203.41
 ---- batch: 090 ----
mean loss: 198.74
 ---- batch: 100 ----
mean loss: 192.25
 ---- batch: 110 ----
mean loss: 195.44
train mean loss: 197.49
epoch train time: 0:00:02.512304
elapsed time: 0:06:50.569272
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 21:52:12.432057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.39
 ---- batch: 020 ----
mean loss: 191.61
 ---- batch: 030 ----
mean loss: 202.46
 ---- batch: 040 ----
mean loss: 197.24
 ---- batch: 050 ----
mean loss: 194.12
 ---- batch: 060 ----
mean loss: 201.70
 ---- batch: 070 ----
mean loss: 198.49
 ---- batch: 080 ----
mean loss: 197.77
 ---- batch: 090 ----
mean loss: 194.32
 ---- batch: 100 ----
mean loss: 201.46
 ---- batch: 110 ----
mean loss: 189.34
train mean loss: 197.13
epoch train time: 0:00:02.516214
elapsed time: 0:06:53.085916
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 21:52:14.948716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.89
 ---- batch: 020 ----
mean loss: 194.63
 ---- batch: 030 ----
mean loss: 194.38
 ---- batch: 040 ----
mean loss: 190.13
 ---- batch: 050 ----
mean loss: 200.65
 ---- batch: 060 ----
mean loss: 193.67
 ---- batch: 070 ----
mean loss: 189.33
 ---- batch: 080 ----
mean loss: 207.21
 ---- batch: 090 ----
mean loss: 203.72
 ---- batch: 100 ----
mean loss: 194.13
 ---- batch: 110 ----
mean loss: 189.81
train mean loss: 196.73
epoch train time: 0:00:02.496720
elapsed time: 0:06:55.583099
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 21:52:17.445911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.65
 ---- batch: 020 ----
mean loss: 186.94
 ---- batch: 030 ----
mean loss: 197.07
 ---- batch: 040 ----
mean loss: 193.41
 ---- batch: 050 ----
mean loss: 202.33
 ---- batch: 060 ----
mean loss: 201.28
 ---- batch: 070 ----
mean loss: 202.18
 ---- batch: 080 ----
mean loss: 194.52
 ---- batch: 090 ----
mean loss: 192.45
 ---- batch: 100 ----
mean loss: 194.28
 ---- batch: 110 ----
mean loss: 198.74
train mean loss: 196.48
epoch train time: 0:00:02.493361
elapsed time: 0:06:58.076915
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 21:52:19.939674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.35
 ---- batch: 020 ----
mean loss: 192.31
 ---- batch: 030 ----
mean loss: 197.90
 ---- batch: 040 ----
mean loss: 193.93
 ---- batch: 050 ----
mean loss: 197.60
 ---- batch: 060 ----
mean loss: 194.91
 ---- batch: 070 ----
mean loss: 196.99
 ---- batch: 080 ----
mean loss: 193.88
 ---- batch: 090 ----
mean loss: 198.93
 ---- batch: 100 ----
mean loss: 190.71
 ---- batch: 110 ----
mean loss: 202.40
train mean loss: 195.94
epoch train time: 0:00:02.481681
elapsed time: 0:07:00.559008
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 21:52:22.421804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.68
 ---- batch: 020 ----
mean loss: 191.65
 ---- batch: 030 ----
mean loss: 192.93
 ---- batch: 040 ----
mean loss: 198.31
 ---- batch: 050 ----
mean loss: 189.70
 ---- batch: 060 ----
mean loss: 192.18
 ---- batch: 070 ----
mean loss: 201.58
 ---- batch: 080 ----
mean loss: 195.14
 ---- batch: 090 ----
mean loss: 200.60
 ---- batch: 100 ----
mean loss: 195.50
 ---- batch: 110 ----
mean loss: 200.81
train mean loss: 195.95
epoch train time: 0:00:02.514274
elapsed time: 0:07:03.073716
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 21:52:24.936491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.58
 ---- batch: 020 ----
mean loss: 192.34
 ---- batch: 030 ----
mean loss: 202.50
 ---- batch: 040 ----
mean loss: 207.00
 ---- batch: 050 ----
mean loss: 191.03
 ---- batch: 060 ----
mean loss: 196.33
 ---- batch: 070 ----
mean loss: 194.80
 ---- batch: 080 ----
mean loss: 190.86
 ---- batch: 090 ----
mean loss: 189.85
 ---- batch: 100 ----
mean loss: 198.01
 ---- batch: 110 ----
mean loss: 192.87
train mean loss: 195.70
epoch train time: 0:00:02.519396
elapsed time: 0:07:05.593530
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 21:52:27.456306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.80
 ---- batch: 020 ----
mean loss: 194.18
 ---- batch: 030 ----
mean loss: 200.20
 ---- batch: 040 ----
mean loss: 193.23
 ---- batch: 050 ----
mean loss: 185.42
 ---- batch: 060 ----
mean loss: 201.72
 ---- batch: 070 ----
mean loss: 189.00
 ---- batch: 080 ----
mean loss: 193.61
 ---- batch: 090 ----
mean loss: 197.58
 ---- batch: 100 ----
mean loss: 200.24
 ---- batch: 110 ----
mean loss: 201.14
train mean loss: 195.41
epoch train time: 0:00:02.492874
elapsed time: 0:07:08.086836
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 21:52:29.949642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.73
 ---- batch: 020 ----
mean loss: 193.74
 ---- batch: 030 ----
mean loss: 197.06
 ---- batch: 040 ----
mean loss: 188.58
 ---- batch: 050 ----
mean loss: 191.29
 ---- batch: 060 ----
mean loss: 192.77
 ---- batch: 070 ----
mean loss: 195.00
 ---- batch: 080 ----
mean loss: 190.55
 ---- batch: 090 ----
mean loss: 188.35
 ---- batch: 100 ----
mean loss: 199.45
 ---- batch: 110 ----
mean loss: 204.92
train mean loss: 195.00
epoch train time: 0:00:02.500592
elapsed time: 0:07:10.587883
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 21:52:32.450736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.19
 ---- batch: 020 ----
mean loss: 193.70
 ---- batch: 030 ----
mean loss: 189.85
 ---- batch: 040 ----
mean loss: 199.16
 ---- batch: 050 ----
mean loss: 197.14
 ---- batch: 060 ----
mean loss: 188.39
 ---- batch: 070 ----
mean loss: 199.65
 ---- batch: 080 ----
mean loss: 193.79
 ---- batch: 090 ----
mean loss: 190.67
 ---- batch: 100 ----
mean loss: 205.62
 ---- batch: 110 ----
mean loss: 191.91
train mean loss: 195.06
epoch train time: 0:00:02.508377
elapsed time: 0:07:13.096778
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 21:52:34.959600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.61
 ---- batch: 020 ----
mean loss: 193.19
 ---- batch: 030 ----
mean loss: 190.19
 ---- batch: 040 ----
mean loss: 193.80
 ---- batch: 050 ----
mean loss: 212.03
 ---- batch: 060 ----
mean loss: 189.91
 ---- batch: 070 ----
mean loss: 188.99
 ---- batch: 080 ----
mean loss: 202.94
 ---- batch: 090 ----
mean loss: 200.82
 ---- batch: 100 ----
mean loss: 183.93
 ---- batch: 110 ----
mean loss: 189.58
train mean loss: 194.42
epoch train time: 0:00:02.488842
elapsed time: 0:07:15.586088
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 21:52:37.448852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.20
 ---- batch: 020 ----
mean loss: 196.98
 ---- batch: 030 ----
mean loss: 197.52
 ---- batch: 040 ----
mean loss: 195.85
 ---- batch: 050 ----
mean loss: 204.03
 ---- batch: 060 ----
mean loss: 195.95
 ---- batch: 070 ----
mean loss: 190.93
 ---- batch: 080 ----
mean loss: 192.57
 ---- batch: 090 ----
mean loss: 192.21
 ---- batch: 100 ----
mean loss: 197.91
 ---- batch: 110 ----
mean loss: 192.58
train mean loss: 194.34
epoch train time: 0:00:02.494970
elapsed time: 0:07:18.081481
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 21:52:39.944244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.79
 ---- batch: 020 ----
mean loss: 198.35
 ---- batch: 030 ----
mean loss: 198.52
 ---- batch: 040 ----
mean loss: 186.51
 ---- batch: 050 ----
mean loss: 206.82
 ---- batch: 060 ----
mean loss: 195.07
 ---- batch: 070 ----
mean loss: 187.16
 ---- batch: 080 ----
mean loss: 177.52
 ---- batch: 090 ----
mean loss: 189.98
 ---- batch: 100 ----
mean loss: 199.34
 ---- batch: 110 ----
mean loss: 198.93
train mean loss: 194.23
epoch train time: 0:00:02.506093
elapsed time: 0:07:20.587980
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 21:52:42.450761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.79
 ---- batch: 020 ----
mean loss: 195.69
 ---- batch: 030 ----
mean loss: 197.73
 ---- batch: 040 ----
mean loss: 203.60
 ---- batch: 050 ----
mean loss: 195.40
 ---- batch: 060 ----
mean loss: 191.31
 ---- batch: 070 ----
mean loss: 193.34
 ---- batch: 080 ----
mean loss: 193.28
 ---- batch: 090 ----
mean loss: 186.01
 ---- batch: 100 ----
mean loss: 184.09
 ---- batch: 110 ----
mean loss: 193.73
train mean loss: 193.80
epoch train time: 0:00:02.537226
elapsed time: 0:07:23.125671
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 21:52:44.988442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.63
 ---- batch: 020 ----
mean loss: 197.67
 ---- batch: 030 ----
mean loss: 190.49
 ---- batch: 040 ----
mean loss: 195.34
 ---- batch: 050 ----
mean loss: 198.43
 ---- batch: 060 ----
mean loss: 189.48
 ---- batch: 070 ----
mean loss: 206.56
 ---- batch: 080 ----
mean loss: 188.20
 ---- batch: 090 ----
mean loss: 185.85
 ---- batch: 100 ----
mean loss: 197.43
 ---- batch: 110 ----
mean loss: 190.15
train mean loss: 193.63
epoch train time: 0:00:02.493390
elapsed time: 0:07:25.619550
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 21:52:47.482334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.27
 ---- batch: 020 ----
mean loss: 187.75
 ---- batch: 030 ----
mean loss: 196.32
 ---- batch: 040 ----
mean loss: 195.26
 ---- batch: 050 ----
mean loss: 207.12
 ---- batch: 060 ----
mean loss: 190.59
 ---- batch: 070 ----
mean loss: 189.43
 ---- batch: 080 ----
mean loss: 195.74
 ---- batch: 090 ----
mean loss: 196.97
 ---- batch: 100 ----
mean loss: 186.81
 ---- batch: 110 ----
mean loss: 184.89
train mean loss: 193.53
epoch train time: 0:00:02.522122
elapsed time: 0:07:28.142120
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 21:52:50.004912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.90
 ---- batch: 020 ----
mean loss: 194.96
 ---- batch: 030 ----
mean loss: 191.91
 ---- batch: 040 ----
mean loss: 186.88
 ---- batch: 050 ----
mean loss: 185.19
 ---- batch: 060 ----
mean loss: 193.60
 ---- batch: 070 ----
mean loss: 181.75
 ---- batch: 080 ----
mean loss: 204.19
 ---- batch: 090 ----
mean loss: 198.28
 ---- batch: 100 ----
mean loss: 209.48
 ---- batch: 110 ----
mean loss: 188.17
train mean loss: 193.42
epoch train time: 0:00:02.515124
elapsed time: 0:07:30.657720
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 21:52:52.520524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.56
 ---- batch: 020 ----
mean loss: 192.95
 ---- batch: 030 ----
mean loss: 187.62
 ---- batch: 040 ----
mean loss: 188.07
 ---- batch: 050 ----
mean loss: 194.66
 ---- batch: 060 ----
mean loss: 189.65
 ---- batch: 070 ----
mean loss: 191.76
 ---- batch: 080 ----
mean loss: 190.41
 ---- batch: 090 ----
mean loss: 200.15
 ---- batch: 100 ----
mean loss: 196.71
 ---- batch: 110 ----
mean loss: 197.40
train mean loss: 193.02
epoch train time: 0:00:02.507329
elapsed time: 0:07:33.165511
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 21:52:55.028306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.86
 ---- batch: 020 ----
mean loss: 199.10
 ---- batch: 030 ----
mean loss: 191.22
 ---- batch: 040 ----
mean loss: 201.90
 ---- batch: 050 ----
mean loss: 202.77
 ---- batch: 060 ----
mean loss: 188.09
 ---- batch: 070 ----
mean loss: 183.21
 ---- batch: 080 ----
mean loss: 191.35
 ---- batch: 090 ----
mean loss: 188.27
 ---- batch: 100 ----
mean loss: 187.29
 ---- batch: 110 ----
mean loss: 192.05
train mean loss: 192.93
epoch train time: 0:00:02.501880
elapsed time: 0:07:35.667852
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 21:52:57.530675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.30
 ---- batch: 020 ----
mean loss: 192.80
 ---- batch: 030 ----
mean loss: 181.28
 ---- batch: 040 ----
mean loss: 203.93
 ---- batch: 050 ----
mean loss: 199.06
 ---- batch: 060 ----
mean loss: 194.02
 ---- batch: 070 ----
mean loss: 193.01
 ---- batch: 080 ----
mean loss: 198.86
 ---- batch: 090 ----
mean loss: 189.08
 ---- batch: 100 ----
mean loss: 194.49
 ---- batch: 110 ----
mean loss: 184.68
train mean loss: 192.76
epoch train time: 0:00:02.511437
elapsed time: 0:07:38.179756
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 21:53:00.042533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.29
 ---- batch: 020 ----
mean loss: 197.21
 ---- batch: 030 ----
mean loss: 198.28
 ---- batch: 040 ----
mean loss: 189.80
 ---- batch: 050 ----
mean loss: 187.11
 ---- batch: 060 ----
mean loss: 198.17
 ---- batch: 070 ----
mean loss: 189.13
 ---- batch: 080 ----
mean loss: 195.20
 ---- batch: 090 ----
mean loss: 197.50
 ---- batch: 100 ----
mean loss: 185.89
 ---- batch: 110 ----
mean loss: 186.95
train mean loss: 192.48
epoch train time: 0:00:02.510953
elapsed time: 0:07:40.691175
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 21:53:02.553972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.42
 ---- batch: 020 ----
mean loss: 203.74
 ---- batch: 030 ----
mean loss: 198.83
 ---- batch: 040 ----
mean loss: 189.81
 ---- batch: 050 ----
mean loss: 192.75
 ---- batch: 060 ----
mean loss: 185.93
 ---- batch: 070 ----
mean loss: 199.33
 ---- batch: 080 ----
mean loss: 188.87
 ---- batch: 090 ----
mean loss: 193.82
 ---- batch: 100 ----
mean loss: 190.66
 ---- batch: 110 ----
mean loss: 189.56
train mean loss: 192.26
epoch train time: 0:00:02.500547
elapsed time: 0:07:43.192149
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 21:53:05.054917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.35
 ---- batch: 020 ----
mean loss: 199.77
 ---- batch: 030 ----
mean loss: 190.89
 ---- batch: 040 ----
mean loss: 184.15
 ---- batch: 050 ----
mean loss: 191.28
 ---- batch: 060 ----
mean loss: 193.34
 ---- batch: 070 ----
mean loss: 182.74
 ---- batch: 080 ----
mean loss: 193.46
 ---- batch: 090 ----
mean loss: 199.07
 ---- batch: 100 ----
mean loss: 191.22
 ---- batch: 110 ----
mean loss: 196.76
train mean loss: 192.04
epoch train time: 0:00:02.508634
elapsed time: 0:07:45.701167
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 21:53:07.563995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.37
 ---- batch: 020 ----
mean loss: 190.79
 ---- batch: 030 ----
mean loss: 198.61
 ---- batch: 040 ----
mean loss: 192.12
 ---- batch: 050 ----
mean loss: 186.35
 ---- batch: 060 ----
mean loss: 191.61
 ---- batch: 070 ----
mean loss: 192.88
 ---- batch: 080 ----
mean loss: 187.87
 ---- batch: 090 ----
mean loss: 199.19
 ---- batch: 100 ----
mean loss: 182.10
 ---- batch: 110 ----
mean loss: 197.00
train mean loss: 192.05
epoch train time: 0:00:02.493100
elapsed time: 0:07:48.194892
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 21:53:10.057499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.57
 ---- batch: 020 ----
mean loss: 197.92
 ---- batch: 030 ----
mean loss: 190.72
 ---- batch: 040 ----
mean loss: 192.46
 ---- batch: 050 ----
mean loss: 195.20
 ---- batch: 060 ----
mean loss: 192.40
 ---- batch: 070 ----
mean loss: 191.16
 ---- batch: 080 ----
mean loss: 183.62
 ---- batch: 090 ----
mean loss: 190.50
 ---- batch: 100 ----
mean loss: 193.19
 ---- batch: 110 ----
mean loss: 190.89
train mean loss: 191.60
epoch train time: 0:00:02.547846
elapsed time: 0:07:50.742979
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 21:53:12.605858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.79
 ---- batch: 020 ----
mean loss: 190.67
 ---- batch: 030 ----
mean loss: 190.19
 ---- batch: 040 ----
mean loss: 189.26
 ---- batch: 050 ----
mean loss: 186.20
 ---- batch: 060 ----
mean loss: 192.96
 ---- batch: 070 ----
mean loss: 197.06
 ---- batch: 080 ----
mean loss: 187.15
 ---- batch: 090 ----
mean loss: 196.92
 ---- batch: 100 ----
mean loss: 193.91
 ---- batch: 110 ----
mean loss: 184.86
train mean loss: 191.63
epoch train time: 0:00:02.561336
elapsed time: 0:07:53.304869
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 21:53:15.167636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.38
 ---- batch: 020 ----
mean loss: 192.21
 ---- batch: 030 ----
mean loss: 181.87
 ---- batch: 040 ----
mean loss: 201.29
 ---- batch: 050 ----
mean loss: 194.14
 ---- batch: 060 ----
mean loss: 190.88
 ---- batch: 070 ----
mean loss: 191.16
 ---- batch: 080 ----
mean loss: 193.72
 ---- batch: 090 ----
mean loss: 188.05
 ---- batch: 100 ----
mean loss: 189.02
 ---- batch: 110 ----
mean loss: 186.44
train mean loss: 191.38
epoch train time: 0:00:02.550823
elapsed time: 0:07:55.856116
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 21:53:17.718896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.97
 ---- batch: 020 ----
mean loss: 201.50
 ---- batch: 030 ----
mean loss: 198.11
 ---- batch: 040 ----
mean loss: 183.49
 ---- batch: 050 ----
mean loss: 190.74
 ---- batch: 060 ----
mean loss: 189.92
 ---- batch: 070 ----
mean loss: 182.04
 ---- batch: 080 ----
mean loss: 193.64
 ---- batch: 090 ----
mean loss: 192.13
 ---- batch: 100 ----
mean loss: 192.81
 ---- batch: 110 ----
mean loss: 182.95
train mean loss: 191.27
epoch train time: 0:00:02.555495
elapsed time: 0:07:58.412024
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 21:53:20.274791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.45
 ---- batch: 020 ----
mean loss: 197.22
 ---- batch: 030 ----
mean loss: 193.81
 ---- batch: 040 ----
mean loss: 186.52
 ---- batch: 050 ----
mean loss: 184.54
 ---- batch: 060 ----
mean loss: 193.21
 ---- batch: 070 ----
mean loss: 190.03
 ---- batch: 080 ----
mean loss: 185.33
 ---- batch: 090 ----
mean loss: 194.85
 ---- batch: 100 ----
mean loss: 188.24
 ---- batch: 110 ----
mean loss: 196.08
train mean loss: 191.17
epoch train time: 0:00:02.541446
elapsed time: 0:08:00.953881
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 21:53:22.816694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.08
 ---- batch: 020 ----
mean loss: 185.88
 ---- batch: 030 ----
mean loss: 193.85
 ---- batch: 040 ----
mean loss: 194.08
 ---- batch: 050 ----
mean loss: 197.40
 ---- batch: 060 ----
mean loss: 184.89
 ---- batch: 070 ----
mean loss: 194.80
 ---- batch: 080 ----
mean loss: 190.25
 ---- batch: 090 ----
mean loss: 186.59
 ---- batch: 100 ----
mean loss: 186.51
 ---- batch: 110 ----
mean loss: 201.76
train mean loss: 191.14
epoch train time: 0:00:02.511970
elapsed time: 0:08:03.466295
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 21:53:25.329109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.34
 ---- batch: 020 ----
mean loss: 200.94
 ---- batch: 030 ----
mean loss: 187.00
 ---- batch: 040 ----
mean loss: 181.46
 ---- batch: 050 ----
mean loss: 197.44
 ---- batch: 060 ----
mean loss: 194.96
 ---- batch: 070 ----
mean loss: 190.44
 ---- batch: 080 ----
mean loss: 193.68
 ---- batch: 090 ----
mean loss: 186.17
 ---- batch: 100 ----
mean loss: 191.86
 ---- batch: 110 ----
mean loss: 190.38
train mean loss: 190.84
epoch train time: 0:00:02.511645
elapsed time: 0:08:05.978392
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 21:53:27.841158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.22
 ---- batch: 020 ----
mean loss: 190.12
 ---- batch: 030 ----
mean loss: 203.29
 ---- batch: 040 ----
mean loss: 180.73
 ---- batch: 050 ----
mean loss: 188.50
 ---- batch: 060 ----
mean loss: 182.91
 ---- batch: 070 ----
mean loss: 186.30
 ---- batch: 080 ----
mean loss: 193.44
 ---- batch: 090 ----
mean loss: 194.67
 ---- batch: 100 ----
mean loss: 194.57
 ---- batch: 110 ----
mean loss: 191.17
train mean loss: 190.65
epoch train time: 0:00:02.500041
elapsed time: 0:08:08.478850
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 21:53:30.341636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.75
 ---- batch: 020 ----
mean loss: 196.01
 ---- batch: 030 ----
mean loss: 188.91
 ---- batch: 040 ----
mean loss: 195.67
 ---- batch: 050 ----
mean loss: 187.82
 ---- batch: 060 ----
mean loss: 194.43
 ---- batch: 070 ----
mean loss: 190.44
 ---- batch: 080 ----
mean loss: 186.61
 ---- batch: 090 ----
mean loss: 191.07
 ---- batch: 100 ----
mean loss: 187.88
 ---- batch: 110 ----
mean loss: 182.70
train mean loss: 190.61
epoch train time: 0:00:02.516597
elapsed time: 0:08:10.995866
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 21:53:32.858648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.58
 ---- batch: 020 ----
mean loss: 194.42
 ---- batch: 030 ----
mean loss: 183.88
 ---- batch: 040 ----
mean loss: 194.65
 ---- batch: 050 ----
mean loss: 180.07
 ---- batch: 060 ----
mean loss: 188.66
 ---- batch: 070 ----
mean loss: 189.11
 ---- batch: 080 ----
mean loss: 192.40
 ---- batch: 090 ----
mean loss: 203.76
 ---- batch: 100 ----
mean loss: 191.56
 ---- batch: 110 ----
mean loss: 184.83
train mean loss: 190.36
epoch train time: 0:00:02.513831
elapsed time: 0:08:13.510119
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 21:53:35.372890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.24
 ---- batch: 020 ----
mean loss: 192.61
 ---- batch: 030 ----
mean loss: 191.04
 ---- batch: 040 ----
mean loss: 187.38
 ---- batch: 050 ----
mean loss: 195.91
 ---- batch: 060 ----
mean loss: 197.03
 ---- batch: 070 ----
mean loss: 191.06
 ---- batch: 080 ----
mean loss: 188.67
 ---- batch: 090 ----
mean loss: 182.67
 ---- batch: 100 ----
mean loss: 186.62
 ---- batch: 110 ----
mean loss: 195.89
train mean loss: 190.18
epoch train time: 0:00:02.505356
elapsed time: 0:08:16.015884
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 21:53:37.878663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.13
 ---- batch: 020 ----
mean loss: 180.55
 ---- batch: 030 ----
mean loss: 196.22
 ---- batch: 040 ----
mean loss: 188.91
 ---- batch: 050 ----
mean loss: 193.89
 ---- batch: 060 ----
mean loss: 191.29
 ---- batch: 070 ----
mean loss: 191.34
 ---- batch: 080 ----
mean loss: 184.85
 ---- batch: 090 ----
mean loss: 188.67
 ---- batch: 100 ----
mean loss: 194.83
 ---- batch: 110 ----
mean loss: 192.84
train mean loss: 190.24
epoch train time: 0:00:02.520191
elapsed time: 0:08:18.536510
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 21:53:40.399346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.01
 ---- batch: 020 ----
mean loss: 199.43
 ---- batch: 030 ----
mean loss: 195.55
 ---- batch: 040 ----
mean loss: 185.77
 ---- batch: 050 ----
mean loss: 189.48
 ---- batch: 060 ----
mean loss: 187.60
 ---- batch: 070 ----
mean loss: 193.02
 ---- batch: 080 ----
mean loss: 187.05
 ---- batch: 090 ----
mean loss: 182.35
 ---- batch: 100 ----
mean loss: 182.98
 ---- batch: 110 ----
mean loss: 189.67
train mean loss: 189.92
epoch train time: 0:00:02.492615
elapsed time: 0:08:21.029605
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 21:53:42.892380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.87
 ---- batch: 020 ----
mean loss: 185.39
 ---- batch: 030 ----
mean loss: 190.18
 ---- batch: 040 ----
mean loss: 194.51
 ---- batch: 050 ----
mean loss: 192.91
 ---- batch: 060 ----
mean loss: 200.81
 ---- batch: 070 ----
mean loss: 186.98
 ---- batch: 080 ----
mean loss: 183.88
 ---- batch: 090 ----
mean loss: 193.84
 ---- batch: 100 ----
mean loss: 187.93
 ---- batch: 110 ----
mean loss: 189.62
train mean loss: 189.94
epoch train time: 0:00:02.490834
elapsed time: 0:08:23.520846
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 21:53:45.383619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.71
 ---- batch: 020 ----
mean loss: 189.80
 ---- batch: 030 ----
mean loss: 201.71
 ---- batch: 040 ----
mean loss: 183.85
 ---- batch: 050 ----
mean loss: 189.57
 ---- batch: 060 ----
mean loss: 185.56
 ---- batch: 070 ----
mean loss: 190.54
 ---- batch: 080 ----
mean loss: 188.42
 ---- batch: 090 ----
mean loss: 190.50
 ---- batch: 100 ----
mean loss: 191.76
 ---- batch: 110 ----
mean loss: 184.41
train mean loss: 189.73
epoch train time: 0:00:02.502422
elapsed time: 0:08:26.023674
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 21:53:47.886472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.27
 ---- batch: 020 ----
mean loss: 190.18
 ---- batch: 030 ----
mean loss: 203.21
 ---- batch: 040 ----
mean loss: 191.32
 ---- batch: 050 ----
mean loss: 189.47
 ---- batch: 060 ----
mean loss: 188.38
 ---- batch: 070 ----
mean loss: 185.64
 ---- batch: 080 ----
mean loss: 193.97
 ---- batch: 090 ----
mean loss: 189.47
 ---- batch: 100 ----
mean loss: 181.50
 ---- batch: 110 ----
mean loss: 183.94
train mean loss: 189.54
epoch train time: 0:00:02.488214
elapsed time: 0:08:28.512321
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 21:53:50.375070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.25
 ---- batch: 020 ----
mean loss: 185.90
 ---- batch: 030 ----
mean loss: 199.01
 ---- batch: 040 ----
mean loss: 186.18
 ---- batch: 050 ----
mean loss: 183.95
 ---- batch: 060 ----
mean loss: 192.68
 ---- batch: 070 ----
mean loss: 199.11
 ---- batch: 080 ----
mean loss: 187.38
 ---- batch: 090 ----
mean loss: 184.55
 ---- batch: 100 ----
mean loss: 190.90
 ---- batch: 110 ----
mean loss: 196.57
train mean loss: 189.58
epoch train time: 0:00:02.502767
elapsed time: 0:08:31.015490
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 21:53:52.878277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.23
 ---- batch: 020 ----
mean loss: 182.38
 ---- batch: 030 ----
mean loss: 187.87
 ---- batch: 040 ----
mean loss: 190.25
 ---- batch: 050 ----
mean loss: 197.54
 ---- batch: 060 ----
mean loss: 192.47
 ---- batch: 070 ----
mean loss: 186.64
 ---- batch: 080 ----
mean loss: 185.33
 ---- batch: 090 ----
mean loss: 193.90
 ---- batch: 100 ----
mean loss: 192.53
 ---- batch: 110 ----
mean loss: 185.47
train mean loss: 189.30
epoch train time: 0:00:02.508685
elapsed time: 0:08:33.524603
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 21:53:55.387441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.70
 ---- batch: 020 ----
mean loss: 186.24
 ---- batch: 030 ----
mean loss: 187.48
 ---- batch: 040 ----
mean loss: 187.73
 ---- batch: 050 ----
mean loss: 195.87
 ---- batch: 060 ----
mean loss: 197.17
 ---- batch: 070 ----
mean loss: 183.63
 ---- batch: 080 ----
mean loss: 189.12
 ---- batch: 090 ----
mean loss: 182.75
 ---- batch: 100 ----
mean loss: 185.93
 ---- batch: 110 ----
mean loss: 188.92
train mean loss: 189.19
epoch train time: 0:00:02.515814
elapsed time: 0:08:36.040914
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 21:53:57.903739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.48
 ---- batch: 020 ----
mean loss: 194.07
 ---- batch: 030 ----
mean loss: 186.07
 ---- batch: 040 ----
mean loss: 193.23
 ---- batch: 050 ----
mean loss: 197.21
 ---- batch: 060 ----
mean loss: 180.18
 ---- batch: 070 ----
mean loss: 181.20
 ---- batch: 080 ----
mean loss: 188.68
 ---- batch: 090 ----
mean loss: 182.08
 ---- batch: 100 ----
mean loss: 195.26
 ---- batch: 110 ----
mean loss: 194.08
train mean loss: 189.10
epoch train time: 0:00:02.503701
elapsed time: 0:08:38.545059
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 21:54:00.407893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.90
 ---- batch: 020 ----
mean loss: 197.61
 ---- batch: 030 ----
mean loss: 194.08
 ---- batch: 040 ----
mean loss: 188.28
 ---- batch: 050 ----
mean loss: 180.77
 ---- batch: 060 ----
mean loss: 188.01
 ---- batch: 070 ----
mean loss: 194.23
 ---- batch: 080 ----
mean loss: 185.73
 ---- batch: 090 ----
mean loss: 190.98
 ---- batch: 100 ----
mean loss: 193.51
 ---- batch: 110 ----
mean loss: 179.90
train mean loss: 188.90
epoch train time: 0:00:02.522918
elapsed time: 0:08:41.068455
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 21:54:02.931235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.79
 ---- batch: 020 ----
mean loss: 194.36
 ---- batch: 030 ----
mean loss: 198.33
 ---- batch: 040 ----
mean loss: 191.84
 ---- batch: 050 ----
mean loss: 180.79
 ---- batch: 060 ----
mean loss: 183.92
 ---- batch: 070 ----
mean loss: 186.87
 ---- batch: 080 ----
mean loss: 185.55
 ---- batch: 090 ----
mean loss: 193.64
 ---- batch: 100 ----
mean loss: 186.15
 ---- batch: 110 ----
mean loss: 194.51
train mean loss: 188.84
epoch train time: 0:00:02.523700
elapsed time: 0:08:43.592580
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 21:54:05.455393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.68
 ---- batch: 020 ----
mean loss: 190.08
 ---- batch: 030 ----
mean loss: 184.30
 ---- batch: 040 ----
mean loss: 199.69
 ---- batch: 050 ----
mean loss: 185.29
 ---- batch: 060 ----
mean loss: 186.42
 ---- batch: 070 ----
mean loss: 187.30
 ---- batch: 080 ----
mean loss: 187.67
 ---- batch: 090 ----
mean loss: 185.13
 ---- batch: 100 ----
mean loss: 179.22
 ---- batch: 110 ----
mean loss: 197.09
train mean loss: 188.64
epoch train time: 0:00:02.519421
elapsed time: 0:08:46.112454
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 21:54:07.975245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.76
 ---- batch: 020 ----
mean loss: 186.77
 ---- batch: 030 ----
mean loss: 192.67
 ---- batch: 040 ----
mean loss: 180.25
 ---- batch: 050 ----
mean loss: 195.90
 ---- batch: 060 ----
mean loss: 182.19
 ---- batch: 070 ----
mean loss: 202.26
 ---- batch: 080 ----
mean loss: 194.00
 ---- batch: 090 ----
mean loss: 187.89
 ---- batch: 100 ----
mean loss: 184.83
 ---- batch: 110 ----
mean loss: 183.14
train mean loss: 188.61
epoch train time: 0:00:02.498472
elapsed time: 0:08:48.611344
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 21:54:10.474120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.98
 ---- batch: 020 ----
mean loss: 185.22
 ---- batch: 030 ----
mean loss: 188.23
 ---- batch: 040 ----
mean loss: 187.93
 ---- batch: 050 ----
mean loss: 186.40
 ---- batch: 060 ----
mean loss: 196.01
 ---- batch: 070 ----
mean loss: 183.11
 ---- batch: 080 ----
mean loss: 183.27
 ---- batch: 090 ----
mean loss: 186.09
 ---- batch: 100 ----
mean loss: 178.70
 ---- batch: 110 ----
mean loss: 195.80
train mean loss: 188.42
epoch train time: 0:00:02.474547
elapsed time: 0:08:51.086299
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 21:54:12.949088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.32
 ---- batch: 020 ----
mean loss: 180.08
 ---- batch: 030 ----
mean loss: 197.99
 ---- batch: 040 ----
mean loss: 180.99
 ---- batch: 050 ----
mean loss: 188.86
 ---- batch: 060 ----
mean loss: 200.96
 ---- batch: 070 ----
mean loss: 189.20
 ---- batch: 080 ----
mean loss: 188.63
 ---- batch: 090 ----
mean loss: 180.54
 ---- batch: 100 ----
mean loss: 183.54
 ---- batch: 110 ----
mean loss: 189.84
train mean loss: 188.33
epoch train time: 0:00:02.501043
elapsed time: 0:08:53.587775
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 21:54:15.450597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.91
 ---- batch: 020 ----
mean loss: 182.99
 ---- batch: 030 ----
mean loss: 189.95
 ---- batch: 040 ----
mean loss: 183.42
 ---- batch: 050 ----
mean loss: 191.35
 ---- batch: 060 ----
mean loss: 192.73
 ---- batch: 070 ----
mean loss: 188.24
 ---- batch: 080 ----
mean loss: 179.37
 ---- batch: 090 ----
mean loss: 186.84
 ---- batch: 100 ----
mean loss: 188.66
 ---- batch: 110 ----
mean loss: 197.97
train mean loss: 188.34
epoch train time: 0:00:02.504583
elapsed time: 0:08:56.092831
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 21:54:17.955650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.88
 ---- batch: 020 ----
mean loss: 188.23
 ---- batch: 030 ----
mean loss: 195.61
 ---- batch: 040 ----
mean loss: 188.71
 ---- batch: 050 ----
mean loss: 182.55
 ---- batch: 060 ----
mean loss: 188.00
 ---- batch: 070 ----
mean loss: 181.78
 ---- batch: 080 ----
mean loss: 188.43
 ---- batch: 090 ----
mean loss: 189.72
 ---- batch: 100 ----
mean loss: 184.74
 ---- batch: 110 ----
mean loss: 189.30
train mean loss: 187.77
epoch train time: 0:00:02.504099
elapsed time: 0:08:58.597676
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 21:54:20.460326
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.18
 ---- batch: 020 ----
mean loss: 182.82
 ---- batch: 030 ----
mean loss: 179.39
 ---- batch: 040 ----
mean loss: 193.64
 ---- batch: 050 ----
mean loss: 189.81
 ---- batch: 060 ----
mean loss: 188.87
 ---- batch: 070 ----
mean loss: 182.01
 ---- batch: 080 ----
mean loss: 197.17
 ---- batch: 090 ----
mean loss: 190.66
 ---- batch: 100 ----
mean loss: 185.23
 ---- batch: 110 ----
mean loss: 180.70
train mean loss: 187.73
epoch train time: 0:00:02.522899
elapsed time: 0:09:01.120857
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 21:54:22.983623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.03
 ---- batch: 020 ----
mean loss: 189.13
 ---- batch: 030 ----
mean loss: 190.13
 ---- batch: 040 ----
mean loss: 189.73
 ---- batch: 050 ----
mean loss: 189.93
 ---- batch: 060 ----
mean loss: 187.58
 ---- batch: 070 ----
mean loss: 183.10
 ---- batch: 080 ----
mean loss: 195.29
 ---- batch: 090 ----
mean loss: 186.32
 ---- batch: 100 ----
mean loss: 186.01
 ---- batch: 110 ----
mean loss: 182.88
train mean loss: 187.73
epoch train time: 0:00:02.523847
elapsed time: 0:09:03.645119
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 21:54:25.507889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.33
 ---- batch: 020 ----
mean loss: 188.33
 ---- batch: 030 ----
mean loss: 183.40
 ---- batch: 040 ----
mean loss: 183.89
 ---- batch: 050 ----
mean loss: 191.69
 ---- batch: 060 ----
mean loss: 187.94
 ---- batch: 070 ----
mean loss: 192.17
 ---- batch: 080 ----
mean loss: 194.40
 ---- batch: 090 ----
mean loss: 191.40
 ---- batch: 100 ----
mean loss: 181.16
 ---- batch: 110 ----
mean loss: 184.75
train mean loss: 187.69
epoch train time: 0:00:02.536553
elapsed time: 0:09:06.182078
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 21:54:28.044857
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.51
 ---- batch: 020 ----
mean loss: 184.14
 ---- batch: 030 ----
mean loss: 189.14
 ---- batch: 040 ----
mean loss: 183.08
 ---- batch: 050 ----
mean loss: 188.10
 ---- batch: 060 ----
mean loss: 191.23
 ---- batch: 070 ----
mean loss: 189.97
 ---- batch: 080 ----
mean loss: 194.77
 ---- batch: 090 ----
mean loss: 183.10
 ---- batch: 100 ----
mean loss: 179.38
 ---- batch: 110 ----
mean loss: 196.60
train mean loss: 187.73
epoch train time: 0:00:02.504467
elapsed time: 0:09:08.686949
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 21:54:30.549770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.04
 ---- batch: 020 ----
mean loss: 185.32
 ---- batch: 030 ----
mean loss: 189.03
 ---- batch: 040 ----
mean loss: 190.12
 ---- batch: 050 ----
mean loss: 180.40
 ---- batch: 060 ----
mean loss: 192.93
 ---- batch: 070 ----
mean loss: 189.23
 ---- batch: 080 ----
mean loss: 189.84
 ---- batch: 090 ----
mean loss: 188.25
 ---- batch: 100 ----
mean loss: 176.65
 ---- batch: 110 ----
mean loss: 192.41
train mean loss: 187.76
epoch train time: 0:00:02.496742
elapsed time: 0:09:11.184149
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 21:54:33.046921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.90
 ---- batch: 020 ----
mean loss: 189.37
 ---- batch: 030 ----
mean loss: 188.94
 ---- batch: 040 ----
mean loss: 198.05
 ---- batch: 050 ----
mean loss: 176.51
 ---- batch: 060 ----
mean loss: 189.06
 ---- batch: 070 ----
mean loss: 179.44
 ---- batch: 080 ----
mean loss: 191.86
 ---- batch: 090 ----
mean loss: 183.78
 ---- batch: 100 ----
mean loss: 196.77
 ---- batch: 110 ----
mean loss: 184.93
train mean loss: 187.71
epoch train time: 0:00:02.517804
elapsed time: 0:09:13.702382
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 21:54:35.565163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.50
 ---- batch: 020 ----
mean loss: 185.79
 ---- batch: 030 ----
mean loss: 179.95
 ---- batch: 040 ----
mean loss: 185.52
 ---- batch: 050 ----
mean loss: 189.76
 ---- batch: 060 ----
mean loss: 196.26
 ---- batch: 070 ----
mean loss: 196.17
 ---- batch: 080 ----
mean loss: 194.14
 ---- batch: 090 ----
mean loss: 184.82
 ---- batch: 100 ----
mean loss: 182.12
 ---- batch: 110 ----
mean loss: 191.49
train mean loss: 187.75
epoch train time: 0:00:02.564802
elapsed time: 0:09:16.267598
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 21:54:38.130370
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.94
 ---- batch: 020 ----
mean loss: 170.63
 ---- batch: 030 ----
mean loss: 192.71
 ---- batch: 040 ----
mean loss: 181.02
 ---- batch: 050 ----
mean loss: 183.53
 ---- batch: 060 ----
mean loss: 194.88
 ---- batch: 070 ----
mean loss: 188.08
 ---- batch: 080 ----
mean loss: 186.30
 ---- batch: 090 ----
mean loss: 189.40
 ---- batch: 100 ----
mean loss: 187.35
 ---- batch: 110 ----
mean loss: 200.86
train mean loss: 187.65
epoch train time: 0:00:02.492409
elapsed time: 0:09:18.760425
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 21:54:40.623258
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.75
 ---- batch: 020 ----
mean loss: 185.29
 ---- batch: 030 ----
mean loss: 189.03
 ---- batch: 040 ----
mean loss: 192.88
 ---- batch: 050 ----
mean loss: 190.67
 ---- batch: 060 ----
mean loss: 191.77
 ---- batch: 070 ----
mean loss: 182.19
 ---- batch: 080 ----
mean loss: 181.79
 ---- batch: 090 ----
mean loss: 179.90
 ---- batch: 100 ----
mean loss: 180.59
 ---- batch: 110 ----
mean loss: 187.01
train mean loss: 187.68
epoch train time: 0:00:02.514399
elapsed time: 0:09:21.275279
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 21:54:43.138041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.30
 ---- batch: 020 ----
mean loss: 193.52
 ---- batch: 030 ----
mean loss: 188.75
 ---- batch: 040 ----
mean loss: 189.85
 ---- batch: 050 ----
mean loss: 194.47
 ---- batch: 060 ----
mean loss: 187.31
 ---- batch: 070 ----
mean loss: 181.14
 ---- batch: 080 ----
mean loss: 183.11
 ---- batch: 090 ----
mean loss: 194.71
 ---- batch: 100 ----
mean loss: 179.13
 ---- batch: 110 ----
mean loss: 183.44
train mean loss: 187.62
epoch train time: 0:00:02.518055
elapsed time: 0:09:23.793750
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 21:54:45.656532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.56
 ---- batch: 020 ----
mean loss: 186.49
 ---- batch: 030 ----
mean loss: 187.62
 ---- batch: 040 ----
mean loss: 192.85
 ---- batch: 050 ----
mean loss: 191.56
 ---- batch: 060 ----
mean loss: 189.50
 ---- batch: 070 ----
mean loss: 191.66
 ---- batch: 080 ----
mean loss: 183.83
 ---- batch: 090 ----
mean loss: 189.05
 ---- batch: 100 ----
mean loss: 185.50
 ---- batch: 110 ----
mean loss: 185.60
train mean loss: 187.60
epoch train time: 0:00:02.540782
elapsed time: 0:09:26.334973
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 21:54:48.197817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.36
 ---- batch: 020 ----
mean loss: 185.17
 ---- batch: 030 ----
mean loss: 178.76
 ---- batch: 040 ----
mean loss: 194.67
 ---- batch: 050 ----
mean loss: 191.57
 ---- batch: 060 ----
mean loss: 192.67
 ---- batch: 070 ----
mean loss: 180.21
 ---- batch: 080 ----
mean loss: 187.15
 ---- batch: 090 ----
mean loss: 177.35
 ---- batch: 100 ----
mean loss: 190.91
 ---- batch: 110 ----
mean loss: 194.81
train mean loss: 187.56
epoch train time: 0:00:02.524801
elapsed time: 0:09:28.860284
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 21:54:50.723081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.03
 ---- batch: 020 ----
mean loss: 188.42
 ---- batch: 030 ----
mean loss: 187.09
 ---- batch: 040 ----
mean loss: 176.47
 ---- batch: 050 ----
mean loss: 191.61
 ---- batch: 060 ----
mean loss: 185.50
 ---- batch: 070 ----
mean loss: 195.77
 ---- batch: 080 ----
mean loss: 189.22
 ---- batch: 090 ----
mean loss: 184.81
 ---- batch: 100 ----
mean loss: 188.10
 ---- batch: 110 ----
mean loss: 192.47
train mean loss: 187.63
epoch train time: 0:00:02.503021
elapsed time: 0:09:31.363753
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 21:54:53.226549
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.42
 ---- batch: 020 ----
mean loss: 193.54
 ---- batch: 030 ----
mean loss: 189.83
 ---- batch: 040 ----
mean loss: 193.31
 ---- batch: 050 ----
mean loss: 181.92
 ---- batch: 060 ----
mean loss: 183.71
 ---- batch: 070 ----
mean loss: 194.63
 ---- batch: 080 ----
mean loss: 181.37
 ---- batch: 090 ----
mean loss: 176.56
 ---- batch: 100 ----
mean loss: 183.98
 ---- batch: 110 ----
mean loss: 187.63
train mean loss: 187.60
epoch train time: 0:00:02.554359
elapsed time: 0:09:33.918563
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 21:54:55.781353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.26
 ---- batch: 020 ----
mean loss: 189.31
 ---- batch: 030 ----
mean loss: 188.56
 ---- batch: 040 ----
mean loss: 188.33
 ---- batch: 050 ----
mean loss: 185.84
 ---- batch: 060 ----
mean loss: 186.71
 ---- batch: 070 ----
mean loss: 183.01
 ---- batch: 080 ----
mean loss: 191.69
 ---- batch: 090 ----
mean loss: 186.53
 ---- batch: 100 ----
mean loss: 189.59
 ---- batch: 110 ----
mean loss: 184.55
train mean loss: 187.63
epoch train time: 0:00:02.571149
elapsed time: 0:09:36.490158
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 21:54:58.352951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.13
 ---- batch: 020 ----
mean loss: 185.48
 ---- batch: 030 ----
mean loss: 188.15
 ---- batch: 040 ----
mean loss: 186.63
 ---- batch: 050 ----
mean loss: 185.17
 ---- batch: 060 ----
mean loss: 190.12
 ---- batch: 070 ----
mean loss: 185.40
 ---- batch: 080 ----
mean loss: 191.03
 ---- batch: 090 ----
mean loss: 187.60
 ---- batch: 100 ----
mean loss: 195.76
 ---- batch: 110 ----
mean loss: 187.03
train mean loss: 187.60
epoch train time: 0:00:02.523196
elapsed time: 0:09:39.013781
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 21:55:00.876556
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.03
 ---- batch: 020 ----
mean loss: 191.05
 ---- batch: 030 ----
mean loss: 194.80
 ---- batch: 040 ----
mean loss: 191.23
 ---- batch: 050 ----
mean loss: 184.55
 ---- batch: 060 ----
mean loss: 191.42
 ---- batch: 070 ----
mean loss: 185.31
 ---- batch: 080 ----
mean loss: 177.75
 ---- batch: 090 ----
mean loss: 184.72
 ---- batch: 100 ----
mean loss: 188.12
 ---- batch: 110 ----
mean loss: 185.52
train mean loss: 187.53
epoch train time: 0:00:02.518419
elapsed time: 0:09:41.532620
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 21:55:03.395420
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.92
 ---- batch: 020 ----
mean loss: 189.85
 ---- batch: 030 ----
mean loss: 184.90
 ---- batch: 040 ----
mean loss: 183.93
 ---- batch: 050 ----
mean loss: 186.81
 ---- batch: 060 ----
mean loss: 179.28
 ---- batch: 070 ----
mean loss: 188.13
 ---- batch: 080 ----
mean loss: 194.38
 ---- batch: 090 ----
mean loss: 189.64
 ---- batch: 100 ----
mean loss: 191.01
 ---- batch: 110 ----
mean loss: 188.65
train mean loss: 187.58
epoch train time: 0:00:02.513018
elapsed time: 0:09:44.046101
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 21:55:05.908871
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.91
 ---- batch: 020 ----
mean loss: 188.83
 ---- batch: 030 ----
mean loss: 190.04
 ---- batch: 040 ----
mean loss: 187.66
 ---- batch: 050 ----
mean loss: 194.53
 ---- batch: 060 ----
mean loss: 183.13
 ---- batch: 070 ----
mean loss: 187.06
 ---- batch: 080 ----
mean loss: 189.58
 ---- batch: 090 ----
mean loss: 188.01
 ---- batch: 100 ----
mean loss: 192.51
 ---- batch: 110 ----
mean loss: 183.86
train mean loss: 187.49
epoch train time: 0:00:02.518589
elapsed time: 0:09:46.565181
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 21:55:08.427972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.27
 ---- batch: 020 ----
mean loss: 181.34
 ---- batch: 030 ----
mean loss: 192.46
 ---- batch: 040 ----
mean loss: 197.79
 ---- batch: 050 ----
mean loss: 180.09
 ---- batch: 060 ----
mean loss: 184.42
 ---- batch: 070 ----
mean loss: 193.00
 ---- batch: 080 ----
mean loss: 193.77
 ---- batch: 090 ----
mean loss: 185.59
 ---- batch: 100 ----
mean loss: 184.97
 ---- batch: 110 ----
mean loss: 190.89
train mean loss: 187.49
epoch train time: 0:00:02.522143
elapsed time: 0:09:49.087775
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 21:55:10.950537
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.45
 ---- batch: 020 ----
mean loss: 193.28
 ---- batch: 030 ----
mean loss: 200.18
 ---- batch: 040 ----
mean loss: 180.45
 ---- batch: 050 ----
mean loss: 186.75
 ---- batch: 060 ----
mean loss: 185.12
 ---- batch: 070 ----
mean loss: 186.85
 ---- batch: 080 ----
mean loss: 181.07
 ---- batch: 090 ----
mean loss: 183.10
 ---- batch: 100 ----
mean loss: 190.73
 ---- batch: 110 ----
mean loss: 183.12
train mean loss: 187.54
epoch train time: 0:00:02.514620
elapsed time: 0:09:51.602789
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 21:55:13.465569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.44
 ---- batch: 020 ----
mean loss: 187.51
 ---- batch: 030 ----
mean loss: 184.13
 ---- batch: 040 ----
mean loss: 190.16
 ---- batch: 050 ----
mean loss: 194.71
 ---- batch: 060 ----
mean loss: 185.94
 ---- batch: 070 ----
mean loss: 183.99
 ---- batch: 080 ----
mean loss: 183.64
 ---- batch: 090 ----
mean loss: 191.41
 ---- batch: 100 ----
mean loss: 189.33
 ---- batch: 110 ----
mean loss: 184.09
train mean loss: 187.48
epoch train time: 0:00:02.532094
elapsed time: 0:09:54.135319
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 21:55:15.998089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.14
 ---- batch: 020 ----
mean loss: 185.56
 ---- batch: 030 ----
mean loss: 187.00
 ---- batch: 040 ----
mean loss: 190.34
 ---- batch: 050 ----
mean loss: 186.92
 ---- batch: 060 ----
mean loss: 198.08
 ---- batch: 070 ----
mean loss: 186.13
 ---- batch: 080 ----
mean loss: 191.38
 ---- batch: 090 ----
mean loss: 189.49
 ---- batch: 100 ----
mean loss: 174.97
 ---- batch: 110 ----
mean loss: 185.69
train mean loss: 187.51
epoch train time: 0:00:02.528589
elapsed time: 0:09:56.664339
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 21:55:18.527149
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.30
 ---- batch: 020 ----
mean loss: 182.93
 ---- batch: 030 ----
mean loss: 193.56
 ---- batch: 040 ----
mean loss: 194.60
 ---- batch: 050 ----
mean loss: 175.44
 ---- batch: 060 ----
mean loss: 190.70
 ---- batch: 070 ----
mean loss: 181.26
 ---- batch: 080 ----
mean loss: 181.64
 ---- batch: 090 ----
mean loss: 192.56
 ---- batch: 100 ----
mean loss: 193.50
 ---- batch: 110 ----
mean loss: 185.27
train mean loss: 187.52
epoch train time: 0:00:02.550018
elapsed time: 0:09:59.214790
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 21:55:21.077600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.80
 ---- batch: 020 ----
mean loss: 192.26
 ---- batch: 030 ----
mean loss: 191.57
 ---- batch: 040 ----
mean loss: 183.84
 ---- batch: 050 ----
mean loss: 189.13
 ---- batch: 060 ----
mean loss: 176.67
 ---- batch: 070 ----
mean loss: 193.17
 ---- batch: 080 ----
mean loss: 182.98
 ---- batch: 090 ----
mean loss: 193.18
 ---- batch: 100 ----
mean loss: 192.31
 ---- batch: 110 ----
mean loss: 188.95
train mean loss: 187.37
epoch train time: 0:00:02.520578
elapsed time: 0:10:01.735812
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 21:55:23.598577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.04
 ---- batch: 020 ----
mean loss: 188.44
 ---- batch: 030 ----
mean loss: 189.17
 ---- batch: 040 ----
mean loss: 183.82
 ---- batch: 050 ----
mean loss: 185.15
 ---- batch: 060 ----
mean loss: 191.01
 ---- batch: 070 ----
mean loss: 195.57
 ---- batch: 080 ----
mean loss: 193.57
 ---- batch: 090 ----
mean loss: 189.84
 ---- batch: 100 ----
mean loss: 187.44
 ---- batch: 110 ----
mean loss: 181.60
train mean loss: 187.44
epoch train time: 0:00:02.498201
elapsed time: 0:10:04.234413
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 21:55:26.097208
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.73
 ---- batch: 020 ----
mean loss: 183.94
 ---- batch: 030 ----
mean loss: 183.75
 ---- batch: 040 ----
mean loss: 192.63
 ---- batch: 050 ----
mean loss: 189.03
 ---- batch: 060 ----
mean loss: 188.97
 ---- batch: 070 ----
mean loss: 192.44
 ---- batch: 080 ----
mean loss: 189.69
 ---- batch: 090 ----
mean loss: 181.58
 ---- batch: 100 ----
mean loss: 182.24
 ---- batch: 110 ----
mean loss: 188.17
train mean loss: 187.47
epoch train time: 0:00:02.504778
elapsed time: 0:10:06.739629
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 21:55:28.602395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.02
 ---- batch: 020 ----
mean loss: 190.11
 ---- batch: 030 ----
mean loss: 197.24
 ---- batch: 040 ----
mean loss: 190.14
 ---- batch: 050 ----
mean loss: 188.08
 ---- batch: 060 ----
mean loss: 188.02
 ---- batch: 070 ----
mean loss: 185.99
 ---- batch: 080 ----
mean loss: 173.74
 ---- batch: 090 ----
mean loss: 190.56
 ---- batch: 100 ----
mean loss: 192.04
 ---- batch: 110 ----
mean loss: 185.96
train mean loss: 187.45
epoch train time: 0:00:02.520037
elapsed time: 0:10:09.260060
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 21:55:31.122828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.93
 ---- batch: 020 ----
mean loss: 181.73
 ---- batch: 030 ----
mean loss: 181.45
 ---- batch: 040 ----
mean loss: 200.12
 ---- batch: 050 ----
mean loss: 183.13
 ---- batch: 060 ----
mean loss: 185.97
 ---- batch: 070 ----
mean loss: 190.03
 ---- batch: 080 ----
mean loss: 187.46
 ---- batch: 090 ----
mean loss: 185.11
 ---- batch: 100 ----
mean loss: 182.20
 ---- batch: 110 ----
mean loss: 190.54
train mean loss: 187.47
epoch train time: 0:00:02.517176
elapsed time: 0:10:11.777661
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 21:55:33.640477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.36
 ---- batch: 020 ----
mean loss: 177.91
 ---- batch: 030 ----
mean loss: 179.70
 ---- batch: 040 ----
mean loss: 193.38
 ---- batch: 050 ----
mean loss: 189.82
 ---- batch: 060 ----
mean loss: 180.89
 ---- batch: 070 ----
mean loss: 190.17
 ---- batch: 080 ----
mean loss: 197.77
 ---- batch: 090 ----
mean loss: 183.94
 ---- batch: 100 ----
mean loss: 195.94
 ---- batch: 110 ----
mean loss: 189.28
train mean loss: 187.43
epoch train time: 0:00:02.486492
elapsed time: 0:10:14.264596
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 21:55:36.127374
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.68
 ---- batch: 020 ----
mean loss: 184.53
 ---- batch: 030 ----
mean loss: 194.55
 ---- batch: 040 ----
mean loss: 189.55
 ---- batch: 050 ----
mean loss: 176.88
 ---- batch: 060 ----
mean loss: 193.00
 ---- batch: 070 ----
mean loss: 190.70
 ---- batch: 080 ----
mean loss: 193.12
 ---- batch: 090 ----
mean loss: 183.50
 ---- batch: 100 ----
mean loss: 184.21
 ---- batch: 110 ----
mean loss: 187.80
train mean loss: 187.33
epoch train time: 0:00:02.506558
elapsed time: 0:10:16.771558
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 21:55:38.634316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.35
 ---- batch: 020 ----
mean loss: 179.10
 ---- batch: 030 ----
mean loss: 185.49
 ---- batch: 040 ----
mean loss: 188.63
 ---- batch: 050 ----
mean loss: 186.84
 ---- batch: 060 ----
mean loss: 197.51
 ---- batch: 070 ----
mean loss: 189.36
 ---- batch: 080 ----
mean loss: 189.51
 ---- batch: 090 ----
mean loss: 185.39
 ---- batch: 100 ----
mean loss: 191.86
 ---- batch: 110 ----
mean loss: 184.41
train mean loss: 187.35
epoch train time: 0:00:02.487777
elapsed time: 0:10:19.259959
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 21:55:41.122572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.07
 ---- batch: 020 ----
mean loss: 185.10
 ---- batch: 030 ----
mean loss: 177.09
 ---- batch: 040 ----
mean loss: 187.02
 ---- batch: 050 ----
mean loss: 182.20
 ---- batch: 060 ----
mean loss: 189.76
 ---- batch: 070 ----
mean loss: 186.59
 ---- batch: 080 ----
mean loss: 197.06
 ---- batch: 090 ----
mean loss: 191.74
 ---- batch: 100 ----
mean loss: 187.83
 ---- batch: 110 ----
mean loss: 187.80
train mean loss: 187.35
epoch train time: 0:00:02.493735
elapsed time: 0:10:21.753960
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 21:55:43.616764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.10
 ---- batch: 020 ----
mean loss: 179.10
 ---- batch: 030 ----
mean loss: 182.54
 ---- batch: 040 ----
mean loss: 195.95
 ---- batch: 050 ----
mean loss: 184.26
 ---- batch: 060 ----
mean loss: 194.83
 ---- batch: 070 ----
mean loss: 192.38
 ---- batch: 080 ----
mean loss: 179.35
 ---- batch: 090 ----
mean loss: 183.71
 ---- batch: 100 ----
mean loss: 190.19
 ---- batch: 110 ----
mean loss: 185.51
train mean loss: 187.37
epoch train time: 0:00:02.508575
elapsed time: 0:10:24.262972
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 21:55:46.125775
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.48
 ---- batch: 020 ----
mean loss: 188.87
 ---- batch: 030 ----
mean loss: 182.85
 ---- batch: 040 ----
mean loss: 191.19
 ---- batch: 050 ----
mean loss: 188.28
 ---- batch: 060 ----
mean loss: 199.93
 ---- batch: 070 ----
mean loss: 189.14
 ---- batch: 080 ----
mean loss: 183.39
 ---- batch: 090 ----
mean loss: 184.94
 ---- batch: 100 ----
mean loss: 182.12
 ---- batch: 110 ----
mean loss: 183.60
train mean loss: 187.38
epoch train time: 0:00:02.551547
elapsed time: 0:10:26.814965
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 21:55:48.677620
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.86
 ---- batch: 020 ----
mean loss: 197.84
 ---- batch: 030 ----
mean loss: 183.72
 ---- batch: 040 ----
mean loss: 184.57
 ---- batch: 050 ----
mean loss: 187.62
 ---- batch: 060 ----
mean loss: 192.33
 ---- batch: 070 ----
mean loss: 177.79
 ---- batch: 080 ----
mean loss: 182.54
 ---- batch: 090 ----
mean loss: 191.70
 ---- batch: 100 ----
mean loss: 191.23
 ---- batch: 110 ----
mean loss: 180.05
train mean loss: 187.34
epoch train time: 0:00:02.509131
elapsed time: 0:10:29.324376
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 21:55:51.187202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.66
 ---- batch: 020 ----
mean loss: 192.09
 ---- batch: 030 ----
mean loss: 187.33
 ---- batch: 040 ----
mean loss: 189.07
 ---- batch: 050 ----
mean loss: 184.91
 ---- batch: 060 ----
mean loss: 187.23
 ---- batch: 070 ----
mean loss: 196.98
 ---- batch: 080 ----
mean loss: 182.00
 ---- batch: 090 ----
mean loss: 179.47
 ---- batch: 100 ----
mean loss: 193.52
 ---- batch: 110 ----
mean loss: 182.39
train mean loss: 187.28
epoch train time: 0:00:02.502323
elapsed time: 0:10:31.827218
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 21:55:53.690003
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.87
 ---- batch: 020 ----
mean loss: 188.34
 ---- batch: 030 ----
mean loss: 189.08
 ---- batch: 040 ----
mean loss: 191.38
 ---- batch: 050 ----
mean loss: 182.57
 ---- batch: 060 ----
mean loss: 194.13
 ---- batch: 070 ----
mean loss: 190.38
 ---- batch: 080 ----
mean loss: 196.12
 ---- batch: 090 ----
mean loss: 180.31
 ---- batch: 100 ----
mean loss: 187.37
 ---- batch: 110 ----
mean loss: 178.18
train mean loss: 187.32
epoch train time: 0:00:02.507461
elapsed time: 0:10:34.335121
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 21:55:56.197905
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.42
 ---- batch: 020 ----
mean loss: 183.19
 ---- batch: 030 ----
mean loss: 191.46
 ---- batch: 040 ----
mean loss: 179.03
 ---- batch: 050 ----
mean loss: 185.94
 ---- batch: 060 ----
mean loss: 189.61
 ---- batch: 070 ----
mean loss: 190.16
 ---- batch: 080 ----
mean loss: 184.21
 ---- batch: 090 ----
mean loss: 199.17
 ---- batch: 100 ----
mean loss: 185.83
 ---- batch: 110 ----
mean loss: 189.24
train mean loss: 187.37
epoch train time: 0:00:02.516933
elapsed time: 0:10:36.852495
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 21:55:58.715297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.84
 ---- batch: 020 ----
mean loss: 184.37
 ---- batch: 030 ----
mean loss: 184.05
 ---- batch: 040 ----
mean loss: 183.87
 ---- batch: 050 ----
mean loss: 191.63
 ---- batch: 060 ----
mean loss: 192.84
 ---- batch: 070 ----
mean loss: 189.66
 ---- batch: 080 ----
mean loss: 188.27
 ---- batch: 090 ----
mean loss: 190.57
 ---- batch: 100 ----
mean loss: 194.50
 ---- batch: 110 ----
mean loss: 182.34
train mean loss: 187.31
epoch train time: 0:00:02.504821
elapsed time: 0:10:39.357818
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 21:56:01.220580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.02
 ---- batch: 020 ----
mean loss: 201.87
 ---- batch: 030 ----
mean loss: 193.74
 ---- batch: 040 ----
mean loss: 177.70
 ---- batch: 050 ----
mean loss: 182.47
 ---- batch: 060 ----
mean loss: 187.73
 ---- batch: 070 ----
mean loss: 192.36
 ---- batch: 080 ----
mean loss: 173.44
 ---- batch: 090 ----
mean loss: 184.87
 ---- batch: 100 ----
mean loss: 192.71
 ---- batch: 110 ----
mean loss: 186.58
train mean loss: 187.26
epoch train time: 0:00:02.501069
elapsed time: 0:10:41.859304
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 21:56:03.722098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.58
 ---- batch: 020 ----
mean loss: 193.33
 ---- batch: 030 ----
mean loss: 182.80
 ---- batch: 040 ----
mean loss: 197.64
 ---- batch: 050 ----
mean loss: 192.50
 ---- batch: 060 ----
mean loss: 184.76
 ---- batch: 070 ----
mean loss: 193.06
 ---- batch: 080 ----
mean loss: 178.71
 ---- batch: 090 ----
mean loss: 179.26
 ---- batch: 100 ----
mean loss: 190.64
 ---- batch: 110 ----
mean loss: 184.37
train mean loss: 187.28
epoch train time: 0:00:02.509988
elapsed time: 0:10:44.369744
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 21:56:06.232537
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.27
 ---- batch: 020 ----
mean loss: 186.59
 ---- batch: 030 ----
mean loss: 187.75
 ---- batch: 040 ----
mean loss: 187.72
 ---- batch: 050 ----
mean loss: 188.20
 ---- batch: 060 ----
mean loss: 183.01
 ---- batch: 070 ----
mean loss: 191.03
 ---- batch: 080 ----
mean loss: 193.85
 ---- batch: 090 ----
mean loss: 185.40
 ---- batch: 100 ----
mean loss: 187.93
 ---- batch: 110 ----
mean loss: 192.66
train mean loss: 187.22
epoch train time: 0:00:02.515091
elapsed time: 0:10:46.885279
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 21:56:08.748090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.38
 ---- batch: 020 ----
mean loss: 185.66
 ---- batch: 030 ----
mean loss: 189.28
 ---- batch: 040 ----
mean loss: 185.27
 ---- batch: 050 ----
mean loss: 184.79
 ---- batch: 060 ----
mean loss: 189.40
 ---- batch: 070 ----
mean loss: 190.23
 ---- batch: 080 ----
mean loss: 182.92
 ---- batch: 090 ----
mean loss: 187.40
 ---- batch: 100 ----
mean loss: 192.82
 ---- batch: 110 ----
mean loss: 192.73
train mean loss: 187.27
epoch train time: 0:00:02.517203
elapsed time: 0:10:49.402961
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 21:56:11.265751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.63
 ---- batch: 020 ----
mean loss: 186.76
 ---- batch: 030 ----
mean loss: 186.89
 ---- batch: 040 ----
mean loss: 186.26
 ---- batch: 050 ----
mean loss: 188.85
 ---- batch: 060 ----
mean loss: 183.59
 ---- batch: 070 ----
mean loss: 190.56
 ---- batch: 080 ----
mean loss: 185.08
 ---- batch: 090 ----
mean loss: 183.78
 ---- batch: 100 ----
mean loss: 185.99
 ---- batch: 110 ----
mean loss: 189.53
train mean loss: 187.25
epoch train time: 0:00:02.507333
elapsed time: 0:10:51.910718
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 21:56:13.773500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.85
 ---- batch: 020 ----
mean loss: 193.97
 ---- batch: 030 ----
mean loss: 190.11
 ---- batch: 040 ----
mean loss: 187.91
 ---- batch: 050 ----
mean loss: 184.84
 ---- batch: 060 ----
mean loss: 189.80
 ---- batch: 070 ----
mean loss: 189.64
 ---- batch: 080 ----
mean loss: 183.45
 ---- batch: 090 ----
mean loss: 186.24
 ---- batch: 100 ----
mean loss: 191.26
 ---- batch: 110 ----
mean loss: 192.11
train mean loss: 187.16
epoch train time: 0:00:02.511527
elapsed time: 0:10:54.422679
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 21:56:16.285465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.80
 ---- batch: 020 ----
mean loss: 188.13
 ---- batch: 030 ----
mean loss: 186.54
 ---- batch: 040 ----
mean loss: 187.24
 ---- batch: 050 ----
mean loss: 183.92
 ---- batch: 060 ----
mean loss: 183.83
 ---- batch: 070 ----
mean loss: 182.63
 ---- batch: 080 ----
mean loss: 187.64
 ---- batch: 090 ----
mean loss: 189.79
 ---- batch: 100 ----
mean loss: 193.30
 ---- batch: 110 ----
mean loss: 186.99
train mean loss: 187.15
epoch train time: 0:00:02.550675
elapsed time: 0:10:56.973825
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 21:56:18.836675
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.95
 ---- batch: 020 ----
mean loss: 187.85
 ---- batch: 030 ----
mean loss: 184.11
 ---- batch: 040 ----
mean loss: 186.25
 ---- batch: 050 ----
mean loss: 180.71
 ---- batch: 060 ----
mean loss: 193.34
 ---- batch: 070 ----
mean loss: 186.53
 ---- batch: 080 ----
mean loss: 188.23
 ---- batch: 090 ----
mean loss: 187.31
 ---- batch: 100 ----
mean loss: 193.36
 ---- batch: 110 ----
mean loss: 187.36
train mean loss: 187.14
epoch train time: 0:00:02.510339
elapsed time: 0:10:59.488581
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_7/checkpoint.pth.tar
**** end time: 2019-09-26 21:56:21.351136 ****
