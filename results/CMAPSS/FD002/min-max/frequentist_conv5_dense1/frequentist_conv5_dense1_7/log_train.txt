Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_7', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 22452
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-25 21:53:56.895324 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 21, 24]             100
              Tanh-2           [-1, 10, 21, 24]               0
            Conv2d-3           [-1, 10, 20, 24]           1,000
              Tanh-4           [-1, 10, 20, 24]               0
            Conv2d-5           [-1, 10, 21, 24]           1,000
              Tanh-6           [-1, 10, 21, 24]               0
            Conv2d-7           [-1, 10, 20, 24]           1,000
              Tanh-8           [-1, 10, 20, 24]               0
            Conv2d-9            [-1, 1, 20, 24]              30
             Tanh-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
          Dropout-12                  [-1, 480]               0
           Linear-13                  [-1, 100]          48,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 51,230
Trainable params: 51,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 21:53:56.904259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3672.71
 ---- batch: 020 ----
mean loss: 1544.36
 ---- batch: 030 ----
mean loss: 1246.68
 ---- batch: 040 ----
mean loss: 1066.71
 ---- batch: 050 ----
mean loss: 1007.72
 ---- batch: 060 ----
mean loss: 982.57
 ---- batch: 070 ----
mean loss: 943.68
 ---- batch: 080 ----
mean loss: 929.29
 ---- batch: 090 ----
mean loss: 913.28
train mean loss: 1338.27
epoch train time: 0:00:35.319093
elapsed time: 0:00:35.330888
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 21:54:32.226251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.75
 ---- batch: 020 ----
mean loss: 850.98
 ---- batch: 030 ----
mean loss: 842.22
 ---- batch: 040 ----
mean loss: 829.71
 ---- batch: 050 ----
mean loss: 825.42
 ---- batch: 060 ----
mean loss: 804.77
 ---- batch: 070 ----
mean loss: 781.43
 ---- batch: 080 ----
mean loss: 790.52
 ---- batch: 090 ----
mean loss: 803.37
train mean loss: 817.83
epoch train time: 0:00:02.438254
elapsed time: 0:00:37.769298
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 21:54:34.664682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.48
 ---- batch: 020 ----
mean loss: 721.56
 ---- batch: 030 ----
mean loss: 728.27
 ---- batch: 040 ----
mean loss: 733.84
 ---- batch: 050 ----
mean loss: 730.31
 ---- batch: 060 ----
mean loss: 693.21
 ---- batch: 070 ----
mean loss: 694.20
 ---- batch: 080 ----
mean loss: 682.13
 ---- batch: 090 ----
mean loss: 672.28
train mean loss: 709.17
epoch train time: 0:00:02.359201
elapsed time: 0:00:40.128695
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 21:54:37.024106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 683.97
 ---- batch: 020 ----
mean loss: 651.07
 ---- batch: 030 ----
mean loss: 663.04
 ---- batch: 040 ----
mean loss: 641.56
 ---- batch: 050 ----
mean loss: 622.37
 ---- batch: 060 ----
mean loss: 626.67
 ---- batch: 070 ----
mean loss: 614.52
 ---- batch: 080 ----
mean loss: 599.48
 ---- batch: 090 ----
mean loss: 613.76
train mean loss: 633.17
epoch train time: 0:00:02.359518
elapsed time: 0:00:42.488419
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 21:54:39.383827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 602.58
 ---- batch: 020 ----
mean loss: 608.01
 ---- batch: 030 ----
mean loss: 602.85
 ---- batch: 040 ----
mean loss: 576.96
 ---- batch: 050 ----
mean loss: 588.09
 ---- batch: 060 ----
mean loss: 583.30
 ---- batch: 070 ----
mean loss: 580.79
 ---- batch: 080 ----
mean loss: 583.23
 ---- batch: 090 ----
mean loss: 560.36
train mean loss: 585.58
epoch train time: 0:00:02.366206
elapsed time: 0:00:44.854838
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 21:54:41.750222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 577.82
 ---- batch: 020 ----
mean loss: 557.89
 ---- batch: 030 ----
mean loss: 558.27
 ---- batch: 040 ----
mean loss: 554.66
 ---- batch: 050 ----
mean loss: 564.96
 ---- batch: 060 ----
mean loss: 539.24
 ---- batch: 070 ----
mean loss: 542.37
 ---- batch: 080 ----
mean loss: 553.78
 ---- batch: 090 ----
mean loss: 543.35
train mean loss: 553.51
epoch train time: 0:00:02.376257
elapsed time: 0:00:47.231271
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 21:54:44.126654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.10
 ---- batch: 020 ----
mean loss: 541.77
 ---- batch: 030 ----
mean loss: 526.15
 ---- batch: 040 ----
mean loss: 546.79
 ---- batch: 050 ----
mean loss: 537.85
 ---- batch: 060 ----
mean loss: 539.26
 ---- batch: 070 ----
mean loss: 534.56
 ---- batch: 080 ----
mean loss: 526.28
 ---- batch: 090 ----
mean loss: 536.31
train mean loss: 535.50
epoch train time: 0:00:02.365119
elapsed time: 0:00:49.596556
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 21:54:46.491936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.33
 ---- batch: 020 ----
mean loss: 524.21
 ---- batch: 030 ----
mean loss: 524.21
 ---- batch: 040 ----
mean loss: 511.92
 ---- batch: 050 ----
mean loss: 516.13
 ---- batch: 060 ----
mean loss: 513.47
 ---- batch: 070 ----
mean loss: 529.64
 ---- batch: 080 ----
mean loss: 494.71
 ---- batch: 090 ----
mean loss: 506.46
train mean loss: 515.46
epoch train time: 0:00:02.364260
elapsed time: 0:00:51.961041
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 21:54:48.856427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.06
 ---- batch: 020 ----
mean loss: 487.16
 ---- batch: 030 ----
mean loss: 505.54
 ---- batch: 040 ----
mean loss: 493.10
 ---- batch: 050 ----
mean loss: 480.56
 ---- batch: 060 ----
mean loss: 491.18
 ---- batch: 070 ----
mean loss: 481.39
 ---- batch: 080 ----
mean loss: 498.14
 ---- batch: 090 ----
mean loss: 479.37
train mean loss: 492.79
epoch train time: 0:00:02.361513
elapsed time: 0:00:54.322736
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 21:54:51.218119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.26
 ---- batch: 020 ----
mean loss: 485.31
 ---- batch: 030 ----
mean loss: 462.42
 ---- batch: 040 ----
mean loss: 476.49
 ---- batch: 050 ----
mean loss: 477.34
 ---- batch: 060 ----
mean loss: 483.47
 ---- batch: 070 ----
mean loss: 480.91
 ---- batch: 080 ----
mean loss: 476.50
 ---- batch: 090 ----
mean loss: 474.23
train mean loss: 479.88
epoch train time: 0:00:02.350548
elapsed time: 0:00:56.673459
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 21:54:53.568844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.96
 ---- batch: 020 ----
mean loss: 486.18
 ---- batch: 030 ----
mean loss: 460.47
 ---- batch: 040 ----
mean loss: 472.42
 ---- batch: 050 ----
mean loss: 479.23
 ---- batch: 060 ----
mean loss: 461.64
 ---- batch: 070 ----
mean loss: 472.69
 ---- batch: 080 ----
mean loss: 467.33
 ---- batch: 090 ----
mean loss: 458.45
train mean loss: 470.81
epoch train time: 0:00:02.363445
elapsed time: 0:00:59.037089
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 21:54:55.932504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.17
 ---- batch: 020 ----
mean loss: 440.23
 ---- batch: 030 ----
mean loss: 450.29
 ---- batch: 040 ----
mean loss: 450.28
 ---- batch: 050 ----
mean loss: 456.52
 ---- batch: 060 ----
mean loss: 446.22
 ---- batch: 070 ----
mean loss: 453.90
 ---- batch: 080 ----
mean loss: 447.88
 ---- batch: 090 ----
mean loss: 450.01
train mean loss: 451.16
epoch train time: 0:00:02.344061
elapsed time: 0:01:01.381366
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 21:54:58.276754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.50
 ---- batch: 020 ----
mean loss: 443.02
 ---- batch: 030 ----
mean loss: 438.76
 ---- batch: 040 ----
mean loss: 439.64
 ---- batch: 050 ----
mean loss: 448.25
 ---- batch: 060 ----
mean loss: 466.86
 ---- batch: 070 ----
mean loss: 443.24
 ---- batch: 080 ----
mean loss: 445.85
 ---- batch: 090 ----
mean loss: 441.92
train mean loss: 444.42
epoch train time: 0:00:02.350400
elapsed time: 0:01:03.731945
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 21:55:00.627327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.66
 ---- batch: 020 ----
mean loss: 439.76
 ---- batch: 030 ----
mean loss: 434.51
 ---- batch: 040 ----
mean loss: 446.65
 ---- batch: 050 ----
mean loss: 438.26
 ---- batch: 060 ----
mean loss: 420.50
 ---- batch: 070 ----
mean loss: 419.50
 ---- batch: 080 ----
mean loss: 426.63
 ---- batch: 090 ----
mean loss: 436.16
train mean loss: 432.55
epoch train time: 0:00:02.355286
elapsed time: 0:01:06.087423
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 21:55:02.982809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.67
 ---- batch: 020 ----
mean loss: 433.01
 ---- batch: 030 ----
mean loss: 435.74
 ---- batch: 040 ----
mean loss: 431.42
 ---- batch: 050 ----
mean loss: 431.38
 ---- batch: 060 ----
mean loss: 429.20
 ---- batch: 070 ----
mean loss: 417.15
 ---- batch: 080 ----
mean loss: 430.07
 ---- batch: 090 ----
mean loss: 429.87
train mean loss: 429.68
epoch train time: 0:00:02.339770
elapsed time: 0:01:08.427378
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 21:55:05.322762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.95
 ---- batch: 020 ----
mean loss: 436.92
 ---- batch: 030 ----
mean loss: 422.27
 ---- batch: 040 ----
mean loss: 425.32
 ---- batch: 050 ----
mean loss: 428.04
 ---- batch: 060 ----
mean loss: 420.24
 ---- batch: 070 ----
mean loss: 423.84
 ---- batch: 080 ----
mean loss: 417.86
 ---- batch: 090 ----
mean loss: 412.58
train mean loss: 425.11
epoch train time: 0:00:02.341315
elapsed time: 0:01:10.768870
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 21:55:07.664259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.56
 ---- batch: 020 ----
mean loss: 437.18
 ---- batch: 030 ----
mean loss: 441.08
 ---- batch: 040 ----
mean loss: 457.19
 ---- batch: 050 ----
mean loss: 439.23
 ---- batch: 060 ----
mean loss: 458.73
 ---- batch: 070 ----
mean loss: 437.70
 ---- batch: 080 ----
mean loss: 446.55
 ---- batch: 090 ----
mean loss: 427.86
train mean loss: 440.61
epoch train time: 0:00:02.349886
elapsed time: 0:01:13.118949
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 21:55:10.014337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.27
 ---- batch: 020 ----
mean loss: 413.39
 ---- batch: 030 ----
mean loss: 417.28
 ---- batch: 040 ----
mean loss: 409.87
 ---- batch: 050 ----
mean loss: 419.33
 ---- batch: 060 ----
mean loss: 431.51
 ---- batch: 070 ----
mean loss: 420.26
 ---- batch: 080 ----
mean loss: 428.00
 ---- batch: 090 ----
mean loss: 427.37
train mean loss: 422.16
epoch train time: 0:00:02.342231
elapsed time: 0:01:15.461350
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 21:55:12.356742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.36
 ---- batch: 020 ----
mean loss: 418.09
 ---- batch: 030 ----
mean loss: 426.14
 ---- batch: 040 ----
mean loss: 409.95
 ---- batch: 050 ----
mean loss: 429.47
 ---- batch: 060 ----
mean loss: 410.05
 ---- batch: 070 ----
mean loss: 418.94
 ---- batch: 080 ----
mean loss: 415.78
 ---- batch: 090 ----
mean loss: 425.50
train mean loss: 418.98
epoch train time: 0:00:02.347653
elapsed time: 0:01:17.809193
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 21:55:14.704600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.69
 ---- batch: 020 ----
mean loss: 409.64
 ---- batch: 030 ----
mean loss: 407.46
 ---- batch: 040 ----
mean loss: 438.22
 ---- batch: 050 ----
mean loss: 422.96
 ---- batch: 060 ----
mean loss: 413.94
 ---- batch: 070 ----
mean loss: 412.86
 ---- batch: 080 ----
mean loss: 414.52
 ---- batch: 090 ----
mean loss: 410.00
train mean loss: 416.82
epoch train time: 0:00:02.341319
elapsed time: 0:01:20.150728
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 21:55:17.046097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.39
 ---- batch: 020 ----
mean loss: 413.86
 ---- batch: 030 ----
mean loss: 406.68
 ---- batch: 040 ----
mean loss: 417.34
 ---- batch: 050 ----
mean loss: 399.55
 ---- batch: 060 ----
mean loss: 422.97
 ---- batch: 070 ----
mean loss: 413.43
 ---- batch: 080 ----
mean loss: 396.89
 ---- batch: 090 ----
mean loss: 425.32
train mean loss: 411.81
epoch train time: 0:00:02.341610
elapsed time: 0:01:22.492506
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 21:55:19.387889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.41
 ---- batch: 020 ----
mean loss: 417.26
 ---- batch: 030 ----
mean loss: 427.37
 ---- batch: 040 ----
mean loss: 409.49
 ---- batch: 050 ----
mean loss: 397.77
 ---- batch: 060 ----
mean loss: 408.23
 ---- batch: 070 ----
mean loss: 418.49
 ---- batch: 080 ----
mean loss: 411.28
 ---- batch: 090 ----
mean loss: 418.37
train mean loss: 412.44
epoch train time: 0:00:02.342871
elapsed time: 0:01:24.835574
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 21:55:21.730959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.35
 ---- batch: 020 ----
mean loss: 407.26
 ---- batch: 030 ----
mean loss: 402.98
 ---- batch: 040 ----
mean loss: 420.88
 ---- batch: 050 ----
mean loss: 417.49
 ---- batch: 060 ----
mean loss: 432.66
 ---- batch: 070 ----
mean loss: 406.73
 ---- batch: 080 ----
mean loss: 402.69
 ---- batch: 090 ----
mean loss: 403.71
train mean loss: 411.85
epoch train time: 0:00:02.333997
elapsed time: 0:01:27.169817
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 21:55:24.065200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.67
 ---- batch: 020 ----
mean loss: 404.26
 ---- batch: 030 ----
mean loss: 419.10
 ---- batch: 040 ----
mean loss: 397.77
 ---- batch: 050 ----
mean loss: 431.60
 ---- batch: 060 ----
mean loss: 407.55
 ---- batch: 070 ----
mean loss: 416.93
 ---- batch: 080 ----
mean loss: 411.52
 ---- batch: 090 ----
mean loss: 407.87
train mean loss: 414.59
epoch train time: 0:00:02.354030
elapsed time: 0:01:29.524028
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 21:55:26.419417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.45
 ---- batch: 020 ----
mean loss: 416.49
 ---- batch: 030 ----
mean loss: 401.64
 ---- batch: 040 ----
mean loss: 412.28
 ---- batch: 050 ----
mean loss: 413.33
 ---- batch: 060 ----
mean loss: 417.29
 ---- batch: 070 ----
mean loss: 393.39
 ---- batch: 080 ----
mean loss: 407.22
 ---- batch: 090 ----
mean loss: 417.35
train mean loss: 408.59
epoch train time: 0:00:02.338424
elapsed time: 0:01:31.862721
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 21:55:28.758100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.31
 ---- batch: 020 ----
mean loss: 407.09
 ---- batch: 030 ----
mean loss: 417.02
 ---- batch: 040 ----
mean loss: 418.35
 ---- batch: 050 ----
mean loss: 409.09
 ---- batch: 060 ----
mean loss: 413.39
 ---- batch: 070 ----
mean loss: 398.17
 ---- batch: 080 ----
mean loss: 402.68
 ---- batch: 090 ----
mean loss: 402.01
train mean loss: 408.90
epoch train time: 0:00:02.338644
elapsed time: 0:01:34.201570
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 21:55:31.096976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.92
 ---- batch: 020 ----
mean loss: 430.70
 ---- batch: 030 ----
mean loss: 405.09
 ---- batch: 040 ----
mean loss: 413.60
 ---- batch: 050 ----
mean loss: 405.69
 ---- batch: 060 ----
mean loss: 407.34
 ---- batch: 070 ----
mean loss: 402.22
 ---- batch: 080 ----
mean loss: 415.41
 ---- batch: 090 ----
mean loss: 408.86
train mean loss: 411.29
epoch train time: 0:00:02.347492
elapsed time: 0:01:36.549271
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 21:55:33.444656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.43
 ---- batch: 020 ----
mean loss: 395.43
 ---- batch: 030 ----
mean loss: 413.13
 ---- batch: 040 ----
mean loss: 407.46
 ---- batch: 050 ----
mean loss: 407.39
 ---- batch: 060 ----
mean loss: 393.52
 ---- batch: 070 ----
mean loss: 397.27
 ---- batch: 080 ----
mean loss: 399.32
 ---- batch: 090 ----
mean loss: 397.57
train mean loss: 403.55
epoch train time: 0:00:02.345519
elapsed time: 0:01:38.894980
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 21:55:35.790416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.83
 ---- batch: 020 ----
mean loss: 417.85
 ---- batch: 030 ----
mean loss: 405.12
 ---- batch: 040 ----
mean loss: 421.35
 ---- batch: 050 ----
mean loss: 401.64
 ---- batch: 060 ----
mean loss: 407.85
 ---- batch: 070 ----
mean loss: 430.86
 ---- batch: 080 ----
mean loss: 412.04
 ---- batch: 090 ----
mean loss: 407.72
train mean loss: 414.34
epoch train time: 0:00:02.346160
elapsed time: 0:01:41.241370
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 21:55:38.136751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.94
 ---- batch: 020 ----
mean loss: 409.75
 ---- batch: 030 ----
mean loss: 395.08
 ---- batch: 040 ----
mean loss: 396.34
 ---- batch: 050 ----
mean loss: 410.94
 ---- batch: 060 ----
mean loss: 433.34
 ---- batch: 070 ----
mean loss: 411.34
 ---- batch: 080 ----
mean loss: 431.26
 ---- batch: 090 ----
mean loss: 410.32
train mean loss: 413.39
epoch train time: 0:00:02.334612
elapsed time: 0:01:43.576147
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 21:55:40.471528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.73
 ---- batch: 020 ----
mean loss: 400.13
 ---- batch: 030 ----
mean loss: 402.18
 ---- batch: 040 ----
mean loss: 414.58
 ---- batch: 050 ----
mean loss: 423.10
 ---- batch: 060 ----
mean loss: 394.05
 ---- batch: 070 ----
mean loss: 396.67
 ---- batch: 080 ----
mean loss: 403.50
 ---- batch: 090 ----
mean loss: 422.01
train mean loss: 406.54
epoch train time: 0:00:02.332409
elapsed time: 0:01:45.908800
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 21:55:42.804220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.23
 ---- batch: 020 ----
mean loss: 401.47
 ---- batch: 030 ----
mean loss: 409.14
 ---- batch: 040 ----
mean loss: 385.40
 ---- batch: 050 ----
mean loss: 397.22
 ---- batch: 060 ----
mean loss: 395.04
 ---- batch: 070 ----
mean loss: 401.83
 ---- batch: 080 ----
mean loss: 413.23
 ---- batch: 090 ----
mean loss: 419.14
train mean loss: 402.74
epoch train time: 0:00:02.346724
elapsed time: 0:01:48.255742
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 21:55:45.151129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.22
 ---- batch: 020 ----
mean loss: 417.45
 ---- batch: 030 ----
mean loss: 416.16
 ---- batch: 040 ----
mean loss: 412.46
 ---- batch: 050 ----
mean loss: 394.06
 ---- batch: 060 ----
mean loss: 409.54
 ---- batch: 070 ----
mean loss: 410.81
 ---- batch: 080 ----
mean loss: 407.89
 ---- batch: 090 ----
mean loss: 405.74
train mean loss: 409.96
epoch train time: 0:00:02.342772
elapsed time: 0:01:50.598747
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 21:55:47.494132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.44
 ---- batch: 020 ----
mean loss: 403.97
 ---- batch: 030 ----
mean loss: 398.70
 ---- batch: 040 ----
mean loss: 391.85
 ---- batch: 050 ----
mean loss: 395.04
 ---- batch: 060 ----
mean loss: 394.47
 ---- batch: 070 ----
mean loss: 402.93
 ---- batch: 080 ----
mean loss: 400.63
 ---- batch: 090 ----
mean loss: 418.78
train mean loss: 404.05
epoch train time: 0:00:02.341325
elapsed time: 0:01:52.940280
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 21:55:49.835684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.64
 ---- batch: 020 ----
mean loss: 401.94
 ---- batch: 030 ----
mean loss: 405.19
 ---- batch: 040 ----
mean loss: 388.16
 ---- batch: 050 ----
mean loss: 409.15
 ---- batch: 060 ----
mean loss: 413.13
 ---- batch: 070 ----
mean loss: 402.61
 ---- batch: 080 ----
mean loss: 409.72
 ---- batch: 090 ----
mean loss: 411.70
train mean loss: 405.54
epoch train time: 0:00:02.332591
elapsed time: 0:01:55.273092
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 21:55:52.168475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.41
 ---- batch: 020 ----
mean loss: 415.83
 ---- batch: 030 ----
mean loss: 394.26
 ---- batch: 040 ----
mean loss: 406.07
 ---- batch: 050 ----
mean loss: 405.04
 ---- batch: 060 ----
mean loss: 391.31
 ---- batch: 070 ----
mean loss: 404.81
 ---- batch: 080 ----
mean loss: 407.36
 ---- batch: 090 ----
mean loss: 395.52
train mean loss: 401.41
epoch train time: 0:00:02.329016
elapsed time: 0:01:57.602315
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 21:55:54.497706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.90
 ---- batch: 020 ----
mean loss: 403.42
 ---- batch: 030 ----
mean loss: 391.53
 ---- batch: 040 ----
mean loss: 402.28
 ---- batch: 050 ----
mean loss: 395.67
 ---- batch: 060 ----
mean loss: 397.62
 ---- batch: 070 ----
mean loss: 405.52
 ---- batch: 080 ----
mean loss: 388.33
 ---- batch: 090 ----
mean loss: 408.79
train mean loss: 402.39
epoch train time: 0:00:02.334625
elapsed time: 0:01:59.937133
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 21:55:56.832519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.06
 ---- batch: 020 ----
mean loss: 391.50
 ---- batch: 030 ----
mean loss: 396.22
 ---- batch: 040 ----
mean loss: 410.83
 ---- batch: 050 ----
mean loss: 390.90
 ---- batch: 060 ----
mean loss: 416.34
 ---- batch: 070 ----
mean loss: 416.24
 ---- batch: 080 ----
mean loss: 433.07
 ---- batch: 090 ----
mean loss: 408.53
train mean loss: 406.64
epoch train time: 0:00:02.345237
elapsed time: 0:02:02.282561
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 21:55:59.177946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.79
 ---- batch: 020 ----
mean loss: 400.97
 ---- batch: 030 ----
mean loss: 392.73
 ---- batch: 040 ----
mean loss: 404.38
 ---- batch: 050 ----
mean loss: 399.26
 ---- batch: 060 ----
mean loss: 393.63
 ---- batch: 070 ----
mean loss: 407.71
 ---- batch: 080 ----
mean loss: 396.50
 ---- batch: 090 ----
mean loss: 404.40
train mean loss: 400.32
epoch train time: 0:00:02.342683
elapsed time: 0:02:04.625419
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 21:56:01.520804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.36
 ---- batch: 020 ----
mean loss: 389.78
 ---- batch: 030 ----
mean loss: 398.30
 ---- batch: 040 ----
mean loss: 404.26
 ---- batch: 050 ----
mean loss: 415.03
 ---- batch: 060 ----
mean loss: 404.23
 ---- batch: 070 ----
mean loss: 408.38
 ---- batch: 080 ----
mean loss: 411.17
 ---- batch: 090 ----
mean loss: 411.33
train mean loss: 406.52
epoch train time: 0:00:02.339521
elapsed time: 0:02:06.965132
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 21:56:03.860525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.82
 ---- batch: 020 ----
mean loss: 403.25
 ---- batch: 030 ----
mean loss: 392.65
 ---- batch: 040 ----
mean loss: 397.97
 ---- batch: 050 ----
mean loss: 402.51
 ---- batch: 060 ----
mean loss: 399.80
 ---- batch: 070 ----
mean loss: 394.56
 ---- batch: 080 ----
mean loss: 405.47
 ---- batch: 090 ----
mean loss: 404.20
train mean loss: 400.84
epoch train time: 0:00:02.335362
elapsed time: 0:02:09.300684
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 21:56:06.196087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.54
 ---- batch: 020 ----
mean loss: 393.95
 ---- batch: 030 ----
mean loss: 403.19
 ---- batch: 040 ----
mean loss: 402.10
 ---- batch: 050 ----
mean loss: 415.07
 ---- batch: 060 ----
mean loss: 408.32
 ---- batch: 070 ----
mean loss: 395.77
 ---- batch: 080 ----
mean loss: 408.95
 ---- batch: 090 ----
mean loss: 397.86
train mean loss: 401.78
epoch train time: 0:00:02.332008
elapsed time: 0:02:11.632935
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 21:56:08.528320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.73
 ---- batch: 020 ----
mean loss: 392.01
 ---- batch: 030 ----
mean loss: 389.38
 ---- batch: 040 ----
mean loss: 388.53
 ---- batch: 050 ----
mean loss: 386.91
 ---- batch: 060 ----
mean loss: 391.60
 ---- batch: 070 ----
mean loss: 405.05
 ---- batch: 080 ----
mean loss: 421.23
 ---- batch: 090 ----
mean loss: 399.14
train mean loss: 398.52
epoch train time: 0:00:02.333251
elapsed time: 0:02:13.966405
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 21:56:10.861788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.31
 ---- batch: 020 ----
mean loss: 399.74
 ---- batch: 030 ----
mean loss: 399.19
 ---- batch: 040 ----
mean loss: 392.26
 ---- batch: 050 ----
mean loss: 413.12
 ---- batch: 060 ----
mean loss: 406.36
 ---- batch: 070 ----
mean loss: 398.45
 ---- batch: 080 ----
mean loss: 416.19
 ---- batch: 090 ----
mean loss: 399.69
train mean loss: 401.90
epoch train time: 0:00:02.346223
elapsed time: 0:02:16.312819
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 21:56:13.208203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.72
 ---- batch: 020 ----
mean loss: 388.16
 ---- batch: 030 ----
mean loss: 412.80
 ---- batch: 040 ----
mean loss: 386.32
 ---- batch: 050 ----
mean loss: 416.16
 ---- batch: 060 ----
mean loss: 398.94
 ---- batch: 070 ----
mean loss: 400.76
 ---- batch: 080 ----
mean loss: 431.62
 ---- batch: 090 ----
mean loss: 397.62
train mean loss: 406.67
epoch train time: 0:00:02.330440
elapsed time: 0:02:18.643437
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 21:56:15.538830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.13
 ---- batch: 020 ----
mean loss: 411.81
 ---- batch: 030 ----
mean loss: 401.63
 ---- batch: 040 ----
mean loss: 407.68
 ---- batch: 050 ----
mean loss: 396.13
 ---- batch: 060 ----
mean loss: 392.82
 ---- batch: 070 ----
mean loss: 408.49
 ---- batch: 080 ----
mean loss: 384.99
 ---- batch: 090 ----
mean loss: 394.40
train mean loss: 401.83
epoch train time: 0:00:02.338025
elapsed time: 0:02:20.981705
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 21:56:17.877094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.61
 ---- batch: 020 ----
mean loss: 393.39
 ---- batch: 030 ----
mean loss: 406.20
 ---- batch: 040 ----
mean loss: 386.01
 ---- batch: 050 ----
mean loss: 391.09
 ---- batch: 060 ----
mean loss: 414.27
 ---- batch: 070 ----
mean loss: 393.34
 ---- batch: 080 ----
mean loss: 399.77
 ---- batch: 090 ----
mean loss: 390.73
train mean loss: 398.12
epoch train time: 0:00:02.342378
elapsed time: 0:02:23.324296
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 21:56:20.219683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.39
 ---- batch: 020 ----
mean loss: 399.17
 ---- batch: 030 ----
mean loss: 398.77
 ---- batch: 040 ----
mean loss: 391.02
 ---- batch: 050 ----
mean loss: 398.30
 ---- batch: 060 ----
mean loss: 414.45
 ---- batch: 070 ----
mean loss: 406.52
 ---- batch: 080 ----
mean loss: 410.12
 ---- batch: 090 ----
mean loss: 410.15
train mean loss: 402.83
epoch train time: 0:00:02.332722
elapsed time: 0:02:25.657194
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 21:56:22.552577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.26
 ---- batch: 020 ----
mean loss: 383.74
 ---- batch: 030 ----
mean loss: 396.15
 ---- batch: 040 ----
mean loss: 392.26
 ---- batch: 050 ----
mean loss: 389.26
 ---- batch: 060 ----
mean loss: 404.38
 ---- batch: 070 ----
mean loss: 405.69
 ---- batch: 080 ----
mean loss: 402.65
 ---- batch: 090 ----
mean loss: 390.28
train mean loss: 395.93
epoch train time: 0:00:02.330250
elapsed time: 0:02:27.987633
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 21:56:24.883032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.76
 ---- batch: 020 ----
mean loss: 375.18
 ---- batch: 030 ----
mean loss: 408.31
 ---- batch: 040 ----
mean loss: 398.94
 ---- batch: 050 ----
mean loss: 414.81
 ---- batch: 060 ----
mean loss: 411.23
 ---- batch: 070 ----
mean loss: 403.75
 ---- batch: 080 ----
mean loss: 393.99
 ---- batch: 090 ----
mean loss: 393.49
train mean loss: 399.39
epoch train time: 0:00:02.334919
elapsed time: 0:02:30.322824
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 21:56:27.218218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.42
 ---- batch: 020 ----
mean loss: 390.83
 ---- batch: 030 ----
mean loss: 394.06
 ---- batch: 040 ----
mean loss: 401.75
 ---- batch: 050 ----
mean loss: 400.71
 ---- batch: 060 ----
mean loss: 406.45
 ---- batch: 070 ----
mean loss: 380.44
 ---- batch: 080 ----
mean loss: 394.99
 ---- batch: 090 ----
mean loss: 390.77
train mean loss: 394.26
epoch train time: 0:00:02.336516
elapsed time: 0:02:32.659537
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 21:56:29.554923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.18
 ---- batch: 020 ----
mean loss: 402.49
 ---- batch: 030 ----
mean loss: 405.63
 ---- batch: 040 ----
mean loss: 405.82
 ---- batch: 050 ----
mean loss: 397.11
 ---- batch: 060 ----
mean loss: 401.98
 ---- batch: 070 ----
mean loss: 396.21
 ---- batch: 080 ----
mean loss: 387.32
 ---- batch: 090 ----
mean loss: 389.17
train mean loss: 398.16
epoch train time: 0:00:02.343887
elapsed time: 0:02:35.003618
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 21:56:31.899005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.94
 ---- batch: 020 ----
mean loss: 400.15
 ---- batch: 030 ----
mean loss: 384.35
 ---- batch: 040 ----
mean loss: 387.87
 ---- batch: 050 ----
mean loss: 403.44
 ---- batch: 060 ----
mean loss: 409.52
 ---- batch: 070 ----
mean loss: 396.92
 ---- batch: 080 ----
mean loss: 396.88
 ---- batch: 090 ----
mean loss: 405.33
train mean loss: 397.12
epoch train time: 0:00:02.333522
elapsed time: 0:02:37.337349
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 21:56:34.232760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.29
 ---- batch: 020 ----
mean loss: 380.13
 ---- batch: 030 ----
mean loss: 397.04
 ---- batch: 040 ----
mean loss: 385.71
 ---- batch: 050 ----
mean loss: 386.98
 ---- batch: 060 ----
mean loss: 383.67
 ---- batch: 070 ----
mean loss: 392.87
 ---- batch: 080 ----
mean loss: 398.42
 ---- batch: 090 ----
mean loss: 383.15
train mean loss: 389.03
epoch train time: 0:00:02.332941
elapsed time: 0:02:39.670522
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 21:56:36.565909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.13
 ---- batch: 020 ----
mean loss: 385.22
 ---- batch: 030 ----
mean loss: 391.53
 ---- batch: 040 ----
mean loss: 396.14
 ---- batch: 050 ----
mean loss: 382.40
 ---- batch: 060 ----
mean loss: 413.05
 ---- batch: 070 ----
mean loss: 418.96
 ---- batch: 080 ----
mean loss: 395.46
 ---- batch: 090 ----
mean loss: 396.92
train mean loss: 397.33
epoch train time: 0:00:02.335390
elapsed time: 0:02:42.006109
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 21:56:38.901508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.21
 ---- batch: 020 ----
mean loss: 394.62
 ---- batch: 030 ----
mean loss: 393.42
 ---- batch: 040 ----
mean loss: 389.93
 ---- batch: 050 ----
mean loss: 376.55
 ---- batch: 060 ----
mean loss: 400.06
 ---- batch: 070 ----
mean loss: 400.89
 ---- batch: 080 ----
mean loss: 377.61
 ---- batch: 090 ----
mean loss: 391.37
train mean loss: 391.01
epoch train time: 0:00:02.340760
elapsed time: 0:02:44.347067
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 21:56:41.242473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.88
 ---- batch: 020 ----
mean loss: 384.69
 ---- batch: 030 ----
mean loss: 408.25
 ---- batch: 040 ----
mean loss: 398.58
 ---- batch: 050 ----
mean loss: 384.14
 ---- batch: 060 ----
mean loss: 391.08
 ---- batch: 070 ----
mean loss: 411.07
 ---- batch: 080 ----
mean loss: 400.65
 ---- batch: 090 ----
mean loss: 391.94
train mean loss: 396.14
epoch train time: 0:00:02.329575
elapsed time: 0:02:46.676840
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 21:56:43.572224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.30
 ---- batch: 020 ----
mean loss: 380.20
 ---- batch: 030 ----
mean loss: 405.70
 ---- batch: 040 ----
mean loss: 386.25
 ---- batch: 050 ----
mean loss: 400.85
 ---- batch: 060 ----
mean loss: 397.62
 ---- batch: 070 ----
mean loss: 388.27
 ---- batch: 080 ----
mean loss: 401.38
 ---- batch: 090 ----
mean loss: 406.63
train mean loss: 393.92
epoch train time: 0:00:02.336673
elapsed time: 0:02:49.013752
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 21:56:45.909152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.21
 ---- batch: 020 ----
mean loss: 394.62
 ---- batch: 030 ----
mean loss: 408.30
 ---- batch: 040 ----
mean loss: 390.29
 ---- batch: 050 ----
mean loss: 392.27
 ---- batch: 060 ----
mean loss: 397.01
 ---- batch: 070 ----
mean loss: 382.40
 ---- batch: 080 ----
mean loss: 395.78
 ---- batch: 090 ----
mean loss: 396.62
train mean loss: 394.14
epoch train time: 0:00:02.334596
elapsed time: 0:02:51.348566
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 21:56:48.243951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.16
 ---- batch: 020 ----
mean loss: 381.02
 ---- batch: 030 ----
mean loss: 376.26
 ---- batch: 040 ----
mean loss: 398.77
 ---- batch: 050 ----
mean loss: 401.29
 ---- batch: 060 ----
mean loss: 387.64
 ---- batch: 070 ----
mean loss: 386.90
 ---- batch: 080 ----
mean loss: 390.47
 ---- batch: 090 ----
mean loss: 387.83
train mean loss: 390.13
epoch train time: 0:00:02.325799
elapsed time: 0:02:53.674586
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 21:56:50.569981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.48
 ---- batch: 020 ----
mean loss: 397.07
 ---- batch: 030 ----
mean loss: 379.56
 ---- batch: 040 ----
mean loss: 388.43
 ---- batch: 050 ----
mean loss: 383.33
 ---- batch: 060 ----
mean loss: 388.99
 ---- batch: 070 ----
mean loss: 405.46
 ---- batch: 080 ----
mean loss: 419.96
 ---- batch: 090 ----
mean loss: 408.29
train mean loss: 394.53
epoch train time: 0:00:02.326324
elapsed time: 0:02:56.001177
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 21:56:52.896579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.25
 ---- batch: 020 ----
mean loss: 400.20
 ---- batch: 030 ----
mean loss: 385.81
 ---- batch: 040 ----
mean loss: 393.06
 ---- batch: 050 ----
mean loss: 409.25
 ---- batch: 060 ----
mean loss: 409.45
 ---- batch: 070 ----
mean loss: 390.84
 ---- batch: 080 ----
mean loss: 394.74
 ---- batch: 090 ----
mean loss: 399.58
train mean loss: 399.43
epoch train time: 0:00:02.349739
elapsed time: 0:02:58.351124
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 21:56:55.246506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.79
 ---- batch: 020 ----
mean loss: 391.29
 ---- batch: 030 ----
mean loss: 388.93
 ---- batch: 040 ----
mean loss: 399.00
 ---- batch: 050 ----
mean loss: 396.72
 ---- batch: 060 ----
mean loss: 383.63
 ---- batch: 070 ----
mean loss: 396.52
 ---- batch: 080 ----
mean loss: 398.51
 ---- batch: 090 ----
mean loss: 403.74
train mean loss: 394.21
epoch train time: 0:00:02.336592
elapsed time: 0:03:00.687905
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 21:56:57.583290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.03
 ---- batch: 020 ----
mean loss: 422.33
 ---- batch: 030 ----
mean loss: 405.95
 ---- batch: 040 ----
mean loss: 389.86
 ---- batch: 050 ----
mean loss: 413.68
 ---- batch: 060 ----
mean loss: 384.31
 ---- batch: 070 ----
mean loss: 391.82
 ---- batch: 080 ----
mean loss: 386.44
 ---- batch: 090 ----
mean loss: 383.71
train mean loss: 396.97
epoch train time: 0:00:02.333657
elapsed time: 0:03:03.021752
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 21:56:59.917146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.16
 ---- batch: 020 ----
mean loss: 411.64
 ---- batch: 030 ----
mean loss: 416.17
 ---- batch: 040 ----
mean loss: 397.67
 ---- batch: 050 ----
mean loss: 385.77
 ---- batch: 060 ----
mean loss: 385.13
 ---- batch: 070 ----
mean loss: 389.64
 ---- batch: 080 ----
mean loss: 392.40
 ---- batch: 090 ----
mean loss: 396.61
train mean loss: 396.81
epoch train time: 0:00:02.343412
elapsed time: 0:03:05.365369
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 21:57:02.260777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.04
 ---- batch: 020 ----
mean loss: 387.47
 ---- batch: 030 ----
mean loss: 394.77
 ---- batch: 040 ----
mean loss: 392.65
 ---- batch: 050 ----
mean loss: 390.81
 ---- batch: 060 ----
mean loss: 389.87
 ---- batch: 070 ----
mean loss: 386.73
 ---- batch: 080 ----
mean loss: 396.41
 ---- batch: 090 ----
mean loss: 380.66
train mean loss: 389.97
epoch train time: 0:00:02.332501
elapsed time: 0:03:07.698099
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 21:57:04.593485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.50
 ---- batch: 020 ----
mean loss: 394.51
 ---- batch: 030 ----
mean loss: 409.46
 ---- batch: 040 ----
mean loss: 405.32
 ---- batch: 050 ----
mean loss: 386.97
 ---- batch: 060 ----
mean loss: 388.64
 ---- batch: 070 ----
mean loss: 380.56
 ---- batch: 080 ----
mean loss: 384.52
 ---- batch: 090 ----
mean loss: 390.67
train mean loss: 391.96
epoch train time: 0:00:02.344787
elapsed time: 0:03:10.043080
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 21:57:06.938467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.90
 ---- batch: 020 ----
mean loss: 395.53
 ---- batch: 030 ----
mean loss: 371.68
 ---- batch: 040 ----
mean loss: 388.59
 ---- batch: 050 ----
mean loss: 386.02
 ---- batch: 060 ----
mean loss: 396.91
 ---- batch: 070 ----
mean loss: 381.47
 ---- batch: 080 ----
mean loss: 392.88
 ---- batch: 090 ----
mean loss: 389.63
train mean loss: 390.22
epoch train time: 0:00:02.334632
elapsed time: 0:03:12.377897
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 21:57:09.273282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.83
 ---- batch: 020 ----
mean loss: 398.56
 ---- batch: 030 ----
mean loss: 392.17
 ---- batch: 040 ----
mean loss: 384.92
 ---- batch: 050 ----
mean loss: 399.16
 ---- batch: 060 ----
mean loss: 394.75
 ---- batch: 070 ----
mean loss: 398.44
 ---- batch: 080 ----
mean loss: 386.15
 ---- batch: 090 ----
mean loss: 390.13
train mean loss: 393.95
epoch train time: 0:00:02.333690
elapsed time: 0:03:14.711776
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 21:57:11.607161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.79
 ---- batch: 020 ----
mean loss: 386.75
 ---- batch: 030 ----
mean loss: 414.01
 ---- batch: 040 ----
mean loss: 399.90
 ---- batch: 050 ----
mean loss: 397.95
 ---- batch: 060 ----
mean loss: 382.77
 ---- batch: 070 ----
mean loss: 382.17
 ---- batch: 080 ----
mean loss: 399.03
 ---- batch: 090 ----
mean loss: 407.28
train mean loss: 395.73
epoch train time: 0:00:02.339591
elapsed time: 0:03:17.051565
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 21:57:13.946950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.18
 ---- batch: 020 ----
mean loss: 383.93
 ---- batch: 030 ----
mean loss: 398.70
 ---- batch: 040 ----
mean loss: 402.70
 ---- batch: 050 ----
mean loss: 377.44
 ---- batch: 060 ----
mean loss: 389.91
 ---- batch: 070 ----
mean loss: 404.77
 ---- batch: 080 ----
mean loss: 395.89
 ---- batch: 090 ----
mean loss: 397.70
train mean loss: 393.42
epoch train time: 0:00:02.337439
elapsed time: 0:03:19.389196
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 21:57:16.284598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.66
 ---- batch: 020 ----
mean loss: 399.27
 ---- batch: 030 ----
mean loss: 395.69
 ---- batch: 040 ----
mean loss: 392.87
 ---- batch: 050 ----
mean loss: 389.64
 ---- batch: 060 ----
mean loss: 401.52
 ---- batch: 070 ----
mean loss: 382.34
 ---- batch: 080 ----
mean loss: 385.24
 ---- batch: 090 ----
mean loss: 390.34
train mean loss: 392.66
epoch train time: 0:00:02.323850
elapsed time: 0:03:21.713270
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 21:57:18.608658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.66
 ---- batch: 020 ----
mean loss: 383.82
 ---- batch: 030 ----
mean loss: 395.54
 ---- batch: 040 ----
mean loss: 381.22
 ---- batch: 050 ----
mean loss: 395.04
 ---- batch: 060 ----
mean loss: 396.66
 ---- batch: 070 ----
mean loss: 385.21
 ---- batch: 080 ----
mean loss: 403.60
 ---- batch: 090 ----
mean loss: 397.36
train mean loss: 390.33
epoch train time: 0:00:02.347641
elapsed time: 0:03:24.061112
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 21:57:20.956493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.67
 ---- batch: 020 ----
mean loss: 392.74
 ---- batch: 030 ----
mean loss: 404.95
 ---- batch: 040 ----
mean loss: 406.07
 ---- batch: 050 ----
mean loss: 403.35
 ---- batch: 060 ----
mean loss: 397.83
 ---- batch: 070 ----
mean loss: 389.86
 ---- batch: 080 ----
mean loss: 388.77
 ---- batch: 090 ----
mean loss: 387.19
train mean loss: 397.22
epoch train time: 0:00:02.340264
elapsed time: 0:03:26.401570
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 21:57:23.296948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.05
 ---- batch: 020 ----
mean loss: 383.01
 ---- batch: 030 ----
mean loss: 387.99
 ---- batch: 040 ----
mean loss: 386.45
 ---- batch: 050 ----
mean loss: 393.89
 ---- batch: 060 ----
mean loss: 385.73
 ---- batch: 070 ----
mean loss: 393.16
 ---- batch: 080 ----
mean loss: 382.35
 ---- batch: 090 ----
mean loss: 391.97
train mean loss: 388.33
epoch train time: 0:00:02.336897
elapsed time: 0:03:28.738647
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 21:57:25.634032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.24
 ---- batch: 020 ----
mean loss: 390.85
 ---- batch: 030 ----
mean loss: 401.34
 ---- batch: 040 ----
mean loss: 400.33
 ---- batch: 050 ----
mean loss: 390.03
 ---- batch: 060 ----
mean loss: 388.66
 ---- batch: 070 ----
mean loss: 398.99
 ---- batch: 080 ----
mean loss: 391.92
 ---- batch: 090 ----
mean loss: 396.26
train mean loss: 393.17
epoch train time: 0:00:02.342116
elapsed time: 0:03:31.080945
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 21:57:27.976340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.37
 ---- batch: 020 ----
mean loss: 384.65
 ---- batch: 030 ----
mean loss: 378.16
 ---- batch: 040 ----
mean loss: 383.44
 ---- batch: 050 ----
mean loss: 395.69
 ---- batch: 060 ----
mean loss: 386.48
 ---- batch: 070 ----
mean loss: 377.15
 ---- batch: 080 ----
mean loss: 386.12
 ---- batch: 090 ----
mean loss: 387.21
train mean loss: 389.27
epoch train time: 0:00:02.329404
elapsed time: 0:03:33.410540
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 21:57:30.305929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.67
 ---- batch: 020 ----
mean loss: 388.28
 ---- batch: 030 ----
mean loss: 388.38
 ---- batch: 040 ----
mean loss: 398.09
 ---- batch: 050 ----
mean loss: 384.21
 ---- batch: 060 ----
mean loss: 397.33
 ---- batch: 070 ----
mean loss: 399.12
 ---- batch: 080 ----
mean loss: 393.02
 ---- batch: 090 ----
mean loss: 386.14
train mean loss: 390.48
epoch train time: 0:00:02.321839
elapsed time: 0:03:35.732581
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 21:57:32.627967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.28
 ---- batch: 020 ----
mean loss: 383.34
 ---- batch: 030 ----
mean loss: 403.99
 ---- batch: 040 ----
mean loss: 407.40
 ---- batch: 050 ----
mean loss: 378.64
 ---- batch: 060 ----
mean loss: 385.65
 ---- batch: 070 ----
mean loss: 393.03
 ---- batch: 080 ----
mean loss: 370.71
 ---- batch: 090 ----
mean loss: 391.00
train mean loss: 388.00
epoch train time: 0:00:02.333997
elapsed time: 0:03:38.066783
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 21:57:34.962199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.12
 ---- batch: 020 ----
mean loss: 396.42
 ---- batch: 030 ----
mean loss: 392.90
 ---- batch: 040 ----
mean loss: 393.29
 ---- batch: 050 ----
mean loss: 391.17
 ---- batch: 060 ----
mean loss: 391.95
 ---- batch: 070 ----
mean loss: 383.11
 ---- batch: 080 ----
mean loss: 399.63
 ---- batch: 090 ----
mean loss: 421.01
train mean loss: 395.32
epoch train time: 0:00:02.337010
elapsed time: 0:03:40.404024
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 21:57:37.299412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.78
 ---- batch: 020 ----
mean loss: 387.39
 ---- batch: 030 ----
mean loss: 389.19
 ---- batch: 040 ----
mean loss: 390.90
 ---- batch: 050 ----
mean loss: 380.70
 ---- batch: 060 ----
mean loss: 387.71
 ---- batch: 070 ----
mean loss: 388.71
 ---- batch: 080 ----
mean loss: 385.78
 ---- batch: 090 ----
mean loss: 390.79
train mean loss: 389.85
epoch train time: 0:00:02.328542
elapsed time: 0:03:42.732741
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 21:57:39.628123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.39
 ---- batch: 020 ----
mean loss: 392.91
 ---- batch: 030 ----
mean loss: 398.54
 ---- batch: 040 ----
mean loss: 376.05
 ---- batch: 050 ----
mean loss: 394.65
 ---- batch: 060 ----
mean loss: 378.95
 ---- batch: 070 ----
mean loss: 373.57
 ---- batch: 080 ----
mean loss: 399.57
 ---- batch: 090 ----
mean loss: 407.03
train mean loss: 390.60
epoch train time: 0:00:02.340907
elapsed time: 0:03:45.073824
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 21:57:41.969206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.98
 ---- batch: 020 ----
mean loss: 397.39
 ---- batch: 030 ----
mean loss: 386.32
 ---- batch: 040 ----
mean loss: 395.14
 ---- batch: 050 ----
mean loss: 383.69
 ---- batch: 060 ----
mean loss: 389.00
 ---- batch: 070 ----
mean loss: 385.92
 ---- batch: 080 ----
mean loss: 400.05
 ---- batch: 090 ----
mean loss: 387.04
train mean loss: 392.00
epoch train time: 0:00:02.326860
elapsed time: 0:03:47.400863
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 21:57:44.296249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.05
 ---- batch: 020 ----
mean loss: 390.56
 ---- batch: 030 ----
mean loss: 385.35
 ---- batch: 040 ----
mean loss: 388.01
 ---- batch: 050 ----
mean loss: 394.12
 ---- batch: 060 ----
mean loss: 393.94
 ---- batch: 070 ----
mean loss: 399.03
 ---- batch: 080 ----
mean loss: 379.24
 ---- batch: 090 ----
mean loss: 395.44
train mean loss: 393.10
epoch train time: 0:00:02.331722
elapsed time: 0:03:49.732780
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 21:57:46.628164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.65
 ---- batch: 020 ----
mean loss: 379.12
 ---- batch: 030 ----
mean loss: 412.61
 ---- batch: 040 ----
mean loss: 392.14
 ---- batch: 050 ----
mean loss: 390.65
 ---- batch: 060 ----
mean loss: 397.91
 ---- batch: 070 ----
mean loss: 383.74
 ---- batch: 080 ----
mean loss: 389.50
 ---- batch: 090 ----
mean loss: 392.51
train mean loss: 392.15
epoch train time: 0:00:02.336068
elapsed time: 0:03:52.069037
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 21:57:48.964434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.74
 ---- batch: 020 ----
mean loss: 395.63
 ---- batch: 030 ----
mean loss: 396.00
 ---- batch: 040 ----
mean loss: 393.96
 ---- batch: 050 ----
mean loss: 379.16
 ---- batch: 060 ----
mean loss: 365.23
 ---- batch: 070 ----
mean loss: 377.99
 ---- batch: 080 ----
mean loss: 382.71
 ---- batch: 090 ----
mean loss: 393.03
train mean loss: 385.96
epoch train time: 0:00:02.339740
elapsed time: 0:03:54.408964
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 21:57:51.304371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.78
 ---- batch: 020 ----
mean loss: 371.57
 ---- batch: 030 ----
mean loss: 387.54
 ---- batch: 040 ----
mean loss: 390.83
 ---- batch: 050 ----
mean loss: 383.40
 ---- batch: 060 ----
mean loss: 395.97
 ---- batch: 070 ----
mean loss: 388.30
 ---- batch: 080 ----
mean loss: 388.51
 ---- batch: 090 ----
mean loss: 379.70
train mean loss: 384.40
epoch train time: 0:00:02.325519
elapsed time: 0:03:56.734690
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 21:57:53.630106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.24
 ---- batch: 020 ----
mean loss: 389.31
 ---- batch: 030 ----
mean loss: 399.04
 ---- batch: 040 ----
mean loss: 376.26
 ---- batch: 050 ----
mean loss: 374.66
 ---- batch: 060 ----
mean loss: 395.43
 ---- batch: 070 ----
mean loss: 390.57
 ---- batch: 080 ----
mean loss: 385.62
 ---- batch: 090 ----
mean loss: 384.44
train mean loss: 387.93
epoch train time: 0:00:02.334394
elapsed time: 0:03:59.069353
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 21:57:55.964786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.24
 ---- batch: 020 ----
mean loss: 381.90
 ---- batch: 030 ----
mean loss: 385.83
 ---- batch: 040 ----
mean loss: 390.94
 ---- batch: 050 ----
mean loss: 379.55
 ---- batch: 060 ----
mean loss: 392.05
 ---- batch: 070 ----
mean loss: 378.08
 ---- batch: 080 ----
mean loss: 380.54
 ---- batch: 090 ----
mean loss: 382.65
train mean loss: 388.18
epoch train time: 0:00:02.337899
elapsed time: 0:04:01.407483
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 21:57:58.302869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.38
 ---- batch: 020 ----
mean loss: 403.68
 ---- batch: 030 ----
mean loss: 392.66
 ---- batch: 040 ----
mean loss: 395.92
 ---- batch: 050 ----
mean loss: 386.40
 ---- batch: 060 ----
mean loss: 380.12
 ---- batch: 070 ----
mean loss: 383.95
 ---- batch: 080 ----
mean loss: 394.67
 ---- batch: 090 ----
mean loss: 399.35
train mean loss: 392.30
epoch train time: 0:00:02.331952
elapsed time: 0:04:03.739622
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 21:58:00.635006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.78
 ---- batch: 020 ----
mean loss: 388.13
 ---- batch: 030 ----
mean loss: 386.57
 ---- batch: 040 ----
mean loss: 390.45
 ---- batch: 050 ----
mean loss: 420.83
 ---- batch: 060 ----
mean loss: 419.80
 ---- batch: 070 ----
mean loss: 401.81
 ---- batch: 080 ----
mean loss: 403.87
 ---- batch: 090 ----
mean loss: 387.00
train mean loss: 396.99
epoch train time: 0:00:02.336949
elapsed time: 0:04:06.076979
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 21:58:02.972390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.09
 ---- batch: 020 ----
mean loss: 399.45
 ---- batch: 030 ----
mean loss: 392.58
 ---- batch: 040 ----
mean loss: 400.41
 ---- batch: 050 ----
mean loss: 402.49
 ---- batch: 060 ----
mean loss: 414.72
 ---- batch: 070 ----
mean loss: 413.51
 ---- batch: 080 ----
mean loss: 395.11
 ---- batch: 090 ----
mean loss: 375.10
train mean loss: 397.09
epoch train time: 0:00:02.332986
elapsed time: 0:04:08.410227
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 21:58:05.305629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.71
 ---- batch: 020 ----
mean loss: 396.26
 ---- batch: 030 ----
mean loss: 397.72
 ---- batch: 040 ----
mean loss: 399.04
 ---- batch: 050 ----
mean loss: 394.23
 ---- batch: 060 ----
mean loss: 376.70
 ---- batch: 070 ----
mean loss: 381.19
 ---- batch: 080 ----
mean loss: 391.49
 ---- batch: 090 ----
mean loss: 387.15
train mean loss: 388.37
epoch train time: 0:00:02.332189
elapsed time: 0:04:10.742614
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 21:58:07.638000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.15
 ---- batch: 020 ----
mean loss: 396.50
 ---- batch: 030 ----
mean loss: 383.44
 ---- batch: 040 ----
mean loss: 395.59
 ---- batch: 050 ----
mean loss: 390.83
 ---- batch: 060 ----
mean loss: 395.34
 ---- batch: 070 ----
mean loss: 392.67
 ---- batch: 080 ----
mean loss: 383.81
 ---- batch: 090 ----
mean loss: 380.57
train mean loss: 391.94
epoch train time: 0:00:02.336116
elapsed time: 0:04:13.078906
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 21:58:09.974321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.98
 ---- batch: 020 ----
mean loss: 395.14
 ---- batch: 030 ----
mean loss: 387.37
 ---- batch: 040 ----
mean loss: 387.40
 ---- batch: 050 ----
mean loss: 396.88
 ---- batch: 060 ----
mean loss: 403.67
 ---- batch: 070 ----
mean loss: 387.26
 ---- batch: 080 ----
mean loss: 391.87
 ---- batch: 090 ----
mean loss: 395.36
train mean loss: 392.51
epoch train time: 0:00:02.338063
elapsed time: 0:04:15.417190
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 21:58:12.312607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.98
 ---- batch: 020 ----
mean loss: 401.13
 ---- batch: 030 ----
mean loss: 400.47
 ---- batch: 040 ----
mean loss: 387.47
 ---- batch: 050 ----
mean loss: 412.76
 ---- batch: 060 ----
mean loss: 398.33
 ---- batch: 070 ----
mean loss: 382.47
 ---- batch: 080 ----
mean loss: 386.43
 ---- batch: 090 ----
mean loss: 390.22
train mean loss: 397.74
epoch train time: 0:00:02.333252
elapsed time: 0:04:17.750688
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 21:58:14.646092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.39
 ---- batch: 020 ----
mean loss: 386.68
 ---- batch: 030 ----
mean loss: 389.85
 ---- batch: 040 ----
mean loss: 391.38
 ---- batch: 050 ----
mean loss: 389.06
 ---- batch: 060 ----
mean loss: 384.77
 ---- batch: 070 ----
mean loss: 387.69
 ---- batch: 080 ----
mean loss: 378.72
 ---- batch: 090 ----
mean loss: 392.98
train mean loss: 391.20
epoch train time: 0:00:02.333814
elapsed time: 0:04:20.084695
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 21:58:16.980095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.93
 ---- batch: 020 ----
mean loss: 393.73
 ---- batch: 030 ----
mean loss: 378.69
 ---- batch: 040 ----
mean loss: 378.14
 ---- batch: 050 ----
mean loss: 378.73
 ---- batch: 060 ----
mean loss: 377.10
 ---- batch: 070 ----
mean loss: 398.69
 ---- batch: 080 ----
mean loss: 394.06
 ---- batch: 090 ----
mean loss: 394.85
train mean loss: 388.63
epoch train time: 0:00:02.331641
elapsed time: 0:04:22.416530
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 21:58:19.311915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.67
 ---- batch: 020 ----
mean loss: 388.65
 ---- batch: 030 ----
mean loss: 376.93
 ---- batch: 040 ----
mean loss: 394.59
 ---- batch: 050 ----
mean loss: 386.80
 ---- batch: 060 ----
mean loss: 385.84
 ---- batch: 070 ----
mean loss: 388.53
 ---- batch: 080 ----
mean loss: 387.48
 ---- batch: 090 ----
mean loss: 383.48
train mean loss: 387.73
epoch train time: 0:00:02.338943
elapsed time: 0:04:24.755660
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 21:58:21.651045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.64
 ---- batch: 020 ----
mean loss: 397.17
 ---- batch: 030 ----
mean loss: 380.27
 ---- batch: 040 ----
mean loss: 390.25
 ---- batch: 050 ----
mean loss: 386.57
 ---- batch: 060 ----
mean loss: 390.33
 ---- batch: 070 ----
mean loss: 384.17
 ---- batch: 080 ----
mean loss: 387.29
 ---- batch: 090 ----
mean loss: 403.92
train mean loss: 388.81
epoch train time: 0:00:02.339656
elapsed time: 0:04:27.095493
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 21:58:23.990884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.51
 ---- batch: 020 ----
mean loss: 375.93
 ---- batch: 030 ----
mean loss: 378.97
 ---- batch: 040 ----
mean loss: 402.02
 ---- batch: 050 ----
mean loss: 413.48
 ---- batch: 060 ----
mean loss: 392.89
 ---- batch: 070 ----
mean loss: 382.21
 ---- batch: 080 ----
mean loss: 376.97
 ---- batch: 090 ----
mean loss: 382.23
train mean loss: 388.93
epoch train time: 0:00:02.331762
elapsed time: 0:04:29.427434
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 21:58:26.322816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.27
 ---- batch: 020 ----
mean loss: 377.35
 ---- batch: 030 ----
mean loss: 382.72
 ---- batch: 040 ----
mean loss: 385.72
 ---- batch: 050 ----
mean loss: 393.99
 ---- batch: 060 ----
mean loss: 376.54
 ---- batch: 070 ----
mean loss: 381.33
 ---- batch: 080 ----
mean loss: 382.82
 ---- batch: 090 ----
mean loss: 390.65
train mean loss: 386.03
epoch train time: 0:00:02.331959
elapsed time: 0:04:31.759586
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 21:58:28.654996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.27
 ---- batch: 020 ----
mean loss: 377.36
 ---- batch: 030 ----
mean loss: 389.53
 ---- batch: 040 ----
mean loss: 381.37
 ---- batch: 050 ----
mean loss: 390.25
 ---- batch: 060 ----
mean loss: 402.01
 ---- batch: 070 ----
mean loss: 381.68
 ---- batch: 080 ----
mean loss: 378.50
 ---- batch: 090 ----
mean loss: 396.78
train mean loss: 388.33
epoch train time: 0:00:02.340904
elapsed time: 0:04:34.100701
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 21:58:30.996084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.10
 ---- batch: 020 ----
mean loss: 385.55
 ---- batch: 030 ----
mean loss: 385.93
 ---- batch: 040 ----
mean loss: 387.44
 ---- batch: 050 ----
mean loss: 386.77
 ---- batch: 060 ----
mean loss: 402.40
 ---- batch: 070 ----
mean loss: 385.03
 ---- batch: 080 ----
mean loss: 390.62
 ---- batch: 090 ----
mean loss: 378.72
train mean loss: 388.40
epoch train time: 0:00:02.342124
elapsed time: 0:04:36.443005
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 21:58:33.338403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.05
 ---- batch: 020 ----
mean loss: 395.92
 ---- batch: 030 ----
mean loss: 400.59
 ---- batch: 040 ----
mean loss: 391.28
 ---- batch: 050 ----
mean loss: 382.95
 ---- batch: 060 ----
mean loss: 390.65
 ---- batch: 070 ----
mean loss: 375.32
 ---- batch: 080 ----
mean loss: 385.92
 ---- batch: 090 ----
mean loss: 391.47
train mean loss: 389.86
epoch train time: 0:00:02.332246
elapsed time: 0:04:38.775453
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 21:58:35.670855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.40
 ---- batch: 020 ----
mean loss: 393.62
 ---- batch: 030 ----
mean loss: 402.37
 ---- batch: 040 ----
mean loss: 385.50
 ---- batch: 050 ----
mean loss: 366.18
 ---- batch: 060 ----
mean loss: 381.74
 ---- batch: 070 ----
mean loss: 387.06
 ---- batch: 080 ----
mean loss: 388.76
 ---- batch: 090 ----
mean loss: 396.18
train mean loss: 389.68
epoch train time: 0:00:02.337169
elapsed time: 0:04:41.112825
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 21:58:38.008210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.51
 ---- batch: 020 ----
mean loss: 383.40
 ---- batch: 030 ----
mean loss: 392.31
 ---- batch: 040 ----
mean loss: 380.32
 ---- batch: 050 ----
mean loss: 391.77
 ---- batch: 060 ----
mean loss: 389.49
 ---- batch: 070 ----
mean loss: 385.03
 ---- batch: 080 ----
mean loss: 381.65
 ---- batch: 090 ----
mean loss: 382.63
train mean loss: 385.88
epoch train time: 0:00:02.337628
elapsed time: 0:04:43.450666
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 21:58:40.346050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.01
 ---- batch: 020 ----
mean loss: 387.04
 ---- batch: 030 ----
mean loss: 395.36
 ---- batch: 040 ----
mean loss: 384.46
 ---- batch: 050 ----
mean loss: 373.76
 ---- batch: 060 ----
mean loss: 381.20
 ---- batch: 070 ----
mean loss: 379.71
 ---- batch: 080 ----
mean loss: 385.04
 ---- batch: 090 ----
mean loss: 377.03
train mean loss: 380.09
epoch train time: 0:00:02.335435
elapsed time: 0:04:45.786327
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 21:58:42.681722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.15
 ---- batch: 020 ----
mean loss: 377.42
 ---- batch: 030 ----
mean loss: 370.15
 ---- batch: 040 ----
mean loss: 388.59
 ---- batch: 050 ----
mean loss: 386.00
 ---- batch: 060 ----
mean loss: 387.53
 ---- batch: 070 ----
mean loss: 386.47
 ---- batch: 080 ----
mean loss: 382.76
 ---- batch: 090 ----
mean loss: 414.44
train mean loss: 389.41
epoch train time: 0:00:02.335159
elapsed time: 0:04:48.121711
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 21:58:45.017115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.48
 ---- batch: 020 ----
mean loss: 381.67
 ---- batch: 030 ----
mean loss: 392.81
 ---- batch: 040 ----
mean loss: 381.63
 ---- batch: 050 ----
mean loss: 383.05
 ---- batch: 060 ----
mean loss: 383.03
 ---- batch: 070 ----
mean loss: 391.56
 ---- batch: 080 ----
mean loss: 376.67
 ---- batch: 090 ----
mean loss: 389.24
train mean loss: 387.99
epoch train time: 0:00:02.340090
elapsed time: 0:04:50.462004
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 21:58:47.357401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.95
 ---- batch: 020 ----
mean loss: 389.80
 ---- batch: 030 ----
mean loss: 391.24
 ---- batch: 040 ----
mean loss: 386.98
 ---- batch: 050 ----
mean loss: 384.15
 ---- batch: 060 ----
mean loss: 387.77
 ---- batch: 070 ----
mean loss: 388.36
 ---- batch: 080 ----
mean loss: 394.91
 ---- batch: 090 ----
mean loss: 391.38
train mean loss: 389.08
epoch train time: 0:00:02.330780
elapsed time: 0:04:52.792971
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 21:58:49.688356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.05
 ---- batch: 020 ----
mean loss: 384.66
 ---- batch: 030 ----
mean loss: 380.15
 ---- batch: 040 ----
mean loss: 410.67
 ---- batch: 050 ----
mean loss: 397.84
 ---- batch: 060 ----
mean loss: 402.12
 ---- batch: 070 ----
mean loss: 395.28
 ---- batch: 080 ----
mean loss: 382.06
 ---- batch: 090 ----
mean loss: 394.97
train mean loss: 390.75
epoch train time: 0:00:02.339701
elapsed time: 0:04:55.132846
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 21:58:52.028228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.77
 ---- batch: 020 ----
mean loss: 386.43
 ---- batch: 030 ----
mean loss: 377.70
 ---- batch: 040 ----
mean loss: 393.87
 ---- batch: 050 ----
mean loss: 373.12
 ---- batch: 060 ----
mean loss: 382.40
 ---- batch: 070 ----
mean loss: 386.75
 ---- batch: 080 ----
mean loss: 373.19
 ---- batch: 090 ----
mean loss: 376.02
train mean loss: 383.28
epoch train time: 0:00:02.329187
elapsed time: 0:04:57.462228
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 21:58:54.357638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.44
 ---- batch: 020 ----
mean loss: 370.36
 ---- batch: 030 ----
mean loss: 374.35
 ---- batch: 040 ----
mean loss: 396.20
 ---- batch: 050 ----
mean loss: 389.17
 ---- batch: 060 ----
mean loss: 388.10
 ---- batch: 070 ----
mean loss: 385.10
 ---- batch: 080 ----
mean loss: 378.12
 ---- batch: 090 ----
mean loss: 376.93
train mean loss: 383.36
epoch train time: 0:00:02.334721
elapsed time: 0:04:59.797148
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 21:58:56.692531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.46
 ---- batch: 020 ----
mean loss: 386.05
 ---- batch: 030 ----
mean loss: 388.05
 ---- batch: 040 ----
mean loss: 393.71
 ---- batch: 050 ----
mean loss: 372.97
 ---- batch: 060 ----
mean loss: 380.29
 ---- batch: 070 ----
mean loss: 391.34
 ---- batch: 080 ----
mean loss: 375.82
 ---- batch: 090 ----
mean loss: 386.01
train mean loss: 384.56
epoch train time: 0:00:02.334599
elapsed time: 0:05:02.131928
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 21:58:59.027312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.56
 ---- batch: 020 ----
mean loss: 382.05
 ---- batch: 030 ----
mean loss: 381.62
 ---- batch: 040 ----
mean loss: 369.33
 ---- batch: 050 ----
mean loss: 397.04
 ---- batch: 060 ----
mean loss: 376.22
 ---- batch: 070 ----
mean loss: 395.59
 ---- batch: 080 ----
mean loss: 372.31
 ---- batch: 090 ----
mean loss: 381.67
train mean loss: 383.97
epoch train time: 0:00:02.328196
elapsed time: 0:05:04.460303
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 21:59:01.355700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.66
 ---- batch: 020 ----
mean loss: 373.34
 ---- batch: 030 ----
mean loss: 369.58
 ---- batch: 040 ----
mean loss: 385.51
 ---- batch: 050 ----
mean loss: 389.07
 ---- batch: 060 ----
mean loss: 416.97
 ---- batch: 070 ----
mean loss: 415.08
 ---- batch: 080 ----
mean loss: 383.64
 ---- batch: 090 ----
mean loss: 398.10
train mean loss: 389.66
epoch train time: 0:00:02.332365
elapsed time: 0:05:06.792880
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 21:59:03.688287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.62
 ---- batch: 020 ----
mean loss: 387.03
 ---- batch: 030 ----
mean loss: 378.81
 ---- batch: 040 ----
mean loss: 385.64
 ---- batch: 050 ----
mean loss: 386.68
 ---- batch: 060 ----
mean loss: 380.51
 ---- batch: 070 ----
mean loss: 396.21
 ---- batch: 080 ----
mean loss: 393.23
 ---- batch: 090 ----
mean loss: 396.37
train mean loss: 389.16
epoch train time: 0:00:02.345642
elapsed time: 0:05:09.138726
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 21:59:06.034110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.53
 ---- batch: 020 ----
mean loss: 411.77
 ---- batch: 030 ----
mean loss: 399.78
 ---- batch: 040 ----
mean loss: 382.91
 ---- batch: 050 ----
mean loss: 382.22
 ---- batch: 060 ----
mean loss: 372.66
 ---- batch: 070 ----
mean loss: 372.41
 ---- batch: 080 ----
mean loss: 384.18
 ---- batch: 090 ----
mean loss: 378.47
train mean loss: 390.70
epoch train time: 0:00:02.339616
elapsed time: 0:05:11.478559
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 21:59:08.373951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.35
 ---- batch: 020 ----
mean loss: 368.28
 ---- batch: 030 ----
mean loss: 379.35
 ---- batch: 040 ----
mean loss: 391.39
 ---- batch: 050 ----
mean loss: 380.75
 ---- batch: 060 ----
mean loss: 391.33
 ---- batch: 070 ----
mean loss: 392.17
 ---- batch: 080 ----
mean loss: 382.96
 ---- batch: 090 ----
mean loss: 378.01
train mean loss: 384.93
epoch train time: 0:00:02.331962
elapsed time: 0:05:13.810713
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 21:59:10.706099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.65
 ---- batch: 020 ----
mean loss: 383.89
 ---- batch: 030 ----
mean loss: 385.98
 ---- batch: 040 ----
mean loss: 404.13
 ---- batch: 050 ----
mean loss: 376.94
 ---- batch: 060 ----
mean loss: 394.17
 ---- batch: 070 ----
mean loss: 378.56
 ---- batch: 080 ----
mean loss: 387.46
 ---- batch: 090 ----
mean loss: 376.10
train mean loss: 384.62
epoch train time: 0:00:02.340151
elapsed time: 0:05:16.151047
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 21:59:13.046432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.89
 ---- batch: 020 ----
mean loss: 382.98
 ---- batch: 030 ----
mean loss: 382.24
 ---- batch: 040 ----
mean loss: 384.34
 ---- batch: 050 ----
mean loss: 390.78
 ---- batch: 060 ----
mean loss: 390.04
 ---- batch: 070 ----
mean loss: 380.22
 ---- batch: 080 ----
mean loss: 398.97
 ---- batch: 090 ----
mean loss: 419.46
train mean loss: 389.10
epoch train time: 0:00:02.329966
elapsed time: 0:05:18.481188
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 21:59:15.376605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.58
 ---- batch: 020 ----
mean loss: 373.32
 ---- batch: 030 ----
mean loss: 380.01
 ---- batch: 040 ----
mean loss: 379.75
 ---- batch: 050 ----
mean loss: 381.72
 ---- batch: 060 ----
mean loss: 380.11
 ---- batch: 070 ----
mean loss: 371.72
 ---- batch: 080 ----
mean loss: 375.30
 ---- batch: 090 ----
mean loss: 372.66
train mean loss: 376.65
epoch train time: 0:00:02.338602
elapsed time: 0:05:20.820002
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 21:59:17.715410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.39
 ---- batch: 020 ----
mean loss: 367.42
 ---- batch: 030 ----
mean loss: 378.19
 ---- batch: 040 ----
mean loss: 388.44
 ---- batch: 050 ----
mean loss: 375.25
 ---- batch: 060 ----
mean loss: 380.05
 ---- batch: 070 ----
mean loss: 388.33
 ---- batch: 080 ----
mean loss: 382.18
 ---- batch: 090 ----
mean loss: 368.41
train mean loss: 378.60
epoch train time: 0:00:02.334288
elapsed time: 0:05:23.154510
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 21:59:20.049923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.65
 ---- batch: 020 ----
mean loss: 386.17
 ---- batch: 030 ----
mean loss: 383.61
 ---- batch: 040 ----
mean loss: 376.50
 ---- batch: 050 ----
mean loss: 383.86
 ---- batch: 060 ----
mean loss: 391.36
 ---- batch: 070 ----
mean loss: 381.19
 ---- batch: 080 ----
mean loss: 393.78
 ---- batch: 090 ----
mean loss: 403.27
train mean loss: 386.02
epoch train time: 0:00:02.335558
elapsed time: 0:05:25.490304
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 21:59:22.385677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.93
 ---- batch: 020 ----
mean loss: 392.19
 ---- batch: 030 ----
mean loss: 373.61
 ---- batch: 040 ----
mean loss: 377.39
 ---- batch: 050 ----
mean loss: 386.89
 ---- batch: 060 ----
mean loss: 387.63
 ---- batch: 070 ----
mean loss: 389.11
 ---- batch: 080 ----
mean loss: 409.41
 ---- batch: 090 ----
mean loss: 377.05
train mean loss: 387.40
epoch train time: 0:00:02.329979
elapsed time: 0:05:27.820469
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 21:59:24.715860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.75
 ---- batch: 020 ----
mean loss: 381.99
 ---- batch: 030 ----
mean loss: 386.46
 ---- batch: 040 ----
mean loss: 390.18
 ---- batch: 050 ----
mean loss: 385.13
 ---- batch: 060 ----
mean loss: 379.68
 ---- batch: 070 ----
mean loss: 374.87
 ---- batch: 080 ----
mean loss: 384.85
 ---- batch: 090 ----
mean loss: 386.19
train mean loss: 382.14
epoch train time: 0:00:02.338584
elapsed time: 0:05:30.159266
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 21:59:27.054635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.15
 ---- batch: 020 ----
mean loss: 375.62
 ---- batch: 030 ----
mean loss: 377.76
 ---- batch: 040 ----
mean loss: 393.38
 ---- batch: 050 ----
mean loss: 388.86
 ---- batch: 060 ----
mean loss: 385.33
 ---- batch: 070 ----
mean loss: 371.78
 ---- batch: 080 ----
mean loss: 370.05
 ---- batch: 090 ----
mean loss: 389.07
train mean loss: 383.05
epoch train time: 0:00:02.331310
elapsed time: 0:05:32.490760
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 21:59:29.386134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.50
 ---- batch: 020 ----
mean loss: 370.35
 ---- batch: 030 ----
mean loss: 389.87
 ---- batch: 040 ----
mean loss: 386.94
 ---- batch: 050 ----
mean loss: 382.05
 ---- batch: 060 ----
mean loss: 387.30
 ---- batch: 070 ----
mean loss: 373.23
 ---- batch: 080 ----
mean loss: 406.07
 ---- batch: 090 ----
mean loss: 409.63
train mean loss: 387.35
epoch train time: 0:00:02.342891
elapsed time: 0:05:34.833825
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 21:59:31.729208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.03
 ---- batch: 020 ----
mean loss: 398.35
 ---- batch: 030 ----
mean loss: 381.88
 ---- batch: 040 ----
mean loss: 380.11
 ---- batch: 050 ----
mean loss: 376.11
 ---- batch: 060 ----
mean loss: 378.31
 ---- batch: 070 ----
mean loss: 385.06
 ---- batch: 080 ----
mean loss: 392.92
 ---- batch: 090 ----
mean loss: 400.27
train mean loss: 387.88
epoch train time: 0:00:02.340778
elapsed time: 0:05:37.174790
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 21:59:34.070178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.98
 ---- batch: 020 ----
mean loss: 390.65
 ---- batch: 030 ----
mean loss: 394.35
 ---- batch: 040 ----
mean loss: 388.13
 ---- batch: 050 ----
mean loss: 385.06
 ---- batch: 060 ----
mean loss: 381.73
 ---- batch: 070 ----
mean loss: 355.70
 ---- batch: 080 ----
mean loss: 376.82
 ---- batch: 090 ----
mean loss: 387.69
train mean loss: 382.39
epoch train time: 0:00:02.332991
elapsed time: 0:05:39.507959
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 21:59:36.403361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.05
 ---- batch: 020 ----
mean loss: 371.89
 ---- batch: 030 ----
mean loss: 382.35
 ---- batch: 040 ----
mean loss: 386.19
 ---- batch: 050 ----
mean loss: 392.97
 ---- batch: 060 ----
mean loss: 389.53
 ---- batch: 070 ----
mean loss: 391.60
 ---- batch: 080 ----
mean loss: 404.53
 ---- batch: 090 ----
mean loss: 396.77
train mean loss: 387.63
epoch train time: 0:00:02.327255
elapsed time: 0:05:41.835427
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 21:59:38.730867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.11
 ---- batch: 020 ----
mean loss: 386.94
 ---- batch: 030 ----
mean loss: 384.06
 ---- batch: 040 ----
mean loss: 375.90
 ---- batch: 050 ----
mean loss: 376.66
 ---- batch: 060 ----
mean loss: 384.54
 ---- batch: 070 ----
mean loss: 373.11
 ---- batch: 080 ----
mean loss: 383.66
 ---- batch: 090 ----
mean loss: 376.58
train mean loss: 381.67
epoch train time: 0:00:02.338134
elapsed time: 0:05:44.173811
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 21:59:41.069195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.38
 ---- batch: 020 ----
mean loss: 375.93
 ---- batch: 030 ----
mean loss: 366.76
 ---- batch: 040 ----
mean loss: 398.98
 ---- batch: 050 ----
mean loss: 392.32
 ---- batch: 060 ----
mean loss: 387.24
 ---- batch: 070 ----
mean loss: 385.77
 ---- batch: 080 ----
mean loss: 377.77
 ---- batch: 090 ----
mean loss: 390.56
train mean loss: 383.85
epoch train time: 0:00:02.337527
elapsed time: 0:05:46.511517
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 21:59:43.406911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.41
 ---- batch: 020 ----
mean loss: 385.99
 ---- batch: 030 ----
mean loss: 389.75
 ---- batch: 040 ----
mean loss: 382.12
 ---- batch: 050 ----
mean loss: 373.02
 ---- batch: 060 ----
mean loss: 392.46
 ---- batch: 070 ----
mean loss: 374.07
 ---- batch: 080 ----
mean loss: 379.69
 ---- batch: 090 ----
mean loss: 381.29
train mean loss: 382.00
epoch train time: 0:00:02.328532
elapsed time: 0:05:48.840248
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 21:59:45.735631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.29
 ---- batch: 020 ----
mean loss: 373.60
 ---- batch: 030 ----
mean loss: 366.54
 ---- batch: 040 ----
mean loss: 372.67
 ---- batch: 050 ----
mean loss: 374.77
 ---- batch: 060 ----
mean loss: 372.72
 ---- batch: 070 ----
mean loss: 385.03
 ---- batch: 080 ----
mean loss: 380.26
 ---- batch: 090 ----
mean loss: 391.49
train mean loss: 378.37
epoch train time: 0:00:02.338671
elapsed time: 0:05:51.179119
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 21:59:48.074501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.32
 ---- batch: 020 ----
mean loss: 371.30
 ---- batch: 030 ----
mean loss: 398.29
 ---- batch: 040 ----
mean loss: 390.61
 ---- batch: 050 ----
mean loss: 368.06
 ---- batch: 060 ----
mean loss: 375.54
 ---- batch: 070 ----
mean loss: 381.19
 ---- batch: 080 ----
mean loss: 380.11
 ---- batch: 090 ----
mean loss: 386.39
train mean loss: 383.44
epoch train time: 0:00:02.323885
elapsed time: 0:05:53.503207
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 21:59:50.398599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.90
 ---- batch: 020 ----
mean loss: 392.99
 ---- batch: 030 ----
mean loss: 377.90
 ---- batch: 040 ----
mean loss: 381.18
 ---- batch: 050 ----
mean loss: 390.36
 ---- batch: 060 ----
mean loss: 376.34
 ---- batch: 070 ----
mean loss: 385.71
 ---- batch: 080 ----
mean loss: 379.92
 ---- batch: 090 ----
mean loss: 367.41
train mean loss: 382.01
epoch train time: 0:00:02.330974
elapsed time: 0:05:55.834380
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 21:59:52.729765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.82
 ---- batch: 020 ----
mean loss: 375.17
 ---- batch: 030 ----
mean loss: 377.53
 ---- batch: 040 ----
mean loss: 369.53
 ---- batch: 050 ----
mean loss: 387.29
 ---- batch: 060 ----
mean loss: 386.13
 ---- batch: 070 ----
mean loss: 394.65
 ---- batch: 080 ----
mean loss: 382.30
 ---- batch: 090 ----
mean loss: 385.63
train mean loss: 380.75
epoch train time: 0:00:02.341921
elapsed time: 0:05:58.176482
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 21:59:55.071890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.89
 ---- batch: 020 ----
mean loss: 385.14
 ---- batch: 030 ----
mean loss: 397.30
 ---- batch: 040 ----
mean loss: 385.76
 ---- batch: 050 ----
mean loss: 373.12
 ---- batch: 060 ----
mean loss: 369.89
 ---- batch: 070 ----
mean loss: 378.99
 ---- batch: 080 ----
mean loss: 394.86
 ---- batch: 090 ----
mean loss: 390.66
train mean loss: 384.72
epoch train time: 0:00:02.334439
elapsed time: 0:06:00.511170
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 21:59:57.406559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.34
 ---- batch: 020 ----
mean loss: 388.74
 ---- batch: 030 ----
mean loss: 381.73
 ---- batch: 040 ----
mean loss: 377.77
 ---- batch: 050 ----
mean loss: 382.87
 ---- batch: 060 ----
mean loss: 366.15
 ---- batch: 070 ----
mean loss: 366.53
 ---- batch: 080 ----
mean loss: 375.92
 ---- batch: 090 ----
mean loss: 384.23
train mean loss: 380.31
epoch train time: 0:00:02.338384
elapsed time: 0:06:02.849752
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 21:59:59.745162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.89
 ---- batch: 020 ----
mean loss: 370.93
 ---- batch: 030 ----
mean loss: 385.57
 ---- batch: 040 ----
mean loss: 366.63
 ---- batch: 050 ----
mean loss: 374.66
 ---- batch: 060 ----
mean loss: 388.17
 ---- batch: 070 ----
mean loss: 381.77
 ---- batch: 080 ----
mean loss: 379.10
 ---- batch: 090 ----
mean loss: 375.30
train mean loss: 378.06
epoch train time: 0:00:02.333907
elapsed time: 0:06:05.183885
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 22:00:02.079269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.49
 ---- batch: 020 ----
mean loss: 381.23
 ---- batch: 030 ----
mean loss: 390.58
 ---- batch: 040 ----
mean loss: 384.22
 ---- batch: 050 ----
mean loss: 381.58
 ---- batch: 060 ----
mean loss: 389.12
 ---- batch: 070 ----
mean loss: 389.38
 ---- batch: 080 ----
mean loss: 373.39
 ---- batch: 090 ----
mean loss: 380.04
train mean loss: 383.73
epoch train time: 0:00:02.337366
elapsed time: 0:06:07.521436
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 22:00:04.416823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.47
 ---- batch: 020 ----
mean loss: 391.54
 ---- batch: 030 ----
mean loss: 368.30
 ---- batch: 040 ----
mean loss: 370.76
 ---- batch: 050 ----
mean loss: 375.68
 ---- batch: 060 ----
mean loss: 373.63
 ---- batch: 070 ----
mean loss: 381.77
 ---- batch: 080 ----
mean loss: 382.18
 ---- batch: 090 ----
mean loss: 372.53
train mean loss: 378.84
epoch train time: 0:00:02.332494
elapsed time: 0:06:09.854125
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 22:00:06.749519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.03
 ---- batch: 020 ----
mean loss: 386.82
 ---- batch: 030 ----
mean loss: 378.07
 ---- batch: 040 ----
mean loss: 386.81
 ---- batch: 050 ----
mean loss: 375.74
 ---- batch: 060 ----
mean loss: 384.92
 ---- batch: 070 ----
mean loss: 380.10
 ---- batch: 080 ----
mean loss: 388.58
 ---- batch: 090 ----
mean loss: 382.16
train mean loss: 380.70
epoch train time: 0:00:02.339359
elapsed time: 0:06:12.193717
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 22:00:09.089125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.19
 ---- batch: 020 ----
mean loss: 375.27
 ---- batch: 030 ----
mean loss: 370.03
 ---- batch: 040 ----
mean loss: 384.18
 ---- batch: 050 ----
mean loss: 382.20
 ---- batch: 060 ----
mean loss: 381.31
 ---- batch: 070 ----
mean loss: 371.44
 ---- batch: 080 ----
mean loss: 399.92
 ---- batch: 090 ----
mean loss: 392.37
train mean loss: 380.38
epoch train time: 0:00:02.329741
elapsed time: 0:06:14.523678
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 22:00:11.419064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.29
 ---- batch: 020 ----
mean loss: 386.27
 ---- batch: 030 ----
mean loss: 379.24
 ---- batch: 040 ----
mean loss: 375.54
 ---- batch: 050 ----
mean loss: 383.19
 ---- batch: 060 ----
mean loss: 369.74
 ---- batch: 070 ----
mean loss: 392.76
 ---- batch: 080 ----
mean loss: 400.68
 ---- batch: 090 ----
mean loss: 376.81
train mean loss: 383.26
epoch train time: 0:00:02.331767
elapsed time: 0:06:16.855639
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 22:00:13.751051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.13
 ---- batch: 020 ----
mean loss: 391.57
 ---- batch: 030 ----
mean loss: 386.85
 ---- batch: 040 ----
mean loss: 373.55
 ---- batch: 050 ----
mean loss: 370.57
 ---- batch: 060 ----
mean loss: 378.21
 ---- batch: 070 ----
mean loss: 359.08
 ---- batch: 080 ----
mean loss: 384.70
 ---- batch: 090 ----
mean loss: 386.16
train mean loss: 379.80
epoch train time: 0:00:02.331996
elapsed time: 0:06:19.187840
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 22:00:16.083226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.76
 ---- batch: 020 ----
mean loss: 373.85
 ---- batch: 030 ----
mean loss: 370.69
 ---- batch: 040 ----
mean loss: 368.01
 ---- batch: 050 ----
mean loss: 376.56
 ---- batch: 060 ----
mean loss: 386.87
 ---- batch: 070 ----
mean loss: 389.63
 ---- batch: 080 ----
mean loss: 394.67
 ---- batch: 090 ----
mean loss: 378.06
train mean loss: 378.58
epoch train time: 0:00:02.335222
elapsed time: 0:06:21.523273
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 22:00:18.418643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.13
 ---- batch: 020 ----
mean loss: 380.41
 ---- batch: 030 ----
mean loss: 374.34
 ---- batch: 040 ----
mean loss: 397.57
 ---- batch: 050 ----
mean loss: 381.64
 ---- batch: 060 ----
mean loss: 384.32
 ---- batch: 070 ----
mean loss: 372.81
 ---- batch: 080 ----
mean loss: 363.07
 ---- batch: 090 ----
mean loss: 375.42
train mean loss: 378.85
epoch train time: 0:00:02.333401
elapsed time: 0:06:23.856842
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 22:00:20.752224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.85
 ---- batch: 020 ----
mean loss: 381.43
 ---- batch: 030 ----
mean loss: 385.50
 ---- batch: 040 ----
mean loss: 393.84
 ---- batch: 050 ----
mean loss: 371.07
 ---- batch: 060 ----
mean loss: 385.96
 ---- batch: 070 ----
mean loss: 373.28
 ---- batch: 080 ----
mean loss: 380.51
 ---- batch: 090 ----
mean loss: 363.75
train mean loss: 377.90
epoch train time: 0:00:02.348094
elapsed time: 0:06:26.205118
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 22:00:23.100508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.56
 ---- batch: 020 ----
mean loss: 379.89
 ---- batch: 030 ----
mean loss: 378.27
 ---- batch: 040 ----
mean loss: 377.79
 ---- batch: 050 ----
mean loss: 371.92
 ---- batch: 060 ----
mean loss: 380.73
 ---- batch: 070 ----
mean loss: 368.51
 ---- batch: 080 ----
mean loss: 370.67
 ---- batch: 090 ----
mean loss: 379.68
train mean loss: 376.67
epoch train time: 0:00:02.341378
elapsed time: 0:06:28.546699
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 22:00:25.442096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.69
 ---- batch: 020 ----
mean loss: 381.07
 ---- batch: 030 ----
mean loss: 375.52
 ---- batch: 040 ----
mean loss: 371.59
 ---- batch: 050 ----
mean loss: 379.53
 ---- batch: 060 ----
mean loss: 374.13
 ---- batch: 070 ----
mean loss: 379.60
 ---- batch: 080 ----
mean loss: 400.40
 ---- batch: 090 ----
mean loss: 394.93
train mean loss: 380.65
epoch train time: 0:00:02.333389
elapsed time: 0:06:30.880294
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 22:00:27.775680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.43
 ---- batch: 020 ----
mean loss: 376.03
 ---- batch: 030 ----
mean loss: 379.49
 ---- batch: 040 ----
mean loss: 372.30
 ---- batch: 050 ----
mean loss: 373.58
 ---- batch: 060 ----
mean loss: 374.06
 ---- batch: 070 ----
mean loss: 365.70
 ---- batch: 080 ----
mean loss: 363.99
 ---- batch: 090 ----
mean loss: 368.93
train mean loss: 375.56
epoch train time: 0:00:02.340447
elapsed time: 0:06:33.220944
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 22:00:30.116345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.32
 ---- batch: 020 ----
mean loss: 380.82
 ---- batch: 030 ----
mean loss: 376.67
 ---- batch: 040 ----
mean loss: 381.15
 ---- batch: 050 ----
mean loss: 372.30
 ---- batch: 060 ----
mean loss: 388.96
 ---- batch: 070 ----
mean loss: 367.96
 ---- batch: 080 ----
mean loss: 367.94
 ---- batch: 090 ----
mean loss: 379.78
train mean loss: 376.81
epoch train time: 0:00:02.333172
elapsed time: 0:06:35.554356
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 22:00:32.449748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.24
 ---- batch: 020 ----
mean loss: 378.68
 ---- batch: 030 ----
mean loss: 387.66
 ---- batch: 040 ----
mean loss: 386.18
 ---- batch: 050 ----
mean loss: 388.76
 ---- batch: 060 ----
mean loss: 399.34
 ---- batch: 070 ----
mean loss: 386.93
 ---- batch: 080 ----
mean loss: 377.49
 ---- batch: 090 ----
mean loss: 393.50
train mean loss: 385.38
epoch train time: 0:00:02.334453
elapsed time: 0:06:37.888999
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 22:00:34.784382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.75
 ---- batch: 020 ----
mean loss: 383.94
 ---- batch: 030 ----
mean loss: 378.12
 ---- batch: 040 ----
mean loss: 380.32
 ---- batch: 050 ----
mean loss: 361.95
 ---- batch: 060 ----
mean loss: 369.41
 ---- batch: 070 ----
mean loss: 379.98
 ---- batch: 080 ----
mean loss: 389.85
 ---- batch: 090 ----
mean loss: 377.80
train mean loss: 377.95
epoch train time: 0:00:02.336396
elapsed time: 0:06:40.225604
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 22:00:37.120991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.88
 ---- batch: 020 ----
mean loss: 373.88
 ---- batch: 030 ----
mean loss: 364.12
 ---- batch: 040 ----
mean loss: 363.77
 ---- batch: 050 ----
mean loss: 377.25
 ---- batch: 060 ----
mean loss: 378.51
 ---- batch: 070 ----
mean loss: 373.71
 ---- batch: 080 ----
mean loss: 375.80
 ---- batch: 090 ----
mean loss: 373.81
train mean loss: 372.27
epoch train time: 0:00:02.337161
elapsed time: 0:06:42.562948
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 22:00:39.458333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.36
 ---- batch: 020 ----
mean loss: 386.79
 ---- batch: 030 ----
mean loss: 398.92
 ---- batch: 040 ----
mean loss: 362.22
 ---- batch: 050 ----
mean loss: 372.67
 ---- batch: 060 ----
mean loss: 376.27
 ---- batch: 070 ----
mean loss: 373.59
 ---- batch: 080 ----
mean loss: 373.85
 ---- batch: 090 ----
mean loss: 369.31
train mean loss: 377.88
epoch train time: 0:00:02.335231
elapsed time: 0:06:44.898383
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 22:00:41.793775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.88
 ---- batch: 020 ----
mean loss: 372.92
 ---- batch: 030 ----
mean loss: 378.94
 ---- batch: 040 ----
mean loss: 385.55
 ---- batch: 050 ----
mean loss: 381.54
 ---- batch: 060 ----
mean loss: 369.29
 ---- batch: 070 ----
mean loss: 379.58
 ---- batch: 080 ----
mean loss: 382.00
 ---- batch: 090 ----
mean loss: 372.78
train mean loss: 378.00
epoch train time: 0:00:02.345942
elapsed time: 0:06:47.244515
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 22:00:44.139903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.04
 ---- batch: 020 ----
mean loss: 370.76
 ---- batch: 030 ----
mean loss: 366.26
 ---- batch: 040 ----
mean loss: 383.54
 ---- batch: 050 ----
mean loss: 376.96
 ---- batch: 060 ----
mean loss: 383.76
 ---- batch: 070 ----
mean loss: 374.49
 ---- batch: 080 ----
mean loss: 377.05
 ---- batch: 090 ----
mean loss: 378.21
train mean loss: 376.51
epoch train time: 0:00:02.326673
elapsed time: 0:06:49.571397
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 22:00:46.466802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.78
 ---- batch: 020 ----
mean loss: 387.87
 ---- batch: 030 ----
mean loss: 380.15
 ---- batch: 040 ----
mean loss: 363.26
 ---- batch: 050 ----
mean loss: 370.65
 ---- batch: 060 ----
mean loss: 388.52
 ---- batch: 070 ----
mean loss: 387.57
 ---- batch: 080 ----
mean loss: 377.72
 ---- batch: 090 ----
mean loss: 373.41
train mean loss: 379.06
epoch train time: 0:00:02.341059
elapsed time: 0:06:51.912677
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 22:00:48.808095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.75
 ---- batch: 020 ----
mean loss: 382.59
 ---- batch: 030 ----
mean loss: 391.30
 ---- batch: 040 ----
mean loss: 381.09
 ---- batch: 050 ----
mean loss: 375.72
 ---- batch: 060 ----
mean loss: 371.29
 ---- batch: 070 ----
mean loss: 377.30
 ---- batch: 080 ----
mean loss: 377.68
 ---- batch: 090 ----
mean loss: 371.31
train mean loss: 378.26
epoch train time: 0:00:02.335128
elapsed time: 0:06:54.248031
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 22:00:51.143412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.07
 ---- batch: 020 ----
mean loss: 359.51
 ---- batch: 030 ----
mean loss: 387.10
 ---- batch: 040 ----
mean loss: 403.00
 ---- batch: 050 ----
mean loss: 408.41
 ---- batch: 060 ----
mean loss: 402.24
 ---- batch: 070 ----
mean loss: 383.65
 ---- batch: 080 ----
mean loss: 374.51
 ---- batch: 090 ----
mean loss: 374.63
train mean loss: 384.89
epoch train time: 0:00:02.336399
elapsed time: 0:06:56.584605
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 22:00:53.479990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.90
 ---- batch: 020 ----
mean loss: 378.11
 ---- batch: 030 ----
mean loss: 380.82
 ---- batch: 040 ----
mean loss: 373.85
 ---- batch: 050 ----
mean loss: 381.24
 ---- batch: 060 ----
mean loss: 377.46
 ---- batch: 070 ----
mean loss: 379.29
 ---- batch: 080 ----
mean loss: 382.53
 ---- batch: 090 ----
mean loss: 380.31
train mean loss: 378.10
epoch train time: 0:00:02.346050
elapsed time: 0:06:58.930843
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 22:00:55.826227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.82
 ---- batch: 020 ----
mean loss: 374.00
 ---- batch: 030 ----
mean loss: 363.34
 ---- batch: 040 ----
mean loss: 372.50
 ---- batch: 050 ----
mean loss: 369.81
 ---- batch: 060 ----
mean loss: 381.44
 ---- batch: 070 ----
mean loss: 369.42
 ---- batch: 080 ----
mean loss: 370.67
 ---- batch: 090 ----
mean loss: 384.28
train mean loss: 372.82
epoch train time: 0:00:02.340555
elapsed time: 0:07:01.271579
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 22:00:58.166965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.54
 ---- batch: 020 ----
mean loss: 380.06
 ---- batch: 030 ----
mean loss: 365.30
 ---- batch: 040 ----
mean loss: 374.14
 ---- batch: 050 ----
mean loss: 391.71
 ---- batch: 060 ----
mean loss: 378.62
 ---- batch: 070 ----
mean loss: 375.32
 ---- batch: 080 ----
mean loss: 381.68
 ---- batch: 090 ----
mean loss: 375.19
train mean loss: 377.92
epoch train time: 0:00:02.335077
elapsed time: 0:07:03.606840
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 22:01:00.502223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.09
 ---- batch: 020 ----
mean loss: 375.19
 ---- batch: 030 ----
mean loss: 379.02
 ---- batch: 040 ----
mean loss: 366.75
 ---- batch: 050 ----
mean loss: 368.11
 ---- batch: 060 ----
mean loss: 367.43
 ---- batch: 070 ----
mean loss: 377.64
 ---- batch: 080 ----
mean loss: 379.39
 ---- batch: 090 ----
mean loss: 400.17
train mean loss: 377.89
epoch train time: 0:00:02.339862
elapsed time: 0:07:05.946896
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 22:01:02.842294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.30
 ---- batch: 020 ----
mean loss: 382.54
 ---- batch: 030 ----
mean loss: 364.36
 ---- batch: 040 ----
mean loss: 371.53
 ---- batch: 050 ----
mean loss: 390.02
 ---- batch: 060 ----
mean loss: 381.89
 ---- batch: 070 ----
mean loss: 373.96
 ---- batch: 080 ----
mean loss: 381.05
 ---- batch: 090 ----
mean loss: 378.96
train mean loss: 378.95
epoch train time: 0:00:02.345923
elapsed time: 0:07:08.293009
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 22:01:05.188408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.42
 ---- batch: 020 ----
mean loss: 366.11
 ---- batch: 030 ----
mean loss: 376.12
 ---- batch: 040 ----
mean loss: 375.46
 ---- batch: 050 ----
mean loss: 383.27
 ---- batch: 060 ----
mean loss: 381.51
 ---- batch: 070 ----
mean loss: 385.92
 ---- batch: 080 ----
mean loss: 370.18
 ---- batch: 090 ----
mean loss: 365.30
train mean loss: 376.29
epoch train time: 0:00:02.337190
elapsed time: 0:07:10.630444
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 22:01:07.525845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.94
 ---- batch: 020 ----
mean loss: 377.41
 ---- batch: 030 ----
mean loss: 379.72
 ---- batch: 040 ----
mean loss: 376.41
 ---- batch: 050 ----
mean loss: 394.93
 ---- batch: 060 ----
mean loss: 369.22
 ---- batch: 070 ----
mean loss: 377.73
 ---- batch: 080 ----
mean loss: 370.29
 ---- batch: 090 ----
mean loss: 369.77
train mean loss: 377.84
epoch train time: 0:00:02.336735
elapsed time: 0:07:12.967411
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 22:01:09.862810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.87
 ---- batch: 020 ----
mean loss: 402.44
 ---- batch: 030 ----
mean loss: 377.50
 ---- batch: 040 ----
mean loss: 369.73
 ---- batch: 050 ----
mean loss: 379.62
 ---- batch: 060 ----
mean loss: 373.00
 ---- batch: 070 ----
mean loss: 382.84
 ---- batch: 080 ----
mean loss: 359.59
 ---- batch: 090 ----
mean loss: 392.45
train mean loss: 382.51
epoch train time: 0:00:02.336860
elapsed time: 0:07:15.304538
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 22:01:12.199922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.10
 ---- batch: 020 ----
mean loss: 367.55
 ---- batch: 030 ----
mean loss: 378.50
 ---- batch: 040 ----
mean loss: 365.67
 ---- batch: 050 ----
mean loss: 370.81
 ---- batch: 060 ----
mean loss: 365.77
 ---- batch: 070 ----
mean loss: 371.77
 ---- batch: 080 ----
mean loss: 381.35
 ---- batch: 090 ----
mean loss: 358.29
train mean loss: 370.89
epoch train time: 0:00:02.332241
elapsed time: 0:07:17.636983
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 22:01:14.532416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.98
 ---- batch: 020 ----
mean loss: 381.54
 ---- batch: 030 ----
mean loss: 371.29
 ---- batch: 040 ----
mean loss: 362.65
 ---- batch: 050 ----
mean loss: 386.59
 ---- batch: 060 ----
mean loss: 394.86
 ---- batch: 070 ----
mean loss: 380.55
 ---- batch: 080 ----
mean loss: 386.95
 ---- batch: 090 ----
mean loss: 376.97
train mean loss: 378.85
epoch train time: 0:00:02.336560
elapsed time: 0:07:19.973784
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 22:01:16.869152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.71
 ---- batch: 020 ----
mean loss: 377.76
 ---- batch: 030 ----
mean loss: 373.86
 ---- batch: 040 ----
mean loss: 371.28
 ---- batch: 050 ----
mean loss: 363.97
 ---- batch: 060 ----
mean loss: 377.66
 ---- batch: 070 ----
mean loss: 371.63
 ---- batch: 080 ----
mean loss: 387.40
 ---- batch: 090 ----
mean loss: 373.81
train mean loss: 374.81
epoch train time: 0:00:02.333532
elapsed time: 0:07:22.307479
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 22:01:19.202865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.19
 ---- batch: 020 ----
mean loss: 373.46
 ---- batch: 030 ----
mean loss: 381.39
 ---- batch: 040 ----
mean loss: 372.77
 ---- batch: 050 ----
mean loss: 371.68
 ---- batch: 060 ----
mean loss: 380.42
 ---- batch: 070 ----
mean loss: 371.36
 ---- batch: 080 ----
mean loss: 363.70
 ---- batch: 090 ----
mean loss: 367.01
train mean loss: 373.41
epoch train time: 0:00:02.325403
elapsed time: 0:07:24.633054
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 22:01:21.528435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.49
 ---- batch: 020 ----
mean loss: 365.79
 ---- batch: 030 ----
mean loss: 374.40
 ---- batch: 040 ----
mean loss: 379.92
 ---- batch: 050 ----
mean loss: 377.54
 ---- batch: 060 ----
mean loss: 379.18
 ---- batch: 070 ----
mean loss: 376.02
 ---- batch: 080 ----
mean loss: 377.79
 ---- batch: 090 ----
mean loss: 387.87
train mean loss: 377.86
epoch train time: 0:00:02.340120
elapsed time: 0:07:26.973362
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 22:01:23.868781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.72
 ---- batch: 020 ----
mean loss: 397.13
 ---- batch: 030 ----
mean loss: 382.45
 ---- batch: 040 ----
mean loss: 379.16
 ---- batch: 050 ----
mean loss: 381.81
 ---- batch: 060 ----
mean loss: 384.99
 ---- batch: 070 ----
mean loss: 376.34
 ---- batch: 080 ----
mean loss: 361.19
 ---- batch: 090 ----
mean loss: 358.31
train mean loss: 377.89
epoch train time: 0:00:02.332982
elapsed time: 0:07:29.306562
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 22:01:26.201941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.83
 ---- batch: 020 ----
mean loss: 384.08
 ---- batch: 030 ----
mean loss: 371.68
 ---- batch: 040 ----
mean loss: 364.16
 ---- batch: 050 ----
mean loss: 364.80
 ---- batch: 060 ----
mean loss: 365.36
 ---- batch: 070 ----
mean loss: 371.02
 ---- batch: 080 ----
mean loss: 369.44
 ---- batch: 090 ----
mean loss: 395.16
train mean loss: 375.15
epoch train time: 0:00:02.335762
elapsed time: 0:07:31.642565
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 22:01:28.537958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.16
 ---- batch: 020 ----
mean loss: 393.70
 ---- batch: 030 ----
mean loss: 374.72
 ---- batch: 040 ----
mean loss: 366.90
 ---- batch: 050 ----
mean loss: 378.87
 ---- batch: 060 ----
mean loss: 365.00
 ---- batch: 070 ----
mean loss: 374.34
 ---- batch: 080 ----
mean loss: 371.47
 ---- batch: 090 ----
mean loss: 375.61
train mean loss: 377.11
epoch train time: 0:00:02.334371
elapsed time: 0:07:33.977149
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 22:01:30.872550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.25
 ---- batch: 020 ----
mean loss: 371.37
 ---- batch: 030 ----
mean loss: 384.29
 ---- batch: 040 ----
mean loss: 382.36
 ---- batch: 050 ----
mean loss: 367.18
 ---- batch: 060 ----
mean loss: 356.78
 ---- batch: 070 ----
mean loss: 368.73
 ---- batch: 080 ----
mean loss: 354.08
 ---- batch: 090 ----
mean loss: 370.37
train mean loss: 369.80
epoch train time: 0:00:02.338546
elapsed time: 0:07:36.315922
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 22:01:33.211305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.89
 ---- batch: 020 ----
mean loss: 377.33
 ---- batch: 030 ----
mean loss: 384.07
 ---- batch: 040 ----
mean loss: 368.91
 ---- batch: 050 ----
mean loss: 366.49
 ---- batch: 060 ----
mean loss: 374.47
 ---- batch: 070 ----
mean loss: 372.05
 ---- batch: 080 ----
mean loss: 380.01
 ---- batch: 090 ----
mean loss: 385.70
train mean loss: 375.66
epoch train time: 0:00:02.336856
elapsed time: 0:07:38.652954
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 22:01:35.548347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.78
 ---- batch: 020 ----
mean loss: 364.37
 ---- batch: 030 ----
mean loss: 372.11
 ---- batch: 040 ----
mean loss: 371.18
 ---- batch: 050 ----
mean loss: 352.05
 ---- batch: 060 ----
mean loss: 391.52
 ---- batch: 070 ----
mean loss: 370.75
 ---- batch: 080 ----
mean loss: 380.65
 ---- batch: 090 ----
mean loss: 376.72
train mean loss: 372.16
epoch train time: 0:00:02.345604
elapsed time: 0:07:40.998763
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 22:01:37.894156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.54
 ---- batch: 020 ----
mean loss: 380.36
 ---- batch: 030 ----
mean loss: 363.46
 ---- batch: 040 ----
mean loss: 371.67
 ---- batch: 050 ----
mean loss: 378.98
 ---- batch: 060 ----
mean loss: 366.37
 ---- batch: 070 ----
mean loss: 373.01
 ---- batch: 080 ----
mean loss: 371.42
 ---- batch: 090 ----
mean loss: 378.73
train mean loss: 372.65
epoch train time: 0:00:02.331596
elapsed time: 0:07:43.330558
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 22:01:40.225980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.34
 ---- batch: 020 ----
mean loss: 390.14
 ---- batch: 030 ----
mean loss: 376.89
 ---- batch: 040 ----
mean loss: 374.55
 ---- batch: 050 ----
mean loss: 383.02
 ---- batch: 060 ----
mean loss: 384.74
 ---- batch: 070 ----
mean loss: 372.59
 ---- batch: 080 ----
mean loss: 360.96
 ---- batch: 090 ----
mean loss: 372.49
train mean loss: 374.79
epoch train time: 0:00:02.349362
elapsed time: 0:07:45.680167
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 22:01:42.575551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.26
 ---- batch: 020 ----
mean loss: 372.87
 ---- batch: 030 ----
mean loss: 366.11
 ---- batch: 040 ----
mean loss: 374.78
 ---- batch: 050 ----
mean loss: 379.97
 ---- batch: 060 ----
mean loss: 374.05
 ---- batch: 070 ----
mean loss: 383.90
 ---- batch: 080 ----
mean loss: 387.08
 ---- batch: 090 ----
mean loss: 377.60
train mean loss: 374.61
epoch train time: 0:00:02.342221
elapsed time: 0:07:48.022583
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 22:01:44.917969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.15
 ---- batch: 020 ----
mean loss: 385.77
 ---- batch: 030 ----
mean loss: 366.24
 ---- batch: 040 ----
mean loss: 378.37
 ---- batch: 050 ----
mean loss: 382.68
 ---- batch: 060 ----
mean loss: 383.71
 ---- batch: 070 ----
mean loss: 366.24
 ---- batch: 080 ----
mean loss: 382.09
 ---- batch: 090 ----
mean loss: 381.49
train mean loss: 378.62
epoch train time: 0:00:02.328419
elapsed time: 0:07:50.351180
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 22:01:47.246561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.97
 ---- batch: 020 ----
mean loss: 372.11
 ---- batch: 030 ----
mean loss: 372.19
 ---- batch: 040 ----
mean loss: 366.89
 ---- batch: 050 ----
mean loss: 365.00
 ---- batch: 060 ----
mean loss: 375.12
 ---- batch: 070 ----
mean loss: 375.96
 ---- batch: 080 ----
mean loss: 364.47
 ---- batch: 090 ----
mean loss: 364.91
train mean loss: 369.10
epoch train time: 0:00:02.340145
elapsed time: 0:07:52.691495
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 22:01:49.586878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.19
 ---- batch: 020 ----
mean loss: 387.61
 ---- batch: 030 ----
mean loss: 389.86
 ---- batch: 040 ----
mean loss: 366.38
 ---- batch: 050 ----
mean loss: 359.08
 ---- batch: 060 ----
mean loss: 369.88
 ---- batch: 070 ----
mean loss: 376.48
 ---- batch: 080 ----
mean loss: 374.88
 ---- batch: 090 ----
mean loss: 373.33
train mean loss: 373.96
epoch train time: 0:00:02.334613
elapsed time: 0:07:55.026315
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 22:01:51.921703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.91
 ---- batch: 020 ----
mean loss: 375.29
 ---- batch: 030 ----
mean loss: 385.02
 ---- batch: 040 ----
mean loss: 356.95
 ---- batch: 050 ----
mean loss: 385.54
 ---- batch: 060 ----
mean loss: 372.17
 ---- batch: 070 ----
mean loss: 372.74
 ---- batch: 080 ----
mean loss: 385.35
 ---- batch: 090 ----
mean loss: 382.58
train mean loss: 377.71
epoch train time: 0:00:02.340617
elapsed time: 0:07:57.367126
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 22:01:54.262511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.73
 ---- batch: 020 ----
mean loss: 365.64
 ---- batch: 030 ----
mean loss: 376.33
 ---- batch: 040 ----
mean loss: 367.19
 ---- batch: 050 ----
mean loss: 386.20
 ---- batch: 060 ----
mean loss: 372.18
 ---- batch: 070 ----
mean loss: 370.05
 ---- batch: 080 ----
mean loss: 373.77
 ---- batch: 090 ----
mean loss: 368.39
train mean loss: 371.72
epoch train time: 0:00:02.337988
elapsed time: 0:07:59.705301
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 22:01:56.600686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.82
 ---- batch: 020 ----
mean loss: 390.99
 ---- batch: 030 ----
mean loss: 390.43
 ---- batch: 040 ----
mean loss: 374.38
 ---- batch: 050 ----
mean loss: 370.09
 ---- batch: 060 ----
mean loss: 358.74
 ---- batch: 070 ----
mean loss: 363.63
 ---- batch: 080 ----
mean loss: 358.06
 ---- batch: 090 ----
mean loss: 373.08
train mean loss: 374.59
epoch train time: 0:00:02.331955
elapsed time: 0:08:02.037441
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 22:01:58.932847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.29
 ---- batch: 020 ----
mean loss: 378.04
 ---- batch: 030 ----
mean loss: 373.83
 ---- batch: 040 ----
mean loss: 374.97
 ---- batch: 050 ----
mean loss: 374.13
 ---- batch: 060 ----
mean loss: 372.31
 ---- batch: 070 ----
mean loss: 360.83
 ---- batch: 080 ----
mean loss: 368.08
 ---- batch: 090 ----
mean loss: 368.17
train mean loss: 371.85
epoch train time: 0:00:02.333687
elapsed time: 0:08:04.371338
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 22:02:01.266722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.64
 ---- batch: 020 ----
mean loss: 371.28
 ---- batch: 030 ----
mean loss: 361.19
 ---- batch: 040 ----
mean loss: 371.93
 ---- batch: 050 ----
mean loss: 372.93
 ---- batch: 060 ----
mean loss: 357.59
 ---- batch: 070 ----
mean loss: 372.20
 ---- batch: 080 ----
mean loss: 388.80
 ---- batch: 090 ----
mean loss: 376.05
train mean loss: 372.06
epoch train time: 0:00:02.338326
elapsed time: 0:08:06.709845
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 22:02:03.605229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.19
 ---- batch: 020 ----
mean loss: 367.92
 ---- batch: 030 ----
mean loss: 365.21
 ---- batch: 040 ----
mean loss: 369.25
 ---- batch: 050 ----
mean loss: 382.44
 ---- batch: 060 ----
mean loss: 370.17
 ---- batch: 070 ----
mean loss: 363.74
 ---- batch: 080 ----
mean loss: 366.56
 ---- batch: 090 ----
mean loss: 367.58
train mean loss: 370.23
epoch train time: 0:00:02.331284
elapsed time: 0:08:09.041304
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 22:02:05.936689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.55
 ---- batch: 020 ----
mean loss: 363.95
 ---- batch: 030 ----
mean loss: 373.59
 ---- batch: 040 ----
mean loss: 372.74
 ---- batch: 050 ----
mean loss: 379.09
 ---- batch: 060 ----
mean loss: 379.92
 ---- batch: 070 ----
mean loss: 363.92
 ---- batch: 080 ----
mean loss: 372.33
 ---- batch: 090 ----
mean loss: 366.86
train mean loss: 371.95
epoch train time: 0:00:02.339889
elapsed time: 0:08:11.381420
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 22:02:08.276819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.64
 ---- batch: 020 ----
mean loss: 381.33
 ---- batch: 030 ----
mean loss: 376.97
 ---- batch: 040 ----
mean loss: 368.14
 ---- batch: 050 ----
mean loss: 373.36
 ---- batch: 060 ----
mean loss: 386.39
 ---- batch: 070 ----
mean loss: 392.43
 ---- batch: 080 ----
mean loss: 385.75
 ---- batch: 090 ----
mean loss: 379.19
train mean loss: 379.47
epoch train time: 0:00:02.331822
elapsed time: 0:08:13.713470
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 22:02:10.608857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.29
 ---- batch: 020 ----
mean loss: 371.83
 ---- batch: 030 ----
mean loss: 377.20
 ---- batch: 040 ----
mean loss: 370.57
 ---- batch: 050 ----
mean loss: 363.24
 ---- batch: 060 ----
mean loss: 370.55
 ---- batch: 070 ----
mean loss: 366.18
 ---- batch: 080 ----
mean loss: 377.45
 ---- batch: 090 ----
mean loss: 378.04
train mean loss: 371.52
epoch train time: 0:00:02.333747
elapsed time: 0:08:16.047393
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 22:02:12.942776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.90
 ---- batch: 020 ----
mean loss: 353.48
 ---- batch: 030 ----
mean loss: 364.42
 ---- batch: 040 ----
mean loss: 374.73
 ---- batch: 050 ----
mean loss: 378.68
 ---- batch: 060 ----
mean loss: 360.27
 ---- batch: 070 ----
mean loss: 384.23
 ---- batch: 080 ----
mean loss: 374.93
 ---- batch: 090 ----
mean loss: 367.61
train mean loss: 370.71
epoch train time: 0:00:02.337598
elapsed time: 0:08:18.385160
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 22:02:15.280545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.45
 ---- batch: 020 ----
mean loss: 364.38
 ---- batch: 030 ----
mean loss: 378.38
 ---- batch: 040 ----
mean loss: 367.91
 ---- batch: 050 ----
mean loss: 364.29
 ---- batch: 060 ----
mean loss: 374.14
 ---- batch: 070 ----
mean loss: 367.32
 ---- batch: 080 ----
mean loss: 381.72
 ---- batch: 090 ----
mean loss: 390.70
train mean loss: 374.39
epoch train time: 0:00:02.346472
elapsed time: 0:08:20.731803
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 22:02:17.627202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.68
 ---- batch: 020 ----
mean loss: 378.14
 ---- batch: 030 ----
mean loss: 371.18
 ---- batch: 040 ----
mean loss: 375.25
 ---- batch: 050 ----
mean loss: 372.94
 ---- batch: 060 ----
mean loss: 385.14
 ---- batch: 070 ----
mean loss: 383.11
 ---- batch: 080 ----
mean loss: 386.14
 ---- batch: 090 ----
mean loss: 378.82
train mean loss: 378.10
epoch train time: 0:00:02.333940
elapsed time: 0:08:23.065946
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 22:02:19.961405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.83
 ---- batch: 020 ----
mean loss: 365.59
 ---- batch: 030 ----
mean loss: 360.92
 ---- batch: 040 ----
mean loss: 373.71
 ---- batch: 050 ----
mean loss: 370.14
 ---- batch: 060 ----
mean loss: 354.14
 ---- batch: 070 ----
mean loss: 364.80
 ---- batch: 080 ----
mean loss: 365.95
 ---- batch: 090 ----
mean loss: 368.65
train mean loss: 366.48
epoch train time: 0:00:02.325747
elapsed time: 0:08:25.391964
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 22:02:22.287333
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.18
 ---- batch: 020 ----
mean loss: 366.34
 ---- batch: 030 ----
mean loss: 368.38
 ---- batch: 040 ----
mean loss: 351.17
 ---- batch: 050 ----
mean loss: 366.98
 ---- batch: 060 ----
mean loss: 363.78
 ---- batch: 070 ----
mean loss: 366.90
 ---- batch: 080 ----
mean loss: 364.91
 ---- batch: 090 ----
mean loss: 356.19
train mean loss: 364.14
epoch train time: 0:00:02.343973
elapsed time: 0:08:27.736103
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 22:02:24.631484
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 374.67
 ---- batch: 020 ----
mean loss: 368.42
 ---- batch: 030 ----
mean loss: 356.29
 ---- batch: 040 ----
mean loss: 356.99
 ---- batch: 050 ----
mean loss: 366.21
 ---- batch: 060 ----
mean loss: 352.69
 ---- batch: 070 ----
mean loss: 358.24
 ---- batch: 080 ----
mean loss: 362.04
 ---- batch: 090 ----
mean loss: 356.71
train mean loss: 360.50
epoch train time: 0:00:02.328326
elapsed time: 0:08:30.064597
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 22:02:26.959980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.99
 ---- batch: 020 ----
mean loss: 369.77
 ---- batch: 030 ----
mean loss: 372.00
 ---- batch: 040 ----
mean loss: 363.80
 ---- batch: 050 ----
mean loss: 355.54
 ---- batch: 060 ----
mean loss: 366.84
 ---- batch: 070 ----
mean loss: 357.29
 ---- batch: 080 ----
mean loss: 376.36
 ---- batch: 090 ----
mean loss: 356.26
train mean loss: 364.01
epoch train time: 0:00:02.335451
elapsed time: 0:08:32.400222
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 22:02:29.295613
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.25
 ---- batch: 020 ----
mean loss: 369.80
 ---- batch: 030 ----
mean loss: 372.52
 ---- batch: 040 ----
mean loss: 355.59
 ---- batch: 050 ----
mean loss: 364.48
 ---- batch: 060 ----
mean loss: 362.92
 ---- batch: 070 ----
mean loss: 358.65
 ---- batch: 080 ----
mean loss: 364.44
 ---- batch: 090 ----
mean loss: 358.75
train mean loss: 362.42
epoch train time: 0:00:02.326961
elapsed time: 0:08:34.727381
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 22:02:31.622767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.56
 ---- batch: 020 ----
mean loss: 356.01
 ---- batch: 030 ----
mean loss: 350.77
 ---- batch: 040 ----
mean loss: 369.63
 ---- batch: 050 ----
mean loss: 363.65
 ---- batch: 060 ----
mean loss: 366.77
 ---- batch: 070 ----
mean loss: 369.67
 ---- batch: 080 ----
mean loss: 371.82
 ---- batch: 090 ----
mean loss: 369.38
train mean loss: 363.49
epoch train time: 0:00:02.330724
elapsed time: 0:08:37.058294
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 22:02:33.953700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 372.03
 ---- batch: 020 ----
mean loss: 355.63
 ---- batch: 030 ----
mean loss: 359.79
 ---- batch: 040 ----
mean loss: 364.18
 ---- batch: 050 ----
mean loss: 363.81
 ---- batch: 060 ----
mean loss: 360.91
 ---- batch: 070 ----
mean loss: 367.93
 ---- batch: 080 ----
mean loss: 363.26
 ---- batch: 090 ----
mean loss: 360.08
train mean loss: 363.56
epoch train time: 0:00:02.331827
elapsed time: 0:08:39.390396
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 22:02:36.285786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.85
 ---- batch: 020 ----
mean loss: 370.87
 ---- batch: 030 ----
mean loss: 364.18
 ---- batch: 040 ----
mean loss: 362.56
 ---- batch: 050 ----
mean loss: 367.66
 ---- batch: 060 ----
mean loss: 362.03
 ---- batch: 070 ----
mean loss: 369.32
 ---- batch: 080 ----
mean loss: 360.45
 ---- batch: 090 ----
mean loss: 364.17
train mean loss: 362.68
epoch train time: 0:00:02.337271
elapsed time: 0:08:41.727851
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 22:02:38.623234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 361.06
 ---- batch: 020 ----
mean loss: 371.55
 ---- batch: 030 ----
mean loss: 364.85
 ---- batch: 040 ----
mean loss: 356.48
 ---- batch: 050 ----
mean loss: 360.08
 ---- batch: 060 ----
mean loss: 363.73
 ---- batch: 070 ----
mean loss: 362.20
 ---- batch: 080 ----
mean loss: 362.30
 ---- batch: 090 ----
mean loss: 351.71
train mean loss: 360.42
epoch train time: 0:00:02.341231
elapsed time: 0:08:44.069259
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 22:02:40.964644
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.08
 ---- batch: 020 ----
mean loss: 373.67
 ---- batch: 030 ----
mean loss: 365.72
 ---- batch: 040 ----
mean loss: 354.64
 ---- batch: 050 ----
mean loss: 351.47
 ---- batch: 060 ----
mean loss: 375.91
 ---- batch: 070 ----
mean loss: 363.15
 ---- batch: 080 ----
mean loss: 361.84
 ---- batch: 090 ----
mean loss: 360.20
train mean loss: 363.09
epoch train time: 0:00:02.337308
elapsed time: 0:08:46.406747
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 22:02:43.302130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.36
 ---- batch: 020 ----
mean loss: 355.80
 ---- batch: 030 ----
mean loss: 353.47
 ---- batch: 040 ----
mean loss: 361.99
 ---- batch: 050 ----
mean loss: 364.66
 ---- batch: 060 ----
mean loss: 360.38
 ---- batch: 070 ----
mean loss: 362.93
 ---- batch: 080 ----
mean loss: 364.98
 ---- batch: 090 ----
mean loss: 356.74
train mean loss: 361.30
epoch train time: 0:00:02.341217
elapsed time: 0:08:48.748168
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 22:02:45.643566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.24
 ---- batch: 020 ----
mean loss: 361.99
 ---- batch: 030 ----
mean loss: 370.80
 ---- batch: 040 ----
mean loss: 362.72
 ---- batch: 050 ----
mean loss: 359.77
 ---- batch: 060 ----
mean loss: 361.81
 ---- batch: 070 ----
mean loss: 362.87
 ---- batch: 080 ----
mean loss: 369.52
 ---- batch: 090 ----
mean loss: 359.23
train mean loss: 362.62
epoch train time: 0:00:02.334209
elapsed time: 0:08:51.082566
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 22:02:47.977958
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.38
 ---- batch: 020 ----
mean loss: 372.31
 ---- batch: 030 ----
mean loss: 353.74
 ---- batch: 040 ----
mean loss: 360.67
 ---- batch: 050 ----
mean loss: 358.18
 ---- batch: 060 ----
mean loss: 374.53
 ---- batch: 070 ----
mean loss: 353.10
 ---- batch: 080 ----
mean loss: 360.63
 ---- batch: 090 ----
mean loss: 356.24
train mean loss: 359.23
epoch train time: 0:00:02.328931
elapsed time: 0:08:53.411688
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 22:02:50.307069
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.77
 ---- batch: 020 ----
mean loss: 350.31
 ---- batch: 030 ----
mean loss: 363.48
 ---- batch: 040 ----
mean loss: 369.69
 ---- batch: 050 ----
mean loss: 356.68
 ---- batch: 060 ----
mean loss: 361.77
 ---- batch: 070 ----
mean loss: 365.65
 ---- batch: 080 ----
mean loss: 361.99
 ---- batch: 090 ----
mean loss: 351.41
train mean loss: 359.94
epoch train time: 0:00:02.343390
elapsed time: 0:08:55.755252
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 22:02:52.650657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.59
 ---- batch: 020 ----
mean loss: 357.88
 ---- batch: 030 ----
mean loss: 352.32
 ---- batch: 040 ----
mean loss: 358.48
 ---- batch: 050 ----
mean loss: 360.32
 ---- batch: 060 ----
mean loss: 363.31
 ---- batch: 070 ----
mean loss: 356.41
 ---- batch: 080 ----
mean loss: 354.35
 ---- batch: 090 ----
mean loss: 359.60
train mean loss: 358.57
epoch train time: 0:00:02.345556
elapsed time: 0:08:58.101018
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 22:02:54.996404
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.81
 ---- batch: 020 ----
mean loss: 357.40
 ---- batch: 030 ----
mean loss: 353.17
 ---- batch: 040 ----
mean loss: 353.32
 ---- batch: 050 ----
mean loss: 359.15
 ---- batch: 060 ----
mean loss: 369.79
 ---- batch: 070 ----
mean loss: 349.40
 ---- batch: 080 ----
mean loss: 375.13
 ---- batch: 090 ----
mean loss: 360.61
train mean loss: 358.82
epoch train time: 0:00:02.331210
elapsed time: 0:09:00.432410
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 22:02:57.327840
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.45
 ---- batch: 020 ----
mean loss: 366.18
 ---- batch: 030 ----
mean loss: 356.83
 ---- batch: 040 ----
mean loss: 360.74
 ---- batch: 050 ----
mean loss: 364.05
 ---- batch: 060 ----
mean loss: 352.94
 ---- batch: 070 ----
mean loss: 352.63
 ---- batch: 080 ----
mean loss: 361.18
 ---- batch: 090 ----
mean loss: 366.99
train mean loss: 360.44
epoch train time: 0:00:02.341580
elapsed time: 0:09:02.774221
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 22:02:59.669635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.91
 ---- batch: 020 ----
mean loss: 361.56
 ---- batch: 030 ----
mean loss: 361.80
 ---- batch: 040 ----
mean loss: 361.66
 ---- batch: 050 ----
mean loss: 359.95
 ---- batch: 060 ----
mean loss: 354.97
 ---- batch: 070 ----
mean loss: 358.17
 ---- batch: 080 ----
mean loss: 363.97
 ---- batch: 090 ----
mean loss: 360.36
train mean loss: 360.72
epoch train time: 0:00:02.330950
elapsed time: 0:09:05.105377
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 22:03:02.000779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.28
 ---- batch: 020 ----
mean loss: 363.71
 ---- batch: 030 ----
mean loss: 362.64
 ---- batch: 040 ----
mean loss: 359.47
 ---- batch: 050 ----
mean loss: 359.53
 ---- batch: 060 ----
mean loss: 365.82
 ---- batch: 070 ----
mean loss: 357.93
 ---- batch: 080 ----
mean loss: 374.80
 ---- batch: 090 ----
mean loss: 358.21
train mean loss: 361.92
epoch train time: 0:00:02.330645
elapsed time: 0:09:07.436219
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 22:03:04.331603
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.90
 ---- batch: 020 ----
mean loss: 361.56
 ---- batch: 030 ----
mean loss: 360.48
 ---- batch: 040 ----
mean loss: 357.96
 ---- batch: 050 ----
mean loss: 353.78
 ---- batch: 060 ----
mean loss: 361.77
 ---- batch: 070 ----
mean loss: 352.83
 ---- batch: 080 ----
mean loss: 374.69
 ---- batch: 090 ----
mean loss: 364.75
train mean loss: 360.01
epoch train time: 0:00:02.342358
elapsed time: 0:09:09.778764
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 22:03:06.674151
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.88
 ---- batch: 020 ----
mean loss: 361.96
 ---- batch: 030 ----
mean loss: 349.35
 ---- batch: 040 ----
mean loss: 361.27
 ---- batch: 050 ----
mean loss: 360.47
 ---- batch: 060 ----
mean loss: 369.30
 ---- batch: 070 ----
mean loss: 357.81
 ---- batch: 080 ----
mean loss: 355.83
 ---- batch: 090 ----
mean loss: 359.96
train mean loss: 360.89
epoch train time: 0:00:02.338464
elapsed time: 0:09:12.117413
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 22:03:09.012818
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.94
 ---- batch: 020 ----
mean loss: 353.56
 ---- batch: 030 ----
mean loss: 368.67
 ---- batch: 040 ----
mean loss: 368.19
 ---- batch: 050 ----
mean loss: 359.13
 ---- batch: 060 ----
mean loss: 367.46
 ---- batch: 070 ----
mean loss: 366.96
 ---- batch: 080 ----
mean loss: 368.63
 ---- batch: 090 ----
mean loss: 364.80
train mean loss: 365.96
epoch train time: 0:00:02.336587
elapsed time: 0:09:14.454209
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 22:03:11.349591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.80
 ---- batch: 020 ----
mean loss: 377.86
 ---- batch: 030 ----
mean loss: 364.57
 ---- batch: 040 ----
mean loss: 359.45
 ---- batch: 050 ----
mean loss: 365.17
 ---- batch: 060 ----
mean loss: 357.83
 ---- batch: 070 ----
mean loss: 369.49
 ---- batch: 080 ----
mean loss: 347.30
 ---- batch: 090 ----
mean loss: 377.32
train mean loss: 363.53
epoch train time: 0:00:02.335085
elapsed time: 0:09:16.789472
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 22:03:13.684854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.14
 ---- batch: 020 ----
mean loss: 363.00
 ---- batch: 030 ----
mean loss: 360.24
 ---- batch: 040 ----
mean loss: 370.58
 ---- batch: 050 ----
mean loss: 352.03
 ---- batch: 060 ----
mean loss: 352.01
 ---- batch: 070 ----
mean loss: 365.93
 ---- batch: 080 ----
mean loss: 375.57
 ---- batch: 090 ----
mean loss: 355.79
train mean loss: 361.22
epoch train time: 0:00:02.336745
elapsed time: 0:09:19.126520
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 22:03:16.021946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.36
 ---- batch: 020 ----
mean loss: 356.08
 ---- batch: 030 ----
mean loss: 373.01
 ---- batch: 040 ----
mean loss: 357.16
 ---- batch: 050 ----
mean loss: 365.01
 ---- batch: 060 ----
mean loss: 367.27
 ---- batch: 070 ----
mean loss: 358.57
 ---- batch: 080 ----
mean loss: 356.36
 ---- batch: 090 ----
mean loss: 383.46
train mean loss: 365.09
epoch train time: 0:00:02.330634
elapsed time: 0:09:21.457394
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 22:03:18.352781
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.09
 ---- batch: 020 ----
mean loss: 360.41
 ---- batch: 030 ----
mean loss: 361.80
 ---- batch: 040 ----
mean loss: 355.59
 ---- batch: 050 ----
mean loss: 344.88
 ---- batch: 060 ----
mean loss: 370.46
 ---- batch: 070 ----
mean loss: 357.21
 ---- batch: 080 ----
mean loss: 368.04
 ---- batch: 090 ----
mean loss: 373.08
train mean loss: 360.95
epoch train time: 0:00:02.334745
elapsed time: 0:09:23.792324
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 22:03:20.687723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.15
 ---- batch: 020 ----
mean loss: 354.43
 ---- batch: 030 ----
mean loss: 359.63
 ---- batch: 040 ----
mean loss: 355.89
 ---- batch: 050 ----
mean loss: 360.39
 ---- batch: 060 ----
mean loss: 354.38
 ---- batch: 070 ----
mean loss: 363.04
 ---- batch: 080 ----
mean loss: 358.54
 ---- batch: 090 ----
mean loss: 358.86
train mean loss: 357.72
epoch train time: 0:00:02.324598
elapsed time: 0:09:26.117115
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 22:03:23.012509
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.89
 ---- batch: 020 ----
mean loss: 352.50
 ---- batch: 030 ----
mean loss: 377.70
 ---- batch: 040 ----
mean loss: 364.53
 ---- batch: 050 ----
mean loss: 366.07
 ---- batch: 060 ----
mean loss: 368.02
 ---- batch: 070 ----
mean loss: 367.97
 ---- batch: 080 ----
mean loss: 354.66
 ---- batch: 090 ----
mean loss: 355.50
train mean loss: 363.44
epoch train time: 0:00:02.326643
elapsed time: 0:09:28.443942
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 22:03:25.339324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.32
 ---- batch: 020 ----
mean loss: 361.77
 ---- batch: 030 ----
mean loss: 356.74
 ---- batch: 040 ----
mean loss: 364.78
 ---- batch: 050 ----
mean loss: 361.91
 ---- batch: 060 ----
mean loss: 359.22
 ---- batch: 070 ----
mean loss: 362.44
 ---- batch: 080 ----
mean loss: 357.27
 ---- batch: 090 ----
mean loss: 363.21
train mean loss: 360.27
epoch train time: 0:00:02.334419
elapsed time: 0:09:30.778537
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 22:03:27.673950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.38
 ---- batch: 020 ----
mean loss: 358.75
 ---- batch: 030 ----
mean loss: 364.25
 ---- batch: 040 ----
mean loss: 363.30
 ---- batch: 050 ----
mean loss: 346.69
 ---- batch: 060 ----
mean loss: 360.79
 ---- batch: 070 ----
mean loss: 362.65
 ---- batch: 080 ----
mean loss: 364.09
 ---- batch: 090 ----
mean loss: 361.47
train mean loss: 358.56
epoch train time: 0:00:02.335426
elapsed time: 0:09:33.114188
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 22:03:30.009572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.57
 ---- batch: 020 ----
mean loss: 358.73
 ---- batch: 030 ----
mean loss: 366.13
 ---- batch: 040 ----
mean loss: 353.17
 ---- batch: 050 ----
mean loss: 358.28
 ---- batch: 060 ----
mean loss: 359.09
 ---- batch: 070 ----
mean loss: 356.18
 ---- batch: 080 ----
mean loss: 364.06
 ---- batch: 090 ----
mean loss: 364.14
train mean loss: 360.02
epoch train time: 0:00:02.325891
elapsed time: 0:09:35.440252
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 22:03:32.335650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.58
 ---- batch: 020 ----
mean loss: 364.71
 ---- batch: 030 ----
mean loss: 365.99
 ---- batch: 040 ----
mean loss: 346.79
 ---- batch: 050 ----
mean loss: 351.97
 ---- batch: 060 ----
mean loss: 371.14
 ---- batch: 070 ----
mean loss: 366.62
 ---- batch: 080 ----
mean loss: 358.25
 ---- batch: 090 ----
mean loss: 365.62
train mean loss: 360.44
epoch train time: 0:00:02.341721
elapsed time: 0:09:37.782171
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 22:03:34.677553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.06
 ---- batch: 020 ----
mean loss: 356.64
 ---- batch: 030 ----
mean loss: 351.50
 ---- batch: 040 ----
mean loss: 368.12
 ---- batch: 050 ----
mean loss: 356.65
 ---- batch: 060 ----
mean loss: 368.99
 ---- batch: 070 ----
mean loss: 365.10
 ---- batch: 080 ----
mean loss: 375.27
 ---- batch: 090 ----
mean loss: 349.86
train mean loss: 361.42
epoch train time: 0:00:02.331519
elapsed time: 0:09:40.113929
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 22:03:37.009301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.69
 ---- batch: 020 ----
mean loss: 371.02
 ---- batch: 030 ----
mean loss: 355.79
 ---- batch: 040 ----
mean loss: 365.64
 ---- batch: 050 ----
mean loss: 354.44
 ---- batch: 060 ----
mean loss: 373.96
 ---- batch: 070 ----
mean loss: 368.45
 ---- batch: 080 ----
mean loss: 361.30
 ---- batch: 090 ----
mean loss: 359.87
train mean loss: 362.65
epoch train time: 0:00:02.354345
elapsed time: 0:09:42.468485
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 22:03:39.363903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.44
 ---- batch: 020 ----
mean loss: 353.64
 ---- batch: 030 ----
mean loss: 363.81
 ---- batch: 040 ----
mean loss: 353.96
 ---- batch: 050 ----
mean loss: 363.47
 ---- batch: 060 ----
mean loss: 368.37
 ---- batch: 070 ----
mean loss: 354.02
 ---- batch: 080 ----
mean loss: 367.15
 ---- batch: 090 ----
mean loss: 360.08
train mean loss: 360.92
epoch train time: 0:00:02.345132
elapsed time: 0:09:44.813850
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 22:03:41.709232
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.11
 ---- batch: 020 ----
mean loss: 366.33
 ---- batch: 030 ----
mean loss: 354.56
 ---- batch: 040 ----
mean loss: 365.33
 ---- batch: 050 ----
mean loss: 358.99
 ---- batch: 060 ----
mean loss: 360.42
 ---- batch: 070 ----
mean loss: 370.97
 ---- batch: 080 ----
mean loss: 357.95
 ---- batch: 090 ----
mean loss: 366.79
train mean loss: 360.73
epoch train time: 0:00:02.337094
elapsed time: 0:09:47.151143
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 22:03:44.046562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.54
 ---- batch: 020 ----
mean loss: 353.93
 ---- batch: 030 ----
mean loss: 364.73
 ---- batch: 040 ----
mean loss: 355.05
 ---- batch: 050 ----
mean loss: 350.68
 ---- batch: 060 ----
mean loss: 351.80
 ---- batch: 070 ----
mean loss: 355.55
 ---- batch: 080 ----
mean loss: 368.90
 ---- batch: 090 ----
mean loss: 362.12
train mean loss: 358.96
epoch train time: 0:00:02.342521
elapsed time: 0:09:49.493900
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 22:03:46.389281
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.52
 ---- batch: 020 ----
mean loss: 361.95
 ---- batch: 030 ----
mean loss: 362.87
 ---- batch: 040 ----
mean loss: 347.85
 ---- batch: 050 ----
mean loss: 361.83
 ---- batch: 060 ----
mean loss: 355.62
 ---- batch: 070 ----
mean loss: 354.18
 ---- batch: 080 ----
mean loss: 361.26
 ---- batch: 090 ----
mean loss: 356.95
train mean loss: 357.53
epoch train time: 0:00:02.340483
elapsed time: 0:09:51.834564
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 22:03:48.729948
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.64
 ---- batch: 020 ----
mean loss: 353.95
 ---- batch: 030 ----
mean loss: 352.55
 ---- batch: 040 ----
mean loss: 348.45
 ---- batch: 050 ----
mean loss: 366.34
 ---- batch: 060 ----
mean loss: 352.22
 ---- batch: 070 ----
mean loss: 346.12
 ---- batch: 080 ----
mean loss: 350.20
 ---- batch: 090 ----
mean loss: 355.55
train mean loss: 354.86
epoch train time: 0:00:02.337324
elapsed time: 0:09:54.172068
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 22:03:51.067461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.44
 ---- batch: 020 ----
mean loss: 363.80
 ---- batch: 030 ----
mean loss: 368.93
 ---- batch: 040 ----
mean loss: 361.99
 ---- batch: 050 ----
mean loss: 361.87
 ---- batch: 060 ----
mean loss: 365.95
 ---- batch: 070 ----
mean loss: 366.52
 ---- batch: 080 ----
mean loss: 361.72
 ---- batch: 090 ----
mean loss: 332.61
train mean loss: 360.78
epoch train time: 0:00:02.329502
elapsed time: 0:09:56.501780
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 22:03:53.397165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 372.20
 ---- batch: 020 ----
mean loss: 361.66
 ---- batch: 030 ----
mean loss: 361.01
 ---- batch: 040 ----
mean loss: 354.19
 ---- batch: 050 ----
mean loss: 360.74
 ---- batch: 060 ----
mean loss: 355.08
 ---- batch: 070 ----
mean loss: 364.88
 ---- batch: 080 ----
mean loss: 364.87
 ---- batch: 090 ----
mean loss: 370.32
train mean loss: 362.36
epoch train time: 0:00:02.333847
elapsed time: 0:09:58.835814
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 22:03:55.731195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.34
 ---- batch: 020 ----
mean loss: 356.40
 ---- batch: 030 ----
mean loss: 358.66
 ---- batch: 040 ----
mean loss: 348.78
 ---- batch: 050 ----
mean loss: 366.92
 ---- batch: 060 ----
mean loss: 354.46
 ---- batch: 070 ----
mean loss: 360.98
 ---- batch: 080 ----
mean loss: 359.72
 ---- batch: 090 ----
mean loss: 350.86
train mean loss: 355.99
epoch train time: 0:00:02.332689
elapsed time: 0:10:01.168695
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 22:03:58.064119
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.65
 ---- batch: 020 ----
mean loss: 362.39
 ---- batch: 030 ----
mean loss: 366.59
 ---- batch: 040 ----
mean loss: 343.40
 ---- batch: 050 ----
mean loss: 359.84
 ---- batch: 060 ----
mean loss: 352.32
 ---- batch: 070 ----
mean loss: 366.04
 ---- batch: 080 ----
mean loss: 362.69
 ---- batch: 090 ----
mean loss: 368.47
train mean loss: 357.75
epoch train time: 0:00:02.329570
elapsed time: 0:10:03.498504
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 22:04:00.393887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.90
 ---- batch: 020 ----
mean loss: 366.92
 ---- batch: 030 ----
mean loss: 354.32
 ---- batch: 040 ----
mean loss: 353.54
 ---- batch: 050 ----
mean loss: 360.29
 ---- batch: 060 ----
mean loss: 358.26
 ---- batch: 070 ----
mean loss: 364.91
 ---- batch: 080 ----
mean loss: 356.64
 ---- batch: 090 ----
mean loss: 356.66
train mean loss: 359.16
epoch train time: 0:00:02.334538
elapsed time: 0:10:05.833423
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 22:04:02.728839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.38
 ---- batch: 020 ----
mean loss: 359.32
 ---- batch: 030 ----
mean loss: 360.08
 ---- batch: 040 ----
mean loss: 363.82
 ---- batch: 050 ----
mean loss: 354.31
 ---- batch: 060 ----
mean loss: 354.59
 ---- batch: 070 ----
mean loss: 365.27
 ---- batch: 080 ----
mean loss: 361.18
 ---- batch: 090 ----
mean loss: 365.17
train mean loss: 357.67
epoch train time: 0:00:02.333123
elapsed time: 0:10:08.166762
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 22:04:05.062164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.01
 ---- batch: 020 ----
mean loss: 352.46
 ---- batch: 030 ----
mean loss: 348.42
 ---- batch: 040 ----
mean loss: 370.32
 ---- batch: 050 ----
mean loss: 369.63
 ---- batch: 060 ----
mean loss: 353.86
 ---- batch: 070 ----
mean loss: 360.29
 ---- batch: 080 ----
mean loss: 347.91
 ---- batch: 090 ----
mean loss: 359.76
train mean loss: 358.61
epoch train time: 0:00:02.336370
elapsed time: 0:10:10.503356
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 22:04:07.398739
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.24
 ---- batch: 020 ----
mean loss: 356.49
 ---- batch: 030 ----
mean loss: 357.39
 ---- batch: 040 ----
mean loss: 367.89
 ---- batch: 050 ----
mean loss: 362.26
 ---- batch: 060 ----
mean loss: 359.26
 ---- batch: 070 ----
mean loss: 350.42
 ---- batch: 080 ----
mean loss: 370.12
 ---- batch: 090 ----
mean loss: 366.20
train mean loss: 360.27
epoch train time: 0:00:02.335322
elapsed time: 0:10:12.838863
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 22:04:09.734249
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.89
 ---- batch: 020 ----
mean loss: 362.65
 ---- batch: 030 ----
mean loss: 357.31
 ---- batch: 040 ----
mean loss: 362.95
 ---- batch: 050 ----
mean loss: 363.14
 ---- batch: 060 ----
mean loss: 360.24
 ---- batch: 070 ----
mean loss: 363.96
 ---- batch: 080 ----
mean loss: 362.86
 ---- batch: 090 ----
mean loss: 346.19
train mean loss: 360.11
epoch train time: 0:00:02.329985
elapsed time: 0:10:15.169031
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 22:04:12.064414
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.94
 ---- batch: 020 ----
mean loss: 355.68
 ---- batch: 030 ----
mean loss: 364.29
 ---- batch: 040 ----
mean loss: 361.25
 ---- batch: 050 ----
mean loss: 356.81
 ---- batch: 060 ----
mean loss: 363.66
 ---- batch: 070 ----
mean loss: 351.01
 ---- batch: 080 ----
mean loss: 358.82
 ---- batch: 090 ----
mean loss: 359.19
train mean loss: 357.47
epoch train time: 0:00:02.332972
elapsed time: 0:10:17.505593
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_7/checkpoint.pth.tar
**** end time: 2019-09-25 22:04:14.400934 ****
