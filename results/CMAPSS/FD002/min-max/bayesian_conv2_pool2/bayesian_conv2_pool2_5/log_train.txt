Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_5', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 19068
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-25 18:22:47.035967 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 16, 11]           1,120
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
    BayesianConv2d-4            [-1, 14, 7, 11]             448
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
    BayesianLinear-8                    [-1, 1]             924
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,492
Trainable params: 2,492
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 18:22:47.046554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4295.73
 ---- batch: 020 ----
mean loss: 4115.76
 ---- batch: 030 ----
mean loss: 3986.62
 ---- batch: 040 ----
mean loss: 3698.09
 ---- batch: 050 ----
mean loss: 3363.20
 ---- batch: 060 ----
mean loss: 3195.81
 ---- batch: 070 ----
mean loss: 2877.57
 ---- batch: 080 ----
mean loss: 2680.13
 ---- batch: 090 ----
mean loss: 2457.40
train mean loss: 3336.38
epoch train time: 0:00:35.463927
elapsed time: 0:00:35.477732
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 18:23:22.513749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2071.54
 ---- batch: 020 ----
mean loss: 1945.60
 ---- batch: 030 ----
mean loss: 1756.99
 ---- batch: 040 ----
mean loss: 1609.25
 ---- batch: 050 ----
mean loss: 1446.36
 ---- batch: 060 ----
mean loss: 1346.12
 ---- batch: 070 ----
mean loss: 1246.33
 ---- batch: 080 ----
mean loss: 1169.95
 ---- batch: 090 ----
mean loss: 1122.05
train mean loss: 1494.27
epoch train time: 0:00:02.633238
elapsed time: 0:00:38.111317
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 18:23:25.147450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1025.66
 ---- batch: 020 ----
mean loss: 980.81
 ---- batch: 030 ----
mean loss: 955.44
 ---- batch: 040 ----
mean loss: 954.94
 ---- batch: 050 ----
mean loss: 943.49
 ---- batch: 060 ----
mean loss: 907.63
 ---- batch: 070 ----
mean loss: 915.49
 ---- batch: 080 ----
mean loss: 883.36
 ---- batch: 090 ----
mean loss: 897.13
train mean loss: 937.52
epoch train time: 0:00:02.650731
elapsed time: 0:00:40.762524
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 18:23:27.798666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.66
 ---- batch: 020 ----
mean loss: 889.68
 ---- batch: 030 ----
mean loss: 890.37
 ---- batch: 040 ----
mean loss: 893.28
 ---- batch: 050 ----
mean loss: 876.12
 ---- batch: 060 ----
mean loss: 880.05
 ---- batch: 070 ----
mean loss: 898.73
 ---- batch: 080 ----
mean loss: 895.39
 ---- batch: 090 ----
mean loss: 878.47
train mean loss: 889.35
epoch train time: 0:00:02.687284
elapsed time: 0:00:43.450283
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 18:23:30.486429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.53
 ---- batch: 020 ----
mean loss: 873.97
 ---- batch: 030 ----
mean loss: 890.53
 ---- batch: 040 ----
mean loss: 888.19
 ---- batch: 050 ----
mean loss: 870.58
 ---- batch: 060 ----
mean loss: 884.25
 ---- batch: 070 ----
mean loss: 895.65
 ---- batch: 080 ----
mean loss: 892.60
 ---- batch: 090 ----
mean loss: 873.73
train mean loss: 884.27
epoch train time: 0:00:02.665131
elapsed time: 0:00:46.115867
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 18:23:33.152008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.31
 ---- batch: 020 ----
mean loss: 881.90
 ---- batch: 030 ----
mean loss: 891.89
 ---- batch: 040 ----
mean loss: 882.80
 ---- batch: 050 ----
mean loss: 879.62
 ---- batch: 060 ----
mean loss: 855.12
 ---- batch: 070 ----
mean loss: 891.56
 ---- batch: 080 ----
mean loss: 883.27
 ---- batch: 090 ----
mean loss: 869.04
train mean loss: 880.93
epoch train time: 0:00:02.667464
elapsed time: 0:00:48.783780
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 18:23:35.819914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.71
 ---- batch: 020 ----
mean loss: 879.00
 ---- batch: 030 ----
mean loss: 888.18
 ---- batch: 040 ----
mean loss: 888.19
 ---- batch: 050 ----
mean loss: 887.07
 ---- batch: 060 ----
mean loss: 876.21
 ---- batch: 070 ----
mean loss: 871.41
 ---- batch: 080 ----
mean loss: 876.60
 ---- batch: 090 ----
mean loss: 871.93
train mean loss: 877.93
epoch train time: 0:00:02.629940
elapsed time: 0:00:51.414176
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 18:23:38.450320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.11
 ---- batch: 020 ----
mean loss: 880.20
 ---- batch: 030 ----
mean loss: 912.10
 ---- batch: 040 ----
mean loss: 877.08
 ---- batch: 050 ----
mean loss: 871.44
 ---- batch: 060 ----
mean loss: 869.02
 ---- batch: 070 ----
mean loss: 886.66
 ---- batch: 080 ----
mean loss: 868.31
 ---- batch: 090 ----
mean loss: 851.08
train mean loss: 873.86
epoch train time: 0:00:02.612812
elapsed time: 0:00:54.027498
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 18:23:41.063633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.96
 ---- batch: 020 ----
mean loss: 865.25
 ---- batch: 030 ----
mean loss: 873.32
 ---- batch: 040 ----
mean loss: 894.81
 ---- batch: 050 ----
mean loss: 856.55
 ---- batch: 060 ----
mean loss: 862.64
 ---- batch: 070 ----
mean loss: 859.78
 ---- batch: 080 ----
mean loss: 854.30
 ---- batch: 090 ----
mean loss: 879.89
train mean loss: 868.73
epoch train time: 0:00:02.662432
elapsed time: 0:00:56.690390
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 18:23:43.726549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.61
 ---- batch: 020 ----
mean loss: 869.71
 ---- batch: 030 ----
mean loss: 841.51
 ---- batch: 040 ----
mean loss: 863.10
 ---- batch: 050 ----
mean loss: 874.84
 ---- batch: 060 ----
mean loss: 878.56
 ---- batch: 070 ----
mean loss: 868.39
 ---- batch: 080 ----
mean loss: 853.64
 ---- batch: 090 ----
mean loss: 858.98
train mean loss: 864.46
epoch train time: 0:00:02.664621
elapsed time: 0:00:59.355539
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 18:23:46.391718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.17
 ---- batch: 020 ----
mean loss: 874.40
 ---- batch: 030 ----
mean loss: 859.45
 ---- batch: 040 ----
mean loss: 869.48
 ---- batch: 050 ----
mean loss: 854.95
 ---- batch: 060 ----
mean loss: 863.65
 ---- batch: 070 ----
mean loss: 859.17
 ---- batch: 080 ----
mean loss: 851.87
 ---- batch: 090 ----
mean loss: 860.96
train mean loss: 859.35
epoch train time: 0:00:02.660337
elapsed time: 0:01:02.016465
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 18:23:49.052604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.27
 ---- batch: 020 ----
mean loss: 843.73
 ---- batch: 030 ----
mean loss: 860.09
 ---- batch: 040 ----
mean loss: 881.37
 ---- batch: 050 ----
mean loss: 875.08
 ---- batch: 060 ----
mean loss: 861.28
 ---- batch: 070 ----
mean loss: 870.40
 ---- batch: 080 ----
mean loss: 849.08
 ---- batch: 090 ----
mean loss: 847.18
train mean loss: 855.20
epoch train time: 0:00:02.621939
elapsed time: 0:01:04.638900
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 18:23:51.675032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.51
 ---- batch: 020 ----
mean loss: 856.39
 ---- batch: 030 ----
mean loss: 856.94
 ---- batch: 040 ----
mean loss: 841.43
 ---- batch: 050 ----
mean loss: 859.62
 ---- batch: 060 ----
mean loss: 854.70
 ---- batch: 070 ----
mean loss: 831.05
 ---- batch: 080 ----
mean loss: 847.87
 ---- batch: 090 ----
mean loss: 864.21
train mean loss: 850.94
epoch train time: 0:00:02.621422
elapsed time: 0:01:07.260795
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 18:23:54.296928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.17
 ---- batch: 020 ----
mean loss: 849.03
 ---- batch: 030 ----
mean loss: 845.26
 ---- batch: 040 ----
mean loss: 824.40
 ---- batch: 050 ----
mean loss: 852.86
 ---- batch: 060 ----
mean loss: 838.46
 ---- batch: 070 ----
mean loss: 868.25
 ---- batch: 080 ----
mean loss: 840.86
 ---- batch: 090 ----
mean loss: 844.58
train mean loss: 845.66
epoch train time: 0:00:02.626289
elapsed time: 0:01:09.887568
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 18:23:56.923711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.53
 ---- batch: 020 ----
mean loss: 841.19
 ---- batch: 030 ----
mean loss: 846.04
 ---- batch: 040 ----
mean loss: 840.70
 ---- batch: 050 ----
mean loss: 833.66
 ---- batch: 060 ----
mean loss: 829.49
 ---- batch: 070 ----
mean loss: 833.43
 ---- batch: 080 ----
mean loss: 853.45
 ---- batch: 090 ----
mean loss: 841.79
train mean loss: 842.33
epoch train time: 0:00:02.625795
elapsed time: 0:01:12.513854
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 18:23:59.550008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.06
 ---- batch: 020 ----
mean loss: 844.61
 ---- batch: 030 ----
mean loss: 830.45
 ---- batch: 040 ----
mean loss: 847.80
 ---- batch: 050 ----
mean loss: 843.61
 ---- batch: 060 ----
mean loss: 826.80
 ---- batch: 070 ----
mean loss: 819.47
 ---- batch: 080 ----
mean loss: 829.49
 ---- batch: 090 ----
mean loss: 842.24
train mean loss: 835.65
epoch train time: 0:00:02.676022
elapsed time: 0:01:15.190374
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 18:24:02.226591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.05
 ---- batch: 020 ----
mean loss: 819.27
 ---- batch: 030 ----
mean loss: 827.90
 ---- batch: 040 ----
mean loss: 839.69
 ---- batch: 050 ----
mean loss: 814.88
 ---- batch: 060 ----
mean loss: 841.18
 ---- batch: 070 ----
mean loss: 843.67
 ---- batch: 080 ----
mean loss: 846.06
 ---- batch: 090 ----
mean loss: 814.75
train mean loss: 832.52
epoch train time: 0:00:02.662098
elapsed time: 0:01:17.852973
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 18:24:04.889106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.50
 ---- batch: 020 ----
mean loss: 822.14
 ---- batch: 030 ----
mean loss: 843.78
 ---- batch: 040 ----
mean loss: 837.87
 ---- batch: 050 ----
mean loss: 811.06
 ---- batch: 060 ----
mean loss: 809.45
 ---- batch: 070 ----
mean loss: 827.64
 ---- batch: 080 ----
mean loss: 824.89
 ---- batch: 090 ----
mean loss: 822.34
train mean loss: 827.11
epoch train time: 0:00:02.632470
elapsed time: 0:01:20.485946
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 18:24:07.522092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.32
 ---- batch: 020 ----
mean loss: 836.07
 ---- batch: 030 ----
mean loss: 807.11
 ---- batch: 040 ----
mean loss: 827.69
 ---- batch: 050 ----
mean loss: 824.79
 ---- batch: 060 ----
mean loss: 822.33
 ---- batch: 070 ----
mean loss: 803.37
 ---- batch: 080 ----
mean loss: 830.25
 ---- batch: 090 ----
mean loss: 829.11
train mean loss: 822.60
epoch train time: 0:00:02.629571
elapsed time: 0:01:23.116017
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 18:24:10.152209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.76
 ---- batch: 020 ----
mean loss: 832.25
 ---- batch: 030 ----
mean loss: 818.86
 ---- batch: 040 ----
mean loss: 821.50
 ---- batch: 050 ----
mean loss: 807.86
 ---- batch: 060 ----
mean loss: 820.04
 ---- batch: 070 ----
mean loss: 820.22
 ---- batch: 080 ----
mean loss: 809.19
 ---- batch: 090 ----
mean loss: 807.35
train mean loss: 817.27
epoch train time: 0:00:02.619667
elapsed time: 0:01:25.736181
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 18:24:12.772318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.21
 ---- batch: 020 ----
mean loss: 810.11
 ---- batch: 030 ----
mean loss: 802.11
 ---- batch: 040 ----
mean loss: 809.89
 ---- batch: 050 ----
mean loss: 823.91
 ---- batch: 060 ----
mean loss: 808.84
 ---- batch: 070 ----
mean loss: 795.95
 ---- batch: 080 ----
mean loss: 821.90
 ---- batch: 090 ----
mean loss: 812.80
train mean loss: 810.36
epoch train time: 0:00:02.678836
elapsed time: 0:01:28.415467
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 18:24:15.451588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 804.09
 ---- batch: 020 ----
mean loss: 818.32
 ---- batch: 030 ----
mean loss: 800.85
 ---- batch: 040 ----
mean loss: 790.37
 ---- batch: 050 ----
mean loss: 801.21
 ---- batch: 060 ----
mean loss: 823.46
 ---- batch: 070 ----
mean loss: 806.78
 ---- batch: 080 ----
mean loss: 804.67
 ---- batch: 090 ----
mean loss: 806.80
train mean loss: 806.86
epoch train time: 0:00:02.652745
elapsed time: 0:01:31.068640
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 18:24:18.104775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.24
 ---- batch: 020 ----
mean loss: 795.75
 ---- batch: 030 ----
mean loss: 785.06
 ---- batch: 040 ----
mean loss: 794.18
 ---- batch: 050 ----
mean loss: 799.62
 ---- batch: 060 ----
mean loss: 798.79
 ---- batch: 070 ----
mean loss: 800.76
 ---- batch: 080 ----
mean loss: 799.59
 ---- batch: 090 ----
mean loss: 805.83
train mean loss: 798.68
epoch train time: 0:00:02.651520
elapsed time: 0:01:33.720616
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 18:24:20.756764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.80
 ---- batch: 020 ----
mean loss: 788.07
 ---- batch: 030 ----
mean loss: 794.54
 ---- batch: 040 ----
mean loss: 779.69
 ---- batch: 050 ----
mean loss: 788.44
 ---- batch: 060 ----
mean loss: 781.56
 ---- batch: 070 ----
mean loss: 794.10
 ---- batch: 080 ----
mean loss: 797.44
 ---- batch: 090 ----
mean loss: 787.15
train mean loss: 792.35
epoch train time: 0:00:02.633299
elapsed time: 0:01:36.354447
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 18:24:23.390602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.66
 ---- batch: 020 ----
mean loss: 800.59
 ---- batch: 030 ----
mean loss: 790.33
 ---- batch: 040 ----
mean loss: 792.81
 ---- batch: 050 ----
mean loss: 788.83
 ---- batch: 060 ----
mean loss: 781.80
 ---- batch: 070 ----
mean loss: 780.83
 ---- batch: 080 ----
mean loss: 776.21
 ---- batch: 090 ----
mean loss: 786.20
train mean loss: 784.06
epoch train time: 0:00:02.586883
elapsed time: 0:01:38.941849
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 18:24:25.977988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 775.70
 ---- batch: 020 ----
mean loss: 775.21
 ---- batch: 030 ----
mean loss: 770.60
 ---- batch: 040 ----
mean loss: 766.21
 ---- batch: 050 ----
mean loss: 768.54
 ---- batch: 060 ----
mean loss: 795.62
 ---- batch: 070 ----
mean loss: 782.07
 ---- batch: 080 ----
mean loss: 776.78
 ---- batch: 090 ----
mean loss: 771.11
train mean loss: 776.47
epoch train time: 0:00:02.635689
elapsed time: 0:01:41.578003
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 18:24:28.614161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 772.36
 ---- batch: 020 ----
mean loss: 778.38
 ---- batch: 030 ----
mean loss: 763.17
 ---- batch: 040 ----
mean loss: 769.33
 ---- batch: 050 ----
mean loss: 755.99
 ---- batch: 060 ----
mean loss: 767.30
 ---- batch: 070 ----
mean loss: 774.75
 ---- batch: 080 ----
mean loss: 773.80
 ---- batch: 090 ----
mean loss: 772.92
train mean loss: 768.31
epoch train time: 0:00:02.650488
elapsed time: 0:01:44.229046
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 18:24:31.265225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 775.37
 ---- batch: 020 ----
mean loss: 758.56
 ---- batch: 030 ----
mean loss: 762.14
 ---- batch: 040 ----
mean loss: 767.84
 ---- batch: 050 ----
mean loss: 762.06
 ---- batch: 060 ----
mean loss: 751.68
 ---- batch: 070 ----
mean loss: 742.06
 ---- batch: 080 ----
mean loss: 773.65
 ---- batch: 090 ----
mean loss: 738.94
train mean loss: 758.11
epoch train time: 0:00:02.671224
elapsed time: 0:01:46.900770
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 18:24:33.936920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.32
 ---- batch: 020 ----
mean loss: 749.00
 ---- batch: 030 ----
mean loss: 739.24
 ---- batch: 040 ----
mean loss: 748.06
 ---- batch: 050 ----
mean loss: 758.56
 ---- batch: 060 ----
mean loss: 756.12
 ---- batch: 070 ----
mean loss: 764.12
 ---- batch: 080 ----
mean loss: 741.00
 ---- batch: 090 ----
mean loss: 733.49
train mean loss: 749.79
epoch train time: 0:00:02.614527
elapsed time: 0:01:49.515765
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 18:24:36.551907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 752.58
 ---- batch: 020 ----
mean loss: 747.85
 ---- batch: 030 ----
mean loss: 737.36
 ---- batch: 040 ----
mean loss: 737.62
 ---- batch: 050 ----
mean loss: 744.15
 ---- batch: 060 ----
mean loss: 748.11
 ---- batch: 070 ----
mean loss: 737.36
 ---- batch: 080 ----
mean loss: 734.95
 ---- batch: 090 ----
mean loss: 716.18
train mean loss: 738.51
epoch train time: 0:00:02.630596
elapsed time: 0:01:52.146825
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 18:24:39.182976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 726.92
 ---- batch: 020 ----
mean loss: 731.86
 ---- batch: 030 ----
mean loss: 725.64
 ---- batch: 040 ----
mean loss: 735.32
 ---- batch: 050 ----
mean loss: 746.29
 ---- batch: 060 ----
mean loss: 722.36
 ---- batch: 070 ----
mean loss: 741.68
 ---- batch: 080 ----
mean loss: 723.68
 ---- batch: 090 ----
mean loss: 710.58
train mean loss: 729.24
epoch train time: 0:00:02.611160
elapsed time: 0:01:54.758518
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 18:24:41.794736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 730.45
 ---- batch: 020 ----
mean loss: 728.86
 ---- batch: 030 ----
mean loss: 720.73
 ---- batch: 040 ----
mean loss: 707.97
 ---- batch: 050 ----
mean loss: 712.65
 ---- batch: 060 ----
mean loss: 727.42
 ---- batch: 070 ----
mean loss: 699.55
 ---- batch: 080 ----
mean loss: 732.25
 ---- batch: 090 ----
mean loss: 712.34
train mean loss: 718.36
epoch train time: 0:00:02.640823
elapsed time: 0:01:57.399900
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 18:24:44.436159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.46
 ---- batch: 020 ----
mean loss: 714.85
 ---- batch: 030 ----
mean loss: 712.35
 ---- batch: 040 ----
mean loss: 706.55
 ---- batch: 050 ----
mean loss: 701.51
 ---- batch: 060 ----
mean loss: 705.41
 ---- batch: 070 ----
mean loss: 704.49
 ---- batch: 080 ----
mean loss: 706.52
 ---- batch: 090 ----
mean loss: 700.13
train mean loss: 708.39
epoch train time: 0:00:02.662545
elapsed time: 0:02:00.063014
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 18:24:47.099151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.23
 ---- batch: 020 ----
mean loss: 705.33
 ---- batch: 030 ----
mean loss: 701.06
 ---- batch: 040 ----
mean loss: 698.12
 ---- batch: 050 ----
mean loss: 707.99
 ---- batch: 060 ----
mean loss: 695.76
 ---- batch: 070 ----
mean loss: 695.56
 ---- batch: 080 ----
mean loss: 681.89
 ---- batch: 090 ----
mean loss: 687.90
train mean loss: 697.36
epoch train time: 0:00:02.617502
elapsed time: 0:02:02.681036
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 18:24:49.717175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.45
 ---- batch: 020 ----
mean loss: 689.38
 ---- batch: 030 ----
mean loss: 678.51
 ---- batch: 040 ----
mean loss: 689.47
 ---- batch: 050 ----
mean loss: 676.97
 ---- batch: 060 ----
mean loss: 693.55
 ---- batch: 070 ----
mean loss: 688.17
 ---- batch: 080 ----
mean loss: 678.81
 ---- batch: 090 ----
mean loss: 691.60
train mean loss: 686.11
epoch train time: 0:00:02.595918
elapsed time: 0:02:05.277437
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 18:24:52.313572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.94
 ---- batch: 020 ----
mean loss: 678.45
 ---- batch: 030 ----
mean loss: 683.39
 ---- batch: 040 ----
mean loss: 684.45
 ---- batch: 050 ----
mean loss: 683.01
 ---- batch: 060 ----
mean loss: 656.69
 ---- batch: 070 ----
mean loss: 663.13
 ---- batch: 080 ----
mean loss: 664.71
 ---- batch: 090 ----
mean loss: 694.28
train mean loss: 675.49
epoch train time: 0:00:02.639167
elapsed time: 0:02:07.917068
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 18:24:54.953210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.25
 ---- batch: 020 ----
mean loss: 666.89
 ---- batch: 030 ----
mean loss: 663.68
 ---- batch: 040 ----
mean loss: 677.05
 ---- batch: 050 ----
mean loss: 675.55
 ---- batch: 060 ----
mean loss: 663.17
 ---- batch: 070 ----
mean loss: 663.57
 ---- batch: 080 ----
mean loss: 640.42
 ---- batch: 090 ----
mean loss: 650.59
train mean loss: 664.40
epoch train time: 0:00:02.635915
elapsed time: 0:02:10.553433
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 18:24:57.589584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.82
 ---- batch: 020 ----
mean loss: 664.29
 ---- batch: 030 ----
mean loss: 654.04
 ---- batch: 040 ----
mean loss: 664.34
 ---- batch: 050 ----
mean loss: 651.75
 ---- batch: 060 ----
mean loss: 650.13
 ---- batch: 070 ----
mean loss: 649.76
 ---- batch: 080 ----
mean loss: 652.06
 ---- batch: 090 ----
mean loss: 637.34
train mean loss: 652.85
epoch train time: 0:00:02.614311
elapsed time: 0:02:13.168194
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 18:25:00.204333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 652.11
 ---- batch: 020 ----
mean loss: 643.30
 ---- batch: 030 ----
mean loss: 638.41
 ---- batch: 040 ----
mean loss: 646.28
 ---- batch: 050 ----
mean loss: 634.80
 ---- batch: 060 ----
mean loss: 645.40
 ---- batch: 070 ----
mean loss: 648.71
 ---- batch: 080 ----
mean loss: 634.35
 ---- batch: 090 ----
mean loss: 638.71
train mean loss: 642.24
epoch train time: 0:00:02.622254
elapsed time: 0:02:15.790918
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 18:25:02.827103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.60
 ---- batch: 020 ----
mean loss: 627.21
 ---- batch: 030 ----
mean loss: 626.83
 ---- batch: 040 ----
mean loss: 638.74
 ---- batch: 050 ----
mean loss: 635.57
 ---- batch: 060 ----
mean loss: 629.52
 ---- batch: 070 ----
mean loss: 626.14
 ---- batch: 080 ----
mean loss: 623.02
 ---- batch: 090 ----
mean loss: 634.16
train mean loss: 631.69
epoch train time: 0:00:02.604429
elapsed time: 0:02:18.395837
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 18:25:05.431979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 634.34
 ---- batch: 020 ----
mean loss: 627.78
 ---- batch: 030 ----
mean loss: 619.20
 ---- batch: 040 ----
mean loss: 618.15
 ---- batch: 050 ----
mean loss: 624.80
 ---- batch: 060 ----
mean loss: 624.96
 ---- batch: 070 ----
mean loss: 622.93
 ---- batch: 080 ----
mean loss: 616.90
 ---- batch: 090 ----
mean loss: 609.79
train mean loss: 621.83
epoch train time: 0:00:02.647729
elapsed time: 0:02:21.044051
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 18:25:08.080187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.26
 ---- batch: 020 ----
mean loss: 617.32
 ---- batch: 030 ----
mean loss: 616.83
 ---- batch: 040 ----
mean loss: 614.65
 ---- batch: 050 ----
mean loss: 601.06
 ---- batch: 060 ----
mean loss: 621.26
 ---- batch: 070 ----
mean loss: 605.40
 ---- batch: 080 ----
mean loss: 604.03
 ---- batch: 090 ----
mean loss: 612.62
train mean loss: 611.33
epoch train time: 0:00:02.637319
elapsed time: 0:02:23.681850
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 18:25:10.718007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 599.93
 ---- batch: 020 ----
mean loss: 593.09
 ---- batch: 030 ----
mean loss: 603.74
 ---- batch: 040 ----
mean loss: 583.19
 ---- batch: 050 ----
mean loss: 599.75
 ---- batch: 060 ----
mean loss: 605.91
 ---- batch: 070 ----
mean loss: 601.55
 ---- batch: 080 ----
mean loss: 611.16
 ---- batch: 090 ----
mean loss: 608.90
train mean loss: 602.35
epoch train time: 0:00:02.634637
elapsed time: 0:02:26.316974
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 18:25:13.353113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.73
 ---- batch: 020 ----
mean loss: 596.86
 ---- batch: 030 ----
mean loss: 602.00
 ---- batch: 040 ----
mean loss: 597.21
 ---- batch: 050 ----
mean loss: 585.41
 ---- batch: 060 ----
mean loss: 589.18
 ---- batch: 070 ----
mean loss: 586.40
 ---- batch: 080 ----
mean loss: 591.60
 ---- batch: 090 ----
mean loss: 591.44
train mean loss: 592.15
epoch train time: 0:00:02.623417
elapsed time: 0:02:28.940865
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 18:25:15.977008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 583.30
 ---- batch: 020 ----
mean loss: 582.65
 ---- batch: 030 ----
mean loss: 602.48
 ---- batch: 040 ----
mean loss: 579.54
 ---- batch: 050 ----
mean loss: 577.75
 ---- batch: 060 ----
mean loss: 584.74
 ---- batch: 070 ----
mean loss: 582.30
 ---- batch: 080 ----
mean loss: 570.73
 ---- batch: 090 ----
mean loss: 586.73
train mean loss: 583.27
epoch train time: 0:00:02.609681
elapsed time: 0:02:31.551047
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 18:25:18.587201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 581.09
 ---- batch: 020 ----
mean loss: 584.14
 ---- batch: 030 ----
mean loss: 580.40
 ---- batch: 040 ----
mean loss: 575.57
 ---- batch: 050 ----
mean loss: 571.32
 ---- batch: 060 ----
mean loss: 570.64
 ---- batch: 070 ----
mean loss: 580.65
 ---- batch: 080 ----
mean loss: 563.51
 ---- batch: 090 ----
mean loss: 571.57
train mean loss: 574.88
epoch train time: 0:00:02.630490
elapsed time: 0:02:34.182036
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 18:25:21.218184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.61
 ---- batch: 020 ----
mean loss: 568.91
 ---- batch: 030 ----
mean loss: 574.76
 ---- batch: 040 ----
mean loss: 559.63
 ---- batch: 050 ----
mean loss: 572.29
 ---- batch: 060 ----
mean loss: 570.50
 ---- batch: 070 ----
mean loss: 570.55
 ---- batch: 080 ----
mean loss: 552.64
 ---- batch: 090 ----
mean loss: 556.84
train mean loss: 566.16
epoch train time: 0:00:02.635841
elapsed time: 0:02:36.818393
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 18:25:23.854565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 561.82
 ---- batch: 020 ----
mean loss: 558.16
 ---- batch: 030 ----
mean loss: 557.04
 ---- batch: 040 ----
mean loss: 549.32
 ---- batch: 050 ----
mean loss: 577.90
 ---- batch: 060 ----
mean loss: 546.34
 ---- batch: 070 ----
mean loss: 559.01
 ---- batch: 080 ----
mean loss: 552.43
 ---- batch: 090 ----
mean loss: 559.25
train mean loss: 557.67
epoch train time: 0:00:02.631978
elapsed time: 0:02:39.450858
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 18:25:26.486998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 547.95
 ---- batch: 020 ----
mean loss: 542.57
 ---- batch: 030 ----
mean loss: 540.58
 ---- batch: 040 ----
mean loss: 538.98
 ---- batch: 050 ----
mean loss: 552.38
 ---- batch: 060 ----
mean loss: 539.73
 ---- batch: 070 ----
mean loss: 554.33
 ---- batch: 080 ----
mean loss: 550.89
 ---- batch: 090 ----
mean loss: 548.83
train mean loss: 545.87
epoch train time: 0:00:02.640940
elapsed time: 0:02:42.092254
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 18:25:29.128393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 537.80
 ---- batch: 020 ----
mean loss: 527.15
 ---- batch: 030 ----
mean loss: 536.49
 ---- batch: 040 ----
mean loss: 538.78
 ---- batch: 050 ----
mean loss: 516.82
 ---- batch: 060 ----
mean loss: 536.65
 ---- batch: 070 ----
mean loss: 517.43
 ---- batch: 080 ----
mean loss: 529.64
 ---- batch: 090 ----
mean loss: 521.08
train mean loss: 528.59
epoch train time: 0:00:02.660180
elapsed time: 0:02:44.752943
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 18:25:31.789086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.96
 ---- batch: 020 ----
mean loss: 517.65
 ---- batch: 030 ----
mean loss: 522.20
 ---- batch: 040 ----
mean loss: 510.11
 ---- batch: 050 ----
mean loss: 504.87
 ---- batch: 060 ----
mean loss: 506.59
 ---- batch: 070 ----
mean loss: 512.43
 ---- batch: 080 ----
mean loss: 506.24
 ---- batch: 090 ----
mean loss: 504.61
train mean loss: 510.83
epoch train time: 0:00:02.616860
elapsed time: 0:02:47.370246
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 18:25:34.406392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.72
 ---- batch: 020 ----
mean loss: 498.65
 ---- batch: 030 ----
mean loss: 502.90
 ---- batch: 040 ----
mean loss: 496.93
 ---- batch: 050 ----
mean loss: 485.01
 ---- batch: 060 ----
mean loss: 486.73
 ---- batch: 070 ----
mean loss: 495.04
 ---- batch: 080 ----
mean loss: 494.66
 ---- batch: 090 ----
mean loss: 497.29
train mean loss: 494.91
epoch train time: 0:00:02.625207
elapsed time: 0:02:49.995933
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 18:25:37.032083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.08
 ---- batch: 020 ----
mean loss: 482.44
 ---- batch: 030 ----
mean loss: 486.61
 ---- batch: 040 ----
mean loss: 473.14
 ---- batch: 050 ----
mean loss: 488.52
 ---- batch: 060 ----
mean loss: 483.44
 ---- batch: 070 ----
mean loss: 467.92
 ---- batch: 080 ----
mean loss: 477.27
 ---- batch: 090 ----
mean loss: 468.46
train mean loss: 479.10
epoch train time: 0:00:02.635177
elapsed time: 0:02:52.631635
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 18:25:39.667776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.58
 ---- batch: 020 ----
mean loss: 462.42
 ---- batch: 030 ----
mean loss: 486.00
 ---- batch: 040 ----
mean loss: 467.19
 ---- batch: 050 ----
mean loss: 468.32
 ---- batch: 060 ----
mean loss: 466.17
 ---- batch: 070 ----
mean loss: 457.24
 ---- batch: 080 ----
mean loss: 472.75
 ---- batch: 090 ----
mean loss: 449.16
train mean loss: 466.31
epoch train time: 0:00:02.666604
elapsed time: 0:02:55.298726
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 18:25:42.334830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.68
 ---- batch: 020 ----
mean loss: 451.36
 ---- batch: 030 ----
mean loss: 464.69
 ---- batch: 040 ----
mean loss: 448.11
 ---- batch: 050 ----
mean loss: 453.24
 ---- batch: 060 ----
mean loss: 458.98
 ---- batch: 070 ----
mean loss: 449.93
 ---- batch: 080 ----
mean loss: 441.24
 ---- batch: 090 ----
mean loss: 453.20
train mean loss: 454.27
epoch train time: 0:00:02.626505
elapsed time: 0:02:57.925712
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 18:25:44.961862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.61
 ---- batch: 020 ----
mean loss: 441.00
 ---- batch: 030 ----
mean loss: 446.86
 ---- batch: 040 ----
mean loss: 446.89
 ---- batch: 050 ----
mean loss: 442.02
 ---- batch: 060 ----
mean loss: 441.30
 ---- batch: 070 ----
mean loss: 450.43
 ---- batch: 080 ----
mean loss: 420.35
 ---- batch: 090 ----
mean loss: 433.54
train mean loss: 441.07
epoch train time: 0:00:02.623497
elapsed time: 0:03:00.549677
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 18:25:47.585822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.62
 ---- batch: 020 ----
mean loss: 426.33
 ---- batch: 030 ----
mean loss: 441.11
 ---- batch: 040 ----
mean loss: 422.66
 ---- batch: 050 ----
mean loss: 428.15
 ---- batch: 060 ----
mean loss: 420.85
 ---- batch: 070 ----
mean loss: 424.69
 ---- batch: 080 ----
mean loss: 422.90
 ---- batch: 090 ----
mean loss: 422.23
train mean loss: 427.57
epoch train time: 0:00:02.619308
elapsed time: 0:03:03.169514
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 18:25:50.205691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.10
 ---- batch: 020 ----
mean loss: 427.68
 ---- batch: 030 ----
mean loss: 425.49
 ---- batch: 040 ----
mean loss: 407.38
 ---- batch: 050 ----
mean loss: 425.46
 ---- batch: 060 ----
mean loss: 413.65
 ---- batch: 070 ----
mean loss: 405.10
 ---- batch: 080 ----
mean loss: 406.72
 ---- batch: 090 ----
mean loss: 404.80
train mean loss: 413.85
epoch train time: 0:00:02.608468
elapsed time: 0:03:05.778524
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 18:25:52.814668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.34
 ---- batch: 020 ----
mean loss: 396.50
 ---- batch: 030 ----
mean loss: 402.50
 ---- batch: 040 ----
mean loss: 398.84
 ---- batch: 050 ----
mean loss: 393.51
 ---- batch: 060 ----
mean loss: 387.47
 ---- batch: 070 ----
mean loss: 395.44
 ---- batch: 080 ----
mean loss: 392.87
 ---- batch: 090 ----
mean loss: 391.80
train mean loss: 396.11
epoch train time: 0:00:02.668393
elapsed time: 0:03:08.447417
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 18:25:55.483579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.18
 ---- batch: 020 ----
mean loss: 383.35
 ---- batch: 030 ----
mean loss: 379.37
 ---- batch: 040 ----
mean loss: 379.87
 ---- batch: 050 ----
mean loss: 375.19
 ---- batch: 060 ----
mean loss: 379.99
 ---- batch: 070 ----
mean loss: 373.79
 ---- batch: 080 ----
mean loss: 367.14
 ---- batch: 090 ----
mean loss: 372.63
train mean loss: 376.87
epoch train time: 0:00:02.637159
elapsed time: 0:03:11.085103
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 18:25:58.121330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.29
 ---- batch: 020 ----
mean loss: 360.39
 ---- batch: 030 ----
mean loss: 351.92
 ---- batch: 040 ----
mean loss: 368.72
 ---- batch: 050 ----
mean loss: 356.17
 ---- batch: 060 ----
mean loss: 353.63
 ---- batch: 070 ----
mean loss: 361.64
 ---- batch: 080 ----
mean loss: 365.44
 ---- batch: 090 ----
mean loss: 359.30
train mean loss: 359.14
epoch train time: 0:00:02.630426
elapsed time: 0:03:13.716093
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 18:26:00.752234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.23
 ---- batch: 020 ----
mean loss: 344.54
 ---- batch: 030 ----
mean loss: 344.22
 ---- batch: 040 ----
mean loss: 335.38
 ---- batch: 050 ----
mean loss: 338.88
 ---- batch: 060 ----
mean loss: 343.01
 ---- batch: 070 ----
mean loss: 338.76
 ---- batch: 080 ----
mean loss: 340.62
 ---- batch: 090 ----
mean loss: 349.83
train mean loss: 342.43
epoch train time: 0:00:02.603040
elapsed time: 0:03:16.319629
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 18:26:03.355795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.92
 ---- batch: 020 ----
mean loss: 328.78
 ---- batch: 030 ----
mean loss: 331.51
 ---- batch: 040 ----
mean loss: 323.30
 ---- batch: 050 ----
mean loss: 333.74
 ---- batch: 060 ----
mean loss: 330.68
 ---- batch: 070 ----
mean loss: 330.09
 ---- batch: 080 ----
mean loss: 328.34
 ---- batch: 090 ----
mean loss: 321.30
train mean loss: 329.14
epoch train time: 0:00:02.625494
elapsed time: 0:03:18.945665
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 18:26:05.981818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.22
 ---- batch: 020 ----
mean loss: 320.13
 ---- batch: 030 ----
mean loss: 321.44
 ---- batch: 040 ----
mean loss: 315.96
 ---- batch: 050 ----
mean loss: 329.73
 ---- batch: 060 ----
mean loss: 311.78
 ---- batch: 070 ----
mean loss: 310.30
 ---- batch: 080 ----
mean loss: 314.75
 ---- batch: 090 ----
mean loss: 311.92
train mean loss: 317.44
epoch train time: 0:00:02.629366
elapsed time: 0:03:21.575503
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 18:26:08.611643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.77
 ---- batch: 020 ----
mean loss: 309.64
 ---- batch: 030 ----
mean loss: 309.89
 ---- batch: 040 ----
mean loss: 315.20
 ---- batch: 050 ----
mean loss: 317.15
 ---- batch: 060 ----
mean loss: 304.28
 ---- batch: 070 ----
mean loss: 293.12
 ---- batch: 080 ----
mean loss: 303.03
 ---- batch: 090 ----
mean loss: 303.76
train mean loss: 308.15
epoch train time: 0:00:02.632221
elapsed time: 0:03:24.208175
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 18:26:11.244332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.66
 ---- batch: 020 ----
mean loss: 302.31
 ---- batch: 030 ----
mean loss: 300.06
 ---- batch: 040 ----
mean loss: 294.85
 ---- batch: 050 ----
mean loss: 300.61
 ---- batch: 060 ----
mean loss: 304.16
 ---- batch: 070 ----
mean loss: 295.38
 ---- batch: 080 ----
mean loss: 299.68
 ---- batch: 090 ----
mean loss: 294.90
train mean loss: 298.75
epoch train time: 0:00:02.644054
elapsed time: 0:03:26.852720
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 18:26:13.888858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.57
 ---- batch: 020 ----
mean loss: 288.76
 ---- batch: 030 ----
mean loss: 303.73
 ---- batch: 040 ----
mean loss: 301.67
 ---- batch: 050 ----
mean loss: 294.74
 ---- batch: 060 ----
mean loss: 292.98
 ---- batch: 070 ----
mean loss: 286.19
 ---- batch: 080 ----
mean loss: 287.67
 ---- batch: 090 ----
mean loss: 290.64
train mean loss: 293.01
epoch train time: 0:00:02.621937
elapsed time: 0:03:29.475114
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 18:26:16.511264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.92
 ---- batch: 020 ----
mean loss: 288.43
 ---- batch: 030 ----
mean loss: 278.72
 ---- batch: 040 ----
mean loss: 279.79
 ---- batch: 050 ----
mean loss: 287.79
 ---- batch: 060 ----
mean loss: 292.10
 ---- batch: 070 ----
mean loss: 297.49
 ---- batch: 080 ----
mean loss: 294.31
 ---- batch: 090 ----
mean loss: 278.38
train mean loss: 287.40
epoch train time: 0:00:02.631602
elapsed time: 0:03:32.107254
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 18:26:19.143411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.10
 ---- batch: 020 ----
mean loss: 286.78
 ---- batch: 030 ----
mean loss: 278.17
 ---- batch: 040 ----
mean loss: 279.54
 ---- batch: 050 ----
mean loss: 278.89
 ---- batch: 060 ----
mean loss: 280.08
 ---- batch: 070 ----
mean loss: 290.08
 ---- batch: 080 ----
mean loss: 282.40
 ---- batch: 090 ----
mean loss: 282.23
train mean loss: 282.88
epoch train time: 0:00:02.609348
elapsed time: 0:03:34.717119
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 18:26:21.753278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.12
 ---- batch: 020 ----
mean loss: 272.25
 ---- batch: 030 ----
mean loss: 279.77
 ---- batch: 040 ----
mean loss: 279.38
 ---- batch: 050 ----
mean loss: 277.76
 ---- batch: 060 ----
mean loss: 270.52
 ---- batch: 070 ----
mean loss: 266.42
 ---- batch: 080 ----
mean loss: 289.62
 ---- batch: 090 ----
mean loss: 285.84
train mean loss: 279.37
epoch train time: 0:00:02.618964
elapsed time: 0:03:37.336574
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 18:26:24.372716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.59
 ---- batch: 020 ----
mean loss: 274.95
 ---- batch: 030 ----
mean loss: 281.78
 ---- batch: 040 ----
mean loss: 283.13
 ---- batch: 050 ----
mean loss: 278.62
 ---- batch: 060 ----
mean loss: 275.06
 ---- batch: 070 ----
mean loss: 273.44
 ---- batch: 080 ----
mean loss: 269.56
 ---- batch: 090 ----
mean loss: 277.11
train mean loss: 275.84
epoch train time: 0:00:02.685110
elapsed time: 0:03:40.022182
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 18:26:27.058318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.14
 ---- batch: 020 ----
mean loss: 271.01
 ---- batch: 030 ----
mean loss: 278.42
 ---- batch: 040 ----
mean loss: 273.87
 ---- batch: 050 ----
mean loss: 276.23
 ---- batch: 060 ----
mean loss: 280.36
 ---- batch: 070 ----
mean loss: 261.99
 ---- batch: 080 ----
mean loss: 270.14
 ---- batch: 090 ----
mean loss: 272.92
train mean loss: 272.62
epoch train time: 0:00:02.658581
elapsed time: 0:03:42.681290
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 18:26:29.717453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.85
 ---- batch: 020 ----
mean loss: 257.41
 ---- batch: 030 ----
mean loss: 266.52
 ---- batch: 040 ----
mean loss: 272.22
 ---- batch: 050 ----
mean loss: 274.95
 ---- batch: 060 ----
mean loss: 271.79
 ---- batch: 070 ----
mean loss: 271.82
 ---- batch: 080 ----
mean loss: 273.48
 ---- batch: 090 ----
mean loss: 272.67
train mean loss: 269.97
epoch train time: 0:00:02.652996
elapsed time: 0:03:45.334815
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 18:26:32.370964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.81
 ---- batch: 020 ----
mean loss: 268.62
 ---- batch: 030 ----
mean loss: 279.33
 ---- batch: 040 ----
mean loss: 263.50
 ---- batch: 050 ----
mean loss: 261.42
 ---- batch: 060 ----
mean loss: 269.67
 ---- batch: 070 ----
mean loss: 267.44
 ---- batch: 080 ----
mean loss: 269.42
 ---- batch: 090 ----
mean loss: 267.28
train mean loss: 267.37
epoch train time: 0:00:02.652218
elapsed time: 0:03:47.987563
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 18:26:35.023725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.40
 ---- batch: 020 ----
mean loss: 270.87
 ---- batch: 030 ----
mean loss: 273.82
 ---- batch: 040 ----
mean loss: 266.66
 ---- batch: 050 ----
mean loss: 263.87
 ---- batch: 060 ----
mean loss: 269.71
 ---- batch: 070 ----
mean loss: 262.50
 ---- batch: 080 ----
mean loss: 262.94
 ---- batch: 090 ----
mean loss: 262.93
train mean loss: 266.16
epoch train time: 0:00:02.662493
elapsed time: 0:03:50.650588
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 18:26:37.686728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.06
 ---- batch: 020 ----
mean loss: 266.59
 ---- batch: 030 ----
mean loss: 267.42
 ---- batch: 040 ----
mean loss: 267.84
 ---- batch: 050 ----
mean loss: 258.51
 ---- batch: 060 ----
mean loss: 264.16
 ---- batch: 070 ----
mean loss: 265.76
 ---- batch: 080 ----
mean loss: 262.67
 ---- batch: 090 ----
mean loss: 267.98
train mean loss: 264.08
epoch train time: 0:00:02.663237
elapsed time: 0:03:53.314355
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 18:26:40.350495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.52
 ---- batch: 020 ----
mean loss: 263.86
 ---- batch: 030 ----
mean loss: 245.28
 ---- batch: 040 ----
mean loss: 261.78
 ---- batch: 050 ----
mean loss: 269.92
 ---- batch: 060 ----
mean loss: 258.71
 ---- batch: 070 ----
mean loss: 264.10
 ---- batch: 080 ----
mean loss: 269.10
 ---- batch: 090 ----
mean loss: 259.97
train mean loss: 262.53
epoch train time: 0:00:02.663099
elapsed time: 0:03:55.977951
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 18:26:43.014122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.24
 ---- batch: 020 ----
mean loss: 263.22
 ---- batch: 030 ----
mean loss: 258.50
 ---- batch: 040 ----
mean loss: 273.38
 ---- batch: 050 ----
mean loss: 256.32
 ---- batch: 060 ----
mean loss: 257.65
 ---- batch: 070 ----
mean loss: 260.14
 ---- batch: 080 ----
mean loss: 261.97
 ---- batch: 090 ----
mean loss: 257.80
train mean loss: 260.68
epoch train time: 0:00:02.616160
elapsed time: 0:03:58.594642
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 18:26:45.630778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.37
 ---- batch: 020 ----
mean loss: 253.74
 ---- batch: 030 ----
mean loss: 255.31
 ---- batch: 040 ----
mean loss: 272.06
 ---- batch: 050 ----
mean loss: 262.25
 ---- batch: 060 ----
mean loss: 253.90
 ---- batch: 070 ----
mean loss: 270.58
 ---- batch: 080 ----
mean loss: 254.35
 ---- batch: 090 ----
mean loss: 261.87
train mean loss: 259.11
epoch train time: 0:00:02.696279
elapsed time: 0:04:01.291509
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 18:26:48.327699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.66
 ---- batch: 020 ----
mean loss: 255.10
 ---- batch: 030 ----
mean loss: 260.48
 ---- batch: 040 ----
mean loss: 262.97
 ---- batch: 050 ----
mean loss: 252.99
 ---- batch: 060 ----
mean loss: 266.16
 ---- batch: 070 ----
mean loss: 248.47
 ---- batch: 080 ----
mean loss: 261.91
 ---- batch: 090 ----
mean loss: 259.26
train mean loss: 257.46
epoch train time: 0:00:02.689799
elapsed time: 0:04:03.981897
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 18:26:51.018048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.15
 ---- batch: 020 ----
mean loss: 254.48
 ---- batch: 030 ----
mean loss: 259.31
 ---- batch: 040 ----
mean loss: 256.20
 ---- batch: 050 ----
mean loss: 254.36
 ---- batch: 060 ----
mean loss: 256.13
 ---- batch: 070 ----
mean loss: 256.08
 ---- batch: 080 ----
mean loss: 256.70
 ---- batch: 090 ----
mean loss: 260.48
train mean loss: 256.87
epoch train time: 0:00:02.693237
elapsed time: 0:04:06.675627
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 18:26:53.711762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.51
 ---- batch: 020 ----
mean loss: 260.56
 ---- batch: 030 ----
mean loss: 264.74
 ---- batch: 040 ----
mean loss: 252.50
 ---- batch: 050 ----
mean loss: 265.43
 ---- batch: 060 ----
mean loss: 251.49
 ---- batch: 070 ----
mean loss: 252.86
 ---- batch: 080 ----
mean loss: 242.44
 ---- batch: 090 ----
mean loss: 257.19
train mean loss: 255.44
epoch train time: 0:00:02.705280
elapsed time: 0:04:09.381378
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 18:26:56.417552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.87
 ---- batch: 020 ----
mean loss: 252.05
 ---- batch: 030 ----
mean loss: 249.21
 ---- batch: 040 ----
mean loss: 250.37
 ---- batch: 050 ----
mean loss: 257.37
 ---- batch: 060 ----
mean loss: 261.38
 ---- batch: 070 ----
mean loss: 248.45
 ---- batch: 080 ----
mean loss: 261.60
 ---- batch: 090 ----
mean loss: 253.87
train mean loss: 254.28
epoch train time: 0:00:02.682606
elapsed time: 0:04:12.064520
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 18:26:59.100664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.99
 ---- batch: 020 ----
mean loss: 247.88
 ---- batch: 030 ----
mean loss: 256.41
 ---- batch: 040 ----
mean loss: 251.08
 ---- batch: 050 ----
mean loss: 260.30
 ---- batch: 060 ----
mean loss: 254.32
 ---- batch: 070 ----
mean loss: 252.02
 ---- batch: 080 ----
mean loss: 248.83
 ---- batch: 090 ----
mean loss: 252.32
train mean loss: 253.28
epoch train time: 0:00:02.662816
elapsed time: 0:04:14.727819
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 18:27:01.763993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.03
 ---- batch: 020 ----
mean loss: 246.15
 ---- batch: 030 ----
mean loss: 251.98
 ---- batch: 040 ----
mean loss: 242.95
 ---- batch: 050 ----
mean loss: 261.18
 ---- batch: 060 ----
mean loss: 254.35
 ---- batch: 070 ----
mean loss: 248.70
 ---- batch: 080 ----
mean loss: 251.92
 ---- batch: 090 ----
mean loss: 259.58
train mean loss: 252.11
epoch train time: 0:00:02.679686
elapsed time: 0:04:17.408014
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 18:27:04.444170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.38
 ---- batch: 020 ----
mean loss: 258.26
 ---- batch: 030 ----
mean loss: 249.11
 ---- batch: 040 ----
mean loss: 254.62
 ---- batch: 050 ----
mean loss: 250.51
 ---- batch: 060 ----
mean loss: 241.92
 ---- batch: 070 ----
mean loss: 239.48
 ---- batch: 080 ----
mean loss: 248.22
 ---- batch: 090 ----
mean loss: 262.18
train mean loss: 251.85
epoch train time: 0:00:02.611806
elapsed time: 0:04:20.020306
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 18:27:07.056443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.08
 ---- batch: 020 ----
mean loss: 252.54
 ---- batch: 030 ----
mean loss: 251.15
 ---- batch: 040 ----
mean loss: 254.95
 ---- batch: 050 ----
mean loss: 250.18
 ---- batch: 060 ----
mean loss: 249.44
 ---- batch: 070 ----
mean loss: 248.50
 ---- batch: 080 ----
mean loss: 249.90
 ---- batch: 090 ----
mean loss: 244.48
train mean loss: 250.51
epoch train time: 0:00:02.672975
elapsed time: 0:04:22.693806
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 18:27:09.729947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.75
 ---- batch: 020 ----
mean loss: 249.47
 ---- batch: 030 ----
mean loss: 255.01
 ---- batch: 040 ----
mean loss: 253.44
 ---- batch: 050 ----
mean loss: 236.95
 ---- batch: 060 ----
mean loss: 256.88
 ---- batch: 070 ----
mean loss: 248.95
 ---- batch: 080 ----
mean loss: 254.32
 ---- batch: 090 ----
mean loss: 246.79
train mean loss: 249.60
epoch train time: 0:00:02.655825
elapsed time: 0:04:25.350143
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 18:27:12.386290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.00
 ---- batch: 020 ----
mean loss: 249.40
 ---- batch: 030 ----
mean loss: 246.78
 ---- batch: 040 ----
mean loss: 246.43
 ---- batch: 050 ----
mean loss: 246.82
 ---- batch: 060 ----
mean loss: 254.27
 ---- batch: 070 ----
mean loss: 245.33
 ---- batch: 080 ----
mean loss: 248.63
 ---- batch: 090 ----
mean loss: 246.67
train mean loss: 249.35
epoch train time: 0:00:02.652137
elapsed time: 0:04:28.002799
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 18:27:15.038947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.29
 ---- batch: 020 ----
mean loss: 255.09
 ---- batch: 030 ----
mean loss: 252.59
 ---- batch: 040 ----
mean loss: 245.25
 ---- batch: 050 ----
mean loss: 248.92
 ---- batch: 060 ----
mean loss: 244.12
 ---- batch: 070 ----
mean loss: 245.42
 ---- batch: 080 ----
mean loss: 249.99
 ---- batch: 090 ----
mean loss: 248.38
train mean loss: 248.15
epoch train time: 0:00:02.648207
elapsed time: 0:04:30.651587
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 18:27:17.687750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.83
 ---- batch: 020 ----
mean loss: 244.97
 ---- batch: 030 ----
mean loss: 235.94
 ---- batch: 040 ----
mean loss: 250.99
 ---- batch: 050 ----
mean loss: 252.23
 ---- batch: 060 ----
mean loss: 254.91
 ---- batch: 070 ----
mean loss: 248.50
 ---- batch: 080 ----
mean loss: 248.96
 ---- batch: 090 ----
mean loss: 240.96
train mean loss: 247.04
epoch train time: 0:00:02.696087
elapsed time: 0:04:33.348205
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 18:27:20.384347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.57
 ---- batch: 020 ----
mean loss: 249.44
 ---- batch: 030 ----
mean loss: 246.43
 ---- batch: 040 ----
mean loss: 243.58
 ---- batch: 050 ----
mean loss: 252.84
 ---- batch: 060 ----
mean loss: 255.18
 ---- batch: 070 ----
mean loss: 246.30
 ---- batch: 080 ----
mean loss: 247.40
 ---- batch: 090 ----
mean loss: 238.03
train mean loss: 246.72
epoch train time: 0:00:02.697478
elapsed time: 0:04:36.046177
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 18:27:23.082338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.99
 ---- batch: 020 ----
mean loss: 251.04
 ---- batch: 030 ----
mean loss: 244.23
 ---- batch: 040 ----
mean loss: 245.79
 ---- batch: 050 ----
mean loss: 244.82
 ---- batch: 060 ----
mean loss: 247.69
 ---- batch: 070 ----
mean loss: 243.18
 ---- batch: 080 ----
mean loss: 248.79
 ---- batch: 090 ----
mean loss: 246.14
train mean loss: 246.25
epoch train time: 0:00:02.688165
elapsed time: 0:04:38.734884
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 18:27:25.771037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.81
 ---- batch: 020 ----
mean loss: 254.23
 ---- batch: 030 ----
mean loss: 248.28
 ---- batch: 040 ----
mean loss: 250.74
 ---- batch: 050 ----
mean loss: 242.26
 ---- batch: 060 ----
mean loss: 249.49
 ---- batch: 070 ----
mean loss: 248.61
 ---- batch: 080 ----
mean loss: 249.51
 ---- batch: 090 ----
mean loss: 236.24
train mean loss: 246.26
epoch train time: 0:00:02.683130
elapsed time: 0:04:41.418558
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 18:27:28.454705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.43
 ---- batch: 020 ----
mean loss: 240.91
 ---- batch: 030 ----
mean loss: 254.20
 ---- batch: 040 ----
mean loss: 235.22
 ---- batch: 050 ----
mean loss: 237.77
 ---- batch: 060 ----
mean loss: 253.11
 ---- batch: 070 ----
mean loss: 244.46
 ---- batch: 080 ----
mean loss: 240.22
 ---- batch: 090 ----
mean loss: 252.45
train mean loss: 244.61
epoch train time: 0:00:02.677502
elapsed time: 0:04:44.096572
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 18:27:31.132713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.67
 ---- batch: 020 ----
mean loss: 237.28
 ---- batch: 030 ----
mean loss: 243.27
 ---- batch: 040 ----
mean loss: 244.37
 ---- batch: 050 ----
mean loss: 244.66
 ---- batch: 060 ----
mean loss: 243.75
 ---- batch: 070 ----
mean loss: 240.67
 ---- batch: 080 ----
mean loss: 242.85
 ---- batch: 090 ----
mean loss: 248.69
train mean loss: 244.06
epoch train time: 0:00:02.654155
elapsed time: 0:04:46.751285
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 18:27:33.787372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.25
 ---- batch: 020 ----
mean loss: 239.40
 ---- batch: 030 ----
mean loss: 241.44
 ---- batch: 040 ----
mean loss: 243.77
 ---- batch: 050 ----
mean loss: 242.44
 ---- batch: 060 ----
mean loss: 244.25
 ---- batch: 070 ----
mean loss: 248.22
 ---- batch: 080 ----
mean loss: 243.38
 ---- batch: 090 ----
mean loss: 244.50
train mean loss: 243.87
epoch train time: 0:00:02.684163
elapsed time: 0:04:49.435941
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 18:27:36.472095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.86
 ---- batch: 020 ----
mean loss: 237.89
 ---- batch: 030 ----
mean loss: 236.78
 ---- batch: 040 ----
mean loss: 245.74
 ---- batch: 050 ----
mean loss: 244.85
 ---- batch: 060 ----
mean loss: 239.83
 ---- batch: 070 ----
mean loss: 243.20
 ---- batch: 080 ----
mean loss: 247.02
 ---- batch: 090 ----
mean loss: 251.19
train mean loss: 242.68
epoch train time: 0:00:02.656484
elapsed time: 0:04:52.092940
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 18:27:39.129080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.63
 ---- batch: 020 ----
mean loss: 244.34
 ---- batch: 030 ----
mean loss: 239.64
 ---- batch: 040 ----
mean loss: 242.50
 ---- batch: 050 ----
mean loss: 235.13
 ---- batch: 060 ----
mean loss: 243.43
 ---- batch: 070 ----
mean loss: 249.99
 ---- batch: 080 ----
mean loss: 246.55
 ---- batch: 090 ----
mean loss: 240.13
train mean loss: 242.52
epoch train time: 0:00:02.665809
elapsed time: 0:04:54.759259
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 18:27:41.795423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.59
 ---- batch: 020 ----
mean loss: 235.58
 ---- batch: 030 ----
mean loss: 239.53
 ---- batch: 040 ----
mean loss: 242.04
 ---- batch: 050 ----
mean loss: 244.93
 ---- batch: 060 ----
mean loss: 242.83
 ---- batch: 070 ----
mean loss: 237.19
 ---- batch: 080 ----
mean loss: 242.23
 ---- batch: 090 ----
mean loss: 243.63
train mean loss: 241.91
epoch train time: 0:00:02.640734
elapsed time: 0:04:57.400541
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 18:27:44.436687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.01
 ---- batch: 020 ----
mean loss: 237.76
 ---- batch: 030 ----
mean loss: 238.40
 ---- batch: 040 ----
mean loss: 242.00
 ---- batch: 050 ----
mean loss: 252.29
 ---- batch: 060 ----
mean loss: 234.86
 ---- batch: 070 ----
mean loss: 233.18
 ---- batch: 080 ----
mean loss: 243.22
 ---- batch: 090 ----
mean loss: 242.16
train mean loss: 241.67
epoch train time: 0:00:02.660667
elapsed time: 0:05:00.061730
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 18:27:47.097874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.44
 ---- batch: 020 ----
mean loss: 244.45
 ---- batch: 030 ----
mean loss: 240.59
 ---- batch: 040 ----
mean loss: 241.96
 ---- batch: 050 ----
mean loss: 244.69
 ---- batch: 060 ----
mean loss: 244.98
 ---- batch: 070 ----
mean loss: 236.98
 ---- batch: 080 ----
mean loss: 235.40
 ---- batch: 090 ----
mean loss: 239.83
train mean loss: 240.81
epoch train time: 0:00:02.634774
elapsed time: 0:05:02.697009
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 18:27:49.733151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.03
 ---- batch: 020 ----
mean loss: 238.72
 ---- batch: 030 ----
mean loss: 239.40
 ---- batch: 040 ----
mean loss: 238.75
 ---- batch: 050 ----
mean loss: 235.16
 ---- batch: 060 ----
mean loss: 246.49
 ---- batch: 070 ----
mean loss: 237.44
 ---- batch: 080 ----
mean loss: 239.66
 ---- batch: 090 ----
mean loss: 246.84
train mean loss: 240.85
epoch train time: 0:00:02.687147
elapsed time: 0:05:05.384647
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 18:27:52.420792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.83
 ---- batch: 020 ----
mean loss: 244.29
 ---- batch: 030 ----
mean loss: 236.24
 ---- batch: 040 ----
mean loss: 238.76
 ---- batch: 050 ----
mean loss: 235.04
 ---- batch: 060 ----
mean loss: 241.53
 ---- batch: 070 ----
mean loss: 245.40
 ---- batch: 080 ----
mean loss: 233.89
 ---- batch: 090 ----
mean loss: 239.38
train mean loss: 240.17
epoch train time: 0:00:02.656049
elapsed time: 0:05:08.041273
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 18:27:55.077467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.70
 ---- batch: 020 ----
mean loss: 244.43
 ---- batch: 030 ----
mean loss: 237.10
 ---- batch: 040 ----
mean loss: 238.91
 ---- batch: 050 ----
mean loss: 235.87
 ---- batch: 060 ----
mean loss: 247.00
 ---- batch: 070 ----
mean loss: 229.10
 ---- batch: 080 ----
mean loss: 239.06
 ---- batch: 090 ----
mean loss: 232.54
train mean loss: 239.35
epoch train time: 0:00:02.636490
elapsed time: 0:05:10.678325
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 18:27:57.714474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.47
 ---- batch: 020 ----
mean loss: 236.56
 ---- batch: 030 ----
mean loss: 250.10
 ---- batch: 040 ----
mean loss: 238.59
 ---- batch: 050 ----
mean loss: 231.47
 ---- batch: 060 ----
mean loss: 241.53
 ---- batch: 070 ----
mean loss: 238.38
 ---- batch: 080 ----
mean loss: 238.07
 ---- batch: 090 ----
mean loss: 240.26
train mean loss: 238.99
epoch train time: 0:00:02.665090
elapsed time: 0:05:13.343912
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 18:28:00.380053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.22
 ---- batch: 020 ----
mean loss: 244.73
 ---- batch: 030 ----
mean loss: 239.89
 ---- batch: 040 ----
mean loss: 226.84
 ---- batch: 050 ----
mean loss: 242.82
 ---- batch: 060 ----
mean loss: 238.48
 ---- batch: 070 ----
mean loss: 235.66
 ---- batch: 080 ----
mean loss: 237.15
 ---- batch: 090 ----
mean loss: 236.92
train mean loss: 238.17
epoch train time: 0:00:02.681067
elapsed time: 0:05:16.025508
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 18:28:03.061650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.69
 ---- batch: 020 ----
mean loss: 237.70
 ---- batch: 030 ----
mean loss: 238.38
 ---- batch: 040 ----
mean loss: 238.89
 ---- batch: 050 ----
mean loss: 235.35
 ---- batch: 060 ----
mean loss: 239.91
 ---- batch: 070 ----
mean loss: 240.49
 ---- batch: 080 ----
mean loss: 246.10
 ---- batch: 090 ----
mean loss: 234.66
train mean loss: 237.65
epoch train time: 0:00:02.683548
elapsed time: 0:05:18.709569
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 18:28:05.745738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.32
 ---- batch: 020 ----
mean loss: 235.73
 ---- batch: 030 ----
mean loss: 231.77
 ---- batch: 040 ----
mean loss: 238.84
 ---- batch: 050 ----
mean loss: 232.07
 ---- batch: 060 ----
mean loss: 239.15
 ---- batch: 070 ----
mean loss: 239.10
 ---- batch: 080 ----
mean loss: 237.54
 ---- batch: 090 ----
mean loss: 237.08
train mean loss: 237.12
epoch train time: 0:00:02.689234
elapsed time: 0:05:21.399306
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 18:28:08.435452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.56
 ---- batch: 020 ----
mean loss: 236.85
 ---- batch: 030 ----
mean loss: 237.13
 ---- batch: 040 ----
mean loss: 231.12
 ---- batch: 050 ----
mean loss: 235.34
 ---- batch: 060 ----
mean loss: 238.10
 ---- batch: 070 ----
mean loss: 239.14
 ---- batch: 080 ----
mean loss: 232.90
 ---- batch: 090 ----
mean loss: 235.12
train mean loss: 237.12
epoch train time: 0:00:02.645123
elapsed time: 0:05:24.044909
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 18:28:11.081059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.15
 ---- batch: 020 ----
mean loss: 237.00
 ---- batch: 030 ----
mean loss: 241.06
 ---- batch: 040 ----
mean loss: 235.46
 ---- batch: 050 ----
mean loss: 234.94
 ---- batch: 060 ----
mean loss: 239.15
 ---- batch: 070 ----
mean loss: 233.59
 ---- batch: 080 ----
mean loss: 233.14
 ---- batch: 090 ----
mean loss: 235.00
train mean loss: 236.56
epoch train time: 0:00:02.644586
elapsed time: 0:05:26.690000
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 18:28:13.726147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.58
 ---- batch: 020 ----
mean loss: 238.10
 ---- batch: 030 ----
mean loss: 240.13
 ---- batch: 040 ----
mean loss: 242.08
 ---- batch: 050 ----
mean loss: 238.71
 ---- batch: 060 ----
mean loss: 238.51
 ---- batch: 070 ----
mean loss: 242.74
 ---- batch: 080 ----
mean loss: 233.00
 ---- batch: 090 ----
mean loss: 228.13
train mean loss: 236.03
epoch train time: 0:00:02.643641
elapsed time: 0:05:29.334121
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 18:28:16.370266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.36
 ---- batch: 020 ----
mean loss: 238.16
 ---- batch: 030 ----
mean loss: 229.93
 ---- batch: 040 ----
mean loss: 229.54
 ---- batch: 050 ----
mean loss: 231.79
 ---- batch: 060 ----
mean loss: 242.66
 ---- batch: 070 ----
mean loss: 244.76
 ---- batch: 080 ----
mean loss: 236.30
 ---- batch: 090 ----
mean loss: 237.51
train mean loss: 236.17
epoch train time: 0:00:02.642006
elapsed time: 0:05:31.976639
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 18:28:19.012785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.07
 ---- batch: 020 ----
mean loss: 231.25
 ---- batch: 030 ----
mean loss: 230.35
 ---- batch: 040 ----
mean loss: 237.67
 ---- batch: 050 ----
mean loss: 237.28
 ---- batch: 060 ----
mean loss: 241.82
 ---- batch: 070 ----
mean loss: 225.55
 ---- batch: 080 ----
mean loss: 238.13
 ---- batch: 090 ----
mean loss: 237.35
train mean loss: 235.58
epoch train time: 0:00:02.665559
elapsed time: 0:05:34.642694
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 18:28:21.678855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.10
 ---- batch: 020 ----
mean loss: 233.37
 ---- batch: 030 ----
mean loss: 230.19
 ---- batch: 040 ----
mean loss: 234.53
 ---- batch: 050 ----
mean loss: 230.98
 ---- batch: 060 ----
mean loss: 237.62
 ---- batch: 070 ----
mean loss: 237.17
 ---- batch: 080 ----
mean loss: 226.06
 ---- batch: 090 ----
mean loss: 239.57
train mean loss: 234.69
epoch train time: 0:00:02.671770
elapsed time: 0:05:37.314990
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 18:28:24.351133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.48
 ---- batch: 020 ----
mean loss: 237.21
 ---- batch: 030 ----
mean loss: 228.92
 ---- batch: 040 ----
mean loss: 225.33
 ---- batch: 050 ----
mean loss: 244.01
 ---- batch: 060 ----
mean loss: 235.63
 ---- batch: 070 ----
mean loss: 240.49
 ---- batch: 080 ----
mean loss: 232.02
 ---- batch: 090 ----
mean loss: 229.30
train mean loss: 234.36
epoch train time: 0:00:02.652559
elapsed time: 0:05:39.968036
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 18:28:27.004193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.96
 ---- batch: 020 ----
mean loss: 226.79
 ---- batch: 030 ----
mean loss: 235.44
 ---- batch: 040 ----
mean loss: 231.07
 ---- batch: 050 ----
mean loss: 233.62
 ---- batch: 060 ----
mean loss: 241.44
 ---- batch: 070 ----
mean loss: 241.19
 ---- batch: 080 ----
mean loss: 241.62
 ---- batch: 090 ----
mean loss: 241.61
train mean loss: 233.92
epoch train time: 0:00:02.631574
elapsed time: 0:05:42.600103
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 18:28:29.636245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.76
 ---- batch: 020 ----
mean loss: 230.47
 ---- batch: 030 ----
mean loss: 231.98
 ---- batch: 040 ----
mean loss: 233.97
 ---- batch: 050 ----
mean loss: 230.88
 ---- batch: 060 ----
mean loss: 224.98
 ---- batch: 070 ----
mean loss: 238.73
 ---- batch: 080 ----
mean loss: 237.01
 ---- batch: 090 ----
mean loss: 240.54
train mean loss: 233.72
epoch train time: 0:00:02.608809
elapsed time: 0:05:45.209372
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 18:28:32.245510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.31
 ---- batch: 020 ----
mean loss: 234.29
 ---- batch: 030 ----
mean loss: 236.75
 ---- batch: 040 ----
mean loss: 240.03
 ---- batch: 050 ----
mean loss: 237.57
 ---- batch: 060 ----
mean loss: 233.87
 ---- batch: 070 ----
mean loss: 232.97
 ---- batch: 080 ----
mean loss: 235.50
 ---- batch: 090 ----
mean loss: 221.17
train mean loss: 233.75
epoch train time: 0:00:02.669613
elapsed time: 0:05:47.879495
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 18:28:34.915638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.92
 ---- batch: 020 ----
mean loss: 232.96
 ---- batch: 030 ----
mean loss: 225.16
 ---- batch: 040 ----
mean loss: 233.94
 ---- batch: 050 ----
mean loss: 224.06
 ---- batch: 060 ----
mean loss: 237.66
 ---- batch: 070 ----
mean loss: 229.80
 ---- batch: 080 ----
mean loss: 241.82
 ---- batch: 090 ----
mean loss: 232.99
train mean loss: 232.97
epoch train time: 0:00:02.667978
elapsed time: 0:05:50.547934
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 18:28:37.584095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.21
 ---- batch: 020 ----
mean loss: 231.22
 ---- batch: 030 ----
mean loss: 230.53
 ---- batch: 040 ----
mean loss: 238.04
 ---- batch: 050 ----
mean loss: 232.25
 ---- batch: 060 ----
mean loss: 238.12
 ---- batch: 070 ----
mean loss: 228.86
 ---- batch: 080 ----
mean loss: 234.72
 ---- batch: 090 ----
mean loss: 233.42
train mean loss: 232.78
epoch train time: 0:00:02.683088
elapsed time: 0:05:53.231621
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 18:28:40.267814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.51
 ---- batch: 020 ----
mean loss: 231.78
 ---- batch: 030 ----
mean loss: 228.80
 ---- batch: 040 ----
mean loss: 232.97
 ---- batch: 050 ----
mean loss: 235.01
 ---- batch: 060 ----
mean loss: 232.56
 ---- batch: 070 ----
mean loss: 233.17
 ---- batch: 080 ----
mean loss: 239.38
 ---- batch: 090 ----
mean loss: 235.55
train mean loss: 232.18
epoch train time: 0:00:02.644172
elapsed time: 0:05:55.876286
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 18:28:42.912439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.81
 ---- batch: 020 ----
mean loss: 235.64
 ---- batch: 030 ----
mean loss: 228.20
 ---- batch: 040 ----
mean loss: 239.36
 ---- batch: 050 ----
mean loss: 232.11
 ---- batch: 060 ----
mean loss: 233.11
 ---- batch: 070 ----
mean loss: 224.46
 ---- batch: 080 ----
mean loss: 231.17
 ---- batch: 090 ----
mean loss: 227.10
train mean loss: 232.03
epoch train time: 0:00:02.635107
elapsed time: 0:05:58.511893
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 18:28:45.548039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.20
 ---- batch: 020 ----
mean loss: 225.75
 ---- batch: 030 ----
mean loss: 228.63
 ---- batch: 040 ----
mean loss: 233.07
 ---- batch: 050 ----
mean loss: 230.92
 ---- batch: 060 ----
mean loss: 233.31
 ---- batch: 070 ----
mean loss: 232.94
 ---- batch: 080 ----
mean loss: 234.57
 ---- batch: 090 ----
mean loss: 231.46
train mean loss: 231.62
epoch train time: 0:00:02.673286
elapsed time: 0:06:01.185676
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 18:28:48.221814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.59
 ---- batch: 020 ----
mean loss: 225.72
 ---- batch: 030 ----
mean loss: 226.27
 ---- batch: 040 ----
mean loss: 232.24
 ---- batch: 050 ----
mean loss: 229.25
 ---- batch: 060 ----
mean loss: 239.85
 ---- batch: 070 ----
mean loss: 233.80
 ---- batch: 080 ----
mean loss: 229.21
 ---- batch: 090 ----
mean loss: 236.10
train mean loss: 231.13
epoch train time: 0:00:02.685662
elapsed time: 0:06:03.871835
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 18:28:50.907977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.66
 ---- batch: 020 ----
mean loss: 228.53
 ---- batch: 030 ----
mean loss: 220.68
 ---- batch: 040 ----
mean loss: 223.93
 ---- batch: 050 ----
mean loss: 233.31
 ---- batch: 060 ----
mean loss: 234.42
 ---- batch: 070 ----
mean loss: 233.79
 ---- batch: 080 ----
mean loss: 238.12
 ---- batch: 090 ----
mean loss: 236.06
train mean loss: 231.03
epoch train time: 0:00:02.664979
elapsed time: 0:06:06.537305
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 18:28:53.573451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.61
 ---- batch: 020 ----
mean loss: 230.58
 ---- batch: 030 ----
mean loss: 234.76
 ---- batch: 040 ----
mean loss: 235.81
 ---- batch: 050 ----
mean loss: 230.24
 ---- batch: 060 ----
mean loss: 227.19
 ---- batch: 070 ----
mean loss: 227.58
 ---- batch: 080 ----
mean loss: 232.20
 ---- batch: 090 ----
mean loss: 236.48
train mean loss: 230.66
epoch train time: 0:00:02.641488
elapsed time: 0:06:09.179348
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 18:28:56.215436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.53
 ---- batch: 020 ----
mean loss: 225.24
 ---- batch: 030 ----
mean loss: 228.76
 ---- batch: 040 ----
mean loss: 237.09
 ---- batch: 050 ----
mean loss: 233.07
 ---- batch: 060 ----
mean loss: 231.23
 ---- batch: 070 ----
mean loss: 220.74
 ---- batch: 080 ----
mean loss: 228.62
 ---- batch: 090 ----
mean loss: 236.30
train mean loss: 230.22
epoch train time: 0:00:02.622753
elapsed time: 0:06:11.802529
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 18:28:58.838670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.87
 ---- batch: 020 ----
mean loss: 223.40
 ---- batch: 030 ----
mean loss: 225.77
 ---- batch: 040 ----
mean loss: 228.61
 ---- batch: 050 ----
mean loss: 224.89
 ---- batch: 060 ----
mean loss: 228.59
 ---- batch: 070 ----
mean loss: 231.74
 ---- batch: 080 ----
mean loss: 235.21
 ---- batch: 090 ----
mean loss: 236.39
train mean loss: 229.71
epoch train time: 0:00:02.638274
elapsed time: 0:06:14.441279
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 18:29:01.477456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.52
 ---- batch: 020 ----
mean loss: 230.12
 ---- batch: 030 ----
mean loss: 223.39
 ---- batch: 040 ----
mean loss: 226.90
 ---- batch: 050 ----
mean loss: 221.18
 ---- batch: 060 ----
mean loss: 223.75
 ---- batch: 070 ----
mean loss: 234.47
 ---- batch: 080 ----
mean loss: 239.49
 ---- batch: 090 ----
mean loss: 231.57
train mean loss: 229.54
epoch train time: 0:00:02.664630
elapsed time: 0:06:17.106428
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 18:29:04.142574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.34
 ---- batch: 020 ----
mean loss: 227.33
 ---- batch: 030 ----
mean loss: 228.96
 ---- batch: 040 ----
mean loss: 238.05
 ---- batch: 050 ----
mean loss: 230.53
 ---- batch: 060 ----
mean loss: 224.55
 ---- batch: 070 ----
mean loss: 229.62
 ---- batch: 080 ----
mean loss: 227.93
 ---- batch: 090 ----
mean loss: 230.85
train mean loss: 229.44
epoch train time: 0:00:02.655995
elapsed time: 0:06:19.762897
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 18:29:06.799033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.93
 ---- batch: 020 ----
mean loss: 226.63
 ---- batch: 030 ----
mean loss: 228.11
 ---- batch: 040 ----
mean loss: 228.11
 ---- batch: 050 ----
mean loss: 230.00
 ---- batch: 060 ----
mean loss: 224.41
 ---- batch: 070 ----
mean loss: 231.42
 ---- batch: 080 ----
mean loss: 236.20
 ---- batch: 090 ----
mean loss: 226.93
train mean loss: 229.32
epoch train time: 0:00:02.638111
elapsed time: 0:06:22.401489
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 18:29:09.437702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.79
 ---- batch: 020 ----
mean loss: 225.38
 ---- batch: 030 ----
mean loss: 224.04
 ---- batch: 040 ----
mean loss: 224.98
 ---- batch: 050 ----
mean loss: 228.30
 ---- batch: 060 ----
mean loss: 233.89
 ---- batch: 070 ----
mean loss: 227.12
 ---- batch: 080 ----
mean loss: 232.89
 ---- batch: 090 ----
mean loss: 225.65
train mean loss: 228.59
epoch train time: 0:00:02.661260
elapsed time: 0:06:25.063276
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 18:29:12.099413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.91
 ---- batch: 020 ----
mean loss: 218.43
 ---- batch: 030 ----
mean loss: 220.87
 ---- batch: 040 ----
mean loss: 231.42
 ---- batch: 050 ----
mean loss: 234.91
 ---- batch: 060 ----
mean loss: 223.31
 ---- batch: 070 ----
mean loss: 234.70
 ---- batch: 080 ----
mean loss: 233.11
 ---- batch: 090 ----
mean loss: 227.93
train mean loss: 228.72
epoch train time: 0:00:02.597116
elapsed time: 0:06:27.660891
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 18:29:14.697050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.54
 ---- batch: 020 ----
mean loss: 230.45
 ---- batch: 030 ----
mean loss: 227.99
 ---- batch: 040 ----
mean loss: 225.46
 ---- batch: 050 ----
mean loss: 224.73
 ---- batch: 060 ----
mean loss: 235.92
 ---- batch: 070 ----
mean loss: 231.24
 ---- batch: 080 ----
mean loss: 223.63
 ---- batch: 090 ----
mean loss: 225.94
train mean loss: 228.20
epoch train time: 0:00:02.649809
elapsed time: 0:06:30.311215
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 18:29:17.347356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.56
 ---- batch: 020 ----
mean loss: 228.93
 ---- batch: 030 ----
mean loss: 221.36
 ---- batch: 040 ----
mean loss: 230.72
 ---- batch: 050 ----
mean loss: 223.89
 ---- batch: 060 ----
mean loss: 228.98
 ---- batch: 070 ----
mean loss: 226.37
 ---- batch: 080 ----
mean loss: 228.30
 ---- batch: 090 ----
mean loss: 229.48
train mean loss: 227.84
epoch train time: 0:00:02.675422
elapsed time: 0:06:32.987108
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 18:29:20.023274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.45
 ---- batch: 020 ----
mean loss: 227.47
 ---- batch: 030 ----
mean loss: 230.00
 ---- batch: 040 ----
mean loss: 223.12
 ---- batch: 050 ----
mean loss: 226.27
 ---- batch: 060 ----
mean loss: 224.69
 ---- batch: 070 ----
mean loss: 224.68
 ---- batch: 080 ----
mean loss: 225.68
 ---- batch: 090 ----
mean loss: 232.23
train mean loss: 227.52
epoch train time: 0:00:02.629495
elapsed time: 0:06:35.617097
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 18:29:22.653312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.18
 ---- batch: 020 ----
mean loss: 224.89
 ---- batch: 030 ----
mean loss: 228.84
 ---- batch: 040 ----
mean loss: 232.96
 ---- batch: 050 ----
mean loss: 225.90
 ---- batch: 060 ----
mean loss: 228.27
 ---- batch: 070 ----
mean loss: 230.03
 ---- batch: 080 ----
mean loss: 230.26
 ---- batch: 090 ----
mean loss: 224.38
train mean loss: 227.30
epoch train time: 0:00:02.658498
elapsed time: 0:06:38.276128
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 18:29:25.312267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.25
 ---- batch: 020 ----
mean loss: 227.19
 ---- batch: 030 ----
mean loss: 232.51
 ---- batch: 040 ----
mean loss: 224.21
 ---- batch: 050 ----
mean loss: 232.55
 ---- batch: 060 ----
mean loss: 223.69
 ---- batch: 070 ----
mean loss: 222.34
 ---- batch: 080 ----
mean loss: 226.58
 ---- batch: 090 ----
mean loss: 232.95
train mean loss: 227.38
epoch train time: 0:00:02.610861
elapsed time: 0:06:40.887476
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 18:29:27.923629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.24
 ---- batch: 020 ----
mean loss: 224.67
 ---- batch: 030 ----
mean loss: 229.29
 ---- batch: 040 ----
mean loss: 222.75
 ---- batch: 050 ----
mean loss: 226.46
 ---- batch: 060 ----
mean loss: 227.44
 ---- batch: 070 ----
mean loss: 225.58
 ---- batch: 080 ----
mean loss: 239.73
 ---- batch: 090 ----
mean loss: 221.31
train mean loss: 226.34
epoch train time: 0:00:02.629430
elapsed time: 0:06:43.517412
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 18:29:30.553555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.12
 ---- batch: 020 ----
mean loss: 224.56
 ---- batch: 030 ----
mean loss: 227.41
 ---- batch: 040 ----
mean loss: 226.05
 ---- batch: 050 ----
mean loss: 229.18
 ---- batch: 060 ----
mean loss: 229.29
 ---- batch: 070 ----
mean loss: 222.20
 ---- batch: 080 ----
mean loss: 226.07
 ---- batch: 090 ----
mean loss: 233.11
train mean loss: 226.96
epoch train time: 0:00:02.619085
elapsed time: 0:06:46.136964
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 18:29:33.173118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.68
 ---- batch: 020 ----
mean loss: 226.52
 ---- batch: 030 ----
mean loss: 228.10
 ---- batch: 040 ----
mean loss: 219.52
 ---- batch: 050 ----
mean loss: 231.68
 ---- batch: 060 ----
mean loss: 226.15
 ---- batch: 070 ----
mean loss: 225.42
 ---- batch: 080 ----
mean loss: 224.35
 ---- batch: 090 ----
mean loss: 225.14
train mean loss: 226.52
epoch train time: 0:00:02.637394
elapsed time: 0:06:48.774869
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 18:29:35.811019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.56
 ---- batch: 020 ----
mean loss: 220.81
 ---- batch: 030 ----
mean loss: 227.16
 ---- batch: 040 ----
mean loss: 226.72
 ---- batch: 050 ----
mean loss: 230.44
 ---- batch: 060 ----
mean loss: 231.73
 ---- batch: 070 ----
mean loss: 222.47
 ---- batch: 080 ----
mean loss: 224.48
 ---- batch: 090 ----
mean loss: 225.21
train mean loss: 226.11
epoch train time: 0:00:02.635948
elapsed time: 0:06:51.411333
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 18:29:38.447486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.96
 ---- batch: 020 ----
mean loss: 227.53
 ---- batch: 030 ----
mean loss: 217.84
 ---- batch: 040 ----
mean loss: 230.28
 ---- batch: 050 ----
mean loss: 223.07
 ---- batch: 060 ----
mean loss: 220.10
 ---- batch: 070 ----
mean loss: 228.38
 ---- batch: 080 ----
mean loss: 222.14
 ---- batch: 090 ----
mean loss: 221.78
train mean loss: 225.65
epoch train time: 0:00:02.654291
elapsed time: 0:06:54.066198
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 18:29:41.102335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.95
 ---- batch: 020 ----
mean loss: 234.58
 ---- batch: 030 ----
mean loss: 223.73
 ---- batch: 040 ----
mean loss: 227.86
 ---- batch: 050 ----
mean loss: 218.18
 ---- batch: 060 ----
mean loss: 230.47
 ---- batch: 070 ----
mean loss: 226.43
 ---- batch: 080 ----
mean loss: 219.91
 ---- batch: 090 ----
mean loss: 228.46
train mean loss: 225.29
epoch train time: 0:00:02.625546
elapsed time: 0:06:56.692224
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 18:29:43.728382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.11
 ---- batch: 020 ----
mean loss: 226.63
 ---- batch: 030 ----
mean loss: 220.91
 ---- batch: 040 ----
mean loss: 226.96
 ---- batch: 050 ----
mean loss: 226.26
 ---- batch: 060 ----
mean loss: 231.42
 ---- batch: 070 ----
mean loss: 219.43
 ---- batch: 080 ----
mean loss: 232.72
 ---- batch: 090 ----
mean loss: 224.20
train mean loss: 224.92
epoch train time: 0:00:02.616884
elapsed time: 0:06:59.309630
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 18:29:46.345795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.52
 ---- batch: 020 ----
mean loss: 226.17
 ---- batch: 030 ----
mean loss: 228.31
 ---- batch: 040 ----
mean loss: 226.51
 ---- batch: 050 ----
mean loss: 222.27
 ---- batch: 060 ----
mean loss: 224.05
 ---- batch: 070 ----
mean loss: 222.22
 ---- batch: 080 ----
mean loss: 224.58
 ---- batch: 090 ----
mean loss: 221.59
train mean loss: 225.18
epoch train time: 0:00:02.648947
elapsed time: 0:07:01.959061
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 18:29:48.995254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.07
 ---- batch: 020 ----
mean loss: 232.62
 ---- batch: 030 ----
mean loss: 225.42
 ---- batch: 040 ----
mean loss: 223.21
 ---- batch: 050 ----
mean loss: 223.21
 ---- batch: 060 ----
mean loss: 227.34
 ---- batch: 070 ----
mean loss: 213.09
 ---- batch: 080 ----
mean loss: 233.37
 ---- batch: 090 ----
mean loss: 227.31
train mean loss: 224.92
epoch train time: 0:00:02.679812
elapsed time: 0:07:04.639450
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 18:29:51.675626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.24
 ---- batch: 020 ----
mean loss: 222.20
 ---- batch: 030 ----
mean loss: 220.61
 ---- batch: 040 ----
mean loss: 228.06
 ---- batch: 050 ----
mean loss: 220.00
 ---- batch: 060 ----
mean loss: 220.90
 ---- batch: 070 ----
mean loss: 227.05
 ---- batch: 080 ----
mean loss: 229.03
 ---- batch: 090 ----
mean loss: 223.75
train mean loss: 224.32
epoch train time: 0:00:02.695819
elapsed time: 0:07:07.335852
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 18:29:54.371940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.05
 ---- batch: 020 ----
mean loss: 223.20
 ---- batch: 030 ----
mean loss: 223.99
 ---- batch: 040 ----
mean loss: 222.61
 ---- batch: 050 ----
mean loss: 227.57
 ---- batch: 060 ----
mean loss: 231.44
 ---- batch: 070 ----
mean loss: 225.27
 ---- batch: 080 ----
mean loss: 219.61
 ---- batch: 090 ----
mean loss: 227.49
train mean loss: 224.31
epoch train time: 0:00:02.673877
elapsed time: 0:07:10.010211
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 18:29:57.046360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.15
 ---- batch: 020 ----
mean loss: 230.24
 ---- batch: 030 ----
mean loss: 218.33
 ---- batch: 040 ----
mean loss: 229.31
 ---- batch: 050 ----
mean loss: 211.30
 ---- batch: 060 ----
mean loss: 226.77
 ---- batch: 070 ----
mean loss: 220.26
 ---- batch: 080 ----
mean loss: 226.96
 ---- batch: 090 ----
mean loss: 223.30
train mean loss: 224.01
epoch train time: 0:00:02.643156
elapsed time: 0:07:12.653899
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 18:29:59.690043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.09
 ---- batch: 020 ----
mean loss: 223.50
 ---- batch: 030 ----
mean loss: 226.12
 ---- batch: 040 ----
mean loss: 219.61
 ---- batch: 050 ----
mean loss: 219.53
 ---- batch: 060 ----
mean loss: 225.66
 ---- batch: 070 ----
mean loss: 223.16
 ---- batch: 080 ----
mean loss: 226.75
 ---- batch: 090 ----
mean loss: 233.93
train mean loss: 223.94
epoch train time: 0:00:02.661542
elapsed time: 0:07:15.315938
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 18:30:02.352122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.97
 ---- batch: 020 ----
mean loss: 229.09
 ---- batch: 030 ----
mean loss: 224.18
 ---- batch: 040 ----
mean loss: 218.52
 ---- batch: 050 ----
mean loss: 223.61
 ---- batch: 060 ----
mean loss: 223.81
 ---- batch: 070 ----
mean loss: 226.65
 ---- batch: 080 ----
mean loss: 220.35
 ---- batch: 090 ----
mean loss: 225.51
train mean loss: 223.44
epoch train time: 0:00:02.669144
elapsed time: 0:07:17.985582
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 18:30:05.021738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.64
 ---- batch: 020 ----
mean loss: 213.54
 ---- batch: 030 ----
mean loss: 228.57
 ---- batch: 040 ----
mean loss: 231.16
 ---- batch: 050 ----
mean loss: 225.64
 ---- batch: 060 ----
mean loss: 219.95
 ---- batch: 070 ----
mean loss: 219.39
 ---- batch: 080 ----
mean loss: 220.95
 ---- batch: 090 ----
mean loss: 220.63
train mean loss: 223.68
epoch train time: 0:00:02.670513
elapsed time: 0:07:20.656618
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 18:30:07.692773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.05
 ---- batch: 020 ----
mean loss: 221.30
 ---- batch: 030 ----
mean loss: 217.93
 ---- batch: 040 ----
mean loss: 225.47
 ---- batch: 050 ----
mean loss: 224.44
 ---- batch: 060 ----
mean loss: 229.06
 ---- batch: 070 ----
mean loss: 220.58
 ---- batch: 080 ----
mean loss: 225.59
 ---- batch: 090 ----
mean loss: 220.18
train mean loss: 223.02
epoch train time: 0:00:02.651932
elapsed time: 0:07:23.309023
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 18:30:10.345160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.11
 ---- batch: 020 ----
mean loss: 225.08
 ---- batch: 030 ----
mean loss: 226.60
 ---- batch: 040 ----
mean loss: 216.79
 ---- batch: 050 ----
mean loss: 222.27
 ---- batch: 060 ----
mean loss: 228.81
 ---- batch: 070 ----
mean loss: 222.25
 ---- batch: 080 ----
mean loss: 219.72
 ---- batch: 090 ----
mean loss: 226.99
train mean loss: 223.34
epoch train time: 0:00:02.652695
elapsed time: 0:07:25.962203
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 18:30:12.998344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.52
 ---- batch: 020 ----
mean loss: 217.22
 ---- batch: 030 ----
mean loss: 226.80
 ---- batch: 040 ----
mean loss: 229.76
 ---- batch: 050 ----
mean loss: 212.89
 ---- batch: 060 ----
mean loss: 221.36
 ---- batch: 070 ----
mean loss: 232.65
 ---- batch: 080 ----
mean loss: 232.55
 ---- batch: 090 ----
mean loss: 217.25
train mean loss: 222.89
epoch train time: 0:00:02.620883
elapsed time: 0:07:28.583579
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 18:30:15.619722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.21
 ---- batch: 020 ----
mean loss: 223.98
 ---- batch: 030 ----
mean loss: 220.23
 ---- batch: 040 ----
mean loss: 225.61
 ---- batch: 050 ----
mean loss: 227.86
 ---- batch: 060 ----
mean loss: 222.95
 ---- batch: 070 ----
mean loss: 220.74
 ---- batch: 080 ----
mean loss: 218.24
 ---- batch: 090 ----
mean loss: 220.91
train mean loss: 222.61
epoch train time: 0:00:02.699177
elapsed time: 0:07:31.283254
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 18:30:18.319402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.42
 ---- batch: 020 ----
mean loss: 218.01
 ---- batch: 030 ----
mean loss: 223.13
 ---- batch: 040 ----
mean loss: 219.08
 ---- batch: 050 ----
mean loss: 214.88
 ---- batch: 060 ----
mean loss: 218.69
 ---- batch: 070 ----
mean loss: 221.79
 ---- batch: 080 ----
mean loss: 232.08
 ---- batch: 090 ----
mean loss: 221.39
train mean loss: 222.63
epoch train time: 0:00:02.695232
elapsed time: 0:07:33.978999
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 18:30:21.015141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.06
 ---- batch: 020 ----
mean loss: 214.70
 ---- batch: 030 ----
mean loss: 227.12
 ---- batch: 040 ----
mean loss: 226.76
 ---- batch: 050 ----
mean loss: 227.92
 ---- batch: 060 ----
mean loss: 220.07
 ---- batch: 070 ----
mean loss: 222.17
 ---- batch: 080 ----
mean loss: 218.79
 ---- batch: 090 ----
mean loss: 219.48
train mean loss: 222.04
epoch train time: 0:00:02.671994
elapsed time: 0:07:36.651489
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 18:30:23.687655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.56
 ---- batch: 020 ----
mean loss: 221.86
 ---- batch: 030 ----
mean loss: 217.88
 ---- batch: 040 ----
mean loss: 224.09
 ---- batch: 050 ----
mean loss: 219.11
 ---- batch: 060 ----
mean loss: 221.80
 ---- batch: 070 ----
mean loss: 225.34
 ---- batch: 080 ----
mean loss: 223.39
 ---- batch: 090 ----
mean loss: 222.86
train mean loss: 222.14
epoch train time: 0:00:02.637080
elapsed time: 0:07:39.289098
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 18:30:26.325244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.88
 ---- batch: 020 ----
mean loss: 214.71
 ---- batch: 030 ----
mean loss: 225.77
 ---- batch: 040 ----
mean loss: 215.76
 ---- batch: 050 ----
mean loss: 219.24
 ---- batch: 060 ----
mean loss: 226.83
 ---- batch: 070 ----
mean loss: 227.32
 ---- batch: 080 ----
mean loss: 218.11
 ---- batch: 090 ----
mean loss: 222.12
train mean loss: 221.67
epoch train time: 0:00:02.631936
elapsed time: 0:07:41.921508
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 18:30:28.957685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.25
 ---- batch: 020 ----
mean loss: 223.31
 ---- batch: 030 ----
mean loss: 232.87
 ---- batch: 040 ----
mean loss: 224.36
 ---- batch: 050 ----
mean loss: 214.74
 ---- batch: 060 ----
mean loss: 226.76
 ---- batch: 070 ----
mean loss: 220.29
 ---- batch: 080 ----
mean loss: 221.40
 ---- batch: 090 ----
mean loss: 216.44
train mean loss: 221.58
epoch train time: 0:00:02.683501
elapsed time: 0:07:44.605550
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 18:30:31.641720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.12
 ---- batch: 020 ----
mean loss: 214.13
 ---- batch: 030 ----
mean loss: 223.49
 ---- batch: 040 ----
mean loss: 225.44
 ---- batch: 050 ----
mean loss: 220.03
 ---- batch: 060 ----
mean loss: 225.17
 ---- batch: 070 ----
mean loss: 222.76
 ---- batch: 080 ----
mean loss: 216.72
 ---- batch: 090 ----
mean loss: 222.88
train mean loss: 221.64
epoch train time: 0:00:02.698922
elapsed time: 0:07:47.304972
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 18:30:34.341109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.36
 ---- batch: 020 ----
mean loss: 222.06
 ---- batch: 030 ----
mean loss: 219.39
 ---- batch: 040 ----
mean loss: 214.40
 ---- batch: 050 ----
mean loss: 230.05
 ---- batch: 060 ----
mean loss: 227.11
 ---- batch: 070 ----
mean loss: 219.33
 ---- batch: 080 ----
mean loss: 215.83
 ---- batch: 090 ----
mean loss: 226.51
train mean loss: 221.41
epoch train time: 0:00:02.657266
elapsed time: 0:07:49.962748
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 18:30:36.998890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.53
 ---- batch: 020 ----
mean loss: 220.67
 ---- batch: 030 ----
mean loss: 214.09
 ---- batch: 040 ----
mean loss: 227.83
 ---- batch: 050 ----
mean loss: 220.59
 ---- batch: 060 ----
mean loss: 233.79
 ---- batch: 070 ----
mean loss: 214.82
 ---- batch: 080 ----
mean loss: 212.35
 ---- batch: 090 ----
mean loss: 226.46
train mean loss: 221.14
epoch train time: 0:00:02.666380
elapsed time: 0:07:52.629633
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 18:30:39.665800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.40
 ---- batch: 020 ----
mean loss: 219.57
 ---- batch: 030 ----
mean loss: 222.78
 ---- batch: 040 ----
mean loss: 218.67
 ---- batch: 050 ----
mean loss: 221.27
 ---- batch: 060 ----
mean loss: 216.12
 ---- batch: 070 ----
mean loss: 213.99
 ---- batch: 080 ----
mean loss: 220.93
 ---- batch: 090 ----
mean loss: 229.09
train mean loss: 220.90
epoch train time: 0:00:02.620785
elapsed time: 0:07:55.250904
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 18:30:42.287087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.34
 ---- batch: 020 ----
mean loss: 216.64
 ---- batch: 030 ----
mean loss: 228.14
 ---- batch: 040 ----
mean loss: 214.85
 ---- batch: 050 ----
mean loss: 221.27
 ---- batch: 060 ----
mean loss: 218.50
 ---- batch: 070 ----
mean loss: 223.97
 ---- batch: 080 ----
mean loss: 217.40
 ---- batch: 090 ----
mean loss: 219.40
train mean loss: 220.79
epoch train time: 0:00:02.667399
elapsed time: 0:07:57.918798
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 18:30:44.954953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.73
 ---- batch: 020 ----
mean loss: 220.22
 ---- batch: 030 ----
mean loss: 213.55
 ---- batch: 040 ----
mean loss: 218.84
 ---- batch: 050 ----
mean loss: 220.15
 ---- batch: 060 ----
mean loss: 227.59
 ---- batch: 070 ----
mean loss: 214.52
 ---- batch: 080 ----
mean loss: 223.85
 ---- batch: 090 ----
mean loss: 218.06
train mean loss: 220.64
epoch train time: 0:00:02.684257
elapsed time: 0:08:00.603530
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 18:30:47.639676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.86
 ---- batch: 020 ----
mean loss: 216.12
 ---- batch: 030 ----
mean loss: 218.68
 ---- batch: 040 ----
mean loss: 224.32
 ---- batch: 050 ----
mean loss: 217.46
 ---- batch: 060 ----
mean loss: 219.36
 ---- batch: 070 ----
mean loss: 223.12
 ---- batch: 080 ----
mean loss: 224.36
 ---- batch: 090 ----
mean loss: 216.37
train mean loss: 220.25
epoch train time: 0:00:02.685363
elapsed time: 0:08:03.289370
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 18:30:50.325501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.14
 ---- batch: 020 ----
mean loss: 216.33
 ---- batch: 030 ----
mean loss: 219.46
 ---- batch: 040 ----
mean loss: 221.28
 ---- batch: 050 ----
mean loss: 227.67
 ---- batch: 060 ----
mean loss: 223.80
 ---- batch: 070 ----
mean loss: 224.71
 ---- batch: 080 ----
mean loss: 209.80
 ---- batch: 090 ----
mean loss: 217.08
train mean loss: 219.83
epoch train time: 0:00:02.647349
elapsed time: 0:08:05.937161
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 18:30:52.973296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.19
 ---- batch: 020 ----
mean loss: 216.69
 ---- batch: 030 ----
mean loss: 214.68
 ---- batch: 040 ----
mean loss: 219.58
 ---- batch: 050 ----
mean loss: 219.14
 ---- batch: 060 ----
mean loss: 221.91
 ---- batch: 070 ----
mean loss: 228.78
 ---- batch: 080 ----
mean loss: 214.50
 ---- batch: 090 ----
mean loss: 224.34
train mean loss: 220.05
epoch train time: 0:00:02.650180
elapsed time: 0:08:08.587850
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 18:30:55.624037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.82
 ---- batch: 020 ----
mean loss: 211.89
 ---- batch: 030 ----
mean loss: 220.05
 ---- batch: 040 ----
mean loss: 225.73
 ---- batch: 050 ----
mean loss: 216.83
 ---- batch: 060 ----
mean loss: 211.41
 ---- batch: 070 ----
mean loss: 220.00
 ---- batch: 080 ----
mean loss: 222.95
 ---- batch: 090 ----
mean loss: 218.46
train mean loss: 219.75
epoch train time: 0:00:02.612207
elapsed time: 0:08:11.200589
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 18:30:58.236729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.29
 ---- batch: 020 ----
mean loss: 214.78
 ---- batch: 030 ----
mean loss: 225.99
 ---- batch: 040 ----
mean loss: 214.16
 ---- batch: 050 ----
mean loss: 216.65
 ---- batch: 060 ----
mean loss: 221.47
 ---- batch: 070 ----
mean loss: 222.04
 ---- batch: 080 ----
mean loss: 225.03
 ---- batch: 090 ----
mean loss: 218.24
train mean loss: 219.62
epoch train time: 0:00:02.675973
elapsed time: 0:08:13.877244
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 18:31:00.913287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.93
 ---- batch: 020 ----
mean loss: 222.19
 ---- batch: 030 ----
mean loss: 231.24
 ---- batch: 040 ----
mean loss: 214.24
 ---- batch: 050 ----
mean loss: 215.20
 ---- batch: 060 ----
mean loss: 224.68
 ---- batch: 070 ----
mean loss: 212.67
 ---- batch: 080 ----
mean loss: 220.42
 ---- batch: 090 ----
mean loss: 218.69
train mean loss: 219.37
epoch train time: 0:00:02.683504
elapsed time: 0:08:16.561127
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 18:31:03.597288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.79
 ---- batch: 020 ----
mean loss: 211.71
 ---- batch: 030 ----
mean loss: 215.14
 ---- batch: 040 ----
mean loss: 222.52
 ---- batch: 050 ----
mean loss: 220.53
 ---- batch: 060 ----
mean loss: 222.93
 ---- batch: 070 ----
mean loss: 224.74
 ---- batch: 080 ----
mean loss: 213.38
 ---- batch: 090 ----
mean loss: 220.29
train mean loss: 219.16
epoch train time: 0:00:02.670992
elapsed time: 0:08:19.232678
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 18:31:06.268818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.14
 ---- batch: 020 ----
mean loss: 217.53
 ---- batch: 030 ----
mean loss: 220.35
 ---- batch: 040 ----
mean loss: 222.83
 ---- batch: 050 ----
mean loss: 224.00
 ---- batch: 060 ----
mean loss: 218.40
 ---- batch: 070 ----
mean loss: 213.69
 ---- batch: 080 ----
mean loss: 212.06
 ---- batch: 090 ----
mean loss: 225.12
train mean loss: 218.91
epoch train time: 0:00:02.660698
elapsed time: 0:08:21.893875
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 18:31:08.930015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.79
 ---- batch: 020 ----
mean loss: 223.84
 ---- batch: 030 ----
mean loss: 216.69
 ---- batch: 040 ----
mean loss: 214.60
 ---- batch: 050 ----
mean loss: 214.02
 ---- batch: 060 ----
mean loss: 221.34
 ---- batch: 070 ----
mean loss: 222.86
 ---- batch: 080 ----
mean loss: 218.19
 ---- batch: 090 ----
mean loss: 214.55
train mean loss: 218.61
epoch train time: 0:00:02.639368
elapsed time: 0:08:24.533802
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 18:31:11.569958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.63
 ---- batch: 020 ----
mean loss: 222.38
 ---- batch: 030 ----
mean loss: 220.59
 ---- batch: 040 ----
mean loss: 220.60
 ---- batch: 050 ----
mean loss: 211.33
 ---- batch: 060 ----
mean loss: 221.08
 ---- batch: 070 ----
mean loss: 217.42
 ---- batch: 080 ----
mean loss: 214.95
 ---- batch: 090 ----
mean loss: 216.15
train mean loss: 218.79
epoch train time: 0:00:02.623387
elapsed time: 0:08:27.157688
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 18:31:14.193828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.86
 ---- batch: 020 ----
mean loss: 216.77
 ---- batch: 030 ----
mean loss: 220.49
 ---- batch: 040 ----
mean loss: 217.07
 ---- batch: 050 ----
mean loss: 224.02
 ---- batch: 060 ----
mean loss: 218.15
 ---- batch: 070 ----
mean loss: 211.08
 ---- batch: 080 ----
mean loss: 219.14
 ---- batch: 090 ----
mean loss: 218.45
train mean loss: 218.66
epoch train time: 0:00:02.628030
elapsed time: 0:08:29.786197
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 18:31:16.822327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.72
 ---- batch: 020 ----
mean loss: 220.91
 ---- batch: 030 ----
mean loss: 218.45
 ---- batch: 040 ----
mean loss: 226.69
 ---- batch: 050 ----
mean loss: 216.93
 ---- batch: 060 ----
mean loss: 215.23
 ---- batch: 070 ----
mean loss: 214.50
 ---- batch: 080 ----
mean loss: 218.50
 ---- batch: 090 ----
mean loss: 217.12
train mean loss: 218.82
epoch train time: 0:00:02.656285
elapsed time: 0:08:32.442932
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 18:31:19.479083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.44
 ---- batch: 020 ----
mean loss: 221.02
 ---- batch: 030 ----
mean loss: 218.96
 ---- batch: 040 ----
mean loss: 219.79
 ---- batch: 050 ----
mean loss: 213.11
 ---- batch: 060 ----
mean loss: 217.51
 ---- batch: 070 ----
mean loss: 219.33
 ---- batch: 080 ----
mean loss: 218.69
 ---- batch: 090 ----
mean loss: 215.69
train mean loss: 218.47
epoch train time: 0:00:02.639362
elapsed time: 0:08:35.082768
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 18:31:22.118924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.45
 ---- batch: 020 ----
mean loss: 216.67
 ---- batch: 030 ----
mean loss: 222.91
 ---- batch: 040 ----
mean loss: 219.94
 ---- batch: 050 ----
mean loss: 214.23
 ---- batch: 060 ----
mean loss: 220.18
 ---- batch: 070 ----
mean loss: 220.13
 ---- batch: 080 ----
mean loss: 220.98
 ---- batch: 090 ----
mean loss: 207.18
train mean loss: 217.98
epoch train time: 0:00:02.640137
elapsed time: 0:08:37.723389
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 18:31:24.759526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.97
 ---- batch: 020 ----
mean loss: 220.56
 ---- batch: 030 ----
mean loss: 211.56
 ---- batch: 040 ----
mean loss: 221.32
 ---- batch: 050 ----
mean loss: 219.43
 ---- batch: 060 ----
mean loss: 218.86
 ---- batch: 070 ----
mean loss: 220.81
 ---- batch: 080 ----
mean loss: 219.50
 ---- batch: 090 ----
mean loss: 213.68
train mean loss: 218.05
epoch train time: 0:00:02.636858
elapsed time: 0:08:40.360719
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 18:31:27.396863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.59
 ---- batch: 020 ----
mean loss: 215.74
 ---- batch: 030 ----
mean loss: 217.35
 ---- batch: 040 ----
mean loss: 221.37
 ---- batch: 050 ----
mean loss: 221.42
 ---- batch: 060 ----
mean loss: 225.65
 ---- batch: 070 ----
mean loss: 214.65
 ---- batch: 080 ----
mean loss: 212.25
 ---- batch: 090 ----
mean loss: 216.68
train mean loss: 218.20
epoch train time: 0:00:02.669879
elapsed time: 0:08:43.031108
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 18:31:30.067257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.33
 ---- batch: 020 ----
mean loss: 209.89
 ---- batch: 030 ----
mean loss: 213.43
 ---- batch: 040 ----
mean loss: 226.67
 ---- batch: 050 ----
mean loss: 226.13
 ---- batch: 060 ----
mean loss: 219.07
 ---- batch: 070 ----
mean loss: 216.54
 ---- batch: 080 ----
mean loss: 217.97
 ---- batch: 090 ----
mean loss: 216.50
train mean loss: 217.52
epoch train time: 0:00:02.668545
elapsed time: 0:08:45.700167
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 18:31:32.736312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.57
 ---- batch: 020 ----
mean loss: 215.31
 ---- batch: 030 ----
mean loss: 219.61
 ---- batch: 040 ----
mean loss: 217.54
 ---- batch: 050 ----
mean loss: 213.29
 ---- batch: 060 ----
mean loss: 218.46
 ---- batch: 070 ----
mean loss: 219.79
 ---- batch: 080 ----
mean loss: 216.00
 ---- batch: 090 ----
mean loss: 220.60
train mean loss: 217.67
epoch train time: 0:00:02.670718
elapsed time: 0:08:48.371380
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 18:31:35.407531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.02
 ---- batch: 020 ----
mean loss: 220.14
 ---- batch: 030 ----
mean loss: 214.71
 ---- batch: 040 ----
mean loss: 210.77
 ---- batch: 050 ----
mean loss: 221.01
 ---- batch: 060 ----
mean loss: 218.24
 ---- batch: 070 ----
mean loss: 216.35
 ---- batch: 080 ----
mean loss: 219.05
 ---- batch: 090 ----
mean loss: 218.62
train mean loss: 217.10
epoch train time: 0:00:02.642966
elapsed time: 0:08:51.014828
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 18:31:38.050964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.45
 ---- batch: 020 ----
mean loss: 222.29
 ---- batch: 030 ----
mean loss: 214.06
 ---- batch: 040 ----
mean loss: 213.14
 ---- batch: 050 ----
mean loss: 218.80
 ---- batch: 060 ----
mean loss: 215.55
 ---- batch: 070 ----
mean loss: 216.83
 ---- batch: 080 ----
mean loss: 221.14
 ---- batch: 090 ----
mean loss: 217.87
train mean loss: 217.48
epoch train time: 0:00:02.602041
elapsed time: 0:08:53.617369
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 18:31:40.653523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.77
 ---- batch: 020 ----
mean loss: 215.61
 ---- batch: 030 ----
mean loss: 226.81
 ---- batch: 040 ----
mean loss: 213.41
 ---- batch: 050 ----
mean loss: 218.07
 ---- batch: 060 ----
mean loss: 208.03
 ---- batch: 070 ----
mean loss: 210.32
 ---- batch: 080 ----
mean loss: 222.87
 ---- batch: 090 ----
mean loss: 219.66
train mean loss: 217.05
epoch train time: 0:00:02.654341
elapsed time: 0:08:56.272176
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 18:31:43.308342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.90
 ---- batch: 020 ----
mean loss: 215.87
 ---- batch: 030 ----
mean loss: 220.48
 ---- batch: 040 ----
mean loss: 219.20
 ---- batch: 050 ----
mean loss: 219.66
 ---- batch: 060 ----
mean loss: 214.62
 ---- batch: 070 ----
mean loss: 217.19
 ---- batch: 080 ----
mean loss: 224.01
 ---- batch: 090 ----
mean loss: 210.33
train mean loss: 217.14
epoch train time: 0:00:02.674648
elapsed time: 0:08:58.947327
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 18:31:45.983470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.01
 ---- batch: 020 ----
mean loss: 221.53
 ---- batch: 030 ----
mean loss: 221.16
 ---- batch: 040 ----
mean loss: 212.32
 ---- batch: 050 ----
mean loss: 217.57
 ---- batch: 060 ----
mean loss: 216.18
 ---- batch: 070 ----
mean loss: 218.54
 ---- batch: 080 ----
mean loss: 213.77
 ---- batch: 090 ----
mean loss: 212.56
train mean loss: 216.77
epoch train time: 0:00:02.666410
elapsed time: 0:09:01.614221
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 18:31:48.650379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.64
 ---- batch: 020 ----
mean loss: 216.00
 ---- batch: 030 ----
mean loss: 210.41
 ---- batch: 040 ----
mean loss: 220.12
 ---- batch: 050 ----
mean loss: 217.60
 ---- batch: 060 ----
mean loss: 222.17
 ---- batch: 070 ----
mean loss: 212.92
 ---- batch: 080 ----
mean loss: 212.08
 ---- batch: 090 ----
mean loss: 215.92
train mean loss: 216.32
epoch train time: 0:00:02.650448
elapsed time: 0:09:04.265145
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 18:31:51.301284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.06
 ---- batch: 020 ----
mean loss: 211.41
 ---- batch: 030 ----
mean loss: 218.65
 ---- batch: 040 ----
mean loss: 217.27
 ---- batch: 050 ----
mean loss: 215.57
 ---- batch: 060 ----
mean loss: 206.17
 ---- batch: 070 ----
mean loss: 221.01
 ---- batch: 080 ----
mean loss: 221.81
 ---- batch: 090 ----
mean loss: 219.16
train mean loss: 216.67
epoch train time: 0:00:02.611768
elapsed time: 0:09:06.877407
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 18:31:53.913550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.31
 ---- batch: 020 ----
mean loss: 214.46
 ---- batch: 030 ----
mean loss: 215.39
 ---- batch: 040 ----
mean loss: 214.78
 ---- batch: 050 ----
mean loss: 211.49
 ---- batch: 060 ----
mean loss: 219.17
 ---- batch: 070 ----
mean loss: 218.64
 ---- batch: 080 ----
mean loss: 216.10
 ---- batch: 090 ----
mean loss: 218.22
train mean loss: 216.55
epoch train time: 0:00:02.677041
elapsed time: 0:09:09.554919
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 18:31:56.591066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.63
 ---- batch: 020 ----
mean loss: 216.86
 ---- batch: 030 ----
mean loss: 220.94
 ---- batch: 040 ----
mean loss: 211.84
 ---- batch: 050 ----
mean loss: 213.24
 ---- batch: 060 ----
mean loss: 219.88
 ---- batch: 070 ----
mean loss: 220.90
 ---- batch: 080 ----
mean loss: 217.53
 ---- batch: 090 ----
mean loss: 212.74
train mean loss: 216.51
epoch train time: 0:00:02.652138
elapsed time: 0:09:12.207531
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 18:31:59.243678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.99
 ---- batch: 020 ----
mean loss: 217.59
 ---- batch: 030 ----
mean loss: 215.06
 ---- batch: 040 ----
mean loss: 216.19
 ---- batch: 050 ----
mean loss: 213.86
 ---- batch: 060 ----
mean loss: 214.84
 ---- batch: 070 ----
mean loss: 215.49
 ---- batch: 080 ----
mean loss: 212.27
 ---- batch: 090 ----
mean loss: 221.89
train mean loss: 216.01
epoch train time: 0:00:02.688647
elapsed time: 0:09:14.896701
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 18:32:01.932866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.48
 ---- batch: 020 ----
mean loss: 217.92
 ---- batch: 030 ----
mean loss: 219.47
 ---- batch: 040 ----
mean loss: 211.52
 ---- batch: 050 ----
mean loss: 216.41
 ---- batch: 060 ----
mean loss: 219.40
 ---- batch: 070 ----
mean loss: 212.29
 ---- batch: 080 ----
mean loss: 210.12
 ---- batch: 090 ----
mean loss: 223.41
train mean loss: 215.96
epoch train time: 0:00:02.660293
elapsed time: 0:09:17.557484
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 18:32:04.593639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.72
 ---- batch: 020 ----
mean loss: 209.43
 ---- batch: 030 ----
mean loss: 213.16
 ---- batch: 040 ----
mean loss: 221.29
 ---- batch: 050 ----
mean loss: 225.84
 ---- batch: 060 ----
mean loss: 216.87
 ---- batch: 070 ----
mean loss: 219.46
 ---- batch: 080 ----
mean loss: 218.70
 ---- batch: 090 ----
mean loss: 209.88
train mean loss: 215.94
epoch train time: 0:00:02.681549
elapsed time: 0:09:20.239511
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 18:32:07.275652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.09
 ---- batch: 020 ----
mean loss: 207.06
 ---- batch: 030 ----
mean loss: 217.47
 ---- batch: 040 ----
mean loss: 216.97
 ---- batch: 050 ----
mean loss: 218.50
 ---- batch: 060 ----
mean loss: 223.07
 ---- batch: 070 ----
mean loss: 212.49
 ---- batch: 080 ----
mean loss: 218.91
 ---- batch: 090 ----
mean loss: 210.46
train mean loss: 215.54
epoch train time: 0:00:02.664120
elapsed time: 0:09:22.904106
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 18:32:09.940255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.93
 ---- batch: 020 ----
mean loss: 217.06
 ---- batch: 030 ----
mean loss: 207.56
 ---- batch: 040 ----
mean loss: 217.01
 ---- batch: 050 ----
mean loss: 216.93
 ---- batch: 060 ----
mean loss: 209.33
 ---- batch: 070 ----
mean loss: 217.12
 ---- batch: 080 ----
mean loss: 220.74
 ---- batch: 090 ----
mean loss: 221.32
train mean loss: 215.77
epoch train time: 0:00:02.675589
elapsed time: 0:09:25.580180
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 18:32:12.616332
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.56
 ---- batch: 020 ----
mean loss: 210.82
 ---- batch: 030 ----
mean loss: 211.48
 ---- batch: 040 ----
mean loss: 223.88
 ---- batch: 050 ----
mean loss: 216.46
 ---- batch: 060 ----
mean loss: 209.40
 ---- batch: 070 ----
mean loss: 216.88
 ---- batch: 080 ----
mean loss: 216.78
 ---- batch: 090 ----
mean loss: 215.55
train mean loss: 215.34
epoch train time: 0:00:02.665937
elapsed time: 0:09:28.246749
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 18:32:15.282767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.95
 ---- batch: 020 ----
mean loss: 211.63
 ---- batch: 030 ----
mean loss: 216.70
 ---- batch: 040 ----
mean loss: 208.17
 ---- batch: 050 ----
mean loss: 219.08
 ---- batch: 060 ----
mean loss: 212.79
 ---- batch: 070 ----
mean loss: 214.98
 ---- batch: 080 ----
mean loss: 214.02
 ---- batch: 090 ----
mean loss: 212.02
train mean loss: 215.18
epoch train time: 0:00:02.669140
elapsed time: 0:09:30.916366
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 18:32:17.952514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.32
 ---- batch: 020 ----
mean loss: 217.55
 ---- batch: 030 ----
mean loss: 211.38
 ---- batch: 040 ----
mean loss: 216.12
 ---- batch: 050 ----
mean loss: 218.12
 ---- batch: 060 ----
mean loss: 208.60
 ---- batch: 070 ----
mean loss: 224.13
 ---- batch: 080 ----
mean loss: 211.90
 ---- batch: 090 ----
mean loss: 213.29
train mean loss: 215.12
epoch train time: 0:00:02.655745
elapsed time: 0:09:33.572599
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 18:32:20.608743
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.65
 ---- batch: 020 ----
mean loss: 214.97
 ---- batch: 030 ----
mean loss: 217.16
 ---- batch: 040 ----
mean loss: 215.53
 ---- batch: 050 ----
mean loss: 214.24
 ---- batch: 060 ----
mean loss: 215.92
 ---- batch: 070 ----
mean loss: 211.95
 ---- batch: 080 ----
mean loss: 224.10
 ---- batch: 090 ----
mean loss: 210.47
train mean loss: 214.83
epoch train time: 0:00:02.649567
elapsed time: 0:09:36.222670
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 18:32:23.258845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.77
 ---- batch: 020 ----
mean loss: 217.86
 ---- batch: 030 ----
mean loss: 222.83
 ---- batch: 040 ----
mean loss: 205.16
 ---- batch: 050 ----
mean loss: 215.56
 ---- batch: 060 ----
mean loss: 216.19
 ---- batch: 070 ----
mean loss: 211.76
 ---- batch: 080 ----
mean loss: 222.64
 ---- batch: 090 ----
mean loss: 213.88
train mean loss: 214.87
epoch train time: 0:00:02.648326
elapsed time: 0:09:38.871570
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 18:32:25.907757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.50
 ---- batch: 020 ----
mean loss: 212.21
 ---- batch: 030 ----
mean loss: 213.59
 ---- batch: 040 ----
mean loss: 223.04
 ---- batch: 050 ----
mean loss: 212.59
 ---- batch: 060 ----
mean loss: 212.65
 ---- batch: 070 ----
mean loss: 218.22
 ---- batch: 080 ----
mean loss: 221.57
 ---- batch: 090 ----
mean loss: 211.40
train mean loss: 215.19
epoch train time: 0:00:02.677217
elapsed time: 0:09:41.549309
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 18:32:28.585460
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.39
 ---- batch: 020 ----
mean loss: 209.13
 ---- batch: 030 ----
mean loss: 213.97
 ---- batch: 040 ----
mean loss: 222.81
 ---- batch: 050 ----
mean loss: 214.62
 ---- batch: 060 ----
mean loss: 209.68
 ---- batch: 070 ----
mean loss: 216.06
 ---- batch: 080 ----
mean loss: 215.13
 ---- batch: 090 ----
mean loss: 216.30
train mean loss: 215.02
epoch train time: 0:00:02.655488
elapsed time: 0:09:44.205260
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 18:32:31.241412
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.06
 ---- batch: 020 ----
mean loss: 214.88
 ---- batch: 030 ----
mean loss: 211.65
 ---- batch: 040 ----
mean loss: 228.01
 ---- batch: 050 ----
mean loss: 215.09
 ---- batch: 060 ----
mean loss: 209.39
 ---- batch: 070 ----
mean loss: 217.85
 ---- batch: 080 ----
mean loss: 215.95
 ---- batch: 090 ----
mean loss: 210.96
train mean loss: 214.82
epoch train time: 0:00:02.667477
elapsed time: 0:09:46.873225
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 18:32:33.909371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.99
 ---- batch: 020 ----
mean loss: 218.82
 ---- batch: 030 ----
mean loss: 216.75
 ---- batch: 040 ----
mean loss: 207.03
 ---- batch: 050 ----
mean loss: 220.74
 ---- batch: 060 ----
mean loss: 220.73
 ---- batch: 070 ----
mean loss: 214.25
 ---- batch: 080 ----
mean loss: 216.37
 ---- batch: 090 ----
mean loss: 211.30
train mean loss: 215.18
epoch train time: 0:00:02.627248
elapsed time: 0:09:49.500952
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 18:32:36.537089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.87
 ---- batch: 020 ----
mean loss: 217.70
 ---- batch: 030 ----
mean loss: 213.82
 ---- batch: 040 ----
mean loss: 215.51
 ---- batch: 050 ----
mean loss: 212.09
 ---- batch: 060 ----
mean loss: 219.55
 ---- batch: 070 ----
mean loss: 211.45
 ---- batch: 080 ----
mean loss: 214.58
 ---- batch: 090 ----
mean loss: 212.46
train mean loss: 214.92
epoch train time: 0:00:02.640497
elapsed time: 0:09:52.141924
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 18:32:39.178079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.10
 ---- batch: 020 ----
mean loss: 214.58
 ---- batch: 030 ----
mean loss: 217.07
 ---- batch: 040 ----
mean loss: 211.90
 ---- batch: 050 ----
mean loss: 213.66
 ---- batch: 060 ----
mean loss: 209.23
 ---- batch: 070 ----
mean loss: 221.54
 ---- batch: 080 ----
mean loss: 214.31
 ---- batch: 090 ----
mean loss: 214.90
train mean loss: 215.31
epoch train time: 0:00:02.664953
elapsed time: 0:09:54.807386
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 18:32:41.843602
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.08
 ---- batch: 020 ----
mean loss: 218.47
 ---- batch: 030 ----
mean loss: 211.81
 ---- batch: 040 ----
mean loss: 219.91
 ---- batch: 050 ----
mean loss: 213.90
 ---- batch: 060 ----
mean loss: 209.66
 ---- batch: 070 ----
mean loss: 215.82
 ---- batch: 080 ----
mean loss: 216.10
 ---- batch: 090 ----
mean loss: 213.34
train mean loss: 214.99
epoch train time: 0:00:02.659748
elapsed time: 0:09:57.467702
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 18:32:44.503845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.25
 ---- batch: 020 ----
mean loss: 217.07
 ---- batch: 030 ----
mean loss: 210.77
 ---- batch: 040 ----
mean loss: 217.34
 ---- batch: 050 ----
mean loss: 219.84
 ---- batch: 060 ----
mean loss: 217.88
 ---- batch: 070 ----
mean loss: 212.53
 ---- batch: 080 ----
mean loss: 212.60
 ---- batch: 090 ----
mean loss: 211.74
train mean loss: 214.87
epoch train time: 0:00:02.634934
elapsed time: 0:10:00.103135
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 18:32:47.139279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.81
 ---- batch: 020 ----
mean loss: 201.30
 ---- batch: 030 ----
mean loss: 211.70
 ---- batch: 040 ----
mean loss: 213.74
 ---- batch: 050 ----
mean loss: 216.98
 ---- batch: 060 ----
mean loss: 220.66
 ---- batch: 070 ----
mean loss: 219.13
 ---- batch: 080 ----
mean loss: 223.16
 ---- batch: 090 ----
mean loss: 217.82
train mean loss: 215.25
epoch train time: 0:00:02.657898
elapsed time: 0:10:02.761531
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 18:32:49.797735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.10
 ---- batch: 020 ----
mean loss: 216.34
 ---- batch: 030 ----
mean loss: 212.27
 ---- batch: 040 ----
mean loss: 214.14
 ---- batch: 050 ----
mean loss: 211.25
 ---- batch: 060 ----
mean loss: 219.24
 ---- batch: 070 ----
mean loss: 211.38
 ---- batch: 080 ----
mean loss: 215.96
 ---- batch: 090 ----
mean loss: 211.97
train mean loss: 215.02
epoch train time: 0:00:02.664980
elapsed time: 0:10:05.427109
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 18:32:52.463278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.62
 ---- batch: 020 ----
mean loss: 212.15
 ---- batch: 030 ----
mean loss: 216.71
 ---- batch: 040 ----
mean loss: 212.83
 ---- batch: 050 ----
mean loss: 213.56
 ---- batch: 060 ----
mean loss: 217.65
 ---- batch: 070 ----
mean loss: 211.42
 ---- batch: 080 ----
mean loss: 220.79
 ---- batch: 090 ----
mean loss: 210.71
train mean loss: 214.63
epoch train time: 0:00:02.642006
elapsed time: 0:10:08.069581
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 18:32:55.105773
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.12
 ---- batch: 020 ----
mean loss: 214.38
 ---- batch: 030 ----
mean loss: 216.61
 ---- batch: 040 ----
mean loss: 212.37
 ---- batch: 050 ----
mean loss: 219.63
 ---- batch: 060 ----
mean loss: 211.19
 ---- batch: 070 ----
mean loss: 209.47
 ---- batch: 080 ----
mean loss: 221.18
 ---- batch: 090 ----
mean loss: 214.13
train mean loss: 215.00
epoch train time: 0:00:02.647628
elapsed time: 0:10:10.717769
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 18:32:57.753916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.12
 ---- batch: 020 ----
mean loss: 217.83
 ---- batch: 030 ----
mean loss: 218.91
 ---- batch: 040 ----
mean loss: 217.46
 ---- batch: 050 ----
mean loss: 211.23
 ---- batch: 060 ----
mean loss: 213.80
 ---- batch: 070 ----
mean loss: 212.60
 ---- batch: 080 ----
mean loss: 218.38
 ---- batch: 090 ----
mean loss: 212.90
train mean loss: 215.09
epoch train time: 0:00:02.683183
elapsed time: 0:10:13.401426
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 18:33:00.437584
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.66
 ---- batch: 020 ----
mean loss: 212.16
 ---- batch: 030 ----
mean loss: 218.72
 ---- batch: 040 ----
mean loss: 218.37
 ---- batch: 050 ----
mean loss: 223.62
 ---- batch: 060 ----
mean loss: 208.19
 ---- batch: 070 ----
mean loss: 210.05
 ---- batch: 080 ----
mean loss: 220.03
 ---- batch: 090 ----
mean loss: 214.34
train mean loss: 214.72
epoch train time: 0:00:02.589791
elapsed time: 0:10:15.991730
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 18:33:03.027891
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.43
 ---- batch: 020 ----
mean loss: 210.89
 ---- batch: 030 ----
mean loss: 215.19
 ---- batch: 040 ----
mean loss: 211.83
 ---- batch: 050 ----
mean loss: 219.40
 ---- batch: 060 ----
mean loss: 213.40
 ---- batch: 070 ----
mean loss: 215.57
 ---- batch: 080 ----
mean loss: 223.09
 ---- batch: 090 ----
mean loss: 212.99
train mean loss: 215.05
epoch train time: 0:00:02.645282
elapsed time: 0:10:18.637525
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 18:33:05.673699
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.23
 ---- batch: 020 ----
mean loss: 211.34
 ---- batch: 030 ----
mean loss: 216.34
 ---- batch: 040 ----
mean loss: 213.41
 ---- batch: 050 ----
mean loss: 215.68
 ---- batch: 060 ----
mean loss: 222.83
 ---- batch: 070 ----
mean loss: 216.56
 ---- batch: 080 ----
mean loss: 220.19
 ---- batch: 090 ----
mean loss: 205.71
train mean loss: 214.86
epoch train time: 0:00:02.650806
elapsed time: 0:10:21.288839
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 18:33:08.325023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.45
 ---- batch: 020 ----
mean loss: 218.41
 ---- batch: 030 ----
mean loss: 213.44
 ---- batch: 040 ----
mean loss: 210.89
 ---- batch: 050 ----
mean loss: 208.11
 ---- batch: 060 ----
mean loss: 215.26
 ---- batch: 070 ----
mean loss: 213.94
 ---- batch: 080 ----
mean loss: 227.10
 ---- batch: 090 ----
mean loss: 215.17
train mean loss: 214.71
epoch train time: 0:00:02.656595
elapsed time: 0:10:23.945958
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 18:33:10.982106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.17
 ---- batch: 020 ----
mean loss: 210.39
 ---- batch: 030 ----
mean loss: 217.67
 ---- batch: 040 ----
mean loss: 213.01
 ---- batch: 050 ----
mean loss: 215.22
 ---- batch: 060 ----
mean loss: 216.93
 ---- batch: 070 ----
mean loss: 214.17
 ---- batch: 080 ----
mean loss: 206.77
 ---- batch: 090 ----
mean loss: 224.45
train mean loss: 214.80
epoch train time: 0:00:02.636819
elapsed time: 0:10:26.583295
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 18:33:13.619441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.86
 ---- batch: 020 ----
mean loss: 210.93
 ---- batch: 030 ----
mean loss: 213.89
 ---- batch: 040 ----
mean loss: 215.10
 ---- batch: 050 ----
mean loss: 213.87
 ---- batch: 060 ----
mean loss: 211.29
 ---- batch: 070 ----
mean loss: 217.47
 ---- batch: 080 ----
mean loss: 224.07
 ---- batch: 090 ----
mean loss: 215.06
train mean loss: 214.78
epoch train time: 0:00:02.647587
elapsed time: 0:10:29.231400
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 18:33:16.267552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.82
 ---- batch: 020 ----
mean loss: 220.91
 ---- batch: 030 ----
mean loss: 211.78
 ---- batch: 040 ----
mean loss: 214.00
 ---- batch: 050 ----
mean loss: 216.18
 ---- batch: 060 ----
mean loss: 212.26
 ---- batch: 070 ----
mean loss: 211.83
 ---- batch: 080 ----
mean loss: 209.61
 ---- batch: 090 ----
mean loss: 216.21
train mean loss: 214.82
epoch train time: 0:00:02.630959
elapsed time: 0:10:31.862840
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 18:33:18.898988
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.85
 ---- batch: 020 ----
mean loss: 219.59
 ---- batch: 030 ----
mean loss: 216.48
 ---- batch: 040 ----
mean loss: 208.97
 ---- batch: 050 ----
mean loss: 210.75
 ---- batch: 060 ----
mean loss: 215.06
 ---- batch: 070 ----
mean loss: 211.16
 ---- batch: 080 ----
mean loss: 219.04
 ---- batch: 090 ----
mean loss: 220.28
train mean loss: 214.83
epoch train time: 0:00:02.680348
elapsed time: 0:10:34.543660
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 18:33:21.579798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.90
 ---- batch: 020 ----
mean loss: 220.21
 ---- batch: 030 ----
mean loss: 212.23
 ---- batch: 040 ----
mean loss: 219.51
 ---- batch: 050 ----
mean loss: 218.69
 ---- batch: 060 ----
mean loss: 212.01
 ---- batch: 070 ----
mean loss: 215.45
 ---- batch: 080 ----
mean loss: 215.43
 ---- batch: 090 ----
mean loss: 213.89
train mean loss: 214.77
epoch train time: 0:00:02.686031
elapsed time: 0:10:37.230204
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 18:33:24.266363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.11
 ---- batch: 020 ----
mean loss: 207.58
 ---- batch: 030 ----
mean loss: 217.27
 ---- batch: 040 ----
mean loss: 217.89
 ---- batch: 050 ----
mean loss: 211.40
 ---- batch: 060 ----
mean loss: 215.41
 ---- batch: 070 ----
mean loss: 218.28
 ---- batch: 080 ----
mean loss: 207.96
 ---- batch: 090 ----
mean loss: 214.03
train mean loss: 214.65
epoch train time: 0:00:02.669492
elapsed time: 0:10:39.900206
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 18:33:26.936349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.52
 ---- batch: 020 ----
mean loss: 217.28
 ---- batch: 030 ----
mean loss: 210.99
 ---- batch: 040 ----
mean loss: 220.57
 ---- batch: 050 ----
mean loss: 221.20
 ---- batch: 060 ----
mean loss: 209.93
 ---- batch: 070 ----
mean loss: 216.91
 ---- batch: 080 ----
mean loss: 207.32
 ---- batch: 090 ----
mean loss: 222.66
train mean loss: 214.68
epoch train time: 0:00:02.668172
elapsed time: 0:10:42.568852
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 18:33:29.605001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.98
 ---- batch: 020 ----
mean loss: 217.44
 ---- batch: 030 ----
mean loss: 221.19
 ---- batch: 040 ----
mean loss: 208.96
 ---- batch: 050 ----
mean loss: 210.79
 ---- batch: 060 ----
mean loss: 211.77
 ---- batch: 070 ----
mean loss: 216.20
 ---- batch: 080 ----
mean loss: 217.45
 ---- batch: 090 ----
mean loss: 213.04
train mean loss: 214.97
epoch train time: 0:00:02.633811
elapsed time: 0:10:45.203173
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 18:33:32.239328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.25
 ---- batch: 020 ----
mean loss: 210.84
 ---- batch: 030 ----
mean loss: 217.71
 ---- batch: 040 ----
mean loss: 212.75
 ---- batch: 050 ----
mean loss: 210.71
 ---- batch: 060 ----
mean loss: 207.25
 ---- batch: 070 ----
mean loss: 214.35
 ---- batch: 080 ----
mean loss: 212.24
 ---- batch: 090 ----
mean loss: 221.95
train mean loss: 214.51
epoch train time: 0:00:02.689239
elapsed time: 0:10:47.892912
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 18:33:34.929053
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.32
 ---- batch: 020 ----
mean loss: 215.79
 ---- batch: 030 ----
mean loss: 215.08
 ---- batch: 040 ----
mean loss: 211.63
 ---- batch: 050 ----
mean loss: 207.53
 ---- batch: 060 ----
mean loss: 219.69
 ---- batch: 070 ----
mean loss: 213.75
 ---- batch: 080 ----
mean loss: 214.57
 ---- batch: 090 ----
mean loss: 216.74
train mean loss: 214.59
epoch train time: 0:00:02.688819
elapsed time: 0:10:50.582186
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 18:33:37.618338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.80
 ---- batch: 020 ----
mean loss: 215.76
 ---- batch: 030 ----
mean loss: 210.95
 ---- batch: 040 ----
mean loss: 209.97
 ---- batch: 050 ----
mean loss: 213.69
 ---- batch: 060 ----
mean loss: 218.13
 ---- batch: 070 ----
mean loss: 206.93
 ---- batch: 080 ----
mean loss: 215.83
 ---- batch: 090 ----
mean loss: 214.89
train mean loss: 214.72
epoch train time: 0:00:02.660775
elapsed time: 0:10:53.243509
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 18:33:40.279524
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.70
 ---- batch: 020 ----
mean loss: 215.66
 ---- batch: 030 ----
mean loss: 209.77
 ---- batch: 040 ----
mean loss: 218.19
 ---- batch: 050 ----
mean loss: 215.09
 ---- batch: 060 ----
mean loss: 214.70
 ---- batch: 070 ----
mean loss: 221.51
 ---- batch: 080 ----
mean loss: 212.74
 ---- batch: 090 ----
mean loss: 214.12
train mean loss: 214.77
epoch train time: 0:00:02.637758
elapsed time: 0:10:55.881672
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 18:33:42.917841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.26
 ---- batch: 020 ----
mean loss: 210.34
 ---- batch: 030 ----
mean loss: 212.91
 ---- batch: 040 ----
mean loss: 206.82
 ---- batch: 050 ----
mean loss: 221.21
 ---- batch: 060 ----
mean loss: 221.44
 ---- batch: 070 ----
mean loss: 214.31
 ---- batch: 080 ----
mean loss: 216.16
 ---- batch: 090 ----
mean loss: 213.00
train mean loss: 214.58
epoch train time: 0:00:02.637470
elapsed time: 0:10:58.519687
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 18:33:45.555826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.82
 ---- batch: 020 ----
mean loss: 216.18
 ---- batch: 030 ----
mean loss: 215.05
 ---- batch: 040 ----
mean loss: 221.14
 ---- batch: 050 ----
mean loss: 213.67
 ---- batch: 060 ----
mean loss: 209.39
 ---- batch: 070 ----
mean loss: 218.47
 ---- batch: 080 ----
mean loss: 214.08
 ---- batch: 090 ----
mean loss: 212.42
train mean loss: 214.59
epoch train time: 0:00:02.674862
elapsed time: 0:11:01.195003
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 18:33:48.231135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.14
 ---- batch: 020 ----
mean loss: 208.14
 ---- batch: 030 ----
mean loss: 220.40
 ---- batch: 040 ----
mean loss: 216.84
 ---- batch: 050 ----
mean loss: 216.27
 ---- batch: 060 ----
mean loss: 206.49
 ---- batch: 070 ----
mean loss: 214.31
 ---- batch: 080 ----
mean loss: 217.94
 ---- batch: 090 ----
mean loss: 213.28
train mean loss: 214.74
epoch train time: 0:00:02.703193
elapsed time: 0:11:03.898667
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 18:33:50.934809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.74
 ---- batch: 020 ----
mean loss: 215.05
 ---- batch: 030 ----
mean loss: 214.99
 ---- batch: 040 ----
mean loss: 212.20
 ---- batch: 050 ----
mean loss: 216.84
 ---- batch: 060 ----
mean loss: 208.34
 ---- batch: 070 ----
mean loss: 213.10
 ---- batch: 080 ----
mean loss: 213.44
 ---- batch: 090 ----
mean loss: 218.36
train mean loss: 214.94
epoch train time: 0:00:02.676509
elapsed time: 0:11:06.575659
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 18:33:53.611797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.08
 ---- batch: 020 ----
mean loss: 214.42
 ---- batch: 030 ----
mean loss: 212.53
 ---- batch: 040 ----
mean loss: 206.56
 ---- batch: 050 ----
mean loss: 214.31
 ---- batch: 060 ----
mean loss: 220.67
 ---- batch: 070 ----
mean loss: 210.20
 ---- batch: 080 ----
mean loss: 216.67
 ---- batch: 090 ----
mean loss: 218.37
train mean loss: 214.70
epoch train time: 0:00:02.637625
elapsed time: 0:11:09.213848
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 18:33:56.250006
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.11
 ---- batch: 020 ----
mean loss: 212.34
 ---- batch: 030 ----
mean loss: 216.38
 ---- batch: 040 ----
mean loss: 211.71
 ---- batch: 050 ----
mean loss: 216.14
 ---- batch: 060 ----
mean loss: 210.45
 ---- batch: 070 ----
mean loss: 216.91
 ---- batch: 080 ----
mean loss: 215.71
 ---- batch: 090 ----
mean loss: 210.07
train mean loss: 214.68
epoch train time: 0:00:02.681656
elapsed time: 0:11:11.896015
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 18:33:58.932154
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.19
 ---- batch: 020 ----
mean loss: 213.40
 ---- batch: 030 ----
mean loss: 219.58
 ---- batch: 040 ----
mean loss: 209.02
 ---- batch: 050 ----
mean loss: 210.96
 ---- batch: 060 ----
mean loss: 209.36
 ---- batch: 070 ----
mean loss: 218.98
 ---- batch: 080 ----
mean loss: 213.48
 ---- batch: 090 ----
mean loss: 217.10
train mean loss: 214.74
epoch train time: 0:00:02.681137
elapsed time: 0:11:14.577687
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 18:34:01.613832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.85
 ---- batch: 020 ----
mean loss: 211.27
 ---- batch: 030 ----
mean loss: 218.29
 ---- batch: 040 ----
mean loss: 212.02
 ---- batch: 050 ----
mean loss: 213.09
 ---- batch: 060 ----
mean loss: 212.95
 ---- batch: 070 ----
mean loss: 212.82
 ---- batch: 080 ----
mean loss: 213.73
 ---- batch: 090 ----
mean loss: 218.83
train mean loss: 214.59
epoch train time: 0:00:02.683677
elapsed time: 0:11:17.261832
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 18:34:04.297986
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.47
 ---- batch: 020 ----
mean loss: 216.57
 ---- batch: 030 ----
mean loss: 211.34
 ---- batch: 040 ----
mean loss: 210.49
 ---- batch: 050 ----
mean loss: 213.54
 ---- batch: 060 ----
mean loss: 220.29
 ---- batch: 070 ----
mean loss: 220.94
 ---- batch: 080 ----
mean loss: 206.78
 ---- batch: 090 ----
mean loss: 219.16
train mean loss: 214.50
epoch train time: 0:00:02.680568
elapsed time: 0:11:19.942872
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 18:34:06.979025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.00
 ---- batch: 020 ----
mean loss: 223.49
 ---- batch: 030 ----
mean loss: 209.39
 ---- batch: 040 ----
mean loss: 210.17
 ---- batch: 050 ----
mean loss: 218.36
 ---- batch: 060 ----
mean loss: 209.84
 ---- batch: 070 ----
mean loss: 215.14
 ---- batch: 080 ----
mean loss: 211.89
 ---- batch: 090 ----
mean loss: 217.28
train mean loss: 214.36
epoch train time: 0:00:02.620294
elapsed time: 0:11:22.563641
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 18:34:09.599773
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.62
 ---- batch: 020 ----
mean loss: 214.37
 ---- batch: 030 ----
mean loss: 213.97
 ---- batch: 040 ----
mean loss: 215.57
 ---- batch: 050 ----
mean loss: 215.07
 ---- batch: 060 ----
mean loss: 215.46
 ---- batch: 070 ----
mean loss: 208.77
 ---- batch: 080 ----
mean loss: 218.33
 ---- batch: 090 ----
mean loss: 219.12
train mean loss: 214.42
epoch train time: 0:00:02.665125
elapsed time: 0:11:25.229335
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 18:34:12.265505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.46
 ---- batch: 020 ----
mean loss: 215.39
 ---- batch: 030 ----
mean loss: 209.58
 ---- batch: 040 ----
mean loss: 218.15
 ---- batch: 050 ----
mean loss: 215.45
 ---- batch: 060 ----
mean loss: 205.67
 ---- batch: 070 ----
mean loss: 214.34
 ---- batch: 080 ----
mean loss: 207.55
 ---- batch: 090 ----
mean loss: 220.90
train mean loss: 214.53
epoch train time: 0:00:02.690148
elapsed time: 0:11:27.920010
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 18:34:14.956156
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 215.51
 ---- batch: 020 ----
mean loss: 212.95
 ---- batch: 030 ----
mean loss: 217.29
 ---- batch: 040 ----
mean loss: 221.58
 ---- batch: 050 ----
mean loss: 216.34
 ---- batch: 060 ----
mean loss: 216.48
 ---- batch: 070 ----
mean loss: 204.80
 ---- batch: 080 ----
mean loss: 206.50
 ---- batch: 090 ----
mean loss: 217.68
train mean loss: 214.40
epoch train time: 0:00:02.664275
elapsed time: 0:11:30.584803
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 18:34:17.620951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.99
 ---- batch: 020 ----
mean loss: 217.25
 ---- batch: 030 ----
mean loss: 215.25
 ---- batch: 040 ----
mean loss: 218.94
 ---- batch: 050 ----
mean loss: 208.87
 ---- batch: 060 ----
mean loss: 216.12
 ---- batch: 070 ----
mean loss: 220.73
 ---- batch: 080 ----
mean loss: 208.79
 ---- batch: 090 ----
mean loss: 209.90
train mean loss: 214.53
epoch train time: 0:00:02.666234
elapsed time: 0:11:33.251511
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 18:34:20.287653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.03
 ---- batch: 020 ----
mean loss: 218.23
 ---- batch: 030 ----
mean loss: 213.77
 ---- batch: 040 ----
mean loss: 207.56
 ---- batch: 050 ----
mean loss: 223.84
 ---- batch: 060 ----
mean loss: 213.94
 ---- batch: 070 ----
mean loss: 209.46
 ---- batch: 080 ----
mean loss: 217.60
 ---- batch: 090 ----
mean loss: 220.00
train mean loss: 214.24
epoch train time: 0:00:02.632699
elapsed time: 0:11:35.888452
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_5/checkpoint.pth.tar
**** end time: 2019-09-25 18:34:22.924436 ****
