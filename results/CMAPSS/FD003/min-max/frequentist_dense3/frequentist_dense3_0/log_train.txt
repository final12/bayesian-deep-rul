Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8399
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistDense3...
Done.
**** start time: 2019-09-20 23:37:39.321397 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
            Linear-2                  [-1, 100]          42,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 62,100
Trainable params: 62,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:37:39.324778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4715.08
 ---- batch: 020 ----
mean loss: 4605.54
 ---- batch: 030 ----
mean loss: 4560.10
 ---- batch: 040 ----
mean loss: 4412.53
train mean loss: 4556.19
epoch train time: 0:00:14.851738
elapsed time: 0:00:14.857116
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:37:54.178551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4354.17
 ---- batch: 020 ----
mean loss: 4227.20
 ---- batch: 030 ----
mean loss: 4146.59
 ---- batch: 040 ----
mean loss: 4181.09
train mean loss: 4215.72
epoch train time: 0:00:00.207512
elapsed time: 0:00:15.064744
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:37:54.386206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4004.77
 ---- batch: 020 ----
mean loss: 3977.70
 ---- batch: 030 ----
mean loss: 3979.07
 ---- batch: 040 ----
mean loss: 3858.22
train mean loss: 3940.16
epoch train time: 0:00:00.205880
elapsed time: 0:00:15.270802
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:37:54.592308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3748.62
 ---- batch: 020 ----
mean loss: 3802.21
 ---- batch: 030 ----
mean loss: 3547.24
 ---- batch: 040 ----
mean loss: 3622.26
train mean loss: 3680.45
epoch train time: 0:00:00.211632
elapsed time: 0:00:15.482624
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:37:54.804067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3544.51
 ---- batch: 020 ----
mean loss: 3444.69
 ---- batch: 030 ----
mean loss: 3422.74
 ---- batch: 040 ----
mean loss: 3337.67
train mean loss: 3428.69
epoch train time: 0:00:00.212583
elapsed time: 0:00:15.695333
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:37:55.016788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3257.69
 ---- batch: 020 ----
mean loss: 3273.33
 ---- batch: 030 ----
mean loss: 3174.11
 ---- batch: 040 ----
mean loss: 3108.30
train mean loss: 3198.58
epoch train time: 0:00:00.198048
elapsed time: 0:00:15.893514
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:37:55.214953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3039.45
 ---- batch: 020 ----
mean loss: 3061.12
 ---- batch: 030 ----
mean loss: 2986.44
 ---- batch: 040 ----
mean loss: 2862.27
train mean loss: 2982.10
epoch train time: 0:00:00.196819
elapsed time: 0:00:16.090456
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:37:55.411893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2872.43
 ---- batch: 020 ----
mean loss: 2763.94
 ---- batch: 030 ----
mean loss: 2794.18
 ---- batch: 040 ----
mean loss: 2709.40
train mean loss: 2780.67
epoch train time: 0:00:00.193356
elapsed time: 0:00:16.283930
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:37:55.605390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2717.51
 ---- batch: 020 ----
mean loss: 2590.73
 ---- batch: 030 ----
mean loss: 2555.39
 ---- batch: 040 ----
mean loss: 2501.58
train mean loss: 2585.09
epoch train time: 0:00:00.196277
elapsed time: 0:00:16.480359
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:37:55.801800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2458.41
 ---- batch: 020 ----
mean loss: 2409.10
 ---- batch: 030 ----
mean loss: 2382.10
 ---- batch: 040 ----
mean loss: 2375.36
train mean loss: 2398.57
epoch train time: 0:00:00.195131
elapsed time: 0:00:16.675608
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:37:55.997044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2282.76
 ---- batch: 020 ----
mean loss: 2260.84
 ---- batch: 030 ----
mean loss: 2234.67
 ---- batch: 040 ----
mean loss: 2148.49
train mean loss: 2233.02
epoch train time: 0:00:00.190960
elapsed time: 0:00:16.866681
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:37:56.188132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2175.63
 ---- batch: 020 ----
mean loss: 2081.73
 ---- batch: 030 ----
mean loss: 2052.23
 ---- batch: 040 ----
mean loss: 2039.72
train mean loss: 2084.35
epoch train time: 0:00:00.191760
elapsed time: 0:00:17.058564
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:37:56.380018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1988.00
 ---- batch: 020 ----
mean loss: 1974.19
 ---- batch: 030 ----
mean loss: 1925.99
 ---- batch: 040 ----
mean loss: 1927.94
train mean loss: 1950.02
epoch train time: 0:00:00.192930
elapsed time: 0:00:17.251620
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:37:56.573053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1869.08
 ---- batch: 020 ----
mean loss: 1844.85
 ---- batch: 030 ----
mean loss: 1810.64
 ---- batch: 040 ----
mean loss: 1801.29
train mean loss: 1830.23
epoch train time: 0:00:00.193353
elapsed time: 0:00:17.445081
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:37:56.766516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1767.23
 ---- batch: 020 ----
mean loss: 1723.10
 ---- batch: 030 ----
mean loss: 1693.80
 ---- batch: 040 ----
mean loss: 1695.14
train mean loss: 1714.04
epoch train time: 0:00:00.196482
elapsed time: 0:00:17.641673
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:37:56.963109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1629.82
 ---- batch: 020 ----
mean loss: 1640.74
 ---- batch: 030 ----
mean loss: 1611.21
 ---- batch: 040 ----
mean loss: 1574.24
train mean loss: 1610.24
epoch train time: 0:00:00.196488
elapsed time: 0:00:17.838276
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:37:57.159715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1531.93
 ---- batch: 020 ----
mean loss: 1524.57
 ---- batch: 030 ----
mean loss: 1514.16
 ---- batch: 040 ----
mean loss: 1504.27
train mean loss: 1518.91
epoch train time: 0:00:00.201569
elapsed time: 0:00:18.039983
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:37:57.361435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1459.34
 ---- batch: 020 ----
mean loss: 1447.12
 ---- batch: 030 ----
mean loss: 1429.01
 ---- batch: 040 ----
mean loss: 1412.42
train mean loss: 1435.39
epoch train time: 0:00:00.204414
elapsed time: 0:00:18.244554
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:37:57.565993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1403.61
 ---- batch: 020 ----
mean loss: 1345.26
 ---- batch: 030 ----
mean loss: 1370.52
 ---- batch: 040 ----
mean loss: 1338.33
train mean loss: 1361.21
epoch train time: 0:00:00.201891
elapsed time: 0:00:18.446575
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:37:57.768014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1307.06
 ---- batch: 020 ----
mean loss: 1309.96
 ---- batch: 030 ----
mean loss: 1308.24
 ---- batch: 040 ----
mean loss: 1261.35
train mean loss: 1296.20
epoch train time: 0:00:00.209020
elapsed time: 0:00:18.655752
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:37:57.977192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1250.42
 ---- batch: 020 ----
mean loss: 1248.56
 ---- batch: 030 ----
mean loss: 1231.44
 ---- batch: 040 ----
mean loss: 1220.31
train mean loss: 1236.27
epoch train time: 0:00:00.206686
elapsed time: 0:00:18.862564
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:37:58.184005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1204.21
 ---- batch: 020 ----
mean loss: 1176.88
 ---- batch: 030 ----
mean loss: 1179.09
 ---- batch: 040 ----
mean loss: 1178.16
train mean loss: 1182.68
epoch train time: 0:00:00.206633
elapsed time: 0:00:19.069337
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:37:58.390778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1154.17
 ---- batch: 020 ----
mean loss: 1141.70
 ---- batch: 030 ----
mean loss: 1129.15
 ---- batch: 040 ----
mean loss: 1117.46
train mean loss: 1135.59
epoch train time: 0:00:00.205989
elapsed time: 0:00:19.275459
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:37:58.596926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1118.83
 ---- batch: 020 ----
mean loss: 1105.62
 ---- batch: 030 ----
mean loss: 1089.17
 ---- batch: 040 ----
mean loss: 1069.83
train mean loss: 1093.40
epoch train time: 0:00:00.201691
elapsed time: 0:00:19.477298
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:37:58.798737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1082.15
 ---- batch: 020 ----
mean loss: 1057.20
 ---- batch: 030 ----
mean loss: 1057.20
 ---- batch: 040 ----
mean loss: 1031.84
train mean loss: 1055.82
epoch train time: 0:00:00.201237
elapsed time: 0:00:19.678703
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:37:59.000138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1028.59
 ---- batch: 020 ----
mean loss: 1045.77
 ---- batch: 030 ----
mean loss: 1019.21
 ---- batch: 040 ----
mean loss: 1005.59
train mean loss: 1022.24
epoch train time: 0:00:00.197826
elapsed time: 0:00:19.876641
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:37:59.198077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1009.21
 ---- batch: 020 ----
mean loss: 987.68
 ---- batch: 030 ----
mean loss: 989.07
 ---- batch: 040 ----
mean loss: 984.59
train mean loss: 992.16
epoch train time: 0:00:00.195952
elapsed time: 0:00:20.072707
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:37:59.394146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 974.90
 ---- batch: 020 ----
mean loss: 972.04
 ---- batch: 030 ----
mean loss: 962.27
 ---- batch: 040 ----
mean loss: 957.17
train mean loss: 966.55
epoch train time: 0:00:00.197797
elapsed time: 0:00:20.270618
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:37:59.592053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 968.25
 ---- batch: 020 ----
mean loss: 936.57
 ---- batch: 030 ----
mean loss: 934.25
 ---- batch: 040 ----
mean loss: 937.48
train mean loss: 943.05
epoch train time: 0:00:00.198540
elapsed time: 0:00:20.469811
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:37:59.791265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.18
 ---- batch: 020 ----
mean loss: 912.25
 ---- batch: 030 ----
mean loss: 931.28
 ---- batch: 040 ----
mean loss: 911.22
train mean loss: 922.67
epoch train time: 0:00:00.198583
elapsed time: 0:00:20.668523
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:37:59.989958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.07
 ---- batch: 020 ----
mean loss: 902.13
 ---- batch: 030 ----
mean loss: 908.77
 ---- batch: 040 ----
mean loss: 901.29
train mean loss: 903.72
epoch train time: 0:00:00.196459
elapsed time: 0:00:20.865093
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:38:00.186529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.75
 ---- batch: 020 ----
mean loss: 871.74
 ---- batch: 030 ----
mean loss: 856.31
 ---- batch: 040 ----
mean loss: 770.41
train mean loss: 841.56
epoch train time: 0:00:00.196222
elapsed time: 0:00:21.061424
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:38:00.382858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 702.81
 ---- batch: 020 ----
mean loss: 695.34
 ---- batch: 030 ----
mean loss: 679.74
 ---- batch: 040 ----
mean loss: 668.64
train mean loss: 683.98
epoch train time: 0:00:00.200128
elapsed time: 0:00:21.261662
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:38:00.583097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.26
 ---- batch: 020 ----
mean loss: 633.50
 ---- batch: 030 ----
mean loss: 597.97
 ---- batch: 040 ----
mean loss: 559.57
train mean loss: 604.91
epoch train time: 0:00:00.196302
elapsed time: 0:00:21.458072
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:38:00.779507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 517.71
 ---- batch: 020 ----
mean loss: 489.52
 ---- batch: 030 ----
mean loss: 471.53
 ---- batch: 040 ----
mean loss: 448.48
train mean loss: 479.50
epoch train time: 0:00:00.211005
elapsed time: 0:00:21.669192
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:38:00.990649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.98
 ---- batch: 020 ----
mean loss: 424.90
 ---- batch: 030 ----
mean loss: 415.17
 ---- batch: 040 ----
mean loss: 411.21
train mean loss: 419.28
epoch train time: 0:00:00.204271
elapsed time: 0:00:21.873603
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:38:01.195049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.42
 ---- batch: 020 ----
mean loss: 384.68
 ---- batch: 030 ----
mean loss: 375.25
 ---- batch: 040 ----
mean loss: 365.10
train mean loss: 377.27
epoch train time: 0:00:00.206322
elapsed time: 0:00:22.080058
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:38:01.401511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.17
 ---- batch: 020 ----
mean loss: 347.13
 ---- batch: 030 ----
mean loss: 337.24
 ---- batch: 040 ----
mean loss: 331.35
train mean loss: 341.43
epoch train time: 0:00:00.207639
elapsed time: 0:00:22.287849
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:38:01.609314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.10
 ---- batch: 020 ----
mean loss: 316.41
 ---- batch: 030 ----
mean loss: 303.84
 ---- batch: 040 ----
mean loss: 303.17
train mean loss: 310.90
epoch train time: 0:00:00.204451
elapsed time: 0:00:22.492483
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:38:01.813934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.58
 ---- batch: 020 ----
mean loss: 286.56
 ---- batch: 030 ----
mean loss: 280.16
 ---- batch: 040 ----
mean loss: 273.03
train mean loss: 283.43
epoch train time: 0:00:00.205328
elapsed time: 0:00:22.697945
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:38:02.019385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.04
 ---- batch: 020 ----
mean loss: 261.60
 ---- batch: 030 ----
mean loss: 258.77
 ---- batch: 040 ----
mean loss: 250.60
train mean loss: 259.35
epoch train time: 0:00:00.204585
elapsed time: 0:00:22.902649
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:38:02.224086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.05
 ---- batch: 020 ----
mean loss: 233.48
 ---- batch: 030 ----
mean loss: 240.85
 ---- batch: 040 ----
mean loss: 232.60
train mean loss: 237.48
epoch train time: 0:00:00.202800
elapsed time: 0:00:23.105566
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:38:02.427019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.85
 ---- batch: 020 ----
mean loss: 222.48
 ---- batch: 030 ----
mean loss: 217.76
 ---- batch: 040 ----
mean loss: 210.32
train mean loss: 218.44
epoch train time: 0:00:00.202944
elapsed time: 0:00:23.308638
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:38:02.630074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.51
 ---- batch: 020 ----
mean loss: 211.01
 ---- batch: 030 ----
mean loss: 195.66
 ---- batch: 040 ----
mean loss: 192.35
train mean loss: 201.10
epoch train time: 0:00:00.197262
elapsed time: 0:00:23.506011
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:38:02.827447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.21
 ---- batch: 020 ----
mean loss: 190.65
 ---- batch: 030 ----
mean loss: 185.74
 ---- batch: 040 ----
mean loss: 182.51
train mean loss: 187.08
epoch train time: 0:00:00.195193
elapsed time: 0:00:23.701368
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:38:03.022803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.26
 ---- batch: 020 ----
mean loss: 174.98
 ---- batch: 030 ----
mean loss: 171.42
 ---- batch: 040 ----
mean loss: 168.05
train mean loss: 172.62
epoch train time: 0:00:00.192887
elapsed time: 0:00:23.894366
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:38:03.215802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.13
 ---- batch: 020 ----
mean loss: 159.53
 ---- batch: 030 ----
mean loss: 163.96
 ---- batch: 040 ----
mean loss: 157.09
train mean loss: 160.12
epoch train time: 0:00:00.194876
elapsed time: 0:00:24.089379
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:38:03.410833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.08
 ---- batch: 020 ----
mean loss: 151.04
 ---- batch: 030 ----
mean loss: 146.01
 ---- batch: 040 ----
mean loss: 149.04
train mean loss: 149.38
epoch train time: 0:00:00.196332
elapsed time: 0:00:24.285840
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:38:03.607306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.30
 ---- batch: 020 ----
mean loss: 139.15
 ---- batch: 030 ----
mean loss: 138.36
 ---- batch: 040 ----
mean loss: 140.74
train mean loss: 139.41
epoch train time: 0:00:00.195635
elapsed time: 0:00:24.481616
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:38:03.803052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.95
 ---- batch: 020 ----
mean loss: 131.72
 ---- batch: 030 ----
mean loss: 127.74
 ---- batch: 040 ----
mean loss: 129.10
train mean loss: 131.17
epoch train time: 0:00:00.204089
elapsed time: 0:00:24.685839
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:38:04.007322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.14
 ---- batch: 020 ----
mean loss: 125.71
 ---- batch: 030 ----
mean loss: 121.84
 ---- batch: 040 ----
mean loss: 121.92
train mean loss: 123.55
epoch train time: 0:00:00.195799
elapsed time: 0:00:24.881795
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:38:04.203250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.19
 ---- batch: 020 ----
mean loss: 115.95
 ---- batch: 030 ----
mean loss: 115.42
 ---- batch: 040 ----
mean loss: 114.46
train mean loss: 116.38
epoch train time: 0:00:00.195281
elapsed time: 0:00:25.077210
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:38:04.398649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.63
 ---- batch: 020 ----
mean loss: 109.97
 ---- batch: 030 ----
mean loss: 111.19
 ---- batch: 040 ----
mean loss: 108.12
train mean loss: 110.60
epoch train time: 0:00:00.198458
elapsed time: 0:00:25.275805
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:38:04.597271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.25
 ---- batch: 020 ----
mean loss: 104.96
 ---- batch: 030 ----
mean loss: 103.79
 ---- batch: 040 ----
mean loss: 104.18
train mean loss: 104.79
epoch train time: 0:00:00.204713
elapsed time: 0:00:25.481061
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:38:04.802513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.48
 ---- batch: 020 ----
mean loss: 98.22
 ---- batch: 030 ----
mean loss: 101.13
 ---- batch: 040 ----
mean loss: 98.10
train mean loss: 100.11
epoch train time: 0:00:00.203888
elapsed time: 0:00:25.685114
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:38:05.006569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.08
 ---- batch: 020 ----
mean loss: 98.45
 ---- batch: 030 ----
mean loss: 97.68
 ---- batch: 040 ----
mean loss: 92.88
train mean loss: 96.83
epoch train time: 0:00:00.204122
elapsed time: 0:00:25.889403
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:38:05.210858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.82
 ---- batch: 020 ----
mean loss: 91.60
 ---- batch: 030 ----
mean loss: 88.87
 ---- batch: 040 ----
mean loss: 92.05
train mean loss: 91.97
epoch train time: 0:00:00.203792
elapsed time: 0:00:26.093352
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:38:05.414792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.02
 ---- batch: 020 ----
mean loss: 87.50
 ---- batch: 030 ----
mean loss: 89.34
 ---- batch: 040 ----
mean loss: 86.87
train mean loss: 88.53
epoch train time: 0:00:00.205700
elapsed time: 0:00:26.299176
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:38:05.620616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.78
 ---- batch: 020 ----
mean loss: 84.66
 ---- batch: 030 ----
mean loss: 83.66
 ---- batch: 040 ----
mean loss: 86.33
train mean loss: 85.40
epoch train time: 0:00:00.206580
elapsed time: 0:00:26.505885
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:38:05.827340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.36
 ---- batch: 020 ----
mean loss: 83.58
 ---- batch: 030 ----
mean loss: 80.91
 ---- batch: 040 ----
mean loss: 81.38
train mean loss: 82.62
epoch train time: 0:00:00.202743
elapsed time: 0:00:26.708766
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:38:06.030207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.23
 ---- batch: 020 ----
mean loss: 79.90
 ---- batch: 030 ----
mean loss: 81.41
 ---- batch: 040 ----
mean loss: 79.35
train mean loss: 79.93
epoch train time: 0:00:00.203518
elapsed time: 0:00:26.912403
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:38:06.233842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.82
 ---- batch: 020 ----
mean loss: 78.33
 ---- batch: 030 ----
mean loss: 75.45
 ---- batch: 040 ----
mean loss: 78.12
train mean loss: 78.00
epoch train time: 0:00:00.201183
elapsed time: 0:00:27.113701
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:38:06.435137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.91
 ---- batch: 020 ----
mean loss: 77.06
 ---- batch: 030 ----
mean loss: 76.12
 ---- batch: 040 ----
mean loss: 76.94
train mean loss: 75.83
epoch train time: 0:00:00.199503
elapsed time: 0:00:27.313328
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:38:06.634765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.54
 ---- batch: 020 ----
mean loss: 72.93
 ---- batch: 030 ----
mean loss: 76.40
 ---- batch: 040 ----
mean loss: 71.72
train mean loss: 73.79
epoch train time: 0:00:00.197665
elapsed time: 0:00:27.511134
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:38:06.832571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.68
 ---- batch: 020 ----
mean loss: 70.47
 ---- batch: 030 ----
mean loss: 71.76
 ---- batch: 040 ----
mean loss: 73.99
train mean loss: 71.73
epoch train time: 0:00:00.197866
elapsed time: 0:00:27.709119
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:38:07.030589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.14
 ---- batch: 020 ----
mean loss: 71.36
 ---- batch: 030 ----
mean loss: 69.30
 ---- batch: 040 ----
mean loss: 70.39
train mean loss: 70.68
epoch train time: 0:00:00.194938
elapsed time: 0:00:27.904209
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:38:07.225648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.52
 ---- batch: 020 ----
mean loss: 69.19
 ---- batch: 030 ----
mean loss: 69.60
 ---- batch: 040 ----
mean loss: 67.75
train mean loss: 68.92
epoch train time: 0:00:00.195721
elapsed time: 0:00:28.100048
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:38:07.421488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.96
 ---- batch: 020 ----
mean loss: 71.20
 ---- batch: 030 ----
mean loss: 70.71
 ---- batch: 040 ----
mean loss: 65.48
train mean loss: 69.00
epoch train time: 0:00:00.201277
elapsed time: 0:00:28.301460
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:38:07.622899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.50
 ---- batch: 020 ----
mean loss: 66.53
 ---- batch: 030 ----
mean loss: 68.49
 ---- batch: 040 ----
mean loss: 68.31
train mean loss: 68.42
epoch train time: 0:00:00.211510
elapsed time: 0:00:28.513085
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:38:07.834520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.27
 ---- batch: 020 ----
mean loss: 64.11
 ---- batch: 030 ----
mean loss: 63.61
 ---- batch: 040 ----
mean loss: 67.70
train mean loss: 65.79
epoch train time: 0:00:00.201136
elapsed time: 0:00:28.714353
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:38:08.035790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.78
 ---- batch: 020 ----
mean loss: 62.80
 ---- batch: 030 ----
mean loss: 64.06
 ---- batch: 040 ----
mean loss: 66.46
train mean loss: 64.88
epoch train time: 0:00:00.203504
elapsed time: 0:00:28.917972
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:38:08.239422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.51
 ---- batch: 020 ----
mean loss: 65.27
 ---- batch: 030 ----
mean loss: 64.36
 ---- batch: 040 ----
mean loss: 62.24
train mean loss: 63.69
epoch train time: 0:00:00.202945
elapsed time: 0:00:29.121052
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:38:08.442502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.97
 ---- batch: 020 ----
mean loss: 68.10
 ---- batch: 030 ----
mean loss: 63.71
 ---- batch: 040 ----
mean loss: 63.30
train mean loss: 63.63
epoch train time: 0:00:00.204227
elapsed time: 0:00:29.325416
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:38:08.646855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.07
 ---- batch: 020 ----
mean loss: 59.65
 ---- batch: 030 ----
mean loss: 63.02
 ---- batch: 040 ----
mean loss: 64.26
train mean loss: 62.52
epoch train time: 0:00:00.206136
elapsed time: 0:00:29.531673
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:38:08.853110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.17
 ---- batch: 020 ----
mean loss: 61.92
 ---- batch: 030 ----
mean loss: 62.93
 ---- batch: 040 ----
mean loss: 62.06
train mean loss: 62.13
epoch train time: 0:00:00.205786
elapsed time: 0:00:29.737588
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:38:09.059052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.80
 ---- batch: 020 ----
mean loss: 61.98
 ---- batch: 030 ----
mean loss: 61.11
 ---- batch: 040 ----
mean loss: 58.35
train mean loss: 60.99
epoch train time: 0:00:00.202499
elapsed time: 0:00:29.940245
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:38:09.261684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.15
 ---- batch: 020 ----
mean loss: 57.43
 ---- batch: 030 ----
mean loss: 62.85
 ---- batch: 040 ----
mean loss: 62.39
train mean loss: 60.73
epoch train time: 0:00:00.202133
elapsed time: 0:00:30.142496
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:38:09.463934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.30
 ---- batch: 020 ----
mean loss: 61.51
 ---- batch: 030 ----
mean loss: 60.47
 ---- batch: 040 ----
mean loss: 58.57
train mean loss: 59.70
epoch train time: 0:00:00.203571
elapsed time: 0:00:30.346209
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:38:09.667648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.73
 ---- batch: 020 ----
mean loss: 56.03
 ---- batch: 030 ----
mean loss: 61.34
 ---- batch: 040 ----
mean loss: 60.02
train mean loss: 59.44
epoch train time: 0:00:00.205345
elapsed time: 0:00:30.551676
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:38:09.873116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.01
 ---- batch: 020 ----
mean loss: 57.63
 ---- batch: 030 ----
mean loss: 58.66
 ---- batch: 040 ----
mean loss: 59.42
train mean loss: 58.84
epoch train time: 0:00:00.201147
elapsed time: 0:00:30.752943
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:38:10.074380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.12
 ---- batch: 020 ----
mean loss: 55.18
 ---- batch: 030 ----
mean loss: 57.90
 ---- batch: 040 ----
mean loss: 58.90
train mean loss: 58.16
epoch train time: 0:00:00.197364
elapsed time: 0:00:30.950430
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:38:10.271866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.12
 ---- batch: 020 ----
mean loss: 58.20
 ---- batch: 030 ----
mean loss: 58.67
 ---- batch: 040 ----
mean loss: 60.51
train mean loss: 58.19
epoch train time: 0:00:00.197278
elapsed time: 0:00:31.147828
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:38:10.469273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.60
 ---- batch: 020 ----
mean loss: 56.19
 ---- batch: 030 ----
mean loss: 56.61
 ---- batch: 040 ----
mean loss: 58.77
train mean loss: 57.81
epoch train time: 0:00:00.198515
elapsed time: 0:00:31.346471
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:38:10.667908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.29
 ---- batch: 020 ----
mean loss: 58.16
 ---- batch: 030 ----
mean loss: 56.89
 ---- batch: 040 ----
mean loss: 58.93
train mean loss: 57.62
epoch train time: 0:00:00.207132
elapsed time: 0:00:31.553716
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:38:10.875171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.68
 ---- batch: 020 ----
mean loss: 54.69
 ---- batch: 030 ----
mean loss: 57.18
 ---- batch: 040 ----
mean loss: 55.55
train mean loss: 56.96
epoch train time: 0:00:00.198724
elapsed time: 0:00:31.752570
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:38:11.074024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.75
 ---- batch: 020 ----
mean loss: 56.42
 ---- batch: 030 ----
mean loss: 55.00
 ---- batch: 040 ----
mean loss: 57.24
train mean loss: 56.36
epoch train time: 0:00:00.194998
elapsed time: 0:00:31.947746
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:38:11.269199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.41
 ---- batch: 020 ----
mean loss: 56.19
 ---- batch: 030 ----
mean loss: 54.47
 ---- batch: 040 ----
mean loss: 57.85
train mean loss: 55.80
epoch train time: 0:00:00.198823
elapsed time: 0:00:32.146699
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:38:11.468136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.27
 ---- batch: 020 ----
mean loss: 55.41
 ---- batch: 030 ----
mean loss: 54.96
 ---- batch: 040 ----
mean loss: 53.91
train mean loss: 55.40
epoch train time: 0:00:00.198993
elapsed time: 0:00:32.345858
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:38:11.667308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.42
 ---- batch: 020 ----
mean loss: 54.55
 ---- batch: 030 ----
mean loss: 52.66
 ---- batch: 040 ----
mean loss: 53.80
train mean loss: 54.74
epoch train time: 0:00:00.211174
elapsed time: 0:00:32.557187
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:38:11.878626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.82
 ---- batch: 020 ----
mean loss: 53.02
 ---- batch: 030 ----
mean loss: 55.08
 ---- batch: 040 ----
mean loss: 57.12
train mean loss: 54.58
epoch train time: 0:00:00.197623
elapsed time: 0:00:32.754939
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:38:12.076376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.49
 ---- batch: 020 ----
mean loss: 56.52
 ---- batch: 030 ----
mean loss: 52.80
 ---- batch: 040 ----
mean loss: 55.89
train mean loss: 54.76
epoch train time: 0:00:00.202056
elapsed time: 0:00:32.957126
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:38:12.278580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.08
 ---- batch: 020 ----
mean loss: 55.10
 ---- batch: 030 ----
mean loss: 53.18
 ---- batch: 040 ----
mean loss: 52.52
train mean loss: 53.89
epoch train time: 0:00:00.201539
elapsed time: 0:00:33.158813
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:38:12.480250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.81
 ---- batch: 020 ----
mean loss: 53.97
 ---- batch: 030 ----
mean loss: 54.97
 ---- batch: 040 ----
mean loss: 53.84
train mean loss: 53.77
epoch train time: 0:00:00.203699
elapsed time: 0:00:33.362653
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:38:12.684149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.39
 ---- batch: 020 ----
mean loss: 54.87
 ---- batch: 030 ----
mean loss: 51.87
 ---- batch: 040 ----
mean loss: 52.98
train mean loss: 53.24
epoch train time: 0:00:00.205623
elapsed time: 0:00:33.568449
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:38:12.889887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.16
 ---- batch: 020 ----
mean loss: 55.52
 ---- batch: 030 ----
mean loss: 52.29
 ---- batch: 040 ----
mean loss: 53.76
train mean loss: 53.17
epoch train time: 0:00:00.200206
elapsed time: 0:00:33.768775
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:38:13.090214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.39
 ---- batch: 020 ----
mean loss: 55.39
 ---- batch: 030 ----
mean loss: 55.16
 ---- batch: 040 ----
mean loss: 53.12
train mean loss: 54.92
epoch train time: 0:00:00.199241
elapsed time: 0:00:33.968149
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:38:13.289600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.42
 ---- batch: 020 ----
mean loss: 55.01
 ---- batch: 030 ----
mean loss: 49.65
 ---- batch: 040 ----
mean loss: 52.20
train mean loss: 52.59
epoch train time: 0:00:00.199836
elapsed time: 0:00:34.168111
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:38:13.489546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.12
 ---- batch: 020 ----
mean loss: 51.18
 ---- batch: 030 ----
mean loss: 49.92
 ---- batch: 040 ----
mean loss: 53.04
train mean loss: 52.01
epoch train time: 0:00:00.201849
elapsed time: 0:00:34.370071
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:38:13.691508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.99
 ---- batch: 020 ----
mean loss: 52.68
 ---- batch: 030 ----
mean loss: 51.26
 ---- batch: 040 ----
mean loss: 53.55
train mean loss: 51.85
epoch train time: 0:00:00.202578
elapsed time: 0:00:34.572764
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:38:13.894200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.34
 ---- batch: 020 ----
mean loss: 55.63
 ---- batch: 030 ----
mean loss: 52.87
 ---- batch: 040 ----
mean loss: 48.50
train mean loss: 52.56
epoch train time: 0:00:00.198795
elapsed time: 0:00:34.771680
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:38:14.093115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.13
 ---- batch: 020 ----
mean loss: 50.92
 ---- batch: 030 ----
mean loss: 51.95
 ---- batch: 040 ----
mean loss: 52.02
train mean loss: 51.28
epoch train time: 0:00:00.196974
elapsed time: 0:00:34.968764
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:38:14.290200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.91
 ---- batch: 020 ----
mean loss: 49.57
 ---- batch: 030 ----
mean loss: 51.04
 ---- batch: 040 ----
mean loss: 52.69
train mean loss: 51.33
epoch train time: 0:00:00.194997
elapsed time: 0:00:35.163871
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:38:14.485305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.27
 ---- batch: 020 ----
mean loss: 48.81
 ---- batch: 030 ----
mean loss: 51.70
 ---- batch: 040 ----
mean loss: 52.95
train mean loss: 50.77
epoch train time: 0:00:00.196220
elapsed time: 0:00:35.360232
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:38:14.681669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.57
 ---- batch: 020 ----
mean loss: 52.31
 ---- batch: 030 ----
mean loss: 50.98
 ---- batch: 040 ----
mean loss: 48.25
train mean loss: 50.72
epoch train time: 0:00:00.206224
elapsed time: 0:00:35.566563
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:38:14.887998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.39
 ---- batch: 020 ----
mean loss: 50.72
 ---- batch: 030 ----
mean loss: 47.64
 ---- batch: 040 ----
mean loss: 52.43
train mean loss: 50.34
epoch train time: 0:00:00.193627
elapsed time: 0:00:35.760302
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:38:15.081747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.89
 ---- batch: 020 ----
mean loss: 50.91
 ---- batch: 030 ----
mean loss: 49.91
 ---- batch: 040 ----
mean loss: 54.67
train mean loss: 50.78
epoch train time: 0:00:00.193726
elapsed time: 0:00:35.954155
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:38:15.275594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.15
 ---- batch: 020 ----
mean loss: 47.89
 ---- batch: 030 ----
mean loss: 50.54
 ---- batch: 040 ----
mean loss: 50.92
train mean loss: 49.91
epoch train time: 0:00:00.190598
elapsed time: 0:00:36.144876
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:38:15.466305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.16
 ---- batch: 020 ----
mean loss: 50.41
 ---- batch: 030 ----
mean loss: 48.90
 ---- batch: 040 ----
mean loss: 49.03
train mean loss: 49.90
epoch train time: 0:00:00.192110
elapsed time: 0:00:36.337091
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:38:15.658544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.38
 ---- batch: 020 ----
mean loss: 47.87
 ---- batch: 030 ----
mean loss: 48.60
 ---- batch: 040 ----
mean loss: 53.55
train mean loss: 49.97
epoch train time: 0:00:00.197568
elapsed time: 0:00:36.534789
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:38:15.856226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.00
 ---- batch: 020 ----
mean loss: 47.21
 ---- batch: 030 ----
mean loss: 51.48
 ---- batch: 040 ----
mean loss: 46.76
train mean loss: 49.07
epoch train time: 0:00:00.199379
elapsed time: 0:00:36.734302
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:38:16.055739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.26
 ---- batch: 020 ----
mean loss: 48.19
 ---- batch: 030 ----
mean loss: 49.74
 ---- batch: 040 ----
mean loss: 48.04
train mean loss: 48.67
epoch train time: 0:00:00.199856
elapsed time: 0:00:36.934278
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:38:16.255718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.37
 ---- batch: 020 ----
mean loss: 50.01
 ---- batch: 030 ----
mean loss: 47.98
 ---- batch: 040 ----
mean loss: 47.95
train mean loss: 48.86
epoch train time: 0:00:00.198877
elapsed time: 0:00:37.133271
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:38:16.454707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.34
 ---- batch: 020 ----
mean loss: 50.11
 ---- batch: 030 ----
mean loss: 47.59
 ---- batch: 040 ----
mean loss: 47.09
train mean loss: 49.04
epoch train time: 0:00:00.200767
elapsed time: 0:00:37.334155
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:38:16.655593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.87
 ---- batch: 020 ----
mean loss: 49.69
 ---- batch: 030 ----
mean loss: 47.86
 ---- batch: 040 ----
mean loss: 46.09
train mean loss: 48.05
epoch train time: 0:00:00.207676
elapsed time: 0:00:37.541954
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:38:16.863395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.41
 ---- batch: 020 ----
mean loss: 46.75
 ---- batch: 030 ----
mean loss: 48.93
 ---- batch: 040 ----
mean loss: 48.48
train mean loss: 48.22
epoch train time: 0:00:00.201150
elapsed time: 0:00:37.743226
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:38:17.064665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.09
 ---- batch: 020 ----
mean loss: 50.15
 ---- batch: 030 ----
mean loss: 47.56
 ---- batch: 040 ----
mean loss: 47.36
train mean loss: 48.67
epoch train time: 0:00:00.199965
elapsed time: 0:00:37.943312
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:38:17.264750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.75
 ---- batch: 020 ----
mean loss: 48.06
 ---- batch: 030 ----
mean loss: 46.63
 ---- batch: 040 ----
mean loss: 51.65
train mean loss: 48.10
epoch train time: 0:00:00.199461
elapsed time: 0:00:38.142891
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:38:17.464330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.11
 ---- batch: 020 ----
mean loss: 50.54
 ---- batch: 030 ----
mean loss: 47.40
 ---- batch: 040 ----
mean loss: 45.81
train mean loss: 48.67
epoch train time: 0:00:00.199221
elapsed time: 0:00:38.342228
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:38:17.663668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.84
 ---- batch: 020 ----
mean loss: 46.01
 ---- batch: 030 ----
mean loss: 46.74
 ---- batch: 040 ----
mean loss: 48.18
train mean loss: 47.22
epoch train time: 0:00:00.195085
elapsed time: 0:00:38.537434
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:38:17.858871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.38
 ---- batch: 020 ----
mean loss: 47.56
 ---- batch: 030 ----
mean loss: 51.20
 ---- batch: 040 ----
mean loss: 45.59
train mean loss: 48.07
epoch train time: 0:00:00.199471
elapsed time: 0:00:38.737021
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:38:18.058473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.69
 ---- batch: 020 ----
mean loss: 47.49
 ---- batch: 030 ----
mean loss: 47.63
 ---- batch: 040 ----
mean loss: 47.25
train mean loss: 46.83
epoch train time: 0:00:00.196615
elapsed time: 0:00:38.933764
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:38:18.255199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.39
 ---- batch: 020 ----
mean loss: 48.48
 ---- batch: 030 ----
mean loss: 44.00
 ---- batch: 040 ----
mean loss: 50.65
train mean loss: 46.95
epoch train time: 0:00:00.198066
elapsed time: 0:00:39.131941
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:38:18.453378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.34
 ---- batch: 020 ----
mean loss: 47.23
 ---- batch: 030 ----
mean loss: 46.73
 ---- batch: 040 ----
mean loss: 46.66
train mean loss: 46.75
epoch train time: 0:00:00.196593
elapsed time: 0:00:39.328652
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:38:18.650092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.80
 ---- batch: 020 ----
mean loss: 46.81
 ---- batch: 030 ----
mean loss: 46.46
 ---- batch: 040 ----
mean loss: 44.83
train mean loss: 46.29
epoch train time: 0:00:00.201754
elapsed time: 0:00:39.530541
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:38:18.852004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.28
 ---- batch: 020 ----
mean loss: 44.04
 ---- batch: 030 ----
mean loss: 43.67
 ---- batch: 040 ----
mean loss: 46.32
train mean loss: 45.60
epoch train time: 0:00:00.201433
elapsed time: 0:00:39.732116
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:38:19.053571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.90
 ---- batch: 020 ----
mean loss: 44.67
 ---- batch: 030 ----
mean loss: 48.68
 ---- batch: 040 ----
mean loss: 45.52
train mean loss: 45.73
epoch train time: 0:00:00.194224
elapsed time: 0:00:39.926469
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:38:19.247905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.82
 ---- batch: 020 ----
mean loss: 46.78
 ---- batch: 030 ----
mean loss: 45.00
 ---- batch: 040 ----
mean loss: 44.63
train mean loss: 46.05
epoch train time: 0:00:00.193122
elapsed time: 0:00:40.119745
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:38:19.441174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.00
 ---- batch: 020 ----
mean loss: 43.98
 ---- batch: 030 ----
mean loss: 45.35
 ---- batch: 040 ----
mean loss: 44.89
train mean loss: 45.18
epoch train time: 0:00:00.201246
elapsed time: 0:00:40.321099
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:38:19.642536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.88
 ---- batch: 020 ----
mean loss: 45.45
 ---- batch: 030 ----
mean loss: 44.27
 ---- batch: 040 ----
mean loss: 47.73
train mean loss: 45.36
epoch train time: 0:00:00.206925
elapsed time: 0:00:40.528143
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:38:19.849583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.38
 ---- batch: 020 ----
mean loss: 45.01
 ---- batch: 030 ----
mean loss: 46.43
 ---- batch: 040 ----
mean loss: 42.24
train mean loss: 45.11
epoch train time: 0:00:00.205484
elapsed time: 0:00:40.733749
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:38:20.055188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.38
 ---- batch: 020 ----
mean loss: 45.69
 ---- batch: 030 ----
mean loss: 43.26
 ---- batch: 040 ----
mean loss: 45.67
train mean loss: 44.81
epoch train time: 0:00:00.203734
elapsed time: 0:00:40.937602
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:38:20.259040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.75
 ---- batch: 020 ----
mean loss: 45.92
 ---- batch: 030 ----
mean loss: 42.97
 ---- batch: 040 ----
mean loss: 43.80
train mean loss: 45.35
epoch train time: 0:00:00.205074
elapsed time: 0:00:41.142797
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:38:20.464250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.10
 ---- batch: 020 ----
mean loss: 45.19
 ---- batch: 030 ----
mean loss: 43.60
 ---- batch: 040 ----
mean loss: 42.96
train mean loss: 44.56
epoch train time: 0:00:00.204645
elapsed time: 0:00:41.347577
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:38:20.669015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.48
 ---- batch: 020 ----
mean loss: 43.91
 ---- batch: 030 ----
mean loss: 45.52
 ---- batch: 040 ----
mean loss: 44.01
train mean loss: 43.87
epoch train time: 0:00:00.204595
elapsed time: 0:00:41.552288
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:38:20.873724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.14
 ---- batch: 020 ----
mean loss: 45.00
 ---- batch: 030 ----
mean loss: 43.82
 ---- batch: 040 ----
mean loss: 43.41
train mean loss: 44.21
epoch train time: 0:00:00.199587
elapsed time: 0:00:41.752021
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:38:21.073460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.66
 ---- batch: 020 ----
mean loss: 43.32
 ---- batch: 030 ----
mean loss: 42.98
 ---- batch: 040 ----
mean loss: 45.27
train mean loss: 43.50
epoch train time: 0:00:00.197777
elapsed time: 0:00:41.949918
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:38:21.271354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.82
 ---- batch: 020 ----
mean loss: 44.23
 ---- batch: 030 ----
mean loss: 42.89
 ---- batch: 040 ----
mean loss: 42.77
train mean loss: 43.28
epoch train time: 0:00:00.196818
elapsed time: 0:00:42.146864
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:38:21.468300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.22
 ---- batch: 020 ----
mean loss: 43.16
 ---- batch: 030 ----
mean loss: 45.03
 ---- batch: 040 ----
mean loss: 44.05
train mean loss: 43.89
epoch train time: 0:00:00.195187
elapsed time: 0:00:42.342179
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:38:21.663616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.41
 ---- batch: 020 ----
mean loss: 46.91
 ---- batch: 030 ----
mean loss: 43.00
 ---- batch: 040 ----
mean loss: 42.38
train mean loss: 43.98
epoch train time: 0:00:00.194711
elapsed time: 0:00:42.537005
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:38:21.858443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.14
 ---- batch: 020 ----
mean loss: 41.76
 ---- batch: 030 ----
mean loss: 42.76
 ---- batch: 040 ----
mean loss: 42.52
train mean loss: 42.76
epoch train time: 0:00:00.193048
elapsed time: 0:00:42.730179
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:38:22.051631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.53
 ---- batch: 020 ----
mean loss: 40.24
 ---- batch: 030 ----
mean loss: 43.09
 ---- batch: 040 ----
mean loss: 42.95
train mean loss: 42.42
epoch train time: 0:00:00.189379
elapsed time: 0:00:42.919682
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:38:22.241116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.06
 ---- batch: 020 ----
mean loss: 43.30
 ---- batch: 030 ----
mean loss: 44.10
 ---- batch: 040 ----
mean loss: 42.76
train mean loss: 42.51
epoch train time: 0:00:00.193421
elapsed time: 0:00:43.113212
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:38:22.434661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.21
 ---- batch: 020 ----
mean loss: 44.53
 ---- batch: 030 ----
mean loss: 43.98
 ---- batch: 040 ----
mean loss: 41.41
train mean loss: 42.44
epoch train time: 0:00:00.188468
elapsed time: 0:00:43.301803
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:38:22.623237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.80
 ---- batch: 020 ----
mean loss: 44.32
 ---- batch: 030 ----
mean loss: 43.39
 ---- batch: 040 ----
mean loss: 41.99
train mean loss: 43.02
epoch train time: 0:00:00.194709
elapsed time: 0:00:43.496620
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:38:22.818070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.43
 ---- batch: 020 ----
mean loss: 43.93
 ---- batch: 030 ----
mean loss: 42.11
 ---- batch: 040 ----
mean loss: 40.20
train mean loss: 42.00
epoch train time: 0:00:00.193725
elapsed time: 0:00:43.690469
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:38:23.011903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.16
 ---- batch: 020 ----
mean loss: 39.06
 ---- batch: 030 ----
mean loss: 42.05
 ---- batch: 040 ----
mean loss: 42.43
train mean loss: 41.61
epoch train time: 0:00:00.189216
elapsed time: 0:00:43.879805
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:38:23.201238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.19
 ---- batch: 020 ----
mean loss: 43.48
 ---- batch: 030 ----
mean loss: 41.12
 ---- batch: 040 ----
mean loss: 43.41
train mean loss: 42.27
epoch train time: 0:00:00.194801
elapsed time: 0:00:44.074718
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:38:23.396170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.83
 ---- batch: 020 ----
mean loss: 41.03
 ---- batch: 030 ----
mean loss: 42.39
 ---- batch: 040 ----
mean loss: 39.93
train mean loss: 41.91
epoch train time: 0:00:00.199327
elapsed time: 0:00:44.274178
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:38:23.595617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.37
 ---- batch: 020 ----
mean loss: 43.99
 ---- batch: 030 ----
mean loss: 40.17
 ---- batch: 040 ----
mean loss: 39.39
train mean loss: 40.78
epoch train time: 0:00:00.209788
elapsed time: 0:00:44.484100
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:38:23.805534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.72
 ---- batch: 020 ----
mean loss: 41.14
 ---- batch: 030 ----
mean loss: 41.38
 ---- batch: 040 ----
mean loss: 40.68
train mean loss: 41.14
epoch train time: 0:00:00.206354
elapsed time: 0:00:44.690568
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:38:24.012025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.28
 ---- batch: 020 ----
mean loss: 40.92
 ---- batch: 030 ----
mean loss: 41.26
 ---- batch: 040 ----
mean loss: 42.82
train mean loss: 41.03
epoch train time: 0:00:00.205779
elapsed time: 0:00:44.896491
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:38:24.217933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.83
 ---- batch: 020 ----
mean loss: 40.53
 ---- batch: 030 ----
mean loss: 40.09
 ---- batch: 040 ----
mean loss: 39.79
train mean loss: 40.16
epoch train time: 0:00:00.204037
elapsed time: 0:00:45.100654
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:38:24.422110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.16
 ---- batch: 020 ----
mean loss: 40.37
 ---- batch: 030 ----
mean loss: 41.61
 ---- batch: 040 ----
mean loss: 40.75
train mean loss: 40.64
epoch train time: 0:00:00.204038
elapsed time: 0:00:45.304826
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:38:24.626263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.16
 ---- batch: 020 ----
mean loss: 40.99
 ---- batch: 030 ----
mean loss: 37.82
 ---- batch: 040 ----
mean loss: 38.90
train mean loss: 39.89
epoch train time: 0:00:00.200192
elapsed time: 0:00:45.505135
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:38:24.826573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.98
 ---- batch: 020 ----
mean loss: 39.52
 ---- batch: 030 ----
mean loss: 40.23
 ---- batch: 040 ----
mean loss: 40.35
train mean loss: 40.05
epoch train time: 0:00:00.200219
elapsed time: 0:00:45.705470
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:38:25.026909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.89
 ---- batch: 020 ----
mean loss: 39.81
 ---- batch: 030 ----
mean loss: 41.62
 ---- batch: 040 ----
mean loss: 38.75
train mean loss: 39.73
epoch train time: 0:00:00.196317
elapsed time: 0:00:45.901913
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:38:25.223377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.82
 ---- batch: 020 ----
mean loss: 40.20
 ---- batch: 030 ----
mean loss: 39.77
 ---- batch: 040 ----
mean loss: 40.05
train mean loss: 39.62
epoch train time: 0:00:00.191172
elapsed time: 0:00:46.093235
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:38:25.414669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.38
 ---- batch: 020 ----
mean loss: 40.13
 ---- batch: 030 ----
mean loss: 39.10
 ---- batch: 040 ----
mean loss: 39.50
train mean loss: 39.13
epoch train time: 0:00:00.189564
elapsed time: 0:00:46.282916
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:38:25.604352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.37
 ---- batch: 020 ----
mean loss: 38.85
 ---- batch: 030 ----
mean loss: 39.25
 ---- batch: 040 ----
mean loss: 37.92
train mean loss: 38.90
epoch train time: 0:00:00.200935
elapsed time: 0:00:46.483993
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:38:25.805434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.61
 ---- batch: 020 ----
mean loss: 37.31
 ---- batch: 030 ----
mean loss: 40.42
 ---- batch: 040 ----
mean loss: 41.26
train mean loss: 39.24
epoch train time: 0:00:00.196276
elapsed time: 0:00:46.680395
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:38:26.001847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.93
 ---- batch: 020 ----
mean loss: 37.84
 ---- batch: 030 ----
mean loss: 38.19
 ---- batch: 040 ----
mean loss: 40.05
train mean loss: 38.68
epoch train time: 0:00:00.198970
elapsed time: 0:00:46.879495
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:38:26.200934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.05
 ---- batch: 020 ----
mean loss: 37.29
 ---- batch: 030 ----
mean loss: 39.35
 ---- batch: 040 ----
mean loss: 39.43
train mean loss: 38.74
epoch train time: 0:00:00.198383
elapsed time: 0:00:47.077992
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:38:26.399428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.61
 ---- batch: 020 ----
mean loss: 38.93
 ---- batch: 030 ----
mean loss: 37.30
 ---- batch: 040 ----
mean loss: 39.27
train mean loss: 38.31
epoch train time: 0:00:00.196299
elapsed time: 0:00:47.274402
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:38:26.595838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.22
 ---- batch: 020 ----
mean loss: 37.15
 ---- batch: 030 ----
mean loss: 38.69
 ---- batch: 040 ----
mean loss: 39.18
train mean loss: 37.98
epoch train time: 0:00:00.195934
elapsed time: 0:00:47.470448
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:38:26.791884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.76
 ---- batch: 020 ----
mean loss: 37.67
 ---- batch: 030 ----
mean loss: 38.80
 ---- batch: 040 ----
mean loss: 37.31
train mean loss: 37.90
epoch train time: 0:00:00.198936
elapsed time: 0:00:47.669498
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:38:26.990935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.61
 ---- batch: 020 ----
mean loss: 38.24
 ---- batch: 030 ----
mean loss: 37.68
 ---- batch: 040 ----
mean loss: 38.71
train mean loss: 38.31
epoch train time: 0:00:00.199562
elapsed time: 0:00:47.869191
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:38:27.190628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.60
 ---- batch: 020 ----
mean loss: 37.49
 ---- batch: 030 ----
mean loss: 38.27
 ---- batch: 040 ----
mean loss: 35.37
train mean loss: 37.62
epoch train time: 0:00:00.203064
elapsed time: 0:00:48.072423
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:38:27.393863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.86
 ---- batch: 020 ----
mean loss: 38.89
 ---- batch: 030 ----
mean loss: 36.75
 ---- batch: 040 ----
mean loss: 36.21
train mean loss: 37.39
epoch train time: 0:00:00.201112
elapsed time: 0:00:48.273655
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:38:27.595109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.97
 ---- batch: 020 ----
mean loss: 38.48
 ---- batch: 030 ----
mean loss: 41.81
 ---- batch: 040 ----
mean loss: 37.23
train mean loss: 38.67
epoch train time: 0:00:00.201427
elapsed time: 0:00:48.475221
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:38:27.796659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.39
 ---- batch: 020 ----
mean loss: 35.98
 ---- batch: 030 ----
mean loss: 39.50
 ---- batch: 040 ----
mean loss: 38.58
train mean loss: 37.41
epoch train time: 0:00:00.200897
elapsed time: 0:00:48.676259
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:38:27.997712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.58
 ---- batch: 020 ----
mean loss: 36.04
 ---- batch: 030 ----
mean loss: 36.89
 ---- batch: 040 ----
mean loss: 37.14
train mean loss: 36.83
epoch train time: 0:00:00.200086
elapsed time: 0:00:48.876477
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:38:28.197913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.19
 ---- batch: 020 ----
mean loss: 35.33
 ---- batch: 030 ----
mean loss: 38.85
 ---- batch: 040 ----
mean loss: 37.33
train mean loss: 36.85
epoch train time: 0:00:00.198834
elapsed time: 0:00:49.075428
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:38:28.396866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.12
 ---- batch: 020 ----
mean loss: 38.61
 ---- batch: 030 ----
mean loss: 37.24
 ---- batch: 040 ----
mean loss: 38.86
train mean loss: 37.96
epoch train time: 0:00:00.200374
elapsed time: 0:00:49.275935
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:38:28.597374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.96
 ---- batch: 020 ----
mean loss: 39.42
 ---- batch: 030 ----
mean loss: 38.09
 ---- batch: 040 ----
mean loss: 38.32
train mean loss: 37.84
epoch train time: 0:00:00.202877
elapsed time: 0:00:49.478949
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:38:28.800412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.11
 ---- batch: 020 ----
mean loss: 36.05
 ---- batch: 030 ----
mean loss: 36.77
 ---- batch: 040 ----
mean loss: 35.78
train mean loss: 36.47
epoch train time: 0:00:00.199894
elapsed time: 0:00:49.678983
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:38:29.000442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.12
 ---- batch: 020 ----
mean loss: 37.74
 ---- batch: 030 ----
mean loss: 36.04
 ---- batch: 040 ----
mean loss: 35.94
train mean loss: 36.83
epoch train time: 0:00:00.197430
elapsed time: 0:00:49.876547
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:38:29.197982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.20
 ---- batch: 020 ----
mean loss: 36.33
 ---- batch: 030 ----
mean loss: 35.92
 ---- batch: 040 ----
mean loss: 37.47
train mean loss: 35.93
epoch train time: 0:00:00.194426
elapsed time: 0:00:50.071118
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:38:29.392555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.39
 ---- batch: 020 ----
mean loss: 34.42
 ---- batch: 030 ----
mean loss: 34.14
 ---- batch: 040 ----
mean loss: 37.13
train mean loss: 35.76
epoch train time: 0:00:00.191535
elapsed time: 0:00:50.262764
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:38:29.584199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.07
 ---- batch: 020 ----
mean loss: 37.20
 ---- batch: 030 ----
mean loss: 33.93
 ---- batch: 040 ----
mean loss: 36.16
train mean loss: 36.23
epoch train time: 0:00:00.199657
elapsed time: 0:00:50.462551
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:38:29.783989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.07
 ---- batch: 020 ----
mean loss: 40.39
 ---- batch: 030 ----
mean loss: 35.71
 ---- batch: 040 ----
mean loss: 34.06
train mean loss: 37.31
epoch train time: 0:00:00.199585
elapsed time: 0:00:50.662256
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:38:29.983740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.83
 ---- batch: 020 ----
mean loss: 36.60
 ---- batch: 030 ----
mean loss: 35.09
 ---- batch: 040 ----
mean loss: 34.65
train mean loss: 35.54
epoch train time: 0:00:00.200088
elapsed time: 0:00:50.862518
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:38:30.183961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.28
 ---- batch: 020 ----
mean loss: 34.62
 ---- batch: 030 ----
mean loss: 34.63
 ---- batch: 040 ----
mean loss: 35.48
train mean loss: 35.22
epoch train time: 0:00:00.198531
elapsed time: 0:00:51.061186
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:38:30.382624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.27
 ---- batch: 020 ----
mean loss: 34.99
 ---- batch: 030 ----
mean loss: 32.92
 ---- batch: 040 ----
mean loss: 36.84
train mean loss: 35.13
epoch train time: 0:00:00.196835
elapsed time: 0:00:51.258165
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:38:30.579633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.00
 ---- batch: 020 ----
mean loss: 35.14
 ---- batch: 030 ----
mean loss: 37.22
 ---- batch: 040 ----
mean loss: 35.46
train mean loss: 35.61
epoch train time: 0:00:00.190989
elapsed time: 0:00:51.449320
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:38:30.770757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.05
 ---- batch: 020 ----
mean loss: 37.27
 ---- batch: 030 ----
mean loss: 34.82
 ---- batch: 040 ----
mean loss: 38.24
train mean loss: 36.01
epoch train time: 0:00:00.200338
elapsed time: 0:00:51.649776
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:38:30.971215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.66
 ---- batch: 020 ----
mean loss: 33.18
 ---- batch: 030 ----
mean loss: 36.95
 ---- batch: 040 ----
mean loss: 33.66
train mean loss: 34.64
epoch train time: 0:00:00.198704
elapsed time: 0:00:51.848611
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:38:31.170046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.11
 ---- batch: 020 ----
mean loss: 35.42
 ---- batch: 030 ----
mean loss: 35.46
 ---- batch: 040 ----
mean loss: 30.73
train mean loss: 34.67
epoch train time: 0:00:00.200688
elapsed time: 0:00:52.049472
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:38:31.370931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.27
 ---- batch: 020 ----
mean loss: 37.34
 ---- batch: 030 ----
mean loss: 34.78
 ---- batch: 040 ----
mean loss: 36.37
train mean loss: 35.36
epoch train time: 0:00:00.202003
elapsed time: 0:00:52.251613
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:38:31.573049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.66
 ---- batch: 020 ----
mean loss: 34.21
 ---- batch: 030 ----
mean loss: 34.39
 ---- batch: 040 ----
mean loss: 35.89
train mean loss: 34.37
epoch train time: 0:00:00.202117
elapsed time: 0:00:52.453845
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:38:31.775283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 32.99
 ---- batch: 020 ----
mean loss: 34.02
 ---- batch: 030 ----
mean loss: 35.81
 ---- batch: 040 ----
mean loss: 36.74
train mean loss: 34.83
epoch train time: 0:00:00.205323
elapsed time: 0:00:52.659287
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:38:31.980733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.10
 ---- batch: 020 ----
mean loss: 36.03
 ---- batch: 030 ----
mean loss: 32.55
 ---- batch: 040 ----
mean loss: 34.95
train mean loss: 34.21
epoch train time: 0:00:00.204147
elapsed time: 0:00:52.863561
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:38:32.184999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 32.60
 ---- batch: 020 ----
mean loss: 33.03
 ---- batch: 030 ----
mean loss: 34.63
 ---- batch: 040 ----
mean loss: 33.88
train mean loss: 33.66
epoch train time: 0:00:00.205537
elapsed time: 0:00:53.069232
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:38:32.390670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.92
 ---- batch: 020 ----
mean loss: 33.22
 ---- batch: 030 ----
mean loss: 35.14
 ---- batch: 040 ----
mean loss: 32.29
train mean loss: 34.28
epoch train time: 0:00:00.203052
elapsed time: 0:00:53.272409
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:38:32.593858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.86
 ---- batch: 020 ----
mean loss: 33.21
 ---- batch: 030 ----
mean loss: 35.78
 ---- batch: 040 ----
mean loss: 33.34
train mean loss: 34.50
epoch train time: 0:00:00.201223
elapsed time: 0:00:53.473775
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:38:32.795214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.25
 ---- batch: 020 ----
mean loss: 35.06
 ---- batch: 030 ----
mean loss: 33.34
 ---- batch: 040 ----
mean loss: 31.85
train mean loss: 33.86
epoch train time: 0:00:00.201537
elapsed time: 0:00:53.675438
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:38:32.996877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 32.71
 ---- batch: 020 ----
mean loss: 32.71
 ---- batch: 030 ----
mean loss: 34.68
 ---- batch: 040 ----
mean loss: 34.50
train mean loss: 33.79
epoch train time: 0:00:00.195978
elapsed time: 0:00:53.871538
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:38:33.192979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.80
 ---- batch: 020 ----
mean loss: 34.39
 ---- batch: 030 ----
mean loss: 32.99
 ---- batch: 040 ----
mean loss: 33.80
train mean loss: 33.45
epoch train time: 0:00:00.197503
elapsed time: 0:00:54.069159
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:38:33.390598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.85
 ---- batch: 020 ----
mean loss: 33.38
 ---- batch: 030 ----
mean loss: 32.43
 ---- batch: 040 ----
mean loss: 32.40
train mean loss: 32.91
epoch train time: 0:00:00.199550
elapsed time: 0:00:54.268870
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:38:33.590309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 32.55
 ---- batch: 020 ----
mean loss: 32.07
 ---- batch: 030 ----
mean loss: 33.99
 ---- batch: 040 ----
mean loss: 33.52
train mean loss: 32.83
epoch train time: 0:00:00.194254
elapsed time: 0:00:54.463259
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:38:33.784697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 32.27
 ---- batch: 020 ----
mean loss: 33.10
 ---- batch: 030 ----
mean loss: 33.64
 ---- batch: 040 ----
mean loss: 32.38
train mean loss: 33.01
epoch train time: 0:00:00.196030
elapsed time: 0:00:54.659402
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:38:33.980841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 32.36
 ---- batch: 020 ----
mean loss: 32.79
 ---- batch: 030 ----
mean loss: 33.80
 ---- batch: 040 ----
mean loss: 34.49
train mean loss: 33.18
epoch train time: 0:00:00.192380
elapsed time: 0:00:54.851893
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:38:34.173328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.51
 ---- batch: 020 ----
mean loss: 31.24
 ---- batch: 030 ----
mean loss: 31.03
 ---- batch: 040 ----
mean loss: 32.37
train mean loss: 31.52
epoch train time: 0:00:00.192912
elapsed time: 0:00:55.044922
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:38:34.366349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.95
 ---- batch: 020 ----
mean loss: 32.36
 ---- batch: 030 ----
mean loss: 31.82
 ---- batch: 040 ----
mean loss: 30.74
train mean loss: 31.28
epoch train time: 0:00:00.191842
elapsed time: 0:00:55.236868
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:38:34.558304
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.92
 ---- batch: 020 ----
mean loss: 31.96
 ---- batch: 030 ----
mean loss: 30.40
 ---- batch: 040 ----
mean loss: 30.56
train mean loss: 31.17
epoch train time: 0:00:00.209899
elapsed time: 0:00:55.446885
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:38:34.768323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.14
 ---- batch: 020 ----
mean loss: 31.38
 ---- batch: 030 ----
mean loss: 31.71
 ---- batch: 040 ----
mean loss: 32.47
train mean loss: 31.24
epoch train time: 0:00:00.208055
elapsed time: 0:00:55.655098
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:38:34.976570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.77
 ---- batch: 020 ----
mean loss: 31.52
 ---- batch: 030 ----
mean loss: 31.89
 ---- batch: 040 ----
mean loss: 30.77
train mean loss: 31.24
epoch train time: 0:00:00.204733
elapsed time: 0:00:55.859983
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:38:35.181421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.54
 ---- batch: 020 ----
mean loss: 30.83
 ---- batch: 030 ----
mean loss: 31.40
 ---- batch: 040 ----
mean loss: 30.97
train mean loss: 31.21
epoch train time: 0:00:00.204114
elapsed time: 0:00:56.064221
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:38:35.385659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.61
 ---- batch: 020 ----
mean loss: 29.68
 ---- batch: 030 ----
mean loss: 31.05
 ---- batch: 040 ----
mean loss: 31.87
train mean loss: 31.30
epoch train time: 0:00:00.203546
elapsed time: 0:00:56.267892
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:38:35.589330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.95
 ---- batch: 020 ----
mean loss: 31.16
 ---- batch: 030 ----
mean loss: 30.27
 ---- batch: 040 ----
mean loss: 30.06
train mean loss: 31.16
epoch train time: 0:00:00.202067
elapsed time: 0:00:56.470076
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:38:35.791514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.07
 ---- batch: 020 ----
mean loss: 31.26
 ---- batch: 030 ----
mean loss: 29.57
 ---- batch: 040 ----
mean loss: 33.83
train mean loss: 31.14
epoch train time: 0:00:00.200688
elapsed time: 0:00:56.670896
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:38:35.992365
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.27
 ---- batch: 020 ----
mean loss: 32.25
 ---- batch: 030 ----
mean loss: 31.19
 ---- batch: 040 ----
mean loss: 30.29
train mean loss: 31.15
epoch train time: 0:00:00.198046
elapsed time: 0:00:56.869105
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:38:36.190544
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.13
 ---- batch: 020 ----
mean loss: 30.46
 ---- batch: 030 ----
mean loss: 30.51
 ---- batch: 040 ----
mean loss: 31.23
train mean loss: 31.11
epoch train time: 0:00:00.197781
elapsed time: 0:00:57.067021
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:38:36.388462
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.23
 ---- batch: 020 ----
mean loss: 30.75
 ---- batch: 030 ----
mean loss: 29.75
 ---- batch: 040 ----
mean loss: 32.47
train mean loss: 31.12
epoch train time: 0:00:00.195733
elapsed time: 0:00:57.262890
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:38:36.584328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.00
 ---- batch: 020 ----
mean loss: 29.48
 ---- batch: 030 ----
mean loss: 30.11
 ---- batch: 040 ----
mean loss: 32.05
train mean loss: 31.14
epoch train time: 0:00:00.195239
elapsed time: 0:00:57.458245
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:38:36.779698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.33
 ---- batch: 020 ----
mean loss: 30.98
 ---- batch: 030 ----
mean loss: 31.33
 ---- batch: 040 ----
mean loss: 31.77
train mean loss: 31.05
epoch train time: 0:00:00.198625
elapsed time: 0:00:57.657050
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:38:36.978485
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.58
 ---- batch: 020 ----
mean loss: 30.22
 ---- batch: 030 ----
mean loss: 32.02
 ---- batch: 040 ----
mean loss: 31.99
train mean loss: 31.02
epoch train time: 0:00:00.194265
elapsed time: 0:00:57.851427
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:38:37.172863
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.06
 ---- batch: 020 ----
mean loss: 30.87
 ---- batch: 030 ----
mean loss: 31.11
 ---- batch: 040 ----
mean loss: 29.22
train mean loss: 31.09
epoch train time: 0:00:00.198534
elapsed time: 0:00:58.050068
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:38:37.371515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.00
 ---- batch: 020 ----
mean loss: 31.74
 ---- batch: 030 ----
mean loss: 31.43
 ---- batch: 040 ----
mean loss: 29.90
train mean loss: 31.00
epoch train time: 0:00:00.205493
elapsed time: 0:00:58.255706
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:38:37.577144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.16
 ---- batch: 020 ----
mean loss: 32.61
 ---- batch: 030 ----
mean loss: 30.62
 ---- batch: 040 ----
mean loss: 31.38
train mean loss: 30.98
epoch train time: 0:00:00.203132
elapsed time: 0:00:58.458953
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:38:37.780409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.12
 ---- batch: 020 ----
mean loss: 31.48
 ---- batch: 030 ----
mean loss: 30.89
 ---- batch: 040 ----
mean loss: 30.17
train mean loss: 31.07
epoch train time: 0:00:00.201206
elapsed time: 0:00:58.660294
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:38:37.981742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.75
 ---- batch: 020 ----
mean loss: 30.90
 ---- batch: 030 ----
mean loss: 32.43
 ---- batch: 040 ----
mean loss: 30.63
train mean loss: 31.00
epoch train time: 0:00:00.196348
elapsed time: 0:00:58.856766
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:38:38.178201
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 28.78
 ---- batch: 020 ----
mean loss: 30.99
 ---- batch: 030 ----
mean loss: 31.12
 ---- batch: 040 ----
mean loss: 32.11
train mean loss: 31.07
epoch train time: 0:00:00.204186
elapsed time: 0:00:59.061093
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:38:38.382528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.37
 ---- batch: 020 ----
mean loss: 32.29
 ---- batch: 030 ----
mean loss: 29.73
 ---- batch: 040 ----
mean loss: 31.15
train mean loss: 30.97
epoch train time: 0:00:00.203981
elapsed time: 0:00:59.265190
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:38:38.586627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.36
 ---- batch: 020 ----
mean loss: 30.70
 ---- batch: 030 ----
mean loss: 30.36
 ---- batch: 040 ----
mean loss: 31.85
train mean loss: 30.89
epoch train time: 0:00:00.206460
elapsed time: 0:00:59.471769
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:38:38.793234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.76
 ---- batch: 020 ----
mean loss: 31.85
 ---- batch: 030 ----
mean loss: 30.06
 ---- batch: 040 ----
mean loss: 31.28
train mean loss: 30.99
epoch train time: 0:00:00.212836
elapsed time: 0:00:59.684752
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:38:39.006192
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.37
 ---- batch: 020 ----
mean loss: 31.53
 ---- batch: 030 ----
mean loss: 31.14
 ---- batch: 040 ----
mean loss: 30.35
train mean loss: 30.95
epoch train time: 0:00:00.202498
elapsed time: 0:00:59.887393
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:38:39.208832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.31
 ---- batch: 020 ----
mean loss: 31.05
 ---- batch: 030 ----
mean loss: 30.86
 ---- batch: 040 ----
mean loss: 31.17
train mean loss: 30.85
epoch train time: 0:00:00.204688
elapsed time: 0:01:00.092203
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:38:39.413642
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.56
 ---- batch: 020 ----
mean loss: 31.17
 ---- batch: 030 ----
mean loss: 31.35
 ---- batch: 040 ----
mean loss: 30.35
train mean loss: 30.81
epoch train time: 0:00:00.204865
elapsed time: 0:01:00.297186
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:38:39.618622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.84
 ---- batch: 020 ----
mean loss: 28.89
 ---- batch: 030 ----
mean loss: 32.28
 ---- batch: 040 ----
mean loss: 30.82
train mean loss: 30.84
epoch train time: 0:00:00.208786
elapsed time: 0:01:00.506088
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:38:39.827553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.19
 ---- batch: 020 ----
mean loss: 31.62
 ---- batch: 030 ----
mean loss: 30.44
 ---- batch: 040 ----
mean loss: 32.20
train mean loss: 30.77
epoch train time: 0:00:00.205625
elapsed time: 0:01:00.711877
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:38:40.033316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.00
 ---- batch: 020 ----
mean loss: 30.88
 ---- batch: 030 ----
mean loss: 31.13
 ---- batch: 040 ----
mean loss: 30.23
train mean loss: 30.85
epoch train time: 0:00:00.198469
elapsed time: 0:01:00.910460
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:38:40.231896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.72
 ---- batch: 020 ----
mean loss: 29.86
 ---- batch: 030 ----
mean loss: 30.64
 ---- batch: 040 ----
mean loss: 31.75
train mean loss: 30.93
epoch train time: 0:00:00.198671
elapsed time: 0:01:01.109265
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:38:40.430718
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.64
 ---- batch: 020 ----
mean loss: 31.72
 ---- batch: 030 ----
mean loss: 31.25
 ---- batch: 040 ----
mean loss: 28.58
train mean loss: 30.71
epoch train time: 0:00:00.196618
elapsed time: 0:01:01.306012
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:38:40.627449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.69
 ---- batch: 020 ----
mean loss: 32.94
 ---- batch: 030 ----
mean loss: 29.33
 ---- batch: 040 ----
mean loss: 29.65
train mean loss: 30.97
epoch train time: 0:00:00.197139
elapsed time: 0:01:01.503312
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:38:40.824770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.13
 ---- batch: 020 ----
mean loss: 30.75
 ---- batch: 030 ----
mean loss: 31.27
 ---- batch: 040 ----
mean loss: 31.29
train mean loss: 30.80
epoch train time: 0:00:00.196962
elapsed time: 0:01:01.700406
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:38:41.021842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.10
 ---- batch: 020 ----
mean loss: 29.76
 ---- batch: 030 ----
mean loss: 29.01
 ---- batch: 040 ----
mean loss: 32.15
train mean loss: 30.77
epoch train time: 0:00:00.195534
elapsed time: 0:01:01.896082
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:38:41.217532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.22
 ---- batch: 020 ----
mean loss: 30.80
 ---- batch: 030 ----
mean loss: 31.18
 ---- batch: 040 ----
mean loss: 30.42
train mean loss: 30.63
epoch train time: 0:00:00.194250
elapsed time: 0:01:02.090491
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:38:41.411928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.43
 ---- batch: 020 ----
mean loss: 28.78
 ---- batch: 030 ----
mean loss: 31.28
 ---- batch: 040 ----
mean loss: 30.47
train mean loss: 30.61
epoch train time: 0:00:00.198722
elapsed time: 0:01:02.289351
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:38:41.610790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.31
 ---- batch: 020 ----
mean loss: 31.34
 ---- batch: 030 ----
mean loss: 30.06
 ---- batch: 040 ----
mean loss: 31.05
train mean loss: 30.68
epoch train time: 0:00:00.198017
elapsed time: 0:01:02.487538
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:38:41.808977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.34
 ---- batch: 020 ----
mean loss: 31.01
 ---- batch: 030 ----
mean loss: 31.64
 ---- batch: 040 ----
mean loss: 30.87
train mean loss: 30.59
epoch train time: 0:00:00.194680
elapsed time: 0:01:02.682350
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:38:42.003786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.33
 ---- batch: 020 ----
mean loss: 31.97
 ---- batch: 030 ----
mean loss: 28.93
 ---- batch: 040 ----
mean loss: 30.87
train mean loss: 30.67
epoch train time: 0:00:00.199407
elapsed time: 0:01:02.881871
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:38:42.203325
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.48
 ---- batch: 020 ----
mean loss: 30.15
 ---- batch: 030 ----
mean loss: 31.19
 ---- batch: 040 ----
mean loss: 30.92
train mean loss: 30.61
epoch train time: 0:00:00.200845
elapsed time: 0:01:03.082867
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:38:42.404306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.44
 ---- batch: 020 ----
mean loss: 31.13
 ---- batch: 030 ----
mean loss: 30.40
 ---- batch: 040 ----
mean loss: 30.36
train mean loss: 30.59
epoch train time: 0:00:00.200040
elapsed time: 0:01:03.283051
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:38:42.604490
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.38
 ---- batch: 020 ----
mean loss: 28.65
 ---- batch: 030 ----
mean loss: 31.90
 ---- batch: 040 ----
mean loss: 30.67
train mean loss: 30.59
epoch train time: 0:00:00.205214
elapsed time: 0:01:03.488408
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:38:42.809847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.15
 ---- batch: 020 ----
mean loss: 30.94
 ---- batch: 030 ----
mean loss: 31.40
 ---- batch: 040 ----
mean loss: 29.98
train mean loss: 30.54
epoch train time: 0:00:00.204192
elapsed time: 0:01:03.692737
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:38:43.014177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.58
 ---- batch: 020 ----
mean loss: 31.17
 ---- batch: 030 ----
mean loss: 31.36
 ---- batch: 040 ----
mean loss: 31.07
train mean loss: 30.48
epoch train time: 0:00:00.203688
elapsed time: 0:01:03.896544
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:38:43.217982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.84
 ---- batch: 020 ----
mean loss: 29.56
 ---- batch: 030 ----
mean loss: 31.25
 ---- batch: 040 ----
mean loss: 32.36
train mean loss: 30.57
epoch train time: 0:00:00.202389
elapsed time: 0:01:04.099088
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:38:43.420529
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.27
 ---- batch: 020 ----
mean loss: 30.64
 ---- batch: 030 ----
mean loss: 30.13
 ---- batch: 040 ----
mean loss: 31.15
train mean loss: 30.59
epoch train time: 0:00:00.202125
elapsed time: 0:01:04.301336
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:38:43.622778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.37
 ---- batch: 020 ----
mean loss: 29.84
 ---- batch: 030 ----
mean loss: 32.05
 ---- batch: 040 ----
mean loss: 29.33
train mean loss: 30.52
epoch train time: 0:00:00.198800
elapsed time: 0:01:04.500257
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:38:43.821710
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.77
 ---- batch: 020 ----
mean loss: 30.85
 ---- batch: 030 ----
mean loss: 29.62
 ---- batch: 040 ----
mean loss: 29.89
train mean loss: 30.44
epoch train time: 0:00:00.194381
elapsed time: 0:01:04.697983
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_0/checkpoint.pth.tar
**** end time: 2019-09-20 23:38:44.019397 ****
