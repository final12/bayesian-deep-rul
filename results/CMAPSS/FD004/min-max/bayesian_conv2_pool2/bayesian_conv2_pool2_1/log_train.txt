Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 12422
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-26 20:37:27.038707 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 11, 11]           1,120
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
    BayesianConv2d-4            [-1, 14, 4, 11]             448
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
    BayesianLinear-8                    [-1, 1]             616
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,184
Trainable params: 2,184
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 20:37:27.050571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4685.63
 ---- batch: 020 ----
mean loss: 4516.17
 ---- batch: 030 ----
mean loss: 4333.26
 ---- batch: 040 ----
mean loss: 4135.21
 ---- batch: 050 ----
mean loss: 3954.44
 ---- batch: 060 ----
mean loss: 3723.38
 ---- batch: 070 ----
mean loss: 3560.91
 ---- batch: 080 ----
mean loss: 3359.59
 ---- batch: 090 ----
mean loss: 3161.59
 ---- batch: 100 ----
mean loss: 3000.69
 ---- batch: 110 ----
mean loss: 2824.90
train mean loss: 3722.07
epoch train time: 0:00:34.879446
elapsed time: 0:00:34.894393
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 20:38:01.933141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2576.07
 ---- batch: 020 ----
mean loss: 2392.29
 ---- batch: 030 ----
mean loss: 2248.41
 ---- batch: 040 ----
mean loss: 2088.01
 ---- batch: 050 ----
mean loss: 1971.15
 ---- batch: 060 ----
mean loss: 1835.83
 ---- batch: 070 ----
mean loss: 1709.63
 ---- batch: 080 ----
mean loss: 1615.32
 ---- batch: 090 ----
mean loss: 1528.84
 ---- batch: 100 ----
mean loss: 1429.74
 ---- batch: 110 ----
mean loss: 1345.43
train mean loss: 1870.73
epoch train time: 0:00:02.513041
elapsed time: 0:00:37.407651
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 20:38:04.446621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1264.33
 ---- batch: 020 ----
mean loss: 1221.36
 ---- batch: 030 ----
mean loss: 1166.62
 ---- batch: 040 ----
mean loss: 1127.85
 ---- batch: 050 ----
mean loss: 1068.07
 ---- batch: 060 ----
mean loss: 1029.17
 ---- batch: 070 ----
mean loss: 1032.18
 ---- batch: 080 ----
mean loss: 1002.36
 ---- batch: 090 ----
mean loss: 972.54
 ---- batch: 100 ----
mean loss: 957.70
 ---- batch: 110 ----
mean loss: 940.62
train mean loss: 1067.13
epoch train time: 0:00:02.513725
elapsed time: 0:00:39.921811
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 20:38:06.960812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.11
 ---- batch: 020 ----
mean loss: 920.71
 ---- batch: 030 ----
mean loss: 919.08
 ---- batch: 040 ----
mean loss: 894.50
 ---- batch: 050 ----
mean loss: 874.28
 ---- batch: 060 ----
mean loss: 880.27
 ---- batch: 070 ----
mean loss: 880.88
 ---- batch: 080 ----
mean loss: 851.12
 ---- batch: 090 ----
mean loss: 880.34
 ---- batch: 100 ----
mean loss: 881.82
 ---- batch: 110 ----
mean loss: 859.65
train mean loss: 887.49
epoch train time: 0:00:02.498006
elapsed time: 0:00:42.420286
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 20:38:09.459236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.81
 ---- batch: 020 ----
mean loss: 853.71
 ---- batch: 030 ----
mean loss: 864.89
 ---- batch: 040 ----
mean loss: 876.17
 ---- batch: 050 ----
mean loss: 859.36
 ---- batch: 060 ----
mean loss: 850.81
 ---- batch: 070 ----
mean loss: 853.68
 ---- batch: 080 ----
mean loss: 851.77
 ---- batch: 090 ----
mean loss: 851.64
 ---- batch: 100 ----
mean loss: 868.06
 ---- batch: 110 ----
mean loss: 867.40
train mean loss: 859.16
epoch train time: 0:00:02.501325
elapsed time: 0:00:44.922018
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 20:38:11.960961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.56
 ---- batch: 020 ----
mean loss: 853.83
 ---- batch: 030 ----
mean loss: 858.13
 ---- batch: 040 ----
mean loss: 835.34
 ---- batch: 050 ----
mean loss: 853.03
 ---- batch: 060 ----
mean loss: 865.70
 ---- batch: 070 ----
mean loss: 849.02
 ---- batch: 080 ----
mean loss: 865.05
 ---- batch: 090 ----
mean loss: 845.04
 ---- batch: 100 ----
mean loss: 859.42
 ---- batch: 110 ----
mean loss: 857.77
train mean loss: 853.80
epoch train time: 0:00:02.501585
elapsed time: 0:00:47.424007
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 20:38:14.462968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.73
 ---- batch: 020 ----
mean loss: 836.87
 ---- batch: 030 ----
mean loss: 838.34
 ---- batch: 040 ----
mean loss: 851.93
 ---- batch: 050 ----
mean loss: 824.55
 ---- batch: 060 ----
mean loss: 849.34
 ---- batch: 070 ----
mean loss: 844.46
 ---- batch: 080 ----
mean loss: 864.88
 ---- batch: 090 ----
mean loss: 861.62
 ---- batch: 100 ----
mean loss: 864.32
 ---- batch: 110 ----
mean loss: 850.14
train mean loss: 850.23
epoch train time: 0:00:02.514907
elapsed time: 0:00:49.939383
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 20:38:16.978311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.96
 ---- batch: 020 ----
mean loss: 830.52
 ---- batch: 030 ----
mean loss: 866.17
 ---- batch: 040 ----
mean loss: 851.24
 ---- batch: 050 ----
mean loss: 845.25
 ---- batch: 060 ----
mean loss: 856.30
 ---- batch: 070 ----
mean loss: 821.44
 ---- batch: 080 ----
mean loss: 847.63
 ---- batch: 090 ----
mean loss: 844.88
 ---- batch: 100 ----
mean loss: 857.64
 ---- batch: 110 ----
mean loss: 844.17
train mean loss: 845.44
epoch train time: 0:00:02.539209
elapsed time: 0:00:52.479020
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 20:38:19.517977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.51
 ---- batch: 020 ----
mean loss: 848.38
 ---- batch: 030 ----
mean loss: 829.43
 ---- batch: 040 ----
mean loss: 824.84
 ---- batch: 050 ----
mean loss: 843.84
 ---- batch: 060 ----
mean loss: 823.64
 ---- batch: 070 ----
mean loss: 850.73
 ---- batch: 080 ----
mean loss: 835.72
 ---- batch: 090 ----
mean loss: 831.83
 ---- batch: 100 ----
mean loss: 855.73
 ---- batch: 110 ----
mean loss: 849.99
train mean loss: 840.34
epoch train time: 0:00:02.516947
elapsed time: 0:00:54.996465
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 20:38:22.035396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.13
 ---- batch: 020 ----
mean loss: 850.55
 ---- batch: 030 ----
mean loss: 820.74
 ---- batch: 040 ----
mean loss: 849.64
 ---- batch: 050 ----
mean loss: 830.42
 ---- batch: 060 ----
mean loss: 840.07
 ---- batch: 070 ----
mean loss: 830.79
 ---- batch: 080 ----
mean loss: 865.19
 ---- batch: 090 ----
mean loss: 824.03
 ---- batch: 100 ----
mean loss: 821.90
 ---- batch: 110 ----
mean loss: 840.79
train mean loss: 836.04
epoch train time: 0:00:02.521950
elapsed time: 0:00:57.518847
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 20:38:24.557835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.77
 ---- batch: 020 ----
mean loss: 816.32
 ---- batch: 030 ----
mean loss: 832.26
 ---- batch: 040 ----
mean loss: 848.82
 ---- batch: 050 ----
mean loss: 826.87
 ---- batch: 060 ----
mean loss: 825.15
 ---- batch: 070 ----
mean loss: 832.73
 ---- batch: 080 ----
mean loss: 820.92
 ---- batch: 090 ----
mean loss: 841.43
 ---- batch: 100 ----
mean loss: 824.02
 ---- batch: 110 ----
mean loss: 827.02
train mean loss: 830.22
epoch train time: 0:00:02.505395
elapsed time: 0:01:00.024710
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 20:38:27.063570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.42
 ---- batch: 020 ----
mean loss: 803.53
 ---- batch: 030 ----
mean loss: 843.94
 ---- batch: 040 ----
mean loss: 835.43
 ---- batch: 050 ----
mean loss: 827.35
 ---- batch: 060 ----
mean loss: 818.31
 ---- batch: 070 ----
mean loss: 826.83
 ---- batch: 080 ----
mean loss: 814.52
 ---- batch: 090 ----
mean loss: 832.35
 ---- batch: 100 ----
mean loss: 820.95
 ---- batch: 110 ----
mean loss: 790.95
train mean loss: 825.22
epoch train time: 0:00:02.554583
elapsed time: 0:01:02.579629
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 20:38:29.618569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.34
 ---- batch: 020 ----
mean loss: 827.33
 ---- batch: 030 ----
mean loss: 829.89
 ---- batch: 040 ----
mean loss: 812.55
 ---- batch: 050 ----
mean loss: 817.49
 ---- batch: 060 ----
mean loss: 813.95
 ---- batch: 070 ----
mean loss: 830.55
 ---- batch: 080 ----
mean loss: 798.70
 ---- batch: 090 ----
mean loss: 819.26
 ---- batch: 100 ----
mean loss: 827.91
 ---- batch: 110 ----
mean loss: 802.10
train mean loss: 820.34
epoch train time: 0:00:02.595980
elapsed time: 0:01:05.176041
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 20:38:32.214986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 802.92
 ---- batch: 020 ----
mean loss: 820.89
 ---- batch: 030 ----
mean loss: 810.74
 ---- batch: 040 ----
mean loss: 801.44
 ---- batch: 050 ----
mean loss: 803.68
 ---- batch: 060 ----
mean loss: 822.84
 ---- batch: 070 ----
mean loss: 818.32
 ---- batch: 080 ----
mean loss: 821.08
 ---- batch: 090 ----
mean loss: 806.57
 ---- batch: 100 ----
mean loss: 822.37
 ---- batch: 110 ----
mean loss: 824.66
train mean loss: 814.99
epoch train time: 0:00:02.590784
elapsed time: 0:01:07.767268
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 20:38:34.806201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.15
 ---- batch: 020 ----
mean loss: 806.74
 ---- batch: 030 ----
mean loss: 821.85
 ---- batch: 040 ----
mean loss: 823.63
 ---- batch: 050 ----
mean loss: 813.99
 ---- batch: 060 ----
mean loss: 802.14
 ---- batch: 070 ----
mean loss: 808.90
 ---- batch: 080 ----
mean loss: 796.54
 ---- batch: 090 ----
mean loss: 807.22
 ---- batch: 100 ----
mean loss: 808.75
 ---- batch: 110 ----
mean loss: 824.77
train mean loss: 810.90
epoch train time: 0:00:02.534051
elapsed time: 0:01:10.301717
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 20:38:37.340653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.00
 ---- batch: 020 ----
mean loss: 812.95
 ---- batch: 030 ----
mean loss: 803.83
 ---- batch: 040 ----
mean loss: 792.92
 ---- batch: 050 ----
mean loss: 803.58
 ---- batch: 060 ----
mean loss: 812.48
 ---- batch: 070 ----
mean loss: 815.11
 ---- batch: 080 ----
mean loss: 795.11
 ---- batch: 090 ----
mean loss: 798.21
 ---- batch: 100 ----
mean loss: 812.92
 ---- batch: 110 ----
mean loss: 793.38
train mean loss: 805.34
epoch train time: 0:00:02.566261
elapsed time: 0:01:12.868379
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 20:38:39.907337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 797.91
 ---- batch: 020 ----
mean loss: 770.60
 ---- batch: 030 ----
mean loss: 797.99
 ---- batch: 040 ----
mean loss: 824.31
 ---- batch: 050 ----
mean loss: 819.85
 ---- batch: 060 ----
mean loss: 813.88
 ---- batch: 070 ----
mean loss: 809.44
 ---- batch: 080 ----
mean loss: 794.95
 ---- batch: 090 ----
mean loss: 784.77
 ---- batch: 100 ----
mean loss: 792.07
 ---- batch: 110 ----
mean loss: 791.29
train mean loss: 799.76
epoch train time: 0:00:02.526738
elapsed time: 0:01:15.395552
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 20:38:42.434513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 775.86
 ---- batch: 020 ----
mean loss: 798.38
 ---- batch: 030 ----
mean loss: 779.50
 ---- batch: 040 ----
mean loss: 800.75
 ---- batch: 050 ----
mean loss: 808.87
 ---- batch: 060 ----
mean loss: 777.50
 ---- batch: 070 ----
mean loss: 807.06
 ---- batch: 080 ----
mean loss: 790.11
 ---- batch: 090 ----
mean loss: 790.46
 ---- batch: 100 ----
mean loss: 807.50
 ---- batch: 110 ----
mean loss: 802.80
train mean loss: 793.96
epoch train time: 0:00:02.512468
elapsed time: 0:01:17.908446
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 20:38:44.947391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 789.84
 ---- batch: 020 ----
mean loss: 802.49
 ---- batch: 030 ----
mean loss: 791.84
 ---- batch: 040 ----
mean loss: 769.93
 ---- batch: 050 ----
mean loss: 779.71
 ---- batch: 060 ----
mean loss: 796.65
 ---- batch: 070 ----
mean loss: 787.24
 ---- batch: 080 ----
mean loss: 789.16
 ---- batch: 090 ----
mean loss: 794.35
 ---- batch: 100 ----
mean loss: 782.64
 ---- batch: 110 ----
mean loss: 811.10
train mean loss: 789.48
epoch train time: 0:00:02.547657
elapsed time: 0:01:20.456566
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 20:38:47.495515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.74
 ---- batch: 020 ----
mean loss: 793.75
 ---- batch: 030 ----
mean loss: 799.76
 ---- batch: 040 ----
mean loss: 779.55
 ---- batch: 050 ----
mean loss: 770.56
 ---- batch: 060 ----
mean loss: 792.36
 ---- batch: 070 ----
mean loss: 790.87
 ---- batch: 080 ----
mean loss: 789.06
 ---- batch: 090 ----
mean loss: 781.66
 ---- batch: 100 ----
mean loss: 783.39
 ---- batch: 110 ----
mean loss: 783.43
train mean loss: 785.56
epoch train time: 0:00:02.531828
elapsed time: 0:01:22.988817
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 20:38:50.027767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 749.47
 ---- batch: 020 ----
mean loss: 819.74
 ---- batch: 030 ----
mean loss: 783.16
 ---- batch: 040 ----
mean loss: 804.34
 ---- batch: 050 ----
mean loss: 784.45
 ---- batch: 060 ----
mean loss: 789.89
 ---- batch: 070 ----
mean loss: 776.80
 ---- batch: 080 ----
mean loss: 788.08
 ---- batch: 090 ----
mean loss: 767.89
 ---- batch: 100 ----
mean loss: 768.64
 ---- batch: 110 ----
mean loss: 761.12
train mean loss: 781.25
epoch train time: 0:00:02.513720
elapsed time: 0:01:25.502982
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 20:38:52.541842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 767.22
 ---- batch: 020 ----
mean loss: 794.91
 ---- batch: 030 ----
mean loss: 769.19
 ---- batch: 040 ----
mean loss: 793.92
 ---- batch: 050 ----
mean loss: 780.90
 ---- batch: 060 ----
mean loss: 773.25
 ---- batch: 070 ----
mean loss: 786.83
 ---- batch: 080 ----
mean loss: 774.90
 ---- batch: 090 ----
mean loss: 767.11
 ---- batch: 100 ----
mean loss: 767.12
 ---- batch: 110 ----
mean loss: 779.32
train mean loss: 777.21
epoch train time: 0:00:02.524496
elapsed time: 0:01:28.027825
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 20:38:55.066807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.94
 ---- batch: 020 ----
mean loss: 758.12
 ---- batch: 030 ----
mean loss: 754.63
 ---- batch: 040 ----
mean loss: 781.42
 ---- batch: 050 ----
mean loss: 790.86
 ---- batch: 060 ----
mean loss: 767.43
 ---- batch: 070 ----
mean loss: 791.06
 ---- batch: 080 ----
mean loss: 763.27
 ---- batch: 090 ----
mean loss: 766.45
 ---- batch: 100 ----
mean loss: 797.25
 ---- batch: 110 ----
mean loss: 766.76
train mean loss: 772.64
epoch train time: 0:00:02.485960
elapsed time: 0:01:30.514230
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 20:38:57.553190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 748.70
 ---- batch: 020 ----
mean loss: 768.76
 ---- batch: 030 ----
mean loss: 784.80
 ---- batch: 040 ----
mean loss: 761.57
 ---- batch: 050 ----
mean loss: 779.19
 ---- batch: 060 ----
mean loss: 763.97
 ---- batch: 070 ----
mean loss: 764.19
 ---- batch: 080 ----
mean loss: 776.00
 ---- batch: 090 ----
mean loss: 765.20
 ---- batch: 100 ----
mean loss: 776.39
 ---- batch: 110 ----
mean loss: 753.22
train mean loss: 767.73
epoch train time: 0:00:02.587198
elapsed time: 0:01:33.101882
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 20:39:00.140838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 761.49
 ---- batch: 020 ----
mean loss: 766.34
 ---- batch: 030 ----
mean loss: 752.53
 ---- batch: 040 ----
mean loss: 769.11
 ---- batch: 050 ----
mean loss: 763.54
 ---- batch: 060 ----
mean loss: 771.93
 ---- batch: 070 ----
mean loss: 746.88
 ---- batch: 080 ----
mean loss: 766.29
 ---- batch: 090 ----
mean loss: 771.28
 ---- batch: 100 ----
mean loss: 752.00
 ---- batch: 110 ----
mean loss: 769.46
train mean loss: 763.21
epoch train time: 0:00:02.552579
elapsed time: 0:01:35.654949
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 20:39:02.693895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.19
 ---- batch: 020 ----
mean loss: 759.19
 ---- batch: 030 ----
mean loss: 751.70
 ---- batch: 040 ----
mean loss: 759.95
 ---- batch: 050 ----
mean loss: 762.32
 ---- batch: 060 ----
mean loss: 766.37
 ---- batch: 070 ----
mean loss: 770.70
 ---- batch: 080 ----
mean loss: 743.40
 ---- batch: 090 ----
mean loss: 767.52
 ---- batch: 100 ----
mean loss: 750.80
 ---- batch: 110 ----
mean loss: 757.25
train mean loss: 758.53
epoch train time: 0:00:02.543476
elapsed time: 0:01:38.198878
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 20:39:05.237882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.06
 ---- batch: 020 ----
mean loss: 762.32
 ---- batch: 030 ----
mean loss: 765.01
 ---- batch: 040 ----
mean loss: 760.06
 ---- batch: 050 ----
mean loss: 753.48
 ---- batch: 060 ----
mean loss: 739.47
 ---- batch: 070 ----
mean loss: 737.32
 ---- batch: 080 ----
mean loss: 772.04
 ---- batch: 090 ----
mean loss: 768.04
 ---- batch: 100 ----
mean loss: 740.53
 ---- batch: 110 ----
mean loss: 745.90
train mean loss: 754.20
epoch train time: 0:00:02.540369
elapsed time: 0:01:40.739752
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 20:39:07.778704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 750.54
 ---- batch: 020 ----
mean loss: 738.64
 ---- batch: 030 ----
mean loss: 765.78
 ---- batch: 040 ----
mean loss: 768.46
 ---- batch: 050 ----
mean loss: 743.59
 ---- batch: 060 ----
mean loss: 746.49
 ---- batch: 070 ----
mean loss: 744.37
 ---- batch: 080 ----
mean loss: 745.15
 ---- batch: 090 ----
mean loss: 755.60
 ---- batch: 100 ----
mean loss: 739.01
 ---- batch: 110 ----
mean loss: 750.66
train mean loss: 748.92
epoch train time: 0:00:02.541441
elapsed time: 0:01:43.281615
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 20:39:10.320551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.36
 ---- batch: 020 ----
mean loss: 738.37
 ---- batch: 030 ----
mean loss: 755.02
 ---- batch: 040 ----
mean loss: 764.26
 ---- batch: 050 ----
mean loss: 764.84
 ---- batch: 060 ----
mean loss: 737.16
 ---- batch: 070 ----
mean loss: 743.16
 ---- batch: 080 ----
mean loss: 744.25
 ---- batch: 090 ----
mean loss: 728.61
 ---- batch: 100 ----
mean loss: 745.87
 ---- batch: 110 ----
mean loss: 740.79
train mean loss: 743.05
epoch train time: 0:00:02.558593
elapsed time: 0:01:45.840617
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 20:39:12.879590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 745.45
 ---- batch: 020 ----
mean loss: 720.21
 ---- batch: 030 ----
mean loss: 737.65
 ---- batch: 040 ----
mean loss: 734.04
 ---- batch: 050 ----
mean loss: 747.13
 ---- batch: 060 ----
mean loss: 735.61
 ---- batch: 070 ----
mean loss: 736.03
 ---- batch: 080 ----
mean loss: 749.16
 ---- batch: 090 ----
mean loss: 736.72
 ---- batch: 100 ----
mean loss: 742.05
 ---- batch: 110 ----
mean loss: 737.03
train mean loss: 737.65
epoch train time: 0:00:02.514259
elapsed time: 0:01:48.355428
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 20:39:15.394384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 734.43
 ---- batch: 020 ----
mean loss: 728.66
 ---- batch: 030 ----
mean loss: 738.89
 ---- batch: 040 ----
mean loss: 737.62
 ---- batch: 050 ----
mean loss: 734.22
 ---- batch: 060 ----
mean loss: 735.95
 ---- batch: 070 ----
mean loss: 698.08
 ---- batch: 080 ----
mean loss: 741.96
 ---- batch: 090 ----
mean loss: 730.33
 ---- batch: 100 ----
mean loss: 733.43
 ---- batch: 110 ----
mean loss: 738.58
train mean loss: 731.90
epoch train time: 0:00:02.525225
elapsed time: 0:01:50.881116
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 20:39:17.920052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.07
 ---- batch: 020 ----
mean loss: 710.67
 ---- batch: 030 ----
mean loss: 741.37
 ---- batch: 040 ----
mean loss: 743.21
 ---- batch: 050 ----
mean loss: 709.58
 ---- batch: 060 ----
mean loss: 729.52
 ---- batch: 070 ----
mean loss: 727.51
 ---- batch: 080 ----
mean loss: 727.37
 ---- batch: 090 ----
mean loss: 720.33
 ---- batch: 100 ----
mean loss: 723.75
 ---- batch: 110 ----
mean loss: 713.80
train mean loss: 725.48
epoch train time: 0:00:02.491792
elapsed time: 0:01:53.373340
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 20:39:20.412283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 710.21
 ---- batch: 020 ----
mean loss: 712.05
 ---- batch: 030 ----
mean loss: 732.30
 ---- batch: 040 ----
mean loss: 735.00
 ---- batch: 050 ----
mean loss: 722.74
 ---- batch: 060 ----
mean loss: 728.88
 ---- batch: 070 ----
mean loss: 693.79
 ---- batch: 080 ----
mean loss: 727.98
 ---- batch: 090 ----
mean loss: 717.29
 ---- batch: 100 ----
mean loss: 726.54
 ---- batch: 110 ----
mean loss: 706.53
train mean loss: 719.24
epoch train time: 0:00:02.536341
elapsed time: 0:01:55.910094
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 20:39:22.949033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 709.33
 ---- batch: 020 ----
mean loss: 718.37
 ---- batch: 030 ----
mean loss: 709.10
 ---- batch: 040 ----
mean loss: 717.03
 ---- batch: 050 ----
mean loss: 698.98
 ---- batch: 060 ----
mean loss: 707.16
 ---- batch: 070 ----
mean loss: 720.32
 ---- batch: 080 ----
mean loss: 720.03
 ---- batch: 090 ----
mean loss: 720.47
 ---- batch: 100 ----
mean loss: 718.56
 ---- batch: 110 ----
mean loss: 713.62
train mean loss: 713.39
epoch train time: 0:00:02.515001
elapsed time: 0:01:58.425518
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 20:39:25.464487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 703.95
 ---- batch: 020 ----
mean loss: 681.56
 ---- batch: 030 ----
mean loss: 703.26
 ---- batch: 040 ----
mean loss: 718.22
 ---- batch: 050 ----
mean loss: 707.48
 ---- batch: 060 ----
mean loss: 717.85
 ---- batch: 070 ----
mean loss: 691.00
 ---- batch: 080 ----
mean loss: 716.02
 ---- batch: 090 ----
mean loss: 715.44
 ---- batch: 100 ----
mean loss: 714.66
 ---- batch: 110 ----
mean loss: 711.61
train mean loss: 706.79
epoch train time: 0:00:02.502590
elapsed time: 0:02:00.928543
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 20:39:27.967329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 729.47
 ---- batch: 020 ----
mean loss: 712.84
 ---- batch: 030 ----
mean loss: 697.67
 ---- batch: 040 ----
mean loss: 697.96
 ---- batch: 050 ----
mean loss: 698.54
 ---- batch: 060 ----
mean loss: 693.12
 ---- batch: 070 ----
mean loss: 696.98
 ---- batch: 080 ----
mean loss: 698.84
 ---- batch: 090 ----
mean loss: 686.05
 ---- batch: 100 ----
mean loss: 696.87
 ---- batch: 110 ----
mean loss: 691.04
train mean loss: 699.94
epoch train time: 0:00:02.518346
elapsed time: 0:02:03.447152
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 20:39:30.486096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.21
 ---- batch: 020 ----
mean loss: 691.76
 ---- batch: 030 ----
mean loss: 700.72
 ---- batch: 040 ----
mean loss: 695.04
 ---- batch: 050 ----
mean loss: 685.75
 ---- batch: 060 ----
mean loss: 699.16
 ---- batch: 070 ----
mean loss: 683.81
 ---- batch: 080 ----
mean loss: 683.63
 ---- batch: 090 ----
mean loss: 702.86
 ---- batch: 100 ----
mean loss: 694.82
 ---- batch: 110 ----
mean loss: 674.55
train mean loss: 692.79
epoch train time: 0:00:02.563326
elapsed time: 0:02:06.010932
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 20:39:33.049884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.75
 ---- batch: 020 ----
mean loss: 696.49
 ---- batch: 030 ----
mean loss: 690.73
 ---- batch: 040 ----
mean loss: 684.73
 ---- batch: 050 ----
mean loss: 705.55
 ---- batch: 060 ----
mean loss: 670.09
 ---- batch: 070 ----
mean loss: 679.19
 ---- batch: 080 ----
mean loss: 677.76
 ---- batch: 090 ----
mean loss: 675.95
 ---- batch: 100 ----
mean loss: 679.81
 ---- batch: 110 ----
mean loss: 697.93
train mean loss: 685.89
epoch train time: 0:00:02.507673
elapsed time: 0:02:08.519048
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 20:39:35.558015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.48
 ---- batch: 020 ----
mean loss: 684.44
 ---- batch: 030 ----
mean loss: 685.68
 ---- batch: 040 ----
mean loss: 684.99
 ---- batch: 050 ----
mean loss: 659.74
 ---- batch: 060 ----
mean loss: 674.64
 ---- batch: 070 ----
mean loss: 680.00
 ---- batch: 080 ----
mean loss: 686.42
 ---- batch: 090 ----
mean loss: 671.25
 ---- batch: 100 ----
mean loss: 670.73
 ---- batch: 110 ----
mean loss: 680.77
train mean loss: 679.40
epoch train time: 0:00:02.530063
elapsed time: 0:02:11.049565
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 20:39:38.088479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.04
 ---- batch: 020 ----
mean loss: 676.93
 ---- batch: 030 ----
mean loss: 663.02
 ---- batch: 040 ----
mean loss: 673.32
 ---- batch: 050 ----
mean loss: 673.60
 ---- batch: 060 ----
mean loss: 667.11
 ---- batch: 070 ----
mean loss: 670.46
 ---- batch: 080 ----
mean loss: 680.39
 ---- batch: 090 ----
mean loss: 668.62
 ---- batch: 100 ----
mean loss: 677.29
 ---- batch: 110 ----
mean loss: 676.77
train mean loss: 672.21
epoch train time: 0:00:02.531628
elapsed time: 0:02:13.581597
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 20:39:40.620596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.86
 ---- batch: 020 ----
mean loss: 680.34
 ---- batch: 030 ----
mean loss: 656.80
 ---- batch: 040 ----
mean loss: 674.61
 ---- batch: 050 ----
mean loss: 671.00
 ---- batch: 060 ----
mean loss: 664.72
 ---- batch: 070 ----
mean loss: 666.62
 ---- batch: 080 ----
mean loss: 671.17
 ---- batch: 090 ----
mean loss: 649.66
 ---- batch: 100 ----
mean loss: 658.27
 ---- batch: 110 ----
mean loss: 665.84
train mean loss: 665.25
epoch train time: 0:00:02.512394
elapsed time: 0:02:16.094466
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 20:39:43.133444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.23
 ---- batch: 020 ----
mean loss: 649.96
 ---- batch: 030 ----
mean loss: 660.59
 ---- batch: 040 ----
mean loss: 659.96
 ---- batch: 050 ----
mean loss: 650.04
 ---- batch: 060 ----
mean loss: 648.01
 ---- batch: 070 ----
mean loss: 672.90
 ---- batch: 080 ----
mean loss: 673.66
 ---- batch: 090 ----
mean loss: 661.59
 ---- batch: 100 ----
mean loss: 656.40
 ---- batch: 110 ----
mean loss: 644.22
train mean loss: 657.96
epoch train time: 0:00:02.519100
elapsed time: 0:02:18.614063
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 20:39:45.653043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 655.24
 ---- batch: 020 ----
mean loss: 648.20
 ---- batch: 030 ----
mean loss: 655.55
 ---- batch: 040 ----
mean loss: 657.09
 ---- batch: 050 ----
mean loss: 657.41
 ---- batch: 060 ----
mean loss: 649.22
 ---- batch: 070 ----
mean loss: 657.04
 ---- batch: 080 ----
mean loss: 652.08
 ---- batch: 090 ----
mean loss: 647.94
 ---- batch: 100 ----
mean loss: 634.07
 ---- batch: 110 ----
mean loss: 653.43
train mean loss: 651.58
epoch train time: 0:00:02.543183
elapsed time: 0:02:21.157728
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 20:39:48.196684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.03
 ---- batch: 020 ----
mean loss: 639.54
 ---- batch: 030 ----
mean loss: 655.28
 ---- batch: 040 ----
mean loss: 645.03
 ---- batch: 050 ----
mean loss: 646.06
 ---- batch: 060 ----
mean loss: 641.97
 ---- batch: 070 ----
mean loss: 629.19
 ---- batch: 080 ----
mean loss: 654.09
 ---- batch: 090 ----
mean loss: 640.24
 ---- batch: 100 ----
mean loss: 648.73
 ---- batch: 110 ----
mean loss: 640.05
train mean loss: 644.18
epoch train time: 0:00:02.509916
elapsed time: 0:02:23.668078
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 20:39:50.707054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 634.27
 ---- batch: 020 ----
mean loss: 639.79
 ---- batch: 030 ----
mean loss: 635.10
 ---- batch: 040 ----
mean loss: 642.08
 ---- batch: 050 ----
mean loss: 634.60
 ---- batch: 060 ----
mean loss: 664.93
 ---- batch: 070 ----
mean loss: 647.72
 ---- batch: 080 ----
mean loss: 629.69
 ---- batch: 090 ----
mean loss: 630.36
 ---- batch: 100 ----
mean loss: 618.78
 ---- batch: 110 ----
mean loss: 637.48
train mean loss: 637.59
epoch train time: 0:00:02.504413
elapsed time: 0:02:26.172945
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 20:39:53.211878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 617.76
 ---- batch: 020 ----
mean loss: 651.82
 ---- batch: 030 ----
mean loss: 642.63
 ---- batch: 040 ----
mean loss: 640.29
 ---- batch: 050 ----
mean loss: 625.23
 ---- batch: 060 ----
mean loss: 625.04
 ---- batch: 070 ----
mean loss: 623.68
 ---- batch: 080 ----
mean loss: 629.12
 ---- batch: 090 ----
mean loss: 619.15
 ---- batch: 100 ----
mean loss: 635.67
 ---- batch: 110 ----
mean loss: 636.49
train mean loss: 631.13
epoch train time: 0:00:02.505855
elapsed time: 0:02:28.679261
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 20:39:55.718282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 623.78
 ---- batch: 020 ----
mean loss: 625.00
 ---- batch: 030 ----
mean loss: 630.13
 ---- batch: 040 ----
mean loss: 625.57
 ---- batch: 050 ----
mean loss: 622.49
 ---- batch: 060 ----
mean loss: 609.41
 ---- batch: 070 ----
mean loss: 641.91
 ---- batch: 080 ----
mean loss: 617.57
 ---- batch: 090 ----
mean loss: 631.80
 ---- batch: 100 ----
mean loss: 618.27
 ---- batch: 110 ----
mean loss: 631.49
train mean loss: 625.18
epoch train time: 0:00:02.514809
elapsed time: 0:02:31.194579
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 20:39:58.233540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 621.62
 ---- batch: 020 ----
mean loss: 628.11
 ---- batch: 030 ----
mean loss: 614.80
 ---- batch: 040 ----
mean loss: 605.63
 ---- batch: 050 ----
mean loss: 624.31
 ---- batch: 060 ----
mean loss: 615.34
 ---- batch: 070 ----
mean loss: 637.04
 ---- batch: 080 ----
mean loss: 616.99
 ---- batch: 090 ----
mean loss: 626.80
 ---- batch: 100 ----
mean loss: 602.61
 ---- batch: 110 ----
mean loss: 618.14
train mean loss: 619.01
epoch train time: 0:00:02.499181
elapsed time: 0:02:33.694205
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 20:40:00.733163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 617.51
 ---- batch: 020 ----
mean loss: 611.70
 ---- batch: 030 ----
mean loss: 621.79
 ---- batch: 040 ----
mean loss: 605.29
 ---- batch: 050 ----
mean loss: 604.61
 ---- batch: 060 ----
mean loss: 619.39
 ---- batch: 070 ----
mean loss: 623.84
 ---- batch: 080 ----
mean loss: 614.97
 ---- batch: 090 ----
mean loss: 604.56
 ---- batch: 100 ----
mean loss: 612.21
 ---- batch: 110 ----
mean loss: 603.00
train mean loss: 612.87
epoch train time: 0:00:02.516331
elapsed time: 0:02:36.210996
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 20:40:03.250003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 611.65
 ---- batch: 020 ----
mean loss: 612.09
 ---- batch: 030 ----
mean loss: 609.45
 ---- batch: 040 ----
mean loss: 609.19
 ---- batch: 050 ----
mean loss: 594.52
 ---- batch: 060 ----
mean loss: 608.27
 ---- batch: 070 ----
mean loss: 598.67
 ---- batch: 080 ----
mean loss: 610.01
 ---- batch: 090 ----
mean loss: 611.84
 ---- batch: 100 ----
mean loss: 597.53
 ---- batch: 110 ----
mean loss: 597.17
train mean loss: 606.14
epoch train time: 0:00:02.512195
elapsed time: 0:02:38.723678
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 20:40:05.762615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.89
 ---- batch: 020 ----
mean loss: 609.37
 ---- batch: 030 ----
mean loss: 600.36
 ---- batch: 040 ----
mean loss: 607.80
 ---- batch: 050 ----
mean loss: 605.44
 ---- batch: 060 ----
mean loss: 606.16
 ---- batch: 070 ----
mean loss: 601.63
 ---- batch: 080 ----
mean loss: 600.13
 ---- batch: 090 ----
mean loss: 593.65
 ---- batch: 100 ----
mean loss: 587.26
 ---- batch: 110 ----
mean loss: 592.18
train mean loss: 600.79
epoch train time: 0:00:02.527084
elapsed time: 0:02:41.251183
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 20:40:08.290131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 597.00
 ---- batch: 020 ----
mean loss: 600.25
 ---- batch: 030 ----
mean loss: 607.20
 ---- batch: 040 ----
mean loss: 592.01
 ---- batch: 050 ----
mean loss: 588.46
 ---- batch: 060 ----
mean loss: 600.72
 ---- batch: 070 ----
mean loss: 594.25
 ---- batch: 080 ----
mean loss: 578.98
 ---- batch: 090 ----
mean loss: 581.71
 ---- batch: 100 ----
mean loss: 604.04
 ---- batch: 110 ----
mean loss: 595.99
train mean loss: 594.73
epoch train time: 0:00:02.540410
elapsed time: 0:02:43.792051
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 20:40:10.831013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.10
 ---- batch: 020 ----
mean loss: 594.94
 ---- batch: 030 ----
mean loss: 583.40
 ---- batch: 040 ----
mean loss: 579.22
 ---- batch: 050 ----
mean loss: 577.40
 ---- batch: 060 ----
mean loss: 591.26
 ---- batch: 070 ----
mean loss: 590.94
 ---- batch: 080 ----
mean loss: 598.62
 ---- batch: 090 ----
mean loss: 579.07
 ---- batch: 100 ----
mean loss: 584.13
 ---- batch: 110 ----
mean loss: 604.09
train mean loss: 588.71
epoch train time: 0:00:02.558083
elapsed time: 0:02:46.350557
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 20:40:13.389569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 590.99
 ---- batch: 020 ----
mean loss: 586.09
 ---- batch: 030 ----
mean loss: 589.55
 ---- batch: 040 ----
mean loss: 575.44
 ---- batch: 050 ----
mean loss: 566.80
 ---- batch: 060 ----
mean loss: 590.49
 ---- batch: 070 ----
mean loss: 594.27
 ---- batch: 080 ----
mean loss: 590.15
 ---- batch: 090 ----
mean loss: 570.10
 ---- batch: 100 ----
mean loss: 582.67
 ---- batch: 110 ----
mean loss: 579.08
train mean loss: 582.74
epoch train time: 0:00:02.508018
elapsed time: 0:02:48.859191
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 20:40:15.898213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 584.76
 ---- batch: 020 ----
mean loss: 580.50
 ---- batch: 030 ----
mean loss: 581.77
 ---- batch: 040 ----
mean loss: 586.33
 ---- batch: 050 ----
mean loss: 584.83
 ---- batch: 060 ----
mean loss: 556.57
 ---- batch: 070 ----
mean loss: 574.30
 ---- batch: 080 ----
mean loss: 561.83
 ---- batch: 090 ----
mean loss: 569.49
 ---- batch: 100 ----
mean loss: 580.44
 ---- batch: 110 ----
mean loss: 575.33
train mean loss: 576.04
epoch train time: 0:00:02.498614
elapsed time: 0:02:51.358296
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 20:40:18.397231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.97
 ---- batch: 020 ----
mean loss: 576.00
 ---- batch: 030 ----
mean loss: 586.37
 ---- batch: 040 ----
mean loss: 562.85
 ---- batch: 050 ----
mean loss: 572.56
 ---- batch: 060 ----
mean loss: 562.04
 ---- batch: 070 ----
mean loss: 554.87
 ---- batch: 080 ----
mean loss: 558.50
 ---- batch: 090 ----
mean loss: 569.85
 ---- batch: 100 ----
mean loss: 563.19
 ---- batch: 110 ----
mean loss: 571.27
train mean loss: 569.60
epoch train time: 0:00:02.532568
elapsed time: 0:02:53.891270
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 20:40:20.930218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 572.45
 ---- batch: 020 ----
mean loss: 565.11
 ---- batch: 030 ----
mean loss: 563.32
 ---- batch: 040 ----
mean loss: 557.48
 ---- batch: 050 ----
mean loss: 570.11
 ---- batch: 060 ----
mean loss: 574.89
 ---- batch: 070 ----
mean loss: 548.67
 ---- batch: 080 ----
mean loss: 554.20
 ---- batch: 090 ----
mean loss: 559.24
 ---- batch: 100 ----
mean loss: 559.83
 ---- batch: 110 ----
mean loss: 560.33
train mean loss: 562.80
epoch train time: 0:00:02.501455
elapsed time: 0:02:56.393181
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 20:40:23.431969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.25
 ---- batch: 020 ----
mean loss: 569.13
 ---- batch: 030 ----
mean loss: 545.02
 ---- batch: 040 ----
mean loss: 554.25
 ---- batch: 050 ----
mean loss: 548.93
 ---- batch: 060 ----
mean loss: 548.37
 ---- batch: 070 ----
mean loss: 552.37
 ---- batch: 080 ----
mean loss: 534.81
 ---- batch: 090 ----
mean loss: 569.95
 ---- batch: 100 ----
mean loss: 574.71
 ---- batch: 110 ----
mean loss: 559.55
train mean loss: 556.51
epoch train time: 0:00:02.492547
elapsed time: 0:02:58.886001
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 20:40:25.924941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.27
 ---- batch: 020 ----
mean loss: 549.44
 ---- batch: 030 ----
mean loss: 554.49
 ---- batch: 040 ----
mean loss: 554.98
 ---- batch: 050 ----
mean loss: 553.37
 ---- batch: 060 ----
mean loss: 527.62
 ---- batch: 070 ----
mean loss: 549.68
 ---- batch: 080 ----
mean loss: 549.04
 ---- batch: 090 ----
mean loss: 545.20
 ---- batch: 100 ----
mean loss: 545.21
 ---- batch: 110 ----
mean loss: 553.93
train mean loss: 548.33
epoch train time: 0:00:02.531439
elapsed time: 0:03:01.417842
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 20:40:28.456770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 544.17
 ---- batch: 020 ----
mean loss: 544.29
 ---- batch: 030 ----
mean loss: 549.85
 ---- batch: 040 ----
mean loss: 537.23
 ---- batch: 050 ----
mean loss: 534.88
 ---- batch: 060 ----
mean loss: 541.22
 ---- batch: 070 ----
mean loss: 529.38
 ---- batch: 080 ----
mean loss: 529.35
 ---- batch: 090 ----
mean loss: 545.38
 ---- batch: 100 ----
mean loss: 531.20
 ---- batch: 110 ----
mean loss: 525.74
train mean loss: 538.08
epoch train time: 0:00:02.531548
elapsed time: 0:03:03.949815
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 20:40:30.988772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.79
 ---- batch: 020 ----
mean loss: 525.83
 ---- batch: 030 ----
mean loss: 533.15
 ---- batch: 040 ----
mean loss: 529.27
 ---- batch: 050 ----
mean loss: 533.51
 ---- batch: 060 ----
mean loss: 502.21
 ---- batch: 070 ----
mean loss: 514.96
 ---- batch: 080 ----
mean loss: 517.50
 ---- batch: 090 ----
mean loss: 502.50
 ---- batch: 100 ----
mean loss: 515.49
 ---- batch: 110 ----
mean loss: 508.38
train mean loss: 519.17
epoch train time: 0:00:02.513213
elapsed time: 0:03:06.463450
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 20:40:33.502406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.01
 ---- batch: 020 ----
mean loss: 505.60
 ---- batch: 030 ----
mean loss: 510.19
 ---- batch: 040 ----
mean loss: 497.56
 ---- batch: 050 ----
mean loss: 499.46
 ---- batch: 060 ----
mean loss: 497.52
 ---- batch: 070 ----
mean loss: 505.44
 ---- batch: 080 ----
mean loss: 494.04
 ---- batch: 090 ----
mean loss: 502.09
 ---- batch: 100 ----
mean loss: 496.52
 ---- batch: 110 ----
mean loss: 492.19
train mean loss: 498.85
epoch train time: 0:00:02.504688
elapsed time: 0:03:08.968565
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 20:40:36.007575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.39
 ---- batch: 020 ----
mean loss: 478.37
 ---- batch: 030 ----
mean loss: 478.42
 ---- batch: 040 ----
mean loss: 497.60
 ---- batch: 050 ----
mean loss: 492.17
 ---- batch: 060 ----
mean loss: 475.64
 ---- batch: 070 ----
mean loss: 459.82
 ---- batch: 080 ----
mean loss: 476.36
 ---- batch: 090 ----
mean loss: 477.70
 ---- batch: 100 ----
mean loss: 474.38
 ---- batch: 110 ----
mean loss: 481.20
train mean loss: 480.74
epoch train time: 0:00:02.529165
elapsed time: 0:03:11.498233
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 20:40:38.537215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.07
 ---- batch: 020 ----
mean loss: 453.47
 ---- batch: 030 ----
mean loss: 460.60
 ---- batch: 040 ----
mean loss: 470.44
 ---- batch: 050 ----
mean loss: 452.00
 ---- batch: 060 ----
mean loss: 464.46
 ---- batch: 070 ----
mean loss: 470.58
 ---- batch: 080 ----
mean loss: 459.02
 ---- batch: 090 ----
mean loss: 450.35
 ---- batch: 100 ----
mean loss: 459.53
 ---- batch: 110 ----
mean loss: 476.68
train mean loss: 461.95
epoch train time: 0:00:02.537826
elapsed time: 0:03:14.036561
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 20:40:41.075552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.36
 ---- batch: 020 ----
mean loss: 460.20
 ---- batch: 030 ----
mean loss: 445.49
 ---- batch: 040 ----
mean loss: 428.84
 ---- batch: 050 ----
mean loss: 439.23
 ---- batch: 060 ----
mean loss: 443.25
 ---- batch: 070 ----
mean loss: 437.91
 ---- batch: 080 ----
mean loss: 441.73
 ---- batch: 090 ----
mean loss: 461.77
 ---- batch: 100 ----
mean loss: 438.90
 ---- batch: 110 ----
mean loss: 436.58
train mean loss: 444.18
epoch train time: 0:00:02.590854
elapsed time: 0:03:16.627909
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 20:40:43.666900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.19
 ---- batch: 020 ----
mean loss: 426.26
 ---- batch: 030 ----
mean loss: 432.17
 ---- batch: 040 ----
mean loss: 434.46
 ---- batch: 050 ----
mean loss: 440.04
 ---- batch: 060 ----
mean loss: 425.51
 ---- batch: 070 ----
mean loss: 419.97
 ---- batch: 080 ----
mean loss: 427.64
 ---- batch: 090 ----
mean loss: 414.14
 ---- batch: 100 ----
mean loss: 410.53
 ---- batch: 110 ----
mean loss: 420.61
train mean loss: 427.38
epoch train time: 0:00:02.606609
elapsed time: 0:03:19.235060
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 20:40:46.274024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.56
 ---- batch: 020 ----
mean loss: 423.39
 ---- batch: 030 ----
mean loss: 413.89
 ---- batch: 040 ----
mean loss: 414.35
 ---- batch: 050 ----
mean loss: 410.90
 ---- batch: 060 ----
mean loss: 418.03
 ---- batch: 070 ----
mean loss: 416.30
 ---- batch: 080 ----
mean loss: 405.85
 ---- batch: 090 ----
mean loss: 404.32
 ---- batch: 100 ----
mean loss: 399.44
 ---- batch: 110 ----
mean loss: 391.00
train mean loss: 409.80
epoch train time: 0:00:02.622452
elapsed time: 0:03:21.857962
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 20:40:48.896903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.63
 ---- batch: 020 ----
mean loss: 400.37
 ---- batch: 030 ----
mean loss: 388.58
 ---- batch: 040 ----
mean loss: 399.51
 ---- batch: 050 ----
mean loss: 387.73
 ---- batch: 060 ----
mean loss: 385.30
 ---- batch: 070 ----
mean loss: 381.18
 ---- batch: 080 ----
mean loss: 377.86
 ---- batch: 090 ----
mean loss: 386.68
 ---- batch: 100 ----
mean loss: 384.95
 ---- batch: 110 ----
mean loss: 388.81
train mean loss: 389.37
epoch train time: 0:00:02.590220
elapsed time: 0:03:24.448650
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 20:40:51.487587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.31
 ---- batch: 020 ----
mean loss: 378.30
 ---- batch: 030 ----
mean loss: 363.49
 ---- batch: 040 ----
mean loss: 372.77
 ---- batch: 050 ----
mean loss: 372.20
 ---- batch: 060 ----
mean loss: 384.02
 ---- batch: 070 ----
mean loss: 373.79
 ---- batch: 080 ----
mean loss: 366.31
 ---- batch: 090 ----
mean loss: 359.04
 ---- batch: 100 ----
mean loss: 352.93
 ---- batch: 110 ----
mean loss: 374.90
train mean loss: 369.58
epoch train time: 0:00:02.561977
elapsed time: 0:03:27.011091
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 20:40:54.050064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.28
 ---- batch: 020 ----
mean loss: 347.72
 ---- batch: 030 ----
mean loss: 359.29
 ---- batch: 040 ----
mean loss: 344.99
 ---- batch: 050 ----
mean loss: 348.79
 ---- batch: 060 ----
mean loss: 363.15
 ---- batch: 070 ----
mean loss: 360.89
 ---- batch: 080 ----
mean loss: 342.87
 ---- batch: 090 ----
mean loss: 352.14
 ---- batch: 100 ----
mean loss: 340.79
 ---- batch: 110 ----
mean loss: 352.75
train mean loss: 352.41
epoch train time: 0:00:02.529356
elapsed time: 0:03:29.540889
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 20:40:56.579839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.52
 ---- batch: 020 ----
mean loss: 343.68
 ---- batch: 030 ----
mean loss: 346.07
 ---- batch: 040 ----
mean loss: 342.42
 ---- batch: 050 ----
mean loss: 336.04
 ---- batch: 060 ----
mean loss: 325.34
 ---- batch: 070 ----
mean loss: 338.27
 ---- batch: 080 ----
mean loss: 336.80
 ---- batch: 090 ----
mean loss: 335.28
 ---- batch: 100 ----
mean loss: 329.22
 ---- batch: 110 ----
mean loss: 330.24
train mean loss: 336.96
epoch train time: 0:00:02.547031
elapsed time: 0:03:32.088348
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 20:40:59.127310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.05
 ---- batch: 020 ----
mean loss: 325.49
 ---- batch: 030 ----
mean loss: 327.86
 ---- batch: 040 ----
mean loss: 322.52
 ---- batch: 050 ----
mean loss: 304.49
 ---- batch: 060 ----
mean loss: 317.73
 ---- batch: 070 ----
mean loss: 328.69
 ---- batch: 080 ----
mean loss: 336.50
 ---- batch: 090 ----
mean loss: 322.05
 ---- batch: 100 ----
mean loss: 326.28
 ---- batch: 110 ----
mean loss: 317.72
train mean loss: 323.80
epoch train time: 0:00:02.579040
elapsed time: 0:03:34.667836
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 20:41:01.706773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.08
 ---- batch: 020 ----
mean loss: 311.14
 ---- batch: 030 ----
mean loss: 315.18
 ---- batch: 040 ----
mean loss: 323.39
 ---- batch: 050 ----
mean loss: 316.93
 ---- batch: 060 ----
mean loss: 310.57
 ---- batch: 070 ----
mean loss: 308.62
 ---- batch: 080 ----
mean loss: 314.73
 ---- batch: 090 ----
mean loss: 312.84
 ---- batch: 100 ----
mean loss: 312.03
 ---- batch: 110 ----
mean loss: 307.52
train mean loss: 312.34
epoch train time: 0:00:02.550527
elapsed time: 0:03:37.218805
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 20:41:04.257798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.20
 ---- batch: 020 ----
mean loss: 310.62
 ---- batch: 030 ----
mean loss: 295.95
 ---- batch: 040 ----
mean loss: 300.43
 ---- batch: 050 ----
mean loss: 298.53
 ---- batch: 060 ----
mean loss: 307.00
 ---- batch: 070 ----
mean loss: 304.81
 ---- batch: 080 ----
mean loss: 307.55
 ---- batch: 090 ----
mean loss: 299.54
 ---- batch: 100 ----
mean loss: 298.42
 ---- batch: 110 ----
mean loss: 299.48
train mean loss: 302.86
epoch train time: 0:00:02.547972
elapsed time: 0:03:39.767259
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 20:41:06.806194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.24
 ---- batch: 020 ----
mean loss: 294.63
 ---- batch: 030 ----
mean loss: 288.06
 ---- batch: 040 ----
mean loss: 291.01
 ---- batch: 050 ----
mean loss: 288.63
 ---- batch: 060 ----
mean loss: 304.61
 ---- batch: 070 ----
mean loss: 299.26
 ---- batch: 080 ----
mean loss: 295.84
 ---- batch: 090 ----
mean loss: 289.73
 ---- batch: 100 ----
mean loss: 295.59
 ---- batch: 110 ----
mean loss: 288.41
train mean loss: 294.70
epoch train time: 0:00:02.562488
elapsed time: 0:03:42.330157
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 20:41:09.369125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.24
 ---- batch: 020 ----
mean loss: 290.29
 ---- batch: 030 ----
mean loss: 299.33
 ---- batch: 040 ----
mean loss: 292.43
 ---- batch: 050 ----
mean loss: 294.09
 ---- batch: 060 ----
mean loss: 284.11
 ---- batch: 070 ----
mean loss: 280.49
 ---- batch: 080 ----
mean loss: 279.88
 ---- batch: 090 ----
mean loss: 286.31
 ---- batch: 100 ----
mean loss: 281.90
 ---- batch: 110 ----
mean loss: 282.94
train mean loss: 287.35
epoch train time: 0:00:02.530966
elapsed time: 0:03:44.861628
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 20:41:11.900566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.56
 ---- batch: 020 ----
mean loss: 281.23
 ---- batch: 030 ----
mean loss: 285.04
 ---- batch: 040 ----
mean loss: 284.57
 ---- batch: 050 ----
mean loss: 280.66
 ---- batch: 060 ----
mean loss: 284.71
 ---- batch: 070 ----
mean loss: 281.50
 ---- batch: 080 ----
mean loss: 281.07
 ---- batch: 090 ----
mean loss: 277.74
 ---- batch: 100 ----
mean loss: 277.04
 ---- batch: 110 ----
mean loss: 281.23
train mean loss: 281.05
epoch train time: 0:00:02.579162
elapsed time: 0:03:47.441248
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 20:41:14.480197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.18
 ---- batch: 020 ----
mean loss: 282.11
 ---- batch: 030 ----
mean loss: 272.33
 ---- batch: 040 ----
mean loss: 275.83
 ---- batch: 050 ----
mean loss: 279.77
 ---- batch: 060 ----
mean loss: 277.83
 ---- batch: 070 ----
mean loss: 268.90
 ---- batch: 080 ----
mean loss: 275.09
 ---- batch: 090 ----
mean loss: 281.82
 ---- batch: 100 ----
mean loss: 269.77
 ---- batch: 110 ----
mean loss: 267.40
train mean loss: 275.28
epoch train time: 0:00:02.578733
elapsed time: 0:03:50.020395
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 20:41:17.059360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.13
 ---- batch: 020 ----
mean loss: 271.74
 ---- batch: 030 ----
mean loss: 266.31
 ---- batch: 040 ----
mean loss: 269.65
 ---- batch: 050 ----
mean loss: 272.95
 ---- batch: 060 ----
mean loss: 277.11
 ---- batch: 070 ----
mean loss: 273.44
 ---- batch: 080 ----
mean loss: 275.64
 ---- batch: 090 ----
mean loss: 263.28
 ---- batch: 100 ----
mean loss: 266.32
 ---- batch: 110 ----
mean loss: 266.52
train mean loss: 270.54
epoch train time: 0:00:02.554341
elapsed time: 0:03:52.575249
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 20:41:19.614212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.43
 ---- batch: 020 ----
mean loss: 265.39
 ---- batch: 030 ----
mean loss: 266.20
 ---- batch: 040 ----
mean loss: 271.89
 ---- batch: 050 ----
mean loss: 269.75
 ---- batch: 060 ----
mean loss: 263.74
 ---- batch: 070 ----
mean loss: 263.11
 ---- batch: 080 ----
mean loss: 264.45
 ---- batch: 090 ----
mean loss: 274.45
 ---- batch: 100 ----
mean loss: 252.61
 ---- batch: 110 ----
mean loss: 269.85
train mean loss: 265.67
epoch train time: 0:00:02.595319
elapsed time: 0:03:55.171032
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 20:41:22.209988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.22
 ---- batch: 020 ----
mean loss: 267.29
 ---- batch: 030 ----
mean loss: 259.70
 ---- batch: 040 ----
mean loss: 254.45
 ---- batch: 050 ----
mean loss: 267.55
 ---- batch: 060 ----
mean loss: 261.17
 ---- batch: 070 ----
mean loss: 268.12
 ---- batch: 080 ----
mean loss: 254.12
 ---- batch: 090 ----
mean loss: 263.66
 ---- batch: 100 ----
mean loss: 266.59
 ---- batch: 110 ----
mean loss: 253.16
train mean loss: 261.38
epoch train time: 0:00:02.514495
elapsed time: 0:03:57.685959
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 20:41:24.724898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.27
 ---- batch: 020 ----
mean loss: 262.43
 ---- batch: 030 ----
mean loss: 250.58
 ---- batch: 040 ----
mean loss: 255.74
 ---- batch: 050 ----
mean loss: 258.38
 ---- batch: 060 ----
mean loss: 254.74
 ---- batch: 070 ----
mean loss: 254.88
 ---- batch: 080 ----
mean loss: 266.92
 ---- batch: 090 ----
mean loss: 261.78
 ---- batch: 100 ----
mean loss: 249.24
 ---- batch: 110 ----
mean loss: 263.34
train mean loss: 257.44
epoch train time: 0:00:02.584291
elapsed time: 0:04:00.270688
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 20:41:27.309690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.65
 ---- batch: 020 ----
mean loss: 264.01
 ---- batch: 030 ----
mean loss: 259.17
 ---- batch: 040 ----
mean loss: 257.41
 ---- batch: 050 ----
mean loss: 253.02
 ---- batch: 060 ----
mean loss: 254.08
 ---- batch: 070 ----
mean loss: 252.80
 ---- batch: 080 ----
mean loss: 250.92
 ---- batch: 090 ----
mean loss: 250.12
 ---- batch: 100 ----
mean loss: 252.04
 ---- batch: 110 ----
mean loss: 254.88
train mean loss: 254.25
epoch train time: 0:00:02.554223
elapsed time: 0:04:02.825415
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 20:41:29.864363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.52
 ---- batch: 020 ----
mean loss: 258.42
 ---- batch: 030 ----
mean loss: 247.21
 ---- batch: 040 ----
mean loss: 246.23
 ---- batch: 050 ----
mean loss: 254.06
 ---- batch: 060 ----
mean loss: 250.37
 ---- batch: 070 ----
mean loss: 241.18
 ---- batch: 080 ----
mean loss: 247.88
 ---- batch: 090 ----
mean loss: 252.66
 ---- batch: 100 ----
mean loss: 254.67
 ---- batch: 110 ----
mean loss: 249.18
train mean loss: 250.73
epoch train time: 0:00:02.586256
elapsed time: 0:04:05.412082
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 20:41:32.451016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.96
 ---- batch: 020 ----
mean loss: 249.60
 ---- batch: 030 ----
mean loss: 250.98
 ---- batch: 040 ----
mean loss: 244.43
 ---- batch: 050 ----
mean loss: 241.13
 ---- batch: 060 ----
mean loss: 247.27
 ---- batch: 070 ----
mean loss: 252.58
 ---- batch: 080 ----
mean loss: 255.85
 ---- batch: 090 ----
mean loss: 248.10
 ---- batch: 100 ----
mean loss: 250.39
 ---- batch: 110 ----
mean loss: 243.93
train mean loss: 248.06
epoch train time: 0:00:02.571183
elapsed time: 0:04:07.983647
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 20:41:35.022563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.50
 ---- batch: 020 ----
mean loss: 254.80
 ---- batch: 030 ----
mean loss: 239.91
 ---- batch: 040 ----
mean loss: 253.15
 ---- batch: 050 ----
mean loss: 251.26
 ---- batch: 060 ----
mean loss: 242.27
 ---- batch: 070 ----
mean loss: 244.48
 ---- batch: 080 ----
mean loss: 249.48
 ---- batch: 090 ----
mean loss: 245.71
 ---- batch: 100 ----
mean loss: 242.23
 ---- batch: 110 ----
mean loss: 240.17
train mean loss: 245.41
epoch train time: 0:00:02.514355
elapsed time: 0:04:10.498405
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 20:41:37.537344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.85
 ---- batch: 020 ----
mean loss: 255.04
 ---- batch: 030 ----
mean loss: 238.79
 ---- batch: 040 ----
mean loss: 243.97
 ---- batch: 050 ----
mean loss: 245.31
 ---- batch: 060 ----
mean loss: 238.61
 ---- batch: 070 ----
mean loss: 236.10
 ---- batch: 080 ----
mean loss: 235.96
 ---- batch: 090 ----
mean loss: 244.86
 ---- batch: 100 ----
mean loss: 243.25
 ---- batch: 110 ----
mean loss: 244.02
train mean loss: 242.87
epoch train time: 0:00:02.493885
elapsed time: 0:04:12.992742
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 20:41:40.031713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.82
 ---- batch: 020 ----
mean loss: 243.57
 ---- batch: 030 ----
mean loss: 238.46
 ---- batch: 040 ----
mean loss: 234.74
 ---- batch: 050 ----
mean loss: 240.73
 ---- batch: 060 ----
mean loss: 240.33
 ---- batch: 070 ----
mean loss: 239.54
 ---- batch: 080 ----
mean loss: 247.89
 ---- batch: 090 ----
mean loss: 239.18
 ---- batch: 100 ----
mean loss: 234.06
 ---- batch: 110 ----
mean loss: 238.05
train mean loss: 240.56
epoch train time: 0:00:02.503022
elapsed time: 0:04:15.496218
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 20:41:42.535240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.57
 ---- batch: 020 ----
mean loss: 234.28
 ---- batch: 030 ----
mean loss: 242.57
 ---- batch: 040 ----
mean loss: 239.15
 ---- batch: 050 ----
mean loss: 243.72
 ---- batch: 060 ----
mean loss: 234.38
 ---- batch: 070 ----
mean loss: 238.40
 ---- batch: 080 ----
mean loss: 243.63
 ---- batch: 090 ----
mean loss: 243.76
 ---- batch: 100 ----
mean loss: 239.98
 ---- batch: 110 ----
mean loss: 237.22
train mean loss: 238.43
epoch train time: 0:00:02.492458
elapsed time: 0:04:17.989247
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 20:41:45.028274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.26
 ---- batch: 020 ----
mean loss: 238.17
 ---- batch: 030 ----
mean loss: 241.84
 ---- batch: 040 ----
mean loss: 243.20
 ---- batch: 050 ----
mean loss: 233.01
 ---- batch: 060 ----
mean loss: 234.71
 ---- batch: 070 ----
mean loss: 240.03
 ---- batch: 080 ----
mean loss: 234.17
 ---- batch: 090 ----
mean loss: 237.49
 ---- batch: 100 ----
mean loss: 232.81
 ---- batch: 110 ----
mean loss: 230.78
train mean loss: 237.20
epoch train time: 0:00:02.480109
elapsed time: 0:04:20.469916
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 20:41:47.508856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.59
 ---- batch: 020 ----
mean loss: 234.95
 ---- batch: 030 ----
mean loss: 225.50
 ---- batch: 040 ----
mean loss: 236.73
 ---- batch: 050 ----
mean loss: 229.49
 ---- batch: 060 ----
mean loss: 242.61
 ---- batch: 070 ----
mean loss: 241.64
 ---- batch: 080 ----
mean loss: 241.87
 ---- batch: 090 ----
mean loss: 232.24
 ---- batch: 100 ----
mean loss: 237.05
 ---- batch: 110 ----
mean loss: 236.33
train mean loss: 235.27
epoch train time: 0:00:02.511928
elapsed time: 0:04:22.982243
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 20:41:50.021173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.81
 ---- batch: 020 ----
mean loss: 245.48
 ---- batch: 030 ----
mean loss: 242.71
 ---- batch: 040 ----
mean loss: 230.48
 ---- batch: 050 ----
mean loss: 233.92
 ---- batch: 060 ----
mean loss: 235.62
 ---- batch: 070 ----
mean loss: 232.24
 ---- batch: 080 ----
mean loss: 229.02
 ---- batch: 090 ----
mean loss: 232.08
 ---- batch: 100 ----
mean loss: 221.05
 ---- batch: 110 ----
mean loss: 236.75
train mean loss: 234.13
epoch train time: 0:00:02.516355
elapsed time: 0:04:25.499063
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 20:41:52.538014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.20
 ---- batch: 020 ----
mean loss: 233.51
 ---- batch: 030 ----
mean loss: 231.60
 ---- batch: 040 ----
mean loss: 238.79
 ---- batch: 050 ----
mean loss: 226.88
 ---- batch: 060 ----
mean loss: 230.95
 ---- batch: 070 ----
mean loss: 237.51
 ---- batch: 080 ----
mean loss: 228.79
 ---- batch: 090 ----
mean loss: 225.68
 ---- batch: 100 ----
mean loss: 228.72
 ---- batch: 110 ----
mean loss: 237.44
train mean loss: 232.54
epoch train time: 0:00:02.511258
elapsed time: 0:04:28.010729
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 20:41:55.049675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.92
 ---- batch: 020 ----
mean loss: 234.68
 ---- batch: 030 ----
mean loss: 232.78
 ---- batch: 040 ----
mean loss: 237.21
 ---- batch: 050 ----
mean loss: 227.66
 ---- batch: 060 ----
mean loss: 221.73
 ---- batch: 070 ----
mean loss: 231.45
 ---- batch: 080 ----
mean loss: 226.15
 ---- batch: 090 ----
mean loss: 238.50
 ---- batch: 100 ----
mean loss: 231.47
 ---- batch: 110 ----
mean loss: 230.23
train mean loss: 231.28
epoch train time: 0:00:02.526242
elapsed time: 0:04:30.537393
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 20:41:57.576338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.62
 ---- batch: 020 ----
mean loss: 238.94
 ---- batch: 030 ----
mean loss: 225.39
 ---- batch: 040 ----
mean loss: 231.12
 ---- batch: 050 ----
mean loss: 225.45
 ---- batch: 060 ----
mean loss: 229.17
 ---- batch: 070 ----
mean loss: 232.87
 ---- batch: 080 ----
mean loss: 237.84
 ---- batch: 090 ----
mean loss: 228.60
 ---- batch: 100 ----
mean loss: 226.03
 ---- batch: 110 ----
mean loss: 226.99
train mean loss: 230.03
epoch train time: 0:00:02.522493
elapsed time: 0:04:33.060301
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 20:42:00.099260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.10
 ---- batch: 020 ----
mean loss: 231.31
 ---- batch: 030 ----
mean loss: 231.59
 ---- batch: 040 ----
mean loss: 228.36
 ---- batch: 050 ----
mean loss: 232.87
 ---- batch: 060 ----
mean loss: 228.76
 ---- batch: 070 ----
mean loss: 234.11
 ---- batch: 080 ----
mean loss: 229.25
 ---- batch: 090 ----
mean loss: 231.01
 ---- batch: 100 ----
mean loss: 226.25
 ---- batch: 110 ----
mean loss: 221.31
train mean loss: 228.94
epoch train time: 0:00:02.531442
elapsed time: 0:04:35.592191
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 20:42:02.631190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.16
 ---- batch: 020 ----
mean loss: 232.86
 ---- batch: 030 ----
mean loss: 228.87
 ---- batch: 040 ----
mean loss: 230.31
 ---- batch: 050 ----
mean loss: 235.48
 ---- batch: 060 ----
mean loss: 225.89
 ---- batch: 070 ----
mean loss: 221.51
 ---- batch: 080 ----
mean loss: 224.35
 ---- batch: 090 ----
mean loss: 228.50
 ---- batch: 100 ----
mean loss: 223.37
 ---- batch: 110 ----
mean loss: 227.32
train mean loss: 228.35
epoch train time: 0:00:02.528861
elapsed time: 0:04:38.121535
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 20:42:05.160481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.54
 ---- batch: 020 ----
mean loss: 220.78
 ---- batch: 030 ----
mean loss: 207.78
 ---- batch: 040 ----
mean loss: 234.46
 ---- batch: 050 ----
mean loss: 241.23
 ---- batch: 060 ----
mean loss: 231.89
 ---- batch: 070 ----
mean loss: 223.01
 ---- batch: 080 ----
mean loss: 225.87
 ---- batch: 090 ----
mean loss: 227.26
 ---- batch: 100 ----
mean loss: 226.13
 ---- batch: 110 ----
mean loss: 235.44
train mean loss: 227.22
epoch train time: 0:00:02.482503
elapsed time: 0:04:40.604463
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 20:42:07.643425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.57
 ---- batch: 020 ----
mean loss: 220.53
 ---- batch: 030 ----
mean loss: 227.41
 ---- batch: 040 ----
mean loss: 224.52
 ---- batch: 050 ----
mean loss: 235.63
 ---- batch: 060 ----
mean loss: 224.21
 ---- batch: 070 ----
mean loss: 223.53
 ---- batch: 080 ----
mean loss: 233.12
 ---- batch: 090 ----
mean loss: 229.27
 ---- batch: 100 ----
mean loss: 228.18
 ---- batch: 110 ----
mean loss: 223.81
train mean loss: 226.40
epoch train time: 0:00:02.544869
elapsed time: 0:04:43.149792
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 20:42:10.188594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.81
 ---- batch: 020 ----
mean loss: 225.10
 ---- batch: 030 ----
mean loss: 226.89
 ---- batch: 040 ----
mean loss: 218.63
 ---- batch: 050 ----
mean loss: 223.21
 ---- batch: 060 ----
mean loss: 230.67
 ---- batch: 070 ----
mean loss: 226.69
 ---- batch: 080 ----
mean loss: 238.92
 ---- batch: 090 ----
mean loss: 227.96
 ---- batch: 100 ----
mean loss: 223.59
 ---- batch: 110 ----
mean loss: 222.31
train mean loss: 225.60
epoch train time: 0:00:02.507091
elapsed time: 0:04:45.657197
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 20:42:12.696156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.54
 ---- batch: 020 ----
mean loss: 233.56
 ---- batch: 030 ----
mean loss: 218.70
 ---- batch: 040 ----
mean loss: 231.07
 ---- batch: 050 ----
mean loss: 217.60
 ---- batch: 060 ----
mean loss: 226.30
 ---- batch: 070 ----
mean loss: 215.01
 ---- batch: 080 ----
mean loss: 215.53
 ---- batch: 090 ----
mean loss: 224.02
 ---- batch: 100 ----
mean loss: 226.01
 ---- batch: 110 ----
mean loss: 232.28
train mean loss: 224.53
epoch train time: 0:00:02.506035
elapsed time: 0:04:48.163652
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 20:42:15.202612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.95
 ---- batch: 020 ----
mean loss: 226.23
 ---- batch: 030 ----
mean loss: 232.48
 ---- batch: 040 ----
mean loss: 220.55
 ---- batch: 050 ----
mean loss: 224.47
 ---- batch: 060 ----
mean loss: 224.06
 ---- batch: 070 ----
mean loss: 224.67
 ---- batch: 080 ----
mean loss: 224.69
 ---- batch: 090 ----
mean loss: 218.06
 ---- batch: 100 ----
mean loss: 229.26
 ---- batch: 110 ----
mean loss: 223.07
train mean loss: 224.04
epoch train time: 0:00:02.499100
elapsed time: 0:04:50.663250
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 20:42:17.702240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.01
 ---- batch: 020 ----
mean loss: 225.96
 ---- batch: 030 ----
mean loss: 227.12
 ---- batch: 040 ----
mean loss: 221.42
 ---- batch: 050 ----
mean loss: 226.51
 ---- batch: 060 ----
mean loss: 218.79
 ---- batch: 070 ----
mean loss: 222.17
 ---- batch: 080 ----
mean loss: 227.25
 ---- batch: 090 ----
mean loss: 224.10
 ---- batch: 100 ----
mean loss: 214.13
 ---- batch: 110 ----
mean loss: 224.98
train mean loss: 223.50
epoch train time: 0:00:02.548981
elapsed time: 0:04:53.212801
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 20:42:20.251770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.53
 ---- batch: 020 ----
mean loss: 221.74
 ---- batch: 030 ----
mean loss: 229.55
 ---- batch: 040 ----
mean loss: 223.62
 ---- batch: 050 ----
mean loss: 218.62
 ---- batch: 060 ----
mean loss: 224.15
 ---- batch: 070 ----
mean loss: 218.78
 ---- batch: 080 ----
mean loss: 227.61
 ---- batch: 090 ----
mean loss: 217.78
 ---- batch: 100 ----
mean loss: 219.13
 ---- batch: 110 ----
mean loss: 218.91
train mean loss: 222.73
epoch train time: 0:00:02.522195
elapsed time: 0:04:55.735466
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 20:42:22.774442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.42
 ---- batch: 020 ----
mean loss: 217.56
 ---- batch: 030 ----
mean loss: 224.56
 ---- batch: 040 ----
mean loss: 220.72
 ---- batch: 050 ----
mean loss: 214.87
 ---- batch: 060 ----
mean loss: 224.13
 ---- batch: 070 ----
mean loss: 220.56
 ---- batch: 080 ----
mean loss: 217.56
 ---- batch: 090 ----
mean loss: 218.11
 ---- batch: 100 ----
mean loss: 224.68
 ---- batch: 110 ----
mean loss: 231.06
train mean loss: 222.13
epoch train time: 0:00:02.519243
elapsed time: 0:04:58.255166
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 20:42:25.294126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.51
 ---- batch: 020 ----
mean loss: 224.09
 ---- batch: 030 ----
mean loss: 220.62
 ---- batch: 040 ----
mean loss: 220.25
 ---- batch: 050 ----
mean loss: 219.77
 ---- batch: 060 ----
mean loss: 215.51
 ---- batch: 070 ----
mean loss: 226.13
 ---- batch: 080 ----
mean loss: 217.29
 ---- batch: 090 ----
mean loss: 220.76
 ---- batch: 100 ----
mean loss: 224.82
 ---- batch: 110 ----
mean loss: 231.79
train mean loss: 221.51
epoch train time: 0:00:02.514952
elapsed time: 0:05:00.770564
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 20:42:27.809517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.92
 ---- batch: 020 ----
mean loss: 220.26
 ---- batch: 030 ----
mean loss: 220.15
 ---- batch: 040 ----
mean loss: 213.68
 ---- batch: 050 ----
mean loss: 213.84
 ---- batch: 060 ----
mean loss: 225.79
 ---- batch: 070 ----
mean loss: 220.58
 ---- batch: 080 ----
mean loss: 219.61
 ---- batch: 090 ----
mean loss: 219.20
 ---- batch: 100 ----
mean loss: 231.47
 ---- batch: 110 ----
mean loss: 218.15
train mean loss: 220.74
epoch train time: 0:00:02.516404
elapsed time: 0:05:03.287518
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 20:42:30.326365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.99
 ---- batch: 020 ----
mean loss: 230.95
 ---- batch: 030 ----
mean loss: 213.69
 ---- batch: 040 ----
mean loss: 222.08
 ---- batch: 050 ----
mean loss: 223.04
 ---- batch: 060 ----
mean loss: 217.53
 ---- batch: 070 ----
mean loss: 216.25
 ---- batch: 080 ----
mean loss: 223.17
 ---- batch: 090 ----
mean loss: 226.31
 ---- batch: 100 ----
mean loss: 211.61
 ---- batch: 110 ----
mean loss: 223.50
train mean loss: 220.27
epoch train time: 0:00:02.529970
elapsed time: 0:05:05.817818
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 20:42:32.856795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.81
 ---- batch: 020 ----
mean loss: 220.83
 ---- batch: 030 ----
mean loss: 223.49
 ---- batch: 040 ----
mean loss: 213.67
 ---- batch: 050 ----
mean loss: 230.33
 ---- batch: 060 ----
mean loss: 210.92
 ---- batch: 070 ----
mean loss: 228.74
 ---- batch: 080 ----
mean loss: 218.90
 ---- batch: 090 ----
mean loss: 224.02
 ---- batch: 100 ----
mean loss: 207.44
 ---- batch: 110 ----
mean loss: 215.32
train mean loss: 219.47
epoch train time: 0:00:02.519122
elapsed time: 0:05:08.337428
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 20:42:35.376498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.65
 ---- batch: 020 ----
mean loss: 223.50
 ---- batch: 030 ----
mean loss: 209.49
 ---- batch: 040 ----
mean loss: 214.33
 ---- batch: 050 ----
mean loss: 221.18
 ---- batch: 060 ----
mean loss: 216.84
 ---- batch: 070 ----
mean loss: 222.04
 ---- batch: 080 ----
mean loss: 220.83
 ---- batch: 090 ----
mean loss: 221.33
 ---- batch: 100 ----
mean loss: 224.91
 ---- batch: 110 ----
mean loss: 220.67
train mean loss: 219.12
epoch train time: 0:00:02.526684
elapsed time: 0:05:10.864653
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 20:42:37.903640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.83
 ---- batch: 020 ----
mean loss: 209.04
 ---- batch: 030 ----
mean loss: 225.89
 ---- batch: 040 ----
mean loss: 211.18
 ---- batch: 050 ----
mean loss: 222.73
 ---- batch: 060 ----
mean loss: 214.74
 ---- batch: 070 ----
mean loss: 225.49
 ---- batch: 080 ----
mean loss: 222.43
 ---- batch: 090 ----
mean loss: 214.07
 ---- batch: 100 ----
mean loss: 219.69
 ---- batch: 110 ----
mean loss: 223.86
train mean loss: 218.74
epoch train time: 0:00:02.507321
elapsed time: 0:05:13.372472
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 20:42:40.411422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.53
 ---- batch: 020 ----
mean loss: 218.43
 ---- batch: 030 ----
mean loss: 214.84
 ---- batch: 040 ----
mean loss: 208.53
 ---- batch: 050 ----
mean loss: 226.60
 ---- batch: 060 ----
mean loss: 217.69
 ---- batch: 070 ----
mean loss: 219.84
 ---- batch: 080 ----
mean loss: 226.58
 ---- batch: 090 ----
mean loss: 214.04
 ---- batch: 100 ----
mean loss: 210.95
 ---- batch: 110 ----
mean loss: 221.70
train mean loss: 218.24
epoch train time: 0:00:02.536792
elapsed time: 0:05:15.909739
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 20:42:42.948692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.62
 ---- batch: 020 ----
mean loss: 216.12
 ---- batch: 030 ----
mean loss: 218.21
 ---- batch: 040 ----
mean loss: 218.43
 ---- batch: 050 ----
mean loss: 210.24
 ---- batch: 060 ----
mean loss: 220.94
 ---- batch: 070 ----
mean loss: 217.55
 ---- batch: 080 ----
mean loss: 221.06
 ---- batch: 090 ----
mean loss: 227.29
 ---- batch: 100 ----
mean loss: 208.89
 ---- batch: 110 ----
mean loss: 220.00
train mean loss: 217.95
epoch train time: 0:00:02.612170
elapsed time: 0:05:18.522355
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 20:42:45.561318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.30
 ---- batch: 020 ----
mean loss: 207.00
 ---- batch: 030 ----
mean loss: 217.24
 ---- batch: 040 ----
mean loss: 205.39
 ---- batch: 050 ----
mean loss: 216.95
 ---- batch: 060 ----
mean loss: 221.78
 ---- batch: 070 ----
mean loss: 218.05
 ---- batch: 080 ----
mean loss: 221.01
 ---- batch: 090 ----
mean loss: 220.34
 ---- batch: 100 ----
mean loss: 220.59
 ---- batch: 110 ----
mean loss: 222.77
train mean loss: 217.14
epoch train time: 0:00:02.523160
elapsed time: 0:05:21.045973
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 20:42:48.084913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.95
 ---- batch: 020 ----
mean loss: 225.09
 ---- batch: 030 ----
mean loss: 213.06
 ---- batch: 040 ----
mean loss: 219.40
 ---- batch: 050 ----
mean loss: 225.24
 ---- batch: 060 ----
mean loss: 231.82
 ---- batch: 070 ----
mean loss: 217.24
 ---- batch: 080 ----
mean loss: 208.12
 ---- batch: 090 ----
mean loss: 214.58
 ---- batch: 100 ----
mean loss: 215.35
 ---- batch: 110 ----
mean loss: 210.94
train mean loss: 217.18
epoch train time: 0:00:02.502688
elapsed time: 0:05:23.549070
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 20:42:50.588037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.95
 ---- batch: 020 ----
mean loss: 223.95
 ---- batch: 030 ----
mean loss: 219.12
 ---- batch: 040 ----
mean loss: 207.73
 ---- batch: 050 ----
mean loss: 215.18
 ---- batch: 060 ----
mean loss: 214.50
 ---- batch: 070 ----
mean loss: 213.59
 ---- batch: 080 ----
mean loss: 221.99
 ---- batch: 090 ----
mean loss: 222.26
 ---- batch: 100 ----
mean loss: 208.91
 ---- batch: 110 ----
mean loss: 209.00
train mean loss: 216.48
epoch train time: 0:00:02.489440
elapsed time: 0:05:26.038961
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 20:42:53.077905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.75
 ---- batch: 020 ----
mean loss: 214.70
 ---- batch: 030 ----
mean loss: 214.21
 ---- batch: 040 ----
mean loss: 211.80
 ---- batch: 050 ----
mean loss: 212.55
 ---- batch: 060 ----
mean loss: 215.95
 ---- batch: 070 ----
mean loss: 222.81
 ---- batch: 080 ----
mean loss: 221.44
 ---- batch: 090 ----
mean loss: 216.73
 ---- batch: 100 ----
mean loss: 223.58
 ---- batch: 110 ----
mean loss: 210.82
train mean loss: 216.07
epoch train time: 0:00:02.529298
elapsed time: 0:05:28.568673
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 20:42:55.607628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.76
 ---- batch: 020 ----
mean loss: 221.62
 ---- batch: 030 ----
mean loss: 214.09
 ---- batch: 040 ----
mean loss: 211.31
 ---- batch: 050 ----
mean loss: 212.71
 ---- batch: 060 ----
mean loss: 215.18
 ---- batch: 070 ----
mean loss: 214.28
 ---- batch: 080 ----
mean loss: 226.84
 ---- batch: 090 ----
mean loss: 213.56
 ---- batch: 100 ----
mean loss: 214.70
 ---- batch: 110 ----
mean loss: 209.01
train mean loss: 215.62
epoch train time: 0:00:02.525999
elapsed time: 0:05:31.095099
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 20:42:58.134058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.86
 ---- batch: 020 ----
mean loss: 214.76
 ---- batch: 030 ----
mean loss: 217.12
 ---- batch: 040 ----
mean loss: 212.88
 ---- batch: 050 ----
mean loss: 214.88
 ---- batch: 060 ----
mean loss: 208.49
 ---- batch: 070 ----
mean loss: 213.20
 ---- batch: 080 ----
mean loss: 221.68
 ---- batch: 090 ----
mean loss: 222.46
 ---- batch: 100 ----
mean loss: 216.04
 ---- batch: 110 ----
mean loss: 212.73
train mean loss: 215.10
epoch train time: 0:00:02.492964
elapsed time: 0:05:33.588494
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 20:43:00.627494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.35
 ---- batch: 020 ----
mean loss: 208.80
 ---- batch: 030 ----
mean loss: 209.09
 ---- batch: 040 ----
mean loss: 218.53
 ---- batch: 050 ----
mean loss: 216.66
 ---- batch: 060 ----
mean loss: 223.27
 ---- batch: 070 ----
mean loss: 212.92
 ---- batch: 080 ----
mean loss: 215.34
 ---- batch: 090 ----
mean loss: 224.21
 ---- batch: 100 ----
mean loss: 211.53
 ---- batch: 110 ----
mean loss: 214.40
train mean loss: 214.69
epoch train time: 0:00:02.509774
elapsed time: 0:05:36.098726
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 20:43:03.137703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.05
 ---- batch: 020 ----
mean loss: 201.60
 ---- batch: 030 ----
mean loss: 218.66
 ---- batch: 040 ----
mean loss: 214.13
 ---- batch: 050 ----
mean loss: 211.20
 ---- batch: 060 ----
mean loss: 214.21
 ---- batch: 070 ----
mean loss: 217.16
 ---- batch: 080 ----
mean loss: 202.29
 ---- batch: 090 ----
mean loss: 222.40
 ---- batch: 100 ----
mean loss: 215.13
 ---- batch: 110 ----
mean loss: 215.64
train mean loss: 214.33
epoch train time: 0:00:02.483010
elapsed time: 0:05:38.582185
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 20:43:05.621146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.23
 ---- batch: 020 ----
mean loss: 212.09
 ---- batch: 030 ----
mean loss: 214.23
 ---- batch: 040 ----
mean loss: 217.84
 ---- batch: 050 ----
mean loss: 208.18
 ---- batch: 060 ----
mean loss: 212.61
 ---- batch: 070 ----
mean loss: 211.35
 ---- batch: 080 ----
mean loss: 213.55
 ---- batch: 090 ----
mean loss: 213.75
 ---- batch: 100 ----
mean loss: 218.41
 ---- batch: 110 ----
mean loss: 214.75
train mean loss: 213.70
epoch train time: 0:00:02.510428
elapsed time: 0:05:41.093064
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 20:43:08.132058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.89
 ---- batch: 020 ----
mean loss: 219.15
 ---- batch: 030 ----
mean loss: 209.93
 ---- batch: 040 ----
mean loss: 217.22
 ---- batch: 050 ----
mean loss: 219.70
 ---- batch: 060 ----
mean loss: 212.97
 ---- batch: 070 ----
mean loss: 216.91
 ---- batch: 080 ----
mean loss: 216.79
 ---- batch: 090 ----
mean loss: 198.70
 ---- batch: 100 ----
mean loss: 217.55
 ---- batch: 110 ----
mean loss: 202.61
train mean loss: 213.30
epoch train time: 0:00:02.503457
elapsed time: 0:05:43.596984
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 20:43:10.635914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.44
 ---- batch: 020 ----
mean loss: 204.45
 ---- batch: 030 ----
mean loss: 212.25
 ---- batch: 040 ----
mean loss: 213.40
 ---- batch: 050 ----
mean loss: 213.61
 ---- batch: 060 ----
mean loss: 217.21
 ---- batch: 070 ----
mean loss: 210.64
 ---- batch: 080 ----
mean loss: 214.26
 ---- batch: 090 ----
mean loss: 221.27
 ---- batch: 100 ----
mean loss: 211.67
 ---- batch: 110 ----
mean loss: 204.33
train mean loss: 212.73
epoch train time: 0:00:02.520757
elapsed time: 0:05:46.118137
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 20:43:13.157078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.36
 ---- batch: 020 ----
mean loss: 208.33
 ---- batch: 030 ----
mean loss: 212.45
 ---- batch: 040 ----
mean loss: 219.74
 ---- batch: 050 ----
mean loss: 219.85
 ---- batch: 060 ----
mean loss: 214.07
 ---- batch: 070 ----
mean loss: 215.74
 ---- batch: 080 ----
mean loss: 209.49
 ---- batch: 090 ----
mean loss: 211.23
 ---- batch: 100 ----
mean loss: 203.24
 ---- batch: 110 ----
mean loss: 210.15
train mean loss: 212.31
epoch train time: 0:00:02.489455
elapsed time: 0:05:48.607997
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 20:43:15.646912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.83
 ---- batch: 020 ----
mean loss: 207.91
 ---- batch: 030 ----
mean loss: 215.79
 ---- batch: 040 ----
mean loss: 218.40
 ---- batch: 050 ----
mean loss: 204.15
 ---- batch: 060 ----
mean loss: 212.38
 ---- batch: 070 ----
mean loss: 220.18
 ---- batch: 080 ----
mean loss: 216.92
 ---- batch: 090 ----
mean loss: 204.93
 ---- batch: 100 ----
mean loss: 204.99
 ---- batch: 110 ----
mean loss: 215.17
train mean loss: 211.96
epoch train time: 0:00:02.488160
elapsed time: 0:05:51.096549
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 20:43:18.135517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.03
 ---- batch: 020 ----
mean loss: 205.80
 ---- batch: 030 ----
mean loss: 206.62
 ---- batch: 040 ----
mean loss: 215.59
 ---- batch: 050 ----
mean loss: 213.03
 ---- batch: 060 ----
mean loss: 209.28
 ---- batch: 070 ----
mean loss: 213.20
 ---- batch: 080 ----
mean loss: 216.27
 ---- batch: 090 ----
mean loss: 209.89
 ---- batch: 100 ----
mean loss: 212.90
 ---- batch: 110 ----
mean loss: 209.93
train mean loss: 211.73
epoch train time: 0:00:02.492444
elapsed time: 0:05:53.589552
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 20:43:20.628387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.36
 ---- batch: 020 ----
mean loss: 212.23
 ---- batch: 030 ----
mean loss: 210.64
 ---- batch: 040 ----
mean loss: 196.81
 ---- batch: 050 ----
mean loss: 224.82
 ---- batch: 060 ----
mean loss: 211.09
 ---- batch: 070 ----
mean loss: 207.24
 ---- batch: 080 ----
mean loss: 210.87
 ---- batch: 090 ----
mean loss: 212.76
 ---- batch: 100 ----
mean loss: 209.36
 ---- batch: 110 ----
mean loss: 211.46
train mean loss: 211.25
epoch train time: 0:00:02.499127
elapsed time: 0:05:56.088976
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 20:43:23.127932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.13
 ---- batch: 020 ----
mean loss: 205.55
 ---- batch: 030 ----
mean loss: 210.87
 ---- batch: 040 ----
mean loss: 207.92
 ---- batch: 050 ----
mean loss: 220.17
 ---- batch: 060 ----
mean loss: 210.73
 ---- batch: 070 ----
mean loss: 213.42
 ---- batch: 080 ----
mean loss: 211.79
 ---- batch: 090 ----
mean loss: 203.92
 ---- batch: 100 ----
mean loss: 216.81
 ---- batch: 110 ----
mean loss: 202.82
train mean loss: 211.02
epoch train time: 0:00:02.532885
elapsed time: 0:05:58.622357
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 20:43:25.661335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.88
 ---- batch: 020 ----
mean loss: 218.53
 ---- batch: 030 ----
mean loss: 209.42
 ---- batch: 040 ----
mean loss: 209.94
 ---- batch: 050 ----
mean loss: 214.55
 ---- batch: 060 ----
mean loss: 215.04
 ---- batch: 070 ----
mean loss: 201.67
 ---- batch: 080 ----
mean loss: 210.49
 ---- batch: 090 ----
mean loss: 209.33
 ---- batch: 100 ----
mean loss: 208.86
 ---- batch: 110 ----
mean loss: 212.49
train mean loss: 210.57
epoch train time: 0:00:02.509937
elapsed time: 0:06:01.132770
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 20:43:28.171711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.29
 ---- batch: 020 ----
mean loss: 209.40
 ---- batch: 030 ----
mean loss: 210.05
 ---- batch: 040 ----
mean loss: 219.36
 ---- batch: 050 ----
mean loss: 202.47
 ---- batch: 060 ----
mean loss: 207.71
 ---- batch: 070 ----
mean loss: 216.72
 ---- batch: 080 ----
mean loss: 210.93
 ---- batch: 090 ----
mean loss: 213.10
 ---- batch: 100 ----
mean loss: 207.69
 ---- batch: 110 ----
mean loss: 206.40
train mean loss: 210.14
epoch train time: 0:00:02.502503
elapsed time: 0:06:03.635727
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 20:43:30.674666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.43
 ---- batch: 020 ----
mean loss: 211.04
 ---- batch: 030 ----
mean loss: 216.35
 ---- batch: 040 ----
mean loss: 210.59
 ---- batch: 050 ----
mean loss: 211.16
 ---- batch: 060 ----
mean loss: 208.19
 ---- batch: 070 ----
mean loss: 211.77
 ---- batch: 080 ----
mean loss: 208.08
 ---- batch: 090 ----
mean loss: 205.18
 ---- batch: 100 ----
mean loss: 212.04
 ---- batch: 110 ----
mean loss: 206.57
train mean loss: 210.35
epoch train time: 0:00:02.517507
elapsed time: 0:06:06.153642
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 20:43:33.192593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.35
 ---- batch: 020 ----
mean loss: 217.63
 ---- batch: 030 ----
mean loss: 211.45
 ---- batch: 040 ----
mean loss: 212.37
 ---- batch: 050 ----
mean loss: 206.83
 ---- batch: 060 ----
mean loss: 210.46
 ---- batch: 070 ----
mean loss: 215.54
 ---- batch: 080 ----
mean loss: 204.79
 ---- batch: 090 ----
mean loss: 197.15
 ---- batch: 100 ----
mean loss: 210.49
 ---- batch: 110 ----
mean loss: 207.76
train mean loss: 209.63
epoch train time: 0:00:02.534041
elapsed time: 0:06:08.688111
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 20:43:35.727055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.10
 ---- batch: 020 ----
mean loss: 207.55
 ---- batch: 030 ----
mean loss: 209.16
 ---- batch: 040 ----
mean loss: 200.42
 ---- batch: 050 ----
mean loss: 206.61
 ---- batch: 060 ----
mean loss: 201.64
 ---- batch: 070 ----
mean loss: 207.18
 ---- batch: 080 ----
mean loss: 208.85
 ---- batch: 090 ----
mean loss: 216.69
 ---- batch: 100 ----
mean loss: 201.47
 ---- batch: 110 ----
mean loss: 217.75
train mean loss: 209.17
epoch train time: 0:00:02.537307
elapsed time: 0:06:11.225843
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 20:43:38.264846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.66
 ---- batch: 020 ----
mean loss: 212.03
 ---- batch: 030 ----
mean loss: 218.43
 ---- batch: 040 ----
mean loss: 209.14
 ---- batch: 050 ----
mean loss: 208.99
 ---- batch: 060 ----
mean loss: 214.46
 ---- batch: 070 ----
mean loss: 207.68
 ---- batch: 080 ----
mean loss: 202.94
 ---- batch: 090 ----
mean loss: 204.23
 ---- batch: 100 ----
mean loss: 209.69
 ---- batch: 110 ----
mean loss: 204.52
train mean loss: 209.10
epoch train time: 0:00:02.526610
elapsed time: 0:06:13.752925
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 20:43:40.791853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.52
 ---- batch: 020 ----
mean loss: 214.21
 ---- batch: 030 ----
mean loss: 215.91
 ---- batch: 040 ----
mean loss: 211.64
 ---- batch: 050 ----
mean loss: 204.76
 ---- batch: 060 ----
mean loss: 206.73
 ---- batch: 070 ----
mean loss: 206.63
 ---- batch: 080 ----
mean loss: 203.66
 ---- batch: 090 ----
mean loss: 213.12
 ---- batch: 100 ----
mean loss: 205.08
 ---- batch: 110 ----
mean loss: 207.22
train mean loss: 208.75
epoch train time: 0:00:02.476054
elapsed time: 0:06:16.229395
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 20:43:43.268336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.85
 ---- batch: 020 ----
mean loss: 212.92
 ---- batch: 030 ----
mean loss: 207.95
 ---- batch: 040 ----
mean loss: 209.54
 ---- batch: 050 ----
mean loss: 201.48
 ---- batch: 060 ----
mean loss: 202.68
 ---- batch: 070 ----
mean loss: 211.70
 ---- batch: 080 ----
mean loss: 204.45
 ---- batch: 090 ----
mean loss: 212.69
 ---- batch: 100 ----
mean loss: 200.78
 ---- batch: 110 ----
mean loss: 202.97
train mean loss: 208.38
epoch train time: 0:00:02.502764
elapsed time: 0:06:18.732569
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 20:43:45.771543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.01
 ---- batch: 020 ----
mean loss: 212.64
 ---- batch: 030 ----
mean loss: 209.89
 ---- batch: 040 ----
mean loss: 209.59
 ---- batch: 050 ----
mean loss: 213.57
 ---- batch: 060 ----
mean loss: 213.16
 ---- batch: 070 ----
mean loss: 197.06
 ---- batch: 080 ----
mean loss: 208.93
 ---- batch: 090 ----
mean loss: 198.36
 ---- batch: 100 ----
mean loss: 215.91
 ---- batch: 110 ----
mean loss: 212.12
train mean loss: 208.05
epoch train time: 0:00:02.494645
elapsed time: 0:06:21.227658
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 20:43:48.266616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.18
 ---- batch: 020 ----
mean loss: 213.56
 ---- batch: 030 ----
mean loss: 203.65
 ---- batch: 040 ----
mean loss: 214.86
 ---- batch: 050 ----
mean loss: 198.43
 ---- batch: 060 ----
mean loss: 211.57
 ---- batch: 070 ----
mean loss: 196.88
 ---- batch: 080 ----
mean loss: 209.87
 ---- batch: 090 ----
mean loss: 202.60
 ---- batch: 100 ----
mean loss: 214.32
 ---- batch: 110 ----
mean loss: 203.82
train mean loss: 207.67
epoch train time: 0:00:02.492898
elapsed time: 0:06:23.721036
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 20:43:50.760053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.01
 ---- batch: 020 ----
mean loss: 216.82
 ---- batch: 030 ----
mean loss: 209.87
 ---- batch: 040 ----
mean loss: 212.55
 ---- batch: 050 ----
mean loss: 210.81
 ---- batch: 060 ----
mean loss: 209.82
 ---- batch: 070 ----
mean loss: 202.19
 ---- batch: 080 ----
mean loss: 201.88
 ---- batch: 090 ----
mean loss: 208.39
 ---- batch: 100 ----
mean loss: 197.14
 ---- batch: 110 ----
mean loss: 208.59
train mean loss: 207.38
epoch train time: 0:00:02.486970
elapsed time: 0:06:26.208497
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 20:43:53.247441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.70
 ---- batch: 020 ----
mean loss: 208.12
 ---- batch: 030 ----
mean loss: 215.31
 ---- batch: 040 ----
mean loss: 212.80
 ---- batch: 050 ----
mean loss: 213.07
 ---- batch: 060 ----
mean loss: 200.34
 ---- batch: 070 ----
mean loss: 211.75
 ---- batch: 080 ----
mean loss: 204.79
 ---- batch: 090 ----
mean loss: 216.05
 ---- batch: 100 ----
mean loss: 205.05
 ---- batch: 110 ----
mean loss: 198.73
train mean loss: 207.36
epoch train time: 0:00:02.478209
elapsed time: 0:06:28.687162
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 20:43:55.726240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.19
 ---- batch: 020 ----
mean loss: 217.46
 ---- batch: 030 ----
mean loss: 208.16
 ---- batch: 040 ----
mean loss: 199.22
 ---- batch: 050 ----
mean loss: 199.45
 ---- batch: 060 ----
mean loss: 209.68
 ---- batch: 070 ----
mean loss: 206.11
 ---- batch: 080 ----
mean loss: 210.65
 ---- batch: 090 ----
mean loss: 205.72
 ---- batch: 100 ----
mean loss: 205.56
 ---- batch: 110 ----
mean loss: 201.89
train mean loss: 206.76
epoch train time: 0:00:02.508477
elapsed time: 0:06:31.196222
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 20:43:58.235215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.99
 ---- batch: 020 ----
mean loss: 215.86
 ---- batch: 030 ----
mean loss: 203.98
 ---- batch: 040 ----
mean loss: 205.45
 ---- batch: 050 ----
mean loss: 203.16
 ---- batch: 060 ----
mean loss: 202.57
 ---- batch: 070 ----
mean loss: 204.89
 ---- batch: 080 ----
mean loss: 207.27
 ---- batch: 090 ----
mean loss: 198.17
 ---- batch: 100 ----
mean loss: 212.18
 ---- batch: 110 ----
mean loss: 215.08
train mean loss: 206.70
epoch train time: 0:00:02.504686
elapsed time: 0:06:33.701364
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 20:44:00.740298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.27
 ---- batch: 020 ----
mean loss: 200.96
 ---- batch: 030 ----
mean loss: 197.64
 ---- batch: 040 ----
mean loss: 206.00
 ---- batch: 050 ----
mean loss: 214.36
 ---- batch: 060 ----
mean loss: 198.03
 ---- batch: 070 ----
mean loss: 208.39
 ---- batch: 080 ----
mean loss: 217.82
 ---- batch: 090 ----
mean loss: 211.06
 ---- batch: 100 ----
mean loss: 210.01
 ---- batch: 110 ----
mean loss: 196.06
train mean loss: 206.29
epoch train time: 0:00:02.483467
elapsed time: 0:06:36.185247
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 20:44:03.224220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.37
 ---- batch: 020 ----
mean loss: 205.06
 ---- batch: 030 ----
mean loss: 207.68
 ---- batch: 040 ----
mean loss: 212.72
 ---- batch: 050 ----
mean loss: 202.17
 ---- batch: 060 ----
mean loss: 209.74
 ---- batch: 070 ----
mean loss: 199.50
 ---- batch: 080 ----
mean loss: 215.37
 ---- batch: 090 ----
mean loss: 203.32
 ---- batch: 100 ----
mean loss: 205.96
 ---- batch: 110 ----
mean loss: 204.22
train mean loss: 205.99
epoch train time: 0:00:02.481495
elapsed time: 0:06:38.667197
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 20:44:05.706138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.73
 ---- batch: 020 ----
mean loss: 212.89
 ---- batch: 030 ----
mean loss: 212.45
 ---- batch: 040 ----
mean loss: 202.32
 ---- batch: 050 ----
mean loss: 212.61
 ---- batch: 060 ----
mean loss: 208.99
 ---- batch: 070 ----
mean loss: 204.20
 ---- batch: 080 ----
mean loss: 205.85
 ---- batch: 090 ----
mean loss: 204.68
 ---- batch: 100 ----
mean loss: 201.13
 ---- batch: 110 ----
mean loss: 203.96
train mean loss: 205.77
epoch train time: 0:00:02.480534
elapsed time: 0:06:41.148141
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 20:44:08.187095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.22
 ---- batch: 020 ----
mean loss: 210.94
 ---- batch: 030 ----
mean loss: 200.75
 ---- batch: 040 ----
mean loss: 208.31
 ---- batch: 050 ----
mean loss: 201.49
 ---- batch: 060 ----
mean loss: 204.61
 ---- batch: 070 ----
mean loss: 209.38
 ---- batch: 080 ----
mean loss: 201.10
 ---- batch: 090 ----
mean loss: 209.56
 ---- batch: 100 ----
mean loss: 198.32
 ---- batch: 110 ----
mean loss: 208.70
train mean loss: 205.85
epoch train time: 0:00:02.484684
elapsed time: 0:06:43.633282
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 20:44:10.672219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.19
 ---- batch: 020 ----
mean loss: 208.04
 ---- batch: 030 ----
mean loss: 203.56
 ---- batch: 040 ----
mean loss: 204.88
 ---- batch: 050 ----
mean loss: 213.59
 ---- batch: 060 ----
mean loss: 202.70
 ---- batch: 070 ----
mean loss: 205.73
 ---- batch: 080 ----
mean loss: 204.46
 ---- batch: 090 ----
mean loss: 204.12
 ---- batch: 100 ----
mean loss: 212.05
 ---- batch: 110 ----
mean loss: 195.63
train mean loss: 205.21
epoch train time: 0:00:02.511707
elapsed time: 0:06:46.145403
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 20:44:13.184382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.32
 ---- batch: 020 ----
mean loss: 208.44
 ---- batch: 030 ----
mean loss: 211.62
 ---- batch: 040 ----
mean loss: 200.92
 ---- batch: 050 ----
mean loss: 205.72
 ---- batch: 060 ----
mean loss: 196.02
 ---- batch: 070 ----
mean loss: 201.70
 ---- batch: 080 ----
mean loss: 214.46
 ---- batch: 090 ----
mean loss: 209.07
 ---- batch: 100 ----
mean loss: 209.70
 ---- batch: 110 ----
mean loss: 197.20
train mean loss: 205.02
epoch train time: 0:00:02.489997
elapsed time: 0:06:48.635998
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 20:44:15.674831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.13
 ---- batch: 020 ----
mean loss: 199.71
 ---- batch: 030 ----
mean loss: 196.01
 ---- batch: 040 ----
mean loss: 193.63
 ---- batch: 050 ----
mean loss: 201.44
 ---- batch: 060 ----
mean loss: 207.62
 ---- batch: 070 ----
mean loss: 208.88
 ---- batch: 080 ----
mean loss: 210.08
 ---- batch: 090 ----
mean loss: 208.24
 ---- batch: 100 ----
mean loss: 207.79
 ---- batch: 110 ----
mean loss: 204.15
train mean loss: 204.70
epoch train time: 0:00:02.487014
elapsed time: 0:06:51.123299
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 20:44:18.162228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.42
 ---- batch: 020 ----
mean loss: 201.37
 ---- batch: 030 ----
mean loss: 212.77
 ---- batch: 040 ----
mean loss: 199.59
 ---- batch: 050 ----
mean loss: 204.75
 ---- batch: 060 ----
mean loss: 213.26
 ---- batch: 070 ----
mean loss: 197.80
 ---- batch: 080 ----
mean loss: 211.48
 ---- batch: 090 ----
mean loss: 205.57
 ---- batch: 100 ----
mean loss: 197.37
 ---- batch: 110 ----
mean loss: 203.23
train mean loss: 204.83
epoch train time: 0:00:02.485909
elapsed time: 0:06:53.609716
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 20:44:20.648707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.72
 ---- batch: 020 ----
mean loss: 200.54
 ---- batch: 030 ----
mean loss: 207.16
 ---- batch: 040 ----
mean loss: 206.29
 ---- batch: 050 ----
mean loss: 203.54
 ---- batch: 060 ----
mean loss: 208.31
 ---- batch: 070 ----
mean loss: 204.32
 ---- batch: 080 ----
mean loss: 203.47
 ---- batch: 090 ----
mean loss: 203.44
 ---- batch: 100 ----
mean loss: 208.30
 ---- batch: 110 ----
mean loss: 197.51
train mean loss: 204.38
epoch train time: 0:00:02.497548
elapsed time: 0:06:56.107810
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 20:44:23.146778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.45
 ---- batch: 020 ----
mean loss: 198.02
 ---- batch: 030 ----
mean loss: 203.02
 ---- batch: 040 ----
mean loss: 196.85
 ---- batch: 050 ----
mean loss: 207.69
 ---- batch: 060 ----
mean loss: 201.13
 ---- batch: 070 ----
mean loss: 196.69
 ---- batch: 080 ----
mean loss: 211.80
 ---- batch: 090 ----
mean loss: 210.55
 ---- batch: 100 ----
mean loss: 203.24
 ---- batch: 110 ----
mean loss: 197.86
train mean loss: 203.98
epoch train time: 0:00:02.501699
elapsed time: 0:06:58.609948
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 20:44:25.648896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.33
 ---- batch: 020 ----
mean loss: 191.78
 ---- batch: 030 ----
mean loss: 202.25
 ---- batch: 040 ----
mean loss: 202.12
 ---- batch: 050 ----
mean loss: 211.44
 ---- batch: 060 ----
mean loss: 207.74
 ---- batch: 070 ----
mean loss: 206.41
 ---- batch: 080 ----
mean loss: 201.70
 ---- batch: 090 ----
mean loss: 199.62
 ---- batch: 100 ----
mean loss: 203.35
 ---- batch: 110 ----
mean loss: 206.85
train mean loss: 203.69
epoch train time: 0:00:02.514900
elapsed time: 0:07:01.125293
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 20:44:28.164275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.61
 ---- batch: 020 ----
mean loss: 200.59
 ---- batch: 030 ----
mean loss: 207.06
 ---- batch: 040 ----
mean loss: 200.93
 ---- batch: 050 ----
mean loss: 203.13
 ---- batch: 060 ----
mean loss: 201.58
 ---- batch: 070 ----
mean loss: 204.57
 ---- batch: 080 ----
mean loss: 201.89
 ---- batch: 090 ----
mean loss: 207.78
 ---- batch: 100 ----
mean loss: 198.23
 ---- batch: 110 ----
mean loss: 208.13
train mean loss: 203.22
epoch train time: 0:00:02.516726
elapsed time: 0:07:03.642475
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 20:44:30.681425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.64
 ---- batch: 020 ----
mean loss: 198.43
 ---- batch: 030 ----
mean loss: 198.36
 ---- batch: 040 ----
mean loss: 204.72
 ---- batch: 050 ----
mean loss: 198.85
 ---- batch: 060 ----
mean loss: 200.96
 ---- batch: 070 ----
mean loss: 209.37
 ---- batch: 080 ----
mean loss: 202.39
 ---- batch: 090 ----
mean loss: 208.20
 ---- batch: 100 ----
mean loss: 204.27
 ---- batch: 110 ----
mean loss: 205.66
train mean loss: 203.23
epoch train time: 0:00:02.510483
elapsed time: 0:07:06.153376
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 20:44:33.192341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.58
 ---- batch: 020 ----
mean loss: 198.45
 ---- batch: 030 ----
mean loss: 209.39
 ---- batch: 040 ----
mean loss: 217.26
 ---- batch: 050 ----
mean loss: 198.08
 ---- batch: 060 ----
mean loss: 200.84
 ---- batch: 070 ----
mean loss: 204.37
 ---- batch: 080 ----
mean loss: 198.95
 ---- batch: 090 ----
mean loss: 196.78
 ---- batch: 100 ----
mean loss: 205.42
 ---- batch: 110 ----
mean loss: 197.15
train mean loss: 202.96
epoch train time: 0:00:02.502411
elapsed time: 0:07:08.656283
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 20:44:35.695248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.63
 ---- batch: 020 ----
mean loss: 203.26
 ---- batch: 030 ----
mean loss: 204.88
 ---- batch: 040 ----
mean loss: 200.41
 ---- batch: 050 ----
mean loss: 194.04
 ---- batch: 060 ----
mean loss: 207.29
 ---- batch: 070 ----
mean loss: 195.71
 ---- batch: 080 ----
mean loss: 202.52
 ---- batch: 090 ----
mean loss: 205.61
 ---- batch: 100 ----
mean loss: 210.36
 ---- batch: 110 ----
mean loss: 207.73
train mean loss: 202.65
epoch train time: 0:00:02.503021
elapsed time: 0:07:11.159764
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 20:44:38.198713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.46
 ---- batch: 020 ----
mean loss: 200.73
 ---- batch: 030 ----
mean loss: 206.13
 ---- batch: 040 ----
mean loss: 197.12
 ---- batch: 050 ----
mean loss: 199.66
 ---- batch: 060 ----
mean loss: 197.28
 ---- batch: 070 ----
mean loss: 203.49
 ---- batch: 080 ----
mean loss: 195.90
 ---- batch: 090 ----
mean loss: 195.02
 ---- batch: 100 ----
mean loss: 207.65
 ---- batch: 110 ----
mean loss: 212.03
train mean loss: 202.28
epoch train time: 0:00:02.492424
elapsed time: 0:07:13.652605
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 20:44:40.691565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.10
 ---- batch: 020 ----
mean loss: 198.86
 ---- batch: 030 ----
mean loss: 198.24
 ---- batch: 040 ----
mean loss: 204.80
 ---- batch: 050 ----
mean loss: 206.96
 ---- batch: 060 ----
mean loss: 196.14
 ---- batch: 070 ----
mean loss: 206.74
 ---- batch: 080 ----
mean loss: 200.87
 ---- batch: 090 ----
mean loss: 196.57
 ---- batch: 100 ----
mean loss: 211.67
 ---- batch: 110 ----
mean loss: 198.95
train mean loss: 202.10
epoch train time: 0:00:02.483907
elapsed time: 0:07:16.136930
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 20:44:43.175998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.55
 ---- batch: 020 ----
mean loss: 202.21
 ---- batch: 030 ----
mean loss: 197.73
 ---- batch: 040 ----
mean loss: 203.72
 ---- batch: 050 ----
mean loss: 218.03
 ---- batch: 060 ----
mean loss: 195.51
 ---- batch: 070 ----
mean loss: 197.62
 ---- batch: 080 ----
mean loss: 207.79
 ---- batch: 090 ----
mean loss: 204.06
 ---- batch: 100 ----
mean loss: 192.22
 ---- batch: 110 ----
mean loss: 197.33
train mean loss: 201.63
epoch train time: 0:00:02.507366
elapsed time: 0:07:18.644850
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 20:44:45.683821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.96
 ---- batch: 020 ----
mean loss: 204.42
 ---- batch: 030 ----
mean loss: 203.79
 ---- batch: 040 ----
mean loss: 204.56
 ---- batch: 050 ----
mean loss: 213.42
 ---- batch: 060 ----
mean loss: 200.43
 ---- batch: 070 ----
mean loss: 198.75
 ---- batch: 080 ----
mean loss: 198.21
 ---- batch: 090 ----
mean loss: 199.27
 ---- batch: 100 ----
mean loss: 206.77
 ---- batch: 110 ----
mean loss: 200.45
train mean loss: 201.40
epoch train time: 0:00:02.520620
elapsed time: 0:07:21.165943
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 20:44:48.204895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.84
 ---- batch: 020 ----
mean loss: 207.37
 ---- batch: 030 ----
mean loss: 205.78
 ---- batch: 040 ----
mean loss: 194.56
 ---- batch: 050 ----
mean loss: 212.49
 ---- batch: 060 ----
mean loss: 201.55
 ---- batch: 070 ----
mean loss: 194.41
 ---- batch: 080 ----
mean loss: 187.96
 ---- batch: 090 ----
mean loss: 196.58
 ---- batch: 100 ----
mean loss: 202.74
 ---- batch: 110 ----
mean loss: 207.00
train mean loss: 201.42
epoch train time: 0:00:02.515599
elapsed time: 0:07:23.681973
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 20:44:50.720909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.19
 ---- batch: 020 ----
mean loss: 202.76
 ---- batch: 030 ----
mean loss: 207.83
 ---- batch: 040 ----
mean loss: 210.68
 ---- batch: 050 ----
mean loss: 200.03
 ---- batch: 060 ----
mean loss: 200.11
 ---- batch: 070 ----
mean loss: 201.58
 ---- batch: 080 ----
mean loss: 199.01
 ---- batch: 090 ----
mean loss: 190.38
 ---- batch: 100 ----
mean loss: 190.81
 ---- batch: 110 ----
mean loss: 201.66
train mean loss: 200.80
epoch train time: 0:00:02.480045
elapsed time: 0:07:26.162428
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 20:44:53.201383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.30
 ---- batch: 020 ----
mean loss: 206.03
 ---- batch: 030 ----
mean loss: 199.11
 ---- batch: 040 ----
mean loss: 201.00
 ---- batch: 050 ----
mean loss: 204.34
 ---- batch: 060 ----
mean loss: 195.98
 ---- batch: 070 ----
mean loss: 214.37
 ---- batch: 080 ----
mean loss: 198.07
 ---- batch: 090 ----
mean loss: 191.16
 ---- batch: 100 ----
mean loss: 200.60
 ---- batch: 110 ----
mean loss: 200.59
train mean loss: 200.79
epoch train time: 0:00:02.505290
elapsed time: 0:07:28.668194
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 20:44:55.707157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.45
 ---- batch: 020 ----
mean loss: 193.32
 ---- batch: 030 ----
mean loss: 204.58
 ---- batch: 040 ----
mean loss: 200.00
 ---- batch: 050 ----
mean loss: 213.71
 ---- batch: 060 ----
mean loss: 199.08
 ---- batch: 070 ----
mean loss: 197.89
 ---- batch: 080 ----
mean loss: 203.55
 ---- batch: 090 ----
mean loss: 202.56
 ---- batch: 100 ----
mean loss: 195.09
 ---- batch: 110 ----
mean loss: 191.10
train mean loss: 200.49
epoch train time: 0:00:02.526204
elapsed time: 0:07:31.194884
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 20:44:58.233864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.21
 ---- batch: 020 ----
mean loss: 202.40
 ---- batch: 030 ----
mean loss: 195.97
 ---- batch: 040 ----
mean loss: 195.95
 ---- batch: 050 ----
mean loss: 191.17
 ---- batch: 060 ----
mean loss: 202.57
 ---- batch: 070 ----
mean loss: 189.05
 ---- batch: 080 ----
mean loss: 210.37
 ---- batch: 090 ----
mean loss: 204.05
 ---- batch: 100 ----
mean loss: 215.88
 ---- batch: 110 ----
mean loss: 194.09
train mean loss: 200.32
epoch train time: 0:00:02.498409
elapsed time: 0:07:33.693768
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 20:45:00.732742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.46
 ---- batch: 020 ----
mean loss: 197.62
 ---- batch: 030 ----
mean loss: 197.98
 ---- batch: 040 ----
mean loss: 192.37
 ---- batch: 050 ----
mean loss: 204.11
 ---- batch: 060 ----
mean loss: 196.69
 ---- batch: 070 ----
mean loss: 196.28
 ---- batch: 080 ----
mean loss: 195.11
 ---- batch: 090 ----
mean loss: 208.84
 ---- batch: 100 ----
mean loss: 204.87
 ---- batch: 110 ----
mean loss: 204.97
train mean loss: 199.93
epoch train time: 0:00:02.485607
elapsed time: 0:07:36.179820
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 20:45:03.218746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.57
 ---- batch: 020 ----
mean loss: 208.41
 ---- batch: 030 ----
mean loss: 200.94
 ---- batch: 040 ----
mean loss: 210.44
 ---- batch: 050 ----
mean loss: 208.23
 ---- batch: 060 ----
mean loss: 195.97
 ---- batch: 070 ----
mean loss: 190.88
 ---- batch: 080 ----
mean loss: 197.07
 ---- batch: 090 ----
mean loss: 191.86
 ---- batch: 100 ----
mean loss: 194.95
 ---- batch: 110 ----
mean loss: 196.80
train mean loss: 199.90
epoch train time: 0:00:02.490988
elapsed time: 0:07:38.671257
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 20:45:05.710210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.10
 ---- batch: 020 ----
mean loss: 201.92
 ---- batch: 030 ----
mean loss: 188.60
 ---- batch: 040 ----
mean loss: 208.79
 ---- batch: 050 ----
mean loss: 205.24
 ---- batch: 060 ----
mean loss: 202.22
 ---- batch: 070 ----
mean loss: 200.33
 ---- batch: 080 ----
mean loss: 205.87
 ---- batch: 090 ----
mean loss: 193.51
 ---- batch: 100 ----
mean loss: 202.13
 ---- batch: 110 ----
mean loss: 192.15
train mean loss: 199.76
epoch train time: 0:00:02.513960
elapsed time: 0:07:41.185651
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 20:45:08.224616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.78
 ---- batch: 020 ----
mean loss: 206.33
 ---- batch: 030 ----
mean loss: 204.44
 ---- batch: 040 ----
mean loss: 196.57
 ---- batch: 050 ----
mean loss: 195.06
 ---- batch: 060 ----
mean loss: 203.41
 ---- batch: 070 ----
mean loss: 195.43
 ---- batch: 080 ----
mean loss: 201.82
 ---- batch: 090 ----
mean loss: 202.82
 ---- batch: 100 ----
mean loss: 192.62
 ---- batch: 110 ----
mean loss: 193.81
train mean loss: 199.45
epoch train time: 0:00:02.475654
elapsed time: 0:07:43.661736
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 20:45:10.700710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.68
 ---- batch: 020 ----
mean loss: 207.95
 ---- batch: 030 ----
mean loss: 204.34
 ---- batch: 040 ----
mean loss: 197.95
 ---- batch: 050 ----
mean loss: 203.24
 ---- batch: 060 ----
mean loss: 191.48
 ---- batch: 070 ----
mean loss: 205.37
 ---- batch: 080 ----
mean loss: 198.06
 ---- batch: 090 ----
mean loss: 200.59
 ---- batch: 100 ----
mean loss: 194.24
 ---- batch: 110 ----
mean loss: 196.01
train mean loss: 199.11
epoch train time: 0:00:02.488550
elapsed time: 0:07:46.150763
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 20:45:13.189737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.31
 ---- batch: 020 ----
mean loss: 205.22
 ---- batch: 030 ----
mean loss: 197.64
 ---- batch: 040 ----
mean loss: 191.20
 ---- batch: 050 ----
mean loss: 197.76
 ---- batch: 060 ----
mean loss: 200.43
 ---- batch: 070 ----
mean loss: 189.36
 ---- batch: 080 ----
mean loss: 201.94
 ---- batch: 090 ----
mean loss: 206.92
 ---- batch: 100 ----
mean loss: 199.95
 ---- batch: 110 ----
mean loss: 203.04
train mean loss: 198.95
epoch train time: 0:00:02.516346
elapsed time: 0:07:48.667582
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 20:45:15.706514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.48
 ---- batch: 020 ----
mean loss: 198.74
 ---- batch: 030 ----
mean loss: 206.03
 ---- batch: 040 ----
mean loss: 201.20
 ---- batch: 050 ----
mean loss: 191.25
 ---- batch: 060 ----
mean loss: 197.32
 ---- batch: 070 ----
mean loss: 198.62
 ---- batch: 080 ----
mean loss: 196.05
 ---- batch: 090 ----
mean loss: 207.05
 ---- batch: 100 ----
mean loss: 189.00
 ---- batch: 110 ----
mean loss: 202.37
train mean loss: 198.85
epoch train time: 0:00:02.511470
elapsed time: 0:07:51.179592
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 20:45:18.218443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.92
 ---- batch: 020 ----
mean loss: 204.67
 ---- batch: 030 ----
mean loss: 197.15
 ---- batch: 040 ----
mean loss: 198.30
 ---- batch: 050 ----
mean loss: 204.90
 ---- batch: 060 ----
mean loss: 196.21
 ---- batch: 070 ----
mean loss: 197.64
 ---- batch: 080 ----
mean loss: 192.84
 ---- batch: 090 ----
mean loss: 195.98
 ---- batch: 100 ----
mean loss: 201.71
 ---- batch: 110 ----
mean loss: 197.59
train mean loss: 198.47
epoch train time: 0:00:02.509345
elapsed time: 0:07:53.689250
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 20:45:20.728254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.19
 ---- batch: 020 ----
mean loss: 198.54
 ---- batch: 030 ----
mean loss: 193.31
 ---- batch: 040 ----
mean loss: 196.54
 ---- batch: 050 ----
mean loss: 193.24
 ---- batch: 060 ----
mean loss: 198.10
 ---- batch: 070 ----
mean loss: 203.13
 ---- batch: 080 ----
mean loss: 192.85
 ---- batch: 090 ----
mean loss: 205.75
 ---- batch: 100 ----
mean loss: 200.73
 ---- batch: 110 ----
mean loss: 195.38
train mean loss: 198.61
epoch train time: 0:00:02.494780
elapsed time: 0:07:56.184510
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 20:45:23.223485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.52
 ---- batch: 020 ----
mean loss: 197.18
 ---- batch: 030 ----
mean loss: 186.67
 ---- batch: 040 ----
mean loss: 209.84
 ---- batch: 050 ----
mean loss: 201.93
 ---- batch: 060 ----
mean loss: 199.98
 ---- batch: 070 ----
mean loss: 199.42
 ---- batch: 080 ----
mean loss: 202.58
 ---- batch: 090 ----
mean loss: 192.46
 ---- batch: 100 ----
mean loss: 195.79
 ---- batch: 110 ----
mean loss: 194.62
train mean loss: 198.26
epoch train time: 0:00:02.479959
elapsed time: 0:07:58.664977
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 20:45:25.703951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.76
 ---- batch: 020 ----
mean loss: 209.54
 ---- batch: 030 ----
mean loss: 204.44
 ---- batch: 040 ----
mean loss: 192.31
 ---- batch: 050 ----
mean loss: 198.64
 ---- batch: 060 ----
mean loss: 194.58
 ---- batch: 070 ----
mean loss: 186.89
 ---- batch: 080 ----
mean loss: 198.79
 ---- batch: 090 ----
mean loss: 200.13
 ---- batch: 100 ----
mean loss: 197.04
 ---- batch: 110 ----
mean loss: 190.84
train mean loss: 198.01
epoch train time: 0:00:02.502134
elapsed time: 0:08:01.167545
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 20:45:28.206473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.99
 ---- batch: 020 ----
mean loss: 205.49
 ---- batch: 030 ----
mean loss: 197.29
 ---- batch: 040 ----
mean loss: 193.48
 ---- batch: 050 ----
mean loss: 193.95
 ---- batch: 060 ----
mean loss: 200.39
 ---- batch: 070 ----
mean loss: 196.72
 ---- batch: 080 ----
mean loss: 192.88
 ---- batch: 090 ----
mean loss: 200.69
 ---- batch: 100 ----
mean loss: 196.74
 ---- batch: 110 ----
mean loss: 201.92
train mean loss: 198.05
epoch train time: 0:00:02.507242
elapsed time: 0:08:03.675188
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 20:45:30.714128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.31
 ---- batch: 020 ----
mean loss: 190.52
 ---- batch: 030 ----
mean loss: 199.98
 ---- batch: 040 ----
mean loss: 202.32
 ---- batch: 050 ----
mean loss: 205.02
 ---- batch: 060 ----
mean loss: 192.41
 ---- batch: 070 ----
mean loss: 201.81
 ---- batch: 080 ----
mean loss: 198.29
 ---- batch: 090 ----
mean loss: 197.72
 ---- batch: 100 ----
mean loss: 193.92
 ---- batch: 110 ----
mean loss: 203.93
train mean loss: 197.92
epoch train time: 0:00:02.502560
elapsed time: 0:08:06.178140
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 20:45:33.217094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.39
 ---- batch: 020 ----
mean loss: 206.83
 ---- batch: 030 ----
mean loss: 191.79
 ---- batch: 040 ----
mean loss: 189.46
 ---- batch: 050 ----
mean loss: 206.06
 ---- batch: 060 ----
mean loss: 202.56
 ---- batch: 070 ----
mean loss: 198.16
 ---- batch: 080 ----
mean loss: 197.81
 ---- batch: 090 ----
mean loss: 190.58
 ---- batch: 100 ----
mean loss: 199.29
 ---- batch: 110 ----
mean loss: 201.30
train mean loss: 197.65
epoch train time: 0:00:02.475363
elapsed time: 0:08:08.653924
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 20:45:35.692858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.60
 ---- batch: 020 ----
mean loss: 198.86
 ---- batch: 030 ----
mean loss: 211.58
 ---- batch: 040 ----
mean loss: 186.84
 ---- batch: 050 ----
mean loss: 193.39
 ---- batch: 060 ----
mean loss: 189.15
 ---- batch: 070 ----
mean loss: 192.26
 ---- batch: 080 ----
mean loss: 199.60
 ---- batch: 090 ----
mean loss: 199.50
 ---- batch: 100 ----
mean loss: 202.44
 ---- batch: 110 ----
mean loss: 199.88
train mean loss: 197.64
epoch train time: 0:00:02.495751
elapsed time: 0:08:11.150073
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 20:45:38.189000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.43
 ---- batch: 020 ----
mean loss: 202.59
 ---- batch: 030 ----
mean loss: 199.28
 ---- batch: 040 ----
mean loss: 202.40
 ---- batch: 050 ----
mean loss: 194.62
 ---- batch: 060 ----
mean loss: 200.85
 ---- batch: 070 ----
mean loss: 198.26
 ---- batch: 080 ----
mean loss: 192.32
 ---- batch: 090 ----
mean loss: 198.44
 ---- batch: 100 ----
mean loss: 191.82
 ---- batch: 110 ----
mean loss: 189.73
train mean loss: 197.40
epoch train time: 0:00:02.513725
elapsed time: 0:08:13.664185
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 20:45:40.703105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.85
 ---- batch: 020 ----
mean loss: 198.88
 ---- batch: 030 ----
mean loss: 189.52
 ---- batch: 040 ----
mean loss: 202.90
 ---- batch: 050 ----
mean loss: 185.05
 ---- batch: 060 ----
mean loss: 195.72
 ---- batch: 070 ----
mean loss: 197.75
 ---- batch: 080 ----
mean loss: 199.09
 ---- batch: 090 ----
mean loss: 209.46
 ---- batch: 100 ----
mean loss: 200.39
 ---- batch: 110 ----
mean loss: 191.73
train mean loss: 197.10
epoch train time: 0:00:02.494874
elapsed time: 0:08:16.159440
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 20:45:43.198362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.62
 ---- batch: 020 ----
mean loss: 200.26
 ---- batch: 030 ----
mean loss: 199.70
 ---- batch: 040 ----
mean loss: 195.72
 ---- batch: 050 ----
mean loss: 200.72
 ---- batch: 060 ----
mean loss: 200.78
 ---- batch: 070 ----
mean loss: 195.99
 ---- batch: 080 ----
mean loss: 195.63
 ---- batch: 090 ----
mean loss: 190.02
 ---- batch: 100 ----
mean loss: 191.64
 ---- batch: 110 ----
mean loss: 202.62
train mean loss: 196.74
epoch train time: 0:00:02.518733
elapsed time: 0:08:18.678566
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 20:45:45.717496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.22
 ---- batch: 020 ----
mean loss: 187.01
 ---- batch: 030 ----
mean loss: 204.08
 ---- batch: 040 ----
mean loss: 194.89
 ---- batch: 050 ----
mean loss: 199.13
 ---- batch: 060 ----
mean loss: 197.65
 ---- batch: 070 ----
mean loss: 197.33
 ---- batch: 080 ----
mean loss: 192.77
 ---- batch: 090 ----
mean loss: 193.94
 ---- batch: 100 ----
mean loss: 202.87
 ---- batch: 110 ----
mean loss: 201.14
train mean loss: 196.93
epoch train time: 0:00:02.474551
elapsed time: 0:08:21.153520
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 20:45:48.192469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.67
 ---- batch: 020 ----
mean loss: 206.80
 ---- batch: 030 ----
mean loss: 199.27
 ---- batch: 040 ----
mean loss: 193.14
 ---- batch: 050 ----
mean loss: 199.29
 ---- batch: 060 ----
mean loss: 194.86
 ---- batch: 070 ----
mean loss: 198.23
 ---- batch: 080 ----
mean loss: 194.46
 ---- batch: 090 ----
mean loss: 189.37
 ---- batch: 100 ----
mean loss: 192.07
 ---- batch: 110 ----
mean loss: 195.68
train mean loss: 196.77
epoch train time: 0:00:02.486861
elapsed time: 0:08:23.640804
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 20:45:50.679783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.71
 ---- batch: 020 ----
mean loss: 192.09
 ---- batch: 030 ----
mean loss: 196.91
 ---- batch: 040 ----
mean loss: 200.43
 ---- batch: 050 ----
mean loss: 200.91
 ---- batch: 060 ----
mean loss: 206.21
 ---- batch: 070 ----
mean loss: 193.29
 ---- batch: 080 ----
mean loss: 190.88
 ---- batch: 090 ----
mean loss: 202.53
 ---- batch: 100 ----
mean loss: 195.08
 ---- batch: 110 ----
mean loss: 196.48
train mean loss: 196.63
epoch train time: 0:00:02.532128
elapsed time: 0:08:26.173378
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 20:45:53.212306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.82
 ---- batch: 020 ----
mean loss: 198.51
 ---- batch: 030 ----
mean loss: 206.40
 ---- batch: 040 ----
mean loss: 189.09
 ---- batch: 050 ----
mean loss: 197.59
 ---- batch: 060 ----
mean loss: 193.29
 ---- batch: 070 ----
mean loss: 198.24
 ---- batch: 080 ----
mean loss: 191.32
 ---- batch: 090 ----
mean loss: 197.27
 ---- batch: 100 ----
mean loss: 196.40
 ---- batch: 110 ----
mean loss: 192.98
train mean loss: 196.40
epoch train time: 0:00:02.561174
elapsed time: 0:08:28.734975
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 20:45:55.773913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.69
 ---- batch: 020 ----
mean loss: 197.26
 ---- batch: 030 ----
mean loss: 205.89
 ---- batch: 040 ----
mean loss: 197.00
 ---- batch: 050 ----
mean loss: 196.28
 ---- batch: 060 ----
mean loss: 196.35
 ---- batch: 070 ----
mean loss: 192.75
 ---- batch: 080 ----
mean loss: 202.73
 ---- batch: 090 ----
mean loss: 197.49
 ---- batch: 100 ----
mean loss: 186.96
 ---- batch: 110 ----
mean loss: 191.31
train mean loss: 196.29
epoch train time: 0:00:02.552587
elapsed time: 0:08:31.288017
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 20:45:58.326953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.29
 ---- batch: 020 ----
mean loss: 195.08
 ---- batch: 030 ----
mean loss: 204.35
 ---- batch: 040 ----
mean loss: 194.31
 ---- batch: 050 ----
mean loss: 190.32
 ---- batch: 060 ----
mean loss: 201.04
 ---- batch: 070 ----
mean loss: 203.51
 ---- batch: 080 ----
mean loss: 192.40
 ---- batch: 090 ----
mean loss: 191.00
 ---- batch: 100 ----
mean loss: 197.37
 ---- batch: 110 ----
mean loss: 203.00
train mean loss: 196.36
epoch train time: 0:00:02.543016
elapsed time: 0:08:33.831488
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 20:46:00.870494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.61
 ---- batch: 020 ----
mean loss: 188.00
 ---- batch: 030 ----
mean loss: 196.23
 ---- batch: 040 ----
mean loss: 197.79
 ---- batch: 050 ----
mean loss: 204.27
 ---- batch: 060 ----
mean loss: 199.70
 ---- batch: 070 ----
mean loss: 193.52
 ---- batch: 080 ----
mean loss: 190.57
 ---- batch: 090 ----
mean loss: 200.25
 ---- batch: 100 ----
mean loss: 199.51
 ---- batch: 110 ----
mean loss: 192.55
train mean loss: 196.11
epoch train time: 0:00:02.545689
elapsed time: 0:08:36.377643
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 20:46:03.416603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.82
 ---- batch: 020 ----
mean loss: 193.28
 ---- batch: 030 ----
mean loss: 192.93
 ---- batch: 040 ----
mean loss: 197.01
 ---- batch: 050 ----
mean loss: 203.21
 ---- batch: 060 ----
mean loss: 205.58
 ---- batch: 070 ----
mean loss: 190.54
 ---- batch: 080 ----
mean loss: 194.31
 ---- batch: 090 ----
mean loss: 192.57
 ---- batch: 100 ----
mean loss: 188.08
 ---- batch: 110 ----
mean loss: 196.14
train mean loss: 196.03
epoch train time: 0:00:02.484181
elapsed time: 0:08:38.862242
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 20:46:05.901180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.44
 ---- batch: 020 ----
mean loss: 199.83
 ---- batch: 030 ----
mean loss: 194.11
 ---- batch: 040 ----
mean loss: 198.69
 ---- batch: 050 ----
mean loss: 201.66
 ---- batch: 060 ----
mean loss: 188.65
 ---- batch: 070 ----
mean loss: 186.71
 ---- batch: 080 ----
mean loss: 194.29
 ---- batch: 090 ----
mean loss: 186.40
 ---- batch: 100 ----
mean loss: 203.29
 ---- batch: 110 ----
mean loss: 202.58
train mean loss: 195.66
epoch train time: 0:00:02.481181
elapsed time: 0:08:41.343885
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 20:46:08.382840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.49
 ---- batch: 020 ----
mean loss: 205.30
 ---- batch: 030 ----
mean loss: 201.53
 ---- batch: 040 ----
mean loss: 193.12
 ---- batch: 050 ----
mean loss: 185.95
 ---- batch: 060 ----
mean loss: 192.43
 ---- batch: 070 ----
mean loss: 206.92
 ---- batch: 080 ----
mean loss: 194.33
 ---- batch: 090 ----
mean loss: 197.02
 ---- batch: 100 ----
mean loss: 199.87
 ---- batch: 110 ----
mean loss: 186.08
train mean loss: 195.65
epoch train time: 0:00:02.515444
elapsed time: 0:08:43.859741
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 20:46:10.898689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.39
 ---- batch: 020 ----
mean loss: 201.50
 ---- batch: 030 ----
mean loss: 205.63
 ---- batch: 040 ----
mean loss: 195.73
 ---- batch: 050 ----
mean loss: 187.81
 ---- batch: 060 ----
mean loss: 191.58
 ---- batch: 070 ----
mean loss: 194.84
 ---- batch: 080 ----
mean loss: 191.71
 ---- batch: 090 ----
mean loss: 199.33
 ---- batch: 100 ----
mean loss: 195.73
 ---- batch: 110 ----
mean loss: 199.10
train mean loss: 195.54
epoch train time: 0:00:02.501324
elapsed time: 0:08:46.361478
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 20:46:13.400409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.67
 ---- batch: 020 ----
mean loss: 197.88
 ---- batch: 030 ----
mean loss: 190.51
 ---- batch: 040 ----
mean loss: 205.81
 ---- batch: 050 ----
mean loss: 190.75
 ---- batch: 060 ----
mean loss: 194.84
 ---- batch: 070 ----
mean loss: 195.30
 ---- batch: 080 ----
mean loss: 195.28
 ---- batch: 090 ----
mean loss: 189.74
 ---- batch: 100 ----
mean loss: 184.42
 ---- batch: 110 ----
mean loss: 204.27
train mean loss: 195.28
epoch train time: 0:00:02.496102
elapsed time: 0:08:48.857991
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 20:46:15.896925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.46
 ---- batch: 020 ----
mean loss: 192.23
 ---- batch: 030 ----
mean loss: 200.33
 ---- batch: 040 ----
mean loss: 188.74
 ---- batch: 050 ----
mean loss: 200.32
 ---- batch: 060 ----
mean loss: 188.20
 ---- batch: 070 ----
mean loss: 209.35
 ---- batch: 080 ----
mean loss: 201.88
 ---- batch: 090 ----
mean loss: 190.55
 ---- batch: 100 ----
mean loss: 192.36
 ---- batch: 110 ----
mean loss: 190.96
train mean loss: 195.21
epoch train time: 0:00:02.444588
elapsed time: 0:08:51.302991
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 20:46:18.341921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.05
 ---- batch: 020 ----
mean loss: 192.00
 ---- batch: 030 ----
mean loss: 193.45
 ---- batch: 040 ----
mean loss: 194.81
 ---- batch: 050 ----
mean loss: 193.10
 ---- batch: 060 ----
mean loss: 201.64
 ---- batch: 070 ----
mean loss: 187.04
 ---- batch: 080 ----
mean loss: 191.21
 ---- batch: 090 ----
mean loss: 194.57
 ---- batch: 100 ----
mean loss: 184.66
 ---- batch: 110 ----
mean loss: 203.29
train mean loss: 195.15
epoch train time: 0:00:02.480889
elapsed time: 0:08:53.784382
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 20:46:20.824308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.26
 ---- batch: 020 ----
mean loss: 189.41
 ---- batch: 030 ----
mean loss: 204.63
 ---- batch: 040 ----
mean loss: 186.34
 ---- batch: 050 ----
mean loss: 197.01
 ---- batch: 060 ----
mean loss: 205.95
 ---- batch: 070 ----
mean loss: 194.86
 ---- batch: 080 ----
mean loss: 197.69
 ---- batch: 090 ----
mean loss: 185.28
 ---- batch: 100 ----
mean loss: 190.46
 ---- batch: 110 ----
mean loss: 196.70
train mean loss: 194.97
epoch train time: 0:00:02.505342
elapsed time: 0:08:56.291116
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 20:46:23.330076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.93
 ---- batch: 020 ----
mean loss: 189.06
 ---- batch: 030 ----
mean loss: 190.44
 ---- batch: 040 ----
mean loss: 189.73
 ---- batch: 050 ----
mean loss: 198.96
 ---- batch: 060 ----
mean loss: 200.59
 ---- batch: 070 ----
mean loss: 197.52
 ---- batch: 080 ----
mean loss: 185.90
 ---- batch: 090 ----
mean loss: 194.28
 ---- batch: 100 ----
mean loss: 196.97
 ---- batch: 110 ----
mean loss: 206.18
train mean loss: 195.05
epoch train time: 0:00:02.514152
elapsed time: 0:08:58.805741
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 20:46:25.844760
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.82
 ---- batch: 020 ----
mean loss: 191.86
 ---- batch: 030 ----
mean loss: 203.14
 ---- batch: 040 ----
mean loss: 196.44
 ---- batch: 050 ----
mean loss: 190.55
 ---- batch: 060 ----
mean loss: 195.37
 ---- batch: 070 ----
mean loss: 189.35
 ---- batch: 080 ----
mean loss: 195.11
 ---- batch: 090 ----
mean loss: 193.87
 ---- batch: 100 ----
mean loss: 187.38
 ---- batch: 110 ----
mean loss: 196.85
train mean loss: 194.42
epoch train time: 0:00:02.503422
elapsed time: 0:09:01.309850
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 20:46:28.348606
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.00
 ---- batch: 020 ----
mean loss: 187.75
 ---- batch: 030 ----
mean loss: 188.18
 ---- batch: 040 ----
mean loss: 198.90
 ---- batch: 050 ----
mean loss: 194.20
 ---- batch: 060 ----
mean loss: 196.32
 ---- batch: 070 ----
mean loss: 187.31
 ---- batch: 080 ----
mean loss: 203.97
 ---- batch: 090 ----
mean loss: 199.92
 ---- batch: 100 ----
mean loss: 190.70
 ---- batch: 110 ----
mean loss: 190.89
train mean loss: 194.38
epoch train time: 0:00:02.508533
elapsed time: 0:09:03.818623
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 20:46:30.857581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.00
 ---- batch: 020 ----
mean loss: 197.68
 ---- batch: 030 ----
mean loss: 196.46
 ---- batch: 040 ----
mean loss: 195.88
 ---- batch: 050 ----
mean loss: 197.82
 ---- batch: 060 ----
mean loss: 194.04
 ---- batch: 070 ----
mean loss: 187.78
 ---- batch: 080 ----
mean loss: 201.76
 ---- batch: 090 ----
mean loss: 195.74
 ---- batch: 100 ----
mean loss: 190.85
 ---- batch: 110 ----
mean loss: 188.24
train mean loss: 194.28
epoch train time: 0:00:02.508129
elapsed time: 0:09:06.327188
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 20:46:33.366130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.78
 ---- batch: 020 ----
mean loss: 195.31
 ---- batch: 030 ----
mean loss: 188.46
 ---- batch: 040 ----
mean loss: 192.31
 ---- batch: 050 ----
mean loss: 197.33
 ---- batch: 060 ----
mean loss: 193.33
 ---- batch: 070 ----
mean loss: 198.98
 ---- batch: 080 ----
mean loss: 198.17
 ---- batch: 090 ----
mean loss: 198.60
 ---- batch: 100 ----
mean loss: 190.35
 ---- batch: 110 ----
mean loss: 190.78
train mean loss: 194.23
epoch train time: 0:00:02.512606
elapsed time: 0:09:08.840238
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 20:46:35.879181
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.80
 ---- batch: 020 ----
mean loss: 190.47
 ---- batch: 030 ----
mean loss: 195.09
 ---- batch: 040 ----
mean loss: 189.90
 ---- batch: 050 ----
mean loss: 195.86
 ---- batch: 060 ----
mean loss: 200.43
 ---- batch: 070 ----
mean loss: 195.58
 ---- batch: 080 ----
mean loss: 199.10
 ---- batch: 090 ----
mean loss: 188.13
 ---- batch: 100 ----
mean loss: 187.70
 ---- batch: 110 ----
mean loss: 201.03
train mean loss: 194.25
epoch train time: 0:00:02.460535
elapsed time: 0:09:11.301220
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 20:46:38.340212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.96
 ---- batch: 020 ----
mean loss: 191.89
 ---- batch: 030 ----
mean loss: 194.46
 ---- batch: 040 ----
mean loss: 196.00
 ---- batch: 050 ----
mean loss: 189.04
 ---- batch: 060 ----
mean loss: 197.49
 ---- batch: 070 ----
mean loss: 197.31
 ---- batch: 080 ----
mean loss: 196.95
 ---- batch: 090 ----
mean loss: 192.87
 ---- batch: 100 ----
mean loss: 185.31
 ---- batch: 110 ----
mean loss: 200.63
train mean loss: 194.34
epoch train time: 0:00:02.472797
elapsed time: 0:09:13.774502
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 20:46:40.813456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.49
 ---- batch: 020 ----
mean loss: 194.29
 ---- batch: 030 ----
mean loss: 196.71
 ---- batch: 040 ----
mean loss: 206.27
 ---- batch: 050 ----
mean loss: 183.18
 ---- batch: 060 ----
mean loss: 194.48
 ---- batch: 070 ----
mean loss: 185.95
 ---- batch: 080 ----
mean loss: 196.18
 ---- batch: 090 ----
mean loss: 190.93
 ---- batch: 100 ----
mean loss: 205.48
 ---- batch: 110 ----
mean loss: 192.33
train mean loss: 194.36
epoch train time: 0:00:02.495769
elapsed time: 0:09:16.270703
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 20:46:43.309728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.04
 ---- batch: 020 ----
mean loss: 192.80
 ---- batch: 030 ----
mean loss: 188.89
 ---- batch: 040 ----
mean loss: 189.42
 ---- batch: 050 ----
mean loss: 195.58
 ---- batch: 060 ----
mean loss: 202.86
 ---- batch: 070 ----
mean loss: 201.60
 ---- batch: 080 ----
mean loss: 200.88
 ---- batch: 090 ----
mean loss: 192.91
 ---- batch: 100 ----
mean loss: 188.30
 ---- batch: 110 ----
mean loss: 196.04
train mean loss: 194.34
epoch train time: 0:00:02.490079
elapsed time: 0:09:18.761289
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 20:46:45.800223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.80
 ---- batch: 020 ----
mean loss: 177.47
 ---- batch: 030 ----
mean loss: 200.59
 ---- batch: 040 ----
mean loss: 187.49
 ---- batch: 050 ----
mean loss: 191.57
 ---- batch: 060 ----
mean loss: 201.87
 ---- batch: 070 ----
mean loss: 193.79
 ---- batch: 080 ----
mean loss: 190.22
 ---- batch: 090 ----
mean loss: 194.99
 ---- batch: 100 ----
mean loss: 196.33
 ---- batch: 110 ----
mean loss: 207.98
train mean loss: 194.23
epoch train time: 0:00:02.485443
elapsed time: 0:09:21.247132
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 20:46:48.286092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.80
 ---- batch: 020 ----
mean loss: 191.07
 ---- batch: 030 ----
mean loss: 194.81
 ---- batch: 040 ----
mean loss: 201.21
 ---- batch: 050 ----
mean loss: 198.12
 ---- batch: 060 ----
mean loss: 200.31
 ---- batch: 070 ----
mean loss: 190.48
 ---- batch: 080 ----
mean loss: 188.04
 ---- batch: 090 ----
mean loss: 181.68
 ---- batch: 100 ----
mean loss: 188.75
 ---- batch: 110 ----
mean loss: 193.59
train mean loss: 194.27
epoch train time: 0:00:02.507406
elapsed time: 0:09:23.754998
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 20:46:50.793957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.35
 ---- batch: 020 ----
mean loss: 201.08
 ---- batch: 030 ----
mean loss: 196.20
 ---- batch: 040 ----
mean loss: 197.79
 ---- batch: 050 ----
mean loss: 201.28
 ---- batch: 060 ----
mean loss: 194.69
 ---- batch: 070 ----
mean loss: 186.94
 ---- batch: 080 ----
mean loss: 187.45
 ---- batch: 090 ----
mean loss: 201.60
 ---- batch: 100 ----
mean loss: 184.45
 ---- batch: 110 ----
mean loss: 190.15
train mean loss: 194.19
epoch train time: 0:00:02.522251
elapsed time: 0:09:26.277681
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 20:46:53.316648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.39
 ---- batch: 020 ----
mean loss: 191.26
 ---- batch: 030 ----
mean loss: 193.57
 ---- batch: 040 ----
mean loss: 197.07
 ---- batch: 050 ----
mean loss: 198.88
 ---- batch: 060 ----
mean loss: 198.67
 ---- batch: 070 ----
mean loss: 198.79
 ---- batch: 080 ----
mean loss: 190.75
 ---- batch: 090 ----
mean loss: 193.37
 ---- batch: 100 ----
mean loss: 194.40
 ---- batch: 110 ----
mean loss: 192.88
train mean loss: 194.12
epoch train time: 0:00:02.497400
elapsed time: 0:09:28.775545
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 20:46:55.814526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.58
 ---- batch: 020 ----
mean loss: 192.39
 ---- batch: 030 ----
mean loss: 184.55
 ---- batch: 040 ----
mean loss: 200.19
 ---- batch: 050 ----
mean loss: 196.05
 ---- batch: 060 ----
mean loss: 199.04
 ---- batch: 070 ----
mean loss: 190.24
 ---- batch: 080 ----
mean loss: 191.82
 ---- batch: 090 ----
mean loss: 185.36
 ---- batch: 100 ----
mean loss: 197.51
 ---- batch: 110 ----
mean loss: 201.74
train mean loss: 194.16
epoch train time: 0:00:02.482974
elapsed time: 0:09:31.258991
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 20:46:58.297922
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.43
 ---- batch: 020 ----
mean loss: 194.61
 ---- batch: 030 ----
mean loss: 195.53
 ---- batch: 040 ----
mean loss: 185.00
 ---- batch: 050 ----
mean loss: 199.06
 ---- batch: 060 ----
mean loss: 191.11
 ---- batch: 070 ----
mean loss: 203.31
 ---- batch: 080 ----
mean loss: 195.16
 ---- batch: 090 ----
mean loss: 190.49
 ---- batch: 100 ----
mean loss: 196.25
 ---- batch: 110 ----
mean loss: 196.61
train mean loss: 194.26
epoch train time: 0:00:02.500019
elapsed time: 0:09:33.759445
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 20:47:00.798405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.42
 ---- batch: 020 ----
mean loss: 200.92
 ---- batch: 030 ----
mean loss: 195.87
 ---- batch: 040 ----
mean loss: 201.31
 ---- batch: 050 ----
mean loss: 194.22
 ---- batch: 060 ----
mean loss: 188.90
 ---- batch: 070 ----
mean loss: 202.32
 ---- batch: 080 ----
mean loss: 182.76
 ---- batch: 090 ----
mean loss: 181.75
 ---- batch: 100 ----
mean loss: 188.75
 ---- batch: 110 ----
mean loss: 193.90
train mean loss: 194.20
epoch train time: 0:00:02.496234
elapsed time: 0:09:36.256106
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 20:47:03.295035
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.45
 ---- batch: 020 ----
mean loss: 193.17
 ---- batch: 030 ----
mean loss: 195.44
 ---- batch: 040 ----
mean loss: 194.15
 ---- batch: 050 ----
mean loss: 191.75
 ---- batch: 060 ----
mean loss: 193.80
 ---- batch: 070 ----
mean loss: 187.57
 ---- batch: 080 ----
mean loss: 198.72
 ---- batch: 090 ----
mean loss: 196.34
 ---- batch: 100 ----
mean loss: 197.08
 ---- batch: 110 ----
mean loss: 194.37
train mean loss: 194.17
epoch train time: 0:00:02.473508
elapsed time: 0:09:38.730036
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 20:47:05.769007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.09
 ---- batch: 020 ----
mean loss: 191.23
 ---- batch: 030 ----
mean loss: 193.38
 ---- batch: 040 ----
mean loss: 194.08
 ---- batch: 050 ----
mean loss: 190.67
 ---- batch: 060 ----
mean loss: 193.95
 ---- batch: 070 ----
mean loss: 192.22
 ---- batch: 080 ----
mean loss: 198.50
 ---- batch: 090 ----
mean loss: 193.78
 ---- batch: 100 ----
mean loss: 204.10
 ---- batch: 110 ----
mean loss: 194.46
train mean loss: 194.20
epoch train time: 0:00:02.498794
elapsed time: 0:09:41.229283
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 20:47:08.268225
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.38
 ---- batch: 020 ----
mean loss: 200.94
 ---- batch: 030 ----
mean loss: 203.29
 ---- batch: 040 ----
mean loss: 197.22
 ---- batch: 050 ----
mean loss: 191.19
 ---- batch: 060 ----
mean loss: 194.72
 ---- batch: 070 ----
mean loss: 193.28
 ---- batch: 080 ----
mean loss: 181.32
 ---- batch: 090 ----
mean loss: 193.41
 ---- batch: 100 ----
mean loss: 196.66
 ---- batch: 110 ----
mean loss: 190.34
train mean loss: 194.12
epoch train time: 0:00:02.528125
elapsed time: 0:09:43.757874
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 20:47:10.796818
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.15
 ---- batch: 020 ----
mean loss: 199.14
 ---- batch: 030 ----
mean loss: 187.83
 ---- batch: 040 ----
mean loss: 189.61
 ---- batch: 050 ----
mean loss: 195.63
 ---- batch: 060 ----
mean loss: 186.68
 ---- batch: 070 ----
mean loss: 192.49
 ---- batch: 080 ----
mean loss: 203.75
 ---- batch: 090 ----
mean loss: 193.12
 ---- batch: 100 ----
mean loss: 197.35
 ---- batch: 110 ----
mean loss: 196.68
train mean loss: 194.13
epoch train time: 0:00:02.522094
elapsed time: 0:09:46.280405
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 20:47:13.319371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.04
 ---- batch: 020 ----
mean loss: 200.79
 ---- batch: 030 ----
mean loss: 196.13
 ---- batch: 040 ----
mean loss: 193.18
 ---- batch: 050 ----
mean loss: 198.71
 ---- batch: 060 ----
mean loss: 191.42
 ---- batch: 070 ----
mean loss: 194.34
 ---- batch: 080 ----
mean loss: 195.17
 ---- batch: 090 ----
mean loss: 195.54
 ---- batch: 100 ----
mean loss: 198.63
 ---- batch: 110 ----
mean loss: 189.11
train mean loss: 194.04
epoch train time: 0:00:02.489251
elapsed time: 0:09:48.770094
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 20:47:15.809043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.17
 ---- batch: 020 ----
mean loss: 188.22
 ---- batch: 030 ----
mean loss: 197.88
 ---- batch: 040 ----
mean loss: 204.61
 ---- batch: 050 ----
mean loss: 187.20
 ---- batch: 060 ----
mean loss: 192.37
 ---- batch: 070 ----
mean loss: 199.98
 ---- batch: 080 ----
mean loss: 199.62
 ---- batch: 090 ----
mean loss: 191.25
 ---- batch: 100 ----
mean loss: 192.37
 ---- batch: 110 ----
mean loss: 195.44
train mean loss: 194.05
epoch train time: 0:00:02.504201
elapsed time: 0:09:51.274697
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 20:47:18.313667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.21
 ---- batch: 020 ----
mean loss: 197.59
 ---- batch: 030 ----
mean loss: 207.78
 ---- batch: 040 ----
mean loss: 187.12
 ---- batch: 050 ----
mean loss: 190.23
 ---- batch: 060 ----
mean loss: 192.25
 ---- batch: 070 ----
mean loss: 195.19
 ---- batch: 080 ----
mean loss: 189.22
 ---- batch: 090 ----
mean loss: 189.99
 ---- batch: 100 ----
mean loss: 196.81
 ---- batch: 110 ----
mean loss: 191.09
train mean loss: 194.15
epoch train time: 0:00:02.505512
elapsed time: 0:09:53.780665
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 20:47:20.819623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.00
 ---- batch: 020 ----
mean loss: 194.94
 ---- batch: 030 ----
mean loss: 188.62
 ---- batch: 040 ----
mean loss: 199.28
 ---- batch: 050 ----
mean loss: 201.62
 ---- batch: 060 ----
mean loss: 190.86
 ---- batch: 070 ----
mean loss: 192.85
 ---- batch: 080 ----
mean loss: 190.46
 ---- batch: 090 ----
mean loss: 198.07
 ---- batch: 100 ----
mean loss: 192.37
 ---- batch: 110 ----
mean loss: 193.77
train mean loss: 194.06
epoch train time: 0:00:02.488152
elapsed time: 0:09:56.269246
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 20:47:23.308184
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.53
 ---- batch: 020 ----
mean loss: 194.29
 ---- batch: 030 ----
mean loss: 194.62
 ---- batch: 040 ----
mean loss: 197.91
 ---- batch: 050 ----
mean loss: 190.71
 ---- batch: 060 ----
mean loss: 203.35
 ---- batch: 070 ----
mean loss: 192.18
 ---- batch: 080 ----
mean loss: 199.91
 ---- batch: 090 ----
mean loss: 198.15
 ---- batch: 100 ----
mean loss: 181.67
 ---- batch: 110 ----
mean loss: 188.89
train mean loss: 194.10
epoch train time: 0:00:02.490875
elapsed time: 0:09:58.760523
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 20:47:25.799507
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.01
 ---- batch: 020 ----
mean loss: 191.39
 ---- batch: 030 ----
mean loss: 198.44
 ---- batch: 040 ----
mean loss: 199.33
 ---- batch: 050 ----
mean loss: 181.73
 ---- batch: 060 ----
mean loss: 194.12
 ---- batch: 070 ----
mean loss: 187.74
 ---- batch: 080 ----
mean loss: 189.77
 ---- batch: 090 ----
mean loss: 199.31
 ---- batch: 100 ----
mean loss: 199.12
 ---- batch: 110 ----
mean loss: 192.96
train mean loss: 194.12
epoch train time: 0:00:02.503434
elapsed time: 0:10:01.264424
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 20:47:28.303384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.68
 ---- batch: 020 ----
mean loss: 198.78
 ---- batch: 030 ----
mean loss: 196.68
 ---- batch: 040 ----
mean loss: 191.56
 ---- batch: 050 ----
mean loss: 194.86
 ---- batch: 060 ----
mean loss: 182.81
 ---- batch: 070 ----
mean loss: 199.39
 ---- batch: 080 ----
mean loss: 189.70
 ---- batch: 090 ----
mean loss: 199.41
 ---- batch: 100 ----
mean loss: 198.12
 ---- batch: 110 ----
mean loss: 194.91
train mean loss: 193.98
epoch train time: 0:00:02.489276
elapsed time: 0:10:03.754121
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 20:47:30.793046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.96
 ---- batch: 020 ----
mean loss: 194.88
 ---- batch: 030 ----
mean loss: 194.58
 ---- batch: 040 ----
mean loss: 186.19
 ---- batch: 050 ----
mean loss: 192.13
 ---- batch: 060 ----
mean loss: 196.44
 ---- batch: 070 ----
mean loss: 201.68
 ---- batch: 080 ----
mean loss: 200.93
 ---- batch: 090 ----
mean loss: 197.90
 ---- batch: 100 ----
mean loss: 194.39
 ---- batch: 110 ----
mean loss: 191.03
train mean loss: 194.01
epoch train time: 0:00:02.471213
elapsed time: 0:10:06.225716
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 20:47:33.264665
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.41
 ---- batch: 020 ----
mean loss: 189.46
 ---- batch: 030 ----
mean loss: 190.54
 ---- batch: 040 ----
mean loss: 196.41
 ---- batch: 050 ----
mean loss: 196.65
 ---- batch: 060 ----
mean loss: 195.28
 ---- batch: 070 ----
mean loss: 201.82
 ---- batch: 080 ----
mean loss: 192.85
 ---- batch: 090 ----
mean loss: 189.35
 ---- batch: 100 ----
mean loss: 192.68
 ---- batch: 110 ----
mean loss: 194.09
train mean loss: 194.07
epoch train time: 0:00:02.505605
elapsed time: 0:10:08.731787
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 20:47:35.770753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.17
 ---- batch: 020 ----
mean loss: 197.48
 ---- batch: 030 ----
mean loss: 200.90
 ---- batch: 040 ----
mean loss: 193.43
 ---- batch: 050 ----
mean loss: 193.37
 ---- batch: 060 ----
mean loss: 195.34
 ---- batch: 070 ----
mean loss: 191.69
 ---- batch: 080 ----
mean loss: 182.11
 ---- batch: 090 ----
mean loss: 199.50
 ---- batch: 100 ----
mean loss: 198.58
 ---- batch: 110 ----
mean loss: 195.70
train mean loss: 193.96
epoch train time: 0:00:02.480088
elapsed time: 0:10:11.212360
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 20:47:38.251318
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.47
 ---- batch: 020 ----
mean loss: 186.71
 ---- batch: 030 ----
mean loss: 188.85
 ---- batch: 040 ----
mean loss: 207.96
 ---- batch: 050 ----
mean loss: 186.34
 ---- batch: 060 ----
mean loss: 194.25
 ---- batch: 070 ----
mean loss: 197.28
 ---- batch: 080 ----
mean loss: 195.37
 ---- batch: 090 ----
mean loss: 191.22
 ---- batch: 100 ----
mean loss: 187.09
 ---- batch: 110 ----
mean loss: 199.37
train mean loss: 194.05
epoch train time: 0:00:02.491864
elapsed time: 0:10:13.704672
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 20:47:40.743639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.97
 ---- batch: 020 ----
mean loss: 183.71
 ---- batch: 030 ----
mean loss: 185.05
 ---- batch: 040 ----
mean loss: 199.47
 ---- batch: 050 ----
mean loss: 195.78
 ---- batch: 060 ----
mean loss: 185.54
 ---- batch: 070 ----
mean loss: 197.43
 ---- batch: 080 ----
mean loss: 203.64
 ---- batch: 090 ----
mean loss: 192.46
 ---- batch: 100 ----
mean loss: 203.01
 ---- batch: 110 ----
mean loss: 195.73
train mean loss: 193.93
epoch train time: 0:00:02.499684
elapsed time: 0:10:16.204805
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 20:47:43.243735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.76
 ---- batch: 020 ----
mean loss: 189.12
 ---- batch: 030 ----
mean loss: 199.46
 ---- batch: 040 ----
mean loss: 196.31
 ---- batch: 050 ----
mean loss: 182.23
 ---- batch: 060 ----
mean loss: 200.09
 ---- batch: 070 ----
mean loss: 196.08
 ---- batch: 080 ----
mean loss: 200.39
 ---- batch: 090 ----
mean loss: 195.17
 ---- batch: 100 ----
mean loss: 188.27
 ---- batch: 110 ----
mean loss: 195.94
train mean loss: 193.89
epoch train time: 0:00:02.481842
elapsed time: 0:10:18.687083
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 20:47:45.726053
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.03
 ---- batch: 020 ----
mean loss: 184.94
 ---- batch: 030 ----
mean loss: 188.03
 ---- batch: 040 ----
mean loss: 194.20
 ---- batch: 050 ----
mean loss: 193.65
 ---- batch: 060 ----
mean loss: 204.39
 ---- batch: 070 ----
mean loss: 200.09
 ---- batch: 080 ----
mean loss: 195.16
 ---- batch: 090 ----
mean loss: 191.72
 ---- batch: 100 ----
mean loss: 198.30
 ---- batch: 110 ----
mean loss: 189.08
train mean loss: 193.95
epoch train time: 0:00:02.481352
elapsed time: 0:10:21.169059
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 20:47:48.207822
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.57
 ---- batch: 020 ----
mean loss: 193.92
 ---- batch: 030 ----
mean loss: 185.35
 ---- batch: 040 ----
mean loss: 196.07
 ---- batch: 050 ----
mean loss: 188.21
 ---- batch: 060 ----
mean loss: 197.75
 ---- batch: 070 ----
mean loss: 190.62
 ---- batch: 080 ----
mean loss: 200.23
 ---- batch: 090 ----
mean loss: 199.18
 ---- batch: 100 ----
mean loss: 194.82
 ---- batch: 110 ----
mean loss: 190.74
train mean loss: 193.91
epoch train time: 0:00:02.488760
elapsed time: 0:10:23.658071
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 20:47:50.697014
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.07
 ---- batch: 020 ----
mean loss: 189.22
 ---- batch: 030 ----
mean loss: 188.94
 ---- batch: 040 ----
mean loss: 201.42
 ---- batch: 050 ----
mean loss: 188.90
 ---- batch: 060 ----
mean loss: 200.98
 ---- batch: 070 ----
mean loss: 198.65
 ---- batch: 080 ----
mean loss: 188.16
 ---- batch: 090 ----
mean loss: 191.67
 ---- batch: 100 ----
mean loss: 197.70
 ---- batch: 110 ----
mean loss: 190.78
train mean loss: 193.91
epoch train time: 0:00:02.494310
elapsed time: 0:10:26.152784
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 20:47:53.191741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.72
 ---- batch: 020 ----
mean loss: 195.32
 ---- batch: 030 ----
mean loss: 188.03
 ---- batch: 040 ----
mean loss: 197.58
 ---- batch: 050 ----
mean loss: 194.85
 ---- batch: 060 ----
mean loss: 203.99
 ---- batch: 070 ----
mean loss: 195.75
 ---- batch: 080 ----
mean loss: 187.48
 ---- batch: 090 ----
mean loss: 193.05
 ---- batch: 100 ----
mean loss: 191.36
 ---- batch: 110 ----
mean loss: 193.45
train mean loss: 193.93
epoch train time: 0:00:02.480289
elapsed time: 0:10:28.633497
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 20:47:55.672277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.85
 ---- batch: 020 ----
mean loss: 205.34
 ---- batch: 030 ----
mean loss: 195.18
 ---- batch: 040 ----
mean loss: 188.21
 ---- batch: 050 ----
mean loss: 193.17
 ---- batch: 060 ----
mean loss: 196.16
 ---- batch: 070 ----
mean loss: 185.50
 ---- batch: 080 ----
mean loss: 189.63
 ---- batch: 090 ----
mean loss: 199.37
 ---- batch: 100 ----
mean loss: 196.66
 ---- batch: 110 ----
mean loss: 186.27
train mean loss: 193.92
epoch train time: 0:00:02.551398
elapsed time: 0:10:31.185172
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 20:47:58.224142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.03
 ---- batch: 020 ----
mean loss: 198.71
 ---- batch: 030 ----
mean loss: 192.43
 ---- batch: 040 ----
mean loss: 193.76
 ---- batch: 050 ----
mean loss: 191.03
 ---- batch: 060 ----
mean loss: 195.23
 ---- batch: 070 ----
mean loss: 203.14
 ---- batch: 080 ----
mean loss: 190.39
 ---- batch: 090 ----
mean loss: 187.34
 ---- batch: 100 ----
mean loss: 199.52
 ---- batch: 110 ----
mean loss: 190.59
train mean loss: 193.90
epoch train time: 0:00:02.517130
elapsed time: 0:10:33.702736
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 20:48:00.741690
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.98
 ---- batch: 020 ----
mean loss: 194.16
 ---- batch: 030 ----
mean loss: 196.74
 ---- batch: 040 ----
mean loss: 200.20
 ---- batch: 050 ----
mean loss: 188.30
 ---- batch: 060 ----
mean loss: 202.61
 ---- batch: 070 ----
mean loss: 194.05
 ---- batch: 080 ----
mean loss: 201.41
 ---- batch: 090 ----
mean loss: 190.12
 ---- batch: 100 ----
mean loss: 192.35
 ---- batch: 110 ----
mean loss: 185.84
train mean loss: 193.90
epoch train time: 0:00:02.483990
elapsed time: 0:10:36.187127
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 20:48:03.226085
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.24
 ---- batch: 020 ----
mean loss: 190.26
 ---- batch: 030 ----
mean loss: 197.21
 ---- batch: 040 ----
mean loss: 183.32
 ---- batch: 050 ----
mean loss: 193.82
 ---- batch: 060 ----
mean loss: 196.63
 ---- batch: 070 ----
mean loss: 195.36
 ---- batch: 080 ----
mean loss: 190.40
 ---- batch: 090 ----
mean loss: 208.18
 ---- batch: 100 ----
mean loss: 191.46
 ---- batch: 110 ----
mean loss: 198.78
train mean loss: 193.88
epoch train time: 0:00:02.491260
elapsed time: 0:10:38.678949
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 20:48:05.717882
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.66
 ---- batch: 020 ----
mean loss: 193.38
 ---- batch: 030 ----
mean loss: 190.20
 ---- batch: 040 ----
mean loss: 192.15
 ---- batch: 050 ----
mean loss: 195.05
 ---- batch: 060 ----
mean loss: 200.55
 ---- batch: 070 ----
mean loss: 195.99
 ---- batch: 080 ----
mean loss: 195.82
 ---- batch: 090 ----
mean loss: 198.51
 ---- batch: 100 ----
mean loss: 200.71
 ---- batch: 110 ----
mean loss: 188.07
train mean loss: 193.84
epoch train time: 0:00:02.494511
elapsed time: 0:10:41.173894
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 20:48:08.212880
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.23
 ---- batch: 020 ----
mean loss: 207.41
 ---- batch: 030 ----
mean loss: 200.94
 ---- batch: 040 ----
mean loss: 186.02
 ---- batch: 050 ----
mean loss: 191.88
 ---- batch: 060 ----
mean loss: 193.70
 ---- batch: 070 ----
mean loss: 198.26
 ---- batch: 080 ----
mean loss: 178.08
 ---- batch: 090 ----
mean loss: 190.28
 ---- batch: 100 ----
mean loss: 199.55
 ---- batch: 110 ----
mean loss: 192.35
train mean loss: 193.82
epoch train time: 0:00:02.473937
elapsed time: 0:10:43.648291
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 20:48:10.687215
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.25
 ---- batch: 020 ----
mean loss: 200.60
 ---- batch: 030 ----
mean loss: 189.25
 ---- batch: 040 ----
mean loss: 201.69
 ---- batch: 050 ----
mean loss: 200.96
 ---- batch: 060 ----
mean loss: 191.96
 ---- batch: 070 ----
mean loss: 198.61
 ---- batch: 080 ----
mean loss: 186.93
 ---- batch: 090 ----
mean loss: 185.49
 ---- batch: 100 ----
mean loss: 197.37
 ---- batch: 110 ----
mean loss: 190.24
train mean loss: 193.84
epoch train time: 0:00:02.500050
elapsed time: 0:10:46.148731
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 20:48:13.187719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.14
 ---- batch: 020 ----
mean loss: 193.83
 ---- batch: 030 ----
mean loss: 194.50
 ---- batch: 040 ----
mean loss: 193.46
 ---- batch: 050 ----
mean loss: 194.72
 ---- batch: 060 ----
mean loss: 185.55
 ---- batch: 070 ----
mean loss: 197.73
 ---- batch: 080 ----
mean loss: 200.67
 ---- batch: 090 ----
mean loss: 193.34
 ---- batch: 100 ----
mean loss: 193.89
 ---- batch: 110 ----
mean loss: 198.92
train mean loss: 193.84
epoch train time: 0:00:02.505502
elapsed time: 0:10:48.654719
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 20:48:15.693736
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.39
 ---- batch: 020 ----
mean loss: 191.21
 ---- batch: 030 ----
mean loss: 198.51
 ---- batch: 040 ----
mean loss: 193.26
 ---- batch: 050 ----
mean loss: 189.11
 ---- batch: 060 ----
mean loss: 196.35
 ---- batch: 070 ----
mean loss: 197.68
 ---- batch: 080 ----
mean loss: 190.83
 ---- batch: 090 ----
mean loss: 193.99
 ---- batch: 100 ----
mean loss: 197.59
 ---- batch: 110 ----
mean loss: 197.54
train mean loss: 193.79
epoch train time: 0:00:02.509816
elapsed time: 0:10:51.165012
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 20:48:18.203988
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.86
 ---- batch: 020 ----
mean loss: 194.07
 ---- batch: 030 ----
mean loss: 194.08
 ---- batch: 040 ----
mean loss: 191.85
 ---- batch: 050 ----
mean loss: 195.05
 ---- batch: 060 ----
mean loss: 192.17
 ---- batch: 070 ----
mean loss: 193.04
 ---- batch: 080 ----
mean loss: 191.80
 ---- batch: 090 ----
mean loss: 190.86
 ---- batch: 100 ----
mean loss: 193.96
 ---- batch: 110 ----
mean loss: 193.42
train mean loss: 193.81
epoch train time: 0:00:02.516136
elapsed time: 0:10:53.681675
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 20:48:20.720623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.71
 ---- batch: 020 ----
mean loss: 200.17
 ---- batch: 030 ----
mean loss: 195.21
 ---- batch: 040 ----
mean loss: 194.40
 ---- batch: 050 ----
mean loss: 192.00
 ---- batch: 060 ----
mean loss: 196.96
 ---- batch: 070 ----
mean loss: 196.69
 ---- batch: 080 ----
mean loss: 189.21
 ---- batch: 090 ----
mean loss: 193.40
 ---- batch: 100 ----
mean loss: 198.17
 ---- batch: 110 ----
mean loss: 200.46
train mean loss: 193.72
epoch train time: 0:00:02.504674
elapsed time: 0:10:56.186774
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 20:48:23.225763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.21
 ---- batch: 020 ----
mean loss: 193.76
 ---- batch: 030 ----
mean loss: 191.48
 ---- batch: 040 ----
mean loss: 196.53
 ---- batch: 050 ----
mean loss: 190.54
 ---- batch: 060 ----
mean loss: 188.68
 ---- batch: 070 ----
mean loss: 191.12
 ---- batch: 080 ----
mean loss: 193.21
 ---- batch: 090 ----
mean loss: 196.89
 ---- batch: 100 ----
mean loss: 200.72
 ---- batch: 110 ----
mean loss: 193.51
train mean loss: 193.66
epoch train time: 0:00:02.496370
elapsed time: 0:10:58.683611
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 20:48:25.722576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.22
 ---- batch: 020 ----
mean loss: 194.66
 ---- batch: 030 ----
mean loss: 190.19
 ---- batch: 040 ----
mean loss: 193.70
 ---- batch: 050 ----
mean loss: 191.32
 ---- batch: 060 ----
mean loss: 199.78
 ---- batch: 070 ----
mean loss: 193.34
 ---- batch: 080 ----
mean loss: 192.47
 ---- batch: 090 ----
mean loss: 194.92
 ---- batch: 100 ----
mean loss: 198.30
 ---- batch: 110 ----
mean loss: 193.86
train mean loss: 193.70
epoch train time: 0:00:02.503070
elapsed time: 0:11:01.191060
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_1/checkpoint.pth.tar
**** end time: 2019-09-26 20:48:28.229784 ****
