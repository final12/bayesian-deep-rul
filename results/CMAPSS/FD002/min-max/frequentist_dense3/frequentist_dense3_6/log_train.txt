Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_6', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24575
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:19:35.164135 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:19:35.167392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4356.44
 ---- batch: 020 ----
mean loss: 4190.93
 ---- batch: 030 ----
mean loss: 4173.90
 ---- batch: 040 ----
mean loss: 4059.43
 ---- batch: 050 ----
mean loss: 3929.38
 ---- batch: 060 ----
mean loss: 3974.84
 ---- batch: 070 ----
mean loss: 3849.34
 ---- batch: 080 ----
mean loss: 3849.80
 ---- batch: 090 ----
mean loss: 3795.08
train mean loss: 4002.22
epoch train time: 0:00:33.826347
elapsed time: 0:00:33.832052
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:20:08.996227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3635.04
 ---- batch: 020 ----
mean loss: 3675.39
 ---- batch: 030 ----
mean loss: 3606.25
 ---- batch: 040 ----
mean loss: 3558.38
 ---- batch: 050 ----
mean loss: 3435.78
 ---- batch: 060 ----
mean loss: 3441.34
 ---- batch: 070 ----
mean loss: 3382.04
 ---- batch: 080 ----
mean loss: 3300.04
 ---- batch: 090 ----
mean loss: 3274.46
train mean loss: 3463.52
epoch train time: 0:00:00.495242
elapsed time: 0:00:34.327435
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:20:09.491619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3151.33
 ---- batch: 020 ----
mean loss: 3089.11
 ---- batch: 030 ----
mean loss: 3074.19
 ---- batch: 040 ----
mean loss: 3025.61
 ---- batch: 050 ----
mean loss: 2991.78
 ---- batch: 060 ----
mean loss: 2940.59
 ---- batch: 070 ----
mean loss: 2925.72
 ---- batch: 080 ----
mean loss: 2851.70
 ---- batch: 090 ----
mean loss: 2775.34
train mean loss: 2970.18
epoch train time: 0:00:00.499514
elapsed time: 0:00:34.827093
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:20:09.991278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2734.39
 ---- batch: 020 ----
mean loss: 2660.76
 ---- batch: 030 ----
mean loss: 2613.92
 ---- batch: 040 ----
mean loss: 2634.71
 ---- batch: 050 ----
mean loss: 2571.91
 ---- batch: 060 ----
mean loss: 2554.93
 ---- batch: 070 ----
mean loss: 2466.59
 ---- batch: 080 ----
mean loss: 2448.31
 ---- batch: 090 ----
mean loss: 2443.69
train mean loss: 2558.71
epoch train time: 0:00:00.497280
elapsed time: 0:00:35.324521
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:20:10.488708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2343.13
 ---- batch: 020 ----
mean loss: 2334.84
 ---- batch: 030 ----
mean loss: 2277.05
 ---- batch: 040 ----
mean loss: 2243.58
 ---- batch: 050 ----
mean loss: 2262.16
 ---- batch: 060 ----
mean loss: 2172.98
 ---- batch: 070 ----
mean loss: 2164.78
 ---- batch: 080 ----
mean loss: 2166.03
 ---- batch: 090 ----
mean loss: 2106.13
train mean loss: 2219.75
epoch train time: 0:00:00.495216
elapsed time: 0:00:35.819884
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:20:10.984084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2022.76
 ---- batch: 020 ----
mean loss: 1988.30
 ---- batch: 030 ----
mean loss: 1996.11
 ---- batch: 040 ----
mean loss: 1946.61
 ---- batch: 050 ----
mean loss: 1981.93
 ---- batch: 060 ----
mean loss: 1869.70
 ---- batch: 070 ----
mean loss: 1890.00
 ---- batch: 080 ----
mean loss: 1893.59
 ---- batch: 090 ----
mean loss: 1808.41
train mean loss: 1923.55
epoch train time: 0:00:00.492971
elapsed time: 0:00:36.313015
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:20:11.477200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1783.13
 ---- batch: 020 ----
mean loss: 1772.41
 ---- batch: 030 ----
mean loss: 1683.41
 ---- batch: 040 ----
mean loss: 1714.32
 ---- batch: 050 ----
mean loss: 1679.74
 ---- batch: 060 ----
mean loss: 1672.06
 ---- batch: 070 ----
mean loss: 1632.60
 ---- batch: 080 ----
mean loss: 1618.63
 ---- batch: 090 ----
mean loss: 1600.56
train mean loss: 1675.09
epoch train time: 0:00:00.501148
elapsed time: 0:00:36.814312
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:20:11.978496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1536.67
 ---- batch: 020 ----
mean loss: 1542.06
 ---- batch: 030 ----
mean loss: 1539.12
 ---- batch: 040 ----
mean loss: 1461.09
 ---- batch: 050 ----
mean loss: 1499.74
 ---- batch: 060 ----
mean loss: 1475.66
 ---- batch: 070 ----
mean loss: 1423.65
 ---- batch: 080 ----
mean loss: 1444.63
 ---- batch: 090 ----
mean loss: 1401.23
train mean loss: 1473.99
epoch train time: 0:00:00.501452
elapsed time: 0:00:37.315913
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:20:12.480100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1389.88
 ---- batch: 020 ----
mean loss: 1344.28
 ---- batch: 030 ----
mean loss: 1340.44
 ---- batch: 040 ----
mean loss: 1320.62
 ---- batch: 050 ----
mean loss: 1346.06
 ---- batch: 060 ----
mean loss: 1287.20
 ---- batch: 070 ----
mean loss: 1302.51
 ---- batch: 080 ----
mean loss: 1255.98
 ---- batch: 090 ----
mean loss: 1283.13
train mean loss: 1315.70
epoch train time: 0:00:00.507177
elapsed time: 0:00:37.823250
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:20:12.987427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1250.45
 ---- batch: 020 ----
mean loss: 1219.96
 ---- batch: 030 ----
mean loss: 1226.38
 ---- batch: 040 ----
mean loss: 1176.15
 ---- batch: 050 ----
mean loss: 1203.89
 ---- batch: 060 ----
mean loss: 1183.42
 ---- batch: 070 ----
mean loss: 1192.20
 ---- batch: 080 ----
mean loss: 1144.37
 ---- batch: 090 ----
mean loss: 1157.53
train mean loss: 1190.19
epoch train time: 0:00:00.508870
elapsed time: 0:00:38.332285
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:20:13.496472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1108.65
 ---- batch: 020 ----
mean loss: 1126.46
 ---- batch: 030 ----
mean loss: 1120.37
 ---- batch: 040 ----
mean loss: 1086.64
 ---- batch: 050 ----
mean loss: 1084.85
 ---- batch: 060 ----
mean loss: 1097.15
 ---- batch: 070 ----
mean loss: 1079.73
 ---- batch: 080 ----
mean loss: 1048.90
 ---- batch: 090 ----
mean loss: 1071.27
train mean loss: 1089.46
epoch train time: 0:00:00.505991
elapsed time: 0:00:38.838427
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:20:14.002612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1032.39
 ---- batch: 020 ----
mean loss: 1042.59
 ---- batch: 030 ----
mean loss: 1046.08
 ---- batch: 040 ----
mean loss: 1033.51
 ---- batch: 050 ----
mean loss: 1026.71
 ---- batch: 060 ----
mean loss: 1021.01
 ---- batch: 070 ----
mean loss: 1020.96
 ---- batch: 080 ----
mean loss: 991.35
 ---- batch: 090 ----
mean loss: 980.74
train mean loss: 1018.36
epoch train time: 0:00:00.493187
elapsed time: 0:00:39.331772
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:20:14.495972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 978.22
 ---- batch: 020 ----
mean loss: 970.81
 ---- batch: 030 ----
mean loss: 985.09
 ---- batch: 040 ----
mean loss: 962.10
 ---- batch: 050 ----
mean loss: 989.11
 ---- batch: 060 ----
mean loss: 962.78
 ---- batch: 070 ----
mean loss: 946.58
 ---- batch: 080 ----
mean loss: 955.50
 ---- batch: 090 ----
mean loss: 954.16
train mean loss: 966.40
epoch train time: 0:00:00.498407
elapsed time: 0:00:39.830346
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:20:14.994544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.35
 ---- batch: 020 ----
mean loss: 938.05
 ---- batch: 030 ----
mean loss: 932.89
 ---- batch: 040 ----
mean loss: 913.20
 ---- batch: 050 ----
mean loss: 930.98
 ---- batch: 060 ----
mean loss: 925.45
 ---- batch: 070 ----
mean loss: 939.66
 ---- batch: 080 ----
mean loss: 924.42
 ---- batch: 090 ----
mean loss: 922.92
train mean loss: 929.37
epoch train time: 0:00:00.500554
elapsed time: 0:00:40.331076
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:20:15.495284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.81
 ---- batch: 020 ----
mean loss: 917.26
 ---- batch: 030 ----
mean loss: 910.47
 ---- batch: 040 ----
mean loss: 895.54
 ---- batch: 050 ----
mean loss: 903.42
 ---- batch: 060 ----
mean loss: 897.44
 ---- batch: 070 ----
mean loss: 899.37
 ---- batch: 080 ----
mean loss: 914.13
 ---- batch: 090 ----
mean loss: 905.92
train mean loss: 907.71
epoch train time: 0:00:00.509620
elapsed time: 0:00:40.840867
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:20:16.005054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.00
 ---- batch: 020 ----
mean loss: 897.18
 ---- batch: 030 ----
mean loss: 895.08
 ---- batch: 040 ----
mean loss: 904.19
 ---- batch: 050 ----
mean loss: 897.89
 ---- batch: 060 ----
mean loss: 886.27
 ---- batch: 070 ----
mean loss: 871.06
 ---- batch: 080 ----
mean loss: 891.82
 ---- batch: 090 ----
mean loss: 892.68
train mean loss: 893.66
epoch train time: 0:00:00.494561
elapsed time: 0:00:41.335598
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:20:16.499773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.80
 ---- batch: 020 ----
mean loss: 868.47
 ---- batch: 030 ----
mean loss: 884.15
 ---- batch: 040 ----
mean loss: 891.61
 ---- batch: 050 ----
mean loss: 868.00
 ---- batch: 060 ----
mean loss: 893.34
 ---- batch: 070 ----
mean loss: 899.07
 ---- batch: 080 ----
mean loss: 900.36
 ---- batch: 090 ----
mean loss: 870.78
train mean loss: 885.91
epoch train time: 0:00:00.501427
elapsed time: 0:00:41.837158
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:20:17.001342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.52
 ---- batch: 020 ----
mean loss: 876.90
 ---- batch: 030 ----
mean loss: 898.61
 ---- batch: 040 ----
mean loss: 889.10
 ---- batch: 050 ----
mean loss: 871.01
 ---- batch: 060 ----
mean loss: 865.49
 ---- batch: 070 ----
mean loss: 878.84
 ---- batch: 080 ----
mean loss: 875.76
 ---- batch: 090 ----
mean loss: 873.42
train mean loss: 880.73
epoch train time: 0:00:00.496810
elapsed time: 0:00:42.334112
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:20:17.498296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.15
 ---- batch: 020 ----
mean loss: 886.72
 ---- batch: 030 ----
mean loss: 865.23
 ---- batch: 040 ----
mean loss: 883.60
 ---- batch: 050 ----
mean loss: 880.93
 ---- batch: 060 ----
mean loss: 874.71
 ---- batch: 070 ----
mean loss: 860.39
 ---- batch: 080 ----
mean loss: 891.18
 ---- batch: 090 ----
mean loss: 885.52
train mean loss: 878.22
epoch train time: 0:00:00.504779
elapsed time: 0:00:42.839047
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:20:18.003230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.64
 ---- batch: 020 ----
mean loss: 889.32
 ---- batch: 030 ----
mean loss: 876.55
 ---- batch: 040 ----
mean loss: 884.03
 ---- batch: 050 ----
mean loss: 867.23
 ---- batch: 060 ----
mean loss: 878.36
 ---- batch: 070 ----
mean loss: 886.30
 ---- batch: 080 ----
mean loss: 864.28
 ---- batch: 090 ----
mean loss: 872.35
train mean loss: 876.96
epoch train time: 0:00:00.493541
elapsed time: 0:00:43.332779
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:20:18.496979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.88
 ---- batch: 020 ----
mean loss: 868.76
 ---- batch: 030 ----
mean loss: 868.06
 ---- batch: 040 ----
mean loss: 876.01
 ---- batch: 050 ----
mean loss: 894.15
 ---- batch: 060 ----
mean loss: 873.77
 ---- batch: 070 ----
mean loss: 857.94
 ---- batch: 080 ----
mean loss: 889.59
 ---- batch: 090 ----
mean loss: 879.37
train mean loss: 875.33
epoch train time: 0:00:00.520389
elapsed time: 0:00:43.853332
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:20:19.017519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.40
 ---- batch: 020 ----
mean loss: 888.09
 ---- batch: 030 ----
mean loss: 867.24
 ---- batch: 040 ----
mean loss: 856.64
 ---- batch: 050 ----
mean loss: 868.14
 ---- batch: 060 ----
mean loss: 891.17
 ---- batch: 070 ----
mean loss: 882.16
 ---- batch: 080 ----
mean loss: 871.29
 ---- batch: 090 ----
mean loss: 876.61
train mean loss: 875.97
epoch train time: 0:00:00.502083
elapsed time: 0:00:44.355561
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:20:19.519745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.01
 ---- batch: 020 ----
mean loss: 870.65
 ---- batch: 030 ----
mean loss: 861.12
 ---- batch: 040 ----
mean loss: 868.36
 ---- batch: 050 ----
mean loss: 878.79
 ---- batch: 060 ----
mean loss: 879.43
 ---- batch: 070 ----
mean loss: 875.82
 ---- batch: 080 ----
mean loss: 878.43
 ---- batch: 090 ----
mean loss: 884.99
train mean loss: 875.11
epoch train time: 0:00:00.504678
elapsed time: 0:00:44.860389
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:20:20.024576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.59
 ---- batch: 020 ----
mean loss: 871.07
 ---- batch: 030 ----
mean loss: 877.14
 ---- batch: 040 ----
mean loss: 858.02
 ---- batch: 050 ----
mean loss: 872.98
 ---- batch: 060 ----
mean loss: 865.62
 ---- batch: 070 ----
mean loss: 881.45
 ---- batch: 080 ----
mean loss: 881.52
 ---- batch: 090 ----
mean loss: 873.40
train mean loss: 876.25
epoch train time: 0:00:00.512853
elapsed time: 0:00:45.373394
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:20:20.537580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.76
 ---- batch: 020 ----
mean loss: 889.01
 ---- batch: 030 ----
mean loss: 883.37
 ---- batch: 040 ----
mean loss: 876.08
 ---- batch: 050 ----
mean loss: 881.02
 ---- batch: 060 ----
mean loss: 877.85
 ---- batch: 070 ----
mean loss: 873.89
 ---- batch: 080 ----
mean loss: 865.61
 ---- batch: 090 ----
mean loss: 883.89
train mean loss: 875.20
epoch train time: 0:00:00.510568
elapsed time: 0:00:45.884117
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:20:21.048304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.50
 ---- batch: 020 ----
mean loss: 869.74
 ---- batch: 030 ----
mean loss: 863.28
 ---- batch: 040 ----
mean loss: 866.22
 ---- batch: 050 ----
mean loss: 864.50
 ---- batch: 060 ----
mean loss: 902.38
 ---- batch: 070 ----
mean loss: 883.60
 ---- batch: 080 ----
mean loss: 878.40
 ---- batch: 090 ----
mean loss: 869.10
train mean loss: 874.90
epoch train time: 0:00:00.492995
elapsed time: 0:00:46.377256
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:20:21.541445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.26
 ---- batch: 020 ----
mean loss: 873.49
 ---- batch: 030 ----
mean loss: 871.79
 ---- batch: 040 ----
mean loss: 872.87
 ---- batch: 050 ----
mean loss: 864.73
 ---- batch: 060 ----
mean loss: 869.84
 ---- batch: 070 ----
mean loss: 879.61
 ---- batch: 080 ----
mean loss: 887.26
 ---- batch: 090 ----
mean loss: 888.05
train mean loss: 875.11
epoch train time: 0:00:00.512129
elapsed time: 0:00:46.889556
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:20:22.053744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.93
 ---- batch: 020 ----
mean loss: 869.18
 ---- batch: 030 ----
mean loss: 878.11
 ---- batch: 040 ----
mean loss: 885.38
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.61
 ---- batch: 080 ----
mean loss: 892.12
 ---- batch: 090 ----
mean loss: 862.39
train mean loss: 874.90
epoch train time: 0:00:00.498933
elapsed time: 0:00:47.388685
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:20:22.552907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.86
 ---- batch: 020 ----
mean loss: 874.97
 ---- batch: 030 ----
mean loss: 861.88
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.70
 ---- batch: 060 ----
mean loss: 883.33
 ---- batch: 070 ----
mean loss: 889.98
 ---- batch: 080 ----
mean loss: 863.96
 ---- batch: 090 ----
mean loss: 859.31
train mean loss: 875.83
epoch train time: 0:00:00.500608
elapsed time: 0:00:47.889514
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:20:23.053718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.46
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 876.61
 ---- batch: 050 ----
mean loss: 875.43
 ---- batch: 060 ----
mean loss: 878.11
 ---- batch: 070 ----
mean loss: 872.73
 ---- batch: 080 ----
mean loss: 870.15
 ---- batch: 090 ----
mean loss: 857.33
train mean loss: 874.97
epoch train time: 0:00:00.495862
elapsed time: 0:00:48.385565
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:20:23.549753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.88
 ---- batch: 020 ----
mean loss: 871.10
 ---- batch: 030 ----
mean loss: 868.37
 ---- batch: 040 ----
mean loss: 878.46
 ---- batch: 050 ----
mean loss: 892.61
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.10
train mean loss: 875.07
epoch train time: 0:00:00.508067
elapsed time: 0:00:48.893786
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:20:24.057973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.11
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.00
 ---- batch: 050 ----
mean loss: 876.37
 ---- batch: 060 ----
mean loss: 886.71
 ---- batch: 070 ----
mean loss: 861.68
 ---- batch: 080 ----
mean loss: 882.78
 ---- batch: 090 ----
mean loss: 870.74
train mean loss: 874.91
epoch train time: 0:00:00.504358
elapsed time: 0:00:49.398303
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:20:24.562491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.46
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.20
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.78
train mean loss: 876.12
epoch train time: 0:00:00.497682
elapsed time: 0:00:49.896134
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:20:25.060318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.88
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.29
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.78
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.492294
elapsed time: 0:00:50.388588
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:20:25.552764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.81
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.56
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.60
 ---- batch: 070 ----
mean loss: 878.83
 ---- batch: 080 ----
mean loss: 875.94
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.71
epoch train time: 0:00:00.508054
elapsed time: 0:00:50.896790
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:20:26.060980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.46
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.43
 ---- batch: 070 ----
mean loss: 861.54
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.22
train mean loss: 875.33
epoch train time: 0:00:00.497649
elapsed time: 0:00:51.394623
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:20:26.558808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.86
 ---- batch: 040 ----
mean loss: 896.14
 ---- batch: 050 ----
mean loss: 895.66
 ---- batch: 060 ----
mean loss: 872.61
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.89
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.502133
elapsed time: 0:00:51.896904
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:20:27.061105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.29
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 882.00
 ---- batch: 090 ----
mean loss: 857.67
train mean loss: 875.43
epoch train time: 0:00:00.501156
elapsed time: 0:00:52.398225
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:20:27.562410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.13
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.71
train mean loss: 875.94
epoch train time: 0:00:00.503349
elapsed time: 0:00:52.901723
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:20:28.065909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.79
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.58
train mean loss: 875.20
epoch train time: 0:00:00.494334
elapsed time: 0:00:53.396203
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:20:28.560387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.90
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.49
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.95
 ---- batch: 090 ----
mean loss: 867.30
train mean loss: 874.55
epoch train time: 0:00:00.492791
elapsed time: 0:00:53.889140
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:20:29.053325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.84
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.497409
elapsed time: 0:00:54.386723
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:20:29.550909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.14
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.48
 ---- batch: 060 ----
mean loss: 873.50
 ---- batch: 070 ----
mean loss: 889.67
 ---- batch: 080 ----
mean loss: 893.94
 ---- batch: 090 ----
mean loss: 900.18
train mean loss: 875.66
epoch train time: 0:00:00.535297
elapsed time: 0:00:54.922201
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:20:30.086403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.42
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.92
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.11
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.24
epoch train time: 0:00:00.506977
elapsed time: 0:00:55.429380
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:20:30.593581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.92
 ---- batch: 020 ----
mean loss: 896.00
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.47
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.66
epoch train time: 0:00:00.498195
elapsed time: 0:00:55.927733
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:20:31.091918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.23
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.63
 ---- batch: 080 ----
mean loss: 876.95
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.494308
elapsed time: 0:00:56.422194
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:20:31.586377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.45
 ---- batch: 070 ----
mean loss: 893.99
 ---- batch: 080 ----
mean loss: 865.68
 ---- batch: 090 ----
mean loss: 871.94
train mean loss: 875.10
epoch train time: 0:00:00.506675
elapsed time: 0:00:56.929014
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:20:32.093199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.71
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.70
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.64
 ---- batch: 060 ----
mean loss: 876.82
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.85
epoch train time: 0:00:00.503412
elapsed time: 0:00:57.432571
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:20:32.596757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.53
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.89
 ---- batch: 050 ----
mean loss: 879.93
 ---- batch: 060 ----
mean loss: 870.32
 ---- batch: 070 ----
mean loss: 893.24
 ---- batch: 080 ----
mean loss: 874.10
 ---- batch: 090 ----
mean loss: 901.17
train mean loss: 874.83
epoch train time: 0:00:00.524509
elapsed time: 0:00:57.957229
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:20:33.121430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.45
 ---- batch: 020 ----
mean loss: 880.66
 ---- batch: 030 ----
mean loss: 847.90
 ---- batch: 040 ----
mean loss: 828.22
 ---- batch: 050 ----
mean loss: 784.39
 ---- batch: 060 ----
mean loss: 765.48
 ---- batch: 070 ----
mean loss: 728.35
 ---- batch: 080 ----
mean loss: 721.34
 ---- batch: 090 ----
mean loss: 679.14
train mean loss: 780.42
epoch train time: 0:00:00.496413
elapsed time: 0:00:58.453804
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:20:33.617987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.66
 ---- batch: 020 ----
mean loss: 548.14
 ---- batch: 030 ----
mean loss: 492.26
 ---- batch: 040 ----
mean loss: 442.79
 ---- batch: 050 ----
mean loss: 420.71
 ---- batch: 060 ----
mean loss: 411.40
 ---- batch: 070 ----
mean loss: 409.38
 ---- batch: 080 ----
mean loss: 384.49
 ---- batch: 090 ----
mean loss: 382.23
train mean loss: 450.39
epoch train time: 0:00:00.515269
elapsed time: 0:00:58.969221
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:20:34.133424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.32
 ---- batch: 020 ----
mean loss: 352.08
 ---- batch: 030 ----
mean loss: 355.20
 ---- batch: 040 ----
mean loss: 347.62
 ---- batch: 050 ----
mean loss: 330.44
 ---- batch: 060 ----
mean loss: 328.39
 ---- batch: 070 ----
mean loss: 316.81
 ---- batch: 080 ----
mean loss: 321.24
 ---- batch: 090 ----
mean loss: 318.29
train mean loss: 335.23
epoch train time: 0:00:00.503465
elapsed time: 0:00:59.472860
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:20:34.637045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.61
 ---- batch: 020 ----
mean loss: 299.96
 ---- batch: 030 ----
mean loss: 292.27
 ---- batch: 040 ----
mean loss: 289.30
 ---- batch: 050 ----
mean loss: 291.21
 ---- batch: 060 ----
mean loss: 295.76
 ---- batch: 070 ----
mean loss: 275.71
 ---- batch: 080 ----
mean loss: 281.58
 ---- batch: 090 ----
mean loss: 285.57
train mean loss: 290.20
epoch train time: 0:00:00.501969
elapsed time: 0:00:59.975010
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:20:35.139186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.46
 ---- batch: 020 ----
mean loss: 269.89
 ---- batch: 030 ----
mean loss: 280.93
 ---- batch: 040 ----
mean loss: 264.23
 ---- batch: 050 ----
mean loss: 262.97
 ---- batch: 060 ----
mean loss: 258.99
 ---- batch: 070 ----
mean loss: 253.55
 ---- batch: 080 ----
mean loss: 271.28
 ---- batch: 090 ----
mean loss: 255.76
train mean loss: 265.07
epoch train time: 0:00:00.497377
elapsed time: 0:01:00.472533
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:20:35.636710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.22
 ---- batch: 020 ----
mean loss: 259.50
 ---- batch: 030 ----
mean loss: 256.13
 ---- batch: 040 ----
mean loss: 252.50
 ---- batch: 050 ----
mean loss: 251.81
 ---- batch: 060 ----
mean loss: 251.67
 ---- batch: 070 ----
mean loss: 259.85
 ---- batch: 080 ----
mean loss: 241.91
 ---- batch: 090 ----
mean loss: 248.01
train mean loss: 252.64
epoch train time: 0:00:00.509523
elapsed time: 0:01:00.982196
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:20:36.146412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.53
 ---- batch: 020 ----
mean loss: 240.15
 ---- batch: 030 ----
mean loss: 244.33
 ---- batch: 040 ----
mean loss: 243.91
 ---- batch: 050 ----
mean loss: 241.98
 ---- batch: 060 ----
mean loss: 242.40
 ---- batch: 070 ----
mean loss: 248.73
 ---- batch: 080 ----
mean loss: 229.59
 ---- batch: 090 ----
mean loss: 231.00
train mean loss: 241.31
epoch train time: 0:00:00.493454
elapsed time: 0:01:01.475841
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:20:36.640040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.82
 ---- batch: 020 ----
mean loss: 231.86
 ---- batch: 030 ----
mean loss: 238.36
 ---- batch: 040 ----
mean loss: 239.07
 ---- batch: 050 ----
mean loss: 234.38
 ---- batch: 060 ----
mean loss: 234.52
 ---- batch: 070 ----
mean loss: 234.95
 ---- batch: 080 ----
mean loss: 229.96
 ---- batch: 090 ----
mean loss: 235.64
train mean loss: 234.74
epoch train time: 0:00:00.509259
elapsed time: 0:01:01.985298
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:20:37.149485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.63
 ---- batch: 020 ----
mean loss: 231.48
 ---- batch: 030 ----
mean loss: 231.07
 ---- batch: 040 ----
mean loss: 226.27
 ---- batch: 050 ----
mean loss: 233.80
 ---- batch: 060 ----
mean loss: 233.19
 ---- batch: 070 ----
mean loss: 223.68
 ---- batch: 080 ----
mean loss: 226.87
 ---- batch: 090 ----
mean loss: 232.30
train mean loss: 229.48
epoch train time: 0:00:00.503446
elapsed time: 0:01:02.488895
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:20:37.653099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.22
 ---- batch: 020 ----
mean loss: 220.36
 ---- batch: 030 ----
mean loss: 224.33
 ---- batch: 040 ----
mean loss: 228.96
 ---- batch: 050 ----
mean loss: 222.45
 ---- batch: 060 ----
mean loss: 226.48
 ---- batch: 070 ----
mean loss: 222.24
 ---- batch: 080 ----
mean loss: 228.81
 ---- batch: 090 ----
mean loss: 232.01
train mean loss: 226.05
epoch train time: 0:00:00.504989
elapsed time: 0:01:02.994049
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:20:38.158233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.17
 ---- batch: 020 ----
mean loss: 229.47
 ---- batch: 030 ----
mean loss: 218.46
 ---- batch: 040 ----
mean loss: 220.15
 ---- batch: 050 ----
mean loss: 224.50
 ---- batch: 060 ----
mean loss: 227.75
 ---- batch: 070 ----
mean loss: 220.09
 ---- batch: 080 ----
mean loss: 219.38
 ---- batch: 090 ----
mean loss: 223.04
train mean loss: 222.71
epoch train time: 0:00:00.491302
elapsed time: 0:01:03.485520
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:20:38.649707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.37
 ---- batch: 020 ----
mean loss: 223.69
 ---- batch: 030 ----
mean loss: 218.03
 ---- batch: 040 ----
mean loss: 216.31
 ---- batch: 050 ----
mean loss: 211.87
 ---- batch: 060 ----
mean loss: 215.77
 ---- batch: 070 ----
mean loss: 222.84
 ---- batch: 080 ----
mean loss: 222.67
 ---- batch: 090 ----
mean loss: 218.96
train mean loss: 218.59
epoch train time: 0:00:00.503522
elapsed time: 0:01:03.989192
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:20:39.153378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.31
 ---- batch: 020 ----
mean loss: 214.30
 ---- batch: 030 ----
mean loss: 208.96
 ---- batch: 040 ----
mean loss: 212.67
 ---- batch: 050 ----
mean loss: 218.03
 ---- batch: 060 ----
mean loss: 223.14
 ---- batch: 070 ----
mean loss: 213.63
 ---- batch: 080 ----
mean loss: 209.99
 ---- batch: 090 ----
mean loss: 224.38
train mean loss: 215.94
epoch train time: 0:00:00.506483
elapsed time: 0:01:04.495833
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:20:39.660020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.10
 ---- batch: 020 ----
mean loss: 204.71
 ---- batch: 030 ----
mean loss: 215.44
 ---- batch: 040 ----
mean loss: 205.86
 ---- batch: 050 ----
mean loss: 213.09
 ---- batch: 060 ----
mean loss: 216.93
 ---- batch: 070 ----
mean loss: 220.38
 ---- batch: 080 ----
mean loss: 218.30
 ---- batch: 090 ----
mean loss: 212.52
train mean loss: 213.26
epoch train time: 0:00:00.505777
elapsed time: 0:01:05.001759
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:20:40.165945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.57
 ---- batch: 020 ----
mean loss: 211.86
 ---- batch: 030 ----
mean loss: 209.01
 ---- batch: 040 ----
mean loss: 212.52
 ---- batch: 050 ----
mean loss: 219.64
 ---- batch: 060 ----
mean loss: 205.73
 ---- batch: 070 ----
mean loss: 207.20
 ---- batch: 080 ----
mean loss: 207.56
 ---- batch: 090 ----
mean loss: 207.13
train mean loss: 210.06
epoch train time: 0:00:00.506684
elapsed time: 0:01:05.508603
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:20:40.672789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.76
 ---- batch: 020 ----
mean loss: 205.93
 ---- batch: 030 ----
mean loss: 214.41
 ---- batch: 040 ----
mean loss: 214.50
 ---- batch: 050 ----
mean loss: 214.07
 ---- batch: 060 ----
mean loss: 209.82
 ---- batch: 070 ----
mean loss: 202.55
 ---- batch: 080 ----
mean loss: 205.56
 ---- batch: 090 ----
mean loss: 205.01
train mean loss: 209.15
epoch train time: 0:00:00.514051
elapsed time: 0:01:06.022818
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:20:41.187004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.22
 ---- batch: 020 ----
mean loss: 206.13
 ---- batch: 030 ----
mean loss: 208.51
 ---- batch: 040 ----
mean loss: 203.01
 ---- batch: 050 ----
mean loss: 206.87
 ---- batch: 060 ----
mean loss: 209.12
 ---- batch: 070 ----
mean loss: 204.18
 ---- batch: 080 ----
mean loss: 212.75
 ---- batch: 090 ----
mean loss: 205.38
train mean loss: 206.28
epoch train time: 0:00:00.518259
elapsed time: 0:01:06.541235
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:20:41.705455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.21
 ---- batch: 020 ----
mean loss: 197.97
 ---- batch: 030 ----
mean loss: 212.22
 ---- batch: 040 ----
mean loss: 213.05
 ---- batch: 050 ----
mean loss: 205.46
 ---- batch: 060 ----
mean loss: 214.97
 ---- batch: 070 ----
mean loss: 200.87
 ---- batch: 080 ----
mean loss: 202.74
 ---- batch: 090 ----
mean loss: 199.00
train mean loss: 205.03
epoch train time: 0:00:00.518006
elapsed time: 0:01:07.059428
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:20:42.223638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.25
 ---- batch: 020 ----
mean loss: 204.59
 ---- batch: 030 ----
mean loss: 195.46
 ---- batch: 040 ----
mean loss: 196.16
 ---- batch: 050 ----
mean loss: 197.51
 ---- batch: 060 ----
mean loss: 200.08
 ---- batch: 070 ----
mean loss: 204.43
 ---- batch: 080 ----
mean loss: 203.87
 ---- batch: 090 ----
mean loss: 196.38
train mean loss: 200.40
epoch train time: 0:00:00.501848
elapsed time: 0:01:07.561463
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:20:42.725661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.35
 ---- batch: 020 ----
mean loss: 203.46
 ---- batch: 030 ----
mean loss: 190.96
 ---- batch: 040 ----
mean loss: 195.40
 ---- batch: 050 ----
mean loss: 193.22
 ---- batch: 060 ----
mean loss: 199.62
 ---- batch: 070 ----
mean loss: 207.23
 ---- batch: 080 ----
mean loss: 198.98
 ---- batch: 090 ----
mean loss: 202.76
train mean loss: 199.25
epoch train time: 0:00:00.501107
elapsed time: 0:01:08.062783
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:20:43.226976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.30
 ---- batch: 020 ----
mean loss: 191.89
 ---- batch: 030 ----
mean loss: 194.32
 ---- batch: 040 ----
mean loss: 192.73
 ---- batch: 050 ----
mean loss: 194.14
 ---- batch: 060 ----
mean loss: 191.93
 ---- batch: 070 ----
mean loss: 187.58
 ---- batch: 080 ----
mean loss: 203.86
 ---- batch: 090 ----
mean loss: 202.50
train mean loss: 196.71
epoch train time: 0:00:00.493278
elapsed time: 0:01:08.556216
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:20:43.720404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.55
 ---- batch: 020 ----
mean loss: 187.94
 ---- batch: 030 ----
mean loss: 193.76
 ---- batch: 040 ----
mean loss: 203.07
 ---- batch: 050 ----
mean loss: 198.74
 ---- batch: 060 ----
mean loss: 191.48
 ---- batch: 070 ----
mean loss: 196.66
 ---- batch: 080 ----
mean loss: 194.40
 ---- batch: 090 ----
mean loss: 196.38
train mean loss: 194.48
epoch train time: 0:00:00.525165
elapsed time: 0:01:09.081557
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:20:44.245762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.01
 ---- batch: 020 ----
mean loss: 190.84
 ---- batch: 030 ----
mean loss: 196.50
 ---- batch: 040 ----
mean loss: 196.93
 ---- batch: 050 ----
mean loss: 194.71
 ---- batch: 060 ----
mean loss: 194.61
 ---- batch: 070 ----
mean loss: 190.21
 ---- batch: 080 ----
mean loss: 197.75
 ---- batch: 090 ----
mean loss: 198.15
train mean loss: 194.05
epoch train time: 0:00:00.500171
elapsed time: 0:01:09.581913
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:20:44.746099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.77
 ---- batch: 020 ----
mean loss: 175.46
 ---- batch: 030 ----
mean loss: 194.64
 ---- batch: 040 ----
mean loss: 190.38
 ---- batch: 050 ----
mean loss: 200.86
 ---- batch: 060 ----
mean loss: 196.74
 ---- batch: 070 ----
mean loss: 193.81
 ---- batch: 080 ----
mean loss: 197.70
 ---- batch: 090 ----
mean loss: 194.42
train mean loss: 191.72
epoch train time: 0:00:00.510602
elapsed time: 0:01:10.092683
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:20:45.256891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.90
 ---- batch: 020 ----
mean loss: 192.04
 ---- batch: 030 ----
mean loss: 195.43
 ---- batch: 040 ----
mean loss: 189.20
 ---- batch: 050 ----
mean loss: 187.19
 ---- batch: 060 ----
mean loss: 194.44
 ---- batch: 070 ----
mean loss: 185.23
 ---- batch: 080 ----
mean loss: 196.89
 ---- batch: 090 ----
mean loss: 193.79
train mean loss: 191.01
epoch train time: 0:00:00.517832
elapsed time: 0:01:10.610688
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:20:45.774876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.80
 ---- batch: 020 ----
mean loss: 190.14
 ---- batch: 030 ----
mean loss: 186.77
 ---- batch: 040 ----
mean loss: 193.25
 ---- batch: 050 ----
mean loss: 186.77
 ---- batch: 060 ----
mean loss: 189.55
 ---- batch: 070 ----
mean loss: 187.09
 ---- batch: 080 ----
mean loss: 187.41
 ---- batch: 090 ----
mean loss: 194.74
train mean loss: 189.58
epoch train time: 0:00:00.505300
elapsed time: 0:01:11.116136
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:20:46.280323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.80
 ---- batch: 020 ----
mean loss: 187.45
 ---- batch: 030 ----
mean loss: 187.71
 ---- batch: 040 ----
mean loss: 192.96
 ---- batch: 050 ----
mean loss: 185.54
 ---- batch: 060 ----
mean loss: 185.13
 ---- batch: 070 ----
mean loss: 194.66
 ---- batch: 080 ----
mean loss: 188.97
 ---- batch: 090 ----
mean loss: 188.69
train mean loss: 188.53
epoch train time: 0:00:00.503008
elapsed time: 0:01:11.619289
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:20:46.783471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.79
 ---- batch: 020 ----
mean loss: 184.29
 ---- batch: 030 ----
mean loss: 175.39
 ---- batch: 040 ----
mean loss: 185.53
 ---- batch: 050 ----
mean loss: 191.60
 ---- batch: 060 ----
mean loss: 186.33
 ---- batch: 070 ----
mean loss: 192.82
 ---- batch: 080 ----
mean loss: 192.26
 ---- batch: 090 ----
mean loss: 188.07
train mean loss: 186.81
epoch train time: 0:00:00.494229
elapsed time: 0:01:12.113660
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:20:47.277844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.75
 ---- batch: 020 ----
mean loss: 186.94
 ---- batch: 030 ----
mean loss: 183.47
 ---- batch: 040 ----
mean loss: 193.12
 ---- batch: 050 ----
mean loss: 176.68
 ---- batch: 060 ----
mean loss: 184.53
 ---- batch: 070 ----
mean loss: 185.86
 ---- batch: 080 ----
mean loss: 186.52
 ---- batch: 090 ----
mean loss: 184.42
train mean loss: 184.32
epoch train time: 0:00:00.494293
elapsed time: 0:01:12.608098
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:20:47.772282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.03
 ---- batch: 020 ----
mean loss: 180.60
 ---- batch: 030 ----
mean loss: 181.14
 ---- batch: 040 ----
mean loss: 193.47
 ---- batch: 050 ----
mean loss: 181.14
 ---- batch: 060 ----
mean loss: 178.26
 ---- batch: 070 ----
mean loss: 194.81
 ---- batch: 080 ----
mean loss: 186.02
 ---- batch: 090 ----
mean loss: 187.55
train mean loss: 183.68
epoch train time: 0:00:00.500115
elapsed time: 0:01:13.108413
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:20:48.272598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.57
 ---- batch: 020 ----
mean loss: 179.18
 ---- batch: 030 ----
mean loss: 177.24
 ---- batch: 040 ----
mean loss: 191.03
 ---- batch: 050 ----
mean loss: 184.67
 ---- batch: 060 ----
mean loss: 188.71
 ---- batch: 070 ----
mean loss: 177.66
 ---- batch: 080 ----
mean loss: 185.55
 ---- batch: 090 ----
mean loss: 184.88
train mean loss: 182.40
epoch train time: 0:00:00.505689
elapsed time: 0:01:13.614251
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:20:48.778456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.61
 ---- batch: 020 ----
mean loss: 172.46
 ---- batch: 030 ----
mean loss: 176.30
 ---- batch: 040 ----
mean loss: 178.47
 ---- batch: 050 ----
mean loss: 186.17
 ---- batch: 060 ----
mean loss: 182.87
 ---- batch: 070 ----
mean loss: 184.37
 ---- batch: 080 ----
mean loss: 185.86
 ---- batch: 090 ----
mean loss: 186.60
train mean loss: 181.86
epoch train time: 0:00:00.496224
elapsed time: 0:01:14.110657
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:20:49.274867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.39
 ---- batch: 020 ----
mean loss: 180.28
 ---- batch: 030 ----
mean loss: 188.24
 ---- batch: 040 ----
mean loss: 180.37
 ---- batch: 050 ----
mean loss: 181.65
 ---- batch: 060 ----
mean loss: 176.97
 ---- batch: 070 ----
mean loss: 179.58
 ---- batch: 080 ----
mean loss: 177.63
 ---- batch: 090 ----
mean loss: 180.43
train mean loss: 180.01
epoch train time: 0:00:00.505360
elapsed time: 0:01:14.616189
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:20:49.780375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.62
 ---- batch: 020 ----
mean loss: 175.32
 ---- batch: 030 ----
mean loss: 174.40
 ---- batch: 040 ----
mean loss: 179.41
 ---- batch: 050 ----
mean loss: 188.90
 ---- batch: 060 ----
mean loss: 180.85
 ---- batch: 070 ----
mean loss: 174.30
 ---- batch: 080 ----
mean loss: 188.07
 ---- batch: 090 ----
mean loss: 181.14
train mean loss: 179.94
epoch train time: 0:00:00.501671
elapsed time: 0:01:15.118029
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:20:50.282231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.48
 ---- batch: 020 ----
mean loss: 170.58
 ---- batch: 030 ----
mean loss: 178.50
 ---- batch: 040 ----
mean loss: 174.42
 ---- batch: 050 ----
mean loss: 183.62
 ---- batch: 060 ----
mean loss: 184.71
 ---- batch: 070 ----
mean loss: 177.06
 ---- batch: 080 ----
mean loss: 183.77
 ---- batch: 090 ----
mean loss: 182.53
train mean loss: 179.35
epoch train time: 0:00:00.505842
elapsed time: 0:01:15.624080
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:20:50.788264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.75
 ---- batch: 020 ----
mean loss: 171.76
 ---- batch: 030 ----
mean loss: 174.83
 ---- batch: 040 ----
mean loss: 174.45
 ---- batch: 050 ----
mean loss: 179.19
 ---- batch: 060 ----
mean loss: 186.35
 ---- batch: 070 ----
mean loss: 169.53
 ---- batch: 080 ----
mean loss: 178.18
 ---- batch: 090 ----
mean loss: 186.81
train mean loss: 177.82
epoch train time: 0:00:00.500578
elapsed time: 0:01:16.124823
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:20:51.289025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.15
 ---- batch: 020 ----
mean loss: 180.62
 ---- batch: 030 ----
mean loss: 172.15
 ---- batch: 040 ----
mean loss: 179.70
 ---- batch: 050 ----
mean loss: 176.42
 ---- batch: 060 ----
mean loss: 175.94
 ---- batch: 070 ----
mean loss: 167.08
 ---- batch: 080 ----
mean loss: 182.24
 ---- batch: 090 ----
mean loss: 183.67
train mean loss: 177.55
epoch train time: 0:00:00.507105
elapsed time: 0:01:16.632102
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:20:51.796295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.38
 ---- batch: 020 ----
mean loss: 172.62
 ---- batch: 030 ----
mean loss: 178.16
 ---- batch: 040 ----
mean loss: 179.79
 ---- batch: 050 ----
mean loss: 173.54
 ---- batch: 060 ----
mean loss: 177.75
 ---- batch: 070 ----
mean loss: 171.82
 ---- batch: 080 ----
mean loss: 177.13
 ---- batch: 090 ----
mean loss: 167.91
train mean loss: 174.96
epoch train time: 0:00:00.516330
elapsed time: 0:01:17.148627
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:20:52.312846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.98
 ---- batch: 020 ----
mean loss: 170.90
 ---- batch: 030 ----
mean loss: 171.15
 ---- batch: 040 ----
mean loss: 175.65
 ---- batch: 050 ----
mean loss: 160.97
 ---- batch: 060 ----
mean loss: 178.15
 ---- batch: 070 ----
mean loss: 177.78
 ---- batch: 080 ----
mean loss: 178.20
 ---- batch: 090 ----
mean loss: 183.04
train mean loss: 175.12
epoch train time: 0:00:00.521231
elapsed time: 0:01:17.670085
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:20:52.834297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.43
 ---- batch: 020 ----
mean loss: 171.69
 ---- batch: 030 ----
mean loss: 172.28
 ---- batch: 040 ----
mean loss: 173.73
 ---- batch: 050 ----
mean loss: 171.90
 ---- batch: 060 ----
mean loss: 180.15
 ---- batch: 070 ----
mean loss: 174.60
 ---- batch: 080 ----
mean loss: 176.63
 ---- batch: 090 ----
mean loss: 173.22
train mean loss: 174.98
epoch train time: 0:00:00.502885
elapsed time: 0:01:18.173165
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:20:53.337368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.51
 ---- batch: 020 ----
mean loss: 174.26
 ---- batch: 030 ----
mean loss: 174.71
 ---- batch: 040 ----
mean loss: 174.51
 ---- batch: 050 ----
mean loss: 175.83
 ---- batch: 060 ----
mean loss: 173.73
 ---- batch: 070 ----
mean loss: 179.32
 ---- batch: 080 ----
mean loss: 176.70
 ---- batch: 090 ----
mean loss: 174.40
train mean loss: 174.51
epoch train time: 0:00:00.508913
elapsed time: 0:01:18.682251
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:20:53.846439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.20
 ---- batch: 020 ----
mean loss: 168.92
 ---- batch: 030 ----
mean loss: 163.48
 ---- batch: 040 ----
mean loss: 169.43
 ---- batch: 050 ----
mean loss: 174.30
 ---- batch: 060 ----
mean loss: 182.19
 ---- batch: 070 ----
mean loss: 169.59
 ---- batch: 080 ----
mean loss: 180.34
 ---- batch: 090 ----
mean loss: 172.85
train mean loss: 172.38
epoch train time: 0:00:00.508249
elapsed time: 0:01:19.190651
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:20:54.354836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.06
 ---- batch: 020 ----
mean loss: 177.71
 ---- batch: 030 ----
mean loss: 166.89
 ---- batch: 040 ----
mean loss: 165.92
 ---- batch: 050 ----
mean loss: 172.96
 ---- batch: 060 ----
mean loss: 176.36
 ---- batch: 070 ----
mean loss: 172.74
 ---- batch: 080 ----
mean loss: 177.19
 ---- batch: 090 ----
mean loss: 170.03
train mean loss: 172.19
epoch train time: 0:00:00.511119
elapsed time: 0:01:19.701926
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:20:54.866119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.58
 ---- batch: 020 ----
mean loss: 175.83
 ---- batch: 030 ----
mean loss: 170.13
 ---- batch: 040 ----
mean loss: 168.07
 ---- batch: 050 ----
mean loss: 166.11
 ---- batch: 060 ----
mean loss: 172.72
 ---- batch: 070 ----
mean loss: 167.44
 ---- batch: 080 ----
mean loss: 179.12
 ---- batch: 090 ----
mean loss: 170.35
train mean loss: 170.91
epoch train time: 0:00:00.507424
elapsed time: 0:01:20.209529
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:20:55.373718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.50
 ---- batch: 020 ----
mean loss: 170.59
 ---- batch: 030 ----
mean loss: 169.46
 ---- batch: 040 ----
mean loss: 172.88
 ---- batch: 050 ----
mean loss: 169.72
 ---- batch: 060 ----
mean loss: 179.70
 ---- batch: 070 ----
mean loss: 178.52
 ---- batch: 080 ----
mean loss: 172.37
 ---- batch: 090 ----
mean loss: 164.10
train mean loss: 170.49
epoch train time: 0:00:00.503090
elapsed time: 0:01:20.712775
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:20:55.876961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.32
 ---- batch: 020 ----
mean loss: 163.10
 ---- batch: 030 ----
mean loss: 175.48
 ---- batch: 040 ----
mean loss: 164.80
 ---- batch: 050 ----
mean loss: 165.88
 ---- batch: 060 ----
mean loss: 180.61
 ---- batch: 070 ----
mean loss: 170.96
 ---- batch: 080 ----
mean loss: 173.60
 ---- batch: 090 ----
mean loss: 177.74
train mean loss: 170.62
epoch train time: 0:00:00.513137
elapsed time: 0:01:21.226064
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:20:56.390275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.44
 ---- batch: 020 ----
mean loss: 167.85
 ---- batch: 030 ----
mean loss: 165.38
 ---- batch: 040 ----
mean loss: 164.74
 ---- batch: 050 ----
mean loss: 175.74
 ---- batch: 060 ----
mean loss: 176.75
 ---- batch: 070 ----
mean loss: 168.53
 ---- batch: 080 ----
mean loss: 169.10
 ---- batch: 090 ----
mean loss: 168.68
train mean loss: 169.70
epoch train time: 0:00:00.517341
elapsed time: 0:01:21.743578
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:20:56.907786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.16
 ---- batch: 020 ----
mean loss: 159.96
 ---- batch: 030 ----
mean loss: 168.13
 ---- batch: 040 ----
mean loss: 168.84
 ---- batch: 050 ----
mean loss: 169.25
 ---- batch: 060 ----
mean loss: 170.03
 ---- batch: 070 ----
mean loss: 177.71
 ---- batch: 080 ----
mean loss: 171.09
 ---- batch: 090 ----
mean loss: 167.42
train mean loss: 168.87
epoch train time: 0:00:00.505231
elapsed time: 0:01:22.248988
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:20:57.413201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.38
 ---- batch: 020 ----
mean loss: 161.17
 ---- batch: 030 ----
mean loss: 167.65
 ---- batch: 040 ----
mean loss: 171.79
 ---- batch: 050 ----
mean loss: 163.68
 ---- batch: 060 ----
mean loss: 167.96
 ---- batch: 070 ----
mean loss: 170.05
 ---- batch: 080 ----
mean loss: 169.66
 ---- batch: 090 ----
mean loss: 174.85
train mean loss: 167.74
epoch train time: 0:00:00.499012
elapsed time: 0:01:22.748199
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:20:57.912401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.30
 ---- batch: 020 ----
mean loss: 169.66
 ---- batch: 030 ----
mean loss: 163.87
 ---- batch: 040 ----
mean loss: 164.43
 ---- batch: 050 ----
mean loss: 169.00
 ---- batch: 060 ----
mean loss: 169.36
 ---- batch: 070 ----
mean loss: 168.96
 ---- batch: 080 ----
mean loss: 172.64
 ---- batch: 090 ----
mean loss: 163.98
train mean loss: 167.48
epoch train time: 0:00:00.496636
elapsed time: 0:01:23.245016
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:20:58.409240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.02
 ---- batch: 020 ----
mean loss: 159.59
 ---- batch: 030 ----
mean loss: 163.37
 ---- batch: 040 ----
mean loss: 159.77
 ---- batch: 050 ----
mean loss: 171.14
 ---- batch: 060 ----
mean loss: 169.14
 ---- batch: 070 ----
mean loss: 162.78
 ---- batch: 080 ----
mean loss: 174.47
 ---- batch: 090 ----
mean loss: 168.80
train mean loss: 166.83
epoch train time: 0:00:00.498186
elapsed time: 0:01:23.743387
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:20:58.907571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.22
 ---- batch: 020 ----
mean loss: 161.06
 ---- batch: 030 ----
mean loss: 164.39
 ---- batch: 040 ----
mean loss: 164.92
 ---- batch: 050 ----
mean loss: 170.49
 ---- batch: 060 ----
mean loss: 159.58
 ---- batch: 070 ----
mean loss: 161.99
 ---- batch: 080 ----
mean loss: 164.21
 ---- batch: 090 ----
mean loss: 170.19
train mean loss: 165.37
epoch train time: 0:00:00.504558
elapsed time: 0:01:24.248098
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:20:59.412286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.76
 ---- batch: 020 ----
mean loss: 165.34
 ---- batch: 030 ----
mean loss: 163.09
 ---- batch: 040 ----
mean loss: 166.62
 ---- batch: 050 ----
mean loss: 174.53
 ---- batch: 060 ----
mean loss: 169.57
 ---- batch: 070 ----
mean loss: 164.31
 ---- batch: 080 ----
mean loss: 164.09
 ---- batch: 090 ----
mean loss: 162.03
train mean loss: 166.09
epoch train time: 0:00:00.503084
elapsed time: 0:01:24.751332
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:20:59.915518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.13
 ---- batch: 020 ----
mean loss: 160.85
 ---- batch: 030 ----
mean loss: 163.18
 ---- batch: 040 ----
mean loss: 162.97
 ---- batch: 050 ----
mean loss: 169.08
 ---- batch: 060 ----
mean loss: 165.98
 ---- batch: 070 ----
mean loss: 165.94
 ---- batch: 080 ----
mean loss: 162.42
 ---- batch: 090 ----
mean loss: 169.55
train mean loss: 165.18
epoch train time: 0:00:00.491924
elapsed time: 0:01:25.243401
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:21:00.407605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.89
 ---- batch: 020 ----
mean loss: 166.24
 ---- batch: 030 ----
mean loss: 160.04
 ---- batch: 040 ----
mean loss: 163.25
 ---- batch: 050 ----
mean loss: 165.00
 ---- batch: 060 ----
mean loss: 160.35
 ---- batch: 070 ----
mean loss: 171.18
 ---- batch: 080 ----
mean loss: 155.65
 ---- batch: 090 ----
mean loss: 163.65
train mean loss: 163.78
epoch train time: 0:00:00.497475
elapsed time: 0:01:25.741043
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:21:00.905228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.14
 ---- batch: 020 ----
mean loss: 164.92
 ---- batch: 030 ----
mean loss: 156.62
 ---- batch: 040 ----
mean loss: 168.61
 ---- batch: 050 ----
mean loss: 163.70
 ---- batch: 060 ----
mean loss: 167.25
 ---- batch: 070 ----
mean loss: 156.42
 ---- batch: 080 ----
mean loss: 158.49
 ---- batch: 090 ----
mean loss: 160.48
train mean loss: 163.14
epoch train time: 0:00:00.503104
elapsed time: 0:01:26.244294
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:21:01.408482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.01
 ---- batch: 020 ----
mean loss: 155.39
 ---- batch: 030 ----
mean loss: 168.37
 ---- batch: 040 ----
mean loss: 160.09
 ---- batch: 050 ----
mean loss: 157.46
 ---- batch: 060 ----
mean loss: 166.64
 ---- batch: 070 ----
mean loss: 164.52
 ---- batch: 080 ----
mean loss: 166.68
 ---- batch: 090 ----
mean loss: 169.80
train mean loss: 162.98
epoch train time: 0:00:00.516886
elapsed time: 0:01:26.761327
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:21:01.925512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.28
 ---- batch: 020 ----
mean loss: 167.37
 ---- batch: 030 ----
mean loss: 161.10
 ---- batch: 040 ----
mean loss: 160.71
 ---- batch: 050 ----
mean loss: 169.87
 ---- batch: 060 ----
mean loss: 164.00
 ---- batch: 070 ----
mean loss: 165.03
 ---- batch: 080 ----
mean loss: 160.60
 ---- batch: 090 ----
mean loss: 160.95
train mean loss: 163.22
epoch train time: 0:00:00.498031
elapsed time: 0:01:27.259521
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:21:02.423696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.59
 ---- batch: 020 ----
mean loss: 162.90
 ---- batch: 030 ----
mean loss: 161.58
 ---- batch: 040 ----
mean loss: 162.59
 ---- batch: 050 ----
mean loss: 159.89
 ---- batch: 060 ----
mean loss: 163.42
 ---- batch: 070 ----
mean loss: 164.89
 ---- batch: 080 ----
mean loss: 167.08
 ---- batch: 090 ----
mean loss: 161.48
train mean loss: 161.64
epoch train time: 0:00:00.498810
elapsed time: 0:01:27.758471
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:21:02.922659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.34
 ---- batch: 020 ----
mean loss: 162.69
 ---- batch: 030 ----
mean loss: 154.47
 ---- batch: 040 ----
mean loss: 162.40
 ---- batch: 050 ----
mean loss: 158.48
 ---- batch: 060 ----
mean loss: 162.33
 ---- batch: 070 ----
mean loss: 162.33
 ---- batch: 080 ----
mean loss: 160.82
 ---- batch: 090 ----
mean loss: 158.60
train mean loss: 160.97
epoch train time: 0:00:00.496709
elapsed time: 0:01:28.255351
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:21:03.419539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.47
 ---- batch: 020 ----
mean loss: 160.04
 ---- batch: 030 ----
mean loss: 162.41
 ---- batch: 040 ----
mean loss: 155.37
 ---- batch: 050 ----
mean loss: 161.72
 ---- batch: 060 ----
mean loss: 165.63
 ---- batch: 070 ----
mean loss: 163.63
 ---- batch: 080 ----
mean loss: 163.59
 ---- batch: 090 ----
mean loss: 163.34
train mean loss: 161.87
epoch train time: 0:00:00.520269
elapsed time: 0:01:28.775765
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:21:03.939949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.03
 ---- batch: 020 ----
mean loss: 156.36
 ---- batch: 030 ----
mean loss: 162.90
 ---- batch: 040 ----
mean loss: 163.81
 ---- batch: 050 ----
mean loss: 163.12
 ---- batch: 060 ----
mean loss: 168.40
 ---- batch: 070 ----
mean loss: 163.60
 ---- batch: 080 ----
mean loss: 160.58
 ---- batch: 090 ----
mean loss: 161.51
train mean loss: 162.86
epoch train time: 0:00:00.496575
elapsed time: 0:01:29.272494
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:21:04.436712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.74
 ---- batch: 020 ----
mean loss: 157.41
 ---- batch: 030 ----
mean loss: 164.53
 ---- batch: 040 ----
mean loss: 161.79
 ---- batch: 050 ----
mean loss: 162.86
 ---- batch: 060 ----
mean loss: 156.09
 ---- batch: 070 ----
mean loss: 163.17
 ---- batch: 080 ----
mean loss: 157.93
 ---- batch: 090 ----
mean loss: 161.08
train mean loss: 159.54
epoch train time: 0:00:00.499306
elapsed time: 0:01:29.771980
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:21:04.936165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.70
 ---- batch: 020 ----
mean loss: 164.78
 ---- batch: 030 ----
mean loss: 162.16
 ---- batch: 040 ----
mean loss: 154.79
 ---- batch: 050 ----
mean loss: 158.98
 ---- batch: 060 ----
mean loss: 163.13
 ---- batch: 070 ----
mean loss: 164.62
 ---- batch: 080 ----
mean loss: 156.74
 ---- batch: 090 ----
mean loss: 160.52
train mean loss: 160.50
epoch train time: 0:00:00.500530
elapsed time: 0:01:30.272658
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:21:05.436844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.66
 ---- batch: 020 ----
mean loss: 156.82
 ---- batch: 030 ----
mean loss: 156.83
 ---- batch: 040 ----
mean loss: 159.81
 ---- batch: 050 ----
mean loss: 164.72
 ---- batch: 060 ----
mean loss: 163.99
 ---- batch: 070 ----
mean loss: 154.20
 ---- batch: 080 ----
mean loss: 157.86
 ---- batch: 090 ----
mean loss: 160.42
train mean loss: 159.74
epoch train time: 0:00:00.500216
elapsed time: 0:01:30.773024
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:21:05.937210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.14
 ---- batch: 020 ----
mean loss: 157.89
 ---- batch: 030 ----
mean loss: 156.98
 ---- batch: 040 ----
mean loss: 159.34
 ---- batch: 050 ----
mean loss: 159.89
 ---- batch: 060 ----
mean loss: 165.77
 ---- batch: 070 ----
mean loss: 165.70
 ---- batch: 080 ----
mean loss: 150.12
 ---- batch: 090 ----
mean loss: 164.07
train mean loss: 159.55
epoch train time: 0:00:00.505042
elapsed time: 0:01:31.278216
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:21:06.442403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.99
 ---- batch: 020 ----
mean loss: 157.59
 ---- batch: 030 ----
mean loss: 154.39
 ---- batch: 040 ----
mean loss: 150.16
 ---- batch: 050 ----
mean loss: 163.54
 ---- batch: 060 ----
mean loss: 157.15
 ---- batch: 070 ----
mean loss: 161.22
 ---- batch: 080 ----
mean loss: 158.22
 ---- batch: 090 ----
mean loss: 166.33
train mean loss: 158.50
epoch train time: 0:00:00.518594
elapsed time: 0:01:31.796959
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:21:06.961160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.36
 ---- batch: 020 ----
mean loss: 148.78
 ---- batch: 030 ----
mean loss: 157.60
 ---- batch: 040 ----
mean loss: 154.60
 ---- batch: 050 ----
mean loss: 161.35
 ---- batch: 060 ----
mean loss: 162.28
 ---- batch: 070 ----
mean loss: 160.24
 ---- batch: 080 ----
mean loss: 163.06
 ---- batch: 090 ----
mean loss: 164.33
train mean loss: 157.84
epoch train time: 0:00:00.495753
elapsed time: 0:01:32.292884
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:21:07.457074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.03
 ---- batch: 020 ----
mean loss: 146.68
 ---- batch: 030 ----
mean loss: 157.55
 ---- batch: 040 ----
mean loss: 155.19
 ---- batch: 050 ----
mean loss: 157.18
 ---- batch: 060 ----
mean loss: 158.22
 ---- batch: 070 ----
mean loss: 158.91
 ---- batch: 080 ----
mean loss: 159.62
 ---- batch: 090 ----
mean loss: 169.40
train mean loss: 158.16
epoch train time: 0:00:00.506062
elapsed time: 0:01:32.799101
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:21:07.963307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.47
 ---- batch: 020 ----
mean loss: 153.13
 ---- batch: 030 ----
mean loss: 160.12
 ---- batch: 040 ----
mean loss: 158.09
 ---- batch: 050 ----
mean loss: 161.91
 ---- batch: 060 ----
mean loss: 158.84
 ---- batch: 070 ----
mean loss: 156.21
 ---- batch: 080 ----
mean loss: 157.11
 ---- batch: 090 ----
mean loss: 154.88
train mean loss: 157.20
epoch train time: 0:00:00.507537
elapsed time: 0:01:33.306814
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:21:08.471001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.32
 ---- batch: 020 ----
mean loss: 152.71
 ---- batch: 030 ----
mean loss: 155.31
 ---- batch: 040 ----
mean loss: 156.78
 ---- batch: 050 ----
mean loss: 158.53
 ---- batch: 060 ----
mean loss: 160.37
 ---- batch: 070 ----
mean loss: 156.30
 ---- batch: 080 ----
mean loss: 160.00
 ---- batch: 090 ----
mean loss: 156.37
train mean loss: 157.02
epoch train time: 0:00:00.502503
elapsed time: 0:01:33.809502
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:21:08.973698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.35
 ---- batch: 020 ----
mean loss: 158.84
 ---- batch: 030 ----
mean loss: 152.39
 ---- batch: 040 ----
mean loss: 156.21
 ---- batch: 050 ----
mean loss: 162.94
 ---- batch: 060 ----
mean loss: 153.70
 ---- batch: 070 ----
mean loss: 152.22
 ---- batch: 080 ----
mean loss: 158.77
 ---- batch: 090 ----
mean loss: 156.80
train mean loss: 156.03
epoch train time: 0:00:00.501006
elapsed time: 0:01:34.310680
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:21:09.474883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.23
 ---- batch: 020 ----
mean loss: 148.99
 ---- batch: 030 ----
mean loss: 152.87
 ---- batch: 040 ----
mean loss: 155.66
 ---- batch: 050 ----
mean loss: 155.03
 ---- batch: 060 ----
mean loss: 155.03
 ---- batch: 070 ----
mean loss: 157.69
 ---- batch: 080 ----
mean loss: 161.33
 ---- batch: 090 ----
mean loss: 158.50
train mean loss: 155.53
epoch train time: 0:00:00.503756
elapsed time: 0:01:34.814603
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:21:09.978788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.26
 ---- batch: 020 ----
mean loss: 157.97
 ---- batch: 030 ----
mean loss: 151.55
 ---- batch: 040 ----
mean loss: 155.11
 ---- batch: 050 ----
mean loss: 157.05
 ---- batch: 060 ----
mean loss: 157.89
 ---- batch: 070 ----
mean loss: 155.97
 ---- batch: 080 ----
mean loss: 154.84
 ---- batch: 090 ----
mean loss: 157.15
train mean loss: 155.85
epoch train time: 0:00:00.497041
elapsed time: 0:01:35.311793
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:21:10.475979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.97
 ---- batch: 020 ----
mean loss: 147.93
 ---- batch: 030 ----
mean loss: 148.87
 ---- batch: 040 ----
mean loss: 154.00
 ---- batch: 050 ----
mean loss: 159.02
 ---- batch: 060 ----
mean loss: 152.22
 ---- batch: 070 ----
mean loss: 157.26
 ---- batch: 080 ----
mean loss: 161.67
 ---- batch: 090 ----
mean loss: 157.81
train mean loss: 154.16
epoch train time: 0:00:00.506854
elapsed time: 0:01:35.818833
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:21:10.983018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.46
 ---- batch: 020 ----
mean loss: 149.68
 ---- batch: 030 ----
mean loss: 151.31
 ---- batch: 040 ----
mean loss: 155.77
 ---- batch: 050 ----
mean loss: 151.09
 ---- batch: 060 ----
mean loss: 163.95
 ---- batch: 070 ----
mean loss: 153.57
 ---- batch: 080 ----
mean loss: 155.38
 ---- batch: 090 ----
mean loss: 154.73
train mean loss: 154.33
epoch train time: 0:00:00.513354
elapsed time: 0:01:36.332341
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:21:11.496530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.35
 ---- batch: 020 ----
mean loss: 157.11
 ---- batch: 030 ----
mean loss: 148.07
 ---- batch: 040 ----
mean loss: 151.37
 ---- batch: 050 ----
mean loss: 151.41
 ---- batch: 060 ----
mean loss: 150.73
 ---- batch: 070 ----
mean loss: 156.25
 ---- batch: 080 ----
mean loss: 159.11
 ---- batch: 090 ----
mean loss: 161.35
train mean loss: 153.90
epoch train time: 0:00:00.527479
elapsed time: 0:01:36.859984
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:21:12.024214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.30
 ---- batch: 020 ----
mean loss: 153.35
 ---- batch: 030 ----
mean loss: 152.01
 ---- batch: 040 ----
mean loss: 154.34
 ---- batch: 050 ----
mean loss: 150.12
 ---- batch: 060 ----
mean loss: 152.40
 ---- batch: 070 ----
mean loss: 153.21
 ---- batch: 080 ----
mean loss: 160.02
 ---- batch: 090 ----
mean loss: 165.88
train mean loss: 154.41
epoch train time: 0:00:00.499691
elapsed time: 0:01:37.359914
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:21:12.524087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.99
 ---- batch: 020 ----
mean loss: 154.77
 ---- batch: 030 ----
mean loss: 150.42
 ---- batch: 040 ----
mean loss: 157.16
 ---- batch: 050 ----
mean loss: 158.50
 ---- batch: 060 ----
mean loss: 151.51
 ---- batch: 070 ----
mean loss: 150.20
 ---- batch: 080 ----
mean loss: 149.71
 ---- batch: 090 ----
mean loss: 167.95
train mean loss: 154.53
epoch train time: 0:00:00.501217
elapsed time: 0:01:37.861275
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:21:13.025471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.38
 ---- batch: 020 ----
mean loss: 147.15
 ---- batch: 030 ----
mean loss: 150.90
 ---- batch: 040 ----
mean loss: 150.18
 ---- batch: 050 ----
mean loss: 150.78
 ---- batch: 060 ----
mean loss: 148.70
 ---- batch: 070 ----
mean loss: 158.99
 ---- batch: 080 ----
mean loss: 157.40
 ---- batch: 090 ----
mean loss: 159.75
train mean loss: 152.34
epoch train time: 0:00:00.501619
elapsed time: 0:01:38.363093
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:21:13.527291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.41
 ---- batch: 020 ----
mean loss: 152.06
 ---- batch: 030 ----
mean loss: 146.94
 ---- batch: 040 ----
mean loss: 155.10
 ---- batch: 050 ----
mean loss: 154.01
 ---- batch: 060 ----
mean loss: 146.11
 ---- batch: 070 ----
mean loss: 152.00
 ---- batch: 080 ----
mean loss: 156.35
 ---- batch: 090 ----
mean loss: 157.82
train mean loss: 152.90
epoch train time: 0:00:00.502489
elapsed time: 0:01:38.865744
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:21:14.029937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.98
 ---- batch: 020 ----
mean loss: 151.66
 ---- batch: 030 ----
mean loss: 149.55
 ---- batch: 040 ----
mean loss: 154.33
 ---- batch: 050 ----
mean loss: 150.76
 ---- batch: 060 ----
mean loss: 149.22
 ---- batch: 070 ----
mean loss: 154.97
 ---- batch: 080 ----
mean loss: 154.75
 ---- batch: 090 ----
mean loss: 160.31
train mean loss: 152.51
epoch train time: 0:00:00.497419
elapsed time: 0:01:39.363316
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:21:14.527500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.53
 ---- batch: 020 ----
mean loss: 144.72
 ---- batch: 030 ----
mean loss: 159.03
 ---- batch: 040 ----
mean loss: 146.69
 ---- batch: 050 ----
mean loss: 150.40
 ---- batch: 060 ----
mean loss: 157.15
 ---- batch: 070 ----
mean loss: 158.67
 ---- batch: 080 ----
mean loss: 160.85
 ---- batch: 090 ----
mean loss: 152.23
train mean loss: 153.91
epoch train time: 0:00:00.504301
elapsed time: 0:01:39.867766
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:21:15.031952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.15
 ---- batch: 020 ----
mean loss: 152.14
 ---- batch: 030 ----
mean loss: 153.14
 ---- batch: 040 ----
mean loss: 144.86
 ---- batch: 050 ----
mean loss: 146.99
 ---- batch: 060 ----
mean loss: 151.54
 ---- batch: 070 ----
mean loss: 150.86
 ---- batch: 080 ----
mean loss: 156.68
 ---- batch: 090 ----
mean loss: 152.74
train mean loss: 151.86
epoch train time: 0:00:00.506988
elapsed time: 0:01:40.374904
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:21:15.539090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.42
 ---- batch: 020 ----
mean loss: 142.14
 ---- batch: 030 ----
mean loss: 149.25
 ---- batch: 040 ----
mean loss: 145.57
 ---- batch: 050 ----
mean loss: 149.57
 ---- batch: 060 ----
mean loss: 146.09
 ---- batch: 070 ----
mean loss: 156.94
 ---- batch: 080 ----
mean loss: 159.50
 ---- batch: 090 ----
mean loss: 161.93
train mean loss: 151.61
epoch train time: 0:00:00.504722
elapsed time: 0:01:40.879774
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:21:16.043977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.71
 ---- batch: 020 ----
mean loss: 149.97
 ---- batch: 030 ----
mean loss: 146.35
 ---- batch: 040 ----
mean loss: 151.70
 ---- batch: 050 ----
mean loss: 145.96
 ---- batch: 060 ----
mean loss: 159.31
 ---- batch: 070 ----
mean loss: 156.07
 ---- batch: 080 ----
mean loss: 150.80
 ---- batch: 090 ----
mean loss: 144.83
train mean loss: 150.35
epoch train time: 0:00:00.506246
elapsed time: 0:01:41.386188
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:21:16.550375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.42
 ---- batch: 020 ----
mean loss: 143.04
 ---- batch: 030 ----
mean loss: 148.83
 ---- batch: 040 ----
mean loss: 149.35
 ---- batch: 050 ----
mean loss: 149.09
 ---- batch: 060 ----
mean loss: 145.82
 ---- batch: 070 ----
mean loss: 154.11
 ---- batch: 080 ----
mean loss: 153.59
 ---- batch: 090 ----
mean loss: 153.52
train mean loss: 150.58
epoch train time: 0:00:00.506642
elapsed time: 0:01:41.893003
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:21:17.057185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.79
 ---- batch: 020 ----
mean loss: 146.97
 ---- batch: 030 ----
mean loss: 146.36
 ---- batch: 040 ----
mean loss: 149.22
 ---- batch: 050 ----
mean loss: 151.49
 ---- batch: 060 ----
mean loss: 151.58
 ---- batch: 070 ----
mean loss: 153.84
 ---- batch: 080 ----
mean loss: 149.77
 ---- batch: 090 ----
mean loss: 153.78
train mean loss: 150.41
epoch train time: 0:00:00.498543
elapsed time: 0:01:42.391690
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:21:17.555875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.81
 ---- batch: 020 ----
mean loss: 146.79
 ---- batch: 030 ----
mean loss: 143.56
 ---- batch: 040 ----
mean loss: 151.90
 ---- batch: 050 ----
mean loss: 146.62
 ---- batch: 060 ----
mean loss: 157.71
 ---- batch: 070 ----
mean loss: 151.86
 ---- batch: 080 ----
mean loss: 148.51
 ---- batch: 090 ----
mean loss: 151.90
train mean loss: 149.70
epoch train time: 0:00:00.512467
elapsed time: 0:01:42.904330
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:21:18.068506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.97
 ---- batch: 020 ----
mean loss: 148.58
 ---- batch: 030 ----
mean loss: 149.98
 ---- batch: 040 ----
mean loss: 146.20
 ---- batch: 050 ----
mean loss: 147.90
 ---- batch: 060 ----
mean loss: 146.18
 ---- batch: 070 ----
mean loss: 148.15
 ---- batch: 080 ----
mean loss: 154.88
 ---- batch: 090 ----
mean loss: 162.64
train mean loss: 149.77
epoch train time: 0:00:00.510543
elapsed time: 0:01:43.415022
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:21:18.579209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.26
 ---- batch: 020 ----
mean loss: 143.50
 ---- batch: 030 ----
mean loss: 150.94
 ---- batch: 040 ----
mean loss: 146.44
 ---- batch: 050 ----
mean loss: 146.22
 ---- batch: 060 ----
mean loss: 153.25
 ---- batch: 070 ----
mean loss: 156.16
 ---- batch: 080 ----
mean loss: 160.19
 ---- batch: 090 ----
mean loss: 150.83
train mean loss: 149.91
epoch train time: 0:00:00.522084
elapsed time: 0:01:43.937272
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:21:19.101459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.41
 ---- batch: 020 ----
mean loss: 150.89
 ---- batch: 030 ----
mean loss: 146.32
 ---- batch: 040 ----
mean loss: 148.06
 ---- batch: 050 ----
mean loss: 148.35
 ---- batch: 060 ----
mean loss: 150.68
 ---- batch: 070 ----
mean loss: 147.24
 ---- batch: 080 ----
mean loss: 150.62
 ---- batch: 090 ----
mean loss: 150.81
train mean loss: 148.69
epoch train time: 0:00:00.495977
elapsed time: 0:01:44.433408
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:21:19.597631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.17
 ---- batch: 020 ----
mean loss: 144.27
 ---- batch: 030 ----
mean loss: 156.41
 ---- batch: 040 ----
mean loss: 138.44
 ---- batch: 050 ----
mean loss: 152.78
 ---- batch: 060 ----
mean loss: 143.41
 ---- batch: 070 ----
mean loss: 154.16
 ---- batch: 080 ----
mean loss: 147.62
 ---- batch: 090 ----
mean loss: 156.26
train mean loss: 148.96
epoch train time: 0:00:00.501529
elapsed time: 0:01:44.935122
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:21:20.099321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.08
 ---- batch: 020 ----
mean loss: 143.67
 ---- batch: 030 ----
mean loss: 142.74
 ---- batch: 040 ----
mean loss: 147.36
 ---- batch: 050 ----
mean loss: 154.25
 ---- batch: 060 ----
mean loss: 149.11
 ---- batch: 070 ----
mean loss: 150.91
 ---- batch: 080 ----
mean loss: 154.89
 ---- batch: 090 ----
mean loss: 149.58
train mean loss: 149.45
epoch train time: 0:00:00.498667
elapsed time: 0:01:45.433951
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:21:20.598150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.50
 ---- batch: 020 ----
mean loss: 151.66
 ---- batch: 030 ----
mean loss: 142.36
 ---- batch: 040 ----
mean loss: 154.84
 ---- batch: 050 ----
mean loss: 141.53
 ---- batch: 060 ----
mean loss: 139.92
 ---- batch: 070 ----
mean loss: 149.32
 ---- batch: 080 ----
mean loss: 147.91
 ---- batch: 090 ----
mean loss: 149.79
train mean loss: 148.10
epoch train time: 0:00:00.512779
elapsed time: 0:01:45.946892
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:21:21.111077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.44
 ---- batch: 020 ----
mean loss: 151.74
 ---- batch: 030 ----
mean loss: 148.37
 ---- batch: 040 ----
mean loss: 149.17
 ---- batch: 050 ----
mean loss: 145.71
 ---- batch: 060 ----
mean loss: 154.19
 ---- batch: 070 ----
mean loss: 149.52
 ---- batch: 080 ----
mean loss: 143.92
 ---- batch: 090 ----
mean loss: 145.74
train mean loss: 147.37
epoch train time: 0:00:00.502782
elapsed time: 0:01:46.449844
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:21:21.614032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.94
 ---- batch: 020 ----
mean loss: 143.57
 ---- batch: 030 ----
mean loss: 139.98
 ---- batch: 040 ----
mean loss: 147.20
 ---- batch: 050 ----
mean loss: 150.86
 ---- batch: 060 ----
mean loss: 152.19
 ---- batch: 070 ----
mean loss: 148.39
 ---- batch: 080 ----
mean loss: 154.53
 ---- batch: 090 ----
mean loss: 149.93
train mean loss: 147.78
epoch train time: 0:00:00.512637
elapsed time: 0:01:46.962658
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:21:22.126846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.47
 ---- batch: 020 ----
mean loss: 144.80
 ---- batch: 030 ----
mean loss: 148.66
 ---- batch: 040 ----
mean loss: 152.71
 ---- batch: 050 ----
mean loss: 149.76
 ---- batch: 060 ----
mean loss: 149.85
 ---- batch: 070 ----
mean loss: 143.97
 ---- batch: 080 ----
mean loss: 146.08
 ---- batch: 090 ----
mean loss: 146.09
train mean loss: 147.51
epoch train time: 0:00:00.516117
elapsed time: 0:01:47.478923
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:21:22.643109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.08
 ---- batch: 020 ----
mean loss: 146.76
 ---- batch: 030 ----
mean loss: 144.10
 ---- batch: 040 ----
mean loss: 143.20
 ---- batch: 050 ----
mean loss: 144.45
 ---- batch: 060 ----
mean loss: 147.77
 ---- batch: 070 ----
mean loss: 141.95
 ---- batch: 080 ----
mean loss: 153.35
 ---- batch: 090 ----
mean loss: 152.25
train mean loss: 146.35
epoch train time: 0:00:00.519734
elapsed time: 0:01:47.998838
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:21:23.163023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.29
 ---- batch: 020 ----
mean loss: 143.84
 ---- batch: 030 ----
mean loss: 140.79
 ---- batch: 040 ----
mean loss: 145.76
 ---- batch: 050 ----
mean loss: 141.29
 ---- batch: 060 ----
mean loss: 144.23
 ---- batch: 070 ----
mean loss: 149.91
 ---- batch: 080 ----
mean loss: 149.74
 ---- batch: 090 ----
mean loss: 151.79
train mean loss: 146.47
epoch train time: 0:00:00.515977
elapsed time: 0:01:48.514981
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:21:23.679202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.11
 ---- batch: 020 ----
mean loss: 141.86
 ---- batch: 030 ----
mean loss: 143.63
 ---- batch: 040 ----
mean loss: 149.34
 ---- batch: 050 ----
mean loss: 148.35
 ---- batch: 060 ----
mean loss: 148.21
 ---- batch: 070 ----
mean loss: 151.67
 ---- batch: 080 ----
mean loss: 143.41
 ---- batch: 090 ----
mean loss: 151.53
train mean loss: 147.03
epoch train time: 0:00:00.525956
elapsed time: 0:01:49.041122
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:21:24.205308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.89
 ---- batch: 020 ----
mean loss: 147.29
 ---- batch: 030 ----
mean loss: 142.24
 ---- batch: 040 ----
mean loss: 153.81
 ---- batch: 050 ----
mean loss: 138.22
 ---- batch: 060 ----
mean loss: 147.91
 ---- batch: 070 ----
mean loss: 140.49
 ---- batch: 080 ----
mean loss: 151.18
 ---- batch: 090 ----
mean loss: 146.14
train mean loss: 145.69
epoch train time: 0:00:00.517215
elapsed time: 0:01:49.558486
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:21:24.722673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.24
 ---- batch: 020 ----
mean loss: 146.09
 ---- batch: 030 ----
mean loss: 139.53
 ---- batch: 040 ----
mean loss: 141.39
 ---- batch: 050 ----
mean loss: 142.92
 ---- batch: 060 ----
mean loss: 149.92
 ---- batch: 070 ----
mean loss: 145.00
 ---- batch: 080 ----
mean loss: 146.58
 ---- batch: 090 ----
mean loss: 152.85
train mean loss: 145.62
epoch train time: 0:00:00.514837
elapsed time: 0:01:50.073491
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:21:25.237694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.21
 ---- batch: 020 ----
mean loss: 144.80
 ---- batch: 030 ----
mean loss: 146.37
 ---- batch: 040 ----
mean loss: 146.10
 ---- batch: 050 ----
mean loss: 140.51
 ---- batch: 060 ----
mean loss: 146.50
 ---- batch: 070 ----
mean loss: 146.86
 ---- batch: 080 ----
mean loss: 145.48
 ---- batch: 090 ----
mean loss: 150.15
train mean loss: 145.12
epoch train time: 0:00:00.518998
elapsed time: 0:01:50.592654
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:21:25.756841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.51
 ---- batch: 020 ----
mean loss: 137.63
 ---- batch: 030 ----
mean loss: 143.94
 ---- batch: 040 ----
mean loss: 152.97
 ---- batch: 050 ----
mean loss: 141.87
 ---- batch: 060 ----
mean loss: 142.63
 ---- batch: 070 ----
mean loss: 144.77
 ---- batch: 080 ----
mean loss: 142.85
 ---- batch: 090 ----
mean loss: 146.42
train mean loss: 144.65
epoch train time: 0:00:00.521514
elapsed time: 0:01:51.114317
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:21:26.278502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.56
 ---- batch: 020 ----
mean loss: 146.28
 ---- batch: 030 ----
mean loss: 135.55
 ---- batch: 040 ----
mean loss: 143.01
 ---- batch: 050 ----
mean loss: 151.47
 ---- batch: 060 ----
mean loss: 149.37
 ---- batch: 070 ----
mean loss: 149.87
 ---- batch: 080 ----
mean loss: 151.08
 ---- batch: 090 ----
mean loss: 148.61
train mean loss: 146.32
epoch train time: 0:00:00.518617
elapsed time: 0:01:51.633080
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:21:26.797264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.96
 ---- batch: 020 ----
mean loss: 146.47
 ---- batch: 030 ----
mean loss: 144.70
 ---- batch: 040 ----
mean loss: 137.67
 ---- batch: 050 ----
mean loss: 138.97
 ---- batch: 060 ----
mean loss: 152.20
 ---- batch: 070 ----
mean loss: 147.29
 ---- batch: 080 ----
mean loss: 142.30
 ---- batch: 090 ----
mean loss: 143.80
train mean loss: 144.06
epoch train time: 0:00:00.517101
elapsed time: 0:01:52.150343
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:21:27.314545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.30
 ---- batch: 020 ----
mean loss: 139.96
 ---- batch: 030 ----
mean loss: 146.84
 ---- batch: 040 ----
mean loss: 144.54
 ---- batch: 050 ----
mean loss: 140.42
 ---- batch: 060 ----
mean loss: 144.91
 ---- batch: 070 ----
mean loss: 145.23
 ---- batch: 080 ----
mean loss: 155.84
 ---- batch: 090 ----
mean loss: 147.36
train mean loss: 144.90
epoch train time: 0:00:00.510807
elapsed time: 0:01:52.661324
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:21:27.825512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.89
 ---- batch: 020 ----
mean loss: 145.88
 ---- batch: 030 ----
mean loss: 146.05
 ---- batch: 040 ----
mean loss: 142.98
 ---- batch: 050 ----
mean loss: 151.07
 ---- batch: 060 ----
mean loss: 142.28
 ---- batch: 070 ----
mean loss: 138.07
 ---- batch: 080 ----
mean loss: 148.74
 ---- batch: 090 ----
mean loss: 143.63
train mean loss: 145.16
epoch train time: 0:00:00.518298
elapsed time: 0:01:53.179783
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:21:28.343969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.01
 ---- batch: 020 ----
mean loss: 138.01
 ---- batch: 030 ----
mean loss: 141.81
 ---- batch: 040 ----
mean loss: 142.77
 ---- batch: 050 ----
mean loss: 140.27
 ---- batch: 060 ----
mean loss: 135.23
 ---- batch: 070 ----
mean loss: 147.16
 ---- batch: 080 ----
mean loss: 151.40
 ---- batch: 090 ----
mean loss: 146.97
train mean loss: 143.58
epoch train time: 0:00:00.517757
elapsed time: 0:01:53.697703
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:21:28.861887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.34
 ---- batch: 020 ----
mean loss: 137.75
 ---- batch: 030 ----
mean loss: 145.36
 ---- batch: 040 ----
mean loss: 143.58
 ---- batch: 050 ----
mean loss: 150.97
 ---- batch: 060 ----
mean loss: 142.96
 ---- batch: 070 ----
mean loss: 146.16
 ---- batch: 080 ----
mean loss: 140.65
 ---- batch: 090 ----
mean loss: 147.76
train mean loss: 142.96
epoch train time: 0:00:00.518639
elapsed time: 0:01:54.216488
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:21:29.380674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.98
 ---- batch: 020 ----
mean loss: 140.28
 ---- batch: 030 ----
mean loss: 141.99
 ---- batch: 040 ----
mean loss: 140.74
 ---- batch: 050 ----
mean loss: 144.08
 ---- batch: 060 ----
mean loss: 144.62
 ---- batch: 070 ----
mean loss: 148.40
 ---- batch: 080 ----
mean loss: 149.30
 ---- batch: 090 ----
mean loss: 143.47
train mean loss: 143.05
epoch train time: 0:00:00.525084
elapsed time: 0:01:54.741769
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:21:29.905977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.26
 ---- batch: 020 ----
mean loss: 136.97
 ---- batch: 030 ----
mean loss: 142.05
 ---- batch: 040 ----
mean loss: 141.93
 ---- batch: 050 ----
mean loss: 140.14
 ---- batch: 060 ----
mean loss: 146.36
 ---- batch: 070 ----
mean loss: 146.92
 ---- batch: 080 ----
mean loss: 142.27
 ---- batch: 090 ----
mean loss: 148.00
train mean loss: 142.87
epoch train time: 0:00:00.514576
elapsed time: 0:01:55.256511
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:21:30.420695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.01
 ---- batch: 020 ----
mean loss: 148.83
 ---- batch: 030 ----
mean loss: 149.71
 ---- batch: 040 ----
mean loss: 145.54
 ---- batch: 050 ----
mean loss: 135.97
 ---- batch: 060 ----
mean loss: 148.74
 ---- batch: 070 ----
mean loss: 146.10
 ---- batch: 080 ----
mean loss: 145.16
 ---- batch: 090 ----
mean loss: 140.91
train mean loss: 143.60
epoch train time: 0:00:00.517242
elapsed time: 0:01:55.773948
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:21:30.938150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.34
 ---- batch: 020 ----
mean loss: 136.61
 ---- batch: 030 ----
mean loss: 138.49
 ---- batch: 040 ----
mean loss: 141.82
 ---- batch: 050 ----
mean loss: 143.06
 ---- batch: 060 ----
mean loss: 145.54
 ---- batch: 070 ----
mean loss: 149.04
 ---- batch: 080 ----
mean loss: 139.17
 ---- batch: 090 ----
mean loss: 146.45
train mean loss: 142.79
epoch train time: 0:00:00.504653
elapsed time: 0:01:56.278784
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:21:31.443005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.47
 ---- batch: 020 ----
mean loss: 137.83
 ---- batch: 030 ----
mean loss: 139.74
 ---- batch: 040 ----
mean loss: 139.33
 ---- batch: 050 ----
mean loss: 144.47
 ---- batch: 060 ----
mean loss: 144.51
 ---- batch: 070 ----
mean loss: 143.32
 ---- batch: 080 ----
mean loss: 139.58
 ---- batch: 090 ----
mean loss: 145.71
train mean loss: 141.96
epoch train time: 0:00:00.524633
elapsed time: 0:01:56.803601
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:21:31.967791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.49
 ---- batch: 020 ----
mean loss: 143.17
 ---- batch: 030 ----
mean loss: 135.95
 ---- batch: 040 ----
mean loss: 148.04
 ---- batch: 050 ----
mean loss: 143.33
 ---- batch: 060 ----
mean loss: 142.58
 ---- batch: 070 ----
mean loss: 145.88
 ---- batch: 080 ----
mean loss: 137.62
 ---- batch: 090 ----
mean loss: 143.47
train mean loss: 141.84
epoch train time: 0:00:00.531471
elapsed time: 0:01:57.335258
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:21:32.499462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.50
 ---- batch: 020 ----
mean loss: 141.78
 ---- batch: 030 ----
mean loss: 144.72
 ---- batch: 040 ----
mean loss: 138.86
 ---- batch: 050 ----
mean loss: 144.20
 ---- batch: 060 ----
mean loss: 138.85
 ---- batch: 070 ----
mean loss: 137.09
 ---- batch: 080 ----
mean loss: 147.02
 ---- batch: 090 ----
mean loss: 144.60
train mean loss: 142.20
epoch train time: 0:00:00.509933
elapsed time: 0:01:57.845366
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:21:33.009554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.94
 ---- batch: 020 ----
mean loss: 137.56
 ---- batch: 030 ----
mean loss: 148.11
 ---- batch: 040 ----
mean loss: 134.17
 ---- batch: 050 ----
mean loss: 140.72
 ---- batch: 060 ----
mean loss: 138.39
 ---- batch: 070 ----
mean loss: 146.36
 ---- batch: 080 ----
mean loss: 139.36
 ---- batch: 090 ----
mean loss: 141.64
train mean loss: 141.06
epoch train time: 0:00:00.505085
elapsed time: 0:01:58.350617
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:21:33.514801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.35
 ---- batch: 020 ----
mean loss: 144.24
 ---- batch: 030 ----
mean loss: 138.18
 ---- batch: 040 ----
mean loss: 133.84
 ---- batch: 050 ----
mean loss: 145.76
 ---- batch: 060 ----
mean loss: 146.37
 ---- batch: 070 ----
mean loss: 142.06
 ---- batch: 080 ----
mean loss: 143.72
 ---- batch: 090 ----
mean loss: 137.37
train mean loss: 141.05
epoch train time: 0:00:00.499431
elapsed time: 0:01:58.850219
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:21:34.014395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.04
 ---- batch: 020 ----
mean loss: 134.62
 ---- batch: 030 ----
mean loss: 139.06
 ---- batch: 040 ----
mean loss: 146.13
 ---- batch: 050 ----
mean loss: 138.21
 ---- batch: 060 ----
mean loss: 138.58
 ---- batch: 070 ----
mean loss: 140.91
 ---- batch: 080 ----
mean loss: 145.34
 ---- batch: 090 ----
mean loss: 142.52
train mean loss: 140.86
epoch train time: 0:00:00.501402
elapsed time: 0:01:59.351762
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:21:34.515955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.17
 ---- batch: 020 ----
mean loss: 140.26
 ---- batch: 030 ----
mean loss: 136.58
 ---- batch: 040 ----
mean loss: 139.43
 ---- batch: 050 ----
mean loss: 142.97
 ---- batch: 060 ----
mean loss: 137.23
 ---- batch: 070 ----
mean loss: 146.51
 ---- batch: 080 ----
mean loss: 137.56
 ---- batch: 090 ----
mean loss: 144.58
train mean loss: 140.53
epoch train time: 0:00:00.510604
elapsed time: 0:01:59.862578
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:21:35.026788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.20
 ---- batch: 020 ----
mean loss: 138.39
 ---- batch: 030 ----
mean loss: 139.36
 ---- batch: 040 ----
mean loss: 137.95
 ---- batch: 050 ----
mean loss: 136.45
 ---- batch: 060 ----
mean loss: 139.88
 ---- batch: 070 ----
mean loss: 142.99
 ---- batch: 080 ----
mean loss: 140.71
 ---- batch: 090 ----
mean loss: 145.11
train mean loss: 140.02
epoch train time: 0:00:00.514393
elapsed time: 0:02:00.377144
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:21:35.541333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.97
 ---- batch: 020 ----
mean loss: 138.57
 ---- batch: 030 ----
mean loss: 142.44
 ---- batch: 040 ----
mean loss: 141.06
 ---- batch: 050 ----
mean loss: 140.99
 ---- batch: 060 ----
mean loss: 135.71
 ---- batch: 070 ----
mean loss: 142.84
 ---- batch: 080 ----
mean loss: 140.78
 ---- batch: 090 ----
mean loss: 140.69
train mean loss: 141.16
epoch train time: 0:00:00.504529
elapsed time: 0:02:00.881853
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:21:36.046038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.78
 ---- batch: 020 ----
mean loss: 135.06
 ---- batch: 030 ----
mean loss: 144.72
 ---- batch: 040 ----
mean loss: 137.83
 ---- batch: 050 ----
mean loss: 134.00
 ---- batch: 060 ----
mean loss: 145.57
 ---- batch: 070 ----
mean loss: 144.77
 ---- batch: 080 ----
mean loss: 140.53
 ---- batch: 090 ----
mean loss: 143.39
train mean loss: 140.10
epoch train time: 0:00:00.505689
elapsed time: 0:02:01.387705
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:21:36.551882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.41
 ---- batch: 020 ----
mean loss: 142.96
 ---- batch: 030 ----
mean loss: 143.60
 ---- batch: 040 ----
mean loss: 136.44
 ---- batch: 050 ----
mean loss: 137.75
 ---- batch: 060 ----
mean loss: 148.95
 ---- batch: 070 ----
mean loss: 136.63
 ---- batch: 080 ----
mean loss: 140.57
 ---- batch: 090 ----
mean loss: 140.31
train mean loss: 140.37
epoch train time: 0:00:00.506013
elapsed time: 0:02:01.893891
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:21:37.058065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.28
 ---- batch: 020 ----
mean loss: 134.12
 ---- batch: 030 ----
mean loss: 141.37
 ---- batch: 040 ----
mean loss: 144.08
 ---- batch: 050 ----
mean loss: 138.11
 ---- batch: 060 ----
mean loss: 142.24
 ---- batch: 070 ----
mean loss: 144.21
 ---- batch: 080 ----
mean loss: 138.96
 ---- batch: 090 ----
mean loss: 136.92
train mean loss: 139.55
epoch train time: 0:00:00.503286
elapsed time: 0:02:02.397312
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:21:37.561504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.83
 ---- batch: 020 ----
mean loss: 137.11
 ---- batch: 030 ----
mean loss: 135.49
 ---- batch: 040 ----
mean loss: 140.09
 ---- batch: 050 ----
mean loss: 143.71
 ---- batch: 060 ----
mean loss: 143.35
 ---- batch: 070 ----
mean loss: 138.97
 ---- batch: 080 ----
mean loss: 133.07
 ---- batch: 090 ----
mean loss: 143.72
train mean loss: 138.95
epoch train time: 0:00:00.504148
elapsed time: 0:02:02.901614
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:21:38.065807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.58
 ---- batch: 020 ----
mean loss: 135.25
 ---- batch: 030 ----
mean loss: 142.45
 ---- batch: 040 ----
mean loss: 131.68
 ---- batch: 050 ----
mean loss: 134.26
 ---- batch: 060 ----
mean loss: 141.31
 ---- batch: 070 ----
mean loss: 146.90
 ---- batch: 080 ----
mean loss: 138.18
 ---- batch: 090 ----
mean loss: 136.26
train mean loss: 138.18
epoch train time: 0:00:00.499565
elapsed time: 0:02:03.401380
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:21:38.565635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.73
 ---- batch: 020 ----
mean loss: 140.07
 ---- batch: 030 ----
mean loss: 137.28
 ---- batch: 040 ----
mean loss: 140.08
 ---- batch: 050 ----
mean loss: 140.35
 ---- batch: 060 ----
mean loss: 141.95
 ---- batch: 070 ----
mean loss: 136.33
 ---- batch: 080 ----
mean loss: 135.97
 ---- batch: 090 ----
mean loss: 142.25
train mean loss: 138.73
epoch train time: 0:00:00.501597
elapsed time: 0:02:03.903196
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:21:39.067416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.03
 ---- batch: 020 ----
mean loss: 136.53
 ---- batch: 030 ----
mean loss: 142.52
 ---- batch: 040 ----
mean loss: 136.55
 ---- batch: 050 ----
mean loss: 140.48
 ---- batch: 060 ----
mean loss: 140.20
 ---- batch: 070 ----
mean loss: 138.46
 ---- batch: 080 ----
mean loss: 137.79
 ---- batch: 090 ----
mean loss: 145.97
train mean loss: 139.02
epoch train time: 0:00:00.505216
elapsed time: 0:02:04.408593
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:21:39.572777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.73
 ---- batch: 020 ----
mean loss: 137.24
 ---- batch: 030 ----
mean loss: 134.94
 ---- batch: 040 ----
mean loss: 143.22
 ---- batch: 050 ----
mean loss: 141.26
 ---- batch: 060 ----
mean loss: 137.09
 ---- batch: 070 ----
mean loss: 138.77
 ---- batch: 080 ----
mean loss: 135.94
 ---- batch: 090 ----
mean loss: 139.25
train mean loss: 138.69
epoch train time: 0:00:00.504083
elapsed time: 0:02:04.912840
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:21:40.077017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.11
 ---- batch: 020 ----
mean loss: 133.71
 ---- batch: 030 ----
mean loss: 134.57
 ---- batch: 040 ----
mean loss: 140.91
 ---- batch: 050 ----
mean loss: 137.83
 ---- batch: 060 ----
mean loss: 144.31
 ---- batch: 070 ----
mean loss: 138.85
 ---- batch: 080 ----
mean loss: 138.44
 ---- batch: 090 ----
mean loss: 138.18
train mean loss: 138.31
epoch train time: 0:00:00.502427
elapsed time: 0:02:05.415411
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:21:40.579606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.13
 ---- batch: 020 ----
mean loss: 137.22
 ---- batch: 030 ----
mean loss: 141.69
 ---- batch: 040 ----
mean loss: 136.97
 ---- batch: 050 ----
mean loss: 137.79
 ---- batch: 060 ----
mean loss: 141.11
 ---- batch: 070 ----
mean loss: 136.56
 ---- batch: 080 ----
mean loss: 138.82
 ---- batch: 090 ----
mean loss: 131.99
train mean loss: 137.53
epoch train time: 0:00:00.507625
elapsed time: 0:02:05.923203
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:21:41.087383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.23
 ---- batch: 020 ----
mean loss: 137.98
 ---- batch: 030 ----
mean loss: 134.17
 ---- batch: 040 ----
mean loss: 138.73
 ---- batch: 050 ----
mean loss: 138.59
 ---- batch: 060 ----
mean loss: 140.20
 ---- batch: 070 ----
mean loss: 137.29
 ---- batch: 080 ----
mean loss: 136.18
 ---- batch: 090 ----
mean loss: 138.02
train mean loss: 138.33
epoch train time: 0:00:00.511010
elapsed time: 0:02:06.434359
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:21:41.598548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.10
 ---- batch: 020 ----
mean loss: 136.22
 ---- batch: 030 ----
mean loss: 132.90
 ---- batch: 040 ----
mean loss: 136.89
 ---- batch: 050 ----
mean loss: 139.00
 ---- batch: 060 ----
mean loss: 143.29
 ---- batch: 070 ----
mean loss: 138.10
 ---- batch: 080 ----
mean loss: 136.73
 ---- batch: 090 ----
mean loss: 142.08
train mean loss: 137.89
epoch train time: 0:00:00.504367
elapsed time: 0:02:06.938890
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:21:42.103075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.47
 ---- batch: 020 ----
mean loss: 130.63
 ---- batch: 030 ----
mean loss: 129.38
 ---- batch: 040 ----
mean loss: 140.83
 ---- batch: 050 ----
mean loss: 146.14
 ---- batch: 060 ----
mean loss: 143.37
 ---- batch: 070 ----
mean loss: 139.00
 ---- batch: 080 ----
mean loss: 137.10
 ---- batch: 090 ----
mean loss: 138.03
train mean loss: 137.69
epoch train time: 0:00:00.505857
elapsed time: 0:02:07.444908
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:21:42.609104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.68
 ---- batch: 020 ----
mean loss: 129.04
 ---- batch: 030 ----
mean loss: 140.26
 ---- batch: 040 ----
mean loss: 139.09
 ---- batch: 050 ----
mean loss: 136.49
 ---- batch: 060 ----
mean loss: 142.39
 ---- batch: 070 ----
mean loss: 151.26
 ---- batch: 080 ----
mean loss: 134.15
 ---- batch: 090 ----
mean loss: 143.87
train mean loss: 138.87
epoch train time: 0:00:00.517279
elapsed time: 0:02:07.962346
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:21:43.126530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.70
 ---- batch: 020 ----
mean loss: 134.62
 ---- batch: 030 ----
mean loss: 135.82
 ---- batch: 040 ----
mean loss: 134.69
 ---- batch: 050 ----
mean loss: 140.39
 ---- batch: 060 ----
mean loss: 139.67
 ---- batch: 070 ----
mean loss: 134.32
 ---- batch: 080 ----
mean loss: 141.32
 ---- batch: 090 ----
mean loss: 141.18
train mean loss: 137.36
epoch train time: 0:00:00.510351
elapsed time: 0:02:08.472845
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:21:43.637032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.56
 ---- batch: 020 ----
mean loss: 133.23
 ---- batch: 030 ----
mean loss: 130.05
 ---- batch: 040 ----
mean loss: 135.09
 ---- batch: 050 ----
mean loss: 136.36
 ---- batch: 060 ----
mean loss: 139.61
 ---- batch: 070 ----
mean loss: 138.16
 ---- batch: 080 ----
mean loss: 141.10
 ---- batch: 090 ----
mean loss: 139.75
train mean loss: 136.58
epoch train time: 0:00:00.526851
elapsed time: 0:02:08.999884
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:21:44.164105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.83
 ---- batch: 020 ----
mean loss: 130.93
 ---- batch: 030 ----
mean loss: 139.50
 ---- batch: 040 ----
mean loss: 132.99
 ---- batch: 050 ----
mean loss: 135.86
 ---- batch: 060 ----
mean loss: 133.74
 ---- batch: 070 ----
mean loss: 135.50
 ---- batch: 080 ----
mean loss: 142.44
 ---- batch: 090 ----
mean loss: 144.82
train mean loss: 136.92
epoch train time: 0:00:00.506502
elapsed time: 0:02:09.506576
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:21:44.670762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.66
 ---- batch: 020 ----
mean loss: 137.93
 ---- batch: 030 ----
mean loss: 136.14
 ---- batch: 040 ----
mean loss: 138.29
 ---- batch: 050 ----
mean loss: 135.97
 ---- batch: 060 ----
mean loss: 134.97
 ---- batch: 070 ----
mean loss: 136.20
 ---- batch: 080 ----
mean loss: 141.66
 ---- batch: 090 ----
mean loss: 134.91
train mean loss: 136.48
epoch train time: 0:00:00.513366
elapsed time: 0:02:10.020091
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:21:45.184277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.60
 ---- batch: 020 ----
mean loss: 133.17
 ---- batch: 030 ----
mean loss: 136.38
 ---- batch: 040 ----
mean loss: 133.85
 ---- batch: 050 ----
mean loss: 136.93
 ---- batch: 060 ----
mean loss: 136.35
 ---- batch: 070 ----
mean loss: 139.61
 ---- batch: 080 ----
mean loss: 139.03
 ---- batch: 090 ----
mean loss: 135.09
train mean loss: 135.48
epoch train time: 0:00:00.512335
elapsed time: 0:02:10.532578
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:21:45.696764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.42
 ---- batch: 020 ----
mean loss: 138.49
 ---- batch: 030 ----
mean loss: 141.46
 ---- batch: 040 ----
mean loss: 131.83
 ---- batch: 050 ----
mean loss: 138.44
 ---- batch: 060 ----
mean loss: 141.32
 ---- batch: 070 ----
mean loss: 132.31
 ---- batch: 080 ----
mean loss: 128.89
 ---- batch: 090 ----
mean loss: 136.96
train mean loss: 135.56
epoch train time: 0:00:00.503806
elapsed time: 0:02:11.036530
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:21:46.200734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.02
 ---- batch: 020 ----
mean loss: 132.44
 ---- batch: 030 ----
mean loss: 132.73
 ---- batch: 040 ----
mean loss: 136.96
 ---- batch: 050 ----
mean loss: 137.08
 ---- batch: 060 ----
mean loss: 128.89
 ---- batch: 070 ----
mean loss: 134.16
 ---- batch: 080 ----
mean loss: 137.52
 ---- batch: 090 ----
mean loss: 143.18
train mean loss: 135.10
epoch train time: 0:00:00.498381
elapsed time: 0:02:11.535096
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:21:46.699292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.54
 ---- batch: 020 ----
mean loss: 134.72
 ---- batch: 030 ----
mean loss: 129.32
 ---- batch: 040 ----
mean loss: 135.70
 ---- batch: 050 ----
mean loss: 138.76
 ---- batch: 060 ----
mean loss: 139.46
 ---- batch: 070 ----
mean loss: 132.36
 ---- batch: 080 ----
mean loss: 137.08
 ---- batch: 090 ----
mean loss: 138.04
train mean loss: 136.10
epoch train time: 0:00:00.509126
elapsed time: 0:02:12.044390
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:21:47.208569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.35
 ---- batch: 020 ----
mean loss: 139.92
 ---- batch: 030 ----
mean loss: 140.97
 ---- batch: 040 ----
mean loss: 129.82
 ---- batch: 050 ----
mean loss: 132.74
 ---- batch: 060 ----
mean loss: 134.22
 ---- batch: 070 ----
mean loss: 137.90
 ---- batch: 080 ----
mean loss: 141.72
 ---- batch: 090 ----
mean loss: 139.60
train mean loss: 136.68
epoch train time: 0:00:00.510511
elapsed time: 0:02:12.555053
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:21:47.719238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.18
 ---- batch: 020 ----
mean loss: 134.84
 ---- batch: 030 ----
mean loss: 134.00
 ---- batch: 040 ----
mean loss: 132.72
 ---- batch: 050 ----
mean loss: 130.60
 ---- batch: 060 ----
mean loss: 137.34
 ---- batch: 070 ----
mean loss: 138.96
 ---- batch: 080 ----
mean loss: 129.27
 ---- batch: 090 ----
mean loss: 144.23
train mean loss: 135.66
epoch train time: 0:00:00.496436
elapsed time: 0:02:13.051679
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:21:48.215864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.20
 ---- batch: 020 ----
mean loss: 130.31
 ---- batch: 030 ----
mean loss: 132.02
 ---- batch: 040 ----
mean loss: 132.71
 ---- batch: 050 ----
mean loss: 131.52
 ---- batch: 060 ----
mean loss: 140.05
 ---- batch: 070 ----
mean loss: 135.80
 ---- batch: 080 ----
mean loss: 134.86
 ---- batch: 090 ----
mean loss: 145.05
train mean loss: 135.24
epoch train time: 0:00:00.497442
elapsed time: 0:02:13.549283
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:21:48.713488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.53
 ---- batch: 020 ----
mean loss: 128.62
 ---- batch: 030 ----
mean loss: 130.33
 ---- batch: 040 ----
mean loss: 136.77
 ---- batch: 050 ----
mean loss: 140.67
 ---- batch: 060 ----
mean loss: 141.06
 ---- batch: 070 ----
mean loss: 137.09
 ---- batch: 080 ----
mean loss: 140.01
 ---- batch: 090 ----
mean loss: 135.82
train mean loss: 134.70
epoch train time: 0:00:00.510555
elapsed time: 0:02:14.060014
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:21:49.224199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.90
 ---- batch: 020 ----
mean loss: 132.08
 ---- batch: 030 ----
mean loss: 135.17
 ---- batch: 040 ----
mean loss: 137.10
 ---- batch: 050 ----
mean loss: 134.71
 ---- batch: 060 ----
mean loss: 137.25
 ---- batch: 070 ----
mean loss: 137.75
 ---- batch: 080 ----
mean loss: 137.59
 ---- batch: 090 ----
mean loss: 135.62
train mean loss: 135.87
epoch train time: 0:00:00.507132
elapsed time: 0:02:14.567295
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:21:49.731480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.06
 ---- batch: 020 ----
mean loss: 137.08
 ---- batch: 030 ----
mean loss: 126.15
 ---- batch: 040 ----
mean loss: 133.35
 ---- batch: 050 ----
mean loss: 134.03
 ---- batch: 060 ----
mean loss: 135.94
 ---- batch: 070 ----
mean loss: 136.54
 ---- batch: 080 ----
mean loss: 141.32
 ---- batch: 090 ----
mean loss: 140.07
train mean loss: 135.49
epoch train time: 0:00:00.505295
elapsed time: 0:02:15.072737
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:21:50.236941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.27
 ---- batch: 020 ----
mean loss: 129.38
 ---- batch: 030 ----
mean loss: 126.39
 ---- batch: 040 ----
mean loss: 134.65
 ---- batch: 050 ----
mean loss: 125.31
 ---- batch: 060 ----
mean loss: 123.53
 ---- batch: 070 ----
mean loss: 129.74
 ---- batch: 080 ----
mean loss: 131.14
 ---- batch: 090 ----
mean loss: 126.53
train mean loss: 129.03
epoch train time: 0:00:00.508029
elapsed time: 0:02:15.580965
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:21:50.745140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.87
 ---- batch: 020 ----
mean loss: 126.96
 ---- batch: 030 ----
mean loss: 129.41
 ---- batch: 040 ----
mean loss: 121.18
 ---- batch: 050 ----
mean loss: 129.78
 ---- batch: 060 ----
mean loss: 129.02
 ---- batch: 070 ----
mean loss: 124.38
 ---- batch: 080 ----
mean loss: 127.07
 ---- batch: 090 ----
mean loss: 125.49
train mean loss: 127.21
epoch train time: 0:00:00.500309
elapsed time: 0:02:16.081410
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:21:51.245594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.36
 ---- batch: 020 ----
mean loss: 130.81
 ---- batch: 030 ----
mean loss: 122.05
 ---- batch: 040 ----
mean loss: 128.66
 ---- batch: 050 ----
mean loss: 129.40
 ---- batch: 060 ----
mean loss: 120.77
 ---- batch: 070 ----
mean loss: 124.63
 ---- batch: 080 ----
mean loss: 129.48
 ---- batch: 090 ----
mean loss: 129.67
train mean loss: 126.83
epoch train time: 0:00:00.503784
elapsed time: 0:02:16.585359
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:21:51.749556
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.67
 ---- batch: 020 ----
mean loss: 123.19
 ---- batch: 030 ----
mean loss: 133.52
 ---- batch: 040 ----
mean loss: 121.62
 ---- batch: 050 ----
mean loss: 125.78
 ---- batch: 060 ----
mean loss: 124.34
 ---- batch: 070 ----
mean loss: 124.28
 ---- batch: 080 ----
mean loss: 133.85
 ---- batch: 090 ----
mean loss: 126.34
train mean loss: 126.57
epoch train time: 0:00:00.516114
elapsed time: 0:02:17.101638
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:21:52.265823
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.29
 ---- batch: 020 ----
mean loss: 129.63
 ---- batch: 030 ----
mean loss: 126.79
 ---- batch: 040 ----
mean loss: 120.77
 ---- batch: 050 ----
mean loss: 128.34
 ---- batch: 060 ----
mean loss: 126.96
 ---- batch: 070 ----
mean loss: 125.94
 ---- batch: 080 ----
mean loss: 131.79
 ---- batch: 090 ----
mean loss: 126.27
train mean loss: 126.76
epoch train time: 0:00:00.511408
elapsed time: 0:02:17.613192
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:21:52.777376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.57
 ---- batch: 020 ----
mean loss: 125.74
 ---- batch: 030 ----
mean loss: 128.85
 ---- batch: 040 ----
mean loss: 130.44
 ---- batch: 050 ----
mean loss: 133.04
 ---- batch: 060 ----
mean loss: 123.41
 ---- batch: 070 ----
mean loss: 125.22
 ---- batch: 080 ----
mean loss: 123.54
 ---- batch: 090 ----
mean loss: 123.80
train mean loss: 126.59
epoch train time: 0:00:00.510772
elapsed time: 0:02:18.124112
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:21:53.288297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.23
 ---- batch: 020 ----
mean loss: 122.83
 ---- batch: 030 ----
mean loss: 128.05
 ---- batch: 040 ----
mean loss: 128.88
 ---- batch: 050 ----
mean loss: 126.56
 ---- batch: 060 ----
mean loss: 119.99
 ---- batch: 070 ----
mean loss: 127.48
 ---- batch: 080 ----
mean loss: 127.83
 ---- batch: 090 ----
mean loss: 127.38
train mean loss: 126.61
epoch train time: 0:00:00.525444
elapsed time: 0:02:18.649712
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:21:53.813922
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.51
 ---- batch: 020 ----
mean loss: 128.89
 ---- batch: 030 ----
mean loss: 128.96
 ---- batch: 040 ----
mean loss: 127.29
 ---- batch: 050 ----
mean loss: 127.55
 ---- batch: 060 ----
mean loss: 128.23
 ---- batch: 070 ----
mean loss: 124.11
 ---- batch: 080 ----
mean loss: 125.68
 ---- batch: 090 ----
mean loss: 120.65
train mean loss: 126.35
epoch train time: 0:00:00.504619
elapsed time: 0:02:19.154511
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:21:54.318722
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.07
 ---- batch: 020 ----
mean loss: 129.06
 ---- batch: 030 ----
mean loss: 124.74
 ---- batch: 040 ----
mean loss: 125.26
 ---- batch: 050 ----
mean loss: 129.22
 ---- batch: 060 ----
mean loss: 128.63
 ---- batch: 070 ----
mean loss: 120.41
 ---- batch: 080 ----
mean loss: 124.84
 ---- batch: 090 ----
mean loss: 127.57
train mean loss: 126.22
epoch train time: 0:00:00.496390
elapsed time: 0:02:19.651086
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:21:54.815272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.30
 ---- batch: 020 ----
mean loss: 126.56
 ---- batch: 030 ----
mean loss: 131.94
 ---- batch: 040 ----
mean loss: 126.86
 ---- batch: 050 ----
mean loss: 122.42
 ---- batch: 060 ----
mean loss: 129.96
 ---- batch: 070 ----
mean loss: 123.40
 ---- batch: 080 ----
mean loss: 129.78
 ---- batch: 090 ----
mean loss: 124.28
train mean loss: 126.26
epoch train time: 0:00:00.495489
elapsed time: 0:02:20.146734
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:21:55.310911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.63
 ---- batch: 020 ----
mean loss: 119.78
 ---- batch: 030 ----
mean loss: 130.28
 ---- batch: 040 ----
mean loss: 126.94
 ---- batch: 050 ----
mean loss: 125.07
 ---- batch: 060 ----
mean loss: 127.25
 ---- batch: 070 ----
mean loss: 128.05
 ---- batch: 080 ----
mean loss: 125.74
 ---- batch: 090 ----
mean loss: 126.36
train mean loss: 126.26
epoch train time: 0:00:00.500493
elapsed time: 0:02:20.647365
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:21:55.811550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.13
 ---- batch: 020 ----
mean loss: 122.15
 ---- batch: 030 ----
mean loss: 125.43
 ---- batch: 040 ----
mean loss: 131.22
 ---- batch: 050 ----
mean loss: 126.42
 ---- batch: 060 ----
mean loss: 121.07
 ---- batch: 070 ----
mean loss: 128.90
 ---- batch: 080 ----
mean loss: 127.81
 ---- batch: 090 ----
mean loss: 125.82
train mean loss: 126.15
epoch train time: 0:00:00.514114
elapsed time: 0:02:21.161628
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:21:56.325812
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.56
 ---- batch: 020 ----
mean loss: 124.23
 ---- batch: 030 ----
mean loss: 120.44
 ---- batch: 040 ----
mean loss: 128.49
 ---- batch: 050 ----
mean loss: 130.60
 ---- batch: 060 ----
mean loss: 128.91
 ---- batch: 070 ----
mean loss: 124.63
 ---- batch: 080 ----
mean loss: 126.22
 ---- batch: 090 ----
mean loss: 128.62
train mean loss: 126.20
epoch train time: 0:00:00.510839
elapsed time: 0:02:21.672620
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:21:56.836809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.77
 ---- batch: 020 ----
mean loss: 118.75
 ---- batch: 030 ----
mean loss: 125.44
 ---- batch: 040 ----
mean loss: 123.54
 ---- batch: 050 ----
mean loss: 122.97
 ---- batch: 060 ----
mean loss: 126.95
 ---- batch: 070 ----
mean loss: 129.64
 ---- batch: 080 ----
mean loss: 134.27
 ---- batch: 090 ----
mean loss: 128.92
train mean loss: 126.24
epoch train time: 0:00:00.502860
elapsed time: 0:02:22.175627
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:21:57.339810
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.10
 ---- batch: 020 ----
mean loss: 123.88
 ---- batch: 030 ----
mean loss: 121.62
 ---- batch: 040 ----
mean loss: 122.59
 ---- batch: 050 ----
mean loss: 125.23
 ---- batch: 060 ----
mean loss: 128.76
 ---- batch: 070 ----
mean loss: 127.36
 ---- batch: 080 ----
mean loss: 133.42
 ---- batch: 090 ----
mean loss: 126.16
train mean loss: 126.19
epoch train time: 0:00:00.523506
elapsed time: 0:02:22.699286
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:21:57.863494
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.79
 ---- batch: 020 ----
mean loss: 120.32
 ---- batch: 030 ----
mean loss: 125.95
 ---- batch: 040 ----
mean loss: 128.89
 ---- batch: 050 ----
mean loss: 124.93
 ---- batch: 060 ----
mean loss: 123.82
 ---- batch: 070 ----
mean loss: 127.98
 ---- batch: 080 ----
mean loss: 127.48
 ---- batch: 090 ----
mean loss: 129.30
train mean loss: 125.97
epoch train time: 0:00:00.517018
elapsed time: 0:02:23.216474
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:21:58.380674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.48
 ---- batch: 020 ----
mean loss: 128.14
 ---- batch: 030 ----
mean loss: 131.77
 ---- batch: 040 ----
mean loss: 120.59
 ---- batch: 050 ----
mean loss: 123.37
 ---- batch: 060 ----
mean loss: 125.47
 ---- batch: 070 ----
mean loss: 125.69
 ---- batch: 080 ----
mean loss: 129.33
 ---- batch: 090 ----
mean loss: 124.03
train mean loss: 126.04
epoch train time: 0:00:00.507460
elapsed time: 0:02:23.724095
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:21:58.888280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.15
 ---- batch: 020 ----
mean loss: 124.71
 ---- batch: 030 ----
mean loss: 127.86
 ---- batch: 040 ----
mean loss: 124.25
 ---- batch: 050 ----
mean loss: 123.93
 ---- batch: 060 ----
mean loss: 125.80
 ---- batch: 070 ----
mean loss: 124.67
 ---- batch: 080 ----
mean loss: 131.31
 ---- batch: 090 ----
mean loss: 133.72
train mean loss: 125.94
epoch train time: 0:00:00.498501
elapsed time: 0:02:24.222763
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:21:59.386946
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.89
 ---- batch: 020 ----
mean loss: 125.19
 ---- batch: 030 ----
mean loss: 129.68
 ---- batch: 040 ----
mean loss: 131.36
 ---- batch: 050 ----
mean loss: 128.58
 ---- batch: 060 ----
mean loss: 122.38
 ---- batch: 070 ----
mean loss: 120.32
 ---- batch: 080 ----
mean loss: 128.07
 ---- batch: 090 ----
mean loss: 125.79
train mean loss: 125.85
epoch train time: 0:00:00.523002
elapsed time: 0:02:24.745939
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:21:59.910127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.95
 ---- batch: 020 ----
mean loss: 123.59
 ---- batch: 030 ----
mean loss: 123.89
 ---- batch: 040 ----
mean loss: 124.79
 ---- batch: 050 ----
mean loss: 124.89
 ---- batch: 060 ----
mean loss: 129.36
 ---- batch: 070 ----
mean loss: 128.09
 ---- batch: 080 ----
mean loss: 128.56
 ---- batch: 090 ----
mean loss: 124.66
train mean loss: 125.85
epoch train time: 0:00:00.511562
elapsed time: 0:02:25.257666
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:22:00.421858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.88
 ---- batch: 020 ----
mean loss: 123.04
 ---- batch: 030 ----
mean loss: 127.15
 ---- batch: 040 ----
mean loss: 125.56
 ---- batch: 050 ----
mean loss: 126.16
 ---- batch: 060 ----
mean loss: 132.70
 ---- batch: 070 ----
mean loss: 121.37
 ---- batch: 080 ----
mean loss: 131.93
 ---- batch: 090 ----
mean loss: 123.39
train mean loss: 125.73
epoch train time: 0:00:00.513449
elapsed time: 0:02:25.771271
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:22:00.935456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.87
 ---- batch: 020 ----
mean loss: 126.20
 ---- batch: 030 ----
mean loss: 128.13
 ---- batch: 040 ----
mean loss: 120.68
 ---- batch: 050 ----
mean loss: 121.71
 ---- batch: 060 ----
mean loss: 128.23
 ---- batch: 070 ----
mean loss: 124.85
 ---- batch: 080 ----
mean loss: 133.14
 ---- batch: 090 ----
mean loss: 126.20
train mean loss: 125.88
epoch train time: 0:00:00.513104
elapsed time: 0:02:26.284534
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:22:01.448722
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.50
 ---- batch: 020 ----
mean loss: 128.41
 ---- batch: 030 ----
mean loss: 129.25
 ---- batch: 040 ----
mean loss: 123.33
 ---- batch: 050 ----
mean loss: 124.79
 ---- batch: 060 ----
mean loss: 125.87
 ---- batch: 070 ----
mean loss: 126.21
 ---- batch: 080 ----
mean loss: 123.81
 ---- batch: 090 ----
mean loss: 131.29
train mean loss: 125.77
epoch train time: 0:00:00.507749
elapsed time: 0:02:26.792433
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:22:01.956638
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.74
 ---- batch: 020 ----
mean loss: 120.22
 ---- batch: 030 ----
mean loss: 127.26
 ---- batch: 040 ----
mean loss: 119.44
 ---- batch: 050 ----
mean loss: 123.04
 ---- batch: 060 ----
mean loss: 126.15
 ---- batch: 070 ----
mean loss: 131.35
 ---- batch: 080 ----
mean loss: 132.21
 ---- batch: 090 ----
mean loss: 129.71
train mean loss: 126.03
epoch train time: 0:00:00.505452
elapsed time: 0:02:27.298056
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:22:02.462244
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.69
 ---- batch: 020 ----
mean loss: 128.70
 ---- batch: 030 ----
mean loss: 125.47
 ---- batch: 040 ----
mean loss: 128.91
 ---- batch: 050 ----
mean loss: 123.18
 ---- batch: 060 ----
mean loss: 127.12
 ---- batch: 070 ----
mean loss: 121.69
 ---- batch: 080 ----
mean loss: 123.72
 ---- batch: 090 ----
mean loss: 130.26
train mean loss: 125.78
epoch train time: 0:00:00.513950
elapsed time: 0:02:27.812155
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:22:02.976339
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.63
 ---- batch: 020 ----
mean loss: 132.88
 ---- batch: 030 ----
mean loss: 122.43
 ---- batch: 040 ----
mean loss: 124.04
 ---- batch: 050 ----
mean loss: 125.50
 ---- batch: 060 ----
mean loss: 124.42
 ---- batch: 070 ----
mean loss: 122.14
 ---- batch: 080 ----
mean loss: 126.30
 ---- batch: 090 ----
mean loss: 129.18
train mean loss: 125.86
epoch train time: 0:00:00.497312
elapsed time: 0:02:28.309614
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:22:03.473800
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.86
 ---- batch: 020 ----
mean loss: 131.54
 ---- batch: 030 ----
mean loss: 128.28
 ---- batch: 040 ----
mean loss: 133.05
 ---- batch: 050 ----
mean loss: 130.36
 ---- batch: 060 ----
mean loss: 122.19
 ---- batch: 070 ----
mean loss: 120.21
 ---- batch: 080 ----
mean loss: 127.26
 ---- batch: 090 ----
mean loss: 120.67
train mean loss: 125.80
epoch train time: 0:00:00.502241
elapsed time: 0:02:28.811998
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:22:03.976180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.09
 ---- batch: 020 ----
mean loss: 123.39
 ---- batch: 030 ----
mean loss: 130.62
 ---- batch: 040 ----
mean loss: 126.44
 ---- batch: 050 ----
mean loss: 126.67
 ---- batch: 060 ----
mean loss: 125.91
 ---- batch: 070 ----
mean loss: 126.42
 ---- batch: 080 ----
mean loss: 123.62
 ---- batch: 090 ----
mean loss: 125.82
train mean loss: 125.66
epoch train time: 0:00:00.493345
elapsed time: 0:02:29.305522
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:22:04.469710
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.18
 ---- batch: 020 ----
mean loss: 129.40
 ---- batch: 030 ----
mean loss: 122.55
 ---- batch: 040 ----
mean loss: 130.56
 ---- batch: 050 ----
mean loss: 128.78
 ---- batch: 060 ----
mean loss: 127.65
 ---- batch: 070 ----
mean loss: 122.90
 ---- batch: 080 ----
mean loss: 121.67
 ---- batch: 090 ----
mean loss: 129.53
train mean loss: 125.63
epoch train time: 0:00:00.496781
elapsed time: 0:02:29.802506
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:22:04.966691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.03
 ---- batch: 020 ----
mean loss: 124.74
 ---- batch: 030 ----
mean loss: 127.47
 ---- batch: 040 ----
mean loss: 123.33
 ---- batch: 050 ----
mean loss: 122.06
 ---- batch: 060 ----
mean loss: 126.78
 ---- batch: 070 ----
mean loss: 129.64
 ---- batch: 080 ----
mean loss: 128.26
 ---- batch: 090 ----
mean loss: 123.69
train mean loss: 125.63
epoch train time: 0:00:00.492639
elapsed time: 0:02:30.295334
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:22:05.459516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.49
 ---- batch: 020 ----
mean loss: 123.38
 ---- batch: 030 ----
mean loss: 128.16
 ---- batch: 040 ----
mean loss: 128.54
 ---- batch: 050 ----
mean loss: 122.70
 ---- batch: 060 ----
mean loss: 120.31
 ---- batch: 070 ----
mean loss: 125.64
 ---- batch: 080 ----
mean loss: 128.61
 ---- batch: 090 ----
mean loss: 123.91
train mean loss: 125.56
epoch train time: 0:00:00.499453
elapsed time: 0:02:30.794952
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:22:05.959151
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.39
 ---- batch: 020 ----
mean loss: 123.56
 ---- batch: 030 ----
mean loss: 120.81
 ---- batch: 040 ----
mean loss: 124.72
 ---- batch: 050 ----
mean loss: 124.89
 ---- batch: 060 ----
mean loss: 125.13
 ---- batch: 070 ----
mean loss: 131.15
 ---- batch: 080 ----
mean loss: 125.24
 ---- batch: 090 ----
mean loss: 123.99
train mean loss: 125.25
epoch train time: 0:00:00.517031
elapsed time: 0:02:31.312144
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:22:06.476347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.45
 ---- batch: 020 ----
mean loss: 125.18
 ---- batch: 030 ----
mean loss: 124.47
 ---- batch: 040 ----
mean loss: 124.25
 ---- batch: 050 ----
mean loss: 123.59
 ---- batch: 060 ----
mean loss: 124.28
 ---- batch: 070 ----
mean loss: 119.63
 ---- batch: 080 ----
mean loss: 130.52
 ---- batch: 090 ----
mean loss: 125.09
train mean loss: 125.71
epoch train time: 0:00:00.501217
elapsed time: 0:02:31.813564
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:22:06.977740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.06
 ---- batch: 020 ----
mean loss: 126.58
 ---- batch: 030 ----
mean loss: 125.01
 ---- batch: 040 ----
mean loss: 125.30
 ---- batch: 050 ----
mean loss: 126.53
 ---- batch: 060 ----
mean loss: 125.83
 ---- batch: 070 ----
mean loss: 128.36
 ---- batch: 080 ----
mean loss: 123.29
 ---- batch: 090 ----
mean loss: 130.98
train mean loss: 125.52
epoch train time: 0:00:00.510377
elapsed time: 0:02:32.324099
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:22:07.488287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.14
 ---- batch: 020 ----
mean loss: 126.96
 ---- batch: 030 ----
mean loss: 119.70
 ---- batch: 040 ----
mean loss: 120.64
 ---- batch: 050 ----
mean loss: 126.33
 ---- batch: 060 ----
mean loss: 128.35
 ---- batch: 070 ----
mean loss: 129.77
 ---- batch: 080 ----
mean loss: 125.13
 ---- batch: 090 ----
mean loss: 125.90
train mean loss: 125.46
epoch train time: 0:00:00.504464
elapsed time: 0:02:32.828742
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:22:07.992967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.01
 ---- batch: 020 ----
mean loss: 122.33
 ---- batch: 030 ----
mean loss: 128.36
 ---- batch: 040 ----
mean loss: 128.90
 ---- batch: 050 ----
mean loss: 125.36
 ---- batch: 060 ----
mean loss: 121.21
 ---- batch: 070 ----
mean loss: 122.63
 ---- batch: 080 ----
mean loss: 125.86
 ---- batch: 090 ----
mean loss: 124.17
train mean loss: 125.35
epoch train time: 0:00:00.504842
elapsed time: 0:02:33.333783
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:22:08.497970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.05
 ---- batch: 020 ----
mean loss: 121.90
 ---- batch: 030 ----
mean loss: 134.92
 ---- batch: 040 ----
mean loss: 122.85
 ---- batch: 050 ----
mean loss: 123.31
 ---- batch: 060 ----
mean loss: 121.75
 ---- batch: 070 ----
mean loss: 128.14
 ---- batch: 080 ----
mean loss: 129.02
 ---- batch: 090 ----
mean loss: 127.35
train mean loss: 125.64
epoch train time: 0:00:00.500908
elapsed time: 0:02:33.834863
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:22:08.999053
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.51
 ---- batch: 020 ----
mean loss: 124.37
 ---- batch: 030 ----
mean loss: 123.80
 ---- batch: 040 ----
mean loss: 126.68
 ---- batch: 050 ----
mean loss: 126.50
 ---- batch: 060 ----
mean loss: 121.08
 ---- batch: 070 ----
mean loss: 130.22
 ---- batch: 080 ----
mean loss: 123.76
 ---- batch: 090 ----
mean loss: 125.97
train mean loss: 125.51
epoch train time: 0:00:00.495341
elapsed time: 0:02:34.330357
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:22:09.494577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.68
 ---- batch: 020 ----
mean loss: 124.56
 ---- batch: 030 ----
mean loss: 126.09
 ---- batch: 040 ----
mean loss: 120.78
 ---- batch: 050 ----
mean loss: 130.02
 ---- batch: 060 ----
mean loss: 126.99
 ---- batch: 070 ----
mean loss: 122.21
 ---- batch: 080 ----
mean loss: 129.29
 ---- batch: 090 ----
mean loss: 122.78
train mean loss: 125.78
epoch train time: 0:00:00.510273
elapsed time: 0:02:34.840835
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:22:10.005035
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.53
 ---- batch: 020 ----
mean loss: 126.28
 ---- batch: 030 ----
mean loss: 120.03
 ---- batch: 040 ----
mean loss: 117.92
 ---- batch: 050 ----
mean loss: 126.38
 ---- batch: 060 ----
mean loss: 125.47
 ---- batch: 070 ----
mean loss: 126.31
 ---- batch: 080 ----
mean loss: 127.60
 ---- batch: 090 ----
mean loss: 118.08
train mean loss: 125.56
epoch train time: 0:00:00.493732
elapsed time: 0:02:35.334731
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:22:10.498916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.21
 ---- batch: 020 ----
mean loss: 120.69
 ---- batch: 030 ----
mean loss: 129.08
 ---- batch: 040 ----
mean loss: 124.41
 ---- batch: 050 ----
mean loss: 125.04
 ---- batch: 060 ----
mean loss: 120.92
 ---- batch: 070 ----
mean loss: 127.58
 ---- batch: 080 ----
mean loss: 125.09
 ---- batch: 090 ----
mean loss: 124.80
train mean loss: 125.45
epoch train time: 0:00:00.504567
elapsed time: 0:02:35.839448
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:22:11.003634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.53
 ---- batch: 020 ----
mean loss: 122.19
 ---- batch: 030 ----
mean loss: 122.31
 ---- batch: 040 ----
mean loss: 127.35
 ---- batch: 050 ----
mean loss: 119.19
 ---- batch: 060 ----
mean loss: 124.33
 ---- batch: 070 ----
mean loss: 128.34
 ---- batch: 080 ----
mean loss: 123.46
 ---- batch: 090 ----
mean loss: 129.67
train mean loss: 125.28
epoch train time: 0:00:00.505413
elapsed time: 0:02:36.345011
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:22:11.509198
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.06
 ---- batch: 020 ----
mean loss: 126.66
 ---- batch: 030 ----
mean loss: 123.19
 ---- batch: 040 ----
mean loss: 124.13
 ---- batch: 050 ----
mean loss: 123.61
 ---- batch: 060 ----
mean loss: 125.48
 ---- batch: 070 ----
mean loss: 125.00
 ---- batch: 080 ----
mean loss: 125.16
 ---- batch: 090 ----
mean loss: 129.06
train mean loss: 125.26
epoch train time: 0:00:00.513758
elapsed time: 0:02:36.858929
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:22:12.023107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.74
 ---- batch: 020 ----
mean loss: 133.84
 ---- batch: 030 ----
mean loss: 121.18
 ---- batch: 040 ----
mean loss: 123.49
 ---- batch: 050 ----
mean loss: 124.62
 ---- batch: 060 ----
mean loss: 123.42
 ---- batch: 070 ----
mean loss: 124.51
 ---- batch: 080 ----
mean loss: 121.75
 ---- batch: 090 ----
mean loss: 130.85
train mean loss: 125.20
epoch train time: 0:00:00.506076
elapsed time: 0:02:37.365217
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:22:12.529396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.21
 ---- batch: 020 ----
mean loss: 125.33
 ---- batch: 030 ----
mean loss: 121.31
 ---- batch: 040 ----
mean loss: 124.06
 ---- batch: 050 ----
mean loss: 126.63
 ---- batch: 060 ----
mean loss: 126.59
 ---- batch: 070 ----
mean loss: 129.14
 ---- batch: 080 ----
mean loss: 127.11
 ---- batch: 090 ----
mean loss: 130.15
train mean loss: 125.01
epoch train time: 0:00:00.504392
elapsed time: 0:02:37.869767
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:22:13.033960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.99
 ---- batch: 020 ----
mean loss: 124.13
 ---- batch: 030 ----
mean loss: 121.79
 ---- batch: 040 ----
mean loss: 123.91
 ---- batch: 050 ----
mean loss: 123.64
 ---- batch: 060 ----
mean loss: 127.43
 ---- batch: 070 ----
mean loss: 129.10
 ---- batch: 080 ----
mean loss: 124.68
 ---- batch: 090 ----
mean loss: 129.49
train mean loss: 125.28
epoch train time: 0:00:00.502629
elapsed time: 0:02:38.372548
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:22:13.536749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.32
 ---- batch: 020 ----
mean loss: 125.31
 ---- batch: 030 ----
mean loss: 125.12
 ---- batch: 040 ----
mean loss: 126.23
 ---- batch: 050 ----
mean loss: 128.39
 ---- batch: 060 ----
mean loss: 126.72
 ---- batch: 070 ----
mean loss: 120.96
 ---- batch: 080 ----
mean loss: 126.24
 ---- batch: 090 ----
mean loss: 125.49
train mean loss: 125.23
epoch train time: 0:00:00.502816
elapsed time: 0:02:38.875526
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:22:14.039715
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.46
 ---- batch: 020 ----
mean loss: 117.86
 ---- batch: 030 ----
mean loss: 119.58
 ---- batch: 040 ----
mean loss: 125.29
 ---- batch: 050 ----
mean loss: 127.90
 ---- batch: 060 ----
mean loss: 129.68
 ---- batch: 070 ----
mean loss: 132.02
 ---- batch: 080 ----
mean loss: 126.29
 ---- batch: 090 ----
mean loss: 124.18
train mean loss: 125.12
epoch train time: 0:00:00.499311
elapsed time: 0:02:39.374985
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:22:14.539171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.08
 ---- batch: 020 ----
mean loss: 126.86
 ---- batch: 030 ----
mean loss: 120.46
 ---- batch: 040 ----
mean loss: 115.51
 ---- batch: 050 ----
mean loss: 128.92
 ---- batch: 060 ----
mean loss: 127.71
 ---- batch: 070 ----
mean loss: 126.25
 ---- batch: 080 ----
mean loss: 129.58
 ---- batch: 090 ----
mean loss: 129.67
train mean loss: 124.94
epoch train time: 0:00:00.511201
elapsed time: 0:02:39.889784
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_6/checkpoint.pth.tar
**** end time: 2019-09-25 23:22:15.053936 ****
