Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31282
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-20 15:09:06.013936 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 15:09:06.029992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2485.50
 ---- batch: 020 ----
mean loss: 1376.03
 ---- batch: 030 ----
mean loss: 1141.45
 ---- batch: 040 ----
mean loss: 1105.50
train mean loss: 1495.88
epoch train time: 0:00:20.184551
elapsed time: 0:00:20.208446
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 15:09:26.222417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1046.47
 ---- batch: 020 ----
mean loss: 1037.22
 ---- batch: 030 ----
mean loss: 1069.64
 ---- batch: 040 ----
mean loss: 1010.93
train mean loss: 1041.83
epoch train time: 0:00:08.115472
elapsed time: 0:00:28.324197
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 15:09:34.338282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 990.28
 ---- batch: 020 ----
mean loss: 992.64
 ---- batch: 030 ----
mean loss: 990.12
 ---- batch: 040 ----
mean loss: 1021.01
train mean loss: 996.56
epoch train time: 0:00:08.034580
elapsed time: 0:00:36.359213
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 15:09:42.373281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 993.11
 ---- batch: 020 ----
mean loss: 956.73
 ---- batch: 030 ----
mean loss: 1004.20
 ---- batch: 040 ----
mean loss: 976.15
train mean loss: 985.68
epoch train time: 0:00:08.003724
elapsed time: 0:00:44.363384
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 15:09:50.377523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 973.13
 ---- batch: 020 ----
mean loss: 953.71
 ---- batch: 030 ----
mean loss: 955.79
 ---- batch: 040 ----
mean loss: 939.71
train mean loss: 957.10
epoch train time: 0:00:07.825589
elapsed time: 0:00:52.189429
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 15:09:58.203481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.94
 ---- batch: 020 ----
mean loss: 923.21
 ---- batch: 030 ----
mean loss: 905.45
 ---- batch: 040 ----
mean loss: 896.75
train mean loss: 911.34
epoch train time: 0:00:07.856739
elapsed time: 0:01:00.046555
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 15:10:06.060618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.14
 ---- batch: 020 ----
mean loss: 796.52
 ---- batch: 030 ----
mean loss: 714.53
 ---- batch: 040 ----
mean loss: 635.37
train mean loss: 737.17
epoch train time: 0:00:07.833017
elapsed time: 0:01:07.879960
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 15:10:13.894032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.71
 ---- batch: 020 ----
mean loss: 461.04
 ---- batch: 030 ----
mean loss: 412.30
 ---- batch: 040 ----
mean loss: 389.94
train mean loss: 440.35
epoch train time: 0:00:07.843354
elapsed time: 0:01:15.723801
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 15:10:21.737939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.71
 ---- batch: 020 ----
mean loss: 372.54
 ---- batch: 030 ----
mean loss: 351.35
 ---- batch: 040 ----
mean loss: 360.70
train mean loss: 362.58
epoch train time: 0:00:07.786158
elapsed time: 0:01:23.510499
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 15:10:29.524596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.45
 ---- batch: 020 ----
mean loss: 330.91
 ---- batch: 030 ----
mean loss: 333.61
 ---- batch: 040 ----
mean loss: 329.55
train mean loss: 333.18
epoch train time: 0:00:07.808899
elapsed time: 0:01:31.319823
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 15:10:37.333940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.44
 ---- batch: 020 ----
mean loss: 326.58
 ---- batch: 030 ----
mean loss: 302.30
 ---- batch: 040 ----
mean loss: 286.46
train mean loss: 308.33
epoch train time: 0:00:07.877087
elapsed time: 0:01:39.197345
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 15:10:45.211420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.85
 ---- batch: 020 ----
mean loss: 301.84
 ---- batch: 030 ----
mean loss: 286.43
 ---- batch: 040 ----
mean loss: 291.68
train mean loss: 296.02
epoch train time: 0:00:07.816871
elapsed time: 0:01:47.014742
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 15:10:53.028860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.18
 ---- batch: 020 ----
mean loss: 282.52
 ---- batch: 030 ----
mean loss: 274.74
 ---- batch: 040 ----
mean loss: 279.56
train mean loss: 284.63
epoch train time: 0:00:07.732442
elapsed time: 0:01:54.747660
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 15:11:00.761735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.92
 ---- batch: 020 ----
mean loss: 278.44
 ---- batch: 030 ----
mean loss: 271.81
 ---- batch: 040 ----
mean loss: 282.64
train mean loss: 279.27
epoch train time: 0:00:07.765244
elapsed time: 0:02:02.513337
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 15:11:08.527411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.83
 ---- batch: 020 ----
mean loss: 271.87
 ---- batch: 030 ----
mean loss: 269.63
 ---- batch: 040 ----
mean loss: 259.91
train mean loss: 269.72
epoch train time: 0:00:07.719771
elapsed time: 0:02:10.233571
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 15:11:16.247658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.18
 ---- batch: 020 ----
mean loss: 263.14
 ---- batch: 030 ----
mean loss: 258.96
 ---- batch: 040 ----
mean loss: 263.19
train mean loss: 263.66
epoch train time: 0:00:07.717612
elapsed time: 0:02:17.951622
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 15:11:23.965691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.65
 ---- batch: 020 ----
mean loss: 256.19
 ---- batch: 030 ----
mean loss: 254.52
 ---- batch: 040 ----
mean loss: 251.49
train mean loss: 253.92
epoch train time: 0:00:07.703607
elapsed time: 0:02:25.655619
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 15:11:31.669699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.38
 ---- batch: 020 ----
mean loss: 252.00
 ---- batch: 030 ----
mean loss: 252.12
 ---- batch: 040 ----
mean loss: 243.19
train mean loss: 248.35
epoch train time: 0:00:07.731319
elapsed time: 0:02:33.387354
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 15:11:39.401463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.10
 ---- batch: 020 ----
mean loss: 253.17
 ---- batch: 030 ----
mean loss: 244.93
 ---- batch: 040 ----
mean loss: 240.80
train mean loss: 245.62
epoch train time: 0:00:07.747087
elapsed time: 0:02:41.134945
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 15:11:47.149025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.47
 ---- batch: 020 ----
mean loss: 242.35
 ---- batch: 030 ----
mean loss: 239.36
 ---- batch: 040 ----
mean loss: 244.87
train mean loss: 242.91
epoch train time: 0:00:07.649360
elapsed time: 0:02:48.784744
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 15:11:54.798842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.54
 ---- batch: 020 ----
mean loss: 241.19
 ---- batch: 030 ----
mean loss: 243.05
 ---- batch: 040 ----
mean loss: 234.60
train mean loss: 238.80
epoch train time: 0:00:07.714099
elapsed time: 0:02:56.499281
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 15:12:02.513318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.32
 ---- batch: 020 ----
mean loss: 242.66
 ---- batch: 030 ----
mean loss: 225.26
 ---- batch: 040 ----
mean loss: 230.25
train mean loss: 232.60
epoch train time: 0:00:07.706378
elapsed time: 0:03:04.206001
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 15:12:10.220032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.24
 ---- batch: 020 ----
mean loss: 228.42
 ---- batch: 030 ----
mean loss: 222.08
 ---- batch: 040 ----
mean loss: 223.73
train mean loss: 226.11
epoch train time: 0:00:07.766316
elapsed time: 0:03:11.972671
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 15:12:17.986749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.42
 ---- batch: 020 ----
mean loss: 228.39
 ---- batch: 030 ----
mean loss: 228.22
 ---- batch: 040 ----
mean loss: 230.22
train mean loss: 226.55
epoch train time: 0:00:07.567674
elapsed time: 0:03:19.540744
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 15:12:25.554912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.35
 ---- batch: 020 ----
mean loss: 227.88
 ---- batch: 030 ----
mean loss: 227.81
 ---- batch: 040 ----
mean loss: 229.36
train mean loss: 225.04
epoch train time: 0:00:07.555881
elapsed time: 0:03:27.097180
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 15:12:33.111295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.88
 ---- batch: 020 ----
mean loss: 215.57
 ---- batch: 030 ----
mean loss: 211.44
 ---- batch: 040 ----
mean loss: 215.10
train mean loss: 216.29
epoch train time: 0:00:07.677332
elapsed time: 0:03:34.775037
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 15:12:40.789084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.61
 ---- batch: 020 ----
mean loss: 214.15
 ---- batch: 030 ----
mean loss: 219.56
 ---- batch: 040 ----
mean loss: 212.97
train mean loss: 215.90
epoch train time: 0:00:07.546372
elapsed time: 0:03:42.321780
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 15:12:48.335905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.68
 ---- batch: 020 ----
mean loss: 213.99
 ---- batch: 030 ----
mean loss: 212.04
 ---- batch: 040 ----
mean loss: 200.32
train mean loss: 209.03
epoch train time: 0:00:07.449349
elapsed time: 0:03:49.771692
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 15:12:55.785783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.10
 ---- batch: 020 ----
mean loss: 206.24
 ---- batch: 030 ----
mean loss: 214.70
 ---- batch: 040 ----
mean loss: 205.03
train mean loss: 208.23
epoch train time: 0:00:07.464066
elapsed time: 0:03:57.236185
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 15:13:03.250268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.54
 ---- batch: 020 ----
mean loss: 214.97
 ---- batch: 030 ----
mean loss: 194.67
 ---- batch: 040 ----
mean loss: 199.72
train mean loss: 201.81
epoch train time: 0:00:07.470306
elapsed time: 0:04:04.706923
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 15:13:10.721042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.13
 ---- batch: 020 ----
mean loss: 198.29
 ---- batch: 030 ----
mean loss: 203.93
 ---- batch: 040 ----
mean loss: 200.23
train mean loss: 202.54
epoch train time: 0:00:07.471545
elapsed time: 0:04:12.178917
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 15:13:18.193003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.88
 ---- batch: 020 ----
mean loss: 191.22
 ---- batch: 030 ----
mean loss: 198.36
 ---- batch: 040 ----
mean loss: 195.74
train mean loss: 198.43
epoch train time: 0:00:07.554749
elapsed time: 0:04:19.734071
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 15:13:25.748158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.25
 ---- batch: 020 ----
mean loss: 186.12
 ---- batch: 030 ----
mean loss: 191.66
 ---- batch: 040 ----
mean loss: 192.41
train mean loss: 193.30
epoch train time: 0:00:07.643845
elapsed time: 0:04:27.378334
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 15:13:33.392431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.05
 ---- batch: 020 ----
mean loss: 190.81
 ---- batch: 030 ----
mean loss: 184.40
 ---- batch: 040 ----
mean loss: 184.48
train mean loss: 188.36
epoch train time: 0:00:07.492153
elapsed time: 0:04:34.870911
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 15:13:40.884998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.58
 ---- batch: 020 ----
mean loss: 189.00
 ---- batch: 030 ----
mean loss: 185.71
 ---- batch: 040 ----
mean loss: 190.04
train mean loss: 187.39
epoch train time: 0:00:07.489621
elapsed time: 0:04:42.360966
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 15:13:48.375065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.22
 ---- batch: 020 ----
mean loss: 184.10
 ---- batch: 030 ----
mean loss: 181.53
 ---- batch: 040 ----
mean loss: 185.18
train mean loss: 184.78
epoch train time: 0:00:07.462205
elapsed time: 0:04:49.823675
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 15:13:55.837767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.43
 ---- batch: 020 ----
mean loss: 182.63
 ---- batch: 030 ----
mean loss: 180.26
 ---- batch: 040 ----
mean loss: 182.27
train mean loss: 179.46
epoch train time: 0:00:07.492313
elapsed time: 0:04:57.316410
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 15:14:03.330511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.98
 ---- batch: 020 ----
mean loss: 171.54
 ---- batch: 030 ----
mean loss: 175.61
 ---- batch: 040 ----
mean loss: 174.76
train mean loss: 175.97
epoch train time: 0:00:07.577194
elapsed time: 0:05:04.894133
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 15:14:10.908236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.24
 ---- batch: 020 ----
mean loss: 177.14
 ---- batch: 030 ----
mean loss: 172.39
 ---- batch: 040 ----
mean loss: 170.68
train mean loss: 173.79
epoch train time: 0:00:07.703119
elapsed time: 0:05:12.597672
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 15:14:18.611765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.75
 ---- batch: 020 ----
mean loss: 174.05
 ---- batch: 030 ----
mean loss: 176.92
 ---- batch: 040 ----
mean loss: 173.07
train mean loss: 173.52
epoch train time: 0:00:07.710979
elapsed time: 0:05:20.309045
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 15:14:26.323121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.43
 ---- batch: 020 ----
mean loss: 171.83
 ---- batch: 030 ----
mean loss: 172.13
 ---- batch: 040 ----
mean loss: 170.56
train mean loss: 172.00
epoch train time: 0:00:07.687759
elapsed time: 0:05:27.997250
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 15:14:34.011376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.29
 ---- batch: 020 ----
mean loss: 163.82
 ---- batch: 030 ----
mean loss: 174.19
 ---- batch: 040 ----
mean loss: 161.89
train mean loss: 165.00
epoch train time: 0:00:07.723189
elapsed time: 0:05:35.720945
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 15:14:41.735036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.72
 ---- batch: 020 ----
mean loss: 164.05
 ---- batch: 030 ----
mean loss: 161.72
 ---- batch: 040 ----
mean loss: 167.43
train mean loss: 164.24
epoch train time: 0:00:07.734618
elapsed time: 0:05:43.455963
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 15:14:49.470005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.51
 ---- batch: 020 ----
mean loss: 167.11
 ---- batch: 030 ----
mean loss: 157.71
 ---- batch: 040 ----
mean loss: 158.55
train mean loss: 162.84
epoch train time: 0:00:07.671712
elapsed time: 0:05:51.128053
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 15:14:57.142132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.36
 ---- batch: 020 ----
mean loss: 163.41
 ---- batch: 030 ----
mean loss: 158.18
 ---- batch: 040 ----
mean loss: 166.68
train mean loss: 162.25
epoch train time: 0:00:07.779832
elapsed time: 0:05:58.908326
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 15:15:04.922393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.68
 ---- batch: 020 ----
mean loss: 158.83
 ---- batch: 030 ----
mean loss: 154.63
 ---- batch: 040 ----
mean loss: 157.77
train mean loss: 158.16
epoch train time: 0:00:07.709179
elapsed time: 0:06:06.617885
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 15:15:12.631918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.68
 ---- batch: 020 ----
mean loss: 155.23
 ---- batch: 030 ----
mean loss: 156.29
 ---- batch: 040 ----
mean loss: 156.29
train mean loss: 156.33
epoch train time: 0:00:07.665679
elapsed time: 0:06:14.283972
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 15:15:20.298048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.34
 ---- batch: 020 ----
mean loss: 147.03
 ---- batch: 030 ----
mean loss: 157.18
 ---- batch: 040 ----
mean loss: 152.72
train mean loss: 153.44
epoch train time: 0:00:07.655787
elapsed time: 0:06:21.940253
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 15:15:27.954357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.13
 ---- batch: 020 ----
mean loss: 152.16
 ---- batch: 030 ----
mean loss: 146.49
 ---- batch: 040 ----
mean loss: 153.68
train mean loss: 151.19
epoch train time: 0:00:07.658915
elapsed time: 0:06:29.599588
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 15:15:35.613664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.49
 ---- batch: 020 ----
mean loss: 149.80
 ---- batch: 030 ----
mean loss: 145.44
 ---- batch: 040 ----
mean loss: 148.33
train mean loss: 149.26
epoch train time: 0:00:07.685549
elapsed time: 0:06:37.285546
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 15:15:43.299636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.39
 ---- batch: 020 ----
mean loss: 148.74
 ---- batch: 030 ----
mean loss: 151.54
 ---- batch: 040 ----
mean loss: 152.16
train mean loss: 150.06
epoch train time: 0:00:07.715448
elapsed time: 0:06:45.001426
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 15:15:51.015475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.14
 ---- batch: 020 ----
mean loss: 146.18
 ---- batch: 030 ----
mean loss: 148.42
 ---- batch: 040 ----
mean loss: 146.79
train mean loss: 146.04
epoch train time: 0:00:07.694486
elapsed time: 0:06:52.696282
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 15:15:58.710377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.07
 ---- batch: 020 ----
mean loss: 141.27
 ---- batch: 030 ----
mean loss: 144.06
 ---- batch: 040 ----
mean loss: 141.68
train mean loss: 145.46
epoch train time: 0:00:07.587661
elapsed time: 0:07:00.284374
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 15:16:06.298473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.71
 ---- batch: 020 ----
mean loss: 145.39
 ---- batch: 030 ----
mean loss: 142.63
 ---- batch: 040 ----
mean loss: 142.13
train mean loss: 142.60
epoch train time: 0:00:07.590847
elapsed time: 0:07:07.875638
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 15:16:13.889712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.31
 ---- batch: 020 ----
mean loss: 144.20
 ---- batch: 030 ----
mean loss: 144.45
 ---- batch: 040 ----
mean loss: 135.81
train mean loss: 141.25
epoch train time: 0:00:07.607805
elapsed time: 0:07:15.483846
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 15:16:21.497926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.32
 ---- batch: 020 ----
mean loss: 140.75
 ---- batch: 030 ----
mean loss: 137.97
 ---- batch: 040 ----
mean loss: 135.88
train mean loss: 140.13
epoch train time: 0:00:07.538921
elapsed time: 0:07:23.023190
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 15:16:29.037287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.34
 ---- batch: 020 ----
mean loss: 142.27
 ---- batch: 030 ----
mean loss: 136.96
 ---- batch: 040 ----
mean loss: 137.95
train mean loss: 139.49
epoch train time: 0:00:07.520967
elapsed time: 0:07:30.544573
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 15:16:36.558641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.17
 ---- batch: 020 ----
mean loss: 135.57
 ---- batch: 030 ----
mean loss: 140.34
 ---- batch: 040 ----
mean loss: 142.33
train mean loss: 138.57
epoch train time: 0:00:07.462188
elapsed time: 0:07:38.007235
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 15:16:44.021343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.00
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 136.71
 ---- batch: 040 ----
mean loss: 138.64
train mean loss: 134.38
epoch train time: 0:00:07.469891
elapsed time: 0:07:45.477568
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 15:16:51.491695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.33
 ---- batch: 020 ----
mean loss: 140.00
 ---- batch: 030 ----
mean loss: 131.81
 ---- batch: 040 ----
mean loss: 129.62
train mean loss: 133.22
epoch train time: 0:00:07.460192
elapsed time: 0:07:52.938249
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 15:16:58.952363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.19
 ---- batch: 020 ----
mean loss: 128.75
 ---- batch: 030 ----
mean loss: 134.25
 ---- batch: 040 ----
mean loss: 134.55
train mean loss: 131.50
epoch train time: 0:00:07.482158
elapsed time: 0:08:00.420961
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 15:17:06.435062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.72
 ---- batch: 020 ----
mean loss: 135.75
 ---- batch: 030 ----
mean loss: 126.84
 ---- batch: 040 ----
mean loss: 134.74
train mean loss: 131.96
epoch train time: 0:00:07.536920
elapsed time: 0:08:07.958307
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 15:17:13.972403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.75
 ---- batch: 020 ----
mean loss: 133.21
 ---- batch: 030 ----
mean loss: 125.95
 ---- batch: 040 ----
mean loss: 134.62
train mean loss: 130.16
epoch train time: 0:00:07.725771
elapsed time: 0:08:15.684529
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 15:17:21.698610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.17
 ---- batch: 020 ----
mean loss: 131.67
 ---- batch: 030 ----
mean loss: 129.53
 ---- batch: 040 ----
mean loss: 130.53
train mean loss: 130.06
epoch train time: 0:00:07.859405
elapsed time: 0:08:23.544335
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 15:17:29.558437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.83
 ---- batch: 020 ----
mean loss: 129.15
 ---- batch: 030 ----
mean loss: 128.75
 ---- batch: 040 ----
mean loss: 123.81
train mean loss: 127.50
epoch train time: 0:00:07.870775
elapsed time: 0:08:31.415532
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 15:17:37.429614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.34
 ---- batch: 020 ----
mean loss: 125.11
 ---- batch: 030 ----
mean loss: 118.57
 ---- batch: 040 ----
mean loss: 129.63
train mean loss: 124.57
epoch train time: 0:00:07.835093
elapsed time: 0:08:39.251031
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 15:17:45.265114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.58
 ---- batch: 020 ----
mean loss: 118.17
 ---- batch: 030 ----
mean loss: 128.36
 ---- batch: 040 ----
mean loss: 127.51
train mean loss: 124.71
epoch train time: 0:00:07.715587
elapsed time: 0:08:46.967029
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 15:17:52.981074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.35
 ---- batch: 020 ----
mean loss: 129.93
 ---- batch: 030 ----
mean loss: 129.93
 ---- batch: 040 ----
mean loss: 122.59
train mean loss: 125.98
epoch train time: 0:00:07.557090
elapsed time: 0:08:54.524489
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 15:18:00.538602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.94
 ---- batch: 020 ----
mean loss: 117.71
 ---- batch: 030 ----
mean loss: 124.63
 ---- batch: 040 ----
mean loss: 124.80
train mean loss: 122.31
epoch train time: 0:00:07.499702
elapsed time: 0:09:02.024703
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 15:18:08.038793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.85
 ---- batch: 020 ----
mean loss: 121.06
 ---- batch: 030 ----
mean loss: 122.77
 ---- batch: 040 ----
mean loss: 122.78
train mean loss: 122.84
epoch train time: 0:00:07.683380
elapsed time: 0:09:09.708535
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 15:18:15.722630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.00
 ---- batch: 020 ----
mean loss: 122.31
 ---- batch: 030 ----
mean loss: 127.40
 ---- batch: 040 ----
mean loss: 119.21
train mean loss: 123.49
epoch train time: 0:00:07.891406
elapsed time: 0:09:17.600353
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 15:18:23.614468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.66
 ---- batch: 020 ----
mean loss: 120.57
 ---- batch: 030 ----
mean loss: 121.02
 ---- batch: 040 ----
mean loss: 117.06
train mean loss: 119.40
epoch train time: 0:00:07.895920
elapsed time: 0:09:25.496756
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 15:18:31.510867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.64
 ---- batch: 020 ----
mean loss: 122.36
 ---- batch: 030 ----
mean loss: 113.49
 ---- batch: 040 ----
mean loss: 116.99
train mean loss: 116.49
epoch train time: 0:00:07.816358
elapsed time: 0:09:33.313586
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 15:18:39.327678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.51
 ---- batch: 020 ----
mean loss: 115.43
 ---- batch: 030 ----
mean loss: 119.08
 ---- batch: 040 ----
mean loss: 115.85
train mean loss: 117.58
epoch train time: 0:00:07.790254
elapsed time: 0:09:41.104271
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 15:18:47.118364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.18
 ---- batch: 020 ----
mean loss: 115.72
 ---- batch: 030 ----
mean loss: 116.06
 ---- batch: 040 ----
mean loss: 115.77
train mean loss: 116.00
epoch train time: 0:00:07.594648
elapsed time: 0:09:48.699421
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 15:18:54.713555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.41
 ---- batch: 020 ----
mean loss: 119.20
 ---- batch: 030 ----
mean loss: 117.96
 ---- batch: 040 ----
mean loss: 114.74
train mean loss: 117.44
epoch train time: 0:00:07.640341
elapsed time: 0:09:56.340232
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 15:19:02.354321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.20
 ---- batch: 020 ----
mean loss: 112.25
 ---- batch: 030 ----
mean loss: 113.17
 ---- batch: 040 ----
mean loss: 114.90
train mean loss: 113.84
epoch train time: 0:00:07.660031
elapsed time: 0:10:04.000711
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 15:19:10.014805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.46
 ---- batch: 020 ----
mean loss: 113.63
 ---- batch: 030 ----
mean loss: 111.82
 ---- batch: 040 ----
mean loss: 109.76
train mean loss: 111.82
epoch train time: 0:00:07.651872
elapsed time: 0:10:11.653067
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 15:19:17.667193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.17
 ---- batch: 020 ----
mean loss: 118.67
 ---- batch: 030 ----
mean loss: 119.52
 ---- batch: 040 ----
mean loss: 111.61
train mean loss: 116.63
epoch train time: 0:00:07.601215
elapsed time: 0:10:19.254716
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 15:19:25.268799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.91
 ---- batch: 020 ----
mean loss: 109.39
 ---- batch: 030 ----
mean loss: 111.93
 ---- batch: 040 ----
mean loss: 113.08
train mean loss: 111.13
epoch train time: 0:00:07.562714
elapsed time: 0:10:26.817847
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 15:19:32.831944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.84
 ---- batch: 020 ----
mean loss: 112.24
 ---- batch: 030 ----
mean loss: 110.51
 ---- batch: 040 ----
mean loss: 107.88
train mean loss: 111.26
epoch train time: 0:00:07.581217
elapsed time: 0:10:34.399538
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 15:19:40.413627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.99
 ---- batch: 020 ----
mean loss: 111.36
 ---- batch: 030 ----
mean loss: 108.36
 ---- batch: 040 ----
mean loss: 108.85
train mean loss: 109.58
epoch train time: 0:00:07.568020
elapsed time: 0:10:41.968005
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 15:19:47.982102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.37
 ---- batch: 020 ----
mean loss: 110.59
 ---- batch: 030 ----
mean loss: 108.65
 ---- batch: 040 ----
mean loss: 108.03
train mean loss: 109.16
epoch train time: 0:00:07.680380
elapsed time: 0:10:49.648840
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 15:19:55.662939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.26
 ---- batch: 020 ----
mean loss: 109.76
 ---- batch: 030 ----
mean loss: 107.21
 ---- batch: 040 ----
mean loss: 112.19
train mean loss: 108.21
epoch train time: 0:00:07.517938
elapsed time: 0:10:57.167227
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 15:20:03.181322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.25
 ---- batch: 020 ----
mean loss: 112.51
 ---- batch: 030 ----
mean loss: 107.97
 ---- batch: 040 ----
mean loss: 102.67
train mean loss: 107.55
epoch train time: 0:00:07.435292
elapsed time: 0:11:04.603030
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 15:20:10.617082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.90
 ---- batch: 020 ----
mean loss: 108.65
 ---- batch: 030 ----
mean loss: 108.25
 ---- batch: 040 ----
mean loss: 107.03
train mean loss: 107.65
epoch train time: 0:00:07.429402
elapsed time: 0:11:12.032824
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 15:20:18.046936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.51
 ---- batch: 020 ----
mean loss: 110.03
 ---- batch: 030 ----
mean loss: 108.55
 ---- batch: 040 ----
mean loss: 107.99
train mean loss: 107.72
epoch train time: 0:00:07.469446
elapsed time: 0:11:19.502816
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 15:20:25.516988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.05
 ---- batch: 020 ----
mean loss: 103.93
 ---- batch: 030 ----
mean loss: 109.97
 ---- batch: 040 ----
mean loss: 106.21
train mean loss: 107.25
epoch train time: 0:00:07.518616
elapsed time: 0:11:27.021953
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 15:20:33.036031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.98
 ---- batch: 020 ----
mean loss: 112.00
 ---- batch: 030 ----
mean loss: 106.05
 ---- batch: 040 ----
mean loss: 105.80
train mean loss: 108.54
epoch train time: 0:00:07.571310
elapsed time: 0:11:34.593678
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 15:20:40.607812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.90
 ---- batch: 020 ----
mean loss: 104.61
 ---- batch: 030 ----
mean loss: 110.80
 ---- batch: 040 ----
mean loss: 106.85
train mean loss: 106.84
epoch train time: 0:00:07.516544
elapsed time: 0:11:42.110800
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 15:20:48.124965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.54
 ---- batch: 020 ----
mean loss: 106.49
 ---- batch: 030 ----
mean loss: 100.75
 ---- batch: 040 ----
mean loss: 103.60
train mean loss: 104.14
epoch train time: 0:00:07.579394
elapsed time: 0:11:49.690762
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 15:20:55.704878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.46
 ---- batch: 020 ----
mean loss: 107.70
 ---- batch: 030 ----
mean loss: 105.24
 ---- batch: 040 ----
mean loss: 105.00
train mean loss: 106.16
epoch train time: 0:00:07.553597
elapsed time: 0:11:57.244804
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 15:21:03.258865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.53
 ---- batch: 020 ----
mean loss: 103.37
 ---- batch: 030 ----
mean loss: 100.66
 ---- batch: 040 ----
mean loss: 107.20
train mean loss: 103.20
epoch train time: 0:00:07.502164
elapsed time: 0:12:04.747342
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 15:21:10.761421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.27
 ---- batch: 020 ----
mean loss: 104.89
 ---- batch: 030 ----
mean loss: 102.70
 ---- batch: 040 ----
mean loss: 100.96
train mean loss: 102.90
epoch train time: 0:00:07.542139
elapsed time: 0:12:12.289867
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 15:21:18.303961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.43
 ---- batch: 020 ----
mean loss: 104.95
 ---- batch: 030 ----
mean loss: 96.77
 ---- batch: 040 ----
mean loss: 104.82
train mean loss: 101.34
epoch train time: 0:00:07.690729
elapsed time: 0:12:19.981021
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 15:21:25.995141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.72
 ---- batch: 020 ----
mean loss: 104.84
 ---- batch: 030 ----
mean loss: 97.14
 ---- batch: 040 ----
mean loss: 98.11
train mean loss: 101.48
epoch train time: 0:00:07.634361
elapsed time: 0:12:27.615827
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 15:21:33.629891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.61
 ---- batch: 020 ----
mean loss: 105.13
 ---- batch: 030 ----
mean loss: 96.33
 ---- batch: 040 ----
mean loss: 101.95
train mean loss: 102.23
epoch train time: 0:00:07.622988
elapsed time: 0:12:35.239248
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 15:21:41.253308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.52
 ---- batch: 020 ----
mean loss: 100.49
 ---- batch: 030 ----
mean loss: 100.03
 ---- batch: 040 ----
mean loss: 99.21
train mean loss: 100.32
epoch train time: 0:00:07.644596
elapsed time: 0:12:42.884253
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 15:21:48.898345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.16
 ---- batch: 020 ----
mean loss: 100.41
 ---- batch: 030 ----
mean loss: 100.61
 ---- batch: 040 ----
mean loss: 99.73
train mean loss: 100.19
epoch train time: 0:00:07.625800
elapsed time: 0:12:50.510510
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 15:21:56.524603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.67
 ---- batch: 020 ----
mean loss: 101.19
 ---- batch: 030 ----
mean loss: 99.83
 ---- batch: 040 ----
mean loss: 97.96
train mean loss: 98.79
epoch train time: 0:00:07.629781
elapsed time: 0:12:58.140740
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 15:22:04.154834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.54
 ---- batch: 020 ----
mean loss: 102.74
 ---- batch: 030 ----
mean loss: 97.75
 ---- batch: 040 ----
mean loss: 99.97
train mean loss: 99.20
epoch train time: 0:00:07.721253
elapsed time: 0:13:05.862429
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 15:22:11.876499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.03
 ---- batch: 020 ----
mean loss: 100.43
 ---- batch: 030 ----
mean loss: 100.93
 ---- batch: 040 ----
mean loss: 105.54
train mean loss: 101.95
epoch train time: 0:00:07.666674
elapsed time: 0:13:13.529498
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 15:22:19.543537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.97
 ---- batch: 020 ----
mean loss: 95.55
 ---- batch: 030 ----
mean loss: 101.96
 ---- batch: 040 ----
mean loss: 101.05
train mean loss: 99.19
epoch train time: 0:00:07.693987
elapsed time: 0:13:21.223868
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 15:22:27.237913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.83
 ---- batch: 020 ----
mean loss: 96.04
 ---- batch: 030 ----
mean loss: 97.46
 ---- batch: 040 ----
mean loss: 95.13
train mean loss: 96.64
epoch train time: 0:00:07.705279
elapsed time: 0:13:28.929580
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 15:22:34.943680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.78
 ---- batch: 020 ----
mean loss: 99.26
 ---- batch: 030 ----
mean loss: 96.68
 ---- batch: 040 ----
mean loss: 100.25
train mean loss: 98.07
epoch train time: 0:00:07.684440
elapsed time: 0:13:36.614432
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 15:22:42.628530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.80
 ---- batch: 020 ----
mean loss: 95.69
 ---- batch: 030 ----
mean loss: 95.12
 ---- batch: 040 ----
mean loss: 96.62
train mean loss: 96.50
epoch train time: 0:00:07.688485
elapsed time: 0:13:44.303331
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 15:22:50.317404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.77
 ---- batch: 020 ----
mean loss: 97.76
 ---- batch: 030 ----
mean loss: 99.40
 ---- batch: 040 ----
mean loss: 96.57
train mean loss: 97.04
epoch train time: 0:00:07.723757
elapsed time: 0:13:52.027568
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 15:22:58.041570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.99
 ---- batch: 020 ----
mean loss: 96.14
 ---- batch: 030 ----
mean loss: 93.09
 ---- batch: 040 ----
mean loss: 92.49
train mean loss: 94.46
epoch train time: 0:00:07.889311
elapsed time: 0:13:59.917224
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 15:23:05.931305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.57
 ---- batch: 020 ----
mean loss: 93.97
 ---- batch: 030 ----
mean loss: 92.58
 ---- batch: 040 ----
mean loss: 98.95
train mean loss: 94.57
epoch train time: 0:00:07.850743
elapsed time: 0:14:07.768438
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 15:23:13.782526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.90
 ---- batch: 020 ----
mean loss: 97.09
 ---- batch: 030 ----
mean loss: 98.53
 ---- batch: 040 ----
mean loss: 90.11
train mean loss: 96.65
epoch train time: 0:00:07.850980
elapsed time: 0:14:15.619816
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 15:23:21.633890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.83
 ---- batch: 020 ----
mean loss: 90.39
 ---- batch: 030 ----
mean loss: 98.91
 ---- batch: 040 ----
mean loss: 93.57
train mean loss: 94.25
epoch train time: 0:00:07.783381
elapsed time: 0:14:23.403605
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 15:23:29.417712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.72
 ---- batch: 020 ----
mean loss: 97.95
 ---- batch: 030 ----
mean loss: 99.38
 ---- batch: 040 ----
mean loss: 93.11
train mean loss: 96.27
epoch train time: 0:00:07.829710
elapsed time: 0:14:31.233738
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 15:23:37.247805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.32
 ---- batch: 020 ----
mean loss: 94.52
 ---- batch: 030 ----
mean loss: 95.24
 ---- batch: 040 ----
mean loss: 91.05
train mean loss: 94.11
epoch train time: 0:00:07.775370
elapsed time: 0:14:39.009589
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 15:23:45.023711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.11
 ---- batch: 020 ----
mean loss: 92.85
 ---- batch: 030 ----
mean loss: 93.07
 ---- batch: 040 ----
mean loss: 90.92
train mean loss: 92.48
epoch train time: 0:00:07.795910
elapsed time: 0:14:46.805969
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 15:23:52.820099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.57
 ---- batch: 020 ----
mean loss: 90.69
 ---- batch: 030 ----
mean loss: 95.58
 ---- batch: 040 ----
mean loss: 91.42
train mean loss: 93.02
epoch train time: 0:00:07.797717
elapsed time: 0:14:54.604231
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 15:24:00.618313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.18
 ---- batch: 020 ----
mean loss: 91.79
 ---- batch: 030 ----
mean loss: 93.44
 ---- batch: 040 ----
mean loss: 88.21
train mean loss: 91.78
epoch train time: 0:00:07.835154
elapsed time: 0:15:02.439811
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 15:24:08.453895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.37
 ---- batch: 020 ----
mean loss: 90.13
 ---- batch: 030 ----
mean loss: 92.14
 ---- batch: 040 ----
mean loss: 95.66
train mean loss: 93.16
epoch train time: 0:00:07.823655
elapsed time: 0:15:10.263856
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 15:24:16.277937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.73
 ---- batch: 020 ----
mean loss: 96.34
 ---- batch: 030 ----
mean loss: 90.83
 ---- batch: 040 ----
mean loss: 91.02
train mean loss: 93.74
epoch train time: 0:00:07.799244
elapsed time: 0:15:18.063517
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 15:24:24.077604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.74
 ---- batch: 020 ----
mean loss: 94.24
 ---- batch: 030 ----
mean loss: 91.89
 ---- batch: 040 ----
mean loss: 88.89
train mean loss: 91.44
epoch train time: 0:00:07.818105
elapsed time: 0:15:25.882109
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 15:24:31.896205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.26
 ---- batch: 020 ----
mean loss: 92.15
 ---- batch: 030 ----
mean loss: 95.22
 ---- batch: 040 ----
mean loss: 87.16
train mean loss: 91.78
epoch train time: 0:00:07.811854
elapsed time: 0:15:33.694424
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 15:24:39.708538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.77
 ---- batch: 020 ----
mean loss: 91.14
 ---- batch: 030 ----
mean loss: 94.97
 ---- batch: 040 ----
mean loss: 92.25
train mean loss: 91.75
epoch train time: 0:00:07.672474
elapsed time: 0:15:41.367316
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 15:24:47.381417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.27
 ---- batch: 020 ----
mean loss: 93.13
 ---- batch: 030 ----
mean loss: 86.78
 ---- batch: 040 ----
mean loss: 92.06
train mean loss: 90.35
epoch train time: 0:00:07.768312
elapsed time: 0:15:49.136041
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 15:24:55.150125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.38
 ---- batch: 020 ----
mean loss: 90.87
 ---- batch: 030 ----
mean loss: 91.63
 ---- batch: 040 ----
mean loss: 88.96
train mean loss: 89.47
epoch train time: 0:00:07.789729
elapsed time: 0:15:56.926209
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 15:25:02.940301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.29
 ---- batch: 020 ----
mean loss: 89.40
 ---- batch: 030 ----
mean loss: 90.01
 ---- batch: 040 ----
mean loss: 86.14
train mean loss: 89.38
epoch train time: 0:00:07.780058
elapsed time: 0:16:04.706735
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 15:25:10.720833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.97
 ---- batch: 020 ----
mean loss: 87.41
 ---- batch: 030 ----
mean loss: 88.15
 ---- batch: 040 ----
mean loss: 89.16
train mean loss: 88.73
epoch train time: 0:00:07.636905
elapsed time: 0:16:12.344087
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 15:25:18.358185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.82
 ---- batch: 020 ----
mean loss: 85.93
 ---- batch: 030 ----
mean loss: 93.16
 ---- batch: 040 ----
mean loss: 88.20
train mean loss: 88.34
epoch train time: 0:00:07.628380
elapsed time: 0:16:19.972992
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 15:25:25.987122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.57
 ---- batch: 020 ----
mean loss: 89.10
 ---- batch: 030 ----
mean loss: 91.93
 ---- batch: 040 ----
mean loss: 84.66
train mean loss: 88.13
epoch train time: 0:00:07.668332
elapsed time: 0:16:27.641899
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 15:25:33.655882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.55
 ---- batch: 020 ----
mean loss: 88.02
 ---- batch: 030 ----
mean loss: 87.91
 ---- batch: 040 ----
mean loss: 90.23
train mean loss: 89.06
epoch train time: 0:00:07.594992
elapsed time: 0:16:35.237182
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 15:25:41.251258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.25
 ---- batch: 020 ----
mean loss: 86.89
 ---- batch: 030 ----
mean loss: 90.93
 ---- batch: 040 ----
mean loss: 89.55
train mean loss: 87.57
epoch train time: 0:00:07.552505
elapsed time: 0:16:42.790127
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 15:25:48.804217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.11
 ---- batch: 020 ----
mean loss: 86.64
 ---- batch: 030 ----
mean loss: 86.86
 ---- batch: 040 ----
mean loss: 88.16
train mean loss: 88.00
epoch train time: 0:00:07.591164
elapsed time: 0:16:50.381725
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 15:25:56.395768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.14
 ---- batch: 020 ----
mean loss: 90.94
 ---- batch: 030 ----
mean loss: 88.76
 ---- batch: 040 ----
mean loss: 85.28
train mean loss: 88.39
epoch train time: 0:00:07.666647
elapsed time: 0:16:58.048734
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 15:26:04.062826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.28
 ---- batch: 020 ----
mean loss: 84.57
 ---- batch: 030 ----
mean loss: 85.19
 ---- batch: 040 ----
mean loss: 86.23
train mean loss: 86.30
epoch train time: 0:00:07.564170
elapsed time: 0:17:05.613314
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 15:26:11.627440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.23
 ---- batch: 020 ----
mean loss: 85.53
 ---- batch: 030 ----
mean loss: 87.41
 ---- batch: 040 ----
mean loss: 83.72
train mean loss: 86.17
epoch train time: 0:00:07.595765
elapsed time: 0:17:13.209579
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 15:26:19.223697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.77
 ---- batch: 020 ----
mean loss: 87.34
 ---- batch: 030 ----
mean loss: 85.93
 ---- batch: 040 ----
mean loss: 84.34
train mean loss: 85.19
epoch train time: 0:00:07.692913
elapsed time: 0:17:20.903047
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 15:26:26.917153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.17
 ---- batch: 020 ----
mean loss: 87.31
 ---- batch: 030 ----
mean loss: 87.12
 ---- batch: 040 ----
mean loss: 86.37
train mean loss: 85.93
epoch train time: 0:00:07.735515
elapsed time: 0:17:28.638987
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 15:26:34.653091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.57
 ---- batch: 020 ----
mean loss: 84.13
 ---- batch: 030 ----
mean loss: 85.18
 ---- batch: 040 ----
mean loss: 87.26
train mean loss: 85.37
epoch train time: 0:00:07.527903
elapsed time: 0:17:36.167352
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 15:26:42.181431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.82
 ---- batch: 020 ----
mean loss: 87.39
 ---- batch: 030 ----
mean loss: 82.33
 ---- batch: 040 ----
mean loss: 82.86
train mean loss: 84.49
epoch train time: 0:00:07.519969
elapsed time: 0:17:43.687773
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 15:26:49.701895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.63
 ---- batch: 020 ----
mean loss: 84.48
 ---- batch: 030 ----
mean loss: 86.27
 ---- batch: 040 ----
mean loss: 89.99
train mean loss: 86.41
epoch train time: 0:00:07.550055
elapsed time: 0:17:51.238247
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 15:26:57.252344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.52
 ---- batch: 020 ----
mean loss: 88.81
 ---- batch: 030 ----
mean loss: 83.32
 ---- batch: 040 ----
mean loss: 82.99
train mean loss: 84.42
epoch train time: 0:00:07.561036
elapsed time: 0:17:58.799867
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 15:27:04.814092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.10
 ---- batch: 020 ----
mean loss: 84.62
 ---- batch: 030 ----
mean loss: 82.17
 ---- batch: 040 ----
mean loss: 82.55
train mean loss: 83.73
epoch train time: 0:00:07.625953
elapsed time: 0:18:06.426363
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 15:27:12.440467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.74
 ---- batch: 020 ----
mean loss: 79.56
 ---- batch: 030 ----
mean loss: 85.46
 ---- batch: 040 ----
mean loss: 86.24
train mean loss: 83.18
epoch train time: 0:00:07.849966
elapsed time: 0:18:14.276748
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 15:27:20.290812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.78
 ---- batch: 020 ----
mean loss: 83.13
 ---- batch: 030 ----
mean loss: 81.23
 ---- batch: 040 ----
mean loss: 85.57
train mean loss: 83.11
epoch train time: 0:00:07.827226
elapsed time: 0:18:22.104342
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 15:27:28.118425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.92
 ---- batch: 020 ----
mean loss: 88.22
 ---- batch: 030 ----
mean loss: 83.32
 ---- batch: 040 ----
mean loss: 79.89
train mean loss: 83.43
epoch train time: 0:00:07.659347
elapsed time: 0:18:29.764116
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 15:27:35.778213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.30
 ---- batch: 020 ----
mean loss: 85.19
 ---- batch: 030 ----
mean loss: 84.87
 ---- batch: 040 ----
mean loss: 83.74
train mean loss: 84.46
epoch train time: 0:00:07.632321
elapsed time: 0:18:37.396879
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 15:27:43.410953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.56
 ---- batch: 020 ----
mean loss: 81.44
 ---- batch: 030 ----
mean loss: 83.93
 ---- batch: 040 ----
mean loss: 81.14
train mean loss: 83.25
epoch train time: 0:00:07.625896
elapsed time: 0:18:45.023161
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 15:27:51.037236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.35
 ---- batch: 020 ----
mean loss: 79.28
 ---- batch: 030 ----
mean loss: 82.61
 ---- batch: 040 ----
mean loss: 81.27
train mean loss: 81.79
epoch train time: 0:00:07.669412
elapsed time: 0:18:52.693011
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 15:27:58.707109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.61
 ---- batch: 020 ----
mean loss: 84.19
 ---- batch: 030 ----
mean loss: 81.72
 ---- batch: 040 ----
mean loss: 83.33
train mean loss: 82.09
epoch train time: 0:00:07.809323
elapsed time: 0:19:00.502768
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 15:28:06.516868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.44
 ---- batch: 020 ----
mean loss: 79.17
 ---- batch: 030 ----
mean loss: 83.88
 ---- batch: 040 ----
mean loss: 80.57
train mean loss: 81.21
epoch train time: 0:00:07.662111
elapsed time: 0:19:08.165310
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 15:28:14.179421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.34
 ---- batch: 020 ----
mean loss: 86.63
 ---- batch: 030 ----
mean loss: 80.82
 ---- batch: 040 ----
mean loss: 77.76
train mean loss: 81.43
epoch train time: 0:00:07.652390
elapsed time: 0:19:15.818216
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 15:28:21.832252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.00
 ---- batch: 020 ----
mean loss: 78.48
 ---- batch: 030 ----
mean loss: 81.68
 ---- batch: 040 ----
mean loss: 79.21
train mean loss: 80.20
epoch train time: 0:00:07.638141
elapsed time: 0:19:23.456756
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 15:28:29.470843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.45
 ---- batch: 020 ----
mean loss: 79.40
 ---- batch: 030 ----
mean loss: 81.50
 ---- batch: 040 ----
mean loss: 79.87
train mean loss: 80.73
epoch train time: 0:00:07.486701
elapsed time: 0:19:30.943923
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 15:28:36.958086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.47
 ---- batch: 020 ----
mean loss: 82.31
 ---- batch: 030 ----
mean loss: 83.43
 ---- batch: 040 ----
mean loss: 85.26
train mean loss: 83.19
epoch train time: 0:00:07.466038
elapsed time: 0:19:38.410424
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 15:28:44.424579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.37
 ---- batch: 020 ----
mean loss: 78.96
 ---- batch: 030 ----
mean loss: 78.87
 ---- batch: 040 ----
mean loss: 81.07
train mean loss: 80.04
epoch train time: 0:00:07.647792
elapsed time: 0:19:46.058787
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 15:28:52.072831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.74
 ---- batch: 020 ----
mean loss: 79.65
 ---- batch: 030 ----
mean loss: 76.02
 ---- batch: 040 ----
mean loss: 80.57
train mean loss: 79.02
epoch train time: 0:00:07.596551
elapsed time: 0:19:53.655783
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 15:28:59.669898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.21
 ---- batch: 020 ----
mean loss: 81.17
 ---- batch: 030 ----
mean loss: 78.99
 ---- batch: 040 ----
mean loss: 82.80
train mean loss: 80.50
epoch train time: 0:00:07.640816
elapsed time: 0:20:01.297083
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 15:29:07.311141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.15
 ---- batch: 020 ----
mean loss: 79.44
 ---- batch: 030 ----
mean loss: 85.93
 ---- batch: 040 ----
mean loss: 80.26
train mean loss: 80.71
epoch train time: 0:00:07.628491
elapsed time: 0:20:08.926004
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 15:29:14.940086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.29
 ---- batch: 020 ----
mean loss: 82.95
 ---- batch: 030 ----
mean loss: 81.26
 ---- batch: 040 ----
mean loss: 84.78
train mean loss: 82.24
epoch train time: 0:00:07.569675
elapsed time: 0:20:16.496081
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 15:29:22.510174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.01
 ---- batch: 020 ----
mean loss: 80.54
 ---- batch: 030 ----
mean loss: 79.30
 ---- batch: 040 ----
mean loss: 78.04
train mean loss: 79.06
epoch train time: 0:00:07.561791
elapsed time: 0:20:24.058314
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 15:29:30.072416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.59
 ---- batch: 020 ----
mean loss: 80.67
 ---- batch: 030 ----
mean loss: 82.30
 ---- batch: 040 ----
mean loss: 77.49
train mean loss: 79.62
epoch train time: 0:00:07.721355
elapsed time: 0:20:31.780108
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 15:29:37.794196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.54
 ---- batch: 020 ----
mean loss: 76.04
 ---- batch: 030 ----
mean loss: 76.32
 ---- batch: 040 ----
mean loss: 81.12
train mean loss: 77.72
epoch train time: 0:00:07.718521
elapsed time: 0:20:39.499032
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 15:29:45.513123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.24
 ---- batch: 020 ----
mean loss: 77.49
 ---- batch: 030 ----
mean loss: 76.96
 ---- batch: 040 ----
mean loss: 80.24
train mean loss: 77.56
epoch train time: 0:00:07.731595
elapsed time: 0:20:47.231042
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 15:29:53.245118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.37
 ---- batch: 020 ----
mean loss: 78.77
 ---- batch: 030 ----
mean loss: 77.66
 ---- batch: 040 ----
mean loss: 79.30
train mean loss: 78.04
epoch train time: 0:00:07.547024
elapsed time: 0:20:54.778498
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 15:30:00.792598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.20
 ---- batch: 020 ----
mean loss: 77.27
 ---- batch: 030 ----
mean loss: 74.57
 ---- batch: 040 ----
mean loss: 79.95
train mean loss: 76.92
epoch train time: 0:00:07.421773
elapsed time: 0:21:02.200688
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 15:30:08.214769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.86
 ---- batch: 020 ----
mean loss: 75.33
 ---- batch: 030 ----
mean loss: 79.70
 ---- batch: 040 ----
mean loss: 78.31
train mean loss: 78.04
epoch train time: 0:00:07.427942
elapsed time: 0:21:09.629065
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 15:30:15.643130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.37
 ---- batch: 020 ----
mean loss: 78.72
 ---- batch: 030 ----
mean loss: 78.02
 ---- batch: 040 ----
mean loss: 79.65
train mean loss: 77.14
epoch train time: 0:00:07.631422
elapsed time: 0:21:17.260849
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 15:30:23.274919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.27
 ---- batch: 020 ----
mean loss: 76.33
 ---- batch: 030 ----
mean loss: 78.32
 ---- batch: 040 ----
mean loss: 80.17
train mean loss: 78.77
epoch train time: 0:00:07.622668
elapsed time: 0:21:24.883956
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 15:30:30.898051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.19
 ---- batch: 020 ----
mean loss: 76.49
 ---- batch: 030 ----
mean loss: 77.92
 ---- batch: 040 ----
mean loss: 75.96
train mean loss: 76.62
epoch train time: 0:00:07.602238
elapsed time: 0:21:32.486623
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 15:30:38.500722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.93
 ---- batch: 020 ----
mean loss: 74.68
 ---- batch: 030 ----
mean loss: 77.18
 ---- batch: 040 ----
mean loss: 79.35
train mean loss: 76.84
epoch train time: 0:00:07.477780
elapsed time: 0:21:39.964815
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 15:30:45.978875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.46
 ---- batch: 020 ----
mean loss: 75.12
 ---- batch: 030 ----
mean loss: 79.67
 ---- batch: 040 ----
mean loss: 81.90
train mean loss: 78.80
epoch train time: 0:00:07.491339
elapsed time: 0:21:47.456536
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 15:30:53.470613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.54
 ---- batch: 020 ----
mean loss: 75.42
 ---- batch: 030 ----
mean loss: 76.29
 ---- batch: 040 ----
mean loss: 76.71
train mean loss: 76.39
epoch train time: 0:00:07.523034
elapsed time: 0:21:54.980016
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 15:31:00.994133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.37
 ---- batch: 020 ----
mean loss: 76.61
 ---- batch: 030 ----
mean loss: 74.72
 ---- batch: 040 ----
mean loss: 76.19
train mean loss: 76.59
epoch train time: 0:00:07.720570
elapsed time: 0:22:02.701025
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 15:31:08.715101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.55
 ---- batch: 020 ----
mean loss: 75.44
 ---- batch: 030 ----
mean loss: 78.73
 ---- batch: 040 ----
mean loss: 74.67
train mean loss: 75.84
epoch train time: 0:00:07.711739
elapsed time: 0:22:10.413164
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 15:31:16.427267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.02
 ---- batch: 020 ----
mean loss: 79.30
 ---- batch: 030 ----
mean loss: 76.13
 ---- batch: 040 ----
mean loss: 79.54
train mean loss: 78.46
epoch train time: 0:00:07.694193
elapsed time: 0:22:18.107870
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 15:31:24.121981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.77
 ---- batch: 020 ----
mean loss: 78.12
 ---- batch: 030 ----
mean loss: 75.25
 ---- batch: 040 ----
mean loss: 73.70
train mean loss: 75.11
epoch train time: 0:00:07.713134
elapsed time: 0:22:25.821598
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 15:31:31.835588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.89
 ---- batch: 020 ----
mean loss: 78.37
 ---- batch: 030 ----
mean loss: 75.07
 ---- batch: 040 ----
mean loss: 72.82
train mean loss: 75.68
epoch train time: 0:00:07.696899
elapsed time: 0:22:33.518799
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 15:31:39.532838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.92
 ---- batch: 020 ----
mean loss: 75.02
 ---- batch: 030 ----
mean loss: 77.81
 ---- batch: 040 ----
mean loss: 72.95
train mean loss: 75.12
epoch train time: 0:00:07.754283
elapsed time: 0:22:41.273468
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 15:31:47.287561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.35
 ---- batch: 020 ----
mean loss: 75.10
 ---- batch: 030 ----
mean loss: 75.06
 ---- batch: 040 ----
mean loss: 74.21
train mean loss: 74.38
epoch train time: 0:00:07.745291
elapsed time: 0:22:49.019206
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 15:31:55.033316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.66
 ---- batch: 020 ----
mean loss: 70.61
 ---- batch: 030 ----
mean loss: 74.09
 ---- batch: 040 ----
mean loss: 76.29
train mean loss: 74.90
epoch train time: 0:00:07.767402
elapsed time: 0:22:56.787058
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 15:32:02.801149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.92
 ---- batch: 020 ----
mean loss: 72.67
 ---- batch: 030 ----
mean loss: 72.33
 ---- batch: 040 ----
mean loss: 73.15
train mean loss: 73.95
epoch train time: 0:00:07.713253
elapsed time: 0:23:04.500737
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 15:32:10.514879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.51
 ---- batch: 020 ----
mean loss: 75.15
 ---- batch: 030 ----
mean loss: 74.22
 ---- batch: 040 ----
mean loss: 72.62
train mean loss: 74.34
epoch train time: 0:00:07.611017
elapsed time: 0:23:12.112247
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 15:32:18.126346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.60
 ---- batch: 020 ----
mean loss: 77.19
 ---- batch: 030 ----
mean loss: 72.00
 ---- batch: 040 ----
mean loss: 73.17
train mean loss: 75.45
epoch train time: 0:00:07.570569
elapsed time: 0:23:19.683300
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 15:32:25.697433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.52
 ---- batch: 020 ----
mean loss: 71.92
 ---- batch: 030 ----
mean loss: 72.63
 ---- batch: 040 ----
mean loss: 73.57
train mean loss: 73.08
epoch train time: 0:00:07.580688
elapsed time: 0:23:27.264542
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 15:32:33.278644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.41
 ---- batch: 020 ----
mean loss: 78.12
 ---- batch: 030 ----
mean loss: 72.87
 ---- batch: 040 ----
mean loss: 72.17
train mean loss: 73.93
epoch train time: 0:00:07.580551
elapsed time: 0:23:34.845574
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 15:32:40.859667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.73
 ---- batch: 020 ----
mean loss: 73.91
 ---- batch: 030 ----
mean loss: 75.95
 ---- batch: 040 ----
mean loss: 76.32
train mean loss: 74.31
epoch train time: 0:00:07.581251
elapsed time: 0:23:42.427237
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 15:32:48.441320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.45
 ---- batch: 020 ----
mean loss: 75.37
 ---- batch: 030 ----
mean loss: 71.26
 ---- batch: 040 ----
mean loss: 76.41
train mean loss: 73.38
epoch train time: 0:00:07.595303
elapsed time: 0:23:50.023033
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 15:32:56.037125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.34
 ---- batch: 020 ----
mean loss: 68.12
 ---- batch: 030 ----
mean loss: 73.03
 ---- batch: 040 ----
mean loss: 74.61
train mean loss: 72.28
epoch train time: 0:00:07.788825
elapsed time: 0:23:57.812306
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 15:33:03.826407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.71
 ---- batch: 020 ----
mean loss: 76.41
 ---- batch: 030 ----
mean loss: 71.39
 ---- batch: 040 ----
mean loss: 69.15
train mean loss: 73.10
epoch train time: 0:00:07.731319
elapsed time: 0:24:05.544040
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 15:33:11.558112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.05
 ---- batch: 020 ----
mean loss: 71.20
 ---- batch: 030 ----
mean loss: 73.12
 ---- batch: 040 ----
mean loss: 74.54
train mean loss: 72.09
epoch train time: 0:00:07.737973
elapsed time: 0:24:13.282433
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 15:33:19.296544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.40
 ---- batch: 020 ----
mean loss: 74.41
 ---- batch: 030 ----
mean loss: 71.68
 ---- batch: 040 ----
mean loss: 74.43
train mean loss: 72.68
epoch train time: 0:00:07.726563
elapsed time: 0:24:21.009476
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 15:33:27.023641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.93
 ---- batch: 020 ----
mean loss: 71.49
 ---- batch: 030 ----
mean loss: 71.21
 ---- batch: 040 ----
mean loss: 75.21
train mean loss: 73.28
epoch train time: 0:00:07.731573
elapsed time: 0:24:28.741578
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 15:33:34.755657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.73
 ---- batch: 020 ----
mean loss: 72.48
 ---- batch: 030 ----
mean loss: 70.20
 ---- batch: 040 ----
mean loss: 71.98
train mean loss: 71.72
epoch train time: 0:00:07.713661
elapsed time: 0:24:36.455655
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 15:33:42.469745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.09
 ---- batch: 020 ----
mean loss: 69.71
 ---- batch: 030 ----
mean loss: 72.17
 ---- batch: 040 ----
mean loss: 69.83
train mean loss: 70.71
epoch train time: 0:00:07.681070
elapsed time: 0:24:44.137201
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 15:33:50.151317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.20
 ---- batch: 020 ----
mean loss: 70.57
 ---- batch: 030 ----
mean loss: 72.66
 ---- batch: 040 ----
mean loss: 72.05
train mean loss: 71.21
epoch train time: 0:00:07.679903
elapsed time: 0:24:51.817543
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 15:33:57.831683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.45
 ---- batch: 020 ----
mean loss: 72.18
 ---- batch: 030 ----
mean loss: 72.63
 ---- batch: 040 ----
mean loss: 70.36
train mean loss: 70.85
epoch train time: 0:00:07.735423
elapsed time: 0:24:59.553439
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 15:34:05.567568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.71
 ---- batch: 020 ----
mean loss: 71.22
 ---- batch: 030 ----
mean loss: 69.70
 ---- batch: 040 ----
mean loss: 71.36
train mean loss: 70.80
epoch train time: 0:00:07.733226
elapsed time: 0:25:07.287137
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 15:34:13.301224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.34
 ---- batch: 020 ----
mean loss: 71.76
 ---- batch: 030 ----
mean loss: 74.42
 ---- batch: 040 ----
mean loss: 70.85
train mean loss: 71.41
epoch train time: 0:00:07.689047
elapsed time: 0:25:14.976607
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 15:34:20.990711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.90
 ---- batch: 020 ----
mean loss: 72.52
 ---- batch: 030 ----
mean loss: 69.51
 ---- batch: 040 ----
mean loss: 69.40
train mean loss: 70.11
epoch train time: 0:00:07.699062
elapsed time: 0:25:22.676087
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 15:34:28.690152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.15
 ---- batch: 020 ----
mean loss: 72.50
 ---- batch: 030 ----
mean loss: 70.56
 ---- batch: 040 ----
mean loss: 68.74
train mean loss: 70.77
epoch train time: 0:00:07.740002
elapsed time: 0:25:30.416516
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 15:34:36.430611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.26
 ---- batch: 020 ----
mean loss: 69.66
 ---- batch: 030 ----
mean loss: 71.29
 ---- batch: 040 ----
mean loss: 70.64
train mean loss: 70.01
epoch train time: 0:00:07.764660
elapsed time: 0:25:38.181605
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 15:34:44.195688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.21
 ---- batch: 020 ----
mean loss: 69.43
 ---- batch: 030 ----
mean loss: 70.79
 ---- batch: 040 ----
mean loss: 70.02
train mean loss: 69.35
epoch train time: 0:00:07.700433
elapsed time: 0:25:45.882470
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 15:34:51.896500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.70
 ---- batch: 020 ----
mean loss: 70.15
 ---- batch: 030 ----
mean loss: 70.57
 ---- batch: 040 ----
mean loss: 70.21
train mean loss: 70.21
epoch train time: 0:00:07.644360
elapsed time: 0:25:53.527273
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 15:34:59.541387
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.23
 ---- batch: 020 ----
mean loss: 68.82
 ---- batch: 030 ----
mean loss: 69.20
 ---- batch: 040 ----
mean loss: 68.48
train mean loss: 68.43
epoch train time: 0:00:07.760884
elapsed time: 0:26:01.288732
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 15:35:07.302716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.48
 ---- batch: 020 ----
mean loss: 69.81
 ---- batch: 030 ----
mean loss: 66.84
 ---- batch: 040 ----
mean loss: 65.57
train mean loss: 67.10
epoch train time: 0:00:07.615326
elapsed time: 0:26:08.904356
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 15:35:14.918396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.29
 ---- batch: 020 ----
mean loss: 70.14
 ---- batch: 030 ----
mean loss: 66.98
 ---- batch: 040 ----
mean loss: 66.45
train mean loss: 67.82
epoch train time: 0:00:07.611605
elapsed time: 0:26:16.516325
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 15:35:22.530430
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.49
 ---- batch: 020 ----
mean loss: 67.24
 ---- batch: 030 ----
mean loss: 67.79
 ---- batch: 040 ----
mean loss: 70.37
train mean loss: 67.69
epoch train time: 0:00:07.622519
elapsed time: 0:26:24.139270
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 15:35:30.153351
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.65
 ---- batch: 020 ----
mean loss: 67.03
 ---- batch: 030 ----
mean loss: 66.50
 ---- batch: 040 ----
mean loss: 67.22
train mean loss: 67.43
epoch train time: 0:00:07.616378
elapsed time: 0:26:31.756059
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 15:35:37.770101
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.04
 ---- batch: 020 ----
mean loss: 67.56
 ---- batch: 030 ----
mean loss: 68.83
 ---- batch: 040 ----
mean loss: 66.01
train mean loss: 67.69
epoch train time: 0:00:07.697942
elapsed time: 0:26:39.454347
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 15:35:45.468444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.58
 ---- batch: 020 ----
mean loss: 68.29
 ---- batch: 030 ----
mean loss: 67.55
 ---- batch: 040 ----
mean loss: 67.32
train mean loss: 68.39
epoch train time: 0:00:07.752884
elapsed time: 0:26:47.207747
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 15:35:53.221857
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.27
 ---- batch: 020 ----
mean loss: 69.51
 ---- batch: 030 ----
mean loss: 66.07
 ---- batch: 040 ----
mean loss: 67.63
train mean loss: 67.50
epoch train time: 0:00:07.677690
elapsed time: 0:26:54.885867
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 15:36:00.899951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.09
 ---- batch: 020 ----
mean loss: 68.20
 ---- batch: 030 ----
mean loss: 64.87
 ---- batch: 040 ----
mean loss: 70.54
train mean loss: 67.25
epoch train time: 0:00:07.696506
elapsed time: 0:27:02.582800
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 15:36:08.596876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.47
 ---- batch: 020 ----
mean loss: 70.12
 ---- batch: 030 ----
mean loss: 69.10
 ---- batch: 040 ----
mean loss: 64.38
train mean loss: 67.76
epoch train time: 0:00:07.708886
elapsed time: 0:27:10.292126
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 15:36:16.306238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.65
 ---- batch: 020 ----
mean loss: 67.66
 ---- batch: 030 ----
mean loss: 65.22
 ---- batch: 040 ----
mean loss: 68.15
train mean loss: 68.05
epoch train time: 0:00:07.710774
elapsed time: 0:27:18.003335
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 15:36:24.017382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.37
 ---- batch: 020 ----
mean loss: 65.90
 ---- batch: 030 ----
mean loss: 66.18
 ---- batch: 040 ----
mean loss: 67.44
train mean loss: 67.06
epoch train time: 0:00:07.692641
elapsed time: 0:27:25.696441
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 15:36:31.710540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.64
 ---- batch: 020 ----
mean loss: 64.37
 ---- batch: 030 ----
mean loss: 65.74
 ---- batch: 040 ----
mean loss: 71.32
train mean loss: 67.52
epoch train time: 0:00:07.719585
elapsed time: 0:27:33.416456
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 15:36:39.430538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.65
 ---- batch: 020 ----
mean loss: 68.69
 ---- batch: 030 ----
mean loss: 67.32
 ---- batch: 040 ----
mean loss: 67.25
train mean loss: 67.70
epoch train time: 0:00:07.710941
elapsed time: 0:27:41.127795
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 15:36:47.141915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.64
 ---- batch: 020 ----
mean loss: 68.37
 ---- batch: 030 ----
mean loss: 66.94
 ---- batch: 040 ----
mean loss: 67.12
train mean loss: 67.58
epoch train time: 0:00:07.720926
elapsed time: 0:27:48.849250
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 15:36:54.863395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.83
 ---- batch: 020 ----
mean loss: 66.94
 ---- batch: 030 ----
mean loss: 68.09
 ---- batch: 040 ----
mean loss: 63.38
train mean loss: 67.52
epoch train time: 0:00:07.716958
elapsed time: 0:27:56.566891
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 15:37:02.581078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.33
 ---- batch: 020 ----
mean loss: 66.34
 ---- batch: 030 ----
mean loss: 69.60
 ---- batch: 040 ----
mean loss: 67.24
train mean loss: 67.74
epoch train time: 0:00:07.744679
elapsed time: 0:28:04.312121
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 15:37:10.326234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.08
 ---- batch: 020 ----
mean loss: 69.56
 ---- batch: 030 ----
mean loss: 68.67
 ---- batch: 040 ----
mean loss: 66.77
train mean loss: 67.52
epoch train time: 0:00:07.604576
elapsed time: 0:28:11.917137
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 15:37:17.931180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.23
 ---- batch: 020 ----
mean loss: 68.51
 ---- batch: 030 ----
mean loss: 67.54
 ---- batch: 040 ----
mean loss: 68.07
train mean loss: 67.91
epoch train time: 0:00:07.561663
elapsed time: 0:28:19.479161
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 15:37:25.493240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.77
 ---- batch: 020 ----
mean loss: 69.43
 ---- batch: 030 ----
mean loss: 69.45
 ---- batch: 040 ----
mean loss: 66.66
train mean loss: 67.54
epoch train time: 0:00:07.593653
elapsed time: 0:28:27.073213
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 15:37:33.087308
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.19
 ---- batch: 020 ----
mean loss: 67.48
 ---- batch: 030 ----
mean loss: 65.91
 ---- batch: 040 ----
mean loss: 69.22
train mean loss: 67.54
epoch train time: 0:00:07.482364
elapsed time: 0:28:34.556016
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 15:37:40.570127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.83
 ---- batch: 020 ----
mean loss: 68.98
 ---- batch: 030 ----
mean loss: 68.24
 ---- batch: 040 ----
mean loss: 66.26
train mean loss: 67.66
epoch train time: 0:00:07.462910
elapsed time: 0:28:42.019393
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 15:37:48.033501
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.10
 ---- batch: 020 ----
mean loss: 67.56
 ---- batch: 030 ----
mean loss: 67.35
 ---- batch: 040 ----
mean loss: 66.21
train mean loss: 67.26
epoch train time: 0:00:07.563112
elapsed time: 0:28:49.582944
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 15:37:55.597020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.47
 ---- batch: 020 ----
mean loss: 67.77
 ---- batch: 030 ----
mean loss: 66.88
 ---- batch: 040 ----
mean loss: 67.98
train mean loss: 67.37
epoch train time: 0:00:07.758907
elapsed time: 0:28:57.342262
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 15:38:03.356364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.37
 ---- batch: 020 ----
mean loss: 65.95
 ---- batch: 030 ----
mean loss: 68.11
 ---- batch: 040 ----
mean loss: 68.40
train mean loss: 67.44
epoch train time: 0:00:07.795274
elapsed time: 0:29:05.137972
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 15:38:11.152069
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.08
 ---- batch: 020 ----
mean loss: 66.54
 ---- batch: 030 ----
mean loss: 66.03
 ---- batch: 040 ----
mean loss: 67.66
train mean loss: 67.11
epoch train time: 0:00:07.602563
elapsed time: 0:29:12.740984
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 15:38:18.755083
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.07
 ---- batch: 020 ----
mean loss: 67.91
 ---- batch: 030 ----
mean loss: 70.84
 ---- batch: 040 ----
mean loss: 63.72
train mean loss: 67.16
epoch train time: 0:00:07.575036
elapsed time: 0:29:20.316516
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 15:38:26.330678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.48
 ---- batch: 020 ----
mean loss: 65.30
 ---- batch: 030 ----
mean loss: 70.10
 ---- batch: 040 ----
mean loss: 65.75
train mean loss: 67.28
epoch train time: 0:00:07.648579
elapsed time: 0:29:27.965621
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 15:38:33.979757
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.16
 ---- batch: 020 ----
mean loss: 66.92
 ---- batch: 030 ----
mean loss: 67.26
 ---- batch: 040 ----
mean loss: 67.43
train mean loss: 66.95
epoch train time: 0:00:07.652829
elapsed time: 0:29:35.618972
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 15:38:41.633072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.53
 ---- batch: 020 ----
mean loss: 67.20
 ---- batch: 030 ----
mean loss: 69.36
 ---- batch: 040 ----
mean loss: 64.17
train mean loss: 67.42
epoch train time: 0:00:07.636463
elapsed time: 0:29:43.255909
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 15:38:49.270011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.41
 ---- batch: 020 ----
mean loss: 65.25
 ---- batch: 030 ----
mean loss: 66.54
 ---- batch: 040 ----
mean loss: 67.91
train mean loss: 67.06
epoch train time: 0:00:07.693404
elapsed time: 0:29:50.949754
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 15:38:56.963810
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.46
 ---- batch: 020 ----
mean loss: 68.15
 ---- batch: 030 ----
mean loss: 67.10
 ---- batch: 040 ----
mean loss: 65.84
train mean loss: 66.69
epoch train time: 0:00:07.705108
elapsed time: 0:29:58.655261
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 15:39:04.669301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.94
 ---- batch: 020 ----
mean loss: 69.68
 ---- batch: 030 ----
mean loss: 66.63
 ---- batch: 040 ----
mean loss: 66.60
train mean loss: 67.86
epoch train time: 0:00:07.871413
elapsed time: 0:30:06.527158
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 15:39:12.541154
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.36
 ---- batch: 020 ----
mean loss: 66.78
 ---- batch: 030 ----
mean loss: 67.24
 ---- batch: 040 ----
mean loss: 66.63
train mean loss: 66.79
epoch train time: 0:00:07.893180
elapsed time: 0:30:14.420649
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 15:39:20.434779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.22
 ---- batch: 020 ----
mean loss: 67.52
 ---- batch: 030 ----
mean loss: 66.34
 ---- batch: 040 ----
mean loss: 69.80
train mean loss: 67.51
epoch train time: 0:00:07.867896
elapsed time: 0:30:22.289013
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 15:39:28.303109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 64.78
 ---- batch: 020 ----
mean loss: 68.80
 ---- batch: 030 ----
mean loss: 68.24
 ---- batch: 040 ----
mean loss: 66.48
train mean loss: 67.28
epoch train time: 0:00:07.895967
elapsed time: 0:30:30.185407
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 15:39:36.199493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.02
 ---- batch: 020 ----
mean loss: 65.78
 ---- batch: 030 ----
mean loss: 66.51
 ---- batch: 040 ----
mean loss: 67.05
train mean loss: 66.58
epoch train time: 0:00:07.892192
elapsed time: 0:30:38.078039
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 15:39:44.092120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.84
 ---- batch: 020 ----
mean loss: 65.57
 ---- batch: 030 ----
mean loss: 68.24
 ---- batch: 040 ----
mean loss: 67.61
train mean loss: 67.16
epoch train time: 0:00:07.845863
elapsed time: 0:30:45.924304
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 15:39:51.938380
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.12
 ---- batch: 020 ----
mean loss: 65.40
 ---- batch: 030 ----
mean loss: 69.06
 ---- batch: 040 ----
mean loss: 68.22
train mean loss: 67.17
epoch train time: 0:00:07.875336
elapsed time: 0:30:53.800072
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 15:39:59.814147
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.97
 ---- batch: 020 ----
mean loss: 68.39
 ---- batch: 030 ----
mean loss: 63.52
 ---- batch: 040 ----
mean loss: 69.35
train mean loss: 66.92
epoch train time: 0:00:07.888166
elapsed time: 0:31:01.688653
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 15:40:07.702702
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.77
 ---- batch: 020 ----
mean loss: 66.83
 ---- batch: 030 ----
mean loss: 67.87
 ---- batch: 040 ----
mean loss: 67.19
train mean loss: 67.06
epoch train time: 0:00:07.907433
elapsed time: 0:31:09.596459
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 15:40:15.610556
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.27
 ---- batch: 020 ----
mean loss: 64.20
 ---- batch: 030 ----
mean loss: 66.49
 ---- batch: 040 ----
mean loss: 68.26
train mean loss: 66.37
epoch train time: 0:00:07.876630
elapsed time: 0:31:17.473605
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 15:40:23.487727
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.71
 ---- batch: 020 ----
mean loss: 66.82
 ---- batch: 030 ----
mean loss: 66.40
 ---- batch: 040 ----
mean loss: 66.82
train mean loss: 66.72
epoch train time: 0:00:07.853757
elapsed time: 0:31:25.327884
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 15:40:31.342005
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.83
 ---- batch: 020 ----
mean loss: 68.05
 ---- batch: 030 ----
mean loss: 66.80
 ---- batch: 040 ----
mean loss: 68.15
train mean loss: 66.98
epoch train time: 0:00:07.856465
elapsed time: 0:31:33.184830
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 15:40:39.198932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.09
 ---- batch: 020 ----
mean loss: 70.83
 ---- batch: 030 ----
mean loss: 66.94
 ---- batch: 040 ----
mean loss: 64.94
train mean loss: 66.70
epoch train time: 0:00:07.845372
elapsed time: 0:31:41.030749
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 15:40:47.044861
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.33
 ---- batch: 020 ----
mean loss: 66.51
 ---- batch: 030 ----
mean loss: 69.48
 ---- batch: 040 ----
mean loss: 66.28
train mean loss: 66.93
epoch train time: 0:00:07.869971
elapsed time: 0:31:48.901229
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 15:40:54.915417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.45
 ---- batch: 020 ----
mean loss: 67.03
 ---- batch: 030 ----
mean loss: 64.98
 ---- batch: 040 ----
mean loss: 66.13
train mean loss: 66.59
epoch train time: 0:00:07.873632
elapsed time: 0:31:56.775436
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 15:41:02.789538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.38
 ---- batch: 020 ----
mean loss: 65.01
 ---- batch: 030 ----
mean loss: 70.46
 ---- batch: 040 ----
mean loss: 64.86
train mean loss: 66.76
epoch train time: 0:00:07.908473
elapsed time: 0:32:04.684353
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 15:41:10.698435
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.51
 ---- batch: 020 ----
mean loss: 66.60
 ---- batch: 030 ----
mean loss: 66.96
 ---- batch: 040 ----
mean loss: 65.21
train mean loss: 66.71
epoch train time: 0:00:07.938837
elapsed time: 0:32:12.632604
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_8/checkpoint.pth.tar
**** end time: 2019-09-20 15:41:18.646557 ****
