Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24480
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:16:39.556852 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:16:39.560150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4317.55
 ---- batch: 020 ----
mean loss: 4145.43
 ---- batch: 030 ----
mean loss: 4120.73
 ---- batch: 040 ----
mean loss: 3998.95
 ---- batch: 050 ----
mean loss: 3861.51
 ---- batch: 060 ----
mean loss: 3899.05
 ---- batch: 070 ----
mean loss: 3768.45
 ---- batch: 080 ----
mean loss: 3761.76
 ---- batch: 090 ----
mean loss: 3701.80
train mean loss: 3933.22
epoch train time: 0:00:33.696503
elapsed time: 0:00:33.702361
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:17:13.259258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3535.39
 ---- batch: 020 ----
mean loss: 3570.65
 ---- batch: 030 ----
mean loss: 3501.53
 ---- batch: 040 ----
mean loss: 3456.28
 ---- batch: 050 ----
mean loss: 3338.30
 ---- batch: 060 ----
mean loss: 3345.95
 ---- batch: 070 ----
mean loss: 3290.75
 ---- batch: 080 ----
mean loss: 3212.90
 ---- batch: 090 ----
mean loss: 3190.87
train mean loss: 3368.29
epoch train time: 0:00:00.503660
elapsed time: 0:00:34.206166
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:17:13.763070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3078.14
 ---- batch: 020 ----
mean loss: 3023.26
 ---- batch: 030 ----
mean loss: 3014.12
 ---- batch: 040 ----
mean loss: 2971.47
 ---- batch: 050 ----
mean loss: 2942.41
 ---- batch: 060 ----
mean loss: 2895.38
 ---- batch: 070 ----
mean loss: 2884.13
 ---- batch: 080 ----
mean loss: 2812.74
 ---- batch: 090 ----
mean loss: 2738.16
train mean loss: 2919.35
epoch train time: 0:00:00.500555
elapsed time: 0:00:34.706869
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:17:14.263773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2698.09
 ---- batch: 020 ----
mean loss: 2624.31
 ---- batch: 030 ----
mean loss: 2578.30
 ---- batch: 040 ----
mean loss: 2600.04
 ---- batch: 050 ----
mean loss: 2538.75
 ---- batch: 060 ----
mean loss: 2522.99
 ---- batch: 070 ----
mean loss: 2436.62
 ---- batch: 080 ----
mean loss: 2419.33
 ---- batch: 090 ----
mean loss: 2415.41
train mean loss: 2526.24
epoch train time: 0:00:00.499591
elapsed time: 0:00:35.206608
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:17:14.763511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2316.79
 ---- batch: 020 ----
mean loss: 2307.44
 ---- batch: 030 ----
mean loss: 2248.92
 ---- batch: 040 ----
mean loss: 2214.64
 ---- batch: 050 ----
mean loss: 2231.81
 ---- batch: 060 ----
mean loss: 2143.07
 ---- batch: 070 ----
mean loss: 2135.23
 ---- batch: 080 ----
mean loss: 2138.04
 ---- batch: 090 ----
mean loss: 2080.59
train mean loss: 2191.85
epoch train time: 0:00:00.501223
elapsed time: 0:00:35.707986
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:17:15.264881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2003.19
 ---- batch: 020 ----
mean loss: 1971.70
 ---- batch: 030 ----
mean loss: 1981.98
 ---- batch: 040 ----
mean loss: 1934.81
 ---- batch: 050 ----
mean loss: 1971.56
 ---- batch: 060 ----
mean loss: 1860.08
 ---- batch: 070 ----
mean loss: 1880.20
 ---- batch: 080 ----
mean loss: 1883.46
 ---- batch: 090 ----
mean loss: 1798.32
train mean loss: 1911.26
epoch train time: 0:00:00.500080
elapsed time: 0:00:36.208223
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:17:15.765138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1773.31
 ---- batch: 020 ----
mean loss: 1763.02
 ---- batch: 030 ----
mean loss: 1675.32
 ---- batch: 040 ----
mean loss: 1707.59
 ---- batch: 050 ----
mean loss: 1674.63
 ---- batch: 060 ----
mean loss: 1668.34
 ---- batch: 070 ----
mean loss: 1630.26
 ---- batch: 080 ----
mean loss: 1617.48
 ---- batch: 090 ----
mean loss: 1600.49
train mean loss: 1670.30
epoch train time: 0:00:00.513308
elapsed time: 0:00:36.721739
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:17:16.278662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1538.07
 ---- batch: 020 ----
mean loss: 1544.26
 ---- batch: 030 ----
mean loss: 1541.98
 ---- batch: 040 ----
mean loss: 1464.45
 ---- batch: 050 ----
mean loss: 1503.55
 ---- batch: 060 ----
mean loss: 1478.51
 ---- batch: 070 ----
mean loss: 1424.58
 ---- batch: 080 ----
mean loss: 1443.90
 ---- batch: 090 ----
mean loss: 1398.94
train mean loss: 1475.28
epoch train time: 0:00:00.511412
elapsed time: 0:00:37.233319
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:17:16.790256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1385.68
 ---- batch: 020 ----
mean loss: 1340.16
 ---- batch: 030 ----
mean loss: 1337.00
 ---- batch: 040 ----
mean loss: 1317.80
 ---- batch: 050 ----
mean loss: 1343.40
 ---- batch: 060 ----
mean loss: 1285.05
 ---- batch: 070 ----
mean loss: 1300.56
 ---- batch: 080 ----
mean loss: 1254.30
 ---- batch: 090 ----
mean loss: 1281.59
train mean loss: 1313.05
epoch train time: 0:00:00.495069
elapsed time: 0:00:37.728569
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:17:17.285471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1249.14
 ---- batch: 020 ----
mean loss: 1218.81
 ---- batch: 030 ----
mean loss: 1225.26
 ---- batch: 040 ----
mean loss: 1175.24
 ---- batch: 050 ----
mean loss: 1203.11
 ---- batch: 060 ----
mean loss: 1183.12
 ---- batch: 070 ----
mean loss: 1193.30
 ---- batch: 080 ----
mean loss: 1147.17
 ---- batch: 090 ----
mean loss: 1161.73
train mean loss: 1190.73
epoch train time: 0:00:00.489530
elapsed time: 0:00:38.218241
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:17:17.775171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1113.32
 ---- batch: 020 ----
mean loss: 1131.64
 ---- batch: 030 ----
mean loss: 1125.97
 ---- batch: 040 ----
mean loss: 1092.11
 ---- batch: 050 ----
mean loss: 1090.72
 ---- batch: 060 ----
mean loss: 1103.37
 ---- batch: 070 ----
mean loss: 1086.02
 ---- batch: 080 ----
mean loss: 1055.21
 ---- batch: 090 ----
mean loss: 1077.84
train mean loss: 1095.35
epoch train time: 0:00:00.499022
elapsed time: 0:00:38.717499
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:17:18.274402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1039.39
 ---- batch: 020 ----
mean loss: 1049.46
 ---- batch: 030 ----
mean loss: 1052.93
 ---- batch: 040 ----
mean loss: 1039.65
 ---- batch: 050 ----
mean loss: 1032.01
 ---- batch: 060 ----
mean loss: 1025.20
 ---- batch: 070 ----
mean loss: 1024.00
 ---- batch: 080 ----
mean loss: 993.61
 ---- batch: 090 ----
mean loss: 982.53
train mean loss: 1022.98
epoch train time: 0:00:00.499704
elapsed time: 0:00:39.217350
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:17:18.774266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.12
 ---- batch: 020 ----
mean loss: 971.00
 ---- batch: 030 ----
mean loss: 984.71
 ---- batch: 040 ----
mean loss: 961.39
 ---- batch: 050 ----
mean loss: 988.43
 ---- batch: 060 ----
mean loss: 962.25
 ---- batch: 070 ----
mean loss: 946.06
 ---- batch: 080 ----
mean loss: 955.05
 ---- batch: 090 ----
mean loss: 953.79
train mean loss: 966.11
epoch train time: 0:00:00.499285
elapsed time: 0:00:39.716822
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:17:19.273720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.01
 ---- batch: 020 ----
mean loss: 937.74
 ---- batch: 030 ----
mean loss: 932.58
 ---- batch: 040 ----
mean loss: 912.89
 ---- batch: 050 ----
mean loss: 930.70
 ---- batch: 060 ----
mean loss: 925.15
 ---- batch: 070 ----
mean loss: 939.39
 ---- batch: 080 ----
mean loss: 924.13
 ---- batch: 090 ----
mean loss: 922.63
train mean loss: 929.07
epoch train time: 0:00:00.500959
elapsed time: 0:00:40.217983
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:17:19.774911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.55
 ---- batch: 020 ----
mean loss: 916.97
 ---- batch: 030 ----
mean loss: 910.21
 ---- batch: 040 ----
mean loss: 895.32
 ---- batch: 050 ----
mean loss: 903.13
 ---- batch: 060 ----
mean loss: 897.16
 ---- batch: 070 ----
mean loss: 899.11
 ---- batch: 080 ----
mean loss: 913.88
 ---- batch: 090 ----
mean loss: 905.67
train mean loss: 907.45
epoch train time: 0:00:00.494625
elapsed time: 0:00:40.712857
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:17:20.269820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.71
 ---- batch: 020 ----
mean loss: 896.97
 ---- batch: 030 ----
mean loss: 894.84
 ---- batch: 040 ----
mean loss: 903.99
 ---- batch: 050 ----
mean loss: 897.69
 ---- batch: 060 ----
mean loss: 886.06
 ---- batch: 070 ----
mean loss: 870.86
 ---- batch: 080 ----
mean loss: 891.60
 ---- batch: 090 ----
mean loss: 892.52
train mean loss: 893.44
epoch train time: 0:00:00.494119
elapsed time: 0:00:41.207180
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:17:20.764081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.63
 ---- batch: 020 ----
mean loss: 868.26
 ---- batch: 030 ----
mean loss: 883.97
 ---- batch: 040 ----
mean loss: 891.45
 ---- batch: 050 ----
mean loss: 867.82
 ---- batch: 060 ----
mean loss: 893.18
 ---- batch: 070 ----
mean loss: 898.93
 ---- batch: 080 ----
mean loss: 900.24
 ---- batch: 090 ----
mean loss: 870.64
train mean loss: 885.75
epoch train time: 0:00:00.493698
elapsed time: 0:00:41.701035
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:17:21.257928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.40
 ---- batch: 020 ----
mean loss: 876.77
 ---- batch: 030 ----
mean loss: 898.48
 ---- batch: 040 ----
mean loss: 888.99
 ---- batch: 050 ----
mean loss: 870.89
 ---- batch: 060 ----
mean loss: 865.35
 ---- batch: 070 ----
mean loss: 878.77
 ---- batch: 080 ----
mean loss: 875.68
 ---- batch: 090 ----
mean loss: 873.34
train mean loss: 880.62
epoch train time: 0:00:00.500748
elapsed time: 0:00:42.201922
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:17:21.758827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.07
 ---- batch: 020 ----
mean loss: 886.67
 ---- batch: 030 ----
mean loss: 865.11
 ---- batch: 040 ----
mean loss: 883.53
 ---- batch: 050 ----
mean loss: 880.86
 ---- batch: 060 ----
mean loss: 874.66
 ---- batch: 070 ----
mean loss: 860.31
 ---- batch: 080 ----
mean loss: 891.13
 ---- batch: 090 ----
mean loss: 885.46
train mean loss: 878.15
epoch train time: 0:00:00.497050
elapsed time: 0:00:42.699148
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:17:22.256048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.59
 ---- batch: 020 ----
mean loss: 889.27
 ---- batch: 030 ----
mean loss: 876.51
 ---- batch: 040 ----
mean loss: 884.00
 ---- batch: 050 ----
mean loss: 867.18
 ---- batch: 060 ----
mean loss: 878.34
 ---- batch: 070 ----
mean loss: 886.26
 ---- batch: 080 ----
mean loss: 864.22
 ---- batch: 090 ----
mean loss: 872.31
train mean loss: 876.92
epoch train time: 0:00:00.492448
elapsed time: 0:00:43.191739
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:17:22.748653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.85
 ---- batch: 020 ----
mean loss: 868.74
 ---- batch: 030 ----
mean loss: 868.03
 ---- batch: 040 ----
mean loss: 875.98
 ---- batch: 050 ----
mean loss: 894.15
 ---- batch: 060 ----
mean loss: 873.74
 ---- batch: 070 ----
mean loss: 857.91
 ---- batch: 080 ----
mean loss: 889.58
 ---- batch: 090 ----
mean loss: 879.35
train mean loss: 875.31
epoch train time: 0:00:00.503059
elapsed time: 0:00:43.694959
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:17:23.251868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.40
 ---- batch: 020 ----
mean loss: 888.08
 ---- batch: 030 ----
mean loss: 867.21
 ---- batch: 040 ----
mean loss: 856.62
 ---- batch: 050 ----
mean loss: 868.13
 ---- batch: 060 ----
mean loss: 891.16
 ---- batch: 070 ----
mean loss: 882.16
 ---- batch: 080 ----
mean loss: 871.27
 ---- batch: 090 ----
mean loss: 876.60
train mean loss: 875.96
epoch train time: 0:00:00.504878
elapsed time: 0:00:44.199991
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:17:23.756912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.01
 ---- batch: 020 ----
mean loss: 870.64
 ---- batch: 030 ----
mean loss: 861.11
 ---- batch: 040 ----
mean loss: 868.36
 ---- batch: 050 ----
mean loss: 878.79
 ---- batch: 060 ----
mean loss: 879.43
 ---- batch: 070 ----
mean loss: 875.82
 ---- batch: 080 ----
mean loss: 878.43
 ---- batch: 090 ----
mean loss: 885.00
train mean loss: 875.11
epoch train time: 0:00:00.510360
elapsed time: 0:00:44.710514
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:17:24.267433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.59
 ---- batch: 020 ----
mean loss: 871.06
 ---- batch: 030 ----
mean loss: 877.14
 ---- batch: 040 ----
mean loss: 858.02
 ---- batch: 050 ----
mean loss: 872.98
 ---- batch: 060 ----
mean loss: 865.63
 ---- batch: 070 ----
mean loss: 881.45
 ---- batch: 080 ----
mean loss: 881.52
 ---- batch: 090 ----
mean loss: 873.40
train mean loss: 876.25
epoch train time: 0:00:00.506491
elapsed time: 0:00:45.217185
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:17:24.774094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.76
 ---- batch: 020 ----
mean loss: 889.01
 ---- batch: 030 ----
mean loss: 883.37
 ---- batch: 040 ----
mean loss: 876.08
 ---- batch: 050 ----
mean loss: 881.02
 ---- batch: 060 ----
mean loss: 877.84
 ---- batch: 070 ----
mean loss: 873.89
 ---- batch: 080 ----
mean loss: 865.61
 ---- batch: 090 ----
mean loss: 883.89
train mean loss: 875.20
epoch train time: 0:00:00.496789
elapsed time: 0:00:45.714128
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:17:25.271032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.50
 ---- batch: 020 ----
mean loss: 869.74
 ---- batch: 030 ----
mean loss: 863.29
 ---- batch: 040 ----
mean loss: 866.25
 ---- batch: 050 ----
mean loss: 864.53
 ---- batch: 060 ----
mean loss: 902.33
 ---- batch: 070 ----
mean loss: 883.60
 ---- batch: 080 ----
mean loss: 878.39
 ---- batch: 090 ----
mean loss: 869.12
train mean loss: 874.90
epoch train time: 0:00:00.500118
elapsed time: 0:00:46.214403
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:17:25.771298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.26
 ---- batch: 020 ----
mean loss: 873.50
 ---- batch: 030 ----
mean loss: 871.79
 ---- batch: 040 ----
mean loss: 872.87
 ---- batch: 050 ----
mean loss: 864.73
 ---- batch: 060 ----
mean loss: 869.85
 ---- batch: 070 ----
mean loss: 879.62
 ---- batch: 080 ----
mean loss: 887.26
 ---- batch: 090 ----
mean loss: 888.04
train mean loss: 875.11
epoch train time: 0:00:00.496848
elapsed time: 0:00:46.711396
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:17:26.268290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.93
 ---- batch: 020 ----
mean loss: 869.18
 ---- batch: 030 ----
mean loss: 878.11
 ---- batch: 040 ----
mean loss: 885.37
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.61
 ---- batch: 080 ----
mean loss: 892.12
 ---- batch: 090 ----
mean loss: 862.40
train mean loss: 874.90
epoch train time: 0:00:00.496411
elapsed time: 0:00:47.207942
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:17:26.764843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.86
 ---- batch: 020 ----
mean loss: 874.97
 ---- batch: 030 ----
mean loss: 861.89
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.70
 ---- batch: 060 ----
mean loss: 883.32
 ---- batch: 070 ----
mean loss: 889.98
 ---- batch: 080 ----
mean loss: 863.96
 ---- batch: 090 ----
mean loss: 859.31
train mean loss: 875.83
epoch train time: 0:00:00.501902
elapsed time: 0:00:47.710000
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:17:27.266902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.46
 ---- batch: 030 ----
mean loss: 870.56
 ---- batch: 040 ----
mean loss: 876.61
 ---- batch: 050 ----
mean loss: 875.43
 ---- batch: 060 ----
mean loss: 878.10
 ---- batch: 070 ----
mean loss: 872.73
 ---- batch: 080 ----
mean loss: 870.15
 ---- batch: 090 ----
mean loss: 857.33
train mean loss: 874.97
epoch train time: 0:00:00.497139
elapsed time: 0:00:48.207291
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:17:27.764203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.88
 ---- batch: 020 ----
mean loss: 871.09
 ---- batch: 030 ----
mean loss: 868.37
 ---- batch: 040 ----
mean loss: 878.46
 ---- batch: 050 ----
mean loss: 892.61
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.11
train mean loss: 875.07
epoch train time: 0:00:00.496636
elapsed time: 0:00:48.704083
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:17:28.260985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.11
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.01
 ---- batch: 050 ----
mean loss: 876.37
 ---- batch: 060 ----
mean loss: 886.69
 ---- batch: 070 ----
mean loss: 861.69
 ---- batch: 080 ----
mean loss: 882.77
 ---- batch: 090 ----
mean loss: 870.74
train mean loss: 874.91
epoch train time: 0:00:00.494629
elapsed time: 0:00:49.198865
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:17:28.755760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.46
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.35
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.20
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.78
train mean loss: 876.12
epoch train time: 0:00:00.509190
elapsed time: 0:00:49.708225
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:17:29.265144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.88
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.29
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.77
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.511414
elapsed time: 0:00:50.219804
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:17:29.776708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.81
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.56
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.59
 ---- batch: 070 ----
mean loss: 878.83
 ---- batch: 080 ----
mean loss: 875.94
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.70
epoch train time: 0:00:00.514354
elapsed time: 0:00:50.734319
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:17:30.291224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.45
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.43
 ---- batch: 070 ----
mean loss: 861.54
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.21
train mean loss: 875.33
epoch train time: 0:00:00.501115
elapsed time: 0:00:51.235589
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:17:30.792503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.86
 ---- batch: 040 ----
mean loss: 896.13
 ---- batch: 050 ----
mean loss: 895.66
 ---- batch: 060 ----
mean loss: 872.61
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.89
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.502856
elapsed time: 0:00:51.738600
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:17:31.295503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.29
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 882.00
 ---- batch: 090 ----
mean loss: 857.67
train mean loss: 875.42
epoch train time: 0:00:00.496233
elapsed time: 0:00:52.234979
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:17:31.791901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.13
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.71
train mean loss: 875.94
epoch train time: 0:00:00.499229
elapsed time: 0:00:52.734373
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:17:32.291276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.78
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.58
train mean loss: 875.20
epoch train time: 0:00:00.495398
elapsed time: 0:00:53.229914
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:17:32.786813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.90
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.49
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.95
 ---- batch: 090 ----
mean loss: 867.30
train mean loss: 874.55
epoch train time: 0:00:00.503274
elapsed time: 0:00:53.733331
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:17:33.290234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.84
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.497336
elapsed time: 0:00:54.230822
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:17:33.787715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.14
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.48
 ---- batch: 060 ----
mean loss: 873.50
 ---- batch: 070 ----
mean loss: 889.67
 ---- batch: 080 ----
mean loss: 893.94
 ---- batch: 090 ----
mean loss: 900.18
train mean loss: 875.66
epoch train time: 0:00:00.501782
elapsed time: 0:00:54.732739
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:17:34.289666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.41
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.92
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.11
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.24
epoch train time: 0:00:00.492302
elapsed time: 0:00:55.225208
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:17:34.782108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.91
 ---- batch: 020 ----
mean loss: 896.00
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.47
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.66
epoch train time: 0:00:00.497353
elapsed time: 0:00:55.722714
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:17:35.279620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.23
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.63
 ---- batch: 080 ----
mean loss: 876.95
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.495994
elapsed time: 0:00:56.218862
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:17:35.775785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.45
 ---- batch: 070 ----
mean loss: 893.99
 ---- batch: 080 ----
mean loss: 865.68
 ---- batch: 090 ----
mean loss: 871.94
train mean loss: 875.10
epoch train time: 0:00:00.509506
elapsed time: 0:00:56.728534
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:17:36.285436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.71
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.70
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.64
 ---- batch: 060 ----
mean loss: 876.82
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.86
epoch train time: 0:00:00.502930
elapsed time: 0:00:57.231638
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:17:36.788568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.54
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.90
 ---- batch: 050 ----
mean loss: 879.94
 ---- batch: 060 ----
mean loss: 870.33
 ---- batch: 070 ----
mean loss: 893.30
 ---- batch: 080 ----
mean loss: 874.30
 ---- batch: 090 ----
mean loss: 901.59
train mean loss: 874.94
epoch train time: 0:00:00.492880
elapsed time: 0:00:57.724697
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:17:37.281591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.79
 ---- batch: 020 ----
mean loss: 885.95
 ---- batch: 030 ----
mean loss: 869.68
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 866.17
 ---- batch: 060 ----
mean loss: 869.62
 ---- batch: 070 ----
mean loss: 865.49
 ---- batch: 080 ----
mean loss: 885.13
 ---- batch: 090 ----
mean loss: 878.95
train mean loss: 874.96
epoch train time: 0:00:00.492717
elapsed time: 0:00:58.217555
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:17:37.774456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.61
 ---- batch: 020 ----
mean loss: 893.23
 ---- batch: 030 ----
mean loss: 872.99
 ---- batch: 040 ----
mean loss: 888.75
 ---- batch: 050 ----
mean loss: 867.76
 ---- batch: 060 ----
mean loss: 869.10
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 858.03
 ---- batch: 090 ----
mean loss: 875.46
train mean loss: 875.16
epoch train time: 0:00:00.498659
elapsed time: 0:00:58.716350
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:17:38.273248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.40
 ---- batch: 020 ----
mean loss: 873.36
 ---- batch: 030 ----
mean loss: 870.54
 ---- batch: 040 ----
mean loss: 862.73
 ---- batch: 050 ----
mean loss: 871.31
 ---- batch: 060 ----
mean loss: 875.62
 ---- batch: 070 ----
mean loss: 898.83
 ---- batch: 080 ----
mean loss: 874.46
 ---- batch: 090 ----
mean loss: 887.67
train mean loss: 875.70
epoch train time: 0:00:00.504221
elapsed time: 0:00:59.220712
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:17:38.777636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.21
 ---- batch: 020 ----
mean loss: 875.67
 ---- batch: 030 ----
mean loss: 868.28
 ---- batch: 040 ----
mean loss: 876.40
 ---- batch: 050 ----
mean loss: 869.83
 ---- batch: 060 ----
mean loss: 874.42
 ---- batch: 070 ----
mean loss: 877.11
 ---- batch: 080 ----
mean loss: 890.25
 ---- batch: 090 ----
mean loss: 870.34
train mean loss: 875.08
epoch train time: 0:00:00.501221
elapsed time: 0:00:59.722108
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:17:39.279027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.17
 ---- batch: 020 ----
mean loss: 862.61
 ---- batch: 030 ----
mean loss: 882.91
 ---- batch: 040 ----
mean loss: 874.43
 ---- batch: 050 ----
mean loss: 855.80
 ---- batch: 060 ----
mean loss: 812.90
 ---- batch: 070 ----
mean loss: 774.40
 ---- batch: 080 ----
mean loss: 731.05
 ---- batch: 090 ----
mean loss: 677.03
train mean loss: 805.93
epoch train time: 0:00:00.494335
elapsed time: 0:01:00.216643
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:17:39.773545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 601.38
 ---- batch: 020 ----
mean loss: 522.52
 ---- batch: 030 ----
mean loss: 472.76
 ---- batch: 040 ----
mean loss: 432.65
 ---- batch: 050 ----
mean loss: 416.38
 ---- batch: 060 ----
mean loss: 409.62
 ---- batch: 070 ----
mean loss: 407.00
 ---- batch: 080 ----
mean loss: 385.93
 ---- batch: 090 ----
mean loss: 380.39
train mean loss: 443.19
epoch train time: 0:00:00.504226
elapsed time: 0:01:00.721052
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:17:40.278952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.12
 ---- batch: 020 ----
mean loss: 339.72
 ---- batch: 030 ----
mean loss: 339.87
 ---- batch: 040 ----
mean loss: 332.74
 ---- batch: 050 ----
mean loss: 327.68
 ---- batch: 060 ----
mean loss: 326.34
 ---- batch: 070 ----
mean loss: 329.72
 ---- batch: 080 ----
mean loss: 298.03
 ---- batch: 090 ----
mean loss: 295.29
train mean loss: 326.54
epoch train time: 0:00:00.503057
elapsed time: 0:01:01.225253
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:17:40.782169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.95
 ---- batch: 020 ----
mean loss: 284.47
 ---- batch: 030 ----
mean loss: 288.41
 ---- batch: 040 ----
mean loss: 286.85
 ---- batch: 050 ----
mean loss: 282.31
 ---- batch: 060 ----
mean loss: 276.51
 ---- batch: 070 ----
mean loss: 276.19
 ---- batch: 080 ----
mean loss: 275.16
 ---- batch: 090 ----
mean loss: 276.67
train mean loss: 281.62
epoch train time: 0:00:00.504671
elapsed time: 0:01:01.730084
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:17:41.286994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.03
 ---- batch: 020 ----
mean loss: 264.10
 ---- batch: 030 ----
mean loss: 262.19
 ---- batch: 040 ----
mean loss: 257.12
 ---- batch: 050 ----
mean loss: 261.00
 ---- batch: 060 ----
mean loss: 264.81
 ---- batch: 070 ----
mean loss: 248.43
 ---- batch: 080 ----
mean loss: 252.60
 ---- batch: 090 ----
mean loss: 255.79
train mean loss: 258.23
epoch train time: 0:00:00.505213
elapsed time: 0:01:02.235456
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:17:41.792371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.78
 ---- batch: 020 ----
mean loss: 244.58
 ---- batch: 030 ----
mean loss: 246.74
 ---- batch: 040 ----
mean loss: 249.99
 ---- batch: 050 ----
mean loss: 244.64
 ---- batch: 060 ----
mean loss: 246.22
 ---- batch: 070 ----
mean loss: 241.48
 ---- batch: 080 ----
mean loss: 247.84
 ---- batch: 090 ----
mean loss: 249.99
train mean loss: 247.04
epoch train time: 0:00:00.518227
elapsed time: 0:01:02.753846
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:17:42.310751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.23
 ---- batch: 020 ----
mean loss: 249.83
 ---- batch: 030 ----
mean loss: 237.13
 ---- batch: 040 ----
mean loss: 238.49
 ---- batch: 050 ----
mean loss: 242.03
 ---- batch: 060 ----
mean loss: 244.07
 ---- batch: 070 ----
mean loss: 236.76
 ---- batch: 080 ----
mean loss: 233.39
 ---- batch: 090 ----
mean loss: 237.85
train mean loss: 239.86
epoch train time: 0:00:00.506741
elapsed time: 0:01:03.260736
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:17:42.817682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.94
 ---- batch: 020 ----
mean loss: 234.98
 ---- batch: 030 ----
mean loss: 230.76
 ---- batch: 040 ----
mean loss: 232.93
 ---- batch: 050 ----
mean loss: 226.65
 ---- batch: 060 ----
mean loss: 230.17
 ---- batch: 070 ----
mean loss: 238.65
 ---- batch: 080 ----
mean loss: 235.08
 ---- batch: 090 ----
mean loss: 233.31
train mean loss: 232.84
epoch train time: 0:00:00.508664
elapsed time: 0:01:03.769619
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:17:43.326594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.79
 ---- batch: 020 ----
mean loss: 227.73
 ---- batch: 030 ----
mean loss: 224.03
 ---- batch: 040 ----
mean loss: 223.78
 ---- batch: 050 ----
mean loss: 229.20
 ---- batch: 060 ----
mean loss: 235.46
 ---- batch: 070 ----
mean loss: 227.16
 ---- batch: 080 ----
mean loss: 222.81
 ---- batch: 090 ----
mean loss: 238.04
train mean loss: 228.80
epoch train time: 0:00:00.512296
elapsed time: 0:01:04.282147
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:17:43.839049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.15
 ---- batch: 020 ----
mean loss: 218.92
 ---- batch: 030 ----
mean loss: 229.90
 ---- batch: 040 ----
mean loss: 216.51
 ---- batch: 050 ----
mean loss: 225.97
 ---- batch: 060 ----
mean loss: 228.98
 ---- batch: 070 ----
mean loss: 228.92
 ---- batch: 080 ----
mean loss: 229.65
 ---- batch: 090 ----
mean loss: 226.29
train mean loss: 225.38
epoch train time: 0:00:00.503279
elapsed time: 0:01:04.785604
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:17:44.342508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.34
 ---- batch: 020 ----
mean loss: 226.19
 ---- batch: 030 ----
mean loss: 217.52
 ---- batch: 040 ----
mean loss: 223.88
 ---- batch: 050 ----
mean loss: 232.57
 ---- batch: 060 ----
mean loss: 217.92
 ---- batch: 070 ----
mean loss: 217.17
 ---- batch: 080 ----
mean loss: 219.67
 ---- batch: 090 ----
mean loss: 215.03
train mean loss: 221.23
epoch train time: 0:00:00.508661
elapsed time: 0:01:05.294410
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:17:44.851326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.23
 ---- batch: 020 ----
mean loss: 217.49
 ---- batch: 030 ----
mean loss: 227.13
 ---- batch: 040 ----
mean loss: 223.92
 ---- batch: 050 ----
mean loss: 224.83
 ---- batch: 060 ----
mean loss: 220.92
 ---- batch: 070 ----
mean loss: 212.22
 ---- batch: 080 ----
mean loss: 214.67
 ---- batch: 090 ----
mean loss: 217.18
train mean loss: 219.99
epoch train time: 0:00:00.498758
elapsed time: 0:01:05.793359
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:17:45.350264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.97
 ---- batch: 020 ----
mean loss: 212.79
 ---- batch: 030 ----
mean loss: 218.77
 ---- batch: 040 ----
mean loss: 213.55
 ---- batch: 050 ----
mean loss: 217.40
 ---- batch: 060 ----
mean loss: 218.05
 ---- batch: 070 ----
mean loss: 214.25
 ---- batch: 080 ----
mean loss: 224.12
 ---- batch: 090 ----
mean loss: 215.97
train mean loss: 216.24
epoch train time: 0:00:00.500234
elapsed time: 0:01:06.293767
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:17:45.850668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.31
 ---- batch: 020 ----
mean loss: 207.04
 ---- batch: 030 ----
mean loss: 224.43
 ---- batch: 040 ----
mean loss: 222.17
 ---- batch: 050 ----
mean loss: 215.32
 ---- batch: 060 ----
mean loss: 223.64
 ---- batch: 070 ----
mean loss: 212.38
 ---- batch: 080 ----
mean loss: 212.20
 ---- batch: 090 ----
mean loss: 206.37
train mean loss: 214.56
epoch train time: 0:00:00.494381
elapsed time: 0:01:06.788314
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:17:46.345218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.66
 ---- batch: 020 ----
mean loss: 212.83
 ---- batch: 030 ----
mean loss: 205.15
 ---- batch: 040 ----
mean loss: 203.36
 ---- batch: 050 ----
mean loss: 210.01
 ---- batch: 060 ----
mean loss: 207.59
 ---- batch: 070 ----
mean loss: 210.19
 ---- batch: 080 ----
mean loss: 210.58
 ---- batch: 090 ----
mean loss: 205.23
train mean loss: 208.87
epoch train time: 0:00:00.495365
elapsed time: 0:01:07.283852
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:17:46.840774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.66
 ---- batch: 020 ----
mean loss: 210.24
 ---- batch: 030 ----
mean loss: 197.78
 ---- batch: 040 ----
mean loss: 205.07
 ---- batch: 050 ----
mean loss: 202.85
 ---- batch: 060 ----
mean loss: 206.58
 ---- batch: 070 ----
mean loss: 218.26
 ---- batch: 080 ----
mean loss: 206.65
 ---- batch: 090 ----
mean loss: 213.08
train mean loss: 207.62
epoch train time: 0:00:00.514193
elapsed time: 0:01:07.798212
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:17:47.355117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.04
 ---- batch: 020 ----
mean loss: 202.32
 ---- batch: 030 ----
mean loss: 201.22
 ---- batch: 040 ----
mean loss: 201.60
 ---- batch: 050 ----
mean loss: 202.88
 ---- batch: 060 ----
mean loss: 199.88
 ---- batch: 070 ----
mean loss: 195.74
 ---- batch: 080 ----
mean loss: 211.51
 ---- batch: 090 ----
mean loss: 211.58
train mean loss: 205.16
epoch train time: 0:00:00.508491
elapsed time: 0:01:08.306883
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:17:47.863818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.30
 ---- batch: 020 ----
mean loss: 194.97
 ---- batch: 030 ----
mean loss: 201.86
 ---- batch: 040 ----
mean loss: 209.80
 ---- batch: 050 ----
mean loss: 203.32
 ---- batch: 060 ----
mean loss: 199.53
 ---- batch: 070 ----
mean loss: 205.32
 ---- batch: 080 ----
mean loss: 201.30
 ---- batch: 090 ----
mean loss: 205.19
train mean loss: 202.05
epoch train time: 0:00:00.500875
elapsed time: 0:01:08.807940
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:17:48.364841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.38
 ---- batch: 020 ----
mean loss: 197.72
 ---- batch: 030 ----
mean loss: 199.69
 ---- batch: 040 ----
mean loss: 203.88
 ---- batch: 050 ----
mean loss: 202.65
 ---- batch: 060 ----
mean loss: 206.79
 ---- batch: 070 ----
mean loss: 197.92
 ---- batch: 080 ----
mean loss: 203.49
 ---- batch: 090 ----
mean loss: 204.32
train mean loss: 201.62
epoch train time: 0:00:00.502111
elapsed time: 0:01:09.310194
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:17:48.867096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.62
 ---- batch: 020 ----
mean loss: 184.94
 ---- batch: 030 ----
mean loss: 202.68
 ---- batch: 040 ----
mean loss: 196.86
 ---- batch: 050 ----
mean loss: 207.02
 ---- batch: 060 ----
mean loss: 198.31
 ---- batch: 070 ----
mean loss: 200.84
 ---- batch: 080 ----
mean loss: 205.47
 ---- batch: 090 ----
mean loss: 201.86
train mean loss: 198.53
epoch train time: 0:00:00.503436
elapsed time: 0:01:09.813792
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:17:49.370695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.98
 ---- batch: 020 ----
mean loss: 198.47
 ---- batch: 030 ----
mean loss: 202.55
 ---- batch: 040 ----
mean loss: 194.04
 ---- batch: 050 ----
mean loss: 195.05
 ---- batch: 060 ----
mean loss: 198.17
 ---- batch: 070 ----
mean loss: 193.48
 ---- batch: 080 ----
mean loss: 204.63
 ---- batch: 090 ----
mean loss: 200.31
train mean loss: 197.87
epoch train time: 0:00:00.504514
elapsed time: 0:01:10.318453
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:17:49.875356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.34
 ---- batch: 020 ----
mean loss: 196.37
 ---- batch: 030 ----
mean loss: 194.81
 ---- batch: 040 ----
mean loss: 195.91
 ---- batch: 050 ----
mean loss: 195.11
 ---- batch: 060 ----
mean loss: 197.31
 ---- batch: 070 ----
mean loss: 192.47
 ---- batch: 080 ----
mean loss: 192.58
 ---- batch: 090 ----
mean loss: 200.98
train mean loss: 195.70
epoch train time: 0:00:00.506528
elapsed time: 0:01:10.825167
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:17:50.382070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.33
 ---- batch: 020 ----
mean loss: 194.51
 ---- batch: 030 ----
mean loss: 194.78
 ---- batch: 040 ----
mean loss: 197.12
 ---- batch: 050 ----
mean loss: 191.19
 ---- batch: 060 ----
mean loss: 191.69
 ---- batch: 070 ----
mean loss: 198.20
 ---- batch: 080 ----
mean loss: 197.93
 ---- batch: 090 ----
mean loss: 195.16
train mean loss: 194.73
epoch train time: 0:00:00.506510
elapsed time: 0:01:11.331821
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:17:50.888747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.72
 ---- batch: 020 ----
mean loss: 191.37
 ---- batch: 030 ----
mean loss: 179.97
 ---- batch: 040 ----
mean loss: 188.10
 ---- batch: 050 ----
mean loss: 197.60
 ---- batch: 060 ----
mean loss: 192.13
 ---- batch: 070 ----
mean loss: 195.89
 ---- batch: 080 ----
mean loss: 198.28
 ---- batch: 090 ----
mean loss: 193.95
train mean loss: 192.09
epoch train time: 0:00:00.497704
elapsed time: 0:01:11.829696
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:17:51.386598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.62
 ---- batch: 020 ----
mean loss: 190.39
 ---- batch: 030 ----
mean loss: 188.31
 ---- batch: 040 ----
mean loss: 197.90
 ---- batch: 050 ----
mean loss: 180.51
 ---- batch: 060 ----
mean loss: 190.40
 ---- batch: 070 ----
mean loss: 192.78
 ---- batch: 080 ----
mean loss: 193.38
 ---- batch: 090 ----
mean loss: 191.46
train mean loss: 190.02
epoch train time: 0:00:00.500443
elapsed time: 0:01:12.330285
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:17:51.887188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.37
 ---- batch: 020 ----
mean loss: 186.52
 ---- batch: 030 ----
mean loss: 185.78
 ---- batch: 040 ----
mean loss: 199.57
 ---- batch: 050 ----
mean loss: 187.40
 ---- batch: 060 ----
mean loss: 183.11
 ---- batch: 070 ----
mean loss: 201.91
 ---- batch: 080 ----
mean loss: 191.13
 ---- batch: 090 ----
mean loss: 193.86
train mean loss: 189.59
epoch train time: 0:00:00.503421
elapsed time: 0:01:12.833852
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:17:52.390754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.54
 ---- batch: 020 ----
mean loss: 182.19
 ---- batch: 030 ----
mean loss: 181.96
 ---- batch: 040 ----
mean loss: 195.26
 ---- batch: 050 ----
mean loss: 189.51
 ---- batch: 060 ----
mean loss: 194.34
 ---- batch: 070 ----
mean loss: 185.82
 ---- batch: 080 ----
mean loss: 192.68
 ---- batch: 090 ----
mean loss: 189.87
train mean loss: 187.82
epoch train time: 0:00:00.500571
elapsed time: 0:01:13.334568
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:17:52.891503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.05
 ---- batch: 020 ----
mean loss: 180.01
 ---- batch: 030 ----
mean loss: 182.70
 ---- batch: 040 ----
mean loss: 182.69
 ---- batch: 050 ----
mean loss: 191.50
 ---- batch: 060 ----
mean loss: 186.64
 ---- batch: 070 ----
mean loss: 188.12
 ---- batch: 080 ----
mean loss: 187.50
 ---- batch: 090 ----
mean loss: 191.82
train mean loss: 186.63
epoch train time: 0:00:00.507258
elapsed time: 0:01:13.842000
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:17:53.398899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.69
 ---- batch: 020 ----
mean loss: 185.59
 ---- batch: 030 ----
mean loss: 192.29
 ---- batch: 040 ----
mean loss: 184.89
 ---- batch: 050 ----
mean loss: 182.54
 ---- batch: 060 ----
mean loss: 180.26
 ---- batch: 070 ----
mean loss: 185.92
 ---- batch: 080 ----
mean loss: 184.93
 ---- batch: 090 ----
mean loss: 185.94
train mean loss: 184.38
epoch train time: 0:00:00.504793
elapsed time: 0:01:14.346940
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:17:53.903841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.06
 ---- batch: 020 ----
mean loss: 178.93
 ---- batch: 030 ----
mean loss: 178.09
 ---- batch: 040 ----
mean loss: 185.14
 ---- batch: 050 ----
mean loss: 189.62
 ---- batch: 060 ----
mean loss: 186.72
 ---- batch: 070 ----
mean loss: 179.65
 ---- batch: 080 ----
mean loss: 193.09
 ---- batch: 090 ----
mean loss: 183.24
train mean loss: 184.33
epoch train time: 0:00:00.502754
elapsed time: 0:01:14.849843
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:17:54.406764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.50
 ---- batch: 020 ----
mean loss: 174.05
 ---- batch: 030 ----
mean loss: 184.23
 ---- batch: 040 ----
mean loss: 181.40
 ---- batch: 050 ----
mean loss: 186.33
 ---- batch: 060 ----
mean loss: 189.84
 ---- batch: 070 ----
mean loss: 184.15
 ---- batch: 080 ----
mean loss: 187.02
 ---- batch: 090 ----
mean loss: 187.52
train mean loss: 184.15
epoch train time: 0:00:00.501708
elapsed time: 0:01:15.351719
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:17:54.908622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.33
 ---- batch: 020 ----
mean loss: 176.16
 ---- batch: 030 ----
mean loss: 179.10
 ---- batch: 040 ----
mean loss: 181.28
 ---- batch: 050 ----
mean loss: 185.67
 ---- batch: 060 ----
mean loss: 189.16
 ---- batch: 070 ----
mean loss: 174.76
 ---- batch: 080 ----
mean loss: 181.87
 ---- batch: 090 ----
mean loss: 193.06
train mean loss: 182.75
epoch train time: 0:00:00.510375
elapsed time: 0:01:15.862242
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:17:55.419144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.19
 ---- batch: 020 ----
mean loss: 187.47
 ---- batch: 030 ----
mean loss: 174.48
 ---- batch: 040 ----
mean loss: 183.56
 ---- batch: 050 ----
mean loss: 181.69
 ---- batch: 060 ----
mean loss: 177.46
 ---- batch: 070 ----
mean loss: 173.02
 ---- batch: 080 ----
mean loss: 186.64
 ---- batch: 090 ----
mean loss: 188.42
train mean loss: 181.87
epoch train time: 0:00:00.516364
elapsed time: 0:01:16.378759
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:17:55.935694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.24
 ---- batch: 020 ----
mean loss: 175.95
 ---- batch: 030 ----
mean loss: 181.60
 ---- batch: 040 ----
mean loss: 183.78
 ---- batch: 050 ----
mean loss: 174.55
 ---- batch: 060 ----
mean loss: 180.48
 ---- batch: 070 ----
mean loss: 179.27
 ---- batch: 080 ----
mean loss: 181.94
 ---- batch: 090 ----
mean loss: 171.36
train mean loss: 178.73
epoch train time: 0:00:00.507731
elapsed time: 0:01:16.886699
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:17:56.443601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.83
 ---- batch: 020 ----
mean loss: 176.96
 ---- batch: 030 ----
mean loss: 176.05
 ---- batch: 040 ----
mean loss: 180.67
 ---- batch: 050 ----
mean loss: 165.84
 ---- batch: 060 ----
mean loss: 180.65
 ---- batch: 070 ----
mean loss: 180.59
 ---- batch: 080 ----
mean loss: 184.45
 ---- batch: 090 ----
mean loss: 186.00
train mean loss: 179.15
epoch train time: 0:00:00.515200
elapsed time: 0:01:17.402040
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:17:56.958944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.71
 ---- batch: 020 ----
mean loss: 178.31
 ---- batch: 030 ----
mean loss: 175.87
 ---- batch: 040 ----
mean loss: 175.68
 ---- batch: 050 ----
mean loss: 172.92
 ---- batch: 060 ----
mean loss: 183.63
 ---- batch: 070 ----
mean loss: 176.13
 ---- batch: 080 ----
mean loss: 180.97
 ---- batch: 090 ----
mean loss: 177.70
train mean loss: 178.27
epoch train time: 0:00:00.497026
elapsed time: 0:01:17.899222
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:17:57.456128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.03
 ---- batch: 020 ----
mean loss: 179.76
 ---- batch: 030 ----
mean loss: 179.54
 ---- batch: 040 ----
mean loss: 178.52
 ---- batch: 050 ----
mean loss: 178.94
 ---- batch: 060 ----
mean loss: 179.44
 ---- batch: 070 ----
mean loss: 181.93
 ---- batch: 080 ----
mean loss: 177.16
 ---- batch: 090 ----
mean loss: 177.80
train mean loss: 177.97
epoch train time: 0:00:00.501036
elapsed time: 0:01:18.400407
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:17:57.957309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.34
 ---- batch: 020 ----
mean loss: 168.40
 ---- batch: 030 ----
mean loss: 168.24
 ---- batch: 040 ----
mean loss: 176.33
 ---- batch: 050 ----
mean loss: 181.12
 ---- batch: 060 ----
mean loss: 187.90
 ---- batch: 070 ----
mean loss: 175.74
 ---- batch: 080 ----
mean loss: 183.66
 ---- batch: 090 ----
mean loss: 174.54
train mean loss: 176.40
epoch train time: 0:00:00.509357
elapsed time: 0:01:18.909920
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:17:58.466843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.33
 ---- batch: 020 ----
mean loss: 179.33
 ---- batch: 030 ----
mean loss: 170.71
 ---- batch: 040 ----
mean loss: 167.81
 ---- batch: 050 ----
mean loss: 179.02
 ---- batch: 060 ----
mean loss: 178.93
 ---- batch: 070 ----
mean loss: 179.38
 ---- batch: 080 ----
mean loss: 178.95
 ---- batch: 090 ----
mean loss: 174.26
train mean loss: 175.56
epoch train time: 0:00:00.507630
elapsed time: 0:01:19.417718
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:17:58.974622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.87
 ---- batch: 020 ----
mean loss: 177.97
 ---- batch: 030 ----
mean loss: 174.33
 ---- batch: 040 ----
mean loss: 172.97
 ---- batch: 050 ----
mean loss: 166.34
 ---- batch: 060 ----
mean loss: 180.27
 ---- batch: 070 ----
mean loss: 174.65
 ---- batch: 080 ----
mean loss: 182.91
 ---- batch: 090 ----
mean loss: 176.55
train mean loss: 175.15
epoch train time: 0:00:00.509886
elapsed time: 0:01:19.927750
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:17:59.484650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.57
 ---- batch: 020 ----
mean loss: 173.91
 ---- batch: 030 ----
mean loss: 172.43
 ---- batch: 040 ----
mean loss: 176.06
 ---- batch: 050 ----
mean loss: 173.74
 ---- batch: 060 ----
mean loss: 182.32
 ---- batch: 070 ----
mean loss: 180.25
 ---- batch: 080 ----
mean loss: 175.92
 ---- batch: 090 ----
mean loss: 167.38
train mean loss: 174.01
epoch train time: 0:00:00.509609
elapsed time: 0:01:20.437507
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:17:59.994412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.20
 ---- batch: 020 ----
mean loss: 166.75
 ---- batch: 030 ----
mean loss: 176.46
 ---- batch: 040 ----
mean loss: 164.55
 ---- batch: 050 ----
mean loss: 170.18
 ---- batch: 060 ----
mean loss: 182.41
 ---- batch: 070 ----
mean loss: 175.96
 ---- batch: 080 ----
mean loss: 176.29
 ---- batch: 090 ----
mean loss: 179.64
train mean loss: 173.43
epoch train time: 0:00:00.509634
elapsed time: 0:01:20.947287
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:18:00.504222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.88
 ---- batch: 020 ----
mean loss: 170.48
 ---- batch: 030 ----
mean loss: 167.25
 ---- batch: 040 ----
mean loss: 167.98
 ---- batch: 050 ----
mean loss: 175.58
 ---- batch: 060 ----
mean loss: 180.86
 ---- batch: 070 ----
mean loss: 173.34
 ---- batch: 080 ----
mean loss: 174.49
 ---- batch: 090 ----
mean loss: 175.28
train mean loss: 172.95
epoch train time: 0:00:00.513593
elapsed time: 0:01:21.461076
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:18:01.017995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.25
 ---- batch: 020 ----
mean loss: 162.43
 ---- batch: 030 ----
mean loss: 169.65
 ---- batch: 040 ----
mean loss: 170.88
 ---- batch: 050 ----
mean loss: 168.83
 ---- batch: 060 ----
mean loss: 172.43
 ---- batch: 070 ----
mean loss: 184.63
 ---- batch: 080 ----
mean loss: 177.64
 ---- batch: 090 ----
mean loss: 170.03
train mean loss: 171.80
epoch train time: 0:00:00.512295
elapsed time: 0:01:21.973537
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:18:01.530441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.54
 ---- batch: 020 ----
mean loss: 166.75
 ---- batch: 030 ----
mean loss: 167.32
 ---- batch: 040 ----
mean loss: 173.16
 ---- batch: 050 ----
mean loss: 167.47
 ---- batch: 060 ----
mean loss: 168.90
 ---- batch: 070 ----
mean loss: 173.69
 ---- batch: 080 ----
mean loss: 178.03
 ---- batch: 090 ----
mean loss: 175.42
train mean loss: 170.52
epoch train time: 0:00:00.507892
elapsed time: 0:01:22.481577
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:18:02.038496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.83
 ---- batch: 020 ----
mean loss: 168.31
 ---- batch: 030 ----
mean loss: 166.43
 ---- batch: 040 ----
mean loss: 167.17
 ---- batch: 050 ----
mean loss: 173.13
 ---- batch: 060 ----
mean loss: 175.00
 ---- batch: 070 ----
mean loss: 176.10
 ---- batch: 080 ----
mean loss: 173.05
 ---- batch: 090 ----
mean loss: 169.55
train mean loss: 170.64
epoch train time: 0:00:00.494390
elapsed time: 0:01:22.976127
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:18:02.533028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.73
 ---- batch: 020 ----
mean loss: 162.49
 ---- batch: 030 ----
mean loss: 164.84
 ---- batch: 040 ----
mean loss: 164.41
 ---- batch: 050 ----
mean loss: 174.75
 ---- batch: 060 ----
mean loss: 172.41
 ---- batch: 070 ----
mean loss: 172.80
 ---- batch: 080 ----
mean loss: 173.79
 ---- batch: 090 ----
mean loss: 175.02
train mean loss: 170.38
epoch train time: 0:00:00.499929
elapsed time: 0:01:23.476201
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:18:03.033103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.74
 ---- batch: 020 ----
mean loss: 165.01
 ---- batch: 030 ----
mean loss: 169.20
 ---- batch: 040 ----
mean loss: 171.44
 ---- batch: 050 ----
mean loss: 171.64
 ---- batch: 060 ----
mean loss: 162.48
 ---- batch: 070 ----
mean loss: 163.45
 ---- batch: 080 ----
mean loss: 170.12
 ---- batch: 090 ----
mean loss: 173.72
train mean loss: 169.00
epoch train time: 0:00:00.503200
elapsed time: 0:01:23.979545
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:18:03.536448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.04
 ---- batch: 020 ----
mean loss: 164.45
 ---- batch: 030 ----
mean loss: 166.45
 ---- batch: 040 ----
mean loss: 170.22
 ---- batch: 050 ----
mean loss: 172.06
 ---- batch: 060 ----
mean loss: 173.22
 ---- batch: 070 ----
mean loss: 167.88
 ---- batch: 080 ----
mean loss: 161.83
 ---- batch: 090 ----
mean loss: 167.43
train mean loss: 168.24
epoch train time: 0:00:00.499232
elapsed time: 0:01:24.478956
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:18:04.035889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.66
 ---- batch: 020 ----
mean loss: 163.47
 ---- batch: 030 ----
mean loss: 165.19
 ---- batch: 040 ----
mean loss: 164.36
 ---- batch: 050 ----
mean loss: 168.14
 ---- batch: 060 ----
mean loss: 172.02
 ---- batch: 070 ----
mean loss: 165.45
 ---- batch: 080 ----
mean loss: 167.14
 ---- batch: 090 ----
mean loss: 171.66
train mean loss: 167.65
epoch train time: 0:00:00.504340
elapsed time: 0:01:24.983472
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:18:04.540376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.13
 ---- batch: 020 ----
mean loss: 167.57
 ---- batch: 030 ----
mean loss: 163.96
 ---- batch: 040 ----
mean loss: 163.61
 ---- batch: 050 ----
mean loss: 164.63
 ---- batch: 060 ----
mean loss: 165.75
 ---- batch: 070 ----
mean loss: 172.74
 ---- batch: 080 ----
mean loss: 158.89
 ---- batch: 090 ----
mean loss: 169.45
train mean loss: 166.24
epoch train time: 0:00:00.508406
elapsed time: 0:01:25.492025
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:18:05.048936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.12
 ---- batch: 020 ----
mean loss: 168.03
 ---- batch: 030 ----
mean loss: 163.02
 ---- batch: 040 ----
mean loss: 170.39
 ---- batch: 050 ----
mean loss: 166.37
 ---- batch: 060 ----
mean loss: 169.46
 ---- batch: 070 ----
mean loss: 163.49
 ---- batch: 080 ----
mean loss: 162.41
 ---- batch: 090 ----
mean loss: 163.31
train mean loss: 166.19
epoch train time: 0:00:00.505313
elapsed time: 0:01:25.997504
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:18:05.554409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.75
 ---- batch: 020 ----
mean loss: 159.03
 ---- batch: 030 ----
mean loss: 171.90
 ---- batch: 040 ----
mean loss: 161.21
 ---- batch: 050 ----
mean loss: 157.69
 ---- batch: 060 ----
mean loss: 170.98
 ---- batch: 070 ----
mean loss: 168.80
 ---- batch: 080 ----
mean loss: 166.93
 ---- batch: 090 ----
mean loss: 170.34
train mean loss: 164.97
epoch train time: 0:00:00.501383
elapsed time: 0:01:26.499034
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:18:06.055963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.71
 ---- batch: 020 ----
mean loss: 164.27
 ---- batch: 030 ----
mean loss: 159.39
 ---- batch: 040 ----
mean loss: 161.26
 ---- batch: 050 ----
mean loss: 171.75
 ---- batch: 060 ----
mean loss: 164.26
 ---- batch: 070 ----
mean loss: 165.83
 ---- batch: 080 ----
mean loss: 164.17
 ---- batch: 090 ----
mean loss: 165.22
train mean loss: 164.93
epoch train time: 0:00:00.492513
elapsed time: 0:01:26.991763
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:18:06.548654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.82
 ---- batch: 020 ----
mean loss: 163.76
 ---- batch: 030 ----
mean loss: 162.42
 ---- batch: 040 ----
mean loss: 168.80
 ---- batch: 050 ----
mean loss: 159.16
 ---- batch: 060 ----
mean loss: 165.39
 ---- batch: 070 ----
mean loss: 168.82
 ---- batch: 080 ----
mean loss: 169.25
 ---- batch: 090 ----
mean loss: 164.96
train mean loss: 164.21
epoch train time: 0:00:00.501518
elapsed time: 0:01:27.493413
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:18:07.050330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.87
 ---- batch: 020 ----
mean loss: 162.04
 ---- batch: 030 ----
mean loss: 157.35
 ---- batch: 040 ----
mean loss: 164.58
 ---- batch: 050 ----
mean loss: 161.44
 ---- batch: 060 ----
mean loss: 166.47
 ---- batch: 070 ----
mean loss: 165.42
 ---- batch: 080 ----
mean loss: 165.53
 ---- batch: 090 ----
mean loss: 159.28
train mean loss: 163.12
epoch train time: 0:00:00.495197
elapsed time: 0:01:27.988821
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:18:07.545724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.67
 ---- batch: 020 ----
mean loss: 159.29
 ---- batch: 030 ----
mean loss: 164.90
 ---- batch: 040 ----
mean loss: 160.05
 ---- batch: 050 ----
mean loss: 165.86
 ---- batch: 060 ----
mean loss: 169.43
 ---- batch: 070 ----
mean loss: 163.44
 ---- batch: 080 ----
mean loss: 165.56
 ---- batch: 090 ----
mean loss: 164.56
train mean loss: 163.83
epoch train time: 0:00:00.506890
elapsed time: 0:01:28.495859
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:18:08.052761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.81
 ---- batch: 020 ----
mean loss: 159.17
 ---- batch: 030 ----
mean loss: 163.11
 ---- batch: 040 ----
mean loss: 160.99
 ---- batch: 050 ----
mean loss: 161.78
 ---- batch: 060 ----
mean loss: 167.93
 ---- batch: 070 ----
mean loss: 167.08
 ---- batch: 080 ----
mean loss: 162.92
 ---- batch: 090 ----
mean loss: 165.29
train mean loss: 164.02
epoch train time: 0:00:00.495486
elapsed time: 0:01:28.991502
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:18:08.548415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.99
 ---- batch: 020 ----
mean loss: 156.14
 ---- batch: 030 ----
mean loss: 167.56
 ---- batch: 040 ----
mean loss: 164.64
 ---- batch: 050 ----
mean loss: 160.46
 ---- batch: 060 ----
mean loss: 161.23
 ---- batch: 070 ----
mean loss: 167.40
 ---- batch: 080 ----
mean loss: 161.12
 ---- batch: 090 ----
mean loss: 162.90
train mean loss: 161.62
epoch train time: 0:00:00.499090
elapsed time: 0:01:29.490747
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:18:09.047646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.45
 ---- batch: 020 ----
mean loss: 166.39
 ---- batch: 030 ----
mean loss: 163.30
 ---- batch: 040 ----
mean loss: 157.52
 ---- batch: 050 ----
mean loss: 159.91
 ---- batch: 060 ----
mean loss: 168.27
 ---- batch: 070 ----
mean loss: 165.62
 ---- batch: 080 ----
mean loss: 159.77
 ---- batch: 090 ----
mean loss: 159.99
train mean loss: 162.47
epoch train time: 0:00:00.501713
elapsed time: 0:01:29.992602
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:18:09.549522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.82
 ---- batch: 020 ----
mean loss: 158.34
 ---- batch: 030 ----
mean loss: 159.86
 ---- batch: 040 ----
mean loss: 162.40
 ---- batch: 050 ----
mean loss: 167.85
 ---- batch: 060 ----
mean loss: 164.30
 ---- batch: 070 ----
mean loss: 153.70
 ---- batch: 080 ----
mean loss: 159.92
 ---- batch: 090 ----
mean loss: 162.66
train mean loss: 161.06
epoch train time: 0:00:00.497176
elapsed time: 0:01:30.489945
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:18:10.046847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.85
 ---- batch: 020 ----
mean loss: 154.48
 ---- batch: 030 ----
mean loss: 159.65
 ---- batch: 040 ----
mean loss: 160.25
 ---- batch: 050 ----
mean loss: 161.69
 ---- batch: 060 ----
mean loss: 169.09
 ---- batch: 070 ----
mean loss: 169.02
 ---- batch: 080 ----
mean loss: 152.45
 ---- batch: 090 ----
mean loss: 167.73
train mean loss: 161.56
epoch train time: 0:00:00.495075
elapsed time: 0:01:30.985169
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:18:10.542075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.27
 ---- batch: 020 ----
mean loss: 155.61
 ---- batch: 030 ----
mean loss: 162.44
 ---- batch: 040 ----
mean loss: 151.76
 ---- batch: 050 ----
mean loss: 162.23
 ---- batch: 060 ----
mean loss: 163.33
 ---- batch: 070 ----
mean loss: 157.87
 ---- batch: 080 ----
mean loss: 156.45
 ---- batch: 090 ----
mean loss: 169.55
train mean loss: 159.90
epoch train time: 0:00:00.494934
elapsed time: 0:01:31.480263
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:18:11.037165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.37
 ---- batch: 020 ----
mean loss: 153.08
 ---- batch: 030 ----
mean loss: 157.20
 ---- batch: 040 ----
mean loss: 160.75
 ---- batch: 050 ----
mean loss: 161.62
 ---- batch: 060 ----
mean loss: 163.99
 ---- batch: 070 ----
mean loss: 159.05
 ---- batch: 080 ----
mean loss: 165.34
 ---- batch: 090 ----
mean loss: 167.82
train mean loss: 159.89
epoch train time: 0:00:00.497971
elapsed time: 0:01:31.978394
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:18:11.535296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.18
 ---- batch: 020 ----
mean loss: 148.81
 ---- batch: 030 ----
mean loss: 157.92
 ---- batch: 040 ----
mean loss: 157.79
 ---- batch: 050 ----
mean loss: 158.28
 ---- batch: 060 ----
mean loss: 154.38
 ---- batch: 070 ----
mean loss: 160.77
 ---- batch: 080 ----
mean loss: 166.96
 ---- batch: 090 ----
mean loss: 164.84
train mean loss: 158.51
epoch train time: 0:00:00.503213
elapsed time: 0:01:32.481753
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:18:12.038656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.07
 ---- batch: 020 ----
mean loss: 154.52
 ---- batch: 030 ----
mean loss: 160.64
 ---- batch: 040 ----
mean loss: 164.20
 ---- batch: 050 ----
mean loss: 163.55
 ---- batch: 060 ----
mean loss: 158.44
 ---- batch: 070 ----
mean loss: 157.79
 ---- batch: 080 ----
mean loss: 159.55
 ---- batch: 090 ----
mean loss: 151.71
train mean loss: 158.63
epoch train time: 0:00:00.496130
elapsed time: 0:01:32.978048
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:18:12.534957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.16
 ---- batch: 020 ----
mean loss: 156.01
 ---- batch: 030 ----
mean loss: 151.24
 ---- batch: 040 ----
mean loss: 156.53
 ---- batch: 050 ----
mean loss: 157.59
 ---- batch: 060 ----
mean loss: 160.60
 ---- batch: 070 ----
mean loss: 158.49
 ---- batch: 080 ----
mean loss: 163.47
 ---- batch: 090 ----
mean loss: 160.54
train mean loss: 158.36
epoch train time: 0:00:00.497673
elapsed time: 0:01:33.475903
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:18:13.032807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.34
 ---- batch: 020 ----
mean loss: 158.71
 ---- batch: 030 ----
mean loss: 150.23
 ---- batch: 040 ----
mean loss: 158.34
 ---- batch: 050 ----
mean loss: 162.14
 ---- batch: 060 ----
mean loss: 160.17
 ---- batch: 070 ----
mean loss: 151.01
 ---- batch: 080 ----
mean loss: 164.26
 ---- batch: 090 ----
mean loss: 158.64
train mean loss: 157.10
epoch train time: 0:00:00.499226
elapsed time: 0:01:33.975277
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:18:13.532196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.01
 ---- batch: 020 ----
mean loss: 151.36
 ---- batch: 030 ----
mean loss: 153.37
 ---- batch: 040 ----
mean loss: 156.99
 ---- batch: 050 ----
mean loss: 158.41
 ---- batch: 060 ----
mean loss: 154.40
 ---- batch: 070 ----
mean loss: 160.94
 ---- batch: 080 ----
mean loss: 165.15
 ---- batch: 090 ----
mean loss: 158.65
train mean loss: 156.79
epoch train time: 0:00:00.512417
elapsed time: 0:01:34.487858
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:18:14.044761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.17
 ---- batch: 020 ----
mean loss: 157.89
 ---- batch: 030 ----
mean loss: 154.50
 ---- batch: 040 ----
mean loss: 157.86
 ---- batch: 050 ----
mean loss: 159.05
 ---- batch: 060 ----
mean loss: 157.78
 ---- batch: 070 ----
mean loss: 154.88
 ---- batch: 080 ----
mean loss: 154.55
 ---- batch: 090 ----
mean loss: 155.90
train mean loss: 156.68
epoch train time: 0:00:00.506267
elapsed time: 0:01:34.994287
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:18:14.551188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.67
 ---- batch: 020 ----
mean loss: 148.60
 ---- batch: 030 ----
mean loss: 149.12
 ---- batch: 040 ----
mean loss: 153.31
 ---- batch: 050 ----
mean loss: 158.75
 ---- batch: 060 ----
mean loss: 154.63
 ---- batch: 070 ----
mean loss: 160.26
 ---- batch: 080 ----
mean loss: 158.47
 ---- batch: 090 ----
mean loss: 160.25
train mean loss: 155.03
epoch train time: 0:00:00.503511
elapsed time: 0:01:35.497945
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:18:15.054848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.83
 ---- batch: 020 ----
mean loss: 149.20
 ---- batch: 030 ----
mean loss: 152.28
 ---- batch: 040 ----
mean loss: 155.62
 ---- batch: 050 ----
mean loss: 154.61
 ---- batch: 060 ----
mean loss: 161.42
 ---- batch: 070 ----
mean loss: 155.70
 ---- batch: 080 ----
mean loss: 157.10
 ---- batch: 090 ----
mean loss: 159.35
train mean loss: 155.55
epoch train time: 0:00:00.496968
elapsed time: 0:01:35.995057
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:18:15.551969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.84
 ---- batch: 020 ----
mean loss: 153.99
 ---- batch: 030 ----
mean loss: 150.95
 ---- batch: 040 ----
mean loss: 152.76
 ---- batch: 050 ----
mean loss: 153.12
 ---- batch: 060 ----
mean loss: 155.76
 ---- batch: 070 ----
mean loss: 156.67
 ---- batch: 080 ----
mean loss: 159.44
 ---- batch: 090 ----
mean loss: 161.31
train mean loss: 154.44
epoch train time: 0:00:00.499628
elapsed time: 0:01:36.494842
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:18:16.051746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.41
 ---- batch: 020 ----
mean loss: 148.58
 ---- batch: 030 ----
mean loss: 152.59
 ---- batch: 040 ----
mean loss: 155.87
 ---- batch: 050 ----
mean loss: 153.60
 ---- batch: 060 ----
mean loss: 158.83
 ---- batch: 070 ----
mean loss: 157.22
 ---- batch: 080 ----
mean loss: 161.15
 ---- batch: 090 ----
mean loss: 165.65
train mean loss: 155.11
epoch train time: 0:00:00.496823
elapsed time: 0:01:36.991825
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:18:16.548715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.35
 ---- batch: 020 ----
mean loss: 152.14
 ---- batch: 030 ----
mean loss: 149.41
 ---- batch: 040 ----
mean loss: 156.59
 ---- batch: 050 ----
mean loss: 159.14
 ---- batch: 060 ----
mean loss: 156.05
 ---- batch: 070 ----
mean loss: 155.39
 ---- batch: 080 ----
mean loss: 153.80
 ---- batch: 090 ----
mean loss: 165.22
train mean loss: 155.35
epoch train time: 0:00:00.498047
elapsed time: 0:01:37.490005
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:18:17.046909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.49
 ---- batch: 020 ----
mean loss: 145.47
 ---- batch: 030 ----
mean loss: 149.13
 ---- batch: 040 ----
mean loss: 151.65
 ---- batch: 050 ----
mean loss: 154.88
 ---- batch: 060 ----
mean loss: 149.42
 ---- batch: 070 ----
mean loss: 159.91
 ---- batch: 080 ----
mean loss: 161.69
 ---- batch: 090 ----
mean loss: 160.52
train mean loss: 153.22
epoch train time: 0:00:00.490272
elapsed time: 0:01:37.980421
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:18:17.537342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.86
 ---- batch: 020 ----
mean loss: 149.39
 ---- batch: 030 ----
mean loss: 150.91
 ---- batch: 040 ----
mean loss: 157.18
 ---- batch: 050 ----
mean loss: 149.87
 ---- batch: 060 ----
mean loss: 145.24
 ---- batch: 070 ----
mean loss: 157.03
 ---- batch: 080 ----
mean loss: 161.51
 ---- batch: 090 ----
mean loss: 155.16
train mean loss: 153.27
epoch train time: 0:00:00.501769
elapsed time: 0:01:38.482360
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:18:18.039266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.56
 ---- batch: 020 ----
mean loss: 153.23
 ---- batch: 030 ----
mean loss: 150.24
 ---- batch: 040 ----
mean loss: 150.18
 ---- batch: 050 ----
mean loss: 152.72
 ---- batch: 060 ----
mean loss: 152.05
 ---- batch: 070 ----
mean loss: 152.59
 ---- batch: 080 ----
mean loss: 155.23
 ---- batch: 090 ----
mean loss: 160.71
train mean loss: 152.86
epoch train time: 0:00:00.504834
elapsed time: 0:01:38.987344
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:18:18.544249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.46
 ---- batch: 020 ----
mean loss: 148.57
 ---- batch: 030 ----
mean loss: 158.09
 ---- batch: 040 ----
mean loss: 151.21
 ---- batch: 050 ----
mean loss: 150.71
 ---- batch: 060 ----
mean loss: 152.61
 ---- batch: 070 ----
mean loss: 151.31
 ---- batch: 080 ----
mean loss: 165.31
 ---- batch: 090 ----
mean loss: 153.37
train mean loss: 153.76
epoch train time: 0:00:00.505604
elapsed time: 0:01:39.493099
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:18:19.050003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.89
 ---- batch: 020 ----
mean loss: 148.54
 ---- batch: 030 ----
mean loss: 151.38
 ---- batch: 040 ----
mean loss: 146.90
 ---- batch: 050 ----
mean loss: 144.35
 ---- batch: 060 ----
mean loss: 156.44
 ---- batch: 070 ----
mean loss: 155.58
 ---- batch: 080 ----
mean loss: 156.02
 ---- batch: 090 ----
mean loss: 153.46
train mean loss: 151.88
epoch train time: 0:00:00.503785
elapsed time: 0:01:39.997035
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:18:19.553969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.40
 ---- batch: 020 ----
mean loss: 141.62
 ---- batch: 030 ----
mean loss: 148.22
 ---- batch: 040 ----
mean loss: 151.76
 ---- batch: 050 ----
mean loss: 149.97
 ---- batch: 060 ----
mean loss: 146.67
 ---- batch: 070 ----
mean loss: 155.67
 ---- batch: 080 ----
mean loss: 162.93
 ---- batch: 090 ----
mean loss: 159.65
train mean loss: 152.56
epoch train time: 0:00:00.507097
elapsed time: 0:01:40.504322
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:18:20.061222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.44
 ---- batch: 020 ----
mean loss: 149.56
 ---- batch: 030 ----
mean loss: 149.25
 ---- batch: 040 ----
mean loss: 146.52
 ---- batch: 050 ----
mean loss: 145.03
 ---- batch: 060 ----
mean loss: 159.75
 ---- batch: 070 ----
mean loss: 155.11
 ---- batch: 080 ----
mean loss: 149.75
 ---- batch: 090 ----
mean loss: 147.15
train mean loss: 150.77
epoch train time: 0:00:00.503598
elapsed time: 0:01:41.008069
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:18:20.564973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.54
 ---- batch: 020 ----
mean loss: 144.27
 ---- batch: 030 ----
mean loss: 148.32
 ---- batch: 040 ----
mean loss: 147.86
 ---- batch: 050 ----
mean loss: 148.42
 ---- batch: 060 ----
mean loss: 150.06
 ---- batch: 070 ----
mean loss: 148.31
 ---- batch: 080 ----
mean loss: 156.62
 ---- batch: 090 ----
mean loss: 155.39
train mean loss: 150.53
epoch train time: 0:00:00.500592
elapsed time: 0:01:41.508826
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:18:21.065748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.19
 ---- batch: 020 ----
mean loss: 147.80
 ---- batch: 030 ----
mean loss: 146.96
 ---- batch: 040 ----
mean loss: 151.86
 ---- batch: 050 ----
mean loss: 151.91
 ---- batch: 060 ----
mean loss: 152.38
 ---- batch: 070 ----
mean loss: 152.19
 ---- batch: 080 ----
mean loss: 151.63
 ---- batch: 090 ----
mean loss: 152.30
train mean loss: 150.84
epoch train time: 0:00:00.496694
elapsed time: 0:01:42.005695
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:18:21.562598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.20
 ---- batch: 020 ----
mean loss: 150.96
 ---- batch: 030 ----
mean loss: 147.36
 ---- batch: 040 ----
mean loss: 156.04
 ---- batch: 050 ----
mean loss: 147.76
 ---- batch: 060 ----
mean loss: 154.42
 ---- batch: 070 ----
mean loss: 152.20
 ---- batch: 080 ----
mean loss: 150.21
 ---- batch: 090 ----
mean loss: 151.92
train mean loss: 150.96
epoch train time: 0:00:00.506882
elapsed time: 0:01:42.512775
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:18:22.069672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.28
 ---- batch: 020 ----
mean loss: 148.48
 ---- batch: 030 ----
mean loss: 152.68
 ---- batch: 040 ----
mean loss: 149.09
 ---- batch: 050 ----
mean loss: 152.83
 ---- batch: 060 ----
mean loss: 149.94
 ---- batch: 070 ----
mean loss: 147.49
 ---- batch: 080 ----
mean loss: 150.84
 ---- batch: 090 ----
mean loss: 158.68
train mean loss: 150.83
epoch train time: 0:00:00.502781
elapsed time: 0:01:43.015727
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:18:22.572620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.54
 ---- batch: 020 ----
mean loss: 143.43
 ---- batch: 030 ----
mean loss: 151.99
 ---- batch: 040 ----
mean loss: 150.34
 ---- batch: 050 ----
mean loss: 147.56
 ---- batch: 060 ----
mean loss: 153.38
 ---- batch: 070 ----
mean loss: 150.73
 ---- batch: 080 ----
mean loss: 160.30
 ---- batch: 090 ----
mean loss: 153.62
train mean loss: 150.13
epoch train time: 0:00:00.501132
elapsed time: 0:01:43.517005
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:18:23.073898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.62
 ---- batch: 020 ----
mean loss: 153.38
 ---- batch: 030 ----
mean loss: 147.08
 ---- batch: 040 ----
mean loss: 150.04
 ---- batch: 050 ----
mean loss: 150.60
 ---- batch: 060 ----
mean loss: 144.98
 ---- batch: 070 ----
mean loss: 149.73
 ---- batch: 080 ----
mean loss: 149.80
 ---- batch: 090 ----
mean loss: 152.29
train mean loss: 149.50
epoch train time: 0:00:00.504289
elapsed time: 0:01:44.021432
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:18:23.578335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.94
 ---- batch: 020 ----
mean loss: 144.82
 ---- batch: 030 ----
mean loss: 155.40
 ---- batch: 040 ----
mean loss: 143.16
 ---- batch: 050 ----
mean loss: 152.44
 ---- batch: 060 ----
mean loss: 145.03
 ---- batch: 070 ----
mean loss: 154.94
 ---- batch: 080 ----
mean loss: 146.73
 ---- batch: 090 ----
mean loss: 155.14
train mean loss: 149.23
epoch train time: 0:00:00.509390
elapsed time: 0:01:44.530969
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:18:24.087872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.88
 ---- batch: 020 ----
mean loss: 144.75
 ---- batch: 030 ----
mean loss: 146.97
 ---- batch: 040 ----
mean loss: 147.90
 ---- batch: 050 ----
mean loss: 153.78
 ---- batch: 060 ----
mean loss: 153.97
 ---- batch: 070 ----
mean loss: 153.09
 ---- batch: 080 ----
mean loss: 150.50
 ---- batch: 090 ----
mean loss: 148.33
train mean loss: 149.71
epoch train time: 0:00:00.503470
elapsed time: 0:01:45.034602
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:18:24.591497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.35
 ---- batch: 020 ----
mean loss: 147.18
 ---- batch: 030 ----
mean loss: 142.82
 ---- batch: 040 ----
mean loss: 151.30
 ---- batch: 050 ----
mean loss: 144.21
 ---- batch: 060 ----
mean loss: 142.55
 ---- batch: 070 ----
mean loss: 151.82
 ---- batch: 080 ----
mean loss: 145.57
 ---- batch: 090 ----
mean loss: 150.28
train mean loss: 148.20
epoch train time: 0:00:00.495784
elapsed time: 0:01:45.530523
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:18:25.087442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.39
 ---- batch: 020 ----
mean loss: 151.58
 ---- batch: 030 ----
mean loss: 145.50
 ---- batch: 040 ----
mean loss: 150.82
 ---- batch: 050 ----
mean loss: 145.23
 ---- batch: 060 ----
mean loss: 152.57
 ---- batch: 070 ----
mean loss: 150.66
 ---- batch: 080 ----
mean loss: 146.94
 ---- batch: 090 ----
mean loss: 145.59
train mean loss: 147.76
epoch train time: 0:00:00.493374
elapsed time: 0:01:46.024058
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:18:25.580962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.11
 ---- batch: 020 ----
mean loss: 147.39
 ---- batch: 030 ----
mean loss: 148.25
 ---- batch: 040 ----
mean loss: 148.30
 ---- batch: 050 ----
mean loss: 154.26
 ---- batch: 060 ----
mean loss: 147.25
 ---- batch: 070 ----
mean loss: 148.69
 ---- batch: 080 ----
mean loss: 159.64
 ---- batch: 090 ----
mean loss: 150.70
train mean loss: 148.72
epoch train time: 0:00:00.507168
elapsed time: 0:01:46.531374
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:18:26.088277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.15
 ---- batch: 020 ----
mean loss: 143.81
 ---- batch: 030 ----
mean loss: 145.15
 ---- batch: 040 ----
mean loss: 146.52
 ---- batch: 050 ----
mean loss: 152.64
 ---- batch: 060 ----
mean loss: 152.03
 ---- batch: 070 ----
mean loss: 145.01
 ---- batch: 080 ----
mean loss: 148.72
 ---- batch: 090 ----
mean loss: 147.73
train mean loss: 148.06
epoch train time: 0:00:00.506745
elapsed time: 0:01:47.038334
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:18:26.595240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.81
 ---- batch: 020 ----
mean loss: 148.74
 ---- batch: 030 ----
mean loss: 144.01
 ---- batch: 040 ----
mean loss: 146.51
 ---- batch: 050 ----
mean loss: 145.80
 ---- batch: 060 ----
mean loss: 150.15
 ---- batch: 070 ----
mean loss: 141.88
 ---- batch: 080 ----
mean loss: 154.16
 ---- batch: 090 ----
mean loss: 148.66
train mean loss: 147.22
epoch train time: 0:00:00.505480
elapsed time: 0:01:47.543991
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:18:27.100887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.88
 ---- batch: 020 ----
mean loss: 140.40
 ---- batch: 030 ----
mean loss: 145.93
 ---- batch: 040 ----
mean loss: 148.87
 ---- batch: 050 ----
mean loss: 144.07
 ---- batch: 060 ----
mean loss: 138.90
 ---- batch: 070 ----
mean loss: 151.22
 ---- batch: 080 ----
mean loss: 149.56
 ---- batch: 090 ----
mean loss: 146.88
train mean loss: 146.04
epoch train time: 0:00:00.500368
elapsed time: 0:01:48.044516
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:18:27.601416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.50
 ---- batch: 020 ----
mean loss: 141.99
 ---- batch: 030 ----
mean loss: 144.72
 ---- batch: 040 ----
mean loss: 148.17
 ---- batch: 050 ----
mean loss: 148.17
 ---- batch: 060 ----
mean loss: 147.65
 ---- batch: 070 ----
mean loss: 147.63
 ---- batch: 080 ----
mean loss: 144.88
 ---- batch: 090 ----
mean loss: 151.96
train mean loss: 147.14
epoch train time: 0:00:00.505301
elapsed time: 0:01:48.549972
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:18:28.106869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.40
 ---- batch: 020 ----
mean loss: 153.45
 ---- batch: 030 ----
mean loss: 143.49
 ---- batch: 040 ----
mean loss: 151.34
 ---- batch: 050 ----
mean loss: 142.11
 ---- batch: 060 ----
mean loss: 145.72
 ---- batch: 070 ----
mean loss: 138.70
 ---- batch: 080 ----
mean loss: 152.50
 ---- batch: 090 ----
mean loss: 146.05
train mean loss: 146.80
epoch train time: 0:00:00.501282
elapsed time: 0:01:49.051402
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:18:28.608314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.31
 ---- batch: 020 ----
mean loss: 144.49
 ---- batch: 030 ----
mean loss: 142.60
 ---- batch: 040 ----
mean loss: 147.14
 ---- batch: 050 ----
mean loss: 145.17
 ---- batch: 060 ----
mean loss: 143.13
 ---- batch: 070 ----
mean loss: 142.02
 ---- batch: 080 ----
mean loss: 149.08
 ---- batch: 090 ----
mean loss: 153.36
train mean loss: 145.95
epoch train time: 0:00:00.500289
elapsed time: 0:01:49.551874
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:18:29.108769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.37
 ---- batch: 020 ----
mean loss: 144.02
 ---- batch: 030 ----
mean loss: 152.53
 ---- batch: 040 ----
mean loss: 141.46
 ---- batch: 050 ----
mean loss: 145.31
 ---- batch: 060 ----
mean loss: 140.99
 ---- batch: 070 ----
mean loss: 146.62
 ---- batch: 080 ----
mean loss: 148.03
 ---- batch: 090 ----
mean loss: 151.01
train mean loss: 145.37
epoch train time: 0:00:00.499449
elapsed time: 0:01:50.051497
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:18:29.608389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.46
 ---- batch: 020 ----
mean loss: 138.77
 ---- batch: 030 ----
mean loss: 147.24
 ---- batch: 040 ----
mean loss: 148.96
 ---- batch: 050 ----
mean loss: 144.26
 ---- batch: 060 ----
mean loss: 143.82
 ---- batch: 070 ----
mean loss: 145.25
 ---- batch: 080 ----
mean loss: 143.54
 ---- batch: 090 ----
mean loss: 148.38
train mean loss: 144.98
epoch train time: 0:00:00.498023
elapsed time: 0:01:50.549665
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:18:30.106584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.42
 ---- batch: 020 ----
mean loss: 143.12
 ---- batch: 030 ----
mean loss: 139.75
 ---- batch: 040 ----
mean loss: 143.57
 ---- batch: 050 ----
mean loss: 149.54
 ---- batch: 060 ----
mean loss: 148.62
 ---- batch: 070 ----
mean loss: 149.86
 ---- batch: 080 ----
mean loss: 150.17
 ---- batch: 090 ----
mean loss: 149.45
train mean loss: 146.29
epoch train time: 0:00:00.498860
elapsed time: 0:01:51.048716
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:18:30.605674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.09
 ---- batch: 020 ----
mean loss: 146.09
 ---- batch: 030 ----
mean loss: 146.77
 ---- batch: 040 ----
mean loss: 143.11
 ---- batch: 050 ----
mean loss: 141.72
 ---- batch: 060 ----
mean loss: 148.67
 ---- batch: 070 ----
mean loss: 143.06
 ---- batch: 080 ----
mean loss: 143.17
 ---- batch: 090 ----
mean loss: 145.53
train mean loss: 144.94
epoch train time: 0:00:00.503743
elapsed time: 0:01:51.552670
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:18:31.109574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.11
 ---- batch: 020 ----
mean loss: 141.00
 ---- batch: 030 ----
mean loss: 146.19
 ---- batch: 040 ----
mean loss: 144.58
 ---- batch: 050 ----
mean loss: 139.76
 ---- batch: 060 ----
mean loss: 148.20
 ---- batch: 070 ----
mean loss: 146.91
 ---- batch: 080 ----
mean loss: 152.78
 ---- batch: 090 ----
mean loss: 143.05
train mean loss: 144.35
epoch train time: 0:00:00.505032
elapsed time: 0:01:52.057859
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:18:31.614753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.82
 ---- batch: 020 ----
mean loss: 145.89
 ---- batch: 030 ----
mean loss: 144.65
 ---- batch: 040 ----
mean loss: 143.93
 ---- batch: 050 ----
mean loss: 152.70
 ---- batch: 060 ----
mean loss: 142.89
 ---- batch: 070 ----
mean loss: 138.02
 ---- batch: 080 ----
mean loss: 143.49
 ---- batch: 090 ----
mean loss: 145.75
train mean loss: 144.83
epoch train time: 0:00:00.506491
elapsed time: 0:01:52.564490
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:18:32.121393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.59
 ---- batch: 020 ----
mean loss: 138.36
 ---- batch: 030 ----
mean loss: 145.59
 ---- batch: 040 ----
mean loss: 143.59
 ---- batch: 050 ----
mean loss: 138.73
 ---- batch: 060 ----
mean loss: 134.98
 ---- batch: 070 ----
mean loss: 147.93
 ---- batch: 080 ----
mean loss: 149.87
 ---- batch: 090 ----
mean loss: 145.56
train mean loss: 143.70
epoch train time: 0:00:00.505913
elapsed time: 0:01:53.070551
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:18:32.627480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.89
 ---- batch: 020 ----
mean loss: 137.62
 ---- batch: 030 ----
mean loss: 140.64
 ---- batch: 040 ----
mean loss: 147.76
 ---- batch: 050 ----
mean loss: 150.06
 ---- batch: 060 ----
mean loss: 145.92
 ---- batch: 070 ----
mean loss: 147.92
 ---- batch: 080 ----
mean loss: 144.13
 ---- batch: 090 ----
mean loss: 144.54
train mean loss: 143.23
epoch train time: 0:00:00.515401
elapsed time: 0:01:53.586136
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:18:33.143031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.98
 ---- batch: 020 ----
mean loss: 138.75
 ---- batch: 030 ----
mean loss: 142.26
 ---- batch: 040 ----
mean loss: 141.87
 ---- batch: 050 ----
mean loss: 142.04
 ---- batch: 060 ----
mean loss: 147.61
 ---- batch: 070 ----
mean loss: 145.05
 ---- batch: 080 ----
mean loss: 149.45
 ---- batch: 090 ----
mean loss: 142.06
train mean loss: 142.65
epoch train time: 0:00:00.509152
elapsed time: 0:01:54.095436
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:18:33.652334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.54
 ---- batch: 020 ----
mean loss: 137.80
 ---- batch: 030 ----
mean loss: 140.83
 ---- batch: 040 ----
mean loss: 144.96
 ---- batch: 050 ----
mean loss: 137.51
 ---- batch: 060 ----
mean loss: 144.74
 ---- batch: 070 ----
mean loss: 148.93
 ---- batch: 080 ----
mean loss: 146.72
 ---- batch: 090 ----
mean loss: 142.88
train mean loss: 142.94
epoch train time: 0:00:00.508956
elapsed time: 0:01:54.604534
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:18:34.161438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.42
 ---- batch: 020 ----
mean loss: 146.55
 ---- batch: 030 ----
mean loss: 151.18
 ---- batch: 040 ----
mean loss: 150.16
 ---- batch: 050 ----
mean loss: 138.99
 ---- batch: 060 ----
mean loss: 142.66
 ---- batch: 070 ----
mean loss: 143.54
 ---- batch: 080 ----
mean loss: 139.61
 ---- batch: 090 ----
mean loss: 139.85
train mean loss: 142.97
epoch train time: 0:00:00.498241
elapsed time: 0:01:55.102933
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:18:34.659860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.31
 ---- batch: 020 ----
mean loss: 139.02
 ---- batch: 030 ----
mean loss: 138.65
 ---- batch: 040 ----
mean loss: 144.16
 ---- batch: 050 ----
mean loss: 143.74
 ---- batch: 060 ----
mean loss: 147.29
 ---- batch: 070 ----
mean loss: 144.37
 ---- batch: 080 ----
mean loss: 141.04
 ---- batch: 090 ----
mean loss: 145.50
train mean loss: 143.15
epoch train time: 0:00:00.500290
elapsed time: 0:01:55.603401
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:18:35.160295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.64
 ---- batch: 020 ----
mean loss: 137.15
 ---- batch: 030 ----
mean loss: 137.65
 ---- batch: 040 ----
mean loss: 138.35
 ---- batch: 050 ----
mean loss: 147.05
 ---- batch: 060 ----
mean loss: 140.42
 ---- batch: 070 ----
mean loss: 151.39
 ---- batch: 080 ----
mean loss: 137.34
 ---- batch: 090 ----
mean loss: 144.10
train mean loss: 142.55
epoch train time: 0:00:00.501389
elapsed time: 0:01:56.104930
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:18:35.661850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.59
 ---- batch: 020 ----
mean loss: 141.36
 ---- batch: 030 ----
mean loss: 140.88
 ---- batch: 040 ----
mean loss: 147.10
 ---- batch: 050 ----
mean loss: 144.01
 ---- batch: 060 ----
mean loss: 147.29
 ---- batch: 070 ----
mean loss: 142.23
 ---- batch: 080 ----
mean loss: 139.56
 ---- batch: 090 ----
mean loss: 140.85
train mean loss: 142.28
epoch train time: 0:00:00.500347
elapsed time: 0:01:56.605453
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:18:36.162364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.83
 ---- batch: 020 ----
mean loss: 143.90
 ---- batch: 030 ----
mean loss: 137.27
 ---- batch: 040 ----
mean loss: 142.13
 ---- batch: 050 ----
mean loss: 140.50
 ---- batch: 060 ----
mean loss: 136.38
 ---- batch: 070 ----
mean loss: 137.49
 ---- batch: 080 ----
mean loss: 143.00
 ---- batch: 090 ----
mean loss: 146.83
train mean loss: 141.39
epoch train time: 0:00:00.501233
elapsed time: 0:01:57.106841
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:18:36.663746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.46
 ---- batch: 020 ----
mean loss: 139.85
 ---- batch: 030 ----
mean loss: 146.44
 ---- batch: 040 ----
mean loss: 133.57
 ---- batch: 050 ----
mean loss: 142.85
 ---- batch: 060 ----
mean loss: 137.61
 ---- batch: 070 ----
mean loss: 145.47
 ---- batch: 080 ----
mean loss: 139.70
 ---- batch: 090 ----
mean loss: 140.36
train mean loss: 141.21
epoch train time: 0:00:00.501272
elapsed time: 0:01:57.608275
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:18:37.165177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.68
 ---- batch: 020 ----
mean loss: 142.98
 ---- batch: 030 ----
mean loss: 133.79
 ---- batch: 040 ----
mean loss: 138.02
 ---- batch: 050 ----
mean loss: 141.10
 ---- batch: 060 ----
mean loss: 148.59
 ---- batch: 070 ----
mean loss: 141.06
 ---- batch: 080 ----
mean loss: 144.46
 ---- batch: 090 ----
mean loss: 140.03
train mean loss: 141.02
epoch train time: 0:00:00.500326
elapsed time: 0:01:58.108772
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:18:37.665679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.38
 ---- batch: 020 ----
mean loss: 133.73
 ---- batch: 030 ----
mean loss: 137.65
 ---- batch: 040 ----
mean loss: 148.04
 ---- batch: 050 ----
mean loss: 137.08
 ---- batch: 060 ----
mean loss: 144.01
 ---- batch: 070 ----
mean loss: 144.61
 ---- batch: 080 ----
mean loss: 145.50
 ---- batch: 090 ----
mean loss: 141.42
train mean loss: 140.86
epoch train time: 0:00:00.514287
elapsed time: 0:01:58.623212
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:18:38.180116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.03
 ---- batch: 020 ----
mean loss: 138.31
 ---- batch: 030 ----
mean loss: 136.15
 ---- batch: 040 ----
mean loss: 139.97
 ---- batch: 050 ----
mean loss: 145.76
 ---- batch: 060 ----
mean loss: 141.44
 ---- batch: 070 ----
mean loss: 145.01
 ---- batch: 080 ----
mean loss: 137.43
 ---- batch: 090 ----
mean loss: 140.25
train mean loss: 140.56
epoch train time: 0:00:00.506570
elapsed time: 0:01:59.129939
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:18:38.686837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.86
 ---- batch: 020 ----
mean loss: 137.55
 ---- batch: 030 ----
mean loss: 137.24
 ---- batch: 040 ----
mean loss: 136.89
 ---- batch: 050 ----
mean loss: 141.05
 ---- batch: 060 ----
mean loss: 142.29
 ---- batch: 070 ----
mean loss: 143.56
 ---- batch: 080 ----
mean loss: 139.76
 ---- batch: 090 ----
mean loss: 146.55
train mean loss: 140.25
epoch train time: 0:00:00.504856
elapsed time: 0:01:59.634944
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:18:39.191840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.73
 ---- batch: 020 ----
mean loss: 135.37
 ---- batch: 030 ----
mean loss: 142.34
 ---- batch: 040 ----
mean loss: 141.96
 ---- batch: 050 ----
mean loss: 140.54
 ---- batch: 060 ----
mean loss: 133.44
 ---- batch: 070 ----
mean loss: 140.22
 ---- batch: 080 ----
mean loss: 144.32
 ---- batch: 090 ----
mean loss: 142.07
train mean loss: 140.50
epoch train time: 0:00:00.498814
elapsed time: 0:02:00.133912
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:18:39.690825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.83
 ---- batch: 020 ----
mean loss: 136.16
 ---- batch: 030 ----
mean loss: 145.83
 ---- batch: 040 ----
mean loss: 135.80
 ---- batch: 050 ----
mean loss: 137.75
 ---- batch: 060 ----
mean loss: 141.35
 ---- batch: 070 ----
mean loss: 140.55
 ---- batch: 080 ----
mean loss: 140.60
 ---- batch: 090 ----
mean loss: 143.77
train mean loss: 139.58
epoch train time: 0:00:00.500854
elapsed time: 0:02:00.634941
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:18:40.191849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.45
 ---- batch: 020 ----
mean loss: 143.89
 ---- batch: 030 ----
mean loss: 140.63
 ---- batch: 040 ----
mean loss: 134.78
 ---- batch: 050 ----
mean loss: 140.98
 ---- batch: 060 ----
mean loss: 141.60
 ---- batch: 070 ----
mean loss: 138.56
 ---- batch: 080 ----
mean loss: 139.44
 ---- batch: 090 ----
mean loss: 136.07
train mean loss: 139.77
epoch train time: 0:00:00.508278
elapsed time: 0:02:01.143387
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:18:40.700299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.73
 ---- batch: 020 ----
mean loss: 130.27
 ---- batch: 030 ----
mean loss: 137.66
 ---- batch: 040 ----
mean loss: 138.07
 ---- batch: 050 ----
mean loss: 137.10
 ---- batch: 060 ----
mean loss: 144.78
 ---- batch: 070 ----
mean loss: 143.09
 ---- batch: 080 ----
mean loss: 138.81
 ---- batch: 090 ----
mean loss: 137.04
train mean loss: 138.48
epoch train time: 0:00:00.510236
elapsed time: 0:02:01.653794
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:18:41.210688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.79
 ---- batch: 020 ----
mean loss: 136.05
 ---- batch: 030 ----
mean loss: 132.06
 ---- batch: 040 ----
mean loss: 145.13
 ---- batch: 050 ----
mean loss: 141.57
 ---- batch: 060 ----
mean loss: 140.53
 ---- batch: 070 ----
mean loss: 137.49
 ---- batch: 080 ----
mean loss: 134.79
 ---- batch: 090 ----
mean loss: 144.81
train mean loss: 138.97
epoch train time: 0:00:00.501827
elapsed time: 0:02:02.155769
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:18:41.712664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.73
 ---- batch: 020 ----
mean loss: 131.98
 ---- batch: 030 ----
mean loss: 141.24
 ---- batch: 040 ----
mean loss: 131.51
 ---- batch: 050 ----
mean loss: 134.18
 ---- batch: 060 ----
mean loss: 140.48
 ---- batch: 070 ----
mean loss: 149.94
 ---- batch: 080 ----
mean loss: 140.23
 ---- batch: 090 ----
mean loss: 137.03
train mean loss: 138.59
epoch train time: 0:00:00.505431
elapsed time: 0:02:02.661367
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:18:42.218269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.55
 ---- batch: 020 ----
mean loss: 137.46
 ---- batch: 030 ----
mean loss: 137.23
 ---- batch: 040 ----
mean loss: 139.63
 ---- batch: 050 ----
mean loss: 142.23
 ---- batch: 060 ----
mean loss: 140.51
 ---- batch: 070 ----
mean loss: 135.17
 ---- batch: 080 ----
mean loss: 134.27
 ---- batch: 090 ----
mean loss: 136.75
train mean loss: 138.46
epoch train time: 0:00:00.506133
elapsed time: 0:02:03.167644
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:18:42.724546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.98
 ---- batch: 020 ----
mean loss: 134.43
 ---- batch: 030 ----
mean loss: 137.81
 ---- batch: 040 ----
mean loss: 136.63
 ---- batch: 050 ----
mean loss: 136.64
 ---- batch: 060 ----
mean loss: 137.85
 ---- batch: 070 ----
mean loss: 136.97
 ---- batch: 080 ----
mean loss: 138.16
 ---- batch: 090 ----
mean loss: 146.92
train mean loss: 137.95
epoch train time: 0:00:00.506280
elapsed time: 0:02:03.674078
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:18:43.230974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.89
 ---- batch: 020 ----
mean loss: 135.46
 ---- batch: 030 ----
mean loss: 137.83
 ---- batch: 040 ----
mean loss: 136.56
 ---- batch: 050 ----
mean loss: 137.72
 ---- batch: 060 ----
mean loss: 132.61
 ---- batch: 070 ----
mean loss: 141.22
 ---- batch: 080 ----
mean loss: 135.22
 ---- batch: 090 ----
mean loss: 140.18
train mean loss: 137.65
epoch train time: 0:00:00.501323
elapsed time: 0:02:04.175553
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:18:43.732466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.67
 ---- batch: 020 ----
mean loss: 134.59
 ---- batch: 030 ----
mean loss: 136.42
 ---- batch: 040 ----
mean loss: 138.78
 ---- batch: 050 ----
mean loss: 136.63
 ---- batch: 060 ----
mean loss: 146.91
 ---- batch: 070 ----
mean loss: 141.65
 ---- batch: 080 ----
mean loss: 139.80
 ---- batch: 090 ----
mean loss: 136.27
train mean loss: 137.97
epoch train time: 0:00:00.505872
elapsed time: 0:02:04.681604
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:18:44.238507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.52
 ---- batch: 020 ----
mean loss: 138.79
 ---- batch: 030 ----
mean loss: 136.98
 ---- batch: 040 ----
mean loss: 138.03
 ---- batch: 050 ----
mean loss: 140.79
 ---- batch: 060 ----
mean loss: 138.75
 ---- batch: 070 ----
mean loss: 135.21
 ---- batch: 080 ----
mean loss: 142.71
 ---- batch: 090 ----
mean loss: 133.13
train mean loss: 138.09
epoch train time: 0:00:00.500812
elapsed time: 0:02:05.182570
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:18:44.739474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.09
 ---- batch: 020 ----
mean loss: 136.72
 ---- batch: 030 ----
mean loss: 132.69
 ---- batch: 040 ----
mean loss: 135.07
 ---- batch: 050 ----
mean loss: 139.01
 ---- batch: 060 ----
mean loss: 140.99
 ---- batch: 070 ----
mean loss: 137.12
 ---- batch: 080 ----
mean loss: 136.78
 ---- batch: 090 ----
mean loss: 131.28
train mean loss: 136.89
epoch train time: 0:00:00.504229
elapsed time: 0:02:05.686980
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:18:45.243881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.31
 ---- batch: 020 ----
mean loss: 137.68
 ---- batch: 030 ----
mean loss: 132.41
 ---- batch: 040 ----
mean loss: 139.56
 ---- batch: 050 ----
mean loss: 134.74
 ---- batch: 060 ----
mean loss: 139.91
 ---- batch: 070 ----
mean loss: 135.17
 ---- batch: 080 ----
mean loss: 135.72
 ---- batch: 090 ----
mean loss: 135.87
train mean loss: 136.32
epoch train time: 0:00:00.507436
elapsed time: 0:02:06.194561
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:18:45.751464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.64
 ---- batch: 020 ----
mean loss: 128.99
 ---- batch: 030 ----
mean loss: 129.40
 ---- batch: 040 ----
mean loss: 140.67
 ---- batch: 050 ----
mean loss: 144.36
 ---- batch: 060 ----
mean loss: 140.86
 ---- batch: 070 ----
mean loss: 138.91
 ---- batch: 080 ----
mean loss: 134.17
 ---- batch: 090 ----
mean loss: 140.26
train mean loss: 137.08
epoch train time: 0:00:00.513473
elapsed time: 0:02:06.708179
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:18:46.265081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.77
 ---- batch: 020 ----
mean loss: 129.62
 ---- batch: 030 ----
mean loss: 133.58
 ---- batch: 040 ----
mean loss: 139.11
 ---- batch: 050 ----
mean loss: 138.02
 ---- batch: 060 ----
mean loss: 140.35
 ---- batch: 070 ----
mean loss: 146.86
 ---- batch: 080 ----
mean loss: 132.35
 ---- batch: 090 ----
mean loss: 144.85
train mean loss: 137.71
epoch train time: 0:00:00.506417
elapsed time: 0:02:07.214787
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:18:46.771703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.94
 ---- batch: 020 ----
mean loss: 140.22
 ---- batch: 030 ----
mean loss: 133.27
 ---- batch: 040 ----
mean loss: 136.34
 ---- batch: 050 ----
mean loss: 140.91
 ---- batch: 060 ----
mean loss: 136.69
 ---- batch: 070 ----
mean loss: 133.23
 ---- batch: 080 ----
mean loss: 138.03
 ---- batch: 090 ----
mean loss: 136.94
train mean loss: 136.60
epoch train time: 0:00:00.513841
elapsed time: 0:02:07.728825
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:18:47.285731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.93
 ---- batch: 020 ----
mean loss: 132.77
 ---- batch: 030 ----
mean loss: 132.24
 ---- batch: 040 ----
mean loss: 131.45
 ---- batch: 050 ----
mean loss: 133.66
 ---- batch: 060 ----
mean loss: 136.35
 ---- batch: 070 ----
mean loss: 137.23
 ---- batch: 080 ----
mean loss: 144.92
 ---- batch: 090 ----
mean loss: 140.18
train mean loss: 136.08
epoch train time: 0:00:00.519549
elapsed time: 0:02:08.248521
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:18:47.805454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.86
 ---- batch: 020 ----
mean loss: 129.76
 ---- batch: 030 ----
mean loss: 141.10
 ---- batch: 040 ----
mean loss: 134.83
 ---- batch: 050 ----
mean loss: 135.67
 ---- batch: 060 ----
mean loss: 131.44
 ---- batch: 070 ----
mean loss: 133.55
 ---- batch: 080 ----
mean loss: 139.71
 ---- batch: 090 ----
mean loss: 142.93
train mean loss: 136.65
epoch train time: 0:00:00.511385
elapsed time: 0:02:08.760084
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:18:48.316987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.21
 ---- batch: 020 ----
mean loss: 137.50
 ---- batch: 030 ----
mean loss: 138.00
 ---- batch: 040 ----
mean loss: 137.49
 ---- batch: 050 ----
mean loss: 134.86
 ---- batch: 060 ----
mean loss: 133.61
 ---- batch: 070 ----
mean loss: 133.80
 ---- batch: 080 ----
mean loss: 139.42
 ---- batch: 090 ----
mean loss: 131.76
train mean loss: 135.72
epoch train time: 0:00:00.507854
elapsed time: 0:02:09.268085
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:18:48.824989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.91
 ---- batch: 020 ----
mean loss: 128.69
 ---- batch: 030 ----
mean loss: 135.10
 ---- batch: 040 ----
mean loss: 132.40
 ---- batch: 050 ----
mean loss: 138.59
 ---- batch: 060 ----
mean loss: 140.30
 ---- batch: 070 ----
mean loss: 137.88
 ---- batch: 080 ----
mean loss: 142.16
 ---- batch: 090 ----
mean loss: 132.51
train mean loss: 135.42
epoch train time: 0:00:00.498458
elapsed time: 0:02:09.766689
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:18:49.323591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.21
 ---- batch: 020 ----
mean loss: 134.58
 ---- batch: 030 ----
mean loss: 136.49
 ---- batch: 040 ----
mean loss: 130.75
 ---- batch: 050 ----
mean loss: 140.24
 ---- batch: 060 ----
mean loss: 141.03
 ---- batch: 070 ----
mean loss: 131.87
 ---- batch: 080 ----
mean loss: 130.61
 ---- batch: 090 ----
mean loss: 139.73
train mean loss: 135.39
epoch train time: 0:00:00.503569
elapsed time: 0:02:10.270407
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:18:49.827310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.46
 ---- batch: 020 ----
mean loss: 128.63
 ---- batch: 030 ----
mean loss: 132.18
 ---- batch: 040 ----
mean loss: 131.87
 ---- batch: 050 ----
mean loss: 136.82
 ---- batch: 060 ----
mean loss: 132.18
 ---- batch: 070 ----
mean loss: 133.56
 ---- batch: 080 ----
mean loss: 135.84
 ---- batch: 090 ----
mean loss: 144.67
train mean loss: 134.03
epoch train time: 0:00:00.504129
elapsed time: 0:02:10.774677
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:18:50.331576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.72
 ---- batch: 020 ----
mean loss: 131.75
 ---- batch: 030 ----
mean loss: 132.95
 ---- batch: 040 ----
mean loss: 134.69
 ---- batch: 050 ----
mean loss: 134.76
 ---- batch: 060 ----
mean loss: 139.12
 ---- batch: 070 ----
mean loss: 132.75
 ---- batch: 080 ----
mean loss: 139.09
 ---- batch: 090 ----
mean loss: 134.28
train mean loss: 135.06
epoch train time: 0:00:00.504580
elapsed time: 0:02:11.279407
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:18:50.836311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.34
 ---- batch: 020 ----
mean loss: 133.81
 ---- batch: 030 ----
mean loss: 135.15
 ---- batch: 040 ----
mean loss: 132.57
 ---- batch: 050 ----
mean loss: 132.37
 ---- batch: 060 ----
mean loss: 132.56
 ---- batch: 070 ----
mean loss: 140.26
 ---- batch: 080 ----
mean loss: 140.73
 ---- batch: 090 ----
mean loss: 139.58
train mean loss: 135.33
epoch train time: 0:00:00.511005
elapsed time: 0:02:11.790558
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:18:51.347478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.81
 ---- batch: 020 ----
mean loss: 131.20
 ---- batch: 030 ----
mean loss: 133.98
 ---- batch: 040 ----
mean loss: 132.41
 ---- batch: 050 ----
mean loss: 126.56
 ---- batch: 060 ----
mean loss: 135.92
 ---- batch: 070 ----
mean loss: 137.50
 ---- batch: 080 ----
mean loss: 129.91
 ---- batch: 090 ----
mean loss: 144.24
train mean loss: 134.48
epoch train time: 0:00:00.503932
elapsed time: 0:02:12.294661
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:18:51.851570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.55
 ---- batch: 020 ----
mean loss: 131.86
 ---- batch: 030 ----
mean loss: 130.88
 ---- batch: 040 ----
mean loss: 132.49
 ---- batch: 050 ----
mean loss: 134.39
 ---- batch: 060 ----
mean loss: 138.08
 ---- batch: 070 ----
mean loss: 132.91
 ---- batch: 080 ----
mean loss: 139.78
 ---- batch: 090 ----
mean loss: 143.71
train mean loss: 135.58
epoch train time: 0:00:00.510233
elapsed time: 0:02:12.805047
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:18:52.361949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.83
 ---- batch: 020 ----
mean loss: 129.12
 ---- batch: 030 ----
mean loss: 131.23
 ---- batch: 040 ----
mean loss: 134.10
 ---- batch: 050 ----
mean loss: 139.88
 ---- batch: 060 ----
mean loss: 138.77
 ---- batch: 070 ----
mean loss: 132.60
 ---- batch: 080 ----
mean loss: 142.29
 ---- batch: 090 ----
mean loss: 138.31
train mean loss: 134.54
epoch train time: 0:00:00.504994
elapsed time: 0:02:13.310186
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:18:52.867087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.11
 ---- batch: 020 ----
mean loss: 131.86
 ---- batch: 030 ----
mean loss: 134.51
 ---- batch: 040 ----
mean loss: 131.54
 ---- batch: 050 ----
mean loss: 135.92
 ---- batch: 060 ----
mean loss: 140.56
 ---- batch: 070 ----
mean loss: 135.94
 ---- batch: 080 ----
mean loss: 138.77
 ---- batch: 090 ----
mean loss: 136.00
train mean loss: 134.96
epoch train time: 0:00:00.505139
elapsed time: 0:02:13.815474
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:18:53.372377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.61
 ---- batch: 020 ----
mean loss: 134.20
 ---- batch: 030 ----
mean loss: 125.31
 ---- batch: 040 ----
mean loss: 135.01
 ---- batch: 050 ----
mean loss: 132.81
 ---- batch: 060 ----
mean loss: 135.93
 ---- batch: 070 ----
mean loss: 138.18
 ---- batch: 080 ----
mean loss: 141.09
 ---- batch: 090 ----
mean loss: 137.84
train mean loss: 134.65
epoch train time: 0:00:00.510986
elapsed time: 0:02:14.326620
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:18:53.883532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.12
 ---- batch: 020 ----
mean loss: 130.04
 ---- batch: 030 ----
mean loss: 119.74
 ---- batch: 040 ----
mean loss: 131.96
 ---- batch: 050 ----
mean loss: 127.29
 ---- batch: 060 ----
mean loss: 119.46
 ---- batch: 070 ----
mean loss: 130.29
 ---- batch: 080 ----
mean loss: 130.88
 ---- batch: 090 ----
mean loss: 123.77
train mean loss: 127.27
epoch train time: 0:00:00.500484
elapsed time: 0:02:14.827281
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:18:54.384172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.62
 ---- batch: 020 ----
mean loss: 121.53
 ---- batch: 030 ----
mean loss: 126.24
 ---- batch: 040 ----
mean loss: 122.26
 ---- batch: 050 ----
mean loss: 127.37
 ---- batch: 060 ----
mean loss: 128.33
 ---- batch: 070 ----
mean loss: 126.37
 ---- batch: 080 ----
mean loss: 124.21
 ---- batch: 090 ----
mean loss: 125.49
train mean loss: 125.90
epoch train time: 0:00:00.504456
elapsed time: 0:02:15.331872
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:18:54.888775
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.45
 ---- batch: 020 ----
mean loss: 124.87
 ---- batch: 030 ----
mean loss: 119.96
 ---- batch: 040 ----
mean loss: 126.55
 ---- batch: 050 ----
mean loss: 127.35
 ---- batch: 060 ----
mean loss: 123.37
 ---- batch: 070 ----
mean loss: 122.46
 ---- batch: 080 ----
mean loss: 130.47
 ---- batch: 090 ----
mean loss: 126.91
train mean loss: 125.38
epoch train time: 0:00:00.502409
elapsed time: 0:02:15.834460
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:18:55.391381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.26
 ---- batch: 020 ----
mean loss: 127.84
 ---- batch: 030 ----
mean loss: 130.31
 ---- batch: 040 ----
mean loss: 121.92
 ---- batch: 050 ----
mean loss: 125.69
 ---- batch: 060 ----
mean loss: 126.99
 ---- batch: 070 ----
mean loss: 118.57
 ---- batch: 080 ----
mean loss: 130.65
 ---- batch: 090 ----
mean loss: 122.43
train mean loss: 125.13
epoch train time: 0:00:00.510732
elapsed time: 0:02:16.345357
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:18:55.902260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.76
 ---- batch: 020 ----
mean loss: 124.61
 ---- batch: 030 ----
mean loss: 129.54
 ---- batch: 040 ----
mean loss: 122.21
 ---- batch: 050 ----
mean loss: 128.24
 ---- batch: 060 ----
mean loss: 124.34
 ---- batch: 070 ----
mean loss: 122.89
 ---- batch: 080 ----
mean loss: 129.29
 ---- batch: 090 ----
mean loss: 122.70
train mean loss: 125.25
epoch train time: 0:00:00.505594
elapsed time: 0:02:16.851100
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:18:56.408015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.76
 ---- batch: 020 ----
mean loss: 122.51
 ---- batch: 030 ----
mean loss: 128.79
 ---- batch: 040 ----
mean loss: 125.08
 ---- batch: 050 ----
mean loss: 128.27
 ---- batch: 060 ----
mean loss: 124.58
 ---- batch: 070 ----
mean loss: 123.40
 ---- batch: 080 ----
mean loss: 125.71
 ---- batch: 090 ----
mean loss: 125.75
train mean loss: 125.15
epoch train time: 0:00:00.505555
elapsed time: 0:02:17.356833
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:18:56.913738
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.26
 ---- batch: 020 ----
mean loss: 120.33
 ---- batch: 030 ----
mean loss: 125.70
 ---- batch: 040 ----
mean loss: 124.87
 ---- batch: 050 ----
mean loss: 126.49
 ---- batch: 060 ----
mean loss: 118.88
 ---- batch: 070 ----
mean loss: 125.90
 ---- batch: 080 ----
mean loss: 125.97
 ---- batch: 090 ----
mean loss: 128.78
train mean loss: 125.10
epoch train time: 0:00:00.495943
elapsed time: 0:02:17.852948
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:18:57.409849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.97
 ---- batch: 020 ----
mean loss: 128.17
 ---- batch: 030 ----
mean loss: 128.94
 ---- batch: 040 ----
mean loss: 130.26
 ---- batch: 050 ----
mean loss: 123.79
 ---- batch: 060 ----
mean loss: 120.38
 ---- batch: 070 ----
mean loss: 124.93
 ---- batch: 080 ----
mean loss: 126.26
 ---- batch: 090 ----
mean loss: 120.75
train mean loss: 124.79
epoch train time: 0:00:00.506499
elapsed time: 0:02:18.359590
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:18:57.916492
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.22
 ---- batch: 020 ----
mean loss: 126.23
 ---- batch: 030 ----
mean loss: 123.57
 ---- batch: 040 ----
mean loss: 120.38
 ---- batch: 050 ----
mean loss: 130.52
 ---- batch: 060 ----
mean loss: 126.91
 ---- batch: 070 ----
mean loss: 120.36
 ---- batch: 080 ----
mean loss: 123.38
 ---- batch: 090 ----
mean loss: 126.48
train mean loss: 124.98
epoch train time: 0:00:00.502344
elapsed time: 0:02:18.862079
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:18:58.418981
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.23
 ---- batch: 020 ----
mean loss: 125.42
 ---- batch: 030 ----
mean loss: 127.28
 ---- batch: 040 ----
mean loss: 126.03
 ---- batch: 050 ----
mean loss: 122.66
 ---- batch: 060 ----
mean loss: 128.79
 ---- batch: 070 ----
mean loss: 120.84
 ---- batch: 080 ----
mean loss: 127.29
 ---- batch: 090 ----
mean loss: 126.38
train mean loss: 124.80
epoch train time: 0:00:00.500792
elapsed time: 0:02:19.363036
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:18:58.919938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.30
 ---- batch: 020 ----
mean loss: 116.11
 ---- batch: 030 ----
mean loss: 129.32
 ---- batch: 040 ----
mean loss: 125.31
 ---- batch: 050 ----
mean loss: 122.06
 ---- batch: 060 ----
mean loss: 125.94
 ---- batch: 070 ----
mean loss: 126.27
 ---- batch: 080 ----
mean loss: 123.32
 ---- batch: 090 ----
mean loss: 129.29
train mean loss: 124.88
epoch train time: 0:00:00.509352
elapsed time: 0:02:19.872582
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:18:59.429539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.17
 ---- batch: 020 ----
mean loss: 119.93
 ---- batch: 030 ----
mean loss: 126.10
 ---- batch: 040 ----
mean loss: 128.58
 ---- batch: 050 ----
mean loss: 126.88
 ---- batch: 060 ----
mean loss: 120.62
 ---- batch: 070 ----
mean loss: 124.95
 ---- batch: 080 ----
mean loss: 128.09
 ---- batch: 090 ----
mean loss: 125.79
train mean loss: 124.92
epoch train time: 0:00:00.512643
elapsed time: 0:02:20.385431
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:18:59.942335
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.88
 ---- batch: 020 ----
mean loss: 124.43
 ---- batch: 030 ----
mean loss: 119.77
 ---- batch: 040 ----
mean loss: 127.19
 ---- batch: 050 ----
mean loss: 132.73
 ---- batch: 060 ----
mean loss: 125.35
 ---- batch: 070 ----
mean loss: 122.67
 ---- batch: 080 ----
mean loss: 124.25
 ---- batch: 090 ----
mean loss: 126.66
train mean loss: 124.76
epoch train time: 0:00:00.505237
elapsed time: 0:02:20.890811
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:19:00.447711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.82
 ---- batch: 020 ----
mean loss: 117.33
 ---- batch: 030 ----
mean loss: 125.12
 ---- batch: 040 ----
mean loss: 122.47
 ---- batch: 050 ----
mean loss: 123.68
 ---- batch: 060 ----
mean loss: 127.83
 ---- batch: 070 ----
mean loss: 127.10
 ---- batch: 080 ----
mean loss: 126.39
 ---- batch: 090 ----
mean loss: 130.25
train mean loss: 124.94
epoch train time: 0:00:00.513571
elapsed time: 0:02:21.404580
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:19:00.961486
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.52
 ---- batch: 020 ----
mean loss: 123.42
 ---- batch: 030 ----
mean loss: 121.60
 ---- batch: 040 ----
mean loss: 121.32
 ---- batch: 050 ----
mean loss: 126.55
 ---- batch: 060 ----
mean loss: 127.63
 ---- batch: 070 ----
mean loss: 125.42
 ---- batch: 080 ----
mean loss: 130.96
 ---- batch: 090 ----
mean loss: 119.93
train mean loss: 124.82
epoch train time: 0:00:00.510006
elapsed time: 0:02:21.914745
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:19:01.471653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.89
 ---- batch: 020 ----
mean loss: 124.81
 ---- batch: 030 ----
mean loss: 125.14
 ---- batch: 040 ----
mean loss: 124.72
 ---- batch: 050 ----
mean loss: 124.50
 ---- batch: 060 ----
mean loss: 121.86
 ---- batch: 070 ----
mean loss: 126.60
 ---- batch: 080 ----
mean loss: 126.38
 ---- batch: 090 ----
mean loss: 125.50
train mean loss: 124.71
epoch train time: 0:00:00.510947
elapsed time: 0:02:22.425846
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:19:01.982748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.76
 ---- batch: 020 ----
mean loss: 129.22
 ---- batch: 030 ----
mean loss: 129.56
 ---- batch: 040 ----
mean loss: 121.42
 ---- batch: 050 ----
mean loss: 126.12
 ---- batch: 060 ----
mean loss: 122.34
 ---- batch: 070 ----
mean loss: 121.65
 ---- batch: 080 ----
mean loss: 127.70
 ---- batch: 090 ----
mean loss: 124.57
train mean loss: 124.65
epoch train time: 0:00:00.502822
elapsed time: 0:02:22.928835
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:19:02.485742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.30
 ---- batch: 020 ----
mean loss: 126.74
 ---- batch: 030 ----
mean loss: 125.99
 ---- batch: 040 ----
mean loss: 123.39
 ---- batch: 050 ----
mean loss: 125.94
 ---- batch: 060 ----
mean loss: 125.37
 ---- batch: 070 ----
mean loss: 116.87
 ---- batch: 080 ----
mean loss: 127.62
 ---- batch: 090 ----
mean loss: 131.43
train mean loss: 124.48
epoch train time: 0:00:00.505491
elapsed time: 0:02:23.434477
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:19:02.991398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.47
 ---- batch: 020 ----
mean loss: 123.17
 ---- batch: 030 ----
mean loss: 129.66
 ---- batch: 040 ----
mean loss: 127.71
 ---- batch: 050 ----
mean loss: 128.77
 ---- batch: 060 ----
mean loss: 117.77
 ---- batch: 070 ----
mean loss: 119.33
 ---- batch: 080 ----
mean loss: 126.37
 ---- batch: 090 ----
mean loss: 121.39
train mean loss: 124.53
epoch train time: 0:00:00.501367
elapsed time: 0:02:23.936040
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:19:03.492950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.68
 ---- batch: 020 ----
mean loss: 124.58
 ---- batch: 030 ----
mean loss: 121.04
 ---- batch: 040 ----
mean loss: 123.89
 ---- batch: 050 ----
mean loss: 125.05
 ---- batch: 060 ----
mean loss: 126.72
 ---- batch: 070 ----
mean loss: 126.10
 ---- batch: 080 ----
mean loss: 124.84
 ---- batch: 090 ----
mean loss: 124.71
train mean loss: 124.44
epoch train time: 0:00:00.513065
elapsed time: 0:02:24.449299
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:19:04.006202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.09
 ---- batch: 020 ----
mean loss: 122.70
 ---- batch: 030 ----
mean loss: 120.52
 ---- batch: 040 ----
mean loss: 123.90
 ---- batch: 050 ----
mean loss: 127.10
 ---- batch: 060 ----
mean loss: 127.86
 ---- batch: 070 ----
mean loss: 123.28
 ---- batch: 080 ----
mean loss: 129.36
 ---- batch: 090 ----
mean loss: 118.75
train mean loss: 124.31
epoch train time: 0:00:00.512691
elapsed time: 0:02:24.962139
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:19:04.519057
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.99
 ---- batch: 020 ----
mean loss: 126.07
 ---- batch: 030 ----
mean loss: 124.91
 ---- batch: 040 ----
mean loss: 122.91
 ---- batch: 050 ----
mean loss: 118.06
 ---- batch: 060 ----
mean loss: 127.87
 ---- batch: 070 ----
mean loss: 124.99
 ---- batch: 080 ----
mean loss: 130.39
 ---- batch: 090 ----
mean loss: 124.37
train mean loss: 124.55
epoch train time: 0:00:00.509240
elapsed time: 0:02:25.471555
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:19:05.028481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.16
 ---- batch: 020 ----
mean loss: 127.24
 ---- batch: 030 ----
mean loss: 125.65
 ---- batch: 040 ----
mean loss: 121.70
 ---- batch: 050 ----
mean loss: 124.87
 ---- batch: 060 ----
mean loss: 123.68
 ---- batch: 070 ----
mean loss: 124.92
 ---- batch: 080 ----
mean loss: 125.69
 ---- batch: 090 ----
mean loss: 128.06
train mean loss: 124.27
epoch train time: 0:00:00.501006
elapsed time: 0:02:25.972777
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:19:05.529682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.89
 ---- batch: 020 ----
mean loss: 116.03
 ---- batch: 030 ----
mean loss: 127.34
 ---- batch: 040 ----
mean loss: 118.69
 ---- batch: 050 ----
mean loss: 123.11
 ---- batch: 060 ----
mean loss: 121.93
 ---- batch: 070 ----
mean loss: 126.46
 ---- batch: 080 ----
mean loss: 132.51
 ---- batch: 090 ----
mean loss: 130.61
train mean loss: 124.51
epoch train time: 0:00:00.501714
elapsed time: 0:02:26.474637
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:19:06.031541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.32
 ---- batch: 020 ----
mean loss: 126.88
 ---- batch: 030 ----
mean loss: 121.66
 ---- batch: 040 ----
mean loss: 121.32
 ---- batch: 050 ----
mean loss: 125.25
 ---- batch: 060 ----
mean loss: 122.24
 ---- batch: 070 ----
mean loss: 124.24
 ---- batch: 080 ----
mean loss: 123.42
 ---- batch: 090 ----
mean loss: 129.34
train mean loss: 124.49
epoch train time: 0:00:00.499225
elapsed time: 0:02:26.974031
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:19:06.530936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.35
 ---- batch: 020 ----
mean loss: 126.03
 ---- batch: 030 ----
mean loss: 124.85
 ---- batch: 040 ----
mean loss: 123.03
 ---- batch: 050 ----
mean loss: 122.06
 ---- batch: 060 ----
mean loss: 122.47
 ---- batch: 070 ----
mean loss: 122.68
 ---- batch: 080 ----
mean loss: 125.89
 ---- batch: 090 ----
mean loss: 129.02
train mean loss: 124.40
epoch train time: 0:00:00.507843
elapsed time: 0:02:27.482027
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:19:07.038948
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.63
 ---- batch: 020 ----
mean loss: 127.13
 ---- batch: 030 ----
mean loss: 123.13
 ---- batch: 040 ----
mean loss: 129.55
 ---- batch: 050 ----
mean loss: 126.73
 ---- batch: 060 ----
mean loss: 123.88
 ---- batch: 070 ----
mean loss: 122.89
 ---- batch: 080 ----
mean loss: 126.42
 ---- batch: 090 ----
mean loss: 119.72
train mean loss: 124.35
epoch train time: 0:00:00.499190
elapsed time: 0:02:27.981381
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:19:07.538283
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.13
 ---- batch: 020 ----
mean loss: 121.95
 ---- batch: 030 ----
mean loss: 126.78
 ---- batch: 040 ----
mean loss: 124.39
 ---- batch: 050 ----
mean loss: 126.09
 ---- batch: 060 ----
mean loss: 123.15
 ---- batch: 070 ----
mean loss: 124.33
 ---- batch: 080 ----
mean loss: 122.60
 ---- batch: 090 ----
mean loss: 127.19
train mean loss: 124.23
epoch train time: 0:00:00.502026
elapsed time: 0:02:28.483551
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:19:08.040454
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.11
 ---- batch: 020 ----
mean loss: 123.05
 ---- batch: 030 ----
mean loss: 121.93
 ---- batch: 040 ----
mean loss: 124.68
 ---- batch: 050 ----
mean loss: 131.50
 ---- batch: 060 ----
mean loss: 122.70
 ---- batch: 070 ----
mean loss: 122.14
 ---- batch: 080 ----
mean loss: 120.75
 ---- batch: 090 ----
mean loss: 128.72
train mean loss: 124.20
epoch train time: 0:00:00.494346
elapsed time: 0:02:28.978055
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:19:08.534955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.52
 ---- batch: 020 ----
mean loss: 127.46
 ---- batch: 030 ----
mean loss: 126.55
 ---- batch: 040 ----
mean loss: 122.81
 ---- batch: 050 ----
mean loss: 121.74
 ---- batch: 060 ----
mean loss: 125.83
 ---- batch: 070 ----
mean loss: 120.70
 ---- batch: 080 ----
mean loss: 127.91
 ---- batch: 090 ----
mean loss: 122.47
train mean loss: 124.22
epoch train time: 0:00:00.512962
elapsed time: 0:02:29.491162
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:19:09.048065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.50
 ---- batch: 020 ----
mean loss: 123.26
 ---- batch: 030 ----
mean loss: 125.48
 ---- batch: 040 ----
mean loss: 124.01
 ---- batch: 050 ----
mean loss: 120.13
 ---- batch: 060 ----
mean loss: 120.32
 ---- batch: 070 ----
mean loss: 124.16
 ---- batch: 080 ----
mean loss: 127.41
 ---- batch: 090 ----
mean loss: 123.12
train mean loss: 124.11
epoch train time: 0:00:00.498506
elapsed time: 0:02:29.989834
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:19:09.546737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.77
 ---- batch: 020 ----
mean loss: 120.93
 ---- batch: 030 ----
mean loss: 120.18
 ---- batch: 040 ----
mean loss: 127.15
 ---- batch: 050 ----
mean loss: 122.43
 ---- batch: 060 ----
mean loss: 125.35
 ---- batch: 070 ----
mean loss: 131.63
 ---- batch: 080 ----
mean loss: 121.29
 ---- batch: 090 ----
mean loss: 124.63
train mean loss: 124.01
epoch train time: 0:00:00.508024
elapsed time: 0:02:30.498003
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:19:10.054920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.65
 ---- batch: 020 ----
mean loss: 124.55
 ---- batch: 030 ----
mean loss: 122.00
 ---- batch: 040 ----
mean loss: 123.05
 ---- batch: 050 ----
mean loss: 123.73
 ---- batch: 060 ----
mean loss: 121.55
 ---- batch: 070 ----
mean loss: 118.89
 ---- batch: 080 ----
mean loss: 131.44
 ---- batch: 090 ----
mean loss: 125.99
train mean loss: 124.38
epoch train time: 0:00:00.511109
elapsed time: 0:02:31.009305
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:19:10.566229
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.56
 ---- batch: 020 ----
mean loss: 126.67
 ---- batch: 030 ----
mean loss: 122.56
 ---- batch: 040 ----
mean loss: 124.00
 ---- batch: 050 ----
mean loss: 126.35
 ---- batch: 060 ----
mean loss: 123.54
 ---- batch: 070 ----
mean loss: 124.78
 ---- batch: 080 ----
mean loss: 122.37
 ---- batch: 090 ----
mean loss: 123.70
train mean loss: 124.12
epoch train time: 0:00:00.513948
elapsed time: 0:02:31.523419
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:19:11.080336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.21
 ---- batch: 020 ----
mean loss: 127.35
 ---- batch: 030 ----
mean loss: 122.15
 ---- batch: 040 ----
mean loss: 120.61
 ---- batch: 050 ----
mean loss: 127.18
 ---- batch: 060 ----
mean loss: 124.45
 ---- batch: 070 ----
mean loss: 124.12
 ---- batch: 080 ----
mean loss: 123.53
 ---- batch: 090 ----
mean loss: 124.86
train mean loss: 124.10
epoch train time: 0:00:00.501482
elapsed time: 0:02:32.025065
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:19:11.581970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.25
 ---- batch: 020 ----
mean loss: 122.01
 ---- batch: 030 ----
mean loss: 125.57
 ---- batch: 040 ----
mean loss: 131.53
 ---- batch: 050 ----
mean loss: 124.17
 ---- batch: 060 ----
mean loss: 119.34
 ---- batch: 070 ----
mean loss: 120.66
 ---- batch: 080 ----
mean loss: 123.57
 ---- batch: 090 ----
mean loss: 119.76
train mean loss: 123.79
epoch train time: 0:00:00.506799
elapsed time: 0:02:32.532015
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:19:12.088920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.55
 ---- batch: 020 ----
mean loss: 123.25
 ---- batch: 030 ----
mean loss: 132.91
 ---- batch: 040 ----
mean loss: 123.69
 ---- batch: 050 ----
mean loss: 124.88
 ---- batch: 060 ----
mean loss: 120.82
 ---- batch: 070 ----
mean loss: 121.43
 ---- batch: 080 ----
mean loss: 128.09
 ---- batch: 090 ----
mean loss: 123.65
train mean loss: 124.17
epoch train time: 0:00:00.517245
elapsed time: 0:02:33.049411
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:19:12.606313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.36
 ---- batch: 020 ----
mean loss: 125.95
 ---- batch: 030 ----
mean loss: 124.40
 ---- batch: 040 ----
mean loss: 123.92
 ---- batch: 050 ----
mean loss: 127.73
 ---- batch: 060 ----
mean loss: 121.44
 ---- batch: 070 ----
mean loss: 125.79
 ---- batch: 080 ----
mean loss: 122.33
 ---- batch: 090 ----
mean loss: 119.81
train mean loss: 124.10
epoch train time: 0:00:00.515817
elapsed time: 0:02:33.565373
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:19:13.122272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.16
 ---- batch: 020 ----
mean loss: 124.88
 ---- batch: 030 ----
mean loss: 124.35
 ---- batch: 040 ----
mean loss: 120.36
 ---- batch: 050 ----
mean loss: 127.88
 ---- batch: 060 ----
mean loss: 126.55
 ---- batch: 070 ----
mean loss: 120.34
 ---- batch: 080 ----
mean loss: 127.91
 ---- batch: 090 ----
mean loss: 123.32
train mean loss: 124.05
epoch train time: 0:00:00.496825
elapsed time: 0:02:34.062355
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:19:13.619257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.97
 ---- batch: 020 ----
mean loss: 125.19
 ---- batch: 030 ----
mean loss: 119.66
 ---- batch: 040 ----
mean loss: 117.05
 ---- batch: 050 ----
mean loss: 124.97
 ---- batch: 060 ----
mean loss: 123.50
 ---- batch: 070 ----
mean loss: 129.37
 ---- batch: 080 ----
mean loss: 123.48
 ---- batch: 090 ----
mean loss: 118.57
train mean loss: 123.82
epoch train time: 0:00:00.507670
elapsed time: 0:02:34.570203
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:19:14.127106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.85
 ---- batch: 020 ----
mean loss: 119.17
 ---- batch: 030 ----
mean loss: 128.26
 ---- batch: 040 ----
mean loss: 124.73
 ---- batch: 050 ----
mean loss: 120.58
 ---- batch: 060 ----
mean loss: 123.07
 ---- batch: 070 ----
mean loss: 126.66
 ---- batch: 080 ----
mean loss: 122.92
 ---- batch: 090 ----
mean loss: 124.06
train mean loss: 124.11
epoch train time: 0:00:00.500116
elapsed time: 0:02:35.070464
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:19:14.627385
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.84
 ---- batch: 020 ----
mean loss: 119.98
 ---- batch: 030 ----
mean loss: 124.06
 ---- batch: 040 ----
mean loss: 128.43
 ---- batch: 050 ----
mean loss: 117.55
 ---- batch: 060 ----
mean loss: 125.79
 ---- batch: 070 ----
mean loss: 125.86
 ---- batch: 080 ----
mean loss: 121.11
 ---- batch: 090 ----
mean loss: 125.79
train mean loss: 123.75
epoch train time: 0:00:00.504252
elapsed time: 0:02:35.574910
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:19:15.131811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.05
 ---- batch: 020 ----
mean loss: 123.87
 ---- batch: 030 ----
mean loss: 123.79
 ---- batch: 040 ----
mean loss: 120.05
 ---- batch: 050 ----
mean loss: 121.34
 ---- batch: 060 ----
mean loss: 124.11
 ---- batch: 070 ----
mean loss: 126.26
 ---- batch: 080 ----
mean loss: 120.80
 ---- batch: 090 ----
mean loss: 128.76
train mean loss: 123.87
epoch train time: 0:00:00.502908
elapsed time: 0:02:36.077963
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:19:15.634865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.97
 ---- batch: 020 ----
mean loss: 129.39
 ---- batch: 030 ----
mean loss: 121.27
 ---- batch: 040 ----
mean loss: 118.98
 ---- batch: 050 ----
mean loss: 123.75
 ---- batch: 060 ----
mean loss: 123.30
 ---- batch: 070 ----
mean loss: 127.32
 ---- batch: 080 ----
mean loss: 118.42
 ---- batch: 090 ----
mean loss: 129.96
train mean loss: 123.84
epoch train time: 0:00:00.506456
elapsed time: 0:02:36.584576
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:19:16.141485
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.90
 ---- batch: 020 ----
mean loss: 124.81
 ---- batch: 030 ----
mean loss: 119.09
 ---- batch: 040 ----
mean loss: 125.72
 ---- batch: 050 ----
mean loss: 127.26
 ---- batch: 060 ----
mean loss: 123.20
 ---- batch: 070 ----
mean loss: 122.50
 ---- batch: 080 ----
mean loss: 126.05
 ---- batch: 090 ----
mean loss: 126.82
train mean loss: 123.57
epoch train time: 0:00:00.503309
elapsed time: 0:02:37.088053
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:19:16.644974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.75
 ---- batch: 020 ----
mean loss: 118.09
 ---- batch: 030 ----
mean loss: 121.93
 ---- batch: 040 ----
mean loss: 124.09
 ---- batch: 050 ----
mean loss: 123.34
 ---- batch: 060 ----
mean loss: 125.65
 ---- batch: 070 ----
mean loss: 125.92
 ---- batch: 080 ----
mean loss: 118.19
 ---- batch: 090 ----
mean loss: 131.56
train mean loss: 123.86
epoch train time: 0:00:00.508150
elapsed time: 0:02:37.596370
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:19:17.153275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.27
 ---- batch: 020 ----
mean loss: 125.48
 ---- batch: 030 ----
mean loss: 121.07
 ---- batch: 040 ----
mean loss: 129.59
 ---- batch: 050 ----
mean loss: 124.00
 ---- batch: 060 ----
mean loss: 124.51
 ---- batch: 070 ----
mean loss: 119.59
 ---- batch: 080 ----
mean loss: 123.17
 ---- batch: 090 ----
mean loss: 123.45
train mean loss: 123.66
epoch train time: 0:00:00.497743
elapsed time: 0:02:38.094260
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:19:17.651163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.89
 ---- batch: 020 ----
mean loss: 118.47
 ---- batch: 030 ----
mean loss: 117.77
 ---- batch: 040 ----
mean loss: 123.33
 ---- batch: 050 ----
mean loss: 125.86
 ---- batch: 060 ----
mean loss: 129.82
 ---- batch: 070 ----
mean loss: 131.73
 ---- batch: 080 ----
mean loss: 124.26
 ---- batch: 090 ----
mean loss: 122.12
train mean loss: 123.55
epoch train time: 0:00:00.502726
elapsed time: 0:02:38.597144
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:19:18.154065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.32
 ---- batch: 020 ----
mean loss: 126.11
 ---- batch: 030 ----
mean loss: 121.51
 ---- batch: 040 ----
mean loss: 118.19
 ---- batch: 050 ----
mean loss: 126.88
 ---- batch: 060 ----
mean loss: 121.89
 ---- batch: 070 ----
mean loss: 124.68
 ---- batch: 080 ----
mean loss: 125.82
 ---- batch: 090 ----
mean loss: 128.81
train mean loss: 123.39
epoch train time: 0:00:00.500395
elapsed time: 0:02:39.103983
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_5/checkpoint.pth.tar
**** end time: 2019-09-25 23:19:18.660852 ****
