Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18382
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-25 17:35:18.942791 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 16, 11]           1,120
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
    BayesianConv2d-4            [-1, 14, 7, 11]             448
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
    BayesianLinear-8                    [-1, 1]             924
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,492
Trainable params: 2,492
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 17:35:18.953874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4192.15
 ---- batch: 020 ----
mean loss: 3958.29
 ---- batch: 030 ----
mean loss: 3837.73
 ---- batch: 040 ----
mean loss: 3545.82
 ---- batch: 050 ----
mean loss: 3258.37
 ---- batch: 060 ----
mean loss: 3110.40
 ---- batch: 070 ----
mean loss: 2822.97
 ---- batch: 080 ----
mean loss: 2645.13
 ---- batch: 090 ----
mean loss: 2445.05
train mean loss: 3247.49
epoch train time: 0:00:36.337692
elapsed time: 0:00:36.352093
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 17:35:55.294930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2091.42
 ---- batch: 020 ----
mean loss: 1964.89
 ---- batch: 030 ----
mean loss: 1787.59
 ---- batch: 040 ----
mean loss: 1639.67
 ---- batch: 050 ----
mean loss: 1471.51
 ---- batch: 060 ----
mean loss: 1380.26
 ---- batch: 070 ----
mean loss: 1273.64
 ---- batch: 080 ----
mean loss: 1195.74
 ---- batch: 090 ----
mean loss: 1148.83
train mean loss: 1519.52
epoch train time: 0:00:02.677642
elapsed time: 0:00:39.030094
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 17:35:57.973056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1044.70
 ---- batch: 020 ----
mean loss: 988.91
 ---- batch: 030 ----
mean loss: 981.35
 ---- batch: 040 ----
mean loss: 974.20
 ---- batch: 050 ----
mean loss: 954.12
 ---- batch: 060 ----
mean loss: 919.06
 ---- batch: 070 ----
mean loss: 930.09
 ---- batch: 080 ----
mean loss: 894.93
 ---- batch: 090 ----
mean loss: 915.01
train mean loss: 952.15
epoch train time: 0:00:02.635131
elapsed time: 0:00:41.665667
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 17:36:00.608629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.23
 ---- batch: 020 ----
mean loss: 899.77
 ---- batch: 030 ----
mean loss: 898.02
 ---- batch: 040 ----
mean loss: 905.23
 ---- batch: 050 ----
mean loss: 893.99
 ---- batch: 060 ----
mean loss: 888.72
 ---- batch: 070 ----
mean loss: 908.16
 ---- batch: 080 ----
mean loss: 902.72
 ---- batch: 090 ----
mean loss: 888.36
train mean loss: 900.44
epoch train time: 0:00:02.659998
elapsed time: 0:00:44.326141
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 17:36:03.269126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.08
 ---- batch: 020 ----
mean loss: 884.33
 ---- batch: 030 ----
mean loss: 903.53
 ---- batch: 040 ----
mean loss: 895.31
 ---- batch: 050 ----
mean loss: 882.82
 ---- batch: 060 ----
mean loss: 892.39
 ---- batch: 070 ----
mean loss: 904.45
 ---- batch: 080 ----
mean loss: 904.20
 ---- batch: 090 ----
mean loss: 880.58
train mean loss: 893.12
epoch train time: 0:00:02.632546
elapsed time: 0:00:46.959321
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 17:36:05.902289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.93
 ---- batch: 020 ----
mean loss: 893.24
 ---- batch: 030 ----
mean loss: 893.17
 ---- batch: 040 ----
mean loss: 895.10
 ---- batch: 050 ----
mean loss: 886.56
 ---- batch: 060 ----
mean loss: 859.78
 ---- batch: 070 ----
mean loss: 891.98
 ---- batch: 080 ----
mean loss: 886.45
 ---- batch: 090 ----
mean loss: 880.01
train mean loss: 888.06
epoch train time: 0:00:02.623741
elapsed time: 0:00:49.583531
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 17:36:08.526503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.98
 ---- batch: 020 ----
mean loss: 886.94
 ---- batch: 030 ----
mean loss: 895.34
 ---- batch: 040 ----
mean loss: 894.01
 ---- batch: 050 ----
mean loss: 890.28
 ---- batch: 060 ----
mean loss: 877.29
 ---- batch: 070 ----
mean loss: 875.93
 ---- batch: 080 ----
mean loss: 878.24
 ---- batch: 090 ----
mean loss: 881.22
train mean loss: 883.21
epoch train time: 0:00:02.684530
elapsed time: 0:00:52.268532
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 17:36:11.211500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.85
 ---- batch: 020 ----
mean loss: 878.15
 ---- batch: 030 ----
mean loss: 914.13
 ---- batch: 040 ----
mean loss: 883.00
 ---- batch: 050 ----
mean loss: 876.44
 ---- batch: 060 ----
mean loss: 875.36
 ---- batch: 070 ----
mean loss: 896.31
 ---- batch: 080 ----
mean loss: 875.25
 ---- batch: 090 ----
mean loss: 850.80
train mean loss: 877.57
epoch train time: 0:00:02.653565
elapsed time: 0:00:54.922559
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 17:36:13.865531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.44
 ---- batch: 020 ----
mean loss: 872.33
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 899.04
 ---- batch: 050 ----
mean loss: 854.72
 ---- batch: 060 ----
mean loss: 869.73
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 855.86
 ---- batch: 090 ----
mean loss: 879.70
train mean loss: 872.22
epoch train time: 0:00:02.652779
elapsed time: 0:00:57.575838
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 17:36:16.518802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.30
 ---- batch: 020 ----
mean loss: 873.27
 ---- batch: 030 ----
mean loss: 851.26
 ---- batch: 040 ----
mean loss: 863.94
 ---- batch: 050 ----
mean loss: 882.74
 ---- batch: 060 ----
mean loss: 885.36
 ---- batch: 070 ----
mean loss: 875.25
 ---- batch: 080 ----
mean loss: 861.41
 ---- batch: 090 ----
mean loss: 861.17
train mean loss: 870.28
epoch train time: 0:00:02.660503
elapsed time: 0:01:00.236811
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 17:36:19.179773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.81
 ---- batch: 020 ----
mean loss: 878.44
 ---- batch: 030 ----
mean loss: 867.11
 ---- batch: 040 ----
mean loss: 871.45
 ---- batch: 050 ----
mean loss: 855.64
 ---- batch: 060 ----
mean loss: 868.60
 ---- batch: 070 ----
mean loss: 866.32
 ---- batch: 080 ----
mean loss: 853.64
 ---- batch: 090 ----
mean loss: 864.91
train mean loss: 864.42
epoch train time: 0:00:02.663459
elapsed time: 0:01:02.900760
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 17:36:21.843711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.04
 ---- batch: 020 ----
mean loss: 850.73
 ---- batch: 030 ----
mean loss: 866.47
 ---- batch: 040 ----
mean loss: 885.04
 ---- batch: 050 ----
mean loss: 880.23
 ---- batch: 060 ----
mean loss: 867.14
 ---- batch: 070 ----
mean loss: 872.84
 ---- batch: 080 ----
mean loss: 860.31
 ---- batch: 090 ----
mean loss: 847.98
train mean loss: 860.15
epoch train time: 0:00:02.646760
elapsed time: 0:01:05.548032
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 17:36:24.491006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.60
 ---- batch: 020 ----
mean loss: 857.36
 ---- batch: 030 ----
mean loss: 860.29
 ---- batch: 040 ----
mean loss: 845.72
 ---- batch: 050 ----
mean loss: 866.32
 ---- batch: 060 ----
mean loss: 862.26
 ---- batch: 070 ----
mean loss: 841.81
 ---- batch: 080 ----
mean loss: 851.72
 ---- batch: 090 ----
mean loss: 866.82
train mean loss: 856.21
epoch train time: 0:00:02.647696
elapsed time: 0:01:08.196221
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 17:36:27.139182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.70
 ---- batch: 020 ----
mean loss: 856.54
 ---- batch: 030 ----
mean loss: 847.65
 ---- batch: 040 ----
mean loss: 828.22
 ---- batch: 050 ----
mean loss: 854.04
 ---- batch: 060 ----
mean loss: 842.18
 ---- batch: 070 ----
mean loss: 871.34
 ---- batch: 080 ----
mean loss: 848.88
 ---- batch: 090 ----
mean loss: 852.07
train mean loss: 850.56
epoch train time: 0:00:02.655651
elapsed time: 0:01:10.852373
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 17:36:29.795338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.53
 ---- batch: 020 ----
mean loss: 847.51
 ---- batch: 030 ----
mean loss: 852.64
 ---- batch: 040 ----
mean loss: 844.79
 ---- batch: 050 ----
mean loss: 835.65
 ---- batch: 060 ----
mean loss: 831.48
 ---- batch: 070 ----
mean loss: 835.01
 ---- batch: 080 ----
mean loss: 857.32
 ---- batch: 090 ----
mean loss: 847.65
train mean loss: 846.23
epoch train time: 0:00:02.657910
elapsed time: 0:01:13.510771
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 17:36:32.453751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.60
 ---- batch: 020 ----
mean loss: 846.50
 ---- batch: 030 ----
mean loss: 837.89
 ---- batch: 040 ----
mean loss: 859.47
 ---- batch: 050 ----
mean loss: 847.98
 ---- batch: 060 ----
mean loss: 830.73
 ---- batch: 070 ----
mean loss: 823.45
 ---- batch: 080 ----
mean loss: 837.35
 ---- batch: 090 ----
mean loss: 843.78
train mean loss: 841.27
epoch train time: 0:00:02.650756
elapsed time: 0:01:16.162003
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 17:36:35.104977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.02
 ---- batch: 020 ----
mean loss: 817.73
 ---- batch: 030 ----
mean loss: 832.80
 ---- batch: 040 ----
mean loss: 846.27
 ---- batch: 050 ----
mean loss: 816.87
 ---- batch: 060 ----
mean loss: 842.08
 ---- batch: 070 ----
mean loss: 847.22
 ---- batch: 080 ----
mean loss: 852.21
 ---- batch: 090 ----
mean loss: 818.48
train mean loss: 836.17
epoch train time: 0:00:02.636418
elapsed time: 0:01:18.798944
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 17:36:37.741910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.33
 ---- batch: 020 ----
mean loss: 827.92
 ---- batch: 030 ----
mean loss: 843.78
 ---- batch: 040 ----
mean loss: 836.33
 ---- batch: 050 ----
mean loss: 816.41
 ---- batch: 060 ----
mean loss: 817.31
 ---- batch: 070 ----
mean loss: 831.97
 ---- batch: 080 ----
mean loss: 828.38
 ---- batch: 090 ----
mean loss: 821.02
train mean loss: 830.30
epoch train time: 0:00:02.656424
elapsed time: 0:01:21.455860
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 17:36:40.398849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.93
 ---- batch: 020 ----
mean loss: 833.24
 ---- batch: 030 ----
mean loss: 813.22
 ---- batch: 040 ----
mean loss: 830.04
 ---- batch: 050 ----
mean loss: 828.83
 ---- batch: 060 ----
mean loss: 825.11
 ---- batch: 070 ----
mean loss: 805.94
 ---- batch: 080 ----
mean loss: 834.50
 ---- batch: 090 ----
mean loss: 828.68
train mean loss: 825.33
epoch train time: 0:00:02.658008
elapsed time: 0:01:24.114361
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 17:36:43.057347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.02
 ---- batch: 020 ----
mean loss: 833.98
 ---- batch: 030 ----
mean loss: 822.84
 ---- batch: 040 ----
mean loss: 828.71
 ---- batch: 050 ----
mean loss: 810.73
 ---- batch: 060 ----
mean loss: 819.36
 ---- batch: 070 ----
mean loss: 821.31
 ---- batch: 080 ----
mean loss: 808.69
 ---- batch: 090 ----
mean loss: 811.58
train mean loss: 820.01
epoch train time: 0:00:02.654585
elapsed time: 0:01:26.769442
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 17:36:45.712404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.43
 ---- batch: 020 ----
mean loss: 814.51
 ---- batch: 030 ----
mean loss: 805.39
 ---- batch: 040 ----
mean loss: 815.49
 ---- batch: 050 ----
mean loss: 827.11
 ---- batch: 060 ----
mean loss: 812.35
 ---- batch: 070 ----
mean loss: 795.55
 ---- batch: 080 ----
mean loss: 823.76
 ---- batch: 090 ----
mean loss: 817.50
train mean loss: 813.51
epoch train time: 0:00:02.661739
elapsed time: 0:01:29.431647
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 17:36:48.374549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.20
 ---- batch: 020 ----
mean loss: 818.86
 ---- batch: 030 ----
mean loss: 804.53
 ---- batch: 040 ----
mean loss: 790.89
 ---- batch: 050 ----
mean loss: 801.02
 ---- batch: 060 ----
mean loss: 822.90
 ---- batch: 070 ----
mean loss: 812.35
 ---- batch: 080 ----
mean loss: 803.41
 ---- batch: 090 ----
mean loss: 809.49
train mean loss: 808.61
epoch train time: 0:00:02.698063
elapsed time: 0:01:32.130148
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 17:36:51.073101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 812.24
 ---- batch: 020 ----
mean loss: 797.93
 ---- batch: 030 ----
mean loss: 787.88
 ---- batch: 040 ----
mean loss: 797.01
 ---- batch: 050 ----
mean loss: 794.86
 ---- batch: 060 ----
mean loss: 799.70
 ---- batch: 070 ----
mean loss: 800.77
 ---- batch: 080 ----
mean loss: 804.26
 ---- batch: 090 ----
mean loss: 808.25
train mean loss: 800.49
epoch train time: 0:00:02.633788
elapsed time: 0:01:34.764408
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 17:36:53.707364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 811.01
 ---- batch: 020 ----
mean loss: 788.01
 ---- batch: 030 ----
mean loss: 795.74
 ---- batch: 040 ----
mean loss: 787.81
 ---- batch: 050 ----
mean loss: 788.36
 ---- batch: 060 ----
mean loss: 781.40
 ---- batch: 070 ----
mean loss: 796.63
 ---- batch: 080 ----
mean loss: 802.49
 ---- batch: 090 ----
mean loss: 786.57
train mean loss: 794.31
epoch train time: 0:00:02.643036
elapsed time: 0:01:37.407856
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 17:36:56.350818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.16
 ---- batch: 020 ----
mean loss: 804.95
 ---- batch: 030 ----
mean loss: 789.97
 ---- batch: 040 ----
mean loss: 791.92
 ---- batch: 050 ----
mean loss: 789.90
 ---- batch: 060 ----
mean loss: 786.44
 ---- batch: 070 ----
mean loss: 778.73
 ---- batch: 080 ----
mean loss: 778.70
 ---- batch: 090 ----
mean loss: 787.14
train mean loss: 785.57
epoch train time: 0:00:02.641284
elapsed time: 0:01:40.049583
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 17:36:58.992547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.86
 ---- batch: 020 ----
mean loss: 774.51
 ---- batch: 030 ----
mean loss: 769.00
 ---- batch: 040 ----
mean loss: 769.10
 ---- batch: 050 ----
mean loss: 766.80
 ---- batch: 060 ----
mean loss: 797.10
 ---- batch: 070 ----
mean loss: 779.12
 ---- batch: 080 ----
mean loss: 775.99
 ---- batch: 090 ----
mean loss: 769.58
train mean loss: 775.86
epoch train time: 0:00:02.633807
elapsed time: 0:01:42.683881
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 17:37:01.626934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.05
 ---- batch: 020 ----
mean loss: 781.01
 ---- batch: 030 ----
mean loss: 766.02
 ---- batch: 040 ----
mean loss: 763.73
 ---- batch: 050 ----
mean loss: 751.86
 ---- batch: 060 ----
mean loss: 766.12
 ---- batch: 070 ----
mean loss: 773.51
 ---- batch: 080 ----
mean loss: 774.44
 ---- batch: 090 ----
mean loss: 774.18
train mean loss: 767.92
epoch train time: 0:00:02.646740
elapsed time: 0:01:45.331164
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 17:37:04.274198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.60
 ---- batch: 020 ----
mean loss: 756.42
 ---- batch: 030 ----
mean loss: 760.00
 ---- batch: 040 ----
mean loss: 769.74
 ---- batch: 050 ----
mean loss: 760.03
 ---- batch: 060 ----
mean loss: 750.41
 ---- batch: 070 ----
mean loss: 744.03
 ---- batch: 080 ----
mean loss: 772.99
 ---- batch: 090 ----
mean loss: 739.86
train mean loss: 757.31
epoch train time: 0:00:02.635490
elapsed time: 0:01:47.967343
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 17:37:06.910294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.47
 ---- batch: 020 ----
mean loss: 750.29
 ---- batch: 030 ----
mean loss: 737.67
 ---- batch: 040 ----
mean loss: 748.28
 ---- batch: 050 ----
mean loss: 751.52
 ---- batch: 060 ----
mean loss: 755.49
 ---- batch: 070 ----
mean loss: 762.90
 ---- batch: 080 ----
mean loss: 742.75
 ---- batch: 090 ----
mean loss: 734.98
train mean loss: 748.91
epoch train time: 0:00:02.635105
elapsed time: 0:01:50.603012
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 17:37:09.545984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.05
 ---- batch: 020 ----
mean loss: 748.72
 ---- batch: 030 ----
mean loss: 734.60
 ---- batch: 040 ----
mean loss: 737.83
 ---- batch: 050 ----
mean loss: 741.47
 ---- batch: 060 ----
mean loss: 747.35
 ---- batch: 070 ----
mean loss: 737.65
 ---- batch: 080 ----
mean loss: 734.10
 ---- batch: 090 ----
mean loss: 711.42
train mean loss: 738.25
epoch train time: 0:00:02.640648
elapsed time: 0:01:53.244145
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 17:37:12.187157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 722.20
 ---- batch: 020 ----
mean loss: 731.48
 ---- batch: 030 ----
mean loss: 725.51
 ---- batch: 040 ----
mean loss: 732.24
 ---- batch: 050 ----
mean loss: 746.15
 ---- batch: 060 ----
mean loss: 720.46
 ---- batch: 070 ----
mean loss: 740.75
 ---- batch: 080 ----
mean loss: 719.32
 ---- batch: 090 ----
mean loss: 710.79
train mean loss: 727.33
epoch train time: 0:00:02.630330
elapsed time: 0:01:55.875035
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 17:37:14.818019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.17
 ---- batch: 020 ----
mean loss: 723.57
 ---- batch: 030 ----
mean loss: 721.70
 ---- batch: 040 ----
mean loss: 711.97
 ---- batch: 050 ----
mean loss: 710.81
 ---- batch: 060 ----
mean loss: 725.75
 ---- batch: 070 ----
mean loss: 700.36
 ---- batch: 080 ----
mean loss: 730.34
 ---- batch: 090 ----
mean loss: 708.71
train mean loss: 716.76
epoch train time: 0:00:02.635775
elapsed time: 0:01:58.511296
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 17:37:17.454261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 709.95
 ---- batch: 020 ----
mean loss: 712.72
 ---- batch: 030 ----
mean loss: 707.75
 ---- batch: 040 ----
mean loss: 705.59
 ---- batch: 050 ----
mean loss: 694.25
 ---- batch: 060 ----
mean loss: 699.99
 ---- batch: 070 ----
mean loss: 700.32
 ---- batch: 080 ----
mean loss: 702.88
 ---- batch: 090 ----
mean loss: 699.35
train mean loss: 704.71
epoch train time: 0:00:02.628329
elapsed time: 0:02:01.140089
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 17:37:20.083049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 702.10
 ---- batch: 020 ----
mean loss: 705.24
 ---- batch: 030 ----
mean loss: 699.58
 ---- batch: 040 ----
mean loss: 696.07
 ---- batch: 050 ----
mean loss: 703.58
 ---- batch: 060 ----
mean loss: 689.31
 ---- batch: 070 ----
mean loss: 691.28
 ---- batch: 080 ----
mean loss: 677.76
 ---- batch: 090 ----
mean loss: 687.08
train mean loss: 694.34
epoch train time: 0:00:02.653484
elapsed time: 0:02:03.794042
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 17:37:22.737024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.53
 ---- batch: 020 ----
mean loss: 688.02
 ---- batch: 030 ----
mean loss: 677.23
 ---- batch: 040 ----
mean loss: 684.91
 ---- batch: 050 ----
mean loss: 680.48
 ---- batch: 060 ----
mean loss: 688.84
 ---- batch: 070 ----
mean loss: 683.95
 ---- batch: 080 ----
mean loss: 677.51
 ---- batch: 090 ----
mean loss: 689.09
train mean loss: 683.98
epoch train time: 0:00:02.649436
elapsed time: 0:02:06.443944
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 17:37:25.386909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.79
 ---- batch: 020 ----
mean loss: 673.99
 ---- batch: 030 ----
mean loss: 679.88
 ---- batch: 040 ----
mean loss: 681.97
 ---- batch: 050 ----
mean loss: 681.23
 ---- batch: 060 ----
mean loss: 652.32
 ---- batch: 070 ----
mean loss: 661.02
 ---- batch: 080 ----
mean loss: 660.55
 ---- batch: 090 ----
mean loss: 688.69
train mean loss: 672.27
epoch train time: 0:00:02.662846
elapsed time: 0:02:09.107313
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 17:37:28.050280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.70
 ---- batch: 020 ----
mean loss: 665.00
 ---- batch: 030 ----
mean loss: 662.41
 ---- batch: 040 ----
mean loss: 676.03
 ---- batch: 050 ----
mean loss: 671.26
 ---- batch: 060 ----
mean loss: 657.24
 ---- batch: 070 ----
mean loss: 658.85
 ---- batch: 080 ----
mean loss: 638.13
 ---- batch: 090 ----
mean loss: 647.99
train mean loss: 661.47
epoch train time: 0:00:02.629694
elapsed time: 0:02:11.737471
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 17:37:30.680434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 645.54
 ---- batch: 020 ----
mean loss: 658.94
 ---- batch: 030 ----
mean loss: 649.32
 ---- batch: 040 ----
mean loss: 659.59
 ---- batch: 050 ----
mean loss: 646.94
 ---- batch: 060 ----
mean loss: 650.37
 ---- batch: 070 ----
mean loss: 648.53
 ---- batch: 080 ----
mean loss: 651.93
 ---- batch: 090 ----
mean loss: 633.68
train mean loss: 650.07
epoch train time: 0:00:02.646021
elapsed time: 0:02:14.383962
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 17:37:33.326932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 645.99
 ---- batch: 020 ----
mean loss: 642.17
 ---- batch: 030 ----
mean loss: 637.13
 ---- batch: 040 ----
mean loss: 643.20
 ---- batch: 050 ----
mean loss: 632.40
 ---- batch: 060 ----
mean loss: 644.00
 ---- batch: 070 ----
mean loss: 647.94
 ---- batch: 080 ----
mean loss: 634.54
 ---- batch: 090 ----
mean loss: 639.14
train mean loss: 640.71
epoch train time: 0:00:02.626759
elapsed time: 0:02:17.011275
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 17:37:35.954247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 638.90
 ---- batch: 020 ----
mean loss: 622.25
 ---- batch: 030 ----
mean loss: 624.74
 ---- batch: 040 ----
mean loss: 638.04
 ---- batch: 050 ----
mean loss: 631.14
 ---- batch: 060 ----
mean loss: 628.62
 ---- batch: 070 ----
mean loss: 623.00
 ---- batch: 080 ----
mean loss: 621.99
 ---- batch: 090 ----
mean loss: 631.70
train mean loss: 629.20
epoch train time: 0:00:02.632799
elapsed time: 0:02:19.644545
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 17:37:38.587525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.89
 ---- batch: 020 ----
mean loss: 627.52
 ---- batch: 030 ----
mean loss: 618.98
 ---- batch: 040 ----
mean loss: 615.06
 ---- batch: 050 ----
mean loss: 619.24
 ---- batch: 060 ----
mean loss: 621.17
 ---- batch: 070 ----
mean loss: 620.18
 ---- batch: 080 ----
mean loss: 613.76
 ---- batch: 090 ----
mean loss: 608.07
train mean loss: 619.22
epoch train time: 0:00:02.644810
elapsed time: 0:02:22.289816
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 17:37:41.232775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 607.02
 ---- batch: 020 ----
mean loss: 612.84
 ---- batch: 030 ----
mean loss: 613.13
 ---- batch: 040 ----
mean loss: 611.28
 ---- batch: 050 ----
mean loss: 600.89
 ---- batch: 060 ----
mean loss: 618.48
 ---- batch: 070 ----
mean loss: 605.99
 ---- batch: 080 ----
mean loss: 602.96
 ---- batch: 090 ----
mean loss: 613.37
train mean loss: 609.95
epoch train time: 0:00:02.669161
elapsed time: 0:02:24.959501
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 17:37:43.902482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.49
 ---- batch: 020 ----
mean loss: 592.98
 ---- batch: 030 ----
mean loss: 607.75
 ---- batch: 040 ----
mean loss: 582.96
 ---- batch: 050 ----
mean loss: 596.67
 ---- batch: 060 ----
mean loss: 602.41
 ---- batch: 070 ----
mean loss: 600.84
 ---- batch: 080 ----
mean loss: 611.28
 ---- batch: 090 ----
mean loss: 604.60
train mean loss: 601.42
epoch train time: 0:00:02.650597
elapsed time: 0:02:27.610595
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 17:37:46.553572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.73
 ---- batch: 020 ----
mean loss: 597.22
 ---- batch: 030 ----
mean loss: 600.92
 ---- batch: 040 ----
mean loss: 594.60
 ---- batch: 050 ----
mean loss: 585.75
 ---- batch: 060 ----
mean loss: 588.59
 ---- batch: 070 ----
mean loss: 586.49
 ---- batch: 080 ----
mean loss: 588.76
 ---- batch: 090 ----
mean loss: 591.82
train mean loss: 591.74
epoch train time: 0:00:02.654978
elapsed time: 0:02:30.266039
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 17:37:49.209020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 582.06
 ---- batch: 020 ----
mean loss: 581.01
 ---- batch: 030 ----
mean loss: 601.71
 ---- batch: 040 ----
mean loss: 578.40
 ---- batch: 050 ----
mean loss: 578.25
 ---- batch: 060 ----
mean loss: 584.47
 ---- batch: 070 ----
mean loss: 587.64
 ---- batch: 080 ----
mean loss: 571.40
 ---- batch: 090 ----
mean loss: 584.34
train mean loss: 583.19
epoch train time: 0:00:02.626627
elapsed time: 0:02:32.893174
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 17:37:51.836141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 579.76
 ---- batch: 020 ----
mean loss: 584.43
 ---- batch: 030 ----
mean loss: 577.64
 ---- batch: 040 ----
mean loss: 579.36
 ---- batch: 050 ----
mean loss: 573.21
 ---- batch: 060 ----
mean loss: 569.22
 ---- batch: 070 ----
mean loss: 577.30
 ---- batch: 080 ----
mean loss: 565.94
 ---- batch: 090 ----
mean loss: 571.01
train mean loss: 574.61
epoch train time: 0:00:02.604656
elapsed time: 0:02:35.498317
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 17:37:54.441297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 570.98
 ---- batch: 020 ----
mean loss: 567.60
 ---- batch: 030 ----
mean loss: 571.89
 ---- batch: 040 ----
mean loss: 561.91
 ---- batch: 050 ----
mean loss: 576.03
 ---- batch: 060 ----
mean loss: 570.48
 ---- batch: 070 ----
mean loss: 571.30
 ---- batch: 080 ----
mean loss: 550.97
 ---- batch: 090 ----
mean loss: 559.15
train mean loss: 566.41
epoch train time: 0:00:02.632122
elapsed time: 0:02:38.130971
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 17:37:57.073937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 560.20
 ---- batch: 020 ----
mean loss: 557.73
 ---- batch: 030 ----
mean loss: 559.34
 ---- batch: 040 ----
mean loss: 549.57
 ---- batch: 050 ----
mean loss: 577.38
 ---- batch: 060 ----
mean loss: 544.90
 ---- batch: 070 ----
mean loss: 560.06
 ---- batch: 080 ----
mean loss: 550.42
 ---- batch: 090 ----
mean loss: 557.67
train mean loss: 557.47
epoch train time: 0:00:02.633602
elapsed time: 0:02:40.765106
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 17:37:59.708150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 550.11
 ---- batch: 020 ----
mean loss: 542.91
 ---- batch: 030 ----
mean loss: 543.45
 ---- batch: 040 ----
mean loss: 539.64
 ---- batch: 050 ----
mean loss: 554.65
 ---- batch: 060 ----
mean loss: 542.64
 ---- batch: 070 ----
mean loss: 557.09
 ---- batch: 080 ----
mean loss: 554.76
 ---- batch: 090 ----
mean loss: 553.93
train mean loss: 548.51
epoch train time: 0:00:02.647965
elapsed time: 0:02:43.413640
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 17:38:02.356628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 544.15
 ---- batch: 020 ----
mean loss: 532.21
 ---- batch: 030 ----
mean loss: 538.57
 ---- batch: 040 ----
mean loss: 543.76
 ---- batch: 050 ----
mean loss: 520.25
 ---- batch: 060 ----
mean loss: 539.71
 ---- batch: 070 ----
mean loss: 521.71
 ---- batch: 080 ----
mean loss: 529.61
 ---- batch: 090 ----
mean loss: 522.09
train mean loss: 531.67
epoch train time: 0:00:02.650027
elapsed time: 0:02:46.064283
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 17:38:05.007321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.28
 ---- batch: 020 ----
mean loss: 518.75
 ---- batch: 030 ----
mean loss: 519.08
 ---- batch: 040 ----
mean loss: 509.73
 ---- batch: 050 ----
mean loss: 501.55
 ---- batch: 060 ----
mean loss: 506.07
 ---- batch: 070 ----
mean loss: 508.28
 ---- batch: 080 ----
mean loss: 499.78
 ---- batch: 090 ----
mean loss: 501.91
train mean loss: 508.57
epoch train time: 0:00:02.634864
elapsed time: 0:02:48.699665
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 17:38:07.642637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.31
 ---- batch: 020 ----
mean loss: 490.72
 ---- batch: 030 ----
mean loss: 495.77
 ---- batch: 040 ----
mean loss: 489.00
 ---- batch: 050 ----
mean loss: 474.17
 ---- batch: 060 ----
mean loss: 479.96
 ---- batch: 070 ----
mean loss: 485.03
 ---- batch: 080 ----
mean loss: 484.12
 ---- batch: 090 ----
mean loss: 486.76
train mean loss: 486.75
epoch train time: 0:00:02.632981
elapsed time: 0:02:51.333094
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 17:38:10.276056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.86
 ---- batch: 020 ----
mean loss: 471.59
 ---- batch: 030 ----
mean loss: 471.28
 ---- batch: 040 ----
mean loss: 455.19
 ---- batch: 050 ----
mean loss: 472.42
 ---- batch: 060 ----
mean loss: 467.83
 ---- batch: 070 ----
mean loss: 451.43
 ---- batch: 080 ----
mean loss: 459.20
 ---- batch: 090 ----
mean loss: 452.48
train mean loss: 462.87
epoch train time: 0:00:02.642387
elapsed time: 0:02:53.975943
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 17:38:12.918912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.47
 ---- batch: 020 ----
mean loss: 444.46
 ---- batch: 030 ----
mean loss: 460.07
 ---- batch: 040 ----
mean loss: 443.90
 ---- batch: 050 ----
mean loss: 440.61
 ---- batch: 060 ----
mean loss: 440.84
 ---- batch: 070 ----
mean loss: 430.51
 ---- batch: 080 ----
mean loss: 446.61
 ---- batch: 090 ----
mean loss: 418.25
train mean loss: 440.82
epoch train time: 0:00:02.647602
elapsed time: 0:02:56.624050
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 17:38:15.566974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.74
 ---- batch: 020 ----
mean loss: 418.67
 ---- batch: 030 ----
mean loss: 429.61
 ---- batch: 040 ----
mean loss: 413.36
 ---- batch: 050 ----
mean loss: 421.58
 ---- batch: 060 ----
mean loss: 423.42
 ---- batch: 070 ----
mean loss: 422.13
 ---- batch: 080 ----
mean loss: 403.72
 ---- batch: 090 ----
mean loss: 414.04
train mean loss: 419.72
epoch train time: 0:00:02.645075
elapsed time: 0:02:59.269520
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 17:38:18.212496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.83
 ---- batch: 020 ----
mean loss: 400.52
 ---- batch: 030 ----
mean loss: 404.23
 ---- batch: 040 ----
mean loss: 409.68
 ---- batch: 050 ----
mean loss: 404.97
 ---- batch: 060 ----
mean loss: 400.85
 ---- batch: 070 ----
mean loss: 407.97
 ---- batch: 080 ----
mean loss: 379.90
 ---- batch: 090 ----
mean loss: 390.93
train mean loss: 400.70
epoch train time: 0:00:02.652994
elapsed time: 0:03:01.923012
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 17:38:20.865982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.64
 ---- batch: 020 ----
mean loss: 381.85
 ---- batch: 030 ----
mean loss: 400.26
 ---- batch: 040 ----
mean loss: 377.87
 ---- batch: 050 ----
mean loss: 385.70
 ---- batch: 060 ----
mean loss: 380.36
 ---- batch: 070 ----
mean loss: 377.35
 ---- batch: 080 ----
mean loss: 378.09
 ---- batch: 090 ----
mean loss: 373.32
train mean loss: 382.85
epoch train time: 0:00:02.652298
elapsed time: 0:03:04.575814
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 17:38:23.518810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.49
 ---- batch: 020 ----
mean loss: 379.94
 ---- batch: 030 ----
mean loss: 380.06
 ---- batch: 040 ----
mean loss: 362.93
 ---- batch: 050 ----
mean loss: 379.15
 ---- batch: 060 ----
mean loss: 370.00
 ---- batch: 070 ----
mean loss: 358.66
 ---- batch: 080 ----
mean loss: 360.13
 ---- batch: 090 ----
mean loss: 360.28
train mean loss: 368.15
epoch train time: 0:00:02.640215
elapsed time: 0:03:07.216555
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 17:38:26.159518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.65
 ---- batch: 020 ----
mean loss: 351.50
 ---- batch: 030 ----
mean loss: 361.53
 ---- batch: 040 ----
mean loss: 354.08
 ---- batch: 050 ----
mean loss: 351.39
 ---- batch: 060 ----
mean loss: 349.61
 ---- batch: 070 ----
mean loss: 356.80
 ---- batch: 080 ----
mean loss: 356.52
 ---- batch: 090 ----
mean loss: 358.66
train mean loss: 355.27
epoch train time: 0:00:02.651424
elapsed time: 0:03:09.868446
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 17:38:28.811405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.49
 ---- batch: 020 ----
mean loss: 346.32
 ---- batch: 030 ----
mean loss: 341.99
 ---- batch: 040 ----
mean loss: 345.45
 ---- batch: 050 ----
mean loss: 345.09
 ---- batch: 060 ----
mean loss: 349.30
 ---- batch: 070 ----
mean loss: 341.91
 ---- batch: 080 ----
mean loss: 341.46
 ---- batch: 090 ----
mean loss: 342.08
train mean loss: 344.63
epoch train time: 0:00:02.661733
elapsed time: 0:03:12.530665
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 17:38:31.473668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.23
 ---- batch: 020 ----
mean loss: 331.86
 ---- batch: 030 ----
mean loss: 326.61
 ---- batch: 040 ----
mean loss: 339.61
 ---- batch: 050 ----
mean loss: 333.42
 ---- batch: 060 ----
mean loss: 330.57
 ---- batch: 070 ----
mean loss: 342.74
 ---- batch: 080 ----
mean loss: 340.56
 ---- batch: 090 ----
mean loss: 336.35
train mean loss: 334.53
epoch train time: 0:00:02.638633
elapsed time: 0:03:15.169814
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 17:38:34.112778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.70
 ---- batch: 020 ----
mean loss: 324.95
 ---- batch: 030 ----
mean loss: 329.00
 ---- batch: 040 ----
mean loss: 316.41
 ---- batch: 050 ----
mean loss: 325.01
 ---- batch: 060 ----
mean loss: 336.59
 ---- batch: 070 ----
mean loss: 325.31
 ---- batch: 080 ----
mean loss: 326.11
 ---- batch: 090 ----
mean loss: 339.49
train mean loss: 327.83
epoch train time: 0:00:02.640341
elapsed time: 0:03:17.810674
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 17:38:36.753673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.19
 ---- batch: 020 ----
mean loss: 312.96
 ---- batch: 030 ----
mean loss: 325.17
 ---- batch: 040 ----
mean loss: 314.37
 ---- batch: 050 ----
mean loss: 323.79
 ---- batch: 060 ----
mean loss: 325.64
 ---- batch: 070 ----
mean loss: 324.06
 ---- batch: 080 ----
mean loss: 320.61
 ---- batch: 090 ----
mean loss: 318.52
train mean loss: 321.10
epoch train time: 0:00:02.655401
elapsed time: 0:03:20.466550
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 17:38:39.409520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.85
 ---- batch: 020 ----
mean loss: 317.19
 ---- batch: 030 ----
mean loss: 316.93
 ---- batch: 040 ----
mean loss: 314.84
 ---- batch: 050 ----
mean loss: 327.77
 ---- batch: 060 ----
mean loss: 310.15
 ---- batch: 070 ----
mean loss: 312.21
 ---- batch: 080 ----
mean loss: 314.93
 ---- batch: 090 ----
mean loss: 309.04
train mean loss: 315.48
epoch train time: 0:00:02.655942
elapsed time: 0:03:23.122981
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 17:38:42.065966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.87
 ---- batch: 020 ----
mean loss: 316.50
 ---- batch: 030 ----
mean loss: 310.55
 ---- batch: 040 ----
mean loss: 316.15
 ---- batch: 050 ----
mean loss: 317.94
 ---- batch: 060 ----
mean loss: 307.98
 ---- batch: 070 ----
mean loss: 301.87
 ---- batch: 080 ----
mean loss: 309.51
 ---- batch: 090 ----
mean loss: 310.67
train mean loss: 311.72
epoch train time: 0:00:02.655412
elapsed time: 0:03:25.778918
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 17:38:44.721942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.17
 ---- batch: 020 ----
mean loss: 309.01
 ---- batch: 030 ----
mean loss: 312.87
 ---- batch: 040 ----
mean loss: 299.97
 ---- batch: 050 ----
mean loss: 304.14
 ---- batch: 060 ----
mean loss: 313.27
 ---- batch: 070 ----
mean loss: 303.58
 ---- batch: 080 ----
mean loss: 306.11
 ---- batch: 090 ----
mean loss: 309.81
train mean loss: 306.57
epoch train time: 0:00:02.643568
elapsed time: 0:03:28.423008
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 17:38:47.365981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.16
 ---- batch: 020 ----
mean loss: 298.59
 ---- batch: 030 ----
mean loss: 313.12
 ---- batch: 040 ----
mean loss: 311.01
 ---- batch: 050 ----
mean loss: 306.61
 ---- batch: 060 ----
mean loss: 302.01
 ---- batch: 070 ----
mean loss: 297.56
 ---- batch: 080 ----
mean loss: 298.43
 ---- batch: 090 ----
mean loss: 300.24
train mean loss: 303.08
epoch train time: 0:00:02.673334
elapsed time: 0:03:31.096824
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 17:38:50.039783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.80
 ---- batch: 020 ----
mean loss: 298.66
 ---- batch: 030 ----
mean loss: 293.80
 ---- batch: 040 ----
mean loss: 292.05
 ---- batch: 050 ----
mean loss: 301.19
 ---- batch: 060 ----
mean loss: 304.91
 ---- batch: 070 ----
mean loss: 311.33
 ---- batch: 080 ----
mean loss: 305.94
 ---- batch: 090 ----
mean loss: 291.51
train mean loss: 299.88
epoch train time: 0:00:02.655981
elapsed time: 0:03:33.753292
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 17:38:52.696253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.64
 ---- batch: 020 ----
mean loss: 302.00
 ---- batch: 030 ----
mean loss: 298.14
 ---- batch: 040 ----
mean loss: 293.52
 ---- batch: 050 ----
mean loss: 294.71
 ---- batch: 060 ----
mean loss: 294.79
 ---- batch: 070 ----
mean loss: 303.00
 ---- batch: 080 ----
mean loss: 291.04
 ---- batch: 090 ----
mean loss: 297.22
train mean loss: 297.39
epoch train time: 0:00:02.644042
elapsed time: 0:03:36.397839
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 17:38:55.340801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.26
 ---- batch: 020 ----
mean loss: 288.94
 ---- batch: 030 ----
mean loss: 293.01
 ---- batch: 040 ----
mean loss: 298.33
 ---- batch: 050 ----
mean loss: 289.49
 ---- batch: 060 ----
mean loss: 285.27
 ---- batch: 070 ----
mean loss: 284.40
 ---- batch: 080 ----
mean loss: 307.22
 ---- batch: 090 ----
mean loss: 302.53
train mean loss: 295.06
epoch train time: 0:00:02.660441
elapsed time: 0:03:39.058724
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 17:38:58.001708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.49
 ---- batch: 020 ----
mean loss: 291.25
 ---- batch: 030 ----
mean loss: 297.56
 ---- batch: 040 ----
mean loss: 300.87
 ---- batch: 050 ----
mean loss: 296.36
 ---- batch: 060 ----
mean loss: 289.62
 ---- batch: 070 ----
mean loss: 292.70
 ---- batch: 080 ----
mean loss: 286.67
 ---- batch: 090 ----
mean loss: 294.81
train mean loss: 292.48
epoch train time: 0:00:02.628943
elapsed time: 0:03:41.688136
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 17:39:00.631103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.65
 ---- batch: 020 ----
mean loss: 285.22
 ---- batch: 030 ----
mean loss: 298.88
 ---- batch: 040 ----
mean loss: 289.80
 ---- batch: 050 ----
mean loss: 297.09
 ---- batch: 060 ----
mean loss: 298.17
 ---- batch: 070 ----
mean loss: 280.56
 ---- batch: 080 ----
mean loss: 287.93
 ---- batch: 090 ----
mean loss: 288.66
train mean loss: 290.31
epoch train time: 0:00:02.626306
elapsed time: 0:03:44.314907
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 17:39:03.257915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.36
 ---- batch: 020 ----
mean loss: 278.34
 ---- batch: 030 ----
mean loss: 283.43
 ---- batch: 040 ----
mean loss: 292.65
 ---- batch: 050 ----
mean loss: 295.39
 ---- batch: 060 ----
mean loss: 290.91
 ---- batch: 070 ----
mean loss: 288.06
 ---- batch: 080 ----
mean loss: 292.74
 ---- batch: 090 ----
mean loss: 288.75
train mean loss: 288.58
epoch train time: 0:00:02.665293
elapsed time: 0:03:46.980796
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 17:39:05.923790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.02
 ---- batch: 020 ----
mean loss: 286.08
 ---- batch: 030 ----
mean loss: 296.69
 ---- batch: 040 ----
mean loss: 281.84
 ---- batch: 050 ----
mean loss: 282.83
 ---- batch: 060 ----
mean loss: 287.12
 ---- batch: 070 ----
mean loss: 287.54
 ---- batch: 080 ----
mean loss: 292.50
 ---- batch: 090 ----
mean loss: 284.54
train mean loss: 286.28
epoch train time: 0:00:02.663816
elapsed time: 0:03:49.645076
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 17:39:08.588034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.39
 ---- batch: 020 ----
mean loss: 287.83
 ---- batch: 030 ----
mean loss: 293.16
 ---- batch: 040 ----
mean loss: 280.66
 ---- batch: 050 ----
mean loss: 281.98
 ---- batch: 060 ----
mean loss: 286.36
 ---- batch: 070 ----
mean loss: 286.07
 ---- batch: 080 ----
mean loss: 283.21
 ---- batch: 090 ----
mean loss: 282.12
train mean loss: 284.88
epoch train time: 0:00:02.648376
elapsed time: 0:03:52.293885
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 17:39:11.236854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.24
 ---- batch: 020 ----
mean loss: 286.12
 ---- batch: 030 ----
mean loss: 283.17
 ---- batch: 040 ----
mean loss: 286.98
 ---- batch: 050 ----
mean loss: 277.20
 ---- batch: 060 ----
mean loss: 286.19
 ---- batch: 070 ----
mean loss: 289.24
 ---- batch: 080 ----
mean loss: 284.44
 ---- batch: 090 ----
mean loss: 287.87
train mean loss: 283.44
epoch train time: 0:00:02.641778
elapsed time: 0:03:54.936178
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 17:39:13.879248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.30
 ---- batch: 020 ----
mean loss: 280.91
 ---- batch: 030 ----
mean loss: 262.93
 ---- batch: 040 ----
mean loss: 283.89
 ---- batch: 050 ----
mean loss: 291.83
 ---- batch: 060 ----
mean loss: 273.55
 ---- batch: 070 ----
mean loss: 282.80
 ---- batch: 080 ----
mean loss: 289.85
 ---- batch: 090 ----
mean loss: 279.16
train mean loss: 281.66
epoch train time: 0:00:02.626728
elapsed time: 0:03:57.563462
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 17:39:16.506423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.01
 ---- batch: 020 ----
mean loss: 283.56
 ---- batch: 030 ----
mean loss: 273.28
 ---- batch: 040 ----
mean loss: 292.13
 ---- batch: 050 ----
mean loss: 275.56
 ---- batch: 060 ----
mean loss: 279.88
 ---- batch: 070 ----
mean loss: 279.50
 ---- batch: 080 ----
mean loss: 282.03
 ---- batch: 090 ----
mean loss: 279.11
train mean loss: 280.10
epoch train time: 0:00:02.629733
elapsed time: 0:04:00.193675
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 17:39:19.136636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.08
 ---- batch: 020 ----
mean loss: 272.43
 ---- batch: 030 ----
mean loss: 275.18
 ---- batch: 040 ----
mean loss: 291.11
 ---- batch: 050 ----
mean loss: 282.43
 ---- batch: 060 ----
mean loss: 272.23
 ---- batch: 070 ----
mean loss: 292.72
 ---- batch: 080 ----
mean loss: 273.87
 ---- batch: 090 ----
mean loss: 282.56
train mean loss: 278.99
epoch train time: 0:00:02.654267
elapsed time: 0:04:02.848418
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 17:39:21.791383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.30
 ---- batch: 020 ----
mean loss: 276.89
 ---- batch: 030 ----
mean loss: 279.42
 ---- batch: 040 ----
mean loss: 285.25
 ---- batch: 050 ----
mean loss: 271.14
 ---- batch: 060 ----
mean loss: 289.06
 ---- batch: 070 ----
mean loss: 265.57
 ---- batch: 080 ----
mean loss: 275.43
 ---- batch: 090 ----
mean loss: 279.70
train mean loss: 277.45
epoch train time: 0:00:02.653965
elapsed time: 0:04:05.502866
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 17:39:24.445855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.49
 ---- batch: 020 ----
mean loss: 273.00
 ---- batch: 030 ----
mean loss: 276.70
 ---- batch: 040 ----
mean loss: 274.76
 ---- batch: 050 ----
mean loss: 277.52
 ---- batch: 060 ----
mean loss: 273.66
 ---- batch: 070 ----
mean loss: 275.51
 ---- batch: 080 ----
mean loss: 278.79
 ---- batch: 090 ----
mean loss: 276.44
train mean loss: 276.85
epoch train time: 0:00:02.650725
elapsed time: 0:04:08.154091
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 17:39:27.097064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.48
 ---- batch: 020 ----
mean loss: 282.64
 ---- batch: 030 ----
mean loss: 283.11
 ---- batch: 040 ----
mean loss: 271.48
 ---- batch: 050 ----
mean loss: 285.04
 ---- batch: 060 ----
mean loss: 271.40
 ---- batch: 070 ----
mean loss: 272.94
 ---- batch: 080 ----
mean loss: 263.86
 ---- batch: 090 ----
mean loss: 271.89
train mean loss: 274.57
epoch train time: 0:00:02.634590
elapsed time: 0:04:10.789223
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 17:39:29.732188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.05
 ---- batch: 020 ----
mean loss: 271.03
 ---- batch: 030 ----
mean loss: 271.26
 ---- batch: 040 ----
mean loss: 271.29
 ---- batch: 050 ----
mean loss: 274.04
 ---- batch: 060 ----
mean loss: 281.60
 ---- batch: 070 ----
mean loss: 267.58
 ---- batch: 080 ----
mean loss: 282.73
 ---- batch: 090 ----
mean loss: 273.82
train mean loss: 273.94
epoch train time: 0:00:02.632035
elapsed time: 0:04:13.421747
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 17:39:32.364709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.00
 ---- batch: 020 ----
mean loss: 264.46
 ---- batch: 030 ----
mean loss: 274.64
 ---- batch: 040 ----
mean loss: 271.11
 ---- batch: 050 ----
mean loss: 276.23
 ---- batch: 060 ----
mean loss: 275.50
 ---- batch: 070 ----
mean loss: 270.43
 ---- batch: 080 ----
mean loss: 269.69
 ---- batch: 090 ----
mean loss: 273.24
train mean loss: 272.79
epoch train time: 0:00:02.631960
elapsed time: 0:04:16.054248
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 17:39:34.997225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.65
 ---- batch: 020 ----
mean loss: 267.92
 ---- batch: 030 ----
mean loss: 270.53
 ---- batch: 040 ----
mean loss: 259.95
 ---- batch: 050 ----
mean loss: 277.54
 ---- batch: 060 ----
mean loss: 275.69
 ---- batch: 070 ----
mean loss: 268.92
 ---- batch: 080 ----
mean loss: 271.65
 ---- batch: 090 ----
mean loss: 280.05
train mean loss: 271.48
epoch train time: 0:00:02.634856
elapsed time: 0:04:18.689578
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 17:39:37.632547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.36
 ---- batch: 020 ----
mean loss: 280.28
 ---- batch: 030 ----
mean loss: 264.77
 ---- batch: 040 ----
mean loss: 276.30
 ---- batch: 050 ----
mean loss: 268.84
 ---- batch: 060 ----
mean loss: 260.97
 ---- batch: 070 ----
mean loss: 257.14
 ---- batch: 080 ----
mean loss: 265.14
 ---- batch: 090 ----
mean loss: 281.99
train mean loss: 270.58
epoch train time: 0:00:02.647542
elapsed time: 0:04:21.337574
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 17:39:40.280549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.13
 ---- batch: 020 ----
mean loss: 269.13
 ---- batch: 030 ----
mean loss: 269.75
 ---- batch: 040 ----
mean loss: 273.96
 ---- batch: 050 ----
mean loss: 269.07
 ---- batch: 060 ----
mean loss: 269.47
 ---- batch: 070 ----
mean loss: 267.23
 ---- batch: 080 ----
mean loss: 271.48
 ---- batch: 090 ----
mean loss: 262.32
train mean loss: 269.82
epoch train time: 0:00:02.641497
elapsed time: 0:04:23.979579
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 17:39:42.922577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.29
 ---- batch: 020 ----
mean loss: 272.27
 ---- batch: 030 ----
mean loss: 273.40
 ---- batch: 040 ----
mean loss: 274.20
 ---- batch: 050 ----
mean loss: 257.73
 ---- batch: 060 ----
mean loss: 275.25
 ---- batch: 070 ----
mean loss: 263.48
 ---- batch: 080 ----
mean loss: 274.74
 ---- batch: 090 ----
mean loss: 266.05
train mean loss: 268.72
epoch train time: 0:00:02.635187
elapsed time: 0:04:26.615274
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 17:39:45.558281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.66
 ---- batch: 020 ----
mean loss: 267.39
 ---- batch: 030 ----
mean loss: 267.92
 ---- batch: 040 ----
mean loss: 266.46
 ---- batch: 050 ----
mean loss: 264.34
 ---- batch: 060 ----
mean loss: 273.71
 ---- batch: 070 ----
mean loss: 262.04
 ---- batch: 080 ----
mean loss: 266.14
 ---- batch: 090 ----
mean loss: 265.19
train mean loss: 267.75
epoch train time: 0:00:02.656158
elapsed time: 0:04:29.271952
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 17:39:48.214926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.23
 ---- batch: 020 ----
mean loss: 272.02
 ---- batch: 030 ----
mean loss: 269.73
 ---- batch: 040 ----
mean loss: 265.01
 ---- batch: 050 ----
mean loss: 268.77
 ---- batch: 060 ----
mean loss: 261.78
 ---- batch: 070 ----
mean loss: 263.39
 ---- batch: 080 ----
mean loss: 266.71
 ---- batch: 090 ----
mean loss: 271.88
train mean loss: 267.24
epoch train time: 0:00:02.643505
elapsed time: 0:04:31.915955
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 17:39:50.858944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.56
 ---- batch: 020 ----
mean loss: 265.05
 ---- batch: 030 ----
mean loss: 255.53
 ---- batch: 040 ----
mean loss: 266.40
 ---- batch: 050 ----
mean loss: 271.37
 ---- batch: 060 ----
mean loss: 279.56
 ---- batch: 070 ----
mean loss: 266.51
 ---- batch: 080 ----
mean loss: 264.44
 ---- batch: 090 ----
mean loss: 262.58
train mean loss: 266.06
epoch train time: 0:00:02.635021
elapsed time: 0:04:34.551477
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 17:39:53.494460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.93
 ---- batch: 020 ----
mean loss: 270.76
 ---- batch: 030 ----
mean loss: 263.90
 ---- batch: 040 ----
mean loss: 263.28
 ---- batch: 050 ----
mean loss: 272.04
 ---- batch: 060 ----
mean loss: 275.84
 ---- batch: 070 ----
mean loss: 263.35
 ---- batch: 080 ----
mean loss: 264.53
 ---- batch: 090 ----
mean loss: 258.98
train mean loss: 265.37
epoch train time: 0:00:02.654060
elapsed time: 0:04:37.206015
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 17:39:56.148979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.70
 ---- batch: 020 ----
mean loss: 269.76
 ---- batch: 030 ----
mean loss: 262.05
 ---- batch: 040 ----
mean loss: 264.61
 ---- batch: 050 ----
mean loss: 263.14
 ---- batch: 060 ----
mean loss: 264.52
 ---- batch: 070 ----
mean loss: 263.44
 ---- batch: 080 ----
mean loss: 264.50
 ---- batch: 090 ----
mean loss: 269.47
train mean loss: 264.80
epoch train time: 0:00:02.650595
elapsed time: 0:04:39.857145
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 17:39:58.800126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.84
 ---- batch: 020 ----
mean loss: 269.82
 ---- batch: 030 ----
mean loss: 266.86
 ---- batch: 040 ----
mean loss: 264.93
 ---- batch: 050 ----
mean loss: 261.77
 ---- batch: 060 ----
mean loss: 269.36
 ---- batch: 070 ----
mean loss: 267.89
 ---- batch: 080 ----
mean loss: 269.79
 ---- batch: 090 ----
mean loss: 255.25
train mean loss: 264.15
epoch train time: 0:00:02.638439
elapsed time: 0:04:42.496102
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 17:40:01.439080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.19
 ---- batch: 020 ----
mean loss: 261.50
 ---- batch: 030 ----
mean loss: 272.34
 ---- batch: 040 ----
mean loss: 255.02
 ---- batch: 050 ----
mean loss: 257.46
 ---- batch: 060 ----
mean loss: 269.21
 ---- batch: 070 ----
mean loss: 268.12
 ---- batch: 080 ----
mean loss: 258.18
 ---- batch: 090 ----
mean loss: 270.78
train mean loss: 263.26
epoch train time: 0:00:02.658024
elapsed time: 0:04:45.154638
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 17:40:04.097641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.25
 ---- batch: 020 ----
mean loss: 254.97
 ---- batch: 030 ----
mean loss: 262.25
 ---- batch: 040 ----
mean loss: 267.30
 ---- batch: 050 ----
mean loss: 261.71
 ---- batch: 060 ----
mean loss: 262.60
 ---- batch: 070 ----
mean loss: 257.81
 ---- batch: 080 ----
mean loss: 260.28
 ---- batch: 090 ----
mean loss: 264.65
train mean loss: 262.09
epoch train time: 0:00:02.660496
elapsed time: 0:04:47.815662
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 17:40:06.758567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.35
 ---- batch: 020 ----
mean loss: 257.40
 ---- batch: 030 ----
mean loss: 264.87
 ---- batch: 040 ----
mean loss: 260.85
 ---- batch: 050 ----
mean loss: 260.12
 ---- batch: 060 ----
mean loss: 261.40
 ---- batch: 070 ----
mean loss: 266.79
 ---- batch: 080 ----
mean loss: 256.01
 ---- batch: 090 ----
mean loss: 266.15
train mean loss: 262.32
epoch train time: 0:00:02.625047
elapsed time: 0:04:50.441158
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 17:40:09.384129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.15
 ---- batch: 020 ----
mean loss: 256.75
 ---- batch: 030 ----
mean loss: 257.33
 ---- batch: 040 ----
mean loss: 264.68
 ---- batch: 050 ----
mean loss: 258.62
 ---- batch: 060 ----
mean loss: 256.18
 ---- batch: 070 ----
mean loss: 263.67
 ---- batch: 080 ----
mean loss: 265.31
 ---- batch: 090 ----
mean loss: 268.51
train mean loss: 261.13
epoch train time: 0:00:02.653105
elapsed time: 0:04:53.094722
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 17:40:12.037715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.39
 ---- batch: 020 ----
mean loss: 264.83
 ---- batch: 030 ----
mean loss: 256.44
 ---- batch: 040 ----
mean loss: 263.64
 ---- batch: 050 ----
mean loss: 254.96
 ---- batch: 060 ----
mean loss: 261.00
 ---- batch: 070 ----
mean loss: 267.14
 ---- batch: 080 ----
mean loss: 259.33
 ---- batch: 090 ----
mean loss: 257.49
train mean loss: 260.59
epoch train time: 0:00:02.639965
elapsed time: 0:04:55.735230
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 17:40:14.678247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.52
 ---- batch: 020 ----
mean loss: 252.06
 ---- batch: 030 ----
mean loss: 257.19
 ---- batch: 040 ----
mean loss: 258.63
 ---- batch: 050 ----
mean loss: 262.32
 ---- batch: 060 ----
mean loss: 262.75
 ---- batch: 070 ----
mean loss: 258.69
 ---- batch: 080 ----
mean loss: 260.34
 ---- batch: 090 ----
mean loss: 261.33
train mean loss: 259.95
epoch train time: 0:00:02.656262
elapsed time: 0:04:58.391995
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 17:40:17.334958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.43
 ---- batch: 020 ----
mean loss: 255.93
 ---- batch: 030 ----
mean loss: 256.48
 ---- batch: 040 ----
mean loss: 256.51
 ---- batch: 050 ----
mean loss: 270.03
 ---- batch: 060 ----
mean loss: 254.61
 ---- batch: 070 ----
mean loss: 249.15
 ---- batch: 080 ----
mean loss: 259.68
 ---- batch: 090 ----
mean loss: 260.63
train mean loss: 259.33
epoch train time: 0:00:02.655244
elapsed time: 0:05:01.047722
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 17:40:19.990690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.03
 ---- batch: 020 ----
mean loss: 262.01
 ---- batch: 030 ----
mean loss: 254.74
 ---- batch: 040 ----
mean loss: 259.86
 ---- batch: 050 ----
mean loss: 266.92
 ---- batch: 060 ----
mean loss: 262.88
 ---- batch: 070 ----
mean loss: 255.53
 ---- batch: 080 ----
mean loss: 252.73
 ---- batch: 090 ----
mean loss: 255.96
train mean loss: 258.67
epoch train time: 0:00:02.673057
elapsed time: 0:05:03.721265
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 17:40:22.664251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.61
 ---- batch: 020 ----
mean loss: 253.21
 ---- batch: 030 ----
mean loss: 255.53
 ---- batch: 040 ----
mean loss: 258.61
 ---- batch: 050 ----
mean loss: 255.44
 ---- batch: 060 ----
mean loss: 261.33
 ---- batch: 070 ----
mean loss: 255.20
 ---- batch: 080 ----
mean loss: 258.87
 ---- batch: 090 ----
mean loss: 263.13
train mean loss: 258.00
epoch train time: 0:00:02.666983
elapsed time: 0:05:06.388720
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 17:40:25.331706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.44
 ---- batch: 020 ----
mean loss: 263.22
 ---- batch: 030 ----
mean loss: 250.59
 ---- batch: 040 ----
mean loss: 257.80
 ---- batch: 050 ----
mean loss: 251.21
 ---- batch: 060 ----
mean loss: 260.60
 ---- batch: 070 ----
mean loss: 262.07
 ---- batch: 080 ----
mean loss: 253.10
 ---- batch: 090 ----
mean loss: 257.71
train mean loss: 257.90
epoch train time: 0:00:02.674650
elapsed time: 0:05:09.063861
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 17:40:28.006830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.78
 ---- batch: 020 ----
mean loss: 259.53
 ---- batch: 030 ----
mean loss: 254.94
 ---- batch: 040 ----
mean loss: 254.45
 ---- batch: 050 ----
mean loss: 256.57
 ---- batch: 060 ----
mean loss: 263.00
 ---- batch: 070 ----
mean loss: 249.62
 ---- batch: 080 ----
mean loss: 258.06
 ---- batch: 090 ----
mean loss: 253.67
train mean loss: 257.35
epoch train time: 0:00:02.675087
elapsed time: 0:05:11.739442
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 17:40:30.682412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.62
 ---- batch: 020 ----
mean loss: 254.41
 ---- batch: 030 ----
mean loss: 264.19
 ---- batch: 040 ----
mean loss: 255.75
 ---- batch: 050 ----
mean loss: 247.74
 ---- batch: 060 ----
mean loss: 260.66
 ---- batch: 070 ----
mean loss: 254.54
 ---- batch: 080 ----
mean loss: 256.24
 ---- batch: 090 ----
mean loss: 261.83
train mean loss: 256.70
epoch train time: 0:00:02.668364
elapsed time: 0:05:14.408295
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 17:40:33.351272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.32
 ---- batch: 020 ----
mean loss: 260.52
 ---- batch: 030 ----
mean loss: 258.71
 ---- batch: 040 ----
mean loss: 245.35
 ---- batch: 050 ----
mean loss: 259.81
 ---- batch: 060 ----
mean loss: 257.60
 ---- batch: 070 ----
mean loss: 252.06
 ---- batch: 080 ----
mean loss: 255.60
 ---- batch: 090 ----
mean loss: 254.95
train mean loss: 256.16
epoch train time: 0:00:02.662001
elapsed time: 0:05:17.070892
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 17:40:36.013816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.54
 ---- batch: 020 ----
mean loss: 257.31
 ---- batch: 030 ----
mean loss: 254.49
 ---- batch: 040 ----
mean loss: 257.62
 ---- batch: 050 ----
mean loss: 252.67
 ---- batch: 060 ----
mean loss: 260.82
 ---- batch: 070 ----
mean loss: 257.65
 ---- batch: 080 ----
mean loss: 260.32
 ---- batch: 090 ----
mean loss: 251.77
train mean loss: 255.23
epoch train time: 0:00:02.653351
elapsed time: 0:05:19.724675
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 17:40:38.667644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.94
 ---- batch: 020 ----
mean loss: 251.85
 ---- batch: 030 ----
mean loss: 254.26
 ---- batch: 040 ----
mean loss: 252.82
 ---- batch: 050 ----
mean loss: 246.66
 ---- batch: 060 ----
mean loss: 259.01
 ---- batch: 070 ----
mean loss: 255.66
 ---- batch: 080 ----
mean loss: 254.28
 ---- batch: 090 ----
mean loss: 256.48
train mean loss: 254.85
epoch train time: 0:00:02.624437
elapsed time: 0:05:22.349591
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 17:40:41.292650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.79
 ---- batch: 020 ----
mean loss: 252.55
 ---- batch: 030 ----
mean loss: 255.30
 ---- batch: 040 ----
mean loss: 249.75
 ---- batch: 050 ----
mean loss: 253.90
 ---- batch: 060 ----
mean loss: 259.96
 ---- batch: 070 ----
mean loss: 259.00
 ---- batch: 080 ----
mean loss: 247.27
 ---- batch: 090 ----
mean loss: 251.04
train mean loss: 255.01
epoch train time: 0:00:02.648076
elapsed time: 0:05:24.998225
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 17:40:43.941219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.33
 ---- batch: 020 ----
mean loss: 255.64
 ---- batch: 030 ----
mean loss: 260.57
 ---- batch: 040 ----
mean loss: 250.59
 ---- batch: 050 ----
mean loss: 251.36
 ---- batch: 060 ----
mean loss: 259.59
 ---- batch: 070 ----
mean loss: 251.72
 ---- batch: 080 ----
mean loss: 249.99
 ---- batch: 090 ----
mean loss: 252.86
train mean loss: 254.06
epoch train time: 0:00:02.653480
elapsed time: 0:05:27.652185
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 17:40:46.595164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.71
 ---- batch: 020 ----
mean loss: 253.57
 ---- batch: 030 ----
mean loss: 257.75
 ---- batch: 040 ----
mean loss: 260.03
 ---- batch: 050 ----
mean loss: 255.84
 ---- batch: 060 ----
mean loss: 260.32
 ---- batch: 070 ----
mean loss: 259.81
 ---- batch: 080 ----
mean loss: 251.10
 ---- batch: 090 ----
mean loss: 243.39
train mean loss: 253.35
epoch train time: 0:00:02.666520
elapsed time: 0:05:30.319264
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 17:40:49.262242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.72
 ---- batch: 020 ----
mean loss: 254.40
 ---- batch: 030 ----
mean loss: 246.61
 ---- batch: 040 ----
mean loss: 247.45
 ---- batch: 050 ----
mean loss: 246.91
 ---- batch: 060 ----
mean loss: 259.55
 ---- batch: 070 ----
mean loss: 268.42
 ---- batch: 080 ----
mean loss: 253.83
 ---- batch: 090 ----
mean loss: 253.51
train mean loss: 253.94
epoch train time: 0:00:02.653710
elapsed time: 0:05:32.973475
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 17:40:51.916443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.50
 ---- batch: 020 ----
mean loss: 244.67
 ---- batch: 030 ----
mean loss: 251.90
 ---- batch: 040 ----
mean loss: 254.82
 ---- batch: 050 ----
mean loss: 250.78
 ---- batch: 060 ----
mean loss: 259.44
 ---- batch: 070 ----
mean loss: 242.89
 ---- batch: 080 ----
mean loss: 256.98
 ---- batch: 090 ----
mean loss: 257.16
train mean loss: 252.93
epoch train time: 0:00:02.649562
elapsed time: 0:05:35.623501
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 17:40:54.566536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.52
 ---- batch: 020 ----
mean loss: 253.79
 ---- batch: 030 ----
mean loss: 244.85
 ---- batch: 040 ----
mean loss: 255.08
 ---- batch: 050 ----
mean loss: 247.58
 ---- batch: 060 ----
mean loss: 253.35
 ---- batch: 070 ----
mean loss: 253.97
 ---- batch: 080 ----
mean loss: 247.51
 ---- batch: 090 ----
mean loss: 250.28
train mean loss: 252.00
epoch train time: 0:00:02.636546
elapsed time: 0:05:38.260648
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 17:40:57.203617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.18
 ---- batch: 020 ----
mean loss: 252.77
 ---- batch: 030 ----
mean loss: 246.22
 ---- batch: 040 ----
mean loss: 239.99
 ---- batch: 050 ----
mean loss: 262.78
 ---- batch: 060 ----
mean loss: 253.02
 ---- batch: 070 ----
mean loss: 263.19
 ---- batch: 080 ----
mean loss: 247.34
 ---- batch: 090 ----
mean loss: 246.08
train mean loss: 252.09
epoch train time: 0:00:02.679993
elapsed time: 0:05:40.941104
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 17:40:59.884090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.90
 ---- batch: 020 ----
mean loss: 240.94
 ---- batch: 030 ----
mean loss: 254.76
 ---- batch: 040 ----
mean loss: 251.39
 ---- batch: 050 ----
mean loss: 253.10
 ---- batch: 060 ----
mean loss: 257.38
 ---- batch: 070 ----
mean loss: 260.92
 ---- batch: 080 ----
mean loss: 260.71
 ---- batch: 090 ----
mean loss: 256.86
train mean loss: 251.17
epoch train time: 0:00:02.650318
elapsed time: 0:05:43.591933
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 17:41:02.534922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.58
 ---- batch: 020 ----
mean loss: 244.07
 ---- batch: 030 ----
mean loss: 250.24
 ---- batch: 040 ----
mean loss: 250.25
 ---- batch: 050 ----
mean loss: 250.63
 ---- batch: 060 ----
mean loss: 245.06
 ---- batch: 070 ----
mean loss: 256.70
 ---- batch: 080 ----
mean loss: 256.20
 ---- batch: 090 ----
mean loss: 257.92
train mean loss: 251.18
epoch train time: 0:00:02.654247
elapsed time: 0:05:46.246653
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 17:41:05.189633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.51
 ---- batch: 020 ----
mean loss: 254.04
 ---- batch: 030 ----
mean loss: 254.85
 ---- batch: 040 ----
mean loss: 256.66
 ---- batch: 050 ----
mean loss: 253.18
 ---- batch: 060 ----
mean loss: 253.76
 ---- batch: 070 ----
mean loss: 247.93
 ---- batch: 080 ----
mean loss: 253.00
 ---- batch: 090 ----
mean loss: 236.48
train mean loss: 251.14
epoch train time: 0:00:02.661510
elapsed time: 0:05:48.908643
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 17:41:07.851617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.54
 ---- batch: 020 ----
mean loss: 249.20
 ---- batch: 030 ----
mean loss: 245.41
 ---- batch: 040 ----
mean loss: 247.59
 ---- batch: 050 ----
mean loss: 241.18
 ---- batch: 060 ----
mean loss: 254.52
 ---- batch: 070 ----
mean loss: 251.33
 ---- batch: 080 ----
mean loss: 260.18
 ---- batch: 090 ----
mean loss: 248.34
train mean loss: 250.49
epoch train time: 0:00:02.641178
elapsed time: 0:05:51.550315
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 17:41:10.493286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.00
 ---- batch: 020 ----
mean loss: 249.08
 ---- batch: 030 ----
mean loss: 247.38
 ---- batch: 040 ----
mean loss: 255.84
 ---- batch: 050 ----
mean loss: 246.79
 ---- batch: 060 ----
mean loss: 253.08
 ---- batch: 070 ----
mean loss: 248.63
 ---- batch: 080 ----
mean loss: 252.66
 ---- batch: 090 ----
mean loss: 251.51
train mean loss: 249.85
epoch train time: 0:00:02.641916
elapsed time: 0:05:54.192700
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 17:41:13.135665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.72
 ---- batch: 020 ----
mean loss: 252.27
 ---- batch: 030 ----
mean loss: 246.76
 ---- batch: 040 ----
mean loss: 249.43
 ---- batch: 050 ----
mean loss: 249.09
 ---- batch: 060 ----
mean loss: 250.40
 ---- batch: 070 ----
mean loss: 251.25
 ---- batch: 080 ----
mean loss: 257.59
 ---- batch: 090 ----
mean loss: 251.69
train mean loss: 249.40
epoch train time: 0:00:02.646825
elapsed time: 0:05:56.839990
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 17:41:15.782945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.27
 ---- batch: 020 ----
mean loss: 251.08
 ---- batch: 030 ----
mean loss: 244.13
 ---- batch: 040 ----
mean loss: 256.09
 ---- batch: 050 ----
mean loss: 253.63
 ---- batch: 060 ----
mean loss: 247.70
 ---- batch: 070 ----
mean loss: 240.19
 ---- batch: 080 ----
mean loss: 250.61
 ---- batch: 090 ----
mean loss: 247.82
train mean loss: 249.40
epoch train time: 0:00:02.666078
elapsed time: 0:05:59.506547
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 17:41:18.449535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.99
 ---- batch: 020 ----
mean loss: 241.91
 ---- batch: 030 ----
mean loss: 245.37
 ---- batch: 040 ----
mean loss: 249.94
 ---- batch: 050 ----
mean loss: 248.67
 ---- batch: 060 ----
mean loss: 249.67
 ---- batch: 070 ----
mean loss: 250.07
 ---- batch: 080 ----
mean loss: 252.79
 ---- batch: 090 ----
mean loss: 247.99
train mean loss: 248.70
epoch train time: 0:00:02.650151
elapsed time: 0:06:02.157210
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 17:41:21.100170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.12
 ---- batch: 020 ----
mean loss: 242.66
 ---- batch: 030 ----
mean loss: 243.84
 ---- batch: 040 ----
mean loss: 251.59
 ---- batch: 050 ----
mean loss: 249.72
 ---- batch: 060 ----
mean loss: 257.52
 ---- batch: 070 ----
mean loss: 249.48
 ---- batch: 080 ----
mean loss: 249.22
 ---- batch: 090 ----
mean loss: 252.79
train mean loss: 248.51
epoch train time: 0:00:02.659655
elapsed time: 0:06:04.817377
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 17:41:23.760363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.66
 ---- batch: 020 ----
mean loss: 243.44
 ---- batch: 030 ----
mean loss: 236.31
 ---- batch: 040 ----
mean loss: 242.52
 ---- batch: 050 ----
mean loss: 251.25
 ---- batch: 060 ----
mean loss: 251.31
 ---- batch: 070 ----
mean loss: 248.08
 ---- batch: 080 ----
mean loss: 258.77
 ---- batch: 090 ----
mean loss: 254.37
train mean loss: 247.76
epoch train time: 0:00:02.630810
elapsed time: 0:06:07.448755
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 17:41:26.391766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.23
 ---- batch: 020 ----
mean loss: 244.48
 ---- batch: 030 ----
mean loss: 252.67
 ---- batch: 040 ----
mean loss: 251.19
 ---- batch: 050 ----
mean loss: 250.82
 ---- batch: 060 ----
mean loss: 242.58
 ---- batch: 070 ----
mean loss: 245.79
 ---- batch: 080 ----
mean loss: 251.80
 ---- batch: 090 ----
mean loss: 255.73
train mean loss: 247.75
epoch train time: 0:00:02.680249
elapsed time: 0:06:10.129589
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 17:41:29.072522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.77
 ---- batch: 020 ----
mean loss: 240.47
 ---- batch: 030 ----
mean loss: 243.94
 ---- batch: 040 ----
mean loss: 254.83
 ---- batch: 050 ----
mean loss: 248.79
 ---- batch: 060 ----
mean loss: 250.65
 ---- batch: 070 ----
mean loss: 234.31
 ---- batch: 080 ----
mean loss: 248.84
 ---- batch: 090 ----
mean loss: 254.43
train mean loss: 247.57
epoch train time: 0:00:02.681420
elapsed time: 0:06:12.811469
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 17:41:31.754472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.58
 ---- batch: 020 ----
mean loss: 236.07
 ---- batch: 030 ----
mean loss: 244.86
 ---- batch: 040 ----
mean loss: 244.90
 ---- batch: 050 ----
mean loss: 244.21
 ---- batch: 060 ----
mean loss: 246.25
 ---- batch: 070 ----
mean loss: 247.43
 ---- batch: 080 ----
mean loss: 249.52
 ---- batch: 090 ----
mean loss: 254.75
train mean loss: 246.81
epoch train time: 0:00:02.664620
elapsed time: 0:06:15.476694
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 17:41:34.419656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.47
 ---- batch: 020 ----
mean loss: 249.28
 ---- batch: 030 ----
mean loss: 239.65
 ---- batch: 040 ----
mean loss: 243.38
 ---- batch: 050 ----
mean loss: 241.26
 ---- batch: 060 ----
mean loss: 238.35
 ---- batch: 070 ----
mean loss: 252.05
 ---- batch: 080 ----
mean loss: 257.11
 ---- batch: 090 ----
mean loss: 248.27
train mean loss: 246.71
epoch train time: 0:00:02.640021
elapsed time: 0:06:18.117179
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 17:41:37.060135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.61
 ---- batch: 020 ----
mean loss: 248.23
 ---- batch: 030 ----
mean loss: 246.74
 ---- batch: 040 ----
mean loss: 254.75
 ---- batch: 050 ----
mean loss: 243.35
 ---- batch: 060 ----
mean loss: 243.30
 ---- batch: 070 ----
mean loss: 247.13
 ---- batch: 080 ----
mean loss: 245.74
 ---- batch: 090 ----
mean loss: 243.80
train mean loss: 246.45
epoch train time: 0:00:02.658005
elapsed time: 0:06:20.775679
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 17:41:39.718649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.72
 ---- batch: 020 ----
mean loss: 242.09
 ---- batch: 030 ----
mean loss: 243.13
 ---- batch: 040 ----
mean loss: 246.34
 ---- batch: 050 ----
mean loss: 248.06
 ---- batch: 060 ----
mean loss: 241.84
 ---- batch: 070 ----
mean loss: 247.15
 ---- batch: 080 ----
mean loss: 254.20
 ---- batch: 090 ----
mean loss: 244.19
train mean loss: 246.29
epoch train time: 0:00:02.628462
elapsed time: 0:06:23.404593
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 17:41:42.347580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.45
 ---- batch: 020 ----
mean loss: 243.75
 ---- batch: 030 ----
mean loss: 239.46
 ---- batch: 040 ----
mean loss: 238.86
 ---- batch: 050 ----
mean loss: 245.94
 ---- batch: 060 ----
mean loss: 249.18
 ---- batch: 070 ----
mean loss: 243.06
 ---- batch: 080 ----
mean loss: 252.20
 ---- batch: 090 ----
mean loss: 246.91
train mean loss: 245.85
epoch train time: 0:00:02.668587
elapsed time: 0:06:26.073662
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 17:41:45.016621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.22
 ---- batch: 020 ----
mean loss: 233.90
 ---- batch: 030 ----
mean loss: 243.14
 ---- batch: 040 ----
mean loss: 246.55
 ---- batch: 050 ----
mean loss: 248.62
 ---- batch: 060 ----
mean loss: 238.93
 ---- batch: 070 ----
mean loss: 253.24
 ---- batch: 080 ----
mean loss: 248.43
 ---- batch: 090 ----
mean loss: 245.30
train mean loss: 245.76
epoch train time: 0:00:02.653560
elapsed time: 0:06:28.727710
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 17:41:47.670675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.95
 ---- batch: 020 ----
mean loss: 243.82
 ---- batch: 030 ----
mean loss: 248.49
 ---- batch: 040 ----
mean loss: 241.65
 ---- batch: 050 ----
mean loss: 240.16
 ---- batch: 060 ----
mean loss: 253.38
 ---- batch: 070 ----
mean loss: 247.97
 ---- batch: 080 ----
mean loss: 242.28
 ---- batch: 090 ----
mean loss: 243.31
train mean loss: 245.11
epoch train time: 0:00:02.645416
elapsed time: 0:06:31.373591
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 17:41:50.316576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.86
 ---- batch: 020 ----
mean loss: 245.18
 ---- batch: 030 ----
mean loss: 239.53
 ---- batch: 040 ----
mean loss: 247.49
 ---- batch: 050 ----
mean loss: 241.14
 ---- batch: 060 ----
mean loss: 243.31
 ---- batch: 070 ----
mean loss: 244.80
 ---- batch: 080 ----
mean loss: 246.44
 ---- batch: 090 ----
mean loss: 245.99
train mean loss: 244.71
epoch train time: 0:00:02.631533
elapsed time: 0:06:34.005697
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 17:41:52.948672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.20
 ---- batch: 020 ----
mean loss: 245.27
 ---- batch: 030 ----
mean loss: 244.83
 ---- batch: 040 ----
mean loss: 238.68
 ---- batch: 050 ----
mean loss: 239.61
 ---- batch: 060 ----
mean loss: 240.70
 ---- batch: 070 ----
mean loss: 240.13
 ---- batch: 080 ----
mean loss: 245.45
 ---- batch: 090 ----
mean loss: 251.24
train mean loss: 244.30
epoch train time: 0:00:02.657498
elapsed time: 0:06:36.663652
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 17:41:55.606613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.64
 ---- batch: 020 ----
mean loss: 244.07
 ---- batch: 030 ----
mean loss: 244.16
 ---- batch: 040 ----
mean loss: 249.46
 ---- batch: 050 ----
mean loss: 243.16
 ---- batch: 060 ----
mean loss: 248.55
 ---- batch: 070 ----
mean loss: 248.65
 ---- batch: 080 ----
mean loss: 244.23
 ---- batch: 090 ----
mean loss: 236.88
train mean loss: 244.08
epoch train time: 0:00:02.660360
elapsed time: 0:06:39.324556
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 17:41:58.267517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.27
 ---- batch: 020 ----
mean loss: 245.93
 ---- batch: 030 ----
mean loss: 247.69
 ---- batch: 040 ----
mean loss: 241.96
 ---- batch: 050 ----
mean loss: 252.71
 ---- batch: 060 ----
mean loss: 239.16
 ---- batch: 070 ----
mean loss: 239.15
 ---- batch: 080 ----
mean loss: 242.99
 ---- batch: 090 ----
mean loss: 248.75
train mean loss: 243.93
epoch train time: 0:00:02.658766
elapsed time: 0:06:41.983857
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 17:42:00.926813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.18
 ---- batch: 020 ----
mean loss: 242.05
 ---- batch: 030 ----
mean loss: 250.42
 ---- batch: 040 ----
mean loss: 235.89
 ---- batch: 050 ----
mean loss: 241.73
 ---- batch: 060 ----
mean loss: 242.64
 ---- batch: 070 ----
mean loss: 243.53
 ---- batch: 080 ----
mean loss: 253.35
 ---- batch: 090 ----
mean loss: 238.64
train mean loss: 242.82
epoch train time: 0:00:02.654172
elapsed time: 0:06:44.638488
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 17:42:03.581480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.95
 ---- batch: 020 ----
mean loss: 241.60
 ---- batch: 030 ----
mean loss: 243.64
 ---- batch: 040 ----
mean loss: 246.07
 ---- batch: 050 ----
mean loss: 244.86
 ---- batch: 060 ----
mean loss: 245.75
 ---- batch: 070 ----
mean loss: 233.16
 ---- batch: 080 ----
mean loss: 241.53
 ---- batch: 090 ----
mean loss: 251.42
train mean loss: 243.48
epoch train time: 0:00:02.668645
elapsed time: 0:06:47.307722
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 17:42:06.250676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.23
 ---- batch: 020 ----
mean loss: 245.74
 ---- batch: 030 ----
mean loss: 244.03
 ---- batch: 040 ----
mean loss: 236.94
 ---- batch: 050 ----
mean loss: 245.88
 ---- batch: 060 ----
mean loss: 243.14
 ---- batch: 070 ----
mean loss: 242.22
 ---- batch: 080 ----
mean loss: 238.89
 ---- batch: 090 ----
mean loss: 240.41
train mean loss: 242.85
epoch train time: 0:00:02.646530
elapsed time: 0:06:49.954713
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 17:42:08.897694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.14
 ---- batch: 020 ----
mean loss: 239.34
 ---- batch: 030 ----
mean loss: 245.47
 ---- batch: 040 ----
mean loss: 242.57
 ---- batch: 050 ----
mean loss: 248.07
 ---- batch: 060 ----
mean loss: 242.82
 ---- batch: 070 ----
mean loss: 241.39
 ---- batch: 080 ----
mean loss: 239.34
 ---- batch: 090 ----
mean loss: 238.24
train mean loss: 242.29
epoch train time: 0:00:02.648897
elapsed time: 0:06:52.604079
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 17:42:11.547045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.86
 ---- batch: 020 ----
mean loss: 245.68
 ---- batch: 030 ----
mean loss: 233.38
 ---- batch: 040 ----
mean loss: 245.06
 ---- batch: 050 ----
mean loss: 238.25
 ---- batch: 060 ----
mean loss: 235.62
 ---- batch: 070 ----
mean loss: 244.94
 ---- batch: 080 ----
mean loss: 239.03
 ---- batch: 090 ----
mean loss: 237.72
train mean loss: 242.16
epoch train time: 0:00:02.661885
elapsed time: 0:06:55.266449
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 17:42:14.209412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.26
 ---- batch: 020 ----
mean loss: 252.40
 ---- batch: 030 ----
mean loss: 237.10
 ---- batch: 040 ----
mean loss: 242.07
 ---- batch: 050 ----
mean loss: 233.06
 ---- batch: 060 ----
mean loss: 248.31
 ---- batch: 070 ----
mean loss: 245.23
 ---- batch: 080 ----
mean loss: 238.59
 ---- batch: 090 ----
mean loss: 243.89
train mean loss: 241.71
epoch train time: 0:00:02.676206
elapsed time: 0:06:57.943124
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 17:42:16.886090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.76
 ---- batch: 020 ----
mean loss: 244.52
 ---- batch: 030 ----
mean loss: 233.75
 ---- batch: 040 ----
mean loss: 244.52
 ---- batch: 050 ----
mean loss: 242.44
 ---- batch: 060 ----
mean loss: 248.06
 ---- batch: 070 ----
mean loss: 236.12
 ---- batch: 080 ----
mean loss: 249.74
 ---- batch: 090 ----
mean loss: 241.32
train mean loss: 241.33
epoch train time: 0:00:02.658417
elapsed time: 0:07:00.601999
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 17:42:19.544994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.39
 ---- batch: 020 ----
mean loss: 244.88
 ---- batch: 030 ----
mean loss: 242.67
 ---- batch: 040 ----
mean loss: 240.14
 ---- batch: 050 ----
mean loss: 238.99
 ---- batch: 060 ----
mean loss: 238.76
 ---- batch: 070 ----
mean loss: 238.17
 ---- batch: 080 ----
mean loss: 241.10
 ---- batch: 090 ----
mean loss: 238.86
train mean loss: 241.50
epoch train time: 0:00:02.678352
elapsed time: 0:07:03.280823
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 17:42:22.223789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.93
 ---- batch: 020 ----
mean loss: 250.58
 ---- batch: 030 ----
mean loss: 242.05
 ---- batch: 040 ----
mean loss: 240.68
 ---- batch: 050 ----
mean loss: 241.03
 ---- batch: 060 ----
mean loss: 240.76
 ---- batch: 070 ----
mean loss: 228.81
 ---- batch: 080 ----
mean loss: 246.18
 ---- batch: 090 ----
mean loss: 243.56
train mean loss: 241.27
epoch train time: 0:00:02.632121
elapsed time: 0:07:05.913457
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 17:42:24.856454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.36
 ---- batch: 020 ----
mean loss: 238.83
 ---- batch: 030 ----
mean loss: 240.01
 ---- batch: 040 ----
mean loss: 247.03
 ---- batch: 050 ----
mean loss: 235.03
 ---- batch: 060 ----
mean loss: 235.73
 ---- batch: 070 ----
mean loss: 239.41
 ---- batch: 080 ----
mean loss: 244.97
 ---- batch: 090 ----
mean loss: 239.21
train mean loss: 240.41
epoch train time: 0:00:02.666605
elapsed time: 0:07:08.580620
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 17:42:27.523509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.66
 ---- batch: 020 ----
mean loss: 239.25
 ---- batch: 030 ----
mean loss: 238.73
 ---- batch: 040 ----
mean loss: 237.93
 ---- batch: 050 ----
mean loss: 243.47
 ---- batch: 060 ----
mean loss: 248.71
 ---- batch: 070 ----
mean loss: 243.69
 ---- batch: 080 ----
mean loss: 234.89
 ---- batch: 090 ----
mean loss: 245.06
train mean loss: 240.53
epoch train time: 0:00:02.660101
elapsed time: 0:07:11.241091
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 17:42:30.184053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.02
 ---- batch: 020 ----
mean loss: 247.32
 ---- batch: 030 ----
mean loss: 236.74
 ---- batch: 040 ----
mean loss: 245.33
 ---- batch: 050 ----
mean loss: 230.39
 ---- batch: 060 ----
mean loss: 241.95
 ---- batch: 070 ----
mean loss: 235.48
 ---- batch: 080 ----
mean loss: 243.70
 ---- batch: 090 ----
mean loss: 237.06
train mean loss: 240.24
epoch train time: 0:00:02.663355
elapsed time: 0:07:13.904930
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 17:42:32.847930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.78
 ---- batch: 020 ----
mean loss: 242.86
 ---- batch: 030 ----
mean loss: 240.18
 ---- batch: 040 ----
mean loss: 237.85
 ---- batch: 050 ----
mean loss: 236.39
 ---- batch: 060 ----
mean loss: 240.59
 ---- batch: 070 ----
mean loss: 238.22
 ---- batch: 080 ----
mean loss: 241.27
 ---- batch: 090 ----
mean loss: 250.57
train mean loss: 239.90
epoch train time: 0:00:02.656920
elapsed time: 0:07:16.562370
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 17:42:35.505349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.57
 ---- batch: 020 ----
mean loss: 248.31
 ---- batch: 030 ----
mean loss: 244.05
 ---- batch: 040 ----
mean loss: 233.67
 ---- batch: 050 ----
mean loss: 240.98
 ---- batch: 060 ----
mean loss: 236.37
 ---- batch: 070 ----
mean loss: 240.46
 ---- batch: 080 ----
mean loss: 237.02
 ---- batch: 090 ----
mean loss: 240.34
train mean loss: 239.44
epoch train time: 0:00:02.662293
elapsed time: 0:07:19.225169
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 17:42:38.168137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.09
 ---- batch: 020 ----
mean loss: 230.29
 ---- batch: 030 ----
mean loss: 245.99
 ---- batch: 040 ----
mean loss: 248.76
 ---- batch: 050 ----
mean loss: 243.41
 ---- batch: 060 ----
mean loss: 235.86
 ---- batch: 070 ----
mean loss: 236.69
 ---- batch: 080 ----
mean loss: 234.77
 ---- batch: 090 ----
mean loss: 234.64
train mean loss: 239.76
epoch train time: 0:00:02.649602
elapsed time: 0:07:21.875289
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 17:42:40.818329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.03
 ---- batch: 020 ----
mean loss: 235.44
 ---- batch: 030 ----
mean loss: 233.11
 ---- batch: 040 ----
mean loss: 243.42
 ---- batch: 050 ----
mean loss: 237.99
 ---- batch: 060 ----
mean loss: 242.91
 ---- batch: 070 ----
mean loss: 236.61
 ---- batch: 080 ----
mean loss: 243.16
 ---- batch: 090 ----
mean loss: 240.38
train mean loss: 239.12
epoch train time: 0:00:02.641110
elapsed time: 0:07:24.516927
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 17:42:43.459909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.15
 ---- batch: 020 ----
mean loss: 242.60
 ---- batch: 030 ----
mean loss: 239.08
 ---- batch: 040 ----
mean loss: 230.31
 ---- batch: 050 ----
mean loss: 239.95
 ---- batch: 060 ----
mean loss: 246.90
 ---- batch: 070 ----
mean loss: 238.12
 ---- batch: 080 ----
mean loss: 235.34
 ---- batch: 090 ----
mean loss: 243.38
train mean loss: 239.20
epoch train time: 0:00:02.635102
elapsed time: 0:07:27.152541
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 17:42:46.095502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.00
 ---- batch: 020 ----
mean loss: 232.74
 ---- batch: 030 ----
mean loss: 242.87
 ---- batch: 040 ----
mean loss: 245.15
 ---- batch: 050 ----
mean loss: 230.62
 ---- batch: 060 ----
mean loss: 241.00
 ---- batch: 070 ----
mean loss: 245.09
 ---- batch: 080 ----
mean loss: 250.72
 ---- batch: 090 ----
mean loss: 230.39
train mean loss: 238.86
epoch train time: 0:00:02.668717
elapsed time: 0:07:29.821741
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 17:42:48.764707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.26
 ---- batch: 020 ----
mean loss: 246.70
 ---- batch: 030 ----
mean loss: 235.83
 ---- batch: 040 ----
mean loss: 238.07
 ---- batch: 050 ----
mean loss: 243.70
 ---- batch: 060 ----
mean loss: 236.15
 ---- batch: 070 ----
mean loss: 236.84
 ---- batch: 080 ----
mean loss: 235.61
 ---- batch: 090 ----
mean loss: 238.82
train mean loss: 238.49
epoch train time: 0:00:02.668320
elapsed time: 0:07:32.490539
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 17:42:51.433521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.43
 ---- batch: 020 ----
mean loss: 230.23
 ---- batch: 030 ----
mean loss: 239.71
 ---- batch: 040 ----
mean loss: 236.98
 ---- batch: 050 ----
mean loss: 231.34
 ---- batch: 060 ----
mean loss: 237.73
 ---- batch: 070 ----
mean loss: 235.01
 ---- batch: 080 ----
mean loss: 246.02
 ---- batch: 090 ----
mean loss: 233.74
train mean loss: 238.38
epoch train time: 0:00:02.650473
elapsed time: 0:07:35.141501
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 17:42:54.084522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.28
 ---- batch: 020 ----
mean loss: 232.41
 ---- batch: 030 ----
mean loss: 244.33
 ---- batch: 040 ----
mean loss: 242.02
 ---- batch: 050 ----
mean loss: 243.98
 ---- batch: 060 ----
mean loss: 235.23
 ---- batch: 070 ----
mean loss: 236.00
 ---- batch: 080 ----
mean loss: 232.82
 ---- batch: 090 ----
mean loss: 236.31
train mean loss: 237.85
epoch train time: 0:00:02.662595
elapsed time: 0:07:37.804604
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 17:42:56.747566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.79
 ---- batch: 020 ----
mean loss: 238.30
 ---- batch: 030 ----
mean loss: 231.15
 ---- batch: 040 ----
mean loss: 237.94
 ---- batch: 050 ----
mean loss: 236.13
 ---- batch: 060 ----
mean loss: 237.08
 ---- batch: 070 ----
mean loss: 240.98
 ---- batch: 080 ----
mean loss: 241.20
 ---- batch: 090 ----
mean loss: 238.29
train mean loss: 237.69
epoch train time: 0:00:02.678243
elapsed time: 0:07:40.483343
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 17:42:59.426365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.15
 ---- batch: 020 ----
mean loss: 230.91
 ---- batch: 030 ----
mean loss: 246.66
 ---- batch: 040 ----
mean loss: 231.33
 ---- batch: 050 ----
mean loss: 232.57
 ---- batch: 060 ----
mean loss: 243.78
 ---- batch: 070 ----
mean loss: 245.69
 ---- batch: 080 ----
mean loss: 231.27
 ---- batch: 090 ----
mean loss: 236.09
train mean loss: 237.33
epoch train time: 0:00:02.672079
elapsed time: 0:07:43.155968
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 17:43:02.098935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.28
 ---- batch: 020 ----
mean loss: 241.85
 ---- batch: 030 ----
mean loss: 249.83
 ---- batch: 040 ----
mean loss: 236.97
 ---- batch: 050 ----
mean loss: 230.73
 ---- batch: 060 ----
mean loss: 245.50
 ---- batch: 070 ----
mean loss: 233.40
 ---- batch: 080 ----
mean loss: 235.98
 ---- batch: 090 ----
mean loss: 233.21
train mean loss: 237.00
epoch train time: 0:00:02.678113
elapsed time: 0:07:45.834570
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 17:43:04.777577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.98
 ---- batch: 020 ----
mean loss: 228.68
 ---- batch: 030 ----
mean loss: 238.90
 ---- batch: 040 ----
mean loss: 238.78
 ---- batch: 050 ----
mean loss: 237.06
 ---- batch: 060 ----
mean loss: 240.41
 ---- batch: 070 ----
mean loss: 238.26
 ---- batch: 080 ----
mean loss: 233.38
 ---- batch: 090 ----
mean loss: 238.29
train mean loss: 237.06
epoch train time: 0:00:02.661176
elapsed time: 0:07:48.496298
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 17:43:07.439267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.50
 ---- batch: 020 ----
mean loss: 234.81
 ---- batch: 030 ----
mean loss: 236.03
 ---- batch: 040 ----
mean loss: 230.25
 ---- batch: 050 ----
mean loss: 244.50
 ---- batch: 060 ----
mean loss: 242.14
 ---- batch: 070 ----
mean loss: 234.66
 ---- batch: 080 ----
mean loss: 227.43
 ---- batch: 090 ----
mean loss: 243.49
train mean loss: 236.56
epoch train time: 0:00:02.662540
elapsed time: 0:07:51.159335
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 17:43:10.102309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.49
 ---- batch: 020 ----
mean loss: 232.40
 ---- batch: 030 ----
mean loss: 228.94
 ---- batch: 040 ----
mean loss: 245.37
 ---- batch: 050 ----
mean loss: 240.05
 ---- batch: 060 ----
mean loss: 247.94
 ---- batch: 070 ----
mean loss: 228.20
 ---- batch: 080 ----
mean loss: 228.19
 ---- batch: 090 ----
mean loss: 238.45
train mean loss: 236.53
epoch train time: 0:00:02.675856
elapsed time: 0:07:53.835832
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 17:43:12.778794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.09
 ---- batch: 020 ----
mean loss: 234.69
 ---- batch: 030 ----
mean loss: 239.18
 ---- batch: 040 ----
mean loss: 233.64
 ---- batch: 050 ----
mean loss: 235.40
 ---- batch: 060 ----
mean loss: 230.87
 ---- batch: 070 ----
mean loss: 227.86
 ---- batch: 080 ----
mean loss: 237.55
 ---- batch: 090 ----
mean loss: 242.60
train mean loss: 236.19
epoch train time: 0:00:02.679795
elapsed time: 0:07:56.516080
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 17:43:15.459058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.83
 ---- batch: 020 ----
mean loss: 234.75
 ---- batch: 030 ----
mean loss: 242.47
 ---- batch: 040 ----
mean loss: 226.28
 ---- batch: 050 ----
mean loss: 236.14
 ---- batch: 060 ----
mean loss: 231.35
 ---- batch: 070 ----
mean loss: 238.80
 ---- batch: 080 ----
mean loss: 235.92
 ---- batch: 090 ----
mean loss: 235.68
train mean loss: 236.06
epoch train time: 0:00:02.670152
elapsed time: 0:07:59.186691
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 17:43:18.129675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.59
 ---- batch: 020 ----
mean loss: 234.10
 ---- batch: 030 ----
mean loss: 228.70
 ---- batch: 040 ----
mean loss: 232.26
 ---- batch: 050 ----
mean loss: 235.39
 ---- batch: 060 ----
mean loss: 242.60
 ---- batch: 070 ----
mean loss: 232.14
 ---- batch: 080 ----
mean loss: 240.59
 ---- batch: 090 ----
mean loss: 233.12
train mean loss: 235.92
epoch train time: 0:00:02.683491
elapsed time: 0:08:01.870693
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 17:43:20.813711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.98
 ---- batch: 020 ----
mean loss: 232.23
 ---- batch: 030 ----
mean loss: 232.65
 ---- batch: 040 ----
mean loss: 237.86
 ---- batch: 050 ----
mean loss: 231.85
 ---- batch: 060 ----
mean loss: 238.27
 ---- batch: 070 ----
mean loss: 240.96
 ---- batch: 080 ----
mean loss: 238.12
 ---- batch: 090 ----
mean loss: 231.09
train mean loss: 235.55
epoch train time: 0:00:02.645418
elapsed time: 0:08:04.516671
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 17:43:23.459671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.50
 ---- batch: 020 ----
mean loss: 236.56
 ---- batch: 030 ----
mean loss: 233.89
 ---- batch: 040 ----
mean loss: 238.88
 ---- batch: 050 ----
mean loss: 239.39
 ---- batch: 060 ----
mean loss: 234.92
 ---- batch: 070 ----
mean loss: 238.00
 ---- batch: 080 ----
mean loss: 225.12
 ---- batch: 090 ----
mean loss: 231.68
train mean loss: 234.92
epoch train time: 0:00:02.618359
elapsed time: 0:08:07.135509
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 17:43:26.078467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.44
 ---- batch: 020 ----
mean loss: 232.44
 ---- batch: 030 ----
mean loss: 223.75
 ---- batch: 040 ----
mean loss: 235.16
 ---- batch: 050 ----
mean loss: 235.42
 ---- batch: 060 ----
mean loss: 239.68
 ---- batch: 070 ----
mean loss: 242.75
 ---- batch: 080 ----
mean loss: 231.50
 ---- batch: 090 ----
mean loss: 239.14
train mean loss: 235.16
epoch train time: 0:00:02.626802
elapsed time: 0:08:09.762769
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 17:43:28.705796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.57
 ---- batch: 020 ----
mean loss: 223.65
 ---- batch: 030 ----
mean loss: 239.05
 ---- batch: 040 ----
mean loss: 243.50
 ---- batch: 050 ----
mean loss: 231.30
 ---- batch: 060 ----
mean loss: 226.20
 ---- batch: 070 ----
mean loss: 233.16
 ---- batch: 080 ----
mean loss: 237.26
 ---- batch: 090 ----
mean loss: 232.48
train mean loss: 234.74
epoch train time: 0:00:02.650569
elapsed time: 0:08:12.413821
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 17:43:31.356789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.26
 ---- batch: 020 ----
mean loss: 229.20
 ---- batch: 030 ----
mean loss: 242.23
 ---- batch: 040 ----
mean loss: 227.27
 ---- batch: 050 ----
mean loss: 228.85
 ---- batch: 060 ----
mean loss: 239.64
 ---- batch: 070 ----
mean loss: 236.89
 ---- batch: 080 ----
mean loss: 238.29
 ---- batch: 090 ----
mean loss: 232.28
train mean loss: 234.40
epoch train time: 0:00:02.641068
elapsed time: 0:08:15.055485
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 17:43:33.998343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.15
 ---- batch: 020 ----
mean loss: 236.82
 ---- batch: 030 ----
mean loss: 247.10
 ---- batch: 040 ----
mean loss: 224.30
 ---- batch: 050 ----
mean loss: 227.31
 ---- batch: 060 ----
mean loss: 240.56
 ---- batch: 070 ----
mean loss: 225.44
 ---- batch: 080 ----
mean loss: 237.56
 ---- batch: 090 ----
mean loss: 234.83
train mean loss: 233.97
epoch train time: 0:00:02.639440
elapsed time: 0:08:17.695346
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 17:43:36.638304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.04
 ---- batch: 020 ----
mean loss: 225.02
 ---- batch: 030 ----
mean loss: 233.61
 ---- batch: 040 ----
mean loss: 237.62
 ---- batch: 050 ----
mean loss: 234.17
 ---- batch: 060 ----
mean loss: 237.42
 ---- batch: 070 ----
mean loss: 237.97
 ---- batch: 080 ----
mean loss: 228.74
 ---- batch: 090 ----
mean loss: 234.24
train mean loss: 233.83
epoch train time: 0:00:02.671484
elapsed time: 0:08:20.367277
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 17:43:39.310240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.83
 ---- batch: 020 ----
mean loss: 232.04
 ---- batch: 030 ----
mean loss: 232.53
 ---- batch: 040 ----
mean loss: 234.85
 ---- batch: 050 ----
mean loss: 241.30
 ---- batch: 060 ----
mean loss: 233.07
 ---- batch: 070 ----
mean loss: 228.57
 ---- batch: 080 ----
mean loss: 224.44
 ---- batch: 090 ----
mean loss: 239.55
train mean loss: 233.48
epoch train time: 0:00:02.653617
elapsed time: 0:08:23.021385
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 17:43:41.964341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.76
 ---- batch: 020 ----
mean loss: 240.65
 ---- batch: 030 ----
mean loss: 231.64
 ---- batch: 040 ----
mean loss: 228.19
 ---- batch: 050 ----
mean loss: 225.39
 ---- batch: 060 ----
mean loss: 236.38
 ---- batch: 070 ----
mean loss: 234.91
 ---- batch: 080 ----
mean loss: 234.40
 ---- batch: 090 ----
mean loss: 228.93
train mean loss: 232.96
epoch train time: 0:00:02.679430
elapsed time: 0:08:25.701280
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 17:43:44.644263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.06
 ---- batch: 020 ----
mean loss: 237.66
 ---- batch: 030 ----
mean loss: 234.87
 ---- batch: 040 ----
mean loss: 235.99
 ---- batch: 050 ----
mean loss: 224.00
 ---- batch: 060 ----
mean loss: 238.20
 ---- batch: 070 ----
mean loss: 226.80
 ---- batch: 080 ----
mean loss: 227.10
 ---- batch: 090 ----
mean loss: 232.01
train mean loss: 233.11
epoch train time: 0:00:02.598206
elapsed time: 0:08:28.299972
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 17:43:47.242933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.76
 ---- batch: 020 ----
mean loss: 233.07
 ---- batch: 030 ----
mean loss: 236.97
 ---- batch: 040 ----
mean loss: 231.52
 ---- batch: 050 ----
mean loss: 236.72
 ---- batch: 060 ----
mean loss: 233.42
 ---- batch: 070 ----
mean loss: 225.14
 ---- batch: 080 ----
mean loss: 232.10
 ---- batch: 090 ----
mean loss: 231.08
train mean loss: 232.96
epoch train time: 0:00:02.592516
elapsed time: 0:08:30.892966
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 17:43:49.835935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.32
 ---- batch: 020 ----
mean loss: 230.29
 ---- batch: 030 ----
mean loss: 232.20
 ---- batch: 040 ----
mean loss: 241.31
 ---- batch: 050 ----
mean loss: 230.16
 ---- batch: 060 ----
mean loss: 229.53
 ---- batch: 070 ----
mean loss: 230.05
 ---- batch: 080 ----
mean loss: 234.22
 ---- batch: 090 ----
mean loss: 229.70
train mean loss: 232.84
epoch train time: 0:00:02.609929
elapsed time: 0:08:33.503380
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 17:43:52.446342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.23
 ---- batch: 020 ----
mean loss: 239.59
 ---- batch: 030 ----
mean loss: 230.02
 ---- batch: 040 ----
mean loss: 234.87
 ---- batch: 050 ----
mean loss: 226.46
 ---- batch: 060 ----
mean loss: 228.44
 ---- batch: 070 ----
mean loss: 234.34
 ---- batch: 080 ----
mean loss: 232.79
 ---- batch: 090 ----
mean loss: 231.12
train mean loss: 232.30
epoch train time: 0:00:02.608598
elapsed time: 0:08:36.112481
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 17:43:55.055492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.93
 ---- batch: 020 ----
mean loss: 230.19
 ---- batch: 030 ----
mean loss: 238.28
 ---- batch: 040 ----
mean loss: 232.02
 ---- batch: 050 ----
mean loss: 229.32
 ---- batch: 060 ----
mean loss: 236.41
 ---- batch: 070 ----
mean loss: 230.50
 ---- batch: 080 ----
mean loss: 235.74
 ---- batch: 090 ----
mean loss: 220.46
train mean loss: 231.95
epoch train time: 0:00:02.584680
elapsed time: 0:08:38.697706
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 17:43:57.640687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.68
 ---- batch: 020 ----
mean loss: 231.76
 ---- batch: 030 ----
mean loss: 227.22
 ---- batch: 040 ----
mean loss: 240.69
 ---- batch: 050 ----
mean loss: 232.95
 ---- batch: 060 ----
mean loss: 230.01
 ---- batch: 070 ----
mean loss: 231.99
 ---- batch: 080 ----
mean loss: 232.60
 ---- batch: 090 ----
mean loss: 227.35
train mean loss: 231.88
epoch train time: 0:00:02.591741
elapsed time: 0:08:41.289969
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 17:44:00.232943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.81
 ---- batch: 020 ----
mean loss: 228.81
 ---- batch: 030 ----
mean loss: 232.42
 ---- batch: 040 ----
mean loss: 232.78
 ---- batch: 050 ----
mean loss: 233.53
 ---- batch: 060 ----
mean loss: 240.43
 ---- batch: 070 ----
mean loss: 227.65
 ---- batch: 080 ----
mean loss: 227.85
 ---- batch: 090 ----
mean loss: 228.50
train mean loss: 231.77
epoch train time: 0:00:02.620790
elapsed time: 0:08:43.911326
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 17:44:02.854316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.54
 ---- batch: 020 ----
mean loss: 224.42
 ---- batch: 030 ----
mean loss: 229.79
 ---- batch: 040 ----
mean loss: 238.99
 ---- batch: 050 ----
mean loss: 241.46
 ---- batch: 060 ----
mean loss: 229.66
 ---- batch: 070 ----
mean loss: 227.93
 ---- batch: 080 ----
mean loss: 229.71
 ---- batch: 090 ----
mean loss: 232.49
train mean loss: 231.23
epoch train time: 0:00:02.610247
elapsed time: 0:08:46.522127
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 17:44:05.465091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.06
 ---- batch: 020 ----
mean loss: 229.08
 ---- batch: 030 ----
mean loss: 233.24
 ---- batch: 040 ----
mean loss: 233.70
 ---- batch: 050 ----
mean loss: 223.97
 ---- batch: 060 ----
mean loss: 232.03
 ---- batch: 070 ----
mean loss: 234.55
 ---- batch: 080 ----
mean loss: 228.02
 ---- batch: 090 ----
mean loss: 234.62
train mean loss: 231.29
epoch train time: 0:00:02.616013
elapsed time: 0:08:49.138650
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 17:44:08.081646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.29
 ---- batch: 020 ----
mean loss: 231.55
 ---- batch: 030 ----
mean loss: 228.77
 ---- batch: 040 ----
mean loss: 227.49
 ---- batch: 050 ----
mean loss: 235.63
 ---- batch: 060 ----
mean loss: 233.66
 ---- batch: 070 ----
mean loss: 226.92
 ---- batch: 080 ----
mean loss: 228.83
 ---- batch: 090 ----
mean loss: 234.54
train mean loss: 230.53
epoch train time: 0:00:02.609566
elapsed time: 0:08:51.748783
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 17:44:10.691740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.37
 ---- batch: 020 ----
mean loss: 232.49
 ---- batch: 030 ----
mean loss: 225.62
 ---- batch: 040 ----
mean loss: 228.85
 ---- batch: 050 ----
mean loss: 237.13
 ---- batch: 060 ----
mean loss: 225.96
 ---- batch: 070 ----
mean loss: 232.67
 ---- batch: 080 ----
mean loss: 232.79
 ---- batch: 090 ----
mean loss: 230.57
train mean loss: 230.85
epoch train time: 0:00:02.585832
elapsed time: 0:08:54.335105
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 17:44:13.278074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.10
 ---- batch: 020 ----
mean loss: 230.97
 ---- batch: 030 ----
mean loss: 239.96
 ---- batch: 040 ----
mean loss: 224.14
 ---- batch: 050 ----
mean loss: 232.17
 ---- batch: 060 ----
mean loss: 221.81
 ---- batch: 070 ----
mean loss: 220.52
 ---- batch: 080 ----
mean loss: 236.09
 ---- batch: 090 ----
mean loss: 234.91
train mean loss: 230.19
epoch train time: 0:00:02.590388
elapsed time: 0:08:56.926000
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 17:44:15.868962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.34
 ---- batch: 020 ----
mean loss: 228.00
 ---- batch: 030 ----
mean loss: 232.61
 ---- batch: 040 ----
mean loss: 233.54
 ---- batch: 050 ----
mean loss: 235.16
 ---- batch: 060 ----
mean loss: 227.26
 ---- batch: 070 ----
mean loss: 226.12
 ---- batch: 080 ----
mean loss: 240.28
 ---- batch: 090 ----
mean loss: 222.25
train mean loss: 230.07
epoch train time: 0:00:02.602799
elapsed time: 0:08:59.529317
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 17:44:18.472286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.68
 ---- batch: 020 ----
mean loss: 232.28
 ---- batch: 030 ----
mean loss: 230.25
 ---- batch: 040 ----
mean loss: 225.02
 ---- batch: 050 ----
mean loss: 233.88
 ---- batch: 060 ----
mean loss: 226.89
 ---- batch: 070 ----
mean loss: 231.67
 ---- batch: 080 ----
mean loss: 228.47
 ---- batch: 090 ----
mean loss: 227.49
train mean loss: 229.57
epoch train time: 0:00:02.638488
elapsed time: 0:09:02.168315
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 17:44:21.111284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.29
 ---- batch: 020 ----
mean loss: 229.20
 ---- batch: 030 ----
mean loss: 222.93
 ---- batch: 040 ----
mean loss: 234.19
 ---- batch: 050 ----
mean loss: 229.39
 ---- batch: 060 ----
mean loss: 231.52
 ---- batch: 070 ----
mean loss: 225.34
 ---- batch: 080 ----
mean loss: 229.85
 ---- batch: 090 ----
mean loss: 228.98
train mean loss: 229.06
epoch train time: 0:00:02.605364
elapsed time: 0:09:04.774243
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 17:44:23.717246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.29
 ---- batch: 020 ----
mean loss: 221.81
 ---- batch: 030 ----
mean loss: 229.44
 ---- batch: 040 ----
mean loss: 229.05
 ---- batch: 050 ----
mean loss: 229.24
 ---- batch: 060 ----
mean loss: 217.46
 ---- batch: 070 ----
mean loss: 233.73
 ---- batch: 080 ----
mean loss: 236.85
 ---- batch: 090 ----
mean loss: 232.12
train mean loss: 229.24
epoch train time: 0:00:02.610051
elapsed time: 0:09:07.384832
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 17:44:26.327797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.67
 ---- batch: 020 ----
mean loss: 227.86
 ---- batch: 030 ----
mean loss: 226.90
 ---- batch: 040 ----
mean loss: 229.02
 ---- batch: 050 ----
mean loss: 223.88
 ---- batch: 060 ----
mean loss: 232.10
 ---- batch: 070 ----
mean loss: 232.69
 ---- batch: 080 ----
mean loss: 226.22
 ---- batch: 090 ----
mean loss: 231.14
train mean loss: 229.09
epoch train time: 0:00:02.619540
elapsed time: 0:09:10.004878
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 17:44:28.947844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.29
 ---- batch: 020 ----
mean loss: 225.60
 ---- batch: 030 ----
mean loss: 234.98
 ---- batch: 040 ----
mean loss: 227.98
 ---- batch: 050 ----
mean loss: 227.96
 ---- batch: 060 ----
mean loss: 232.37
 ---- batch: 070 ----
mean loss: 230.51
 ---- batch: 080 ----
mean loss: 227.47
 ---- batch: 090 ----
mean loss: 226.07
train mean loss: 229.03
epoch train time: 0:00:02.608208
elapsed time: 0:09:12.613571
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 17:44:31.556532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.93
 ---- batch: 020 ----
mean loss: 226.54
 ---- batch: 030 ----
mean loss: 226.57
 ---- batch: 040 ----
mean loss: 231.49
 ---- batch: 050 ----
mean loss: 225.99
 ---- batch: 060 ----
mean loss: 223.39
 ---- batch: 070 ----
mean loss: 229.07
 ---- batch: 080 ----
mean loss: 225.50
 ---- batch: 090 ----
mean loss: 234.27
train mean loss: 228.15
epoch train time: 0:00:02.640041
elapsed time: 0:09:15.254139
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 17:44:34.197144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.32
 ---- batch: 020 ----
mean loss: 231.36
 ---- batch: 030 ----
mean loss: 229.45
 ---- batch: 040 ----
mean loss: 225.55
 ---- batch: 050 ----
mean loss: 226.85
 ---- batch: 060 ----
mean loss: 232.16
 ---- batch: 070 ----
mean loss: 222.70
 ---- batch: 080 ----
mean loss: 224.91
 ---- batch: 090 ----
mean loss: 236.41
train mean loss: 228.07
epoch train time: 0:00:02.623520
elapsed time: 0:09:17.878256
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 17:44:36.821220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.65
 ---- batch: 020 ----
mean loss: 225.80
 ---- batch: 030 ----
mean loss: 226.21
 ---- batch: 040 ----
mean loss: 233.90
 ---- batch: 050 ----
mean loss: 236.32
 ---- batch: 060 ----
mean loss: 228.48
 ---- batch: 070 ----
mean loss: 232.43
 ---- batch: 080 ----
mean loss: 229.93
 ---- batch: 090 ----
mean loss: 219.02
train mean loss: 228.01
epoch train time: 0:00:02.600903
elapsed time: 0:09:20.479658
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 17:44:39.422634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.15
 ---- batch: 020 ----
mean loss: 217.54
 ---- batch: 030 ----
mean loss: 231.68
 ---- batch: 040 ----
mean loss: 225.94
 ---- batch: 050 ----
mean loss: 232.01
 ---- batch: 060 ----
mean loss: 229.93
 ---- batch: 070 ----
mean loss: 225.01
 ---- batch: 080 ----
mean loss: 233.98
 ---- batch: 090 ----
mean loss: 222.55
train mean loss: 227.19
epoch train time: 0:00:02.609133
elapsed time: 0:09:23.089307
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 17:44:42.032293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.80
 ---- batch: 020 ----
mean loss: 229.01
 ---- batch: 030 ----
mean loss: 221.82
 ---- batch: 040 ----
mean loss: 229.00
 ---- batch: 050 ----
mean loss: 226.37
 ---- batch: 060 ----
mean loss: 221.51
 ---- batch: 070 ----
mean loss: 229.04
 ---- batch: 080 ----
mean loss: 230.54
 ---- batch: 090 ----
mean loss: 231.13
train mean loss: 227.68
epoch train time: 0:00:02.620424
elapsed time: 0:09:25.710287
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 17:44:44.653257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.54
 ---- batch: 020 ----
mean loss: 224.02
 ---- batch: 030 ----
mean loss: 225.23
 ---- batch: 040 ----
mean loss: 238.42
 ---- batch: 050 ----
mean loss: 226.44
 ---- batch: 060 ----
mean loss: 214.01
 ---- batch: 070 ----
mean loss: 225.71
 ---- batch: 080 ----
mean loss: 229.57
 ---- batch: 090 ----
mean loss: 228.71
train mean loss: 226.87
epoch train time: 0:00:02.631551
elapsed time: 0:09:28.342498
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 17:44:47.285344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 233.03
 ---- batch: 020 ----
mean loss: 224.65
 ---- batch: 030 ----
mean loss: 228.40
 ---- batch: 040 ----
mean loss: 220.39
 ---- batch: 050 ----
mean loss: 229.11
 ---- batch: 060 ----
mean loss: 220.05
 ---- batch: 070 ----
mean loss: 224.63
 ---- batch: 080 ----
mean loss: 228.47
 ---- batch: 090 ----
mean loss: 225.79
train mean loss: 226.73
epoch train time: 0:00:02.609080
elapsed time: 0:09:30.951943
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 17:44:49.894903
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.73
 ---- batch: 020 ----
mean loss: 226.59
 ---- batch: 030 ----
mean loss: 220.83
 ---- batch: 040 ----
mean loss: 227.55
 ---- batch: 050 ----
mean loss: 230.27
 ---- batch: 060 ----
mean loss: 221.15
 ---- batch: 070 ----
mean loss: 237.49
 ---- batch: 080 ----
mean loss: 224.67
 ---- batch: 090 ----
mean loss: 227.93
train mean loss: 226.64
epoch train time: 0:00:02.620790
elapsed time: 0:09:33.573261
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 17:44:52.516232
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.88
 ---- batch: 020 ----
mean loss: 230.82
 ---- batch: 030 ----
mean loss: 229.91
 ---- batch: 040 ----
mean loss: 225.51
 ---- batch: 050 ----
mean loss: 226.76
 ---- batch: 060 ----
mean loss: 229.18
 ---- batch: 070 ----
mean loss: 222.39
 ---- batch: 080 ----
mean loss: 233.99
 ---- batch: 090 ----
mean loss: 219.74
train mean loss: 226.44
epoch train time: 0:00:02.608933
elapsed time: 0:09:36.182725
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 17:44:55.125709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.20
 ---- batch: 020 ----
mean loss: 228.52
 ---- batch: 030 ----
mean loss: 237.06
 ---- batch: 040 ----
mean loss: 217.29
 ---- batch: 050 ----
mean loss: 228.31
 ---- batch: 060 ----
mean loss: 227.15
 ---- batch: 070 ----
mean loss: 227.74
 ---- batch: 080 ----
mean loss: 234.97
 ---- batch: 090 ----
mean loss: 222.61
train mean loss: 226.54
epoch train time: 0:00:02.604056
elapsed time: 0:09:38.787343
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 17:44:57.730309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.50
 ---- batch: 020 ----
mean loss: 221.85
 ---- batch: 030 ----
mean loss: 223.34
 ---- batch: 040 ----
mean loss: 238.02
 ---- batch: 050 ----
mean loss: 222.78
 ---- batch: 060 ----
mean loss: 224.25
 ---- batch: 070 ----
mean loss: 231.26
 ---- batch: 080 ----
mean loss: 233.74
 ---- batch: 090 ----
mean loss: 223.20
train mean loss: 226.72
epoch train time: 0:00:02.630962
elapsed time: 0:09:41.418810
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 17:45:00.361797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.60
 ---- batch: 020 ----
mean loss: 221.14
 ---- batch: 030 ----
mean loss: 225.67
 ---- batch: 040 ----
mean loss: 235.40
 ---- batch: 050 ----
mean loss: 226.12
 ---- batch: 060 ----
mean loss: 219.55
 ---- batch: 070 ----
mean loss: 229.79
 ---- batch: 080 ----
mean loss: 226.48
 ---- batch: 090 ----
mean loss: 226.74
train mean loss: 226.59
epoch train time: 0:00:02.608962
elapsed time: 0:09:44.028319
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 17:45:02.971313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.00
 ---- batch: 020 ----
mean loss: 229.30
 ---- batch: 030 ----
mean loss: 223.33
 ---- batch: 040 ----
mean loss: 240.07
 ---- batch: 050 ----
mean loss: 230.16
 ---- batch: 060 ----
mean loss: 220.32
 ---- batch: 070 ----
mean loss: 223.97
 ---- batch: 080 ----
mean loss: 226.90
 ---- batch: 090 ----
mean loss: 223.16
train mean loss: 226.42
epoch train time: 0:00:02.608752
elapsed time: 0:09:46.637615
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 17:45:05.580583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.19
 ---- batch: 020 ----
mean loss: 230.72
 ---- batch: 030 ----
mean loss: 226.69
 ---- batch: 040 ----
mean loss: 222.26
 ---- batch: 050 ----
mean loss: 231.90
 ---- batch: 060 ----
mean loss: 230.49
 ---- batch: 070 ----
mean loss: 227.32
 ---- batch: 080 ----
mean loss: 226.72
 ---- batch: 090 ----
mean loss: 220.98
train mean loss: 226.52
epoch train time: 0:00:02.616493
elapsed time: 0:09:49.254611
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 17:45:08.197588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.73
 ---- batch: 020 ----
mean loss: 230.47
 ---- batch: 030 ----
mean loss: 228.03
 ---- batch: 040 ----
mean loss: 221.39
 ---- batch: 050 ----
mean loss: 224.36
 ---- batch: 060 ----
mean loss: 234.01
 ---- batch: 070 ----
mean loss: 222.92
 ---- batch: 080 ----
mean loss: 224.93
 ---- batch: 090 ----
mean loss: 221.88
train mean loss: 226.24
epoch train time: 0:00:02.599462
elapsed time: 0:09:51.854659
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 17:45:10.797655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.88
 ---- batch: 020 ----
mean loss: 226.40
 ---- batch: 030 ----
mean loss: 225.35
 ---- batch: 040 ----
mean loss: 224.30
 ---- batch: 050 ----
mean loss: 223.12
 ---- batch: 060 ----
mean loss: 226.54
 ---- batch: 070 ----
mean loss: 231.81
 ---- batch: 080 ----
mean loss: 223.77
 ---- batch: 090 ----
mean loss: 225.62
train mean loss: 226.64
epoch train time: 0:00:02.616134
elapsed time: 0:09:54.471315
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 17:45:13.414277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.64
 ---- batch: 020 ----
mean loss: 229.65
 ---- batch: 030 ----
mean loss: 225.05
 ---- batch: 040 ----
mean loss: 230.27
 ---- batch: 050 ----
mean loss: 222.85
 ---- batch: 060 ----
mean loss: 216.82
 ---- batch: 070 ----
mean loss: 230.02
 ---- batch: 080 ----
mean loss: 227.40
 ---- batch: 090 ----
mean loss: 227.03
train mean loss: 226.58
epoch train time: 0:00:02.598004
elapsed time: 0:09:57.069794
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 17:45:16.012762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.45
 ---- batch: 020 ----
mean loss: 227.08
 ---- batch: 030 ----
mean loss: 225.33
 ---- batch: 040 ----
mean loss: 227.84
 ---- batch: 050 ----
mean loss: 234.52
 ---- batch: 060 ----
mean loss: 228.36
 ---- batch: 070 ----
mean loss: 221.73
 ---- batch: 080 ----
mean loss: 223.99
 ---- batch: 090 ----
mean loss: 222.50
train mean loss: 226.28
epoch train time: 0:00:02.589098
elapsed time: 0:09:59.659352
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 17:45:18.602316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.10
 ---- batch: 020 ----
mean loss: 211.97
 ---- batch: 030 ----
mean loss: 222.95
 ---- batch: 040 ----
mean loss: 225.08
 ---- batch: 050 ----
mean loss: 229.33
 ---- batch: 060 ----
mean loss: 233.34
 ---- batch: 070 ----
mean loss: 235.06
 ---- batch: 080 ----
mean loss: 234.13
 ---- batch: 090 ----
mean loss: 228.91
train mean loss: 226.81
epoch train time: 0:00:02.610839
elapsed time: 0:10:02.270718
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 17:45:21.213735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.18
 ---- batch: 020 ----
mean loss: 225.97
 ---- batch: 030 ----
mean loss: 226.97
 ---- batch: 040 ----
mean loss: 225.35
 ---- batch: 050 ----
mean loss: 222.78
 ---- batch: 060 ----
mean loss: 234.01
 ---- batch: 070 ----
mean loss: 221.98
 ---- batch: 080 ----
mean loss: 226.07
 ---- batch: 090 ----
mean loss: 221.92
train mean loss: 226.45
epoch train time: 0:00:02.651505
elapsed time: 0:10:04.922827
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 17:45:23.865799
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.02
 ---- batch: 020 ----
mean loss: 225.48
 ---- batch: 030 ----
mean loss: 227.10
 ---- batch: 040 ----
mean loss: 223.50
 ---- batch: 050 ----
mean loss: 226.18
 ---- batch: 060 ----
mean loss: 229.02
 ---- batch: 070 ----
mean loss: 220.31
 ---- batch: 080 ----
mean loss: 229.90
 ---- batch: 090 ----
mean loss: 227.66
train mean loss: 226.13
epoch train time: 0:00:02.614592
elapsed time: 0:10:07.537950
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 17:45:26.480914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.72
 ---- batch: 020 ----
mean loss: 228.26
 ---- batch: 030 ----
mean loss: 230.83
 ---- batch: 040 ----
mean loss: 223.43
 ---- batch: 050 ----
mean loss: 227.09
 ---- batch: 060 ----
mean loss: 223.32
 ---- batch: 070 ----
mean loss: 219.42
 ---- batch: 080 ----
mean loss: 234.07
 ---- batch: 090 ----
mean loss: 225.15
train mean loss: 226.34
epoch train time: 0:00:02.625256
elapsed time: 0:10:10.163724
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 17:45:29.106695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.30
 ---- batch: 020 ----
mean loss: 230.33
 ---- batch: 030 ----
mean loss: 230.36
 ---- batch: 040 ----
mean loss: 228.57
 ---- batch: 050 ----
mean loss: 220.87
 ---- batch: 060 ----
mean loss: 224.72
 ---- batch: 070 ----
mean loss: 222.25
 ---- batch: 080 ----
mean loss: 229.89
 ---- batch: 090 ----
mean loss: 224.92
train mean loss: 226.53
epoch train time: 0:00:02.631596
elapsed time: 0:10:12.795833
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 17:45:31.738811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.37
 ---- batch: 020 ----
mean loss: 221.66
 ---- batch: 030 ----
mean loss: 231.98
 ---- batch: 040 ----
mean loss: 229.90
 ---- batch: 050 ----
mean loss: 234.70
 ---- batch: 060 ----
mean loss: 217.15
 ---- batch: 070 ----
mean loss: 222.79
 ---- batch: 080 ----
mean loss: 234.32
 ---- batch: 090 ----
mean loss: 226.43
train mean loss: 226.20
epoch train time: 0:00:02.593214
elapsed time: 0:10:15.389558
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 17:45:34.332533
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.53
 ---- batch: 020 ----
mean loss: 222.08
 ---- batch: 030 ----
mean loss: 227.48
 ---- batch: 040 ----
mean loss: 221.20
 ---- batch: 050 ----
mean loss: 229.49
 ---- batch: 060 ----
mean loss: 225.16
 ---- batch: 070 ----
mean loss: 228.01
 ---- batch: 080 ----
mean loss: 234.81
 ---- batch: 090 ----
mean loss: 223.17
train mean loss: 226.51
epoch train time: 0:00:02.584702
elapsed time: 0:10:17.974794
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 17:45:36.917780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 218.51
 ---- batch: 020 ----
mean loss: 222.16
 ---- batch: 030 ----
mean loss: 227.00
 ---- batch: 040 ----
mean loss: 225.24
 ---- batch: 050 ----
mean loss: 228.73
 ---- batch: 060 ----
mean loss: 233.26
 ---- batch: 070 ----
mean loss: 226.26
 ---- batch: 080 ----
mean loss: 235.77
 ---- batch: 090 ----
mean loss: 215.70
train mean loss: 226.05
epoch train time: 0:00:02.584207
elapsed time: 0:10:20.559501
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 17:45:39.502518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.79
 ---- batch: 020 ----
mean loss: 224.82
 ---- batch: 030 ----
mean loss: 222.56
 ---- batch: 040 ----
mean loss: 221.50
 ---- batch: 050 ----
mean loss: 220.31
 ---- batch: 060 ----
mean loss: 227.69
 ---- batch: 070 ----
mean loss: 227.62
 ---- batch: 080 ----
mean loss: 238.38
 ---- batch: 090 ----
mean loss: 229.68
train mean loss: 226.14
epoch train time: 0:00:02.602367
elapsed time: 0:10:23.162449
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 17:45:42.105400
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.08
 ---- batch: 020 ----
mean loss: 222.55
 ---- batch: 030 ----
mean loss: 230.87
 ---- batch: 040 ----
mean loss: 223.33
 ---- batch: 050 ----
mean loss: 227.56
 ---- batch: 060 ----
mean loss: 221.74
 ---- batch: 070 ----
mean loss: 227.11
 ---- batch: 080 ----
mean loss: 218.69
 ---- batch: 090 ----
mean loss: 237.49
train mean loss: 226.09
epoch train time: 0:00:02.597135
elapsed time: 0:10:25.760072
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 17:45:44.703031
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.33
 ---- batch: 020 ----
mean loss: 218.86
 ---- batch: 030 ----
mean loss: 224.49
 ---- batch: 040 ----
mean loss: 227.31
 ---- batch: 050 ----
mean loss: 227.16
 ---- batch: 060 ----
mean loss: 223.19
 ---- batch: 070 ----
mean loss: 226.63
 ---- batch: 080 ----
mean loss: 237.37
 ---- batch: 090 ----
mean loss: 229.49
train mean loss: 226.05
epoch train time: 0:00:02.589854
elapsed time: 0:10:28.350394
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 17:45:47.293359
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.62
 ---- batch: 020 ----
mean loss: 231.79
 ---- batch: 030 ----
mean loss: 223.70
 ---- batch: 040 ----
mean loss: 223.57
 ---- batch: 050 ----
mean loss: 226.50
 ---- batch: 060 ----
mean loss: 223.29
 ---- batch: 070 ----
mean loss: 227.00
 ---- batch: 080 ----
mean loss: 223.51
 ---- batch: 090 ----
mean loss: 227.49
train mean loss: 225.99
epoch train time: 0:00:02.583315
elapsed time: 0:10:30.934213
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 17:45:49.877172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.69
 ---- batch: 020 ----
mean loss: 230.53
 ---- batch: 030 ----
mean loss: 227.83
 ---- batch: 040 ----
mean loss: 218.97
 ---- batch: 050 ----
mean loss: 224.24
 ---- batch: 060 ----
mean loss: 227.93
 ---- batch: 070 ----
mean loss: 217.60
 ---- batch: 080 ----
mean loss: 227.63
 ---- batch: 090 ----
mean loss: 233.31
train mean loss: 226.14
epoch train time: 0:00:02.553807
elapsed time: 0:10:33.488460
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 17:45:52.431419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.24
 ---- batch: 020 ----
mean loss: 231.68
 ---- batch: 030 ----
mean loss: 222.70
 ---- batch: 040 ----
mean loss: 229.24
 ---- batch: 050 ----
mean loss: 232.24
 ---- batch: 060 ----
mean loss: 221.50
 ---- batch: 070 ----
mean loss: 226.59
 ---- batch: 080 ----
mean loss: 227.43
 ---- batch: 090 ----
mean loss: 223.49
train mean loss: 226.03
epoch train time: 0:00:02.596276
elapsed time: 0:10:36.085178
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 17:45:55.028137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.06
 ---- batch: 020 ----
mean loss: 218.32
 ---- batch: 030 ----
mean loss: 227.86
 ---- batch: 040 ----
mean loss: 226.36
 ---- batch: 050 ----
mean loss: 221.01
 ---- batch: 060 ----
mean loss: 227.75
 ---- batch: 070 ----
mean loss: 230.82
 ---- batch: 080 ----
mean loss: 221.41
 ---- batch: 090 ----
mean loss: 226.62
train mean loss: 226.05
epoch train time: 0:00:02.563901
elapsed time: 0:10:38.649523
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 17:45:57.592482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.45
 ---- batch: 020 ----
mean loss: 225.58
 ---- batch: 030 ----
mean loss: 222.40
 ---- batch: 040 ----
mean loss: 232.71
 ---- batch: 050 ----
mean loss: 230.30
 ---- batch: 060 ----
mean loss: 220.54
 ---- batch: 070 ----
mean loss: 230.37
 ---- batch: 080 ----
mean loss: 220.87
 ---- batch: 090 ----
mean loss: 234.70
train mean loss: 225.98
epoch train time: 0:00:02.572064
elapsed time: 0:10:41.222018
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 17:46:00.165038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.82
 ---- batch: 020 ----
mean loss: 229.45
 ---- batch: 030 ----
mean loss: 235.04
 ---- batch: 040 ----
mean loss: 220.78
 ---- batch: 050 ----
mean loss: 217.87
 ---- batch: 060 ----
mean loss: 219.51
 ---- batch: 070 ----
mean loss: 227.62
 ---- batch: 080 ----
mean loss: 229.66
 ---- batch: 090 ----
mean loss: 226.02
train mean loss: 226.10
epoch train time: 0:00:02.591019
elapsed time: 0:10:43.813590
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 17:46:02.756570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 234.04
 ---- batch: 020 ----
mean loss: 221.74
 ---- batch: 030 ----
mean loss: 227.40
 ---- batch: 040 ----
mean loss: 223.19
 ---- batch: 050 ----
mean loss: 221.25
 ---- batch: 060 ----
mean loss: 218.94
 ---- batch: 070 ----
mean loss: 222.11
 ---- batch: 080 ----
mean loss: 226.49
 ---- batch: 090 ----
mean loss: 234.04
train mean loss: 225.77
epoch train time: 0:00:02.597499
elapsed time: 0:10:46.411563
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 17:46:05.354535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 233.68
 ---- batch: 020 ----
mean loss: 227.35
 ---- batch: 030 ----
mean loss: 229.71
 ---- batch: 040 ----
mean loss: 222.74
 ---- batch: 050 ----
mean loss: 218.78
 ---- batch: 060 ----
mean loss: 229.35
 ---- batch: 070 ----
mean loss: 225.89
 ---- batch: 080 ----
mean loss: 221.83
 ---- batch: 090 ----
mean loss: 229.23
train mean loss: 225.80
epoch train time: 0:00:02.578920
elapsed time: 0:10:48.990982
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 17:46:07.933959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.27
 ---- batch: 020 ----
mean loss: 222.83
 ---- batch: 030 ----
mean loss: 220.97
 ---- batch: 040 ----
mean loss: 221.98
 ---- batch: 050 ----
mean loss: 227.19
 ---- batch: 060 ----
mean loss: 225.93
 ---- batch: 070 ----
mean loss: 219.99
 ---- batch: 080 ----
mean loss: 230.67
 ---- batch: 090 ----
mean loss: 227.79
train mean loss: 225.90
epoch train time: 0:00:02.582903
elapsed time: 0:10:51.574419
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 17:46:10.517259
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.38
 ---- batch: 020 ----
mean loss: 224.44
 ---- batch: 030 ----
mean loss: 226.49
 ---- batch: 040 ----
mean loss: 232.14
 ---- batch: 050 ----
mean loss: 227.54
 ---- batch: 060 ----
mean loss: 225.21
 ---- batch: 070 ----
mean loss: 230.67
 ---- batch: 080 ----
mean loss: 221.68
 ---- batch: 090 ----
mean loss: 223.06
train mean loss: 225.88
epoch train time: 0:00:02.584764
elapsed time: 0:10:54.159517
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 17:46:13.102491
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.20
 ---- batch: 020 ----
mean loss: 220.72
 ---- batch: 030 ----
mean loss: 225.88
 ---- batch: 040 ----
mean loss: 219.05
 ---- batch: 050 ----
mean loss: 229.96
 ---- batch: 060 ----
mean loss: 234.79
 ---- batch: 070 ----
mean loss: 223.20
 ---- batch: 080 ----
mean loss: 225.92
 ---- batch: 090 ----
mean loss: 224.13
train mean loss: 225.71
epoch train time: 0:00:02.582469
elapsed time: 0:10:56.742465
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 17:46:15.685441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.26
 ---- batch: 020 ----
mean loss: 228.34
 ---- batch: 030 ----
mean loss: 224.04
 ---- batch: 040 ----
mean loss: 232.87
 ---- batch: 050 ----
mean loss: 225.98
 ---- batch: 060 ----
mean loss: 217.96
 ---- batch: 070 ----
mean loss: 232.58
 ---- batch: 080 ----
mean loss: 226.29
 ---- batch: 090 ----
mean loss: 226.35
train mean loss: 225.75
epoch train time: 0:00:02.603369
elapsed time: 0:10:59.346307
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 17:46:18.289273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.00
 ---- batch: 020 ----
mean loss: 220.85
 ---- batch: 030 ----
mean loss: 229.29
 ---- batch: 040 ----
mean loss: 226.64
 ---- batch: 050 ----
mean loss: 225.03
 ---- batch: 060 ----
mean loss: 219.35
 ---- batch: 070 ----
mean loss: 227.42
 ---- batch: 080 ----
mean loss: 228.69
 ---- batch: 090 ----
mean loss: 226.86
train mean loss: 226.00
epoch train time: 0:00:02.611599
elapsed time: 0:11:01.958429
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 17:46:20.901417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.75
 ---- batch: 020 ----
mean loss: 223.78
 ---- batch: 030 ----
mean loss: 227.28
 ---- batch: 040 ----
mean loss: 222.38
 ---- batch: 050 ----
mean loss: 228.70
 ---- batch: 060 ----
mean loss: 218.61
 ---- batch: 070 ----
mean loss: 223.17
 ---- batch: 080 ----
mean loss: 226.14
 ---- batch: 090 ----
mean loss: 229.80
train mean loss: 226.00
epoch train time: 0:00:02.636106
elapsed time: 0:11:04.595103
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 17:46:23.538103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.39
 ---- batch: 020 ----
mean loss: 228.38
 ---- batch: 030 ----
mean loss: 222.12
 ---- batch: 040 ----
mean loss: 218.21
 ---- batch: 050 ----
mean loss: 223.06
 ---- batch: 060 ----
mean loss: 234.26
 ---- batch: 070 ----
mean loss: 220.39
 ---- batch: 080 ----
mean loss: 228.44
 ---- batch: 090 ----
mean loss: 229.19
train mean loss: 225.81
epoch train time: 0:00:02.628571
elapsed time: 0:11:07.224237
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 17:46:26.167193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 229.27
 ---- batch: 020 ----
mean loss: 223.19
 ---- batch: 030 ----
mean loss: 225.55
 ---- batch: 040 ----
mean loss: 224.06
 ---- batch: 050 ----
mean loss: 225.54
 ---- batch: 060 ----
mean loss: 219.75
 ---- batch: 070 ----
mean loss: 226.44
 ---- batch: 080 ----
mean loss: 229.76
 ---- batch: 090 ----
mean loss: 222.66
train mean loss: 225.93
epoch train time: 0:00:02.589606
elapsed time: 0:11:09.814311
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 17:46:28.757274
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.51
 ---- batch: 020 ----
mean loss: 222.80
 ---- batch: 030 ----
mean loss: 232.01
 ---- batch: 040 ----
mean loss: 217.59
 ---- batch: 050 ----
mean loss: 226.55
 ---- batch: 060 ----
mean loss: 218.61
 ---- batch: 070 ----
mean loss: 231.11
 ---- batch: 080 ----
mean loss: 219.50
 ---- batch: 090 ----
mean loss: 230.75
train mean loss: 225.94
epoch train time: 0:00:02.583504
elapsed time: 0:11:12.398254
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 17:46:31.341222
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.74
 ---- batch: 020 ----
mean loss: 221.55
 ---- batch: 030 ----
mean loss: 230.01
 ---- batch: 040 ----
mean loss: 218.70
 ---- batch: 050 ----
mean loss: 220.27
 ---- batch: 060 ----
mean loss: 224.07
 ---- batch: 070 ----
mean loss: 226.77
 ---- batch: 080 ----
mean loss: 224.45
 ---- batch: 090 ----
mean loss: 234.50
train mean loss: 225.76
epoch train time: 0:00:02.661357
elapsed time: 0:11:15.060247
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 17:46:34.003237
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 223.91
 ---- batch: 020 ----
mean loss: 229.54
 ---- batch: 030 ----
mean loss: 224.50
 ---- batch: 040 ----
mean loss: 216.70
 ---- batch: 050 ----
mean loss: 224.07
 ---- batch: 060 ----
mean loss: 232.11
 ---- batch: 070 ----
mean loss: 229.29
 ---- batch: 080 ----
mean loss: 217.35
 ---- batch: 090 ----
mean loss: 232.86
train mean loss: 225.45
epoch train time: 0:00:02.640822
elapsed time: 0:11:17.701670
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 17:46:36.644630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.66
 ---- batch: 020 ----
mean loss: 235.17
 ---- batch: 030 ----
mean loss: 219.48
 ---- batch: 040 ----
mean loss: 223.98
 ---- batch: 050 ----
mean loss: 229.40
 ---- batch: 060 ----
mean loss: 218.16
 ---- batch: 070 ----
mean loss: 224.06
 ---- batch: 080 ----
mean loss: 222.83
 ---- batch: 090 ----
mean loss: 230.44
train mean loss: 225.39
epoch train time: 0:00:02.645979
elapsed time: 0:11:20.348155
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 17:46:39.291117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.36
 ---- batch: 020 ----
mean loss: 223.76
 ---- batch: 030 ----
mean loss: 223.53
 ---- batch: 040 ----
mean loss: 229.09
 ---- batch: 050 ----
mean loss: 226.94
 ---- batch: 060 ----
mean loss: 225.18
 ---- batch: 070 ----
mean loss: 220.93
 ---- batch: 080 ----
mean loss: 233.36
 ---- batch: 090 ----
mean loss: 227.76
train mean loss: 225.54
epoch train time: 0:00:02.613629
elapsed time: 0:11:22.962387
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 17:46:41.905356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.61
 ---- batch: 020 ----
mean loss: 227.96
 ---- batch: 030 ----
mean loss: 220.25
 ---- batch: 040 ----
mean loss: 232.03
 ---- batch: 050 ----
mean loss: 226.02
 ---- batch: 060 ----
mean loss: 216.01
 ---- batch: 070 ----
mean loss: 223.90
 ---- batch: 080 ----
mean loss: 220.26
 ---- batch: 090 ----
mean loss: 232.67
train mean loss: 225.53
epoch train time: 0:00:02.613407
elapsed time: 0:11:25.576330
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 17:46:44.519314
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.57
 ---- batch: 020 ----
mean loss: 223.89
 ---- batch: 030 ----
mean loss: 227.32
 ---- batch: 040 ----
mean loss: 231.23
 ---- batch: 050 ----
mean loss: 227.55
 ---- batch: 060 ----
mean loss: 229.19
 ---- batch: 070 ----
mean loss: 216.65
 ---- batch: 080 ----
mean loss: 217.11
 ---- batch: 090 ----
mean loss: 228.87
train mean loss: 225.33
epoch train time: 0:00:02.617818
elapsed time: 0:11:28.194731
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 17:46:47.137728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.17
 ---- batch: 020 ----
mean loss: 227.50
 ---- batch: 030 ----
mean loss: 224.60
 ---- batch: 040 ----
mean loss: 229.68
 ---- batch: 050 ----
mean loss: 220.71
 ---- batch: 060 ----
mean loss: 226.12
 ---- batch: 070 ----
mean loss: 235.49
 ---- batch: 080 ----
mean loss: 219.55
 ---- batch: 090 ----
mean loss: 222.39
train mean loss: 225.49
epoch train time: 0:00:02.631758
elapsed time: 0:11:30.827065
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 17:46:49.770029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.20
 ---- batch: 020 ----
mean loss: 228.48
 ---- batch: 030 ----
mean loss: 222.35
 ---- batch: 040 ----
mean loss: 218.22
 ---- batch: 050 ----
mean loss: 236.45
 ---- batch: 060 ----
mean loss: 223.56
 ---- batch: 070 ----
mean loss: 222.89
 ---- batch: 080 ----
mean loss: 230.03
 ---- batch: 090 ----
mean loss: 231.05
train mean loss: 225.25
epoch train time: 0:00:02.633196
elapsed time: 0:11:33.464652
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_1/checkpoint.pth.tar
**** end time: 2019-09-25 17:46:52.407461 ****
