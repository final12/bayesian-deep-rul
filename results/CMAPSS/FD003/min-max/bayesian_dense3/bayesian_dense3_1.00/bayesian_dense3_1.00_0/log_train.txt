Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_0', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5593
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-09-20 21:33:46.228641 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:33:46.238211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4475.64
 ---- batch: 020 ----
mean loss: 4147.56
 ---- batch: 030 ----
mean loss: 3901.90
 ---- batch: 040 ----
mean loss: 3600.12
train mean loss: 3989.85
epoch train time: 0:00:14.925907
elapsed time: 0:00:14.941555
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:34:01.170252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3382.71
 ---- batch: 020 ----
mean loss: 3173.80
 ---- batch: 030 ----
mean loss: 3035.45
 ---- batch: 040 ----
mean loss: 2989.24
train mean loss: 3126.79
epoch train time: 0:00:00.689062
elapsed time: 0:00:15.630880
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:34:01.859612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2800.50
 ---- batch: 020 ----
mean loss: 2754.18
 ---- batch: 030 ----
mean loss: 2735.74
 ---- batch: 040 ----
mean loss: 2637.99
train mean loss: 2719.28
epoch train time: 0:00:00.693536
elapsed time: 0:00:16.324693
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:34:02.553430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2547.16
 ---- batch: 020 ----
mean loss: 2576.21
 ---- batch: 030 ----
mean loss: 2402.70
 ---- batch: 040 ----
mean loss: 2458.04
train mean loss: 2497.24
epoch train time: 0:00:00.669843
elapsed time: 0:00:16.994782
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:34:03.223495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2410.67
 ---- batch: 020 ----
mean loss: 2341.03
 ---- batch: 030 ----
mean loss: 2337.89
 ---- batch: 040 ----
mean loss: 2273.19
train mean loss: 2336.03
epoch train time: 0:00:00.654988
elapsed time: 0:00:17.649978
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:34:03.878692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2221.99
 ---- batch: 020 ----
mean loss: 2243.56
 ---- batch: 030 ----
mean loss: 2180.90
 ---- batch: 040 ----
mean loss: 2141.41
train mean loss: 2194.68
epoch train time: 0:00:00.639233
elapsed time: 0:00:18.289411
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:34:04.518113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2093.40
 ---- batch: 020 ----
mean loss: 2124.88
 ---- batch: 030 ----
mean loss: 2067.50
 ---- batch: 040 ----
mean loss: 1997.40
train mean loss: 2068.04
epoch train time: 0:00:00.679635
elapsed time: 0:00:18.969290
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:34:05.198012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2007.18
 ---- batch: 020 ----
mean loss: 1930.17
 ---- batch: 030 ----
mean loss: 1972.16
 ---- batch: 040 ----
mean loss: 1915.77
train mean loss: 1954.32
epoch train time: 0:00:00.690806
elapsed time: 0:00:19.660340
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:34:05.889063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1943.03
 ---- batch: 020 ----
mean loss: 1856.26
 ---- batch: 030 ----
mean loss: 1835.85
 ---- batch: 040 ----
mean loss: 1804.02
train mean loss: 1857.17
epoch train time: 0:00:00.669386
elapsed time: 0:00:20.329953
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:34:06.558662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1785.42
 ---- batch: 020 ----
mean loss: 1754.39
 ---- batch: 030 ----
mean loss: 1744.42
 ---- batch: 040 ----
mean loss: 1752.12
train mean loss: 1754.05
epoch train time: 0:00:00.667197
elapsed time: 0:00:20.997378
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:34:07.226087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1684.57
 ---- batch: 020 ----
mean loss: 1684.80
 ---- batch: 030 ----
mean loss: 1670.97
 ---- batch: 040 ----
mean loss: 1612.20
train mean loss: 1664.27
epoch train time: 0:00:00.655581
elapsed time: 0:00:21.653164
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:34:07.881876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1643.04
 ---- batch: 020 ----
mean loss: 1569.19
 ---- batch: 030 ----
mean loss: 1553.44
 ---- batch: 040 ----
mean loss: 1549.26
train mean loss: 1576.65
epoch train time: 0:00:00.673679
elapsed time: 0:00:22.327149
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:34:08.555868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1506.37
 ---- batch: 020 ----
mean loss: 1495.28
 ---- batch: 030 ----
mean loss: 1454.39
 ---- batch: 040 ----
mean loss: 1452.16
train mean loss: 1474.07
epoch train time: 0:00:00.702601
elapsed time: 0:00:23.030047
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:34:09.258777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1407.97
 ---- batch: 020 ----
mean loss: 1387.88
 ---- batch: 030 ----
mean loss: 1354.01
 ---- batch: 040 ----
mean loss: 1349.77
train mean loss: 1373.77
epoch train time: 0:00:00.692910
elapsed time: 0:00:23.723237
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:34:09.951988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1320.80
 ---- batch: 020 ----
mean loss: 1284.74
 ---- batch: 030 ----
mean loss: 1266.50
 ---- batch: 040 ----
mean loss: 1270.20
train mean loss: 1281.48
epoch train time: 0:00:00.647515
elapsed time: 0:00:24.371001
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:34:10.599715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1224.17
 ---- batch: 020 ----
mean loss: 1227.09
 ---- batch: 030 ----
mean loss: 1205.00
 ---- batch: 040 ----
mean loss: 1181.59
train mean loss: 1207.42
epoch train time: 0:00:00.661002
elapsed time: 0:00:25.032226
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:34:11.260941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1149.82
 ---- batch: 020 ----
mean loss: 1145.37
 ---- batch: 030 ----
mean loss: 1131.10
 ---- batch: 040 ----
mean loss: 1123.91
train mean loss: 1137.30
epoch train time: 0:00:00.646394
elapsed time: 0:00:25.678822
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:34:11.907540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1088.48
 ---- batch: 020 ----
mean loss: 1079.62
 ---- batch: 030 ----
mean loss: 1060.22
 ---- batch: 040 ----
mean loss: 1051.45
train mean loss: 1067.61
epoch train time: 0:00:00.661863
elapsed time: 0:00:26.340911
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:34:12.569622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1038.00
 ---- batch: 020 ----
mean loss: 991.35
 ---- batch: 030 ----
mean loss: 1009.32
 ---- batch: 040 ----
mean loss: 972.54
train mean loss: 999.70
epoch train time: 0:00:00.690834
elapsed time: 0:00:27.031992
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:34:13.260727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.66
 ---- batch: 020 ----
mean loss: 953.07
 ---- batch: 030 ----
mean loss: 949.55
 ---- batch: 040 ----
mean loss: 903.69
train mean loss: 939.99
epoch train time: 0:00:00.690207
elapsed time: 0:00:27.722422
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:34:13.951140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.13
 ---- batch: 020 ----
mean loss: 893.15
 ---- batch: 030 ----
mean loss: 880.73
 ---- batch: 040 ----
mean loss: 864.97
train mean loss: 881.96
epoch train time: 0:00:00.638538
elapsed time: 0:00:28.361171
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:34:14.589891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.61
 ---- batch: 020 ----
mean loss: 820.14
 ---- batch: 030 ----
mean loss: 824.48
 ---- batch: 040 ----
mean loss: 817.95
train mean loss: 826.72
epoch train time: 0:00:00.655813
elapsed time: 0:00:29.017187
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:34:15.245887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 796.14
 ---- batch: 020 ----
mean loss: 786.89
 ---- batch: 030 ----
mean loss: 775.71
 ---- batch: 040 ----
mean loss: 748.19
train mean loss: 775.79
epoch train time: 0:00:00.647727
elapsed time: 0:00:29.665164
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:34:15.893898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 752.01
 ---- batch: 020 ----
mean loss: 747.57
 ---- batch: 030 ----
mean loss: 726.91
 ---- batch: 040 ----
mean loss: 710.86
train mean loss: 730.43
epoch train time: 0:00:00.686840
elapsed time: 0:00:30.352258
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:34:16.580986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 718.17
 ---- batch: 020 ----
mean loss: 686.23
 ---- batch: 030 ----
mean loss: 681.15
 ---- batch: 040 ----
mean loss: 669.00
train mean loss: 685.96
epoch train time: 0:00:00.709233
elapsed time: 0:00:31.061778
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:34:17.290500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 665.07
 ---- batch: 020 ----
mean loss: 660.28
 ---- batch: 030 ----
mean loss: 637.98
 ---- batch: 040 ----
mean loss: 625.76
train mean loss: 643.61
epoch train time: 0:00:00.686569
elapsed time: 0:00:31.748586
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:34:17.977310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 626.03
 ---- batch: 020 ----
mean loss: 612.77
 ---- batch: 030 ----
mean loss: 595.16
 ---- batch: 040 ----
mean loss: 591.79
train mean loss: 605.15
epoch train time: 0:00:00.659263
elapsed time: 0:00:32.408069
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:34:18.636781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 586.70
 ---- batch: 020 ----
mean loss: 571.35
 ---- batch: 030 ----
mean loss: 565.68
 ---- batch: 040 ----
mean loss: 557.38
train mean loss: 567.81
epoch train time: 0:00:00.654097
elapsed time: 0:00:33.062406
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:34:19.291151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 554.25
 ---- batch: 020 ----
mean loss: 540.11
 ---- batch: 030 ----
mean loss: 529.04
 ---- batch: 040 ----
mean loss: 523.03
train mean loss: 534.94
epoch train time: 0:00:00.672805
elapsed time: 0:00:33.735506
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:34:19.964226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.59
 ---- batch: 020 ----
mean loss: 497.90
 ---- batch: 030 ----
mean loss: 503.36
 ---- batch: 040 ----
mean loss: 494.20
train mean loss: 501.00
epoch train time: 0:00:00.673043
elapsed time: 0:00:34.408822
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:34:20.637548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.18
 ---- batch: 020 ----
mean loss: 479.24
 ---- batch: 030 ----
mean loss: 473.05
 ---- batch: 040 ----
mean loss: 464.54
train mean loss: 469.95
epoch train time: 0:00:00.690461
elapsed time: 0:00:35.099559
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:34:21.328275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.64
 ---- batch: 020 ----
mean loss: 452.83
 ---- batch: 030 ----
mean loss: 439.93
 ---- batch: 040 ----
mean loss: 432.29
train mean loss: 442.91
epoch train time: 0:00:00.668145
elapsed time: 0:00:35.767911
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:34:21.996619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.74
 ---- batch: 020 ----
mean loss: 421.07
 ---- batch: 030 ----
mean loss: 408.19
 ---- batch: 040 ----
mean loss: 416.73
train mean loss: 415.72
epoch train time: 0:00:00.626681
elapsed time: 0:00:36.394830
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:34:22.623544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.69
 ---- batch: 020 ----
mean loss: 391.24
 ---- batch: 030 ----
mean loss: 385.52
 ---- batch: 040 ----
mean loss: 385.32
train mean loss: 390.35
epoch train time: 0:00:00.640660
elapsed time: 0:00:37.035720
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:34:23.264478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.34
 ---- batch: 020 ----
mean loss: 372.73
 ---- batch: 030 ----
mean loss: 367.70
 ---- batch: 040 ----
mean loss: 357.24
train mean loss: 369.36
epoch train time: 0:00:00.699144
elapsed time: 0:00:37.735158
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:34:23.963881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.58
 ---- batch: 020 ----
mean loss: 350.42
 ---- batch: 030 ----
mean loss: 345.19
 ---- batch: 040 ----
mean loss: 343.45
train mean loss: 346.85
epoch train time: 0:00:00.684671
elapsed time: 0:00:38.420103
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:34:24.648829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.72
 ---- batch: 020 ----
mean loss: 330.25
 ---- batch: 030 ----
mean loss: 324.80
 ---- batch: 040 ----
mean loss: 322.59
train mean loss: 327.15
epoch train time: 0:00:00.663609
elapsed time: 0:00:39.084021
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:34:25.312739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.72
 ---- batch: 020 ----
mean loss: 306.69
 ---- batch: 030 ----
mean loss: 305.28
 ---- batch: 040 ----
mean loss: 303.18
train mean loss: 306.93
epoch train time: 0:00:00.634047
elapsed time: 0:00:39.718326
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:34:25.947068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.97
 ---- batch: 020 ----
mean loss: 293.66
 ---- batch: 030 ----
mean loss: 281.48
 ---- batch: 040 ----
mean loss: 288.41
train mean loss: 289.87
epoch train time: 0:00:00.627438
elapsed time: 0:00:40.346031
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:34:26.574767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.14
 ---- batch: 020 ----
mean loss: 271.69
 ---- batch: 030 ----
mean loss: 274.13
 ---- batch: 040 ----
mean loss: 268.80
train mean loss: 273.97
epoch train time: 0:00:00.663846
elapsed time: 0:00:41.010162
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:34:27.238893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.76
 ---- batch: 020 ----
mean loss: 261.08
 ---- batch: 030 ----
mean loss: 259.20
 ---- batch: 040 ----
mean loss: 253.31
train mean loss: 258.66
epoch train time: 0:00:00.705785
elapsed time: 0:00:41.716227
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:34:27.944998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.50
 ---- batch: 020 ----
mean loss: 239.41
 ---- batch: 030 ----
mean loss: 249.32
 ---- batch: 040 ----
mean loss: 242.00
train mean loss: 244.20
epoch train time: 0:00:00.688140
elapsed time: 0:00:42.404710
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:34:28.633439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.02
 ---- batch: 020 ----
mean loss: 230.67
 ---- batch: 030 ----
mean loss: 229.68
 ---- batch: 040 ----
mean loss: 227.91
train mean loss: 230.96
epoch train time: 0:00:00.660114
elapsed time: 0:00:43.065044
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:34:29.293762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.90
 ---- batch: 020 ----
mean loss: 228.33
 ---- batch: 030 ----
mean loss: 214.32
 ---- batch: 040 ----
mean loss: 210.25
train mean loss: 218.60
epoch train time: 0:00:00.655761
elapsed time: 0:00:43.721015
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:34:29.949737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.21
 ---- batch: 020 ----
mean loss: 210.65
 ---- batch: 030 ----
mean loss: 205.98
 ---- batch: 040 ----
mean loss: 203.77
train mean loss: 207.83
epoch train time: 0:00:00.640156
elapsed time: 0:00:44.361415
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:34:30.590142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.88
 ---- batch: 020 ----
mean loss: 201.22
 ---- batch: 030 ----
mean loss: 196.58
 ---- batch: 040 ----
mean loss: 191.52
train mean loss: 197.73
epoch train time: 0:00:00.689916
elapsed time: 0:00:45.051608
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:34:31.280335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.52
 ---- batch: 020 ----
mean loss: 185.99
 ---- batch: 030 ----
mean loss: 194.39
 ---- batch: 040 ----
mean loss: 186.18
train mean loss: 188.64
epoch train time: 0:00:00.686923
elapsed time: 0:00:45.738808
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:34:31.967529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.52
 ---- batch: 020 ----
mean loss: 180.70
 ---- batch: 030 ----
mean loss: 175.91
 ---- batch: 040 ----
mean loss: 177.06
train mean loss: 178.31
epoch train time: 0:00:00.666391
elapsed time: 0:00:46.405467
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:34:32.634163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.85
 ---- batch: 020 ----
mean loss: 169.43
 ---- batch: 030 ----
mean loss: 166.28
 ---- batch: 040 ----
mean loss: 170.74
train mean loss: 168.86
epoch train time: 0:00:00.649893
elapsed time: 0:00:47.055577
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:34:33.284301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.40
 ---- batch: 020 ----
mean loss: 163.28
 ---- batch: 030 ----
mean loss: 158.64
 ---- batch: 040 ----
mean loss: 162.55
train mean loss: 162.58
epoch train time: 0:00:00.666391
elapsed time: 0:00:47.722227
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:34:33.950957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.48
 ---- batch: 020 ----
mean loss: 158.47
 ---- batch: 030 ----
mean loss: 153.35
 ---- batch: 040 ----
mean loss: 154.75
train mean loss: 156.56
epoch train time: 0:00:00.663563
elapsed time: 0:00:48.386073
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:34:34.614796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.90
 ---- batch: 020 ----
mean loss: 148.41
 ---- batch: 030 ----
mean loss: 149.21
 ---- batch: 040 ----
mean loss: 145.12
train mean loss: 148.92
epoch train time: 0:00:00.697950
elapsed time: 0:00:49.084287
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:34:35.313013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.46
 ---- batch: 020 ----
mean loss: 143.69
 ---- batch: 030 ----
mean loss: 144.66
 ---- batch: 040 ----
mean loss: 139.89
train mean loss: 143.13
epoch train time: 0:00:00.693103
elapsed time: 0:00:49.777663
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:34:36.006391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.33
 ---- batch: 020 ----
mean loss: 138.43
 ---- batch: 030 ----
mean loss: 133.64
 ---- batch: 040 ----
mean loss: 135.20
train mean loss: 136.28
epoch train time: 0:00:00.668116
elapsed time: 0:00:50.446003
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:34:36.674721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.44
 ---- batch: 020 ----
mean loss: 130.46
 ---- batch: 030 ----
mean loss: 131.34
 ---- batch: 040 ----
mean loss: 128.61
train mean loss: 131.21
epoch train time: 0:00:00.647593
elapsed time: 0:00:51.093838
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:34:37.322567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.80
 ---- batch: 020 ----
mean loss: 128.91
 ---- batch: 030 ----
mean loss: 126.31
 ---- batch: 040 ----
mean loss: 122.41
train mean loss: 127.09
epoch train time: 0:00:00.671486
elapsed time: 0:00:51.765597
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:34:37.994318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.73
 ---- batch: 020 ----
mean loss: 121.92
 ---- batch: 030 ----
mean loss: 119.33
 ---- batch: 040 ----
mean loss: 122.16
train mean loss: 122.01
epoch train time: 0:00:00.690280
elapsed time: 0:00:52.456131
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:34:38.684878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.11
 ---- batch: 020 ----
mean loss: 118.07
 ---- batch: 030 ----
mean loss: 117.57
 ---- batch: 040 ----
mean loss: 113.66
train mean loss: 117.36
epoch train time: 0:00:00.690867
elapsed time: 0:00:53.147300
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:34:39.376022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.39
 ---- batch: 020 ----
mean loss: 116.06
 ---- batch: 030 ----
mean loss: 116.26
 ---- batch: 040 ----
mean loss: 113.92
train mean loss: 115.36
epoch train time: 0:00:00.678319
elapsed time: 0:00:53.825842
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:34:40.054592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.11
 ---- batch: 020 ----
mean loss: 110.83
 ---- batch: 030 ----
mean loss: 108.28
 ---- batch: 040 ----
mean loss: 108.79
train mean loss: 110.09
epoch train time: 0:00:00.656192
elapsed time: 0:00:54.482306
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:34:40.711024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.40
 ---- batch: 020 ----
mean loss: 106.07
 ---- batch: 030 ----
mean loss: 110.25
 ---- batch: 040 ----
mean loss: 104.70
train mean loss: 106.72
epoch train time: 0:00:00.663367
elapsed time: 0:00:55.145881
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:34:41.374609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.56
 ---- batch: 020 ----
mean loss: 102.79
 ---- batch: 030 ----
mean loss: 101.29
 ---- batch: 040 ----
mean loss: 104.46
train mean loss: 103.46
epoch train time: 0:00:00.653711
elapsed time: 0:00:55.799896
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:34:42.028662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.14
 ---- batch: 020 ----
mean loss: 101.85
 ---- batch: 030 ----
mean loss: 99.26
 ---- batch: 040 ----
mean loss: 101.51
train mean loss: 100.47
epoch train time: 0:00:00.680220
elapsed time: 0:00:56.480462
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:34:42.709217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.91
 ---- batch: 020 ----
mean loss: 95.94
 ---- batch: 030 ----
mean loss: 100.31
 ---- batch: 040 ----
mean loss: 95.91
train mean loss: 97.91
epoch train time: 0:00:00.709321
elapsed time: 0:00:57.190070
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:34:43.418792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.81
 ---- batch: 020 ----
mean loss: 94.89
 ---- batch: 030 ----
mean loss: 95.30
 ---- batch: 040 ----
mean loss: 97.83
train mean loss: 96.33
epoch train time: 0:00:00.677520
elapsed time: 0:00:57.867823
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:34:44.096538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.81
 ---- batch: 020 ----
mean loss: 95.65
 ---- batch: 030 ----
mean loss: 90.47
 ---- batch: 040 ----
mean loss: 92.63
train mean loss: 93.33
epoch train time: 0:00:00.649833
elapsed time: 0:00:58.517861
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:34:44.746573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.29
 ---- batch: 020 ----
mean loss: 93.00
 ---- batch: 030 ----
mean loss: 91.39
 ---- batch: 040 ----
mean loss: 90.95
train mean loss: 92.38
epoch train time: 0:00:00.670622
elapsed time: 0:00:59.188706
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:34:45.417435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.31
 ---- batch: 020 ----
mean loss: 90.82
 ---- batch: 030 ----
mean loss: 90.56
 ---- batch: 040 ----
mean loss: 87.50
train mean loss: 89.12
epoch train time: 0:00:00.697296
elapsed time: 0:00:59.886333
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:34:46.115069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.66
 ---- batch: 020 ----
mean loss: 86.66
 ---- batch: 030 ----
mean loss: 86.06
 ---- batch: 040 ----
mean loss: 88.80
train mean loss: 87.66
epoch train time: 0:00:00.668428
elapsed time: 0:01:00.555041
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:34:46.783794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.69
 ---- batch: 020 ----
mean loss: 83.33
 ---- batch: 030 ----
mean loss: 83.78
 ---- batch: 040 ----
mean loss: 87.41
train mean loss: 85.50
epoch train time: 0:00:00.673805
elapsed time: 0:01:01.229107
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:34:47.457830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.92
 ---- batch: 020 ----
mean loss: 81.84
 ---- batch: 030 ----
mean loss: 82.91
 ---- batch: 040 ----
mean loss: 86.21
train mean loss: 84.35
epoch train time: 0:00:00.645087
elapsed time: 0:01:01.874405
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:34:48.103131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.95
 ---- batch: 020 ----
mean loss: 81.29
 ---- batch: 030 ----
mean loss: 83.60
 ---- batch: 040 ----
mean loss: 80.29
train mean loss: 82.16
epoch train time: 0:00:00.643885
elapsed time: 0:01:02.518521
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:34:48.747226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.14
 ---- batch: 020 ----
mean loss: 87.46
 ---- batch: 030 ----
mean loss: 80.90
 ---- batch: 040 ----
mean loss: 82.70
train mean loss: 81.95
epoch train time: 0:00:00.648391
elapsed time: 0:01:03.167128
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:34:49.395855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.91
 ---- batch: 020 ----
mean loss: 78.79
 ---- batch: 030 ----
mean loss: 82.23
 ---- batch: 040 ----
mean loss: 79.77
train mean loss: 80.18
epoch train time: 0:00:00.674909
elapsed time: 0:01:03.842333
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:34:50.071107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.02
 ---- batch: 020 ----
mean loss: 80.32
 ---- batch: 030 ----
mean loss: 80.69
 ---- batch: 040 ----
mean loss: 78.15
train mean loss: 79.23
epoch train time: 0:00:00.673051
elapsed time: 0:01:04.515706
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:34:50.744437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.33
 ---- batch: 020 ----
mean loss: 76.34
 ---- batch: 030 ----
mean loss: 77.70
 ---- batch: 040 ----
mean loss: 74.07
train mean loss: 76.38
epoch train time: 0:00:00.657484
elapsed time: 0:01:05.173461
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:34:51.402192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.00
 ---- batch: 020 ----
mean loss: 74.26
 ---- batch: 030 ----
mean loss: 78.80
 ---- batch: 040 ----
mean loss: 78.90
train mean loss: 77.04
epoch train time: 0:00:00.658547
elapsed time: 0:01:05.832285
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:34:52.061007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.12
 ---- batch: 020 ----
mean loss: 76.73
 ---- batch: 030 ----
mean loss: 75.91
 ---- batch: 040 ----
mean loss: 74.87
train mean loss: 75.57
epoch train time: 0:00:00.643131
elapsed time: 0:01:06.475644
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:34:52.704363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.13
 ---- batch: 020 ----
mean loss: 72.75
 ---- batch: 030 ----
mean loss: 75.35
 ---- batch: 040 ----
mean loss: 73.97
train mean loss: 74.42
epoch train time: 0:00:00.662801
elapsed time: 0:01:07.138754
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:34:53.367470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.08
 ---- batch: 020 ----
mean loss: 73.00
 ---- batch: 030 ----
mean loss: 73.71
 ---- batch: 040 ----
mean loss: 74.76
train mean loss: 74.31
epoch train time: 0:00:00.700964
elapsed time: 0:01:07.839966
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:34:54.068674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.83
 ---- batch: 020 ----
mean loss: 71.54
 ---- batch: 030 ----
mean loss: 73.03
 ---- batch: 040 ----
mean loss: 72.39
train mean loss: 73.44
epoch train time: 0:00:00.669078
elapsed time: 0:01:08.509262
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:34:54.737974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.56
 ---- batch: 020 ----
mean loss: 72.26
 ---- batch: 030 ----
mean loss: 73.66
 ---- batch: 040 ----
mean loss: 72.86
train mean loss: 72.32
epoch train time: 0:00:00.660240
elapsed time: 0:01:09.169902
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:34:55.398687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.05
 ---- batch: 020 ----
mean loss: 68.93
 ---- batch: 030 ----
mean loss: 70.12
 ---- batch: 040 ----
mean loss: 72.16
train mean loss: 71.49
epoch train time: 0:00:00.655947
elapsed time: 0:01:09.826137
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:34:56.054853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.25
 ---- batch: 020 ----
mean loss: 72.84
 ---- batch: 030 ----
mean loss: 68.47
 ---- batch: 040 ----
mean loss: 72.97
train mean loss: 70.79
epoch train time: 0:00:00.669304
elapsed time: 0:01:10.495644
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:34:56.724350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.00
 ---- batch: 020 ----
mean loss: 69.51
 ---- batch: 030 ----
mean loss: 69.36
 ---- batch: 040 ----
mean loss: 69.18
train mean loss: 70.46
epoch train time: 0:00:00.678425
elapsed time: 0:01:11.174320
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:34:57.403052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.58
 ---- batch: 020 ----
mean loss: 69.48
 ---- batch: 030 ----
mean loss: 70.08
 ---- batch: 040 ----
mean loss: 70.05
train mean loss: 69.97
epoch train time: 0:00:00.679396
elapsed time: 0:01:11.854011
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:34:58.082742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.27
 ---- batch: 020 ----
mean loss: 69.58
 ---- batch: 030 ----
mean loss: 66.48
 ---- batch: 040 ----
mean loss: 68.30
train mean loss: 68.21
epoch train time: 0:00:00.670723
elapsed time: 0:01:12.524981
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:34:58.753707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.74
 ---- batch: 020 ----
mean loss: 68.82
 ---- batch: 030 ----
mean loss: 69.10
 ---- batch: 040 ----
mean loss: 67.16
train mean loss: 68.82
epoch train time: 0:00:00.674044
elapsed time: 0:01:13.199296
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:34:59.428012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.22
 ---- batch: 020 ----
mean loss: 68.11
 ---- batch: 030 ----
mean loss: 64.65
 ---- batch: 040 ----
mean loss: 65.81
train mean loss: 67.62
epoch train time: 0:00:00.653563
elapsed time: 0:01:13.853120
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:35:00.081859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.85
 ---- batch: 020 ----
mean loss: 67.03
 ---- batch: 030 ----
mean loss: 67.04
 ---- batch: 040 ----
mean loss: 68.44
train mean loss: 67.42
epoch train time: 0:00:00.662236
elapsed time: 0:01:14.515643
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:35:00.744361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.40
 ---- batch: 020 ----
mean loss: 68.14
 ---- batch: 030 ----
mean loss: 64.81
 ---- batch: 040 ----
mean loss: 67.97
train mean loss: 66.40
epoch train time: 0:00:00.713270
elapsed time: 0:01:15.229224
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:35:01.457958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.66
 ---- batch: 020 ----
mean loss: 67.32
 ---- batch: 030 ----
mean loss: 65.43
 ---- batch: 040 ----
mean loss: 65.26
train mean loss: 66.46
epoch train time: 0:00:00.686922
elapsed time: 0:01:15.916442
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:35:02.145182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.79
 ---- batch: 020 ----
mean loss: 64.93
 ---- batch: 030 ----
mean loss: 65.96
 ---- batch: 040 ----
mean loss: 65.77
train mean loss: 65.44
epoch train time: 0:00:00.650991
elapsed time: 0:01:16.567702
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:35:02.796442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.71
 ---- batch: 020 ----
mean loss: 67.70
 ---- batch: 030 ----
mean loss: 63.70
 ---- batch: 040 ----
mean loss: 64.23
train mean loss: 65.19
epoch train time: 0:00:00.629478
elapsed time: 0:01:17.197427
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:35:03.426152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.74
 ---- batch: 020 ----
mean loss: 67.20
 ---- batch: 030 ----
mean loss: 62.88
 ---- batch: 040 ----
mean loss: 65.94
train mean loss: 64.95
epoch train time: 0:00:00.648889
elapsed time: 0:01:17.846536
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:35:04.075250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.65
 ---- batch: 020 ----
mean loss: 64.28
 ---- batch: 030 ----
mean loss: 63.97
 ---- batch: 040 ----
mean loss: 63.38
train mean loss: 64.75
epoch train time: 0:00:00.686479
elapsed time: 0:01:18.533276
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:35:04.762055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.39
 ---- batch: 020 ----
mean loss: 66.66
 ---- batch: 030 ----
mean loss: 61.16
 ---- batch: 040 ----
mean loss: 62.32
train mean loss: 63.70
epoch train time: 0:00:00.675665
elapsed time: 0:01:19.209239
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:35:05.437970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.78
 ---- batch: 020 ----
mean loss: 62.76
 ---- batch: 030 ----
mean loss: 61.97
 ---- batch: 040 ----
mean loss: 63.78
train mean loss: 63.47
epoch train time: 0:00:00.660823
elapsed time: 0:01:19.870324
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:35:06.099052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.57
 ---- batch: 020 ----
mean loss: 63.74
 ---- batch: 030 ----
mean loss: 61.62
 ---- batch: 040 ----
mean loss: 64.47
train mean loss: 62.57
epoch train time: 0:00:00.639635
elapsed time: 0:01:20.510181
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:35:06.738895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.13
 ---- batch: 020 ----
mean loss: 66.02
 ---- batch: 030 ----
mean loss: 62.34
 ---- batch: 040 ----
mean loss: 59.29
train mean loss: 63.06
epoch train time: 0:00:00.662599
elapsed time: 0:01:21.172994
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:35:07.401709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.17
 ---- batch: 020 ----
mean loss: 60.06
 ---- batch: 030 ----
mean loss: 62.83
 ---- batch: 040 ----
mean loss: 63.99
train mean loss: 61.89
epoch train time: 0:00:00.674704
elapsed time: 0:01:21.847927
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:35:08.076647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.01
 ---- batch: 020 ----
mean loss: 59.21
 ---- batch: 030 ----
mean loss: 61.76
 ---- batch: 040 ----
mean loss: 62.11
train mean loss: 61.59
epoch train time: 0:00:00.695346
elapsed time: 0:01:22.543554
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:35:08.772282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.81
 ---- batch: 020 ----
mean loss: 58.92
 ---- batch: 030 ----
mean loss: 62.29
 ---- batch: 040 ----
mean loss: 62.52
train mean loss: 61.21
epoch train time: 0:00:00.697459
elapsed time: 0:01:23.241251
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:35:09.469971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.77
 ---- batch: 020 ----
mean loss: 62.07
 ---- batch: 030 ----
mean loss: 61.97
 ---- batch: 040 ----
mean loss: 59.63
train mean loss: 61.30
epoch train time: 0:00:00.667896
elapsed time: 0:01:23.909360
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:35:10.138074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.31
 ---- batch: 020 ----
mean loss: 62.86
 ---- batch: 030 ----
mean loss: 58.98
 ---- batch: 040 ----
mean loss: 63.39
train mean loss: 61.55
epoch train time: 0:00:00.637423
elapsed time: 0:01:24.546999
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:35:10.775724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.27
 ---- batch: 020 ----
mean loss: 60.33
 ---- batch: 030 ----
mean loss: 60.12
 ---- batch: 040 ----
mean loss: 62.05
train mean loss: 60.28
epoch train time: 0:00:00.651118
elapsed time: 0:01:25.198362
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:35:11.427075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.85
 ---- batch: 020 ----
mean loss: 57.27
 ---- batch: 030 ----
mean loss: 60.03
 ---- batch: 040 ----
mean loss: 60.57
train mean loss: 59.80
epoch train time: 0:00:00.661750
elapsed time: 0:01:25.860374
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:35:12.089072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.15
 ---- batch: 020 ----
mean loss: 58.44
 ---- batch: 030 ----
mean loss: 59.17
 ---- batch: 040 ----
mean loss: 59.94
train mean loss: 59.53
epoch train time: 0:00:00.679408
elapsed time: 0:01:26.540027
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:35:12.768763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.45
 ---- batch: 020 ----
mean loss: 56.76
 ---- batch: 030 ----
mean loss: 58.87
 ---- batch: 040 ----
mean loss: 63.09
train mean loss: 59.29
epoch train time: 0:00:00.683837
elapsed time: 0:01:27.224135
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:35:13.452859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.36
 ---- batch: 020 ----
mean loss: 59.01
 ---- batch: 030 ----
mean loss: 61.17
 ---- batch: 040 ----
mean loss: 57.37
train mean loss: 59.87
epoch train time: 0:00:00.661940
elapsed time: 0:01:27.886291
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:35:14.115021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.47
 ---- batch: 020 ----
mean loss: 57.56
 ---- batch: 030 ----
mean loss: 60.20
 ---- batch: 040 ----
mean loss: 58.10
train mean loss: 58.61
epoch train time: 0:00:00.653870
elapsed time: 0:01:28.540386
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:35:14.769098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.91
 ---- batch: 020 ----
mean loss: 59.65
 ---- batch: 030 ----
mean loss: 58.63
 ---- batch: 040 ----
mean loss: 58.28
train mean loss: 58.80
epoch train time: 0:00:00.669698
elapsed time: 0:01:29.210328
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:35:15.439042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.79
 ---- batch: 020 ----
mean loss: 60.32
 ---- batch: 030 ----
mean loss: 58.28
 ---- batch: 040 ----
mean loss: 55.43
train mean loss: 58.28
epoch train time: 0:00:00.696628
elapsed time: 0:01:29.907201
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:35:16.135919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.76
 ---- batch: 020 ----
mean loss: 60.37
 ---- batch: 030 ----
mean loss: 59.37
 ---- batch: 040 ----
mean loss: 55.46
train mean loss: 58.44
epoch train time: 0:00:00.680646
elapsed time: 0:01:30.588106
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:35:16.816829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.84
 ---- batch: 020 ----
mean loss: 56.65
 ---- batch: 030 ----
mean loss: 59.15
 ---- batch: 040 ----
mean loss: 57.05
train mean loss: 57.50
epoch train time: 0:00:00.669360
elapsed time: 0:01:31.257692
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:35:17.486406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.06
 ---- batch: 020 ----
mean loss: 57.41
 ---- batch: 030 ----
mean loss: 56.74
 ---- batch: 040 ----
mean loss: 55.73
train mean loss: 57.91
epoch train time: 0:00:00.655374
elapsed time: 0:01:31.913298
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:35:18.142057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.37
 ---- batch: 020 ----
mean loss: 56.19
 ---- batch: 030 ----
mean loss: 56.49
 ---- batch: 040 ----
mean loss: 59.48
train mean loss: 57.03
epoch train time: 0:00:00.640295
elapsed time: 0:01:32.553854
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:35:18.782582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.91
 ---- batch: 020 ----
mean loss: 58.29
 ---- batch: 030 ----
mean loss: 57.52
 ---- batch: 040 ----
mean loss: 54.25
train mean loss: 57.17
epoch train time: 0:00:00.663905
elapsed time: 0:01:33.218058
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:35:19.446793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.90
 ---- batch: 020 ----
mean loss: 56.44
 ---- batch: 030 ----
mean loss: 58.12
 ---- batch: 040 ----
mean loss: 55.75
train mean loss: 56.64
epoch train time: 0:00:00.703373
elapsed time: 0:01:33.921734
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:35:20.150463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.19
 ---- batch: 020 ----
mean loss: 57.19
 ---- batch: 030 ----
mean loss: 58.11
 ---- batch: 040 ----
mean loss: 54.59
train mean loss: 56.40
epoch train time: 0:00:00.683235
elapsed time: 0:01:34.605235
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:35:20.833952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.97
 ---- batch: 020 ----
mean loss: 55.64
 ---- batch: 030 ----
mean loss: 57.17
 ---- batch: 040 ----
mean loss: 55.59
train mean loss: 55.54
epoch train time: 0:00:00.670382
elapsed time: 0:01:35.275967
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:35:21.504697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.65
 ---- batch: 020 ----
mean loss: 57.33
 ---- batch: 030 ----
mean loss: 51.24
 ---- batch: 040 ----
mean loss: 58.66
train mean loss: 55.23
epoch train time: 0:00:00.661313
elapsed time: 0:01:35.937553
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:35:22.166309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.86
 ---- batch: 020 ----
mean loss: 56.25
 ---- batch: 030 ----
mean loss: 56.36
 ---- batch: 040 ----
mean loss: 54.19
train mean loss: 55.51
epoch train time: 0:00:00.647280
elapsed time: 0:01:36.585085
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:35:22.813827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.75
 ---- batch: 020 ----
mean loss: 54.10
 ---- batch: 030 ----
mean loss: 55.17
 ---- batch: 040 ----
mean loss: 54.22
train mean loss: 54.55
epoch train time: 0:00:00.636104
elapsed time: 0:01:37.221454
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:35:23.450167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.51
 ---- batch: 020 ----
mean loss: 53.58
 ---- batch: 030 ----
mean loss: 53.10
 ---- batch: 040 ----
mean loss: 53.98
train mean loss: 54.43
epoch train time: 0:00:00.688842
elapsed time: 0:01:37.910543
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:35:24.139257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.82
 ---- batch: 020 ----
mean loss: 53.92
 ---- batch: 030 ----
mean loss: 57.91
 ---- batch: 040 ----
mean loss: 53.43
train mean loss: 54.30
epoch train time: 0:00:00.694329
elapsed time: 0:01:38.605125
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:35:24.833861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.98
 ---- batch: 020 ----
mean loss: 54.04
 ---- batch: 030 ----
mean loss: 53.66
 ---- batch: 040 ----
mean loss: 53.71
train mean loss: 53.97
epoch train time: 0:00:00.667121
elapsed time: 0:01:39.272569
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:35:25.501247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.69
 ---- batch: 020 ----
mean loss: 51.16
 ---- batch: 030 ----
mean loss: 54.20
 ---- batch: 040 ----
mean loss: 53.48
train mean loss: 53.88
epoch train time: 0:00:00.646823
elapsed time: 0:01:39.919574
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:35:26.148283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.13
 ---- batch: 020 ----
mean loss: 53.90
 ---- batch: 030 ----
mean loss: 53.53
 ---- batch: 040 ----
mean loss: 54.22
train mean loss: 53.62
epoch train time: 0:00:00.663135
elapsed time: 0:01:40.582968
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:35:26.811659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.82
 ---- batch: 020 ----
mean loss: 54.08
 ---- batch: 030 ----
mean loss: 54.55
 ---- batch: 040 ----
mean loss: 50.79
train mean loss: 53.78
epoch train time: 0:00:00.631742
elapsed time: 0:01:41.214967
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:35:27.443673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.49
 ---- batch: 020 ----
mean loss: 55.24
 ---- batch: 030 ----
mean loss: 50.41
 ---- batch: 040 ----
mean loss: 53.91
train mean loss: 53.45
epoch train time: 0:00:00.676462
elapsed time: 0:01:41.891654
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:35:28.120420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.22
 ---- batch: 020 ----
mean loss: 53.21
 ---- batch: 030 ----
mean loss: 50.74
 ---- batch: 040 ----
mean loss: 53.34
train mean loss: 52.88
epoch train time: 0:00:00.665817
elapsed time: 0:01:42.557779
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:35:28.786501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.90
 ---- batch: 020 ----
mean loss: 53.65
 ---- batch: 030 ----
mean loss: 53.86
 ---- batch: 040 ----
mean loss: 51.18
train mean loss: 53.41
epoch train time: 0:00:00.669106
elapsed time: 0:01:43.227131
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:35:29.455841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.74
 ---- batch: 020 ----
mean loss: 51.83
 ---- batch: 030 ----
mean loss: 53.96
 ---- batch: 040 ----
mean loss: 53.78
train mean loss: 52.42
epoch train time: 0:00:00.669691
elapsed time: 0:01:43.897038
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:35:30.125749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.20
 ---- batch: 020 ----
mean loss: 53.18
 ---- batch: 030 ----
mean loss: 51.94
 ---- batch: 040 ----
mean loss: 52.00
train mean loss: 52.19
epoch train time: 0:00:00.656298
elapsed time: 0:01:44.553546
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:35:30.782285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.52
 ---- batch: 020 ----
mean loss: 51.53
 ---- batch: 030 ----
mean loss: 51.24
 ---- batch: 040 ----
mean loss: 52.96
train mean loss: 52.01
epoch train time: 0:00:00.648735
elapsed time: 0:01:45.202517
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:35:31.431228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.44
 ---- batch: 020 ----
mean loss: 53.35
 ---- batch: 030 ----
mean loss: 51.51
 ---- batch: 040 ----
mean loss: 49.83
train mean loss: 51.55
epoch train time: 0:00:00.670176
elapsed time: 0:01:45.872958
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:35:32.101676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.51
 ---- batch: 020 ----
mean loss: 50.17
 ---- batch: 030 ----
mean loss: 50.81
 ---- batch: 040 ----
mean loss: 53.05
train mean loss: 51.38
epoch train time: 0:00:00.686897
elapsed time: 0:01:46.560120
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:35:32.788897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.00
 ---- batch: 020 ----
mean loss: 55.03
 ---- batch: 030 ----
mean loss: 50.45
 ---- batch: 040 ----
mean loss: 50.18
train mean loss: 50.86
epoch train time: 0:00:00.676099
elapsed time: 0:01:47.236558
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:35:33.465285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.90
 ---- batch: 020 ----
mean loss: 49.40
 ---- batch: 030 ----
mean loss: 50.61
 ---- batch: 040 ----
mean loss: 50.84
train mean loss: 50.57
epoch train time: 0:00:00.661473
elapsed time: 0:01:47.898276
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:35:34.126987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.69
 ---- batch: 020 ----
mean loss: 46.47
 ---- batch: 030 ----
mean loss: 51.50
 ---- batch: 040 ----
mean loss: 52.16
train mean loss: 50.42
epoch train time: 0:00:00.641675
elapsed time: 0:01:48.540155
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:35:34.768867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.79
 ---- batch: 020 ----
mean loss: 51.57
 ---- batch: 030 ----
mean loss: 51.08
 ---- batch: 040 ----
mean loss: 51.41
train mean loss: 50.31
epoch train time: 0:00:00.656373
elapsed time: 0:01:49.196754
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:35:35.425497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.09
 ---- batch: 020 ----
mean loss: 53.61
 ---- batch: 030 ----
mean loss: 50.65
 ---- batch: 040 ----
mean loss: 47.62
train mean loss: 49.61
epoch train time: 0:00:00.687362
elapsed time: 0:01:49.884402
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:35:36.113118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.71
 ---- batch: 020 ----
mean loss: 49.71
 ---- batch: 030 ----
mean loss: 50.37
 ---- batch: 040 ----
mean loss: 50.14
train mean loss: 49.60
epoch train time: 0:00:00.677180
elapsed time: 0:01:50.561854
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:35:36.790578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.87
 ---- batch: 020 ----
mean loss: 51.49
 ---- batch: 030 ----
mean loss: 49.08
 ---- batch: 040 ----
mean loss: 47.16
train mean loss: 49.19
epoch train time: 0:00:00.668266
elapsed time: 0:01:51.230359
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:35:37.459069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.12
 ---- batch: 020 ----
mean loss: 46.53
 ---- batch: 030 ----
mean loss: 49.68
 ---- batch: 040 ----
mean loss: 49.70
train mean loss: 49.10
epoch train time: 0:00:00.657770
elapsed time: 0:01:51.888335
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:35:38.117092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.17
 ---- batch: 020 ----
mean loss: 50.14
 ---- batch: 030 ----
mean loss: 48.29
 ---- batch: 040 ----
mean loss: 49.97
train mean loss: 49.12
epoch train time: 0:00:00.631484
elapsed time: 0:01:52.520077
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:35:38.748830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.11
 ---- batch: 020 ----
mean loss: 47.35
 ---- batch: 030 ----
mean loss: 49.67
 ---- batch: 040 ----
mean loss: 47.40
train mean loss: 48.87
epoch train time: 0:00:00.655137
elapsed time: 0:01:53.175491
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:35:39.404210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.62
 ---- batch: 020 ----
mean loss: 51.04
 ---- batch: 030 ----
mean loss: 48.40
 ---- batch: 040 ----
mean loss: 46.54
train mean loss: 48.04
epoch train time: 0:00:00.663925
elapsed time: 0:01:53.839693
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:35:40.068373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.53
 ---- batch: 020 ----
mean loss: 48.83
 ---- batch: 030 ----
mean loss: 48.35
 ---- batch: 040 ----
mean loss: 47.62
train mean loss: 48.36
epoch train time: 0:00:00.673485
elapsed time: 0:01:54.513414
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:35:40.742141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.20
 ---- batch: 020 ----
mean loss: 46.78
 ---- batch: 030 ----
mean loss: 49.05
 ---- batch: 040 ----
mean loss: 49.38
train mean loss: 47.61
epoch train time: 0:00:00.665187
elapsed time: 0:01:55.178844
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:35:41.407564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.38
 ---- batch: 020 ----
mean loss: 48.46
 ---- batch: 030 ----
mean loss: 47.00
 ---- batch: 040 ----
mean loss: 48.27
train mean loss: 47.52
epoch train time: 0:00:00.682751
elapsed time: 0:01:55.861848
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:35:42.090579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.45
 ---- batch: 020 ----
mean loss: 48.48
 ---- batch: 030 ----
mean loss: 47.69
 ---- batch: 040 ----
mean loss: 47.50
train mean loss: 47.42
epoch train time: 0:00:00.674034
elapsed time: 0:01:56.536106
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:35:42.764825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.14
 ---- batch: 020 ----
mean loss: 48.11
 ---- batch: 030 ----
mean loss: 44.26
 ---- batch: 040 ----
mean loss: 47.10
train mean loss: 46.91
epoch train time: 0:00:00.670890
elapsed time: 0:01:57.207220
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:35:43.435934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.28
 ---- batch: 020 ----
mean loss: 47.02
 ---- batch: 030 ----
mean loss: 46.19
 ---- batch: 040 ----
mean loss: 48.37
train mean loss: 46.77
epoch train time: 0:00:00.659278
elapsed time: 0:01:57.866798
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:35:44.095550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.95
 ---- batch: 020 ----
mean loss: 47.27
 ---- batch: 030 ----
mean loss: 46.97
 ---- batch: 040 ----
mean loss: 45.21
train mean loss: 45.93
epoch train time: 0:00:00.691670
elapsed time: 0:01:58.558774
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:35:44.787494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.51
 ---- batch: 020 ----
mean loss: 45.82
 ---- batch: 030 ----
mean loss: 46.14
 ---- batch: 040 ----
mean loss: 44.74
train mean loss: 45.84
epoch train time: 0:00:00.686266
elapsed time: 0:01:59.245286
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:35:45.474009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.00
 ---- batch: 020 ----
mean loss: 46.03
 ---- batch: 030 ----
mean loss: 46.64
 ---- batch: 040 ----
mean loss: 45.86
train mean loss: 45.32
epoch train time: 0:00:00.677764
elapsed time: 0:01:59.923280
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:35:46.152006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.46
 ---- batch: 020 ----
mean loss: 46.15
 ---- batch: 030 ----
mean loss: 44.58
 ---- batch: 040 ----
mean loss: 44.39
train mean loss: 44.85
epoch train time: 0:00:00.644084
elapsed time: 0:02:00.567590
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:35:46.796314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.36
 ---- batch: 020 ----
mean loss: 44.64
 ---- batch: 030 ----
mean loss: 44.85
 ---- batch: 040 ----
mean loss: 47.54
train mean loss: 45.35
epoch train time: 0:00:00.640829
elapsed time: 0:02:01.208655
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:35:47.437376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.16
 ---- batch: 020 ----
mean loss: 46.20
 ---- batch: 030 ----
mean loss: 44.50
 ---- batch: 040 ----
mean loss: 46.62
train mean loss: 45.48
epoch train time: 0:00:00.672212
elapsed time: 0:02:01.881109
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:35:48.109829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.22
 ---- batch: 020 ----
mean loss: 41.62
 ---- batch: 030 ----
mean loss: 45.58
 ---- batch: 040 ----
mean loss: 46.99
train mean loss: 44.78
epoch train time: 0:00:00.687148
elapsed time: 0:02:02.568558
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:35:48.797304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.29
 ---- batch: 020 ----
mean loss: 45.77
 ---- batch: 030 ----
mean loss: 42.34
 ---- batch: 040 ----
mean loss: 45.28
train mean loss: 44.19
epoch train time: 0:00:00.710040
elapsed time: 0:02:03.278869
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:35:49.507615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.59
 ---- batch: 020 ----
mean loss: 42.51
 ---- batch: 030 ----
mean loss: 45.79
 ---- batch: 040 ----
mean loss: 45.01
train mean loss: 43.80
epoch train time: 0:00:00.656301
elapsed time: 0:02:03.935460
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:35:50.164226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.90
 ---- batch: 020 ----
mean loss: 43.31
 ---- batch: 030 ----
mean loss: 44.45
 ---- batch: 040 ----
mean loss: 44.46
train mean loss: 43.95
epoch train time: 0:00:00.654184
elapsed time: 0:02:04.589915
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:35:50.818635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.73
 ---- batch: 020 ----
mean loss: 43.15
 ---- batch: 030 ----
mean loss: 42.58
 ---- batch: 040 ----
mean loss: 44.07
train mean loss: 43.47
epoch train time: 0:00:00.651299
elapsed time: 0:02:05.241451
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:35:51.470169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.46
 ---- batch: 020 ----
mean loss: 41.97
 ---- batch: 030 ----
mean loss: 44.83
 ---- batch: 040 ----
mean loss: 39.77
train mean loss: 43.01
epoch train time: 0:00:00.680600
elapsed time: 0:02:05.922297
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:35:52.151014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.51
 ---- batch: 020 ----
mean loss: 42.15
 ---- batch: 030 ----
mean loss: 42.77
 ---- batch: 040 ----
mean loss: 41.99
train mean loss: 43.06
epoch train time: 0:00:00.687308
elapsed time: 0:02:06.609861
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:35:52.838591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.85
 ---- batch: 020 ----
mean loss: 43.05
 ---- batch: 030 ----
mean loss: 44.29
 ---- batch: 040 ----
mean loss: 41.21
train mean loss: 42.39
epoch train time: 0:00:00.674852
elapsed time: 0:02:07.284944
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:35:53.513692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.47
 ---- batch: 020 ----
mean loss: 39.65
 ---- batch: 030 ----
mean loss: 44.52
 ---- batch: 040 ----
mean loss: 43.51
train mean loss: 42.00
epoch train time: 0:00:00.676082
elapsed time: 0:02:07.961273
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:35:54.189985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.64
 ---- batch: 020 ----
mean loss: 41.21
 ---- batch: 030 ----
mean loss: 41.99
 ---- batch: 040 ----
mean loss: 42.68
train mean loss: 42.01
epoch train time: 0:00:00.654078
elapsed time: 0:02:08.615566
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:35:54.844324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.84
 ---- batch: 020 ----
mean loss: 39.58
 ---- batch: 030 ----
mean loss: 43.05
 ---- batch: 040 ----
mean loss: 42.05
train mean loss: 41.63
epoch train time: 0:00:00.675021
elapsed time: 0:02:09.290884
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:35:55.519605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.99
 ---- batch: 020 ----
mean loss: 42.31
 ---- batch: 030 ----
mean loss: 40.39
 ---- batch: 040 ----
mean loss: 43.71
train mean loss: 41.94
epoch train time: 0:00:00.701569
elapsed time: 0:02:09.992734
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:35:56.221459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.31
 ---- batch: 020 ----
mean loss: 44.09
 ---- batch: 030 ----
mean loss: 42.36
 ---- batch: 040 ----
mean loss: 40.17
train mean loss: 41.48
epoch train time: 0:00:00.703332
elapsed time: 0:02:10.696391
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:35:56.925071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.75
 ---- batch: 020 ----
mean loss: 41.37
 ---- batch: 030 ----
mean loss: 40.91
 ---- batch: 040 ----
mean loss: 39.56
train mean loss: 40.68
epoch train time: 0:00:00.665179
elapsed time: 0:02:11.361797
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:35:57.590518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.07
 ---- batch: 020 ----
mean loss: 40.71
 ---- batch: 030 ----
mean loss: 41.94
 ---- batch: 040 ----
mean loss: 39.09
train mean loss: 40.86
epoch train time: 0:00:00.663583
elapsed time: 0:02:12.025613
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:35:58.254358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.62
 ---- batch: 020 ----
mean loss: 40.57
 ---- batch: 030 ----
mean loss: 39.20
 ---- batch: 040 ----
mean loss: 41.90
train mean loss: 40.33
epoch train time: 0:00:00.663780
elapsed time: 0:02:12.689762
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:35:58.918534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.35
 ---- batch: 020 ----
mean loss: 39.49
 ---- batch: 030 ----
mean loss: 39.42
 ---- batch: 040 ----
mean loss: 40.95
train mean loss: 40.24
epoch train time: 0:00:00.681996
elapsed time: 0:02:13.372066
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:35:59.600779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.60
 ---- batch: 020 ----
mean loss: 37.88
 ---- batch: 030 ----
mean loss: 37.85
 ---- batch: 040 ----
mean loss: 40.62
train mean loss: 39.30
epoch train time: 0:00:00.677881
elapsed time: 0:02:14.050187
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:36:00.278896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.47
 ---- batch: 020 ----
mean loss: 40.17
 ---- batch: 030 ----
mean loss: 39.66
 ---- batch: 040 ----
mean loss: 39.01
train mean loss: 39.20
epoch train time: 0:00:00.666893
elapsed time: 0:02:14.717291
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:36:00.945999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.66
 ---- batch: 020 ----
mean loss: 41.33
 ---- batch: 030 ----
mean loss: 38.47
 ---- batch: 040 ----
mean loss: 38.65
train mean loss: 39.15
epoch train time: 0:00:00.638354
elapsed time: 0:02:15.355876
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:36:01.584604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.37
 ---- batch: 020 ----
mean loss: 36.85
 ---- batch: 030 ----
mean loss: 38.95
 ---- batch: 040 ----
mean loss: 39.95
train mean loss: 38.87
epoch train time: 0:00:00.628671
elapsed time: 0:02:15.984764
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:36:02.213491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.55
 ---- batch: 020 ----
mean loss: 38.17
 ---- batch: 030 ----
mean loss: 35.98
 ---- batch: 040 ----
mean loss: 40.57
train mean loss: 38.58
epoch train time: 0:00:00.680167
elapsed time: 0:02:16.665208
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:36:02.893940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.16
 ---- batch: 020 ----
mean loss: 37.71
 ---- batch: 030 ----
mean loss: 39.51
 ---- batch: 040 ----
mean loss: 38.35
train mean loss: 38.34
epoch train time: 0:00:00.688881
elapsed time: 0:02:17.354333
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:36:03.583051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.10
 ---- batch: 020 ----
mean loss: 40.65
 ---- batch: 030 ----
mean loss: 35.86
 ---- batch: 040 ----
mean loss: 39.81
train mean loss: 37.95
epoch train time: 0:00:00.687673
elapsed time: 0:02:18.042298
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:36:04.271034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.34
 ---- batch: 020 ----
mean loss: 36.14
 ---- batch: 030 ----
mean loss: 39.64
 ---- batch: 040 ----
mean loss: 37.88
train mean loss: 37.51
epoch train time: 0:00:00.659400
elapsed time: 0:02:18.701969
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:36:04.930697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.02
 ---- batch: 020 ----
mean loss: 40.07
 ---- batch: 030 ----
mean loss: 41.21
 ---- batch: 040 ----
mean loss: 34.16
train mean loss: 38.98
epoch train time: 0:00:00.644732
elapsed time: 0:02:19.346975
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:36:05.575722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.84
 ---- batch: 020 ----
mean loss: 36.60
 ---- batch: 030 ----
mean loss: 36.28
 ---- batch: 040 ----
mean loss: 39.33
train mean loss: 37.25
epoch train time: 0:00:00.638143
elapsed time: 0:02:19.985385
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:36:06.214129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.36
 ---- batch: 020 ----
mean loss: 36.78
 ---- batch: 030 ----
mean loss: 36.65
 ---- batch: 040 ----
mean loss: 38.22
train mean loss: 36.56
epoch train time: 0:00:00.675610
elapsed time: 0:02:20.661276
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:36:06.890011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.01
 ---- batch: 020 ----
mean loss: 35.73
 ---- batch: 030 ----
mean loss: 39.18
 ---- batch: 040 ----
mean loss: 38.15
train mean loss: 36.88
epoch train time: 0:00:00.693709
elapsed time: 0:02:21.355290
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:36:07.584000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.32
 ---- batch: 020 ----
mean loss: 36.32
 ---- batch: 030 ----
mean loss: 34.85
 ---- batch: 040 ----
mean loss: 36.81
train mean loss: 35.76
epoch train time: 0:00:00.699773
elapsed time: 0:02:22.055305
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:36:08.284030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.85
 ---- batch: 020 ----
mean loss: 34.27
 ---- batch: 030 ----
mean loss: 37.66
 ---- batch: 040 ----
mean loss: 34.80
train mean loss: 35.44
epoch train time: 0:00:00.658581
elapsed time: 0:02:22.714101
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:36:08.942823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.02
 ---- batch: 020 ----
mean loss: 35.09
 ---- batch: 030 ----
mean loss: 37.92
 ---- batch: 040 ----
mean loss: 34.37
train mean loss: 36.17
epoch train time: 0:00:00.658412
elapsed time: 0:02:23.372766
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:36:09.601482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.86
 ---- batch: 020 ----
mean loss: 32.86
 ---- batch: 030 ----
mean loss: 38.20
 ---- batch: 040 ----
mean loss: 35.21
train mean loss: 35.54
epoch train time: 0:00:00.649820
elapsed time: 0:02:24.022858
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:36:10.251606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.22
 ---- batch: 020 ----
mean loss: 36.88
 ---- batch: 030 ----
mean loss: 35.38
 ---- batch: 040 ----
mean loss: 33.80
train mean loss: 35.93
epoch train time: 0:00:00.702594
elapsed time: 0:02:24.725725
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:36:10.954510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.45
 ---- batch: 020 ----
mean loss: 33.76
 ---- batch: 030 ----
mean loss: 36.66
 ---- batch: 040 ----
mean loss: 35.46
train mean loss: 35.25
epoch train time: 0:00:00.690324
elapsed time: 0:02:25.416451
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:36:11.645232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.98
 ---- batch: 020 ----
mean loss: 34.96
 ---- batch: 030 ----
mean loss: 34.91
 ---- batch: 040 ----
mean loss: 33.83
train mean loss: 34.55
epoch train time: 0:00:00.674923
elapsed time: 0:02:26.091654
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:36:12.320408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.31
 ---- batch: 020 ----
mean loss: 34.73
 ---- batch: 030 ----
mean loss: 34.75
 ---- batch: 040 ----
mean loss: 35.26
train mean loss: 34.38
epoch train time: 0:00:00.652135
elapsed time: 0:02:26.744033
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:36:12.972744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.98
 ---- batch: 020 ----
mean loss: 34.67
 ---- batch: 030 ----
mean loss: 34.13
 ---- batch: 040 ----
mean loss: 34.26
train mean loss: 34.36
epoch train time: 0:00:00.656479
elapsed time: 0:02:27.400718
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:36:13.629431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 31.63
 ---- batch: 020 ----
mean loss: 32.97
 ---- batch: 030 ----
mean loss: 35.25
 ---- batch: 040 ----
mean loss: 34.70
train mean loss: 33.61
epoch train time: 0:00:00.671956
elapsed time: 0:02:28.072904
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:36:14.301636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.19
 ---- batch: 020 ----
mean loss: 32.76
 ---- batch: 030 ----
mean loss: 33.85
 ---- batch: 040 ----
mean loss: 35.39
train mean loss: 33.92
epoch train time: 0:00:00.712886
elapsed time: 0:02:28.786064
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:36:15.014781
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.91
 ---- batch: 020 ----
mean loss: 32.25
 ---- batch: 030 ----
mean loss: 32.72
 ---- batch: 040 ----
mean loss: 34.04
train mean loss: 32.38
epoch train time: 0:00:00.689970
elapsed time: 0:02:29.476385
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:36:15.705084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.96
 ---- batch: 020 ----
mean loss: 33.52
 ---- batch: 030 ----
mean loss: 31.44
 ---- batch: 040 ----
mean loss: 32.47
train mean loss: 32.15
epoch train time: 0:00:00.648268
elapsed time: 0:02:30.124872
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:36:16.353598
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.19
 ---- batch: 020 ----
mean loss: 31.36
 ---- batch: 030 ----
mean loss: 31.76
 ---- batch: 040 ----
mean loss: 31.92
train mean loss: 32.20
epoch train time: 0:00:00.640975
elapsed time: 0:02:30.766059
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:36:16.994796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.18
 ---- batch: 020 ----
mean loss: 32.66
 ---- batch: 030 ----
mean loss: 32.26
 ---- batch: 040 ----
mean loss: 32.35
train mean loss: 32.12
epoch train time: 0:00:00.640190
elapsed time: 0:02:31.406493
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:36:17.635219
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.62
 ---- batch: 020 ----
mean loss: 31.24
 ---- batch: 030 ----
mean loss: 32.65
 ---- batch: 040 ----
mean loss: 31.40
train mean loss: 31.95
epoch train time: 0:00:00.671198
elapsed time: 0:02:32.078029
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:36:18.306782
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.66
 ---- batch: 020 ----
mean loss: 31.14
 ---- batch: 030 ----
mean loss: 32.65
 ---- batch: 040 ----
mean loss: 31.86
train mean loss: 31.98
epoch train time: 0:00:00.694078
elapsed time: 0:02:32.772404
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:36:19.001132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.84
 ---- batch: 020 ----
mean loss: 31.70
 ---- batch: 030 ----
mean loss: 31.21
 ---- batch: 040 ----
mean loss: 32.14
train mean loss: 32.04
epoch train time: 0:00:00.688917
elapsed time: 0:02:33.461571
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:36:19.690293
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.08
 ---- batch: 020 ----
mean loss: 32.42
 ---- batch: 030 ----
mean loss: 30.98
 ---- batch: 040 ----
mean loss: 32.29
train mean loss: 32.20
epoch train time: 0:00:00.656867
elapsed time: 0:02:34.118651
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:36:20.347361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.64
 ---- batch: 020 ----
mean loss: 33.40
 ---- batch: 030 ----
mean loss: 31.37
 ---- batch: 040 ----
mean loss: 33.14
train mean loss: 32.33
epoch train time: 0:00:00.663501
elapsed time: 0:02:34.782396
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:36:21.011115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.63
 ---- batch: 020 ----
mean loss: 32.34
 ---- batch: 030 ----
mean loss: 32.12
 ---- batch: 040 ----
mean loss: 29.17
train mean loss: 31.54
epoch train time: 0:00:00.658980
elapsed time: 0:02:35.441613
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:36:21.670463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.05
 ---- batch: 020 ----
mean loss: 32.40
 ---- batch: 030 ----
mean loss: 31.88
 ---- batch: 040 ----
mean loss: 31.51
train mean loss: 32.09
epoch train time: 0:00:00.685303
elapsed time: 0:02:36.127345
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:36:22.356109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.71
 ---- batch: 020 ----
mean loss: 31.26
 ---- batch: 030 ----
mean loss: 30.50
 ---- batch: 040 ----
mean loss: 32.56
train mean loss: 32.02
epoch train time: 0:00:00.694002
elapsed time: 0:02:36.821628
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:36:23.050373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.68
 ---- batch: 020 ----
mean loss: 30.00
 ---- batch: 030 ----
mean loss: 30.54
 ---- batch: 040 ----
mean loss: 32.35
train mean loss: 31.70
epoch train time: 0:00:00.686068
elapsed time: 0:02:37.507956
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:36:23.736668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.01
 ---- batch: 020 ----
mean loss: 31.36
 ---- batch: 030 ----
mean loss: 32.87
 ---- batch: 040 ----
mean loss: 31.74
train mean loss: 32.02
epoch train time: 0:00:00.658992
elapsed time: 0:02:38.167162
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:36:24.395888
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.82
 ---- batch: 020 ----
mean loss: 29.57
 ---- batch: 030 ----
mean loss: 32.06
 ---- batch: 040 ----
mean loss: 32.31
train mean loss: 31.61
epoch train time: 0:00:00.663184
elapsed time: 0:02:38.830563
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:36:25.059272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.49
 ---- batch: 020 ----
mean loss: 31.84
 ---- batch: 030 ----
mean loss: 32.00
 ---- batch: 040 ----
mean loss: 30.28
train mean loss: 31.79
epoch train time: 0:00:00.679099
elapsed time: 0:02:39.509908
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:36:25.738632
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.69
 ---- batch: 020 ----
mean loss: 32.58
 ---- batch: 030 ----
mean loss: 32.24
 ---- batch: 040 ----
mean loss: 30.74
train mean loss: 32.07
epoch train time: 0:00:00.685376
elapsed time: 0:02:40.195556
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:36:26.424282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.95
 ---- batch: 020 ----
mean loss: 32.94
 ---- batch: 030 ----
mean loss: 31.19
 ---- batch: 040 ----
mean loss: 32.12
train mean loss: 31.72
epoch train time: 0:00:00.684744
elapsed time: 0:02:40.880549
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:36:27.109280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.38
 ---- batch: 020 ----
mean loss: 32.01
 ---- batch: 030 ----
mean loss: 32.40
 ---- batch: 040 ----
mean loss: 29.92
train mean loss: 31.73
epoch train time: 0:00:00.663579
elapsed time: 0:02:41.544378
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:36:27.773107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.47
 ---- batch: 020 ----
mean loss: 33.56
 ---- batch: 030 ----
mean loss: 32.45
 ---- batch: 040 ----
mean loss: 31.08
train mean loss: 31.74
epoch train time: 0:00:00.649412
elapsed time: 0:02:42.194048
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:36:28.422762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.84
 ---- batch: 020 ----
mean loss: 31.90
 ---- batch: 030 ----
mean loss: 31.82
 ---- batch: 040 ----
mean loss: 31.82
train mean loss: 31.58
epoch train time: 0:00:00.669460
elapsed time: 0:02:42.863776
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:36:29.092511
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.52
 ---- batch: 020 ----
mean loss: 33.03
 ---- batch: 030 ----
mean loss: 30.47
 ---- batch: 040 ----
mean loss: 31.24
train mean loss: 31.46
epoch train time: 0:00:00.688058
elapsed time: 0:02:43.552125
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:36:29.780851
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.04
 ---- batch: 020 ----
mean loss: 30.20
 ---- batch: 030 ----
mean loss: 31.77
 ---- batch: 040 ----
mean loss: 31.51
train mean loss: 31.48
epoch train time: 0:00:00.674825
elapsed time: 0:02:44.227209
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:36:30.455931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.25
 ---- batch: 020 ----
mean loss: 32.61
 ---- batch: 030 ----
mean loss: 29.44
 ---- batch: 040 ----
mean loss: 32.14
train mean loss: 31.63
epoch train time: 0:00:00.676353
elapsed time: 0:02:44.903813
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:36:31.132528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.69
 ---- batch: 020 ----
mean loss: 32.17
 ---- batch: 030 ----
mean loss: 31.84
 ---- batch: 040 ----
mean loss: 31.45
train mean loss: 31.67
epoch train time: 0:00:00.663771
elapsed time: 0:02:45.567843
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:36:31.796561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.96
 ---- batch: 020 ----
mean loss: 31.01
 ---- batch: 030 ----
mean loss: 32.85
 ---- batch: 040 ----
mean loss: 31.86
train mean loss: 31.46
epoch train time: 0:00:00.644910
elapsed time: 0:02:46.213011
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:36:32.441748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.37
 ---- batch: 020 ----
mean loss: 32.21
 ---- batch: 030 ----
mean loss: 32.44
 ---- batch: 040 ----
mean loss: 30.65
train mean loss: 31.38
epoch train time: 0:00:00.676149
elapsed time: 0:02:46.889439
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:36:33.118165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.00
 ---- batch: 020 ----
mean loss: 29.21
 ---- batch: 030 ----
mean loss: 33.65
 ---- batch: 040 ----
mean loss: 31.05
train mean loss: 31.08
epoch train time: 0:00:00.700418
elapsed time: 0:02:47.590184
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:36:33.818918
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.04
 ---- batch: 020 ----
mean loss: 31.80
 ---- batch: 030 ----
mean loss: 31.33
 ---- batch: 040 ----
mean loss: 31.98
train mean loss: 31.27
epoch train time: 0:00:00.701651
elapsed time: 0:02:48.292122
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:36:34.520845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.00
 ---- batch: 020 ----
mean loss: 32.69
 ---- batch: 030 ----
mean loss: 31.87
 ---- batch: 040 ----
mean loss: 29.54
train mean loss: 31.51
epoch train time: 0:00:00.681534
elapsed time: 0:02:48.973869
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:36:35.202579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.38
 ---- batch: 020 ----
mean loss: 29.08
 ---- batch: 030 ----
mean loss: 31.73
 ---- batch: 040 ----
mean loss: 32.82
train mean loss: 31.47
epoch train time: 0:00:00.646661
elapsed time: 0:02:49.620742
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:36:35.849462
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.32
 ---- batch: 020 ----
mean loss: 31.98
 ---- batch: 030 ----
mean loss: 31.11
 ---- batch: 040 ----
mean loss: 29.94
train mean loss: 31.37
epoch train time: 0:00:00.636157
elapsed time: 0:02:50.257123
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:36:36.485834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.72
 ---- batch: 020 ----
mean loss: 32.67
 ---- batch: 030 ----
mean loss: 30.23
 ---- batch: 040 ----
mean loss: 30.20
train mean loss: 31.39
epoch train time: 0:00:00.694290
elapsed time: 0:02:50.951704
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:36:37.180403
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.12
 ---- batch: 020 ----
mean loss: 31.78
 ---- batch: 030 ----
mean loss: 31.68
 ---- batch: 040 ----
mean loss: 30.67
train mean loss: 31.29
epoch train time: 0:00:00.687334
elapsed time: 0:02:51.639267
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:36:37.867981
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.72
 ---- batch: 020 ----
mean loss: 30.42
 ---- batch: 030 ----
mean loss: 29.89
 ---- batch: 040 ----
mean loss: 33.17
train mean loss: 31.35
epoch train time: 0:00:00.677251
elapsed time: 0:02:52.316743
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:36:38.545455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.95
 ---- batch: 020 ----
mean loss: 31.33
 ---- batch: 030 ----
mean loss: 32.02
 ---- batch: 040 ----
mean loss: 30.84
train mean loss: 31.15
epoch train time: 0:00:00.647835
elapsed time: 0:02:52.964785
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:36:39.193506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.78
 ---- batch: 020 ----
mean loss: 30.28
 ---- batch: 030 ----
mean loss: 31.36
 ---- batch: 040 ----
mean loss: 31.03
train mean loss: 30.97
epoch train time: 0:00:00.669505
elapsed time: 0:02:53.634506
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:36:39.863233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.78
 ---- batch: 020 ----
mean loss: 32.36
 ---- batch: 030 ----
mean loss: 31.13
 ---- batch: 040 ----
mean loss: 30.64
train mean loss: 31.25
epoch train time: 0:00:00.666444
elapsed time: 0:02:54.301233
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:36:40.529963
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 28.99
 ---- batch: 020 ----
mean loss: 31.84
 ---- batch: 030 ----
mean loss: 31.60
 ---- batch: 040 ----
mean loss: 31.98
train mean loss: 31.12
epoch train time: 0:00:00.686066
elapsed time: 0:02:54.987564
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:36:41.216297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.63
 ---- batch: 020 ----
mean loss: 32.28
 ---- batch: 030 ----
mean loss: 29.58
 ---- batch: 040 ----
mean loss: 31.23
train mean loss: 31.02
epoch train time: 0:00:00.679938
elapsed time: 0:02:55.667812
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:36:41.896544
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.78
 ---- batch: 020 ----
mean loss: 30.07
 ---- batch: 030 ----
mean loss: 31.65
 ---- batch: 040 ----
mean loss: 31.83
train mean loss: 30.98
epoch train time: 0:00:00.658798
elapsed time: 0:02:56.326849
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:36:42.555560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.33
 ---- batch: 020 ----
mean loss: 30.35
 ---- batch: 030 ----
mean loss: 31.14
 ---- batch: 040 ----
mean loss: 29.76
train mean loss: 30.82
epoch train time: 0:00:00.640441
elapsed time: 0:02:56.967487
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:36:43.196193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.63
 ---- batch: 020 ----
mean loss: 29.45
 ---- batch: 030 ----
mean loss: 31.70
 ---- batch: 040 ----
mean loss: 31.09
train mean loss: 30.92
epoch train time: 0:00:00.655176
elapsed time: 0:02:57.622877
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:36:43.851604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.12
 ---- batch: 020 ----
mean loss: 30.66
 ---- batch: 030 ----
mean loss: 30.92
 ---- batch: 040 ----
mean loss: 30.79
train mean loss: 30.71
epoch train time: 0:00:00.654614
elapsed time: 0:02:58.277746
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:36:44.506460
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.49
 ---- batch: 020 ----
mean loss: 31.38
 ---- batch: 030 ----
mean loss: 30.51
 ---- batch: 040 ----
mean loss: 31.74
train mean loss: 30.81
epoch train time: 0:00:00.684928
elapsed time: 0:02:58.962941
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:36:45.191695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.82
 ---- batch: 020 ----
mean loss: 29.79
 ---- batch: 030 ----
mean loss: 31.43
 ---- batch: 040 ----
mean loss: 33.63
train mean loss: 30.89
epoch train time: 0:00:00.681965
elapsed time: 0:02:59.645267
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:36:45.873984
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.91
 ---- batch: 020 ----
mean loss: 30.86
 ---- batch: 030 ----
mean loss: 30.61
 ---- batch: 040 ----
mean loss: 31.10
train mean loss: 30.75
epoch train time: 0:00:00.651620
elapsed time: 0:03:00.297101
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:36:46.525815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.65
 ---- batch: 020 ----
mean loss: 30.15
 ---- batch: 030 ----
mean loss: 31.00
 ---- batch: 040 ----
mean loss: 29.83
train mean loss: 30.92
epoch train time: 0:00:00.647803
elapsed time: 0:03:00.945106
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:36:47.173822
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.58
 ---- batch: 020 ----
mean loss: 30.39
 ---- batch: 030 ----
mean loss: 31.06
 ---- batch: 040 ----
mean loss: 30.56
train mean loss: 30.77
epoch train time: 0:00:00.656098
elapsed time: 0:03:01.607855
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_0/checkpoint.pth.tar
**** end time: 2019-09-20 21:36:47.836510 ****
