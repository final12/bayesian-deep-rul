Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_1', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6553
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 22:14:46.994132 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 22:14:47.001851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4462.74
 ---- batch: 020 ----
mean loss: 2382.03
 ---- batch: 030 ----
mean loss: 631.56
 ---- batch: 040 ----
mean loss: 435.77
train mean loss: 1871.75
epoch train time: 0:00:15.136524
elapsed time: 0:00:15.146761
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 22:15:02.140930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.22
 ---- batch: 020 ----
mean loss: 350.93
 ---- batch: 030 ----
mean loss: 332.99
 ---- batch: 040 ----
mean loss: 317.70
train mean loss: 345.35
epoch train time: 0:00:01.824519
elapsed time: 0:00:16.971409
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 22:15:03.965597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.21
 ---- batch: 020 ----
mean loss: 312.76
 ---- batch: 030 ----
mean loss: 302.28
 ---- batch: 040 ----
mean loss: 302.93
train mean loss: 307.47
epoch train time: 0:00:01.682490
elapsed time: 0:00:18.654051
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 22:15:05.648231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.08
 ---- batch: 020 ----
mean loss: 273.54
 ---- batch: 030 ----
mean loss: 262.95
 ---- batch: 040 ----
mean loss: 249.60
train mean loss: 265.62
epoch train time: 0:00:01.666287
elapsed time: 0:00:20.320484
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 22:15:07.314678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.26
 ---- batch: 020 ----
mean loss: 245.79
 ---- batch: 030 ----
mean loss: 240.88
 ---- batch: 040 ----
mean loss: 225.49
train mean loss: 239.01
epoch train time: 0:00:01.775219
elapsed time: 0:00:22.095856
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 22:15:09.090034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.34
 ---- batch: 020 ----
mean loss: 213.52
 ---- batch: 030 ----
mean loss: 226.13
 ---- batch: 040 ----
mean loss: 222.64
train mean loss: 222.83
epoch train time: 0:00:01.670841
elapsed time: 0:00:23.766834
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 22:15:10.761016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.12
 ---- batch: 020 ----
mean loss: 228.21
 ---- batch: 030 ----
mean loss: 220.37
 ---- batch: 040 ----
mean loss: 216.71
train mean loss: 219.58
epoch train time: 0:00:01.698958
elapsed time: 0:00:25.465933
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 22:15:12.460111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.10
 ---- batch: 020 ----
mean loss: 220.33
 ---- batch: 030 ----
mean loss: 208.89
 ---- batch: 040 ----
mean loss: 203.44
train mean loss: 211.53
epoch train time: 0:00:01.677158
elapsed time: 0:00:27.143244
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 22:15:14.137441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.03
 ---- batch: 020 ----
mean loss: 213.47
 ---- batch: 030 ----
mean loss: 201.94
 ---- batch: 040 ----
mean loss: 220.58
train mean loss: 210.58
epoch train time: 0:00:01.750776
elapsed time: 0:00:28.894169
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 22:15:15.888348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.58
 ---- batch: 020 ----
mean loss: 202.83
 ---- batch: 030 ----
mean loss: 206.04
 ---- batch: 040 ----
mean loss: 209.88
train mean loss: 207.63
epoch train time: 0:00:01.698928
elapsed time: 0:00:30.593235
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 22:15:17.587431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.25
 ---- batch: 020 ----
mean loss: 211.26
 ---- batch: 030 ----
mean loss: 196.13
 ---- batch: 040 ----
mean loss: 202.16
train mean loss: 204.40
epoch train time: 0:00:01.675968
elapsed time: 0:00:32.269358
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 22:15:19.263550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.45
 ---- batch: 020 ----
mean loss: 206.87
 ---- batch: 030 ----
mean loss: 197.03
 ---- batch: 040 ----
mean loss: 203.93
train mean loss: 201.78
epoch train time: 0:00:01.779930
elapsed time: 0:00:34.049482
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 22:15:21.043678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.50
 ---- batch: 020 ----
mean loss: 201.60
 ---- batch: 030 ----
mean loss: 199.19
 ---- batch: 040 ----
mean loss: 195.41
train mean loss: 200.12
epoch train time: 0:00:01.675808
elapsed time: 0:00:35.725447
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 22:15:22.719655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.55
 ---- batch: 020 ----
mean loss: 197.87
 ---- batch: 030 ----
mean loss: 200.11
 ---- batch: 040 ----
mean loss: 198.98
train mean loss: 202.10
epoch train time: 0:00:01.771898
elapsed time: 0:00:37.497512
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 22:15:24.491690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.49
 ---- batch: 020 ----
mean loss: 200.50
 ---- batch: 030 ----
mean loss: 192.38
 ---- batch: 040 ----
mean loss: 197.16
train mean loss: 198.30
epoch train time: 0:00:01.675241
elapsed time: 0:00:39.172892
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 22:15:26.167070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.64
 ---- batch: 020 ----
mean loss: 197.54
 ---- batch: 030 ----
mean loss: 188.62
 ---- batch: 040 ----
mean loss: 195.33
train mean loss: 196.25
epoch train time: 0:00:01.673615
elapsed time: 0:00:40.846642
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 22:15:27.840822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.57
 ---- batch: 020 ----
mean loss: 198.88
 ---- batch: 030 ----
mean loss: 200.49
 ---- batch: 040 ----
mean loss: 192.10
train mean loss: 196.67
epoch train time: 0:00:01.786134
elapsed time: 0:00:42.632912
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 22:15:29.627086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.97
 ---- batch: 020 ----
mean loss: 195.84
 ---- batch: 030 ----
mean loss: 201.88
 ---- batch: 040 ----
mean loss: 199.11
train mean loss: 196.75
epoch train time: 0:00:01.682322
elapsed time: 0:00:44.315380
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 22:15:31.309562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.19
 ---- batch: 020 ----
mean loss: 200.93
 ---- batch: 030 ----
mean loss: 205.14
 ---- batch: 040 ----
mean loss: 186.63
train mean loss: 197.93
epoch train time: 0:00:01.787427
elapsed time: 0:00:46.102937
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 22:15:33.097113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.85
 ---- batch: 020 ----
mean loss: 192.78
 ---- batch: 030 ----
mean loss: 192.56
 ---- batch: 040 ----
mean loss: 198.06
train mean loss: 193.35
epoch train time: 0:00:01.681020
elapsed time: 0:00:47.784092
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 22:15:34.778305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.89
 ---- batch: 020 ----
mean loss: 192.54
 ---- batch: 030 ----
mean loss: 192.65
 ---- batch: 040 ----
mean loss: 188.07
train mean loss: 191.67
epoch train time: 0:00:01.773890
elapsed time: 0:00:49.558141
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 22:15:36.552316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.98
 ---- batch: 020 ----
mean loss: 202.98
 ---- batch: 030 ----
mean loss: 187.29
 ---- batch: 040 ----
mean loss: 191.39
train mean loss: 192.24
epoch train time: 0:00:01.687803
elapsed time: 0:00:51.246068
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 22:15:38.240244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.99
 ---- batch: 020 ----
mean loss: 193.98
 ---- batch: 030 ----
mean loss: 186.93
 ---- batch: 040 ----
mean loss: 192.19
train mean loss: 190.62
epoch train time: 0:00:01.682830
elapsed time: 0:00:52.929073
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 22:15:39.923261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.02
 ---- batch: 020 ----
mean loss: 189.24
 ---- batch: 030 ----
mean loss: 192.17
 ---- batch: 040 ----
mean loss: 191.17
train mean loss: 190.43
epoch train time: 0:00:01.779603
elapsed time: 0:00:54.708818
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 22:15:41.702995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.93
 ---- batch: 020 ----
mean loss: 196.16
 ---- batch: 030 ----
mean loss: 195.78
 ---- batch: 040 ----
mean loss: 202.41
train mean loss: 195.58
epoch train time: 0:00:01.682427
elapsed time: 0:00:56.391438
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 22:15:43.385618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.83
 ---- batch: 020 ----
mean loss: 192.85
 ---- batch: 030 ----
mean loss: 183.11
 ---- batch: 040 ----
mean loss: 193.45
train mean loss: 191.18
epoch train time: 0:00:01.794829
elapsed time: 0:00:58.186413
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 22:15:45.180631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.64
 ---- batch: 020 ----
mean loss: 195.41
 ---- batch: 030 ----
mean loss: 191.09
 ---- batch: 040 ----
mean loss: 184.52
train mean loss: 189.97
epoch train time: 0:00:01.681587
elapsed time: 0:00:59.868203
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 22:15:46.862385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.92
 ---- batch: 020 ----
mean loss: 191.80
 ---- batch: 030 ----
mean loss: 193.99
 ---- batch: 040 ----
mean loss: 195.11
train mean loss: 190.42
epoch train time: 0:00:01.675555
elapsed time: 0:01:01.543899
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 22:15:48.538076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.90
 ---- batch: 020 ----
mean loss: 195.13
 ---- batch: 030 ----
mean loss: 193.63
 ---- batch: 040 ----
mean loss: 190.46
train mean loss: 194.17
epoch train time: 0:00:01.800849
elapsed time: 0:01:03.344884
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 22:15:50.339083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.70
 ---- batch: 020 ----
mean loss: 199.76
 ---- batch: 030 ----
mean loss: 184.84
 ---- batch: 040 ----
mean loss: 182.18
train mean loss: 188.35
epoch train time: 0:00:01.679709
elapsed time: 0:01:05.024766
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 22:15:52.018952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.76
 ---- batch: 020 ----
mean loss: 186.10
 ---- batch: 030 ----
mean loss: 190.20
 ---- batch: 040 ----
mean loss: 186.56
train mean loss: 187.30
epoch train time: 0:00:01.799217
elapsed time: 0:01:06.824154
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 22:15:53.818374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.16
 ---- batch: 020 ----
mean loss: 191.93
 ---- batch: 030 ----
mean loss: 186.22
 ---- batch: 040 ----
mean loss: 183.74
train mean loss: 188.37
epoch train time: 0:00:01.687995
elapsed time: 0:01:08.512330
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 22:15:55.506509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.64
 ---- batch: 020 ----
mean loss: 183.82
 ---- batch: 030 ----
mean loss: 185.48
 ---- batch: 040 ----
mean loss: 192.24
train mean loss: 188.69
epoch train time: 0:00:01.789332
elapsed time: 0:01:10.301805
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 22:15:57.295982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.77
 ---- batch: 020 ----
mean loss: 191.62
 ---- batch: 030 ----
mean loss: 185.26
 ---- batch: 040 ----
mean loss: 185.30
train mean loss: 188.47
epoch train time: 0:00:01.680605
elapsed time: 0:01:11.982586
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 22:15:58.976766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.96
 ---- batch: 020 ----
mean loss: 187.15
 ---- batch: 030 ----
mean loss: 182.76
 ---- batch: 040 ----
mean loss: 183.11
train mean loss: 183.43
epoch train time: 0:00:01.678131
elapsed time: 0:01:13.660864
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 22:16:00.655087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.71
 ---- batch: 020 ----
mean loss: 178.09
 ---- batch: 030 ----
mean loss: 180.44
 ---- batch: 040 ----
mean loss: 189.78
train mean loss: 183.86
epoch train time: 0:00:01.786658
elapsed time: 0:01:15.447702
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 22:16:02.441922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.77
 ---- batch: 020 ----
mean loss: 183.85
 ---- batch: 030 ----
mean loss: 185.60
 ---- batch: 040 ----
mean loss: 185.18
train mean loss: 183.92
epoch train time: 0:00:01.685569
elapsed time: 0:01:17.133466
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 22:16:04.127645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.92
 ---- batch: 020 ----
mean loss: 181.07
 ---- batch: 030 ----
mean loss: 182.14
 ---- batch: 040 ----
mean loss: 186.92
train mean loss: 184.46
epoch train time: 0:00:01.790317
elapsed time: 0:01:18.923916
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 22:16:05.918096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.89
 ---- batch: 020 ----
mean loss: 182.11
 ---- batch: 030 ----
mean loss: 185.77
 ---- batch: 040 ----
mean loss: 192.49
train mean loss: 185.87
epoch train time: 0:00:01.683163
elapsed time: 0:01:20.607242
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 22:16:07.601421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.22
 ---- batch: 020 ----
mean loss: 181.43
 ---- batch: 030 ----
mean loss: 187.66
 ---- batch: 040 ----
mean loss: 181.28
train mean loss: 183.20
epoch train time: 0:00:01.779297
elapsed time: 0:01:22.386710
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 22:16:09.380888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.03
 ---- batch: 020 ----
mean loss: 182.39
 ---- batch: 030 ----
mean loss: 187.85
 ---- batch: 040 ----
mean loss: 191.15
train mean loss: 185.93
epoch train time: 0:00:01.693490
elapsed time: 0:01:24.080340
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 22:16:11.074521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.00
 ---- batch: 020 ----
mean loss: 176.05
 ---- batch: 030 ----
mean loss: 193.97
 ---- batch: 040 ----
mean loss: 177.12
train mean loss: 183.83
epoch train time: 0:00:01.679115
elapsed time: 0:01:25.759611
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 22:16:12.753799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.96
 ---- batch: 020 ----
mean loss: 183.80
 ---- batch: 030 ----
mean loss: 190.04
 ---- batch: 040 ----
mean loss: 183.38
train mean loss: 184.09
epoch train time: 0:00:01.789820
elapsed time: 0:01:27.549572
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 22:16:14.543773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.42
 ---- batch: 020 ----
mean loss: 181.62
 ---- batch: 030 ----
mean loss: 180.00
 ---- batch: 040 ----
mean loss: 185.52
train mean loss: 183.44
epoch train time: 0:00:01.679282
elapsed time: 0:01:29.229015
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 22:16:16.223199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.28
 ---- batch: 020 ----
mean loss: 178.72
 ---- batch: 030 ----
mean loss: 177.91
 ---- batch: 040 ----
mean loss: 182.67
train mean loss: 181.41
epoch train time: 0:00:01.784850
elapsed time: 0:01:31.013994
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 22:16:18.008170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.96
 ---- batch: 020 ----
mean loss: 180.35
 ---- batch: 030 ----
mean loss: 181.76
 ---- batch: 040 ----
mean loss: 183.90
train mean loss: 181.13
epoch train time: 0:00:01.684085
elapsed time: 0:01:32.698232
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 22:16:19.692441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.00
 ---- batch: 020 ----
mean loss: 181.79
 ---- batch: 030 ----
mean loss: 185.23
 ---- batch: 040 ----
mean loss: 181.58
train mean loss: 182.34
epoch train time: 0:00:01.685143
elapsed time: 0:01:34.383571
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 22:16:21.377766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.98
 ---- batch: 020 ----
mean loss: 173.54
 ---- batch: 030 ----
mean loss: 188.81
 ---- batch: 040 ----
mean loss: 183.02
train mean loss: 181.65
epoch train time: 0:00:01.778991
elapsed time: 0:01:36.162717
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 22:16:23.156901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.96
 ---- batch: 020 ----
mean loss: 180.17
 ---- batch: 030 ----
mean loss: 185.44
 ---- batch: 040 ----
mean loss: 187.92
train mean loss: 183.20
epoch train time: 0:00:01.689203
elapsed time: 0:01:37.852058
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 22:16:24.846236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.99
 ---- batch: 020 ----
mean loss: 176.18
 ---- batch: 030 ----
mean loss: 177.34
 ---- batch: 040 ----
mean loss: 183.14
train mean loss: 179.03
epoch train time: 0:00:01.776106
elapsed time: 0:01:39.628286
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 22:16:26.622488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.46
 ---- batch: 020 ----
mean loss: 180.63
 ---- batch: 030 ----
mean loss: 175.35
 ---- batch: 040 ----
mean loss: 178.70
train mean loss: 178.97
epoch train time: 0:00:01.687937
elapsed time: 0:01:41.316380
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 22:16:28.310574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.41
 ---- batch: 020 ----
mean loss: 174.13
 ---- batch: 030 ----
mean loss: 185.12
 ---- batch: 040 ----
mean loss: 173.47
train mean loss: 177.98
epoch train time: 0:00:01.790450
elapsed time: 0:01:43.106967
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 22:16:30.101141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.02
 ---- batch: 020 ----
mean loss: 169.80
 ---- batch: 030 ----
mean loss: 171.89
 ---- batch: 040 ----
mean loss: 176.90
train mean loss: 177.05
epoch train time: 0:00:01.685201
elapsed time: 0:01:44.792318
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 22:16:31.786505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.43
 ---- batch: 020 ----
mean loss: 176.69
 ---- batch: 030 ----
mean loss: 181.61
 ---- batch: 040 ----
mean loss: 180.19
train mean loss: 179.03
epoch train time: 0:00:01.676636
elapsed time: 0:01:46.469091
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 22:16:33.463281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.30
 ---- batch: 020 ----
mean loss: 178.90
 ---- batch: 030 ----
mean loss: 175.37
 ---- batch: 040 ----
mean loss: 171.05
train mean loss: 176.96
epoch train time: 0:00:01.778900
elapsed time: 0:01:48.248157
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 22:16:35.242395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.03
 ---- batch: 020 ----
mean loss: 174.12
 ---- batch: 030 ----
mean loss: 175.61
 ---- batch: 040 ----
mean loss: 173.19
train mean loss: 175.13
epoch train time: 0:00:01.683052
elapsed time: 0:01:49.931420
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 22:16:36.925613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.05
 ---- batch: 020 ----
mean loss: 175.57
 ---- batch: 030 ----
mean loss: 170.25
 ---- batch: 040 ----
mean loss: 175.17
train mean loss: 175.67
epoch train time: 0:00:01.788117
elapsed time: 0:01:51.719821
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 22:16:38.714018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.92
 ---- batch: 020 ----
mean loss: 171.96
 ---- batch: 030 ----
mean loss: 178.31
 ---- batch: 040 ----
mean loss: 174.56
train mean loss: 174.48
epoch train time: 0:00:01.677313
elapsed time: 0:01:53.397283
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 22:16:40.391463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.89
 ---- batch: 020 ----
mean loss: 170.95
 ---- batch: 030 ----
mean loss: 172.63
 ---- batch: 040 ----
mean loss: 176.28
train mean loss: 172.16
epoch train time: 0:00:01.778423
elapsed time: 0:01:55.175852
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 22:16:42.170076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.15
 ---- batch: 020 ----
mean loss: 169.63
 ---- batch: 030 ----
mean loss: 171.18
 ---- batch: 040 ----
mean loss: 170.65
train mean loss: 170.54
epoch train time: 0:00:01.692061
elapsed time: 0:01:56.868103
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 22:16:43.862300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.83
 ---- batch: 020 ----
mean loss: 166.25
 ---- batch: 030 ----
mean loss: 174.15
 ---- batch: 040 ----
mean loss: 170.06
train mean loss: 169.22
epoch train time: 0:00:01.680712
elapsed time: 0:01:58.548959
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 22:16:45.543137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.48
 ---- batch: 020 ----
mean loss: 174.82
 ---- batch: 030 ----
mean loss: 166.41
 ---- batch: 040 ----
mean loss: 170.37
train mean loss: 170.09
epoch train time: 0:00:01.783281
elapsed time: 0:02:00.332384
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 22:16:47.326568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.04
 ---- batch: 020 ----
mean loss: 168.84
 ---- batch: 030 ----
mean loss: 169.15
 ---- batch: 040 ----
mean loss: 174.23
train mean loss: 170.21
epoch train time: 0:00:01.686670
elapsed time: 0:02:02.019212
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 22:16:49.013408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.12
 ---- batch: 020 ----
mean loss: 169.94
 ---- batch: 030 ----
mean loss: 175.40
 ---- batch: 040 ----
mean loss: 165.58
train mean loss: 169.97
epoch train time: 0:00:01.787267
elapsed time: 0:02:03.806657
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 22:16:50.800837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.26
 ---- batch: 020 ----
mean loss: 164.85
 ---- batch: 030 ----
mean loss: 171.61
 ---- batch: 040 ----
mean loss: 163.33
train mean loss: 166.37
epoch train time: 0:00:01.691213
elapsed time: 0:02:05.498005
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 22:16:52.492184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.83
 ---- batch: 020 ----
mean loss: 167.60
 ---- batch: 030 ----
mean loss: 165.82
 ---- batch: 040 ----
mean loss: 166.01
train mean loss: 166.64
epoch train time: 0:00:01.733717
elapsed time: 0:02:07.231867
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 22:16:54.226047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.05
 ---- batch: 020 ----
mean loss: 160.97
 ---- batch: 030 ----
mean loss: 165.56
 ---- batch: 040 ----
mean loss: 171.10
train mean loss: 165.34
epoch train time: 0:00:01.730442
elapsed time: 0:02:08.962448
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 22:16:55.956640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.04
 ---- batch: 020 ----
mean loss: 166.89
 ---- batch: 030 ----
mean loss: 170.01
 ---- batch: 040 ----
mean loss: 161.78
train mean loss: 166.68
epoch train time: 0:00:01.683581
elapsed time: 0:02:10.646178
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 22:16:57.640359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.45
 ---- batch: 020 ----
mean loss: 161.18
 ---- batch: 030 ----
mean loss: 161.34
 ---- batch: 040 ----
mean loss: 164.95
train mean loss: 163.26
epoch train time: 0:00:01.797288
elapsed time: 0:02:12.443595
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 22:16:59.437771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.04
 ---- batch: 020 ----
mean loss: 167.22
 ---- batch: 030 ----
mean loss: 157.56
 ---- batch: 040 ----
mean loss: 164.21
train mean loss: 164.04
epoch train time: 0:00:01.683458
elapsed time: 0:02:14.127216
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 22:17:01.121395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.80
 ---- batch: 020 ----
mean loss: 164.12
 ---- batch: 030 ----
mean loss: 168.63
 ---- batch: 040 ----
mean loss: 164.30
train mean loss: 165.96
epoch train time: 0:00:01.784625
elapsed time: 0:02:15.911963
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 22:17:02.906138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.24
 ---- batch: 020 ----
mean loss: 165.26
 ---- batch: 030 ----
mean loss: 162.96
 ---- batch: 040 ----
mean loss: 162.26
train mean loss: 163.16
epoch train time: 0:00:01.694767
elapsed time: 0:02:17.606876
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 22:17:04.601057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.49
 ---- batch: 020 ----
mean loss: 168.82
 ---- batch: 030 ----
mean loss: 164.42
 ---- batch: 040 ----
mean loss: 162.77
train mean loss: 163.27
epoch train time: 0:00:01.681487
elapsed time: 0:02:19.288492
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 22:17:06.282682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.74
 ---- batch: 020 ----
mean loss: 155.47
 ---- batch: 030 ----
mean loss: 159.62
 ---- batch: 040 ----
mean loss: 159.94
train mean loss: 158.97
epoch train time: 0:00:01.789666
elapsed time: 0:02:21.078313
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 22:17:08.072495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.06
 ---- batch: 020 ----
mean loss: 158.79
 ---- batch: 030 ----
mean loss: 158.21
 ---- batch: 040 ----
mean loss: 158.00
train mean loss: 158.43
epoch train time: 0:00:01.680840
elapsed time: 0:02:22.759291
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 22:17:09.753469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.34
 ---- batch: 020 ----
mean loss: 156.53
 ---- batch: 030 ----
mean loss: 163.00
 ---- batch: 040 ----
mean loss: 159.18
train mean loss: 159.37
epoch train time: 0:00:01.785490
elapsed time: 0:02:24.544925
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 22:17:11.539102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.06
 ---- batch: 020 ----
mean loss: 159.58
 ---- batch: 030 ----
mean loss: 154.44
 ---- batch: 040 ----
mean loss: 165.26
train mean loss: 159.12
epoch train time: 0:00:01.681860
elapsed time: 0:02:26.226965
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 22:17:13.221191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.62
 ---- batch: 020 ----
mean loss: 160.06
 ---- batch: 030 ----
mean loss: 156.04
 ---- batch: 040 ----
mean loss: 154.37
train mean loss: 157.49
epoch train time: 0:00:01.791340
elapsed time: 0:02:28.018509
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 22:17:15.012716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.23
 ---- batch: 020 ----
mean loss: 149.24
 ---- batch: 030 ----
mean loss: 152.35
 ---- batch: 040 ----
mean loss: 155.84
train mean loss: 154.10
epoch train time: 0:00:01.692597
elapsed time: 0:02:29.711285
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 22:17:16.705489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.77
 ---- batch: 020 ----
mean loss: 161.59
 ---- batch: 030 ----
mean loss: 156.61
 ---- batch: 040 ----
mean loss: 154.04
train mean loss: 157.85
epoch train time: 0:00:01.681580
elapsed time: 0:02:31.393024
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 22:17:18.387231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.28
 ---- batch: 020 ----
mean loss: 158.71
 ---- batch: 030 ----
mean loss: 156.79
 ---- batch: 040 ----
mean loss: 155.40
train mean loss: 156.61
epoch train time: 0:00:01.791317
elapsed time: 0:02:33.184534
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 22:17:20.178715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.06
 ---- batch: 020 ----
mean loss: 160.15
 ---- batch: 030 ----
mean loss: 158.73
 ---- batch: 040 ----
mean loss: 156.25
train mean loss: 155.36
epoch train time: 0:00:01.691517
elapsed time: 0:02:34.876202
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 22:17:21.870410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.18
 ---- batch: 020 ----
mean loss: 161.32
 ---- batch: 030 ----
mean loss: 154.28
 ---- batch: 040 ----
mean loss: 155.22
train mean loss: 157.34
epoch train time: 0:00:01.786624
elapsed time: 0:02:36.663014
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 22:17:23.657191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.61
 ---- batch: 020 ----
mean loss: 155.49
 ---- batch: 030 ----
mean loss: 151.88
 ---- batch: 040 ----
mean loss: 157.79
train mean loss: 153.31
epoch train time: 0:00:01.683967
elapsed time: 0:02:38.347108
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 22:17:25.341301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.41
 ---- batch: 020 ----
mean loss: 151.91
 ---- batch: 030 ----
mean loss: 155.26
 ---- batch: 040 ----
mean loss: 149.87
train mean loss: 153.67
epoch train time: 0:00:01.778487
elapsed time: 0:02:40.125735
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 22:17:27.119911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.38
 ---- batch: 020 ----
mean loss: 157.03
 ---- batch: 030 ----
mean loss: 154.82
 ---- batch: 040 ----
mean loss: 154.85
train mean loss: 153.57
epoch train time: 0:00:01.691964
elapsed time: 0:02:41.817871
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 22:17:28.812054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.81
 ---- batch: 020 ----
mean loss: 161.42
 ---- batch: 030 ----
mean loss: 153.82
 ---- batch: 040 ----
mean loss: 158.48
train mean loss: 160.59
epoch train time: 0:00:01.684175
elapsed time: 0:02:43.502178
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 22:17:30.496357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.26
 ---- batch: 020 ----
mean loss: 149.55
 ---- batch: 030 ----
mean loss: 155.88
 ---- batch: 040 ----
mean loss: 147.75
train mean loss: 152.20
epoch train time: 0:00:01.786856
elapsed time: 0:02:45.289157
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 22:17:32.283332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.10
 ---- batch: 020 ----
mean loss: 149.43
 ---- batch: 030 ----
mean loss: 149.02
 ---- batch: 040 ----
mean loss: 146.55
train mean loss: 149.62
epoch train time: 0:00:01.686866
elapsed time: 0:02:46.976154
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 22:17:33.970335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.85
 ---- batch: 020 ----
mean loss: 145.41
 ---- batch: 030 ----
mean loss: 149.42
 ---- batch: 040 ----
mean loss: 154.87
train mean loss: 150.08
epoch train time: 0:00:01.783823
elapsed time: 0:02:48.760103
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 22:17:35.754318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.96
 ---- batch: 020 ----
mean loss: 155.65
 ---- batch: 030 ----
mean loss: 146.06
 ---- batch: 040 ----
mean loss: 152.09
train mean loss: 151.38
epoch train time: 0:00:01.688103
elapsed time: 0:02:50.448407
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 22:17:37.442629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.97
 ---- batch: 020 ----
mean loss: 151.59
 ---- batch: 030 ----
mean loss: 145.49
 ---- batch: 040 ----
mean loss: 152.40
train mean loss: 150.39
epoch train time: 0:00:01.678534
elapsed time: 0:02:52.127139
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 22:17:39.121315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.70
 ---- batch: 020 ----
mean loss: 151.40
 ---- batch: 030 ----
mean loss: 152.26
 ---- batch: 040 ----
mean loss: 149.55
train mean loss: 150.06
epoch train time: 0:00:01.790480
elapsed time: 0:02:53.917756
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 22:17:40.911936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.70
 ---- batch: 020 ----
mean loss: 153.18
 ---- batch: 030 ----
mean loss: 145.95
 ---- batch: 040 ----
mean loss: 151.71
train mean loss: 150.10
epoch train time: 0:00:01.682315
elapsed time: 0:02:55.600234
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 22:17:42.594413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.80
 ---- batch: 020 ----
mean loss: 148.74
 ---- batch: 030 ----
mean loss: 146.46
 ---- batch: 040 ----
mean loss: 147.91
train mean loss: 147.83
epoch train time: 0:00:01.713795
elapsed time: 0:02:57.314170
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 22:17:44.308347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.45
 ---- batch: 020 ----
mean loss: 149.24
 ---- batch: 030 ----
mean loss: 145.88
 ---- batch: 040 ----
mean loss: 152.11
train mean loss: 149.62
epoch train time: 0:00:01.689921
elapsed time: 0:02:59.004237
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 22:17:45.998408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.16
 ---- batch: 020 ----
mean loss: 152.76
 ---- batch: 030 ----
mean loss: 144.86
 ---- batch: 040 ----
mean loss: 152.93
train mean loss: 153.32
epoch train time: 0:00:01.779237
elapsed time: 0:03:00.783603
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 22:17:47.777779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.72
 ---- batch: 020 ----
mean loss: 149.23
 ---- batch: 030 ----
mean loss: 157.21
 ---- batch: 040 ----
mean loss: 151.74
train mean loss: 152.37
epoch train time: 0:00:01.691567
elapsed time: 0:03:02.475305
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 22:17:49.469486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.26
 ---- batch: 020 ----
mean loss: 152.63
 ---- batch: 030 ----
mean loss: 152.67
 ---- batch: 040 ----
mean loss: 149.49
train mean loss: 149.30
epoch train time: 0:00:01.678217
elapsed time: 0:03:04.153663
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 22:17:51.147843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.04
 ---- batch: 020 ----
mean loss: 152.77
 ---- batch: 030 ----
mean loss: 150.79
 ---- batch: 040 ----
mean loss: 148.37
train mean loss: 149.73
epoch train time: 0:00:01.785671
elapsed time: 0:03:05.939465
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 22:17:52.933642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.59
 ---- batch: 020 ----
mean loss: 155.25
 ---- batch: 030 ----
mean loss: 157.82
 ---- batch: 040 ----
mean loss: 151.15
train mean loss: 153.67
epoch train time: 0:00:01.692701
elapsed time: 0:03:07.632302
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 22:17:54.626485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.03
 ---- batch: 020 ----
mean loss: 149.41
 ---- batch: 030 ----
mean loss: 150.95
 ---- batch: 040 ----
mean loss: 152.07
train mean loss: 152.12
epoch train time: 0:00:01.788479
elapsed time: 0:03:09.420910
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 22:17:56.415101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.88
 ---- batch: 020 ----
mean loss: 144.78
 ---- batch: 030 ----
mean loss: 148.35
 ---- batch: 040 ----
mean loss: 148.27
train mean loss: 146.47
epoch train time: 0:00:01.690106
elapsed time: 0:03:11.111165
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 22:17:58.105356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.17
 ---- batch: 020 ----
mean loss: 154.74
 ---- batch: 030 ----
mean loss: 149.56
 ---- batch: 040 ----
mean loss: 150.13
train mean loss: 152.36
epoch train time: 0:00:01.678333
elapsed time: 0:03:12.789669
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 22:17:59.783893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.02
 ---- batch: 020 ----
mean loss: 151.71
 ---- batch: 030 ----
mean loss: 144.85
 ---- batch: 040 ----
mean loss: 152.40
train mean loss: 150.47
epoch train time: 0:00:01.780297
elapsed time: 0:03:14.570148
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 22:18:01.564329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.08
 ---- batch: 020 ----
mean loss: 151.28
 ---- batch: 030 ----
mean loss: 151.83
 ---- batch: 040 ----
mean loss: 151.65
train mean loss: 151.72
epoch train time: 0:00:01.680718
elapsed time: 0:03:16.251019
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 22:18:03.245200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.90
 ---- batch: 020 ----
mean loss: 149.40
 ---- batch: 030 ----
mean loss: 143.14
 ---- batch: 040 ----
mean loss: 148.18
train mean loss: 147.98
epoch train time: 0:00:01.782433
elapsed time: 0:03:18.033596
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 22:18:05.027810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.82
 ---- batch: 020 ----
mean loss: 151.20
 ---- batch: 030 ----
mean loss: 147.32
 ---- batch: 040 ----
mean loss: 145.22
train mean loss: 149.56
epoch train time: 0:00:01.682364
elapsed time: 0:03:19.716142
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 22:18:06.710336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.35
 ---- batch: 020 ----
mean loss: 150.57
 ---- batch: 030 ----
mean loss: 152.69
 ---- batch: 040 ----
mean loss: 155.76
train mean loss: 152.47
epoch train time: 0:00:01.782895
elapsed time: 0:03:21.499229
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 22:18:08.493436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.37
 ---- batch: 020 ----
mean loss: 149.27
 ---- batch: 030 ----
mean loss: 153.62
 ---- batch: 040 ----
mean loss: 143.71
train mean loss: 148.91
epoch train time: 0:00:01.689746
elapsed time: 0:03:23.189137
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 22:18:10.183318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.46
 ---- batch: 020 ----
mean loss: 148.06
 ---- batch: 030 ----
mean loss: 152.83
 ---- batch: 040 ----
mean loss: 150.30
train mean loss: 149.74
epoch train time: 0:00:01.674575
elapsed time: 0:03:24.863859
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 22:18:11.858038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.32
 ---- batch: 020 ----
mean loss: 148.94
 ---- batch: 030 ----
mean loss: 142.23
 ---- batch: 040 ----
mean loss: 150.58
train mean loss: 148.94
epoch train time: 0:00:01.781528
elapsed time: 0:03:26.645510
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 22:18:13.639686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.60
 ---- batch: 020 ----
mean loss: 151.69
 ---- batch: 030 ----
mean loss: 150.83
 ---- batch: 040 ----
mean loss: 147.42
train mean loss: 150.02
epoch train time: 0:00:01.681990
elapsed time: 0:03:28.327648
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 22:18:15.321828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.95
 ---- batch: 020 ----
mean loss: 149.90
 ---- batch: 030 ----
mean loss: 147.56
 ---- batch: 040 ----
mean loss: 148.99
train mean loss: 146.89
epoch train time: 0:00:01.797656
elapsed time: 0:03:30.125446
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 22:18:17.119637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.92
 ---- batch: 020 ----
mean loss: 146.87
 ---- batch: 030 ----
mean loss: 151.59
 ---- batch: 040 ----
mean loss: 148.54
train mean loss: 148.76
epoch train time: 0:00:01.688020
elapsed time: 0:03:31.813621
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 22:18:18.807833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.79
 ---- batch: 020 ----
mean loss: 147.22
 ---- batch: 030 ----
mean loss: 152.24
 ---- batch: 040 ----
mean loss: 145.63
train mean loss: 147.87
epoch train time: 0:00:01.775276
elapsed time: 0:03:33.589076
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 22:18:20.583254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.61
 ---- batch: 020 ----
mean loss: 149.75
 ---- batch: 030 ----
mean loss: 148.65
 ---- batch: 040 ----
mean loss: 153.91
train mean loss: 150.66
epoch train time: 0:00:01.682680
elapsed time: 0:03:35.271897
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 22:18:22.266095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.50
 ---- batch: 020 ----
mean loss: 151.11
 ---- batch: 030 ----
mean loss: 147.66
 ---- batch: 040 ----
mean loss: 145.31
train mean loss: 147.45
epoch train time: 0:00:01.682272
elapsed time: 0:03:36.954339
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 22:18:23.948518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.28
 ---- batch: 020 ----
mean loss: 149.44
 ---- batch: 030 ----
mean loss: 146.72
 ---- batch: 040 ----
mean loss: 148.26
train mean loss: 148.15
epoch train time: 0:00:01.790355
elapsed time: 0:03:38.744840
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 22:18:25.739034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.85
 ---- batch: 020 ----
mean loss: 150.61
 ---- batch: 030 ----
mean loss: 151.37
 ---- batch: 040 ----
mean loss: 142.73
train mean loss: 148.71
epoch train time: 0:00:01.681082
elapsed time: 0:03:40.426079
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 22:18:27.420261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.74
 ---- batch: 020 ----
mean loss: 142.53
 ---- batch: 030 ----
mean loss: 149.90
 ---- batch: 040 ----
mean loss: 143.93
train mean loss: 146.45
epoch train time: 0:00:01.777971
elapsed time: 0:03:42.204185
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 22:18:29.198360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.39
 ---- batch: 020 ----
mean loss: 149.93
 ---- batch: 030 ----
mean loss: 142.21
 ---- batch: 040 ----
mean loss: 152.07
train mean loss: 146.65
epoch train time: 0:00:01.683820
elapsed time: 0:03:43.888174
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 22:18:30.882356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.96
 ---- batch: 020 ----
mean loss: 151.20
 ---- batch: 030 ----
mean loss: 149.44
 ---- batch: 040 ----
mean loss: 146.28
train mean loss: 150.95
epoch train time: 0:00:01.789239
elapsed time: 0:03:45.677548
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 22:18:32.671754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.31
 ---- batch: 020 ----
mean loss: 149.55
 ---- batch: 030 ----
mean loss: 149.91
 ---- batch: 040 ----
mean loss: 153.63
train mean loss: 150.25
epoch train time: 0:00:01.687039
elapsed time: 0:03:47.364754
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 22:18:34.358949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.76
 ---- batch: 020 ----
mean loss: 142.18
 ---- batch: 030 ----
mean loss: 146.09
 ---- batch: 040 ----
mean loss: 147.65
train mean loss: 146.34
epoch train time: 0:00:01.786011
elapsed time: 0:03:49.150907
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 22:18:36.145097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.20
 ---- batch: 020 ----
mean loss: 147.65
 ---- batch: 030 ----
mean loss: 150.41
 ---- batch: 040 ----
mean loss: 144.44
train mean loss: 146.94
epoch train time: 0:00:01.687216
elapsed time: 0:03:50.838314
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 22:18:37.832501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.75
 ---- batch: 020 ----
mean loss: 150.83
 ---- batch: 030 ----
mean loss: 154.13
 ---- batch: 040 ----
mean loss: 146.72
train mean loss: 149.93
epoch train time: 0:00:01.737660
elapsed time: 0:03:52.576146
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 22:18:39.570328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.09
 ---- batch: 020 ----
mean loss: 149.71
 ---- batch: 030 ----
mean loss: 148.20
 ---- batch: 040 ----
mean loss: 148.13
train mean loss: 148.85
epoch train time: 0:00:01.754660
elapsed time: 0:03:54.330970
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 22:18:41.325153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.12
 ---- batch: 020 ----
mean loss: 143.54
 ---- batch: 030 ----
mean loss: 152.76
 ---- batch: 040 ----
mean loss: 147.91
train mean loss: 146.94
epoch train time: 0:00:01.685190
elapsed time: 0:03:56.016298
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 22:18:43.010480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.79
 ---- batch: 020 ----
mean loss: 141.75
 ---- batch: 030 ----
mean loss: 145.57
 ---- batch: 040 ----
mean loss: 145.88
train mean loss: 145.94
epoch train time: 0:00:01.787214
elapsed time: 0:03:57.803666
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 22:18:44.797848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.24
 ---- batch: 020 ----
mean loss: 146.18
 ---- batch: 030 ----
mean loss: 144.57
 ---- batch: 040 ----
mean loss: 145.09
train mean loss: 146.17
epoch train time: 0:00:01.687153
elapsed time: 0:03:59.490952
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 22:18:46.485132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.78
 ---- batch: 020 ----
mean loss: 148.68
 ---- batch: 030 ----
mean loss: 143.61
 ---- batch: 040 ----
mean loss: 144.67
train mean loss: 146.02
epoch train time: 0:00:01.789728
elapsed time: 0:04:01.280810
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 22:18:48.274987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.19
 ---- batch: 020 ----
mean loss: 150.81
 ---- batch: 030 ----
mean loss: 148.44
 ---- batch: 040 ----
mean loss: 148.18
train mean loss: 149.24
epoch train time: 0:00:01.683152
elapsed time: 0:04:02.964093
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 22:18:49.958271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.73
 ---- batch: 020 ----
mean loss: 148.71
 ---- batch: 030 ----
mean loss: 145.65
 ---- batch: 040 ----
mean loss: 138.26
train mean loss: 145.11
epoch train time: 0:00:01.781673
elapsed time: 0:04:04.745933
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 22:18:51.740125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.98
 ---- batch: 020 ----
mean loss: 147.99
 ---- batch: 030 ----
mean loss: 147.60
 ---- batch: 040 ----
mean loss: 152.06
train mean loss: 147.38
epoch train time: 0:00:01.688058
elapsed time: 0:04:06.434155
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 22:18:53.428338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.06
 ---- batch: 020 ----
mean loss: 141.66
 ---- batch: 030 ----
mean loss: 142.64
 ---- batch: 040 ----
mean loss: 148.12
train mean loss: 143.58
epoch train time: 0:00:01.789589
elapsed time: 0:04:08.223876
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 22:18:55.218054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.24
 ---- batch: 020 ----
mean loss: 145.17
 ---- batch: 030 ----
mean loss: 139.49
 ---- batch: 040 ----
mean loss: 144.57
train mean loss: 144.04
epoch train time: 0:00:01.691632
elapsed time: 0:04:09.915670
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 22:18:56.909863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.30
 ---- batch: 020 ----
mean loss: 143.07
 ---- batch: 030 ----
mean loss: 148.10
 ---- batch: 040 ----
mean loss: 143.98
train mean loss: 146.33
epoch train time: 0:00:01.785316
elapsed time: 0:04:11.701151
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 22:18:58.695328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.02
 ---- batch: 020 ----
mean loss: 148.94
 ---- batch: 030 ----
mean loss: 148.40
 ---- batch: 040 ----
mean loss: 145.10
train mean loss: 145.94
epoch train time: 0:00:01.690400
elapsed time: 0:04:13.391697
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 22:19:00.385904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.88
 ---- batch: 020 ----
mean loss: 142.99
 ---- batch: 030 ----
mean loss: 141.17
 ---- batch: 040 ----
mean loss: 147.83
train mean loss: 145.37
epoch train time: 0:00:01.771112
elapsed time: 0:04:15.162965
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 22:19:02.157142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.22
 ---- batch: 020 ----
mean loss: 145.21
 ---- batch: 030 ----
mean loss: 145.57
 ---- batch: 040 ----
mean loss: 145.30
train mean loss: 145.86
epoch train time: 0:00:01.697797
elapsed time: 0:04:16.860901
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 22:19:03.855083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.88
 ---- batch: 020 ----
mean loss: 147.32
 ---- batch: 030 ----
mean loss: 146.36
 ---- batch: 040 ----
mean loss: 146.01
train mean loss: 144.81
epoch train time: 0:00:01.671811
elapsed time: 0:04:18.532842
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 22:19:05.527018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.76
 ---- batch: 020 ----
mean loss: 150.78
 ---- batch: 030 ----
mean loss: 147.26
 ---- batch: 040 ----
mean loss: 140.49
train mean loss: 145.20
epoch train time: 0:00:01.738624
elapsed time: 0:04:20.271601
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 22:19:07.265782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.77
 ---- batch: 020 ----
mean loss: 144.84
 ---- batch: 030 ----
mean loss: 150.65
 ---- batch: 040 ----
mean loss: 146.50
train mean loss: 146.94
epoch train time: 0:00:01.681331
elapsed time: 0:04:21.953082
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 22:19:08.947258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.31
 ---- batch: 020 ----
mean loss: 148.92
 ---- batch: 030 ----
mean loss: 150.34
 ---- batch: 040 ----
mean loss: 142.24
train mean loss: 147.15
epoch train time: 0:00:01.789519
elapsed time: 0:04:23.742732
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 22:19:10.736920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.20
 ---- batch: 020 ----
mean loss: 143.51
 ---- batch: 030 ----
mean loss: 141.80
 ---- batch: 040 ----
mean loss: 143.19
train mean loss: 143.23
epoch train time: 0:00:01.679200
elapsed time: 0:04:25.422115
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 22:19:12.416298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.59
 ---- batch: 020 ----
mean loss: 143.73
 ---- batch: 030 ----
mean loss: 146.10
 ---- batch: 040 ----
mean loss: 146.84
train mean loss: 144.60
epoch train time: 0:00:01.783925
elapsed time: 0:04:27.206180
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 22:19:14.200360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.03
 ---- batch: 020 ----
mean loss: 142.26
 ---- batch: 030 ----
mean loss: 144.69
 ---- batch: 040 ----
mean loss: 141.57
train mean loss: 144.66
epoch train time: 0:00:01.685916
elapsed time: 0:04:28.892239
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 22:19:15.886422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.82
 ---- batch: 020 ----
mean loss: 147.31
 ---- batch: 030 ----
mean loss: 143.30
 ---- batch: 040 ----
mean loss: 145.01
train mean loss: 144.55
epoch train time: 0:00:01.781725
elapsed time: 0:04:30.674108
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 22:19:17.668286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.78
 ---- batch: 020 ----
mean loss: 141.82
 ---- batch: 030 ----
mean loss: 146.63
 ---- batch: 040 ----
mean loss: 138.44
train mean loss: 144.67
epoch train time: 0:00:01.680663
elapsed time: 0:04:32.354915
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 22:19:19.349126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.05
 ---- batch: 020 ----
mean loss: 148.03
 ---- batch: 030 ----
mean loss: 144.95
 ---- batch: 040 ----
mean loss: 146.34
train mean loss: 146.07
epoch train time: 0:00:01.787237
elapsed time: 0:04:34.142377
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 22:19:21.136605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.48
 ---- batch: 020 ----
mean loss: 146.46
 ---- batch: 030 ----
mean loss: 143.16
 ---- batch: 040 ----
mean loss: 142.00
train mean loss: 144.36
epoch train time: 0:00:01.691414
elapsed time: 0:04:35.834028
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 22:19:22.828211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.44
 ---- batch: 020 ----
mean loss: 144.35
 ---- batch: 030 ----
mean loss: 143.96
 ---- batch: 040 ----
mean loss: 147.26
train mean loss: 144.85
epoch train time: 0:00:01.786491
elapsed time: 0:04:37.620660
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 22:19:24.614847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.56
 ---- batch: 020 ----
mean loss: 144.05
 ---- batch: 030 ----
mean loss: 138.65
 ---- batch: 040 ----
mean loss: 150.19
train mean loss: 146.16
epoch train time: 0:00:01.699257
elapsed time: 0:04:39.320067
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 22:19:26.314249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.14
 ---- batch: 020 ----
mean loss: 148.37
 ---- batch: 030 ----
mean loss: 139.76
 ---- batch: 040 ----
mean loss: 144.78
train mean loss: 145.11
epoch train time: 0:00:01.681302
elapsed time: 0:04:41.001563
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 22:19:27.995741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.28
 ---- batch: 020 ----
mean loss: 147.81
 ---- batch: 030 ----
mean loss: 149.56
 ---- batch: 040 ----
mean loss: 140.43
train mean loss: 143.05
epoch train time: 0:00:01.783729
elapsed time: 0:04:42.785438
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 22:19:29.779618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.35
 ---- batch: 020 ----
mean loss: 150.31
 ---- batch: 030 ----
mean loss: 142.21
 ---- batch: 040 ----
mean loss: 144.83
train mean loss: 143.80
epoch train time: 0:00:01.688576
elapsed time: 0:04:44.474154
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 22:19:31.468335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.95
 ---- batch: 020 ----
mean loss: 145.08
 ---- batch: 030 ----
mean loss: 146.33
 ---- batch: 040 ----
mean loss: 141.93
train mean loss: 144.12
epoch train time: 0:00:01.798237
elapsed time: 0:04:46.272531
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 22:19:33.266721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.45
 ---- batch: 020 ----
mean loss: 140.71
 ---- batch: 030 ----
mean loss: 147.54
 ---- batch: 040 ----
mean loss: 146.85
train mean loss: 144.86
epoch train time: 0:00:01.695689
elapsed time: 0:04:47.968375
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 22:19:34.962556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.78
 ---- batch: 020 ----
mean loss: 143.83
 ---- batch: 030 ----
mean loss: 137.33
 ---- batch: 040 ----
mean loss: 145.36
train mean loss: 142.00
epoch train time: 0:00:01.781825
elapsed time: 0:04:49.750349
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 22:19:36.744526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.59
 ---- batch: 020 ----
mean loss: 140.60
 ---- batch: 030 ----
mean loss: 142.07
 ---- batch: 040 ----
mean loss: 144.72
train mean loss: 141.98
epoch train time: 0:00:01.686073
elapsed time: 0:04:51.436569
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 22:19:38.430767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.05
 ---- batch: 020 ----
mean loss: 140.99
 ---- batch: 030 ----
mean loss: 139.54
 ---- batch: 040 ----
mean loss: 141.66
train mean loss: 140.93
epoch train time: 0:00:01.792091
elapsed time: 0:04:53.228815
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 22:19:40.222995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.98
 ---- batch: 020 ----
mean loss: 140.99
 ---- batch: 030 ----
mean loss: 143.60
 ---- batch: 040 ----
mean loss: 141.58
train mean loss: 141.57
epoch train time: 0:00:01.693639
elapsed time: 0:04:54.922628
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 22:19:41.916832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.06
 ---- batch: 020 ----
mean loss: 138.29
 ---- batch: 030 ----
mean loss: 143.00
 ---- batch: 040 ----
mean loss: 142.36
train mean loss: 141.13
epoch train time: 0:00:01.779228
elapsed time: 0:04:56.702008
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 22:19:43.696188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.71
 ---- batch: 020 ----
mean loss: 144.67
 ---- batch: 030 ----
mean loss: 144.94
 ---- batch: 040 ----
mean loss: 141.68
train mean loss: 142.61
epoch train time: 0:00:01.691100
elapsed time: 0:04:58.393245
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 22:19:45.387424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.80
 ---- batch: 020 ----
mean loss: 137.20
 ---- batch: 030 ----
mean loss: 139.75
 ---- batch: 040 ----
mean loss: 145.31
train mean loss: 142.02
epoch train time: 0:00:01.783887
elapsed time: 0:05:00.177278
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 22:19:47.171457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.07
 ---- batch: 020 ----
mean loss: 147.58
 ---- batch: 030 ----
mean loss: 152.90
 ---- batch: 040 ----
mean loss: 137.27
train mean loss: 144.74
epoch train time: 0:00:01.684993
elapsed time: 0:05:01.862431
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 22:19:48.856617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.44
 ---- batch: 020 ----
mean loss: 142.63
 ---- batch: 030 ----
mean loss: 138.40
 ---- batch: 040 ----
mean loss: 144.68
train mean loss: 141.30
epoch train time: 0:00:01.678774
elapsed time: 0:05:03.541345
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 22:19:50.535524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.31
 ---- batch: 020 ----
mean loss: 137.27
 ---- batch: 030 ----
mean loss: 141.47
 ---- batch: 040 ----
mean loss: 136.72
train mean loss: 138.84
epoch train time: 0:00:01.794785
elapsed time: 0:05:05.336257
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 22:19:52.330435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.41
 ---- batch: 020 ----
mean loss: 135.95
 ---- batch: 030 ----
mean loss: 144.18
 ---- batch: 040 ----
mean loss: 140.15
train mean loss: 139.54
epoch train time: 0:00:01.688784
elapsed time: 0:05:07.025181
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 22:19:54.019396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.02
 ---- batch: 020 ----
mean loss: 138.76
 ---- batch: 030 ----
mean loss: 139.70
 ---- batch: 040 ----
mean loss: 139.46
train mean loss: 139.75
epoch train time: 0:00:01.785436
elapsed time: 0:05:08.810791
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 22:19:55.804973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.44
 ---- batch: 020 ----
mean loss: 139.72
 ---- batch: 030 ----
mean loss: 138.74
 ---- batch: 040 ----
mean loss: 135.46
train mean loss: 138.07
epoch train time: 0:00:01.687612
elapsed time: 0:05:10.498549
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 22:19:57.492737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.92
 ---- batch: 020 ----
mean loss: 139.22
 ---- batch: 030 ----
mean loss: 136.54
 ---- batch: 040 ----
mean loss: 141.48
train mean loss: 137.50
epoch train time: 0:00:01.732028
elapsed time: 0:05:12.230721
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 22:19:59.224907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.18
 ---- batch: 020 ----
mean loss: 137.78
 ---- batch: 030 ----
mean loss: 139.13
 ---- batch: 040 ----
mean loss: 138.30
train mean loss: 137.04
epoch train time: 0:00:01.714119
elapsed time: 0:05:13.945026
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 22:20:00.939195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.08
 ---- batch: 020 ----
mean loss: 139.78
 ---- batch: 030 ----
mean loss: 138.02
 ---- batch: 040 ----
mean loss: 145.03
train mean loss: 139.42
epoch train time: 0:00:01.682012
elapsed time: 0:05:15.627171
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 22:20:02.621371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.26
 ---- batch: 020 ----
mean loss: 136.53
 ---- batch: 030 ----
mean loss: 140.11
 ---- batch: 040 ----
mean loss: 140.37
train mean loss: 138.22
epoch train time: 0:00:01.786534
elapsed time: 0:05:17.413866
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 22:20:04.408041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.30
 ---- batch: 020 ----
mean loss: 137.79
 ---- batch: 030 ----
mean loss: 138.89
 ---- batch: 040 ----
mean loss: 136.69
train mean loss: 137.26
epoch train time: 0:00:01.680808
elapsed time: 0:05:19.094819
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 22:20:06.089000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.57
 ---- batch: 020 ----
mean loss: 132.28
 ---- batch: 030 ----
mean loss: 132.79
 ---- batch: 040 ----
mean loss: 138.65
train mean loss: 135.79
epoch train time: 0:00:01.777082
elapsed time: 0:05:20.872030
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 22:20:07.866207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.11
 ---- batch: 020 ----
mean loss: 133.91
 ---- batch: 030 ----
mean loss: 131.79
 ---- batch: 040 ----
mean loss: 134.01
train mean loss: 134.96
epoch train time: 0:00:01.680620
elapsed time: 0:05:22.552790
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 22:20:09.546971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.00
 ---- batch: 020 ----
mean loss: 144.56
 ---- batch: 030 ----
mean loss: 134.50
 ---- batch: 040 ----
mean loss: 137.73
train mean loss: 139.05
epoch train time: 0:00:01.674940
elapsed time: 0:05:24.227897
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 22:20:11.222079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.67
 ---- batch: 020 ----
mean loss: 141.33
 ---- batch: 030 ----
mean loss: 133.11
 ---- batch: 040 ----
mean loss: 135.32
train mean loss: 135.93
epoch train time: 0:00:01.798080
elapsed time: 0:05:26.026130
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 22:20:13.020322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.42
 ---- batch: 020 ----
mean loss: 133.94
 ---- batch: 030 ----
mean loss: 134.56
 ---- batch: 040 ----
mean loss: 137.97
train mean loss: 135.41
epoch train time: 0:00:01.685382
elapsed time: 0:05:27.711708
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 22:20:14.705914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.95
 ---- batch: 020 ----
mean loss: 138.08
 ---- batch: 030 ----
mean loss: 134.46
 ---- batch: 040 ----
mean loss: 136.45
train mean loss: 136.37
epoch train time: 0:00:01.785621
elapsed time: 0:05:29.497532
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 22:20:16.491725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.77
 ---- batch: 020 ----
mean loss: 137.09
 ---- batch: 030 ----
mean loss: 143.82
 ---- batch: 040 ----
mean loss: 139.26
train mean loss: 137.08
epoch train time: 0:00:01.679498
elapsed time: 0:05:31.177179
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 22:20:18.171358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.97
 ---- batch: 020 ----
mean loss: 127.56
 ---- batch: 030 ----
mean loss: 134.64
 ---- batch: 040 ----
mean loss: 133.47
train mean loss: 132.53
epoch train time: 0:00:01.781601
elapsed time: 0:05:32.958909
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 22:20:19.953104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.75
 ---- batch: 020 ----
mean loss: 129.32
 ---- batch: 030 ----
mean loss: 133.75
 ---- batch: 040 ----
mean loss: 135.94
train mean loss: 133.58
epoch train time: 0:00:01.687318
elapsed time: 0:05:34.646408
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 22:20:21.640596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.81
 ---- batch: 020 ----
mean loss: 136.33
 ---- batch: 030 ----
mean loss: 130.68
 ---- batch: 040 ----
mean loss: 129.77
train mean loss: 133.00
epoch train time: 0:00:01.679122
elapsed time: 0:05:36.325671
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 22:20:23.319866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.57
 ---- batch: 020 ----
mean loss: 126.99
 ---- batch: 030 ----
mean loss: 131.83
 ---- batch: 040 ----
mean loss: 139.97
train mean loss: 131.95
epoch train time: 0:00:01.715078
elapsed time: 0:05:38.040897
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 22:20:25.035081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.43
 ---- batch: 020 ----
mean loss: 129.83
 ---- batch: 030 ----
mean loss: 135.85
 ---- batch: 040 ----
mean loss: 133.08
train mean loss: 132.36
epoch train time: 0:00:01.682861
elapsed time: 0:05:39.723896
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 22:20:26.718089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.38
 ---- batch: 020 ----
mean loss: 133.44
 ---- batch: 030 ----
mean loss: 132.98
 ---- batch: 040 ----
mean loss: 134.73
train mean loss: 133.22
epoch train time: 0:00:01.781717
elapsed time: 0:05:41.505752
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 22:20:28.499928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.51
 ---- batch: 020 ----
mean loss: 131.20
 ---- batch: 030 ----
mean loss: 131.08
 ---- batch: 040 ----
mean loss: 130.48
train mean loss: 130.07
epoch train time: 0:00:01.681329
elapsed time: 0:05:43.187221
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 22:20:30.181401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.59
 ---- batch: 020 ----
mean loss: 133.78
 ---- batch: 030 ----
mean loss: 130.13
 ---- batch: 040 ----
mean loss: 137.87
train mean loss: 132.88
epoch train time: 0:00:01.742623
elapsed time: 0:05:44.929989
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 22:20:31.924166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.33
 ---- batch: 020 ----
mean loss: 133.34
 ---- batch: 030 ----
mean loss: 127.74
 ---- batch: 040 ----
mean loss: 132.13
train mean loss: 133.16
epoch train time: 0:00:01.731776
elapsed time: 0:05:46.661903
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 22:20:33.656081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.01
 ---- batch: 020 ----
mean loss: 138.37
 ---- batch: 030 ----
mean loss: 134.08
 ---- batch: 040 ----
mean loss: 131.95
train mean loss: 132.81
epoch train time: 0:00:01.682604
elapsed time: 0:05:48.344648
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 22:20:35.338845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.87
 ---- batch: 020 ----
mean loss: 130.55
 ---- batch: 030 ----
mean loss: 136.59
 ---- batch: 040 ----
mean loss: 129.78
train mean loss: 131.68
epoch train time: 0:00:01.804385
elapsed time: 0:05:50.149199
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 22:20:37.143406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.53
 ---- batch: 020 ----
mean loss: 131.05
 ---- batch: 030 ----
mean loss: 132.15
 ---- batch: 040 ----
mean loss: 134.97
train mean loss: 131.84
epoch train time: 0:00:01.688795
elapsed time: 0:05:51.838172
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 22:20:38.832352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.51
 ---- batch: 020 ----
mean loss: 133.70
 ---- batch: 030 ----
mean loss: 130.74
 ---- batch: 040 ----
mean loss: 131.52
train mean loss: 131.41
epoch train time: 0:00:01.788334
elapsed time: 0:05:53.626652
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 22:20:40.620835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.09
 ---- batch: 020 ----
mean loss: 131.08
 ---- batch: 030 ----
mean loss: 137.04
 ---- batch: 040 ----
mean loss: 131.10
train mean loss: 134.10
epoch train time: 0:00:01.683031
elapsed time: 0:05:55.309822
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 22:20:42.304000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.89
 ---- batch: 020 ----
mean loss: 130.81
 ---- batch: 030 ----
mean loss: 133.48
 ---- batch: 040 ----
mean loss: 128.80
train mean loss: 130.91
epoch train time: 0:00:01.675795
elapsed time: 0:05:56.985787
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 22:20:43.979980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.54
 ---- batch: 020 ----
mean loss: 129.03
 ---- batch: 030 ----
mean loss: 133.59
 ---- batch: 040 ----
mean loss: 130.97
train mean loss: 129.62
epoch train time: 0:00:01.791165
elapsed time: 0:05:58.777109
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 22:20:45.771292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.37
 ---- batch: 020 ----
mean loss: 126.38
 ---- batch: 030 ----
mean loss: 130.19
 ---- batch: 040 ----
mean loss: 130.56
train mean loss: 129.74
epoch train time: 0:00:01.683008
elapsed time: 0:06:00.460265
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 22:20:47.454445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.98
 ---- batch: 020 ----
mean loss: 129.06
 ---- batch: 030 ----
mean loss: 125.47
 ---- batch: 040 ----
mean loss: 127.49
train mean loss: 127.32
epoch train time: 0:00:01.792106
elapsed time: 0:06:02.252525
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 22:20:49.246691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.53
 ---- batch: 020 ----
mean loss: 128.76
 ---- batch: 030 ----
mean loss: 128.76
 ---- batch: 040 ----
mean loss: 122.56
train mean loss: 127.69
epoch train time: 0:00:01.693970
elapsed time: 0:06:03.946635
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 22:20:50.940830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.74
 ---- batch: 020 ----
mean loss: 129.43
 ---- batch: 030 ----
mean loss: 124.21
 ---- batch: 040 ----
mean loss: 126.13
train mean loss: 125.85
epoch train time: 0:00:01.778602
elapsed time: 0:06:05.725480
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 22:20:52.719689
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.10
 ---- batch: 020 ----
mean loss: 129.46
 ---- batch: 030 ----
mean loss: 124.68
 ---- batch: 040 ----
mean loss: 129.47
train mean loss: 127.62
epoch train time: 0:00:01.680961
elapsed time: 0:06:07.406628
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 22:20:54.400806
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.66
 ---- batch: 020 ----
mean loss: 126.14
 ---- batch: 030 ----
mean loss: 129.21
 ---- batch: 040 ----
mean loss: 125.47
train mean loss: 127.26
epoch train time: 0:00:01.679659
elapsed time: 0:06:09.086452
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 22:20:56.080634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.56
 ---- batch: 020 ----
mean loss: 124.91
 ---- batch: 030 ----
mean loss: 128.85
 ---- batch: 040 ----
mean loss: 124.59
train mean loss: 126.20
epoch train time: 0:00:01.780938
elapsed time: 0:06:10.867536
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 22:20:57.861711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.57
 ---- batch: 020 ----
mean loss: 124.23
 ---- batch: 030 ----
mean loss: 125.92
 ---- batch: 040 ----
mean loss: 126.55
train mean loss: 125.98
epoch train time: 0:00:01.688487
elapsed time: 0:06:12.556158
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 22:20:59.550337
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.46
 ---- batch: 020 ----
mean loss: 127.43
 ---- batch: 030 ----
mean loss: 124.63
 ---- batch: 040 ----
mean loss: 125.80
train mean loss: 126.26
epoch train time: 0:00:01.794239
elapsed time: 0:06:14.350576
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 22:21:01.344780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.09
 ---- batch: 020 ----
mean loss: 128.26
 ---- batch: 030 ----
mean loss: 125.75
 ---- batch: 040 ----
mean loss: 128.07
train mean loss: 125.96
epoch train time: 0:00:01.695338
elapsed time: 0:06:16.046076
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 22:21:03.040257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.56
 ---- batch: 020 ----
mean loss: 125.45
 ---- batch: 030 ----
mean loss: 130.28
 ---- batch: 040 ----
mean loss: 126.85
train mean loss: 127.45
epoch train time: 0:00:01.742825
elapsed time: 0:06:17.789054
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 22:21:04.783266
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.33
 ---- batch: 020 ----
mean loss: 127.06
 ---- batch: 030 ----
mean loss: 124.74
 ---- batch: 040 ----
mean loss: 123.64
train mean loss: 125.78
epoch train time: 0:00:01.722439
elapsed time: 0:06:19.511663
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 22:21:06.505856
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.16
 ---- batch: 020 ----
mean loss: 128.25
 ---- batch: 030 ----
mean loss: 123.46
 ---- batch: 040 ----
mean loss: 123.20
train mean loss: 126.85
epoch train time: 0:00:01.686281
elapsed time: 0:06:21.198089
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 22:21:08.192265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.49
 ---- batch: 020 ----
mean loss: 122.49
 ---- batch: 030 ----
mean loss: 129.94
 ---- batch: 040 ----
mean loss: 124.77
train mean loss: 126.91
epoch train time: 0:00:01.797722
elapsed time: 0:06:22.995972
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 22:21:09.990153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.62
 ---- batch: 020 ----
mean loss: 127.77
 ---- batch: 030 ----
mean loss: 133.79
 ---- batch: 040 ----
mean loss: 125.88
train mean loss: 128.58
epoch train time: 0:00:01.680750
elapsed time: 0:06:24.676858
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 22:21:11.671034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.39
 ---- batch: 020 ----
mean loss: 124.97
 ---- batch: 030 ----
mean loss: 128.43
 ---- batch: 040 ----
mean loss: 127.28
train mean loss: 126.79
epoch train time: 0:00:01.785938
elapsed time: 0:06:26.462919
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 22:21:13.457112
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.44
 ---- batch: 020 ----
mean loss: 126.32
 ---- batch: 030 ----
mean loss: 125.54
 ---- batch: 040 ----
mean loss: 123.10
train mean loss: 126.16
epoch train time: 0:00:01.682873
elapsed time: 0:06:28.145953
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 22:21:15.140161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.36
 ---- batch: 020 ----
mean loss: 127.02
 ---- batch: 030 ----
mean loss: 130.55
 ---- batch: 040 ----
mean loss: 125.81
train mean loss: 126.87
epoch train time: 0:00:01.678379
elapsed time: 0:06:29.824500
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 22:21:16.818684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.13
 ---- batch: 020 ----
mean loss: 132.25
 ---- batch: 030 ----
mean loss: 121.82
 ---- batch: 040 ----
mean loss: 126.62
train mean loss: 126.59
epoch train time: 0:00:01.794217
elapsed time: 0:06:31.618860
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 22:21:18.613048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.81
 ---- batch: 020 ----
mean loss: 124.94
 ---- batch: 030 ----
mean loss: 128.48
 ---- batch: 040 ----
mean loss: 126.83
train mean loss: 126.79
epoch train time: 0:00:01.682737
elapsed time: 0:06:33.301745
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 22:21:20.295939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.09
 ---- batch: 020 ----
mean loss: 126.13
 ---- batch: 030 ----
mean loss: 130.71
 ---- batch: 040 ----
mean loss: 129.07
train mean loss: 127.01
epoch train time: 0:00:01.799486
elapsed time: 0:06:35.101370
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 22:21:22.095561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.50
 ---- batch: 020 ----
mean loss: 125.84
 ---- batch: 030 ----
mean loss: 121.92
 ---- batch: 040 ----
mean loss: 131.66
train mean loss: 127.18
epoch train time: 0:00:01.689385
elapsed time: 0:06:36.790913
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 22:21:23.785111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.75
 ---- batch: 020 ----
mean loss: 129.25
 ---- batch: 030 ----
mean loss: 128.87
 ---- batch: 040 ----
mean loss: 123.10
train mean loss: 126.61
epoch train time: 0:00:01.787736
elapsed time: 0:06:38.578800
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 22:21:25.572977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.41
 ---- batch: 020 ----
mean loss: 125.91
 ---- batch: 030 ----
mean loss: 128.72
 ---- batch: 040 ----
mean loss: 127.46
train mean loss: 127.47
epoch train time: 0:00:01.684495
elapsed time: 0:06:40.263427
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 22:21:27.257637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.55
 ---- batch: 020 ----
mean loss: 125.86
 ---- batch: 030 ----
mean loss: 126.12
 ---- batch: 040 ----
mean loss: 124.76
train mean loss: 126.39
epoch train time: 0:00:01.686147
elapsed time: 0:06:41.949738
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 22:21:28.943916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.93
 ---- batch: 020 ----
mean loss: 125.24
 ---- batch: 030 ----
mean loss: 127.55
 ---- batch: 040 ----
mean loss: 128.65
train mean loss: 127.53
epoch train time: 0:00:01.785960
elapsed time: 0:06:43.735831
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 22:21:30.730011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.91
 ---- batch: 020 ----
mean loss: 126.30
 ---- batch: 030 ----
mean loss: 123.06
 ---- batch: 040 ----
mean loss: 129.62
train mean loss: 128.01
epoch train time: 0:00:01.685592
elapsed time: 0:06:45.421565
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 22:21:32.415759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.10
 ---- batch: 020 ----
mean loss: 129.03
 ---- batch: 030 ----
mean loss: 127.91
 ---- batch: 040 ----
mean loss: 123.31
train mean loss: 126.92
epoch train time: 0:00:01.710737
elapsed time: 0:06:47.132439
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 22:21:34.126626
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.47
 ---- batch: 020 ----
mean loss: 127.61
 ---- batch: 030 ----
mean loss: 123.87
 ---- batch: 040 ----
mean loss: 126.67
train mean loss: 126.52
epoch train time: 0:00:01.695032
elapsed time: 0:06:48.827624
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 22:21:35.821805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.94
 ---- batch: 020 ----
mean loss: 120.60
 ---- batch: 030 ----
mean loss: 128.78
 ---- batch: 040 ----
mean loss: 129.12
train mean loss: 125.40
epoch train time: 0:00:01.782931
elapsed time: 0:06:50.610746
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 22:21:37.604945
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.92
 ---- batch: 020 ----
mean loss: 130.13
 ---- batch: 030 ----
mean loss: 125.67
 ---- batch: 040 ----
mean loss: 124.67
train mean loss: 125.64
epoch train time: 0:00:01.698112
elapsed time: 0:06:52.309035
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 22:21:39.303215
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.71
 ---- batch: 020 ----
mean loss: 127.72
 ---- batch: 030 ----
mean loss: 132.06
 ---- batch: 040 ----
mean loss: 129.34
train mean loss: 127.95
epoch train time: 0:00:01.680373
elapsed time: 0:06:53.989550
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 22:21:40.983741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.51
 ---- batch: 020 ----
mean loss: 128.79
 ---- batch: 030 ----
mean loss: 129.87
 ---- batch: 040 ----
mean loss: 122.39
train mean loss: 127.19
epoch train time: 0:00:01.788845
elapsed time: 0:06:55.778582
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 22:21:42.772775
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.06
 ---- batch: 020 ----
mean loss: 127.78
 ---- batch: 030 ----
mean loss: 127.32
 ---- batch: 040 ----
mean loss: 128.64
train mean loss: 127.30
epoch train time: 0:00:01.682297
elapsed time: 0:06:57.461050
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 22:21:44.455240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.66
 ---- batch: 020 ----
mean loss: 127.07
 ---- batch: 030 ----
mean loss: 130.10
 ---- batch: 040 ----
mean loss: 127.87
train mean loss: 128.74
epoch train time: 0:00:01.793288
elapsed time: 0:06:59.254482
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 22:21:46.248659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.39
 ---- batch: 020 ----
mean loss: 125.80
 ---- batch: 030 ----
mean loss: 125.19
 ---- batch: 040 ----
mean loss: 129.95
train mean loss: 126.97
epoch train time: 0:00:01.692302
elapsed time: 0:07:00.946919
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 22:21:47.941098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.40
 ---- batch: 020 ----
mean loss: 122.92
 ---- batch: 030 ----
mean loss: 126.51
 ---- batch: 040 ----
mean loss: 123.62
train mean loss: 125.97
epoch train time: 0:00:01.784024
elapsed time: 0:07:02.731108
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 22:21:49.725293
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.45
 ---- batch: 020 ----
mean loss: 125.36
 ---- batch: 030 ----
mean loss: 126.59
 ---- batch: 040 ----
mean loss: 125.70
train mean loss: 126.55
epoch train time: 0:00:01.687422
elapsed time: 0:07:04.418677
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 22:21:51.412858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.37
 ---- batch: 020 ----
mean loss: 122.61
 ---- batch: 030 ----
mean loss: 127.58
 ---- batch: 040 ----
mean loss: 128.06
train mean loss: 126.45
epoch train time: 0:00:01.783812
elapsed time: 0:07:06.202620
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 22:21:53.196799
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.63
 ---- batch: 020 ----
mean loss: 126.83
 ---- batch: 030 ----
mean loss: 125.10
 ---- batch: 040 ----
mean loss: 126.49
train mean loss: 126.43
epoch train time: 0:00:01.688476
elapsed time: 0:07:07.891240
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 22:21:54.885428
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.24
 ---- batch: 020 ----
mean loss: 125.36
 ---- batch: 030 ----
mean loss: 123.46
 ---- batch: 040 ----
mean loss: 127.12
train mean loss: 125.62
epoch train time: 0:00:01.682400
elapsed time: 0:07:09.573791
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 22:21:56.568007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.33
 ---- batch: 020 ----
mean loss: 123.78
 ---- batch: 030 ----
mean loss: 130.04
 ---- batch: 040 ----
mean loss: 126.69
train mean loss: 126.05
epoch train time: 0:00:01.793570
elapsed time: 0:07:11.367557
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 22:21:58.361737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.40
 ---- batch: 020 ----
mean loss: 129.55
 ---- batch: 030 ----
mean loss: 128.12
 ---- batch: 040 ----
mean loss: 126.43
train mean loss: 127.31
epoch train time: 0:00:01.686209
elapsed time: 0:07:13.053918
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 22:22:00.048114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.26
 ---- batch: 020 ----
mean loss: 126.46
 ---- batch: 030 ----
mean loss: 129.46
 ---- batch: 040 ----
mean loss: 125.85
train mean loss: 126.99
epoch train time: 0:00:01.789576
elapsed time: 0:07:14.843654
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 22:22:01.837834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.68
 ---- batch: 020 ----
mean loss: 122.96
 ---- batch: 030 ----
mean loss: 125.19
 ---- batch: 040 ----
mean loss: 125.48
train mean loss: 124.53
epoch train time: 0:00:01.690283
elapsed time: 0:07:16.534073
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 22:22:03.528262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.45
 ---- batch: 020 ----
mean loss: 132.05
 ---- batch: 030 ----
mean loss: 126.44
 ---- batch: 040 ----
mean loss: 124.95
train mean loss: 126.62
epoch train time: 0:00:01.787704
elapsed time: 0:07:18.321928
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 22:22:05.316117
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.22
 ---- batch: 020 ----
mean loss: 128.36
 ---- batch: 030 ----
mean loss: 130.85
 ---- batch: 040 ----
mean loss: 122.53
train mean loss: 127.09
epoch train time: 0:00:01.689579
elapsed time: 0:07:20.011693
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 22:22:07.005876
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.21
 ---- batch: 020 ----
mean loss: 125.34
 ---- batch: 030 ----
mean loss: 125.41
 ---- batch: 040 ----
mean loss: 128.42
train mean loss: 127.11
epoch train time: 0:00:01.791435
elapsed time: 0:07:21.803317
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 22:22:08.797496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.58
 ---- batch: 020 ----
mean loss: 124.22
 ---- batch: 030 ----
mean loss: 128.79
 ---- batch: 040 ----
mean loss: 122.59
train mean loss: 125.31
epoch train time: 0:00:01.689060
elapsed time: 0:07:23.492527
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 22:22:10.486706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.95
 ---- batch: 020 ----
mean loss: 125.00
 ---- batch: 030 ----
mean loss: 126.94
 ---- batch: 040 ----
mean loss: 126.06
train mean loss: 125.64
epoch train time: 0:00:01.783421
elapsed time: 0:07:25.279264
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_1/checkpoint.pth.tar
**** end time: 2019-09-20 22:22:12.273410 ****
