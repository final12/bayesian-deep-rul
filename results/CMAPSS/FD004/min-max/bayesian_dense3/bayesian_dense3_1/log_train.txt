Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 14189
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 22:28:24.889754 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 22:28:24.899586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4559.57
 ---- batch: 020 ----
mean loss: 4282.35
 ---- batch: 030 ----
mean loss: 4027.15
 ---- batch: 040 ----
mean loss: 3755.59
 ---- batch: 050 ----
mean loss: 3535.02
 ---- batch: 060 ----
mean loss: 3301.53
 ---- batch: 070 ----
mean loss: 3165.82
 ---- batch: 080 ----
mean loss: 3021.92
 ---- batch: 090 ----
mean loss: 2888.55
 ---- batch: 100 ----
mean loss: 2809.68
 ---- batch: 110 ----
mean loss: 2730.56
train mean loss: 3440.42
epoch train time: 0:00:34.445403
elapsed time: 0:00:34.461691
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 22:28:59.351491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2608.45
 ---- batch: 020 ----
mean loss: 2513.29
 ---- batch: 030 ----
mean loss: 2483.51
 ---- batch: 040 ----
mean loss: 2417.10
 ---- batch: 050 ----
mean loss: 2394.06
 ---- batch: 060 ----
mean loss: 2329.55
 ---- batch: 070 ----
mean loss: 2282.38
 ---- batch: 080 ----
mean loss: 2256.05
 ---- batch: 090 ----
mean loss: 2207.73
 ---- batch: 100 ----
mean loss: 2141.29
 ---- batch: 110 ----
mean loss: 2084.45
train mean loss: 2333.22
epoch train time: 0:00:01.911683
elapsed time: 0:00:36.373629
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 22:29:01.263843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2062.41
 ---- batch: 020 ----
mean loss: 2035.16
 ---- batch: 030 ----
mean loss: 2027.52
 ---- batch: 040 ----
mean loss: 1992.58
 ---- batch: 050 ----
mean loss: 1950.63
 ---- batch: 060 ----
mean loss: 1911.88
 ---- batch: 070 ----
mean loss: 1908.87
 ---- batch: 080 ----
mean loss: 1849.40
 ---- batch: 090 ----
mean loss: 1815.43
 ---- batch: 100 ----
mean loss: 1814.32
 ---- batch: 110 ----
mean loss: 1746.19
train mean loss: 1916.29
epoch train time: 0:00:01.886786
elapsed time: 0:00:38.261064
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 22:29:03.151235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1743.00
 ---- batch: 020 ----
mean loss: 1697.24
 ---- batch: 030 ----
mean loss: 1724.42
 ---- batch: 040 ----
mean loss: 1691.15
 ---- batch: 050 ----
mean loss: 1629.88
 ---- batch: 060 ----
mean loss: 1633.66
 ---- batch: 070 ----
mean loss: 1613.43
 ---- batch: 080 ----
mean loss: 1609.46
 ---- batch: 090 ----
mean loss: 1557.74
 ---- batch: 100 ----
mean loss: 1560.46
 ---- batch: 110 ----
mean loss: 1540.66
train mean loss: 1633.53
epoch train time: 0:00:01.928798
elapsed time: 0:00:40.190481
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 22:29:05.080646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1533.52
 ---- batch: 020 ----
mean loss: 1497.75
 ---- batch: 030 ----
mean loss: 1472.39
 ---- batch: 040 ----
mean loss: 1456.35
 ---- batch: 050 ----
mean loss: 1455.32
 ---- batch: 060 ----
mean loss: 1408.77
 ---- batch: 070 ----
mean loss: 1390.16
 ---- batch: 080 ----
mean loss: 1359.96
 ---- batch: 090 ----
mean loss: 1374.21
 ---- batch: 100 ----
mean loss: 1367.76
 ---- batch: 110 ----
mean loss: 1357.45
train mean loss: 1422.05
epoch train time: 0:00:01.914535
elapsed time: 0:00:42.105688
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 22:29:06.995798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1322.74
 ---- batch: 020 ----
mean loss: 1319.01
 ---- batch: 030 ----
mean loss: 1300.06
 ---- batch: 040 ----
mean loss: 1298.27
 ---- batch: 050 ----
mean loss: 1256.15
 ---- batch: 060 ----
mean loss: 1263.44
 ---- batch: 070 ----
mean loss: 1228.29
 ---- batch: 080 ----
mean loss: 1222.16
 ---- batch: 090 ----
mean loss: 1206.88
 ---- batch: 100 ----
mean loss: 1217.11
 ---- batch: 110 ----
mean loss: 1198.26
train mean loss: 1255.84
epoch train time: 0:00:01.866258
elapsed time: 0:00:43.972532
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 22:29:08.862641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1190.47
 ---- batch: 020 ----
mean loss: 1162.74
 ---- batch: 030 ----
mean loss: 1150.26
 ---- batch: 040 ----
mean loss: 1156.60
 ---- batch: 050 ----
mean loss: 1156.69
 ---- batch: 060 ----
mean loss: 1118.64
 ---- batch: 070 ----
mean loss: 1116.80
 ---- batch: 080 ----
mean loss: 1111.50
 ---- batch: 090 ----
mean loss: 1104.87
 ---- batch: 100 ----
mean loss: 1096.74
 ---- batch: 110 ----
mean loss: 1101.19
train mean loss: 1132.17
epoch train time: 0:00:01.887043
elapsed time: 0:00:45.860154
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 22:29:10.750269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1069.64
 ---- batch: 020 ----
mean loss: 1072.34
 ---- batch: 030 ----
mean loss: 1064.24
 ---- batch: 040 ----
mean loss: 1044.81
 ---- batch: 050 ----
mean loss: 1055.26
 ---- batch: 060 ----
mean loss: 1042.17
 ---- batch: 070 ----
mean loss: 1021.18
 ---- batch: 080 ----
mean loss: 1024.71
 ---- batch: 090 ----
mean loss: 1034.72
 ---- batch: 100 ----
mean loss: 1038.30
 ---- batch: 110 ----
mean loss: 996.45
train mean loss: 1041.23
epoch train time: 0:00:01.888255
elapsed time: 0:00:47.748951
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 22:29:12.639048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.80
 ---- batch: 020 ----
mean loss: 1008.42
 ---- batch: 030 ----
mean loss: 985.10
 ---- batch: 040 ----
mean loss: 972.37
 ---- batch: 050 ----
mean loss: 980.68
 ---- batch: 060 ----
mean loss: 979.69
 ---- batch: 070 ----
mean loss: 986.03
 ---- batch: 080 ----
mean loss: 965.20
 ---- batch: 090 ----
mean loss: 961.74
 ---- batch: 100 ----
mean loss: 980.34
 ---- batch: 110 ----
mean loss: 967.60
train mean loss: 979.45
epoch train time: 0:00:01.892087
elapsed time: 0:00:49.641590
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 22:29:14.531751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.75
 ---- batch: 020 ----
mean loss: 961.05
 ---- batch: 030 ----
mean loss: 932.63
 ---- batch: 040 ----
mean loss: 947.02
 ---- batch: 050 ----
mean loss: 934.28
 ---- batch: 060 ----
mean loss: 936.92
 ---- batch: 070 ----
mean loss: 931.88
 ---- batch: 080 ----
mean loss: 947.02
 ---- batch: 090 ----
mean loss: 922.62
 ---- batch: 100 ----
mean loss: 915.28
 ---- batch: 110 ----
mean loss: 933.13
train mean loss: 937.87
epoch train time: 0:00:01.934681
elapsed time: 0:00:51.576908
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 22:29:16.467030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.42
 ---- batch: 020 ----
mean loss: 908.24
 ---- batch: 030 ----
mean loss: 913.40
 ---- batch: 040 ----
mean loss: 917.78
 ---- batch: 050 ----
mean loss: 898.79
 ---- batch: 060 ----
mean loss: 904.33
 ---- batch: 070 ----
mean loss: 903.70
 ---- batch: 080 ----
mean loss: 902.42
 ---- batch: 090 ----
mean loss: 911.55
 ---- batch: 100 ----
mean loss: 903.10
 ---- batch: 110 ----
mean loss: 891.70
train mean loss: 905.97
epoch train time: 0:00:01.901293
elapsed time: 0:00:53.478797
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 22:29:18.368736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.96
 ---- batch: 020 ----
mean loss: 879.13
 ---- batch: 030 ----
mean loss: 895.53
 ---- batch: 040 ----
mean loss: 898.09
 ---- batch: 050 ----
mean loss: 881.97
 ---- batch: 060 ----
mean loss: 884.11
 ---- batch: 070 ----
mean loss: 896.40
 ---- batch: 080 ----
mean loss: 877.89
 ---- batch: 090 ----
mean loss: 890.20
 ---- batch: 100 ----
mean loss: 881.32
 ---- batch: 110 ----
mean loss: 865.02
train mean loss: 888.01
epoch train time: 0:00:01.890464
elapsed time: 0:00:55.369619
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 22:29:20.259781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.11
 ---- batch: 020 ----
mean loss: 889.20
 ---- batch: 030 ----
mean loss: 879.46
 ---- batch: 040 ----
mean loss: 873.63
 ---- batch: 050 ----
mean loss: 873.51
 ---- batch: 060 ----
mean loss: 866.48
 ---- batch: 070 ----
mean loss: 885.13
 ---- batch: 080 ----
mean loss: 851.58
 ---- batch: 090 ----
mean loss: 870.28
 ---- batch: 100 ----
mean loss: 888.63
 ---- batch: 110 ----
mean loss: 863.49
train mean loss: 876.09
epoch train time: 0:00:01.885694
elapsed time: 0:00:57.255917
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 22:29:22.146029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.70
 ---- batch: 020 ----
mean loss: 869.29
 ---- batch: 030 ----
mean loss: 860.92
 ---- batch: 040 ----
mean loss: 861.47
 ---- batch: 050 ----
mean loss: 861.91
 ---- batch: 060 ----
mean loss: 878.74
 ---- batch: 070 ----
mean loss: 870.54
 ---- batch: 080 ----
mean loss: 874.32
 ---- batch: 090 ----
mean loss: 863.65
 ---- batch: 100 ----
mean loss: 873.25
 ---- batch: 110 ----
mean loss: 874.66
train mean loss: 868.74
epoch train time: 0:00:01.936152
elapsed time: 0:00:59.192609
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 22:29:24.082778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.40
 ---- batch: 020 ----
mean loss: 860.81
 ---- batch: 030 ----
mean loss: 877.13
 ---- batch: 040 ----
mean loss: 875.72
 ---- batch: 050 ----
mean loss: 873.82
 ---- batch: 060 ----
mean loss: 856.61
 ---- batch: 070 ----
mean loss: 857.30
 ---- batch: 080 ----
mean loss: 857.10
 ---- batch: 090 ----
mean loss: 865.43
 ---- batch: 100 ----
mean loss: 854.36
 ---- batch: 110 ----
mean loss: 883.18
train mean loss: 864.82
epoch train time: 0:00:01.909732
elapsed time: 0:01:01.103014
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 22:29:25.993166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.48
 ---- batch: 020 ----
mean loss: 866.44
 ---- batch: 030 ----
mean loss: 862.05
 ---- batch: 040 ----
mean loss: 848.30
 ---- batch: 050 ----
mean loss: 862.63
 ---- batch: 060 ----
mean loss: 863.30
 ---- batch: 070 ----
mean loss: 875.13
 ---- batch: 080 ----
mean loss: 851.94
 ---- batch: 090 ----
mean loss: 857.44
 ---- batch: 100 ----
mean loss: 877.01
 ---- batch: 110 ----
mean loss: 844.64
train mean loss: 862.28
epoch train time: 0:00:01.968662
elapsed time: 0:01:03.072274
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 22:29:27.962403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.00
 ---- batch: 020 ----
mean loss: 830.96
 ---- batch: 030 ----
mean loss: 857.91
 ---- batch: 040 ----
mean loss: 877.87
 ---- batch: 050 ----
mean loss: 882.91
 ---- batch: 060 ----
mean loss: 874.73
 ---- batch: 070 ----
mean loss: 870.01
 ---- batch: 080 ----
mean loss: 862.51
 ---- batch: 090 ----
mean loss: 845.24
 ---- batch: 100 ----
mean loss: 850.57
 ---- batch: 110 ----
mean loss: 856.77
train mean loss: 860.12
epoch train time: 0:00:01.956514
elapsed time: 0:01:05.029391
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 22:29:29.919535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.10
 ---- batch: 020 ----
mean loss: 862.80
 ---- batch: 030 ----
mean loss: 847.08
 ---- batch: 040 ----
mean loss: 863.52
 ---- batch: 050 ----
mean loss: 870.90
 ---- batch: 060 ----
mean loss: 839.74
 ---- batch: 070 ----
mean loss: 869.12
 ---- batch: 080 ----
mean loss: 855.30
 ---- batch: 090 ----
mean loss: 858.51
 ---- batch: 100 ----
mean loss: 874.73
 ---- batch: 110 ----
mean loss: 872.63
train mean loss: 858.89
epoch train time: 0:00:01.959002
elapsed time: 0:01:06.988976
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 22:29:31.879083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.78
 ---- batch: 020 ----
mean loss: 868.62
 ---- batch: 030 ----
mean loss: 851.85
 ---- batch: 040 ----
mean loss: 831.66
 ---- batch: 050 ----
mean loss: 845.37
 ---- batch: 060 ----
mean loss: 860.30
 ---- batch: 070 ----
mean loss: 853.43
 ---- batch: 080 ----
mean loss: 856.89
 ---- batch: 090 ----
mean loss: 861.32
 ---- batch: 100 ----
mean loss: 853.78
 ---- batch: 110 ----
mean loss: 874.74
train mean loss: 855.37
epoch train time: 0:00:01.921929
elapsed time: 0:01:08.911465
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 22:29:33.801643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.70
 ---- batch: 020 ----
mean loss: 865.09
 ---- batch: 030 ----
mean loss: 867.09
 ---- batch: 040 ----
mean loss: 846.30
 ---- batch: 050 ----
mean loss: 843.35
 ---- batch: 060 ----
mean loss: 864.82
 ---- batch: 070 ----
mean loss: 854.48
 ---- batch: 080 ----
mean loss: 861.49
 ---- batch: 090 ----
mean loss: 846.50
 ---- batch: 100 ----
mean loss: 858.68
 ---- batch: 110 ----
mean loss: 857.24
train mean loss: 856.03
epoch train time: 0:00:01.935294
elapsed time: 0:01:10.847374
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 22:29:35.737494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.98
 ---- batch: 020 ----
mean loss: 893.61
 ---- batch: 030 ----
mean loss: 853.05
 ---- batch: 040 ----
mean loss: 879.82
 ---- batch: 050 ----
mean loss: 863.11
 ---- batch: 060 ----
mean loss: 873.60
 ---- batch: 070 ----
mean loss: 850.69
 ---- batch: 080 ----
mean loss: 871.65
 ---- batch: 090 ----
mean loss: 841.59
 ---- batch: 100 ----
mean loss: 836.19
 ---- batch: 110 ----
mean loss: 839.14
train mean loss: 856.13
epoch train time: 0:00:01.894152
elapsed time: 0:01:12.742085
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 22:29:37.632020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.27
 ---- batch: 020 ----
mean loss: 867.53
 ---- batch: 030 ----
mean loss: 840.13
 ---- batch: 040 ----
mean loss: 866.64
 ---- batch: 050 ----
mean loss: 862.03
 ---- batch: 060 ----
mean loss: 852.31
 ---- batch: 070 ----
mean loss: 865.17
 ---- batch: 080 ----
mean loss: 848.79
 ---- batch: 090 ----
mean loss: 846.22
 ---- batch: 100 ----
mean loss: 839.51
 ---- batch: 110 ----
mean loss: 867.03
train mean loss: 854.71
epoch train time: 0:00:01.910950
elapsed time: 0:01:14.653416
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 22:29:39.543558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.56
 ---- batch: 020 ----
mean loss: 843.20
 ---- batch: 030 ----
mean loss: 833.61
 ---- batch: 040 ----
mean loss: 852.36
 ---- batch: 050 ----
mean loss: 879.45
 ---- batch: 060 ----
mean loss: 840.98
 ---- batch: 070 ----
mean loss: 869.16
 ---- batch: 080 ----
mean loss: 839.32
 ---- batch: 090 ----
mean loss: 856.14
 ---- batch: 100 ----
mean loss: 873.83
 ---- batch: 110 ----
mean loss: 856.81
train mean loss: 853.44
epoch train time: 0:00:01.913286
elapsed time: 0:01:16.567278
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 22:29:41.457496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.42
 ---- batch: 020 ----
mean loss: 850.94
 ---- batch: 030 ----
mean loss: 866.06
 ---- batch: 040 ----
mean loss: 854.14
 ---- batch: 050 ----
mean loss: 872.87
 ---- batch: 060 ----
mean loss: 839.66
 ---- batch: 070 ----
mean loss: 844.13
 ---- batch: 080 ----
mean loss: 868.45
 ---- batch: 090 ----
mean loss: 850.04
 ---- batch: 100 ----
mean loss: 860.02
 ---- batch: 110 ----
mean loss: 838.71
train mean loss: 853.31
epoch train time: 0:00:01.880680
elapsed time: 0:01:18.448605
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 22:29:43.338766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.11
 ---- batch: 020 ----
mean loss: 852.65
 ---- batch: 030 ----
mean loss: 834.76
 ---- batch: 040 ----
mean loss: 857.90
 ---- batch: 050 ----
mean loss: 854.24
 ---- batch: 060 ----
mean loss: 864.50
 ---- batch: 070 ----
mean loss: 829.53
 ---- batch: 080 ----
mean loss: 857.60
 ---- batch: 090 ----
mean loss: 861.97
 ---- batch: 100 ----
mean loss: 840.93
 ---- batch: 110 ----
mean loss: 863.48
train mean loss: 852.79
epoch train time: 0:00:01.916003
elapsed time: 0:01:20.365216
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 22:29:45.255372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.77
 ---- batch: 020 ----
mean loss: 853.29
 ---- batch: 030 ----
mean loss: 851.11
 ---- batch: 040 ----
mean loss: 848.77
 ---- batch: 050 ----
mean loss: 845.13
 ---- batch: 060 ----
mean loss: 863.71
 ---- batch: 070 ----
mean loss: 875.94
 ---- batch: 080 ----
mean loss: 839.98
 ---- batch: 090 ----
mean loss: 853.43
 ---- batch: 100 ----
mean loss: 849.09
 ---- batch: 110 ----
mean loss: 844.90
train mean loss: 852.75
epoch train time: 0:00:01.921885
elapsed time: 0:01:22.287689
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 22:29:47.177855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.79
 ---- batch: 020 ----
mean loss: 862.84
 ---- batch: 030 ----
mean loss: 865.42
 ---- batch: 040 ----
mean loss: 853.42
 ---- batch: 050 ----
mean loss: 847.91
 ---- batch: 060 ----
mean loss: 827.13
 ---- batch: 070 ----
mean loss: 833.48
 ---- batch: 080 ----
mean loss: 865.45
 ---- batch: 090 ----
mean loss: 871.05
 ---- batch: 100 ----
mean loss: 844.54
 ---- batch: 110 ----
mean loss: 844.98
train mean loss: 851.40
epoch train time: 0:00:01.916459
elapsed time: 0:01:24.204761
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 22:29:49.094901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.76
 ---- batch: 020 ----
mean loss: 842.97
 ---- batch: 030 ----
mean loss: 860.75
 ---- batch: 040 ----
mean loss: 870.05
 ---- batch: 050 ----
mean loss: 845.84
 ---- batch: 060 ----
mean loss: 847.56
 ---- batch: 070 ----
mean loss: 836.75
 ---- batch: 080 ----
mean loss: 857.59
 ---- batch: 090 ----
mean loss: 861.75
 ---- batch: 100 ----
mean loss: 840.76
 ---- batch: 110 ----
mean loss: 850.48
train mean loss: 850.70
epoch train time: 0:00:01.884648
elapsed time: 0:01:26.089984
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 22:29:50.980097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.65
 ---- batch: 020 ----
mean loss: 845.09
 ---- batch: 030 ----
mean loss: 861.62
 ---- batch: 040 ----
mean loss: 869.58
 ---- batch: 050 ----
mean loss: 863.58
 ---- batch: 060 ----
mean loss: 846.69
 ---- batch: 070 ----
mean loss: 855.75
 ---- batch: 080 ----
mean loss: 854.01
 ---- batch: 090 ----
mean loss: 827.34
 ---- batch: 100 ----
mean loss: 861.59
 ---- batch: 110 ----
mean loss: 846.64
train mean loss: 850.66
epoch train time: 0:00:01.903448
elapsed time: 0:01:27.994062
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 22:29:52.884257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.78
 ---- batch: 020 ----
mean loss: 831.89
 ---- batch: 030 ----
mean loss: 839.01
 ---- batch: 040 ----
mean loss: 848.69
 ---- batch: 050 ----
mean loss: 852.68
 ---- batch: 060 ----
mean loss: 847.82
 ---- batch: 070 ----
mean loss: 856.54
 ---- batch: 080 ----
mean loss: 866.95
 ---- batch: 090 ----
mean loss: 848.66
 ---- batch: 100 ----
mean loss: 856.47
 ---- batch: 110 ----
mean loss: 847.14
train mean loss: 850.51
epoch train time: 0:00:01.913457
elapsed time: 0:01:29.908160
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 22:29:54.798318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.17
 ---- batch: 020 ----
mean loss: 844.09
 ---- batch: 030 ----
mean loss: 853.10
 ---- batch: 040 ----
mean loss: 864.81
 ---- batch: 050 ----
mean loss: 851.36
 ---- batch: 060 ----
mean loss: 845.84
 ---- batch: 070 ----
mean loss: 810.27
 ---- batch: 080 ----
mean loss: 855.84
 ---- batch: 090 ----
mean loss: 852.23
 ---- batch: 100 ----
mean loss: 863.18
 ---- batch: 110 ----
mean loss: 862.32
train mean loss: 849.79
epoch train time: 0:00:01.917863
elapsed time: 0:01:31.826701
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 22:29:56.716860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.28
 ---- batch: 020 ----
mean loss: 842.26
 ---- batch: 030 ----
mean loss: 870.81
 ---- batch: 040 ----
mean loss: 872.57
 ---- batch: 050 ----
mean loss: 823.01
 ---- batch: 060 ----
mean loss: 839.98
 ---- batch: 070 ----
mean loss: 863.12
 ---- batch: 080 ----
mean loss: 851.22
 ---- batch: 090 ----
mean loss: 849.58
 ---- batch: 100 ----
mean loss: 850.29
 ---- batch: 110 ----
mean loss: 836.34
train mean loss: 850.08
epoch train time: 0:00:01.909173
elapsed time: 0:01:33.736455
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 22:29:58.626562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.55
 ---- batch: 020 ----
mean loss: 837.80
 ---- batch: 030 ----
mean loss: 864.07
 ---- batch: 040 ----
mean loss: 870.39
 ---- batch: 050 ----
mean loss: 857.95
 ---- batch: 060 ----
mean loss: 860.44
 ---- batch: 070 ----
mean loss: 828.11
 ---- batch: 080 ----
mean loss: 856.25
 ---- batch: 090 ----
mean loss: 847.55
 ---- batch: 100 ----
mean loss: 856.37
 ---- batch: 110 ----
mean loss: 837.44
train mean loss: 849.97
epoch train time: 0:00:01.893386
elapsed time: 0:01:35.630392
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 22:30:00.520509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.51
 ---- batch: 020 ----
mean loss: 855.13
 ---- batch: 030 ----
mean loss: 844.71
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 831.58
 ---- batch: 060 ----
mean loss: 838.34
 ---- batch: 070 ----
mean loss: 847.92
 ---- batch: 080 ----
mean loss: 863.86
 ---- batch: 090 ----
mean loss: 862.91
 ---- batch: 100 ----
mean loss: 853.91
 ---- batch: 110 ----
mean loss: 864.14
train mean loss: 849.35
epoch train time: 0:00:01.921655
elapsed time: 0:01:37.552608
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 22:30:02.442512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.85
 ---- batch: 020 ----
mean loss: 818.70
 ---- batch: 030 ----
mean loss: 842.31
 ---- batch: 040 ----
mean loss: 854.56
 ---- batch: 050 ----
mean loss: 853.57
 ---- batch: 060 ----
mean loss: 858.97
 ---- batch: 070 ----
mean loss: 839.01
 ---- batch: 080 ----
mean loss: 858.02
 ---- batch: 090 ----
mean loss: 859.29
 ---- batch: 100 ----
mean loss: 859.62
 ---- batch: 110 ----
mean loss: 845.18
train mean loss: 848.93
epoch train time: 0:00:01.874785
elapsed time: 0:01:39.427778
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 22:30:04.317967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.78
 ---- batch: 020 ----
mean loss: 866.81
 ---- batch: 030 ----
mean loss: 840.22
 ---- batch: 040 ----
mean loss: 850.11
 ---- batch: 050 ----
mean loss: 842.20
 ---- batch: 060 ----
mean loss: 840.79
 ---- batch: 070 ----
mean loss: 847.28
 ---- batch: 080 ----
mean loss: 853.14
 ---- batch: 090 ----
mean loss: 833.58
 ---- batch: 100 ----
mean loss: 839.44
 ---- batch: 110 ----
mean loss: 837.82
train mean loss: 848.11
epoch train time: 0:00:01.900432
elapsed time: 0:01:41.328867
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 22:30:06.218960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.59
 ---- batch: 020 ----
mean loss: 846.95
 ---- batch: 030 ----
mean loss: 863.34
 ---- batch: 040 ----
mean loss: 847.16
 ---- batch: 050 ----
mean loss: 830.64
 ---- batch: 060 ----
mean loss: 853.28
 ---- batch: 070 ----
mean loss: 840.49
 ---- batch: 080 ----
mean loss: 834.28
 ---- batch: 090 ----
mean loss: 867.89
 ---- batch: 100 ----
mean loss: 857.65
 ---- batch: 110 ----
mean loss: 829.80
train mean loss: 848.16
epoch train time: 0:00:01.899771
elapsed time: 0:01:43.229199
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 22:30:08.119323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.57
 ---- batch: 020 ----
mean loss: 859.03
 ---- batch: 030 ----
mean loss: 834.33
 ---- batch: 040 ----
mean loss: 845.35
 ---- batch: 050 ----
mean loss: 868.39
 ---- batch: 060 ----
mean loss: 836.48
 ---- batch: 070 ----
mean loss: 840.90
 ---- batch: 080 ----
mean loss: 834.93
 ---- batch: 090 ----
mean loss: 832.09
 ---- batch: 100 ----
mean loss: 858.42
 ---- batch: 110 ----
mean loss: 859.06
train mean loss: 847.52
epoch train time: 0:00:01.874204
elapsed time: 0:01:45.103977
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 22:30:09.994112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.52
 ---- batch: 020 ----
mean loss: 841.55
 ---- batch: 030 ----
mean loss: 858.70
 ---- batch: 040 ----
mean loss: 864.98
 ---- batch: 050 ----
mean loss: 821.15
 ---- batch: 060 ----
mean loss: 836.45
 ---- batch: 070 ----
mean loss: 864.12
 ---- batch: 080 ----
mean loss: 858.80
 ---- batch: 090 ----
mean loss: 829.11
 ---- batch: 100 ----
mean loss: 845.58
 ---- batch: 110 ----
mean loss: 847.32
train mean loss: 848.15
epoch train time: 0:00:01.904355
elapsed time: 0:01:47.008921
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 22:30:11.899033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.18
 ---- batch: 020 ----
mean loss: 848.59
 ---- batch: 030 ----
mean loss: 845.19
 ---- batch: 040 ----
mean loss: 850.97
 ---- batch: 050 ----
mean loss: 838.03
 ---- batch: 060 ----
mean loss: 840.77
 ---- batch: 070 ----
mean loss: 843.97
 ---- batch: 080 ----
mean loss: 856.66
 ---- batch: 090 ----
mean loss: 848.44
 ---- batch: 100 ----
mean loss: 853.06
 ---- batch: 110 ----
mean loss: 852.81
train mean loss: 847.81
epoch train time: 0:00:01.879649
elapsed time: 0:01:48.889131
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 22:30:13.779257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.41
 ---- batch: 020 ----
mean loss: 856.65
 ---- batch: 030 ----
mean loss: 837.14
 ---- batch: 040 ----
mean loss: 861.29
 ---- batch: 050 ----
mean loss: 857.00
 ---- batch: 060 ----
mean loss: 847.63
 ---- batch: 070 ----
mean loss: 835.04
 ---- batch: 080 ----
mean loss: 847.94
 ---- batch: 090 ----
mean loss: 839.69
 ---- batch: 100 ----
mean loss: 841.52
 ---- batch: 110 ----
mean loss: 849.89
train mean loss: 846.63
epoch train time: 0:00:01.873292
elapsed time: 0:01:50.763090
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 22:30:15.653196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.44
 ---- batch: 020 ----
mean loss: 830.26
 ---- batch: 030 ----
mean loss: 843.03
 ---- batch: 040 ----
mean loss: 850.66
 ---- batch: 050 ----
mean loss: 838.46
 ---- batch: 060 ----
mean loss: 854.11
 ---- batch: 070 ----
mean loss: 847.34
 ---- batch: 080 ----
mean loss: 866.81
 ---- batch: 090 ----
mean loss: 857.15
 ---- batch: 100 ----
mean loss: 842.93
 ---- batch: 110 ----
mean loss: 823.45
train mean loss: 846.04
epoch train time: 0:00:01.881737
elapsed time: 0:01:52.645376
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 22:30:17.535636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.63
 ---- batch: 020 ----
mean loss: 829.05
 ---- batch: 030 ----
mean loss: 843.51
 ---- batch: 040 ----
mean loss: 827.50
 ---- batch: 050 ----
mean loss: 850.92
 ---- batch: 060 ----
mean loss: 810.43
 ---- batch: 070 ----
mean loss: 803.58
 ---- batch: 080 ----
mean loss: 786.87
 ---- batch: 090 ----
mean loss: 742.88
 ---- batch: 100 ----
mean loss: 693.16
 ---- batch: 110 ----
mean loss: 644.63
train mean loss: 784.26
epoch train time: 0:00:01.903133
elapsed time: 0:01:54.549262
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 22:30:19.439436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.24
 ---- batch: 020 ----
mean loss: 544.30
 ---- batch: 030 ----
mean loss: 521.14
 ---- batch: 040 ----
mean loss: 496.31
 ---- batch: 050 ----
mean loss: 466.63
 ---- batch: 060 ----
mean loss: 453.03
 ---- batch: 070 ----
mean loss: 449.12
 ---- batch: 080 ----
mean loss: 435.58
 ---- batch: 090 ----
mean loss: 419.74
 ---- batch: 100 ----
mean loss: 407.76
 ---- batch: 110 ----
mean loss: 390.99
train mean loss: 468.45
epoch train time: 0:00:01.897524
elapsed time: 0:01:56.447391
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 22:30:21.337525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.38
 ---- batch: 020 ----
mean loss: 387.51
 ---- batch: 030 ----
mean loss: 375.29
 ---- batch: 040 ----
mean loss: 381.01
 ---- batch: 050 ----
mean loss: 370.88
 ---- batch: 060 ----
mean loss: 384.61
 ---- batch: 070 ----
mean loss: 361.24
 ---- batch: 080 ----
mean loss: 343.46
 ---- batch: 090 ----
mean loss: 363.52
 ---- batch: 100 ----
mean loss: 349.86
 ---- batch: 110 ----
mean loss: 333.83
train mean loss: 366.06
epoch train time: 0:00:01.913243
elapsed time: 0:01:58.361218
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 22:30:23.251437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.11
 ---- batch: 020 ----
mean loss: 349.30
 ---- batch: 030 ----
mean loss: 337.63
 ---- batch: 040 ----
mean loss: 337.66
 ---- batch: 050 ----
mean loss: 325.85
 ---- batch: 060 ----
mean loss: 319.08
 ---- batch: 070 ----
mean loss: 328.89
 ---- batch: 080 ----
mean loss: 337.00
 ---- batch: 090 ----
mean loss: 315.50
 ---- batch: 100 ----
mean loss: 325.76
 ---- batch: 110 ----
mean loss: 318.20
train mean loss: 330.25
epoch train time: 0:00:01.925191
elapsed time: 0:02:00.287050
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 22:30:25.177168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.81
 ---- batch: 020 ----
mean loss: 318.96
 ---- batch: 030 ----
mean loss: 306.46
 ---- batch: 040 ----
mean loss: 303.76
 ---- batch: 050 ----
mean loss: 312.84
 ---- batch: 060 ----
mean loss: 306.04
 ---- batch: 070 ----
mean loss: 314.18
 ---- batch: 080 ----
mean loss: 312.35
 ---- batch: 090 ----
mean loss: 298.68
 ---- batch: 100 ----
mean loss: 304.22
 ---- batch: 110 ----
mean loss: 304.00
train mean loss: 308.64
epoch train time: 0:00:01.883920
elapsed time: 0:02:02.171557
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 22:30:27.061739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.15
 ---- batch: 020 ----
mean loss: 309.13
 ---- batch: 030 ----
mean loss: 296.36
 ---- batch: 040 ----
mean loss: 309.71
 ---- batch: 050 ----
mean loss: 290.65
 ---- batch: 060 ----
mean loss: 294.72
 ---- batch: 070 ----
mean loss: 302.38
 ---- batch: 080 ----
mean loss: 299.40
 ---- batch: 090 ----
mean loss: 286.96
 ---- batch: 100 ----
mean loss: 275.86
 ---- batch: 110 ----
mean loss: 274.74
train mean loss: 293.61
epoch train time: 0:00:01.906862
elapsed time: 0:02:04.079045
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 22:30:28.969175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.60
 ---- batch: 020 ----
mean loss: 284.51
 ---- batch: 030 ----
mean loss: 278.99
 ---- batch: 040 ----
mean loss: 285.21
 ---- batch: 050 ----
mean loss: 279.67
 ---- batch: 060 ----
mean loss: 296.76
 ---- batch: 070 ----
mean loss: 288.02
 ---- batch: 080 ----
mean loss: 294.55
 ---- batch: 090 ----
mean loss: 277.92
 ---- batch: 100 ----
mean loss: 278.36
 ---- batch: 110 ----
mean loss: 280.46
train mean loss: 284.50
epoch train time: 0:00:01.906859
elapsed time: 0:02:05.986470
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 22:30:30.876600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.30
 ---- batch: 020 ----
mean loss: 283.40
 ---- batch: 030 ----
mean loss: 274.84
 ---- batch: 040 ----
mean loss: 289.02
 ---- batch: 050 ----
mean loss: 269.31
 ---- batch: 060 ----
mean loss: 275.45
 ---- batch: 070 ----
mean loss: 269.56
 ---- batch: 080 ----
mean loss: 269.63
 ---- batch: 090 ----
mean loss: 266.21
 ---- batch: 100 ----
mean loss: 271.18
 ---- batch: 110 ----
mean loss: 262.23
train mean loss: 274.37
epoch train time: 0:00:01.911266
elapsed time: 0:02:07.898326
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 22:30:32.788503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.82
 ---- batch: 020 ----
mean loss: 262.20
 ---- batch: 030 ----
mean loss: 258.46
 ---- batch: 040 ----
mean loss: 268.06
 ---- batch: 050 ----
mean loss: 270.68
 ---- batch: 060 ----
mean loss: 267.46
 ---- batch: 070 ----
mean loss: 270.01
 ---- batch: 080 ----
mean loss: 263.55
 ---- batch: 090 ----
mean loss: 279.05
 ---- batch: 100 ----
mean loss: 265.80
 ---- batch: 110 ----
mean loss: 277.15
train mean loss: 268.02
epoch train time: 0:00:01.920803
elapsed time: 0:02:09.819779
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 22:30:34.709958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.19
 ---- batch: 020 ----
mean loss: 266.04
 ---- batch: 030 ----
mean loss: 273.50
 ---- batch: 040 ----
mean loss: 259.38
 ---- batch: 050 ----
mean loss: 255.98
 ---- batch: 060 ----
mean loss: 254.39
 ---- batch: 070 ----
mean loss: 265.56
 ---- batch: 080 ----
mean loss: 265.80
 ---- batch: 090 ----
mean loss: 264.23
 ---- batch: 100 ----
mean loss: 265.08
 ---- batch: 110 ----
mean loss: 268.05
train mean loss: 263.41
epoch train time: 0:00:01.979281
elapsed time: 0:02:11.799708
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 22:30:36.689922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.57
 ---- batch: 020 ----
mean loss: 262.08
 ---- batch: 030 ----
mean loss: 257.60
 ---- batch: 040 ----
mean loss: 257.51
 ---- batch: 050 ----
mean loss: 245.28
 ---- batch: 060 ----
mean loss: 252.52
 ---- batch: 070 ----
mean loss: 254.94
 ---- batch: 080 ----
mean loss: 249.55
 ---- batch: 090 ----
mean loss: 243.42
 ---- batch: 100 ----
mean loss: 251.10
 ---- batch: 110 ----
mean loss: 265.32
train mean loss: 255.46
epoch train time: 0:00:01.930992
elapsed time: 0:02:13.731361
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 22:30:38.621536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.04
 ---- batch: 020 ----
mean loss: 255.56
 ---- batch: 030 ----
mean loss: 257.37
 ---- batch: 040 ----
mean loss: 251.90
 ---- batch: 050 ----
mean loss: 253.90
 ---- batch: 060 ----
mean loss: 252.90
 ---- batch: 070 ----
mean loss: 247.33
 ---- batch: 080 ----
mean loss: 242.65
 ---- batch: 090 ----
mean loss: 247.87
 ---- batch: 100 ----
mean loss: 247.73
 ---- batch: 110 ----
mean loss: 253.33
train mean loss: 251.20
epoch train time: 0:00:01.899315
elapsed time: 0:02:15.631279
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 22:30:40.521413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.96
 ---- batch: 020 ----
mean loss: 237.80
 ---- batch: 030 ----
mean loss: 254.19
 ---- batch: 040 ----
mean loss: 245.09
 ---- batch: 050 ----
mean loss: 243.72
 ---- batch: 060 ----
mean loss: 232.59
 ---- batch: 070 ----
mean loss: 230.34
 ---- batch: 080 ----
mean loss: 241.74
 ---- batch: 090 ----
mean loss: 243.16
 ---- batch: 100 ----
mean loss: 250.31
 ---- batch: 110 ----
mean loss: 249.22
train mean loss: 243.57
epoch train time: 0:00:01.913819
elapsed time: 0:02:17.545665
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 22:30:42.435801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.94
 ---- batch: 020 ----
mean loss: 240.74
 ---- batch: 030 ----
mean loss: 252.93
 ---- batch: 040 ----
mean loss: 244.77
 ---- batch: 050 ----
mean loss: 246.11
 ---- batch: 060 ----
mean loss: 247.04
 ---- batch: 070 ----
mean loss: 238.26
 ---- batch: 080 ----
mean loss: 231.87
 ---- batch: 090 ----
mean loss: 241.08
 ---- batch: 100 ----
mean loss: 234.87
 ---- batch: 110 ----
mean loss: 238.41
train mean loss: 241.73
epoch train time: 0:00:01.916970
elapsed time: 0:02:19.463207
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 22:30:44.353072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.99
 ---- batch: 020 ----
mean loss: 242.82
 ---- batch: 030 ----
mean loss: 247.11
 ---- batch: 040 ----
mean loss: 228.57
 ---- batch: 050 ----
mean loss: 239.89
 ---- batch: 060 ----
mean loss: 246.07
 ---- batch: 070 ----
mean loss: 240.94
 ---- batch: 080 ----
mean loss: 233.60
 ---- batch: 090 ----
mean loss: 239.85
 ---- batch: 100 ----
mean loss: 225.30
 ---- batch: 110 ----
mean loss: 234.17
train mean loss: 237.78
epoch train time: 0:00:01.917708
elapsed time: 0:02:21.381220
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 22:30:46.271362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.11
 ---- batch: 020 ----
mean loss: 230.77
 ---- batch: 030 ----
mean loss: 231.28
 ---- batch: 040 ----
mean loss: 226.01
 ---- batch: 050 ----
mean loss: 236.94
 ---- batch: 060 ----
mean loss: 234.16
 ---- batch: 070 ----
mean loss: 239.83
 ---- batch: 080 ----
mean loss: 230.19
 ---- batch: 090 ----
mean loss: 235.77
 ---- batch: 100 ----
mean loss: 230.14
 ---- batch: 110 ----
mean loss: 237.65
train mean loss: 233.92
epoch train time: 0:00:01.933936
elapsed time: 0:02:23.315755
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 22:30:48.205926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.55
 ---- batch: 020 ----
mean loss: 234.42
 ---- batch: 030 ----
mean loss: 228.90
 ---- batch: 040 ----
mean loss: 228.88
 ---- batch: 050 ----
mean loss: 222.23
 ---- batch: 060 ----
mean loss: 224.16
 ---- batch: 070 ----
mean loss: 225.90
 ---- batch: 080 ----
mean loss: 241.59
 ---- batch: 090 ----
mean loss: 226.01
 ---- batch: 100 ----
mean loss: 234.14
 ---- batch: 110 ----
mean loss: 227.02
train mean loss: 228.20
epoch train time: 0:00:01.883629
elapsed time: 0:02:25.200008
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 22:30:50.090111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.00
 ---- batch: 020 ----
mean loss: 225.13
 ---- batch: 030 ----
mean loss: 226.33
 ---- batch: 040 ----
mean loss: 228.55
 ---- batch: 050 ----
mean loss: 221.88
 ---- batch: 060 ----
mean loss: 221.54
 ---- batch: 070 ----
mean loss: 220.90
 ---- batch: 080 ----
mean loss: 224.88
 ---- batch: 090 ----
mean loss: 237.11
 ---- batch: 100 ----
mean loss: 230.73
 ---- batch: 110 ----
mean loss: 234.90
train mean loss: 226.66
epoch train time: 0:00:01.905540
elapsed time: 0:02:27.106081
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 22:30:51.996240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.10
 ---- batch: 020 ----
mean loss: 218.15
 ---- batch: 030 ----
mean loss: 235.56
 ---- batch: 040 ----
mean loss: 225.01
 ---- batch: 050 ----
mean loss: 217.61
 ---- batch: 060 ----
mean loss: 221.54
 ---- batch: 070 ----
mean loss: 224.52
 ---- batch: 080 ----
mean loss: 225.71
 ---- batch: 090 ----
mean loss: 227.94
 ---- batch: 100 ----
mean loss: 220.62
 ---- batch: 110 ----
mean loss: 218.29
train mean loss: 223.33
epoch train time: 0:00:01.890850
elapsed time: 0:02:28.997536
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 22:30:53.887747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.63
 ---- batch: 020 ----
mean loss: 225.65
 ---- batch: 030 ----
mean loss: 216.14
 ---- batch: 040 ----
mean loss: 211.22
 ---- batch: 050 ----
mean loss: 222.99
 ---- batch: 060 ----
mean loss: 213.91
 ---- batch: 070 ----
mean loss: 222.06
 ---- batch: 080 ----
mean loss: 217.42
 ---- batch: 090 ----
mean loss: 223.91
 ---- batch: 100 ----
mean loss: 211.83
 ---- batch: 110 ----
mean loss: 222.99
train mean loss: 218.35
epoch train time: 0:00:01.908345
elapsed time: 0:02:30.906579
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 22:30:55.796766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.13
 ---- batch: 020 ----
mean loss: 205.99
 ---- batch: 030 ----
mean loss: 218.50
 ---- batch: 040 ----
mean loss: 220.83
 ---- batch: 050 ----
mean loss: 210.33
 ---- batch: 060 ----
mean loss: 220.98
 ---- batch: 070 ----
mean loss: 205.51
 ---- batch: 080 ----
mean loss: 224.94
 ---- batch: 090 ----
mean loss: 213.74
 ---- batch: 100 ----
mean loss: 220.27
 ---- batch: 110 ----
mean loss: 213.12
train mean loss: 216.52
epoch train time: 0:00:01.912329
elapsed time: 0:02:32.819545
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 22:30:57.709764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.97
 ---- batch: 020 ----
mean loss: 211.05
 ---- batch: 030 ----
mean loss: 205.61
 ---- batch: 040 ----
mean loss: 215.82
 ---- batch: 050 ----
mean loss: 206.24
 ---- batch: 060 ----
mean loss: 211.09
 ---- batch: 070 ----
mean loss: 219.62
 ---- batch: 080 ----
mean loss: 211.00
 ---- batch: 090 ----
mean loss: 212.96
 ---- batch: 100 ----
mean loss: 211.75
 ---- batch: 110 ----
mean loss: 225.88
train mean loss: 212.85
epoch train time: 0:00:01.893507
elapsed time: 0:02:34.713726
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 22:30:59.603822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.73
 ---- batch: 020 ----
mean loss: 212.21
 ---- batch: 030 ----
mean loss: 206.71
 ---- batch: 040 ----
mean loss: 197.77
 ---- batch: 050 ----
mean loss: 210.71
 ---- batch: 060 ----
mean loss: 212.81
 ---- batch: 070 ----
mean loss: 215.64
 ---- batch: 080 ----
mean loss: 211.58
 ---- batch: 090 ----
mean loss: 214.27
 ---- batch: 100 ----
mean loss: 211.31
 ---- batch: 110 ----
mean loss: 206.27
train mean loss: 210.16
epoch train time: 0:00:01.905327
elapsed time: 0:02:36.619584
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 22:31:01.509785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.66
 ---- batch: 020 ----
mean loss: 199.97
 ---- batch: 030 ----
mean loss: 209.78
 ---- batch: 040 ----
mean loss: 212.80
 ---- batch: 050 ----
mean loss: 215.75
 ---- batch: 060 ----
mean loss: 208.12
 ---- batch: 070 ----
mean loss: 214.78
 ---- batch: 080 ----
mean loss: 208.36
 ---- batch: 090 ----
mean loss: 198.14
 ---- batch: 100 ----
mean loss: 203.17
 ---- batch: 110 ----
mean loss: 214.56
train mean loss: 209.11
epoch train time: 0:00:01.898843
elapsed time: 0:02:38.519082
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 22:31:03.409206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.85
 ---- batch: 020 ----
mean loss: 202.36
 ---- batch: 030 ----
mean loss: 205.89
 ---- batch: 040 ----
mean loss: 204.91
 ---- batch: 050 ----
mean loss: 197.96
 ---- batch: 060 ----
mean loss: 207.61
 ---- batch: 070 ----
mean loss: 203.97
 ---- batch: 080 ----
mean loss: 209.83
 ---- batch: 090 ----
mean loss: 203.91
 ---- batch: 100 ----
mean loss: 205.98
 ---- batch: 110 ----
mean loss: 203.46
train mean loss: 205.22
epoch train time: 0:00:01.905728
elapsed time: 0:02:40.425409
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 22:31:05.315571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.05
 ---- batch: 020 ----
mean loss: 198.21
 ---- batch: 030 ----
mean loss: 189.47
 ---- batch: 040 ----
mean loss: 204.51
 ---- batch: 050 ----
mean loss: 204.42
 ---- batch: 060 ----
mean loss: 198.69
 ---- batch: 070 ----
mean loss: 206.24
 ---- batch: 080 ----
mean loss: 200.35
 ---- batch: 090 ----
mean loss: 208.23
 ---- batch: 100 ----
mean loss: 203.91
 ---- batch: 110 ----
mean loss: 208.18
train mean loss: 202.69
epoch train time: 0:00:01.923846
elapsed time: 0:02:42.349853
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 22:31:07.239966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.46
 ---- batch: 020 ----
mean loss: 201.32
 ---- batch: 030 ----
mean loss: 194.13
 ---- batch: 040 ----
mean loss: 202.52
 ---- batch: 050 ----
mean loss: 207.58
 ---- batch: 060 ----
mean loss: 206.55
 ---- batch: 070 ----
mean loss: 200.24
 ---- batch: 080 ----
mean loss: 199.13
 ---- batch: 090 ----
mean loss: 199.64
 ---- batch: 100 ----
mean loss: 189.83
 ---- batch: 110 ----
mean loss: 207.68
train mean loss: 200.54
epoch train time: 0:00:01.896407
elapsed time: 0:02:44.246799
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 22:31:09.136968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.86
 ---- batch: 020 ----
mean loss: 201.80
 ---- batch: 030 ----
mean loss: 196.79
 ---- batch: 040 ----
mean loss: 191.23
 ---- batch: 050 ----
mean loss: 198.52
 ---- batch: 060 ----
mean loss: 203.73
 ---- batch: 070 ----
mean loss: 193.51
 ---- batch: 080 ----
mean loss: 192.15
 ---- batch: 090 ----
mean loss: 198.93
 ---- batch: 100 ----
mean loss: 194.11
 ---- batch: 110 ----
mean loss: 201.38
train mean loss: 197.79
epoch train time: 0:00:01.877691
elapsed time: 0:02:46.125172
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 22:31:11.015281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.04
 ---- batch: 020 ----
mean loss: 192.49
 ---- batch: 030 ----
mean loss: 202.30
 ---- batch: 040 ----
mean loss: 198.62
 ---- batch: 050 ----
mean loss: 191.53
 ---- batch: 060 ----
mean loss: 191.00
 ---- batch: 070 ----
mean loss: 194.90
 ---- batch: 080 ----
mean loss: 195.67
 ---- batch: 090 ----
mean loss: 195.39
 ---- batch: 100 ----
mean loss: 194.04
 ---- batch: 110 ----
mean loss: 199.48
train mean loss: 194.73
epoch train time: 0:00:01.901089
elapsed time: 0:02:48.026837
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 22:31:12.917056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.90
 ---- batch: 020 ----
mean loss: 187.48
 ---- batch: 030 ----
mean loss: 193.80
 ---- batch: 040 ----
mean loss: 190.90
 ---- batch: 050 ----
mean loss: 180.46
 ---- batch: 060 ----
mean loss: 195.67
 ---- batch: 070 ----
mean loss: 195.85
 ---- batch: 080 ----
mean loss: 201.74
 ---- batch: 090 ----
mean loss: 192.89
 ---- batch: 100 ----
mean loss: 195.63
 ---- batch: 110 ----
mean loss: 200.85
train mean loss: 193.54
epoch train time: 0:00:01.867024
elapsed time: 0:02:49.894538
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 22:31:14.784685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.03
 ---- batch: 020 ----
mean loss: 189.66
 ---- batch: 030 ----
mean loss: 187.73
 ---- batch: 040 ----
mean loss: 192.32
 ---- batch: 050 ----
mean loss: 200.73
 ---- batch: 060 ----
mean loss: 193.71
 ---- batch: 070 ----
mean loss: 194.46
 ---- batch: 080 ----
mean loss: 190.15
 ---- batch: 090 ----
mean loss: 185.88
 ---- batch: 100 ----
mean loss: 197.28
 ---- batch: 110 ----
mean loss: 186.96
train mean loss: 191.73
epoch train time: 0:00:01.903134
elapsed time: 0:02:51.798354
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 22:31:16.688485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.64
 ---- batch: 020 ----
mean loss: 193.11
 ---- batch: 030 ----
mean loss: 185.39
 ---- batch: 040 ----
mean loss: 186.65
 ---- batch: 050 ----
mean loss: 186.34
 ---- batch: 060 ----
mean loss: 194.90
 ---- batch: 070 ----
mean loss: 188.88
 ---- batch: 080 ----
mean loss: 197.09
 ---- batch: 090 ----
mean loss: 190.88
 ---- batch: 100 ----
mean loss: 193.99
 ---- batch: 110 ----
mean loss: 189.74
train mean loss: 191.00
epoch train time: 0:00:01.885355
elapsed time: 0:02:53.684278
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 22:31:18.574402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.74
 ---- batch: 020 ----
mean loss: 187.05
 ---- batch: 030 ----
mean loss: 183.75
 ---- batch: 040 ----
mean loss: 180.84
 ---- batch: 050 ----
mean loss: 187.02
 ---- batch: 060 ----
mean loss: 188.07
 ---- batch: 070 ----
mean loss: 192.30
 ---- batch: 080 ----
mean loss: 191.06
 ---- batch: 090 ----
mean loss: 187.44
 ---- batch: 100 ----
mean loss: 188.32
 ---- batch: 110 ----
mean loss: 186.65
train mean loss: 187.95
epoch train time: 0:00:01.904290
elapsed time: 0:02:55.589129
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 22:31:20.479238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.66
 ---- batch: 020 ----
mean loss: 182.13
 ---- batch: 030 ----
mean loss: 198.26
 ---- batch: 040 ----
mean loss: 185.68
 ---- batch: 050 ----
mean loss: 190.11
 ---- batch: 060 ----
mean loss: 178.17
 ---- batch: 070 ----
mean loss: 185.85
 ---- batch: 080 ----
mean loss: 187.13
 ---- batch: 090 ----
mean loss: 186.52
 ---- batch: 100 ----
mean loss: 183.02
 ---- batch: 110 ----
mean loss: 195.82
train mean loss: 186.99
epoch train time: 0:00:01.906832
elapsed time: 0:02:57.496504
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 22:31:22.386653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.44
 ---- batch: 020 ----
mean loss: 177.76
 ---- batch: 030 ----
mean loss: 190.22
 ---- batch: 040 ----
mean loss: 185.73
 ---- batch: 050 ----
mean loss: 186.15
 ---- batch: 060 ----
mean loss: 189.38
 ---- batch: 070 ----
mean loss: 183.00
 ---- batch: 080 ----
mean loss: 180.44
 ---- batch: 090 ----
mean loss: 186.29
 ---- batch: 100 ----
mean loss: 175.17
 ---- batch: 110 ----
mean loss: 185.58
train mean loss: 184.58
epoch train time: 0:00:01.906653
elapsed time: 0:02:59.403758
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 22:31:24.293937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.04
 ---- batch: 020 ----
mean loss: 187.05
 ---- batch: 030 ----
mean loss: 174.71
 ---- batch: 040 ----
mean loss: 181.04
 ---- batch: 050 ----
mean loss: 189.32
 ---- batch: 060 ----
mean loss: 185.85
 ---- batch: 070 ----
mean loss: 184.36
 ---- batch: 080 ----
mean loss: 183.11
 ---- batch: 090 ----
mean loss: 187.00
 ---- batch: 100 ----
mean loss: 187.90
 ---- batch: 110 ----
mean loss: 178.99
train mean loss: 183.52
epoch train time: 0:00:01.929672
elapsed time: 0:03:01.334050
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 22:31:26.224229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.71
 ---- batch: 020 ----
mean loss: 178.31
 ---- batch: 030 ----
mean loss: 178.20
 ---- batch: 040 ----
mean loss: 182.48
 ---- batch: 050 ----
mean loss: 181.50
 ---- batch: 060 ----
mean loss: 189.15
 ---- batch: 070 ----
mean loss: 182.51
 ---- batch: 080 ----
mean loss: 188.18
 ---- batch: 090 ----
mean loss: 181.25
 ---- batch: 100 ----
mean loss: 179.37
 ---- batch: 110 ----
mean loss: 181.63
train mean loss: 181.14
epoch train time: 0:00:01.917873
elapsed time: 0:03:03.252536
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 22:31:28.142676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.98
 ---- batch: 020 ----
mean loss: 182.54
 ---- batch: 030 ----
mean loss: 170.11
 ---- batch: 040 ----
mean loss: 183.39
 ---- batch: 050 ----
mean loss: 180.62
 ---- batch: 060 ----
mean loss: 186.55
 ---- batch: 070 ----
mean loss: 179.76
 ---- batch: 080 ----
mean loss: 175.64
 ---- batch: 090 ----
mean loss: 188.60
 ---- batch: 100 ----
mean loss: 177.06
 ---- batch: 110 ----
mean loss: 192.71
train mean loss: 180.79
epoch train time: 0:00:01.915666
elapsed time: 0:03:05.168793
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 22:31:30.058924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.78
 ---- batch: 020 ----
mean loss: 177.86
 ---- batch: 030 ----
mean loss: 173.44
 ---- batch: 040 ----
mean loss: 175.00
 ---- batch: 050 ----
mean loss: 175.47
 ---- batch: 060 ----
mean loss: 184.95
 ---- batch: 070 ----
mean loss: 182.60
 ---- batch: 080 ----
mean loss: 178.97
 ---- batch: 090 ----
mean loss: 181.32
 ---- batch: 100 ----
mean loss: 183.07
 ---- batch: 110 ----
mean loss: 176.01
train mean loss: 178.25
epoch train time: 0:00:01.921637
elapsed time: 0:03:07.091058
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 22:31:31.981208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.55
 ---- batch: 020 ----
mean loss: 175.89
 ---- batch: 030 ----
mean loss: 172.32
 ---- batch: 040 ----
mean loss: 178.65
 ---- batch: 050 ----
mean loss: 181.85
 ---- batch: 060 ----
mean loss: 173.25
 ---- batch: 070 ----
mean loss: 173.37
 ---- batch: 080 ----
mean loss: 195.83
 ---- batch: 090 ----
mean loss: 182.17
 ---- batch: 100 ----
mean loss: 166.62
 ---- batch: 110 ----
mean loss: 182.61
train mean loss: 177.43
epoch train time: 0:00:01.892051
elapsed time: 0:03:08.983693
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 22:31:33.873941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.14
 ---- batch: 020 ----
mean loss: 179.14
 ---- batch: 030 ----
mean loss: 178.68
 ---- batch: 040 ----
mean loss: 180.40
 ---- batch: 050 ----
mean loss: 172.51
 ---- batch: 060 ----
mean loss: 179.56
 ---- batch: 070 ----
mean loss: 181.74
 ---- batch: 080 ----
mean loss: 178.96
 ---- batch: 090 ----
mean loss: 179.38
 ---- batch: 100 ----
mean loss: 176.96
 ---- batch: 110 ----
mean loss: 178.00
train mean loss: 177.38
epoch train time: 0:00:01.891114
elapsed time: 0:03:10.875506
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 22:31:35.765649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.99
 ---- batch: 020 ----
mean loss: 174.25
 ---- batch: 030 ----
mean loss: 172.33
 ---- batch: 040 ----
mean loss: 167.95
 ---- batch: 050 ----
mean loss: 180.19
 ---- batch: 060 ----
mean loss: 177.06
 ---- batch: 070 ----
mean loss: 172.79
 ---- batch: 080 ----
mean loss: 173.04
 ---- batch: 090 ----
mean loss: 175.84
 ---- batch: 100 ----
mean loss: 176.54
 ---- batch: 110 ----
mean loss: 174.34
train mean loss: 174.85
epoch train time: 0:00:01.904721
elapsed time: 0:03:12.780863
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 22:31:37.671037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.63
 ---- batch: 020 ----
mean loss: 175.69
 ---- batch: 030 ----
mean loss: 171.01
 ---- batch: 040 ----
mean loss: 174.07
 ---- batch: 050 ----
mean loss: 168.65
 ---- batch: 060 ----
mean loss: 174.82
 ---- batch: 070 ----
mean loss: 179.16
 ---- batch: 080 ----
mean loss: 183.52
 ---- batch: 090 ----
mean loss: 175.33
 ---- batch: 100 ----
mean loss: 184.35
 ---- batch: 110 ----
mean loss: 172.75
train mean loss: 175.16
epoch train time: 0:00:01.927251
elapsed time: 0:03:14.708717
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 22:31:39.598854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.73
 ---- batch: 020 ----
mean loss: 175.89
 ---- batch: 030 ----
mean loss: 163.10
 ---- batch: 040 ----
mean loss: 176.67
 ---- batch: 050 ----
mean loss: 177.35
 ---- batch: 060 ----
mean loss: 174.64
 ---- batch: 070 ----
mean loss: 175.11
 ---- batch: 080 ----
mean loss: 179.49
 ---- batch: 090 ----
mean loss: 176.78
 ---- batch: 100 ----
mean loss: 175.72
 ---- batch: 110 ----
mean loss: 174.01
train mean loss: 173.98
epoch train time: 0:00:01.924244
elapsed time: 0:03:16.633548
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 22:31:41.523692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.00
 ---- batch: 020 ----
mean loss: 183.06
 ---- batch: 030 ----
mean loss: 164.71
 ---- batch: 040 ----
mean loss: 170.61
 ---- batch: 050 ----
mean loss: 177.11
 ---- batch: 060 ----
mean loss: 169.10
 ---- batch: 070 ----
mean loss: 170.98
 ---- batch: 080 ----
mean loss: 167.63
 ---- batch: 090 ----
mean loss: 177.56
 ---- batch: 100 ----
mean loss: 173.48
 ---- batch: 110 ----
mean loss: 178.65
train mean loss: 172.05
epoch train time: 0:00:01.912222
elapsed time: 0:03:18.546381
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 22:31:43.436608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.29
 ---- batch: 020 ----
mean loss: 175.35
 ---- batch: 030 ----
mean loss: 168.49
 ---- batch: 040 ----
mean loss: 162.93
 ---- batch: 050 ----
mean loss: 169.98
 ---- batch: 060 ----
mean loss: 171.72
 ---- batch: 070 ----
mean loss: 170.79
 ---- batch: 080 ----
mean loss: 181.54
 ---- batch: 090 ----
mean loss: 172.95
 ---- batch: 100 ----
mean loss: 165.62
 ---- batch: 110 ----
mean loss: 181.15
train mean loss: 171.55
epoch train time: 0:00:01.901991
elapsed time: 0:03:20.449052
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 22:31:45.339256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.19
 ---- batch: 020 ----
mean loss: 163.72
 ---- batch: 030 ----
mean loss: 173.02
 ---- batch: 040 ----
mean loss: 169.32
 ---- batch: 050 ----
mean loss: 170.39
 ---- batch: 060 ----
mean loss: 161.93
 ---- batch: 070 ----
mean loss: 175.39
 ---- batch: 080 ----
mean loss: 170.47
 ---- batch: 090 ----
mean loss: 174.47
 ---- batch: 100 ----
mean loss: 172.61
 ---- batch: 110 ----
mean loss: 167.39
train mean loss: 168.79
epoch train time: 0:00:01.922521
elapsed time: 0:03:22.372200
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 22:31:47.262325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.51
 ---- batch: 020 ----
mean loss: 170.22
 ---- batch: 030 ----
mean loss: 166.67
 ---- batch: 040 ----
mean loss: 172.06
 ---- batch: 050 ----
mean loss: 157.94
 ---- batch: 060 ----
mean loss: 167.73
 ---- batch: 070 ----
mean loss: 174.73
 ---- batch: 080 ----
mean loss: 173.17
 ---- batch: 090 ----
mean loss: 166.46
 ---- batch: 100 ----
mean loss: 166.09
 ---- batch: 110 ----
mean loss: 172.56
train mean loss: 168.56
epoch train time: 0:00:01.865385
elapsed time: 0:03:24.238165
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 22:31:49.128280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.72
 ---- batch: 020 ----
mean loss: 164.73
 ---- batch: 030 ----
mean loss: 162.60
 ---- batch: 040 ----
mean loss: 163.60
 ---- batch: 050 ----
mean loss: 160.15
 ---- batch: 060 ----
mean loss: 175.28
 ---- batch: 070 ----
mean loss: 176.75
 ---- batch: 080 ----
mean loss: 172.30
 ---- batch: 090 ----
mean loss: 165.64
 ---- batch: 100 ----
mean loss: 171.04
 ---- batch: 110 ----
mean loss: 170.98
train mean loss: 168.20
epoch train time: 0:00:01.875439
elapsed time: 0:03:26.114167
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 22:31:51.004288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.95
 ---- batch: 020 ----
mean loss: 169.80
 ---- batch: 030 ----
mean loss: 170.66
 ---- batch: 040 ----
mean loss: 159.47
 ---- batch: 050 ----
mean loss: 167.71
 ---- batch: 060 ----
mean loss: 168.98
 ---- batch: 070 ----
mean loss: 172.67
 ---- batch: 080 ----
mean loss: 165.72
 ---- batch: 090 ----
mean loss: 169.45
 ---- batch: 100 ----
mean loss: 167.75
 ---- batch: 110 ----
mean loss: 167.14
train mean loss: 167.48
epoch train time: 0:00:01.900731
elapsed time: 0:03:28.015485
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 22:31:52.905578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.64
 ---- batch: 020 ----
mean loss: 167.72
 ---- batch: 030 ----
mean loss: 159.94
 ---- batch: 040 ----
mean loss: 173.31
 ---- batch: 050 ----
mean loss: 156.22
 ---- batch: 060 ----
mean loss: 166.57
 ---- batch: 070 ----
mean loss: 173.28
 ---- batch: 080 ----
mean loss: 173.45
 ---- batch: 090 ----
mean loss: 163.18
 ---- batch: 100 ----
mean loss: 164.27
 ---- batch: 110 ----
mean loss: 170.99
train mean loss: 166.71
epoch train time: 0:00:01.910041
elapsed time: 0:03:29.926069
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 22:31:54.816250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.38
 ---- batch: 020 ----
mean loss: 168.41
 ---- batch: 030 ----
mean loss: 164.29
 ---- batch: 040 ----
mean loss: 172.20
 ---- batch: 050 ----
mean loss: 165.77
 ---- batch: 060 ----
mean loss: 155.55
 ---- batch: 070 ----
mean loss: 171.72
 ---- batch: 080 ----
mean loss: 155.33
 ---- batch: 090 ----
mean loss: 165.59
 ---- batch: 100 ----
mean loss: 166.80
 ---- batch: 110 ----
mean loss: 164.46
train mean loss: 165.29
epoch train time: 0:00:01.899417
elapsed time: 0:03:31.826132
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 22:31:56.716284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.45
 ---- batch: 020 ----
mean loss: 170.59
 ---- batch: 030 ----
mean loss: 163.44
 ---- batch: 040 ----
mean loss: 160.05
 ---- batch: 050 ----
mean loss: 162.84
 ---- batch: 060 ----
mean loss: 169.06
 ---- batch: 070 ----
mean loss: 169.11
 ---- batch: 080 ----
mean loss: 168.97
 ---- batch: 090 ----
mean loss: 160.16
 ---- batch: 100 ----
mean loss: 163.86
 ---- batch: 110 ----
mean loss: 167.89
train mean loss: 164.85
epoch train time: 0:00:01.885386
elapsed time: 0:03:33.712143
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 22:31:58.602271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.67
 ---- batch: 020 ----
mean loss: 160.47
 ---- batch: 030 ----
mean loss: 173.56
 ---- batch: 040 ----
mean loss: 162.77
 ---- batch: 050 ----
mean loss: 165.28
 ---- batch: 060 ----
mean loss: 163.46
 ---- batch: 070 ----
mean loss: 163.67
 ---- batch: 080 ----
mean loss: 161.57
 ---- batch: 090 ----
mean loss: 161.02
 ---- batch: 100 ----
mean loss: 162.20
 ---- batch: 110 ----
mean loss: 164.60
train mean loss: 163.77
epoch train time: 0:00:01.899973
elapsed time: 0:03:35.612676
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 22:32:00.502847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.71
 ---- batch: 020 ----
mean loss: 163.01
 ---- batch: 030 ----
mean loss: 161.40
 ---- batch: 040 ----
mean loss: 167.94
 ---- batch: 050 ----
mean loss: 169.06
 ---- batch: 060 ----
mean loss: 157.43
 ---- batch: 070 ----
mean loss: 158.08
 ---- batch: 080 ----
mean loss: 157.27
 ---- batch: 090 ----
mean loss: 164.54
 ---- batch: 100 ----
mean loss: 162.58
 ---- batch: 110 ----
mean loss: 166.53
train mean loss: 163.20
epoch train time: 0:00:01.892693
elapsed time: 0:03:37.505969
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 22:32:02.396135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.92
 ---- batch: 020 ----
mean loss: 155.59
 ---- batch: 030 ----
mean loss: 150.29
 ---- batch: 040 ----
mean loss: 163.78
 ---- batch: 050 ----
mean loss: 172.91
 ---- batch: 060 ----
mean loss: 160.33
 ---- batch: 070 ----
mean loss: 165.44
 ---- batch: 080 ----
mean loss: 166.28
 ---- batch: 090 ----
mean loss: 159.93
 ---- batch: 100 ----
mean loss: 163.00
 ---- batch: 110 ----
mean loss: 168.74
train mean loss: 162.70
epoch train time: 0:00:01.903168
elapsed time: 0:03:39.409782
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 22:32:04.299656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.93
 ---- batch: 020 ----
mean loss: 160.46
 ---- batch: 030 ----
mean loss: 155.94
 ---- batch: 040 ----
mean loss: 160.63
 ---- batch: 050 ----
mean loss: 169.52
 ---- batch: 060 ----
mean loss: 155.08
 ---- batch: 070 ----
mean loss: 156.83
 ---- batch: 080 ----
mean loss: 168.25
 ---- batch: 090 ----
mean loss: 165.08
 ---- batch: 100 ----
mean loss: 161.26
 ---- batch: 110 ----
mean loss: 158.76
train mean loss: 160.53
epoch train time: 0:00:01.954363
elapsed time: 0:03:41.364479
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 22:32:06.254606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.35
 ---- batch: 020 ----
mean loss: 161.81
 ---- batch: 030 ----
mean loss: 160.67
 ---- batch: 040 ----
mean loss: 153.88
 ---- batch: 050 ----
mean loss: 163.66
 ---- batch: 060 ----
mean loss: 169.22
 ---- batch: 070 ----
mean loss: 162.09
 ---- batch: 080 ----
mean loss: 169.27
 ---- batch: 090 ----
mean loss: 156.02
 ---- batch: 100 ----
mean loss: 160.02
 ---- batch: 110 ----
mean loss: 157.72
train mean loss: 160.72
epoch train time: 0:00:01.937370
elapsed time: 0:03:43.302564
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 22:32:08.192716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.29
 ---- batch: 020 ----
mean loss: 163.64
 ---- batch: 030 ----
mean loss: 154.41
 ---- batch: 040 ----
mean loss: 158.98
 ---- batch: 050 ----
mean loss: 154.06
 ---- batch: 060 ----
mean loss: 163.07
 ---- batch: 070 ----
mean loss: 150.77
 ---- batch: 080 ----
mean loss: 148.84
 ---- batch: 090 ----
mean loss: 161.32
 ---- batch: 100 ----
mean loss: 166.35
 ---- batch: 110 ----
mean loss: 167.49
train mean loss: 159.64
epoch train time: 0:00:01.904241
elapsed time: 0:03:45.207392
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 22:32:10.097586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.08
 ---- batch: 020 ----
mean loss: 156.29
 ---- batch: 030 ----
mean loss: 163.15
 ---- batch: 040 ----
mean loss: 155.14
 ---- batch: 050 ----
mean loss: 159.03
 ---- batch: 060 ----
mean loss: 162.71
 ---- batch: 070 ----
mean loss: 158.13
 ---- batch: 080 ----
mean loss: 161.67
 ---- batch: 090 ----
mean loss: 157.90
 ---- batch: 100 ----
mean loss: 167.38
 ---- batch: 110 ----
mean loss: 159.62
train mean loss: 159.47
epoch train time: 0:00:01.897277
elapsed time: 0:03:47.105305
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 22:32:11.995455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.29
 ---- batch: 020 ----
mean loss: 157.06
 ---- batch: 030 ----
mean loss: 158.69
 ---- batch: 040 ----
mean loss: 154.62
 ---- batch: 050 ----
mean loss: 163.85
 ---- batch: 060 ----
mean loss: 154.25
 ---- batch: 070 ----
mean loss: 158.51
 ---- batch: 080 ----
mean loss: 161.28
 ---- batch: 090 ----
mean loss: 159.98
 ---- batch: 100 ----
mean loss: 151.27
 ---- batch: 110 ----
mean loss: 159.81
train mean loss: 158.39
epoch train time: 0:00:01.874934
elapsed time: 0:03:48.980828
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 22:32:13.870925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.43
 ---- batch: 020 ----
mean loss: 159.41
 ---- batch: 030 ----
mean loss: 155.14
 ---- batch: 040 ----
mean loss: 159.62
 ---- batch: 050 ----
mean loss: 153.25
 ---- batch: 060 ----
mean loss: 159.25
 ---- batch: 070 ----
mean loss: 152.35
 ---- batch: 080 ----
mean loss: 161.31
 ---- batch: 090 ----
mean loss: 151.25
 ---- batch: 100 ----
mean loss: 159.19
 ---- batch: 110 ----
mean loss: 157.00
train mean loss: 157.47
epoch train time: 0:00:01.893166
elapsed time: 0:03:50.874541
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 22:32:15.764692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.51
 ---- batch: 020 ----
mean loss: 153.51
 ---- batch: 030 ----
mean loss: 158.49
 ---- batch: 040 ----
mean loss: 158.86
 ---- batch: 050 ----
mean loss: 151.68
 ---- batch: 060 ----
mean loss: 160.84
 ---- batch: 070 ----
mean loss: 150.16
 ---- batch: 080 ----
mean loss: 155.41
 ---- batch: 090 ----
mean loss: 153.35
 ---- batch: 100 ----
mean loss: 163.83
 ---- batch: 110 ----
mean loss: 160.67
train mean loss: 157.36
epoch train time: 0:00:01.876593
elapsed time: 0:03:52.751753
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 22:32:17.641977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.90
 ---- batch: 020 ----
mean loss: 154.77
 ---- batch: 030 ----
mean loss: 159.86
 ---- batch: 040 ----
mean loss: 155.43
 ---- batch: 050 ----
mean loss: 154.68
 ---- batch: 060 ----
mean loss: 155.23
 ---- batch: 070 ----
mean loss: 157.25
 ---- batch: 080 ----
mean loss: 151.08
 ---- batch: 090 ----
mean loss: 160.13
 ---- batch: 100 ----
mean loss: 162.23
 ---- batch: 110 ----
mean loss: 165.66
train mean loss: 156.33
epoch train time: 0:00:01.926827
elapsed time: 0:03:54.679273
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 22:32:19.569454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.25
 ---- batch: 020 ----
mean loss: 149.91
 ---- batch: 030 ----
mean loss: 156.47
 ---- batch: 040 ----
mean loss: 149.17
 ---- batch: 050 ----
mean loss: 148.22
 ---- batch: 060 ----
mean loss: 163.14
 ---- batch: 070 ----
mean loss: 160.33
 ---- batch: 080 ----
mean loss: 152.13
 ---- batch: 090 ----
mean loss: 155.97
 ---- batch: 100 ----
mean loss: 163.79
 ---- batch: 110 ----
mean loss: 152.80
train mean loss: 156.04
epoch train time: 0:00:01.903154
elapsed time: 0:03:56.583307
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 22:32:21.473242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.28
 ---- batch: 020 ----
mean loss: 162.06
 ---- batch: 030 ----
mean loss: 152.83
 ---- batch: 040 ----
mean loss: 153.39
 ---- batch: 050 ----
mean loss: 154.79
 ---- batch: 060 ----
mean loss: 153.36
 ---- batch: 070 ----
mean loss: 149.61
 ---- batch: 080 ----
mean loss: 162.82
 ---- batch: 090 ----
mean loss: 161.78
 ---- batch: 100 ----
mean loss: 152.30
 ---- batch: 110 ----
mean loss: 157.56
train mean loss: 155.66
epoch train time: 0:00:01.923832
elapsed time: 0:03:58.507523
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 22:32:23.397713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.30
 ---- batch: 020 ----
mean loss: 159.32
 ---- batch: 030 ----
mean loss: 155.65
 ---- batch: 040 ----
mean loss: 151.21
 ---- batch: 050 ----
mean loss: 155.68
 ---- batch: 060 ----
mean loss: 151.33
 ---- batch: 070 ----
mean loss: 163.97
 ---- batch: 080 ----
mean loss: 159.91
 ---- batch: 090 ----
mean loss: 160.06
 ---- batch: 100 ----
mean loss: 144.78
 ---- batch: 110 ----
mean loss: 157.16
train mean loss: 155.78
epoch train time: 0:00:01.915517
elapsed time: 0:04:00.423666
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 22:32:25.313838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.09
 ---- batch: 020 ----
mean loss: 155.09
 ---- batch: 030 ----
mean loss: 144.92
 ---- batch: 040 ----
mean loss: 155.26
 ---- batch: 050 ----
mean loss: 156.18
 ---- batch: 060 ----
mean loss: 152.24
 ---- batch: 070 ----
mean loss: 156.04
 ---- batch: 080 ----
mean loss: 152.99
 ---- batch: 090 ----
mean loss: 158.92
 ---- batch: 100 ----
mean loss: 161.72
 ---- batch: 110 ----
mean loss: 151.78
train mean loss: 154.28
epoch train time: 0:00:01.900952
elapsed time: 0:04:02.325300
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 22:32:27.215502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.98
 ---- batch: 020 ----
mean loss: 142.13
 ---- batch: 030 ----
mean loss: 153.63
 ---- batch: 040 ----
mean loss: 150.04
 ---- batch: 050 ----
mean loss: 156.82
 ---- batch: 060 ----
mean loss: 153.25
 ---- batch: 070 ----
mean loss: 163.92
 ---- batch: 080 ----
mean loss: 156.95
 ---- batch: 090 ----
mean loss: 147.77
 ---- batch: 100 ----
mean loss: 157.46
 ---- batch: 110 ----
mean loss: 158.13
train mean loss: 153.66
epoch train time: 0:00:01.891201
elapsed time: 0:04:04.217165
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 22:32:29.107379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.62
 ---- batch: 020 ----
mean loss: 151.31
 ---- batch: 030 ----
mean loss: 147.71
 ---- batch: 040 ----
mean loss: 145.81
 ---- batch: 050 ----
mean loss: 156.53
 ---- batch: 060 ----
mean loss: 149.18
 ---- batch: 070 ----
mean loss: 153.02
 ---- batch: 080 ----
mean loss: 160.07
 ---- batch: 090 ----
mean loss: 148.07
 ---- batch: 100 ----
mean loss: 149.08
 ---- batch: 110 ----
mean loss: 158.31
train mean loss: 152.59
epoch train time: 0:00:01.925843
elapsed time: 0:04:06.143681
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 22:32:31.033824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.06
 ---- batch: 020 ----
mean loss: 145.66
 ---- batch: 030 ----
mean loss: 145.15
 ---- batch: 040 ----
mean loss: 151.20
 ---- batch: 050 ----
mean loss: 151.84
 ---- batch: 060 ----
mean loss: 155.16
 ---- batch: 070 ----
mean loss: 155.91
 ---- batch: 080 ----
mean loss: 159.38
 ---- batch: 090 ----
mean loss: 161.43
 ---- batch: 100 ----
mean loss: 148.61
 ---- batch: 110 ----
mean loss: 152.44
train mean loss: 152.42
epoch train time: 0:00:01.940193
elapsed time: 0:04:08.084456
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 22:32:32.974603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.10
 ---- batch: 020 ----
mean loss: 143.25
 ---- batch: 030 ----
mean loss: 148.86
 ---- batch: 040 ----
mean loss: 142.04
 ---- batch: 050 ----
mean loss: 151.11
 ---- batch: 060 ----
mean loss: 153.04
 ---- batch: 070 ----
mean loss: 143.46
 ---- batch: 080 ----
mean loss: 158.54
 ---- batch: 090 ----
mean loss: 155.92
 ---- batch: 100 ----
mean loss: 158.07
 ---- batch: 110 ----
mean loss: 161.62
train mean loss: 151.79
epoch train time: 0:00:01.913410
elapsed time: 0:04:09.998468
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 22:32:34.888613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.50
 ---- batch: 020 ----
mean loss: 160.31
 ---- batch: 030 ----
mean loss: 148.00
 ---- batch: 040 ----
mean loss: 152.95
 ---- batch: 050 ----
mean loss: 154.60
 ---- batch: 060 ----
mean loss: 161.52
 ---- batch: 070 ----
mean loss: 149.44
 ---- batch: 080 ----
mean loss: 148.10
 ---- batch: 090 ----
mean loss: 150.34
 ---- batch: 100 ----
mean loss: 152.20
 ---- batch: 110 ----
mean loss: 147.20
train mean loss: 152.00
epoch train time: 0:00:01.912182
elapsed time: 0:04:11.911296
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 22:32:36.801472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.71
 ---- batch: 020 ----
mean loss: 155.29
 ---- batch: 030 ----
mean loss: 150.62
 ---- batch: 040 ----
mean loss: 147.17
 ---- batch: 050 ----
mean loss: 151.77
 ---- batch: 060 ----
mean loss: 150.14
 ---- batch: 070 ----
mean loss: 151.53
 ---- batch: 080 ----
mean loss: 154.36
 ---- batch: 090 ----
mean loss: 150.69
 ---- batch: 100 ----
mean loss: 141.47
 ---- batch: 110 ----
mean loss: 142.68
train mean loss: 150.14
epoch train time: 0:00:01.907085
elapsed time: 0:04:13.819042
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 22:32:38.709174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.65
 ---- batch: 020 ----
mean loss: 151.12
 ---- batch: 030 ----
mean loss: 149.16
 ---- batch: 040 ----
mean loss: 147.92
 ---- batch: 050 ----
mean loss: 151.45
 ---- batch: 060 ----
mean loss: 153.12
 ---- batch: 070 ----
mean loss: 156.15
 ---- batch: 080 ----
mean loss: 154.87
 ---- batch: 090 ----
mean loss: 147.17
 ---- batch: 100 ----
mean loss: 156.26
 ---- batch: 110 ----
mean loss: 147.81
train mean loss: 150.95
epoch train time: 0:00:01.904705
elapsed time: 0:04:15.724375
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 22:32:40.614529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.36
 ---- batch: 020 ----
mean loss: 151.19
 ---- batch: 030 ----
mean loss: 146.13
 ---- batch: 040 ----
mean loss: 143.02
 ---- batch: 050 ----
mean loss: 149.28
 ---- batch: 060 ----
mean loss: 150.53
 ---- batch: 070 ----
mean loss: 152.90
 ---- batch: 080 ----
mean loss: 158.21
 ---- batch: 090 ----
mean loss: 152.85
 ---- batch: 100 ----
mean loss: 151.91
 ---- batch: 110 ----
mean loss: 149.09
train mean loss: 150.70
epoch train time: 0:00:01.910014
elapsed time: 0:04:17.634992
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 22:32:42.525139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.63
 ---- batch: 020 ----
mean loss: 144.48
 ---- batch: 030 ----
mean loss: 143.32
 ---- batch: 040 ----
mean loss: 151.21
 ---- batch: 050 ----
mean loss: 148.60
 ---- batch: 060 ----
mean loss: 148.15
 ---- batch: 070 ----
mean loss: 149.60
 ---- batch: 080 ----
mean loss: 154.12
 ---- batch: 090 ----
mean loss: 153.37
 ---- batch: 100 ----
mean loss: 152.68
 ---- batch: 110 ----
mean loss: 152.90
train mean loss: 149.63
epoch train time: 0:00:01.888520
elapsed time: 0:04:19.524112
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 22:32:44.414287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.24
 ---- batch: 020 ----
mean loss: 143.88
 ---- batch: 030 ----
mean loss: 142.45
 ---- batch: 040 ----
mean loss: 148.21
 ---- batch: 050 ----
mean loss: 148.16
 ---- batch: 060 ----
mean loss: 152.25
 ---- batch: 070 ----
mean loss: 149.17
 ---- batch: 080 ----
mean loss: 151.99
 ---- batch: 090 ----
mean loss: 156.65
 ---- batch: 100 ----
mean loss: 151.49
 ---- batch: 110 ----
mean loss: 148.95
train mean loss: 149.20
epoch train time: 0:00:01.915208
elapsed time: 0:04:21.439995
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 22:32:46.330139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.50
 ---- batch: 020 ----
mean loss: 136.92
 ---- batch: 030 ----
mean loss: 152.51
 ---- batch: 040 ----
mean loss: 147.01
 ---- batch: 050 ----
mean loss: 145.23
 ---- batch: 060 ----
mean loss: 148.65
 ---- batch: 070 ----
mean loss: 155.72
 ---- batch: 080 ----
mean loss: 141.85
 ---- batch: 090 ----
mean loss: 157.29
 ---- batch: 100 ----
mean loss: 148.49
 ---- batch: 110 ----
mean loss: 147.19
train mean loss: 148.09
epoch train time: 0:00:01.906787
elapsed time: 0:04:23.347356
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 22:32:48.237544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.81
 ---- batch: 020 ----
mean loss: 141.32
 ---- batch: 030 ----
mean loss: 144.59
 ---- batch: 040 ----
mean loss: 146.69
 ---- batch: 050 ----
mean loss: 135.69
 ---- batch: 060 ----
mean loss: 150.30
 ---- batch: 070 ----
mean loss: 153.50
 ---- batch: 080 ----
mean loss: 152.59
 ---- batch: 090 ----
mean loss: 148.82
 ---- batch: 100 ----
mean loss: 152.01
 ---- batch: 110 ----
mean loss: 150.91
train mean loss: 147.81
epoch train time: 0:00:01.904686
elapsed time: 0:04:25.252663
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 22:32:50.142774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.02
 ---- batch: 020 ----
mean loss: 152.85
 ---- batch: 030 ----
mean loss: 143.25
 ---- batch: 040 ----
mean loss: 146.51
 ---- batch: 050 ----
mean loss: 151.45
 ---- batch: 060 ----
mean loss: 149.40
 ---- batch: 070 ----
mean loss: 153.52
 ---- batch: 080 ----
mean loss: 147.60
 ---- batch: 090 ----
mean loss: 148.42
 ---- batch: 100 ----
mean loss: 148.75
 ---- batch: 110 ----
mean loss: 140.98
train mean loss: 147.83
epoch train time: 0:00:01.927835
elapsed time: 0:04:27.181037
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 22:32:52.071180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.95
 ---- batch: 020 ----
mean loss: 138.65
 ---- batch: 030 ----
mean loss: 145.16
 ---- batch: 040 ----
mean loss: 149.25
 ---- batch: 050 ----
mean loss: 145.01
 ---- batch: 060 ----
mean loss: 149.41
 ---- batch: 070 ----
mean loss: 146.64
 ---- batch: 080 ----
mean loss: 151.00
 ---- batch: 090 ----
mean loss: 151.31
 ---- batch: 100 ----
mean loss: 152.73
 ---- batch: 110 ----
mean loss: 140.32
train mean loss: 146.79
epoch train time: 0:00:01.909625
elapsed time: 0:04:29.091250
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 22:32:53.981374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.26
 ---- batch: 020 ----
mean loss: 138.81
 ---- batch: 030 ----
mean loss: 143.40
 ---- batch: 040 ----
mean loss: 147.11
 ---- batch: 050 ----
mean loss: 150.45
 ---- batch: 060 ----
mean loss: 156.61
 ---- batch: 070 ----
mean loss: 156.05
 ---- batch: 080 ----
mean loss: 147.02
 ---- batch: 090 ----
mean loss: 148.80
 ---- batch: 100 ----
mean loss: 144.81
 ---- batch: 110 ----
mean loss: 144.22
train mean loss: 147.37
epoch train time: 0:00:01.926927
elapsed time: 0:04:31.018798
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 22:32:55.909028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.97
 ---- batch: 020 ----
mean loss: 141.07
 ---- batch: 030 ----
mean loss: 144.63
 ---- batch: 040 ----
mean loss: 148.29
 ---- batch: 050 ----
mean loss: 136.25
 ---- batch: 060 ----
mean loss: 146.88
 ---- batch: 070 ----
mean loss: 153.30
 ---- batch: 080 ----
mean loss: 152.48
 ---- batch: 090 ----
mean loss: 144.80
 ---- batch: 100 ----
mean loss: 152.62
 ---- batch: 110 ----
mean loss: 151.25
train mean loss: 146.76
epoch train time: 0:00:01.911111
elapsed time: 0:04:32.930614
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 22:32:57.820747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.60
 ---- batch: 020 ----
mean loss: 138.75
 ---- batch: 030 ----
mean loss: 145.86
 ---- batch: 040 ----
mean loss: 146.96
 ---- batch: 050 ----
mean loss: 146.02
 ---- batch: 060 ----
mean loss: 141.04
 ---- batch: 070 ----
mean loss: 146.98
 ---- batch: 080 ----
mean loss: 154.63
 ---- batch: 090 ----
mean loss: 151.04
 ---- batch: 100 ----
mean loss: 143.12
 ---- batch: 110 ----
mean loss: 143.88
train mean loss: 145.97
epoch train time: 0:00:01.884243
elapsed time: 0:04:34.815680
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 22:32:59.705570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.38
 ---- batch: 020 ----
mean loss: 144.50
 ---- batch: 030 ----
mean loss: 139.72
 ---- batch: 040 ----
mean loss: 127.98
 ---- batch: 050 ----
mean loss: 156.22
 ---- batch: 060 ----
mean loss: 146.91
 ---- batch: 070 ----
mean loss: 145.42
 ---- batch: 080 ----
mean loss: 149.46
 ---- batch: 090 ----
mean loss: 148.43
 ---- batch: 100 ----
mean loss: 143.55
 ---- batch: 110 ----
mean loss: 151.99
train mean loss: 145.38
epoch train time: 0:00:01.900874
elapsed time: 0:04:36.716871
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 22:33:01.606990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.23
 ---- batch: 020 ----
mean loss: 135.13
 ---- batch: 030 ----
mean loss: 148.88
 ---- batch: 040 ----
mean loss: 142.00
 ---- batch: 050 ----
mean loss: 146.62
 ---- batch: 060 ----
mean loss: 146.00
 ---- batch: 070 ----
mean loss: 145.20
 ---- batch: 080 ----
mean loss: 149.09
 ---- batch: 090 ----
mean loss: 139.12
 ---- batch: 100 ----
mean loss: 154.22
 ---- batch: 110 ----
mean loss: 139.30
train mean loss: 145.05
epoch train time: 0:00:01.911713
elapsed time: 0:04:38.629136
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 22:33:03.519273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.41
 ---- batch: 020 ----
mean loss: 146.88
 ---- batch: 030 ----
mean loss: 143.80
 ---- batch: 040 ----
mean loss: 145.63
 ---- batch: 050 ----
mean loss: 145.63
 ---- batch: 060 ----
mean loss: 144.85
 ---- batch: 070 ----
mean loss: 136.18
 ---- batch: 080 ----
mean loss: 142.59
 ---- batch: 090 ----
mean loss: 143.71
 ---- batch: 100 ----
mean loss: 146.51
 ---- batch: 110 ----
mean loss: 148.94
train mean loss: 144.27
epoch train time: 0:00:01.909842
elapsed time: 0:04:40.539539
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 22:33:05.429683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.39
 ---- batch: 020 ----
mean loss: 138.50
 ---- batch: 030 ----
mean loss: 143.33
 ---- batch: 040 ----
mean loss: 147.94
 ---- batch: 050 ----
mean loss: 140.08
 ---- batch: 060 ----
mean loss: 141.53
 ---- batch: 070 ----
mean loss: 147.07
 ---- batch: 080 ----
mean loss: 149.36
 ---- batch: 090 ----
mean loss: 147.30
 ---- batch: 100 ----
mean loss: 137.76
 ---- batch: 110 ----
mean loss: 148.29
train mean loss: 144.08
epoch train time: 0:00:01.907017
elapsed time: 0:04:42.447131
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 22:33:07.337247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.95
 ---- batch: 020 ----
mean loss: 144.69
 ---- batch: 030 ----
mean loss: 141.20
 ---- batch: 040 ----
mean loss: 145.75
 ---- batch: 050 ----
mean loss: 141.29
 ---- batch: 060 ----
mean loss: 145.45
 ---- batch: 070 ----
mean loss: 142.90
 ---- batch: 080 ----
mean loss: 142.12
 ---- batch: 090 ----
mean loss: 138.50
 ---- batch: 100 ----
mean loss: 147.87
 ---- batch: 110 ----
mean loss: 146.54
train mean loss: 143.59
epoch train time: 0:00:01.915829
elapsed time: 0:04:44.363501
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 22:33:09.253688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.36
 ---- batch: 020 ----
mean loss: 142.80
 ---- batch: 030 ----
mean loss: 141.30
 ---- batch: 040 ----
mean loss: 145.83
 ---- batch: 050 ----
mean loss: 144.74
 ---- batch: 060 ----
mean loss: 148.29
 ---- batch: 070 ----
mean loss: 150.99
 ---- batch: 080 ----
mean loss: 134.62
 ---- batch: 090 ----
mean loss: 136.09
 ---- batch: 100 ----
mean loss: 141.54
 ---- batch: 110 ----
mean loss: 141.15
train mean loss: 143.03
epoch train time: 0:00:01.894212
elapsed time: 0:04:46.258328
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 22:33:11.148474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.11
 ---- batch: 020 ----
mean loss: 143.27
 ---- batch: 030 ----
mean loss: 139.47
 ---- batch: 040 ----
mean loss: 137.09
 ---- batch: 050 ----
mean loss: 145.08
 ---- batch: 060 ----
mean loss: 136.43
 ---- batch: 070 ----
mean loss: 144.16
 ---- batch: 080 ----
mean loss: 141.89
 ---- batch: 090 ----
mean loss: 150.42
 ---- batch: 100 ----
mean loss: 138.05
 ---- batch: 110 ----
mean loss: 146.41
train mean loss: 143.16
epoch train time: 0:00:01.926531
elapsed time: 0:04:48.185484
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 22:33:13.075621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.11
 ---- batch: 020 ----
mean loss: 144.27
 ---- batch: 030 ----
mean loss: 146.79
 ---- batch: 040 ----
mean loss: 144.06
 ---- batch: 050 ----
mean loss: 140.77
 ---- batch: 060 ----
mean loss: 141.85
 ---- batch: 070 ----
mean loss: 142.79
 ---- batch: 080 ----
mean loss: 136.90
 ---- batch: 090 ----
mean loss: 142.15
 ---- batch: 100 ----
mean loss: 151.94
 ---- batch: 110 ----
mean loss: 143.19
train mean loss: 142.95
epoch train time: 0:00:01.921906
elapsed time: 0:04:50.108037
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 22:33:14.998207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.17
 ---- batch: 020 ----
mean loss: 145.64
 ---- batch: 030 ----
mean loss: 145.10
 ---- batch: 040 ----
mean loss: 145.24
 ---- batch: 050 ----
mean loss: 136.09
 ---- batch: 060 ----
mean loss: 139.65
 ---- batch: 070 ----
mean loss: 142.11
 ---- batch: 080 ----
mean loss: 145.09
 ---- batch: 090 ----
mean loss: 145.81
 ---- batch: 100 ----
mean loss: 137.85
 ---- batch: 110 ----
mean loss: 147.18
train mean loss: 142.63
epoch train time: 0:00:01.883276
elapsed time: 0:04:51.991948
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 22:33:16.882069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.10
 ---- batch: 020 ----
mean loss: 143.41
 ---- batch: 030 ----
mean loss: 143.94
 ---- batch: 040 ----
mean loss: 138.67
 ---- batch: 050 ----
mean loss: 134.95
 ---- batch: 060 ----
mean loss: 142.34
 ---- batch: 070 ----
mean loss: 146.39
 ---- batch: 080 ----
mean loss: 140.11
 ---- batch: 090 ----
mean loss: 143.35
 ---- batch: 100 ----
mean loss: 137.14
 ---- batch: 110 ----
mean loss: 138.01
train mean loss: 141.81
epoch train time: 0:00:01.893632
elapsed time: 0:04:53.886174
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 22:33:18.776325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.41
 ---- batch: 020 ----
mean loss: 140.53
 ---- batch: 030 ----
mean loss: 138.27
 ---- batch: 040 ----
mean loss: 139.18
 ---- batch: 050 ----
mean loss: 140.73
 ---- batch: 060 ----
mean loss: 146.89
 ---- batch: 070 ----
mean loss: 136.25
 ---- batch: 080 ----
mean loss: 139.66
 ---- batch: 090 ----
mean loss: 133.72
 ---- batch: 100 ----
mean loss: 153.16
 ---- batch: 110 ----
mean loss: 151.60
train mean loss: 140.98
epoch train time: 0:00:01.925939
elapsed time: 0:04:55.812836
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 22:33:20.703038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.39
 ---- batch: 020 ----
mean loss: 141.76
 ---- batch: 030 ----
mean loss: 130.66
 ---- batch: 040 ----
mean loss: 147.43
 ---- batch: 050 ----
mean loss: 136.67
 ---- batch: 060 ----
mean loss: 145.34
 ---- batch: 070 ----
mean loss: 136.33
 ---- batch: 080 ----
mean loss: 140.33
 ---- batch: 090 ----
mean loss: 140.58
 ---- batch: 100 ----
mean loss: 148.50
 ---- batch: 110 ----
mean loss: 141.00
train mean loss: 141.06
epoch train time: 0:00:01.891181
elapsed time: 0:04:57.704666
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 22:33:22.594774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.86
 ---- batch: 020 ----
mean loss: 139.81
 ---- batch: 030 ----
mean loss: 144.21
 ---- batch: 040 ----
mean loss: 145.23
 ---- batch: 050 ----
mean loss: 145.76
 ---- batch: 060 ----
mean loss: 146.56
 ---- batch: 070 ----
mean loss: 140.11
 ---- batch: 080 ----
mean loss: 134.83
 ---- batch: 090 ----
mean loss: 142.72
 ---- batch: 100 ----
mean loss: 131.06
 ---- batch: 110 ----
mean loss: 146.03
train mean loss: 140.84
epoch train time: 0:00:01.914029
elapsed time: 0:04:59.619233
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 22:33:24.509370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.40
 ---- batch: 020 ----
mean loss: 133.96
 ---- batch: 030 ----
mean loss: 144.15
 ---- batch: 040 ----
mean loss: 142.14
 ---- batch: 050 ----
mean loss: 144.80
 ---- batch: 060 ----
mean loss: 131.22
 ---- batch: 070 ----
mean loss: 139.40
 ---- batch: 080 ----
mean loss: 144.15
 ---- batch: 090 ----
mean loss: 151.36
 ---- batch: 100 ----
mean loss: 138.30
 ---- batch: 110 ----
mean loss: 139.98
train mean loss: 140.29
epoch train time: 0:00:01.864119
elapsed time: 0:05:01.483968
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 22:33:26.374100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.44
 ---- batch: 020 ----
mean loss: 140.47
 ---- batch: 030 ----
mean loss: 139.93
 ---- batch: 040 ----
mean loss: 137.42
 ---- batch: 050 ----
mean loss: 141.35
 ---- batch: 060 ----
mean loss: 140.30
 ---- batch: 070 ----
mean loss: 136.49
 ---- batch: 080 ----
mean loss: 143.67
 ---- batch: 090 ----
mean loss: 143.30
 ---- batch: 100 ----
mean loss: 134.34
 ---- batch: 110 ----
mean loss: 137.41
train mean loss: 139.69
epoch train time: 0:00:01.925323
elapsed time: 0:05:03.409900
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 22:33:28.300081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.79
 ---- batch: 020 ----
mean loss: 142.58
 ---- batch: 030 ----
mean loss: 136.91
 ---- batch: 040 ----
mean loss: 138.58
 ---- batch: 050 ----
mean loss: 132.82
 ---- batch: 060 ----
mean loss: 138.23
 ---- batch: 070 ----
mean loss: 135.08
 ---- batch: 080 ----
mean loss: 144.23
 ---- batch: 090 ----
mean loss: 138.93
 ---- batch: 100 ----
mean loss: 148.48
 ---- batch: 110 ----
mean loss: 147.51
train mean loss: 139.76
epoch train time: 0:00:01.896651
elapsed time: 0:05:05.307171
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 22:33:30.197379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.83
 ---- batch: 020 ----
mean loss: 134.35
 ---- batch: 030 ----
mean loss: 132.93
 ---- batch: 040 ----
mean loss: 139.76
 ---- batch: 050 ----
mean loss: 143.41
 ---- batch: 060 ----
mean loss: 135.77
 ---- batch: 070 ----
mean loss: 139.05
 ---- batch: 080 ----
mean loss: 143.31
 ---- batch: 090 ----
mean loss: 151.24
 ---- batch: 100 ----
mean loss: 142.28
 ---- batch: 110 ----
mean loss: 133.26
train mean loss: 139.16
epoch train time: 0:00:01.885899
elapsed time: 0:05:07.193711
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 22:33:32.083835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.70
 ---- batch: 020 ----
mean loss: 133.95
 ---- batch: 030 ----
mean loss: 140.36
 ---- batch: 040 ----
mean loss: 141.49
 ---- batch: 050 ----
mean loss: 136.80
 ---- batch: 060 ----
mean loss: 141.48
 ---- batch: 070 ----
mean loss: 136.08
 ---- batch: 080 ----
mean loss: 141.90
 ---- batch: 090 ----
mean loss: 141.75
 ---- batch: 100 ----
mean loss: 140.40
 ---- batch: 110 ----
mean loss: 139.58
train mean loss: 139.23
epoch train time: 0:00:01.917402
elapsed time: 0:05:09.111684
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 22:33:34.001829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.19
 ---- batch: 020 ----
mean loss: 142.19
 ---- batch: 030 ----
mean loss: 142.98
 ---- batch: 040 ----
mean loss: 141.77
 ---- batch: 050 ----
mean loss: 138.67
 ---- batch: 060 ----
mean loss: 142.52
 ---- batch: 070 ----
mean loss: 144.20
 ---- batch: 080 ----
mean loss: 139.09
 ---- batch: 090 ----
mean loss: 139.64
 ---- batch: 100 ----
mean loss: 138.32
 ---- batch: 110 ----
mean loss: 136.36
train mean loss: 138.96
epoch train time: 0:00:01.914754
elapsed time: 0:05:11.027035
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 22:33:35.917149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.42
 ---- batch: 020 ----
mean loss: 137.06
 ---- batch: 030 ----
mean loss: 134.70
 ---- batch: 040 ----
mean loss: 139.44
 ---- batch: 050 ----
mean loss: 134.15
 ---- batch: 060 ----
mean loss: 137.07
 ---- batch: 070 ----
mean loss: 140.28
 ---- batch: 080 ----
mean loss: 134.29
 ---- batch: 090 ----
mean loss: 138.19
 ---- batch: 100 ----
mean loss: 139.04
 ---- batch: 110 ----
mean loss: 141.58
train mean loss: 137.67
epoch train time: 0:00:01.862923
elapsed time: 0:05:12.890514
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 22:33:37.780637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.55
 ---- batch: 020 ----
mean loss: 140.32
 ---- batch: 030 ----
mean loss: 133.43
 ---- batch: 040 ----
mean loss: 135.52
 ---- batch: 050 ----
mean loss: 139.10
 ---- batch: 060 ----
mean loss: 137.18
 ---- batch: 070 ----
mean loss: 136.60
 ---- batch: 080 ----
mean loss: 142.14
 ---- batch: 090 ----
mean loss: 140.50
 ---- batch: 100 ----
mean loss: 142.69
 ---- batch: 110 ----
mean loss: 134.90
train mean loss: 138.20
epoch train time: 0:00:01.897317
elapsed time: 0:05:14.788412
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 22:33:39.678583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.96
 ---- batch: 020 ----
mean loss: 139.07
 ---- batch: 030 ----
mean loss: 141.63
 ---- batch: 040 ----
mean loss: 131.19
 ---- batch: 050 ----
mean loss: 136.26
 ---- batch: 060 ----
mean loss: 131.19
 ---- batch: 070 ----
mean loss: 135.24
 ---- batch: 080 ----
mean loss: 143.57
 ---- batch: 090 ----
mean loss: 139.22
 ---- batch: 100 ----
mean loss: 144.72
 ---- batch: 110 ----
mean loss: 135.37
train mean loss: 137.21
epoch train time: 0:00:01.883203
elapsed time: 0:05:16.672484
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 22:33:41.562365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.27
 ---- batch: 020 ----
mean loss: 133.36
 ---- batch: 030 ----
mean loss: 130.83
 ---- batch: 040 ----
mean loss: 125.24
 ---- batch: 050 ----
mean loss: 128.63
 ---- batch: 060 ----
mean loss: 135.06
 ---- batch: 070 ----
mean loss: 143.15
 ---- batch: 080 ----
mean loss: 143.60
 ---- batch: 090 ----
mean loss: 142.86
 ---- batch: 100 ----
mean loss: 142.81
 ---- batch: 110 ----
mean loss: 142.61
train mean loss: 137.18
epoch train time: 0:00:01.917267
elapsed time: 0:05:18.590062
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 22:33:43.480179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.96
 ---- batch: 020 ----
mean loss: 129.21
 ---- batch: 030 ----
mean loss: 136.77
 ---- batch: 040 ----
mean loss: 135.81
 ---- batch: 050 ----
mean loss: 136.18
 ---- batch: 060 ----
mean loss: 144.12
 ---- batch: 070 ----
mean loss: 134.19
 ---- batch: 080 ----
mean loss: 142.51
 ---- batch: 090 ----
mean loss: 142.51
 ---- batch: 100 ----
mean loss: 131.62
 ---- batch: 110 ----
mean loss: 135.24
train mean loss: 136.73
epoch train time: 0:00:01.936218
elapsed time: 0:05:20.526844
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 22:33:45.416689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.17
 ---- batch: 020 ----
mean loss: 131.71
 ---- batch: 030 ----
mean loss: 142.88
 ---- batch: 040 ----
mean loss: 133.98
 ---- batch: 050 ----
mean loss: 130.24
 ---- batch: 060 ----
mean loss: 139.63
 ---- batch: 070 ----
mean loss: 138.75
 ---- batch: 080 ----
mean loss: 137.14
 ---- batch: 090 ----
mean loss: 136.34
 ---- batch: 100 ----
mean loss: 137.95
 ---- batch: 110 ----
mean loss: 133.65
train mean loss: 136.52
epoch train time: 0:00:01.909889
elapsed time: 0:05:22.437021
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 22:33:47.327155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.46
 ---- batch: 020 ----
mean loss: 131.75
 ---- batch: 030 ----
mean loss: 128.93
 ---- batch: 040 ----
mean loss: 131.58
 ---- batch: 050 ----
mean loss: 136.00
 ---- batch: 060 ----
mean loss: 134.94
 ---- batch: 070 ----
mean loss: 132.54
 ---- batch: 080 ----
mean loss: 144.34
 ---- batch: 090 ----
mean loss: 139.95
 ---- batch: 100 ----
mean loss: 137.99
 ---- batch: 110 ----
mean loss: 133.64
train mean loss: 135.69
epoch train time: 0:00:01.906403
elapsed time: 0:05:24.344009
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 22:33:49.234116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.60
 ---- batch: 020 ----
mean loss: 125.57
 ---- batch: 030 ----
mean loss: 131.29
 ---- batch: 040 ----
mean loss: 134.30
 ---- batch: 050 ----
mean loss: 141.58
 ---- batch: 060 ----
mean loss: 142.51
 ---- batch: 070 ----
mean loss: 142.81
 ---- batch: 080 ----
mean loss: 135.52
 ---- batch: 090 ----
mean loss: 130.70
 ---- batch: 100 ----
mean loss: 136.83
 ---- batch: 110 ----
mean loss: 135.52
train mean loss: 136.21
epoch train time: 0:00:01.905973
elapsed time: 0:05:26.250534
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 22:33:51.140677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.55
 ---- batch: 020 ----
mean loss: 129.24
 ---- batch: 030 ----
mean loss: 133.05
 ---- batch: 040 ----
mean loss: 137.73
 ---- batch: 050 ----
mean loss: 137.90
 ---- batch: 060 ----
mean loss: 136.99
 ---- batch: 070 ----
mean loss: 134.13
 ---- batch: 080 ----
mean loss: 134.42
 ---- batch: 090 ----
mean loss: 142.73
 ---- batch: 100 ----
mean loss: 130.57
 ---- batch: 110 ----
mean loss: 141.70
train mean loss: 134.94
epoch train time: 0:00:01.892180
elapsed time: 0:05:28.143319
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 22:33:53.033480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.03
 ---- batch: 020 ----
mean loss: 129.01
 ---- batch: 030 ----
mean loss: 138.91
 ---- batch: 040 ----
mean loss: 136.86
 ---- batch: 050 ----
mean loss: 129.44
 ---- batch: 060 ----
mean loss: 131.11
 ---- batch: 070 ----
mean loss: 143.24
 ---- batch: 080 ----
mean loss: 137.23
 ---- batch: 090 ----
mean loss: 137.34
 ---- batch: 100 ----
mean loss: 136.19
 ---- batch: 110 ----
mean loss: 139.75
train mean loss: 135.87
epoch train time: 0:00:01.897573
elapsed time: 0:05:30.041535
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 22:33:54.931672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.58
 ---- batch: 020 ----
mean loss: 128.48
 ---- batch: 030 ----
mean loss: 139.93
 ---- batch: 040 ----
mean loss: 144.11
 ---- batch: 050 ----
mean loss: 135.55
 ---- batch: 060 ----
mean loss: 133.05
 ---- batch: 070 ----
mean loss: 136.87
 ---- batch: 080 ----
mean loss: 135.27
 ---- batch: 090 ----
mean loss: 132.01
 ---- batch: 100 ----
mean loss: 140.94
 ---- batch: 110 ----
mean loss: 128.77
train mean loss: 135.17
epoch train time: 0:00:01.875460
elapsed time: 0:05:31.917589
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 22:33:56.807771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.88
 ---- batch: 020 ----
mean loss: 129.67
 ---- batch: 030 ----
mean loss: 136.14
 ---- batch: 040 ----
mean loss: 132.30
 ---- batch: 050 ----
mean loss: 126.79
 ---- batch: 060 ----
mean loss: 139.77
 ---- batch: 070 ----
mean loss: 129.38
 ---- batch: 080 ----
mean loss: 132.79
 ---- batch: 090 ----
mean loss: 140.86
 ---- batch: 100 ----
mean loss: 135.00
 ---- batch: 110 ----
mean loss: 143.21
train mean loss: 134.32
epoch train time: 0:00:01.901695
elapsed time: 0:05:33.819937
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 22:33:58.710063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.23
 ---- batch: 020 ----
mean loss: 130.29
 ---- batch: 030 ----
mean loss: 137.56
 ---- batch: 040 ----
mean loss: 128.53
 ---- batch: 050 ----
mean loss: 132.13
 ---- batch: 060 ----
mean loss: 133.14
 ---- batch: 070 ----
mean loss: 139.51
 ---- batch: 080 ----
mean loss: 134.46
 ---- batch: 090 ----
mean loss: 124.39
 ---- batch: 100 ----
mean loss: 138.34
 ---- batch: 110 ----
mean loss: 140.36
train mean loss: 134.66
epoch train time: 0:00:01.908819
elapsed time: 0:05:35.729342
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 22:34:00.619485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.73
 ---- batch: 020 ----
mean loss: 131.01
 ---- batch: 030 ----
mean loss: 128.46
 ---- batch: 040 ----
mean loss: 139.72
 ---- batch: 050 ----
mean loss: 134.93
 ---- batch: 060 ----
mean loss: 122.76
 ---- batch: 070 ----
mean loss: 136.85
 ---- batch: 080 ----
mean loss: 135.21
 ---- batch: 090 ----
mean loss: 131.42
 ---- batch: 100 ----
mean loss: 139.90
 ---- batch: 110 ----
mean loss: 129.58
train mean loss: 133.34
epoch train time: 0:00:01.904324
elapsed time: 0:05:37.634244
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 22:34:02.524362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.67
 ---- batch: 020 ----
mean loss: 128.10
 ---- batch: 030 ----
mean loss: 131.90
 ---- batch: 040 ----
mean loss: 136.61
 ---- batch: 050 ----
mean loss: 144.61
 ---- batch: 060 ----
mean loss: 131.26
 ---- batch: 070 ----
mean loss: 125.49
 ---- batch: 080 ----
mean loss: 141.66
 ---- batch: 090 ----
mean loss: 138.78
 ---- batch: 100 ----
mean loss: 126.77
 ---- batch: 110 ----
mean loss: 127.78
train mean loss: 133.26
epoch train time: 0:00:01.914440
elapsed time: 0:05:39.549242
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 22:34:04.439405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.01
 ---- batch: 020 ----
mean loss: 131.67
 ---- batch: 030 ----
mean loss: 126.28
 ---- batch: 040 ----
mean loss: 134.45
 ---- batch: 050 ----
mean loss: 137.74
 ---- batch: 060 ----
mean loss: 137.36
 ---- batch: 070 ----
mean loss: 131.74
 ---- batch: 080 ----
mean loss: 135.78
 ---- batch: 090 ----
mean loss: 135.17
 ---- batch: 100 ----
mean loss: 140.97
 ---- batch: 110 ----
mean loss: 132.31
train mean loss: 133.39
epoch train time: 0:00:01.897118
elapsed time: 0:05:41.446956
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 22:34:06.337075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.80
 ---- batch: 020 ----
mean loss: 134.65
 ---- batch: 030 ----
mean loss: 130.99
 ---- batch: 040 ----
mean loss: 129.63
 ---- batch: 050 ----
mean loss: 145.68
 ---- batch: 060 ----
mean loss: 135.08
 ---- batch: 070 ----
mean loss: 128.94
 ---- batch: 080 ----
mean loss: 123.18
 ---- batch: 090 ----
mean loss: 128.01
 ---- batch: 100 ----
mean loss: 136.77
 ---- batch: 110 ----
mean loss: 141.00
train mean loss: 133.17
epoch train time: 0:00:01.904756
elapsed time: 0:05:43.352287
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 22:34:08.242416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.74
 ---- batch: 020 ----
mean loss: 135.01
 ---- batch: 030 ----
mean loss: 138.40
 ---- batch: 040 ----
mean loss: 130.32
 ---- batch: 050 ----
mean loss: 132.09
 ---- batch: 060 ----
mean loss: 132.40
 ---- batch: 070 ----
mean loss: 131.18
 ---- batch: 080 ----
mean loss: 135.36
 ---- batch: 090 ----
mean loss: 131.07
 ---- batch: 100 ----
mean loss: 125.77
 ---- batch: 110 ----
mean loss: 134.09
train mean loss: 132.62
epoch train time: 0:00:01.935310
elapsed time: 0:05:45.288206
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 22:34:10.178366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.45
 ---- batch: 020 ----
mean loss: 132.39
 ---- batch: 030 ----
mean loss: 130.15
 ---- batch: 040 ----
mean loss: 128.43
 ---- batch: 050 ----
mean loss: 127.84
 ---- batch: 060 ----
mean loss: 135.91
 ---- batch: 070 ----
mean loss: 140.76
 ---- batch: 080 ----
mean loss: 132.33
 ---- batch: 090 ----
mean loss: 129.79
 ---- batch: 100 ----
mean loss: 136.29
 ---- batch: 110 ----
mean loss: 138.02
train mean loss: 132.96
epoch train time: 0:00:01.899882
elapsed time: 0:05:47.188741
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 22:34:12.078942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.09
 ---- batch: 020 ----
mean loss: 125.82
 ---- batch: 030 ----
mean loss: 132.45
 ---- batch: 040 ----
mean loss: 132.43
 ---- batch: 050 ----
mean loss: 144.00
 ---- batch: 060 ----
mean loss: 129.83
 ---- batch: 070 ----
mean loss: 134.69
 ---- batch: 080 ----
mean loss: 131.93
 ---- batch: 090 ----
mean loss: 135.61
 ---- batch: 100 ----
mean loss: 130.23
 ---- batch: 110 ----
mean loss: 122.27
train mean loss: 132.24
epoch train time: 0:00:01.940279
elapsed time: 0:05:49.129679
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 22:34:14.019798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.38
 ---- batch: 020 ----
mean loss: 127.35
 ---- batch: 030 ----
mean loss: 127.60
 ---- batch: 040 ----
mean loss: 128.56
 ---- batch: 050 ----
mean loss: 126.77
 ---- batch: 060 ----
mean loss: 131.40
 ---- batch: 070 ----
mean loss: 123.84
 ---- batch: 080 ----
mean loss: 143.93
 ---- batch: 090 ----
mean loss: 133.89
 ---- batch: 100 ----
mean loss: 149.97
 ---- batch: 110 ----
mean loss: 128.05
train mean loss: 131.92
epoch train time: 0:00:01.913118
elapsed time: 0:05:51.043351
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 22:34:15.933657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.73
 ---- batch: 020 ----
mean loss: 132.14
 ---- batch: 030 ----
mean loss: 128.69
 ---- batch: 040 ----
mean loss: 125.49
 ---- batch: 050 ----
mean loss: 130.46
 ---- batch: 060 ----
mean loss: 127.77
 ---- batch: 070 ----
mean loss: 132.76
 ---- batch: 080 ----
mean loss: 126.41
 ---- batch: 090 ----
mean loss: 139.10
 ---- batch: 100 ----
mean loss: 133.85
 ---- batch: 110 ----
mean loss: 137.89
train mean loss: 131.34
epoch train time: 0:00:01.891460
elapsed time: 0:05:52.935565
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 22:34:17.825901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.07
 ---- batch: 020 ----
mean loss: 127.44
 ---- batch: 030 ----
mean loss: 126.52
 ---- batch: 040 ----
mean loss: 139.10
 ---- batch: 050 ----
mean loss: 136.12
 ---- batch: 060 ----
mean loss: 127.33
 ---- batch: 070 ----
mean loss: 123.83
 ---- batch: 080 ----
mean loss: 132.51
 ---- batch: 090 ----
mean loss: 129.80
 ---- batch: 100 ----
mean loss: 129.46
 ---- batch: 110 ----
mean loss: 133.01
train mean loss: 130.74
epoch train time: 0:00:01.886772
elapsed time: 0:05:54.823141
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 22:34:19.713282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.66
 ---- batch: 020 ----
mean loss: 132.74
 ---- batch: 030 ----
mean loss: 120.89
 ---- batch: 040 ----
mean loss: 137.92
 ---- batch: 050 ----
mean loss: 131.97
 ---- batch: 060 ----
mean loss: 134.23
 ---- batch: 070 ----
mean loss: 127.74
 ---- batch: 080 ----
mean loss: 138.98
 ---- batch: 090 ----
mean loss: 128.06
 ---- batch: 100 ----
mean loss: 135.21
 ---- batch: 110 ----
mean loss: 125.95
train mean loss: 130.84
epoch train time: 0:00:01.903024
elapsed time: 0:05:56.726764
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 22:34:21.616872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.88
 ---- batch: 020 ----
mean loss: 131.22
 ---- batch: 030 ----
mean loss: 128.72
 ---- batch: 040 ----
mean loss: 130.70
 ---- batch: 050 ----
mean loss: 131.93
 ---- batch: 060 ----
mean loss: 137.04
 ---- batch: 070 ----
mean loss: 129.25
 ---- batch: 080 ----
mean loss: 135.89
 ---- batch: 090 ----
mean loss: 131.28
 ---- batch: 100 ----
mean loss: 126.14
 ---- batch: 110 ----
mean loss: 130.72
train mean loss: 131.49
epoch train time: 0:00:01.930607
elapsed time: 0:05:58.657930
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 22:34:23.548105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.80
 ---- batch: 020 ----
mean loss: 136.46
 ---- batch: 030 ----
mean loss: 132.62
 ---- batch: 040 ----
mean loss: 128.86
 ---- batch: 050 ----
mean loss: 131.24
 ---- batch: 060 ----
mean loss: 122.49
 ---- batch: 070 ----
mean loss: 139.47
 ---- batch: 080 ----
mean loss: 128.50
 ---- batch: 090 ----
mean loss: 132.17
 ---- batch: 100 ----
mean loss: 130.73
 ---- batch: 110 ----
mean loss: 133.07
train mean loss: 130.38
epoch train time: 0:00:01.920120
elapsed time: 0:06:00.578679
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 22:34:25.468945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.00
 ---- batch: 020 ----
mean loss: 130.46
 ---- batch: 030 ----
mean loss: 128.03
 ---- batch: 040 ----
mean loss: 123.86
 ---- batch: 050 ----
mean loss: 127.92
 ---- batch: 060 ----
mean loss: 130.89
 ---- batch: 070 ----
mean loss: 126.34
 ---- batch: 080 ----
mean loss: 130.92
 ---- batch: 090 ----
mean loss: 136.86
 ---- batch: 100 ----
mean loss: 129.81
 ---- batch: 110 ----
mean loss: 137.95
train mean loss: 129.66
epoch train time: 0:00:01.910083
elapsed time: 0:06:02.489488
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 22:34:27.379617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.51
 ---- batch: 020 ----
mean loss: 122.99
 ---- batch: 030 ----
mean loss: 131.04
 ---- batch: 040 ----
mean loss: 130.91
 ---- batch: 050 ----
mean loss: 124.69
 ---- batch: 060 ----
mean loss: 128.62
 ---- batch: 070 ----
mean loss: 133.55
 ---- batch: 080 ----
mean loss: 125.15
 ---- batch: 090 ----
mean loss: 139.00
 ---- batch: 100 ----
mean loss: 124.74
 ---- batch: 110 ----
mean loss: 136.38
train mean loss: 129.96
epoch train time: 0:00:01.921427
elapsed time: 0:06:04.411771
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 22:34:29.301689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.74
 ---- batch: 020 ----
mean loss: 132.11
 ---- batch: 030 ----
mean loss: 126.64
 ---- batch: 040 ----
mean loss: 131.03
 ---- batch: 050 ----
mean loss: 133.70
 ---- batch: 060 ----
mean loss: 128.75
 ---- batch: 070 ----
mean loss: 131.52
 ---- batch: 080 ----
mean loss: 131.45
 ---- batch: 090 ----
mean loss: 132.90
 ---- batch: 100 ----
mean loss: 131.83
 ---- batch: 110 ----
mean loss: 130.81
train mean loss: 129.82
epoch train time: 0:00:01.903375
elapsed time: 0:06:06.315524
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 22:34:31.205709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.37
 ---- batch: 020 ----
mean loss: 124.01
 ---- batch: 030 ----
mean loss: 129.67
 ---- batch: 040 ----
mean loss: 129.59
 ---- batch: 050 ----
mean loss: 125.54
 ---- batch: 060 ----
mean loss: 127.40
 ---- batch: 070 ----
mean loss: 138.10
 ---- batch: 080 ----
mean loss: 130.97
 ---- batch: 090 ----
mean loss: 133.88
 ---- batch: 100 ----
mean loss: 128.21
 ---- batch: 110 ----
mean loss: 128.58
train mean loss: 129.89
epoch train time: 0:00:01.916689
elapsed time: 0:06:08.232917
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 22:34:33.123120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.69
 ---- batch: 020 ----
mean loss: 132.74
 ---- batch: 030 ----
mean loss: 123.02
 ---- batch: 040 ----
mean loss: 139.93
 ---- batch: 050 ----
mean loss: 132.54
 ---- batch: 060 ----
mean loss: 127.92
 ---- batch: 070 ----
mean loss: 128.23
 ---- batch: 080 ----
mean loss: 130.11
 ---- batch: 090 ----
mean loss: 123.44
 ---- batch: 100 ----
mean loss: 125.66
 ---- batch: 110 ----
mean loss: 130.07
train mean loss: 128.90
epoch train time: 0:00:01.927445
elapsed time: 0:06:10.161001
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 22:34:35.051146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.24
 ---- batch: 020 ----
mean loss: 131.01
 ---- batch: 030 ----
mean loss: 130.36
 ---- batch: 040 ----
mean loss: 123.53
 ---- batch: 050 ----
mean loss: 130.48
 ---- batch: 060 ----
mean loss: 130.82
 ---- batch: 070 ----
mean loss: 126.67
 ---- batch: 080 ----
mean loss: 126.68
 ---- batch: 090 ----
mean loss: 131.31
 ---- batch: 100 ----
mean loss: 133.32
 ---- batch: 110 ----
mean loss: 123.55
train mean loss: 128.32
epoch train time: 0:00:01.914773
elapsed time: 0:06:12.076363
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 22:34:36.966504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.23
 ---- batch: 020 ----
mean loss: 131.66
 ---- batch: 030 ----
mean loss: 130.05
 ---- batch: 040 ----
mean loss: 120.66
 ---- batch: 050 ----
mean loss: 125.51
 ---- batch: 060 ----
mean loss: 135.06
 ---- batch: 070 ----
mean loss: 123.73
 ---- batch: 080 ----
mean loss: 128.96
 ---- batch: 090 ----
mean loss: 130.79
 ---- batch: 100 ----
mean loss: 125.60
 ---- batch: 110 ----
mean loss: 130.19
train mean loss: 128.01
epoch train time: 0:00:01.942710
elapsed time: 0:06:14.019684
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 22:34:38.909840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.71
 ---- batch: 020 ----
mean loss: 120.57
 ---- batch: 030 ----
mean loss: 124.79
 ---- batch: 040 ----
mean loss: 132.17
 ---- batch: 050 ----
mean loss: 129.92
 ---- batch: 060 ----
mean loss: 122.70
 ---- batch: 070 ----
mean loss: 132.62
 ---- batch: 080 ----
mean loss: 128.20
 ---- batch: 090 ----
mean loss: 128.14
 ---- batch: 100 ----
mean loss: 130.35
 ---- batch: 110 ----
mean loss: 135.41
train mean loss: 128.41
epoch train time: 0:00:01.920528
elapsed time: 0:06:15.940854
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 22:34:40.830986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.21
 ---- batch: 020 ----
mean loss: 128.46
 ---- batch: 030 ----
mean loss: 121.71
 ---- batch: 040 ----
mean loss: 127.13
 ---- batch: 050 ----
mean loss: 137.32
 ---- batch: 060 ----
mean loss: 127.03
 ---- batch: 070 ----
mean loss: 134.21
 ---- batch: 080 ----
mean loss: 129.88
 ---- batch: 090 ----
mean loss: 125.42
 ---- batch: 100 ----
mean loss: 131.49
 ---- batch: 110 ----
mean loss: 128.01
train mean loss: 128.16
epoch train time: 0:00:01.929148
elapsed time: 0:06:17.870593
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 22:34:42.760805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.64
 ---- batch: 020 ----
mean loss: 126.92
 ---- batch: 030 ----
mean loss: 138.28
 ---- batch: 040 ----
mean loss: 116.05
 ---- batch: 050 ----
mean loss: 127.83
 ---- batch: 060 ----
mean loss: 122.41
 ---- batch: 070 ----
mean loss: 125.53
 ---- batch: 080 ----
mean loss: 127.59
 ---- batch: 090 ----
mean loss: 132.16
 ---- batch: 100 ----
mean loss: 131.45
 ---- batch: 110 ----
mean loss: 130.88
train mean loss: 127.36
epoch train time: 0:00:01.942263
elapsed time: 0:06:19.813550
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 22:34:44.703710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.66
 ---- batch: 020 ----
mean loss: 128.81
 ---- batch: 030 ----
mean loss: 128.64
 ---- batch: 040 ----
mean loss: 126.53
 ---- batch: 050 ----
mean loss: 125.24
 ---- batch: 060 ----
mean loss: 127.60
 ---- batch: 070 ----
mean loss: 126.20
 ---- batch: 080 ----
mean loss: 127.02
 ---- batch: 090 ----
mean loss: 130.01
 ---- batch: 100 ----
mean loss: 128.01
 ---- batch: 110 ----
mean loss: 128.90
train mean loss: 127.72
epoch train time: 0:00:01.930645
elapsed time: 0:06:21.744839
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 22:34:46.634944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.26
 ---- batch: 020 ----
mean loss: 131.36
 ---- batch: 030 ----
mean loss: 118.87
 ---- batch: 040 ----
mean loss: 125.25
 ---- batch: 050 ----
mean loss: 121.86
 ---- batch: 060 ----
mean loss: 119.00
 ---- batch: 070 ----
mean loss: 130.61
 ---- batch: 080 ----
mean loss: 127.11
 ---- batch: 090 ----
mean loss: 138.54
 ---- batch: 100 ----
mean loss: 128.88
 ---- batch: 110 ----
mean loss: 128.05
train mean loss: 127.06
epoch train time: 0:00:01.898864
elapsed time: 0:06:23.644242
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 22:34:48.534366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.94
 ---- batch: 020 ----
mean loss: 124.91
 ---- batch: 030 ----
mean loss: 131.02
 ---- batch: 040 ----
mean loss: 120.46
 ---- batch: 050 ----
mean loss: 130.70
 ---- batch: 060 ----
mean loss: 129.07
 ---- batch: 070 ----
mean loss: 128.89
 ---- batch: 080 ----
mean loss: 123.91
 ---- batch: 090 ----
mean loss: 120.37
 ---- batch: 100 ----
mean loss: 132.24
 ---- batch: 110 ----
mean loss: 132.99
train mean loss: 126.66
epoch train time: 0:00:01.904284
elapsed time: 0:06:25.549087
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 22:34:50.439209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.89
 ---- batch: 020 ----
mean loss: 121.48
 ---- batch: 030 ----
mean loss: 129.72
 ---- batch: 040 ----
mean loss: 125.72
 ---- batch: 050 ----
mean loss: 128.66
 ---- batch: 060 ----
mean loss: 124.50
 ---- batch: 070 ----
mean loss: 130.27
 ---- batch: 080 ----
mean loss: 122.37
 ---- batch: 090 ----
mean loss: 125.27
 ---- batch: 100 ----
mean loss: 134.29
 ---- batch: 110 ----
mean loss: 130.56
train mean loss: 126.46
epoch train time: 0:00:01.891564
elapsed time: 0:06:27.441202
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 22:34:52.331320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.72
 ---- batch: 020 ----
mean loss: 128.28
 ---- batch: 030 ----
mean loss: 131.64
 ---- batch: 040 ----
mean loss: 131.30
 ---- batch: 050 ----
mean loss: 126.83
 ---- batch: 060 ----
mean loss: 127.48
 ---- batch: 070 ----
mean loss: 127.97
 ---- batch: 080 ----
mean loss: 131.09
 ---- batch: 090 ----
mean loss: 123.30
 ---- batch: 100 ----
mean loss: 121.17
 ---- batch: 110 ----
mean loss: 125.50
train mean loss: 127.05
epoch train time: 0:00:01.893219
elapsed time: 0:06:29.334985
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 22:34:54.225129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.52
 ---- batch: 020 ----
mean loss: 124.39
 ---- batch: 030 ----
mean loss: 124.98
 ---- batch: 040 ----
mean loss: 128.08
 ---- batch: 050 ----
mean loss: 129.58
 ---- batch: 060 ----
mean loss: 130.10
 ---- batch: 070 ----
mean loss: 123.23
 ---- batch: 080 ----
mean loss: 117.21
 ---- batch: 090 ----
mean loss: 129.69
 ---- batch: 100 ----
mean loss: 130.51
 ---- batch: 110 ----
mean loss: 124.65
train mean loss: 125.59
epoch train time: 0:00:01.915348
elapsed time: 0:06:31.250903
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 22:34:56.141055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.04
 ---- batch: 020 ----
mean loss: 127.23
 ---- batch: 030 ----
mean loss: 133.96
 ---- batch: 040 ----
mean loss: 117.30
 ---- batch: 050 ----
mean loss: 128.76
 ---- batch: 060 ----
mean loss: 123.89
 ---- batch: 070 ----
mean loss: 125.89
 ---- batch: 080 ----
mean loss: 126.15
 ---- batch: 090 ----
mean loss: 131.29
 ---- batch: 100 ----
mean loss: 124.79
 ---- batch: 110 ----
mean loss: 121.51
train mean loss: 126.07
epoch train time: 0:00:01.937305
elapsed time: 0:06:33.188815
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 22:34:58.078969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.31
 ---- batch: 020 ----
mean loss: 118.54
 ---- batch: 030 ----
mean loss: 135.92
 ---- batch: 040 ----
mean loss: 124.53
 ---- batch: 050 ----
mean loss: 116.30
 ---- batch: 060 ----
mean loss: 120.09
 ---- batch: 070 ----
mean loss: 123.34
 ---- batch: 080 ----
mean loss: 131.95
 ---- batch: 090 ----
mean loss: 127.36
 ---- batch: 100 ----
mean loss: 121.55
 ---- batch: 110 ----
mean loss: 126.59
train mean loss: 124.94
epoch train time: 0:00:01.904335
elapsed time: 0:06:35.093738
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 22:34:59.983871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.49
 ---- batch: 020 ----
mean loss: 117.75
 ---- batch: 030 ----
mean loss: 124.85
 ---- batch: 040 ----
mean loss: 121.30
 ---- batch: 050 ----
mean loss: 122.49
 ---- batch: 060 ----
mean loss: 131.40
 ---- batch: 070 ----
mean loss: 132.92
 ---- batch: 080 ----
mean loss: 122.49
 ---- batch: 090 ----
mean loss: 123.26
 ---- batch: 100 ----
mean loss: 128.19
 ---- batch: 110 ----
mean loss: 131.64
train mean loss: 125.04
epoch train time: 0:00:01.906420
elapsed time: 0:06:37.000786
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 22:35:01.890952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.71
 ---- batch: 020 ----
mean loss: 118.67
 ---- batch: 030 ----
mean loss: 124.07
 ---- batch: 040 ----
mean loss: 124.40
 ---- batch: 050 ----
mean loss: 127.53
 ---- batch: 060 ----
mean loss: 127.24
 ---- batch: 070 ----
mean loss: 123.08
 ---- batch: 080 ----
mean loss: 124.66
 ---- batch: 090 ----
mean loss: 126.47
 ---- batch: 100 ----
mean loss: 128.90
 ---- batch: 110 ----
mean loss: 123.96
train mean loss: 124.56
epoch train time: 0:00:01.915049
elapsed time: 0:06:38.916446
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 22:35:03.806579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.56
 ---- batch: 020 ----
mean loss: 125.08
 ---- batch: 030 ----
mean loss: 123.46
 ---- batch: 040 ----
mean loss: 128.82
 ---- batch: 050 ----
mean loss: 122.78
 ---- batch: 060 ----
mean loss: 131.99
 ---- batch: 070 ----
mean loss: 119.08
 ---- batch: 080 ----
mean loss: 125.36
 ---- batch: 090 ----
mean loss: 119.19
 ---- batch: 100 ----
mean loss: 123.54
 ---- batch: 110 ----
mean loss: 128.29
train mean loss: 124.83
epoch train time: 0:00:01.919830
elapsed time: 0:06:40.836893
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 22:35:05.727038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.42
 ---- batch: 020 ----
mean loss: 125.00
 ---- batch: 030 ----
mean loss: 117.56
 ---- batch: 040 ----
mean loss: 125.37
 ---- batch: 050 ----
mean loss: 121.15
 ---- batch: 060 ----
mean loss: 119.25
 ---- batch: 070 ----
mean loss: 124.56
 ---- batch: 080 ----
mean loss: 119.33
 ---- batch: 090 ----
mean loss: 124.15
 ---- batch: 100 ----
mean loss: 134.53
 ---- batch: 110 ----
mean loss: 134.60
train mean loss: 124.45
epoch train time: 0:00:01.903132
elapsed time: 0:06:42.740622
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 22:35:07.630746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.87
 ---- batch: 020 ----
mean loss: 130.86
 ---- batch: 030 ----
mean loss: 125.56
 ---- batch: 040 ----
mean loss: 122.78
 ---- batch: 050 ----
mean loss: 119.14
 ---- batch: 060 ----
mean loss: 119.12
 ---- batch: 070 ----
mean loss: 128.87
 ---- batch: 080 ----
mean loss: 124.84
 ---- batch: 090 ----
mean loss: 124.70
 ---- batch: 100 ----
mean loss: 126.07
 ---- batch: 110 ----
mean loss: 125.27
train mean loss: 124.32
epoch train time: 0:00:01.899857
elapsed time: 0:06:44.641039
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 22:35:09.531157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.02
 ---- batch: 020 ----
mean loss: 126.07
 ---- batch: 030 ----
mean loss: 130.65
 ---- batch: 040 ----
mean loss: 125.67
 ---- batch: 050 ----
mean loss: 118.03
 ---- batch: 060 ----
mean loss: 123.50
 ---- batch: 070 ----
mean loss: 126.85
 ---- batch: 080 ----
mean loss: 119.73
 ---- batch: 090 ----
mean loss: 129.64
 ---- batch: 100 ----
mean loss: 118.96
 ---- batch: 110 ----
mean loss: 127.68
train mean loss: 124.05
epoch train time: 0:00:01.875934
elapsed time: 0:06:46.517546
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 22:35:11.407733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.63
 ---- batch: 020 ----
mean loss: 119.36
 ---- batch: 030 ----
mean loss: 118.86
 ---- batch: 040 ----
mean loss: 129.45
 ---- batch: 050 ----
mean loss: 118.99
 ---- batch: 060 ----
mean loss: 123.00
 ---- batch: 070 ----
mean loss: 127.09
 ---- batch: 080 ----
mean loss: 126.61
 ---- batch: 090 ----
mean loss: 123.55
 ---- batch: 100 ----
mean loss: 119.69
 ---- batch: 110 ----
mean loss: 131.27
train mean loss: 124.18
epoch train time: 0:00:01.914229
elapsed time: 0:06:48.432385
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 22:35:13.322512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.99
 ---- batch: 020 ----
mean loss: 122.59
 ---- batch: 030 ----
mean loss: 124.10
 ---- batch: 040 ----
mean loss: 118.46
 ---- batch: 050 ----
mean loss: 123.43
 ---- batch: 060 ----
mean loss: 117.31
 ---- batch: 070 ----
mean loss: 132.86
 ---- batch: 080 ----
mean loss: 125.07
 ---- batch: 090 ----
mean loss: 127.07
 ---- batch: 100 ----
mean loss: 121.06
 ---- batch: 110 ----
mean loss: 122.27
train mean loss: 123.22
epoch train time: 0:00:01.908732
elapsed time: 0:06:50.341705
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 22:35:15.231852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.31
 ---- batch: 020 ----
mean loss: 118.89
 ---- batch: 030 ----
mean loss: 126.11
 ---- batch: 040 ----
mean loss: 121.18
 ---- batch: 050 ----
mean loss: 121.51
 ---- batch: 060 ----
mean loss: 127.59
 ---- batch: 070 ----
mean loss: 117.40
 ---- batch: 080 ----
mean loss: 126.23
 ---- batch: 090 ----
mean loss: 117.89
 ---- batch: 100 ----
mean loss: 119.51
 ---- batch: 110 ----
mean loss: 131.38
train mean loss: 123.13
epoch train time: 0:00:01.892840
elapsed time: 0:06:52.235155
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 22:35:17.125323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.65
 ---- batch: 020 ----
mean loss: 118.47
 ---- batch: 030 ----
mean loss: 128.58
 ---- batch: 040 ----
mean loss: 115.54
 ---- batch: 050 ----
mean loss: 120.99
 ---- batch: 060 ----
mean loss: 134.51
 ---- batch: 070 ----
mean loss: 127.60
 ---- batch: 080 ----
mean loss: 127.57
 ---- batch: 090 ----
mean loss: 117.86
 ---- batch: 100 ----
mean loss: 121.06
 ---- batch: 110 ----
mean loss: 124.03
train mean loss: 123.46
epoch train time: 0:00:01.876547
elapsed time: 0:06:54.112288
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 22:35:19.002395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.37
 ---- batch: 020 ----
mean loss: 114.95
 ---- batch: 030 ----
mean loss: 117.01
 ---- batch: 040 ----
mean loss: 117.93
 ---- batch: 050 ----
mean loss: 121.18
 ---- batch: 060 ----
mean loss: 125.64
 ---- batch: 070 ----
mean loss: 122.13
 ---- batch: 080 ----
mean loss: 118.93
 ---- batch: 090 ----
mean loss: 128.67
 ---- batch: 100 ----
mean loss: 125.21
 ---- batch: 110 ----
mean loss: 132.55
train mean loss: 122.88
epoch train time: 0:00:01.897553
elapsed time: 0:06:56.010401
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 22:35:20.900568
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.40
 ---- batch: 020 ----
mean loss: 114.98
 ---- batch: 030 ----
mean loss: 126.34
 ---- batch: 040 ----
mean loss: 118.43
 ---- batch: 050 ----
mean loss: 116.83
 ---- batch: 060 ----
mean loss: 113.38
 ---- batch: 070 ----
mean loss: 112.99
 ---- batch: 080 ----
mean loss: 120.93
 ---- batch: 090 ----
mean loss: 122.55
 ---- batch: 100 ----
mean loss: 121.96
 ---- batch: 110 ----
mean loss: 112.26
train mean loss: 117.83
epoch train time: 0:00:01.877003
elapsed time: 0:06:57.888359
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 22:35:22.778162
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.58
 ---- batch: 020 ----
mean loss: 116.48
 ---- batch: 030 ----
mean loss: 110.74
 ---- batch: 040 ----
mean loss: 115.94
 ---- batch: 050 ----
mean loss: 118.04
 ---- batch: 060 ----
mean loss: 121.10
 ---- batch: 070 ----
mean loss: 109.56
 ---- batch: 080 ----
mean loss: 122.91
 ---- batch: 090 ----
mean loss: 118.09
 ---- batch: 100 ----
mean loss: 119.09
 ---- batch: 110 ----
mean loss: 114.41
train mean loss: 117.08
epoch train time: 0:00:01.934374
elapsed time: 0:06:59.822997
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 22:35:24.713112
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.70
 ---- batch: 020 ----
mean loss: 116.46
 ---- batch: 030 ----
mean loss: 122.27
 ---- batch: 040 ----
mean loss: 114.03
 ---- batch: 050 ----
mean loss: 120.48
 ---- batch: 060 ----
mean loss: 118.93
 ---- batch: 070 ----
mean loss: 113.79
 ---- batch: 080 ----
mean loss: 118.80
 ---- batch: 090 ----
mean loss: 116.44
 ---- batch: 100 ----
mean loss: 115.09
 ---- batch: 110 ----
mean loss: 119.08
train mean loss: 116.91
epoch train time: 0:00:01.888368
elapsed time: 0:07:01.711957
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 22:35:26.601805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.80
 ---- batch: 020 ----
mean loss: 113.54
 ---- batch: 030 ----
mean loss: 115.13
 ---- batch: 040 ----
mean loss: 112.88
 ---- batch: 050 ----
mean loss: 122.96
 ---- batch: 060 ----
mean loss: 118.33
 ---- batch: 070 ----
mean loss: 124.16
 ---- batch: 080 ----
mean loss: 116.45
 ---- batch: 090 ----
mean loss: 120.18
 ---- batch: 100 ----
mean loss: 111.08
 ---- batch: 110 ----
mean loss: 112.74
train mean loss: 116.82
epoch train time: 0:00:01.914326
elapsed time: 0:07:03.626596
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 22:35:28.516742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.56
 ---- batch: 020 ----
mean loss: 112.46
 ---- batch: 030 ----
mean loss: 119.31
 ---- batch: 040 ----
mean loss: 110.93
 ---- batch: 050 ----
mean loss: 115.26
 ---- batch: 060 ----
mean loss: 117.61
 ---- batch: 070 ----
mean loss: 118.88
 ---- batch: 080 ----
mean loss: 127.58
 ---- batch: 090 ----
mean loss: 111.88
 ---- batch: 100 ----
mean loss: 112.26
 ---- batch: 110 ----
mean loss: 121.80
train mean loss: 116.86
epoch train time: 0:00:01.916073
elapsed time: 0:07:05.543242
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 22:35:30.433401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.14
 ---- batch: 020 ----
mean loss: 113.43
 ---- batch: 030 ----
mean loss: 115.40
 ---- batch: 040 ----
mean loss: 118.34
 ---- batch: 050 ----
mean loss: 108.97
 ---- batch: 060 ----
mean loss: 119.68
 ---- batch: 070 ----
mean loss: 117.59
 ---- batch: 080 ----
mean loss: 122.70
 ---- batch: 090 ----
mean loss: 117.34
 ---- batch: 100 ----
mean loss: 111.16
 ---- batch: 110 ----
mean loss: 118.55
train mean loss: 116.79
epoch train time: 0:00:01.941262
elapsed time: 0:07:07.485162
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 22:35:32.375355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.01
 ---- batch: 020 ----
mean loss: 120.11
 ---- batch: 030 ----
mean loss: 115.85
 ---- batch: 040 ----
mean loss: 123.14
 ---- batch: 050 ----
mean loss: 114.79
 ---- batch: 060 ----
mean loss: 120.30
 ---- batch: 070 ----
mean loss: 109.35
 ---- batch: 080 ----
mean loss: 120.28
 ---- batch: 090 ----
mean loss: 114.70
 ---- batch: 100 ----
mean loss: 119.38
 ---- batch: 110 ----
mean loss: 113.82
train mean loss: 116.79
epoch train time: 0:00:01.899458
elapsed time: 0:07:09.385253
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 22:35:34.275395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.11
 ---- batch: 020 ----
mean loss: 118.51
 ---- batch: 030 ----
mean loss: 112.92
 ---- batch: 040 ----
mean loss: 117.48
 ---- batch: 050 ----
mean loss: 116.65
 ---- batch: 060 ----
mean loss: 122.24
 ---- batch: 070 ----
mean loss: 120.09
 ---- batch: 080 ----
mean loss: 116.43
 ---- batch: 090 ----
mean loss: 119.30
 ---- batch: 100 ----
mean loss: 110.69
 ---- batch: 110 ----
mean loss: 117.28
train mean loss: 116.69
epoch train time: 0:00:01.945202
elapsed time: 0:07:11.331023
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 22:35:36.221206
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.39
 ---- batch: 020 ----
mean loss: 108.96
 ---- batch: 030 ----
mean loss: 118.72
 ---- batch: 040 ----
mean loss: 114.86
 ---- batch: 050 ----
mean loss: 112.89
 ---- batch: 060 ----
mean loss: 119.90
 ---- batch: 070 ----
mean loss: 116.42
 ---- batch: 080 ----
mean loss: 110.57
 ---- batch: 090 ----
mean loss: 118.38
 ---- batch: 100 ----
mean loss: 115.37
 ---- batch: 110 ----
mean loss: 125.36
train mean loss: 116.57
epoch train time: 0:00:01.914092
elapsed time: 0:07:13.245745
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 22:35:38.135869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.39
 ---- batch: 020 ----
mean loss: 113.35
 ---- batch: 030 ----
mean loss: 116.80
 ---- batch: 040 ----
mean loss: 120.86
 ---- batch: 050 ----
mean loss: 117.54
 ---- batch: 060 ----
mean loss: 117.88
 ---- batch: 070 ----
mean loss: 113.30
 ---- batch: 080 ----
mean loss: 112.78
 ---- batch: 090 ----
mean loss: 116.19
 ---- batch: 100 ----
mean loss: 112.71
 ---- batch: 110 ----
mean loss: 115.29
train mean loss: 116.61
epoch train time: 0:00:01.943906
elapsed time: 0:07:15.190211
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 22:35:40.080346
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.41
 ---- batch: 020 ----
mean loss: 121.63
 ---- batch: 030 ----
mean loss: 119.08
 ---- batch: 040 ----
mean loss: 114.69
 ---- batch: 050 ----
mean loss: 122.01
 ---- batch: 060 ----
mean loss: 118.13
 ---- batch: 070 ----
mean loss: 108.83
 ---- batch: 080 ----
mean loss: 113.80
 ---- batch: 090 ----
mean loss: 122.38
 ---- batch: 100 ----
mean loss: 108.21
 ---- batch: 110 ----
mean loss: 115.82
train mean loss: 116.55
epoch train time: 0:00:01.926850
elapsed time: 0:07:17.117676
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 22:35:42.007804
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.89
 ---- batch: 020 ----
mean loss: 111.75
 ---- batch: 030 ----
mean loss: 117.87
 ---- batch: 040 ----
mean loss: 117.68
 ---- batch: 050 ----
mean loss: 115.89
 ---- batch: 060 ----
mean loss: 116.80
 ---- batch: 070 ----
mean loss: 117.60
 ---- batch: 080 ----
mean loss: 114.65
 ---- batch: 090 ----
mean loss: 117.96
 ---- batch: 100 ----
mean loss: 119.78
 ---- batch: 110 ----
mean loss: 116.71
train mean loss: 116.50
epoch train time: 0:00:01.965650
elapsed time: 0:07:19.083962
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 22:35:43.974223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.44
 ---- batch: 020 ----
mean loss: 117.06
 ---- batch: 030 ----
mean loss: 106.76
 ---- batch: 040 ----
mean loss: 124.13
 ---- batch: 050 ----
mean loss: 117.61
 ---- batch: 060 ----
mean loss: 120.17
 ---- batch: 070 ----
mean loss: 111.14
 ---- batch: 080 ----
mean loss: 114.56
 ---- batch: 090 ----
mean loss: 111.56
 ---- batch: 100 ----
mean loss: 115.80
 ---- batch: 110 ----
mean loss: 124.34
train mean loss: 116.43
epoch train time: 0:00:01.915051
elapsed time: 0:07:20.999713
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 22:35:45.890123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.01
 ---- batch: 020 ----
mean loss: 111.17
 ---- batch: 030 ----
mean loss: 117.47
 ---- batch: 040 ----
mean loss: 110.13
 ---- batch: 050 ----
mean loss: 117.34
 ---- batch: 060 ----
mean loss: 112.87
 ---- batch: 070 ----
mean loss: 126.35
 ---- batch: 080 ----
mean loss: 116.08
 ---- batch: 090 ----
mean loss: 117.56
 ---- batch: 100 ----
mean loss: 117.33
 ---- batch: 110 ----
mean loss: 122.50
train mean loss: 116.46
epoch train time: 0:00:01.899624
elapsed time: 0:07:22.900201
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 22:35:47.790329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.02
 ---- batch: 020 ----
mean loss: 114.25
 ---- batch: 030 ----
mean loss: 116.21
 ---- batch: 040 ----
mean loss: 118.97
 ---- batch: 050 ----
mean loss: 114.15
 ---- batch: 060 ----
mean loss: 114.66
 ---- batch: 070 ----
mean loss: 119.26
 ---- batch: 080 ----
mean loss: 114.26
 ---- batch: 090 ----
mean loss: 110.28
 ---- batch: 100 ----
mean loss: 119.82
 ---- batch: 110 ----
mean loss: 119.06
train mean loss: 116.50
epoch train time: 0:00:01.911293
elapsed time: 0:07:24.812066
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 22:35:49.702219
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.12
 ---- batch: 020 ----
mean loss: 118.75
 ---- batch: 030 ----
mean loss: 114.28
 ---- batch: 040 ----
mean loss: 115.43
 ---- batch: 050 ----
mean loss: 113.83
 ---- batch: 060 ----
mean loss: 118.28
 ---- batch: 070 ----
mean loss: 114.77
 ---- batch: 080 ----
mean loss: 118.74
 ---- batch: 090 ----
mean loss: 115.29
 ---- batch: 100 ----
mean loss: 117.15
 ---- batch: 110 ----
mean loss: 114.99
train mean loss: 116.37
epoch train time: 0:00:01.919647
elapsed time: 0:07:26.732346
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 22:35:51.622530
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.80
 ---- batch: 020 ----
mean loss: 113.87
 ---- batch: 030 ----
mean loss: 117.14
 ---- batch: 040 ----
mean loss: 116.54
 ---- batch: 050 ----
mean loss: 115.73
 ---- batch: 060 ----
mean loss: 121.48
 ---- batch: 070 ----
mean loss: 112.50
 ---- batch: 080 ----
mean loss: 118.43
 ---- batch: 090 ----
mean loss: 114.37
 ---- batch: 100 ----
mean loss: 125.99
 ---- batch: 110 ----
mean loss: 115.44
train mean loss: 116.33
epoch train time: 0:00:01.902712
elapsed time: 0:07:28.635694
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 22:35:53.525873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.03
 ---- batch: 020 ----
mean loss: 118.14
 ---- batch: 030 ----
mean loss: 118.62
 ---- batch: 040 ----
mean loss: 119.81
 ---- batch: 050 ----
mean loss: 115.26
 ---- batch: 060 ----
mean loss: 113.09
 ---- batch: 070 ----
mean loss: 115.12
 ---- batch: 080 ----
mean loss: 115.28
 ---- batch: 090 ----
mean loss: 118.07
 ---- batch: 100 ----
mean loss: 119.62
 ---- batch: 110 ----
mean loss: 111.30
train mean loss: 116.25
epoch train time: 0:00:01.906019
elapsed time: 0:07:30.542315
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 22:35:55.432505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.87
 ---- batch: 020 ----
mean loss: 110.99
 ---- batch: 030 ----
mean loss: 113.56
 ---- batch: 040 ----
mean loss: 111.99
 ---- batch: 050 ----
mean loss: 116.21
 ---- batch: 060 ----
mean loss: 109.80
 ---- batch: 070 ----
mean loss: 115.25
 ---- batch: 080 ----
mean loss: 122.97
 ---- batch: 090 ----
mean loss: 120.91
 ---- batch: 100 ----
mean loss: 119.83
 ---- batch: 110 ----
mean loss: 121.82
train mean loss: 116.32
epoch train time: 0:00:01.927990
elapsed time: 0:07:32.470958
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 22:35:57.361119
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.13
 ---- batch: 020 ----
mean loss: 117.09
 ---- batch: 030 ----
mean loss: 121.34
 ---- batch: 040 ----
mean loss: 114.30
 ---- batch: 050 ----
mean loss: 121.17
 ---- batch: 060 ----
mean loss: 109.20
 ---- batch: 070 ----
mean loss: 114.37
 ---- batch: 080 ----
mean loss: 117.01
 ---- batch: 090 ----
mean loss: 117.47
 ---- batch: 100 ----
mean loss: 119.98
 ---- batch: 110 ----
mean loss: 115.50
train mean loss: 116.18
epoch train time: 0:00:01.937146
elapsed time: 0:07:34.408714
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 22:35:59.298847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.12
 ---- batch: 020 ----
mean loss: 113.09
 ---- batch: 030 ----
mean loss: 119.32
 ---- batch: 040 ----
mean loss: 114.16
 ---- batch: 050 ----
mean loss: 115.49
 ---- batch: 060 ----
mean loss: 115.67
 ---- batch: 070 ----
mean loss: 122.43
 ---- batch: 080 ----
mean loss: 123.85
 ---- batch: 090 ----
mean loss: 114.92
 ---- batch: 100 ----
mean loss: 114.61
 ---- batch: 110 ----
mean loss: 117.50
train mean loss: 116.24
epoch train time: 0:00:01.900863
elapsed time: 0:07:36.310147
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 22:36:01.200301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.87
 ---- batch: 020 ----
mean loss: 117.89
 ---- batch: 030 ----
mean loss: 120.50
 ---- batch: 040 ----
mean loss: 109.31
 ---- batch: 050 ----
mean loss: 115.87
 ---- batch: 060 ----
mean loss: 118.11
 ---- batch: 070 ----
mean loss: 117.48
 ---- batch: 080 ----
mean loss: 113.63
 ---- batch: 090 ----
mean loss: 113.41
 ---- batch: 100 ----
mean loss: 114.97
 ---- batch: 110 ----
mean loss: 114.41
train mean loss: 116.20
epoch train time: 0:00:01.896275
elapsed time: 0:07:38.207067
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 22:36:03.097398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.85
 ---- batch: 020 ----
mean loss: 116.27
 ---- batch: 030 ----
mean loss: 113.44
 ---- batch: 040 ----
mean loss: 116.57
 ---- batch: 050 ----
mean loss: 118.72
 ---- batch: 060 ----
mean loss: 118.59
 ---- batch: 070 ----
mean loss: 118.28
 ---- batch: 080 ----
mean loss: 118.44
 ---- batch: 090 ----
mean loss: 119.51
 ---- batch: 100 ----
mean loss: 113.13
 ---- batch: 110 ----
mean loss: 110.97
train mean loss: 116.12
epoch train time: 0:00:01.888388
elapsed time: 0:07:40.096250
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 22:36:04.986342
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.44
 ---- batch: 020 ----
mean loss: 113.48
 ---- batch: 030 ----
mean loss: 113.43
 ---- batch: 040 ----
mean loss: 115.68
 ---- batch: 050 ----
mean loss: 119.77
 ---- batch: 060 ----
mean loss: 115.46
 ---- batch: 070 ----
mean loss: 116.82
 ---- batch: 080 ----
mean loss: 125.32
 ---- batch: 090 ----
mean loss: 117.32
 ---- batch: 100 ----
mean loss: 107.43
 ---- batch: 110 ----
mean loss: 120.22
train mean loss: 116.05
epoch train time: 0:00:01.912142
elapsed time: 0:07:42.008936
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 22:36:06.899114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.39
 ---- batch: 020 ----
mean loss: 113.10
 ---- batch: 030 ----
mean loss: 119.10
 ---- batch: 040 ----
mean loss: 118.84
 ---- batch: 050 ----
mean loss: 113.41
 ---- batch: 060 ----
mean loss: 118.11
 ---- batch: 070 ----
mean loss: 113.39
 ---- batch: 080 ----
mean loss: 108.93
 ---- batch: 090 ----
mean loss: 115.94
 ---- batch: 100 ----
mean loss: 122.33
 ---- batch: 110 ----
mean loss: 118.05
train mean loss: 116.18
epoch train time: 0:00:01.880647
elapsed time: 0:07:43.890261
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 22:36:08.780364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.60
 ---- batch: 020 ----
mean loss: 122.53
 ---- batch: 030 ----
mean loss: 117.36
 ---- batch: 040 ----
mean loss: 113.78
 ---- batch: 050 ----
mean loss: 111.80
 ---- batch: 060 ----
mean loss: 107.02
 ---- batch: 070 ----
mean loss: 126.71
 ---- batch: 080 ----
mean loss: 110.71
 ---- batch: 090 ----
mean loss: 119.43
 ---- batch: 100 ----
mean loss: 116.05
 ---- batch: 110 ----
mean loss: 117.42
train mean loss: 115.96
epoch train time: 0:00:01.907831
elapsed time: 0:07:45.798688
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 22:36:10.688906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.22
 ---- batch: 020 ----
mean loss: 113.20
 ---- batch: 030 ----
mean loss: 115.64
 ---- batch: 040 ----
mean loss: 113.47
 ---- batch: 050 ----
mean loss: 111.73
 ---- batch: 060 ----
mean loss: 121.26
 ---- batch: 070 ----
mean loss: 122.96
 ---- batch: 080 ----
mean loss: 124.73
 ---- batch: 090 ----
mean loss: 112.00
 ---- batch: 100 ----
mean loss: 117.58
 ---- batch: 110 ----
mean loss: 115.74
train mean loss: 115.99
epoch train time: 0:00:01.867443
elapsed time: 0:07:47.666786
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 22:36:12.556908
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.89
 ---- batch: 020 ----
mean loss: 112.32
 ---- batch: 030 ----
mean loss: 118.27
 ---- batch: 040 ----
mean loss: 120.56
 ---- batch: 050 ----
mean loss: 117.19
 ---- batch: 060 ----
mean loss: 110.80
 ---- batch: 070 ----
mean loss: 120.01
 ---- batch: 080 ----
mean loss: 117.17
 ---- batch: 090 ----
mean loss: 113.14
 ---- batch: 100 ----
mean loss: 113.51
 ---- batch: 110 ----
mean loss: 117.05
train mean loss: 115.91
epoch train time: 0:00:01.889152
elapsed time: 0:07:49.556522
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 22:36:14.446663
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.87
 ---- batch: 020 ----
mean loss: 116.41
 ---- batch: 030 ----
mean loss: 122.22
 ---- batch: 040 ----
mean loss: 122.53
 ---- batch: 050 ----
mean loss: 116.46
 ---- batch: 060 ----
mean loss: 120.28
 ---- batch: 070 ----
mean loss: 114.92
 ---- batch: 080 ----
mean loss: 104.34
 ---- batch: 090 ----
mean loss: 117.73
 ---- batch: 100 ----
mean loss: 117.78
 ---- batch: 110 ----
mean loss: 113.11
train mean loss: 115.89
epoch train time: 0:00:01.917815
elapsed time: 0:07:51.474918
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 22:36:16.365029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.62
 ---- batch: 020 ----
mean loss: 113.28
 ---- batch: 030 ----
mean loss: 110.56
 ---- batch: 040 ----
mean loss: 119.49
 ---- batch: 050 ----
mean loss: 115.34
 ---- batch: 060 ----
mean loss: 112.71
 ---- batch: 070 ----
mean loss: 117.50
 ---- batch: 080 ----
mean loss: 116.52
 ---- batch: 090 ----
mean loss: 113.03
 ---- batch: 100 ----
mean loss: 114.04
 ---- batch: 110 ----
mean loss: 120.98
train mean loss: 116.04
epoch train time: 0:00:01.931547
elapsed time: 0:07:53.407068
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 22:36:18.297174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.62
 ---- batch: 020 ----
mean loss: 110.38
 ---- batch: 030 ----
mean loss: 113.85
 ---- batch: 040 ----
mean loss: 120.30
 ---- batch: 050 ----
mean loss: 115.41
 ---- batch: 060 ----
mean loss: 107.79
 ---- batch: 070 ----
mean loss: 116.38
 ---- batch: 080 ----
mean loss: 120.39
 ---- batch: 090 ----
mean loss: 117.85
 ---- batch: 100 ----
mean loss: 122.88
 ---- batch: 110 ----
mean loss: 120.01
train mean loss: 115.87
epoch train time: 0:00:01.893374
elapsed time: 0:07:55.300986
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 22:36:20.191125
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.44
 ---- batch: 020 ----
mean loss: 114.70
 ---- batch: 030 ----
mean loss: 115.36
 ---- batch: 040 ----
mean loss: 117.93
 ---- batch: 050 ----
mean loss: 107.18
 ---- batch: 060 ----
mean loss: 122.89
 ---- batch: 070 ----
mean loss: 120.70
 ---- batch: 080 ----
mean loss: 118.28
 ---- batch: 090 ----
mean loss: 117.48
 ---- batch: 100 ----
mean loss: 110.53
 ---- batch: 110 ----
mean loss: 111.75
train mean loss: 115.82
epoch train time: 0:00:01.894092
elapsed time: 0:07:57.195666
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 22:36:22.085844
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.41
 ---- batch: 020 ----
mean loss: 108.79
 ---- batch: 030 ----
mean loss: 112.68
 ---- batch: 040 ----
mean loss: 117.55
 ---- batch: 050 ----
mean loss: 110.18
 ---- batch: 060 ----
mean loss: 119.32
 ---- batch: 070 ----
mean loss: 117.67
 ---- batch: 080 ----
mean loss: 117.18
 ---- batch: 090 ----
mean loss: 118.48
 ---- batch: 100 ----
mean loss: 121.28
 ---- batch: 110 ----
mean loss: 116.11
train mean loss: 115.86
epoch train time: 0:00:01.905562
elapsed time: 0:07:59.102118
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 22:36:23.992020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.10
 ---- batch: 020 ----
mean loss: 114.19
 ---- batch: 030 ----
mean loss: 108.79
 ---- batch: 040 ----
mean loss: 116.54
 ---- batch: 050 ----
mean loss: 109.47
 ---- batch: 060 ----
mean loss: 115.80
 ---- batch: 070 ----
mean loss: 114.98
 ---- batch: 080 ----
mean loss: 118.32
 ---- batch: 090 ----
mean loss: 123.44
 ---- batch: 100 ----
mean loss: 121.44
 ---- batch: 110 ----
mean loss: 115.45
train mean loss: 115.73
epoch train time: 0:00:01.912908
elapsed time: 0:08:01.015356
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 22:36:25.905487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.90
 ---- batch: 020 ----
mean loss: 114.33
 ---- batch: 030 ----
mean loss: 111.43
 ---- batch: 040 ----
mean loss: 121.95
 ---- batch: 050 ----
mean loss: 115.91
 ---- batch: 060 ----
mean loss: 120.15
 ---- batch: 070 ----
mean loss: 117.22
 ---- batch: 080 ----
mean loss: 112.76
 ---- batch: 090 ----
mean loss: 112.93
 ---- batch: 100 ----
mean loss: 113.76
 ---- batch: 110 ----
mean loss: 109.72
train mean loss: 115.68
epoch train time: 0:00:01.897820
elapsed time: 0:08:02.913752
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 22:36:27.803880
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.16
 ---- batch: 020 ----
mean loss: 116.51
 ---- batch: 030 ----
mean loss: 114.44
 ---- batch: 040 ----
mean loss: 121.88
 ---- batch: 050 ----
mean loss: 113.08
 ---- batch: 060 ----
mean loss: 126.67
 ---- batch: 070 ----
mean loss: 119.98
 ---- batch: 080 ----
mean loss: 114.23
 ---- batch: 090 ----
mean loss: 113.48
 ---- batch: 100 ----
mean loss: 112.91
 ---- batch: 110 ----
mean loss: 108.67
train mean loss: 115.76
epoch train time: 0:00:01.910505
elapsed time: 0:08:04.824855
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 22:36:29.714995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.00
 ---- batch: 020 ----
mean loss: 124.45
 ---- batch: 030 ----
mean loss: 114.04
 ---- batch: 040 ----
mean loss: 110.74
 ---- batch: 050 ----
mean loss: 112.93
 ---- batch: 060 ----
mean loss: 119.64
 ---- batch: 070 ----
mean loss: 108.81
 ---- batch: 080 ----
mean loss: 116.14
 ---- batch: 090 ----
mean loss: 115.74
 ---- batch: 100 ----
mean loss: 114.95
 ---- batch: 110 ----
mean loss: 113.71
train mean loss: 115.68
epoch train time: 0:00:01.906381
elapsed time: 0:08:06.731806
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 22:36:31.622017
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.42
 ---- batch: 020 ----
mean loss: 118.24
 ---- batch: 030 ----
mean loss: 109.13
 ---- batch: 040 ----
mean loss: 118.20
 ---- batch: 050 ----
mean loss: 116.83
 ---- batch: 060 ----
mean loss: 118.46
 ---- batch: 070 ----
mean loss: 120.12
 ---- batch: 080 ----
mean loss: 112.19
 ---- batch: 090 ----
mean loss: 112.37
 ---- batch: 100 ----
mean loss: 115.71
 ---- batch: 110 ----
mean loss: 115.67
train mean loss: 115.66
epoch train time: 0:00:01.902704
elapsed time: 0:08:08.635139
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 22:36:33.525248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.60
 ---- batch: 020 ----
mean loss: 118.49
 ---- batch: 030 ----
mean loss: 117.98
 ---- batch: 040 ----
mean loss: 115.37
 ---- batch: 050 ----
mean loss: 113.62
 ---- batch: 060 ----
mean loss: 120.98
 ---- batch: 070 ----
mean loss: 119.72
 ---- batch: 080 ----
mean loss: 118.56
 ---- batch: 090 ----
mean loss: 114.48
 ---- batch: 100 ----
mean loss: 112.51
 ---- batch: 110 ----
mean loss: 110.47
train mean loss: 115.64
epoch train time: 0:00:01.888074
elapsed time: 0:08:10.523756
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 22:36:35.413899
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.10
 ---- batch: 020 ----
mean loss: 113.05
 ---- batch: 030 ----
mean loss: 118.95
 ---- batch: 040 ----
mean loss: 115.18
 ---- batch: 050 ----
mean loss: 110.36
 ---- batch: 060 ----
mean loss: 114.75
 ---- batch: 070 ----
mean loss: 115.36
 ---- batch: 080 ----
mean loss: 116.27
 ---- batch: 090 ----
mean loss: 127.37
 ---- batch: 100 ----
mean loss: 108.83
 ---- batch: 110 ----
mean loss: 117.69
train mean loss: 115.59
epoch train time: 0:00:01.915170
elapsed time: 0:08:12.439501
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 22:36:37.329704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.57
 ---- batch: 020 ----
mean loss: 112.40
 ---- batch: 030 ----
mean loss: 112.80
 ---- batch: 040 ----
mean loss: 114.92
 ---- batch: 050 ----
mean loss: 118.16
 ---- batch: 060 ----
mean loss: 116.29
 ---- batch: 070 ----
mean loss: 117.88
 ---- batch: 080 ----
mean loss: 118.18
 ---- batch: 090 ----
mean loss: 118.89
 ---- batch: 100 ----
mean loss: 118.88
 ---- batch: 110 ----
mean loss: 111.31
train mean loss: 115.56
epoch train time: 0:00:01.906886
elapsed time: 0:08:14.347031
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 22:36:39.237177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.41
 ---- batch: 020 ----
mean loss: 122.10
 ---- batch: 030 ----
mean loss: 119.11
 ---- batch: 040 ----
mean loss: 110.28
 ---- batch: 050 ----
mean loss: 112.84
 ---- batch: 060 ----
mean loss: 116.86
 ---- batch: 070 ----
mean loss: 115.24
 ---- batch: 080 ----
mean loss: 106.30
 ---- batch: 090 ----
mean loss: 112.55
 ---- batch: 100 ----
mean loss: 124.36
 ---- batch: 110 ----
mean loss: 119.02
train mean loss: 115.59
epoch train time: 0:00:01.903135
elapsed time: 0:08:16.250777
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 22:36:41.140957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.87
 ---- batch: 020 ----
mean loss: 122.51
 ---- batch: 030 ----
mean loss: 110.27
 ---- batch: 040 ----
mean loss: 123.82
 ---- batch: 050 ----
mean loss: 116.38
 ---- batch: 060 ----
mean loss: 110.37
 ---- batch: 070 ----
mean loss: 121.83
 ---- batch: 080 ----
mean loss: 108.76
 ---- batch: 090 ----
mean loss: 110.56
 ---- batch: 100 ----
mean loss: 123.57
 ---- batch: 110 ----
mean loss: 112.83
train mean loss: 115.54
epoch train time: 0:00:01.935295
elapsed time: 0:08:18.186709
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 22:36:43.076897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.34
 ---- batch: 020 ----
mean loss: 111.13
 ---- batch: 030 ----
mean loss: 112.67
 ---- batch: 040 ----
mean loss: 114.60
 ---- batch: 050 ----
mean loss: 117.99
 ---- batch: 060 ----
mean loss: 113.05
 ---- batch: 070 ----
mean loss: 118.56
 ---- batch: 080 ----
mean loss: 119.44
 ---- batch: 090 ----
mean loss: 114.21
 ---- batch: 100 ----
mean loss: 119.67
 ---- batch: 110 ----
mean loss: 117.83
train mean loss: 115.45
epoch train time: 0:00:01.909632
elapsed time: 0:08:20.097020
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 22:36:44.987158
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.01
 ---- batch: 020 ----
mean loss: 115.25
 ---- batch: 030 ----
mean loss: 116.37
 ---- batch: 040 ----
mean loss: 114.56
 ---- batch: 050 ----
mean loss: 108.84
 ---- batch: 060 ----
mean loss: 120.68
 ---- batch: 070 ----
mean loss: 118.78
 ---- batch: 080 ----
mean loss: 109.89
 ---- batch: 090 ----
mean loss: 115.05
 ---- batch: 100 ----
mean loss: 118.22
 ---- batch: 110 ----
mean loss: 121.98
train mean loss: 115.40
epoch train time: 0:00:01.888667
elapsed time: 0:08:21.986259
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 22:36:46.876373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.18
 ---- batch: 020 ----
mean loss: 117.10
 ---- batch: 030 ----
mean loss: 112.45
 ---- batch: 040 ----
mean loss: 113.71
 ---- batch: 050 ----
mean loss: 117.62
 ---- batch: 060 ----
mean loss: 112.71
 ---- batch: 070 ----
mean loss: 119.50
 ---- batch: 080 ----
mean loss: 114.46
 ---- batch: 090 ----
mean loss: 114.38
 ---- batch: 100 ----
mean loss: 119.43
 ---- batch: 110 ----
mean loss: 109.44
train mean loss: 115.45
epoch train time: 0:00:01.915146
elapsed time: 0:08:23.902003
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 22:36:48.792165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.86
 ---- batch: 020 ----
mean loss: 117.64
 ---- batch: 030 ----
mean loss: 116.32
 ---- batch: 040 ----
mean loss: 112.21
 ---- batch: 050 ----
mean loss: 116.16
 ---- batch: 060 ----
mean loss: 119.21
 ---- batch: 070 ----
mean loss: 116.94
 ---- batch: 080 ----
mean loss: 106.53
 ---- batch: 090 ----
mean loss: 117.67
 ---- batch: 100 ----
mean loss: 118.28
 ---- batch: 110 ----
mean loss: 122.80
train mean loss: 115.27
epoch train time: 0:00:01.860551
elapsed time: 0:08:25.763171
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 22:36:50.653301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.83
 ---- batch: 020 ----
mean loss: 113.58
 ---- batch: 030 ----
mean loss: 112.68
 ---- batch: 040 ----
mean loss: 117.82
 ---- batch: 050 ----
mean loss: 111.54
 ---- batch: 060 ----
mean loss: 117.95
 ---- batch: 070 ----
mean loss: 111.62
 ---- batch: 080 ----
mean loss: 110.64
 ---- batch: 090 ----
mean loss: 118.85
 ---- batch: 100 ----
mean loss: 120.13
 ---- batch: 110 ----
mean loss: 119.77
train mean loss: 115.26
epoch train time: 0:00:01.880391
elapsed time: 0:08:27.644113
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 22:36:52.534273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.37
 ---- batch: 020 ----
mean loss: 110.87
 ---- batch: 030 ----
mean loss: 110.79
 ---- batch: 040 ----
mean loss: 115.08
 ---- batch: 050 ----
mean loss: 115.31
 ---- batch: 060 ----
mean loss: 122.90
 ---- batch: 070 ----
mean loss: 112.81
 ---- batch: 080 ----
mean loss: 114.34
 ---- batch: 090 ----
mean loss: 115.94
 ---- batch: 100 ----
mean loss: 120.35
 ---- batch: 110 ----
mean loss: 114.41
train mean loss: 115.26
epoch train time: 0:00:01.906412
elapsed time: 0:08:29.559193
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_1/checkpoint.pth.tar
**** end time: 2019-09-26 22:36:54.448964 ****
