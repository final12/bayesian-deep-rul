Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 14311
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 22:37:11.945176 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 22:37:11.955230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4672.07
 ---- batch: 020 ----
mean loss: 4394.62
 ---- batch: 030 ----
mean loss: 4118.91
 ---- batch: 040 ----
mean loss: 3839.34
 ---- batch: 050 ----
mean loss: 3595.58
 ---- batch: 060 ----
mean loss: 3356.18
 ---- batch: 070 ----
mean loss: 3219.68
 ---- batch: 080 ----
mean loss: 3051.53
 ---- batch: 090 ----
mean loss: 2915.80
 ---- batch: 100 ----
mean loss: 2833.04
 ---- batch: 110 ----
mean loss: 2745.73
train mean loss: 3499.54
epoch train time: 0:00:34.088192
elapsed time: 0:00:34.104580
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 22:37:46.049798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2614.05
 ---- batch: 020 ----
mean loss: 2522.88
 ---- batch: 030 ----
mean loss: 2468.95
 ---- batch: 040 ----
mean loss: 2422.01
 ---- batch: 050 ----
mean loss: 2388.91
 ---- batch: 060 ----
mean loss: 2322.27
 ---- batch: 070 ----
mean loss: 2273.08
 ---- batch: 080 ----
mean loss: 2250.23
 ---- batch: 090 ----
mean loss: 2202.84
 ---- batch: 100 ----
mean loss: 2158.06
 ---- batch: 110 ----
mean loss: 2100.69
train mean loss: 2333.04
epoch train time: 0:00:01.897730
elapsed time: 0:00:36.002530
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 22:37:47.948076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2081.74
 ---- batch: 020 ----
mean loss: 2059.76
 ---- batch: 030 ----
mean loss: 2039.45
 ---- batch: 040 ----
mean loss: 2015.77
 ---- batch: 050 ----
mean loss: 1976.14
 ---- batch: 060 ----
mean loss: 1932.93
 ---- batch: 070 ----
mean loss: 1932.75
 ---- batch: 080 ----
mean loss: 1885.17
 ---- batch: 090 ----
mean loss: 1848.05
 ---- batch: 100 ----
mean loss: 1840.81
 ---- batch: 110 ----
mean loss: 1779.19
train mean loss: 1941.11
epoch train time: 0:00:01.859690
elapsed time: 0:00:37.862780
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 22:37:49.808347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1776.75
 ---- batch: 020 ----
mean loss: 1727.41
 ---- batch: 030 ----
mean loss: 1759.56
 ---- batch: 040 ----
mean loss: 1729.97
 ---- batch: 050 ----
mean loss: 1682.99
 ---- batch: 060 ----
mean loss: 1676.68
 ---- batch: 070 ----
mean loss: 1658.15
 ---- batch: 080 ----
mean loss: 1646.64
 ---- batch: 090 ----
mean loss: 1618.05
 ---- batch: 100 ----
mean loss: 1596.04
 ---- batch: 110 ----
mean loss: 1580.79
train mean loss: 1674.05
epoch train time: 0:00:01.876114
elapsed time: 0:00:39.739488
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 22:37:51.685048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1581.58
 ---- batch: 020 ----
mean loss: 1556.18
 ---- batch: 030 ----
mean loss: 1515.25
 ---- batch: 040 ----
mean loss: 1501.92
 ---- batch: 050 ----
mean loss: 1494.23
 ---- batch: 060 ----
mean loss: 1460.43
 ---- batch: 070 ----
mean loss: 1440.08
 ---- batch: 080 ----
mean loss: 1422.08
 ---- batch: 090 ----
mean loss: 1424.19
 ---- batch: 100 ----
mean loss: 1407.82
 ---- batch: 110 ----
mean loss: 1410.93
train mean loss: 1471.12
epoch train time: 0:00:01.898502
elapsed time: 0:00:41.638584
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 22:37:53.584171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1377.87
 ---- batch: 020 ----
mean loss: 1364.76
 ---- batch: 030 ----
mean loss: 1360.95
 ---- batch: 040 ----
mean loss: 1349.12
 ---- batch: 050 ----
mean loss: 1312.68
 ---- batch: 060 ----
mean loss: 1304.43
 ---- batch: 070 ----
mean loss: 1283.66
 ---- batch: 080 ----
mean loss: 1278.46
 ---- batch: 090 ----
mean loss: 1268.15
 ---- batch: 100 ----
mean loss: 1269.41
 ---- batch: 110 ----
mean loss: 1249.95
train mean loss: 1309.52
epoch train time: 0:00:01.890566
elapsed time: 0:00:43.529752
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 22:37:55.475323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1238.07
 ---- batch: 020 ----
mean loss: 1222.90
 ---- batch: 030 ----
mean loss: 1201.69
 ---- batch: 040 ----
mean loss: 1220.43
 ---- batch: 050 ----
mean loss: 1209.32
 ---- batch: 060 ----
mean loss: 1172.40
 ---- batch: 070 ----
mean loss: 1173.44
 ---- batch: 080 ----
mean loss: 1165.74
 ---- batch: 090 ----
mean loss: 1168.97
 ---- batch: 100 ----
mean loss: 1143.68
 ---- batch: 110 ----
mean loss: 1171.81
train mean loss: 1188.68
epoch train time: 0:00:01.907095
elapsed time: 0:00:45.437454
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 22:37:57.382997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1117.61
 ---- batch: 020 ----
mean loss: 1125.24
 ---- batch: 030 ----
mean loss: 1113.34
 ---- batch: 040 ----
mean loss: 1095.09
 ---- batch: 050 ----
mean loss: 1105.63
 ---- batch: 060 ----
mean loss: 1096.15
 ---- batch: 070 ----
mean loss: 1084.46
 ---- batch: 080 ----
mean loss: 1079.29
 ---- batch: 090 ----
mean loss: 1093.55
 ---- batch: 100 ----
mean loss: 1085.33
 ---- batch: 110 ----
mean loss: 1050.47
train mean loss: 1094.11
epoch train time: 0:00:01.906079
elapsed time: 0:00:47.344085
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 22:37:59.289631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1055.80
 ---- batch: 020 ----
mean loss: 1057.20
 ---- batch: 030 ----
mean loss: 1028.36
 ---- batch: 040 ----
mean loss: 1012.57
 ---- batch: 050 ----
mean loss: 1033.18
 ---- batch: 060 ----
mean loss: 1024.08
 ---- batch: 070 ----
mean loss: 1023.67
 ---- batch: 080 ----
mean loss: 1006.88
 ---- batch: 090 ----
mean loss: 1005.16
 ---- batch: 100 ----
mean loss: 1008.44
 ---- batch: 110 ----
mean loss: 1011.08
train mean loss: 1022.99
epoch train time: 0:00:01.866694
elapsed time: 0:00:49.211334
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 22:38:01.156906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 988.13
 ---- batch: 020 ----
mean loss: 1000.59
 ---- batch: 030 ----
mean loss: 970.85
 ---- batch: 040 ----
mean loss: 976.69
 ---- batch: 050 ----
mean loss: 975.77
 ---- batch: 060 ----
mean loss: 979.06
 ---- batch: 070 ----
mean loss: 964.83
 ---- batch: 080 ----
mean loss: 982.31
 ---- batch: 090 ----
mean loss: 959.05
 ---- batch: 100 ----
mean loss: 943.74
 ---- batch: 110 ----
mean loss: 964.31
train mean loss: 972.92
epoch train time: 0:00:01.886110
elapsed time: 0:00:51.098030
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 22:38:03.043596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.14
 ---- batch: 020 ----
mean loss: 939.70
 ---- batch: 030 ----
mean loss: 940.21
 ---- batch: 040 ----
mean loss: 950.59
 ---- batch: 050 ----
mean loss: 935.08
 ---- batch: 060 ----
mean loss: 928.55
 ---- batch: 070 ----
mean loss: 922.67
 ---- batch: 080 ----
mean loss: 924.57
 ---- batch: 090 ----
mean loss: 928.71
 ---- batch: 100 ----
mean loss: 927.50
 ---- batch: 110 ----
mean loss: 912.78
train mean loss: 932.43
epoch train time: 0:00:01.893386
elapsed time: 0:00:52.992005
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 22:38:04.937366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.23
 ---- batch: 020 ----
mean loss: 901.29
 ---- batch: 030 ----
mean loss: 916.71
 ---- batch: 040 ----
mean loss: 908.99
 ---- batch: 050 ----
mean loss: 894.80
 ---- batch: 060 ----
mean loss: 906.10
 ---- batch: 070 ----
mean loss: 910.55
 ---- batch: 080 ----
mean loss: 900.59
 ---- batch: 090 ----
mean loss: 900.42
 ---- batch: 100 ----
mean loss: 896.84
 ---- batch: 110 ----
mean loss: 884.95
train mean loss: 906.09
epoch train time: 0:00:01.911344
elapsed time: 0:00:54.903723
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 22:38:06.849250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.29
 ---- batch: 020 ----
mean loss: 901.43
 ---- batch: 030 ----
mean loss: 902.33
 ---- batch: 040 ----
mean loss: 894.64
 ---- batch: 050 ----
mean loss: 883.08
 ---- batch: 060 ----
mean loss: 873.91
 ---- batch: 070 ----
mean loss: 899.27
 ---- batch: 080 ----
mean loss: 869.23
 ---- batch: 090 ----
mean loss: 882.85
 ---- batch: 100 ----
mean loss: 898.73
 ---- batch: 110 ----
mean loss: 870.69
train mean loss: 889.68
epoch train time: 0:00:01.868162
elapsed time: 0:00:56.772456
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 22:38:08.718007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.92
 ---- batch: 020 ----
mean loss: 879.81
 ---- batch: 030 ----
mean loss: 875.94
 ---- batch: 040 ----
mean loss: 868.84
 ---- batch: 050 ----
mean loss: 870.38
 ---- batch: 060 ----
mean loss: 890.57
 ---- batch: 070 ----
mean loss: 881.63
 ---- batch: 080 ----
mean loss: 885.02
 ---- batch: 090 ----
mean loss: 865.97
 ---- batch: 100 ----
mean loss: 878.51
 ---- batch: 110 ----
mean loss: 880.36
train mean loss: 877.94
epoch train time: 0:00:01.882921
elapsed time: 0:00:58.655933
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 22:38:10.601498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.88
 ---- batch: 020 ----
mean loss: 870.08
 ---- batch: 030 ----
mean loss: 883.60
 ---- batch: 040 ----
mean loss: 885.49
 ---- batch: 050 ----
mean loss: 872.17
 ---- batch: 060 ----
mean loss: 863.95
 ---- batch: 070 ----
mean loss: 863.81
 ---- batch: 080 ----
mean loss: 856.90
 ---- batch: 090 ----
mean loss: 862.78
 ---- batch: 100 ----
mean loss: 861.28
 ---- batch: 110 ----
mean loss: 881.00
train mean loss: 869.44
epoch train time: 0:00:01.887999
elapsed time: 0:01:00.544540
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 22:38:12.490053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.83
 ---- batch: 020 ----
mean loss: 863.19
 ---- batch: 030 ----
mean loss: 863.11
 ---- batch: 040 ----
mean loss: 850.22
 ---- batch: 050 ----
mean loss: 864.88
 ---- batch: 060 ----
mean loss: 873.75
 ---- batch: 070 ----
mean loss: 883.08
 ---- batch: 080 ----
mean loss: 855.72
 ---- batch: 090 ----
mean loss: 861.51
 ---- batch: 100 ----
mean loss: 876.25
 ---- batch: 110 ----
mean loss: 852.70
train mean loss: 866.08
epoch train time: 0:00:01.896776
elapsed time: 0:01:02.441823
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 22:38:14.387424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.06
 ---- batch: 020 ----
mean loss: 832.17
 ---- batch: 030 ----
mean loss: 858.60
 ---- batch: 040 ----
mean loss: 890.24
 ---- batch: 050 ----
mean loss: 880.75
 ---- batch: 060 ----
mean loss: 878.66
 ---- batch: 070 ----
mean loss: 871.46
 ---- batch: 080 ----
mean loss: 862.68
 ---- batch: 090 ----
mean loss: 843.42
 ---- batch: 100 ----
mean loss: 848.19
 ---- batch: 110 ----
mean loss: 848.80
train mean loss: 861.21
epoch train time: 0:00:01.884707
elapsed time: 0:01:04.327139
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 22:38:16.272702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.91
 ---- batch: 020 ----
mean loss: 862.41
 ---- batch: 030 ----
mean loss: 850.87
 ---- batch: 040 ----
mean loss: 862.62
 ---- batch: 050 ----
mean loss: 873.91
 ---- batch: 060 ----
mean loss: 840.78
 ---- batch: 070 ----
mean loss: 874.23
 ---- batch: 080 ----
mean loss: 857.34
 ---- batch: 090 ----
mean loss: 852.98
 ---- batch: 100 ----
mean loss: 873.59
 ---- batch: 110 ----
mean loss: 870.08
train mean loss: 859.41
epoch train time: 0:00:01.877076
elapsed time: 0:01:06.204783
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 22:38:18.150410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.64
 ---- batch: 020 ----
mean loss: 865.29
 ---- batch: 030 ----
mean loss: 854.39
 ---- batch: 040 ----
mean loss: 837.02
 ---- batch: 050 ----
mean loss: 847.67
 ---- batch: 060 ----
mean loss: 863.26
 ---- batch: 070 ----
mean loss: 850.44
 ---- batch: 080 ----
mean loss: 856.11
 ---- batch: 090 ----
mean loss: 868.65
 ---- batch: 100 ----
mean loss: 849.67
 ---- batch: 110 ----
mean loss: 875.71
train mean loss: 856.52
epoch train time: 0:00:01.849635
elapsed time: 0:01:08.055075
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 22:38:20.000645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.72
 ---- batch: 020 ----
mean loss: 864.77
 ---- batch: 030 ----
mean loss: 869.52
 ---- batch: 040 ----
mean loss: 849.91
 ---- batch: 050 ----
mean loss: 836.97
 ---- batch: 060 ----
mean loss: 862.83
 ---- batch: 070 ----
mean loss: 851.13
 ---- batch: 080 ----
mean loss: 861.16
 ---- batch: 090 ----
mean loss: 847.11
 ---- batch: 100 ----
mean loss: 858.48
 ---- batch: 110 ----
mean loss: 859.35
train mean loss: 855.06
epoch train time: 0:00:01.861476
elapsed time: 0:01:09.917126
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 22:38:21.862637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.80
 ---- batch: 020 ----
mean loss: 896.60
 ---- batch: 030 ----
mean loss: 851.54
 ---- batch: 040 ----
mean loss: 876.90
 ---- batch: 050 ----
mean loss: 861.50
 ---- batch: 060 ----
mean loss: 871.43
 ---- batch: 070 ----
mean loss: 847.20
 ---- batch: 080 ----
mean loss: 866.18
 ---- batch: 090 ----
mean loss: 843.00
 ---- batch: 100 ----
mean loss: 839.32
 ---- batch: 110 ----
mean loss: 844.70
train mean loss: 855.52
epoch train time: 0:00:01.847102
elapsed time: 0:01:11.764766
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 22:38:23.710141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.35
 ---- batch: 020 ----
mean loss: 873.89
 ---- batch: 030 ----
mean loss: 837.00
 ---- batch: 040 ----
mean loss: 867.23
 ---- batch: 050 ----
mean loss: 862.47
 ---- batch: 060 ----
mean loss: 850.73
 ---- batch: 070 ----
mean loss: 862.91
 ---- batch: 080 ----
mean loss: 843.81
 ---- batch: 090 ----
mean loss: 846.70
 ---- batch: 100 ----
mean loss: 839.28
 ---- batch: 110 ----
mean loss: 857.90
train mean loss: 853.65
epoch train time: 0:00:01.896731
elapsed time: 0:01:13.661903
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 22:38:25.607552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.80
 ---- batch: 020 ----
mean loss: 841.48
 ---- batch: 030 ----
mean loss: 832.69
 ---- batch: 040 ----
mean loss: 858.72
 ---- batch: 050 ----
mean loss: 877.66
 ---- batch: 060 ----
mean loss: 843.73
 ---- batch: 070 ----
mean loss: 874.97
 ---- batch: 080 ----
mean loss: 841.22
 ---- batch: 090 ----
mean loss: 855.17
 ---- batch: 100 ----
mean loss: 870.63
 ---- batch: 110 ----
mean loss: 857.15
train mean loss: 854.10
epoch train time: 0:00:01.896342
elapsed time: 0:01:15.558935
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 22:38:27.504512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.37
 ---- batch: 020 ----
mean loss: 856.29
 ---- batch: 030 ----
mean loss: 868.05
 ---- batch: 040 ----
mean loss: 855.22
 ---- batch: 050 ----
mean loss: 873.23
 ---- batch: 060 ----
mean loss: 843.35
 ---- batch: 070 ----
mean loss: 842.16
 ---- batch: 080 ----
mean loss: 865.76
 ---- batch: 090 ----
mean loss: 854.19
 ---- batch: 100 ----
mean loss: 858.37
 ---- batch: 110 ----
mean loss: 838.84
train mean loss: 854.67
epoch train time: 0:00:01.917495
elapsed time: 0:01:17.477057
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 22:38:29.422600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.58
 ---- batch: 020 ----
mean loss: 851.30
 ---- batch: 030 ----
mean loss: 839.11
 ---- batch: 040 ----
mean loss: 863.84
 ---- batch: 050 ----
mean loss: 856.92
 ---- batch: 060 ----
mean loss: 869.51
 ---- batch: 070 ----
mean loss: 826.36
 ---- batch: 080 ----
mean loss: 853.58
 ---- batch: 090 ----
mean loss: 863.41
 ---- batch: 100 ----
mean loss: 844.94
 ---- batch: 110 ----
mean loss: 864.78
train mean loss: 853.37
epoch train time: 0:00:01.906794
elapsed time: 0:01:19.384410
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 22:38:31.329998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 853.59
 ---- batch: 030 ----
mean loss: 846.08
 ---- batch: 040 ----
mean loss: 848.67
 ---- batch: 050 ----
mean loss: 842.05
 ---- batch: 060 ----
mean loss: 853.88
 ---- batch: 070 ----
mean loss: 872.44
 ---- batch: 080 ----
mean loss: 840.90
 ---- batch: 090 ----
mean loss: 855.93
 ---- batch: 100 ----
mean loss: 846.96
 ---- batch: 110 ----
mean loss: 847.78
train mean loss: 851.03
epoch train time: 0:00:01.922744
elapsed time: 0:01:21.307788
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 22:38:33.253346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.51
 ---- batch: 020 ----
mean loss: 859.64
 ---- batch: 030 ----
mean loss: 866.14
 ---- batch: 040 ----
mean loss: 853.23
 ---- batch: 050 ----
mean loss: 848.04
 ---- batch: 060 ----
mean loss: 828.43
 ---- batch: 070 ----
mean loss: 838.97
 ---- batch: 080 ----
mean loss: 867.32
 ---- batch: 090 ----
mean loss: 872.57
 ---- batch: 100 ----
mean loss: 842.68
 ---- batch: 110 ----
mean loss: 849.55
train mean loss: 852.34
epoch train time: 0:00:01.890661
elapsed time: 0:01:23.199053
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 22:38:35.144786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.29
 ---- batch: 020 ----
mean loss: 841.56
 ---- batch: 030 ----
mean loss: 863.95
 ---- batch: 040 ----
mean loss: 871.38
 ---- batch: 050 ----
mean loss: 846.93
 ---- batch: 060 ----
mean loss: 844.39
 ---- batch: 070 ----
mean loss: 834.45
 ---- batch: 080 ----
mean loss: 854.61
 ---- batch: 090 ----
mean loss: 860.69
 ---- batch: 100 ----
mean loss: 842.87
 ---- batch: 110 ----
mean loss: 852.49
train mean loss: 850.56
epoch train time: 0:00:01.867679
elapsed time: 0:01:25.067523
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 22:38:37.013072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.96
 ---- batch: 020 ----
mean loss: 846.35
 ---- batch: 030 ----
mean loss: 863.32
 ---- batch: 040 ----
mean loss: 869.80
 ---- batch: 050 ----
mean loss: 866.09
 ---- batch: 060 ----
mean loss: 849.21
 ---- batch: 070 ----
mean loss: 858.14
 ---- batch: 080 ----
mean loss: 851.61
 ---- batch: 090 ----
mean loss: 830.09
 ---- batch: 100 ----
mean loss: 860.02
 ---- batch: 110 ----
mean loss: 843.17
train mean loss: 850.93
epoch train time: 0:00:01.938250
elapsed time: 0:01:27.006336
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 22:38:38.951887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.34
 ---- batch: 020 ----
mean loss: 825.25
 ---- batch: 030 ----
mean loss: 842.26
 ---- batch: 040 ----
mean loss: 851.76
 ---- batch: 050 ----
mean loss: 853.25
 ---- batch: 060 ----
mean loss: 851.63
 ---- batch: 070 ----
mean loss: 854.20
 ---- batch: 080 ----
mean loss: 863.51
 ---- batch: 090 ----
mean loss: 848.62
 ---- batch: 100 ----
mean loss: 854.44
 ---- batch: 110 ----
mean loss: 845.96
train mean loss: 849.98
epoch train time: 0:00:01.920697
elapsed time: 0:01:28.927587
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 22:38:40.873281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.04
 ---- batch: 020 ----
mean loss: 841.41
 ---- batch: 030 ----
mean loss: 852.10
 ---- batch: 040 ----
mean loss: 865.67
 ---- batch: 050 ----
mean loss: 849.89
 ---- batch: 060 ----
mean loss: 841.07
 ---- batch: 070 ----
mean loss: 815.05
 ---- batch: 080 ----
mean loss: 856.50
 ---- batch: 090 ----
mean loss: 853.77
 ---- batch: 100 ----
mean loss: 857.91
 ---- batch: 110 ----
mean loss: 862.49
train mean loss: 849.05
epoch train time: 0:00:01.878143
elapsed time: 0:01:30.806441
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 22:38:42.751969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.45
 ---- batch: 020 ----
mean loss: 839.46
 ---- batch: 030 ----
mean loss: 868.24
 ---- batch: 040 ----
mean loss: 875.09
 ---- batch: 050 ----
mean loss: 821.45
 ---- batch: 060 ----
mean loss: 836.14
 ---- batch: 070 ----
mean loss: 862.14
 ---- batch: 080 ----
mean loss: 851.07
 ---- batch: 090 ----
mean loss: 847.34
 ---- batch: 100 ----
mean loss: 849.58
 ---- batch: 110 ----
mean loss: 836.82
train mean loss: 848.95
epoch train time: 0:00:01.883271
elapsed time: 0:01:32.690257
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 22:38:44.635810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.13
 ---- batch: 020 ----
mean loss: 840.09
 ---- batch: 030 ----
mean loss: 859.89
 ---- batch: 040 ----
mean loss: 869.77
 ---- batch: 050 ----
mean loss: 855.06
 ---- batch: 060 ----
mean loss: 857.44
 ---- batch: 070 ----
mean loss: 833.33
 ---- batch: 080 ----
mean loss: 858.23
 ---- batch: 090 ----
mean loss: 844.91
 ---- batch: 100 ----
mean loss: 859.43
 ---- batch: 110 ----
mean loss: 839.27
train mean loss: 849.99
epoch train time: 0:00:01.887990
elapsed time: 0:01:34.578825
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 22:38:46.524392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.17
 ---- batch: 020 ----
mean loss: 852.86
 ---- batch: 030 ----
mean loss: 846.26
 ---- batch: 040 ----
mean loss: 847.62
 ---- batch: 050 ----
mean loss: 831.97
 ---- batch: 060 ----
mean loss: 842.64
 ---- batch: 070 ----
mean loss: 846.53
 ---- batch: 080 ----
mean loss: 864.85
 ---- batch: 090 ----
mean loss: 861.16
 ---- batch: 100 ----
mean loss: 852.18
 ---- batch: 110 ----
mean loss: 862.39
train mean loss: 849.21
epoch train time: 0:00:01.887368
elapsed time: 0:01:36.466836
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 22:38:48.412226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.34
 ---- batch: 020 ----
mean loss: 815.29
 ---- batch: 030 ----
mean loss: 842.67
 ---- batch: 040 ----
mean loss: 857.62
 ---- batch: 050 ----
mean loss: 852.99
 ---- batch: 060 ----
mean loss: 860.20
 ---- batch: 070 ----
mean loss: 839.50
 ---- batch: 080 ----
mean loss: 853.56
 ---- batch: 090 ----
mean loss: 858.41
 ---- batch: 100 ----
mean loss: 855.17
 ---- batch: 110 ----
mean loss: 842.96
train mean loss: 847.77
epoch train time: 0:00:01.882730
elapsed time: 0:01:38.349974
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 22:38:50.295566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.22
 ---- batch: 020 ----
mean loss: 867.40
 ---- batch: 030 ----
mean loss: 842.81
 ---- batch: 040 ----
mean loss: 848.21
 ---- batch: 050 ----
mean loss: 842.67
 ---- batch: 060 ----
mean loss: 835.32
 ---- batch: 070 ----
mean loss: 846.40
 ---- batch: 080 ----
mean loss: 855.32
 ---- batch: 090 ----
mean loss: 837.26
 ---- batch: 100 ----
mean loss: 840.00
 ---- batch: 110 ----
mean loss: 838.86
train mean loss: 848.49
epoch train time: 0:00:01.884554
elapsed time: 0:01:40.235162
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 22:38:52.180723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.70
 ---- batch: 020 ----
mean loss: 846.27
 ---- batch: 030 ----
mean loss: 858.75
 ---- batch: 040 ----
mean loss: 848.03
 ---- batch: 050 ----
mean loss: 828.14
 ---- batch: 060 ----
mean loss: 854.35
 ---- batch: 070 ----
mean loss: 842.06
 ---- batch: 080 ----
mean loss: 831.81
 ---- batch: 090 ----
mean loss: 868.75
 ---- batch: 100 ----
mean loss: 856.93
 ---- batch: 110 ----
mean loss: 832.10
train mean loss: 848.25
epoch train time: 0:00:01.898832
elapsed time: 0:01:42.134586
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 22:38:54.080155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.18
 ---- batch: 020 ----
mean loss: 859.44
 ---- batch: 030 ----
mean loss: 834.74
 ---- batch: 040 ----
mean loss: 840.28
 ---- batch: 050 ----
mean loss: 871.28
 ---- batch: 060 ----
mean loss: 837.66
 ---- batch: 070 ----
mean loss: 844.22
 ---- batch: 080 ----
mean loss: 832.49
 ---- batch: 090 ----
mean loss: 828.85
 ---- batch: 100 ----
mean loss: 857.94
 ---- batch: 110 ----
mean loss: 856.33
train mean loss: 847.38
epoch train time: 0:00:01.851924
elapsed time: 0:01:43.987117
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 22:38:55.932710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.00
 ---- batch: 020 ----
mean loss: 840.81
 ---- batch: 030 ----
mean loss: 854.44
 ---- batch: 040 ----
mean loss: 864.19
 ---- batch: 050 ----
mean loss: 822.17
 ---- batch: 060 ----
mean loss: 837.00
 ---- batch: 070 ----
mean loss: 861.08
 ---- batch: 080 ----
mean loss: 860.29
 ---- batch: 090 ----
mean loss: 831.34
 ---- batch: 100 ----
mean loss: 847.32
 ---- batch: 110 ----
mean loss: 848.14
train mean loss: 848.48
epoch train time: 0:00:01.896515
elapsed time: 0:01:45.884242
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 22:38:57.829813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.11
 ---- batch: 020 ----
mean loss: 850.39
 ---- batch: 030 ----
mean loss: 844.17
 ---- batch: 040 ----
mean loss: 848.57
 ---- batch: 050 ----
mean loss: 839.08
 ---- batch: 060 ----
mean loss: 839.46
 ---- batch: 070 ----
mean loss: 842.68
 ---- batch: 080 ----
mean loss: 857.77
 ---- batch: 090 ----
mean loss: 846.91
 ---- batch: 100 ----
mean loss: 854.80
 ---- batch: 110 ----
mean loss: 853.69
train mean loss: 847.78
epoch train time: 0:00:01.898447
elapsed time: 0:01:47.783289
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 22:38:59.728824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.63
 ---- batch: 020 ----
mean loss: 858.82
 ---- batch: 030 ----
mean loss: 834.01
 ---- batch: 040 ----
mean loss: 863.73
 ---- batch: 050 ----
mean loss: 856.90
 ---- batch: 060 ----
mean loss: 849.52
 ---- batch: 070 ----
mean loss: 837.89
 ---- batch: 080 ----
mean loss: 849.34
 ---- batch: 090 ----
mean loss: 838.03
 ---- batch: 100 ----
mean loss: 840.55
 ---- batch: 110 ----
mean loss: 847.82
train mean loss: 847.01
epoch train time: 0:00:01.888529
elapsed time: 0:01:49.672362
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 22:39:01.618075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.97
 ---- batch: 020 ----
mean loss: 831.33
 ---- batch: 030 ----
mean loss: 840.77
 ---- batch: 040 ----
mean loss: 853.12
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 854.42
 ---- batch: 070 ----
mean loss: 847.65
 ---- batch: 080 ----
mean loss: 868.73
 ---- batch: 090 ----
mean loss: 858.45
 ---- batch: 100 ----
mean loss: 844.95
 ---- batch: 110 ----
mean loss: 827.28
train mean loss: 847.44
epoch train time: 0:00:01.865047
elapsed time: 0:01:51.538164
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 22:39:03.483751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.40
 ---- batch: 020 ----
mean loss: 837.33
 ---- batch: 030 ----
mean loss: 852.01
 ---- batch: 040 ----
mean loss: 839.55
 ---- batch: 050 ----
mean loss: 870.17
 ---- batch: 060 ----
mean loss: 835.22
 ---- batch: 070 ----
mean loss: 849.14
 ---- batch: 080 ----
mean loss: 858.64
 ---- batch: 090 ----
mean loss: 846.28
 ---- batch: 100 ----
mean loss: 843.32
 ---- batch: 110 ----
mean loss: 843.12
train mean loss: 847.25
epoch train time: 0:00:01.872000
elapsed time: 0:01:53.410755
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 22:39:05.356288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.72
 ---- batch: 020 ----
mean loss: 849.68
 ---- batch: 030 ----
mean loss: 858.08
 ---- batch: 040 ----
mean loss: 837.81
 ---- batch: 050 ----
mean loss: 848.94
 ---- batch: 060 ----
mean loss: 844.67
 ---- batch: 070 ----
mean loss: 813.47
 ---- batch: 080 ----
mean loss: 856.29
 ---- batch: 090 ----
mean loss: 848.90
 ---- batch: 100 ----
mean loss: 852.08
 ---- batch: 110 ----
mean loss: 853.44
train mean loss: 846.74
epoch train time: 0:00:01.890231
elapsed time: 0:01:55.301530
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 22:39:07.247080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.23
 ---- batch: 020 ----
mean loss: 836.23
 ---- batch: 030 ----
mean loss: 851.86
 ---- batch: 040 ----
mean loss: 862.64
 ---- batch: 050 ----
mean loss: 836.79
 ---- batch: 060 ----
mean loss: 868.35
 ---- batch: 070 ----
mean loss: 854.65
 ---- batch: 080 ----
mean loss: 835.64
 ---- batch: 090 ----
mean loss: 824.83
 ---- batch: 100 ----
mean loss: 831.70
 ---- batch: 110 ----
mean loss: 846.32
train mean loss: 846.40
epoch train time: 0:00:01.902328
elapsed time: 0:01:57.204468
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 22:39:09.150074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.15
 ---- batch: 020 ----
mean loss: 872.83
 ---- batch: 030 ----
mean loss: 860.27
 ---- batch: 040 ----
mean loss: 852.86
 ---- batch: 050 ----
mean loss: 835.14
 ---- batch: 060 ----
mean loss: 845.60
 ---- batch: 070 ----
mean loss: 840.71
 ---- batch: 080 ----
mean loss: 845.55
 ---- batch: 090 ----
mean loss: 845.43
 ---- batch: 100 ----
mean loss: 845.80
 ---- batch: 110 ----
mean loss: 855.72
train mean loss: 846.63
epoch train time: 0:00:01.876264
elapsed time: 0:01:59.081411
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 22:39:11.027019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.72
 ---- batch: 020 ----
mean loss: 828.59
 ---- batch: 030 ----
mean loss: 859.20
 ---- batch: 040 ----
mean loss: 856.81
 ---- batch: 050 ----
mean loss: 831.29
 ---- batch: 060 ----
mean loss: 829.01
 ---- batch: 070 ----
mean loss: 864.75
 ---- batch: 080 ----
mean loss: 824.17
 ---- batch: 090 ----
mean loss: 872.14
 ---- batch: 100 ----
mean loss: 841.72
 ---- batch: 110 ----
mean loss: 857.80
train mean loss: 846.63
epoch train time: 0:00:01.869295
elapsed time: 0:02:00.951379
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 22:39:12.896935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.81
 ---- batch: 020 ----
mean loss: 842.75
 ---- batch: 030 ----
mean loss: 833.65
 ---- batch: 040 ----
mean loss: 834.44
 ---- batch: 050 ----
mean loss: 846.04
 ---- batch: 060 ----
mean loss: 842.89
 ---- batch: 070 ----
mean loss: 867.81
 ---- batch: 080 ----
mean loss: 852.05
 ---- batch: 090 ----
mean loss: 866.97
 ---- batch: 100 ----
mean loss: 833.19
 ---- batch: 110 ----
mean loss: 842.12
train mean loss: 846.25
epoch train time: 0:00:01.918108
elapsed time: 0:02:02.870103
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 22:39:14.815801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.48
 ---- batch: 020 ----
mean loss: 845.48
 ---- batch: 030 ----
mean loss: 854.63
 ---- batch: 040 ----
mean loss: 859.09
 ---- batch: 050 ----
mean loss: 835.47
 ---- batch: 060 ----
mean loss: 856.16
 ---- batch: 070 ----
mean loss: 868.59
 ---- batch: 080 ----
mean loss: 834.34
 ---- batch: 090 ----
mean loss: 837.70
 ---- batch: 100 ----
mean loss: 853.41
 ---- batch: 110 ----
mean loss: 829.45
train mean loss: 846.74
epoch train time: 0:00:01.876638
elapsed time: 0:02:04.747474
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 22:39:16.693009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.50
 ---- batch: 020 ----
mean loss: 851.59
 ---- batch: 030 ----
mean loss: 856.65
 ---- batch: 040 ----
mean loss: 835.42
 ---- batch: 050 ----
mean loss: 836.22
 ---- batch: 060 ----
mean loss: 848.88
 ---- batch: 070 ----
mean loss: 830.97
 ---- batch: 080 ----
mean loss: 847.04
 ---- batch: 090 ----
mean loss: 849.07
 ---- batch: 100 ----
mean loss: 841.34
 ---- batch: 110 ----
mean loss: 849.96
train mean loss: 845.94
epoch train time: 0:00:01.860766
elapsed time: 0:02:06.608822
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 22:39:18.554365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.09
 ---- batch: 020 ----
mean loss: 846.09
 ---- batch: 030 ----
mean loss: 864.52
 ---- batch: 040 ----
mean loss: 854.31
 ---- batch: 050 ----
mean loss: 837.25
 ---- batch: 060 ----
mean loss: 839.40
 ---- batch: 070 ----
mean loss: 858.92
 ---- batch: 080 ----
mean loss: 844.73
 ---- batch: 090 ----
mean loss: 821.90
 ---- batch: 100 ----
mean loss: 796.66
 ---- batch: 110 ----
mean loss: 803.29
train mean loss: 835.82
epoch train time: 0:00:01.879043
elapsed time: 0:02:08.488448
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 22:39:20.433996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 782.34
 ---- batch: 020 ----
mean loss: 738.64
 ---- batch: 030 ----
mean loss: 684.66
 ---- batch: 040 ----
mean loss: 626.45
 ---- batch: 050 ----
mean loss: 572.42
 ---- batch: 060 ----
mean loss: 541.19
 ---- batch: 070 ----
mean loss: 501.95
 ---- batch: 080 ----
mean loss: 480.08
 ---- batch: 090 ----
mean loss: 453.06
 ---- batch: 100 ----
mean loss: 439.92
 ---- batch: 110 ----
mean loss: 436.18
train mean loss: 564.56
epoch train time: 0:00:01.888955
elapsed time: 0:02:10.378048
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 22:39:22.323608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.20
 ---- batch: 020 ----
mean loss: 409.81
 ---- batch: 030 ----
mean loss: 408.98
 ---- batch: 040 ----
mean loss: 397.77
 ---- batch: 050 ----
mean loss: 386.75
 ---- batch: 060 ----
mean loss: 382.97
 ---- batch: 070 ----
mean loss: 387.99
 ---- batch: 080 ----
mean loss: 376.97
 ---- batch: 090 ----
mean loss: 364.61
 ---- batch: 100 ----
mean loss: 362.68
 ---- batch: 110 ----
mean loss: 374.86
train mean loss: 388.82
epoch train time: 0:00:01.893459
elapsed time: 0:02:12.272077
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 22:39:24.217661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.96
 ---- batch: 020 ----
mean loss: 368.53
 ---- batch: 030 ----
mean loss: 361.27
 ---- batch: 040 ----
mean loss: 352.32
 ---- batch: 050 ----
mean loss: 345.01
 ---- batch: 060 ----
mean loss: 357.30
 ---- batch: 070 ----
mean loss: 333.02
 ---- batch: 080 ----
mean loss: 340.72
 ---- batch: 090 ----
mean loss: 348.72
 ---- batch: 100 ----
mean loss: 340.16
 ---- batch: 110 ----
mean loss: 342.09
train mean loss: 350.14
epoch train time: 0:00:01.883196
elapsed time: 0:02:14.155916
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 22:39:26.101503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.39
 ---- batch: 020 ----
mean loss: 323.42
 ---- batch: 030 ----
mean loss: 334.40
 ---- batch: 040 ----
mean loss: 331.73
 ---- batch: 050 ----
mean loss: 328.95
 ---- batch: 060 ----
mean loss: 312.13
 ---- batch: 070 ----
mean loss: 303.06
 ---- batch: 080 ----
mean loss: 322.08
 ---- batch: 090 ----
mean loss: 313.09
 ---- batch: 100 ----
mean loss: 320.64
 ---- batch: 110 ----
mean loss: 317.56
train mean loss: 321.40
epoch train time: 0:00:01.894524
elapsed time: 0:02:16.051070
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 22:39:27.996686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.73
 ---- batch: 020 ----
mean loss: 314.61
 ---- batch: 030 ----
mean loss: 319.91
 ---- batch: 040 ----
mean loss: 312.86
 ---- batch: 050 ----
mean loss: 308.42
 ---- batch: 060 ----
mean loss: 307.15
 ---- batch: 070 ----
mean loss: 299.20
 ---- batch: 080 ----
mean loss: 301.67
 ---- batch: 090 ----
mean loss: 302.63
 ---- batch: 100 ----
mean loss: 294.01
 ---- batch: 110 ----
mean loss: 292.08
train mean loss: 305.76
epoch train time: 0:00:01.888117
elapsed time: 0:02:17.939898
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 22:39:29.885169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.47
 ---- batch: 020 ----
mean loss: 293.42
 ---- batch: 030 ----
mean loss: 307.86
 ---- batch: 040 ----
mean loss: 286.09
 ---- batch: 050 ----
mean loss: 302.97
 ---- batch: 060 ----
mean loss: 306.19
 ---- batch: 070 ----
mean loss: 296.21
 ---- batch: 080 ----
mean loss: 283.67
 ---- batch: 090 ----
mean loss: 296.37
 ---- batch: 100 ----
mean loss: 279.13
 ---- batch: 110 ----
mean loss: 290.48
train mean loss: 294.55
epoch train time: 0:00:01.879284
elapsed time: 0:02:19.819477
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 22:39:31.765112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.41
 ---- batch: 020 ----
mean loss: 281.99
 ---- batch: 030 ----
mean loss: 281.97
 ---- batch: 040 ----
mean loss: 279.54
 ---- batch: 050 ----
mean loss: 297.24
 ---- batch: 060 ----
mean loss: 278.45
 ---- batch: 070 ----
mean loss: 283.39
 ---- batch: 080 ----
mean loss: 283.10
 ---- batch: 090 ----
mean loss: 285.08
 ---- batch: 100 ----
mean loss: 278.73
 ---- batch: 110 ----
mean loss: 292.62
train mean loss: 284.90
epoch train time: 0:00:01.893771
elapsed time: 0:02:21.713957
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 22:39:33.659576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.72
 ---- batch: 020 ----
mean loss: 291.28
 ---- batch: 030 ----
mean loss: 276.98
 ---- batch: 040 ----
mean loss: 271.64
 ---- batch: 050 ----
mean loss: 276.29
 ---- batch: 060 ----
mean loss: 265.41
 ---- batch: 070 ----
mean loss: 271.14
 ---- batch: 080 ----
mean loss: 287.83
 ---- batch: 090 ----
mean loss: 273.77
 ---- batch: 100 ----
mean loss: 293.48
 ---- batch: 110 ----
mean loss: 275.65
train mean loss: 278.03
epoch train time: 0:00:01.876732
elapsed time: 0:02:23.591328
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 22:39:35.536873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.78
 ---- batch: 020 ----
mean loss: 270.38
 ---- batch: 030 ----
mean loss: 274.53
 ---- batch: 040 ----
mean loss: 272.81
 ---- batch: 050 ----
mean loss: 262.33
 ---- batch: 060 ----
mean loss: 262.38
 ---- batch: 070 ----
mean loss: 259.55
 ---- batch: 080 ----
mean loss: 270.38
 ---- batch: 090 ----
mean loss: 281.72
 ---- batch: 100 ----
mean loss: 279.75
 ---- batch: 110 ----
mean loss: 274.21
train mean loss: 270.32
epoch train time: 0:00:01.880668
elapsed time: 0:02:25.472605
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 22:39:37.418159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.28
 ---- batch: 020 ----
mean loss: 260.29
 ---- batch: 030 ----
mean loss: 274.03
 ---- batch: 040 ----
mean loss: 267.10
 ---- batch: 050 ----
mean loss: 261.29
 ---- batch: 060 ----
mean loss: 265.93
 ---- batch: 070 ----
mean loss: 262.55
 ---- batch: 080 ----
mean loss: 269.32
 ---- batch: 090 ----
mean loss: 266.51
 ---- batch: 100 ----
mean loss: 263.67
 ---- batch: 110 ----
mean loss: 261.55
train mean loss: 265.49
epoch train time: 0:00:01.879134
elapsed time: 0:02:27.352311
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 22:39:39.298023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.92
 ---- batch: 020 ----
mean loss: 267.95
 ---- batch: 030 ----
mean loss: 255.71
 ---- batch: 040 ----
mean loss: 250.45
 ---- batch: 050 ----
mean loss: 257.48
 ---- batch: 060 ----
mean loss: 252.59
 ---- batch: 070 ----
mean loss: 258.75
 ---- batch: 080 ----
mean loss: 255.94
 ---- batch: 090 ----
mean loss: 267.40
 ---- batch: 100 ----
mean loss: 255.20
 ---- batch: 110 ----
mean loss: 270.81
train mean loss: 258.68
epoch train time: 0:00:01.899669
elapsed time: 0:02:29.252738
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 22:39:41.198323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.32
 ---- batch: 020 ----
mean loss: 243.06
 ---- batch: 030 ----
mean loss: 250.15
 ---- batch: 040 ----
mean loss: 269.56
 ---- batch: 050 ----
mean loss: 248.86
 ---- batch: 060 ----
mean loss: 257.35
 ---- batch: 070 ----
mean loss: 239.97
 ---- batch: 080 ----
mean loss: 266.99
 ---- batch: 090 ----
mean loss: 256.44
 ---- batch: 100 ----
mean loss: 257.59
 ---- batch: 110 ----
mean loss: 248.45
train mean loss: 254.49
epoch train time: 0:00:01.905560
elapsed time: 0:02:31.158968
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 22:39:43.104532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.73
 ---- batch: 020 ----
mean loss: 246.67
 ---- batch: 030 ----
mean loss: 243.70
 ---- batch: 040 ----
mean loss: 250.82
 ---- batch: 050 ----
mean loss: 245.31
 ---- batch: 060 ----
mean loss: 251.33
 ---- batch: 070 ----
mean loss: 249.97
 ---- batch: 080 ----
mean loss: 245.87
 ---- batch: 090 ----
mean loss: 241.04
 ---- batch: 100 ----
mean loss: 249.23
 ---- batch: 110 ----
mean loss: 254.45
train mean loss: 247.94
epoch train time: 0:00:01.897396
elapsed time: 0:02:33.056945
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 22:39:45.002521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.01
 ---- batch: 020 ----
mean loss: 246.32
 ---- batch: 030 ----
mean loss: 241.14
 ---- batch: 040 ----
mean loss: 231.05
 ---- batch: 050 ----
mean loss: 238.83
 ---- batch: 060 ----
mean loss: 247.80
 ---- batch: 070 ----
mean loss: 252.62
 ---- batch: 080 ----
mean loss: 249.20
 ---- batch: 090 ----
mean loss: 255.07
 ---- batch: 100 ----
mean loss: 240.18
 ---- batch: 110 ----
mean loss: 241.57
train mean loss: 244.37
epoch train time: 0:00:01.883393
elapsed time: 0:02:34.941020
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 22:39:46.886705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.61
 ---- batch: 020 ----
mean loss: 229.59
 ---- batch: 030 ----
mean loss: 248.12
 ---- batch: 040 ----
mean loss: 245.91
 ---- batch: 050 ----
mean loss: 247.98
 ---- batch: 060 ----
mean loss: 239.18
 ---- batch: 070 ----
mean loss: 248.71
 ---- batch: 080 ----
mean loss: 233.56
 ---- batch: 090 ----
mean loss: 228.90
 ---- batch: 100 ----
mean loss: 234.31
 ---- batch: 110 ----
mean loss: 244.28
train mean loss: 240.48
epoch train time: 0:00:01.915858
elapsed time: 0:02:36.857599
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 22:39:48.803152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.47
 ---- batch: 020 ----
mean loss: 236.14
 ---- batch: 030 ----
mean loss: 237.94
 ---- batch: 040 ----
mean loss: 237.60
 ---- batch: 050 ----
mean loss: 229.40
 ---- batch: 060 ----
mean loss: 240.99
 ---- batch: 070 ----
mean loss: 237.39
 ---- batch: 080 ----
mean loss: 241.43
 ---- batch: 090 ----
mean loss: 230.99
 ---- batch: 100 ----
mean loss: 237.80
 ---- batch: 110 ----
mean loss: 231.27
train mean loss: 236.39
epoch train time: 0:00:01.939666
elapsed time: 0:02:38.797842
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 22:39:50.743406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.72
 ---- batch: 020 ----
mean loss: 227.34
 ---- batch: 030 ----
mean loss: 219.80
 ---- batch: 040 ----
mean loss: 232.78
 ---- batch: 050 ----
mean loss: 234.95
 ---- batch: 060 ----
mean loss: 223.89
 ---- batch: 070 ----
mean loss: 234.31
 ---- batch: 080 ----
mean loss: 234.30
 ---- batch: 090 ----
mean loss: 237.44
 ---- batch: 100 ----
mean loss: 233.73
 ---- batch: 110 ----
mean loss: 241.40
train mean loss: 232.55
epoch train time: 0:00:01.948381
elapsed time: 0:02:40.746855
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 22:39:52.692473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.10
 ---- batch: 020 ----
mean loss: 232.20
 ---- batch: 030 ----
mean loss: 222.92
 ---- batch: 040 ----
mean loss: 236.38
 ---- batch: 050 ----
mean loss: 236.28
 ---- batch: 060 ----
mean loss: 239.99
 ---- batch: 070 ----
mean loss: 235.01
 ---- batch: 080 ----
mean loss: 222.51
 ---- batch: 090 ----
mean loss: 225.82
 ---- batch: 100 ----
mean loss: 218.12
 ---- batch: 110 ----
mean loss: 233.00
train mean loss: 229.49
epoch train time: 0:00:01.926633
elapsed time: 0:02:42.674174
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 22:39:54.619742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.49
 ---- batch: 020 ----
mean loss: 224.67
 ---- batch: 030 ----
mean loss: 225.34
 ---- batch: 040 ----
mean loss: 218.24
 ---- batch: 050 ----
mean loss: 224.94
 ---- batch: 060 ----
mean loss: 227.13
 ---- batch: 070 ----
mean loss: 217.39
 ---- batch: 080 ----
mean loss: 222.65
 ---- batch: 090 ----
mean loss: 227.86
 ---- batch: 100 ----
mean loss: 221.82
 ---- batch: 110 ----
mean loss: 228.63
train mean loss: 225.24
epoch train time: 0:00:01.933447
elapsed time: 0:02:44.608233
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 22:39:56.553782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.20
 ---- batch: 020 ----
mean loss: 227.59
 ---- batch: 030 ----
mean loss: 230.70
 ---- batch: 040 ----
mean loss: 222.37
 ---- batch: 050 ----
mean loss: 219.30
 ---- batch: 060 ----
mean loss: 216.58
 ---- batch: 070 ----
mean loss: 221.30
 ---- batch: 080 ----
mean loss: 216.93
 ---- batch: 090 ----
mean loss: 220.08
 ---- batch: 100 ----
mean loss: 217.58
 ---- batch: 110 ----
mean loss: 229.66
train mean loss: 221.51
epoch train time: 0:00:01.934895
elapsed time: 0:02:46.543750
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 22:39:58.489274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.64
 ---- batch: 020 ----
mean loss: 207.88
 ---- batch: 030 ----
mean loss: 222.43
 ---- batch: 040 ----
mean loss: 220.39
 ---- batch: 050 ----
mean loss: 204.51
 ---- batch: 060 ----
mean loss: 218.78
 ---- batch: 070 ----
mean loss: 223.43
 ---- batch: 080 ----
mean loss: 224.92
 ---- batch: 090 ----
mean loss: 222.49
 ---- batch: 100 ----
mean loss: 219.63
 ---- batch: 110 ----
mean loss: 219.41
train mean loss: 218.73
epoch train time: 0:00:01.906757
elapsed time: 0:02:48.451034
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 22:40:00.396583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.78
 ---- batch: 020 ----
mean loss: 208.57
 ---- batch: 030 ----
mean loss: 215.76
 ---- batch: 040 ----
mean loss: 217.59
 ---- batch: 050 ----
mean loss: 222.04
 ---- batch: 060 ----
mean loss: 212.95
 ---- batch: 070 ----
mean loss: 216.00
 ---- batch: 080 ----
mean loss: 209.56
 ---- batch: 090 ----
mean loss: 215.31
 ---- batch: 100 ----
mean loss: 224.31
 ---- batch: 110 ----
mean loss: 210.04
train mean loss: 214.85
epoch train time: 0:00:01.879420
elapsed time: 0:02:50.331030
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 22:40:02.276615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.58
 ---- batch: 020 ----
mean loss: 218.13
 ---- batch: 030 ----
mean loss: 214.35
 ---- batch: 040 ----
mean loss: 211.61
 ---- batch: 050 ----
mean loss: 209.83
 ---- batch: 060 ----
mean loss: 216.25
 ---- batch: 070 ----
mean loss: 215.88
 ---- batch: 080 ----
mean loss: 211.46
 ---- batch: 090 ----
mean loss: 213.49
 ---- batch: 100 ----
mean loss: 215.34
 ---- batch: 110 ----
mean loss: 208.66
train mean loss: 213.49
epoch train time: 0:00:01.880854
elapsed time: 0:02:52.212519
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 22:40:04.158074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.63
 ---- batch: 020 ----
mean loss: 208.37
 ---- batch: 030 ----
mean loss: 203.95
 ---- batch: 040 ----
mean loss: 199.77
 ---- batch: 050 ----
mean loss: 205.89
 ---- batch: 060 ----
mean loss: 213.37
 ---- batch: 070 ----
mean loss: 213.26
 ---- batch: 080 ----
mean loss: 210.56
 ---- batch: 090 ----
mean loss: 214.17
 ---- batch: 100 ----
mean loss: 207.80
 ---- batch: 110 ----
mean loss: 209.72
train mean loss: 209.72
epoch train time: 0:00:01.873432
elapsed time: 0:02:54.086511
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 22:40:06.032082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.54
 ---- batch: 020 ----
mean loss: 205.47
 ---- batch: 030 ----
mean loss: 214.31
 ---- batch: 040 ----
mean loss: 212.41
 ---- batch: 050 ----
mean loss: 210.91
 ---- batch: 060 ----
mean loss: 198.56
 ---- batch: 070 ----
mean loss: 208.33
 ---- batch: 080 ----
mean loss: 207.21
 ---- batch: 090 ----
mean loss: 205.87
 ---- batch: 100 ----
mean loss: 199.64
 ---- batch: 110 ----
mean loss: 215.26
train mean loss: 207.26
epoch train time: 0:00:01.882029
elapsed time: 0:02:55.969110
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 22:40:07.914719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.47
 ---- batch: 020 ----
mean loss: 196.68
 ---- batch: 030 ----
mean loss: 209.83
 ---- batch: 040 ----
mean loss: 208.96
 ---- batch: 050 ----
mean loss: 204.03
 ---- batch: 060 ----
mean loss: 202.60
 ---- batch: 070 ----
mean loss: 205.09
 ---- batch: 080 ----
mean loss: 202.90
 ---- batch: 090 ----
mean loss: 201.55
 ---- batch: 100 ----
mean loss: 193.89
 ---- batch: 110 ----
mean loss: 207.79
train mean loss: 203.65
epoch train time: 0:00:01.898389
elapsed time: 0:02:57.868146
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 22:40:09.813719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.75
 ---- batch: 020 ----
mean loss: 208.42
 ---- batch: 030 ----
mean loss: 192.82
 ---- batch: 040 ----
mean loss: 201.14
 ---- batch: 050 ----
mean loss: 207.45
 ---- batch: 060 ----
mean loss: 202.61
 ---- batch: 070 ----
mean loss: 201.35
 ---- batch: 080 ----
mean loss: 198.70
 ---- batch: 090 ----
mean loss: 204.04
 ---- batch: 100 ----
mean loss: 202.13
 ---- batch: 110 ----
mean loss: 190.33
train mean loss: 201.13
epoch train time: 0:00:01.874906
elapsed time: 0:02:59.743658
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 22:40:11.689187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.62
 ---- batch: 020 ----
mean loss: 200.83
 ---- batch: 030 ----
mean loss: 191.27
 ---- batch: 040 ----
mean loss: 203.62
 ---- batch: 050 ----
mean loss: 198.16
 ---- batch: 060 ----
mean loss: 210.79
 ---- batch: 070 ----
mean loss: 200.89
 ---- batch: 080 ----
mean loss: 202.67
 ---- batch: 090 ----
mean loss: 198.85
 ---- batch: 100 ----
mean loss: 198.01
 ---- batch: 110 ----
mean loss: 205.83
train mean loss: 199.38
epoch train time: 0:00:01.854451
elapsed time: 0:03:01.598634
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 22:40:13.544207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.92
 ---- batch: 020 ----
mean loss: 198.88
 ---- batch: 030 ----
mean loss: 192.39
 ---- batch: 040 ----
mean loss: 203.73
 ---- batch: 050 ----
mean loss: 194.29
 ---- batch: 060 ----
mean loss: 201.50
 ---- batch: 070 ----
mean loss: 198.09
 ---- batch: 080 ----
mean loss: 191.58
 ---- batch: 090 ----
mean loss: 206.42
 ---- batch: 100 ----
mean loss: 186.10
 ---- batch: 110 ----
mean loss: 206.65
train mean loss: 197.21
epoch train time: 0:00:01.865885
elapsed time: 0:03:03.465164
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 22:40:15.410728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.80
 ---- batch: 020 ----
mean loss: 195.70
 ---- batch: 030 ----
mean loss: 191.16
 ---- batch: 040 ----
mean loss: 192.54
 ---- batch: 050 ----
mean loss: 194.75
 ---- batch: 060 ----
mean loss: 197.98
 ---- batch: 070 ----
mean loss: 203.15
 ---- batch: 080 ----
mean loss: 194.28
 ---- batch: 090 ----
mean loss: 195.68
 ---- batch: 100 ----
mean loss: 198.59
 ---- batch: 110 ----
mean loss: 192.15
train mean loss: 195.20
epoch train time: 0:00:01.884981
elapsed time: 0:03:05.350709
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 22:40:17.296284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.69
 ---- batch: 020 ----
mean loss: 188.40
 ---- batch: 030 ----
mean loss: 187.18
 ---- batch: 040 ----
mean loss: 192.15
 ---- batch: 050 ----
mean loss: 194.88
 ---- batch: 060 ----
mean loss: 190.06
 ---- batch: 070 ----
mean loss: 190.05
 ---- batch: 080 ----
mean loss: 209.44
 ---- batch: 090 ----
mean loss: 196.75
 ---- batch: 100 ----
mean loss: 184.53
 ---- batch: 110 ----
mean loss: 201.05
train mean loss: 192.67
epoch train time: 0:00:01.903863
elapsed time: 0:03:07.255183
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 22:40:19.200772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.76
 ---- batch: 020 ----
mean loss: 194.15
 ---- batch: 030 ----
mean loss: 193.60
 ---- batch: 040 ----
mean loss: 194.51
 ---- batch: 050 ----
mean loss: 187.12
 ---- batch: 060 ----
mean loss: 191.33
 ---- batch: 070 ----
mean loss: 195.53
 ---- batch: 080 ----
mean loss: 191.77
 ---- batch: 090 ----
mean loss: 194.99
 ---- batch: 100 ----
mean loss: 194.77
 ---- batch: 110 ----
mean loss: 188.74
train mean loss: 192.07
epoch train time: 0:00:01.916448
elapsed time: 0:03:09.172223
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 22:40:21.117914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.40
 ---- batch: 020 ----
mean loss: 193.31
 ---- batch: 030 ----
mean loss: 185.53
 ---- batch: 040 ----
mean loss: 184.94
 ---- batch: 050 ----
mean loss: 197.15
 ---- batch: 060 ----
mean loss: 187.71
 ---- batch: 070 ----
mean loss: 186.16
 ---- batch: 080 ----
mean loss: 184.21
 ---- batch: 090 ----
mean loss: 183.11
 ---- batch: 100 ----
mean loss: 194.80
 ---- batch: 110 ----
mean loss: 191.71
train mean loss: 188.63
epoch train time: 0:00:01.895674
elapsed time: 0:03:11.068594
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 22:40:23.014110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.79
 ---- batch: 020 ----
mean loss: 189.88
 ---- batch: 030 ----
mean loss: 185.93
 ---- batch: 040 ----
mean loss: 185.89
 ---- batch: 050 ----
mean loss: 180.92
 ---- batch: 060 ----
mean loss: 190.62
 ---- batch: 070 ----
mean loss: 196.89
 ---- batch: 080 ----
mean loss: 195.54
 ---- batch: 090 ----
mean loss: 185.39
 ---- batch: 100 ----
mean loss: 194.35
 ---- batch: 110 ----
mean loss: 183.49
train mean loss: 188.40
epoch train time: 0:00:01.884163
elapsed time: 0:03:12.953282
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 22:40:24.898808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.18
 ---- batch: 020 ----
mean loss: 190.43
 ---- batch: 030 ----
mean loss: 175.02
 ---- batch: 040 ----
mean loss: 192.02
 ---- batch: 050 ----
mean loss: 188.36
 ---- batch: 060 ----
mean loss: 187.01
 ---- batch: 070 ----
mean loss: 185.01
 ---- batch: 080 ----
mean loss: 191.68
 ---- batch: 090 ----
mean loss: 192.19
 ---- batch: 100 ----
mean loss: 190.58
 ---- batch: 110 ----
mean loss: 188.17
train mean loss: 186.83
epoch train time: 0:00:01.867097
elapsed time: 0:03:14.820945
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 22:40:26.766587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.40
 ---- batch: 020 ----
mean loss: 192.20
 ---- batch: 030 ----
mean loss: 178.38
 ---- batch: 040 ----
mean loss: 179.92
 ---- batch: 050 ----
mean loss: 187.17
 ---- batch: 060 ----
mean loss: 178.69
 ---- batch: 070 ----
mean loss: 182.95
 ---- batch: 080 ----
mean loss: 177.94
 ---- batch: 090 ----
mean loss: 187.32
 ---- batch: 100 ----
mean loss: 190.86
 ---- batch: 110 ----
mean loss: 190.06
train mean loss: 183.56
epoch train time: 0:00:01.870918
elapsed time: 0:03:16.692533
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 22:40:28.638181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.76
 ---- batch: 020 ----
mean loss: 189.95
 ---- batch: 030 ----
mean loss: 182.94
 ---- batch: 040 ----
mean loss: 174.10
 ---- batch: 050 ----
mean loss: 178.35
 ---- batch: 060 ----
mean loss: 185.18
 ---- batch: 070 ----
mean loss: 183.58
 ---- batch: 080 ----
mean loss: 192.90
 ---- batch: 090 ----
mean loss: 184.93
 ---- batch: 100 ----
mean loss: 177.21
 ---- batch: 110 ----
mean loss: 187.01
train mean loss: 183.35
epoch train time: 0:00:01.916166
elapsed time: 0:03:18.609419
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 22:40:30.555050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.62
 ---- batch: 020 ----
mean loss: 173.35
 ---- batch: 030 ----
mean loss: 182.47
 ---- batch: 040 ----
mean loss: 179.36
 ---- batch: 050 ----
mean loss: 183.36
 ---- batch: 060 ----
mean loss: 175.03
 ---- batch: 070 ----
mean loss: 184.82
 ---- batch: 080 ----
mean loss: 183.59
 ---- batch: 090 ----
mean loss: 187.03
 ---- batch: 100 ----
mean loss: 186.06
 ---- batch: 110 ----
mean loss: 179.41
train mean loss: 180.21
epoch train time: 0:00:01.898770
elapsed time: 0:03:20.508857
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 22:40:32.454418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.97
 ---- batch: 020 ----
mean loss: 178.57
 ---- batch: 030 ----
mean loss: 175.86
 ---- batch: 040 ----
mean loss: 185.53
 ---- batch: 050 ----
mean loss: 173.93
 ---- batch: 060 ----
mean loss: 177.87
 ---- batch: 070 ----
mean loss: 185.16
 ---- batch: 080 ----
mean loss: 185.55
 ---- batch: 090 ----
mean loss: 178.38
 ---- batch: 100 ----
mean loss: 173.48
 ---- batch: 110 ----
mean loss: 179.27
train mean loss: 179.20
epoch train time: 0:00:01.896586
elapsed time: 0:03:22.406073
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 22:40:34.351648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.81
 ---- batch: 020 ----
mean loss: 173.92
 ---- batch: 030 ----
mean loss: 170.17
 ---- batch: 040 ----
mean loss: 177.09
 ---- batch: 050 ----
mean loss: 171.71
 ---- batch: 060 ----
mean loss: 183.51
 ---- batch: 070 ----
mean loss: 189.38
 ---- batch: 080 ----
mean loss: 182.54
 ---- batch: 090 ----
mean loss: 175.64
 ---- batch: 100 ----
mean loss: 179.99
 ---- batch: 110 ----
mean loss: 181.44
train mean loss: 178.56
epoch train time: 0:00:01.895965
elapsed time: 0:03:24.302661
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 22:40:36.248275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.67
 ---- batch: 020 ----
mean loss: 180.83
 ---- batch: 030 ----
mean loss: 178.62
 ---- batch: 040 ----
mean loss: 170.38
 ---- batch: 050 ----
mean loss: 177.26
 ---- batch: 060 ----
mean loss: 180.38
 ---- batch: 070 ----
mean loss: 177.33
 ---- batch: 080 ----
mean loss: 176.56
 ---- batch: 090 ----
mean loss: 175.76
 ---- batch: 100 ----
mean loss: 176.23
 ---- batch: 110 ----
mean loss: 179.06
train mean loss: 176.79
epoch train time: 0:00:01.881717
elapsed time: 0:03:26.185006
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 22:40:38.130540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.46
 ---- batch: 020 ----
mean loss: 174.71
 ---- batch: 030 ----
mean loss: 169.85
 ---- batch: 040 ----
mean loss: 178.90
 ---- batch: 050 ----
mean loss: 171.06
 ---- batch: 060 ----
mean loss: 176.92
 ---- batch: 070 ----
mean loss: 183.31
 ---- batch: 080 ----
mean loss: 180.15
 ---- batch: 090 ----
mean loss: 172.35
 ---- batch: 100 ----
mean loss: 174.62
 ---- batch: 110 ----
mean loss: 179.26
train mean loss: 176.13
epoch train time: 0:00:01.876631
elapsed time: 0:03:28.062189
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 22:40:40.007773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.06
 ---- batch: 020 ----
mean loss: 178.66
 ---- batch: 030 ----
mean loss: 175.26
 ---- batch: 040 ----
mean loss: 178.93
 ---- batch: 050 ----
mean loss: 168.52
 ---- batch: 060 ----
mean loss: 164.48
 ---- batch: 070 ----
mean loss: 177.28
 ---- batch: 080 ----
mean loss: 163.11
 ---- batch: 090 ----
mean loss: 179.19
 ---- batch: 100 ----
mean loss: 176.11
 ---- batch: 110 ----
mean loss: 175.81
train mean loss: 174.07
epoch train time: 0:00:01.904664
elapsed time: 0:03:29.967443
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 22:40:41.912987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.86
 ---- batch: 020 ----
mean loss: 180.97
 ---- batch: 030 ----
mean loss: 175.91
 ---- batch: 040 ----
mean loss: 170.08
 ---- batch: 050 ----
mean loss: 173.56
 ---- batch: 060 ----
mean loss: 173.53
 ---- batch: 070 ----
mean loss: 182.55
 ---- batch: 080 ----
mean loss: 177.36
 ---- batch: 090 ----
mean loss: 164.70
 ---- batch: 100 ----
mean loss: 173.74
 ---- batch: 110 ----
mean loss: 167.73
train mean loss: 173.01
epoch train time: 0:00:01.897581
elapsed time: 0:03:31.865590
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 22:40:43.811167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.76
 ---- batch: 020 ----
mean loss: 170.60
 ---- batch: 030 ----
mean loss: 175.16
 ---- batch: 040 ----
mean loss: 168.49
 ---- batch: 050 ----
mean loss: 173.66
 ---- batch: 060 ----
mean loss: 172.82
 ---- batch: 070 ----
mean loss: 172.97
 ---- batch: 080 ----
mean loss: 171.25
 ---- batch: 090 ----
mean loss: 170.23
 ---- batch: 100 ----
mean loss: 177.23
 ---- batch: 110 ----
mean loss: 169.70
train mean loss: 171.86
epoch train time: 0:00:01.943566
elapsed time: 0:03:33.809788
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 22:40:45.755432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.81
 ---- batch: 020 ----
mean loss: 171.82
 ---- batch: 030 ----
mean loss: 166.47
 ---- batch: 040 ----
mean loss: 175.59
 ---- batch: 050 ----
mean loss: 176.19
 ---- batch: 060 ----
mean loss: 169.34
 ---- batch: 070 ----
mean loss: 165.56
 ---- batch: 080 ----
mean loss: 166.27
 ---- batch: 090 ----
mean loss: 171.16
 ---- batch: 100 ----
mean loss: 172.76
 ---- batch: 110 ----
mean loss: 173.58
train mean loss: 171.24
epoch train time: 0:00:01.934549
elapsed time: 0:03:35.745029
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 22:40:47.690591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.60
 ---- batch: 020 ----
mean loss: 162.23
 ---- batch: 030 ----
mean loss: 155.86
 ---- batch: 040 ----
mean loss: 172.96
 ---- batch: 050 ----
mean loss: 181.15
 ---- batch: 060 ----
mean loss: 169.72
 ---- batch: 070 ----
mean loss: 169.94
 ---- batch: 080 ----
mean loss: 171.18
 ---- batch: 090 ----
mean loss: 169.21
 ---- batch: 100 ----
mean loss: 172.65
 ---- batch: 110 ----
mean loss: 174.90
train mean loss: 169.75
epoch train time: 0:00:01.920189
elapsed time: 0:03:37.665851
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 22:40:49.611118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.26
 ---- batch: 020 ----
mean loss: 170.10
 ---- batch: 030 ----
mean loss: 163.50
 ---- batch: 040 ----
mean loss: 163.41
 ---- batch: 050 ----
mean loss: 179.52
 ---- batch: 060 ----
mean loss: 167.58
 ---- batch: 070 ----
mean loss: 163.33
 ---- batch: 080 ----
mean loss: 176.68
 ---- batch: 090 ----
mean loss: 173.53
 ---- batch: 100 ----
mean loss: 171.04
 ---- batch: 110 ----
mean loss: 163.82
train mean loss: 168.88
epoch train time: 0:00:01.914443
elapsed time: 0:03:39.580575
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 22:40:51.526136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.40
 ---- batch: 020 ----
mean loss: 168.00
 ---- batch: 030 ----
mean loss: 166.18
 ---- batch: 040 ----
mean loss: 164.25
 ---- batch: 050 ----
mean loss: 169.95
 ---- batch: 060 ----
mean loss: 167.47
 ---- batch: 070 ----
mean loss: 166.88
 ---- batch: 080 ----
mean loss: 174.51
 ---- batch: 090 ----
mean loss: 166.31
 ---- batch: 100 ----
mean loss: 169.20
 ---- batch: 110 ----
mean loss: 166.25
train mean loss: 167.23
epoch train time: 0:00:01.905624
elapsed time: 0:03:41.486783
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 22:40:53.432300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.57
 ---- batch: 020 ----
mean loss: 171.62
 ---- batch: 030 ----
mean loss: 155.67
 ---- batch: 040 ----
mean loss: 166.30
 ---- batch: 050 ----
mean loss: 164.97
 ---- batch: 060 ----
mean loss: 166.93
 ---- batch: 070 ----
mean loss: 163.57
 ---- batch: 080 ----
mean loss: 158.20
 ---- batch: 090 ----
mean loss: 165.24
 ---- batch: 100 ----
mean loss: 174.84
 ---- batch: 110 ----
mean loss: 178.69
train mean loss: 166.91
epoch train time: 0:00:01.891318
elapsed time: 0:03:43.378621
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 22:40:55.324188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.70
 ---- batch: 020 ----
mean loss: 163.98
 ---- batch: 030 ----
mean loss: 166.07
 ---- batch: 040 ----
mean loss: 161.03
 ---- batch: 050 ----
mean loss: 165.78
 ---- batch: 060 ----
mean loss: 173.49
 ---- batch: 070 ----
mean loss: 165.24
 ---- batch: 080 ----
mean loss: 165.72
 ---- batch: 090 ----
mean loss: 162.81
 ---- batch: 100 ----
mean loss: 170.07
 ---- batch: 110 ----
mean loss: 170.43
train mean loss: 165.84
epoch train time: 0:00:01.899600
elapsed time: 0:03:45.278845
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 22:40:57.224393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.80
 ---- batch: 020 ----
mean loss: 163.70
 ---- batch: 030 ----
mean loss: 167.31
 ---- batch: 040 ----
mean loss: 155.64
 ---- batch: 050 ----
mean loss: 169.93
 ---- batch: 060 ----
mean loss: 160.28
 ---- batch: 070 ----
mean loss: 166.97
 ---- batch: 080 ----
mean loss: 169.40
 ---- batch: 090 ----
mean loss: 163.62
 ---- batch: 100 ----
mean loss: 158.10
 ---- batch: 110 ----
mean loss: 167.19
train mean loss: 164.58
epoch train time: 0:00:01.881281
elapsed time: 0:03:47.160680
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 22:40:59.106221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.92
 ---- batch: 020 ----
mean loss: 167.49
 ---- batch: 030 ----
mean loss: 165.71
 ---- batch: 040 ----
mean loss: 167.12
 ---- batch: 050 ----
mean loss: 164.50
 ---- batch: 060 ----
mean loss: 165.33
 ---- batch: 070 ----
mean loss: 166.67
 ---- batch: 080 ----
mean loss: 167.47
 ---- batch: 090 ----
mean loss: 159.40
 ---- batch: 100 ----
mean loss: 164.66
 ---- batch: 110 ----
mean loss: 159.64
train mean loss: 165.03
epoch train time: 0:00:01.904533
elapsed time: 0:03:49.065789
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 22:41:01.011371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.38
 ---- batch: 020 ----
mean loss: 158.94
 ---- batch: 030 ----
mean loss: 163.58
 ---- batch: 040 ----
mean loss: 165.88
 ---- batch: 050 ----
mean loss: 159.38
 ---- batch: 060 ----
mean loss: 169.39
 ---- batch: 070 ----
mean loss: 157.36
 ---- batch: 080 ----
mean loss: 158.58
 ---- batch: 090 ----
mean loss: 161.48
 ---- batch: 100 ----
mean loss: 164.98
 ---- batch: 110 ----
mean loss: 172.04
train mean loss: 163.75
epoch train time: 0:00:01.875166
elapsed time: 0:03:50.941561
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 22:41:02.887144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.02
 ---- batch: 020 ----
mean loss: 162.37
 ---- batch: 030 ----
mean loss: 160.49
 ---- batch: 040 ----
mean loss: 159.41
 ---- batch: 050 ----
mean loss: 162.74
 ---- batch: 060 ----
mean loss: 158.38
 ---- batch: 070 ----
mean loss: 161.49
 ---- batch: 080 ----
mean loss: 157.41
 ---- batch: 090 ----
mean loss: 162.30
 ---- batch: 100 ----
mean loss: 169.52
 ---- batch: 110 ----
mean loss: 173.91
train mean loss: 161.64
epoch train time: 0:00:01.898652
elapsed time: 0:03:52.840864
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 22:41:04.786467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.91
 ---- batch: 020 ----
mean loss: 157.60
 ---- batch: 030 ----
mean loss: 162.87
 ---- batch: 040 ----
mean loss: 158.93
 ---- batch: 050 ----
mean loss: 158.37
 ---- batch: 060 ----
mean loss: 166.96
 ---- batch: 070 ----
mean loss: 163.57
 ---- batch: 080 ----
mean loss: 159.24
 ---- batch: 090 ----
mean loss: 164.76
 ---- batch: 100 ----
mean loss: 169.47
 ---- batch: 110 ----
mean loss: 157.58
train mean loss: 162.31
epoch train time: 0:00:01.900471
elapsed time: 0:03:54.742198
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 22:41:06.687551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.08
 ---- batch: 020 ----
mean loss: 168.23
 ---- batch: 030 ----
mean loss: 155.74
 ---- batch: 040 ----
mean loss: 155.38
 ---- batch: 050 ----
mean loss: 158.56
 ---- batch: 060 ----
mean loss: 156.87
 ---- batch: 070 ----
mean loss: 158.52
 ---- batch: 080 ----
mean loss: 170.68
 ---- batch: 090 ----
mean loss: 167.62
 ---- batch: 100 ----
mean loss: 157.44
 ---- batch: 110 ----
mean loss: 169.18
train mean loss: 161.16
epoch train time: 0:00:01.912779
elapsed time: 0:03:56.655379
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 22:41:08.601027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.31
 ---- batch: 020 ----
mean loss: 165.62
 ---- batch: 030 ----
mean loss: 161.86
 ---- batch: 040 ----
mean loss: 157.69
 ---- batch: 050 ----
mean loss: 166.07
 ---- batch: 060 ----
mean loss: 155.39
 ---- batch: 070 ----
mean loss: 170.76
 ---- batch: 080 ----
mean loss: 164.72
 ---- batch: 090 ----
mean loss: 162.18
 ---- batch: 100 ----
mean loss: 149.20
 ---- batch: 110 ----
mean loss: 158.68
train mean loss: 160.81
epoch train time: 0:00:01.883859
elapsed time: 0:03:58.539914
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 22:41:10.485537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.98
 ---- batch: 020 ----
mean loss: 162.94
 ---- batch: 030 ----
mean loss: 151.05
 ---- batch: 040 ----
mean loss: 153.27
 ---- batch: 050 ----
mean loss: 164.76
 ---- batch: 060 ----
mean loss: 158.80
 ---- batch: 070 ----
mean loss: 161.71
 ---- batch: 080 ----
mean loss: 162.58
 ---- batch: 090 ----
mean loss: 166.53
 ---- batch: 100 ----
mean loss: 165.42
 ---- batch: 110 ----
mean loss: 161.79
train mean loss: 160.55
epoch train time: 0:00:01.886233
elapsed time: 0:04:00.426782
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 22:41:12.372354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.14
 ---- batch: 020 ----
mean loss: 153.53
 ---- batch: 030 ----
mean loss: 160.95
 ---- batch: 040 ----
mean loss: 151.28
 ---- batch: 050 ----
mean loss: 162.48
 ---- batch: 060 ----
mean loss: 157.53
 ---- batch: 070 ----
mean loss: 165.75
 ---- batch: 080 ----
mean loss: 162.47
 ---- batch: 090 ----
mean loss: 152.93
 ---- batch: 100 ----
mean loss: 156.31
 ---- batch: 110 ----
mean loss: 164.74
train mean loss: 158.58
epoch train time: 0:00:01.884302
elapsed time: 0:04:02.311703
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 22:41:14.257286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.79
 ---- batch: 020 ----
mean loss: 156.65
 ---- batch: 030 ----
mean loss: 154.95
 ---- batch: 040 ----
mean loss: 147.73
 ---- batch: 050 ----
mean loss: 162.63
 ---- batch: 060 ----
mean loss: 155.55
 ---- batch: 070 ----
mean loss: 159.39
 ---- batch: 080 ----
mean loss: 168.98
 ---- batch: 090 ----
mean loss: 157.64
 ---- batch: 100 ----
mean loss: 158.48
 ---- batch: 110 ----
mean loss: 162.82
train mean loss: 158.84
epoch train time: 0:00:01.887948
elapsed time: 0:04:04.200256
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 22:41:16.145812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.10
 ---- batch: 020 ----
mean loss: 150.50
 ---- batch: 030 ----
mean loss: 156.13
 ---- batch: 040 ----
mean loss: 157.70
 ---- batch: 050 ----
mean loss: 151.90
 ---- batch: 060 ----
mean loss: 157.01
 ---- batch: 070 ----
mean loss: 156.71
 ---- batch: 080 ----
mean loss: 163.52
 ---- batch: 090 ----
mean loss: 166.31
 ---- batch: 100 ----
mean loss: 151.55
 ---- batch: 110 ----
mean loss: 158.23
train mean loss: 157.09
epoch train time: 0:00:01.891729
elapsed time: 0:04:06.092599
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 22:41:18.038179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.22
 ---- batch: 020 ----
mean loss: 150.08
 ---- batch: 030 ----
mean loss: 154.03
 ---- batch: 040 ----
mean loss: 149.53
 ---- batch: 050 ----
mean loss: 156.44
 ---- batch: 060 ----
mean loss: 159.03
 ---- batch: 070 ----
mean loss: 148.10
 ---- batch: 080 ----
mean loss: 162.93
 ---- batch: 090 ----
mean loss: 158.33
 ---- batch: 100 ----
mean loss: 160.31
 ---- batch: 110 ----
mean loss: 166.42
train mean loss: 156.95
epoch train time: 0:00:01.871950
elapsed time: 0:04:07.965176
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 22:41:19.910737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.60
 ---- batch: 020 ----
mean loss: 162.56
 ---- batch: 030 ----
mean loss: 156.41
 ---- batch: 040 ----
mean loss: 154.43
 ---- batch: 050 ----
mean loss: 160.26
 ---- batch: 060 ----
mean loss: 166.57
 ---- batch: 070 ----
mean loss: 152.37
 ---- batch: 080 ----
mean loss: 149.95
 ---- batch: 090 ----
mean loss: 158.48
 ---- batch: 100 ----
mean loss: 159.14
 ---- batch: 110 ----
mean loss: 153.51
train mean loss: 156.66
epoch train time: 0:00:01.897086
elapsed time: 0:04:09.862901
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 22:41:21.808488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.93
 ---- batch: 020 ----
mean loss: 162.00
 ---- batch: 030 ----
mean loss: 153.90
 ---- batch: 040 ----
mean loss: 151.75
 ---- batch: 050 ----
mean loss: 157.19
 ---- batch: 060 ----
mean loss: 153.94
 ---- batch: 070 ----
mean loss: 155.19
 ---- batch: 080 ----
mean loss: 163.74
 ---- batch: 090 ----
mean loss: 155.66
 ---- batch: 100 ----
mean loss: 146.79
 ---- batch: 110 ----
mean loss: 149.74
train mean loss: 155.28
epoch train time: 0:00:01.880691
elapsed time: 0:04:11.744232
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 22:41:23.689801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.27
 ---- batch: 020 ----
mean loss: 156.09
 ---- batch: 030 ----
mean loss: 149.21
 ---- batch: 040 ----
mean loss: 156.25
 ---- batch: 050 ----
mean loss: 153.09
 ---- batch: 060 ----
mean loss: 156.52
 ---- batch: 070 ----
mean loss: 162.58
 ---- batch: 080 ----
mean loss: 164.39
 ---- batch: 090 ----
mean loss: 151.39
 ---- batch: 100 ----
mean loss: 160.76
 ---- batch: 110 ----
mean loss: 152.45
train mean loss: 155.87
epoch train time: 0:00:01.885409
elapsed time: 0:04:13.630289
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 22:41:25.575825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.84
 ---- batch: 020 ----
mean loss: 155.86
 ---- batch: 030 ----
mean loss: 154.64
 ---- batch: 040 ----
mean loss: 150.76
 ---- batch: 050 ----
mean loss: 147.67
 ---- batch: 060 ----
mean loss: 155.13
 ---- batch: 070 ----
mean loss: 159.20
 ---- batch: 080 ----
mean loss: 161.17
 ---- batch: 090 ----
mean loss: 158.28
 ---- batch: 100 ----
mean loss: 156.50
 ---- batch: 110 ----
mean loss: 152.68
train mean loss: 155.57
epoch train time: 0:00:01.868483
elapsed time: 0:04:15.499335
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 22:41:27.444906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.87
 ---- batch: 020 ----
mean loss: 150.93
 ---- batch: 030 ----
mean loss: 151.22
 ---- batch: 040 ----
mean loss: 153.82
 ---- batch: 050 ----
mean loss: 153.28
 ---- batch: 060 ----
mean loss: 152.99
 ---- batch: 070 ----
mean loss: 156.12
 ---- batch: 080 ----
mean loss: 156.95
 ---- batch: 090 ----
mean loss: 154.73
 ---- batch: 100 ----
mean loss: 156.26
 ---- batch: 110 ----
mean loss: 155.01
train mean loss: 153.51
epoch train time: 0:00:01.861537
elapsed time: 0:04:17.361460
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 22:41:29.307008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.97
 ---- batch: 020 ----
mean loss: 148.70
 ---- batch: 030 ----
mean loss: 150.94
 ---- batch: 040 ----
mean loss: 151.72
 ---- batch: 050 ----
mean loss: 154.31
 ---- batch: 060 ----
mean loss: 155.16
 ---- batch: 070 ----
mean loss: 150.58
 ---- batch: 080 ----
mean loss: 154.01
 ---- batch: 090 ----
mean loss: 161.18
 ---- batch: 100 ----
mean loss: 154.53
 ---- batch: 110 ----
mean loss: 154.21
train mean loss: 153.04
epoch train time: 0:00:01.897707
elapsed time: 0:04:19.259719
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 22:41:31.205370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.20
 ---- batch: 020 ----
mean loss: 141.17
 ---- batch: 030 ----
mean loss: 155.97
 ---- batch: 040 ----
mean loss: 155.07
 ---- batch: 050 ----
mean loss: 152.49
 ---- batch: 060 ----
mean loss: 151.86
 ---- batch: 070 ----
mean loss: 156.13
 ---- batch: 080 ----
mean loss: 143.55
 ---- batch: 090 ----
mean loss: 162.54
 ---- batch: 100 ----
mean loss: 155.55
 ---- batch: 110 ----
mean loss: 155.56
train mean loss: 152.69
epoch train time: 0:00:01.879070
elapsed time: 0:04:21.139449
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 22:41:33.085020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.47
 ---- batch: 020 ----
mean loss: 145.67
 ---- batch: 030 ----
mean loss: 152.64
 ---- batch: 040 ----
mean loss: 150.22
 ---- batch: 050 ----
mean loss: 143.28
 ---- batch: 060 ----
mean loss: 153.64
 ---- batch: 070 ----
mean loss: 153.55
 ---- batch: 080 ----
mean loss: 155.30
 ---- batch: 090 ----
mean loss: 149.99
 ---- batch: 100 ----
mean loss: 157.79
 ---- batch: 110 ----
mean loss: 159.97
train mean loss: 152.24
epoch train time: 0:00:01.860546
elapsed time: 0:04:23.000609
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 22:41:34.946159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.26
 ---- batch: 020 ----
mean loss: 155.47
 ---- batch: 030 ----
mean loss: 145.93
 ---- batch: 040 ----
mean loss: 152.96
 ---- batch: 050 ----
mean loss: 153.44
 ---- batch: 060 ----
mean loss: 157.57
 ---- batch: 070 ----
mean loss: 156.70
 ---- batch: 080 ----
mean loss: 154.24
 ---- batch: 090 ----
mean loss: 148.28
 ---- batch: 100 ----
mean loss: 151.15
 ---- batch: 110 ----
mean loss: 145.95
train mean loss: 151.86
epoch train time: 0:00:01.890931
elapsed time: 0:04:24.892105
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 22:41:36.837774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.58
 ---- batch: 020 ----
mean loss: 144.53
 ---- batch: 030 ----
mean loss: 147.28
 ---- batch: 040 ----
mean loss: 153.67
 ---- batch: 050 ----
mean loss: 153.27
 ---- batch: 060 ----
mean loss: 149.55
 ---- batch: 070 ----
mean loss: 150.84
 ---- batch: 080 ----
mean loss: 152.81
 ---- batch: 090 ----
mean loss: 159.02
 ---- batch: 100 ----
mean loss: 154.98
 ---- batch: 110 ----
mean loss: 145.92
train mean loss: 150.86
epoch train time: 0:00:01.885831
elapsed time: 0:04:26.778632
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 22:41:38.724177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.44
 ---- batch: 020 ----
mean loss: 140.48
 ---- batch: 030 ----
mean loss: 149.72
 ---- batch: 040 ----
mean loss: 153.06
 ---- batch: 050 ----
mean loss: 159.25
 ---- batch: 060 ----
mean loss: 157.10
 ---- batch: 070 ----
mean loss: 157.06
 ---- batch: 080 ----
mean loss: 148.76
 ---- batch: 090 ----
mean loss: 149.50
 ---- batch: 100 ----
mean loss: 147.81
 ---- batch: 110 ----
mean loss: 147.88
train mean loss: 150.78
epoch train time: 0:00:01.880810
elapsed time: 0:04:28.659996
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 22:41:40.605533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.46
 ---- batch: 020 ----
mean loss: 146.49
 ---- batch: 030 ----
mean loss: 148.87
 ---- batch: 040 ----
mean loss: 155.25
 ---- batch: 050 ----
mean loss: 140.87
 ---- batch: 060 ----
mean loss: 151.30
 ---- batch: 070 ----
mean loss: 157.01
 ---- batch: 080 ----
mean loss: 154.22
 ---- batch: 090 ----
mean loss: 150.30
 ---- batch: 100 ----
mean loss: 149.53
 ---- batch: 110 ----
mean loss: 155.96
train mean loss: 150.85
epoch train time: 0:00:01.888987
elapsed time: 0:04:30.549533
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 22:41:42.495068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.20
 ---- batch: 020 ----
mean loss: 146.08
 ---- batch: 030 ----
mean loss: 148.11
 ---- batch: 040 ----
mean loss: 148.87
 ---- batch: 050 ----
mean loss: 148.68
 ---- batch: 060 ----
mean loss: 152.18
 ---- batch: 070 ----
mean loss: 147.40
 ---- batch: 080 ----
mean loss: 155.38
 ---- batch: 090 ----
mean loss: 151.42
 ---- batch: 100 ----
mean loss: 147.61
 ---- batch: 110 ----
mean loss: 144.80
train mean loss: 149.10
epoch train time: 0:00:01.887860
elapsed time: 0:04:32.438240
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 22:41:44.383595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.32
 ---- batch: 020 ----
mean loss: 147.37
 ---- batch: 030 ----
mean loss: 149.38
 ---- batch: 040 ----
mean loss: 130.20
 ---- batch: 050 ----
mean loss: 160.98
 ---- batch: 060 ----
mean loss: 148.06
 ---- batch: 070 ----
mean loss: 148.29
 ---- batch: 080 ----
mean loss: 150.38
 ---- batch: 090 ----
mean loss: 153.56
 ---- batch: 100 ----
mean loss: 147.34
 ---- batch: 110 ----
mean loss: 152.74
train mean loss: 148.67
epoch train time: 0:00:01.921268
elapsed time: 0:04:34.359858
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 22:41:46.305403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.32
 ---- batch: 020 ----
mean loss: 135.66
 ---- batch: 030 ----
mean loss: 153.15
 ---- batch: 040 ----
mean loss: 146.86
 ---- batch: 050 ----
mean loss: 151.47
 ---- batch: 060 ----
mean loss: 147.05
 ---- batch: 070 ----
mean loss: 152.28
 ---- batch: 080 ----
mean loss: 151.98
 ---- batch: 090 ----
mean loss: 143.99
 ---- batch: 100 ----
mean loss: 155.67
 ---- batch: 110 ----
mean loss: 143.26
train mean loss: 148.50
epoch train time: 0:00:01.880171
elapsed time: 0:04:36.240602
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 22:41:48.186127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.97
 ---- batch: 020 ----
mean loss: 152.80
 ---- batch: 030 ----
mean loss: 146.88
 ---- batch: 040 ----
mean loss: 144.65
 ---- batch: 050 ----
mean loss: 146.72
 ---- batch: 060 ----
mean loss: 151.38
 ---- batch: 070 ----
mean loss: 140.56
 ---- batch: 080 ----
mean loss: 148.58
 ---- batch: 090 ----
mean loss: 149.38
 ---- batch: 100 ----
mean loss: 149.11
 ---- batch: 110 ----
mean loss: 152.94
train mean loss: 148.02
epoch train time: 0:00:01.886380
elapsed time: 0:04:38.127529
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 22:41:50.073083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.80
 ---- batch: 020 ----
mean loss: 147.13
 ---- batch: 030 ----
mean loss: 143.89
 ---- batch: 040 ----
mean loss: 150.18
 ---- batch: 050 ----
mean loss: 140.03
 ---- batch: 060 ----
mean loss: 146.38
 ---- batch: 070 ----
mean loss: 153.75
 ---- batch: 080 ----
mean loss: 152.18
 ---- batch: 090 ----
mean loss: 151.91
 ---- batch: 100 ----
mean loss: 142.13
 ---- batch: 110 ----
mean loss: 144.53
train mean loss: 147.22
epoch train time: 0:00:01.889976
elapsed time: 0:04:40.018062
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 22:41:51.963613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.46
 ---- batch: 020 ----
mean loss: 144.70
 ---- batch: 030 ----
mean loss: 142.79
 ---- batch: 040 ----
mean loss: 148.37
 ---- batch: 050 ----
mean loss: 148.23
 ---- batch: 060 ----
mean loss: 144.37
 ---- batch: 070 ----
mean loss: 143.63
 ---- batch: 080 ----
mean loss: 148.20
 ---- batch: 090 ----
mean loss: 142.56
 ---- batch: 100 ----
mean loss: 153.58
 ---- batch: 110 ----
mean loss: 154.02
train mean loss: 146.78
epoch train time: 0:00:01.871156
elapsed time: 0:04:41.889835
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 22:41:53.835560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.79
 ---- batch: 020 ----
mean loss: 148.72
 ---- batch: 030 ----
mean loss: 142.68
 ---- batch: 040 ----
mean loss: 147.46
 ---- batch: 050 ----
mean loss: 145.35
 ---- batch: 060 ----
mean loss: 152.40
 ---- batch: 070 ----
mean loss: 154.52
 ---- batch: 080 ----
mean loss: 138.45
 ---- batch: 090 ----
mean loss: 138.77
 ---- batch: 100 ----
mean loss: 146.75
 ---- batch: 110 ----
mean loss: 145.18
train mean loss: 146.23
epoch train time: 0:00:01.869665
elapsed time: 0:04:43.760318
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 22:41:55.706027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.53
 ---- batch: 020 ----
mean loss: 142.35
 ---- batch: 030 ----
mean loss: 144.88
 ---- batch: 040 ----
mean loss: 140.40
 ---- batch: 050 ----
mean loss: 145.42
 ---- batch: 060 ----
mean loss: 141.39
 ---- batch: 070 ----
mean loss: 146.69
 ---- batch: 080 ----
mean loss: 145.56
 ---- batch: 090 ----
mean loss: 153.70
 ---- batch: 100 ----
mean loss: 139.59
 ---- batch: 110 ----
mean loss: 152.63
train mean loss: 146.24
epoch train time: 0:00:01.900047
elapsed time: 0:04:45.661172
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 22:41:57.606729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.88
 ---- batch: 020 ----
mean loss: 145.02
 ---- batch: 030 ----
mean loss: 152.37
 ---- batch: 040 ----
mean loss: 142.56
 ---- batch: 050 ----
mean loss: 147.08
 ---- batch: 060 ----
mean loss: 143.83
 ---- batch: 070 ----
mean loss: 145.32
 ---- batch: 080 ----
mean loss: 141.67
 ---- batch: 090 ----
mean loss: 143.91
 ---- batch: 100 ----
mean loss: 151.74
 ---- batch: 110 ----
mean loss: 146.51
train mean loss: 145.45
epoch train time: 0:00:01.872893
elapsed time: 0:04:47.534677
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 22:41:59.480266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.49
 ---- batch: 020 ----
mean loss: 146.26
 ---- batch: 030 ----
mean loss: 148.09
 ---- batch: 040 ----
mean loss: 147.87
 ---- batch: 050 ----
mean loss: 140.35
 ---- batch: 060 ----
mean loss: 144.53
 ---- batch: 070 ----
mean loss: 145.74
 ---- batch: 080 ----
mean loss: 144.40
 ---- batch: 090 ----
mean loss: 143.24
 ---- batch: 100 ----
mean loss: 138.14
 ---- batch: 110 ----
mean loss: 150.39
train mean loss: 144.56
epoch train time: 0:00:01.888395
elapsed time: 0:04:49.423787
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 22:42:01.369559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.58
 ---- batch: 020 ----
mean loss: 144.09
 ---- batch: 030 ----
mean loss: 145.35
 ---- batch: 040 ----
mean loss: 142.03
 ---- batch: 050 ----
mean loss: 137.79
 ---- batch: 060 ----
mean loss: 140.63
 ---- batch: 070 ----
mean loss: 149.43
 ---- batch: 080 ----
mean loss: 147.95
 ---- batch: 090 ----
mean loss: 149.72
 ---- batch: 100 ----
mean loss: 140.94
 ---- batch: 110 ----
mean loss: 141.69
train mean loss: 144.85
epoch train time: 0:00:01.891501
elapsed time: 0:04:51.316085
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 22:42:03.261642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.36
 ---- batch: 020 ----
mean loss: 141.37
 ---- batch: 030 ----
mean loss: 137.35
 ---- batch: 040 ----
mean loss: 146.15
 ---- batch: 050 ----
mean loss: 146.56
 ---- batch: 060 ----
mean loss: 150.07
 ---- batch: 070 ----
mean loss: 139.14
 ---- batch: 080 ----
mean loss: 143.15
 ---- batch: 090 ----
mean loss: 138.21
 ---- batch: 100 ----
mean loss: 155.59
 ---- batch: 110 ----
mean loss: 155.79
train mean loss: 144.15
epoch train time: 0:00:01.889371
elapsed time: 0:04:53.206047
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 22:42:05.151600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.51
 ---- batch: 020 ----
mean loss: 149.15
 ---- batch: 030 ----
mean loss: 133.50
 ---- batch: 040 ----
mean loss: 150.37
 ---- batch: 050 ----
mean loss: 138.37
 ---- batch: 060 ----
mean loss: 149.72
 ---- batch: 070 ----
mean loss: 137.98
 ---- batch: 080 ----
mean loss: 141.66
 ---- batch: 090 ----
mean loss: 137.83
 ---- batch: 100 ----
mean loss: 148.08
 ---- batch: 110 ----
mean loss: 142.75
train mean loss: 143.63
epoch train time: 0:00:01.890779
elapsed time: 0:04:55.097400
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 22:42:07.042941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.37
 ---- batch: 020 ----
mean loss: 141.32
 ---- batch: 030 ----
mean loss: 147.46
 ---- batch: 040 ----
mean loss: 148.07
 ---- batch: 050 ----
mean loss: 150.02
 ---- batch: 060 ----
mean loss: 149.54
 ---- batch: 070 ----
mean loss: 139.00
 ---- batch: 080 ----
mean loss: 138.61
 ---- batch: 090 ----
mean loss: 144.21
 ---- batch: 100 ----
mean loss: 133.27
 ---- batch: 110 ----
mean loss: 151.44
train mean loss: 143.81
epoch train time: 0:00:01.856002
elapsed time: 0:04:56.953960
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 22:42:08.899616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.61
 ---- batch: 020 ----
mean loss: 138.90
 ---- batch: 030 ----
mean loss: 146.71
 ---- batch: 040 ----
mean loss: 146.81
 ---- batch: 050 ----
mean loss: 142.88
 ---- batch: 060 ----
mean loss: 135.89
 ---- batch: 070 ----
mean loss: 145.37
 ---- batch: 080 ----
mean loss: 143.26
 ---- batch: 090 ----
mean loss: 150.94
 ---- batch: 100 ----
mean loss: 140.61
 ---- batch: 110 ----
mean loss: 138.61
train mean loss: 142.49
epoch train time: 0:00:01.885198
elapsed time: 0:04:58.839829
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 22:42:10.785362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.19
 ---- batch: 020 ----
mean loss: 140.31
 ---- batch: 030 ----
mean loss: 146.47
 ---- batch: 040 ----
mean loss: 139.51
 ---- batch: 050 ----
mean loss: 141.68
 ---- batch: 060 ----
mean loss: 145.17
 ---- batch: 070 ----
mean loss: 137.48
 ---- batch: 080 ----
mean loss: 146.49
 ---- batch: 090 ----
mean loss: 143.40
 ---- batch: 100 ----
mean loss: 141.73
 ---- batch: 110 ----
mean loss: 140.83
train mean loss: 142.60
epoch train time: 0:00:01.867960
elapsed time: 0:05:00.708324
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 22:42:12.653894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.36
 ---- batch: 020 ----
mean loss: 144.54
 ---- batch: 030 ----
mean loss: 139.01
 ---- batch: 040 ----
mean loss: 142.47
 ---- batch: 050 ----
mean loss: 136.49
 ---- batch: 060 ----
mean loss: 141.84
 ---- batch: 070 ----
mean loss: 139.40
 ---- batch: 080 ----
mean loss: 140.14
 ---- batch: 090 ----
mean loss: 139.88
 ---- batch: 100 ----
mean loss: 148.97
 ---- batch: 110 ----
mean loss: 150.78
train mean loss: 141.81
epoch train time: 0:00:01.875534
elapsed time: 0:05:02.584488
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 22:42:14.530041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.07
 ---- batch: 020 ----
mean loss: 133.36
 ---- batch: 030 ----
mean loss: 135.27
 ---- batch: 040 ----
mean loss: 143.92
 ---- batch: 050 ----
mean loss: 144.05
 ---- batch: 060 ----
mean loss: 136.42
 ---- batch: 070 ----
mean loss: 145.83
 ---- batch: 080 ----
mean loss: 150.08
 ---- batch: 090 ----
mean loss: 153.63
 ---- batch: 100 ----
mean loss: 143.22
 ---- batch: 110 ----
mean loss: 137.51
train mean loss: 141.98
epoch train time: 0:00:01.884291
elapsed time: 0:05:04.469385
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 22:42:16.414936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.91
 ---- batch: 020 ----
mean loss: 138.71
 ---- batch: 030 ----
mean loss: 144.12
 ---- batch: 040 ----
mean loss: 145.49
 ---- batch: 050 ----
mean loss: 140.70
 ---- batch: 060 ----
mean loss: 140.06
 ---- batch: 070 ----
mean loss: 137.61
 ---- batch: 080 ----
mean loss: 142.15
 ---- batch: 090 ----
mean loss: 141.45
 ---- batch: 100 ----
mean loss: 143.67
 ---- batch: 110 ----
mean loss: 141.86
train mean loss: 141.31
epoch train time: 0:00:01.889921
elapsed time: 0:05:06.359869
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 22:42:18.305421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.80
 ---- batch: 020 ----
mean loss: 144.74
 ---- batch: 030 ----
mean loss: 141.26
 ---- batch: 040 ----
mean loss: 138.84
 ---- batch: 050 ----
mean loss: 145.14
 ---- batch: 060 ----
mean loss: 141.30
 ---- batch: 070 ----
mean loss: 140.47
 ---- batch: 080 ----
mean loss: 142.23
 ---- batch: 090 ----
mean loss: 144.01
 ---- batch: 100 ----
mean loss: 138.01
 ---- batch: 110 ----
mean loss: 137.42
train mean loss: 140.31
epoch train time: 0:00:01.889945
elapsed time: 0:05:08.250376
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 22:42:20.195933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.57
 ---- batch: 020 ----
mean loss: 139.82
 ---- batch: 030 ----
mean loss: 133.25
 ---- batch: 040 ----
mean loss: 139.78
 ---- batch: 050 ----
mean loss: 138.15
 ---- batch: 060 ----
mean loss: 137.14
 ---- batch: 070 ----
mean loss: 143.93
 ---- batch: 080 ----
mean loss: 140.24
 ---- batch: 090 ----
mean loss: 139.84
 ---- batch: 100 ----
mean loss: 140.97
 ---- batch: 110 ----
mean loss: 141.54
train mean loss: 139.85
epoch train time: 0:00:01.934452
elapsed time: 0:05:10.185429
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 22:42:22.131065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.31
 ---- batch: 020 ----
mean loss: 140.85
 ---- batch: 030 ----
mean loss: 136.43
 ---- batch: 040 ----
mean loss: 134.25
 ---- batch: 050 ----
mean loss: 141.09
 ---- batch: 060 ----
mean loss: 136.20
 ---- batch: 070 ----
mean loss: 143.68
 ---- batch: 080 ----
mean loss: 144.36
 ---- batch: 090 ----
mean loss: 143.93
 ---- batch: 100 ----
mean loss: 146.33
 ---- batch: 110 ----
mean loss: 134.28
train mean loss: 139.91
epoch train time: 0:00:01.881562
elapsed time: 0:05:12.067666
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 22:42:24.013213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.45
 ---- batch: 020 ----
mean loss: 141.37
 ---- batch: 030 ----
mean loss: 139.50
 ---- batch: 040 ----
mean loss: 135.97
 ---- batch: 050 ----
mean loss: 140.20
 ---- batch: 060 ----
mean loss: 132.72
 ---- batch: 070 ----
mean loss: 142.38
 ---- batch: 080 ----
mean loss: 146.26
 ---- batch: 090 ----
mean loss: 141.24
 ---- batch: 100 ----
mean loss: 142.95
 ---- batch: 110 ----
mean loss: 137.54
train mean loss: 139.24
epoch train time: 0:00:01.884871
elapsed time: 0:05:13.953430
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 22:42:25.898704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.58
 ---- batch: 020 ----
mean loss: 136.16
 ---- batch: 030 ----
mean loss: 130.63
 ---- batch: 040 ----
mean loss: 129.84
 ---- batch: 050 ----
mean loss: 135.92
 ---- batch: 060 ----
mean loss: 141.39
 ---- batch: 070 ----
mean loss: 144.20
 ---- batch: 080 ----
mean loss: 146.79
 ---- batch: 090 ----
mean loss: 143.10
 ---- batch: 100 ----
mean loss: 143.06
 ---- batch: 110 ----
mean loss: 139.33
train mean loss: 139.37
epoch train time: 0:00:01.888988
elapsed time: 0:05:15.842732
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 22:42:27.788273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.99
 ---- batch: 020 ----
mean loss: 133.13
 ---- batch: 030 ----
mean loss: 135.55
 ---- batch: 040 ----
mean loss: 136.49
 ---- batch: 050 ----
mean loss: 138.78
 ---- batch: 060 ----
mean loss: 142.70
 ---- batch: 070 ----
mean loss: 141.21
 ---- batch: 080 ----
mean loss: 142.52
 ---- batch: 090 ----
mean loss: 144.73
 ---- batch: 100 ----
mean loss: 137.23
 ---- batch: 110 ----
mean loss: 141.62
train mean loss: 139.18
epoch train time: 0:00:01.888192
elapsed time: 0:05:17.731482
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 22:42:29.676772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.40
 ---- batch: 020 ----
mean loss: 130.92
 ---- batch: 030 ----
mean loss: 140.50
 ---- batch: 040 ----
mean loss: 136.99
 ---- batch: 050 ----
mean loss: 133.61
 ---- batch: 060 ----
mean loss: 142.10
 ---- batch: 070 ----
mean loss: 143.57
 ---- batch: 080 ----
mean loss: 142.28
 ---- batch: 090 ----
mean loss: 141.28
 ---- batch: 100 ----
mean loss: 146.90
 ---- batch: 110 ----
mean loss: 135.63
train mean loss: 139.44
epoch train time: 0:00:01.891851
elapsed time: 0:05:19.623656
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 22:42:31.569246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.56
 ---- batch: 020 ----
mean loss: 129.37
 ---- batch: 030 ----
mean loss: 134.57
 ---- batch: 040 ----
mean loss: 129.93
 ---- batch: 050 ----
mean loss: 139.46
 ---- batch: 060 ----
mean loss: 136.46
 ---- batch: 070 ----
mean loss: 140.43
 ---- batch: 080 ----
mean loss: 141.45
 ---- batch: 090 ----
mean loss: 144.77
 ---- batch: 100 ----
mean loss: 143.38
 ---- batch: 110 ----
mean loss: 137.09
train mean loss: 138.29
epoch train time: 0:00:01.872250
elapsed time: 0:05:21.496728
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 22:42:33.442322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.93
 ---- batch: 020 ----
mean loss: 125.85
 ---- batch: 030 ----
mean loss: 138.90
 ---- batch: 040 ----
mean loss: 134.84
 ---- batch: 050 ----
mean loss: 143.32
 ---- batch: 060 ----
mean loss: 143.64
 ---- batch: 070 ----
mean loss: 142.45
 ---- batch: 080 ----
mean loss: 138.90
 ---- batch: 090 ----
mean loss: 135.46
 ---- batch: 100 ----
mean loss: 136.27
 ---- batch: 110 ----
mean loss: 140.14
train mean loss: 138.34
epoch train time: 0:00:01.856289
elapsed time: 0:05:23.353614
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 22:42:35.299168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.57
 ---- batch: 020 ----
mean loss: 129.24
 ---- batch: 030 ----
mean loss: 136.67
 ---- batch: 040 ----
mean loss: 133.47
 ---- batch: 050 ----
mean loss: 136.61
 ---- batch: 060 ----
mean loss: 138.00
 ---- batch: 070 ----
mean loss: 135.11
 ---- batch: 080 ----
mean loss: 131.43
 ---- batch: 090 ----
mean loss: 142.89
 ---- batch: 100 ----
mean loss: 136.95
 ---- batch: 110 ----
mean loss: 145.19
train mean loss: 136.45
epoch train time: 0:00:01.850645
elapsed time: 0:05:25.204817
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 22:42:37.150357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.11
 ---- batch: 020 ----
mean loss: 130.82
 ---- batch: 030 ----
mean loss: 139.00
 ---- batch: 040 ----
mean loss: 134.38
 ---- batch: 050 ----
mean loss: 135.05
 ---- batch: 060 ----
mean loss: 134.48
 ---- batch: 070 ----
mean loss: 146.34
 ---- batch: 080 ----
mean loss: 138.79
 ---- batch: 090 ----
mean loss: 138.39
 ---- batch: 100 ----
mean loss: 138.00
 ---- batch: 110 ----
mean loss: 141.23
train mean loss: 137.41
epoch train time: 0:00:01.882710
elapsed time: 0:05:27.088089
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 22:42:39.033709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.39
 ---- batch: 020 ----
mean loss: 130.51
 ---- batch: 030 ----
mean loss: 138.52
 ---- batch: 040 ----
mean loss: 146.44
 ---- batch: 050 ----
mean loss: 137.49
 ---- batch: 060 ----
mean loss: 134.57
 ---- batch: 070 ----
mean loss: 140.89
 ---- batch: 080 ----
mean loss: 133.86
 ---- batch: 090 ----
mean loss: 136.21
 ---- batch: 100 ----
mean loss: 140.95
 ---- batch: 110 ----
mean loss: 139.11
train mean loss: 137.23
epoch train time: 0:00:01.890284
elapsed time: 0:05:28.979002
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 22:42:40.924535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.93
 ---- batch: 020 ----
mean loss: 135.13
 ---- batch: 030 ----
mean loss: 139.83
 ---- batch: 040 ----
mean loss: 132.61
 ---- batch: 050 ----
mean loss: 129.05
 ---- batch: 060 ----
mean loss: 142.65
 ---- batch: 070 ----
mean loss: 128.74
 ---- batch: 080 ----
mean loss: 133.63
 ---- batch: 090 ----
mean loss: 143.35
 ---- batch: 100 ----
mean loss: 138.63
 ---- batch: 110 ----
mean loss: 142.92
train mean loss: 136.15
epoch train time: 0:00:01.884564
elapsed time: 0:05:30.864141
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 22:42:42.809717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.83
 ---- batch: 020 ----
mean loss: 135.53
 ---- batch: 030 ----
mean loss: 133.93
 ---- batch: 040 ----
mean loss: 132.32
 ---- batch: 050 ----
mean loss: 133.54
 ---- batch: 060 ----
mean loss: 131.54
 ---- batch: 070 ----
mean loss: 141.35
 ---- batch: 080 ----
mean loss: 133.84
 ---- batch: 090 ----
mean loss: 126.06
 ---- batch: 100 ----
mean loss: 141.59
 ---- batch: 110 ----
mean loss: 145.82
train mean loss: 135.88
epoch train time: 0:00:01.885212
elapsed time: 0:05:32.749991
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 22:42:44.695533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.53
 ---- batch: 020 ----
mean loss: 130.11
 ---- batch: 030 ----
mean loss: 129.38
 ---- batch: 040 ----
mean loss: 140.80
 ---- batch: 050 ----
mean loss: 136.30
 ---- batch: 060 ----
mean loss: 129.97
 ---- batch: 070 ----
mean loss: 139.62
 ---- batch: 080 ----
mean loss: 136.09
 ---- batch: 090 ----
mean loss: 133.41
 ---- batch: 100 ----
mean loss: 140.81
 ---- batch: 110 ----
mean loss: 134.92
train mean loss: 135.24
epoch train time: 0:00:01.853388
elapsed time: 0:05:34.603932
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 22:42:46.549475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.79
 ---- batch: 020 ----
mean loss: 132.37
 ---- batch: 030 ----
mean loss: 130.97
 ---- batch: 040 ----
mean loss: 140.88
 ---- batch: 050 ----
mean loss: 143.91
 ---- batch: 060 ----
mean loss: 129.14
 ---- batch: 070 ----
mean loss: 130.56
 ---- batch: 080 ----
mean loss: 145.57
 ---- batch: 090 ----
mean loss: 138.82
 ---- batch: 100 ----
mean loss: 124.97
 ---- batch: 110 ----
mean loss: 129.52
train mean loss: 134.56
epoch train time: 0:00:01.893753
elapsed time: 0:05:36.498281
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 22:42:48.443827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.92
 ---- batch: 020 ----
mean loss: 133.75
 ---- batch: 030 ----
mean loss: 129.08
 ---- batch: 040 ----
mean loss: 135.46
 ---- batch: 050 ----
mean loss: 141.70
 ---- batch: 060 ----
mean loss: 133.10
 ---- batch: 070 ----
mean loss: 132.03
 ---- batch: 080 ----
mean loss: 135.25
 ---- batch: 090 ----
mean loss: 135.93
 ---- batch: 100 ----
mean loss: 143.30
 ---- batch: 110 ----
mean loss: 136.98
train mean loss: 134.96
epoch train time: 0:00:01.886001
elapsed time: 0:05:38.384842
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 22:42:50.330398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.49
 ---- batch: 020 ----
mean loss: 136.50
 ---- batch: 030 ----
mean loss: 135.06
 ---- batch: 040 ----
mean loss: 127.46
 ---- batch: 050 ----
mean loss: 139.33
 ---- batch: 060 ----
mean loss: 139.28
 ---- batch: 070 ----
mean loss: 133.62
 ---- batch: 080 ----
mean loss: 120.32
 ---- batch: 090 ----
mean loss: 130.18
 ---- batch: 100 ----
mean loss: 143.55
 ---- batch: 110 ----
mean loss: 140.10
train mean loss: 134.33
epoch train time: 0:00:01.868933
elapsed time: 0:05:40.254353
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 22:42:52.199893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.32
 ---- batch: 020 ----
mean loss: 134.96
 ---- batch: 030 ----
mean loss: 134.35
 ---- batch: 040 ----
mean loss: 138.02
 ---- batch: 050 ----
mean loss: 132.63
 ---- batch: 060 ----
mean loss: 133.81
 ---- batch: 070 ----
mean loss: 131.87
 ---- batch: 080 ----
mean loss: 132.52
 ---- batch: 090 ----
mean loss: 134.04
 ---- batch: 100 ----
mean loss: 125.83
 ---- batch: 110 ----
mean loss: 133.44
train mean loss: 133.64
epoch train time: 0:00:01.881873
elapsed time: 0:05:42.136809
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 22:42:54.082344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.78
 ---- batch: 020 ----
mean loss: 134.92
 ---- batch: 030 ----
mean loss: 133.37
 ---- batch: 040 ----
mean loss: 134.38
 ---- batch: 050 ----
mean loss: 130.33
 ---- batch: 060 ----
mean loss: 132.24
 ---- batch: 070 ----
mean loss: 140.79
 ---- batch: 080 ----
mean loss: 133.37
 ---- batch: 090 ----
mean loss: 127.02
 ---- batch: 100 ----
mean loss: 134.89
 ---- batch: 110 ----
mean loss: 134.94
train mean loss: 133.51
epoch train time: 0:00:01.871213
elapsed time: 0:05:44.008600
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 22:42:55.954183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.60
 ---- batch: 020 ----
mean loss: 125.99
 ---- batch: 030 ----
mean loss: 133.19
 ---- batch: 040 ----
mean loss: 130.99
 ---- batch: 050 ----
mean loss: 145.50
 ---- batch: 060 ----
mean loss: 126.92
 ---- batch: 070 ----
mean loss: 134.89
 ---- batch: 080 ----
mean loss: 135.36
 ---- batch: 090 ----
mean loss: 141.85
 ---- batch: 100 ----
mean loss: 131.21
 ---- batch: 110 ----
mean loss: 122.30
train mean loss: 133.38
epoch train time: 0:00:01.906659
elapsed time: 0:05:45.915904
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 22:42:57.861511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.36
 ---- batch: 020 ----
mean loss: 133.67
 ---- batch: 030 ----
mean loss: 128.37
 ---- batch: 040 ----
mean loss: 130.76
 ---- batch: 050 ----
mean loss: 130.92
 ---- batch: 060 ----
mean loss: 133.35
 ---- batch: 070 ----
mean loss: 126.64
 ---- batch: 080 ----
mean loss: 144.17
 ---- batch: 090 ----
mean loss: 135.61
 ---- batch: 100 ----
mean loss: 146.61
 ---- batch: 110 ----
mean loss: 129.36
train mean loss: 133.10
epoch train time: 0:00:01.891765
elapsed time: 0:05:47.808347
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 22:42:59.754025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.43
 ---- batch: 020 ----
mean loss: 130.82
 ---- batch: 030 ----
mean loss: 130.59
 ---- batch: 040 ----
mean loss: 129.82
 ---- batch: 050 ----
mean loss: 133.74
 ---- batch: 060 ----
mean loss: 128.83
 ---- batch: 070 ----
mean loss: 133.69
 ---- batch: 080 ----
mean loss: 128.50
 ---- batch: 090 ----
mean loss: 136.45
 ---- batch: 100 ----
mean loss: 137.85
 ---- batch: 110 ----
mean loss: 141.54
train mean loss: 132.57
epoch train time: 0:00:01.864571
elapsed time: 0:05:49.673615
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 22:43:01.619157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.64
 ---- batch: 020 ----
mean loss: 134.06
 ---- batch: 030 ----
mean loss: 127.03
 ---- batch: 040 ----
mean loss: 139.48
 ---- batch: 050 ----
mean loss: 136.59
 ---- batch: 060 ----
mean loss: 126.45
 ---- batch: 070 ----
mean loss: 123.65
 ---- batch: 080 ----
mean loss: 134.99
 ---- batch: 090 ----
mean loss: 132.72
 ---- batch: 100 ----
mean loss: 130.03
 ---- batch: 110 ----
mean loss: 134.93
train mean loss: 131.46
epoch train time: 0:00:01.873474
elapsed time: 0:05:51.547634
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 22:43:03.493192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.82
 ---- batch: 020 ----
mean loss: 132.53
 ---- batch: 030 ----
mean loss: 124.79
 ---- batch: 040 ----
mean loss: 136.67
 ---- batch: 050 ----
mean loss: 135.21
 ---- batch: 060 ----
mean loss: 134.94
 ---- batch: 070 ----
mean loss: 132.06
 ---- batch: 080 ----
mean loss: 134.36
 ---- batch: 090 ----
mean loss: 133.40
 ---- batch: 100 ----
mean loss: 136.78
 ---- batch: 110 ----
mean loss: 125.15
train mean loss: 132.01
epoch train time: 0:00:01.885827
elapsed time: 0:05:53.434063
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 22:43:05.379646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.38
 ---- batch: 020 ----
mean loss: 135.80
 ---- batch: 030 ----
mean loss: 133.18
 ---- batch: 040 ----
mean loss: 132.21
 ---- batch: 050 ----
mean loss: 132.82
 ---- batch: 060 ----
mean loss: 138.38
 ---- batch: 070 ----
mean loss: 125.67
 ---- batch: 080 ----
mean loss: 133.70
 ---- batch: 090 ----
mean loss: 134.72
 ---- batch: 100 ----
mean loss: 124.01
 ---- batch: 110 ----
mean loss: 130.76
train mean loss: 132.15
epoch train time: 0:00:01.901408
elapsed time: 0:05:55.336086
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 22:43:07.281687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.27
 ---- batch: 020 ----
mean loss: 140.44
 ---- batch: 030 ----
mean loss: 139.69
 ---- batch: 040 ----
mean loss: 127.09
 ---- batch: 050 ----
mean loss: 129.05
 ---- batch: 060 ----
mean loss: 126.17
 ---- batch: 070 ----
mean loss: 139.11
 ---- batch: 080 ----
mean loss: 129.20
 ---- batch: 090 ----
mean loss: 133.09
 ---- batch: 100 ----
mean loss: 131.45
 ---- batch: 110 ----
mean loss: 132.92
train mean loss: 131.80
epoch train time: 0:00:01.911748
elapsed time: 0:05:57.248498
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 22:43:09.194050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.15
 ---- batch: 020 ----
mean loss: 131.58
 ---- batch: 030 ----
mean loss: 128.56
 ---- batch: 040 ----
mean loss: 125.31
 ---- batch: 050 ----
mean loss: 133.55
 ---- batch: 060 ----
mean loss: 132.53
 ---- batch: 070 ----
mean loss: 122.72
 ---- batch: 080 ----
mean loss: 131.64
 ---- batch: 090 ----
mean loss: 137.31
 ---- batch: 100 ----
mean loss: 135.56
 ---- batch: 110 ----
mean loss: 135.93
train mean loss: 130.98
epoch train time: 0:00:01.906774
elapsed time: 0:05:59.155828
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 22:43:11.101384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.76
 ---- batch: 020 ----
mean loss: 131.26
 ---- batch: 030 ----
mean loss: 133.12
 ---- batch: 040 ----
mean loss: 131.82
 ---- batch: 050 ----
mean loss: 124.29
 ---- batch: 060 ----
mean loss: 131.88
 ---- batch: 070 ----
mean loss: 133.87
 ---- batch: 080 ----
mean loss: 125.56
 ---- batch: 090 ----
mean loss: 135.56
 ---- batch: 100 ----
mean loss: 126.26
 ---- batch: 110 ----
mean loss: 138.28
train mean loss: 131.46
epoch train time: 0:00:01.900289
elapsed time: 0:06:01.057013
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 22:43:13.002350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.86
 ---- batch: 020 ----
mean loss: 134.39
 ---- batch: 030 ----
mean loss: 124.55
 ---- batch: 040 ----
mean loss: 129.85
 ---- batch: 050 ----
mean loss: 133.96
 ---- batch: 060 ----
mean loss: 131.46
 ---- batch: 070 ----
mean loss: 132.63
 ---- batch: 080 ----
mean loss: 126.31
 ---- batch: 090 ----
mean loss: 132.85
 ---- batch: 100 ----
mean loss: 136.48
 ---- batch: 110 ----
mean loss: 131.75
train mean loss: 130.84
epoch train time: 0:00:01.896393
elapsed time: 0:06:02.953778
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 22:43:14.899321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.50
 ---- batch: 020 ----
mean loss: 130.56
 ---- batch: 030 ----
mean loss: 128.49
 ---- batch: 040 ----
mean loss: 126.06
 ---- batch: 050 ----
mean loss: 127.53
 ---- batch: 060 ----
mean loss: 132.89
 ---- batch: 070 ----
mean loss: 131.12
 ---- batch: 080 ----
mean loss: 129.93
 ---- batch: 090 ----
mean loss: 130.83
 ---- batch: 100 ----
mean loss: 127.28
 ---- batch: 110 ----
mean loss: 129.49
train mean loss: 130.02
epoch train time: 0:00:01.873603
elapsed time: 0:06:04.827978
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 22:43:16.773527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.55
 ---- batch: 020 ----
mean loss: 131.74
 ---- batch: 030 ----
mean loss: 120.31
 ---- batch: 040 ----
mean loss: 138.64
 ---- batch: 050 ----
mean loss: 131.57
 ---- batch: 060 ----
mean loss: 128.90
 ---- batch: 070 ----
mean loss: 130.86
 ---- batch: 080 ----
mean loss: 128.49
 ---- batch: 090 ----
mean loss: 122.48
 ---- batch: 100 ----
mean loss: 132.55
 ---- batch: 110 ----
mean loss: 128.41
train mean loss: 129.15
epoch train time: 0:00:01.876390
elapsed time: 0:06:06.704967
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 22:43:18.650519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.37
 ---- batch: 020 ----
mean loss: 134.36
 ---- batch: 030 ----
mean loss: 133.57
 ---- batch: 040 ----
mean loss: 126.57
 ---- batch: 050 ----
mean loss: 126.60
 ---- batch: 060 ----
mean loss: 131.38
 ---- batch: 070 ----
mean loss: 125.29
 ---- batch: 080 ----
mean loss: 127.60
 ---- batch: 090 ----
mean loss: 133.49
 ---- batch: 100 ----
mean loss: 133.85
 ---- batch: 110 ----
mean loss: 124.23
train mean loss: 129.69
epoch train time: 0:00:01.889886
elapsed time: 0:06:08.595415
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 22:43:20.540966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.63
 ---- batch: 020 ----
mean loss: 134.91
 ---- batch: 030 ----
mean loss: 129.42
 ---- batch: 040 ----
mean loss: 123.15
 ---- batch: 050 ----
mean loss: 128.44
 ---- batch: 060 ----
mean loss: 128.97
 ---- batch: 070 ----
mean loss: 121.89
 ---- batch: 080 ----
mean loss: 130.22
 ---- batch: 090 ----
mean loss: 132.10
 ---- batch: 100 ----
mean loss: 127.43
 ---- batch: 110 ----
mean loss: 131.37
train mean loss: 128.75
epoch train time: 0:00:01.854852
elapsed time: 0:06:10.450837
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 22:43:22.396380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.01
 ---- batch: 020 ----
mean loss: 123.04
 ---- batch: 030 ----
mean loss: 126.63
 ---- batch: 040 ----
mean loss: 135.99
 ---- batch: 050 ----
mean loss: 128.81
 ---- batch: 060 ----
mean loss: 121.94
 ---- batch: 070 ----
mean loss: 132.18
 ---- batch: 080 ----
mean loss: 130.60
 ---- batch: 090 ----
mean loss: 129.00
 ---- batch: 100 ----
mean loss: 131.29
 ---- batch: 110 ----
mean loss: 137.84
train mean loss: 129.07
epoch train time: 0:00:01.900449
elapsed time: 0:06:12.351848
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 22:43:24.297531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.49
 ---- batch: 020 ----
mean loss: 130.85
 ---- batch: 030 ----
mean loss: 122.36
 ---- batch: 040 ----
mean loss: 124.38
 ---- batch: 050 ----
mean loss: 137.70
 ---- batch: 060 ----
mean loss: 131.72
 ---- batch: 070 ----
mean loss: 132.08
 ---- batch: 080 ----
mean loss: 128.94
 ---- batch: 090 ----
mean loss: 125.85
 ---- batch: 100 ----
mean loss: 129.30
 ---- batch: 110 ----
mean loss: 133.23
train mean loss: 128.72
epoch train time: 0:00:01.907492
elapsed time: 0:06:14.260035
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 22:43:26.205567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.38
 ---- batch: 020 ----
mean loss: 126.46
 ---- batch: 030 ----
mean loss: 135.78
 ---- batch: 040 ----
mean loss: 119.62
 ---- batch: 050 ----
mean loss: 128.94
 ---- batch: 060 ----
mean loss: 119.65
 ---- batch: 070 ----
mean loss: 127.88
 ---- batch: 080 ----
mean loss: 130.01
 ---- batch: 090 ----
mean loss: 134.78
 ---- batch: 100 ----
mean loss: 134.23
 ---- batch: 110 ----
mean loss: 133.13
train mean loss: 128.28
epoch train time: 0:00:01.886603
elapsed time: 0:06:16.147177
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 22:43:28.092716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.47
 ---- batch: 020 ----
mean loss: 127.22
 ---- batch: 030 ----
mean loss: 124.05
 ---- batch: 040 ----
mean loss: 130.47
 ---- batch: 050 ----
mean loss: 127.99
 ---- batch: 060 ----
mean loss: 129.21
 ---- batch: 070 ----
mean loss: 127.12
 ---- batch: 080 ----
mean loss: 123.23
 ---- batch: 090 ----
mean loss: 131.86
 ---- batch: 100 ----
mean loss: 131.19
 ---- batch: 110 ----
mean loss: 127.62
train mean loss: 128.21
epoch train time: 0:00:01.930554
elapsed time: 0:06:18.078279
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 22:43:30.023821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.72
 ---- batch: 020 ----
mean loss: 128.48
 ---- batch: 030 ----
mean loss: 121.00
 ---- batch: 040 ----
mean loss: 126.27
 ---- batch: 050 ----
mean loss: 124.21
 ---- batch: 060 ----
mean loss: 121.67
 ---- batch: 070 ----
mean loss: 128.73
 ---- batch: 080 ----
mean loss: 133.00
 ---- batch: 090 ----
mean loss: 135.22
 ---- batch: 100 ----
mean loss: 128.82
 ---- batch: 110 ----
mean loss: 126.64
train mean loss: 127.33
epoch train time: 0:00:01.883116
elapsed time: 0:06:19.961961
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 22:43:31.907512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.91
 ---- batch: 020 ----
mean loss: 125.82
 ---- batch: 030 ----
mean loss: 127.91
 ---- batch: 040 ----
mean loss: 123.45
 ---- batch: 050 ----
mean loss: 129.79
 ---- batch: 060 ----
mean loss: 130.45
 ---- batch: 070 ----
mean loss: 128.53
 ---- batch: 080 ----
mean loss: 127.59
 ---- batch: 090 ----
mean loss: 118.72
 ---- batch: 100 ----
mean loss: 129.25
 ---- batch: 110 ----
mean loss: 137.45
train mean loss: 127.33
epoch train time: 0:00:01.882819
elapsed time: 0:06:21.845406
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 22:43:33.790960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.94
 ---- batch: 020 ----
mean loss: 125.38
 ---- batch: 030 ----
mean loss: 134.36
 ---- batch: 040 ----
mean loss: 123.54
 ---- batch: 050 ----
mean loss: 125.38
 ---- batch: 060 ----
mean loss: 126.11
 ---- batch: 070 ----
mean loss: 129.24
 ---- batch: 080 ----
mean loss: 119.22
 ---- batch: 090 ----
mean loss: 124.08
 ---- batch: 100 ----
mean loss: 132.00
 ---- batch: 110 ----
mean loss: 133.54
train mean loss: 126.81
epoch train time: 0:00:01.887357
elapsed time: 0:06:23.733381
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 22:43:35.678944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.98
 ---- batch: 020 ----
mean loss: 128.76
 ---- batch: 030 ----
mean loss: 130.72
 ---- batch: 040 ----
mean loss: 126.53
 ---- batch: 050 ----
mean loss: 127.01
 ---- batch: 060 ----
mean loss: 127.50
 ---- batch: 070 ----
mean loss: 126.18
 ---- batch: 080 ----
mean loss: 126.13
 ---- batch: 090 ----
mean loss: 127.26
 ---- batch: 100 ----
mean loss: 123.30
 ---- batch: 110 ----
mean loss: 127.07
train mean loss: 126.60
epoch train time: 0:00:01.908853
elapsed time: 0:06:25.642832
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 22:43:37.588370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.17
 ---- batch: 020 ----
mean loss: 122.44
 ---- batch: 030 ----
mean loss: 122.45
 ---- batch: 040 ----
mean loss: 130.44
 ---- batch: 050 ----
mean loss: 126.79
 ---- batch: 060 ----
mean loss: 135.59
 ---- batch: 070 ----
mean loss: 121.35
 ---- batch: 080 ----
mean loss: 119.53
 ---- batch: 090 ----
mean loss: 133.54
 ---- batch: 100 ----
mean loss: 128.04
 ---- batch: 110 ----
mean loss: 126.61
train mean loss: 125.95
epoch train time: 0:00:01.895385
elapsed time: 0:06:27.538839
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 22:43:39.484461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.37
 ---- batch: 020 ----
mean loss: 126.72
 ---- batch: 030 ----
mean loss: 135.08
 ---- batch: 040 ----
mean loss: 119.12
 ---- batch: 050 ----
mean loss: 126.95
 ---- batch: 060 ----
mean loss: 121.91
 ---- batch: 070 ----
mean loss: 129.30
 ---- batch: 080 ----
mean loss: 128.03
 ---- batch: 090 ----
mean loss: 130.78
 ---- batch: 100 ----
mean loss: 129.14
 ---- batch: 110 ----
mean loss: 122.37
train mean loss: 126.82
epoch train time: 0:00:01.895873
elapsed time: 0:06:29.435416
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 22:43:41.381059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.76
 ---- batch: 020 ----
mean loss: 124.79
 ---- batch: 030 ----
mean loss: 136.09
 ---- batch: 040 ----
mean loss: 124.12
 ---- batch: 050 ----
mean loss: 120.66
 ---- batch: 060 ----
mean loss: 123.19
 ---- batch: 070 ----
mean loss: 121.88
 ---- batch: 080 ----
mean loss: 129.37
 ---- batch: 090 ----
mean loss: 125.89
 ---- batch: 100 ----
mean loss: 121.90
 ---- batch: 110 ----
mean loss: 124.86
train mean loss: 125.07
epoch train time: 0:00:01.913860
elapsed time: 0:06:31.349938
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 22:43:43.295509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.60
 ---- batch: 020 ----
mean loss: 119.97
 ---- batch: 030 ----
mean loss: 126.56
 ---- batch: 040 ----
mean loss: 121.06
 ---- batch: 050 ----
mean loss: 121.03
 ---- batch: 060 ----
mean loss: 133.07
 ---- batch: 070 ----
mean loss: 132.87
 ---- batch: 080 ----
mean loss: 124.03
 ---- batch: 090 ----
mean loss: 128.84
 ---- batch: 100 ----
mean loss: 123.18
 ---- batch: 110 ----
mean loss: 133.73
train mean loss: 125.86
epoch train time: 0:00:01.933394
elapsed time: 0:06:33.283905
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 22:43:45.229444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.48
 ---- batch: 020 ----
mean loss: 117.21
 ---- batch: 030 ----
mean loss: 119.55
 ---- batch: 040 ----
mean loss: 124.03
 ---- batch: 050 ----
mean loss: 129.61
 ---- batch: 060 ----
mean loss: 130.88
 ---- batch: 070 ----
mean loss: 126.35
 ---- batch: 080 ----
mean loss: 123.85
 ---- batch: 090 ----
mean loss: 127.58
 ---- batch: 100 ----
mean loss: 125.97
 ---- batch: 110 ----
mean loss: 125.99
train mean loss: 124.83
epoch train time: 0:00:01.892282
elapsed time: 0:06:35.176740
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 22:43:47.122294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.25
 ---- batch: 020 ----
mean loss: 126.27
 ---- batch: 030 ----
mean loss: 129.86
 ---- batch: 040 ----
mean loss: 121.57
 ---- batch: 050 ----
mean loss: 126.72
 ---- batch: 060 ----
mean loss: 131.13
 ---- batch: 070 ----
mean loss: 116.54
 ---- batch: 080 ----
mean loss: 123.43
 ---- batch: 090 ----
mean loss: 123.47
 ---- batch: 100 ----
mean loss: 120.02
 ---- batch: 110 ----
mean loss: 124.83
train mean loss: 124.96
epoch train time: 0:00:01.909521
elapsed time: 0:06:37.086838
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 22:43:49.032397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.32
 ---- batch: 020 ----
mean loss: 122.62
 ---- batch: 030 ----
mean loss: 118.46
 ---- batch: 040 ----
mean loss: 123.79
 ---- batch: 050 ----
mean loss: 123.56
 ---- batch: 060 ----
mean loss: 118.72
 ---- batch: 070 ----
mean loss: 125.59
 ---- batch: 080 ----
mean loss: 119.17
 ---- batch: 090 ----
mean loss: 120.23
 ---- batch: 100 ----
mean loss: 135.93
 ---- batch: 110 ----
mean loss: 130.88
train mean loss: 124.25
epoch train time: 0:00:01.905074
elapsed time: 0:06:38.992494
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 22:43:50.938085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.27
 ---- batch: 020 ----
mean loss: 133.15
 ---- batch: 030 ----
mean loss: 127.17
 ---- batch: 040 ----
mean loss: 123.30
 ---- batch: 050 ----
mean loss: 119.31
 ---- batch: 060 ----
mean loss: 119.21
 ---- batch: 070 ----
mean loss: 127.60
 ---- batch: 080 ----
mean loss: 119.71
 ---- batch: 090 ----
mean loss: 120.51
 ---- batch: 100 ----
mean loss: 123.91
 ---- batch: 110 ----
mean loss: 124.38
train mean loss: 123.79
epoch train time: 0:00:01.906517
elapsed time: 0:06:40.899609
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 22:43:52.845129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.96
 ---- batch: 020 ----
mean loss: 126.40
 ---- batch: 030 ----
mean loss: 132.53
 ---- batch: 040 ----
mean loss: 127.93
 ---- batch: 050 ----
mean loss: 115.55
 ---- batch: 060 ----
mean loss: 121.36
 ---- batch: 070 ----
mean loss: 129.11
 ---- batch: 080 ----
mean loss: 120.08
 ---- batch: 090 ----
mean loss: 127.11
 ---- batch: 100 ----
mean loss: 122.73
 ---- batch: 110 ----
mean loss: 127.46
train mean loss: 124.50
epoch train time: 0:00:01.866809
elapsed time: 0:06:42.766986
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 22:43:54.712498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.62
 ---- batch: 020 ----
mean loss: 118.01
 ---- batch: 030 ----
mean loss: 116.27
 ---- batch: 040 ----
mean loss: 130.14
 ---- batch: 050 ----
mean loss: 120.11
 ---- batch: 060 ----
mean loss: 119.72
 ---- batch: 070 ----
mean loss: 128.69
 ---- batch: 080 ----
mean loss: 126.66
 ---- batch: 090 ----
mean loss: 124.53
 ---- batch: 100 ----
mean loss: 120.97
 ---- batch: 110 ----
mean loss: 132.14
train mean loss: 123.66
epoch train time: 0:00:01.855105
elapsed time: 0:06:44.622606
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 22:43:56.568159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.94
 ---- batch: 020 ----
mean loss: 119.21
 ---- batch: 030 ----
mean loss: 120.39
 ---- batch: 040 ----
mean loss: 119.20
 ---- batch: 050 ----
mean loss: 127.10
 ---- batch: 060 ----
mean loss: 116.35
 ---- batch: 070 ----
mean loss: 132.26
 ---- batch: 080 ----
mean loss: 127.51
 ---- batch: 090 ----
mean loss: 125.08
 ---- batch: 100 ----
mean loss: 122.69
 ---- batch: 110 ----
mean loss: 122.49
train mean loss: 122.91
epoch train time: 0:00:01.871296
elapsed time: 0:06:46.494490
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 22:43:58.440016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.28
 ---- batch: 020 ----
mean loss: 119.18
 ---- batch: 030 ----
mean loss: 122.68
 ---- batch: 040 ----
mean loss: 121.75
 ---- batch: 050 ----
mean loss: 118.68
 ---- batch: 060 ----
mean loss: 127.91
 ---- batch: 070 ----
mean loss: 120.82
 ---- batch: 080 ----
mean loss: 123.85
 ---- batch: 090 ----
mean loss: 120.26
 ---- batch: 100 ----
mean loss: 123.61
 ---- batch: 110 ----
mean loss: 129.13
train mean loss: 122.80
epoch train time: 0:00:01.884116
elapsed time: 0:06:48.379139
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 22:44:00.324695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.33
 ---- batch: 020 ----
mean loss: 117.67
 ---- batch: 030 ----
mean loss: 126.50
 ---- batch: 040 ----
mean loss: 117.26
 ---- batch: 050 ----
mean loss: 122.12
 ---- batch: 060 ----
mean loss: 131.93
 ---- batch: 070 ----
mean loss: 124.80
 ---- batch: 080 ----
mean loss: 122.37
 ---- batch: 090 ----
mean loss: 117.73
 ---- batch: 100 ----
mean loss: 120.28
 ---- batch: 110 ----
mean loss: 128.17
train mean loss: 122.79
epoch train time: 0:00:01.900642
elapsed time: 0:06:50.280380
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 22:44:02.225994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.52
 ---- batch: 020 ----
mean loss: 114.85
 ---- batch: 030 ----
mean loss: 114.76
 ---- batch: 040 ----
mean loss: 118.84
 ---- batch: 050 ----
mean loss: 125.79
 ---- batch: 060 ----
mean loss: 126.42
 ---- batch: 070 ----
mean loss: 121.97
 ---- batch: 080 ----
mean loss: 120.65
 ---- batch: 090 ----
mean loss: 128.93
 ---- batch: 100 ----
mean loss: 125.24
 ---- batch: 110 ----
mean loss: 132.55
train mean loss: 122.98
epoch train time: 0:00:01.880343
elapsed time: 0:06:52.161428
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 22:44:04.107065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.67
 ---- batch: 020 ----
mean loss: 117.10
 ---- batch: 030 ----
mean loss: 122.10
 ---- batch: 040 ----
mean loss: 118.63
 ---- batch: 050 ----
mean loss: 113.67
 ---- batch: 060 ----
mean loss: 118.07
 ---- batch: 070 ----
mean loss: 111.05
 ---- batch: 080 ----
mean loss: 115.45
 ---- batch: 090 ----
mean loss: 120.97
 ---- batch: 100 ----
mean loss: 119.89
 ---- batch: 110 ----
mean loss: 115.90
train mean loss: 117.35
epoch train time: 0:00:01.899176
elapsed time: 0:06:54.061605
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 22:44:06.006839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.05
 ---- batch: 020 ----
mean loss: 116.82
 ---- batch: 030 ----
mean loss: 110.02
 ---- batch: 040 ----
mean loss: 117.69
 ---- batch: 050 ----
mean loss: 116.83
 ---- batch: 060 ----
mean loss: 118.59
 ---- batch: 070 ----
mean loss: 112.96
 ---- batch: 080 ----
mean loss: 121.14
 ---- batch: 090 ----
mean loss: 119.21
 ---- batch: 100 ----
mean loss: 118.53
 ---- batch: 110 ----
mean loss: 113.80
train mean loss: 116.83
epoch train time: 0:00:01.879071
elapsed time: 0:06:55.940962
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 22:44:07.886521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.74
 ---- batch: 020 ----
mean loss: 115.94
 ---- batch: 030 ----
mean loss: 119.71
 ---- batch: 040 ----
mean loss: 115.91
 ---- batch: 050 ----
mean loss: 117.43
 ---- batch: 060 ----
mean loss: 117.94
 ---- batch: 070 ----
mean loss: 114.20
 ---- batch: 080 ----
mean loss: 123.89
 ---- batch: 090 ----
mean loss: 114.18
 ---- batch: 100 ----
mean loss: 119.16
 ---- batch: 110 ----
mean loss: 113.30
train mean loss: 116.67
epoch train time: 0:00:01.880318
elapsed time: 0:06:57.821846
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 22:44:09.767148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.27
 ---- batch: 020 ----
mean loss: 115.20
 ---- batch: 030 ----
mean loss: 112.30
 ---- batch: 040 ----
mean loss: 110.81
 ---- batch: 050 ----
mean loss: 122.89
 ---- batch: 060 ----
mean loss: 115.45
 ---- batch: 070 ----
mean loss: 126.40
 ---- batch: 080 ----
mean loss: 118.72
 ---- batch: 090 ----
mean loss: 119.13
 ---- batch: 100 ----
mean loss: 110.63
 ---- batch: 110 ----
mean loss: 112.73
train mean loss: 116.48
epoch train time: 0:00:01.901489
elapsed time: 0:06:59.723654
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 22:44:11.669207
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.71
 ---- batch: 020 ----
mean loss: 113.30
 ---- batch: 030 ----
mean loss: 115.45
 ---- batch: 040 ----
mean loss: 115.83
 ---- batch: 050 ----
mean loss: 114.20
 ---- batch: 060 ----
mean loss: 120.49
 ---- batch: 070 ----
mean loss: 116.89
 ---- batch: 080 ----
mean loss: 123.17
 ---- batch: 090 ----
mean loss: 107.30
 ---- batch: 100 ----
mean loss: 115.71
 ---- batch: 110 ----
mean loss: 121.79
train mean loss: 116.51
epoch train time: 0:00:01.908128
elapsed time: 0:07:01.632350
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 22:44:13.577945
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.60
 ---- batch: 020 ----
mean loss: 113.90
 ---- batch: 030 ----
mean loss: 115.22
 ---- batch: 040 ----
mean loss: 119.47
 ---- batch: 050 ----
mean loss: 109.49
 ---- batch: 060 ----
mean loss: 118.48
 ---- batch: 070 ----
mean loss: 119.13
 ---- batch: 080 ----
mean loss: 118.25
 ---- batch: 090 ----
mean loss: 119.31
 ---- batch: 100 ----
mean loss: 111.08
 ---- batch: 110 ----
mean loss: 118.69
train mean loss: 116.49
epoch train time: 0:00:01.875720
elapsed time: 0:07:03.508723
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 22:44:15.454316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.46
 ---- batch: 020 ----
mean loss: 118.13
 ---- batch: 030 ----
mean loss: 118.40
 ---- batch: 040 ----
mean loss: 124.97
 ---- batch: 050 ----
mean loss: 116.47
 ---- batch: 060 ----
mean loss: 116.89
 ---- batch: 070 ----
mean loss: 108.23
 ---- batch: 080 ----
mean loss: 117.59
 ---- batch: 090 ----
mean loss: 115.40
 ---- batch: 100 ----
mean loss: 119.07
 ---- batch: 110 ----
mean loss: 114.91
train mean loss: 116.35
epoch train time: 0:00:01.891219
elapsed time: 0:07:05.400580
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 22:44:17.346260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.27
 ---- batch: 020 ----
mean loss: 112.44
 ---- batch: 030 ----
mean loss: 114.39
 ---- batch: 040 ----
mean loss: 113.56
 ---- batch: 050 ----
mean loss: 116.41
 ---- batch: 060 ----
mean loss: 122.39
 ---- batch: 070 ----
mean loss: 120.73
 ---- batch: 080 ----
mean loss: 119.16
 ---- batch: 090 ----
mean loss: 117.85
 ---- batch: 100 ----
mean loss: 113.01
 ---- batch: 110 ----
mean loss: 118.54
train mean loss: 116.26
epoch train time: 0:00:01.879611
elapsed time: 0:07:07.280883
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 22:44:19.226421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.66
 ---- batch: 020 ----
mean loss: 106.26
 ---- batch: 030 ----
mean loss: 123.45
 ---- batch: 040 ----
mean loss: 114.69
 ---- batch: 050 ----
mean loss: 111.43
 ---- batch: 060 ----
mean loss: 119.03
 ---- batch: 070 ----
mean loss: 120.71
 ---- batch: 080 ----
mean loss: 110.34
 ---- batch: 090 ----
mean loss: 117.63
 ---- batch: 100 ----
mean loss: 113.82
 ---- batch: 110 ----
mean loss: 127.97
train mean loss: 116.25
epoch train time: 0:00:01.887232
elapsed time: 0:07:09.168678
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 22:44:21.114255
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.84
 ---- batch: 020 ----
mean loss: 114.15
 ---- batch: 030 ----
mean loss: 113.52
 ---- batch: 040 ----
mean loss: 121.78
 ---- batch: 050 ----
mean loss: 118.38
 ---- batch: 060 ----
mean loss: 117.45
 ---- batch: 070 ----
mean loss: 112.28
 ---- batch: 080 ----
mean loss: 113.08
 ---- batch: 090 ----
mean loss: 113.08
 ---- batch: 100 ----
mean loss: 114.64
 ---- batch: 110 ----
mean loss: 114.32
train mean loss: 116.21
epoch train time: 0:00:01.886536
elapsed time: 0:07:11.055817
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 22:44:23.001373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.95
 ---- batch: 020 ----
mean loss: 119.76
 ---- batch: 030 ----
mean loss: 117.11
 ---- batch: 040 ----
mean loss: 117.30
 ---- batch: 050 ----
mean loss: 120.26
 ---- batch: 060 ----
mean loss: 112.39
 ---- batch: 070 ----
mean loss: 108.39
 ---- batch: 080 ----
mean loss: 113.71
 ---- batch: 090 ----
mean loss: 120.76
 ---- batch: 100 ----
mean loss: 110.07
 ---- batch: 110 ----
mean loss: 120.69
train mean loss: 116.24
epoch train time: 0:00:01.876560
elapsed time: 0:07:12.933024
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 22:44:24.878585
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.63
 ---- batch: 020 ----
mean loss: 111.31
 ---- batch: 030 ----
mean loss: 114.95
 ---- batch: 040 ----
mean loss: 122.84
 ---- batch: 050 ----
mean loss: 116.82
 ---- batch: 060 ----
mean loss: 116.23
 ---- batch: 070 ----
mean loss: 117.40
 ---- batch: 080 ----
mean loss: 114.09
 ---- batch: 090 ----
mean loss: 118.02
 ---- batch: 100 ----
mean loss: 118.15
 ---- batch: 110 ----
mean loss: 114.55
train mean loss: 116.08
epoch train time: 0:00:01.884829
elapsed time: 0:07:14.818425
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 22:44:26.763994
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.81
 ---- batch: 020 ----
mean loss: 113.60
 ---- batch: 030 ----
mean loss: 107.58
 ---- batch: 040 ----
mean loss: 124.05
 ---- batch: 050 ----
mean loss: 119.84
 ---- batch: 060 ----
mean loss: 120.62
 ---- batch: 070 ----
mean loss: 113.07
 ---- batch: 080 ----
mean loss: 112.43
 ---- batch: 090 ----
mean loss: 107.89
 ---- batch: 100 ----
mean loss: 117.37
 ---- batch: 110 ----
mean loss: 121.62
train mean loss: 116.10
epoch train time: 0:00:01.892104
elapsed time: 0:07:16.711168
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 22:44:28.656705
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.88
 ---- batch: 020 ----
mean loss: 112.13
 ---- batch: 030 ----
mean loss: 113.98
 ---- batch: 040 ----
mean loss: 113.26
 ---- batch: 050 ----
mean loss: 119.66
 ---- batch: 060 ----
mean loss: 110.01
 ---- batch: 070 ----
mean loss: 122.30
 ---- batch: 080 ----
mean loss: 117.44
 ---- batch: 090 ----
mean loss: 118.20
 ---- batch: 100 ----
mean loss: 116.91
 ---- batch: 110 ----
mean loss: 123.58
train mean loss: 116.16
epoch train time: 0:00:01.887790
elapsed time: 0:07:18.599495
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 22:44:30.545091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.66
 ---- batch: 020 ----
mean loss: 113.43
 ---- batch: 030 ----
mean loss: 117.83
 ---- batch: 040 ----
mean loss: 119.41
 ---- batch: 050 ----
mean loss: 114.66
 ---- batch: 060 ----
mean loss: 112.17
 ---- batch: 070 ----
mean loss: 120.52
 ---- batch: 080 ----
mean loss: 114.10
 ---- batch: 090 ----
mean loss: 107.23
 ---- batch: 100 ----
mean loss: 115.32
 ---- batch: 110 ----
mean loss: 118.59
train mean loss: 116.10
epoch train time: 0:00:01.859980
elapsed time: 0:07:20.460091
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 22:44:32.405652
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.20
 ---- batch: 020 ----
mean loss: 119.25
 ---- batch: 030 ----
mean loss: 117.08
 ---- batch: 040 ----
mean loss: 116.25
 ---- batch: 050 ----
mean loss: 111.51
 ---- batch: 060 ----
mean loss: 115.03
 ---- batch: 070 ----
mean loss: 115.16
 ---- batch: 080 ----
mean loss: 118.54
 ---- batch: 090 ----
mean loss: 116.33
 ---- batch: 100 ----
mean loss: 115.46
 ---- batch: 110 ----
mean loss: 111.89
train mean loss: 115.97
epoch train time: 0:00:01.903937
elapsed time: 0:07:22.364617
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 22:44:34.310183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.08
 ---- batch: 020 ----
mean loss: 118.03
 ---- batch: 030 ----
mean loss: 120.88
 ---- batch: 040 ----
mean loss: 111.96
 ---- batch: 050 ----
mean loss: 117.50
 ---- batch: 060 ----
mean loss: 118.14
 ---- batch: 070 ----
mean loss: 113.60
 ---- batch: 080 ----
mean loss: 116.66
 ---- batch: 090 ----
mean loss: 116.51
 ---- batch: 100 ----
mean loss: 119.60
 ---- batch: 110 ----
mean loss: 111.56
train mean loss: 116.01
epoch train time: 0:00:01.877756
elapsed time: 0:07:24.242984
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 22:44:36.188542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.85
 ---- batch: 020 ----
mean loss: 117.23
 ---- batch: 030 ----
mean loss: 117.18
 ---- batch: 040 ----
mean loss: 115.31
 ---- batch: 050 ----
mean loss: 113.30
 ---- batch: 060 ----
mean loss: 115.20
 ---- batch: 070 ----
mean loss: 112.86
 ---- batch: 080 ----
mean loss: 115.83
 ---- batch: 090 ----
mean loss: 117.67
 ---- batch: 100 ----
mean loss: 117.87
 ---- batch: 110 ----
mean loss: 113.58
train mean loss: 115.94
epoch train time: 0:00:01.903928
elapsed time: 0:07:26.147492
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 22:44:38.093121
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.70
 ---- batch: 020 ----
mean loss: 115.69
 ---- batch: 030 ----
mean loss: 112.92
 ---- batch: 040 ----
mean loss: 112.75
 ---- batch: 050 ----
mean loss: 115.62
 ---- batch: 060 ----
mean loss: 108.43
 ---- batch: 070 ----
mean loss: 113.75
 ---- batch: 080 ----
mean loss: 118.97
 ---- batch: 090 ----
mean loss: 120.52
 ---- batch: 100 ----
mean loss: 117.96
 ---- batch: 110 ----
mean loss: 118.64
train mean loss: 116.03
epoch train time: 0:00:01.914603
elapsed time: 0:07:28.062748
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 22:44:40.008306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.43
 ---- batch: 020 ----
mean loss: 113.56
 ---- batch: 030 ----
mean loss: 119.56
 ---- batch: 040 ----
mean loss: 117.81
 ---- batch: 050 ----
mean loss: 119.96
 ---- batch: 060 ----
mean loss: 111.69
 ---- batch: 070 ----
mean loss: 113.12
 ---- batch: 080 ----
mean loss: 115.11
 ---- batch: 090 ----
mean loss: 115.40
 ---- batch: 100 ----
mean loss: 121.15
 ---- batch: 110 ----
mean loss: 116.69
train mean loss: 115.90
epoch train time: 0:00:01.865250
elapsed time: 0:07:29.928606
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 22:44:41.874156
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.11
 ---- batch: 020 ----
mean loss: 109.88
 ---- batch: 030 ----
mean loss: 120.33
 ---- batch: 040 ----
mean loss: 115.25
 ---- batch: 050 ----
mean loss: 113.49
 ---- batch: 060 ----
mean loss: 115.58
 ---- batch: 070 ----
mean loss: 123.12
 ---- batch: 080 ----
mean loss: 123.30
 ---- batch: 090 ----
mean loss: 111.45
 ---- batch: 100 ----
mean loss: 112.25
 ---- batch: 110 ----
mean loss: 116.91
train mean loss: 115.73
epoch train time: 0:00:01.895724
elapsed time: 0:07:31.824912
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 22:44:43.770489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.32
 ---- batch: 020 ----
mean loss: 114.32
 ---- batch: 030 ----
mean loss: 121.44
 ---- batch: 040 ----
mean loss: 111.68
 ---- batch: 050 ----
mean loss: 116.64
 ---- batch: 060 ----
mean loss: 118.01
 ---- batch: 070 ----
mean loss: 113.65
 ---- batch: 080 ----
mean loss: 111.99
 ---- batch: 090 ----
mean loss: 110.95
 ---- batch: 100 ----
mean loss: 115.44
 ---- batch: 110 ----
mean loss: 114.46
train mean loss: 115.82
epoch train time: 0:00:01.909175
elapsed time: 0:07:33.734683
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 22:44:45.680251
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.82
 ---- batch: 020 ----
mean loss: 119.41
 ---- batch: 030 ----
mean loss: 113.49
 ---- batch: 040 ----
mean loss: 112.87
 ---- batch: 050 ----
mean loss: 118.78
 ---- batch: 060 ----
mean loss: 115.21
 ---- batch: 070 ----
mean loss: 116.77
 ---- batch: 080 ----
mean loss: 116.52
 ---- batch: 090 ----
mean loss: 119.89
 ---- batch: 100 ----
mean loss: 114.70
 ---- batch: 110 ----
mean loss: 110.72
train mean loss: 115.83
epoch train time: 0:00:01.902694
elapsed time: 0:07:35.637966
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 22:44:47.583555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.74
 ---- batch: 020 ----
mean loss: 117.35
 ---- batch: 030 ----
mean loss: 113.99
 ---- batch: 040 ----
mean loss: 115.53
 ---- batch: 050 ----
mean loss: 119.95
 ---- batch: 060 ----
mean loss: 115.67
 ---- batch: 070 ----
mean loss: 117.46
 ---- batch: 080 ----
mean loss: 120.41
 ---- batch: 090 ----
mean loss: 113.60
 ---- batch: 100 ----
mean loss: 106.84
 ---- batch: 110 ----
mean loss: 117.28
train mean loss: 115.61
epoch train time: 0:00:01.877327
elapsed time: 0:07:37.515920
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 22:44:49.461449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.98
 ---- batch: 020 ----
mean loss: 108.96
 ---- batch: 030 ----
mean loss: 122.31
 ---- batch: 040 ----
mean loss: 120.62
 ---- batch: 050 ----
mean loss: 111.32
 ---- batch: 060 ----
mean loss: 117.09
 ---- batch: 070 ----
mean loss: 109.99
 ---- batch: 080 ----
mean loss: 107.76
 ---- batch: 090 ----
mean loss: 117.10
 ---- batch: 100 ----
mean loss: 122.18
 ---- batch: 110 ----
mean loss: 118.30
train mean loss: 115.70
epoch train time: 0:00:01.899915
elapsed time: 0:07:39.416517
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 22:44:51.362119
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.06
 ---- batch: 020 ----
mean loss: 123.33
 ---- batch: 030 ----
mean loss: 116.40
 ---- batch: 040 ----
mean loss: 113.54
 ---- batch: 050 ----
mean loss: 113.42
 ---- batch: 060 ----
mean loss: 109.40
 ---- batch: 070 ----
mean loss: 126.09
 ---- batch: 080 ----
mean loss: 113.14
 ---- batch: 090 ----
mean loss: 118.35
 ---- batch: 100 ----
mean loss: 114.33
 ---- batch: 110 ----
mean loss: 116.27
train mean loss: 115.63
epoch train time: 0:00:01.865919
elapsed time: 0:07:41.283045
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 22:44:53.228607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.36
 ---- batch: 020 ----
mean loss: 115.45
 ---- batch: 030 ----
mean loss: 114.64
 ---- batch: 040 ----
mean loss: 111.32
 ---- batch: 050 ----
mean loss: 113.57
 ---- batch: 060 ----
mean loss: 119.14
 ---- batch: 070 ----
mean loss: 117.03
 ---- batch: 080 ----
mean loss: 124.41
 ---- batch: 090 ----
mean loss: 116.01
 ---- batch: 100 ----
mean loss: 116.99
 ---- batch: 110 ----
mean loss: 116.77
train mean loss: 115.60
epoch train time: 0:00:01.917202
elapsed time: 0:07:43.200883
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 22:44:55.146417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.03
 ---- batch: 020 ----
mean loss: 115.68
 ---- batch: 030 ----
mean loss: 113.29
 ---- batch: 040 ----
mean loss: 115.45
 ---- batch: 050 ----
mean loss: 110.89
 ---- batch: 060 ----
mean loss: 113.46
 ---- batch: 070 ----
mean loss: 123.33
 ---- batch: 080 ----
mean loss: 119.79
 ---- batch: 090 ----
mean loss: 114.16
 ---- batch: 100 ----
mean loss: 113.61
 ---- batch: 110 ----
mean loss: 114.68
train mean loss: 115.57
epoch train time: 0:00:01.866917
elapsed time: 0:07:45.068328
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 22:44:57.013871
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.12
 ---- batch: 020 ----
mean loss: 116.81
 ---- batch: 030 ----
mean loss: 122.17
 ---- batch: 040 ----
mean loss: 119.83
 ---- batch: 050 ----
mean loss: 118.69
 ---- batch: 060 ----
mean loss: 116.01
 ---- batch: 070 ----
mean loss: 112.01
 ---- batch: 080 ----
mean loss: 105.51
 ---- batch: 090 ----
mean loss: 118.75
 ---- batch: 100 ----
mean loss: 117.31
 ---- batch: 110 ----
mean loss: 114.75
train mean loss: 115.60
epoch train time: 0:00:01.902144
elapsed time: 0:07:46.971047
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 22:44:58.916663
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.65
 ---- batch: 020 ----
mean loss: 111.50
 ---- batch: 030 ----
mean loss: 109.53
 ---- batch: 040 ----
mean loss: 124.86
 ---- batch: 050 ----
mean loss: 112.35
 ---- batch: 060 ----
mean loss: 114.50
 ---- batch: 070 ----
mean loss: 115.40
 ---- batch: 080 ----
mean loss: 117.58
 ---- batch: 090 ----
mean loss: 114.61
 ---- batch: 100 ----
mean loss: 113.13
 ---- batch: 110 ----
mean loss: 116.99
train mean loss: 115.58
epoch train time: 0:00:01.917564
elapsed time: 0:07:48.889270
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 22:45:00.834823
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.74
 ---- batch: 020 ----
mean loss: 108.70
 ---- batch: 030 ----
mean loss: 113.07
 ---- batch: 040 ----
mean loss: 120.54
 ---- batch: 050 ----
mean loss: 116.94
 ---- batch: 060 ----
mean loss: 107.14
 ---- batch: 070 ----
mean loss: 111.58
 ---- batch: 080 ----
mean loss: 123.93
 ---- batch: 090 ----
mean loss: 118.35
 ---- batch: 100 ----
mean loss: 124.35
 ---- batch: 110 ----
mean loss: 118.32
train mean loss: 115.61
epoch train time: 0:00:01.937362
elapsed time: 0:07:50.827226
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 22:45:02.772813
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.44
 ---- batch: 020 ----
mean loss: 114.86
 ---- batch: 030 ----
mean loss: 119.59
 ---- batch: 040 ----
mean loss: 116.06
 ---- batch: 050 ----
mean loss: 105.60
 ---- batch: 060 ----
mean loss: 120.16
 ---- batch: 070 ----
mean loss: 118.13
 ---- batch: 080 ----
mean loss: 118.39
 ---- batch: 090 ----
mean loss: 111.97
 ---- batch: 100 ----
mean loss: 110.87
 ---- batch: 110 ----
mean loss: 115.27
train mean loss: 115.37
epoch train time: 0:00:01.901603
elapsed time: 0:07:52.729441
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 22:45:04.674975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.13
 ---- batch: 020 ----
mean loss: 105.78
 ---- batch: 030 ----
mean loss: 112.52
 ---- batch: 040 ----
mean loss: 116.08
 ---- batch: 050 ----
mean loss: 111.74
 ---- batch: 060 ----
mean loss: 122.21
 ---- batch: 070 ----
mean loss: 114.74
 ---- batch: 080 ----
mean loss: 117.54
 ---- batch: 090 ----
mean loss: 116.82
 ---- batch: 100 ----
mean loss: 120.13
 ---- batch: 110 ----
mean loss: 115.12
train mean loss: 115.38
epoch train time: 0:00:01.914886
elapsed time: 0:07:54.645134
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 22:45:06.590489
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.61
 ---- batch: 020 ----
mean loss: 117.29
 ---- batch: 030 ----
mean loss: 109.77
 ---- batch: 040 ----
mean loss: 118.87
 ---- batch: 050 ----
mean loss: 110.39
 ---- batch: 060 ----
mean loss: 121.11
 ---- batch: 070 ----
mean loss: 112.53
 ---- batch: 080 ----
mean loss: 116.46
 ---- batch: 090 ----
mean loss: 117.35
 ---- batch: 100 ----
mean loss: 118.62
 ---- batch: 110 ----
mean loss: 115.48
train mean loss: 115.25
epoch train time: 0:00:01.917891
elapsed time: 0:07:56.563410
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 22:45:08.508956
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.35
 ---- batch: 020 ----
mean loss: 109.71
 ---- batch: 030 ----
mean loss: 112.28
 ---- batch: 040 ----
mean loss: 123.80
 ---- batch: 050 ----
mean loss: 110.10
 ---- batch: 060 ----
mean loss: 119.54
 ---- batch: 070 ----
mean loss: 119.01
 ---- batch: 080 ----
mean loss: 113.20
 ---- batch: 090 ----
mean loss: 118.50
 ---- batch: 100 ----
mean loss: 114.96
 ---- batch: 110 ----
mean loss: 109.16
train mean loss: 115.31
epoch train time: 0:00:01.871551
elapsed time: 0:07:58.435516
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 22:45:10.381043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.46
 ---- batch: 020 ----
mean loss: 113.42
 ---- batch: 030 ----
mean loss: 117.01
 ---- batch: 040 ----
mean loss: 122.54
 ---- batch: 050 ----
mean loss: 114.72
 ---- batch: 060 ----
mean loss: 127.47
 ---- batch: 070 ----
mean loss: 116.77
 ---- batch: 080 ----
mean loss: 115.75
 ---- batch: 090 ----
mean loss: 108.14
 ---- batch: 100 ----
mean loss: 112.86
 ---- batch: 110 ----
mean loss: 107.55
train mean loss: 115.27
epoch train time: 0:00:01.886404
elapsed time: 0:08:00.322458
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 22:45:12.267997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.02
 ---- batch: 020 ----
mean loss: 121.35
 ---- batch: 030 ----
mean loss: 114.94
 ---- batch: 040 ----
mean loss: 114.77
 ---- batch: 050 ----
mean loss: 114.23
 ---- batch: 060 ----
mean loss: 116.48
 ---- batch: 070 ----
mean loss: 105.73
 ---- batch: 080 ----
mean loss: 114.17
 ---- batch: 090 ----
mean loss: 122.22
 ---- batch: 100 ----
mean loss: 111.64
 ---- batch: 110 ----
mean loss: 113.67
train mean loss: 115.28
epoch train time: 0:00:01.917446
elapsed time: 0:08:02.240460
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 22:45:14.186026
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.50
 ---- batch: 020 ----
mean loss: 115.78
 ---- batch: 030 ----
mean loss: 114.23
 ---- batch: 040 ----
mean loss: 116.53
 ---- batch: 050 ----
mean loss: 114.95
 ---- batch: 060 ----
mean loss: 117.21
 ---- batch: 070 ----
mean loss: 122.39
 ---- batch: 080 ----
mean loss: 107.28
 ---- batch: 090 ----
mean loss: 108.67
 ---- batch: 100 ----
mean loss: 119.79
 ---- batch: 110 ----
mean loss: 113.91
train mean loss: 115.20
epoch train time: 0:00:01.926251
elapsed time: 0:08:04.167291
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 22:45:16.112869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.58
 ---- batch: 020 ----
mean loss: 119.88
 ---- batch: 030 ----
mean loss: 119.43
 ---- batch: 040 ----
mean loss: 114.79
 ---- batch: 050 ----
mean loss: 112.76
 ---- batch: 060 ----
mean loss: 123.72
 ---- batch: 070 ----
mean loss: 120.43
 ---- batch: 080 ----
mean loss: 118.34
 ---- batch: 090 ----
mean loss: 112.90
 ---- batch: 100 ----
mean loss: 111.12
 ---- batch: 110 ----
mean loss: 109.41
train mean loss: 115.26
epoch train time: 0:00:01.871487
elapsed time: 0:08:06.039367
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 22:45:17.984951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.22
 ---- batch: 020 ----
mean loss: 111.16
 ---- batch: 030 ----
mean loss: 119.36
 ---- batch: 040 ----
mean loss: 112.25
 ---- batch: 050 ----
mean loss: 109.14
 ---- batch: 060 ----
mean loss: 114.31
 ---- batch: 070 ----
mean loss: 116.68
 ---- batch: 080 ----
mean loss: 113.73
 ---- batch: 090 ----
mean loss: 127.88
 ---- batch: 100 ----
mean loss: 109.72
 ---- batch: 110 ----
mean loss: 121.12
train mean loss: 115.24
epoch train time: 0:00:01.918375
elapsed time: 0:08:07.958358
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 22:45:19.903891
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.08
 ---- batch: 020 ----
mean loss: 111.99
 ---- batch: 030 ----
mean loss: 113.46
 ---- batch: 040 ----
mean loss: 111.22
 ---- batch: 050 ----
mean loss: 119.51
 ---- batch: 060 ----
mean loss: 119.24
 ---- batch: 070 ----
mean loss: 118.68
 ---- batch: 080 ----
mean loss: 113.20
 ---- batch: 090 ----
mean loss: 116.70
 ---- batch: 100 ----
mean loss: 121.19
 ---- batch: 110 ----
mean loss: 110.26
train mean loss: 115.03
epoch train time: 0:00:01.859181
elapsed time: 0:08:09.818081
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 22:45:21.763692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.97
 ---- batch: 020 ----
mean loss: 120.55
 ---- batch: 030 ----
mean loss: 120.61
 ---- batch: 040 ----
mean loss: 106.64
 ---- batch: 050 ----
mean loss: 114.35
 ---- batch: 060 ----
mean loss: 117.35
 ---- batch: 070 ----
mean loss: 114.52
 ---- batch: 080 ----
mean loss: 105.97
 ---- batch: 090 ----
mean loss: 108.93
 ---- batch: 100 ----
mean loss: 123.69
 ---- batch: 110 ----
mean loss: 120.58
train mean loss: 115.17
epoch train time: 0:00:01.891054
elapsed time: 0:08:11.709781
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 22:45:23.655382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.88
 ---- batch: 020 ----
mean loss: 119.06
 ---- batch: 030 ----
mean loss: 111.40
 ---- batch: 040 ----
mean loss: 124.21
 ---- batch: 050 ----
mean loss: 115.57
 ---- batch: 060 ----
mean loss: 111.29
 ---- batch: 070 ----
mean loss: 121.64
 ---- batch: 080 ----
mean loss: 110.32
 ---- batch: 090 ----
mean loss: 110.27
 ---- batch: 100 ----
mean loss: 119.02
 ---- batch: 110 ----
mean loss: 114.66
train mean loss: 115.08
epoch train time: 0:00:01.893124
elapsed time: 0:08:13.603514
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 22:45:25.549050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.55
 ---- batch: 020 ----
mean loss: 110.52
 ---- batch: 030 ----
mean loss: 109.61
 ---- batch: 040 ----
mean loss: 114.11
 ---- batch: 050 ----
mean loss: 119.96
 ---- batch: 060 ----
mean loss: 115.51
 ---- batch: 070 ----
mean loss: 119.71
 ---- batch: 080 ----
mean loss: 121.50
 ---- batch: 090 ----
mean loss: 108.01
 ---- batch: 100 ----
mean loss: 122.53
 ---- batch: 110 ----
mean loss: 113.97
train mean loss: 115.08
epoch train time: 0:00:01.908207
elapsed time: 0:08:15.512268
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 22:45:27.457884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.30
 ---- batch: 020 ----
mean loss: 116.91
 ---- batch: 030 ----
mean loss: 118.89
 ---- batch: 040 ----
mean loss: 117.41
 ---- batch: 050 ----
mean loss: 109.36
 ---- batch: 060 ----
mean loss: 115.47
 ---- batch: 070 ----
mean loss: 114.99
 ---- batch: 080 ----
mean loss: 112.90
 ---- batch: 090 ----
mean loss: 111.34
 ---- batch: 100 ----
mean loss: 117.48
 ---- batch: 110 ----
mean loss: 119.60
train mean loss: 115.00
epoch train time: 0:00:01.889766
elapsed time: 0:08:17.402725
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 22:45:29.348277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.81
 ---- batch: 020 ----
mean loss: 110.93
 ---- batch: 030 ----
mean loss: 109.33
 ---- batch: 040 ----
mean loss: 110.83
 ---- batch: 050 ----
mean loss: 115.45
 ---- batch: 060 ----
mean loss: 116.21
 ---- batch: 070 ----
mean loss: 120.48
 ---- batch: 080 ----
mean loss: 114.12
 ---- batch: 090 ----
mean loss: 115.80
 ---- batch: 100 ----
mean loss: 120.92
 ---- batch: 110 ----
mean loss: 111.29
train mean loss: 114.93
epoch train time: 0:00:01.886040
elapsed time: 0:08:19.289323
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 22:45:31.234848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.73
 ---- batch: 020 ----
mean loss: 118.97
 ---- batch: 030 ----
mean loss: 114.12
 ---- batch: 040 ----
mean loss: 110.16
 ---- batch: 050 ----
mean loss: 113.90
 ---- batch: 060 ----
mean loss: 120.51
 ---- batch: 070 ----
mean loss: 117.51
 ---- batch: 080 ----
mean loss: 106.27
 ---- batch: 090 ----
mean loss: 118.09
 ---- batch: 100 ----
mean loss: 116.97
 ---- batch: 110 ----
mean loss: 122.82
train mean loss: 114.88
epoch train time: 0:00:01.876459
elapsed time: 0:08:21.166336
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 22:45:33.111870
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.29
 ---- batch: 020 ----
mean loss: 113.86
 ---- batch: 030 ----
mean loss: 112.03
 ---- batch: 040 ----
mean loss: 113.33
 ---- batch: 050 ----
mean loss: 115.87
 ---- batch: 060 ----
mean loss: 115.47
 ---- batch: 070 ----
mean loss: 116.23
 ---- batch: 080 ----
mean loss: 112.18
 ---- batch: 090 ----
mean loss: 117.62
 ---- batch: 100 ----
mean loss: 120.41
 ---- batch: 110 ----
mean loss: 113.12
train mean loss: 114.88
epoch train time: 0:00:01.884033
elapsed time: 0:08:23.050958
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 22:45:34.996506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.13
 ---- batch: 020 ----
mean loss: 113.83
 ---- batch: 030 ----
mean loss: 110.49
 ---- batch: 040 ----
mean loss: 111.71
 ---- batch: 050 ----
mean loss: 112.97
 ---- batch: 060 ----
mean loss: 121.98
 ---- batch: 070 ----
mean loss: 113.14
 ---- batch: 080 ----
mean loss: 117.60
 ---- batch: 090 ----
mean loss: 118.62
 ---- batch: 100 ----
mean loss: 119.35
 ---- batch: 110 ----
mean loss: 109.79
train mean loss: 114.75
epoch train time: 0:00:01.860971
elapsed time: 0:08:24.920250
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_2/checkpoint.pth.tar
**** end time: 2019-09-26 22:45:36.865443 ****
