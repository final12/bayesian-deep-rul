Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_3', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5836
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-09-20 21:43:45.081753 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:43:45.090669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4587.13
 ---- batch: 020 ----
mean loss: 4295.66
 ---- batch: 030 ----
mean loss: 4064.23
 ---- batch: 040 ----
mean loss: 3758.45
train mean loss: 4135.12
epoch train time: 0:00:14.730927
elapsed time: 0:00:14.746392
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:43:59.828191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3528.78
 ---- batch: 020 ----
mean loss: 3314.49
 ---- batch: 030 ----
mean loss: 3172.04
 ---- batch: 040 ----
mean loss: 3129.73
train mean loss: 3267.54
epoch train time: 0:00:00.677213
elapsed time: 0:00:15.423785
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:44:00.505645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2938.17
 ---- batch: 020 ----
mean loss: 2869.66
 ---- batch: 030 ----
mean loss: 2855.36
 ---- batch: 040 ----
mean loss: 2748.97
train mean loss: 2838.50
epoch train time: 0:00:00.648767
elapsed time: 0:00:16.072851
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:44:01.154726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2647.60
 ---- batch: 020 ----
mean loss: 2673.01
 ---- batch: 030 ----
mean loss: 2477.34
 ---- batch: 040 ----
mean loss: 2533.60
train mean loss: 2583.37
epoch train time: 0:00:00.646524
elapsed time: 0:00:16.719707
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:44:01.801548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2484.96
 ---- batch: 020 ----
mean loss: 2405.54
 ---- batch: 030 ----
mean loss: 2399.11
 ---- batch: 040 ----
mean loss: 2335.21
train mean loss: 2400.30
epoch train time: 0:00:00.657926
elapsed time: 0:00:17.377869
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:44:02.459694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2291.55
 ---- batch: 020 ----
mean loss: 2297.41
 ---- batch: 030 ----
mean loss: 2227.40
 ---- batch: 040 ----
mean loss: 2195.39
train mean loss: 2250.35
epoch train time: 0:00:00.683690
elapsed time: 0:00:18.061783
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:44:03.143605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2156.01
 ---- batch: 020 ----
mean loss: 2173.62
 ---- batch: 030 ----
mean loss: 2113.00
 ---- batch: 040 ----
mean loss: 2036.14
train mean loss: 2116.64
epoch train time: 0:00:00.693513
elapsed time: 0:00:18.755570
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:44:03.837409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2052.06
 ---- batch: 020 ----
mean loss: 1971.89
 ---- batch: 030 ----
mean loss: 2003.24
 ---- batch: 040 ----
mean loss: 1946.26
train mean loss: 1990.32
epoch train time: 0:00:00.666789
elapsed time: 0:00:19.422585
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:44:04.504430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1958.19
 ---- batch: 020 ----
mean loss: 1881.51
 ---- batch: 030 ----
mean loss: 1855.88
 ---- batch: 040 ----
mean loss: 1828.05
train mean loss: 1877.43
epoch train time: 0:00:00.666826
elapsed time: 0:00:20.089645
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:44:05.171510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1812.23
 ---- batch: 020 ----
mean loss: 1778.01
 ---- batch: 030 ----
mean loss: 1767.89
 ---- batch: 040 ----
mean loss: 1769.87
train mean loss: 1777.46
epoch train time: 0:00:00.687919
elapsed time: 0:00:20.777814
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:44:05.859643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1705.22
 ---- batch: 020 ----
mean loss: 1694.51
 ---- batch: 030 ----
mean loss: 1688.44
 ---- batch: 040 ----
mean loss: 1621.47
train mean loss: 1678.03
epoch train time: 0:00:00.680409
elapsed time: 0:00:21.458470
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:44:06.540331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1650.67
 ---- batch: 020 ----
mean loss: 1589.73
 ---- batch: 030 ----
mean loss: 1572.27
 ---- batch: 040 ----
mean loss: 1563.39
train mean loss: 1592.43
epoch train time: 0:00:00.697849
elapsed time: 0:00:22.156641
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:44:07.238471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1524.66
 ---- batch: 020 ----
mean loss: 1507.99
 ---- batch: 030 ----
mean loss: 1478.42
 ---- batch: 040 ----
mean loss: 1467.98
train mean loss: 1492.12
epoch train time: 0:00:00.676685
elapsed time: 0:00:22.833544
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:44:07.915387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1411.89
 ---- batch: 020 ----
mean loss: 1384.55
 ---- batch: 030 ----
mean loss: 1360.69
 ---- batch: 040 ----
mean loss: 1351.47
train mean loss: 1375.37
epoch train time: 0:00:00.655299
elapsed time: 0:00:23.489100
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:44:08.570941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1321.50
 ---- batch: 020 ----
mean loss: 1292.20
 ---- batch: 030 ----
mean loss: 1274.99
 ---- batch: 040 ----
mean loss: 1267.61
train mean loss: 1283.57
epoch train time: 0:00:00.666185
elapsed time: 0:00:24.155511
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:44:09.237337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1221.53
 ---- batch: 020 ----
mean loss: 1218.16
 ---- batch: 030 ----
mean loss: 1201.61
 ---- batch: 040 ----
mean loss: 1177.28
train mean loss: 1202.59
epoch train time: 0:00:00.671227
elapsed time: 0:00:24.826994
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:44:09.908842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1141.26
 ---- batch: 020 ----
mean loss: 1134.07
 ---- batch: 030 ----
mean loss: 1129.32
 ---- batch: 040 ----
mean loss: 1115.67
train mean loss: 1131.63
epoch train time: 0:00:00.676649
elapsed time: 0:00:25.503948
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:44:10.585840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1076.65
 ---- batch: 020 ----
mean loss: 1062.62
 ---- batch: 030 ----
mean loss: 1057.65
 ---- batch: 040 ----
mean loss: 1043.93
train mean loss: 1057.09
epoch train time: 0:00:00.675434
elapsed time: 0:00:26.179696
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:44:11.261531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1028.37
 ---- batch: 020 ----
mean loss: 981.21
 ---- batch: 030 ----
mean loss: 1001.98
 ---- batch: 040 ----
mean loss: 967.62
train mean loss: 990.81
epoch train time: 0:00:00.661532
elapsed time: 0:00:26.841469
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:44:11.923279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.62
 ---- batch: 020 ----
mean loss: 943.67
 ---- batch: 030 ----
mean loss: 943.89
 ---- batch: 040 ----
mean loss: 891.76
train mean loss: 930.27
epoch train time: 0:00:00.630700
elapsed time: 0:00:27.472350
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:44:12.554216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.66
 ---- batch: 020 ----
mean loss: 882.86
 ---- batch: 030 ----
mean loss: 866.52
 ---- batch: 040 ----
mean loss: 844.95
train mean loss: 868.78
epoch train time: 0:00:00.644825
elapsed time: 0:00:28.117448
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:44:13.199243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.34
 ---- batch: 020 ----
mean loss: 807.38
 ---- batch: 030 ----
mean loss: 811.88
 ---- batch: 040 ----
mean loss: 801.01
train mean loss: 812.51
epoch train time: 0:00:00.668814
elapsed time: 0:00:28.786537
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:44:13.868345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 788.29
 ---- batch: 020 ----
mean loss: 770.79
 ---- batch: 030 ----
mean loss: 754.44
 ---- batch: 040 ----
mean loss: 739.57
train mean loss: 761.55
epoch train time: 0:00:00.674487
elapsed time: 0:00:29.461263
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:44:14.543070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 737.56
 ---- batch: 020 ----
mean loss: 736.11
 ---- batch: 030 ----
mean loss: 704.71
 ---- batch: 040 ----
mean loss: 697.09
train mean loss: 714.97
epoch train time: 0:00:00.688042
elapsed time: 0:00:30.149536
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:44:15.231365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.81
 ---- batch: 020 ----
mean loss: 672.13
 ---- batch: 030 ----
mean loss: 663.34
 ---- batch: 040 ----
mean loss: 652.61
train mean loss: 667.51
epoch train time: 0:00:00.655036
elapsed time: 0:00:30.804779
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:44:15.886619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 641.42
 ---- batch: 020 ----
mean loss: 650.22
 ---- batch: 030 ----
mean loss: 621.89
 ---- batch: 040 ----
mean loss: 613.49
train mean loss: 628.49
epoch train time: 0:00:00.634392
elapsed time: 0:00:31.439443
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:44:16.521316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.67
 ---- batch: 020 ----
mean loss: 601.02
 ---- batch: 030 ----
mean loss: 578.86
 ---- batch: 040 ----
mean loss: 576.77
train mean loss: 590.51
epoch train time: 0:00:00.638429
elapsed time: 0:00:32.078127
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:44:17.160000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.19
 ---- batch: 020 ----
mean loss: 550.56
 ---- batch: 030 ----
mean loss: 555.25
 ---- batch: 040 ----
mean loss: 541.98
train mean loss: 552.70
epoch train time: 0:00:00.663947
elapsed time: 0:00:32.742438
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:44:17.824265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.04
 ---- batch: 020 ----
mean loss: 517.86
 ---- batch: 030 ----
mean loss: 510.83
 ---- batch: 040 ----
mean loss: 505.71
train mean loss: 515.50
epoch train time: 0:00:00.668645
elapsed time: 0:00:33.411305
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:44:18.493132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.73
 ---- batch: 020 ----
mean loss: 480.79
 ---- batch: 030 ----
mean loss: 493.65
 ---- batch: 040 ----
mean loss: 476.67
train mean loss: 485.71
epoch train time: 0:00:00.673092
elapsed time: 0:00:34.084656
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:44:19.166497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.71
 ---- batch: 020 ----
mean loss: 465.12
 ---- batch: 030 ----
mean loss: 456.92
 ---- batch: 040 ----
mean loss: 446.41
train mean loss: 455.11
epoch train time: 0:00:00.637293
elapsed time: 0:00:34.722190
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:44:19.804015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.83
 ---- batch: 020 ----
mean loss: 437.49
 ---- batch: 030 ----
mean loss: 429.67
 ---- batch: 040 ----
mean loss: 416.57
train mean loss: 428.22
epoch train time: 0:00:00.642557
elapsed time: 0:00:35.364965
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:44:20.446789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.51
 ---- batch: 020 ----
mean loss: 412.98
 ---- batch: 030 ----
mean loss: 391.09
 ---- batch: 040 ----
mean loss: 405.89
train mean loss: 402.45
epoch train time: 0:00:00.643782
elapsed time: 0:00:36.008979
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:44:21.090809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.34
 ---- batch: 020 ----
mean loss: 377.15
 ---- batch: 030 ----
mean loss: 371.91
 ---- batch: 040 ----
mean loss: 370.30
train mean loss: 375.45
epoch train time: 0:00:00.669960
elapsed time: 0:00:36.679240
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:44:21.761092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.59
 ---- batch: 020 ----
mean loss: 357.96
 ---- batch: 030 ----
mean loss: 355.75
 ---- batch: 040 ----
mean loss: 342.63
train mean loss: 355.65
epoch train time: 0:00:00.664268
elapsed time: 0:00:37.343758
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:44:22.425583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.71
 ---- batch: 020 ----
mean loss: 335.36
 ---- batch: 030 ----
mean loss: 332.13
 ---- batch: 040 ----
mean loss: 332.96
train mean loss: 332.60
epoch train time: 0:00:00.656087
elapsed time: 0:00:38.000072
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:44:23.081900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.16
 ---- batch: 020 ----
mean loss: 316.91
 ---- batch: 030 ----
mean loss: 316.74
 ---- batch: 040 ----
mean loss: 311.80
train mean loss: 314.78
epoch train time: 0:00:00.635900
elapsed time: 0:00:38.636203
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:44:23.718029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.77
 ---- batch: 020 ----
mean loss: 295.05
 ---- batch: 030 ----
mean loss: 291.25
 ---- batch: 040 ----
mean loss: 290.38
train mean loss: 294.45
epoch train time: 0:00:00.642437
elapsed time: 0:00:39.278848
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:44:24.360692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.05
 ---- batch: 020 ----
mean loss: 285.13
 ---- batch: 030 ----
mean loss: 273.64
 ---- batch: 040 ----
mean loss: 278.73
train mean loss: 281.40
epoch train time: 0:00:00.667555
elapsed time: 0:00:39.946730
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:44:25.028580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.06
 ---- batch: 020 ----
mean loss: 265.66
 ---- batch: 030 ----
mean loss: 261.67
 ---- batch: 040 ----
mean loss: 255.46
train mean loss: 263.25
epoch train time: 0:00:00.684655
elapsed time: 0:00:40.631669
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:44:25.713509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.18
 ---- batch: 020 ----
mean loss: 247.87
 ---- batch: 030 ----
mean loss: 244.88
 ---- batch: 040 ----
mean loss: 245.35
train mean loss: 247.35
epoch train time: 0:00:00.686738
elapsed time: 0:00:41.318692
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:44:26.400526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.13
 ---- batch: 020 ----
mean loss: 226.72
 ---- batch: 030 ----
mean loss: 238.94
 ---- batch: 040 ----
mean loss: 230.55
train mean loss: 233.95
epoch train time: 0:00:00.650146
elapsed time: 0:00:41.969079
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:44:27.050921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.84
 ---- batch: 020 ----
mean loss: 225.89
 ---- batch: 030 ----
mean loss: 224.16
 ---- batch: 040 ----
mean loss: 216.82
train mean loss: 223.14
epoch train time: 0:00:00.637803
elapsed time: 0:00:42.607126
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:44:27.688995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.30
 ---- batch: 020 ----
mean loss: 218.54
 ---- batch: 030 ----
mean loss: 207.72
 ---- batch: 040 ----
mean loss: 202.18
train mean loss: 211.38
epoch train time: 0:00:00.643696
elapsed time: 0:00:43.251120
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:44:28.332971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.92
 ---- batch: 020 ----
mean loss: 202.86
 ---- batch: 030 ----
mean loss: 198.29
 ---- batch: 040 ----
mean loss: 199.13
train mean loss: 200.60
epoch train time: 0:00:00.686218
elapsed time: 0:00:43.937620
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:44:29.019470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.94
 ---- batch: 020 ----
mean loss: 189.23
 ---- batch: 030 ----
mean loss: 190.16
 ---- batch: 040 ----
mean loss: 186.43
train mean loss: 189.21
epoch train time: 0:00:00.687208
elapsed time: 0:00:44.625093
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:44:29.706973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.71
 ---- batch: 020 ----
mean loss: 177.63
 ---- batch: 030 ----
mean loss: 184.80
 ---- batch: 040 ----
mean loss: 179.22
train mean loss: 180.07
epoch train time: 0:00:00.700067
elapsed time: 0:00:45.325449
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:44:30.407279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.65
 ---- batch: 020 ----
mean loss: 172.85
 ---- batch: 030 ----
mean loss: 168.70
 ---- batch: 040 ----
mean loss: 174.45
train mean loss: 172.60
epoch train time: 0:00:00.656173
elapsed time: 0:00:45.981905
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:44:31.063731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.31
 ---- batch: 020 ----
mean loss: 164.94
 ---- batch: 030 ----
mean loss: 163.66
 ---- batch: 040 ----
mean loss: 168.04
train mean loss: 164.16
epoch train time: 0:00:00.631653
elapsed time: 0:00:46.613765
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:44:31.695588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.21
 ---- batch: 020 ----
mean loss: 157.15
 ---- batch: 030 ----
mean loss: 152.92
 ---- batch: 040 ----
mean loss: 158.37
train mean loss: 157.59
epoch train time: 0:00:00.663579
elapsed time: 0:00:47.277620
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:44:32.359470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.14
 ---- batch: 020 ----
mean loss: 152.69
 ---- batch: 030 ----
mean loss: 148.35
 ---- batch: 040 ----
mean loss: 148.45
train mean loss: 150.57
epoch train time: 0:00:00.685358
elapsed time: 0:00:47.963232
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:44:33.045080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.55
 ---- batch: 020 ----
mean loss: 144.72
 ---- batch: 030 ----
mean loss: 141.67
 ---- batch: 040 ----
mean loss: 141.37
train mean loss: 144.49
epoch train time: 0:00:00.670971
elapsed time: 0:00:48.634517
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:44:33.716349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.19
 ---- batch: 020 ----
mean loss: 138.59
 ---- batch: 030 ----
mean loss: 136.99
 ---- batch: 040 ----
mean loss: 136.73
train mean loss: 138.32
epoch train time: 0:00:00.681752
elapsed time: 0:00:49.316544
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:44:34.398414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.57
 ---- batch: 020 ----
mean loss: 131.37
 ---- batch: 030 ----
mean loss: 132.48
 ---- batch: 040 ----
mean loss: 134.38
train mean loss: 133.59
epoch train time: 0:00:00.675310
elapsed time: 0:00:49.992110
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:44:35.073935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.66
 ---- batch: 020 ----
mean loss: 127.86
 ---- batch: 030 ----
mean loss: 128.69
 ---- batch: 040 ----
mean loss: 126.26
train mean loss: 128.34
epoch train time: 0:00:00.653167
elapsed time: 0:00:50.645509
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:44:35.727355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.83
 ---- batch: 020 ----
mean loss: 128.29
 ---- batch: 030 ----
mean loss: 125.74
 ---- batch: 040 ----
mean loss: 120.49
train mean loss: 124.98
epoch train time: 0:00:00.689612
elapsed time: 0:00:51.335413
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:44:36.417240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.62
 ---- batch: 020 ----
mean loss: 120.81
 ---- batch: 030 ----
mean loss: 115.95
 ---- batch: 040 ----
mean loss: 119.14
train mean loss: 119.87
epoch train time: 0:00:00.687769
elapsed time: 0:00:52.023436
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:44:37.105280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.13
 ---- batch: 020 ----
mean loss: 117.34
 ---- batch: 030 ----
mean loss: 115.18
 ---- batch: 040 ----
mean loss: 115.89
train mean loss: 116.79
epoch train time: 0:00:00.681793
elapsed time: 0:00:52.705472
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:44:37.787320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.02
 ---- batch: 020 ----
mean loss: 113.16
 ---- batch: 030 ----
mean loss: 112.40
 ---- batch: 040 ----
mean loss: 114.02
train mean loss: 113.14
epoch train time: 0:00:00.655741
elapsed time: 0:00:53.361457
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:44:38.443282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.19
 ---- batch: 020 ----
mean loss: 109.55
 ---- batch: 030 ----
mean loss: 109.52
 ---- batch: 040 ----
mean loss: 104.89
train mean loss: 109.50
epoch train time: 0:00:00.647660
elapsed time: 0:00:54.009341
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:44:39.091167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.30
 ---- batch: 020 ----
mean loss: 106.94
 ---- batch: 030 ----
mean loss: 108.11
 ---- batch: 040 ----
mean loss: 105.26
train mean loss: 106.30
epoch train time: 0:00:00.645318
elapsed time: 0:00:54.654904
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:44:39.736734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.40
 ---- batch: 020 ----
mean loss: 103.86
 ---- batch: 030 ----
mean loss: 102.04
 ---- batch: 040 ----
mean loss: 105.05
train mean loss: 104.58
epoch train time: 0:00:00.686195
elapsed time: 0:00:55.341356
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:44:40.423245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.83
 ---- batch: 020 ----
mean loss: 101.46
 ---- batch: 030 ----
mean loss: 101.82
 ---- batch: 040 ----
mean loss: 102.74
train mean loss: 100.88
epoch train time: 0:00:00.687115
elapsed time: 0:00:56.028767
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:44:41.110601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.05
 ---- batch: 020 ----
mean loss: 99.28
 ---- batch: 030 ----
mean loss: 101.00
 ---- batch: 040 ----
mean loss: 95.83
train mean loss: 98.74
epoch train time: 0:00:00.668267
elapsed time: 0:00:56.697295
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:44:41.779123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.78
 ---- batch: 020 ----
mean loss: 92.86
 ---- batch: 030 ----
mean loss: 97.74
 ---- batch: 040 ----
mean loss: 98.31
train mean loss: 96.42
epoch train time: 0:00:00.645761
elapsed time: 0:00:57.343280
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:44:42.425112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.46
 ---- batch: 020 ----
mean loss: 96.68
 ---- batch: 030 ----
mean loss: 92.25
 ---- batch: 040 ----
mean loss: 94.53
train mean loss: 95.05
epoch train time: 0:00:00.653543
elapsed time: 0:00:57.997065
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:44:43.078891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.83
 ---- batch: 020 ----
mean loss: 93.02
 ---- batch: 030 ----
mean loss: 92.04
 ---- batch: 040 ----
mean loss: 90.51
train mean loss: 91.75
epoch train time: 0:00:00.642323
elapsed time: 0:00:58.639638
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:44:43.721480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.14
 ---- batch: 020 ----
mean loss: 92.17
 ---- batch: 030 ----
mean loss: 92.06
 ---- batch: 040 ----
mean loss: 88.44
train mean loss: 90.93
epoch train time: 0:00:00.680724
elapsed time: 0:00:59.320845
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:44:44.402713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.61
 ---- batch: 020 ----
mean loss: 88.53
 ---- batch: 030 ----
mean loss: 88.35
 ---- batch: 040 ----
mean loss: 89.29
train mean loss: 89.03
epoch train time: 0:00:00.685457
elapsed time: 0:01:00.006590
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:44:45.088467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.51
 ---- batch: 020 ----
mean loss: 85.89
 ---- batch: 030 ----
mean loss: 84.88
 ---- batch: 040 ----
mean loss: 87.66
train mean loss: 86.79
epoch train time: 0:00:00.622530
elapsed time: 0:01:00.629404
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:44:45.711317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.45
 ---- batch: 020 ----
mean loss: 85.17
 ---- batch: 030 ----
mean loss: 86.12
 ---- batch: 040 ----
mean loss: 85.48
train mean loss: 86.39
epoch train time: 0:00:00.635720
elapsed time: 0:01:01.265461
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:44:46.347283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.53
 ---- batch: 020 ----
mean loss: 85.72
 ---- batch: 030 ----
mean loss: 84.45
 ---- batch: 040 ----
mean loss: 81.99
train mean loss: 83.92
epoch train time: 0:00:00.635149
elapsed time: 0:01:01.900838
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:44:46.982637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.14
 ---- batch: 020 ----
mean loss: 87.87
 ---- batch: 030 ----
mean loss: 83.09
 ---- batch: 040 ----
mean loss: 80.90
train mean loss: 82.70
epoch train time: 0:00:00.663436
elapsed time: 0:01:02.564487
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:44:47.646311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.05
 ---- batch: 020 ----
mean loss: 79.39
 ---- batch: 030 ----
mean loss: 82.94
 ---- batch: 040 ----
mean loss: 81.99
train mean loss: 81.77
epoch train time: 0:00:00.682498
elapsed time: 0:01:03.247225
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:44:48.329068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.89
 ---- batch: 020 ----
mean loss: 80.25
 ---- batch: 030 ----
mean loss: 82.75
 ---- batch: 040 ----
mean loss: 80.26
train mean loss: 81.45
epoch train time: 0:00:00.674093
elapsed time: 0:01:03.921545
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:44:49.003382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.62
 ---- batch: 020 ----
mean loss: 80.25
 ---- batch: 030 ----
mean loss: 78.89
 ---- batch: 040 ----
mean loss: 76.43
train mean loss: 79.12
epoch train time: 0:00:00.642824
elapsed time: 0:01:04.564632
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:44:49.646452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.74
 ---- batch: 020 ----
mean loss: 73.48
 ---- batch: 030 ----
mean loss: 78.41
 ---- batch: 040 ----
mean loss: 80.20
train mean loss: 77.69
epoch train time: 0:00:00.636099
elapsed time: 0:01:05.200926
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:44:50.282745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.25
 ---- batch: 020 ----
mean loss: 80.76
 ---- batch: 030 ----
mean loss: 78.34
 ---- batch: 040 ----
mean loss: 78.40
train mean loss: 78.51
epoch train time: 0:00:00.643191
elapsed time: 0:01:05.844345
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:44:50.926169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.59
 ---- batch: 020 ----
mean loss: 74.24
 ---- batch: 030 ----
mean loss: 79.47
 ---- batch: 040 ----
mean loss: 75.79
train mean loss: 76.46
epoch train time: 0:00:00.681797
elapsed time: 0:01:06.526390
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:44:51.608222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.32
 ---- batch: 020 ----
mean loss: 75.82
 ---- batch: 030 ----
mean loss: 77.60
 ---- batch: 040 ----
mean loss: 77.21
train mean loss: 76.50
epoch train time: 0:00:00.678642
elapsed time: 0:01:07.205278
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:44:52.287118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.53
 ---- batch: 020 ----
mean loss: 72.94
 ---- batch: 030 ----
mean loss: 75.76
 ---- batch: 040 ----
mean loss: 73.19
train mean loss: 74.84
epoch train time: 0:00:00.665936
elapsed time: 0:01:07.871446
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:44:52.953265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.32
 ---- batch: 020 ----
mean loss: 74.18
 ---- batch: 030 ----
mean loss: 75.06
 ---- batch: 040 ----
mean loss: 75.55
train mean loss: 73.78
epoch train time: 0:00:00.641622
elapsed time: 0:01:08.513268
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:44:53.595088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.18
 ---- batch: 020 ----
mean loss: 71.97
 ---- batch: 030 ----
mean loss: 72.09
 ---- batch: 040 ----
mean loss: 74.52
train mean loss: 73.82
epoch train time: 0:00:00.634776
elapsed time: 0:01:09.148274
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:44:54.230103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.00
 ---- batch: 020 ----
mean loss: 73.78
 ---- batch: 030 ----
mean loss: 73.00
 ---- batch: 040 ----
mean loss: 75.16
train mean loss: 73.10
epoch train time: 0:00:00.664792
elapsed time: 0:01:09.813292
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:44:54.895123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.33
 ---- batch: 020 ----
mean loss: 71.11
 ---- batch: 030 ----
mean loss: 73.62
 ---- batch: 040 ----
mean loss: 71.17
train mean loss: 72.85
epoch train time: 0:00:00.663350
elapsed time: 0:01:10.476899
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:44:55.558722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.15
 ---- batch: 020 ----
mean loss: 73.23
 ---- batch: 030 ----
mean loss: 69.96
 ---- batch: 040 ----
mean loss: 70.17
train mean loss: 71.42
epoch train time: 0:00:00.662548
elapsed time: 0:01:11.139687
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:44:56.221572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.97
 ---- batch: 020 ----
mean loss: 72.98
 ---- batch: 030 ----
mean loss: 69.48
 ---- batch: 040 ----
mean loss: 73.11
train mean loss: 71.47
epoch train time: 0:00:00.630405
elapsed time: 0:01:11.770354
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:44:56.852177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.75
 ---- batch: 020 ----
mean loss: 70.30
 ---- batch: 030 ----
mean loss: 71.87
 ---- batch: 040 ----
mean loss: 69.03
train mean loss: 70.78
epoch train time: 0:00:00.626164
elapsed time: 0:01:12.396736
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:44:57.478578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.47
 ---- batch: 020 ----
mean loss: 68.75
 ---- batch: 030 ----
mean loss: 67.07
 ---- batch: 040 ----
mean loss: 68.61
train mean loss: 69.29
epoch train time: 0:00:00.649965
elapsed time: 0:01:13.046943
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:44:58.128767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.31
 ---- batch: 020 ----
mean loss: 67.96
 ---- batch: 030 ----
mean loss: 70.50
 ---- batch: 040 ----
mean loss: 70.56
train mean loss: 69.39
epoch train time: 0:00:00.675328
elapsed time: 0:01:13.722546
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:44:58.804376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.64
 ---- batch: 020 ----
mean loss: 70.26
 ---- batch: 030 ----
mean loss: 65.58
 ---- batch: 040 ----
mean loss: 67.42
train mean loss: 67.57
epoch train time: 0:00:00.692560
elapsed time: 0:01:14.415373
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:44:59.497212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.49
 ---- batch: 020 ----
mean loss: 69.37
 ---- batch: 030 ----
mean loss: 65.44
 ---- batch: 040 ----
mean loss: 66.22
train mean loss: 67.19
epoch train time: 0:00:00.673929
elapsed time: 0:01:15.089532
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:45:00.171371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.60
 ---- batch: 020 ----
mean loss: 66.85
 ---- batch: 030 ----
mean loss: 67.66
 ---- batch: 040 ----
mean loss: 67.81
train mean loss: 67.20
epoch train time: 0:00:00.654548
elapsed time: 0:01:15.744319
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:45:00.826145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.93
 ---- batch: 020 ----
mean loss: 68.93
 ---- batch: 030 ----
mean loss: 65.17
 ---- batch: 040 ----
mean loss: 67.22
train mean loss: 67.03
epoch train time: 0:00:00.638383
elapsed time: 0:01:16.382959
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:45:01.464786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.91
 ---- batch: 020 ----
mean loss: 67.45
 ---- batch: 030 ----
mean loss: 65.70
 ---- batch: 040 ----
mean loss: 67.57
train mean loss: 66.25
epoch train time: 0:00:00.652079
elapsed time: 0:01:17.035296
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:45:02.117149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.90
 ---- batch: 020 ----
mean loss: 65.55
 ---- batch: 030 ----
mean loss: 67.69
 ---- batch: 040 ----
mean loss: 65.25
train mean loss: 66.52
epoch train time: 0:00:00.693491
elapsed time: 0:01:17.729070
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:45:02.810898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.31
 ---- batch: 020 ----
mean loss: 69.18
 ---- batch: 030 ----
mean loss: 62.01
 ---- batch: 040 ----
mean loss: 65.94
train mean loss: 65.91
epoch train time: 0:00:00.681164
elapsed time: 0:01:18.410477
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:45:03.492303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.03
 ---- batch: 020 ----
mean loss: 62.94
 ---- batch: 030 ----
mean loss: 63.31
 ---- batch: 040 ----
mean loss: 66.45
train mean loss: 65.60
epoch train time: 0:00:00.679990
elapsed time: 0:01:19.090698
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:45:04.172552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.94
 ---- batch: 020 ----
mean loss: 65.52
 ---- batch: 030 ----
mean loss: 63.25
 ---- batch: 040 ----
mean loss: 66.98
train mean loss: 65.00
epoch train time: 0:00:00.643127
elapsed time: 0:01:19.734084
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:45:04.815930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.65
 ---- batch: 020 ----
mean loss: 66.03
 ---- batch: 030 ----
mean loss: 64.04
 ---- batch: 040 ----
mean loss: 62.70
train mean loss: 64.62
epoch train time: 0:00:00.639074
elapsed time: 0:01:20.373395
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:45:05.455218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.06
 ---- batch: 020 ----
mean loss: 64.24
 ---- batch: 030 ----
mean loss: 65.84
 ---- batch: 040 ----
mean loss: 65.62
train mean loss: 64.65
epoch train time: 0:00:00.662469
elapsed time: 0:01:21.036144
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:45:06.117972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.08
 ---- batch: 020 ----
mean loss: 62.47
 ---- batch: 030 ----
mean loss: 62.97
 ---- batch: 040 ----
mean loss: 65.38
train mean loss: 64.12
epoch train time: 0:00:00.673724
elapsed time: 0:01:21.710171
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:45:06.792000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.80
 ---- batch: 020 ----
mean loss: 61.07
 ---- batch: 030 ----
mean loss: 64.56
 ---- batch: 040 ----
mean loss: 66.18
train mean loss: 63.14
epoch train time: 0:00:00.672368
elapsed time: 0:01:22.382771
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:45:07.464596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.47
 ---- batch: 020 ----
mean loss: 64.16
 ---- batch: 030 ----
mean loss: 63.42
 ---- batch: 040 ----
mean loss: 61.44
train mean loss: 63.32
epoch train time: 0:00:00.639089
elapsed time: 0:01:23.022079
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:45:08.103903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.63
 ---- batch: 020 ----
mean loss: 64.18
 ---- batch: 030 ----
mean loss: 60.59
 ---- batch: 040 ----
mean loss: 66.88
train mean loss: 63.73
epoch train time: 0:00:00.639108
elapsed time: 0:01:23.661420
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:45:08.743244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.69
 ---- batch: 020 ----
mean loss: 64.62
 ---- batch: 030 ----
mean loss: 62.13
 ---- batch: 040 ----
mean loss: 64.60
train mean loss: 63.02
epoch train time: 0:00:00.647961
elapsed time: 0:01:24.309591
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:45:09.391418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.88
 ---- batch: 020 ----
mean loss: 60.98
 ---- batch: 030 ----
mean loss: 62.28
 ---- batch: 040 ----
mean loss: 62.54
train mean loss: 62.15
epoch train time: 0:00:00.680259
elapsed time: 0:01:24.990169
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:45:10.071973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.62
 ---- batch: 020 ----
mean loss: 59.75
 ---- batch: 030 ----
mean loss: 62.65
 ---- batch: 040 ----
mean loss: 60.49
train mean loss: 61.65
epoch train time: 0:00:00.666801
elapsed time: 0:01:25.657181
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:45:10.739046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.70
 ---- batch: 020 ----
mean loss: 59.53
 ---- batch: 030 ----
mean loss: 59.17
 ---- batch: 040 ----
mean loss: 64.92
train mean loss: 61.02
epoch train time: 0:00:00.654899
elapsed time: 0:01:26.312322
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:45:11.394145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.52
 ---- batch: 020 ----
mean loss: 60.97
 ---- batch: 030 ----
mean loss: 62.28
 ---- batch: 040 ----
mean loss: 59.32
train mean loss: 61.74
epoch train time: 0:00:00.650110
elapsed time: 0:01:26.962654
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:45:12.044519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.64
 ---- batch: 020 ----
mean loss: 60.44
 ---- batch: 030 ----
mean loss: 63.03
 ---- batch: 040 ----
mean loss: 60.69
train mean loss: 61.05
epoch train time: 0:00:00.646668
elapsed time: 0:01:27.609582
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:45:12.691411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.52
 ---- batch: 020 ----
mean loss: 61.02
 ---- batch: 030 ----
mean loss: 58.61
 ---- batch: 040 ----
mean loss: 58.53
train mean loss: 60.24
epoch train time: 0:00:00.649077
elapsed time: 0:01:28.258924
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:45:13.340770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.02
 ---- batch: 020 ----
mean loss: 62.58
 ---- batch: 030 ----
mean loss: 58.90
 ---- batch: 040 ----
mean loss: 58.32
train mean loss: 59.96
epoch train time: 0:00:00.688295
elapsed time: 0:01:28.947535
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:45:14.029376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.65
 ---- batch: 020 ----
mean loss: 61.05
 ---- batch: 030 ----
mean loss: 59.24
 ---- batch: 040 ----
mean loss: 58.71
train mean loss: 59.56
epoch train time: 0:00:00.698705
elapsed time: 0:01:29.646516
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:45:14.728341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.75
 ---- batch: 020 ----
mean loss: 57.09
 ---- batch: 030 ----
mean loss: 61.67
 ---- batch: 040 ----
mean loss: 60.00
train mean loss: 59.54
epoch train time: 0:00:00.663842
elapsed time: 0:01:30.310559
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:45:15.392399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.47
 ---- batch: 020 ----
mean loss: 60.97
 ---- batch: 030 ----
mean loss: 58.50
 ---- batch: 040 ----
mean loss: 58.37
train mean loss: 60.37
epoch train time: 0:00:00.652699
elapsed time: 0:01:30.963505
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:45:16.045339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.95
 ---- batch: 020 ----
mean loss: 59.38
 ---- batch: 030 ----
mean loss: 58.18
 ---- batch: 040 ----
mean loss: 60.58
train mean loss: 58.74
epoch train time: 0:00:00.617886
elapsed time: 0:01:31.581602
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:45:16.663440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.62
 ---- batch: 020 ----
mean loss: 60.20
 ---- batch: 030 ----
mean loss: 58.88
 ---- batch: 040 ----
mean loss: 55.76
train mean loss: 58.35
epoch train time: 0:00:00.670832
elapsed time: 0:01:32.252686
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:45:17.334512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.69
 ---- batch: 020 ----
mean loss: 57.70
 ---- batch: 030 ----
mean loss: 58.76
 ---- batch: 040 ----
mean loss: 58.15
train mean loss: 58.43
epoch train time: 0:00:00.687928
elapsed time: 0:01:32.940858
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:45:18.022687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.08
 ---- batch: 020 ----
mean loss: 59.32
 ---- batch: 030 ----
mean loss: 59.93
 ---- batch: 040 ----
mean loss: 55.79
train mean loss: 58.09
epoch train time: 0:00:00.660409
elapsed time: 0:01:33.601540
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:45:18.683378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.74
 ---- batch: 020 ----
mean loss: 58.05
 ---- batch: 030 ----
mean loss: 58.70
 ---- batch: 040 ----
mean loss: 58.18
train mean loss: 57.17
epoch train time: 0:00:00.661841
elapsed time: 0:01:34.263599
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:45:19.345422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.09
 ---- batch: 020 ----
mean loss: 59.54
 ---- batch: 030 ----
mean loss: 54.34
 ---- batch: 040 ----
mean loss: 59.20
train mean loss: 57.47
epoch train time: 0:00:00.648021
elapsed time: 0:01:34.911862
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:45:19.993699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.63
 ---- batch: 020 ----
mean loss: 58.17
 ---- batch: 030 ----
mean loss: 57.57
 ---- batch: 040 ----
mean loss: 55.36
train mean loss: 57.01
epoch train time: 0:00:00.641777
elapsed time: 0:01:35.553920
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:45:20.635763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.96
 ---- batch: 020 ----
mean loss: 55.81
 ---- batch: 030 ----
mean loss: 57.16
 ---- batch: 040 ----
mean loss: 55.23
train mean loss: 56.36
epoch train time: 0:00:00.657655
elapsed time: 0:01:36.211812
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:45:21.293663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.10
 ---- batch: 020 ----
mean loss: 56.38
 ---- batch: 030 ----
mean loss: 54.62
 ---- batch: 040 ----
mean loss: 56.54
train mean loss: 56.52
epoch train time: 0:00:00.669261
elapsed time: 0:01:36.881320
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:45:21.963149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.74
 ---- batch: 020 ----
mean loss: 55.57
 ---- batch: 030 ----
mean loss: 59.40
 ---- batch: 040 ----
mean loss: 56.25
train mean loss: 56.17
epoch train time: 0:00:00.657438
elapsed time: 0:01:37.538985
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:45:22.620823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.78
 ---- batch: 020 ----
mean loss: 56.14
 ---- batch: 030 ----
mean loss: 56.71
 ---- batch: 040 ----
mean loss: 54.48
train mean loss: 56.10
epoch train time: 0:00:00.647798
elapsed time: 0:01:38.187041
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:45:23.268828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.41
 ---- batch: 020 ----
mean loss: 54.39
 ---- batch: 030 ----
mean loss: 55.06
 ---- batch: 040 ----
mean loss: 55.22
train mean loss: 55.81
epoch train time: 0:00:00.660702
elapsed time: 0:01:38.847965
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:45:23.929792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.89
 ---- batch: 020 ----
mean loss: 56.57
 ---- batch: 030 ----
mean loss: 55.52
 ---- batch: 040 ----
mean loss: 56.22
train mean loss: 55.49
epoch train time: 0:00:00.653738
elapsed time: 0:01:39.501922
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:45:24.583742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.04
 ---- batch: 020 ----
mean loss: 54.98
 ---- batch: 030 ----
mean loss: 56.91
 ---- batch: 040 ----
mean loss: 50.94
train mean loss: 55.22
epoch train time: 0:00:00.681138
elapsed time: 0:01:40.183280
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:45:25.265107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.92
 ---- batch: 020 ----
mean loss: 57.23
 ---- batch: 030 ----
mean loss: 54.20
 ---- batch: 040 ----
mean loss: 55.74
train mean loss: 55.48
epoch train time: 0:00:00.663389
elapsed time: 0:01:40.846954
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:45:25.928794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.54
 ---- batch: 020 ----
mean loss: 53.98
 ---- batch: 030 ----
mean loss: 53.08
 ---- batch: 040 ----
mean loss: 54.17
train mean loss: 54.54
epoch train time: 0:00:00.635352
elapsed time: 0:01:41.482545
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:45:26.564368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.47
 ---- batch: 020 ----
mean loss: 56.59
 ---- batch: 030 ----
mean loss: 53.96
 ---- batch: 040 ----
mean loss: 53.55
train mean loss: 55.13
epoch train time: 0:00:00.645506
elapsed time: 0:01:42.128289
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:45:27.210116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.36
 ---- batch: 020 ----
mean loss: 53.82
 ---- batch: 030 ----
mean loss: 56.41
 ---- batch: 040 ----
mean loss: 54.24
train mean loss: 54.15
epoch train time: 0:00:00.637229
elapsed time: 0:01:42.765780
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:45:27.847616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.58
 ---- batch: 020 ----
mean loss: 54.87
 ---- batch: 030 ----
mean loss: 53.51
 ---- batch: 040 ----
mean loss: 53.47
train mean loss: 53.65
epoch train time: 0:00:00.695020
elapsed time: 0:01:43.461047
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:45:28.542873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.78
 ---- batch: 020 ----
mean loss: 53.74
 ---- batch: 030 ----
mean loss: 52.36
 ---- batch: 040 ----
mean loss: 54.94
train mean loss: 53.98
epoch train time: 0:00:00.681390
elapsed time: 0:01:44.142702
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:45:29.224538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.55
 ---- batch: 020 ----
mean loss: 54.19
 ---- batch: 030 ----
mean loss: 53.30
 ---- batch: 040 ----
mean loss: 51.83
train mean loss: 53.68
epoch train time: 0:00:00.682226
elapsed time: 0:01:44.825160
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:45:29.906988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.23
 ---- batch: 020 ----
mean loss: 51.94
 ---- batch: 030 ----
mean loss: 54.01
 ---- batch: 040 ----
mean loss: 53.76
train mean loss: 53.24
epoch train time: 0:00:00.644194
elapsed time: 0:01:45.469569
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:45:30.551393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.33
 ---- batch: 020 ----
mean loss: 56.68
 ---- batch: 030 ----
mean loss: 51.82
 ---- batch: 040 ----
mean loss: 51.38
train mean loss: 52.73
epoch train time: 0:00:00.645845
elapsed time: 0:01:46.115629
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:45:31.197469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.99
 ---- batch: 020 ----
mean loss: 52.39
 ---- batch: 030 ----
mean loss: 52.80
 ---- batch: 040 ----
mean loss: 51.85
train mean loss: 52.57
epoch train time: 0:00:00.656674
elapsed time: 0:01:46.772616
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:45:31.854452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.68
 ---- batch: 020 ----
mean loss: 49.54
 ---- batch: 030 ----
mean loss: 52.42
 ---- batch: 040 ----
mean loss: 53.35
train mean loss: 52.12
epoch train time: 0:00:00.670986
elapsed time: 0:01:47.443892
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:45:32.525748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.75
 ---- batch: 020 ----
mean loss: 52.11
 ---- batch: 030 ----
mean loss: 54.54
 ---- batch: 040 ----
mean loss: 51.93
train mean loss: 52.71
epoch train time: 0:00:00.684881
elapsed time: 0:01:48.129068
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:45:33.210898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.02
 ---- batch: 020 ----
mean loss: 56.05
 ---- batch: 030 ----
mean loss: 52.71
 ---- batch: 040 ----
mean loss: 49.39
train mean loss: 51.60
epoch train time: 0:00:00.659085
elapsed time: 0:01:48.788376
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:45:33.870207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.95
 ---- batch: 020 ----
mean loss: 51.38
 ---- batch: 030 ----
mean loss: 52.16
 ---- batch: 040 ----
mean loss: 53.79
train mean loss: 51.31
epoch train time: 0:00:00.625948
elapsed time: 0:01:49.414565
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:45:34.496401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.82
 ---- batch: 020 ----
mean loss: 52.78
 ---- batch: 030 ----
mean loss: 51.53
 ---- batch: 040 ----
mean loss: 49.73
train mean loss: 51.23
epoch train time: 0:00:00.635350
elapsed time: 0:01:50.050160
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:45:35.132065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.06
 ---- batch: 020 ----
mean loss: 49.20
 ---- batch: 030 ----
mean loss: 50.46
 ---- batch: 040 ----
mean loss: 52.66
train mean loss: 51.27
epoch train time: 0:00:00.661530
elapsed time: 0:01:50.712012
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:45:35.793846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.25
 ---- batch: 020 ----
mean loss: 51.14
 ---- batch: 030 ----
mean loss: 49.39
 ---- batch: 040 ----
mean loss: 52.17
train mean loss: 50.64
epoch train time: 0:00:00.662754
elapsed time: 0:01:51.375061
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:45:36.456970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.90
 ---- batch: 020 ----
mean loss: 49.30
 ---- batch: 030 ----
mean loss: 51.98
 ---- batch: 040 ----
mean loss: 49.31
train mean loss: 50.69
epoch train time: 0:00:00.674841
elapsed time: 0:01:52.050235
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:45:37.132066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.86
 ---- batch: 020 ----
mean loss: 52.72
 ---- batch: 030 ----
mean loss: 49.42
 ---- batch: 040 ----
mean loss: 47.86
train mean loss: 49.74
epoch train time: 0:00:00.652382
elapsed time: 0:01:52.702867
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:45:37.784670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.59
 ---- batch: 020 ----
mean loss: 50.14
 ---- batch: 030 ----
mean loss: 50.86
 ---- batch: 040 ----
mean loss: 48.91
train mean loss: 50.13
epoch train time: 0:00:00.640640
elapsed time: 0:01:53.343707
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:45:38.425530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.84
 ---- batch: 020 ----
mean loss: 49.53
 ---- batch: 030 ----
mean loss: 49.22
 ---- batch: 040 ----
mean loss: 50.80
train mean loss: 49.40
epoch train time: 0:00:00.641276
elapsed time: 0:01:53.985187
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:45:39.067010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.00
 ---- batch: 020 ----
mean loss: 49.13
 ---- batch: 030 ----
mean loss: 48.30
 ---- batch: 040 ----
mean loss: 49.80
train mean loss: 49.22
epoch train time: 0:00:00.684128
elapsed time: 0:01:54.669564
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:45:39.751395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.14
 ---- batch: 020 ----
mean loss: 51.57
 ---- batch: 030 ----
mean loss: 48.48
 ---- batch: 040 ----
mean loss: 49.07
train mean loss: 49.79
epoch train time: 0:00:00.665832
elapsed time: 0:01:55.335678
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:45:40.417509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.94
 ---- batch: 020 ----
mean loss: 50.33
 ---- batch: 030 ----
mean loss: 46.56
 ---- batch: 040 ----
mean loss: 47.90
train mean loss: 48.67
epoch train time: 0:00:00.664999
elapsed time: 0:01:56.000886
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:45:41.082709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.69
 ---- batch: 020 ----
mean loss: 48.31
 ---- batch: 030 ----
mean loss: 47.35
 ---- batch: 040 ----
mean loss: 49.50
train mean loss: 48.13
epoch train time: 0:00:00.634623
elapsed time: 0:01:56.635731
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:45:41.717589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.55
 ---- batch: 020 ----
mean loss: 49.67
 ---- batch: 030 ----
mean loss: 50.72
 ---- batch: 040 ----
mean loss: 48.81
train mean loss: 49.06
epoch train time: 0:00:00.671210
elapsed time: 0:01:57.307179
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:45:42.389005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.49
 ---- batch: 020 ----
mean loss: 48.32
 ---- batch: 030 ----
mean loss: 47.74
 ---- batch: 040 ----
mean loss: 48.45
train mean loss: 48.06
epoch train time: 0:00:00.644871
elapsed time: 0:01:57.952329
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:45:43.034159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.02
 ---- batch: 020 ----
mean loss: 47.72
 ---- batch: 030 ----
mean loss: 47.82
 ---- batch: 040 ----
mean loss: 47.93
train mean loss: 46.66
epoch train time: 0:00:00.668905
elapsed time: 0:01:58.621468
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:45:43.703298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.99
 ---- batch: 020 ----
mean loss: 48.58
 ---- batch: 030 ----
mean loss: 47.29
 ---- batch: 040 ----
mean loss: 47.14
train mean loss: 47.29
epoch train time: 0:00:00.658501
elapsed time: 0:01:59.280232
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:45:44.362111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.62
 ---- batch: 020 ----
mean loss: 46.00
 ---- batch: 030 ----
mean loss: 46.15
 ---- batch: 040 ----
mean loss: 49.53
train mean loss: 46.92
epoch train time: 0:00:00.648136
elapsed time: 0:01:59.928643
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:45:45.010467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.33
 ---- batch: 020 ----
mean loss: 46.28
 ---- batch: 030 ----
mean loss: 45.62
 ---- batch: 040 ----
mean loss: 49.25
train mean loss: 46.61
epoch train time: 0:00:00.626915
elapsed time: 0:02:00.555768
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:45:45.637608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.77
 ---- batch: 020 ----
mean loss: 44.05
 ---- batch: 030 ----
mean loss: 47.14
 ---- batch: 040 ----
mean loss: 48.56
train mean loss: 46.37
epoch train time: 0:00:00.640700
elapsed time: 0:02:01.196690
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:45:46.278542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.81
 ---- batch: 020 ----
mean loss: 47.70
 ---- batch: 030 ----
mean loss: 45.09
 ---- batch: 040 ----
mean loss: 48.55
train mean loss: 46.72
epoch train time: 0:00:00.652534
elapsed time: 0:02:01.849483
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:45:46.931316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.76
 ---- batch: 020 ----
mean loss: 44.29
 ---- batch: 030 ----
mean loss: 46.51
 ---- batch: 040 ----
mean loss: 46.72
train mean loss: 45.72
epoch train time: 0:00:00.694759
elapsed time: 0:02:02.544568
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:45:47.626418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.03
 ---- batch: 020 ----
mean loss: 43.74
 ---- batch: 030 ----
mean loss: 47.84
 ---- batch: 040 ----
mean loss: 46.67
train mean loss: 45.62
epoch train time: 0:00:00.694680
elapsed time: 0:02:03.239502
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:45:48.321362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.22
 ---- batch: 020 ----
mean loss: 44.46
 ---- batch: 030 ----
mean loss: 45.14
 ---- batch: 040 ----
mean loss: 45.52
train mean loss: 45.39
epoch train time: 0:00:00.653268
elapsed time: 0:02:03.893016
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:45:48.974868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.39
 ---- batch: 020 ----
mean loss: 44.55
 ---- batch: 030 ----
mean loss: 44.65
 ---- batch: 040 ----
mean loss: 42.20
train mean loss: 44.49
epoch train time: 0:00:00.642738
elapsed time: 0:02:04.535986
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:45:49.617811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.88
 ---- batch: 020 ----
mean loss: 43.67
 ---- batch: 030 ----
mean loss: 44.40
 ---- batch: 040 ----
mean loss: 45.08
train mean loss: 45.04
epoch train time: 0:00:00.647571
elapsed time: 0:02:05.183775
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:45:50.265599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.40
 ---- batch: 020 ----
mean loss: 46.05
 ---- batch: 030 ----
mean loss: 45.11
 ---- batch: 040 ----
mean loss: 42.80
train mean loss: 44.64
epoch train time: 0:00:00.690880
elapsed time: 0:02:05.874897
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:45:50.956761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.11
 ---- batch: 020 ----
mean loss: 41.80
 ---- batch: 030 ----
mean loss: 45.79
 ---- batch: 040 ----
mean loss: 44.99
train mean loss: 43.95
epoch train time: 0:00:00.677608
elapsed time: 0:02:06.552798
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:45:51.634627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.31
 ---- batch: 020 ----
mean loss: 43.39
 ---- batch: 030 ----
mean loss: 43.04
 ---- batch: 040 ----
mean loss: 44.13
train mean loss: 43.59
epoch train time: 0:00:00.698124
elapsed time: 0:02:07.251137
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:45:52.332977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.76
 ---- batch: 020 ----
mean loss: 42.12
 ---- batch: 030 ----
mean loss: 45.64
 ---- batch: 040 ----
mean loss: 43.48
train mean loss: 43.54
epoch train time: 0:00:00.649274
elapsed time: 0:02:07.900689
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:45:52.982560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.38
 ---- batch: 020 ----
mean loss: 44.64
 ---- batch: 030 ----
mean loss: 43.41
 ---- batch: 040 ----
mean loss: 45.55
train mean loss: 43.67
epoch train time: 0:00:00.637207
elapsed time: 0:02:08.538190
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:45:53.620031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.46
 ---- batch: 020 ----
mean loss: 44.34
 ---- batch: 030 ----
mean loss: 43.92
 ---- batch: 040 ----
mean loss: 42.41
train mean loss: 42.82
epoch train time: 0:00:00.655977
elapsed time: 0:02:09.194475
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:45:54.276266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.83
 ---- batch: 020 ----
mean loss: 42.92
 ---- batch: 030 ----
mean loss: 42.98
 ---- batch: 040 ----
mean loss: 40.63
train mean loss: 42.50
epoch train time: 0:00:00.673692
elapsed time: 0:02:09.868371
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:45:54.950206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.37
 ---- batch: 020 ----
mean loss: 42.90
 ---- batch: 030 ----
mean loss: 43.07
 ---- batch: 040 ----
mean loss: 41.19
train mean loss: 42.54
epoch train time: 0:00:00.666027
elapsed time: 0:02:10.534707
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:45:55.616577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.27
 ---- batch: 020 ----
mean loss: 41.78
 ---- batch: 030 ----
mean loss: 41.66
 ---- batch: 040 ----
mean loss: 44.51
train mean loss: 42.05
epoch train time: 0:00:00.637512
elapsed time: 0:02:11.172525
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:45:56.254359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.06
 ---- batch: 020 ----
mean loss: 40.03
 ---- batch: 030 ----
mean loss: 41.08
 ---- batch: 040 ----
mean loss: 43.80
train mean loss: 41.71
epoch train time: 0:00:00.628329
elapsed time: 0:02:11.801103
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:45:56.882919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.21
 ---- batch: 020 ----
mean loss: 41.68
 ---- batch: 030 ----
mean loss: 38.80
 ---- batch: 040 ----
mean loss: 40.95
train mean loss: 41.44
epoch train time: 0:00:00.624783
elapsed time: 0:02:12.426083
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:45:57.507914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.30
 ---- batch: 020 ----
mean loss: 43.02
 ---- batch: 030 ----
mean loss: 41.81
 ---- batch: 040 ----
mean loss: 38.98
train mean loss: 41.18
epoch train time: 0:00:00.666329
elapsed time: 0:02:13.092662
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:45:58.174490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.73
 ---- batch: 020 ----
mean loss: 43.46
 ---- batch: 030 ----
mean loss: 40.71
 ---- batch: 040 ----
mean loss: 39.17
train mean loss: 40.97
epoch train time: 0:00:00.664616
elapsed time: 0:02:13.757536
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:45:58.839375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.10
 ---- batch: 020 ----
mean loss: 39.74
 ---- batch: 030 ----
mean loss: 39.60
 ---- batch: 040 ----
mean loss: 40.42
train mean loss: 40.14
epoch train time: 0:00:00.656109
elapsed time: 0:02:14.413873
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:45:59.495707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.70
 ---- batch: 020 ----
mean loss: 40.82
 ---- batch: 030 ----
mean loss: 37.67
 ---- batch: 040 ----
mean loss: 41.90
train mean loss: 40.28
epoch train time: 0:00:00.636462
elapsed time: 0:02:15.050549
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:46:00.132400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.50
 ---- batch: 020 ----
mean loss: 39.84
 ---- batch: 030 ----
mean loss: 39.99
 ---- batch: 040 ----
mean loss: 40.55
train mean loss: 39.67
epoch train time: 0:00:00.631090
elapsed time: 0:02:15.681914
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:46:00.763742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.64
 ---- batch: 020 ----
mean loss: 42.77
 ---- batch: 030 ----
mean loss: 37.77
 ---- batch: 040 ----
mean loss: 41.77
train mean loss: 39.68
epoch train time: 0:00:00.648205
elapsed time: 0:02:16.330362
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:46:01.412202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.56
 ---- batch: 020 ----
mean loss: 38.52
 ---- batch: 030 ----
mean loss: 41.62
 ---- batch: 040 ----
mean loss: 38.35
train mean loss: 39.20
epoch train time: 0:00:00.672462
elapsed time: 0:02:17.003062
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:46:02.084893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.02
 ---- batch: 020 ----
mean loss: 40.99
 ---- batch: 030 ----
mean loss: 41.74
 ---- batch: 040 ----
mean loss: 34.81
train mean loss: 39.81
epoch train time: 0:00:00.655520
elapsed time: 0:02:17.658824
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:46:02.740664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.84
 ---- batch: 020 ----
mean loss: 37.39
 ---- batch: 030 ----
mean loss: 37.80
 ---- batch: 040 ----
mean loss: 40.09
train mean loss: 38.39
epoch train time: 0:00:00.641017
elapsed time: 0:02:18.300068
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:46:03.381903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.96
 ---- batch: 020 ----
mean loss: 39.30
 ---- batch: 030 ----
mean loss: 38.85
 ---- batch: 040 ----
mean loss: 39.83
train mean loss: 38.79
epoch train time: 0:00:00.646534
elapsed time: 0:02:18.946815
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:46:04.028634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.50
 ---- batch: 020 ----
mean loss: 38.23
 ---- batch: 030 ----
mean loss: 39.31
 ---- batch: 040 ----
mean loss: 41.15
train mean loss: 38.54
epoch train time: 0:00:00.630016
elapsed time: 0:02:19.577043
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:46:04.658862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.39
 ---- batch: 020 ----
mean loss: 38.61
 ---- batch: 030 ----
mean loss: 37.39
 ---- batch: 040 ----
mean loss: 38.21
train mean loss: 37.87
epoch train time: 0:00:00.683005
elapsed time: 0:02:20.260300
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:46:05.342135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.01
 ---- batch: 020 ----
mean loss: 36.57
 ---- batch: 030 ----
mean loss: 39.82
 ---- batch: 040 ----
mean loss: 36.39
train mean loss: 37.46
epoch train time: 0:00:00.685777
elapsed time: 0:02:20.946353
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:46:06.028193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.24
 ---- batch: 020 ----
mean loss: 36.76
 ---- batch: 030 ----
mean loss: 37.85
 ---- batch: 040 ----
mean loss: 35.49
train mean loss: 37.41
epoch train time: 0:00:00.674330
elapsed time: 0:02:21.620917
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:46:06.702743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.69
 ---- batch: 020 ----
mean loss: 35.48
 ---- batch: 030 ----
mean loss: 37.36
 ---- batch: 040 ----
mean loss: 36.82
train mean loss: 36.94
epoch train time: 0:00:00.650013
elapsed time: 0:02:22.271152
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:46:07.352974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.56
 ---- batch: 020 ----
mean loss: 38.05
 ---- batch: 030 ----
mean loss: 38.05
 ---- batch: 040 ----
mean loss: 35.13
train mean loss: 37.16
epoch train time: 0:00:00.640418
elapsed time: 0:02:22.911779
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:46:07.993614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.81
 ---- batch: 020 ----
mean loss: 36.00
 ---- batch: 030 ----
mean loss: 36.91
 ---- batch: 040 ----
mean loss: 36.83
train mean loss: 36.55
epoch train time: 0:00:00.623844
elapsed time: 0:02:23.535886
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:46:08.617739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.53
 ---- batch: 020 ----
mean loss: 37.17
 ---- batch: 030 ----
mean loss: 36.97
 ---- batch: 040 ----
mean loss: 35.90
train mean loss: 36.32
epoch train time: 0:00:00.670239
elapsed time: 0:02:24.206405
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:46:09.288233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.44
 ---- batch: 020 ----
mean loss: 36.50
 ---- batch: 030 ----
mean loss: 36.40
 ---- batch: 040 ----
mean loss: 35.84
train mean loss: 36.13
epoch train time: 0:00:00.681981
elapsed time: 0:02:24.888643
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:46:09.970480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.73
 ---- batch: 020 ----
mean loss: 34.67
 ---- batch: 030 ----
mean loss: 36.94
 ---- batch: 040 ----
mean loss: 36.57
train mean loss: 35.79
epoch train time: 0:00:00.669510
elapsed time: 0:02:25.558391
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:46:10.640224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.29
 ---- batch: 020 ----
mean loss: 33.78
 ---- batch: 030 ----
mean loss: 37.01
 ---- batch: 040 ----
mean loss: 35.80
train mean loss: 35.42
epoch train time: 0:00:00.663429
elapsed time: 0:02:26.222065
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:46:11.303892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.87
 ---- batch: 020 ----
mean loss: 33.95
 ---- batch: 030 ----
mean loss: 35.59
 ---- batch: 040 ----
mean loss: 36.77
train mean loss: 35.87
epoch train time: 0:00:00.649682
elapsed time: 0:02:26.872026
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:46:11.953855
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.03
 ---- batch: 020 ----
mean loss: 33.85
 ---- batch: 030 ----
mean loss: 34.30
 ---- batch: 040 ----
mean loss: 35.51
train mean loss: 34.09
epoch train time: 0:00:00.668463
elapsed time: 0:02:27.540806
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:46:12.622605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.84
 ---- batch: 020 ----
mean loss: 34.65
 ---- batch: 030 ----
mean loss: 34.68
 ---- batch: 040 ----
mean loss: 34.32
train mean loss: 33.91
epoch train time: 0:00:00.691698
elapsed time: 0:02:28.232727
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:46:13.314568
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.15
 ---- batch: 020 ----
mean loss: 33.64
 ---- batch: 030 ----
mean loss: 33.48
 ---- batch: 040 ----
mean loss: 33.03
train mean loss: 34.06
epoch train time: 0:00:00.703933
elapsed time: 0:02:28.936885
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:46:14.018726
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.69
 ---- batch: 020 ----
mean loss: 34.16
 ---- batch: 030 ----
mean loss: 33.88
 ---- batch: 040 ----
mean loss: 35.00
train mean loss: 33.86
epoch train time: 0:00:00.635404
elapsed time: 0:02:29.572508
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:46:14.654347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.41
 ---- batch: 020 ----
mean loss: 33.55
 ---- batch: 030 ----
mean loss: 33.96
 ---- batch: 040 ----
mean loss: 33.39
train mean loss: 33.85
epoch train time: 0:00:00.649376
elapsed time: 0:02:30.222121
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:46:15.303965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.98
 ---- batch: 020 ----
mean loss: 32.54
 ---- batch: 030 ----
mean loss: 34.10
 ---- batch: 040 ----
mean loss: 33.01
train mean loss: 33.53
epoch train time: 0:00:00.658248
elapsed time: 0:02:30.880661
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:46:15.962547
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.45
 ---- batch: 020 ----
mean loss: 32.81
 ---- batch: 030 ----
mean loss: 33.03
 ---- batch: 040 ----
mean loss: 33.44
train mean loss: 33.53
epoch train time: 0:00:00.685611
elapsed time: 0:02:31.566600
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:46:16.648455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.20
 ---- batch: 020 ----
mean loss: 33.79
 ---- batch: 030 ----
mean loss: 32.88
 ---- batch: 040 ----
mean loss: 32.72
train mean loss: 33.73
epoch train time: 0:00:00.707796
elapsed time: 0:02:32.274708
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:46:17.356538
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.81
 ---- batch: 020 ----
mean loss: 35.52
 ---- batch: 030 ----
mean loss: 30.33
 ---- batch: 040 ----
mean loss: 34.69
train mean loss: 33.50
epoch train time: 0:00:00.653924
elapsed time: 0:02:32.928857
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:46:18.010716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.46
 ---- batch: 020 ----
mean loss: 33.65
 ---- batch: 030 ----
mean loss: 33.95
 ---- batch: 040 ----
mean loss: 31.73
train mean loss: 33.43
epoch train time: 0:00:00.651251
elapsed time: 0:02:33.580408
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:46:18.662243
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.93
 ---- batch: 020 ----
mean loss: 34.41
 ---- batch: 030 ----
mean loss: 32.50
 ---- batch: 040 ----
mean loss: 33.10
train mean loss: 33.70
epoch train time: 0:00:00.655602
elapsed time: 0:02:34.236256
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:46:19.318076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.00
 ---- batch: 020 ----
mean loss: 32.22
 ---- batch: 030 ----
mean loss: 32.31
 ---- batch: 040 ----
mean loss: 33.72
train mean loss: 33.35
epoch train time: 0:00:00.689510
elapsed time: 0:02:34.926039
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:46:20.007919
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.21
 ---- batch: 020 ----
mean loss: 31.36
 ---- batch: 030 ----
mean loss: 32.96
 ---- batch: 040 ----
mean loss: 34.76
train mean loss: 33.42
epoch train time: 0:00:00.675850
elapsed time: 0:02:35.602215
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:46:20.684055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.60
 ---- batch: 020 ----
mean loss: 33.45
 ---- batch: 030 ----
mean loss: 34.42
 ---- batch: 040 ----
mean loss: 33.87
train mean loss: 33.61
epoch train time: 0:00:00.675675
elapsed time: 0:02:36.278134
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:46:21.359988
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.62
 ---- batch: 020 ----
mean loss: 32.00
 ---- batch: 030 ----
mean loss: 34.27
 ---- batch: 040 ----
mean loss: 33.08
train mean loss: 33.13
epoch train time: 0:00:00.646252
elapsed time: 0:02:36.924690
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:46:22.006523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.00
 ---- batch: 020 ----
mean loss: 33.18
 ---- batch: 030 ----
mean loss: 32.84
 ---- batch: 040 ----
mean loss: 31.48
train mean loss: 33.35
epoch train time: 0:00:00.644879
elapsed time: 0:02:37.569791
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:46:22.651622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.51
 ---- batch: 020 ----
mean loss: 33.72
 ---- batch: 030 ----
mean loss: 34.66
 ---- batch: 040 ----
mean loss: 32.20
train mean loss: 33.39
epoch train time: 0:00:00.674081
elapsed time: 0:02:38.244143
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:46:23.325980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.64
 ---- batch: 020 ----
mean loss: 35.15
 ---- batch: 030 ----
mean loss: 32.56
 ---- batch: 040 ----
mean loss: 33.29
train mean loss: 33.33
epoch train time: 0:00:00.696155
elapsed time: 0:02:38.940574
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:46:24.022458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.45
 ---- batch: 020 ----
mean loss: 34.77
 ---- batch: 030 ----
mean loss: 32.57
 ---- batch: 040 ----
mean loss: 31.26
train mean loss: 33.27
epoch train time: 0:00:00.678120
elapsed time: 0:02:39.618975
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:46:24.700809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.33
 ---- batch: 020 ----
mean loss: 33.57
 ---- batch: 030 ----
mean loss: 35.19
 ---- batch: 040 ----
mean loss: 32.50
train mean loss: 33.30
epoch train time: 0:00:00.633766
elapsed time: 0:02:40.252964
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:46:25.334786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.65
 ---- batch: 020 ----
mean loss: 32.92
 ---- batch: 030 ----
mean loss: 32.94
 ---- batch: 040 ----
mean loss: 34.62
train mean loss: 33.27
epoch train time: 0:00:00.631319
elapsed time: 0:02:40.884479
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:46:25.966300
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.82
 ---- batch: 020 ----
mean loss: 34.93
 ---- batch: 030 ----
mean loss: 32.40
 ---- batch: 040 ----
mean loss: 34.11
train mean loss: 33.51
epoch train time: 0:00:00.630383
elapsed time: 0:02:41.515070
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:46:26.596913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.58
 ---- batch: 020 ----
mean loss: 32.45
 ---- batch: 030 ----
mean loss: 33.37
 ---- batch: 040 ----
mean loss: 34.26
train mean loss: 33.36
epoch train time: 0:00:00.683958
elapsed time: 0:02:42.199297
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:46:27.281174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.96
 ---- batch: 020 ----
mean loss: 33.66
 ---- batch: 030 ----
mean loss: 32.39
 ---- batch: 040 ----
mean loss: 33.07
train mean loss: 33.18
epoch train time: 0:00:00.684560
elapsed time: 0:02:42.884151
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:46:27.965981
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.68
 ---- batch: 020 ----
mean loss: 33.21
 ---- batch: 030 ----
mean loss: 33.27
 ---- batch: 040 ----
mean loss: 33.12
train mean loss: 33.14
epoch train time: 0:00:00.654651
elapsed time: 0:02:43.539029
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:46:28.620858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.82
 ---- batch: 020 ----
mean loss: 33.71
 ---- batch: 030 ----
mean loss: 33.46
 ---- batch: 040 ----
mean loss: 34.04
train mean loss: 33.40
epoch train time: 0:00:00.662738
elapsed time: 0:02:44.201981
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:46:29.283804
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.69
 ---- batch: 020 ----
mean loss: 33.14
 ---- batch: 030 ----
mean loss: 33.80
 ---- batch: 040 ----
mean loss: 33.07
train mean loss: 32.91
epoch train time: 0:00:00.656411
elapsed time: 0:02:44.858593
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:46:29.940434
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.84
 ---- batch: 020 ----
mean loss: 31.10
 ---- batch: 030 ----
mean loss: 34.92
 ---- batch: 040 ----
mean loss: 33.55
train mean loss: 33.15
epoch train time: 0:00:00.663277
elapsed time: 0:02:45.522130
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:46:30.603960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.19
 ---- batch: 020 ----
mean loss: 34.36
 ---- batch: 030 ----
mean loss: 32.19
 ---- batch: 040 ----
mean loss: 34.10
train mean loss: 33.00
epoch train time: 0:00:00.679932
elapsed time: 0:02:46.202343
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:46:31.284212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.75
 ---- batch: 020 ----
mean loss: 32.92
 ---- batch: 030 ----
mean loss: 33.57
 ---- batch: 040 ----
mean loss: 31.71
train mean loss: 32.93
epoch train time: 0:00:00.673350
elapsed time: 0:02:46.875982
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:46:31.957807
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.42
 ---- batch: 020 ----
mean loss: 30.25
 ---- batch: 030 ----
mean loss: 33.01
 ---- batch: 040 ----
mean loss: 34.37
train mean loss: 33.01
epoch train time: 0:00:00.632399
elapsed time: 0:02:47.508597
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:46:32.590429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.70
 ---- batch: 020 ----
mean loss: 33.44
 ---- batch: 030 ----
mean loss: 32.84
 ---- batch: 040 ----
mean loss: 31.28
train mean loss: 32.78
epoch train time: 0:00:00.658994
elapsed time: 0:02:48.167806
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:46:33.249633
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.75
 ---- batch: 020 ----
mean loss: 34.22
 ---- batch: 030 ----
mean loss: 32.07
 ---- batch: 040 ----
mean loss: 32.45
train mean loss: 32.87
epoch train time: 0:00:00.650175
elapsed time: 0:02:48.818274
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:46:33.900067
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.67
 ---- batch: 020 ----
mean loss: 33.45
 ---- batch: 030 ----
mean loss: 33.31
 ---- batch: 040 ----
mean loss: 32.23
train mean loss: 33.04
epoch train time: 0:00:00.673256
elapsed time: 0:02:49.491765
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:46:34.573585
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.31
 ---- batch: 020 ----
mean loss: 32.30
 ---- batch: 030 ----
mean loss: 30.91
 ---- batch: 040 ----
mean loss: 34.09
train mean loss: 32.71
epoch train time: 0:00:00.678468
elapsed time: 0:02:50.170467
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:46:35.252307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.94
 ---- batch: 020 ----
mean loss: 33.53
 ---- batch: 030 ----
mean loss: 33.12
 ---- batch: 040 ----
mean loss: 33.01
train mean loss: 32.79
epoch train time: 0:00:00.654034
elapsed time: 0:02:50.824712
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:46:35.906526
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.16
 ---- batch: 020 ----
mean loss: 31.10
 ---- batch: 030 ----
mean loss: 32.93
 ---- batch: 040 ----
mean loss: 32.72
train mean loss: 32.84
epoch train time: 0:00:00.625737
elapsed time: 0:02:51.450651
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:46:36.532471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.59
 ---- batch: 020 ----
mean loss: 33.05
 ---- batch: 030 ----
mean loss: 32.66
 ---- batch: 040 ----
mean loss: 33.11
train mean loss: 32.87
epoch train time: 0:00:00.656735
elapsed time: 0:02:52.107700
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:46:37.189528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.17
 ---- batch: 020 ----
mean loss: 33.04
 ---- batch: 030 ----
mean loss: 34.23
 ---- batch: 040 ----
mean loss: 33.63
train mean loss: 32.90
epoch train time: 0:00:00.655148
elapsed time: 0:02:52.763093
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:46:37.844918
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.83
 ---- batch: 020 ----
mean loss: 33.31
 ---- batch: 030 ----
mean loss: 30.91
 ---- batch: 040 ----
mean loss: 32.73
train mean loss: 32.81
epoch train time: 0:00:00.670834
elapsed time: 0:02:53.434168
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:46:38.515995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.81
 ---- batch: 020 ----
mean loss: 31.74
 ---- batch: 030 ----
mean loss: 32.95
 ---- batch: 040 ----
mean loss: 33.03
train mean loss: 32.67
epoch train time: 0:00:00.692908
elapsed time: 0:02:54.127319
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:46:39.209150
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.50
 ---- batch: 020 ----
mean loss: 31.62
 ---- batch: 030 ----
mean loss: 32.34
 ---- batch: 040 ----
mean loss: 32.05
train mean loss: 32.33
epoch train time: 0:00:00.653581
elapsed time: 0:02:54.781137
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:46:39.862967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.83
 ---- batch: 020 ----
mean loss: 31.05
 ---- batch: 030 ----
mean loss: 33.34
 ---- batch: 040 ----
mean loss: 32.21
train mean loss: 32.60
epoch train time: 0:00:00.633069
elapsed time: 0:02:55.414432
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:46:40.496267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.93
 ---- batch: 020 ----
mean loss: 32.86
 ---- batch: 030 ----
mean loss: 33.35
 ---- batch: 040 ----
mean loss: 32.06
train mean loss: 32.60
epoch train time: 0:00:00.653318
elapsed time: 0:02:56.067973
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:46:41.149823
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.51
 ---- batch: 020 ----
mean loss: 34.37
 ---- batch: 030 ----
mean loss: 32.24
 ---- batch: 040 ----
mean loss: 32.31
train mean loss: 32.35
epoch train time: 0:00:00.688140
elapsed time: 0:02:56.756390
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:46:41.838219
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.68
 ---- batch: 020 ----
mean loss: 31.53
 ---- batch: 030 ----
mean loss: 33.05
 ---- batch: 040 ----
mean loss: 33.46
train mean loss: 32.32
epoch train time: 0:00:00.684163
elapsed time: 0:02:57.440794
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:46:42.522641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.33
 ---- batch: 020 ----
mean loss: 33.32
 ---- batch: 030 ----
mean loss: 32.93
 ---- batch: 040 ----
mean loss: 32.58
train mean loss: 32.56
epoch train time: 0:00:00.677672
elapsed time: 0:02:58.118715
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:46:43.200541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.88
 ---- batch: 020 ----
mean loss: 32.26
 ---- batch: 030 ----
mean loss: 32.15
 ---- batch: 040 ----
mean loss: 31.88
train mean loss: 32.41
epoch train time: 0:00:00.646857
elapsed time: 0:02:58.765832
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:46:43.847669
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.31
 ---- batch: 020 ----
mean loss: 32.69
 ---- batch: 030 ----
mean loss: 32.37
 ---- batch: 040 ----
mean loss: 31.95
train mean loss: 32.48
epoch train time: 0:00:00.633814
elapsed time: 0:02:59.406904
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_3/checkpoint.pth.tar
**** end time: 2019-09-20 21:46:44.488672 ****
