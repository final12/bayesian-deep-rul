Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_3', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 22674
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:26:57.822119 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:26:57.831987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4533.38
 ---- batch: 020 ----
mean loss: 4307.27
train mean loss: 4410.93
epoch train time: 0:00:07.944612
elapsed time: 0:00:07.960701
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:27:05.782857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4102.08
 ---- batch: 020 ----
mean loss: 3886.00
train mean loss: 3976.30
epoch train time: 0:00:00.348370
elapsed time: 0:00:08.309219
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:27:06.131409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3633.32
 ---- batch: 020 ----
mean loss: 3600.30
train mean loss: 3611.18
epoch train time: 0:00:00.346123
elapsed time: 0:00:08.655524
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:27:06.477715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3394.60
 ---- batch: 020 ----
mean loss: 3250.54
train mean loss: 3297.67
epoch train time: 0:00:00.346152
elapsed time: 0:00:09.001866
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:27:06.824050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3118.30
 ---- batch: 020 ----
mean loss: 2989.69
train mean loss: 3055.25
epoch train time: 0:00:00.350829
elapsed time: 0:00:09.352876
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:27:07.175057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2906.77
 ---- batch: 020 ----
mean loss: 2785.23
train mean loss: 2832.37
epoch train time: 0:00:00.349357
elapsed time: 0:00:09.702407
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:27:07.524585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2702.46
 ---- batch: 020 ----
mean loss: 2609.21
train mean loss: 2648.90
epoch train time: 0:00:00.351918
elapsed time: 0:00:10.054529
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:27:07.876704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2556.08
 ---- batch: 020 ----
mean loss: 2455.89
train mean loss: 2501.24
epoch train time: 0:00:00.355804
elapsed time: 0:00:10.410507
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:27:08.232682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2418.89
 ---- batch: 020 ----
mean loss: 2339.16
train mean loss: 2365.29
epoch train time: 0:00:00.360573
elapsed time: 0:00:10.771254
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:27:08.593429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2301.15
 ---- batch: 020 ----
mean loss: 2240.62
train mean loss: 2260.63
epoch train time: 0:00:00.360204
elapsed time: 0:00:11.131660
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:27:08.953839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2160.41
 ---- batch: 020 ----
mean loss: 2163.82
train mean loss: 2164.15
epoch train time: 0:00:00.356890
elapsed time: 0:00:11.488737
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:27:09.310913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2099.96
 ---- batch: 020 ----
mean loss: 2024.49
train mean loss: 2068.30
epoch train time: 0:00:00.359108
elapsed time: 0:00:11.848022
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:27:09.670204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1988.37
 ---- batch: 020 ----
mean loss: 1993.48
train mean loss: 1986.48
epoch train time: 0:00:00.355831
elapsed time: 0:00:12.204043
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:27:10.026220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1914.43
 ---- batch: 020 ----
mean loss: 1890.74
train mean loss: 1900.71
epoch train time: 0:00:00.353411
elapsed time: 0:00:12.557677
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:27:10.379882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1869.62
 ---- batch: 020 ----
mean loss: 1802.38
train mean loss: 1840.73
epoch train time: 0:00:00.356094
elapsed time: 0:00:12.913973
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:27:10.736162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1799.84
 ---- batch: 020 ----
mean loss: 1741.51
train mean loss: 1765.97
epoch train time: 0:00:00.353414
elapsed time: 0:00:13.267577
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:27:11.089782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1729.46
 ---- batch: 020 ----
mean loss: 1701.31
train mean loss: 1709.00
epoch train time: 0:00:00.361349
elapsed time: 0:00:13.629178
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:27:11.451358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1675.21
 ---- batch: 020 ----
mean loss: 1631.67
train mean loss: 1655.22
epoch train time: 0:00:00.354319
elapsed time: 0:00:13.983703
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:27:11.805885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1605.91
 ---- batch: 020 ----
mean loss: 1596.74
train mean loss: 1600.91
epoch train time: 0:00:00.358193
elapsed time: 0:00:14.342100
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:27:12.164296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1549.58
 ---- batch: 020 ----
mean loss: 1555.50
train mean loss: 1544.57
epoch train time: 0:00:00.359322
elapsed time: 0:00:14.701622
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:27:12.523799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1515.24
 ---- batch: 020 ----
mean loss: 1477.00
train mean loss: 1490.66
epoch train time: 0:00:00.382333
elapsed time: 0:00:15.084143
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:27:12.906338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1460.61
 ---- batch: 020 ----
mean loss: 1436.55
train mean loss: 1442.06
epoch train time: 0:00:00.367124
elapsed time: 0:00:15.451464
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:27:13.273658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1417.10
 ---- batch: 020 ----
mean loss: 1390.48
train mean loss: 1394.22
epoch train time: 0:00:00.366870
elapsed time: 0:00:15.818544
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:27:13.640738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1355.43
 ---- batch: 020 ----
mean loss: 1342.18
train mean loss: 1345.08
epoch train time: 0:00:00.359613
elapsed time: 0:00:16.178349
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:27:14.000527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1289.68
 ---- batch: 020 ----
mean loss: 1260.53
train mean loss: 1280.13
epoch train time: 0:00:00.354399
elapsed time: 0:00:16.532937
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:27:14.355113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1240.59
 ---- batch: 020 ----
mean loss: 1217.09
train mean loss: 1218.47
epoch train time: 0:00:00.353214
elapsed time: 0:00:16.886329
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:27:14.708511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1182.43
 ---- batch: 020 ----
mean loss: 1164.22
train mean loss: 1162.77
epoch train time: 0:00:00.351676
elapsed time: 0:00:17.238243
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:27:15.060416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1140.03
 ---- batch: 020 ----
mean loss: 1097.59
train mean loss: 1120.54
epoch train time: 0:00:00.350339
elapsed time: 0:00:17.588761
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:27:15.410942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1098.77
 ---- batch: 020 ----
mean loss: 1062.29
train mean loss: 1074.88
epoch train time: 0:00:00.351652
elapsed time: 0:00:17.940592
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:27:15.762776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1050.38
 ---- batch: 020 ----
mean loss: 1026.04
train mean loss: 1040.09
epoch train time: 0:00:00.354338
elapsed time: 0:00:18.295142
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:27:16.117317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1002.58
 ---- batch: 020 ----
mean loss: 984.87
train mean loss: 995.51
epoch train time: 0:00:00.358151
elapsed time: 0:00:18.653477
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:27:16.475655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 967.68
 ---- batch: 020 ----
mean loss: 958.15
train mean loss: 962.55
epoch train time: 0:00:00.356108
elapsed time: 0:00:19.009756
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:27:16.831935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.07
 ---- batch: 020 ----
mean loss: 921.92
train mean loss: 926.97
epoch train time: 0:00:00.366712
elapsed time: 0:00:19.376658
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:27:17.198835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.15
 ---- batch: 020 ----
mean loss: 891.15
train mean loss: 887.97
epoch train time: 0:00:00.364001
elapsed time: 0:00:19.740847
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:27:17.563049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.32
 ---- batch: 020 ----
mean loss: 851.86
train mean loss: 854.48
epoch train time: 0:00:00.358632
elapsed time: 0:00:20.099728
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:27:17.921923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.31
 ---- batch: 020 ----
mean loss: 818.26
train mean loss: 823.35
epoch train time: 0:00:00.361734
elapsed time: 0:00:20.461702
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:27:18.283894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.40
 ---- batch: 020 ----
mean loss: 781.28
train mean loss: 791.81
epoch train time: 0:00:00.364224
elapsed time: 0:00:20.826163
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:27:18.648343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.95
 ---- batch: 020 ----
mean loss: 753.56
train mean loss: 761.09
epoch train time: 0:00:00.353953
elapsed time: 0:00:21.180298
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:27:19.002478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.18
 ---- batch: 020 ----
mean loss: 718.18
train mean loss: 731.41
epoch train time: 0:00:00.366844
elapsed time: 0:00:21.547339
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:27:19.369520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.77
 ---- batch: 020 ----
mean loss: 706.81
train mean loss: 706.10
epoch train time: 0:00:00.363167
elapsed time: 0:00:21.910697
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:27:19.732891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.49
 ---- batch: 020 ----
mean loss: 674.27
train mean loss: 683.42
epoch train time: 0:00:00.358257
elapsed time: 0:00:22.269146
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:27:20.091349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.68
 ---- batch: 020 ----
mean loss: 656.26
train mean loss: 654.05
epoch train time: 0:00:00.353310
elapsed time: 0:00:22.622667
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:27:20.444844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 646.45
 ---- batch: 020 ----
mean loss: 616.18
train mean loss: 628.74
epoch train time: 0:00:00.352506
elapsed time: 0:00:22.975344
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:27:20.797523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 616.55
 ---- batch: 020 ----
mean loss: 602.73
train mean loss: 606.15
epoch train time: 0:00:00.360534
elapsed time: 0:00:23.336054
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:27:21.158228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 588.94
 ---- batch: 020 ----
mean loss: 581.23
train mean loss: 586.12
epoch train time: 0:00:00.352473
elapsed time: 0:00:23.688733
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:27:21.510928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 577.73
 ---- batch: 020 ----
mean loss: 551.77
train mean loss: 561.64
epoch train time: 0:00:00.351811
elapsed time: 0:00:24.040739
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:27:21.862941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 551.36
 ---- batch: 020 ----
mean loss: 534.09
train mean loss: 539.86
epoch train time: 0:00:00.348370
elapsed time: 0:00:24.389307
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:27:22.211485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.21
 ---- batch: 020 ----
mean loss: 515.96
train mean loss: 523.03
epoch train time: 0:00:00.344779
elapsed time: 0:00:24.734271
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:27:22.556462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.11
 ---- batch: 020 ----
mean loss: 508.42
train mean loss: 505.79
epoch train time: 0:00:00.352699
elapsed time: 0:00:25.087160
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:27:22.909339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.81
 ---- batch: 020 ----
mean loss: 483.67
train mean loss: 484.63
epoch train time: 0:00:00.341765
elapsed time: 0:00:25.429134
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:27:23.251338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.83
 ---- batch: 020 ----
mean loss: 459.11
train mean loss: 466.03
epoch train time: 0:00:00.343300
elapsed time: 0:00:25.772704
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:27:23.594894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.47
 ---- batch: 020 ----
mean loss: 444.52
train mean loss: 453.99
epoch train time: 0:00:00.357965
elapsed time: 0:00:26.130875
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:27:23.953059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.45
 ---- batch: 020 ----
mean loss: 432.27
train mean loss: 435.01
epoch train time: 0:00:00.352621
elapsed time: 0:00:26.483696
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:27:24.305875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.97
 ---- batch: 020 ----
mean loss: 419.42
train mean loss: 419.95
epoch train time: 0:00:00.355441
elapsed time: 0:00:26.839329
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:27:24.661505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.23
 ---- batch: 020 ----
mean loss: 403.67
train mean loss: 404.27
epoch train time: 0:00:00.360716
elapsed time: 0:00:27.200220
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:27:25.022403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.80
 ---- batch: 020 ----
mean loss: 378.80
train mean loss: 387.46
epoch train time: 0:00:00.353861
elapsed time: 0:00:27.554262
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:27:25.376453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.72
 ---- batch: 020 ----
mean loss: 372.67
train mean loss: 377.40
epoch train time: 0:00:00.351708
elapsed time: 0:00:27.906165
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:27:25.728340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.48
 ---- batch: 020 ----
mean loss: 363.10
train mean loss: 364.28
epoch train time: 0:00:00.353757
elapsed time: 0:00:28.260095
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:27:26.082280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.37
 ---- batch: 020 ----
mean loss: 350.56
train mean loss: 351.76
epoch train time: 0:00:00.355175
elapsed time: 0:00:28.615452
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:27:26.437643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.25
 ---- batch: 020 ----
mean loss: 337.50
train mean loss: 338.44
epoch train time: 0:00:00.356205
elapsed time: 0:00:28.971846
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:27:26.794021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.79
 ---- batch: 020 ----
mean loss: 325.90
train mean loss: 327.37
epoch train time: 0:00:00.352122
elapsed time: 0:00:29.324149
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:27:27.146328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.91
 ---- batch: 020 ----
mean loss: 320.60
train mean loss: 318.54
epoch train time: 0:00:00.360093
elapsed time: 0:00:29.684435
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:27:27.506626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.62
 ---- batch: 020 ----
mean loss: 301.83
train mean loss: 306.15
epoch train time: 0:00:00.363112
elapsed time: 0:00:30.047766
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:27:27.869953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.11
 ---- batch: 020 ----
mean loss: 296.46
train mean loss: 296.21
epoch train time: 0:00:00.383313
elapsed time: 0:00:30.431271
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:27:28.253448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.78
 ---- batch: 020 ----
mean loss: 285.65
train mean loss: 288.52
epoch train time: 0:00:00.366183
elapsed time: 0:00:30.797652
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:27:28.619863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.76
 ---- batch: 020 ----
mean loss: 283.29
train mean loss: 278.99
epoch train time: 0:00:00.378534
elapsed time: 0:00:31.176400
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:27:28.998578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.59
 ---- batch: 020 ----
mean loss: 267.54
train mean loss: 270.38
epoch train time: 0:00:00.358467
elapsed time: 0:00:31.535043
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:27:29.357229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.28
 ---- batch: 020 ----
mean loss: 259.17
train mean loss: 261.49
epoch train time: 0:00:00.354095
elapsed time: 0:00:31.889317
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:27:29.711523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.75
 ---- batch: 020 ----
mean loss: 249.87
train mean loss: 250.92
epoch train time: 0:00:00.359069
elapsed time: 0:00:32.248607
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:27:30.070786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.73
 ---- batch: 020 ----
mean loss: 245.27
train mean loss: 245.54
epoch train time: 0:00:00.362776
elapsed time: 0:00:32.611558
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:27:30.433733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.51
 ---- batch: 020 ----
mean loss: 236.14
train mean loss: 235.83
epoch train time: 0:00:00.361588
elapsed time: 0:00:32.973332
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:27:30.795531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.84
 ---- batch: 020 ----
mean loss: 227.91
train mean loss: 229.25
epoch train time: 0:00:00.362549
elapsed time: 0:00:33.336076
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:27:31.158251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.42
 ---- batch: 020 ----
mean loss: 218.99
train mean loss: 220.22
epoch train time: 0:00:00.348674
elapsed time: 0:00:33.684920
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:27:31.507092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.68
 ---- batch: 020 ----
mean loss: 216.58
train mean loss: 216.02
epoch train time: 0:00:00.371007
elapsed time: 0:00:34.056139
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:27:31.878315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.07
 ---- batch: 020 ----
mean loss: 208.00
train mean loss: 210.45
epoch train time: 0:00:00.353801
elapsed time: 0:00:34.410123
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:27:32.232300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.07
 ---- batch: 020 ----
mean loss: 204.05
train mean loss: 205.03
epoch train time: 0:00:00.354255
elapsed time: 0:00:34.764590
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:27:32.586775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.93
 ---- batch: 020 ----
mean loss: 197.62
train mean loss: 198.26
epoch train time: 0:00:00.360552
elapsed time: 0:00:35.125323
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:27:32.947497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.80
 ---- batch: 020 ----
mean loss: 192.96
train mean loss: 191.89
epoch train time: 0:00:00.357678
elapsed time: 0:00:35.483194
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:27:33.305372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.76
 ---- batch: 020 ----
mean loss: 188.64
train mean loss: 187.17
epoch train time: 0:00:00.356695
elapsed time: 0:00:35.840064
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:27:33.662248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.70
 ---- batch: 020 ----
mean loss: 183.33
train mean loss: 182.59
epoch train time: 0:00:00.359690
elapsed time: 0:00:36.199941
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:27:34.022119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.83
 ---- batch: 020 ----
mean loss: 177.41
train mean loss: 176.71
epoch train time: 0:00:00.351841
elapsed time: 0:00:36.551958
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:27:34.374146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.10
 ---- batch: 020 ----
mean loss: 174.10
train mean loss: 175.35
epoch train time: 0:00:00.360685
elapsed time: 0:00:36.912903
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:27:34.735091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.46
 ---- batch: 020 ----
mean loss: 169.56
train mean loss: 169.07
epoch train time: 0:00:00.361281
elapsed time: 0:00:37.274369
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:27:35.096546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.02
 ---- batch: 020 ----
mean loss: 157.45
train mean loss: 163.60
epoch train time: 0:00:00.349279
elapsed time: 0:00:37.623881
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:27:35.446094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.07
 ---- batch: 020 ----
mean loss: 161.68
train mean loss: 163.33
epoch train time: 0:00:00.354043
elapsed time: 0:00:37.978146
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:27:35.800327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.36
 ---- batch: 020 ----
mean loss: 157.25
train mean loss: 156.49
epoch train time: 0:00:00.356307
elapsed time: 0:00:38.334638
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:27:36.156833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.02
 ---- batch: 020 ----
mean loss: 152.30
train mean loss: 152.37
epoch train time: 0:00:00.356507
elapsed time: 0:00:38.691339
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:27:36.513517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.53
 ---- batch: 020 ----
mean loss: 148.31
train mean loss: 148.99
epoch train time: 0:00:00.385176
elapsed time: 0:00:39.076716
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:27:36.898898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.74
 ---- batch: 020 ----
mean loss: 144.45
train mean loss: 146.36
epoch train time: 0:00:00.367461
elapsed time: 0:00:39.444385
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:27:37.266579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.74
 ---- batch: 020 ----
mean loss: 143.29
train mean loss: 143.32
epoch train time: 0:00:00.360692
elapsed time: 0:00:39.805279
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:27:37.627460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.75
 ---- batch: 020 ----
mean loss: 141.96
train mean loss: 141.21
epoch train time: 0:00:00.366570
elapsed time: 0:00:40.172029
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:27:37.994223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.76
 ---- batch: 020 ----
mean loss: 134.00
train mean loss: 136.34
epoch train time: 0:00:00.352334
elapsed time: 0:00:40.524603
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:27:38.346799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.30
 ---- batch: 020 ----
mean loss: 133.63
train mean loss: 135.75
epoch train time: 0:00:00.346645
elapsed time: 0:00:40.871473
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:27:38.693669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.63
 ---- batch: 020 ----
mean loss: 129.18
train mean loss: 131.96
epoch train time: 0:00:00.368147
elapsed time: 0:00:41.239814
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:27:39.061989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.10
 ---- batch: 020 ----
mean loss: 128.57
train mean loss: 129.87
epoch train time: 0:00:00.349090
elapsed time: 0:00:41.589077
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:27:39.411264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.90
 ---- batch: 020 ----
mean loss: 128.11
train mean loss: 127.12
epoch train time: 0:00:00.348712
elapsed time: 0:00:41.937968
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:27:39.760165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.92
 ---- batch: 020 ----
mean loss: 120.21
train mean loss: 123.96
epoch train time: 0:00:00.346825
elapsed time: 0:00:42.285009
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:27:40.107187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.94
 ---- batch: 020 ----
mean loss: 126.40
train mean loss: 123.47
epoch train time: 0:00:00.359481
elapsed time: 0:00:42.644694
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:27:40.466891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.55
 ---- batch: 020 ----
mean loss: 118.48
train mean loss: 118.63
epoch train time: 0:00:00.361021
elapsed time: 0:00:43.005911
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:27:40.828085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.76
 ---- batch: 020 ----
mean loss: 118.91
train mean loss: 117.95
epoch train time: 0:00:00.354816
elapsed time: 0:00:43.360900
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:27:41.183081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.65
 ---- batch: 020 ----
mean loss: 116.22
train mean loss: 115.78
epoch train time: 0:00:00.349932
elapsed time: 0:00:43.711013
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:27:41.533221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.06
 ---- batch: 020 ----
mean loss: 113.61
train mean loss: 115.06
epoch train time: 0:00:00.350323
elapsed time: 0:00:44.061555
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:27:41.883747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.42
 ---- batch: 020 ----
mean loss: 115.21
train mean loss: 112.27
epoch train time: 0:00:00.343268
elapsed time: 0:00:44.405005
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:27:42.227177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.42
 ---- batch: 020 ----
mean loss: 110.33
train mean loss: 110.02
epoch train time: 0:00:00.347709
elapsed time: 0:00:44.752889
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:27:42.575065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.32
 ---- batch: 020 ----
mean loss: 111.81
train mean loss: 108.70
epoch train time: 0:00:00.365979
elapsed time: 0:00:45.119036
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:27:42.941208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.03
 ---- batch: 020 ----
mean loss: 109.20
train mean loss: 107.74
epoch train time: 0:00:00.346192
elapsed time: 0:00:45.465429
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:27:43.287611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.00
 ---- batch: 020 ----
mean loss: 105.82
train mean loss: 105.42
epoch train time: 0:00:00.356828
elapsed time: 0:00:45.822473
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:27:43.644630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.50
 ---- batch: 020 ----
mean loss: 107.15
train mean loss: 104.85
epoch train time: 0:00:00.355257
elapsed time: 0:00:46.177886
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:27:44.000061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.40
 ---- batch: 020 ----
mean loss: 102.45
train mean loss: 103.53
epoch train time: 0:00:00.353842
elapsed time: 0:00:46.531904
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:27:44.354092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.65
 ---- batch: 020 ----
mean loss: 101.83
train mean loss: 101.19
epoch train time: 0:00:00.350279
elapsed time: 0:00:46.882362
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:27:44.704567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.09
 ---- batch: 020 ----
mean loss: 101.08
train mean loss: 100.37
epoch train time: 0:00:00.363539
elapsed time: 0:00:47.246107
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:27:45.068284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.92
 ---- batch: 020 ----
mean loss: 94.71
train mean loss: 99.44
epoch train time: 0:00:00.345504
elapsed time: 0:00:47.591785
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:27:45.413979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.25
 ---- batch: 020 ----
mean loss: 97.96
train mean loss: 98.36
epoch train time: 0:00:00.345407
elapsed time: 0:00:47.937410
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:27:45.759593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.06
 ---- batch: 020 ----
mean loss: 99.40
train mean loss: 96.01
epoch train time: 0:00:00.349395
elapsed time: 0:00:48.286998
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:27:46.109181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.67
 ---- batch: 020 ----
mean loss: 95.15
train mean loss: 96.22
epoch train time: 0:00:00.352519
elapsed time: 0:00:48.639725
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:27:46.461906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.60
 ---- batch: 020 ----
mean loss: 95.82
train mean loss: 92.65
epoch train time: 0:00:00.370126
elapsed time: 0:00:49.010030
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:27:46.832207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.48
 ---- batch: 020 ----
mean loss: 87.97
train mean loss: 93.79
epoch train time: 0:00:00.357223
elapsed time: 0:00:49.367431
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:27:47.189606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.28
 ---- batch: 020 ----
mean loss: 92.98
train mean loss: 93.14
epoch train time: 0:00:00.356853
elapsed time: 0:00:49.724486
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:27:47.546662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.48
 ---- batch: 020 ----
mean loss: 90.97
train mean loss: 90.32
epoch train time: 0:00:00.349914
elapsed time: 0:00:50.074594
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:27:47.896766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.40
 ---- batch: 020 ----
mean loss: 91.26
train mean loss: 90.50
epoch train time: 0:00:00.347280
elapsed time: 0:00:50.422042
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:27:48.244236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.26
 ---- batch: 020 ----
mean loss: 89.94
train mean loss: 88.79
epoch train time: 0:00:00.349116
elapsed time: 0:00:50.771345
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:27:48.593518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.12
 ---- batch: 020 ----
mean loss: 90.43
train mean loss: 89.38
epoch train time: 0:00:00.371619
elapsed time: 0:00:51.143131
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:27:48.965303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.79
 ---- batch: 020 ----
mean loss: 87.11
train mean loss: 88.12
epoch train time: 0:00:00.347555
elapsed time: 0:00:51.490856
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:27:49.313031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.25
 ---- batch: 020 ----
mean loss: 83.97
train mean loss: 85.77
epoch train time: 0:00:00.347991
elapsed time: 0:00:51.839047
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:27:49.661238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.76
 ---- batch: 020 ----
mean loss: 88.76
train mean loss: 88.37
epoch train time: 0:00:00.348001
elapsed time: 0:00:52.187251
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:27:50.009425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.63
 ---- batch: 020 ----
mean loss: 84.54
train mean loss: 84.75
epoch train time: 0:00:00.343885
elapsed time: 0:00:52.531304
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:27:50.353482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.16
 ---- batch: 020 ----
mean loss: 84.05
train mean loss: 84.39
epoch train time: 0:00:00.347442
elapsed time: 0:00:52.878990
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:27:50.701148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.11
 ---- batch: 020 ----
mean loss: 82.41
train mean loss: 83.71
epoch train time: 0:00:00.364744
elapsed time: 0:00:53.243954
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:27:51.066176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.17
 ---- batch: 020 ----
mean loss: 82.91
train mean loss: 83.74
epoch train time: 0:00:00.349661
elapsed time: 0:00:53.593833
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:27:51.416005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.25
 ---- batch: 020 ----
mean loss: 84.84
train mean loss: 82.26
epoch train time: 0:00:00.350193
elapsed time: 0:00:53.944206
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:27:51.766380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.95
 ---- batch: 020 ----
mean loss: 84.42
train mean loss: 82.72
epoch train time: 0:00:00.346810
elapsed time: 0:00:54.291187
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:27:52.113377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.04
 ---- batch: 020 ----
mean loss: 80.19
train mean loss: 80.88
epoch train time: 0:00:00.348575
elapsed time: 0:00:54.639949
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:27:52.462127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.26
 ---- batch: 020 ----
mean loss: 85.16
train mean loss: 81.42
epoch train time: 0:00:00.362173
elapsed time: 0:00:55.002298
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:27:52.824476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.67
 ---- batch: 020 ----
mean loss: 82.41
train mean loss: 80.84
epoch train time: 0:00:00.345559
elapsed time: 0:00:55.348035
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:27:53.170240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.63
 ---- batch: 020 ----
mean loss: 79.52
train mean loss: 80.02
epoch train time: 0:00:00.347216
elapsed time: 0:00:55.695446
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:27:53.517616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.75
 ---- batch: 020 ----
mean loss: 81.62
train mean loss: 77.95
epoch train time: 0:00:00.346007
elapsed time: 0:00:56.041614
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:27:53.863800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.91
 ---- batch: 020 ----
mean loss: 75.35
train mean loss: 76.68
epoch train time: 0:00:00.338637
elapsed time: 0:00:56.380437
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:27:54.202615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.82
 ---- batch: 020 ----
mean loss: 78.31
train mean loss: 77.05
epoch train time: 0:00:00.342040
elapsed time: 0:00:56.722688
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:27:54.544878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.71
 ---- batch: 020 ----
mean loss: 76.70
train mean loss: 76.85
epoch train time: 0:00:00.375466
elapsed time: 0:00:57.098346
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:27:54.920525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.13
 ---- batch: 020 ----
mean loss: 75.23
train mean loss: 75.89
epoch train time: 0:00:00.347960
elapsed time: 0:00:57.446481
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:27:55.268670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.53
 ---- batch: 020 ----
mean loss: 79.04
train mean loss: 75.27
epoch train time: 0:00:00.349574
elapsed time: 0:00:57.796245
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:27:55.618424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.80
 ---- batch: 020 ----
mean loss: 73.29
train mean loss: 76.62
epoch train time: 0:00:00.348943
elapsed time: 0:00:58.145356
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:27:55.967595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.12
 ---- batch: 020 ----
mean loss: 74.65
train mean loss: 76.06
epoch train time: 0:00:00.343260
elapsed time: 0:00:58.488850
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:27:56.311062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.59
 ---- batch: 020 ----
mean loss: 76.25
train mean loss: 75.83
epoch train time: 0:00:00.338032
elapsed time: 0:00:58.827085
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:27:56.649280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.70
 ---- batch: 020 ----
mean loss: 77.87
train mean loss: 76.54
epoch train time: 0:00:00.375296
elapsed time: 0:00:59.202576
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:27:57.024757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.16
 ---- batch: 020 ----
mean loss: 74.80
train mean loss: 73.78
epoch train time: 0:00:00.355111
elapsed time: 0:00:59.557864
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:27:57.380040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.29
 ---- batch: 020 ----
mean loss: 73.01
train mean loss: 72.31
epoch train time: 0:00:00.363631
elapsed time: 0:00:59.921674
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:27:57.743852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.68
 ---- batch: 020 ----
mean loss: 70.95
train mean loss: 73.51
epoch train time: 0:00:00.352232
elapsed time: 0:01:00.274110
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:27:58.096323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.80
 ---- batch: 020 ----
mean loss: 71.35
train mean loss: 73.13
epoch train time: 0:00:00.356416
elapsed time: 0:01:00.630811
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:27:58.452970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.06
 ---- batch: 020 ----
mean loss: 69.09
train mean loss: 70.89
epoch train time: 0:00:00.357580
elapsed time: 0:01:00.988598
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:27:58.810777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.73
 ---- batch: 020 ----
mean loss: 72.94
train mean loss: 72.72
epoch train time: 0:00:00.352639
elapsed time: 0:01:01.341473
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:27:59.163653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.32
 ---- batch: 020 ----
mean loss: 71.88
train mean loss: 72.52
epoch train time: 0:00:00.356521
elapsed time: 0:01:01.698179
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:27:59.520370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.42
 ---- batch: 020 ----
mean loss: 72.54
train mean loss: 70.67
epoch train time: 0:00:00.345239
elapsed time: 0:01:02.043602
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:27:59.865812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.09
 ---- batch: 020 ----
mean loss: 70.44
train mean loss: 70.88
epoch train time: 0:00:00.337575
elapsed time: 0:01:02.381378
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:28:00.203566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.14
 ---- batch: 020 ----
mean loss: 67.92
train mean loss: 69.99
epoch train time: 0:00:00.343123
elapsed time: 0:01:02.724689
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:28:00.546857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.33
 ---- batch: 020 ----
mean loss: 70.24
train mean loss: 69.98
epoch train time: 0:00:00.371218
elapsed time: 0:01:03.096085
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:28:00.918265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.91
 ---- batch: 020 ----
mean loss: 71.05
train mean loss: 69.82
epoch train time: 0:00:00.351317
elapsed time: 0:01:03.447590
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:28:01.269780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.75
 ---- batch: 020 ----
mean loss: 66.97
train mean loss: 69.30
epoch train time: 0:00:00.349104
elapsed time: 0:01:03.796888
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:28:01.619071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.57
 ---- batch: 020 ----
mean loss: 69.69
train mean loss: 70.25
epoch train time: 0:00:00.353702
elapsed time: 0:01:04.150782
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:28:01.972980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.91
 ---- batch: 020 ----
mean loss: 69.66
train mean loss: 70.00
epoch train time: 0:00:00.347024
elapsed time: 0:01:04.498030
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:28:02.320219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.49
 ---- batch: 020 ----
mean loss: 69.36
train mean loss: 68.71
epoch train time: 0:00:00.351844
elapsed time: 0:01:04.850054
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:28:02.672233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.07
 ---- batch: 020 ----
mean loss: 69.75
train mean loss: 68.07
epoch train time: 0:00:00.351440
elapsed time: 0:01:05.201696
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:28:03.023871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.60
 ---- batch: 020 ----
mean loss: 69.61
train mean loss: 66.68
epoch train time: 0:00:00.342972
elapsed time: 0:01:05.544876
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:28:03.367048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.64
 ---- batch: 020 ----
mean loss: 65.78
train mean loss: 67.93
epoch train time: 0:00:00.338460
elapsed time: 0:01:05.883500
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:28:03.705676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.88
 ---- batch: 020 ----
mean loss: 69.00
train mean loss: 68.70
epoch train time: 0:00:00.341783
elapsed time: 0:01:06.225453
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:28:04.047641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.54
 ---- batch: 020 ----
mean loss: 67.92
train mean loss: 68.53
epoch train time: 0:00:00.340265
elapsed time: 0:01:06.565898
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:28:04.388070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.24
 ---- batch: 020 ----
mean loss: 64.04
train mean loss: 67.34
epoch train time: 0:00:00.355126
elapsed time: 0:01:06.921193
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:28:04.743370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.40
 ---- batch: 020 ----
mean loss: 64.89
train mean loss: 66.31
epoch train time: 0:00:00.345241
elapsed time: 0:01:07.266633
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:28:05.088870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.49
 ---- batch: 020 ----
mean loss: 66.31
train mean loss: 66.41
epoch train time: 0:00:00.342489
elapsed time: 0:01:07.609349
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:28:05.431523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.17
 ---- batch: 020 ----
mean loss: 63.63
train mean loss: 65.73
epoch train time: 0:00:00.344957
elapsed time: 0:01:07.954476
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:28:05.776653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.16
 ---- batch: 020 ----
mean loss: 67.17
train mean loss: 66.71
epoch train time: 0:00:00.356159
elapsed time: 0:01:08.310847
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:28:06.133028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.82
 ---- batch: 020 ----
mean loss: 65.15
train mean loss: 64.32
epoch train time: 0:00:00.350989
elapsed time: 0:01:08.662017
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:28:06.484198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.97
 ---- batch: 020 ----
mean loss: 64.90
train mean loss: 64.52
epoch train time: 0:00:00.351116
elapsed time: 0:01:09.013315
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:28:06.835501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.59
 ---- batch: 020 ----
mean loss: 62.73
train mean loss: 63.20
epoch train time: 0:00:00.338902
elapsed time: 0:01:09.352486
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:28:07.174657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.08
 ---- batch: 020 ----
mean loss: 65.75
train mean loss: 63.56
epoch train time: 0:00:00.341165
elapsed time: 0:01:09.693814
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:28:07.516023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.40
 ---- batch: 020 ----
mean loss: 65.72
train mean loss: 64.54
epoch train time: 0:00:00.339256
elapsed time: 0:01:10.033274
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:28:07.855450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.61
 ---- batch: 020 ----
mean loss: 62.85
train mean loss: 63.86
epoch train time: 0:00:00.333965
elapsed time: 0:01:10.367401
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:28:08.189572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.87
 ---- batch: 020 ----
mean loss: 64.39
train mean loss: 63.23
epoch train time: 0:00:00.337586
elapsed time: 0:01:10.705198
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:28:08.527368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.74
 ---- batch: 020 ----
mean loss: 64.01
train mean loss: 63.05
epoch train time: 0:00:00.359419
elapsed time: 0:01:11.064820
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:28:08.887000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.43
 ---- batch: 020 ----
mean loss: 61.75
train mean loss: 63.01
epoch train time: 0:00:00.354475
elapsed time: 0:01:11.419472
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:28:09.241648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.57
 ---- batch: 020 ----
mean loss: 64.55
train mean loss: 63.29
epoch train time: 0:00:00.355928
elapsed time: 0:01:11.775581
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:28:09.597772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.02
 ---- batch: 020 ----
mean loss: 62.36
train mean loss: 61.77
epoch train time: 0:00:00.352220
elapsed time: 0:01:12.127987
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:28:09.950162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.44
 ---- batch: 020 ----
mean loss: 63.07
train mean loss: 62.54
epoch train time: 0:00:00.354782
elapsed time: 0:01:12.482946
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:28:10.305122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.39
 ---- batch: 020 ----
mean loss: 60.65
train mean loss: 61.52
epoch train time: 0:00:00.354920
elapsed time: 0:01:12.838084
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:28:10.660272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.53
 ---- batch: 020 ----
mean loss: 61.03
train mean loss: 61.95
epoch train time: 0:00:00.356876
elapsed time: 0:01:13.195140
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:28:11.017356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.05
 ---- batch: 020 ----
mean loss: 61.47
train mean loss: 61.16
epoch train time: 0:00:00.340155
elapsed time: 0:01:13.535533
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:28:11.357721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.85
 ---- batch: 020 ----
mean loss: 62.63
train mean loss: 60.94
epoch train time: 0:00:00.350670
elapsed time: 0:01:13.886415
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:28:11.708596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.53
 ---- batch: 020 ----
mean loss: 61.60
train mean loss: 60.27
epoch train time: 0:00:00.366314
elapsed time: 0:01:14.252911
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:28:12.075091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.76
 ---- batch: 020 ----
mean loss: 61.28
train mean loss: 61.64
epoch train time: 0:00:00.348482
elapsed time: 0:01:14.601603
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:28:12.423793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.34
 ---- batch: 020 ----
mean loss: 60.93
train mean loss: 60.39
epoch train time: 0:00:00.351661
elapsed time: 0:01:14.953444
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:28:12.775618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.35
 ---- batch: 020 ----
mean loss: 61.42
train mean loss: 60.08
epoch train time: 0:00:00.346919
elapsed time: 0:01:15.300555
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:28:13.122758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.17
 ---- batch: 020 ----
mean loss: 59.39
train mean loss: 60.43
epoch train time: 0:00:00.348937
elapsed time: 0:01:15.649703
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:28:13.471882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.19
 ---- batch: 020 ----
mean loss: 57.81
train mean loss: 59.94
epoch train time: 0:00:00.349619
elapsed time: 0:01:15.999494
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:28:13.821671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.51
 ---- batch: 020 ----
mean loss: 60.00
train mean loss: 59.92
epoch train time: 0:00:00.338697
elapsed time: 0:01:16.338369
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:28:14.160546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.22
 ---- batch: 020 ----
mean loss: 60.89
train mean loss: 59.08
epoch train time: 0:00:00.343445
elapsed time: 0:01:16.682005
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:28:14.504212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.86
 ---- batch: 020 ----
mean loss: 59.87
train mean loss: 59.44
epoch train time: 0:00:00.361712
elapsed time: 0:01:17.043967
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:28:14.866143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.41
 ---- batch: 020 ----
mean loss: 60.59
train mean loss: 58.53
epoch train time: 0:00:00.338746
elapsed time: 0:01:17.382903
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:28:15.205084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.03
 ---- batch: 020 ----
mean loss: 60.60
train mean loss: 58.02
epoch train time: 0:00:00.345809
elapsed time: 0:01:17.728973
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:28:15.551152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.10
 ---- batch: 020 ----
mean loss: 57.02
train mean loss: 57.19
epoch train time: 0:00:00.360667
elapsed time: 0:01:18.089814
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:28:15.911992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.77
 ---- batch: 020 ----
mean loss: 60.50
train mean loss: 58.74
epoch train time: 0:00:00.347181
elapsed time: 0:01:18.437164
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:28:16.259340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.38
 ---- batch: 020 ----
mean loss: 58.69
train mean loss: 57.80
epoch train time: 0:00:00.346353
elapsed time: 0:01:18.783721
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:28:16.605908
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.51
 ---- batch: 020 ----
mean loss: 56.69
train mean loss: 57.15
epoch train time: 0:00:00.349755
elapsed time: 0:01:19.133690
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:28:16.955845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.98
 ---- batch: 020 ----
mean loss: 60.41
train mean loss: 57.01
epoch train time: 0:00:00.353655
elapsed time: 0:01:19.487498
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:28:17.309713
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.22
 ---- batch: 020 ----
mean loss: 57.65
train mean loss: 57.00
epoch train time: 0:00:00.352302
elapsed time: 0:01:19.840014
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:28:17.662194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.32
 ---- batch: 020 ----
mean loss: 56.74
train mean loss: 56.00
epoch train time: 0:00:00.363875
elapsed time: 0:01:20.204074
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:28:18.026273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.55
 ---- batch: 020 ----
mean loss: 55.96
train mean loss: 55.87
epoch train time: 0:00:00.359772
elapsed time: 0:01:20.564044
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:28:18.386248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.14
 ---- batch: 020 ----
mean loss: 57.26
train mean loss: 56.00
epoch train time: 0:00:00.344517
elapsed time: 0:01:20.908764
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:28:18.730943
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.88
 ---- batch: 020 ----
mean loss: 54.09
train mean loss: 55.73
epoch train time: 0:00:00.356715
elapsed time: 0:01:21.265658
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:28:19.087827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.61
 ---- batch: 020 ----
mean loss: 56.85
train mean loss: 57.25
epoch train time: 0:00:00.330145
elapsed time: 0:01:21.595960
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:28:19.418131
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.23
 ---- batch: 020 ----
mean loss: 55.51
train mean loss: 56.52
epoch train time: 0:00:00.349261
elapsed time: 0:01:21.945386
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:28:19.767560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.64
 ---- batch: 020 ----
mean loss: 57.53
train mean loss: 57.03
epoch train time: 0:00:00.343599
elapsed time: 0:01:22.289167
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:28:20.111347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.65
 ---- batch: 020 ----
mean loss: 55.49
train mean loss: 56.60
epoch train time: 0:00:00.344379
elapsed time: 0:01:22.633721
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:28:20.455894
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.76
 ---- batch: 020 ----
mean loss: 55.92
train mean loss: 56.23
epoch train time: 0:00:00.355465
elapsed time: 0:01:22.989368
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:28:20.811541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.22
 ---- batch: 020 ----
mean loss: 57.42
train mean loss: 55.74
epoch train time: 0:00:00.337628
elapsed time: 0:01:23.327159
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:28:21.149331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.49
 ---- batch: 020 ----
mean loss: 57.11
train mean loss: 56.10
epoch train time: 0:00:00.339775
elapsed time: 0:01:23.667098
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:28:21.489275
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.18
 ---- batch: 020 ----
mean loss: 55.39
train mean loss: 56.06
epoch train time: 0:00:00.347094
elapsed time: 0:01:24.014407
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:28:21.836602
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.91
 ---- batch: 020 ----
mean loss: 58.17
train mean loss: 56.28
epoch train time: 0:00:00.345996
elapsed time: 0:01:24.360592
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:28:22.182766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.24
 ---- batch: 020 ----
mean loss: 55.29
train mean loss: 55.68
epoch train time: 0:00:00.352398
elapsed time: 0:01:24.713233
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:28:22.535407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.69
 ---- batch: 020 ----
mean loss: 56.99
train mean loss: 57.19
epoch train time: 0:00:00.347510
elapsed time: 0:01:25.060924
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:28:22.883134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.42
 ---- batch: 020 ----
mean loss: 57.06
train mean loss: 55.56
epoch train time: 0:00:00.351160
elapsed time: 0:01:25.412357
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:28:23.234533
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.88
 ---- batch: 020 ----
mean loss: 54.18
train mean loss: 55.88
epoch train time: 0:00:00.352561
elapsed time: 0:01:25.765094
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:28:23.587272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.84
 ---- batch: 020 ----
mean loss: 58.80
train mean loss: 56.71
epoch train time: 0:00:00.370057
elapsed time: 0:01:26.135332
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:28:23.957529
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.18
 ---- batch: 020 ----
mean loss: 56.62
train mean loss: 56.36
epoch train time: 0:00:00.348737
elapsed time: 0:01:26.484267
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:28:24.306444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.37
 ---- batch: 020 ----
mean loss: 55.97
train mean loss: 55.78
epoch train time: 0:00:00.360648
elapsed time: 0:01:26.845114
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:28:24.667299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.85
 ---- batch: 020 ----
mean loss: 56.40
train mean loss: 56.94
epoch train time: 0:00:00.370107
elapsed time: 0:01:27.215395
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:28:25.037566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.56
 ---- batch: 020 ----
mean loss: 57.43
train mean loss: 56.30
epoch train time: 0:00:00.340492
elapsed time: 0:01:27.556085
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:28:25.378262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.83
 ---- batch: 020 ----
mean loss: 57.74
train mean loss: 56.21
epoch train time: 0:00:00.357149
elapsed time: 0:01:27.913419
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:28:25.735604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.08
 ---- batch: 020 ----
mean loss: 55.48
train mean loss: 56.57
epoch train time: 0:00:00.366142
elapsed time: 0:01:28.279762
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:28:26.101936
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.76
 ---- batch: 020 ----
mean loss: 57.06
train mean loss: 55.70
epoch train time: 0:00:00.347034
elapsed time: 0:01:28.627000
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:28:26.449194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.32
 ---- batch: 020 ----
mean loss: 58.73
train mean loss: 56.22
epoch train time: 0:00:00.344798
elapsed time: 0:01:28.971989
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:28:26.794161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.17
 ---- batch: 020 ----
mean loss: 55.28
train mean loss: 54.78
epoch train time: 0:00:00.345186
elapsed time: 0:01:29.317348
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:28:27.139525
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.58
 ---- batch: 020 ----
mean loss: 57.86
train mean loss: 55.72
epoch train time: 0:00:00.339527
elapsed time: 0:01:29.657047
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:28:27.479224
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.06
 ---- batch: 020 ----
mean loss: 55.78
train mean loss: 56.53
epoch train time: 0:00:00.338455
elapsed time: 0:01:29.995705
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:28:27.817887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.80
 ---- batch: 020 ----
mean loss: 54.76
train mean loss: 56.31
epoch train time: 0:00:00.360328
elapsed time: 0:01:30.356268
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:28:28.178425
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.78
 ---- batch: 020 ----
mean loss: 57.19
train mean loss: 55.72
epoch train time: 0:00:00.353141
elapsed time: 0:01:30.709573
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:28:28.531748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.70
 ---- batch: 020 ----
mean loss: 55.35
train mean loss: 55.41
epoch train time: 0:00:00.358577
elapsed time: 0:01:31.068321
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:28:28.890494
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.25
 ---- batch: 020 ----
mean loss: 56.75
train mean loss: 56.18
epoch train time: 0:00:00.339605
elapsed time: 0:01:31.408114
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:28:29.230322
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.70
 ---- batch: 020 ----
mean loss: 55.45
train mean loss: 55.65
epoch train time: 0:00:00.340591
elapsed time: 0:01:31.748903
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:28:29.571133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.12
 ---- batch: 020 ----
mean loss: 53.91
train mean loss: 55.86
epoch train time: 0:00:00.348123
elapsed time: 0:01:32.097253
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:28:29.919427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.95
 ---- batch: 020 ----
mean loss: 56.84
train mean loss: 55.29
epoch train time: 0:00:00.345955
elapsed time: 0:01:32.443416
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:28:30.265598
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.77
 ---- batch: 020 ----
mean loss: 54.78
train mean loss: 55.98
epoch train time: 0:00:00.356500
elapsed time: 0:01:32.800102
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:28:30.622280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.24
 ---- batch: 020 ----
mean loss: 57.02
train mean loss: 55.76
epoch train time: 0:00:00.350378
elapsed time: 0:01:33.150655
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:28:30.972852
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.86
 ---- batch: 020 ----
mean loss: 57.48
train mean loss: 56.31
epoch train time: 0:00:00.347167
elapsed time: 0:01:33.498038
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:28:31.320246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.95
 ---- batch: 020 ----
mean loss: 56.56
train mean loss: 55.59
epoch train time: 0:00:00.344465
elapsed time: 0:01:33.842708
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:28:31.664887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.61
 ---- batch: 020 ----
mean loss: 57.61
train mean loss: 55.56
epoch train time: 0:00:00.368804
elapsed time: 0:01:34.211703
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:28:32.033881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.12
 ---- batch: 020 ----
mean loss: 58.33
train mean loss: 56.26
epoch train time: 0:00:00.336803
elapsed time: 0:01:34.548675
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:28:32.370854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.87
 ---- batch: 020 ----
mean loss: 55.11
train mean loss: 56.91
epoch train time: 0:00:00.350201
elapsed time: 0:01:34.899074
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:28:32.721261
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.22
 ---- batch: 020 ----
mean loss: 54.31
train mean loss: 55.31
epoch train time: 0:00:00.354834
elapsed time: 0:01:35.254089
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:28:33.076277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.30
 ---- batch: 020 ----
mean loss: 54.61
train mean loss: 55.56
epoch train time: 0:00:00.349698
elapsed time: 0:01:35.603974
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:28:33.426155
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 53.94
 ---- batch: 020 ----
mean loss: 55.40
train mean loss: 54.57
epoch train time: 0:00:00.348356
elapsed time: 0:01:35.960379
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_3/checkpoint.pth.tar
**** end time: 2019-10-01 14:28:33.782517 ****
