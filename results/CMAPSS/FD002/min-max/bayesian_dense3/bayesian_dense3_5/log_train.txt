Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_5', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 20593
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 20:01:25.168098 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 20:01:25.177200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4102.34
 ---- batch: 020 ----
mean loss: 3768.46
 ---- batch: 030 ----
mean loss: 3550.90
 ---- batch: 040 ----
mean loss: 3260.95
 ---- batch: 050 ----
mean loss: 2979.03
 ---- batch: 060 ----
mean loss: 2878.05
 ---- batch: 070 ----
mean loss: 2683.32
 ---- batch: 080 ----
mean loss: 2617.48
 ---- batch: 090 ----
mean loss: 2518.55
train mean loss: 3107.57
epoch train time: 0:00:34.980374
elapsed time: 0:00:34.996620
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 20:02:00.164759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2347.53
 ---- batch: 020 ----
mean loss: 2351.08
 ---- batch: 030 ----
mean loss: 2275.24
 ---- batch: 040 ----
mean loss: 2230.62
 ---- batch: 050 ----
mean loss: 2132.19
 ---- batch: 060 ----
mean loss: 2122.68
 ---- batch: 070 ----
mean loss: 2079.02
 ---- batch: 080 ----
mean loss: 2023.49
 ---- batch: 090 ----
mean loss: 2014.10
train mean loss: 2162.97
epoch train time: 0:00:01.650176
elapsed time: 0:00:36.647196
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 20:02:01.815583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1931.30
 ---- batch: 020 ----
mean loss: 1898.01
 ---- batch: 030 ----
mean loss: 1895.22
 ---- batch: 040 ----
mean loss: 1876.35
 ---- batch: 050 ----
mean loss: 1852.95
 ---- batch: 060 ----
mean loss: 1828.12
 ---- batch: 070 ----
mean loss: 1826.06
 ---- batch: 080 ----
mean loss: 1773.48
 ---- batch: 090 ----
mean loss: 1738.94
train mean loss: 1841.52
epoch train time: 0:00:01.644536
elapsed time: 0:00:38.292356
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 20:02:03.460750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1719.11
 ---- batch: 020 ----
mean loss: 1672.89
 ---- batch: 030 ----
mean loss: 1646.54
 ---- batch: 040 ----
mean loss: 1666.18
 ---- batch: 050 ----
mean loss: 1632.70
 ---- batch: 060 ----
mean loss: 1632.96
 ---- batch: 070 ----
mean loss: 1591.15
 ---- batch: 080 ----
mean loss: 1575.38
 ---- batch: 090 ----
mean loss: 1568.42
train mean loss: 1628.83
epoch train time: 0:00:01.673663
elapsed time: 0:00:39.966625
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 20:02:05.135066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1520.60
 ---- batch: 020 ----
mean loss: 1507.16
 ---- batch: 030 ----
mean loss: 1481.96
 ---- batch: 040 ----
mean loss: 1472.61
 ---- batch: 050 ----
mean loss: 1471.13
 ---- batch: 060 ----
mean loss: 1435.35
 ---- batch: 070 ----
mean loss: 1440.18
 ---- batch: 080 ----
mean loss: 1442.64
 ---- batch: 090 ----
mean loss: 1401.12
train mean loss: 1458.13
epoch train time: 0:00:01.646301
elapsed time: 0:00:41.613653
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 20:02:06.782053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1362.67
 ---- batch: 020 ----
mean loss: 1348.29
 ---- batch: 030 ----
mean loss: 1360.22
 ---- batch: 040 ----
mean loss: 1331.46
 ---- batch: 050 ----
mean loss: 1356.24
 ---- batch: 060 ----
mean loss: 1290.86
 ---- batch: 070 ----
mean loss: 1316.60
 ---- batch: 080 ----
mean loss: 1320.93
 ---- batch: 090 ----
mean loss: 1269.79
train mean loss: 1324.97
epoch train time: 0:00:01.659275
elapsed time: 0:00:43.273590
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 20:02:08.442051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1258.29
 ---- batch: 020 ----
mean loss: 1260.59
 ---- batch: 030 ----
mean loss: 1211.99
 ---- batch: 040 ----
mean loss: 1241.82
 ---- batch: 050 ----
mean loss: 1226.62
 ---- batch: 060 ----
mean loss: 1218.90
 ---- batch: 070 ----
mean loss: 1196.09
 ---- batch: 080 ----
mean loss: 1192.71
 ---- batch: 090 ----
mean loss: 1186.17
train mean loss: 1217.52
epoch train time: 0:00:01.647161
elapsed time: 0:00:44.921588
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 20:02:10.090001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1150.39
 ---- batch: 020 ----
mean loss: 1171.40
 ---- batch: 030 ----
mean loss: 1180.84
 ---- batch: 040 ----
mean loss: 1117.67
 ---- batch: 050 ----
mean loss: 1151.46
 ---- batch: 060 ----
mean loss: 1136.22
 ---- batch: 070 ----
mean loss: 1112.63
 ---- batch: 080 ----
mean loss: 1124.50
 ---- batch: 090 ----
mean loss: 1092.22
train mean loss: 1133.72
epoch train time: 0:00:01.637122
elapsed time: 0:00:46.559339
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 20:02:11.727727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1097.00
 ---- batch: 020 ----
mean loss: 1073.62
 ---- batch: 030 ----
mean loss: 1075.06
 ---- batch: 040 ----
mean loss: 1077.81
 ---- batch: 050 ----
mean loss: 1081.75
 ---- batch: 060 ----
mean loss: 1050.60
 ---- batch: 070 ----
mean loss: 1070.21
 ---- batch: 080 ----
mean loss: 1035.78
 ---- batch: 090 ----
mean loss: 1061.13
train mean loss: 1068.54
epoch train time: 0:00:01.668136
elapsed time: 0:00:48.228091
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 20:02:13.396510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1043.82
 ---- batch: 020 ----
mean loss: 1027.28
 ---- batch: 030 ----
mean loss: 1021.97
 ---- batch: 040 ----
mean loss: 999.05
 ---- batch: 050 ----
mean loss: 1022.73
 ---- batch: 060 ----
mean loss: 1018.73
 ---- batch: 070 ----
mean loss: 1025.68
 ---- batch: 080 ----
mean loss: 993.41
 ---- batch: 090 ----
mean loss: 1003.07
train mean loss: 1015.12
epoch train time: 0:00:01.687418
elapsed time: 0:00:49.916139
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 20:02:15.084547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 984.97
 ---- batch: 020 ----
mean loss: 995.63
 ---- batch: 030 ----
mean loss: 990.21
 ---- batch: 040 ----
mean loss: 975.28
 ---- batch: 050 ----
mean loss: 979.27
 ---- batch: 060 ----
mean loss: 984.18
 ---- batch: 070 ----
mean loss: 973.05
 ---- batch: 080 ----
mean loss: 955.26
 ---- batch: 090 ----
mean loss: 975.14
train mean loss: 977.38
epoch train time: 0:00:01.654194
elapsed time: 0:00:51.570988
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 20:02:16.739415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.91
 ---- batch: 020 ----
mean loss: 958.54
 ---- batch: 030 ----
mean loss: 964.83
 ---- batch: 040 ----
mean loss: 964.95
 ---- batch: 050 ----
mean loss: 958.40
 ---- batch: 060 ----
mean loss: 953.34
 ---- batch: 070 ----
mean loss: 958.27
 ---- batch: 080 ----
mean loss: 934.19
 ---- batch: 090 ----
mean loss: 925.76
train mean loss: 948.55
epoch train time: 0:00:01.645642
elapsed time: 0:00:53.217302
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 20:02:18.385635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.66
 ---- batch: 020 ----
mean loss: 925.19
 ---- batch: 030 ----
mean loss: 935.56
 ---- batch: 040 ----
mean loss: 916.34
 ---- batch: 050 ----
mean loss: 943.46
 ---- batch: 060 ----
mean loss: 926.57
 ---- batch: 070 ----
mean loss: 909.57
 ---- batch: 080 ----
mean loss: 920.61
 ---- batch: 090 ----
mean loss: 928.16
train mean loss: 927.01
epoch train time: 0:00:01.662064
elapsed time: 0:00:54.879890
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 20:02:20.048319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.93
 ---- batch: 020 ----
mean loss: 913.40
 ---- batch: 030 ----
mean loss: 916.92
 ---- batch: 040 ----
mean loss: 893.18
 ---- batch: 050 ----
mean loss: 916.53
 ---- batch: 060 ----
mean loss: 910.76
 ---- batch: 070 ----
mean loss: 928.95
 ---- batch: 080 ----
mean loss: 917.79
 ---- batch: 090 ----
mean loss: 911.34
train mean loss: 913.73
epoch train time: 0:00:01.653976
elapsed time: 0:00:56.534581
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 20:02:21.702974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.24
 ---- batch: 020 ----
mean loss: 911.86
 ---- batch: 030 ----
mean loss: 903.82
 ---- batch: 040 ----
mean loss: 895.52
 ---- batch: 050 ----
mean loss: 895.16
 ---- batch: 060 ----
mean loss: 886.49
 ---- batch: 070 ----
mean loss: 896.41
 ---- batch: 080 ----
mean loss: 912.42
 ---- batch: 090 ----
mean loss: 905.61
train mean loss: 903.28
epoch train time: 0:00:01.655440
elapsed time: 0:00:58.190684
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 20:02:23.359090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.98
 ---- batch: 020 ----
mean loss: 891.68
 ---- batch: 030 ----
mean loss: 899.12
 ---- batch: 040 ----
mean loss: 908.70
 ---- batch: 050 ----
mean loss: 901.26
 ---- batch: 060 ----
mean loss: 887.45
 ---- batch: 070 ----
mean loss: 873.45
 ---- batch: 080 ----
mean loss: 899.37
 ---- batch: 090 ----
mean loss: 898.20
train mean loss: 896.06
epoch train time: 0:00:01.667206
elapsed time: 0:00:59.858471
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 20:02:25.026878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.44
 ---- batch: 020 ----
mean loss: 875.09
 ---- batch: 030 ----
mean loss: 884.66
 ---- batch: 040 ----
mean loss: 898.10
 ---- batch: 050 ----
mean loss: 873.91
 ---- batch: 060 ----
mean loss: 898.73
 ---- batch: 070 ----
mean loss: 904.77
 ---- batch: 080 ----
mean loss: 912.36
 ---- batch: 090 ----
mean loss: 880.54
train mean loss: 891.94
epoch train time: 0:00:01.632495
elapsed time: 0:01:01.491650
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 20:02:26.660164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.65
 ---- batch: 020 ----
mean loss: 882.43
 ---- batch: 030 ----
mean loss: 910.06
 ---- batch: 040 ----
mean loss: 896.70
 ---- batch: 050 ----
mean loss: 878.05
 ---- batch: 060 ----
mean loss: 878.43
 ---- batch: 070 ----
mean loss: 883.11
 ---- batch: 080 ----
mean loss: 883.47
 ---- batch: 090 ----
mean loss: 881.77
train mean loss: 888.79
epoch train time: 0:00:01.689820
elapsed time: 0:01:03.182262
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 20:02:28.350683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.28
 ---- batch: 020 ----
mean loss: 893.16
 ---- batch: 030 ----
mean loss: 874.15
 ---- batch: 040 ----
mean loss: 892.74
 ---- batch: 050 ----
mean loss: 888.74
 ---- batch: 060 ----
mean loss: 882.55
 ---- batch: 070 ----
mean loss: 869.17
 ---- batch: 080 ----
mean loss: 902.60
 ---- batch: 090 ----
mean loss: 887.62
train mean loss: 885.86
epoch train time: 0:00:01.649233
elapsed time: 0:01:04.832215
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 20:02:30.000627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.66
 ---- batch: 020 ----
mean loss: 896.78
 ---- batch: 030 ----
mean loss: 886.18
 ---- batch: 040 ----
mean loss: 886.98
 ---- batch: 050 ----
mean loss: 872.19
 ---- batch: 060 ----
mean loss: 882.50
 ---- batch: 070 ----
mean loss: 894.82
 ---- batch: 080 ----
mean loss: 872.50
 ---- batch: 090 ----
mean loss: 879.37
train mean loss: 883.65
epoch train time: 0:00:01.667070
elapsed time: 0:01:06.500019
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 20:02:31.668440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.82
 ---- batch: 020 ----
mean loss: 877.74
 ---- batch: 030 ----
mean loss: 873.61
 ---- batch: 040 ----
mean loss: 882.05
 ---- batch: 050 ----
mean loss: 900.70
 ---- batch: 060 ----
mean loss: 880.88
 ---- batch: 070 ----
mean loss: 867.61
 ---- batch: 080 ----
mean loss: 897.65
 ---- batch: 090 ----
mean loss: 884.68
train mean loss: 882.66
epoch train time: 0:00:01.673082
elapsed time: 0:01:08.173857
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 20:02:33.342319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.38
 ---- batch: 020 ----
mean loss: 895.05
 ---- batch: 030 ----
mean loss: 873.80
 ---- batch: 040 ----
mean loss: 868.12
 ---- batch: 050 ----
mean loss: 875.78
 ---- batch: 060 ----
mean loss: 897.67
 ---- batch: 070 ----
mean loss: 891.55
 ---- batch: 080 ----
mean loss: 878.32
 ---- batch: 090 ----
mean loss: 881.95
train mean loss: 883.33
epoch train time: 0:00:01.673003
elapsed time: 0:01:09.847628
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 20:02:35.015866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.42
 ---- batch: 020 ----
mean loss: 877.70
 ---- batch: 030 ----
mean loss: 866.09
 ---- batch: 040 ----
mean loss: 874.78
 ---- batch: 050 ----
mean loss: 881.99
 ---- batch: 060 ----
mean loss: 882.46
 ---- batch: 070 ----
mean loss: 882.21
 ---- batch: 080 ----
mean loss: 882.16
 ---- batch: 090 ----
mean loss: 889.89
train mean loss: 880.36
epoch train time: 0:00:01.683814
elapsed time: 0:01:11.532044
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 20:02:36.700520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.41
 ---- batch: 020 ----
mean loss: 877.59
 ---- batch: 030 ----
mean loss: 883.34
 ---- batch: 040 ----
mean loss: 863.36
 ---- batch: 050 ----
mean loss: 877.37
 ---- batch: 060 ----
mean loss: 871.51
 ---- batch: 070 ----
mean loss: 887.89
 ---- batch: 080 ----
mean loss: 888.16
 ---- batch: 090 ----
mean loss: 880.27
train mean loss: 882.01
epoch train time: 0:00:01.686987
elapsed time: 0:01:13.219741
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 20:02:38.388137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.32
 ---- batch: 020 ----
mean loss: 896.99
 ---- batch: 030 ----
mean loss: 889.78
 ---- batch: 040 ----
mean loss: 884.36
 ---- batch: 050 ----
mean loss: 886.46
 ---- batch: 060 ----
mean loss: 880.37
 ---- batch: 070 ----
mean loss: 881.52
 ---- batch: 080 ----
mean loss: 868.32
 ---- batch: 090 ----
mean loss: 884.61
train mean loss: 880.17
epoch train time: 0:00:01.671758
elapsed time: 0:01:14.892132
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 20:02:40.060536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.77
 ---- batch: 020 ----
mean loss: 875.17
 ---- batch: 030 ----
mean loss: 868.14
 ---- batch: 040 ----
mean loss: 870.73
 ---- batch: 050 ----
mean loss: 867.21
 ---- batch: 060 ----
mean loss: 909.32
 ---- batch: 070 ----
mean loss: 888.02
 ---- batch: 080 ----
mean loss: 882.18
 ---- batch: 090 ----
mean loss: 874.91
train mean loss: 879.82
epoch train time: 0:00:01.697192
elapsed time: 0:01:16.589995
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 20:02:41.758400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.43
 ---- batch: 020 ----
mean loss: 877.95
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 875.63
 ---- batch: 050 ----
mean loss: 870.46
 ---- batch: 060 ----
mean loss: 876.18
 ---- batch: 070 ----
mean loss: 888.19
 ---- batch: 080 ----
mean loss: 890.88
 ---- batch: 090 ----
mean loss: 891.83
train mean loss: 879.61
epoch train time: 0:00:01.663951
elapsed time: 0:01:18.254598
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 20:02:43.423000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.39
 ---- batch: 020 ----
mean loss: 871.48
 ---- batch: 030 ----
mean loss: 883.14
 ---- batch: 040 ----
mean loss: 889.41
 ---- batch: 050 ----
mean loss: 875.97
 ---- batch: 060 ----
mean loss: 873.72
 ---- batch: 070 ----
mean loss: 865.92
 ---- batch: 080 ----
mean loss: 896.88
 ---- batch: 090 ----
mean loss: 867.89
train mean loss: 879.08
epoch train time: 0:00:01.682568
elapsed time: 0:01:19.937856
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 20:02:45.106297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.52
 ---- batch: 020 ----
mean loss: 880.74
 ---- batch: 030 ----
mean loss: 865.21
 ---- batch: 040 ----
mean loss: 880.78
 ---- batch: 050 ----
mean loss: 886.19
 ---- batch: 060 ----
mean loss: 886.99
 ---- batch: 070 ----
mean loss: 894.60
 ---- batch: 080 ----
mean loss: 866.23
 ---- batch: 090 ----
mean loss: 863.63
train mean loss: 879.87
epoch train time: 0:00:01.667431
elapsed time: 0:01:21.606054
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 20:02:46.774472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.84
 ---- batch: 020 ----
mean loss: 888.70
 ---- batch: 030 ----
mean loss: 874.67
 ---- batch: 040 ----
mean loss: 883.69
 ---- batch: 050 ----
mean loss: 881.36
 ---- batch: 060 ----
mean loss: 884.43
 ---- batch: 070 ----
mean loss: 877.39
 ---- batch: 080 ----
mean loss: 876.29
 ---- batch: 090 ----
mean loss: 860.17
train mean loss: 879.40
epoch train time: 0:00:01.646596
elapsed time: 0:01:23.253246
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 20:02:48.421682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.02
 ---- batch: 020 ----
mean loss: 874.24
 ---- batch: 030 ----
mean loss: 873.60
 ---- batch: 040 ----
mean loss: 884.48
 ---- batch: 050 ----
mean loss: 901.57
 ---- batch: 060 ----
mean loss: 869.50
 ---- batch: 070 ----
mean loss: 894.24
 ---- batch: 080 ----
mean loss: 873.10
 ---- batch: 090 ----
mean loss: 864.77
train mean loss: 879.28
epoch train time: 0:00:01.646719
elapsed time: 0:01:24.900630
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 20:02:50.069082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.57
 ---- batch: 020 ----
mean loss: 887.18
 ---- batch: 030 ----
mean loss: 878.17
 ---- batch: 040 ----
mean loss: 871.86
 ---- batch: 050 ----
mean loss: 880.00
 ---- batch: 060 ----
mean loss: 891.22
 ---- batch: 070 ----
mean loss: 866.82
 ---- batch: 080 ----
mean loss: 884.62
 ---- batch: 090 ----
mean loss: 873.97
train mean loss: 879.08
epoch train time: 0:00:01.640018
elapsed time: 0:01:26.541331
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 20:02:51.709852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.16
 ---- batch: 020 ----
mean loss: 877.77
 ---- batch: 030 ----
mean loss: 880.24
 ---- batch: 040 ----
mean loss: 874.52
 ---- batch: 050 ----
mean loss: 873.19
 ---- batch: 060 ----
mean loss: 871.77
 ---- batch: 070 ----
mean loss: 873.98
 ---- batch: 080 ----
mean loss: 882.12
 ---- batch: 090 ----
mean loss: 882.68
train mean loss: 879.92
epoch train time: 0:00:01.674822
elapsed time: 0:01:28.216900
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 20:02:53.385310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.57
 ---- batch: 020 ----
mean loss: 885.67
 ---- batch: 030 ----
mean loss: 872.61
 ---- batch: 040 ----
mean loss: 882.56
 ---- batch: 050 ----
mean loss: 891.52
 ---- batch: 060 ----
mean loss: 883.53
 ---- batch: 070 ----
mean loss: 887.04
 ---- batch: 080 ----
mean loss: 859.42
 ---- batch: 090 ----
mean loss: 877.13
train mean loss: 878.85
epoch train time: 0:00:01.668219
elapsed time: 0:01:29.885921
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 20:02:55.054377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.01
 ---- batch: 020 ----
mean loss: 875.72
 ---- batch: 030 ----
mean loss: 860.77
 ---- batch: 040 ----
mean loss: 880.46
 ---- batch: 050 ----
mean loss: 873.51
 ---- batch: 060 ----
mean loss: 892.76
 ---- batch: 070 ----
mean loss: 880.87
 ---- batch: 080 ----
mean loss: 878.87
 ---- batch: 090 ----
mean loss: 891.89
train mean loss: 878.36
epoch train time: 0:00:01.669528
elapsed time: 0:01:31.556173
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 20:02:56.724585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.99
 ---- batch: 020 ----
mean loss: 873.96
 ---- batch: 030 ----
mean loss: 879.88
 ---- batch: 040 ----
mean loss: 892.68
 ---- batch: 050 ----
mean loss: 892.35
 ---- batch: 060 ----
mean loss: 861.51
 ---- batch: 070 ----
mean loss: 862.69
 ---- batch: 080 ----
mean loss: 871.90
 ---- batch: 090 ----
mean loss: 912.22
train mean loss: 879.29
epoch train time: 0:00:01.671240
elapsed time: 0:01:33.228115
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 20:02:58.396560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.77
 ---- batch: 020 ----
mean loss: 870.53
 ---- batch: 030 ----
mean loss: 882.50
 ---- batch: 040 ----
mean loss: 898.57
 ---- batch: 050 ----
mean loss: 897.99
 ---- batch: 060 ----
mean loss: 872.70
 ---- batch: 070 ----
mean loss: 876.38
 ---- batch: 080 ----
mean loss: 852.80
 ---- batch: 090 ----
mean loss: 871.18
train mean loss: 877.07
epoch train time: 0:00:01.709197
elapsed time: 0:01:34.938052
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 20:03:00.106476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.09
 ---- batch: 020 ----
mean loss: 888.58
 ---- batch: 030 ----
mean loss: 883.23
 ---- batch: 040 ----
mean loss: 882.96
 ---- batch: 050 ----
mean loss: 867.92
 ---- batch: 060 ----
mean loss: 886.47
 ---- batch: 070 ----
mean loss: 874.63
 ---- batch: 080 ----
mean loss: 884.64
 ---- batch: 090 ----
mean loss: 859.14
train mean loss: 878.54
epoch train time: 0:00:01.677645
elapsed time: 0:01:36.616416
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 20:03:01.784824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.79
 ---- batch: 020 ----
mean loss: 876.30
 ---- batch: 030 ----
mean loss: 872.74
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 860.37
 ---- batch: 060 ----
mean loss: 886.63
 ---- batch: 070 ----
mean loss: 884.16
 ---- batch: 080 ----
mean loss: 873.94
 ---- batch: 090 ----
mean loss: 885.29
train mean loss: 878.52
epoch train time: 0:00:01.670908
elapsed time: 0:01:38.288026
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 20:03:03.456448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.35
 ---- batch: 020 ----
mean loss: 864.98
 ---- batch: 030 ----
mean loss: 869.83
 ---- batch: 040 ----
mean loss: 875.26
 ---- batch: 050 ----
mean loss: 878.60
 ---- batch: 060 ----
mean loss: 876.09
 ---- batch: 070 ----
mean loss: 878.00
 ---- batch: 080 ----
mean loss: 881.77
 ---- batch: 090 ----
mean loss: 884.43
train mean loss: 878.38
epoch train time: 0:00:01.697501
elapsed time: 0:01:39.986297
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 20:03:05.154752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.05
 ---- batch: 020 ----
mean loss: 890.62
 ---- batch: 030 ----
mean loss: 877.00
 ---- batch: 040 ----
mean loss: 867.43
 ---- batch: 050 ----
mean loss: 891.65
 ---- batch: 060 ----
mean loss: 879.00
 ---- batch: 070 ----
mean loss: 877.24
 ---- batch: 080 ----
mean loss: 864.73
 ---- batch: 090 ----
mean loss: 870.02
train mean loss: 877.30
epoch train time: 0:00:01.703780
elapsed time: 0:01:41.690782
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 20:03:06.859187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.97
 ---- batch: 020 ----
mean loss: 882.70
 ---- batch: 030 ----
mean loss: 882.96
 ---- batch: 040 ----
mean loss: 871.85
 ---- batch: 050 ----
mean loss: 865.14
 ---- batch: 060 ----
mean loss: 887.25
 ---- batch: 070 ----
mean loss: 880.06
 ---- batch: 080 ----
mean loss: 877.65
 ---- batch: 090 ----
mean loss: 885.94
train mean loss: 878.21
epoch train time: 0:00:01.654606
elapsed time: 0:01:43.346057
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 20:03:08.514482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.70
 ---- batch: 020 ----
mean loss: 864.22
 ---- batch: 030 ----
mean loss: 881.36
 ---- batch: 040 ----
mean loss: 850.26
 ---- batch: 050 ----
mean loss: 856.01
 ---- batch: 060 ----
mean loss: 874.57
 ---- batch: 070 ----
mean loss: 891.59
 ---- batch: 080 ----
mean loss: 895.53
 ---- batch: 090 ----
mean loss: 900.83
train mean loss: 877.64
epoch train time: 0:00:01.700301
elapsed time: 0:01:45.046987
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 20:03:10.215427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.44
 ---- batch: 020 ----
mean loss: 862.63
 ---- batch: 030 ----
mean loss: 883.87
 ---- batch: 040 ----
mean loss: 890.94
 ---- batch: 050 ----
mean loss: 870.82
 ---- batch: 060 ----
mean loss: 877.78
 ---- batch: 070 ----
mean loss: 873.45
 ---- batch: 080 ----
mean loss: 887.15
 ---- batch: 090 ----
mean loss: 888.36
train mean loss: 877.07
epoch train time: 0:00:01.675489
elapsed time: 0:01:46.723194
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 20:03:11.891420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.07
 ---- batch: 020 ----
mean loss: 895.02
 ---- batch: 030 ----
mean loss: 896.76
 ---- batch: 040 ----
mean loss: 878.30
 ---- batch: 050 ----
mean loss: 854.52
 ---- batch: 060 ----
mean loss: 888.21
 ---- batch: 070 ----
mean loss: 876.15
 ---- batch: 080 ----
mean loss: 875.46
 ---- batch: 090 ----
mean loss: 872.22
train mean loss: 877.35
epoch train time: 0:00:01.665734
elapsed time: 0:01:48.389375
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 20:03:13.557831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.30
 ---- batch: 020 ----
mean loss: 872.26
 ---- batch: 030 ----
mean loss: 879.96
 ---- batch: 040 ----
mean loss: 879.52
 ---- batch: 050 ----
mean loss: 878.89
 ---- batch: 060 ----
mean loss: 868.68
 ---- batch: 070 ----
mean loss: 881.02
 ---- batch: 080 ----
mean loss: 878.09
 ---- batch: 090 ----
mean loss: 866.27
train mean loss: 876.73
epoch train time: 0:00:01.661144
elapsed time: 0:01:50.051227
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 20:03:15.219652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.90
 ---- batch: 020 ----
mean loss: 877.09
 ---- batch: 030 ----
mean loss: 885.46
 ---- batch: 040 ----
mean loss: 872.25
 ---- batch: 050 ----
mean loss: 888.61
 ---- batch: 060 ----
mean loss: 880.41
 ---- batch: 070 ----
mean loss: 895.15
 ---- batch: 080 ----
mean loss: 868.72
 ---- batch: 090 ----
mean loss: 871.91
train mean loss: 876.49
epoch train time: 0:00:01.649846
elapsed time: 0:01:51.701871
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 20:03:16.870315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.53
 ---- batch: 020 ----
mean loss: 861.44
 ---- batch: 030 ----
mean loss: 862.27
 ---- batch: 040 ----
mean loss: 865.17
 ---- batch: 050 ----
mean loss: 851.35
 ---- batch: 060 ----
mean loss: 836.28
 ---- batch: 070 ----
mean loss: 819.17
 ---- batch: 080 ----
mean loss: 788.49
 ---- batch: 090 ----
mean loss: 756.50
train mean loss: 825.08
epoch train time: 0:00:01.706710
elapsed time: 0:01:53.409246
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 20:03:18.577716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 666.81
 ---- batch: 020 ----
mean loss: 613.78
 ---- batch: 030 ----
mean loss: 582.24
 ---- batch: 040 ----
mean loss: 545.68
 ---- batch: 050 ----
mean loss: 523.53
 ---- batch: 060 ----
mean loss: 500.70
 ---- batch: 070 ----
mean loss: 489.19
 ---- batch: 080 ----
mean loss: 470.82
 ---- batch: 090 ----
mean loss: 457.43
train mean loss: 533.73
epoch train time: 0:00:01.657503
elapsed time: 0:01:55.067456
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 20:03:20.235851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.78
 ---- batch: 020 ----
mean loss: 430.77
 ---- batch: 030 ----
mean loss: 434.58
 ---- batch: 040 ----
mean loss: 418.96
 ---- batch: 050 ----
mean loss: 425.66
 ---- batch: 060 ----
mean loss: 421.19
 ---- batch: 070 ----
mean loss: 403.49
 ---- batch: 080 ----
mean loss: 422.24
 ---- batch: 090 ----
mean loss: 395.49
train mean loss: 420.60
epoch train time: 0:00:01.691944
elapsed time: 0:01:56.760148
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 20:03:21.928570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.68
 ---- batch: 020 ----
mean loss: 388.16
 ---- batch: 030 ----
mean loss: 386.92
 ---- batch: 040 ----
mean loss: 367.31
 ---- batch: 050 ----
mean loss: 373.77
 ---- batch: 060 ----
mean loss: 369.38
 ---- batch: 070 ----
mean loss: 372.88
 ---- batch: 080 ----
mean loss: 366.43
 ---- batch: 090 ----
mean loss: 368.33
train mean loss: 375.05
epoch train time: 0:00:01.697434
elapsed time: 0:01:58.458253
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 20:03:23.626707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.89
 ---- batch: 020 ----
mean loss: 349.73
 ---- batch: 030 ----
mean loss: 349.28
 ---- batch: 040 ----
mean loss: 364.19
 ---- batch: 050 ----
mean loss: 347.99
 ---- batch: 060 ----
mean loss: 342.49
 ---- batch: 070 ----
mean loss: 340.75
 ---- batch: 080 ----
mean loss: 351.29
 ---- batch: 090 ----
mean loss: 347.38
train mean loss: 350.26
epoch train time: 0:00:01.687892
elapsed time: 0:02:00.146830
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 20:03:25.315290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.17
 ---- batch: 020 ----
mean loss: 341.58
 ---- batch: 030 ----
mean loss: 332.10
 ---- batch: 040 ----
mean loss: 335.60
 ---- batch: 050 ----
mean loss: 329.84
 ---- batch: 060 ----
mean loss: 337.79
 ---- batch: 070 ----
mean loss: 321.81
 ---- batch: 080 ----
mean loss: 330.39
 ---- batch: 090 ----
mean loss: 332.72
train mean loss: 332.96
epoch train time: 0:00:01.666842
elapsed time: 0:02:01.814417
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 20:03:26.982768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.84
 ---- batch: 020 ----
mean loss: 311.39
 ---- batch: 030 ----
mean loss: 330.19
 ---- batch: 040 ----
mean loss: 314.72
 ---- batch: 050 ----
mean loss: 313.75
 ---- batch: 060 ----
mean loss: 310.51
 ---- batch: 070 ----
mean loss: 309.02
 ---- batch: 080 ----
mean loss: 327.62
 ---- batch: 090 ----
mean loss: 313.29
train mean loss: 316.99
epoch train time: 0:00:01.700188
elapsed time: 0:02:03.515263
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 20:03:28.683615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.86
 ---- batch: 020 ----
mean loss: 307.68
 ---- batch: 030 ----
mean loss: 309.31
 ---- batch: 040 ----
mean loss: 304.40
 ---- batch: 050 ----
mean loss: 307.53
 ---- batch: 060 ----
mean loss: 311.02
 ---- batch: 070 ----
mean loss: 313.38
 ---- batch: 080 ----
mean loss: 293.64
 ---- batch: 090 ----
mean loss: 307.88
train mean loss: 308.28
epoch train time: 0:00:01.685002
elapsed time: 0:02:05.200859
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 20:03:30.369254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.32
 ---- batch: 020 ----
mean loss: 294.87
 ---- batch: 030 ----
mean loss: 300.31
 ---- batch: 040 ----
mean loss: 307.51
 ---- batch: 050 ----
mean loss: 298.85
 ---- batch: 060 ----
mean loss: 302.47
 ---- batch: 070 ----
mean loss: 308.51
 ---- batch: 080 ----
mean loss: 288.48
 ---- batch: 090 ----
mean loss: 285.88
train mean loss: 298.52
epoch train time: 0:00:01.604832
elapsed time: 0:02:06.806328
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 20:03:31.974715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.46
 ---- batch: 020 ----
mean loss: 285.00
 ---- batch: 030 ----
mean loss: 297.25
 ---- batch: 040 ----
mean loss: 294.69
 ---- batch: 050 ----
mean loss: 293.42
 ---- batch: 060 ----
mean loss: 287.93
 ---- batch: 070 ----
mean loss: 291.53
 ---- batch: 080 ----
mean loss: 286.81
 ---- batch: 090 ----
mean loss: 294.48
train mean loss: 290.54
epoch train time: 0:00:01.626520
elapsed time: 0:02:08.433465
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 20:03:33.601951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.32
 ---- batch: 020 ----
mean loss: 287.93
 ---- batch: 030 ----
mean loss: 292.23
 ---- batch: 040 ----
mean loss: 285.81
 ---- batch: 050 ----
mean loss: 290.59
 ---- batch: 060 ----
mean loss: 286.46
 ---- batch: 070 ----
mean loss: 278.05
 ---- batch: 080 ----
mean loss: 282.87
 ---- batch: 090 ----
mean loss: 283.37
train mean loss: 286.25
epoch train time: 0:00:01.675750
elapsed time: 0:02:10.109946
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 20:03:35.278365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.74
 ---- batch: 020 ----
mean loss: 276.69
 ---- batch: 030 ----
mean loss: 291.64
 ---- batch: 040 ----
mean loss: 277.37
 ---- batch: 050 ----
mean loss: 273.76
 ---- batch: 060 ----
mean loss: 277.01
 ---- batch: 070 ----
mean loss: 274.04
 ---- batch: 080 ----
mean loss: 286.16
 ---- batch: 090 ----
mean loss: 280.88
train mean loss: 279.65
epoch train time: 0:00:01.674618
elapsed time: 0:02:11.785222
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 20:03:36.953683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.49
 ---- batch: 020 ----
mean loss: 286.23
 ---- batch: 030 ----
mean loss: 269.36
 ---- batch: 040 ----
mean loss: 272.69
 ---- batch: 050 ----
mean loss: 270.37
 ---- batch: 060 ----
mean loss: 277.85
 ---- batch: 070 ----
mean loss: 277.74
 ---- batch: 080 ----
mean loss: 267.72
 ---- batch: 090 ----
mean loss: 272.96
train mean loss: 274.82
epoch train time: 0:00:01.642041
elapsed time: 0:02:13.427990
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 20:03:38.596423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.60
 ---- batch: 020 ----
mean loss: 268.81
 ---- batch: 030 ----
mean loss: 267.82
 ---- batch: 040 ----
mean loss: 267.88
 ---- batch: 050 ----
mean loss: 264.96
 ---- batch: 060 ----
mean loss: 268.17
 ---- batch: 070 ----
mean loss: 275.68
 ---- batch: 080 ----
mean loss: 277.20
 ---- batch: 090 ----
mean loss: 276.62
train mean loss: 270.03
epoch train time: 0:00:01.670582
elapsed time: 0:02:15.099244
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 20:03:40.267745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.38
 ---- batch: 020 ----
mean loss: 263.57
 ---- batch: 030 ----
mean loss: 262.87
 ---- batch: 040 ----
mean loss: 258.00
 ---- batch: 050 ----
mean loss: 272.70
 ---- batch: 060 ----
mean loss: 277.44
 ---- batch: 070 ----
mean loss: 263.49
 ---- batch: 080 ----
mean loss: 260.88
 ---- batch: 090 ----
mean loss: 279.98
train mean loss: 267.49
epoch train time: 0:00:01.668951
elapsed time: 0:02:16.768980
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 20:03:41.937391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.04
 ---- batch: 020 ----
mean loss: 257.14
 ---- batch: 030 ----
mean loss: 268.48
 ---- batch: 040 ----
mean loss: 256.35
 ---- batch: 050 ----
mean loss: 265.67
 ---- batch: 060 ----
mean loss: 262.70
 ---- batch: 070 ----
mean loss: 266.47
 ---- batch: 080 ----
mean loss: 264.33
 ---- batch: 090 ----
mean loss: 257.87
train mean loss: 262.19
epoch train time: 0:00:01.667566
elapsed time: 0:02:18.437184
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 20:03:43.605644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.15
 ---- batch: 020 ----
mean loss: 262.13
 ---- batch: 030 ----
mean loss: 249.89
 ---- batch: 040 ----
mean loss: 252.26
 ---- batch: 050 ----
mean loss: 262.60
 ---- batch: 060 ----
mean loss: 247.51
 ---- batch: 070 ----
mean loss: 259.45
 ---- batch: 080 ----
mean loss: 256.11
 ---- batch: 090 ----
mean loss: 248.59
train mean loss: 254.56
epoch train time: 0:00:01.652307
elapsed time: 0:02:20.090167
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 20:03:45.258564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.73
 ---- batch: 020 ----
mean loss: 250.88
 ---- batch: 030 ----
mean loss: 255.62
 ---- batch: 040 ----
mean loss: 257.72
 ---- batch: 050 ----
mean loss: 258.00
 ---- batch: 060 ----
mean loss: 256.54
 ---- batch: 070 ----
mean loss: 243.37
 ---- batch: 080 ----
mean loss: 245.47
 ---- batch: 090 ----
mean loss: 252.24
train mean loss: 253.26
epoch train time: 0:00:01.632904
elapsed time: 0:02:21.723745
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 20:03:46.892167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.16
 ---- batch: 020 ----
mean loss: 243.53
 ---- batch: 030 ----
mean loss: 250.52
 ---- batch: 040 ----
mean loss: 249.03
 ---- batch: 050 ----
mean loss: 256.24
 ---- batch: 060 ----
mean loss: 254.62
 ---- batch: 070 ----
mean loss: 249.28
 ---- batch: 080 ----
mean loss: 260.30
 ---- batch: 090 ----
mean loss: 245.24
train mean loss: 249.42
epoch train time: 0:00:01.624913
elapsed time: 0:02:23.349329
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 20:03:48.517774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.29
 ---- batch: 020 ----
mean loss: 238.89
 ---- batch: 030 ----
mean loss: 258.89
 ---- batch: 040 ----
mean loss: 251.16
 ---- batch: 050 ----
mean loss: 251.38
 ---- batch: 060 ----
mean loss: 250.48
 ---- batch: 070 ----
mean loss: 244.59
 ---- batch: 080 ----
mean loss: 244.75
 ---- batch: 090 ----
mean loss: 236.54
train mean loss: 246.49
epoch train time: 0:00:01.637909
elapsed time: 0:02:24.987904
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 20:03:50.156347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.52
 ---- batch: 020 ----
mean loss: 245.71
 ---- batch: 030 ----
mean loss: 240.49
 ---- batch: 040 ----
mean loss: 238.13
 ---- batch: 050 ----
mean loss: 242.11
 ---- batch: 060 ----
mean loss: 237.64
 ---- batch: 070 ----
mean loss: 245.10
 ---- batch: 080 ----
mean loss: 246.75
 ---- batch: 090 ----
mean loss: 242.61
train mean loss: 243.25
epoch train time: 0:00:01.637719
elapsed time: 0:02:26.626315
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 20:03:51.794709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.46
 ---- batch: 020 ----
mean loss: 240.48
 ---- batch: 030 ----
mean loss: 229.33
 ---- batch: 040 ----
mean loss: 244.41
 ---- batch: 050 ----
mean loss: 236.45
 ---- batch: 060 ----
mean loss: 236.22
 ---- batch: 070 ----
mean loss: 251.24
 ---- batch: 080 ----
mean loss: 236.65
 ---- batch: 090 ----
mean loss: 242.30
train mean loss: 239.80
epoch train time: 0:00:01.625026
elapsed time: 0:02:28.251973
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 20:03:53.420368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.69
 ---- batch: 020 ----
mean loss: 227.72
 ---- batch: 030 ----
mean loss: 233.22
 ---- batch: 040 ----
mean loss: 236.53
 ---- batch: 050 ----
mean loss: 236.48
 ---- batch: 060 ----
mean loss: 229.25
 ---- batch: 070 ----
mean loss: 223.75
 ---- batch: 080 ----
mean loss: 247.96
 ---- batch: 090 ----
mean loss: 244.12
train mean loss: 236.64
epoch train time: 0:00:01.642551
elapsed time: 0:02:29.895313
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 20:03:55.063724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.59
 ---- batch: 020 ----
mean loss: 228.60
 ---- batch: 030 ----
mean loss: 236.48
 ---- batch: 040 ----
mean loss: 240.76
 ---- batch: 050 ----
mean loss: 236.98
 ---- batch: 060 ----
mean loss: 231.75
 ---- batch: 070 ----
mean loss: 239.33
 ---- batch: 080 ----
mean loss: 233.94
 ---- batch: 090 ----
mean loss: 236.10
train mean loss: 234.95
epoch train time: 0:00:01.636988
elapsed time: 0:02:31.532992
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 20:03:56.701423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.88
 ---- batch: 020 ----
mean loss: 228.91
 ---- batch: 030 ----
mean loss: 229.96
 ---- batch: 040 ----
mean loss: 231.11
 ---- batch: 050 ----
mean loss: 234.99
 ---- batch: 060 ----
mean loss: 238.45
 ---- batch: 070 ----
mean loss: 226.57
 ---- batch: 080 ----
mean loss: 229.97
 ---- batch: 090 ----
mean loss: 236.90
train mean loss: 231.73
epoch train time: 0:00:01.660437
elapsed time: 0:02:33.194067
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 20:03:58.362481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.41
 ---- batch: 020 ----
mean loss: 216.47
 ---- batch: 030 ----
mean loss: 228.37
 ---- batch: 040 ----
mean loss: 226.27
 ---- batch: 050 ----
mean loss: 238.50
 ---- batch: 060 ----
mean loss: 230.98
 ---- batch: 070 ----
mean loss: 231.64
 ---- batch: 080 ----
mean loss: 241.23
 ---- batch: 090 ----
mean loss: 232.15
train mean loss: 229.11
epoch train time: 0:00:01.655593
elapsed time: 0:02:34.850308
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 20:04:00.018722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.16
 ---- batch: 020 ----
mean loss: 226.53
 ---- batch: 030 ----
mean loss: 235.23
 ---- batch: 040 ----
mean loss: 223.48
 ---- batch: 050 ----
mean loss: 222.58
 ---- batch: 060 ----
mean loss: 230.88
 ---- batch: 070 ----
mean loss: 223.39
 ---- batch: 080 ----
mean loss: 228.21
 ---- batch: 090 ----
mean loss: 228.43
train mean loss: 226.42
epoch train time: 0:00:01.630064
elapsed time: 0:02:36.481024
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 20:04:01.649418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.75
 ---- batch: 020 ----
mean loss: 224.05
 ---- batch: 030 ----
mean loss: 224.28
 ---- batch: 040 ----
mean loss: 224.02
 ---- batch: 050 ----
mean loss: 223.17
 ---- batch: 060 ----
mean loss: 222.99
 ---- batch: 070 ----
mean loss: 213.57
 ---- batch: 080 ----
mean loss: 223.89
 ---- batch: 090 ----
mean loss: 227.01
train mean loss: 222.63
epoch train time: 0:00:01.604334
elapsed time: 0:02:38.086022
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 20:04:03.254431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.29
 ---- batch: 020 ----
mean loss: 216.49
 ---- batch: 030 ----
mean loss: 225.34
 ---- batch: 040 ----
mean loss: 230.06
 ---- batch: 050 ----
mean loss: 218.18
 ---- batch: 060 ----
mean loss: 217.61
 ---- batch: 070 ----
mean loss: 225.79
 ---- batch: 080 ----
mean loss: 224.99
 ---- batch: 090 ----
mean loss: 221.65
train mean loss: 221.17
epoch train time: 0:00:01.636745
elapsed time: 0:02:39.723414
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 20:04:04.891865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.79
 ---- batch: 020 ----
mean loss: 218.65
 ---- batch: 030 ----
mean loss: 206.74
 ---- batch: 040 ----
mean loss: 211.82
 ---- batch: 050 ----
mean loss: 219.91
 ---- batch: 060 ----
mean loss: 220.32
 ---- batch: 070 ----
mean loss: 221.00
 ---- batch: 080 ----
mean loss: 226.74
 ---- batch: 090 ----
mean loss: 221.51
train mean loss: 218.89
epoch train time: 0:00:01.667738
elapsed time: 0:02:41.391847
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 20:04:06.560254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.52
 ---- batch: 020 ----
mean loss: 220.66
 ---- batch: 030 ----
mean loss: 213.43
 ---- batch: 040 ----
mean loss: 219.47
 ---- batch: 050 ----
mean loss: 207.09
 ---- batch: 060 ----
mean loss: 215.21
 ---- batch: 070 ----
mean loss: 216.77
 ---- batch: 080 ----
mean loss: 219.55
 ---- batch: 090 ----
mean loss: 217.79
train mean loss: 215.73
epoch train time: 0:00:01.698667
elapsed time: 0:02:43.091179
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 20:04:08.259640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.97
 ---- batch: 020 ----
mean loss: 206.06
 ---- batch: 030 ----
mean loss: 210.93
 ---- batch: 040 ----
mean loss: 223.87
 ---- batch: 050 ----
mean loss: 217.80
 ---- batch: 060 ----
mean loss: 210.46
 ---- batch: 070 ----
mean loss: 226.75
 ---- batch: 080 ----
mean loss: 212.33
 ---- batch: 090 ----
mean loss: 218.74
train mean loss: 213.94
epoch train time: 0:00:01.654515
elapsed time: 0:02:44.746399
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 20:04:09.914789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.76
 ---- batch: 020 ----
mean loss: 207.06
 ---- batch: 030 ----
mean loss: 208.43
 ---- batch: 040 ----
mean loss: 216.92
 ---- batch: 050 ----
mean loss: 210.23
 ---- batch: 060 ----
mean loss: 216.31
 ---- batch: 070 ----
mean loss: 207.75
 ---- batch: 080 ----
mean loss: 214.76
 ---- batch: 090 ----
mean loss: 213.58
train mean loss: 211.07
epoch train time: 0:00:01.657399
elapsed time: 0:02:46.404428
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 20:04:11.572839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.93
 ---- batch: 020 ----
mean loss: 203.58
 ---- batch: 030 ----
mean loss: 204.58
 ---- batch: 040 ----
mean loss: 210.28
 ---- batch: 050 ----
mean loss: 210.94
 ---- batch: 060 ----
mean loss: 210.03
 ---- batch: 070 ----
mean loss: 209.08
 ---- batch: 080 ----
mean loss: 208.19
 ---- batch: 090 ----
mean loss: 217.45
train mean loss: 209.42
epoch train time: 0:00:01.673432
elapsed time: 0:02:48.078509
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 20:04:13.246925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.93
 ---- batch: 020 ----
mean loss: 211.25
 ---- batch: 030 ----
mean loss: 215.18
 ---- batch: 040 ----
mean loss: 203.45
 ---- batch: 050 ----
mean loss: 215.61
 ---- batch: 060 ----
mean loss: 203.98
 ---- batch: 070 ----
mean loss: 205.01
 ---- batch: 080 ----
mean loss: 206.15
 ---- batch: 090 ----
mean loss: 208.15
train mean loss: 207.88
epoch train time: 0:00:01.629158
elapsed time: 0:02:49.708341
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 20:04:14.876741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.38
 ---- batch: 020 ----
mean loss: 204.28
 ---- batch: 030 ----
mean loss: 198.35
 ---- batch: 040 ----
mean loss: 207.05
 ---- batch: 050 ----
mean loss: 211.30
 ---- batch: 060 ----
mean loss: 210.47
 ---- batch: 070 ----
mean loss: 204.51
 ---- batch: 080 ----
mean loss: 213.32
 ---- batch: 090 ----
mean loss: 194.80
train mean loss: 205.95
epoch train time: 0:00:01.626022
elapsed time: 0:02:51.334974
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 20:04:16.503468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.43
 ---- batch: 020 ----
mean loss: 194.85
 ---- batch: 030 ----
mean loss: 203.38
 ---- batch: 040 ----
mean loss: 206.37
 ---- batch: 050 ----
mean loss: 209.49
 ---- batch: 060 ----
mean loss: 208.25
 ---- batch: 070 ----
mean loss: 204.44
 ---- batch: 080 ----
mean loss: 205.51
 ---- batch: 090 ----
mean loss: 207.00
train mean loss: 204.99
epoch train time: 0:00:01.603650
elapsed time: 0:02:52.939395
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 20:04:18.107802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.56
 ---- batch: 020 ----
mean loss: 197.23
 ---- batch: 030 ----
mean loss: 199.69
 ---- batch: 040 ----
mean loss: 198.85
 ---- batch: 050 ----
mean loss: 206.00
 ---- batch: 060 ----
mean loss: 203.39
 ---- batch: 070 ----
mean loss: 191.73
 ---- batch: 080 ----
mean loss: 200.24
 ---- batch: 090 ----
mean loss: 207.78
train mean loss: 201.00
epoch train time: 0:00:01.632175
elapsed time: 0:02:54.572261
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 20:04:19.740677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.38
 ---- batch: 020 ----
mean loss: 209.62
 ---- batch: 030 ----
mean loss: 200.62
 ---- batch: 040 ----
mean loss: 207.84
 ---- batch: 050 ----
mean loss: 199.66
 ---- batch: 060 ----
mean loss: 192.55
 ---- batch: 070 ----
mean loss: 189.29
 ---- batch: 080 ----
mean loss: 199.25
 ---- batch: 090 ----
mean loss: 204.89
train mean loss: 200.75
epoch train time: 0:00:01.698808
elapsed time: 0:02:56.271730
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 20:04:21.439974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.95
 ---- batch: 020 ----
mean loss: 191.39
 ---- batch: 030 ----
mean loss: 199.57
 ---- batch: 040 ----
mean loss: 203.26
 ---- batch: 050 ----
mean loss: 193.42
 ---- batch: 060 ----
mean loss: 200.00
 ---- batch: 070 ----
mean loss: 193.46
 ---- batch: 080 ----
mean loss: 200.87
 ---- batch: 090 ----
mean loss: 192.59
train mean loss: 197.19
epoch train time: 0:00:01.681761
elapsed time: 0:02:57.953996
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 20:04:23.122445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.28
 ---- batch: 020 ----
mean loss: 194.79
 ---- batch: 030 ----
mean loss: 192.48
 ---- batch: 040 ----
mean loss: 197.57
 ---- batch: 050 ----
mean loss: 183.85
 ---- batch: 060 ----
mean loss: 199.92
 ---- batch: 070 ----
mean loss: 196.98
 ---- batch: 080 ----
mean loss: 200.46
 ---- batch: 090 ----
mean loss: 201.90
train mean loss: 196.50
epoch train time: 0:00:01.640292
elapsed time: 0:02:59.595061
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 20:04:24.763603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.86
 ---- batch: 020 ----
mean loss: 196.49
 ---- batch: 030 ----
mean loss: 191.15
 ---- batch: 040 ----
mean loss: 197.57
 ---- batch: 050 ----
mean loss: 188.13
 ---- batch: 060 ----
mean loss: 199.03
 ---- batch: 070 ----
mean loss: 194.17
 ---- batch: 080 ----
mean loss: 195.19
 ---- batch: 090 ----
mean loss: 192.75
train mean loss: 194.51
epoch train time: 0:00:01.660249
elapsed time: 0:03:01.256166
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 20:04:26.424592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.55
 ---- batch: 020 ----
mean loss: 193.20
 ---- batch: 030 ----
mean loss: 190.23
 ---- batch: 040 ----
mean loss: 192.06
 ---- batch: 050 ----
mean loss: 197.50
 ---- batch: 060 ----
mean loss: 190.01
 ---- batch: 070 ----
mean loss: 193.85
 ---- batch: 080 ----
mean loss: 192.64
 ---- batch: 090 ----
mean loss: 194.59
train mean loss: 192.53
epoch train time: 0:00:01.677185
elapsed time: 0:03:02.934044
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 20:04:28.102472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.55
 ---- batch: 020 ----
mean loss: 189.74
 ---- batch: 030 ----
mean loss: 183.79
 ---- batch: 040 ----
mean loss: 188.90
 ---- batch: 050 ----
mean loss: 195.20
 ---- batch: 060 ----
mean loss: 204.68
 ---- batch: 070 ----
mean loss: 186.70
 ---- batch: 080 ----
mean loss: 199.63
 ---- batch: 090 ----
mean loss: 188.77
train mean loss: 191.51
epoch train time: 0:00:01.663084
elapsed time: 0:03:04.597940
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 20:04:29.766406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.49
 ---- batch: 020 ----
mean loss: 194.16
 ---- batch: 030 ----
mean loss: 184.15
 ---- batch: 040 ----
mean loss: 187.05
 ---- batch: 050 ----
mean loss: 189.44
 ---- batch: 060 ----
mean loss: 192.82
 ---- batch: 070 ----
mean loss: 186.93
 ---- batch: 080 ----
mean loss: 194.70
 ---- batch: 090 ----
mean loss: 184.50
train mean loss: 189.05
epoch train time: 0:00:01.689844
elapsed time: 0:03:06.288540
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 20:04:31.456957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.24
 ---- batch: 020 ----
mean loss: 188.42
 ---- batch: 030 ----
mean loss: 186.16
 ---- batch: 040 ----
mean loss: 188.16
 ---- batch: 050 ----
mean loss: 182.62
 ---- batch: 060 ----
mean loss: 192.92
 ---- batch: 070 ----
mean loss: 186.71
 ---- batch: 080 ----
mean loss: 193.88
 ---- batch: 090 ----
mean loss: 189.02
train mean loss: 187.69
epoch train time: 0:00:01.626607
elapsed time: 0:03:07.915816
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 20:04:33.084233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.09
 ---- batch: 020 ----
mean loss: 188.26
 ---- batch: 030 ----
mean loss: 182.83
 ---- batch: 040 ----
mean loss: 186.66
 ---- batch: 050 ----
mean loss: 180.46
 ---- batch: 060 ----
mean loss: 195.46
 ---- batch: 070 ----
mean loss: 192.46
 ---- batch: 080 ----
mean loss: 189.94
 ---- batch: 090 ----
mean loss: 182.09
train mean loss: 186.05
epoch train time: 0:00:01.656602
elapsed time: 0:03:09.573092
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 20:04:34.741517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.91
 ---- batch: 020 ----
mean loss: 179.60
 ---- batch: 030 ----
mean loss: 189.49
 ---- batch: 040 ----
mean loss: 179.67
 ---- batch: 050 ----
mean loss: 177.94
 ---- batch: 060 ----
mean loss: 190.45
 ---- batch: 070 ----
mean loss: 185.56
 ---- batch: 080 ----
mean loss: 184.89
 ---- batch: 090 ----
mean loss: 190.94
train mean loss: 185.12
epoch train time: 0:00:01.652379
elapsed time: 0:03:11.226184
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 20:04:36.394656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.90
 ---- batch: 020 ----
mean loss: 179.48
 ---- batch: 030 ----
mean loss: 178.50
 ---- batch: 040 ----
mean loss: 178.28
 ---- batch: 050 ----
mean loss: 182.93
 ---- batch: 060 ----
mean loss: 184.90
 ---- batch: 070 ----
mean loss: 178.32
 ---- batch: 080 ----
mean loss: 188.27
 ---- batch: 090 ----
mean loss: 186.64
train mean loss: 182.87
epoch train time: 0:00:01.663226
elapsed time: 0:03:12.890106
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 20:04:38.058541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.67
 ---- batch: 020 ----
mean loss: 174.32
 ---- batch: 030 ----
mean loss: 178.27
 ---- batch: 040 ----
mean loss: 185.78
 ---- batch: 050 ----
mean loss: 178.80
 ---- batch: 060 ----
mean loss: 185.65
 ---- batch: 070 ----
mean loss: 191.40
 ---- batch: 080 ----
mean loss: 185.59
 ---- batch: 090 ----
mean loss: 181.81
train mean loss: 181.90
epoch train time: 0:00:01.657924
elapsed time: 0:03:14.548708
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 20:04:39.717112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.13
 ---- batch: 020 ----
mean loss: 177.62
 ---- batch: 030 ----
mean loss: 180.81
 ---- batch: 040 ----
mean loss: 178.52
 ---- batch: 050 ----
mean loss: 180.50
 ---- batch: 060 ----
mean loss: 177.63
 ---- batch: 070 ----
mean loss: 186.22
 ---- batch: 080 ----
mean loss: 182.17
 ---- batch: 090 ----
mean loss: 182.25
train mean loss: 180.80
epoch train time: 0:00:01.678536
elapsed time: 0:03:16.227870
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 20:04:41.396268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.82
 ---- batch: 020 ----
mean loss: 178.37
 ---- batch: 030 ----
mean loss: 173.39
 ---- batch: 040 ----
mean loss: 176.66
 ---- batch: 050 ----
mean loss: 178.73
 ---- batch: 060 ----
mean loss: 178.28
 ---- batch: 070 ----
mean loss: 183.80
 ---- batch: 080 ----
mean loss: 180.15
 ---- batch: 090 ----
mean loss: 180.73
train mean loss: 178.32
epoch train time: 0:00:01.658968
elapsed time: 0:03:17.887600
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 20:04:43.056036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.10
 ---- batch: 020 ----
mean loss: 167.61
 ---- batch: 030 ----
mean loss: 169.30
 ---- batch: 040 ----
mean loss: 173.31
 ---- batch: 050 ----
mean loss: 182.23
 ---- batch: 060 ----
mean loss: 183.29
 ---- batch: 070 ----
mean loss: 171.69
 ---- batch: 080 ----
mean loss: 178.46
 ---- batch: 090 ----
mean loss: 178.51
train mean loss: 176.89
epoch train time: 0:00:01.654180
elapsed time: 0:03:19.542510
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 20:04:44.710942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.84
 ---- batch: 020 ----
mean loss: 167.03
 ---- batch: 030 ----
mean loss: 171.91
 ---- batch: 040 ----
mean loss: 172.90
 ---- batch: 050 ----
mean loss: 182.49
 ---- batch: 060 ----
mean loss: 168.57
 ---- batch: 070 ----
mean loss: 174.95
 ---- batch: 080 ----
mean loss: 180.87
 ---- batch: 090 ----
mean loss: 175.37
train mean loss: 175.22
epoch train time: 0:00:01.673761
elapsed time: 0:03:21.217010
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 20:04:46.385421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.02
 ---- batch: 020 ----
mean loss: 172.27
 ---- batch: 030 ----
mean loss: 173.90
 ---- batch: 040 ----
mean loss: 170.43
 ---- batch: 050 ----
mean loss: 180.38
 ---- batch: 060 ----
mean loss: 186.99
 ---- batch: 070 ----
mean loss: 176.44
 ---- batch: 080 ----
mean loss: 174.89
 ---- batch: 090 ----
mean loss: 174.93
train mean loss: 175.65
epoch train time: 0:00:01.656043
elapsed time: 0:03:22.873750
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 20:04:48.042164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.84
 ---- batch: 020 ----
mean loss: 165.70
 ---- batch: 030 ----
mean loss: 173.72
 ---- batch: 040 ----
mean loss: 169.42
 ---- batch: 050 ----
mean loss: 171.27
 ---- batch: 060 ----
mean loss: 179.43
 ---- batch: 070 ----
mean loss: 173.40
 ---- batch: 080 ----
mean loss: 172.88
 ---- batch: 090 ----
mean loss: 179.53
train mean loss: 173.15
epoch train time: 0:00:01.685110
elapsed time: 0:03:24.559550
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 20:04:49.727969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.68
 ---- batch: 020 ----
mean loss: 167.09
 ---- batch: 030 ----
mean loss: 166.46
 ---- batch: 040 ----
mean loss: 171.08
 ---- batch: 050 ----
mean loss: 171.15
 ---- batch: 060 ----
mean loss: 169.79
 ---- batch: 070 ----
mean loss: 182.11
 ---- batch: 080 ----
mean loss: 168.12
 ---- batch: 090 ----
mean loss: 174.80
train mean loss: 171.99
epoch train time: 0:00:01.672349
elapsed time: 0:03:26.232594
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 20:04:51.401007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.89
 ---- batch: 020 ----
mean loss: 175.51
 ---- batch: 030 ----
mean loss: 167.68
 ---- batch: 040 ----
mean loss: 171.84
 ---- batch: 050 ----
mean loss: 168.72
 ---- batch: 060 ----
mean loss: 168.21
 ---- batch: 070 ----
mean loss: 166.03
 ---- batch: 080 ----
mean loss: 172.69
 ---- batch: 090 ----
mean loss: 170.21
train mean loss: 170.62
epoch train time: 0:00:01.667057
elapsed time: 0:03:27.900316
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 20:04:53.068721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.22
 ---- batch: 020 ----
mean loss: 163.26
 ---- batch: 030 ----
mean loss: 174.16
 ---- batch: 040 ----
mean loss: 169.47
 ---- batch: 050 ----
mean loss: 159.36
 ---- batch: 060 ----
mean loss: 174.17
 ---- batch: 070 ----
mean loss: 171.71
 ---- batch: 080 ----
mean loss: 170.79
 ---- batch: 090 ----
mean loss: 172.45
train mean loss: 169.71
epoch train time: 0:00:01.699741
elapsed time: 0:03:29.600714
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 20:04:54.769113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.45
 ---- batch: 020 ----
mean loss: 169.93
 ---- batch: 030 ----
mean loss: 163.17
 ---- batch: 040 ----
mean loss: 164.28
 ---- batch: 050 ----
mean loss: 172.19
 ---- batch: 060 ----
mean loss: 168.83
 ---- batch: 070 ----
mean loss: 167.28
 ---- batch: 080 ----
mean loss: 167.57
 ---- batch: 090 ----
mean loss: 174.67
train mean loss: 168.61
epoch train time: 0:00:01.710097
elapsed time: 0:03:31.311623
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 20:04:56.479879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.40
 ---- batch: 020 ----
mean loss: 167.48
 ---- batch: 030 ----
mean loss: 164.30
 ---- batch: 040 ----
mean loss: 168.15
 ---- batch: 050 ----
mean loss: 168.99
 ---- batch: 060 ----
mean loss: 166.70
 ---- batch: 070 ----
mean loss: 171.23
 ---- batch: 080 ----
mean loss: 173.38
 ---- batch: 090 ----
mean loss: 167.66
train mean loss: 167.26
epoch train time: 0:00:01.698432
elapsed time: 0:03:33.010520
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 20:04:58.178926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.65
 ---- batch: 020 ----
mean loss: 166.24
 ---- batch: 030 ----
mean loss: 160.81
 ---- batch: 040 ----
mean loss: 165.23
 ---- batch: 050 ----
mean loss: 165.25
 ---- batch: 060 ----
mean loss: 171.19
 ---- batch: 070 ----
mean loss: 168.54
 ---- batch: 080 ----
mean loss: 168.80
 ---- batch: 090 ----
mean loss: 163.46
train mean loss: 166.03
epoch train time: 0:00:01.657331
elapsed time: 0:03:34.668532
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 20:04:59.836944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.28
 ---- batch: 020 ----
mean loss: 167.34
 ---- batch: 030 ----
mean loss: 165.34
 ---- batch: 040 ----
mean loss: 157.98
 ---- batch: 050 ----
mean loss: 158.97
 ---- batch: 060 ----
mean loss: 169.27
 ---- batch: 070 ----
mean loss: 169.02
 ---- batch: 080 ----
mean loss: 169.83
 ---- batch: 090 ----
mean loss: 165.65
train mean loss: 165.33
epoch train time: 0:00:01.660280
elapsed time: 0:03:36.329578
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 20:05:01.497970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.57
 ---- batch: 020 ----
mean loss: 162.39
 ---- batch: 030 ----
mean loss: 160.85
 ---- batch: 040 ----
mean loss: 160.99
 ---- batch: 050 ----
mean loss: 162.27
 ---- batch: 060 ----
mean loss: 165.84
 ---- batch: 070 ----
mean loss: 168.12
 ---- batch: 080 ----
mean loss: 164.01
 ---- batch: 090 ----
mean loss: 166.27
train mean loss: 164.40
epoch train time: 0:00:01.648896
elapsed time: 0:03:37.987717
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 20:05:03.156146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.04
 ---- batch: 020 ----
mean loss: 157.37
 ---- batch: 030 ----
mean loss: 165.29
 ---- batch: 040 ----
mean loss: 167.95
 ---- batch: 050 ----
mean loss: 165.94
 ---- batch: 060 ----
mean loss: 164.67
 ---- batch: 070 ----
mean loss: 163.45
 ---- batch: 080 ----
mean loss: 161.23
 ---- batch: 090 ----
mean loss: 163.52
train mean loss: 162.80
epoch train time: 0:00:01.624939
elapsed time: 0:03:39.613393
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 20:05:04.781805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.45
 ---- batch: 020 ----
mean loss: 165.70
 ---- batch: 030 ----
mean loss: 159.58
 ---- batch: 040 ----
mean loss: 153.86
 ---- batch: 050 ----
mean loss: 159.36
 ---- batch: 060 ----
mean loss: 165.68
 ---- batch: 070 ----
mean loss: 169.85
 ---- batch: 080 ----
mean loss: 159.61
 ---- batch: 090 ----
mean loss: 161.52
train mean loss: 162.01
epoch train time: 0:00:01.639280
elapsed time: 0:03:41.253323
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 20:05:06.421732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.35
 ---- batch: 020 ----
mean loss: 156.92
 ---- batch: 030 ----
mean loss: 157.43
 ---- batch: 040 ----
mean loss: 160.59
 ---- batch: 050 ----
mean loss: 159.10
 ---- batch: 060 ----
mean loss: 167.67
 ---- batch: 070 ----
mean loss: 158.61
 ---- batch: 080 ----
mean loss: 167.11
 ---- batch: 090 ----
mean loss: 162.45
train mean loss: 161.11
epoch train time: 0:00:01.658700
elapsed time: 0:03:42.912663
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 20:05:08.081049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.89
 ---- batch: 020 ----
mean loss: 154.25
 ---- batch: 030 ----
mean loss: 160.21
 ---- batch: 040 ----
mean loss: 158.26
 ---- batch: 050 ----
mean loss: 159.34
 ---- batch: 060 ----
mean loss: 163.89
 ---- batch: 070 ----
mean loss: 166.03
 ---- batch: 080 ----
mean loss: 152.74
 ---- batch: 090 ----
mean loss: 164.96
train mean loss: 160.03
epoch train time: 0:00:01.699325
elapsed time: 0:03:44.612660
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 20:05:09.781053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.44
 ---- batch: 020 ----
mean loss: 155.15
 ---- batch: 030 ----
mean loss: 157.33
 ---- batch: 040 ----
mean loss: 147.98
 ---- batch: 050 ----
mean loss: 163.30
 ---- batch: 060 ----
mean loss: 161.25
 ---- batch: 070 ----
mean loss: 163.39
 ---- batch: 080 ----
mean loss: 163.71
 ---- batch: 090 ----
mean loss: 167.87
train mean loss: 159.34
epoch train time: 0:00:01.658660
elapsed time: 0:03:46.272044
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 20:05:11.440494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.07
 ---- batch: 020 ----
mean loss: 147.08
 ---- batch: 030 ----
mean loss: 153.13
 ---- batch: 040 ----
mean loss: 157.85
 ---- batch: 050 ----
mean loss: 156.26
 ---- batch: 060 ----
mean loss: 161.37
 ---- batch: 070 ----
mean loss: 160.69
 ---- batch: 080 ----
mean loss: 161.58
 ---- batch: 090 ----
mean loss: 172.12
train mean loss: 157.60
epoch train time: 0:00:01.714994
elapsed time: 0:03:47.987713
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 20:05:13.156126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.48
 ---- batch: 020 ----
mean loss: 146.63
 ---- batch: 030 ----
mean loss: 153.69
 ---- batch: 040 ----
mean loss: 158.36
 ---- batch: 050 ----
mean loss: 158.86
 ---- batch: 060 ----
mean loss: 153.04
 ---- batch: 070 ----
mean loss: 165.16
 ---- batch: 080 ----
mean loss: 163.33
 ---- batch: 090 ----
mean loss: 170.05
train mean loss: 158.16
epoch train time: 0:00:01.736519
elapsed time: 0:03:49.724888
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 20:05:14.893280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.93
 ---- batch: 020 ----
mean loss: 159.02
 ---- batch: 030 ----
mean loss: 151.33
 ---- batch: 040 ----
mean loss: 161.15
 ---- batch: 050 ----
mean loss: 159.54
 ---- batch: 060 ----
mean loss: 157.71
 ---- batch: 070 ----
mean loss: 157.67
 ---- batch: 080 ----
mean loss: 160.29
 ---- batch: 090 ----
mean loss: 149.33
train mean loss: 156.48
epoch train time: 0:00:01.663720
elapsed time: 0:03:51.389213
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 20:05:16.557636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.75
 ---- batch: 020 ----
mean loss: 152.52
 ---- batch: 030 ----
mean loss: 155.48
 ---- batch: 040 ----
mean loss: 154.01
 ---- batch: 050 ----
mean loss: 151.23
 ---- batch: 060 ----
mean loss: 157.96
 ---- batch: 070 ----
mean loss: 155.92
 ---- batch: 080 ----
mean loss: 164.25
 ---- batch: 090 ----
mean loss: 154.08
train mean loss: 155.52
epoch train time: 0:00:01.675592
elapsed time: 0:03:53.065492
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 20:05:18.233945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.83
 ---- batch: 020 ----
mean loss: 151.32
 ---- batch: 030 ----
mean loss: 151.93
 ---- batch: 040 ----
mean loss: 153.48
 ---- batch: 050 ----
mean loss: 157.78
 ---- batch: 060 ----
mean loss: 154.23
 ---- batch: 070 ----
mean loss: 149.97
 ---- batch: 080 ----
mean loss: 161.89
 ---- batch: 090 ----
mean loss: 155.82
train mean loss: 154.18
epoch train time: 0:00:01.645555
elapsed time: 0:03:54.711757
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 20:05:19.880213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.03
 ---- batch: 020 ----
mean loss: 149.64
 ---- batch: 030 ----
mean loss: 153.74
 ---- batch: 040 ----
mean loss: 152.25
 ---- batch: 050 ----
mean loss: 157.77
 ---- batch: 060 ----
mean loss: 149.74
 ---- batch: 070 ----
mean loss: 160.39
 ---- batch: 080 ----
mean loss: 160.15
 ---- batch: 090 ----
mean loss: 152.94
train mean loss: 153.72
epoch train time: 0:00:01.695450
elapsed time: 0:03:56.407905
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 20:05:21.576323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.89
 ---- batch: 020 ----
mean loss: 151.77
 ---- batch: 030 ----
mean loss: 152.95
 ---- batch: 040 ----
mean loss: 153.26
 ---- batch: 050 ----
mean loss: 155.67
 ---- batch: 060 ----
mean loss: 153.21
 ---- batch: 070 ----
mean loss: 148.50
 ---- batch: 080 ----
mean loss: 155.38
 ---- batch: 090 ----
mean loss: 147.24
train mean loss: 152.76
epoch train time: 0:00:01.669616
elapsed time: 0:03:58.078160
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 20:05:23.246570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.58
 ---- batch: 020 ----
mean loss: 146.57
 ---- batch: 030 ----
mean loss: 147.32
 ---- batch: 040 ----
mean loss: 152.66
 ---- batch: 050 ----
mean loss: 152.93
 ---- batch: 060 ----
mean loss: 149.31
 ---- batch: 070 ----
mean loss: 155.18
 ---- batch: 080 ----
mean loss: 158.02
 ---- batch: 090 ----
mean loss: 156.46
train mean loss: 151.41
epoch train time: 0:00:01.687196
elapsed time: 0:03:59.766015
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 20:05:24.934408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.19
 ---- batch: 020 ----
mean loss: 155.20
 ---- batch: 030 ----
mean loss: 149.87
 ---- batch: 040 ----
mean loss: 149.11
 ---- batch: 050 ----
mean loss: 145.59
 ---- batch: 060 ----
mean loss: 157.11
 ---- batch: 070 ----
mean loss: 153.20
 ---- batch: 080 ----
mean loss: 152.96
 ---- batch: 090 ----
mean loss: 153.85
train mean loss: 151.16
epoch train time: 0:00:01.650247
elapsed time: 0:04:01.416861
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 20:05:26.585276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.91
 ---- batch: 020 ----
mean loss: 149.55
 ---- batch: 030 ----
mean loss: 145.73
 ---- batch: 040 ----
mean loss: 145.88
 ---- batch: 050 ----
mean loss: 149.73
 ---- batch: 060 ----
mean loss: 150.13
 ---- batch: 070 ----
mean loss: 149.33
 ---- batch: 080 ----
mean loss: 155.39
 ---- batch: 090 ----
mean loss: 157.60
train mean loss: 149.62
epoch train time: 0:00:01.652469
elapsed time: 0:04:03.069973
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 20:05:28.238392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.92
 ---- batch: 020 ----
mean loss: 145.87
 ---- batch: 030 ----
mean loss: 148.35
 ---- batch: 040 ----
mean loss: 146.75
 ---- batch: 050 ----
mean loss: 144.82
 ---- batch: 060 ----
mean loss: 150.57
 ---- batch: 070 ----
mean loss: 157.28
 ---- batch: 080 ----
mean loss: 152.42
 ---- batch: 090 ----
mean loss: 158.97
train mean loss: 149.37
epoch train time: 0:00:01.620102
elapsed time: 0:04:04.691077
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 20:05:29.859314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.51
 ---- batch: 020 ----
mean loss: 150.36
 ---- batch: 030 ----
mean loss: 145.32
 ---- batch: 040 ----
mean loss: 146.69
 ---- batch: 050 ----
mean loss: 151.49
 ---- batch: 060 ----
mean loss: 148.48
 ---- batch: 070 ----
mean loss: 143.00
 ---- batch: 080 ----
mean loss: 151.07
 ---- batch: 090 ----
mean loss: 159.52
train mean loss: 148.88
epoch train time: 0:00:01.663287
elapsed time: 0:04:06.354913
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 20:05:31.523300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.69
 ---- batch: 020 ----
mean loss: 139.09
 ---- batch: 030 ----
mean loss: 145.19
 ---- batch: 040 ----
mean loss: 146.07
 ---- batch: 050 ----
mean loss: 150.46
 ---- batch: 060 ----
mean loss: 144.08
 ---- batch: 070 ----
mean loss: 155.58
 ---- batch: 080 ----
mean loss: 152.74
 ---- batch: 090 ----
mean loss: 155.37
train mean loss: 148.73
epoch train time: 0:00:01.683878
elapsed time: 0:04:08.039482
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 20:05:33.207882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.60
 ---- batch: 020 ----
mean loss: 147.22
 ---- batch: 030 ----
mean loss: 143.04
 ---- batch: 040 ----
mean loss: 148.14
 ---- batch: 050 ----
mean loss: 143.73
 ---- batch: 060 ----
mean loss: 143.91
 ---- batch: 070 ----
mean loss: 151.52
 ---- batch: 080 ----
mean loss: 151.48
 ---- batch: 090 ----
mean loss: 152.85
train mean loss: 148.21
epoch train time: 0:00:01.636566
elapsed time: 0:04:09.676707
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 20:05:34.845110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.99
 ---- batch: 020 ----
mean loss: 146.23
 ---- batch: 030 ----
mean loss: 145.53
 ---- batch: 040 ----
mean loss: 143.18
 ---- batch: 050 ----
mean loss: 147.46
 ---- batch: 060 ----
mean loss: 144.51
 ---- batch: 070 ----
mean loss: 148.91
 ---- batch: 080 ----
mean loss: 147.92
 ---- batch: 090 ----
mean loss: 150.92
train mean loss: 145.92
epoch train time: 0:00:01.660852
elapsed time: 0:04:11.338175
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 20:05:36.506594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.36
 ---- batch: 020 ----
mean loss: 138.07
 ---- batch: 030 ----
mean loss: 146.00
 ---- batch: 040 ----
mean loss: 140.96
 ---- batch: 050 ----
mean loss: 142.87
 ---- batch: 060 ----
mean loss: 146.61
 ---- batch: 070 ----
mean loss: 149.36
 ---- batch: 080 ----
mean loss: 153.67
 ---- batch: 090 ----
mean loss: 148.00
train mean loss: 146.10
epoch train time: 0:00:01.672718
elapsed time: 0:04:13.011572
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 20:05:38.179989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.05
 ---- batch: 020 ----
mean loss: 140.11
 ---- batch: 030 ----
mean loss: 142.85
 ---- batch: 040 ----
mean loss: 144.12
 ---- batch: 050 ----
mean loss: 140.01
 ---- batch: 060 ----
mean loss: 147.83
 ---- batch: 070 ----
mean loss: 145.73
 ---- batch: 080 ----
mean loss: 148.41
 ---- batch: 090 ----
mean loss: 142.83
train mean loss: 144.52
epoch train time: 0:00:01.634907
elapsed time: 0:04:14.647222
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 20:05:39.815627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.82
 ---- batch: 020 ----
mean loss: 135.40
 ---- batch: 030 ----
mean loss: 139.93
 ---- batch: 040 ----
mean loss: 142.76
 ---- batch: 050 ----
mean loss: 144.08
 ---- batch: 060 ----
mean loss: 141.26
 ---- batch: 070 ----
mean loss: 154.67
 ---- batch: 080 ----
mean loss: 148.81
 ---- batch: 090 ----
mean loss: 148.92
train mean loss: 144.11
epoch train time: 0:00:01.646687
elapsed time: 0:04:16.294549
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 20:05:41.462993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.54
 ---- batch: 020 ----
mean loss: 145.59
 ---- batch: 030 ----
mean loss: 140.28
 ---- batch: 040 ----
mean loss: 137.82
 ---- batch: 050 ----
mean loss: 138.01
 ---- batch: 060 ----
mean loss: 148.28
 ---- batch: 070 ----
mean loss: 150.30
 ---- batch: 080 ----
mean loss: 147.93
 ---- batch: 090 ----
mean loss: 140.61
train mean loss: 143.15
epoch train time: 0:00:01.633810
elapsed time: 0:04:17.929120
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 20:05:43.097589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.62
 ---- batch: 020 ----
mean loss: 137.39
 ---- batch: 030 ----
mean loss: 137.36
 ---- batch: 040 ----
mean loss: 143.08
 ---- batch: 050 ----
mean loss: 136.28
 ---- batch: 060 ----
mean loss: 139.34
 ---- batch: 070 ----
mean loss: 143.81
 ---- batch: 080 ----
mean loss: 146.58
 ---- batch: 090 ----
mean loss: 150.14
train mean loss: 142.48
epoch train time: 0:00:01.644364
elapsed time: 0:04:19.574272
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 20:05:44.742684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.35
 ---- batch: 020 ----
mean loss: 133.65
 ---- batch: 030 ----
mean loss: 138.86
 ---- batch: 040 ----
mean loss: 141.63
 ---- batch: 050 ----
mean loss: 141.92
 ---- batch: 060 ----
mean loss: 142.53
 ---- batch: 070 ----
mean loss: 140.20
 ---- batch: 080 ----
mean loss: 144.15
 ---- batch: 090 ----
mean loss: 140.97
train mean loss: 140.78
epoch train time: 0:00:01.685010
elapsed time: 0:04:21.259918
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 20:05:46.428324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.57
 ---- batch: 020 ----
mean loss: 139.28
 ---- batch: 030 ----
mean loss: 137.53
 ---- batch: 040 ----
mean loss: 143.78
 ---- batch: 050 ----
mean loss: 140.82
 ---- batch: 060 ----
mean loss: 149.48
 ---- batch: 070 ----
mean loss: 143.66
 ---- batch: 080 ----
mean loss: 141.37
 ---- batch: 090 ----
mean loss: 143.78
train mean loss: 141.29
epoch train time: 0:00:01.676452
elapsed time: 0:04:22.937004
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 20:05:48.105398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.87
 ---- batch: 020 ----
mean loss: 137.89
 ---- batch: 030 ----
mean loss: 138.50
 ---- batch: 040 ----
mean loss: 143.27
 ---- batch: 050 ----
mean loss: 144.01
 ---- batch: 060 ----
mean loss: 141.15
 ---- batch: 070 ----
mean loss: 132.08
 ---- batch: 080 ----
mean loss: 144.36
 ---- batch: 090 ----
mean loss: 148.23
train mean loss: 140.65
epoch train time: 0:00:01.682840
elapsed time: 0:04:24.620507
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 20:05:49.788931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.33
 ---- batch: 020 ----
mean loss: 133.90
 ---- batch: 030 ----
mean loss: 142.68
 ---- batch: 040 ----
mean loss: 136.73
 ---- batch: 050 ----
mean loss: 138.38
 ---- batch: 060 ----
mean loss: 141.56
 ---- batch: 070 ----
mean loss: 142.58
 ---- batch: 080 ----
mean loss: 148.48
 ---- batch: 090 ----
mean loss: 142.18
train mean loss: 140.08
epoch train time: 0:00:01.650424
elapsed time: 0:04:26.271608
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 20:05:51.440027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.64
 ---- batch: 020 ----
mean loss: 139.67
 ---- batch: 030 ----
mean loss: 141.07
 ---- batch: 040 ----
mean loss: 136.51
 ---- batch: 050 ----
mean loss: 136.66
 ---- batch: 060 ----
mean loss: 137.68
 ---- batch: 070 ----
mean loss: 136.29
 ---- batch: 080 ----
mean loss: 143.55
 ---- batch: 090 ----
mean loss: 141.90
train mean loss: 138.67
epoch train time: 0:00:01.671353
elapsed time: 0:04:27.943679
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 20:05:53.112145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.35
 ---- batch: 020 ----
mean loss: 134.82
 ---- batch: 030 ----
mean loss: 142.51
 ---- batch: 040 ----
mean loss: 131.36
 ---- batch: 050 ----
mean loss: 145.18
 ---- batch: 060 ----
mean loss: 135.30
 ---- batch: 070 ----
mean loss: 142.00
 ---- batch: 080 ----
mean loss: 138.12
 ---- batch: 090 ----
mean loss: 143.70
train mean loss: 138.34
epoch train time: 0:00:01.669408
elapsed time: 0:04:29.613867
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 20:05:54.782319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.57
 ---- batch: 020 ----
mean loss: 136.93
 ---- batch: 030 ----
mean loss: 133.60
 ---- batch: 040 ----
mean loss: 137.05
 ---- batch: 050 ----
mean loss: 138.99
 ---- batch: 060 ----
mean loss: 139.04
 ---- batch: 070 ----
mean loss: 141.08
 ---- batch: 080 ----
mean loss: 139.44
 ---- batch: 090 ----
mean loss: 138.75
train mean loss: 137.90
epoch train time: 0:00:01.643026
elapsed time: 0:04:31.257711
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 20:05:56.426120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.87
 ---- batch: 020 ----
mean loss: 136.84
 ---- batch: 030 ----
mean loss: 128.12
 ---- batch: 040 ----
mean loss: 140.65
 ---- batch: 050 ----
mean loss: 136.46
 ---- batch: 060 ----
mean loss: 134.84
 ---- batch: 070 ----
mean loss: 145.53
 ---- batch: 080 ----
mean loss: 132.53
 ---- batch: 090 ----
mean loss: 135.92
train mean loss: 136.78
epoch train time: 0:00:01.646214
elapsed time: 0:04:32.904611
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 20:05:58.073002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.38
 ---- batch: 020 ----
mean loss: 137.09
 ---- batch: 030 ----
mean loss: 136.86
 ---- batch: 040 ----
mean loss: 137.75
 ---- batch: 050 ----
mean loss: 132.42
 ---- batch: 060 ----
mean loss: 141.32
 ---- batch: 070 ----
mean loss: 136.83
 ---- batch: 080 ----
mean loss: 134.15
 ---- batch: 090 ----
mean loss: 139.80
train mean loss: 136.57
epoch train time: 0:00:01.637113
elapsed time: 0:04:34.542380
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 20:05:59.710813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.98
 ---- batch: 020 ----
mean loss: 134.24
 ---- batch: 030 ----
mean loss: 134.27
 ---- batch: 040 ----
mean loss: 137.63
 ---- batch: 050 ----
mean loss: 143.49
 ---- batch: 060 ----
mean loss: 140.32
 ---- batch: 070 ----
mean loss: 131.95
 ---- batch: 080 ----
mean loss: 138.08
 ---- batch: 090 ----
mean loss: 133.42
train mean loss: 135.91
epoch train time: 0:00:01.685593
elapsed time: 0:04:36.228636
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 20:06:01.397054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.51
 ---- batch: 020 ----
mean loss: 128.04
 ---- batch: 030 ----
mean loss: 135.49
 ---- batch: 040 ----
mean loss: 139.24
 ---- batch: 050 ----
mean loss: 134.52
 ---- batch: 060 ----
mean loss: 138.61
 ---- batch: 070 ----
mean loss: 135.92
 ---- batch: 080 ----
mean loss: 136.53
 ---- batch: 090 ----
mean loss: 133.82
train mean loss: 135.77
epoch train time: 0:00:01.651801
elapsed time: 0:04:37.881123
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 20:06:03.049545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.65
 ---- batch: 020 ----
mean loss: 134.28
 ---- batch: 030 ----
mean loss: 132.21
 ---- batch: 040 ----
mean loss: 134.12
 ---- batch: 050 ----
mean loss: 132.00
 ---- batch: 060 ----
mean loss: 140.25
 ---- batch: 070 ----
mean loss: 133.37
 ---- batch: 080 ----
mean loss: 140.57
 ---- batch: 090 ----
mean loss: 137.62
train mean loss: 134.32
epoch train time: 0:00:01.665769
elapsed time: 0:04:39.547617
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 20:06:04.716035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.36
 ---- batch: 020 ----
mean loss: 133.23
 ---- batch: 030 ----
mean loss: 130.06
 ---- batch: 040 ----
mean loss: 132.95
 ---- batch: 050 ----
mean loss: 134.09
 ---- batch: 060 ----
mean loss: 129.73
 ---- batch: 070 ----
mean loss: 138.65
 ---- batch: 080 ----
mean loss: 137.75
 ---- batch: 090 ----
mean loss: 138.46
train mean loss: 134.50
epoch train time: 0:00:01.711847
elapsed time: 0:04:41.260423
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 20:06:06.428612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.47
 ---- batch: 020 ----
mean loss: 128.71
 ---- batch: 030 ----
mean loss: 134.42
 ---- batch: 040 ----
mean loss: 133.15
 ---- batch: 050 ----
mean loss: 133.51
 ---- batch: 060 ----
mean loss: 140.98
 ---- batch: 070 ----
mean loss: 136.35
 ---- batch: 080 ----
mean loss: 131.54
 ---- batch: 090 ----
mean loss: 138.27
train mean loss: 133.57
epoch train time: 0:00:01.661074
elapsed time: 0:04:42.921887
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 20:06:08.090315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.91
 ---- batch: 020 ----
mean loss: 136.94
 ---- batch: 030 ----
mean loss: 130.36
 ---- batch: 040 ----
mean loss: 135.77
 ---- batch: 050 ----
mean loss: 130.71
 ---- batch: 060 ----
mean loss: 129.70
 ---- batch: 070 ----
mean loss: 129.55
 ---- batch: 080 ----
mean loss: 139.96
 ---- batch: 090 ----
mean loss: 131.65
train mean loss: 133.07
epoch train time: 0:00:01.642987
elapsed time: 0:04:44.565585
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 20:06:09.734009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.03
 ---- batch: 020 ----
mean loss: 129.98
 ---- batch: 030 ----
mean loss: 129.27
 ---- batch: 040 ----
mean loss: 132.73
 ---- batch: 050 ----
mean loss: 132.92
 ---- batch: 060 ----
mean loss: 131.44
 ---- batch: 070 ----
mean loss: 133.10
 ---- batch: 080 ----
mean loss: 136.93
 ---- batch: 090 ----
mean loss: 136.80
train mean loss: 132.75
epoch train time: 0:00:01.660131
elapsed time: 0:04:46.226364
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 20:06:11.394762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.42
 ---- batch: 020 ----
mean loss: 128.12
 ---- batch: 030 ----
mean loss: 135.48
 ---- batch: 040 ----
mean loss: 131.36
 ---- batch: 050 ----
mean loss: 130.10
 ---- batch: 060 ----
mean loss: 131.40
 ---- batch: 070 ----
mean loss: 131.88
 ---- batch: 080 ----
mean loss: 132.90
 ---- batch: 090 ----
mean loss: 137.75
train mean loss: 131.46
epoch train time: 0:00:01.639514
elapsed time: 0:04:47.866509
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 20:06:13.034907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.17
 ---- batch: 020 ----
mean loss: 123.96
 ---- batch: 030 ----
mean loss: 132.38
 ---- batch: 040 ----
mean loss: 129.38
 ---- batch: 050 ----
mean loss: 127.11
 ---- batch: 060 ----
mean loss: 130.22
 ---- batch: 070 ----
mean loss: 132.04
 ---- batch: 080 ----
mean loss: 129.29
 ---- batch: 090 ----
mean loss: 136.05
train mean loss: 131.02
epoch train time: 0:00:01.641944
elapsed time: 0:04:49.509191
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 20:06:14.677631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.29
 ---- batch: 020 ----
mean loss: 129.77
 ---- batch: 030 ----
mean loss: 122.81
 ---- batch: 040 ----
mean loss: 133.07
 ---- batch: 050 ----
mean loss: 132.90
 ---- batch: 060 ----
mean loss: 136.97
 ---- batch: 070 ----
mean loss: 129.98
 ---- batch: 080 ----
mean loss: 136.78
 ---- batch: 090 ----
mean loss: 130.42
train mean loss: 130.94
epoch train time: 0:00:01.690405
elapsed time: 0:04:51.200263
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 20:06:16.368688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.78
 ---- batch: 020 ----
mean loss: 131.91
 ---- batch: 030 ----
mean loss: 129.78
 ---- batch: 040 ----
mean loss: 126.03
 ---- batch: 050 ----
mean loss: 129.91
 ---- batch: 060 ----
mean loss: 135.49
 ---- batch: 070 ----
mean loss: 131.55
 ---- batch: 080 ----
mean loss: 129.22
 ---- batch: 090 ----
mean loss: 127.40
train mean loss: 129.67
epoch train time: 0:00:01.693892
elapsed time: 0:04:52.894851
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 20:06:18.063427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.76
 ---- batch: 020 ----
mean loss: 124.21
 ---- batch: 030 ----
mean loss: 135.65
 ---- batch: 040 ----
mean loss: 126.74
 ---- batch: 050 ----
mean loss: 126.94
 ---- batch: 060 ----
mean loss: 130.97
 ---- batch: 070 ----
mean loss: 133.44
 ---- batch: 080 ----
mean loss: 139.45
 ---- batch: 090 ----
mean loss: 130.06
train mean loss: 130.11
epoch train time: 0:00:01.678118
elapsed time: 0:04:54.573809
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 20:06:19.742224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.72
 ---- batch: 020 ----
mean loss: 127.41
 ---- batch: 030 ----
mean loss: 127.14
 ---- batch: 040 ----
mean loss: 130.16
 ---- batch: 050 ----
mean loss: 130.75
 ---- batch: 060 ----
mean loss: 128.62
 ---- batch: 070 ----
mean loss: 129.19
 ---- batch: 080 ----
mean loss: 131.53
 ---- batch: 090 ----
mean loss: 129.11
train mean loss: 129.32
epoch train time: 0:00:01.676584
elapsed time: 0:04:56.251008
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 20:06:21.419412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.39
 ---- batch: 020 ----
mean loss: 123.70
 ---- batch: 030 ----
mean loss: 134.19
 ---- batch: 040 ----
mean loss: 122.62
 ---- batch: 050 ----
mean loss: 126.02
 ---- batch: 060 ----
mean loss: 126.36
 ---- batch: 070 ----
mean loss: 126.98
 ---- batch: 080 ----
mean loss: 135.39
 ---- batch: 090 ----
mean loss: 135.10
train mean loss: 128.84
epoch train time: 0:00:01.672200
elapsed time: 0:04:57.923809
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 20:06:23.092206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.82
 ---- batch: 020 ----
mean loss: 120.14
 ---- batch: 030 ----
mean loss: 124.55
 ---- batch: 040 ----
mean loss: 132.75
 ---- batch: 050 ----
mean loss: 134.77
 ---- batch: 060 ----
mean loss: 126.60
 ---- batch: 070 ----
mean loss: 129.62
 ---- batch: 080 ----
mean loss: 129.13
 ---- batch: 090 ----
mean loss: 131.79
train mean loss: 127.98
epoch train time: 0:00:01.662231
elapsed time: 0:04:59.586794
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 20:06:24.755235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.11
 ---- batch: 020 ----
mean loss: 123.66
 ---- batch: 030 ----
mean loss: 126.01
 ---- batch: 040 ----
mean loss: 126.13
 ---- batch: 050 ----
mean loss: 128.35
 ---- batch: 060 ----
mean loss: 129.81
 ---- batch: 070 ----
mean loss: 132.38
 ---- batch: 080 ----
mean loss: 130.10
 ---- batch: 090 ----
mean loss: 123.22
train mean loss: 127.40
epoch train time: 0:00:01.656047
elapsed time: 0:05:01.243564
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 20:06:26.411913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.70
 ---- batch: 020 ----
mean loss: 125.29
 ---- batch: 030 ----
mean loss: 123.42
 ---- batch: 040 ----
mean loss: 127.50
 ---- batch: 050 ----
mean loss: 124.17
 ---- batch: 060 ----
mean loss: 123.42
 ---- batch: 070 ----
mean loss: 134.62
 ---- batch: 080 ----
mean loss: 128.30
 ---- batch: 090 ----
mean loss: 126.71
train mean loss: 126.93
epoch train time: 0:00:01.658978
elapsed time: 0:05:02.903092
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 20:06:28.071510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.39
 ---- batch: 020 ----
mean loss: 126.03
 ---- batch: 030 ----
mean loss: 126.30
 ---- batch: 040 ----
mean loss: 128.37
 ---- batch: 050 ----
mean loss: 125.52
 ---- batch: 060 ----
mean loss: 128.30
 ---- batch: 070 ----
mean loss: 132.09
 ---- batch: 080 ----
mean loss: 129.62
 ---- batch: 090 ----
mean loss: 123.02
train mean loss: 126.35
epoch train time: 0:00:01.670203
elapsed time: 0:05:04.573960
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 20:06:29.742396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.11
 ---- batch: 020 ----
mean loss: 120.23
 ---- batch: 030 ----
mean loss: 123.31
 ---- batch: 040 ----
mean loss: 130.47
 ---- batch: 050 ----
mean loss: 124.62
 ---- batch: 060 ----
mean loss: 126.20
 ---- batch: 070 ----
mean loss: 129.90
 ---- batch: 080 ----
mean loss: 123.92
 ---- batch: 090 ----
mean loss: 129.99
train mean loss: 126.24
epoch train time: 0:00:01.667043
elapsed time: 0:05:06.241661
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 20:06:31.410104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.17
 ---- batch: 020 ----
mean loss: 121.77
 ---- batch: 030 ----
mean loss: 119.04
 ---- batch: 040 ----
mean loss: 115.44
 ---- batch: 050 ----
mean loss: 128.40
 ---- batch: 060 ----
mean loss: 131.78
 ---- batch: 070 ----
mean loss: 129.29
 ---- batch: 080 ----
mean loss: 128.88
 ---- batch: 090 ----
mean loss: 132.92
train mean loss: 125.72
epoch train time: 0:00:01.667302
elapsed time: 0:05:07.909721
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 20:06:33.078132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.68
 ---- batch: 020 ----
mean loss: 120.40
 ---- batch: 030 ----
mean loss: 122.86
 ---- batch: 040 ----
mean loss: 129.31
 ---- batch: 050 ----
mean loss: 127.10
 ---- batch: 060 ----
mean loss: 125.02
 ---- batch: 070 ----
mean loss: 126.96
 ---- batch: 080 ----
mean loss: 123.92
 ---- batch: 090 ----
mean loss: 126.42
train mean loss: 125.13
epoch train time: 0:00:01.700436
elapsed time: 0:05:09.610785
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 20:06:34.779205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.10
 ---- batch: 020 ----
mean loss: 126.89
 ---- batch: 030 ----
mean loss: 126.82
 ---- batch: 040 ----
mean loss: 122.67
 ---- batch: 050 ----
mean loss: 122.68
 ---- batch: 060 ----
mean loss: 120.95
 ---- batch: 070 ----
mean loss: 120.36
 ---- batch: 080 ----
mean loss: 125.12
 ---- batch: 090 ----
mean loss: 128.33
train mean loss: 124.83
epoch train time: 0:00:01.660394
elapsed time: 0:05:11.271874
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 20:06:36.440351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.09
 ---- batch: 020 ----
mean loss: 122.43
 ---- batch: 030 ----
mean loss: 127.31
 ---- batch: 040 ----
mean loss: 118.04
 ---- batch: 050 ----
mean loss: 120.49
 ---- batch: 060 ----
mean loss: 121.95
 ---- batch: 070 ----
mean loss: 131.83
 ---- batch: 080 ----
mean loss: 127.12
 ---- batch: 090 ----
mean loss: 127.01
train mean loss: 124.55
epoch train time: 0:00:01.669051
elapsed time: 0:05:12.941728
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 20:06:38.110138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.31
 ---- batch: 020 ----
mean loss: 124.43
 ---- batch: 030 ----
mean loss: 122.99
 ---- batch: 040 ----
mean loss: 118.04
 ---- batch: 050 ----
mean loss: 128.87
 ---- batch: 060 ----
mean loss: 126.09
 ---- batch: 070 ----
mean loss: 124.11
 ---- batch: 080 ----
mean loss: 125.99
 ---- batch: 090 ----
mean loss: 124.01
train mean loss: 123.82
epoch train time: 0:00:01.677632
elapsed time: 0:05:14.619980
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 20:06:39.788378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.29
 ---- batch: 020 ----
mean loss: 121.43
 ---- batch: 030 ----
mean loss: 117.37
 ---- batch: 040 ----
mean loss: 120.85
 ---- batch: 050 ----
mean loss: 123.62
 ---- batch: 060 ----
mean loss: 126.66
 ---- batch: 070 ----
mean loss: 123.47
 ---- batch: 080 ----
mean loss: 126.50
 ---- batch: 090 ----
mean loss: 128.79
train mean loss: 123.00
epoch train time: 0:00:01.685588
elapsed time: 0:05:16.306169
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 20:06:41.474567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.72
 ---- batch: 020 ----
mean loss: 122.14
 ---- batch: 030 ----
mean loss: 118.77
 ---- batch: 040 ----
mean loss: 122.30
 ---- batch: 050 ----
mean loss: 127.33
 ---- batch: 060 ----
mean loss: 126.16
 ---- batch: 070 ----
mean loss: 123.96
 ---- batch: 080 ----
mean loss: 122.26
 ---- batch: 090 ----
mean loss: 122.80
train mean loss: 122.31
epoch train time: 0:00:01.684982
elapsed time: 0:05:17.991915
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 20:06:43.160371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.36
 ---- batch: 020 ----
mean loss: 115.69
 ---- batch: 030 ----
mean loss: 123.89
 ---- batch: 040 ----
mean loss: 120.09
 ---- batch: 050 ----
mean loss: 120.47
 ---- batch: 060 ----
mean loss: 123.75
 ---- batch: 070 ----
mean loss: 126.02
 ---- batch: 080 ----
mean loss: 120.43
 ---- batch: 090 ----
mean loss: 126.53
train mean loss: 121.83
epoch train time: 0:00:01.672241
elapsed time: 0:05:19.664863
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 20:06:44.833284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.14
 ---- batch: 020 ----
mean loss: 116.23
 ---- batch: 030 ----
mean loss: 120.64
 ---- batch: 040 ----
mean loss: 122.42
 ---- batch: 050 ----
mean loss: 117.61
 ---- batch: 060 ----
mean loss: 120.84
 ---- batch: 070 ----
mean loss: 119.75
 ---- batch: 080 ----
mean loss: 124.96
 ---- batch: 090 ----
mean loss: 128.03
train mean loss: 121.96
epoch train time: 0:00:01.720930
elapsed time: 0:05:21.386499
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 20:06:46.554908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.99
 ---- batch: 020 ----
mean loss: 118.69
 ---- batch: 030 ----
mean loss: 124.48
 ---- batch: 040 ----
mean loss: 119.73
 ---- batch: 050 ----
mean loss: 120.08
 ---- batch: 060 ----
mean loss: 126.40
 ---- batch: 070 ----
mean loss: 123.42
 ---- batch: 080 ----
mean loss: 122.61
 ---- batch: 090 ----
mean loss: 124.88
train mean loss: 121.77
epoch train time: 0:00:01.649536
elapsed time: 0:05:23.036990
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 20:06:48.205170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.34
 ---- batch: 020 ----
mean loss: 121.37
 ---- batch: 030 ----
mean loss: 123.74
 ---- batch: 040 ----
mean loss: 121.12
 ---- batch: 050 ----
mean loss: 120.29
 ---- batch: 060 ----
mean loss: 122.32
 ---- batch: 070 ----
mean loss: 120.01
 ---- batch: 080 ----
mean loss: 124.29
 ---- batch: 090 ----
mean loss: 120.37
train mean loss: 120.94
epoch train time: 0:00:01.728382
elapsed time: 0:05:24.765770
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 20:06:49.934176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.98
 ---- batch: 020 ----
mean loss: 109.36
 ---- batch: 030 ----
mean loss: 119.80
 ---- batch: 040 ----
mean loss: 122.78
 ---- batch: 050 ----
mean loss: 122.66
 ---- batch: 060 ----
mean loss: 123.64
 ---- batch: 070 ----
mean loss: 122.16
 ---- batch: 080 ----
mean loss: 122.43
 ---- batch: 090 ----
mean loss: 122.99
train mean loss: 120.52
epoch train time: 0:00:01.665655
elapsed time: 0:05:26.432047
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 20:06:51.600440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.72
 ---- batch: 020 ----
mean loss: 117.28
 ---- batch: 030 ----
mean loss: 112.49
 ---- batch: 040 ----
mean loss: 117.37
 ---- batch: 050 ----
mean loss: 122.61
 ---- batch: 060 ----
mean loss: 121.38
 ---- batch: 070 ----
mean loss: 120.06
 ---- batch: 080 ----
mean loss: 118.74
 ---- batch: 090 ----
mean loss: 128.01
train mean loss: 119.73
epoch train time: 0:00:01.639512
elapsed time: 0:05:28.072194
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 20:06:53.240635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.26
 ---- batch: 020 ----
mean loss: 114.97
 ---- batch: 030 ----
mean loss: 119.38
 ---- batch: 040 ----
mean loss: 119.56
 ---- batch: 050 ----
mean loss: 115.07
 ---- batch: 060 ----
mean loss: 118.99
 ---- batch: 070 ----
mean loss: 124.74
 ---- batch: 080 ----
mean loss: 117.47
 ---- batch: 090 ----
mean loss: 116.03
train mean loss: 118.95
epoch train time: 0:00:01.658636
elapsed time: 0:05:29.731536
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 20:06:54.899930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.11
 ---- batch: 020 ----
mean loss: 119.73
 ---- batch: 030 ----
mean loss: 117.63
 ---- batch: 040 ----
mean loss: 114.40
 ---- batch: 050 ----
mean loss: 118.63
 ---- batch: 060 ----
mean loss: 122.29
 ---- batch: 070 ----
mean loss: 119.65
 ---- batch: 080 ----
mean loss: 118.87
 ---- batch: 090 ----
mean loss: 124.14
train mean loss: 118.92
epoch train time: 0:00:01.675100
elapsed time: 0:05:31.407267
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 20:06:56.575705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.22
 ---- batch: 020 ----
mean loss: 115.80
 ---- batch: 030 ----
mean loss: 119.82
 ---- batch: 040 ----
mean loss: 116.36
 ---- batch: 050 ----
mean loss: 118.48
 ---- batch: 060 ----
mean loss: 117.39
 ---- batch: 070 ----
mean loss: 119.56
 ---- batch: 080 ----
mean loss: 123.11
 ---- batch: 090 ----
mean loss: 124.31
train mean loss: 118.71
epoch train time: 0:00:01.679991
elapsed time: 0:05:33.087931
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 20:06:58.256343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.51
 ---- batch: 020 ----
mean loss: 113.62
 ---- batch: 030 ----
mean loss: 116.73
 ---- batch: 040 ----
mean loss: 118.83
 ---- batch: 050 ----
mean loss: 117.31
 ---- batch: 060 ----
mean loss: 115.18
 ---- batch: 070 ----
mean loss: 121.55
 ---- batch: 080 ----
mean loss: 119.57
 ---- batch: 090 ----
mean loss: 122.96
train mean loss: 118.32
epoch train time: 0:00:01.669756
elapsed time: 0:05:34.758321
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 20:06:59.926764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.69
 ---- batch: 020 ----
mean loss: 117.69
 ---- batch: 030 ----
mean loss: 113.66
 ---- batch: 040 ----
mean loss: 116.80
 ---- batch: 050 ----
mean loss: 118.56
 ---- batch: 060 ----
mean loss: 117.28
 ---- batch: 070 ----
mean loss: 120.65
 ---- batch: 080 ----
mean loss: 121.24
 ---- batch: 090 ----
mean loss: 119.06
train mean loss: 117.51
epoch train time: 0:00:01.671076
elapsed time: 0:05:36.430080
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 20:07:01.598557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.74
 ---- batch: 020 ----
mean loss: 116.14
 ---- batch: 030 ----
mean loss: 117.85
 ---- batch: 040 ----
mean loss: 114.78
 ---- batch: 050 ----
mean loss: 115.81
 ---- batch: 060 ----
mean loss: 121.26
 ---- batch: 070 ----
mean loss: 113.38
 ---- batch: 080 ----
mean loss: 120.32
 ---- batch: 090 ----
mean loss: 117.90
train mean loss: 116.73
epoch train time: 0:00:01.654488
elapsed time: 0:05:38.085278
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 20:07:03.253726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.34
 ---- batch: 020 ----
mean loss: 115.48
 ---- batch: 030 ----
mean loss: 110.86
 ---- batch: 040 ----
mean loss: 115.69
 ---- batch: 050 ----
mean loss: 115.87
 ---- batch: 060 ----
mean loss: 114.07
 ---- batch: 070 ----
mean loss: 122.19
 ---- batch: 080 ----
mean loss: 119.29
 ---- batch: 090 ----
mean loss: 119.54
train mean loss: 116.98
epoch train time: 0:00:01.710773
elapsed time: 0:05:39.796725
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 20:07:04.965117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.24
 ---- batch: 020 ----
mean loss: 115.28
 ---- batch: 030 ----
mean loss: 110.70
 ---- batch: 040 ----
mean loss: 117.40
 ---- batch: 050 ----
mean loss: 119.30
 ---- batch: 060 ----
mean loss: 120.52
 ---- batch: 070 ----
mean loss: 112.81
 ---- batch: 080 ----
mean loss: 118.48
 ---- batch: 090 ----
mean loss: 115.75
train mean loss: 116.41
epoch train time: 0:00:01.656338
elapsed time: 0:05:41.453747
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 20:07:06.622164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.25
 ---- batch: 020 ----
mean loss: 109.43
 ---- batch: 030 ----
mean loss: 113.11
 ---- batch: 040 ----
mean loss: 116.64
 ---- batch: 050 ----
mean loss: 122.38
 ---- batch: 060 ----
mean loss: 118.58
 ---- batch: 070 ----
mean loss: 117.77
 ---- batch: 080 ----
mean loss: 112.09
 ---- batch: 090 ----
mean loss: 117.38
train mean loss: 115.35
epoch train time: 0:00:01.675771
elapsed time: 0:05:43.130154
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 20:07:08.298574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.83
 ---- batch: 020 ----
mean loss: 111.74
 ---- batch: 030 ----
mean loss: 113.72
 ---- batch: 040 ----
mean loss: 116.01
 ---- batch: 050 ----
mean loss: 113.45
 ---- batch: 060 ----
mean loss: 116.33
 ---- batch: 070 ----
mean loss: 126.25
 ---- batch: 080 ----
mean loss: 112.67
 ---- batch: 090 ----
mean loss: 119.13
train mean loss: 115.97
epoch train time: 0:00:01.663376
elapsed time: 0:05:44.794299
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 20:07:09.963680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.56
 ---- batch: 020 ----
mean loss: 111.68
 ---- batch: 030 ----
mean loss: 112.53
 ---- batch: 040 ----
mean loss: 115.85
 ---- batch: 050 ----
mean loss: 112.03
 ---- batch: 060 ----
mean loss: 118.74
 ---- batch: 070 ----
mean loss: 114.11
 ---- batch: 080 ----
mean loss: 116.87
 ---- batch: 090 ----
mean loss: 121.65
train mean loss: 115.61
epoch train time: 0:00:01.674346
elapsed time: 0:05:46.470262
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 20:07:11.638671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.04
 ---- batch: 020 ----
mean loss: 112.31
 ---- batch: 030 ----
mean loss: 111.75
 ---- batch: 040 ----
mean loss: 111.59
 ---- batch: 050 ----
mean loss: 114.64
 ---- batch: 060 ----
mean loss: 116.28
 ---- batch: 070 ----
mean loss: 113.68
 ---- batch: 080 ----
mean loss: 121.44
 ---- batch: 090 ----
mean loss: 115.90
train mean loss: 114.53
epoch train time: 0:00:01.698999
elapsed time: 0:05:48.169928
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 20:07:13.338345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.19
 ---- batch: 020 ----
mean loss: 108.51
 ---- batch: 030 ----
mean loss: 114.02
 ---- batch: 040 ----
mean loss: 111.56
 ---- batch: 050 ----
mean loss: 114.22
 ---- batch: 060 ----
mean loss: 115.59
 ---- batch: 070 ----
mean loss: 115.35
 ---- batch: 080 ----
mean loss: 124.91
 ---- batch: 090 ----
mean loss: 119.96
train mean loss: 114.73
epoch train time: 0:00:01.664564
elapsed time: 0:05:49.835135
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 20:07:15.003542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.88
 ---- batch: 020 ----
mean loss: 116.92
 ---- batch: 030 ----
mean loss: 116.13
 ---- batch: 040 ----
mean loss: 111.59
 ---- batch: 050 ----
mean loss: 115.87
 ---- batch: 060 ----
mean loss: 115.89
 ---- batch: 070 ----
mean loss: 115.55
 ---- batch: 080 ----
mean loss: 120.21
 ---- batch: 090 ----
mean loss: 110.35
train mean loss: 113.83
epoch train time: 0:00:01.664001
elapsed time: 0:05:51.499849
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 20:07:16.668261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.89
 ---- batch: 020 ----
mean loss: 108.02
 ---- batch: 030 ----
mean loss: 114.66
 ---- batch: 040 ----
mean loss: 111.96
 ---- batch: 050 ----
mean loss: 112.22
 ---- batch: 060 ----
mean loss: 116.84
 ---- batch: 070 ----
mean loss: 119.76
 ---- batch: 080 ----
mean loss: 119.86
 ---- batch: 090 ----
mean loss: 113.19
train mean loss: 114.05
epoch train time: 0:00:01.694797
elapsed time: 0:05:53.195358
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 20:07:18.363837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.83
 ---- batch: 020 ----
mean loss: 112.92
 ---- batch: 030 ----
mean loss: 113.28
 ---- batch: 040 ----
mean loss: 113.86
 ---- batch: 050 ----
mean loss: 115.10
 ---- batch: 060 ----
mean loss: 116.82
 ---- batch: 070 ----
mean loss: 111.84
 ---- batch: 080 ----
mean loss: 112.29
 ---- batch: 090 ----
mean loss: 113.36
train mean loss: 113.30
epoch train time: 0:00:01.656858
elapsed time: 0:05:54.852993
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 20:07:20.021405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.32
 ---- batch: 020 ----
mean loss: 106.26
 ---- batch: 030 ----
mean loss: 112.29
 ---- batch: 040 ----
mean loss: 111.38
 ---- batch: 050 ----
mean loss: 113.87
 ---- batch: 060 ----
mean loss: 108.52
 ---- batch: 070 ----
mean loss: 115.30
 ---- batch: 080 ----
mean loss: 114.18
 ---- batch: 090 ----
mean loss: 120.27
train mean loss: 113.09
epoch train time: 0:00:01.682989
elapsed time: 0:05:56.536650
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 20:07:21.705159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.18
 ---- batch: 020 ----
mean loss: 109.17
 ---- batch: 030 ----
mean loss: 110.54
 ---- batch: 040 ----
mean loss: 113.58
 ---- batch: 050 ----
mean loss: 107.55
 ---- batch: 060 ----
mean loss: 111.61
 ---- batch: 070 ----
mean loss: 112.28
 ---- batch: 080 ----
mean loss: 119.89
 ---- batch: 090 ----
mean loss: 117.19
train mean loss: 112.73
epoch train time: 0:00:01.696479
elapsed time: 0:05:58.233841
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 20:07:23.402273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.93
 ---- batch: 020 ----
mean loss: 109.09
 ---- batch: 030 ----
mean loss: 111.25
 ---- batch: 040 ----
mean loss: 110.65
 ---- batch: 050 ----
mean loss: 109.09
 ---- batch: 060 ----
mean loss: 110.37
 ---- batch: 070 ----
mean loss: 115.00
 ---- batch: 080 ----
mean loss: 116.76
 ---- batch: 090 ----
mean loss: 112.91
train mean loss: 111.90
epoch train time: 0:00:01.662062
elapsed time: 0:05:59.896689
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 20:07:25.065150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.57
 ---- batch: 020 ----
mean loss: 110.76
 ---- batch: 030 ----
mean loss: 108.15
 ---- batch: 040 ----
mean loss: 110.83
 ---- batch: 050 ----
mean loss: 107.74
 ---- batch: 060 ----
mean loss: 114.48
 ---- batch: 070 ----
mean loss: 112.22
 ---- batch: 080 ----
mean loss: 108.14
 ---- batch: 090 ----
mean loss: 113.85
train mean loss: 111.68
epoch train time: 0:00:01.652819
elapsed time: 0:06:01.550258
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 20:07:26.718670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.23
 ---- batch: 020 ----
mean loss: 113.15
 ---- batch: 030 ----
mean loss: 107.18
 ---- batch: 040 ----
mean loss: 104.34
 ---- batch: 050 ----
mean loss: 109.40
 ---- batch: 060 ----
mean loss: 112.81
 ---- batch: 070 ----
mean loss: 113.38
 ---- batch: 080 ----
mean loss: 111.34
 ---- batch: 090 ----
mean loss: 119.24
train mean loss: 111.16
epoch train time: 0:00:01.638680
elapsed time: 0:06:03.189633
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 20:07:28.358076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.04
 ---- batch: 020 ----
mean loss: 106.27
 ---- batch: 030 ----
mean loss: 109.60
 ---- batch: 040 ----
mean loss: 110.27
 ---- batch: 050 ----
mean loss: 112.68
 ---- batch: 060 ----
mean loss: 111.01
 ---- batch: 070 ----
mean loss: 109.25
 ---- batch: 080 ----
mean loss: 119.50
 ---- batch: 090 ----
mean loss: 117.38
train mean loss: 110.84
epoch train time: 0:00:01.670253
elapsed time: 0:06:04.860579
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 20:07:30.028982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.28
 ---- batch: 020 ----
mean loss: 105.94
 ---- batch: 030 ----
mean loss: 104.94
 ---- batch: 040 ----
mean loss: 112.05
 ---- batch: 050 ----
mean loss: 109.21
 ---- batch: 060 ----
mean loss: 112.43
 ---- batch: 070 ----
mean loss: 110.80
 ---- batch: 080 ----
mean loss: 113.05
 ---- batch: 090 ----
mean loss: 112.18
train mean loss: 110.40
epoch train time: 0:00:01.669644
elapsed time: 0:06:06.530898
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 20:07:31.699309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.86
 ---- batch: 020 ----
mean loss: 111.22
 ---- batch: 030 ----
mean loss: 103.15
 ---- batch: 040 ----
mean loss: 106.37
 ---- batch: 050 ----
mean loss: 109.70
 ---- batch: 060 ----
mean loss: 107.87
 ---- batch: 070 ----
mean loss: 116.51
 ---- batch: 080 ----
mean loss: 117.31
 ---- batch: 090 ----
mean loss: 114.80
train mean loss: 110.54
epoch train time: 0:00:01.646056
elapsed time: 0:06:08.177613
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 20:07:33.346016
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.29
 ---- batch: 020 ----
mean loss: 103.64
 ---- batch: 030 ----
mean loss: 100.12
 ---- batch: 040 ----
mean loss: 110.40
 ---- batch: 050 ----
mean loss: 103.57
 ---- batch: 060 ----
mean loss: 104.50
 ---- batch: 070 ----
mean loss: 102.86
 ---- batch: 080 ----
mean loss: 107.02
 ---- batch: 090 ----
mean loss: 106.15
train mean loss: 104.99
epoch train time: 0:00:01.700584
elapsed time: 0:06:09.879106
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 20:07:35.047287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.83
 ---- batch: 020 ----
mean loss: 97.91
 ---- batch: 030 ----
mean loss: 104.24
 ---- batch: 040 ----
mean loss: 102.54
 ---- batch: 050 ----
mean loss: 104.67
 ---- batch: 060 ----
mean loss: 107.13
 ---- batch: 070 ----
mean loss: 103.46
 ---- batch: 080 ----
mean loss: 106.13
 ---- batch: 090 ----
mean loss: 101.36
train mean loss: 104.18
epoch train time: 0:00:01.659014
elapsed time: 0:06:11.538532
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 20:07:36.706933
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.02
 ---- batch: 020 ----
mean loss: 101.65
 ---- batch: 030 ----
mean loss: 97.96
 ---- batch: 040 ----
mean loss: 108.05
 ---- batch: 050 ----
mean loss: 108.53
 ---- batch: 060 ----
mean loss: 98.13
 ---- batch: 070 ----
mean loss: 104.08
 ---- batch: 080 ----
mean loss: 105.86
 ---- batch: 090 ----
mean loss: 106.53
train mean loss: 103.89
epoch train time: 0:00:01.678199
elapsed time: 0:06:13.217336
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 20:07:38.385761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.13
 ---- batch: 020 ----
mean loss: 101.06
 ---- batch: 030 ----
mean loss: 106.80
 ---- batch: 040 ----
mean loss: 96.94
 ---- batch: 050 ----
mean loss: 103.23
 ---- batch: 060 ----
mean loss: 105.40
 ---- batch: 070 ----
mean loss: 104.27
 ---- batch: 080 ----
mean loss: 108.57
 ---- batch: 090 ----
mean loss: 99.88
train mean loss: 103.58
epoch train time: 0:00:01.672736
elapsed time: 0:06:14.890721
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 20:07:40.059139
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.01
 ---- batch: 020 ----
mean loss: 105.92
 ---- batch: 030 ----
mean loss: 104.32
 ---- batch: 040 ----
mean loss: 98.57
 ---- batch: 050 ----
mean loss: 104.38
 ---- batch: 060 ----
mean loss: 102.77
 ---- batch: 070 ----
mean loss: 105.87
 ---- batch: 080 ----
mean loss: 110.36
 ---- batch: 090 ----
mean loss: 100.10
train mean loss: 103.47
epoch train time: 0:00:01.646044
elapsed time: 0:06:16.537429
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 20:07:41.705883
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.75
 ---- batch: 020 ----
mean loss: 97.79
 ---- batch: 030 ----
mean loss: 104.77
 ---- batch: 040 ----
mean loss: 101.27
 ---- batch: 050 ----
mean loss: 107.49
 ---- batch: 060 ----
mean loss: 105.58
 ---- batch: 070 ----
mean loss: 104.74
 ---- batch: 080 ----
mean loss: 102.68
 ---- batch: 090 ----
mean loss: 106.90
train mean loss: 103.46
epoch train time: 0:00:01.658371
elapsed time: 0:06:18.196552
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 20:07:43.364989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.42
 ---- batch: 020 ----
mean loss: 101.11
 ---- batch: 030 ----
mean loss: 104.86
 ---- batch: 040 ----
mean loss: 106.06
 ---- batch: 050 ----
mean loss: 103.58
 ---- batch: 060 ----
mean loss: 100.29
 ---- batch: 070 ----
mean loss: 100.08
 ---- batch: 080 ----
mean loss: 105.78
 ---- batch: 090 ----
mean loss: 104.69
train mean loss: 103.49
epoch train time: 0:00:01.643588
elapsed time: 0:06:19.840862
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 20:07:45.009267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.14
 ---- batch: 020 ----
mean loss: 109.30
 ---- batch: 030 ----
mean loss: 103.83
 ---- batch: 040 ----
mean loss: 103.93
 ---- batch: 050 ----
mean loss: 104.39
 ---- batch: 060 ----
mean loss: 98.06
 ---- batch: 070 ----
mean loss: 100.99
 ---- batch: 080 ----
mean loss: 103.52
 ---- batch: 090 ----
mean loss: 100.84
train mean loss: 103.49
epoch train time: 0:00:01.650395
elapsed time: 0:06:21.491957
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 20:07:46.660425
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.30
 ---- batch: 020 ----
mean loss: 105.10
 ---- batch: 030 ----
mean loss: 101.59
 ---- batch: 040 ----
mean loss: 104.28
 ---- batch: 050 ----
mean loss: 103.11
 ---- batch: 060 ----
mean loss: 104.24
 ---- batch: 070 ----
mean loss: 97.82
 ---- batch: 080 ----
mean loss: 101.04
 ---- batch: 090 ----
mean loss: 104.09
train mean loss: 103.54
epoch train time: 0:00:01.690117
elapsed time: 0:06:23.182787
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 20:07:48.351236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.16
 ---- batch: 020 ----
mean loss: 103.07
 ---- batch: 030 ----
mean loss: 108.25
 ---- batch: 040 ----
mean loss: 104.28
 ---- batch: 050 ----
mean loss: 99.10
 ---- batch: 060 ----
mean loss: 102.80
 ---- batch: 070 ----
mean loss: 103.64
 ---- batch: 080 ----
mean loss: 106.46
 ---- batch: 090 ----
mean loss: 100.19
train mean loss: 103.14
epoch train time: 0:00:01.662362
elapsed time: 0:06:24.845948
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 20:07:50.014376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.36
 ---- batch: 020 ----
mean loss: 99.11
 ---- batch: 030 ----
mean loss: 102.94
 ---- batch: 040 ----
mean loss: 107.18
 ---- batch: 050 ----
mean loss: 104.69
 ---- batch: 060 ----
mean loss: 105.15
 ---- batch: 070 ----
mean loss: 104.23
 ---- batch: 080 ----
mean loss: 100.67
 ---- batch: 090 ----
mean loss: 104.46
train mean loss: 103.22
epoch train time: 0:00:01.679323
elapsed time: 0:06:26.525975
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 20:07:51.694387
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.75
 ---- batch: 020 ----
mean loss: 99.10
 ---- batch: 030 ----
mean loss: 103.79
 ---- batch: 040 ----
mean loss: 107.63
 ---- batch: 050 ----
mean loss: 104.46
 ---- batch: 060 ----
mean loss: 101.77
 ---- batch: 070 ----
mean loss: 103.67
 ---- batch: 080 ----
mean loss: 106.18
 ---- batch: 090 ----
mean loss: 98.04
train mean loss: 103.41
epoch train time: 0:00:01.690890
elapsed time: 0:06:28.217486
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 20:07:53.385932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.35
 ---- batch: 020 ----
mean loss: 99.00
 ---- batch: 030 ----
mean loss: 103.74
 ---- batch: 040 ----
mean loss: 105.60
 ---- batch: 050 ----
mean loss: 106.10
 ---- batch: 060 ----
mean loss: 104.15
 ---- batch: 070 ----
mean loss: 103.78
 ---- batch: 080 ----
mean loss: 105.35
 ---- batch: 090 ----
mean loss: 98.22
train mean loss: 103.31
epoch train time: 0:00:01.700263
elapsed time: 0:06:29.918509
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 20:07:55.086905
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.58
 ---- batch: 020 ----
mean loss: 94.15
 ---- batch: 030 ----
mean loss: 101.83
 ---- batch: 040 ----
mean loss: 102.26
 ---- batch: 050 ----
mean loss: 100.14
 ---- batch: 060 ----
mean loss: 107.51
 ---- batch: 070 ----
mean loss: 104.19
 ---- batch: 080 ----
mean loss: 107.37
 ---- batch: 090 ----
mean loss: 106.89
train mean loss: 103.20
epoch train time: 0:00:01.667460
elapsed time: 0:06:31.586614
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 20:07:56.755030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.65
 ---- batch: 020 ----
mean loss: 104.29
 ---- batch: 030 ----
mean loss: 99.73
 ---- batch: 040 ----
mean loss: 98.83
 ---- batch: 050 ----
mean loss: 101.36
 ---- batch: 060 ----
mean loss: 103.67
 ---- batch: 070 ----
mean loss: 101.57
 ---- batch: 080 ----
mean loss: 106.91
 ---- batch: 090 ----
mean loss: 104.62
train mean loss: 103.05
epoch train time: 0:00:01.647718
elapsed time: 0:06:33.235011
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 20:07:58.403420
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.71
 ---- batch: 020 ----
mean loss: 99.82
 ---- batch: 030 ----
mean loss: 100.22
 ---- batch: 040 ----
mean loss: 106.49
 ---- batch: 050 ----
mean loss: 101.30
 ---- batch: 060 ----
mean loss: 100.84
 ---- batch: 070 ----
mean loss: 104.87
 ---- batch: 080 ----
mean loss: 106.18
 ---- batch: 090 ----
mean loss: 104.86
train mean loss: 102.95
epoch train time: 0:00:01.637883
elapsed time: 0:06:34.873503
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 20:08:00.041926
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.78
 ---- batch: 020 ----
mean loss: 104.41
 ---- batch: 030 ----
mean loss: 104.29
 ---- batch: 040 ----
mean loss: 97.48
 ---- batch: 050 ----
mean loss: 103.35
 ---- batch: 060 ----
mean loss: 103.42
 ---- batch: 070 ----
mean loss: 105.79
 ---- batch: 080 ----
mean loss: 103.92
 ---- batch: 090 ----
mean loss: 106.90
train mean loss: 102.96
epoch train time: 0:00:01.632148
elapsed time: 0:06:36.506288
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 20:08:01.674677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.47
 ---- batch: 020 ----
mean loss: 95.72
 ---- batch: 030 ----
mean loss: 103.99
 ---- batch: 040 ----
mean loss: 103.54
 ---- batch: 050 ----
mean loss: 102.49
 ---- batch: 060 ----
mean loss: 104.44
 ---- batch: 070 ----
mean loss: 100.15
 ---- batch: 080 ----
mean loss: 106.42
 ---- batch: 090 ----
mean loss: 109.49
train mean loss: 103.02
epoch train time: 0:00:01.658625
elapsed time: 0:06:38.165606
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 20:08:03.334002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.43
 ---- batch: 020 ----
mean loss: 104.07
 ---- batch: 030 ----
mean loss: 104.75
 ---- batch: 040 ----
mean loss: 107.27
 ---- batch: 050 ----
mean loss: 107.10
 ---- batch: 060 ----
mean loss: 98.90
 ---- batch: 070 ----
mean loss: 99.43
 ---- batch: 080 ----
mean loss: 99.37
 ---- batch: 090 ----
mean loss: 103.17
train mean loss: 103.01
epoch train time: 0:00:01.669033
elapsed time: 0:06:39.835289
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 20:08:05.003682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.27
 ---- batch: 020 ----
mean loss: 103.59
 ---- batch: 030 ----
mean loss: 98.79
 ---- batch: 040 ----
mean loss: 104.10
 ---- batch: 050 ----
mean loss: 104.52
 ---- batch: 060 ----
mean loss: 103.79
 ---- batch: 070 ----
mean loss: 100.50
 ---- batch: 080 ----
mean loss: 104.32
 ---- batch: 090 ----
mean loss: 98.94
train mean loss: 102.73
epoch train time: 0:00:01.644878
elapsed time: 0:06:41.480761
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 20:08:06.649217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.98
 ---- batch: 020 ----
mean loss: 100.57
 ---- batch: 030 ----
mean loss: 102.60
 ---- batch: 040 ----
mean loss: 102.90
 ---- batch: 050 ----
mean loss: 104.30
 ---- batch: 060 ----
mean loss: 103.92
 ---- batch: 070 ----
mean loss: 103.32
 ---- batch: 080 ----
mean loss: 106.16
 ---- batch: 090 ----
mean loss: 96.91
train mean loss: 102.75
epoch train time: 0:00:01.668340
elapsed time: 0:06:43.149785
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 20:08:08.318189
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.03
 ---- batch: 020 ----
mean loss: 101.61
 ---- batch: 030 ----
mean loss: 102.58
 ---- batch: 040 ----
mean loss: 100.43
 ---- batch: 050 ----
mean loss: 97.07
 ---- batch: 060 ----
mean loss: 104.96
 ---- batch: 070 ----
mean loss: 107.48
 ---- batch: 080 ----
mean loss: 103.34
 ---- batch: 090 ----
mean loss: 102.81
train mean loss: 102.76
epoch train time: 0:00:01.687134
elapsed time: 0:06:44.837593
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 20:08:10.005862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.89
 ---- batch: 020 ----
mean loss: 102.69
 ---- batch: 030 ----
mean loss: 108.00
 ---- batch: 040 ----
mean loss: 107.65
 ---- batch: 050 ----
mean loss: 103.56
 ---- batch: 060 ----
mean loss: 99.32
 ---- batch: 070 ----
mean loss: 99.72
 ---- batch: 080 ----
mean loss: 99.58
 ---- batch: 090 ----
mean loss: 106.11
train mean loss: 102.74
epoch train time: 0:00:01.669602
elapsed time: 0:06:46.507710
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 20:08:11.676137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.77
 ---- batch: 020 ----
mean loss: 94.61
 ---- batch: 030 ----
mean loss: 106.65
 ---- batch: 040 ----
mean loss: 103.32
 ---- batch: 050 ----
mean loss: 100.72
 ---- batch: 060 ----
mean loss: 104.54
 ---- batch: 070 ----
mean loss: 100.04
 ---- batch: 080 ----
mean loss: 107.06
 ---- batch: 090 ----
mean loss: 105.74
train mean loss: 102.70
epoch train time: 0:00:01.690864
elapsed time: 0:06:48.199214
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 20:08:13.367618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.81
 ---- batch: 020 ----
mean loss: 106.35
 ---- batch: 030 ----
mean loss: 98.53
 ---- batch: 040 ----
mean loss: 102.44
 ---- batch: 050 ----
mean loss: 104.74
 ---- batch: 060 ----
mean loss: 100.26
 ---- batch: 070 ----
mean loss: 99.15
 ---- batch: 080 ----
mean loss: 102.21
 ---- batch: 090 ----
mean loss: 104.03
train mean loss: 102.60
epoch train time: 0:00:01.656754
elapsed time: 0:06:49.856617
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 20:08:15.025008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.28
 ---- batch: 020 ----
mean loss: 103.72
 ---- batch: 030 ----
mean loss: 101.83
 ---- batch: 040 ----
mean loss: 100.25
 ---- batch: 050 ----
mean loss: 101.43
 ---- batch: 060 ----
mean loss: 106.08
 ---- batch: 070 ----
mean loss: 99.34
 ---- batch: 080 ----
mean loss: 102.76
 ---- batch: 090 ----
mean loss: 103.72
train mean loss: 102.52
epoch train time: 0:00:01.676923
elapsed time: 0:06:51.534186
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 20:08:16.702648
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.03
 ---- batch: 020 ----
mean loss: 104.42
 ---- batch: 030 ----
mean loss: 101.89
 ---- batch: 040 ----
mean loss: 103.24
 ---- batch: 050 ----
mean loss: 105.46
 ---- batch: 060 ----
mean loss: 102.84
 ---- batch: 070 ----
mean loss: 103.46
 ---- batch: 080 ----
mean loss: 104.75
 ---- batch: 090 ----
mean loss: 96.43
train mean loss: 102.68
epoch train time: 0:00:01.649665
elapsed time: 0:06:53.184627
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 20:08:18.353094
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.31
 ---- batch: 020 ----
mean loss: 100.88
 ---- batch: 030 ----
mean loss: 101.62
 ---- batch: 040 ----
mean loss: 104.20
 ---- batch: 050 ----
mean loss: 102.59
 ---- batch: 060 ----
mean loss: 104.26
 ---- batch: 070 ----
mean loss: 105.76
 ---- batch: 080 ----
mean loss: 98.08
 ---- batch: 090 ----
mean loss: 104.71
train mean loss: 102.28
epoch train time: 0:00:01.678056
elapsed time: 0:06:54.863364
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 20:08:20.031758
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.08
 ---- batch: 020 ----
mean loss: 101.81
 ---- batch: 030 ----
mean loss: 101.75
 ---- batch: 040 ----
mean loss: 102.35
 ---- batch: 050 ----
mean loss: 106.17
 ---- batch: 060 ----
mean loss: 99.79
 ---- batch: 070 ----
mean loss: 100.97
 ---- batch: 080 ----
mean loss: 101.01
 ---- batch: 090 ----
mean loss: 111.99
train mean loss: 102.42
epoch train time: 0:00:01.642069
elapsed time: 0:06:56.506038
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 20:08:21.674428
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 105.68
 ---- batch: 020 ----
mean loss: 100.84
 ---- batch: 030 ----
mean loss: 101.78
 ---- batch: 040 ----
mean loss: 99.51
 ---- batch: 050 ----
mean loss: 103.16
 ---- batch: 060 ----
mean loss: 101.24
 ---- batch: 070 ----
mean loss: 104.33
 ---- batch: 080 ----
mean loss: 106.55
 ---- batch: 090 ----
mean loss: 99.88
train mean loss: 102.32
epoch train time: 0:00:01.660419
elapsed time: 0:06:58.167093
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 20:08:23.335495
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.77
 ---- batch: 020 ----
mean loss: 103.85
 ---- batch: 030 ----
mean loss: 102.77
 ---- batch: 040 ----
mean loss: 104.49
 ---- batch: 050 ----
mean loss: 95.56
 ---- batch: 060 ----
mean loss: 98.55
 ---- batch: 070 ----
mean loss: 105.46
 ---- batch: 080 ----
mean loss: 103.94
 ---- batch: 090 ----
mean loss: 99.97
train mean loss: 102.27
epoch train time: 0:00:01.637403
elapsed time: 0:06:59.805112
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 20:08:24.973519
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.82
 ---- batch: 020 ----
mean loss: 96.70
 ---- batch: 030 ----
mean loss: 103.50
 ---- batch: 040 ----
mean loss: 101.28
 ---- batch: 050 ----
mean loss: 100.65
 ---- batch: 060 ----
mean loss: 102.96
 ---- batch: 070 ----
mean loss: 108.25
 ---- batch: 080 ----
mean loss: 104.39
 ---- batch: 090 ----
mean loss: 102.28
train mean loss: 102.15
epoch train time: 0:00:01.634367
elapsed time: 0:07:01.440111
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 20:08:26.608546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.78
 ---- batch: 020 ----
mean loss: 102.64
 ---- batch: 030 ----
mean loss: 99.48
 ---- batch: 040 ----
mean loss: 98.81
 ---- batch: 050 ----
mean loss: 101.65
 ---- batch: 060 ----
mean loss: 100.66
 ---- batch: 070 ----
mean loss: 100.51
 ---- batch: 080 ----
mean loss: 108.03
 ---- batch: 090 ----
mean loss: 103.44
train mean loss: 102.37
epoch train time: 0:00:01.676274
elapsed time: 0:07:03.117550
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 20:08:28.285708
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.29
 ---- batch: 020 ----
mean loss: 101.45
 ---- batch: 030 ----
mean loss: 99.35
 ---- batch: 040 ----
mean loss: 105.34
 ---- batch: 050 ----
mean loss: 106.39
 ---- batch: 060 ----
mean loss: 101.64
 ---- batch: 070 ----
mean loss: 102.71
 ---- batch: 080 ----
mean loss: 103.01
 ---- batch: 090 ----
mean loss: 100.05
train mean loss: 102.14
epoch train time: 0:00:01.666895
elapsed time: 0:07:04.784903
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 20:08:29.953300
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.75
 ---- batch: 020 ----
mean loss: 103.05
 ---- batch: 030 ----
mean loss: 98.41
 ---- batch: 040 ----
mean loss: 95.10
 ---- batch: 050 ----
mean loss: 102.36
 ---- batch: 060 ----
mean loss: 105.65
 ---- batch: 070 ----
mean loss: 104.21
 ---- batch: 080 ----
mean loss: 105.72
 ---- batch: 090 ----
mean loss: 102.42
train mean loss: 102.10
epoch train time: 0:00:01.660412
elapsed time: 0:07:06.445996
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 20:08:31.614439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.04
 ---- batch: 020 ----
mean loss: 107.54
 ---- batch: 030 ----
mean loss: 105.89
 ---- batch: 040 ----
mean loss: 103.05
 ---- batch: 050 ----
mean loss: 100.80
 ---- batch: 060 ----
mean loss: 95.80
 ---- batch: 070 ----
mean loss: 97.60
 ---- batch: 080 ----
mean loss: 102.66
 ---- batch: 090 ----
mean loss: 104.12
train mean loss: 102.09
epoch train time: 0:00:01.658965
elapsed time: 0:07:08.105657
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 20:08:33.274127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.23
 ---- batch: 020 ----
mean loss: 101.57
 ---- batch: 030 ----
mean loss: 106.64
 ---- batch: 040 ----
mean loss: 99.78
 ---- batch: 050 ----
mean loss: 99.97
 ---- batch: 060 ----
mean loss: 100.94
 ---- batch: 070 ----
mean loss: 101.11
 ---- batch: 080 ----
mean loss: 107.22
 ---- batch: 090 ----
mean loss: 103.64
train mean loss: 101.97
epoch train time: 0:00:01.661892
elapsed time: 0:07:09.768245
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 20:08:34.936639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.93
 ---- batch: 020 ----
mean loss: 105.31
 ---- batch: 030 ----
mean loss: 102.68
 ---- batch: 040 ----
mean loss: 97.79
 ---- batch: 050 ----
mean loss: 102.92
 ---- batch: 060 ----
mean loss: 103.29
 ---- batch: 070 ----
mean loss: 103.23
 ---- batch: 080 ----
mean loss: 101.11
 ---- batch: 090 ----
mean loss: 98.09
train mean loss: 101.95
epoch train time: 0:00:01.649954
elapsed time: 0:07:11.418809
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 20:08:36.587233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.75
 ---- batch: 020 ----
mean loss: 105.08
 ---- batch: 030 ----
mean loss: 101.41
 ---- batch: 040 ----
mean loss: 100.47
 ---- batch: 050 ----
mean loss: 102.76
 ---- batch: 060 ----
mean loss: 100.21
 ---- batch: 070 ----
mean loss: 100.35
 ---- batch: 080 ----
mean loss: 103.17
 ---- batch: 090 ----
mean loss: 102.23
train mean loss: 102.04
epoch train time: 0:00:01.656332
elapsed time: 0:07:13.075848
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 20:08:38.244241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.51
 ---- batch: 020 ----
mean loss: 104.34
 ---- batch: 030 ----
mean loss: 104.59
 ---- batch: 040 ----
mean loss: 97.18
 ---- batch: 050 ----
mean loss: 101.07
 ---- batch: 060 ----
mean loss: 100.99
 ---- batch: 070 ----
mean loss: 99.00
 ---- batch: 080 ----
mean loss: 101.59
 ---- batch: 090 ----
mean loss: 99.23
train mean loss: 102.05
epoch train time: 0:00:01.634825
elapsed time: 0:07:14.711302
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 20:08:39.879731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 104.81
 ---- batch: 020 ----
mean loss: 96.88
 ---- batch: 030 ----
mean loss: 105.07
 ---- batch: 040 ----
mean loss: 102.00
 ---- batch: 050 ----
mean loss: 99.54
 ---- batch: 060 ----
mean loss: 95.59
 ---- batch: 070 ----
mean loss: 106.18
 ---- batch: 080 ----
mean loss: 100.31
 ---- batch: 090 ----
mean loss: 101.50
train mean loss: 101.94
epoch train time: 0:00:01.678385
elapsed time: 0:07:16.390438
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 20:08:41.558873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.80
 ---- batch: 020 ----
mean loss: 96.03
 ---- batch: 030 ----
mean loss: 104.30
 ---- batch: 040 ----
mean loss: 102.17
 ---- batch: 050 ----
mean loss: 98.16
 ---- batch: 060 ----
mean loss: 98.57
 ---- batch: 070 ----
mean loss: 104.50
 ---- batch: 080 ----
mean loss: 105.27
 ---- batch: 090 ----
mean loss: 102.87
train mean loss: 101.85
epoch train time: 0:00:01.696390
elapsed time: 0:07:18.087603
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 20:08:43.256022
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.39
 ---- batch: 020 ----
mean loss: 105.38
 ---- batch: 030 ----
mean loss: 98.56
 ---- batch: 040 ----
mean loss: 99.16
 ---- batch: 050 ----
mean loss: 99.05
 ---- batch: 060 ----
mean loss: 106.49
 ---- batch: 070 ----
mean loss: 103.24
 ---- batch: 080 ----
mean loss: 96.77
 ---- batch: 090 ----
mean loss: 108.50
train mean loss: 101.78
epoch train time: 0:00:01.675213
elapsed time: 0:07:19.763485
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 20:08:44.931874
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.99
 ---- batch: 020 ----
mean loss: 107.40
 ---- batch: 030 ----
mean loss: 97.71
 ---- batch: 040 ----
mean loss: 97.35
 ---- batch: 050 ----
mean loss: 101.83
 ---- batch: 060 ----
mean loss: 104.28
 ---- batch: 070 ----
mean loss: 102.18
 ---- batch: 080 ----
mean loss: 102.38
 ---- batch: 090 ----
mean loss: 104.72
train mean loss: 101.77
epoch train time: 0:00:01.653960
elapsed time: 0:07:21.418092
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 20:08:46.586511
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.33
 ---- batch: 020 ----
mean loss: 104.83
 ---- batch: 030 ----
mean loss: 99.37
 ---- batch: 040 ----
mean loss: 101.09
 ---- batch: 050 ----
mean loss: 102.88
 ---- batch: 060 ----
mean loss: 102.11
 ---- batch: 070 ----
mean loss: 102.70
 ---- batch: 080 ----
mean loss: 104.86
 ---- batch: 090 ----
mean loss: 102.11
train mean loss: 101.60
epoch train time: 0:00:01.670241
elapsed time: 0:07:23.089038
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 20:08:48.257453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.10
 ---- batch: 020 ----
mean loss: 93.07
 ---- batch: 030 ----
mean loss: 99.41
 ---- batch: 040 ----
mean loss: 106.07
 ---- batch: 050 ----
mean loss: 101.31
 ---- batch: 060 ----
mean loss: 104.49
 ---- batch: 070 ----
mean loss: 106.17
 ---- batch: 080 ----
mean loss: 100.11
 ---- batch: 090 ----
mean loss: 104.17
train mean loss: 101.75
epoch train time: 0:00:01.636787
elapsed time: 0:07:24.726479
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 20:08:49.894897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.28
 ---- batch: 020 ----
mean loss: 102.74
 ---- batch: 030 ----
mean loss: 100.19
 ---- batch: 040 ----
mean loss: 100.76
 ---- batch: 050 ----
mean loss: 105.98
 ---- batch: 060 ----
mean loss: 104.09
 ---- batch: 070 ----
mean loss: 98.14
 ---- batch: 080 ----
mean loss: 101.27
 ---- batch: 090 ----
mean loss: 101.67
train mean loss: 101.52
epoch train time: 0:00:01.629233
elapsed time: 0:07:26.356337
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 20:08:51.524736
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.57
 ---- batch: 020 ----
mean loss: 95.54
 ---- batch: 030 ----
mean loss: 99.14
 ---- batch: 040 ----
mean loss: 102.24
 ---- batch: 050 ----
mean loss: 101.60
 ---- batch: 060 ----
mean loss: 110.31
 ---- batch: 070 ----
mean loss: 107.26
 ---- batch: 080 ----
mean loss: 97.02
 ---- batch: 090 ----
mean loss: 100.92
train mean loss: 101.47
epoch train time: 0:00:01.647791
elapsed time: 0:07:28.004733
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 20:08:53.173155
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.57
 ---- batch: 020 ----
mean loss: 102.85
 ---- batch: 030 ----
mean loss: 101.64
 ---- batch: 040 ----
mean loss: 94.38
 ---- batch: 050 ----
mean loss: 100.21
 ---- batch: 060 ----
mean loss: 103.00
 ---- batch: 070 ----
mean loss: 100.90
 ---- batch: 080 ----
mean loss: 107.06
 ---- batch: 090 ----
mean loss: 106.17
train mean loss: 101.48
epoch train time: 0:00:01.633009
elapsed time: 0:07:29.646549
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_5/checkpoint.pth.tar
**** end time: 2019-09-25 20:08:54.814665 ****
