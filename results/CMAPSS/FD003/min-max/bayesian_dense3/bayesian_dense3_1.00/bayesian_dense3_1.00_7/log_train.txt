Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6150
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-09-20 21:57:04.717739 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:57:04.726616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4286.08
 ---- batch: 020 ----
mean loss: 4008.67
 ---- batch: 030 ----
mean loss: 3797.47
 ---- batch: 040 ----
mean loss: 3508.48
train mean loss: 3860.73
epoch train time: 0:00:15.063117
elapsed time: 0:00:15.078123
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:57:19.795898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3275.70
 ---- batch: 020 ----
mean loss: 3050.83
 ---- batch: 030 ----
mean loss: 2891.43
 ---- batch: 040 ----
mean loss: 2839.17
train mean loss: 2992.96
epoch train time: 0:00:00.693342
elapsed time: 0:00:15.771698
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:57:20.489556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2604.64
 ---- batch: 020 ----
mean loss: 2556.00
 ---- batch: 030 ----
mean loss: 2513.40
 ---- batch: 040 ----
mean loss: 2414.05
train mean loss: 2505.81
epoch train time: 0:00:00.712516
elapsed time: 0:00:16.484503
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:57:21.202348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2314.66
 ---- batch: 020 ----
mean loss: 2334.17
 ---- batch: 030 ----
mean loss: 2160.87
 ---- batch: 040 ----
mean loss: 2200.94
train mean loss: 2254.03
epoch train time: 0:00:00.696556
elapsed time: 0:00:17.181290
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:57:21.899123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2151.70
 ---- batch: 020 ----
mean loss: 2070.49
 ---- batch: 030 ----
mean loss: 2071.19
 ---- batch: 040 ----
mean loss: 2002.26
train mean loss: 2069.71
epoch train time: 0:00:00.669599
elapsed time: 0:00:17.851131
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:57:22.568949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1977.15
 ---- batch: 020 ----
mean loss: 1973.11
 ---- batch: 030 ----
mean loss: 1917.31
 ---- batch: 040 ----
mean loss: 1883.08
train mean loss: 1934.90
epoch train time: 0:00:00.685810
elapsed time: 0:00:18.537188
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:57:23.255040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1854.79
 ---- batch: 020 ----
mean loss: 1853.22
 ---- batch: 030 ----
mean loss: 1812.24
 ---- batch: 040 ----
mean loss: 1744.32
train mean loss: 1814.40
epoch train time: 0:00:00.692778
elapsed time: 0:00:19.230252
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:57:23.948080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1756.44
 ---- batch: 020 ----
mean loss: 1678.88
 ---- batch: 030 ----
mean loss: 1722.70
 ---- batch: 040 ----
mean loss: 1682.21
train mean loss: 1707.21
epoch train time: 0:00:00.705022
elapsed time: 0:00:19.935525
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:57:24.653334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1671.86
 ---- batch: 020 ----
mean loss: 1608.70
 ---- batch: 030 ----
mean loss: 1585.35
 ---- batch: 040 ----
mean loss: 1552.77
train mean loss: 1603.19
epoch train time: 0:00:00.693691
elapsed time: 0:00:20.629463
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:57:25.347276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1537.37
 ---- batch: 020 ----
mean loss: 1512.98
 ---- batch: 030 ----
mean loss: 1507.21
 ---- batch: 040 ----
mean loss: 1496.11
train mean loss: 1507.45
epoch train time: 0:00:00.666487
elapsed time: 0:00:21.296174
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:57:26.014015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1425.40
 ---- batch: 020 ----
mean loss: 1415.92
 ---- batch: 030 ----
mean loss: 1410.98
 ---- batch: 040 ----
mean loss: 1334.79
train mean loss: 1397.90
epoch train time: 0:00:00.654862
elapsed time: 0:00:21.951278
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:57:26.669091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1366.01
 ---- batch: 020 ----
mean loss: 1296.26
 ---- batch: 030 ----
mean loss: 1265.11
 ---- batch: 040 ----
mean loss: 1259.34
train mean loss: 1296.29
epoch train time: 0:00:00.661218
elapsed time: 0:00:22.612719
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:57:27.330542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1230.37
 ---- batch: 020 ----
mean loss: 1213.53
 ---- batch: 030 ----
mean loss: 1204.01
 ---- batch: 040 ----
mean loss: 1179.76
train mean loss: 1203.13
epoch train time: 0:00:00.708711
elapsed time: 0:00:23.321741
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:57:28.039576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1139.24
 ---- batch: 020 ----
mean loss: 1140.25
 ---- batch: 030 ----
mean loss: 1108.56
 ---- batch: 040 ----
mean loss: 1118.13
train mean loss: 1125.58
epoch train time: 0:00:00.711947
elapsed time: 0:00:24.033953
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:57:28.751785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1084.61
 ---- batch: 020 ----
mean loss: 1054.20
 ---- batch: 030 ----
mean loss: 1041.49
 ---- batch: 040 ----
mean loss: 1037.14
train mean loss: 1049.39
epoch train time: 0:00:00.679889
elapsed time: 0:00:24.714073
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:57:29.431881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 995.26
 ---- batch: 020 ----
mean loss: 999.62
 ---- batch: 030 ----
mean loss: 977.73
 ---- batch: 040 ----
mean loss: 960.76
train mean loss: 982.13
epoch train time: 0:00:00.676049
elapsed time: 0:00:25.390341
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:57:30.108161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.27
 ---- batch: 020 ----
mean loss: 929.77
 ---- batch: 030 ----
mean loss: 916.33
 ---- batch: 040 ----
mean loss: 901.87
train mean loss: 919.38
epoch train time: 0:00:00.670825
elapsed time: 0:00:26.061380
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:57:30.779188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.94
 ---- batch: 020 ----
mean loss: 866.28
 ---- batch: 030 ----
mean loss: 848.18
 ---- batch: 040 ----
mean loss: 856.89
train mean loss: 862.31
epoch train time: 0:00:00.669320
elapsed time: 0:00:26.730931
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:57:31.448743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.23
 ---- batch: 020 ----
mean loss: 794.32
 ---- batch: 030 ----
mean loss: 816.20
 ---- batch: 040 ----
mean loss: 783.19
train mean loss: 803.04
epoch train time: 0:00:00.718119
elapsed time: 0:00:27.449298
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:57:32.167128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 756.15
 ---- batch: 020 ----
mean loss: 760.82
 ---- batch: 030 ----
mean loss: 746.45
 ---- batch: 040 ----
mean loss: 724.10
train mean loss: 747.54
epoch train time: 0:00:00.703294
elapsed time: 0:00:28.152869
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:57:32.870718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 710.33
 ---- batch: 020 ----
mean loss: 712.98
 ---- batch: 030 ----
mean loss: 696.08
 ---- batch: 040 ----
mean loss: 687.11
train mean loss: 700.59
epoch train time: 0:00:00.680579
elapsed time: 0:00:28.833728
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:57:33.551569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.13
 ---- batch: 020 ----
mean loss: 654.57
 ---- batch: 030 ----
mean loss: 645.62
 ---- batch: 040 ----
mean loss: 649.02
train mean loss: 652.48
epoch train time: 0:00:00.695598
elapsed time: 0:00:29.529582
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:57:34.247404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 626.08
 ---- batch: 020 ----
mean loss: 615.91
 ---- batch: 030 ----
mean loss: 599.91
 ---- batch: 040 ----
mean loss: 590.39
train mean loss: 606.99
epoch train time: 0:00:00.668092
elapsed time: 0:00:30.198051
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:57:34.915874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 579.88
 ---- batch: 020 ----
mean loss: 587.50
 ---- batch: 030 ----
mean loss: 560.40
 ---- batch: 040 ----
mean loss: 555.94
train mean loss: 568.32
epoch train time: 0:00:00.719047
elapsed time: 0:00:30.917381
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:57:35.635212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.73
 ---- batch: 020 ----
mean loss: 535.12
 ---- batch: 030 ----
mean loss: 527.53
 ---- batch: 040 ----
mean loss: 506.84
train mean loss: 526.45
epoch train time: 0:00:00.700372
elapsed time: 0:00:31.618023
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:57:36.335865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 509.37
 ---- batch: 020 ----
mean loss: 510.21
 ---- batch: 030 ----
mean loss: 489.60
 ---- batch: 040 ----
mean loss: 484.10
train mean loss: 496.79
epoch train time: 0:00:00.702056
elapsed time: 0:00:32.320313
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:57:37.038118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.57
 ---- batch: 020 ----
mean loss: 470.08
 ---- batch: 030 ----
mean loss: 445.00
 ---- batch: 040 ----
mean loss: 457.44
train mean loss: 460.25
epoch train time: 0:00:00.670031
elapsed time: 0:00:32.990537
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:57:37.708344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.26
 ---- batch: 020 ----
mean loss: 434.49
 ---- batch: 030 ----
mean loss: 432.68
 ---- batch: 040 ----
mean loss: 414.77
train mean loss: 430.61
epoch train time: 0:00:00.674921
elapsed time: 0:00:33.665678
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:57:38.383487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.76
 ---- batch: 020 ----
mean loss: 403.02
 ---- batch: 030 ----
mean loss: 400.66
 ---- batch: 040 ----
mean loss: 395.92
train mean loss: 403.17
epoch train time: 0:00:00.732654
elapsed time: 0:00:34.398616
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:57:39.116454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.38
 ---- batch: 020 ----
mean loss: 380.67
 ---- batch: 030 ----
mean loss: 378.35
 ---- batch: 040 ----
mean loss: 369.33
train mean loss: 378.81
epoch train time: 0:00:00.688054
elapsed time: 0:00:35.086939
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:57:39.804755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.30
 ---- batch: 020 ----
mean loss: 360.53
 ---- batch: 030 ----
mean loss: 356.29
 ---- batch: 040 ----
mean loss: 348.48
train mean loss: 353.70
epoch train time: 0:00:00.688650
elapsed time: 0:00:35.775808
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:57:40.493615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.99
 ---- batch: 020 ----
mean loss: 340.00
 ---- batch: 030 ----
mean loss: 334.71
 ---- batch: 040 ----
mean loss: 333.72
train mean loss: 338.23
epoch train time: 0:00:00.677503
elapsed time: 0:00:36.453565
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:57:41.171380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.00
 ---- batch: 020 ----
mean loss: 317.75
 ---- batch: 030 ----
mean loss: 304.36
 ---- batch: 040 ----
mean loss: 317.77
train mean loss: 312.52
epoch train time: 0:00:00.680004
elapsed time: 0:00:37.133800
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:57:41.851651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.68
 ---- batch: 020 ----
mean loss: 297.39
 ---- batch: 030 ----
mean loss: 298.39
 ---- batch: 040 ----
mean loss: 293.36
train mean loss: 298.42
epoch train time: 0:00:00.689888
elapsed time: 0:00:37.823952
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:57:42.541764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.36
 ---- batch: 020 ----
mean loss: 290.50
 ---- batch: 030 ----
mean loss: 278.03
 ---- batch: 040 ----
mean loss: 273.23
train mean loss: 282.11
epoch train time: 0:00:00.727240
elapsed time: 0:00:38.551442
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:57:43.269255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.25
 ---- batch: 020 ----
mean loss: 260.51
 ---- batch: 030 ----
mean loss: 267.15
 ---- batch: 040 ----
mean loss: 258.83
train mean loss: 263.81
epoch train time: 0:00:00.694330
elapsed time: 0:00:39.246004
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:57:43.963819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.50
 ---- batch: 020 ----
mean loss: 252.19
 ---- batch: 030 ----
mean loss: 249.99
 ---- batch: 040 ----
mean loss: 247.08
train mean loss: 250.84
epoch train time: 0:00:00.656999
elapsed time: 0:00:39.903277
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:57:44.621116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.10
 ---- batch: 020 ----
mean loss: 234.28
 ---- batch: 030 ----
mean loss: 233.52
 ---- batch: 040 ----
mean loss: 229.65
train mean loss: 234.23
epoch train time: 0:00:00.655594
elapsed time: 0:00:40.559119
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:57:45.276930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.71
 ---- batch: 020 ----
mean loss: 226.88
 ---- batch: 030 ----
mean loss: 227.41
 ---- batch: 040 ----
mean loss: 220.36
train mean loss: 226.30
epoch train time: 0:00:00.695380
elapsed time: 0:00:41.254734
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:57:45.972560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.35
 ---- batch: 020 ----
mean loss: 218.88
 ---- batch: 030 ----
mean loss: 215.06
 ---- batch: 040 ----
mean loss: 209.67
train mean loss: 214.76
epoch train time: 0:00:00.698810
elapsed time: 0:00:41.953813
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:57:46.671635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.26
 ---- batch: 020 ----
mean loss: 209.90
 ---- batch: 030 ----
mean loss: 204.88
 ---- batch: 040 ----
mean loss: 201.43
train mean loss: 205.28
epoch train time: 0:00:00.703353
elapsed time: 0:00:42.657414
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:57:47.375224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.21
 ---- batch: 020 ----
mean loss: 191.27
 ---- batch: 030 ----
mean loss: 195.35
 ---- batch: 040 ----
mean loss: 190.88
train mean loss: 193.86
epoch train time: 0:00:00.709581
elapsed time: 0:00:43.367211
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:57:48.085035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.61
 ---- batch: 020 ----
mean loss: 186.70
 ---- batch: 030 ----
mean loss: 186.65
 ---- batch: 040 ----
mean loss: 183.60
train mean loss: 186.30
epoch train time: 0:00:00.677605
elapsed time: 0:00:44.045149
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:57:48.762962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.41
 ---- batch: 020 ----
mean loss: 179.86
 ---- batch: 030 ----
mean loss: 172.09
 ---- batch: 040 ----
mean loss: 171.34
train mean loss: 176.99
epoch train time: 0:00:00.647749
elapsed time: 0:00:44.693121
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:57:49.410935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.47
 ---- batch: 020 ----
mean loss: 171.49
 ---- batch: 030 ----
mean loss: 166.67
 ---- batch: 040 ----
mean loss: 165.63
train mean loss: 169.90
epoch train time: 0:00:00.699871
elapsed time: 0:00:45.393243
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:57:50.111060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.36
 ---- batch: 020 ----
mean loss: 169.01
 ---- batch: 030 ----
mean loss: 164.08
 ---- batch: 040 ----
mean loss: 164.75
train mean loss: 165.60
epoch train time: 0:00:00.724397
elapsed time: 0:00:46.117905
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:57:50.835731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.76
 ---- batch: 020 ----
mean loss: 154.57
 ---- batch: 030 ----
mean loss: 159.05
 ---- batch: 040 ----
mean loss: 158.37
train mean loss: 156.91
epoch train time: 0:00:00.712720
elapsed time: 0:00:46.830863
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:57:51.548694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.75
 ---- batch: 020 ----
mean loss: 149.65
 ---- batch: 030 ----
mean loss: 150.48
 ---- batch: 040 ----
mean loss: 149.29
train mean loss: 151.03
epoch train time: 0:00:00.682416
elapsed time: 0:00:47.513529
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:57:52.231372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.05
 ---- batch: 020 ----
mean loss: 149.19
 ---- batch: 030 ----
mean loss: 146.16
 ---- batch: 040 ----
mean loss: 149.66
train mean loss: 147.76
epoch train time: 0:00:00.677703
elapsed time: 0:00:48.191492
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:57:52.909331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.58
 ---- batch: 020 ----
mean loss: 141.61
 ---- batch: 030 ----
mean loss: 138.62
 ---- batch: 040 ----
mean loss: 142.32
train mean loss: 141.86
epoch train time: 0:00:00.677255
elapsed time: 0:00:48.869022
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:57:53.586863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.99
 ---- batch: 020 ----
mean loss: 138.24
 ---- batch: 030 ----
mean loss: 133.21
 ---- batch: 040 ----
mean loss: 133.58
train mean loss: 135.94
epoch train time: 0:00:00.718949
elapsed time: 0:00:49.588337
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:57:54.306173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.51
 ---- batch: 020 ----
mean loss: 134.10
 ---- batch: 030 ----
mean loss: 132.65
 ---- batch: 040 ----
mean loss: 132.20
train mean loss: 133.01
epoch train time: 0:00:00.728682
elapsed time: 0:00:50.317387
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:57:55.035217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.79
 ---- batch: 020 ----
mean loss: 131.27
 ---- batch: 030 ----
mean loss: 130.21
 ---- batch: 040 ----
mean loss: 129.30
train mean loss: 130.68
epoch train time: 0:00:00.669929
elapsed time: 0:00:50.987541
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:57:55.705348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.70
 ---- batch: 020 ----
mean loss: 126.02
 ---- batch: 030 ----
mean loss: 127.64
 ---- batch: 040 ----
mean loss: 125.15
train mean loss: 126.29
epoch train time: 0:00:00.676122
elapsed time: 0:00:51.663865
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:57:56.381675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.73
 ---- batch: 020 ----
mean loss: 119.75
 ---- batch: 030 ----
mean loss: 126.73
 ---- batch: 040 ----
mean loss: 123.18
train mean loss: 123.39
epoch train time: 0:00:00.680147
elapsed time: 0:00:52.344307
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:57:57.062143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.40
 ---- batch: 020 ----
mean loss: 123.16
 ---- batch: 030 ----
mean loss: 120.66
 ---- batch: 040 ----
mean loss: 114.45
train mean loss: 120.35
epoch train time: 0:00:00.705475
elapsed time: 0:00:53.050135
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:57:57.767987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.96
 ---- batch: 020 ----
mean loss: 115.92
 ---- batch: 030 ----
mean loss: 116.06
 ---- batch: 040 ----
mean loss: 115.49
train mean loss: 116.61
epoch train time: 0:00:00.703778
elapsed time: 0:00:53.754179
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:57:58.471994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.62
 ---- batch: 020 ----
mean loss: 111.33
 ---- batch: 030 ----
mean loss: 113.73
 ---- batch: 040 ----
mean loss: 112.77
train mean loss: 113.12
epoch train time: 0:00:00.688151
elapsed time: 0:00:54.442585
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:57:59.160432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.50
 ---- batch: 020 ----
mean loss: 109.42
 ---- batch: 030 ----
mean loss: 110.64
 ---- batch: 040 ----
mean loss: 113.18
train mean loss: 111.16
epoch train time: 0:00:00.674300
elapsed time: 0:00:55.117198
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:57:59.835009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.67
 ---- batch: 020 ----
mean loss: 109.06
 ---- batch: 030 ----
mean loss: 109.45
 ---- batch: 040 ----
mean loss: 103.81
train mean loss: 108.39
epoch train time: 0:00:00.677242
elapsed time: 0:00:55.794669
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:58:00.512483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.37
 ---- batch: 020 ----
mean loss: 105.17
 ---- batch: 030 ----
mean loss: 108.76
 ---- batch: 040 ----
mean loss: 109.34
train mean loss: 107.06
epoch train time: 0:00:00.680410
elapsed time: 0:00:56.475311
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:58:01.193157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.96
 ---- batch: 020 ----
mean loss: 108.09
 ---- batch: 030 ----
mean loss: 103.43
 ---- batch: 040 ----
mean loss: 106.10
train mean loss: 106.31
epoch train time: 0:00:00.695419
elapsed time: 0:00:57.170989
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:58:01.888825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.49
 ---- batch: 020 ----
mean loss: 106.21
 ---- batch: 030 ----
mean loss: 103.50
 ---- batch: 040 ----
mean loss: 104.98
train mean loss: 103.58
epoch train time: 0:00:00.691131
elapsed time: 0:00:57.862379
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:58:02.580197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.77
 ---- batch: 020 ----
mean loss: 99.40
 ---- batch: 030 ----
mean loss: 104.29
 ---- batch: 040 ----
mean loss: 96.47
train mean loss: 100.77
epoch train time: 0:00:00.686523
elapsed time: 0:00:58.549115
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:58:03.266942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.06
 ---- batch: 020 ----
mean loss: 97.53
 ---- batch: 030 ----
mean loss: 98.83
 ---- batch: 040 ----
mean loss: 100.98
train mean loss: 99.34
epoch train time: 0:00:00.680589
elapsed time: 0:00:59.229927
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:58:03.947741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.26
 ---- batch: 020 ----
mean loss: 99.90
 ---- batch: 030 ----
mean loss: 97.46
 ---- batch: 040 ----
mean loss: 101.32
train mean loss: 99.48
epoch train time: 0:00:00.670709
elapsed time: 0:00:59.900871
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:58:04.618683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.74
 ---- batch: 020 ----
mean loss: 96.53
 ---- batch: 030 ----
mean loss: 96.53
 ---- batch: 040 ----
mean loss: 95.48
train mean loss: 96.75
epoch train time: 0:00:00.709102
elapsed time: 0:01:00.610209
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:58:05.328038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.02
 ---- batch: 020 ----
mean loss: 94.67
 ---- batch: 030 ----
mean loss: 99.62
 ---- batch: 040 ----
mean loss: 93.58
train mean loss: 96.51
epoch train time: 0:00:00.730862
elapsed time: 0:01:01.341413
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:58:06.059249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.85
 ---- batch: 020 ----
mean loss: 91.62
 ---- batch: 030 ----
mean loss: 93.28
 ---- batch: 040 ----
mean loss: 94.79
train mean loss: 93.29
epoch train time: 0:00:00.702879
elapsed time: 0:01:02.044553
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:58:06.762398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.76
 ---- batch: 020 ----
mean loss: 90.20
 ---- batch: 030 ----
mean loss: 90.60
 ---- batch: 040 ----
mean loss: 92.35
train mean loss: 91.67
epoch train time: 0:00:00.664799
elapsed time: 0:01:02.709721
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:58:07.427581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.66
 ---- batch: 020 ----
mean loss: 87.67
 ---- batch: 030 ----
mean loss: 92.50
 ---- batch: 040 ----
mean loss: 91.47
train mean loss: 91.02
epoch train time: 0:00:00.668710
elapsed time: 0:01:03.378707
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:58:08.096535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.27
 ---- batch: 020 ----
mean loss: 91.55
 ---- batch: 030 ----
mean loss: 90.29
 ---- batch: 040 ----
mean loss: 89.79
train mean loss: 90.43
epoch train time: 0:00:00.694907
elapsed time: 0:01:04.073925
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:58:08.791728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.88
 ---- batch: 020 ----
mean loss: 91.53
 ---- batch: 030 ----
mean loss: 88.82
 ---- batch: 040 ----
mean loss: 89.80
train mean loss: 88.56
epoch train time: 0:00:00.696343
elapsed time: 0:01:04.770494
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:58:09.488336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.24
 ---- batch: 020 ----
mean loss: 83.83
 ---- batch: 030 ----
mean loss: 89.25
 ---- batch: 040 ----
mean loss: 87.03
train mean loss: 86.87
epoch train time: 0:00:00.715733
elapsed time: 0:01:05.486498
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:58:10.204305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.88
 ---- batch: 020 ----
mean loss: 86.97
 ---- batch: 030 ----
mean loss: 85.80
 ---- batch: 040 ----
mean loss: 86.31
train mean loss: 85.96
epoch train time: 0:00:00.694561
elapsed time: 0:01:06.181280
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:58:10.899089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.71
 ---- batch: 020 ----
mean loss: 84.49
 ---- batch: 030 ----
mean loss: 85.05
 ---- batch: 040 ----
mean loss: 81.32
train mean loss: 83.75
epoch train time: 0:00:00.657711
elapsed time: 0:01:06.839193
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:58:11.556996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.27
 ---- batch: 020 ----
mean loss: 80.14
 ---- batch: 030 ----
mean loss: 84.02
 ---- batch: 040 ----
mean loss: 89.89
train mean loss: 84.20
epoch train time: 0:00:00.673677
elapsed time: 0:01:07.513097
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:58:12.230922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.87
 ---- batch: 020 ----
mean loss: 88.41
 ---- batch: 030 ----
mean loss: 87.56
 ---- batch: 040 ----
mean loss: 82.34
train mean loss: 85.28
epoch train time: 0:00:00.714876
elapsed time: 0:01:08.228233
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:58:12.946043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.50
 ---- batch: 020 ----
mean loss: 82.19
 ---- batch: 030 ----
mean loss: 84.98
 ---- batch: 040 ----
mean loss: 82.38
train mean loss: 82.89
epoch train time: 0:00:00.720999
elapsed time: 0:01:08.949509
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:58:13.667356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.50
 ---- batch: 020 ----
mean loss: 82.99
 ---- batch: 030 ----
mean loss: 85.63
 ---- batch: 040 ----
mean loss: 82.97
train mean loss: 83.43
epoch train time: 0:00:00.698881
elapsed time: 0:01:09.648639
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:58:14.366449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.45
 ---- batch: 020 ----
mean loss: 80.60
 ---- batch: 030 ----
mean loss: 81.91
 ---- batch: 040 ----
mean loss: 80.43
train mean loss: 81.00
epoch train time: 0:00:00.690024
elapsed time: 0:01:10.338888
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:58:15.056705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.69
 ---- batch: 020 ----
mean loss: 80.27
 ---- batch: 030 ----
mean loss: 80.55
 ---- batch: 040 ----
mean loss: 82.50
train mean loss: 80.86
epoch train time: 0:00:00.672344
elapsed time: 0:01:11.011454
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:58:15.729261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.94
 ---- batch: 020 ----
mean loss: 77.80
 ---- batch: 030 ----
mean loss: 78.44
 ---- batch: 040 ----
mean loss: 77.78
train mean loss: 79.51
epoch train time: 0:00:00.739372
elapsed time: 0:01:11.751090
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:58:16.468929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.39
 ---- batch: 020 ----
mean loss: 82.58
 ---- batch: 030 ----
mean loss: 78.14
 ---- batch: 040 ----
mean loss: 81.97
train mean loss: 79.97
epoch train time: 0:00:00.730522
elapsed time: 0:01:12.481876
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:58:17.199690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.67
 ---- batch: 020 ----
mean loss: 78.56
 ---- batch: 030 ----
mean loss: 78.59
 ---- batch: 040 ----
mean loss: 76.27
train mean loss: 78.18
epoch train time: 0:00:00.702898
elapsed time: 0:01:13.184980
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:58:17.902792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.36
 ---- batch: 020 ----
mean loss: 77.06
 ---- batch: 030 ----
mean loss: 78.25
 ---- batch: 040 ----
mean loss: 80.12
train mean loss: 78.83
epoch train time: 0:00:00.678034
elapsed time: 0:01:13.863240
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:58:18.581056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.84
 ---- batch: 020 ----
mean loss: 79.65
 ---- batch: 030 ----
mean loss: 75.20
 ---- batch: 040 ----
mean loss: 78.47
train mean loss: 77.68
epoch train time: 0:00:00.668775
elapsed time: 0:01:14.532263
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:58:19.250085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.71
 ---- batch: 020 ----
mean loss: 76.38
 ---- batch: 030 ----
mean loss: 77.46
 ---- batch: 040 ----
mean loss: 76.60
train mean loss: 77.09
epoch train time: 0:00:00.685814
elapsed time: 0:01:15.218319
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:58:19.936127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.95
 ---- batch: 020 ----
mean loss: 76.75
 ---- batch: 030 ----
mean loss: 73.28
 ---- batch: 040 ----
mean loss: 75.92
train mean loss: 76.30
epoch train time: 0:00:00.701908
elapsed time: 0:01:15.920469
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:58:20.638287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.40
 ---- batch: 020 ----
mean loss: 74.39
 ---- batch: 030 ----
mean loss: 76.79
 ---- batch: 040 ----
mean loss: 77.88
train mean loss: 76.17
epoch train time: 0:00:00.698341
elapsed time: 0:01:16.619047
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:58:21.336881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.70
 ---- batch: 020 ----
mean loss: 78.02
 ---- batch: 030 ----
mean loss: 70.39
 ---- batch: 040 ----
mean loss: 77.98
train mean loss: 74.78
epoch train time: 0:00:00.668172
elapsed time: 0:01:17.287465
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:58:22.005287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.51
 ---- batch: 020 ----
mean loss: 75.34
 ---- batch: 030 ----
mean loss: 74.30
 ---- batch: 040 ----
mean loss: 72.76
train mean loss: 74.07
epoch train time: 0:00:00.670492
elapsed time: 0:01:17.958188
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:58:22.675998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.00
 ---- batch: 020 ----
mean loss: 72.80
 ---- batch: 030 ----
mean loss: 73.30
 ---- batch: 040 ----
mean loss: 73.27
train mean loss: 72.76
epoch train time: 0:00:00.673713
elapsed time: 0:01:18.632131
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:58:23.349938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.43
 ---- batch: 020 ----
mean loss: 75.64
 ---- batch: 030 ----
mean loss: 72.45
 ---- batch: 040 ----
mean loss: 72.13
train mean loss: 73.26
epoch train time: 0:00:00.704337
elapsed time: 0:01:19.336708
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:58:24.054540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.13
 ---- batch: 020 ----
mean loss: 74.00
 ---- batch: 030 ----
mean loss: 69.65
 ---- batch: 040 ----
mean loss: 73.31
train mean loss: 72.14
epoch train time: 0:00:00.694549
elapsed time: 0:01:20.031522
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:58:24.749342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.37
 ---- batch: 020 ----
mean loss: 73.38
 ---- batch: 030 ----
mean loss: 72.81
 ---- batch: 040 ----
mean loss: 69.18
train mean loss: 72.20
epoch train time: 0:00:00.700174
elapsed time: 0:01:20.731929
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:58:25.449793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.01
 ---- batch: 020 ----
mean loss: 74.95
 ---- batch: 030 ----
mean loss: 68.06
 ---- batch: 040 ----
mean loss: 70.82
train mean loss: 71.45
epoch train time: 0:00:00.667278
elapsed time: 0:01:21.399463
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:58:26.117270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.16
 ---- batch: 020 ----
mean loss: 70.25
 ---- batch: 030 ----
mean loss: 69.88
 ---- batch: 040 ----
mean loss: 69.84
train mean loss: 71.04
epoch train time: 0:00:00.686443
elapsed time: 0:01:22.086144
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:58:26.803975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.61
 ---- batch: 020 ----
mean loss: 70.46
 ---- batch: 030 ----
mean loss: 68.85
 ---- batch: 040 ----
mean loss: 71.05
train mean loss: 69.83
epoch train time: 0:00:00.680084
elapsed time: 0:01:22.766548
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:58:27.484400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.15
 ---- batch: 020 ----
mean loss: 72.59
 ---- batch: 030 ----
mean loss: 68.58
 ---- batch: 040 ----
mean loss: 67.21
train mean loss: 69.51
epoch train time: 0:00:00.746742
elapsed time: 0:01:23.513586
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:58:28.231439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.88
 ---- batch: 020 ----
mean loss: 69.39
 ---- batch: 030 ----
mean loss: 71.75
 ---- batch: 040 ----
mean loss: 70.10
train mean loss: 69.71
epoch train time: 0:00:00.741330
elapsed time: 0:01:24.255171
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:58:28.972986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.56
 ---- batch: 020 ----
mean loss: 67.53
 ---- batch: 030 ----
mean loss: 68.53
 ---- batch: 040 ----
mean loss: 71.79
train mean loss: 69.81
epoch train time: 0:00:00.661987
elapsed time: 0:01:24.917366
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:58:29.635176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.28
 ---- batch: 020 ----
mean loss: 68.39
 ---- batch: 030 ----
mean loss: 67.60
 ---- batch: 040 ----
mean loss: 70.90
train mean loss: 68.76
epoch train time: 0:00:00.660029
elapsed time: 0:01:25.577617
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:58:30.295445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.09
 ---- batch: 020 ----
mean loss: 68.34
 ---- batch: 030 ----
mean loss: 68.41
 ---- batch: 040 ----
mean loss: 66.06
train mean loss: 68.29
epoch train time: 0:00:00.673995
elapsed time: 0:01:26.251871
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:58:30.969693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.09
 ---- batch: 020 ----
mean loss: 70.17
 ---- batch: 030 ----
mean loss: 65.63
 ---- batch: 040 ----
mean loss: 70.03
train mean loss: 68.54
epoch train time: 0:00:00.719577
elapsed time: 0:01:26.971728
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:58:31.689552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.42
 ---- batch: 020 ----
mean loss: 70.08
 ---- batch: 030 ----
mean loss: 65.65
 ---- batch: 040 ----
mean loss: 69.42
train mean loss: 67.73
epoch train time: 0:00:00.712309
elapsed time: 0:01:27.684310
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:58:32.402133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.07
 ---- batch: 020 ----
mean loss: 64.87
 ---- batch: 030 ----
mean loss: 68.43
 ---- batch: 040 ----
mean loss: 69.25
train mean loss: 67.77
epoch train time: 0:00:00.711012
elapsed time: 0:01:28.395626
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:58:33.113411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.09
 ---- batch: 020 ----
mean loss: 65.79
 ---- batch: 030 ----
mean loss: 68.10
 ---- batch: 040 ----
mean loss: 64.21
train mean loss: 66.35
epoch train time: 0:00:00.689547
elapsed time: 0:01:29.085415
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:58:33.803246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.78
 ---- batch: 020 ----
mean loss: 64.88
 ---- batch: 030 ----
mean loss: 65.57
 ---- batch: 040 ----
mean loss: 70.88
train mean loss: 66.91
epoch train time: 0:00:00.666205
elapsed time: 0:01:29.751859
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:58:34.469676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.74
 ---- batch: 020 ----
mean loss: 64.24
 ---- batch: 030 ----
mean loss: 71.09
 ---- batch: 040 ----
mean loss: 62.93
train mean loss: 67.07
epoch train time: 0:00:00.701504
elapsed time: 0:01:30.453627
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:58:35.171487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.47
 ---- batch: 020 ----
mean loss: 65.68
 ---- batch: 030 ----
mean loss: 68.76
 ---- batch: 040 ----
mean loss: 66.89
train mean loss: 66.87
epoch train time: 0:00:00.735480
elapsed time: 0:01:31.189410
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:58:35.907225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.43
 ---- batch: 020 ----
mean loss: 64.00
 ---- batch: 030 ----
mean loss: 64.37
 ---- batch: 040 ----
mean loss: 62.84
train mean loss: 64.52
epoch train time: 0:00:00.707103
elapsed time: 0:01:31.896746
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:58:36.614556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.22
 ---- batch: 020 ----
mean loss: 66.70
 ---- batch: 030 ----
mean loss: 64.53
 ---- batch: 040 ----
mean loss: 61.55
train mean loss: 64.51
epoch train time: 0:00:00.679440
elapsed time: 0:01:32.576417
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:58:37.294228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.37
 ---- batch: 020 ----
mean loss: 64.72
 ---- batch: 030 ----
mean loss: 65.71
 ---- batch: 040 ----
mean loss: 60.66
train mean loss: 64.03
epoch train time: 0:00:00.692172
elapsed time: 0:01:33.268823
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:58:37.986657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.97
 ---- batch: 020 ----
mean loss: 61.83
 ---- batch: 030 ----
mean loss: 65.81
 ---- batch: 040 ----
mean loss: 63.62
train mean loss: 64.17
epoch train time: 0:00:00.696590
elapsed time: 0:01:33.965714
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:58:38.683563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.35
 ---- batch: 020 ----
mean loss: 63.93
 ---- batch: 030 ----
mean loss: 65.00
 ---- batch: 040 ----
mean loss: 61.64
train mean loss: 65.10
epoch train time: 0:00:00.728296
elapsed time: 0:01:34.694354
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:58:39.412181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.07
 ---- batch: 020 ----
mean loss: 61.16
 ---- batch: 030 ----
mean loss: 64.50
 ---- batch: 040 ----
mean loss: 66.58
train mean loss: 63.32
epoch train time: 0:00:00.703532
elapsed time: 0:01:35.398133
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:58:40.115951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.67
 ---- batch: 020 ----
mean loss: 63.62
 ---- batch: 030 ----
mean loss: 62.24
 ---- batch: 040 ----
mean loss: 61.90
train mean loss: 63.15
epoch train time: 0:00:00.659414
elapsed time: 0:01:36.057774
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:58:40.775585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.62
 ---- batch: 020 ----
mean loss: 61.37
 ---- batch: 030 ----
mean loss: 62.91
 ---- batch: 040 ----
mean loss: 60.75
train mean loss: 61.87
epoch train time: 0:00:00.670679
elapsed time: 0:01:36.728667
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:58:41.446494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.57
 ---- batch: 020 ----
mean loss: 62.91
 ---- batch: 030 ----
mean loss: 65.12
 ---- batch: 040 ----
mean loss: 60.16
train mean loss: 62.59
epoch train time: 0:00:00.697100
elapsed time: 0:01:37.426028
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:58:42.143861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.90
 ---- batch: 020 ----
mean loss: 62.38
 ---- batch: 030 ----
mean loss: 62.87
 ---- batch: 040 ----
mean loss: 62.59
train mean loss: 61.36
epoch train time: 0:00:00.716648
elapsed time: 0:01:38.142972
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:58:42.860813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.81
 ---- batch: 020 ----
mean loss: 62.54
 ---- batch: 030 ----
mean loss: 58.20
 ---- batch: 040 ----
mean loss: 63.98
train mean loss: 61.22
epoch train time: 0:00:00.712736
elapsed time: 0:01:38.856020
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:58:43.573883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.26
 ---- batch: 020 ----
mean loss: 62.26
 ---- batch: 030 ----
mean loss: 60.89
 ---- batch: 040 ----
mean loss: 61.00
train mean loss: 61.22
epoch train time: 0:00:00.698390
elapsed time: 0:01:39.554685
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:58:44.272498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.24
 ---- batch: 020 ----
mean loss: 60.95
 ---- batch: 030 ----
mean loss: 61.93
 ---- batch: 040 ----
mean loss: 58.04
train mean loss: 60.58
epoch train time: 0:00:00.696691
elapsed time: 0:01:40.251606
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:58:44.969455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.98
 ---- batch: 020 ----
mean loss: 60.24
 ---- batch: 030 ----
mean loss: 59.05
 ---- batch: 040 ----
mean loss: 59.76
train mean loss: 60.13
epoch train time: 0:00:00.678883
elapsed time: 0:01:40.930749
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:58:45.648554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.53
 ---- batch: 020 ----
mean loss: 61.03
 ---- batch: 030 ----
mean loss: 62.95
 ---- batch: 040 ----
mean loss: 60.25
train mean loss: 60.43
epoch train time: 0:00:00.705133
elapsed time: 0:01:41.636124
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:58:46.353943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.33
 ---- batch: 020 ----
mean loss: 58.30
 ---- batch: 030 ----
mean loss: 59.13
 ---- batch: 040 ----
mean loss: 58.21
train mean loss: 59.03
epoch train time: 0:00:00.719029
elapsed time: 0:01:42.355500
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:58:47.073278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.27
 ---- batch: 020 ----
mean loss: 59.46
 ---- batch: 030 ----
mean loss: 59.44
 ---- batch: 040 ----
mean loss: 60.25
train mean loss: 60.29
epoch train time: 0:00:00.690414
elapsed time: 0:01:43.046117
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:58:47.763925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.30
 ---- batch: 020 ----
mean loss: 60.39
 ---- batch: 030 ----
mean loss: 60.44
 ---- batch: 040 ----
mean loss: 60.76
train mean loss: 59.48
epoch train time: 0:00:00.673733
elapsed time: 0:01:43.720053
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:58:48.437859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.42
 ---- batch: 020 ----
mean loss: 57.62
 ---- batch: 030 ----
mean loss: 59.74
 ---- batch: 040 ----
mean loss: 56.79
train mean loss: 58.76
epoch train time: 0:00:00.696936
elapsed time: 0:01:44.417196
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:58:49.135003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.39
 ---- batch: 020 ----
mean loss: 60.19
 ---- batch: 030 ----
mean loss: 55.67
 ---- batch: 040 ----
mean loss: 59.33
train mean loss: 58.61
epoch train time: 0:00:00.680546
elapsed time: 0:01:45.098061
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:58:49.815874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.39
 ---- batch: 020 ----
mean loss: 57.96
 ---- batch: 030 ----
mean loss: 56.75
 ---- batch: 040 ----
mean loss: 57.09
train mean loss: 58.26
epoch train time: 0:00:00.696759
elapsed time: 0:01:45.795133
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:58:50.512950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.24
 ---- batch: 020 ----
mean loss: 57.46
 ---- batch: 030 ----
mean loss: 56.42
 ---- batch: 040 ----
mean loss: 56.89
train mean loss: 58.09
epoch train time: 0:00:00.698792
elapsed time: 0:01:46.494181
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:58:51.211990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.36
 ---- batch: 020 ----
mean loss: 57.39
 ---- batch: 030 ----
mean loss: 58.79
 ---- batch: 040 ----
mean loss: 57.03
train mean loss: 57.20
epoch train time: 0:00:00.675855
elapsed time: 0:01:47.170251
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:58:51.888067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.55
 ---- batch: 020 ----
mean loss: 57.69
 ---- batch: 030 ----
mean loss: 58.35
 ---- batch: 040 ----
mean loss: 57.79
train mean loss: 57.54
epoch train time: 0:00:00.669885
elapsed time: 0:01:47.840340
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:58:52.558145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.92
 ---- batch: 020 ----
mean loss: 56.08
 ---- batch: 030 ----
mean loss: 56.12
 ---- batch: 040 ----
mean loss: 57.56
train mean loss: 56.56
epoch train time: 0:00:00.678962
elapsed time: 0:01:48.519589
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:58:53.237418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.80
 ---- batch: 020 ----
mean loss: 57.72
 ---- batch: 030 ----
mean loss: 56.33
 ---- batch: 040 ----
mean loss: 55.86
train mean loss: 56.72
epoch train time: 0:00:00.705497
elapsed time: 0:01:49.225407
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:58:53.943223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.24
 ---- batch: 020 ----
mean loss: 58.01
 ---- batch: 030 ----
mean loss: 58.44
 ---- batch: 040 ----
mean loss: 54.86
train mean loss: 56.90
epoch train time: 0:00:00.693381
elapsed time: 0:01:49.919131
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:58:54.637006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.75
 ---- batch: 020 ----
mean loss: 60.57
 ---- batch: 030 ----
mean loss: 55.02
 ---- batch: 040 ----
mean loss: 55.41
train mean loss: 56.27
epoch train time: 0:00:00.703806
elapsed time: 0:01:50.623275
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:58:55.341095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.11
 ---- batch: 020 ----
mean loss: 55.36
 ---- batch: 030 ----
mean loss: 56.32
 ---- batch: 040 ----
mean loss: 56.43
train mean loss: 56.18
epoch train time: 0:00:00.695895
elapsed time: 0:01:51.319407
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:58:56.037226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.37
 ---- batch: 020 ----
mean loss: 50.60
 ---- batch: 030 ----
mean loss: 55.37
 ---- batch: 040 ----
mean loss: 56.76
train mean loss: 54.68
epoch train time: 0:00:00.685266
elapsed time: 0:01:52.004915
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:58:56.722733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.50
 ---- batch: 020 ----
mean loss: 56.20
 ---- batch: 030 ----
mean loss: 56.80
 ---- batch: 040 ----
mean loss: 54.67
train mean loss: 55.11
epoch train time: 0:00:00.704327
elapsed time: 0:01:52.709488
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:58:57.427305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.45
 ---- batch: 020 ----
mean loss: 58.60
 ---- batch: 030 ----
mean loss: 55.05
 ---- batch: 040 ----
mean loss: 53.29
train mean loss: 54.58
epoch train time: 0:00:00.706660
elapsed time: 0:01:53.416381
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:58:58.134192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.81
 ---- batch: 020 ----
mean loss: 55.08
 ---- batch: 030 ----
mean loss: 55.03
 ---- batch: 040 ----
mean loss: 56.81
train mean loss: 54.73
epoch train time: 0:00:00.696198
elapsed time: 0:01:54.112819
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:58:58.830656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.20
 ---- batch: 020 ----
mean loss: 56.16
 ---- batch: 030 ----
mean loss: 55.02
 ---- batch: 040 ----
mean loss: 53.45
train mean loss: 54.76
epoch train time: 0:00:00.679397
elapsed time: 0:01:54.792457
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:58:59.510308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.54
 ---- batch: 020 ----
mean loss: 52.46
 ---- batch: 030 ----
mean loss: 54.15
 ---- batch: 040 ----
mean loss: 54.41
train mean loss: 54.14
epoch train time: 0:00:00.698865
elapsed time: 0:01:55.491575
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:59:00.209433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.23
 ---- batch: 020 ----
mean loss: 54.53
 ---- batch: 030 ----
mean loss: 52.48
 ---- batch: 040 ----
mean loss: 55.57
train mean loss: 53.80
epoch train time: 0:00:00.672280
elapsed time: 0:01:56.164189
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:59:00.882024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.19
 ---- batch: 020 ----
mean loss: 50.63
 ---- batch: 030 ----
mean loss: 56.10
 ---- batch: 040 ----
mean loss: 52.95
train mean loss: 53.87
epoch train time: 0:00:00.713091
elapsed time: 0:01:56.877550
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:59:01.595392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.52
 ---- batch: 020 ----
mean loss: 55.92
 ---- batch: 030 ----
mean loss: 53.07
 ---- batch: 040 ----
mean loss: 50.96
train mean loss: 52.63
epoch train time: 0:00:00.704164
elapsed time: 0:01:57.582045
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:59:02.299826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.63
 ---- batch: 020 ----
mean loss: 52.60
 ---- batch: 030 ----
mean loss: 52.94
 ---- batch: 040 ----
mean loss: 52.25
train mean loss: 52.91
epoch train time: 0:00:00.689450
elapsed time: 0:01:58.271702
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:59:02.989516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.65
 ---- batch: 020 ----
mean loss: 52.14
 ---- batch: 030 ----
mean loss: 52.53
 ---- batch: 040 ----
mean loss: 54.17
train mean loss: 52.49
epoch train time: 0:00:00.664935
elapsed time: 0:01:58.936842
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:59:03.654650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.09
 ---- batch: 020 ----
mean loss: 51.41
 ---- batch: 030 ----
mean loss: 51.25
 ---- batch: 040 ----
mean loss: 53.82
train mean loss: 52.00
epoch train time: 0:00:00.678451
elapsed time: 0:01:59.615532
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:59:04.333357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.23
 ---- batch: 020 ----
mean loss: 53.62
 ---- batch: 030 ----
mean loss: 50.68
 ---- batch: 040 ----
mean loss: 52.08
train mean loss: 52.21
epoch train time: 0:00:00.698526
elapsed time: 0:02:00.314308
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:59:05.032124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.02
 ---- batch: 020 ----
mean loss: 53.51
 ---- batch: 030 ----
mean loss: 48.17
 ---- batch: 040 ----
mean loss: 51.72
train mean loss: 51.25
epoch train time: 0:00:00.717595
elapsed time: 0:02:01.032201
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:59:05.750013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.27
 ---- batch: 020 ----
mean loss: 50.82
 ---- batch: 030 ----
mean loss: 50.15
 ---- batch: 040 ----
mean loss: 50.86
train mean loss: 50.43
epoch train time: 0:00:00.714333
elapsed time: 0:02:01.746759
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:59:06.464570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.95
 ---- batch: 020 ----
mean loss: 51.77
 ---- batch: 030 ----
mean loss: 52.44
 ---- batch: 040 ----
mean loss: 50.74
train mean loss: 50.93
epoch train time: 0:00:00.679269
elapsed time: 0:02:02.426286
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:59:07.144096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.61
 ---- batch: 020 ----
mean loss: 50.86
 ---- batch: 030 ----
mean loss: 50.22
 ---- batch: 040 ----
mean loss: 49.74
train mean loss: 50.14
epoch train time: 0:00:00.658745
elapsed time: 0:02:03.085241
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:59:07.803049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.32
 ---- batch: 020 ----
mean loss: 50.69
 ---- batch: 030 ----
mean loss: 50.35
 ---- batch: 040 ----
mean loss: 50.94
train mean loss: 49.75
epoch train time: 0:00:00.694449
elapsed time: 0:02:03.779938
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:59:08.497769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.37
 ---- batch: 020 ----
mean loss: 49.69
 ---- batch: 030 ----
mean loss: 50.40
 ---- batch: 040 ----
mean loss: 49.73
train mean loss: 49.56
epoch train time: 0:00:00.717691
elapsed time: 0:02:04.497911
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:59:09.215734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.09
 ---- batch: 020 ----
mean loss: 49.32
 ---- batch: 030 ----
mean loss: 47.78
 ---- batch: 040 ----
mean loss: 52.28
train mean loss: 49.60
epoch train time: 0:00:00.724123
elapsed time: 0:02:05.222316
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:59:09.940151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.48
 ---- batch: 020 ----
mean loss: 49.69
 ---- batch: 030 ----
mean loss: 48.44
 ---- batch: 040 ----
mean loss: 51.46
train mean loss: 49.26
epoch train time: 0:00:00.690025
elapsed time: 0:02:05.912621
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:59:10.630450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.18
 ---- batch: 020 ----
mean loss: 46.93
 ---- batch: 030 ----
mean loss: 49.74
 ---- batch: 040 ----
mean loss: 52.53
train mean loss: 49.26
epoch train time: 0:00:00.675317
elapsed time: 0:02:06.588189
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:59:11.305996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.92
 ---- batch: 020 ----
mean loss: 51.08
 ---- batch: 030 ----
mean loss: 46.51
 ---- batch: 040 ----
mean loss: 51.25
train mean loss: 49.24
epoch train time: 0:00:00.677383
elapsed time: 0:02:07.265773
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:59:11.983582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.43
 ---- batch: 020 ----
mean loss: 47.36
 ---- batch: 030 ----
mean loss: 48.94
 ---- batch: 040 ----
mean loss: 49.33
train mean loss: 47.90
epoch train time: 0:00:00.716099
elapsed time: 0:02:07.982158
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:59:12.700000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.38
 ---- batch: 020 ----
mean loss: 46.88
 ---- batch: 030 ----
mean loss: 48.68
 ---- batch: 040 ----
mean loss: 48.16
train mean loss: 47.65
epoch train time: 0:00:00.717484
elapsed time: 0:02:08.699981
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:59:13.417805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.90
 ---- batch: 020 ----
mean loss: 46.71
 ---- batch: 030 ----
mean loss: 47.85
 ---- batch: 040 ----
mean loss: 47.96
train mean loss: 47.84
epoch train time: 0:00:00.701801
elapsed time: 0:02:09.402031
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:59:14.119841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.60
 ---- batch: 020 ----
mean loss: 47.09
 ---- batch: 030 ----
mean loss: 49.09
 ---- batch: 040 ----
mean loss: 45.05
train mean loss: 47.44
epoch train time: 0:00:00.679478
elapsed time: 0:02:10.081752
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:59:14.799573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.31
 ---- batch: 020 ----
mean loss: 46.51
 ---- batch: 030 ----
mean loss: 47.40
 ---- batch: 040 ----
mean loss: 47.69
train mean loss: 47.53
epoch train time: 0:00:00.691587
elapsed time: 0:02:10.773583
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:59:15.491406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.89
 ---- batch: 020 ----
mean loss: 47.30
 ---- batch: 030 ----
mean loss: 49.24
 ---- batch: 040 ----
mean loss: 44.46
train mean loss: 47.14
epoch train time: 0:00:00.698250
elapsed time: 0:02:11.472096
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:59:16.189904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.60
 ---- batch: 020 ----
mean loss: 43.85
 ---- batch: 030 ----
mean loss: 49.35
 ---- batch: 040 ----
mean loss: 48.01
train mean loss: 46.67
epoch train time: 0:00:00.704301
elapsed time: 0:02:12.176630
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:59:16.894492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.25
 ---- batch: 020 ----
mean loss: 45.04
 ---- batch: 030 ----
mean loss: 45.87
 ---- batch: 040 ----
mean loss: 45.80
train mean loss: 45.98
epoch train time: 0:00:00.692482
elapsed time: 0:02:12.869381
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:59:17.587196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.35
 ---- batch: 020 ----
mean loss: 44.08
 ---- batch: 030 ----
mean loss: 47.37
 ---- batch: 040 ----
mean loss: 46.62
train mean loss: 45.98
epoch train time: 0:00:00.659014
elapsed time: 0:02:13.528597
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:59:18.246402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.40
 ---- batch: 020 ----
mean loss: 46.62
 ---- batch: 030 ----
mean loss: 45.24
 ---- batch: 040 ----
mean loss: 47.35
train mean loss: 45.83
epoch train time: 0:00:00.666586
elapsed time: 0:02:14.195390
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:59:18.913216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.72
 ---- batch: 020 ----
mean loss: 47.29
 ---- batch: 030 ----
mean loss: 45.01
 ---- batch: 040 ----
mean loss: 43.55
train mean loss: 45.07
epoch train time: 0:00:00.661722
elapsed time: 0:02:14.857413
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:59:19.575192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.37
 ---- batch: 020 ----
mean loss: 45.56
 ---- batch: 030 ----
mean loss: 44.65
 ---- batch: 040 ----
mean loss: 43.39
train mean loss: 44.72
epoch train time: 0:00:00.729123
elapsed time: 0:02:15.586774
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:59:20.304604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.27
 ---- batch: 020 ----
mean loss: 45.30
 ---- batch: 030 ----
mean loss: 46.77
 ---- batch: 040 ----
mean loss: 44.07
train mean loss: 45.27
epoch train time: 0:00:00.716822
elapsed time: 0:02:16.303858
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:59:21.021675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.08
 ---- batch: 020 ----
mean loss: 44.32
 ---- batch: 030 ----
mean loss: 44.33
 ---- batch: 040 ----
mean loss: 45.48
train mean loss: 44.02
epoch train time: 0:00:00.692761
elapsed time: 0:02:16.996829
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:59:21.714639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.49
 ---- batch: 020 ----
mean loss: 43.17
 ---- batch: 030 ----
mean loss: 43.92
 ---- batch: 040 ----
mean loss: 45.60
train mean loss: 44.57
epoch train time: 0:00:00.683440
elapsed time: 0:02:17.680472
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:59:22.398281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.08
 ---- batch: 020 ----
mean loss: 44.71
 ---- batch: 030 ----
mean loss: 42.89
 ---- batch: 040 ----
mean loss: 45.20
train mean loss: 44.33
epoch train time: 0:00:00.684181
elapsed time: 0:02:18.364856
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:59:23.082693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.02
 ---- batch: 020 ----
mean loss: 45.00
 ---- batch: 030 ----
mean loss: 44.15
 ---- batch: 040 ----
mean loss: 42.25
train mean loss: 43.69
epoch train time: 0:00:00.683724
elapsed time: 0:02:19.048894
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:59:23.766707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.45
 ---- batch: 020 ----
mean loss: 45.23
 ---- batch: 030 ----
mean loss: 42.51
 ---- batch: 040 ----
mean loss: 41.93
train mean loss: 43.07
epoch train time: 0:00:00.698393
elapsed time: 0:02:19.747557
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:59:24.465389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.36
 ---- batch: 020 ----
mean loss: 42.20
 ---- batch: 030 ----
mean loss: 42.43
 ---- batch: 040 ----
mean loss: 43.61
train mean loss: 43.04
epoch train time: 0:00:00.698196
elapsed time: 0:02:20.445986
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:59:25.163801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.86
 ---- batch: 020 ----
mean loss: 43.35
 ---- batch: 030 ----
mean loss: 41.08
 ---- batch: 040 ----
mean loss: 44.45
train mean loss: 42.87
epoch train time: 0:00:00.673711
elapsed time: 0:02:21.119932
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:59:25.837766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.97
 ---- batch: 020 ----
mean loss: 43.88
 ---- batch: 030 ----
mean loss: 42.17
 ---- batch: 040 ----
mean loss: 42.80
train mean loss: 42.26
epoch train time: 0:00:00.664434
elapsed time: 0:02:21.784630
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:59:26.502438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.80
 ---- batch: 020 ----
mean loss: 44.08
 ---- batch: 030 ----
mean loss: 41.50
 ---- batch: 040 ----
mean loss: 44.33
train mean loss: 42.25
epoch train time: 0:00:00.687943
elapsed time: 0:02:22.472835
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:59:27.190673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.88
 ---- batch: 020 ----
mean loss: 39.85
 ---- batch: 030 ----
mean loss: 43.15
 ---- batch: 040 ----
mean loss: 41.96
train mean loss: 41.60
epoch train time: 0:00:00.713185
elapsed time: 0:02:23.186318
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:59:27.904131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.12
 ---- batch: 020 ----
mean loss: 44.43
 ---- batch: 030 ----
mean loss: 42.88
 ---- batch: 040 ----
mean loss: 37.44
train mean loss: 41.79
epoch train time: 0:00:00.712066
elapsed time: 0:02:23.898632
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:59:28.616468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.15
 ---- batch: 020 ----
mean loss: 40.97
 ---- batch: 030 ----
mean loss: 39.92
 ---- batch: 040 ----
mean loss: 43.78
train mean loss: 41.27
epoch train time: 0:00:00.683854
elapsed time: 0:02:24.582743
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:59:29.300586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.55
 ---- batch: 020 ----
mean loss: 40.40
 ---- batch: 030 ----
mean loss: 41.11
 ---- batch: 040 ----
mean loss: 42.86
train mean loss: 40.42
epoch train time: 0:00:00.690229
elapsed time: 0:02:25.273228
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:59:29.991040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.79
 ---- batch: 020 ----
mean loss: 39.27
 ---- batch: 030 ----
mean loss: 43.02
 ---- batch: 040 ----
mean loss: 42.14
train mean loss: 40.82
epoch train time: 0:00:00.670508
elapsed time: 0:02:25.943945
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:59:30.661756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.67
 ---- batch: 020 ----
mean loss: 40.27
 ---- batch: 030 ----
mean loss: 38.32
 ---- batch: 040 ----
mean loss: 41.14
train mean loss: 39.60
epoch train time: 0:00:00.690142
elapsed time: 0:02:26.634337
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:59:31.352153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.80
 ---- batch: 020 ----
mean loss: 39.55
 ---- batch: 030 ----
mean loss: 42.46
 ---- batch: 040 ----
mean loss: 38.32
train mean loss: 39.95
epoch train time: 0:00:00.702126
elapsed time: 0:02:27.336705
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:59:32.054519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.65
 ---- batch: 020 ----
mean loss: 38.79
 ---- batch: 030 ----
mean loss: 40.43
 ---- batch: 040 ----
mean loss: 39.33
train mean loss: 39.68
epoch train time: 0:00:00.699933
elapsed time: 0:02:28.036853
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:59:32.754679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.33
 ---- batch: 020 ----
mean loss: 38.43
 ---- batch: 030 ----
mean loss: 41.09
 ---- batch: 040 ----
mean loss: 37.97
train mean loss: 39.40
epoch train time: 0:00:00.660021
elapsed time: 0:02:28.697157
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:59:33.414982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.76
 ---- batch: 020 ----
mean loss: 39.97
 ---- batch: 030 ----
mean loss: 38.92
 ---- batch: 040 ----
mean loss: 36.67
train mean loss: 38.92
epoch train time: 0:00:00.673550
elapsed time: 0:02:29.370926
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:59:34.088760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.41
 ---- batch: 020 ----
mean loss: 38.67
 ---- batch: 030 ----
mean loss: 38.63
 ---- batch: 040 ----
mean loss: 39.77
train mean loss: 39.14
epoch train time: 0:00:00.687957
elapsed time: 0:02:30.059180
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:59:34.776995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.43
 ---- batch: 020 ----
mean loss: 39.36
 ---- batch: 030 ----
mean loss: 38.25
 ---- batch: 040 ----
mean loss: 38.22
train mean loss: 38.18
epoch train time: 0:00:00.705881
elapsed time: 0:02:30.765299
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:59:35.483115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.10
 ---- batch: 020 ----
mean loss: 37.98
 ---- batch: 030 ----
mean loss: 38.11
 ---- batch: 040 ----
mean loss: 38.71
train mean loss: 37.99
epoch train time: 0:00:00.718817
elapsed time: 0:02:31.484386
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:59:36.202222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.96
 ---- batch: 020 ----
mean loss: 36.90
 ---- batch: 030 ----
mean loss: 38.53
 ---- batch: 040 ----
mean loss: 38.26
train mean loss: 37.74
epoch train time: 0:00:00.674499
elapsed time: 0:02:32.159140
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:59:36.876949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.71
 ---- batch: 020 ----
mean loss: 35.30
 ---- batch: 030 ----
mean loss: 38.70
 ---- batch: 040 ----
mean loss: 39.46
train mean loss: 37.45
epoch train time: 0:00:00.673168
elapsed time: 0:02:32.832539
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:59:37.550355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.61
 ---- batch: 020 ----
mean loss: 36.60
 ---- batch: 030 ----
mean loss: 39.32
 ---- batch: 040 ----
mean loss: 37.99
train mean loss: 38.09
epoch train time: 0:00:00.678978
elapsed time: 0:02:33.511771
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:59:38.229585
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.04
 ---- batch: 020 ----
mean loss: 35.77
 ---- batch: 030 ----
mean loss: 36.52
 ---- batch: 040 ----
mean loss: 36.81
train mean loss: 35.96
epoch train time: 0:00:00.700965
elapsed time: 0:02:34.213040
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:59:38.930831
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.86
 ---- batch: 020 ----
mean loss: 36.67
 ---- batch: 030 ----
mean loss: 36.58
 ---- batch: 040 ----
mean loss: 36.31
train mean loss: 36.05
epoch train time: 0:00:00.703668
elapsed time: 0:02:34.916922
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:59:39.634784
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.10
 ---- batch: 020 ----
mean loss: 35.43
 ---- batch: 030 ----
mean loss: 35.45
 ---- batch: 040 ----
mean loss: 35.28
train mean loss: 35.74
epoch train time: 0:00:00.706486
elapsed time: 0:02:35.623687
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:59:40.341510
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.96
 ---- batch: 020 ----
mean loss: 36.83
 ---- batch: 030 ----
mean loss: 36.57
 ---- batch: 040 ----
mean loss: 36.81
train mean loss: 35.99
epoch train time: 0:00:00.666496
elapsed time: 0:02:36.290411
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:59:41.008225
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.79
 ---- batch: 020 ----
mean loss: 34.11
 ---- batch: 030 ----
mean loss: 36.64
 ---- batch: 040 ----
mean loss: 35.93
train mean loss: 35.87
epoch train time: 0:00:00.660145
elapsed time: 0:02:36.950777
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:59:41.668581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.65
 ---- batch: 020 ----
mean loss: 34.76
 ---- batch: 030 ----
mean loss: 36.24
 ---- batch: 040 ----
mean loss: 34.83
train mean loss: 35.67
epoch train time: 0:00:00.663352
elapsed time: 0:02:37.614387
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:59:42.332236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.52
 ---- batch: 020 ----
mean loss: 33.87
 ---- batch: 030 ----
mean loss: 36.23
 ---- batch: 040 ----
mean loss: 36.10
train mean loss: 35.86
epoch train time: 0:00:00.707142
elapsed time: 0:02:38.321811
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:59:43.039631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.82
 ---- batch: 020 ----
mean loss: 35.93
 ---- batch: 030 ----
mean loss: 34.31
 ---- batch: 040 ----
mean loss: 34.73
train mean loss: 35.61
epoch train time: 0:00:00.692466
elapsed time: 0:02:39.014520
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:59:43.732343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.81
 ---- batch: 020 ----
mean loss: 35.51
 ---- batch: 030 ----
mean loss: 34.93
 ---- batch: 040 ----
mean loss: 37.13
train mean loss: 35.51
epoch train time: 0:00:00.684859
elapsed time: 0:02:39.699651
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:59:44.417463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.58
 ---- batch: 020 ----
mean loss: 36.27
 ---- batch: 030 ----
mean loss: 37.08
 ---- batch: 040 ----
mean loss: 33.80
train mean loss: 35.76
epoch train time: 0:00:00.669376
elapsed time: 0:02:40.369246
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:59:45.087058
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.73
 ---- batch: 020 ----
mean loss: 36.08
 ---- batch: 030 ----
mean loss: 35.14
 ---- batch: 040 ----
mean loss: 34.98
train mean loss: 35.68
epoch train time: 0:00:00.659573
elapsed time: 0:02:41.029024
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:59:45.746884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.81
 ---- batch: 020 ----
mean loss: 34.67
 ---- batch: 030 ----
mean loss: 35.14
 ---- batch: 040 ----
mean loss: 35.81
train mean loss: 35.66
epoch train time: 0:00:00.691038
elapsed time: 0:02:41.720347
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:59:46.438174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.93
 ---- batch: 020 ----
mean loss: 33.15
 ---- batch: 030 ----
mean loss: 34.62
 ---- batch: 040 ----
mean loss: 36.72
train mean loss: 35.28
epoch train time: 0:00:00.733547
elapsed time: 0:02:42.454185
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:59:47.172023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.60
 ---- batch: 020 ----
mean loss: 34.73
 ---- batch: 030 ----
mean loss: 34.90
 ---- batch: 040 ----
mean loss: 35.77
train mean loss: 35.38
epoch train time: 0:00:00.707381
elapsed time: 0:02:43.161814
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:59:47.879631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.73
 ---- batch: 020 ----
mean loss: 33.77
 ---- batch: 030 ----
mean loss: 36.04
 ---- batch: 040 ----
mean loss: 35.81
train mean loss: 35.55
epoch train time: 0:00:00.655199
elapsed time: 0:02:43.817226
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:59:48.535036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.03
 ---- batch: 020 ----
mean loss: 35.72
 ---- batch: 030 ----
mean loss: 35.61
 ---- batch: 040 ----
mean loss: 33.68
train mean loss: 35.78
epoch train time: 0:00:00.682843
elapsed time: 0:02:44.500289
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:59:49.218103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.27
 ---- batch: 020 ----
mean loss: 35.10
 ---- batch: 030 ----
mean loss: 37.26
 ---- batch: 040 ----
mean loss: 34.49
train mean loss: 35.76
epoch train time: 0:00:00.685653
elapsed time: 0:02:45.186192
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:59:49.904025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.04
 ---- batch: 020 ----
mean loss: 36.50
 ---- batch: 030 ----
mean loss: 34.76
 ---- batch: 040 ----
mean loss: 35.96
train mean loss: 35.18
epoch train time: 0:00:00.714754
elapsed time: 0:02:45.901270
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:59:50.619116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.36
 ---- batch: 020 ----
mean loss: 36.16
 ---- batch: 030 ----
mean loss: 34.98
 ---- batch: 040 ----
mean loss: 33.46
train mean loss: 35.10
epoch train time: 0:00:00.699422
elapsed time: 0:02:46.600973
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:59:51.318786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.16
 ---- batch: 020 ----
mean loss: 36.34
 ---- batch: 030 ----
mean loss: 37.13
 ---- batch: 040 ----
mean loss: 34.86
train mean loss: 35.31
epoch train time: 0:00:00.675702
elapsed time: 0:02:47.276878
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:59:51.994687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.42
 ---- batch: 020 ----
mean loss: 34.87
 ---- batch: 030 ----
mean loss: 36.95
 ---- batch: 040 ----
mean loss: 35.48
train mean loss: 35.25
epoch train time: 0:00:00.674020
elapsed time: 0:02:47.951133
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:59:52.668941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.48
 ---- batch: 020 ----
mean loss: 37.31
 ---- batch: 030 ----
mean loss: 34.46
 ---- batch: 040 ----
mean loss: 35.17
train mean loss: 35.49
epoch train time: 0:00:00.652916
elapsed time: 0:02:48.604354
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:59:53.322171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.28
 ---- batch: 020 ----
mean loss: 35.39
 ---- batch: 030 ----
mean loss: 35.24
 ---- batch: 040 ----
mean loss: 35.51
train mean loss: 35.33
epoch train time: 0:00:00.691813
elapsed time: 0:02:49.296410
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:59:54.014228
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.71
 ---- batch: 020 ----
mean loss: 36.10
 ---- batch: 030 ----
mean loss: 34.05
 ---- batch: 040 ----
mean loss: 36.09
train mean loss: 35.50
epoch train time: 0:00:00.712884
elapsed time: 0:02:50.009625
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:59:54.727447
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.72
 ---- batch: 020 ----
mean loss: 35.40
 ---- batch: 030 ----
mean loss: 35.51
 ---- batch: 040 ----
mean loss: 34.45
train mean loss: 34.97
epoch train time: 0:00:00.698780
elapsed time: 0:02:50.708653
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:59:55.426604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.11
 ---- batch: 020 ----
mean loss: 35.91
 ---- batch: 030 ----
mean loss: 35.20
 ---- batch: 040 ----
mean loss: 35.63
train mean loss: 35.16
epoch train time: 0:00:00.695140
elapsed time: 0:02:51.404155
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:59:56.121964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.78
 ---- batch: 020 ----
mean loss: 35.13
 ---- batch: 030 ----
mean loss: 36.38
 ---- batch: 040 ----
mean loss: 34.96
train mean loss: 35.12
epoch train time: 0:00:00.678134
elapsed time: 0:02:52.082619
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:59:56.800554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.62
 ---- batch: 020 ----
mean loss: 33.57
 ---- batch: 030 ----
mean loss: 37.93
 ---- batch: 040 ----
mean loss: 34.30
train mean loss: 35.05
epoch train time: 0:00:00.696031
elapsed time: 0:02:52.779018
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:59:57.496843
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.33
 ---- batch: 020 ----
mean loss: 35.86
 ---- batch: 030 ----
mean loss: 34.51
 ---- batch: 040 ----
mean loss: 36.84
train mean loss: 35.06
epoch train time: 0:00:00.719813
elapsed time: 0:02:53.499114
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:59:58.216938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.14
 ---- batch: 020 ----
mean loss: 34.97
 ---- batch: 030 ----
mean loss: 34.73
 ---- batch: 040 ----
mean loss: 34.06
train mean loss: 34.82
epoch train time: 0:00:00.713586
elapsed time: 0:02:54.212987
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:59:58.930808
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.28
 ---- batch: 020 ----
mean loss: 32.92
 ---- batch: 030 ----
mean loss: 35.05
 ---- batch: 040 ----
mean loss: 35.89
train mean loss: 35.06
epoch train time: 0:00:00.682537
elapsed time: 0:02:54.895739
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:59:59.613560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.67
 ---- batch: 020 ----
mean loss: 35.18
 ---- batch: 030 ----
mean loss: 35.06
 ---- batch: 040 ----
mean loss: 33.36
train mean loss: 34.52
epoch train time: 0:00:00.694993
elapsed time: 0:02:55.590966
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 22:00:00.308771
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.75
 ---- batch: 020 ----
mean loss: 36.27
 ---- batch: 030 ----
mean loss: 34.87
 ---- batch: 040 ----
mean loss: 33.91
train mean loss: 35.33
epoch train time: 0:00:00.692273
elapsed time: 0:02:56.283519
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 22:00:01.001301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.02
 ---- batch: 020 ----
mean loss: 36.03
 ---- batch: 030 ----
mean loss: 35.22
 ---- batch: 040 ----
mean loss: 34.67
train mean loss: 35.12
epoch train time: 0:00:00.705067
elapsed time: 0:02:56.988795
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 22:00:01.706604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.15
 ---- batch: 020 ----
mean loss: 34.45
 ---- batch: 030 ----
mean loss: 33.33
 ---- batch: 040 ----
mean loss: 35.50
train mean loss: 34.85
epoch train time: 0:00:00.699154
elapsed time: 0:02:57.688167
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 22:00:02.405991
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.35
 ---- batch: 020 ----
mean loss: 34.98
 ---- batch: 030 ----
mean loss: 35.35
 ---- batch: 040 ----
mean loss: 34.47
train mean loss: 34.67
epoch train time: 0:00:00.688509
elapsed time: 0:02:58.376913
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 22:00:03.094719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.20
 ---- batch: 020 ----
mean loss: 33.28
 ---- batch: 030 ----
mean loss: 35.46
 ---- batch: 040 ----
mean loss: 34.30
train mean loss: 34.67
epoch train time: 0:00:00.686610
elapsed time: 0:02:59.063741
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 22:00:03.781554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.02
 ---- batch: 020 ----
mean loss: 35.88
 ---- batch: 030 ----
mean loss: 33.85
 ---- batch: 040 ----
mean loss: 34.90
train mean loss: 34.82
epoch train time: 0:00:00.674236
elapsed time: 0:02:59.738205
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 22:00:04.456014
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.32
 ---- batch: 020 ----
mean loss: 35.26
 ---- batch: 030 ----
mean loss: 36.69
 ---- batch: 040 ----
mean loss: 34.47
train mean loss: 34.58
epoch train time: 0:00:00.701041
elapsed time: 0:03:00.439475
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 22:00:05.157284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.65
 ---- batch: 020 ----
mean loss: 35.18
 ---- batch: 030 ----
mean loss: 32.88
 ---- batch: 040 ----
mean loss: 35.39
train mean loss: 34.59
epoch train time: 0:00:00.702769
elapsed time: 0:03:01.142512
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 22:00:05.860359
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.30
 ---- batch: 020 ----
mean loss: 33.54
 ---- batch: 030 ----
mean loss: 34.11
 ---- batch: 040 ----
mean loss: 35.87
train mean loss: 34.50
epoch train time: 0:00:00.691753
elapsed time: 0:03:01.834507
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 22:00:06.552361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.96
 ---- batch: 020 ----
mean loss: 34.52
 ---- batch: 030 ----
mean loss: 34.08
 ---- batch: 040 ----
mean loss: 34.46
train mean loss: 34.58
epoch train time: 0:00:00.661000
elapsed time: 0:03:02.495769
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 22:00:07.213615
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.35
 ---- batch: 020 ----
mean loss: 32.24
 ---- batch: 030 ----
mean loss: 35.48
 ---- batch: 040 ----
mean loss: 34.41
train mean loss: 34.35
epoch train time: 0:00:00.682979
elapsed time: 0:03:03.179022
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 22:00:07.896836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.77
 ---- batch: 020 ----
mean loss: 35.61
 ---- batch: 030 ----
mean loss: 34.36
 ---- batch: 040 ----
mean loss: 35.35
train mean loss: 34.57
epoch train time: 0:00:00.685541
elapsed time: 0:03:03.864813
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 22:00:08.582631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.70
 ---- batch: 020 ----
mean loss: 35.27
 ---- batch: 030 ----
mean loss: 35.16
 ---- batch: 040 ----
mean loss: 35.21
train mean loss: 34.63
epoch train time: 0:00:00.717560
elapsed time: 0:03:04.582629
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 22:00:09.300463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.49
 ---- batch: 020 ----
mean loss: 32.98
 ---- batch: 030 ----
mean loss: 35.58
 ---- batch: 040 ----
mean loss: 35.87
train mean loss: 34.60
epoch train time: 0:00:00.717852
elapsed time: 0:03:05.300759
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 22:00:10.018574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.48
 ---- batch: 020 ----
mean loss: 34.68
 ---- batch: 030 ----
mean loss: 34.38
 ---- batch: 040 ----
mean loss: 34.65
train mean loss: 34.63
epoch train time: 0:00:00.682906
elapsed time: 0:03:05.983930
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 22:00:10.701759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.27
 ---- batch: 020 ----
mean loss: 33.38
 ---- batch: 030 ----
mean loss: 34.44
 ---- batch: 040 ----
mean loss: 33.20
train mean loss: 34.40
epoch train time: 0:00:00.673959
elapsed time: 0:03:06.658141
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 22:00:11.375952
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.34
 ---- batch: 020 ----
mean loss: 34.95
 ---- batch: 030 ----
mean loss: 34.73
 ---- batch: 040 ----
mean loss: 34.00
train mean loss: 34.55
epoch train time: 0:00:00.677032
elapsed time: 0:03:07.342416
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_7/checkpoint.pth.tar
**** end time: 2019-09-20 22:00:12.060170 ****
