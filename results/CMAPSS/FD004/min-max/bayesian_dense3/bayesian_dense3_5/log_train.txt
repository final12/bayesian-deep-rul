Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_5', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 14735
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 23:03:42.005879 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 23:03:42.016270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4827.11
 ---- batch: 020 ----
mean loss: 4702.83
 ---- batch: 030 ----
mean loss: 4550.71
 ---- batch: 040 ----
mean loss: 4374.59
 ---- batch: 050 ----
mean loss: 4203.99
 ---- batch: 060 ----
mean loss: 3996.75
 ---- batch: 070 ----
mean loss: 3855.31
 ---- batch: 080 ----
mean loss: 3724.93
 ---- batch: 090 ----
mean loss: 3594.16
 ---- batch: 100 ----
mean loss: 3501.87
 ---- batch: 110 ----
mean loss: 3429.05
train mean loss: 4049.52
epoch train time: 0:00:34.477930
elapsed time: 0:00:34.494839
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 23:04:16.500772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3284.77
 ---- batch: 020 ----
mean loss: 3177.29
 ---- batch: 030 ----
mean loss: 3114.27
 ---- batch: 040 ----
mean loss: 3037.90
 ---- batch: 050 ----
mean loss: 3010.77
 ---- batch: 060 ----
mean loss: 2929.40
 ---- batch: 070 ----
mean loss: 2873.69
 ---- batch: 080 ----
mean loss: 2849.28
 ---- batch: 090 ----
mean loss: 2792.03
 ---- batch: 100 ----
mean loss: 2713.54
 ---- batch: 110 ----
mean loss: 2642.81
train mean loss: 2940.49
epoch train time: 0:00:01.955933
elapsed time: 0:00:36.451079
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 23:04:18.457417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2632.65
 ---- batch: 020 ----
mean loss: 2614.37
 ---- batch: 030 ----
mean loss: 2581.66
 ---- batch: 040 ----
mean loss: 2541.47
 ---- batch: 050 ----
mean loss: 2508.14
 ---- batch: 060 ----
mean loss: 2468.85
 ---- batch: 070 ----
mean loss: 2450.39
 ---- batch: 080 ----
mean loss: 2388.35
 ---- batch: 090 ----
mean loss: 2335.94
 ---- batch: 100 ----
mean loss: 2358.34
 ---- batch: 110 ----
mean loss: 2249.29
train mean loss: 2462.64
epoch train time: 0:00:01.926273
elapsed time: 0:00:38.377998
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 23:04:20.384237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2258.52
 ---- batch: 020 ----
mean loss: 2174.69
 ---- batch: 030 ----
mean loss: 2225.61
 ---- batch: 040 ----
mean loss: 2192.85
 ---- batch: 050 ----
mean loss: 2131.66
 ---- batch: 060 ----
mean loss: 2145.58
 ---- batch: 070 ----
mean loss: 2114.21
 ---- batch: 080 ----
mean loss: 2112.71
 ---- batch: 090 ----
mean loss: 2042.50
 ---- batch: 100 ----
mean loss: 2026.08
 ---- batch: 110 ----
mean loss: 1991.19
train mean loss: 2124.13
epoch train time: 0:00:01.968904
elapsed time: 0:00:40.347447
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 23:04:22.353718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1987.24
 ---- batch: 020 ----
mean loss: 1957.27
 ---- batch: 030 ----
mean loss: 1913.87
 ---- batch: 040 ----
mean loss: 1883.28
 ---- batch: 050 ----
mean loss: 1861.68
 ---- batch: 060 ----
mean loss: 1828.65
 ---- batch: 070 ----
mean loss: 1798.63
 ---- batch: 080 ----
mean loss: 1776.75
 ---- batch: 090 ----
mean loss: 1784.51
 ---- batch: 100 ----
mean loss: 1769.60
 ---- batch: 110 ----
mean loss: 1763.21
train mean loss: 1844.27
epoch train time: 0:00:01.957324
elapsed time: 0:00:42.305367
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 23:04:24.311642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1730.34
 ---- batch: 020 ----
mean loss: 1705.47
 ---- batch: 030 ----
mean loss: 1672.03
 ---- batch: 040 ----
mean loss: 1681.89
 ---- batch: 050 ----
mean loss: 1614.13
 ---- batch: 060 ----
mean loss: 1617.63
 ---- batch: 070 ----
mean loss: 1581.27
 ---- batch: 080 ----
mean loss: 1571.04
 ---- batch: 090 ----
mean loss: 1557.79
 ---- batch: 100 ----
mean loss: 1574.88
 ---- batch: 110 ----
mean loss: 1525.51
train mean loss: 1618.81
epoch train time: 0:00:01.949073
elapsed time: 0:00:44.255090
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 23:04:26.261348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1510.83
 ---- batch: 020 ----
mean loss: 1495.99
 ---- batch: 030 ----
mean loss: 1467.08
 ---- batch: 040 ----
mean loss: 1488.73
 ---- batch: 050 ----
mean loss: 1478.52
 ---- batch: 060 ----
mean loss: 1414.00
 ---- batch: 070 ----
mean loss: 1412.56
 ---- batch: 080 ----
mean loss: 1396.35
 ---- batch: 090 ----
mean loss: 1394.16
 ---- batch: 100 ----
mean loss: 1364.96
 ---- batch: 110 ----
mean loss: 1386.26
train mean loss: 1435.10
epoch train time: 0:00:01.956308
elapsed time: 0:00:46.212020
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 23:04:28.218361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1342.79
 ---- batch: 020 ----
mean loss: 1349.81
 ---- batch: 030 ----
mean loss: 1316.52
 ---- batch: 040 ----
mean loss: 1300.71
 ---- batch: 050 ----
mean loss: 1312.36
 ---- batch: 060 ----
mean loss: 1295.77
 ---- batch: 070 ----
mean loss: 1282.88
 ---- batch: 080 ----
mean loss: 1270.78
 ---- batch: 090 ----
mean loss: 1280.18
 ---- batch: 100 ----
mean loss: 1273.66
 ---- batch: 110 ----
mean loss: 1222.62
train mean loss: 1294.07
epoch train time: 0:00:01.979640
elapsed time: 0:00:48.192349
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 23:04:30.198679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1216.36
 ---- batch: 020 ----
mean loss: 1220.78
 ---- batch: 030 ----
mean loss: 1193.77
 ---- batch: 040 ----
mean loss: 1181.18
 ---- batch: 050 ----
mean loss: 1187.13
 ---- batch: 060 ----
mean loss: 1203.60
 ---- batch: 070 ----
mean loss: 1172.37
 ---- batch: 080 ----
mean loss: 1166.39
 ---- batch: 090 ----
mean loss: 1151.18
 ---- batch: 100 ----
mean loss: 1161.26
 ---- batch: 110 ----
mean loss: 1150.49
train mean loss: 1180.24
epoch train time: 0:00:01.936855
elapsed time: 0:00:50.129916
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 23:04:32.136165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1141.95
 ---- batch: 020 ----
mean loss: 1137.16
 ---- batch: 030 ----
mean loss: 1106.05
 ---- batch: 040 ----
mean loss: 1097.89
 ---- batch: 050 ----
mean loss: 1104.38
 ---- batch: 060 ----
mean loss: 1101.64
 ---- batch: 070 ----
mean loss: 1090.23
 ---- batch: 080 ----
mean loss: 1093.74
 ---- batch: 090 ----
mean loss: 1085.52
 ---- batch: 100 ----
mean loss: 1068.73
 ---- batch: 110 ----
mean loss: 1081.54
train mean loss: 1100.07
epoch train time: 0:00:01.971723
elapsed time: 0:00:52.102263
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 23:04:34.108538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1053.16
 ---- batch: 020 ----
mean loss: 1048.49
 ---- batch: 030 ----
mean loss: 1057.51
 ---- batch: 040 ----
mean loss: 1064.48
 ---- batch: 050 ----
mean loss: 1043.84
 ---- batch: 060 ----
mean loss: 1038.71
 ---- batch: 070 ----
mean loss: 1010.92
 ---- batch: 080 ----
mean loss: 1027.44
 ---- batch: 090 ----
mean loss: 1032.26
 ---- batch: 100 ----
mean loss: 1019.26
 ---- batch: 110 ----
mean loss: 1008.36
train mean loss: 1036.37
epoch train time: 0:00:01.941810
elapsed time: 0:00:54.044742
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 23:04:36.050860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1014.13
 ---- batch: 020 ----
mean loss: 996.48
 ---- batch: 030 ----
mean loss: 1001.20
 ---- batch: 040 ----
mean loss: 993.86
 ---- batch: 050 ----
mean loss: 978.45
 ---- batch: 060 ----
mean loss: 990.29
 ---- batch: 070 ----
mean loss: 987.30
 ---- batch: 080 ----
mean loss: 983.14
 ---- batch: 090 ----
mean loss: 975.52
 ---- batch: 100 ----
mean loss: 974.94
 ---- batch: 110 ----
mean loss: 961.86
train mean loss: 987.16
epoch train time: 0:00:01.974343
elapsed time: 0:00:56.019535
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 23:04:38.025865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 982.85
 ---- batch: 020 ----
mean loss: 973.56
 ---- batch: 030 ----
mean loss: 970.19
 ---- batch: 040 ----
mean loss: 954.81
 ---- batch: 050 ----
mean loss: 942.12
 ---- batch: 060 ----
mean loss: 937.85
 ---- batch: 070 ----
mean loss: 964.25
 ---- batch: 080 ----
mean loss: 928.51
 ---- batch: 090 ----
mean loss: 946.79
 ---- batch: 100 ----
mean loss: 944.69
 ---- batch: 110 ----
mean loss: 925.56
train mean loss: 951.45
epoch train time: 0:00:01.973676
elapsed time: 0:00:57.993894
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 23:04:40.000126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.42
 ---- batch: 020 ----
mean loss: 934.01
 ---- batch: 030 ----
mean loss: 923.75
 ---- batch: 040 ----
mean loss: 913.10
 ---- batch: 050 ----
mean loss: 917.97
 ---- batch: 060 ----
mean loss: 926.31
 ---- batch: 070 ----
mean loss: 928.92
 ---- batch: 080 ----
mean loss: 924.21
 ---- batch: 090 ----
mean loss: 904.87
 ---- batch: 100 ----
mean loss: 923.26
 ---- batch: 110 ----
mean loss: 926.92
train mean loss: 922.59
epoch train time: 0:00:01.976007
elapsed time: 0:00:59.970456
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 23:04:41.976694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.99
 ---- batch: 020 ----
mean loss: 909.15
 ---- batch: 030 ----
mean loss: 928.77
 ---- batch: 040 ----
mean loss: 919.10
 ---- batch: 050 ----
mean loss: 911.67
 ---- batch: 060 ----
mean loss: 900.46
 ---- batch: 070 ----
mean loss: 895.46
 ---- batch: 080 ----
mean loss: 899.64
 ---- batch: 090 ----
mean loss: 898.92
 ---- batch: 100 ----
mean loss: 902.46
 ---- batch: 110 ----
mean loss: 910.73
train mean loss: 907.76
epoch train time: 0:00:02.007559
elapsed time: 0:01:01.978568
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 23:04:43.984804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 907.75
 ---- batch: 020 ----
mean loss: 907.47
 ---- batch: 030 ----
mean loss: 883.07
 ---- batch: 040 ----
mean loss: 873.52
 ---- batch: 050 ----
mean loss: 894.38
 ---- batch: 060 ----
mean loss: 897.35
 ---- batch: 070 ----
mean loss: 912.26
 ---- batch: 080 ----
mean loss: 885.87
 ---- batch: 090 ----
mean loss: 885.24
 ---- batch: 100 ----
mean loss: 891.07
 ---- batch: 110 ----
mean loss: 871.30
train mean loss: 892.47
epoch train time: 0:00:01.953079
elapsed time: 0:01:03.932230
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 23:04:45.938460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.77
 ---- batch: 020 ----
mean loss: 865.50
 ---- batch: 030 ----
mean loss: 883.70
 ---- batch: 040 ----
mean loss: 898.76
 ---- batch: 050 ----
mean loss: 903.43
 ---- batch: 060 ----
mean loss: 895.66
 ---- batch: 070 ----
mean loss: 891.01
 ---- batch: 080 ----
mean loss: 891.90
 ---- batch: 090 ----
mean loss: 874.58
 ---- batch: 100 ----
mean loss: 881.81
 ---- batch: 110 ----
mean loss: 879.71
train mean loss: 885.75
epoch train time: 0:00:01.942214
elapsed time: 0:01:05.875026
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 23:04:47.881269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.23
 ---- batch: 020 ----
mean loss: 888.41
 ---- batch: 030 ----
mean loss: 869.24
 ---- batch: 040 ----
mean loss: 881.46
 ---- batch: 050 ----
mean loss: 894.06
 ---- batch: 060 ----
mean loss: 862.21
 ---- batch: 070 ----
mean loss: 896.08
 ---- batch: 080 ----
mean loss: 871.52
 ---- batch: 090 ----
mean loss: 877.40
 ---- batch: 100 ----
mean loss: 897.36
 ---- batch: 110 ----
mean loss: 888.40
train mean loss: 880.12
epoch train time: 0:00:02.008028
elapsed time: 0:01:07.883622
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 23:04:49.889921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.63
 ---- batch: 020 ----
mean loss: 878.20
 ---- batch: 030 ----
mean loss: 865.80
 ---- batch: 040 ----
mean loss: 852.89
 ---- batch: 050 ----
mean loss: 868.11
 ---- batch: 060 ----
mean loss: 878.54
 ---- batch: 070 ----
mean loss: 866.16
 ---- batch: 080 ----
mean loss: 869.19
 ---- batch: 090 ----
mean loss: 878.33
 ---- batch: 100 ----
mean loss: 866.79
 ---- batch: 110 ----
mean loss: 887.74
train mean loss: 870.83
epoch train time: 0:00:01.964258
elapsed time: 0:01:09.848539
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 23:04:51.854791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.95
 ---- batch: 020 ----
mean loss: 873.56
 ---- batch: 030 ----
mean loss: 879.49
 ---- batch: 040 ----
mean loss: 864.87
 ---- batch: 050 ----
mean loss: 854.19
 ---- batch: 060 ----
mean loss: 878.81
 ---- batch: 070 ----
mean loss: 871.60
 ---- batch: 080 ----
mean loss: 874.73
 ---- batch: 090 ----
mean loss: 862.61
 ---- batch: 100 ----
mean loss: 865.02
 ---- batch: 110 ----
mean loss: 875.77
train mean loss: 868.88
epoch train time: 0:00:01.958016
elapsed time: 0:01:11.807154
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 23:04:53.813410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.14
 ---- batch: 020 ----
mean loss: 898.61
 ---- batch: 030 ----
mean loss: 867.85
 ---- batch: 040 ----
mean loss: 887.14
 ---- batch: 050 ----
mean loss: 876.03
 ---- batch: 060 ----
mean loss: 876.85
 ---- batch: 070 ----
mean loss: 860.86
 ---- batch: 080 ----
mean loss: 882.54
 ---- batch: 090 ----
mean loss: 856.52
 ---- batch: 100 ----
mean loss: 849.97
 ---- batch: 110 ----
mean loss: 853.75
train mean loss: 866.85
epoch train time: 0:00:01.936646
elapsed time: 0:01:13.744373
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 23:04:55.750435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.64
 ---- batch: 020 ----
mean loss: 878.50
 ---- batch: 030 ----
mean loss: 843.28
 ---- batch: 040 ----
mean loss: 882.69
 ---- batch: 050 ----
mean loss: 869.37
 ---- batch: 060 ----
mean loss: 852.63
 ---- batch: 070 ----
mean loss: 878.76
 ---- batch: 080 ----
mean loss: 854.73
 ---- batch: 090 ----
mean loss: 858.46
 ---- batch: 100 ----
mean loss: 852.53
 ---- batch: 110 ----
mean loss: 872.30
train mean loss: 863.86
epoch train time: 0:00:01.964152
elapsed time: 0:01:15.708965
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 23:04:57.715233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.15
 ---- batch: 020 ----
mean loss: 851.55
 ---- batch: 030 ----
mean loss: 840.72
 ---- batch: 040 ----
mean loss: 866.36
 ---- batch: 050 ----
mean loss: 889.88
 ---- batch: 060 ----
mean loss: 854.49
 ---- batch: 070 ----
mean loss: 885.23
 ---- batch: 080 ----
mean loss: 851.17
 ---- batch: 090 ----
mean loss: 860.34
 ---- batch: 100 ----
mean loss: 881.70
 ---- batch: 110 ----
mean loss: 860.19
train mean loss: 862.79
epoch train time: 0:00:01.965882
elapsed time: 0:01:17.675467
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 23:04:59.681796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.28
 ---- batch: 020 ----
mean loss: 863.54
 ---- batch: 030 ----
mean loss: 873.93
 ---- batch: 040 ----
mean loss: 858.05
 ---- batch: 050 ----
mean loss: 873.60
 ---- batch: 060 ----
mean loss: 853.79
 ---- batch: 070 ----
mean loss: 848.12
 ---- batch: 080 ----
mean loss: 874.56
 ---- batch: 090 ----
mean loss: 857.76
 ---- batch: 100 ----
mean loss: 869.96
 ---- batch: 110 ----
mean loss: 847.95
train mean loss: 860.98
epoch train time: 0:00:01.943606
elapsed time: 0:01:19.619777
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 23:05:01.626037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.72
 ---- batch: 020 ----
mean loss: 853.85
 ---- batch: 030 ----
mean loss: 843.01
 ---- batch: 040 ----
mean loss: 865.96
 ---- batch: 050 ----
mean loss: 860.71
 ---- batch: 060 ----
mean loss: 873.84
 ---- batch: 070 ----
mean loss: 833.42
 ---- batch: 080 ----
mean loss: 862.60
 ---- batch: 090 ----
mean loss: 859.06
 ---- batch: 100 ----
mean loss: 849.70
 ---- batch: 110 ----
mean loss: 866.57
train mean loss: 858.38
epoch train time: 0:00:01.970153
elapsed time: 0:01:21.590511
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 23:05:03.596798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.53
 ---- batch: 020 ----
mean loss: 861.78
 ---- batch: 030 ----
mean loss: 856.71
 ---- batch: 040 ----
mean loss: 852.30
 ---- batch: 050 ----
mean loss: 849.78
 ---- batch: 060 ----
mean loss: 871.68
 ---- batch: 070 ----
mean loss: 876.12
 ---- batch: 080 ----
mean loss: 845.78
 ---- batch: 090 ----
mean loss: 864.22
 ---- batch: 100 ----
mean loss: 853.25
 ---- batch: 110 ----
mean loss: 856.24
train mean loss: 858.81
epoch train time: 0:00:02.001121
elapsed time: 0:01:23.592236
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 23:05:05.598560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.50
 ---- batch: 020 ----
mean loss: 867.23
 ---- batch: 030 ----
mean loss: 872.26
 ---- batch: 040 ----
mean loss: 861.67
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 835.12
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 875.44
 ---- batch: 090 ----
mean loss: 876.71
 ---- batch: 100 ----
mean loss: 855.11
 ---- batch: 110 ----
mean loss: 850.25
train mean loss: 859.45
epoch train time: 0:00:01.954805
elapsed time: 0:01:25.547774
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 23:05:07.554110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.74
 ---- batch: 020 ----
mean loss: 846.52
 ---- batch: 030 ----
mean loss: 868.72
 ---- batch: 040 ----
mean loss: 877.44
 ---- batch: 050 ----
mean loss: 855.05
 ---- batch: 060 ----
mean loss: 852.70
 ---- batch: 070 ----
mean loss: 846.71
 ---- batch: 080 ----
mean loss: 864.25
 ---- batch: 090 ----
mean loss: 864.72
 ---- batch: 100 ----
mean loss: 850.93
 ---- batch: 110 ----
mean loss: 861.12
train mean loss: 858.06
epoch train time: 0:00:01.919557
elapsed time: 0:01:27.468003
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 23:05:09.474236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.73
 ---- batch: 020 ----
mean loss: 847.54
 ---- batch: 030 ----
mean loss: 870.53
 ---- batch: 040 ----
mean loss: 875.79
 ---- batch: 050 ----
mean loss: 871.64
 ---- batch: 060 ----
mean loss: 856.65
 ---- batch: 070 ----
mean loss: 864.24
 ---- batch: 080 ----
mean loss: 856.44
 ---- batch: 090 ----
mean loss: 829.42
 ---- batch: 100 ----
mean loss: 866.40
 ---- batch: 110 ----
mean loss: 849.64
train mean loss: 855.97
epoch train time: 0:00:01.963702
elapsed time: 0:01:29.432253
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 23:05:11.438546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.50
 ---- batch: 020 ----
mean loss: 839.02
 ---- batch: 030 ----
mean loss: 850.06
 ---- batch: 040 ----
mean loss: 853.24
 ---- batch: 050 ----
mean loss: 858.83
 ---- batch: 060 ----
mean loss: 856.15
 ---- batch: 070 ----
mean loss: 857.50
 ---- batch: 080 ----
mean loss: 871.48
 ---- batch: 090 ----
mean loss: 857.48
 ---- batch: 100 ----
mean loss: 862.65
 ---- batch: 110 ----
mean loss: 852.12
train mean loss: 856.59
epoch train time: 0:00:01.935122
elapsed time: 0:01:31.367979
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 23:05:13.374271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.69
 ---- batch: 020 ----
mean loss: 851.79
 ---- batch: 030 ----
mean loss: 860.48
 ---- batch: 040 ----
mean loss: 866.67
 ---- batch: 050 ----
mean loss: 861.09
 ---- batch: 060 ----
mean loss: 850.65
 ---- batch: 070 ----
mean loss: 820.29
 ---- batch: 080 ----
mean loss: 860.84
 ---- batch: 090 ----
mean loss: 857.17
 ---- batch: 100 ----
mean loss: 864.15
 ---- batch: 110 ----
mean loss: 864.34
train mean loss: 854.69
epoch train time: 0:00:01.956903
elapsed time: 0:01:33.325513
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 23:05:15.331780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.15
 ---- batch: 020 ----
mean loss: 847.45
 ---- batch: 030 ----
mean loss: 881.21
 ---- batch: 040 ----
mean loss: 874.41
 ---- batch: 050 ----
mean loss: 831.14
 ---- batch: 060 ----
mean loss: 843.93
 ---- batch: 070 ----
mean loss: 867.51
 ---- batch: 080 ----
mean loss: 857.03
 ---- batch: 090 ----
mean loss: 856.84
 ---- batch: 100 ----
mean loss: 852.36
 ---- batch: 110 ----
mean loss: 842.46
train mean loss: 855.72
epoch train time: 0:00:01.970750
elapsed time: 0:01:35.296859
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 23:05:17.303116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.69
 ---- batch: 020 ----
mean loss: 844.86
 ---- batch: 030 ----
mean loss: 869.67
 ---- batch: 040 ----
mean loss: 871.29
 ---- batch: 050 ----
mean loss: 861.10
 ---- batch: 060 ----
mean loss: 865.67
 ---- batch: 070 ----
mean loss: 834.52
 ---- batch: 080 ----
mean loss: 860.12
 ---- batch: 090 ----
mean loss: 852.13
 ---- batch: 100 ----
mean loss: 859.86
 ---- batch: 110 ----
mean loss: 837.84
train mean loss: 853.82
epoch train time: 0:00:01.979340
elapsed time: 0:01:37.276875
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 23:05:19.283216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.74
 ---- batch: 020 ----
mean loss: 858.80
 ---- batch: 030 ----
mean loss: 846.80
 ---- batch: 040 ----
mean loss: 856.21
 ---- batch: 050 ----
mean loss: 837.92
 ---- batch: 060 ----
mean loss: 845.60
 ---- batch: 070 ----
mean loss: 851.54
 ---- batch: 080 ----
mean loss: 869.54
 ---- batch: 090 ----
mean loss: 866.30
 ---- batch: 100 ----
mean loss: 858.16
 ---- batch: 110 ----
mean loss: 865.76
train mean loss: 853.66
epoch train time: 0:00:01.957280
elapsed time: 0:01:39.234838
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 23:05:21.240868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.39
 ---- batch: 020 ----
mean loss: 820.88
 ---- batch: 030 ----
mean loss: 845.62
 ---- batch: 040 ----
mean loss: 861.29
 ---- batch: 050 ----
mean loss: 860.13
 ---- batch: 060 ----
mean loss: 862.05
 ---- batch: 070 ----
mean loss: 845.21
 ---- batch: 080 ----
mean loss: 859.53
 ---- batch: 090 ----
mean loss: 868.33
 ---- batch: 100 ----
mean loss: 864.00
 ---- batch: 110 ----
mean loss: 850.31
train mean loss: 853.70
epoch train time: 0:00:01.945277
elapsed time: 0:01:41.180473
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 23:05:23.186745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.61
 ---- batch: 020 ----
mean loss: 869.03
 ---- batch: 030 ----
mean loss: 848.42
 ---- batch: 040 ----
mean loss: 852.22
 ---- batch: 050 ----
mean loss: 841.74
 ---- batch: 060 ----
mean loss: 842.63
 ---- batch: 070 ----
mean loss: 854.40
 ---- batch: 080 ----
mean loss: 859.86
 ---- batch: 090 ----
mean loss: 841.06
 ---- batch: 100 ----
mean loss: 840.40
 ---- batch: 110 ----
mean loss: 845.15
train mean loss: 852.29
epoch train time: 0:00:01.951167
elapsed time: 0:01:43.132300
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 23:05:25.138582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.23
 ---- batch: 020 ----
mean loss: 854.89
 ---- batch: 030 ----
mean loss: 863.27
 ---- batch: 040 ----
mean loss: 850.47
 ---- batch: 050 ----
mean loss: 832.11
 ---- batch: 060 ----
mean loss: 859.76
 ---- batch: 070 ----
mean loss: 840.55
 ---- batch: 080 ----
mean loss: 830.95
 ---- batch: 090 ----
mean loss: 871.14
 ---- batch: 100 ----
mean loss: 860.12
 ---- batch: 110 ----
mean loss: 835.33
train mean loss: 850.78
epoch train time: 0:00:01.974353
elapsed time: 0:01:45.107277
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 23:05:27.113536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.93
 ---- batch: 020 ----
mean loss: 860.47
 ---- batch: 030 ----
mean loss: 838.62
 ---- batch: 040 ----
mean loss: 847.36
 ---- batch: 050 ----
mean loss: 877.11
 ---- batch: 060 ----
mean loss: 838.25
 ---- batch: 070 ----
mean loss: 848.97
 ---- batch: 080 ----
mean loss: 840.92
 ---- batch: 090 ----
mean loss: 834.78
 ---- batch: 100 ----
mean loss: 862.44
 ---- batch: 110 ----
mean loss: 863.46
train mean loss: 851.99
epoch train time: 0:00:01.955440
elapsed time: 0:01:47.063365
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 23:05:29.069665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.61
 ---- batch: 020 ----
mean loss: 844.15
 ---- batch: 030 ----
mean loss: 862.93
 ---- batch: 040 ----
mean loss: 868.08
 ---- batch: 050 ----
mean loss: 827.82
 ---- batch: 060 ----
mean loss: 835.43
 ---- batch: 070 ----
mean loss: 866.13
 ---- batch: 080 ----
mean loss: 861.19
 ---- batch: 090 ----
mean loss: 832.96
 ---- batch: 100 ----
mean loss: 846.27
 ---- batch: 110 ----
mean loss: 851.65
train mean loss: 851.18
epoch train time: 0:00:01.964567
elapsed time: 0:01:49.028624
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 23:05:31.034887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.94
 ---- batch: 020 ----
mean loss: 855.65
 ---- batch: 030 ----
mean loss: 849.37
 ---- batch: 040 ----
mean loss: 851.54
 ---- batch: 050 ----
mean loss: 838.91
 ---- batch: 060 ----
mean loss: 836.58
 ---- batch: 070 ----
mean loss: 843.85
 ---- batch: 080 ----
mean loss: 858.59
 ---- batch: 090 ----
mean loss: 851.50
 ---- batch: 100 ----
mean loss: 857.22
 ---- batch: 110 ----
mean loss: 862.41
train mean loss: 850.66
epoch train time: 0:00:01.987042
elapsed time: 0:01:51.016259
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 23:05:33.022546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.72
 ---- batch: 020 ----
mean loss: 862.48
 ---- batch: 030 ----
mean loss: 838.28
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 858.77
 ---- batch: 060 ----
mean loss: 853.45
 ---- batch: 070 ----
mean loss: 838.82
 ---- batch: 080 ----
mean loss: 849.86
 ---- batch: 090 ----
mean loss: 843.90
 ---- batch: 100 ----
mean loss: 842.64
 ---- batch: 110 ----
mean loss: 852.52
train mean loss: 850.17
epoch train time: 0:00:01.956627
elapsed time: 0:01:52.973498
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 23:05:34.979759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.52
 ---- batch: 020 ----
mean loss: 832.35
 ---- batch: 030 ----
mean loss: 847.21
 ---- batch: 040 ----
mean loss: 857.06
 ---- batch: 050 ----
mean loss: 842.98
 ---- batch: 060 ----
mean loss: 855.80
 ---- batch: 070 ----
mean loss: 852.40
 ---- batch: 080 ----
mean loss: 872.61
 ---- batch: 090 ----
mean loss: 858.28
 ---- batch: 100 ----
mean loss: 851.51
 ---- batch: 110 ----
mean loss: 828.64
train mean loss: 850.28
epoch train time: 0:00:01.973949
elapsed time: 0:01:54.948049
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 23:05:36.954292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.00
 ---- batch: 020 ----
mean loss: 847.11
 ---- batch: 030 ----
mean loss: 854.98
 ---- batch: 040 ----
mean loss: 845.13
 ---- batch: 050 ----
mean loss: 873.76
 ---- batch: 060 ----
mean loss: 836.85
 ---- batch: 070 ----
mean loss: 846.84
 ---- batch: 080 ----
mean loss: 865.05
 ---- batch: 090 ----
mean loss: 844.85
 ---- batch: 100 ----
mean loss: 848.24
 ---- batch: 110 ----
mean loss: 846.28
train mean loss: 850.48
epoch train time: 0:00:01.975652
elapsed time: 0:01:56.924290
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 23:05:38.930527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.90
 ---- batch: 020 ----
mean loss: 853.89
 ---- batch: 030 ----
mean loss: 856.84
 ---- batch: 040 ----
mean loss: 844.13
 ---- batch: 050 ----
mean loss: 851.85
 ---- batch: 060 ----
mean loss: 849.14
 ---- batch: 070 ----
mean loss: 813.28
 ---- batch: 080 ----
mean loss: 861.21
 ---- batch: 090 ----
mean loss: 850.97
 ---- batch: 100 ----
mean loss: 853.02
 ---- batch: 110 ----
mean loss: 857.20
train mean loss: 849.71
epoch train time: 0:00:01.942856
elapsed time: 0:01:58.867755
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 23:05:40.874012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.93
 ---- batch: 020 ----
mean loss: 842.07
 ---- batch: 030 ----
mean loss: 854.50
 ---- batch: 040 ----
mean loss: 861.88
 ---- batch: 050 ----
mean loss: 842.66
 ---- batch: 060 ----
mean loss: 870.30
 ---- batch: 070 ----
mean loss: 857.43
 ---- batch: 080 ----
mean loss: 837.41
 ---- batch: 090 ----
mean loss: 828.44
 ---- batch: 100 ----
mean loss: 837.55
 ---- batch: 110 ----
mean loss: 851.72
train mean loss: 849.53
epoch train time: 0:00:01.968224
elapsed time: 0:02:00.836641
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 23:05:42.842892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.50
 ---- batch: 020 ----
mean loss: 875.78
 ---- batch: 030 ----
mean loss: 861.07
 ---- batch: 040 ----
mean loss: 854.78
 ---- batch: 050 ----
mean loss: 840.16
 ---- batch: 060 ----
mean loss: 850.24
 ---- batch: 070 ----
mean loss: 840.95
 ---- batch: 080 ----
mean loss: 852.61
 ---- batch: 090 ----
mean loss: 845.63
 ---- batch: 100 ----
mean loss: 847.45
 ---- batch: 110 ----
mean loss: 860.68
train mean loss: 849.37
epoch train time: 0:00:02.013210
elapsed time: 0:02:02.850446
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 23:05:44.856701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.20
 ---- batch: 020 ----
mean loss: 830.69
 ---- batch: 030 ----
mean loss: 861.90
 ---- batch: 040 ----
mean loss: 860.87
 ---- batch: 050 ----
mean loss: 832.36
 ---- batch: 060 ----
mean loss: 829.92
 ---- batch: 070 ----
mean loss: 867.57
 ---- batch: 080 ----
mean loss: 827.78
 ---- batch: 090 ----
mean loss: 873.13
 ---- batch: 100 ----
mean loss: 842.23
 ---- batch: 110 ----
mean loss: 858.91
train mean loss: 848.31
epoch train time: 0:00:02.019207
elapsed time: 0:02:04.870273
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 23:05:46.876635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.28
 ---- batch: 020 ----
mean loss: 846.16
 ---- batch: 030 ----
mean loss: 834.73
 ---- batch: 040 ----
mean loss: 841.24
 ---- batch: 050 ----
mean loss: 849.29
 ---- batch: 060 ----
mean loss: 846.07
 ---- batch: 070 ----
mean loss: 867.71
 ---- batch: 080 ----
mean loss: 851.81
 ---- batch: 090 ----
mean loss: 869.61
 ---- batch: 100 ----
mean loss: 836.17
 ---- batch: 110 ----
mean loss: 843.11
train mean loss: 848.92
epoch train time: 0:00:01.979923
elapsed time: 0:02:06.850900
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 23:05:48.857160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.23
 ---- batch: 020 ----
mean loss: 846.73
 ---- batch: 030 ----
mean loss: 855.78
 ---- batch: 040 ----
mean loss: 859.57
 ---- batch: 050 ----
mean loss: 837.94
 ---- batch: 060 ----
mean loss: 856.00
 ---- batch: 070 ----
mean loss: 869.15
 ---- batch: 080 ----
mean loss: 835.52
 ---- batch: 090 ----
mean loss: 840.34
 ---- batch: 100 ----
mean loss: 856.30
 ---- batch: 110 ----
mean loss: 832.06
train mean loss: 848.61
epoch train time: 0:00:01.997381
elapsed time: 0:02:08.848874
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 23:05:50.855222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.29
 ---- batch: 020 ----
mean loss: 854.18
 ---- batch: 030 ----
mean loss: 855.02
 ---- batch: 040 ----
mean loss: 839.88
 ---- batch: 050 ----
mean loss: 838.42
 ---- batch: 060 ----
mean loss: 850.50
 ---- batch: 070 ----
mean loss: 835.64
 ---- batch: 080 ----
mean loss: 848.58
 ---- batch: 090 ----
mean loss: 851.19
 ---- batch: 100 ----
mean loss: 844.14
 ---- batch: 110 ----
mean loss: 850.53
train mean loss: 848.30
epoch train time: 0:00:01.964913
elapsed time: 0:02:10.814539
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 23:05:52.820819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.13
 ---- batch: 020 ----
mean loss: 849.94
 ---- batch: 030 ----
mean loss: 867.79
 ---- batch: 040 ----
mean loss: 857.63
 ---- batch: 050 ----
mean loss: 842.98
 ---- batch: 060 ----
mean loss: 843.36
 ---- batch: 070 ----
mean loss: 864.72
 ---- batch: 080 ----
mean loss: 857.23
 ---- batch: 090 ----
mean loss: 834.64
 ---- batch: 100 ----
mean loss: 820.89
 ---- batch: 110 ----
mean loss: 842.70
train mean loss: 847.66
epoch train time: 0:00:01.981428
elapsed time: 0:02:12.796587
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 23:05:54.802878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.33
 ---- batch: 020 ----
mean loss: 847.15
 ---- batch: 030 ----
mean loss: 846.02
 ---- batch: 040 ----
mean loss: 842.29
 ---- batch: 050 ----
mean loss: 848.43
 ---- batch: 060 ----
mean loss: 869.25
 ---- batch: 070 ----
mean loss: 847.92
 ---- batch: 080 ----
mean loss: 812.72
 ---- batch: 090 ----
mean loss: 836.32
 ---- batch: 100 ----
mean loss: 862.42
 ---- batch: 110 ----
mean loss: 851.47
train mean loss: 847.96
epoch train time: 0:00:01.952235
elapsed time: 0:02:14.749462
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 23:05:56.755747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.72
 ---- batch: 020 ----
mean loss: 839.28
 ---- batch: 030 ----
mean loss: 842.65
 ---- batch: 040 ----
mean loss: 828.08
 ---- batch: 050 ----
mean loss: 843.30
 ---- batch: 060 ----
mean loss: 860.59
 ---- batch: 070 ----
mean loss: 826.78
 ---- batch: 080 ----
mean loss: 859.10
 ---- batch: 090 ----
mean loss: 839.48
 ---- batch: 100 ----
mean loss: 854.36
 ---- batch: 110 ----
mean loss: 865.02
train mean loss: 847.26
epoch train time: 0:00:01.947573
elapsed time: 0:02:16.697693
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 23:05:58.703959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.25
 ---- batch: 020 ----
mean loss: 857.70
 ---- batch: 030 ----
mean loss: 852.50
 ---- batch: 040 ----
mean loss: 843.85
 ---- batch: 050 ----
mean loss: 832.71
 ---- batch: 060 ----
mean loss: 844.05
 ---- batch: 070 ----
mean loss: 858.20
 ---- batch: 080 ----
mean loss: 853.68
 ---- batch: 090 ----
mean loss: 835.64
 ---- batch: 100 ----
mean loss: 844.64
 ---- batch: 110 ----
mean loss: 848.63
train mean loss: 847.25
epoch train time: 0:00:01.933599
elapsed time: 0:02:18.631978
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 23:06:00.638242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.36
 ---- batch: 020 ----
mean loss: 854.74
 ---- batch: 030 ----
mean loss: 854.20
 ---- batch: 040 ----
mean loss: 847.51
 ---- batch: 050 ----
mean loss: 861.35
 ---- batch: 060 ----
mean loss: 817.91
 ---- batch: 070 ----
mean loss: 863.08
 ---- batch: 080 ----
mean loss: 819.55
 ---- batch: 090 ----
mean loss: 836.78
 ---- batch: 100 ----
mean loss: 846.83
 ---- batch: 110 ----
mean loss: 858.81
train mean loss: 847.32
epoch train time: 0:00:01.952543
elapsed time: 0:02:20.585129
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 23:06:02.591397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.38
 ---- batch: 020 ----
mean loss: 839.65
 ---- batch: 030 ----
mean loss: 847.21
 ---- batch: 040 ----
mean loss: 855.43
 ---- batch: 050 ----
mean loss: 848.93
 ---- batch: 060 ----
mean loss: 838.91
 ---- batch: 070 ----
mean loss: 845.86
 ---- batch: 080 ----
mean loss: 831.38
 ---- batch: 090 ----
mean loss: 850.18
 ---- batch: 100 ----
mean loss: 846.27
 ---- batch: 110 ----
mean loss: 852.40
train mean loss: 846.84
epoch train time: 0:00:01.934492
elapsed time: 0:02:22.520220
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 23:06:04.526192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.38
 ---- batch: 020 ----
mean loss: 854.25
 ---- batch: 030 ----
mean loss: 836.43
 ---- batch: 040 ----
mean loss: 831.51
 ---- batch: 050 ----
mean loss: 840.23
 ---- batch: 060 ----
mean loss: 847.10
 ---- batch: 070 ----
mean loss: 846.19
 ---- batch: 080 ----
mean loss: 849.64
 ---- batch: 090 ----
mean loss: 840.03
 ---- batch: 100 ----
mean loss: 838.44
 ---- batch: 110 ----
mean loss: 849.71
train mean loss: 846.63
epoch train time: 0:00:01.955992
elapsed time: 0:02:24.476553
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 23:06:06.482831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.73
 ---- batch: 020 ----
mean loss: 856.34
 ---- batch: 030 ----
mean loss: 826.06
 ---- batch: 040 ----
mean loss: 844.14
 ---- batch: 050 ----
mean loss: 838.86
 ---- batch: 060 ----
mean loss: 842.52
 ---- batch: 070 ----
mean loss: 842.92
 ---- batch: 080 ----
mean loss: 814.50
 ---- batch: 090 ----
mean loss: 883.77
 ---- batch: 100 ----
mean loss: 870.01
 ---- batch: 110 ----
mean loss: 852.27
train mean loss: 846.66
epoch train time: 0:00:01.967662
elapsed time: 0:02:26.444867
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 23:06:08.451255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.93
 ---- batch: 020 ----
mean loss: 825.61
 ---- batch: 030 ----
mean loss: 847.69
 ---- batch: 040 ----
mean loss: 849.51
 ---- batch: 050 ----
mean loss: 843.53
 ---- batch: 060 ----
mean loss: 839.37
 ---- batch: 070 ----
mean loss: 866.78
 ---- batch: 080 ----
mean loss: 844.47
 ---- batch: 090 ----
mean loss: 842.13
 ---- batch: 100 ----
mean loss: 843.93
 ---- batch: 110 ----
mean loss: 859.77
train mean loss: 846.92
epoch train time: 0:00:01.995977
elapsed time: 0:02:28.441565
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 23:06:10.447826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.44
 ---- batch: 020 ----
mean loss: 856.36
 ---- batch: 030 ----
mean loss: 861.69
 ---- batch: 040 ----
mean loss: 835.85
 ---- batch: 050 ----
mean loss: 843.77
 ---- batch: 060 ----
mean loss: 841.91
 ---- batch: 070 ----
mean loss: 857.02
 ---- batch: 080 ----
mean loss: 830.81
 ---- batch: 090 ----
mean loss: 858.63
 ---- batch: 100 ----
mean loss: 840.50
 ---- batch: 110 ----
mean loss: 825.70
train mean loss: 846.51
epoch train time: 0:00:01.975511
elapsed time: 0:02:30.417685
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 23:06:12.423927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.69
 ---- batch: 020 ----
mean loss: 844.71
 ---- batch: 030 ----
mean loss: 859.70
 ---- batch: 040 ----
mean loss: 858.55
 ---- batch: 050 ----
mean loss: 840.11
 ---- batch: 060 ----
mean loss: 831.81
 ---- batch: 070 ----
mean loss: 833.43
 ---- batch: 080 ----
mean loss: 860.28
 ---- batch: 090 ----
mean loss: 830.91
 ---- batch: 100 ----
mean loss: 850.14
 ---- batch: 110 ----
mean loss: 849.46
train mean loss: 846.65
epoch train time: 0:00:01.988418
elapsed time: 0:02:32.406719
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 23:06:14.412984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.16
 ---- batch: 020 ----
mean loss: 843.08
 ---- batch: 030 ----
mean loss: 854.10
 ---- batch: 040 ----
mean loss: 855.46
 ---- batch: 050 ----
mean loss: 836.70
 ---- batch: 060 ----
mean loss: 854.55
 ---- batch: 070 ----
mean loss: 842.62
 ---- batch: 080 ----
mean loss: 854.45
 ---- batch: 090 ----
mean loss: 856.98
 ---- batch: 100 ----
mean loss: 844.01
 ---- batch: 110 ----
mean loss: 841.81
train mean loss: 846.46
epoch train time: 0:00:02.012608
elapsed time: 0:02:34.419929
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 23:06:16.426244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.89
 ---- batch: 020 ----
mean loss: 836.85
 ---- batch: 030 ----
mean loss: 834.61
 ---- batch: 040 ----
mean loss: 850.28
 ---- batch: 050 ----
mean loss: 874.48
 ---- batch: 060 ----
mean loss: 822.00
 ---- batch: 070 ----
mean loss: 838.93
 ---- batch: 080 ----
mean loss: 857.86
 ---- batch: 090 ----
mean loss: 824.65
 ---- batch: 100 ----
mean loss: 840.81
 ---- batch: 110 ----
mean loss: 863.01
train mean loss: 846.12
epoch train time: 0:00:01.995681
elapsed time: 0:02:36.416259
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 23:06:18.422569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.98
 ---- batch: 020 ----
mean loss: 821.26
 ---- batch: 030 ----
mean loss: 843.05
 ---- batch: 040 ----
mean loss: 838.31
 ---- batch: 050 ----
mean loss: 834.17
 ---- batch: 060 ----
mean loss: 839.95
 ---- batch: 070 ----
mean loss: 834.71
 ---- batch: 080 ----
mean loss: 832.53
 ---- batch: 090 ----
mean loss: 798.01
 ---- batch: 100 ----
mean loss: 798.14
 ---- batch: 110 ----
mean loss: 746.42
train mean loss: 819.30
epoch train time: 0:00:01.968043
elapsed time: 0:02:38.384946
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 23:06:20.391223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 651.86
 ---- batch: 020 ----
mean loss: 583.35
 ---- batch: 030 ----
mean loss: 525.98
 ---- batch: 040 ----
mean loss: 496.31
 ---- batch: 050 ----
mean loss: 473.66
 ---- batch: 060 ----
mean loss: 453.59
 ---- batch: 070 ----
mean loss: 435.99
 ---- batch: 080 ----
mean loss: 432.67
 ---- batch: 090 ----
mean loss: 435.87
 ---- batch: 100 ----
mean loss: 405.14
 ---- batch: 110 ----
mean loss: 405.92
train mean loss: 479.74
epoch train time: 0:00:01.961146
elapsed time: 0:02:40.346739
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 23:06:22.353193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.57
 ---- batch: 020 ----
mean loss: 385.15
 ---- batch: 030 ----
mean loss: 380.91
 ---- batch: 040 ----
mean loss: 386.96
 ---- batch: 050 ----
mean loss: 382.44
 ---- batch: 060 ----
mean loss: 369.25
 ---- batch: 070 ----
mean loss: 365.96
 ---- batch: 080 ----
mean loss: 351.53
 ---- batch: 090 ----
mean loss: 355.56
 ---- batch: 100 ----
mean loss: 342.73
 ---- batch: 110 ----
mean loss: 350.37
train mean loss: 370.23
epoch train time: 0:00:01.952064
elapsed time: 0:02:42.299598
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 23:06:24.305878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.98
 ---- batch: 020 ----
mean loss: 338.31
 ---- batch: 030 ----
mean loss: 338.85
 ---- batch: 040 ----
mean loss: 337.41
 ---- batch: 050 ----
mean loss: 330.32
 ---- batch: 060 ----
mean loss: 336.87
 ---- batch: 070 ----
mean loss: 330.66
 ---- batch: 080 ----
mean loss: 339.47
 ---- batch: 090 ----
mean loss: 324.34
 ---- batch: 100 ----
mean loss: 325.58
 ---- batch: 110 ----
mean loss: 317.70
train mean loss: 332.83
epoch train time: 0:00:01.954563
elapsed time: 0:02:44.254750
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 23:06:26.261045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.68
 ---- batch: 020 ----
mean loss: 323.56
 ---- batch: 030 ----
mean loss: 310.07
 ---- batch: 040 ----
mean loss: 316.93
 ---- batch: 050 ----
mean loss: 320.84
 ---- batch: 060 ----
mean loss: 309.81
 ---- batch: 070 ----
mean loss: 318.87
 ---- batch: 080 ----
mean loss: 311.11
 ---- batch: 090 ----
mean loss: 315.54
 ---- batch: 100 ----
mean loss: 310.91
 ---- batch: 110 ----
mean loss: 318.76
train mean loss: 316.34
epoch train time: 0:00:01.950628
elapsed time: 0:02:46.206033
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 23:06:28.212279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.18
 ---- batch: 020 ----
mean loss: 311.41
 ---- batch: 030 ----
mean loss: 296.83
 ---- batch: 040 ----
mean loss: 302.74
 ---- batch: 050 ----
mean loss: 306.54
 ---- batch: 060 ----
mean loss: 310.54
 ---- batch: 070 ----
mean loss: 309.32
 ---- batch: 080 ----
mean loss: 301.92
 ---- batch: 090 ----
mean loss: 300.62
 ---- batch: 100 ----
mean loss: 292.38
 ---- batch: 110 ----
mean loss: 307.54
train mean loss: 304.04
epoch train time: 0:00:01.948937
elapsed time: 0:02:48.155580
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 23:06:30.161850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.47
 ---- batch: 020 ----
mean loss: 290.16
 ---- batch: 030 ----
mean loss: 297.79
 ---- batch: 040 ----
mean loss: 285.51
 ---- batch: 050 ----
mean loss: 285.18
 ---- batch: 060 ----
mean loss: 297.52
 ---- batch: 070 ----
mean loss: 297.53
 ---- batch: 080 ----
mean loss: 284.57
 ---- batch: 090 ----
mean loss: 291.06
 ---- batch: 100 ----
mean loss: 287.74
 ---- batch: 110 ----
mean loss: 296.29
train mean loss: 291.67
epoch train time: 0:00:01.955546
elapsed time: 0:02:50.111744
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 23:06:32.117996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.45
 ---- batch: 020 ----
mean loss: 279.39
 ---- batch: 030 ----
mean loss: 292.81
 ---- batch: 040 ----
mean loss: 284.76
 ---- batch: 050 ----
mean loss: 269.20
 ---- batch: 060 ----
mean loss: 275.32
 ---- batch: 070 ----
mean loss: 290.42
 ---- batch: 080 ----
mean loss: 279.92
 ---- batch: 090 ----
mean loss: 282.14
 ---- batch: 100 ----
mean loss: 281.65
 ---- batch: 110 ----
mean loss: 281.29
train mean loss: 281.94
epoch train time: 0:00:01.942295
elapsed time: 0:02:52.054602
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 23:06:34.060869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.02
 ---- batch: 020 ----
mean loss: 266.05
 ---- batch: 030 ----
mean loss: 276.56
 ---- batch: 040 ----
mean loss: 281.18
 ---- batch: 050 ----
mean loss: 261.58
 ---- batch: 060 ----
mean loss: 275.49
 ---- batch: 070 ----
mean loss: 278.06
 ---- batch: 080 ----
mean loss: 275.21
 ---- batch: 090 ----
mean loss: 276.10
 ---- batch: 100 ----
mean loss: 277.91
 ---- batch: 110 ----
mean loss: 273.59
train mean loss: 274.66
epoch train time: 0:00:01.935797
elapsed time: 0:02:53.990998
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 23:06:35.997253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.12
 ---- batch: 020 ----
mean loss: 262.03
 ---- batch: 030 ----
mean loss: 267.73
 ---- batch: 040 ----
mean loss: 270.23
 ---- batch: 050 ----
mean loss: 275.10
 ---- batch: 060 ----
mean loss: 264.81
 ---- batch: 070 ----
mean loss: 269.20
 ---- batch: 080 ----
mean loss: 267.47
 ---- batch: 090 ----
mean loss: 269.45
 ---- batch: 100 ----
mean loss: 274.86
 ---- batch: 110 ----
mean loss: 267.21
train mean loss: 267.66
epoch train time: 0:00:01.982874
elapsed time: 0:02:55.974460
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 23:06:37.980706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.96
 ---- batch: 020 ----
mean loss: 263.73
 ---- batch: 030 ----
mean loss: 266.85
 ---- batch: 040 ----
mean loss: 253.33
 ---- batch: 050 ----
mean loss: 256.25
 ---- batch: 060 ----
mean loss: 265.37
 ---- batch: 070 ----
mean loss: 258.97
 ---- batch: 080 ----
mean loss: 267.75
 ---- batch: 090 ----
mean loss: 265.88
 ---- batch: 100 ----
mean loss: 258.22
 ---- batch: 110 ----
mean loss: 259.09
train mean loss: 262.20
epoch train time: 0:00:01.964943
elapsed time: 0:02:57.939996
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 23:06:39.946250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.59
 ---- batch: 020 ----
mean loss: 256.35
 ---- batch: 030 ----
mean loss: 253.53
 ---- batch: 040 ----
mean loss: 256.67
 ---- batch: 050 ----
mean loss: 243.38
 ---- batch: 060 ----
mean loss: 265.17
 ---- batch: 070 ----
mean loss: 256.10
 ---- batch: 080 ----
mean loss: 258.49
 ---- batch: 090 ----
mean loss: 256.62
 ---- batch: 100 ----
mean loss: 253.26
 ---- batch: 110 ----
mean loss: 253.46
train mean loss: 256.04
epoch train time: 0:00:01.981820
elapsed time: 0:02:59.922450
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 23:06:41.928813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.66
 ---- batch: 020 ----
mean loss: 252.54
 ---- batch: 030 ----
mean loss: 264.25
 ---- batch: 040 ----
mean loss: 262.24
 ---- batch: 050 ----
mean loss: 259.42
 ---- batch: 060 ----
mean loss: 242.21
 ---- batch: 070 ----
mean loss: 247.88
 ---- batch: 080 ----
mean loss: 256.99
 ---- batch: 090 ----
mean loss: 245.75
 ---- batch: 100 ----
mean loss: 249.39
 ---- batch: 110 ----
mean loss: 255.41
train mean loss: 253.05
epoch train time: 0:00:01.953338
elapsed time: 0:03:01.876540
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 23:06:43.882899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.62
 ---- batch: 020 ----
mean loss: 244.19
 ---- batch: 030 ----
mean loss: 249.31
 ---- batch: 040 ----
mean loss: 249.34
 ---- batch: 050 ----
mean loss: 244.62
 ---- batch: 060 ----
mean loss: 249.57
 ---- batch: 070 ----
mean loss: 246.41
 ---- batch: 080 ----
mean loss: 250.09
 ---- batch: 090 ----
mean loss: 244.99
 ---- batch: 100 ----
mean loss: 236.41
 ---- batch: 110 ----
mean loss: 239.34
train mean loss: 245.35
epoch train time: 0:00:02.023667
elapsed time: 0:03:03.900918
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 23:06:45.907202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.73
 ---- batch: 020 ----
mean loss: 251.34
 ---- batch: 030 ----
mean loss: 238.29
 ---- batch: 040 ----
mean loss: 240.69
 ---- batch: 050 ----
mean loss: 247.98
 ---- batch: 060 ----
mean loss: 242.32
 ---- batch: 070 ----
mean loss: 238.13
 ---- batch: 080 ----
mean loss: 237.50
 ---- batch: 090 ----
mean loss: 251.95
 ---- batch: 100 ----
mean loss: 233.19
 ---- batch: 110 ----
mean loss: 229.49
train mean loss: 240.95
epoch train time: 0:00:01.936954
elapsed time: 0:03:05.838480
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 23:06:47.844778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.52
 ---- batch: 020 ----
mean loss: 238.60
 ---- batch: 030 ----
mean loss: 228.14
 ---- batch: 040 ----
mean loss: 237.65
 ---- batch: 050 ----
mean loss: 237.73
 ---- batch: 060 ----
mean loss: 247.86
 ---- batch: 070 ----
mean loss: 238.71
 ---- batch: 080 ----
mean loss: 245.36
 ---- batch: 090 ----
mean loss: 234.76
 ---- batch: 100 ----
mean loss: 233.67
 ---- batch: 110 ----
mean loss: 241.20
train mean loss: 237.61
epoch train time: 0:00:01.968316
elapsed time: 0:03:07.807448
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 23:06:49.813754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.25
 ---- batch: 020 ----
mean loss: 236.49
 ---- batch: 030 ----
mean loss: 231.75
 ---- batch: 040 ----
mean loss: 239.08
 ---- batch: 050 ----
mean loss: 236.60
 ---- batch: 060 ----
mean loss: 233.46
 ---- batch: 070 ----
mean loss: 237.47
 ---- batch: 080 ----
mean loss: 230.77
 ---- batch: 090 ----
mean loss: 238.32
 ---- batch: 100 ----
mean loss: 222.04
 ---- batch: 110 ----
mean loss: 246.23
train mean loss: 235.23
epoch train time: 0:00:01.947427
elapsed time: 0:03:09.755572
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 23:06:51.761885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.78
 ---- batch: 020 ----
mean loss: 236.02
 ---- batch: 030 ----
mean loss: 230.33
 ---- batch: 040 ----
mean loss: 229.07
 ---- batch: 050 ----
mean loss: 231.45
 ---- batch: 060 ----
mean loss: 239.19
 ---- batch: 070 ----
mean loss: 239.70
 ---- batch: 080 ----
mean loss: 224.90
 ---- batch: 090 ----
mean loss: 225.04
 ---- batch: 100 ----
mean loss: 236.64
 ---- batch: 110 ----
mean loss: 222.64
train mean loss: 230.89
epoch train time: 0:00:01.935294
elapsed time: 0:03:11.691517
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 23:06:53.697793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.44
 ---- batch: 020 ----
mean loss: 222.25
 ---- batch: 030 ----
mean loss: 222.36
 ---- batch: 040 ----
mean loss: 226.26
 ---- batch: 050 ----
mean loss: 226.36
 ---- batch: 060 ----
mean loss: 218.10
 ---- batch: 070 ----
mean loss: 225.81
 ---- batch: 080 ----
mean loss: 241.07
 ---- batch: 090 ----
mean loss: 230.14
 ---- batch: 100 ----
mean loss: 219.08
 ---- batch: 110 ----
mean loss: 235.44
train mean loss: 226.08
epoch train time: 0:00:01.980982
elapsed time: 0:03:13.673112
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 23:06:55.679357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.94
 ---- batch: 020 ----
mean loss: 231.78
 ---- batch: 030 ----
mean loss: 221.72
 ---- batch: 040 ----
mean loss: 229.44
 ---- batch: 050 ----
mean loss: 225.59
 ---- batch: 060 ----
mean loss: 226.07
 ---- batch: 070 ----
mean loss: 225.39
 ---- batch: 080 ----
mean loss: 220.80
 ---- batch: 090 ----
mean loss: 221.04
 ---- batch: 100 ----
mean loss: 224.22
 ---- batch: 110 ----
mean loss: 218.90
train mean loss: 223.83
epoch train time: 0:00:01.981794
elapsed time: 0:03:15.655480
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 23:06:57.661793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.47
 ---- batch: 020 ----
mean loss: 227.24
 ---- batch: 030 ----
mean loss: 218.62
 ---- batch: 040 ----
mean loss: 214.70
 ---- batch: 050 ----
mean loss: 220.18
 ---- batch: 060 ----
mean loss: 221.25
 ---- batch: 070 ----
mean loss: 214.44
 ---- batch: 080 ----
mean loss: 215.48
 ---- batch: 090 ----
mean loss: 220.31
 ---- batch: 100 ----
mean loss: 226.80
 ---- batch: 110 ----
mean loss: 219.35
train mean loss: 219.75
epoch train time: 0:00:01.957991
elapsed time: 0:03:17.614126
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 23:06:59.620403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.36
 ---- batch: 020 ----
mean loss: 219.42
 ---- batch: 030 ----
mean loss: 218.50
 ---- batch: 040 ----
mean loss: 212.58
 ---- batch: 050 ----
mean loss: 212.19
 ---- batch: 060 ----
mean loss: 216.06
 ---- batch: 070 ----
mean loss: 225.43
 ---- batch: 080 ----
mean loss: 228.47
 ---- batch: 090 ----
mean loss: 214.51
 ---- batch: 100 ----
mean loss: 223.74
 ---- batch: 110 ----
mean loss: 212.03
train mean loss: 217.49
epoch train time: 0:00:01.945414
elapsed time: 0:03:19.560174
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 23:07:01.566440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.17
 ---- batch: 020 ----
mean loss: 224.94
 ---- batch: 030 ----
mean loss: 206.32
 ---- batch: 040 ----
mean loss: 215.32
 ---- batch: 050 ----
mean loss: 215.94
 ---- batch: 060 ----
mean loss: 216.22
 ---- batch: 070 ----
mean loss: 221.23
 ---- batch: 080 ----
mean loss: 222.78
 ---- batch: 090 ----
mean loss: 217.09
 ---- batch: 100 ----
mean loss: 213.78
 ---- batch: 110 ----
mean loss: 212.52
train mean loss: 215.31
epoch train time: 0:00:01.956202
elapsed time: 0:03:21.516965
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 23:07:03.523278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.28
 ---- batch: 020 ----
mean loss: 222.94
 ---- batch: 030 ----
mean loss: 204.01
 ---- batch: 040 ----
mean loss: 204.88
 ---- batch: 050 ----
mean loss: 213.03
 ---- batch: 060 ----
mean loss: 206.84
 ---- batch: 070 ----
mean loss: 213.95
 ---- batch: 080 ----
mean loss: 205.75
 ---- batch: 090 ----
mean loss: 219.82
 ---- batch: 100 ----
mean loss: 208.83
 ---- batch: 110 ----
mean loss: 215.59
train mean loss: 210.83
epoch train time: 0:00:01.954853
elapsed time: 0:03:23.472441
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 23:07:05.478732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.70
 ---- batch: 020 ----
mean loss: 214.42
 ---- batch: 030 ----
mean loss: 208.08
 ---- batch: 040 ----
mean loss: 198.52
 ---- batch: 050 ----
mean loss: 208.13
 ---- batch: 060 ----
mean loss: 209.94
 ---- batch: 070 ----
mean loss: 206.16
 ---- batch: 080 ----
mean loss: 219.26
 ---- batch: 090 ----
mean loss: 215.47
 ---- batch: 100 ----
mean loss: 198.97
 ---- batch: 110 ----
mean loss: 212.72
train mean loss: 209.22
epoch train time: 0:00:01.937217
elapsed time: 0:03:25.410252
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 23:07:07.416493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.72
 ---- batch: 020 ----
mean loss: 203.85
 ---- batch: 030 ----
mean loss: 209.95
 ---- batch: 040 ----
mean loss: 204.71
 ---- batch: 050 ----
mean loss: 211.97
 ---- batch: 060 ----
mean loss: 198.12
 ---- batch: 070 ----
mean loss: 209.92
 ---- batch: 080 ----
mean loss: 211.59
 ---- batch: 090 ----
mean loss: 210.76
 ---- batch: 100 ----
mean loss: 213.48
 ---- batch: 110 ----
mean loss: 203.94
train mean loss: 206.25
epoch train time: 0:00:01.937839
elapsed time: 0:03:27.348764
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 23:07:09.355033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.12
 ---- batch: 020 ----
mean loss: 201.43
 ---- batch: 030 ----
mean loss: 205.26
 ---- batch: 040 ----
mean loss: 211.75
 ---- batch: 050 ----
mean loss: 196.40
 ---- batch: 060 ----
mean loss: 197.21
 ---- batch: 070 ----
mean loss: 208.33
 ---- batch: 080 ----
mean loss: 207.19
 ---- batch: 090 ----
mean loss: 205.15
 ---- batch: 100 ----
mean loss: 200.53
 ---- batch: 110 ----
mean loss: 201.77
train mean loss: 203.61
epoch train time: 0:00:01.928578
elapsed time: 0:03:29.277940
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 23:07:11.284313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.78
 ---- batch: 020 ----
mean loss: 200.45
 ---- batch: 030 ----
mean loss: 193.59
 ---- batch: 040 ----
mean loss: 196.17
 ---- batch: 050 ----
mean loss: 192.58
 ---- batch: 060 ----
mean loss: 209.70
 ---- batch: 070 ----
mean loss: 210.78
 ---- batch: 080 ----
mean loss: 206.11
 ---- batch: 090 ----
mean loss: 202.81
 ---- batch: 100 ----
mean loss: 207.06
 ---- batch: 110 ----
mean loss: 205.38
train mean loss: 202.24
epoch train time: 0:00:01.942025
elapsed time: 0:03:31.220660
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 23:07:13.226964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.40
 ---- batch: 020 ----
mean loss: 199.42
 ---- batch: 030 ----
mean loss: 201.80
 ---- batch: 040 ----
mean loss: 196.93
 ---- batch: 050 ----
mean loss: 195.60
 ---- batch: 060 ----
mean loss: 200.34
 ---- batch: 070 ----
mean loss: 198.41
 ---- batch: 080 ----
mean loss: 195.84
 ---- batch: 090 ----
mean loss: 194.53
 ---- batch: 100 ----
mean loss: 197.42
 ---- batch: 110 ----
mean loss: 206.84
train mean loss: 198.56
epoch train time: 0:00:01.963777
elapsed time: 0:03:33.185043
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 23:07:15.191291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.98
 ---- batch: 020 ----
mean loss: 198.18
 ---- batch: 030 ----
mean loss: 193.61
 ---- batch: 040 ----
mean loss: 201.23
 ---- batch: 050 ----
mean loss: 191.20
 ---- batch: 060 ----
mean loss: 199.83
 ---- batch: 070 ----
mean loss: 203.78
 ---- batch: 080 ----
mean loss: 197.73
 ---- batch: 090 ----
mean loss: 190.97
 ---- batch: 100 ----
mean loss: 195.85
 ---- batch: 110 ----
mean loss: 198.96
train mean loss: 197.01
epoch train time: 0:00:01.981603
elapsed time: 0:03:35.167235
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 23:07:17.173494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.51
 ---- batch: 020 ----
mean loss: 198.36
 ---- batch: 030 ----
mean loss: 193.09
 ---- batch: 040 ----
mean loss: 198.82
 ---- batch: 050 ----
mean loss: 193.00
 ---- batch: 060 ----
mean loss: 186.05
 ---- batch: 070 ----
mean loss: 200.79
 ---- batch: 080 ----
mean loss: 183.05
 ---- batch: 090 ----
mean loss: 197.72
 ---- batch: 100 ----
mean loss: 193.36
 ---- batch: 110 ----
mean loss: 195.63
train mean loss: 194.25
epoch train time: 0:00:01.953491
elapsed time: 0:03:37.121314
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 23:07:19.127587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.70
 ---- batch: 020 ----
mean loss: 196.42
 ---- batch: 030 ----
mean loss: 199.19
 ---- batch: 040 ----
mean loss: 187.62
 ---- batch: 050 ----
mean loss: 189.92
 ---- batch: 060 ----
mean loss: 195.24
 ---- batch: 070 ----
mean loss: 199.72
 ---- batch: 080 ----
mean loss: 199.72
 ---- batch: 090 ----
mean loss: 185.64
 ---- batch: 100 ----
mean loss: 192.28
 ---- batch: 110 ----
mean loss: 187.96
train mean loss: 192.87
epoch train time: 0:00:01.955131
elapsed time: 0:03:39.077080
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 23:07:21.083319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.52
 ---- batch: 020 ----
mean loss: 193.51
 ---- batch: 030 ----
mean loss: 196.00
 ---- batch: 040 ----
mean loss: 185.44
 ---- batch: 050 ----
mean loss: 191.28
 ---- batch: 060 ----
mean loss: 187.21
 ---- batch: 070 ----
mean loss: 190.17
 ---- batch: 080 ----
mean loss: 189.53
 ---- batch: 090 ----
mean loss: 188.94
 ---- batch: 100 ----
mean loss: 189.69
 ---- batch: 110 ----
mean loss: 185.71
train mean loss: 189.70
epoch train time: 0:00:01.959476
elapsed time: 0:03:41.037169
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 23:07:23.043478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.64
 ---- batch: 020 ----
mean loss: 188.73
 ---- batch: 030 ----
mean loss: 186.30
 ---- batch: 040 ----
mean loss: 196.37
 ---- batch: 050 ----
mean loss: 197.74
 ---- batch: 060 ----
mean loss: 183.48
 ---- batch: 070 ----
mean loss: 183.69
 ---- batch: 080 ----
mean loss: 183.81
 ---- batch: 090 ----
mean loss: 190.14
 ---- batch: 100 ----
mean loss: 195.42
 ---- batch: 110 ----
mean loss: 194.78
train mean loss: 189.98
epoch train time: 0:00:01.960553
elapsed time: 0:03:42.998415
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 23:07:25.004733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.68
 ---- batch: 020 ----
mean loss: 181.39
 ---- batch: 030 ----
mean loss: 171.07
 ---- batch: 040 ----
mean loss: 189.86
 ---- batch: 050 ----
mean loss: 201.06
 ---- batch: 060 ----
mean loss: 189.75
 ---- batch: 070 ----
mean loss: 188.53
 ---- batch: 080 ----
mean loss: 187.36
 ---- batch: 090 ----
mean loss: 183.59
 ---- batch: 100 ----
mean loss: 186.05
 ---- batch: 110 ----
mean loss: 194.91
train mean loss: 187.29
epoch train time: 0:00:01.963632
elapsed time: 0:03:44.962691
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 23:07:26.968660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.40
 ---- batch: 020 ----
mean loss: 184.40
 ---- batch: 030 ----
mean loss: 186.28
 ---- batch: 040 ----
mean loss: 183.27
 ---- batch: 050 ----
mean loss: 196.08
 ---- batch: 060 ----
mean loss: 183.02
 ---- batch: 070 ----
mean loss: 180.92
 ---- batch: 080 ----
mean loss: 191.66
 ---- batch: 090 ----
mean loss: 190.83
 ---- batch: 100 ----
mean loss: 185.09
 ---- batch: 110 ----
mean loss: 183.43
train mean loss: 185.67
epoch train time: 0:00:01.983042
elapsed time: 0:03:46.946042
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 23:07:28.952270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.66
 ---- batch: 020 ----
mean loss: 185.48
 ---- batch: 030 ----
mean loss: 182.28
 ---- batch: 040 ----
mean loss: 179.18
 ---- batch: 050 ----
mean loss: 184.06
 ---- batch: 060 ----
mean loss: 191.51
 ---- batch: 070 ----
mean loss: 188.17
 ---- batch: 080 ----
mean loss: 192.84
 ---- batch: 090 ----
mean loss: 183.88
 ---- batch: 100 ----
mean loss: 182.83
 ---- batch: 110 ----
mean loss: 185.88
train mean loss: 184.72
epoch train time: 0:00:01.940282
elapsed time: 0:03:48.886925
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 23:07:30.893296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.81
 ---- batch: 020 ----
mean loss: 188.06
 ---- batch: 030 ----
mean loss: 172.16
 ---- batch: 040 ----
mean loss: 185.28
 ---- batch: 050 ----
mean loss: 178.52
 ---- batch: 060 ----
mean loss: 184.29
 ---- batch: 070 ----
mean loss: 171.71
 ---- batch: 080 ----
mean loss: 176.17
 ---- batch: 090 ----
mean loss: 180.56
 ---- batch: 100 ----
mean loss: 193.94
 ---- batch: 110 ----
mean loss: 194.87
train mean loss: 183.08
epoch train time: 0:00:01.935885
elapsed time: 0:03:50.823502
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 23:07:32.829805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.79
 ---- batch: 020 ----
mean loss: 182.66
 ---- batch: 030 ----
mean loss: 185.93
 ---- batch: 040 ----
mean loss: 178.12
 ---- batch: 050 ----
mean loss: 180.99
 ---- batch: 060 ----
mean loss: 185.47
 ---- batch: 070 ----
mean loss: 175.84
 ---- batch: 080 ----
mean loss: 183.31
 ---- batch: 090 ----
mean loss: 178.78
 ---- batch: 100 ----
mean loss: 185.68
 ---- batch: 110 ----
mean loss: 183.80
train mean loss: 181.46
epoch train time: 0:00:01.979300
elapsed time: 0:03:52.803429
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 23:07:34.809728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.38
 ---- batch: 020 ----
mean loss: 176.09
 ---- batch: 030 ----
mean loss: 182.17
 ---- batch: 040 ----
mean loss: 174.57
 ---- batch: 050 ----
mean loss: 184.72
 ---- batch: 060 ----
mean loss: 172.58
 ---- batch: 070 ----
mean loss: 179.37
 ---- batch: 080 ----
mean loss: 184.18
 ---- batch: 090 ----
mean loss: 178.58
 ---- batch: 100 ----
mean loss: 177.55
 ---- batch: 110 ----
mean loss: 182.74
train mean loss: 179.79
epoch train time: 0:00:01.951465
elapsed time: 0:03:54.755524
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 23:07:36.761877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.35
 ---- batch: 020 ----
mean loss: 179.32
 ---- batch: 030 ----
mean loss: 179.08
 ---- batch: 040 ----
mean loss: 180.47
 ---- batch: 050 ----
mean loss: 177.00
 ---- batch: 060 ----
mean loss: 183.33
 ---- batch: 070 ----
mean loss: 176.15
 ---- batch: 080 ----
mean loss: 185.98
 ---- batch: 090 ----
mean loss: 177.27
 ---- batch: 100 ----
mean loss: 174.79
 ---- batch: 110 ----
mean loss: 174.63
train mean loss: 179.37
epoch train time: 0:00:01.988725
elapsed time: 0:03:56.744912
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 23:07:38.751183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.57
 ---- batch: 020 ----
mean loss: 171.92
 ---- batch: 030 ----
mean loss: 176.57
 ---- batch: 040 ----
mean loss: 178.82
 ---- batch: 050 ----
mean loss: 171.54
 ---- batch: 060 ----
mean loss: 184.08
 ---- batch: 070 ----
mean loss: 173.91
 ---- batch: 080 ----
mean loss: 172.39
 ---- batch: 090 ----
mean loss: 174.44
 ---- batch: 100 ----
mean loss: 181.42
 ---- batch: 110 ----
mean loss: 182.68
train mean loss: 177.60
epoch train time: 0:00:01.951270
elapsed time: 0:03:58.696828
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 23:07:40.703212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.96
 ---- batch: 020 ----
mean loss: 180.38
 ---- batch: 030 ----
mean loss: 175.01
 ---- batch: 040 ----
mean loss: 178.03
 ---- batch: 050 ----
mean loss: 173.80
 ---- batch: 060 ----
mean loss: 171.17
 ---- batch: 070 ----
mean loss: 179.95
 ---- batch: 080 ----
mean loss: 169.78
 ---- batch: 090 ----
mean loss: 175.01
 ---- batch: 100 ----
mean loss: 181.92
 ---- batch: 110 ----
mean loss: 184.18
train mean loss: 175.96
epoch train time: 0:00:01.931951
elapsed time: 0:04:00.629534
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 23:07:42.635949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.59
 ---- batch: 020 ----
mean loss: 169.61
 ---- batch: 030 ----
mean loss: 174.84
 ---- batch: 040 ----
mean loss: 168.63
 ---- batch: 050 ----
mean loss: 169.67
 ---- batch: 060 ----
mean loss: 180.23
 ---- batch: 070 ----
mean loss: 175.93
 ---- batch: 080 ----
mean loss: 173.84
 ---- batch: 090 ----
mean loss: 172.91
 ---- batch: 100 ----
mean loss: 183.84
 ---- batch: 110 ----
mean loss: 174.57
train mean loss: 175.01
epoch train time: 0:00:01.963546
elapsed time: 0:04:02.594051
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 23:07:44.600095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.70
 ---- batch: 020 ----
mean loss: 179.31
 ---- batch: 030 ----
mean loss: 169.37
 ---- batch: 040 ----
mean loss: 171.95
 ---- batch: 050 ----
mean loss: 174.85
 ---- batch: 060 ----
mean loss: 171.89
 ---- batch: 070 ----
mean loss: 172.75
 ---- batch: 080 ----
mean loss: 179.47
 ---- batch: 090 ----
mean loss: 177.52
 ---- batch: 100 ----
mean loss: 170.12
 ---- batch: 110 ----
mean loss: 180.54
train mean loss: 174.34
epoch train time: 0:00:01.978103
elapsed time: 0:04:04.572576
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 23:07:46.578833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.58
 ---- batch: 020 ----
mean loss: 172.21
 ---- batch: 030 ----
mean loss: 174.25
 ---- batch: 040 ----
mean loss: 164.58
 ---- batch: 050 ----
mean loss: 179.26
 ---- batch: 060 ----
mean loss: 168.19
 ---- batch: 070 ----
mean loss: 184.37
 ---- batch: 080 ----
mean loss: 177.54
 ---- batch: 090 ----
mean loss: 173.53
 ---- batch: 100 ----
mean loss: 161.12
 ---- batch: 110 ----
mean loss: 172.61
train mean loss: 172.61
epoch train time: 0:00:01.937773
elapsed time: 0:04:06.510928
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 23:07:48.517166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.93
 ---- batch: 020 ----
mean loss: 178.08
 ---- batch: 030 ----
mean loss: 163.22
 ---- batch: 040 ----
mean loss: 168.40
 ---- batch: 050 ----
mean loss: 174.06
 ---- batch: 060 ----
mean loss: 169.39
 ---- batch: 070 ----
mean loss: 174.40
 ---- batch: 080 ----
mean loss: 173.89
 ---- batch: 090 ----
mean loss: 176.70
 ---- batch: 100 ----
mean loss: 174.58
 ---- batch: 110 ----
mean loss: 174.34
train mean loss: 172.11
epoch train time: 0:00:01.962585
elapsed time: 0:04:08.474170
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 23:07:50.480467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.45
 ---- batch: 020 ----
mean loss: 164.94
 ---- batch: 030 ----
mean loss: 173.72
 ---- batch: 040 ----
mean loss: 167.82
 ---- batch: 050 ----
mean loss: 173.34
 ---- batch: 060 ----
mean loss: 166.18
 ---- batch: 070 ----
mean loss: 178.95
 ---- batch: 080 ----
mean loss: 171.99
 ---- batch: 090 ----
mean loss: 167.66
 ---- batch: 100 ----
mean loss: 167.92
 ---- batch: 110 ----
mean loss: 175.50
train mean loss: 170.21
epoch train time: 0:00:01.963399
elapsed time: 0:04:10.438198
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 23:07:52.444500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.45
 ---- batch: 020 ----
mean loss: 167.35
 ---- batch: 030 ----
mean loss: 167.79
 ---- batch: 040 ----
mean loss: 158.61
 ---- batch: 050 ----
mean loss: 175.12
 ---- batch: 060 ----
mean loss: 170.28
 ---- batch: 070 ----
mean loss: 168.60
 ---- batch: 080 ----
mean loss: 179.31
 ---- batch: 090 ----
mean loss: 169.48
 ---- batch: 100 ----
mean loss: 171.38
 ---- batch: 110 ----
mean loss: 173.63
train mean loss: 170.73
epoch train time: 0:00:01.947845
elapsed time: 0:04:12.386678
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 23:07:54.392987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.22
 ---- batch: 020 ----
mean loss: 168.30
 ---- batch: 030 ----
mean loss: 165.41
 ---- batch: 040 ----
mean loss: 166.58
 ---- batch: 050 ----
mean loss: 166.36
 ---- batch: 060 ----
mean loss: 173.30
 ---- batch: 070 ----
mean loss: 173.27
 ---- batch: 080 ----
mean loss: 175.33
 ---- batch: 090 ----
mean loss: 173.60
 ---- batch: 100 ----
mean loss: 164.29
 ---- batch: 110 ----
mean loss: 168.39
train mean loss: 169.53
epoch train time: 0:00:01.966748
elapsed time: 0:04:14.354097
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 23:07:56.360414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.85
 ---- batch: 020 ----
mean loss: 156.34
 ---- batch: 030 ----
mean loss: 168.33
 ---- batch: 040 ----
mean loss: 159.25
 ---- batch: 050 ----
mean loss: 169.23
 ---- batch: 060 ----
mean loss: 171.24
 ---- batch: 070 ----
mean loss: 160.37
 ---- batch: 080 ----
mean loss: 179.28
 ---- batch: 090 ----
mean loss: 169.03
 ---- batch: 100 ----
mean loss: 172.40
 ---- batch: 110 ----
mean loss: 180.64
train mean loss: 168.72
epoch train time: 0:00:01.969172
elapsed time: 0:04:16.323970
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 23:07:58.330235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.91
 ---- batch: 020 ----
mean loss: 174.52
 ---- batch: 030 ----
mean loss: 165.78
 ---- batch: 040 ----
mean loss: 168.32
 ---- batch: 050 ----
mean loss: 173.21
 ---- batch: 060 ----
mean loss: 180.83
 ---- batch: 070 ----
mean loss: 166.02
 ---- batch: 080 ----
mean loss: 160.65
 ---- batch: 090 ----
mean loss: 165.71
 ---- batch: 100 ----
mean loss: 168.82
 ---- batch: 110 ----
mean loss: 163.32
train mean loss: 168.22
epoch train time: 0:00:01.945741
elapsed time: 0:04:18.270332
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 23:08:00.276594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.58
 ---- batch: 020 ----
mean loss: 172.81
 ---- batch: 030 ----
mean loss: 171.42
 ---- batch: 040 ----
mean loss: 163.29
 ---- batch: 050 ----
mean loss: 172.57
 ---- batch: 060 ----
mean loss: 164.78
 ---- batch: 070 ----
mean loss: 166.83
 ---- batch: 080 ----
mean loss: 174.11
 ---- batch: 090 ----
mean loss: 161.91
 ---- batch: 100 ----
mean loss: 158.78
 ---- batch: 110 ----
mean loss: 158.38
train mean loss: 166.63
epoch train time: 0:00:01.995931
elapsed time: 0:04:20.266874
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 23:08:02.273140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.11
 ---- batch: 020 ----
mean loss: 163.28
 ---- batch: 030 ----
mean loss: 165.16
 ---- batch: 040 ----
mean loss: 169.49
 ---- batch: 050 ----
mean loss: 166.71
 ---- batch: 060 ----
mean loss: 169.27
 ---- batch: 070 ----
mean loss: 171.09
 ---- batch: 080 ----
mean loss: 171.43
 ---- batch: 090 ----
mean loss: 164.59
 ---- batch: 100 ----
mean loss: 171.25
 ---- batch: 110 ----
mean loss: 162.84
train mean loss: 166.92
epoch train time: 0:00:01.989315
elapsed time: 0:04:22.256807
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 23:08:04.263079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.65
 ---- batch: 020 ----
mean loss: 162.91
 ---- batch: 030 ----
mean loss: 160.75
 ---- batch: 040 ----
mean loss: 156.63
 ---- batch: 050 ----
mean loss: 163.83
 ---- batch: 060 ----
mean loss: 173.46
 ---- batch: 070 ----
mean loss: 168.39
 ---- batch: 080 ----
mean loss: 173.00
 ---- batch: 090 ----
mean loss: 169.31
 ---- batch: 100 ----
mean loss: 168.10
 ---- batch: 110 ----
mean loss: 163.75
train mean loss: 166.77
epoch train time: 0:00:01.928317
elapsed time: 0:04:24.185726
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 23:08:06.191982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.10
 ---- batch: 020 ----
mean loss: 159.91
 ---- batch: 030 ----
mean loss: 159.91
 ---- batch: 040 ----
mean loss: 160.60
 ---- batch: 050 ----
mean loss: 164.86
 ---- batch: 060 ----
mean loss: 168.29
 ---- batch: 070 ----
mean loss: 162.66
 ---- batch: 080 ----
mean loss: 169.89
 ---- batch: 090 ----
mean loss: 170.12
 ---- batch: 100 ----
mean loss: 167.93
 ---- batch: 110 ----
mean loss: 168.25
train mean loss: 164.80
epoch train time: 0:00:01.972684
elapsed time: 0:04:26.159068
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 23:08:08.165342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.80
 ---- batch: 020 ----
mean loss: 161.33
 ---- batch: 030 ----
mean loss: 157.09
 ---- batch: 040 ----
mean loss: 164.83
 ---- batch: 050 ----
mean loss: 166.42
 ---- batch: 060 ----
mean loss: 169.95
 ---- batch: 070 ----
mean loss: 163.26
 ---- batch: 080 ----
mean loss: 167.57
 ---- batch: 090 ----
mean loss: 168.46
 ---- batch: 100 ----
mean loss: 164.13
 ---- batch: 110 ----
mean loss: 164.43
train mean loss: 164.25
epoch train time: 0:00:01.968313
elapsed time: 0:04:28.127978
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 23:08:10.134214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.98
 ---- batch: 020 ----
mean loss: 150.38
 ---- batch: 030 ----
mean loss: 172.29
 ---- batch: 040 ----
mean loss: 162.34
 ---- batch: 050 ----
mean loss: 161.86
 ---- batch: 060 ----
mean loss: 163.21
 ---- batch: 070 ----
mean loss: 168.75
 ---- batch: 080 ----
mean loss: 157.90
 ---- batch: 090 ----
mean loss: 167.94
 ---- batch: 100 ----
mean loss: 167.11
 ---- batch: 110 ----
mean loss: 163.64
train mean loss: 163.37
epoch train time: 0:00:01.985922
elapsed time: 0:04:30.114449
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 23:08:12.120765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.03
 ---- batch: 020 ----
mean loss: 160.02
 ---- batch: 030 ----
mean loss: 163.97
 ---- batch: 040 ----
mean loss: 160.59
 ---- batch: 050 ----
mean loss: 149.09
 ---- batch: 060 ----
mean loss: 163.05
 ---- batch: 070 ----
mean loss: 169.20
 ---- batch: 080 ----
mean loss: 168.57
 ---- batch: 090 ----
mean loss: 162.94
 ---- batch: 100 ----
mean loss: 171.42
 ---- batch: 110 ----
mean loss: 167.00
train mean loss: 163.34
epoch train time: 0:00:01.977021
elapsed time: 0:04:32.092114
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 23:08:14.098378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.67
 ---- batch: 020 ----
mean loss: 164.71
 ---- batch: 030 ----
mean loss: 157.43
 ---- batch: 040 ----
mean loss: 156.98
 ---- batch: 050 ----
mean loss: 166.22
 ---- batch: 060 ----
mean loss: 166.64
 ---- batch: 070 ----
mean loss: 167.66
 ---- batch: 080 ----
mean loss: 165.39
 ---- batch: 090 ----
mean loss: 159.92
 ---- batch: 100 ----
mean loss: 168.21
 ---- batch: 110 ----
mean loss: 152.43
train mean loss: 162.26
epoch train time: 0:00:02.014479
elapsed time: 0:04:34.107181
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 23:08:16.113442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.03
 ---- batch: 020 ----
mean loss: 151.62
 ---- batch: 030 ----
mean loss: 159.14
 ---- batch: 040 ----
mean loss: 161.48
 ---- batch: 050 ----
mean loss: 157.79
 ---- batch: 060 ----
mean loss: 165.02
 ---- batch: 070 ----
mean loss: 166.46
 ---- batch: 080 ----
mean loss: 166.94
 ---- batch: 090 ----
mean loss: 169.96
 ---- batch: 100 ----
mean loss: 163.35
 ---- batch: 110 ----
mean loss: 152.35
train mean loss: 161.13
epoch train time: 0:00:01.951077
elapsed time: 0:04:36.058883
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 23:08:18.065240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.31
 ---- batch: 020 ----
mean loss: 150.83
 ---- batch: 030 ----
mean loss: 161.60
 ---- batch: 040 ----
mean loss: 160.93
 ---- batch: 050 ----
mean loss: 167.97
 ---- batch: 060 ----
mean loss: 168.55
 ---- batch: 070 ----
mean loss: 165.94
 ---- batch: 080 ----
mean loss: 156.63
 ---- batch: 090 ----
mean loss: 162.64
 ---- batch: 100 ----
mean loss: 157.02
 ---- batch: 110 ----
mean loss: 162.23
train mean loss: 160.95
epoch train time: 0:00:01.937596
elapsed time: 0:04:37.997169
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 23:08:20.003486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.73
 ---- batch: 020 ----
mean loss: 151.15
 ---- batch: 030 ----
mean loss: 162.82
 ---- batch: 040 ----
mean loss: 162.22
 ---- batch: 050 ----
mean loss: 152.53
 ---- batch: 060 ----
mean loss: 160.02
 ---- batch: 070 ----
mean loss: 169.12
 ---- batch: 080 ----
mean loss: 169.56
 ---- batch: 090 ----
mean loss: 157.23
 ---- batch: 100 ----
mean loss: 160.27
 ---- batch: 110 ----
mean loss: 165.92
train mean loss: 160.73
epoch train time: 0:00:01.943844
elapsed time: 0:04:39.941634
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 23:08:21.947859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.62
 ---- batch: 020 ----
mean loss: 154.57
 ---- batch: 030 ----
mean loss: 158.82
 ---- batch: 040 ----
mean loss: 162.19
 ---- batch: 050 ----
mean loss: 159.05
 ---- batch: 060 ----
mean loss: 158.74
 ---- batch: 070 ----
mean loss: 159.09
 ---- batch: 080 ----
mean loss: 164.78
 ---- batch: 090 ----
mean loss: 159.46
 ---- batch: 100 ----
mean loss: 159.42
 ---- batch: 110 ----
mean loss: 160.89
train mean loss: 159.57
epoch train time: 0:00:01.984640
elapsed time: 0:04:41.927115
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 23:08:23.933139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.90
 ---- batch: 020 ----
mean loss: 159.69
 ---- batch: 030 ----
mean loss: 156.19
 ---- batch: 040 ----
mean loss: 141.05
 ---- batch: 050 ----
mean loss: 170.92
 ---- batch: 060 ----
mean loss: 159.01
 ---- batch: 070 ----
mean loss: 156.03
 ---- batch: 080 ----
mean loss: 159.42
 ---- batch: 090 ----
mean loss: 161.06
 ---- batch: 100 ----
mean loss: 156.68
 ---- batch: 110 ----
mean loss: 163.05
train mean loss: 158.54
epoch train time: 0:00:01.947181
elapsed time: 0:04:43.874666
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 23:08:25.880918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.93
 ---- batch: 020 ----
mean loss: 150.36
 ---- batch: 030 ----
mean loss: 158.08
 ---- batch: 040 ----
mean loss: 156.35
 ---- batch: 050 ----
mean loss: 161.44
 ---- batch: 060 ----
mean loss: 158.32
 ---- batch: 070 ----
mean loss: 162.48
 ---- batch: 080 ----
mean loss: 162.07
 ---- batch: 090 ----
mean loss: 153.45
 ---- batch: 100 ----
mean loss: 163.62
 ---- batch: 110 ----
mean loss: 149.40
train mean loss: 158.12
epoch train time: 0:00:01.981364
elapsed time: 0:04:45.856640
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 23:08:27.862890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.01
 ---- batch: 020 ----
mean loss: 164.20
 ---- batch: 030 ----
mean loss: 153.18
 ---- batch: 040 ----
mean loss: 157.78
 ---- batch: 050 ----
mean loss: 160.38
 ---- batch: 060 ----
mean loss: 159.98
 ---- batch: 070 ----
mean loss: 150.44
 ---- batch: 080 ----
mean loss: 157.97
 ---- batch: 090 ----
mean loss: 156.62
 ---- batch: 100 ----
mean loss: 158.35
 ---- batch: 110 ----
mean loss: 161.52
train mean loss: 157.62
epoch train time: 0:00:01.981341
elapsed time: 0:04:47.838573
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 23:08:29.844930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.17
 ---- batch: 020 ----
mean loss: 151.33
 ---- batch: 030 ----
mean loss: 158.03
 ---- batch: 040 ----
mean loss: 166.26
 ---- batch: 050 ----
mean loss: 145.33
 ---- batch: 060 ----
mean loss: 156.42
 ---- batch: 070 ----
mean loss: 163.04
 ---- batch: 080 ----
mean loss: 160.58
 ---- batch: 090 ----
mean loss: 159.34
 ---- batch: 100 ----
mean loss: 152.01
 ---- batch: 110 ----
mean loss: 156.64
train mean loss: 156.62
epoch train time: 0:00:02.017302
elapsed time: 0:04:49.856636
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 23:08:31.862906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.16
 ---- batch: 020 ----
mean loss: 156.90
 ---- batch: 030 ----
mean loss: 158.58
 ---- batch: 040 ----
mean loss: 160.20
 ---- batch: 050 ----
mean loss: 154.26
 ---- batch: 060 ----
mean loss: 154.07
 ---- batch: 070 ----
mean loss: 156.92
 ---- batch: 080 ----
mean loss: 153.60
 ---- batch: 090 ----
mean loss: 153.98
 ---- batch: 100 ----
mean loss: 162.43
 ---- batch: 110 ----
mean loss: 161.35
train mean loss: 157.13
epoch train time: 0:00:01.986834
elapsed time: 0:04:51.844057
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 23:08:33.850299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.88
 ---- batch: 020 ----
mean loss: 155.99
 ---- batch: 030 ----
mean loss: 151.10
 ---- batch: 040 ----
mean loss: 157.34
 ---- batch: 050 ----
mean loss: 154.59
 ---- batch: 060 ----
mean loss: 158.63
 ---- batch: 070 ----
mean loss: 167.34
 ---- batch: 080 ----
mean loss: 152.44
 ---- batch: 090 ----
mean loss: 150.33
 ---- batch: 100 ----
mean loss: 154.84
 ---- batch: 110 ----
mean loss: 153.95
train mean loss: 155.85
epoch train time: 0:00:01.944677
elapsed time: 0:04:53.789383
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 23:08:35.795655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.59
 ---- batch: 020 ----
mean loss: 152.58
 ---- batch: 030 ----
mean loss: 154.44
 ---- batch: 040 ----
mean loss: 147.61
 ---- batch: 050 ----
mean loss: 155.43
 ---- batch: 060 ----
mean loss: 146.20
 ---- batch: 070 ----
mean loss: 161.56
 ---- batch: 080 ----
mean loss: 154.94
 ---- batch: 090 ----
mean loss: 164.31
 ---- batch: 100 ----
mean loss: 146.58
 ---- batch: 110 ----
mean loss: 160.95
train mean loss: 155.73
epoch train time: 0:00:01.924533
elapsed time: 0:04:55.714528
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 23:08:37.720775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.14
 ---- batch: 020 ----
mean loss: 155.38
 ---- batch: 030 ----
mean loss: 159.67
 ---- batch: 040 ----
mean loss: 157.39
 ---- batch: 050 ----
mean loss: 153.28
 ---- batch: 060 ----
mean loss: 154.59
 ---- batch: 070 ----
mean loss: 156.12
 ---- batch: 080 ----
mean loss: 152.57
 ---- batch: 090 ----
mean loss: 152.93
 ---- batch: 100 ----
mean loss: 158.87
 ---- batch: 110 ----
mean loss: 151.18
train mean loss: 154.88
epoch train time: 0:00:01.908543
elapsed time: 0:04:57.623634
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 23:08:39.629884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.68
 ---- batch: 020 ----
mean loss: 155.91
 ---- batch: 030 ----
mean loss: 158.54
 ---- batch: 040 ----
mean loss: 159.14
 ---- batch: 050 ----
mean loss: 147.70
 ---- batch: 060 ----
mean loss: 156.07
 ---- batch: 070 ----
mean loss: 155.36
 ---- batch: 080 ----
mean loss: 153.97
 ---- batch: 090 ----
mean loss: 154.88
 ---- batch: 100 ----
mean loss: 153.00
 ---- batch: 110 ----
mean loss: 158.55
train mean loss: 155.08
epoch train time: 0:00:01.954705
elapsed time: 0:04:59.578910
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 23:08:41.585335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.09
 ---- batch: 020 ----
mean loss: 149.58
 ---- batch: 030 ----
mean loss: 151.60
 ---- batch: 040 ----
mean loss: 151.73
 ---- batch: 050 ----
mean loss: 147.43
 ---- batch: 060 ----
mean loss: 153.21
 ---- batch: 070 ----
mean loss: 158.65
 ---- batch: 080 ----
mean loss: 157.12
 ---- batch: 090 ----
mean loss: 155.00
 ---- batch: 100 ----
mean loss: 149.29
 ---- batch: 110 ----
mean loss: 152.10
train mean loss: 153.87
epoch train time: 0:00:02.009103
elapsed time: 0:05:01.588754
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 23:08:43.595051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.35
 ---- batch: 020 ----
mean loss: 152.32
 ---- batch: 030 ----
mean loss: 148.81
 ---- batch: 040 ----
mean loss: 153.02
 ---- batch: 050 ----
mean loss: 154.55
 ---- batch: 060 ----
mean loss: 156.34
 ---- batch: 070 ----
mean loss: 147.15
 ---- batch: 080 ----
mean loss: 153.01
 ---- batch: 090 ----
mean loss: 149.26
 ---- batch: 100 ----
mean loss: 165.92
 ---- batch: 110 ----
mean loss: 162.69
train mean loss: 152.89
epoch train time: 0:00:01.938224
elapsed time: 0:05:03.527583
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 23:08:45.533837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.69
 ---- batch: 020 ----
mean loss: 156.17
 ---- batch: 030 ----
mean loss: 146.19
 ---- batch: 040 ----
mean loss: 160.22
 ---- batch: 050 ----
mean loss: 144.85
 ---- batch: 060 ----
mean loss: 159.60
 ---- batch: 070 ----
mean loss: 149.63
 ---- batch: 080 ----
mean loss: 151.50
 ---- batch: 090 ----
mean loss: 148.05
 ---- batch: 100 ----
mean loss: 158.53
 ---- batch: 110 ----
mean loss: 153.44
train mean loss: 153.30
epoch train time: 0:00:01.902332
elapsed time: 0:05:05.430520
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 23:08:47.436765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.73
 ---- batch: 020 ----
mean loss: 155.09
 ---- batch: 030 ----
mean loss: 158.02
 ---- batch: 040 ----
mean loss: 155.03
 ---- batch: 050 ----
mean loss: 157.90
 ---- batch: 060 ----
mean loss: 156.33
 ---- batch: 070 ----
mean loss: 149.82
 ---- batch: 080 ----
mean loss: 149.36
 ---- batch: 090 ----
mean loss: 153.77
 ---- batch: 100 ----
mean loss: 144.97
 ---- batch: 110 ----
mean loss: 153.31
train mean loss: 152.83
epoch train time: 0:00:01.943931
elapsed time: 0:05:07.375031
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 23:08:49.381271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.21
 ---- batch: 020 ----
mean loss: 145.46
 ---- batch: 030 ----
mean loss: 153.72
 ---- batch: 040 ----
mean loss: 157.23
 ---- batch: 050 ----
mean loss: 153.40
 ---- batch: 060 ----
mean loss: 146.42
 ---- batch: 070 ----
mean loss: 150.77
 ---- batch: 080 ----
mean loss: 155.70
 ---- batch: 090 ----
mean loss: 163.62
 ---- batch: 100 ----
mean loss: 151.35
 ---- batch: 110 ----
mean loss: 149.46
train mean loss: 152.03
epoch train time: 0:00:01.958110
elapsed time: 0:05:09.333737
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 23:08:51.340024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.18
 ---- batch: 020 ----
mean loss: 151.58
 ---- batch: 030 ----
mean loss: 151.61
 ---- batch: 040 ----
mean loss: 147.47
 ---- batch: 050 ----
mean loss: 152.77
 ---- batch: 060 ----
mean loss: 149.31
 ---- batch: 070 ----
mean loss: 149.80
 ---- batch: 080 ----
mean loss: 156.03
 ---- batch: 090 ----
mean loss: 154.19
 ---- batch: 100 ----
mean loss: 150.01
 ---- batch: 110 ----
mean loss: 146.95
train mean loss: 151.50
epoch train time: 0:00:01.958536
elapsed time: 0:05:11.292873
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 23:08:53.299142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.11
 ---- batch: 020 ----
mean loss: 159.12
 ---- batch: 030 ----
mean loss: 151.82
 ---- batch: 040 ----
mean loss: 149.61
 ---- batch: 050 ----
mean loss: 144.92
 ---- batch: 060 ----
mean loss: 150.90
 ---- batch: 070 ----
mean loss: 148.52
 ---- batch: 080 ----
mean loss: 151.15
 ---- batch: 090 ----
mean loss: 150.41
 ---- batch: 100 ----
mean loss: 158.89
 ---- batch: 110 ----
mean loss: 160.03
train mean loss: 151.86
epoch train time: 0:00:01.957670
elapsed time: 0:05:13.251184
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 23:08:55.257470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.66
 ---- batch: 020 ----
mean loss: 143.40
 ---- batch: 030 ----
mean loss: 144.55
 ---- batch: 040 ----
mean loss: 153.52
 ---- batch: 050 ----
mean loss: 150.87
 ---- batch: 060 ----
mean loss: 142.23
 ---- batch: 070 ----
mean loss: 156.48
 ---- batch: 080 ----
mean loss: 156.60
 ---- batch: 090 ----
mean loss: 163.27
 ---- batch: 100 ----
mean loss: 153.17
 ---- batch: 110 ----
mean loss: 140.40
train mean loss: 150.49
epoch train time: 0:00:01.949025
elapsed time: 0:05:15.200837
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 23:08:57.207099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.56
 ---- batch: 020 ----
mean loss: 145.18
 ---- batch: 030 ----
mean loss: 151.75
 ---- batch: 040 ----
mean loss: 153.65
 ---- batch: 050 ----
mean loss: 147.88
 ---- batch: 060 ----
mean loss: 154.32
 ---- batch: 070 ----
mean loss: 146.00
 ---- batch: 080 ----
mean loss: 150.28
 ---- batch: 090 ----
mean loss: 148.68
 ---- batch: 100 ----
mean loss: 155.33
 ---- batch: 110 ----
mean loss: 148.21
train mean loss: 149.82
epoch train time: 0:00:01.974208
elapsed time: 0:05:17.175634
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 23:08:59.181987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.25
 ---- batch: 020 ----
mean loss: 153.39
 ---- batch: 030 ----
mean loss: 150.56
 ---- batch: 040 ----
mean loss: 148.26
 ---- batch: 050 ----
mean loss: 150.81
 ---- batch: 060 ----
mean loss: 151.04
 ---- batch: 070 ----
mean loss: 151.95
 ---- batch: 080 ----
mean loss: 149.62
 ---- batch: 090 ----
mean loss: 151.52
 ---- batch: 100 ----
mean loss: 150.23
 ---- batch: 110 ----
mean loss: 152.77
train mean loss: 149.48
epoch train time: 0:00:01.946403
elapsed time: 0:05:19.122724
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 23:09:01.129003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.06
 ---- batch: 020 ----
mean loss: 150.43
 ---- batch: 030 ----
mean loss: 141.27
 ---- batch: 040 ----
mean loss: 146.65
 ---- batch: 050 ----
mean loss: 149.87
 ---- batch: 060 ----
mean loss: 149.35
 ---- batch: 070 ----
mean loss: 153.37
 ---- batch: 080 ----
mean loss: 144.84
 ---- batch: 090 ----
mean loss: 150.77
 ---- batch: 100 ----
mean loss: 145.99
 ---- batch: 110 ----
mean loss: 153.11
train mean loss: 148.73
epoch train time: 0:00:01.953186
elapsed time: 0:05:21.076487
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 23:09:03.082730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.22
 ---- batch: 020 ----
mean loss: 153.30
 ---- batch: 030 ----
mean loss: 143.66
 ---- batch: 040 ----
mean loss: 145.05
 ---- batch: 050 ----
mean loss: 152.70
 ---- batch: 060 ----
mean loss: 148.26
 ---- batch: 070 ----
mean loss: 148.86
 ---- batch: 080 ----
mean loss: 152.33
 ---- batch: 090 ----
mean loss: 153.73
 ---- batch: 100 ----
mean loss: 154.36
 ---- batch: 110 ----
mean loss: 142.37
train mean loss: 149.18
epoch train time: 0:00:01.972199
elapsed time: 0:05:23.049259
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 23:09:05.055516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.36
 ---- batch: 020 ----
mean loss: 151.42
 ---- batch: 030 ----
mean loss: 149.75
 ---- batch: 040 ----
mean loss: 139.43
 ---- batch: 050 ----
mean loss: 146.10
 ---- batch: 060 ----
mean loss: 146.84
 ---- batch: 070 ----
mean loss: 149.65
 ---- batch: 080 ----
mean loss: 153.96
 ---- batch: 090 ----
mean loss: 151.52
 ---- batch: 100 ----
mean loss: 152.47
 ---- batch: 110 ----
mean loss: 145.90
train mean loss: 147.80
epoch train time: 0:00:01.971895
elapsed time: 0:05:25.022119
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 23:09:07.028071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.65
 ---- batch: 020 ----
mean loss: 142.89
 ---- batch: 030 ----
mean loss: 145.02
 ---- batch: 040 ----
mean loss: 133.75
 ---- batch: 050 ----
mean loss: 140.34
 ---- batch: 060 ----
mean loss: 147.49
 ---- batch: 070 ----
mean loss: 150.80
 ---- batch: 080 ----
mean loss: 152.60
 ---- batch: 090 ----
mean loss: 154.43
 ---- batch: 100 ----
mean loss: 153.70
 ---- batch: 110 ----
mean loss: 146.10
train mean loss: 147.31
epoch train time: 0:00:01.973802
elapsed time: 0:05:26.996205
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 23:09:09.002484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.26
 ---- batch: 020 ----
mean loss: 142.88
 ---- batch: 030 ----
mean loss: 145.10
 ---- batch: 040 ----
mean loss: 141.88
 ---- batch: 050 ----
mean loss: 143.65
 ---- batch: 060 ----
mean loss: 153.48
 ---- batch: 070 ----
mean loss: 146.32
 ---- batch: 080 ----
mean loss: 152.71
 ---- batch: 090 ----
mean loss: 153.61
 ---- batch: 100 ----
mean loss: 148.63
 ---- batch: 110 ----
mean loss: 147.23
train mean loss: 147.63
epoch train time: 0:00:01.975638
elapsed time: 0:05:28.972466
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 23:09:10.978435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.97
 ---- batch: 020 ----
mean loss: 145.26
 ---- batch: 030 ----
mean loss: 148.74
 ---- batch: 040 ----
mean loss: 145.22
 ---- batch: 050 ----
mean loss: 137.77
 ---- batch: 060 ----
mean loss: 151.40
 ---- batch: 070 ----
mean loss: 155.09
 ---- batch: 080 ----
mean loss: 150.39
 ---- batch: 090 ----
mean loss: 147.35
 ---- batch: 100 ----
mean loss: 157.55
 ---- batch: 110 ----
mean loss: 145.65
train mean loss: 148.07
epoch train time: 0:00:02.002642
elapsed time: 0:05:30.975407
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 23:09:12.981720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.02
 ---- batch: 020 ----
mean loss: 140.52
 ---- batch: 030 ----
mean loss: 144.97
 ---- batch: 040 ----
mean loss: 143.76
 ---- batch: 050 ----
mean loss: 146.63
 ---- batch: 060 ----
mean loss: 143.18
 ---- batch: 070 ----
mean loss: 142.87
 ---- batch: 080 ----
mean loss: 150.71
 ---- batch: 090 ----
mean loss: 152.21
 ---- batch: 100 ----
mean loss: 152.07
 ---- batch: 110 ----
mean loss: 146.95
train mean loss: 147.11
epoch train time: 0:00:01.967627
elapsed time: 0:05:32.943657
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 23:09:14.949920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.83
 ---- batch: 020 ----
mean loss: 138.15
 ---- batch: 030 ----
mean loss: 146.97
 ---- batch: 040 ----
mean loss: 142.06
 ---- batch: 050 ----
mean loss: 150.27
 ---- batch: 060 ----
mean loss: 148.86
 ---- batch: 070 ----
mean loss: 152.13
 ---- batch: 080 ----
mean loss: 143.67
 ---- batch: 090 ----
mean loss: 143.36
 ---- batch: 100 ----
mean loss: 146.41
 ---- batch: 110 ----
mean loss: 150.29
train mean loss: 146.65
epoch train time: 0:00:01.958341
elapsed time: 0:05:34.902864
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 23:09:16.909273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.69
 ---- batch: 020 ----
mean loss: 141.03
 ---- batch: 030 ----
mean loss: 143.62
 ---- batch: 040 ----
mean loss: 146.28
 ---- batch: 050 ----
mean loss: 144.31
 ---- batch: 060 ----
mean loss: 146.16
 ---- batch: 070 ----
mean loss: 139.27
 ---- batch: 080 ----
mean loss: 141.89
 ---- batch: 090 ----
mean loss: 154.47
 ---- batch: 100 ----
mean loss: 145.78
 ---- batch: 110 ----
mean loss: 153.89
train mean loss: 145.42
epoch train time: 0:00:01.959631
elapsed time: 0:05:36.863243
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 23:09:18.869513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.68
 ---- batch: 020 ----
mean loss: 141.67
 ---- batch: 030 ----
mean loss: 149.37
 ---- batch: 040 ----
mean loss: 142.00
 ---- batch: 050 ----
mean loss: 139.32
 ---- batch: 060 ----
mean loss: 145.74
 ---- batch: 070 ----
mean loss: 157.18
 ---- batch: 080 ----
mean loss: 147.66
 ---- batch: 090 ----
mean loss: 150.15
 ---- batch: 100 ----
mean loss: 146.11
 ---- batch: 110 ----
mean loss: 150.12
train mean loss: 146.41
epoch train time: 0:00:01.952009
elapsed time: 0:05:38.816187
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 23:09:20.822491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.09
 ---- batch: 020 ----
mean loss: 138.20
 ---- batch: 030 ----
mean loss: 152.56
 ---- batch: 040 ----
mean loss: 153.72
 ---- batch: 050 ----
mean loss: 145.14
 ---- batch: 060 ----
mean loss: 141.70
 ---- batch: 070 ----
mean loss: 147.22
 ---- batch: 080 ----
mean loss: 142.01
 ---- batch: 090 ----
mean loss: 140.21
 ---- batch: 100 ----
mean loss: 151.44
 ---- batch: 110 ----
mean loss: 146.93
train mean loss: 145.41
epoch train time: 0:00:01.927023
elapsed time: 0:05:40.743869
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 23:09:22.750140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.48
 ---- batch: 020 ----
mean loss: 140.52
 ---- batch: 030 ----
mean loss: 145.88
 ---- batch: 040 ----
mean loss: 141.04
 ---- batch: 050 ----
mean loss: 138.01
 ---- batch: 060 ----
mean loss: 150.12
 ---- batch: 070 ----
mean loss: 134.22
 ---- batch: 080 ----
mean loss: 141.01
 ---- batch: 090 ----
mean loss: 149.22
 ---- batch: 100 ----
mean loss: 143.04
 ---- batch: 110 ----
mean loss: 157.44
train mean loss: 144.16
epoch train time: 0:00:01.944503
elapsed time: 0:05:42.689001
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 23:09:24.695285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.34
 ---- batch: 020 ----
mean loss: 141.26
 ---- batch: 030 ----
mean loss: 143.49
 ---- batch: 040 ----
mean loss: 142.94
 ---- batch: 050 ----
mean loss: 143.59
 ---- batch: 060 ----
mean loss: 142.35
 ---- batch: 070 ----
mean loss: 145.70
 ---- batch: 080 ----
mean loss: 142.25
 ---- batch: 090 ----
mean loss: 130.68
 ---- batch: 100 ----
mean loss: 146.57
 ---- batch: 110 ----
mean loss: 154.19
train mean loss: 144.00
epoch train time: 0:00:01.964108
elapsed time: 0:05:44.653718
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 23:09:26.659955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.20
 ---- batch: 020 ----
mean loss: 142.96
 ---- batch: 030 ----
mean loss: 138.80
 ---- batch: 040 ----
mean loss: 149.61
 ---- batch: 050 ----
mean loss: 144.30
 ---- batch: 060 ----
mean loss: 138.47
 ---- batch: 070 ----
mean loss: 143.79
 ---- batch: 080 ----
mean loss: 146.99
 ---- batch: 090 ----
mean loss: 142.87
 ---- batch: 100 ----
mean loss: 149.96
 ---- batch: 110 ----
mean loss: 137.08
train mean loss: 143.87
epoch train time: 0:00:01.971609
elapsed time: 0:05:46.625900
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 23:09:28.632198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.80
 ---- batch: 020 ----
mean loss: 138.08
 ---- batch: 030 ----
mean loss: 139.09
 ---- batch: 040 ----
mean loss: 147.98
 ---- batch: 050 ----
mean loss: 152.52
 ---- batch: 060 ----
mean loss: 143.24
 ---- batch: 070 ----
mean loss: 136.23
 ---- batch: 080 ----
mean loss: 153.16
 ---- batch: 090 ----
mean loss: 149.23
 ---- batch: 100 ----
mean loss: 137.30
 ---- batch: 110 ----
mean loss: 136.97
train mean loss: 143.23
epoch train time: 0:00:01.990779
elapsed time: 0:05:48.617317
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 23:09:30.623578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.72
 ---- batch: 020 ----
mean loss: 144.26
 ---- batch: 030 ----
mean loss: 141.13
 ---- batch: 040 ----
mean loss: 144.92
 ---- batch: 050 ----
mean loss: 150.97
 ---- batch: 060 ----
mean loss: 143.77
 ---- batch: 070 ----
mean loss: 141.89
 ---- batch: 080 ----
mean loss: 140.56
 ---- batch: 090 ----
mean loss: 145.61
 ---- batch: 100 ----
mean loss: 150.35
 ---- batch: 110 ----
mean loss: 140.62
train mean loss: 143.47
epoch train time: 0:00:01.936760
elapsed time: 0:05:50.554671
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 23:09:32.560923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.86
 ---- batch: 020 ----
mean loss: 148.08
 ---- batch: 030 ----
mean loss: 142.67
 ---- batch: 040 ----
mean loss: 140.79
 ---- batch: 050 ----
mean loss: 153.92
 ---- batch: 060 ----
mean loss: 142.01
 ---- batch: 070 ----
mean loss: 136.31
 ---- batch: 080 ----
mean loss: 127.67
 ---- batch: 090 ----
mean loss: 142.28
 ---- batch: 100 ----
mean loss: 152.46
 ---- batch: 110 ----
mean loss: 146.10
train mean loss: 143.00
epoch train time: 0:00:01.967376
elapsed time: 0:05:52.522621
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 23:09:34.528908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.24
 ---- batch: 020 ----
mean loss: 140.77
 ---- batch: 030 ----
mean loss: 141.50
 ---- batch: 040 ----
mean loss: 145.99
 ---- batch: 050 ----
mean loss: 142.73
 ---- batch: 060 ----
mean loss: 141.50
 ---- batch: 070 ----
mean loss: 142.10
 ---- batch: 080 ----
mean loss: 142.38
 ---- batch: 090 ----
mean loss: 141.51
 ---- batch: 100 ----
mean loss: 132.50
 ---- batch: 110 ----
mean loss: 141.15
train mean loss: 141.79
epoch train time: 0:00:01.944599
elapsed time: 0:05:54.467832
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 23:09:36.474061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.12
 ---- batch: 020 ----
mean loss: 145.06
 ---- batch: 030 ----
mean loss: 140.86
 ---- batch: 040 ----
mean loss: 140.44
 ---- batch: 050 ----
mean loss: 144.00
 ---- batch: 060 ----
mean loss: 138.04
 ---- batch: 070 ----
mean loss: 150.49
 ---- batch: 080 ----
mean loss: 137.03
 ---- batch: 090 ----
mean loss: 135.31
 ---- batch: 100 ----
mean loss: 148.79
 ---- batch: 110 ----
mean loss: 141.78
train mean loss: 142.15
epoch train time: 0:00:01.952123
elapsed time: 0:05:56.420499
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 23:09:38.426857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.47
 ---- batch: 020 ----
mean loss: 134.32
 ---- batch: 030 ----
mean loss: 146.03
 ---- batch: 040 ----
mean loss: 141.71
 ---- batch: 050 ----
mean loss: 152.91
 ---- batch: 060 ----
mean loss: 139.01
 ---- batch: 070 ----
mean loss: 139.19
 ---- batch: 080 ----
mean loss: 138.98
 ---- batch: 090 ----
mean loss: 148.93
 ---- batch: 100 ----
mean loss: 141.65
 ---- batch: 110 ----
mean loss: 131.11
train mean loss: 141.76
epoch train time: 0:00:01.948498
elapsed time: 0:05:58.369688
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 23:09:40.376019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.10
 ---- batch: 020 ----
mean loss: 142.09
 ---- batch: 030 ----
mean loss: 136.86
 ---- batch: 040 ----
mean loss: 140.19
 ---- batch: 050 ----
mean loss: 134.12
 ---- batch: 060 ----
mean loss: 138.81
 ---- batch: 070 ----
mean loss: 128.47
 ---- batch: 080 ----
mean loss: 153.23
 ---- batch: 090 ----
mean loss: 144.32
 ---- batch: 100 ----
mean loss: 158.00
 ---- batch: 110 ----
mean loss: 140.14
train mean loss: 141.10
epoch train time: 0:00:01.949674
elapsed time: 0:06:00.320065
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 23:09:42.326329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.18
 ---- batch: 020 ----
mean loss: 141.69
 ---- batch: 030 ----
mean loss: 136.76
 ---- batch: 040 ----
mean loss: 138.10
 ---- batch: 050 ----
mean loss: 141.15
 ---- batch: 060 ----
mean loss: 137.82
 ---- batch: 070 ----
mean loss: 143.87
 ---- batch: 080 ----
mean loss: 135.46
 ---- batch: 090 ----
mean loss: 153.29
 ---- batch: 100 ----
mean loss: 144.09
 ---- batch: 110 ----
mean loss: 145.34
train mean loss: 141.42
epoch train time: 0:00:01.959163
elapsed time: 0:06:02.279828
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 23:09:44.286092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.35
 ---- batch: 020 ----
mean loss: 141.27
 ---- batch: 030 ----
mean loss: 137.16
 ---- batch: 040 ----
mean loss: 151.06
 ---- batch: 050 ----
mean loss: 148.10
 ---- batch: 060 ----
mean loss: 136.15
 ---- batch: 070 ----
mean loss: 132.99
 ---- batch: 080 ----
mean loss: 141.33
 ---- batch: 090 ----
mean loss: 137.98
 ---- batch: 100 ----
mean loss: 137.49
 ---- batch: 110 ----
mean loss: 140.73
train mean loss: 140.18
epoch train time: 0:00:01.940525
elapsed time: 0:06:04.220951
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 23:09:46.227223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.14
 ---- batch: 020 ----
mean loss: 141.39
 ---- batch: 030 ----
mean loss: 132.58
 ---- batch: 040 ----
mean loss: 145.28
 ---- batch: 050 ----
mean loss: 142.58
 ---- batch: 060 ----
mean loss: 142.28
 ---- batch: 070 ----
mean loss: 138.02
 ---- batch: 080 ----
mean loss: 146.15
 ---- batch: 090 ----
mean loss: 137.22
 ---- batch: 100 ----
mean loss: 147.88
 ---- batch: 110 ----
mean loss: 137.25
train mean loss: 140.40
epoch train time: 0:00:01.926884
elapsed time: 0:06:06.148418
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 23:09:48.154669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.32
 ---- batch: 020 ----
mean loss: 138.33
 ---- batch: 030 ----
mean loss: 137.41
 ---- batch: 040 ----
mean loss: 138.52
 ---- batch: 050 ----
mean loss: 136.83
 ---- batch: 060 ----
mean loss: 144.83
 ---- batch: 070 ----
mean loss: 136.46
 ---- batch: 080 ----
mean loss: 142.04
 ---- batch: 090 ----
mean loss: 145.97
 ---- batch: 100 ----
mean loss: 138.21
 ---- batch: 110 ----
mean loss: 138.76
train mean loss: 139.95
epoch train time: 0:00:01.911294
elapsed time: 0:06:08.060293
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 23:09:50.066539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.95
 ---- batch: 020 ----
mean loss: 146.11
 ---- batch: 030 ----
mean loss: 143.61
 ---- batch: 040 ----
mean loss: 137.58
 ---- batch: 050 ----
mean loss: 135.60
 ---- batch: 060 ----
mean loss: 134.63
 ---- batch: 070 ----
mean loss: 146.81
 ---- batch: 080 ----
mean loss: 134.04
 ---- batch: 090 ----
mean loss: 141.82
 ---- batch: 100 ----
mean loss: 143.79
 ---- batch: 110 ----
mean loss: 140.80
train mean loss: 139.65
epoch train time: 0:00:01.925701
elapsed time: 0:06:09.986592
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 23:09:51.992840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.13
 ---- batch: 020 ----
mean loss: 139.76
 ---- batch: 030 ----
mean loss: 137.40
 ---- batch: 040 ----
mean loss: 132.08
 ---- batch: 050 ----
mean loss: 137.27
 ---- batch: 060 ----
mean loss: 139.10
 ---- batch: 070 ----
mean loss: 132.33
 ---- batch: 080 ----
mean loss: 141.63
 ---- batch: 090 ----
mean loss: 149.33
 ---- batch: 100 ----
mean loss: 137.29
 ---- batch: 110 ----
mean loss: 145.98
train mean loss: 138.79
epoch train time: 0:00:01.962951
elapsed time: 0:06:11.950242
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 23:09:53.956490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.06
 ---- batch: 020 ----
mean loss: 137.18
 ---- batch: 030 ----
mean loss: 139.84
 ---- batch: 040 ----
mean loss: 138.19
 ---- batch: 050 ----
mean loss: 132.21
 ---- batch: 060 ----
mean loss: 138.79
 ---- batch: 070 ----
mean loss: 140.58
 ---- batch: 080 ----
mean loss: 135.86
 ---- batch: 090 ----
mean loss: 144.28
 ---- batch: 100 ----
mean loss: 133.42
 ---- batch: 110 ----
mean loss: 149.69
train mean loss: 139.53
epoch train time: 0:00:01.964016
elapsed time: 0:06:13.915178
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 23:09:55.921141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.26
 ---- batch: 020 ----
mean loss: 140.68
 ---- batch: 030 ----
mean loss: 135.82
 ---- batch: 040 ----
mean loss: 140.15
 ---- batch: 050 ----
mean loss: 145.43
 ---- batch: 060 ----
mean loss: 138.06
 ---- batch: 070 ----
mean loss: 139.37
 ---- batch: 080 ----
mean loss: 133.54
 ---- batch: 090 ----
mean loss: 145.76
 ---- batch: 100 ----
mean loss: 140.93
 ---- batch: 110 ----
mean loss: 137.86
train mean loss: 138.66
epoch train time: 0:00:01.924860
elapsed time: 0:06:15.840358
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 23:09:57.846655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.69
 ---- batch: 020 ----
mean loss: 136.78
 ---- batch: 030 ----
mean loss: 139.66
 ---- batch: 040 ----
mean loss: 137.56
 ---- batch: 050 ----
mean loss: 133.53
 ---- batch: 060 ----
mean loss: 142.68
 ---- batch: 070 ----
mean loss: 140.65
 ---- batch: 080 ----
mean loss: 134.97
 ---- batch: 090 ----
mean loss: 138.72
 ---- batch: 100 ----
mean loss: 140.61
 ---- batch: 110 ----
mean loss: 133.11
train mean loss: 138.38
epoch train time: 0:00:01.957262
elapsed time: 0:06:17.798280
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 23:09:59.804540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.59
 ---- batch: 020 ----
mean loss: 141.71
 ---- batch: 030 ----
mean loss: 124.20
 ---- batch: 040 ----
mean loss: 143.89
 ---- batch: 050 ----
mean loss: 138.19
 ---- batch: 060 ----
mean loss: 140.09
 ---- batch: 070 ----
mean loss: 136.85
 ---- batch: 080 ----
mean loss: 144.96
 ---- batch: 090 ----
mean loss: 133.63
 ---- batch: 100 ----
mean loss: 140.25
 ---- batch: 110 ----
mean loss: 135.05
train mean loss: 137.98
epoch train time: 0:00:01.928974
elapsed time: 0:06:19.727859
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 23:10:01.734114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.30
 ---- batch: 020 ----
mean loss: 139.74
 ---- batch: 030 ----
mean loss: 140.29
 ---- batch: 040 ----
mean loss: 132.35
 ---- batch: 050 ----
mean loss: 133.38
 ---- batch: 060 ----
mean loss: 138.15
 ---- batch: 070 ----
mean loss: 134.66
 ---- batch: 080 ----
mean loss: 133.21
 ---- batch: 090 ----
mean loss: 140.81
 ---- batch: 100 ----
mean loss: 142.85
 ---- batch: 110 ----
mean loss: 134.98
train mean loss: 137.34
epoch train time: 0:00:01.988916
elapsed time: 0:06:21.717359
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 23:10:03.723604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.39
 ---- batch: 020 ----
mean loss: 142.21
 ---- batch: 030 ----
mean loss: 137.61
 ---- batch: 040 ----
mean loss: 130.44
 ---- batch: 050 ----
mean loss: 135.30
 ---- batch: 060 ----
mean loss: 140.27
 ---- batch: 070 ----
mean loss: 133.88
 ---- batch: 080 ----
mean loss: 137.27
 ---- batch: 090 ----
mean loss: 144.32
 ---- batch: 100 ----
mean loss: 135.49
 ---- batch: 110 ----
mean loss: 136.36
train mean loss: 137.25
epoch train time: 0:00:01.961908
elapsed time: 0:06:23.679880
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 23:10:05.686139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.71
 ---- batch: 020 ----
mean loss: 134.41
 ---- batch: 030 ----
mean loss: 136.19
 ---- batch: 040 ----
mean loss: 140.71
 ---- batch: 050 ----
mean loss: 136.23
 ---- batch: 060 ----
mean loss: 130.93
 ---- batch: 070 ----
mean loss: 137.88
 ---- batch: 080 ----
mean loss: 140.76
 ---- batch: 090 ----
mean loss: 135.72
 ---- batch: 100 ----
mean loss: 139.92
 ---- batch: 110 ----
mean loss: 144.80
train mean loss: 137.51
epoch train time: 0:00:01.955583
elapsed time: 0:06:25.636074
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 23:10:07.642378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.92
 ---- batch: 020 ----
mean loss: 137.43
 ---- batch: 030 ----
mean loss: 129.72
 ---- batch: 040 ----
mean loss: 132.15
 ---- batch: 050 ----
mean loss: 147.36
 ---- batch: 060 ----
mean loss: 138.20
 ---- batch: 070 ----
mean loss: 140.24
 ---- batch: 080 ----
mean loss: 138.83
 ---- batch: 090 ----
mean loss: 134.84
 ---- batch: 100 ----
mean loss: 137.70
 ---- batch: 110 ----
mean loss: 141.81
train mean loss: 137.01
epoch train time: 0:00:01.934204
elapsed time: 0:06:27.570927
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 23:10:09.577181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.86
 ---- batch: 020 ----
mean loss: 137.88
 ---- batch: 030 ----
mean loss: 144.95
 ---- batch: 040 ----
mean loss: 124.89
 ---- batch: 050 ----
mean loss: 132.17
 ---- batch: 060 ----
mean loss: 126.86
 ---- batch: 070 ----
mean loss: 132.10
 ---- batch: 080 ----
mean loss: 138.95
 ---- batch: 090 ----
mean loss: 143.56
 ---- batch: 100 ----
mean loss: 143.93
 ---- batch: 110 ----
mean loss: 143.34
train mean loss: 136.38
epoch train time: 0:00:01.933284
elapsed time: 0:06:29.504855
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 23:10:11.511226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.15
 ---- batch: 020 ----
mean loss: 140.61
 ---- batch: 030 ----
mean loss: 129.44
 ---- batch: 040 ----
mean loss: 141.60
 ---- batch: 050 ----
mean loss: 135.96
 ---- batch: 060 ----
mean loss: 138.88
 ---- batch: 070 ----
mean loss: 134.75
 ---- batch: 080 ----
mean loss: 132.45
 ---- batch: 090 ----
mean loss: 138.73
 ---- batch: 100 ----
mean loss: 137.83
 ---- batch: 110 ----
mean loss: 135.04
train mean loss: 136.61
epoch train time: 0:00:01.940182
elapsed time: 0:06:31.445746
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 23:10:13.451992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.59
 ---- batch: 020 ----
mean loss: 136.75
 ---- batch: 030 ----
mean loss: 125.58
 ---- batch: 040 ----
mean loss: 132.24
 ---- batch: 050 ----
mean loss: 131.88
 ---- batch: 060 ----
mean loss: 132.45
 ---- batch: 070 ----
mean loss: 139.54
 ---- batch: 080 ----
mean loss: 139.97
 ---- batch: 090 ----
mean loss: 142.70
 ---- batch: 100 ----
mean loss: 139.47
 ---- batch: 110 ----
mean loss: 137.84
train mean loss: 135.60
epoch train time: 0:00:01.950098
elapsed time: 0:06:33.396411
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 23:10:15.402666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.74
 ---- batch: 020 ----
mean loss: 135.46
 ---- batch: 030 ----
mean loss: 139.67
 ---- batch: 040 ----
mean loss: 132.44
 ---- batch: 050 ----
mean loss: 136.46
 ---- batch: 060 ----
mean loss: 136.66
 ---- batch: 070 ----
mean loss: 136.52
 ---- batch: 080 ----
mean loss: 138.68
 ---- batch: 090 ----
mean loss: 124.86
 ---- batch: 100 ----
mean loss: 134.18
 ---- batch: 110 ----
mean loss: 144.79
train mean loss: 135.48
epoch train time: 0:00:01.975492
elapsed time: 0:06:35.372558
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 23:10:17.378791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.18
 ---- batch: 020 ----
mean loss: 128.48
 ---- batch: 030 ----
mean loss: 136.72
 ---- batch: 040 ----
mean loss: 132.16
 ---- batch: 050 ----
mean loss: 139.10
 ---- batch: 060 ----
mean loss: 134.95
 ---- batch: 070 ----
mean loss: 136.65
 ---- batch: 080 ----
mean loss: 130.50
 ---- batch: 090 ----
mean loss: 135.14
 ---- batch: 100 ----
mean loss: 140.77
 ---- batch: 110 ----
mean loss: 138.70
train mean loss: 134.89
epoch train time: 0:00:01.976777
elapsed time: 0:06:37.349900
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 23:10:19.356181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.21
 ---- batch: 020 ----
mean loss: 140.65
 ---- batch: 030 ----
mean loss: 139.25
 ---- batch: 040 ----
mean loss: 133.32
 ---- batch: 050 ----
mean loss: 139.16
 ---- batch: 060 ----
mean loss: 134.74
 ---- batch: 070 ----
mean loss: 134.33
 ---- batch: 080 ----
mean loss: 135.34
 ---- batch: 090 ----
mean loss: 137.12
 ---- batch: 100 ----
mean loss: 129.50
 ---- batch: 110 ----
mean loss: 132.48
train mean loss: 135.01
epoch train time: 0:00:01.968326
elapsed time: 0:06:39.318830
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 23:10:21.325087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.15
 ---- batch: 020 ----
mean loss: 128.78
 ---- batch: 030 ----
mean loss: 134.32
 ---- batch: 040 ----
mean loss: 140.40
 ---- batch: 050 ----
mean loss: 140.70
 ---- batch: 060 ----
mean loss: 140.96
 ---- batch: 070 ----
mean loss: 125.56
 ---- batch: 080 ----
mean loss: 130.94
 ---- batch: 090 ----
mean loss: 135.51
 ---- batch: 100 ----
mean loss: 136.64
 ---- batch: 110 ----
mean loss: 135.40
train mean loss: 134.18
epoch train time: 0:00:01.969486
elapsed time: 0:06:41.288940
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 23:10:23.295216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.16
 ---- batch: 020 ----
mean loss: 136.68
 ---- batch: 030 ----
mean loss: 141.63
 ---- batch: 040 ----
mean loss: 125.48
 ---- batch: 050 ----
mean loss: 132.22
 ---- batch: 060 ----
mean loss: 130.82
 ---- batch: 070 ----
mean loss: 134.85
 ---- batch: 080 ----
mean loss: 137.04
 ---- batch: 090 ----
mean loss: 139.15
 ---- batch: 100 ----
mean loss: 135.43
 ---- batch: 110 ----
mean loss: 132.75
train mean loss: 134.52
epoch train time: 0:00:01.955571
elapsed time: 0:06:43.245149
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 23:10:25.251444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.13
 ---- batch: 020 ----
mean loss: 131.95
 ---- batch: 030 ----
mean loss: 143.58
 ---- batch: 040 ----
mean loss: 131.67
 ---- batch: 050 ----
mean loss: 126.87
 ---- batch: 060 ----
mean loss: 134.88
 ---- batch: 070 ----
mean loss: 130.99
 ---- batch: 080 ----
mean loss: 139.72
 ---- batch: 090 ----
mean loss: 131.38
 ---- batch: 100 ----
mean loss: 131.54
 ---- batch: 110 ----
mean loss: 133.30
train mean loss: 133.68
epoch train time: 0:00:01.929721
elapsed time: 0:06:45.175549
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 23:10:27.181916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.79
 ---- batch: 020 ----
mean loss: 125.53
 ---- batch: 030 ----
mean loss: 135.40
 ---- batch: 040 ----
mean loss: 128.74
 ---- batch: 050 ----
mean loss: 130.69
 ---- batch: 060 ----
mean loss: 140.57
 ---- batch: 070 ----
mean loss: 142.07
 ---- batch: 080 ----
mean loss: 132.45
 ---- batch: 090 ----
mean loss: 133.65
 ---- batch: 100 ----
mean loss: 135.09
 ---- batch: 110 ----
mean loss: 139.34
train mean loss: 133.67
epoch train time: 0:00:01.943860
elapsed time: 0:06:47.120113
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 23:10:29.126367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.39
 ---- batch: 020 ----
mean loss: 125.15
 ---- batch: 030 ----
mean loss: 132.53
 ---- batch: 040 ----
mean loss: 133.56
 ---- batch: 050 ----
mean loss: 136.76
 ---- batch: 060 ----
mean loss: 138.63
 ---- batch: 070 ----
mean loss: 131.01
 ---- batch: 080 ----
mean loss: 130.07
 ---- batch: 090 ----
mean loss: 134.53
 ---- batch: 100 ----
mean loss: 139.66
 ---- batch: 110 ----
mean loss: 130.56
train mean loss: 132.70
epoch train time: 0:00:01.961534
elapsed time: 0:06:49.082226
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 23:10:31.088477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.54
 ---- batch: 020 ----
mean loss: 131.85
 ---- batch: 030 ----
mean loss: 137.38
 ---- batch: 040 ----
mean loss: 136.43
 ---- batch: 050 ----
mean loss: 136.08
 ---- batch: 060 ----
mean loss: 137.00
 ---- batch: 070 ----
mean loss: 126.15
 ---- batch: 080 ----
mean loss: 133.67
 ---- batch: 090 ----
mean loss: 127.71
 ---- batch: 100 ----
mean loss: 131.62
 ---- batch: 110 ----
mean loss: 134.40
train mean loss: 133.57
epoch train time: 0:00:01.967888
elapsed time: 0:06:51.050689
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 23:10:33.056983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.09
 ---- batch: 020 ----
mean loss: 132.10
 ---- batch: 030 ----
mean loss: 127.03
 ---- batch: 040 ----
mean loss: 138.12
 ---- batch: 050 ----
mean loss: 133.40
 ---- batch: 060 ----
mean loss: 125.97
 ---- batch: 070 ----
mean loss: 131.04
 ---- batch: 080 ----
mean loss: 131.60
 ---- batch: 090 ----
mean loss: 129.37
 ---- batch: 100 ----
mean loss: 139.13
 ---- batch: 110 ----
mean loss: 140.66
train mean loss: 132.67
epoch train time: 0:00:01.947505
elapsed time: 0:06:52.998948
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 23:10:35.005239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.59
 ---- batch: 020 ----
mean loss: 139.88
 ---- batch: 030 ----
mean loss: 136.09
 ---- batch: 040 ----
mean loss: 132.73
 ---- batch: 050 ----
mean loss: 127.80
 ---- batch: 060 ----
mean loss: 126.08
 ---- batch: 070 ----
mean loss: 134.67
 ---- batch: 080 ----
mean loss: 131.05
 ---- batch: 090 ----
mean loss: 130.87
 ---- batch: 100 ----
mean loss: 132.77
 ---- batch: 110 ----
mean loss: 133.46
train mean loss: 131.93
epoch train time: 0:00:01.961155
elapsed time: 0:06:54.960702
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 23:10:36.966970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.34
 ---- batch: 020 ----
mean loss: 135.70
 ---- batch: 030 ----
mean loss: 136.94
 ---- batch: 040 ----
mean loss: 131.61
 ---- batch: 050 ----
mean loss: 121.89
 ---- batch: 060 ----
mean loss: 128.03
 ---- batch: 070 ----
mean loss: 137.01
 ---- batch: 080 ----
mean loss: 129.17
 ---- batch: 090 ----
mean loss: 138.17
 ---- batch: 100 ----
mean loss: 132.79
 ---- batch: 110 ----
mean loss: 138.01
train mean loss: 132.17
epoch train time: 0:00:01.937967
elapsed time: 0:06:56.899256
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 23:10:38.905549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.97
 ---- batch: 020 ----
mean loss: 128.18
 ---- batch: 030 ----
mean loss: 127.30
 ---- batch: 040 ----
mean loss: 138.03
 ---- batch: 050 ----
mean loss: 125.23
 ---- batch: 060 ----
mean loss: 131.80
 ---- batch: 070 ----
mean loss: 138.73
 ---- batch: 080 ----
mean loss: 131.86
 ---- batch: 090 ----
mean loss: 132.05
 ---- batch: 100 ----
mean loss: 125.40
 ---- batch: 110 ----
mean loss: 139.91
train mean loss: 132.08
epoch train time: 0:00:01.949655
elapsed time: 0:06:58.849544
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 23:10:40.855794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.46
 ---- batch: 020 ----
mean loss: 127.88
 ---- batch: 030 ----
mean loss: 133.09
 ---- batch: 040 ----
mean loss: 124.85
 ---- batch: 050 ----
mean loss: 132.30
 ---- batch: 060 ----
mean loss: 126.19
 ---- batch: 070 ----
mean loss: 143.56
 ---- batch: 080 ----
mean loss: 135.35
 ---- batch: 090 ----
mean loss: 135.07
 ---- batch: 100 ----
mean loss: 129.61
 ---- batch: 110 ----
mean loss: 130.50
train mean loss: 131.33
epoch train time: 0:00:01.950082
elapsed time: 0:07:00.800200
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 23:10:42.806432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.93
 ---- batch: 020 ----
mean loss: 122.33
 ---- batch: 030 ----
mean loss: 128.69
 ---- batch: 040 ----
mean loss: 129.07
 ---- batch: 050 ----
mean loss: 125.71
 ---- batch: 060 ----
mean loss: 136.91
 ---- batch: 070 ----
mean loss: 124.00
 ---- batch: 080 ----
mean loss: 134.49
 ---- batch: 090 ----
mean loss: 127.55
 ---- batch: 100 ----
mean loss: 132.17
 ---- batch: 110 ----
mean loss: 139.42
train mean loss: 130.68
epoch train time: 0:00:01.953774
elapsed time: 0:07:02.754549
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 23:10:44.760828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.87
 ---- batch: 020 ----
mean loss: 127.52
 ---- batch: 030 ----
mean loss: 135.47
 ---- batch: 040 ----
mean loss: 122.86
 ---- batch: 050 ----
mean loss: 127.46
 ---- batch: 060 ----
mean loss: 139.19
 ---- batch: 070 ----
mean loss: 137.80
 ---- batch: 080 ----
mean loss: 133.39
 ---- batch: 090 ----
mean loss: 125.65
 ---- batch: 100 ----
mean loss: 127.73
 ---- batch: 110 ----
mean loss: 132.58
train mean loss: 131.18
epoch train time: 0:00:01.936289
elapsed time: 0:07:04.691462
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 23:10:46.697762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.92
 ---- batch: 020 ----
mean loss: 127.38
 ---- batch: 030 ----
mean loss: 129.03
 ---- batch: 040 ----
mean loss: 126.99
 ---- batch: 050 ----
mean loss: 130.34
 ---- batch: 060 ----
mean loss: 131.33
 ---- batch: 070 ----
mean loss: 128.80
 ---- batch: 080 ----
mean loss: 126.70
 ---- batch: 090 ----
mean loss: 137.89
 ---- batch: 100 ----
mean loss: 136.24
 ---- batch: 110 ----
mean loss: 138.97
train mean loss: 130.85
epoch train time: 0:00:01.926647
elapsed time: 0:07:06.618733
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 23:10:48.624990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.58
 ---- batch: 020 ----
mean loss: 127.09
 ---- batch: 030 ----
mean loss: 135.29
 ---- batch: 040 ----
mean loss: 122.94
 ---- batch: 050 ----
mean loss: 123.88
 ---- batch: 060 ----
mean loss: 125.43
 ---- batch: 070 ----
mean loss: 120.36
 ---- batch: 080 ----
mean loss: 123.37
 ---- batch: 090 ----
mean loss: 133.03
 ---- batch: 100 ----
mean loss: 129.25
 ---- batch: 110 ----
mean loss: 119.04
train mean loss: 125.51
epoch train time: 0:00:01.937929
elapsed time: 0:07:08.557569
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 23:10:50.563516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.75
 ---- batch: 020 ----
mean loss: 124.09
 ---- batch: 030 ----
mean loss: 118.85
 ---- batch: 040 ----
mean loss: 128.53
 ---- batch: 050 ----
mean loss: 124.58
 ---- batch: 060 ----
mean loss: 126.14
 ---- batch: 070 ----
mean loss: 117.29
 ---- batch: 080 ----
mean loss: 128.74
 ---- batch: 090 ----
mean loss: 126.79
 ---- batch: 100 ----
mean loss: 129.42
 ---- batch: 110 ----
mean loss: 120.74
train mean loss: 124.87
epoch train time: 0:00:01.945296
elapsed time: 0:07:10.503203
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 23:10:52.509465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.99
 ---- batch: 020 ----
mean loss: 126.54
 ---- batch: 030 ----
mean loss: 128.69
 ---- batch: 040 ----
mean loss: 124.68
 ---- batch: 050 ----
mean loss: 125.86
 ---- batch: 060 ----
mean loss: 121.49
 ---- batch: 070 ----
mean loss: 119.78
 ---- batch: 080 ----
mean loss: 127.90
 ---- batch: 090 ----
mean loss: 127.33
 ---- batch: 100 ----
mean loss: 125.56
 ---- batch: 110 ----
mean loss: 124.51
train mean loss: 124.71
epoch train time: 0:00:01.977511
elapsed time: 0:07:12.481305
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 23:10:54.487280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.31
 ---- batch: 020 ----
mean loss: 121.45
 ---- batch: 030 ----
mean loss: 124.47
 ---- batch: 040 ----
mean loss: 120.38
 ---- batch: 050 ----
mean loss: 128.70
 ---- batch: 060 ----
mean loss: 124.54
 ---- batch: 070 ----
mean loss: 129.55
 ---- batch: 080 ----
mean loss: 128.96
 ---- batch: 090 ----
mean loss: 128.19
 ---- batch: 100 ----
mean loss: 119.75
 ---- batch: 110 ----
mean loss: 121.81
train mean loss: 124.58
epoch train time: 0:00:01.997990
elapsed time: 0:07:14.479645
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 23:10:56.485950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.43
 ---- batch: 020 ----
mean loss: 118.52
 ---- batch: 030 ----
mean loss: 127.20
 ---- batch: 040 ----
mean loss: 123.70
 ---- batch: 050 ----
mean loss: 123.14
 ---- batch: 060 ----
mean loss: 123.77
 ---- batch: 070 ----
mean loss: 124.37
 ---- batch: 080 ----
mean loss: 131.09
 ---- batch: 090 ----
mean loss: 118.81
 ---- batch: 100 ----
mean loss: 125.61
 ---- batch: 110 ----
mean loss: 129.95
train mean loss: 124.60
epoch train time: 0:00:02.015359
elapsed time: 0:07:16.495673
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 23:10:58.502004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.22
 ---- batch: 020 ----
mean loss: 120.95
 ---- batch: 030 ----
mean loss: 124.07
 ---- batch: 040 ----
mean loss: 126.12
 ---- batch: 050 ----
mean loss: 116.34
 ---- batch: 060 ----
mean loss: 126.58
 ---- batch: 070 ----
mean loss: 127.52
 ---- batch: 080 ----
mean loss: 129.34
 ---- batch: 090 ----
mean loss: 129.29
 ---- batch: 100 ----
mean loss: 118.21
 ---- batch: 110 ----
mean loss: 125.78
train mean loss: 124.60
epoch train time: 0:00:01.990568
elapsed time: 0:07:18.486908
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 23:11:00.493182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.16
 ---- batch: 020 ----
mean loss: 125.77
 ---- batch: 030 ----
mean loss: 126.91
 ---- batch: 040 ----
mean loss: 130.49
 ---- batch: 050 ----
mean loss: 121.04
 ---- batch: 060 ----
mean loss: 125.17
 ---- batch: 070 ----
mean loss: 115.39
 ---- batch: 080 ----
mean loss: 131.38
 ---- batch: 090 ----
mean loss: 121.26
 ---- batch: 100 ----
mean loss: 129.05
 ---- batch: 110 ----
mean loss: 125.75
train mean loss: 124.48
epoch train time: 0:00:01.981888
elapsed time: 0:07:20.469410
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 23:11:02.475671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.10
 ---- batch: 020 ----
mean loss: 122.63
 ---- batch: 030 ----
mean loss: 119.39
 ---- batch: 040 ----
mean loss: 124.64
 ---- batch: 050 ----
mean loss: 125.50
 ---- batch: 060 ----
mean loss: 131.02
 ---- batch: 070 ----
mean loss: 128.82
 ---- batch: 080 ----
mean loss: 125.35
 ---- batch: 090 ----
mean loss: 125.52
 ---- batch: 100 ----
mean loss: 121.93
 ---- batch: 110 ----
mean loss: 126.94
train mean loss: 124.46
epoch train time: 0:00:01.942778
elapsed time: 0:07:22.412800
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 23:11:04.419095
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.54
 ---- batch: 020 ----
mean loss: 117.63
 ---- batch: 030 ----
mean loss: 127.34
 ---- batch: 040 ----
mean loss: 123.40
 ---- batch: 050 ----
mean loss: 125.21
 ---- batch: 060 ----
mean loss: 127.53
 ---- batch: 070 ----
mean loss: 126.64
 ---- batch: 080 ----
mean loss: 122.07
 ---- batch: 090 ----
mean loss: 126.98
 ---- batch: 100 ----
mean loss: 120.20
 ---- batch: 110 ----
mean loss: 131.44
train mean loss: 124.31
epoch train time: 0:00:01.937100
elapsed time: 0:07:24.350526
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 23:11:06.356776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.17
 ---- batch: 020 ----
mean loss: 123.94
 ---- batch: 030 ----
mean loss: 123.66
 ---- batch: 040 ----
mean loss: 130.17
 ---- batch: 050 ----
mean loss: 126.10
 ---- batch: 060 ----
mean loss: 124.25
 ---- batch: 070 ----
mean loss: 118.27
 ---- batch: 080 ----
mean loss: 122.15
 ---- batch: 090 ----
mean loss: 119.91
 ---- batch: 100 ----
mean loss: 120.17
 ---- batch: 110 ----
mean loss: 122.93
train mean loss: 124.32
epoch train time: 0:00:01.996846
elapsed time: 0:07:26.347929
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 23:11:08.354179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.24
 ---- batch: 020 ----
mean loss: 128.83
 ---- batch: 030 ----
mean loss: 127.82
 ---- batch: 040 ----
mean loss: 120.71
 ---- batch: 050 ----
mean loss: 125.35
 ---- batch: 060 ----
mean loss: 121.95
 ---- batch: 070 ----
mean loss: 120.41
 ---- batch: 080 ----
mean loss: 122.55
 ---- batch: 090 ----
mean loss: 128.07
 ---- batch: 100 ----
mean loss: 120.14
 ---- batch: 110 ----
mean loss: 125.48
train mean loss: 124.29
epoch train time: 0:00:01.945754
elapsed time: 0:07:28.294268
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 23:11:10.300780
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.40
 ---- batch: 020 ----
mean loss: 118.54
 ---- batch: 030 ----
mean loss: 120.94
 ---- batch: 040 ----
mean loss: 129.31
 ---- batch: 050 ----
mean loss: 128.53
 ---- batch: 060 ----
mean loss: 125.41
 ---- batch: 070 ----
mean loss: 128.60
 ---- batch: 080 ----
mean loss: 121.58
 ---- batch: 090 ----
mean loss: 125.82
 ---- batch: 100 ----
mean loss: 127.02
 ---- batch: 110 ----
mean loss: 119.37
train mean loss: 124.25
epoch train time: 0:00:01.922193
elapsed time: 0:07:30.217292
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 23:11:12.223570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.45
 ---- batch: 020 ----
mean loss: 124.92
 ---- batch: 030 ----
mean loss: 115.26
 ---- batch: 040 ----
mean loss: 125.91
 ---- batch: 050 ----
mean loss: 125.83
 ---- batch: 060 ----
mean loss: 128.53
 ---- batch: 070 ----
mean loss: 120.24
 ---- batch: 080 ----
mean loss: 121.77
 ---- batch: 090 ----
mean loss: 119.86
 ---- batch: 100 ----
mean loss: 125.94
 ---- batch: 110 ----
mean loss: 131.77
train mean loss: 124.20
epoch train time: 0:00:01.963746
elapsed time: 0:07:32.181662
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 23:11:14.187924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.82
 ---- batch: 020 ----
mean loss: 117.98
 ---- batch: 030 ----
mean loss: 121.50
 ---- batch: 040 ----
mean loss: 120.70
 ---- batch: 050 ----
mean loss: 125.76
 ---- batch: 060 ----
mean loss: 121.48
 ---- batch: 070 ----
mean loss: 129.60
 ---- batch: 080 ----
mean loss: 122.77
 ---- batch: 090 ----
mean loss: 129.28
 ---- batch: 100 ----
mean loss: 127.31
 ---- batch: 110 ----
mean loss: 127.36
train mean loss: 124.24
epoch train time: 0:00:01.937917
elapsed time: 0:07:34.120167
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 23:11:16.126464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.95
 ---- batch: 020 ----
mean loss: 122.11
 ---- batch: 030 ----
mean loss: 126.19
 ---- batch: 040 ----
mean loss: 130.12
 ---- batch: 050 ----
mean loss: 116.47
 ---- batch: 060 ----
mean loss: 118.54
 ---- batch: 070 ----
mean loss: 125.85
 ---- batch: 080 ----
mean loss: 125.30
 ---- batch: 090 ----
mean loss: 116.34
 ---- batch: 100 ----
mean loss: 123.86
 ---- batch: 110 ----
mean loss: 128.36
train mean loss: 124.11
epoch train time: 0:00:01.942150
elapsed time: 0:07:36.062988
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 23:11:18.069271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.49
 ---- batch: 020 ----
mean loss: 128.34
 ---- batch: 030 ----
mean loss: 120.30
 ---- batch: 040 ----
mean loss: 124.36
 ---- batch: 050 ----
mean loss: 122.64
 ---- batch: 060 ----
mean loss: 121.68
 ---- batch: 070 ----
mean loss: 122.59
 ---- batch: 080 ----
mean loss: 126.64
 ---- batch: 090 ----
mean loss: 124.29
 ---- batch: 100 ----
mean loss: 123.36
 ---- batch: 110 ----
mean loss: 122.25
train mean loss: 124.03
epoch train time: 0:00:01.979158
elapsed time: 0:07:38.042789
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 23:11:20.049037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.91
 ---- batch: 020 ----
mean loss: 120.73
 ---- batch: 030 ----
mean loss: 126.17
 ---- batch: 040 ----
mean loss: 123.15
 ---- batch: 050 ----
mean loss: 125.52
 ---- batch: 060 ----
mean loss: 125.39
 ---- batch: 070 ----
mean loss: 121.22
 ---- batch: 080 ----
mean loss: 126.12
 ---- batch: 090 ----
mean loss: 126.16
 ---- batch: 100 ----
mean loss: 127.06
 ---- batch: 110 ----
mean loss: 122.66
train mean loss: 123.97
epoch train time: 0:00:02.005968
elapsed time: 0:07:40.049405
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 23:11:22.055683
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.80
 ---- batch: 020 ----
mean loss: 124.62
 ---- batch: 030 ----
mean loss: 126.41
 ---- batch: 040 ----
mean loss: 122.48
 ---- batch: 050 ----
mean loss: 123.04
 ---- batch: 060 ----
mean loss: 124.56
 ---- batch: 070 ----
mean loss: 124.39
 ---- batch: 080 ----
mean loss: 122.01
 ---- batch: 090 ----
mean loss: 127.59
 ---- batch: 100 ----
mean loss: 126.31
 ---- batch: 110 ----
mean loss: 121.32
train mean loss: 124.08
epoch train time: 0:00:01.986906
elapsed time: 0:07:42.036927
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 23:11:24.043259
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.00
 ---- batch: 020 ----
mean loss: 125.31
 ---- batch: 030 ----
mean loss: 115.58
 ---- batch: 040 ----
mean loss: 121.63
 ---- batch: 050 ----
mean loss: 126.93
 ---- batch: 060 ----
mean loss: 117.30
 ---- batch: 070 ----
mean loss: 126.07
 ---- batch: 080 ----
mean loss: 131.56
 ---- batch: 090 ----
mean loss: 125.06
 ---- batch: 100 ----
mean loss: 125.71
 ---- batch: 110 ----
mean loss: 128.44
train mean loss: 123.99
epoch train time: 0:00:01.966687
elapsed time: 0:07:44.004272
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 23:11:26.010589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.07
 ---- batch: 020 ----
mean loss: 126.42
 ---- batch: 030 ----
mean loss: 132.21
 ---- batch: 040 ----
mean loss: 121.72
 ---- batch: 050 ----
mean loss: 126.49
 ---- batch: 060 ----
mean loss: 115.24
 ---- batch: 070 ----
mean loss: 122.40
 ---- batch: 080 ----
mean loss: 121.64
 ---- batch: 090 ----
mean loss: 124.49
 ---- batch: 100 ----
mean loss: 126.75
 ---- batch: 110 ----
mean loss: 125.87
train mean loss: 123.81
epoch train time: 0:00:01.953901
elapsed time: 0:07:45.958834
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 23:11:27.965086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.54
 ---- batch: 020 ----
mean loss: 119.06
 ---- batch: 030 ----
mean loss: 128.04
 ---- batch: 040 ----
mean loss: 124.21
 ---- batch: 050 ----
mean loss: 116.89
 ---- batch: 060 ----
mean loss: 122.61
 ---- batch: 070 ----
mean loss: 127.52
 ---- batch: 080 ----
mean loss: 131.27
 ---- batch: 090 ----
mean loss: 124.37
 ---- batch: 100 ----
mean loss: 123.64
 ---- batch: 110 ----
mean loss: 124.55
train mean loss: 123.84
epoch train time: 0:00:01.967872
elapsed time: 0:07:47.927283
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 23:11:29.933649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.08
 ---- batch: 020 ----
mean loss: 128.34
 ---- batch: 030 ----
mean loss: 128.93
 ---- batch: 040 ----
mean loss: 115.18
 ---- batch: 050 ----
mean loss: 120.86
 ---- batch: 060 ----
mean loss: 128.23
 ---- batch: 070 ----
mean loss: 126.02
 ---- batch: 080 ----
mean loss: 118.58
 ---- batch: 090 ----
mean loss: 121.47
 ---- batch: 100 ----
mean loss: 121.47
 ---- batch: 110 ----
mean loss: 119.92
train mean loss: 123.82
epoch train time: 0:00:01.942262
elapsed time: 0:07:49.870239
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 23:11:31.876504
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.18
 ---- batch: 020 ----
mean loss: 122.19
 ---- batch: 030 ----
mean loss: 123.31
 ---- batch: 040 ----
mean loss: 120.61
 ---- batch: 050 ----
mean loss: 130.04
 ---- batch: 060 ----
mean loss: 126.85
 ---- batch: 070 ----
mean loss: 121.85
 ---- batch: 080 ----
mean loss: 124.34
 ---- batch: 090 ----
mean loss: 128.14
 ---- batch: 100 ----
mean loss: 118.74
 ---- batch: 110 ----
mean loss: 118.61
train mean loss: 124.02
epoch train time: 0:00:01.903167
elapsed time: 0:07:51.774001
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 23:11:33.780250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.99
 ---- batch: 020 ----
mean loss: 127.24
 ---- batch: 030 ----
mean loss: 122.64
 ---- batch: 040 ----
mean loss: 120.41
 ---- batch: 050 ----
mean loss: 122.29
 ---- batch: 060 ----
mean loss: 127.31
 ---- batch: 070 ----
mean loss: 124.91
 ---- batch: 080 ----
mean loss: 130.76
 ---- batch: 090 ----
mean loss: 128.76
 ---- batch: 100 ----
mean loss: 111.99
 ---- batch: 110 ----
mean loss: 123.57
train mean loss: 123.86
epoch train time: 0:00:01.940018
elapsed time: 0:07:53.714655
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 23:11:35.720896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.67
 ---- batch: 020 ----
mean loss: 119.41
 ---- batch: 030 ----
mean loss: 127.39
 ---- batch: 040 ----
mean loss: 127.91
 ---- batch: 050 ----
mean loss: 117.37
 ---- batch: 060 ----
mean loss: 125.41
 ---- batch: 070 ----
mean loss: 119.49
 ---- batch: 080 ----
mean loss: 114.60
 ---- batch: 090 ----
mean loss: 128.77
 ---- batch: 100 ----
mean loss: 126.60
 ---- batch: 110 ----
mean loss: 122.89
train mean loss: 123.74
epoch train time: 0:00:01.972164
elapsed time: 0:07:55.687385
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 23:11:37.693658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.75
 ---- batch: 020 ----
mean loss: 132.69
 ---- batch: 030 ----
mean loss: 124.24
 ---- batch: 040 ----
mean loss: 123.62
 ---- batch: 050 ----
mean loss: 120.26
 ---- batch: 060 ----
mean loss: 116.62
 ---- batch: 070 ----
mean loss: 133.17
 ---- batch: 080 ----
mean loss: 119.82
 ---- batch: 090 ----
mean loss: 125.17
 ---- batch: 100 ----
mean loss: 121.50
 ---- batch: 110 ----
mean loss: 125.04
train mean loss: 123.66
epoch train time: 0:00:01.946171
elapsed time: 0:07:57.634155
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 23:11:39.640416
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.77
 ---- batch: 020 ----
mean loss: 124.94
 ---- batch: 030 ----
mean loss: 122.90
 ---- batch: 040 ----
mean loss: 119.73
 ---- batch: 050 ----
mean loss: 117.98
 ---- batch: 060 ----
mean loss: 128.25
 ---- batch: 070 ----
mean loss: 131.37
 ---- batch: 080 ----
mean loss: 131.20
 ---- batch: 090 ----
mean loss: 119.71
 ---- batch: 100 ----
mean loss: 123.12
 ---- batch: 110 ----
mean loss: 123.81
train mean loss: 123.65
epoch train time: 0:00:01.948752
elapsed time: 0:07:59.583567
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 23:11:41.589900
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.01
 ---- batch: 020 ----
mean loss: 118.03
 ---- batch: 030 ----
mean loss: 122.25
 ---- batch: 040 ----
mean loss: 129.29
 ---- batch: 050 ----
mean loss: 120.83
 ---- batch: 060 ----
mean loss: 121.08
 ---- batch: 070 ----
mean loss: 128.89
 ---- batch: 080 ----
mean loss: 126.33
 ---- batch: 090 ----
mean loss: 121.90
 ---- batch: 100 ----
mean loss: 122.14
 ---- batch: 110 ----
mean loss: 123.58
train mean loss: 123.55
epoch train time: 0:00:01.954508
elapsed time: 0:08:01.538789
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 23:11:43.545063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.33
 ---- batch: 020 ----
mean loss: 127.88
 ---- batch: 030 ----
mean loss: 132.37
 ---- batch: 040 ----
mean loss: 127.40
 ---- batch: 050 ----
mean loss: 125.58
 ---- batch: 060 ----
mean loss: 127.48
 ---- batch: 070 ----
mean loss: 119.53
 ---- batch: 080 ----
mean loss: 115.79
 ---- batch: 090 ----
mean loss: 125.39
 ---- batch: 100 ----
mean loss: 124.08
 ---- batch: 110 ----
mean loss: 120.42
train mean loss: 123.58
epoch train time: 0:00:01.918938
elapsed time: 0:08:03.458325
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 23:11:45.464557
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.86
 ---- batch: 020 ----
mean loss: 120.15
 ---- batch: 030 ----
mean loss: 117.71
 ---- batch: 040 ----
mean loss: 132.56
 ---- batch: 050 ----
mean loss: 121.12
 ---- batch: 060 ----
mean loss: 122.21
 ---- batch: 070 ----
mean loss: 121.31
 ---- batch: 080 ----
mean loss: 126.34
 ---- batch: 090 ----
mean loss: 120.42
 ---- batch: 100 ----
mean loss: 122.18
 ---- batch: 110 ----
mean loss: 131.58
train mean loss: 123.66
epoch train time: 0:00:01.923475
elapsed time: 0:08:05.382341
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 23:11:47.388585
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.14
 ---- batch: 020 ----
mean loss: 118.31
 ---- batch: 030 ----
mean loss: 120.11
 ---- batch: 040 ----
mean loss: 131.78
 ---- batch: 050 ----
mean loss: 124.01
 ---- batch: 060 ----
mean loss: 113.25
 ---- batch: 070 ----
mean loss: 126.33
 ---- batch: 080 ----
mean loss: 130.15
 ---- batch: 090 ----
mean loss: 123.29
 ---- batch: 100 ----
mean loss: 132.15
 ---- batch: 110 ----
mean loss: 127.04
train mean loss: 123.64
epoch train time: 0:00:01.916075
elapsed time: 0:08:07.298960
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 23:11:49.305201
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.00
 ---- batch: 020 ----
mean loss: 123.16
 ---- batch: 030 ----
mean loss: 124.39
 ---- batch: 040 ----
mean loss: 131.45
 ---- batch: 050 ----
mean loss: 118.46
 ---- batch: 060 ----
mean loss: 125.71
 ---- batch: 070 ----
mean loss: 128.03
 ---- batch: 080 ----
mean loss: 123.93
 ---- batch: 090 ----
mean loss: 126.15
 ---- batch: 100 ----
mean loss: 114.81
 ---- batch: 110 ----
mean loss: 121.58
train mean loss: 123.51
epoch train time: 0:00:01.964393
elapsed time: 0:08:09.263914
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 23:11:51.270169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.43
 ---- batch: 020 ----
mean loss: 118.41
 ---- batch: 030 ----
mean loss: 122.09
 ---- batch: 040 ----
mean loss: 127.09
 ---- batch: 050 ----
mean loss: 116.80
 ---- batch: 060 ----
mean loss: 128.80
 ---- batch: 070 ----
mean loss: 125.20
 ---- batch: 080 ----
mean loss: 128.84
 ---- batch: 090 ----
mean loss: 120.34
 ---- batch: 100 ----
mean loss: 125.16
 ---- batch: 110 ----
mean loss: 125.15
train mean loss: 123.39
epoch train time: 0:00:01.941551
elapsed time: 0:08:11.206307
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 23:11:53.212333
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.25
 ---- batch: 020 ----
mean loss: 120.56
 ---- batch: 030 ----
mean loss: 117.19
 ---- batch: 040 ----
mean loss: 125.39
 ---- batch: 050 ----
mean loss: 122.00
 ---- batch: 060 ----
mean loss: 125.91
 ---- batch: 070 ----
mean loss: 123.51
 ---- batch: 080 ----
mean loss: 124.94
 ---- batch: 090 ----
mean loss: 125.90
 ---- batch: 100 ----
mean loss: 128.40
 ---- batch: 110 ----
mean loss: 122.41
train mean loss: 123.41
epoch train time: 0:00:01.901049
elapsed time: 0:08:13.107700
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 23:11:55.113972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.41
 ---- batch: 020 ----
mean loss: 119.00
 ---- batch: 030 ----
mean loss: 120.27
 ---- batch: 040 ----
mean loss: 131.58
 ---- batch: 050 ----
mean loss: 118.45
 ---- batch: 060 ----
mean loss: 127.54
 ---- batch: 070 ----
mean loss: 128.83
 ---- batch: 080 ----
mean loss: 118.87
 ---- batch: 090 ----
mean loss: 126.26
 ---- batch: 100 ----
mean loss: 124.52
 ---- batch: 110 ----
mean loss: 118.44
train mean loss: 123.48
epoch train time: 0:00:01.931365
elapsed time: 0:08:15.039664
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 23:11:57.045933
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.28
 ---- batch: 020 ----
mean loss: 124.90
 ---- batch: 030 ----
mean loss: 124.30
 ---- batch: 040 ----
mean loss: 127.63
 ---- batch: 050 ----
mean loss: 122.32
 ---- batch: 060 ----
mean loss: 131.09
 ---- batch: 070 ----
mean loss: 119.75
 ---- batch: 080 ----
mean loss: 123.78
 ---- batch: 090 ----
mean loss: 118.03
 ---- batch: 100 ----
mean loss: 124.27
 ---- batch: 110 ----
mean loss: 118.38
train mean loss: 123.35
epoch train time: 0:00:01.949840
elapsed time: 0:08:16.990144
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 23:11:58.996412
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.81
 ---- batch: 020 ----
mean loss: 129.98
 ---- batch: 030 ----
mean loss: 123.63
 ---- batch: 040 ----
mean loss: 127.21
 ---- batch: 050 ----
mean loss: 120.40
 ---- batch: 060 ----
mean loss: 123.10
 ---- batch: 070 ----
mean loss: 117.65
 ---- batch: 080 ----
mean loss: 122.20
 ---- batch: 090 ----
mean loss: 122.25
 ---- batch: 100 ----
mean loss: 119.45
 ---- batch: 110 ----
mean loss: 118.40
train mean loss: 123.20
epoch train time: 0:00:01.955980
elapsed time: 0:08:18.946716
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 23:12:00.953050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.48
 ---- batch: 020 ----
mean loss: 127.38
 ---- batch: 030 ----
mean loss: 120.84
 ---- batch: 040 ----
mean loss: 123.15
 ---- batch: 050 ----
mean loss: 124.52
 ---- batch: 060 ----
mean loss: 124.88
 ---- batch: 070 ----
mean loss: 129.69
 ---- batch: 080 ----
mean loss: 117.55
 ---- batch: 090 ----
mean loss: 115.43
 ---- batch: 100 ----
mean loss: 124.83
 ---- batch: 110 ----
mean loss: 122.67
train mean loss: 123.19
epoch train time: 0:00:01.936066
elapsed time: 0:08:20.883436
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 23:12:02.889711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.84
 ---- batch: 020 ----
mean loss: 124.59
 ---- batch: 030 ----
mean loss: 124.20
 ---- batch: 040 ----
mean loss: 122.30
 ---- batch: 050 ----
mean loss: 120.80
 ---- batch: 060 ----
mean loss: 133.99
 ---- batch: 070 ----
mean loss: 131.92
 ---- batch: 080 ----
mean loss: 128.52
 ---- batch: 090 ----
mean loss: 115.69
 ---- batch: 100 ----
mean loss: 121.84
 ---- batch: 110 ----
mean loss: 116.23
train mean loss: 123.40
epoch train time: 0:00:01.951946
elapsed time: 0:08:22.835977
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 23:12:04.842210
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.48
 ---- batch: 020 ----
mean loss: 122.97
 ---- batch: 030 ----
mean loss: 126.31
 ---- batch: 040 ----
mean loss: 120.71
 ---- batch: 050 ----
mean loss: 120.45
 ---- batch: 060 ----
mean loss: 120.35
 ---- batch: 070 ----
mean loss: 122.16
 ---- batch: 080 ----
mean loss: 121.74
 ---- batch: 090 ----
mean loss: 138.60
 ---- batch: 100 ----
mean loss: 115.03
 ---- batch: 110 ----
mean loss: 130.48
train mean loss: 123.15
epoch train time: 0:00:01.925090
elapsed time: 0:08:24.761640
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 23:12:06.767968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.51
 ---- batch: 020 ----
mean loss: 115.49
 ---- batch: 030 ----
mean loss: 119.99
 ---- batch: 040 ----
mean loss: 118.59
 ---- batch: 050 ----
mean loss: 131.71
 ---- batch: 060 ----
mean loss: 128.17
 ---- batch: 070 ----
mean loss: 125.72
 ---- batch: 080 ----
mean loss: 122.18
 ---- batch: 090 ----
mean loss: 126.42
 ---- batch: 100 ----
mean loss: 128.81
 ---- batch: 110 ----
mean loss: 116.71
train mean loss: 123.18
epoch train time: 0:00:01.956154
elapsed time: 0:08:26.718499
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 23:12:08.724811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.60
 ---- batch: 020 ----
mean loss: 128.08
 ---- batch: 030 ----
mean loss: 128.86
 ---- batch: 040 ----
mean loss: 115.44
 ---- batch: 050 ----
mean loss: 123.13
 ---- batch: 060 ----
mean loss: 124.37
 ---- batch: 070 ----
mean loss: 125.66
 ---- batch: 080 ----
mean loss: 110.68
 ---- batch: 090 ----
mean loss: 119.42
 ---- batch: 100 ----
mean loss: 129.10
 ---- batch: 110 ----
mean loss: 126.77
train mean loss: 123.17
epoch train time: 0:00:01.937586
elapsed time: 0:08:28.656740
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 23:12:10.663002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.25
 ---- batch: 020 ----
mean loss: 127.54
 ---- batch: 030 ----
mean loss: 119.60
 ---- batch: 040 ----
mean loss: 130.10
 ---- batch: 050 ----
mean loss: 125.18
 ---- batch: 060 ----
mean loss: 120.65
 ---- batch: 070 ----
mean loss: 128.20
 ---- batch: 080 ----
mean loss: 119.42
 ---- batch: 090 ----
mean loss: 119.34
 ---- batch: 100 ----
mean loss: 126.70
 ---- batch: 110 ----
mean loss: 120.96
train mean loss: 123.19
epoch train time: 0:00:01.972172
elapsed time: 0:08:30.629512
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 23:12:12.635758
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.82
 ---- batch: 020 ----
mean loss: 114.88
 ---- batch: 030 ----
mean loss: 119.69
 ---- batch: 040 ----
mean loss: 120.84
 ---- batch: 050 ----
mean loss: 126.69
 ---- batch: 060 ----
mean loss: 120.33
 ---- batch: 070 ----
mean loss: 132.60
 ---- batch: 080 ----
mean loss: 124.11
 ---- batch: 090 ----
mean loss: 119.86
 ---- batch: 100 ----
mean loss: 131.76
 ---- batch: 110 ----
mean loss: 123.60
train mean loss: 123.08
epoch train time: 0:00:01.942309
elapsed time: 0:08:32.572379
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 23:12:14.578639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.74
 ---- batch: 020 ----
mean loss: 124.25
 ---- batch: 030 ----
mean loss: 124.61
 ---- batch: 040 ----
mean loss: 124.40
 ---- batch: 050 ----
mean loss: 118.16
 ---- batch: 060 ----
mean loss: 126.44
 ---- batch: 070 ----
mean loss: 121.90
 ---- batch: 080 ----
mean loss: 118.49
 ---- batch: 090 ----
mean loss: 123.20
 ---- batch: 100 ----
mean loss: 125.30
 ---- batch: 110 ----
mean loss: 127.09
train mean loss: 123.07
epoch train time: 0:00:01.934722
elapsed time: 0:08:34.507688
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 23:12:16.513955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.34
 ---- batch: 020 ----
mean loss: 123.35
 ---- batch: 030 ----
mean loss: 121.12
 ---- batch: 040 ----
mean loss: 121.36
 ---- batch: 050 ----
mean loss: 121.00
 ---- batch: 060 ----
mean loss: 122.60
 ---- batch: 070 ----
mean loss: 130.65
 ---- batch: 080 ----
mean loss: 121.66
 ---- batch: 090 ----
mean loss: 122.40
 ---- batch: 100 ----
mean loss: 125.50
 ---- batch: 110 ----
mean loss: 117.69
train mean loss: 123.06
epoch train time: 0:00:01.941476
elapsed time: 0:08:36.449796
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 23:12:18.456015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.58
 ---- batch: 020 ----
mean loss: 124.75
 ---- batch: 030 ----
mean loss: 122.28
 ---- batch: 040 ----
mean loss: 119.60
 ---- batch: 050 ----
mean loss: 122.73
 ---- batch: 060 ----
mean loss: 126.27
 ---- batch: 070 ----
mean loss: 125.98
 ---- batch: 080 ----
mean loss: 118.66
 ---- batch: 090 ----
mean loss: 119.95
 ---- batch: 100 ----
mean loss: 125.47
 ---- batch: 110 ----
mean loss: 131.97
train mean loss: 122.89
epoch train time: 0:00:01.928199
elapsed time: 0:08:38.378521
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 23:12:20.384869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.80
 ---- batch: 020 ----
mean loss: 119.71
 ---- batch: 030 ----
mean loss: 119.98
 ---- batch: 040 ----
mean loss: 120.45
 ---- batch: 050 ----
mean loss: 119.27
 ---- batch: 060 ----
mean loss: 126.57
 ---- batch: 070 ----
mean loss: 120.80
 ---- batch: 080 ----
mean loss: 123.25
 ---- batch: 090 ----
mean loss: 124.69
 ---- batch: 100 ----
mean loss: 128.02
 ---- batch: 110 ----
mean loss: 124.34
train mean loss: 122.98
epoch train time: 0:00:01.936594
elapsed time: 0:08:40.315782
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 23:12:22.322020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.06
 ---- batch: 020 ----
mean loss: 122.90
 ---- batch: 030 ----
mean loss: 119.00
 ---- batch: 040 ----
mean loss: 121.10
 ---- batch: 050 ----
mean loss: 120.83
 ---- batch: 060 ----
mean loss: 126.91
 ---- batch: 070 ----
mean loss: 121.23
 ---- batch: 080 ----
mean loss: 125.47
 ---- batch: 090 ----
mean loss: 125.20
 ---- batch: 100 ----
mean loss: 128.00
 ---- batch: 110 ----
mean loss: 122.62
train mean loss: 122.89
epoch train time: 0:00:01.981198
elapsed time: 0:08:42.305226
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_5/checkpoint.pth.tar
**** end time: 2019-09-26 23:12:24.311124 ****
