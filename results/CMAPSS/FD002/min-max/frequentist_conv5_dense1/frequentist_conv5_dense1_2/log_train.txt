Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 21554
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-25 21:01:14.105982 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 21, 24]             100
              Tanh-2           [-1, 10, 21, 24]               0
            Conv2d-3           [-1, 10, 20, 24]           1,000
              Tanh-4           [-1, 10, 20, 24]               0
            Conv2d-5           [-1, 10, 21, 24]           1,000
              Tanh-6           [-1, 10, 21, 24]               0
            Conv2d-7           [-1, 10, 20, 24]           1,000
              Tanh-8           [-1, 10, 20, 24]               0
            Conv2d-9            [-1, 1, 20, 24]              30
             Tanh-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
          Dropout-12                  [-1, 480]               0
           Linear-13                  [-1, 100]          48,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 51,230
Trainable params: 51,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 21:01:14.114592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3956.87
 ---- batch: 020 ----
mean loss: 1805.96
 ---- batch: 030 ----
mean loss: 1238.65
 ---- batch: 040 ----
mean loss: 1076.14
 ---- batch: 050 ----
mean loss: 1015.69
 ---- batch: 060 ----
mean loss: 970.58
 ---- batch: 070 ----
mean loss: 936.18
 ---- batch: 080 ----
mean loss: 914.28
 ---- batch: 090 ----
mean loss: 904.91
train mean loss: 1390.30
epoch train time: 0:00:34.994146
elapsed time: 0:00:35.005582
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 21:01:49.111607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.91
 ---- batch: 020 ----
mean loss: 836.21
 ---- batch: 030 ----
mean loss: 823.92
 ---- batch: 040 ----
mean loss: 814.72
 ---- batch: 050 ----
mean loss: 796.98
 ---- batch: 060 ----
mean loss: 785.40
 ---- batch: 070 ----
mean loss: 769.72
 ---- batch: 080 ----
mean loss: 771.62
 ---- batch: 090 ----
mean loss: 779.74
train mean loss: 798.92
epoch train time: 0:00:02.423147
elapsed time: 0:00:37.428891
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 21:01:51.534935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.92
 ---- batch: 020 ----
mean loss: 708.71
 ---- batch: 030 ----
mean loss: 706.52
 ---- batch: 040 ----
mean loss: 711.33
 ---- batch: 050 ----
mean loss: 704.05
 ---- batch: 060 ----
mean loss: 667.05
 ---- batch: 070 ----
mean loss: 669.92
 ---- batch: 080 ----
mean loss: 658.19
 ---- batch: 090 ----
mean loss: 659.39
train mean loss: 687.04
epoch train time: 0:00:02.352093
elapsed time: 0:00:39.781173
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 21:01:53.887221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 655.11
 ---- batch: 020 ----
mean loss: 633.48
 ---- batch: 030 ----
mean loss: 625.89
 ---- batch: 040 ----
mean loss: 610.91
 ---- batch: 050 ----
mean loss: 618.26
 ---- batch: 060 ----
mean loss: 600.57
 ---- batch: 070 ----
mean loss: 593.40
 ---- batch: 080 ----
mean loss: 590.67
 ---- batch: 090 ----
mean loss: 588.29
train mean loss: 610.66
epoch train time: 0:00:02.357208
elapsed time: 0:00:42.138567
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 21:01:56.244630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 588.69
 ---- batch: 020 ----
mean loss: 584.84
 ---- batch: 030 ----
mean loss: 587.42
 ---- batch: 040 ----
mean loss: 576.68
 ---- batch: 050 ----
mean loss: 565.74
 ---- batch: 060 ----
mean loss: 565.96
 ---- batch: 070 ----
mean loss: 556.67
 ---- batch: 080 ----
mean loss: 565.12
 ---- batch: 090 ----
mean loss: 542.51
train mean loss: 568.95
epoch train time: 0:00:02.359066
elapsed time: 0:00:44.497862
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 21:01:58.603907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 548.44
 ---- batch: 020 ----
mean loss: 544.09
 ---- batch: 030 ----
mean loss: 524.18
 ---- batch: 040 ----
mean loss: 521.75
 ---- batch: 050 ----
mean loss: 513.35
 ---- batch: 060 ----
mean loss: 518.62
 ---- batch: 070 ----
mean loss: 526.00
 ---- batch: 080 ----
mean loss: 506.71
 ---- batch: 090 ----
mean loss: 489.36
train mean loss: 519.91
epoch train time: 0:00:02.353019
elapsed time: 0:00:46.851146
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 21:02:00.957184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.49
 ---- batch: 020 ----
mean loss: 484.21
 ---- batch: 030 ----
mean loss: 488.77
 ---- batch: 040 ----
mean loss: 483.26
 ---- batch: 050 ----
mean loss: 460.24
 ---- batch: 060 ----
mean loss: 490.90
 ---- batch: 070 ----
mean loss: 464.36
 ---- batch: 080 ----
mean loss: 465.22
 ---- batch: 090 ----
mean loss: 477.14
train mean loss: 476.97
epoch train time: 0:00:02.366519
elapsed time: 0:00:49.217835
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 21:02:03.323896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.64
 ---- batch: 020 ----
mean loss: 453.97
 ---- batch: 030 ----
mean loss: 445.97
 ---- batch: 040 ----
mean loss: 458.92
 ---- batch: 050 ----
mean loss: 458.93
 ---- batch: 060 ----
mean loss: 448.04
 ---- batch: 070 ----
mean loss: 453.56
 ---- batch: 080 ----
mean loss: 443.56
 ---- batch: 090 ----
mean loss: 451.25
train mean loss: 450.86
epoch train time: 0:00:02.360099
elapsed time: 0:00:51.578145
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 21:02:05.684203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.48
 ---- batch: 020 ----
mean loss: 449.21
 ---- batch: 030 ----
mean loss: 445.26
 ---- batch: 040 ----
mean loss: 439.44
 ---- batch: 050 ----
mean loss: 425.00
 ---- batch: 060 ----
mean loss: 442.29
 ---- batch: 070 ----
mean loss: 435.67
 ---- batch: 080 ----
mean loss: 452.25
 ---- batch: 090 ----
mean loss: 436.26
train mean loss: 440.82
epoch train time: 0:00:02.356437
elapsed time: 0:00:53.934781
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 21:02:08.040824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.04
 ---- batch: 020 ----
mean loss: 464.11
 ---- batch: 030 ----
mean loss: 421.37
 ---- batch: 040 ----
mean loss: 424.80
 ---- batch: 050 ----
mean loss: 448.03
 ---- batch: 060 ----
mean loss: 441.35
 ---- batch: 070 ----
mean loss: 439.80
 ---- batch: 080 ----
mean loss: 441.79
 ---- batch: 090 ----
mean loss: 428.92
train mean loss: 440.00
epoch train time: 0:00:02.360165
elapsed time: 0:00:56.295133
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 21:02:10.401175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.69
 ---- batch: 020 ----
mean loss: 447.09
 ---- batch: 030 ----
mean loss: 420.36
 ---- batch: 040 ----
mean loss: 430.22
 ---- batch: 050 ----
mean loss: 439.84
 ---- batch: 060 ----
mean loss: 431.45
 ---- batch: 070 ----
mean loss: 431.41
 ---- batch: 080 ----
mean loss: 437.61
 ---- batch: 090 ----
mean loss: 420.23
train mean loss: 432.93
epoch train time: 0:00:02.346754
elapsed time: 0:00:58.642092
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 21:02:12.748130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.29
 ---- batch: 020 ----
mean loss: 448.37
 ---- batch: 030 ----
mean loss: 441.39
 ---- batch: 040 ----
mean loss: 432.83
 ---- batch: 050 ----
mean loss: 426.67
 ---- batch: 060 ----
mean loss: 421.01
 ---- batch: 070 ----
mean loss: 424.60
 ---- batch: 080 ----
mean loss: 425.56
 ---- batch: 090 ----
mean loss: 416.98
train mean loss: 427.65
epoch train time: 0:00:02.363000
elapsed time: 0:01:01.005272
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 21:02:15.111318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.07
 ---- batch: 020 ----
mean loss: 420.13
 ---- batch: 030 ----
mean loss: 422.80
 ---- batch: 040 ----
mean loss: 425.57
 ---- batch: 050 ----
mean loss: 441.16
 ---- batch: 060 ----
mean loss: 433.10
 ---- batch: 070 ----
mean loss: 441.79
 ---- batch: 080 ----
mean loss: 441.96
 ---- batch: 090 ----
mean loss: 426.49
train mean loss: 428.28
epoch train time: 0:00:02.349248
elapsed time: 0:01:03.354731
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 21:02:17.460772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.05
 ---- batch: 020 ----
mean loss: 443.01
 ---- batch: 030 ----
mean loss: 415.89
 ---- batch: 040 ----
mean loss: 415.20
 ---- batch: 050 ----
mean loss: 417.92
 ---- batch: 060 ----
mean loss: 425.74
 ---- batch: 070 ----
mean loss: 425.35
 ---- batch: 080 ----
mean loss: 433.98
 ---- batch: 090 ----
mean loss: 426.96
train mean loss: 427.01
epoch train time: 0:00:02.361086
elapsed time: 0:01:05.716013
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 21:02:19.822057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.79
 ---- batch: 020 ----
mean loss: 420.66
 ---- batch: 030 ----
mean loss: 409.26
 ---- batch: 040 ----
mean loss: 418.58
 ---- batch: 050 ----
mean loss: 420.79
 ---- batch: 060 ----
mean loss: 426.31
 ---- batch: 070 ----
mean loss: 409.06
 ---- batch: 080 ----
mean loss: 411.19
 ---- batch: 090 ----
mean loss: 417.63
train mean loss: 417.55
epoch train time: 0:00:02.350580
elapsed time: 0:01:08.066785
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 21:02:22.172854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.50
 ---- batch: 020 ----
mean loss: 419.18
 ---- batch: 030 ----
mean loss: 424.94
 ---- batch: 040 ----
mean loss: 409.93
 ---- batch: 050 ----
mean loss: 421.66
 ---- batch: 060 ----
mean loss: 404.28
 ---- batch: 070 ----
mean loss: 414.63
 ---- batch: 080 ----
mean loss: 401.28
 ---- batch: 090 ----
mean loss: 422.30
train mean loss: 415.36
epoch train time: 0:00:02.358115
elapsed time: 0:01:10.425099
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 21:02:24.531142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.60
 ---- batch: 020 ----
mean loss: 419.63
 ---- batch: 030 ----
mean loss: 413.94
 ---- batch: 040 ----
mean loss: 406.81
 ---- batch: 050 ----
mean loss: 401.46
 ---- batch: 060 ----
mean loss: 409.87
 ---- batch: 070 ----
mean loss: 405.18
 ---- batch: 080 ----
mean loss: 429.27
 ---- batch: 090 ----
mean loss: 410.52
train mean loss: 415.20
epoch train time: 0:00:02.349035
elapsed time: 0:01:12.774336
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 21:02:26.880374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.87
 ---- batch: 020 ----
mean loss: 409.14
 ---- batch: 030 ----
mean loss: 413.39
 ---- batch: 040 ----
mean loss: 409.76
 ---- batch: 050 ----
mean loss: 411.48
 ---- batch: 060 ----
mean loss: 415.84
 ---- batch: 070 ----
mean loss: 420.94
 ---- batch: 080 ----
mean loss: 424.23
 ---- batch: 090 ----
mean loss: 407.84
train mean loss: 414.74
epoch train time: 0:00:02.340952
elapsed time: 0:01:15.115461
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 21:02:29.221515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.97
 ---- batch: 020 ----
mean loss: 404.65
 ---- batch: 030 ----
mean loss: 408.55
 ---- batch: 040 ----
mean loss: 427.75
 ---- batch: 050 ----
mean loss: 410.73
 ---- batch: 060 ----
mean loss: 420.22
 ---- batch: 070 ----
mean loss: 403.05
 ---- batch: 080 ----
mean loss: 420.39
 ---- batch: 090 ----
mean loss: 423.67
train mean loss: 414.06
epoch train time: 0:00:02.341567
elapsed time: 0:01:17.457234
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 21:02:31.563265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.48
 ---- batch: 020 ----
mean loss: 402.88
 ---- batch: 030 ----
mean loss: 399.82
 ---- batch: 040 ----
mean loss: 416.44
 ---- batch: 050 ----
mean loss: 412.38
 ---- batch: 060 ----
mean loss: 406.51
 ---- batch: 070 ----
mean loss: 395.76
 ---- batch: 080 ----
mean loss: 404.66
 ---- batch: 090 ----
mean loss: 426.62
train mean loss: 408.59
epoch train time: 0:00:02.337002
elapsed time: 0:01:19.794432
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 21:02:33.900495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.54
 ---- batch: 020 ----
mean loss: 405.87
 ---- batch: 030 ----
mean loss: 388.89
 ---- batch: 040 ----
mean loss: 404.86
 ---- batch: 050 ----
mean loss: 400.93
 ---- batch: 060 ----
mean loss: 422.20
 ---- batch: 070 ----
mean loss: 408.28
 ---- batch: 080 ----
mean loss: 391.62
 ---- batch: 090 ----
mean loss: 398.37
train mean loss: 404.19
epoch train time: 0:00:02.342080
elapsed time: 0:01:22.136712
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 21:02:36.242760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.67
 ---- batch: 020 ----
mean loss: 412.35
 ---- batch: 030 ----
mean loss: 422.93
 ---- batch: 040 ----
mean loss: 410.84
 ---- batch: 050 ----
mean loss: 409.76
 ---- batch: 060 ----
mean loss: 411.87
 ---- batch: 070 ----
mean loss: 412.29
 ---- batch: 080 ----
mean loss: 408.39
 ---- batch: 090 ----
mean loss: 426.21
train mean loss: 413.61
epoch train time: 0:00:02.345713
elapsed time: 0:01:24.482616
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 21:02:38.588658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.67
 ---- batch: 020 ----
mean loss: 402.07
 ---- batch: 030 ----
mean loss: 404.66
 ---- batch: 040 ----
mean loss: 423.97
 ---- batch: 050 ----
mean loss: 413.83
 ---- batch: 060 ----
mean loss: 406.81
 ---- batch: 070 ----
mean loss: 403.82
 ---- batch: 080 ----
mean loss: 407.25
 ---- batch: 090 ----
mean loss: 408.65
train mean loss: 408.52
epoch train time: 0:00:02.334210
elapsed time: 0:01:26.817013
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 21:02:40.923081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.23
 ---- batch: 020 ----
mean loss: 401.09
 ---- batch: 030 ----
mean loss: 413.39
 ---- batch: 040 ----
mean loss: 428.30
 ---- batch: 050 ----
mean loss: 442.74
 ---- batch: 060 ----
mean loss: 429.98
 ---- batch: 070 ----
mean loss: 409.19
 ---- batch: 080 ----
mean loss: 402.89
 ---- batch: 090 ----
mean loss: 405.87
train mean loss: 415.99
epoch train time: 0:00:02.337412
elapsed time: 0:01:29.154632
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 21:02:43.260675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.97
 ---- batch: 020 ----
mean loss: 399.57
 ---- batch: 030 ----
mean loss: 411.82
 ---- batch: 040 ----
mean loss: 413.40
 ---- batch: 050 ----
mean loss: 421.27
 ---- batch: 060 ----
mean loss: 418.73
 ---- batch: 070 ----
mean loss: 393.91
 ---- batch: 080 ----
mean loss: 394.09
 ---- batch: 090 ----
mean loss: 407.29
train mean loss: 407.32
epoch train time: 0:00:02.351540
elapsed time: 0:01:31.506348
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 21:02:45.612392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.58
 ---- batch: 020 ----
mean loss: 408.68
 ---- batch: 030 ----
mean loss: 408.79
 ---- batch: 040 ----
mean loss: 415.01
 ---- batch: 050 ----
mean loss: 407.47
 ---- batch: 060 ----
mean loss: 412.03
 ---- batch: 070 ----
mean loss: 398.10
 ---- batch: 080 ----
mean loss: 416.34
 ---- batch: 090 ----
mean loss: 397.73
train mean loss: 407.21
epoch train time: 0:00:02.345641
elapsed time: 0:01:33.852177
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 21:02:47.958219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.81
 ---- batch: 020 ----
mean loss: 426.88
 ---- batch: 030 ----
mean loss: 408.31
 ---- batch: 040 ----
mean loss: 410.67
 ---- batch: 050 ----
mean loss: 409.93
 ---- batch: 060 ----
mean loss: 424.69
 ---- batch: 070 ----
mean loss: 395.30
 ---- batch: 080 ----
mean loss: 400.93
 ---- batch: 090 ----
mean loss: 400.79
train mean loss: 408.91
epoch train time: 0:00:02.330823
elapsed time: 0:01:36.183191
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 21:02:50.289237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.60
 ---- batch: 020 ----
mean loss: 410.20
 ---- batch: 030 ----
mean loss: 408.52
 ---- batch: 040 ----
mean loss: 411.29
 ---- batch: 050 ----
mean loss: 399.81
 ---- batch: 060 ----
mean loss: 401.82
 ---- batch: 070 ----
mean loss: 386.93
 ---- batch: 080 ----
mean loss: 395.62
 ---- batch: 090 ----
mean loss: 388.07
train mean loss: 403.12
epoch train time: 0:00:02.344098
elapsed time: 0:01:38.527490
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 21:02:52.633518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.90
 ---- batch: 020 ----
mean loss: 388.13
 ---- batch: 030 ----
mean loss: 392.98
 ---- batch: 040 ----
mean loss: 414.39
 ---- batch: 050 ----
mean loss: 398.15
 ---- batch: 060 ----
mean loss: 393.19
 ---- batch: 070 ----
mean loss: 419.69
 ---- batch: 080 ----
mean loss: 401.59
 ---- batch: 090 ----
mean loss: 409.92
train mean loss: 401.59
epoch train time: 0:00:02.355345
elapsed time: 0:01:40.883037
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 21:02:54.989080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.95
 ---- batch: 020 ----
mean loss: 411.89
 ---- batch: 030 ----
mean loss: 402.79
 ---- batch: 040 ----
mean loss: 396.66
 ---- batch: 050 ----
mean loss: 408.34
 ---- batch: 060 ----
mean loss: 413.75
 ---- batch: 070 ----
mean loss: 432.88
 ---- batch: 080 ----
mean loss: 444.40
 ---- batch: 090 ----
mean loss: 410.14
train mean loss: 416.59
epoch train time: 0:00:02.337417
elapsed time: 0:01:43.220642
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 21:02:57.326702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.04
 ---- batch: 020 ----
mean loss: 404.56
 ---- batch: 030 ----
mean loss: 412.12
 ---- batch: 040 ----
mean loss: 426.40
 ---- batch: 050 ----
mean loss: 408.44
 ---- batch: 060 ----
mean loss: 386.42
 ---- batch: 070 ----
mean loss: 396.39
 ---- batch: 080 ----
mean loss: 416.66
 ---- batch: 090 ----
mean loss: 417.93
train mean loss: 409.82
epoch train time: 0:00:02.335966
elapsed time: 0:01:45.556854
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 21:02:59.662910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.95
 ---- batch: 020 ----
mean loss: 411.90
 ---- batch: 030 ----
mean loss: 407.21
 ---- batch: 040 ----
mean loss: 402.62
 ---- batch: 050 ----
mean loss: 403.79
 ---- batch: 060 ----
mean loss: 415.77
 ---- batch: 070 ----
mean loss: 398.39
 ---- batch: 080 ----
mean loss: 414.30
 ---- batch: 090 ----
mean loss: 421.36
train mean loss: 408.40
epoch train time: 0:00:02.330301
elapsed time: 0:01:47.887357
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 21:03:01.993428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.09
 ---- batch: 020 ----
mean loss: 398.04
 ---- batch: 030 ----
mean loss: 405.13
 ---- batch: 040 ----
mean loss: 399.63
 ---- batch: 050 ----
mean loss: 405.60
 ---- batch: 060 ----
mean loss: 398.18
 ---- batch: 070 ----
mean loss: 418.78
 ---- batch: 080 ----
mean loss: 410.30
 ---- batch: 090 ----
mean loss: 390.12
train mean loss: 405.32
epoch train time: 0:00:02.343296
elapsed time: 0:01:50.230865
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 21:03:04.336915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.09
 ---- batch: 020 ----
mean loss: 408.33
 ---- batch: 030 ----
mean loss: 412.52
 ---- batch: 040 ----
mean loss: 396.63
 ---- batch: 050 ----
mean loss: 390.06
 ---- batch: 060 ----
mean loss: 389.36
 ---- batch: 070 ----
mean loss: 398.75
 ---- batch: 080 ----
mean loss: 403.69
 ---- batch: 090 ----
mean loss: 420.63
train mean loss: 404.46
epoch train time: 0:00:02.345478
elapsed time: 0:01:52.576533
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 21:03:06.682592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.66
 ---- batch: 020 ----
mean loss: 409.20
 ---- batch: 030 ----
mean loss: 397.25
 ---- batch: 040 ----
mean loss: 403.09
 ---- batch: 050 ----
mean loss: 392.94
 ---- batch: 060 ----
mean loss: 414.16
 ---- batch: 070 ----
mean loss: 422.36
 ---- batch: 080 ----
mean loss: 401.25
 ---- batch: 090 ----
mean loss: 405.50
train mean loss: 406.65
epoch train time: 0:00:02.335419
elapsed time: 0:01:54.912157
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 21:03:09.018199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.10
 ---- batch: 020 ----
mean loss: 407.68
 ---- batch: 030 ----
mean loss: 406.47
 ---- batch: 040 ----
mean loss: 408.42
 ---- batch: 050 ----
mean loss: 398.08
 ---- batch: 060 ----
mean loss: 395.91
 ---- batch: 070 ----
mean loss: 394.18
 ---- batch: 080 ----
mean loss: 395.13
 ---- batch: 090 ----
mean loss: 391.22
train mean loss: 399.87
epoch train time: 0:00:02.345450
elapsed time: 0:01:57.257794
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 21:03:11.363841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.06
 ---- batch: 020 ----
mean loss: 398.67
 ---- batch: 030 ----
mean loss: 387.08
 ---- batch: 040 ----
mean loss: 400.63
 ---- batch: 050 ----
mean loss: 402.79
 ---- batch: 060 ----
mean loss: 401.66
 ---- batch: 070 ----
mean loss: 401.56
 ---- batch: 080 ----
mean loss: 390.45
 ---- batch: 090 ----
mean loss: 388.41
train mean loss: 397.91
epoch train time: 0:00:02.338691
elapsed time: 0:01:59.596737
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 21:03:13.702798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.61
 ---- batch: 020 ----
mean loss: 401.19
 ---- batch: 030 ----
mean loss: 410.25
 ---- batch: 040 ----
mean loss: 402.92
 ---- batch: 050 ----
mean loss: 394.25
 ---- batch: 060 ----
mean loss: 396.47
 ---- batch: 070 ----
mean loss: 423.54
 ---- batch: 080 ----
mean loss: 422.48
 ---- batch: 090 ----
mean loss: 401.29
train mean loss: 404.97
epoch train time: 0:00:02.335549
elapsed time: 0:02:01.932498
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 21:03:16.038561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.68
 ---- batch: 020 ----
mean loss: 396.47
 ---- batch: 030 ----
mean loss: 388.72
 ---- batch: 040 ----
mean loss: 394.18
 ---- batch: 050 ----
mean loss: 401.79
 ---- batch: 060 ----
mean loss: 399.54
 ---- batch: 070 ----
mean loss: 406.32
 ---- batch: 080 ----
mean loss: 391.40
 ---- batch: 090 ----
mean loss: 399.61
train mean loss: 398.93
epoch train time: 0:00:02.334548
elapsed time: 0:02:04.267240
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 21:03:18.373281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.98
 ---- batch: 020 ----
mean loss: 398.60
 ---- batch: 030 ----
mean loss: 421.68
 ---- batch: 040 ----
mean loss: 431.68
 ---- batch: 050 ----
mean loss: 398.85
 ---- batch: 060 ----
mean loss: 404.34
 ---- batch: 070 ----
mean loss: 404.68
 ---- batch: 080 ----
mean loss: 401.65
 ---- batch: 090 ----
mean loss: 416.13
train mean loss: 409.34
epoch train time: 0:00:02.331322
elapsed time: 0:02:06.598746
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 21:03:20.704791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.25
 ---- batch: 020 ----
mean loss: 394.37
 ---- batch: 030 ----
mean loss: 408.44
 ---- batch: 040 ----
mean loss: 399.74
 ---- batch: 050 ----
mean loss: 392.74
 ---- batch: 060 ----
mean loss: 398.79
 ---- batch: 070 ----
mean loss: 409.95
 ---- batch: 080 ----
mean loss: 404.97
 ---- batch: 090 ----
mean loss: 395.56
train mean loss: 400.55
epoch train time: 0:00:02.338838
elapsed time: 0:02:08.937771
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 21:03:23.043814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.39
 ---- batch: 020 ----
mean loss: 407.37
 ---- batch: 030 ----
mean loss: 400.86
 ---- batch: 040 ----
mean loss: 392.17
 ---- batch: 050 ----
mean loss: 408.08
 ---- batch: 060 ----
mean loss: 402.38
 ---- batch: 070 ----
mean loss: 407.14
 ---- batch: 080 ----
mean loss: 414.12
 ---- batch: 090 ----
mean loss: 390.81
train mean loss: 401.21
epoch train time: 0:00:02.334753
elapsed time: 0:02:11.272725
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 21:03:25.378771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.99
 ---- batch: 020 ----
mean loss: 444.84
 ---- batch: 030 ----
mean loss: 453.34
 ---- batch: 040 ----
mean loss: 398.40
 ---- batch: 050 ----
mean loss: 403.19
 ---- batch: 060 ----
mean loss: 411.45
 ---- batch: 070 ----
mean loss: 404.36
 ---- batch: 080 ----
mean loss: 395.37
 ---- batch: 090 ----
mean loss: 394.30
train mean loss: 411.89
epoch train time: 0:00:02.336279
elapsed time: 0:02:13.609203
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 21:03:27.715246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.33
 ---- batch: 020 ----
mean loss: 416.16
 ---- batch: 030 ----
mean loss: 400.59
 ---- batch: 040 ----
mean loss: 398.05
 ---- batch: 050 ----
mean loss: 413.96
 ---- batch: 060 ----
mean loss: 399.24
 ---- batch: 070 ----
mean loss: 392.44
 ---- batch: 080 ----
mean loss: 415.02
 ---- batch: 090 ----
mean loss: 403.74
train mean loss: 405.59
epoch train time: 0:00:02.323482
elapsed time: 0:02:15.932872
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 21:03:30.038916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.92
 ---- batch: 020 ----
mean loss: 415.89
 ---- batch: 030 ----
mean loss: 425.08
 ---- batch: 040 ----
mean loss: 382.10
 ---- batch: 050 ----
mean loss: 393.90
 ---- batch: 060 ----
mean loss: 390.01
 ---- batch: 070 ----
mean loss: 393.40
 ---- batch: 080 ----
mean loss: 402.47
 ---- batch: 090 ----
mean loss: 397.40
train mean loss: 401.77
epoch train time: 0:00:02.355096
elapsed time: 0:02:18.288158
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 21:03:32.394225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.78
 ---- batch: 020 ----
mean loss: 404.49
 ---- batch: 030 ----
mean loss: 393.60
 ---- batch: 040 ----
mean loss: 383.99
 ---- batch: 050 ----
mean loss: 389.77
 ---- batch: 060 ----
mean loss: 391.08
 ---- batch: 070 ----
mean loss: 415.73
 ---- batch: 080 ----
mean loss: 401.92
 ---- batch: 090 ----
mean loss: 398.46
train mean loss: 397.94
epoch train time: 0:00:02.330030
elapsed time: 0:02:20.618549
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 21:03:34.724596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.90
 ---- batch: 020 ----
mean loss: 393.45
 ---- batch: 030 ----
mean loss: 402.66
 ---- batch: 040 ----
mean loss: 393.16
 ---- batch: 050 ----
mean loss: 392.15
 ---- batch: 060 ----
mean loss: 406.77
 ---- batch: 070 ----
mean loss: 395.97
 ---- batch: 080 ----
mean loss: 390.85
 ---- batch: 090 ----
mean loss: 393.88
train mean loss: 397.34
epoch train time: 0:00:02.333175
elapsed time: 0:02:22.952000
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 21:03:37.058048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.44
 ---- batch: 020 ----
mean loss: 420.54
 ---- batch: 030 ----
mean loss: 411.22
 ---- batch: 040 ----
mean loss: 404.12
 ---- batch: 050 ----
mean loss: 408.24
 ---- batch: 060 ----
mean loss: 386.66
 ---- batch: 070 ----
mean loss: 412.21
 ---- batch: 080 ----
mean loss: 397.08
 ---- batch: 090 ----
mean loss: 393.20
train mean loss: 401.57
epoch train time: 0:00:02.329947
elapsed time: 0:02:25.282173
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 21:03:39.388230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.74
 ---- batch: 020 ----
mean loss: 392.05
 ---- batch: 030 ----
mean loss: 392.29
 ---- batch: 040 ----
mean loss: 396.09
 ---- batch: 050 ----
mean loss: 393.39
 ---- batch: 060 ----
mean loss: 396.35
 ---- batch: 070 ----
mean loss: 414.34
 ---- batch: 080 ----
mean loss: 394.35
 ---- batch: 090 ----
mean loss: 388.41
train mean loss: 396.10
epoch train time: 0:00:02.329883
elapsed time: 0:02:27.612251
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 21:03:41.718296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.32
 ---- batch: 020 ----
mean loss: 388.55
 ---- batch: 030 ----
mean loss: 399.61
 ---- batch: 040 ----
mean loss: 391.44
 ---- batch: 050 ----
mean loss: 385.69
 ---- batch: 060 ----
mean loss: 405.13
 ---- batch: 070 ----
mean loss: 392.21
 ---- batch: 080 ----
mean loss: 412.77
 ---- batch: 090 ----
mean loss: 400.06
train mean loss: 397.22
epoch train time: 0:00:02.346337
elapsed time: 0:02:29.958776
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 21:03:44.064819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.23
 ---- batch: 020 ----
mean loss: 389.32
 ---- batch: 030 ----
mean loss: 401.97
 ---- batch: 040 ----
mean loss: 406.92
 ---- batch: 050 ----
mean loss: 398.00
 ---- batch: 060 ----
mean loss: 415.31
 ---- batch: 070 ----
mean loss: 400.95
 ---- batch: 080 ----
mean loss: 395.14
 ---- batch: 090 ----
mean loss: 392.37
train mean loss: 397.83
epoch train time: 0:00:02.329487
elapsed time: 0:02:32.288448
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 21:03:46.394493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.60
 ---- batch: 020 ----
mean loss: 397.72
 ---- batch: 030 ----
mean loss: 397.10
 ---- batch: 040 ----
mean loss: 411.71
 ---- batch: 050 ----
mean loss: 387.03
 ---- batch: 060 ----
mean loss: 400.20
 ---- batch: 070 ----
mean loss: 392.11
 ---- batch: 080 ----
mean loss: 388.53
 ---- batch: 090 ----
mean loss: 401.22
train mean loss: 396.51
epoch train time: 0:00:02.351233
elapsed time: 0:02:34.639866
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 21:03:48.745915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.23
 ---- batch: 020 ----
mean loss: 398.14
 ---- batch: 030 ----
mean loss: 399.08
 ---- batch: 040 ----
mean loss: 395.09
 ---- batch: 050 ----
mean loss: 397.37
 ---- batch: 060 ----
mean loss: 412.74
 ---- batch: 070 ----
mean loss: 389.52
 ---- batch: 080 ----
mean loss: 412.21
 ---- batch: 090 ----
mean loss: 413.27
train mean loss: 400.47
epoch train time: 0:00:02.337321
elapsed time: 0:02:36.977380
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 21:03:51.083440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.49
 ---- batch: 020 ----
mean loss: 407.36
 ---- batch: 030 ----
mean loss: 404.99
 ---- batch: 040 ----
mean loss: 389.38
 ---- batch: 050 ----
mean loss: 385.21
 ---- batch: 060 ----
mean loss: 385.87
 ---- batch: 070 ----
mean loss: 388.28
 ---- batch: 080 ----
mean loss: 403.50
 ---- batch: 090 ----
mean loss: 388.48
train mean loss: 395.63
epoch train time: 0:00:02.332524
elapsed time: 0:02:39.310125
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 21:03:53.416169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.87
 ---- batch: 020 ----
mean loss: 403.97
 ---- batch: 030 ----
mean loss: 385.67
 ---- batch: 040 ----
mean loss: 382.09
 ---- batch: 050 ----
mean loss: 392.24
 ---- batch: 060 ----
mean loss: 403.93
 ---- batch: 070 ----
mean loss: 396.46
 ---- batch: 080 ----
mean loss: 387.74
 ---- batch: 090 ----
mean loss: 400.80
train mean loss: 396.20
epoch train time: 0:00:02.328992
elapsed time: 0:02:41.639293
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 21:03:55.745335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.17
 ---- batch: 020 ----
mean loss: 405.60
 ---- batch: 030 ----
mean loss: 396.96
 ---- batch: 040 ----
mean loss: 383.43
 ---- batch: 050 ----
mean loss: 391.14
 ---- batch: 060 ----
mean loss: 388.44
 ---- batch: 070 ----
mean loss: 400.33
 ---- batch: 080 ----
mean loss: 384.61
 ---- batch: 090 ----
mean loss: 396.47
train mean loss: 396.11
epoch train time: 0:00:02.332449
elapsed time: 0:02:43.971953
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 21:03:58.077998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.74
 ---- batch: 020 ----
mean loss: 396.18
 ---- batch: 030 ----
mean loss: 401.93
 ---- batch: 040 ----
mean loss: 406.55
 ---- batch: 050 ----
mean loss: 399.71
 ---- batch: 060 ----
mean loss: 400.97
 ---- batch: 070 ----
mean loss: 404.07
 ---- batch: 080 ----
mean loss: 414.06
 ---- batch: 090 ----
mean loss: 405.40
train mean loss: 401.13
epoch train time: 0:00:02.338708
elapsed time: 0:02:46.310857
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 21:04:00.416902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.04
 ---- batch: 020 ----
mean loss: 395.54
 ---- batch: 030 ----
mean loss: 398.55
 ---- batch: 040 ----
mean loss: 395.02
 ---- batch: 050 ----
mean loss: 395.78
 ---- batch: 060 ----
mean loss: 404.13
 ---- batch: 070 ----
mean loss: 383.48
 ---- batch: 080 ----
mean loss: 394.65
 ---- batch: 090 ----
mean loss: 393.31
train mean loss: 393.80
epoch train time: 0:00:02.331485
elapsed time: 0:02:48.642618
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 21:04:02.748708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.31
 ---- batch: 020 ----
mean loss: 411.46
 ---- batch: 030 ----
mean loss: 400.27
 ---- batch: 040 ----
mean loss: 394.22
 ---- batch: 050 ----
mean loss: 394.66
 ---- batch: 060 ----
mean loss: 391.97
 ---- batch: 070 ----
mean loss: 392.73
 ---- batch: 080 ----
mean loss: 400.28
 ---- batch: 090 ----
mean loss: 402.44
train mean loss: 399.05
epoch train time: 0:00:02.336474
elapsed time: 0:02:50.979322
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 21:04:05.085363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.86
 ---- batch: 020 ----
mean loss: 392.27
 ---- batch: 030 ----
mean loss: 382.82
 ---- batch: 040 ----
mean loss: 409.48
 ---- batch: 050 ----
mean loss: 387.53
 ---- batch: 060 ----
mean loss: 405.42
 ---- batch: 070 ----
mean loss: 390.05
 ---- batch: 080 ----
mean loss: 392.60
 ---- batch: 090 ----
mean loss: 384.85
train mean loss: 394.55
epoch train time: 0:00:02.338123
elapsed time: 0:02:53.317644
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 21:04:07.423702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.99
 ---- batch: 020 ----
mean loss: 380.90
 ---- batch: 030 ----
mean loss: 405.17
 ---- batch: 040 ----
mean loss: 395.71
 ---- batch: 050 ----
mean loss: 406.01
 ---- batch: 060 ----
mean loss: 393.79
 ---- batch: 070 ----
mean loss: 413.20
 ---- batch: 080 ----
mean loss: 403.89
 ---- batch: 090 ----
mean loss: 398.03
train mean loss: 397.18
epoch train time: 0:00:02.325552
elapsed time: 0:02:55.643392
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 21:04:09.749432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.27
 ---- batch: 020 ----
mean loss: 399.71
 ---- batch: 030 ----
mean loss: 395.05
 ---- batch: 040 ----
mean loss: 398.19
 ---- batch: 050 ----
mean loss: 388.85
 ---- batch: 060 ----
mean loss: 398.29
 ---- batch: 070 ----
mean loss: 400.64
 ---- batch: 080 ----
mean loss: 393.40
 ---- batch: 090 ----
mean loss: 405.75
train mean loss: 398.52
epoch train time: 0:00:02.327658
elapsed time: 0:02:57.971246
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 21:04:12.077291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.32
 ---- batch: 020 ----
mean loss: 387.59
 ---- batch: 030 ----
mean loss: 407.69
 ---- batch: 040 ----
mean loss: 384.36
 ---- batch: 050 ----
mean loss: 398.67
 ---- batch: 060 ----
mean loss: 401.57
 ---- batch: 070 ----
mean loss: 398.08
 ---- batch: 080 ----
mean loss: 397.39
 ---- batch: 090 ----
mean loss: 395.53
train mean loss: 398.16
epoch train time: 0:00:02.329624
elapsed time: 0:03:00.301058
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 21:04:14.407106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.96
 ---- batch: 020 ----
mean loss: 395.31
 ---- batch: 030 ----
mean loss: 399.91
 ---- batch: 040 ----
mean loss: 398.27
 ---- batch: 050 ----
mean loss: 430.23
 ---- batch: 060 ----
mean loss: 400.71
 ---- batch: 070 ----
mean loss: 402.61
 ---- batch: 080 ----
mean loss: 392.14
 ---- batch: 090 ----
mean loss: 389.39
train mean loss: 399.76
epoch train time: 0:00:02.334556
elapsed time: 0:03:02.635805
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 21:04:16.741862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.39
 ---- batch: 020 ----
mean loss: 395.95
 ---- batch: 030 ----
mean loss: 418.48
 ---- batch: 040 ----
mean loss: 409.56
 ---- batch: 050 ----
mean loss: 397.10
 ---- batch: 060 ----
mean loss: 402.07
 ---- batch: 070 ----
mean loss: 390.56
 ---- batch: 080 ----
mean loss: 389.28
 ---- batch: 090 ----
mean loss: 389.11
train mean loss: 400.11
epoch train time: 0:00:02.336245
elapsed time: 0:03:04.972286
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 21:04:19.078328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.20
 ---- batch: 020 ----
mean loss: 394.50
 ---- batch: 030 ----
mean loss: 397.69
 ---- batch: 040 ----
mean loss: 397.04
 ---- batch: 050 ----
mean loss: 393.13
 ---- batch: 060 ----
mean loss: 381.02
 ---- batch: 070 ----
mean loss: 388.38
 ---- batch: 080 ----
mean loss: 392.79
 ---- batch: 090 ----
mean loss: 383.20
train mean loss: 389.93
epoch train time: 0:00:02.336218
elapsed time: 0:03:07.308705
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 21:04:21.414749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.41
 ---- batch: 020 ----
mean loss: 393.05
 ---- batch: 030 ----
mean loss: 414.33
 ---- batch: 040 ----
mean loss: 404.86
 ---- batch: 050 ----
mean loss: 401.23
 ---- batch: 060 ----
mean loss: 398.87
 ---- batch: 070 ----
mean loss: 391.62
 ---- batch: 080 ----
mean loss: 393.99
 ---- batch: 090 ----
mean loss: 370.74
train mean loss: 395.35
epoch train time: 0:00:02.333145
elapsed time: 0:03:09.642041
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 21:04:23.748086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.56
 ---- batch: 020 ----
mean loss: 390.20
 ---- batch: 030 ----
mean loss: 391.99
 ---- batch: 040 ----
mean loss: 385.27
 ---- batch: 050 ----
mean loss: 396.26
 ---- batch: 060 ----
mean loss: 396.93
 ---- batch: 070 ----
mean loss: 394.15
 ---- batch: 080 ----
mean loss: 402.44
 ---- batch: 090 ----
mean loss: 384.49
train mean loss: 391.62
epoch train time: 0:00:02.339319
elapsed time: 0:03:11.981584
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 21:04:26.087663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.07
 ---- batch: 020 ----
mean loss: 383.92
 ---- batch: 030 ----
mean loss: 387.00
 ---- batch: 040 ----
mean loss: 382.40
 ---- batch: 050 ----
mean loss: 388.75
 ---- batch: 060 ----
mean loss: 387.12
 ---- batch: 070 ----
mean loss: 393.30
 ---- batch: 080 ----
mean loss: 383.50
 ---- batch: 090 ----
mean loss: 390.12
train mean loss: 387.35
epoch train time: 0:00:02.333330
elapsed time: 0:03:14.315172
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 21:04:28.421218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.91
 ---- batch: 020 ----
mean loss: 394.36
 ---- batch: 030 ----
mean loss: 400.23
 ---- batch: 040 ----
mean loss: 389.93
 ---- batch: 050 ----
mean loss: 398.56
 ---- batch: 060 ----
mean loss: 385.27
 ---- batch: 070 ----
mean loss: 389.26
 ---- batch: 080 ----
mean loss: 398.12
 ---- batch: 090 ----
mean loss: 390.83
train mean loss: 394.80
epoch train time: 0:00:02.338998
elapsed time: 0:03:16.654364
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 21:04:30.760407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.10
 ---- batch: 020 ----
mean loss: 385.62
 ---- batch: 030 ----
mean loss: 402.54
 ---- batch: 040 ----
mean loss: 398.20
 ---- batch: 050 ----
mean loss: 411.46
 ---- batch: 060 ----
mean loss: 395.40
 ---- batch: 070 ----
mean loss: 407.22
 ---- batch: 080 ----
mean loss: 395.72
 ---- batch: 090 ----
mean loss: 389.04
train mean loss: 397.74
epoch train time: 0:00:02.337454
elapsed time: 0:03:18.992004
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 21:04:33.098049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.85
 ---- batch: 020 ----
mean loss: 395.55
 ---- batch: 030 ----
mean loss: 430.80
 ---- batch: 040 ----
mean loss: 398.79
 ---- batch: 050 ----
mean loss: 395.58
 ---- batch: 060 ----
mean loss: 407.43
 ---- batch: 070 ----
mean loss: 385.97
 ---- batch: 080 ----
mean loss: 385.48
 ---- batch: 090 ----
mean loss: 382.31
train mean loss: 398.14
epoch train time: 0:00:02.339597
elapsed time: 0:03:21.331817
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 21:04:35.437891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.87
 ---- batch: 020 ----
mean loss: 387.67
 ---- batch: 030 ----
mean loss: 395.15
 ---- batch: 040 ----
mean loss: 400.09
 ---- batch: 050 ----
mean loss: 408.64
 ---- batch: 060 ----
mean loss: 391.14
 ---- batch: 070 ----
mean loss: 389.15
 ---- batch: 080 ----
mean loss: 400.66
 ---- batch: 090 ----
mean loss: 389.34
train mean loss: 394.82
epoch train time: 0:00:02.327906
elapsed time: 0:03:23.659993
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 21:04:37.766054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.93
 ---- batch: 020 ----
mean loss: 392.94
 ---- batch: 030 ----
mean loss: 393.23
 ---- batch: 040 ----
mean loss: 396.23
 ---- batch: 050 ----
mean loss: 383.35
 ---- batch: 060 ----
mean loss: 389.31
 ---- batch: 070 ----
mean loss: 395.21
 ---- batch: 080 ----
mean loss: 396.09
 ---- batch: 090 ----
mean loss: 397.12
train mean loss: 393.60
epoch train time: 0:00:02.338046
elapsed time: 0:03:25.998237
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 21:04:40.104277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.90
 ---- batch: 020 ----
mean loss: 389.15
 ---- batch: 030 ----
mean loss: 396.59
 ---- batch: 040 ----
mean loss: 390.72
 ---- batch: 050 ----
mean loss: 380.45
 ---- batch: 060 ----
mean loss: 395.92
 ---- batch: 070 ----
mean loss: 385.05
 ---- batch: 080 ----
mean loss: 405.73
 ---- batch: 090 ----
mean loss: 394.84
train mean loss: 392.32
epoch train time: 0:00:02.339541
elapsed time: 0:03:28.337959
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 21:04:42.444023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.48
 ---- batch: 020 ----
mean loss: 399.94
 ---- batch: 030 ----
mean loss: 390.01
 ---- batch: 040 ----
mean loss: 390.14
 ---- batch: 050 ----
mean loss: 404.15
 ---- batch: 060 ----
mean loss: 395.11
 ---- batch: 070 ----
mean loss: 403.33
 ---- batch: 080 ----
mean loss: 393.71
 ---- batch: 090 ----
mean loss: 407.88
train mean loss: 396.90
epoch train time: 0:00:02.329751
elapsed time: 0:03:30.667939
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 21:04:44.773987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.74
 ---- batch: 020 ----
mean loss: 383.22
 ---- batch: 030 ----
mean loss: 384.42
 ---- batch: 040 ----
mean loss: 398.08
 ---- batch: 050 ----
mean loss: 395.53
 ---- batch: 060 ----
mean loss: 389.80
 ---- batch: 070 ----
mean loss: 389.53
 ---- batch: 080 ----
mean loss: 398.94
 ---- batch: 090 ----
mean loss: 391.50
train mean loss: 393.14
epoch train time: 0:00:02.331504
elapsed time: 0:03:32.999649
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 21:04:47.105722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.58
 ---- batch: 020 ----
mean loss: 395.31
 ---- batch: 030 ----
mean loss: 396.88
 ---- batch: 040 ----
mean loss: 396.20
 ---- batch: 050 ----
mean loss: 392.35
 ---- batch: 060 ----
mean loss: 389.65
 ---- batch: 070 ----
mean loss: 390.50
 ---- batch: 080 ----
mean loss: 393.04
 ---- batch: 090 ----
mean loss: 399.01
train mean loss: 392.75
epoch train time: 0:00:02.350017
elapsed time: 0:03:35.349877
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 21:04:49.455937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.98
 ---- batch: 020 ----
mean loss: 400.00
 ---- batch: 030 ----
mean loss: 388.35
 ---- batch: 040 ----
mean loss: 407.53
 ---- batch: 050 ----
mean loss: 395.44
 ---- batch: 060 ----
mean loss: 389.35
 ---- batch: 070 ----
mean loss: 391.48
 ---- batch: 080 ----
mean loss: 386.59
 ---- batch: 090 ----
mean loss: 373.87
train mean loss: 391.01
epoch train time: 0:00:02.329208
elapsed time: 0:03:37.679283
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 21:04:51.785325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.27
 ---- batch: 020 ----
mean loss: 398.36
 ---- batch: 030 ----
mean loss: 392.37
 ---- batch: 040 ----
mean loss: 397.30
 ---- batch: 050 ----
mean loss: 391.06
 ---- batch: 060 ----
mean loss: 390.51
 ---- batch: 070 ----
mean loss: 378.45
 ---- batch: 080 ----
mean loss: 383.34
 ---- batch: 090 ----
mean loss: 391.52
train mean loss: 389.56
epoch train time: 0:00:02.325795
elapsed time: 0:03:40.005264
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 21:04:54.111307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.04
 ---- batch: 020 ----
mean loss: 392.82
 ---- batch: 030 ----
mean loss: 403.77
 ---- batch: 040 ----
mean loss: 402.47
 ---- batch: 050 ----
mean loss: 409.80
 ---- batch: 060 ----
mean loss: 408.11
 ---- batch: 070 ----
mean loss: 396.98
 ---- batch: 080 ----
mean loss: 399.48
 ---- batch: 090 ----
mean loss: 405.57
train mean loss: 400.43
epoch train time: 0:00:02.324136
elapsed time: 0:03:42.329589
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 21:04:56.435630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.22
 ---- batch: 020 ----
mean loss: 390.63
 ---- batch: 030 ----
mean loss: 390.32
 ---- batch: 040 ----
mean loss: 391.33
 ---- batch: 050 ----
mean loss: 392.23
 ---- batch: 060 ----
mean loss: 383.00
 ---- batch: 070 ----
mean loss: 383.35
 ---- batch: 080 ----
mean loss: 394.74
 ---- batch: 090 ----
mean loss: 406.49
train mean loss: 392.84
epoch train time: 0:00:02.332693
elapsed time: 0:03:44.662477
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 21:04:58.768546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.17
 ---- batch: 020 ----
mean loss: 393.97
 ---- batch: 030 ----
mean loss: 387.35
 ---- batch: 040 ----
mean loss: 390.06
 ---- batch: 050 ----
mean loss: 407.67
 ---- batch: 060 ----
mean loss: 390.69
 ---- batch: 070 ----
mean loss: 390.96
 ---- batch: 080 ----
mean loss: 400.18
 ---- batch: 090 ----
mean loss: 388.77
train mean loss: 394.89
epoch train time: 0:00:02.322198
elapsed time: 0:03:46.984893
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 21:05:01.090957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.78
 ---- batch: 020 ----
mean loss: 388.72
 ---- batch: 030 ----
mean loss: 379.53
 ---- batch: 040 ----
mean loss: 380.83
 ---- batch: 050 ----
mean loss: 379.83
 ---- batch: 060 ----
mean loss: 393.80
 ---- batch: 070 ----
mean loss: 398.34
 ---- batch: 080 ----
mean loss: 377.69
 ---- batch: 090 ----
mean loss: 395.78
train mean loss: 392.11
epoch train time: 0:00:02.335653
elapsed time: 0:03:49.320748
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 21:05:03.426792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.51
 ---- batch: 020 ----
mean loss: 381.43
 ---- batch: 030 ----
mean loss: 381.90
 ---- batch: 040 ----
mean loss: 376.26
 ---- batch: 050 ----
mean loss: 394.72
 ---- batch: 060 ----
mean loss: 399.31
 ---- batch: 070 ----
mean loss: 391.34
 ---- batch: 080 ----
mean loss: 391.69
 ---- batch: 090 ----
mean loss: 397.87
train mean loss: 389.59
epoch train time: 0:00:02.332673
elapsed time: 0:03:51.653604
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 21:05:05.759646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.27
 ---- batch: 020 ----
mean loss: 391.75
 ---- batch: 030 ----
mean loss: 384.44
 ---- batch: 040 ----
mean loss: 392.46
 ---- batch: 050 ----
mean loss: 386.43
 ---- batch: 060 ----
mean loss: 380.84
 ---- batch: 070 ----
mean loss: 373.72
 ---- batch: 080 ----
mean loss: 387.63
 ---- batch: 090 ----
mean loss: 392.26
train mean loss: 389.94
epoch train time: 0:00:02.329730
elapsed time: 0:03:53.983537
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 21:05:08.089586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.20
 ---- batch: 020 ----
mean loss: 389.47
 ---- batch: 030 ----
mean loss: 391.94
 ---- batch: 040 ----
mean loss: 378.31
 ---- batch: 050 ----
mean loss: 392.92
 ---- batch: 060 ----
mean loss: 401.76
 ---- batch: 070 ----
mean loss: 413.51
 ---- batch: 080 ----
mean loss: 388.25
 ---- batch: 090 ----
mean loss: 381.67
train mean loss: 393.34
epoch train time: 0:00:02.331592
elapsed time: 0:03:56.315311
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 21:05:10.421367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.44
 ---- batch: 020 ----
mean loss: 392.50
 ---- batch: 030 ----
mean loss: 399.90
 ---- batch: 040 ----
mean loss: 390.90
 ---- batch: 050 ----
mean loss: 367.85
 ---- batch: 060 ----
mean loss: 390.63
 ---- batch: 070 ----
mean loss: 387.74
 ---- batch: 080 ----
mean loss: 398.21
 ---- batch: 090 ----
mean loss: 403.27
train mean loss: 390.86
epoch train time: 0:00:02.336177
elapsed time: 0:03:58.651748
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 21:05:12.757792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.71
 ---- batch: 020 ----
mean loss: 383.60
 ---- batch: 030 ----
mean loss: 388.83
 ---- batch: 040 ----
mean loss: 397.21
 ---- batch: 050 ----
mean loss: 394.84
 ---- batch: 060 ----
mean loss: 411.73
 ---- batch: 070 ----
mean loss: 391.82
 ---- batch: 080 ----
mean loss: 387.93
 ---- batch: 090 ----
mean loss: 385.48
train mean loss: 395.91
epoch train time: 0:00:02.322583
elapsed time: 0:04:00.974532
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 21:05:15.080578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.26
 ---- batch: 020 ----
mean loss: 395.95
 ---- batch: 030 ----
mean loss: 373.93
 ---- batch: 040 ----
mean loss: 389.27
 ---- batch: 050 ----
mean loss: 401.87
 ---- batch: 060 ----
mean loss: 396.00
 ---- batch: 070 ----
mean loss: 399.41
 ---- batch: 080 ----
mean loss: 392.35
 ---- batch: 090 ----
mean loss: 383.69
train mean loss: 390.14
epoch train time: 0:00:02.338723
elapsed time: 0:04:03.313437
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 21:05:17.419497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.21
 ---- batch: 020 ----
mean loss: 402.04
 ---- batch: 030 ----
mean loss: 382.92
 ---- batch: 040 ----
mean loss: 409.34
 ---- batch: 050 ----
mean loss: 413.94
 ---- batch: 060 ----
mean loss: 404.80
 ---- batch: 070 ----
mean loss: 387.12
 ---- batch: 080 ----
mean loss: 392.90
 ---- batch: 090 ----
mean loss: 391.70
train mean loss: 397.88
epoch train time: 0:00:02.330650
elapsed time: 0:04:05.644302
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 21:05:19.750345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.21
 ---- batch: 020 ----
mean loss: 401.74
 ---- batch: 030 ----
mean loss: 396.43
 ---- batch: 040 ----
mean loss: 396.32
 ---- batch: 050 ----
mean loss: 410.57
 ---- batch: 060 ----
mean loss: 400.50
 ---- batch: 070 ----
mean loss: 392.84
 ---- batch: 080 ----
mean loss: 388.37
 ---- batch: 090 ----
mean loss: 397.32
train mean loss: 394.97
epoch train time: 0:00:02.325834
elapsed time: 0:04:07.970324
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 21:05:22.076366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.71
 ---- batch: 020 ----
mean loss: 404.46
 ---- batch: 030 ----
mean loss: 392.82
 ---- batch: 040 ----
mean loss: 402.85
 ---- batch: 050 ----
mean loss: 398.49
 ---- batch: 060 ----
mean loss: 396.71
 ---- batch: 070 ----
mean loss: 391.08
 ---- batch: 080 ----
mean loss: 391.99
 ---- batch: 090 ----
mean loss: 378.62
train mean loss: 392.38
epoch train time: 0:00:02.336976
elapsed time: 0:04:10.307492
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 21:05:24.413552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.29
 ---- batch: 020 ----
mean loss: 412.39
 ---- batch: 030 ----
mean loss: 393.72
 ---- batch: 040 ----
mean loss: 385.53
 ---- batch: 050 ----
mean loss: 383.18
 ---- batch: 060 ----
mean loss: 396.12
 ---- batch: 070 ----
mean loss: 397.64
 ---- batch: 080 ----
mean loss: 387.43
 ---- batch: 090 ----
mean loss: 377.13
train mean loss: 390.14
epoch train time: 0:00:02.331247
elapsed time: 0:04:12.638993
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 21:05:26.745042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.60
 ---- batch: 020 ----
mean loss: 392.22
 ---- batch: 030 ----
mean loss: 393.54
 ---- batch: 040 ----
mean loss: 384.21
 ---- batch: 050 ----
mean loss: 396.48
 ---- batch: 060 ----
mean loss: 394.37
 ---- batch: 070 ----
mean loss: 389.02
 ---- batch: 080 ----
mean loss: 389.99
 ---- batch: 090 ----
mean loss: 385.43
train mean loss: 387.97
epoch train time: 0:00:02.331011
elapsed time: 0:04:14.970213
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 21:05:29.076265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.83
 ---- batch: 020 ----
mean loss: 382.62
 ---- batch: 030 ----
mean loss: 399.28
 ---- batch: 040 ----
mean loss: 383.53
 ---- batch: 050 ----
mean loss: 377.90
 ---- batch: 060 ----
mean loss: 390.88
 ---- batch: 070 ----
mean loss: 389.42
 ---- batch: 080 ----
mean loss: 391.85
 ---- batch: 090 ----
mean loss: 400.50
train mean loss: 389.90
epoch train time: 0:00:02.333707
elapsed time: 0:04:17.304137
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 21:05:31.410181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.73
 ---- batch: 020 ----
mean loss: 382.39
 ---- batch: 030 ----
mean loss: 391.28
 ---- batch: 040 ----
mean loss: 383.05
 ---- batch: 050 ----
mean loss: 391.34
 ---- batch: 060 ----
mean loss: 390.80
 ---- batch: 070 ----
mean loss: 399.22
 ---- batch: 080 ----
mean loss: 384.76
 ---- batch: 090 ----
mean loss: 398.03
train mean loss: 392.39
epoch train time: 0:00:02.327869
elapsed time: 0:04:19.632199
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 21:05:33.738242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.29
 ---- batch: 020 ----
mean loss: 388.91
 ---- batch: 030 ----
mean loss: 383.20
 ---- batch: 040 ----
mean loss: 397.44
 ---- batch: 050 ----
mean loss: 384.74
 ---- batch: 060 ----
mean loss: 384.98
 ---- batch: 070 ----
mean loss: 396.02
 ---- batch: 080 ----
mean loss: 386.26
 ---- batch: 090 ----
mean loss: 391.74
train mean loss: 391.76
epoch train time: 0:00:02.322879
elapsed time: 0:04:21.955338
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 21:05:36.061416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.36
 ---- batch: 020 ----
mean loss: 399.36
 ---- batch: 030 ----
mean loss: 373.51
 ---- batch: 040 ----
mean loss: 389.01
 ---- batch: 050 ----
mean loss: 382.08
 ---- batch: 060 ----
mean loss: 389.25
 ---- batch: 070 ----
mean loss: 386.21
 ---- batch: 080 ----
mean loss: 395.20
 ---- batch: 090 ----
mean loss: 400.12
train mean loss: 390.54
epoch train time: 0:00:02.327948
elapsed time: 0:04:24.283501
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 21:05:38.389544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.71
 ---- batch: 020 ----
mean loss: 385.27
 ---- batch: 030 ----
mean loss: 387.31
 ---- batch: 040 ----
mean loss: 389.56
 ---- batch: 050 ----
mean loss: 396.65
 ---- batch: 060 ----
mean loss: 390.29
 ---- batch: 070 ----
mean loss: 396.98
 ---- batch: 080 ----
mean loss: 403.06
 ---- batch: 090 ----
mean loss: 403.57
train mean loss: 395.33
epoch train time: 0:00:02.327734
elapsed time: 0:04:26.611442
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 21:05:40.717489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.74
 ---- batch: 020 ----
mean loss: 374.19
 ---- batch: 030 ----
mean loss: 383.19
 ---- batch: 040 ----
mean loss: 400.44
 ---- batch: 050 ----
mean loss: 397.14
 ---- batch: 060 ----
mean loss: 394.77
 ---- batch: 070 ----
mean loss: 387.18
 ---- batch: 080 ----
mean loss: 397.00
 ---- batch: 090 ----
mean loss: 398.48
train mean loss: 393.38
epoch train time: 0:00:02.332669
elapsed time: 0:04:28.944312
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 21:05:43.050369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.06
 ---- batch: 020 ----
mean loss: 388.67
 ---- batch: 030 ----
mean loss: 376.02
 ---- batch: 040 ----
mean loss: 379.97
 ---- batch: 050 ----
mean loss: 394.85
 ---- batch: 060 ----
mean loss: 387.57
 ---- batch: 070 ----
mean loss: 391.08
 ---- batch: 080 ----
mean loss: 401.23
 ---- batch: 090 ----
mean loss: 391.83
train mean loss: 389.57
epoch train time: 0:00:02.333521
elapsed time: 0:04:31.278036
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 21:05:45.384080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.34
 ---- batch: 020 ----
mean loss: 386.65
 ---- batch: 030 ----
mean loss: 381.07
 ---- batch: 040 ----
mean loss: 394.03
 ---- batch: 050 ----
mean loss: 381.91
 ---- batch: 060 ----
mean loss: 386.17
 ---- batch: 070 ----
mean loss: 390.14
 ---- batch: 080 ----
mean loss: 386.25
 ---- batch: 090 ----
mean loss: 399.85
train mean loss: 391.02
epoch train time: 0:00:02.335516
elapsed time: 0:04:33.613747
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 21:05:47.719788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.81
 ---- batch: 020 ----
mean loss: 382.58
 ---- batch: 030 ----
mean loss: 388.32
 ---- batch: 040 ----
mean loss: 384.98
 ---- batch: 050 ----
mean loss: 390.31
 ---- batch: 060 ----
mean loss: 394.03
 ---- batch: 070 ----
mean loss: 401.05
 ---- batch: 080 ----
mean loss: 399.78
 ---- batch: 090 ----
mean loss: 398.35
train mean loss: 392.71
epoch train time: 0:00:02.328248
elapsed time: 0:04:35.942181
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 21:05:50.048225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.35
 ---- batch: 020 ----
mean loss: 414.14
 ---- batch: 030 ----
mean loss: 422.62
 ---- batch: 040 ----
mean loss: 407.07
 ---- batch: 050 ----
mean loss: 391.31
 ---- batch: 060 ----
mean loss: 388.84
 ---- batch: 070 ----
mean loss: 389.56
 ---- batch: 080 ----
mean loss: 384.62
 ---- batch: 090 ----
mean loss: 378.26
train mean loss: 397.67
epoch train time: 0:00:02.327508
elapsed time: 0:04:38.269870
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 21:05:52.375912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.85
 ---- batch: 020 ----
mean loss: 403.90
 ---- batch: 030 ----
mean loss: 390.44
 ---- batch: 040 ----
mean loss: 383.56
 ---- batch: 050 ----
mean loss: 378.96
 ---- batch: 060 ----
mean loss: 394.61
 ---- batch: 070 ----
mean loss: 385.97
 ---- batch: 080 ----
mean loss: 390.88
 ---- batch: 090 ----
mean loss: 396.63
train mean loss: 391.45
epoch train time: 0:00:02.326989
elapsed time: 0:04:40.597080
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 21:05:54.703145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.36
 ---- batch: 020 ----
mean loss: 403.53
 ---- batch: 030 ----
mean loss: 391.51
 ---- batch: 040 ----
mean loss: 381.85
 ---- batch: 050 ----
mean loss: 395.91
 ---- batch: 060 ----
mean loss: 389.78
 ---- batch: 070 ----
mean loss: 382.23
 ---- batch: 080 ----
mean loss: 393.18
 ---- batch: 090 ----
mean loss: 391.02
train mean loss: 389.49
epoch train time: 0:00:02.325201
elapsed time: 0:04:42.922521
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 21:05:57.028548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.41
 ---- batch: 020 ----
mean loss: 395.77
 ---- batch: 030 ----
mean loss: 402.63
 ---- batch: 040 ----
mean loss: 386.40
 ---- batch: 050 ----
mean loss: 393.29
 ---- batch: 060 ----
mean loss: 384.30
 ---- batch: 070 ----
mean loss: 385.77
 ---- batch: 080 ----
mean loss: 407.84
 ---- batch: 090 ----
mean loss: 393.72
train mean loss: 390.60
epoch train time: 0:00:02.325895
elapsed time: 0:04:45.248582
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 21:05:59.354641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.40
 ---- batch: 020 ----
mean loss: 388.77
 ---- batch: 030 ----
mean loss: 376.38
 ---- batch: 040 ----
mean loss: 390.10
 ---- batch: 050 ----
mean loss: 385.51
 ---- batch: 060 ----
mean loss: 394.60
 ---- batch: 070 ----
mean loss: 394.39
 ---- batch: 080 ----
mean loss: 385.17
 ---- batch: 090 ----
mean loss: 389.49
train mean loss: 389.14
epoch train time: 0:00:02.326636
elapsed time: 0:04:47.575426
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 21:06:01.681474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.02
 ---- batch: 020 ----
mean loss: 394.98
 ---- batch: 030 ----
mean loss: 394.80
 ---- batch: 040 ----
mean loss: 391.20
 ---- batch: 050 ----
mean loss: 373.03
 ---- batch: 060 ----
mean loss: 394.95
 ---- batch: 070 ----
mean loss: 391.77
 ---- batch: 080 ----
mean loss: 386.80
 ---- batch: 090 ----
mean loss: 395.97
train mean loss: 391.03
epoch train time: 0:00:02.333301
elapsed time: 0:04:49.908911
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 21:06:04.014952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.15
 ---- batch: 020 ----
mean loss: 388.45
 ---- batch: 030 ----
mean loss: 400.46
 ---- batch: 040 ----
mean loss: 398.49
 ---- batch: 050 ----
mean loss: 387.10
 ---- batch: 060 ----
mean loss: 390.71
 ---- batch: 070 ----
mean loss: 399.35
 ---- batch: 080 ----
mean loss: 387.72
 ---- batch: 090 ----
mean loss: 384.13
train mean loss: 393.74
epoch train time: 0:00:02.331397
elapsed time: 0:04:52.240496
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 21:06:06.346544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.36
 ---- batch: 020 ----
mean loss: 381.11
 ---- batch: 030 ----
mean loss: 393.39
 ---- batch: 040 ----
mean loss: 404.21
 ---- batch: 050 ----
mean loss: 387.53
 ---- batch: 060 ----
mean loss: 392.97
 ---- batch: 070 ----
mean loss: 396.55
 ---- batch: 080 ----
mean loss: 377.36
 ---- batch: 090 ----
mean loss: 372.08
train mean loss: 387.29
epoch train time: 0:00:02.328777
elapsed time: 0:04:54.569500
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 21:06:08.675539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.41
 ---- batch: 020 ----
mean loss: 393.59
 ---- batch: 030 ----
mean loss: 396.24
 ---- batch: 040 ----
mean loss: 383.54
 ---- batch: 050 ----
mean loss: 387.05
 ---- batch: 060 ----
mean loss: 384.88
 ---- batch: 070 ----
mean loss: 387.85
 ---- batch: 080 ----
mean loss: 378.81
 ---- batch: 090 ----
mean loss: 386.74
train mean loss: 387.58
epoch train time: 0:00:02.334956
elapsed time: 0:04:56.904636
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 21:06:11.010699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.06
 ---- batch: 020 ----
mean loss: 382.85
 ---- batch: 030 ----
mean loss: 386.42
 ---- batch: 040 ----
mean loss: 397.68
 ---- batch: 050 ----
mean loss: 400.95
 ---- batch: 060 ----
mean loss: 397.72
 ---- batch: 070 ----
mean loss: 381.02
 ---- batch: 080 ----
mean loss: 388.27
 ---- batch: 090 ----
mean loss: 396.27
train mean loss: 390.55
epoch train time: 0:00:02.320667
elapsed time: 0:04:59.225508
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 21:06:13.331551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.38
 ---- batch: 020 ----
mean loss: 380.48
 ---- batch: 030 ----
mean loss: 388.57
 ---- batch: 040 ----
mean loss: 398.47
 ---- batch: 050 ----
mean loss: 379.96
 ---- batch: 060 ----
mean loss: 396.86
 ---- batch: 070 ----
mean loss: 385.64
 ---- batch: 080 ----
mean loss: 382.34
 ---- batch: 090 ----
mean loss: 388.40
train mean loss: 387.81
epoch train time: 0:00:02.325183
elapsed time: 0:05:01.550869
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 21:06:15.656912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.64
 ---- batch: 020 ----
mean loss: 389.42
 ---- batch: 030 ----
mean loss: 386.21
 ---- batch: 040 ----
mean loss: 385.58
 ---- batch: 050 ----
mean loss: 396.38
 ---- batch: 060 ----
mean loss: 402.63
 ---- batch: 070 ----
mean loss: 393.58
 ---- batch: 080 ----
mean loss: 391.05
 ---- batch: 090 ----
mean loss: 401.57
train mean loss: 395.03
epoch train time: 0:00:02.330617
elapsed time: 0:05:03.881669
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 21:06:17.987709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.21
 ---- batch: 020 ----
mean loss: 377.90
 ---- batch: 030 ----
mean loss: 381.77
 ---- batch: 040 ----
mean loss: 400.13
 ---- batch: 050 ----
mean loss: 416.29
 ---- batch: 060 ----
mean loss: 417.36
 ---- batch: 070 ----
mean loss: 403.81
 ---- batch: 080 ----
mean loss: 395.98
 ---- batch: 090 ----
mean loss: 400.24
train mean loss: 396.34
epoch train time: 0:00:02.327215
elapsed time: 0:05:06.209062
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 21:06:20.315105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.22
 ---- batch: 020 ----
mean loss: 388.68
 ---- batch: 030 ----
mean loss: 388.10
 ---- batch: 040 ----
mean loss: 378.68
 ---- batch: 050 ----
mean loss: 385.03
 ---- batch: 060 ----
mean loss: 389.79
 ---- batch: 070 ----
mean loss: 407.96
 ---- batch: 080 ----
mean loss: 412.40
 ---- batch: 090 ----
mean loss: 395.19
train mean loss: 393.15
epoch train time: 0:00:02.325198
elapsed time: 0:05:08.534436
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 21:06:22.640477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.08
 ---- batch: 020 ----
mean loss: 403.30
 ---- batch: 030 ----
mean loss: 387.32
 ---- batch: 040 ----
mean loss: 392.62
 ---- batch: 050 ----
mean loss: 387.14
 ---- batch: 060 ----
mean loss: 382.80
 ---- batch: 070 ----
mean loss: 394.12
 ---- batch: 080 ----
mean loss: 387.10
 ---- batch: 090 ----
mean loss: 380.44
train mean loss: 391.02
epoch train time: 0:00:02.336487
elapsed time: 0:05:10.871102
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 21:06:24.977146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.43
 ---- batch: 020 ----
mean loss: 384.92
 ---- batch: 030 ----
mean loss: 391.38
 ---- batch: 040 ----
mean loss: 385.47
 ---- batch: 050 ----
mean loss: 384.20
 ---- batch: 060 ----
mean loss: 399.33
 ---- batch: 070 ----
mean loss: 391.67
 ---- batch: 080 ----
mean loss: 394.20
 ---- batch: 090 ----
mean loss: 385.64
train mean loss: 388.09
epoch train time: 0:00:02.328169
elapsed time: 0:05:13.199473
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 21:06:27.305516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.23
 ---- batch: 020 ----
mean loss: 392.95
 ---- batch: 030 ----
mean loss: 380.99
 ---- batch: 040 ----
mean loss: 394.76
 ---- batch: 050 ----
mean loss: 386.27
 ---- batch: 060 ----
mean loss: 388.90
 ---- batch: 070 ----
mean loss: 366.54
 ---- batch: 080 ----
mean loss: 386.55
 ---- batch: 090 ----
mean loss: 388.74
train mean loss: 385.59
epoch train time: 0:00:02.330921
elapsed time: 0:05:15.530605
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 21:06:29.636672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.55
 ---- batch: 020 ----
mean loss: 392.04
 ---- batch: 030 ----
mean loss: 374.86
 ---- batch: 040 ----
mean loss: 386.11
 ---- batch: 050 ----
mean loss: 392.51
 ---- batch: 060 ----
mean loss: 387.76
 ---- batch: 070 ----
mean loss: 395.80
 ---- batch: 080 ----
mean loss: 422.17
 ---- batch: 090 ----
mean loss: 430.16
train mean loss: 396.03
epoch train time: 0:00:02.335862
elapsed time: 0:05:17.866674
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 21:06:31.972718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.40
 ---- batch: 020 ----
mean loss: 389.82
 ---- batch: 030 ----
mean loss: 397.11
 ---- batch: 040 ----
mean loss: 393.87
 ---- batch: 050 ----
mean loss: 380.12
 ---- batch: 060 ----
mean loss: 394.78
 ---- batch: 070 ----
mean loss: 387.96
 ---- batch: 080 ----
mean loss: 385.50
 ---- batch: 090 ----
mean loss: 384.40
train mean loss: 391.26
epoch train time: 0:00:02.322334
elapsed time: 0:05:20.189211
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 21:06:34.295256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.83
 ---- batch: 020 ----
mean loss: 389.71
 ---- batch: 030 ----
mean loss: 386.17
 ---- batch: 040 ----
mean loss: 385.05
 ---- batch: 050 ----
mean loss: 375.07
 ---- batch: 060 ----
mean loss: 399.26
 ---- batch: 070 ----
mean loss: 401.46
 ---- batch: 080 ----
mean loss: 396.02
 ---- batch: 090 ----
mean loss: 394.33
train mean loss: 390.17
epoch train time: 0:00:02.324082
elapsed time: 0:05:22.513481
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 21:06:36.619526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.07
 ---- batch: 020 ----
mean loss: 391.03
 ---- batch: 030 ----
mean loss: 374.62
 ---- batch: 040 ----
mean loss: 380.37
 ---- batch: 050 ----
mean loss: 389.92
 ---- batch: 060 ----
mean loss: 385.73
 ---- batch: 070 ----
mean loss: 394.28
 ---- batch: 080 ----
mean loss: 387.65
 ---- batch: 090 ----
mean loss: 378.66
train mean loss: 384.68
epoch train time: 0:00:02.334827
elapsed time: 0:05:24.848495
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 21:06:38.954533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.27
 ---- batch: 020 ----
mean loss: 403.36
 ---- batch: 030 ----
mean loss: 381.98
 ---- batch: 040 ----
mean loss: 384.17
 ---- batch: 050 ----
mean loss: 378.67
 ---- batch: 060 ----
mean loss: 387.23
 ---- batch: 070 ----
mean loss: 388.84
 ---- batch: 080 ----
mean loss: 391.39
 ---- batch: 090 ----
mean loss: 389.29
train mean loss: 387.73
epoch train time: 0:00:02.334234
elapsed time: 0:05:27.182960
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 21:06:41.289009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.95
 ---- batch: 020 ----
mean loss: 388.19
 ---- batch: 030 ----
mean loss: 386.90
 ---- batch: 040 ----
mean loss: 387.88
 ---- batch: 050 ----
mean loss: 380.04
 ---- batch: 060 ----
mean loss: 381.23
 ---- batch: 070 ----
mean loss: 391.95
 ---- batch: 080 ----
mean loss: 387.87
 ---- batch: 090 ----
mean loss: 388.01
train mean loss: 385.20
epoch train time: 0:00:02.334920
elapsed time: 0:05:29.518086
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 21:06:43.624111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.32
 ---- batch: 020 ----
mean loss: 382.22
 ---- batch: 030 ----
mean loss: 387.02
 ---- batch: 040 ----
mean loss: 379.22
 ---- batch: 050 ----
mean loss: 380.20
 ---- batch: 060 ----
mean loss: 384.64
 ---- batch: 070 ----
mean loss: 374.09
 ---- batch: 080 ----
mean loss: 367.28
 ---- batch: 090 ----
mean loss: 403.58
train mean loss: 383.98
epoch train time: 0:00:02.333233
elapsed time: 0:05:31.851685
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 21:06:45.957753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.26
 ---- batch: 020 ----
mean loss: 366.14
 ---- batch: 030 ----
mean loss: 375.00
 ---- batch: 040 ----
mean loss: 401.41
 ---- batch: 050 ----
mean loss: 391.65
 ---- batch: 060 ----
mean loss: 381.81
 ---- batch: 070 ----
mean loss: 390.84
 ---- batch: 080 ----
mean loss: 398.38
 ---- batch: 090 ----
mean loss: 375.98
train mean loss: 384.20
epoch train time: 0:00:02.338523
elapsed time: 0:05:34.190413
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 21:06:48.296458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.36
 ---- batch: 020 ----
mean loss: 381.17
 ---- batch: 030 ----
mean loss: 380.94
 ---- batch: 040 ----
mean loss: 377.27
 ---- batch: 050 ----
mean loss: 387.95
 ---- batch: 060 ----
mean loss: 385.02
 ---- batch: 070 ----
mean loss: 386.04
 ---- batch: 080 ----
mean loss: 396.99
 ---- batch: 090 ----
mean loss: 387.14
train mean loss: 384.22
epoch train time: 0:00:02.322740
elapsed time: 0:05:36.513333
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 21:06:50.619374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.68
 ---- batch: 020 ----
mean loss: 371.41
 ---- batch: 030 ----
mean loss: 379.67
 ---- batch: 040 ----
mean loss: 397.01
 ---- batch: 050 ----
mean loss: 388.76
 ---- batch: 060 ----
mean loss: 392.05
 ---- batch: 070 ----
mean loss: 398.69
 ---- batch: 080 ----
mean loss: 407.87
 ---- batch: 090 ----
mean loss: 399.08
train mean loss: 390.12
epoch train time: 0:00:02.335356
elapsed time: 0:05:38.848871
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 21:06:52.954912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.14
 ---- batch: 020 ----
mean loss: 377.22
 ---- batch: 030 ----
mean loss: 390.96
 ---- batch: 040 ----
mean loss: 389.36
 ---- batch: 050 ----
mean loss: 408.17
 ---- batch: 060 ----
mean loss: 401.80
 ---- batch: 070 ----
mean loss: 413.48
 ---- batch: 080 ----
mean loss: 409.55
 ---- batch: 090 ----
mean loss: 387.82
train mean loss: 395.39
epoch train time: 0:00:02.330687
elapsed time: 0:05:41.179764
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 21:06:55.285814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.80
 ---- batch: 020 ----
mean loss: 382.62
 ---- batch: 030 ----
mean loss: 380.65
 ---- batch: 040 ----
mean loss: 387.32
 ---- batch: 050 ----
mean loss: 397.92
 ---- batch: 060 ----
mean loss: 388.75
 ---- batch: 070 ----
mean loss: 384.26
 ---- batch: 080 ----
mean loss: 394.98
 ---- batch: 090 ----
mean loss: 380.15
train mean loss: 387.64
epoch train time: 0:00:02.335118
elapsed time: 0:05:43.515084
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 21:06:57.621137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.28
 ---- batch: 020 ----
mean loss: 377.18
 ---- batch: 030 ----
mean loss: 381.78
 ---- batch: 040 ----
mean loss: 405.20
 ---- batch: 050 ----
mean loss: 389.25
 ---- batch: 060 ----
mean loss: 385.57
 ---- batch: 070 ----
mean loss: 378.39
 ---- batch: 080 ----
mean loss: 397.37
 ---- batch: 090 ----
mean loss: 414.15
train mean loss: 392.15
epoch train time: 0:00:02.332343
elapsed time: 0:05:45.847611
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 21:06:59.953671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.22
 ---- batch: 020 ----
mean loss: 399.24
 ---- batch: 030 ----
mean loss: 388.57
 ---- batch: 040 ----
mean loss: 383.36
 ---- batch: 050 ----
mean loss: 379.17
 ---- batch: 060 ----
mean loss: 388.62
 ---- batch: 070 ----
mean loss: 386.33
 ---- batch: 080 ----
mean loss: 395.40
 ---- batch: 090 ----
mean loss: 402.83
train mean loss: 392.06
epoch train time: 0:00:02.334068
elapsed time: 0:05:48.181931
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 21:07:02.287977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.19
 ---- batch: 020 ----
mean loss: 392.72
 ---- batch: 030 ----
mean loss: 369.89
 ---- batch: 040 ----
mean loss: 390.00
 ---- batch: 050 ----
mean loss: 388.72
 ---- batch: 060 ----
mean loss: 377.28
 ---- batch: 070 ----
mean loss: 384.18
 ---- batch: 080 ----
mean loss: 393.11
 ---- batch: 090 ----
mean loss: 398.07
train mean loss: 389.09
epoch train time: 0:00:02.330389
elapsed time: 0:05:50.512514
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 21:07:04.618575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.16
 ---- batch: 020 ----
mean loss: 388.51
 ---- batch: 030 ----
mean loss: 385.75
 ---- batch: 040 ----
mean loss: 381.16
 ---- batch: 050 ----
mean loss: 377.63
 ---- batch: 060 ----
mean loss: 384.93
 ---- batch: 070 ----
mean loss: 385.52
 ---- batch: 080 ----
mean loss: 378.92
 ---- batch: 090 ----
mean loss: 401.38
train mean loss: 385.53
epoch train time: 0:00:02.333124
elapsed time: 0:05:52.845843
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 21:07:06.951886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.63
 ---- batch: 020 ----
mean loss: 382.05
 ---- batch: 030 ----
mean loss: 383.56
 ---- batch: 040 ----
mean loss: 396.72
 ---- batch: 050 ----
mean loss: 383.99
 ---- batch: 060 ----
mean loss: 388.58
 ---- batch: 070 ----
mean loss: 392.43
 ---- batch: 080 ----
mean loss: 373.90
 ---- batch: 090 ----
mean loss: 402.58
train mean loss: 388.81
epoch train time: 0:00:02.334703
elapsed time: 0:05:55.180724
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 21:07:09.286764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.86
 ---- batch: 020 ----
mean loss: 386.55
 ---- batch: 030 ----
mean loss: 383.23
 ---- batch: 040 ----
mean loss: 380.34
 ---- batch: 050 ----
mean loss: 391.61
 ---- batch: 060 ----
mean loss: 383.53
 ---- batch: 070 ----
mean loss: 396.62
 ---- batch: 080 ----
mean loss: 388.58
 ---- batch: 090 ----
mean loss: 377.58
train mean loss: 386.15
epoch train time: 0:00:02.331410
elapsed time: 0:05:57.512316
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 21:07:11.618359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.22
 ---- batch: 020 ----
mean loss: 385.63
 ---- batch: 030 ----
mean loss: 401.14
 ---- batch: 040 ----
mean loss: 401.32
 ---- batch: 050 ----
mean loss: 385.50
 ---- batch: 060 ----
mean loss: 373.92
 ---- batch: 070 ----
mean loss: 390.01
 ---- batch: 080 ----
mean loss: 405.01
 ---- batch: 090 ----
mean loss: 391.38
train mean loss: 391.60
epoch train time: 0:00:02.351555
elapsed time: 0:05:59.864052
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 21:07:13.970136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.16
 ---- batch: 020 ----
mean loss: 377.69
 ---- batch: 030 ----
mean loss: 376.47
 ---- batch: 040 ----
mean loss: 376.90
 ---- batch: 050 ----
mean loss: 380.07
 ---- batch: 060 ----
mean loss: 377.12
 ---- batch: 070 ----
mean loss: 376.90
 ---- batch: 080 ----
mean loss: 381.20
 ---- batch: 090 ----
mean loss: 387.54
train mean loss: 380.83
epoch train time: 0:00:02.337234
elapsed time: 0:06:02.201513
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 21:07:16.307556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.85
 ---- batch: 020 ----
mean loss: 382.19
 ---- batch: 030 ----
mean loss: 387.12
 ---- batch: 040 ----
mean loss: 376.78
 ---- batch: 050 ----
mean loss: 382.88
 ---- batch: 060 ----
mean loss: 387.16
 ---- batch: 070 ----
mean loss: 375.52
 ---- batch: 080 ----
mean loss: 380.32
 ---- batch: 090 ----
mean loss: 400.45
train mean loss: 384.50
epoch train time: 0:00:02.327133
elapsed time: 0:06:04.528823
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 21:07:18.634864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.17
 ---- batch: 020 ----
mean loss: 389.55
 ---- batch: 030 ----
mean loss: 380.68
 ---- batch: 040 ----
mean loss: 376.50
 ---- batch: 050 ----
mean loss: 388.94
 ---- batch: 060 ----
mean loss: 381.98
 ---- batch: 070 ----
mean loss: 386.81
 ---- batch: 080 ----
mean loss: 400.83
 ---- batch: 090 ----
mean loss: 381.07
train mean loss: 388.90
epoch train time: 0:00:02.336987
elapsed time: 0:06:06.866022
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 21:07:20.972089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.10
 ---- batch: 020 ----
mean loss: 397.14
 ---- batch: 030 ----
mean loss: 384.84
 ---- batch: 040 ----
mean loss: 382.00
 ---- batch: 050 ----
mean loss: 382.91
 ---- batch: 060 ----
mean loss: 378.22
 ---- batch: 070 ----
mean loss: 384.22
 ---- batch: 080 ----
mean loss: 396.39
 ---- batch: 090 ----
mean loss: 392.64
train mean loss: 386.32
epoch train time: 0:00:02.337072
elapsed time: 0:06:09.203295
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 21:07:23.309338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.67
 ---- batch: 020 ----
mean loss: 381.22
 ---- batch: 030 ----
mean loss: 387.48
 ---- batch: 040 ----
mean loss: 397.10
 ---- batch: 050 ----
mean loss: 375.19
 ---- batch: 060 ----
mean loss: 392.71
 ---- batch: 070 ----
mean loss: 393.74
 ---- batch: 080 ----
mean loss: 387.72
 ---- batch: 090 ----
mean loss: 384.52
train mean loss: 386.33
epoch train time: 0:00:02.333950
elapsed time: 0:06:11.537432
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 21:07:25.643475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.89
 ---- batch: 020 ----
mean loss: 390.09
 ---- batch: 030 ----
mean loss: 378.42
 ---- batch: 040 ----
mean loss: 390.20
 ---- batch: 050 ----
mean loss: 393.45
 ---- batch: 060 ----
mean loss: 381.34
 ---- batch: 070 ----
mean loss: 369.85
 ---- batch: 080 ----
mean loss: 388.14
 ---- batch: 090 ----
mean loss: 384.75
train mean loss: 383.09
epoch train time: 0:00:02.352589
elapsed time: 0:06:13.890208
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 21:07:27.996252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.70
 ---- batch: 020 ----
mean loss: 390.24
 ---- batch: 030 ----
mean loss: 382.27
 ---- batch: 040 ----
mean loss: 389.67
 ---- batch: 050 ----
mean loss: 387.30
 ---- batch: 060 ----
mean loss: 382.00
 ---- batch: 070 ----
mean loss: 384.64
 ---- batch: 080 ----
mean loss: 397.49
 ---- batch: 090 ----
mean loss: 383.64
train mean loss: 386.23
epoch train time: 0:00:02.334678
elapsed time: 0:06:16.225087
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 21:07:30.331128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.65
 ---- batch: 020 ----
mean loss: 407.51
 ---- batch: 030 ----
mean loss: 391.55
 ---- batch: 040 ----
mean loss: 383.82
 ---- batch: 050 ----
mean loss: 377.20
 ---- batch: 060 ----
mean loss: 373.07
 ---- batch: 070 ----
mean loss: 370.03
 ---- batch: 080 ----
mean loss: 388.31
 ---- batch: 090 ----
mean loss: 398.54
train mean loss: 385.21
epoch train time: 0:00:02.333074
elapsed time: 0:06:18.558379
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 21:07:32.664447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.91
 ---- batch: 020 ----
mean loss: 386.01
 ---- batch: 030 ----
mean loss: 383.15
 ---- batch: 040 ----
mean loss: 377.25
 ---- batch: 050 ----
mean loss: 382.80
 ---- batch: 060 ----
mean loss: 377.77
 ---- batch: 070 ----
mean loss: 400.95
 ---- batch: 080 ----
mean loss: 377.68
 ---- batch: 090 ----
mean loss: 387.26
train mean loss: 382.79
epoch train time: 0:00:02.342448
elapsed time: 0:06:20.901062
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 21:07:35.007086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.28
 ---- batch: 020 ----
mean loss: 416.60
 ---- batch: 030 ----
mean loss: 400.07
 ---- batch: 040 ----
mean loss: 411.64
 ---- batch: 050 ----
mean loss: 391.58
 ---- batch: 060 ----
mean loss: 395.70
 ---- batch: 070 ----
mean loss: 393.66
 ---- batch: 080 ----
mean loss: 368.88
 ---- batch: 090 ----
mean loss: 376.31
train mean loss: 396.75
epoch train time: 0:00:02.334667
elapsed time: 0:06:23.235927
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 21:07:37.341973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.96
 ---- batch: 020 ----
mean loss: 392.04
 ---- batch: 030 ----
mean loss: 372.36
 ---- batch: 040 ----
mean loss: 389.99
 ---- batch: 050 ----
mean loss: 369.46
 ---- batch: 060 ----
mean loss: 386.50
 ---- batch: 070 ----
mean loss: 380.73
 ---- batch: 080 ----
mean loss: 385.01
 ---- batch: 090 ----
mean loss: 371.77
train mean loss: 381.46
epoch train time: 0:00:02.327559
elapsed time: 0:06:25.563696
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 21:07:39.669756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.06
 ---- batch: 020 ----
mean loss: 383.52
 ---- batch: 030 ----
mean loss: 383.56
 ---- batch: 040 ----
mean loss: 385.91
 ---- batch: 050 ----
mean loss: 371.75
 ---- batch: 060 ----
mean loss: 391.05
 ---- batch: 070 ----
mean loss: 383.41
 ---- batch: 080 ----
mean loss: 385.33
 ---- batch: 090 ----
mean loss: 386.20
train mean loss: 383.27
epoch train time: 0:00:02.334718
elapsed time: 0:06:27.898628
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 21:07:42.004673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.29
 ---- batch: 020 ----
mean loss: 378.22
 ---- batch: 030 ----
mean loss: 375.20
 ---- batch: 040 ----
mean loss: 375.80
 ---- batch: 050 ----
mean loss: 385.93
 ---- batch: 060 ----
mean loss: 394.85
 ---- batch: 070 ----
mean loss: 387.12
 ---- batch: 080 ----
mean loss: 393.52
 ---- batch: 090 ----
mean loss: 384.67
train mean loss: 383.89
epoch train time: 0:00:02.328524
elapsed time: 0:06:30.227333
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 21:07:44.333371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.25
 ---- batch: 020 ----
mean loss: 383.56
 ---- batch: 030 ----
mean loss: 384.65
 ---- batch: 040 ----
mean loss: 390.81
 ---- batch: 050 ----
mean loss: 386.19
 ---- batch: 060 ----
mean loss: 390.73
 ---- batch: 070 ----
mean loss: 375.56
 ---- batch: 080 ----
mean loss: 379.96
 ---- batch: 090 ----
mean loss: 377.98
train mean loss: 384.07
epoch train time: 0:00:02.326997
elapsed time: 0:06:32.554518
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 21:07:46.660560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.81
 ---- batch: 020 ----
mean loss: 375.63
 ---- batch: 030 ----
mean loss: 378.51
 ---- batch: 040 ----
mean loss: 387.91
 ---- batch: 050 ----
mean loss: 366.44
 ---- batch: 060 ----
mean loss: 405.14
 ---- batch: 070 ----
mean loss: 396.43
 ---- batch: 080 ----
mean loss: 383.92
 ---- batch: 090 ----
mean loss: 387.12
train mean loss: 383.30
epoch train time: 0:00:02.333178
elapsed time: 0:06:34.887897
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 21:07:48.993950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.04
 ---- batch: 020 ----
mean loss: 404.84
 ---- batch: 030 ----
mean loss: 388.63
 ---- batch: 040 ----
mean loss: 398.00
 ---- batch: 050 ----
mean loss: 372.79
 ---- batch: 060 ----
mean loss: 378.42
 ---- batch: 070 ----
mean loss: 399.83
 ---- batch: 080 ----
mean loss: 389.18
 ---- batch: 090 ----
mean loss: 390.73
train mean loss: 390.57
epoch train time: 0:00:02.334986
elapsed time: 0:06:37.223072
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 21:07:51.329114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.06
 ---- batch: 020 ----
mean loss: 374.13
 ---- batch: 030 ----
mean loss: 381.29
 ---- batch: 040 ----
mean loss: 387.75
 ---- batch: 050 ----
mean loss: 385.17
 ---- batch: 060 ----
mean loss: 386.50
 ---- batch: 070 ----
mean loss: 395.32
 ---- batch: 080 ----
mean loss: 422.33
 ---- batch: 090 ----
mean loss: 387.88
train mean loss: 390.39
epoch train time: 0:00:02.330914
elapsed time: 0:06:39.554179
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 21:07:53.660220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.83
 ---- batch: 020 ----
mean loss: 373.88
 ---- batch: 030 ----
mean loss: 377.45
 ---- batch: 040 ----
mean loss: 383.27
 ---- batch: 050 ----
mean loss: 385.85
 ---- batch: 060 ----
mean loss: 369.91
 ---- batch: 070 ----
mean loss: 372.20
 ---- batch: 080 ----
mean loss: 383.08
 ---- batch: 090 ----
mean loss: 378.94
train mean loss: 378.80
epoch train time: 0:00:02.337021
elapsed time: 0:06:41.891382
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 21:07:55.997448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.00
 ---- batch: 020 ----
mean loss: 377.41
 ---- batch: 030 ----
mean loss: 383.26
 ---- batch: 040 ----
mean loss: 383.16
 ---- batch: 050 ----
mean loss: 375.08
 ---- batch: 060 ----
mean loss: 390.00
 ---- batch: 070 ----
mean loss: 386.38
 ---- batch: 080 ----
mean loss: 393.36
 ---- batch: 090 ----
mean loss: 373.70
train mean loss: 383.34
epoch train time: 0:00:02.332274
elapsed time: 0:06:44.223851
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 21:07:58.329914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.19
 ---- batch: 020 ----
mean loss: 377.19
 ---- batch: 030 ----
mean loss: 393.09
 ---- batch: 040 ----
mean loss: 391.36
 ---- batch: 050 ----
mean loss: 382.72
 ---- batch: 060 ----
mean loss: 382.97
 ---- batch: 070 ----
mean loss: 378.70
 ---- batch: 080 ----
mean loss: 373.28
 ---- batch: 090 ----
mean loss: 378.05
train mean loss: 383.41
epoch train time: 0:00:02.328857
elapsed time: 0:06:46.552969
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 21:08:00.659008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.92
 ---- batch: 020 ----
mean loss: 393.45
 ---- batch: 030 ----
mean loss: 375.88
 ---- batch: 040 ----
mean loss: 374.31
 ---- batch: 050 ----
mean loss: 401.64
 ---- batch: 060 ----
mean loss: 396.68
 ---- batch: 070 ----
mean loss: 385.60
 ---- batch: 080 ----
mean loss: 378.22
 ---- batch: 090 ----
mean loss: 382.55
train mean loss: 385.17
epoch train time: 0:00:02.355483
elapsed time: 0:06:48.908639
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 21:08:03.014683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.74
 ---- batch: 020 ----
mean loss: 374.57
 ---- batch: 030 ----
mean loss: 397.78
 ---- batch: 040 ----
mean loss: 381.32
 ---- batch: 050 ----
mean loss: 379.70
 ---- batch: 060 ----
mean loss: 390.50
 ---- batch: 070 ----
mean loss: 378.18
 ---- batch: 080 ----
mean loss: 390.12
 ---- batch: 090 ----
mean loss: 386.89
train mean loss: 383.73
epoch train time: 0:00:02.333023
elapsed time: 0:06:51.241842
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 21:08:05.347882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.23
 ---- batch: 020 ----
mean loss: 388.51
 ---- batch: 030 ----
mean loss: 389.45
 ---- batch: 040 ----
mean loss: 384.92
 ---- batch: 050 ----
mean loss: 388.61
 ---- batch: 060 ----
mean loss: 379.33
 ---- batch: 070 ----
mean loss: 384.08
 ---- batch: 080 ----
mean loss: 380.75
 ---- batch: 090 ----
mean loss: 369.07
train mean loss: 382.81
epoch train time: 0:00:02.336274
elapsed time: 0:06:53.578293
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 21:08:07.684337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.49
 ---- batch: 020 ----
mean loss: 382.06
 ---- batch: 030 ----
mean loss: 384.93
 ---- batch: 040 ----
mean loss: 386.20
 ---- batch: 050 ----
mean loss: 378.67
 ---- batch: 060 ----
mean loss: 418.50
 ---- batch: 070 ----
mean loss: 386.59
 ---- batch: 080 ----
mean loss: 375.71
 ---- batch: 090 ----
mean loss: 378.73
train mean loss: 385.99
epoch train time: 0:00:02.335963
elapsed time: 0:06:55.914440
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 21:08:10.020483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.65
 ---- batch: 020 ----
mean loss: 374.73
 ---- batch: 030 ----
mean loss: 375.84
 ---- batch: 040 ----
mean loss: 383.77
 ---- batch: 050 ----
mean loss: 388.65
 ---- batch: 060 ----
mean loss: 374.88
 ---- batch: 070 ----
mean loss: 382.54
 ---- batch: 080 ----
mean loss: 389.25
 ---- batch: 090 ----
mean loss: 381.04
train mean loss: 382.01
epoch train time: 0:00:02.337369
elapsed time: 0:06:58.252006
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 21:08:12.358047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.05
 ---- batch: 020 ----
mean loss: 388.49
 ---- batch: 030 ----
mean loss: 378.99
 ---- batch: 040 ----
mean loss: 378.25
 ---- batch: 050 ----
mean loss: 377.99
 ---- batch: 060 ----
mean loss: 388.48
 ---- batch: 070 ----
mean loss: 387.61
 ---- batch: 080 ----
mean loss: 367.57
 ---- batch: 090 ----
mean loss: 384.67
train mean loss: 381.64
epoch train time: 0:00:02.328630
elapsed time: 0:07:00.580825
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 21:08:14.686899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.78
 ---- batch: 020 ----
mean loss: 382.51
 ---- batch: 030 ----
mean loss: 368.31
 ---- batch: 040 ----
mean loss: 379.84
 ---- batch: 050 ----
mean loss: 398.89
 ---- batch: 060 ----
mean loss: 393.63
 ---- batch: 070 ----
mean loss: 385.91
 ---- batch: 080 ----
mean loss: 385.31
 ---- batch: 090 ----
mean loss: 399.21
train mean loss: 386.20
epoch train time: 0:00:02.333841
elapsed time: 0:07:02.914882
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 21:08:17.020925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.51
 ---- batch: 020 ----
mean loss: 364.89
 ---- batch: 030 ----
mean loss: 382.55
 ---- batch: 040 ----
mean loss: 363.65
 ---- batch: 050 ----
mean loss: 392.42
 ---- batch: 060 ----
mean loss: 394.59
 ---- batch: 070 ----
mean loss: 387.24
 ---- batch: 080 ----
mean loss: 383.03
 ---- batch: 090 ----
mean loss: 393.04
train mean loss: 382.54
epoch train time: 0:00:02.329986
elapsed time: 0:07:05.245049
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 21:08:19.351092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.12
 ---- batch: 020 ----
mean loss: 372.54
 ---- batch: 030 ----
mean loss: 372.90
 ---- batch: 040 ----
mean loss: 378.34
 ---- batch: 050 ----
mean loss: 380.16
 ---- batch: 060 ----
mean loss: 391.67
 ---- batch: 070 ----
mean loss: 379.07
 ---- batch: 080 ----
mean loss: 385.61
 ---- batch: 090 ----
mean loss: 376.39
train mean loss: 380.99
epoch train time: 0:00:02.333588
elapsed time: 0:07:07.578826
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 21:08:21.684877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.99
 ---- batch: 020 ----
mean loss: 373.43
 ---- batch: 030 ----
mean loss: 378.28
 ---- batch: 040 ----
mean loss: 370.62
 ---- batch: 050 ----
mean loss: 398.62
 ---- batch: 060 ----
mean loss: 387.67
 ---- batch: 070 ----
mean loss: 382.47
 ---- batch: 080 ----
mean loss: 376.99
 ---- batch: 090 ----
mean loss: 363.98
train mean loss: 379.88
epoch train time: 0:00:02.332302
elapsed time: 0:07:09.911341
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 21:08:24.017442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.95
 ---- batch: 020 ----
mean loss: 405.27
 ---- batch: 030 ----
mean loss: 392.39
 ---- batch: 040 ----
mean loss: 393.14
 ---- batch: 050 ----
mean loss: 387.52
 ---- batch: 060 ----
mean loss: 373.79
 ---- batch: 070 ----
mean loss: 377.99
 ---- batch: 080 ----
mean loss: 375.84
 ---- batch: 090 ----
mean loss: 376.81
train mean loss: 387.27
epoch train time: 0:00:02.329210
elapsed time: 0:07:12.240785
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 21:08:26.346831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.56
 ---- batch: 020 ----
mean loss: 408.05
 ---- batch: 030 ----
mean loss: 390.09
 ---- batch: 040 ----
mean loss: 394.85
 ---- batch: 050 ----
mean loss: 389.80
 ---- batch: 060 ----
mean loss: 376.41
 ---- batch: 070 ----
mean loss: 382.88
 ---- batch: 080 ----
mean loss: 375.99
 ---- batch: 090 ----
mean loss: 390.42
train mean loss: 388.93
epoch train time: 0:00:02.326310
elapsed time: 0:07:14.567273
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 21:08:28.673315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.76
 ---- batch: 020 ----
mean loss: 378.82
 ---- batch: 030 ----
mean loss: 366.93
 ---- batch: 040 ----
mean loss: 389.44
 ---- batch: 050 ----
mean loss: 376.94
 ---- batch: 060 ----
mean loss: 368.04
 ---- batch: 070 ----
mean loss: 381.73
 ---- batch: 080 ----
mean loss: 385.40
 ---- batch: 090 ----
mean loss: 386.31
train mean loss: 379.29
epoch train time: 0:00:02.333248
elapsed time: 0:07:16.900697
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 21:08:31.006760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.23
 ---- batch: 020 ----
mean loss: 368.75
 ---- batch: 030 ----
mean loss: 374.00
 ---- batch: 040 ----
mean loss: 370.76
 ---- batch: 050 ----
mean loss: 377.86
 ---- batch: 060 ----
mean loss: 409.30
 ---- batch: 070 ----
mean loss: 391.26
 ---- batch: 080 ----
mean loss: 401.65
 ---- batch: 090 ----
mean loss: 378.20
train mean loss: 384.24
epoch train time: 0:00:02.339672
elapsed time: 0:07:19.240596
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 21:08:33.346623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.30
 ---- batch: 020 ----
mean loss: 375.66
 ---- batch: 030 ----
mean loss: 394.77
 ---- batch: 040 ----
mean loss: 375.14
 ---- batch: 050 ----
mean loss: 374.43
 ---- batch: 060 ----
mean loss: 393.19
 ---- batch: 070 ----
mean loss: 373.12
 ---- batch: 080 ----
mean loss: 387.35
 ---- batch: 090 ----
mean loss: 376.71
train mean loss: 379.20
epoch train time: 0:00:02.329262
elapsed time: 0:07:21.570034
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 21:08:35.676075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.62
 ---- batch: 020 ----
mean loss: 381.43
 ---- batch: 030 ----
mean loss: 387.03
 ---- batch: 040 ----
mean loss: 383.71
 ---- batch: 050 ----
mean loss: 372.71
 ---- batch: 060 ----
mean loss: 386.20
 ---- batch: 070 ----
mean loss: 379.10
 ---- batch: 080 ----
mean loss: 378.32
 ---- batch: 090 ----
mean loss: 387.54
train mean loss: 381.82
epoch train time: 0:00:02.331973
elapsed time: 0:07:23.902197
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 21:08:38.008235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.60
 ---- batch: 020 ----
mean loss: 381.37
 ---- batch: 030 ----
mean loss: 389.79
 ---- batch: 040 ----
mean loss: 382.47
 ---- batch: 050 ----
mean loss: 378.81
 ---- batch: 060 ----
mean loss: 371.39
 ---- batch: 070 ----
mean loss: 378.44
 ---- batch: 080 ----
mean loss: 365.53
 ---- batch: 090 ----
mean loss: 391.53
train mean loss: 379.54
epoch train time: 0:00:02.330631
elapsed time: 0:07:26.233001
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 21:08:40.339043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.15
 ---- batch: 020 ----
mean loss: 398.54
 ---- batch: 030 ----
mean loss: 385.04
 ---- batch: 040 ----
mean loss: 372.27
 ---- batch: 050 ----
mean loss: 381.49
 ---- batch: 060 ----
mean loss: 381.07
 ---- batch: 070 ----
mean loss: 385.89
 ---- batch: 080 ----
mean loss: 383.80
 ---- batch: 090 ----
mean loss: 371.44
train mean loss: 383.05
epoch train time: 0:00:02.337576
elapsed time: 0:07:28.570807
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 21:08:42.676855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.28
 ---- batch: 020 ----
mean loss: 379.91
 ---- batch: 030 ----
mean loss: 380.83
 ---- batch: 040 ----
mean loss: 373.30
 ---- batch: 050 ----
mean loss: 375.12
 ---- batch: 060 ----
mean loss: 373.56
 ---- batch: 070 ----
mean loss: 375.95
 ---- batch: 080 ----
mean loss: 391.02
 ---- batch: 090 ----
mean loss: 372.08
train mean loss: 377.70
epoch train time: 0:00:02.335898
elapsed time: 0:07:30.906890
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 21:08:45.012929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.13
 ---- batch: 020 ----
mean loss: 377.64
 ---- batch: 030 ----
mean loss: 389.25
 ---- batch: 040 ----
mean loss: 375.29
 ---- batch: 050 ----
mean loss: 379.78
 ---- batch: 060 ----
mean loss: 380.54
 ---- batch: 070 ----
mean loss: 362.65
 ---- batch: 080 ----
mean loss: 386.92
 ---- batch: 090 ----
mean loss: 380.48
train mean loss: 380.29
epoch train time: 0:00:02.336482
elapsed time: 0:07:33.243582
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 21:08:47.349643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.52
 ---- batch: 020 ----
mean loss: 363.25
 ---- batch: 030 ----
mean loss: 379.26
 ---- batch: 040 ----
mean loss: 377.63
 ---- batch: 050 ----
mean loss: 378.96
 ---- batch: 060 ----
mean loss: 365.08
 ---- batch: 070 ----
mean loss: 376.57
 ---- batch: 080 ----
mean loss: 374.51
 ---- batch: 090 ----
mean loss: 380.68
train mean loss: 375.32
epoch train time: 0:00:02.335207
elapsed time: 0:07:35.578999
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 21:08:49.685062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.86
 ---- batch: 020 ----
mean loss: 383.47
 ---- batch: 030 ----
mean loss: 380.88
 ---- batch: 040 ----
mean loss: 371.37
 ---- batch: 050 ----
mean loss: 363.93
 ---- batch: 060 ----
mean loss: 377.42
 ---- batch: 070 ----
mean loss: 383.84
 ---- batch: 080 ----
mean loss: 375.86
 ---- batch: 090 ----
mean loss: 382.23
train mean loss: 379.37
epoch train time: 0:00:02.332523
elapsed time: 0:07:37.911733
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 21:08:52.017787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.77
 ---- batch: 020 ----
mean loss: 371.88
 ---- batch: 030 ----
mean loss: 391.32
 ---- batch: 040 ----
mean loss: 380.17
 ---- batch: 050 ----
mean loss: 370.11
 ---- batch: 060 ----
mean loss: 387.87
 ---- batch: 070 ----
mean loss: 380.94
 ---- batch: 080 ----
mean loss: 371.09
 ---- batch: 090 ----
mean loss: 373.03
train mean loss: 378.18
epoch train time: 0:00:02.331001
elapsed time: 0:07:40.242945
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 21:08:54.349018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.76
 ---- batch: 020 ----
mean loss: 381.45
 ---- batch: 030 ----
mean loss: 376.99
 ---- batch: 040 ----
mean loss: 387.61
 ---- batch: 050 ----
mean loss: 384.47
 ---- batch: 060 ----
mean loss: 384.41
 ---- batch: 070 ----
mean loss: 380.34
 ---- batch: 080 ----
mean loss: 382.76
 ---- batch: 090 ----
mean loss: 378.08
train mean loss: 380.53
epoch train time: 0:00:02.329866
elapsed time: 0:07:42.573066
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 21:08:56.679109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.94
 ---- batch: 020 ----
mean loss: 376.75
 ---- batch: 030 ----
mean loss: 377.11
 ---- batch: 040 ----
mean loss: 386.57
 ---- batch: 050 ----
mean loss: 386.72
 ---- batch: 060 ----
mean loss: 388.91
 ---- batch: 070 ----
mean loss: 373.67
 ---- batch: 080 ----
mean loss: 372.79
 ---- batch: 090 ----
mean loss: 379.77
train mean loss: 380.33
epoch train time: 0:00:02.332958
elapsed time: 0:07:44.906206
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 21:08:59.012252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.61
 ---- batch: 020 ----
mean loss: 365.19
 ---- batch: 030 ----
mean loss: 364.21
 ---- batch: 040 ----
mean loss: 387.70
 ---- batch: 050 ----
mean loss: 373.89
 ---- batch: 060 ----
mean loss: 372.48
 ---- batch: 070 ----
mean loss: 378.88
 ---- batch: 080 ----
mean loss: 376.19
 ---- batch: 090 ----
mean loss: 386.02
train mean loss: 374.26
epoch train time: 0:00:02.333918
elapsed time: 0:07:47.240306
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 21:09:01.346349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.18
 ---- batch: 020 ----
mean loss: 377.74
 ---- batch: 030 ----
mean loss: 367.38
 ---- batch: 040 ----
mean loss: 375.57
 ---- batch: 050 ----
mean loss: 366.37
 ---- batch: 060 ----
mean loss: 378.81
 ---- batch: 070 ----
mean loss: 389.22
 ---- batch: 080 ----
mean loss: 386.87
 ---- batch: 090 ----
mean loss: 390.97
train mean loss: 380.67
epoch train time: 0:00:02.329974
elapsed time: 0:07:49.570458
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 21:09:03.676500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.51
 ---- batch: 020 ----
mean loss: 382.00
 ---- batch: 030 ----
mean loss: 361.88
 ---- batch: 040 ----
mean loss: 365.18
 ---- batch: 050 ----
mean loss: 368.89
 ---- batch: 060 ----
mean loss: 380.12
 ---- batch: 070 ----
mean loss: 374.82
 ---- batch: 080 ----
mean loss: 375.66
 ---- batch: 090 ----
mean loss: 371.95
train mean loss: 376.74
epoch train time: 0:00:02.342670
elapsed time: 0:07:51.913343
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 21:09:06.019409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.89
 ---- batch: 020 ----
mean loss: 388.29
 ---- batch: 030 ----
mean loss: 363.55
 ---- batch: 040 ----
mean loss: 378.06
 ---- batch: 050 ----
mean loss: 377.03
 ---- batch: 060 ----
mean loss: 372.78
 ---- batch: 070 ----
mean loss: 384.44
 ---- batch: 080 ----
mean loss: 377.66
 ---- batch: 090 ----
mean loss: 368.08
train mean loss: 376.11
epoch train time: 0:00:02.341258
elapsed time: 0:07:54.254810
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 21:09:08.360873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.17
 ---- batch: 020 ----
mean loss: 373.36
 ---- batch: 030 ----
mean loss: 389.63
 ---- batch: 040 ----
mean loss: 367.25
 ---- batch: 050 ----
mean loss: 368.54
 ---- batch: 060 ----
mean loss: 361.51
 ---- batch: 070 ----
mean loss: 372.14
 ---- batch: 080 ----
mean loss: 387.57
 ---- batch: 090 ----
mean loss: 375.15
train mean loss: 374.27
epoch train time: 0:00:02.325562
elapsed time: 0:07:56.580581
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 21:09:10.686626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.97
 ---- batch: 020 ----
mean loss: 369.31
 ---- batch: 030 ----
mean loss: 384.01
 ---- batch: 040 ----
mean loss: 366.66
 ---- batch: 050 ----
mean loss: 372.73
 ---- batch: 060 ----
mean loss: 373.65
 ---- batch: 070 ----
mean loss: 381.92
 ---- batch: 080 ----
mean loss: 368.55
 ---- batch: 090 ----
mean loss: 359.96
train mean loss: 372.27
epoch train time: 0:00:02.330348
elapsed time: 0:07:58.911129
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 21:09:13.017170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.61
 ---- batch: 020 ----
mean loss: 390.17
 ---- batch: 030 ----
mean loss: 373.81
 ---- batch: 040 ----
mean loss: 367.20
 ---- batch: 050 ----
mean loss: 374.03
 ---- batch: 060 ----
mean loss: 375.04
 ---- batch: 070 ----
mean loss: 374.72
 ---- batch: 080 ----
mean loss: 360.56
 ---- batch: 090 ----
mean loss: 376.93
train mean loss: 373.56
epoch train time: 0:00:02.335835
elapsed time: 0:08:01.247141
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 21:09:15.353183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.92
 ---- batch: 020 ----
mean loss: 358.64
 ---- batch: 030 ----
mean loss: 375.94
 ---- batch: 040 ----
mean loss: 381.59
 ---- batch: 050 ----
mean loss: 375.85
 ---- batch: 060 ----
mean loss: 385.18
 ---- batch: 070 ----
mean loss: 373.09
 ---- batch: 080 ----
mean loss: 373.75
 ---- batch: 090 ----
mean loss: 372.74
train mean loss: 373.91
epoch train time: 0:00:02.333111
elapsed time: 0:08:03.580431
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 21:09:17.686474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.10
 ---- batch: 020 ----
mean loss: 370.88
 ---- batch: 030 ----
mean loss: 363.51
 ---- batch: 040 ----
mean loss: 363.51
 ---- batch: 050 ----
mean loss: 376.48
 ---- batch: 060 ----
mean loss: 362.20
 ---- batch: 070 ----
mean loss: 366.85
 ---- batch: 080 ----
mean loss: 379.44
 ---- batch: 090 ----
mean loss: 374.58
train mean loss: 371.37
epoch train time: 0:00:02.333396
elapsed time: 0:08:05.914003
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 21:09:20.020045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.13
 ---- batch: 020 ----
mean loss: 365.62
 ---- batch: 030 ----
mean loss: 377.53
 ---- batch: 040 ----
mean loss: 378.65
 ---- batch: 050 ----
mean loss: 376.56
 ---- batch: 060 ----
mean loss: 377.81
 ---- batch: 070 ----
mean loss: 377.23
 ---- batch: 080 ----
mean loss: 372.22
 ---- batch: 090 ----
mean loss: 377.78
train mean loss: 375.31
epoch train time: 0:00:02.344468
elapsed time: 0:08:08.258650
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 21:09:22.364692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.05
 ---- batch: 020 ----
mean loss: 370.13
 ---- batch: 030 ----
mean loss: 373.02
 ---- batch: 040 ----
mean loss: 363.19
 ---- batch: 050 ----
mean loss: 374.61
 ---- batch: 060 ----
mean loss: 391.52
 ---- batch: 070 ----
mean loss: 382.24
 ---- batch: 080 ----
mean loss: 369.83
 ---- batch: 090 ----
mean loss: 370.08
train mean loss: 373.78
epoch train time: 0:00:02.336705
elapsed time: 0:08:10.595541
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 21:09:24.701633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.14
 ---- batch: 020 ----
mean loss: 387.31
 ---- batch: 030 ----
mean loss: 376.61
 ---- batch: 040 ----
mean loss: 360.92
 ---- batch: 050 ----
mean loss: 373.40
 ---- batch: 060 ----
mean loss: 382.52
 ---- batch: 070 ----
mean loss: 392.11
 ---- batch: 080 ----
mean loss: 372.88
 ---- batch: 090 ----
mean loss: 380.32
train mean loss: 377.58
epoch train time: 0:00:02.345382
elapsed time: 0:08:12.941187
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 21:09:27.047256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.87
 ---- batch: 020 ----
mean loss: 372.64
 ---- batch: 030 ----
mean loss: 364.83
 ---- batch: 040 ----
mean loss: 375.98
 ---- batch: 050 ----
mean loss: 369.32
 ---- batch: 060 ----
mean loss: 381.40
 ---- batch: 070 ----
mean loss: 365.59
 ---- batch: 080 ----
mean loss: 375.72
 ---- batch: 090 ----
mean loss: 387.04
train mean loss: 372.13
epoch train time: 0:00:02.337526
elapsed time: 0:08:15.278926
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 21:09:29.384970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.54
 ---- batch: 020 ----
mean loss: 359.51
 ---- batch: 030 ----
mean loss: 363.99
 ---- batch: 040 ----
mean loss: 374.02
 ---- batch: 050 ----
mean loss: 378.34
 ---- batch: 060 ----
mean loss: 362.46
 ---- batch: 070 ----
mean loss: 371.08
 ---- batch: 080 ----
mean loss: 378.94
 ---- batch: 090 ----
mean loss: 363.66
train mean loss: 370.40
epoch train time: 0:00:02.332344
elapsed time: 0:08:17.611463
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 21:09:31.717509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.18
 ---- batch: 020 ----
mean loss: 370.05
 ---- batch: 030 ----
mean loss: 374.93
 ---- batch: 040 ----
mean loss: 373.90
 ---- batch: 050 ----
mean loss: 376.13
 ---- batch: 060 ----
mean loss: 371.31
 ---- batch: 070 ----
mean loss: 364.41
 ---- batch: 080 ----
mean loss: 371.16
 ---- batch: 090 ----
mean loss: 383.29
train mean loss: 374.93
epoch train time: 0:00:02.338309
elapsed time: 0:08:19.949964
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 21:09:34.056026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.13
 ---- batch: 020 ----
mean loss: 377.08
 ---- batch: 030 ----
mean loss: 372.46
 ---- batch: 040 ----
mean loss: 384.85
 ---- batch: 050 ----
mean loss: 372.96
 ---- batch: 060 ----
mean loss: 368.84
 ---- batch: 070 ----
mean loss: 379.71
 ---- batch: 080 ----
mean loss: 391.09
 ---- batch: 090 ----
mean loss: 414.67
train mean loss: 382.77
epoch train time: 0:00:02.335133
elapsed time: 0:08:22.285300
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 21:09:36.391347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 407.91
 ---- batch: 020 ----
mean loss: 379.48
 ---- batch: 030 ----
mean loss: 376.50
 ---- batch: 040 ----
mean loss: 380.30
 ---- batch: 050 ----
mean loss: 379.55
 ---- batch: 060 ----
mean loss: 355.82
 ---- batch: 070 ----
mean loss: 360.48
 ---- batch: 080 ----
mean loss: 362.80
 ---- batch: 090 ----
mean loss: 364.06
train mean loss: 374.30
epoch train time: 0:00:02.337726
elapsed time: 0:08:24.623246
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 21:09:38.729277
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.00
 ---- batch: 020 ----
mean loss: 360.54
 ---- batch: 030 ----
mean loss: 370.48
 ---- batch: 040 ----
mean loss: 354.51
 ---- batch: 050 ----
mean loss: 365.36
 ---- batch: 060 ----
mean loss: 367.44
 ---- batch: 070 ----
mean loss: 366.90
 ---- batch: 080 ----
mean loss: 352.97
 ---- batch: 090 ----
mean loss: 366.49
train mean loss: 363.90
epoch train time: 0:00:02.335622
elapsed time: 0:08:26.959038
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 21:09:41.065081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.01
 ---- batch: 020 ----
mean loss: 372.42
 ---- batch: 030 ----
mean loss: 364.14
 ---- batch: 040 ----
mean loss: 354.22
 ---- batch: 050 ----
mean loss: 377.36
 ---- batch: 060 ----
mean loss: 355.62
 ---- batch: 070 ----
mean loss: 370.32
 ---- batch: 080 ----
mean loss: 366.08
 ---- batch: 090 ----
mean loss: 365.71
train mean loss: 364.23
epoch train time: 0:00:02.332023
elapsed time: 0:08:29.291238
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 21:09:43.397307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.35
 ---- batch: 020 ----
mean loss: 367.50
 ---- batch: 030 ----
mean loss: 358.49
 ---- batch: 040 ----
mean loss: 368.22
 ---- batch: 050 ----
mean loss: 353.78
 ---- batch: 060 ----
mean loss: 367.70
 ---- batch: 070 ----
mean loss: 362.49
 ---- batch: 080 ----
mean loss: 373.17
 ---- batch: 090 ----
mean loss: 354.22
train mean loss: 362.81
epoch train time: 0:00:02.322907
elapsed time: 0:08:31.614352
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 21:09:45.720404
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.85
 ---- batch: 020 ----
mean loss: 371.55
 ---- batch: 030 ----
mean loss: 371.07
 ---- batch: 040 ----
mean loss: 360.83
 ---- batch: 050 ----
mean loss: 370.22
 ---- batch: 060 ----
mean loss: 350.62
 ---- batch: 070 ----
mean loss: 362.70
 ---- batch: 080 ----
mean loss: 372.26
 ---- batch: 090 ----
mean loss: 363.28
train mean loss: 363.98
epoch train time: 0:00:02.343664
elapsed time: 0:08:33.958208
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 21:09:48.064250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.78
 ---- batch: 020 ----
mean loss: 365.41
 ---- batch: 030 ----
mean loss: 353.40
 ---- batch: 040 ----
mean loss: 372.38
 ---- batch: 050 ----
mean loss: 360.72
 ---- batch: 060 ----
mean loss: 373.54
 ---- batch: 070 ----
mean loss: 368.90
 ---- batch: 080 ----
mean loss: 369.60
 ---- batch: 090 ----
mean loss: 361.09
train mean loss: 365.61
epoch train time: 0:00:02.337805
elapsed time: 0:08:36.296198
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 21:09:50.402251
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.06
 ---- batch: 020 ----
mean loss: 351.31
 ---- batch: 030 ----
mean loss: 365.74
 ---- batch: 040 ----
mean loss: 374.67
 ---- batch: 050 ----
mean loss: 365.23
 ---- batch: 060 ----
mean loss: 349.36
 ---- batch: 070 ----
mean loss: 368.83
 ---- batch: 080 ----
mean loss: 352.44
 ---- batch: 090 ----
mean loss: 357.13
train mean loss: 362.28
epoch train time: 0:00:02.327978
elapsed time: 0:08:38.624377
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 21:09:52.730435
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.42
 ---- batch: 020 ----
mean loss: 365.62
 ---- batch: 030 ----
mean loss: 355.19
 ---- batch: 040 ----
mean loss: 361.81
 ---- batch: 050 ----
mean loss: 365.43
 ---- batch: 060 ----
mean loss: 361.37
 ---- batch: 070 ----
mean loss: 368.20
 ---- batch: 080 ----
mean loss: 365.08
 ---- batch: 090 ----
mean loss: 365.76
train mean loss: 362.21
epoch train time: 0:00:02.335781
elapsed time: 0:08:40.960356
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 21:09:55.066397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.04
 ---- batch: 020 ----
mean loss: 351.36
 ---- batch: 030 ----
mean loss: 368.23
 ---- batch: 040 ----
mean loss: 363.78
 ---- batch: 050 ----
mean loss: 366.71
 ---- batch: 060 ----
mean loss: 369.34
 ---- batch: 070 ----
mean loss: 363.97
 ---- batch: 080 ----
mean loss: 366.38
 ---- batch: 090 ----
mean loss: 358.48
train mean loss: 362.11
epoch train time: 0:00:02.330309
elapsed time: 0:08:43.290891
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 21:09:57.396932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.86
 ---- batch: 020 ----
mean loss: 369.40
 ---- batch: 030 ----
mean loss: 360.66
 ---- batch: 040 ----
mean loss: 355.39
 ---- batch: 050 ----
mean loss: 352.13
 ---- batch: 060 ----
mean loss: 373.93
 ---- batch: 070 ----
mean loss: 367.08
 ---- batch: 080 ----
mean loss: 360.72
 ---- batch: 090 ----
mean loss: 358.71
train mean loss: 362.55
epoch train time: 0:00:02.327232
elapsed time: 0:08:45.618302
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 21:09:59.724361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 376.88
 ---- batch: 020 ----
mean loss: 366.25
 ---- batch: 030 ----
mean loss: 358.63
 ---- batch: 040 ----
mean loss: 350.36
 ---- batch: 050 ----
mean loss: 361.51
 ---- batch: 060 ----
mean loss: 369.68
 ---- batch: 070 ----
mean loss: 363.28
 ---- batch: 080 ----
mean loss: 354.62
 ---- batch: 090 ----
mean loss: 353.31
train mean loss: 361.70
epoch train time: 0:00:02.328491
elapsed time: 0:08:47.946997
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 21:10:02.053043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.93
 ---- batch: 020 ----
mean loss: 359.67
 ---- batch: 030 ----
mean loss: 355.26
 ---- batch: 040 ----
mean loss: 375.39
 ---- batch: 050 ----
mean loss: 357.92
 ---- batch: 060 ----
mean loss: 358.66
 ---- batch: 070 ----
mean loss: 359.17
 ---- batch: 080 ----
mean loss: 366.36
 ---- batch: 090 ----
mean loss: 351.96
train mean loss: 359.68
epoch train time: 0:00:02.330950
elapsed time: 0:08:50.278129
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 21:10:04.384168
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.14
 ---- batch: 020 ----
mean loss: 359.10
 ---- batch: 030 ----
mean loss: 358.44
 ---- batch: 040 ----
mean loss: 356.30
 ---- batch: 050 ----
mean loss: 366.26
 ---- batch: 060 ----
mean loss: 360.72
 ---- batch: 070 ----
mean loss: 349.22
 ---- batch: 080 ----
mean loss: 363.29
 ---- batch: 090 ----
mean loss: 355.46
train mean loss: 357.99
epoch train time: 0:00:02.335770
elapsed time: 0:08:52.614127
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 21:10:06.720170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.12
 ---- batch: 020 ----
mean loss: 354.16
 ---- batch: 030 ----
mean loss: 369.68
 ---- batch: 040 ----
mean loss: 372.03
 ---- batch: 050 ----
mean loss: 358.10
 ---- batch: 060 ----
mean loss: 360.61
 ---- batch: 070 ----
mean loss: 360.38
 ---- batch: 080 ----
mean loss: 357.61
 ---- batch: 090 ----
mean loss: 354.11
train mean loss: 360.60
epoch train time: 0:00:02.336817
elapsed time: 0:08:54.951128
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 21:10:09.057176
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.29
 ---- batch: 020 ----
mean loss: 367.38
 ---- batch: 030 ----
mean loss: 360.42
 ---- batch: 040 ----
mean loss: 354.88
 ---- batch: 050 ----
mean loss: 360.27
 ---- batch: 060 ----
mean loss: 364.48
 ---- batch: 070 ----
mean loss: 359.13
 ---- batch: 080 ----
mean loss: 353.21
 ---- batch: 090 ----
mean loss: 358.30
train mean loss: 359.97
epoch train time: 0:00:02.336301
elapsed time: 0:08:57.287656
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 21:10:11.393704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 375.01
 ---- batch: 020 ----
mean loss: 353.07
 ---- batch: 030 ----
mean loss: 363.39
 ---- batch: 040 ----
mean loss: 355.63
 ---- batch: 050 ----
mean loss: 348.77
 ---- batch: 060 ----
mean loss: 364.67
 ---- batch: 070 ----
mean loss: 360.04
 ---- batch: 080 ----
mean loss: 369.26
 ---- batch: 090 ----
mean loss: 352.69
train mean loss: 359.00
epoch train time: 0:00:02.324420
elapsed time: 0:08:59.612268
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 21:10:13.718309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.45
 ---- batch: 020 ----
mean loss: 353.93
 ---- batch: 030 ----
mean loss: 361.84
 ---- batch: 040 ----
mean loss: 348.10
 ---- batch: 050 ----
mean loss: 373.79
 ---- batch: 060 ----
mean loss: 348.96
 ---- batch: 070 ----
mean loss: 349.76
 ---- batch: 080 ----
mean loss: 370.90
 ---- batch: 090 ----
mean loss: 366.00
train mean loss: 359.38
epoch train time: 0:00:02.329923
elapsed time: 0:09:01.942371
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 21:10:16.048415
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.42
 ---- batch: 020 ----
mean loss: 364.56
 ---- batch: 030 ----
mean loss: 360.06
 ---- batch: 040 ----
mean loss: 353.61
 ---- batch: 050 ----
mean loss: 357.35
 ---- batch: 060 ----
mean loss: 360.20
 ---- batch: 070 ----
mean loss: 348.15
 ---- batch: 080 ----
mean loss: 354.17
 ---- batch: 090 ----
mean loss: 363.90
train mean loss: 357.99
epoch train time: 0:00:02.332679
elapsed time: 0:09:04.275260
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 21:10:18.381319
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.58
 ---- batch: 020 ----
mean loss: 360.06
 ---- batch: 030 ----
mean loss: 364.86
 ---- batch: 040 ----
mean loss: 366.67
 ---- batch: 050 ----
mean loss: 358.35
 ---- batch: 060 ----
mean loss: 355.44
 ---- batch: 070 ----
mean loss: 366.96
 ---- batch: 080 ----
mean loss: 376.72
 ---- batch: 090 ----
mean loss: 355.55
train mean loss: 363.53
epoch train time: 0:00:02.331300
elapsed time: 0:09:06.606764
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 21:10:20.712806
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.91
 ---- batch: 020 ----
mean loss: 353.46
 ---- batch: 030 ----
mean loss: 361.43
 ---- batch: 040 ----
mean loss: 357.06
 ---- batch: 050 ----
mean loss: 363.80
 ---- batch: 060 ----
mean loss: 363.04
 ---- batch: 070 ----
mean loss: 361.32
 ---- batch: 080 ----
mean loss: 361.21
 ---- batch: 090 ----
mean loss: 366.15
train mean loss: 360.56
epoch train time: 0:00:02.336651
elapsed time: 0:09:08.943602
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 21:10:23.049667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.71
 ---- batch: 020 ----
mean loss: 363.74
 ---- batch: 030 ----
mean loss: 356.35
 ---- batch: 040 ----
mean loss: 365.34
 ---- batch: 050 ----
mean loss: 368.12
 ---- batch: 060 ----
mean loss: 370.54
 ---- batch: 070 ----
mean loss: 357.02
 ---- batch: 080 ----
mean loss: 357.55
 ---- batch: 090 ----
mean loss: 364.18
train mean loss: 360.35
epoch train time: 0:00:02.342247
elapsed time: 0:09:11.286072
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 21:10:25.392143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.12
 ---- batch: 020 ----
mean loss: 357.69
 ---- batch: 030 ----
mean loss: 367.52
 ---- batch: 040 ----
mean loss: 357.78
 ---- batch: 050 ----
mean loss: 357.78
 ---- batch: 060 ----
mean loss: 358.41
 ---- batch: 070 ----
mean loss: 352.10
 ---- batch: 080 ----
mean loss: 368.95
 ---- batch: 090 ----
mean loss: 361.67
train mean loss: 361.05
epoch train time: 0:00:02.343417
elapsed time: 0:09:13.629712
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 21:10:27.735789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.97
 ---- batch: 020 ----
mean loss: 362.97
 ---- batch: 030 ----
mean loss: 355.07
 ---- batch: 040 ----
mean loss: 355.56
 ---- batch: 050 ----
mean loss: 363.57
 ---- batch: 060 ----
mean loss: 360.85
 ---- batch: 070 ----
mean loss: 370.75
 ---- batch: 080 ----
mean loss: 350.54
 ---- batch: 090 ----
mean loss: 371.92
train mean loss: 361.00
epoch train time: 0:00:02.334805
elapsed time: 0:09:15.964737
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 21:10:30.070778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.77
 ---- batch: 020 ----
mean loss: 362.76
 ---- batch: 030 ----
mean loss: 363.29
 ---- batch: 040 ----
mean loss: 360.94
 ---- batch: 050 ----
mean loss: 358.39
 ---- batch: 060 ----
mean loss: 362.78
 ---- batch: 070 ----
mean loss: 363.77
 ---- batch: 080 ----
mean loss: 362.94
 ---- batch: 090 ----
mean loss: 367.07
train mean loss: 360.46
epoch train time: 0:00:02.333266
elapsed time: 0:09:18.298185
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 21:10:32.404229
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.98
 ---- batch: 020 ----
mean loss: 352.76
 ---- batch: 030 ----
mean loss: 366.01
 ---- batch: 040 ----
mean loss: 370.22
 ---- batch: 050 ----
mean loss: 366.12
 ---- batch: 060 ----
mean loss: 360.74
 ---- batch: 070 ----
mean loss: 359.85
 ---- batch: 080 ----
mean loss: 346.96
 ---- batch: 090 ----
mean loss: 357.08
train mean loss: 361.21
epoch train time: 0:00:02.330817
elapsed time: 0:09:20.629205
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 21:10:34.735270
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.27
 ---- batch: 020 ----
mean loss: 364.97
 ---- batch: 030 ----
mean loss: 373.59
 ---- batch: 040 ----
mean loss: 350.33
 ---- batch: 050 ----
mean loss: 357.21
 ---- batch: 060 ----
mean loss: 362.13
 ---- batch: 070 ----
mean loss: 358.25
 ---- batch: 080 ----
mean loss: 361.83
 ---- batch: 090 ----
mean loss: 380.79
train mean loss: 362.17
epoch train time: 0:00:02.335008
elapsed time: 0:09:22.964434
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 21:10:37.070479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.19
 ---- batch: 020 ----
mean loss: 365.23
 ---- batch: 030 ----
mean loss: 344.45
 ---- batch: 040 ----
mean loss: 356.58
 ---- batch: 050 ----
mean loss: 355.55
 ---- batch: 060 ----
mean loss: 348.94
 ---- batch: 070 ----
mean loss: 356.45
 ---- batch: 080 ----
mean loss: 363.83
 ---- batch: 090 ----
mean loss: 358.32
train mean loss: 355.78
epoch train time: 0:00:02.335832
elapsed time: 0:09:25.300469
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 21:10:39.406512
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.38
 ---- batch: 020 ----
mean loss: 348.96
 ---- batch: 030 ----
mean loss: 370.79
 ---- batch: 040 ----
mean loss: 353.86
 ---- batch: 050 ----
mean loss: 357.62
 ---- batch: 060 ----
mean loss: 358.65
 ---- batch: 070 ----
mean loss: 357.69
 ---- batch: 080 ----
mean loss: 361.40
 ---- batch: 090 ----
mean loss: 358.08
train mean loss: 358.47
epoch train time: 0:00:02.332194
elapsed time: 0:09:27.632862
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 21:10:41.738921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.94
 ---- batch: 020 ----
mean loss: 360.44
 ---- batch: 030 ----
mean loss: 357.24
 ---- batch: 040 ----
mean loss: 366.72
 ---- batch: 050 ----
mean loss: 360.59
 ---- batch: 060 ----
mean loss: 352.62
 ---- batch: 070 ----
mean loss: 362.21
 ---- batch: 080 ----
mean loss: 361.30
 ---- batch: 090 ----
mean loss: 362.85
train mean loss: 359.95
epoch train time: 0:00:02.338797
elapsed time: 0:09:29.971861
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 21:10:44.077922
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.51
 ---- batch: 020 ----
mean loss: 361.94
 ---- batch: 030 ----
mean loss: 358.67
 ---- batch: 040 ----
mean loss: 359.73
 ---- batch: 050 ----
mean loss: 347.38
 ---- batch: 060 ----
mean loss: 355.00
 ---- batch: 070 ----
mean loss: 366.06
 ---- batch: 080 ----
mean loss: 357.63
 ---- batch: 090 ----
mean loss: 361.64
train mean loss: 358.59
epoch train time: 0:00:02.325265
elapsed time: 0:09:32.297347
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 21:10:46.403384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 361.16
 ---- batch: 020 ----
mean loss: 359.74
 ---- batch: 030 ----
mean loss: 365.35
 ---- batch: 040 ----
mean loss: 348.69
 ---- batch: 050 ----
mean loss: 363.93
 ---- batch: 060 ----
mean loss: 353.66
 ---- batch: 070 ----
mean loss: 361.25
 ---- batch: 080 ----
mean loss: 358.32
 ---- batch: 090 ----
mean loss: 354.82
train mean loss: 358.89
epoch train time: 0:00:02.329454
elapsed time: 0:09:34.626980
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 21:10:48.733025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.24
 ---- batch: 020 ----
mean loss: 362.23
 ---- batch: 030 ----
mean loss: 370.98
 ---- batch: 040 ----
mean loss: 347.01
 ---- batch: 050 ----
mean loss: 352.28
 ---- batch: 060 ----
mean loss: 366.84
 ---- batch: 070 ----
mean loss: 362.10
 ---- batch: 080 ----
mean loss: 368.51
 ---- batch: 090 ----
mean loss: 361.06
train mean loss: 362.26
epoch train time: 0:00:02.337096
elapsed time: 0:09:36.964263
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 21:10:51.070307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.13
 ---- batch: 020 ----
mean loss: 347.87
 ---- batch: 030 ----
mean loss: 360.07
 ---- batch: 040 ----
mean loss: 357.93
 ---- batch: 050 ----
mean loss: 355.51
 ---- batch: 060 ----
mean loss: 352.29
 ---- batch: 070 ----
mean loss: 363.64
 ---- batch: 080 ----
mean loss: 366.58
 ---- batch: 090 ----
mean loss: 358.26
train mean loss: 358.86
epoch train time: 0:00:02.335540
elapsed time: 0:09:39.300017
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 21:10:53.406072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.18
 ---- batch: 020 ----
mean loss: 359.96
 ---- batch: 030 ----
mean loss: 338.25
 ---- batch: 040 ----
mean loss: 350.67
 ---- batch: 050 ----
mean loss: 364.29
 ---- batch: 060 ----
mean loss: 356.39
 ---- batch: 070 ----
mean loss: 359.75
 ---- batch: 080 ----
mean loss: 350.35
 ---- batch: 090 ----
mean loss: 346.84
train mean loss: 353.12
epoch train time: 0:00:02.329476
elapsed time: 0:09:41.629699
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 21:10:55.735740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.53
 ---- batch: 020 ----
mean loss: 356.30
 ---- batch: 030 ----
mean loss: 360.63
 ---- batch: 040 ----
mean loss: 353.57
 ---- batch: 050 ----
mean loss: 359.07
 ---- batch: 060 ----
mean loss: 359.16
 ---- batch: 070 ----
mean loss: 355.35
 ---- batch: 080 ----
mean loss: 357.21
 ---- batch: 090 ----
mean loss: 360.49
train mean loss: 358.35
epoch train time: 0:00:02.325123
elapsed time: 0:09:43.955014
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 21:10:58.061060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.44
 ---- batch: 020 ----
mean loss: 360.87
 ---- batch: 030 ----
mean loss: 353.00
 ---- batch: 040 ----
mean loss: 365.55
 ---- batch: 050 ----
mean loss: 356.27
 ---- batch: 060 ----
mean loss: 350.70
 ---- batch: 070 ----
mean loss: 361.68
 ---- batch: 080 ----
mean loss: 358.79
 ---- batch: 090 ----
mean loss: 348.78
train mean loss: 355.49
epoch train time: 0:00:02.334032
elapsed time: 0:09:46.289276
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 21:11:00.395331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.99
 ---- batch: 020 ----
mean loss: 349.65
 ---- batch: 030 ----
mean loss: 358.78
 ---- batch: 040 ----
mean loss: 368.64
 ---- batch: 050 ----
mean loss: 359.43
 ---- batch: 060 ----
mean loss: 348.45
 ---- batch: 070 ----
mean loss: 361.03
 ---- batch: 080 ----
mean loss: 366.99
 ---- batch: 090 ----
mean loss: 357.57
train mean loss: 359.76
epoch train time: 0:00:02.345090
elapsed time: 0:09:48.634558
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 21:11:02.740618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 377.58
 ---- batch: 020 ----
mean loss: 354.50
 ---- batch: 030 ----
mean loss: 364.07
 ---- batch: 040 ----
mean loss: 355.52
 ---- batch: 050 ----
mean loss: 364.09
 ---- batch: 060 ----
mean loss: 357.26
 ---- batch: 070 ----
mean loss: 362.18
 ---- batch: 080 ----
mean loss: 358.16
 ---- batch: 090 ----
mean loss: 357.92
train mean loss: 361.68
epoch train time: 0:00:02.336858
elapsed time: 0:09:50.971617
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 21:11:05.077680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.98
 ---- batch: 020 ----
mean loss: 351.41
 ---- batch: 030 ----
mean loss: 354.68
 ---- batch: 040 ----
mean loss: 352.86
 ---- batch: 050 ----
mean loss: 346.70
 ---- batch: 060 ----
mean loss: 361.21
 ---- batch: 070 ----
mean loss: 362.59
 ---- batch: 080 ----
mean loss: 357.51
 ---- batch: 090 ----
mean loss: 367.64
train mean loss: 355.46
epoch train time: 0:00:02.333323
elapsed time: 0:09:53.305168
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 21:11:07.411209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.57
 ---- batch: 020 ----
mean loss: 362.51
 ---- batch: 030 ----
mean loss: 362.71
 ---- batch: 040 ----
mean loss: 361.51
 ---- batch: 050 ----
mean loss: 361.85
 ---- batch: 060 ----
mean loss: 371.10
 ---- batch: 070 ----
mean loss: 360.06
 ---- batch: 080 ----
mean loss: 362.87
 ---- batch: 090 ----
mean loss: 347.36
train mean loss: 360.64
epoch train time: 0:00:02.322037
elapsed time: 0:09:55.627398
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 21:11:09.733455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.49
 ---- batch: 020 ----
mean loss: 354.01
 ---- batch: 030 ----
mean loss: 360.34
 ---- batch: 040 ----
mean loss: 359.89
 ---- batch: 050 ----
mean loss: 353.13
 ---- batch: 060 ----
mean loss: 346.10
 ---- batch: 070 ----
mean loss: 351.35
 ---- batch: 080 ----
mean loss: 357.18
 ---- batch: 090 ----
mean loss: 373.11
train mean loss: 358.37
epoch train time: 0:00:02.334223
elapsed time: 0:09:57.961835
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 21:11:12.067875
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.21
 ---- batch: 020 ----
mean loss: 343.86
 ---- batch: 030 ----
mean loss: 370.65
 ---- batch: 040 ----
mean loss: 349.23
 ---- batch: 050 ----
mean loss: 356.72
 ---- batch: 060 ----
mean loss: 358.27
 ---- batch: 070 ----
mean loss: 355.37
 ---- batch: 080 ----
mean loss: 354.18
 ---- batch: 090 ----
mean loss: 359.25
train mean loss: 356.54
epoch train time: 0:00:02.335336
elapsed time: 0:10:00.297357
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 21:11:14.403401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.80
 ---- batch: 020 ----
mean loss: 351.46
 ---- batch: 030 ----
mean loss: 356.92
 ---- batch: 040 ----
mean loss: 353.62
 ---- batch: 050 ----
mean loss: 354.85
 ---- batch: 060 ----
mean loss: 361.94
 ---- batch: 070 ----
mean loss: 360.04
 ---- batch: 080 ----
mean loss: 358.64
 ---- batch: 090 ----
mean loss: 363.92
train mean loss: 356.56
epoch train time: 0:00:02.326614
elapsed time: 0:10:02.624144
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 21:11:16.730184
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 361.49
 ---- batch: 020 ----
mean loss: 357.28
 ---- batch: 030 ----
mean loss: 349.89
 ---- batch: 040 ----
mean loss: 346.72
 ---- batch: 050 ----
mean loss: 362.33
 ---- batch: 060 ----
mean loss: 355.13
 ---- batch: 070 ----
mean loss: 360.04
 ---- batch: 080 ----
mean loss: 349.37
 ---- batch: 090 ----
mean loss: 367.39
train mean loss: 357.00
epoch train time: 0:00:02.337215
elapsed time: 0:10:04.961533
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 21:11:19.067578
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.89
 ---- batch: 020 ----
mean loss: 352.39
 ---- batch: 030 ----
mean loss: 351.13
 ---- batch: 040 ----
mean loss: 358.10
 ---- batch: 050 ----
mean loss: 356.92
 ---- batch: 060 ----
mean loss: 349.92
 ---- batch: 070 ----
mean loss: 359.62
 ---- batch: 080 ----
mean loss: 369.72
 ---- batch: 090 ----
mean loss: 359.13
train mean loss: 357.19
epoch train time: 0:00:02.337415
elapsed time: 0:10:07.299120
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 21:11:21.405159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.91
 ---- batch: 020 ----
mean loss: 358.38
 ---- batch: 030 ----
mean loss: 353.95
 ---- batch: 040 ----
mean loss: 371.27
 ---- batch: 050 ----
mean loss: 364.92
 ---- batch: 060 ----
mean loss: 349.97
 ---- batch: 070 ----
mean loss: 364.44
 ---- batch: 080 ----
mean loss: 351.50
 ---- batch: 090 ----
mean loss: 355.64
train mean loss: 359.77
epoch train time: 0:00:02.332559
elapsed time: 0:10:09.631867
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 21:11:23.737928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.23
 ---- batch: 020 ----
mean loss: 357.89
 ---- batch: 030 ----
mean loss: 363.20
 ---- batch: 040 ----
mean loss: 360.77
 ---- batch: 050 ----
mean loss: 366.01
 ---- batch: 060 ----
mean loss: 358.74
 ---- batch: 070 ----
mean loss: 345.17
 ---- batch: 080 ----
mean loss: 362.64
 ---- batch: 090 ----
mean loss: 355.07
train mean loss: 358.61
epoch train time: 0:00:02.325029
elapsed time: 0:10:11.957119
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 21:11:26.063160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.98
 ---- batch: 020 ----
mean loss: 365.72
 ---- batch: 030 ----
mean loss: 353.78
 ---- batch: 040 ----
mean loss: 358.61
 ---- batch: 050 ----
mean loss: 345.45
 ---- batch: 060 ----
mean loss: 354.59
 ---- batch: 070 ----
mean loss: 359.30
 ---- batch: 080 ----
mean loss: 352.83
 ---- batch: 090 ----
mean loss: 341.52
train mean loss: 354.75
epoch train time: 0:00:02.336219
elapsed time: 0:10:14.293517
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 21:11:28.399597
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.61
 ---- batch: 020 ----
mean loss: 357.49
 ---- batch: 030 ----
mean loss: 352.92
 ---- batch: 040 ----
mean loss: 354.94
 ---- batch: 050 ----
mean loss: 354.98
 ---- batch: 060 ----
mean loss: 366.39
 ---- batch: 070 ----
mean loss: 350.81
 ---- batch: 080 ----
mean loss: 354.99
 ---- batch: 090 ----
mean loss: 364.64
train mean loss: 357.07
epoch train time: 0:00:02.323404
elapsed time: 0:10:16.620637
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_2/checkpoint.pth.tar
**** end time: 2019-09-25 21:11:30.726635 ****
