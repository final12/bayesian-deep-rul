Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_4', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6981
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 22:37:51.862546 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 22:37:51.870525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4220.19
 ---- batch: 020 ----
mean loss: 1875.33
 ---- batch: 030 ----
mean loss: 524.73
 ---- batch: 040 ----
mean loss: 423.20
train mean loss: 1668.56
epoch train time: 0:00:15.568110
elapsed time: 0:00:15.578646
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 22:38:07.441235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.46
 ---- batch: 020 ----
mean loss: 351.51
 ---- batch: 030 ----
mean loss: 328.77
 ---- batch: 040 ----
mean loss: 318.62
train mean loss: 340.75
epoch train time: 0:00:01.820310
elapsed time: 0:00:17.399101
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 22:38:09.261721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.92
 ---- batch: 020 ----
mean loss: 310.37
 ---- batch: 030 ----
mean loss: 302.52
 ---- batch: 040 ----
mean loss: 302.79
train mean loss: 305.54
epoch train time: 0:00:01.675242
elapsed time: 0:00:19.074518
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 22:38:10.937115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.15
 ---- batch: 020 ----
mean loss: 278.26
 ---- batch: 030 ----
mean loss: 272.56
 ---- batch: 040 ----
mean loss: 265.36
train mean loss: 274.78
epoch train time: 0:00:01.764038
elapsed time: 0:00:20.838713
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 22:38:12.701318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.01
 ---- batch: 020 ----
mean loss: 252.74
 ---- batch: 030 ----
mean loss: 243.55
 ---- batch: 040 ----
mean loss: 235.29
train mean loss: 245.38
epoch train time: 0:00:01.655888
elapsed time: 0:00:22.494788
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 22:38:14.357389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.28
 ---- batch: 020 ----
mean loss: 222.97
 ---- batch: 030 ----
mean loss: 228.92
 ---- batch: 040 ----
mean loss: 225.55
train mean loss: 228.12
epoch train time: 0:00:01.765506
elapsed time: 0:00:24.260442
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 22:38:16.123055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.30
 ---- batch: 020 ----
mean loss: 225.11
 ---- batch: 030 ----
mean loss: 216.38
 ---- batch: 040 ----
mean loss: 212.45
train mean loss: 217.68
epoch train time: 0:00:01.657431
elapsed time: 0:00:25.918047
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 22:38:17.780644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.67
 ---- batch: 020 ----
mean loss: 215.80
 ---- batch: 030 ----
mean loss: 202.08
 ---- batch: 040 ----
mean loss: 199.60
train mean loss: 207.85
epoch train time: 0:00:01.657642
elapsed time: 0:00:27.575836
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 22:38:19.438433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.84
 ---- batch: 020 ----
mean loss: 212.54
 ---- batch: 030 ----
mean loss: 201.61
 ---- batch: 040 ----
mean loss: 214.24
train mean loss: 209.51
epoch train time: 0:00:01.771511
elapsed time: 0:00:29.347509
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 22:38:21.210106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.91
 ---- batch: 020 ----
mean loss: 203.76
 ---- batch: 030 ----
mean loss: 202.91
 ---- batch: 040 ----
mean loss: 209.56
train mean loss: 205.45
epoch train time: 0:00:01.662451
elapsed time: 0:00:31.010110
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 22:38:22.872705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.89
 ---- batch: 020 ----
mean loss: 206.69
 ---- batch: 030 ----
mean loss: 198.12
 ---- batch: 040 ----
mean loss: 203.84
train mean loss: 204.22
epoch train time: 0:00:01.769336
elapsed time: 0:00:32.779592
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 22:38:24.642190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.16
 ---- batch: 020 ----
mean loss: 206.11
 ---- batch: 030 ----
mean loss: 198.66
 ---- batch: 040 ----
mean loss: 200.19
train mean loss: 200.71
epoch train time: 0:00:01.663779
elapsed time: 0:00:34.443531
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 22:38:26.306131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.23
 ---- batch: 020 ----
mean loss: 199.86
 ---- batch: 030 ----
mean loss: 200.17
 ---- batch: 040 ----
mean loss: 202.89
train mean loss: 201.61
epoch train time: 0:00:01.777485
elapsed time: 0:00:36.221163
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 22:38:28.083759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.79
 ---- batch: 020 ----
mean loss: 199.57
 ---- batch: 030 ----
mean loss: 202.13
 ---- batch: 040 ----
mean loss: 199.85
train mean loss: 201.10
epoch train time: 0:00:01.659524
elapsed time: 0:00:37.880833
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 22:38:29.743430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.99
 ---- batch: 020 ----
mean loss: 202.98
 ---- batch: 030 ----
mean loss: 194.77
 ---- batch: 040 ----
mean loss: 203.05
train mean loss: 200.54
epoch train time: 0:00:01.763346
elapsed time: 0:00:39.644331
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 22:38:31.506926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.28
 ---- batch: 020 ----
mean loss: 197.51
 ---- batch: 030 ----
mean loss: 196.13
 ---- batch: 040 ----
mean loss: 198.98
train mean loss: 197.92
epoch train time: 0:00:01.676411
elapsed time: 0:00:41.320900
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 22:38:33.183498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.14
 ---- batch: 020 ----
mean loss: 201.47
 ---- batch: 030 ----
mean loss: 199.12
 ---- batch: 040 ----
mean loss: 196.79
train mean loss: 200.32
epoch train time: 0:00:01.662775
elapsed time: 0:00:42.983858
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 22:38:34.846457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.70
 ---- batch: 020 ----
mean loss: 199.71
 ---- batch: 030 ----
mean loss: 205.09
 ---- batch: 040 ----
mean loss: 197.32
train mean loss: 199.65
epoch train time: 0:00:01.790150
elapsed time: 0:00:44.774154
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 22:38:36.636774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.79
 ---- batch: 020 ----
mean loss: 201.27
 ---- batch: 030 ----
mean loss: 200.23
 ---- batch: 040 ----
mean loss: 187.53
train mean loss: 195.38
epoch train time: 0:00:01.675855
elapsed time: 0:00:46.450180
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 22:38:38.312775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.93
 ---- batch: 020 ----
mean loss: 201.21
 ---- batch: 030 ----
mean loss: 192.97
 ---- batch: 040 ----
mean loss: 200.09
train mean loss: 197.39
epoch train time: 0:00:01.777212
elapsed time: 0:00:48.227528
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 22:38:40.090122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.75
 ---- batch: 020 ----
mean loss: 198.85
 ---- batch: 030 ----
mean loss: 200.62
 ---- batch: 040 ----
mean loss: 194.93
train mean loss: 196.07
epoch train time: 0:00:01.680370
elapsed time: 0:00:49.908051
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 22:38:41.770648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.98
 ---- batch: 020 ----
mean loss: 204.62
 ---- batch: 030 ----
mean loss: 190.48
 ---- batch: 040 ----
mean loss: 191.49
train mean loss: 194.87
epoch train time: 0:00:01.722566
elapsed time: 0:00:51.630762
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 22:38:43.493357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.19
 ---- batch: 020 ----
mean loss: 200.66
 ---- batch: 030 ----
mean loss: 197.31
 ---- batch: 040 ----
mean loss: 192.52
train mean loss: 196.99
epoch train time: 0:00:01.725617
elapsed time: 0:00:53.356532
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 22:38:45.219127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.50
 ---- batch: 020 ----
mean loss: 190.82
 ---- batch: 030 ----
mean loss: 198.12
 ---- batch: 040 ----
mean loss: 199.39
train mean loss: 195.35
epoch train time: 0:00:01.675438
elapsed time: 0:00:55.032114
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 22:38:46.894709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.40
 ---- batch: 020 ----
mean loss: 197.45
 ---- batch: 030 ----
mean loss: 197.63
 ---- batch: 040 ----
mean loss: 196.49
train mean loss: 194.70
epoch train time: 0:00:01.713220
elapsed time: 0:00:56.745468
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 22:38:48.608092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.81
 ---- batch: 020 ----
mean loss: 194.17
 ---- batch: 030 ----
mean loss: 189.22
 ---- batch: 040 ----
mean loss: 201.92
train mean loss: 194.62
epoch train time: 0:00:01.672320
elapsed time: 0:00:58.417969
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 22:38:50.280571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.48
 ---- batch: 020 ----
mean loss: 191.11
 ---- batch: 030 ----
mean loss: 196.36
 ---- batch: 040 ----
mean loss: 192.49
train mean loss: 191.61
epoch train time: 0:00:01.771861
elapsed time: 0:01:00.189971
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 22:38:52.052562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.47
 ---- batch: 020 ----
mean loss: 199.29
 ---- batch: 030 ----
mean loss: 193.51
 ---- batch: 040 ----
mean loss: 190.78
train mean loss: 193.99
epoch train time: 0:00:01.680523
elapsed time: 0:01:01.870640
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 22:38:53.733234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.60
 ---- batch: 020 ----
mean loss: 191.49
 ---- batch: 030 ----
mean loss: 189.38
 ---- batch: 040 ----
mean loss: 203.29
train mean loss: 194.66
epoch train time: 0:00:01.674823
elapsed time: 0:01:03.545610
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 22:38:55.408202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.86
 ---- batch: 020 ----
mean loss: 204.23
 ---- batch: 030 ----
mean loss: 187.53
 ---- batch: 040 ----
mean loss: 187.28
train mean loss: 194.92
epoch train time: 0:00:01.777815
elapsed time: 0:01:05.323590
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 22:38:57.186204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.81
 ---- batch: 020 ----
mean loss: 188.17
 ---- batch: 030 ----
mean loss: 197.40
 ---- batch: 040 ----
mean loss: 188.41
train mean loss: 191.92
epoch train time: 0:00:01.680906
elapsed time: 0:01:07.004666
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 22:38:58.867258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.30
 ---- batch: 020 ----
mean loss: 188.28
 ---- batch: 030 ----
mean loss: 189.67
 ---- batch: 040 ----
mean loss: 187.66
train mean loss: 190.47
epoch train time: 0:00:01.790531
elapsed time: 0:01:08.795331
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 22:39:00.657926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.05
 ---- batch: 020 ----
mean loss: 183.89
 ---- batch: 030 ----
mean loss: 189.37
 ---- batch: 040 ----
mean loss: 189.35
train mean loss: 190.81
epoch train time: 0:00:01.671498
elapsed time: 0:01:10.466988
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 22:39:02.329586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.17
 ---- batch: 020 ----
mean loss: 194.23
 ---- batch: 030 ----
mean loss: 181.89
 ---- batch: 040 ----
mean loss: 192.46
train mean loss: 189.26
epoch train time: 0:00:01.780198
elapsed time: 0:01:12.247322
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 22:39:04.109927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.17
 ---- batch: 020 ----
mean loss: 188.70
 ---- batch: 030 ----
mean loss: 187.02
 ---- batch: 040 ----
mean loss: 188.47
train mean loss: 189.23
epoch train time: 0:00:01.670684
elapsed time: 0:01:13.918170
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 22:39:05.780768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.23
 ---- batch: 020 ----
mean loss: 187.27
 ---- batch: 030 ----
mean loss: 183.70
 ---- batch: 040 ----
mean loss: 188.30
train mean loss: 188.50
epoch train time: 0:00:01.777578
elapsed time: 0:01:15.695883
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 22:39:07.558472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.74
 ---- batch: 020 ----
mean loss: 194.89
 ---- batch: 030 ----
mean loss: 191.56
 ---- batch: 040 ----
mean loss: 191.99
train mean loss: 189.83
epoch train time: 0:00:01.671456
elapsed time: 0:01:17.367477
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 22:39:09.230073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.48
 ---- batch: 020 ----
mean loss: 183.60
 ---- batch: 030 ----
mean loss: 185.00
 ---- batch: 040 ----
mean loss: 187.60
train mean loss: 187.89
epoch train time: 0:00:01.767934
elapsed time: 0:01:19.135564
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 22:39:10.998157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.63
 ---- batch: 020 ----
mean loss: 187.65
 ---- batch: 030 ----
mean loss: 186.87
 ---- batch: 040 ----
mean loss: 191.54
train mean loss: 189.12
epoch train time: 0:00:01.679302
elapsed time: 0:01:20.815082
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 22:39:12.677766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.78
 ---- batch: 020 ----
mean loss: 187.55
 ---- batch: 030 ----
mean loss: 195.14
 ---- batch: 040 ----
mean loss: 185.96
train mean loss: 188.94
epoch train time: 0:00:01.664868
elapsed time: 0:01:22.480183
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 22:39:14.342777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.34
 ---- batch: 020 ----
mean loss: 187.49
 ---- batch: 030 ----
mean loss: 195.12
 ---- batch: 040 ----
mean loss: 195.35
train mean loss: 191.84
epoch train time: 0:00:01.793505
elapsed time: 0:01:24.273834
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 22:39:16.136476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.33
 ---- batch: 020 ----
mean loss: 187.80
 ---- batch: 030 ----
mean loss: 190.18
 ---- batch: 040 ----
mean loss: 183.94
train mean loss: 188.28
epoch train time: 0:00:01.664000
elapsed time: 0:01:25.938085
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 22:39:17.800687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.15
 ---- batch: 020 ----
mean loss: 187.54
 ---- batch: 030 ----
mean loss: 190.53
 ---- batch: 040 ----
mean loss: 185.11
train mean loss: 186.26
epoch train time: 0:00:01.708832
elapsed time: 0:01:27.647066
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 22:39:19.509660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.24
 ---- batch: 020 ----
mean loss: 187.87
 ---- batch: 030 ----
mean loss: 183.21
 ---- batch: 040 ----
mean loss: 181.95
train mean loss: 186.22
epoch train time: 0:00:01.674692
elapsed time: 0:01:29.321932
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 22:39:21.184545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.74
 ---- batch: 020 ----
mean loss: 186.49
 ---- batch: 030 ----
mean loss: 179.62
 ---- batch: 040 ----
mean loss: 179.90
train mean loss: 182.63
epoch train time: 0:00:01.781150
elapsed time: 0:01:31.103241
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 22:39:22.965835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.51
 ---- batch: 020 ----
mean loss: 183.97
 ---- batch: 030 ----
mean loss: 188.45
 ---- batch: 040 ----
mean loss: 189.49
train mean loss: 185.68
epoch train time: 0:00:01.662123
elapsed time: 0:01:32.765509
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 22:39:24.628105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.64
 ---- batch: 020 ----
mean loss: 184.93
 ---- batch: 030 ----
mean loss: 183.38
 ---- batch: 040 ----
mean loss: 185.14
train mean loss: 184.83
epoch train time: 0:00:01.763852
elapsed time: 0:01:34.529547
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 22:39:26.392142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.74
 ---- batch: 020 ----
mean loss: 175.29
 ---- batch: 030 ----
mean loss: 191.72
 ---- batch: 040 ----
mean loss: 181.51
train mean loss: 183.15
epoch train time: 0:00:01.689006
elapsed time: 0:01:36.218745
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 22:39:28.081343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.50
 ---- batch: 020 ----
mean loss: 181.54
 ---- batch: 030 ----
mean loss: 183.94
 ---- batch: 040 ----
mean loss: 190.58
train mean loss: 184.45
epoch train time: 0:00:01.677022
elapsed time: 0:01:37.895916
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 22:39:29.758512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.64
 ---- batch: 020 ----
mean loss: 181.57
 ---- batch: 030 ----
mean loss: 175.41
 ---- batch: 040 ----
mean loss: 183.46
train mean loss: 181.43
epoch train time: 0:00:01.796775
elapsed time: 0:01:39.692827
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 22:39:31.555422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.95
 ---- batch: 020 ----
mean loss: 188.96
 ---- batch: 030 ----
mean loss: 182.58
 ---- batch: 040 ----
mean loss: 182.70
train mean loss: 185.53
epoch train time: 0:00:01.684706
elapsed time: 0:01:41.377681
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 22:39:33.240278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.52
 ---- batch: 020 ----
mean loss: 180.74
 ---- batch: 030 ----
mean loss: 186.88
 ---- batch: 040 ----
mean loss: 176.60
train mean loss: 182.09
epoch train time: 0:00:01.778343
elapsed time: 0:01:43.156165
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 22:39:35.018769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.96
 ---- batch: 020 ----
mean loss: 178.63
 ---- batch: 030 ----
mean loss: 184.71
 ---- batch: 040 ----
mean loss: 178.68
train mean loss: 182.19
epoch train time: 0:00:01.678219
elapsed time: 0:01:44.834537
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 22:39:36.697145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.72
 ---- batch: 020 ----
mean loss: 177.77
 ---- batch: 030 ----
mean loss: 182.66
 ---- batch: 040 ----
mean loss: 184.39
train mean loss: 180.47
epoch train time: 0:00:01.665522
elapsed time: 0:01:46.500218
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 22:39:38.362812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.62
 ---- batch: 020 ----
mean loss: 185.86
 ---- batch: 030 ----
mean loss: 183.50
 ---- batch: 040 ----
mean loss: 174.97
train mean loss: 183.00
epoch train time: 0:00:01.790120
elapsed time: 0:01:48.290478
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 22:39:40.153090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.45
 ---- batch: 020 ----
mean loss: 181.16
 ---- batch: 030 ----
mean loss: 185.29
 ---- batch: 040 ----
mean loss: 177.78
train mean loss: 183.69
epoch train time: 0:00:01.674574
elapsed time: 0:01:49.965228
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 22:39:41.827825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.92
 ---- batch: 020 ----
mean loss: 180.09
 ---- batch: 030 ----
mean loss: 176.90
 ---- batch: 040 ----
mean loss: 181.23
train mean loss: 180.92
epoch train time: 0:00:01.788947
elapsed time: 0:01:51.754316
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 22:39:43.616910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.77
 ---- batch: 020 ----
mean loss: 173.85
 ---- batch: 030 ----
mean loss: 183.66
 ---- batch: 040 ----
mean loss: 180.21
train mean loss: 177.72
epoch train time: 0:00:01.681214
elapsed time: 0:01:53.435675
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 22:39:45.298279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.18
 ---- batch: 020 ----
mean loss: 171.35
 ---- batch: 030 ----
mean loss: 178.73
 ---- batch: 040 ----
mean loss: 179.44
train mean loss: 175.93
epoch train time: 0:00:01.784694
elapsed time: 0:01:55.220547
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 22:39:47.083140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.92
 ---- batch: 020 ----
mean loss: 181.94
 ---- batch: 030 ----
mean loss: 174.35
 ---- batch: 040 ----
mean loss: 176.95
train mean loss: 178.18
epoch train time: 0:00:01.682551
elapsed time: 0:01:56.903251
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 22:39:48.765857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.22
 ---- batch: 020 ----
mean loss: 178.40
 ---- batch: 030 ----
mean loss: 182.31
 ---- batch: 040 ----
mean loss: 188.98
train mean loss: 180.24
epoch train time: 0:00:01.779884
elapsed time: 0:01:58.683285
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 22:39:50.545881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.42
 ---- batch: 020 ----
mean loss: 181.54
 ---- batch: 030 ----
mean loss: 169.64
 ---- batch: 040 ----
mean loss: 183.86
train mean loss: 176.76
epoch train time: 0:00:01.686585
elapsed time: 0:02:00.370029
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 22:39:52.232622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.87
 ---- batch: 020 ----
mean loss: 171.44
 ---- batch: 030 ----
mean loss: 174.19
 ---- batch: 040 ----
mean loss: 175.64
train mean loss: 173.89
epoch train time: 0:00:01.782187
elapsed time: 0:02:02.152350
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 22:39:54.014958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.43
 ---- batch: 020 ----
mean loss: 177.57
 ---- batch: 030 ----
mean loss: 176.09
 ---- batch: 040 ----
mean loss: 175.07
train mean loss: 175.46
epoch train time: 0:00:01.688605
elapsed time: 0:02:03.841121
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 22:39:55.703718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.54
 ---- batch: 020 ----
mean loss: 177.85
 ---- batch: 030 ----
mean loss: 179.04
 ---- batch: 040 ----
mean loss: 173.02
train mean loss: 175.42
epoch train time: 0:00:01.777283
elapsed time: 0:02:05.618535
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 22:39:57.481139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.25
 ---- batch: 020 ----
mean loss: 174.55
 ---- batch: 030 ----
mean loss: 175.49
 ---- batch: 040 ----
mean loss: 173.02
train mean loss: 174.32
epoch train time: 0:00:01.689375
elapsed time: 0:02:07.308068
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 22:39:59.170669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.17
 ---- batch: 020 ----
mean loss: 167.53
 ---- batch: 030 ----
mean loss: 177.40
 ---- batch: 040 ----
mean loss: 172.95
train mean loss: 172.77
epoch train time: 0:00:01.679149
elapsed time: 0:02:08.987364
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 22:40:00.849960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.51
 ---- batch: 020 ----
mean loss: 172.63
 ---- batch: 030 ----
mean loss: 173.07
 ---- batch: 040 ----
mean loss: 166.71
train mean loss: 171.38
epoch train time: 0:00:01.794015
elapsed time: 0:02:10.781535
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 22:40:02.644131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.85
 ---- batch: 020 ----
mean loss: 159.04
 ---- batch: 030 ----
mean loss: 176.72
 ---- batch: 040 ----
mean loss: 171.28
train mean loss: 169.52
epoch train time: 0:00:01.675861
elapsed time: 0:02:12.457548
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 22:40:04.320144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.04
 ---- batch: 020 ----
mean loss: 175.25
 ---- batch: 030 ----
mean loss: 172.22
 ---- batch: 040 ----
mean loss: 173.97
train mean loss: 174.56
epoch train time: 0:00:01.779406
elapsed time: 0:02:14.237102
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 22:40:06.099696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.51
 ---- batch: 020 ----
mean loss: 172.35
 ---- batch: 030 ----
mean loss: 176.85
 ---- batch: 040 ----
mean loss: 170.06
train mean loss: 174.28
epoch train time: 0:00:01.682306
elapsed time: 0:02:15.919562
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 22:40:07.782159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.27
 ---- batch: 020 ----
mean loss: 175.03
 ---- batch: 030 ----
mean loss: 173.65
 ---- batch: 040 ----
mean loss: 163.41
train mean loss: 169.23
epoch train time: 0:00:01.777384
elapsed time: 0:02:17.697076
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 22:40:09.559668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.93
 ---- batch: 020 ----
mean loss: 171.39
 ---- batch: 030 ----
mean loss: 174.87
 ---- batch: 040 ----
mean loss: 168.06
train mean loss: 169.85
epoch train time: 0:00:01.690483
elapsed time: 0:02:19.387719
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 22:40:11.250319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.07
 ---- batch: 020 ----
mean loss: 162.18
 ---- batch: 030 ----
mean loss: 166.96
 ---- batch: 040 ----
mean loss: 172.78
train mean loss: 167.31
epoch train time: 0:00:01.675306
elapsed time: 0:02:21.063182
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 22:40:12.925782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.35
 ---- batch: 020 ----
mean loss: 169.02
 ---- batch: 030 ----
mean loss: 165.75
 ---- batch: 040 ----
mean loss: 169.50
train mean loss: 169.46
epoch train time: 0:00:01.799355
elapsed time: 0:02:22.862682
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 22:40:14.725292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.41
 ---- batch: 020 ----
mean loss: 167.91
 ---- batch: 030 ----
mean loss: 168.84
 ---- batch: 040 ----
mean loss: 173.26
train mean loss: 169.76
epoch train time: 0:00:01.678559
elapsed time: 0:02:24.541410
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 22:40:16.404006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.58
 ---- batch: 020 ----
mean loss: 167.15
 ---- batch: 030 ----
mean loss: 165.92
 ---- batch: 040 ----
mean loss: 171.82
train mean loss: 169.37
epoch train time: 0:00:01.772688
elapsed time: 0:02:26.314296
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 22:40:18.176932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.68
 ---- batch: 020 ----
mean loss: 176.77
 ---- batch: 030 ----
mean loss: 171.13
 ---- batch: 040 ----
mean loss: 159.68
train mean loss: 169.20
epoch train time: 0:00:01.695904
elapsed time: 0:02:28.010404
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 22:40:19.873008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.07
 ---- batch: 020 ----
mean loss: 170.49
 ---- batch: 030 ----
mean loss: 167.43
 ---- batch: 040 ----
mean loss: 165.87
train mean loss: 169.82
epoch train time: 0:00:01.677249
elapsed time: 0:02:29.687805
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 22:40:21.550406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.40
 ---- batch: 020 ----
mean loss: 166.35
 ---- batch: 030 ----
mean loss: 171.31
 ---- batch: 040 ----
mean loss: 163.48
train mean loss: 168.65
epoch train time: 0:00:01.785244
elapsed time: 0:02:31.473208
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 22:40:23.335808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.41
 ---- batch: 020 ----
mean loss: 172.17
 ---- batch: 030 ----
mean loss: 166.58
 ---- batch: 040 ----
mean loss: 162.78
train mean loss: 168.32
epoch train time: 0:00:01.680877
elapsed time: 0:02:33.154241
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 22:40:25.016853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.08
 ---- batch: 020 ----
mean loss: 165.64
 ---- batch: 030 ----
mean loss: 163.90
 ---- batch: 040 ----
mean loss: 164.67
train mean loss: 164.70
epoch train time: 0:00:01.788709
elapsed time: 0:02:34.943141
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 22:40:26.805766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.90
 ---- batch: 020 ----
mean loss: 167.93
 ---- batch: 030 ----
mean loss: 160.97
 ---- batch: 040 ----
mean loss: 159.62
train mean loss: 163.46
epoch train time: 0:00:01.679127
elapsed time: 0:02:36.622454
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 22:40:28.485052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.14
 ---- batch: 020 ----
mean loss: 170.29
 ---- batch: 030 ----
mean loss: 164.99
 ---- batch: 040 ----
mean loss: 171.01
train mean loss: 166.65
epoch train time: 0:00:01.777633
elapsed time: 0:02:38.400238
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 22:40:30.262852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.35
 ---- batch: 020 ----
mean loss: 167.22
 ---- batch: 030 ----
mean loss: 162.73
 ---- batch: 040 ----
mean loss: 164.38
train mean loss: 163.95
epoch train time: 0:00:01.677947
elapsed time: 0:02:40.078343
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 22:40:31.940949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.45
 ---- batch: 020 ----
mean loss: 171.66
 ---- batch: 030 ----
mean loss: 159.97
 ---- batch: 040 ----
mean loss: 167.32
train mean loss: 165.85
epoch train time: 0:00:01.777990
elapsed time: 0:02:41.856498
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 22:40:33.719127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.72
 ---- batch: 020 ----
mean loss: 163.67
 ---- batch: 030 ----
mean loss: 158.55
 ---- batch: 040 ----
mean loss: 167.54
train mean loss: 163.02
epoch train time: 0:00:01.680044
elapsed time: 0:02:43.536734
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 22:40:35.399331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.04
 ---- batch: 020 ----
mean loss: 164.80
 ---- batch: 030 ----
mean loss: 167.08
 ---- batch: 040 ----
mean loss: 163.05
train mean loss: 165.87
epoch train time: 0:00:01.677248
elapsed time: 0:02:45.214136
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 22:40:37.076733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.43
 ---- batch: 020 ----
mean loss: 162.28
 ---- batch: 030 ----
mean loss: 161.74
 ---- batch: 040 ----
mean loss: 158.93
train mean loss: 161.72
epoch train time: 0:00:01.789927
elapsed time: 0:02:47.004206
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 22:40:38.866830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.02
 ---- batch: 020 ----
mean loss: 156.20
 ---- batch: 030 ----
mean loss: 166.29
 ---- batch: 040 ----
mean loss: 162.89
train mean loss: 161.44
epoch train time: 0:00:01.679825
elapsed time: 0:02:48.684221
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 22:40:40.546834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.09
 ---- batch: 020 ----
mean loss: 169.80
 ---- batch: 030 ----
mean loss: 164.17
 ---- batch: 040 ----
mean loss: 166.44
train mean loss: 165.52
epoch train time: 0:00:01.786813
elapsed time: 0:02:50.471188
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 22:40:42.333781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.51
 ---- batch: 020 ----
mean loss: 158.91
 ---- batch: 030 ----
mean loss: 155.00
 ---- batch: 040 ----
mean loss: 156.30
train mean loss: 157.29
epoch train time: 0:00:01.692347
elapsed time: 0:02:52.163690
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 22:40:44.026294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.27
 ---- batch: 020 ----
mean loss: 160.75
 ---- batch: 030 ----
mean loss: 152.80
 ---- batch: 040 ----
mean loss: 164.31
train mean loss: 157.83
epoch train time: 0:00:01.701281
elapsed time: 0:02:53.865150
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 22:40:45.727747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.62
 ---- batch: 020 ----
mean loss: 160.97
 ---- batch: 030 ----
mean loss: 152.46
 ---- batch: 040 ----
mean loss: 154.47
train mean loss: 158.12
epoch train time: 0:00:01.764168
elapsed time: 0:02:55.629467
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 22:40:47.492080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.72
 ---- batch: 020 ----
mean loss: 163.51
 ---- batch: 030 ----
mean loss: 153.70
 ---- batch: 040 ----
mean loss: 163.07
train mean loss: 158.85
epoch train time: 0:00:01.674087
elapsed time: 0:02:57.303726
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 22:40:49.166333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.59
 ---- batch: 020 ----
mean loss: 158.56
 ---- batch: 030 ----
mean loss: 153.80
 ---- batch: 040 ----
mean loss: 156.02
train mean loss: 157.75
epoch train time: 0:00:01.791270
elapsed time: 0:02:59.095168
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 22:40:50.957765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.36
 ---- batch: 020 ----
mean loss: 156.59
 ---- batch: 030 ----
mean loss: 157.30
 ---- batch: 040 ----
mean loss: 159.22
train mean loss: 158.50
epoch train time: 0:00:01.671575
elapsed time: 0:03:00.766897
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 22:40:52.629493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.78
 ---- batch: 020 ----
mean loss: 150.05
 ---- batch: 030 ----
mean loss: 155.03
 ---- batch: 040 ----
mean loss: 153.03
train mean loss: 153.52
epoch train time: 0:00:01.774738
elapsed time: 0:03:02.541775
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 22:40:54.404379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.35
 ---- batch: 020 ----
mean loss: 157.64
 ---- batch: 030 ----
mean loss: 157.52
 ---- batch: 040 ----
mean loss: 155.46
train mean loss: 156.37
epoch train time: 0:00:01.678893
elapsed time: 0:03:04.220825
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 22:40:56.083420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.26
 ---- batch: 020 ----
mean loss: 151.94
 ---- batch: 030 ----
mean loss: 151.69
 ---- batch: 040 ----
mean loss: 150.95
train mean loss: 151.79
epoch train time: 0:00:01.661999
elapsed time: 0:03:05.882969
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 22:40:57.745562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.21
 ---- batch: 020 ----
mean loss: 156.88
 ---- batch: 030 ----
mean loss: 152.87
 ---- batch: 040 ----
mean loss: 155.04
train mean loss: 152.61
epoch train time: 0:00:01.782222
elapsed time: 0:03:07.665325
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 22:40:59.527933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.78
 ---- batch: 020 ----
mean loss: 154.14
 ---- batch: 030 ----
mean loss: 154.57
 ---- batch: 040 ----
mean loss: 157.12
train mean loss: 154.84
epoch train time: 0:00:01.668710
elapsed time: 0:03:09.334211
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 22:41:01.196828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.20
 ---- batch: 020 ----
mean loss: 150.37
 ---- batch: 030 ----
mean loss: 152.48
 ---- batch: 040 ----
mean loss: 151.62
train mean loss: 151.32
epoch train time: 0:00:01.780216
elapsed time: 0:03:11.114581
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 22:41:02.977172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.73
 ---- batch: 020 ----
mean loss: 160.30
 ---- batch: 030 ----
mean loss: 160.21
 ---- batch: 040 ----
mean loss: 153.63
train mean loss: 156.69
epoch train time: 0:00:01.678552
elapsed time: 0:03:12.793278
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 22:41:04.655874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.77
 ---- batch: 020 ----
mean loss: 161.75
 ---- batch: 030 ----
mean loss: 152.15
 ---- batch: 040 ----
mean loss: 158.99
train mean loss: 156.32
epoch train time: 0:00:01.773015
elapsed time: 0:03:14.566454
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 22:41:06.429049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.86
 ---- batch: 020 ----
mean loss: 149.06
 ---- batch: 030 ----
mean loss: 152.28
 ---- batch: 040 ----
mean loss: 150.15
train mean loss: 152.70
epoch train time: 0:00:01.673256
elapsed time: 0:03:16.239858
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 22:41:08.102455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.59
 ---- batch: 020 ----
mean loss: 147.63
 ---- batch: 030 ----
mean loss: 148.51
 ---- batch: 040 ----
mean loss: 150.57
train mean loss: 150.94
epoch train time: 0:00:01.663895
elapsed time: 0:03:17.903921
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 22:41:09.766507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.54
 ---- batch: 020 ----
mean loss: 149.34
 ---- batch: 030 ----
mean loss: 152.22
 ---- batch: 040 ----
mean loss: 150.22
train mean loss: 151.26
epoch train time: 0:00:01.776212
elapsed time: 0:03:19.680263
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 22:41:11.542857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.95
 ---- batch: 020 ----
mean loss: 148.55
 ---- batch: 030 ----
mean loss: 145.58
 ---- batch: 040 ----
mean loss: 155.81
train mean loss: 150.24
epoch train time: 0:00:01.671308
elapsed time: 0:03:21.351720
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 22:41:13.214316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.99
 ---- batch: 020 ----
mean loss: 153.30
 ---- batch: 030 ----
mean loss: 158.84
 ---- batch: 040 ----
mean loss: 144.84
train mean loss: 154.16
epoch train time: 0:00:01.775780
elapsed time: 0:03:23.127636
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 22:41:14.990230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.26
 ---- batch: 020 ----
mean loss: 147.46
 ---- batch: 030 ----
mean loss: 147.43
 ---- batch: 040 ----
mean loss: 148.72
train mean loss: 148.47
epoch train time: 0:00:01.683162
elapsed time: 0:03:24.810939
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 22:41:16.673533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.41
 ---- batch: 020 ----
mean loss: 152.66
 ---- batch: 030 ----
mean loss: 148.48
 ---- batch: 040 ----
mean loss: 147.58
train mean loss: 149.61
epoch train time: 0:00:01.658189
elapsed time: 0:03:26.469265
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 22:41:18.331859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.04
 ---- batch: 020 ----
mean loss: 149.09
 ---- batch: 030 ----
mean loss: 152.49
 ---- batch: 040 ----
mean loss: 148.05
train mean loss: 149.24
epoch train time: 0:00:01.802076
elapsed time: 0:03:28.271488
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 22:41:20.134097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.48
 ---- batch: 020 ----
mean loss: 149.96
 ---- batch: 030 ----
mean loss: 151.69
 ---- batch: 040 ----
mean loss: 149.53
train mean loss: 150.34
epoch train time: 0:00:01.681229
elapsed time: 0:03:29.952937
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 22:41:21.815590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.78
 ---- batch: 020 ----
mean loss: 149.43
 ---- batch: 030 ----
mean loss: 149.65
 ---- batch: 040 ----
mean loss: 147.09
train mean loss: 147.47
epoch train time: 0:00:01.789594
elapsed time: 0:03:31.742754
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 22:41:23.605344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.83
 ---- batch: 020 ----
mean loss: 147.93
 ---- batch: 030 ----
mean loss: 153.51
 ---- batch: 040 ----
mean loss: 144.08
train mean loss: 148.40
epoch train time: 0:00:01.680135
elapsed time: 0:03:33.423038
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 22:41:25.285636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.32
 ---- batch: 020 ----
mean loss: 146.06
 ---- batch: 030 ----
mean loss: 148.96
 ---- batch: 040 ----
mean loss: 146.28
train mean loss: 149.02
epoch train time: 0:00:01.771279
elapsed time: 0:03:35.194453
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 22:41:27.057044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.93
 ---- batch: 020 ----
mean loss: 151.15
 ---- batch: 030 ----
mean loss: 143.22
 ---- batch: 040 ----
mean loss: 147.45
train mean loss: 148.17
epoch train time: 0:00:01.677281
elapsed time: 0:03:36.871885
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 22:41:28.734482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.18
 ---- batch: 020 ----
mean loss: 149.41
 ---- batch: 030 ----
mean loss: 150.70
 ---- batch: 040 ----
mean loss: 143.89
train mean loss: 147.64
epoch train time: 0:00:01.674614
elapsed time: 0:03:38.546640
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 22:41:30.409249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.83
 ---- batch: 020 ----
mean loss: 148.71
 ---- batch: 030 ----
mean loss: 153.87
 ---- batch: 040 ----
mean loss: 146.56
train mean loss: 149.30
epoch train time: 0:00:01.792001
elapsed time: 0:03:40.338795
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 22:41:32.201394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.31
 ---- batch: 020 ----
mean loss: 145.29
 ---- batch: 030 ----
mean loss: 147.12
 ---- batch: 040 ----
mean loss: 144.23
train mean loss: 144.82
epoch train time: 0:00:01.673243
elapsed time: 0:03:42.012203
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 22:41:33.874800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.07
 ---- batch: 020 ----
mean loss: 150.44
 ---- batch: 030 ----
mean loss: 145.73
 ---- batch: 040 ----
mean loss: 144.43
train mean loss: 147.84
epoch train time: 0:00:01.783145
elapsed time: 0:03:43.795530
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 22:41:35.658123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.28
 ---- batch: 020 ----
mean loss: 145.27
 ---- batch: 030 ----
mean loss: 149.59
 ---- batch: 040 ----
mean loss: 143.33
train mean loss: 145.19
epoch train time: 0:00:01.682232
elapsed time: 0:03:45.477923
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 22:41:37.340523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.95
 ---- batch: 020 ----
mean loss: 147.42
 ---- batch: 030 ----
mean loss: 147.74
 ---- batch: 040 ----
mean loss: 151.57
train mean loss: 148.21
epoch train time: 0:00:01.665171
elapsed time: 0:03:47.143268
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 22:41:39.005862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.12
 ---- batch: 020 ----
mean loss: 141.83
 ---- batch: 030 ----
mean loss: 147.08
 ---- batch: 040 ----
mean loss: 151.15
train mean loss: 147.03
epoch train time: 0:00:01.716194
elapsed time: 0:03:48.859612
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 22:41:40.722229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.00
 ---- batch: 020 ----
mean loss: 143.51
 ---- batch: 030 ----
mean loss: 145.52
 ---- batch: 040 ----
mean loss: 145.87
train mean loss: 144.80
epoch train time: 0:00:01.672394
elapsed time: 0:03:50.532168
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 22:41:42.394775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.09
 ---- batch: 020 ----
mean loss: 147.48
 ---- batch: 030 ----
mean loss: 152.05
 ---- batch: 040 ----
mean loss: 145.59
train mean loss: 147.30
epoch train time: 0:00:01.786457
elapsed time: 0:03:52.318786
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 22:41:44.181369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.10
 ---- batch: 020 ----
mean loss: 142.87
 ---- batch: 030 ----
mean loss: 144.53
 ---- batch: 040 ----
mean loss: 152.31
train mean loss: 147.45
epoch train time: 0:00:01.675371
elapsed time: 0:03:53.994306
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 22:41:45.856903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.76
 ---- batch: 020 ----
mean loss: 145.80
 ---- batch: 030 ----
mean loss: 143.38
 ---- batch: 040 ----
mean loss: 145.23
train mean loss: 144.51
epoch train time: 0:00:01.783679
elapsed time: 0:03:55.778169
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 22:41:47.640778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.25
 ---- batch: 020 ----
mean loss: 144.96
 ---- batch: 030 ----
mean loss: 144.08
 ---- batch: 040 ----
mean loss: 141.97
train mean loss: 143.31
epoch train time: 0:00:01.678271
elapsed time: 0:03:57.456609
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 22:41:49.319203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.23
 ---- batch: 020 ----
mean loss: 145.30
 ---- batch: 030 ----
mean loss: 145.32
 ---- batch: 040 ----
mean loss: 144.93
train mean loss: 145.54
epoch train time: 0:00:01.774493
elapsed time: 0:03:59.231231
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 22:41:51.093823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.58
 ---- batch: 020 ----
mean loss: 144.47
 ---- batch: 030 ----
mean loss: 146.86
 ---- batch: 040 ----
mean loss: 143.00
train mean loss: 145.02
epoch train time: 0:00:01.683425
elapsed time: 0:04:00.914817
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 22:41:52.777413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.77
 ---- batch: 020 ----
mean loss: 144.04
 ---- batch: 030 ----
mean loss: 141.99
 ---- batch: 040 ----
mean loss: 143.52
train mean loss: 144.54
epoch train time: 0:00:01.773221
elapsed time: 0:04:02.688178
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 22:41:54.550781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.40
 ---- batch: 020 ----
mean loss: 146.09
 ---- batch: 030 ----
mean loss: 148.77
 ---- batch: 040 ----
mean loss: 141.52
train mean loss: 143.81
epoch train time: 0:00:01.694036
elapsed time: 0:04:04.382407
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 22:41:56.245004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.67
 ---- batch: 020 ----
mean loss: 145.69
 ---- batch: 030 ----
mean loss: 143.45
 ---- batch: 040 ----
mean loss: 148.08
train mean loss: 144.23
epoch train time: 0:00:01.668490
elapsed time: 0:04:06.051045
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 22:41:57.913642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.18
 ---- batch: 020 ----
mean loss: 143.56
 ---- batch: 030 ----
mean loss: 139.65
 ---- batch: 040 ----
mean loss: 145.79
train mean loss: 143.64
epoch train time: 0:00:01.790219
elapsed time: 0:04:07.841424
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 22:41:59.704028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.87
 ---- batch: 020 ----
mean loss: 147.86
 ---- batch: 030 ----
mean loss: 138.30
 ---- batch: 040 ----
mean loss: 142.25
train mean loss: 144.72
epoch train time: 0:00:01.672924
elapsed time: 0:04:09.514504
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 22:42:01.377120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.19
 ---- batch: 020 ----
mean loss: 145.30
 ---- batch: 030 ----
mean loss: 141.71
 ---- batch: 040 ----
mean loss: 143.79
train mean loss: 144.54
epoch train time: 0:00:01.781011
elapsed time: 0:04:11.295674
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 22:42:03.158267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.96
 ---- batch: 020 ----
mean loss: 149.31
 ---- batch: 030 ----
mean loss: 141.23
 ---- batch: 040 ----
mean loss: 143.19
train mean loss: 144.93
epoch train time: 0:00:01.672416
elapsed time: 0:04:12.968237
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 22:42:04.830833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.83
 ---- batch: 020 ----
mean loss: 143.10
 ---- batch: 030 ----
mean loss: 139.76
 ---- batch: 040 ----
mean loss: 145.00
train mean loss: 142.95
epoch train time: 0:00:01.771432
elapsed time: 0:04:14.739844
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 22:42:06.602443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.14
 ---- batch: 020 ----
mean loss: 139.51
 ---- batch: 030 ----
mean loss: 142.78
 ---- batch: 040 ----
mean loss: 152.80
train mean loss: 144.41
epoch train time: 0:00:01.673367
elapsed time: 0:04:16.413355
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 22:42:08.275948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.39
 ---- batch: 020 ----
mean loss: 142.81
 ---- batch: 030 ----
mean loss: 140.56
 ---- batch: 040 ----
mean loss: 146.36
train mean loss: 143.15
epoch train time: 0:00:01.778041
elapsed time: 0:04:18.191545
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 22:42:10.054136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.66
 ---- batch: 020 ----
mean loss: 146.17
 ---- batch: 030 ----
mean loss: 146.64
 ---- batch: 040 ----
mean loss: 137.59
train mean loss: 142.90
epoch train time: 0:00:01.675344
elapsed time: 0:04:19.867050
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 22:42:11.729650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.04
 ---- batch: 020 ----
mean loss: 140.83
 ---- batch: 030 ----
mean loss: 144.26
 ---- batch: 040 ----
mean loss: 143.74
train mean loss: 142.59
epoch train time: 0:00:01.782920
elapsed time: 0:04:21.650111
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 22:42:13.512704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.16
 ---- batch: 020 ----
mean loss: 148.00
 ---- batch: 030 ----
mean loss: 144.17
 ---- batch: 040 ----
mean loss: 141.20
train mean loss: 143.40
epoch train time: 0:00:01.682556
elapsed time: 0:04:23.332812
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 22:42:15.195406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.74
 ---- batch: 020 ----
mean loss: 144.37
 ---- batch: 030 ----
mean loss: 141.43
 ---- batch: 040 ----
mean loss: 138.72
train mean loss: 144.24
epoch train time: 0:00:01.702735
elapsed time: 0:04:25.035691
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 22:42:16.898284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.90
 ---- batch: 020 ----
mean loss: 143.90
 ---- batch: 030 ----
mean loss: 141.49
 ---- batch: 040 ----
mean loss: 141.85
train mean loss: 141.32
epoch train time: 0:00:01.744914
elapsed time: 0:04:26.780857
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 22:42:18.643479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.31
 ---- batch: 020 ----
mean loss: 139.60
 ---- batch: 030 ----
mean loss: 139.15
 ---- batch: 040 ----
mean loss: 142.24
train mean loss: 140.98
epoch train time: 0:00:01.669335
elapsed time: 0:04:28.450362
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 22:42:20.312957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.88
 ---- batch: 020 ----
mean loss: 146.11
 ---- batch: 030 ----
mean loss: 139.83
 ---- batch: 040 ----
mean loss: 143.58
train mean loss: 142.87
epoch train time: 0:00:01.790503
elapsed time: 0:04:30.241026
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 22:42:22.103626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.62
 ---- batch: 020 ----
mean loss: 141.26
 ---- batch: 030 ----
mean loss: 142.49
 ---- batch: 040 ----
mean loss: 143.84
train mean loss: 142.76
epoch train time: 0:00:01.669145
elapsed time: 0:04:31.910340
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 22:42:23.772937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.78
 ---- batch: 020 ----
mean loss: 139.79
 ---- batch: 030 ----
mean loss: 138.02
 ---- batch: 040 ----
mean loss: 142.09
train mean loss: 140.01
epoch train time: 0:00:01.796935
elapsed time: 0:04:33.707421
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 22:42:25.570030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.02
 ---- batch: 020 ----
mean loss: 140.23
 ---- batch: 030 ----
mean loss: 143.24
 ---- batch: 040 ----
mean loss: 140.84
train mean loss: 142.02
epoch train time: 0:00:01.678187
elapsed time: 0:04:35.385766
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 22:42:27.248364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.12
 ---- batch: 020 ----
mean loss: 141.33
 ---- batch: 030 ----
mean loss: 135.62
 ---- batch: 040 ----
mean loss: 142.05
train mean loss: 139.64
epoch train time: 0:00:01.787323
elapsed time: 0:04:37.173220
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 22:42:29.035810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.58
 ---- batch: 020 ----
mean loss: 138.60
 ---- batch: 030 ----
mean loss: 134.36
 ---- batch: 040 ----
mean loss: 147.38
train mean loss: 141.21
epoch train time: 0:00:01.678407
elapsed time: 0:04:38.851769
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 22:42:30.714365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.95
 ---- batch: 020 ----
mean loss: 148.41
 ---- batch: 030 ----
mean loss: 140.15
 ---- batch: 040 ----
mean loss: 142.56
train mean loss: 143.44
epoch train time: 0:00:01.771097
elapsed time: 0:04:40.622995
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 22:42:32.485602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.12
 ---- batch: 020 ----
mean loss: 138.35
 ---- batch: 030 ----
mean loss: 147.33
 ---- batch: 040 ----
mean loss: 135.70
train mean loss: 138.90
epoch train time: 0:00:01.679917
elapsed time: 0:04:42.303077
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 22:42:34.165672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.76
 ---- batch: 020 ----
mean loss: 141.48
 ---- batch: 030 ----
mean loss: 140.75
 ---- batch: 040 ----
mean loss: 145.37
train mean loss: 140.85
epoch train time: 0:00:01.768787
elapsed time: 0:04:44.072004
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 22:42:35.934597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.27
 ---- batch: 020 ----
mean loss: 139.93
 ---- batch: 030 ----
mean loss: 139.61
 ---- batch: 040 ----
mean loss: 139.49
train mean loss: 139.51
epoch train time: 0:00:01.674265
elapsed time: 0:04:45.746425
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 22:42:37.609025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.89
 ---- batch: 020 ----
mean loss: 144.13
 ---- batch: 030 ----
mean loss: 145.23
 ---- batch: 040 ----
mean loss: 143.28
train mean loss: 142.81
epoch train time: 0:00:01.670823
elapsed time: 0:04:47.417388
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 22:42:39.279982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.06
 ---- batch: 020 ----
mean loss: 136.54
 ---- batch: 030 ----
mean loss: 139.61
 ---- batch: 040 ----
mean loss: 145.77
train mean loss: 140.89
epoch train time: 0:00:01.787278
elapsed time: 0:04:49.204814
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 22:42:41.067409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.09
 ---- batch: 020 ----
mean loss: 133.70
 ---- batch: 030 ----
mean loss: 140.73
 ---- batch: 040 ----
mean loss: 144.04
train mean loss: 139.35
epoch train time: 0:00:01.669826
elapsed time: 0:04:50.874808
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 22:42:42.737421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.88
 ---- batch: 020 ----
mean loss: 143.23
 ---- batch: 030 ----
mean loss: 136.00
 ---- batch: 040 ----
mean loss: 139.08
train mean loss: 139.91
epoch train time: 0:00:01.771582
elapsed time: 0:04:52.646552
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 22:42:44.509146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.33
 ---- batch: 020 ----
mean loss: 137.33
 ---- batch: 030 ----
mean loss: 143.84
 ---- batch: 040 ----
mean loss: 140.86
train mean loss: 139.41
epoch train time: 0:00:01.679438
elapsed time: 0:04:54.326140
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 22:42:46.188738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.35
 ---- batch: 020 ----
mean loss: 136.33
 ---- batch: 030 ----
mean loss: 139.90
 ---- batch: 040 ----
mean loss: 136.54
train mean loss: 138.07
epoch train time: 0:00:01.709193
elapsed time: 0:04:56.035487
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 22:42:47.898084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.53
 ---- batch: 020 ----
mean loss: 136.96
 ---- batch: 030 ----
mean loss: 139.15
 ---- batch: 040 ----
mean loss: 141.51
train mean loss: 139.72
epoch train time: 0:00:01.680475
elapsed time: 0:04:57.716120
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 22:42:49.578716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.45
 ---- batch: 020 ----
mean loss: 136.56
 ---- batch: 030 ----
mean loss: 134.34
 ---- batch: 040 ----
mean loss: 139.28
train mean loss: 138.26
epoch train time: 0:00:01.783602
elapsed time: 0:04:59.499888
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 22:42:51.362481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.96
 ---- batch: 020 ----
mean loss: 137.81
 ---- batch: 030 ----
mean loss: 142.47
 ---- batch: 040 ----
mean loss: 136.76
train mean loss: 138.64
epoch train time: 0:00:01.686524
elapsed time: 0:05:01.186567
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 22:42:53.049167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.52
 ---- batch: 020 ----
mean loss: 141.32
 ---- batch: 030 ----
mean loss: 141.66
 ---- batch: 040 ----
mean loss: 142.60
train mean loss: 141.58
epoch train time: 0:00:01.770980
elapsed time: 0:05:02.957709
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 22:42:54.820300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.52
 ---- batch: 020 ----
mean loss: 133.30
 ---- batch: 030 ----
mean loss: 139.26
 ---- batch: 040 ----
mean loss: 138.38
train mean loss: 137.69
epoch train time: 0:00:01.694842
elapsed time: 0:05:04.652701
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 22:42:56.515296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.46
 ---- batch: 020 ----
mean loss: 139.61
 ---- batch: 030 ----
mean loss: 146.80
 ---- batch: 040 ----
mean loss: 148.25
train mean loss: 143.91
epoch train time: 0:00:01.670416
elapsed time: 0:05:06.323298
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 22:42:58.185896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.21
 ---- batch: 020 ----
mean loss: 145.93
 ---- batch: 030 ----
mean loss: 142.17
 ---- batch: 040 ----
mean loss: 137.58
train mean loss: 142.68
epoch train time: 0:00:01.787790
elapsed time: 0:05:08.111250
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 22:42:59.973848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.09
 ---- batch: 020 ----
mean loss: 141.78
 ---- batch: 030 ----
mean loss: 144.38
 ---- batch: 040 ----
mean loss: 134.65
train mean loss: 138.85
epoch train time: 0:00:01.677159
elapsed time: 0:05:09.788564
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 22:43:01.651159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.87
 ---- batch: 020 ----
mean loss: 138.75
 ---- batch: 030 ----
mean loss: 139.70
 ---- batch: 040 ----
mean loss: 146.51
train mean loss: 140.28
epoch train time: 0:00:01.778926
elapsed time: 0:05:11.567640
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 22:43:03.430292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.94
 ---- batch: 020 ----
mean loss: 138.57
 ---- batch: 030 ----
mean loss: 139.43
 ---- batch: 040 ----
mean loss: 133.67
train mean loss: 137.07
epoch train time: 0:00:01.691206
elapsed time: 0:05:13.259071
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 22:43:05.121656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.88
 ---- batch: 020 ----
mean loss: 138.79
 ---- batch: 030 ----
mean loss: 138.47
 ---- batch: 040 ----
mean loss: 138.39
train mean loss: 139.47
epoch train time: 0:00:01.663969
elapsed time: 0:05:14.923178
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 22:43:06.785773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.56
 ---- batch: 020 ----
mean loss: 140.82
 ---- batch: 030 ----
mean loss: 134.30
 ---- batch: 040 ----
mean loss: 141.29
train mean loss: 137.57
epoch train time: 0:00:01.785868
elapsed time: 0:05:16.709221
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 22:43:08.571825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.36
 ---- batch: 020 ----
mean loss: 139.73
 ---- batch: 030 ----
mean loss: 136.75
 ---- batch: 040 ----
mean loss: 143.34
train mean loss: 139.81
epoch train time: 0:00:01.676211
elapsed time: 0:05:18.385591
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 22:43:10.248187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.27
 ---- batch: 020 ----
mean loss: 136.10
 ---- batch: 030 ----
mean loss: 137.31
 ---- batch: 040 ----
mean loss: 140.15
train mean loss: 138.23
epoch train time: 0:00:01.798239
elapsed time: 0:05:20.183978
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 22:43:12.046572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.15
 ---- batch: 020 ----
mean loss: 139.62
 ---- batch: 030 ----
mean loss: 133.50
 ---- batch: 040 ----
mean loss: 136.15
train mean loss: 138.45
epoch train time: 0:00:01.677281
elapsed time: 0:05:21.861415
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 22:43:13.724015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.29
 ---- batch: 020 ----
mean loss: 138.16
 ---- batch: 030 ----
mean loss: 143.12
 ---- batch: 040 ----
mean loss: 136.41
train mean loss: 139.51
epoch train time: 0:00:01.779563
elapsed time: 0:05:23.641126
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 22:43:15.503721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.25
 ---- batch: 020 ----
mean loss: 143.09
 ---- batch: 030 ----
mean loss: 136.00
 ---- batch: 040 ----
mean loss: 135.64
train mean loss: 139.59
epoch train time: 0:00:01.691891
elapsed time: 0:05:25.333160
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 22:43:17.195756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.89
 ---- batch: 020 ----
mean loss: 138.15
 ---- batch: 030 ----
mean loss: 137.62
 ---- batch: 040 ----
mean loss: 144.87
train mean loss: 139.30
epoch train time: 0:00:01.671019
elapsed time: 0:05:27.004332
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 22:43:18.866930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.47
 ---- batch: 020 ----
mean loss: 138.51
 ---- batch: 030 ----
mean loss: 134.43
 ---- batch: 040 ----
mean loss: 137.19
train mean loss: 136.34
epoch train time: 0:00:01.794477
elapsed time: 0:05:28.798964
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 22:43:20.661568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.23
 ---- batch: 020 ----
mean loss: 138.57
 ---- batch: 030 ----
mean loss: 135.53
 ---- batch: 040 ----
mean loss: 138.31
train mean loss: 136.41
epoch train time: 0:00:01.669364
elapsed time: 0:05:30.468482
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 22:43:22.331079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.14
 ---- batch: 020 ----
mean loss: 140.08
 ---- batch: 030 ----
mean loss: 141.91
 ---- batch: 040 ----
mean loss: 140.75
train mean loss: 140.34
epoch train time: 0:00:01.785060
elapsed time: 0:05:32.253679
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 22:43:24.116303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.97
 ---- batch: 020 ----
mean loss: 135.16
 ---- batch: 030 ----
mean loss: 139.27
 ---- batch: 040 ----
mean loss: 143.21
train mean loss: 138.63
epoch train time: 0:00:01.677429
elapsed time: 0:05:33.931283
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 22:43:25.793878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.35
 ---- batch: 020 ----
mean loss: 140.96
 ---- batch: 030 ----
mean loss: 133.37
 ---- batch: 040 ----
mean loss: 137.60
train mean loss: 138.44
epoch train time: 0:00:01.721130
elapsed time: 0:05:35.652548
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 22:43:27.515154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.20
 ---- batch: 020 ----
mean loss: 139.96
 ---- batch: 030 ----
mean loss: 134.85
 ---- batch: 040 ----
mean loss: 135.18
train mean loss: 137.25
epoch train time: 0:00:01.745048
elapsed time: 0:05:37.397759
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 22:43:29.260358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.62
 ---- batch: 020 ----
mean loss: 136.37
 ---- batch: 030 ----
mean loss: 138.23
 ---- batch: 040 ----
mean loss: 133.92
train mean loss: 136.75
epoch train time: 0:00:01.669770
elapsed time: 0:05:39.067672
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 22:43:30.930265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.73
 ---- batch: 020 ----
mean loss: 139.47
 ---- batch: 030 ----
mean loss: 138.47
 ---- batch: 040 ----
mean loss: 143.07
train mean loss: 139.12
epoch train time: 0:00:01.785852
elapsed time: 0:05:40.853680
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 22:43:32.716285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.61
 ---- batch: 020 ----
mean loss: 133.67
 ---- batch: 030 ----
mean loss: 133.90
 ---- batch: 040 ----
mean loss: 136.72
train mean loss: 134.96
epoch train time: 0:00:01.678490
elapsed time: 0:05:42.532321
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 22:43:34.394916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.17
 ---- batch: 020 ----
mean loss: 133.89
 ---- batch: 030 ----
mean loss: 138.96
 ---- batch: 040 ----
mean loss: 141.13
train mean loss: 137.17
epoch train time: 0:00:01.777268
elapsed time: 0:05:44.309729
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 22:43:36.172324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.38
 ---- batch: 020 ----
mean loss: 132.43
 ---- batch: 030 ----
mean loss: 134.05
 ---- batch: 040 ----
mean loss: 137.54
train mean loss: 134.87
epoch train time: 0:00:01.688739
elapsed time: 0:05:45.998614
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 22:43:37.861211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.98
 ---- batch: 020 ----
mean loss: 138.31
 ---- batch: 030 ----
mean loss: 138.18
 ---- batch: 040 ----
mean loss: 134.54
train mean loss: 136.13
epoch train time: 0:00:01.677411
elapsed time: 0:05:47.676165
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 22:43:39.538774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.84
 ---- batch: 020 ----
mean loss: 144.14
 ---- batch: 030 ----
mean loss: 128.24
 ---- batch: 040 ----
mean loss: 137.70
train mean loss: 136.99
epoch train time: 0:00:01.782808
elapsed time: 0:05:49.459122
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 22:43:41.321715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.62
 ---- batch: 020 ----
mean loss: 141.06
 ---- batch: 030 ----
mean loss: 136.24
 ---- batch: 040 ----
mean loss: 135.33
train mean loss: 135.83
epoch train time: 0:00:01.675618
elapsed time: 0:05:51.134884
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 22:43:42.997480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.43
 ---- batch: 020 ----
mean loss: 138.57
 ---- batch: 030 ----
mean loss: 133.91
 ---- batch: 040 ----
mean loss: 133.37
train mean loss: 135.38
epoch train time: 0:00:01.794418
elapsed time: 0:05:52.929442
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 22:43:44.792054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.77
 ---- batch: 020 ----
mean loss: 132.09
 ---- batch: 030 ----
mean loss: 134.95
 ---- batch: 040 ----
mean loss: 136.04
train mean loss: 135.23
epoch train time: 0:00:01.666580
elapsed time: 0:05:54.596191
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 22:43:46.458786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.18
 ---- batch: 020 ----
mean loss: 130.82
 ---- batch: 030 ----
mean loss: 137.72
 ---- batch: 040 ----
mean loss: 132.87
train mean loss: 135.44
epoch train time: 0:00:01.771373
elapsed time: 0:05:56.367705
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 22:43:48.230300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.02
 ---- batch: 020 ----
mean loss: 136.16
 ---- batch: 030 ----
mean loss: 136.59
 ---- batch: 040 ----
mean loss: 131.42
train mean loss: 133.36
epoch train time: 0:00:01.689802
elapsed time: 0:05:58.057658
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 22:43:49.920254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.32
 ---- batch: 020 ----
mean loss: 134.23
 ---- batch: 030 ----
mean loss: 129.58
 ---- batch: 040 ----
mean loss: 134.97
train mean loss: 134.15
epoch train time: 0:00:01.670021
elapsed time: 0:05:59.727823
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 22:43:51.590418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.17
 ---- batch: 020 ----
mean loss: 134.06
 ---- batch: 030 ----
mean loss: 128.20
 ---- batch: 040 ----
mean loss: 136.37
train mean loss: 132.23
epoch train time: 0:00:01.777246
elapsed time: 0:06:01.505219
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 22:43:53.367802
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.76
 ---- batch: 020 ----
mean loss: 139.74
 ---- batch: 030 ----
mean loss: 132.23
 ---- batch: 040 ----
mean loss: 129.46
train mean loss: 133.16
epoch train time: 0:00:01.664867
elapsed time: 0:06:03.170230
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 22:43:55.032830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.40
 ---- batch: 020 ----
mean loss: 137.89
 ---- batch: 030 ----
mean loss: 129.88
 ---- batch: 040 ----
mean loss: 132.33
train mean loss: 133.06
epoch train time: 0:00:01.777890
elapsed time: 0:06:04.948270
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 22:43:56.810866
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.60
 ---- batch: 020 ----
mean loss: 133.73
 ---- batch: 030 ----
mean loss: 134.45
 ---- batch: 040 ----
mean loss: 131.75
train mean loss: 132.03
epoch train time: 0:00:01.689236
elapsed time: 0:06:06.637668
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 22:43:58.500265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.78
 ---- batch: 020 ----
mean loss: 132.35
 ---- batch: 030 ----
mean loss: 132.72
 ---- batch: 040 ----
mean loss: 131.06
train mean loss: 131.56
epoch train time: 0:00:01.744060
elapsed time: 0:06:08.381888
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 22:44:00.244485
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.24
 ---- batch: 020 ----
mean loss: 132.59
 ---- batch: 030 ----
mean loss: 129.31
 ---- batch: 040 ----
mean loss: 131.25
train mean loss: 131.84
epoch train time: 0:00:01.727443
elapsed time: 0:06:10.109489
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 22:44:01.972085
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.64
 ---- batch: 020 ----
mean loss: 129.31
 ---- batch: 030 ----
mean loss: 134.17
 ---- batch: 040 ----
mean loss: 130.48
train mean loss: 133.23
epoch train time: 0:00:01.680402
elapsed time: 0:06:11.790061
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 22:44:03.652663
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.37
 ---- batch: 020 ----
mean loss: 132.99
 ---- batch: 030 ----
mean loss: 128.35
 ---- batch: 040 ----
mean loss: 131.77
train mean loss: 130.91
epoch train time: 0:00:01.789012
elapsed time: 0:06:13.579230
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 22:44:05.441854
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.09
 ---- batch: 020 ----
mean loss: 133.50
 ---- batch: 030 ----
mean loss: 130.34
 ---- batch: 040 ----
mean loss: 138.69
train mean loss: 131.95
epoch train time: 0:00:01.680809
elapsed time: 0:06:15.260215
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 22:44:07.122809
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.08
 ---- batch: 020 ----
mean loss: 129.27
 ---- batch: 030 ----
mean loss: 133.44
 ---- batch: 040 ----
mean loss: 131.01
train mean loss: 132.48
epoch train time: 0:00:01.788071
elapsed time: 0:06:17.048429
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 22:44:08.911023
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.21
 ---- batch: 020 ----
mean loss: 133.61
 ---- batch: 030 ----
mean loss: 127.50
 ---- batch: 040 ----
mean loss: 131.59
train mean loss: 132.06
epoch train time: 0:00:01.690524
elapsed time: 0:06:18.739095
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 22:44:10.601694
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.47
 ---- batch: 020 ----
mean loss: 127.16
 ---- batch: 030 ----
mean loss: 129.28
 ---- batch: 040 ----
mean loss: 129.83
train mean loss: 130.69
epoch train time: 0:00:01.667113
elapsed time: 0:06:20.406375
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 22:44:12.268970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.12
 ---- batch: 020 ----
mean loss: 129.95
 ---- batch: 030 ----
mean loss: 130.02
 ---- batch: 040 ----
mean loss: 135.17
train mean loss: 131.70
epoch train time: 0:00:01.803900
elapsed time: 0:06:22.210408
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 22:44:14.072998
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.84
 ---- batch: 020 ----
mean loss: 135.31
 ---- batch: 030 ----
mean loss: 133.98
 ---- batch: 040 ----
mean loss: 130.84
train mean loss: 133.50
epoch train time: 0:00:01.676533
elapsed time: 0:06:23.887110
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 22:44:15.749704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.42
 ---- batch: 020 ----
mean loss: 135.82
 ---- batch: 030 ----
mean loss: 130.33
 ---- batch: 040 ----
mean loss: 130.47
train mean loss: 132.43
epoch train time: 0:00:01.787612
elapsed time: 0:06:25.674856
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 22:44:17.537463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.87
 ---- batch: 020 ----
mean loss: 133.02
 ---- batch: 030 ----
mean loss: 129.43
 ---- batch: 040 ----
mean loss: 131.31
train mean loss: 131.76
epoch train time: 0:00:01.679023
elapsed time: 0:06:27.354061
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 22:44:19.216654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.66
 ---- batch: 020 ----
mean loss: 131.27
 ---- batch: 030 ----
mean loss: 135.17
 ---- batch: 040 ----
mean loss: 126.24
train mean loss: 130.67
epoch train time: 0:00:01.775003
elapsed time: 0:06:29.129193
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 22:44:20.991807
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.38
 ---- batch: 020 ----
mean loss: 135.21
 ---- batch: 030 ----
mean loss: 130.68
 ---- batch: 040 ----
mean loss: 128.58
train mean loss: 130.95
epoch train time: 0:00:01.695731
elapsed time: 0:06:30.825103
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 22:44:22.687700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.79
 ---- batch: 020 ----
mean loss: 131.45
 ---- batch: 030 ----
mean loss: 131.14
 ---- batch: 040 ----
mean loss: 133.22
train mean loss: 131.66
epoch train time: 0:00:01.668366
elapsed time: 0:06:32.493614
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 22:44:24.356209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.73
 ---- batch: 020 ----
mean loss: 130.75
 ---- batch: 030 ----
mean loss: 133.70
 ---- batch: 040 ----
mean loss: 125.75
train mean loss: 129.75
epoch train time: 0:00:01.798187
elapsed time: 0:06:34.291975
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 22:44:26.154608
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.30
 ---- batch: 020 ----
mean loss: 130.15
 ---- batch: 030 ----
mean loss: 124.34
 ---- batch: 040 ----
mean loss: 134.69
train mean loss: 131.14
epoch train time: 0:00:01.678295
elapsed time: 0:06:35.970453
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 22:44:27.833062
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.17
 ---- batch: 020 ----
mean loss: 136.69
 ---- batch: 030 ----
mean loss: 134.12
 ---- batch: 040 ----
mean loss: 129.43
train mean loss: 133.06
epoch train time: 0:00:01.790738
elapsed time: 0:06:37.761347
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 22:44:29.623940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.70
 ---- batch: 020 ----
mean loss: 131.22
 ---- batch: 030 ----
mean loss: 135.91
 ---- batch: 040 ----
mean loss: 127.96
train mean loss: 132.19
epoch train time: 0:00:01.685954
elapsed time: 0:06:39.447445
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 22:44:31.310042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.73
 ---- batch: 020 ----
mean loss: 133.23
 ---- batch: 030 ----
mean loss: 128.87
 ---- batch: 040 ----
mean loss: 130.19
train mean loss: 131.96
epoch train time: 0:00:01.725003
elapsed time: 0:06:41.172589
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 22:44:33.035183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.35
 ---- batch: 020 ----
mean loss: 127.83
 ---- batch: 030 ----
mean loss: 136.30
 ---- batch: 040 ----
mean loss: 132.17
train mean loss: 131.80
epoch train time: 0:00:01.743525
elapsed time: 0:06:42.916266
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 22:44:34.778861
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.40
 ---- batch: 020 ----
mean loss: 133.93
 ---- batch: 030 ----
mean loss: 129.11
 ---- batch: 040 ----
mean loss: 133.54
train mean loss: 132.83
epoch train time: 0:00:01.670282
elapsed time: 0:06:44.586699
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 22:44:36.449306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.90
 ---- batch: 020 ----
mean loss: 133.70
 ---- batch: 030 ----
mean loss: 134.72
 ---- batch: 040 ----
mean loss: 129.88
train mean loss: 132.03
epoch train time: 0:00:01.791525
elapsed time: 0:06:46.378388
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 22:44:38.241015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.43
 ---- batch: 020 ----
mean loss: 130.12
 ---- batch: 030 ----
mean loss: 131.45
 ---- batch: 040 ----
mean loss: 136.49
train mean loss: 132.44
epoch train time: 0:00:01.674590
elapsed time: 0:06:48.053170
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 22:44:39.915776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.75
 ---- batch: 020 ----
mean loss: 131.30
 ---- batch: 030 ----
mean loss: 134.38
 ---- batch: 040 ----
mean loss: 134.89
train mean loss: 132.55
epoch train time: 0:00:01.781758
elapsed time: 0:06:49.835082
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 22:44:41.697679
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.14
 ---- batch: 020 ----
mean loss: 135.32
 ---- batch: 030 ----
mean loss: 134.75
 ---- batch: 040 ----
mean loss: 133.40
train mean loss: 133.64
epoch train time: 0:00:01.683577
elapsed time: 0:06:51.518809
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 22:44:43.381405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.73
 ---- batch: 020 ----
mean loss: 129.06
 ---- batch: 030 ----
mean loss: 129.95
 ---- batch: 040 ----
mean loss: 129.31
train mean loss: 130.01
epoch train time: 0:00:01.673300
elapsed time: 0:06:53.192263
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 22:44:45.054870
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.52
 ---- batch: 020 ----
mean loss: 136.03
 ---- batch: 030 ----
mean loss: 129.44
 ---- batch: 040 ----
mean loss: 128.57
train mean loss: 131.04
epoch train time: 0:00:01.791356
elapsed time: 0:06:54.983852
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 22:44:46.846459
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.15
 ---- batch: 020 ----
mean loss: 132.95
 ---- batch: 030 ----
mean loss: 129.30
 ---- batch: 040 ----
mean loss: 129.70
train mean loss: 131.02
epoch train time: 0:00:01.673688
elapsed time: 0:06:56.657714
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 22:44:48.520299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.05
 ---- batch: 020 ----
mean loss: 131.90
 ---- batch: 030 ----
mean loss: 131.77
 ---- batch: 040 ----
mean loss: 126.02
train mean loss: 130.53
epoch train time: 0:00:01.788819
elapsed time: 0:06:58.446662
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 22:44:50.309257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.90
 ---- batch: 020 ----
mean loss: 130.09
 ---- batch: 030 ----
mean loss: 131.40
 ---- batch: 040 ----
mean loss: 127.25
train mean loss: 130.30
epoch train time: 0:00:01.682200
elapsed time: 0:07:00.129012
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 22:44:51.991610
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.33
 ---- batch: 020 ----
mean loss: 130.20
 ---- batch: 030 ----
mean loss: 133.33
 ---- batch: 040 ----
mean loss: 127.89
train mean loss: 131.54
epoch train time: 0:00:01.784002
elapsed time: 0:07:01.913199
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 22:44:53.775795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.41
 ---- batch: 020 ----
mean loss: 126.74
 ---- batch: 030 ----
mean loss: 130.65
 ---- batch: 040 ----
mean loss: 131.95
train mean loss: 131.55
epoch train time: 0:00:01.689478
elapsed time: 0:07:03.602870
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 22:44:55.465477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.99
 ---- batch: 020 ----
mean loss: 131.95
 ---- batch: 030 ----
mean loss: 135.47
 ---- batch: 040 ----
mean loss: 132.93
train mean loss: 132.49
epoch train time: 0:00:01.669832
elapsed time: 0:07:05.272889
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 22:44:57.135481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.91
 ---- batch: 020 ----
mean loss: 127.60
 ---- batch: 030 ----
mean loss: 132.03
 ---- batch: 040 ----
mean loss: 134.89
train mean loss: 131.90
epoch train time: 0:00:01.777558
elapsed time: 0:07:07.050589
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 22:44:58.913185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.79
 ---- batch: 020 ----
mean loss: 130.46
 ---- batch: 030 ----
mean loss: 127.20
 ---- batch: 040 ----
mean loss: 132.79
train mean loss: 131.18
epoch train time: 0:00:01.673104
elapsed time: 0:07:08.723841
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 22:45:00.586437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.52
 ---- batch: 020 ----
mean loss: 133.63
 ---- batch: 030 ----
mean loss: 130.99
 ---- batch: 040 ----
mean loss: 133.31
train mean loss: 131.75
epoch train time: 0:00:01.767963
elapsed time: 0:07:10.491948
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 22:45:02.354545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.84
 ---- batch: 020 ----
mean loss: 127.89
 ---- batch: 030 ----
mean loss: 131.52
 ---- batch: 040 ----
mean loss: 131.46
train mean loss: 129.50
epoch train time: 0:00:01.687883
elapsed time: 0:07:12.179976
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 22:45:04.042570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.28
 ---- batch: 020 ----
mean loss: 130.74
 ---- batch: 030 ----
mean loss: 130.21
 ---- batch: 040 ----
mean loss: 126.67
train mean loss: 129.46
epoch train time: 0:00:01.670515
elapsed time: 0:07:13.850694
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 22:45:05.713303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.42
 ---- batch: 020 ----
mean loss: 131.21
 ---- batch: 030 ----
mean loss: 133.76
 ---- batch: 040 ----
mean loss: 127.42
train mean loss: 131.13
epoch train time: 0:00:01.792434
elapsed time: 0:07:15.643294
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 22:45:07.505892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.43
 ---- batch: 020 ----
mean loss: 135.47
 ---- batch: 030 ----
mean loss: 132.71
 ---- batch: 040 ----
mean loss: 133.13
train mean loss: 131.88
epoch train time: 0:00:01.678877
elapsed time: 0:07:17.322364
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 22:45:09.184980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.55
 ---- batch: 020 ----
mean loss: 130.84
 ---- batch: 030 ----
mean loss: 134.56
 ---- batch: 040 ----
mean loss: 128.81
train mean loss: 130.35
epoch train time: 0:00:01.783358
elapsed time: 0:07:19.105897
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 22:45:10.968492
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.06
 ---- batch: 020 ----
mean loss: 132.94
 ---- batch: 030 ----
mean loss: 130.14
 ---- batch: 040 ----
mean loss: 132.71
train mean loss: 132.74
epoch train time: 0:00:01.679262
elapsed time: 0:07:20.785306
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 22:45:12.647913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.42
 ---- batch: 020 ----
mean loss: 129.45
 ---- batch: 030 ----
mean loss: 131.97
 ---- batch: 040 ----
mean loss: 128.76
train mean loss: 130.51
epoch train time: 0:00:01.775442
elapsed time: 0:07:22.560891
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 22:45:14.423482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.77
 ---- batch: 020 ----
mean loss: 132.18
 ---- batch: 030 ----
mean loss: 136.03
 ---- batch: 040 ----
mean loss: 130.27
train mean loss: 133.11
epoch train time: 0:00:01.678607
elapsed time: 0:07:24.243181
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_4/checkpoint.pth.tar
**** end time: 2019-09-20 22:45:16.105745 ****
