Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_0', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 14032
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 22:19:37.838425 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 22:19:37.848743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4821.20
 ---- batch: 020 ----
mean loss: 4638.66
 ---- batch: 030 ----
mean loss: 4401.78
 ---- batch: 040 ----
mean loss: 4119.84
 ---- batch: 050 ----
mean loss: 3875.08
 ---- batch: 060 ----
mean loss: 3613.76
 ---- batch: 070 ----
mean loss: 3457.28
 ---- batch: 080 ----
mean loss: 3296.99
 ---- batch: 090 ----
mean loss: 3181.48
 ---- batch: 100 ----
mean loss: 3099.21
 ---- batch: 110 ----
mean loss: 3022.79
train mean loss: 3753.95
epoch train time: 0:00:34.791772
elapsed time: 0:00:34.808452
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 22:20:12.646925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2905.30
 ---- batch: 020 ----
mean loss: 2817.30
 ---- batch: 030 ----
mean loss: 2792.02
 ---- batch: 040 ----
mean loss: 2719.21
 ---- batch: 050 ----
mean loss: 2705.40
 ---- batch: 060 ----
mean loss: 2657.80
 ---- batch: 070 ----
mean loss: 2579.25
 ---- batch: 080 ----
mean loss: 2578.10
 ---- batch: 090 ----
mean loss: 2523.47
 ---- batch: 100 ----
mean loss: 2476.18
 ---- batch: 110 ----
mean loss: 2418.76
train mean loss: 2647.35
epoch train time: 0:00:01.946634
elapsed time: 0:00:36.755346
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 22:20:14.594155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2413.32
 ---- batch: 020 ----
mean loss: 2381.10
 ---- batch: 030 ----
mean loss: 2382.70
 ---- batch: 040 ----
mean loss: 2328.71
 ---- batch: 050 ----
mean loss: 2299.02
 ---- batch: 060 ----
mean loss: 2262.90
 ---- batch: 070 ----
mean loss: 2262.81
 ---- batch: 080 ----
mean loss: 2200.05
 ---- batch: 090 ----
mean loss: 2179.81
 ---- batch: 100 ----
mean loss: 2157.41
 ---- batch: 110 ----
mean loss: 2086.78
train mean loss: 2265.79
epoch train time: 0:00:01.924460
elapsed time: 0:00:38.680381
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 22:20:16.519153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2095.57
 ---- batch: 020 ----
mean loss: 2040.48
 ---- batch: 030 ----
mean loss: 2070.90
 ---- batch: 040 ----
mean loss: 2048.77
 ---- batch: 050 ----
mean loss: 1983.69
 ---- batch: 060 ----
mean loss: 1982.10
 ---- batch: 070 ----
mean loss: 1967.60
 ---- batch: 080 ----
mean loss: 1977.70
 ---- batch: 090 ----
mean loss: 1918.37
 ---- batch: 100 ----
mean loss: 1904.79
 ---- batch: 110 ----
mean loss: 1893.72
train mean loss: 1987.09
epoch train time: 0:00:01.926723
elapsed time: 0:00:40.607637
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 22:20:18.446439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1888.44
 ---- batch: 020 ----
mean loss: 1839.24
 ---- batch: 030 ----
mean loss: 1813.99
 ---- batch: 040 ----
mean loss: 1802.89
 ---- batch: 050 ----
mean loss: 1791.07
 ---- batch: 060 ----
mean loss: 1750.99
 ---- batch: 070 ----
mean loss: 1718.90
 ---- batch: 080 ----
mean loss: 1705.03
 ---- batch: 090 ----
mean loss: 1712.96
 ---- batch: 100 ----
mean loss: 1687.21
 ---- batch: 110 ----
mean loss: 1683.60
train mean loss: 1759.93
epoch train time: 0:00:01.895520
elapsed time: 0:00:42.503708
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 22:20:20.342476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1668.82
 ---- batch: 020 ----
mean loss: 1648.20
 ---- batch: 030 ----
mean loss: 1626.58
 ---- batch: 040 ----
mean loss: 1640.05
 ---- batch: 050 ----
mean loss: 1575.49
 ---- batch: 060 ----
mean loss: 1569.61
 ---- batch: 070 ----
mean loss: 1534.49
 ---- batch: 080 ----
mean loss: 1525.08
 ---- batch: 090 ----
mean loss: 1521.70
 ---- batch: 100 ----
mean loss: 1527.06
 ---- batch: 110 ----
mean loss: 1494.77
train mean loss: 1573.47
epoch train time: 0:00:01.913338
elapsed time: 0:00:44.417584
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 22:20:22.256420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1495.00
 ---- batch: 020 ----
mean loss: 1473.93
 ---- batch: 030 ----
mean loss: 1446.05
 ---- batch: 040 ----
mean loss: 1452.19
 ---- batch: 050 ----
mean loss: 1465.31
 ---- batch: 060 ----
mean loss: 1401.55
 ---- batch: 070 ----
mean loss: 1409.61
 ---- batch: 080 ----
mean loss: 1387.26
 ---- batch: 090 ----
mean loss: 1395.46
 ---- batch: 100 ----
mean loss: 1380.16
 ---- batch: 110 ----
mean loss: 1392.96
train mean loss: 1425.44
epoch train time: 0:00:01.977643
elapsed time: 0:00:46.395824
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 22:20:24.234614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1328.78
 ---- batch: 020 ----
mean loss: 1347.56
 ---- batch: 030 ----
mean loss: 1323.60
 ---- batch: 040 ----
mean loss: 1304.87
 ---- batch: 050 ----
mean loss: 1317.01
 ---- batch: 060 ----
mean loss: 1297.38
 ---- batch: 070 ----
mean loss: 1288.49
 ---- batch: 080 ----
mean loss: 1276.68
 ---- batch: 090 ----
mean loss: 1285.22
 ---- batch: 100 ----
mean loss: 1286.34
 ---- batch: 110 ----
mean loss: 1232.72
train mean loss: 1297.97
epoch train time: 0:00:01.935776
elapsed time: 0:00:48.332226
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 22:20:26.171002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1231.62
 ---- batch: 020 ----
mean loss: 1245.81
 ---- batch: 030 ----
mean loss: 1210.01
 ---- batch: 040 ----
mean loss: 1193.50
 ---- batch: 050 ----
mean loss: 1208.87
 ---- batch: 060 ----
mean loss: 1214.87
 ---- batch: 070 ----
mean loss: 1200.05
 ---- batch: 080 ----
mean loss: 1191.40
 ---- batch: 090 ----
mean loss: 1172.07
 ---- batch: 100 ----
mean loss: 1184.48
 ---- batch: 110 ----
mean loss: 1168.84
train mean loss: 1200.22
epoch train time: 0:00:01.923975
elapsed time: 0:00:50.256852
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 22:20:28.095761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1144.53
 ---- batch: 020 ----
mean loss: 1140.33
 ---- batch: 030 ----
mean loss: 1125.86
 ---- batch: 040 ----
mean loss: 1122.97
 ---- batch: 050 ----
mean loss: 1134.12
 ---- batch: 060 ----
mean loss: 1122.60
 ---- batch: 070 ----
mean loss: 1117.67
 ---- batch: 080 ----
mean loss: 1119.27
 ---- batch: 090 ----
mean loss: 1104.29
 ---- batch: 100 ----
mean loss: 1086.11
 ---- batch: 110 ----
mean loss: 1110.07
train mean loss: 1120.27
epoch train time: 0:00:01.910923
elapsed time: 0:00:52.168441
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 22:20:30.007218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1081.67
 ---- batch: 020 ----
mean loss: 1077.89
 ---- batch: 030 ----
mean loss: 1071.55
 ---- batch: 040 ----
mean loss: 1080.31
 ---- batch: 050 ----
mean loss: 1052.47
 ---- batch: 060 ----
mean loss: 1061.25
 ---- batch: 070 ----
mean loss: 1036.50
 ---- batch: 080 ----
mean loss: 1046.29
 ---- batch: 090 ----
mean loss: 1049.04
 ---- batch: 100 ----
mean loss: 1048.00
 ---- batch: 110 ----
mean loss: 1025.97
train mean loss: 1056.54
epoch train time: 0:00:01.908168
elapsed time: 0:00:54.077163
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 22:20:31.915750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1031.22
 ---- batch: 020 ----
mean loss: 1015.49
 ---- batch: 030 ----
mean loss: 1026.69
 ---- batch: 040 ----
mean loss: 1010.37
 ---- batch: 050 ----
mean loss: 996.59
 ---- batch: 060 ----
mean loss: 1005.52
 ---- batch: 070 ----
mean loss: 1016.20
 ---- batch: 080 ----
mean loss: 1002.19
 ---- batch: 090 ----
mean loss: 993.19
 ---- batch: 100 ----
mean loss: 997.27
 ---- batch: 110 ----
mean loss: 983.86
train mean loss: 1007.62
epoch train time: 0:00:01.852278
elapsed time: 0:00:55.929804
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 22:20:33.768583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1005.47
 ---- batch: 020 ----
mean loss: 996.58
 ---- batch: 030 ----
mean loss: 989.72
 ---- batch: 040 ----
mean loss: 982.32
 ---- batch: 050 ----
mean loss: 965.46
 ---- batch: 060 ----
mean loss: 952.10
 ---- batch: 070 ----
mean loss: 975.28
 ---- batch: 080 ----
mean loss: 963.73
 ---- batch: 090 ----
mean loss: 957.60
 ---- batch: 100 ----
mean loss: 968.71
 ---- batch: 110 ----
mean loss: 948.16
train mean loss: 971.96
epoch train time: 0:00:01.919557
elapsed time: 0:00:57.849972
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 22:20:35.688817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.55
 ---- batch: 020 ----
mean loss: 934.08
 ---- batch: 030 ----
mean loss: 957.35
 ---- batch: 040 ----
mean loss: 943.57
 ---- batch: 050 ----
mean loss: 936.98
 ---- batch: 060 ----
mean loss: 947.57
 ---- batch: 070 ----
mean loss: 949.52
 ---- batch: 080 ----
mean loss: 937.48
 ---- batch: 090 ----
mean loss: 925.04
 ---- batch: 100 ----
mean loss: 935.66
 ---- batch: 110 ----
mean loss: 926.61
train mean loss: 938.77
epoch train time: 0:00:01.893421
elapsed time: 0:00:59.744055
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 22:20:37.582834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.82
 ---- batch: 020 ----
mean loss: 917.50
 ---- batch: 030 ----
mean loss: 941.39
 ---- batch: 040 ----
mean loss: 936.87
 ---- batch: 050 ----
mean loss: 917.04
 ---- batch: 060 ----
mean loss: 902.71
 ---- batch: 070 ----
mean loss: 913.08
 ---- batch: 080 ----
mean loss: 907.68
 ---- batch: 090 ----
mean loss: 911.70
 ---- batch: 100 ----
mean loss: 903.16
 ---- batch: 110 ----
mean loss: 929.78
train mean loss: 918.15
epoch train time: 0:00:01.915179
elapsed time: 0:01:01.659781
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 22:20:39.498602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.25
 ---- batch: 020 ----
mean loss: 913.47
 ---- batch: 030 ----
mean loss: 894.76
 ---- batch: 040 ----
mean loss: 898.87
 ---- batch: 050 ----
mean loss: 898.69
 ---- batch: 060 ----
mean loss: 905.80
 ---- batch: 070 ----
mean loss: 908.56
 ---- batch: 080 ----
mean loss: 893.09
 ---- batch: 090 ----
mean loss: 893.21
 ---- batch: 100 ----
mean loss: 906.99
 ---- batch: 110 ----
mean loss: 892.30
train mean loss: 902.56
epoch train time: 0:00:01.899466
elapsed time: 0:01:03.559836
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 22:20:41.398641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.17
 ---- batch: 020 ----
mean loss: 869.05
 ---- batch: 030 ----
mean loss: 887.98
 ---- batch: 040 ----
mean loss: 913.13
 ---- batch: 050 ----
mean loss: 902.03
 ---- batch: 060 ----
mean loss: 904.96
 ---- batch: 070 ----
mean loss: 907.71
 ---- batch: 080 ----
mean loss: 894.67
 ---- batch: 090 ----
mean loss: 876.99
 ---- batch: 100 ----
mean loss: 875.08
 ---- batch: 110 ----
mean loss: 885.77
train mean loss: 892.00
epoch train time: 0:00:01.881850
elapsed time: 0:01:05.442276
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 22:20:43.281077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.94
 ---- batch: 020 ----
mean loss: 886.24
 ---- batch: 030 ----
mean loss: 869.73
 ---- batch: 040 ----
mean loss: 880.51
 ---- batch: 050 ----
mean loss: 893.94
 ---- batch: 060 ----
mean loss: 869.07
 ---- batch: 070 ----
mean loss: 902.61
 ---- batch: 080 ----
mean loss: 874.27
 ---- batch: 090 ----
mean loss: 881.94
 ---- batch: 100 ----
mean loss: 886.30
 ---- batch: 110 ----
mean loss: 884.39
train mean loss: 882.34
epoch train time: 0:00:01.939537
elapsed time: 0:01:07.382401
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 22:20:45.221215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.29
 ---- batch: 020 ----
mean loss: 888.53
 ---- batch: 030 ----
mean loss: 876.17
 ---- batch: 040 ----
mean loss: 867.40
 ---- batch: 050 ----
mean loss: 869.05
 ---- batch: 060 ----
mean loss: 883.16
 ---- batch: 070 ----
mean loss: 871.66
 ---- batch: 080 ----
mean loss: 877.74
 ---- batch: 090 ----
mean loss: 880.48
 ---- batch: 100 ----
mean loss: 865.02
 ---- batch: 110 ----
mean loss: 889.99
train mean loss: 876.79
epoch train time: 0:00:01.886669
elapsed time: 0:01:09.269663
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 22:20:47.108544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.18
 ---- batch: 020 ----
mean loss: 881.75
 ---- batch: 030 ----
mean loss: 878.50
 ---- batch: 040 ----
mean loss: 872.38
 ---- batch: 050 ----
mean loss: 860.70
 ---- batch: 060 ----
mean loss: 886.88
 ---- batch: 070 ----
mean loss: 866.96
 ---- batch: 080 ----
mean loss: 874.98
 ---- batch: 090 ----
mean loss: 863.97
 ---- batch: 100 ----
mean loss: 873.88
 ---- batch: 110 ----
mean loss: 877.37
train mean loss: 872.35
epoch train time: 0:00:01.903537
elapsed time: 0:01:11.173842
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 22:20:49.012617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.01
 ---- batch: 020 ----
mean loss: 903.94
 ---- batch: 030 ----
mean loss: 878.01
 ---- batch: 040 ----
mean loss: 891.97
 ---- batch: 050 ----
mean loss: 870.05
 ---- batch: 060 ----
mean loss: 881.58
 ---- batch: 070 ----
mean loss: 873.90
 ---- batch: 080 ----
mean loss: 881.62
 ---- batch: 090 ----
mean loss: 851.75
 ---- batch: 100 ----
mean loss: 855.07
 ---- batch: 110 ----
mean loss: 854.72
train mean loss: 870.67
epoch train time: 0:00:01.900162
elapsed time: 0:01:13.074550
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 22:20:50.913059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.00
 ---- batch: 020 ----
mean loss: 878.72
 ---- batch: 030 ----
mean loss: 846.67
 ---- batch: 040 ----
mean loss: 877.00
 ---- batch: 050 ----
mean loss: 873.33
 ---- batch: 060 ----
mean loss: 863.98
 ---- batch: 070 ----
mean loss: 873.39
 ---- batch: 080 ----
mean loss: 860.83
 ---- batch: 090 ----
mean loss: 856.41
 ---- batch: 100 ----
mean loss: 848.73
 ---- batch: 110 ----
mean loss: 875.52
train mean loss: 865.40
epoch train time: 0:00:01.885551
elapsed time: 0:01:14.960402
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 22:20:52.799218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.98
 ---- batch: 020 ----
mean loss: 855.80
 ---- batch: 030 ----
mean loss: 845.77
 ---- batch: 040 ----
mean loss: 860.67
 ---- batch: 050 ----
mean loss: 896.15
 ---- batch: 060 ----
mean loss: 857.57
 ---- batch: 070 ----
mean loss: 886.59
 ---- batch: 080 ----
mean loss: 846.70
 ---- batch: 090 ----
mean loss: 863.77
 ---- batch: 100 ----
mean loss: 875.20
 ---- batch: 110 ----
mean loss: 863.76
train mean loss: 863.99
epoch train time: 0:00:01.864722
elapsed time: 0:01:16.825708
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 22:20:54.664529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.28
 ---- batch: 020 ----
mean loss: 863.77
 ---- batch: 030 ----
mean loss: 879.16
 ---- batch: 040 ----
mean loss: 860.59
 ---- batch: 050 ----
mean loss: 879.75
 ---- batch: 060 ----
mean loss: 859.45
 ---- batch: 070 ----
mean loss: 854.42
 ---- batch: 080 ----
mean loss: 876.23
 ---- batch: 090 ----
mean loss: 858.53
 ---- batch: 100 ----
mean loss: 865.83
 ---- batch: 110 ----
mean loss: 850.50
train mean loss: 863.61
epoch train time: 0:00:01.882837
elapsed time: 0:01:18.709147
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 22:20:56.547917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.50
 ---- batch: 020 ----
mean loss: 860.05
 ---- batch: 030 ----
mean loss: 849.12
 ---- batch: 040 ----
mean loss: 871.52
 ---- batch: 050 ----
mean loss: 860.35
 ---- batch: 060 ----
mean loss: 880.87
 ---- batch: 070 ----
mean loss: 838.03
 ---- batch: 080 ----
mean loss: 863.08
 ---- batch: 090 ----
mean loss: 872.32
 ---- batch: 100 ----
mean loss: 855.25
 ---- batch: 110 ----
mean loss: 876.22
train mean loss: 863.53
epoch train time: 0:00:01.876644
elapsed time: 0:01:20.586360
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 22:20:58.425182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.20
 ---- batch: 020 ----
mean loss: 864.96
 ---- batch: 030 ----
mean loss: 859.27
 ---- batch: 040 ----
mean loss: 856.93
 ---- batch: 050 ----
mean loss: 844.85
 ---- batch: 060 ----
mean loss: 871.83
 ---- batch: 070 ----
mean loss: 876.49
 ---- batch: 080 ----
mean loss: 841.27
 ---- batch: 090 ----
mean loss: 856.14
 ---- batch: 100 ----
mean loss: 852.62
 ---- batch: 110 ----
mean loss: 856.89
train mean loss: 858.98
epoch train time: 0:00:01.909611
elapsed time: 0:01:22.496568
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 22:21:00.335434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.96
 ---- batch: 020 ----
mean loss: 869.57
 ---- batch: 030 ----
mean loss: 875.48
 ---- batch: 040 ----
mean loss: 860.59
 ---- batch: 050 ----
mean loss: 857.26
 ---- batch: 060 ----
mean loss: 841.27
 ---- batch: 070 ----
mean loss: 851.94
 ---- batch: 080 ----
mean loss: 879.13
 ---- batch: 090 ----
mean loss: 873.14
 ---- batch: 100 ----
mean loss: 853.81
 ---- batch: 110 ----
mean loss: 856.12
train mean loss: 861.01
epoch train time: 0:00:01.885972
elapsed time: 0:01:24.383170
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 22:21:02.221988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.02
 ---- batch: 020 ----
mean loss: 845.40
 ---- batch: 030 ----
mean loss: 872.46
 ---- batch: 040 ----
mean loss: 881.87
 ---- batch: 050 ----
mean loss: 854.57
 ---- batch: 060 ----
mean loss: 855.13
 ---- batch: 070 ----
mean loss: 845.37
 ---- batch: 080 ----
mean loss: 860.13
 ---- batch: 090 ----
mean loss: 864.58
 ---- batch: 100 ----
mean loss: 851.76
 ---- batch: 110 ----
mean loss: 861.61
train mean loss: 858.36
epoch train time: 0:00:01.896334
elapsed time: 0:01:26.280189
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 22:21:04.119015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 820.15
 ---- batch: 020 ----
mean loss: 852.05
 ---- batch: 030 ----
mean loss: 873.32
 ---- batch: 040 ----
mean loss: 878.79
 ---- batch: 050 ----
mean loss: 875.32
 ---- batch: 060 ----
mean loss: 853.45
 ---- batch: 070 ----
mean loss: 860.75
 ---- batch: 080 ----
mean loss: 858.50
 ---- batch: 090 ----
mean loss: 837.32
 ---- batch: 100 ----
mean loss: 870.31
 ---- batch: 110 ----
mean loss: 854.63
train mean loss: 858.23
epoch train time: 0:00:01.899875
elapsed time: 0:01:28.180665
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 22:21:06.019473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.98
 ---- batch: 020 ----
mean loss: 836.26
 ---- batch: 030 ----
mean loss: 842.92
 ---- batch: 040 ----
mean loss: 857.46
 ---- batch: 050 ----
mean loss: 856.72
 ---- batch: 060 ----
mean loss: 851.15
 ---- batch: 070 ----
mean loss: 860.81
 ---- batch: 080 ----
mean loss: 870.76
 ---- batch: 090 ----
mean loss: 861.33
 ---- batch: 100 ----
mean loss: 862.58
 ---- batch: 110 ----
mean loss: 855.05
train mean loss: 856.01
epoch train time: 0:00:01.903433
elapsed time: 0:01:30.084729
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 22:21:07.923581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.71
 ---- batch: 020 ----
mean loss: 852.91
 ---- batch: 030 ----
mean loss: 860.44
 ---- batch: 040 ----
mean loss: 872.76
 ---- batch: 050 ----
mean loss: 858.45
 ---- batch: 060 ----
mean loss: 851.50
 ---- batch: 070 ----
mean loss: 817.82
 ---- batch: 080 ----
mean loss: 856.08
 ---- batch: 090 ----
mean loss: 861.26
 ---- batch: 100 ----
mean loss: 864.28
 ---- batch: 110 ----
mean loss: 869.04
train mean loss: 855.83
epoch train time: 0:00:01.905316
elapsed time: 0:01:31.990660
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 22:21:09.829473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.29
 ---- batch: 020 ----
mean loss: 845.73
 ---- batch: 030 ----
mean loss: 868.23
 ---- batch: 040 ----
mean loss: 879.92
 ---- batch: 050 ----
mean loss: 829.22
 ---- batch: 060 ----
mean loss: 843.27
 ---- batch: 070 ----
mean loss: 870.02
 ---- batch: 080 ----
mean loss: 857.91
 ---- batch: 090 ----
mean loss: 850.30
 ---- batch: 100 ----
mean loss: 857.35
 ---- batch: 110 ----
mean loss: 843.60
train mean loss: 854.82
epoch train time: 0:00:01.911829
elapsed time: 0:01:33.903087
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 22:21:11.741985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.70
 ---- batch: 020 ----
mean loss: 838.03
 ---- batch: 030 ----
mean loss: 868.41
 ---- batch: 040 ----
mean loss: 874.50
 ---- batch: 050 ----
mean loss: 862.18
 ---- batch: 060 ----
mean loss: 866.03
 ---- batch: 070 ----
mean loss: 838.42
 ---- batch: 080 ----
mean loss: 865.40
 ---- batch: 090 ----
mean loss: 852.08
 ---- batch: 100 ----
mean loss: 860.97
 ---- batch: 110 ----
mean loss: 841.08
train mean loss: 854.60
epoch train time: 0:00:01.893977
elapsed time: 0:01:35.797725
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 22:21:13.636512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.76
 ---- batch: 020 ----
mean loss: 857.98
 ---- batch: 030 ----
mean loss: 856.42
 ---- batch: 040 ----
mean loss: 850.24
 ---- batch: 050 ----
mean loss: 834.35
 ---- batch: 060 ----
mean loss: 848.51
 ---- batch: 070 ----
mean loss: 851.89
 ---- batch: 080 ----
mean loss: 867.67
 ---- batch: 090 ----
mean loss: 872.16
 ---- batch: 100 ----
mean loss: 865.32
 ---- batch: 110 ----
mean loss: 863.62
train mean loss: 855.10
epoch train time: 0:00:01.921684
elapsed time: 0:01:37.719973
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 22:21:15.558488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.34
 ---- batch: 020 ----
mean loss: 825.53
 ---- batch: 030 ----
mean loss: 844.79
 ---- batch: 040 ----
mean loss: 859.18
 ---- batch: 050 ----
mean loss: 854.98
 ---- batch: 060 ----
mean loss: 860.29
 ---- batch: 070 ----
mean loss: 844.79
 ---- batch: 080 ----
mean loss: 859.63
 ---- batch: 090 ----
mean loss: 867.00
 ---- batch: 100 ----
mean loss: 863.47
 ---- batch: 110 ----
mean loss: 849.72
train mean loss: 853.31
epoch train time: 0:00:01.857860
elapsed time: 0:01:39.578111
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 22:21:17.416922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.80
 ---- batch: 020 ----
mean loss: 875.18
 ---- batch: 030 ----
mean loss: 843.62
 ---- batch: 040 ----
mean loss: 853.95
 ---- batch: 050 ----
mean loss: 845.03
 ---- batch: 060 ----
mean loss: 847.89
 ---- batch: 070 ----
mean loss: 850.03
 ---- batch: 080 ----
mean loss: 861.66
 ---- batch: 090 ----
mean loss: 843.57
 ---- batch: 100 ----
mean loss: 844.92
 ---- batch: 110 ----
mean loss: 847.39
train mean loss: 853.75
epoch train time: 0:00:01.899241
elapsed time: 0:01:41.477963
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 22:21:19.316760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.89
 ---- batch: 020 ----
mean loss: 851.41
 ---- batch: 030 ----
mean loss: 863.85
 ---- batch: 040 ----
mean loss: 856.76
 ---- batch: 050 ----
mean loss: 836.73
 ---- batch: 060 ----
mean loss: 861.02
 ---- batch: 070 ----
mean loss: 845.40
 ---- batch: 080 ----
mean loss: 835.38
 ---- batch: 090 ----
mean loss: 871.76
 ---- batch: 100 ----
mean loss: 858.17
 ---- batch: 110 ----
mean loss: 834.80
train mean loss: 852.69
epoch train time: 0:00:01.875278
elapsed time: 0:01:43.353804
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 22:21:21.192612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.46
 ---- batch: 020 ----
mean loss: 864.17
 ---- batch: 030 ----
mean loss: 837.16
 ---- batch: 040 ----
mean loss: 849.29
 ---- batch: 050 ----
mean loss: 873.97
 ---- batch: 060 ----
mean loss: 841.26
 ---- batch: 070 ----
mean loss: 847.96
 ---- batch: 080 ----
mean loss: 839.32
 ---- batch: 090 ----
mean loss: 837.37
 ---- batch: 100 ----
mean loss: 866.16
 ---- batch: 110 ----
mean loss: 859.53
train mean loss: 852.38
epoch train time: 0:00:01.908681
elapsed time: 0:01:45.263109
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 22:21:23.101929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.66
 ---- batch: 020 ----
mean loss: 841.41
 ---- batch: 030 ----
mean loss: 863.32
 ---- batch: 040 ----
mean loss: 866.78
 ---- batch: 050 ----
mean loss: 825.16
 ---- batch: 060 ----
mean loss: 838.34
 ---- batch: 070 ----
mean loss: 864.62
 ---- batch: 080 ----
mean loss: 864.38
 ---- batch: 090 ----
mean loss: 833.96
 ---- batch: 100 ----
mean loss: 847.59
 ---- batch: 110 ----
mean loss: 850.46
train mean loss: 851.03
epoch train time: 0:00:01.920142
elapsed time: 0:01:47.183874
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 22:21:25.022712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.36
 ---- batch: 020 ----
mean loss: 852.65
 ---- batch: 030 ----
mean loss: 850.86
 ---- batch: 040 ----
mean loss: 853.65
 ---- batch: 050 ----
mean loss: 839.97
 ---- batch: 060 ----
mean loss: 842.66
 ---- batch: 070 ----
mean loss: 851.11
 ---- batch: 080 ----
mean loss: 859.64
 ---- batch: 090 ----
mean loss: 853.18
 ---- batch: 100 ----
mean loss: 854.52
 ---- batch: 110 ----
mean loss: 856.67
train mean loss: 851.61
epoch train time: 0:00:01.913249
elapsed time: 0:01:49.097718
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 22:21:26.936547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.89
 ---- batch: 020 ----
mean loss: 864.33
 ---- batch: 030 ----
mean loss: 838.82
 ---- batch: 040 ----
mean loss: 866.33
 ---- batch: 050 ----
mean loss: 866.48
 ---- batch: 060 ----
mean loss: 849.45
 ---- batch: 070 ----
mean loss: 841.30
 ---- batch: 080 ----
mean loss: 853.09
 ---- batch: 090 ----
mean loss: 844.45
 ---- batch: 100 ----
mean loss: 848.00
 ---- batch: 110 ----
mean loss: 853.56
train mean loss: 851.49
epoch train time: 0:00:01.905912
elapsed time: 0:01:51.004264
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 22:21:28.843083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.99
 ---- batch: 020 ----
mean loss: 830.68
 ---- batch: 030 ----
mean loss: 845.19
 ---- batch: 040 ----
mean loss: 852.87
 ---- batch: 050 ----
mean loss: 844.08
 ---- batch: 060 ----
mean loss: 855.46
 ---- batch: 070 ----
mean loss: 854.95
 ---- batch: 080 ----
mean loss: 874.45
 ---- batch: 090 ----
mean loss: 863.73
 ---- batch: 100 ----
mean loss: 846.89
 ---- batch: 110 ----
mean loss: 828.71
train mean loss: 850.40
epoch train time: 0:00:01.911211
elapsed time: 0:01:52.916059
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 22:21:30.754878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.24
 ---- batch: 020 ----
mean loss: 839.18
 ---- batch: 030 ----
mean loss: 851.33
 ---- batch: 040 ----
mean loss: 843.69
 ---- batch: 050 ----
mean loss: 872.25
 ---- batch: 060 ----
mean loss: 837.92
 ---- batch: 070 ----
mean loss: 851.25
 ---- batch: 080 ----
mean loss: 862.79
 ---- batch: 090 ----
mean loss: 848.89
 ---- batch: 100 ----
mean loss: 850.19
 ---- batch: 110 ----
mean loss: 849.04
train mean loss: 850.54
epoch train time: 0:00:01.868926
elapsed time: 0:01:54.785666
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 22:21:32.624469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.18
 ---- batch: 020 ----
mean loss: 853.01
 ---- batch: 030 ----
mean loss: 860.41
 ---- batch: 040 ----
mean loss: 844.41
 ---- batch: 050 ----
mean loss: 851.65
 ---- batch: 060 ----
mean loss: 848.91
 ---- batch: 070 ----
mean loss: 816.21
 ---- batch: 080 ----
mean loss: 864.41
 ---- batch: 090 ----
mean loss: 849.17
 ---- batch: 100 ----
mean loss: 856.50
 ---- batch: 110 ----
mean loss: 855.09
train mean loss: 850.60
epoch train time: 0:00:01.914638
elapsed time: 0:01:56.700883
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 22:21:34.539716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.08
 ---- batch: 020 ----
mean loss: 839.71
 ---- batch: 030 ----
mean loss: 855.41
 ---- batch: 040 ----
mean loss: 861.31
 ---- batch: 050 ----
mean loss: 837.93
 ---- batch: 060 ----
mean loss: 872.77
 ---- batch: 070 ----
mean loss: 858.19
 ---- batch: 080 ----
mean loss: 840.85
 ---- batch: 090 ----
mean loss: 825.07
 ---- batch: 100 ----
mean loss: 837.18
 ---- batch: 110 ----
mean loss: 848.45
train mean loss: 849.47
epoch train time: 0:00:01.892696
elapsed time: 0:01:58.594197
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 22:21:36.433017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 825.93
 ---- batch: 020 ----
mean loss: 872.68
 ---- batch: 030 ----
mean loss: 864.51
 ---- batch: 040 ----
mean loss: 852.74
 ---- batch: 050 ----
mean loss: 839.23
 ---- batch: 060 ----
mean loss: 847.08
 ---- batch: 070 ----
mean loss: 842.16
 ---- batch: 080 ----
mean loss: 848.65
 ---- batch: 090 ----
mean loss: 845.06
 ---- batch: 100 ----
mean loss: 847.56
 ---- batch: 110 ----
mean loss: 860.34
train mean loss: 849.13
epoch train time: 0:00:01.875058
elapsed time: 0:02:00.469860
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 22:21:38.308642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.91
 ---- batch: 020 ----
mean loss: 829.51
 ---- batch: 030 ----
mean loss: 860.42
 ---- batch: 040 ----
mean loss: 861.21
 ---- batch: 050 ----
mean loss: 832.32
 ---- batch: 060 ----
mean loss: 830.86
 ---- batch: 070 ----
mean loss: 865.13
 ---- batch: 080 ----
mean loss: 827.47
 ---- batch: 090 ----
mean loss: 872.17
 ---- batch: 100 ----
mean loss: 849.71
 ---- batch: 110 ----
mean loss: 861.22
train mean loss: 849.13
epoch train time: 0:00:01.904131
elapsed time: 0:02:02.374565
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 22:21:40.213356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.75
 ---- batch: 020 ----
mean loss: 844.08
 ---- batch: 030 ----
mean loss: 835.83
 ---- batch: 040 ----
mean loss: 840.90
 ---- batch: 050 ----
mean loss: 850.02
 ---- batch: 060 ----
mean loss: 845.38
 ---- batch: 070 ----
mean loss: 868.83
 ---- batch: 080 ----
mean loss: 854.59
 ---- batch: 090 ----
mean loss: 869.23
 ---- batch: 100 ----
mean loss: 833.66
 ---- batch: 110 ----
mean loss: 842.06
train mean loss: 848.41
epoch train time: 0:00:01.920049
elapsed time: 0:02:04.295174
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 22:21:42.134010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.38
 ---- batch: 020 ----
mean loss: 849.92
 ---- batch: 030 ----
mean loss: 855.80
 ---- batch: 040 ----
mean loss: 860.13
 ---- batch: 050 ----
mean loss: 838.46
 ---- batch: 060 ----
mean loss: 858.93
 ---- batch: 070 ----
mean loss: 870.61
 ---- batch: 080 ----
mean loss: 838.33
 ---- batch: 090 ----
mean loss: 836.42
 ---- batch: 100 ----
mean loss: 855.05
 ---- batch: 110 ----
mean loss: 831.35
train mean loss: 849.02
epoch train time: 0:00:01.902916
elapsed time: 0:02:06.198699
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 22:21:44.037515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.84
 ---- batch: 020 ----
mean loss: 853.36
 ---- batch: 030 ----
mean loss: 856.77
 ---- batch: 040 ----
mean loss: 837.63
 ---- batch: 050 ----
mean loss: 841.45
 ---- batch: 060 ----
mean loss: 851.60
 ---- batch: 070 ----
mean loss: 832.79
 ---- batch: 080 ----
mean loss: 849.03
 ---- batch: 090 ----
mean loss: 851.00
 ---- batch: 100 ----
mean loss: 844.88
 ---- batch: 110 ----
mean loss: 849.27
train mean loss: 848.37
epoch train time: 0:00:01.908682
elapsed time: 0:02:08.107998
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 22:21:45.946778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.59
 ---- batch: 020 ----
mean loss: 849.83
 ---- batch: 030 ----
mean loss: 865.07
 ---- batch: 040 ----
mean loss: 858.50
 ---- batch: 050 ----
mean loss: 845.99
 ---- batch: 060 ----
mean loss: 847.19
 ---- batch: 070 ----
mean loss: 863.87
 ---- batch: 080 ----
mean loss: 860.47
 ---- batch: 090 ----
mean loss: 838.22
 ---- batch: 100 ----
mean loss: 820.95
 ---- batch: 110 ----
mean loss: 843.23
train mean loss: 848.69
epoch train time: 0:00:01.902523
elapsed time: 0:02:10.011098
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 22:21:47.850043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.82
 ---- batch: 020 ----
mean loss: 846.07
 ---- batch: 030 ----
mean loss: 843.97
 ---- batch: 040 ----
mean loss: 842.22
 ---- batch: 050 ----
mean loss: 850.64
 ---- batch: 060 ----
mean loss: 868.97
 ---- batch: 070 ----
mean loss: 847.94
 ---- batch: 080 ----
mean loss: 811.29
 ---- batch: 090 ----
mean loss: 835.23
 ---- batch: 100 ----
mean loss: 862.90
 ---- batch: 110 ----
mean loss: 853.10
train mean loss: 847.83
epoch train time: 0:00:01.920941
elapsed time: 0:02:11.932749
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 22:21:49.771569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.10
 ---- batch: 020 ----
mean loss: 838.76
 ---- batch: 030 ----
mean loss: 841.18
 ---- batch: 040 ----
mean loss: 827.50
 ---- batch: 050 ----
mean loss: 844.81
 ---- batch: 060 ----
mean loss: 860.29
 ---- batch: 070 ----
mean loss: 825.32
 ---- batch: 080 ----
mean loss: 860.65
 ---- batch: 090 ----
mean loss: 842.65
 ---- batch: 100 ----
mean loss: 854.27
 ---- batch: 110 ----
mean loss: 867.19
train mean loss: 847.77
epoch train time: 0:00:01.880224
elapsed time: 0:02:13.813578
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 22:21:51.652383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.56
 ---- batch: 020 ----
mean loss: 862.68
 ---- batch: 030 ----
mean loss: 856.30
 ---- batch: 040 ----
mean loss: 844.68
 ---- batch: 050 ----
mean loss: 834.28
 ---- batch: 060 ----
mean loss: 846.68
 ---- batch: 070 ----
mean loss: 862.05
 ---- batch: 080 ----
mean loss: 853.11
 ---- batch: 090 ----
mean loss: 833.17
 ---- batch: 100 ----
mean loss: 841.48
 ---- batch: 110 ----
mean loss: 843.26
train mean loss: 847.55
epoch train time: 0:00:01.919004
elapsed time: 0:02:15.733144
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 22:21:53.571970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.29
 ---- batch: 020 ----
mean loss: 852.65
 ---- batch: 030 ----
mean loss: 850.83
 ---- batch: 040 ----
mean loss: 846.44
 ---- batch: 050 ----
mean loss: 861.70
 ---- batch: 060 ----
mean loss: 820.65
 ---- batch: 070 ----
mean loss: 862.50
 ---- batch: 080 ----
mean loss: 813.97
 ---- batch: 090 ----
mean loss: 837.28
 ---- batch: 100 ----
mean loss: 847.05
 ---- batch: 110 ----
mean loss: 861.23
train mean loss: 846.57
epoch train time: 0:00:01.899966
elapsed time: 0:02:17.633722
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 22:21:55.472614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.31
 ---- batch: 020 ----
mean loss: 841.59
 ---- batch: 030 ----
mean loss: 846.72
 ---- batch: 040 ----
mean loss: 858.66
 ---- batch: 050 ----
mean loss: 846.49
 ---- batch: 060 ----
mean loss: 841.35
 ---- batch: 070 ----
mean loss: 849.10
 ---- batch: 080 ----
mean loss: 827.93
 ---- batch: 090 ----
mean loss: 853.36
 ---- batch: 100 ----
mean loss: 847.54
 ---- batch: 110 ----
mean loss: 851.95
train mean loss: 847.40
epoch train time: 0:00:01.888967
elapsed time: 0:02:19.523420
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 22:21:57.361952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.71
 ---- batch: 020 ----
mean loss: 854.36
 ---- batch: 030 ----
mean loss: 838.00
 ---- batch: 040 ----
mean loss: 836.07
 ---- batch: 050 ----
mean loss: 842.97
 ---- batch: 060 ----
mean loss: 846.87
 ---- batch: 070 ----
mean loss: 848.35
 ---- batch: 080 ----
mean loss: 852.33
 ---- batch: 090 ----
mean loss: 839.08
 ---- batch: 100 ----
mean loss: 838.66
 ---- batch: 110 ----
mean loss: 849.78
train mean loss: 847.66
epoch train time: 0:00:01.927667
elapsed time: 0:02:21.451384
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 22:21:59.290208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.13
 ---- batch: 020 ----
mean loss: 852.99
 ---- batch: 030 ----
mean loss: 825.48
 ---- batch: 040 ----
mean loss: 840.29
 ---- batch: 050 ----
mean loss: 835.96
 ---- batch: 060 ----
mean loss: 839.78
 ---- batch: 070 ----
mean loss: 835.93
 ---- batch: 080 ----
mean loss: 806.37
 ---- batch: 090 ----
mean loss: 859.42
 ---- batch: 100 ----
mean loss: 836.68
 ---- batch: 110 ----
mean loss: 795.76
train mean loss: 832.52
epoch train time: 0:00:01.927620
elapsed time: 0:02:23.379590
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 22:22:01.218413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.93
 ---- batch: 020 ----
mean loss: 696.64
 ---- batch: 030 ----
mean loss: 657.26
 ---- batch: 040 ----
mean loss: 602.49
 ---- batch: 050 ----
mean loss: 560.77
 ---- batch: 060 ----
mean loss: 520.38
 ---- batch: 070 ----
mean loss: 498.67
 ---- batch: 080 ----
mean loss: 486.89
 ---- batch: 090 ----
mean loss: 481.75
 ---- batch: 100 ----
mean loss: 467.20
 ---- batch: 110 ----
mean loss: 449.79
train mean loss: 558.68
epoch train time: 0:00:01.885934
elapsed time: 0:02:25.266192
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 22:22:03.105006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.93
 ---- batch: 020 ----
mean loss: 433.49
 ---- batch: 030 ----
mean loss: 419.18
 ---- batch: 040 ----
mean loss: 407.23
 ---- batch: 050 ----
mean loss: 398.25
 ---- batch: 060 ----
mean loss: 397.84
 ---- batch: 070 ----
mean loss: 383.96
 ---- batch: 080 ----
mean loss: 394.74
 ---- batch: 090 ----
mean loss: 409.77
 ---- batch: 100 ----
mean loss: 387.30
 ---- batch: 110 ----
mean loss: 384.17
train mean loss: 405.76
epoch train time: 0:00:01.937906
elapsed time: 0:02:27.204686
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 22:22:05.043516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.70
 ---- batch: 020 ----
mean loss: 376.60
 ---- batch: 030 ----
mean loss: 379.96
 ---- batch: 040 ----
mean loss: 367.87
 ---- batch: 050 ----
mean loss: 371.72
 ---- batch: 060 ----
mean loss: 364.86
 ---- batch: 070 ----
mean loss: 354.50
 ---- batch: 080 ----
mean loss: 362.97
 ---- batch: 090 ----
mean loss: 357.13
 ---- batch: 100 ----
mean loss: 349.51
 ---- batch: 110 ----
mean loss: 351.07
train mean loss: 364.55
epoch train time: 0:00:01.907609
elapsed time: 0:02:29.112925
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 22:22:06.951735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.50
 ---- batch: 020 ----
mean loss: 349.56
 ---- batch: 030 ----
mean loss: 337.54
 ---- batch: 040 ----
mean loss: 334.61
 ---- batch: 050 ----
mean loss: 343.16
 ---- batch: 060 ----
mean loss: 334.48
 ---- batch: 070 ----
mean loss: 336.73
 ---- batch: 080 ----
mean loss: 329.07
 ---- batch: 090 ----
mean loss: 349.44
 ---- batch: 100 ----
mean loss: 321.66
 ---- batch: 110 ----
mean loss: 337.79
train mean loss: 336.93
epoch train time: 0:00:01.896092
elapsed time: 0:02:31.009612
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 22:22:08.848411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.72
 ---- batch: 020 ----
mean loss: 318.61
 ---- batch: 030 ----
mean loss: 328.40
 ---- batch: 040 ----
mean loss: 328.48
 ---- batch: 050 ----
mean loss: 324.91
 ---- batch: 060 ----
mean loss: 331.01
 ---- batch: 070 ----
mean loss: 309.66
 ---- batch: 080 ----
mean loss: 325.76
 ---- batch: 090 ----
mean loss: 323.26
 ---- batch: 100 ----
mean loss: 314.78
 ---- batch: 110 ----
mean loss: 313.69
train mean loss: 322.54
epoch train time: 0:00:01.915385
elapsed time: 0:02:32.925562
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 22:22:10.764360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.04
 ---- batch: 020 ----
mean loss: 300.81
 ---- batch: 030 ----
mean loss: 303.19
 ---- batch: 040 ----
mean loss: 313.54
 ---- batch: 050 ----
mean loss: 302.61
 ---- batch: 060 ----
mean loss: 309.02
 ---- batch: 070 ----
mean loss: 315.49
 ---- batch: 080 ----
mean loss: 297.62
 ---- batch: 090 ----
mean loss: 307.14
 ---- batch: 100 ----
mean loss: 309.11
 ---- batch: 110 ----
mean loss: 311.76
train mean loss: 307.44
epoch train time: 0:00:01.881013
elapsed time: 0:02:34.807285
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 22:22:12.646099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.06
 ---- batch: 020 ----
mean loss: 295.36
 ---- batch: 030 ----
mean loss: 296.19
 ---- batch: 040 ----
mean loss: 281.00
 ---- batch: 050 ----
mean loss: 294.44
 ---- batch: 060 ----
mean loss: 300.65
 ---- batch: 070 ----
mean loss: 301.02
 ---- batch: 080 ----
mean loss: 299.19
 ---- batch: 090 ----
mean loss: 321.77
 ---- batch: 100 ----
mean loss: 296.85
 ---- batch: 110 ----
mean loss: 283.12
train mean loss: 296.68
epoch train time: 0:00:01.930514
elapsed time: 0:02:36.738425
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 22:22:14.577233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.88
 ---- batch: 020 ----
mean loss: 284.88
 ---- batch: 030 ----
mean loss: 293.11
 ---- batch: 040 ----
mean loss: 288.75
 ---- batch: 050 ----
mean loss: 295.76
 ---- batch: 060 ----
mean loss: 285.39
 ---- batch: 070 ----
mean loss: 296.33
 ---- batch: 080 ----
mean loss: 281.40
 ---- batch: 090 ----
mean loss: 275.33
 ---- batch: 100 ----
mean loss: 281.84
 ---- batch: 110 ----
mean loss: 293.98
train mean loss: 288.61
epoch train time: 0:00:01.890272
elapsed time: 0:02:38.629305
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 22:22:16.468149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.45
 ---- batch: 020 ----
mean loss: 279.63
 ---- batch: 030 ----
mean loss: 289.74
 ---- batch: 040 ----
mean loss: 279.62
 ---- batch: 050 ----
mean loss: 271.70
 ---- batch: 060 ----
mean loss: 286.78
 ---- batch: 070 ----
mean loss: 281.53
 ---- batch: 080 ----
mean loss: 279.93
 ---- batch: 090 ----
mean loss: 276.41
 ---- batch: 100 ----
mean loss: 277.71
 ---- batch: 110 ----
mean loss: 276.08
train mean loss: 280.03
epoch train time: 0:00:01.899280
elapsed time: 0:02:40.529179
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 22:22:18.367975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.63
 ---- batch: 020 ----
mean loss: 277.12
 ---- batch: 030 ----
mean loss: 259.66
 ---- batch: 040 ----
mean loss: 276.48
 ---- batch: 050 ----
mean loss: 273.67
 ---- batch: 060 ----
mean loss: 271.25
 ---- batch: 070 ----
mean loss: 285.82
 ---- batch: 080 ----
mean loss: 270.60
 ---- batch: 090 ----
mean loss: 276.28
 ---- batch: 100 ----
mean loss: 279.99
 ---- batch: 110 ----
mean loss: 280.15
train mean loss: 276.43
epoch train time: 0:00:01.890418
elapsed time: 0:02:42.420158
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 22:22:20.258929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.63
 ---- batch: 020 ----
mean loss: 269.40
 ---- batch: 030 ----
mean loss: 261.82
 ---- batch: 040 ----
mean loss: 280.17
 ---- batch: 050 ----
mean loss: 277.90
 ---- batch: 060 ----
mean loss: 284.26
 ---- batch: 070 ----
mean loss: 268.09
 ---- batch: 080 ----
mean loss: 260.53
 ---- batch: 090 ----
mean loss: 268.81
 ---- batch: 100 ----
mean loss: 266.51
 ---- batch: 110 ----
mean loss: 276.04
train mean loss: 270.52
epoch train time: 0:00:01.886180
elapsed time: 0:02:44.306894
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 22:22:22.145705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.85
 ---- batch: 020 ----
mean loss: 263.87
 ---- batch: 030 ----
mean loss: 264.45
 ---- batch: 040 ----
mean loss: 257.68
 ---- batch: 050 ----
mean loss: 256.24
 ---- batch: 060 ----
mean loss: 271.65
 ---- batch: 070 ----
mean loss: 265.20
 ---- batch: 080 ----
mean loss: 259.58
 ---- batch: 090 ----
mean loss: 260.56
 ---- batch: 100 ----
mean loss: 260.20
 ---- batch: 110 ----
mean loss: 263.43
train mean loss: 263.35
epoch train time: 0:00:01.936460
elapsed time: 0:02:46.243925
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 22:22:24.082731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.15
 ---- batch: 020 ----
mean loss: 263.63
 ---- batch: 030 ----
mean loss: 268.54
 ---- batch: 040 ----
mean loss: 263.08
 ---- batch: 050 ----
mean loss: 258.26
 ---- batch: 060 ----
mean loss: 255.33
 ---- batch: 070 ----
mean loss: 265.07
 ---- batch: 080 ----
mean loss: 252.17
 ---- batch: 090 ----
mean loss: 258.33
 ---- batch: 100 ----
mean loss: 260.75
 ---- batch: 110 ----
mean loss: 264.38
train mean loss: 260.64
epoch train time: 0:00:01.873008
elapsed time: 0:02:48.117503
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 22:22:25.956368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.33
 ---- batch: 020 ----
mean loss: 250.04
 ---- batch: 030 ----
mean loss: 256.22
 ---- batch: 040 ----
mean loss: 251.27
 ---- batch: 050 ----
mean loss: 235.82
 ---- batch: 060 ----
mean loss: 251.80
 ---- batch: 070 ----
mean loss: 261.79
 ---- batch: 080 ----
mean loss: 263.39
 ---- batch: 090 ----
mean loss: 256.79
 ---- batch: 100 ----
mean loss: 255.97
 ---- batch: 110 ----
mean loss: 252.32
train mean loss: 253.89
epoch train time: 0:00:01.889136
elapsed time: 0:02:50.007311
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 22:22:27.846131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.09
 ---- batch: 020 ----
mean loss: 242.14
 ---- batch: 030 ----
mean loss: 250.26
 ---- batch: 040 ----
mean loss: 250.68
 ---- batch: 050 ----
mean loss: 259.06
 ---- batch: 060 ----
mean loss: 243.35
 ---- batch: 070 ----
mean loss: 243.78
 ---- batch: 080 ----
mean loss: 242.19
 ---- batch: 090 ----
mean loss: 242.57
 ---- batch: 100 ----
mean loss: 253.15
 ---- batch: 110 ----
mean loss: 243.22
train mean loss: 246.86
epoch train time: 0:00:01.918776
elapsed time: 0:02:51.926668
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 22:22:29.765526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.75
 ---- batch: 020 ----
mean loss: 247.55
 ---- batch: 030 ----
mean loss: 244.49
 ---- batch: 040 ----
mean loss: 239.96
 ---- batch: 050 ----
mean loss: 242.46
 ---- batch: 060 ----
mean loss: 251.75
 ---- batch: 070 ----
mean loss: 239.77
 ---- batch: 080 ----
mean loss: 249.15
 ---- batch: 090 ----
mean loss: 243.42
 ---- batch: 100 ----
mean loss: 241.06
 ---- batch: 110 ----
mean loss: 242.06
train mean loss: 244.52
epoch train time: 0:00:01.896213
elapsed time: 0:02:53.823557
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 22:22:31.662401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.20
 ---- batch: 020 ----
mean loss: 241.12
 ---- batch: 030 ----
mean loss: 237.87
 ---- batch: 040 ----
mean loss: 236.92
 ---- batch: 050 ----
mean loss: 232.54
 ---- batch: 060 ----
mean loss: 242.56
 ---- batch: 070 ----
mean loss: 242.45
 ---- batch: 080 ----
mean loss: 243.85
 ---- batch: 090 ----
mean loss: 241.29
 ---- batch: 100 ----
mean loss: 240.31
 ---- batch: 110 ----
mean loss: 237.84
train mean loss: 240.99
epoch train time: 0:00:01.924029
elapsed time: 0:02:55.748203
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 22:22:33.587057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.01
 ---- batch: 020 ----
mean loss: 235.64
 ---- batch: 030 ----
mean loss: 247.62
 ---- batch: 040 ----
mean loss: 245.50
 ---- batch: 050 ----
mean loss: 244.35
 ---- batch: 060 ----
mean loss: 228.95
 ---- batch: 070 ----
mean loss: 231.47
 ---- batch: 080 ----
mean loss: 234.49
 ---- batch: 090 ----
mean loss: 238.78
 ---- batch: 100 ----
mean loss: 232.36
 ---- batch: 110 ----
mean loss: 242.21
train mean loss: 237.65
epoch train time: 0:00:01.923393
elapsed time: 0:02:57.672214
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 22:22:35.511013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.47
 ---- batch: 020 ----
mean loss: 231.83
 ---- batch: 030 ----
mean loss: 238.40
 ---- batch: 040 ----
mean loss: 242.46
 ---- batch: 050 ----
mean loss: 234.42
 ---- batch: 060 ----
mean loss: 239.43
 ---- batch: 070 ----
mean loss: 240.62
 ---- batch: 080 ----
mean loss: 237.96
 ---- batch: 090 ----
mean loss: 234.40
 ---- batch: 100 ----
mean loss: 222.43
 ---- batch: 110 ----
mean loss: 235.20
train mean loss: 235.45
epoch train time: 0:00:01.923157
elapsed time: 0:02:59.595929
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 22:22:37.434751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.94
 ---- batch: 020 ----
mean loss: 235.10
 ---- batch: 030 ----
mean loss: 221.46
 ---- batch: 040 ----
mean loss: 228.06
 ---- batch: 050 ----
mean loss: 234.70
 ---- batch: 060 ----
mean loss: 233.28
 ---- batch: 070 ----
mean loss: 230.90
 ---- batch: 080 ----
mean loss: 228.09
 ---- batch: 090 ----
mean loss: 237.19
 ---- batch: 100 ----
mean loss: 225.15
 ---- batch: 110 ----
mean loss: 223.06
train mean loss: 229.46
epoch train time: 0:00:01.901164
elapsed time: 0:03:01.497720
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 22:22:39.336540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.78
 ---- batch: 020 ----
mean loss: 224.11
 ---- batch: 030 ----
mean loss: 215.57
 ---- batch: 040 ----
mean loss: 230.20
 ---- batch: 050 ----
mean loss: 227.10
 ---- batch: 060 ----
mean loss: 237.21
 ---- batch: 070 ----
mean loss: 231.03
 ---- batch: 080 ----
mean loss: 230.26
 ---- batch: 090 ----
mean loss: 226.40
 ---- batch: 100 ----
mean loss: 226.27
 ---- batch: 110 ----
mean loss: 226.90
train mean loss: 226.62
epoch train time: 0:00:01.895464
elapsed time: 0:03:03.393812
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 22:22:41.232642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.26
 ---- batch: 020 ----
mean loss: 226.95
 ---- batch: 030 ----
mean loss: 215.43
 ---- batch: 040 ----
mean loss: 232.40
 ---- batch: 050 ----
mean loss: 221.05
 ---- batch: 060 ----
mean loss: 222.90
 ---- batch: 070 ----
mean loss: 228.92
 ---- batch: 080 ----
mean loss: 225.35
 ---- batch: 090 ----
mean loss: 227.80
 ---- batch: 100 ----
mean loss: 209.88
 ---- batch: 110 ----
mean loss: 236.64
train mean loss: 224.02
epoch train time: 0:00:01.894113
elapsed time: 0:03:05.288523
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 22:22:43.127321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.91
 ---- batch: 020 ----
mean loss: 226.15
 ---- batch: 030 ----
mean loss: 214.07
 ---- batch: 040 ----
mean loss: 216.49
 ---- batch: 050 ----
mean loss: 221.24
 ---- batch: 060 ----
mean loss: 228.76
 ---- batch: 070 ----
mean loss: 226.78
 ---- batch: 080 ----
mean loss: 215.59
 ---- batch: 090 ----
mean loss: 220.95
 ---- batch: 100 ----
mean loss: 227.57
 ---- batch: 110 ----
mean loss: 214.98
train mean loss: 220.91
epoch train time: 0:00:01.902590
elapsed time: 0:03:07.191709
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 22:22:45.030625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.88
 ---- batch: 020 ----
mean loss: 216.41
 ---- batch: 030 ----
mean loss: 212.61
 ---- batch: 040 ----
mean loss: 215.20
 ---- batch: 050 ----
mean loss: 218.61
 ---- batch: 060 ----
mean loss: 210.63
 ---- batch: 070 ----
mean loss: 212.62
 ---- batch: 080 ----
mean loss: 233.82
 ---- batch: 090 ----
mean loss: 224.13
 ---- batch: 100 ----
mean loss: 204.31
 ---- batch: 110 ----
mean loss: 221.98
train mean loss: 216.16
epoch train time: 0:00:01.910289
elapsed time: 0:03:09.102694
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 22:22:46.941503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.00
 ---- batch: 020 ----
mean loss: 219.46
 ---- batch: 030 ----
mean loss: 212.17
 ---- batch: 040 ----
mean loss: 216.64
 ---- batch: 050 ----
mean loss: 216.31
 ---- batch: 060 ----
mean loss: 213.88
 ---- batch: 070 ----
mean loss: 217.11
 ---- batch: 080 ----
mean loss: 206.46
 ---- batch: 090 ----
mean loss: 215.60
 ---- batch: 100 ----
mean loss: 213.62
 ---- batch: 110 ----
mean loss: 215.77
train mean loss: 213.89
epoch train time: 0:00:01.919294
elapsed time: 0:03:11.022581
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 22:22:48.861422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.19
 ---- batch: 020 ----
mean loss: 215.17
 ---- batch: 030 ----
mean loss: 206.53
 ---- batch: 040 ----
mean loss: 208.61
 ---- batch: 050 ----
mean loss: 214.88
 ---- batch: 060 ----
mean loss: 212.33
 ---- batch: 070 ----
mean loss: 210.94
 ---- batch: 080 ----
mean loss: 211.10
 ---- batch: 090 ----
mean loss: 207.54
 ---- batch: 100 ----
mean loss: 213.41
 ---- batch: 110 ----
mean loss: 211.07
train mean loss: 211.30
epoch train time: 0:00:01.917350
elapsed time: 0:03:12.940568
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 22:22:50.779430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.36
 ---- batch: 020 ----
mean loss: 210.44
 ---- batch: 030 ----
mean loss: 203.89
 ---- batch: 040 ----
mean loss: 203.87
 ---- batch: 050 ----
mean loss: 202.79
 ---- batch: 060 ----
mean loss: 208.88
 ---- batch: 070 ----
mean loss: 221.59
 ---- batch: 080 ----
mean loss: 219.20
 ---- batch: 090 ----
mean loss: 207.57
 ---- batch: 100 ----
mean loss: 213.45
 ---- batch: 110 ----
mean loss: 201.12
train mean loss: 209.03
epoch train time: 0:00:01.893335
elapsed time: 0:03:14.834590
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 22:22:52.673402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.74
 ---- batch: 020 ----
mean loss: 213.88
 ---- batch: 030 ----
mean loss: 196.92
 ---- batch: 040 ----
mean loss: 206.11
 ---- batch: 050 ----
mean loss: 209.35
 ---- batch: 060 ----
mean loss: 205.44
 ---- batch: 070 ----
mean loss: 209.16
 ---- batch: 080 ----
mean loss: 213.89
 ---- batch: 090 ----
mean loss: 210.89
 ---- batch: 100 ----
mean loss: 206.46
 ---- batch: 110 ----
mean loss: 205.52
train mean loss: 206.70
epoch train time: 0:00:01.885809
elapsed time: 0:03:16.720999
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 22:22:54.559892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.59
 ---- batch: 020 ----
mean loss: 213.64
 ---- batch: 030 ----
mean loss: 200.05
 ---- batch: 040 ----
mean loss: 201.31
 ---- batch: 050 ----
mean loss: 208.82
 ---- batch: 060 ----
mean loss: 197.50
 ---- batch: 070 ----
mean loss: 199.93
 ---- batch: 080 ----
mean loss: 202.50
 ---- batch: 090 ----
mean loss: 206.82
 ---- batch: 100 ----
mean loss: 208.27
 ---- batch: 110 ----
mean loss: 215.31
train mean loss: 204.55
epoch train time: 0:00:01.890563
elapsed time: 0:03:18.612210
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 22:22:56.451011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.57
 ---- batch: 020 ----
mean loss: 210.01
 ---- batch: 030 ----
mean loss: 202.96
 ---- batch: 040 ----
mean loss: 191.76
 ---- batch: 050 ----
mean loss: 200.23
 ---- batch: 060 ----
mean loss: 197.82
 ---- batch: 070 ----
mean loss: 203.68
 ---- batch: 080 ----
mean loss: 206.21
 ---- batch: 090 ----
mean loss: 201.43
 ---- batch: 100 ----
mean loss: 193.43
 ---- batch: 110 ----
mean loss: 202.97
train mean loss: 201.15
epoch train time: 0:00:01.881259
elapsed time: 0:03:20.494039
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 22:22:58.332879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.95
 ---- batch: 020 ----
mean loss: 197.35
 ---- batch: 030 ----
mean loss: 197.63
 ---- batch: 040 ----
mean loss: 196.22
 ---- batch: 050 ----
mean loss: 202.65
 ---- batch: 060 ----
mean loss: 189.41
 ---- batch: 070 ----
mean loss: 201.86
 ---- batch: 080 ----
mean loss: 201.25
 ---- batch: 090 ----
mean loss: 202.40
 ---- batch: 100 ----
mean loss: 204.64
 ---- batch: 110 ----
mean loss: 192.94
train mean loss: 197.08
epoch train time: 0:00:01.871900
elapsed time: 0:03:22.366600
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 22:23:00.205418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.03
 ---- batch: 020 ----
mean loss: 195.51
 ---- batch: 030 ----
mean loss: 200.20
 ---- batch: 040 ----
mean loss: 204.41
 ---- batch: 050 ----
mean loss: 190.23
 ---- batch: 060 ----
mean loss: 192.35
 ---- batch: 070 ----
mean loss: 200.07
 ---- batch: 080 ----
mean loss: 197.62
 ---- batch: 090 ----
mean loss: 193.11
 ---- batch: 100 ----
mean loss: 197.58
 ---- batch: 110 ----
mean loss: 197.00
train mean loss: 196.81
epoch train time: 0:00:01.896125
elapsed time: 0:03:24.263468
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 22:23:02.102290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.64
 ---- batch: 020 ----
mean loss: 192.81
 ---- batch: 030 ----
mean loss: 184.35
 ---- batch: 040 ----
mean loss: 192.35
 ---- batch: 050 ----
mean loss: 189.22
 ---- batch: 060 ----
mean loss: 204.75
 ---- batch: 070 ----
mean loss: 202.40
 ---- batch: 080 ----
mean loss: 197.71
 ---- batch: 090 ----
mean loss: 196.86
 ---- batch: 100 ----
mean loss: 198.66
 ---- batch: 110 ----
mean loss: 196.74
train mean loss: 196.01
epoch train time: 0:00:01.902231
elapsed time: 0:03:26.166315
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 22:23:04.005154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.78
 ---- batch: 020 ----
mean loss: 198.43
 ---- batch: 030 ----
mean loss: 194.74
 ---- batch: 040 ----
mean loss: 187.87
 ---- batch: 050 ----
mean loss: 190.24
 ---- batch: 060 ----
mean loss: 196.18
 ---- batch: 070 ----
mean loss: 191.17
 ---- batch: 080 ----
mean loss: 185.96
 ---- batch: 090 ----
mean loss: 193.00
 ---- batch: 100 ----
mean loss: 189.09
 ---- batch: 110 ----
mean loss: 196.80
train mean loss: 192.15
epoch train time: 0:00:01.916298
elapsed time: 0:03:28.083305
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 22:23:05.922129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.05
 ---- batch: 020 ----
mean loss: 191.46
 ---- batch: 030 ----
mean loss: 186.31
 ---- batch: 040 ----
mean loss: 194.04
 ---- batch: 050 ----
mean loss: 186.58
 ---- batch: 060 ----
mean loss: 190.61
 ---- batch: 070 ----
mean loss: 200.27
 ---- batch: 080 ----
mean loss: 192.22
 ---- batch: 090 ----
mean loss: 186.29
 ---- batch: 100 ----
mean loss: 190.50
 ---- batch: 110 ----
mean loss: 195.54
train mean loss: 191.43
epoch train time: 0:00:01.886689
elapsed time: 0:03:29.970583
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 22:23:07.809503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.84
 ---- batch: 020 ----
mean loss: 194.64
 ---- batch: 030 ----
mean loss: 186.42
 ---- batch: 040 ----
mean loss: 192.84
 ---- batch: 050 ----
mean loss: 187.86
 ---- batch: 060 ----
mean loss: 177.67
 ---- batch: 070 ----
mean loss: 190.26
 ---- batch: 080 ----
mean loss: 181.85
 ---- batch: 090 ----
mean loss: 192.66
 ---- batch: 100 ----
mean loss: 188.31
 ---- batch: 110 ----
mean loss: 190.15
train mean loss: 188.40
epoch train time: 0:00:01.864936
elapsed time: 0:03:31.836226
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 22:23:09.675062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.03
 ---- batch: 020 ----
mean loss: 195.94
 ---- batch: 030 ----
mean loss: 187.64
 ---- batch: 040 ----
mean loss: 182.80
 ---- batch: 050 ----
mean loss: 186.16
 ---- batch: 060 ----
mean loss: 190.89
 ---- batch: 070 ----
mean loss: 191.79
 ---- batch: 080 ----
mean loss: 197.25
 ---- batch: 090 ----
mean loss: 178.54
 ---- batch: 100 ----
mean loss: 186.38
 ---- batch: 110 ----
mean loss: 183.24
train mean loss: 187.33
epoch train time: 0:00:01.900747
elapsed time: 0:03:33.737570
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 22:23:11.576392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.18
 ---- batch: 020 ----
mean loss: 185.50
 ---- batch: 030 ----
mean loss: 193.02
 ---- batch: 040 ----
mean loss: 181.07
 ---- batch: 050 ----
mean loss: 189.40
 ---- batch: 060 ----
mean loss: 185.61
 ---- batch: 070 ----
mean loss: 183.72
 ---- batch: 080 ----
mean loss: 186.13
 ---- batch: 090 ----
mean loss: 182.82
 ---- batch: 100 ----
mean loss: 188.69
 ---- batch: 110 ----
mean loss: 188.80
train mean loss: 186.16
epoch train time: 0:00:01.922783
elapsed time: 0:03:35.660967
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 22:23:13.499771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.54
 ---- batch: 020 ----
mean loss: 182.98
 ---- batch: 030 ----
mean loss: 182.22
 ---- batch: 040 ----
mean loss: 194.06
 ---- batch: 050 ----
mean loss: 193.45
 ---- batch: 060 ----
mean loss: 178.41
 ---- batch: 070 ----
mean loss: 173.01
 ---- batch: 080 ----
mean loss: 176.52
 ---- batch: 090 ----
mean loss: 184.26
 ---- batch: 100 ----
mean loss: 188.98
 ---- batch: 110 ----
mean loss: 185.04
train mean loss: 184.66
epoch train time: 0:00:01.908611
elapsed time: 0:03:37.570169
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 22:23:15.409047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.84
 ---- batch: 020 ----
mean loss: 173.96
 ---- batch: 030 ----
mean loss: 165.23
 ---- batch: 040 ----
mean loss: 182.17
 ---- batch: 050 ----
mean loss: 194.08
 ---- batch: 060 ----
mean loss: 184.12
 ---- batch: 070 ----
mean loss: 183.46
 ---- batch: 080 ----
mean loss: 182.21
 ---- batch: 090 ----
mean loss: 180.70
 ---- batch: 100 ----
mean loss: 177.72
 ---- batch: 110 ----
mean loss: 189.45
train mean loss: 181.43
epoch train time: 0:00:01.910240
elapsed time: 0:03:39.481057
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 22:23:17.319589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.35
 ---- batch: 020 ----
mean loss: 177.15
 ---- batch: 030 ----
mean loss: 177.36
 ---- batch: 040 ----
mean loss: 176.11
 ---- batch: 050 ----
mean loss: 189.26
 ---- batch: 060 ----
mean loss: 179.56
 ---- batch: 070 ----
mean loss: 176.91
 ---- batch: 080 ----
mean loss: 186.21
 ---- batch: 090 ----
mean loss: 187.13
 ---- batch: 100 ----
mean loss: 185.12
 ---- batch: 110 ----
mean loss: 177.86
train mean loss: 181.07
epoch train time: 0:00:01.904177
elapsed time: 0:03:41.385526
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 22:23:19.224329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.43
 ---- batch: 020 ----
mean loss: 178.97
 ---- batch: 030 ----
mean loss: 177.59
 ---- batch: 040 ----
mean loss: 173.65
 ---- batch: 050 ----
mean loss: 180.38
 ---- batch: 060 ----
mean loss: 182.69
 ---- batch: 070 ----
mean loss: 179.96
 ---- batch: 080 ----
mean loss: 189.58
 ---- batch: 090 ----
mean loss: 185.06
 ---- batch: 100 ----
mean loss: 178.48
 ---- batch: 110 ----
mean loss: 174.27
train mean loss: 179.22
epoch train time: 0:00:01.937426
elapsed time: 0:03:43.323539
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 22:23:21.162337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.16
 ---- batch: 020 ----
mean loss: 182.94
 ---- batch: 030 ----
mean loss: 170.29
 ---- batch: 040 ----
mean loss: 179.83
 ---- batch: 050 ----
mean loss: 172.79
 ---- batch: 060 ----
mean loss: 175.71
 ---- batch: 070 ----
mean loss: 173.41
 ---- batch: 080 ----
mean loss: 171.13
 ---- batch: 090 ----
mean loss: 178.19
 ---- batch: 100 ----
mean loss: 188.71
 ---- batch: 110 ----
mean loss: 183.41
train mean loss: 178.22
epoch train time: 0:00:01.897747
elapsed time: 0:03:45.221863
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 22:23:23.060662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.07
 ---- batch: 020 ----
mean loss: 174.37
 ---- batch: 030 ----
mean loss: 179.45
 ---- batch: 040 ----
mean loss: 173.69
 ---- batch: 050 ----
mean loss: 179.00
 ---- batch: 060 ----
mean loss: 180.11
 ---- batch: 070 ----
mean loss: 176.76
 ---- batch: 080 ----
mean loss: 178.21
 ---- batch: 090 ----
mean loss: 171.73
 ---- batch: 100 ----
mean loss: 180.89
 ---- batch: 110 ----
mean loss: 175.40
train mean loss: 176.08
epoch train time: 0:00:01.942447
elapsed time: 0:03:47.164874
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 22:23:25.003689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.39
 ---- batch: 020 ----
mean loss: 176.88
 ---- batch: 030 ----
mean loss: 175.24
 ---- batch: 040 ----
mean loss: 169.27
 ---- batch: 050 ----
mean loss: 183.74
 ---- batch: 060 ----
mean loss: 168.14
 ---- batch: 070 ----
mean loss: 179.49
 ---- batch: 080 ----
mean loss: 178.40
 ---- batch: 090 ----
mean loss: 173.99
 ---- batch: 100 ----
mean loss: 167.55
 ---- batch: 110 ----
mean loss: 177.84
train mean loss: 175.37
epoch train time: 0:00:01.923568
elapsed time: 0:03:49.089055
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 22:23:26.927847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.23
 ---- batch: 020 ----
mean loss: 174.77
 ---- batch: 030 ----
mean loss: 174.80
 ---- batch: 040 ----
mean loss: 173.58
 ---- batch: 050 ----
mean loss: 175.29
 ---- batch: 060 ----
mean loss: 178.23
 ---- batch: 070 ----
mean loss: 173.77
 ---- batch: 080 ----
mean loss: 179.35
 ---- batch: 090 ----
mean loss: 168.61
 ---- batch: 100 ----
mean loss: 172.75
 ---- batch: 110 ----
mean loss: 172.68
train mean loss: 174.48
epoch train time: 0:00:01.912992
elapsed time: 0:03:51.002655
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 22:23:28.841495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.01
 ---- batch: 020 ----
mean loss: 167.92
 ---- batch: 030 ----
mean loss: 172.55
 ---- batch: 040 ----
mean loss: 174.10
 ---- batch: 050 ----
mean loss: 163.22
 ---- batch: 060 ----
mean loss: 178.29
 ---- batch: 070 ----
mean loss: 169.96
 ---- batch: 080 ----
mean loss: 171.96
 ---- batch: 090 ----
mean loss: 169.37
 ---- batch: 100 ----
mean loss: 174.51
 ---- batch: 110 ----
mean loss: 174.33
train mean loss: 171.98
epoch train time: 0:00:01.914573
elapsed time: 0:03:52.917857
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 22:23:30.756660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.33
 ---- batch: 020 ----
mean loss: 173.20
 ---- batch: 030 ----
mean loss: 172.18
 ---- batch: 040 ----
mean loss: 168.97
 ---- batch: 050 ----
mean loss: 170.52
 ---- batch: 060 ----
mean loss: 163.63
 ---- batch: 070 ----
mean loss: 168.07
 ---- batch: 080 ----
mean loss: 166.49
 ---- batch: 090 ----
mean loss: 172.06
 ---- batch: 100 ----
mean loss: 178.94
 ---- batch: 110 ----
mean loss: 181.23
train mean loss: 170.65
epoch train time: 0:00:01.910876
elapsed time: 0:03:54.829369
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 22:23:32.668231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.38
 ---- batch: 020 ----
mean loss: 163.77
 ---- batch: 030 ----
mean loss: 170.01
 ---- batch: 040 ----
mean loss: 163.84
 ---- batch: 050 ----
mean loss: 163.60
 ---- batch: 060 ----
mean loss: 174.58
 ---- batch: 070 ----
mean loss: 169.06
 ---- batch: 080 ----
mean loss: 167.90
 ---- batch: 090 ----
mean loss: 172.26
 ---- batch: 100 ----
mean loss: 177.24
 ---- batch: 110 ----
mean loss: 170.60
train mean loss: 170.09
epoch train time: 0:00:01.885170
elapsed time: 0:03:56.715281
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 22:23:34.554006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.49
 ---- batch: 020 ----
mean loss: 173.92
 ---- batch: 030 ----
mean loss: 163.81
 ---- batch: 040 ----
mean loss: 166.01
 ---- batch: 050 ----
mean loss: 166.36
 ---- batch: 060 ----
mean loss: 167.42
 ---- batch: 070 ----
mean loss: 164.07
 ---- batch: 080 ----
mean loss: 175.12
 ---- batch: 090 ----
mean loss: 175.17
 ---- batch: 100 ----
mean loss: 162.08
 ---- batch: 110 ----
mean loss: 176.62
train mean loss: 168.66
epoch train time: 0:00:01.887864
elapsed time: 0:03:58.603642
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 22:23:36.442425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.70
 ---- batch: 020 ----
mean loss: 170.69
 ---- batch: 030 ----
mean loss: 165.78
 ---- batch: 040 ----
mean loss: 163.03
 ---- batch: 050 ----
mean loss: 171.54
 ---- batch: 060 ----
mean loss: 160.75
 ---- batch: 070 ----
mean loss: 178.65
 ---- batch: 080 ----
mean loss: 171.30
 ---- batch: 090 ----
mean loss: 169.35
 ---- batch: 100 ----
mean loss: 153.89
 ---- batch: 110 ----
mean loss: 171.93
train mean loss: 167.91
epoch train time: 0:00:01.939577
elapsed time: 0:04:00.543785
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 22:23:38.382603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.47
 ---- batch: 020 ----
mean loss: 172.11
 ---- batch: 030 ----
mean loss: 155.58
 ---- batch: 040 ----
mean loss: 160.37
 ---- batch: 050 ----
mean loss: 171.39
 ---- batch: 060 ----
mean loss: 167.77
 ---- batch: 070 ----
mean loss: 169.88
 ---- batch: 080 ----
mean loss: 167.33
 ---- batch: 090 ----
mean loss: 175.85
 ---- batch: 100 ----
mean loss: 170.10
 ---- batch: 110 ----
mean loss: 167.44
train mean loss: 167.38
epoch train time: 0:00:01.915880
elapsed time: 0:04:02.460287
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 22:23:40.299164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.80
 ---- batch: 020 ----
mean loss: 156.95
 ---- batch: 030 ----
mean loss: 165.65
 ---- batch: 040 ----
mean loss: 161.65
 ---- batch: 050 ----
mean loss: 169.21
 ---- batch: 060 ----
mean loss: 167.30
 ---- batch: 070 ----
mean loss: 172.55
 ---- batch: 080 ----
mean loss: 167.44
 ---- batch: 090 ----
mean loss: 163.90
 ---- batch: 100 ----
mean loss: 164.52
 ---- batch: 110 ----
mean loss: 170.74
train mean loss: 165.93
epoch train time: 0:00:01.886946
elapsed time: 0:04:04.347878
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 22:23:42.186695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.06
 ---- batch: 020 ----
mean loss: 164.59
 ---- batch: 030 ----
mean loss: 162.95
 ---- batch: 040 ----
mean loss: 157.25
 ---- batch: 050 ----
mean loss: 169.47
 ---- batch: 060 ----
mean loss: 164.57
 ---- batch: 070 ----
mean loss: 163.67
 ---- batch: 080 ----
mean loss: 167.32
 ---- batch: 090 ----
mean loss: 162.67
 ---- batch: 100 ----
mean loss: 166.22
 ---- batch: 110 ----
mean loss: 169.12
train mean loss: 165.42
epoch train time: 0:00:01.890713
elapsed time: 0:04:06.239219
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 22:23:44.078092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.83
 ---- batch: 020 ----
mean loss: 158.23
 ---- batch: 030 ----
mean loss: 164.37
 ---- batch: 040 ----
mean loss: 163.29
 ---- batch: 050 ----
mean loss: 159.56
 ---- batch: 060 ----
mean loss: 166.63
 ---- batch: 070 ----
mean loss: 163.50
 ---- batch: 080 ----
mean loss: 170.10
 ---- batch: 090 ----
mean loss: 171.45
 ---- batch: 100 ----
mean loss: 157.00
 ---- batch: 110 ----
mean loss: 167.80
train mean loss: 163.88
epoch train time: 0:00:01.913545
elapsed time: 0:04:08.153422
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 22:23:45.992240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.33
 ---- batch: 020 ----
mean loss: 154.19
 ---- batch: 030 ----
mean loss: 161.59
 ---- batch: 040 ----
mean loss: 153.12
 ---- batch: 050 ----
mean loss: 163.80
 ---- batch: 060 ----
mean loss: 164.63
 ---- batch: 070 ----
mean loss: 157.32
 ---- batch: 080 ----
mean loss: 171.23
 ---- batch: 090 ----
mean loss: 163.11
 ---- batch: 100 ----
mean loss: 165.46
 ---- batch: 110 ----
mean loss: 174.71
train mean loss: 163.26
epoch train time: 0:00:01.894197
elapsed time: 0:04:10.048263
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 22:23:47.887085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.20
 ---- batch: 020 ----
mean loss: 166.93
 ---- batch: 030 ----
mean loss: 154.43
 ---- batch: 040 ----
mean loss: 163.14
 ---- batch: 050 ----
mean loss: 167.23
 ---- batch: 060 ----
mean loss: 172.97
 ---- batch: 070 ----
mean loss: 161.09
 ---- batch: 080 ----
mean loss: 158.77
 ---- batch: 090 ----
mean loss: 159.53
 ---- batch: 100 ----
mean loss: 165.53
 ---- batch: 110 ----
mean loss: 161.84
train mean loss: 162.48
epoch train time: 0:00:01.933491
elapsed time: 0:04:11.982355
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 22:23:49.821166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.56
 ---- batch: 020 ----
mean loss: 165.40
 ---- batch: 030 ----
mean loss: 164.35
 ---- batch: 040 ----
mean loss: 154.28
 ---- batch: 050 ----
mean loss: 163.67
 ---- batch: 060 ----
mean loss: 162.77
 ---- batch: 070 ----
mean loss: 161.08
 ---- batch: 080 ----
mean loss: 169.19
 ---- batch: 090 ----
mean loss: 160.84
 ---- batch: 100 ----
mean loss: 154.85
 ---- batch: 110 ----
mean loss: 157.43
train mean loss: 161.58
epoch train time: 0:00:01.907066
elapsed time: 0:04:13.890006
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 22:23:51.728823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.20
 ---- batch: 020 ----
mean loss: 156.83
 ---- batch: 030 ----
mean loss: 156.57
 ---- batch: 040 ----
mean loss: 159.46
 ---- batch: 050 ----
mean loss: 159.29
 ---- batch: 060 ----
mean loss: 161.04
 ---- batch: 070 ----
mean loss: 167.19
 ---- batch: 080 ----
mean loss: 169.34
 ---- batch: 090 ----
mean loss: 159.11
 ---- batch: 100 ----
mean loss: 168.95
 ---- batch: 110 ----
mean loss: 158.77
train mean loss: 161.45
epoch train time: 0:00:01.916064
elapsed time: 0:04:15.806678
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 22:23:53.645515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.30
 ---- batch: 020 ----
mean loss: 159.23
 ---- batch: 030 ----
mean loss: 158.67
 ---- batch: 040 ----
mean loss: 154.65
 ---- batch: 050 ----
mean loss: 154.17
 ---- batch: 060 ----
mean loss: 162.70
 ---- batch: 070 ----
mean loss: 165.13
 ---- batch: 080 ----
mean loss: 164.86
 ---- batch: 090 ----
mean loss: 162.30
 ---- batch: 100 ----
mean loss: 159.42
 ---- batch: 110 ----
mean loss: 159.14
train mean loss: 160.65
epoch train time: 0:00:01.906809
elapsed time: 0:04:17.714078
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 22:23:55.552918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.22
 ---- batch: 020 ----
mean loss: 156.95
 ---- batch: 030 ----
mean loss: 157.01
 ---- batch: 040 ----
mean loss: 155.38
 ---- batch: 050 ----
mean loss: 156.78
 ---- batch: 060 ----
mean loss: 157.48
 ---- batch: 070 ----
mean loss: 163.27
 ---- batch: 080 ----
mean loss: 164.53
 ---- batch: 090 ----
mean loss: 163.51
 ---- batch: 100 ----
mean loss: 161.56
 ---- batch: 110 ----
mean loss: 160.41
train mean loss: 159.35
epoch train time: 0:00:01.919235
elapsed time: 0:04:19.633949
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 22:23:57.472822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.88
 ---- batch: 020 ----
mean loss: 153.78
 ---- batch: 030 ----
mean loss: 150.68
 ---- batch: 040 ----
mean loss: 157.03
 ---- batch: 050 ----
mean loss: 163.20
 ---- batch: 060 ----
mean loss: 163.10
 ---- batch: 070 ----
mean loss: 160.59
 ---- batch: 080 ----
mean loss: 161.68
 ---- batch: 090 ----
mean loss: 163.79
 ---- batch: 100 ----
mean loss: 159.37
 ---- batch: 110 ----
mean loss: 159.81
train mean loss: 158.76
epoch train time: 0:00:01.918918
elapsed time: 0:04:21.553496
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 22:23:59.392331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.51
 ---- batch: 020 ----
mean loss: 145.93
 ---- batch: 030 ----
mean loss: 164.02
 ---- batch: 040 ----
mean loss: 155.44
 ---- batch: 050 ----
mean loss: 152.04
 ---- batch: 060 ----
mean loss: 156.08
 ---- batch: 070 ----
mean loss: 164.21
 ---- batch: 080 ----
mean loss: 153.82
 ---- batch: 090 ----
mean loss: 168.07
 ---- batch: 100 ----
mean loss: 159.14
 ---- batch: 110 ----
mean loss: 159.76
train mean loss: 157.49
epoch train time: 0:00:01.902014
elapsed time: 0:04:23.456127
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 22:24:01.294946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.11
 ---- batch: 020 ----
mean loss: 152.64
 ---- batch: 030 ----
mean loss: 154.76
 ---- batch: 040 ----
mean loss: 155.44
 ---- batch: 050 ----
mean loss: 146.05
 ---- batch: 060 ----
mean loss: 156.26
 ---- batch: 070 ----
mean loss: 160.60
 ---- batch: 080 ----
mean loss: 161.06
 ---- batch: 090 ----
mean loss: 155.58
 ---- batch: 100 ----
mean loss: 161.36
 ---- batch: 110 ----
mean loss: 162.83
train mean loss: 157.12
epoch train time: 0:00:01.891220
elapsed time: 0:04:25.347932
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 22:24:03.186888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.94
 ---- batch: 020 ----
mean loss: 160.17
 ---- batch: 030 ----
mean loss: 151.25
 ---- batch: 040 ----
mean loss: 155.77
 ---- batch: 050 ----
mean loss: 158.21
 ---- batch: 060 ----
mean loss: 160.31
 ---- batch: 070 ----
mean loss: 158.14
 ---- batch: 080 ----
mean loss: 159.50
 ---- batch: 090 ----
mean loss: 150.65
 ---- batch: 100 ----
mean loss: 156.21
 ---- batch: 110 ----
mean loss: 153.11
train mean loss: 156.38
epoch train time: 0:00:01.916760
elapsed time: 0:04:27.265418
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 22:24:05.104228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.21
 ---- batch: 020 ----
mean loss: 147.64
 ---- batch: 030 ----
mean loss: 153.32
 ---- batch: 040 ----
mean loss: 158.28
 ---- batch: 050 ----
mean loss: 154.38
 ---- batch: 060 ----
mean loss: 155.40
 ---- batch: 070 ----
mean loss: 157.06
 ---- batch: 080 ----
mean loss: 158.83
 ---- batch: 090 ----
mean loss: 161.95
 ---- batch: 100 ----
mean loss: 157.34
 ---- batch: 110 ----
mean loss: 149.51
train mean loss: 155.16
epoch train time: 0:00:01.906728
elapsed time: 0:04:29.172745
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 22:24:07.011555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.32
 ---- batch: 020 ----
mean loss: 142.21
 ---- batch: 030 ----
mean loss: 156.20
 ---- batch: 040 ----
mean loss: 156.05
 ---- batch: 050 ----
mean loss: 160.90
 ---- batch: 060 ----
mean loss: 160.61
 ---- batch: 070 ----
mean loss: 163.34
 ---- batch: 080 ----
mean loss: 151.91
 ---- batch: 090 ----
mean loss: 157.86
 ---- batch: 100 ----
mean loss: 152.65
 ---- batch: 110 ----
mean loss: 155.05
train mean loss: 155.73
epoch train time: 0:00:01.899151
elapsed time: 0:04:31.072471
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 22:24:08.911338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.77
 ---- batch: 020 ----
mean loss: 146.38
 ---- batch: 030 ----
mean loss: 157.17
 ---- batch: 040 ----
mean loss: 156.57
 ---- batch: 050 ----
mean loss: 144.46
 ---- batch: 060 ----
mean loss: 158.75
 ---- batch: 070 ----
mean loss: 164.69
 ---- batch: 080 ----
mean loss: 157.40
 ---- batch: 090 ----
mean loss: 154.84
 ---- batch: 100 ----
mean loss: 156.87
 ---- batch: 110 ----
mean loss: 155.65
train mean loss: 155.03
epoch train time: 0:00:01.886780
elapsed time: 0:04:32.959883
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 22:24:10.798686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.72
 ---- batch: 020 ----
mean loss: 149.80
 ---- batch: 030 ----
mean loss: 152.32
 ---- batch: 040 ----
mean loss: 157.34
 ---- batch: 050 ----
mean loss: 154.91
 ---- batch: 060 ----
mean loss: 153.60
 ---- batch: 070 ----
mean loss: 152.90
 ---- batch: 080 ----
mean loss: 162.71
 ---- batch: 090 ----
mean loss: 156.37
 ---- batch: 100 ----
mean loss: 152.60
 ---- batch: 110 ----
mean loss: 151.45
train mean loss: 154.18
epoch train time: 0:00:01.908863
elapsed time: 0:04:34.869672
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 22:24:12.708201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.50
 ---- batch: 020 ----
mean loss: 151.66
 ---- batch: 030 ----
mean loss: 148.36
 ---- batch: 040 ----
mean loss: 139.47
 ---- batch: 050 ----
mean loss: 166.78
 ---- batch: 060 ----
mean loss: 155.54
 ---- batch: 070 ----
mean loss: 154.16
 ---- batch: 080 ----
mean loss: 155.11
 ---- batch: 090 ----
mean loss: 155.35
 ---- batch: 100 ----
mean loss: 151.82
 ---- batch: 110 ----
mean loss: 156.46
train mean loss: 153.57
epoch train time: 0:00:01.934124
elapsed time: 0:04:36.804099
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 22:24:14.642943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.66
 ---- batch: 020 ----
mean loss: 143.33
 ---- batch: 030 ----
mean loss: 153.24
 ---- batch: 040 ----
mean loss: 152.65
 ---- batch: 050 ----
mean loss: 154.47
 ---- batch: 060 ----
mean loss: 152.42
 ---- batch: 070 ----
mean loss: 152.65
 ---- batch: 080 ----
mean loss: 158.40
 ---- batch: 090 ----
mean loss: 149.31
 ---- batch: 100 ----
mean loss: 162.68
 ---- batch: 110 ----
mean loss: 140.81
train mean loss: 152.46
epoch train time: 0:00:01.938915
elapsed time: 0:04:38.743666
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 22:24:16.582463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.69
 ---- batch: 020 ----
mean loss: 155.73
 ---- batch: 030 ----
mean loss: 146.84
 ---- batch: 040 ----
mean loss: 152.85
 ---- batch: 050 ----
mean loss: 157.97
 ---- batch: 060 ----
mean loss: 150.99
 ---- batch: 070 ----
mean loss: 142.52
 ---- batch: 080 ----
mean loss: 154.21
 ---- batch: 090 ----
mean loss: 153.57
 ---- batch: 100 ----
mean loss: 153.03
 ---- batch: 110 ----
mean loss: 157.29
train mean loss: 152.53
epoch train time: 0:00:01.945121
elapsed time: 0:04:40.689379
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 22:24:18.528179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.64
 ---- batch: 020 ----
mean loss: 145.21
 ---- batch: 030 ----
mean loss: 151.89
 ---- batch: 040 ----
mean loss: 156.86
 ---- batch: 050 ----
mean loss: 143.11
 ---- batch: 060 ----
mean loss: 147.73
 ---- batch: 070 ----
mean loss: 155.48
 ---- batch: 080 ----
mean loss: 156.91
 ---- batch: 090 ----
mean loss: 156.12
 ---- batch: 100 ----
mean loss: 146.96
 ---- batch: 110 ----
mean loss: 152.72
train mean loss: 151.00
epoch train time: 0:00:01.959784
elapsed time: 0:04:42.649737
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 22:24:20.488544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.69
 ---- batch: 020 ----
mean loss: 151.30
 ---- batch: 030 ----
mean loss: 151.47
 ---- batch: 040 ----
mean loss: 155.18
 ---- batch: 050 ----
mean loss: 153.20
 ---- batch: 060 ----
mean loss: 149.16
 ---- batch: 070 ----
mean loss: 152.50
 ---- batch: 080 ----
mean loss: 147.72
 ---- batch: 090 ----
mean loss: 143.33
 ---- batch: 100 ----
mean loss: 158.38
 ---- batch: 110 ----
mean loss: 156.33
train mean loss: 151.47
epoch train time: 0:00:01.940117
elapsed time: 0:04:44.590442
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 22:24:22.429274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.93
 ---- batch: 020 ----
mean loss: 150.68
 ---- batch: 030 ----
mean loss: 147.91
 ---- batch: 040 ----
mean loss: 153.75
 ---- batch: 050 ----
mean loss: 148.82
 ---- batch: 060 ----
mean loss: 154.01
 ---- batch: 070 ----
mean loss: 157.70
 ---- batch: 080 ----
mean loss: 144.68
 ---- batch: 090 ----
mean loss: 145.69
 ---- batch: 100 ----
mean loss: 150.96
 ---- batch: 110 ----
mean loss: 147.34
train mean loss: 149.99
epoch train time: 0:00:01.957099
elapsed time: 0:04:46.548132
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 22:24:24.386933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.65
 ---- batch: 020 ----
mean loss: 147.25
 ---- batch: 030 ----
mean loss: 144.13
 ---- batch: 040 ----
mean loss: 144.99
 ---- batch: 050 ----
mean loss: 149.46
 ---- batch: 060 ----
mean loss: 141.91
 ---- batch: 070 ----
mean loss: 152.56
 ---- batch: 080 ----
mean loss: 145.06
 ---- batch: 090 ----
mean loss: 156.37
 ---- batch: 100 ----
mean loss: 145.67
 ---- batch: 110 ----
mean loss: 155.87
train mean loss: 150.00
epoch train time: 0:00:01.911309
elapsed time: 0:04:48.460089
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 22:24:26.298920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.76
 ---- batch: 020 ----
mean loss: 149.39
 ---- batch: 030 ----
mean loss: 154.33
 ---- batch: 040 ----
mean loss: 147.64
 ---- batch: 050 ----
mean loss: 150.87
 ---- batch: 060 ----
mean loss: 149.93
 ---- batch: 070 ----
mean loss: 149.24
 ---- batch: 080 ----
mean loss: 146.61
 ---- batch: 090 ----
mean loss: 149.52
 ---- batch: 100 ----
mean loss: 149.41
 ---- batch: 110 ----
mean loss: 151.59
train mean loss: 149.50
epoch train time: 0:00:01.885688
elapsed time: 0:04:50.346378
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 22:24:28.185174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.66
 ---- batch: 020 ----
mean loss: 150.04
 ---- batch: 030 ----
mean loss: 149.67
 ---- batch: 040 ----
mean loss: 156.26
 ---- batch: 050 ----
mean loss: 140.93
 ---- batch: 060 ----
mean loss: 150.04
 ---- batch: 070 ----
mean loss: 151.22
 ---- batch: 080 ----
mean loss: 146.34
 ---- batch: 090 ----
mean loss: 151.13
 ---- batch: 100 ----
mean loss: 144.23
 ---- batch: 110 ----
mean loss: 150.72
train mean loss: 148.83
epoch train time: 0:00:01.891990
elapsed time: 0:04:52.238952
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 22:24:30.077790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.06
 ---- batch: 020 ----
mean loss: 144.79
 ---- batch: 030 ----
mean loss: 150.03
 ---- batch: 040 ----
mean loss: 145.58
 ---- batch: 050 ----
mean loss: 143.55
 ---- batch: 060 ----
mean loss: 141.10
 ---- batch: 070 ----
mean loss: 153.99
 ---- batch: 080 ----
mean loss: 149.51
 ---- batch: 090 ----
mean loss: 152.70
 ---- batch: 100 ----
mean loss: 143.76
 ---- batch: 110 ----
mean loss: 147.57
train mean loss: 148.52
epoch train time: 0:00:01.882420
elapsed time: 0:04:54.121981
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 22:24:31.960811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.66
 ---- batch: 020 ----
mean loss: 148.38
 ---- batch: 030 ----
mean loss: 144.46
 ---- batch: 040 ----
mean loss: 145.26
 ---- batch: 050 ----
mean loss: 148.83
 ---- batch: 060 ----
mean loss: 155.46
 ---- batch: 070 ----
mean loss: 140.82
 ---- batch: 080 ----
mean loss: 147.47
 ---- batch: 090 ----
mean loss: 141.47
 ---- batch: 100 ----
mean loss: 159.24
 ---- batch: 110 ----
mean loss: 154.84
train mean loss: 147.17
epoch train time: 0:00:01.885595
elapsed time: 0:04:56.008250
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 22:24:33.847065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.36
 ---- batch: 020 ----
mean loss: 149.64
 ---- batch: 030 ----
mean loss: 138.42
 ---- batch: 040 ----
mean loss: 151.69
 ---- batch: 050 ----
mean loss: 141.29
 ---- batch: 060 ----
mean loss: 151.70
 ---- batch: 070 ----
mean loss: 144.96
 ---- batch: 080 ----
mean loss: 145.03
 ---- batch: 090 ----
mean loss: 144.43
 ---- batch: 100 ----
mean loss: 153.70
 ---- batch: 110 ----
mean loss: 149.02
train mean loss: 147.15
epoch train time: 0:00:01.896648
elapsed time: 0:04:57.905558
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 22:24:35.744349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.88
 ---- batch: 020 ----
mean loss: 146.91
 ---- batch: 030 ----
mean loss: 148.68
 ---- batch: 040 ----
mean loss: 152.04
 ---- batch: 050 ----
mean loss: 152.27
 ---- batch: 060 ----
mean loss: 149.28
 ---- batch: 070 ----
mean loss: 146.89
 ---- batch: 080 ----
mean loss: 144.54
 ---- batch: 090 ----
mean loss: 146.04
 ---- batch: 100 ----
mean loss: 139.67
 ---- batch: 110 ----
mean loss: 151.89
train mean loss: 147.04
epoch train time: 0:00:01.890406
elapsed time: 0:04:59.796666
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 22:24:37.635510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.82
 ---- batch: 020 ----
mean loss: 141.50
 ---- batch: 030 ----
mean loss: 148.83
 ---- batch: 040 ----
mean loss: 148.70
 ---- batch: 050 ----
mean loss: 147.57
 ---- batch: 060 ----
mean loss: 143.86
 ---- batch: 070 ----
mean loss: 147.05
 ---- batch: 080 ----
mean loss: 147.11
 ---- batch: 090 ----
mean loss: 157.79
 ---- batch: 100 ----
mean loss: 144.49
 ---- batch: 110 ----
mean loss: 142.26
train mean loss: 146.32
epoch train time: 0:00:01.914472
elapsed time: 0:05:01.711771
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 22:24:39.550632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.71
 ---- batch: 020 ----
mean loss: 147.65
 ---- batch: 030 ----
mean loss: 147.16
 ---- batch: 040 ----
mean loss: 136.68
 ---- batch: 050 ----
mean loss: 148.44
 ---- batch: 060 ----
mean loss: 150.86
 ---- batch: 070 ----
mean loss: 143.44
 ---- batch: 080 ----
mean loss: 148.46
 ---- batch: 090 ----
mean loss: 145.17
 ---- batch: 100 ----
mean loss: 145.94
 ---- batch: 110 ----
mean loss: 141.60
train mean loss: 145.99
epoch train time: 0:00:01.913313
elapsed time: 0:05:03.625709
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 22:24:41.464525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.75
 ---- batch: 020 ----
mean loss: 149.68
 ---- batch: 030 ----
mean loss: 143.59
 ---- batch: 040 ----
mean loss: 144.81
 ---- batch: 050 ----
mean loss: 140.68
 ---- batch: 060 ----
mean loss: 145.02
 ---- batch: 070 ----
mean loss: 143.18
 ---- batch: 080 ----
mean loss: 146.08
 ---- batch: 090 ----
mean loss: 142.76
 ---- batch: 100 ----
mean loss: 151.52
 ---- batch: 110 ----
mean loss: 153.10
train mean loss: 145.62
epoch train time: 0:00:01.915980
elapsed time: 0:05:05.542344
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 22:24:43.381193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.97
 ---- batch: 020 ----
mean loss: 136.99
 ---- batch: 030 ----
mean loss: 139.34
 ---- batch: 040 ----
mean loss: 147.15
 ---- batch: 050 ----
mean loss: 147.96
 ---- batch: 060 ----
mean loss: 139.93
 ---- batch: 070 ----
mean loss: 147.70
 ---- batch: 080 ----
mean loss: 148.05
 ---- batch: 090 ----
mean loss: 153.10
 ---- batch: 100 ----
mean loss: 149.89
 ---- batch: 110 ----
mean loss: 142.07
train mean loss: 145.12
epoch train time: 0:00:01.918703
elapsed time: 0:05:07.461718
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 22:24:45.300538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.32
 ---- batch: 020 ----
mean loss: 141.55
 ---- batch: 030 ----
mean loss: 153.01
 ---- batch: 040 ----
mean loss: 150.10
 ---- batch: 050 ----
mean loss: 142.45
 ---- batch: 060 ----
mean loss: 145.12
 ---- batch: 070 ----
mean loss: 141.01
 ---- batch: 080 ----
mean loss: 145.94
 ---- batch: 090 ----
mean loss: 145.37
 ---- batch: 100 ----
mean loss: 147.35
 ---- batch: 110 ----
mean loss: 143.99
train mean loss: 145.05
epoch train time: 0:00:01.894623
elapsed time: 0:05:09.356951
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 22:24:47.195754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.56
 ---- batch: 020 ----
mean loss: 147.60
 ---- batch: 030 ----
mean loss: 144.27
 ---- batch: 040 ----
mean loss: 147.39
 ---- batch: 050 ----
mean loss: 147.51
 ---- batch: 060 ----
mean loss: 145.19
 ---- batch: 070 ----
mean loss: 145.82
 ---- batch: 080 ----
mean loss: 144.43
 ---- batch: 090 ----
mean loss: 142.38
 ---- batch: 100 ----
mean loss: 143.11
 ---- batch: 110 ----
mean loss: 142.06
train mean loss: 143.99
epoch train time: 0:00:01.910759
elapsed time: 0:05:11.268281
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 22:24:49.107077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.83
 ---- batch: 020 ----
mean loss: 142.79
 ---- batch: 030 ----
mean loss: 138.31
 ---- batch: 040 ----
mean loss: 140.89
 ---- batch: 050 ----
mean loss: 140.99
 ---- batch: 060 ----
mean loss: 145.10
 ---- batch: 070 ----
mean loss: 149.44
 ---- batch: 080 ----
mean loss: 143.83
 ---- batch: 090 ----
mean loss: 143.83
 ---- batch: 100 ----
mean loss: 139.94
 ---- batch: 110 ----
mean loss: 150.75
train mean loss: 143.73
epoch train time: 0:00:01.874924
elapsed time: 0:05:13.143773
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 22:24:50.982553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.89
 ---- batch: 020 ----
mean loss: 144.33
 ---- batch: 030 ----
mean loss: 138.69
 ---- batch: 040 ----
mean loss: 137.99
 ---- batch: 050 ----
mean loss: 144.58
 ---- batch: 060 ----
mean loss: 143.20
 ---- batch: 070 ----
mean loss: 148.04
 ---- batch: 080 ----
mean loss: 148.39
 ---- batch: 090 ----
mean loss: 147.04
 ---- batch: 100 ----
mean loss: 153.93
 ---- batch: 110 ----
mean loss: 135.19
train mean loss: 144.02
epoch train time: 0:00:01.915552
elapsed time: 0:05:15.059878
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 22:24:52.898701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.26
 ---- batch: 020 ----
mean loss: 145.18
 ---- batch: 030 ----
mean loss: 143.08
 ---- batch: 040 ----
mean loss: 133.34
 ---- batch: 050 ----
mean loss: 143.32
 ---- batch: 060 ----
mean loss: 140.70
 ---- batch: 070 ----
mean loss: 142.26
 ---- batch: 080 ----
mean loss: 153.79
 ---- batch: 090 ----
mean loss: 145.27
 ---- batch: 100 ----
mean loss: 147.36
 ---- batch: 110 ----
mean loss: 141.12
train mean loss: 142.97
epoch train time: 0:00:01.911746
elapsed time: 0:05:16.972613
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 22:24:54.811088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.08
 ---- batch: 020 ----
mean loss: 138.10
 ---- batch: 030 ----
mean loss: 132.71
 ---- batch: 040 ----
mean loss: 131.79
 ---- batch: 050 ----
mean loss: 136.24
 ---- batch: 060 ----
mean loss: 143.80
 ---- batch: 070 ----
mean loss: 149.16
 ---- batch: 080 ----
mean loss: 148.61
 ---- batch: 090 ----
mean loss: 150.19
 ---- batch: 100 ----
mean loss: 145.36
 ---- batch: 110 ----
mean loss: 144.36
train mean loss: 142.36
epoch train time: 0:00:01.900244
elapsed time: 0:05:18.873155
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 22:24:56.711994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.03
 ---- batch: 020 ----
mean loss: 137.17
 ---- batch: 030 ----
mean loss: 142.97
 ---- batch: 040 ----
mean loss: 136.45
 ---- batch: 050 ----
mean loss: 142.34
 ---- batch: 060 ----
mean loss: 147.46
 ---- batch: 070 ----
mean loss: 143.55
 ---- batch: 080 ----
mean loss: 148.02
 ---- batch: 090 ----
mean loss: 144.86
 ---- batch: 100 ----
mean loss: 138.94
 ---- batch: 110 ----
mean loss: 139.64
train mean loss: 142.10
epoch train time: 0:00:01.912124
elapsed time: 0:05:20.785903
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 22:24:58.624424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.89
 ---- batch: 020 ----
mean loss: 137.47
 ---- batch: 030 ----
mean loss: 145.97
 ---- batch: 040 ----
mean loss: 141.62
 ---- batch: 050 ----
mean loss: 133.11
 ---- batch: 060 ----
mean loss: 138.91
 ---- batch: 070 ----
mean loss: 144.99
 ---- batch: 080 ----
mean loss: 144.18
 ---- batch: 090 ----
mean loss: 141.96
 ---- batch: 100 ----
mean loss: 148.92
 ---- batch: 110 ----
mean loss: 141.51
train mean loss: 141.89
epoch train time: 0:00:01.897671
elapsed time: 0:05:22.683867
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 22:25:00.522685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.63
 ---- batch: 020 ----
mean loss: 134.26
 ---- batch: 030 ----
mean loss: 139.27
 ---- batch: 040 ----
mean loss: 134.79
 ---- batch: 050 ----
mean loss: 142.47
 ---- batch: 060 ----
mean loss: 139.47
 ---- batch: 070 ----
mean loss: 140.81
 ---- batch: 080 ----
mean loss: 146.55
 ---- batch: 090 ----
mean loss: 146.47
 ---- batch: 100 ----
mean loss: 142.40
 ---- batch: 110 ----
mean loss: 139.08
train mean loss: 141.21
epoch train time: 0:00:01.910802
elapsed time: 0:05:24.595309
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 22:25:02.434138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.20
 ---- batch: 020 ----
mean loss: 133.42
 ---- batch: 030 ----
mean loss: 143.85
 ---- batch: 040 ----
mean loss: 135.52
 ---- batch: 050 ----
mean loss: 141.17
 ---- batch: 060 ----
mean loss: 144.94
 ---- batch: 070 ----
mean loss: 146.96
 ---- batch: 080 ----
mean loss: 142.28
 ---- batch: 090 ----
mean loss: 136.74
 ---- batch: 100 ----
mean loss: 144.25
 ---- batch: 110 ----
mean loss: 139.56
train mean loss: 141.31
epoch train time: 0:00:01.901972
elapsed time: 0:05:26.497924
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 22:25:04.336797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.20
 ---- batch: 020 ----
mean loss: 134.46
 ---- batch: 030 ----
mean loss: 140.82
 ---- batch: 040 ----
mean loss: 142.17
 ---- batch: 050 ----
mean loss: 140.16
 ---- batch: 060 ----
mean loss: 139.70
 ---- batch: 070 ----
mean loss: 138.24
 ---- batch: 080 ----
mean loss: 136.48
 ---- batch: 090 ----
mean loss: 148.81
 ---- batch: 100 ----
mean loss: 138.57
 ---- batch: 110 ----
mean loss: 147.49
train mean loss: 140.23
epoch train time: 0:00:01.905452
elapsed time: 0:05:28.404008
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 22:25:06.242810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.99
 ---- batch: 020 ----
mean loss: 136.65
 ---- batch: 030 ----
mean loss: 140.57
 ---- batch: 040 ----
mean loss: 139.05
 ---- batch: 050 ----
mean loss: 137.86
 ---- batch: 060 ----
mean loss: 134.65
 ---- batch: 070 ----
mean loss: 149.76
 ---- batch: 080 ----
mean loss: 140.84
 ---- batch: 090 ----
mean loss: 148.03
 ---- batch: 100 ----
mean loss: 142.34
 ---- batch: 110 ----
mean loss: 140.58
train mean loss: 140.89
epoch train time: 0:00:01.896053
elapsed time: 0:05:30.300643
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 22:25:08.139449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.74
 ---- batch: 020 ----
mean loss: 133.40
 ---- batch: 030 ----
mean loss: 144.97
 ---- batch: 040 ----
mean loss: 150.37
 ---- batch: 050 ----
mean loss: 137.22
 ---- batch: 060 ----
mean loss: 137.68
 ---- batch: 070 ----
mean loss: 139.54
 ---- batch: 080 ----
mean loss: 139.79
 ---- batch: 090 ----
mean loss: 136.35
 ---- batch: 100 ----
mean loss: 142.86
 ---- batch: 110 ----
mean loss: 139.36
train mean loss: 139.86
epoch train time: 0:00:01.871725
elapsed time: 0:05:32.172943
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 22:25:10.011808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.47
 ---- batch: 020 ----
mean loss: 134.20
 ---- batch: 030 ----
mean loss: 141.22
 ---- batch: 040 ----
mean loss: 133.73
 ---- batch: 050 ----
mean loss: 131.01
 ---- batch: 060 ----
mean loss: 147.19
 ---- batch: 070 ----
mean loss: 132.28
 ---- batch: 080 ----
mean loss: 140.43
 ---- batch: 090 ----
mean loss: 148.70
 ---- batch: 100 ----
mean loss: 140.16
 ---- batch: 110 ----
mean loss: 149.56
train mean loss: 139.41
epoch train time: 0:00:01.916543
elapsed time: 0:05:34.090191
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 22:25:11.929186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.46
 ---- batch: 020 ----
mean loss: 137.40
 ---- batch: 030 ----
mean loss: 134.33
 ---- batch: 040 ----
mean loss: 133.34
 ---- batch: 050 ----
mean loss: 138.98
 ---- batch: 060 ----
mean loss: 132.76
 ---- batch: 070 ----
mean loss: 141.75
 ---- batch: 080 ----
mean loss: 140.25
 ---- batch: 090 ----
mean loss: 129.32
 ---- batch: 100 ----
mean loss: 141.87
 ---- batch: 110 ----
mean loss: 142.82
train mean loss: 138.01
epoch train time: 0:00:01.903368
elapsed time: 0:05:35.994394
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 22:25:13.833212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.47
 ---- batch: 020 ----
mean loss: 138.83
 ---- batch: 030 ----
mean loss: 132.38
 ---- batch: 040 ----
mean loss: 142.83
 ---- batch: 050 ----
mean loss: 139.61
 ---- batch: 060 ----
mean loss: 132.43
 ---- batch: 070 ----
mean loss: 139.52
 ---- batch: 080 ----
mean loss: 141.67
 ---- batch: 090 ----
mean loss: 137.64
 ---- batch: 100 ----
mean loss: 147.98
 ---- batch: 110 ----
mean loss: 133.00
train mean loss: 138.56
epoch train time: 0:00:01.907510
elapsed time: 0:05:37.902494
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 22:25:15.741310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.22
 ---- batch: 020 ----
mean loss: 131.88
 ---- batch: 030 ----
mean loss: 138.41
 ---- batch: 040 ----
mean loss: 143.64
 ---- batch: 050 ----
mean loss: 148.40
 ---- batch: 060 ----
mean loss: 132.65
 ---- batch: 070 ----
mean loss: 132.12
 ---- batch: 080 ----
mean loss: 145.52
 ---- batch: 090 ----
mean loss: 145.89
 ---- batch: 100 ----
mean loss: 133.39
 ---- batch: 110 ----
mean loss: 130.06
train mean loss: 138.04
epoch train time: 0:00:01.885801
elapsed time: 0:05:39.788885
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 22:25:17.627726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.81
 ---- batch: 020 ----
mean loss: 136.90
 ---- batch: 030 ----
mean loss: 131.64
 ---- batch: 040 ----
mean loss: 139.81
 ---- batch: 050 ----
mean loss: 143.95
 ---- batch: 060 ----
mean loss: 139.66
 ---- batch: 070 ----
mean loss: 135.22
 ---- batch: 080 ----
mean loss: 135.47
 ---- batch: 090 ----
mean loss: 136.88
 ---- batch: 100 ----
mean loss: 143.46
 ---- batch: 110 ----
mean loss: 136.06
train mean loss: 137.43
epoch train time: 0:00:01.902784
elapsed time: 0:05:41.692306
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 22:25:19.531133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.48
 ---- batch: 020 ----
mean loss: 137.72
 ---- batch: 030 ----
mean loss: 136.57
 ---- batch: 040 ----
mean loss: 129.61
 ---- batch: 050 ----
mean loss: 145.08
 ---- batch: 060 ----
mean loss: 138.63
 ---- batch: 070 ----
mean loss: 137.07
 ---- batch: 080 ----
mean loss: 123.69
 ---- batch: 090 ----
mean loss: 135.04
 ---- batch: 100 ----
mean loss: 144.39
 ---- batch: 110 ----
mean loss: 142.69
train mean loss: 137.60
epoch train time: 0:00:01.900165
elapsed time: 0:05:43.593118
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 22:25:21.432057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.26
 ---- batch: 020 ----
mean loss: 133.26
 ---- batch: 030 ----
mean loss: 138.36
 ---- batch: 040 ----
mean loss: 145.12
 ---- batch: 050 ----
mean loss: 139.65
 ---- batch: 060 ----
mean loss: 133.34
 ---- batch: 070 ----
mean loss: 135.05
 ---- batch: 080 ----
mean loss: 135.24
 ---- batch: 090 ----
mean loss: 134.43
 ---- batch: 100 ----
mean loss: 130.09
 ---- batch: 110 ----
mean loss: 138.55
train mean loss: 136.75
epoch train time: 0:00:01.902038
elapsed time: 0:05:45.495903
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 22:25:23.334758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.31
 ---- batch: 020 ----
mean loss: 140.28
 ---- batch: 030 ----
mean loss: 139.25
 ---- batch: 040 ----
mean loss: 134.27
 ---- batch: 050 ----
mean loss: 137.53
 ---- batch: 060 ----
mean loss: 135.93
 ---- batch: 070 ----
mean loss: 145.13
 ---- batch: 080 ----
mean loss: 137.22
 ---- batch: 090 ----
mean loss: 131.19
 ---- batch: 100 ----
mean loss: 140.09
 ---- batch: 110 ----
mean loss: 135.01
train mean loss: 137.24
epoch train time: 0:00:01.933854
elapsed time: 0:05:47.430375
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 22:25:25.269171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.82
 ---- batch: 020 ----
mean loss: 129.82
 ---- batch: 030 ----
mean loss: 135.15
 ---- batch: 040 ----
mean loss: 136.18
 ---- batch: 050 ----
mean loss: 148.76
 ---- batch: 060 ----
mean loss: 133.73
 ---- batch: 070 ----
mean loss: 135.85
 ---- batch: 080 ----
mean loss: 139.18
 ---- batch: 090 ----
mean loss: 140.34
 ---- batch: 100 ----
mean loss: 134.66
 ---- batch: 110 ----
mean loss: 126.88
train mean loss: 136.16
epoch train time: 0:00:01.918875
elapsed time: 0:05:49.349850
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 22:25:27.188667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.99
 ---- batch: 020 ----
mean loss: 137.73
 ---- batch: 030 ----
mean loss: 132.79
 ---- batch: 040 ----
mean loss: 135.78
 ---- batch: 050 ----
mean loss: 131.95
 ---- batch: 060 ----
mean loss: 133.19
 ---- batch: 070 ----
mean loss: 130.18
 ---- batch: 080 ----
mean loss: 144.25
 ---- batch: 090 ----
mean loss: 134.42
 ---- batch: 100 ----
mean loss: 151.51
 ---- batch: 110 ----
mean loss: 131.35
train mean loss: 135.76
epoch train time: 0:00:01.934339
elapsed time: 0:05:51.284781
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 22:25:29.123564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.97
 ---- batch: 020 ----
mean loss: 132.09
 ---- batch: 030 ----
mean loss: 131.48
 ---- batch: 040 ----
mean loss: 130.16
 ---- batch: 050 ----
mean loss: 135.60
 ---- batch: 060 ----
mean loss: 132.42
 ---- batch: 070 ----
mean loss: 139.12
 ---- batch: 080 ----
mean loss: 131.75
 ---- batch: 090 ----
mean loss: 145.17
 ---- batch: 100 ----
mean loss: 142.21
 ---- batch: 110 ----
mean loss: 142.80
train mean loss: 135.72
epoch train time: 0:00:01.934401
elapsed time: 0:05:53.219752
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 22:25:31.058556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.55
 ---- batch: 020 ----
mean loss: 133.57
 ---- batch: 030 ----
mean loss: 130.57
 ---- batch: 040 ----
mean loss: 142.59
 ---- batch: 050 ----
mean loss: 144.84
 ---- batch: 060 ----
mean loss: 127.91
 ---- batch: 070 ----
mean loss: 124.06
 ---- batch: 080 ----
mean loss: 136.09
 ---- batch: 090 ----
mean loss: 137.42
 ---- batch: 100 ----
mean loss: 134.86
 ---- batch: 110 ----
mean loss: 139.94
train mean loss: 134.89
epoch train time: 0:00:01.897182
elapsed time: 0:05:55.117497
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 22:25:32.956289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.21
 ---- batch: 020 ----
mean loss: 136.51
 ---- batch: 030 ----
mean loss: 128.70
 ---- batch: 040 ----
mean loss: 142.86
 ---- batch: 050 ----
mean loss: 136.11
 ---- batch: 060 ----
mean loss: 134.07
 ---- batch: 070 ----
mean loss: 136.83
 ---- batch: 080 ----
mean loss: 137.01
 ---- batch: 090 ----
mean loss: 135.33
 ---- batch: 100 ----
mean loss: 138.85
 ---- batch: 110 ----
mean loss: 131.61
train mean loss: 135.43
epoch train time: 0:00:01.855581
elapsed time: 0:05:56.973670
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 22:25:34.812529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.31
 ---- batch: 020 ----
mean loss: 135.20
 ---- batch: 030 ----
mean loss: 133.55
 ---- batch: 040 ----
mean loss: 133.38
 ---- batch: 050 ----
mean loss: 134.85
 ---- batch: 060 ----
mean loss: 136.69
 ---- batch: 070 ----
mean loss: 128.62
 ---- batch: 080 ----
mean loss: 140.16
 ---- batch: 090 ----
mean loss: 145.54
 ---- batch: 100 ----
mean loss: 127.49
 ---- batch: 110 ----
mean loss: 129.27
train mean loss: 134.83
epoch train time: 0:00:01.893104
elapsed time: 0:05:58.867466
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 22:25:36.706298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.20
 ---- batch: 020 ----
mean loss: 145.28
 ---- batch: 030 ----
mean loss: 141.27
 ---- batch: 040 ----
mean loss: 130.86
 ---- batch: 050 ----
mean loss: 125.60
 ---- batch: 060 ----
mean loss: 129.86
 ---- batch: 070 ----
mean loss: 147.66
 ---- batch: 080 ----
mean loss: 134.72
 ---- batch: 090 ----
mean loss: 132.04
 ---- batch: 100 ----
mean loss: 136.17
 ---- batch: 110 ----
mean loss: 133.02
train mean loss: 134.73
epoch train time: 0:00:01.904069
elapsed time: 0:06:00.772197
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 22:25:38.611002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.63
 ---- batch: 020 ----
mean loss: 132.04
 ---- batch: 030 ----
mean loss: 130.90
 ---- batch: 040 ----
mean loss: 131.43
 ---- batch: 050 ----
mean loss: 136.97
 ---- batch: 060 ----
mean loss: 130.44
 ---- batch: 070 ----
mean loss: 126.33
 ---- batch: 080 ----
mean loss: 133.13
 ---- batch: 090 ----
mean loss: 140.17
 ---- batch: 100 ----
mean loss: 136.80
 ---- batch: 110 ----
mean loss: 141.16
train mean loss: 133.63
epoch train time: 0:00:01.924601
elapsed time: 0:06:02.697392
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 22:25:40.536178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.12
 ---- batch: 020 ----
mean loss: 131.28
 ---- batch: 030 ----
mean loss: 133.43
 ---- batch: 040 ----
mean loss: 133.27
 ---- batch: 050 ----
mean loss: 127.68
 ---- batch: 060 ----
mean loss: 130.98
 ---- batch: 070 ----
mean loss: 140.51
 ---- batch: 080 ----
mean loss: 128.72
 ---- batch: 090 ----
mean loss: 140.63
 ---- batch: 100 ----
mean loss: 125.83
 ---- batch: 110 ----
mean loss: 142.71
train mean loss: 133.93
epoch train time: 0:00:01.889111
elapsed time: 0:06:04.587406
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 22:25:42.425882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.41
 ---- batch: 020 ----
mean loss: 135.43
 ---- batch: 030 ----
mean loss: 126.98
 ---- batch: 040 ----
mean loss: 131.68
 ---- batch: 050 ----
mean loss: 138.56
 ---- batch: 060 ----
mean loss: 131.00
 ---- batch: 070 ----
mean loss: 137.06
 ---- batch: 080 ----
mean loss: 130.82
 ---- batch: 090 ----
mean loss: 137.23
 ---- batch: 100 ----
mean loss: 139.49
 ---- batch: 110 ----
mean loss: 136.72
train mean loss: 133.29
epoch train time: 0:00:01.907350
elapsed time: 0:06:06.494999
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 22:25:44.333850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.96
 ---- batch: 020 ----
mean loss: 132.26
 ---- batch: 030 ----
mean loss: 130.90
 ---- batch: 040 ----
mean loss: 133.52
 ---- batch: 050 ----
mean loss: 128.35
 ---- batch: 060 ----
mean loss: 135.57
 ---- batch: 070 ----
mean loss: 132.86
 ---- batch: 080 ----
mean loss: 130.83
 ---- batch: 090 ----
mean loss: 138.37
 ---- batch: 100 ----
mean loss: 133.91
 ---- batch: 110 ----
mean loss: 132.54
train mean loss: 133.32
epoch train time: 0:00:01.898503
elapsed time: 0:06:08.394128
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 22:25:46.233010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.50
 ---- batch: 020 ----
mean loss: 133.72
 ---- batch: 030 ----
mean loss: 123.53
 ---- batch: 040 ----
mean loss: 142.19
 ---- batch: 050 ----
mean loss: 134.36
 ---- batch: 060 ----
mean loss: 131.35
 ---- batch: 070 ----
mean loss: 129.36
 ---- batch: 080 ----
mean loss: 133.16
 ---- batch: 090 ----
mean loss: 124.52
 ---- batch: 100 ----
mean loss: 134.68
 ---- batch: 110 ----
mean loss: 134.80
train mean loss: 132.27
epoch train time: 0:00:01.913599
elapsed time: 0:06:10.308407
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 22:25:48.147213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.70
 ---- batch: 020 ----
mean loss: 138.75
 ---- batch: 030 ----
mean loss: 136.59
 ---- batch: 040 ----
mean loss: 124.94
 ---- batch: 050 ----
mean loss: 131.01
 ---- batch: 060 ----
mean loss: 132.93
 ---- batch: 070 ----
mean loss: 126.65
 ---- batch: 080 ----
mean loss: 130.54
 ---- batch: 090 ----
mean loss: 133.64
 ---- batch: 100 ----
mean loss: 138.11
 ---- batch: 110 ----
mean loss: 132.53
train mean loss: 132.56
epoch train time: 0:00:01.921544
elapsed time: 0:06:12.230540
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 22:25:50.069391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.55
 ---- batch: 020 ----
mean loss: 135.07
 ---- batch: 030 ----
mean loss: 135.79
 ---- batch: 040 ----
mean loss: 123.57
 ---- batch: 050 ----
mean loss: 129.35
 ---- batch: 060 ----
mean loss: 133.95
 ---- batch: 070 ----
mean loss: 127.55
 ---- batch: 080 ----
mean loss: 131.22
 ---- batch: 090 ----
mean loss: 136.32
 ---- batch: 100 ----
mean loss: 128.78
 ---- batch: 110 ----
mean loss: 137.99
train mean loss: 131.74
epoch train time: 0:00:01.914274
elapsed time: 0:06:14.145441
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 22:25:51.984256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.02
 ---- batch: 020 ----
mean loss: 126.97
 ---- batch: 030 ----
mean loss: 127.76
 ---- batch: 040 ----
mean loss: 140.83
 ---- batch: 050 ----
mean loss: 132.31
 ---- batch: 060 ----
mean loss: 123.09
 ---- batch: 070 ----
mean loss: 131.18
 ---- batch: 080 ----
mean loss: 137.19
 ---- batch: 090 ----
mean loss: 129.49
 ---- batch: 100 ----
mean loss: 130.51
 ---- batch: 110 ----
mean loss: 138.20
train mean loss: 131.54
epoch train time: 0:00:01.917338
elapsed time: 0:06:16.063397
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 22:25:53.902210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.25
 ---- batch: 020 ----
mean loss: 132.10
 ---- batch: 030 ----
mean loss: 127.04
 ---- batch: 040 ----
mean loss: 129.26
 ---- batch: 050 ----
mean loss: 141.69
 ---- batch: 060 ----
mean loss: 132.03
 ---- batch: 070 ----
mean loss: 134.77
 ---- batch: 080 ----
mean loss: 132.26
 ---- batch: 090 ----
mean loss: 127.82
 ---- batch: 100 ----
mean loss: 132.66
 ---- batch: 110 ----
mean loss: 135.03
train mean loss: 131.50
epoch train time: 0:00:01.853162
elapsed time: 0:06:17.917122
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 22:25:55.755913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.47
 ---- batch: 020 ----
mean loss: 129.62
 ---- batch: 030 ----
mean loss: 137.94
 ---- batch: 040 ----
mean loss: 121.58
 ---- batch: 050 ----
mean loss: 130.67
 ---- batch: 060 ----
mean loss: 122.75
 ---- batch: 070 ----
mean loss: 127.88
 ---- batch: 080 ----
mean loss: 135.55
 ---- batch: 090 ----
mean loss: 135.75
 ---- batch: 100 ----
mean loss: 134.31
 ---- batch: 110 ----
mean loss: 136.70
train mean loss: 130.64
epoch train time: 0:00:01.908549
elapsed time: 0:06:19.826247
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 22:25:57.665062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.96
 ---- batch: 020 ----
mean loss: 131.63
 ---- batch: 030 ----
mean loss: 130.20
 ---- batch: 040 ----
mean loss: 130.73
 ---- batch: 050 ----
mean loss: 127.79
 ---- batch: 060 ----
mean loss: 134.71
 ---- batch: 070 ----
mean loss: 129.96
 ---- batch: 080 ----
mean loss: 128.33
 ---- batch: 090 ----
mean loss: 136.00
 ---- batch: 100 ----
mean loss: 134.65
 ---- batch: 110 ----
mean loss: 128.78
train mean loss: 131.23
epoch train time: 0:00:01.919879
elapsed time: 0:06:21.746713
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 22:25:59.585543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.88
 ---- batch: 020 ----
mean loss: 132.63
 ---- batch: 030 ----
mean loss: 121.26
 ---- batch: 040 ----
mean loss: 130.92
 ---- batch: 050 ----
mean loss: 123.57
 ---- batch: 060 ----
mean loss: 122.50
 ---- batch: 070 ----
mean loss: 133.34
 ---- batch: 080 ----
mean loss: 138.50
 ---- batch: 090 ----
mean loss: 137.01
 ---- batch: 100 ----
mean loss: 130.61
 ---- batch: 110 ----
mean loss: 129.43
train mean loss: 130.23
epoch train time: 0:00:01.917300
elapsed time: 0:06:23.664629
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 22:26:01.503437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.78
 ---- batch: 020 ----
mean loss: 127.83
 ---- batch: 030 ----
mean loss: 130.65
 ---- batch: 040 ----
mean loss: 124.59
 ---- batch: 050 ----
mean loss: 134.88
 ---- batch: 060 ----
mean loss: 130.79
 ---- batch: 070 ----
mean loss: 135.66
 ---- batch: 080 ----
mean loss: 131.83
 ---- batch: 090 ----
mean loss: 124.31
 ---- batch: 100 ----
mean loss: 130.28
 ---- batch: 110 ----
mean loss: 139.17
train mean loss: 130.46
epoch train time: 0:00:01.885913
elapsed time: 0:06:25.551129
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 22:26:03.390171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.31
 ---- batch: 020 ----
mean loss: 123.13
 ---- batch: 030 ----
mean loss: 135.28
 ---- batch: 040 ----
mean loss: 124.43
 ---- batch: 050 ----
mean loss: 134.64
 ---- batch: 060 ----
mean loss: 127.99
 ---- batch: 070 ----
mean loss: 129.95
 ---- batch: 080 ----
mean loss: 123.77
 ---- batch: 090 ----
mean loss: 129.80
 ---- batch: 100 ----
mean loss: 133.86
 ---- batch: 110 ----
mean loss: 135.92
train mean loss: 129.77
epoch train time: 0:00:01.872414
elapsed time: 0:06:27.424342
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 22:26:05.263148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.19
 ---- batch: 020 ----
mean loss: 130.64
 ---- batch: 030 ----
mean loss: 134.18
 ---- batch: 040 ----
mean loss: 126.39
 ---- batch: 050 ----
mean loss: 130.18
 ---- batch: 060 ----
mean loss: 129.40
 ---- batch: 070 ----
mean loss: 126.91
 ---- batch: 080 ----
mean loss: 130.65
 ---- batch: 090 ----
mean loss: 132.58
 ---- batch: 100 ----
mean loss: 125.24
 ---- batch: 110 ----
mean loss: 131.56
train mean loss: 129.29
epoch train time: 0:00:01.951093
elapsed time: 0:06:29.376093
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 22:26:07.214886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.04
 ---- batch: 020 ----
mean loss: 123.12
 ---- batch: 030 ----
mean loss: 125.12
 ---- batch: 040 ----
mean loss: 130.79
 ---- batch: 050 ----
mean loss: 134.49
 ---- batch: 060 ----
mean loss: 136.19
 ---- batch: 070 ----
mean loss: 124.06
 ---- batch: 080 ----
mean loss: 124.67
 ---- batch: 090 ----
mean loss: 136.77
 ---- batch: 100 ----
mean loss: 130.07
 ---- batch: 110 ----
mean loss: 131.65
train mean loss: 129.04
epoch train time: 0:00:01.894392
elapsed time: 0:06:31.271043
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 22:26:09.109897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.38
 ---- batch: 020 ----
mean loss: 131.22
 ---- batch: 030 ----
mean loss: 136.48
 ---- batch: 040 ----
mean loss: 119.07
 ---- batch: 050 ----
mean loss: 128.35
 ---- batch: 060 ----
mean loss: 128.86
 ---- batch: 070 ----
mean loss: 130.06
 ---- batch: 080 ----
mean loss: 129.31
 ---- batch: 090 ----
mean loss: 129.53
 ---- batch: 100 ----
mean loss: 125.01
 ---- batch: 110 ----
mean loss: 126.15
train mean loss: 128.52
epoch train time: 0:00:01.889026
elapsed time: 0:06:33.160704
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 22:26:10.999530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.96
 ---- batch: 020 ----
mean loss: 126.55
 ---- batch: 030 ----
mean loss: 141.25
 ---- batch: 040 ----
mean loss: 129.18
 ---- batch: 050 ----
mean loss: 121.79
 ---- batch: 060 ----
mean loss: 124.15
 ---- batch: 070 ----
mean loss: 125.62
 ---- batch: 080 ----
mean loss: 134.95
 ---- batch: 090 ----
mean loss: 129.79
 ---- batch: 100 ----
mean loss: 120.26
 ---- batch: 110 ----
mean loss: 126.80
train mean loss: 128.00
epoch train time: 0:00:01.904649
elapsed time: 0:06:35.065955
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 22:26:12.904832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.42
 ---- batch: 020 ----
mean loss: 118.76
 ---- batch: 030 ----
mean loss: 131.61
 ---- batch: 040 ----
mean loss: 124.78
 ---- batch: 050 ----
mean loss: 124.13
 ---- batch: 060 ----
mean loss: 131.67
 ---- batch: 070 ----
mean loss: 139.19
 ---- batch: 080 ----
mean loss: 127.28
 ---- batch: 090 ----
mean loss: 126.22
 ---- batch: 100 ----
mean loss: 129.81
 ---- batch: 110 ----
mean loss: 133.68
train mean loss: 127.89
epoch train time: 0:00:01.854205
elapsed time: 0:06:36.920802
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 22:26:14.759576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.54
 ---- batch: 020 ----
mean loss: 120.76
 ---- batch: 030 ----
mean loss: 123.14
 ---- batch: 040 ----
mean loss: 125.92
 ---- batch: 050 ----
mean loss: 128.85
 ---- batch: 060 ----
mean loss: 128.71
 ---- batch: 070 ----
mean loss: 127.70
 ---- batch: 080 ----
mean loss: 125.15
 ---- batch: 090 ----
mean loss: 131.66
 ---- batch: 100 ----
mean loss: 132.52
 ---- batch: 110 ----
mean loss: 130.25
train mean loss: 127.41
epoch train time: 0:00:01.923476
elapsed time: 0:06:38.844835
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 22:26:16.683645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.51
 ---- batch: 020 ----
mean loss: 121.07
 ---- batch: 030 ----
mean loss: 132.04
 ---- batch: 040 ----
mean loss: 128.36
 ---- batch: 050 ----
mean loss: 129.69
 ---- batch: 060 ----
mean loss: 131.58
 ---- batch: 070 ----
mean loss: 120.60
 ---- batch: 080 ----
mean loss: 127.14
 ---- batch: 090 ----
mean loss: 120.61
 ---- batch: 100 ----
mean loss: 128.14
 ---- batch: 110 ----
mean loss: 130.80
train mean loss: 127.38
epoch train time: 0:00:01.904307
elapsed time: 0:06:40.749714
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 22:26:18.588519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.74
 ---- batch: 020 ----
mean loss: 129.60
 ---- batch: 030 ----
mean loss: 121.63
 ---- batch: 040 ----
mean loss: 131.05
 ---- batch: 050 ----
mean loss: 128.64
 ---- batch: 060 ----
mean loss: 121.77
 ---- batch: 070 ----
mean loss: 121.64
 ---- batch: 080 ----
mean loss: 120.90
 ---- batch: 090 ----
mean loss: 126.44
 ---- batch: 100 ----
mean loss: 137.21
 ---- batch: 110 ----
mean loss: 137.27
train mean loss: 127.21
epoch train time: 0:00:01.945616
elapsed time: 0:06:42.695994
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 22:26:20.534802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.04
 ---- batch: 020 ----
mean loss: 134.71
 ---- batch: 030 ----
mean loss: 127.13
 ---- batch: 040 ----
mean loss: 126.70
 ---- batch: 050 ----
mean loss: 121.02
 ---- batch: 060 ----
mean loss: 120.08
 ---- batch: 070 ----
mean loss: 131.65
 ---- batch: 080 ----
mean loss: 123.54
 ---- batch: 090 ----
mean loss: 123.46
 ---- batch: 100 ----
mean loss: 134.75
 ---- batch: 110 ----
mean loss: 125.44
train mean loss: 126.82
epoch train time: 0:00:01.905929
elapsed time: 0:06:44.602477
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 22:26:22.441340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.70
 ---- batch: 020 ----
mean loss: 129.11
 ---- batch: 030 ----
mean loss: 132.81
 ---- batch: 040 ----
mean loss: 127.34
 ---- batch: 050 ----
mean loss: 119.55
 ---- batch: 060 ----
mean loss: 125.79
 ---- batch: 070 ----
mean loss: 132.06
 ---- batch: 080 ----
mean loss: 122.43
 ---- batch: 090 ----
mean loss: 132.90
 ---- batch: 100 ----
mean loss: 123.61
 ---- batch: 110 ----
mean loss: 131.00
train mean loss: 127.20
epoch train time: 0:00:01.920723
elapsed time: 0:06:46.523817
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 22:26:24.362634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.48
 ---- batch: 020 ----
mean loss: 119.55
 ---- batch: 030 ----
mean loss: 123.71
 ---- batch: 040 ----
mean loss: 134.81
 ---- batch: 050 ----
mean loss: 123.45
 ---- batch: 060 ----
mean loss: 123.08
 ---- batch: 070 ----
mean loss: 130.87
 ---- batch: 080 ----
mean loss: 126.30
 ---- batch: 090 ----
mean loss: 129.62
 ---- batch: 100 ----
mean loss: 121.09
 ---- batch: 110 ----
mean loss: 132.12
train mean loss: 126.71
epoch train time: 0:00:01.906857
elapsed time: 0:06:48.431275
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 22:26:26.270090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.41
 ---- batch: 020 ----
mean loss: 122.75
 ---- batch: 030 ----
mean loss: 126.57
 ---- batch: 040 ----
mean loss: 120.18
 ---- batch: 050 ----
mean loss: 129.17
 ---- batch: 060 ----
mean loss: 121.14
 ---- batch: 070 ----
mean loss: 136.08
 ---- batch: 080 ----
mean loss: 132.11
 ---- batch: 090 ----
mean loss: 130.71
 ---- batch: 100 ----
mean loss: 126.91
 ---- batch: 110 ----
mean loss: 120.67
train mean loss: 126.54
epoch train time: 0:00:01.879158
elapsed time: 0:06:50.311013
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 22:26:28.149906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.58
 ---- batch: 020 ----
mean loss: 123.29
 ---- batch: 030 ----
mean loss: 121.70
 ---- batch: 040 ----
mean loss: 125.00
 ---- batch: 050 ----
mean loss: 122.33
 ---- batch: 060 ----
mean loss: 130.11
 ---- batch: 070 ----
mean loss: 118.90
 ---- batch: 080 ----
mean loss: 128.86
 ---- batch: 090 ----
mean loss: 123.55
 ---- batch: 100 ----
mean loss: 127.66
 ---- batch: 110 ----
mean loss: 130.13
train mean loss: 125.81
epoch train time: 0:00:01.913013
elapsed time: 0:06:52.224673
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 22:26:30.063467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.32
 ---- batch: 020 ----
mean loss: 122.64
 ---- batch: 030 ----
mean loss: 128.34
 ---- batch: 040 ----
mean loss: 116.26
 ---- batch: 050 ----
mean loss: 123.91
 ---- batch: 060 ----
mean loss: 135.22
 ---- batch: 070 ----
mean loss: 126.70
 ---- batch: 080 ----
mean loss: 128.30
 ---- batch: 090 ----
mean loss: 126.71
 ---- batch: 100 ----
mean loss: 121.54
 ---- batch: 110 ----
mean loss: 132.04
train mean loss: 125.79
epoch train time: 0:00:01.889096
elapsed time: 0:06:54.114352
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 22:26:31.953139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.74
 ---- batch: 020 ----
mean loss: 116.03
 ---- batch: 030 ----
mean loss: 121.75
 ---- batch: 040 ----
mean loss: 119.36
 ---- batch: 050 ----
mean loss: 126.29
 ---- batch: 060 ----
mean loss: 127.46
 ---- batch: 070 ----
mean loss: 127.70
 ---- batch: 080 ----
mean loss: 122.19
 ---- batch: 090 ----
mean loss: 133.36
 ---- batch: 100 ----
mean loss: 126.91
 ---- batch: 110 ----
mean loss: 135.38
train mean loss: 125.70
epoch train time: 0:00:01.899735
elapsed time: 0:06:56.014644
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 22:26:33.853437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.46
 ---- batch: 020 ----
mean loss: 118.88
 ---- batch: 030 ----
mean loss: 127.94
 ---- batch: 040 ----
mean loss: 119.28
 ---- batch: 050 ----
mean loss: 117.43
 ---- batch: 060 ----
mean loss: 120.32
 ---- batch: 070 ----
mean loss: 115.53
 ---- batch: 080 ----
mean loss: 116.68
 ---- batch: 090 ----
mean loss: 124.38
 ---- batch: 100 ----
mean loss: 127.41
 ---- batch: 110 ----
mean loss: 117.04
train mean loss: 120.28
epoch train time: 0:00:01.919724
elapsed time: 0:06:57.935301
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 22:26:35.773804
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.40
 ---- batch: 020 ----
mean loss: 117.13
 ---- batch: 030 ----
mean loss: 114.76
 ---- batch: 040 ----
mean loss: 119.40
 ---- batch: 050 ----
mean loss: 124.21
 ---- batch: 060 ----
mean loss: 118.03
 ---- batch: 070 ----
mean loss: 114.70
 ---- batch: 080 ----
mean loss: 123.35
 ---- batch: 090 ----
mean loss: 119.12
 ---- batch: 100 ----
mean loss: 118.96
 ---- batch: 110 ----
mean loss: 118.76
train mean loss: 119.51
epoch train time: 0:00:01.917582
elapsed time: 0:06:59.853182
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 22:26:37.692130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.01
 ---- batch: 020 ----
mean loss: 119.47
 ---- batch: 030 ----
mean loss: 122.65
 ---- batch: 040 ----
mean loss: 118.02
 ---- batch: 050 ----
mean loss: 121.02
 ---- batch: 060 ----
mean loss: 119.34
 ---- batch: 070 ----
mean loss: 116.67
 ---- batch: 080 ----
mean loss: 124.74
 ---- batch: 090 ----
mean loss: 119.82
 ---- batch: 100 ----
mean loss: 122.31
 ---- batch: 110 ----
mean loss: 115.34
train mean loss: 119.34
epoch train time: 0:00:01.909900
elapsed time: 0:07:01.763869
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 22:26:39.602378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.05
 ---- batch: 020 ----
mean loss: 117.57
 ---- batch: 030 ----
mean loss: 116.51
 ---- batch: 040 ----
mean loss: 114.76
 ---- batch: 050 ----
mean loss: 121.05
 ---- batch: 060 ----
mean loss: 120.91
 ---- batch: 070 ----
mean loss: 125.98
 ---- batch: 080 ----
mean loss: 120.28
 ---- batch: 090 ----
mean loss: 124.32
 ---- batch: 100 ----
mean loss: 111.77
 ---- batch: 110 ----
mean loss: 115.51
train mean loss: 119.27
epoch train time: 0:00:01.911643
elapsed time: 0:07:03.675830
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 22:26:41.514722
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.86
 ---- batch: 020 ----
mean loss: 117.47
 ---- batch: 030 ----
mean loss: 115.96
 ---- batch: 040 ----
mean loss: 118.55
 ---- batch: 050 ----
mean loss: 121.41
 ---- batch: 060 ----
mean loss: 121.64
 ---- batch: 070 ----
mean loss: 121.05
 ---- batch: 080 ----
mean loss: 124.81
 ---- batch: 090 ----
mean loss: 111.37
 ---- batch: 100 ----
mean loss: 116.27
 ---- batch: 110 ----
mean loss: 121.35
train mean loss: 119.20
epoch train time: 0:00:01.918236
elapsed time: 0:07:05.594738
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 22:26:43.433594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.40
 ---- batch: 020 ----
mean loss: 115.67
 ---- batch: 030 ----
mean loss: 118.32
 ---- batch: 040 ----
mean loss: 123.08
 ---- batch: 050 ----
mean loss: 111.46
 ---- batch: 060 ----
mean loss: 121.82
 ---- batch: 070 ----
mean loss: 120.63
 ---- batch: 080 ----
mean loss: 122.41
 ---- batch: 090 ----
mean loss: 120.82
 ---- batch: 100 ----
mean loss: 110.77
 ---- batch: 110 ----
mean loss: 122.92
train mean loss: 119.12
epoch train time: 0:00:01.893929
elapsed time: 0:07:07.489300
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 22:26:45.328130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.59
 ---- batch: 020 ----
mean loss: 120.91
 ---- batch: 030 ----
mean loss: 121.63
 ---- batch: 040 ----
mean loss: 125.53
 ---- batch: 050 ----
mean loss: 116.46
 ---- batch: 060 ----
mean loss: 119.62
 ---- batch: 070 ----
mean loss: 112.61
 ---- batch: 080 ----
mean loss: 124.25
 ---- batch: 090 ----
mean loss: 120.79
 ---- batch: 100 ----
mean loss: 120.76
 ---- batch: 110 ----
mean loss: 117.19
train mean loss: 119.12
epoch train time: 0:00:01.878290
elapsed time: 0:07:09.368256
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 22:26:47.208116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.07
 ---- batch: 020 ----
mean loss: 116.85
 ---- batch: 030 ----
mean loss: 113.57
 ---- batch: 040 ----
mean loss: 118.48
 ---- batch: 050 ----
mean loss: 120.45
 ---- batch: 060 ----
mean loss: 123.52
 ---- batch: 070 ----
mean loss: 127.11
 ---- batch: 080 ----
mean loss: 118.26
 ---- batch: 090 ----
mean loss: 119.45
 ---- batch: 100 ----
mean loss: 118.11
 ---- batch: 110 ----
mean loss: 122.92
train mean loss: 119.07
epoch train time: 0:00:01.881908
elapsed time: 0:07:11.251776
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 22:26:49.090582
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.67
 ---- batch: 020 ----
mean loss: 110.65
 ---- batch: 030 ----
mean loss: 122.91
 ---- batch: 040 ----
mean loss: 119.33
 ---- batch: 050 ----
mean loss: 118.20
 ---- batch: 060 ----
mean loss: 122.02
 ---- batch: 070 ----
mean loss: 119.08
 ---- batch: 080 ----
mean loss: 114.98
 ---- batch: 090 ----
mean loss: 119.18
 ---- batch: 100 ----
mean loss: 118.29
 ---- batch: 110 ----
mean loss: 129.89
train mean loss: 119.02
epoch train time: 0:00:01.895612
elapsed time: 0:07:13.147952
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 22:26:50.986820
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.22
 ---- batch: 020 ----
mean loss: 118.39
 ---- batch: 030 ----
mean loss: 118.87
 ---- batch: 040 ----
mean loss: 125.05
 ---- batch: 050 ----
mean loss: 119.89
 ---- batch: 060 ----
mean loss: 118.16
 ---- batch: 070 ----
mean loss: 114.90
 ---- batch: 080 ----
mean loss: 114.53
 ---- batch: 090 ----
mean loss: 114.34
 ---- batch: 100 ----
mean loss: 116.31
 ---- batch: 110 ----
mean loss: 119.47
train mean loss: 118.96
epoch train time: 0:00:01.889290
elapsed time: 0:07:15.037871
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 22:26:52.876656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.62
 ---- batch: 020 ----
mean loss: 122.28
 ---- batch: 030 ----
mean loss: 123.73
 ---- batch: 040 ----
mean loss: 118.42
 ---- batch: 050 ----
mean loss: 119.43
 ---- batch: 060 ----
mean loss: 116.21
 ---- batch: 070 ----
mean loss: 113.10
 ---- batch: 080 ----
mean loss: 115.00
 ---- batch: 090 ----
mean loss: 124.09
 ---- batch: 100 ----
mean loss: 113.46
 ---- batch: 110 ----
mean loss: 119.78
train mean loss: 118.82
epoch train time: 0:00:01.897115
elapsed time: 0:07:16.935587
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 22:26:54.774410
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.14
 ---- batch: 020 ----
mean loss: 115.15
 ---- batch: 030 ----
mean loss: 118.97
 ---- batch: 040 ----
mean loss: 119.59
 ---- batch: 050 ----
mean loss: 120.80
 ---- batch: 060 ----
mean loss: 117.97
 ---- batch: 070 ----
mean loss: 122.46
 ---- batch: 080 ----
mean loss: 119.14
 ---- batch: 090 ----
mean loss: 118.88
 ---- batch: 100 ----
mean loss: 121.55
 ---- batch: 110 ----
mean loss: 117.14
train mean loss: 118.88
epoch train time: 0:00:01.880294
elapsed time: 0:07:18.816499
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 22:26:56.655404
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.41
 ---- batch: 020 ----
mean loss: 115.30
 ---- batch: 030 ----
mean loss: 106.48
 ---- batch: 040 ----
mean loss: 125.96
 ---- batch: 050 ----
mean loss: 120.59
 ---- batch: 060 ----
mean loss: 125.71
 ---- batch: 070 ----
mean loss: 116.81
 ---- batch: 080 ----
mean loss: 115.72
 ---- batch: 090 ----
mean loss: 111.89
 ---- batch: 100 ----
mean loss: 120.91
 ---- batch: 110 ----
mean loss: 121.28
train mean loss: 118.71
epoch train time: 0:00:01.895894
elapsed time: 0:07:20.713078
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 22:26:58.552042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.79
 ---- batch: 020 ----
mean loss: 115.39
 ---- batch: 030 ----
mean loss: 117.90
 ---- batch: 040 ----
mean loss: 115.17
 ---- batch: 050 ----
mean loss: 123.44
 ---- batch: 060 ----
mean loss: 113.44
 ---- batch: 070 ----
mean loss: 123.38
 ---- batch: 080 ----
mean loss: 119.56
 ---- batch: 090 ----
mean loss: 119.80
 ---- batch: 100 ----
mean loss: 120.02
 ---- batch: 110 ----
mean loss: 127.12
train mean loss: 118.75
epoch train time: 0:00:01.923945
elapsed time: 0:07:22.637748
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 22:27:00.476652
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.13
 ---- batch: 020 ----
mean loss: 119.46
 ---- batch: 030 ----
mean loss: 120.26
 ---- batch: 040 ----
mean loss: 122.27
 ---- batch: 050 ----
mean loss: 115.66
 ---- batch: 060 ----
mean loss: 115.95
 ---- batch: 070 ----
mean loss: 118.58
 ---- batch: 080 ----
mean loss: 115.42
 ---- batch: 090 ----
mean loss: 111.77
 ---- batch: 100 ----
mean loss: 118.89
 ---- batch: 110 ----
mean loss: 121.23
train mean loss: 118.80
epoch train time: 0:00:01.916968
elapsed time: 0:07:24.555465
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 22:27:02.394298
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.44
 ---- batch: 020 ----
mean loss: 123.39
 ---- batch: 030 ----
mean loss: 120.28
 ---- batch: 040 ----
mean loss: 120.13
 ---- batch: 050 ----
mean loss: 117.02
 ---- batch: 060 ----
mean loss: 113.01
 ---- batch: 070 ----
mean loss: 119.83
 ---- batch: 080 ----
mean loss: 119.38
 ---- batch: 090 ----
mean loss: 115.94
 ---- batch: 100 ----
mean loss: 120.17
 ---- batch: 110 ----
mean loss: 116.32
train mean loss: 118.78
epoch train time: 0:00:01.884859
elapsed time: 0:07:26.440940
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 22:27:04.279748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.16
 ---- batch: 020 ----
mean loss: 116.76
 ---- batch: 030 ----
mean loss: 121.57
 ---- batch: 040 ----
mean loss: 117.81
 ---- batch: 050 ----
mean loss: 113.86
 ---- batch: 060 ----
mean loss: 117.93
 ---- batch: 070 ----
mean loss: 121.76
 ---- batch: 080 ----
mean loss: 121.95
 ---- batch: 090 ----
mean loss: 118.95
 ---- batch: 100 ----
mean loss: 121.65
 ---- batch: 110 ----
mean loss: 115.84
train mean loss: 118.62
epoch train time: 0:00:01.907261
elapsed time: 0:07:28.348775
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 22:27:06.187594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.08
 ---- batch: 020 ----
mean loss: 119.41
 ---- batch: 030 ----
mean loss: 119.92
 ---- batch: 040 ----
mean loss: 118.99
 ---- batch: 050 ----
mean loss: 115.11
 ---- batch: 060 ----
mean loss: 118.05
 ---- batch: 070 ----
mean loss: 117.62
 ---- batch: 080 ----
mean loss: 114.84
 ---- batch: 090 ----
mean loss: 119.01
 ---- batch: 100 ----
mean loss: 121.75
 ---- batch: 110 ----
mean loss: 116.16
train mean loss: 118.60
epoch train time: 0:00:01.908081
elapsed time: 0:07:30.257441
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 22:27:08.096268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.68
 ---- batch: 020 ----
mean loss: 120.46
 ---- batch: 030 ----
mean loss: 111.83
 ---- batch: 040 ----
mean loss: 116.02
 ---- batch: 050 ----
mean loss: 122.81
 ---- batch: 060 ----
mean loss: 109.79
 ---- batch: 070 ----
mean loss: 118.14
 ---- batch: 080 ----
mean loss: 123.76
 ---- batch: 090 ----
mean loss: 119.35
 ---- batch: 100 ----
mean loss: 121.70
 ---- batch: 110 ----
mean loss: 119.67
train mean loss: 118.53
epoch train time: 0:00:01.915387
elapsed time: 0:07:32.173407
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 22:27:10.012207
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.56
 ---- batch: 020 ----
mean loss: 121.63
 ---- batch: 030 ----
mean loss: 124.08
 ---- batch: 040 ----
mean loss: 118.24
 ---- batch: 050 ----
mean loss: 124.95
 ---- batch: 060 ----
mean loss: 112.04
 ---- batch: 070 ----
mean loss: 115.36
 ---- batch: 080 ----
mean loss: 116.67
 ---- batch: 090 ----
mean loss: 120.25
 ---- batch: 100 ----
mean loss: 120.48
 ---- batch: 110 ----
mean loss: 118.25
train mean loss: 118.61
epoch train time: 0:00:01.895115
elapsed time: 0:07:34.069169
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 22:27:11.907980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.26
 ---- batch: 020 ----
mean loss: 111.79
 ---- batch: 030 ----
mean loss: 123.51
 ---- batch: 040 ----
mean loss: 117.59
 ---- batch: 050 ----
mean loss: 115.69
 ---- batch: 060 ----
mean loss: 116.45
 ---- batch: 070 ----
mean loss: 123.86
 ---- batch: 080 ----
mean loss: 127.65
 ---- batch: 090 ----
mean loss: 113.05
 ---- batch: 100 ----
mean loss: 116.34
 ---- batch: 110 ----
mean loss: 123.60
train mean loss: 118.49
epoch train time: 0:00:01.908488
elapsed time: 0:07:35.978249
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 22:27:13.817082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.01
 ---- batch: 020 ----
mean loss: 117.85
 ---- batch: 030 ----
mean loss: 123.68
 ---- batch: 040 ----
mean loss: 116.60
 ---- batch: 050 ----
mean loss: 120.71
 ---- batch: 060 ----
mean loss: 118.66
 ---- batch: 070 ----
mean loss: 117.65
 ---- batch: 080 ----
mean loss: 113.89
 ---- batch: 090 ----
mean loss: 114.44
 ---- batch: 100 ----
mean loss: 117.51
 ---- batch: 110 ----
mean loss: 115.98
train mean loss: 118.48
epoch train time: 0:00:01.938266
elapsed time: 0:07:37.917167
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 22:27:15.756163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.13
 ---- batch: 020 ----
mean loss: 127.03
 ---- batch: 030 ----
mean loss: 114.02
 ---- batch: 040 ----
mean loss: 115.58
 ---- batch: 050 ----
mean loss: 123.48
 ---- batch: 060 ----
mean loss: 117.33
 ---- batch: 070 ----
mean loss: 118.82
 ---- batch: 080 ----
mean loss: 119.84
 ---- batch: 090 ----
mean loss: 122.05
 ---- batch: 100 ----
mean loss: 117.91
 ---- batch: 110 ----
mean loss: 113.33
train mean loss: 118.49
epoch train time: 0:00:01.913963
elapsed time: 0:07:39.831920
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 22:27:17.670765
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.92
 ---- batch: 020 ----
mean loss: 119.67
 ---- batch: 030 ----
mean loss: 114.55
 ---- batch: 040 ----
mean loss: 118.12
 ---- batch: 050 ----
mean loss: 121.44
 ---- batch: 060 ----
mean loss: 119.97
 ---- batch: 070 ----
mean loss: 117.28
 ---- batch: 080 ----
mean loss: 125.99
 ---- batch: 090 ----
mean loss: 117.51
 ---- batch: 100 ----
mean loss: 112.84
 ---- batch: 110 ----
mean loss: 120.71
train mean loss: 118.46
epoch train time: 0:00:01.886293
elapsed time: 0:07:41.718902
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 22:27:19.557764
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.82
 ---- batch: 020 ----
mean loss: 114.94
 ---- batch: 030 ----
mean loss: 119.78
 ---- batch: 040 ----
mean loss: 122.58
 ---- batch: 050 ----
mean loss: 112.89
 ---- batch: 060 ----
mean loss: 123.26
 ---- batch: 070 ----
mean loss: 114.74
 ---- batch: 080 ----
mean loss: 113.91
 ---- batch: 090 ----
mean loss: 120.19
 ---- batch: 100 ----
mean loss: 123.36
 ---- batch: 110 ----
mean loss: 118.80
train mean loss: 118.48
epoch train time: 0:00:01.914610
elapsed time: 0:07:43.634140
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 22:27:21.472929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.31
 ---- batch: 020 ----
mean loss: 126.47
 ---- batch: 030 ----
mean loss: 119.93
 ---- batch: 040 ----
mean loss: 116.48
 ---- batch: 050 ----
mean loss: 116.96
 ---- batch: 060 ----
mean loss: 111.06
 ---- batch: 070 ----
mean loss: 127.92
 ---- batch: 080 ----
mean loss: 114.15
 ---- batch: 090 ----
mean loss: 120.05
 ---- batch: 100 ----
mean loss: 117.66
 ---- batch: 110 ----
mean loss: 118.31
train mean loss: 118.27
epoch train time: 0:00:01.891663
elapsed time: 0:07:45.526356
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 22:27:23.365155
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.67
 ---- batch: 020 ----
mean loss: 117.37
 ---- batch: 030 ----
mean loss: 115.15
 ---- batch: 040 ----
mean loss: 115.70
 ---- batch: 050 ----
mean loss: 114.55
 ---- batch: 060 ----
mean loss: 122.62
 ---- batch: 070 ----
mean loss: 127.93
 ---- batch: 080 ----
mean loss: 124.81
 ---- batch: 090 ----
mean loss: 113.76
 ---- batch: 100 ----
mean loss: 118.22
 ---- batch: 110 ----
mean loss: 120.31
train mean loss: 118.25
epoch train time: 0:00:01.919810
elapsed time: 0:07:47.446719
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 22:27:25.285535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.35
 ---- batch: 020 ----
mean loss: 115.10
 ---- batch: 030 ----
mean loss: 120.72
 ---- batch: 040 ----
mean loss: 120.07
 ---- batch: 050 ----
mean loss: 117.30
 ---- batch: 060 ----
mean loss: 117.56
 ---- batch: 070 ----
mean loss: 122.63
 ---- batch: 080 ----
mean loss: 119.28
 ---- batch: 090 ----
mean loss: 116.37
 ---- batch: 100 ----
mean loss: 113.21
 ---- batch: 110 ----
mean loss: 119.43
train mean loss: 118.23
epoch train time: 0:00:01.898352
elapsed time: 0:07:49.345639
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 22:27:27.184430
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.07
 ---- batch: 020 ----
mean loss: 116.45
 ---- batch: 030 ----
mean loss: 124.15
 ---- batch: 040 ----
mean loss: 125.06
 ---- batch: 050 ----
mean loss: 118.66
 ---- batch: 060 ----
mean loss: 125.52
 ---- batch: 070 ----
mean loss: 112.90
 ---- batch: 080 ----
mean loss: 107.60
 ---- batch: 090 ----
mean loss: 120.11
 ---- batch: 100 ----
mean loss: 119.41
 ---- batch: 110 ----
mean loss: 120.38
train mean loss: 118.17
epoch train time: 0:00:01.917905
elapsed time: 0:07:51.264105
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 22:27:29.102919
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.81
 ---- batch: 020 ----
mean loss: 114.55
 ---- batch: 030 ----
mean loss: 115.17
 ---- batch: 040 ----
mean loss: 128.17
 ---- batch: 050 ----
mean loss: 115.67
 ---- batch: 060 ----
mean loss: 121.37
 ---- batch: 070 ----
mean loss: 115.40
 ---- batch: 080 ----
mean loss: 121.15
 ---- batch: 090 ----
mean loss: 113.67
 ---- batch: 100 ----
mean loss: 111.91
 ---- batch: 110 ----
mean loss: 121.28
train mean loss: 118.21
epoch train time: 0:00:01.883315
elapsed time: 0:07:53.148004
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 22:27:30.986837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.34
 ---- batch: 020 ----
mean loss: 111.58
 ---- batch: 030 ----
mean loss: 116.53
 ---- batch: 040 ----
mean loss: 121.57
 ---- batch: 050 ----
mean loss: 118.16
 ---- batch: 060 ----
mean loss: 109.54
 ---- batch: 070 ----
mean loss: 119.67
 ---- batch: 080 ----
mean loss: 124.85
 ---- batch: 090 ----
mean loss: 116.37
 ---- batch: 100 ----
mean loss: 128.29
 ---- batch: 110 ----
mean loss: 122.12
train mean loss: 118.16
epoch train time: 0:00:01.915204
elapsed time: 0:07:55.063858
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 22:27:32.902645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.03
 ---- batch: 020 ----
mean loss: 116.61
 ---- batch: 030 ----
mean loss: 117.54
 ---- batch: 040 ----
mean loss: 121.46
 ---- batch: 050 ----
mean loss: 110.15
 ---- batch: 060 ----
mean loss: 124.20
 ---- batch: 070 ----
mean loss: 122.27
 ---- batch: 080 ----
mean loss: 120.89
 ---- batch: 090 ----
mean loss: 122.64
 ---- batch: 100 ----
mean loss: 110.73
 ---- batch: 110 ----
mean loss: 113.64
train mean loss: 118.06
epoch train time: 0:00:01.899673
elapsed time: 0:07:56.964137
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 22:27:34.802983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.49
 ---- batch: 020 ----
mean loss: 111.98
 ---- batch: 030 ----
mean loss: 117.65
 ---- batch: 040 ----
mean loss: 120.20
 ---- batch: 050 ----
mean loss: 111.02
 ---- batch: 060 ----
mean loss: 121.56
 ---- batch: 070 ----
mean loss: 120.96
 ---- batch: 080 ----
mean loss: 123.01
 ---- batch: 090 ----
mean loss: 117.33
 ---- batch: 100 ----
mean loss: 121.49
 ---- batch: 110 ----
mean loss: 116.74
train mean loss: 118.07
epoch train time: 0:00:01.895249
elapsed time: 0:07:58.860333
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 22:27:36.698830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.24
 ---- batch: 020 ----
mean loss: 118.57
 ---- batch: 030 ----
mean loss: 111.80
 ---- batch: 040 ----
mean loss: 118.34
 ---- batch: 050 ----
mean loss: 113.76
 ---- batch: 060 ----
mean loss: 122.44
 ---- batch: 070 ----
mean loss: 114.34
 ---- batch: 080 ----
mean loss: 119.48
 ---- batch: 090 ----
mean loss: 122.27
 ---- batch: 100 ----
mean loss: 121.24
 ---- batch: 110 ----
mean loss: 117.78
train mean loss: 117.97
epoch train time: 0:00:01.882454
elapsed time: 0:08:00.743059
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 22:27:38.581961
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.79
 ---- batch: 020 ----
mean loss: 112.51
 ---- batch: 030 ----
mean loss: 112.05
 ---- batch: 040 ----
mean loss: 125.66
 ---- batch: 050 ----
mean loss: 114.25
 ---- batch: 060 ----
mean loss: 124.18
 ---- batch: 070 ----
mean loss: 120.09
 ---- batch: 080 ----
mean loss: 115.15
 ---- batch: 090 ----
mean loss: 121.18
 ---- batch: 100 ----
mean loss: 118.05
 ---- batch: 110 ----
mean loss: 114.69
train mean loss: 118.04
epoch train time: 0:00:01.895515
elapsed time: 0:08:02.639329
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 22:27:40.478135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.57
 ---- batch: 020 ----
mean loss: 119.72
 ---- batch: 030 ----
mean loss: 119.92
 ---- batch: 040 ----
mean loss: 123.40
 ---- batch: 050 ----
mean loss: 117.11
 ---- batch: 060 ----
mean loss: 126.38
 ---- batch: 070 ----
mean loss: 116.30
 ---- batch: 080 ----
mean loss: 121.34
 ---- batch: 090 ----
mean loss: 112.28
 ---- batch: 100 ----
mean loss: 114.24
 ---- batch: 110 ----
mean loss: 113.65
train mean loss: 117.95
epoch train time: 0:00:01.916928
elapsed time: 0:08:04.556831
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 22:27:42.395605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.05
 ---- batch: 020 ----
mean loss: 125.56
 ---- batch: 030 ----
mean loss: 114.46
 ---- batch: 040 ----
mean loss: 115.20
 ---- batch: 050 ----
mean loss: 116.82
 ---- batch: 060 ----
mean loss: 118.62
 ---- batch: 070 ----
mean loss: 112.26
 ---- batch: 080 ----
mean loss: 120.82
 ---- batch: 090 ----
mean loss: 118.87
 ---- batch: 100 ----
mean loss: 118.20
 ---- batch: 110 ----
mean loss: 114.51
train mean loss: 117.85
epoch train time: 0:00:01.901400
elapsed time: 0:08:06.458764
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 22:27:44.297566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.49
 ---- batch: 020 ----
mean loss: 120.92
 ---- batch: 030 ----
mean loss: 113.01
 ---- batch: 040 ----
mean loss: 122.29
 ---- batch: 050 ----
mean loss: 120.45
 ---- batch: 060 ----
mean loss: 119.15
 ---- batch: 070 ----
mean loss: 125.47
 ---- batch: 080 ----
mean loss: 111.93
 ---- batch: 090 ----
mean loss: 112.72
 ---- batch: 100 ----
mean loss: 120.05
 ---- batch: 110 ----
mean loss: 114.79
train mean loss: 117.88
epoch train time: 0:00:01.900859
elapsed time: 0:08:08.360179
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 22:27:46.198968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.61
 ---- batch: 020 ----
mean loss: 120.52
 ---- batch: 030 ----
mean loss: 123.93
 ---- batch: 040 ----
mean loss: 117.75
 ---- batch: 050 ----
mean loss: 113.02
 ---- batch: 060 ----
mean loss: 125.41
 ---- batch: 070 ----
mean loss: 119.60
 ---- batch: 080 ----
mean loss: 123.24
 ---- batch: 090 ----
mean loss: 114.52
 ---- batch: 100 ----
mean loss: 116.35
 ---- batch: 110 ----
mean loss: 114.31
train mean loss: 117.91
epoch train time: 0:00:01.913419
elapsed time: 0:08:10.274137
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 22:27:48.112906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.95
 ---- batch: 020 ----
mean loss: 116.54
 ---- batch: 030 ----
mean loss: 119.27
 ---- batch: 040 ----
mean loss: 113.93
 ---- batch: 050 ----
mean loss: 112.31
 ---- batch: 060 ----
mean loss: 117.46
 ---- batch: 070 ----
mean loss: 119.08
 ---- batch: 080 ----
mean loss: 116.56
 ---- batch: 090 ----
mean loss: 131.73
 ---- batch: 100 ----
mean loss: 112.07
 ---- batch: 110 ----
mean loss: 122.14
train mean loss: 117.83
epoch train time: 0:00:01.882744
elapsed time: 0:08:12.157432
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 22:27:49.996250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.62
 ---- batch: 020 ----
mean loss: 114.39
 ---- batch: 030 ----
mean loss: 114.66
 ---- batch: 040 ----
mean loss: 115.47
 ---- batch: 050 ----
mean loss: 127.46
 ---- batch: 060 ----
mean loss: 125.15
 ---- batch: 070 ----
mean loss: 119.11
 ---- batch: 080 ----
mean loss: 116.96
 ---- batch: 090 ----
mean loss: 118.61
 ---- batch: 100 ----
mean loss: 119.08
 ---- batch: 110 ----
mean loss: 111.39
train mean loss: 117.64
epoch train time: 0:00:01.910884
elapsed time: 0:08:14.068910
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 22:27:51.907749
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.52
 ---- batch: 020 ----
mean loss: 123.95
 ---- batch: 030 ----
mean loss: 123.16
 ---- batch: 040 ----
mean loss: 112.46
 ---- batch: 050 ----
mean loss: 113.47
 ---- batch: 060 ----
mean loss: 121.08
 ---- batch: 070 ----
mean loss: 117.62
 ---- batch: 080 ----
mean loss: 107.12
 ---- batch: 090 ----
mean loss: 112.73
 ---- batch: 100 ----
mean loss: 121.74
 ---- batch: 110 ----
mean loss: 122.38
train mean loss: 117.76
epoch train time: 0:00:01.889222
elapsed time: 0:08:15.958763
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 22:27:53.797636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.46
 ---- batch: 020 ----
mean loss: 121.82
 ---- batch: 030 ----
mean loss: 111.23
 ---- batch: 040 ----
mean loss: 125.31
 ---- batch: 050 ----
mean loss: 122.77
 ---- batch: 060 ----
mean loss: 113.09
 ---- batch: 070 ----
mean loss: 122.75
 ---- batch: 080 ----
mean loss: 110.04
 ---- batch: 090 ----
mean loss: 112.64
 ---- batch: 100 ----
mean loss: 123.87
 ---- batch: 110 ----
mean loss: 117.33
train mean loss: 117.75
epoch train time: 0:00:01.891008
elapsed time: 0:08:17.850412
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 22:27:55.689287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.93
 ---- batch: 020 ----
mean loss: 112.55
 ---- batch: 030 ----
mean loss: 113.61
 ---- batch: 040 ----
mean loss: 115.96
 ---- batch: 050 ----
mean loss: 119.14
 ---- batch: 060 ----
mean loss: 121.71
 ---- batch: 070 ----
mean loss: 119.96
 ---- batch: 080 ----
mean loss: 123.79
 ---- batch: 090 ----
mean loss: 116.27
 ---- batch: 100 ----
mean loss: 123.79
 ---- batch: 110 ----
mean loss: 118.44
train mean loss: 117.69
epoch train time: 0:00:01.931578
elapsed time: 0:08:19.782631
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 22:27:57.621462
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.45
 ---- batch: 020 ----
mean loss: 115.94
 ---- batch: 030 ----
mean loss: 119.02
 ---- batch: 040 ----
mean loss: 118.37
 ---- batch: 050 ----
mean loss: 112.15
 ---- batch: 060 ----
mean loss: 121.91
 ---- batch: 070 ----
mean loss: 118.45
 ---- batch: 080 ----
mean loss: 113.06
 ---- batch: 090 ----
mean loss: 115.35
 ---- batch: 100 ----
mean loss: 120.41
 ---- batch: 110 ----
mean loss: 127.29
train mean loss: 117.68
epoch train time: 0:00:01.889396
elapsed time: 0:08:21.672660
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 22:27:59.511473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.66
 ---- batch: 020 ----
mean loss: 118.22
 ---- batch: 030 ----
mean loss: 117.53
 ---- batch: 040 ----
mean loss: 116.62
 ---- batch: 050 ----
mean loss: 116.35
 ---- batch: 060 ----
mean loss: 118.02
 ---- batch: 070 ----
mean loss: 120.01
 ---- batch: 080 ----
mean loss: 115.33
 ---- batch: 090 ----
mean loss: 115.92
 ---- batch: 100 ----
mean loss: 121.26
 ---- batch: 110 ----
mean loss: 110.90
train mean loss: 117.56
epoch train time: 0:00:01.932202
elapsed time: 0:08:23.605486
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 22:28:01.444287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.06
 ---- batch: 020 ----
mean loss: 118.51
 ---- batch: 030 ----
mean loss: 117.11
 ---- batch: 040 ----
mean loss: 116.60
 ---- batch: 050 ----
mean loss: 115.57
 ---- batch: 060 ----
mean loss: 121.06
 ---- batch: 070 ----
mean loss: 117.42
 ---- batch: 080 ----
mean loss: 115.70
 ---- batch: 090 ----
mean loss: 120.13
 ---- batch: 100 ----
mean loss: 119.13
 ---- batch: 110 ----
mean loss: 125.45
train mean loss: 117.44
epoch train time: 0:00:01.931745
elapsed time: 0:08:25.537807
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 22:28:03.376625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.48
 ---- batch: 020 ----
mean loss: 117.74
 ---- batch: 030 ----
mean loss: 115.26
 ---- batch: 040 ----
mean loss: 119.01
 ---- batch: 050 ----
mean loss: 116.24
 ---- batch: 060 ----
mean loss: 117.85
 ---- batch: 070 ----
mean loss: 114.79
 ---- batch: 080 ----
mean loss: 114.44
 ---- batch: 090 ----
mean loss: 119.90
 ---- batch: 100 ----
mean loss: 122.90
 ---- batch: 110 ----
mean loss: 118.36
train mean loss: 117.52
epoch train time: 0:00:01.903861
elapsed time: 0:08:27.442245
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 22:28:05.281051
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.20
 ---- batch: 020 ----
mean loss: 116.71
 ---- batch: 030 ----
mean loss: 115.57
 ---- batch: 040 ----
mean loss: 118.23
 ---- batch: 050 ----
mean loss: 114.46
 ---- batch: 060 ----
mean loss: 123.11
 ---- batch: 070 ----
mean loss: 114.58
 ---- batch: 080 ----
mean loss: 120.69
 ---- batch: 090 ----
mean loss: 118.11
 ---- batch: 100 ----
mean loss: 121.61
 ---- batch: 110 ----
mean loss: 114.81
train mean loss: 117.54
epoch train time: 0:00:01.887786
elapsed time: 0:08:29.338563
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_0/checkpoint.pth.tar
**** end time: 2019-09-26 22:28:07.177006 ****
