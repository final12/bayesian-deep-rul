Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 16047
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 14:55:07.718256 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 14:55:07.736661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2768.78
 ---- batch: 020 ----
mean loss: 1453.92
train mean loss: 1970.48
epoch train time: 0:00:11.779125
elapsed time: 0:00:11.805800
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 14:55:19.524097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1218.21
 ---- batch: 020 ----
mean loss: 1133.51
train mean loss: 1148.77
epoch train time: 0:00:04.225804
elapsed time: 0:00:16.032062
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 14:55:23.750472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1069.72
 ---- batch: 020 ----
mean loss: 1045.96
train mean loss: 1055.03
epoch train time: 0:00:04.224047
elapsed time: 0:00:20.256711
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 14:55:27.975083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1027.04
 ---- batch: 020 ----
mean loss: 1021.28
train mean loss: 1021.96
epoch train time: 0:00:04.228673
elapsed time: 0:00:24.485894
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 14:55:32.204283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.84
 ---- batch: 020 ----
mean loss: 996.30
train mean loss: 996.18
epoch train time: 0:00:04.217492
elapsed time: 0:00:28.703909
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 14:55:36.422291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1000.03
 ---- batch: 020 ----
mean loss: 972.09
train mean loss: 979.35
epoch train time: 0:00:04.225971
elapsed time: 0:00:32.930398
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 14:55:40.648768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 973.84
 ---- batch: 020 ----
mean loss: 959.45
train mean loss: 964.31
epoch train time: 0:00:04.228797
elapsed time: 0:00:37.159720
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 14:55:44.878101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 975.46
 ---- batch: 020 ----
mean loss: 959.37
train mean loss: 968.75
epoch train time: 0:00:04.223082
elapsed time: 0:00:41.383368
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 14:55:49.101777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.00
 ---- batch: 020 ----
mean loss: 947.32
train mean loss: 944.64
epoch train time: 0:00:04.241733
elapsed time: 0:00:45.625699
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 14:55:53.344108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.88
 ---- batch: 020 ----
mean loss: 941.60
train mean loss: 945.31
epoch train time: 0:00:04.228881
elapsed time: 0:00:49.855129
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 14:55:57.573783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.34
 ---- batch: 020 ----
mean loss: 929.28
train mean loss: 944.68
epoch train time: 0:00:04.243927
elapsed time: 0:00:54.099854
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 14:56:01.818233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 963.68
 ---- batch: 020 ----
mean loss: 934.14
train mean loss: 945.44
epoch train time: 0:00:04.238704
elapsed time: 0:00:58.339063
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 14:56:06.057451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.48
 ---- batch: 020 ----
mean loss: 930.58
train mean loss: 936.49
epoch train time: 0:00:04.217941
elapsed time: 0:01:02.557516
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 14:56:10.275882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.16
 ---- batch: 020 ----
mean loss: 928.24
train mean loss: 933.79
epoch train time: 0:00:04.231431
elapsed time: 0:01:06.789435
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 14:56:14.507806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.79
 ---- batch: 020 ----
mean loss: 931.18
train mean loss: 927.53
epoch train time: 0:00:04.228026
elapsed time: 0:01:11.017995
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 14:56:18.736366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.39
 ---- batch: 020 ----
mean loss: 926.88
train mean loss: 927.84
epoch train time: 0:00:04.234929
elapsed time: 0:01:15.253447
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 14:56:22.971874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.84
 ---- batch: 020 ----
mean loss: 929.44
train mean loss: 927.16
epoch train time: 0:00:04.221336
elapsed time: 0:01:19.475520
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 14:56:27.193915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.96
 ---- batch: 020 ----
mean loss: 921.16
train mean loss: 923.70
epoch train time: 0:00:04.220886
elapsed time: 0:01:23.696938
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 14:56:31.415316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.41
 ---- batch: 020 ----
mean loss: 931.62
train mean loss: 919.12
epoch train time: 0:00:04.232117
elapsed time: 0:01:27.929607
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 14:56:35.647996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.37
 ---- batch: 020 ----
mean loss: 908.67
train mean loss: 918.18
epoch train time: 0:00:04.243881
elapsed time: 0:01:32.174190
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 14:56:39.892572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 918.99
 ---- batch: 020 ----
mean loss: 924.27
train mean loss: 916.42
epoch train time: 0:00:04.230854
elapsed time: 0:01:36.405590
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 14:56:44.123976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 918.59
 ---- batch: 020 ----
mean loss: 909.76
train mean loss: 919.13
epoch train time: 0:00:04.210113
elapsed time: 0:01:40.616216
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 14:56:48.334593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.47
 ---- batch: 020 ----
mean loss: 909.88
train mean loss: 910.62
epoch train time: 0:00:04.224785
elapsed time: 0:01:44.841494
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 14:56:52.559866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.73
 ---- batch: 020 ----
mean loss: 910.25
train mean loss: 915.60
epoch train time: 0:00:04.229390
elapsed time: 0:01:49.071472
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 14:56:56.789916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.10
 ---- batch: 020 ----
mean loss: 921.11
train mean loss: 916.36
epoch train time: 0:00:04.227505
elapsed time: 0:01:53.299605
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 14:57:01.018033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 920.30
 ---- batch: 020 ----
mean loss: 920.77
train mean loss: 916.19
epoch train time: 0:00:04.237935
elapsed time: 0:01:57.538091
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 14:57:05.256467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.83
 ---- batch: 020 ----
mean loss: 922.69
train mean loss: 917.63
epoch train time: 0:00:04.236175
elapsed time: 0:02:01.774780
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 14:57:09.493169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.35
 ---- batch: 020 ----
mean loss: 913.82
train mean loss: 907.79
epoch train time: 0:00:04.234079
elapsed time: 0:02:06.009504
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 14:57:13.727994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.87
 ---- batch: 020 ----
mean loss: 910.03
train mean loss: 911.70
epoch train time: 0:00:04.247428
elapsed time: 0:02:10.257638
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 14:57:17.976043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.31
 ---- batch: 020 ----
mean loss: 906.85
train mean loss: 907.83
epoch train time: 0:00:04.216433
elapsed time: 0:02:14.474683
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 14:57:22.193049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.72
 ---- batch: 020 ----
mean loss: 896.52
train mean loss: 914.22
epoch train time: 0:00:04.230640
elapsed time: 0:02:18.705844
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 14:57:26.424233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.18
 ---- batch: 020 ----
mean loss: 909.45
train mean loss: 907.02
epoch train time: 0:00:04.238483
elapsed time: 0:02:22.944908
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 14:57:30.663284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.30
 ---- batch: 020 ----
mean loss: 906.53
train mean loss: 907.75
epoch train time: 0:00:04.224143
elapsed time: 0:02:27.169606
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 14:57:34.888009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 897.31
 ---- batch: 020 ----
mean loss: 921.48
train mean loss: 911.02
epoch train time: 0:00:04.225390
elapsed time: 0:02:31.395654
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 14:57:39.114031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.02
 ---- batch: 020 ----
mean loss: 896.37
train mean loss: 904.53
epoch train time: 0:00:04.229968
elapsed time: 0:02:35.626144
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 14:57:43.344549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.91
 ---- batch: 020 ----
mean loss: 910.06
train mean loss: 906.05
epoch train time: 0:00:04.231879
elapsed time: 0:02:39.858591
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 14:57:47.576985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.07
 ---- batch: 020 ----
mean loss: 903.97
train mean loss: 907.09
epoch train time: 0:00:04.242547
elapsed time: 0:02:44.101691
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 14:57:51.820090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.45
 ---- batch: 020 ----
mean loss: 901.41
train mean loss: 902.86
epoch train time: 0:00:04.236630
elapsed time: 0:02:48.338874
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 14:57:56.057249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.33
 ---- batch: 020 ----
mean loss: 889.32
train mean loss: 903.16
epoch train time: 0:00:04.239141
elapsed time: 0:02:52.578517
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 14:58:00.296913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.78
 ---- batch: 020 ----
mean loss: 921.28
train mean loss: 900.89
epoch train time: 0:00:04.230277
elapsed time: 0:02:56.809355
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 14:58:04.527751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.19
 ---- batch: 020 ----
mean loss: 880.83
train mean loss: 897.16
epoch train time: 0:00:04.219716
elapsed time: 0:03:01.029623
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 14:58:08.748027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.28
 ---- batch: 020 ----
mean loss: 902.58
train mean loss: 900.25
epoch train time: 0:00:04.221684
elapsed time: 0:03:05.251923
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 14:58:12.970299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.34
 ---- batch: 020 ----
mean loss: 910.56
train mean loss: 899.54
epoch train time: 0:00:04.216145
elapsed time: 0:03:09.468574
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 14:58:17.186957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.97
 ---- batch: 020 ----
mean loss: 897.57
train mean loss: 899.06
epoch train time: 0:00:04.224049
elapsed time: 0:03:13.693182
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 14:58:21.411567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.54
 ---- batch: 020 ----
mean loss: 889.02
train mean loss: 898.19
epoch train time: 0:00:04.251584
elapsed time: 0:03:17.945329
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 14:58:25.663707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.74
 ---- batch: 020 ----
mean loss: 896.67
train mean loss: 893.79
epoch train time: 0:00:04.249020
elapsed time: 0:03:22.194868
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 14:58:29.913249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.69
 ---- batch: 020 ----
mean loss: 888.25
train mean loss: 888.15
epoch train time: 0:00:04.234010
elapsed time: 0:03:26.429429
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 14:58:34.147820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.74
 ---- batch: 020 ----
mean loss: 886.10
train mean loss: 893.91
epoch train time: 0:00:04.218587
elapsed time: 0:03:30.648567
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 14:58:38.366988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.26
 ---- batch: 020 ----
mean loss: 888.39
train mean loss: 887.99
epoch train time: 0:00:04.223473
elapsed time: 0:03:34.872569
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 14:58:42.590947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.35
 ---- batch: 020 ----
mean loss: 881.85
train mean loss: 887.35
epoch train time: 0:00:04.241026
elapsed time: 0:03:39.114106
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 14:58:46.832504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.95
 ---- batch: 020 ----
mean loss: 885.06
train mean loss: 886.35
epoch train time: 0:00:04.222838
elapsed time: 0:03:43.337489
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 14:58:51.055861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.83
 ---- batch: 020 ----
mean loss: 876.95
train mean loss: 881.04
epoch train time: 0:00:04.231571
elapsed time: 0:03:47.569566
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 14:58:55.287959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.82
 ---- batch: 020 ----
mean loss: 877.09
train mean loss: 877.64
epoch train time: 0:00:04.232116
elapsed time: 0:03:51.802271
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 14:58:59.520647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.19
 ---- batch: 020 ----
mean loss: 869.58
train mean loss: 872.57
epoch train time: 0:00:04.233932
elapsed time: 0:03:56.036717
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 14:59:03.755099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.98
 ---- batch: 020 ----
mean loss: 876.61
train mean loss: 859.97
epoch train time: 0:00:04.237129
elapsed time: 0:04:00.274371
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 14:59:07.992748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.44
 ---- batch: 020 ----
mean loss: 861.49
train mean loss: 855.22
epoch train time: 0:00:04.256448
elapsed time: 0:04:04.531318
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 14:59:12.249702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.67
 ---- batch: 020 ----
mean loss: 836.34
train mean loss: 838.83
epoch train time: 0:00:04.236405
elapsed time: 0:04:08.768242
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 14:59:16.486623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 815.33
 ---- batch: 020 ----
mean loss: 838.96
train mean loss: 823.66
epoch train time: 0:00:04.231632
elapsed time: 0:04:13.000444
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 14:59:20.718823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 800.79
 ---- batch: 020 ----
mean loss: 806.56
train mean loss: 802.60
epoch train time: 0:00:04.244438
elapsed time: 0:04:17.245411
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 14:59:24.963794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.85
 ---- batch: 020 ----
mean loss: 772.70
train mean loss: 778.39
epoch train time: 0:00:04.233751
elapsed time: 0:04:21.479748
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 14:59:29.198124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.46
 ---- batch: 020 ----
mean loss: 742.56
train mean loss: 746.68
epoch train time: 0:00:04.229484
elapsed time: 0:04:25.709749
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 14:59:33.428145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 723.62
 ---- batch: 020 ----
mean loss: 704.92
train mean loss: 714.15
epoch train time: 0:00:04.221430
elapsed time: 0:04:29.931750
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 14:59:37.650124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 697.94
 ---- batch: 020 ----
mean loss: 701.35
train mean loss: 693.31
epoch train time: 0:00:04.245559
elapsed time: 0:04:34.177883
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 14:59:41.896256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.40
 ---- batch: 020 ----
mean loss: 680.83
train mean loss: 680.00
epoch train time: 0:00:04.259104
elapsed time: 0:04:38.437499
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 14:59:46.155894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.97
 ---- batch: 020 ----
mean loss: 667.87
train mean loss: 665.69
epoch train time: 0:00:04.235804
elapsed time: 0:04:42.673846
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 14:59:50.392225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 652.08
 ---- batch: 020 ----
mean loss: 627.72
train mean loss: 638.19
epoch train time: 0:00:04.233490
elapsed time: 0:04:46.907897
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 14:59:54.626314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 617.77
 ---- batch: 020 ----
mean loss: 628.13
train mean loss: 622.34
epoch train time: 0:00:04.226733
elapsed time: 0:04:51.135210
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 14:59:58.853585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 601.19
 ---- batch: 020 ----
mean loss: 595.83
train mean loss: 601.34
epoch train time: 0:00:04.229443
elapsed time: 0:04:55.365141
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 15:00:03.083529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 594.12
 ---- batch: 020 ----
mean loss: 577.00
train mean loss: 583.35
epoch train time: 0:00:04.224027
elapsed time: 0:04:59.589764
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 15:00:07.308158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 557.21
 ---- batch: 020 ----
mean loss: 552.72
train mean loss: 551.01
epoch train time: 0:00:04.224675
elapsed time: 0:05:03.814956
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 15:00:11.533331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.51
 ---- batch: 020 ----
mean loss: 546.48
train mean loss: 543.36
epoch train time: 0:00:04.237143
elapsed time: 0:05:08.052685
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 15:00:15.771062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.33
 ---- batch: 020 ----
mean loss: 523.33
train mean loss: 530.19
epoch train time: 0:00:04.232423
elapsed time: 0:05:12.285628
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 15:00:20.004005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.80
 ---- batch: 020 ----
mean loss: 506.23
train mean loss: 512.47
epoch train time: 0:00:04.223816
elapsed time: 0:05:16.509946
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 15:00:24.228320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.43
 ---- batch: 020 ----
mean loss: 499.95
train mean loss: 501.16
epoch train time: 0:00:04.221042
elapsed time: 0:05:20.731532
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 15:00:28.449900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.95
 ---- batch: 020 ----
mean loss: 491.35
train mean loss: 494.91
epoch train time: 0:00:04.234941
elapsed time: 0:05:24.966994
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 15:00:32.685379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.67
 ---- batch: 020 ----
mean loss: 469.54
train mean loss: 476.83
epoch train time: 0:00:04.234508
elapsed time: 0:05:29.202055
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 15:00:36.920431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.68
 ---- batch: 020 ----
mean loss: 480.27
train mean loss: 478.36
epoch train time: 0:00:04.229131
elapsed time: 0:05:33.431772
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 15:00:41.150147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.65
 ---- batch: 020 ----
mean loss: 471.81
train mean loss: 460.95
epoch train time: 0:00:04.229622
elapsed time: 0:05:37.661970
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 15:00:45.380393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.28
 ---- batch: 020 ----
mean loss: 452.02
train mean loss: 459.64
epoch train time: 0:00:04.215987
elapsed time: 0:05:41.878583
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 15:00:49.596959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.19
 ---- batch: 020 ----
mean loss: 442.11
train mean loss: 455.56
epoch train time: 0:00:04.230317
elapsed time: 0:05:46.109445
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 15:00:53.827849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.74
 ---- batch: 020 ----
mean loss: 448.39
train mean loss: 445.88
epoch train time: 0:00:04.229377
elapsed time: 0:05:50.339401
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 15:00:58.057788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.69
 ---- batch: 020 ----
mean loss: 438.17
train mean loss: 436.38
epoch train time: 0:00:04.248863
elapsed time: 0:05:54.588787
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 15:01:02.307163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 439.34
 ---- batch: 020 ----
mean loss: 430.10
train mean loss: 433.45
epoch train time: 0:00:04.223750
elapsed time: 0:05:58.813120
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 15:01:06.531507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.27
 ---- batch: 020 ----
mean loss: 423.70
train mean loss: 429.08
epoch train time: 0:00:04.235383
elapsed time: 0:06:03.049041
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 15:01:10.767436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.59
 ---- batch: 020 ----
mean loss: 416.94
train mean loss: 419.46
epoch train time: 0:00:04.228199
elapsed time: 0:06:07.277857
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 15:01:14.996282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.81
 ---- batch: 020 ----
mean loss: 426.76
train mean loss: 424.27
epoch train time: 0:00:04.238671
elapsed time: 0:06:11.517088
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 15:01:19.235474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.71
 ---- batch: 020 ----
mean loss: 403.17
train mean loss: 408.89
epoch train time: 0:00:04.225983
elapsed time: 0:06:15.743586
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 15:01:23.461982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.56
 ---- batch: 020 ----
mean loss: 403.21
train mean loss: 412.57
epoch train time: 0:00:04.224874
elapsed time: 0:06:19.969039
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 15:01:27.687430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.46
 ---- batch: 020 ----
mean loss: 409.27
train mean loss: 403.70
epoch train time: 0:00:04.237386
elapsed time: 0:06:24.207000
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 15:01:31.925374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.15
 ---- batch: 020 ----
mean loss: 398.28
train mean loss: 405.68
epoch train time: 0:00:04.224112
elapsed time: 0:06:28.431653
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 15:01:36.150024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.52
 ---- batch: 020 ----
mean loss: 401.83
train mean loss: 397.54
epoch train time: 0:00:04.224551
elapsed time: 0:06:32.656833
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 15:01:40.375230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.26
 ---- batch: 020 ----
mean loss: 390.58
train mean loss: 391.91
epoch train time: 0:00:04.229856
elapsed time: 0:06:36.887218
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 15:01:44.605626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.64
 ---- batch: 020 ----
mean loss: 390.10
train mean loss: 390.11
epoch train time: 0:00:04.236682
elapsed time: 0:06:41.124482
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 15:01:48.842854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.32
 ---- batch: 020 ----
mean loss: 372.13
train mean loss: 378.28
epoch train time: 0:00:04.229476
elapsed time: 0:06:45.354489
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 15:01:53.072864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.25
 ---- batch: 020 ----
mean loss: 382.09
train mean loss: 381.61
epoch train time: 0:00:04.220384
elapsed time: 0:06:49.575416
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 15:01:57.293864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.15
 ---- batch: 020 ----
mean loss: 374.44
train mean loss: 373.87
epoch train time: 0:00:04.238710
elapsed time: 0:06:53.814677
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 15:02:01.533055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.46
 ---- batch: 020 ----
mean loss: 382.88
train mean loss: 374.02
epoch train time: 0:00:04.230803
elapsed time: 0:06:58.046050
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 15:02:05.764445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.13
 ---- batch: 020 ----
mean loss: 369.40
train mean loss: 370.84
epoch train time: 0:00:04.225231
elapsed time: 0:07:02.271862
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 15:02:09.990301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.92
 ---- batch: 020 ----
mean loss: 362.51
train mean loss: 368.81
epoch train time: 0:00:04.228287
elapsed time: 0:07:06.500720
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 15:02:14.219095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.01
 ---- batch: 020 ----
mean loss: 375.56
train mean loss: 369.01
epoch train time: 0:00:04.243591
elapsed time: 0:07:10.744786
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 15:02:18.463157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.06
 ---- batch: 020 ----
mean loss: 361.59
train mean loss: 360.13
epoch train time: 0:00:04.229231
elapsed time: 0:07:14.974749
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 15:02:22.693158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.83
 ---- batch: 020 ----
mean loss: 357.34
train mean loss: 357.18
epoch train time: 0:00:04.218730
elapsed time: 0:07:19.194024
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 15:02:26.912404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.36
 ---- batch: 020 ----
mean loss: 353.89
train mean loss: 355.93
epoch train time: 0:00:04.215632
elapsed time: 0:07:23.410195
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 15:02:31.128570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.01
 ---- batch: 020 ----
mean loss: 363.68
train mean loss: 354.87
epoch train time: 0:00:04.224866
elapsed time: 0:07:27.635606
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 15:02:35.353991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.24
 ---- batch: 020 ----
mean loss: 358.95
train mean loss: 356.22
epoch train time: 0:00:04.224320
elapsed time: 0:07:31.860462
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 15:02:39.578834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.06
 ---- batch: 020 ----
mean loss: 361.24
train mean loss: 356.68
epoch train time: 0:00:04.230026
elapsed time: 0:07:36.091042
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 15:02:43.809418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.82
 ---- batch: 020 ----
mean loss: 347.84
train mean loss: 350.37
epoch train time: 0:00:04.227005
elapsed time: 0:07:40.318638
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 15:02:48.036946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.08
 ---- batch: 020 ----
mean loss: 351.36
train mean loss: 350.79
epoch train time: 0:00:04.227234
elapsed time: 0:07:44.546339
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 15:02:52.264709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.24
 ---- batch: 020 ----
mean loss: 350.54
train mean loss: 354.89
epoch train time: 0:00:04.235741
elapsed time: 0:07:48.782576
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 15:02:56.500959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.96
 ---- batch: 020 ----
mean loss: 348.36
train mean loss: 344.54
epoch train time: 0:00:04.224738
elapsed time: 0:07:53.007844
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 15:03:00.726218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.96
 ---- batch: 020 ----
mean loss: 341.71
train mean loss: 342.66
epoch train time: 0:00:04.220742
elapsed time: 0:07:57.229106
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 15:03:04.947513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.92
 ---- batch: 020 ----
mean loss: 338.47
train mean loss: 341.10
epoch train time: 0:00:04.234786
elapsed time: 0:08:01.464447
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 15:03:09.182824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.38
 ---- batch: 020 ----
mean loss: 335.43
train mean loss: 336.73
epoch train time: 0:00:04.231347
elapsed time: 0:08:05.696269
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 15:03:13.414648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.53
 ---- batch: 020 ----
mean loss: 332.84
train mean loss: 335.85
epoch train time: 0:00:04.246108
elapsed time: 0:08:09.942930
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 15:03:17.661301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.61
 ---- batch: 020 ----
mean loss: 334.40
train mean loss: 342.97
epoch train time: 0:00:04.234478
elapsed time: 0:08:14.177948
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 15:03:21.896333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.82
 ---- batch: 020 ----
mean loss: 337.15
train mean loss: 333.41
epoch train time: 0:00:04.223732
elapsed time: 0:08:18.402249
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 15:03:26.120631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.52
 ---- batch: 020 ----
mean loss: 337.49
train mean loss: 333.24
epoch train time: 0:00:04.250891
elapsed time: 0:08:22.653681
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 15:03:30.372076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.44
 ---- batch: 020 ----
mean loss: 326.44
train mean loss: 327.73
epoch train time: 0:00:04.269212
elapsed time: 0:08:26.923703
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 15:03:34.642093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.32
 ---- batch: 020 ----
mean loss: 324.98
train mean loss: 328.55
epoch train time: 0:00:04.227256
elapsed time: 0:08:31.151555
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 15:03:38.869942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.57
 ---- batch: 020 ----
mean loss: 323.78
train mean loss: 326.76
epoch train time: 0:00:04.225874
elapsed time: 0:08:35.377947
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 15:03:43.096324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.40
 ---- batch: 020 ----
mean loss: 324.38
train mean loss: 324.53
epoch train time: 0:00:04.238271
elapsed time: 0:08:39.616761
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 15:03:47.335140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.72
 ---- batch: 020 ----
mean loss: 317.08
train mean loss: 320.30
epoch train time: 0:00:04.236687
elapsed time: 0:08:43.853958
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 15:03:51.572334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.47
 ---- batch: 020 ----
mean loss: 315.96
train mean loss: 317.67
epoch train time: 0:00:04.243395
elapsed time: 0:08:48.097862
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 15:03:55.816233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.74
 ---- batch: 020 ----
mean loss: 318.91
train mean loss: 320.55
epoch train time: 0:00:04.244798
elapsed time: 0:08:52.343205
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 15:04:00.061593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.37
 ---- batch: 020 ----
mean loss: 321.41
train mean loss: 317.91
epoch train time: 0:00:04.242043
elapsed time: 0:08:56.585757
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 15:04:04.304127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.42
 ---- batch: 020 ----
mean loss: 315.06
train mean loss: 317.44
epoch train time: 0:00:04.232119
elapsed time: 0:09:00.818413
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 15:04:08.536825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.90
 ---- batch: 020 ----
mean loss: 311.18
train mean loss: 314.50
epoch train time: 0:00:04.236541
elapsed time: 0:09:05.055623
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 15:04:12.773929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.33
 ---- batch: 020 ----
mean loss: 314.16
train mean loss: 313.83
epoch train time: 0:00:04.236450
elapsed time: 0:09:09.292582
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 15:04:17.010958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.05
 ---- batch: 020 ----
mean loss: 312.48
train mean loss: 312.69
epoch train time: 0:00:04.232389
elapsed time: 0:09:13.525490
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 15:04:21.243860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.72
 ---- batch: 020 ----
mean loss: 307.82
train mean loss: 312.92
epoch train time: 0:00:04.232449
elapsed time: 0:09:17.758436
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 15:04:25.476812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.50
 ---- batch: 020 ----
mean loss: 309.95
train mean loss: 311.17
epoch train time: 0:00:04.244954
elapsed time: 0:09:22.003946
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 15:04:29.722324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.21
 ---- batch: 020 ----
mean loss: 314.95
train mean loss: 313.29
epoch train time: 0:00:04.244075
elapsed time: 0:09:26.248524
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 15:04:33.966927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.47
 ---- batch: 020 ----
mean loss: 313.07
train mean loss: 310.53
epoch train time: 0:00:04.231269
elapsed time: 0:09:30.480399
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 15:04:38.198779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.44
 ---- batch: 020 ----
mean loss: 307.63
train mean loss: 305.43
epoch train time: 0:00:04.228777
elapsed time: 0:09:34.709670
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 15:04:42.428048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.70
 ---- batch: 020 ----
mean loss: 304.74
train mean loss: 302.28
epoch train time: 0:00:04.235676
elapsed time: 0:09:38.945899
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 15:04:46.664275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.39
 ---- batch: 020 ----
mean loss: 312.62
train mean loss: 306.93
epoch train time: 0:00:04.256194
elapsed time: 0:09:43.202717
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 15:04:50.921119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.26
 ---- batch: 020 ----
mean loss: 293.62
train mean loss: 303.64
epoch train time: 0:00:04.287548
elapsed time: 0:09:47.490837
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 15:04:55.209207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.80
 ---- batch: 020 ----
mean loss: 289.41
train mean loss: 300.97
epoch train time: 0:00:04.263800
elapsed time: 0:09:51.755298
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 15:04:59.473719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.99
 ---- batch: 020 ----
mean loss: 296.35
train mean loss: 301.49
epoch train time: 0:00:04.244097
elapsed time: 0:09:55.999975
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 15:05:03.718348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.63
 ---- batch: 020 ----
mean loss: 311.66
train mean loss: 301.22
epoch train time: 0:00:04.248662
elapsed time: 0:10:00.249173
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 15:05:07.967551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.93
 ---- batch: 020 ----
mean loss: 295.42
train mean loss: 297.29
epoch train time: 0:00:04.235917
elapsed time: 0:10:04.485622
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 15:05:12.204021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.02
 ---- batch: 020 ----
mean loss: 311.25
train mean loss: 301.78
epoch train time: 0:00:04.251943
elapsed time: 0:10:08.738155
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 15:05:16.456545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.21
 ---- batch: 020 ----
mean loss: 297.04
train mean loss: 300.42
epoch train time: 0:00:04.235533
elapsed time: 0:10:12.974265
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 15:05:20.692633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.03
 ---- batch: 020 ----
mean loss: 290.42
train mean loss: 293.08
epoch train time: 0:00:04.242178
elapsed time: 0:10:17.217005
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 15:05:24.935379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.71
 ---- batch: 020 ----
mean loss: 299.60
train mean loss: 295.33
epoch train time: 0:00:04.227404
elapsed time: 0:10:21.444956
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 15:05:29.163331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.19
 ---- batch: 020 ----
mean loss: 298.73
train mean loss: 295.09
epoch train time: 0:00:04.244995
elapsed time: 0:10:25.690491
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 15:05:33.408875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.29
 ---- batch: 020 ----
mean loss: 300.40
train mean loss: 291.98
epoch train time: 0:00:04.240650
elapsed time: 0:10:29.931799
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 15:05:37.650252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.38
 ---- batch: 020 ----
mean loss: 286.39
train mean loss: 288.45
epoch train time: 0:00:04.239187
elapsed time: 0:10:34.171708
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 15:05:41.890173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.00
 ---- batch: 020 ----
mean loss: 295.80
train mean loss: 291.84
epoch train time: 0:00:04.246893
elapsed time: 0:10:38.419330
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 15:05:46.137693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.40
 ---- batch: 020 ----
mean loss: 287.70
train mean loss: 289.45
epoch train time: 0:00:04.238780
elapsed time: 0:10:42.658595
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 15:05:50.377010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.12
 ---- batch: 020 ----
mean loss: 292.97
train mean loss: 285.11
epoch train time: 0:00:04.241503
elapsed time: 0:10:46.900657
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 15:05:54.619024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.78
 ---- batch: 020 ----
mean loss: 284.04
train mean loss: 290.45
epoch train time: 0:00:04.243400
elapsed time: 0:10:51.144614
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 15:05:58.862982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.00
 ---- batch: 020 ----
mean loss: 293.88
train mean loss: 289.87
epoch train time: 0:00:04.250690
elapsed time: 0:10:55.395850
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 15:06:03.114238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.06
 ---- batch: 020 ----
mean loss: 281.75
train mean loss: 285.54
epoch train time: 0:00:04.257408
elapsed time: 0:10:59.653894
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 15:06:07.372316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.51
 ---- batch: 020 ----
mean loss: 292.35
train mean loss: 291.04
epoch train time: 0:00:04.252540
elapsed time: 0:11:03.907061
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 15:06:11.625435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.94
 ---- batch: 020 ----
mean loss: 275.93
train mean loss: 281.11
epoch train time: 0:00:04.260294
elapsed time: 0:11:08.167871
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 15:06:15.886243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.75
 ---- batch: 020 ----
mean loss: 284.30
train mean loss: 284.33
epoch train time: 0:00:04.248054
elapsed time: 0:11:12.416439
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 15:06:20.134827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.86
 ---- batch: 020 ----
mean loss: 282.91
train mean loss: 280.80
epoch train time: 0:00:04.250797
elapsed time: 0:11:16.667754
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 15:06:24.386128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.00
 ---- batch: 020 ----
mean loss: 284.32
train mean loss: 280.20
epoch train time: 0:00:04.231282
elapsed time: 0:11:20.899542
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 15:06:28.617923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.63
 ---- batch: 020 ----
mean loss: 273.95
train mean loss: 278.68
epoch train time: 0:00:04.248198
elapsed time: 0:11:25.148266
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 15:06:32.866639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.22
 ---- batch: 020 ----
mean loss: 275.95
train mean loss: 280.81
epoch train time: 0:00:04.247952
elapsed time: 0:11:29.396769
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 15:06:37.115139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.45
 ---- batch: 020 ----
mean loss: 291.12
train mean loss: 283.81
epoch train time: 0:00:04.241379
elapsed time: 0:11:33.638719
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 15:06:41.357100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.37
 ---- batch: 020 ----
mean loss: 269.91
train mean loss: 276.51
epoch train time: 0:00:04.233742
elapsed time: 0:11:37.873037
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 15:06:45.591413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.10
 ---- batch: 020 ----
mean loss: 266.99
train mean loss: 272.96
epoch train time: 0:00:04.244540
elapsed time: 0:11:42.118095
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 15:06:49.836477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.42
 ---- batch: 020 ----
mean loss: 272.78
train mean loss: 276.67
epoch train time: 0:00:04.240358
elapsed time: 0:11:46.358999
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 15:06:54.077372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.51
 ---- batch: 020 ----
mean loss: 273.92
train mean loss: 272.82
epoch train time: 0:00:04.249400
elapsed time: 0:11:50.608915
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 15:06:58.327287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.77
 ---- batch: 020 ----
mean loss: 285.59
train mean loss: 277.28
epoch train time: 0:00:04.249719
elapsed time: 0:11:54.859138
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 15:07:02.577509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.92
 ---- batch: 020 ----
mean loss: 269.73
train mean loss: 272.48
epoch train time: 0:00:04.250817
elapsed time: 0:11:59.110646
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 15:07:06.829084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.43
 ---- batch: 020 ----
mean loss: 276.23
train mean loss: 273.02
epoch train time: 0:00:04.253690
elapsed time: 0:12:03.365031
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 15:07:11.083418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.43
 ---- batch: 020 ----
mean loss: 272.36
train mean loss: 272.45
epoch train time: 0:00:04.263500
elapsed time: 0:12:07.629153
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 15:07:15.347541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.99
 ---- batch: 020 ----
mean loss: 278.35
train mean loss: 271.40
epoch train time: 0:00:04.248082
elapsed time: 0:12:11.877795
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 15:07:19.596183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.05
 ---- batch: 020 ----
mean loss: 271.90
train mean loss: 270.05
epoch train time: 0:00:04.233890
elapsed time: 0:12:16.112414
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 15:07:23.830800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.65
 ---- batch: 020 ----
mean loss: 269.34
train mean loss: 269.12
epoch train time: 0:00:04.251304
elapsed time: 0:12:20.364290
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 15:07:28.082679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.00
 ---- batch: 020 ----
mean loss: 270.07
train mean loss: 269.54
epoch train time: 0:00:04.244956
elapsed time: 0:12:24.609854
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 15:07:32.328162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.62
 ---- batch: 020 ----
mean loss: 272.58
train mean loss: 269.75
epoch train time: 0:00:04.244006
elapsed time: 0:12:28.854334
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 15:07:36.572708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.24
 ---- batch: 020 ----
mean loss: 263.50
train mean loss: 266.75
epoch train time: 0:00:04.246471
elapsed time: 0:12:33.101393
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 15:07:40.819772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.56
 ---- batch: 020 ----
mean loss: 265.88
train mean loss: 268.00
epoch train time: 0:00:04.242842
elapsed time: 0:12:37.344778
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 15:07:45.063179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.91
 ---- batch: 020 ----
mean loss: 269.19
train mean loss: 269.73
epoch train time: 0:00:04.237508
elapsed time: 0:12:41.582901
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 15:07:49.301289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.49
 ---- batch: 020 ----
mean loss: 256.15
train mean loss: 264.29
epoch train time: 0:00:04.232096
elapsed time: 0:12:45.815516
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 15:07:53.533889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.92
 ---- batch: 020 ----
mean loss: 272.13
train mean loss: 269.30
epoch train time: 0:00:04.238877
elapsed time: 0:12:50.054979
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 15:07:57.773369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.52
 ---- batch: 020 ----
mean loss: 266.89
train mean loss: 270.74
epoch train time: 0:00:04.253936
elapsed time: 0:12:54.309440
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 15:08:02.027816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.28
 ---- batch: 020 ----
mean loss: 260.17
train mean loss: 268.34
epoch train time: 0:00:04.243950
elapsed time: 0:12:58.554018
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 15:08:06.272394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.41
 ---- batch: 020 ----
mean loss: 262.55
train mean loss: 263.04
epoch train time: 0:00:04.232668
elapsed time: 0:13:02.787173
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 15:08:10.505552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.04
 ---- batch: 020 ----
mean loss: 259.75
train mean loss: 262.53
epoch train time: 0:00:04.243564
elapsed time: 0:13:07.031287
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 15:08:14.749689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.65
 ---- batch: 020 ----
mean loss: 261.15
train mean loss: 262.87
epoch train time: 0:00:04.243530
elapsed time: 0:13:11.275381
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 15:08:18.993764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.99
 ---- batch: 020 ----
mean loss: 270.17
train mean loss: 264.23
epoch train time: 0:00:04.240498
elapsed time: 0:13:15.516453
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 15:08:23.234820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.50
 ---- batch: 020 ----
mean loss: 260.66
train mean loss: 263.29
epoch train time: 0:00:04.230464
elapsed time: 0:13:19.747450
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 15:08:27.465828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.60
 ---- batch: 020 ----
mean loss: 267.47
train mean loss: 258.70
epoch train time: 0:00:04.235731
elapsed time: 0:13:23.983741
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 15:08:31.702148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.71
 ---- batch: 020 ----
mean loss: 276.16
train mean loss: 264.09
epoch train time: 0:00:04.244913
elapsed time: 0:13:28.229203
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 15:08:35.947611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.86
 ---- batch: 020 ----
mean loss: 256.17
train mean loss: 259.42
epoch train time: 0:00:04.266440
elapsed time: 0:13:32.496251
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 15:08:40.214630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.22
 ---- batch: 020 ----
mean loss: 263.22
train mean loss: 265.86
epoch train time: 0:00:04.245454
elapsed time: 0:13:36.742179
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 15:08:44.460549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.93
 ---- batch: 020 ----
mean loss: 262.98
train mean loss: 261.01
epoch train time: 0:00:04.236930
elapsed time: 0:13:40.979734
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 15:08:48.698125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.18
 ---- batch: 020 ----
mean loss: 262.40
train mean loss: 259.21
epoch train time: 0:00:04.231885
elapsed time: 0:13:45.212155
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 15:08:52.930532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.40
 ---- batch: 020 ----
mean loss: 258.39
train mean loss: 257.38
epoch train time: 0:00:04.242003
elapsed time: 0:13:49.454665
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 15:08:57.173039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.26
 ---- batch: 020 ----
mean loss: 255.70
train mean loss: 258.21
epoch train time: 0:00:04.252755
elapsed time: 0:13:53.707951
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 15:09:01.426339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.31
 ---- batch: 020 ----
mean loss: 251.99
train mean loss: 255.93
epoch train time: 0:00:04.238738
elapsed time: 0:13:57.947214
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 15:09:05.665623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.24
 ---- batch: 020 ----
mean loss: 255.38
train mean loss: 255.53
epoch train time: 0:00:04.246836
elapsed time: 0:14:02.194776
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 15:09:09.913200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.15
 ---- batch: 020 ----
mean loss: 257.42
train mean loss: 256.33
epoch train time: 0:00:04.245796
elapsed time: 0:14:06.441156
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 15:09:14.159535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.76
 ---- batch: 020 ----
mean loss: 246.60
train mean loss: 252.41
epoch train time: 0:00:04.240732
elapsed time: 0:14:10.682522
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 15:09:18.400908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.45
 ---- batch: 020 ----
mean loss: 255.01
train mean loss: 253.19
epoch train time: 0:00:04.243452
elapsed time: 0:14:14.926505
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 15:09:22.644896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.27
 ---- batch: 020 ----
mean loss: 251.60
train mean loss: 253.77
epoch train time: 0:00:04.246968
elapsed time: 0:14:19.174023
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 15:09:26.892423
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.80
 ---- batch: 020 ----
mean loss: 254.35
train mean loss: 250.45
epoch train time: 0:00:04.248876
elapsed time: 0:14:23.423556
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 15:09:31.141866
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.45
 ---- batch: 020 ----
mean loss: 253.53
train mean loss: 249.18
epoch train time: 0:00:04.242953
elapsed time: 0:14:27.666948
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 15:09:35.385338
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.15
 ---- batch: 020 ----
mean loss: 248.83
train mean loss: 248.98
epoch train time: 0:00:04.242371
elapsed time: 0:14:31.909905
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 15:09:39.628316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.40
 ---- batch: 020 ----
mean loss: 252.88
train mean loss: 252.66
epoch train time: 0:00:04.249653
elapsed time: 0:14:36.160135
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 15:09:43.878525
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 245.05
 ---- batch: 020 ----
mean loss: 259.23
train mean loss: 251.47
epoch train time: 0:00:04.254028
elapsed time: 0:14:40.414679
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 15:09:48.133054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.85
 ---- batch: 020 ----
mean loss: 245.20
train mean loss: 250.11
epoch train time: 0:00:04.254160
elapsed time: 0:14:44.669405
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 15:09:52.387778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.99
 ---- batch: 020 ----
mean loss: 252.19
train mean loss: 249.00
epoch train time: 0:00:04.256366
elapsed time: 0:14:48.926299
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 15:09:56.644675
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 248.16
 ---- batch: 020 ----
mean loss: 245.52
train mean loss: 248.43
epoch train time: 0:00:04.269513
elapsed time: 0:14:53.196974
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 15:10:00.915356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.98
 ---- batch: 020 ----
mean loss: 247.62
train mean loss: 249.39
epoch train time: 0:00:04.267742
elapsed time: 0:14:57.465317
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 15:10:05.183695
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.62
 ---- batch: 020 ----
mean loss: 250.26
train mean loss: 250.48
epoch train time: 0:00:04.246282
elapsed time: 0:15:01.712130
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 15:10:09.430516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.11
 ---- batch: 020 ----
mean loss: 242.72
train mean loss: 248.04
epoch train time: 0:00:04.222858
elapsed time: 0:15:05.935563
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 15:10:13.653943
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.18
 ---- batch: 020 ----
mean loss: 248.49
train mean loss: 252.93
epoch train time: 0:00:04.245208
elapsed time: 0:15:10.181291
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 15:10:17.899661
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.59
 ---- batch: 020 ----
mean loss: 247.49
train mean loss: 250.19
epoch train time: 0:00:04.229432
elapsed time: 0:15:14.411267
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 15:10:22.129680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.47
 ---- batch: 020 ----
mean loss: 251.18
train mean loss: 251.44
epoch train time: 0:00:04.225281
elapsed time: 0:15:18.637136
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 15:10:26.355515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.85
 ---- batch: 020 ----
mean loss: 250.11
train mean loss: 250.39
epoch train time: 0:00:04.216645
elapsed time: 0:15:22.854306
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 15:10:30.572672
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 244.52
 ---- batch: 020 ----
mean loss: 260.18
train mean loss: 252.21
epoch train time: 0:00:04.232771
elapsed time: 0:15:27.087603
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 15:10:34.805987
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.40
 ---- batch: 020 ----
mean loss: 249.75
train mean loss: 250.65
epoch train time: 0:00:04.223938
elapsed time: 0:15:31.312126
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 15:10:39.030493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 248.31
 ---- batch: 020 ----
mean loss: 250.73
train mean loss: 251.03
epoch train time: 0:00:04.222073
elapsed time: 0:15:35.534703
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 15:10:43.253077
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.25
 ---- batch: 020 ----
mean loss: 247.09
train mean loss: 248.76
epoch train time: 0:00:04.218656
elapsed time: 0:15:39.753858
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 15:10:47.472260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.12
 ---- batch: 020 ----
mean loss: 249.18
train mean loss: 251.51
epoch train time: 0:00:04.217160
elapsed time: 0:15:43.971586
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 15:10:51.689981
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 246.82
 ---- batch: 020 ----
mean loss: 258.91
train mean loss: 253.22
epoch train time: 0:00:04.222144
elapsed time: 0:15:48.194329
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 15:10:55.912735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 244.03
 ---- batch: 020 ----
mean loss: 247.83
train mean loss: 250.86
epoch train time: 0:00:04.233108
elapsed time: 0:15:52.428005
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 15:11:00.146392
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 248.66
 ---- batch: 020 ----
mean loss: 250.80
train mean loss: 248.64
epoch train time: 0:00:04.237928
elapsed time: 0:15:56.666461
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 15:11:04.384843
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.20
 ---- batch: 020 ----
mean loss: 254.37
train mean loss: 251.17
epoch train time: 0:00:04.254207
elapsed time: 0:16:00.921252
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 15:11:08.639642
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.64
 ---- batch: 020 ----
mean loss: 248.43
train mean loss: 250.73
epoch train time: 0:00:04.236629
elapsed time: 0:16:05.158489
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 15:11:12.876895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.93
 ---- batch: 020 ----
mean loss: 251.83
train mean loss: 252.70
epoch train time: 0:00:04.268212
elapsed time: 0:16:09.427235
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 15:11:17.145662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 248.06
 ---- batch: 020 ----
mean loss: 249.38
train mean loss: 247.78
epoch train time: 0:00:04.241312
elapsed time: 0:16:13.669111
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 15:11:21.387502
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.96
 ---- batch: 020 ----
mean loss: 253.18
train mean loss: 250.99
epoch train time: 0:00:04.234906
elapsed time: 0:16:17.904586
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 15:11:25.622989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.21
 ---- batch: 020 ----
mean loss: 246.19
train mean loss: 247.77
epoch train time: 0:00:04.236193
elapsed time: 0:16:22.141414
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 15:11:29.859806
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.03
 ---- batch: 020 ----
mean loss: 246.28
train mean loss: 249.46
epoch train time: 0:00:04.237921
elapsed time: 0:16:26.379878
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 15:11:34.098252
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 243.34
 ---- batch: 020 ----
mean loss: 258.01
train mean loss: 249.30
epoch train time: 0:00:04.247884
elapsed time: 0:16:30.628317
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 15:11:38.346751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.13
 ---- batch: 020 ----
mean loss: 252.21
train mean loss: 250.60
epoch train time: 0:00:04.242692
elapsed time: 0:16:34.871577
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 15:11:42.589973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 248.07
 ---- batch: 020 ----
mean loss: 250.29
train mean loss: 248.78
epoch train time: 0:00:04.246742
elapsed time: 0:16:39.118991
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 15:11:46.837295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 245.75
 ---- batch: 020 ----
mean loss: 248.62
train mean loss: 248.76
epoch train time: 0:00:04.252074
elapsed time: 0:16:43.371555
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 15:11:51.089989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.07
 ---- batch: 020 ----
mean loss: 241.26
train mean loss: 248.82
epoch train time: 0:00:04.251765
elapsed time: 0:16:47.623902
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 15:11:55.342291
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.02
 ---- batch: 020 ----
mean loss: 240.78
train mean loss: 247.90
epoch train time: 0:00:04.251202
elapsed time: 0:16:51.875639
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 15:11:59.594019
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.92
 ---- batch: 020 ----
mean loss: 253.82
train mean loss: 249.97
epoch train time: 0:00:04.244743
elapsed time: 0:16:56.120980
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 15:12:03.839356
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 239.94
 ---- batch: 020 ----
mean loss: 255.15
train mean loss: 249.02
epoch train time: 0:00:04.237667
elapsed time: 0:17:00.359170
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 15:12:08.077564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.27
 ---- batch: 020 ----
mean loss: 248.30
train mean loss: 248.15
epoch train time: 0:00:04.247181
elapsed time: 0:17:04.606890
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 15:12:12.325266
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.49
 ---- batch: 020 ----
mean loss: 250.68
train mean loss: 249.36
epoch train time: 0:00:04.238251
elapsed time: 0:17:08.845709
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 15:12:16.564105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.45
 ---- batch: 020 ----
mean loss: 244.75
train mean loss: 247.81
epoch train time: 0:00:04.242905
elapsed time: 0:17:13.089153
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 15:12:20.807550
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 244.11
 ---- batch: 020 ----
mean loss: 256.47
train mean loss: 249.75
epoch train time: 0:00:04.242845
elapsed time: 0:17:17.332530
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 15:12:25.050901
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.13
 ---- batch: 020 ----
mean loss: 247.00
train mean loss: 248.26
epoch train time: 0:00:04.238514
elapsed time: 0:17:21.571562
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 15:12:29.289957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.77
 ---- batch: 020 ----
mean loss: 246.82
train mean loss: 247.55
epoch train time: 0:00:04.244929
elapsed time: 0:17:25.817060
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 15:12:33.535445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.96
 ---- batch: 020 ----
mean loss: 252.19
train mean loss: 248.66
epoch train time: 0:00:04.260282
elapsed time: 0:17:30.077887
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 15:12:37.796269
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 245.67
 ---- batch: 020 ----
mean loss: 245.88
train mean loss: 247.63
epoch train time: 0:00:04.239605
elapsed time: 0:17:34.318022
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 15:12:42.036397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 249.34
 ---- batch: 020 ----
mean loss: 249.18
train mean loss: 247.88
epoch train time: 0:00:04.266149
elapsed time: 0:17:38.584718
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 15:12:46.303089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.61
 ---- batch: 020 ----
mean loss: 251.49
train mean loss: 250.07
epoch train time: 0:00:04.234773
elapsed time: 0:17:42.819998
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 15:12:50.538421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 245.94
 ---- batch: 020 ----
mean loss: 246.97
train mean loss: 247.60
epoch train time: 0:00:04.235890
elapsed time: 0:17:47.065983
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_7/checkpoint.pth.tar
**** end time: 2019-09-25 15:12:54.784259 ****
