Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 1624
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 08:20:32.644918 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 08:20:32.663354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2433.21
 ---- batch: 020 ----
mean loss: 1582.88
 ---- batch: 030 ----
mean loss: 1394.08
 ---- batch: 040 ----
mean loss: 1295.18
 ---- batch: 050 ----
mean loss: 1237.01
 ---- batch: 060 ----
mean loss: 1201.44
 ---- batch: 070 ----
mean loss: 1157.47
 ---- batch: 080 ----
mean loss: 1131.65
 ---- batch: 090 ----
mean loss: 1098.42
 ---- batch: 100 ----
mean loss: 1075.21
 ---- batch: 110 ----
mean loss: 1027.87
train mean loss: 1323.34
epoch train time: 0:00:46.177675
elapsed time: 0:00:46.204814
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 08:21:18.849784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1048.52
 ---- batch: 020 ----
mean loss: 1057.65
 ---- batch: 030 ----
mean loss: 1047.62
 ---- batch: 040 ----
mean loss: 1021.28
 ---- batch: 050 ----
mean loss: 993.68
 ---- batch: 060 ----
mean loss: 1006.62
 ---- batch: 070 ----
mean loss: 1039.69
 ---- batch: 080 ----
mean loss: 997.77
 ---- batch: 090 ----
mean loss: 1019.40
 ---- batch: 100 ----
mean loss: 1009.81
 ---- batch: 110 ----
mean loss: 1024.24
train mean loss: 1023.97
epoch train time: 0:00:15.492454
elapsed time: 0:01:01.697734
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 08:21:34.343292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 995.52
 ---- batch: 020 ----
mean loss: 1017.56
 ---- batch: 030 ----
mean loss: 1007.33
 ---- batch: 040 ----
mean loss: 979.16
 ---- batch: 050 ----
mean loss: 972.09
 ---- batch: 060 ----
mean loss: 969.06
 ---- batch: 070 ----
mean loss: 1024.71
 ---- batch: 080 ----
mean loss: 1006.73
 ---- batch: 090 ----
mean loss: 1019.69
 ---- batch: 100 ----
mean loss: 979.88
 ---- batch: 110 ----
mean loss: 992.68
train mean loss: 997.05
epoch train time: 0:00:15.473986
elapsed time: 0:01:17.172849
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 08:21:49.818300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1007.97
 ---- batch: 020 ----
mean loss: 997.59
 ---- batch: 030 ----
mean loss: 989.05
 ---- batch: 040 ----
mean loss: 965.13
 ---- batch: 050 ----
mean loss: 965.08
 ---- batch: 060 ----
mean loss: 968.54
 ---- batch: 070 ----
mean loss: 986.30
 ---- batch: 080 ----
mean loss: 960.42
 ---- batch: 090 ----
mean loss: 962.38
 ---- batch: 100 ----
mean loss: 981.14
 ---- batch: 110 ----
mean loss: 950.39
train mean loss: 975.54
epoch train time: 0:00:15.459466
elapsed time: 0:01:32.633256
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 08:22:05.278746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.06
 ---- batch: 020 ----
mean loss: 954.67
 ---- batch: 030 ----
mean loss: 972.63
 ---- batch: 040 ----
mean loss: 966.39
 ---- batch: 050 ----
mean loss: 975.74
 ---- batch: 060 ----
mean loss: 936.63
 ---- batch: 070 ----
mean loss: 959.73
 ---- batch: 080 ----
mean loss: 950.68
 ---- batch: 090 ----
mean loss: 958.13
 ---- batch: 100 ----
mean loss: 955.62
 ---- batch: 110 ----
mean loss: 956.45
train mean loss: 957.23
epoch train time: 0:00:15.415852
elapsed time: 0:01:48.050184
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 08:22:20.695696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 954.74
 ---- batch: 020 ----
mean loss: 957.43
 ---- batch: 030 ----
mean loss: 954.97
 ---- batch: 040 ----
mean loss: 928.89
 ---- batch: 050 ----
mean loss: 933.79
 ---- batch: 060 ----
mean loss: 952.67
 ---- batch: 070 ----
mean loss: 940.35
 ---- batch: 080 ----
mean loss: 961.07
 ---- batch: 090 ----
mean loss: 948.17
 ---- batch: 100 ----
mean loss: 950.79
 ---- batch: 110 ----
mean loss: 949.99
train mean loss: 948.85
epoch train time: 0:00:15.494804
elapsed time: 0:02:03.546050
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 08:22:36.191479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.61
 ---- batch: 020 ----
mean loss: 932.67
 ---- batch: 030 ----
mean loss: 928.77
 ---- batch: 040 ----
mean loss: 933.62
 ---- batch: 050 ----
mean loss: 909.79
 ---- batch: 060 ----
mean loss: 931.28
 ---- batch: 070 ----
mean loss: 933.17
 ---- batch: 080 ----
mean loss: 926.97
 ---- batch: 090 ----
mean loss: 946.63
 ---- batch: 100 ----
mean loss: 947.54
 ---- batch: 110 ----
mean loss: 939.27
train mean loss: 934.22
epoch train time: 0:00:15.509747
elapsed time: 0:02:19.056825
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 08:22:51.702325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.66
 ---- batch: 020 ----
mean loss: 916.09
 ---- batch: 030 ----
mean loss: 950.63
 ---- batch: 040 ----
mean loss: 928.79
 ---- batch: 050 ----
mean loss: 915.57
 ---- batch: 060 ----
mean loss: 935.37
 ---- batch: 070 ----
mean loss: 907.21
 ---- batch: 080 ----
mean loss: 922.28
 ---- batch: 090 ----
mean loss: 932.21
 ---- batch: 100 ----
mean loss: 942.15
 ---- batch: 110 ----
mean loss: 919.11
train mean loss: 926.12
epoch train time: 0:00:15.449694
elapsed time: 0:02:34.507554
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 08:23:07.152979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.08
 ---- batch: 020 ----
mean loss: 925.68
 ---- batch: 030 ----
mean loss: 903.92
 ---- batch: 040 ----
mean loss: 899.95
 ---- batch: 050 ----
mean loss: 917.20
 ---- batch: 060 ----
mean loss: 900.42
 ---- batch: 070 ----
mean loss: 919.61
 ---- batch: 080 ----
mean loss: 907.25
 ---- batch: 090 ----
mean loss: 898.47
 ---- batch: 100 ----
mean loss: 917.16
 ---- batch: 110 ----
mean loss: 907.43
train mean loss: 911.64
epoch train time: 0:00:15.470381
elapsed time: 0:02:49.978869
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 08:23:22.624271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.32
 ---- batch: 020 ----
mean loss: 908.61
 ---- batch: 030 ----
mean loss: 857.85
 ---- batch: 040 ----
mean loss: 905.86
 ---- batch: 050 ----
mean loss: 897.62
 ---- batch: 060 ----
mean loss: 894.73
 ---- batch: 070 ----
mean loss: 891.04
 ---- batch: 080 ----
mean loss: 914.27
 ---- batch: 090 ----
mean loss: 895.00
 ---- batch: 100 ----
mean loss: 876.64
 ---- batch: 110 ----
mean loss: 904.12
train mean loss: 893.43
epoch train time: 0:00:15.472214
elapsed time: 0:03:05.452001
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 08:23:38.097426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.96
 ---- batch: 020 ----
mean loss: 864.65
 ---- batch: 030 ----
mean loss: 884.75
 ---- batch: 040 ----
mean loss: 899.92
 ---- batch: 050 ----
mean loss: 876.82
 ---- batch: 060 ----
mean loss: 864.65
 ---- batch: 070 ----
mean loss: 886.09
 ---- batch: 080 ----
mean loss: 883.97
 ---- batch: 090 ----
mean loss: 887.42
 ---- batch: 100 ----
mean loss: 871.65
 ---- batch: 110 ----
mean loss: 862.16
train mean loss: 878.49
epoch train time: 0:00:15.439869
elapsed time: 0:03:20.892878
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 08:23:53.538290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.27
 ---- batch: 020 ----
mean loss: 836.15
 ---- batch: 030 ----
mean loss: 877.37
 ---- batch: 040 ----
mean loss: 864.89
 ---- batch: 050 ----
mean loss: 854.28
 ---- batch: 060 ----
mean loss: 836.31
 ---- batch: 070 ----
mean loss: 849.26
 ---- batch: 080 ----
mean loss: 815.46
 ---- batch: 090 ----
mean loss: 846.29
 ---- batch: 100 ----
mean loss: 830.32
 ---- batch: 110 ----
mean loss: 822.29
train mean loss: 847.75
epoch train time: 0:00:15.601768
elapsed time: 0:03:36.495510
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 08:24:09.141028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.22
 ---- batch: 020 ----
mean loss: 827.73
 ---- batch: 030 ----
mean loss: 826.51
 ---- batch: 040 ----
mean loss: 796.64
 ---- batch: 050 ----
mean loss: 815.60
 ---- batch: 060 ----
mean loss: 792.29
 ---- batch: 070 ----
mean loss: 809.94
 ---- batch: 080 ----
mean loss: 766.01
 ---- batch: 090 ----
mean loss: 784.17
 ---- batch: 100 ----
mean loss: 786.44
 ---- batch: 110 ----
mean loss: 767.46
train mean loss: 801.40
epoch train time: 0:00:15.601863
elapsed time: 0:03:52.098394
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 08:24:24.743849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 767.90
 ---- batch: 020 ----
mean loss: 754.25
 ---- batch: 030 ----
mean loss: 757.60
 ---- batch: 040 ----
mean loss: 741.38
 ---- batch: 050 ----
mean loss: 748.18
 ---- batch: 060 ----
mean loss: 729.86
 ---- batch: 070 ----
mean loss: 764.99
 ---- batch: 080 ----
mean loss: 743.76
 ---- batch: 090 ----
mean loss: 729.11
 ---- batch: 100 ----
mean loss: 735.96
 ---- batch: 110 ----
mean loss: 725.26
train mean loss: 745.45
epoch train time: 0:00:15.593968
elapsed time: 0:04:07.693322
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 08:24:40.338768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 719.70
 ---- batch: 020 ----
mean loss: 700.68
 ---- batch: 030 ----
mean loss: 721.61
 ---- batch: 040 ----
mean loss: 713.75
 ---- batch: 050 ----
mean loss: 692.19
 ---- batch: 060 ----
mean loss: 689.01
 ---- batch: 070 ----
mean loss: 688.74
 ---- batch: 080 ----
mean loss: 681.95
 ---- batch: 090 ----
mean loss: 674.22
 ---- batch: 100 ----
mean loss: 686.60
 ---- batch: 110 ----
mean loss: 679.69
train mean loss: 693.91
epoch train time: 0:00:15.603736
elapsed time: 0:04:23.298026
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 08:24:55.943462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.89
 ---- batch: 020 ----
mean loss: 669.75
 ---- batch: 030 ----
mean loss: 658.53
 ---- batch: 040 ----
mean loss: 650.23
 ---- batch: 050 ----
mean loss: 657.32
 ---- batch: 060 ----
mean loss: 661.93
 ---- batch: 070 ----
mean loss: 663.00
 ---- batch: 080 ----
mean loss: 644.89
 ---- batch: 090 ----
mean loss: 641.92
 ---- batch: 100 ----
mean loss: 652.21
 ---- batch: 110 ----
mean loss: 642.57
train mean loss: 657.15
epoch train time: 0:00:15.613966
elapsed time: 0:04:38.912948
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 08:25:11.558370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.78
 ---- batch: 020 ----
mean loss: 616.55
 ---- batch: 030 ----
mean loss: 626.04
 ---- batch: 040 ----
mean loss: 631.10
 ---- batch: 050 ----
mean loss: 621.31
 ---- batch: 060 ----
mean loss: 630.90
 ---- batch: 070 ----
mean loss: 617.80
 ---- batch: 080 ----
mean loss: 614.66
 ---- batch: 090 ----
mean loss: 594.19
 ---- batch: 100 ----
mean loss: 595.29
 ---- batch: 110 ----
mean loss: 600.19
train mean loss: 616.77
epoch train time: 0:00:15.729218
elapsed time: 0:04:54.643099
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 08:25:27.288528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.59
 ---- batch: 020 ----
mean loss: 609.55
 ---- batch: 030 ----
mean loss: 581.04
 ---- batch: 040 ----
mean loss: 595.50
 ---- batch: 050 ----
mean loss: 588.65
 ---- batch: 060 ----
mean loss: 584.34
 ---- batch: 070 ----
mean loss: 583.51
 ---- batch: 080 ----
mean loss: 579.65
 ---- batch: 090 ----
mean loss: 572.46
 ---- batch: 100 ----
mean loss: 585.59
 ---- batch: 110 ----
mean loss: 577.36
train mean loss: 586.78
epoch train time: 0:00:15.647857
elapsed time: 0:05:10.291943
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 08:25:42.937401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 563.71
 ---- batch: 020 ----
mean loss: 565.92
 ---- batch: 030 ----
mean loss: 571.95
 ---- batch: 040 ----
mean loss: 535.27
 ---- batch: 050 ----
mean loss: 556.13
 ---- batch: 060 ----
mean loss: 548.27
 ---- batch: 070 ----
mean loss: 554.30
 ---- batch: 080 ----
mean loss: 562.12
 ---- batch: 090 ----
mean loss: 538.16
 ---- batch: 100 ----
mean loss: 545.97
 ---- batch: 110 ----
mean loss: 556.65
train mean loss: 553.57
epoch train time: 0:00:15.648820
elapsed time: 0:05:25.941770
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 08:25:58.587224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.15
 ---- batch: 020 ----
mean loss: 542.00
 ---- batch: 030 ----
mean loss: 540.56
 ---- batch: 040 ----
mean loss: 539.23
 ---- batch: 050 ----
mean loss: 526.09
 ---- batch: 060 ----
mean loss: 539.25
 ---- batch: 070 ----
mean loss: 534.52
 ---- batch: 080 ----
mean loss: 526.22
 ---- batch: 090 ----
mean loss: 511.81
 ---- batch: 100 ----
mean loss: 521.57
 ---- batch: 110 ----
mean loss: 519.42
train mean loss: 529.19
epoch train time: 0:00:15.625105
elapsed time: 0:05:41.567867
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 08:26:14.213307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.59
 ---- batch: 020 ----
mean loss: 514.90
 ---- batch: 030 ----
mean loss: 518.00
 ---- batch: 040 ----
mean loss: 520.26
 ---- batch: 050 ----
mean loss: 524.53
 ---- batch: 060 ----
mean loss: 496.80
 ---- batch: 070 ----
mean loss: 504.79
 ---- batch: 080 ----
mean loss: 478.16
 ---- batch: 090 ----
mean loss: 485.00
 ---- batch: 100 ----
mean loss: 485.47
 ---- batch: 110 ----
mean loss: 485.90
train mean loss: 500.72
epoch train time: 0:00:15.491812
elapsed time: 0:05:57.060671
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 08:26:29.706139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.38
 ---- batch: 020 ----
mean loss: 488.24
 ---- batch: 030 ----
mean loss: 494.25
 ---- batch: 040 ----
mean loss: 488.89
 ---- batch: 050 ----
mean loss: 491.53
 ---- batch: 060 ----
mean loss: 488.16
 ---- batch: 070 ----
mean loss: 482.21
 ---- batch: 080 ----
mean loss: 468.96
 ---- batch: 090 ----
mean loss: 466.87
 ---- batch: 100 ----
mean loss: 463.78
 ---- batch: 110 ----
mean loss: 459.43
train mean loss: 479.24
epoch train time: 0:00:15.422793
elapsed time: 0:06:12.484313
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 08:26:45.129793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.81
 ---- batch: 020 ----
mean loss: 460.97
 ---- batch: 030 ----
mean loss: 452.17
 ---- batch: 040 ----
mean loss: 459.10
 ---- batch: 050 ----
mean loss: 453.76
 ---- batch: 060 ----
mean loss: 457.53
 ---- batch: 070 ----
mean loss: 474.85
 ---- batch: 080 ----
mean loss: 454.60
 ---- batch: 090 ----
mean loss: 449.25
 ---- batch: 100 ----
mean loss: 464.03
 ---- batch: 110 ----
mean loss: 438.13
train mean loss: 456.57
epoch train time: 0:00:15.434642
elapsed time: 0:06:27.919913
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 08:27:00.565381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 446.97
 ---- batch: 020 ----
mean loss: 446.67
 ---- batch: 030 ----
mean loss: 441.76
 ---- batch: 040 ----
mean loss: 438.69
 ---- batch: 050 ----
mean loss: 441.76
 ---- batch: 060 ----
mean loss: 447.91
 ---- batch: 070 ----
mean loss: 440.97
 ---- batch: 080 ----
mean loss: 423.85
 ---- batch: 090 ----
mean loss: 420.30
 ---- batch: 100 ----
mean loss: 431.52
 ---- batch: 110 ----
mean loss: 431.27
train mean loss: 436.87
epoch train time: 0:00:15.451900
elapsed time: 0:06:43.372795
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 08:27:16.018236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.39
 ---- batch: 020 ----
mean loss: 419.20
 ---- batch: 030 ----
mean loss: 419.09
 ---- batch: 040 ----
mean loss: 415.43
 ---- batch: 050 ----
mean loss: 423.11
 ---- batch: 060 ----
mean loss: 424.91
 ---- batch: 070 ----
mean loss: 423.48
 ---- batch: 080 ----
mean loss: 416.34
 ---- batch: 090 ----
mean loss: 425.81
 ---- batch: 100 ----
mean loss: 411.96
 ---- batch: 110 ----
mean loss: 418.90
train mean loss: 420.34
epoch train time: 0:00:15.421393
elapsed time: 0:06:58.795147
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 08:27:31.440566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.34
 ---- batch: 020 ----
mean loss: 417.70
 ---- batch: 030 ----
mean loss: 399.12
 ---- batch: 040 ----
mean loss: 421.35
 ---- batch: 050 ----
mean loss: 406.36
 ---- batch: 060 ----
mean loss: 406.06
 ---- batch: 070 ----
mean loss: 408.05
 ---- batch: 080 ----
mean loss: 410.02
 ---- batch: 090 ----
mean loss: 394.39
 ---- batch: 100 ----
mean loss: 404.50
 ---- batch: 110 ----
mean loss: 401.37
train mean loss: 407.31
epoch train time: 0:00:15.446017
elapsed time: 0:07:14.241976
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 08:27:46.887454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.37
 ---- batch: 020 ----
mean loss: 406.77
 ---- batch: 030 ----
mean loss: 401.39
 ---- batch: 040 ----
mean loss: 393.33
 ---- batch: 050 ----
mean loss: 388.41
 ---- batch: 060 ----
mean loss: 389.17
 ---- batch: 070 ----
mean loss: 380.65
 ---- batch: 080 ----
mean loss: 402.58
 ---- batch: 090 ----
mean loss: 396.31
 ---- batch: 100 ----
mean loss: 389.21
 ---- batch: 110 ----
mean loss: 388.67
train mean loss: 395.09
epoch train time: 0:00:15.424015
elapsed time: 0:07:29.667031
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 08:28:02.312549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.97
 ---- batch: 020 ----
mean loss: 374.84
 ---- batch: 030 ----
mean loss: 394.62
 ---- batch: 040 ----
mean loss: 387.38
 ---- batch: 050 ----
mean loss: 396.72
 ---- batch: 060 ----
mean loss: 369.16
 ---- batch: 070 ----
mean loss: 382.78
 ---- batch: 080 ----
mean loss: 377.52
 ---- batch: 090 ----
mean loss: 374.36
 ---- batch: 100 ----
mean loss: 373.07
 ---- batch: 110 ----
mean loss: 374.17
train mean loss: 380.23
epoch train time: 0:00:15.459647
elapsed time: 0:07:45.127723
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 08:28:17.773241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.99
 ---- batch: 020 ----
mean loss: 376.13
 ---- batch: 030 ----
mean loss: 365.94
 ---- batch: 040 ----
mean loss: 371.48
 ---- batch: 050 ----
mean loss: 377.14
 ---- batch: 060 ----
mean loss: 366.27
 ---- batch: 070 ----
mean loss: 373.11
 ---- batch: 080 ----
mean loss: 386.53
 ---- batch: 090 ----
mean loss: 368.56
 ---- batch: 100 ----
mean loss: 375.02
 ---- batch: 110 ----
mean loss: 379.56
train mean loss: 373.88
epoch train time: 0:00:15.570534
elapsed time: 0:08:00.699279
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 08:28:33.344819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.35
 ---- batch: 020 ----
mean loss: 375.27
 ---- batch: 030 ----
mean loss: 363.25
 ---- batch: 040 ----
mean loss: 374.79
 ---- batch: 050 ----
mean loss: 363.83
 ---- batch: 060 ----
mean loss: 357.20
 ---- batch: 070 ----
mean loss: 349.35
 ---- batch: 080 ----
mean loss: 348.13
 ---- batch: 090 ----
mean loss: 356.93
 ---- batch: 100 ----
mean loss: 369.16
 ---- batch: 110 ----
mean loss: 376.69
train mean loss: 364.17
epoch train time: 0:00:15.530491
elapsed time: 0:08:16.230872
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 08:28:48.876338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.00
 ---- batch: 020 ----
mean loss: 367.80
 ---- batch: 030 ----
mean loss: 364.29
 ---- batch: 040 ----
mean loss: 355.56
 ---- batch: 050 ----
mean loss: 350.36
 ---- batch: 060 ----
mean loss: 368.57
 ---- batch: 070 ----
mean loss: 339.95
 ---- batch: 080 ----
mean loss: 374.73
 ---- batch: 090 ----
mean loss: 355.70
 ---- batch: 100 ----
mean loss: 352.70
 ---- batch: 110 ----
mean loss: 362.26
train mean loss: 359.52
epoch train time: 0:00:15.406660
elapsed time: 0:08:31.638482
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 08:29:04.283891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.80
 ---- batch: 020 ----
mean loss: 354.10
 ---- batch: 030 ----
mean loss: 350.54
 ---- batch: 040 ----
mean loss: 354.31
 ---- batch: 050 ----
mean loss: 346.10
 ---- batch: 060 ----
mean loss: 362.93
 ---- batch: 070 ----
mean loss: 331.92
 ---- batch: 080 ----
mean loss: 346.77
 ---- batch: 090 ----
mean loss: 343.88
 ---- batch: 100 ----
mean loss: 348.46
 ---- batch: 110 ----
mean loss: 345.24
train mean loss: 349.04
epoch train time: 0:00:15.448623
elapsed time: 0:08:47.088039
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 08:29:19.733514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.24
 ---- batch: 020 ----
mean loss: 342.38
 ---- batch: 030 ----
mean loss: 345.13
 ---- batch: 040 ----
mean loss: 333.17
 ---- batch: 050 ----
mean loss: 348.84
 ---- batch: 060 ----
mean loss: 344.64
 ---- batch: 070 ----
mean loss: 331.10
 ---- batch: 080 ----
mean loss: 361.08
 ---- batch: 090 ----
mean loss: 346.02
 ---- batch: 100 ----
mean loss: 334.22
 ---- batch: 110 ----
mean loss: 337.10
train mean loss: 342.02
epoch train time: 0:00:15.487161
elapsed time: 0:09:02.576195
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 08:29:35.221663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.05
 ---- batch: 020 ----
mean loss: 337.73
 ---- batch: 030 ----
mean loss: 327.41
 ---- batch: 040 ----
mean loss: 331.07
 ---- batch: 050 ----
mean loss: 339.16
 ---- batch: 060 ----
mean loss: 330.01
 ---- batch: 070 ----
mean loss: 350.17
 ---- batch: 080 ----
mean loss: 337.68
 ---- batch: 090 ----
mean loss: 344.75
 ---- batch: 100 ----
mean loss: 321.18
 ---- batch: 110 ----
mean loss: 329.61
train mean loss: 335.02
epoch train time: 0:00:15.518881
elapsed time: 0:09:18.096095
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 08:29:50.741554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.58
 ---- batch: 020 ----
mean loss: 324.05
 ---- batch: 030 ----
mean loss: 339.48
 ---- batch: 040 ----
mean loss: 337.23
 ---- batch: 050 ----
mean loss: 317.36
 ---- batch: 060 ----
mean loss: 334.15
 ---- batch: 070 ----
mean loss: 322.48
 ---- batch: 080 ----
mean loss: 331.55
 ---- batch: 090 ----
mean loss: 324.71
 ---- batch: 100 ----
mean loss: 331.11
 ---- batch: 110 ----
mean loss: 344.51
train mean loss: 330.23
epoch train time: 0:00:15.489207
elapsed time: 0:09:33.586253
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 08:30:06.231725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.74
 ---- batch: 020 ----
mean loss: 325.73
 ---- batch: 030 ----
mean loss: 318.86
 ---- batch: 040 ----
mean loss: 315.86
 ---- batch: 050 ----
mean loss: 325.77
 ---- batch: 060 ----
mean loss: 332.97
 ---- batch: 070 ----
mean loss: 331.07
 ---- batch: 080 ----
mean loss: 328.07
 ---- batch: 090 ----
mean loss: 313.92
 ---- batch: 100 ----
mean loss: 341.36
 ---- batch: 110 ----
mean loss: 313.86
train mean loss: 325.78
epoch train time: 0:00:15.491351
elapsed time: 0:09:49.078599
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 08:30:21.724030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.73
 ---- batch: 020 ----
mean loss: 325.86
 ---- batch: 030 ----
mean loss: 327.50
 ---- batch: 040 ----
mean loss: 313.23
 ---- batch: 050 ----
mean loss: 322.26
 ---- batch: 060 ----
mean loss: 322.79
 ---- batch: 070 ----
mean loss: 322.12
 ---- batch: 080 ----
mean loss: 314.32
 ---- batch: 090 ----
mean loss: 316.49
 ---- batch: 100 ----
mean loss: 319.75
 ---- batch: 110 ----
mean loss: 316.75
train mean loss: 320.26
epoch train time: 0:00:15.551035
elapsed time: 0:10:04.630584
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 08:30:37.276038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.02
 ---- batch: 020 ----
mean loss: 311.88
 ---- batch: 030 ----
mean loss: 324.26
 ---- batch: 040 ----
mean loss: 321.09
 ---- batch: 050 ----
mean loss: 315.68
 ---- batch: 060 ----
mean loss: 319.53
 ---- batch: 070 ----
mean loss: 301.29
 ---- batch: 080 ----
mean loss: 310.92
 ---- batch: 090 ----
mean loss: 310.23
 ---- batch: 100 ----
mean loss: 315.93
 ---- batch: 110 ----
mean loss: 321.89
train mean loss: 314.22
epoch train time: 0:00:15.412563
elapsed time: 0:10:20.044092
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 08:30:52.689546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.79
 ---- batch: 020 ----
mean loss: 318.95
 ---- batch: 030 ----
mean loss: 309.71
 ---- batch: 040 ----
mean loss: 314.48
 ---- batch: 050 ----
mean loss: 304.75
 ---- batch: 060 ----
mean loss: 311.10
 ---- batch: 070 ----
mean loss: 301.95
 ---- batch: 080 ----
mean loss: 328.36
 ---- batch: 090 ----
mean loss: 303.73
 ---- batch: 100 ----
mean loss: 305.88
 ---- batch: 110 ----
mean loss: 303.15
train mean loss: 311.16
epoch train time: 0:00:15.441488
elapsed time: 0:10:35.486549
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 08:31:08.132208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.98
 ---- batch: 020 ----
mean loss: 302.52
 ---- batch: 030 ----
mean loss: 298.46
 ---- batch: 040 ----
mean loss: 306.88
 ---- batch: 050 ----
mean loss: 307.40
 ---- batch: 060 ----
mean loss: 308.71
 ---- batch: 070 ----
mean loss: 311.32
 ---- batch: 080 ----
mean loss: 299.40
 ---- batch: 090 ----
mean loss: 313.23
 ---- batch: 100 ----
mean loss: 314.60
 ---- batch: 110 ----
mean loss: 309.11
train mean loss: 307.00
epoch train time: 0:00:15.629633
elapsed time: 0:10:51.117354
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 08:31:23.762818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.34
 ---- batch: 020 ----
mean loss: 300.89
 ---- batch: 030 ----
mean loss: 309.53
 ---- batch: 040 ----
mean loss: 306.28
 ---- batch: 050 ----
mean loss: 302.39
 ---- batch: 060 ----
mean loss: 312.44
 ---- batch: 070 ----
mean loss: 299.12
 ---- batch: 080 ----
mean loss: 295.32
 ---- batch: 090 ----
mean loss: 301.26
 ---- batch: 100 ----
mean loss: 298.86
 ---- batch: 110 ----
mean loss: 301.92
train mean loss: 302.26
epoch train time: 0:00:15.485551
elapsed time: 0:11:06.603849
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 08:31:39.249312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.93
 ---- batch: 020 ----
mean loss: 302.42
 ---- batch: 030 ----
mean loss: 304.54
 ---- batch: 040 ----
mean loss: 307.03
 ---- batch: 050 ----
mean loss: 296.60
 ---- batch: 060 ----
mean loss: 289.72
 ---- batch: 070 ----
mean loss: 306.76
 ---- batch: 080 ----
mean loss: 300.17
 ---- batch: 090 ----
mean loss: 299.04
 ---- batch: 100 ----
mean loss: 297.32
 ---- batch: 110 ----
mean loss: 299.87
train mean loss: 299.31
epoch train time: 0:00:15.443548
elapsed time: 0:11:22.048229
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 08:31:54.693768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.57
 ---- batch: 020 ----
mean loss: 291.37
 ---- batch: 030 ----
mean loss: 300.02
 ---- batch: 040 ----
mean loss: 299.17
 ---- batch: 050 ----
mean loss: 290.77
 ---- batch: 060 ----
mean loss: 301.79
 ---- batch: 070 ----
mean loss: 295.41
 ---- batch: 080 ----
mean loss: 292.11
 ---- batch: 090 ----
mean loss: 288.60
 ---- batch: 100 ----
mean loss: 289.10
 ---- batch: 110 ----
mean loss: 300.11
train mean loss: 295.03
epoch train time: 0:00:15.535215
elapsed time: 0:11:37.584512
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 08:32:10.229978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.56
 ---- batch: 020 ----
mean loss: 290.89
 ---- batch: 030 ----
mean loss: 294.47
 ---- batch: 040 ----
mean loss: 294.29
 ---- batch: 050 ----
mean loss: 291.75
 ---- batch: 060 ----
mean loss: 285.91
 ---- batch: 070 ----
mean loss: 294.67
 ---- batch: 080 ----
mean loss: 302.02
 ---- batch: 090 ----
mean loss: 290.04
 ---- batch: 100 ----
mean loss: 293.42
 ---- batch: 110 ----
mean loss: 288.32
train mean loss: 292.69
epoch train time: 0:00:15.389361
elapsed time: 0:11:52.974691
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 08:32:25.620125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.38
 ---- batch: 020 ----
mean loss: 296.12
 ---- batch: 030 ----
mean loss: 285.51
 ---- batch: 040 ----
mean loss: 284.16
 ---- batch: 050 ----
mean loss: 291.76
 ---- batch: 060 ----
mean loss: 299.41
 ---- batch: 070 ----
mean loss: 283.38
 ---- batch: 080 ----
mean loss: 283.92
 ---- batch: 090 ----
mean loss: 293.84
 ---- batch: 100 ----
mean loss: 290.58
 ---- batch: 110 ----
mean loss: 277.63
train mean loss: 288.68
epoch train time: 0:00:15.419528
elapsed time: 0:12:08.395210
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 08:32:41.040704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.11
 ---- batch: 020 ----
mean loss: 290.42
 ---- batch: 030 ----
mean loss: 291.66
 ---- batch: 040 ----
mean loss: 288.75
 ---- batch: 050 ----
mean loss: 277.72
 ---- batch: 060 ----
mean loss: 275.32
 ---- batch: 070 ----
mean loss: 292.55
 ---- batch: 080 ----
mean loss: 296.30
 ---- batch: 090 ----
mean loss: 283.59
 ---- batch: 100 ----
mean loss: 290.93
 ---- batch: 110 ----
mean loss: 279.39
train mean loss: 287.36
epoch train time: 0:00:15.416856
elapsed time: 0:12:23.813037
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 08:32:56.458469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.71
 ---- batch: 020 ----
mean loss: 281.53
 ---- batch: 030 ----
mean loss: 282.11
 ---- batch: 040 ----
mean loss: 275.19
 ---- batch: 050 ----
mean loss: 291.55
 ---- batch: 060 ----
mean loss: 286.01
 ---- batch: 070 ----
mean loss: 289.00
 ---- batch: 080 ----
mean loss: 281.93
 ---- batch: 090 ----
mean loss: 280.33
 ---- batch: 100 ----
mean loss: 282.64
 ---- batch: 110 ----
mean loss: 280.11
train mean loss: 283.02
epoch train time: 0:00:15.377752
elapsed time: 0:12:39.191787
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 08:33:11.837327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.51
 ---- batch: 020 ----
mean loss: 292.60
 ---- batch: 030 ----
mean loss: 274.49
 ---- batch: 040 ----
mean loss: 288.00
 ---- batch: 050 ----
mean loss: 279.71
 ---- batch: 060 ----
mean loss: 286.43
 ---- batch: 070 ----
mean loss: 286.69
 ---- batch: 080 ----
mean loss: 285.31
 ---- batch: 090 ----
mean loss: 281.77
 ---- batch: 100 ----
mean loss: 261.53
 ---- batch: 110 ----
mean loss: 277.19
train mean loss: 280.75
epoch train time: 0:00:15.450519
elapsed time: 0:12:54.643391
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 08:33:27.288841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.92
 ---- batch: 020 ----
mean loss: 274.86
 ---- batch: 030 ----
mean loss: 275.74
 ---- batch: 040 ----
mean loss: 275.98
 ---- batch: 050 ----
mean loss: 270.01
 ---- batch: 060 ----
mean loss: 276.72
 ---- batch: 070 ----
mean loss: 284.37
 ---- batch: 080 ----
mean loss: 283.55
 ---- batch: 090 ----
mean loss: 278.33
 ---- batch: 100 ----
mean loss: 276.36
 ---- batch: 110 ----
mean loss: 279.72
train mean loss: 277.85
epoch train time: 0:00:15.414847
elapsed time: 0:13:10.059090
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 08:33:42.704541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.46
 ---- batch: 020 ----
mean loss: 277.52
 ---- batch: 030 ----
mean loss: 270.69
 ---- batch: 040 ----
mean loss: 288.31
 ---- batch: 050 ----
mean loss: 276.11
 ---- batch: 060 ----
mean loss: 269.81
 ---- batch: 070 ----
mean loss: 270.18
 ---- batch: 080 ----
mean loss: 268.64
 ---- batch: 090 ----
mean loss: 270.46
 ---- batch: 100 ----
mean loss: 274.68
 ---- batch: 110 ----
mean loss: 266.11
train mean loss: 274.49
epoch train time: 0:00:15.374708
elapsed time: 0:13:25.434781
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 08:33:58.080256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.04
 ---- batch: 020 ----
mean loss: 268.58
 ---- batch: 030 ----
mean loss: 267.79
 ---- batch: 040 ----
mean loss: 266.84
 ---- batch: 050 ----
mean loss: 276.62
 ---- batch: 060 ----
mean loss: 270.28
 ---- batch: 070 ----
mean loss: 272.95
 ---- batch: 080 ----
mean loss: 260.12
 ---- batch: 090 ----
mean loss: 286.36
 ---- batch: 100 ----
mean loss: 273.99
 ---- batch: 110 ----
mean loss: 285.94
train mean loss: 272.47
epoch train time: 0:00:15.337735
elapsed time: 0:13:40.773555
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 08:34:13.419070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.10
 ---- batch: 020 ----
mean loss: 270.96
 ---- batch: 030 ----
mean loss: 281.42
 ---- batch: 040 ----
mean loss: 264.57
 ---- batch: 050 ----
mean loss: 267.43
 ---- batch: 060 ----
mean loss: 267.01
 ---- batch: 070 ----
mean loss: 272.18
 ---- batch: 080 ----
mean loss: 272.51
 ---- batch: 090 ----
mean loss: 266.57
 ---- batch: 100 ----
mean loss: 271.03
 ---- batch: 110 ----
mean loss: 268.49
train mean loss: 269.80
epoch train time: 0:00:15.348692
elapsed time: 0:13:56.123257
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 08:34:28.768679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.49
 ---- batch: 020 ----
mean loss: 267.40
 ---- batch: 030 ----
mean loss: 265.10
 ---- batch: 040 ----
mean loss: 268.24
 ---- batch: 050 ----
mean loss: 261.52
 ---- batch: 060 ----
mean loss: 262.34
 ---- batch: 070 ----
mean loss: 267.75
 ---- batch: 080 ----
mean loss: 262.64
 ---- batch: 090 ----
mean loss: 260.92
 ---- batch: 100 ----
mean loss: 263.36
 ---- batch: 110 ----
mean loss: 269.66
train mean loss: 265.96
epoch train time: 0:00:15.352300
elapsed time: 0:14:11.476532
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 08:34:44.121988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.20
 ---- batch: 020 ----
mean loss: 267.54
 ---- batch: 030 ----
mean loss: 263.52
 ---- batch: 040 ----
mean loss: 261.28
 ---- batch: 050 ----
mean loss: 271.59
 ---- batch: 060 ----
mean loss: 270.23
 ---- batch: 070 ----
mean loss: 266.45
 ---- batch: 080 ----
mean loss: 255.52
 ---- batch: 090 ----
mean loss: 260.81
 ---- batch: 100 ----
mean loss: 263.81
 ---- batch: 110 ----
mean loss: 268.81
train mean loss: 265.49
epoch train time: 0:00:15.427053
elapsed time: 0:14:26.904567
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 08:34:59.550059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.88
 ---- batch: 020 ----
mean loss: 261.86
 ---- batch: 030 ----
mean loss: 266.57
 ---- batch: 040 ----
mean loss: 257.71
 ---- batch: 050 ----
mean loss: 269.11
 ---- batch: 060 ----
mean loss: 255.28
 ---- batch: 070 ----
mean loss: 246.76
 ---- batch: 080 ----
mean loss: 260.54
 ---- batch: 090 ----
mean loss: 259.36
 ---- batch: 100 ----
mean loss: 268.10
 ---- batch: 110 ----
mean loss: 266.12
train mean loss: 261.57
epoch train time: 0:00:15.369902
elapsed time: 0:14:42.275381
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 08:35:14.920807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.24
 ---- batch: 020 ----
mean loss: 251.89
 ---- batch: 030 ----
mean loss: 271.43
 ---- batch: 040 ----
mean loss: 263.49
 ---- batch: 050 ----
mean loss: 262.48
 ---- batch: 060 ----
mean loss: 256.08
 ---- batch: 070 ----
mean loss: 257.53
 ---- batch: 080 ----
mean loss: 252.19
 ---- batch: 090 ----
mean loss: 264.25
 ---- batch: 100 ----
mean loss: 258.95
 ---- batch: 110 ----
mean loss: 259.91
train mean loss: 260.38
epoch train time: 0:00:15.399627
elapsed time: 0:14:57.675822
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 08:35:30.321344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.88
 ---- batch: 020 ----
mean loss: 266.08
 ---- batch: 030 ----
mean loss: 271.14
 ---- batch: 040 ----
mean loss: 251.92
 ---- batch: 050 ----
mean loss: 264.02
 ---- batch: 060 ----
mean loss: 261.87
 ---- batch: 070 ----
mean loss: 254.82
 ---- batch: 080 ----
mean loss: 256.04
 ---- batch: 090 ----
mean loss: 256.73
 ---- batch: 100 ----
mean loss: 251.28
 ---- batch: 110 ----
mean loss: 258.82
train mean loss: 259.69
epoch train time: 0:00:15.483548
elapsed time: 0:15:13.160428
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 08:35:45.805879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.14
 ---- batch: 020 ----
mean loss: 253.13
 ---- batch: 030 ----
mean loss: 252.05
 ---- batch: 040 ----
mean loss: 247.97
 ---- batch: 050 ----
mean loss: 258.38
 ---- batch: 060 ----
mean loss: 253.70
 ---- batch: 070 ----
mean loss: 257.57
 ---- batch: 080 ----
mean loss: 254.17
 ---- batch: 090 ----
mean loss: 251.06
 ---- batch: 100 ----
mean loss: 253.47
 ---- batch: 110 ----
mean loss: 267.00
train mean loss: 255.51
epoch train time: 0:00:15.405511
elapsed time: 0:15:28.566931
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 08:36:01.212342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.90
 ---- batch: 020 ----
mean loss: 259.78
 ---- batch: 030 ----
mean loss: 253.88
 ---- batch: 040 ----
mean loss: 255.09
 ---- batch: 050 ----
mean loss: 249.12
 ---- batch: 060 ----
mean loss: 249.18
 ---- batch: 070 ----
mean loss: 257.95
 ---- batch: 080 ----
mean loss: 258.10
 ---- batch: 090 ----
mean loss: 251.51
 ---- batch: 100 ----
mean loss: 259.35
 ---- batch: 110 ----
mean loss: 247.92
train mean loss: 253.77
epoch train time: 0:00:15.363690
elapsed time: 0:15:43.931414
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 08:36:16.576925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.07
 ---- batch: 020 ----
mean loss: 250.93
 ---- batch: 030 ----
mean loss: 254.51
 ---- batch: 040 ----
mean loss: 250.92
 ---- batch: 050 ----
mean loss: 240.36
 ---- batch: 060 ----
mean loss: 252.10
 ---- batch: 070 ----
mean loss: 247.69
 ---- batch: 080 ----
mean loss: 257.34
 ---- batch: 090 ----
mean loss: 260.89
 ---- batch: 100 ----
mean loss: 259.80
 ---- batch: 110 ----
mean loss: 264.08
train mean loss: 253.43
epoch train time: 0:00:15.408933
elapsed time: 0:15:59.341286
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 08:36:31.986838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.34
 ---- batch: 020 ----
mean loss: 243.27
 ---- batch: 030 ----
mean loss: 251.51
 ---- batch: 040 ----
mean loss: 253.94
 ---- batch: 050 ----
mean loss: 249.52
 ---- batch: 060 ----
mean loss: 245.96
 ---- batch: 070 ----
mean loss: 242.27
 ---- batch: 080 ----
mean loss: 256.08
 ---- batch: 090 ----
mean loss: 253.15
 ---- batch: 100 ----
mean loss: 251.00
 ---- batch: 110 ----
mean loss: 249.70
train mean loss: 249.82
epoch train time: 0:00:15.447167
elapsed time: 0:16:14.789548
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 08:36:47.435076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.83
 ---- batch: 020 ----
mean loss: 254.28
 ---- batch: 030 ----
mean loss: 248.45
 ---- batch: 040 ----
mean loss: 236.30
 ---- batch: 050 ----
mean loss: 247.56
 ---- batch: 060 ----
mean loss: 245.15
 ---- batch: 070 ----
mean loss: 259.30
 ---- batch: 080 ----
mean loss: 247.72
 ---- batch: 090 ----
mean loss: 250.89
 ---- batch: 100 ----
mean loss: 236.09
 ---- batch: 110 ----
mean loss: 251.58
train mean loss: 247.12
epoch train time: 0:00:15.430603
elapsed time: 0:16:30.221204
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 08:37:02.866665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.49
 ---- batch: 020 ----
mean loss: 236.00
 ---- batch: 030 ----
mean loss: 245.50
 ---- batch: 040 ----
mean loss: 256.63
 ---- batch: 050 ----
mean loss: 244.36
 ---- batch: 060 ----
mean loss: 248.04
 ---- batch: 070 ----
mean loss: 232.04
 ---- batch: 080 ----
mean loss: 254.01
 ---- batch: 090 ----
mean loss: 247.65
 ---- batch: 100 ----
mean loss: 246.29
 ---- batch: 110 ----
mean loss: 248.61
train mean loss: 246.74
epoch train time: 0:00:15.392354
elapsed time: 0:16:45.614571
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 08:37:18.260034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.25
 ---- batch: 020 ----
mean loss: 239.21
 ---- batch: 030 ----
mean loss: 243.49
 ---- batch: 040 ----
mean loss: 243.74
 ---- batch: 050 ----
mean loss: 241.23
 ---- batch: 060 ----
mean loss: 244.47
 ---- batch: 070 ----
mean loss: 248.41
 ---- batch: 080 ----
mean loss: 241.42
 ---- batch: 090 ----
mean loss: 240.32
 ---- batch: 100 ----
mean loss: 246.73
 ---- batch: 110 ----
mean loss: 256.37
train mean loss: 244.09
epoch train time: 0:00:15.427446
elapsed time: 0:17:01.043041
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 08:37:33.688471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.27
 ---- batch: 020 ----
mean loss: 244.96
 ---- batch: 030 ----
mean loss: 239.92
 ---- batch: 040 ----
mean loss: 226.48
 ---- batch: 050 ----
mean loss: 243.38
 ---- batch: 060 ----
mean loss: 245.86
 ---- batch: 070 ----
mean loss: 245.87
 ---- batch: 080 ----
mean loss: 244.03
 ---- batch: 090 ----
mean loss: 248.97
 ---- batch: 100 ----
mean loss: 242.01
 ---- batch: 110 ----
mean loss: 234.80
train mean loss: 242.13
epoch train time: 0:00:15.529456
elapsed time: 0:17:16.573432
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 08:37:49.218829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.98
 ---- batch: 020 ----
mean loss: 230.20
 ---- batch: 030 ----
mean loss: 249.72
 ---- batch: 040 ----
mean loss: 249.24
 ---- batch: 050 ----
mean loss: 247.13
 ---- batch: 060 ----
mean loss: 242.97
 ---- batch: 070 ----
mean loss: 246.74
 ---- batch: 080 ----
mean loss: 233.95
 ---- batch: 090 ----
mean loss: 234.88
 ---- batch: 100 ----
mean loss: 236.34
 ---- batch: 110 ----
mean loss: 240.93
train mean loss: 242.11
epoch train time: 0:00:15.430012
elapsed time: 0:17:32.004330
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 08:38:04.649789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.33
 ---- batch: 020 ----
mean loss: 239.68
 ---- batch: 030 ----
mean loss: 236.44
 ---- batch: 040 ----
mean loss: 239.45
 ---- batch: 050 ----
mean loss: 230.79
 ---- batch: 060 ----
mean loss: 243.29
 ---- batch: 070 ----
mean loss: 235.09
 ---- batch: 080 ----
mean loss: 244.29
 ---- batch: 090 ----
mean loss: 238.85
 ---- batch: 100 ----
mean loss: 244.12
 ---- batch: 110 ----
mean loss: 240.68
train mean loss: 239.65
epoch train time: 0:00:15.414452
elapsed time: 0:17:47.419731
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 08:38:20.065182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.70
 ---- batch: 020 ----
mean loss: 237.43
 ---- batch: 030 ----
mean loss: 228.83
 ---- batch: 040 ----
mean loss: 239.66
 ---- batch: 050 ----
mean loss: 246.58
 ---- batch: 060 ----
mean loss: 227.09
 ---- batch: 070 ----
mean loss: 240.29
 ---- batch: 080 ----
mean loss: 232.63
 ---- batch: 090 ----
mean loss: 242.29
 ---- batch: 100 ----
mean loss: 236.08
 ---- batch: 110 ----
mean loss: 239.13
train mean loss: 237.08
epoch train time: 0:00:15.432387
elapsed time: 0:18:02.853003
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 08:38:35.498552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.19
 ---- batch: 020 ----
mean loss: 238.81
 ---- batch: 030 ----
mean loss: 231.09
 ---- batch: 040 ----
mean loss: 242.89
 ---- batch: 050 ----
mean loss: 238.70
 ---- batch: 060 ----
mean loss: 244.46
 ---- batch: 070 ----
mean loss: 237.86
 ---- batch: 080 ----
mean loss: 230.68
 ---- batch: 090 ----
mean loss: 244.73
 ---- batch: 100 ----
mean loss: 229.44
 ---- batch: 110 ----
mean loss: 237.65
train mean loss: 237.69
epoch train time: 0:00:15.534262
elapsed time: 0:18:18.388250
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 08:38:51.033689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.10
 ---- batch: 020 ----
mean loss: 236.03
 ---- batch: 030 ----
mean loss: 240.71
 ---- batch: 040 ----
mean loss: 227.74
 ---- batch: 050 ----
mean loss: 236.81
 ---- batch: 060 ----
mean loss: 239.84
 ---- batch: 070 ----
mean loss: 233.76
 ---- batch: 080 ----
mean loss: 234.00
 ---- batch: 090 ----
mean loss: 234.88
 ---- batch: 100 ----
mean loss: 233.90
 ---- batch: 110 ----
mean loss: 239.80
train mean loss: 236.45
epoch train time: 0:00:15.461792
elapsed time: 0:18:33.850883
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 08:39:06.496324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.33
 ---- batch: 020 ----
mean loss: 233.86
 ---- batch: 030 ----
mean loss: 239.90
 ---- batch: 040 ----
mean loss: 235.38
 ---- batch: 050 ----
mean loss: 223.32
 ---- batch: 060 ----
mean loss: 226.70
 ---- batch: 070 ----
mean loss: 233.11
 ---- batch: 080 ----
mean loss: 234.89
 ---- batch: 090 ----
mean loss: 235.98
 ---- batch: 100 ----
mean loss: 235.15
 ---- batch: 110 ----
mean loss: 238.30
train mean loss: 233.40
epoch train time: 0:00:15.439608
elapsed time: 0:18:49.291413
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 08:39:21.936839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.87
 ---- batch: 020 ----
mean loss: 227.42
 ---- batch: 030 ----
mean loss: 237.01
 ---- batch: 040 ----
mean loss: 231.53
 ---- batch: 050 ----
mean loss: 216.94
 ---- batch: 060 ----
mean loss: 233.78
 ---- batch: 070 ----
mean loss: 230.59
 ---- batch: 080 ----
mean loss: 236.84
 ---- batch: 090 ----
mean loss: 228.91
 ---- batch: 100 ----
mean loss: 227.98
 ---- batch: 110 ----
mean loss: 237.93
train mean loss: 231.16
epoch train time: 0:00:15.422980
elapsed time: 0:19:04.715304
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 08:39:37.360735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.32
 ---- batch: 020 ----
mean loss: 227.89
 ---- batch: 030 ----
mean loss: 230.15
 ---- batch: 040 ----
mean loss: 231.00
 ---- batch: 050 ----
mean loss: 235.63
 ---- batch: 060 ----
mean loss: 226.82
 ---- batch: 070 ----
mean loss: 221.32
 ---- batch: 080 ----
mean loss: 226.48
 ---- batch: 090 ----
mean loss: 224.69
 ---- batch: 100 ----
mean loss: 234.62
 ---- batch: 110 ----
mean loss: 225.70
train mean loss: 228.18
epoch train time: 0:00:15.419167
elapsed time: 0:19:20.135452
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 08:39:52.780900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.53
 ---- batch: 020 ----
mean loss: 234.33
 ---- batch: 030 ----
mean loss: 229.07
 ---- batch: 040 ----
mean loss: 224.11
 ---- batch: 050 ----
mean loss: 222.64
 ---- batch: 060 ----
mean loss: 235.84
 ---- batch: 070 ----
mean loss: 224.28
 ---- batch: 080 ----
mean loss: 234.72
 ---- batch: 090 ----
mean loss: 231.08
 ---- batch: 100 ----
mean loss: 228.26
 ---- batch: 110 ----
mean loss: 223.32
train mean loss: 228.72
epoch train time: 0:00:15.453600
elapsed time: 0:19:35.590010
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 08:40:08.235439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.40
 ---- batch: 020 ----
mean loss: 224.87
 ---- batch: 030 ----
mean loss: 231.21
 ---- batch: 040 ----
mean loss: 227.16
 ---- batch: 050 ----
mean loss: 220.65
 ---- batch: 060 ----
mean loss: 232.91
 ---- batch: 070 ----
mean loss: 232.86
 ---- batch: 080 ----
mean loss: 232.22
 ---- batch: 090 ----
mean loss: 225.29
 ---- batch: 100 ----
mean loss: 227.37
 ---- batch: 110 ----
mean loss: 228.28
train mean loss: 228.78
epoch train time: 0:00:15.404694
elapsed time: 0:19:50.995702
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 08:40:23.641305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.71
 ---- batch: 020 ----
mean loss: 221.46
 ---- batch: 030 ----
mean loss: 233.86
 ---- batch: 040 ----
mean loss: 227.76
 ---- batch: 050 ----
mean loss: 233.29
 ---- batch: 060 ----
mean loss: 222.83
 ---- batch: 070 ----
mean loss: 221.46
 ---- batch: 080 ----
mean loss: 223.17
 ---- batch: 090 ----
mean loss: 223.24
 ---- batch: 100 ----
mean loss: 224.33
 ---- batch: 110 ----
mean loss: 228.93
train mean loss: 225.93
epoch train time: 0:00:15.400490
elapsed time: 0:20:06.397295
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 08:40:39.042721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.07
 ---- batch: 020 ----
mean loss: 221.85
 ---- batch: 030 ----
mean loss: 228.56
 ---- batch: 040 ----
mean loss: 227.99
 ---- batch: 050 ----
mean loss: 228.98
 ---- batch: 060 ----
mean loss: 228.04
 ---- batch: 070 ----
mean loss: 226.45
 ---- batch: 080 ----
mean loss: 223.43
 ---- batch: 090 ----
mean loss: 218.29
 ---- batch: 100 ----
mean loss: 213.94
 ---- batch: 110 ----
mean loss: 231.20
train mean loss: 224.90
epoch train time: 0:00:15.465969
elapsed time: 0:20:21.864191
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 08:40:54.509641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.78
 ---- batch: 020 ----
mean loss: 228.39
 ---- batch: 030 ----
mean loss: 216.97
 ---- batch: 040 ----
mean loss: 222.47
 ---- batch: 050 ----
mean loss: 226.09
 ---- batch: 060 ----
mean loss: 227.38
 ---- batch: 070 ----
mean loss: 217.95
 ---- batch: 080 ----
mean loss: 222.07
 ---- batch: 090 ----
mean loss: 229.79
 ---- batch: 100 ----
mean loss: 231.51
 ---- batch: 110 ----
mean loss: 222.58
train mean loss: 224.30
epoch train time: 0:00:15.405605
elapsed time: 0:20:37.270807
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 08:41:09.916228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.77
 ---- batch: 020 ----
mean loss: 217.70
 ---- batch: 030 ----
mean loss: 213.65
 ---- batch: 040 ----
mean loss: 225.15
 ---- batch: 050 ----
mean loss: 227.00
 ---- batch: 060 ----
mean loss: 233.86
 ---- batch: 070 ----
mean loss: 233.23
 ---- batch: 080 ----
mean loss: 228.27
 ---- batch: 090 ----
mean loss: 220.08
 ---- batch: 100 ----
mean loss: 215.24
 ---- batch: 110 ----
mean loss: 223.19
train mean loss: 223.12
epoch train time: 0:00:15.394883
elapsed time: 0:20:52.666658
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 08:41:25.312095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.92
 ---- batch: 020 ----
mean loss: 223.74
 ---- batch: 030 ----
mean loss: 215.78
 ---- batch: 040 ----
mean loss: 224.83
 ---- batch: 050 ----
mean loss: 224.20
 ---- batch: 060 ----
mean loss: 224.78
 ---- batch: 070 ----
mean loss: 228.68
 ---- batch: 080 ----
mean loss: 215.50
 ---- batch: 090 ----
mean loss: 226.74
 ---- batch: 100 ----
mean loss: 212.55
 ---- batch: 110 ----
mean loss: 232.58
train mean loss: 222.15
epoch train time: 0:00:15.374283
elapsed time: 0:21:08.041914
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 08:41:40.687333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.23
 ---- batch: 020 ----
mean loss: 224.39
 ---- batch: 030 ----
mean loss: 216.73
 ---- batch: 040 ----
mean loss: 220.58
 ---- batch: 050 ----
mean loss: 224.19
 ---- batch: 060 ----
mean loss: 222.30
 ---- batch: 070 ----
mean loss: 231.00
 ---- batch: 080 ----
mean loss: 216.21
 ---- batch: 090 ----
mean loss: 222.18
 ---- batch: 100 ----
mean loss: 220.32
 ---- batch: 110 ----
mean loss: 217.00
train mean loss: 220.87
epoch train time: 0:00:15.538004
elapsed time: 0:21:23.580855
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 08:41:56.226294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.15
 ---- batch: 020 ----
mean loss: 216.88
 ---- batch: 030 ----
mean loss: 212.20
 ---- batch: 040 ----
mean loss: 220.46
 ---- batch: 050 ----
mean loss: 220.71
 ---- batch: 060 ----
mean loss: 213.31
 ---- batch: 070 ----
mean loss: 214.90
 ---- batch: 080 ----
mean loss: 238.21
 ---- batch: 090 ----
mean loss: 223.93
 ---- batch: 100 ----
mean loss: 211.47
 ---- batch: 110 ----
mean loss: 228.10
train mean loss: 219.53
epoch train time: 0:00:15.371923
elapsed time: 0:21:38.953579
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 08:42:11.599006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.82
 ---- batch: 020 ----
mean loss: 224.09
 ---- batch: 030 ----
mean loss: 224.71
 ---- batch: 040 ----
mean loss: 220.16
 ---- batch: 050 ----
mean loss: 213.55
 ---- batch: 060 ----
mean loss: 220.01
 ---- batch: 070 ----
mean loss: 220.10
 ---- batch: 080 ----
mean loss: 213.75
 ---- batch: 090 ----
mean loss: 213.15
 ---- batch: 100 ----
mean loss: 215.80
 ---- batch: 110 ----
mean loss: 214.96
train mean loss: 217.86
epoch train time: 0:00:15.364944
elapsed time: 0:21:54.319476
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 08:42:26.965003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.32
 ---- batch: 020 ----
mean loss: 219.56
 ---- batch: 030 ----
mean loss: 217.65
 ---- batch: 040 ----
mean loss: 210.21
 ---- batch: 050 ----
mean loss: 215.57
 ---- batch: 060 ----
mean loss: 216.65
 ---- batch: 070 ----
mean loss: 212.87
 ---- batch: 080 ----
mean loss: 219.32
 ---- batch: 090 ----
mean loss: 217.22
 ---- batch: 100 ----
mean loss: 225.09
 ---- batch: 110 ----
mean loss: 220.06
train mean loss: 217.08
epoch train time: 0:00:15.348523
elapsed time: 0:22:09.668961
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 08:42:42.314366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.26
 ---- batch: 020 ----
mean loss: 218.90
 ---- batch: 030 ----
mean loss: 212.53
 ---- batch: 040 ----
mean loss: 212.86
 ---- batch: 050 ----
mean loss: 207.63
 ---- batch: 060 ----
mean loss: 214.54
 ---- batch: 070 ----
mean loss: 221.24
 ---- batch: 080 ----
mean loss: 220.89
 ---- batch: 090 ----
mean loss: 215.48
 ---- batch: 100 ----
mean loss: 219.24
 ---- batch: 110 ----
mean loss: 209.65
train mean loss: 215.04
epoch train time: 0:00:15.325690
elapsed time: 0:22:24.995601
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 08:42:57.641080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.89
 ---- batch: 020 ----
mean loss: 218.09
 ---- batch: 030 ----
mean loss: 204.46
 ---- batch: 040 ----
mean loss: 221.05
 ---- batch: 050 ----
mean loss: 214.92
 ---- batch: 060 ----
mean loss: 213.45
 ---- batch: 070 ----
mean loss: 215.23
 ---- batch: 080 ----
mean loss: 220.26
 ---- batch: 090 ----
mean loss: 218.89
 ---- batch: 100 ----
mean loss: 211.38
 ---- batch: 110 ----
mean loss: 215.50
train mean loss: 214.36
epoch train time: 0:00:15.306646
elapsed time: 0:22:40.303250
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 08:43:12.948692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.48
 ---- batch: 020 ----
mean loss: 224.21
 ---- batch: 030 ----
mean loss: 208.15
 ---- batch: 040 ----
mean loss: 209.72
 ---- batch: 050 ----
mean loss: 213.32
 ---- batch: 060 ----
mean loss: 212.76
 ---- batch: 070 ----
mean loss: 204.16
 ---- batch: 080 ----
mean loss: 210.58
 ---- batch: 090 ----
mean loss: 216.81
 ---- batch: 100 ----
mean loss: 219.00
 ---- batch: 110 ----
mean loss: 218.93
train mean loss: 213.40
epoch train time: 0:00:15.353619
elapsed time: 0:22:55.657830
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 08:43:28.303225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.75
 ---- batch: 020 ----
mean loss: 214.44
 ---- batch: 030 ----
mean loss: 212.39
 ---- batch: 040 ----
mean loss: 205.28
 ---- batch: 050 ----
mean loss: 212.18
 ---- batch: 060 ----
mean loss: 216.49
 ---- batch: 070 ----
mean loss: 214.30
 ---- batch: 080 ----
mean loss: 229.66
 ---- batch: 090 ----
mean loss: 213.91
 ---- batch: 100 ----
mean loss: 201.84
 ---- batch: 110 ----
mean loss: 216.43
train mean loss: 213.59
epoch train time: 0:00:15.375749
elapsed time: 0:23:11.034535
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 08:43:43.679971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.45
 ---- batch: 020 ----
mean loss: 205.72
 ---- batch: 030 ----
mean loss: 216.61
 ---- batch: 040 ----
mean loss: 213.18
 ---- batch: 050 ----
mean loss: 214.55
 ---- batch: 060 ----
mean loss: 211.62
 ---- batch: 070 ----
mean loss: 213.23
 ---- batch: 080 ----
mean loss: 213.84
 ---- batch: 090 ----
mean loss: 220.16
 ---- batch: 100 ----
mean loss: 214.97
 ---- batch: 110 ----
mean loss: 211.95
train mean loss: 212.45
epoch train time: 0:00:15.495867
elapsed time: 0:23:26.531350
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 08:43:59.176883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.12
 ---- batch: 020 ----
mean loss: 214.83
 ---- batch: 030 ----
mean loss: 210.51
 ---- batch: 040 ----
mean loss: 217.87
 ---- batch: 050 ----
mean loss: 203.55
 ---- batch: 060 ----
mean loss: 203.29
 ---- batch: 070 ----
mean loss: 213.01
 ---- batch: 080 ----
mean loss: 211.62
 ---- batch: 090 ----
mean loss: 209.61
 ---- batch: 100 ----
mean loss: 205.47
 ---- batch: 110 ----
mean loss: 208.18
train mean loss: 209.94
epoch train time: 0:00:15.429805
elapsed time: 0:23:41.962109
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 08:44:14.607507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.57
 ---- batch: 020 ----
mean loss: 206.65
 ---- batch: 030 ----
mean loss: 202.02
 ---- batch: 040 ----
mean loss: 210.27
 ---- batch: 050 ----
mean loss: 201.16
 ---- batch: 060 ----
mean loss: 217.69
 ---- batch: 070 ----
mean loss: 218.62
 ---- batch: 080 ----
mean loss: 212.07
 ---- batch: 090 ----
mean loss: 206.45
 ---- batch: 100 ----
mean loss: 211.49
 ---- batch: 110 ----
mean loss: 210.03
train mean loss: 209.61
epoch train time: 0:00:15.356193
elapsed time: 0:23:57.319323
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 08:44:29.964769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.57
 ---- batch: 020 ----
mean loss: 212.89
 ---- batch: 030 ----
mean loss: 212.06
 ---- batch: 040 ----
mean loss: 209.17
 ---- batch: 050 ----
mean loss: 205.51
 ---- batch: 060 ----
mean loss: 213.29
 ---- batch: 070 ----
mean loss: 212.24
 ---- batch: 080 ----
mean loss: 204.09
 ---- batch: 090 ----
mean loss: 209.67
 ---- batch: 100 ----
mean loss: 205.20
 ---- batch: 110 ----
mean loss: 213.43
train mean loss: 209.56
epoch train time: 0:00:15.324141
elapsed time: 0:24:12.644432
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 08:44:45.289895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.21
 ---- batch: 020 ----
mean loss: 212.22
 ---- batch: 030 ----
mean loss: 204.41
 ---- batch: 040 ----
mean loss: 217.53
 ---- batch: 050 ----
mean loss: 199.02
 ---- batch: 060 ----
mean loss: 203.92
 ---- batch: 070 ----
mean loss: 212.93
 ---- batch: 080 ----
mean loss: 213.19
 ---- batch: 090 ----
mean loss: 205.89
 ---- batch: 100 ----
mean loss: 205.02
 ---- batch: 110 ----
mean loss: 208.27
train mean loss: 208.18
epoch train time: 0:00:15.421814
elapsed time: 0:24:28.067270
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 08:45:00.712705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.86
 ---- batch: 020 ----
mean loss: 215.71
 ---- batch: 030 ----
mean loss: 211.50
 ---- batch: 040 ----
mean loss: 213.64
 ---- batch: 050 ----
mean loss: 208.39
 ---- batch: 060 ----
mean loss: 199.65
 ---- batch: 070 ----
mean loss: 204.28
 ---- batch: 080 ----
mean loss: 193.14
 ---- batch: 090 ----
mean loss: 211.76
 ---- batch: 100 ----
mean loss: 204.82
 ---- batch: 110 ----
mean loss: 206.87
train mean loss: 206.91
epoch train time: 0:00:15.366817
elapsed time: 0:24:43.434983
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 08:45:16.080368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.98
 ---- batch: 020 ----
mean loss: 212.29
 ---- batch: 030 ----
mean loss: 209.51
 ---- batch: 040 ----
mean loss: 200.96
 ---- batch: 050 ----
mean loss: 199.25
 ---- batch: 060 ----
mean loss: 213.41
 ---- batch: 070 ----
mean loss: 217.04
 ---- batch: 080 ----
mean loss: 209.63
 ---- batch: 090 ----
mean loss: 199.41
 ---- batch: 100 ----
mean loss: 204.40
 ---- batch: 110 ----
mean loss: 198.03
train mean loss: 206.19
epoch train time: 0:00:15.327136
elapsed time: 0:24:58.762857
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 08:45:31.408277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.56
 ---- batch: 020 ----
mean loss: 206.14
 ---- batch: 030 ----
mean loss: 208.86
 ---- batch: 040 ----
mean loss: 200.16
 ---- batch: 050 ----
mean loss: 204.31
 ---- batch: 060 ----
mean loss: 198.95
 ---- batch: 070 ----
mean loss: 205.19
 ---- batch: 080 ----
mean loss: 208.97
 ---- batch: 090 ----
mean loss: 203.51
 ---- batch: 100 ----
mean loss: 205.56
 ---- batch: 110 ----
mean loss: 199.20
train mean loss: 203.90
epoch train time: 0:00:15.285251
elapsed time: 0:25:14.049030
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 08:45:46.694443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.45
 ---- batch: 020 ----
mean loss: 205.92
 ---- batch: 030 ----
mean loss: 209.11
 ---- batch: 040 ----
mean loss: 215.26
 ---- batch: 050 ----
mean loss: 212.14
 ---- batch: 060 ----
mean loss: 197.17
 ---- batch: 070 ----
mean loss: 194.95
 ---- batch: 080 ----
mean loss: 196.88
 ---- batch: 090 ----
mean loss: 209.20
 ---- batch: 100 ----
mean loss: 206.17
 ---- batch: 110 ----
mean loss: 203.83
train mean loss: 205.72
epoch train time: 0:00:15.393345
elapsed time: 0:25:29.443279
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 08:46:02.088757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.90
 ---- batch: 020 ----
mean loss: 195.47
 ---- batch: 030 ----
mean loss: 190.16
 ---- batch: 040 ----
mean loss: 211.57
 ---- batch: 050 ----
mean loss: 211.38
 ---- batch: 060 ----
mean loss: 209.71
 ---- batch: 070 ----
mean loss: 209.64
 ---- batch: 080 ----
mean loss: 202.73
 ---- batch: 090 ----
mean loss: 199.26
 ---- batch: 100 ----
mean loss: 201.65
 ---- batch: 110 ----
mean loss: 207.47
train mean loss: 203.62
epoch train time: 0:00:15.320512
elapsed time: 0:25:44.764797
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 08:46:17.410253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.96
 ---- batch: 020 ----
mean loss: 201.91
 ---- batch: 030 ----
mean loss: 203.84
 ---- batch: 040 ----
mean loss: 205.24
 ---- batch: 050 ----
mean loss: 213.59
 ---- batch: 060 ----
mean loss: 193.99
 ---- batch: 070 ----
mean loss: 199.31
 ---- batch: 080 ----
mean loss: 208.50
 ---- batch: 090 ----
mean loss: 205.74
 ---- batch: 100 ----
mean loss: 208.70
 ---- batch: 110 ----
mean loss: 199.43
train mean loss: 203.18
epoch train time: 0:00:15.320902
elapsed time: 0:26:00.086688
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 08:46:32.732210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.99
 ---- batch: 020 ----
mean loss: 204.52
 ---- batch: 030 ----
mean loss: 202.37
 ---- batch: 040 ----
mean loss: 196.08
 ---- batch: 050 ----
mean loss: 206.06
 ---- batch: 060 ----
mean loss: 208.31
 ---- batch: 070 ----
mean loss: 204.57
 ---- batch: 080 ----
mean loss: 210.56
 ---- batch: 090 ----
mean loss: 206.78
 ---- batch: 100 ----
mean loss: 206.06
 ---- batch: 110 ----
mean loss: 200.64
train mean loss: 204.07
epoch train time: 0:00:15.289622
elapsed time: 0:26:15.377203
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 08:46:48.022614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.18
 ---- batch: 020 ----
mean loss: 206.88
 ---- batch: 030 ----
mean loss: 189.65
 ---- batch: 040 ----
mean loss: 205.69
 ---- batch: 050 ----
mean loss: 199.36
 ---- batch: 060 ----
mean loss: 206.81
 ---- batch: 070 ----
mean loss: 196.02
 ---- batch: 080 ----
mean loss: 190.12
 ---- batch: 090 ----
mean loss: 202.72
 ---- batch: 100 ----
mean loss: 202.02
 ---- batch: 110 ----
mean loss: 206.80
train mean loss: 201.46
epoch train time: 0:00:15.294106
elapsed time: 0:26:30.672230
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 08:47:03.317675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.01
 ---- batch: 020 ----
mean loss: 201.08
 ---- batch: 030 ----
mean loss: 209.59
 ---- batch: 040 ----
mean loss: 198.52
 ---- batch: 050 ----
mean loss: 201.30
 ---- batch: 060 ----
mean loss: 204.14
 ---- batch: 070 ----
mean loss: 197.50
 ---- batch: 080 ----
mean loss: 199.08
 ---- batch: 090 ----
mean loss: 193.86
 ---- batch: 100 ----
mean loss: 204.27
 ---- batch: 110 ----
mean loss: 201.95
train mean loss: 200.29
epoch train time: 0:00:15.279381
elapsed time: 0:26:45.952447
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 08:47:18.597881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.39
 ---- batch: 020 ----
mean loss: 202.76
 ---- batch: 030 ----
mean loss: 203.80
 ---- batch: 040 ----
mean loss: 190.31
 ---- batch: 050 ----
mean loss: 206.37
 ---- batch: 060 ----
mean loss: 196.97
 ---- batch: 070 ----
mean loss: 200.13
 ---- batch: 080 ----
mean loss: 200.53
 ---- batch: 090 ----
mean loss: 196.60
 ---- batch: 100 ----
mean loss: 191.42
 ---- batch: 110 ----
mean loss: 201.47
train mean loss: 199.54
epoch train time: 0:00:15.284988
elapsed time: 0:27:01.238263
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 08:47:33.883689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.42
 ---- batch: 020 ----
mean loss: 201.32
 ---- batch: 030 ----
mean loss: 201.29
 ---- batch: 040 ----
mean loss: 201.49
 ---- batch: 050 ----
mean loss: 197.48
 ---- batch: 060 ----
mean loss: 204.70
 ---- batch: 070 ----
mean loss: 194.55
 ---- batch: 080 ----
mean loss: 203.63
 ---- batch: 090 ----
mean loss: 191.86
 ---- batch: 100 ----
mean loss: 196.65
 ---- batch: 110 ----
mean loss: 195.18
train mean loss: 199.20
epoch train time: 0:00:15.289584
elapsed time: 0:27:16.528786
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 08:47:49.174220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.14
 ---- batch: 020 ----
mean loss: 195.73
 ---- batch: 030 ----
mean loss: 206.43
 ---- batch: 040 ----
mean loss: 195.61
 ---- batch: 050 ----
mean loss: 193.69
 ---- batch: 060 ----
mean loss: 204.03
 ---- batch: 070 ----
mean loss: 195.12
 ---- batch: 080 ----
mean loss: 195.70
 ---- batch: 090 ----
mean loss: 192.25
 ---- batch: 100 ----
mean loss: 198.04
 ---- batch: 110 ----
mean loss: 205.13
train mean loss: 198.81
epoch train time: 0:00:15.346239
elapsed time: 0:27:31.875942
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 08:48:04.521383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.87
 ---- batch: 020 ----
mean loss: 198.47
 ---- batch: 030 ----
mean loss: 198.33
 ---- batch: 040 ----
mean loss: 194.25
 ---- batch: 050 ----
mean loss: 198.50
 ---- batch: 060 ----
mean loss: 195.50
 ---- batch: 070 ----
mean loss: 195.96
 ---- batch: 080 ----
mean loss: 194.94
 ---- batch: 090 ----
mean loss: 196.90
 ---- batch: 100 ----
mean loss: 205.97
 ---- batch: 110 ----
mean loss: 210.80
train mean loss: 197.97
epoch train time: 0:00:15.269108
elapsed time: 0:27:47.145978
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 08:48:19.791505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.39
 ---- batch: 020 ----
mean loss: 195.47
 ---- batch: 030 ----
mean loss: 201.84
 ---- batch: 040 ----
mean loss: 195.62
 ---- batch: 050 ----
mean loss: 190.92
 ---- batch: 060 ----
mean loss: 202.91
 ---- batch: 070 ----
mean loss: 199.27
 ---- batch: 080 ----
mean loss: 192.36
 ---- batch: 090 ----
mean loss: 195.87
 ---- batch: 100 ----
mean loss: 200.27
 ---- batch: 110 ----
mean loss: 194.67
train mean loss: 197.38
epoch train time: 0:00:15.290610
elapsed time: 0:28:02.437843
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 08:48:35.083255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.21
 ---- batch: 020 ----
mean loss: 209.20
 ---- batch: 030 ----
mean loss: 193.46
 ---- batch: 040 ----
mean loss: 199.70
 ---- batch: 050 ----
mean loss: 202.30
 ---- batch: 060 ----
mean loss: 199.47
 ---- batch: 070 ----
mean loss: 192.30
 ---- batch: 080 ----
mean loss: 203.72
 ---- batch: 090 ----
mean loss: 207.30
 ---- batch: 100 ----
mean loss: 190.45
 ---- batch: 110 ----
mean loss: 200.38
train mean loss: 199.04
epoch train time: 0:00:15.242986
elapsed time: 0:28:17.681730
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 08:48:50.327112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.22
 ---- batch: 020 ----
mean loss: 203.07
 ---- batch: 030 ----
mean loss: 201.03
 ---- batch: 040 ----
mean loss: 194.48
 ---- batch: 050 ----
mean loss: 205.14
 ---- batch: 060 ----
mean loss: 186.39
 ---- batch: 070 ----
mean loss: 202.54
 ---- batch: 080 ----
mean loss: 200.65
 ---- batch: 090 ----
mean loss: 198.78
 ---- batch: 100 ----
mean loss: 180.73
 ---- batch: 110 ----
mean loss: 197.89
train mean loss: 197.63
epoch train time: 0:00:15.275836
elapsed time: 0:28:32.958432
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 08:49:05.603831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.75
 ---- batch: 020 ----
mean loss: 201.98
 ---- batch: 030 ----
mean loss: 185.01
 ---- batch: 040 ----
mean loss: 193.72
 ---- batch: 050 ----
mean loss: 199.25
 ---- batch: 060 ----
mean loss: 197.54
 ---- batch: 070 ----
mean loss: 196.60
 ---- batch: 080 ----
mean loss: 200.20
 ---- batch: 090 ----
mean loss: 205.11
 ---- batch: 100 ----
mean loss: 201.31
 ---- batch: 110 ----
mean loss: 196.15
train mean loss: 197.25
epoch train time: 0:00:15.300896
elapsed time: 0:28:48.260269
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 08:49:20.905691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.71
 ---- batch: 020 ----
mean loss: 190.72
 ---- batch: 030 ----
mean loss: 196.10
 ---- batch: 040 ----
mean loss: 192.83
 ---- batch: 050 ----
mean loss: 190.99
 ---- batch: 060 ----
mean loss: 196.23
 ---- batch: 070 ----
mean loss: 203.31
 ---- batch: 080 ----
mean loss: 196.06
 ---- batch: 090 ----
mean loss: 188.00
 ---- batch: 100 ----
mean loss: 194.71
 ---- batch: 110 ----
mean loss: 199.63
train mean loss: 194.83
epoch train time: 0:00:15.298626
elapsed time: 0:29:03.559845
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 08:49:36.205268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.56
 ---- batch: 020 ----
mean loss: 195.85
 ---- batch: 030 ----
mean loss: 191.91
 ---- batch: 040 ----
mean loss: 184.32
 ---- batch: 050 ----
mean loss: 202.00
 ---- batch: 060 ----
mean loss: 192.00
 ---- batch: 070 ----
mean loss: 194.91
 ---- batch: 080 ----
mean loss: 201.81
 ---- batch: 090 ----
mean loss: 190.58
 ---- batch: 100 ----
mean loss: 191.16
 ---- batch: 110 ----
mean loss: 201.67
train mean loss: 194.90
epoch train time: 0:00:15.257579
elapsed time: 0:29:18.818540
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 08:49:51.464044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.17
 ---- batch: 020 ----
mean loss: 189.07
 ---- batch: 030 ----
mean loss: 194.69
 ---- batch: 040 ----
mean loss: 197.46
 ---- batch: 050 ----
mean loss: 192.38
 ---- batch: 060 ----
mean loss: 197.55
 ---- batch: 070 ----
mean loss: 200.18
 ---- batch: 080 ----
mean loss: 198.13
 ---- batch: 090 ----
mean loss: 200.69
 ---- batch: 100 ----
mean loss: 188.65
 ---- batch: 110 ----
mean loss: 195.33
train mean loss: 195.61
epoch train time: 0:00:15.255020
elapsed time: 0:29:34.074555
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 08:50:06.719996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.56
 ---- batch: 020 ----
mean loss: 186.71
 ---- batch: 030 ----
mean loss: 194.31
 ---- batch: 040 ----
mean loss: 185.67
 ---- batch: 050 ----
mean loss: 194.69
 ---- batch: 060 ----
mean loss: 198.43
 ---- batch: 070 ----
mean loss: 187.51
 ---- batch: 080 ----
mean loss: 200.01
 ---- batch: 090 ----
mean loss: 197.26
 ---- batch: 100 ----
mean loss: 193.42
 ---- batch: 110 ----
mean loss: 200.35
train mean loss: 194.03
epoch train time: 0:00:15.258675
elapsed time: 0:29:49.334362
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 08:50:21.979840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.12
 ---- batch: 020 ----
mean loss: 203.26
 ---- batch: 030 ----
mean loss: 184.83
 ---- batch: 040 ----
mean loss: 198.07
 ---- batch: 050 ----
mean loss: 206.20
 ---- batch: 060 ----
mean loss: 208.73
 ---- batch: 070 ----
mean loss: 194.58
 ---- batch: 080 ----
mean loss: 185.48
 ---- batch: 090 ----
mean loss: 192.26
 ---- batch: 100 ----
mean loss: 190.93
 ---- batch: 110 ----
mean loss: 193.08
train mean loss: 195.11
epoch train time: 0:00:15.241200
elapsed time: 0:30:04.576544
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 08:50:37.222011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.80
 ---- batch: 020 ----
mean loss: 201.13
 ---- batch: 030 ----
mean loss: 191.42
 ---- batch: 040 ----
mean loss: 188.00
 ---- batch: 050 ----
mean loss: 193.39
 ---- batch: 060 ----
mean loss: 190.25
 ---- batch: 070 ----
mean loss: 189.05
 ---- batch: 080 ----
mean loss: 204.57
 ---- batch: 090 ----
mean loss: 197.42
 ---- batch: 100 ----
mean loss: 180.72
 ---- batch: 110 ----
mean loss: 186.44
train mean loss: 192.93
epoch train time: 0:00:15.209324
elapsed time: 0:30:19.786820
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 08:50:52.432259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.73
 ---- batch: 020 ----
mean loss: 191.86
 ---- batch: 030 ----
mean loss: 191.97
 ---- batch: 040 ----
mean loss: 190.86
 ---- batch: 050 ----
mean loss: 189.30
 ---- batch: 060 ----
mean loss: 192.14
 ---- batch: 070 ----
mean loss: 208.46
 ---- batch: 080 ----
mean loss: 201.15
 ---- batch: 090 ----
mean loss: 190.11
 ---- batch: 100 ----
mean loss: 200.76
 ---- batch: 110 ----
mean loss: 187.91
train mean loss: 193.86
epoch train time: 0:00:15.289983
elapsed time: 0:30:35.077777
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 08:51:07.723246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.71
 ---- batch: 020 ----
mean loss: 193.62
 ---- batch: 030 ----
mean loss: 189.96
 ---- batch: 040 ----
mean loss: 190.43
 ---- batch: 050 ----
mean loss: 188.27
 ---- batch: 060 ----
mean loss: 197.72
 ---- batch: 070 ----
mean loss: 195.11
 ---- batch: 080 ----
mean loss: 199.63
 ---- batch: 090 ----
mean loss: 185.95
 ---- batch: 100 ----
mean loss: 193.06
 ---- batch: 110 ----
mean loss: 193.79
train mean loss: 192.95
epoch train time: 0:00:15.283806
elapsed time: 0:30:50.362535
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 08:51:23.007929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.95
 ---- batch: 020 ----
mean loss: 190.55
 ---- batch: 030 ----
mean loss: 190.32
 ---- batch: 040 ----
mean loss: 187.53
 ---- batch: 050 ----
mean loss: 191.60
 ---- batch: 060 ----
mean loss: 189.92
 ---- batch: 070 ----
mean loss: 189.29
 ---- batch: 080 ----
mean loss: 199.25
 ---- batch: 090 ----
mean loss: 201.16
 ---- batch: 100 ----
mean loss: 198.73
 ---- batch: 110 ----
mean loss: 189.65
train mean loss: 192.35
epoch train time: 0:00:15.260024
elapsed time: 0:31:05.623446
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 08:51:38.268878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.51
 ---- batch: 020 ----
mean loss: 189.79
 ---- batch: 030 ----
mean loss: 188.11
 ---- batch: 040 ----
mean loss: 188.73
 ---- batch: 050 ----
mean loss: 195.08
 ---- batch: 060 ----
mean loss: 199.85
 ---- batch: 070 ----
mean loss: 188.88
 ---- batch: 080 ----
mean loss: 188.39
 ---- batch: 090 ----
mean loss: 195.56
 ---- batch: 100 ----
mean loss: 190.68
 ---- batch: 110 ----
mean loss: 190.86
train mean loss: 190.88
epoch train time: 0:00:15.277309
elapsed time: 0:31:20.901669
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 08:51:53.547192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.17
 ---- batch: 020 ----
mean loss: 179.72
 ---- batch: 030 ----
mean loss: 195.21
 ---- batch: 040 ----
mean loss: 190.97
 ---- batch: 050 ----
mean loss: 187.50
 ---- batch: 060 ----
mean loss: 183.63
 ---- batch: 070 ----
mean loss: 198.54
 ---- batch: 080 ----
mean loss: 181.21
 ---- batch: 090 ----
mean loss: 196.39
 ---- batch: 100 ----
mean loss: 194.43
 ---- batch: 110 ----
mean loss: 197.98
train mean loss: 190.73
epoch train time: 0:00:15.268791
elapsed time: 0:31:36.171468
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 08:52:08.816898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.92
 ---- batch: 020 ----
mean loss: 190.01
 ---- batch: 030 ----
mean loss: 188.60
 ---- batch: 040 ----
mean loss: 190.32
 ---- batch: 050 ----
mean loss: 182.15
 ---- batch: 060 ----
mean loss: 188.82
 ---- batch: 070 ----
mean loss: 193.02
 ---- batch: 080 ----
mean loss: 193.79
 ---- batch: 090 ----
mean loss: 190.02
 ---- batch: 100 ----
mean loss: 193.10
 ---- batch: 110 ----
mean loss: 192.13
train mean loss: 190.25
epoch train time: 0:00:15.172847
elapsed time: 0:31:51.345341
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 08:52:23.990836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.95
 ---- batch: 020 ----
mean loss: 198.67
 ---- batch: 030 ----
mean loss: 185.77
 ---- batch: 040 ----
mean loss: 195.47
 ---- batch: 050 ----
mean loss: 194.86
 ---- batch: 060 ----
mean loss: 187.98
 ---- batch: 070 ----
mean loss: 192.19
 ---- batch: 080 ----
mean loss: 193.21
 ---- batch: 090 ----
mean loss: 181.89
 ---- batch: 100 ----
mean loss: 191.20
 ---- batch: 110 ----
mean loss: 183.18
train mean loss: 190.91
epoch train time: 0:00:15.196816
elapsed time: 0:32:06.543126
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 08:52:39.188520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.94
 ---- batch: 020 ----
mean loss: 188.89
 ---- batch: 030 ----
mean loss: 185.38
 ---- batch: 040 ----
mean loss: 191.72
 ---- batch: 050 ----
mean loss: 188.23
 ---- batch: 060 ----
mean loss: 190.03
 ---- batch: 070 ----
mean loss: 188.75
 ---- batch: 080 ----
mean loss: 193.14
 ---- batch: 090 ----
mean loss: 197.34
 ---- batch: 100 ----
mean loss: 192.30
 ---- batch: 110 ----
mean loss: 183.75
train mean loss: 190.53
epoch train time: 0:00:15.178247
elapsed time: 0:32:21.722143
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 08:52:54.367599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.88
 ---- batch: 020 ----
mean loss: 181.32
 ---- batch: 030 ----
mean loss: 191.26
 ---- batch: 040 ----
mean loss: 195.77
 ---- batch: 050 ----
mean loss: 196.49
 ---- batch: 060 ----
mean loss: 191.91
 ---- batch: 070 ----
mean loss: 196.59
 ---- batch: 080 ----
mean loss: 183.58
 ---- batch: 090 ----
mean loss: 189.54
 ---- batch: 100 ----
mean loss: 185.93
 ---- batch: 110 ----
mean loss: 188.58
train mean loss: 190.09
epoch train time: 0:00:15.288304
elapsed time: 0:32:37.011330
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 08:53:09.656798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.47
 ---- batch: 020 ----
mean loss: 181.59
 ---- batch: 030 ----
mean loss: 188.55
 ---- batch: 040 ----
mean loss: 199.09
 ---- batch: 050 ----
mean loss: 184.04
 ---- batch: 060 ----
mean loss: 192.81
 ---- batch: 070 ----
mean loss: 197.68
 ---- batch: 080 ----
mean loss: 195.70
 ---- batch: 090 ----
mean loss: 182.16
 ---- batch: 100 ----
mean loss: 183.42
 ---- batch: 110 ----
mean loss: 191.61
train mean loss: 189.57
epoch train time: 0:00:15.281950
elapsed time: 0:32:52.294306
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 08:53:24.939893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.51
 ---- batch: 020 ----
mean loss: 185.12
 ---- batch: 030 ----
mean loss: 189.65
 ---- batch: 040 ----
mean loss: 193.00
 ---- batch: 050 ----
mean loss: 187.71
 ---- batch: 060 ----
mean loss: 190.94
 ---- batch: 070 ----
mean loss: 187.72
 ---- batch: 080 ----
mean loss: 195.16
 ---- batch: 090 ----
mean loss: 187.94
 ---- batch: 100 ----
mean loss: 182.73
 ---- batch: 110 ----
mean loss: 185.78
train mean loss: 188.76
epoch train time: 0:00:15.273690
elapsed time: 0:33:07.569348
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 08:53:40.214769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.70
 ---- batch: 020 ----
mean loss: 191.74
 ---- batch: 030 ----
mean loss: 190.51
 ---- batch: 040 ----
mean loss: 173.68
 ---- batch: 050 ----
mean loss: 201.96
 ---- batch: 060 ----
mean loss: 188.63
 ---- batch: 070 ----
mean loss: 189.19
 ---- batch: 080 ----
mean loss: 187.22
 ---- batch: 090 ----
mean loss: 192.20
 ---- batch: 100 ----
mean loss: 183.57
 ---- batch: 110 ----
mean loss: 190.48
train mean loss: 189.37
epoch train time: 0:00:15.271538
elapsed time: 0:33:22.841825
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 08:53:55.487244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.94
 ---- batch: 020 ----
mean loss: 181.26
 ---- batch: 030 ----
mean loss: 187.75
 ---- batch: 040 ----
mean loss: 187.16
 ---- batch: 050 ----
mean loss: 194.74
 ---- batch: 060 ----
mean loss: 189.77
 ---- batch: 070 ----
mean loss: 190.12
 ---- batch: 080 ----
mean loss: 188.80
 ---- batch: 090 ----
mean loss: 178.20
 ---- batch: 100 ----
mean loss: 197.08
 ---- batch: 110 ----
mean loss: 181.88
train mean loss: 188.46
epoch train time: 0:00:15.226208
elapsed time: 0:33:38.068960
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 08:54:10.714347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.20
 ---- batch: 020 ----
mean loss: 198.89
 ---- batch: 030 ----
mean loss: 190.54
 ---- batch: 040 ----
mean loss: 189.45
 ---- batch: 050 ----
mean loss: 193.29
 ---- batch: 060 ----
mean loss: 193.64
 ---- batch: 070 ----
mean loss: 182.94
 ---- batch: 080 ----
mean loss: 184.81
 ---- batch: 090 ----
mean loss: 182.46
 ---- batch: 100 ----
mean loss: 183.03
 ---- batch: 110 ----
mean loss: 190.41
train mean loss: 188.84
epoch train time: 0:00:15.454168
elapsed time: 0:33:53.524130
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 08:54:26.169629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.55
 ---- batch: 020 ----
mean loss: 186.91
 ---- batch: 030 ----
mean loss: 187.44
 ---- batch: 040 ----
mean loss: 193.93
 ---- batch: 050 ----
mean loss: 180.96
 ---- batch: 060 ----
mean loss: 184.71
 ---- batch: 070 ----
mean loss: 193.41
 ---- batch: 080 ----
mean loss: 188.67
 ---- batch: 090 ----
mean loss: 188.35
 ---- batch: 100 ----
mean loss: 186.77
 ---- batch: 110 ----
mean loss: 183.10
train mean loss: 186.77
epoch train time: 0:00:15.336829
elapsed time: 0:34:08.861951
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 08:54:41.507408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.89
 ---- batch: 020 ----
mean loss: 188.46
 ---- batch: 030 ----
mean loss: 188.90
 ---- batch: 040 ----
mean loss: 189.44
 ---- batch: 050 ----
mean loss: 187.12
 ---- batch: 060 ----
mean loss: 182.40
 ---- batch: 070 ----
mean loss: 186.49
 ---- batch: 080 ----
mean loss: 183.49
 ---- batch: 090 ----
mean loss: 182.30
 ---- batch: 100 ----
mean loss: 193.48
 ---- batch: 110 ----
mean loss: 188.93
train mean loss: 187.07
epoch train time: 0:00:15.464939
elapsed time: 0:34:24.327843
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 08:54:56.973301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.93
 ---- batch: 020 ----
mean loss: 185.51
 ---- batch: 030 ----
mean loss: 187.80
 ---- batch: 040 ----
mean loss: 190.92
 ---- batch: 050 ----
mean loss: 184.63
 ---- batch: 060 ----
mean loss: 189.24
 ---- batch: 070 ----
mean loss: 195.45
 ---- batch: 080 ----
mean loss: 181.45
 ---- batch: 090 ----
mean loss: 179.21
 ---- batch: 100 ----
mean loss: 183.70
 ---- batch: 110 ----
mean loss: 182.20
train mean loss: 186.35
epoch train time: 0:00:15.504929
elapsed time: 0:34:39.833746
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 08:55:12.479180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.01
 ---- batch: 020 ----
mean loss: 191.28
 ---- batch: 030 ----
mean loss: 193.81
 ---- batch: 040 ----
mean loss: 187.91
 ---- batch: 050 ----
mean loss: 193.13
 ---- batch: 060 ----
mean loss: 181.22
 ---- batch: 070 ----
mean loss: 181.04
 ---- batch: 080 ----
mean loss: 181.94
 ---- batch: 090 ----
mean loss: 193.82
 ---- batch: 100 ----
mean loss: 179.34
 ---- batch: 110 ----
mean loss: 192.84
train mean loss: 189.41
epoch train time: 0:00:15.549478
elapsed time: 0:34:55.384195
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 08:55:28.029663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.91
 ---- batch: 020 ----
mean loss: 195.49
 ---- batch: 030 ----
mean loss: 194.91
 ---- batch: 040 ----
mean loss: 187.38
 ---- batch: 050 ----
mean loss: 187.44
 ---- batch: 060 ----
mean loss: 187.83
 ---- batch: 070 ----
mean loss: 184.91
 ---- batch: 080 ----
mean loss: 179.91
 ---- batch: 090 ----
mean loss: 183.37
 ---- batch: 100 ----
mean loss: 189.66
 ---- batch: 110 ----
mean loss: 183.55
train mean loss: 186.82
epoch train time: 0:00:15.564280
elapsed time: 0:35:10.949491
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 08:55:43.594941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.85
 ---- batch: 020 ----
mean loss: 196.24
 ---- batch: 030 ----
mean loss: 188.48
 ---- batch: 040 ----
mean loss: 187.55
 ---- batch: 050 ----
mean loss: 179.44
 ---- batch: 060 ----
mean loss: 190.24
 ---- batch: 070 ----
mean loss: 182.49
 ---- batch: 080 ----
mean loss: 182.03
 ---- batch: 090 ----
mean loss: 188.51
 ---- batch: 100 ----
mean loss: 179.33
 ---- batch: 110 ----
mean loss: 187.06
train mean loss: 185.79
epoch train time: 0:00:15.549764
elapsed time: 0:35:26.500227
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 08:55:59.145688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.14
 ---- batch: 020 ----
mean loss: 188.80
 ---- batch: 030 ----
mean loss: 188.07
 ---- batch: 040 ----
mean loss: 183.25
 ---- batch: 050 ----
mean loss: 178.95
 ---- batch: 060 ----
mean loss: 182.64
 ---- batch: 070 ----
mean loss: 190.14
 ---- batch: 080 ----
mean loss: 189.12
 ---- batch: 090 ----
mean loss: 193.10
 ---- batch: 100 ----
mean loss: 180.47
 ---- batch: 110 ----
mean loss: 189.01
train mean loss: 187.32
epoch train time: 0:00:15.567352
elapsed time: 0:35:42.068615
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 08:56:14.714083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.80
 ---- batch: 020 ----
mean loss: 191.46
 ---- batch: 030 ----
mean loss: 186.15
 ---- batch: 040 ----
mean loss: 187.34
 ---- batch: 050 ----
mean loss: 185.92
 ---- batch: 060 ----
mean loss: 189.85
 ---- batch: 070 ----
mean loss: 182.02
 ---- batch: 080 ----
mean loss: 185.66
 ---- batch: 090 ----
mean loss: 173.66
 ---- batch: 100 ----
mean loss: 191.98
 ---- batch: 110 ----
mean loss: 194.26
train mean loss: 185.93
epoch train time: 0:00:15.608657
elapsed time: 0:35:57.678268
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 08:56:30.323702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.42
 ---- batch: 020 ----
mean loss: 197.31
 ---- batch: 030 ----
mean loss: 177.04
 ---- batch: 040 ----
mean loss: 197.57
 ---- batch: 050 ----
mean loss: 179.18
 ---- batch: 060 ----
mean loss: 191.36
 ---- batch: 070 ----
mean loss: 176.95
 ---- batch: 080 ----
mean loss: 185.72
 ---- batch: 090 ----
mean loss: 177.81
 ---- batch: 100 ----
mean loss: 188.80
 ---- batch: 110 ----
mean loss: 182.73
train mean loss: 186.49
epoch train time: 0:00:15.540411
elapsed time: 0:36:13.219638
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 08:56:45.865082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.17
 ---- batch: 020 ----
mean loss: 187.64
 ---- batch: 030 ----
mean loss: 187.48
 ---- batch: 040 ----
mean loss: 190.95
 ---- batch: 050 ----
mean loss: 191.61
 ---- batch: 060 ----
mean loss: 194.55
 ---- batch: 070 ----
mean loss: 176.78
 ---- batch: 080 ----
mean loss: 176.17
 ---- batch: 090 ----
mean loss: 183.56
 ---- batch: 100 ----
mean loss: 173.56
 ---- batch: 110 ----
mean loss: 188.46
train mean loss: 184.74
epoch train time: 0:00:15.587685
elapsed time: 0:36:28.808262
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 08:57:01.453718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.99
 ---- batch: 020 ----
mean loss: 186.86
 ---- batch: 030 ----
mean loss: 191.74
 ---- batch: 040 ----
mean loss: 195.69
 ---- batch: 050 ----
mean loss: 189.75
 ---- batch: 060 ----
mean loss: 178.33
 ---- batch: 070 ----
mean loss: 185.86
 ---- batch: 080 ----
mean loss: 185.59
 ---- batch: 090 ----
mean loss: 189.90
 ---- batch: 100 ----
mean loss: 187.04
 ---- batch: 110 ----
mean loss: 181.07
train mean loss: 186.63
epoch train time: 0:00:15.574463
elapsed time: 0:36:44.383729
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 08:57:17.029191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.33
 ---- batch: 020 ----
mean loss: 189.83
 ---- batch: 030 ----
mean loss: 182.18
 ---- batch: 040 ----
mean loss: 183.39
 ---- batch: 050 ----
mean loss: 184.52
 ---- batch: 060 ----
mean loss: 184.50
 ---- batch: 070 ----
mean loss: 186.27
 ---- batch: 080 ----
mean loss: 185.19
 ---- batch: 090 ----
mean loss: 185.90
 ---- batch: 100 ----
mean loss: 181.94
 ---- batch: 110 ----
mean loss: 179.88
train mean loss: 185.24
epoch train time: 0:00:15.575080
elapsed time: 0:36:59.959693
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 08:57:32.605137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.47
 ---- batch: 020 ----
mean loss: 195.17
 ---- batch: 030 ----
mean loss: 182.92
 ---- batch: 040 ----
mean loss: 187.49
 ---- batch: 050 ----
mean loss: 186.20
 ---- batch: 060 ----
mean loss: 188.74
 ---- batch: 070 ----
mean loss: 183.46
 ---- batch: 080 ----
mean loss: 181.40
 ---- batch: 090 ----
mean loss: 171.87
 ---- batch: 100 ----
mean loss: 189.88
 ---- batch: 110 ----
mean loss: 197.09
train mean loss: 185.96
epoch train time: 0:00:15.590464
elapsed time: 0:37:15.551176
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 08:57:48.196671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.12
 ---- batch: 020 ----
mean loss: 178.87
 ---- batch: 030 ----
mean loss: 178.47
 ---- batch: 040 ----
mean loss: 183.68
 ---- batch: 050 ----
mean loss: 188.96
 ---- batch: 060 ----
mean loss: 177.89
 ---- batch: 070 ----
mean loss: 187.96
 ---- batch: 080 ----
mean loss: 193.34
 ---- batch: 090 ----
mean loss: 192.97
 ---- batch: 100 ----
mean loss: 185.65
 ---- batch: 110 ----
mean loss: 174.63
train mean loss: 184.82
epoch train time: 0:00:15.586372
elapsed time: 0:37:31.138581
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 08:58:03.784004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.78
 ---- batch: 020 ----
mean loss: 182.40
 ---- batch: 030 ----
mean loss: 182.63
 ---- batch: 040 ----
mean loss: 187.92
 ---- batch: 050 ----
mean loss: 183.53
 ---- batch: 060 ----
mean loss: 181.81
 ---- batch: 070 ----
mean loss: 181.71
 ---- batch: 080 ----
mean loss: 189.63
 ---- batch: 090 ----
mean loss: 183.57
 ---- batch: 100 ----
mean loss: 187.76
 ---- batch: 110 ----
mean loss: 183.28
train mean loss: 184.35
epoch train time: 0:00:15.613391
elapsed time: 0:37:46.753030
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 08:58:19.398474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.66
 ---- batch: 020 ----
mean loss: 188.28
 ---- batch: 030 ----
mean loss: 191.30
 ---- batch: 040 ----
mean loss: 180.36
 ---- batch: 050 ----
mean loss: 189.70
 ---- batch: 060 ----
mean loss: 185.94
 ---- batch: 070 ----
mean loss: 184.34
 ---- batch: 080 ----
mean loss: 182.51
 ---- batch: 090 ----
mean loss: 180.12
 ---- batch: 100 ----
mean loss: 180.51
 ---- batch: 110 ----
mean loss: 183.59
train mean loss: 183.95
epoch train time: 0:00:15.625307
elapsed time: 0:38:02.379376
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 08:58:35.024789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.79
 ---- batch: 020 ----
mean loss: 191.14
 ---- batch: 030 ----
mean loss: 178.59
 ---- batch: 040 ----
mean loss: 185.51
 ---- batch: 050 ----
mean loss: 179.95
 ---- batch: 060 ----
mean loss: 183.95
 ---- batch: 070 ----
mean loss: 189.14
 ---- batch: 080 ----
mean loss: 181.85
 ---- batch: 090 ----
mean loss: 184.30
 ---- batch: 100 ----
mean loss: 180.31
 ---- batch: 110 ----
mean loss: 186.46
train mean loss: 185.07
epoch train time: 0:00:15.611464
elapsed time: 0:38:17.991826
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 08:58:50.637257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.62
 ---- batch: 020 ----
mean loss: 187.19
 ---- batch: 030 ----
mean loss: 179.83
 ---- batch: 040 ----
mean loss: 178.43
 ---- batch: 050 ----
mean loss: 188.97
 ---- batch: 060 ----
mean loss: 181.77
 ---- batch: 070 ----
mean loss: 185.11
 ---- batch: 080 ----
mean loss: 190.24
 ---- batch: 090 ----
mean loss: 191.52
 ---- batch: 100 ----
mean loss: 194.08
 ---- batch: 110 ----
mean loss: 176.91
train mean loss: 184.81
epoch train time: 0:00:15.624135
elapsed time: 0:38:33.616960
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 08:59:06.262555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.57
 ---- batch: 020 ----
mean loss: 188.53
 ---- batch: 030 ----
mean loss: 186.36
 ---- batch: 040 ----
mean loss: 182.35
 ---- batch: 050 ----
mean loss: 181.77
 ---- batch: 060 ----
mean loss: 180.63
 ---- batch: 070 ----
mean loss: 183.57
 ---- batch: 080 ----
mean loss: 189.93
 ---- batch: 090 ----
mean loss: 184.42
 ---- batch: 100 ----
mean loss: 188.33
 ---- batch: 110 ----
mean loss: 178.62
train mean loss: 183.85
epoch train time: 0:00:15.643676
elapsed time: 0:38:49.262457
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 08:59:21.907598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.17
 ---- batch: 020 ----
mean loss: 179.90
 ---- batch: 030 ----
mean loss: 179.33
 ---- batch: 040 ----
mean loss: 175.40
 ---- batch: 050 ----
mean loss: 178.95
 ---- batch: 060 ----
mean loss: 184.69
 ---- batch: 070 ----
mean loss: 188.25
 ---- batch: 080 ----
mean loss: 187.67
 ---- batch: 090 ----
mean loss: 182.96
 ---- batch: 100 ----
mean loss: 187.61
 ---- batch: 110 ----
mean loss: 178.99
train mean loss: 183.42
epoch train time: 0:00:15.631571
elapsed time: 0:39:04.894720
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 08:59:37.540159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.00
 ---- batch: 020 ----
mean loss: 183.39
 ---- batch: 030 ----
mean loss: 188.62
 ---- batch: 040 ----
mean loss: 179.81
 ---- batch: 050 ----
mean loss: 182.05
 ---- batch: 060 ----
mean loss: 190.24
 ---- batch: 070 ----
mean loss: 180.11
 ---- batch: 080 ----
mean loss: 189.37
 ---- batch: 090 ----
mean loss: 183.43
 ---- batch: 100 ----
mean loss: 178.38
 ---- batch: 110 ----
mean loss: 180.84
train mean loss: 183.53
epoch train time: 0:00:15.616735
elapsed time: 0:39:20.512431
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 08:59:53.157866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.73
 ---- batch: 020 ----
mean loss: 180.55
 ---- batch: 030 ----
mean loss: 191.70
 ---- batch: 040 ----
mean loss: 187.85
 ---- batch: 050 ----
mean loss: 178.27
 ---- batch: 060 ----
mean loss: 183.58
 ---- batch: 070 ----
mean loss: 183.66
 ---- batch: 080 ----
mean loss: 183.04
 ---- batch: 090 ----
mean loss: 181.08
 ---- batch: 100 ----
mean loss: 184.48
 ---- batch: 110 ----
mean loss: 176.36
train mean loss: 183.54
epoch train time: 0:00:15.649792
elapsed time: 0:39:36.163258
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 09:00:08.808735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.06
 ---- batch: 020 ----
mean loss: 180.39
 ---- batch: 030 ----
mean loss: 180.84
 ---- batch: 040 ----
mean loss: 182.06
 ---- batch: 050 ----
mean loss: 189.02
 ---- batch: 060 ----
mean loss: 183.01
 ---- batch: 070 ----
mean loss: 177.85
 ---- batch: 080 ----
mean loss: 186.92
 ---- batch: 090 ----
mean loss: 185.35
 ---- batch: 100 ----
mean loss: 181.79
 ---- batch: 110 ----
mean loss: 177.16
train mean loss: 183.45
epoch train time: 0:00:15.614136
elapsed time: 0:39:51.778447
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 09:00:24.423931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.78
 ---- batch: 020 ----
mean loss: 178.59
 ---- batch: 030 ----
mean loss: 183.25
 ---- batch: 040 ----
mean loss: 181.98
 ---- batch: 050 ----
mean loss: 185.73
 ---- batch: 060 ----
mean loss: 187.81
 ---- batch: 070 ----
mean loss: 188.06
 ---- batch: 080 ----
mean loss: 179.49
 ---- batch: 090 ----
mean loss: 175.00
 ---- batch: 100 ----
mean loss: 180.61
 ---- batch: 110 ----
mean loss: 188.81
train mean loss: 183.56
epoch train time: 0:00:15.680285
elapsed time: 0:40:07.459731
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 09:00:40.105215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.04
 ---- batch: 020 ----
mean loss: 179.77
 ---- batch: 030 ----
mean loss: 183.60
 ---- batch: 040 ----
mean loss: 181.63
 ---- batch: 050 ----
mean loss: 187.13
 ---- batch: 060 ----
mean loss: 181.81
 ---- batch: 070 ----
mean loss: 184.24
 ---- batch: 080 ----
mean loss: 174.86
 ---- batch: 090 ----
mean loss: 182.48
 ---- batch: 100 ----
mean loss: 178.70
 ---- batch: 110 ----
mean loss: 187.14
train mean loss: 181.95
epoch train time: 0:00:15.617882
elapsed time: 0:40:23.078632
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 09:00:55.724124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.70
 ---- batch: 020 ----
mean loss: 177.16
 ---- batch: 030 ----
mean loss: 176.38
 ---- batch: 040 ----
mean loss: 178.02
 ---- batch: 050 ----
mean loss: 178.32
 ---- batch: 060 ----
mean loss: 181.25
 ---- batch: 070 ----
mean loss: 190.95
 ---- batch: 080 ----
mean loss: 179.60
 ---- batch: 090 ----
mean loss: 185.79
 ---- batch: 100 ----
mean loss: 179.18
 ---- batch: 110 ----
mean loss: 190.76
train mean loss: 182.17
epoch train time: 0:00:15.610531
elapsed time: 0:40:38.690186
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 09:01:11.335608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.50
 ---- batch: 020 ----
mean loss: 176.26
 ---- batch: 030 ----
mean loss: 194.06
 ---- batch: 040 ----
mean loss: 191.38
 ---- batch: 050 ----
mean loss: 177.78
 ---- batch: 060 ----
mean loss: 178.64
 ---- batch: 070 ----
mean loss: 177.52
 ---- batch: 080 ----
mean loss: 178.10
 ---- batch: 090 ----
mean loss: 178.81
 ---- batch: 100 ----
mean loss: 184.27
 ---- batch: 110 ----
mean loss: 181.28
train mean loss: 182.17
epoch train time: 0:00:15.593388
elapsed time: 0:40:54.284531
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 09:01:26.930098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.77
 ---- batch: 020 ----
mean loss: 179.35
 ---- batch: 030 ----
mean loss: 184.47
 ---- batch: 040 ----
mean loss: 183.18
 ---- batch: 050 ----
mean loss: 173.90
 ---- batch: 060 ----
mean loss: 189.51
 ---- batch: 070 ----
mean loss: 172.04
 ---- batch: 080 ----
mean loss: 178.76
 ---- batch: 090 ----
mean loss: 189.23
 ---- batch: 100 ----
mean loss: 188.54
 ---- batch: 110 ----
mean loss: 187.47
train mean loss: 182.25
epoch train time: 0:00:15.691112
elapsed time: 0:41:09.976731
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 09:01:42.622171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.55
 ---- batch: 020 ----
mean loss: 177.45
 ---- batch: 030 ----
mean loss: 182.39
 ---- batch: 040 ----
mean loss: 177.20
 ---- batch: 050 ----
mean loss: 181.83
 ---- batch: 060 ----
mean loss: 178.58
 ---- batch: 070 ----
mean loss: 189.94
 ---- batch: 080 ----
mean loss: 181.19
 ---- batch: 090 ----
mean loss: 171.77
 ---- batch: 100 ----
mean loss: 186.06
 ---- batch: 110 ----
mean loss: 188.63
train mean loss: 182.18
epoch train time: 0:00:15.619227
elapsed time: 0:41:25.597003
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 09:01:58.242463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.98
 ---- batch: 020 ----
mean loss: 178.37
 ---- batch: 030 ----
mean loss: 178.45
 ---- batch: 040 ----
mean loss: 189.52
 ---- batch: 050 ----
mean loss: 181.25
 ---- batch: 060 ----
mean loss: 175.84
 ---- batch: 070 ----
mean loss: 185.63
 ---- batch: 080 ----
mean loss: 180.08
 ---- batch: 090 ----
mean loss: 175.50
 ---- batch: 100 ----
mean loss: 187.51
 ---- batch: 110 ----
mean loss: 178.63
train mean loss: 181.75
epoch train time: 0:00:15.598536
elapsed time: 0:41:41.196518
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 09:02:13.841968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.87
 ---- batch: 020 ----
mean loss: 179.99
 ---- batch: 030 ----
mean loss: 174.74
 ---- batch: 040 ----
mean loss: 187.62
 ---- batch: 050 ----
mean loss: 197.38
 ---- batch: 060 ----
mean loss: 174.70
 ---- batch: 070 ----
mean loss: 175.14
 ---- batch: 080 ----
mean loss: 185.81
 ---- batch: 090 ----
mean loss: 184.33
 ---- batch: 100 ----
mean loss: 169.82
 ---- batch: 110 ----
mean loss: 174.16
train mean loss: 181.46
epoch train time: 0:00:15.624132
elapsed time: 0:41:56.821703
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 09:02:29.467227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.67
 ---- batch: 020 ----
mean loss: 186.02
 ---- batch: 030 ----
mean loss: 182.15
 ---- batch: 040 ----
mean loss: 186.63
 ---- batch: 050 ----
mean loss: 192.21
 ---- batch: 060 ----
mean loss: 180.24
 ---- batch: 070 ----
mean loss: 181.03
 ---- batch: 080 ----
mean loss: 175.56
 ---- batch: 090 ----
mean loss: 182.02
 ---- batch: 100 ----
mean loss: 194.70
 ---- batch: 110 ----
mean loss: 178.68
train mean loss: 182.89
epoch train time: 0:00:15.647646
elapsed time: 0:42:12.470425
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 09:02:45.115859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.37
 ---- batch: 020 ----
mean loss: 184.09
 ---- batch: 030 ----
mean loss: 186.17
 ---- batch: 040 ----
mean loss: 173.07
 ---- batch: 050 ----
mean loss: 192.71
 ---- batch: 060 ----
mean loss: 181.64
 ---- batch: 070 ----
mean loss: 179.09
 ---- batch: 080 ----
mean loss: 167.23
 ---- batch: 090 ----
mean loss: 179.37
 ---- batch: 100 ----
mean loss: 188.75
 ---- batch: 110 ----
mean loss: 184.64
train mean loss: 181.89
epoch train time: 0:00:15.678784
elapsed time: 0:42:28.150190
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 09:03:00.795637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.47
 ---- batch: 020 ----
mean loss: 185.42
 ---- batch: 030 ----
mean loss: 188.29
 ---- batch: 040 ----
mean loss: 185.53
 ---- batch: 050 ----
mean loss: 182.78
 ---- batch: 060 ----
mean loss: 179.10
 ---- batch: 070 ----
mean loss: 175.93
 ---- batch: 080 ----
mean loss: 183.16
 ---- batch: 090 ----
mean loss: 174.90
 ---- batch: 100 ----
mean loss: 173.26
 ---- batch: 110 ----
mean loss: 182.55
train mean loss: 181.71
epoch train time: 0:00:15.670840
elapsed time: 0:42:43.822022
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 09:03:16.467461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.60
 ---- batch: 020 ----
mean loss: 188.77
 ---- batch: 030 ----
mean loss: 180.42
 ---- batch: 040 ----
mean loss: 184.47
 ---- batch: 050 ----
mean loss: 187.22
 ---- batch: 060 ----
mean loss: 178.64
 ---- batch: 070 ----
mean loss: 195.37
 ---- batch: 080 ----
mean loss: 177.10
 ---- batch: 090 ----
mean loss: 170.58
 ---- batch: 100 ----
mean loss: 182.61
 ---- batch: 110 ----
mean loss: 181.14
train mean loss: 181.90
epoch train time: 0:00:15.677359
elapsed time: 0:42:59.500306
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 09:03:32.145856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.17
 ---- batch: 020 ----
mean loss: 172.76
 ---- batch: 030 ----
mean loss: 183.42
 ---- batch: 040 ----
mean loss: 183.09
 ---- batch: 050 ----
mean loss: 192.53
 ---- batch: 060 ----
mean loss: 177.26
 ---- batch: 070 ----
mean loss: 177.25
 ---- batch: 080 ----
mean loss: 180.49
 ---- batch: 090 ----
mean loss: 184.61
 ---- batch: 100 ----
mean loss: 174.57
 ---- batch: 110 ----
mean loss: 167.66
train mean loss: 180.75
epoch train time: 0:00:15.639738
elapsed time: 0:43:15.141172
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 09:03:47.786634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.42
 ---- batch: 020 ----
mean loss: 181.49
 ---- batch: 030 ----
mean loss: 179.03
 ---- batch: 040 ----
mean loss: 178.70
 ---- batch: 050 ----
mean loss: 174.64
 ---- batch: 060 ----
mean loss: 178.98
 ---- batch: 070 ----
mean loss: 167.30
 ---- batch: 080 ----
mean loss: 189.56
 ---- batch: 090 ----
mean loss: 187.55
 ---- batch: 100 ----
mean loss: 197.59
 ---- batch: 110 ----
mean loss: 174.66
train mean loss: 180.98
epoch train time: 0:00:15.662101
elapsed time: 0:43:30.804262
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 09:04:03.449699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.01
 ---- batch: 020 ----
mean loss: 181.95
 ---- batch: 030 ----
mean loss: 173.09
 ---- batch: 040 ----
mean loss: 173.50
 ---- batch: 050 ----
mean loss: 184.79
 ---- batch: 060 ----
mean loss: 177.81
 ---- batch: 070 ----
mean loss: 179.97
 ---- batch: 080 ----
mean loss: 171.51
 ---- batch: 090 ----
mean loss: 190.35
 ---- batch: 100 ----
mean loss: 183.09
 ---- batch: 110 ----
mean loss: 191.63
train mean loss: 181.03
epoch train time: 0:00:15.637504
elapsed time: 0:43:46.442812
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 09:04:19.088281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.30
 ---- batch: 020 ----
mean loss: 184.39
 ---- batch: 030 ----
mean loss: 179.24
 ---- batch: 040 ----
mean loss: 188.83
 ---- batch: 050 ----
mean loss: 188.76
 ---- batch: 060 ----
mean loss: 179.90
 ---- batch: 070 ----
mean loss: 171.93
 ---- batch: 080 ----
mean loss: 181.28
 ---- batch: 090 ----
mean loss: 174.76
 ---- batch: 100 ----
mean loss: 176.84
 ---- batch: 110 ----
mean loss: 178.16
train mean loss: 180.98
epoch train time: 0:00:15.641186
elapsed time: 0:44:02.085078
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 09:04:34.730505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.04
 ---- batch: 020 ----
mean loss: 182.29
 ---- batch: 030 ----
mean loss: 169.26
 ---- batch: 040 ----
mean loss: 188.47
 ---- batch: 050 ----
mean loss: 184.97
 ---- batch: 060 ----
mean loss: 185.98
 ---- batch: 070 ----
mean loss: 177.96
 ---- batch: 080 ----
mean loss: 184.71
 ---- batch: 090 ----
mean loss: 179.83
 ---- batch: 100 ----
mean loss: 178.93
 ---- batch: 110 ----
mean loss: 170.11
train mean loss: 180.12
epoch train time: 0:00:15.633911
elapsed time: 0:44:17.719962
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 09:04:50.365384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.72
 ---- batch: 020 ----
mean loss: 187.28
 ---- batch: 030 ----
mean loss: 183.70
 ---- batch: 040 ----
mean loss: 178.56
 ---- batch: 050 ----
mean loss: 179.91
 ---- batch: 060 ----
mean loss: 188.68
 ---- batch: 070 ----
mean loss: 173.74
 ---- batch: 080 ----
mean loss: 181.50
 ---- batch: 090 ----
mean loss: 180.86
 ---- batch: 100 ----
mean loss: 173.57
 ---- batch: 110 ----
mean loss: 172.04
train mean loss: 180.26
epoch train time: 0:00:15.662192
elapsed time: 0:44:33.383121
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 09:05:06.028559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.61
 ---- batch: 020 ----
mean loss: 190.94
 ---- batch: 030 ----
mean loss: 187.33
 ---- batch: 040 ----
mean loss: 181.18
 ---- batch: 050 ----
mean loss: 177.10
 ---- batch: 060 ----
mean loss: 172.52
 ---- batch: 070 ----
mean loss: 188.13
 ---- batch: 080 ----
mean loss: 177.93
 ---- batch: 090 ----
mean loss: 179.04
 ---- batch: 100 ----
mean loss: 183.20
 ---- batch: 110 ----
mean loss: 180.44
train mean loss: 180.64
epoch train time: 0:00:15.685835
elapsed time: 0:44:49.069980
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 09:05:21.715481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.08
 ---- batch: 020 ----
mean loss: 184.14
 ---- batch: 030 ----
mean loss: 183.63
 ---- batch: 040 ----
mean loss: 171.89
 ---- batch: 050 ----
mean loss: 174.75
 ---- batch: 060 ----
mean loss: 182.05
 ---- batch: 070 ----
mean loss: 175.35
 ---- batch: 080 ----
mean loss: 180.27
 ---- batch: 090 ----
mean loss: 183.46
 ---- batch: 100 ----
mean loss: 185.65
 ---- batch: 110 ----
mean loss: 180.54
train mean loss: 180.08
epoch train time: 0:00:15.732482
elapsed time: 0:45:04.803512
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 09:05:37.449097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.88
 ---- batch: 020 ----
mean loss: 176.34
 ---- batch: 030 ----
mean loss: 185.29
 ---- batch: 040 ----
mean loss: 180.85
 ---- batch: 050 ----
mean loss: 170.12
 ---- batch: 060 ----
mean loss: 179.76
 ---- batch: 070 ----
mean loss: 187.75
 ---- batch: 080 ----
mean loss: 182.42
 ---- batch: 090 ----
mean loss: 191.42
 ---- batch: 100 ----
mean loss: 176.65
 ---- batch: 110 ----
mean loss: 188.59
train mean loss: 181.43
epoch train time: 0:00:15.692179
elapsed time: 0:45:20.497132
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 09:05:53.142521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.10
 ---- batch: 020 ----
mean loss: 186.70
 ---- batch: 030 ----
mean loss: 180.68
 ---- batch: 040 ----
mean loss: 180.37
 ---- batch: 050 ----
mean loss: 188.56
 ---- batch: 060 ----
mean loss: 180.19
 ---- batch: 070 ----
mean loss: 181.34
 ---- batch: 080 ----
mean loss: 174.64
 ---- batch: 090 ----
mean loss: 181.20
 ---- batch: 100 ----
mean loss: 182.96
 ---- batch: 110 ----
mean loss: 184.80
train mean loss: 181.54
epoch train time: 0:00:15.708117
elapsed time: 0:45:36.206254
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 09:06:08.851747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.83
 ---- batch: 020 ----
mean loss: 179.65
 ---- batch: 030 ----
mean loss: 177.39
 ---- batch: 040 ----
mean loss: 180.93
 ---- batch: 050 ----
mean loss: 178.57
 ---- batch: 060 ----
mean loss: 177.55
 ---- batch: 070 ----
mean loss: 184.79
 ---- batch: 080 ----
mean loss: 181.61
 ---- batch: 090 ----
mean loss: 182.75
 ---- batch: 100 ----
mean loss: 181.25
 ---- batch: 110 ----
mean loss: 173.57
train mean loss: 180.59
epoch train time: 0:00:15.648777
elapsed time: 0:45:51.856054
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 09:06:24.501513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.34
 ---- batch: 020 ----
mean loss: 180.45
 ---- batch: 030 ----
mean loss: 170.71
 ---- batch: 040 ----
mean loss: 194.09
 ---- batch: 050 ----
mean loss: 182.32
 ---- batch: 060 ----
mean loss: 180.22
 ---- batch: 070 ----
mean loss: 180.67
 ---- batch: 080 ----
mean loss: 182.31
 ---- batch: 090 ----
mean loss: 172.32
 ---- batch: 100 ----
mean loss: 180.63
 ---- batch: 110 ----
mean loss: 189.53
train mean loss: 181.37
epoch train time: 0:00:15.668895
elapsed time: 0:46:07.525960
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 09:06:40.171481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.39
 ---- batch: 020 ----
mean loss: 186.59
 ---- batch: 030 ----
mean loss: 183.94
 ---- batch: 040 ----
mean loss: 172.35
 ---- batch: 050 ----
mean loss: 178.35
 ---- batch: 060 ----
mean loss: 174.95
 ---- batch: 070 ----
mean loss: 174.87
 ---- batch: 080 ----
mean loss: 179.96
 ---- batch: 090 ----
mean loss: 184.95
 ---- batch: 100 ----
mean loss: 184.95
 ---- batch: 110 ----
mean loss: 168.87
train mean loss: 179.65
epoch train time: 0:00:15.718183
elapsed time: 0:46:23.245236
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 09:06:55.890687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.48
 ---- batch: 020 ----
mean loss: 192.87
 ---- batch: 030 ----
mean loss: 187.41
 ---- batch: 040 ----
mean loss: 174.72
 ---- batch: 050 ----
mean loss: 171.56
 ---- batch: 060 ----
mean loss: 181.70
 ---- batch: 070 ----
mean loss: 173.74
 ---- batch: 080 ----
mean loss: 173.99
 ---- batch: 090 ----
mean loss: 183.03
 ---- batch: 100 ----
mean loss: 173.45
 ---- batch: 110 ----
mean loss: 181.45
train mean loss: 179.76
epoch train time: 0:00:15.677595
elapsed time: 0:46:38.923867
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 09:07:11.569274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.08
 ---- batch: 020 ----
mean loss: 173.30
 ---- batch: 030 ----
mean loss: 177.91
 ---- batch: 040 ----
mean loss: 181.80
 ---- batch: 050 ----
mean loss: 181.14
 ---- batch: 060 ----
mean loss: 175.57
 ---- batch: 070 ----
mean loss: 187.24
 ---- batch: 080 ----
mean loss: 180.86
 ---- batch: 090 ----
mean loss: 177.81
 ---- batch: 100 ----
mean loss: 177.08
 ---- batch: 110 ----
mean loss: 187.71
train mean loss: 179.28
epoch train time: 0:00:15.687947
elapsed time: 0:46:54.612772
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 09:07:27.258224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.66
 ---- batch: 020 ----
mean loss: 183.66
 ---- batch: 030 ----
mean loss: 175.53
 ---- batch: 040 ----
mean loss: 173.49
 ---- batch: 050 ----
mean loss: 188.48
 ---- batch: 060 ----
mean loss: 182.00
 ---- batch: 070 ----
mean loss: 182.92
 ---- batch: 080 ----
mean loss: 182.49
 ---- batch: 090 ----
mean loss: 178.34
 ---- batch: 100 ----
mean loss: 186.61
 ---- batch: 110 ----
mean loss: 181.74
train mean loss: 181.20
epoch train time: 0:00:15.648493
elapsed time: 0:47:10.262333
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 09:07:42.907744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.22
 ---- batch: 020 ----
mean loss: 179.99
 ---- batch: 030 ----
mean loss: 189.79
 ---- batch: 040 ----
mean loss: 175.65
 ---- batch: 050 ----
mean loss: 177.51
 ---- batch: 060 ----
mean loss: 167.78
 ---- batch: 070 ----
mean loss: 177.08
 ---- batch: 080 ----
mean loss: 178.06
 ---- batch: 090 ----
mean loss: 178.13
 ---- batch: 100 ----
mean loss: 183.35
 ---- batch: 110 ----
mean loss: 183.90
train mean loss: 178.71
epoch train time: 0:00:15.623449
elapsed time: 0:47:25.886812
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 09:07:58.532274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.77
 ---- batch: 020 ----
mean loss: 183.48
 ---- batch: 030 ----
mean loss: 175.24
 ---- batch: 040 ----
mean loss: 181.59
 ---- batch: 050 ----
mean loss: 179.42
 ---- batch: 060 ----
mean loss: 185.40
 ---- batch: 070 ----
mean loss: 175.04
 ---- batch: 080 ----
mean loss: 175.61
 ---- batch: 090 ----
mean loss: 176.44
 ---- batch: 100 ----
mean loss: 179.36
 ---- batch: 110 ----
mean loss: 170.96
train mean loss: 178.68
epoch train time: 0:00:15.586016
elapsed time: 0:47:41.473819
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 09:08:14.119244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.80
 ---- batch: 020 ----
mean loss: 179.51
 ---- batch: 030 ----
mean loss: 171.97
 ---- batch: 040 ----
mean loss: 178.22
 ---- batch: 050 ----
mean loss: 174.14
 ---- batch: 060 ----
mean loss: 175.56
 ---- batch: 070 ----
mean loss: 183.20
 ---- batch: 080 ----
mean loss: 180.00
 ---- batch: 090 ----
mean loss: 192.82
 ---- batch: 100 ----
mean loss: 178.48
 ---- batch: 110 ----
mean loss: 175.42
train mean loss: 178.75
epoch train time: 0:00:15.581340
elapsed time: 0:47:57.056093
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 09:08:29.701524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.73
 ---- batch: 020 ----
mean loss: 184.97
 ---- batch: 030 ----
mean loss: 181.41
 ---- batch: 040 ----
mean loss: 176.30
 ---- batch: 050 ----
mean loss: 186.74
 ---- batch: 060 ----
mean loss: 190.48
 ---- batch: 070 ----
mean loss: 183.51
 ---- batch: 080 ----
mean loss: 179.27
 ---- batch: 090 ----
mean loss: 165.64
 ---- batch: 100 ----
mean loss: 172.77
 ---- batch: 110 ----
mean loss: 184.34
train mean loss: 180.27
epoch train time: 0:00:15.579003
elapsed time: 0:48:12.636042
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 09:08:45.281535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.30
 ---- batch: 020 ----
mean loss: 171.85
 ---- batch: 030 ----
mean loss: 185.70
 ---- batch: 040 ----
mean loss: 180.34
 ---- batch: 050 ----
mean loss: 181.29
 ---- batch: 060 ----
mean loss: 179.77
 ---- batch: 070 ----
mean loss: 181.13
 ---- batch: 080 ----
mean loss: 168.69
 ---- batch: 090 ----
mean loss: 176.43
 ---- batch: 100 ----
mean loss: 177.72
 ---- batch: 110 ----
mean loss: 180.07
train mean loss: 178.26
epoch train time: 0:00:15.625986
elapsed time: 0:48:28.263042
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 09:09:00.908487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.04
 ---- batch: 020 ----
mean loss: 185.02
 ---- batch: 030 ----
mean loss: 185.06
 ---- batch: 040 ----
mean loss: 174.65
 ---- batch: 050 ----
mean loss: 175.37
 ---- batch: 060 ----
mean loss: 178.32
 ---- batch: 070 ----
mean loss: 177.40
 ---- batch: 080 ----
mean loss: 174.81
 ---- batch: 090 ----
mean loss: 173.68
 ---- batch: 100 ----
mean loss: 167.87
 ---- batch: 110 ----
mean loss: 177.56
train mean loss: 177.98
epoch train time: 0:00:15.634951
elapsed time: 0:48:43.898956
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 09:09:16.544440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.06
 ---- batch: 020 ----
mean loss: 183.85
 ---- batch: 030 ----
mean loss: 181.85
 ---- batch: 040 ----
mean loss: 184.25
 ---- batch: 050 ----
mean loss: 186.34
 ---- batch: 060 ----
mean loss: 181.51
 ---- batch: 070 ----
mean loss: 169.80
 ---- batch: 080 ----
mean loss: 173.42
 ---- batch: 090 ----
mean loss: 182.04
 ---- batch: 100 ----
mean loss: 180.02
 ---- batch: 110 ----
mean loss: 179.19
train mean loss: 179.39
epoch train time: 0:00:15.660861
elapsed time: 0:48:59.560873
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 09:09:32.206338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.96
 ---- batch: 020 ----
mean loss: 180.13
 ---- batch: 030 ----
mean loss: 186.80
 ---- batch: 040 ----
mean loss: 167.65
 ---- batch: 050 ----
mean loss: 178.63
 ---- batch: 060 ----
mean loss: 177.28
 ---- batch: 070 ----
mean loss: 177.14
 ---- batch: 080 ----
mean loss: 174.13
 ---- batch: 090 ----
mean loss: 184.64
 ---- batch: 100 ----
mean loss: 180.04
 ---- batch: 110 ----
mean loss: 174.03
train mean loss: 178.41
epoch train time: 0:00:15.649617
elapsed time: 0:49:15.211558
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 09:09:47.856993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.51
 ---- batch: 020 ----
mean loss: 177.58
 ---- batch: 030 ----
mean loss: 192.32
 ---- batch: 040 ----
mean loss: 188.24
 ---- batch: 050 ----
mean loss: 178.16
 ---- batch: 060 ----
mean loss: 174.36
 ---- batch: 070 ----
mean loss: 171.85
 ---- batch: 080 ----
mean loss: 187.04
 ---- batch: 090 ----
mean loss: 176.76
 ---- batch: 100 ----
mean loss: 168.04
 ---- batch: 110 ----
mean loss: 174.42
train mean loss: 179.16
epoch train time: 0:00:15.620719
elapsed time: 0:49:30.833258
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 09:10:03.478721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.42
 ---- batch: 020 ----
mean loss: 174.37
 ---- batch: 030 ----
mean loss: 183.63
 ---- batch: 040 ----
mean loss: 175.69
 ---- batch: 050 ----
mean loss: 177.27
 ---- batch: 060 ----
mean loss: 184.43
 ---- batch: 070 ----
mean loss: 185.27
 ---- batch: 080 ----
mean loss: 176.31
 ---- batch: 090 ----
mean loss: 174.24
 ---- batch: 100 ----
mean loss: 179.32
 ---- batch: 110 ----
mean loss: 186.79
train mean loss: 178.95
epoch train time: 0:00:15.619131
elapsed time: 0:49:46.453402
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 09:10:19.098903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.54
 ---- batch: 020 ----
mean loss: 171.65
 ---- batch: 030 ----
mean loss: 177.75
 ---- batch: 040 ----
mean loss: 180.19
 ---- batch: 050 ----
mean loss: 188.22
 ---- batch: 060 ----
mean loss: 181.24
 ---- batch: 070 ----
mean loss: 181.59
 ---- batch: 080 ----
mean loss: 175.48
 ---- batch: 090 ----
mean loss: 183.90
 ---- batch: 100 ----
mean loss: 180.76
 ---- batch: 110 ----
mean loss: 181.16
train mean loss: 180.24
epoch train time: 0:00:15.648868
elapsed time: 0:50:02.103387
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 09:10:34.748840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.75
 ---- batch: 020 ----
mean loss: 176.29
 ---- batch: 030 ----
mean loss: 178.65
 ---- batch: 040 ----
mean loss: 177.06
 ---- batch: 050 ----
mean loss: 183.34
 ---- batch: 060 ----
mean loss: 185.09
 ---- batch: 070 ----
mean loss: 169.32
 ---- batch: 080 ----
mean loss: 178.71
 ---- batch: 090 ----
mean loss: 170.56
 ---- batch: 100 ----
mean loss: 173.18
 ---- batch: 110 ----
mean loss: 181.22
train mean loss: 178.29
epoch train time: 0:00:15.650420
elapsed time: 0:50:17.754788
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 09:10:50.400210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.62
 ---- batch: 020 ----
mean loss: 182.15
 ---- batch: 030 ----
mean loss: 177.82
 ---- batch: 040 ----
mean loss: 182.12
 ---- batch: 050 ----
mean loss: 180.70
 ---- batch: 060 ----
mean loss: 168.04
 ---- batch: 070 ----
mean loss: 171.17
 ---- batch: 080 ----
mean loss: 176.09
 ---- batch: 090 ----
mean loss: 167.44
 ---- batch: 100 ----
mean loss: 185.96
 ---- batch: 110 ----
mean loss: 180.49
train mean loss: 177.48
epoch train time: 0:00:15.615166
elapsed time: 0:50:33.370916
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 09:11:06.016436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.27
 ---- batch: 020 ----
mean loss: 192.26
 ---- batch: 030 ----
mean loss: 181.60
 ---- batch: 040 ----
mean loss: 177.79
 ---- batch: 050 ----
mean loss: 173.73
 ---- batch: 060 ----
mean loss: 175.89
 ---- batch: 070 ----
mean loss: 186.35
 ---- batch: 080 ----
mean loss: 176.62
 ---- batch: 090 ----
mean loss: 180.67
 ---- batch: 100 ----
mean loss: 178.25
 ---- batch: 110 ----
mean loss: 167.53
train mean loss: 178.34
epoch train time: 0:00:15.622951
elapsed time: 0:50:48.994913
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 09:11:21.640332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.43
 ---- batch: 020 ----
mean loss: 183.85
 ---- batch: 030 ----
mean loss: 187.17
 ---- batch: 040 ----
mean loss: 186.92
 ---- batch: 050 ----
mean loss: 173.61
 ---- batch: 060 ----
mean loss: 173.65
 ---- batch: 070 ----
mean loss: 177.46
 ---- batch: 080 ----
mean loss: 177.34
 ---- batch: 090 ----
mean loss: 183.81
 ---- batch: 100 ----
mean loss: 171.87
 ---- batch: 110 ----
mean loss: 184.21
train mean loss: 179.30
epoch train time: 0:00:15.635829
elapsed time: 0:51:04.631708
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 09:11:37.277226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.14
 ---- batch: 020 ----
mean loss: 176.19
 ---- batch: 030 ----
mean loss: 176.31
 ---- batch: 040 ----
mean loss: 193.67
 ---- batch: 050 ----
mean loss: 175.53
 ---- batch: 060 ----
mean loss: 169.25
 ---- batch: 070 ----
mean loss: 177.96
 ---- batch: 080 ----
mean loss: 180.36
 ---- batch: 090 ----
mean loss: 175.58
 ---- batch: 100 ----
mean loss: 168.04
 ---- batch: 110 ----
mean loss: 182.28
train mean loss: 177.79
epoch train time: 0:00:15.704553
elapsed time: 0:51:20.337398
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 09:11:52.982834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.75
 ---- batch: 020 ----
mean loss: 181.07
 ---- batch: 030 ----
mean loss: 182.09
 ---- batch: 040 ----
mean loss: 166.23
 ---- batch: 050 ----
mean loss: 181.71
 ---- batch: 060 ----
mean loss: 170.96
 ---- batch: 070 ----
mean loss: 190.88
 ---- batch: 080 ----
mean loss: 183.98
 ---- batch: 090 ----
mean loss: 175.15
 ---- batch: 100 ----
mean loss: 175.39
 ---- batch: 110 ----
mean loss: 174.40
train mean loss: 177.88
epoch train time: 0:00:15.676124
elapsed time: 0:51:36.014529
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 09:12:08.660010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.91
 ---- batch: 020 ----
mean loss: 173.29
 ---- batch: 030 ----
mean loss: 174.79
 ---- batch: 040 ----
mean loss: 178.51
 ---- batch: 050 ----
mean loss: 176.42
 ---- batch: 060 ----
mean loss: 184.89
 ---- batch: 070 ----
mean loss: 172.42
 ---- batch: 080 ----
mean loss: 179.02
 ---- batch: 090 ----
mean loss: 173.77
 ---- batch: 100 ----
mean loss: 169.13
 ---- batch: 110 ----
mean loss: 179.04
train mean loss: 176.88
epoch train time: 0:00:15.547795
elapsed time: 0:51:51.563304
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 09:12:24.208708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.09
 ---- batch: 020 ----
mean loss: 167.21
 ---- batch: 030 ----
mean loss: 190.02
 ---- batch: 040 ----
mean loss: 166.72
 ---- batch: 050 ----
mean loss: 179.62
 ---- batch: 060 ----
mean loss: 192.18
 ---- batch: 070 ----
mean loss: 183.76
 ---- batch: 080 ----
mean loss: 175.11
 ---- batch: 090 ----
mean loss: 169.03
 ---- batch: 100 ----
mean loss: 176.51
 ---- batch: 110 ----
mean loss: 178.96
train mean loss: 177.92
epoch train time: 0:00:15.413094
elapsed time: 0:52:06.977306
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 09:12:39.622734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.28
 ---- batch: 020 ----
mean loss: 174.15
 ---- batch: 030 ----
mean loss: 176.99
 ---- batch: 040 ----
mean loss: 175.43
 ---- batch: 050 ----
mean loss: 176.21
 ---- batch: 060 ----
mean loss: 183.51
 ---- batch: 070 ----
mean loss: 174.27
 ---- batch: 080 ----
mean loss: 167.22
 ---- batch: 090 ----
mean loss: 175.98
 ---- batch: 100 ----
mean loss: 181.09
 ---- batch: 110 ----
mean loss: 187.87
train mean loss: 177.21
epoch train time: 0:00:15.402744
elapsed time: 0:52:22.381027
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 09:12:55.026599
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.59
 ---- batch: 020 ----
mean loss: 173.83
 ---- batch: 030 ----
mean loss: 184.89
 ---- batch: 040 ----
mean loss: 174.18
 ---- batch: 050 ----
mean loss: 168.12
 ---- batch: 060 ----
mean loss: 177.16
 ---- batch: 070 ----
mean loss: 171.85
 ---- batch: 080 ----
mean loss: 175.68
 ---- batch: 090 ----
mean loss: 175.17
 ---- batch: 100 ----
mean loss: 176.29
 ---- batch: 110 ----
mean loss: 170.92
train mean loss: 174.74
epoch train time: 0:00:15.409194
elapsed time: 0:52:37.792151
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 09:13:10.437150
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.05
 ---- batch: 020 ----
mean loss: 167.80
 ---- batch: 030 ----
mean loss: 167.72
 ---- batch: 040 ----
mean loss: 178.27
 ---- batch: 050 ----
mean loss: 176.14
 ---- batch: 060 ----
mean loss: 175.67
 ---- batch: 070 ----
mean loss: 167.76
 ---- batch: 080 ----
mean loss: 184.26
 ---- batch: 090 ----
mean loss: 179.20
 ---- batch: 100 ----
mean loss: 175.30
 ---- batch: 110 ----
mean loss: 168.62
train mean loss: 174.61
epoch train time: 0:00:15.458190
elapsed time: 0:52:53.250980
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 09:13:25.896494
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.23
 ---- batch: 020 ----
mean loss: 176.26
 ---- batch: 030 ----
mean loss: 180.95
 ---- batch: 040 ----
mean loss: 175.76
 ---- batch: 050 ----
mean loss: 175.99
 ---- batch: 060 ----
mean loss: 180.05
 ---- batch: 070 ----
mean loss: 168.59
 ---- batch: 080 ----
mean loss: 180.69
 ---- batch: 090 ----
mean loss: 170.79
 ---- batch: 100 ----
mean loss: 172.49
 ---- batch: 110 ----
mean loss: 171.02
train mean loss: 174.65
epoch train time: 0:00:15.449381
elapsed time: 0:53:08.701424
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 09:13:41.346881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.10
 ---- batch: 020 ----
mean loss: 170.92
 ---- batch: 030 ----
mean loss: 169.43
 ---- batch: 040 ----
mean loss: 173.25
 ---- batch: 050 ----
mean loss: 178.31
 ---- batch: 060 ----
mean loss: 176.44
 ---- batch: 070 ----
mean loss: 178.97
 ---- batch: 080 ----
mean loss: 179.05
 ---- batch: 090 ----
mean loss: 178.60
 ---- batch: 100 ----
mean loss: 166.73
 ---- batch: 110 ----
mean loss: 172.23
train mean loss: 174.60
epoch train time: 0:00:15.416132
elapsed time: 0:53:24.118396
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 09:13:56.763798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.64
 ---- batch: 020 ----
mean loss: 171.75
 ---- batch: 030 ----
mean loss: 175.88
 ---- batch: 040 ----
mean loss: 172.69
 ---- batch: 050 ----
mean loss: 175.21
 ---- batch: 060 ----
mean loss: 176.98
 ---- batch: 070 ----
mean loss: 176.54
 ---- batch: 080 ----
mean loss: 181.49
 ---- batch: 090 ----
mean loss: 166.01
 ---- batch: 100 ----
mean loss: 168.28
 ---- batch: 110 ----
mean loss: 178.84
train mean loss: 174.70
epoch train time: 0:00:15.406370
elapsed time: 0:53:39.525680
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 09:14:12.171135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.69
 ---- batch: 020 ----
mean loss: 171.76
 ---- batch: 030 ----
mean loss: 174.62
 ---- batch: 040 ----
mean loss: 177.19
 ---- batch: 050 ----
mean loss: 168.76
 ---- batch: 060 ----
mean loss: 179.86
 ---- batch: 070 ----
mean loss: 176.12
 ---- batch: 080 ----
mean loss: 179.43
 ---- batch: 090 ----
mean loss: 170.38
 ---- batch: 100 ----
mean loss: 166.57
 ---- batch: 110 ----
mean loss: 180.35
train mean loss: 174.60
epoch train time: 0:00:15.458816
elapsed time: 0:53:54.985428
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 09:14:27.630839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.31
 ---- batch: 020 ----
mean loss: 179.02
 ---- batch: 030 ----
mean loss: 176.06
 ---- batch: 040 ----
mean loss: 186.57
 ---- batch: 050 ----
mean loss: 166.05
 ---- batch: 060 ----
mean loss: 175.54
 ---- batch: 070 ----
mean loss: 166.39
 ---- batch: 080 ----
mean loss: 181.61
 ---- batch: 090 ----
mean loss: 168.62
 ---- batch: 100 ----
mean loss: 181.45
 ---- batch: 110 ----
mean loss: 172.07
train mean loss: 174.59
epoch train time: 0:00:15.396708
elapsed time: 0:54:10.383064
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 09:14:43.028470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.61
 ---- batch: 020 ----
mean loss: 173.93
 ---- batch: 030 ----
mean loss: 170.54
 ---- batch: 040 ----
mean loss: 169.71
 ---- batch: 050 ----
mean loss: 174.96
 ---- batch: 060 ----
mean loss: 182.01
 ---- batch: 070 ----
mean loss: 181.10
 ---- batch: 080 ----
mean loss: 175.40
 ---- batch: 090 ----
mean loss: 174.66
 ---- batch: 100 ----
mean loss: 168.47
 ---- batch: 110 ----
mean loss: 178.71
train mean loss: 174.54
epoch train time: 0:00:15.425443
elapsed time: 0:54:25.809418
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 09:14:58.454827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.61
 ---- batch: 020 ----
mean loss: 160.09
 ---- batch: 030 ----
mean loss: 180.56
 ---- batch: 040 ----
mean loss: 170.69
 ---- batch: 050 ----
mean loss: 170.21
 ---- batch: 060 ----
mean loss: 177.73
 ---- batch: 070 ----
mean loss: 175.80
 ---- batch: 080 ----
mean loss: 173.39
 ---- batch: 090 ----
mean loss: 177.61
 ---- batch: 100 ----
mean loss: 172.21
 ---- batch: 110 ----
mean loss: 186.44
train mean loss: 174.62
epoch train time: 0:00:15.399885
elapsed time: 0:54:41.210324
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 09:15:13.855835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.82
 ---- batch: 020 ----
mean loss: 173.58
 ---- batch: 030 ----
mean loss: 169.35
 ---- batch: 040 ----
mean loss: 177.99
 ---- batch: 050 ----
mean loss: 178.86
 ---- batch: 060 ----
mean loss: 177.34
 ---- batch: 070 ----
mean loss: 169.04
 ---- batch: 080 ----
mean loss: 169.83
 ---- batch: 090 ----
mean loss: 169.25
 ---- batch: 100 ----
mean loss: 171.66
 ---- batch: 110 ----
mean loss: 170.13
train mean loss: 174.51
epoch train time: 0:00:15.447110
elapsed time: 0:54:56.658511
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 09:15:29.303985
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.75
 ---- batch: 020 ----
mean loss: 176.69
 ---- batch: 030 ----
mean loss: 176.04
 ---- batch: 040 ----
mean loss: 178.84
 ---- batch: 050 ----
mean loss: 176.09
 ---- batch: 060 ----
mean loss: 177.04
 ---- batch: 070 ----
mean loss: 167.99
 ---- batch: 080 ----
mean loss: 172.51
 ---- batch: 090 ----
mean loss: 177.33
 ---- batch: 100 ----
mean loss: 167.53
 ---- batch: 110 ----
mean loss: 175.35
train mean loss: 174.83
epoch train time: 0:00:15.493369
elapsed time: 0:55:12.152902
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 09:15:44.798364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.97
 ---- batch: 020 ----
mean loss: 172.57
 ---- batch: 030 ----
mean loss: 174.11
 ---- batch: 040 ----
mean loss: 181.50
 ---- batch: 050 ----
mean loss: 177.20
 ---- batch: 060 ----
mean loss: 174.98
 ---- batch: 070 ----
mean loss: 178.08
 ---- batch: 080 ----
mean loss: 172.15
 ---- batch: 090 ----
mean loss: 173.57
 ---- batch: 100 ----
mean loss: 175.65
 ---- batch: 110 ----
mean loss: 169.61
train mean loss: 174.38
epoch train time: 0:00:15.430212
elapsed time: 0:55:27.584021
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 09:16:00.229428
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.41
 ---- batch: 020 ----
mean loss: 168.37
 ---- batch: 030 ----
mean loss: 164.19
 ---- batch: 040 ----
mean loss: 184.68
 ---- batch: 050 ----
mean loss: 176.45
 ---- batch: 060 ----
mean loss: 180.48
 ---- batch: 070 ----
mean loss: 167.00
 ---- batch: 080 ----
mean loss: 171.17
 ---- batch: 090 ----
mean loss: 166.73
 ---- batch: 100 ----
mean loss: 182.96
 ---- batch: 110 ----
mean loss: 180.48
train mean loss: 174.43
epoch train time: 0:00:15.438629
elapsed time: 0:55:43.023575
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 09:16:15.668997
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.55
 ---- batch: 020 ----
mean loss: 178.61
 ---- batch: 030 ----
mean loss: 169.85
 ---- batch: 040 ----
mean loss: 165.16
 ---- batch: 050 ----
mean loss: 179.52
 ---- batch: 060 ----
mean loss: 172.02
 ---- batch: 070 ----
mean loss: 183.89
 ---- batch: 080 ----
mean loss: 175.44
 ---- batch: 090 ----
mean loss: 171.68
 ---- batch: 100 ----
mean loss: 177.35
 ---- batch: 110 ----
mean loss: 177.53
train mean loss: 174.57
epoch train time: 0:00:15.402090
elapsed time: 0:55:58.426573
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 09:16:31.072028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.44
 ---- batch: 020 ----
mean loss: 179.58
 ---- batch: 030 ----
mean loss: 178.58
 ---- batch: 040 ----
mean loss: 177.58
 ---- batch: 050 ----
mean loss: 169.85
 ---- batch: 060 ----
mean loss: 172.33
 ---- batch: 070 ----
mean loss: 180.01
 ---- batch: 080 ----
mean loss: 166.72
 ---- batch: 090 ----
mean loss: 164.67
 ---- batch: 100 ----
mean loss: 172.04
 ---- batch: 110 ----
mean loss: 175.16
train mean loss: 174.62
epoch train time: 0:00:15.443080
elapsed time: 0:56:13.870639
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 09:16:46.516090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.89
 ---- batch: 020 ----
mean loss: 175.96
 ---- batch: 030 ----
mean loss: 173.83
 ---- batch: 040 ----
mean loss: 174.94
 ---- batch: 050 ----
mean loss: 170.29
 ---- batch: 060 ----
mean loss: 172.05
 ---- batch: 070 ----
mean loss: 168.15
 ---- batch: 080 ----
mean loss: 175.50
 ---- batch: 090 ----
mean loss: 178.36
 ---- batch: 100 ----
mean loss: 177.34
 ---- batch: 110 ----
mean loss: 173.32
train mean loss: 174.49
epoch train time: 0:00:15.466017
elapsed time: 0:56:29.337594
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 09:17:01.982998
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.50
 ---- batch: 020 ----
mean loss: 168.73
 ---- batch: 030 ----
mean loss: 178.42
 ---- batch: 040 ----
mean loss: 171.85
 ---- batch: 050 ----
mean loss: 175.22
 ---- batch: 060 ----
mean loss: 177.54
 ---- batch: 070 ----
mean loss: 176.27
 ---- batch: 080 ----
mean loss: 176.63
 ---- batch: 090 ----
mean loss: 176.70
 ---- batch: 100 ----
mean loss: 182.24
 ---- batch: 110 ----
mean loss: 168.98
train mean loss: 174.45
epoch train time: 0:00:15.481125
elapsed time: 0:56:44.819627
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 09:17:17.465078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.08
 ---- batch: 020 ----
mean loss: 178.54
 ---- batch: 030 ----
mean loss: 180.92
 ---- batch: 040 ----
mean loss: 174.74
 ---- batch: 050 ----
mean loss: 172.52
 ---- batch: 060 ----
mean loss: 176.67
 ---- batch: 070 ----
mean loss: 169.45
 ---- batch: 080 ----
mean loss: 166.75
 ---- batch: 090 ----
mean loss: 168.59
 ---- batch: 100 ----
mean loss: 179.09
 ---- batch: 110 ----
mean loss: 173.29
train mean loss: 174.35
epoch train time: 0:00:15.400707
elapsed time: 0:57:00.221284
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 09:17:32.866680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.50
 ---- batch: 020 ----
mean loss: 175.01
 ---- batch: 030 ----
mean loss: 169.67
 ---- batch: 040 ----
mean loss: 171.29
 ---- batch: 050 ----
mean loss: 179.30
 ---- batch: 060 ----
mean loss: 168.13
 ---- batch: 070 ----
mean loss: 170.79
 ---- batch: 080 ----
mean loss: 185.03
 ---- batch: 090 ----
mean loss: 174.94
 ---- batch: 100 ----
mean loss: 173.44
 ---- batch: 110 ----
mean loss: 177.36
train mean loss: 174.49
epoch train time: 0:00:15.388877
elapsed time: 0:57:15.611074
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 09:17:48.256492
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.45
 ---- batch: 020 ----
mean loss: 175.13
 ---- batch: 030 ----
mean loss: 178.24
 ---- batch: 040 ----
mean loss: 176.22
 ---- batch: 050 ----
mean loss: 181.73
 ---- batch: 060 ----
mean loss: 170.61
 ---- batch: 070 ----
mean loss: 171.91
 ---- batch: 080 ----
mean loss: 172.35
 ---- batch: 090 ----
mean loss: 174.49
 ---- batch: 100 ----
mean loss: 176.91
 ---- batch: 110 ----
mean loss: 174.98
train mean loss: 174.48
epoch train time: 0:00:15.390952
elapsed time: 0:57:31.002927
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 09:18:03.648367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.86
 ---- batch: 020 ----
mean loss: 171.24
 ---- batch: 030 ----
mean loss: 178.68
 ---- batch: 040 ----
mean loss: 181.86
 ---- batch: 050 ----
mean loss: 168.56
 ---- batch: 060 ----
mean loss: 171.71
 ---- batch: 070 ----
mean loss: 180.47
 ---- batch: 080 ----
mean loss: 181.76
 ---- batch: 090 ----
mean loss: 169.33
 ---- batch: 100 ----
mean loss: 170.63
 ---- batch: 110 ----
mean loss: 174.97
train mean loss: 174.42
epoch train time: 0:00:15.382606
elapsed time: 0:57:46.386469
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 09:18:19.031859
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.44
 ---- batch: 020 ----
mean loss: 175.70
 ---- batch: 030 ----
mean loss: 183.78
 ---- batch: 040 ----
mean loss: 171.31
 ---- batch: 050 ----
mean loss: 175.14
 ---- batch: 060 ----
mean loss: 175.78
 ---- batch: 070 ----
mean loss: 171.23
 ---- batch: 080 ----
mean loss: 171.85
 ---- batch: 090 ----
mean loss: 170.78
 ---- batch: 100 ----
mean loss: 177.47
 ---- batch: 110 ----
mean loss: 170.57
train mean loss: 174.49
epoch train time: 0:00:15.384637
elapsed time: 0:58:01.772016
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 09:18:34.417482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.36
 ---- batch: 020 ----
mean loss: 171.06
 ---- batch: 030 ----
mean loss: 169.46
 ---- batch: 040 ----
mean loss: 172.23
 ---- batch: 050 ----
mean loss: 183.61
 ---- batch: 060 ----
mean loss: 173.68
 ---- batch: 070 ----
mean loss: 176.15
 ---- batch: 080 ----
mean loss: 173.19
 ---- batch: 090 ----
mean loss: 180.06
 ---- batch: 100 ----
mean loss: 177.18
 ---- batch: 110 ----
mean loss: 168.44
train mean loss: 174.43
epoch train time: 0:00:15.383192
elapsed time: 0:58:17.156199
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 09:18:49.801726
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.50
 ---- batch: 020 ----
mean loss: 178.16
 ---- batch: 030 ----
mean loss: 176.36
 ---- batch: 040 ----
mean loss: 171.94
 ---- batch: 050 ----
mean loss: 176.38
 ---- batch: 060 ----
mean loss: 180.90
 ---- batch: 070 ----
mean loss: 172.61
 ---- batch: 080 ----
mean loss: 179.33
 ---- batch: 090 ----
mean loss: 178.03
 ---- batch: 100 ----
mean loss: 164.51
 ---- batch: 110 ----
mean loss: 171.96
train mean loss: 174.57
epoch train time: 0:00:15.374632
elapsed time: 0:58:32.531881
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 09:19:05.177353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.90
 ---- batch: 020 ----
mean loss: 169.19
 ---- batch: 030 ----
mean loss: 182.94
 ---- batch: 040 ----
mean loss: 180.46
 ---- batch: 050 ----
mean loss: 168.44
 ---- batch: 060 ----
mean loss: 180.63
 ---- batch: 070 ----
mean loss: 167.33
 ---- batch: 080 ----
mean loss: 167.67
 ---- batch: 090 ----
mean loss: 179.12
 ---- batch: 100 ----
mean loss: 177.26
 ---- batch: 110 ----
mean loss: 174.54
train mean loss: 174.65
epoch train time: 0:00:15.410949
elapsed time: 0:58:47.943792
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 09:19:20.589311
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.95
 ---- batch: 020 ----
mean loss: 183.91
 ---- batch: 030 ----
mean loss: 175.96
 ---- batch: 040 ----
mean loss: 168.55
 ---- batch: 050 ----
mean loss: 176.29
 ---- batch: 060 ----
mean loss: 159.37
 ---- batch: 070 ----
mean loss: 183.36
 ---- batch: 080 ----
mean loss: 170.21
 ---- batch: 090 ----
mean loss: 180.25
 ---- batch: 100 ----
mean loss: 175.99
 ---- batch: 110 ----
mean loss: 173.97
train mean loss: 174.20
epoch train time: 0:00:15.376410
elapsed time: 0:59:03.321260
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 09:19:35.966801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.80
 ---- batch: 020 ----
mean loss: 172.95
 ---- batch: 030 ----
mean loss: 176.61
 ---- batch: 040 ----
mean loss: 170.05
 ---- batch: 050 ----
mean loss: 173.83
 ---- batch: 060 ----
mean loss: 175.21
 ---- batch: 070 ----
mean loss: 184.76
 ---- batch: 080 ----
mean loss: 181.38
 ---- batch: 090 ----
mean loss: 174.04
 ---- batch: 100 ----
mean loss: 173.75
 ---- batch: 110 ----
mean loss: 167.58
train mean loss: 174.06
epoch train time: 0:00:15.387703
elapsed time: 0:59:18.709961
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 09:19:51.355450
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.87
 ---- batch: 020 ----
mean loss: 170.28
 ---- batch: 030 ----
mean loss: 168.89
 ---- batch: 040 ----
mean loss: 177.97
 ---- batch: 050 ----
mean loss: 176.87
 ---- batch: 060 ----
mean loss: 176.34
 ---- batch: 070 ----
mean loss: 177.41
 ---- batch: 080 ----
mean loss: 178.16
 ---- batch: 090 ----
mean loss: 171.98
 ---- batch: 100 ----
mean loss: 171.81
 ---- batch: 110 ----
mean loss: 173.77
train mean loss: 174.48
epoch train time: 0:00:15.384628
elapsed time: 0:59:34.095638
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 09:20:06.741123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.75
 ---- batch: 020 ----
mean loss: 180.56
 ---- batch: 030 ----
mean loss: 183.07
 ---- batch: 040 ----
mean loss: 177.72
 ---- batch: 050 ----
mean loss: 175.35
 ---- batch: 060 ----
mean loss: 177.00
 ---- batch: 070 ----
mean loss: 173.11
 ---- batch: 080 ----
mean loss: 164.44
 ---- batch: 090 ----
mean loss: 176.50
 ---- batch: 100 ----
mean loss: 173.81
 ---- batch: 110 ----
mean loss: 170.57
train mean loss: 174.43
epoch train time: 0:00:15.393044
elapsed time: 0:59:49.489561
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 09:20:22.134987
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.00
 ---- batch: 020 ----
mean loss: 166.74
 ---- batch: 030 ----
mean loss: 168.65
 ---- batch: 040 ----
mean loss: 184.72
 ---- batch: 050 ----
mean loss: 171.72
 ---- batch: 060 ----
mean loss: 175.47
 ---- batch: 070 ----
mean loss: 178.20
 ---- batch: 080 ----
mean loss: 172.78
 ---- batch: 090 ----
mean loss: 173.91
 ---- batch: 100 ----
mean loss: 167.32
 ---- batch: 110 ----
mean loss: 178.39
train mean loss: 174.20
epoch train time: 0:00:15.370716
elapsed time: 1:00:04.861178
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 09:20:37.506588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.34
 ---- batch: 020 ----
mean loss: 164.86
 ---- batch: 030 ----
mean loss: 167.17
 ---- batch: 040 ----
mean loss: 183.45
 ---- batch: 050 ----
mean loss: 180.74
 ---- batch: 060 ----
mean loss: 165.73
 ---- batch: 070 ----
mean loss: 174.33
 ---- batch: 080 ----
mean loss: 180.90
 ---- batch: 090 ----
mean loss: 170.39
 ---- batch: 100 ----
mean loss: 180.96
 ---- batch: 110 ----
mean loss: 178.44
train mean loss: 174.25
epoch train time: 0:00:15.421938
elapsed time: 1:00:20.284016
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 09:20:52.929418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.35
 ---- batch: 020 ----
mean loss: 174.20
 ---- batch: 030 ----
mean loss: 174.84
 ---- batch: 040 ----
mean loss: 178.58
 ---- batch: 050 ----
mean loss: 167.39
 ---- batch: 060 ----
mean loss: 182.31
 ---- batch: 070 ----
mean loss: 174.50
 ---- batch: 080 ----
mean loss: 177.38
 ---- batch: 090 ----
mean loss: 174.90
 ---- batch: 100 ----
mean loss: 169.36
 ---- batch: 110 ----
mean loss: 171.13
train mean loss: 174.37
epoch train time: 0:00:15.376906
elapsed time: 1:00:35.661838
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 09:21:08.307474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.79
 ---- batch: 020 ----
mean loss: 169.04
 ---- batch: 030 ----
mean loss: 176.55
 ---- batch: 040 ----
mean loss: 178.61
 ---- batch: 050 ----
mean loss: 169.95
 ---- batch: 060 ----
mean loss: 178.99
 ---- batch: 070 ----
mean loss: 178.59
 ---- batch: 080 ----
mean loss: 175.85
 ---- batch: 090 ----
mean loss: 171.46
 ---- batch: 100 ----
mean loss: 181.79
 ---- batch: 110 ----
mean loss: 170.55
train mean loss: 174.82
epoch train time: 0:00:15.389476
elapsed time: 1:00:51.053149
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 09:21:23.698284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.86
 ---- batch: 020 ----
mean loss: 171.70
 ---- batch: 030 ----
mean loss: 164.51
 ---- batch: 040 ----
mean loss: 175.31
 ---- batch: 050 ----
mean loss: 170.66
 ---- batch: 060 ----
mean loss: 178.63
 ---- batch: 070 ----
mean loss: 172.04
 ---- batch: 080 ----
mean loss: 180.00
 ---- batch: 090 ----
mean loss: 179.96
 ---- batch: 100 ----
mean loss: 175.69
 ---- batch: 110 ----
mean loss: 176.15
train mean loss: 174.24
epoch train time: 0:00:15.391339
elapsed time: 1:01:06.445132
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 09:21:39.090600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.80
 ---- batch: 020 ----
mean loss: 165.27
 ---- batch: 030 ----
mean loss: 169.55
 ---- batch: 040 ----
mean loss: 183.86
 ---- batch: 050 ----
mean loss: 171.20
 ---- batch: 060 ----
mean loss: 181.14
 ---- batch: 070 ----
mean loss: 176.83
 ---- batch: 080 ----
mean loss: 169.48
 ---- batch: 090 ----
mean loss: 171.79
 ---- batch: 100 ----
mean loss: 172.20
 ---- batch: 110 ----
mean loss: 172.35
train mean loss: 174.26
epoch train time: 0:00:15.384846
elapsed time: 1:01:21.830850
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 09:21:54.476268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.78
 ---- batch: 020 ----
mean loss: 177.24
 ---- batch: 030 ----
mean loss: 171.80
 ---- batch: 040 ----
mean loss: 178.75
 ---- batch: 050 ----
mean loss: 174.78
 ---- batch: 060 ----
mean loss: 185.91
 ---- batch: 070 ----
mean loss: 173.69
 ---- batch: 080 ----
mean loss: 171.13
 ---- batch: 090 ----
mean loss: 168.71
 ---- batch: 100 ----
mean loss: 171.30
 ---- batch: 110 ----
mean loss: 169.25
train mean loss: 174.05
epoch train time: 0:00:15.360832
elapsed time: 1:01:37.192634
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 09:22:09.838130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.81
 ---- batch: 020 ----
mean loss: 186.64
 ---- batch: 030 ----
mean loss: 171.94
 ---- batch: 040 ----
mean loss: 171.66
 ---- batch: 050 ----
mean loss: 175.31
 ---- batch: 060 ----
mean loss: 178.76
 ---- batch: 070 ----
mean loss: 167.38
 ---- batch: 080 ----
mean loss: 169.38
 ---- batch: 090 ----
mean loss: 177.46
 ---- batch: 100 ----
mean loss: 171.00
 ---- batch: 110 ----
mean loss: 167.98
train mean loss: 173.98
epoch train time: 0:00:15.425876
elapsed time: 1:01:52.619480
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 09:22:25.264931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.21
 ---- batch: 020 ----
mean loss: 176.16
 ---- batch: 030 ----
mean loss: 175.14
 ---- batch: 040 ----
mean loss: 178.71
 ---- batch: 050 ----
mean loss: 173.23
 ---- batch: 060 ----
mean loss: 176.16
 ---- batch: 070 ----
mean loss: 186.18
 ---- batch: 080 ----
mean loss: 163.97
 ---- batch: 090 ----
mean loss: 165.28
 ---- batch: 100 ----
mean loss: 179.53
 ---- batch: 110 ----
mean loss: 168.95
train mean loss: 174.02
epoch train time: 0:00:15.385228
elapsed time: 1:02:08.005595
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 09:22:40.651017
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.63
 ---- batch: 020 ----
mean loss: 176.95
 ---- batch: 030 ----
mean loss: 178.40
 ---- batch: 040 ----
mean loss: 173.66
 ---- batch: 050 ----
mean loss: 172.48
 ---- batch: 060 ----
mean loss: 183.90
 ---- batch: 070 ----
mean loss: 179.16
 ---- batch: 080 ----
mean loss: 181.64
 ---- batch: 090 ----
mean loss: 167.90
 ---- batch: 100 ----
mean loss: 170.74
 ---- batch: 110 ----
mean loss: 163.25
train mean loss: 174.17
epoch train time: 0:00:15.351363
elapsed time: 1:02:23.357882
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 09:22:56.003326
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.39
 ---- batch: 020 ----
mean loss: 169.18
 ---- batch: 030 ----
mean loss: 175.99
 ---- batch: 040 ----
mean loss: 168.12
 ---- batch: 050 ----
mean loss: 171.89
 ---- batch: 060 ----
mean loss: 173.97
 ---- batch: 070 ----
mean loss: 180.94
 ---- batch: 080 ----
mean loss: 173.20
 ---- batch: 090 ----
mean loss: 187.26
 ---- batch: 100 ----
mean loss: 170.70
 ---- batch: 110 ----
mean loss: 174.64
train mean loss: 174.26
epoch train time: 0:00:15.372490
elapsed time: 1:02:38.731333
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 09:23:11.376939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.66
 ---- batch: 020 ----
mean loss: 170.51
 ---- batch: 030 ----
mean loss: 174.02
 ---- batch: 040 ----
mean loss: 172.64
 ---- batch: 050 ----
mean loss: 180.07
 ---- batch: 060 ----
mean loss: 181.60
 ---- batch: 070 ----
mean loss: 175.71
 ---- batch: 080 ----
mean loss: 170.66
 ---- batch: 090 ----
mean loss: 171.90
 ---- batch: 100 ----
mean loss: 182.63
 ---- batch: 110 ----
mean loss: 167.98
train mean loss: 173.93
epoch train time: 0:00:15.374706
elapsed time: 1:02:54.107222
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 09:23:26.752629
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.93
 ---- batch: 020 ----
mean loss: 184.37
 ---- batch: 030 ----
mean loss: 182.99
 ---- batch: 040 ----
mean loss: 167.82
 ---- batch: 050 ----
mean loss: 168.28
 ---- batch: 060 ----
mean loss: 173.53
 ---- batch: 070 ----
mean loss: 176.67
 ---- batch: 080 ----
mean loss: 158.18
 ---- batch: 090 ----
mean loss: 170.88
 ---- batch: 100 ----
mean loss: 178.56
 ---- batch: 110 ----
mean loss: 176.34
train mean loss: 174.03
epoch train time: 0:00:15.398802
elapsed time: 1:03:09.506926
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 09:23:42.152346
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.81
 ---- batch: 020 ----
mean loss: 184.80
 ---- batch: 030 ----
mean loss: 166.88
 ---- batch: 040 ----
mean loss: 185.59
 ---- batch: 050 ----
mean loss: 179.91
 ---- batch: 060 ----
mean loss: 170.65
 ---- batch: 070 ----
mean loss: 179.97
 ---- batch: 080 ----
mean loss: 166.65
 ---- batch: 090 ----
mean loss: 165.30
 ---- batch: 100 ----
mean loss: 178.84
 ---- batch: 110 ----
mean loss: 170.43
train mean loss: 174.10
epoch train time: 0:00:15.418651
elapsed time: 1:03:24.926544
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 09:23:57.572025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.22
 ---- batch: 020 ----
mean loss: 173.00
 ---- batch: 030 ----
mean loss: 171.80
 ---- batch: 040 ----
mean loss: 171.18
 ---- batch: 050 ----
mean loss: 173.82
 ---- batch: 060 ----
mean loss: 170.92
 ---- batch: 070 ----
mean loss: 180.77
 ---- batch: 080 ----
mean loss: 180.08
 ---- batch: 090 ----
mean loss: 169.63
 ---- batch: 100 ----
mean loss: 178.09
 ---- batch: 110 ----
mean loss: 179.71
train mean loss: 173.89
epoch train time: 0:00:15.391530
elapsed time: 1:03:40.319099
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 09:24:12.964537
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.29
 ---- batch: 020 ----
mean loss: 170.98
 ---- batch: 030 ----
mean loss: 178.94
 ---- batch: 040 ----
mean loss: 174.49
 ---- batch: 050 ----
mean loss: 167.05
 ---- batch: 060 ----
mean loss: 178.74
 ---- batch: 070 ----
mean loss: 174.65
 ---- batch: 080 ----
mean loss: 167.27
 ---- batch: 090 ----
mean loss: 172.35
 ---- batch: 100 ----
mean loss: 178.03
 ---- batch: 110 ----
mean loss: 179.87
train mean loss: 174.08
epoch train time: 0:00:15.380585
elapsed time: 1:03:55.700677
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 09:24:28.346184
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.13
 ---- batch: 020 ----
mean loss: 177.04
 ---- batch: 030 ----
mean loss: 172.78
 ---- batch: 040 ----
mean loss: 172.85
 ---- batch: 050 ----
mean loss: 174.04
 ---- batch: 060 ----
mean loss: 171.37
 ---- batch: 070 ----
mean loss: 175.34
 ---- batch: 080 ----
mean loss: 171.35
 ---- batch: 090 ----
mean loss: 172.37
 ---- batch: 100 ----
mean loss: 173.25
 ---- batch: 110 ----
mean loss: 174.87
train mean loss: 174.33
epoch train time: 0:00:15.350827
elapsed time: 1:04:11.052396
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 09:24:43.697825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.43
 ---- batch: 020 ----
mean loss: 178.19
 ---- batch: 030 ----
mean loss: 174.18
 ---- batch: 040 ----
mean loss: 170.87
 ---- batch: 050 ----
mean loss: 170.43
 ---- batch: 060 ----
mean loss: 175.76
 ---- batch: 070 ----
mean loss: 175.15
 ---- batch: 080 ----
mean loss: 169.14
 ---- batch: 090 ----
mean loss: 173.08
 ---- batch: 100 ----
mean loss: 179.92
 ---- batch: 110 ----
mean loss: 184.48
train mean loss: 173.84
epoch train time: 0:00:15.362405
elapsed time: 1:04:26.415713
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 09:24:59.061137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.59
 ---- batch: 020 ----
mean loss: 172.22
 ---- batch: 030 ----
mean loss: 169.83
 ---- batch: 040 ----
mean loss: 173.58
 ---- batch: 050 ----
mean loss: 175.33
 ---- batch: 060 ----
mean loss: 169.37
 ---- batch: 070 ----
mean loss: 170.08
 ---- batch: 080 ----
mean loss: 174.59
 ---- batch: 090 ----
mean loss: 179.99
 ---- batch: 100 ----
mean loss: 182.19
 ---- batch: 110 ----
mean loss: 174.13
train mean loss: 173.98
epoch train time: 0:00:15.404373
elapsed time: 1:04:41.821024
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 09:25:14.466477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.84
 ---- batch: 020 ----
mean loss: 172.29
 ---- batch: 030 ----
mean loss: 168.29
 ---- batch: 040 ----
mean loss: 175.97
 ---- batch: 050 ----
mean loss: 169.19
 ---- batch: 060 ----
mean loss: 178.30
 ---- batch: 070 ----
mean loss: 171.66
 ---- batch: 080 ----
mean loss: 178.58
 ---- batch: 090 ----
mean loss: 176.84
 ---- batch: 100 ----
mean loss: 178.12
 ---- batch: 110 ----
mean loss: 173.69
train mean loss: 173.94
epoch train time: 0:00:15.417034
elapsed time: 1:04:57.248665
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_8/checkpoint.pth.tar
**** end time: 2019-09-26 09:25:29.893621 ****
