Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 28129
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-20 11:17:51.521687 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 11:17:51.538689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4023.71
 ---- batch: 020 ----
mean loss: 1605.09
 ---- batch: 030 ----
mean loss: 1299.84
 ---- batch: 040 ----
mean loss: 1190.94
train mean loss: 1968.60
epoch train time: 0:00:20.615611
elapsed time: 0:00:20.640473
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 11:18:12.162201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1075.48
 ---- batch: 020 ----
mean loss: 1085.08
 ---- batch: 030 ----
mean loss: 1066.22
 ---- batch: 040 ----
mean loss: 1006.59
train mean loss: 1058.21
epoch train time: 0:00:08.264719
elapsed time: 0:00:28.905476
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 11:18:20.427295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1039.42
 ---- batch: 020 ----
mean loss: 1002.67
 ---- batch: 030 ----
mean loss: 993.59
 ---- batch: 040 ----
mean loss: 1017.53
train mean loss: 1009.99
epoch train time: 0:00:08.168708
elapsed time: 0:00:37.074589
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 11:18:28.596457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1000.73
 ---- batch: 020 ----
mean loss: 940.78
 ---- batch: 030 ----
mean loss: 989.36
 ---- batch: 040 ----
mean loss: 953.88
train mean loss: 972.28
epoch train time: 0:00:07.971645
elapsed time: 0:00:45.046673
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 11:18:36.568527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 968.68
 ---- batch: 020 ----
mean loss: 943.76
 ---- batch: 030 ----
mean loss: 932.00
 ---- batch: 040 ----
mean loss: 902.58
train mean loss: 933.81
epoch train time: 0:00:07.948534
elapsed time: 0:00:52.995630
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 11:18:44.517489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.39
 ---- batch: 020 ----
mean loss: 818.88
 ---- batch: 030 ----
mean loss: 764.48
 ---- batch: 040 ----
mean loss: 707.87
train mean loss: 779.04
epoch train time: 0:00:07.951306
elapsed time: 0:01:00.947365
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 11:18:52.469229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 599.66
 ---- batch: 020 ----
mean loss: 524.78
 ---- batch: 030 ----
mean loss: 463.55
 ---- batch: 040 ----
mean loss: 429.66
train mean loss: 496.58
epoch train time: 0:00:08.042847
elapsed time: 0:01:08.990784
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 11:19:00.512638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.54
 ---- batch: 020 ----
mean loss: 411.23
 ---- batch: 030 ----
mean loss: 382.86
 ---- batch: 040 ----
mean loss: 370.51
train mean loss: 394.14
epoch train time: 0:00:08.088289
elapsed time: 0:01:17.079581
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 11:19:08.601469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.00
 ---- batch: 020 ----
mean loss: 366.78
 ---- batch: 030 ----
mean loss: 341.95
 ---- batch: 040 ----
mean loss: 358.33
train mean loss: 358.54
epoch train time: 0:00:08.016859
elapsed time: 0:01:25.097195
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 11:19:16.619052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.08
 ---- batch: 020 ----
mean loss: 330.62
 ---- batch: 030 ----
mean loss: 327.64
 ---- batch: 040 ----
mean loss: 324.88
train mean loss: 331.53
epoch train time: 0:00:08.002883
elapsed time: 0:01:33.100568
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 11:19:24.622435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.17
 ---- batch: 020 ----
mean loss: 315.63
 ---- batch: 030 ----
mean loss: 323.87
 ---- batch: 040 ----
mean loss: 296.25
train mean loss: 314.04
epoch train time: 0:00:07.983458
elapsed time: 0:01:41.084497
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 11:19:32.606338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.01
 ---- batch: 020 ----
mean loss: 314.86
 ---- batch: 030 ----
mean loss: 297.63
 ---- batch: 040 ----
mean loss: 295.79
train mean loss: 304.54
epoch train time: 0:00:07.934120
elapsed time: 0:01:49.019022
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 11:19:40.540879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.16
 ---- batch: 020 ----
mean loss: 290.78
 ---- batch: 030 ----
mean loss: 291.77
 ---- batch: 040 ----
mean loss: 293.09
train mean loss: 295.27
epoch train time: 0:00:07.862321
elapsed time: 0:01:56.881790
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 11:19:48.403642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.09
 ---- batch: 020 ----
mean loss: 289.31
 ---- batch: 030 ----
mean loss: 282.88
 ---- batch: 040 ----
mean loss: 280.62
train mean loss: 287.07
epoch train time: 0:00:07.959068
elapsed time: 0:02:04.841269
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 11:19:56.363116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.83
 ---- batch: 020 ----
mean loss: 277.57
 ---- batch: 030 ----
mean loss: 278.50
 ---- batch: 040 ----
mean loss: 267.07
train mean loss: 276.30
epoch train time: 0:00:08.065055
elapsed time: 0:02:12.906859
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 11:20:04.428740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.81
 ---- batch: 020 ----
mean loss: 267.77
 ---- batch: 030 ----
mean loss: 266.48
 ---- batch: 040 ----
mean loss: 270.24
train mean loss: 271.81
epoch train time: 0:00:07.989398
elapsed time: 0:02:20.896817
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 11:20:12.418706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.56
 ---- batch: 020 ----
mean loss: 271.93
 ---- batch: 030 ----
mean loss: 267.96
 ---- batch: 040 ----
mean loss: 258.44
train mean loss: 267.31
epoch train time: 0:00:07.979457
elapsed time: 0:02:28.876792
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 11:20:20.398674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.37
 ---- batch: 020 ----
mean loss: 265.96
 ---- batch: 030 ----
mean loss: 268.00
 ---- batch: 040 ----
mean loss: 259.65
train mean loss: 262.63
epoch train time: 0:00:07.935187
elapsed time: 0:02:36.812458
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 11:20:28.334311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.99
 ---- batch: 020 ----
mean loss: 264.92
 ---- batch: 030 ----
mean loss: 258.61
 ---- batch: 040 ----
mean loss: 251.65
train mean loss: 260.54
epoch train time: 0:00:07.918638
elapsed time: 0:02:44.731563
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 11:20:36.253414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.81
 ---- batch: 020 ----
mean loss: 253.03
 ---- batch: 030 ----
mean loss: 249.16
 ---- batch: 040 ----
mean loss: 261.94
train mean loss: 253.40
epoch train time: 0:00:07.970321
elapsed time: 0:02:52.702340
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 11:20:44.224173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.36
 ---- batch: 020 ----
mean loss: 251.29
 ---- batch: 030 ----
mean loss: 252.19
 ---- batch: 040 ----
mean loss: 244.88
train mean loss: 248.45
epoch train time: 0:00:07.955548
elapsed time: 0:03:00.658297
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 11:20:52.180133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.54
 ---- batch: 020 ----
mean loss: 250.90
 ---- batch: 030 ----
mean loss: 244.03
 ---- batch: 040 ----
mean loss: 237.88
train mean loss: 243.75
epoch train time: 0:00:07.875853
elapsed time: 0:03:08.534570
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 11:21:00.056362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.09
 ---- batch: 020 ----
mean loss: 245.73
 ---- batch: 030 ----
mean loss: 231.30
 ---- batch: 040 ----
mean loss: 239.49
train mean loss: 241.75
epoch train time: 0:00:07.758207
elapsed time: 0:03:16.293221
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 11:21:07.815100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.16
 ---- batch: 020 ----
mean loss: 244.31
 ---- batch: 030 ----
mean loss: 235.37
 ---- batch: 040 ----
mean loss: 239.70
train mean loss: 238.72
epoch train time: 0:00:07.737878
elapsed time: 0:03:24.031543
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 11:21:15.553420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.14
 ---- batch: 020 ----
mean loss: 231.37
 ---- batch: 030 ----
mean loss: 240.97
 ---- batch: 040 ----
mean loss: 236.70
train mean loss: 234.10
epoch train time: 0:00:07.745932
elapsed time: 0:03:31.777925
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 11:21:23.299779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.50
 ---- batch: 020 ----
mean loss: 227.45
 ---- batch: 030 ----
mean loss: 231.61
 ---- batch: 040 ----
mean loss: 230.60
train mean loss: 233.04
epoch train time: 0:00:07.777518
elapsed time: 0:03:39.555990
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 11:21:31.077857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.20
 ---- batch: 020 ----
mean loss: 224.96
 ---- batch: 030 ----
mean loss: 235.40
 ---- batch: 040 ----
mean loss: 224.41
train mean loss: 225.78
epoch train time: 0:00:07.676448
elapsed time: 0:03:47.232945
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 11:21:38.754834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.52
 ---- batch: 020 ----
mean loss: 232.19
 ---- batch: 030 ----
mean loss: 229.17
 ---- batch: 040 ----
mean loss: 214.49
train mean loss: 224.36
epoch train time: 0:00:07.797533
elapsed time: 0:03:55.031096
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 11:21:46.552988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.70
 ---- batch: 020 ----
mean loss: 220.55
 ---- batch: 030 ----
mean loss: 224.43
 ---- batch: 040 ----
mean loss: 224.53
train mean loss: 221.40
epoch train time: 0:00:07.985225
elapsed time: 0:04:03.016839
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 11:21:54.538751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.54
 ---- batch: 020 ----
mean loss: 225.15
 ---- batch: 030 ----
mean loss: 211.09
 ---- batch: 040 ----
mean loss: 211.12
train mean loss: 217.29
epoch train time: 0:00:07.735807
elapsed time: 0:04:10.753124
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 11:22:02.274994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.91
 ---- batch: 020 ----
mean loss: 210.99
 ---- batch: 030 ----
mean loss: 221.40
 ---- batch: 040 ----
mean loss: 216.36
train mean loss: 215.46
epoch train time: 0:00:07.752852
elapsed time: 0:04:18.506410
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 11:22:10.028232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.87
 ---- batch: 020 ----
mean loss: 207.84
 ---- batch: 030 ----
mean loss: 213.54
 ---- batch: 040 ----
mean loss: 207.65
train mean loss: 213.19
epoch train time: 0:00:07.770697
elapsed time: 0:04:26.277568
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 11:22:17.799423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.37
 ---- batch: 020 ----
mean loss: 201.31
 ---- batch: 030 ----
mean loss: 208.73
 ---- batch: 040 ----
mean loss: 211.17
train mean loss: 210.23
epoch train time: 0:00:07.735046
elapsed time: 0:04:34.013022
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 11:22:25.534867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.21
 ---- batch: 020 ----
mean loss: 213.44
 ---- batch: 030 ----
mean loss: 204.45
 ---- batch: 040 ----
mean loss: 205.78
train mean loss: 208.60
epoch train time: 0:00:07.715996
elapsed time: 0:04:41.729583
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 11:22:33.251419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.17
 ---- batch: 020 ----
mean loss: 210.10
 ---- batch: 030 ----
mean loss: 201.83
 ---- batch: 040 ----
mean loss: 202.97
train mean loss: 203.49
epoch train time: 0:00:07.719869
elapsed time: 0:04:49.449861
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 11:22:40.971694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.10
 ---- batch: 020 ----
mean loss: 199.74
 ---- batch: 030 ----
mean loss: 202.77
 ---- batch: 040 ----
mean loss: 200.99
train mean loss: 200.53
epoch train time: 0:00:07.715806
elapsed time: 0:04:57.166115
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 11:22:48.687943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.84
 ---- batch: 020 ----
mean loss: 206.82
 ---- batch: 030 ----
mean loss: 206.48
 ---- batch: 040 ----
mean loss: 202.79
train mean loss: 202.57
epoch train time: 0:00:07.741021
elapsed time: 0:05:04.907544
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 11:22:56.429390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.09
 ---- batch: 020 ----
mean loss: 190.34
 ---- batch: 030 ----
mean loss: 196.06
 ---- batch: 040 ----
mean loss: 197.57
train mean loss: 196.94
epoch train time: 0:00:07.747789
elapsed time: 0:05:12.655835
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 11:23:04.177717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.06
 ---- batch: 020 ----
mean loss: 194.82
 ---- batch: 030 ----
mean loss: 193.84
 ---- batch: 040 ----
mean loss: 196.57
train mean loss: 196.10
epoch train time: 0:00:07.746967
elapsed time: 0:05:20.403340
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 11:23:11.925190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.48
 ---- batch: 020 ----
mean loss: 190.92
 ---- batch: 030 ----
mean loss: 192.84
 ---- batch: 040 ----
mean loss: 185.56
train mean loss: 191.69
epoch train time: 0:00:07.862562
elapsed time: 0:05:28.266392
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 11:23:19.788255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.18
 ---- batch: 020 ----
mean loss: 193.55
 ---- batch: 030 ----
mean loss: 197.87
 ---- batch: 040 ----
mean loss: 185.62
train mean loss: 193.05
epoch train time: 0:00:07.911063
elapsed time: 0:05:36.177884
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 11:23:27.699739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.96
 ---- batch: 020 ----
mean loss: 185.13
 ---- batch: 030 ----
mean loss: 201.01
 ---- batch: 040 ----
mean loss: 182.74
train mean loss: 189.00
epoch train time: 0:00:07.722406
elapsed time: 0:05:43.900761
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 11:23:35.422630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.53
 ---- batch: 020 ----
mean loss: 181.56
 ---- batch: 030 ----
mean loss: 189.52
 ---- batch: 040 ----
mean loss: 185.92
train mean loss: 186.59
epoch train time: 0:00:07.566734
elapsed time: 0:05:51.467921
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 11:23:42.989712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.63
 ---- batch: 020 ----
mean loss: 183.25
 ---- batch: 030 ----
mean loss: 179.90
 ---- batch: 040 ----
mean loss: 187.44
train mean loss: 185.07
epoch train time: 0:00:07.573051
elapsed time: 0:05:59.041312
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 11:23:50.563154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.89
 ---- batch: 020 ----
mean loss: 185.85
 ---- batch: 030 ----
mean loss: 182.32
 ---- batch: 040 ----
mean loss: 179.01
train mean loss: 183.06
epoch train time: 0:00:07.584695
elapsed time: 0:06:06.626445
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 11:23:58.148308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.58
 ---- batch: 020 ----
mean loss: 181.58
 ---- batch: 030 ----
mean loss: 177.37
 ---- batch: 040 ----
mean loss: 180.93
train mean loss: 179.32
epoch train time: 0:00:07.703095
elapsed time: 0:06:14.330085
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 11:24:05.851895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.39
 ---- batch: 020 ----
mean loss: 178.50
 ---- batch: 030 ----
mean loss: 180.61
 ---- batch: 040 ----
mean loss: 175.50
train mean loss: 178.38
epoch train time: 0:00:08.063729
elapsed time: 0:06:22.394228
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 11:24:13.916064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.00
 ---- batch: 020 ----
mean loss: 173.34
 ---- batch: 030 ----
mean loss: 185.43
 ---- batch: 040 ----
mean loss: 171.74
train mean loss: 177.11
epoch train time: 0:00:08.010424
elapsed time: 0:06:30.405108
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 11:24:21.927016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.06
 ---- batch: 020 ----
mean loss: 172.39
 ---- batch: 030 ----
mean loss: 172.85
 ---- batch: 040 ----
mean loss: 178.46
train mean loss: 174.70
epoch train time: 0:00:07.991147
elapsed time: 0:06:38.396783
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 11:24:29.918626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.54
 ---- batch: 020 ----
mean loss: 169.99
 ---- batch: 030 ----
mean loss: 167.24
 ---- batch: 040 ----
mean loss: 171.16
train mean loss: 171.48
epoch train time: 0:00:08.047705
elapsed time: 0:06:46.444966
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 11:24:37.966821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.07
 ---- batch: 020 ----
mean loss: 168.48
 ---- batch: 030 ----
mean loss: 169.11
 ---- batch: 040 ----
mean loss: 173.66
train mean loss: 171.04
epoch train time: 0:00:08.047327
elapsed time: 0:06:54.492767
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 11:24:46.014627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.69
 ---- batch: 020 ----
mean loss: 165.81
 ---- batch: 030 ----
mean loss: 175.41
 ---- batch: 040 ----
mean loss: 162.53
train mean loss: 167.73
epoch train time: 0:00:08.044182
elapsed time: 0:07:02.537397
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 11:24:54.059303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.41
 ---- batch: 020 ----
mean loss: 161.20
 ---- batch: 030 ----
mean loss: 163.53
 ---- batch: 040 ----
mean loss: 165.85
train mean loss: 167.31
epoch train time: 0:00:08.036689
elapsed time: 0:07:10.574581
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 11:25:02.096476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.43
 ---- batch: 020 ----
mean loss: 165.18
 ---- batch: 030 ----
mean loss: 164.19
 ---- batch: 040 ----
mean loss: 165.78
train mean loss: 164.87
epoch train time: 0:00:07.994601
elapsed time: 0:07:18.569728
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 11:25:10.091585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.90
 ---- batch: 020 ----
mean loss: 167.55
 ---- batch: 030 ----
mean loss: 171.36
 ---- batch: 040 ----
mean loss: 162.03
train mean loss: 166.73
epoch train time: 0:00:07.843670
elapsed time: 0:07:26.413857
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 11:25:17.935696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.82
 ---- batch: 020 ----
mean loss: 159.68
 ---- batch: 030 ----
mean loss: 157.92
 ---- batch: 040 ----
mean loss: 150.00
train mean loss: 159.18
epoch train time: 0:00:07.962214
elapsed time: 0:07:34.376498
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 11:25:25.898342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.30
 ---- batch: 020 ----
mean loss: 155.07
 ---- batch: 030 ----
mean loss: 155.73
 ---- batch: 040 ----
mean loss: 162.19
train mean loss: 159.07
epoch train time: 0:00:07.955884
elapsed time: 0:07:42.332808
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 11:25:33.854636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.81
 ---- batch: 020 ----
mean loss: 157.12
 ---- batch: 030 ----
mean loss: 157.80
 ---- batch: 040 ----
mean loss: 157.51
train mean loss: 157.43
epoch train time: 0:00:07.987292
elapsed time: 0:07:50.320572
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 11:25:41.842434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.98
 ---- batch: 020 ----
mean loss: 155.64
 ---- batch: 030 ----
mean loss: 157.13
 ---- batch: 040 ----
mean loss: 155.95
train mean loss: 154.85
epoch train time: 0:00:07.974679
elapsed time: 0:07:58.295870
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 11:25:49.817819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.02
 ---- batch: 020 ----
mean loss: 163.02
 ---- batch: 030 ----
mean loss: 152.11
 ---- batch: 040 ----
mean loss: 152.85
train mean loss: 155.23
epoch train time: 0:00:07.928981
elapsed time: 0:08:06.225389
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 11:25:57.747231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.10
 ---- batch: 020 ----
mean loss: 152.20
 ---- batch: 030 ----
mean loss: 154.05
 ---- batch: 040 ----
mean loss: 154.79
train mean loss: 153.55
epoch train time: 0:00:07.922414
elapsed time: 0:08:14.148227
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 11:26:05.670068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.53
 ---- batch: 020 ----
mean loss: 152.09
 ---- batch: 030 ----
mean loss: 145.34
 ---- batch: 040 ----
mean loss: 159.61
train mean loss: 151.99
epoch train time: 0:00:07.896388
elapsed time: 0:08:22.045048
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 11:26:13.566902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.91
 ---- batch: 020 ----
mean loss: 151.65
 ---- batch: 030 ----
mean loss: 148.89
 ---- batch: 040 ----
mean loss: 154.18
train mean loss: 151.46
epoch train time: 0:00:07.960548
elapsed time: 0:08:30.006044
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 11:26:21.527884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.03
 ---- batch: 020 ----
mean loss: 150.21
 ---- batch: 030 ----
mean loss: 149.17
 ---- batch: 040 ----
mean loss: 143.95
train mean loss: 146.69
epoch train time: 0:00:08.024158
elapsed time: 0:08:38.030652
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 11:26:29.552520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.39
 ---- batch: 020 ----
mean loss: 143.29
 ---- batch: 030 ----
mean loss: 147.19
 ---- batch: 040 ----
mean loss: 141.57
train mean loss: 145.34
epoch train time: 0:00:07.942717
elapsed time: 0:08:45.973860
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 11:26:37.495708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.35
 ---- batch: 020 ----
mean loss: 147.07
 ---- batch: 030 ----
mean loss: 140.58
 ---- batch: 040 ----
mean loss: 147.77
train mean loss: 144.85
epoch train time: 0:00:07.904824
elapsed time: 0:08:53.879107
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 11:26:45.401030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.73
 ---- batch: 020 ----
mean loss: 136.91
 ---- batch: 030 ----
mean loss: 142.53
 ---- batch: 040 ----
mean loss: 146.63
train mean loss: 142.43
epoch train time: 0:00:07.958382
elapsed time: 0:09:01.838037
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 11:26:53.359901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.86
 ---- batch: 020 ----
mean loss: 141.10
 ---- batch: 030 ----
mean loss: 146.09
 ---- batch: 040 ----
mean loss: 140.50
train mean loss: 142.55
epoch train time: 0:00:07.938708
elapsed time: 0:09:09.777236
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 11:27:01.299090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.10
 ---- batch: 020 ----
mean loss: 137.15
 ---- batch: 030 ----
mean loss: 141.33
 ---- batch: 040 ----
mean loss: 143.36
train mean loss: 141.09
epoch train time: 0:00:07.974677
elapsed time: 0:09:17.752374
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 11:27:09.274215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.41
 ---- batch: 020 ----
mean loss: 138.03
 ---- batch: 030 ----
mean loss: 141.27
 ---- batch: 040 ----
mean loss: 137.69
train mean loss: 139.52
epoch train time: 0:00:07.922333
elapsed time: 0:09:25.675132
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 11:27:17.196976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.03
 ---- batch: 020 ----
mean loss: 136.58
 ---- batch: 030 ----
mean loss: 139.83
 ---- batch: 040 ----
mean loss: 138.50
train mean loss: 137.37
epoch train time: 0:00:07.853462
elapsed time: 0:09:33.529008
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 11:27:25.050867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.31
 ---- batch: 020 ----
mean loss: 136.13
 ---- batch: 030 ----
mean loss: 138.62
 ---- batch: 040 ----
mean loss: 133.98
train mean loss: 137.19
epoch train time: 0:00:07.879678
elapsed time: 0:09:41.409146
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 11:27:32.930999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.77
 ---- batch: 020 ----
mean loss: 140.41
 ---- batch: 030 ----
mean loss: 132.62
 ---- batch: 040 ----
mean loss: 135.51
train mean loss: 135.40
epoch train time: 0:00:08.029123
elapsed time: 0:09:49.438727
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 11:27:40.960574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.37
 ---- batch: 020 ----
mean loss: 131.42
 ---- batch: 030 ----
mean loss: 134.28
 ---- batch: 040 ----
mean loss: 138.12
train mean loss: 135.57
epoch train time: 0:00:08.012037
elapsed time: 0:09:57.451194
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 11:27:48.973039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.87
 ---- batch: 020 ----
mean loss: 131.44
 ---- batch: 030 ----
mean loss: 132.55
 ---- batch: 040 ----
mean loss: 134.83
train mean loss: 133.73
epoch train time: 0:00:07.829513
elapsed time: 0:10:05.281152
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 11:27:56.802996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.83
 ---- batch: 020 ----
mean loss: 133.44
 ---- batch: 030 ----
mean loss: 130.98
 ---- batch: 040 ----
mean loss: 129.22
train mean loss: 131.67
epoch train time: 0:00:07.822165
elapsed time: 0:10:13.103735
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 11:28:04.625593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.20
 ---- batch: 020 ----
mean loss: 131.95
 ---- batch: 030 ----
mean loss: 132.87
 ---- batch: 040 ----
mean loss: 135.71
train mean loss: 133.44
epoch train time: 0:00:07.790202
elapsed time: 0:10:20.894418
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 11:28:12.416279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.18
 ---- batch: 020 ----
mean loss: 135.58
 ---- batch: 030 ----
mean loss: 129.83
 ---- batch: 040 ----
mean loss: 127.32
train mean loss: 129.90
epoch train time: 0:00:07.823872
elapsed time: 0:10:28.718844
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 11:28:20.240737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.54
 ---- batch: 020 ----
mean loss: 129.81
 ---- batch: 030 ----
mean loss: 129.35
 ---- batch: 040 ----
mean loss: 126.00
train mean loss: 129.25
epoch train time: 0:00:07.916974
elapsed time: 0:10:36.636348
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 11:28:28.158317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.20
 ---- batch: 020 ----
mean loss: 125.64
 ---- batch: 030 ----
mean loss: 134.46
 ---- batch: 040 ----
mean loss: 132.06
train mean loss: 130.33
epoch train time: 0:00:07.974735
elapsed time: 0:10:44.611693
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 11:28:36.133497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.07
 ---- batch: 020 ----
mean loss: 126.15
 ---- batch: 030 ----
mean loss: 127.37
 ---- batch: 040 ----
mean loss: 126.64
train mean loss: 128.29
epoch train time: 0:00:07.949242
elapsed time: 0:10:52.561333
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 11:28:44.083190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.34
 ---- batch: 020 ----
mean loss: 129.85
 ---- batch: 030 ----
mean loss: 127.09
 ---- batch: 040 ----
mean loss: 125.75
train mean loss: 126.55
epoch train time: 0:00:07.786440
elapsed time: 0:11:00.348332
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 11:28:51.870207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.85
 ---- batch: 020 ----
mean loss: 129.22
 ---- batch: 030 ----
mean loss: 130.55
 ---- batch: 040 ----
mean loss: 126.42
train mean loss: 128.59
epoch train time: 0:00:07.824066
elapsed time: 0:11:08.172860
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 11:28:59.694739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.89
 ---- batch: 020 ----
mean loss: 126.28
 ---- batch: 030 ----
mean loss: 125.33
 ---- batch: 040 ----
mean loss: 127.54
train mean loss: 124.68
epoch train time: 0:00:07.840452
elapsed time: 0:11:16.013762
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 11:29:07.535609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.34
 ---- batch: 020 ----
mean loss: 126.07
 ---- batch: 030 ----
mean loss: 123.85
 ---- batch: 040 ----
mean loss: 121.10
train mean loss: 123.88
epoch train time: 0:00:08.034581
elapsed time: 0:11:24.048771
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 11:29:15.570561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.50
 ---- batch: 020 ----
mean loss: 125.27
 ---- batch: 030 ----
mean loss: 121.74
 ---- batch: 040 ----
mean loss: 127.03
train mean loss: 122.84
epoch train time: 0:00:07.996904
elapsed time: 0:11:32.046045
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 11:29:23.567895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.92
 ---- batch: 020 ----
mean loss: 121.84
 ---- batch: 030 ----
mean loss: 123.25
 ---- batch: 040 ----
mean loss: 122.82
train mean loss: 122.82
epoch train time: 0:00:07.983572
elapsed time: 0:11:40.030031
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 11:29:31.551875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.13
 ---- batch: 020 ----
mean loss: 119.83
 ---- batch: 030 ----
mean loss: 127.41
 ---- batch: 040 ----
mean loss: 120.48
train mean loss: 121.44
epoch train time: 0:00:08.000479
elapsed time: 0:11:48.030955
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 11:29:39.552837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.07
 ---- batch: 020 ----
mean loss: 124.41
 ---- batch: 030 ----
mean loss: 118.82
 ---- batch: 040 ----
mean loss: 115.62
train mean loss: 120.86
epoch train time: 0:00:07.814384
elapsed time: 0:11:55.845858
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 11:29:47.367699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.22
 ---- batch: 020 ----
mean loss: 118.72
 ---- batch: 030 ----
mean loss: 120.29
 ---- batch: 040 ----
mean loss: 121.79
train mean loss: 120.71
epoch train time: 0:00:07.860864
elapsed time: 0:12:03.707236
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 11:29:55.229119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.35
 ---- batch: 020 ----
mean loss: 123.87
 ---- batch: 030 ----
mean loss: 116.54
 ---- batch: 040 ----
mean loss: 121.61
train mean loss: 120.34
epoch train time: 0:00:07.921167
elapsed time: 0:12:11.628961
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 11:30:03.150770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.51
 ---- batch: 020 ----
mean loss: 119.91
 ---- batch: 030 ----
mean loss: 116.40
 ---- batch: 040 ----
mean loss: 118.31
train mean loss: 119.16
epoch train time: 0:00:07.848714
elapsed time: 0:12:19.478065
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 11:30:10.999908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.49
 ---- batch: 020 ----
mean loss: 118.89
 ---- batch: 030 ----
mean loss: 120.81
 ---- batch: 040 ----
mean loss: 118.39
train mean loss: 118.56
epoch train time: 0:00:07.843205
elapsed time: 0:12:27.321729
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 11:30:18.843575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.07
 ---- batch: 020 ----
mean loss: 118.64
 ---- batch: 030 ----
mean loss: 117.93
 ---- batch: 040 ----
mean loss: 120.51
train mean loss: 118.44
epoch train time: 0:00:07.865648
elapsed time: 0:12:35.187802
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 11:30:26.709654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.59
 ---- batch: 020 ----
mean loss: 120.51
 ---- batch: 030 ----
mean loss: 113.68
 ---- batch: 040 ----
mean loss: 121.04
train mean loss: 117.56
epoch train time: 0:00:08.001963
elapsed time: 0:12:43.190222
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 11:30:34.712086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.42
 ---- batch: 020 ----
mean loss: 122.25
 ---- batch: 030 ----
mean loss: 110.91
 ---- batch: 040 ----
mean loss: 114.79
train mean loss: 117.72
epoch train time: 0:00:07.821596
elapsed time: 0:12:51.012256
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 11:30:42.534112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.30
 ---- batch: 020 ----
mean loss: 114.92
 ---- batch: 030 ----
mean loss: 111.48
 ---- batch: 040 ----
mean loss: 116.26
train mean loss: 115.39
epoch train time: 0:00:07.789610
elapsed time: 0:12:58.802293
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 11:30:50.324136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.64
 ---- batch: 020 ----
mean loss: 109.05
 ---- batch: 030 ----
mean loss: 113.25
 ---- batch: 040 ----
mean loss: 113.65
train mean loss: 113.86
epoch train time: 0:00:07.805687
elapsed time: 0:13:06.608400
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 11:30:58.130263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.42
 ---- batch: 020 ----
mean loss: 116.13
 ---- batch: 030 ----
mean loss: 117.55
 ---- batch: 040 ----
mean loss: 115.38
train mean loss: 114.65
epoch train time: 0:00:07.801784
elapsed time: 0:13:14.410646
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 11:31:05.932512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.59
 ---- batch: 020 ----
mean loss: 116.48
 ---- batch: 030 ----
mean loss: 112.49
 ---- batch: 040 ----
mean loss: 112.70
train mean loss: 113.99
epoch train time: 0:00:07.871964
elapsed time: 0:13:22.283120
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 11:31:13.804970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.96
 ---- batch: 020 ----
mean loss: 116.08
 ---- batch: 030 ----
mean loss: 113.45
 ---- batch: 040 ----
mean loss: 115.24
train mean loss: 113.83
epoch train time: 0:00:07.972513
elapsed time: 0:13:30.256116
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 11:31:21.777942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.88
 ---- batch: 020 ----
mean loss: 114.13
 ---- batch: 030 ----
mean loss: 113.46
 ---- batch: 040 ----
mean loss: 113.73
train mean loss: 112.95
epoch train time: 0:00:07.903501
elapsed time: 0:13:38.160101
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 11:31:29.681958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.68
 ---- batch: 020 ----
mean loss: 108.69
 ---- batch: 030 ----
mean loss: 111.81
 ---- batch: 040 ----
mean loss: 117.64
train mean loss: 112.38
epoch train time: 0:00:07.847175
elapsed time: 0:13:46.007706
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 11:31:37.529539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.84
 ---- batch: 020 ----
mean loss: 112.87
 ---- batch: 030 ----
mean loss: 112.14
 ---- batch: 040 ----
mean loss: 106.85
train mean loss: 111.29
epoch train time: 0:00:07.710711
elapsed time: 0:13:53.718831
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 11:31:45.240661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.00
 ---- batch: 020 ----
mean loss: 112.14
 ---- batch: 030 ----
mean loss: 104.98
 ---- batch: 040 ----
mean loss: 113.46
train mean loss: 109.66
epoch train time: 0:00:07.508802
elapsed time: 0:14:01.228046
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 11:31:52.749871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.63
 ---- batch: 020 ----
mean loss: 107.35
 ---- batch: 030 ----
mean loss: 107.40
 ---- batch: 040 ----
mean loss: 110.58
train mean loss: 109.36
epoch train time: 0:00:07.720895
elapsed time: 0:14:08.949325
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 11:32:00.471222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.84
 ---- batch: 020 ----
mean loss: 108.57
 ---- batch: 030 ----
mean loss: 109.20
 ---- batch: 040 ----
mean loss: 105.39
train mean loss: 108.30
epoch train time: 0:00:07.961680
elapsed time: 0:14:16.911557
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 11:32:08.433366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.96
 ---- batch: 020 ----
mean loss: 108.29
 ---- batch: 030 ----
mean loss: 106.92
 ---- batch: 040 ----
mean loss: 104.44
train mean loss: 107.47
epoch train time: 0:00:07.960768
elapsed time: 0:14:24.872756
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 11:32:16.394596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.59
 ---- batch: 020 ----
mean loss: 108.31
 ---- batch: 030 ----
mean loss: 106.85
 ---- batch: 040 ----
mean loss: 112.54
train mean loss: 108.33
epoch train time: 0:00:07.953222
elapsed time: 0:14:32.826406
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 11:32:24.348254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.63
 ---- batch: 020 ----
mean loss: 105.70
 ---- batch: 030 ----
mean loss: 108.14
 ---- batch: 040 ----
mean loss: 102.52
train mean loss: 107.22
epoch train time: 0:00:07.944755
elapsed time: 0:14:40.771620
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 11:32:32.293447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.79
 ---- batch: 020 ----
mean loss: 106.61
 ---- batch: 030 ----
mean loss: 105.59
 ---- batch: 040 ----
mean loss: 106.23
train mean loss: 106.61
epoch train time: 0:00:07.971258
elapsed time: 0:14:48.743485
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 11:32:40.265336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.90
 ---- batch: 020 ----
mean loss: 106.57
 ---- batch: 030 ----
mean loss: 108.45
 ---- batch: 040 ----
mean loss: 106.24
train mean loss: 106.69
epoch train time: 0:00:08.003846
elapsed time: 0:14:56.747802
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 11:32:48.269646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.99
 ---- batch: 020 ----
mean loss: 108.23
 ---- batch: 030 ----
mean loss: 107.19
 ---- batch: 040 ----
mean loss: 104.12
train mean loss: 106.81
epoch train time: 0:00:07.866064
elapsed time: 0:15:04.614283
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 11:32:56.136121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.08
 ---- batch: 020 ----
mean loss: 103.81
 ---- batch: 030 ----
mean loss: 105.13
 ---- batch: 040 ----
mean loss: 103.23
train mean loss: 104.82
epoch train time: 0:00:07.875744
elapsed time: 0:15:12.490480
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 11:33:04.012323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.04
 ---- batch: 020 ----
mean loss: 100.70
 ---- batch: 030 ----
mean loss: 105.68
 ---- batch: 040 ----
mean loss: 103.27
train mean loss: 103.92
epoch train time: 0:00:07.846603
elapsed time: 0:15:20.337555
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 11:33:11.859407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.23
 ---- batch: 020 ----
mean loss: 100.37
 ---- batch: 030 ----
mean loss: 105.09
 ---- batch: 040 ----
mean loss: 102.27
train mean loss: 103.37
epoch train time: 0:00:07.841841
elapsed time: 0:15:28.179911
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 11:33:19.701786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.40
 ---- batch: 020 ----
mean loss: 103.28
 ---- batch: 030 ----
mean loss: 103.82
 ---- batch: 040 ----
mean loss: 105.17
train mean loss: 104.36
epoch train time: 0:00:07.804055
elapsed time: 0:15:35.984488
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 11:33:27.506367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.83
 ---- batch: 020 ----
mean loss: 108.88
 ---- batch: 030 ----
mean loss: 102.09
 ---- batch: 040 ----
mean loss: 103.59
train mean loss: 104.28
epoch train time: 0:00:07.809700
elapsed time: 0:15:43.794727
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 11:33:35.316605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.97
 ---- batch: 020 ----
mean loss: 101.33
 ---- batch: 030 ----
mean loss: 103.85
 ---- batch: 040 ----
mean loss: 97.05
train mean loss: 101.19
epoch train time: 0:00:07.827671
elapsed time: 0:15:51.622904
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 11:33:43.144777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.10
 ---- batch: 020 ----
mean loss: 104.93
 ---- batch: 030 ----
mean loss: 100.57
 ---- batch: 040 ----
mean loss: 99.44
train mean loss: 101.65
epoch train time: 0:00:07.958852
elapsed time: 0:15:59.582289
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 11:33:51.104120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.17
 ---- batch: 020 ----
mean loss: 102.10
 ---- batch: 030 ----
mean loss: 104.71
 ---- batch: 040 ----
mean loss: 100.58
train mean loss: 101.38
epoch train time: 0:00:07.805174
elapsed time: 0:16:07.387897
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 11:33:58.909695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.78
 ---- batch: 020 ----
mean loss: 102.82
 ---- batch: 030 ----
mean loss: 100.41
 ---- batch: 040 ----
mean loss: 100.27
train mean loss: 100.76
epoch train time: 0:00:07.865122
elapsed time: 0:16:15.253515
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 11:34:06.775408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.27
 ---- batch: 020 ----
mean loss: 101.04
 ---- batch: 030 ----
mean loss: 101.58
 ---- batch: 040 ----
mean loss: 99.40
train mean loss: 100.38
epoch train time: 0:00:07.888263
elapsed time: 0:16:23.142297
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 11:34:14.664130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.24
 ---- batch: 020 ----
mean loss: 98.20
 ---- batch: 030 ----
mean loss: 101.40
 ---- batch: 040 ----
mean loss: 100.39
train mean loss: 99.99
epoch train time: 0:00:07.823196
elapsed time: 0:16:30.965924
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 11:34:22.487802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.82
 ---- batch: 020 ----
mean loss: 102.01
 ---- batch: 030 ----
mean loss: 97.14
 ---- batch: 040 ----
mean loss: 99.72
train mean loss: 99.80
epoch train time: 0:00:07.807273
elapsed time: 0:16:38.773650
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 11:34:30.295494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.03
 ---- batch: 020 ----
mean loss: 97.36
 ---- batch: 030 ----
mean loss: 104.16
 ---- batch: 040 ----
mean loss: 97.63
train mean loss: 98.16
epoch train time: 0:00:07.866584
elapsed time: 0:16:46.640746
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 11:34:38.162619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.41
 ---- batch: 020 ----
mean loss: 100.03
 ---- batch: 030 ----
mean loss: 100.53
 ---- batch: 040 ----
mean loss: 93.57
train mean loss: 97.63
epoch train time: 0:00:07.908711
elapsed time: 0:16:54.549963
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 11:34:46.071693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.88
 ---- batch: 020 ----
mean loss: 95.09
 ---- batch: 030 ----
mean loss: 97.04
 ---- batch: 040 ----
mean loss: 99.71
train mean loss: 98.24
epoch train time: 0:00:07.733483
elapsed time: 0:17:02.283781
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 11:34:53.805626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.32
 ---- batch: 020 ----
mean loss: 98.44
 ---- batch: 030 ----
mean loss: 98.73
 ---- batch: 040 ----
mean loss: 96.08
train mean loss: 96.79
epoch train time: 0:00:07.751984
elapsed time: 0:17:10.036227
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 11:35:01.558064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.51
 ---- batch: 020 ----
mean loss: 96.29
 ---- batch: 030 ----
mean loss: 96.72
 ---- batch: 040 ----
mean loss: 95.82
train mean loss: 96.60
epoch train time: 0:00:07.755012
elapsed time: 0:17:17.791642
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 11:35:09.313480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.26
 ---- batch: 020 ----
mean loss: 98.33
 ---- batch: 030 ----
mean loss: 94.23
 ---- batch: 040 ----
mean loss: 93.81
train mean loss: 96.02
epoch train time: 0:00:07.727060
elapsed time: 0:17:25.519156
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 11:35:17.041020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.22
 ---- batch: 020 ----
mean loss: 92.56
 ---- batch: 030 ----
mean loss: 94.41
 ---- batch: 040 ----
mean loss: 96.76
train mean loss: 96.35
epoch train time: 0:00:07.655256
elapsed time: 0:17:33.174908
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 11:35:24.696745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.67
 ---- batch: 020 ----
mean loss: 94.72
 ---- batch: 030 ----
mean loss: 93.97
 ---- batch: 040 ----
mean loss: 92.54
train mean loss: 94.40
epoch train time: 0:00:07.684950
elapsed time: 0:17:40.860272
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 11:35:32.382085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.47
 ---- batch: 020 ----
mean loss: 92.86
 ---- batch: 030 ----
mean loss: 98.12
 ---- batch: 040 ----
mean loss: 93.20
train mean loss: 94.48
epoch train time: 0:00:07.674964
elapsed time: 0:17:48.535606
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 11:35:40.057444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.05
 ---- batch: 020 ----
mean loss: 95.76
 ---- batch: 030 ----
mean loss: 93.84
 ---- batch: 040 ----
mean loss: 94.57
train mean loss: 94.91
epoch train time: 0:00:07.663675
elapsed time: 0:17:56.199756
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 11:35:47.721567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.84
 ---- batch: 020 ----
mean loss: 93.61
 ---- batch: 030 ----
mean loss: 92.63
 ---- batch: 040 ----
mean loss: 96.04
train mean loss: 94.17
epoch train time: 0:00:07.423074
elapsed time: 0:18:03.623225
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 11:35:55.145020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.28
 ---- batch: 020 ----
mean loss: 96.34
 ---- batch: 030 ----
mean loss: 91.88
 ---- batch: 040 ----
mean loss: 90.21
train mean loss: 93.48
epoch train time: 0:00:07.390384
elapsed time: 0:18:11.013946
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 11:36:02.535730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.03
 ---- batch: 020 ----
mean loss: 94.49
 ---- batch: 030 ----
mean loss: 94.28
 ---- batch: 040 ----
mean loss: 94.45
train mean loss: 93.97
epoch train time: 0:00:07.475503
elapsed time: 0:18:18.489795
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 11:36:10.011637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.81
 ---- batch: 020 ----
mean loss: 97.88
 ---- batch: 030 ----
mean loss: 92.96
 ---- batch: 040 ----
mean loss: 87.60
train mean loss: 92.21
epoch train time: 0:00:07.450399
elapsed time: 0:18:25.940659
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 11:36:17.462509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.69
 ---- batch: 020 ----
mean loss: 91.84
 ---- batch: 030 ----
mean loss: 92.96
 ---- batch: 040 ----
mean loss: 92.76
train mean loss: 92.66
epoch train time: 0:00:07.439905
elapsed time: 0:18:33.381004
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 11:36:24.902803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.91
 ---- batch: 020 ----
mean loss: 87.27
 ---- batch: 030 ----
mean loss: 92.15
 ---- batch: 040 ----
mean loss: 94.25
train mean loss: 91.40
epoch train time: 0:00:07.458758
elapsed time: 0:18:40.840089
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 11:36:32.361933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.66
 ---- batch: 020 ----
mean loss: 92.34
 ---- batch: 030 ----
mean loss: 91.22
 ---- batch: 040 ----
mean loss: 93.71
train mean loss: 91.27
epoch train time: 0:00:07.463002
elapsed time: 0:18:48.303555
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 11:36:39.825411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.85
 ---- batch: 020 ----
mean loss: 93.26
 ---- batch: 030 ----
mean loss: 90.87
 ---- batch: 040 ----
mean loss: 86.46
train mean loss: 89.89
epoch train time: 0:00:07.477202
elapsed time: 0:18:55.781149
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 11:36:47.303029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.72
 ---- batch: 020 ----
mean loss: 90.06
 ---- batch: 030 ----
mean loss: 89.78
 ---- batch: 040 ----
mean loss: 89.05
train mean loss: 89.96
epoch train time: 0:00:07.520779
elapsed time: 0:19:03.302447
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 11:36:54.824291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.44
 ---- batch: 020 ----
mean loss: 91.61
 ---- batch: 030 ----
mean loss: 90.31
 ---- batch: 040 ----
mean loss: 87.01
train mean loss: 89.37
epoch train time: 0:00:07.612539
elapsed time: 0:19:10.915426
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 11:37:02.437276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.17
 ---- batch: 020 ----
mean loss: 87.64
 ---- batch: 030 ----
mean loss: 89.47
 ---- batch: 040 ----
mean loss: 89.18
train mean loss: 89.28
epoch train time: 0:00:07.636392
elapsed time: 0:19:18.552206
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 11:37:10.073994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.41
 ---- batch: 020 ----
mean loss: 92.22
 ---- batch: 030 ----
mean loss: 88.33
 ---- batch: 040 ----
mean loss: 89.04
train mean loss: 89.04
epoch train time: 0:00:07.617431
elapsed time: 0:19:26.170037
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 11:37:17.691930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.10
 ---- batch: 020 ----
mean loss: 85.20
 ---- batch: 030 ----
mean loss: 89.40
 ---- batch: 040 ----
mean loss: 88.81
train mean loss: 88.73
epoch train time: 0:00:07.611847
elapsed time: 0:19:33.782357
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 11:37:25.304208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.13
 ---- batch: 020 ----
mean loss: 91.44
 ---- batch: 030 ----
mean loss: 88.71
 ---- batch: 040 ----
mean loss: 83.58
train mean loss: 87.52
epoch train time: 0:00:07.649116
elapsed time: 0:19:41.432200
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 11:37:32.953986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.36
 ---- batch: 020 ----
mean loss: 86.68
 ---- batch: 030 ----
mean loss: 88.59
 ---- batch: 040 ----
mean loss: 86.85
train mean loss: 87.52
epoch train time: 0:00:07.576669
elapsed time: 0:19:49.009199
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 11:37:40.531041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.14
 ---- batch: 020 ----
mean loss: 88.49
 ---- batch: 030 ----
mean loss: 87.59
 ---- batch: 040 ----
mean loss: 87.87
train mean loss: 87.50
epoch train time: 0:00:07.569962
elapsed time: 0:19:56.579579
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 11:37:48.101508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.89
 ---- batch: 020 ----
mean loss: 85.40
 ---- batch: 030 ----
mean loss: 85.94
 ---- batch: 040 ----
mean loss: 86.78
train mean loss: 86.62
epoch train time: 0:00:07.573759
elapsed time: 0:20:04.153825
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 11:37:55.675655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.94
 ---- batch: 020 ----
mean loss: 85.97
 ---- batch: 030 ----
mean loss: 86.15
 ---- batch: 040 ----
mean loss: 89.02
train mean loss: 86.83
epoch train time: 0:00:07.601542
elapsed time: 0:20:11.755743
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 11:38:03.277588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.49
 ---- batch: 020 ----
mean loss: 87.27
 ---- batch: 030 ----
mean loss: 82.48
 ---- batch: 040 ----
mean loss: 86.60
train mean loss: 85.62
epoch train time: 0:00:07.598638
elapsed time: 0:20:19.354818
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 11:38:10.876647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.07
 ---- batch: 020 ----
mean loss: 87.63
 ---- batch: 030 ----
mean loss: 86.25
 ---- batch: 040 ----
mean loss: 89.37
train mean loss: 86.77
epoch train time: 0:00:07.698779
elapsed time: 0:20:27.054006
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 11:38:18.575906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.20
 ---- batch: 020 ----
mean loss: 87.68
 ---- batch: 030 ----
mean loss: 90.77
 ---- batch: 040 ----
mean loss: 83.58
train mean loss: 86.14
epoch train time: 0:00:07.564455
elapsed time: 0:20:34.618941
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 11:38:26.140771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.61
 ---- batch: 020 ----
mean loss: 86.95
 ---- batch: 030 ----
mean loss: 84.11
 ---- batch: 040 ----
mean loss: 87.50
train mean loss: 85.26
epoch train time: 0:00:07.625551
elapsed time: 0:20:42.244977
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 11:38:33.766845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.12
 ---- batch: 020 ----
mean loss: 85.34
 ---- batch: 030 ----
mean loss: 85.49
 ---- batch: 040 ----
mean loss: 82.01
train mean loss: 84.42
epoch train time: 0:00:07.792206
elapsed time: 0:20:50.037629
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 11:38:41.559425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.25
 ---- batch: 020 ----
mean loss: 85.67
 ---- batch: 030 ----
mean loss: 86.40
 ---- batch: 040 ----
mean loss: 83.73
train mean loss: 85.37
epoch train time: 0:00:07.763646
elapsed time: 0:20:57.801622
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 11:38:49.323463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.85
 ---- batch: 020 ----
mean loss: 83.64
 ---- batch: 030 ----
mean loss: 80.82
 ---- batch: 040 ----
mean loss: 87.97
train mean loss: 84.16
epoch train time: 0:00:07.755003
elapsed time: 0:21:05.557197
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 11:38:57.079091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.78
 ---- batch: 020 ----
mean loss: 83.88
 ---- batch: 030 ----
mean loss: 84.40
 ---- batch: 040 ----
mean loss: 84.83
train mean loss: 83.58
epoch train time: 0:00:07.659927
elapsed time: 0:21:13.217588
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 11:39:04.739437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.58
 ---- batch: 020 ----
mean loss: 81.76
 ---- batch: 030 ----
mean loss: 83.17
 ---- batch: 040 ----
mean loss: 85.69
train mean loss: 83.75
epoch train time: 0:00:07.751152
elapsed time: 0:21:20.969204
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 11:39:12.491035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.95
 ---- batch: 020 ----
mean loss: 85.56
 ---- batch: 030 ----
mean loss: 80.76
 ---- batch: 040 ----
mean loss: 85.86
train mean loss: 84.01
epoch train time: 0:00:07.713889
elapsed time: 0:21:28.683631
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 11:39:20.205506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.26
 ---- batch: 020 ----
mean loss: 80.77
 ---- batch: 030 ----
mean loss: 85.45
 ---- batch: 040 ----
mean loss: 81.64
train mean loss: 82.12
epoch train time: 0:00:07.715419
elapsed time: 0:21:36.399489
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 11:39:27.921305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.35
 ---- batch: 020 ----
mean loss: 83.60
 ---- batch: 030 ----
mean loss: 84.14
 ---- batch: 040 ----
mean loss: 84.44
train mean loss: 82.56
epoch train time: 0:00:07.678686
elapsed time: 0:21:44.078551
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 11:39:35.600399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.88
 ---- batch: 020 ----
mean loss: 79.20
 ---- batch: 030 ----
mean loss: 82.99
 ---- batch: 040 ----
mean loss: 81.26
train mean loss: 82.16
epoch train time: 0:00:07.668168
elapsed time: 0:21:51.747148
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 11:39:43.268975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.21
 ---- batch: 020 ----
mean loss: 81.63
 ---- batch: 030 ----
mean loss: 81.41
 ---- batch: 040 ----
mean loss: 80.55
train mean loss: 81.57
epoch train time: 0:00:07.615384
elapsed time: 0:21:59.362959
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 11:39:50.884783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.97
 ---- batch: 020 ----
mean loss: 81.21
 ---- batch: 030 ----
mean loss: 81.19
 ---- batch: 040 ----
mean loss: 81.43
train mean loss: 81.76
epoch train time: 0:00:07.469041
elapsed time: 0:22:06.832418
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 11:39:58.354275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.40
 ---- batch: 020 ----
mean loss: 81.85
 ---- batch: 030 ----
mean loss: 90.69
 ---- batch: 040 ----
mean loss: 82.51
train mean loss: 84.64
epoch train time: 0:00:07.461368
elapsed time: 0:22:14.294255
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 11:40:05.816152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.26
 ---- batch: 020 ----
mean loss: 83.12
 ---- batch: 030 ----
mean loss: 88.55
 ---- batch: 040 ----
mean loss: 82.59
train mean loss: 84.01
epoch train time: 0:00:07.557302
elapsed time: 0:22:21.852008
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 11:40:13.373837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.87
 ---- batch: 020 ----
mean loss: 78.93
 ---- batch: 030 ----
mean loss: 80.52
 ---- batch: 040 ----
mean loss: 80.72
train mean loss: 80.76
epoch train time: 0:00:07.837174
elapsed time: 0:22:29.689616
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 11:40:21.211451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.22
 ---- batch: 020 ----
mean loss: 76.99
 ---- batch: 030 ----
mean loss: 84.59
 ---- batch: 040 ----
mean loss: 80.08
train mean loss: 80.90
epoch train time: 0:00:07.653765
elapsed time: 0:22:37.343872
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 11:40:28.865733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.76
 ---- batch: 020 ----
mean loss: 87.90
 ---- batch: 030 ----
mean loss: 81.43
 ---- batch: 040 ----
mean loss: 85.44
train mean loss: 85.60
epoch train time: 0:00:07.610842
elapsed time: 0:22:44.955185
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 11:40:36.477019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.71
 ---- batch: 020 ----
mean loss: 84.43
 ---- batch: 030 ----
mean loss: 82.48
 ---- batch: 040 ----
mean loss: 79.62
train mean loss: 81.64
epoch train time: 0:00:07.418506
elapsed time: 0:22:52.374177
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 11:40:43.895975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.50
 ---- batch: 020 ----
mean loss: 81.40
 ---- batch: 030 ----
mean loss: 80.14
 ---- batch: 040 ----
mean loss: 76.84
train mean loss: 79.75
epoch train time: 0:00:07.393292
elapsed time: 0:22:59.767832
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 11:40:51.289669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.29
 ---- batch: 020 ----
mean loss: 81.35
 ---- batch: 030 ----
mean loss: 81.67
 ---- batch: 040 ----
mean loss: 79.28
train mean loss: 80.80
epoch train time: 0:00:07.411942
elapsed time: 0:23:07.180203
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 11:40:58.702082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.28
 ---- batch: 020 ----
mean loss: 80.37
 ---- batch: 030 ----
mean loss: 79.77
 ---- batch: 040 ----
mean loss: 80.65
train mean loss: 79.52
epoch train time: 0:00:07.613706
elapsed time: 0:23:14.794400
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 11:41:06.316269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.30
 ---- batch: 020 ----
mean loss: 75.96
 ---- batch: 030 ----
mean loss: 76.66
 ---- batch: 040 ----
mean loss: 80.59
train mean loss: 78.92
epoch train time: 0:00:07.590598
elapsed time: 0:23:22.385431
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 11:41:13.907261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.10
 ---- batch: 020 ----
mean loss: 76.27
 ---- batch: 030 ----
mean loss: 76.78
 ---- batch: 040 ----
mean loss: 78.39
train mean loss: 78.35
epoch train time: 0:00:07.618758
elapsed time: 0:23:30.004585
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 11:41:21.526424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.11
 ---- batch: 020 ----
mean loss: 85.38
 ---- batch: 030 ----
mean loss: 82.61
 ---- batch: 040 ----
mean loss: 80.47
train mean loss: 81.88
epoch train time: 0:00:07.606900
elapsed time: 0:23:37.611967
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 11:41:29.133818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.33
 ---- batch: 020 ----
mean loss: 82.72
 ---- batch: 030 ----
mean loss: 77.77
 ---- batch: 040 ----
mean loss: 76.71
train mean loss: 78.59
epoch train time: 0:00:07.631454
elapsed time: 0:23:45.243836
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 11:41:36.765671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.02
 ---- batch: 020 ----
mean loss: 77.41
 ---- batch: 030 ----
mean loss: 77.38
 ---- batch: 040 ----
mean loss: 77.63
train mean loss: 78.08
epoch train time: 0:00:07.615427
elapsed time: 0:23:52.859655
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 11:41:44.381490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.75
 ---- batch: 020 ----
mean loss: 81.65
 ---- batch: 030 ----
mean loss: 76.77
 ---- batch: 040 ----
mean loss: 80.69
train mean loss: 78.86
epoch train time: 0:00:07.727614
elapsed time: 0:24:00.587718
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 11:41:52.109603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.16
 ---- batch: 020 ----
mean loss: 80.49
 ---- batch: 030 ----
mean loss: 79.45
 ---- batch: 040 ----
mean loss: 79.23
train mean loss: 79.11
epoch train time: 0:00:07.775159
elapsed time: 0:24:08.363365
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 11:41:59.885181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.04
 ---- batch: 020 ----
mean loss: 80.61
 ---- batch: 030 ----
mean loss: 74.21
 ---- batch: 040 ----
mean loss: 80.45
train mean loss: 77.79
epoch train time: 0:00:07.770069
elapsed time: 0:24:16.133824
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 11:42:07.655680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.71
 ---- batch: 020 ----
mean loss: 72.57
 ---- batch: 030 ----
mean loss: 80.17
 ---- batch: 040 ----
mean loss: 77.09
train mean loss: 77.07
epoch train time: 0:00:07.663249
elapsed time: 0:24:23.797590
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 11:42:15.319462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.65
 ---- batch: 020 ----
mean loss: 80.56
 ---- batch: 030 ----
mean loss: 77.93
 ---- batch: 040 ----
mean loss: 72.50
train mean loss: 77.06
epoch train time: 0:00:07.640870
elapsed time: 0:24:31.439043
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 11:42:22.960920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.76
 ---- batch: 020 ----
mean loss: 72.92
 ---- batch: 030 ----
mean loss: 78.43
 ---- batch: 040 ----
mean loss: 79.51
train mean loss: 76.97
epoch train time: 0:00:07.613402
elapsed time: 0:24:39.053096
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 11:42:30.575062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.86
 ---- batch: 020 ----
mean loss: 76.52
 ---- batch: 030 ----
mean loss: 77.49
 ---- batch: 040 ----
mean loss: 78.16
train mean loss: 77.09
epoch train time: 0:00:07.629016
elapsed time: 0:24:46.682738
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 11:42:38.204618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.37
 ---- batch: 020 ----
mean loss: 77.27
 ---- batch: 030 ----
mean loss: 77.32
 ---- batch: 040 ----
mean loss: 84.05
train mean loss: 78.71
epoch train time: 0:00:07.686601
elapsed time: 0:24:54.369795
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 11:42:45.891659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.47
 ---- batch: 020 ----
mean loss: 76.01
 ---- batch: 030 ----
mean loss: 77.32
 ---- batch: 040 ----
mean loss: 78.31
train mean loss: 77.10
epoch train time: 0:00:07.712127
elapsed time: 0:25:02.082402
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 11:42:53.604265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.64
 ---- batch: 020 ----
mean loss: 75.68
 ---- batch: 030 ----
mean loss: 78.26
 ---- batch: 040 ----
mean loss: 74.41
train mean loss: 75.83
epoch train time: 0:00:07.749764
elapsed time: 0:25:09.832624
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 11:43:01.354490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.83
 ---- batch: 020 ----
mean loss: 76.67
 ---- batch: 030 ----
mean loss: 77.75
 ---- batch: 040 ----
mean loss: 76.27
train mean loss: 76.89
epoch train time: 0:00:07.727609
elapsed time: 0:25:17.560716
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 11:43:09.082557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.36
 ---- batch: 020 ----
mean loss: 76.58
 ---- batch: 030 ----
mean loss: 77.82
 ---- batch: 040 ----
mean loss: 76.63
train mean loss: 76.41
epoch train time: 0:00:07.707752
elapsed time: 0:25:25.268917
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 11:43:16.790766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.16
 ---- batch: 020 ----
mean loss: 77.91
 ---- batch: 030 ----
mean loss: 75.57
 ---- batch: 040 ----
mean loss: 76.05
train mean loss: 75.96
epoch train time: 0:00:07.716569
elapsed time: 0:25:32.985961
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 11:43:24.507811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.23
 ---- batch: 020 ----
mean loss: 76.82
 ---- batch: 030 ----
mean loss: 78.84
 ---- batch: 040 ----
mean loss: 76.64
train mean loss: 76.60
epoch train time: 0:00:07.745117
elapsed time: 0:25:40.731486
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 11:43:32.253276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.68
 ---- batch: 020 ----
mean loss: 77.08
 ---- batch: 030 ----
mean loss: 73.90
 ---- batch: 040 ----
mean loss: 75.29
train mean loss: 75.60
epoch train time: 0:00:07.741168
elapsed time: 0:25:48.473028
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 11:43:39.994873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.39
 ---- batch: 020 ----
mean loss: 75.26
 ---- batch: 030 ----
mean loss: 75.07
 ---- batch: 040 ----
mean loss: 73.25
train mean loss: 75.20
epoch train time: 0:00:07.639170
elapsed time: 0:25:56.112653
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 11:43:47.634524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.13
 ---- batch: 020 ----
mean loss: 76.83
 ---- batch: 030 ----
mean loss: 76.60
 ---- batch: 040 ----
mean loss: 76.34
train mean loss: 75.96
epoch train time: 0:00:07.664087
elapsed time: 0:26:03.777181
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 11:43:55.299067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.73
 ---- batch: 020 ----
mean loss: 78.56
 ---- batch: 030 ----
mean loss: 77.04
 ---- batch: 040 ----
mean loss: 75.74
train mean loss: 75.71
epoch train time: 0:00:07.639384
elapsed time: 0:26:11.417125
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 11:44:02.939000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.73
 ---- batch: 020 ----
mean loss: 75.33
 ---- batch: 030 ----
mean loss: 74.05
 ---- batch: 040 ----
mean loss: 75.88
train mean loss: 75.16
epoch train time: 0:00:07.578367
elapsed time: 0:26:18.995987
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 11:44:10.517839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.95
 ---- batch: 020 ----
mean loss: 73.02
 ---- batch: 030 ----
mean loss: 75.17
 ---- batch: 040 ----
mean loss: 75.55
train mean loss: 73.90
epoch train time: 0:00:07.573312
elapsed time: 0:26:26.569893
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 11:44:18.091631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.41
 ---- batch: 020 ----
mean loss: 77.78
 ---- batch: 030 ----
mean loss: 72.42
 ---- batch: 040 ----
mean loss: 72.53
train mean loss: 73.73
epoch train time: 0:00:07.701850
elapsed time: 0:26:34.272084
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 11:44:25.793872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.85
 ---- batch: 020 ----
mean loss: 76.41
 ---- batch: 030 ----
mean loss: 72.37
 ---- batch: 040 ----
mean loss: 71.20
train mean loss: 73.13
epoch train time: 0:00:07.797608
elapsed time: 0:26:42.070070
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 11:44:33.591963
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.49
 ---- batch: 020 ----
mean loss: 72.81
 ---- batch: 030 ----
mean loss: 74.37
 ---- batch: 040 ----
mean loss: 76.02
train mean loss: 73.60
epoch train time: 0:00:07.687103
elapsed time: 0:26:49.757675
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 11:44:41.279512
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.10
 ---- batch: 020 ----
mean loss: 73.95
 ---- batch: 030 ----
mean loss: 73.33
 ---- batch: 040 ----
mean loss: 73.16
train mean loss: 73.46
epoch train time: 0:00:07.677089
elapsed time: 0:26:57.435199
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 11:44:48.957039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 77.52
 ---- batch: 020 ----
mean loss: 73.37
 ---- batch: 030 ----
mean loss: 73.73
 ---- batch: 040 ----
mean loss: 70.29
train mean loss: 73.42
epoch train time: 0:00:07.668776
elapsed time: 0:27:05.104396
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 11:44:56.626230
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 74.51
 ---- batch: 020 ----
mean loss: 71.43
 ---- batch: 030 ----
mean loss: 73.98
 ---- batch: 040 ----
mean loss: 71.97
train mean loss: 73.69
epoch train time: 0:00:07.682547
elapsed time: 0:27:12.787344
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 11:45:04.309185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.36
 ---- batch: 020 ----
mean loss: 76.31
 ---- batch: 030 ----
mean loss: 70.26
 ---- batch: 040 ----
mean loss: 74.08
train mean loss: 73.41
epoch train time: 0:00:07.666532
elapsed time: 0:27:20.454283
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 11:45:11.976108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.61
 ---- batch: 020 ----
mean loss: 74.23
 ---- batch: 030 ----
mean loss: 70.70
 ---- batch: 040 ----
mean loss: 76.28
train mean loss: 73.53
epoch train time: 0:00:07.666783
elapsed time: 0:27:28.121450
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 11:45:19.643292
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 74.75
 ---- batch: 020 ----
mean loss: 75.61
 ---- batch: 030 ----
mean loss: 73.43
 ---- batch: 040 ----
mean loss: 70.23
train mean loss: 73.42
epoch train time: 0:00:07.746189
elapsed time: 0:27:35.868047
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 11:45:27.389896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 75.88
 ---- batch: 020 ----
mean loss: 72.42
 ---- batch: 030 ----
mean loss: 71.20
 ---- batch: 040 ----
mean loss: 73.46
train mean loss: 73.17
epoch train time: 0:00:07.630041
elapsed time: 0:27:43.498506
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 11:45:35.020340
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 75.62
 ---- batch: 020 ----
mean loss: 73.08
 ---- batch: 030 ----
mean loss: 71.83
 ---- batch: 040 ----
mean loss: 74.13
train mean loss: 73.29
epoch train time: 0:00:07.608631
elapsed time: 0:27:51.107541
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 11:45:42.629377
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 74.76
 ---- batch: 020 ----
mean loss: 69.96
 ---- batch: 030 ----
mean loss: 71.44
 ---- batch: 040 ----
mean loss: 76.26
train mean loss: 73.10
epoch train time: 0:00:07.587695
elapsed time: 0:27:58.695634
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 11:45:50.217467
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.88
 ---- batch: 020 ----
mean loss: 74.27
 ---- batch: 030 ----
mean loss: 71.27
 ---- batch: 040 ----
mean loss: 71.83
train mean loss: 72.87
epoch train time: 0:00:07.589148
elapsed time: 0:28:06.285188
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 11:45:57.807019
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.51
 ---- batch: 020 ----
mean loss: 74.90
 ---- batch: 030 ----
mean loss: 73.91
 ---- batch: 040 ----
mean loss: 73.28
train mean loss: 73.51
epoch train time: 0:00:07.703277
elapsed time: 0:28:13.988876
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 11:46:05.510741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 75.37
 ---- batch: 020 ----
mean loss: 72.70
 ---- batch: 030 ----
mean loss: 73.42
 ---- batch: 040 ----
mean loss: 69.36
train mean loss: 73.19
epoch train time: 0:00:07.775044
elapsed time: 0:28:21.764424
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 11:46:13.286262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.01
 ---- batch: 020 ----
mean loss: 70.76
 ---- batch: 030 ----
mean loss: 74.97
 ---- batch: 040 ----
mean loss: 75.34
train mean loss: 73.62
epoch train time: 0:00:07.806885
elapsed time: 0:28:29.571716
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 11:46:21.093564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.32
 ---- batch: 020 ----
mean loss: 76.35
 ---- batch: 030 ----
mean loss: 73.75
 ---- batch: 040 ----
mean loss: 73.37
train mean loss: 73.56
epoch train time: 0:00:07.795113
elapsed time: 0:28:37.367274
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 11:46:28.889104
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.38
 ---- batch: 020 ----
mean loss: 73.35
 ---- batch: 030 ----
mean loss: 73.61
 ---- batch: 040 ----
mean loss: 72.57
train mean loss: 73.09
epoch train time: 0:00:07.670397
elapsed time: 0:28:45.038150
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 11:46:36.560063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.42
 ---- batch: 020 ----
mean loss: 74.72
 ---- batch: 030 ----
mean loss: 74.42
 ---- batch: 040 ----
mean loss: 72.97
train mean loss: 73.24
epoch train time: 0:00:07.641748
elapsed time: 0:28:52.680459
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 11:46:44.202295
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.46
 ---- batch: 020 ----
mean loss: 72.60
 ---- batch: 030 ----
mean loss: 70.28
 ---- batch: 040 ----
mean loss: 76.07
train mean loss: 73.46
epoch train time: 0:00:07.753702
elapsed time: 0:29:00.434630
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 11:46:51.956527
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.92
 ---- batch: 020 ----
mean loss: 74.85
 ---- batch: 030 ----
mean loss: 74.23
 ---- batch: 040 ----
mean loss: 71.57
train mean loss: 73.38
epoch train time: 0:00:07.807309
elapsed time: 0:29:08.242442
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 11:46:59.764283
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.53
 ---- batch: 020 ----
mean loss: 72.74
 ---- batch: 030 ----
mean loss: 73.84
 ---- batch: 040 ----
mean loss: 72.79
train mean loss: 73.22
epoch train time: 0:00:07.775862
elapsed time: 0:29:16.018782
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 11:47:07.540629
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.46
 ---- batch: 020 ----
mean loss: 73.71
 ---- batch: 030 ----
mean loss: 72.99
 ---- batch: 040 ----
mean loss: 74.36
train mean loss: 73.37
epoch train time: 0:00:07.782283
elapsed time: 0:29:23.801515
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 11:47:15.323370
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.48
 ---- batch: 020 ----
mean loss: 71.19
 ---- batch: 030 ----
mean loss: 73.77
 ---- batch: 040 ----
mean loss: 74.78
train mean loss: 73.40
epoch train time: 0:00:07.617367
elapsed time: 0:29:31.419323
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 11:47:22.941163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 74.66
 ---- batch: 020 ----
mean loss: 72.61
 ---- batch: 030 ----
mean loss: 71.53
 ---- batch: 040 ----
mean loss: 72.53
train mean loss: 72.68
epoch train time: 0:00:07.607005
elapsed time: 0:29:39.026738
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 11:47:30.548537
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.55
 ---- batch: 020 ----
mean loss: 71.95
 ---- batch: 030 ----
mean loss: 75.80
 ---- batch: 040 ----
mean loss: 70.50
train mean loss: 72.92
epoch train time: 0:00:07.732003
elapsed time: 0:29:46.759201
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 11:47:38.281082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 74.87
 ---- batch: 020 ----
mean loss: 72.89
 ---- batch: 030 ----
mean loss: 75.31
 ---- batch: 040 ----
mean loss: 71.11
train mean loss: 73.26
epoch train time: 0:00:07.611271
elapsed time: 0:29:54.370924
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 11:47:45.892756
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.59
 ---- batch: 020 ----
mean loss: 73.78
 ---- batch: 030 ----
mean loss: 72.45
 ---- batch: 040 ----
mean loss: 74.52
train mean loss: 73.40
epoch train time: 0:00:07.576597
elapsed time: 0:30:01.947919
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 11:47:53.469755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 74.94
 ---- batch: 020 ----
mean loss: 74.29
 ---- batch: 030 ----
mean loss: 74.16
 ---- batch: 040 ----
mean loss: 69.06
train mean loss: 73.15
epoch train time: 0:00:07.597692
elapsed time: 0:30:09.546028
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 11:48:01.067884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.49
 ---- batch: 020 ----
mean loss: 69.47
 ---- batch: 030 ----
mean loss: 71.57
 ---- batch: 040 ----
mean loss: 74.11
train mean loss: 72.58
epoch train time: 0:00:07.424620
elapsed time: 0:30:16.971054
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 11:48:08.492886
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.80
 ---- batch: 020 ----
mean loss: 73.49
 ---- batch: 030 ----
mean loss: 74.84
 ---- batch: 040 ----
mean loss: 71.68
train mean loss: 72.65
epoch train time: 0:00:07.425330
elapsed time: 0:30:24.396816
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 11:48:15.918617
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.89
 ---- batch: 020 ----
mean loss: 73.23
 ---- batch: 030 ----
mean loss: 73.21
 ---- batch: 040 ----
mean loss: 71.75
train mean loss: 73.04
epoch train time: 0:00:07.574400
elapsed time: 0:30:31.971694
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 11:48:23.493427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.66
 ---- batch: 020 ----
mean loss: 73.31
 ---- batch: 030 ----
mean loss: 73.11
 ---- batch: 040 ----
mean loss: 72.13
train mean loss: 72.55
epoch train time: 0:00:07.878051
elapsed time: 0:30:39.850033
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 11:48:31.371889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 74.77
 ---- batch: 020 ----
mean loss: 72.57
 ---- batch: 030 ----
mean loss: 71.03
 ---- batch: 040 ----
mean loss: 74.00
train mean loss: 72.81
epoch train time: 0:00:07.904743
elapsed time: 0:30:47.755189
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 11:48:39.277029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.97
 ---- batch: 020 ----
mean loss: 73.16
 ---- batch: 030 ----
mean loss: 72.87
 ---- batch: 040 ----
mean loss: 71.09
train mean loss: 72.53
epoch train time: 0:00:07.643216
elapsed time: 0:30:55.398845
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 11:48:46.920691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 76.17
 ---- batch: 020 ----
mean loss: 70.40
 ---- batch: 030 ----
mean loss: 71.91
 ---- batch: 040 ----
mean loss: 73.56
train mean loss: 72.83
epoch train time: 0:00:07.636365
elapsed time: 0:31:03.035602
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 11:48:54.557436
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.48
 ---- batch: 020 ----
mean loss: 71.53
 ---- batch: 030 ----
mean loss: 73.84
 ---- batch: 040 ----
mean loss: 73.44
train mean loss: 72.72
epoch train time: 0:00:07.687222
elapsed time: 0:31:10.723209
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 11:49:02.245044
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.08
 ---- batch: 020 ----
mean loss: 72.04
 ---- batch: 030 ----
mean loss: 75.03
 ---- batch: 040 ----
mean loss: 73.54
train mean loss: 72.75
epoch train time: 0:00:07.591598
elapsed time: 0:31:18.315255
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 11:49:09.837123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.70
 ---- batch: 020 ----
mean loss: 73.22
 ---- batch: 030 ----
mean loss: 69.65
 ---- batch: 040 ----
mean loss: 74.25
train mean loss: 72.76
epoch train time: 0:00:07.892676
elapsed time: 0:31:26.208437
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 11:49:17.730339
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.19
 ---- batch: 020 ----
mean loss: 71.63
 ---- batch: 030 ----
mean loss: 73.84
 ---- batch: 040 ----
mean loss: 73.22
train mean loss: 72.51
epoch train time: 0:00:07.852729
elapsed time: 0:31:34.061630
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 11:49:25.583435
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 74.48
 ---- batch: 020 ----
mean loss: 70.78
 ---- batch: 030 ----
mean loss: 72.89
 ---- batch: 040 ----
mean loss: 73.34
train mean loss: 72.49
epoch train time: 0:00:07.884564
elapsed time: 0:31:41.946554
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 11:49:33.468436
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.63
 ---- batch: 020 ----
mean loss: 72.71
 ---- batch: 030 ----
mean loss: 72.50
 ---- batch: 040 ----
mean loss: 74.03
train mean loss: 72.58
epoch train time: 0:00:07.986356
elapsed time: 0:31:49.933391
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 11:49:41.455235
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.69
 ---- batch: 020 ----
mean loss: 74.17
 ---- batch: 030 ----
mean loss: 72.49
 ---- batch: 040 ----
mean loss: 72.36
train mean loss: 72.52
epoch train time: 0:00:07.777663
elapsed time: 0:31:57.711452
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 11:49:49.233305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.43
 ---- batch: 020 ----
mean loss: 75.19
 ---- batch: 030 ----
mean loss: 71.61
 ---- batch: 040 ----
mean loss: 73.46
train mean loss: 72.49
epoch train time: 0:00:07.805229
elapsed time: 0:32:05.517089
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 11:49:57.038964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.15
 ---- batch: 020 ----
mean loss: 71.24
 ---- batch: 030 ----
mean loss: 76.00
 ---- batch: 040 ----
mean loss: 72.76
train mean loss: 72.56
epoch train time: 0:00:07.775886
elapsed time: 0:32:13.293447
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 11:50:04.815343
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.96
 ---- batch: 020 ----
mean loss: 73.82
 ---- batch: 030 ----
mean loss: 70.55
 ---- batch: 040 ----
mean loss: 71.93
train mean loss: 72.52
epoch train time: 0:00:07.752118
elapsed time: 0:32:21.046035
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 11:50:12.567826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.93
 ---- batch: 020 ----
mean loss: 72.25
 ---- batch: 030 ----
mean loss: 74.88
 ---- batch: 040 ----
mean loss: 69.06
train mean loss: 72.54
epoch train time: 0:00:07.761724
elapsed time: 0:32:28.808098
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 11:50:20.329961
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.94
 ---- batch: 020 ----
mean loss: 72.67
 ---- batch: 030 ----
mean loss: 71.80
 ---- batch: 040 ----
mean loss: 71.33
train mean loss: 72.39
epoch train time: 0:00:07.647133
elapsed time: 0:32:36.464305
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1/checkpoint.pth.tar
**** end time: 2019-09-20 11:50:27.986009 ****
