Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_4', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 24398
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistDense3...
Done.
**** start time: 2019-09-25 23:13:42.685470 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
            Linear-2                  [-1, 100]          48,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 68,100
Trainable params: 68,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 23:13:42.688719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4306.11
 ---- batch: 020 ----
mean loss: 4132.28
 ---- batch: 030 ----
mean loss: 4099.82
 ---- batch: 040 ----
mean loss: 3971.73
 ---- batch: 050 ----
mean loss: 3831.82
 ---- batch: 060 ----
mean loss: 3866.82
 ---- batch: 070 ----
mean loss: 3736.52
 ---- batch: 080 ----
mean loss: 3731.57
 ---- batch: 090 ----
mean loss: 3675.36
train mean loss: 3908.57
epoch train time: 0:00:33.747186
elapsed time: 0:00:33.752907
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 23:14:16.438419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3517.41
 ---- batch: 020 ----
mean loss: 3556.81
 ---- batch: 030 ----
mean loss: 3491.44
 ---- batch: 040 ----
mean loss: 3449.36
 ---- batch: 050 ----
mean loss: 3334.16
 ---- batch: 060 ----
mean loss: 3344.14
 ---- batch: 070 ----
mean loss: 3290.73
 ---- batch: 080 ----
mean loss: 3212.58
 ---- batch: 090 ----
mean loss: 3188.42
train mean loss: 3362.03
epoch train time: 0:00:00.515078
elapsed time: 0:00:34.268159
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 23:14:16.953685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3072.57
 ---- batch: 020 ----
mean loss: 3016.26
 ---- batch: 030 ----
mean loss: 3006.08
 ---- batch: 040 ----
mean loss: 2962.53
 ---- batch: 050 ----
mean loss: 2932.60
 ---- batch: 060 ----
mean loss: 2884.76
 ---- batch: 070 ----
mean loss: 2872.66
 ---- batch: 080 ----
mean loss: 2800.31
 ---- batch: 090 ----
mean loss: 2725.41
train mean loss: 2909.51
epoch train time: 0:00:00.498210
elapsed time: 0:00:34.766518
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 23:14:17.452058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2685.53
 ---- batch: 020 ----
mean loss: 2613.10
 ---- batch: 030 ----
mean loss: 2568.27
 ---- batch: 040 ----
mean loss: 2589.77
 ---- batch: 050 ----
mean loss: 2525.67
 ---- batch: 060 ----
mean loss: 2505.22
 ---- batch: 070 ----
mean loss: 2412.84
 ---- batch: 080 ----
mean loss: 2387.88
 ---- batch: 090 ----
mean loss: 2376.27
train mean loss: 2505.88
epoch train time: 0:00:00.510446
elapsed time: 0:00:35.277135
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 23:14:17.962659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2269.65
 ---- batch: 020 ----
mean loss: 2255.99
 ---- batch: 030 ----
mean loss: 2196.05
 ---- batch: 040 ----
mean loss: 2159.73
 ---- batch: 050 ----
mean loss: 2173.47
 ---- batch: 060 ----
mean loss: 2084.46
 ---- batch: 070 ----
mean loss: 2074.16
 ---- batch: 080 ----
mean loss: 2071.82
 ---- batch: 090 ----
mean loss: 2009.39
train mean loss: 2132.78
epoch train time: 0:00:00.510409
elapsed time: 0:00:35.787737
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 23:14:18.473258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1926.42
 ---- batch: 020 ----
mean loss: 1891.51
 ---- batch: 030 ----
mean loss: 1897.78
 ---- batch: 040 ----
mean loss: 1848.35
 ---- batch: 050 ----
mean loss: 1880.26
 ---- batch: 060 ----
mean loss: 1770.93
 ---- batch: 070 ----
mean loss: 1790.52
 ---- batch: 080 ----
mean loss: 1792.17
 ---- batch: 090 ----
mean loss: 1710.46
train mean loss: 1824.98
epoch train time: 0:00:00.502940
elapsed time: 0:00:36.290840
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 23:14:18.976374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1686.55
 ---- batch: 020 ----
mean loss: 1677.85
 ---- batch: 030 ----
mean loss: 1594.97
 ---- batch: 040 ----
mean loss: 1625.51
 ---- batch: 050 ----
mean loss: 1594.13
 ---- batch: 060 ----
mean loss: 1587.32
 ---- batch: 070 ----
mean loss: 1551.08
 ---- batch: 080 ----
mean loss: 1538.58
 ---- batch: 090 ----
mean loss: 1521.98
train mean loss: 1589.33
epoch train time: 0:00:00.502083
elapsed time: 0:00:36.793091
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 23:14:19.478617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1462.54
 ---- batch: 020 ----
mean loss: 1468.99
 ---- batch: 030 ----
mean loss: 1468.75
 ---- batch: 040 ----
mean loss: 1393.71
 ---- batch: 050 ----
mean loss: 1430.01
 ---- batch: 060 ----
mean loss: 1407.54
 ---- batch: 070 ----
mean loss: 1360.33
 ---- batch: 080 ----
mean loss: 1379.28
 ---- batch: 090 ----
mean loss: 1337.70
train mean loss: 1406.06
epoch train time: 0:00:00.507118
elapsed time: 0:00:37.300364
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 23:14:19.985889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1328.80
 ---- batch: 020 ----
mean loss: 1287.09
 ---- batch: 030 ----
mean loss: 1285.78
 ---- batch: 040 ----
mean loss: 1270.32
 ---- batch: 050 ----
mean loss: 1292.81
 ---- batch: 060 ----
mean loss: 1239.30
 ---- batch: 070 ----
mean loss: 1254.78
 ---- batch: 080 ----
mean loss: 1211.39
 ---- batch: 090 ----
mean loss: 1239.17
train mean loss: 1265.03
epoch train time: 0:00:00.498082
elapsed time: 0:00:37.798595
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 23:14:20.484114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1209.16
 ---- batch: 020 ----
mean loss: 1181.31
 ---- batch: 030 ----
mean loss: 1186.36
 ---- batch: 040 ----
mean loss: 1140.87
 ---- batch: 050 ----
mean loss: 1168.61
 ---- batch: 060 ----
mean loss: 1150.08
 ---- batch: 070 ----
mean loss: 1159.84
 ---- batch: 080 ----
mean loss: 1115.98
 ---- batch: 090 ----
mean loss: 1130.61
train mean loss: 1156.32
epoch train time: 0:00:00.505670
elapsed time: 0:00:38.304411
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 23:14:20.989958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1087.13
 ---- batch: 020 ----
mean loss: 1106.17
 ---- batch: 030 ----
mean loss: 1101.28
 ---- batch: 040 ----
mean loss: 1070.23
 ---- batch: 050 ----
mean loss: 1068.69
 ---- batch: 060 ----
mean loss: 1081.24
 ---- batch: 070 ----
mean loss: 1064.74
 ---- batch: 080 ----
mean loss: 1034.81
 ---- batch: 090 ----
mean loss: 1057.44
train mean loss: 1072.70
epoch train time: 0:00:00.498726
elapsed time: 0:00:38.803319
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 23:14:21.488838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1018.89
 ---- batch: 020 ----
mean loss: 1029.99
 ---- batch: 030 ----
mean loss: 1033.98
 ---- batch: 040 ----
mean loss: 1022.57
 ---- batch: 050 ----
mean loss: 1015.73
 ---- batch: 060 ----
mean loss: 1009.54
 ---- batch: 070 ----
mean loss: 1009.78
 ---- batch: 080 ----
mean loss: 980.93
 ---- batch: 090 ----
mean loss: 970.83
train mean loss: 1007.01
epoch train time: 0:00:00.507237
elapsed time: 0:00:39.310763
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 23:14:21.996284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.15
 ---- batch: 020 ----
mean loss: 962.69
 ---- batch: 030 ----
mean loss: 976.66
 ---- batch: 040 ----
mean loss: 954.40
 ---- batch: 050 ----
mean loss: 982.07
 ---- batch: 060 ----
mean loss: 957.53
 ---- batch: 070 ----
mean loss: 941.53
 ---- batch: 080 ----
mean loss: 951.32
 ---- batch: 090 ----
mean loss: 950.94
train mean loss: 960.16
epoch train time: 0:00:00.501111
elapsed time: 0:00:39.812027
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 23:14:22.497549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.83
 ---- batch: 020 ----
mean loss: 936.03
 ---- batch: 030 ----
mean loss: 931.23
 ---- batch: 040 ----
mean loss: 911.84
 ---- batch: 050 ----
mean loss: 930.05
 ---- batch: 060 ----
mean loss: 924.74
 ---- batch: 070 ----
mean loss: 939.26
 ---- batch: 080 ----
mean loss: 924.23
 ---- batch: 090 ----
mean loss: 922.94
train mean loss: 928.36
epoch train time: 0:00:00.499893
elapsed time: 0:00:40.312065
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 23:14:22.997624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.09
 ---- batch: 020 ----
mean loss: 917.71
 ---- batch: 030 ----
mean loss: 911.00
 ---- batch: 040 ----
mean loss: 896.10
 ---- batch: 050 ----
mean loss: 904.26
 ---- batch: 060 ----
mean loss: 898.39
 ---- batch: 070 ----
mean loss: 900.33
 ---- batch: 080 ----
mean loss: 915.11
 ---- batch: 090 ----
mean loss: 907.01
train mean loss: 908.46
epoch train time: 0:00:00.500367
elapsed time: 0:00:40.812631
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 23:14:23.498159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 900.34
 ---- batch: 020 ----
mean loss: 898.22
 ---- batch: 030 ----
mean loss: 896.34
 ---- batch: 040 ----
mean loss: 905.23
 ---- batch: 050 ----
mean loss: 899.02
 ---- batch: 060 ----
mean loss: 887.46
 ---- batch: 070 ----
mean loss: 872.23
 ---- batch: 080 ----
mean loss: 893.13
 ---- batch: 090 ----
mean loss: 893.68
train mean loss: 894.83
epoch train time: 0:00:00.509160
elapsed time: 0:00:41.321955
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 23:14:24.007481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.83
 ---- batch: 020 ----
mean loss: 869.85
 ---- batch: 030 ----
mean loss: 885.31
 ---- batch: 040 ----
mean loss: 892.70
 ---- batch: 050 ----
mean loss: 869.20
 ---- batch: 060 ----
mean loss: 894.39
 ---- batch: 070 ----
mean loss: 900.03
 ---- batch: 080 ----
mean loss: 901.20
 ---- batch: 090 ----
mean loss: 871.75
train mean loss: 886.97
epoch train time: 0:00:00.500501
elapsed time: 0:00:41.822616
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 23:14:24.508151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.37
 ---- batch: 020 ----
mean loss: 877.78
 ---- batch: 030 ----
mean loss: 899.52
 ---- batch: 040 ----
mean loss: 889.93
 ---- batch: 050 ----
mean loss: 871.83
 ---- batch: 060 ----
mean loss: 866.35
 ---- batch: 070 ----
mean loss: 879.27
 ---- batch: 080 ----
mean loss: 876.18
 ---- batch: 090 ----
mean loss: 873.75
train mean loss: 881.41
epoch train time: 0:00:00.509324
elapsed time: 0:00:42.332102
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 23:14:25.017668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.36
 ---- batch: 020 ----
mean loss: 886.82
 ---- batch: 030 ----
mean loss: 865.39
 ---- batch: 040 ----
mean loss: 883.64
 ---- batch: 050 ----
mean loss: 880.93
 ---- batch: 060 ----
mean loss: 874.68
 ---- batch: 070 ----
mean loss: 860.30
 ---- batch: 080 ----
mean loss: 891.08
 ---- batch: 090 ----
mean loss: 885.40
train mean loss: 878.23
epoch train time: 0:00:00.516580
elapsed time: 0:00:42.848875
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 23:14:25.534398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.50
 ---- batch: 020 ----
mean loss: 889.15
 ---- batch: 030 ----
mean loss: 876.41
 ---- batch: 040 ----
mean loss: 883.92
 ---- batch: 050 ----
mean loss: 867.00
 ---- batch: 060 ----
mean loss: 878.25
 ---- batch: 070 ----
mean loss: 886.10
 ---- batch: 080 ----
mean loss: 863.99
 ---- batch: 090 ----
mean loss: 872.11
train mean loss: 876.78
epoch train time: 0:00:00.519317
elapsed time: 0:00:43.368345
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 23:14:26.053869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.70
 ---- batch: 020 ----
mean loss: 868.62
 ---- batch: 030 ----
mean loss: 867.85
 ---- batch: 040 ----
mean loss: 875.77
 ---- batch: 050 ----
mean loss: 894.10
 ---- batch: 060 ----
mean loss: 873.59
 ---- batch: 070 ----
mean loss: 857.65
 ---- batch: 080 ----
mean loss: 889.54
 ---- batch: 090 ----
mean loss: 879.21
train mean loss: 875.15
epoch train time: 0:00:00.516995
elapsed time: 0:00:43.885496
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 23:14:26.571020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.37
 ---- batch: 020 ----
mean loss: 888.00
 ---- batch: 030 ----
mean loss: 866.97
 ---- batch: 040 ----
mean loss: 856.53
 ---- batch: 050 ----
mean loss: 868.04
 ---- batch: 060 ----
mean loss: 891.07
 ---- batch: 070 ----
mean loss: 882.11
 ---- batch: 080 ----
mean loss: 871.16
 ---- batch: 090 ----
mean loss: 876.53
train mean loss: 875.87
epoch train time: 0:00:00.541410
elapsed time: 0:00:44.427056
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 23:14:27.112580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.98
 ---- batch: 020 ----
mean loss: 870.49
 ---- batch: 030 ----
mean loss: 860.96
 ---- batch: 040 ----
mean loss: 868.38
 ---- batch: 050 ----
mean loss: 878.78
 ---- batch: 060 ----
mean loss: 879.33
 ---- batch: 070 ----
mean loss: 875.77
 ---- batch: 080 ----
mean loss: 878.36
 ---- batch: 090 ----
mean loss: 885.03
train mean loss: 875.06
epoch train time: 0:00:00.523217
elapsed time: 0:00:44.950426
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 23:14:27.635985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.59
 ---- batch: 020 ----
mean loss: 871.05
 ---- batch: 030 ----
mean loss: 877.12
 ---- batch: 040 ----
mean loss: 857.89
 ---- batch: 050 ----
mean loss: 872.97
 ---- batch: 060 ----
mean loss: 865.50
 ---- batch: 070 ----
mean loss: 881.42
 ---- batch: 080 ----
mean loss: 881.56
 ---- batch: 090 ----
mean loss: 873.43
train mean loss: 876.23
epoch train time: 0:00:00.519401
elapsed time: 0:00:45.470037
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 23:14:28.155590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.73
 ---- batch: 020 ----
mean loss: 888.97
 ---- batch: 030 ----
mean loss: 883.41
 ---- batch: 040 ----
mean loss: 876.11
 ---- batch: 050 ----
mean loss: 881.02
 ---- batch: 060 ----
mean loss: 877.85
 ---- batch: 070 ----
mean loss: 873.89
 ---- batch: 080 ----
mean loss: 865.54
 ---- batch: 090 ----
mean loss: 883.91
train mean loss: 875.19
epoch train time: 0:00:00.522040
elapsed time: 0:00:45.992259
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 23:14:28.677783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.50
 ---- batch: 020 ----
mean loss: 869.74
 ---- batch: 030 ----
mean loss: 863.25
 ---- batch: 040 ----
mean loss: 866.19
 ---- batch: 050 ----
mean loss: 864.48
 ---- batch: 060 ----
mean loss: 902.43
 ---- batch: 070 ----
mean loss: 883.60
 ---- batch: 080 ----
mean loss: 878.41
 ---- batch: 090 ----
mean loss: 869.08
train mean loss: 874.90
epoch train time: 0:00:00.553722
elapsed time: 0:00:46.546136
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 23:14:29.231676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.27
 ---- batch: 020 ----
mean loss: 873.49
 ---- batch: 030 ----
mean loss: 871.79
 ---- batch: 040 ----
mean loss: 872.86
 ---- batch: 050 ----
mean loss: 864.73
 ---- batch: 060 ----
mean loss: 869.84
 ---- batch: 070 ----
mean loss: 879.61
 ---- batch: 080 ----
mean loss: 887.27
 ---- batch: 090 ----
mean loss: 888.06
train mean loss: 875.11
epoch train time: 0:00:00.536140
elapsed time: 0:00:47.082451
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 23:14:29.767967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.94
 ---- batch: 020 ----
mean loss: 869.18
 ---- batch: 030 ----
mean loss: 878.10
 ---- batch: 040 ----
mean loss: 885.39
 ---- batch: 050 ----
mean loss: 873.31
 ---- batch: 060 ----
mean loss: 871.07
 ---- batch: 070 ----
mean loss: 862.61
 ---- batch: 080 ----
mean loss: 892.13
 ---- batch: 090 ----
mean loss: 862.39
train mean loss: 874.90
epoch train time: 0:00:00.518051
elapsed time: 0:00:47.600648
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 23:14:30.286172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.86
 ---- batch: 020 ----
mean loss: 874.97
 ---- batch: 030 ----
mean loss: 861.88
 ---- batch: 040 ----
mean loss: 874.73
 ---- batch: 050 ----
mean loss: 883.70
 ---- batch: 060 ----
mean loss: 883.33
 ---- batch: 070 ----
mean loss: 889.99
 ---- batch: 080 ----
mean loss: 863.96
 ---- batch: 090 ----
mean loss: 859.30
train mean loss: 875.83
epoch train time: 0:00:00.505366
elapsed time: 0:00:48.106180
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 23:14:30.791716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.98
 ---- batch: 020 ----
mean loss: 886.47
 ---- batch: 030 ----
mean loss: 870.55
 ---- batch: 040 ----
mean loss: 876.61
 ---- batch: 050 ----
mean loss: 875.43
 ---- batch: 060 ----
mean loss: 878.11
 ---- batch: 070 ----
mean loss: 872.73
 ---- batch: 080 ----
mean loss: 870.15
 ---- batch: 090 ----
mean loss: 857.33
train mean loss: 874.97
epoch train time: 0:00:00.499157
elapsed time: 0:00:48.605514
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 23:14:31.291037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.88
 ---- batch: 020 ----
mean loss: 871.10
 ---- batch: 030 ----
mean loss: 868.37
 ---- batch: 040 ----
mean loss: 878.46
 ---- batch: 050 ----
mean loss: 892.61
 ---- batch: 060 ----
mean loss: 866.61
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 868.92
 ---- batch: 090 ----
mean loss: 861.10
train mean loss: 875.07
epoch train time: 0:00:00.502451
elapsed time: 0:00:49.108130
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 23:14:31.793739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.11
 ---- batch: 020 ----
mean loss: 879.14
 ---- batch: 030 ----
mean loss: 874.04
 ---- batch: 040 ----
mean loss: 868.00
 ---- batch: 050 ----
mean loss: 876.37
 ---- batch: 060 ----
mean loss: 886.70
 ---- batch: 070 ----
mean loss: 861.68
 ---- batch: 080 ----
mean loss: 882.78
 ---- batch: 090 ----
mean loss: 870.74
train mean loss: 874.91
epoch train time: 0:00:00.506303
elapsed time: 0:00:49.614669
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 23:14:32.300190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.46
 ---- batch: 020 ----
mean loss: 876.58
 ---- batch: 030 ----
mean loss: 877.96
 ---- batch: 040 ----
mean loss: 870.24
 ---- batch: 050 ----
mean loss: 869.36
 ---- batch: 060 ----
mean loss: 868.26
 ---- batch: 070 ----
mean loss: 868.20
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 877.78
train mean loss: 876.12
epoch train time: 0:00:00.508197
elapsed time: 0:00:50.123025
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 23:14:32.808563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.88
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 866.29
 ---- batch: 040 ----
mean loss: 878.07
 ---- batch: 050 ----
mean loss: 889.38
 ---- batch: 060 ----
mean loss: 879.60
 ---- batch: 070 ----
mean loss: 884.78
 ---- batch: 080 ----
mean loss: 857.08
 ---- batch: 090 ----
mean loss: 873.48
train mean loss: 875.11
epoch train time: 0:00:00.497129
elapsed time: 0:00:50.620350
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 23:14:33.305894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 864.81
 ---- batch: 020 ----
mean loss: 872.41
 ---- batch: 030 ----
mean loss: 858.52
 ---- batch: 040 ----
mean loss: 878.56
 ---- batch: 050 ----
mean loss: 870.22
 ---- batch: 060 ----
mean loss: 889.60
 ---- batch: 070 ----
mean loss: 878.83
 ---- batch: 080 ----
mean loss: 875.94
 ---- batch: 090 ----
mean loss: 887.80
train mean loss: 875.71
epoch train time: 0:00:00.508677
elapsed time: 0:00:51.129195
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 23:14:33.814714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.65
 ---- batch: 020 ----
mean loss: 870.46
 ---- batch: 030 ----
mean loss: 875.57
 ---- batch: 040 ----
mean loss: 886.46
 ---- batch: 050 ----
mean loss: 888.80
 ---- batch: 060 ----
mean loss: 856.43
 ---- batch: 070 ----
mean loss: 861.54
 ---- batch: 080 ----
mean loss: 865.08
 ---- batch: 090 ----
mean loss: 908.21
train mean loss: 875.33
epoch train time: 0:00:00.502657
elapsed time: 0:00:51.632004
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 23:14:34.317527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.66
 ---- batch: 020 ----
mean loss: 867.37
 ---- batch: 030 ----
mean loss: 881.86
 ---- batch: 040 ----
mean loss: 896.14
 ---- batch: 050 ----
mean loss: 895.66
 ---- batch: 060 ----
mean loss: 872.61
 ---- batch: 070 ----
mean loss: 871.27
 ---- batch: 080 ----
mean loss: 850.89
 ---- batch: 090 ----
mean loss: 868.18
train mean loss: 874.80
epoch train time: 0:00:00.509894
elapsed time: 0:00:52.142085
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 23:14:34.827616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.34
 ---- batch: 020 ----
mean loss: 884.28
 ---- batch: 030 ----
mean loss: 879.71
 ---- batch: 040 ----
mean loss: 880.95
 ---- batch: 050 ----
mean loss: 865.29
 ---- batch: 060 ----
mean loss: 882.77
 ---- batch: 070 ----
mean loss: 869.99
 ---- batch: 080 ----
mean loss: 882.00
 ---- batch: 090 ----
mean loss: 857.67
train mean loss: 875.43
epoch train time: 0:00:00.505206
elapsed time: 0:00:52.647502
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 23:14:35.333054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.88
 ---- batch: 020 ----
mean loss: 874.52
 ---- batch: 030 ----
mean loss: 869.88
 ---- batch: 040 ----
mean loss: 872.46
 ---- batch: 050 ----
mean loss: 860.89
 ---- batch: 060 ----
mean loss: 884.13
 ---- batch: 070 ----
mean loss: 882.33
 ---- batch: 080 ----
mean loss: 868.00
 ---- batch: 090 ----
mean loss: 884.71
train mean loss: 875.94
epoch train time: 0:00:00.500576
elapsed time: 0:00:53.148257
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 23:14:35.833780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.36
 ---- batch: 020 ----
mean loss: 863.17
 ---- batch: 030 ----
mean loss: 868.58
 ---- batch: 040 ----
mean loss: 872.91
 ---- batch: 050 ----
mean loss: 875.23
 ---- batch: 060 ----
mean loss: 874.13
 ---- batch: 070 ----
mean loss: 875.79
 ---- batch: 080 ----
mean loss: 872.87
 ---- batch: 090 ----
mean loss: 880.58
train mean loss: 875.20
epoch train time: 0:00:00.499802
elapsed time: 0:00:53.648239
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 23:14:36.333761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.07
 ---- batch: 020 ----
mean loss: 888.19
 ---- batch: 030 ----
mean loss: 876.08
 ---- batch: 040 ----
mean loss: 864.90
 ---- batch: 050 ----
mean loss: 887.36
 ---- batch: 060 ----
mean loss: 877.49
 ---- batch: 070 ----
mean loss: 873.63
 ---- batch: 080 ----
mean loss: 861.95
 ---- batch: 090 ----
mean loss: 867.30
train mean loss: 874.55
epoch train time: 0:00:00.506315
elapsed time: 0:00:54.154709
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 23:14:36.840251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.19
 ---- batch: 020 ----
mean loss: 880.51
 ---- batch: 030 ----
mean loss: 878.84
 ---- batch: 040 ----
mean loss: 868.87
 ---- batch: 050 ----
mean loss: 860.44
 ---- batch: 060 ----
mean loss: 885.18
 ---- batch: 070 ----
mean loss: 876.98
 ---- batch: 080 ----
mean loss: 875.13
 ---- batch: 090 ----
mean loss: 883.68
train mean loss: 875.45
epoch train time: 0:00:00.512470
elapsed time: 0:00:54.667360
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 23:14:37.352900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.89
 ---- batch: 020 ----
mean loss: 862.14
 ---- batch: 030 ----
mean loss: 878.96
 ---- batch: 040 ----
mean loss: 849.74
 ---- batch: 050 ----
mean loss: 852.48
 ---- batch: 060 ----
mean loss: 873.50
 ---- batch: 070 ----
mean loss: 889.67
 ---- batch: 080 ----
mean loss: 893.94
 ---- batch: 090 ----
mean loss: 900.18
train mean loss: 875.66
epoch train time: 0:00:00.518291
elapsed time: 0:00:55.185849
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 23:14:37.871369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.25
 ---- batch: 020 ----
mean loss: 859.42
 ---- batch: 030 ----
mean loss: 880.51
 ---- batch: 040 ----
mean loss: 889.92
 ---- batch: 050 ----
mean loss: 869.13
 ---- batch: 060 ----
mean loss: 877.74
 ---- batch: 070 ----
mean loss: 872.11
 ---- batch: 080 ----
mean loss: 886.11
 ---- batch: 090 ----
mean loss: 885.30
train mean loss: 875.24
epoch train time: 0:00:00.503306
elapsed time: 0:00:55.689307
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 23:14:38.374829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.92
 ---- batch: 020 ----
mean loss: 896.00
 ---- batch: 030 ----
mean loss: 895.89
 ---- batch: 040 ----
mean loss: 875.86
 ---- batch: 050 ----
mean loss: 854.47
 ---- batch: 060 ----
mean loss: 885.08
 ---- batch: 070 ----
mean loss: 872.57
 ---- batch: 080 ----
mean loss: 874.54
 ---- batch: 090 ----
mean loss: 870.22
train mean loss: 875.66
epoch train time: 0:00:00.516303
elapsed time: 0:00:56.205773
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 23:14:38.891294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.48
 ---- batch: 020 ----
mean loss: 871.28
 ---- batch: 030 ----
mean loss: 875.70
 ---- batch: 040 ----
mean loss: 877.23
 ---- batch: 050 ----
mean loss: 878.31
 ---- batch: 060 ----
mean loss: 867.03
 ---- batch: 070 ----
mean loss: 879.63
 ---- batch: 080 ----
mean loss: 876.95
 ---- batch: 090 ----
mean loss: 864.58
train mean loss: 874.88
epoch train time: 0:00:00.499002
elapsed time: 0:00:56.704921
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 23:14:39.390442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.57
 ---- batch: 020 ----
mean loss: 874.57
 ---- batch: 030 ----
mean loss: 885.12
 ---- batch: 040 ----
mean loss: 870.30
 ---- batch: 050 ----
mean loss: 885.77
 ---- batch: 060 ----
mean loss: 877.45
 ---- batch: 070 ----
mean loss: 893.99
 ---- batch: 080 ----
mean loss: 865.68
 ---- batch: 090 ----
mean loss: 871.94
train mean loss: 875.10
epoch train time: 0:00:00.509308
elapsed time: 0:00:57.214374
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 23:14:39.899896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.71
 ---- batch: 020 ----
mean loss: 867.95
 ---- batch: 030 ----
mean loss: 868.70
 ---- batch: 040 ----
mean loss: 880.39
 ---- batch: 050 ----
mean loss: 874.64
 ---- batch: 060 ----
mean loss: 876.82
 ---- batch: 070 ----
mean loss: 871.95
 ---- batch: 080 ----
mean loss: 877.46
 ---- batch: 090 ----
mean loss: 896.57
train mean loss: 874.86
epoch train time: 0:00:00.507344
elapsed time: 0:00:57.721868
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 23:14:40.407426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.53
 ---- batch: 020 ----
mean loss: 874.88
 ---- batch: 030 ----
mean loss: 842.41
 ---- batch: 040 ----
mean loss: 870.90
 ---- batch: 050 ----
mean loss: 879.93
 ---- batch: 060 ----
mean loss: 870.33
 ---- batch: 070 ----
mean loss: 893.30
 ---- batch: 080 ----
mean loss: 874.29
 ---- batch: 090 ----
mean loss: 901.58
train mean loss: 874.93
epoch train time: 0:00:00.510598
elapsed time: 0:00:58.232657
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 23:14:40.918196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.75
 ---- batch: 020 ----
mean loss: 885.85
 ---- batch: 030 ----
mean loss: 869.46
 ---- batch: 040 ----
mean loss: 881.20
 ---- batch: 050 ----
mean loss: 864.83
 ---- batch: 060 ----
mean loss: 865.54
 ---- batch: 070 ----
mean loss: 846.26
 ---- batch: 080 ----
mean loss: 835.24
 ---- batch: 090 ----
mean loss: 799.40
train mean loss: 852.40
epoch train time: 0:00:00.511691
elapsed time: 0:00:58.744515
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 23:14:41.430053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.04
 ---- batch: 020 ----
mean loss: 743.86
 ---- batch: 030 ----
mean loss: 703.84
 ---- batch: 040 ----
mean loss: 666.30
 ---- batch: 050 ----
mean loss: 605.93
 ---- batch: 060 ----
mean loss: 552.28
 ---- batch: 070 ----
mean loss: 502.34
 ---- batch: 080 ----
mean loss: 456.24
 ---- batch: 090 ----
mean loss: 438.61
train mean loss: 592.27
epoch train time: 0:00:00.510912
elapsed time: 0:00:59.255594
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 23:14:41.941116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.28
 ---- batch: 020 ----
mean loss: 393.00
 ---- batch: 030 ----
mean loss: 392.91
 ---- batch: 040 ----
mean loss: 380.57
 ---- batch: 050 ----
mean loss: 363.61
 ---- batch: 060 ----
mean loss: 357.42
 ---- batch: 070 ----
mean loss: 344.56
 ---- batch: 080 ----
mean loss: 348.62
 ---- batch: 090 ----
mean loss: 341.95
train mean loss: 367.80
epoch train time: 0:00:00.507040
elapsed time: 0:00:59.762811
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 23:14:42.448331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.72
 ---- batch: 020 ----
mean loss: 319.37
 ---- batch: 030 ----
mean loss: 307.66
 ---- batch: 040 ----
mean loss: 302.77
 ---- batch: 050 ----
mean loss: 305.83
 ---- batch: 060 ----
mean loss: 308.76
 ---- batch: 070 ----
mean loss: 288.72
 ---- batch: 080 ----
mean loss: 292.93
 ---- batch: 090 ----
mean loss: 298.77
train mean loss: 304.81
epoch train time: 0:00:00.507194
elapsed time: 0:01:00.270153
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 23:14:42.955677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.63
 ---- batch: 020 ----
mean loss: 278.82
 ---- batch: 030 ----
mean loss: 289.31
 ---- batch: 040 ----
mean loss: 271.94
 ---- batch: 050 ----
mean loss: 269.38
 ---- batch: 060 ----
mean loss: 266.92
 ---- batch: 070 ----
mean loss: 261.23
 ---- batch: 080 ----
mean loss: 276.73
 ---- batch: 090 ----
mean loss: 260.48
train mean loss: 272.73
epoch train time: 0:00:00.515432
elapsed time: 0:01:00.785735
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 23:14:43.471272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.02
 ---- batch: 020 ----
mean loss: 265.37
 ---- batch: 030 ----
mean loss: 260.56
 ---- batch: 040 ----
mean loss: 257.54
 ---- batch: 050 ----
mean loss: 253.90
 ---- batch: 060 ----
mean loss: 254.46
 ---- batch: 070 ----
mean loss: 263.04
 ---- batch: 080 ----
mean loss: 245.76
 ---- batch: 090 ----
mean loss: 250.82
train mean loss: 256.60
epoch train time: 0:00:00.515707
elapsed time: 0:01:01.301605
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 23:14:43.987128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.81
 ---- batch: 020 ----
mean loss: 244.91
 ---- batch: 030 ----
mean loss: 245.72
 ---- batch: 040 ----
mean loss: 248.13
 ---- batch: 050 ----
mean loss: 245.59
 ---- batch: 060 ----
mean loss: 246.41
 ---- batch: 070 ----
mean loss: 252.14
 ---- batch: 080 ----
mean loss: 229.84
 ---- batch: 090 ----
mean loss: 233.66
train mean loss: 244.20
epoch train time: 0:00:00.503044
elapsed time: 0:01:01.804815
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 23:14:44.490402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.49
 ---- batch: 020 ----
mean loss: 233.10
 ---- batch: 030 ----
mean loss: 239.48
 ---- batch: 040 ----
mean loss: 240.45
 ---- batch: 050 ----
mean loss: 237.30
 ---- batch: 060 ----
mean loss: 236.81
 ---- batch: 070 ----
mean loss: 235.73
 ---- batch: 080 ----
mean loss: 232.69
 ---- batch: 090 ----
mean loss: 239.73
train mean loss: 237.01
epoch train time: 0:00:00.510084
elapsed time: 0:01:02.315121
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 23:14:45.000647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.90
 ---- batch: 020 ----
mean loss: 235.36
 ---- batch: 030 ----
mean loss: 231.90
 ---- batch: 040 ----
mean loss: 229.19
 ---- batch: 050 ----
mean loss: 234.19
 ---- batch: 060 ----
mean loss: 237.88
 ---- batch: 070 ----
mean loss: 225.01
 ---- batch: 080 ----
mean loss: 229.77
 ---- batch: 090 ----
mean loss: 232.37
train mean loss: 231.34
epoch train time: 0:00:00.503577
elapsed time: 0:01:02.818911
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 23:14:45.504463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.98
 ---- batch: 020 ----
mean loss: 223.35
 ---- batch: 030 ----
mean loss: 226.45
 ---- batch: 040 ----
mean loss: 230.33
 ---- batch: 050 ----
mean loss: 224.18
 ---- batch: 060 ----
mean loss: 228.94
 ---- batch: 070 ----
mean loss: 224.31
 ---- batch: 080 ----
mean loss: 228.68
 ---- batch: 090 ----
mean loss: 232.96
train mean loss: 227.90
epoch train time: 0:00:00.512160
elapsed time: 0:01:03.331248
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 23:14:46.016768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.47
 ---- batch: 020 ----
mean loss: 232.16
 ---- batch: 030 ----
mean loss: 218.96
 ---- batch: 040 ----
mean loss: 221.19
 ---- batch: 050 ----
mean loss: 225.75
 ---- batch: 060 ----
mean loss: 225.62
 ---- batch: 070 ----
mean loss: 224.25
 ---- batch: 080 ----
mean loss: 222.94
 ---- batch: 090 ----
mean loss: 226.13
train mean loss: 224.58
epoch train time: 0:00:00.500833
elapsed time: 0:01:03.832251
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 23:14:46.517772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.74
 ---- batch: 020 ----
mean loss: 222.64
 ---- batch: 030 ----
mean loss: 218.78
 ---- batch: 040 ----
mean loss: 219.40
 ---- batch: 050 ----
mean loss: 213.61
 ---- batch: 060 ----
mean loss: 218.31
 ---- batch: 070 ----
mean loss: 224.82
 ---- batch: 080 ----
mean loss: 224.45
 ---- batch: 090 ----
mean loss: 220.56
train mean loss: 220.46
epoch train time: 0:00:00.516035
elapsed time: 0:01:04.348435
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 23:14:47.033960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.56
 ---- batch: 020 ----
mean loss: 217.79
 ---- batch: 030 ----
mean loss: 213.38
 ---- batch: 040 ----
mean loss: 213.15
 ---- batch: 050 ----
mean loss: 220.16
 ---- batch: 060 ----
mean loss: 225.95
 ---- batch: 070 ----
mean loss: 215.51
 ---- batch: 080 ----
mean loss: 211.15
 ---- batch: 090 ----
mean loss: 227.01
train mean loss: 218.10
epoch train time: 0:00:00.501974
elapsed time: 0:01:04.850557
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 23:14:47.536078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.24
 ---- batch: 020 ----
mean loss: 206.47
 ---- batch: 030 ----
mean loss: 218.24
 ---- batch: 040 ----
mean loss: 207.06
 ---- batch: 050 ----
mean loss: 216.02
 ---- batch: 060 ----
mean loss: 219.99
 ---- batch: 070 ----
mean loss: 221.99
 ---- batch: 080 ----
mean loss: 219.72
 ---- batch: 090 ----
mean loss: 215.14
train mean loss: 215.11
epoch train time: 0:00:00.499684
elapsed time: 0:01:05.350388
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 23:14:48.035926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.97
 ---- batch: 020 ----
mean loss: 215.43
 ---- batch: 030 ----
mean loss: 210.77
 ---- batch: 040 ----
mean loss: 214.83
 ---- batch: 050 ----
mean loss: 221.84
 ---- batch: 060 ----
mean loss: 208.09
 ---- batch: 070 ----
mean loss: 208.14
 ---- batch: 080 ----
mean loss: 208.69
 ---- batch: 090 ----
mean loss: 208.09
train mean loss: 211.79
epoch train time: 0:00:00.496519
elapsed time: 0:01:05.847095
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 23:14:48.532613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.23
 ---- batch: 020 ----
mean loss: 208.34
 ---- batch: 030 ----
mean loss: 216.27
 ---- batch: 040 ----
mean loss: 216.73
 ---- batch: 050 ----
mean loss: 218.24
 ---- batch: 060 ----
mean loss: 210.40
 ---- batch: 070 ----
mean loss: 206.12
 ---- batch: 080 ----
mean loss: 205.85
 ---- batch: 090 ----
mean loss: 207.33
train mean loss: 211.32
epoch train time: 0:00:00.518637
elapsed time: 0:01:06.365893
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 23:14:49.051414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.29
 ---- batch: 020 ----
mean loss: 205.44
 ---- batch: 030 ----
mean loss: 211.88
 ---- batch: 040 ----
mean loss: 204.42
 ---- batch: 050 ----
mean loss: 209.87
 ---- batch: 060 ----
mean loss: 210.80
 ---- batch: 070 ----
mean loss: 207.14
 ---- batch: 080 ----
mean loss: 216.25
 ---- batch: 090 ----
mean loss: 205.10
train mean loss: 208.19
epoch train time: 0:00:00.511528
elapsed time: 0:01:06.877604
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 23:14:49.563125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.66
 ---- batch: 020 ----
mean loss: 199.90
 ---- batch: 030 ----
mean loss: 214.10
 ---- batch: 040 ----
mean loss: 213.85
 ---- batch: 050 ----
mean loss: 208.30
 ---- batch: 060 ----
mean loss: 213.76
 ---- batch: 070 ----
mean loss: 202.22
 ---- batch: 080 ----
mean loss: 201.95
 ---- batch: 090 ----
mean loss: 198.86
train mean loss: 205.88
epoch train time: 0:00:00.512179
elapsed time: 0:01:07.389932
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 23:14:50.075465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.21
 ---- batch: 020 ----
mean loss: 208.16
 ---- batch: 030 ----
mean loss: 197.72
 ---- batch: 040 ----
mean loss: 198.12
 ---- batch: 050 ----
mean loss: 199.41
 ---- batch: 060 ----
mean loss: 201.54
 ---- batch: 070 ----
mean loss: 206.13
 ---- batch: 080 ----
mean loss: 207.31
 ---- batch: 090 ----
mean loss: 196.19
train mean loss: 202.42
epoch train time: 0:00:00.502472
elapsed time: 0:01:07.892576
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 23:14:50.578090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.36
 ---- batch: 020 ----
mean loss: 204.35
 ---- batch: 030 ----
mean loss: 194.61
 ---- batch: 040 ----
mean loss: 198.72
 ---- batch: 050 ----
mean loss: 194.02
 ---- batch: 060 ----
mean loss: 200.53
 ---- batch: 070 ----
mean loss: 209.71
 ---- batch: 080 ----
mean loss: 200.58
 ---- batch: 090 ----
mean loss: 204.85
train mean loss: 201.23
epoch train time: 0:00:00.509829
elapsed time: 0:01:08.402542
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 23:14:51.088064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.09
 ---- batch: 020 ----
mean loss: 194.24
 ---- batch: 030 ----
mean loss: 195.57
 ---- batch: 040 ----
mean loss: 196.31
 ---- batch: 050 ----
mean loss: 197.01
 ---- batch: 060 ----
mean loss: 193.88
 ---- batch: 070 ----
mean loss: 189.45
 ---- batch: 080 ----
mean loss: 204.26
 ---- batch: 090 ----
mean loss: 206.41
train mean loss: 198.79
epoch train time: 0:00:00.505024
elapsed time: 0:01:08.907748
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 23:14:51.593289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.90
 ---- batch: 020 ----
mean loss: 190.01
 ---- batch: 030 ----
mean loss: 195.50
 ---- batch: 040 ----
mean loss: 205.04
 ---- batch: 050 ----
mean loss: 195.62
 ---- batch: 060 ----
mean loss: 195.19
 ---- batch: 070 ----
mean loss: 198.01
 ---- batch: 080 ----
mean loss: 196.77
 ---- batch: 090 ----
mean loss: 198.08
train mean loss: 196.08
epoch train time: 0:00:00.522011
elapsed time: 0:01:09.429927
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 23:14:52.115474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.46
 ---- batch: 020 ----
mean loss: 193.07
 ---- batch: 030 ----
mean loss: 196.05
 ---- batch: 040 ----
mean loss: 197.06
 ---- batch: 050 ----
mean loss: 193.22
 ---- batch: 060 ----
mean loss: 199.55
 ---- batch: 070 ----
mean loss: 193.00
 ---- batch: 080 ----
mean loss: 199.25
 ---- batch: 090 ----
mean loss: 200.72
train mean loss: 196.05
epoch train time: 0:00:00.504874
elapsed time: 0:01:09.934971
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 23:14:52.620493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.47
 ---- batch: 020 ----
mean loss: 179.67
 ---- batch: 030 ----
mean loss: 196.70
 ---- batch: 040 ----
mean loss: 190.88
 ---- batch: 050 ----
mean loss: 203.93
 ---- batch: 060 ----
mean loss: 195.38
 ---- batch: 070 ----
mean loss: 196.65
 ---- batch: 080 ----
mean loss: 201.38
 ---- batch: 090 ----
mean loss: 197.04
train mean loss: 193.65
epoch train time: 0:00:00.503055
elapsed time: 0:01:10.438173
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 23:14:53.123693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.48
 ---- batch: 020 ----
mean loss: 189.94
 ---- batch: 030 ----
mean loss: 199.02
 ---- batch: 040 ----
mean loss: 189.26
 ---- batch: 050 ----
mean loss: 190.62
 ---- batch: 060 ----
mean loss: 196.50
 ---- batch: 070 ----
mean loss: 189.81
 ---- batch: 080 ----
mean loss: 200.44
 ---- batch: 090 ----
mean loss: 192.84
train mean loss: 192.69
epoch train time: 0:00:00.499480
elapsed time: 0:01:10.937799
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 23:14:53.623320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.38
 ---- batch: 020 ----
mean loss: 190.65
 ---- batch: 030 ----
mean loss: 187.44
 ---- batch: 040 ----
mean loss: 195.65
 ---- batch: 050 ----
mean loss: 189.60
 ---- batch: 060 ----
mean loss: 190.74
 ---- batch: 070 ----
mean loss: 189.05
 ---- batch: 080 ----
mean loss: 190.31
 ---- batch: 090 ----
mean loss: 198.84
train mean loss: 191.66
epoch train time: 0:00:00.521290
elapsed time: 0:01:11.459240
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 23:14:54.144764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.66
 ---- batch: 020 ----
mean loss: 189.96
 ---- batch: 030 ----
mean loss: 190.13
 ---- batch: 040 ----
mean loss: 194.54
 ---- batch: 050 ----
mean loss: 186.07
 ---- batch: 060 ----
mean loss: 186.57
 ---- batch: 070 ----
mean loss: 193.10
 ---- batch: 080 ----
mean loss: 191.20
 ---- batch: 090 ----
mean loss: 191.13
train mean loss: 189.96
epoch train time: 0:00:00.514880
elapsed time: 0:01:11.974275
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 23:14:54.659798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.10
 ---- batch: 020 ----
mean loss: 185.57
 ---- batch: 030 ----
mean loss: 177.94
 ---- batch: 040 ----
mean loss: 188.23
 ---- batch: 050 ----
mean loss: 191.91
 ---- batch: 060 ----
mean loss: 186.02
 ---- batch: 070 ----
mean loss: 195.23
 ---- batch: 080 ----
mean loss: 193.20
 ---- batch: 090 ----
mean loss: 192.24
train mean loss: 188.68
epoch train time: 0:00:00.518413
elapsed time: 0:01:12.492838
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 23:14:55.178361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.39
 ---- batch: 020 ----
mean loss: 188.38
 ---- batch: 030 ----
mean loss: 187.71
 ---- batch: 040 ----
mean loss: 194.93
 ---- batch: 050 ----
mean loss: 176.72
 ---- batch: 060 ----
mean loss: 185.87
 ---- batch: 070 ----
mean loss: 185.17
 ---- batch: 080 ----
mean loss: 187.03
 ---- batch: 090 ----
mean loss: 188.64
train mean loss: 186.26
epoch train time: 0:00:00.503746
elapsed time: 0:01:12.996739
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 23:14:55.682279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.98
 ---- batch: 020 ----
mean loss: 182.97
 ---- batch: 030 ----
mean loss: 181.65
 ---- batch: 040 ----
mean loss: 192.94
 ---- batch: 050 ----
mean loss: 182.93
 ---- batch: 060 ----
mean loss: 181.58
 ---- batch: 070 ----
mean loss: 195.88
 ---- batch: 080 ----
mean loss: 189.00
 ---- batch: 090 ----
mean loss: 190.56
train mean loss: 185.72
epoch train time: 0:00:00.513048
elapsed time: 0:01:13.509957
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 23:14:56.195497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.22
 ---- batch: 020 ----
mean loss: 178.64
 ---- batch: 030 ----
mean loss: 182.60
 ---- batch: 040 ----
mean loss: 194.08
 ---- batch: 050 ----
mean loss: 185.11
 ---- batch: 060 ----
mean loss: 188.36
 ---- batch: 070 ----
mean loss: 181.75
 ---- batch: 080 ----
mean loss: 185.56
 ---- batch: 090 ----
mean loss: 188.54
train mean loss: 184.67
epoch train time: 0:00:00.516013
elapsed time: 0:01:14.026144
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 23:14:56.711686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.01
 ---- batch: 020 ----
mean loss: 176.02
 ---- batch: 030 ----
mean loss: 178.76
 ---- batch: 040 ----
mean loss: 181.61
 ---- batch: 050 ----
mean loss: 184.27
 ---- batch: 060 ----
mean loss: 184.83
 ---- batch: 070 ----
mean loss: 185.24
 ---- batch: 080 ----
mean loss: 184.24
 ---- batch: 090 ----
mean loss: 190.69
train mean loss: 183.73
epoch train time: 0:00:00.511724
elapsed time: 0:01:14.538039
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 23:14:57.223570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.76
 ---- batch: 020 ----
mean loss: 180.65
 ---- batch: 030 ----
mean loss: 189.44
 ---- batch: 040 ----
mean loss: 182.23
 ---- batch: 050 ----
mean loss: 186.04
 ---- batch: 060 ----
mean loss: 179.41
 ---- batch: 070 ----
mean loss: 178.35
 ---- batch: 080 ----
mean loss: 180.75
 ---- batch: 090 ----
mean loss: 181.73
train mean loss: 181.81
epoch train time: 0:00:00.511318
elapsed time: 0:01:15.049517
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 23:14:57.735045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.72
 ---- batch: 020 ----
mean loss: 177.40
 ---- batch: 030 ----
mean loss: 173.21
 ---- batch: 040 ----
mean loss: 180.16
 ---- batch: 050 ----
mean loss: 188.41
 ---- batch: 060 ----
mean loss: 183.75
 ---- batch: 070 ----
mean loss: 176.36
 ---- batch: 080 ----
mean loss: 191.67
 ---- batch: 090 ----
mean loss: 182.45
train mean loss: 181.43
epoch train time: 0:00:00.499595
elapsed time: 0:01:15.549261
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 23:14:58.234781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.20
 ---- batch: 020 ----
mean loss: 171.98
 ---- batch: 030 ----
mean loss: 182.00
 ---- batch: 040 ----
mean loss: 178.89
 ---- batch: 050 ----
mean loss: 183.93
 ---- batch: 060 ----
mean loss: 186.78
 ---- batch: 070 ----
mean loss: 176.30
 ---- batch: 080 ----
mean loss: 183.49
 ---- batch: 090 ----
mean loss: 186.03
train mean loss: 180.79
epoch train time: 0:00:00.501008
elapsed time: 0:01:16.050435
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 23:14:58.735957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.43
 ---- batch: 020 ----
mean loss: 173.93
 ---- batch: 030 ----
mean loss: 176.12
 ---- batch: 040 ----
mean loss: 177.05
 ---- batch: 050 ----
mean loss: 180.23
 ---- batch: 060 ----
mean loss: 192.04
 ---- batch: 070 ----
mean loss: 171.24
 ---- batch: 080 ----
mean loss: 179.70
 ---- batch: 090 ----
mean loss: 191.02
train mean loss: 180.44
epoch train time: 0:00:00.513457
elapsed time: 0:01:16.564048
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 23:14:59.249563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.30
 ---- batch: 020 ----
mean loss: 185.50
 ---- batch: 030 ----
mean loss: 172.04
 ---- batch: 040 ----
mean loss: 183.18
 ---- batch: 050 ----
mean loss: 180.16
 ---- batch: 060 ----
mean loss: 172.97
 ---- batch: 070 ----
mean loss: 169.41
 ---- batch: 080 ----
mean loss: 184.84
 ---- batch: 090 ----
mean loss: 185.28
train mean loss: 179.61
epoch train time: 0:00:00.505337
elapsed time: 0:01:17.069551
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 23:14:59.755088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.82
 ---- batch: 020 ----
mean loss: 175.71
 ---- batch: 030 ----
mean loss: 182.56
 ---- batch: 040 ----
mean loss: 179.36
 ---- batch: 050 ----
mean loss: 176.01
 ---- batch: 060 ----
mean loss: 177.15
 ---- batch: 070 ----
mean loss: 177.21
 ---- batch: 080 ----
mean loss: 181.69
 ---- batch: 090 ----
mean loss: 169.50
train mean loss: 177.35
epoch train time: 0:00:00.500167
elapsed time: 0:01:17.569880
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 23:15:00.255401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.55
 ---- batch: 020 ----
mean loss: 174.75
 ---- batch: 030 ----
mean loss: 173.23
 ---- batch: 040 ----
mean loss: 180.48
 ---- batch: 050 ----
mean loss: 165.63
 ---- batch: 060 ----
mean loss: 177.45
 ---- batch: 070 ----
mean loss: 179.98
 ---- batch: 080 ----
mean loss: 179.40
 ---- batch: 090 ----
mean loss: 183.28
train mean loss: 176.79
epoch train time: 0:00:00.497973
elapsed time: 0:01:18.068018
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 23:15:00.753542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.10
 ---- batch: 020 ----
mean loss: 174.39
 ---- batch: 030 ----
mean loss: 174.91
 ---- batch: 040 ----
mean loss: 172.92
 ---- batch: 050 ----
mean loss: 173.33
 ---- batch: 060 ----
mean loss: 183.92
 ---- batch: 070 ----
mean loss: 176.12
 ---- batch: 080 ----
mean loss: 176.92
 ---- batch: 090 ----
mean loss: 173.78
train mean loss: 176.43
epoch train time: 0:00:00.502926
elapsed time: 0:01:18.571095
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 23:15:01.256633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.77
 ---- batch: 020 ----
mean loss: 179.50
 ---- batch: 030 ----
mean loss: 173.62
 ---- batch: 040 ----
mean loss: 176.13
 ---- batch: 050 ----
mean loss: 177.82
 ---- batch: 060 ----
mean loss: 175.05
 ---- batch: 070 ----
mean loss: 181.13
 ---- batch: 080 ----
mean loss: 176.89
 ---- batch: 090 ----
mean loss: 175.95
train mean loss: 175.89
epoch train time: 0:00:00.504534
elapsed time: 0:01:19.075793
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 23:15:01.761314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.04
 ---- batch: 020 ----
mean loss: 168.46
 ---- batch: 030 ----
mean loss: 166.40
 ---- batch: 040 ----
mean loss: 176.07
 ---- batch: 050 ----
mean loss: 178.38
 ---- batch: 060 ----
mean loss: 183.96
 ---- batch: 070 ----
mean loss: 172.12
 ---- batch: 080 ----
mean loss: 185.35
 ---- batch: 090 ----
mean loss: 174.69
train mean loss: 174.96
epoch train time: 0:00:00.509205
elapsed time: 0:01:19.585152
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 23:15:02.270675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.65
 ---- batch: 020 ----
mean loss: 177.63
 ---- batch: 030 ----
mean loss: 168.18
 ---- batch: 040 ----
mean loss: 169.54
 ---- batch: 050 ----
mean loss: 176.72
 ---- batch: 060 ----
mean loss: 181.81
 ---- batch: 070 ----
mean loss: 174.68
 ---- batch: 080 ----
mean loss: 175.70
 ---- batch: 090 ----
mean loss: 172.09
train mean loss: 174.82
epoch train time: 0:00:00.516751
elapsed time: 0:01:20.102050
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 23:15:02.787573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.38
 ---- batch: 020 ----
mean loss: 177.36
 ---- batch: 030 ----
mean loss: 174.20
 ---- batch: 040 ----
mean loss: 170.82
 ---- batch: 050 ----
mean loss: 164.86
 ---- batch: 060 ----
mean loss: 177.81
 ---- batch: 070 ----
mean loss: 170.03
 ---- batch: 080 ----
mean loss: 180.31
 ---- batch: 090 ----
mean loss: 175.61
train mean loss: 173.45
epoch train time: 0:00:00.505621
elapsed time: 0:01:20.607822
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 23:15:03.293365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.80
 ---- batch: 020 ----
mean loss: 173.31
 ---- batch: 030 ----
mean loss: 171.08
 ---- batch: 040 ----
mean loss: 175.78
 ---- batch: 050 ----
mean loss: 171.87
 ---- batch: 060 ----
mean loss: 181.46
 ---- batch: 070 ----
mean loss: 179.24
 ---- batch: 080 ----
mean loss: 170.69
 ---- batch: 090 ----
mean loss: 170.94
train mean loss: 172.54
epoch train time: 0:00:00.508224
elapsed time: 0:01:21.116249
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 23:15:03.801781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.30
 ---- batch: 020 ----
mean loss: 162.73
 ---- batch: 030 ----
mean loss: 179.89
 ---- batch: 040 ----
mean loss: 166.15
 ---- batch: 050 ----
mean loss: 165.83
 ---- batch: 060 ----
mean loss: 181.46
 ---- batch: 070 ----
mean loss: 173.92
 ---- batch: 080 ----
mean loss: 172.11
 ---- batch: 090 ----
mean loss: 177.74
train mean loss: 172.14
epoch train time: 0:00:00.512268
elapsed time: 0:01:21.628676
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 23:15:04.314197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.83
 ---- batch: 020 ----
mean loss: 170.14
 ---- batch: 030 ----
mean loss: 171.24
 ---- batch: 040 ----
mean loss: 168.10
 ---- batch: 050 ----
mean loss: 178.76
 ---- batch: 060 ----
mean loss: 179.49
 ---- batch: 070 ----
mean loss: 171.04
 ---- batch: 080 ----
mean loss: 172.76
 ---- batch: 090 ----
mean loss: 174.31
train mean loss: 172.73
epoch train time: 0:00:00.510278
elapsed time: 0:01:22.139100
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 23:15:04.824620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.82
 ---- batch: 020 ----
mean loss: 162.59
 ---- batch: 030 ----
mean loss: 171.67
 ---- batch: 040 ----
mean loss: 168.18
 ---- batch: 050 ----
mean loss: 167.70
 ---- batch: 060 ----
mean loss: 169.75
 ---- batch: 070 ----
mean loss: 178.70
 ---- batch: 080 ----
mean loss: 173.57
 ---- batch: 090 ----
mean loss: 169.54
train mean loss: 170.03
epoch train time: 0:00:00.505497
elapsed time: 0:01:22.644743
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 23:15:05.330293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.85
 ---- batch: 020 ----
mean loss: 165.00
 ---- batch: 030 ----
mean loss: 169.07
 ---- batch: 040 ----
mean loss: 171.98
 ---- batch: 050 ----
mean loss: 167.47
 ---- batch: 060 ----
mean loss: 167.52
 ---- batch: 070 ----
mean loss: 172.97
 ---- batch: 080 ----
mean loss: 173.71
 ---- batch: 090 ----
mean loss: 176.97
train mean loss: 169.71
epoch train time: 0:00:00.505926
elapsed time: 0:01:23.150844
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 23:15:05.836367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.03
 ---- batch: 020 ----
mean loss: 171.35
 ---- batch: 030 ----
mean loss: 165.05
 ---- batch: 040 ----
mean loss: 164.77
 ---- batch: 050 ----
mean loss: 171.64
 ---- batch: 060 ----
mean loss: 169.69
 ---- batch: 070 ----
mean loss: 174.22
 ---- batch: 080 ----
mean loss: 170.71
 ---- batch: 090 ----
mean loss: 166.34
train mean loss: 169.23
epoch train time: 0:00:00.501463
elapsed time: 0:01:23.652455
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 23:15:06.337984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.58
 ---- batch: 020 ----
mean loss: 160.21
 ---- batch: 030 ----
mean loss: 166.74
 ---- batch: 040 ----
mean loss: 163.83
 ---- batch: 050 ----
mean loss: 173.84
 ---- batch: 060 ----
mean loss: 173.63
 ---- batch: 070 ----
mean loss: 167.08
 ---- batch: 080 ----
mean loss: 176.02
 ---- batch: 090 ----
mean loss: 169.08
train mean loss: 169.53
epoch train time: 0:00:00.498762
elapsed time: 0:01:24.151371
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 23:15:06.836890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.06
 ---- batch: 020 ----
mean loss: 163.37
 ---- batch: 030 ----
mean loss: 167.24
 ---- batch: 040 ----
mean loss: 164.72
 ---- batch: 050 ----
mean loss: 169.58
 ---- batch: 060 ----
mean loss: 162.71
 ---- batch: 070 ----
mean loss: 168.05
 ---- batch: 080 ----
mean loss: 168.94
 ---- batch: 090 ----
mean loss: 174.35
train mean loss: 167.89
epoch train time: 0:00:00.496862
elapsed time: 0:01:24.648376
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 23:15:07.333911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.17
 ---- batch: 020 ----
mean loss: 169.55
 ---- batch: 030 ----
mean loss: 168.12
 ---- batch: 040 ----
mean loss: 166.62
 ---- batch: 050 ----
mean loss: 171.96
 ---- batch: 060 ----
mean loss: 173.08
 ---- batch: 070 ----
mean loss: 167.68
 ---- batch: 080 ----
mean loss: 164.18
 ---- batch: 090 ----
mean loss: 164.58
train mean loss: 168.48
epoch train time: 0:00:00.496725
elapsed time: 0:01:25.145273
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 23:15:07.830804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.76
 ---- batch: 020 ----
mean loss: 166.90
 ---- batch: 030 ----
mean loss: 169.72
 ---- batch: 040 ----
mean loss: 167.76
 ---- batch: 050 ----
mean loss: 167.04
 ---- batch: 060 ----
mean loss: 168.91
 ---- batch: 070 ----
mean loss: 164.38
 ---- batch: 080 ----
mean loss: 166.44
 ---- batch: 090 ----
mean loss: 170.15
train mean loss: 168.00
epoch train time: 0:00:00.502395
elapsed time: 0:01:25.647839
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 23:15:08.333361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.66
 ---- batch: 020 ----
mean loss: 164.92
 ---- batch: 030 ----
mean loss: 162.85
 ---- batch: 040 ----
mean loss: 166.58
 ---- batch: 050 ----
mean loss: 165.77
 ---- batch: 060 ----
mean loss: 164.96
 ---- batch: 070 ----
mean loss: 170.21
 ---- batch: 080 ----
mean loss: 159.57
 ---- batch: 090 ----
mean loss: 169.08
train mean loss: 166.02
epoch train time: 0:00:00.505611
elapsed time: 0:01:26.153598
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 23:15:08.839118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.42
 ---- batch: 020 ----
mean loss: 166.15
 ---- batch: 030 ----
mean loss: 161.80
 ---- batch: 040 ----
mean loss: 168.94
 ---- batch: 050 ----
mean loss: 166.32
 ---- batch: 060 ----
mean loss: 170.49
 ---- batch: 070 ----
mean loss: 161.81
 ---- batch: 080 ----
mean loss: 161.77
 ---- batch: 090 ----
mean loss: 164.76
train mean loss: 165.87
epoch train time: 0:00:00.504720
elapsed time: 0:01:26.658461
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 23:15:09.343982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.74
 ---- batch: 020 ----
mean loss: 159.32
 ---- batch: 030 ----
mean loss: 171.33
 ---- batch: 040 ----
mean loss: 163.35
 ---- batch: 050 ----
mean loss: 157.84
 ---- batch: 060 ----
mean loss: 169.09
 ---- batch: 070 ----
mean loss: 168.87
 ---- batch: 080 ----
mean loss: 167.97
 ---- batch: 090 ----
mean loss: 168.55
train mean loss: 165.35
epoch train time: 0:00:00.504945
elapsed time: 0:01:27.163560
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 23:15:09.849097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.08
 ---- batch: 020 ----
mean loss: 169.97
 ---- batch: 030 ----
mean loss: 156.81
 ---- batch: 040 ----
mean loss: 160.20
 ---- batch: 050 ----
mean loss: 172.10
 ---- batch: 060 ----
mean loss: 166.12
 ---- batch: 070 ----
mean loss: 166.70
 ---- batch: 080 ----
mean loss: 165.21
 ---- batch: 090 ----
mean loss: 167.32
train mean loss: 165.08
epoch train time: 0:00:00.505361
elapsed time: 0:01:27.669117
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 23:15:10.354628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.31
 ---- batch: 020 ----
mean loss: 164.55
 ---- batch: 030 ----
mean loss: 162.65
 ---- batch: 040 ----
mean loss: 166.25
 ---- batch: 050 ----
mean loss: 164.57
 ---- batch: 060 ----
mean loss: 162.63
 ---- batch: 070 ----
mean loss: 168.26
 ---- batch: 080 ----
mean loss: 163.39
 ---- batch: 090 ----
mean loss: 167.64
train mean loss: 163.96
epoch train time: 0:00:00.503301
elapsed time: 0:01:28.172557
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 23:15:10.858112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.07
 ---- batch: 020 ----
mean loss: 166.85
 ---- batch: 030 ----
mean loss: 159.74
 ---- batch: 040 ----
mean loss: 162.19
 ---- batch: 050 ----
mean loss: 162.62
 ---- batch: 060 ----
mean loss: 165.87
 ---- batch: 070 ----
mean loss: 160.87
 ---- batch: 080 ----
mean loss: 165.31
 ---- batch: 090 ----
mean loss: 162.05
train mean loss: 163.80
epoch train time: 0:00:00.505208
elapsed time: 0:01:28.677954
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 23:15:11.363478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.07
 ---- batch: 020 ----
mean loss: 164.53
 ---- batch: 030 ----
mean loss: 166.15
 ---- batch: 040 ----
mean loss: 162.47
 ---- batch: 050 ----
mean loss: 163.20
 ---- batch: 060 ----
mean loss: 165.48
 ---- batch: 070 ----
mean loss: 163.40
 ---- batch: 080 ----
mean loss: 165.24
 ---- batch: 090 ----
mean loss: 161.12
train mean loss: 164.29
epoch train time: 0:00:00.516514
elapsed time: 0:01:29.194615
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 23:15:11.880135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.05
 ---- batch: 020 ----
mean loss: 160.96
 ---- batch: 030 ----
mean loss: 160.26
 ---- batch: 040 ----
mean loss: 163.69
 ---- batch: 050 ----
mean loss: 161.81
 ---- batch: 060 ----
mean loss: 170.05
 ---- batch: 070 ----
mean loss: 164.29
 ---- batch: 080 ----
mean loss: 161.71
 ---- batch: 090 ----
mean loss: 166.79
train mean loss: 164.50
epoch train time: 0:00:00.496990
elapsed time: 0:01:29.691754
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 23:15:12.377276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.27
 ---- batch: 020 ----
mean loss: 160.46
 ---- batch: 030 ----
mean loss: 166.36
 ---- batch: 040 ----
mean loss: 163.98
 ---- batch: 050 ----
mean loss: 162.18
 ---- batch: 060 ----
mean loss: 161.89
 ---- batch: 070 ----
mean loss: 165.47
 ---- batch: 080 ----
mean loss: 160.69
 ---- batch: 090 ----
mean loss: 164.15
train mean loss: 161.81
epoch train time: 0:00:00.497072
elapsed time: 0:01:30.188976
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 23:15:12.874497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.51
 ---- batch: 020 ----
mean loss: 166.46
 ---- batch: 030 ----
mean loss: 162.69
 ---- batch: 040 ----
mean loss: 153.77
 ---- batch: 050 ----
mean loss: 158.32
 ---- batch: 060 ----
mean loss: 163.57
 ---- batch: 070 ----
mean loss: 169.30
 ---- batch: 080 ----
mean loss: 160.97
 ---- batch: 090 ----
mean loss: 166.03
train mean loss: 162.23
epoch train time: 0:00:00.505841
elapsed time: 0:01:30.695006
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 23:15:13.380545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.28
 ---- batch: 020 ----
mean loss: 157.49
 ---- batch: 030 ----
mean loss: 159.88
 ---- batch: 040 ----
mean loss: 161.85
 ---- batch: 050 ----
mean loss: 164.89
 ---- batch: 060 ----
mean loss: 162.98
 ---- batch: 070 ----
mean loss: 157.17
 ---- batch: 080 ----
mean loss: 162.37
 ---- batch: 090 ----
mean loss: 164.15
train mean loss: 161.88
epoch train time: 0:00:00.506566
elapsed time: 0:01:31.201734
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 23:15:13.887272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.54
 ---- batch: 020 ----
mean loss: 157.94
 ---- batch: 030 ----
mean loss: 157.68
 ---- batch: 040 ----
mean loss: 159.11
 ---- batch: 050 ----
mean loss: 164.19
 ---- batch: 060 ----
mean loss: 170.60
 ---- batch: 070 ----
mean loss: 165.86
 ---- batch: 080 ----
mean loss: 154.21
 ---- batch: 090 ----
mean loss: 168.04
train mean loss: 162.27
epoch train time: 0:00:00.497698
elapsed time: 0:01:31.699591
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 23:15:14.385111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.03
 ---- batch: 020 ----
mean loss: 161.64
 ---- batch: 030 ----
mean loss: 160.74
 ---- batch: 040 ----
mean loss: 150.30
 ---- batch: 050 ----
mean loss: 165.94
 ---- batch: 060 ----
mean loss: 157.91
 ---- batch: 070 ----
mean loss: 163.08
 ---- batch: 080 ----
mean loss: 155.39
 ---- batch: 090 ----
mean loss: 167.84
train mean loss: 160.60
epoch train time: 0:00:00.517755
elapsed time: 0:01:32.217504
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 23:15:14.903064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.24
 ---- batch: 020 ----
mean loss: 152.09
 ---- batch: 030 ----
mean loss: 158.64
 ---- batch: 040 ----
mean loss: 158.26
 ---- batch: 050 ----
mean loss: 160.07
 ---- batch: 060 ----
mean loss: 167.99
 ---- batch: 070 ----
mean loss: 160.80
 ---- batch: 080 ----
mean loss: 165.05
 ---- batch: 090 ----
mean loss: 168.25
train mean loss: 160.19
epoch train time: 0:00:00.497919
elapsed time: 0:01:32.715607
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 23:15:15.401128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.50
 ---- batch: 020 ----
mean loss: 150.53
 ---- batch: 030 ----
mean loss: 157.14
 ---- batch: 040 ----
mean loss: 160.03
 ---- batch: 050 ----
mean loss: 159.93
 ---- batch: 060 ----
mean loss: 157.12
 ---- batch: 070 ----
mean loss: 157.07
 ---- batch: 080 ----
mean loss: 162.14
 ---- batch: 090 ----
mean loss: 170.65
train mean loss: 159.73
epoch train time: 0:00:00.509378
elapsed time: 0:01:33.225136
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 23:15:15.910675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.89
 ---- batch: 020 ----
mean loss: 155.09
 ---- batch: 030 ----
mean loss: 161.50
 ---- batch: 040 ----
mean loss: 162.08
 ---- batch: 050 ----
mean loss: 161.09
 ---- batch: 060 ----
mean loss: 157.10
 ---- batch: 070 ----
mean loss: 158.49
 ---- batch: 080 ----
mean loss: 160.75
 ---- batch: 090 ----
mean loss: 160.58
train mean loss: 159.44
epoch train time: 0:00:00.506092
elapsed time: 0:01:33.731390
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 23:15:16.416910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.24
 ---- batch: 020 ----
mean loss: 156.78
 ---- batch: 030 ----
mean loss: 154.86
 ---- batch: 040 ----
mean loss: 159.40
 ---- batch: 050 ----
mean loss: 160.88
 ---- batch: 060 ----
mean loss: 163.45
 ---- batch: 070 ----
mean loss: 161.36
 ---- batch: 080 ----
mean loss: 164.33
 ---- batch: 090 ----
mean loss: 159.81
train mean loss: 159.87
epoch train time: 0:00:00.511075
elapsed time: 0:01:34.242606
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 23:15:16.928127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.76
 ---- batch: 020 ----
mean loss: 156.84
 ---- batch: 030 ----
mean loss: 156.86
 ---- batch: 040 ----
mean loss: 157.68
 ---- batch: 050 ----
mean loss: 162.27
 ---- batch: 060 ----
mean loss: 159.75
 ---- batch: 070 ----
mean loss: 150.54
 ---- batch: 080 ----
mean loss: 162.84
 ---- batch: 090 ----
mean loss: 159.73
train mean loss: 157.74
epoch train time: 0:00:00.490833
elapsed time: 0:01:34.733585
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 23:15:17.419105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.88
 ---- batch: 020 ----
mean loss: 155.27
 ---- batch: 030 ----
mean loss: 154.14
 ---- batch: 040 ----
mean loss: 158.43
 ---- batch: 050 ----
mean loss: 158.61
 ---- batch: 060 ----
mean loss: 153.97
 ---- batch: 070 ----
mean loss: 159.33
 ---- batch: 080 ----
mean loss: 166.32
 ---- batch: 090 ----
mean loss: 160.48
train mean loss: 158.26
epoch train time: 0:00:00.506391
elapsed time: 0:01:35.240157
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 23:15:17.925718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.38
 ---- batch: 020 ----
mean loss: 159.50
 ---- batch: 030 ----
mean loss: 154.20
 ---- batch: 040 ----
mean loss: 158.11
 ---- batch: 050 ----
mean loss: 157.87
 ---- batch: 060 ----
mean loss: 156.18
 ---- batch: 070 ----
mean loss: 157.19
 ---- batch: 080 ----
mean loss: 156.49
 ---- batch: 090 ----
mean loss: 160.55
train mean loss: 157.38
epoch train time: 0:00:00.503362
elapsed time: 0:01:35.743703
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 23:15:18.429238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.50
 ---- batch: 020 ----
mean loss: 152.13
 ---- batch: 030 ----
mean loss: 147.65
 ---- batch: 040 ----
mean loss: 158.18
 ---- batch: 050 ----
mean loss: 161.33
 ---- batch: 060 ----
mean loss: 155.79
 ---- batch: 070 ----
mean loss: 159.16
 ---- batch: 080 ----
mean loss: 161.60
 ---- batch: 090 ----
mean loss: 161.58
train mean loss: 156.44
epoch train time: 0:00:00.514902
elapsed time: 0:01:36.258800
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 23:15:18.944365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.06
 ---- batch: 020 ----
mean loss: 158.02
 ---- batch: 030 ----
mean loss: 152.03
 ---- batch: 040 ----
mean loss: 161.18
 ---- batch: 050 ----
mean loss: 155.88
 ---- batch: 060 ----
mean loss: 162.31
 ---- batch: 070 ----
mean loss: 155.72
 ---- batch: 080 ----
mean loss: 155.14
 ---- batch: 090 ----
mean loss: 156.83
train mean loss: 157.14
epoch train time: 0:00:00.506025
elapsed time: 0:01:36.765045
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 23:15:19.450566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.92
 ---- batch: 020 ----
mean loss: 158.13
 ---- batch: 030 ----
mean loss: 152.18
 ---- batch: 040 ----
mean loss: 155.02
 ---- batch: 050 ----
mean loss: 151.29
 ---- batch: 060 ----
mean loss: 157.32
 ---- batch: 070 ----
mean loss: 157.68
 ---- batch: 080 ----
mean loss: 160.28
 ---- batch: 090 ----
mean loss: 161.73
train mean loss: 156.06
epoch train time: 0:00:00.502563
elapsed time: 0:01:37.267754
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 23:15:19.953279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.15
 ---- batch: 020 ----
mean loss: 153.76
 ---- batch: 030 ----
mean loss: 150.65
 ---- batch: 040 ----
mean loss: 157.42
 ---- batch: 050 ----
mean loss: 152.62
 ---- batch: 060 ----
mean loss: 159.69
 ---- batch: 070 ----
mean loss: 160.29
 ---- batch: 080 ----
mean loss: 159.54
 ---- batch: 090 ----
mean loss: 163.89
train mean loss: 155.64
epoch train time: 0:00:00.502864
elapsed time: 0:01:37.770787
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 23:15:20.456297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.81
 ---- batch: 020 ----
mean loss: 154.85
 ---- batch: 030 ----
mean loss: 153.37
 ---- batch: 040 ----
mean loss: 160.35
 ---- batch: 050 ----
mean loss: 158.58
 ---- batch: 060 ----
mean loss: 157.85
 ---- batch: 070 ----
mean loss: 155.00
 ---- batch: 080 ----
mean loss: 156.48
 ---- batch: 090 ----
mean loss: 169.40
train mean loss: 157.54
epoch train time: 0:00:00.516884
elapsed time: 0:01:38.287816
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 23:15:20.973336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.30
 ---- batch: 020 ----
mean loss: 147.65
 ---- batch: 030 ----
mean loss: 152.13
 ---- batch: 040 ----
mean loss: 151.84
 ---- batch: 050 ----
mean loss: 153.02
 ---- batch: 060 ----
mean loss: 153.12
 ---- batch: 070 ----
mean loss: 162.57
 ---- batch: 080 ----
mean loss: 161.35
 ---- batch: 090 ----
mean loss: 161.92
train mean loss: 154.74
epoch train time: 0:00:00.503320
elapsed time: 0:01:38.791315
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 23:15:21.476838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.26
 ---- batch: 020 ----
mean loss: 155.34
 ---- batch: 030 ----
mean loss: 145.66
 ---- batch: 040 ----
mean loss: 155.73
 ---- batch: 050 ----
mean loss: 153.18
 ---- batch: 060 ----
mean loss: 147.92
 ---- batch: 070 ----
mean loss: 154.04
 ---- batch: 080 ----
mean loss: 162.32
 ---- batch: 090 ----
mean loss: 160.23
train mean loss: 154.56
epoch train time: 0:00:00.518932
elapsed time: 0:01:39.310439
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 23:15:21.995983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.04
 ---- batch: 020 ----
mean loss: 152.30
 ---- batch: 030 ----
mean loss: 154.74
 ---- batch: 040 ----
mean loss: 156.65
 ---- batch: 050 ----
mean loss: 154.71
 ---- batch: 060 ----
mean loss: 149.50
 ---- batch: 070 ----
mean loss: 156.88
 ---- batch: 080 ----
mean loss: 155.86
 ---- batch: 090 ----
mean loss: 159.30
train mean loss: 154.07
epoch train time: 0:00:00.496182
elapsed time: 0:01:39.806789
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 23:15:22.492326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.54
 ---- batch: 020 ----
mean loss: 149.10
 ---- batch: 030 ----
mean loss: 157.45
 ---- batch: 040 ----
mean loss: 151.76
 ---- batch: 050 ----
mean loss: 155.18
 ---- batch: 060 ----
mean loss: 151.46
 ---- batch: 070 ----
mean loss: 157.89
 ---- batch: 080 ----
mean loss: 162.39
 ---- batch: 090 ----
mean loss: 155.36
train mean loss: 154.91
epoch train time: 0:00:00.503729
elapsed time: 0:01:40.310679
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 23:15:22.996199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.10
 ---- batch: 020 ----
mean loss: 155.44
 ---- batch: 030 ----
mean loss: 153.29
 ---- batch: 040 ----
mean loss: 148.18
 ---- batch: 050 ----
mean loss: 143.19
 ---- batch: 060 ----
mean loss: 158.92
 ---- batch: 070 ----
mean loss: 150.23
 ---- batch: 080 ----
mean loss: 155.55
 ---- batch: 090 ----
mean loss: 154.68
train mean loss: 153.51
epoch train time: 0:00:00.498479
elapsed time: 0:01:40.809304
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 23:15:23.494825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.86
 ---- batch: 020 ----
mean loss: 142.29
 ---- batch: 030 ----
mean loss: 151.22
 ---- batch: 040 ----
mean loss: 152.84
 ---- batch: 050 ----
mean loss: 150.67
 ---- batch: 060 ----
mean loss: 147.63
 ---- batch: 070 ----
mean loss: 155.57
 ---- batch: 080 ----
mean loss: 162.55
 ---- batch: 090 ----
mean loss: 161.38
train mean loss: 153.49
epoch train time: 0:00:00.511979
elapsed time: 0:01:41.321484
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 23:15:24.007039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.72
 ---- batch: 020 ----
mean loss: 152.79
 ---- batch: 030 ----
mean loss: 148.85
 ---- batch: 040 ----
mean loss: 150.71
 ---- batch: 050 ----
mean loss: 149.77
 ---- batch: 060 ----
mean loss: 158.93
 ---- batch: 070 ----
mean loss: 160.71
 ---- batch: 080 ----
mean loss: 151.09
 ---- batch: 090 ----
mean loss: 150.60
train mean loss: 152.58
epoch train time: 0:00:00.498536
elapsed time: 0:01:41.820218
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 23:15:24.505742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.81
 ---- batch: 020 ----
mean loss: 146.68
 ---- batch: 030 ----
mean loss: 148.80
 ---- batch: 040 ----
mean loss: 150.96
 ---- batch: 050 ----
mean loss: 146.20
 ---- batch: 060 ----
mean loss: 151.60
 ---- batch: 070 ----
mean loss: 153.43
 ---- batch: 080 ----
mean loss: 160.08
 ---- batch: 090 ----
mean loss: 160.50
train mean loss: 153.06
epoch train time: 0:00:00.504304
elapsed time: 0:01:42.324673
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 23:15:25.010214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.60
 ---- batch: 020 ----
mean loss: 147.30
 ---- batch: 030 ----
mean loss: 145.51
 ---- batch: 040 ----
mean loss: 152.16
 ---- batch: 050 ----
mean loss: 155.95
 ---- batch: 060 ----
mean loss: 153.89
 ---- batch: 070 ----
mean loss: 157.48
 ---- batch: 080 ----
mean loss: 154.16
 ---- batch: 090 ----
mean loss: 157.40
train mean loss: 153.04
epoch train time: 0:00:00.498405
elapsed time: 0:01:42.823266
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 23:15:25.508801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.97
 ---- batch: 020 ----
mean loss: 151.01
 ---- batch: 030 ----
mean loss: 149.25
 ---- batch: 040 ----
mean loss: 156.03
 ---- batch: 050 ----
mean loss: 149.09
 ---- batch: 060 ----
mean loss: 158.46
 ---- batch: 070 ----
mean loss: 150.39
 ---- batch: 080 ----
mean loss: 154.66
 ---- batch: 090 ----
mean loss: 153.67
train mean loss: 152.33
epoch train time: 0:00:00.504357
elapsed time: 0:01:43.327787
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 23:15:26.013308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.18
 ---- batch: 020 ----
mean loss: 151.92
 ---- batch: 030 ----
mean loss: 152.42
 ---- batch: 040 ----
mean loss: 153.69
 ---- batch: 050 ----
mean loss: 157.91
 ---- batch: 060 ----
mean loss: 143.42
 ---- batch: 070 ----
mean loss: 146.38
 ---- batch: 080 ----
mean loss: 158.12
 ---- batch: 090 ----
mean loss: 160.28
train mean loss: 152.27
epoch train time: 0:00:00.499177
elapsed time: 0:01:43.827113
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 23:15:26.512656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.64
 ---- batch: 020 ----
mean loss: 145.01
 ---- batch: 030 ----
mean loss: 150.31
 ---- batch: 040 ----
mean loss: 152.50
 ---- batch: 050 ----
mean loss: 148.48
 ---- batch: 060 ----
mean loss: 153.12
 ---- batch: 070 ----
mean loss: 153.72
 ---- batch: 080 ----
mean loss: 160.02
 ---- batch: 090 ----
mean loss: 154.37
train mean loss: 151.58
epoch train time: 0:00:00.515127
elapsed time: 0:01:44.342424
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 23:15:27.027950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.93
 ---- batch: 020 ----
mean loss: 155.33
 ---- batch: 030 ----
mean loss: 148.78
 ---- batch: 040 ----
mean loss: 149.53
 ---- batch: 050 ----
mean loss: 151.13
 ---- batch: 060 ----
mean loss: 145.69
 ---- batch: 070 ----
mean loss: 153.93
 ---- batch: 080 ----
mean loss: 152.35
 ---- batch: 090 ----
mean loss: 156.05
train mean loss: 151.16
epoch train time: 0:00:00.499656
elapsed time: 0:01:44.842227
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 23:15:27.527746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.04
 ---- batch: 020 ----
mean loss: 148.33
 ---- batch: 030 ----
mean loss: 155.60
 ---- batch: 040 ----
mean loss: 137.44
 ---- batch: 050 ----
mean loss: 153.91
 ---- batch: 060 ----
mean loss: 147.74
 ---- batch: 070 ----
mean loss: 157.93
 ---- batch: 080 ----
mean loss: 149.78
 ---- batch: 090 ----
mean loss: 157.01
train mean loss: 150.83
epoch train time: 0:00:00.504720
elapsed time: 0:01:45.347105
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 23:15:28.032628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.32
 ---- batch: 020 ----
mean loss: 147.28
 ---- batch: 030 ----
mean loss: 145.24
 ---- batch: 040 ----
mean loss: 152.07
 ---- batch: 050 ----
mean loss: 154.98
 ---- batch: 060 ----
mean loss: 155.15
 ---- batch: 070 ----
mean loss: 148.44
 ---- batch: 080 ----
mean loss: 151.76
 ---- batch: 090 ----
mean loss: 148.75
train mean loss: 150.58
epoch train time: 0:00:00.504336
elapsed time: 0:01:45.851585
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 23:15:28.537103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.91
 ---- batch: 020 ----
mean loss: 150.26
 ---- batch: 030 ----
mean loss: 144.34
 ---- batch: 040 ----
mean loss: 155.95
 ---- batch: 050 ----
mean loss: 143.20
 ---- batch: 060 ----
mean loss: 148.34
 ---- batch: 070 ----
mean loss: 151.86
 ---- batch: 080 ----
mean loss: 149.31
 ---- batch: 090 ----
mean loss: 150.73
train mean loss: 150.08
epoch train time: 0:00:00.503201
elapsed time: 0:01:46.354946
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 23:15:29.040476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.22
 ---- batch: 020 ----
mean loss: 151.91
 ---- batch: 030 ----
mean loss: 153.60
 ---- batch: 040 ----
mean loss: 151.29
 ---- batch: 050 ----
mean loss: 145.44
 ---- batch: 060 ----
mean loss: 152.27
 ---- batch: 070 ----
mean loss: 149.59
 ---- batch: 080 ----
mean loss: 145.51
 ---- batch: 090 ----
mean loss: 152.49
train mean loss: 149.49
epoch train time: 0:00:00.504168
elapsed time: 0:01:46.859269
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 23:15:29.544789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.99
 ---- batch: 020 ----
mean loss: 152.21
 ---- batch: 030 ----
mean loss: 144.40
 ---- batch: 040 ----
mean loss: 151.66
 ---- batch: 050 ----
mean loss: 159.47
 ---- batch: 060 ----
mean loss: 149.32
 ---- batch: 070 ----
mean loss: 149.40
 ---- batch: 080 ----
mean loss: 154.02
 ---- batch: 090 ----
mean loss: 153.76
train mean loss: 150.47
epoch train time: 0:00:00.522541
elapsed time: 0:01:47.381991
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 23:15:30.067533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.90
 ---- batch: 020 ----
mean loss: 145.10
 ---- batch: 030 ----
mean loss: 151.42
 ---- batch: 040 ----
mean loss: 151.51
 ---- batch: 050 ----
mean loss: 151.47
 ---- batch: 060 ----
mean loss: 154.76
 ---- batch: 070 ----
mean loss: 145.68
 ---- batch: 080 ----
mean loss: 151.44
 ---- batch: 090 ----
mean loss: 148.38
train mean loss: 149.62
epoch train time: 0:00:00.512002
elapsed time: 0:01:47.894170
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 23:15:30.579725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.91
 ---- batch: 020 ----
mean loss: 149.60
 ---- batch: 030 ----
mean loss: 141.77
 ---- batch: 040 ----
mean loss: 150.15
 ---- batch: 050 ----
mean loss: 151.68
 ---- batch: 060 ----
mean loss: 154.13
 ---- batch: 070 ----
mean loss: 145.17
 ---- batch: 080 ----
mean loss: 157.36
 ---- batch: 090 ----
mean loss: 153.03
train mean loss: 149.02
epoch train time: 0:00:00.512656
elapsed time: 0:01:48.407019
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 23:15:31.092540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.17
 ---- batch: 020 ----
mean loss: 143.08
 ---- batch: 030 ----
mean loss: 143.00
 ---- batch: 040 ----
mean loss: 148.38
 ---- batch: 050 ----
mean loss: 143.08
 ---- batch: 060 ----
mean loss: 145.37
 ---- batch: 070 ----
mean loss: 150.55
 ---- batch: 080 ----
mean loss: 151.36
 ---- batch: 090 ----
mean loss: 154.40
train mean loss: 147.86
epoch train time: 0:00:00.500894
elapsed time: 0:01:48.908072
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 23:15:31.593632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.98
 ---- batch: 020 ----
mean loss: 144.24
 ---- batch: 030 ----
mean loss: 149.21
 ---- batch: 040 ----
mean loss: 149.68
 ---- batch: 050 ----
mean loss: 151.80
 ---- batch: 060 ----
mean loss: 152.02
 ---- batch: 070 ----
mean loss: 150.52
 ---- batch: 080 ----
mean loss: 148.40
 ---- batch: 090 ----
mean loss: 153.17
train mean loss: 149.26
epoch train time: 0:00:00.499480
elapsed time: 0:01:49.407751
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 23:15:32.093271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.73
 ---- batch: 020 ----
mean loss: 148.37
 ---- batch: 030 ----
mean loss: 143.92
 ---- batch: 040 ----
mean loss: 154.29
 ---- batch: 050 ----
mean loss: 145.82
 ---- batch: 060 ----
mean loss: 147.82
 ---- batch: 070 ----
mean loss: 143.17
 ---- batch: 080 ----
mean loss: 156.81
 ---- batch: 090 ----
mean loss: 151.36
train mean loss: 148.29
epoch train time: 0:00:00.497666
elapsed time: 0:01:49.905568
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 23:15:32.591089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.65
 ---- batch: 020 ----
mean loss: 146.00
 ---- batch: 030 ----
mean loss: 144.14
 ---- batch: 040 ----
mean loss: 145.75
 ---- batch: 050 ----
mean loss: 145.00
 ---- batch: 060 ----
mean loss: 149.17
 ---- batch: 070 ----
mean loss: 146.26
 ---- batch: 080 ----
mean loss: 149.12
 ---- batch: 090 ----
mean loss: 153.14
train mean loss: 147.46
epoch train time: 0:00:00.507794
elapsed time: 0:01:50.413519
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 23:15:33.099043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.76
 ---- batch: 020 ----
mean loss: 148.82
 ---- batch: 030 ----
mean loss: 152.54
 ---- batch: 040 ----
mean loss: 141.26
 ---- batch: 050 ----
mean loss: 146.20
 ---- batch: 060 ----
mean loss: 147.87
 ---- batch: 070 ----
mean loss: 154.11
 ---- batch: 080 ----
mean loss: 146.26
 ---- batch: 090 ----
mean loss: 151.20
train mean loss: 147.29
epoch train time: 0:00:00.503134
elapsed time: 0:01:50.916800
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 23:15:33.602335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.20
 ---- batch: 020 ----
mean loss: 145.50
 ---- batch: 030 ----
mean loss: 144.05
 ---- batch: 040 ----
mean loss: 149.49
 ---- batch: 050 ----
mean loss: 142.24
 ---- batch: 060 ----
mean loss: 142.39
 ---- batch: 070 ----
mean loss: 148.83
 ---- batch: 080 ----
mean loss: 148.51
 ---- batch: 090 ----
mean loss: 150.53
train mean loss: 146.81
epoch train time: 0:00:00.501033
elapsed time: 0:01:51.417991
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 23:15:34.103527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.01
 ---- batch: 020 ----
mean loss: 147.29
 ---- batch: 030 ----
mean loss: 141.11
 ---- batch: 040 ----
mean loss: 147.22
 ---- batch: 050 ----
mean loss: 153.50
 ---- batch: 060 ----
mean loss: 152.12
 ---- batch: 070 ----
mean loss: 150.58
 ---- batch: 080 ----
mean loss: 155.01
 ---- batch: 090 ----
mean loss: 150.13
train mean loss: 148.69
epoch train time: 0:00:00.513375
elapsed time: 0:01:51.931531
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 23:15:34.617052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.56
 ---- batch: 020 ----
mean loss: 144.33
 ---- batch: 030 ----
mean loss: 145.09
 ---- batch: 040 ----
mean loss: 144.30
 ---- batch: 050 ----
mean loss: 142.69
 ---- batch: 060 ----
mean loss: 153.21
 ---- batch: 070 ----
mean loss: 148.22
 ---- batch: 080 ----
mean loss: 148.99
 ---- batch: 090 ----
mean loss: 145.51
train mean loss: 146.61
epoch train time: 0:00:00.511516
elapsed time: 0:01:52.443191
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 23:15:35.128711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.46
 ---- batch: 020 ----
mean loss: 142.63
 ---- batch: 030 ----
mean loss: 151.27
 ---- batch: 040 ----
mean loss: 149.30
 ---- batch: 050 ----
mean loss: 139.35
 ---- batch: 060 ----
mean loss: 148.39
 ---- batch: 070 ----
mean loss: 147.82
 ---- batch: 080 ----
mean loss: 154.40
 ---- batch: 090 ----
mean loss: 150.52
train mean loss: 146.87
epoch train time: 0:00:00.500938
elapsed time: 0:01:52.944277
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 23:15:35.629800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.40
 ---- batch: 020 ----
mean loss: 151.11
 ---- batch: 030 ----
mean loss: 143.62
 ---- batch: 040 ----
mean loss: 145.43
 ---- batch: 050 ----
mean loss: 149.45
 ---- batch: 060 ----
mean loss: 144.89
 ---- batch: 070 ----
mean loss: 139.47
 ---- batch: 080 ----
mean loss: 145.67
 ---- batch: 090 ----
mean loss: 146.37
train mean loss: 146.28
epoch train time: 0:00:00.504402
elapsed time: 0:01:53.448842
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 23:15:36.134364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.50
 ---- batch: 020 ----
mean loss: 139.08
 ---- batch: 030 ----
mean loss: 147.19
 ---- batch: 040 ----
mean loss: 145.16
 ---- batch: 050 ----
mean loss: 142.17
 ---- batch: 060 ----
mean loss: 138.66
 ---- batch: 070 ----
mean loss: 147.19
 ---- batch: 080 ----
mean loss: 149.06
 ---- batch: 090 ----
mean loss: 148.47
train mean loss: 145.64
epoch train time: 0:00:00.502925
elapsed time: 0:01:53.951913
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 23:15:36.637442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.27
 ---- batch: 020 ----
mean loss: 141.21
 ---- batch: 030 ----
mean loss: 147.49
 ---- batch: 040 ----
mean loss: 147.91
 ---- batch: 050 ----
mean loss: 152.57
 ---- batch: 060 ----
mean loss: 147.85
 ---- batch: 070 ----
mean loss: 147.56
 ---- batch: 080 ----
mean loss: 147.97
 ---- batch: 090 ----
mean loss: 146.33
train mean loss: 146.38
epoch train time: 0:00:00.502888
elapsed time: 0:01:54.454966
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 23:15:37.140503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.30
 ---- batch: 020 ----
mean loss: 143.89
 ---- batch: 030 ----
mean loss: 141.10
 ---- batch: 040 ----
mean loss: 143.21
 ---- batch: 050 ----
mean loss: 142.36
 ---- batch: 060 ----
mean loss: 147.98
 ---- batch: 070 ----
mean loss: 150.94
 ---- batch: 080 ----
mean loss: 151.74
 ---- batch: 090 ----
mean loss: 141.67
train mean loss: 144.95
epoch train time: 0:00:00.499063
elapsed time: 0:01:54.954188
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 23:15:37.639726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.26
 ---- batch: 020 ----
mean loss: 139.31
 ---- batch: 030 ----
mean loss: 141.61
 ---- batch: 040 ----
mean loss: 142.99
 ---- batch: 050 ----
mean loss: 141.12
 ---- batch: 060 ----
mean loss: 148.61
 ---- batch: 070 ----
mean loss: 152.13
 ---- batch: 080 ----
mean loss: 145.81
 ---- batch: 090 ----
mean loss: 145.99
train mean loss: 144.99
epoch train time: 0:00:00.517951
elapsed time: 0:01:55.472349
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 23:15:38.157870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.37
 ---- batch: 020 ----
mean loss: 147.45
 ---- batch: 030 ----
mean loss: 149.82
 ---- batch: 040 ----
mean loss: 147.97
 ---- batch: 050 ----
mean loss: 139.79
 ---- batch: 060 ----
mean loss: 152.12
 ---- batch: 070 ----
mean loss: 150.50
 ---- batch: 080 ----
mean loss: 147.54
 ---- batch: 090 ----
mean loss: 139.82
train mean loss: 145.51
epoch train time: 0:00:00.511009
elapsed time: 0:01:55.983520
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 23:15:38.669062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.67
 ---- batch: 020 ----
mean loss: 139.98
 ---- batch: 030 ----
mean loss: 142.76
 ---- batch: 040 ----
mean loss: 143.24
 ---- batch: 050 ----
mean loss: 144.99
 ---- batch: 060 ----
mean loss: 150.65
 ---- batch: 070 ----
mean loss: 147.44
 ---- batch: 080 ----
mean loss: 138.80
 ---- batch: 090 ----
mean loss: 152.52
train mean loss: 145.04
epoch train time: 0:00:00.505447
elapsed time: 0:01:56.489148
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 23:15:39.174672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.89
 ---- batch: 020 ----
mean loss: 138.39
 ---- batch: 030 ----
mean loss: 143.62
 ---- batch: 040 ----
mean loss: 139.75
 ---- batch: 050 ----
mean loss: 148.70
 ---- batch: 060 ----
mean loss: 144.19
 ---- batch: 070 ----
mean loss: 148.17
 ---- batch: 080 ----
mean loss: 139.13
 ---- batch: 090 ----
mean loss: 150.01
train mean loss: 144.64
epoch train time: 0:00:00.511643
elapsed time: 0:01:57.000942
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 23:15:39.686463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.65
 ---- batch: 020 ----
mean loss: 143.05
 ---- batch: 030 ----
mean loss: 139.20
 ---- batch: 040 ----
mean loss: 149.63
 ---- batch: 050 ----
mean loss: 142.28
 ---- batch: 060 ----
mean loss: 148.04
 ---- batch: 070 ----
mean loss: 145.09
 ---- batch: 080 ----
mean loss: 142.54
 ---- batch: 090 ----
mean loss: 141.75
train mean loss: 143.62
epoch train time: 0:00:00.504664
elapsed time: 0:01:57.505752
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 23:15:40.191275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.68
 ---- batch: 020 ----
mean loss: 140.36
 ---- batch: 030 ----
mean loss: 144.33
 ---- batch: 040 ----
mean loss: 139.40
 ---- batch: 050 ----
mean loss: 144.83
 ---- batch: 060 ----
mean loss: 140.62
 ---- batch: 070 ----
mean loss: 137.00
 ---- batch: 080 ----
mean loss: 147.59
 ---- batch: 090 ----
mean loss: 151.98
train mean loss: 144.23
epoch train time: 0:00:00.497393
elapsed time: 0:01:58.003292
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 23:15:40.688814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.99
 ---- batch: 020 ----
mean loss: 137.97
 ---- batch: 030 ----
mean loss: 150.84
 ---- batch: 040 ----
mean loss: 139.65
 ---- batch: 050 ----
mean loss: 145.28
 ---- batch: 060 ----
mean loss: 139.44
 ---- batch: 070 ----
mean loss: 144.30
 ---- batch: 080 ----
mean loss: 143.39
 ---- batch: 090 ----
mean loss: 145.74
train mean loss: 143.67
epoch train time: 0:00:00.498845
elapsed time: 0:01:58.502285
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 23:15:41.187806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.86
 ---- batch: 020 ----
mean loss: 142.65
 ---- batch: 030 ----
mean loss: 138.18
 ---- batch: 040 ----
mean loss: 142.44
 ---- batch: 050 ----
mean loss: 147.02
 ---- batch: 060 ----
mean loss: 151.03
 ---- batch: 070 ----
mean loss: 144.58
 ---- batch: 080 ----
mean loss: 144.64
 ---- batch: 090 ----
mean loss: 144.69
train mean loss: 144.15
epoch train time: 0:00:00.502607
elapsed time: 0:01:59.005040
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 23:15:41.690578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.20
 ---- batch: 020 ----
mean loss: 137.50
 ---- batch: 030 ----
mean loss: 140.50
 ---- batch: 040 ----
mean loss: 147.49
 ---- batch: 050 ----
mean loss: 139.69
 ---- batch: 060 ----
mean loss: 148.43
 ---- batch: 070 ----
mean loss: 144.91
 ---- batch: 080 ----
mean loss: 143.84
 ---- batch: 090 ----
mean loss: 140.94
train mean loss: 142.77
epoch train time: 0:00:00.512377
elapsed time: 0:01:59.517581
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 23:15:42.203103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.70
 ---- batch: 020 ----
mean loss: 142.47
 ---- batch: 030 ----
mean loss: 140.01
 ---- batch: 040 ----
mean loss: 143.80
 ---- batch: 050 ----
mean loss: 148.62
 ---- batch: 060 ----
mean loss: 139.62
 ---- batch: 070 ----
mean loss: 150.29
 ---- batch: 080 ----
mean loss: 140.04
 ---- batch: 090 ----
mean loss: 142.27
train mean loss: 143.00
epoch train time: 0:00:00.511749
elapsed time: 0:02:00.029481
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 23:15:42.715003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.65
 ---- batch: 020 ----
mean loss: 140.11
 ---- batch: 030 ----
mean loss: 141.12
 ---- batch: 040 ----
mean loss: 141.84
 ---- batch: 050 ----
mean loss: 142.06
 ---- batch: 060 ----
mean loss: 148.04
 ---- batch: 070 ----
mean loss: 144.97
 ---- batch: 080 ----
mean loss: 141.41
 ---- batch: 090 ----
mean loss: 145.87
train mean loss: 142.94
epoch train time: 0:00:00.522999
elapsed time: 0:02:00.552636
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 23:15:43.238161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.73
 ---- batch: 020 ----
mean loss: 139.32
 ---- batch: 030 ----
mean loss: 138.62
 ---- batch: 040 ----
mean loss: 144.50
 ---- batch: 050 ----
mean loss: 139.81
 ---- batch: 060 ----
mean loss: 141.65
 ---- batch: 070 ----
mean loss: 144.76
 ---- batch: 080 ----
mean loss: 144.55
 ---- batch: 090 ----
mean loss: 146.62
train mean loss: 142.58
epoch train time: 0:00:00.509070
elapsed time: 0:02:01.061856
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 23:15:43.747381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.71
 ---- batch: 020 ----
mean loss: 138.61
 ---- batch: 030 ----
mean loss: 146.22
 ---- batch: 040 ----
mean loss: 136.50
 ---- batch: 050 ----
mean loss: 140.83
 ---- batch: 060 ----
mean loss: 147.16
 ---- batch: 070 ----
mean loss: 142.03
 ---- batch: 080 ----
mean loss: 141.75
 ---- batch: 090 ----
mean loss: 145.02
train mean loss: 142.02
epoch train time: 0:00:00.515257
elapsed time: 0:02:01.577277
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 23:15:44.262786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.98
 ---- batch: 020 ----
mean loss: 142.67
 ---- batch: 030 ----
mean loss: 144.29
 ---- batch: 040 ----
mean loss: 139.37
 ---- batch: 050 ----
mean loss: 143.39
 ---- batch: 060 ----
mean loss: 149.39
 ---- batch: 070 ----
mean loss: 142.32
 ---- batch: 080 ----
mean loss: 139.47
 ---- batch: 090 ----
mean loss: 142.22
train mean loss: 142.67
epoch train time: 0:00:00.509958
elapsed time: 0:02:02.087380
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 23:15:44.772912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.03
 ---- batch: 020 ----
mean loss: 135.18
 ---- batch: 030 ----
mean loss: 143.67
 ---- batch: 040 ----
mean loss: 142.36
 ---- batch: 050 ----
mean loss: 138.99
 ---- batch: 060 ----
mean loss: 146.72
 ---- batch: 070 ----
mean loss: 150.06
 ---- batch: 080 ----
mean loss: 141.68
 ---- batch: 090 ----
mean loss: 140.09
train mean loss: 142.30
epoch train time: 0:00:00.513362
elapsed time: 0:02:02.600901
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 23:15:45.286445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.74
 ---- batch: 020 ----
mean loss: 137.43
 ---- batch: 030 ----
mean loss: 136.62
 ---- batch: 040 ----
mean loss: 146.54
 ---- batch: 050 ----
mean loss: 148.16
 ---- batch: 060 ----
mean loss: 143.51
 ---- batch: 070 ----
mean loss: 140.51
 ---- batch: 080 ----
mean loss: 139.11
 ---- batch: 090 ----
mean loss: 147.97
train mean loss: 142.15
epoch train time: 0:00:00.501139
elapsed time: 0:02:03.102206
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 23:15:45.787726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.29
 ---- batch: 020 ----
mean loss: 134.73
 ---- batch: 030 ----
mean loss: 147.04
 ---- batch: 040 ----
mean loss: 135.90
 ---- batch: 050 ----
mean loss: 138.07
 ---- batch: 060 ----
mean loss: 142.81
 ---- batch: 070 ----
mean loss: 149.80
 ---- batch: 080 ----
mean loss: 138.27
 ---- batch: 090 ----
mean loss: 137.01
train mean loss: 140.62
epoch train time: 0:00:00.494667
elapsed time: 0:02:03.597038
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 23:15:46.282603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.51
 ---- batch: 020 ----
mean loss: 142.93
 ---- batch: 030 ----
mean loss: 140.71
 ---- batch: 040 ----
mean loss: 140.66
 ---- batch: 050 ----
mean loss: 138.57
 ---- batch: 060 ----
mean loss: 147.77
 ---- batch: 070 ----
mean loss: 137.76
 ---- batch: 080 ----
mean loss: 140.09
 ---- batch: 090 ----
mean loss: 139.94
train mean loss: 140.78
epoch train time: 0:00:00.497694
elapsed time: 0:02:04.094920
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 23:15:46.780450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.94
 ---- batch: 020 ----
mean loss: 135.84
 ---- batch: 030 ----
mean loss: 139.69
 ---- batch: 040 ----
mean loss: 140.10
 ---- batch: 050 ----
mean loss: 142.93
 ---- batch: 060 ----
mean loss: 140.90
 ---- batch: 070 ----
mean loss: 139.59
 ---- batch: 080 ----
mean loss: 143.00
 ---- batch: 090 ----
mean loss: 149.35
train mean loss: 140.95
epoch train time: 0:00:00.519781
elapsed time: 0:02:04.614856
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 23:15:47.300376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.11
 ---- batch: 020 ----
mean loss: 138.09
 ---- batch: 030 ----
mean loss: 137.18
 ---- batch: 040 ----
mean loss: 140.31
 ---- batch: 050 ----
mean loss: 138.88
 ---- batch: 060 ----
mean loss: 136.60
 ---- batch: 070 ----
mean loss: 145.83
 ---- batch: 080 ----
mean loss: 136.67
 ---- batch: 090 ----
mean loss: 146.12
train mean loss: 140.25
epoch train time: 0:00:00.512584
elapsed time: 0:02:05.127589
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 23:15:47.813118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.07
 ---- batch: 020 ----
mean loss: 139.53
 ---- batch: 030 ----
mean loss: 140.35
 ---- batch: 040 ----
mean loss: 140.20
 ---- batch: 050 ----
mean loss: 140.03
 ---- batch: 060 ----
mean loss: 148.16
 ---- batch: 070 ----
mean loss: 143.05
 ---- batch: 080 ----
mean loss: 140.76
 ---- batch: 090 ----
mean loss: 140.76
train mean loss: 141.34
epoch train time: 0:00:00.506031
elapsed time: 0:02:05.633773
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 23:15:48.319294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.24
 ---- batch: 020 ----
mean loss: 141.08
 ---- batch: 030 ----
mean loss: 141.74
 ---- batch: 040 ----
mean loss: 141.86
 ---- batch: 050 ----
mean loss: 135.89
 ---- batch: 060 ----
mean loss: 139.96
 ---- batch: 070 ----
mean loss: 137.85
 ---- batch: 080 ----
mean loss: 137.23
 ---- batch: 090 ----
mean loss: 133.88
train mean loss: 139.41
epoch train time: 0:00:00.503641
elapsed time: 0:02:06.137574
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 23:15:48.823097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.57
 ---- batch: 020 ----
mean loss: 138.69
 ---- batch: 030 ----
mean loss: 133.81
 ---- batch: 040 ----
mean loss: 138.74
 ---- batch: 050 ----
mean loss: 142.30
 ---- batch: 060 ----
mean loss: 143.91
 ---- batch: 070 ----
mean loss: 138.65
 ---- batch: 080 ----
mean loss: 136.52
 ---- batch: 090 ----
mean loss: 138.35
train mean loss: 139.39
epoch train time: 0:00:00.514315
elapsed time: 0:02:06.652037
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 23:15:49.337580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.19
 ---- batch: 020 ----
mean loss: 139.78
 ---- batch: 030 ----
mean loss: 132.36
 ---- batch: 040 ----
mean loss: 143.20
 ---- batch: 050 ----
mean loss: 145.43
 ---- batch: 060 ----
mean loss: 142.91
 ---- batch: 070 ----
mean loss: 137.49
 ---- batch: 080 ----
mean loss: 137.13
 ---- batch: 090 ----
mean loss: 141.41
train mean loss: 139.67
epoch train time: 0:00:00.518209
elapsed time: 0:02:07.170424
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 23:15:49.855945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.99
 ---- batch: 020 ----
mean loss: 133.59
 ---- batch: 030 ----
mean loss: 133.81
 ---- batch: 040 ----
mean loss: 142.08
 ---- batch: 050 ----
mean loss: 148.59
 ---- batch: 060 ----
mean loss: 141.14
 ---- batch: 070 ----
mean loss: 139.50
 ---- batch: 080 ----
mean loss: 140.10
 ---- batch: 090 ----
mean loss: 141.42
train mean loss: 139.59
epoch train time: 0:00:00.499798
elapsed time: 0:02:07.670610
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 23:15:50.356158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.14
 ---- batch: 020 ----
mean loss: 135.19
 ---- batch: 030 ----
mean loss: 140.20
 ---- batch: 040 ----
mean loss: 142.51
 ---- batch: 050 ----
mean loss: 142.42
 ---- batch: 060 ----
mean loss: 139.78
 ---- batch: 070 ----
mean loss: 145.19
 ---- batch: 080 ----
mean loss: 133.65
 ---- batch: 090 ----
mean loss: 147.23
train mean loss: 140.49
epoch train time: 0:00:00.499837
elapsed time: 0:02:08.170623
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 23:15:50.856145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.19
 ---- batch: 020 ----
mean loss: 139.91
 ---- batch: 030 ----
mean loss: 137.99
 ---- batch: 040 ----
mean loss: 141.00
 ---- batch: 050 ----
mean loss: 141.96
 ---- batch: 060 ----
mean loss: 141.57
 ---- batch: 070 ----
mean loss: 139.63
 ---- batch: 080 ----
mean loss: 145.37
 ---- batch: 090 ----
mean loss: 139.27
train mean loss: 140.00
epoch train time: 0:00:00.504396
elapsed time: 0:02:08.675162
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 23:15:51.360682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.10
 ---- batch: 020 ----
mean loss: 136.06
 ---- batch: 030 ----
mean loss: 135.24
 ---- batch: 040 ----
mean loss: 136.48
 ---- batch: 050 ----
mean loss: 138.58
 ---- batch: 060 ----
mean loss: 142.13
 ---- batch: 070 ----
mean loss: 141.47
 ---- batch: 080 ----
mean loss: 149.53
 ---- batch: 090 ----
mean loss: 142.56
train mean loss: 139.74
epoch train time: 0:00:00.505690
elapsed time: 0:02:09.180997
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 23:15:51.866519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.10
 ---- batch: 020 ----
mean loss: 132.71
 ---- batch: 030 ----
mean loss: 140.44
 ---- batch: 040 ----
mean loss: 138.91
 ---- batch: 050 ----
mean loss: 136.37
 ---- batch: 060 ----
mean loss: 134.35
 ---- batch: 070 ----
mean loss: 138.27
 ---- batch: 080 ----
mean loss: 143.47
 ---- batch: 090 ----
mean loss: 146.97
train mean loss: 138.97
epoch train time: 0:00:00.498295
elapsed time: 0:02:09.679433
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 23:15:52.364959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.61
 ---- batch: 020 ----
mean loss: 137.93
 ---- batch: 030 ----
mean loss: 141.07
 ---- batch: 040 ----
mean loss: 143.57
 ---- batch: 050 ----
mean loss: 137.38
 ---- batch: 060 ----
mean loss: 138.86
 ---- batch: 070 ----
mean loss: 139.01
 ---- batch: 080 ----
mean loss: 139.27
 ---- batch: 090 ----
mean loss: 133.73
train mean loss: 138.48
epoch train time: 0:00:00.501771
elapsed time: 0:02:10.181358
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 23:15:52.866897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.84
 ---- batch: 020 ----
mean loss: 133.45
 ---- batch: 030 ----
mean loss: 139.01
 ---- batch: 040 ----
mean loss: 136.04
 ---- batch: 050 ----
mean loss: 137.90
 ---- batch: 060 ----
mean loss: 141.07
 ---- batch: 070 ----
mean loss: 143.85
 ---- batch: 080 ----
mean loss: 143.02
 ---- batch: 090 ----
mean loss: 136.93
train mean loss: 138.04
epoch train time: 0:00:00.499921
elapsed time: 0:02:10.681488
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 23:15:53.367003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.25
 ---- batch: 020 ----
mean loss: 136.75
 ---- batch: 030 ----
mean loss: 143.62
 ---- batch: 040 ----
mean loss: 138.26
 ---- batch: 050 ----
mean loss: 138.37
 ---- batch: 060 ----
mean loss: 138.95
 ---- batch: 070 ----
mean loss: 135.52
 ---- batch: 080 ----
mean loss: 136.72
 ---- batch: 090 ----
mean loss: 139.64
train mean loss: 138.24
epoch train time: 0:00:00.500432
elapsed time: 0:02:11.182055
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 23:15:53.867575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.79
 ---- batch: 020 ----
mean loss: 131.11
 ---- batch: 030 ----
mean loss: 136.55
 ---- batch: 040 ----
mean loss: 135.23
 ---- batch: 050 ----
mean loss: 138.84
 ---- batch: 060 ----
mean loss: 129.76
 ---- batch: 070 ----
mean loss: 140.25
 ---- batch: 080 ----
mean loss: 139.60
 ---- batch: 090 ----
mean loss: 148.04
train mean loss: 136.74
epoch train time: 0:00:00.498013
elapsed time: 0:02:11.680263
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 23:15:54.365784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.49
 ---- batch: 020 ----
mean loss: 134.20
 ---- batch: 030 ----
mean loss: 133.73
 ---- batch: 040 ----
mean loss: 140.74
 ---- batch: 050 ----
mean loss: 137.94
 ---- batch: 060 ----
mean loss: 137.87
 ---- batch: 070 ----
mean loss: 135.94
 ---- batch: 080 ----
mean loss: 137.23
 ---- batch: 090 ----
mean loss: 139.32
train mean loss: 137.77
epoch train time: 0:00:00.501787
elapsed time: 0:02:12.182203
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 23:15:54.867726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.84
 ---- batch: 020 ----
mean loss: 135.10
 ---- batch: 030 ----
mean loss: 142.79
 ---- batch: 040 ----
mean loss: 137.13
 ---- batch: 050 ----
mean loss: 136.35
 ---- batch: 060 ----
mean loss: 139.22
 ---- batch: 070 ----
mean loss: 137.35
 ---- batch: 080 ----
mean loss: 140.04
 ---- batch: 090 ----
mean loss: 140.65
train mean loss: 138.11
epoch train time: 0:00:00.505358
elapsed time: 0:02:12.687709
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 23:15:55.373248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.01
 ---- batch: 020 ----
mean loss: 140.28
 ---- batch: 030 ----
mean loss: 137.94
 ---- batch: 040 ----
mean loss: 134.05
 ---- batch: 050 ----
mean loss: 130.10
 ---- batch: 060 ----
mean loss: 140.69
 ---- batch: 070 ----
mean loss: 140.23
 ---- batch: 080 ----
mean loss: 126.99
 ---- batch: 090 ----
mean loss: 145.45
train mean loss: 137.87
epoch train time: 0:00:00.503989
elapsed time: 0:02:13.191860
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 23:15:55.877382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.17
 ---- batch: 020 ----
mean loss: 139.05
 ---- batch: 030 ----
mean loss: 134.74
 ---- batch: 040 ----
mean loss: 134.90
 ---- batch: 050 ----
mean loss: 133.29
 ---- batch: 060 ----
mean loss: 140.60
 ---- batch: 070 ----
mean loss: 135.53
 ---- batch: 080 ----
mean loss: 134.06
 ---- batch: 090 ----
mean loss: 147.31
train mean loss: 137.03
epoch train time: 0:00:00.511232
elapsed time: 0:02:13.703260
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 23:15:56.388773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.57
 ---- batch: 020 ----
mean loss: 132.13
 ---- batch: 030 ----
mean loss: 134.21
 ---- batch: 040 ----
mean loss: 137.35
 ---- batch: 050 ----
mean loss: 140.72
 ---- batch: 060 ----
mean loss: 140.40
 ---- batch: 070 ----
mean loss: 138.24
 ---- batch: 080 ----
mean loss: 142.91
 ---- batch: 090 ----
mean loss: 137.30
train mean loss: 136.86
epoch train time: 0:00:00.508452
elapsed time: 0:02:14.211852
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 23:15:56.897394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.67
 ---- batch: 020 ----
mean loss: 134.86
 ---- batch: 030 ----
mean loss: 137.49
 ---- batch: 040 ----
mean loss: 136.70
 ---- batch: 050 ----
mean loss: 137.10
 ---- batch: 060 ----
mean loss: 141.41
 ---- batch: 070 ----
mean loss: 139.77
 ---- batch: 080 ----
mean loss: 142.04
 ---- batch: 090 ----
mean loss: 137.19
train mean loss: 137.89
epoch train time: 0:00:00.508780
elapsed time: 0:02:14.720825
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 23:15:57.406336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.87
 ---- batch: 020 ----
mean loss: 140.08
 ---- batch: 030 ----
mean loss: 131.25
 ---- batch: 040 ----
mean loss: 138.71
 ---- batch: 050 ----
mean loss: 134.44
 ---- batch: 060 ----
mean loss: 135.91
 ---- batch: 070 ----
mean loss: 141.83
 ---- batch: 080 ----
mean loss: 144.11
 ---- batch: 090 ----
mean loss: 140.53
train mean loss: 137.52
epoch train time: 0:00:00.503939
elapsed time: 0:02:15.224903
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 23:15:57.910426
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.34
 ---- batch: 020 ----
mean loss: 128.44
 ---- batch: 030 ----
mean loss: 123.64
 ---- batch: 040 ----
mean loss: 136.70
 ---- batch: 050 ----
mean loss: 127.84
 ---- batch: 060 ----
mean loss: 125.60
 ---- batch: 070 ----
mean loss: 133.29
 ---- batch: 080 ----
mean loss: 134.26
 ---- batch: 090 ----
mean loss: 129.00
train mean loss: 130.57
epoch train time: 0:00:00.501551
elapsed time: 0:02:15.726616
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 23:15:58.412123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.64
 ---- batch: 020 ----
mean loss: 125.29
 ---- batch: 030 ----
mean loss: 128.32
 ---- batch: 040 ----
mean loss: 124.53
 ---- batch: 050 ----
mean loss: 130.54
 ---- batch: 060 ----
mean loss: 133.80
 ---- batch: 070 ----
mean loss: 127.43
 ---- batch: 080 ----
mean loss: 130.82
 ---- batch: 090 ----
mean loss: 123.62
train mean loss: 129.15
epoch train time: 0:00:00.509195
elapsed time: 0:02:16.235955
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 23:15:58.921469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.85
 ---- batch: 020 ----
mean loss: 125.03
 ---- batch: 030 ----
mean loss: 125.05
 ---- batch: 040 ----
mean loss: 131.02
 ---- batch: 050 ----
mean loss: 132.05
 ---- batch: 060 ----
mean loss: 123.20
 ---- batch: 070 ----
mean loss: 130.61
 ---- batch: 080 ----
mean loss: 129.33
 ---- batch: 090 ----
mean loss: 132.16
train mean loss: 128.52
epoch train time: 0:00:00.500828
elapsed time: 0:02:16.736939
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 23:15:59.422461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.66
 ---- batch: 020 ----
mean loss: 126.49
 ---- batch: 030 ----
mean loss: 130.94
 ---- batch: 040 ----
mean loss: 124.57
 ---- batch: 050 ----
mean loss: 129.61
 ---- batch: 060 ----
mean loss: 130.81
 ---- batch: 070 ----
mean loss: 123.49
 ---- batch: 080 ----
mean loss: 134.11
 ---- batch: 090 ----
mean loss: 129.15
train mean loss: 128.43
epoch train time: 0:00:00.522549
elapsed time: 0:02:17.259651
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 23:15:59.945173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.97
 ---- batch: 020 ----
mean loss: 131.53
 ---- batch: 030 ----
mean loss: 130.21
 ---- batch: 040 ----
mean loss: 121.40
 ---- batch: 050 ----
mean loss: 131.04
 ---- batch: 060 ----
mean loss: 133.18
 ---- batch: 070 ----
mean loss: 123.18
 ---- batch: 080 ----
mean loss: 132.84
 ---- batch: 090 ----
mean loss: 127.22
train mean loss: 128.31
epoch train time: 0:00:00.494720
elapsed time: 0:02:17.754520
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 23:16:00.440048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.17
 ---- batch: 020 ----
mean loss: 125.78
 ---- batch: 030 ----
mean loss: 129.07
 ---- batch: 040 ----
mean loss: 131.05
 ---- batch: 050 ----
mean loss: 129.46
 ---- batch: 060 ----
mean loss: 127.97
 ---- batch: 070 ----
mean loss: 128.07
 ---- batch: 080 ----
mean loss: 129.19
 ---- batch: 090 ----
mean loss: 130.40
train mean loss: 128.50
epoch train time: 0:00:00.504702
elapsed time: 0:02:18.259376
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 23:16:00.944916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.42
 ---- batch: 020 ----
mean loss: 123.91
 ---- batch: 030 ----
mean loss: 126.98
 ---- batch: 040 ----
mean loss: 129.21
 ---- batch: 050 ----
mean loss: 129.86
 ---- batch: 060 ----
mean loss: 123.12
 ---- batch: 070 ----
mean loss: 126.34
 ---- batch: 080 ----
mean loss: 130.05
 ---- batch: 090 ----
mean loss: 130.87
train mean loss: 128.34
epoch train time: 0:00:00.502137
elapsed time: 0:02:18.761691
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 23:16:01.447226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.28
 ---- batch: 020 ----
mean loss: 130.05
 ---- batch: 030 ----
mean loss: 128.63
 ---- batch: 040 ----
mean loss: 131.26
 ---- batch: 050 ----
mean loss: 128.14
 ---- batch: 060 ----
mean loss: 126.91
 ---- batch: 070 ----
mean loss: 128.16
 ---- batch: 080 ----
mean loss: 130.07
 ---- batch: 090 ----
mean loss: 121.38
train mean loss: 128.01
epoch train time: 0:00:00.514017
elapsed time: 0:02:19.275869
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 23:16:01.961390
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.78
 ---- batch: 020 ----
mean loss: 128.58
 ---- batch: 030 ----
mean loss: 125.32
 ---- batch: 040 ----
mean loss: 124.64
 ---- batch: 050 ----
mean loss: 134.36
 ---- batch: 060 ----
mean loss: 130.99
 ---- batch: 070 ----
mean loss: 126.28
 ---- batch: 080 ----
mean loss: 127.03
 ---- batch: 090 ----
mean loss: 131.74
train mean loss: 127.99
epoch train time: 0:00:00.499234
elapsed time: 0:02:19.775247
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 23:16:02.460766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.81
 ---- batch: 020 ----
mean loss: 130.03
 ---- batch: 030 ----
mean loss: 134.02
 ---- batch: 040 ----
mean loss: 132.95
 ---- batch: 050 ----
mean loss: 124.55
 ---- batch: 060 ----
mean loss: 130.81
 ---- batch: 070 ----
mean loss: 124.94
 ---- batch: 080 ----
mean loss: 126.72
 ---- batch: 090 ----
mean loss: 125.83
train mean loss: 128.02
epoch train time: 0:00:00.512087
elapsed time: 0:02:20.287481
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 23:16:02.973004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.80
 ---- batch: 020 ----
mean loss: 118.67
 ---- batch: 030 ----
mean loss: 131.48
 ---- batch: 040 ----
mean loss: 129.42
 ---- batch: 050 ----
mean loss: 127.46
 ---- batch: 060 ----
mean loss: 128.17
 ---- batch: 070 ----
mean loss: 130.22
 ---- batch: 080 ----
mean loss: 127.74
 ---- batch: 090 ----
mean loss: 130.52
train mean loss: 127.93
epoch train time: 0:00:00.512265
elapsed time: 0:02:20.799895
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 23:16:03.485417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.37
 ---- batch: 020 ----
mean loss: 117.85
 ---- batch: 030 ----
mean loss: 126.95
 ---- batch: 040 ----
mean loss: 136.77
 ---- batch: 050 ----
mean loss: 128.45
 ---- batch: 060 ----
mean loss: 123.95
 ---- batch: 070 ----
mean loss: 128.02
 ---- batch: 080 ----
mean loss: 126.85
 ---- batch: 090 ----
mean loss: 128.79
train mean loss: 128.28
epoch train time: 0:00:00.520553
elapsed time: 0:02:21.320618
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 23:16:04.006143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.58
 ---- batch: 020 ----
mean loss: 125.06
 ---- batch: 030 ----
mean loss: 125.64
 ---- batch: 040 ----
mean loss: 132.22
 ---- batch: 050 ----
mean loss: 133.99
 ---- batch: 060 ----
mean loss: 127.10
 ---- batch: 070 ----
mean loss: 127.23
 ---- batch: 080 ----
mean loss: 124.71
 ---- batch: 090 ----
mean loss: 129.65
train mean loss: 127.99
epoch train time: 0:00:00.508416
elapsed time: 0:02:21.829201
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 23:16:04.514741
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.12
 ---- batch: 020 ----
mean loss: 118.38
 ---- batch: 030 ----
mean loss: 128.44
 ---- batch: 040 ----
mean loss: 126.97
 ---- batch: 050 ----
mean loss: 128.44
 ---- batch: 060 ----
mean loss: 129.93
 ---- batch: 070 ----
mean loss: 128.59
 ---- batch: 080 ----
mean loss: 133.50
 ---- batch: 090 ----
mean loss: 130.00
train mean loss: 127.85
epoch train time: 0:00:00.509149
elapsed time: 0:02:22.338527
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 23:16:05.024069
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.64
 ---- batch: 020 ----
mean loss: 124.37
 ---- batch: 030 ----
mean loss: 128.28
 ---- batch: 040 ----
mean loss: 126.48
 ---- batch: 050 ----
mean loss: 127.62
 ---- batch: 060 ----
mean loss: 132.26
 ---- batch: 070 ----
mean loss: 127.38
 ---- batch: 080 ----
mean loss: 129.12
 ---- batch: 090 ----
mean loss: 126.47
train mean loss: 127.87
epoch train time: 0:00:00.504928
elapsed time: 0:02:22.843622
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 23:16:05.529144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.10
 ---- batch: 020 ----
mean loss: 128.16
 ---- batch: 030 ----
mean loss: 130.47
 ---- batch: 040 ----
mean loss: 126.72
 ---- batch: 050 ----
mean loss: 127.45
 ---- batch: 060 ----
mean loss: 127.07
 ---- batch: 070 ----
mean loss: 128.72
 ---- batch: 080 ----
mean loss: 128.61
 ---- batch: 090 ----
mean loss: 128.44
train mean loss: 127.84
epoch train time: 0:00:00.511614
elapsed time: 0:02:23.355412
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 23:16:06.040943
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.41
 ---- batch: 020 ----
mean loss: 130.59
 ---- batch: 030 ----
mean loss: 128.66
 ---- batch: 040 ----
mean loss: 123.91
 ---- batch: 050 ----
mean loss: 129.34
 ---- batch: 060 ----
mean loss: 126.69
 ---- batch: 070 ----
mean loss: 126.75
 ---- batch: 080 ----
mean loss: 132.88
 ---- batch: 090 ----
mean loss: 128.81
train mean loss: 127.68
epoch train time: 0:00:00.504903
elapsed time: 0:02:23.860479
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 23:16:06.546001
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.92
 ---- batch: 020 ----
mean loss: 128.02
 ---- batch: 030 ----
mean loss: 127.05
 ---- batch: 040 ----
mean loss: 124.86
 ---- batch: 050 ----
mean loss: 126.02
 ---- batch: 060 ----
mean loss: 131.61
 ---- batch: 070 ----
mean loss: 127.10
 ---- batch: 080 ----
mean loss: 133.57
 ---- batch: 090 ----
mean loss: 131.99
train mean loss: 127.57
epoch train time: 0:00:00.509400
elapsed time: 0:02:24.370042
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 23:16:07.055578
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.99
 ---- batch: 020 ----
mean loss: 121.64
 ---- batch: 030 ----
mean loss: 128.07
 ---- batch: 040 ----
mean loss: 133.60
 ---- batch: 050 ----
mean loss: 129.10
 ---- batch: 060 ----
mean loss: 122.00
 ---- batch: 070 ----
mean loss: 126.86
 ---- batch: 080 ----
mean loss: 134.73
 ---- batch: 090 ----
mean loss: 126.02
train mean loss: 127.59
epoch train time: 0:00:00.502979
elapsed time: 0:02:24.873187
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 23:16:07.558700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.85
 ---- batch: 020 ----
mean loss: 126.54
 ---- batch: 030 ----
mean loss: 125.52
 ---- batch: 040 ----
mean loss: 126.92
 ---- batch: 050 ----
mean loss: 129.37
 ---- batch: 060 ----
mean loss: 127.86
 ---- batch: 070 ----
mean loss: 124.87
 ---- batch: 080 ----
mean loss: 134.76
 ---- batch: 090 ----
mean loss: 126.66
train mean loss: 127.66
epoch train time: 0:00:00.504116
elapsed time: 0:02:25.377447
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 23:16:08.062983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.07
 ---- batch: 020 ----
mean loss: 124.61
 ---- batch: 030 ----
mean loss: 125.32
 ---- batch: 040 ----
mean loss: 128.06
 ---- batch: 050 ----
mean loss: 130.79
 ---- batch: 060 ----
mean loss: 135.01
 ---- batch: 070 ----
mean loss: 127.95
 ---- batch: 080 ----
mean loss: 129.79
 ---- batch: 090 ----
mean loss: 121.72
train mean loss: 127.49
epoch train time: 0:00:00.507275
elapsed time: 0:02:25.884881
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 23:16:08.570418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.44
 ---- batch: 020 ----
mean loss: 125.16
 ---- batch: 030 ----
mean loss: 127.87
 ---- batch: 040 ----
mean loss: 122.79
 ---- batch: 050 ----
mean loss: 123.70
 ---- batch: 060 ----
mean loss: 135.19
 ---- batch: 070 ----
mean loss: 129.73
 ---- batch: 080 ----
mean loss: 133.87
 ---- batch: 090 ----
mean loss: 126.82
train mean loss: 127.53
epoch train time: 0:00:00.513388
elapsed time: 0:02:26.398445
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 23:16:09.083972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.42
 ---- batch: 020 ----
mean loss: 126.65
 ---- batch: 030 ----
mean loss: 126.17
 ---- batch: 040 ----
mean loss: 123.66
 ---- batch: 050 ----
mean loss: 128.44
 ---- batch: 060 ----
mean loss: 126.07
 ---- batch: 070 ----
mean loss: 127.09
 ---- batch: 080 ----
mean loss: 126.98
 ---- batch: 090 ----
mean loss: 135.10
train mean loss: 127.58
epoch train time: 0:00:00.500130
elapsed time: 0:02:26.898747
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 23:16:09.584267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.91
 ---- batch: 020 ----
mean loss: 120.20
 ---- batch: 030 ----
mean loss: 128.55
 ---- batch: 040 ----
mean loss: 124.13
 ---- batch: 050 ----
mean loss: 126.21
 ---- batch: 060 ----
mean loss: 128.12
 ---- batch: 070 ----
mean loss: 128.17
 ---- batch: 080 ----
mean loss: 134.89
 ---- batch: 090 ----
mean loss: 132.89
train mean loss: 127.63
epoch train time: 0:00:00.510951
elapsed time: 0:02:27.409844
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 23:16:10.095368
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.38
 ---- batch: 020 ----
mean loss: 134.11
 ---- batch: 030 ----
mean loss: 127.96
 ---- batch: 040 ----
mean loss: 126.52
 ---- batch: 050 ----
mean loss: 123.66
 ---- batch: 060 ----
mean loss: 129.28
 ---- batch: 070 ----
mean loss: 122.43
 ---- batch: 080 ----
mean loss: 125.15
 ---- batch: 090 ----
mean loss: 128.31
train mean loss: 127.54
epoch train time: 0:00:00.497101
elapsed time: 0:02:27.907130
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 23:16:10.592650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.36
 ---- batch: 020 ----
mean loss: 133.99
 ---- batch: 030 ----
mean loss: 128.95
 ---- batch: 040 ----
mean loss: 122.66
 ---- batch: 050 ----
mean loss: 126.45
 ---- batch: 060 ----
mean loss: 125.70
 ---- batch: 070 ----
mean loss: 122.48
 ---- batch: 080 ----
mean loss: 129.75
 ---- batch: 090 ----
mean loss: 133.29
train mean loss: 127.69
epoch train time: 0:00:00.497882
elapsed time: 0:02:28.405164
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 23:16:11.090685
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.99
 ---- batch: 020 ----
mean loss: 137.77
 ---- batch: 030 ----
mean loss: 125.98
 ---- batch: 040 ----
mean loss: 134.66
 ---- batch: 050 ----
mean loss: 130.60
 ---- batch: 060 ----
mean loss: 125.98
 ---- batch: 070 ----
mean loss: 123.90
 ---- batch: 080 ----
mean loss: 128.13
 ---- batch: 090 ----
mean loss: 122.00
train mean loss: 127.62
epoch train time: 0:00:00.499505
elapsed time: 0:02:28.904815
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 23:16:11.590336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.08
 ---- batch: 020 ----
mean loss: 123.64
 ---- batch: 030 ----
mean loss: 130.19
 ---- batch: 040 ----
mean loss: 127.03
 ---- batch: 050 ----
mean loss: 128.08
 ---- batch: 060 ----
mean loss: 127.55
 ---- batch: 070 ----
mean loss: 130.46
 ---- batch: 080 ----
mean loss: 126.91
 ---- batch: 090 ----
mean loss: 128.03
train mean loss: 127.19
epoch train time: 0:00:00.536845
elapsed time: 0:02:29.441823
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 23:16:12.127393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.20
 ---- batch: 020 ----
mean loss: 130.02
 ---- batch: 030 ----
mean loss: 126.32
 ---- batch: 040 ----
mean loss: 126.73
 ---- batch: 050 ----
mean loss: 130.44
 ---- batch: 060 ----
mean loss: 127.57
 ---- batch: 070 ----
mean loss: 124.78
 ---- batch: 080 ----
mean loss: 121.90
 ---- batch: 090 ----
mean loss: 132.02
train mean loss: 127.24
epoch train time: 0:00:00.514103
elapsed time: 0:02:29.956119
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 23:16:12.641662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.19
 ---- batch: 020 ----
mean loss: 129.13
 ---- batch: 030 ----
mean loss: 129.62
 ---- batch: 040 ----
mean loss: 123.44
 ---- batch: 050 ----
mean loss: 127.05
 ---- batch: 060 ----
mean loss: 126.70
 ---- batch: 070 ----
mean loss: 128.35
 ---- batch: 080 ----
mean loss: 126.60
 ---- batch: 090 ----
mean loss: 127.77
train mean loss: 127.46
epoch train time: 0:00:00.527180
elapsed time: 0:02:30.483515
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 23:16:13.169037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.05
 ---- batch: 020 ----
mean loss: 128.07
 ---- batch: 030 ----
mean loss: 130.68
 ---- batch: 040 ----
mean loss: 126.87
 ---- batch: 050 ----
mean loss: 121.88
 ---- batch: 060 ----
mean loss: 121.03
 ---- batch: 070 ----
mean loss: 126.69
 ---- batch: 080 ----
mean loss: 131.89
 ---- batch: 090 ----
mean loss: 129.56
train mean loss: 127.26
epoch train time: 0:00:00.515648
elapsed time: 0:02:30.999341
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 23:16:13.684865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.35
 ---- batch: 020 ----
mean loss: 124.56
 ---- batch: 030 ----
mean loss: 124.45
 ---- batch: 040 ----
mean loss: 126.87
 ---- batch: 050 ----
mean loss: 127.82
 ---- batch: 060 ----
mean loss: 125.90
 ---- batch: 070 ----
mean loss: 130.27
 ---- batch: 080 ----
mean loss: 127.42
 ---- batch: 090 ----
mean loss: 126.26
train mean loss: 127.15
epoch train time: 0:00:00.538186
elapsed time: 0:02:31.537696
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 23:16:14.223221
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.81
 ---- batch: 020 ----
mean loss: 125.68
 ---- batch: 030 ----
mean loss: 124.94
 ---- batch: 040 ----
mean loss: 124.92
 ---- batch: 050 ----
mean loss: 131.28
 ---- batch: 060 ----
mean loss: 125.41
 ---- batch: 070 ----
mean loss: 123.75
 ---- batch: 080 ----
mean loss: 130.75
 ---- batch: 090 ----
mean loss: 126.44
train mean loss: 127.30
epoch train time: 0:00:00.525118
elapsed time: 0:02:32.063011
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 23:16:14.748530
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.59
 ---- batch: 020 ----
mean loss: 130.57
 ---- batch: 030 ----
mean loss: 122.01
 ---- batch: 040 ----
mean loss: 129.11
 ---- batch: 050 ----
mean loss: 129.88
 ---- batch: 060 ----
mean loss: 129.36
 ---- batch: 070 ----
mean loss: 130.11
 ---- batch: 080 ----
mean loss: 125.22
 ---- batch: 090 ----
mean loss: 127.32
train mean loss: 127.22
epoch train time: 0:00:00.532633
elapsed time: 0:02:32.595804
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 23:16:15.281327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.37
 ---- batch: 020 ----
mean loss: 124.33
 ---- batch: 030 ----
mean loss: 124.32
 ---- batch: 040 ----
mean loss: 121.11
 ---- batch: 050 ----
mean loss: 129.50
 ---- batch: 060 ----
mean loss: 130.91
 ---- batch: 070 ----
mean loss: 129.56
 ---- batch: 080 ----
mean loss: 125.01
 ---- batch: 090 ----
mean loss: 130.36
train mean loss: 127.18
epoch train time: 0:00:00.517934
elapsed time: 0:02:33.113955
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 23:16:15.799496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.49
 ---- batch: 020 ----
mean loss: 129.04
 ---- batch: 030 ----
mean loss: 129.04
 ---- batch: 040 ----
mean loss: 132.01
 ---- batch: 050 ----
mean loss: 123.94
 ---- batch: 060 ----
mean loss: 125.03
 ---- batch: 070 ----
mean loss: 124.00
 ---- batch: 080 ----
mean loss: 129.44
 ---- batch: 090 ----
mean loss: 124.63
train mean loss: 127.11
epoch train time: 0:00:00.520936
elapsed time: 0:02:33.635061
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 23:16:16.320636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.56
 ---- batch: 020 ----
mean loss: 121.83
 ---- batch: 030 ----
mean loss: 133.97
 ---- batch: 040 ----
mean loss: 127.28
 ---- batch: 050 ----
mean loss: 127.30
 ---- batch: 060 ----
mean loss: 126.08
 ---- batch: 070 ----
mean loss: 129.58
 ---- batch: 080 ----
mean loss: 131.96
 ---- batch: 090 ----
mean loss: 128.29
train mean loss: 127.27
epoch train time: 0:00:00.529325
elapsed time: 0:02:34.164591
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 23:16:16.850114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.19
 ---- batch: 020 ----
mean loss: 124.98
 ---- batch: 030 ----
mean loss: 128.03
 ---- batch: 040 ----
mean loss: 125.81
 ---- batch: 050 ----
mean loss: 131.38
 ---- batch: 060 ----
mean loss: 125.23
 ---- batch: 070 ----
mean loss: 129.35
 ---- batch: 080 ----
mean loss: 126.37
 ---- batch: 090 ----
mean loss: 126.65
train mean loss: 127.16
epoch train time: 0:00:00.520572
elapsed time: 0:02:34.685315
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 23:16:17.370837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.22
 ---- batch: 020 ----
mean loss: 125.08
 ---- batch: 030 ----
mean loss: 126.94
 ---- batch: 040 ----
mean loss: 122.88
 ---- batch: 050 ----
mean loss: 130.94
 ---- batch: 060 ----
mean loss: 125.35
 ---- batch: 070 ----
mean loss: 122.11
 ---- batch: 080 ----
mean loss: 130.14
 ---- batch: 090 ----
mean loss: 132.29
train mean loss: 127.21
epoch train time: 0:00:00.525967
elapsed time: 0:02:35.211442
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 23:16:17.896967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.39
 ---- batch: 020 ----
mean loss: 132.88
 ---- batch: 030 ----
mean loss: 123.13
 ---- batch: 040 ----
mean loss: 120.33
 ---- batch: 050 ----
mean loss: 128.47
 ---- batch: 060 ----
mean loss: 125.13
 ---- batch: 070 ----
mean loss: 126.35
 ---- batch: 080 ----
mean loss: 127.68
 ---- batch: 090 ----
mean loss: 122.61
train mean loss: 127.13
epoch train time: 0:00:00.528028
elapsed time: 0:02:35.739621
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 23:16:18.425143
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.10
 ---- batch: 020 ----
mean loss: 124.43
 ---- batch: 030 ----
mean loss: 130.12
 ---- batch: 040 ----
mean loss: 125.97
 ---- batch: 050 ----
mean loss: 123.96
 ---- batch: 060 ----
mean loss: 122.91
 ---- batch: 070 ----
mean loss: 131.21
 ---- batch: 080 ----
mean loss: 124.17
 ---- batch: 090 ----
mean loss: 127.24
train mean loss: 127.26
epoch train time: 0:00:00.528803
elapsed time: 0:02:36.268606
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 23:16:18.954139
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.26
 ---- batch: 020 ----
mean loss: 123.38
 ---- batch: 030 ----
mean loss: 126.77
 ---- batch: 040 ----
mean loss: 128.58
 ---- batch: 050 ----
mean loss: 118.52
 ---- batch: 060 ----
mean loss: 129.54
 ---- batch: 070 ----
mean loss: 125.73
 ---- batch: 080 ----
mean loss: 126.30
 ---- batch: 090 ----
mean loss: 130.06
train mean loss: 126.95
epoch train time: 0:00:00.527291
elapsed time: 0:02:36.796058
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 23:16:19.481583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.38
 ---- batch: 020 ----
mean loss: 128.84
 ---- batch: 030 ----
mean loss: 124.37
 ---- batch: 040 ----
mean loss: 122.89
 ---- batch: 050 ----
mean loss: 128.12
 ---- batch: 060 ----
mean loss: 132.52
 ---- batch: 070 ----
mean loss: 127.81
 ---- batch: 080 ----
mean loss: 123.08
 ---- batch: 090 ----
mean loss: 131.24
train mean loss: 127.08
epoch train time: 0:00:00.520826
elapsed time: 0:02:37.317054
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 23:16:20.002638
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.23
 ---- batch: 020 ----
mean loss: 132.26
 ---- batch: 030 ----
mean loss: 121.80
 ---- batch: 040 ----
mean loss: 125.25
 ---- batch: 050 ----
mean loss: 126.33
 ---- batch: 060 ----
mean loss: 125.16
 ---- batch: 070 ----
mean loss: 128.60
 ---- batch: 080 ----
mean loss: 123.18
 ---- batch: 090 ----
mean loss: 133.67
train mean loss: 126.89
epoch train time: 0:00:00.516631
elapsed time: 0:02:37.833894
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 23:16:20.519419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.20
 ---- batch: 020 ----
mean loss: 128.25
 ---- batch: 030 ----
mean loss: 125.58
 ---- batch: 040 ----
mean loss: 122.33
 ---- batch: 050 ----
mean loss: 129.43
 ---- batch: 060 ----
mean loss: 128.16
 ---- batch: 070 ----
mean loss: 126.81
 ---- batch: 080 ----
mean loss: 129.59
 ---- batch: 090 ----
mean loss: 132.47
train mean loss: 126.71
epoch train time: 0:00:00.515645
elapsed time: 0:02:38.349754
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 23:16:21.035279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.33
 ---- batch: 020 ----
mean loss: 127.47
 ---- batch: 030 ----
mean loss: 122.63
 ---- batch: 040 ----
mean loss: 125.69
 ---- batch: 050 ----
mean loss: 127.90
 ---- batch: 060 ----
mean loss: 127.19
 ---- batch: 070 ----
mean loss: 125.47
 ---- batch: 080 ----
mean loss: 126.48
 ---- batch: 090 ----
mean loss: 128.91
train mean loss: 126.89
epoch train time: 0:00:00.512243
elapsed time: 0:02:38.862148
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 23:16:21.547669
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.66
 ---- batch: 020 ----
mean loss: 127.91
 ---- batch: 030 ----
mean loss: 126.17
 ---- batch: 040 ----
mean loss: 130.41
 ---- batch: 050 ----
mean loss: 128.75
 ---- batch: 060 ----
mean loss: 128.91
 ---- batch: 070 ----
mean loss: 124.62
 ---- batch: 080 ----
mean loss: 125.51
 ---- batch: 090 ----
mean loss: 124.19
train mean loss: 126.93
epoch train time: 0:00:00.503846
elapsed time: 0:02:39.366135
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 23:16:22.051654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.27
 ---- batch: 020 ----
mean loss: 119.98
 ---- batch: 030 ----
mean loss: 122.73
 ---- batch: 040 ----
mean loss: 127.75
 ---- batch: 050 ----
mean loss: 130.91
 ---- batch: 060 ----
mean loss: 129.27
 ---- batch: 070 ----
mean loss: 135.14
 ---- batch: 080 ----
mean loss: 126.16
 ---- batch: 090 ----
mean loss: 124.50
train mean loss: 126.98
epoch train time: 0:00:00.500080
elapsed time: 0:02:39.866357
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 23:16:22.551877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.58
 ---- batch: 020 ----
mean loss: 131.05
 ---- batch: 030 ----
mean loss: 125.30
 ---- batch: 040 ----
mean loss: 117.69
 ---- batch: 050 ----
mean loss: 130.89
 ---- batch: 060 ----
mean loss: 125.24
 ---- batch: 070 ----
mean loss: 131.93
 ---- batch: 080 ----
mean loss: 127.31
 ---- batch: 090 ----
mean loss: 127.64
train mean loss: 126.52
epoch train time: 0:00:00.500675
elapsed time: 0:02:40.370648
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_dense3/frequentist_dense3_4/checkpoint.pth.tar
**** end time: 2019-09-25 23:16:23.056134 ****
