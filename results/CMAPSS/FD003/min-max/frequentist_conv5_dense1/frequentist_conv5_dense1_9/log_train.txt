Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_9', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7698
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 23:16:15.084621 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:16:15.092273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4037.66
 ---- batch: 020 ----
mean loss: 1536.78
 ---- batch: 030 ----
mean loss: 436.38
 ---- batch: 040 ----
mean loss: 450.22
train mean loss: 1530.37
epoch train time: 0:00:15.502018
elapsed time: 0:00:15.512276
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:16:30.596935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.90
 ---- batch: 020 ----
mean loss: 347.35
 ---- batch: 030 ----
mean loss: 320.58
 ---- batch: 040 ----
mean loss: 307.53
train mean loss: 330.40
epoch train time: 0:00:01.819760
elapsed time: 0:00:17.332175
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:16:32.416856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.05
 ---- batch: 020 ----
mean loss: 303.61
 ---- batch: 030 ----
mean loss: 293.80
 ---- batch: 040 ----
mean loss: 292.40
train mean loss: 297.04
epoch train time: 0:00:01.680172
elapsed time: 0:00:19.012510
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:16:34.097182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.82
 ---- batch: 020 ----
mean loss: 274.50
 ---- batch: 030 ----
mean loss: 262.66
 ---- batch: 040 ----
mean loss: 252.28
train mean loss: 265.21
epoch train time: 0:00:01.776191
elapsed time: 0:00:20.788837
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:16:35.873501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.65
 ---- batch: 020 ----
mean loss: 244.95
 ---- batch: 030 ----
mean loss: 234.24
 ---- batch: 040 ----
mean loss: 230.03
train mean loss: 238.93
epoch train time: 0:00:01.662963
elapsed time: 0:00:22.451972
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:16:37.536646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.41
 ---- batch: 020 ----
mean loss: 215.24
 ---- batch: 030 ----
mean loss: 227.67
 ---- batch: 040 ----
mean loss: 228.41
train mean loss: 225.75
epoch train time: 0:00:01.773944
elapsed time: 0:00:24.226050
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:16:39.310731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.48
 ---- batch: 020 ----
mean loss: 222.49
 ---- batch: 030 ----
mean loss: 215.76
 ---- batch: 040 ----
mean loss: 219.57
train mean loss: 217.90
epoch train time: 0:00:01.669817
elapsed time: 0:00:25.896017
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:16:40.980684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.23
 ---- batch: 020 ----
mean loss: 220.35
 ---- batch: 030 ----
mean loss: 208.86
 ---- batch: 040 ----
mean loss: 206.38
train mean loss: 213.53
epoch train time: 0:00:01.666009
elapsed time: 0:00:27.562160
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:16:42.646830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.05
 ---- batch: 020 ----
mean loss: 217.04
 ---- batch: 030 ----
mean loss: 205.42
 ---- batch: 040 ----
mean loss: 216.86
train mean loss: 212.34
epoch train time: 0:00:01.702343
elapsed time: 0:00:29.264646
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:16:44.349317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.53
 ---- batch: 020 ----
mean loss: 209.33
 ---- batch: 030 ----
mean loss: 205.97
 ---- batch: 040 ----
mean loss: 209.39
train mean loss: 208.95
epoch train time: 0:00:01.673739
elapsed time: 0:00:30.938550
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:16:46.023222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.64
 ---- batch: 020 ----
mean loss: 209.34
 ---- batch: 030 ----
mean loss: 203.43
 ---- batch: 040 ----
mean loss: 204.91
train mean loss: 207.07
epoch train time: 0:00:01.770790
elapsed time: 0:00:32.709476
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:16:47.794143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.88
 ---- batch: 020 ----
mean loss: 206.83
 ---- batch: 030 ----
mean loss: 205.88
 ---- batch: 040 ----
mean loss: 207.44
train mean loss: 206.22
epoch train time: 0:00:01.667057
elapsed time: 0:00:34.376669
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:16:49.461337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.77
 ---- batch: 020 ----
mean loss: 204.28
 ---- batch: 030 ----
mean loss: 194.74
 ---- batch: 040 ----
mean loss: 198.79
train mean loss: 201.11
epoch train time: 0:00:01.776403
elapsed time: 0:00:36.153251
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:16:51.237932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.75
 ---- batch: 020 ----
mean loss: 197.35
 ---- batch: 030 ----
mean loss: 195.46
 ---- batch: 040 ----
mean loss: 204.99
train mean loss: 202.75
epoch train time: 0:00:01.675293
elapsed time: 0:00:37.828700
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:16:52.913386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.95
 ---- batch: 020 ----
mean loss: 202.58
 ---- batch: 030 ----
mean loss: 202.46
 ---- batch: 040 ----
mean loss: 200.81
train mean loss: 201.17
epoch train time: 0:00:01.776692
elapsed time: 0:00:39.605549
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:16:54.690236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.18
 ---- batch: 020 ----
mean loss: 195.04
 ---- batch: 030 ----
mean loss: 200.30
 ---- batch: 040 ----
mean loss: 204.64
train mean loss: 200.37
epoch train time: 0:00:01.672879
elapsed time: 0:00:41.278586
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:16:56.363255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.59
 ---- batch: 020 ----
mean loss: 200.92
 ---- batch: 030 ----
mean loss: 198.63
 ---- batch: 040 ----
mean loss: 195.23
train mean loss: 198.53
epoch train time: 0:00:01.772846
elapsed time: 0:00:43.051592
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:16:58.136266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.79
 ---- batch: 020 ----
mean loss: 199.03
 ---- batch: 030 ----
mean loss: 208.70
 ---- batch: 040 ----
mean loss: 198.29
train mean loss: 198.93
epoch train time: 0:00:01.690166
elapsed time: 0:00:44.741916
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:16:59.826644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.78
 ---- batch: 020 ----
mean loss: 202.53
 ---- batch: 030 ----
mean loss: 205.15
 ---- batch: 040 ----
mean loss: 188.65
train mean loss: 198.39
epoch train time: 0:00:01.666381
elapsed time: 0:00:46.408483
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:17:01.493150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.79
 ---- batch: 020 ----
mean loss: 196.46
 ---- batch: 030 ----
mean loss: 195.14
 ---- batch: 040 ----
mean loss: 200.28
train mean loss: 196.41
epoch train time: 0:00:01.786253
elapsed time: 0:00:48.194876
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:17:03.279545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.99
 ---- batch: 020 ----
mean loss: 196.74
 ---- batch: 030 ----
mean loss: 198.89
 ---- batch: 040 ----
mean loss: 190.74
train mean loss: 195.30
epoch train time: 0:00:01.671211
elapsed time: 0:00:49.866235
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:17:04.950920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.41
 ---- batch: 020 ----
mean loss: 209.42
 ---- batch: 030 ----
mean loss: 185.41
 ---- batch: 040 ----
mean loss: 198.82
train mean loss: 196.31
epoch train time: 0:00:01.772967
elapsed time: 0:00:51.639368
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:17:06.724035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.16
 ---- batch: 020 ----
mean loss: 198.24
 ---- batch: 030 ----
mean loss: 194.18
 ---- batch: 040 ----
mean loss: 193.96
train mean loss: 195.93
epoch train time: 0:00:01.674152
elapsed time: 0:00:53.313672
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:17:08.398345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.24
 ---- batch: 020 ----
mean loss: 187.61
 ---- batch: 030 ----
mean loss: 194.63
 ---- batch: 040 ----
mean loss: 200.97
train mean loss: 192.79
epoch train time: 0:00:01.776465
elapsed time: 0:00:55.090273
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:17:10.174941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.13
 ---- batch: 020 ----
mean loss: 203.66
 ---- batch: 030 ----
mean loss: 196.88
 ---- batch: 040 ----
mean loss: 191.22
train mean loss: 195.51
epoch train time: 0:00:01.670029
elapsed time: 0:00:56.760454
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:17:11.845124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.50
 ---- batch: 020 ----
mean loss: 188.92
 ---- batch: 030 ----
mean loss: 185.76
 ---- batch: 040 ----
mean loss: 195.78
train mean loss: 190.97
epoch train time: 0:00:01.787201
elapsed time: 0:00:58.547807
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:17:13.632485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.84
 ---- batch: 020 ----
mean loss: 193.50
 ---- batch: 030 ----
mean loss: 197.77
 ---- batch: 040 ----
mean loss: 186.64
train mean loss: 192.22
epoch train time: 0:00:01.676177
elapsed time: 0:01:00.224135
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:17:15.308806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.22
 ---- batch: 020 ----
mean loss: 194.92
 ---- batch: 030 ----
mean loss: 194.78
 ---- batch: 040 ----
mean loss: 194.68
train mean loss: 192.91
epoch train time: 0:00:01.770952
elapsed time: 0:01:01.995226
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:17:17.079891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.26
 ---- batch: 020 ----
mean loss: 198.31
 ---- batch: 030 ----
mean loss: 196.17
 ---- batch: 040 ----
mean loss: 198.19
train mean loss: 197.14
epoch train time: 0:00:01.681151
elapsed time: 0:01:03.676519
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:17:18.761193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.78
 ---- batch: 020 ----
mean loss: 203.16
 ---- batch: 030 ----
mean loss: 192.24
 ---- batch: 040 ----
mean loss: 183.51
train mean loss: 193.46
epoch train time: 0:00:01.671784
elapsed time: 0:01:05.348439
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:17:20.433126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.07
 ---- batch: 020 ----
mean loss: 185.75
 ---- batch: 030 ----
mean loss: 188.39
 ---- batch: 040 ----
mean loss: 187.85
train mean loss: 188.76
epoch train time: 0:00:01.794283
elapsed time: 0:01:07.142889
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:17:22.227558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.12
 ---- batch: 020 ----
mean loss: 194.55
 ---- batch: 030 ----
mean loss: 186.00
 ---- batch: 040 ----
mean loss: 188.69
train mean loss: 192.06
epoch train time: 0:00:01.671274
elapsed time: 0:01:08.814308
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:17:23.899007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.46
 ---- batch: 020 ----
mean loss: 181.73
 ---- batch: 030 ----
mean loss: 185.53
 ---- batch: 040 ----
mean loss: 187.75
train mean loss: 187.83
epoch train time: 0:00:01.790044
elapsed time: 0:01:10.604518
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:17:25.689184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.53
 ---- batch: 020 ----
mean loss: 200.20
 ---- batch: 030 ----
mean loss: 184.65
 ---- batch: 040 ----
mean loss: 192.32
train mean loss: 192.06
epoch train time: 0:00:01.671645
elapsed time: 0:01:12.276349
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:17:27.361019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.34
 ---- batch: 020 ----
mean loss: 192.44
 ---- batch: 030 ----
mean loss: 192.46
 ---- batch: 040 ----
mean loss: 186.62
train mean loss: 188.82
epoch train time: 0:00:01.783281
elapsed time: 0:01:14.059826
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:17:29.144497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.67
 ---- batch: 020 ----
mean loss: 185.50
 ---- batch: 030 ----
mean loss: 180.71
 ---- batch: 040 ----
mean loss: 187.77
train mean loss: 185.99
epoch train time: 0:00:01.677098
elapsed time: 0:01:15.737071
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:17:30.821740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.29
 ---- batch: 020 ----
mean loss: 190.94
 ---- batch: 030 ----
mean loss: 186.93
 ---- batch: 040 ----
mean loss: 182.12
train mean loss: 186.13
epoch train time: 0:00:01.769370
elapsed time: 0:01:17.506587
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:17:32.591281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.57
 ---- batch: 020 ----
mean loss: 184.18
 ---- batch: 030 ----
mean loss: 182.43
 ---- batch: 040 ----
mean loss: 179.56
train mean loss: 185.05
epoch train time: 0:00:01.674351
elapsed time: 0:01:19.181096
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:17:34.265767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.43
 ---- batch: 020 ----
mean loss: 183.04
 ---- batch: 030 ----
mean loss: 188.97
 ---- batch: 040 ----
mean loss: 192.88
train mean loss: 187.63
epoch train time: 0:00:01.773589
elapsed time: 0:01:20.954816
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:17:36.039481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.20
 ---- batch: 020 ----
mean loss: 183.37
 ---- batch: 030 ----
mean loss: 182.09
 ---- batch: 040 ----
mean loss: 186.95
train mean loss: 185.08
epoch train time: 0:00:01.684018
elapsed time: 0:01:22.638996
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:17:37.723669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.88
 ---- batch: 020 ----
mean loss: 181.99
 ---- batch: 030 ----
mean loss: 180.36
 ---- batch: 040 ----
mean loss: 180.83
train mean loss: 182.12
epoch train time: 0:00:01.674625
elapsed time: 0:01:24.313750
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:17:39.398415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.09
 ---- batch: 020 ----
mean loss: 182.65
 ---- batch: 030 ----
mean loss: 192.37
 ---- batch: 040 ----
mean loss: 183.24
train mean loss: 184.96
epoch train time: 0:00:01.769561
elapsed time: 0:01:26.083449
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:17:41.168136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.85
 ---- batch: 020 ----
mean loss: 180.46
 ---- batch: 030 ----
mean loss: 181.60
 ---- batch: 040 ----
mean loss: 177.56
train mean loss: 180.15
epoch train time: 0:00:01.672185
elapsed time: 0:01:27.755817
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:17:42.840492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.98
 ---- batch: 020 ----
mean loss: 182.56
 ---- batch: 030 ----
mean loss: 182.75
 ---- batch: 040 ----
mean loss: 179.04
train mean loss: 183.47
epoch train time: 0:00:01.774209
elapsed time: 0:01:29.530173
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:17:44.614843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.17
 ---- batch: 020 ----
mean loss: 192.39
 ---- batch: 030 ----
mean loss: 177.82
 ---- batch: 040 ----
mean loss: 183.61
train mean loss: 184.50
epoch train time: 0:00:01.674877
elapsed time: 0:01:31.205262
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:17:46.289958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.15
 ---- batch: 020 ----
mean loss: 182.74
 ---- batch: 030 ----
mean loss: 177.43
 ---- batch: 040 ----
mean loss: 181.83
train mean loss: 181.05
epoch train time: 0:00:01.709386
elapsed time: 0:01:32.914806
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:17:47.999474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.37
 ---- batch: 020 ----
mean loss: 179.96
 ---- batch: 030 ----
mean loss: 183.83
 ---- batch: 040 ----
mean loss: 174.50
train mean loss: 178.89
epoch train time: 0:00:01.674291
elapsed time: 0:01:34.589314
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:17:49.673988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.05
 ---- batch: 020 ----
mean loss: 179.03
 ---- batch: 030 ----
mean loss: 188.74
 ---- batch: 040 ----
mean loss: 176.43
train mean loss: 182.62
epoch train time: 0:00:01.775485
elapsed time: 0:01:36.364935
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:17:51.449601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.21
 ---- batch: 020 ----
mean loss: 179.42
 ---- batch: 030 ----
mean loss: 176.39
 ---- batch: 040 ----
mean loss: 185.65
train mean loss: 181.21
epoch train time: 0:00:01.679540
elapsed time: 0:01:38.044618
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:17:53.129290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.67
 ---- batch: 020 ----
mean loss: 179.76
 ---- batch: 030 ----
mean loss: 174.60
 ---- batch: 040 ----
mean loss: 180.56
train mean loss: 178.74
epoch train time: 0:00:01.779609
elapsed time: 0:01:39.824377
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:17:54.909043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.40
 ---- batch: 020 ----
mean loss: 179.18
 ---- batch: 030 ----
mean loss: 173.01
 ---- batch: 040 ----
mean loss: 177.48
train mean loss: 177.56
epoch train time: 0:00:01.683945
elapsed time: 0:01:41.508476
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:17:56.593146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.93
 ---- batch: 020 ----
mean loss: 174.93
 ---- batch: 030 ----
mean loss: 180.87
 ---- batch: 040 ----
mean loss: 171.43
train mean loss: 176.42
epoch train time: 0:00:01.773397
elapsed time: 0:01:43.282084
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:17:58.366783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.29
 ---- batch: 020 ----
mean loss: 174.38
 ---- batch: 030 ----
mean loss: 173.37
 ---- batch: 040 ----
mean loss: 173.06
train mean loss: 176.87
epoch train time: 0:00:01.685818
elapsed time: 0:01:44.968095
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:18:00.052767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.99
 ---- batch: 020 ----
mean loss: 180.43
 ---- batch: 030 ----
mean loss: 174.76
 ---- batch: 040 ----
mean loss: 178.53
train mean loss: 177.90
epoch train time: 0:00:01.668186
elapsed time: 0:01:46.636416
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:18:01.721110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.40
 ---- batch: 020 ----
mean loss: 175.15
 ---- batch: 030 ----
mean loss: 179.34
 ---- batch: 040 ----
mean loss: 169.19
train mean loss: 175.21
epoch train time: 0:00:01.786259
elapsed time: 0:01:48.422835
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:18:03.507504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.74
 ---- batch: 020 ----
mean loss: 181.92
 ---- batch: 030 ----
mean loss: 181.42
 ---- batch: 040 ----
mean loss: 172.90
train mean loss: 179.25
epoch train time: 0:00:01.676422
elapsed time: 0:01:50.099397
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:18:05.184086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.68
 ---- batch: 020 ----
mean loss: 174.61
 ---- batch: 030 ----
mean loss: 171.27
 ---- batch: 040 ----
mean loss: 173.26
train mean loss: 173.63
epoch train time: 0:00:01.777240
elapsed time: 0:01:51.876791
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:18:06.961454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.67
 ---- batch: 020 ----
mean loss: 172.14
 ---- batch: 030 ----
mean loss: 177.84
 ---- batch: 040 ----
mean loss: 172.50
train mean loss: 174.24
epoch train time: 0:00:01.680441
elapsed time: 0:01:53.557402
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:18:08.642075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.26
 ---- batch: 020 ----
mean loss: 172.23
 ---- batch: 030 ----
mean loss: 176.52
 ---- batch: 040 ----
mean loss: 178.43
train mean loss: 173.32
epoch train time: 0:00:01.774657
elapsed time: 0:01:55.332262
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:18:10.416931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.87
 ---- batch: 020 ----
mean loss: 172.28
 ---- batch: 030 ----
mean loss: 172.99
 ---- batch: 040 ----
mean loss: 173.03
train mean loss: 172.65
epoch train time: 0:00:01.675921
elapsed time: 0:01:57.008327
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:18:12.093011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.73
 ---- batch: 020 ----
mean loss: 172.35
 ---- batch: 030 ----
mean loss: 172.65
 ---- batch: 040 ----
mean loss: 171.74
train mean loss: 172.45
epoch train time: 0:00:01.796491
elapsed time: 0:01:58.804976
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:18:13.889640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.39
 ---- batch: 020 ----
mean loss: 174.47
 ---- batch: 030 ----
mean loss: 162.63
 ---- batch: 040 ----
mean loss: 176.23
train mean loss: 170.58
epoch train time: 0:00:01.682365
elapsed time: 0:02:00.487479
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:18:15.572152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.01
 ---- batch: 020 ----
mean loss: 169.68
 ---- batch: 030 ----
mean loss: 165.24
 ---- batch: 040 ----
mean loss: 173.38
train mean loss: 171.54
epoch train time: 0:00:01.778629
elapsed time: 0:02:02.266258
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:18:17.350926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.04
 ---- batch: 020 ----
mean loss: 173.03
 ---- batch: 030 ----
mean loss: 172.00
 ---- batch: 040 ----
mean loss: 165.42
train mean loss: 170.35
epoch train time: 0:00:01.686379
elapsed time: 0:02:03.952833
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:18:19.037516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.17
 ---- batch: 020 ----
mean loss: 170.40
 ---- batch: 030 ----
mean loss: 176.40
 ---- batch: 040 ----
mean loss: 166.26
train mean loss: 170.36
epoch train time: 0:00:01.777981
elapsed time: 0:02:05.730987
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:18:20.815699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.43
 ---- batch: 020 ----
mean loss: 171.52
 ---- batch: 030 ----
mean loss: 167.00
 ---- batch: 040 ----
mean loss: 162.94
train mean loss: 165.75
epoch train time: 0:00:01.687632
elapsed time: 0:02:07.418805
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:18:22.503490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.08
 ---- batch: 020 ----
mean loss: 162.50
 ---- batch: 030 ----
mean loss: 167.34
 ---- batch: 040 ----
mean loss: 174.98
train mean loss: 168.77
epoch train time: 0:00:01.668566
elapsed time: 0:02:09.087534
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:18:24.172209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.68
 ---- batch: 020 ----
mean loss: 171.48
 ---- batch: 030 ----
mean loss: 167.51
 ---- batch: 040 ----
mean loss: 163.23
train mean loss: 167.47
epoch train time: 0:00:01.787225
elapsed time: 0:02:10.874904
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:18:25.959572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.51
 ---- batch: 020 ----
mean loss: 163.99
 ---- batch: 030 ----
mean loss: 171.10
 ---- batch: 040 ----
mean loss: 171.06
train mean loss: 169.15
epoch train time: 0:00:01.679285
elapsed time: 0:02:12.554339
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:18:27.639007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.03
 ---- batch: 020 ----
mean loss: 169.00
 ---- batch: 030 ----
mean loss: 167.08
 ---- batch: 040 ----
mean loss: 168.09
train mean loss: 168.81
epoch train time: 0:00:01.784210
elapsed time: 0:02:14.338688
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:18:29.423379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.26
 ---- batch: 020 ----
mean loss: 169.09
 ---- batch: 030 ----
mean loss: 169.26
 ---- batch: 040 ----
mean loss: 166.90
train mean loss: 168.54
epoch train time: 0:00:01.684052
elapsed time: 0:02:16.022904
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:18:31.107575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.21
 ---- batch: 020 ----
mean loss: 171.85
 ---- batch: 030 ----
mean loss: 167.12
 ---- batch: 040 ----
mean loss: 165.44
train mean loss: 166.34
epoch train time: 0:00:01.784653
elapsed time: 0:02:17.807702
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:18:32.892371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.97
 ---- batch: 020 ----
mean loss: 171.92
 ---- batch: 030 ----
mean loss: 166.26
 ---- batch: 040 ----
mean loss: 163.06
train mean loss: 167.80
epoch train time: 0:00:01.676059
elapsed time: 0:02:19.483914
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:18:34.568582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.67
 ---- batch: 020 ----
mean loss: 157.70
 ---- batch: 030 ----
mean loss: 167.01
 ---- batch: 040 ----
mean loss: 166.37
train mean loss: 165.73
epoch train time: 0:00:01.779737
elapsed time: 0:02:21.263793
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:18:36.348465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.30
 ---- batch: 020 ----
mean loss: 167.38
 ---- batch: 030 ----
mean loss: 162.51
 ---- batch: 040 ----
mean loss: 169.27
train mean loss: 166.74
epoch train time: 0:00:01.672666
elapsed time: 0:02:22.936603
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:18:38.021270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.71
 ---- batch: 020 ----
mean loss: 161.34
 ---- batch: 030 ----
mean loss: 167.29
 ---- batch: 040 ----
mean loss: 166.16
train mean loss: 165.92
epoch train time: 0:00:01.783389
elapsed time: 0:02:24.720130
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:18:39.804798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.46
 ---- batch: 020 ----
mean loss: 162.93
 ---- batch: 030 ----
mean loss: 167.05
 ---- batch: 040 ----
mean loss: 169.41
train mean loss: 166.20
epoch train time: 0:00:01.675511
elapsed time: 0:02:26.395836
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:18:41.480554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.27
 ---- batch: 020 ----
mean loss: 168.93
 ---- batch: 030 ----
mean loss: 167.26
 ---- batch: 040 ----
mean loss: 159.42
train mean loss: 165.41
epoch train time: 0:00:01.696630
elapsed time: 0:02:28.092657
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:18:43.177358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.87
 ---- batch: 020 ----
mean loss: 167.90
 ---- batch: 030 ----
mean loss: 164.33
 ---- batch: 040 ----
mean loss: 171.34
train mean loss: 169.04
epoch train time: 0:00:01.761388
elapsed time: 0:02:29.854222
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:18:44.938901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.19
 ---- batch: 020 ----
mean loss: 164.98
 ---- batch: 030 ----
mean loss: 169.25
 ---- batch: 040 ----
mean loss: 159.20
train mean loss: 165.14
epoch train time: 0:00:01.677755
elapsed time: 0:02:31.532141
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:18:46.616868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.42
 ---- batch: 020 ----
mean loss: 165.14
 ---- batch: 030 ----
mean loss: 166.03
 ---- batch: 040 ----
mean loss: 162.54
train mean loss: 164.87
epoch train time: 0:00:01.791339
elapsed time: 0:02:33.323676
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:18:48.408342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.22
 ---- batch: 020 ----
mean loss: 168.89
 ---- batch: 030 ----
mean loss: 165.57
 ---- batch: 040 ----
mean loss: 162.16
train mean loss: 165.58
epoch train time: 0:00:01.679304
elapsed time: 0:02:35.003155
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:18:50.087827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.88
 ---- batch: 020 ----
mean loss: 167.22
 ---- batch: 030 ----
mean loss: 161.03
 ---- batch: 040 ----
mean loss: 161.07
train mean loss: 163.70
epoch train time: 0:00:01.779496
elapsed time: 0:02:36.782797
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:18:51.867497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.72
 ---- batch: 020 ----
mean loss: 169.44
 ---- batch: 030 ----
mean loss: 164.91
 ---- batch: 040 ----
mean loss: 163.46
train mean loss: 164.48
epoch train time: 0:00:01.679242
elapsed time: 0:02:38.462238
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:18:53.546908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.23
 ---- batch: 020 ----
mean loss: 164.61
 ---- batch: 030 ----
mean loss: 164.46
 ---- batch: 040 ----
mean loss: 161.40
train mean loss: 164.30
epoch train time: 0:00:01.781364
elapsed time: 0:02:40.243734
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:18:55.328418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.30
 ---- batch: 020 ----
mean loss: 170.14
 ---- batch: 030 ----
mean loss: 166.41
 ---- batch: 040 ----
mean loss: 170.61
train mean loss: 167.15
epoch train time: 0:00:01.675113
elapsed time: 0:02:41.919014
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:18:57.003685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.85
 ---- batch: 020 ----
mean loss: 163.90
 ---- batch: 030 ----
mean loss: 160.60
 ---- batch: 040 ----
mean loss: 170.40
train mean loss: 164.05
epoch train time: 0:00:01.781833
elapsed time: 0:02:43.700982
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:18:58.785680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.35
 ---- batch: 020 ----
mean loss: 162.31
 ---- batch: 030 ----
mean loss: 169.42
 ---- batch: 040 ----
mean loss: 163.25
train mean loss: 164.96
epoch train time: 0:00:01.675250
elapsed time: 0:02:45.376402
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:19:00.461072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.87
 ---- batch: 020 ----
mean loss: 163.70
 ---- batch: 030 ----
mean loss: 164.87
 ---- batch: 040 ----
mean loss: 165.97
train mean loss: 166.27
epoch train time: 0:00:01.772487
elapsed time: 0:02:47.149025
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:19:02.233694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.07
 ---- batch: 020 ----
mean loss: 159.49
 ---- batch: 030 ----
mean loss: 163.20
 ---- batch: 040 ----
mean loss: 169.10
train mean loss: 164.51
epoch train time: 0:00:01.687411
elapsed time: 0:02:48.836578
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:19:03.921271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.71
 ---- batch: 020 ----
mean loss: 164.50
 ---- batch: 030 ----
mean loss: 160.20
 ---- batch: 040 ----
mean loss: 163.95
train mean loss: 162.29
epoch train time: 0:00:01.670014
elapsed time: 0:02:50.506743
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:19:05.591406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.98
 ---- batch: 020 ----
mean loss: 165.72
 ---- batch: 030 ----
mean loss: 157.50
 ---- batch: 040 ----
mean loss: 162.31
train mean loss: 164.03
epoch train time: 0:00:01.789388
elapsed time: 0:02:52.296267
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:19:07.380950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.37
 ---- batch: 020 ----
mean loss: 162.03
 ---- batch: 030 ----
mean loss: 159.65
 ---- batch: 040 ----
mean loss: 163.42
train mean loss: 161.21
epoch train time: 0:00:01.673351
elapsed time: 0:02:53.969833
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:19:09.054502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.51
 ---- batch: 020 ----
mean loss: 163.39
 ---- batch: 030 ----
mean loss: 160.30
 ---- batch: 040 ----
mean loss: 162.46
train mean loss: 163.20
epoch train time: 0:00:01.790781
elapsed time: 0:02:55.760764
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:19:10.845432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.70
 ---- batch: 020 ----
mean loss: 168.27
 ---- batch: 030 ----
mean loss: 166.02
 ---- batch: 040 ----
mean loss: 164.92
train mean loss: 164.58
epoch train time: 0:00:01.670494
elapsed time: 0:02:57.431416
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:19:12.516088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.78
 ---- batch: 020 ----
mean loss: 162.88
 ---- batch: 030 ----
mean loss: 161.67
 ---- batch: 040 ----
mean loss: 163.87
train mean loss: 165.26
epoch train time: 0:00:01.784328
elapsed time: 0:02:59.215874
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:19:14.300537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.81
 ---- batch: 020 ----
mean loss: 164.10
 ---- batch: 030 ----
mean loss: 154.64
 ---- batch: 040 ----
mean loss: 159.45
train mean loss: 161.91
epoch train time: 0:00:01.681579
elapsed time: 0:03:00.897590
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:19:15.982260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.66
 ---- batch: 020 ----
mean loss: 157.62
 ---- batch: 030 ----
mean loss: 160.03
 ---- batch: 040 ----
mean loss: 165.98
train mean loss: 161.40
epoch train time: 0:00:01.782374
elapsed time: 0:03:02.680126
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:19:17.764829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.71
 ---- batch: 020 ----
mean loss: 162.05
 ---- batch: 030 ----
mean loss: 163.44
 ---- batch: 040 ----
mean loss: 164.28
train mean loss: 161.87
epoch train time: 0:00:01.676972
elapsed time: 0:03:04.357266
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:19:19.441932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.95
 ---- batch: 020 ----
mean loss: 163.11
 ---- batch: 030 ----
mean loss: 160.11
 ---- batch: 040 ----
mean loss: 159.27
train mean loss: 160.62
epoch train time: 0:00:01.772106
elapsed time: 0:03:06.129513
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:19:21.214196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.70
 ---- batch: 020 ----
mean loss: 159.89
 ---- batch: 030 ----
mean loss: 161.30
 ---- batch: 040 ----
mean loss: 161.03
train mean loss: 158.32
epoch train time: 0:00:01.683422
elapsed time: 0:03:07.813092
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:19:22.897776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.72
 ---- batch: 020 ----
mean loss: 161.30
 ---- batch: 030 ----
mean loss: 161.57
 ---- batch: 040 ----
mean loss: 155.62
train mean loss: 159.56
epoch train time: 0:00:01.774871
elapsed time: 0:03:09.588112
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:19:24.672802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.74
 ---- batch: 020 ----
mean loss: 159.50
 ---- batch: 030 ----
mean loss: 160.22
 ---- batch: 040 ----
mean loss: 155.56
train mean loss: 157.99
epoch train time: 0:00:01.686902
elapsed time: 0:03:11.275186
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:19:26.359871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.74
 ---- batch: 020 ----
mean loss: 156.44
 ---- batch: 030 ----
mean loss: 156.55
 ---- batch: 040 ----
mean loss: 155.44
train mean loss: 156.15
epoch train time: 0:00:01.683598
elapsed time: 0:03:12.958926
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:19:28.043590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.49
 ---- batch: 020 ----
mean loss: 161.75
 ---- batch: 030 ----
mean loss: 156.20
 ---- batch: 040 ----
mean loss: 159.70
train mean loss: 158.92
epoch train time: 0:00:01.777048
elapsed time: 0:03:14.736137
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:19:29.820833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.10
 ---- batch: 020 ----
mean loss: 155.72
 ---- batch: 030 ----
mean loss: 154.40
 ---- batch: 040 ----
mean loss: 154.40
train mean loss: 156.54
epoch train time: 0:00:01.681990
elapsed time: 0:03:16.418288
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:19:31.502972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.23
 ---- batch: 020 ----
mean loss: 156.11
 ---- batch: 030 ----
mean loss: 156.24
 ---- batch: 040 ----
mean loss: 158.46
train mean loss: 159.88
epoch train time: 0:00:01.783234
elapsed time: 0:03:18.201685
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:19:33.286344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.04
 ---- batch: 020 ----
mean loss: 162.63
 ---- batch: 030 ----
mean loss: 157.63
 ---- batch: 040 ----
mean loss: 152.07
train mean loss: 158.62
epoch train time: 0:00:01.675876
elapsed time: 0:03:19.877725
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:19:34.962395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.84
 ---- batch: 020 ----
mean loss: 155.33
 ---- batch: 030 ----
mean loss: 153.09
 ---- batch: 040 ----
mean loss: 161.63
train mean loss: 155.43
epoch train time: 0:00:01.784293
elapsed time: 0:03:21.662180
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:19:36.746860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.55
 ---- batch: 020 ----
mean loss: 154.61
 ---- batch: 030 ----
mean loss: 160.89
 ---- batch: 040 ----
mean loss: 147.28
train mean loss: 154.64
epoch train time: 0:00:01.681325
elapsed time: 0:03:23.343670
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:19:38.428339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.80
 ---- batch: 020 ----
mean loss: 154.13
 ---- batch: 030 ----
mean loss: 158.53
 ---- batch: 040 ----
mean loss: 156.29
train mean loss: 156.58
epoch train time: 0:00:01.774816
elapsed time: 0:03:25.118626
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:19:40.203310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.82
 ---- batch: 020 ----
mean loss: 150.38
 ---- batch: 030 ----
mean loss: 150.71
 ---- batch: 040 ----
mean loss: 152.31
train mean loss: 153.13
epoch train time: 0:00:01.679228
elapsed time: 0:03:26.798008
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:19:41.882675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.65
 ---- batch: 020 ----
mean loss: 154.71
 ---- batch: 030 ----
mean loss: 154.81
 ---- batch: 040 ----
mean loss: 151.73
train mean loss: 154.03
epoch train time: 0:00:01.772011
elapsed time: 0:03:28.570150
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:19:43.654835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.33
 ---- batch: 020 ----
mean loss: 146.88
 ---- batch: 030 ----
mean loss: 158.12
 ---- batch: 040 ----
mean loss: 151.28
train mean loss: 152.31
epoch train time: 0:00:01.679713
elapsed time: 0:03:30.250036
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:19:45.334729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.79
 ---- batch: 020 ----
mean loss: 148.73
 ---- batch: 030 ----
mean loss: 155.75
 ---- batch: 040 ----
mean loss: 154.07
train mean loss: 153.30
epoch train time: 0:00:01.774079
elapsed time: 0:03:32.024299
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:19:47.108968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.46
 ---- batch: 020 ----
mean loss: 150.01
 ---- batch: 030 ----
mean loss: 155.24
 ---- batch: 040 ----
mean loss: 151.45
train mean loss: 152.78
epoch train time: 0:00:01.682773
elapsed time: 0:03:33.707221
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:19:48.791894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.35
 ---- batch: 020 ----
mean loss: 150.22
 ---- batch: 030 ----
mean loss: 153.31
 ---- batch: 040 ----
mean loss: 152.17
train mean loss: 152.56
epoch train time: 0:00:01.786878
elapsed time: 0:03:35.494235
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:19:50.578903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.09
 ---- batch: 020 ----
mean loss: 154.90
 ---- batch: 030 ----
mean loss: 147.72
 ---- batch: 040 ----
mean loss: 150.77
train mean loss: 150.29
epoch train time: 0:00:01.679701
elapsed time: 0:03:37.174071
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:19:52.258739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.09
 ---- batch: 020 ----
mean loss: 157.70
 ---- batch: 030 ----
mean loss: 151.14
 ---- batch: 040 ----
mean loss: 145.95
train mean loss: 150.18
epoch train time: 0:00:01.776337
elapsed time: 0:03:38.950571
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:19:54.035252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.20
 ---- batch: 020 ----
mean loss: 152.75
 ---- batch: 030 ----
mean loss: 155.35
 ---- batch: 040 ----
mean loss: 147.38
train mean loss: 150.64
epoch train time: 0:00:01.674094
elapsed time: 0:03:40.624844
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:19:55.709806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.35
 ---- batch: 020 ----
mean loss: 149.09
 ---- batch: 030 ----
mean loss: 153.92
 ---- batch: 040 ----
mean loss: 151.67
train mean loss: 151.40
epoch train time: 0:00:01.775729
elapsed time: 0:03:42.401024
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:19:57.485693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.94
 ---- batch: 020 ----
mean loss: 151.61
 ---- batch: 030 ----
mean loss: 143.92
 ---- batch: 040 ----
mean loss: 154.47
train mean loss: 150.08
epoch train time: 0:00:01.678565
elapsed time: 0:03:44.079732
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:19:59.164430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.66
 ---- batch: 020 ----
mean loss: 149.80
 ---- batch: 030 ----
mean loss: 156.14
 ---- batch: 040 ----
mean loss: 145.25
train mean loss: 148.69
epoch train time: 0:00:01.780889
elapsed time: 0:03:45.860882
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:20:00.945594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.95
 ---- batch: 020 ----
mean loss: 150.32
 ---- batch: 030 ----
mean loss: 151.75
 ---- batch: 040 ----
mean loss: 148.21
train mean loss: 151.27
epoch train time: 0:00:01.679408
elapsed time: 0:03:47.540486
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:20:02.625169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.14
 ---- batch: 020 ----
mean loss: 162.28
 ---- batch: 030 ----
mean loss: 145.92
 ---- batch: 040 ----
mean loss: 150.18
train mean loss: 152.95
epoch train time: 0:00:01.707983
elapsed time: 0:03:49.248619
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:20:04.333286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.81
 ---- batch: 020 ----
mean loss: 152.93
 ---- batch: 030 ----
mean loss: 150.72
 ---- batch: 040 ----
mean loss: 147.49
train mean loss: 149.21
epoch train time: 0:00:01.744556
elapsed time: 0:03:50.993314
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:20:06.077993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.76
 ---- batch: 020 ----
mean loss: 150.23
 ---- batch: 030 ----
mean loss: 150.19
 ---- batch: 040 ----
mean loss: 145.25
train mean loss: 146.67
epoch train time: 0:00:01.675616
elapsed time: 0:03:52.669086
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:20:07.753741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.88
 ---- batch: 020 ----
mean loss: 143.39
 ---- batch: 030 ----
mean loss: 144.36
 ---- batch: 040 ----
mean loss: 149.87
train mean loss: 146.73
epoch train time: 0:00:01.778069
elapsed time: 0:03:54.447331
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:20:09.532023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.42
 ---- batch: 020 ----
mean loss: 155.53
 ---- batch: 030 ----
mean loss: 152.11
 ---- batch: 040 ----
mean loss: 154.22
train mean loss: 151.21
epoch train time: 0:00:01.676587
elapsed time: 0:03:56.124081
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:20:11.208750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.23
 ---- batch: 020 ----
mean loss: 148.89
 ---- batch: 030 ----
mean loss: 144.18
 ---- batch: 040 ----
mean loss: 141.28
train mean loss: 146.87
epoch train time: 0:00:01.788015
elapsed time: 0:03:57.912228
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:20:12.996897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.75
 ---- batch: 020 ----
mean loss: 143.14
 ---- batch: 030 ----
mean loss: 142.62
 ---- batch: 040 ----
mean loss: 141.94
train mean loss: 143.60
epoch train time: 0:00:01.677482
elapsed time: 0:03:59.589885
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:20:14.674574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.10
 ---- batch: 020 ----
mean loss: 146.99
 ---- batch: 030 ----
mean loss: 145.90
 ---- batch: 040 ----
mean loss: 140.91
train mean loss: 142.83
epoch train time: 0:00:01.787866
elapsed time: 0:04:01.377899
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:20:16.462565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.81
 ---- batch: 020 ----
mean loss: 143.47
 ---- batch: 030 ----
mean loss: 147.06
 ---- batch: 040 ----
mean loss: 139.98
train mean loss: 143.88
epoch train time: 0:00:01.674566
elapsed time: 0:04:03.052604
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:20:18.137279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.72
 ---- batch: 020 ----
mean loss: 138.43
 ---- batch: 030 ----
mean loss: 142.61
 ---- batch: 040 ----
mean loss: 139.79
train mean loss: 140.73
epoch train time: 0:00:01.780775
elapsed time: 0:04:04.833514
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:20:19.918181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.26
 ---- batch: 020 ----
mean loss: 145.42
 ---- batch: 030 ----
mean loss: 146.71
 ---- batch: 040 ----
mean loss: 149.06
train mean loss: 147.74
epoch train time: 0:00:01.670989
elapsed time: 0:04:06.504651
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:20:21.589324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.50
 ---- batch: 020 ----
mean loss: 139.51
 ---- batch: 030 ----
mean loss: 139.04
 ---- batch: 040 ----
mean loss: 144.19
train mean loss: 140.95
epoch train time: 0:00:01.778224
elapsed time: 0:04:08.283011
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:20:23.367680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.43
 ---- batch: 020 ----
mean loss: 145.92
 ---- batch: 030 ----
mean loss: 137.76
 ---- batch: 040 ----
mean loss: 138.84
train mean loss: 141.52
epoch train time: 0:00:01.680132
elapsed time: 0:04:09.963281
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:20:25.047952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.96
 ---- batch: 020 ----
mean loss: 142.34
 ---- batch: 030 ----
mean loss: 143.75
 ---- batch: 040 ----
mean loss: 140.48
train mean loss: 141.81
epoch train time: 0:00:01.785079
elapsed time: 0:04:11.748559
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:20:26.833233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.88
 ---- batch: 020 ----
mean loss: 151.35
 ---- batch: 030 ----
mean loss: 140.62
 ---- batch: 040 ----
mean loss: 139.61
train mean loss: 143.38
epoch train time: 0:00:01.669933
elapsed time: 0:04:13.418663
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:20:28.503359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.68
 ---- batch: 020 ----
mean loss: 136.98
 ---- batch: 030 ----
mean loss: 139.59
 ---- batch: 040 ----
mean loss: 135.21
train mean loss: 138.41
epoch train time: 0:00:01.772508
elapsed time: 0:04:15.191328
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:20:30.275996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.17
 ---- batch: 020 ----
mean loss: 137.70
 ---- batch: 030 ----
mean loss: 141.86
 ---- batch: 040 ----
mean loss: 141.68
train mean loss: 140.20
epoch train time: 0:00:01.681883
elapsed time: 0:04:16.873347
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:20:31.958015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.90
 ---- batch: 020 ----
mean loss: 141.66
 ---- batch: 030 ----
mean loss: 135.67
 ---- batch: 040 ----
mean loss: 142.94
train mean loss: 139.85
epoch train time: 0:00:01.772479
elapsed time: 0:04:18.645956
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:20:33.730623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.63
 ---- batch: 020 ----
mean loss: 137.42
 ---- batch: 030 ----
mean loss: 142.11
 ---- batch: 040 ----
mean loss: 134.84
train mean loss: 137.50
epoch train time: 0:00:01.682815
elapsed time: 0:04:20.328909
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:20:35.413579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.98
 ---- batch: 020 ----
mean loss: 140.97
 ---- batch: 030 ----
mean loss: 144.34
 ---- batch: 040 ----
mean loss: 141.64
train mean loss: 141.31
epoch train time: 0:00:01.673256
elapsed time: 0:04:22.002297
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:20:37.086964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.67
 ---- batch: 020 ----
mean loss: 140.40
 ---- batch: 030 ----
mean loss: 138.49
 ---- batch: 040 ----
mean loss: 140.98
train mean loss: 140.51
epoch train time: 0:00:01.790367
elapsed time: 0:04:23.792801
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:20:38.877468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.72
 ---- batch: 020 ----
mean loss: 139.34
 ---- batch: 030 ----
mean loss: 138.00
 ---- batch: 040 ----
mean loss: 139.02
train mean loss: 138.98
epoch train time: 0:00:01.672452
elapsed time: 0:04:25.465402
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:20:40.550085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.27
 ---- batch: 020 ----
mean loss: 133.18
 ---- batch: 030 ----
mean loss: 133.52
 ---- batch: 040 ----
mean loss: 136.24
train mean loss: 134.42
epoch train time: 0:00:01.778509
elapsed time: 0:04:27.244066
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:20:42.328735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.30
 ---- batch: 020 ----
mean loss: 130.66
 ---- batch: 030 ----
mean loss: 136.43
 ---- batch: 040 ----
mean loss: 138.83
train mean loss: 135.91
epoch train time: 0:00:01.666806
elapsed time: 0:04:28.911020
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:20:43.995692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.33
 ---- batch: 020 ----
mean loss: 136.46
 ---- batch: 030 ----
mean loss: 134.95
 ---- batch: 040 ----
mean loss: 134.28
train mean loss: 136.02
epoch train time: 0:00:01.794978
elapsed time: 0:04:30.706204
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:20:45.790892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.74
 ---- batch: 020 ----
mean loss: 140.00
 ---- batch: 030 ----
mean loss: 135.75
 ---- batch: 040 ----
mean loss: 135.44
train mean loss: 138.24
epoch train time: 0:00:01.671396
elapsed time: 0:04:32.377784
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:20:47.462473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.54
 ---- batch: 020 ----
mean loss: 135.99
 ---- batch: 030 ----
mean loss: 137.35
 ---- batch: 040 ----
mean loss: 136.33
train mean loss: 136.97
epoch train time: 0:00:01.794613
elapsed time: 0:04:34.172579
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:20:49.257262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.32
 ---- batch: 020 ----
mean loss: 138.06
 ---- batch: 030 ----
mean loss: 136.17
 ---- batch: 040 ----
mean loss: 133.39
train mean loss: 136.24
epoch train time: 0:00:01.676940
elapsed time: 0:04:35.849685
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:20:50.934358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.25
 ---- batch: 020 ----
mean loss: 135.19
 ---- batch: 030 ----
mean loss: 134.60
 ---- batch: 040 ----
mean loss: 136.69
train mean loss: 134.84
epoch train time: 0:00:01.788424
elapsed time: 0:04:37.638329
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:20:52.723003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.08
 ---- batch: 020 ----
mean loss: 135.59
 ---- batch: 030 ----
mean loss: 133.71
 ---- batch: 040 ----
mean loss: 138.74
train mean loss: 139.98
epoch train time: 0:00:01.670800
elapsed time: 0:04:39.309292
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:20:54.393964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.75
 ---- batch: 020 ----
mean loss: 143.91
 ---- batch: 030 ----
mean loss: 136.83
 ---- batch: 040 ----
mean loss: 141.05
train mean loss: 139.97
epoch train time: 0:00:01.776774
elapsed time: 0:04:41.086206
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:20:56.170877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.46
 ---- batch: 020 ----
mean loss: 134.35
 ---- batch: 030 ----
mean loss: 137.51
 ---- batch: 040 ----
mean loss: 134.26
train mean loss: 133.66
epoch train time: 0:00:01.673637
elapsed time: 0:04:42.760011
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:20:57.844684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.64
 ---- batch: 020 ----
mean loss: 133.58
 ---- batch: 030 ----
mean loss: 133.03
 ---- batch: 040 ----
mean loss: 138.64
train mean loss: 132.73
epoch train time: 0:00:01.780438
elapsed time: 0:04:44.540629
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:20:59.625298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.21
 ---- batch: 020 ----
mean loss: 133.37
 ---- batch: 030 ----
mean loss: 132.80
 ---- batch: 040 ----
mean loss: 142.23
train mean loss: 136.18
epoch train time: 0:00:01.678937
elapsed time: 0:04:46.219717
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:21:01.304410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.29
 ---- batch: 020 ----
mean loss: 133.66
 ---- batch: 030 ----
mean loss: 136.90
 ---- batch: 040 ----
mean loss: 135.88
train mean loss: 137.55
epoch train time: 0:00:01.780298
elapsed time: 0:04:48.000196
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:21:03.084869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.46
 ---- batch: 020 ----
mean loss: 126.02
 ---- batch: 030 ----
mean loss: 129.38
 ---- batch: 040 ----
mean loss: 141.13
train mean loss: 133.30
epoch train time: 0:00:01.681052
elapsed time: 0:04:49.681425
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:21:04.766109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.92
 ---- batch: 020 ----
mean loss: 128.96
 ---- batch: 030 ----
mean loss: 134.84
 ---- batch: 040 ----
mean loss: 133.07
train mean loss: 132.09
epoch train time: 0:00:01.776532
elapsed time: 0:04:51.458120
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:21:06.542798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.22
 ---- batch: 020 ----
mean loss: 133.04
 ---- batch: 030 ----
mean loss: 136.62
 ---- batch: 040 ----
mean loss: 132.02
train mean loss: 133.03
epoch train time: 0:00:01.688509
elapsed time: 0:04:53.146799
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:21:08.231487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.56
 ---- batch: 020 ----
mean loss: 129.47
 ---- batch: 030 ----
mean loss: 136.15
 ---- batch: 040 ----
mean loss: 135.18
train mean loss: 133.03
epoch train time: 0:00:01.731706
elapsed time: 0:04:54.878688
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:21:09.963360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.73
 ---- batch: 020 ----
mean loss: 128.76
 ---- batch: 030 ----
mean loss: 135.34
 ---- batch: 040 ----
mean loss: 133.10
train mean loss: 132.87
epoch train time: 0:00:01.734473
elapsed time: 0:04:56.613320
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:21:11.697994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.00
 ---- batch: 020 ----
mean loss: 132.17
 ---- batch: 030 ----
mean loss: 134.04
 ---- batch: 040 ----
mean loss: 130.36
train mean loss: 132.41
epoch train time: 0:00:01.661940
elapsed time: 0:04:58.275421
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:21:13.360121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.57
 ---- batch: 020 ----
mean loss: 135.95
 ---- batch: 030 ----
mean loss: 133.22
 ---- batch: 040 ----
mean loss: 131.11
train mean loss: 135.89
epoch train time: 0:00:01.712325
elapsed time: 0:04:59.987928
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:21:15.072599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.79
 ---- batch: 020 ----
mean loss: 131.80
 ---- batch: 030 ----
mean loss: 136.24
 ---- batch: 040 ----
mean loss: 137.12
train mean loss: 134.76
epoch train time: 0:00:01.675111
elapsed time: 0:05:01.663204
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:21:16.747877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.13
 ---- batch: 020 ----
mean loss: 136.92
 ---- batch: 030 ----
mean loss: 133.14
 ---- batch: 040 ----
mean loss: 133.32
train mean loss: 135.64
epoch train time: 0:00:01.796231
elapsed time: 0:05:03.459606
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:21:18.544274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.24
 ---- batch: 020 ----
mean loss: 131.30
 ---- batch: 030 ----
mean loss: 137.15
 ---- batch: 040 ----
mean loss: 136.37
train mean loss: 134.47
epoch train time: 0:00:01.666259
elapsed time: 0:05:05.126010
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:21:20.210691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.71
 ---- batch: 020 ----
mean loss: 130.38
 ---- batch: 030 ----
mean loss: 133.14
 ---- batch: 040 ----
mean loss: 139.23
train mean loss: 134.73
epoch train time: 0:00:01.781413
elapsed time: 0:05:06.907572
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:21:21.992243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.17
 ---- batch: 020 ----
mean loss: 133.90
 ---- batch: 030 ----
mean loss: 130.67
 ---- batch: 040 ----
mean loss: 131.04
train mean loss: 133.76
epoch train time: 0:00:01.670233
elapsed time: 0:05:08.577970
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:21:23.662644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.04
 ---- batch: 020 ----
mean loss: 130.52
 ---- batch: 030 ----
mean loss: 134.69
 ---- batch: 040 ----
mean loss: 132.47
train mean loss: 133.23
epoch train time: 0:00:01.794483
elapsed time: 0:05:10.372614
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:21:25.457302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.07
 ---- batch: 020 ----
mean loss: 135.63
 ---- batch: 030 ----
mean loss: 132.53
 ---- batch: 040 ----
mean loss: 135.89
train mean loss: 133.57
epoch train time: 0:00:01.675757
elapsed time: 0:05:12.048551
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:21:27.133226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.09
 ---- batch: 020 ----
mean loss: 139.38
 ---- batch: 030 ----
mean loss: 130.45
 ---- batch: 040 ----
mean loss: 136.08
train mean loss: 134.31
epoch train time: 0:00:01.782584
elapsed time: 0:05:13.831307
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:21:28.915986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.67
 ---- batch: 020 ----
mean loss: 129.69
 ---- batch: 030 ----
mean loss: 133.92
 ---- batch: 040 ----
mean loss: 131.44
train mean loss: 131.86
epoch train time: 0:00:01.683735
elapsed time: 0:05:15.515224
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:21:30.599895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.48
 ---- batch: 020 ----
mean loss: 129.29
 ---- batch: 030 ----
mean loss: 132.46
 ---- batch: 040 ----
mean loss: 129.94
train mean loss: 130.59
epoch train time: 0:00:01.769572
elapsed time: 0:05:17.284949
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:21:32.369616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.93
 ---- batch: 020 ----
mean loss: 133.45
 ---- batch: 030 ----
mean loss: 129.98
 ---- batch: 040 ----
mean loss: 128.07
train mean loss: 130.04
epoch train time: 0:00:01.684546
elapsed time: 0:05:18.969660
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:21:34.054332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.42
 ---- batch: 020 ----
mean loss: 127.65
 ---- batch: 030 ----
mean loss: 135.46
 ---- batch: 040 ----
mean loss: 134.70
train mean loss: 132.39
epoch train time: 0:00:01.767428
elapsed time: 0:05:20.737238
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:21:35.821908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.98
 ---- batch: 020 ----
mean loss: 132.44
 ---- batch: 030 ----
mean loss: 126.58
 ---- batch: 040 ----
mean loss: 129.08
train mean loss: 129.79
epoch train time: 0:00:01.685433
elapsed time: 0:05:22.422821
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:21:37.507491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.79
 ---- batch: 020 ----
mean loss: 127.84
 ---- batch: 030 ----
mean loss: 130.86
 ---- batch: 040 ----
mean loss: 128.11
train mean loss: 130.18
epoch train time: 0:00:01.669285
elapsed time: 0:05:24.092246
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:21:39.176945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.48
 ---- batch: 020 ----
mean loss: 133.07
 ---- batch: 030 ----
mean loss: 128.72
 ---- batch: 040 ----
mean loss: 134.85
train mean loss: 133.23
epoch train time: 0:00:01.801946
elapsed time: 0:05:25.894382
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:21:40.979053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.16
 ---- batch: 020 ----
mean loss: 124.12
 ---- batch: 030 ----
mean loss: 128.57
 ---- batch: 040 ----
mean loss: 133.19
train mean loss: 129.36
epoch train time: 0:00:01.670019
elapsed time: 0:05:27.564545
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:21:42.649214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.78
 ---- batch: 020 ----
mean loss: 140.21
 ---- batch: 030 ----
mean loss: 130.94
 ---- batch: 040 ----
mean loss: 128.17
train mean loss: 133.24
epoch train time: 0:00:01.792541
elapsed time: 0:05:29.357231
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:21:44.441901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.99
 ---- batch: 020 ----
mean loss: 131.03
 ---- batch: 030 ----
mean loss: 134.75
 ---- batch: 040 ----
mean loss: 135.71
train mean loss: 131.61
epoch train time: 0:00:01.669325
elapsed time: 0:05:31.026707
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:21:46.111378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.21
 ---- batch: 020 ----
mean loss: 129.21
 ---- batch: 030 ----
mean loss: 126.18
 ---- batch: 040 ----
mean loss: 129.75
train mean loss: 128.78
epoch train time: 0:00:01.786197
elapsed time: 0:05:32.813067
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:21:47.897764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.93
 ---- batch: 020 ----
mean loss: 126.52
 ---- batch: 030 ----
mean loss: 128.16
 ---- batch: 040 ----
mean loss: 128.83
train mean loss: 128.05
epoch train time: 0:00:01.670964
elapsed time: 0:05:34.484222
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:21:49.568894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.92
 ---- batch: 020 ----
mean loss: 128.47
 ---- batch: 030 ----
mean loss: 130.53
 ---- batch: 040 ----
mean loss: 132.25
train mean loss: 129.20
epoch train time: 0:00:01.802022
elapsed time: 0:05:36.286392
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:21:51.371063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.03
 ---- batch: 020 ----
mean loss: 127.95
 ---- batch: 030 ----
mean loss: 131.99
 ---- batch: 040 ----
mean loss: 132.67
train mean loss: 131.60
epoch train time: 0:00:01.667931
elapsed time: 0:05:37.954470
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:21:53.039141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.93
 ---- batch: 020 ----
mean loss: 131.79
 ---- batch: 030 ----
mean loss: 127.70
 ---- batch: 040 ----
mean loss: 127.98
train mean loss: 129.32
epoch train time: 0:00:01.787987
elapsed time: 0:05:39.742602
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:21:54.827299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.12
 ---- batch: 020 ----
mean loss: 131.80
 ---- batch: 030 ----
mean loss: 125.72
 ---- batch: 040 ----
mean loss: 140.94
train mean loss: 130.93
epoch train time: 0:00:01.675180
elapsed time: 0:05:41.417966
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:21:56.502641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.37
 ---- batch: 020 ----
mean loss: 140.48
 ---- batch: 030 ----
mean loss: 137.20
 ---- batch: 040 ----
mean loss: 132.70
train mean loss: 137.89
epoch train time: 0:00:01.779874
elapsed time: 0:05:43.197980
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:21:58.282652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.76
 ---- batch: 020 ----
mean loss: 134.50
 ---- batch: 030 ----
mean loss: 127.90
 ---- batch: 040 ----
mean loss: 130.98
train mean loss: 130.56
epoch train time: 0:00:01.681821
elapsed time: 0:05:44.879960
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:21:59.964631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.95
 ---- batch: 020 ----
mean loss: 127.13
 ---- batch: 030 ----
mean loss: 128.35
 ---- batch: 040 ----
mean loss: 125.69
train mean loss: 126.79
epoch train time: 0:00:01.767463
elapsed time: 0:05:46.647566
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:22:01.732249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.84
 ---- batch: 020 ----
mean loss: 129.35
 ---- batch: 030 ----
mean loss: 126.18
 ---- batch: 040 ----
mean loss: 131.42
train mean loss: 127.24
epoch train time: 0:00:01.680376
elapsed time: 0:05:48.328124
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:22:03.412828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.41
 ---- batch: 020 ----
mean loss: 128.45
 ---- batch: 030 ----
mean loss: 125.64
 ---- batch: 040 ----
mean loss: 124.46
train mean loss: 126.65
epoch train time: 0:00:01.671294
elapsed time: 0:05:49.999611
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:22:05.084286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.80
 ---- batch: 020 ----
mean loss: 129.65
 ---- batch: 030 ----
mean loss: 125.11
 ---- batch: 040 ----
mean loss: 126.88
train mean loss: 127.62
epoch train time: 0:00:01.783437
elapsed time: 0:05:51.783207
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:22:06.867882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.98
 ---- batch: 020 ----
mean loss: 126.22
 ---- batch: 030 ----
mean loss: 125.39
 ---- batch: 040 ----
mean loss: 124.53
train mean loss: 125.28
epoch train time: 0:00:01.668940
elapsed time: 0:05:53.452302
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:22:08.536991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.21
 ---- batch: 020 ----
mean loss: 129.76
 ---- batch: 030 ----
mean loss: 130.44
 ---- batch: 040 ----
mean loss: 132.02
train mean loss: 129.88
epoch train time: 0:00:01.773401
elapsed time: 0:05:55.225868
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:22:10.310536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.12
 ---- batch: 020 ----
mean loss: 128.88
 ---- batch: 030 ----
mean loss: 130.81
 ---- batch: 040 ----
mean loss: 133.84
train mean loss: 131.11
epoch train time: 0:00:01.667451
elapsed time: 0:05:56.893471
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:22:11.978145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.51
 ---- batch: 020 ----
mean loss: 127.42
 ---- batch: 030 ----
mean loss: 130.06
 ---- batch: 040 ----
mean loss: 124.97
train mean loss: 127.10
epoch train time: 0:00:01.806094
elapsed time: 0:05:58.699734
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:22:13.784428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.02
 ---- batch: 020 ----
mean loss: 129.65
 ---- batch: 030 ----
mean loss: 129.70
 ---- batch: 040 ----
mean loss: 128.24
train mean loss: 128.68
epoch train time: 0:00:01.670088
elapsed time: 0:06:00.370003
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:22:15.454676
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.36
 ---- batch: 020 ----
mean loss: 124.78
 ---- batch: 030 ----
mean loss: 121.47
 ---- batch: 040 ----
mean loss: 125.47
train mean loss: 124.59
epoch train time: 0:00:01.735171
elapsed time: 0:06:02.105357
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:22:17.190020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.09
 ---- batch: 020 ----
mean loss: 126.41
 ---- batch: 030 ----
mean loss: 125.42
 ---- batch: 040 ----
mean loss: 122.77
train mean loss: 125.69
epoch train time: 0:00:01.677204
elapsed time: 0:06:03.782715
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:22:18.867389
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.07
 ---- batch: 020 ----
mean loss: 126.54
 ---- batch: 030 ----
mean loss: 121.97
 ---- batch: 040 ----
mean loss: 121.93
train mean loss: 124.44
epoch train time: 0:00:01.781181
elapsed time: 0:06:05.564051
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:22:20.648744
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.46
 ---- batch: 020 ----
mean loss: 123.09
 ---- batch: 030 ----
mean loss: 123.67
 ---- batch: 040 ----
mean loss: 129.11
train mean loss: 124.57
epoch train time: 0:00:01.677241
elapsed time: 0:06:07.241471
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:22:22.326142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.53
 ---- batch: 020 ----
mean loss: 123.62
 ---- batch: 030 ----
mean loss: 121.44
 ---- batch: 040 ----
mean loss: 123.39
train mean loss: 123.57
epoch train time: 0:00:01.774055
elapsed time: 0:06:09.015665
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:22:24.100373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.17
 ---- batch: 020 ----
mean loss: 125.00
 ---- batch: 030 ----
mean loss: 127.93
 ---- batch: 040 ----
mean loss: 122.35
train mean loss: 125.30
epoch train time: 0:00:01.675430
elapsed time: 0:06:10.691293
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:22:25.775967
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.23
 ---- batch: 020 ----
mean loss: 123.20
 ---- batch: 030 ----
mean loss: 124.30
 ---- batch: 040 ----
mean loss: 122.81
train mean loss: 123.68
epoch train time: 0:00:01.770362
elapsed time: 0:06:12.461813
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:22:27.546487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.51
 ---- batch: 020 ----
mean loss: 126.65
 ---- batch: 030 ----
mean loss: 124.30
 ---- batch: 040 ----
mean loss: 128.26
train mean loss: 125.28
epoch train time: 0:00:01.693634
elapsed time: 0:06:14.155605
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:22:29.240294
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.75
 ---- batch: 020 ----
mean loss: 129.46
 ---- batch: 030 ----
mean loss: 122.35
 ---- batch: 040 ----
mean loss: 122.77
train mean loss: 123.63
epoch train time: 0:00:01.670456
elapsed time: 0:06:15.826222
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:22:30.910898
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.52
 ---- batch: 020 ----
mean loss: 126.27
 ---- batch: 030 ----
mean loss: 126.04
 ---- batch: 040 ----
mean loss: 125.00
train mean loss: 125.98
epoch train time: 0:00:01.796836
elapsed time: 0:06:17.623233
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:22:32.707915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.50
 ---- batch: 020 ----
mean loss: 123.06
 ---- batch: 030 ----
mean loss: 118.76
 ---- batch: 040 ----
mean loss: 126.38
train mean loss: 123.54
epoch train time: 0:00:01.677896
elapsed time: 0:06:19.301283
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:22:34.385955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.38
 ---- batch: 020 ----
mean loss: 125.25
 ---- batch: 030 ----
mean loss: 123.00
 ---- batch: 040 ----
mean loss: 122.39
train mean loss: 124.23
epoch train time: 0:00:01.787579
elapsed time: 0:06:21.089013
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:22:36.173679
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.51
 ---- batch: 020 ----
mean loss: 119.58
 ---- batch: 030 ----
mean loss: 125.05
 ---- batch: 040 ----
mean loss: 127.56
train mean loss: 124.42
epoch train time: 0:00:01.675476
elapsed time: 0:06:22.764643
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:22:37.849317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.06
 ---- batch: 020 ----
mean loss: 122.64
 ---- batch: 030 ----
mean loss: 126.95
 ---- batch: 040 ----
mean loss: 121.56
train mean loss: 122.86
epoch train time: 0:00:01.791620
elapsed time: 0:06:24.556404
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:22:39.641122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.96
 ---- batch: 020 ----
mean loss: 127.67
 ---- batch: 030 ----
mean loss: 123.47
 ---- batch: 040 ----
mean loss: 121.73
train mean loss: 124.78
epoch train time: 0:00:01.674414
elapsed time: 0:06:26.231007
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:22:41.315676
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.04
 ---- batch: 020 ----
mean loss: 126.57
 ---- batch: 030 ----
mean loss: 126.07
 ---- batch: 040 ----
mean loss: 119.90
train mean loss: 124.69
epoch train time: 0:00:01.764691
elapsed time: 0:06:27.995849
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:22:43.080520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.10
 ---- batch: 020 ----
mean loss: 120.98
 ---- batch: 030 ----
mean loss: 126.41
 ---- batch: 040 ----
mean loss: 122.74
train mean loss: 123.88
epoch train time: 0:00:01.676223
elapsed time: 0:06:29.672231
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:22:44.756904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.41
 ---- batch: 020 ----
mean loss: 127.44
 ---- batch: 030 ----
mean loss: 122.56
 ---- batch: 040 ----
mean loss: 120.99
train mean loss: 124.52
epoch train time: 0:00:01.774314
elapsed time: 0:06:31.446695
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:22:46.531363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.96
 ---- batch: 020 ----
mean loss: 127.09
 ---- batch: 030 ----
mean loss: 123.88
 ---- batch: 040 ----
mean loss: 124.27
train mean loss: 124.98
epoch train time: 0:00:01.680578
elapsed time: 0:06:33.127420
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:22:48.212090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.43
 ---- batch: 020 ----
mean loss: 123.57
 ---- batch: 030 ----
mean loss: 124.03
 ---- batch: 040 ----
mean loss: 123.40
train mean loss: 124.06
epoch train time: 0:00:01.677073
elapsed time: 0:06:34.804628
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:22:49.889327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.93
 ---- batch: 020 ----
mean loss: 125.39
 ---- batch: 030 ----
mean loss: 120.47
 ---- batch: 040 ----
mean loss: 127.34
train mean loss: 125.14
epoch train time: 0:00:01.782541
elapsed time: 0:06:36.587390
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:22:51.672071
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.01
 ---- batch: 020 ----
mean loss: 123.77
 ---- batch: 030 ----
mean loss: 120.44
 ---- batch: 040 ----
mean loss: 123.98
train mean loss: 123.48
epoch train time: 0:00:01.682765
elapsed time: 0:06:38.270311
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:22:53.354981
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.43
 ---- batch: 020 ----
mean loss: 124.14
 ---- batch: 030 ----
mean loss: 125.22
 ---- batch: 040 ----
mean loss: 121.08
train mean loss: 124.19
epoch train time: 0:00:01.775993
elapsed time: 0:06:40.046436
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:22:55.131103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.91
 ---- batch: 020 ----
mean loss: 126.42
 ---- batch: 030 ----
mean loss: 122.23
 ---- batch: 040 ----
mean loss: 122.36
train mean loss: 123.91
epoch train time: 0:00:01.675113
elapsed time: 0:06:41.721703
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:22:56.806373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.91
 ---- batch: 020 ----
mean loss: 119.95
 ---- batch: 030 ----
mean loss: 126.31
 ---- batch: 040 ----
mean loss: 128.05
train mean loss: 124.35
epoch train time: 0:00:01.781513
elapsed time: 0:06:43.503363
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:22:58.588033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.48
 ---- batch: 020 ----
mean loss: 123.52
 ---- batch: 030 ----
mean loss: 122.11
 ---- batch: 040 ----
mean loss: 126.82
train mean loss: 122.99
epoch train time: 0:00:01.683258
elapsed time: 0:06:45.186759
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:23:00.271429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.57
 ---- batch: 020 ----
mean loss: 122.50
 ---- batch: 030 ----
mean loss: 124.09
 ---- batch: 040 ----
mean loss: 121.52
train mean loss: 122.72
epoch train time: 0:00:01.781867
elapsed time: 0:06:46.968804
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:23:02.053488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.66
 ---- batch: 020 ----
mean loss: 119.68
 ---- batch: 030 ----
mean loss: 124.61
 ---- batch: 040 ----
mean loss: 122.48
train mean loss: 122.98
epoch train time: 0:00:01.695623
elapsed time: 0:06:48.664596
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:23:03.749282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.05
 ---- batch: 020 ----
mean loss: 123.69
 ---- batch: 030 ----
mean loss: 124.40
 ---- batch: 040 ----
mean loss: 123.62
train mean loss: 123.52
epoch train time: 0:00:01.781438
elapsed time: 0:06:50.446182
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:23:05.530850
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.61
 ---- batch: 020 ----
mean loss: 124.42
 ---- batch: 030 ----
mean loss: 123.89
 ---- batch: 040 ----
mean loss: 122.46
train mean loss: 123.40
epoch train time: 0:00:01.684321
elapsed time: 0:06:52.130646
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:23:07.215316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.68
 ---- batch: 020 ----
mean loss: 120.77
 ---- batch: 030 ----
mean loss: 123.94
 ---- batch: 040 ----
mean loss: 122.05
train mean loss: 122.08
epoch train time: 0:00:01.669861
elapsed time: 0:06:53.800647
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:23:08.885317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.79
 ---- batch: 020 ----
mean loss: 126.95
 ---- batch: 030 ----
mean loss: 121.05
 ---- batch: 040 ----
mean loss: 121.16
train mean loss: 121.98
epoch train time: 0:00:01.803725
elapsed time: 0:06:55.604530
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:23:10.689253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.86
 ---- batch: 020 ----
mean loss: 121.02
 ---- batch: 030 ----
mean loss: 126.40
 ---- batch: 040 ----
mean loss: 123.59
train mean loss: 124.19
epoch train time: 0:00:01.679828
elapsed time: 0:06:57.284562
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:23:12.369220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.39
 ---- batch: 020 ----
mean loss: 123.92
 ---- batch: 030 ----
mean loss: 124.09
 ---- batch: 040 ----
mean loss: 122.08
train mean loss: 122.75
epoch train time: 0:00:01.777103
elapsed time: 0:06:59.061808
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:23:14.146496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.98
 ---- batch: 020 ----
mean loss: 126.86
 ---- batch: 030 ----
mean loss: 119.10
 ---- batch: 040 ----
mean loss: 123.43
train mean loss: 123.67
epoch train time: 0:00:01.674412
elapsed time: 0:07:00.736389
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:23:15.821063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.13
 ---- batch: 020 ----
mean loss: 122.50
 ---- batch: 030 ----
mean loss: 120.76
 ---- batch: 040 ----
mean loss: 126.51
train mean loss: 123.89
epoch train time: 0:00:01.773980
elapsed time: 0:07:02.510503
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:23:17.595185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.96
 ---- batch: 020 ----
mean loss: 119.10
 ---- batch: 030 ----
mean loss: 121.08
 ---- batch: 040 ----
mean loss: 123.31
train mean loss: 122.65
epoch train time: 0:00:01.681739
elapsed time: 0:07:04.192389
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:23:19.277056
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.80
 ---- batch: 020 ----
mean loss: 126.06
 ---- batch: 030 ----
mean loss: 125.21
 ---- batch: 040 ----
mean loss: 124.93
train mean loss: 125.29
epoch train time: 0:00:01.781625
elapsed time: 0:07:05.974174
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:23:21.058852
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.36
 ---- batch: 020 ----
mean loss: 120.97
 ---- batch: 030 ----
mean loss: 125.35
 ---- batch: 040 ----
mean loss: 124.19
train mean loss: 123.80
epoch train time: 0:00:01.676923
elapsed time: 0:07:07.651258
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:23:22.735928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.14
 ---- batch: 020 ----
mean loss: 124.08
 ---- batch: 030 ----
mean loss: 121.22
 ---- batch: 040 ----
mean loss: 127.29
train mean loss: 124.11
epoch train time: 0:00:01.782392
elapsed time: 0:07:09.433797
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:23:24.518463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.73
 ---- batch: 020 ----
mean loss: 121.50
 ---- batch: 030 ----
mean loss: 125.79
 ---- batch: 040 ----
mean loss: 124.04
train mean loss: 123.35
epoch train time: 0:00:01.685437
elapsed time: 0:07:11.119382
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:23:26.204071
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.62
 ---- batch: 020 ----
mean loss: 122.77
 ---- batch: 030 ----
mean loss: 124.23
 ---- batch: 040 ----
mean loss: 119.90
train mean loss: 121.63
epoch train time: 0:00:01.770492
elapsed time: 0:07:12.890034
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:23:27.974703
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.94
 ---- batch: 020 ----
mean loss: 122.30
 ---- batch: 030 ----
mean loss: 125.39
 ---- batch: 040 ----
mean loss: 124.45
train mean loss: 124.28
epoch train time: 0:00:01.682766
elapsed time: 0:07:14.572950
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:23:29.657621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.47
 ---- batch: 020 ----
mean loss: 124.14
 ---- batch: 030 ----
mean loss: 125.93
 ---- batch: 040 ----
mean loss: 120.95
train mean loss: 123.56
epoch train time: 0:00:01.675699
elapsed time: 0:07:16.248786
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:23:31.333473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.76
 ---- batch: 020 ----
mean loss: 130.41
 ---- batch: 030 ----
mean loss: 128.46
 ---- batch: 040 ----
mean loss: 119.33
train mean loss: 124.15
epoch train time: 0:00:01.783522
elapsed time: 0:07:18.032475
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:23:33.117145
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.52
 ---- batch: 020 ----
mean loss: 124.66
 ---- batch: 030 ----
mean loss: 125.46
 ---- batch: 040 ----
mean loss: 125.00
train mean loss: 124.20
epoch train time: 0:00:01.675234
elapsed time: 0:07:19.707887
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:23:34.792558
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.19
 ---- batch: 020 ----
mean loss: 123.89
 ---- batch: 030 ----
mean loss: 125.63
 ---- batch: 040 ----
mean loss: 125.83
train mean loss: 125.13
epoch train time: 0:00:01.789557
elapsed time: 0:07:21.497584
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:23:36.582280
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.67
 ---- batch: 020 ----
mean loss: 125.20
 ---- batch: 030 ----
mean loss: 126.42
 ---- batch: 040 ----
mean loss: 121.61
train mean loss: 125.06
epoch train time: 0:00:01.680557
elapsed time: 0:07:23.178319
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:23:38.262990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.23
 ---- batch: 020 ----
mean loss: 123.03
 ---- batch: 030 ----
mean loss: 124.41
 ---- batch: 040 ----
mean loss: 123.52
train mean loss: 124.33
epoch train time: 0:00:01.774661
elapsed time: 0:07:24.956511
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_9/checkpoint.pth.tar
**** end time: 2019-09-20 23:23:40.041148 ****
