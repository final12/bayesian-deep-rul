Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_5', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 13090
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-26 21:22:55.653505 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 11, 11]           1,120
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
    BayesianConv2d-4            [-1, 14, 4, 11]             448
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
    BayesianLinear-8                    [-1, 1]             616
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,184
Trainable params: 2,184
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 21:22:55.665279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4863.02
 ---- batch: 020 ----
mean loss: 4745.34
 ---- batch: 030 ----
mean loss: 4614.42
 ---- batch: 040 ----
mean loss: 4426.94
 ---- batch: 050 ----
mean loss: 4249.62
 ---- batch: 060 ----
mean loss: 4014.24
 ---- batch: 070 ----
mean loss: 3846.35
 ---- batch: 080 ----
mean loss: 3659.70
 ---- batch: 090 ----
mean loss: 3457.32
 ---- batch: 100 ----
mean loss: 3308.32
 ---- batch: 110 ----
mean loss: 3136.59
train mean loss: 4001.71
epoch train time: 0:00:34.856575
elapsed time: 0:00:34.871737
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 21:23:30.525299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2868.49
 ---- batch: 020 ----
mean loss: 2665.92
 ---- batch: 030 ----
mean loss: 2516.16
 ---- batch: 040 ----
mean loss: 2361.26
 ---- batch: 050 ----
mean loss: 2228.06
 ---- batch: 060 ----
mean loss: 2058.92
 ---- batch: 070 ----
mean loss: 1923.05
 ---- batch: 080 ----
mean loss: 1816.41
 ---- batch: 090 ----
mean loss: 1701.64
 ---- batch: 100 ----
mean loss: 1581.56
 ---- batch: 110 ----
mean loss: 1482.82
train mean loss: 2091.91
epoch train time: 0:00:02.510003
elapsed time: 0:00:37.381978
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 21:23:33.035708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1389.10
 ---- batch: 020 ----
mean loss: 1326.13
 ---- batch: 030 ----
mean loss: 1261.53
 ---- batch: 040 ----
mean loss: 1202.16
 ---- batch: 050 ----
mean loss: 1146.09
 ---- batch: 060 ----
mean loss: 1090.45
 ---- batch: 070 ----
mean loss: 1083.24
 ---- batch: 080 ----
mean loss: 1031.14
 ---- batch: 090 ----
mean loss: 998.38
 ---- batch: 100 ----
mean loss: 989.77
 ---- batch: 110 ----
mean loss: 967.02
train mean loss: 1129.74
epoch train time: 0:00:02.466754
elapsed time: 0:00:39.849139
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 21:23:35.502867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 950.95
 ---- batch: 020 ----
mean loss: 934.77
 ---- batch: 030 ----
mean loss: 927.27
 ---- batch: 040 ----
mean loss: 903.53
 ---- batch: 050 ----
mean loss: 888.09
 ---- batch: 060 ----
mean loss: 889.83
 ---- batch: 070 ----
mean loss: 888.80
 ---- batch: 080 ----
mean loss: 866.36
 ---- batch: 090 ----
mean loss: 887.24
 ---- batch: 100 ----
mean loss: 894.30
 ---- batch: 110 ----
mean loss: 878.18
train mean loss: 900.03
epoch train time: 0:00:02.470588
elapsed time: 0:00:42.320143
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 21:23:37.973903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.75
 ---- batch: 020 ----
mean loss: 868.90
 ---- batch: 030 ----
mean loss: 881.79
 ---- batch: 040 ----
mean loss: 875.82
 ---- batch: 050 ----
mean loss: 871.80
 ---- batch: 060 ----
mean loss: 857.80
 ---- batch: 070 ----
mean loss: 861.45
 ---- batch: 080 ----
mean loss: 865.90
 ---- batch: 090 ----
mean loss: 862.83
 ---- batch: 100 ----
mean loss: 877.86
 ---- batch: 110 ----
mean loss: 867.89
train mean loss: 868.83
epoch train time: 0:00:02.472247
elapsed time: 0:00:44.792928
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 21:23:40.446773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.13
 ---- batch: 020 ----
mean loss: 866.33
 ---- batch: 030 ----
mean loss: 868.73
 ---- batch: 040 ----
mean loss: 840.10
 ---- batch: 050 ----
mean loss: 855.80
 ---- batch: 060 ----
mean loss: 881.50
 ---- batch: 070 ----
mean loss: 849.82
 ---- batch: 080 ----
mean loss: 882.35
 ---- batch: 090 ----
mean loss: 858.81
 ---- batch: 100 ----
mean loss: 868.54
 ---- batch: 110 ----
mean loss: 861.63
train mean loss: 863.19
epoch train time: 0:00:02.506242
elapsed time: 0:00:47.299748
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 21:23:42.953492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.17
 ---- batch: 020 ----
mean loss: 848.79
 ---- batch: 030 ----
mean loss: 858.08
 ---- batch: 040 ----
mean loss: 860.34
 ---- batch: 050 ----
mean loss: 831.20
 ---- batch: 060 ----
mean loss: 862.48
 ---- batch: 070 ----
mean loss: 847.53
 ---- batch: 080 ----
mean loss: 871.28
 ---- batch: 090 ----
mean loss: 868.22
 ---- batch: 100 ----
mean loss: 875.30
 ---- batch: 110 ----
mean loss: 862.05
train mean loss: 860.06
epoch train time: 0:00:02.491479
elapsed time: 0:00:49.791660
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 21:23:45.445387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.09
 ---- batch: 020 ----
mean loss: 836.61
 ---- batch: 030 ----
mean loss: 872.77
 ---- batch: 040 ----
mean loss: 854.05
 ---- batch: 050 ----
mean loss: 853.00
 ---- batch: 060 ----
mean loss: 861.02
 ---- batch: 070 ----
mean loss: 831.67
 ---- batch: 080 ----
mean loss: 857.60
 ---- batch: 090 ----
mean loss: 860.23
 ---- batch: 100 ----
mean loss: 862.15
 ---- batch: 110 ----
mean loss: 851.97
train mean loss: 852.96
epoch train time: 0:00:02.480992
elapsed time: 0:00:52.273071
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 21:23:47.926808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.07
 ---- batch: 020 ----
mean loss: 849.87
 ---- batch: 030 ----
mean loss: 835.91
 ---- batch: 040 ----
mean loss: 843.79
 ---- batch: 050 ----
mean loss: 857.62
 ---- batch: 060 ----
mean loss: 839.07
 ---- batch: 070 ----
mean loss: 860.28
 ---- batch: 080 ----
mean loss: 840.47
 ---- batch: 090 ----
mean loss: 840.45
 ---- batch: 100 ----
mean loss: 867.37
 ---- batch: 110 ----
mean loss: 850.39
train mean loss: 849.72
epoch train time: 0:00:02.480988
elapsed time: 0:00:54.754472
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 21:23:50.408208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.50
 ---- batch: 020 ----
mean loss: 850.22
 ---- batch: 030 ----
mean loss: 827.29
 ---- batch: 040 ----
mean loss: 859.41
 ---- batch: 050 ----
mean loss: 845.77
 ---- batch: 060 ----
mean loss: 846.66
 ---- batch: 070 ----
mean loss: 838.68
 ---- batch: 080 ----
mean loss: 874.24
 ---- batch: 090 ----
mean loss: 833.83
 ---- batch: 100 ----
mean loss: 829.03
 ---- batch: 110 ----
mean loss: 848.30
train mean loss: 844.31
epoch train time: 0:00:02.500536
elapsed time: 0:00:57.255530
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 21:23:52.909381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.28
 ---- batch: 020 ----
mean loss: 824.45
 ---- batch: 030 ----
mean loss: 840.99
 ---- batch: 040 ----
mean loss: 853.01
 ---- batch: 050 ----
mean loss: 828.07
 ---- batch: 060 ----
mean loss: 836.89
 ---- batch: 070 ----
mean loss: 842.92
 ---- batch: 080 ----
mean loss: 837.33
 ---- batch: 090 ----
mean loss: 851.65
 ---- batch: 100 ----
mean loss: 837.59
 ---- batch: 110 ----
mean loss: 832.06
train mean loss: 839.41
epoch train time: 0:00:02.481217
elapsed time: 0:00:59.737325
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 21:23:55.391010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 874.97
 ---- batch: 020 ----
mean loss: 821.54
 ---- batch: 030 ----
mean loss: 846.88
 ---- batch: 040 ----
mean loss: 843.43
 ---- batch: 050 ----
mean loss: 832.99
 ---- batch: 060 ----
mean loss: 825.90
 ---- batch: 070 ----
mean loss: 829.92
 ---- batch: 080 ----
mean loss: 827.74
 ---- batch: 090 ----
mean loss: 838.11
 ---- batch: 100 ----
mean loss: 834.47
 ---- batch: 110 ----
mean loss: 803.45
train mean loss: 834.38
epoch train time: 0:00:02.512692
elapsed time: 0:01:02.250405
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 21:23:57.904151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.03
 ---- batch: 020 ----
mean loss: 841.57
 ---- batch: 030 ----
mean loss: 836.23
 ---- batch: 040 ----
mean loss: 821.19
 ---- batch: 050 ----
mean loss: 823.46
 ---- batch: 060 ----
mean loss: 819.91
 ---- batch: 070 ----
mean loss: 834.05
 ---- batch: 080 ----
mean loss: 803.56
 ---- batch: 090 ----
mean loss: 828.30
 ---- batch: 100 ----
mean loss: 835.88
 ---- batch: 110 ----
mean loss: 805.72
train mean loss: 827.13
epoch train time: 0:00:02.510592
elapsed time: 0:01:04.761443
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 21:24:00.415307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 814.49
 ---- batch: 020 ----
mean loss: 824.81
 ---- batch: 030 ----
mean loss: 815.54
 ---- batch: 040 ----
mean loss: 813.09
 ---- batch: 050 ----
mean loss: 820.13
 ---- batch: 060 ----
mean loss: 829.88
 ---- batch: 070 ----
mean loss: 826.05
 ---- batch: 080 ----
mean loss: 829.69
 ---- batch: 090 ----
mean loss: 814.77
 ---- batch: 100 ----
mean loss: 834.53
 ---- batch: 110 ----
mean loss: 840.29
train mean loss: 824.67
epoch train time: 0:00:02.494024
elapsed time: 0:01:07.256009
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 21:24:02.909768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.76
 ---- batch: 020 ----
mean loss: 809.70
 ---- batch: 030 ----
mean loss: 831.19
 ---- batch: 040 ----
mean loss: 833.54
 ---- batch: 050 ----
mean loss: 825.17
 ---- batch: 060 ----
mean loss: 814.19
 ---- batch: 070 ----
mean loss: 811.50
 ---- batch: 080 ----
mean loss: 805.40
 ---- batch: 090 ----
mean loss: 815.49
 ---- batch: 100 ----
mean loss: 813.17
 ---- batch: 110 ----
mean loss: 836.58
train mean loss: 819.43
epoch train time: 0:00:02.511901
elapsed time: 0:01:09.768348
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 21:24:05.422091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.43
 ---- batch: 020 ----
mean loss: 820.75
 ---- batch: 030 ----
mean loss: 813.64
 ---- batch: 040 ----
mean loss: 799.26
 ---- batch: 050 ----
mean loss: 810.71
 ---- batch: 060 ----
mean loss: 818.92
 ---- batch: 070 ----
mean loss: 820.19
 ---- batch: 080 ----
mean loss: 798.48
 ---- batch: 090 ----
mean loss: 804.23
 ---- batch: 100 ----
mean loss: 822.13
 ---- batch: 110 ----
mean loss: 799.53
train mean loss: 812.75
epoch train time: 0:00:02.520107
elapsed time: 0:01:12.288870
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 21:24:07.942678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.22
 ---- batch: 020 ----
mean loss: 777.51
 ---- batch: 030 ----
mean loss: 804.85
 ---- batch: 040 ----
mean loss: 830.00
 ---- batch: 050 ----
mean loss: 825.94
 ---- batch: 060 ----
mean loss: 823.16
 ---- batch: 070 ----
mean loss: 817.15
 ---- batch: 080 ----
mean loss: 804.29
 ---- batch: 090 ----
mean loss: 794.74
 ---- batch: 100 ----
mean loss: 800.28
 ---- batch: 110 ----
mean loss: 798.73
train mean loss: 807.34
epoch train time: 0:00:02.492775
elapsed time: 0:01:14.782109
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 21:24:10.435858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.60
 ---- batch: 020 ----
mean loss: 808.58
 ---- batch: 030 ----
mean loss: 796.11
 ---- batch: 040 ----
mean loss: 813.55
 ---- batch: 050 ----
mean loss: 817.97
 ---- batch: 060 ----
mean loss: 790.06
 ---- batch: 070 ----
mean loss: 812.22
 ---- batch: 080 ----
mean loss: 795.54
 ---- batch: 090 ----
mean loss: 798.15
 ---- batch: 100 ----
mean loss: 817.33
 ---- batch: 110 ----
mean loss: 808.37
train mean loss: 803.70
epoch train time: 0:00:02.509330
elapsed time: 0:01:17.291849
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 21:24:12.945626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 795.86
 ---- batch: 020 ----
mean loss: 810.79
 ---- batch: 030 ----
mean loss: 798.63
 ---- batch: 040 ----
mean loss: 779.94
 ---- batch: 050 ----
mean loss: 787.18
 ---- batch: 060 ----
mean loss: 803.33
 ---- batch: 070 ----
mean loss: 801.01
 ---- batch: 080 ----
mean loss: 801.85
 ---- batch: 090 ----
mean loss: 802.48
 ---- batch: 100 ----
mean loss: 792.60
 ---- batch: 110 ----
mean loss: 817.82
train mean loss: 798.29
epoch train time: 0:00:02.493448
elapsed time: 0:01:19.785737
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 21:24:15.439477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 792.28
 ---- batch: 020 ----
mean loss: 802.84
 ---- batch: 030 ----
mean loss: 806.11
 ---- batch: 040 ----
mean loss: 787.45
 ---- batch: 050 ----
mean loss: 780.89
 ---- batch: 060 ----
mean loss: 804.29
 ---- batch: 070 ----
mean loss: 794.03
 ---- batch: 080 ----
mean loss: 795.38
 ---- batch: 090 ----
mean loss: 787.99
 ---- batch: 100 ----
mean loss: 795.21
 ---- batch: 110 ----
mean loss: 795.97
train mean loss: 793.71
epoch train time: 0:00:02.495810
elapsed time: 0:01:22.281970
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 21:24:17.935768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.55
 ---- batch: 020 ----
mean loss: 824.55
 ---- batch: 030 ----
mean loss: 791.71
 ---- batch: 040 ----
mean loss: 813.77
 ---- batch: 050 ----
mean loss: 791.43
 ---- batch: 060 ----
mean loss: 798.94
 ---- batch: 070 ----
mean loss: 784.86
 ---- batch: 080 ----
mean loss: 795.05
 ---- batch: 090 ----
mean loss: 776.39
 ---- batch: 100 ----
mean loss: 770.78
 ---- batch: 110 ----
mean loss: 770.73
train mean loss: 788.48
epoch train time: 0:00:02.521318
elapsed time: 0:01:24.803765
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 21:24:20.457441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 778.72
 ---- batch: 020 ----
mean loss: 799.10
 ---- batch: 030 ----
mean loss: 776.31
 ---- batch: 040 ----
mean loss: 804.85
 ---- batch: 050 ----
mean loss: 789.32
 ---- batch: 060 ----
mean loss: 779.63
 ---- batch: 070 ----
mean loss: 792.74
 ---- batch: 080 ----
mean loss: 781.48
 ---- batch: 090 ----
mean loss: 775.20
 ---- batch: 100 ----
mean loss: 772.17
 ---- batch: 110 ----
mean loss: 789.78
train mean loss: 784.72
epoch train time: 0:00:02.536273
elapsed time: 0:01:27.340405
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 21:24:22.994183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 767.98
 ---- batch: 020 ----
mean loss: 767.75
 ---- batch: 030 ----
mean loss: 755.17
 ---- batch: 040 ----
mean loss: 788.04
 ---- batch: 050 ----
mean loss: 797.82
 ---- batch: 060 ----
mean loss: 775.25
 ---- batch: 070 ----
mean loss: 800.70
 ---- batch: 080 ----
mean loss: 770.50
 ---- batch: 090 ----
mean loss: 772.41
 ---- batch: 100 ----
mean loss: 803.52
 ---- batch: 110 ----
mean loss: 771.26
train mean loss: 779.76
epoch train time: 0:00:02.532740
elapsed time: 0:01:29.873627
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 21:24:25.527365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.19
 ---- batch: 020 ----
mean loss: 778.18
 ---- batch: 030 ----
mean loss: 789.46
 ---- batch: 040 ----
mean loss: 769.12
 ---- batch: 050 ----
mean loss: 789.48
 ---- batch: 060 ----
mean loss: 768.55
 ---- batch: 070 ----
mean loss: 771.70
 ---- batch: 080 ----
mean loss: 784.23
 ---- batch: 090 ----
mean loss: 769.82
 ---- batch: 100 ----
mean loss: 783.09
 ---- batch: 110 ----
mean loss: 757.31
train mean loss: 774.62
epoch train time: 0:00:02.513343
elapsed time: 0:01:32.387384
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 21:24:28.041146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 772.49
 ---- batch: 020 ----
mean loss: 770.61
 ---- batch: 030 ----
mean loss: 756.90
 ---- batch: 040 ----
mean loss: 776.05
 ---- batch: 050 ----
mean loss: 771.70
 ---- batch: 060 ----
mean loss: 779.44
 ---- batch: 070 ----
mean loss: 752.87
 ---- batch: 080 ----
mean loss: 769.65
 ---- batch: 090 ----
mean loss: 776.34
 ---- batch: 100 ----
mean loss: 754.65
 ---- batch: 110 ----
mean loss: 777.68
train mean loss: 769.37
epoch train time: 0:00:02.502285
elapsed time: 0:01:34.890130
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 21:24:30.543874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.54
 ---- batch: 020 ----
mean loss: 764.81
 ---- batch: 030 ----
mean loss: 760.27
 ---- batch: 040 ----
mean loss: 765.07
 ---- batch: 050 ----
mean loss: 761.95
 ---- batch: 060 ----
mean loss: 774.37
 ---- batch: 070 ----
mean loss: 776.94
 ---- batch: 080 ----
mean loss: 752.14
 ---- batch: 090 ----
mean loss: 770.31
 ---- batch: 100 ----
mean loss: 755.03
 ---- batch: 110 ----
mean loss: 759.53
train mean loss: 763.85
epoch train time: 0:00:02.489599
elapsed time: 0:01:37.380198
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 21:24:33.033961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 763.79
 ---- batch: 020 ----
mean loss: 766.04
 ---- batch: 030 ----
mean loss: 769.27
 ---- batch: 040 ----
mean loss: 765.22
 ---- batch: 050 ----
mean loss: 753.89
 ---- batch: 060 ----
mean loss: 744.94
 ---- batch: 070 ----
mean loss: 741.03
 ---- batch: 080 ----
mean loss: 777.54
 ---- batch: 090 ----
mean loss: 771.46
 ---- batch: 100 ----
mean loss: 743.75
 ---- batch: 110 ----
mean loss: 750.07
train mean loss: 758.21
epoch train time: 0:00:02.504531
elapsed time: 0:01:39.885170
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 21:24:35.538898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 755.01
 ---- batch: 020 ----
mean loss: 741.17
 ---- batch: 030 ----
mean loss: 772.96
 ---- batch: 040 ----
mean loss: 771.72
 ---- batch: 050 ----
mean loss: 750.38
 ---- batch: 060 ----
mean loss: 749.49
 ---- batch: 070 ----
mean loss: 745.91
 ---- batch: 080 ----
mean loss: 746.92
 ---- batch: 090 ----
mean loss: 754.50
 ---- batch: 100 ----
mean loss: 743.67
 ---- batch: 110 ----
mean loss: 749.47
train mean loss: 751.97
epoch train time: 0:00:02.502138
elapsed time: 0:01:42.387712
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 21:24:38.041454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.88
 ---- batch: 020 ----
mean loss: 744.97
 ---- batch: 030 ----
mean loss: 759.22
 ---- batch: 040 ----
mean loss: 767.56
 ---- batch: 050 ----
mean loss: 767.17
 ---- batch: 060 ----
mean loss: 735.74
 ---- batch: 070 ----
mean loss: 745.87
 ---- batch: 080 ----
mean loss: 749.12
 ---- batch: 090 ----
mean loss: 729.77
 ---- batch: 100 ----
mean loss: 751.15
 ---- batch: 110 ----
mean loss: 738.89
train mean loss: 745.98
epoch train time: 0:00:02.491632
elapsed time: 0:01:44.879779
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 21:24:40.533512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 747.80
 ---- batch: 020 ----
mean loss: 725.66
 ---- batch: 030 ----
mean loss: 732.47
 ---- batch: 040 ----
mean loss: 736.28
 ---- batch: 050 ----
mean loss: 749.67
 ---- batch: 060 ----
mean loss: 735.88
 ---- batch: 070 ----
mean loss: 736.15
 ---- batch: 080 ----
mean loss: 751.45
 ---- batch: 090 ----
mean loss: 738.89
 ---- batch: 100 ----
mean loss: 745.72
 ---- batch: 110 ----
mean loss: 737.98
train mean loss: 739.11
epoch train time: 0:00:02.516562
elapsed time: 0:01:47.396806
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 21:24:43.050600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 735.27
 ---- batch: 020 ----
mean loss: 729.47
 ---- batch: 030 ----
mean loss: 740.47
 ---- batch: 040 ----
mean loss: 741.58
 ---- batch: 050 ----
mean loss: 735.84
 ---- batch: 060 ----
mean loss: 733.99
 ---- batch: 070 ----
mean loss: 700.37
 ---- batch: 080 ----
mean loss: 742.51
 ---- batch: 090 ----
mean loss: 733.47
 ---- batch: 100 ----
mean loss: 735.01
 ---- batch: 110 ----
mean loss: 738.77
train mean loss: 733.45
epoch train time: 0:00:02.492405
elapsed time: 0:01:49.889712
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 21:24:45.543473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.82
 ---- batch: 020 ----
mean loss: 712.67
 ---- batch: 030 ----
mean loss: 744.71
 ---- batch: 040 ----
mean loss: 747.61
 ---- batch: 050 ----
mean loss: 708.38
 ---- batch: 060 ----
mean loss: 730.31
 ---- batch: 070 ----
mean loss: 730.09
 ---- batch: 080 ----
mean loss: 728.39
 ---- batch: 090 ----
mean loss: 721.40
 ---- batch: 100 ----
mean loss: 726.41
 ---- batch: 110 ----
mean loss: 714.89
train mean loss: 727.06
epoch train time: 0:00:02.489288
elapsed time: 0:01:52.379431
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 21:24:48.033194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.28
 ---- batch: 020 ----
mean loss: 713.17
 ---- batch: 030 ----
mean loss: 735.69
 ---- batch: 040 ----
mean loss: 736.00
 ---- batch: 050 ----
mean loss: 726.71
 ---- batch: 060 ----
mean loss: 730.54
 ---- batch: 070 ----
mean loss: 693.93
 ---- batch: 080 ----
mean loss: 731.09
 ---- batch: 090 ----
mean loss: 715.37
 ---- batch: 100 ----
mean loss: 724.53
 ---- batch: 110 ----
mean loss: 707.47
train mean loss: 720.34
epoch train time: 0:00:02.502531
elapsed time: 0:01:54.882415
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 21:24:50.536165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.27
 ---- batch: 020 ----
mean loss: 716.34
 ---- batch: 030 ----
mean loss: 709.88
 ---- batch: 040 ----
mean loss: 713.52
 ---- batch: 050 ----
mean loss: 698.49
 ---- batch: 060 ----
mean loss: 708.72
 ---- batch: 070 ----
mean loss: 720.67
 ---- batch: 080 ----
mean loss: 722.20
 ---- batch: 090 ----
mean loss: 721.72
 ---- batch: 100 ----
mean loss: 717.16
 ---- batch: 110 ----
mean loss: 713.24
train mean loss: 713.29
epoch train time: 0:00:02.488634
elapsed time: 0:01:57.371483
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 21:24:53.025231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.25
 ---- batch: 020 ----
mean loss: 684.23
 ---- batch: 030 ----
mean loss: 701.70
 ---- batch: 040 ----
mean loss: 719.11
 ---- batch: 050 ----
mean loss: 707.98
 ---- batch: 060 ----
mean loss: 716.28
 ---- batch: 070 ----
mean loss: 691.63
 ---- batch: 080 ----
mean loss: 713.02
 ---- batch: 090 ----
mean loss: 716.45
 ---- batch: 100 ----
mean loss: 714.95
 ---- batch: 110 ----
mean loss: 710.54
train mean loss: 706.85
epoch train time: 0:00:02.503668
elapsed time: 0:01:59.875576
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 21:24:55.529211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.30
 ---- batch: 020 ----
mean loss: 709.79
 ---- batch: 030 ----
mean loss: 698.18
 ---- batch: 040 ----
mean loss: 699.08
 ---- batch: 050 ----
mean loss: 696.98
 ---- batch: 060 ----
mean loss: 694.16
 ---- batch: 070 ----
mean loss: 696.47
 ---- batch: 080 ----
mean loss: 696.09
 ---- batch: 090 ----
mean loss: 685.39
 ---- batch: 100 ----
mean loss: 696.55
 ---- batch: 110 ----
mean loss: 689.95
train mean loss: 698.74
epoch train time: 0:00:02.520232
elapsed time: 0:02:02.396210
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 21:24:58.049966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.18
 ---- batch: 020 ----
mean loss: 692.33
 ---- batch: 030 ----
mean loss: 701.87
 ---- batch: 040 ----
mean loss: 694.83
 ---- batch: 050 ----
mean loss: 684.79
 ---- batch: 060 ----
mean loss: 699.04
 ---- batch: 070 ----
mean loss: 683.47
 ---- batch: 080 ----
mean loss: 682.49
 ---- batch: 090 ----
mean loss: 700.20
 ---- batch: 100 ----
mean loss: 694.60
 ---- batch: 110 ----
mean loss: 674.08
train mean loss: 692.41
epoch train time: 0:00:02.542390
elapsed time: 0:02:04.939091
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 21:25:00.592825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.48
 ---- batch: 020 ----
mean loss: 695.05
 ---- batch: 030 ----
mean loss: 687.83
 ---- batch: 040 ----
mean loss: 687.38
 ---- batch: 050 ----
mean loss: 702.36
 ---- batch: 060 ----
mean loss: 669.02
 ---- batch: 070 ----
mean loss: 676.71
 ---- batch: 080 ----
mean loss: 678.45
 ---- batch: 090 ----
mean loss: 675.05
 ---- batch: 100 ----
mean loss: 683.39
 ---- batch: 110 ----
mean loss: 700.41
train mean loss: 685.86
epoch train time: 0:00:02.513409
elapsed time: 0:02:07.452933
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 21:25:03.106766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.23
 ---- batch: 020 ----
mean loss: 683.86
 ---- batch: 030 ----
mean loss: 684.49
 ---- batch: 040 ----
mean loss: 684.34
 ---- batch: 050 ----
mean loss: 658.58
 ---- batch: 060 ----
mean loss: 673.47
 ---- batch: 070 ----
mean loss: 677.60
 ---- batch: 080 ----
mean loss: 686.51
 ---- batch: 090 ----
mean loss: 672.39
 ---- batch: 100 ----
mean loss: 671.71
 ---- batch: 110 ----
mean loss: 682.10
train mean loss: 679.15
epoch train time: 0:00:02.508197
elapsed time: 0:02:09.961662
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 21:25:05.615478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.15
 ---- batch: 020 ----
mean loss: 676.80
 ---- batch: 030 ----
mean loss: 662.71
 ---- batch: 040 ----
mean loss: 674.20
 ---- batch: 050 ----
mean loss: 674.57
 ---- batch: 060 ----
mean loss: 668.14
 ---- batch: 070 ----
mean loss: 670.25
 ---- batch: 080 ----
mean loss: 680.45
 ---- batch: 090 ----
mean loss: 668.40
 ---- batch: 100 ----
mean loss: 677.59
 ---- batch: 110 ----
mean loss: 673.85
train mean loss: 672.06
epoch train time: 0:00:02.510520
elapsed time: 0:02:12.472678
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 21:25:08.126413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.25
 ---- batch: 020 ----
mean loss: 679.58
 ---- batch: 030 ----
mean loss: 658.54
 ---- batch: 040 ----
mean loss: 676.49
 ---- batch: 050 ----
mean loss: 670.31
 ---- batch: 060 ----
mean loss: 662.16
 ---- batch: 070 ----
mean loss: 666.40
 ---- batch: 080 ----
mean loss: 667.14
 ---- batch: 090 ----
mean loss: 650.33
 ---- batch: 100 ----
mean loss: 660.07
 ---- batch: 110 ----
mean loss: 663.33
train mean loss: 664.75
epoch train time: 0:00:02.496221
elapsed time: 0:02:14.969346
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 21:25:10.623174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.38
 ---- batch: 020 ----
mean loss: 649.09
 ---- batch: 030 ----
mean loss: 662.25
 ---- batch: 040 ----
mean loss: 659.49
 ---- batch: 050 ----
mean loss: 652.26
 ---- batch: 060 ----
mean loss: 649.40
 ---- batch: 070 ----
mean loss: 671.84
 ---- batch: 080 ----
mean loss: 673.14
 ---- batch: 090 ----
mean loss: 660.44
 ---- batch: 100 ----
mean loss: 656.89
 ---- batch: 110 ----
mean loss: 643.44
train mean loss: 657.99
epoch train time: 0:00:02.510545
elapsed time: 0:02:17.480401
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 21:25:13.134198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 654.22
 ---- batch: 020 ----
mean loss: 647.05
 ---- batch: 030 ----
mean loss: 654.15
 ---- batch: 040 ----
mean loss: 657.18
 ---- batch: 050 ----
mean loss: 657.04
 ---- batch: 060 ----
mean loss: 647.85
 ---- batch: 070 ----
mean loss: 655.48
 ---- batch: 080 ----
mean loss: 651.11
 ---- batch: 090 ----
mean loss: 646.96
 ---- batch: 100 ----
mean loss: 634.82
 ---- batch: 110 ----
mean loss: 650.84
train mean loss: 650.57
epoch train time: 0:00:02.490417
elapsed time: 0:02:19.971292
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 21:25:15.625031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.38
 ---- batch: 020 ----
mean loss: 636.44
 ---- batch: 030 ----
mean loss: 656.30
 ---- batch: 040 ----
mean loss: 646.38
 ---- batch: 050 ----
mean loss: 643.93
 ---- batch: 060 ----
mean loss: 640.28
 ---- batch: 070 ----
mean loss: 631.14
 ---- batch: 080 ----
mean loss: 652.01
 ---- batch: 090 ----
mean loss: 639.80
 ---- batch: 100 ----
mean loss: 646.48
 ---- batch: 110 ----
mean loss: 640.33
train mean loss: 643.54
epoch train time: 0:00:02.488756
elapsed time: 0:02:22.460454
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 21:25:18.114208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 632.51
 ---- batch: 020 ----
mean loss: 635.30
 ---- batch: 030 ----
mean loss: 636.87
 ---- batch: 040 ----
mean loss: 642.33
 ---- batch: 050 ----
mean loss: 633.20
 ---- batch: 060 ----
mean loss: 664.69
 ---- batch: 070 ----
mean loss: 645.35
 ---- batch: 080 ----
mean loss: 627.50
 ---- batch: 090 ----
mean loss: 627.59
 ---- batch: 100 ----
mean loss: 617.65
 ---- batch: 110 ----
mean loss: 634.40
train mean loss: 636.03
epoch train time: 0:00:02.499455
elapsed time: 0:02:24.960372
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 21:25:20.614110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 617.48
 ---- batch: 020 ----
mean loss: 648.46
 ---- batch: 030 ----
mean loss: 642.01
 ---- batch: 040 ----
mean loss: 637.85
 ---- batch: 050 ----
mean loss: 623.00
 ---- batch: 060 ----
mean loss: 620.23
 ---- batch: 070 ----
mean loss: 620.81
 ---- batch: 080 ----
mean loss: 627.01
 ---- batch: 090 ----
mean loss: 617.56
 ---- batch: 100 ----
mean loss: 631.70
 ---- batch: 110 ----
mean loss: 630.99
train mean loss: 628.27
epoch train time: 0:00:02.508282
elapsed time: 0:02:27.469059
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 21:25:23.122785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 621.50
 ---- batch: 020 ----
mean loss: 622.34
 ---- batch: 030 ----
mean loss: 626.70
 ---- batch: 040 ----
mean loss: 620.25
 ---- batch: 050 ----
mean loss: 618.05
 ---- batch: 060 ----
mean loss: 608.26
 ---- batch: 070 ----
mean loss: 637.69
 ---- batch: 080 ----
mean loss: 611.95
 ---- batch: 090 ----
mean loss: 628.40
 ---- batch: 100 ----
mean loss: 610.33
 ---- batch: 110 ----
mean loss: 626.38
train mean loss: 620.93
epoch train time: 0:00:02.493480
elapsed time: 0:02:29.962948
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 21:25:25.616711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 613.97
 ---- batch: 020 ----
mean loss: 623.83
 ---- batch: 030 ----
mean loss: 607.64
 ---- batch: 040 ----
mean loss: 599.56
 ---- batch: 050 ----
mean loss: 618.83
 ---- batch: 060 ----
mean loss: 609.91
 ---- batch: 070 ----
mean loss: 628.83
 ---- batch: 080 ----
mean loss: 609.36
 ---- batch: 090 ----
mean loss: 620.85
 ---- batch: 100 ----
mean loss: 594.23
 ---- batch: 110 ----
mean loss: 606.13
train mean loss: 611.99
epoch train time: 0:00:02.495276
elapsed time: 0:02:32.458657
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 21:25:28.112402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 607.56
 ---- batch: 020 ----
mean loss: 603.20
 ---- batch: 030 ----
mean loss: 614.56
 ---- batch: 040 ----
mean loss: 597.91
 ---- batch: 050 ----
mean loss: 594.04
 ---- batch: 060 ----
mean loss: 608.76
 ---- batch: 070 ----
mean loss: 611.54
 ---- batch: 080 ----
mean loss: 602.24
 ---- batch: 090 ----
mean loss: 592.77
 ---- batch: 100 ----
mean loss: 599.47
 ---- batch: 110 ----
mean loss: 591.19
train mean loss: 602.17
epoch train time: 0:00:02.518158
elapsed time: 0:02:34.977292
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 21:25:30.631048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 601.62
 ---- batch: 020 ----
mean loss: 598.55
 ---- batch: 030 ----
mean loss: 597.10
 ---- batch: 040 ----
mean loss: 597.57
 ---- batch: 050 ----
mean loss: 585.69
 ---- batch: 060 ----
mean loss: 594.80
 ---- batch: 070 ----
mean loss: 586.46
 ---- batch: 080 ----
mean loss: 593.04
 ---- batch: 090 ----
mean loss: 594.75
 ---- batch: 100 ----
mean loss: 582.49
 ---- batch: 110 ----
mean loss: 580.18
train mean loss: 592.49
epoch train time: 0:00:02.510977
elapsed time: 0:02:37.488709
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 21:25:33.142477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 584.79
 ---- batch: 020 ----
mean loss: 593.38
 ---- batch: 030 ----
mean loss: 581.02
 ---- batch: 040 ----
mean loss: 589.44
 ---- batch: 050 ----
mean loss: 584.91
 ---- batch: 060 ----
mean loss: 583.08
 ---- batch: 070 ----
mean loss: 581.46
 ---- batch: 080 ----
mean loss: 581.03
 ---- batch: 090 ----
mean loss: 573.19
 ---- batch: 100 ----
mean loss: 564.18
 ---- batch: 110 ----
mean loss: 568.98
train mean loss: 580.53
epoch train time: 0:00:02.484408
elapsed time: 0:02:39.973559
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 21:25:35.627314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 575.24
 ---- batch: 020 ----
mean loss: 574.31
 ---- batch: 030 ----
mean loss: 582.35
 ---- batch: 040 ----
mean loss: 563.53
 ---- batch: 050 ----
mean loss: 560.33
 ---- batch: 060 ----
mean loss: 569.37
 ---- batch: 070 ----
mean loss: 565.80
 ---- batch: 080 ----
mean loss: 552.14
 ---- batch: 090 ----
mean loss: 552.32
 ---- batch: 100 ----
mean loss: 570.05
 ---- batch: 110 ----
mean loss: 563.98
train mean loss: 566.35
epoch train time: 0:00:02.484008
elapsed time: 0:02:42.458007
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 21:25:38.111742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 563.33
 ---- batch: 020 ----
mean loss: 561.03
 ---- batch: 030 ----
mean loss: 549.88
 ---- batch: 040 ----
mean loss: 543.42
 ---- batch: 050 ----
mean loss: 541.40
 ---- batch: 060 ----
mean loss: 553.48
 ---- batch: 070 ----
mean loss: 552.10
 ---- batch: 080 ----
mean loss: 558.60
 ---- batch: 090 ----
mean loss: 540.74
 ---- batch: 100 ----
mean loss: 539.84
 ---- batch: 110 ----
mean loss: 559.87
train mean loss: 550.96
epoch train time: 0:00:02.491869
elapsed time: 0:02:44.950286
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 21:25:40.604053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 547.45
 ---- batch: 020 ----
mean loss: 544.92
 ---- batch: 030 ----
mean loss: 543.24
 ---- batch: 040 ----
mean loss: 525.31
 ---- batch: 050 ----
mean loss: 519.49
 ---- batch: 060 ----
mean loss: 538.93
 ---- batch: 070 ----
mean loss: 539.07
 ---- batch: 080 ----
mean loss: 535.28
 ---- batch: 090 ----
mean loss: 517.96
 ---- batch: 100 ----
mean loss: 529.02
 ---- batch: 110 ----
mean loss: 526.60
train mean loss: 532.77
epoch train time: 0:00:02.485293
elapsed time: 0:02:47.436055
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 21:25:43.089807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.02
 ---- batch: 020 ----
mean loss: 524.49
 ---- batch: 030 ----
mean loss: 524.80
 ---- batch: 040 ----
mean loss: 525.28
 ---- batch: 050 ----
mean loss: 522.54
 ---- batch: 060 ----
mean loss: 494.84
 ---- batch: 070 ----
mean loss: 508.39
 ---- batch: 080 ----
mean loss: 501.65
 ---- batch: 090 ----
mean loss: 502.16
 ---- batch: 100 ----
mean loss: 516.66
 ---- batch: 110 ----
mean loss: 506.57
train mean loss: 513.83
epoch train time: 0:00:02.466344
elapsed time: 0:02:49.902889
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 21:25:45.556637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.38
 ---- batch: 020 ----
mean loss: 499.87
 ---- batch: 030 ----
mean loss: 511.93
 ---- batch: 040 ----
mean loss: 493.38
 ---- batch: 050 ----
mean loss: 495.60
 ---- batch: 060 ----
mean loss: 485.37
 ---- batch: 070 ----
mean loss: 481.89
 ---- batch: 080 ----
mean loss: 483.17
 ---- batch: 090 ----
mean loss: 492.41
 ---- batch: 100 ----
mean loss: 480.58
 ---- batch: 110 ----
mean loss: 485.37
train mean loss: 493.04
epoch train time: 0:00:02.503237
elapsed time: 0:02:52.406542
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 21:25:48.060271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.42
 ---- batch: 020 ----
mean loss: 478.21
 ---- batch: 030 ----
mean loss: 480.57
 ---- batch: 040 ----
mean loss: 473.39
 ---- batch: 050 ----
mean loss: 483.22
 ---- batch: 060 ----
mean loss: 482.53
 ---- batch: 070 ----
mean loss: 460.57
 ---- batch: 080 ----
mean loss: 462.05
 ---- batch: 090 ----
mean loss: 464.60
 ---- batch: 100 ----
mean loss: 464.73
 ---- batch: 110 ----
mean loss: 463.31
train mean loss: 472.73
epoch train time: 0:00:02.484276
elapsed time: 0:02:54.891228
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 21:25:50.544842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.00
 ---- batch: 020 ----
mean loss: 466.60
 ---- batch: 030 ----
mean loss: 446.59
 ---- batch: 040 ----
mean loss: 452.08
 ---- batch: 050 ----
mean loss: 450.23
 ---- batch: 060 ----
mean loss: 443.86
 ---- batch: 070 ----
mean loss: 449.34
 ---- batch: 080 ----
mean loss: 432.45
 ---- batch: 090 ----
mean loss: 460.04
 ---- batch: 100 ----
mean loss: 454.08
 ---- batch: 110 ----
mean loss: 452.73
train mean loss: 452.10
epoch train time: 0:00:02.490624
elapsed time: 0:02:57.382191
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 21:25:53.035969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.00
 ---- batch: 020 ----
mean loss: 438.09
 ---- batch: 030 ----
mean loss: 440.66
 ---- batch: 040 ----
mean loss: 436.65
 ---- batch: 050 ----
mean loss: 434.10
 ---- batch: 060 ----
mean loss: 416.17
 ---- batch: 070 ----
mean loss: 430.92
 ---- batch: 080 ----
mean loss: 435.52
 ---- batch: 090 ----
mean loss: 427.43
 ---- batch: 100 ----
mean loss: 425.60
 ---- batch: 110 ----
mean loss: 427.20
train mean loss: 431.61
epoch train time: 0:00:02.493136
elapsed time: 0:02:59.875792
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 21:25:55.529561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.10
 ---- batch: 020 ----
mean loss: 421.35
 ---- batch: 030 ----
mean loss: 424.94
 ---- batch: 040 ----
mean loss: 411.55
 ---- batch: 050 ----
mean loss: 407.29
 ---- batch: 060 ----
mean loss: 408.81
 ---- batch: 070 ----
mean loss: 397.88
 ---- batch: 080 ----
mean loss: 408.96
 ---- batch: 090 ----
mean loss: 418.23
 ---- batch: 100 ----
mean loss: 407.72
 ---- batch: 110 ----
mean loss: 405.37
train mean loss: 412.43
epoch train time: 0:00:02.496311
elapsed time: 0:03:02.372576
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 21:25:58.026329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.57
 ---- batch: 020 ----
mean loss: 396.93
 ---- batch: 030 ----
mean loss: 405.48
 ---- batch: 040 ----
mean loss: 397.16
 ---- batch: 050 ----
mean loss: 404.79
 ---- batch: 060 ----
mean loss: 383.44
 ---- batch: 070 ----
mean loss: 386.67
 ---- batch: 080 ----
mean loss: 393.66
 ---- batch: 090 ----
mean loss: 385.48
 ---- batch: 100 ----
mean loss: 387.34
 ---- batch: 110 ----
mean loss: 386.34
train mean loss: 393.68
epoch train time: 0:00:02.488251
elapsed time: 0:03:04.861275
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 21:26:00.515031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.91
 ---- batch: 020 ----
mean loss: 382.87
 ---- batch: 030 ----
mean loss: 384.65
 ---- batch: 040 ----
mean loss: 375.76
 ---- batch: 050 ----
mean loss: 377.81
 ---- batch: 060 ----
mean loss: 377.60
 ---- batch: 070 ----
mean loss: 382.09
 ---- batch: 080 ----
mean loss: 370.87
 ---- batch: 090 ----
mean loss: 375.90
 ---- batch: 100 ----
mean loss: 363.63
 ---- batch: 110 ----
mean loss: 373.28
train mean loss: 375.72
epoch train time: 0:00:02.477522
elapsed time: 0:03:07.339229
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 21:26:02.993018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.27
 ---- batch: 020 ----
mean loss: 358.27
 ---- batch: 030 ----
mean loss: 357.83
 ---- batch: 040 ----
mean loss: 367.85
 ---- batch: 050 ----
mean loss: 363.86
 ---- batch: 060 ----
mean loss: 357.54
 ---- batch: 070 ----
mean loss: 347.01
 ---- batch: 080 ----
mean loss: 357.70
 ---- batch: 090 ----
mean loss: 355.74
 ---- batch: 100 ----
mean loss: 357.24
 ---- batch: 110 ----
mean loss: 359.46
train mean loss: 359.63
epoch train time: 0:00:02.500724
elapsed time: 0:03:09.840421
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 21:26:05.494225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.89
 ---- batch: 020 ----
mean loss: 336.49
 ---- batch: 030 ----
mean loss: 343.71
 ---- batch: 040 ----
mean loss: 348.48
 ---- batch: 050 ----
mean loss: 336.12
 ---- batch: 060 ----
mean loss: 342.89
 ---- batch: 070 ----
mean loss: 352.10
 ---- batch: 080 ----
mean loss: 339.40
 ---- batch: 090 ----
mean loss: 336.39
 ---- batch: 100 ----
mean loss: 343.40
 ---- batch: 110 ----
mean loss: 355.30
train mean loss: 343.29
epoch train time: 0:00:02.505902
elapsed time: 0:03:12.346795
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 21:26:08.000549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.04
 ---- batch: 020 ----
mean loss: 338.55
 ---- batch: 030 ----
mean loss: 333.56
 ---- batch: 040 ----
mean loss: 318.46
 ---- batch: 050 ----
mean loss: 331.89
 ---- batch: 060 ----
mean loss: 332.36
 ---- batch: 070 ----
mean loss: 333.01
 ---- batch: 080 ----
mean loss: 328.26
 ---- batch: 090 ----
mean loss: 336.90
 ---- batch: 100 ----
mean loss: 322.04
 ---- batch: 110 ----
mean loss: 317.61
train mean loss: 329.67
epoch train time: 0:00:02.476215
elapsed time: 0:03:14.823425
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 21:26:10.477155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.49
 ---- batch: 020 ----
mean loss: 317.55
 ---- batch: 030 ----
mean loss: 319.59
 ---- batch: 040 ----
mean loss: 321.68
 ---- batch: 050 ----
mean loss: 324.14
 ---- batch: 060 ----
mean loss: 312.49
 ---- batch: 070 ----
mean loss: 316.39
 ---- batch: 080 ----
mean loss: 313.20
 ---- batch: 090 ----
mean loss: 307.06
 ---- batch: 100 ----
mean loss: 303.12
 ---- batch: 110 ----
mean loss: 315.78
train mean loss: 316.57
epoch train time: 0:00:02.501267
elapsed time: 0:03:17.325190
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 21:26:12.978969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.37
 ---- batch: 020 ----
mean loss: 312.38
 ---- batch: 030 ----
mean loss: 309.34
 ---- batch: 040 ----
mean loss: 309.46
 ---- batch: 050 ----
mean loss: 303.54
 ---- batch: 060 ----
mean loss: 312.89
 ---- batch: 070 ----
mean loss: 303.06
 ---- batch: 080 ----
mean loss: 308.76
 ---- batch: 090 ----
mean loss: 301.99
 ---- batch: 100 ----
mean loss: 300.43
 ---- batch: 110 ----
mean loss: 293.82
train mean loss: 305.58
epoch train time: 0:00:02.463248
elapsed time: 0:03:19.788881
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 21:26:15.442688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.38
 ---- batch: 020 ----
mean loss: 297.69
 ---- batch: 030 ----
mean loss: 290.58
 ---- batch: 040 ----
mean loss: 300.60
 ---- batch: 050 ----
mean loss: 296.38
 ---- batch: 060 ----
mean loss: 290.16
 ---- batch: 070 ----
mean loss: 288.55
 ---- batch: 080 ----
mean loss: 290.06
 ---- batch: 090 ----
mean loss: 294.12
 ---- batch: 100 ----
mean loss: 295.22
 ---- batch: 110 ----
mean loss: 299.89
train mean loss: 295.58
epoch train time: 0:00:02.498851
elapsed time: 0:03:22.288262
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 21:26:17.942006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.14
 ---- batch: 020 ----
mean loss: 291.56
 ---- batch: 030 ----
mean loss: 284.61
 ---- batch: 040 ----
mean loss: 289.22
 ---- batch: 050 ----
mean loss: 286.14
 ---- batch: 060 ----
mean loss: 296.35
 ---- batch: 070 ----
mean loss: 290.09
 ---- batch: 080 ----
mean loss: 280.48
 ---- batch: 090 ----
mean loss: 286.82
 ---- batch: 100 ----
mean loss: 276.62
 ---- batch: 110 ----
mean loss: 290.31
train mean loss: 286.87
epoch train time: 0:00:02.504833
elapsed time: 0:03:24.793504
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 21:26:20.447230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.63
 ---- batch: 020 ----
mean loss: 277.49
 ---- batch: 030 ----
mean loss: 280.06
 ---- batch: 040 ----
mean loss: 274.86
 ---- batch: 050 ----
mean loss: 276.09
 ---- batch: 060 ----
mean loss: 289.03
 ---- batch: 070 ----
mean loss: 279.28
 ---- batch: 080 ----
mean loss: 275.30
 ---- batch: 090 ----
mean loss: 280.32
 ---- batch: 100 ----
mean loss: 267.51
 ---- batch: 110 ----
mean loss: 283.12
train mean loss: 278.90
epoch train time: 0:00:02.503110
elapsed time: 0:03:27.297003
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 21:26:22.950751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.73
 ---- batch: 020 ----
mean loss: 275.66
 ---- batch: 030 ----
mean loss: 279.23
 ---- batch: 040 ----
mean loss: 277.02
 ---- batch: 050 ----
mean loss: 270.46
 ---- batch: 060 ----
mean loss: 261.42
 ---- batch: 070 ----
mean loss: 278.81
 ---- batch: 080 ----
mean loss: 271.31
 ---- batch: 090 ----
mean loss: 271.31
 ---- batch: 100 ----
mean loss: 273.37
 ---- batch: 110 ----
mean loss: 269.34
train mean loss: 272.21
epoch train time: 0:00:02.492006
elapsed time: 0:03:29.789424
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 21:26:25.443199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.89
 ---- batch: 020 ----
mean loss: 264.45
 ---- batch: 030 ----
mean loss: 270.41
 ---- batch: 040 ----
mean loss: 264.96
 ---- batch: 050 ----
mean loss: 253.18
 ---- batch: 060 ----
mean loss: 263.84
 ---- batch: 070 ----
mean loss: 272.14
 ---- batch: 080 ----
mean loss: 278.65
 ---- batch: 090 ----
mean loss: 266.10
 ---- batch: 100 ----
mean loss: 267.50
 ---- batch: 110 ----
mean loss: 261.84
train mean loss: 266.66
epoch train time: 0:00:02.504190
elapsed time: 0:03:32.294050
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 21:26:27.947770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.97
 ---- batch: 020 ----
mean loss: 264.28
 ---- batch: 030 ----
mean loss: 263.83
 ---- batch: 040 ----
mean loss: 267.01
 ---- batch: 050 ----
mean loss: 268.67
 ---- batch: 060 ----
mean loss: 259.98
 ---- batch: 070 ----
mean loss: 258.32
 ---- batch: 080 ----
mean loss: 264.84
 ---- batch: 090 ----
mean loss: 259.83
 ---- batch: 100 ----
mean loss: 261.09
 ---- batch: 110 ----
mean loss: 256.90
train mean loss: 261.55
epoch train time: 0:00:02.512280
elapsed time: 0:03:34.806768
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 21:26:30.460460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.95
 ---- batch: 020 ----
mean loss: 262.18
 ---- batch: 030 ----
mean loss: 253.93
 ---- batch: 040 ----
mean loss: 253.78
 ---- batch: 050 ----
mean loss: 250.95
 ---- batch: 060 ----
mean loss: 264.01
 ---- batch: 070 ----
mean loss: 255.10
 ---- batch: 080 ----
mean loss: 264.17
 ---- batch: 090 ----
mean loss: 258.48
 ---- batch: 100 ----
mean loss: 256.39
 ---- batch: 110 ----
mean loss: 253.38
train mean loss: 257.64
epoch train time: 0:00:02.515825
elapsed time: 0:03:37.323002
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 21:26:32.976771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.65
 ---- batch: 020 ----
mean loss: 252.25
 ---- batch: 030 ----
mean loss: 250.93
 ---- batch: 040 ----
mean loss: 251.81
 ---- batch: 050 ----
mean loss: 246.60
 ---- batch: 060 ----
mean loss: 260.91
 ---- batch: 070 ----
mean loss: 256.65
 ---- batch: 080 ----
mean loss: 256.00
 ---- batch: 090 ----
mean loss: 250.48
 ---- batch: 100 ----
mean loss: 254.10
 ---- batch: 110 ----
mean loss: 246.05
train mean loss: 253.48
epoch train time: 0:00:02.500720
elapsed time: 0:03:39.824200
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 21:26:35.477928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.13
 ---- batch: 020 ----
mean loss: 250.76
 ---- batch: 030 ----
mean loss: 262.11
 ---- batch: 040 ----
mean loss: 253.08
 ---- batch: 050 ----
mean loss: 255.64
 ---- batch: 060 ----
mean loss: 244.23
 ---- batch: 070 ----
mean loss: 245.84
 ---- batch: 080 ----
mean loss: 244.74
 ---- batch: 090 ----
mean loss: 249.81
 ---- batch: 100 ----
mean loss: 245.48
 ---- batch: 110 ----
mean loss: 248.57
train mean loss: 249.91
epoch train time: 0:00:02.480740
elapsed time: 0:03:42.305340
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 21:26:37.959070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.11
 ---- batch: 020 ----
mean loss: 245.61
 ---- batch: 030 ----
mean loss: 250.64
 ---- batch: 040 ----
mean loss: 249.32
 ---- batch: 050 ----
mean loss: 245.80
 ---- batch: 060 ----
mean loss: 248.55
 ---- batch: 070 ----
mean loss: 245.53
 ---- batch: 080 ----
mean loss: 244.60
 ---- batch: 090 ----
mean loss: 247.29
 ---- batch: 100 ----
mean loss: 244.44
 ---- batch: 110 ----
mean loss: 248.29
train mean loss: 246.58
epoch train time: 0:00:02.498783
elapsed time: 0:03:44.804526
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 21:26:40.458288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.23
 ---- batch: 020 ----
mean loss: 252.08
 ---- batch: 030 ----
mean loss: 243.35
 ---- batch: 040 ----
mean loss: 246.86
 ---- batch: 050 ----
mean loss: 249.30
 ---- batch: 060 ----
mean loss: 247.13
 ---- batch: 070 ----
mean loss: 237.09
 ---- batch: 080 ----
mean loss: 242.66
 ---- batch: 090 ----
mean loss: 250.16
 ---- batch: 100 ----
mean loss: 241.02
 ---- batch: 110 ----
mean loss: 235.70
train mean loss: 244.32
epoch train time: 0:00:02.494038
elapsed time: 0:03:47.299046
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 21:26:42.952787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.66
 ---- batch: 020 ----
mean loss: 242.37
 ---- batch: 030 ----
mean loss: 236.11
 ---- batch: 040 ----
mean loss: 242.39
 ---- batch: 050 ----
mean loss: 247.26
 ---- batch: 060 ----
mean loss: 247.51
 ---- batch: 070 ----
mean loss: 241.88
 ---- batch: 080 ----
mean loss: 247.19
 ---- batch: 090 ----
mean loss: 238.94
 ---- batch: 100 ----
mean loss: 238.16
 ---- batch: 110 ----
mean loss: 240.11
train mean loss: 241.85
epoch train time: 0:00:02.501783
elapsed time: 0:03:49.801288
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 21:26:45.455036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.21
 ---- batch: 020 ----
mean loss: 241.85
 ---- batch: 030 ----
mean loss: 236.27
 ---- batch: 040 ----
mean loss: 244.65
 ---- batch: 050 ----
mean loss: 238.94
 ---- batch: 060 ----
mean loss: 238.88
 ---- batch: 070 ----
mean loss: 239.01
 ---- batch: 080 ----
mean loss: 236.16
 ---- batch: 090 ----
mean loss: 247.09
 ---- batch: 100 ----
mean loss: 228.82
 ---- batch: 110 ----
mean loss: 248.61
train mean loss: 239.65
epoch train time: 0:00:02.505778
elapsed time: 0:03:52.307501
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 21:26:47.961331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.48
 ---- batch: 020 ----
mean loss: 243.85
 ---- batch: 030 ----
mean loss: 232.16
 ---- batch: 040 ----
mean loss: 232.02
 ---- batch: 050 ----
mean loss: 243.61
 ---- batch: 060 ----
mean loss: 237.81
 ---- batch: 070 ----
mean loss: 247.22
 ---- batch: 080 ----
mean loss: 234.06
 ---- batch: 090 ----
mean loss: 241.33
 ---- batch: 100 ----
mean loss: 238.69
 ---- batch: 110 ----
mean loss: 230.60
train mean loss: 238.06
epoch train time: 0:00:02.499374
elapsed time: 0:03:54.807370
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 21:26:50.461106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.13
 ---- batch: 020 ----
mean loss: 238.39
 ---- batch: 030 ----
mean loss: 231.81
 ---- batch: 040 ----
mean loss: 235.97
 ---- batch: 050 ----
mean loss: 239.56
 ---- batch: 060 ----
mean loss: 233.65
 ---- batch: 070 ----
mean loss: 232.69
 ---- batch: 080 ----
mean loss: 248.90
 ---- batch: 090 ----
mean loss: 241.84
 ---- batch: 100 ----
mean loss: 228.93
 ---- batch: 110 ----
mean loss: 240.22
train mean loss: 236.56
epoch train time: 0:00:02.494493
elapsed time: 0:03:57.302283
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 21:26:52.956018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.27
 ---- batch: 020 ----
mean loss: 240.72
 ---- batch: 030 ----
mean loss: 241.94
 ---- batch: 040 ----
mean loss: 238.71
 ---- batch: 050 ----
mean loss: 234.39
 ---- batch: 060 ----
mean loss: 235.90
 ---- batch: 070 ----
mean loss: 235.39
 ---- batch: 080 ----
mean loss: 230.82
 ---- batch: 090 ----
mean loss: 234.70
 ---- batch: 100 ----
mean loss: 236.38
 ---- batch: 110 ----
mean loss: 233.84
train mean loss: 235.64
epoch train time: 0:00:02.513102
elapsed time: 0:03:59.815847
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 21:26:55.469596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.66
 ---- batch: 020 ----
mean loss: 240.65
 ---- batch: 030 ----
mean loss: 233.05
 ---- batch: 040 ----
mean loss: 227.60
 ---- batch: 050 ----
mean loss: 238.63
 ---- batch: 060 ----
mean loss: 233.61
 ---- batch: 070 ----
mean loss: 224.74
 ---- batch: 080 ----
mean loss: 230.43
 ---- batch: 090 ----
mean loss: 235.86
 ---- batch: 100 ----
mean loss: 237.08
 ---- batch: 110 ----
mean loss: 235.19
train mean loss: 234.19
epoch train time: 0:00:02.489230
elapsed time: 0:04:02.305510
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 21:26:57.959267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.70
 ---- batch: 020 ----
mean loss: 238.25
 ---- batch: 030 ----
mean loss: 232.20
 ---- batch: 040 ----
mean loss: 229.26
 ---- batch: 050 ----
mean loss: 227.25
 ---- batch: 060 ----
mean loss: 232.42
 ---- batch: 070 ----
mean loss: 239.61
 ---- batch: 080 ----
mean loss: 242.45
 ---- batch: 090 ----
mean loss: 230.05
 ---- batch: 100 ----
mean loss: 236.69
 ---- batch: 110 ----
mean loss: 227.40
train mean loss: 232.78
epoch train time: 0:00:02.489822
elapsed time: 0:04:04.795745
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 21:27:00.449466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.82
 ---- batch: 020 ----
mean loss: 234.46
 ---- batch: 030 ----
mean loss: 223.60
 ---- batch: 040 ----
mean loss: 237.96
 ---- batch: 050 ----
mean loss: 237.20
 ---- batch: 060 ----
mean loss: 227.79
 ---- batch: 070 ----
mean loss: 233.63
 ---- batch: 080 ----
mean loss: 238.13
 ---- batch: 090 ----
mean loss: 234.83
 ---- batch: 100 ----
mean loss: 231.38
 ---- batch: 110 ----
mean loss: 228.43
train mean loss: 231.76
epoch train time: 0:00:02.480449
elapsed time: 0:04:07.276636
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 21:27:02.930372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.19
 ---- batch: 020 ----
mean loss: 239.49
 ---- batch: 030 ----
mean loss: 227.48
 ---- batch: 040 ----
mean loss: 230.48
 ---- batch: 050 ----
mean loss: 231.69
 ---- batch: 060 ----
mean loss: 228.19
 ---- batch: 070 ----
mean loss: 222.79
 ---- batch: 080 ----
mean loss: 227.30
 ---- batch: 090 ----
mean loss: 233.11
 ---- batch: 100 ----
mean loss: 232.53
 ---- batch: 110 ----
mean loss: 234.91
train mean loss: 230.55
epoch train time: 0:00:02.510454
elapsed time: 0:04:09.787507
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 21:27:05.441243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.20
 ---- batch: 020 ----
mean loss: 232.08
 ---- batch: 030 ----
mean loss: 230.11
 ---- batch: 040 ----
mean loss: 221.25
 ---- batch: 050 ----
mean loss: 228.41
 ---- batch: 060 ----
mean loss: 226.65
 ---- batch: 070 ----
mean loss: 229.21
 ---- batch: 080 ----
mean loss: 238.35
 ---- batch: 090 ----
mean loss: 231.98
 ---- batch: 100 ----
mean loss: 222.00
 ---- batch: 110 ----
mean loss: 228.71
train mean loss: 229.42
epoch train time: 0:00:02.511893
elapsed time: 0:04:12.299804
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 21:27:07.953575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.62
 ---- batch: 020 ----
mean loss: 224.59
 ---- batch: 030 ----
mean loss: 234.25
 ---- batch: 040 ----
mean loss: 233.85
 ---- batch: 050 ----
mean loss: 233.19
 ---- batch: 060 ----
mean loss: 224.24
 ---- batch: 070 ----
mean loss: 227.70
 ---- batch: 080 ----
mean loss: 233.32
 ---- batch: 090 ----
mean loss: 231.84
 ---- batch: 100 ----
mean loss: 231.82
 ---- batch: 110 ----
mean loss: 227.41
train mean loss: 228.67
epoch train time: 0:00:02.494418
elapsed time: 0:04:14.794728
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 21:27:10.448530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.27
 ---- batch: 020 ----
mean loss: 229.00
 ---- batch: 030 ----
mean loss: 231.05
 ---- batch: 040 ----
mean loss: 233.59
 ---- batch: 050 ----
mean loss: 219.80
 ---- batch: 060 ----
mean loss: 223.50
 ---- batch: 070 ----
mean loss: 229.45
 ---- batch: 080 ----
mean loss: 227.95
 ---- batch: 090 ----
mean loss: 228.39
 ---- batch: 100 ----
mean loss: 226.41
 ---- batch: 110 ----
mean loss: 221.40
train mean loss: 227.97
epoch train time: 0:00:02.502425
elapsed time: 0:04:17.297660
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 21:27:12.951436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.71
 ---- batch: 020 ----
mean loss: 225.98
 ---- batch: 030 ----
mean loss: 219.41
 ---- batch: 040 ----
mean loss: 228.27
 ---- batch: 050 ----
mean loss: 222.15
 ---- batch: 060 ----
mean loss: 233.66
 ---- batch: 070 ----
mean loss: 232.54
 ---- batch: 080 ----
mean loss: 230.94
 ---- batch: 090 ----
mean loss: 225.30
 ---- batch: 100 ----
mean loss: 226.62
 ---- batch: 110 ----
mean loss: 229.42
train mean loss: 227.07
epoch train time: 0:00:02.481545
elapsed time: 0:04:19.779653
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 21:27:15.433389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.53
 ---- batch: 020 ----
mean loss: 234.49
 ---- batch: 030 ----
mean loss: 233.02
 ---- batch: 040 ----
mean loss: 221.82
 ---- batch: 050 ----
mean loss: 223.89
 ---- batch: 060 ----
mean loss: 227.44
 ---- batch: 070 ----
mean loss: 226.03
 ---- batch: 080 ----
mean loss: 220.80
 ---- batch: 090 ----
mean loss: 222.70
 ---- batch: 100 ----
mean loss: 216.45
 ---- batch: 110 ----
mean loss: 233.69
train mean loss: 226.21
epoch train time: 0:00:02.504624
elapsed time: 0:04:22.284722
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 21:27:17.938477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.64
 ---- batch: 020 ----
mean loss: 226.68
 ---- batch: 030 ----
mean loss: 226.28
 ---- batch: 040 ----
mean loss: 235.89
 ---- batch: 050 ----
mean loss: 219.22
 ---- batch: 060 ----
mean loss: 222.73
 ---- batch: 070 ----
mean loss: 227.89
 ---- batch: 080 ----
mean loss: 223.34
 ---- batch: 090 ----
mean loss: 221.24
 ---- batch: 100 ----
mean loss: 221.94
 ---- batch: 110 ----
mean loss: 227.40
train mean loss: 225.42
epoch train time: 0:00:02.540698
elapsed time: 0:04:24.825844
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 21:27:20.479615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.29
 ---- batch: 020 ----
mean loss: 228.50
 ---- batch: 030 ----
mean loss: 227.35
 ---- batch: 040 ----
mean loss: 230.22
 ---- batch: 050 ----
mean loss: 221.43
 ---- batch: 060 ----
mean loss: 215.35
 ---- batch: 070 ----
mean loss: 227.75
 ---- batch: 080 ----
mean loss: 217.39
 ---- batch: 090 ----
mean loss: 231.50
 ---- batch: 100 ----
mean loss: 225.79
 ---- batch: 110 ----
mean loss: 220.48
train mean loss: 224.80
epoch train time: 0:00:02.569620
elapsed time: 0:04:27.395943
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 21:27:23.049735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.37
 ---- batch: 020 ----
mean loss: 232.22
 ---- batch: 030 ----
mean loss: 219.82
 ---- batch: 040 ----
mean loss: 222.49
 ---- batch: 050 ----
mean loss: 219.93
 ---- batch: 060 ----
mean loss: 226.37
 ---- batch: 070 ----
mean loss: 226.15
 ---- batch: 080 ----
mean loss: 231.24
 ---- batch: 090 ----
mean loss: 218.42
 ---- batch: 100 ----
mean loss: 221.43
 ---- batch: 110 ----
mean loss: 220.99
train mean loss: 223.65
epoch train time: 0:00:02.536147
elapsed time: 0:04:29.932553
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 21:27:25.586277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.17
 ---- batch: 020 ----
mean loss: 226.51
 ---- batch: 030 ----
mean loss: 226.82
 ---- batch: 040 ----
mean loss: 219.95
 ---- batch: 050 ----
mean loss: 224.69
 ---- batch: 060 ----
mean loss: 223.12
 ---- batch: 070 ----
mean loss: 229.98
 ---- batch: 080 ----
mean loss: 221.36
 ---- batch: 090 ----
mean loss: 223.32
 ---- batch: 100 ----
mean loss: 224.63
 ---- batch: 110 ----
mean loss: 217.63
train mean loss: 223.21
epoch train time: 0:00:02.557137
elapsed time: 0:04:32.490098
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 21:27:28.143835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.93
 ---- batch: 020 ----
mean loss: 220.89
 ---- batch: 030 ----
mean loss: 225.09
 ---- batch: 040 ----
mean loss: 227.71
 ---- batch: 050 ----
mean loss: 231.03
 ---- batch: 060 ----
mean loss: 219.37
 ---- batch: 070 ----
mean loss: 215.39
 ---- batch: 080 ----
mean loss: 216.75
 ---- batch: 090 ----
mean loss: 225.01
 ---- batch: 100 ----
mean loss: 223.32
 ---- batch: 110 ----
mean loss: 220.83
train mean loss: 222.79
epoch train time: 0:00:02.490115
elapsed time: 0:04:34.980633
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 21:27:30.634382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.91
 ---- batch: 020 ----
mean loss: 215.91
 ---- batch: 030 ----
mean loss: 205.35
 ---- batch: 040 ----
mean loss: 226.35
 ---- batch: 050 ----
mean loss: 234.37
 ---- batch: 060 ----
mean loss: 226.43
 ---- batch: 070 ----
mean loss: 221.70
 ---- batch: 080 ----
mean loss: 220.01
 ---- batch: 090 ----
mean loss: 221.05
 ---- batch: 100 ----
mean loss: 220.60
 ---- batch: 110 ----
mean loss: 228.35
train mean loss: 221.82
epoch train time: 0:00:02.501712
elapsed time: 0:04:37.482768
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 21:27:33.136538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.37
 ---- batch: 020 ----
mean loss: 216.65
 ---- batch: 030 ----
mean loss: 220.36
 ---- batch: 040 ----
mean loss: 220.15
 ---- batch: 050 ----
mean loss: 231.98
 ---- batch: 060 ----
mean loss: 217.47
 ---- batch: 070 ----
mean loss: 220.41
 ---- batch: 080 ----
mean loss: 228.16
 ---- batch: 090 ----
mean loss: 225.16
 ---- batch: 100 ----
mean loss: 221.00
 ---- batch: 110 ----
mean loss: 220.09
train mean loss: 221.29
epoch train time: 0:00:02.505984
elapsed time: 0:04:39.989244
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 21:27:35.642862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.96
 ---- batch: 020 ----
mean loss: 221.32
 ---- batch: 030 ----
mean loss: 222.05
 ---- batch: 040 ----
mean loss: 212.08
 ---- batch: 050 ----
mean loss: 219.24
 ---- batch: 060 ----
mean loss: 225.03
 ---- batch: 070 ----
mean loss: 220.21
 ---- batch: 080 ----
mean loss: 231.53
 ---- batch: 090 ----
mean loss: 221.82
 ---- batch: 100 ----
mean loss: 220.03
 ---- batch: 110 ----
mean loss: 217.58
train mean loss: 220.49
epoch train time: 0:00:02.476777
elapsed time: 0:04:42.466314
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 21:27:38.120056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.11
 ---- batch: 020 ----
mean loss: 226.84
 ---- batch: 030 ----
mean loss: 212.16
 ---- batch: 040 ----
mean loss: 225.98
 ---- batch: 050 ----
mean loss: 214.67
 ---- batch: 060 ----
mean loss: 225.52
 ---- batch: 070 ----
mean loss: 213.36
 ---- batch: 080 ----
mean loss: 209.83
 ---- batch: 090 ----
mean loss: 217.38
 ---- batch: 100 ----
mean loss: 222.50
 ---- batch: 110 ----
mean loss: 226.74
train mean loss: 220.15
epoch train time: 0:00:02.501341
elapsed time: 0:04:44.968118
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 21:27:40.621856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.84
 ---- batch: 020 ----
mean loss: 220.47
 ---- batch: 030 ----
mean loss: 227.38
 ---- batch: 040 ----
mean loss: 217.54
 ---- batch: 050 ----
mean loss: 219.82
 ---- batch: 060 ----
mean loss: 223.34
 ---- batch: 070 ----
mean loss: 218.58
 ---- batch: 080 ----
mean loss: 217.67
 ---- batch: 090 ----
mean loss: 214.09
 ---- batch: 100 ----
mean loss: 226.42
 ---- batch: 110 ----
mean loss: 217.11
train mean loss: 219.32
epoch train time: 0:00:02.507276
elapsed time: 0:04:47.475825
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 21:27:43.129571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.37
 ---- batch: 020 ----
mean loss: 218.01
 ---- batch: 030 ----
mean loss: 221.42
 ---- batch: 040 ----
mean loss: 212.96
 ---- batch: 050 ----
mean loss: 224.86
 ---- batch: 060 ----
mean loss: 213.25
 ---- batch: 070 ----
mean loss: 216.89
 ---- batch: 080 ----
mean loss: 221.93
 ---- batch: 090 ----
mean loss: 220.06
 ---- batch: 100 ----
mean loss: 212.48
 ---- batch: 110 ----
mean loss: 219.04
train mean loss: 218.64
epoch train time: 0:00:02.500698
elapsed time: 0:04:49.976931
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 21:27:45.630686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.77
 ---- batch: 020 ----
mean loss: 217.88
 ---- batch: 030 ----
mean loss: 224.45
 ---- batch: 040 ----
mean loss: 217.59
 ---- batch: 050 ----
mean loss: 216.50
 ---- batch: 060 ----
mean loss: 221.51
 ---- batch: 070 ----
mean loss: 212.71
 ---- batch: 080 ----
mean loss: 223.20
 ---- batch: 090 ----
mean loss: 212.01
 ---- batch: 100 ----
mean loss: 214.07
 ---- batch: 110 ----
mean loss: 214.31
train mean loss: 218.07
epoch train time: 0:00:02.491459
elapsed time: 0:04:52.468825
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 21:27:48.122589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.32
 ---- batch: 020 ----
mean loss: 211.39
 ---- batch: 030 ----
mean loss: 221.35
 ---- batch: 040 ----
mean loss: 214.24
 ---- batch: 050 ----
mean loss: 211.09
 ---- batch: 060 ----
mean loss: 220.74
 ---- batch: 070 ----
mean loss: 214.83
 ---- batch: 080 ----
mean loss: 215.43
 ---- batch: 090 ----
mean loss: 212.34
 ---- batch: 100 ----
mean loss: 221.22
 ---- batch: 110 ----
mean loss: 224.26
train mean loss: 217.60
epoch train time: 0:00:02.488647
elapsed time: 0:04:54.957920
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 21:27:50.611647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.55
 ---- batch: 020 ----
mean loss: 218.80
 ---- batch: 030 ----
mean loss: 216.41
 ---- batch: 040 ----
mean loss: 214.62
 ---- batch: 050 ----
mean loss: 215.11
 ---- batch: 060 ----
mean loss: 212.22
 ---- batch: 070 ----
mean loss: 222.40
 ---- batch: 080 ----
mean loss: 211.82
 ---- batch: 090 ----
mean loss: 217.22
 ---- batch: 100 ----
mean loss: 221.79
 ---- batch: 110 ----
mean loss: 228.77
train mean loss: 217.19
epoch train time: 0:00:02.497163
elapsed time: 0:04:57.455483
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 21:27:53.109223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.92
 ---- batch: 020 ----
mean loss: 212.21
 ---- batch: 030 ----
mean loss: 216.94
 ---- batch: 040 ----
mean loss: 211.71
 ---- batch: 050 ----
mean loss: 211.01
 ---- batch: 060 ----
mean loss: 223.03
 ---- batch: 070 ----
mean loss: 216.51
 ---- batch: 080 ----
mean loss: 217.00
 ---- batch: 090 ----
mean loss: 216.07
 ---- batch: 100 ----
mean loss: 223.01
 ---- batch: 110 ----
mean loss: 213.79
train mean loss: 216.46
epoch train time: 0:00:02.496753
elapsed time: 0:04:59.952766
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 21:27:55.606412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.69
 ---- batch: 020 ----
mean loss: 226.60
 ---- batch: 030 ----
mean loss: 211.16
 ---- batch: 040 ----
mean loss: 218.28
 ---- batch: 050 ----
mean loss: 219.67
 ---- batch: 060 ----
mean loss: 214.31
 ---- batch: 070 ----
mean loss: 210.55
 ---- batch: 080 ----
mean loss: 223.37
 ---- batch: 090 ----
mean loss: 218.23
 ---- batch: 100 ----
mean loss: 206.49
 ---- batch: 110 ----
mean loss: 218.64
train mean loss: 216.10
epoch train time: 0:00:02.498791
elapsed time: 0:05:02.451890
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 21:27:58.105664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.01
 ---- batch: 020 ----
mean loss: 217.37
 ---- batch: 030 ----
mean loss: 221.04
 ---- batch: 040 ----
mean loss: 210.09
 ---- batch: 050 ----
mean loss: 223.88
 ---- batch: 060 ----
mean loss: 207.60
 ---- batch: 070 ----
mean loss: 224.25
 ---- batch: 080 ----
mean loss: 215.62
 ---- batch: 090 ----
mean loss: 220.34
 ---- batch: 100 ----
mean loss: 201.17
 ---- batch: 110 ----
mean loss: 210.58
train mean loss: 215.43
epoch train time: 0:00:02.509195
elapsed time: 0:05:04.961573
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 21:28:00.615322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.66
 ---- batch: 020 ----
mean loss: 220.50
 ---- batch: 030 ----
mean loss: 204.33
 ---- batch: 040 ----
mean loss: 210.20
 ---- batch: 050 ----
mean loss: 216.81
 ---- batch: 060 ----
mean loss: 214.45
 ---- batch: 070 ----
mean loss: 218.12
 ---- batch: 080 ----
mean loss: 215.25
 ---- batch: 090 ----
mean loss: 220.55
 ---- batch: 100 ----
mean loss: 219.06
 ---- batch: 110 ----
mean loss: 214.76
train mean loss: 214.90
epoch train time: 0:00:02.501802
elapsed time: 0:05:07.463783
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 21:28:03.117547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.33
 ---- batch: 020 ----
mean loss: 203.67
 ---- batch: 030 ----
mean loss: 220.29
 ---- batch: 040 ----
mean loss: 207.37
 ---- batch: 050 ----
mean loss: 219.30
 ---- batch: 060 ----
mean loss: 211.06
 ---- batch: 070 ----
mean loss: 222.55
 ---- batch: 080 ----
mean loss: 214.45
 ---- batch: 090 ----
mean loss: 211.42
 ---- batch: 100 ----
mean loss: 217.73
 ---- batch: 110 ----
mean loss: 218.45
train mean loss: 214.52
epoch train time: 0:00:02.504131
elapsed time: 0:05:09.968387
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 21:28:05.622148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.02
 ---- batch: 020 ----
mean loss: 213.52
 ---- batch: 030 ----
mean loss: 208.56
 ---- batch: 040 ----
mean loss: 204.24
 ---- batch: 050 ----
mean loss: 223.74
 ---- batch: 060 ----
mean loss: 212.99
 ---- batch: 070 ----
mean loss: 214.71
 ---- batch: 080 ----
mean loss: 222.55
 ---- batch: 090 ----
mean loss: 208.21
 ---- batch: 100 ----
mean loss: 207.88
 ---- batch: 110 ----
mean loss: 218.13
train mean loss: 213.78
epoch train time: 0:00:02.488594
elapsed time: 0:05:12.457440
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 21:28:08.111189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.18
 ---- batch: 020 ----
mean loss: 213.72
 ---- batch: 030 ----
mean loss: 215.89
 ---- batch: 040 ----
mean loss: 214.17
 ---- batch: 050 ----
mean loss: 204.19
 ---- batch: 060 ----
mean loss: 215.92
 ---- batch: 070 ----
mean loss: 212.98
 ---- batch: 080 ----
mean loss: 217.53
 ---- batch: 090 ----
mean loss: 221.33
 ---- batch: 100 ----
mean loss: 204.98
 ---- batch: 110 ----
mean loss: 212.91
train mean loss: 213.47
epoch train time: 0:00:02.504499
elapsed time: 0:05:14.962358
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 21:28:10.616104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.84
 ---- batch: 020 ----
mean loss: 202.21
 ---- batch: 030 ----
mean loss: 213.44
 ---- batch: 040 ----
mean loss: 202.71
 ---- batch: 050 ----
mean loss: 215.09
 ---- batch: 060 ----
mean loss: 215.49
 ---- batch: 070 ----
mean loss: 215.64
 ---- batch: 080 ----
mean loss: 218.47
 ---- batch: 090 ----
mean loss: 212.80
 ---- batch: 100 ----
mean loss: 215.18
 ---- batch: 110 ----
mean loss: 216.61
train mean loss: 212.97
epoch train time: 0:00:02.486542
elapsed time: 0:05:17.449362
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 21:28:13.103150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.47
 ---- batch: 020 ----
mean loss: 221.35
 ---- batch: 030 ----
mean loss: 210.98
 ---- batch: 040 ----
mean loss: 212.73
 ---- batch: 050 ----
mean loss: 220.89
 ---- batch: 060 ----
mean loss: 227.24
 ---- batch: 070 ----
mean loss: 215.05
 ---- batch: 080 ----
mean loss: 205.43
 ---- batch: 090 ----
mean loss: 209.87
 ---- batch: 100 ----
mean loss: 212.89
 ---- batch: 110 ----
mean loss: 204.18
train mean loss: 213.05
epoch train time: 0:00:02.494084
elapsed time: 0:05:19.943899
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 21:28:15.597659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.05
 ---- batch: 020 ----
mean loss: 219.41
 ---- batch: 030 ----
mean loss: 215.72
 ---- batch: 040 ----
mean loss: 199.58
 ---- batch: 050 ----
mean loss: 214.39
 ---- batch: 060 ----
mean loss: 212.08
 ---- batch: 070 ----
mean loss: 209.91
 ---- batch: 080 ----
mean loss: 218.21
 ---- batch: 090 ----
mean loss: 213.95
 ---- batch: 100 ----
mean loss: 202.93
 ---- batch: 110 ----
mean loss: 206.36
train mean loss: 212.00
epoch train time: 0:00:02.502690
elapsed time: 0:05:22.447013
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 21:28:18.100777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.28
 ---- batch: 020 ----
mean loss: 211.34
 ---- batch: 030 ----
mean loss: 207.49
 ---- batch: 040 ----
mean loss: 210.97
 ---- batch: 050 ----
mean loss: 210.14
 ---- batch: 060 ----
mean loss: 212.88
 ---- batch: 070 ----
mean loss: 215.82
 ---- batch: 080 ----
mean loss: 214.73
 ---- batch: 090 ----
mean loss: 210.82
 ---- batch: 100 ----
mean loss: 217.27
 ---- batch: 110 ----
mean loss: 208.04
train mean loss: 211.71
epoch train time: 0:00:02.479620
elapsed time: 0:05:24.927086
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 21:28:20.580823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.57
 ---- batch: 020 ----
mean loss: 215.28
 ---- batch: 030 ----
mean loss: 214.27
 ---- batch: 040 ----
mean loss: 208.24
 ---- batch: 050 ----
mean loss: 205.02
 ---- batch: 060 ----
mean loss: 209.36
 ---- batch: 070 ----
mean loss: 208.46
 ---- batch: 080 ----
mean loss: 222.87
 ---- batch: 090 ----
mean loss: 209.09
 ---- batch: 100 ----
mean loss: 212.02
 ---- batch: 110 ----
mean loss: 204.45
train mean loss: 211.27
epoch train time: 0:00:02.507379
elapsed time: 0:05:27.434972
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 21:28:23.088754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.79
 ---- batch: 020 ----
mean loss: 209.73
 ---- batch: 030 ----
mean loss: 212.09
 ---- batch: 040 ----
mean loss: 207.92
 ---- batch: 050 ----
mean loss: 208.68
 ---- batch: 060 ----
mean loss: 207.07
 ---- batch: 070 ----
mean loss: 209.39
 ---- batch: 080 ----
mean loss: 215.57
 ---- batch: 090 ----
mean loss: 216.35
 ---- batch: 100 ----
mean loss: 209.35
 ---- batch: 110 ----
mean loss: 210.40
train mean loss: 210.47
epoch train time: 0:00:02.501312
elapsed time: 0:05:29.936778
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 21:28:25.590599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.56
 ---- batch: 020 ----
mean loss: 205.02
 ---- batch: 030 ----
mean loss: 207.08
 ---- batch: 040 ----
mean loss: 213.04
 ---- batch: 050 ----
mean loss: 211.42
 ---- batch: 060 ----
mean loss: 220.49
 ---- batch: 070 ----
mean loss: 207.15
 ---- batch: 080 ----
mean loss: 207.92
 ---- batch: 090 ----
mean loss: 219.98
 ---- batch: 100 ----
mean loss: 205.20
 ---- batch: 110 ----
mean loss: 212.64
train mean loss: 210.44
epoch train time: 0:00:02.505032
elapsed time: 0:05:32.442305
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 21:28:28.096029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.14
 ---- batch: 020 ----
mean loss: 196.31
 ---- batch: 030 ----
mean loss: 215.28
 ---- batch: 040 ----
mean loss: 208.74
 ---- batch: 050 ----
mean loss: 204.31
 ---- batch: 060 ----
mean loss: 209.22
 ---- batch: 070 ----
mean loss: 217.54
 ---- batch: 080 ----
mean loss: 199.86
 ---- batch: 090 ----
mean loss: 220.80
 ---- batch: 100 ----
mean loss: 208.47
 ---- batch: 110 ----
mean loss: 211.96
train mean loss: 209.89
epoch train time: 0:00:02.501001
elapsed time: 0:05:34.943703
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 21:28:30.597459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.86
 ---- batch: 020 ----
mean loss: 207.64
 ---- batch: 030 ----
mean loss: 207.31
 ---- batch: 040 ----
mean loss: 214.04
 ---- batch: 050 ----
mean loss: 202.95
 ---- batch: 060 ----
mean loss: 208.65
 ---- batch: 070 ----
mean loss: 207.34
 ---- batch: 080 ----
mean loss: 210.38
 ---- batch: 090 ----
mean loss: 208.32
 ---- batch: 100 ----
mean loss: 215.38
 ---- batch: 110 ----
mean loss: 210.23
train mean loss: 209.25
epoch train time: 0:00:02.506297
elapsed time: 0:05:37.450443
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 21:28:33.104205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.40
 ---- batch: 020 ----
mean loss: 214.64
 ---- batch: 030 ----
mean loss: 205.70
 ---- batch: 040 ----
mean loss: 210.41
 ---- batch: 050 ----
mean loss: 214.77
 ---- batch: 060 ----
mean loss: 209.77
 ---- batch: 070 ----
mean loss: 213.40
 ---- batch: 080 ----
mean loss: 211.83
 ---- batch: 090 ----
mean loss: 198.91
 ---- batch: 100 ----
mean loss: 213.79
 ---- batch: 110 ----
mean loss: 197.25
train mean loss: 208.92
epoch train time: 0:00:02.519651
elapsed time: 0:05:39.970557
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 21:28:35.624307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.41
 ---- batch: 020 ----
mean loss: 198.35
 ---- batch: 030 ----
mean loss: 208.12
 ---- batch: 040 ----
mean loss: 210.98
 ---- batch: 050 ----
mean loss: 207.52
 ---- batch: 060 ----
mean loss: 214.05
 ---- batch: 070 ----
mean loss: 208.08
 ---- batch: 080 ----
mean loss: 210.72
 ---- batch: 090 ----
mean loss: 215.39
 ---- batch: 100 ----
mean loss: 207.75
 ---- batch: 110 ----
mean loss: 199.39
train mean loss: 208.62
epoch train time: 0:00:02.499918
elapsed time: 0:05:42.470914
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 21:28:38.124655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.79
 ---- batch: 020 ----
mean loss: 203.80
 ---- batch: 030 ----
mean loss: 207.46
 ---- batch: 040 ----
mean loss: 214.42
 ---- batch: 050 ----
mean loss: 219.01
 ---- batch: 060 ----
mean loss: 210.82
 ---- batch: 070 ----
mean loss: 211.41
 ---- batch: 080 ----
mean loss: 206.00
 ---- batch: 090 ----
mean loss: 207.13
 ---- batch: 100 ----
mean loss: 197.63
 ---- batch: 110 ----
mean loss: 203.91
train mean loss: 208.24
epoch train time: 0:00:02.485552
elapsed time: 0:05:44.956875
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 21:28:40.610606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.82
 ---- batch: 020 ----
mean loss: 203.97
 ---- batch: 030 ----
mean loss: 208.46
 ---- batch: 040 ----
mean loss: 215.32
 ---- batch: 050 ----
mean loss: 200.71
 ---- batch: 060 ----
mean loss: 209.60
 ---- batch: 070 ----
mean loss: 213.58
 ---- batch: 080 ----
mean loss: 213.44
 ---- batch: 090 ----
mean loss: 203.16
 ---- batch: 100 ----
mean loss: 198.95
 ---- batch: 110 ----
mean loss: 211.65
train mean loss: 207.78
epoch train time: 0:00:02.485882
elapsed time: 0:05:47.443160
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 21:28:43.096905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.07
 ---- batch: 020 ----
mean loss: 201.22
 ---- batch: 030 ----
mean loss: 201.71
 ---- batch: 040 ----
mean loss: 210.58
 ---- batch: 050 ----
mean loss: 206.47
 ---- batch: 060 ----
mean loss: 203.87
 ---- batch: 070 ----
mean loss: 207.13
 ---- batch: 080 ----
mean loss: 215.30
 ---- batch: 090 ----
mean loss: 210.30
 ---- batch: 100 ----
mean loss: 207.87
 ---- batch: 110 ----
mean loss: 205.65
train mean loss: 207.48
epoch train time: 0:00:02.483375
elapsed time: 0:05:49.927066
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 21:28:45.580733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.74
 ---- batch: 020 ----
mean loss: 205.79
 ---- batch: 030 ----
mean loss: 206.58
 ---- batch: 040 ----
mean loss: 192.22
 ---- batch: 050 ----
mean loss: 219.29
 ---- batch: 060 ----
mean loss: 211.08
 ---- batch: 070 ----
mean loss: 204.03
 ---- batch: 080 ----
mean loss: 208.48
 ---- batch: 090 ----
mean loss: 209.31
 ---- batch: 100 ----
mean loss: 205.88
 ---- batch: 110 ----
mean loss: 205.67
train mean loss: 206.93
epoch train time: 0:00:02.520636
elapsed time: 0:05:52.448046
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 21:28:48.101829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.77
 ---- batch: 020 ----
mean loss: 199.04
 ---- batch: 030 ----
mean loss: 208.15
 ---- batch: 040 ----
mean loss: 206.00
 ---- batch: 050 ----
mean loss: 215.89
 ---- batch: 060 ----
mean loss: 206.13
 ---- batch: 070 ----
mean loss: 208.07
 ---- batch: 080 ----
mean loss: 208.05
 ---- batch: 090 ----
mean loss: 198.73
 ---- batch: 100 ----
mean loss: 212.93
 ---- batch: 110 ----
mean loss: 198.30
train mean loss: 206.54
epoch train time: 0:00:02.504363
elapsed time: 0:05:54.953045
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 21:28:50.606865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.10
 ---- batch: 020 ----
mean loss: 214.27
 ---- batch: 030 ----
mean loss: 204.57
 ---- batch: 040 ----
mean loss: 207.55
 ---- batch: 050 ----
mean loss: 211.09
 ---- batch: 060 ----
mean loss: 208.88
 ---- batch: 070 ----
mean loss: 198.61
 ---- batch: 080 ----
mean loss: 206.57
 ---- batch: 090 ----
mean loss: 204.32
 ---- batch: 100 ----
mean loss: 204.54
 ---- batch: 110 ----
mean loss: 207.38
train mean loss: 206.31
epoch train time: 0:00:02.479902
elapsed time: 0:05:57.433487
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 21:28:53.087247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.28
 ---- batch: 020 ----
mean loss: 203.11
 ---- batch: 030 ----
mean loss: 207.14
 ---- batch: 040 ----
mean loss: 215.10
 ---- batch: 050 ----
mean loss: 199.05
 ---- batch: 060 ----
mean loss: 203.26
 ---- batch: 070 ----
mean loss: 214.58
 ---- batch: 080 ----
mean loss: 206.65
 ---- batch: 090 ----
mean loss: 208.50
 ---- batch: 100 ----
mean loss: 200.97
 ---- batch: 110 ----
mean loss: 202.53
train mean loss: 205.71
epoch train time: 0:00:02.502402
elapsed time: 0:05:59.936324
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 21:28:55.590102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.29
 ---- batch: 020 ----
mean loss: 205.13
 ---- batch: 030 ----
mean loss: 211.68
 ---- batch: 040 ----
mean loss: 205.14
 ---- batch: 050 ----
mean loss: 206.06
 ---- batch: 060 ----
mean loss: 203.00
 ---- batch: 070 ----
mean loss: 206.69
 ---- batch: 080 ----
mean loss: 205.21
 ---- batch: 090 ----
mean loss: 198.65
 ---- batch: 100 ----
mean loss: 209.77
 ---- batch: 110 ----
mean loss: 204.45
train mean loss: 205.73
epoch train time: 0:00:02.522782
elapsed time: 0:06:02.459570
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 21:28:58.113322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.57
 ---- batch: 020 ----
mean loss: 209.71
 ---- batch: 030 ----
mean loss: 208.00
 ---- batch: 040 ----
mean loss: 205.31
 ---- batch: 050 ----
mean loss: 202.56
 ---- batch: 060 ----
mean loss: 208.16
 ---- batch: 070 ----
mean loss: 212.16
 ---- batch: 080 ----
mean loss: 203.41
 ---- batch: 090 ----
mean loss: 194.27
 ---- batch: 100 ----
mean loss: 201.69
 ---- batch: 110 ----
mean loss: 205.08
train mean loss: 205.24
epoch train time: 0:00:02.510472
elapsed time: 0:06:04.970468
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 21:29:00.624212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.45
 ---- batch: 020 ----
mean loss: 204.38
 ---- batch: 030 ----
mean loss: 207.18
 ---- batch: 040 ----
mean loss: 198.07
 ---- batch: 050 ----
mean loss: 203.93
 ---- batch: 060 ----
mean loss: 198.71
 ---- batch: 070 ----
mean loss: 202.96
 ---- batch: 080 ----
mean loss: 203.66
 ---- batch: 090 ----
mean loss: 212.22
 ---- batch: 100 ----
mean loss: 193.02
 ---- batch: 110 ----
mean loss: 211.34
train mean loss: 204.85
epoch train time: 0:00:02.511495
elapsed time: 0:06:07.482401
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 21:29:03.136169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.53
 ---- batch: 020 ----
mean loss: 205.74
 ---- batch: 030 ----
mean loss: 213.36
 ---- batch: 040 ----
mean loss: 206.53
 ---- batch: 050 ----
mean loss: 204.15
 ---- batch: 060 ----
mean loss: 207.51
 ---- batch: 070 ----
mean loss: 204.54
 ---- batch: 080 ----
mean loss: 197.12
 ---- batch: 090 ----
mean loss: 202.29
 ---- batch: 100 ----
mean loss: 204.72
 ---- batch: 110 ----
mean loss: 202.62
train mean loss: 204.60
epoch train time: 0:00:02.517378
elapsed time: 0:06:10.000320
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 21:29:05.654075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.88
 ---- batch: 020 ----
mean loss: 209.32
 ---- batch: 030 ----
mean loss: 211.06
 ---- batch: 040 ----
mean loss: 211.22
 ---- batch: 050 ----
mean loss: 201.28
 ---- batch: 060 ----
mean loss: 203.82
 ---- batch: 070 ----
mean loss: 200.01
 ---- batch: 080 ----
mean loss: 199.05
 ---- batch: 090 ----
mean loss: 210.17
 ---- batch: 100 ----
mean loss: 201.11
 ---- batch: 110 ----
mean loss: 201.90
train mean loss: 204.23
epoch train time: 0:00:02.489043
elapsed time: 0:06:12.489791
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 21:29:08.143586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.31
 ---- batch: 020 ----
mean loss: 209.31
 ---- batch: 030 ----
mean loss: 205.29
 ---- batch: 040 ----
mean loss: 202.76
 ---- batch: 050 ----
mean loss: 199.12
 ---- batch: 060 ----
mean loss: 197.56
 ---- batch: 070 ----
mean loss: 207.87
 ---- batch: 080 ----
mean loss: 198.70
 ---- batch: 090 ----
mean loss: 205.35
 ---- batch: 100 ----
mean loss: 197.59
 ---- batch: 110 ----
mean loss: 198.25
train mean loss: 203.76
epoch train time: 0:00:02.504872
elapsed time: 0:06:14.995164
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 21:29:10.648913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.78
 ---- batch: 020 ----
mean loss: 206.50
 ---- batch: 030 ----
mean loss: 204.44
 ---- batch: 040 ----
mean loss: 206.50
 ---- batch: 050 ----
mean loss: 207.93
 ---- batch: 060 ----
mean loss: 206.64
 ---- batch: 070 ----
mean loss: 196.50
 ---- batch: 080 ----
mean loss: 204.41
 ---- batch: 090 ----
mean loss: 192.42
 ---- batch: 100 ----
mean loss: 210.33
 ---- batch: 110 ----
mean loss: 207.34
train mean loss: 203.45
epoch train time: 0:00:02.486626
elapsed time: 0:06:17.482218
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 21:29:13.135972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.44
 ---- batch: 020 ----
mean loss: 211.27
 ---- batch: 030 ----
mean loss: 200.86
 ---- batch: 040 ----
mean loss: 210.50
 ---- batch: 050 ----
mean loss: 194.79
 ---- batch: 060 ----
mean loss: 208.61
 ---- batch: 070 ----
mean loss: 193.44
 ---- batch: 080 ----
mean loss: 203.29
 ---- batch: 090 ----
mean loss: 195.03
 ---- batch: 100 ----
mean loss: 207.83
 ---- batch: 110 ----
mean loss: 198.49
train mean loss: 203.13
epoch train time: 0:00:02.515144
elapsed time: 0:06:19.997822
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 21:29:15.651559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.46
 ---- batch: 020 ----
mean loss: 210.23
 ---- batch: 030 ----
mean loss: 208.23
 ---- batch: 040 ----
mean loss: 207.48
 ---- batch: 050 ----
mean loss: 204.98
 ---- batch: 060 ----
mean loss: 207.02
 ---- batch: 070 ----
mean loss: 195.59
 ---- batch: 080 ----
mean loss: 198.97
 ---- batch: 090 ----
mean loss: 202.69
 ---- batch: 100 ----
mean loss: 190.43
 ---- batch: 110 ----
mean loss: 205.76
train mean loss: 202.89
epoch train time: 0:00:02.537164
elapsed time: 0:06:22.535386
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 21:29:18.189117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.62
 ---- batch: 020 ----
mean loss: 202.56
 ---- batch: 030 ----
mean loss: 212.91
 ---- batch: 040 ----
mean loss: 208.11
 ---- batch: 050 ----
mean loss: 207.68
 ---- batch: 060 ----
mean loss: 195.35
 ---- batch: 070 ----
mean loss: 205.09
 ---- batch: 080 ----
mean loss: 204.11
 ---- batch: 090 ----
mean loss: 210.55
 ---- batch: 100 ----
mean loss: 197.78
 ---- batch: 110 ----
mean loss: 195.28
train mean loss: 202.60
epoch train time: 0:00:02.508524
elapsed time: 0:06:25.044342
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 21:29:20.698095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.61
 ---- batch: 020 ----
mean loss: 209.23
 ---- batch: 030 ----
mean loss: 200.97
 ---- batch: 040 ----
mean loss: 198.09
 ---- batch: 050 ----
mean loss: 198.83
 ---- batch: 060 ----
mean loss: 206.59
 ---- batch: 070 ----
mean loss: 201.22
 ---- batch: 080 ----
mean loss: 204.42
 ---- batch: 090 ----
mean loss: 200.37
 ---- batch: 100 ----
mean loss: 200.34
 ---- batch: 110 ----
mean loss: 195.82
train mean loss: 202.13
epoch train time: 0:00:02.482378
elapsed time: 0:06:27.527265
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 21:29:23.181032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.67
 ---- batch: 020 ----
mean loss: 211.69
 ---- batch: 030 ----
mean loss: 201.59
 ---- batch: 040 ----
mean loss: 198.71
 ---- batch: 050 ----
mean loss: 197.90
 ---- batch: 060 ----
mean loss: 199.40
 ---- batch: 070 ----
mean loss: 198.99
 ---- batch: 080 ----
mean loss: 199.15
 ---- batch: 090 ----
mean loss: 195.08
 ---- batch: 100 ----
mean loss: 208.04
 ---- batch: 110 ----
mean loss: 210.52
train mean loss: 202.08
epoch train time: 0:00:02.539681
elapsed time: 0:06:30.067386
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 21:29:25.721168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.33
 ---- batch: 020 ----
mean loss: 196.98
 ---- batch: 030 ----
mean loss: 194.21
 ---- batch: 040 ----
mean loss: 200.00
 ---- batch: 050 ----
mean loss: 208.50
 ---- batch: 060 ----
mean loss: 198.39
 ---- batch: 070 ----
mean loss: 201.96
 ---- batch: 080 ----
mean loss: 211.61
 ---- batch: 090 ----
mean loss: 206.61
 ---- batch: 100 ----
mean loss: 206.16
 ---- batch: 110 ----
mean loss: 192.45
train mean loss: 201.70
epoch train time: 0:00:02.563857
elapsed time: 0:06:32.631727
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 21:29:28.285484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.81
 ---- batch: 020 ----
mean loss: 201.77
 ---- batch: 030 ----
mean loss: 202.33
 ---- batch: 040 ----
mean loss: 205.44
 ---- batch: 050 ----
mean loss: 197.19
 ---- batch: 060 ----
mean loss: 203.01
 ---- batch: 070 ----
mean loss: 197.21
 ---- batch: 080 ----
mean loss: 209.91
 ---- batch: 090 ----
mean loss: 198.88
 ---- batch: 100 ----
mean loss: 204.21
 ---- batch: 110 ----
mean loss: 197.25
train mean loss: 201.37
epoch train time: 0:00:02.532309
elapsed time: 0:06:35.164480
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 21:29:30.818212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.92
 ---- batch: 020 ----
mean loss: 205.67
 ---- batch: 030 ----
mean loss: 210.87
 ---- batch: 040 ----
mean loss: 199.48
 ---- batch: 050 ----
mean loss: 210.25
 ---- batch: 060 ----
mean loss: 204.76
 ---- batch: 070 ----
mean loss: 198.23
 ---- batch: 080 ----
mean loss: 200.97
 ---- batch: 090 ----
mean loss: 198.44
 ---- batch: 100 ----
mean loss: 195.99
 ---- batch: 110 ----
mean loss: 199.49
train mean loss: 201.14
epoch train time: 0:00:02.504320
elapsed time: 0:06:37.669197
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 21:29:33.322939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.69
 ---- batch: 020 ----
mean loss: 204.72
 ---- batch: 030 ----
mean loss: 195.58
 ---- batch: 040 ----
mean loss: 204.49
 ---- batch: 050 ----
mean loss: 195.53
 ---- batch: 060 ----
mean loss: 199.50
 ---- batch: 070 ----
mean loss: 204.86
 ---- batch: 080 ----
mean loss: 199.61
 ---- batch: 090 ----
mean loss: 201.70
 ---- batch: 100 ----
mean loss: 194.23
 ---- batch: 110 ----
mean loss: 203.37
train mean loss: 201.01
epoch train time: 0:00:02.480963
elapsed time: 0:06:40.150606
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 21:29:35.804356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.61
 ---- batch: 020 ----
mean loss: 204.59
 ---- batch: 030 ----
mean loss: 202.63
 ---- batch: 040 ----
mean loss: 197.19
 ---- batch: 050 ----
mean loss: 207.79
 ---- batch: 060 ----
mean loss: 198.65
 ---- batch: 070 ----
mean loss: 200.38
 ---- batch: 080 ----
mean loss: 197.50
 ---- batch: 090 ----
mean loss: 200.27
 ---- batch: 100 ----
mean loss: 208.75
 ---- batch: 110 ----
mean loss: 190.22
train mean loss: 200.57
epoch train time: 0:00:02.481606
elapsed time: 0:06:42.632628
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 21:29:38.286365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.41
 ---- batch: 020 ----
mean loss: 204.05
 ---- batch: 030 ----
mean loss: 206.74
 ---- batch: 040 ----
mean loss: 195.19
 ---- batch: 050 ----
mean loss: 199.75
 ---- batch: 060 ----
mean loss: 191.26
 ---- batch: 070 ----
mean loss: 197.89
 ---- batch: 080 ----
mean loss: 208.95
 ---- batch: 090 ----
mean loss: 205.53
 ---- batch: 100 ----
mean loss: 203.58
 ---- batch: 110 ----
mean loss: 194.51
train mean loss: 200.24
epoch train time: 0:00:02.495511
elapsed time: 0:06:45.128703
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 21:29:40.782350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.60
 ---- batch: 020 ----
mean loss: 196.48
 ---- batch: 030 ----
mean loss: 195.17
 ---- batch: 040 ----
mean loss: 188.55
 ---- batch: 050 ----
mean loss: 198.15
 ---- batch: 060 ----
mean loss: 202.26
 ---- batch: 070 ----
mean loss: 204.11
 ---- batch: 080 ----
mean loss: 206.88
 ---- batch: 090 ----
mean loss: 202.67
 ---- batch: 100 ----
mean loss: 202.63
 ---- batch: 110 ----
mean loss: 196.98
train mean loss: 200.04
epoch train time: 0:00:02.474862
elapsed time: 0:06:47.603925
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 21:29:43.257684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.25
 ---- batch: 020 ----
mean loss: 196.40
 ---- batch: 030 ----
mean loss: 205.85
 ---- batch: 040 ----
mean loss: 194.36
 ---- batch: 050 ----
mean loss: 200.91
 ---- batch: 060 ----
mean loss: 205.53
 ---- batch: 070 ----
mean loss: 194.75
 ---- batch: 080 ----
mean loss: 206.54
 ---- batch: 090 ----
mean loss: 199.23
 ---- batch: 100 ----
mean loss: 195.14
 ---- batch: 110 ----
mean loss: 196.57
train mean loss: 200.02
epoch train time: 0:00:02.477022
elapsed time: 0:06:50.081427
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 21:29:45.735194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.94
 ---- batch: 020 ----
mean loss: 193.83
 ---- batch: 030 ----
mean loss: 203.49
 ---- batch: 040 ----
mean loss: 201.15
 ---- batch: 050 ----
mean loss: 197.09
 ---- batch: 060 ----
mean loss: 202.70
 ---- batch: 070 ----
mean loss: 198.85
 ---- batch: 080 ----
mean loss: 200.42
 ---- batch: 090 ----
mean loss: 200.52
 ---- batch: 100 ----
mean loss: 206.05
 ---- batch: 110 ----
mean loss: 194.58
train mean loss: 199.74
epoch train time: 0:00:02.509785
elapsed time: 0:06:52.591647
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 21:29:48.245396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.39
 ---- batch: 020 ----
mean loss: 196.85
 ---- batch: 030 ----
mean loss: 198.97
 ---- batch: 040 ----
mean loss: 191.37
 ---- batch: 050 ----
mean loss: 201.42
 ---- batch: 060 ----
mean loss: 195.65
 ---- batch: 070 ----
mean loss: 196.14
 ---- batch: 080 ----
mean loss: 206.44
 ---- batch: 090 ----
mean loss: 206.18
 ---- batch: 100 ----
mean loss: 196.20
 ---- batch: 110 ----
mean loss: 194.00
train mean loss: 199.35
epoch train time: 0:00:02.504435
elapsed time: 0:06:55.096523
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 21:29:50.750323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.69
 ---- batch: 020 ----
mean loss: 188.85
 ---- batch: 030 ----
mean loss: 201.69
 ---- batch: 040 ----
mean loss: 192.92
 ---- batch: 050 ----
mean loss: 206.61
 ---- batch: 060 ----
mean loss: 202.38
 ---- batch: 070 ----
mean loss: 201.17
 ---- batch: 080 ----
mean loss: 198.77
 ---- batch: 090 ----
mean loss: 194.31
 ---- batch: 100 ----
mean loss: 197.05
 ---- batch: 110 ----
mean loss: 203.80
train mean loss: 199.02
epoch train time: 0:00:02.524526
elapsed time: 0:06:57.621564
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 21:29:53.275361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.84
 ---- batch: 020 ----
mean loss: 195.09
 ---- batch: 030 ----
mean loss: 200.40
 ---- batch: 040 ----
mean loss: 195.29
 ---- batch: 050 ----
mean loss: 201.72
 ---- batch: 060 ----
mean loss: 195.62
 ---- batch: 070 ----
mean loss: 200.41
 ---- batch: 080 ----
mean loss: 198.86
 ---- batch: 090 ----
mean loss: 203.54
 ---- batch: 100 ----
mean loss: 193.91
 ---- batch: 110 ----
mean loss: 205.56
train mean loss: 198.66
epoch train time: 0:00:02.496286
elapsed time: 0:07:00.118327
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 21:29:55.772075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.96
 ---- batch: 020 ----
mean loss: 195.83
 ---- batch: 030 ----
mean loss: 198.03
 ---- batch: 040 ----
mean loss: 200.84
 ---- batch: 050 ----
mean loss: 190.45
 ---- batch: 060 ----
mean loss: 195.34
 ---- batch: 070 ----
mean loss: 203.28
 ---- batch: 080 ----
mean loss: 197.73
 ---- batch: 090 ----
mean loss: 200.30
 ---- batch: 100 ----
mean loss: 199.77
 ---- batch: 110 ----
mean loss: 203.43
train mean loss: 198.67
epoch train time: 0:00:02.516102
elapsed time: 0:07:02.634849
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 21:29:58.288633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.42
 ---- batch: 020 ----
mean loss: 195.49
 ---- batch: 030 ----
mean loss: 204.28
 ---- batch: 040 ----
mean loss: 211.63
 ---- batch: 050 ----
mean loss: 194.38
 ---- batch: 060 ----
mean loss: 197.07
 ---- batch: 070 ----
mean loss: 199.25
 ---- batch: 080 ----
mean loss: 192.68
 ---- batch: 090 ----
mean loss: 193.60
 ---- batch: 100 ----
mean loss: 200.61
 ---- batch: 110 ----
mean loss: 195.28
train mean loss: 198.51
epoch train time: 0:00:02.507306
elapsed time: 0:07:05.142610
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 21:30:00.796354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.02
 ---- batch: 020 ----
mean loss: 197.33
 ---- batch: 030 ----
mean loss: 201.73
 ---- batch: 040 ----
mean loss: 195.41
 ---- batch: 050 ----
mean loss: 187.97
 ---- batch: 060 ----
mean loss: 203.52
 ---- batch: 070 ----
mean loss: 191.52
 ---- batch: 080 ----
mean loss: 199.35
 ---- batch: 090 ----
mean loss: 198.70
 ---- batch: 100 ----
mean loss: 204.99
 ---- batch: 110 ----
mean loss: 203.65
train mean loss: 198.33
epoch train time: 0:00:02.490965
elapsed time: 0:07:07.633981
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 21:30:03.287777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.21
 ---- batch: 020 ----
mean loss: 195.25
 ---- batch: 030 ----
mean loss: 202.61
 ---- batch: 040 ----
mean loss: 191.02
 ---- batch: 050 ----
mean loss: 195.78
 ---- batch: 060 ----
mean loss: 195.87
 ---- batch: 070 ----
mean loss: 197.63
 ---- batch: 080 ----
mean loss: 195.41
 ---- batch: 090 ----
mean loss: 191.50
 ---- batch: 100 ----
mean loss: 202.00
 ---- batch: 110 ----
mean loss: 206.11
train mean loss: 197.98
epoch train time: 0:00:02.499213
elapsed time: 0:07:10.133681
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 21:30:05.787410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.12
 ---- batch: 020 ----
mean loss: 195.41
 ---- batch: 030 ----
mean loss: 194.96
 ---- batch: 040 ----
mean loss: 200.65
 ---- batch: 050 ----
mean loss: 198.58
 ---- batch: 060 ----
mean loss: 193.49
 ---- batch: 070 ----
mean loss: 201.32
 ---- batch: 080 ----
mean loss: 195.18
 ---- batch: 090 ----
mean loss: 194.49
 ---- batch: 100 ----
mean loss: 207.01
 ---- batch: 110 ----
mean loss: 193.22
train mean loss: 197.84
epoch train time: 0:00:02.527504
elapsed time: 0:07:12.661602
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 21:30:08.315364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.63
 ---- batch: 020 ----
mean loss: 197.12
 ---- batch: 030 ----
mean loss: 191.44
 ---- batch: 040 ----
mean loss: 198.89
 ---- batch: 050 ----
mean loss: 214.33
 ---- batch: 060 ----
mean loss: 190.77
 ---- batch: 070 ----
mean loss: 191.19
 ---- batch: 080 ----
mean loss: 207.91
 ---- batch: 090 ----
mean loss: 201.63
 ---- batch: 100 ----
mean loss: 186.40
 ---- batch: 110 ----
mean loss: 193.47
train mean loss: 197.39
epoch train time: 0:00:02.503764
elapsed time: 0:07:15.165862
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 21:30:10.819625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.53
 ---- batch: 020 ----
mean loss: 199.63
 ---- batch: 030 ----
mean loss: 200.57
 ---- batch: 040 ----
mean loss: 199.32
 ---- batch: 050 ----
mean loss: 205.77
 ---- batch: 060 ----
mean loss: 196.05
 ---- batch: 070 ----
mean loss: 193.80
 ---- batch: 080 ----
mean loss: 193.39
 ---- batch: 090 ----
mean loss: 195.75
 ---- batch: 100 ----
mean loss: 205.61
 ---- batch: 110 ----
mean loss: 196.83
train mean loss: 197.18
epoch train time: 0:00:02.493285
elapsed time: 0:07:17.659630
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 21:30:13.313411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.77
 ---- batch: 020 ----
mean loss: 202.13
 ---- batch: 030 ----
mean loss: 201.02
 ---- batch: 040 ----
mean loss: 189.71
 ---- batch: 050 ----
mean loss: 209.44
 ---- batch: 060 ----
mean loss: 198.83
 ---- batch: 070 ----
mean loss: 188.98
 ---- batch: 080 ----
mean loss: 182.36
 ---- batch: 090 ----
mean loss: 193.85
 ---- batch: 100 ----
mean loss: 199.77
 ---- batch: 110 ----
mean loss: 204.21
train mean loss: 197.29
epoch train time: 0:00:02.514002
elapsed time: 0:07:20.174111
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 21:30:15.827872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.97
 ---- batch: 020 ----
mean loss: 199.67
 ---- batch: 030 ----
mean loss: 203.84
 ---- batch: 040 ----
mean loss: 204.95
 ---- batch: 050 ----
mean loss: 197.81
 ---- batch: 060 ----
mean loss: 193.37
 ---- batch: 070 ----
mean loss: 196.79
 ---- batch: 080 ----
mean loss: 196.67
 ---- batch: 090 ----
mean loss: 187.31
 ---- batch: 100 ----
mean loss: 188.90
 ---- batch: 110 ----
mean loss: 195.14
train mean loss: 196.79
epoch train time: 0:00:02.477651
elapsed time: 0:07:22.652253
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 21:30:18.306024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.69
 ---- batch: 020 ----
mean loss: 201.27
 ---- batch: 030 ----
mean loss: 195.44
 ---- batch: 040 ----
mean loss: 196.43
 ---- batch: 050 ----
mean loss: 200.87
 ---- batch: 060 ----
mean loss: 194.65
 ---- batch: 070 ----
mean loss: 207.83
 ---- batch: 080 ----
mean loss: 192.49
 ---- batch: 090 ----
mean loss: 189.06
 ---- batch: 100 ----
mean loss: 197.14
 ---- batch: 110 ----
mean loss: 193.85
train mean loss: 196.75
epoch train time: 0:00:02.503354
elapsed time: 0:07:25.156058
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 21:30:20.809811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.76
 ---- batch: 020 ----
mean loss: 189.07
 ---- batch: 030 ----
mean loss: 199.82
 ---- batch: 040 ----
mean loss: 194.62
 ---- batch: 050 ----
mean loss: 210.66
 ---- batch: 060 ----
mean loss: 194.99
 ---- batch: 070 ----
mean loss: 191.78
 ---- batch: 080 ----
mean loss: 201.10
 ---- batch: 090 ----
mean loss: 199.51
 ---- batch: 100 ----
mean loss: 189.22
 ---- batch: 110 ----
mean loss: 186.50
train mean loss: 196.60
epoch train time: 0:00:02.549165
elapsed time: 0:07:27.705624
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 21:30:23.359387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.87
 ---- batch: 020 ----
mean loss: 198.97
 ---- batch: 030 ----
mean loss: 191.65
 ---- batch: 040 ----
mean loss: 190.85
 ---- batch: 050 ----
mean loss: 189.32
 ---- batch: 060 ----
mean loss: 193.74
 ---- batch: 070 ----
mean loss: 185.01
 ---- batch: 080 ----
mean loss: 208.83
 ---- batch: 090 ----
mean loss: 198.98
 ---- batch: 100 ----
mean loss: 212.84
 ---- batch: 110 ----
mean loss: 192.41
train mean loss: 196.33
epoch train time: 0:00:02.497218
elapsed time: 0:07:30.203324
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 21:30:25.857116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.39
 ---- batch: 020 ----
mean loss: 193.46
 ---- batch: 030 ----
mean loss: 192.66
 ---- batch: 040 ----
mean loss: 190.54
 ---- batch: 050 ----
mean loss: 196.41
 ---- batch: 060 ----
mean loss: 193.21
 ---- batch: 070 ----
mean loss: 193.69
 ---- batch: 080 ----
mean loss: 194.24
 ---- batch: 090 ----
mean loss: 202.16
 ---- batch: 100 ----
mean loss: 202.04
 ---- batch: 110 ----
mean loss: 200.48
train mean loss: 195.98
epoch train time: 0:00:02.492367
elapsed time: 0:07:32.696226
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 21:30:28.349957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.80
 ---- batch: 020 ----
mean loss: 203.04
 ---- batch: 030 ----
mean loss: 198.29
 ---- batch: 040 ----
mean loss: 207.17
 ---- batch: 050 ----
mean loss: 207.91
 ---- batch: 060 ----
mean loss: 189.90
 ---- batch: 070 ----
mean loss: 188.09
 ---- batch: 080 ----
mean loss: 193.32
 ---- batch: 090 ----
mean loss: 189.20
 ---- batch: 100 ----
mean loss: 189.59
 ---- batch: 110 ----
mean loss: 191.00
train mean loss: 195.96
epoch train time: 0:00:02.537291
elapsed time: 0:07:35.233956
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 21:30:30.887720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.62
 ---- batch: 020 ----
mean loss: 197.73
 ---- batch: 030 ----
mean loss: 184.65
 ---- batch: 040 ----
mean loss: 203.72
 ---- batch: 050 ----
mean loss: 202.86
 ---- batch: 060 ----
mean loss: 199.59
 ---- batch: 070 ----
mean loss: 193.35
 ---- batch: 080 ----
mean loss: 200.67
 ---- batch: 090 ----
mean loss: 193.95
 ---- batch: 100 ----
mean loss: 198.35
 ---- batch: 110 ----
mean loss: 188.15
train mean loss: 195.78
epoch train time: 0:00:02.507497
elapsed time: 0:07:37.741909
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 21:30:33.395668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.57
 ---- batch: 020 ----
mean loss: 200.40
 ---- batch: 030 ----
mean loss: 198.96
 ---- batch: 040 ----
mean loss: 192.45
 ---- batch: 050 ----
mean loss: 191.67
 ---- batch: 060 ----
mean loss: 201.52
 ---- batch: 070 ----
mean loss: 190.93
 ---- batch: 080 ----
mean loss: 198.22
 ---- batch: 090 ----
mean loss: 198.46
 ---- batch: 100 ----
mean loss: 190.45
 ---- batch: 110 ----
mean loss: 190.61
train mean loss: 195.44
epoch train time: 0:00:02.509853
elapsed time: 0:07:40.252212
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 21:30:35.905952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.54
 ---- batch: 020 ----
mean loss: 205.39
 ---- batch: 030 ----
mean loss: 202.37
 ---- batch: 040 ----
mean loss: 193.73
 ---- batch: 050 ----
mean loss: 196.58
 ---- batch: 060 ----
mean loss: 189.60
 ---- batch: 070 ----
mean loss: 201.43
 ---- batch: 080 ----
mean loss: 192.63
 ---- batch: 090 ----
mean loss: 194.56
 ---- batch: 100 ----
mean loss: 192.27
 ---- batch: 110 ----
mean loss: 193.95
train mean loss: 195.23
epoch train time: 0:00:02.494082
elapsed time: 0:07:42.746767
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 21:30:38.400527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.76
 ---- batch: 020 ----
mean loss: 201.87
 ---- batch: 030 ----
mean loss: 193.55
 ---- batch: 040 ----
mean loss: 189.03
 ---- batch: 050 ----
mean loss: 194.61
 ---- batch: 060 ----
mean loss: 197.98
 ---- batch: 070 ----
mean loss: 186.87
 ---- batch: 080 ----
mean loss: 198.77
 ---- batch: 090 ----
mean loss: 201.29
 ---- batch: 100 ----
mean loss: 193.87
 ---- batch: 110 ----
mean loss: 199.75
train mean loss: 195.06
epoch train time: 0:00:02.497331
elapsed time: 0:07:45.244535
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 21:30:40.898268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.20
 ---- batch: 020 ----
mean loss: 196.06
 ---- batch: 030 ----
mean loss: 200.67
 ---- batch: 040 ----
mean loss: 197.23
 ---- batch: 050 ----
mean loss: 188.82
 ---- batch: 060 ----
mean loss: 192.15
 ---- batch: 070 ----
mean loss: 194.55
 ---- batch: 080 ----
mean loss: 192.75
 ---- batch: 090 ----
mean loss: 202.93
 ---- batch: 100 ----
mean loss: 186.48
 ---- batch: 110 ----
mean loss: 198.80
train mean loss: 194.97
epoch train time: 0:00:02.517019
elapsed time: 0:07:47.762172
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 21:30:43.415740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.80
 ---- batch: 020 ----
mean loss: 200.04
 ---- batch: 030 ----
mean loss: 193.27
 ---- batch: 040 ----
mean loss: 194.89
 ---- batch: 050 ----
mean loss: 200.63
 ---- batch: 060 ----
mean loss: 193.37
 ---- batch: 070 ----
mean loss: 192.22
 ---- batch: 080 ----
mean loss: 188.36
 ---- batch: 090 ----
mean loss: 193.24
 ---- batch: 100 ----
mean loss: 197.84
 ---- batch: 110 ----
mean loss: 193.06
train mean loss: 194.63
epoch train time: 0:00:02.504188
elapsed time: 0:07:50.266601
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 21:30:45.920382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.37
 ---- batch: 020 ----
mean loss: 196.44
 ---- batch: 030 ----
mean loss: 192.28
 ---- batch: 040 ----
mean loss: 193.47
 ---- batch: 050 ----
mean loss: 189.29
 ---- batch: 060 ----
mean loss: 197.40
 ---- batch: 070 ----
mean loss: 195.65
 ---- batch: 080 ----
mean loss: 189.91
 ---- batch: 090 ----
mean loss: 198.70
 ---- batch: 100 ----
mean loss: 195.89
 ---- batch: 110 ----
mean loss: 189.62
train mean loss: 194.67
epoch train time: 0:00:02.484887
elapsed time: 0:07:52.751949
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 21:30:48.405726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.73
 ---- batch: 020 ----
mean loss: 192.25
 ---- batch: 030 ----
mean loss: 182.44
 ---- batch: 040 ----
mean loss: 205.63
 ---- batch: 050 ----
mean loss: 199.22
 ---- batch: 060 ----
mean loss: 193.01
 ---- batch: 070 ----
mean loss: 195.77
 ---- batch: 080 ----
mean loss: 196.06
 ---- batch: 090 ----
mean loss: 191.42
 ---- batch: 100 ----
mean loss: 193.88
 ---- batch: 110 ----
mean loss: 192.42
train mean loss: 194.36
epoch train time: 0:00:02.511238
elapsed time: 0:07:55.263629
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 21:30:50.917356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.43
 ---- batch: 020 ----
mean loss: 203.36
 ---- batch: 030 ----
mean loss: 199.54
 ---- batch: 040 ----
mean loss: 184.48
 ---- batch: 050 ----
mean loss: 191.92
 ---- batch: 060 ----
mean loss: 192.28
 ---- batch: 070 ----
mean loss: 187.22
 ---- batch: 080 ----
mean loss: 199.84
 ---- batch: 090 ----
mean loss: 195.24
 ---- batch: 100 ----
mean loss: 195.16
 ---- batch: 110 ----
mean loss: 186.58
train mean loss: 194.23
epoch train time: 0:00:02.514843
elapsed time: 0:07:57.778862
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 21:30:53.432593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.07
 ---- batch: 020 ----
mean loss: 201.43
 ---- batch: 030 ----
mean loss: 195.60
 ---- batch: 040 ----
mean loss: 189.42
 ---- batch: 050 ----
mean loss: 187.72
 ---- batch: 060 ----
mean loss: 197.07
 ---- batch: 070 ----
mean loss: 192.12
 ---- batch: 080 ----
mean loss: 189.53
 ---- batch: 090 ----
mean loss: 198.44
 ---- batch: 100 ----
mean loss: 192.47
 ---- batch: 110 ----
mean loss: 198.41
train mean loss: 194.18
epoch train time: 0:00:02.534987
elapsed time: 0:08:00.314265
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 21:30:55.967998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.05
 ---- batch: 020 ----
mean loss: 189.60
 ---- batch: 030 ----
mean loss: 195.67
 ---- batch: 040 ----
mean loss: 197.17
 ---- batch: 050 ----
mean loss: 197.59
 ---- batch: 060 ----
mean loss: 186.96
 ---- batch: 070 ----
mean loss: 198.91
 ---- batch: 080 ----
mean loss: 193.12
 ---- batch: 090 ----
mean loss: 193.11
 ---- batch: 100 ----
mean loss: 189.04
 ---- batch: 110 ----
mean loss: 202.11
train mean loss: 194.07
epoch train time: 0:00:02.499770
elapsed time: 0:08:02.814442
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 21:30:58.468175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.34
 ---- batch: 020 ----
mean loss: 202.53
 ---- batch: 030 ----
mean loss: 189.30
 ---- batch: 040 ----
mean loss: 185.79
 ---- batch: 050 ----
mean loss: 201.99
 ---- batch: 060 ----
mean loss: 198.93
 ---- batch: 070 ----
mean loss: 196.61
 ---- batch: 080 ----
mean loss: 193.65
 ---- batch: 090 ----
mean loss: 184.97
 ---- batch: 100 ----
mean loss: 191.18
 ---- batch: 110 ----
mean loss: 197.99
train mean loss: 193.77
epoch train time: 0:00:02.510877
elapsed time: 0:08:05.325717
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 21:31:00.979443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.01
 ---- batch: 020 ----
mean loss: 195.11
 ---- batch: 030 ----
mean loss: 206.84
 ---- batch: 040 ----
mean loss: 184.84
 ---- batch: 050 ----
mean loss: 190.20
 ---- batch: 060 ----
mean loss: 184.75
 ---- batch: 070 ----
mean loss: 188.32
 ---- batch: 080 ----
mean loss: 195.09
 ---- batch: 090 ----
mean loss: 198.12
 ---- batch: 100 ----
mean loss: 198.34
 ---- batch: 110 ----
mean loss: 194.86
train mean loss: 193.58
epoch train time: 0:00:02.506795
elapsed time: 0:08:07.832908
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 21:31:03.486655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.76
 ---- batch: 020 ----
mean loss: 199.81
 ---- batch: 030 ----
mean loss: 193.58
 ---- batch: 040 ----
mean loss: 198.18
 ---- batch: 050 ----
mean loss: 190.25
 ---- batch: 060 ----
mean loss: 198.73
 ---- batch: 070 ----
mean loss: 193.85
 ---- batch: 080 ----
mean loss: 186.78
 ---- batch: 090 ----
mean loss: 194.36
 ---- batch: 100 ----
mean loss: 189.29
 ---- batch: 110 ----
mean loss: 185.91
train mean loss: 193.59
epoch train time: 0:00:02.508234
elapsed time: 0:08:10.341616
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 21:31:05.995374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.13
 ---- batch: 020 ----
mean loss: 198.78
 ---- batch: 030 ----
mean loss: 185.58
 ---- batch: 040 ----
mean loss: 198.26
 ---- batch: 050 ----
mean loss: 182.55
 ---- batch: 060 ----
mean loss: 192.96
 ---- batch: 070 ----
mean loss: 190.76
 ---- batch: 080 ----
mean loss: 195.40
 ---- batch: 090 ----
mean loss: 204.76
 ---- batch: 100 ----
mean loss: 195.87
 ---- batch: 110 ----
mean loss: 188.03
train mean loss: 193.13
epoch train time: 0:00:02.504001
elapsed time: 0:08:12.846034
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 21:31:08.499756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.85
 ---- batch: 020 ----
mean loss: 197.38
 ---- batch: 030 ----
mean loss: 194.68
 ---- batch: 040 ----
mean loss: 188.26
 ---- batch: 050 ----
mean loss: 200.00
 ---- batch: 060 ----
mean loss: 198.09
 ---- batch: 070 ----
mean loss: 192.10
 ---- batch: 080 ----
mean loss: 190.82
 ---- batch: 090 ----
mean loss: 187.24
 ---- batch: 100 ----
mean loss: 190.86
 ---- batch: 110 ----
mean loss: 196.92
train mean loss: 192.97
epoch train time: 0:00:02.490629
elapsed time: 0:08:15.337121
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 21:31:10.990859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.54
 ---- batch: 020 ----
mean loss: 183.71
 ---- batch: 030 ----
mean loss: 200.71
 ---- batch: 040 ----
mean loss: 189.91
 ---- batch: 050 ----
mean loss: 197.36
 ---- batch: 060 ----
mean loss: 193.93
 ---- batch: 070 ----
mean loss: 192.20
 ---- batch: 080 ----
mean loss: 189.08
 ---- batch: 090 ----
mean loss: 189.68
 ---- batch: 100 ----
mean loss: 199.04
 ---- batch: 110 ----
mean loss: 197.19
train mean loss: 193.01
epoch train time: 0:00:02.484017
elapsed time: 0:08:17.821542
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 21:31:13.475304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.00
 ---- batch: 020 ----
mean loss: 202.99
 ---- batch: 030 ----
mean loss: 197.09
 ---- batch: 040 ----
mean loss: 190.25
 ---- batch: 050 ----
mean loss: 189.92
 ---- batch: 060 ----
mean loss: 191.74
 ---- batch: 070 ----
mean loss: 194.60
 ---- batch: 080 ----
mean loss: 190.08
 ---- batch: 090 ----
mean loss: 185.87
 ---- batch: 100 ----
mean loss: 185.13
 ---- batch: 110 ----
mean loss: 193.94
train mean loss: 192.72
epoch train time: 0:00:02.520258
elapsed time: 0:08:20.342246
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 21:31:15.995991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.74
 ---- batch: 020 ----
mean loss: 188.65
 ---- batch: 030 ----
mean loss: 192.02
 ---- batch: 040 ----
mean loss: 195.55
 ---- batch: 050 ----
mean loss: 196.93
 ---- batch: 060 ----
mean loss: 203.16
 ---- batch: 070 ----
mean loss: 190.09
 ---- batch: 080 ----
mean loss: 188.03
 ---- batch: 090 ----
mean loss: 198.70
 ---- batch: 100 ----
mean loss: 191.90
 ---- batch: 110 ----
mean loss: 190.79
train mean loss: 192.70
epoch train time: 0:00:02.495533
elapsed time: 0:08:22.838211
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 21:31:18.491966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.64
 ---- batch: 020 ----
mean loss: 194.08
 ---- batch: 030 ----
mean loss: 201.39
 ---- batch: 040 ----
mean loss: 185.20
 ---- batch: 050 ----
mean loss: 193.70
 ---- batch: 060 ----
mean loss: 189.30
 ---- batch: 070 ----
mean loss: 195.84
 ---- batch: 080 ----
mean loss: 190.03
 ---- batch: 090 ----
mean loss: 195.17
 ---- batch: 100 ----
mean loss: 191.47
 ---- batch: 110 ----
mean loss: 188.15
train mean loss: 192.46
epoch train time: 0:00:02.475254
elapsed time: 0:08:25.313900
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 21:31:20.967670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.28
 ---- batch: 020 ----
mean loss: 190.90
 ---- batch: 030 ----
mean loss: 203.61
 ---- batch: 040 ----
mean loss: 192.61
 ---- batch: 050 ----
mean loss: 191.47
 ---- batch: 060 ----
mean loss: 192.20
 ---- batch: 070 ----
mean loss: 191.20
 ---- batch: 080 ----
mean loss: 199.05
 ---- batch: 090 ----
mean loss: 194.27
 ---- batch: 100 ----
mean loss: 184.05
 ---- batch: 110 ----
mean loss: 187.32
train mean loss: 192.40
epoch train time: 0:00:02.493138
elapsed time: 0:08:27.807528
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 21:31:23.461273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.52
 ---- batch: 020 ----
mean loss: 190.10
 ---- batch: 030 ----
mean loss: 202.53
 ---- batch: 040 ----
mean loss: 189.52
 ---- batch: 050 ----
mean loss: 185.79
 ---- batch: 060 ----
mean loss: 196.69
 ---- batch: 070 ----
mean loss: 201.00
 ---- batch: 080 ----
mean loss: 190.60
 ---- batch: 090 ----
mean loss: 187.35
 ---- batch: 100 ----
mean loss: 193.93
 ---- batch: 110 ----
mean loss: 195.84
train mean loss: 192.37
epoch train time: 0:00:02.515865
elapsed time: 0:08:30.323826
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 21:31:25.977557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.00
 ---- batch: 020 ----
mean loss: 183.07
 ---- batch: 030 ----
mean loss: 190.52
 ---- batch: 040 ----
mean loss: 193.21
 ---- batch: 050 ----
mean loss: 198.57
 ---- batch: 060 ----
mean loss: 196.62
 ---- batch: 070 ----
mean loss: 189.75
 ---- batch: 080 ----
mean loss: 185.97
 ---- batch: 090 ----
mean loss: 195.98
 ---- batch: 100 ----
mean loss: 195.95
 ---- batch: 110 ----
mean loss: 186.74
train mean loss: 192.05
epoch train time: 0:00:02.505492
elapsed time: 0:08:32.829749
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 21:31:28.483488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.33
 ---- batch: 020 ----
mean loss: 189.72
 ---- batch: 030 ----
mean loss: 187.83
 ---- batch: 040 ----
mean loss: 193.36
 ---- batch: 050 ----
mean loss: 197.81
 ---- batch: 060 ----
mean loss: 203.19
 ---- batch: 070 ----
mean loss: 182.78
 ---- batch: 080 ----
mean loss: 191.88
 ---- batch: 090 ----
mean loss: 188.11
 ---- batch: 100 ----
mean loss: 187.42
 ---- batch: 110 ----
mean loss: 191.49
train mean loss: 192.01
epoch train time: 0:00:02.502075
elapsed time: 0:08:35.332275
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 21:31:30.986080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.07
 ---- batch: 020 ----
mean loss: 195.42
 ---- batch: 030 ----
mean loss: 190.18
 ---- batch: 040 ----
mean loss: 196.13
 ---- batch: 050 ----
mean loss: 199.50
 ---- batch: 060 ----
mean loss: 183.46
 ---- batch: 070 ----
mean loss: 183.37
 ---- batch: 080 ----
mean loss: 191.02
 ---- batch: 090 ----
mean loss: 184.87
 ---- batch: 100 ----
mean loss: 199.30
 ---- batch: 110 ----
mean loss: 197.47
train mean loss: 191.76
epoch train time: 0:00:02.491178
elapsed time: 0:08:37.823926
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 21:31:33.477662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.21
 ---- batch: 020 ----
mean loss: 201.21
 ---- batch: 030 ----
mean loss: 195.05
 ---- batch: 040 ----
mean loss: 190.69
 ---- batch: 050 ----
mean loss: 183.53
 ---- batch: 060 ----
mean loss: 189.50
 ---- batch: 070 ----
mean loss: 200.53
 ---- batch: 080 ----
mean loss: 185.91
 ---- batch: 090 ----
mean loss: 193.85
 ---- batch: 100 ----
mean loss: 194.64
 ---- batch: 110 ----
mean loss: 184.27
train mean loss: 191.49
epoch train time: 0:00:02.528079
elapsed time: 0:08:40.352408
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 21:31:36.006180
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.99
 ---- batch: 020 ----
mean loss: 199.87
 ---- batch: 030 ----
mean loss: 199.26
 ---- batch: 040 ----
mean loss: 196.67
 ---- batch: 050 ----
mean loss: 182.81
 ---- batch: 060 ----
mean loss: 185.94
 ---- batch: 070 ----
mean loss: 190.79
 ---- batch: 080 ----
mean loss: 187.26
 ---- batch: 090 ----
mean loss: 196.05
 ---- batch: 100 ----
mean loss: 187.94
 ---- batch: 110 ----
mean loss: 195.69
train mean loss: 191.47
epoch train time: 0:00:02.494142
elapsed time: 0:08:42.847017
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 21:31:38.500743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.54
 ---- batch: 020 ----
mean loss: 192.65
 ---- batch: 030 ----
mean loss: 186.78
 ---- batch: 040 ----
mean loss: 201.37
 ---- batch: 050 ----
mean loss: 186.69
 ---- batch: 060 ----
mean loss: 187.72
 ---- batch: 070 ----
mean loss: 191.64
 ---- batch: 080 ----
mean loss: 190.44
 ---- batch: 090 ----
mean loss: 188.75
 ---- batch: 100 ----
mean loss: 181.75
 ---- batch: 110 ----
mean loss: 199.70
train mean loss: 191.21
epoch train time: 0:00:02.504169
elapsed time: 0:08:45.351624
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 21:31:41.005389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.33
 ---- batch: 020 ----
mean loss: 186.83
 ---- batch: 030 ----
mean loss: 197.05
 ---- batch: 040 ----
mean loss: 183.90
 ---- batch: 050 ----
mean loss: 197.11
 ---- batch: 060 ----
mean loss: 181.54
 ---- batch: 070 ----
mean loss: 206.92
 ---- batch: 080 ----
mean loss: 196.72
 ---- batch: 090 ----
mean loss: 190.25
 ---- batch: 100 ----
mean loss: 187.86
 ---- batch: 110 ----
mean loss: 185.78
train mean loss: 191.08
epoch train time: 0:00:02.476938
elapsed time: 0:08:47.828992
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 21:31:43.482721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.06
 ---- batch: 020 ----
mean loss: 187.08
 ---- batch: 030 ----
mean loss: 190.70
 ---- batch: 040 ----
mean loss: 190.94
 ---- batch: 050 ----
mean loss: 187.60
 ---- batch: 060 ----
mean loss: 199.18
 ---- batch: 070 ----
mean loss: 182.67
 ---- batch: 080 ----
mean loss: 186.95
 ---- batch: 090 ----
mean loss: 191.01
 ---- batch: 100 ----
mean loss: 184.68
 ---- batch: 110 ----
mean loss: 197.82
train mean loss: 191.02
epoch train time: 0:00:02.501962
elapsed time: 0:08:50.331420
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 21:31:45.985168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.30
 ---- batch: 020 ----
mean loss: 184.24
 ---- batch: 030 ----
mean loss: 200.97
 ---- batch: 040 ----
mean loss: 180.54
 ---- batch: 050 ----
mean loss: 192.43
 ---- batch: 060 ----
mean loss: 202.87
 ---- batch: 070 ----
mean loss: 189.49
 ---- batch: 080 ----
mean loss: 191.80
 ---- batch: 090 ----
mean loss: 180.97
 ---- batch: 100 ----
mean loss: 188.57
 ---- batch: 110 ----
mean loss: 194.89
train mean loss: 190.82
epoch train time: 0:00:02.484671
elapsed time: 0:08:52.816501
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 21:31:48.470313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.07
 ---- batch: 020 ----
mean loss: 185.18
 ---- batch: 030 ----
mean loss: 187.81
 ---- batch: 040 ----
mean loss: 185.21
 ---- batch: 050 ----
mean loss: 192.37
 ---- batch: 060 ----
mean loss: 195.91
 ---- batch: 070 ----
mean loss: 193.10
 ---- batch: 080 ----
mean loss: 182.39
 ---- batch: 090 ----
mean loss: 192.63
 ---- batch: 100 ----
mean loss: 192.88
 ---- batch: 110 ----
mean loss: 200.72
train mean loss: 190.76
epoch train time: 0:00:02.512464
elapsed time: 0:08:55.329479
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 21:31:50.983236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.75
 ---- batch: 020 ----
mean loss: 187.46
 ---- batch: 030 ----
mean loss: 199.15
 ---- batch: 040 ----
mean loss: 193.49
 ---- batch: 050 ----
mean loss: 185.22
 ---- batch: 060 ----
mean loss: 191.90
 ---- batch: 070 ----
mean loss: 185.35
 ---- batch: 080 ----
mean loss: 189.64
 ---- batch: 090 ----
mean loss: 192.52
 ---- batch: 100 ----
mean loss: 185.41
 ---- batch: 110 ----
mean loss: 189.38
train mean loss: 190.24
epoch train time: 0:00:02.495499
elapsed time: 0:08:57.825616
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 21:31:53.479192
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.66
 ---- batch: 020 ----
mean loss: 184.47
 ---- batch: 030 ----
mean loss: 183.53
 ---- batch: 040 ----
mean loss: 196.22
 ---- batch: 050 ----
mean loss: 189.10
 ---- batch: 060 ----
mean loss: 189.24
 ---- batch: 070 ----
mean loss: 185.60
 ---- batch: 080 ----
mean loss: 197.07
 ---- batch: 090 ----
mean loss: 196.54
 ---- batch: 100 ----
mean loss: 189.01
 ---- batch: 110 ----
mean loss: 185.76
train mean loss: 190.19
epoch train time: 0:00:02.501337
elapsed time: 0:09:00.327191
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 21:31:55.980931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.62
 ---- batch: 020 ----
mean loss: 191.99
 ---- batch: 030 ----
mean loss: 193.97
 ---- batch: 040 ----
mean loss: 190.01
 ---- batch: 050 ----
mean loss: 192.69
 ---- batch: 060 ----
mean loss: 189.15
 ---- batch: 070 ----
mean loss: 187.69
 ---- batch: 080 ----
mean loss: 198.95
 ---- batch: 090 ----
mean loss: 191.19
 ---- batch: 100 ----
mean loss: 188.94
 ---- batch: 110 ----
mean loss: 181.27
train mean loss: 190.10
epoch train time: 0:00:02.492738
elapsed time: 0:09:02.820331
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 21:31:58.474064
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.45
 ---- batch: 020 ----
mean loss: 192.81
 ---- batch: 030 ----
mean loss: 184.13
 ---- batch: 040 ----
mean loss: 189.91
 ---- batch: 050 ----
mean loss: 193.34
 ---- batch: 060 ----
mean loss: 189.30
 ---- batch: 070 ----
mean loss: 192.08
 ---- batch: 080 ----
mean loss: 195.22
 ---- batch: 090 ----
mean loss: 193.75
 ---- batch: 100 ----
mean loss: 184.24
 ---- batch: 110 ----
mean loss: 186.77
train mean loss: 190.09
epoch train time: 0:00:02.498656
elapsed time: 0:09:05.319418
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 21:32:00.973148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.46
 ---- batch: 020 ----
mean loss: 186.92
 ---- batch: 030 ----
mean loss: 190.66
 ---- batch: 040 ----
mean loss: 184.45
 ---- batch: 050 ----
mean loss: 192.43
 ---- batch: 060 ----
mean loss: 192.58
 ---- batch: 070 ----
mean loss: 191.72
 ---- batch: 080 ----
mean loss: 198.75
 ---- batch: 090 ----
mean loss: 184.68
 ---- batch: 100 ----
mean loss: 182.92
 ---- batch: 110 ----
mean loss: 196.40
train mean loss: 190.14
epoch train time: 0:00:02.487770
elapsed time: 0:09:07.807588
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 21:32:03.461322
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.77
 ---- batch: 020 ----
mean loss: 187.66
 ---- batch: 030 ----
mean loss: 192.06
 ---- batch: 040 ----
mean loss: 192.29
 ---- batch: 050 ----
mean loss: 183.50
 ---- batch: 060 ----
mean loss: 193.58
 ---- batch: 070 ----
mean loss: 191.91
 ---- batch: 080 ----
mean loss: 192.79
 ---- batch: 090 ----
mean loss: 187.03
 ---- batch: 100 ----
mean loss: 181.02
 ---- batch: 110 ----
mean loss: 196.43
train mean loss: 190.15
epoch train time: 0:00:02.512726
elapsed time: 0:09:10.320717
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 21:32:05.974460
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.12
 ---- batch: 020 ----
mean loss: 191.80
 ---- batch: 030 ----
mean loss: 190.53
 ---- batch: 040 ----
mean loss: 203.15
 ---- batch: 050 ----
mean loss: 181.40
 ---- batch: 060 ----
mean loss: 192.07
 ---- batch: 070 ----
mean loss: 180.22
 ---- batch: 080 ----
mean loss: 191.97
 ---- batch: 090 ----
mean loss: 185.67
 ---- batch: 100 ----
mean loss: 200.32
 ---- batch: 110 ----
mean loss: 187.18
train mean loss: 190.13
epoch train time: 0:00:02.474319
elapsed time: 0:09:12.795436
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 21:32:08.449182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.92
 ---- batch: 020 ----
mean loss: 190.31
 ---- batch: 030 ----
mean loss: 184.42
 ---- batch: 040 ----
mean loss: 186.48
 ---- batch: 050 ----
mean loss: 192.35
 ---- batch: 060 ----
mean loss: 197.26
 ---- batch: 070 ----
mean loss: 195.69
 ---- batch: 080 ----
mean loss: 196.84
 ---- batch: 090 ----
mean loss: 183.26
 ---- batch: 100 ----
mean loss: 184.58
 ---- batch: 110 ----
mean loss: 197.82
train mean loss: 190.15
epoch train time: 0:00:02.482843
elapsed time: 0:09:15.278724
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 21:32:10.932458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.23
 ---- batch: 020 ----
mean loss: 173.16
 ---- batch: 030 ----
mean loss: 195.51
 ---- batch: 040 ----
mean loss: 185.55
 ---- batch: 050 ----
mean loss: 189.03
 ---- batch: 060 ----
mean loss: 196.70
 ---- batch: 070 ----
mean loss: 190.70
 ---- batch: 080 ----
mean loss: 185.78
 ---- batch: 090 ----
mean loss: 190.63
 ---- batch: 100 ----
mean loss: 189.95
 ---- batch: 110 ----
mean loss: 202.41
train mean loss: 190.02
epoch train time: 0:00:02.490761
elapsed time: 0:09:17.769941
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 21:32:13.423680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.41
 ---- batch: 020 ----
mean loss: 187.48
 ---- batch: 030 ----
mean loss: 189.72
 ---- batch: 040 ----
mean loss: 195.38
 ---- batch: 050 ----
mean loss: 192.34
 ---- batch: 060 ----
mean loss: 193.30
 ---- batch: 070 ----
mean loss: 185.36
 ---- batch: 080 ----
mean loss: 184.74
 ---- batch: 090 ----
mean loss: 181.30
 ---- batch: 100 ----
mean loss: 183.91
 ---- batch: 110 ----
mean loss: 189.26
train mean loss: 190.02
epoch train time: 0:00:02.489165
elapsed time: 0:09:20.259517
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 21:32:15.913265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.35
 ---- batch: 020 ----
mean loss: 194.59
 ---- batch: 030 ----
mean loss: 191.24
 ---- batch: 040 ----
mean loss: 193.28
 ---- batch: 050 ----
mean loss: 197.18
 ---- batch: 060 ----
mean loss: 188.67
 ---- batch: 070 ----
mean loss: 183.45
 ---- batch: 080 ----
mean loss: 183.83
 ---- batch: 090 ----
mean loss: 198.07
 ---- batch: 100 ----
mean loss: 181.15
 ---- batch: 110 ----
mean loss: 187.12
train mean loss: 190.03
epoch train time: 0:00:02.489242
elapsed time: 0:09:22.749201
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 21:32:18.402989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.52
 ---- batch: 020 ----
mean loss: 188.75
 ---- batch: 030 ----
mean loss: 190.51
 ---- batch: 040 ----
mean loss: 192.19
 ---- batch: 050 ----
mean loss: 193.49
 ---- batch: 060 ----
mean loss: 192.75
 ---- batch: 070 ----
mean loss: 193.31
 ---- batch: 080 ----
mean loss: 184.64
 ---- batch: 090 ----
mean loss: 189.41
 ---- batch: 100 ----
mean loss: 190.76
 ---- batch: 110 ----
mean loss: 188.18
train mean loss: 189.93
epoch train time: 0:00:02.526729
elapsed time: 0:09:25.276412
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 21:32:20.930198
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.63
 ---- batch: 020 ----
mean loss: 185.88
 ---- batch: 030 ----
mean loss: 179.70
 ---- batch: 040 ----
mean loss: 193.80
 ---- batch: 050 ----
mean loss: 194.15
 ---- batch: 060 ----
mean loss: 194.75
 ---- batch: 070 ----
mean loss: 183.68
 ---- batch: 080 ----
mean loss: 189.45
 ---- batch: 090 ----
mean loss: 181.58
 ---- batch: 100 ----
mean loss: 195.05
 ---- batch: 110 ----
mean loss: 200.65
train mean loss: 189.96
epoch train time: 0:00:02.482412
elapsed time: 0:09:27.759274
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 21:32:23.413024
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.99
 ---- batch: 020 ----
mean loss: 189.48
 ---- batch: 030 ----
mean loss: 191.00
 ---- batch: 040 ----
mean loss: 181.30
 ---- batch: 050 ----
mean loss: 192.53
 ---- batch: 060 ----
mean loss: 186.80
 ---- batch: 070 ----
mean loss: 200.81
 ---- batch: 080 ----
mean loss: 189.12
 ---- batch: 090 ----
mean loss: 187.73
 ---- batch: 100 ----
mean loss: 189.35
 ---- batch: 110 ----
mean loss: 194.92
train mean loss: 190.02
epoch train time: 0:00:02.495686
elapsed time: 0:09:30.255370
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 21:32:25.909128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.99
 ---- batch: 020 ----
mean loss: 192.36
 ---- batch: 030 ----
mean loss: 192.74
 ---- batch: 040 ----
mean loss: 195.05
 ---- batch: 050 ----
mean loss: 187.77
 ---- batch: 060 ----
mean loss: 184.75
 ---- batch: 070 ----
mean loss: 196.47
 ---- batch: 080 ----
mean loss: 183.84
 ---- batch: 090 ----
mean loss: 177.28
 ---- batch: 100 ----
mean loss: 186.95
 ---- batch: 110 ----
mean loss: 189.72
train mean loss: 189.91
epoch train time: 0:00:02.518753
elapsed time: 0:09:32.774551
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 21:32:28.428331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.11
 ---- batch: 020 ----
mean loss: 190.46
 ---- batch: 030 ----
mean loss: 190.36
 ---- batch: 040 ----
mean loss: 190.89
 ---- batch: 050 ----
mean loss: 187.05
 ---- batch: 060 ----
mean loss: 187.26
 ---- batch: 070 ----
mean loss: 183.79
 ---- batch: 080 ----
mean loss: 193.69
 ---- batch: 090 ----
mean loss: 191.30
 ---- batch: 100 ----
mean loss: 192.83
 ---- batch: 110 ----
mean loss: 188.23
train mean loss: 189.97
epoch train time: 0:00:02.552793
elapsed time: 0:09:35.327824
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 21:32:30.981572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.67
 ---- batch: 020 ----
mean loss: 186.88
 ---- batch: 030 ----
mean loss: 189.09
 ---- batch: 040 ----
mean loss: 191.44
 ---- batch: 050 ----
mean loss: 188.47
 ---- batch: 060 ----
mean loss: 190.72
 ---- batch: 070 ----
mean loss: 186.58
 ---- batch: 080 ----
mean loss: 189.50
 ---- batch: 090 ----
mean loss: 189.46
 ---- batch: 100 ----
mean loss: 200.40
 ---- batch: 110 ----
mean loss: 191.84
train mean loss: 190.03
epoch train time: 0:00:02.543629
elapsed time: 0:09:37.871869
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 21:32:33.525620
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.15
 ---- batch: 020 ----
mean loss: 193.31
 ---- batch: 030 ----
mean loss: 196.74
 ---- batch: 040 ----
mean loss: 193.19
 ---- batch: 050 ----
mean loss: 187.95
 ---- batch: 060 ----
mean loss: 192.54
 ---- batch: 070 ----
mean loss: 190.12
 ---- batch: 080 ----
mean loss: 178.03
 ---- batch: 090 ----
mean loss: 188.89
 ---- batch: 100 ----
mean loss: 188.76
 ---- batch: 110 ----
mean loss: 188.50
train mean loss: 189.88
epoch train time: 0:00:02.537461
elapsed time: 0:09:40.409785
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 21:32:36.063540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.78
 ---- batch: 020 ----
mean loss: 192.06
 ---- batch: 030 ----
mean loss: 185.23
 ---- batch: 040 ----
mean loss: 188.91
 ---- batch: 050 ----
mean loss: 190.24
 ---- batch: 060 ----
mean loss: 181.77
 ---- batch: 070 ----
mean loss: 189.39
 ---- batch: 080 ----
mean loss: 197.18
 ---- batch: 090 ----
mean loss: 189.79
 ---- batch: 100 ----
mean loss: 194.67
 ---- batch: 110 ----
mean loss: 191.14
train mean loss: 189.93
epoch train time: 0:00:02.548701
elapsed time: 0:09:42.958923
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 21:32:38.612654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.99
 ---- batch: 020 ----
mean loss: 192.32
 ---- batch: 030 ----
mean loss: 192.10
 ---- batch: 040 ----
mean loss: 190.19
 ---- batch: 050 ----
mean loss: 194.62
 ---- batch: 060 ----
mean loss: 183.42
 ---- batch: 070 ----
mean loss: 191.64
 ---- batch: 080 ----
mean loss: 192.59
 ---- batch: 090 ----
mean loss: 190.29
 ---- batch: 100 ----
mean loss: 195.94
 ---- batch: 110 ----
mean loss: 187.06
train mean loss: 189.84
epoch train time: 0:00:02.513537
elapsed time: 0:09:45.472857
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 21:32:41.126658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.09
 ---- batch: 020 ----
mean loss: 183.49
 ---- batch: 030 ----
mean loss: 194.31
 ---- batch: 040 ----
mean loss: 202.03
 ---- batch: 050 ----
mean loss: 182.17
 ---- batch: 060 ----
mean loss: 189.64
 ---- batch: 070 ----
mean loss: 193.84
 ---- batch: 080 ----
mean loss: 194.63
 ---- batch: 090 ----
mean loss: 188.61
 ---- batch: 100 ----
mean loss: 186.22
 ---- batch: 110 ----
mean loss: 192.47
train mean loss: 189.86
epoch train time: 0:00:02.493207
elapsed time: 0:09:47.966538
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 21:32:43.620294
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.94
 ---- batch: 020 ----
mean loss: 196.57
 ---- batch: 030 ----
mean loss: 203.47
 ---- batch: 040 ----
mean loss: 182.14
 ---- batch: 050 ----
mean loss: 189.24
 ---- batch: 060 ----
mean loss: 187.28
 ---- batch: 070 ----
mean loss: 191.91
 ---- batch: 080 ----
mean loss: 183.54
 ---- batch: 090 ----
mean loss: 185.07
 ---- batch: 100 ----
mean loss: 189.45
 ---- batch: 110 ----
mean loss: 184.19
train mean loss: 189.87
epoch train time: 0:00:02.510275
elapsed time: 0:09:50.477233
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 21:32:46.130964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.83
 ---- batch: 020 ----
mean loss: 189.82
 ---- batch: 030 ----
mean loss: 185.01
 ---- batch: 040 ----
mean loss: 194.79
 ---- batch: 050 ----
mean loss: 195.55
 ---- batch: 060 ----
mean loss: 187.71
 ---- batch: 070 ----
mean loss: 187.56
 ---- batch: 080 ----
mean loss: 185.99
 ---- batch: 090 ----
mean loss: 195.35
 ---- batch: 100 ----
mean loss: 191.29
 ---- batch: 110 ----
mean loss: 186.02
train mean loss: 189.86
epoch train time: 0:00:02.484085
elapsed time: 0:09:52.961748
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 21:32:48.615518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.49
 ---- batch: 020 ----
mean loss: 191.23
 ---- batch: 030 ----
mean loss: 187.68
 ---- batch: 040 ----
mean loss: 190.37
 ---- batch: 050 ----
mean loss: 187.60
 ---- batch: 060 ----
mean loss: 202.01
 ---- batch: 070 ----
mean loss: 187.97
 ---- batch: 080 ----
mean loss: 194.15
 ---- batch: 090 ----
mean loss: 193.16
 ---- batch: 100 ----
mean loss: 176.65
 ---- batch: 110 ----
mean loss: 187.69
train mean loss: 189.86
epoch train time: 0:00:02.465181
elapsed time: 0:09:55.427392
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 21:32:51.081120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.89
 ---- batch: 020 ----
mean loss: 186.70
 ---- batch: 030 ----
mean loss: 193.57
 ---- batch: 040 ----
mean loss: 193.77
 ---- batch: 050 ----
mean loss: 180.79
 ---- batch: 060 ----
mean loss: 192.88
 ---- batch: 070 ----
mean loss: 184.80
 ---- batch: 080 ----
mean loss: 186.01
 ---- batch: 090 ----
mean loss: 192.49
 ---- batch: 100 ----
mean loss: 194.26
 ---- batch: 110 ----
mean loss: 188.51
train mean loss: 189.87
epoch train time: 0:00:02.500458
elapsed time: 0:09:57.928293
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 21:32:53.582038
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.95
 ---- batch: 020 ----
mean loss: 194.95
 ---- batch: 030 ----
mean loss: 193.66
 ---- batch: 040 ----
mean loss: 184.77
 ---- batch: 050 ----
mean loss: 190.54
 ---- batch: 060 ----
mean loss: 179.35
 ---- batch: 070 ----
mean loss: 194.79
 ---- batch: 080 ----
mean loss: 184.08
 ---- batch: 090 ----
mean loss: 195.87
 ---- batch: 100 ----
mean loss: 195.66
 ---- batch: 110 ----
mean loss: 193.09
train mean loss: 189.76
epoch train time: 0:00:02.478333
elapsed time: 0:10:00.407071
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 21:32:56.060822
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.77
 ---- batch: 020 ----
mean loss: 190.52
 ---- batch: 030 ----
mean loss: 193.36
 ---- batch: 040 ----
mean loss: 183.60
 ---- batch: 050 ----
mean loss: 185.82
 ---- batch: 060 ----
mean loss: 193.00
 ---- batch: 070 ----
mean loss: 199.03
 ---- batch: 080 ----
mean loss: 198.03
 ---- batch: 090 ----
mean loss: 191.00
 ---- batch: 100 ----
mean loss: 187.80
 ---- batch: 110 ----
mean loss: 187.18
train mean loss: 189.79
epoch train time: 0:00:02.510911
elapsed time: 0:10:02.918409
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 21:32:58.572151
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.32
 ---- batch: 020 ----
mean loss: 182.02
 ---- batch: 030 ----
mean loss: 188.22
 ---- batch: 040 ----
mean loss: 194.85
 ---- batch: 050 ----
mean loss: 192.85
 ---- batch: 060 ----
mean loss: 191.97
 ---- batch: 070 ----
mean loss: 198.10
 ---- batch: 080 ----
mean loss: 189.57
 ---- batch: 090 ----
mean loss: 182.76
 ---- batch: 100 ----
mean loss: 186.42
 ---- batch: 110 ----
mean loss: 188.35
train mean loss: 189.78
epoch train time: 0:00:02.500625
elapsed time: 0:10:05.419457
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 21:33:01.073212
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.98
 ---- batch: 020 ----
mean loss: 193.80
 ---- batch: 030 ----
mean loss: 198.63
 ---- batch: 040 ----
mean loss: 192.20
 ---- batch: 050 ----
mean loss: 188.27
 ---- batch: 060 ----
mean loss: 189.55
 ---- batch: 070 ----
mean loss: 186.79
 ---- batch: 080 ----
mean loss: 181.30
 ---- batch: 090 ----
mean loss: 192.84
 ---- batch: 100 ----
mean loss: 190.83
 ---- batch: 110 ----
mean loss: 189.53
train mean loss: 189.71
epoch train time: 0:00:02.501487
elapsed time: 0:10:07.921363
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 21:33:03.575089
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.99
 ---- batch: 020 ----
mean loss: 183.87
 ---- batch: 030 ----
mean loss: 185.50
 ---- batch: 040 ----
mean loss: 203.27
 ---- batch: 050 ----
mean loss: 181.67
 ---- batch: 060 ----
mean loss: 189.03
 ---- batch: 070 ----
mean loss: 190.98
 ---- batch: 080 ----
mean loss: 192.04
 ---- batch: 090 ----
mean loss: 188.00
 ---- batch: 100 ----
mean loss: 184.63
 ---- batch: 110 ----
mean loss: 193.89
train mean loss: 189.78
epoch train time: 0:00:02.500004
elapsed time: 0:10:10.421867
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 21:33:06.075634
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.88
 ---- batch: 020 ----
mean loss: 180.99
 ---- batch: 030 ----
mean loss: 182.42
 ---- batch: 040 ----
mean loss: 195.71
 ---- batch: 050 ----
mean loss: 193.44
 ---- batch: 060 ----
mean loss: 182.52
 ---- batch: 070 ----
mean loss: 189.92
 ---- batch: 080 ----
mean loss: 198.62
 ---- batch: 090 ----
mean loss: 187.87
 ---- batch: 100 ----
mean loss: 198.02
 ---- batch: 110 ----
mean loss: 191.85
train mean loss: 189.68
epoch train time: 0:00:02.511920
elapsed time: 0:10:12.934234
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 21:33:08.587971
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.64
 ---- batch: 020 ----
mean loss: 187.88
 ---- batch: 030 ----
mean loss: 196.13
 ---- batch: 040 ----
mean loss: 194.00
 ---- batch: 050 ----
mean loss: 180.48
 ---- batch: 060 ----
mean loss: 195.34
 ---- batch: 070 ----
mean loss: 191.64
 ---- batch: 080 ----
mean loss: 191.23
 ---- batch: 090 ----
mean loss: 187.09
 ---- batch: 100 ----
mean loss: 184.85
 ---- batch: 110 ----
mean loss: 191.73
train mean loss: 189.66
epoch train time: 0:00:02.496058
elapsed time: 0:10:15.430723
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 21:33:11.084449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.25
 ---- batch: 020 ----
mean loss: 181.97
 ---- batch: 030 ----
mean loss: 183.71
 ---- batch: 040 ----
mean loss: 190.79
 ---- batch: 050 ----
mean loss: 188.06
 ---- batch: 060 ----
mean loss: 199.89
 ---- batch: 070 ----
mean loss: 192.94
 ---- batch: 080 ----
mean loss: 193.88
 ---- batch: 090 ----
mean loss: 185.37
 ---- batch: 100 ----
mean loss: 193.85
 ---- batch: 110 ----
mean loss: 188.00
train mean loss: 189.68
epoch train time: 0:00:02.502015
elapsed time: 0:10:17.933330
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 21:33:13.586887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.99
 ---- batch: 020 ----
mean loss: 186.44
 ---- batch: 030 ----
mean loss: 178.23
 ---- batch: 040 ----
mean loss: 190.15
 ---- batch: 050 ----
mean loss: 185.88
 ---- batch: 060 ----
mean loss: 193.04
 ---- batch: 070 ----
mean loss: 187.26
 ---- batch: 080 ----
mean loss: 196.48
 ---- batch: 090 ----
mean loss: 197.22
 ---- batch: 100 ----
mean loss: 189.35
 ---- batch: 110 ----
mean loss: 188.60
train mean loss: 189.60
epoch train time: 0:00:02.476406
elapsed time: 0:10:20.409959
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 21:33:16.063706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.31
 ---- batch: 020 ----
mean loss: 183.65
 ---- batch: 030 ----
mean loss: 186.84
 ---- batch: 040 ----
mean loss: 198.45
 ---- batch: 050 ----
mean loss: 182.61
 ---- batch: 060 ----
mean loss: 195.57
 ---- batch: 070 ----
mean loss: 195.03
 ---- batch: 080 ----
mean loss: 182.37
 ---- batch: 090 ----
mean loss: 187.85
 ---- batch: 100 ----
mean loss: 193.91
 ---- batch: 110 ----
mean loss: 186.16
train mean loss: 189.63
epoch train time: 0:00:02.488683
elapsed time: 0:10:22.899068
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 21:33:18.552825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.28
 ---- batch: 020 ----
mean loss: 190.55
 ---- batch: 030 ----
mean loss: 186.30
 ---- batch: 040 ----
mean loss: 194.32
 ---- batch: 050 ----
mean loss: 191.57
 ---- batch: 060 ----
mean loss: 200.62
 ---- batch: 070 ----
mean loss: 194.18
 ---- batch: 080 ----
mean loss: 184.66
 ---- batch: 090 ----
mean loss: 185.85
 ---- batch: 100 ----
mean loss: 183.97
 ---- batch: 110 ----
mean loss: 184.93
train mean loss: 189.70
epoch train time: 0:00:02.525330
elapsed time: 0:10:25.424864
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 21:33:21.078452
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.95
 ---- batch: 020 ----
mean loss: 199.13
 ---- batch: 030 ----
mean loss: 189.45
 ---- batch: 040 ----
mean loss: 185.86
 ---- batch: 050 ----
mean loss: 189.14
 ---- batch: 060 ----
mean loss: 191.84
 ---- batch: 070 ----
mean loss: 180.07
 ---- batch: 080 ----
mean loss: 185.79
 ---- batch: 090 ----
mean loss: 197.97
 ---- batch: 100 ----
mean loss: 193.63
 ---- batch: 110 ----
mean loss: 183.59
train mean loss: 189.67
epoch train time: 0:00:02.504306
elapsed time: 0:10:27.929459
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 21:33:23.583288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.03
 ---- batch: 020 ----
mean loss: 193.43
 ---- batch: 030 ----
mean loss: 188.14
 ---- batch: 040 ----
mean loss: 188.79
 ---- batch: 050 ----
mean loss: 186.85
 ---- batch: 060 ----
mean loss: 191.27
 ---- batch: 070 ----
mean loss: 199.66
 ---- batch: 080 ----
mean loss: 186.30
 ---- batch: 090 ----
mean loss: 181.08
 ---- batch: 100 ----
mean loss: 195.55
 ---- batch: 110 ----
mean loss: 187.16
train mean loss: 189.57
epoch train time: 0:00:02.497571
elapsed time: 0:10:30.427535
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 21:33:26.081279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.25
 ---- batch: 020 ----
mean loss: 192.63
 ---- batch: 030 ----
mean loss: 192.32
 ---- batch: 040 ----
mean loss: 193.99
 ---- batch: 050 ----
mean loss: 184.13
 ---- batch: 060 ----
mean loss: 198.10
 ---- batch: 070 ----
mean loss: 192.36
 ---- batch: 080 ----
mean loss: 197.98
 ---- batch: 090 ----
mean loss: 183.15
 ---- batch: 100 ----
mean loss: 187.39
 ---- batch: 110 ----
mean loss: 180.47
train mean loss: 189.59
epoch train time: 0:00:02.513811
elapsed time: 0:10:32.941815
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 21:33:28.595572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.95
 ---- batch: 020 ----
mean loss: 185.16
 ---- batch: 030 ----
mean loss: 192.17
 ---- batch: 040 ----
mean loss: 182.01
 ---- batch: 050 ----
mean loss: 185.81
 ---- batch: 060 ----
mean loss: 190.97
 ---- batch: 070 ----
mean loss: 192.08
 ---- batch: 080 ----
mean loss: 186.30
 ---- batch: 090 ----
mean loss: 201.80
 ---- batch: 100 ----
mean loss: 188.17
 ---- batch: 110 ----
mean loss: 197.40
train mean loss: 189.61
epoch train time: 0:00:02.494362
elapsed time: 0:10:35.436632
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 21:33:31.090382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.68
 ---- batch: 020 ----
mean loss: 188.20
 ---- batch: 030 ----
mean loss: 184.93
 ---- batch: 040 ----
mean loss: 188.83
 ---- batch: 050 ----
mean loss: 192.56
 ---- batch: 060 ----
mean loss: 194.15
 ---- batch: 070 ----
mean loss: 192.92
 ---- batch: 080 ----
mean loss: 188.82
 ---- batch: 090 ----
mean loss: 194.41
 ---- batch: 100 ----
mean loss: 195.59
 ---- batch: 110 ----
mean loss: 183.91
train mean loss: 189.59
epoch train time: 0:00:02.495897
elapsed time: 0:10:37.932954
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 21:33:33.586687
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.03
 ---- batch: 020 ----
mean loss: 202.65
 ---- batch: 030 ----
mean loss: 198.95
 ---- batch: 040 ----
mean loss: 182.28
 ---- batch: 050 ----
mean loss: 185.35
 ---- batch: 060 ----
mean loss: 188.44
 ---- batch: 070 ----
mean loss: 193.43
 ---- batch: 080 ----
mean loss: 173.86
 ---- batch: 090 ----
mean loss: 187.39
 ---- batch: 100 ----
mean loss: 194.59
 ---- batch: 110 ----
mean loss: 189.82
train mean loss: 189.53
epoch train time: 0:00:02.519874
elapsed time: 0:10:40.453270
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 21:33:36.107011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.75
 ---- batch: 020 ----
mean loss: 199.14
 ---- batch: 030 ----
mean loss: 185.00
 ---- batch: 040 ----
mean loss: 200.23
 ---- batch: 050 ----
mean loss: 195.09
 ---- batch: 060 ----
mean loss: 185.31
 ---- batch: 070 ----
mean loss: 193.94
 ---- batch: 080 ----
mean loss: 178.59
 ---- batch: 090 ----
mean loss: 182.83
 ---- batch: 100 ----
mean loss: 192.20
 ---- batch: 110 ----
mean loss: 186.92
train mean loss: 189.58
epoch train time: 0:00:02.506373
elapsed time: 0:10:42.960049
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 21:33:38.613805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.90
 ---- batch: 020 ----
mean loss: 187.25
 ---- batch: 030 ----
mean loss: 191.45
 ---- batch: 040 ----
mean loss: 190.60
 ---- batch: 050 ----
mean loss: 190.45
 ---- batch: 060 ----
mean loss: 183.86
 ---- batch: 070 ----
mean loss: 193.24
 ---- batch: 080 ----
mean loss: 197.96
 ---- batch: 090 ----
mean loss: 187.52
 ---- batch: 100 ----
mean loss: 191.40
 ---- batch: 110 ----
mean loss: 191.16
train mean loss: 189.56
epoch train time: 0:00:02.499463
elapsed time: 0:10:45.459930
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 21:33:41.113685
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.80
 ---- batch: 020 ----
mean loss: 185.32
 ---- batch: 030 ----
mean loss: 193.55
 ---- batch: 040 ----
mean loss: 190.44
 ---- batch: 050 ----
mean loss: 187.78
 ---- batch: 060 ----
mean loss: 191.71
 ---- batch: 070 ----
mean loss: 192.22
 ---- batch: 080 ----
mean loss: 184.28
 ---- batch: 090 ----
mean loss: 191.61
 ---- batch: 100 ----
mean loss: 191.39
 ---- batch: 110 ----
mean loss: 194.06
train mean loss: 189.51
epoch train time: 0:00:02.485836
elapsed time: 0:10:47.946226
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 21:33:43.599993
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.41
 ---- batch: 020 ----
mean loss: 187.43
 ---- batch: 030 ----
mean loss: 192.14
 ---- batch: 040 ----
mean loss: 189.65
 ---- batch: 050 ----
mean loss: 190.52
 ---- batch: 060 ----
mean loss: 186.67
 ---- batch: 070 ----
mean loss: 189.82
 ---- batch: 080 ----
mean loss: 188.12
 ---- batch: 090 ----
mean loss: 185.08
 ---- batch: 100 ----
mean loss: 190.76
 ---- batch: 110 ----
mean loss: 190.23
train mean loss: 189.56
epoch train time: 0:00:02.505978
elapsed time: 0:10:50.452637
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 21:33:46.106364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.62
 ---- batch: 020 ----
mean loss: 193.97
 ---- batch: 030 ----
mean loss: 193.83
 ---- batch: 040 ----
mean loss: 192.99
 ---- batch: 050 ----
mean loss: 187.60
 ---- batch: 060 ----
mean loss: 191.51
 ---- batch: 070 ----
mean loss: 193.86
 ---- batch: 080 ----
mean loss: 183.46
 ---- batch: 090 ----
mean loss: 186.97
 ---- batch: 100 ----
mean loss: 190.51
 ---- batch: 110 ----
mean loss: 197.11
train mean loss: 189.45
epoch train time: 0:00:02.491275
elapsed time: 0:10:52.944351
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 21:33:48.598114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.07
 ---- batch: 020 ----
mean loss: 191.43
 ---- batch: 030 ----
mean loss: 186.94
 ---- batch: 040 ----
mean loss: 186.66
 ---- batch: 050 ----
mean loss: 185.92
 ---- batch: 060 ----
mean loss: 190.76
 ---- batch: 070 ----
mean loss: 182.94
 ---- batch: 080 ----
mean loss: 189.83
 ---- batch: 090 ----
mean loss: 192.87
 ---- batch: 100 ----
mean loss: 195.69
 ---- batch: 110 ----
mean loss: 189.58
train mean loss: 189.36
epoch train time: 0:00:02.496080
elapsed time: 0:10:55.440858
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 21:33:51.094594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.28
 ---- batch: 020 ----
mean loss: 192.93
 ---- batch: 030 ----
mean loss: 185.28
 ---- batch: 040 ----
mean loss: 188.27
 ---- batch: 050 ----
mean loss: 183.33
 ---- batch: 060 ----
mean loss: 196.65
 ---- batch: 070 ----
mean loss: 189.69
 ---- batch: 080 ----
mean loss: 188.39
 ---- batch: 090 ----
mean loss: 189.41
 ---- batch: 100 ----
mean loss: 194.47
 ---- batch: 110 ----
mean loss: 190.33
train mean loss: 189.45
epoch train time: 0:00:02.486174
elapsed time: 0:10:57.931391
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_5/checkpoint.pth.tar
**** end time: 2019-09-26 21:33:53.584912 ****
