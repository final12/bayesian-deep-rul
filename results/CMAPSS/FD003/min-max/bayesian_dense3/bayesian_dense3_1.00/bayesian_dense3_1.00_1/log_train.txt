Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5671
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-09-20 21:37:04.399265 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:37:04.409015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4539.32
 ---- batch: 020 ----
mean loss: 4367.61
 ---- batch: 030 ----
mean loss: 4224.16
 ---- batch: 040 ----
mean loss: 3993.04
train mean loss: 4249.85
epoch train time: 0:00:14.919705
elapsed time: 0:00:14.935775
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:37:19.335098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3782.93
 ---- batch: 020 ----
mean loss: 3558.42
 ---- batch: 030 ----
mean loss: 3423.50
 ---- batch: 040 ----
mean loss: 3379.16
train mean loss: 3515.66
epoch train time: 0:00:00.683182
elapsed time: 0:00:15.619150
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:37:20.018508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3123.40
 ---- batch: 020 ----
mean loss: 3068.84
 ---- batch: 030 ----
mean loss: 3012.21
 ---- batch: 040 ----
mean loss: 2888.70
train mean loss: 3005.84
epoch train time: 0:00:00.651453
elapsed time: 0:00:16.270840
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:37:20.670170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2758.08
 ---- batch: 020 ----
mean loss: 2765.59
 ---- batch: 030 ----
mean loss: 2549.46
 ---- batch: 040 ----
mean loss: 2585.17
train mean loss: 2662.69
epoch train time: 0:00:00.653802
elapsed time: 0:00:16.924875
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:37:21.324227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2503.08
 ---- batch: 020 ----
mean loss: 2437.95
 ---- batch: 030 ----
mean loss: 2402.70
 ---- batch: 040 ----
mean loss: 2333.03
train mean loss: 2412.55
epoch train time: 0:00:00.654783
elapsed time: 0:00:17.579910
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:37:21.979264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2273.27
 ---- batch: 020 ----
mean loss: 2283.20
 ---- batch: 030 ----
mean loss: 2203.83
 ---- batch: 040 ----
mean loss: 2166.13
train mean loss: 2228.03
epoch train time: 0:00:00.701780
elapsed time: 0:00:18.281961
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:37:22.681308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2113.49
 ---- batch: 020 ----
mean loss: 2128.86
 ---- batch: 030 ----
mean loss: 2080.81
 ---- batch: 040 ----
mean loss: 1985.51
train mean loss: 2071.60
epoch train time: 0:00:00.713176
elapsed time: 0:00:18.995412
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:37:23.394771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2004.59
 ---- batch: 020 ----
mean loss: 1917.37
 ---- batch: 030 ----
mean loss: 1956.20
 ---- batch: 040 ----
mean loss: 1897.81
train mean loss: 1942.66
epoch train time: 0:00:00.682147
elapsed time: 0:00:19.677806
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:37:24.077142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1901.57
 ---- batch: 020 ----
mean loss: 1825.96
 ---- batch: 030 ----
mean loss: 1801.77
 ---- batch: 040 ----
mean loss: 1768.57
train mean loss: 1821.17
epoch train time: 0:00:00.666451
elapsed time: 0:00:20.344455
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:37:24.743835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1750.51
 ---- batch: 020 ----
mean loss: 1711.91
 ---- batch: 030 ----
mean loss: 1725.42
 ---- batch: 040 ----
mean loss: 1700.46
train mean loss: 1715.34
epoch train time: 0:00:00.679357
elapsed time: 0:00:21.024103
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:37:25.423447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1642.71
 ---- batch: 020 ----
mean loss: 1628.06
 ---- batch: 030 ----
mean loss: 1611.74
 ---- batch: 040 ----
mean loss: 1554.84
train mean loss: 1610.35
epoch train time: 0:00:00.700416
elapsed time: 0:00:21.724783
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:37:26.124125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1542.21
 ---- batch: 020 ----
mean loss: 1464.66
 ---- batch: 030 ----
mean loss: 1455.91
 ---- batch: 040 ----
mean loss: 1437.96
train mean loss: 1473.05
epoch train time: 0:00:00.708774
elapsed time: 0:00:22.433903
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:37:26.833246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1404.36
 ---- batch: 020 ----
mean loss: 1399.59
 ---- batch: 030 ----
mean loss: 1348.92
 ---- batch: 040 ----
mean loss: 1360.74
train mean loss: 1375.11
epoch train time: 0:00:00.699306
elapsed time: 0:00:23.133430
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:37:27.532768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1313.08
 ---- batch: 020 ----
mean loss: 1291.02
 ---- batch: 030 ----
mean loss: 1269.69
 ---- batch: 040 ----
mean loss: 1264.32
train mean loss: 1285.16
epoch train time: 0:00:00.657867
elapsed time: 0:00:23.791515
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:37:28.190860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1233.07
 ---- batch: 020 ----
mean loss: 1190.55
 ---- batch: 030 ----
mean loss: 1186.95
 ---- batch: 040 ----
mean loss: 1179.40
train mean loss: 1193.35
epoch train time: 0:00:00.672236
elapsed time: 0:00:24.464007
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:37:28.863357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1136.38
 ---- batch: 020 ----
mean loss: 1126.00
 ---- batch: 030 ----
mean loss: 1113.47
 ---- batch: 040 ----
mean loss: 1088.99
train mean loss: 1112.26
epoch train time: 0:00:00.663126
elapsed time: 0:00:25.127395
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:37:29.526736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1054.54
 ---- batch: 020 ----
mean loss: 1046.86
 ---- batch: 030 ----
mean loss: 1018.44
 ---- batch: 040 ----
mean loss: 1022.60
train mean loss: 1036.23
epoch train time: 0:00:00.706387
elapsed time: 0:00:25.834018
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:37:30.233359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 993.80
 ---- batch: 020 ----
mean loss: 973.50
 ---- batch: 030 ----
mean loss: 967.56
 ---- batch: 040 ----
mean loss: 944.32
train mean loss: 967.52
epoch train time: 0:00:00.691374
elapsed time: 0:00:26.525646
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:37:30.925041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.15
 ---- batch: 020 ----
mean loss: 889.97
 ---- batch: 030 ----
mean loss: 910.91
 ---- batch: 040 ----
mean loss: 887.09
train mean loss: 903.53
epoch train time: 0:00:00.664278
elapsed time: 0:00:27.190191
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:37:31.589531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.12
 ---- batch: 020 ----
mean loss: 849.13
 ---- batch: 030 ----
mean loss: 841.53
 ---- batch: 040 ----
mean loss: 812.46
train mean loss: 836.63
epoch train time: 0:00:00.663428
elapsed time: 0:00:27.853825
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:37:32.253153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 788.97
 ---- batch: 020 ----
mean loss: 800.75
 ---- batch: 030 ----
mean loss: 790.61
 ---- batch: 040 ----
mean loss: 765.17
train mean loss: 783.84
epoch train time: 0:00:00.679882
elapsed time: 0:00:28.533900
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:37:32.933232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.14
 ---- batch: 020 ----
mean loss: 733.38
 ---- batch: 030 ----
mean loss: 737.94
 ---- batch: 040 ----
mean loss: 727.36
train mean loss: 735.74
epoch train time: 0:00:00.698473
elapsed time: 0:00:29.232635
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:37:33.631985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 703.48
 ---- batch: 020 ----
mean loss: 691.61
 ---- batch: 030 ----
mean loss: 675.91
 ---- batch: 040 ----
mean loss: 657.18
train mean loss: 680.06
epoch train time: 0:00:00.714603
elapsed time: 0:00:29.947494
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:37:34.346848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 653.64
 ---- batch: 020 ----
mean loss: 655.59
 ---- batch: 030 ----
mean loss: 624.36
 ---- batch: 040 ----
mean loss: 618.94
train mean loss: 634.93
epoch train time: 0:00:00.694683
elapsed time: 0:00:30.642433
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:37:35.041790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 626.43
 ---- batch: 020 ----
mean loss: 595.98
 ---- batch: 030 ----
mean loss: 588.74
 ---- batch: 040 ----
mean loss: 580.72
train mean loss: 595.40
epoch train time: 0:00:00.649529
elapsed time: 0:00:31.292199
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:37:35.691534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.10
 ---- batch: 020 ----
mean loss: 577.61
 ---- batch: 030 ----
mean loss: 548.45
 ---- batch: 040 ----
mean loss: 530.04
train mean loss: 551.84
epoch train time: 0:00:00.647808
elapsed time: 0:00:31.940221
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:37:36.339569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.24
 ---- batch: 020 ----
mean loss: 517.02
 ---- batch: 030 ----
mean loss: 504.34
 ---- batch: 040 ----
mean loss: 502.29
train mean loss: 514.85
epoch train time: 0:00:00.667766
elapsed time: 0:00:32.608269
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:37:37.007608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.20
 ---- batch: 020 ----
mean loss: 494.28
 ---- batch: 030 ----
mean loss: 485.24
 ---- batch: 040 ----
mean loss: 477.32
train mean loss: 486.53
epoch train time: 0:00:00.695953
elapsed time: 0:00:33.304458
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:37:37.703828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.78
 ---- batch: 020 ----
mean loss: 454.72
 ---- batch: 030 ----
mean loss: 454.49
 ---- batch: 040 ----
mean loss: 449.59
train mean loss: 456.13
epoch train time: 0:00:00.705058
elapsed time: 0:00:34.009862
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:37:38.409234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 440.14
 ---- batch: 020 ----
mean loss: 420.91
 ---- batch: 030 ----
mean loss: 423.90
 ---- batch: 040 ----
mean loss: 416.24
train mean loss: 423.72
epoch train time: 0:00:00.698704
elapsed time: 0:00:34.708807
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:37:39.108143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.85
 ---- batch: 020 ----
mean loss: 412.52
 ---- batch: 030 ----
mean loss: 395.37
 ---- batch: 040 ----
mean loss: 392.81
train mean loss: 399.53
epoch train time: 0:00:00.679840
elapsed time: 0:00:35.388882
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:37:39.788233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.71
 ---- batch: 020 ----
mean loss: 383.29
 ---- batch: 030 ----
mean loss: 369.31
 ---- batch: 040 ----
mean loss: 360.07
train mean loss: 372.53
epoch train time: 0:00:00.650976
elapsed time: 0:00:36.040072
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:37:40.439413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.95
 ---- batch: 020 ----
mean loss: 360.02
 ---- batch: 030 ----
mean loss: 345.03
 ---- batch: 040 ----
mean loss: 357.53
train mean loss: 353.54
epoch train time: 0:00:00.686788
elapsed time: 0:00:36.727159
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:37:41.126514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.03
 ---- batch: 020 ----
mean loss: 329.59
 ---- batch: 030 ----
mean loss: 330.56
 ---- batch: 040 ----
mean loss: 327.47
train mean loss: 330.69
epoch train time: 0:00:00.716038
elapsed time: 0:00:37.443479
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:37:41.842833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.78
 ---- batch: 020 ----
mean loss: 313.04
 ---- batch: 030 ----
mean loss: 312.28
 ---- batch: 040 ----
mean loss: 301.05
train mean loss: 312.37
epoch train time: 0:00:00.707284
elapsed time: 0:00:38.151032
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:37:42.550378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.37
 ---- batch: 020 ----
mean loss: 303.18
 ---- batch: 030 ----
mean loss: 292.98
 ---- batch: 040 ----
mean loss: 294.92
train mean loss: 296.35
epoch train time: 0:00:00.663035
elapsed time: 0:00:38.814283
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:37:43.213617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.91
 ---- batch: 020 ----
mean loss: 279.09
 ---- batch: 030 ----
mean loss: 272.20
 ---- batch: 040 ----
mean loss: 275.46
train mean loss: 276.41
epoch train time: 0:00:00.649460
elapsed time: 0:00:39.463950
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:37:43.863287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.22
 ---- batch: 020 ----
mean loss: 260.64
 ---- batch: 030 ----
mean loss: 260.30
 ---- batch: 040 ----
mean loss: 257.42
train mean loss: 263.14
epoch train time: 0:00:00.666851
elapsed time: 0:00:40.131088
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:37:44.530452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.64
 ---- batch: 020 ----
mean loss: 249.57
 ---- batch: 030 ----
mean loss: 243.84
 ---- batch: 040 ----
mean loss: 249.03
train mean loss: 249.53
epoch train time: 0:00:00.699197
elapsed time: 0:00:40.830562
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:37:45.229897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.90
 ---- batch: 020 ----
mean loss: 233.14
 ---- batch: 030 ----
mean loss: 235.68
 ---- batch: 040 ----
mean loss: 227.40
train mean loss: 233.58
epoch train time: 0:00:00.700087
elapsed time: 0:00:41.530905
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:37:45.930275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.41
 ---- batch: 020 ----
mean loss: 224.92
 ---- batch: 030 ----
mean loss: 224.40
 ---- batch: 040 ----
mean loss: 224.19
train mean loss: 224.48
epoch train time: 0:00:00.687514
elapsed time: 0:00:42.218725
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:37:46.618064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.41
 ---- batch: 020 ----
mean loss: 209.27
 ---- batch: 030 ----
mean loss: 221.42
 ---- batch: 040 ----
mean loss: 211.04
train mean loss: 213.97
epoch train time: 0:00:00.667588
elapsed time: 0:00:42.886532
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:37:47.285868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.79
 ---- batch: 020 ----
mean loss: 204.08
 ---- batch: 030 ----
mean loss: 203.73
 ---- batch: 040 ----
mean loss: 191.95
train mean loss: 200.93
epoch train time: 0:00:00.677143
elapsed time: 0:00:43.563891
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:37:47.963221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.18
 ---- batch: 020 ----
mean loss: 199.58
 ---- batch: 030 ----
mean loss: 188.12
 ---- batch: 040 ----
mean loss: 185.58
train mean loss: 193.52
epoch train time: 0:00:00.678869
elapsed time: 0:00:44.242998
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:37:48.642393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.46
 ---- batch: 020 ----
mean loss: 185.30
 ---- batch: 030 ----
mean loss: 183.68
 ---- batch: 040 ----
mean loss: 180.28
train mean loss: 184.91
epoch train time: 0:00:00.698058
elapsed time: 0:00:44.941371
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:37:49.340730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.89
 ---- batch: 020 ----
mean loss: 178.27
 ---- batch: 030 ----
mean loss: 178.49
 ---- batch: 040 ----
mean loss: 172.59
train mean loss: 177.19
epoch train time: 0:00:00.710325
elapsed time: 0:00:45.651947
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:37:50.051322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.65
 ---- batch: 020 ----
mean loss: 170.70
 ---- batch: 030 ----
mean loss: 174.05
 ---- batch: 040 ----
mean loss: 169.22
train mean loss: 170.31
epoch train time: 0:00:00.675176
elapsed time: 0:00:46.327415
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:37:50.726768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.37
 ---- batch: 020 ----
mean loss: 161.81
 ---- batch: 030 ----
mean loss: 162.87
 ---- batch: 040 ----
mean loss: 161.50
train mean loss: 163.35
epoch train time: 0:00:00.679213
elapsed time: 0:00:47.006842
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:37:51.406174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.48
 ---- batch: 020 ----
mean loss: 159.05
 ---- batch: 030 ----
mean loss: 158.82
 ---- batch: 040 ----
mean loss: 158.05
train mean loss: 157.29
epoch train time: 0:00:00.692832
elapsed time: 0:00:47.699896
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:37:52.099238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.16
 ---- batch: 020 ----
mean loss: 150.90
 ---- batch: 030 ----
mean loss: 145.63
 ---- batch: 040 ----
mean loss: 149.18
train mean loss: 149.50
epoch train time: 0:00:00.728226
elapsed time: 0:00:48.428454
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:37:52.827812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.76
 ---- batch: 020 ----
mean loss: 152.29
 ---- batch: 030 ----
mean loss: 149.44
 ---- batch: 040 ----
mean loss: 147.72
train mean loss: 148.78
epoch train time: 0:00:00.698383
elapsed time: 0:00:49.127099
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:37:53.526440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.81
 ---- batch: 020 ----
mean loss: 139.95
 ---- batch: 030 ----
mean loss: 140.65
 ---- batch: 040 ----
mean loss: 137.56
train mean loss: 140.17
epoch train time: 0:00:00.688861
elapsed time: 0:00:49.816169
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:37:54.215506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.99
 ---- batch: 020 ----
mean loss: 134.93
 ---- batch: 030 ----
mean loss: 137.60
 ---- batch: 040 ----
mean loss: 134.69
train mean loss: 135.85
epoch train time: 0:00:00.681355
elapsed time: 0:00:50.497746
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:37:54.897083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.56
 ---- batch: 020 ----
mean loss: 132.05
 ---- batch: 030 ----
mean loss: 130.78
 ---- batch: 040 ----
mean loss: 136.10
train mean loss: 132.82
epoch train time: 0:00:00.679047
elapsed time: 0:00:51.177001
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:37:55.576339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.70
 ---- batch: 020 ----
mean loss: 130.36
 ---- batch: 030 ----
mean loss: 133.16
 ---- batch: 040 ----
mean loss: 127.40
train mean loss: 129.76
epoch train time: 0:00:00.708193
elapsed time: 0:00:51.885446
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:37:56.284804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.66
 ---- batch: 020 ----
mean loss: 129.68
 ---- batch: 030 ----
mean loss: 128.88
 ---- batch: 040 ----
mean loss: 121.40
train mean loss: 126.68
epoch train time: 0:00:00.714645
elapsed time: 0:00:52.600362
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:37:56.999706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.13
 ---- batch: 020 ----
mean loss: 122.53
 ---- batch: 030 ----
mean loss: 118.04
 ---- batch: 040 ----
mean loss: 123.26
train mean loss: 121.96
epoch train time: 0:00:00.702984
elapsed time: 0:00:53.303598
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:37:57.702983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.90
 ---- batch: 020 ----
mean loss: 118.50
 ---- batch: 030 ----
mean loss: 119.12
 ---- batch: 040 ----
mean loss: 118.17
train mean loss: 119.16
epoch train time: 0:00:00.686467
elapsed time: 0:00:53.990364
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:37:58.389707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.18
 ---- batch: 020 ----
mean loss: 115.87
 ---- batch: 030 ----
mean loss: 116.71
 ---- batch: 040 ----
mean loss: 117.59
train mean loss: 115.84
epoch train time: 0:00:00.660238
elapsed time: 0:00:54.650942
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:37:59.050301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.44
 ---- batch: 020 ----
mean loss: 115.06
 ---- batch: 030 ----
mean loss: 111.39
 ---- batch: 040 ----
mean loss: 112.10
train mean loss: 113.84
epoch train time: 0:00:00.671896
elapsed time: 0:00:55.323180
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:37:59.722553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.96
 ---- batch: 020 ----
mean loss: 110.75
 ---- batch: 030 ----
mean loss: 109.35
 ---- batch: 040 ----
mean loss: 111.50
train mean loss: 109.52
epoch train time: 0:00:00.697519
elapsed time: 0:00:56.021040
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:38:00.420373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.39
 ---- batch: 020 ----
mean loss: 106.96
 ---- batch: 030 ----
mean loss: 102.63
 ---- batch: 040 ----
mean loss: 110.95
train mean loss: 107.50
epoch train time: 0:00:00.692109
elapsed time: 0:00:56.713384
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:38:01.112745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.25
 ---- batch: 020 ----
mean loss: 111.20
 ---- batch: 030 ----
mean loss: 109.35
 ---- batch: 040 ----
mean loss: 106.20
train mean loss: 107.62
epoch train time: 0:00:00.686738
elapsed time: 0:00:57.400375
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:38:01.799713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.15
 ---- batch: 020 ----
mean loss: 102.89
 ---- batch: 030 ----
mean loss: 104.53
 ---- batch: 040 ----
mean loss: 102.19
train mean loss: 103.99
epoch train time: 0:00:00.683810
elapsed time: 0:00:58.084390
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:38:02.483725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.13
 ---- batch: 020 ----
mean loss: 100.16
 ---- batch: 030 ----
mean loss: 102.26
 ---- batch: 040 ----
mean loss: 102.29
train mean loss: 101.55
epoch train time: 0:00:00.663842
elapsed time: 0:00:58.748435
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:38:03.147786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.64
 ---- batch: 020 ----
mean loss: 102.47
 ---- batch: 030 ----
mean loss: 99.78
 ---- batch: 040 ----
mean loss: 104.41
train mean loss: 101.82
epoch train time: 0:00:00.689369
elapsed time: 0:00:59.438116
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:38:03.837460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.37
 ---- batch: 020 ----
mean loss: 101.51
 ---- batch: 030 ----
mean loss: 100.06
 ---- batch: 040 ----
mean loss: 97.14
train mean loss: 99.27
epoch train time: 0:00:00.702420
elapsed time: 0:01:00.140791
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:38:04.540151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.70
 ---- batch: 020 ----
mean loss: 98.98
 ---- batch: 030 ----
mean loss: 98.96
 ---- batch: 040 ----
mean loss: 94.01
train mean loss: 97.58
epoch train time: 0:00:00.693061
elapsed time: 0:01:00.834160
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:38:05.233517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.24
 ---- batch: 020 ----
mean loss: 94.33
 ---- batch: 030 ----
mean loss: 96.09
 ---- batch: 040 ----
mean loss: 96.78
train mean loss: 95.91
epoch train time: 0:00:00.665564
elapsed time: 0:01:01.500007
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:38:05.899344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.96
 ---- batch: 020 ----
mean loss: 93.87
 ---- batch: 030 ----
mean loss: 91.32
 ---- batch: 040 ----
mean loss: 94.86
train mean loss: 93.50
epoch train time: 0:00:00.647144
elapsed time: 0:01:02.147370
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:38:06.546711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.89
 ---- batch: 020 ----
mean loss: 92.06
 ---- batch: 030 ----
mean loss: 92.02
 ---- batch: 040 ----
mean loss: 93.48
train mean loss: 93.35
epoch train time: 0:00:00.694076
elapsed time: 0:01:02.841683
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:38:07.241021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.70
 ---- batch: 020 ----
mean loss: 93.96
 ---- batch: 030 ----
mean loss: 93.47
 ---- batch: 040 ----
mean loss: 90.42
train mean loss: 92.79
epoch train time: 0:00:00.720166
elapsed time: 0:01:03.562098
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:38:07.961455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.75
 ---- batch: 020 ----
mean loss: 94.84
 ---- batch: 030 ----
mean loss: 86.73
 ---- batch: 040 ----
mean loss: 89.71
train mean loss: 89.36
epoch train time: 0:00:00.695144
elapsed time: 0:01:04.257509
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:38:08.656890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.61
 ---- batch: 020 ----
mean loss: 85.24
 ---- batch: 030 ----
mean loss: 92.48
 ---- batch: 040 ----
mean loss: 90.34
train mean loss: 89.18
epoch train time: 0:00:00.701478
elapsed time: 0:01:04.959261
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:38:09.358611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.63
 ---- batch: 020 ----
mean loss: 88.36
 ---- batch: 030 ----
mean loss: 88.06
 ---- batch: 040 ----
mean loss: 87.85
train mean loss: 87.63
epoch train time: 0:00:00.669430
elapsed time: 0:01:05.628917
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:38:10.028251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.86
 ---- batch: 020 ----
mean loss: 87.69
 ---- batch: 030 ----
mean loss: 86.37
 ---- batch: 040 ----
mean loss: 86.15
train mean loss: 87.26
epoch train time: 0:00:00.664217
elapsed time: 0:01:06.293341
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:38:10.692677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.94
 ---- batch: 020 ----
mean loss: 80.58
 ---- batch: 030 ----
mean loss: 92.54
 ---- batch: 040 ----
mean loss: 86.90
train mean loss: 85.80
epoch train time: 0:00:00.706113
elapsed time: 0:01:06.999743
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:38:11.399086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.27
 ---- batch: 020 ----
mean loss: 85.11
 ---- batch: 030 ----
mean loss: 86.19
 ---- batch: 040 ----
mean loss: 84.03
train mean loss: 85.16
epoch train time: 0:00:00.707898
elapsed time: 0:01:07.707881
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:38:12.107236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.92
 ---- batch: 020 ----
mean loss: 83.88
 ---- batch: 030 ----
mean loss: 87.62
 ---- batch: 040 ----
mean loss: 85.13
train mean loss: 84.75
epoch train time: 0:00:00.691215
elapsed time: 0:01:08.399329
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:38:12.798695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.01
 ---- batch: 020 ----
mean loss: 83.23
 ---- batch: 030 ----
mean loss: 83.45
 ---- batch: 040 ----
mean loss: 84.43
train mean loss: 83.20
epoch train time: 0:00:00.655433
elapsed time: 0:01:09.055004
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:38:13.454336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.01
 ---- batch: 020 ----
mean loss: 79.24
 ---- batch: 030 ----
mean loss: 80.78
 ---- batch: 040 ----
mean loss: 82.56
train mean loss: 81.76
epoch train time: 0:00:00.651095
elapsed time: 0:01:09.706296
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:38:14.105622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.16
 ---- batch: 020 ----
mean loss: 83.55
 ---- batch: 030 ----
mean loss: 82.30
 ---- batch: 040 ----
mean loss: 82.16
train mean loss: 81.38
epoch train time: 0:00:00.671014
elapsed time: 0:01:10.377572
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:38:14.776927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.92
 ---- batch: 020 ----
mean loss: 78.10
 ---- batch: 030 ----
mean loss: 78.69
 ---- batch: 040 ----
mean loss: 82.61
train mean loss: 80.75
epoch train time: 0:00:00.703851
elapsed time: 0:01:11.081681
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:38:15.481022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.44
 ---- batch: 020 ----
mean loss: 80.12
 ---- batch: 030 ----
mean loss: 81.49
 ---- batch: 040 ----
mean loss: 82.21
train mean loss: 80.74
epoch train time: 0:00:00.688663
elapsed time: 0:01:11.770559
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:38:16.169891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.26
 ---- batch: 020 ----
mean loss: 78.35
 ---- batch: 030 ----
mean loss: 81.10
 ---- batch: 040 ----
mean loss: 78.04
train mean loss: 79.66
epoch train time: 0:00:00.669816
elapsed time: 0:01:12.440583
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:38:16.839918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.12
 ---- batch: 020 ----
mean loss: 79.17
 ---- batch: 030 ----
mean loss: 77.42
 ---- batch: 040 ----
mean loss: 77.98
train mean loss: 78.90
epoch train time: 0:00:00.665654
elapsed time: 0:01:13.106442
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:38:17.505809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.84
 ---- batch: 020 ----
mean loss: 77.99
 ---- batch: 030 ----
mean loss: 77.19
 ---- batch: 040 ----
mean loss: 78.65
train mean loss: 77.96
epoch train time: 0:00:00.672075
elapsed time: 0:01:13.778766
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:38:18.178103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.97
 ---- batch: 020 ----
mean loss: 75.94
 ---- batch: 030 ----
mean loss: 76.54
 ---- batch: 040 ----
mean loss: 76.05
train mean loss: 76.73
epoch train time: 0:00:00.707222
elapsed time: 0:01:14.486252
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:38:18.885605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.05
 ---- batch: 020 ----
mean loss: 76.98
 ---- batch: 030 ----
mean loss: 73.41
 ---- batch: 040 ----
mean loss: 75.33
train mean loss: 75.97
epoch train time: 0:00:00.703089
elapsed time: 0:01:15.189582
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:38:19.588923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.44
 ---- batch: 020 ----
mean loss: 75.59
 ---- batch: 030 ----
mean loss: 77.54
 ---- batch: 040 ----
mean loss: 76.97
train mean loss: 76.31
epoch train time: 0:00:00.682459
elapsed time: 0:01:15.872302
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:38:20.271656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.36
 ---- batch: 020 ----
mean loss: 76.78
 ---- batch: 030 ----
mean loss: 71.99
 ---- batch: 040 ----
mean loss: 75.61
train mean loss: 74.64
epoch train time: 0:00:00.683030
elapsed time: 0:01:16.555602
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:38:20.954956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.17
 ---- batch: 020 ----
mean loss: 77.33
 ---- batch: 030 ----
mean loss: 74.78
 ---- batch: 040 ----
mean loss: 73.26
train mean loss: 74.82
epoch train time: 0:00:00.673552
elapsed time: 0:01:17.229413
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:38:21.628751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.97
 ---- batch: 020 ----
mean loss: 73.03
 ---- batch: 030 ----
mean loss: 75.04
 ---- batch: 040 ----
mean loss: 73.59
train mean loss: 73.78
epoch train time: 0:00:00.714847
elapsed time: 0:01:17.944521
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:38:22.343871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.74
 ---- batch: 020 ----
mean loss: 75.95
 ---- batch: 030 ----
mean loss: 69.93
 ---- batch: 040 ----
mean loss: 74.35
train mean loss: 73.00
epoch train time: 0:00:00.723061
elapsed time: 0:01:18.667832
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:38:23.067182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.26
 ---- batch: 020 ----
mean loss: 74.22
 ---- batch: 030 ----
mean loss: 72.60
 ---- batch: 040 ----
mean loss: 73.33
train mean loss: 72.77
epoch train time: 0:00:00.684029
elapsed time: 0:01:19.352102
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:38:23.751457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.97
 ---- batch: 020 ----
mean loss: 72.45
 ---- batch: 030 ----
mean loss: 72.85
 ---- batch: 040 ----
mean loss: 71.97
train mean loss: 72.40
epoch train time: 0:00:00.657667
elapsed time: 0:01:20.009989
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:38:24.409345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.79
 ---- batch: 020 ----
mean loss: 74.91
 ---- batch: 030 ----
mean loss: 67.84
 ---- batch: 040 ----
mean loss: 71.65
train mean loss: 72.21
epoch train time: 0:00:00.661656
elapsed time: 0:01:20.671866
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:38:25.071203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.17
 ---- batch: 020 ----
mean loss: 68.70
 ---- batch: 030 ----
mean loss: 70.25
 ---- batch: 040 ----
mean loss: 72.82
train mean loss: 71.16
epoch train time: 0:00:00.686128
elapsed time: 0:01:21.358244
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:38:25.757641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.13
 ---- batch: 020 ----
mean loss: 71.28
 ---- batch: 030 ----
mean loss: 68.05
 ---- batch: 040 ----
mean loss: 70.64
train mean loss: 70.21
epoch train time: 0:00:00.696680
elapsed time: 0:01:22.055207
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:38:26.454544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.01
 ---- batch: 020 ----
mean loss: 72.83
 ---- batch: 030 ----
mean loss: 68.50
 ---- batch: 040 ----
mean loss: 68.10
train mean loss: 70.22
epoch train time: 0:00:00.701221
elapsed time: 0:01:22.756676
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:38:27.156015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.36
 ---- batch: 020 ----
mean loss: 70.47
 ---- batch: 030 ----
mean loss: 71.75
 ---- batch: 040 ----
mean loss: 70.97
train mean loss: 70.43
epoch train time: 0:00:00.677136
elapsed time: 0:01:23.434025
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:38:27.833362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.88
 ---- batch: 020 ----
mean loss: 67.20
 ---- batch: 030 ----
mean loss: 67.30
 ---- batch: 040 ----
mean loss: 70.67
train mean loss: 68.60
epoch train time: 0:00:00.665201
elapsed time: 0:01:24.099429
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:38:28.498764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.69
 ---- batch: 020 ----
mean loss: 65.00
 ---- batch: 030 ----
mean loss: 69.71
 ---- batch: 040 ----
mean loss: 68.60
train mean loss: 67.82
epoch train time: 0:00:00.684783
elapsed time: 0:01:24.784497
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:38:29.183925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.94
 ---- batch: 020 ----
mean loss: 69.10
 ---- batch: 030 ----
mean loss: 68.64
 ---- batch: 040 ----
mean loss: 67.29
train mean loss: 68.21
epoch train time: 0:00:00.727029
elapsed time: 0:01:25.511913
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:38:29.911263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.50
 ---- batch: 020 ----
mean loss: 68.77
 ---- batch: 030 ----
mean loss: 64.76
 ---- batch: 040 ----
mean loss: 70.04
train mean loss: 68.06
epoch train time: 0:00:00.708727
elapsed time: 0:01:26.220926
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:38:30.620293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.23
 ---- batch: 020 ----
mean loss: 69.01
 ---- batch: 030 ----
mean loss: 66.14
 ---- batch: 040 ----
mean loss: 69.04
train mean loss: 67.67
epoch train time: 0:00:00.679889
elapsed time: 0:01:26.901103
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:38:31.300470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.82
 ---- batch: 020 ----
mean loss: 66.32
 ---- batch: 030 ----
mean loss: 68.99
 ---- batch: 040 ----
mean loss: 67.99
train mean loss: 67.76
epoch train time: 0:00:00.662142
elapsed time: 0:01:27.563518
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:38:31.962819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.55
 ---- batch: 020 ----
mean loss: 67.34
 ---- batch: 030 ----
mean loss: 64.96
 ---- batch: 040 ----
mean loss: 65.48
train mean loss: 66.55
epoch train time: 0:00:00.664203
elapsed time: 0:01:28.227918
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:38:32.627259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.08
 ---- batch: 020 ----
mean loss: 63.24
 ---- batch: 030 ----
mean loss: 64.50
 ---- batch: 040 ----
mean loss: 70.42
train mean loss: 65.69
epoch train time: 0:00:00.717811
elapsed time: 0:01:28.945984
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:38:33.345343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.46
 ---- batch: 020 ----
mean loss: 65.61
 ---- batch: 030 ----
mean loss: 68.81
 ---- batch: 040 ----
mean loss: 62.63
train mean loss: 67.17
epoch train time: 0:00:00.711879
elapsed time: 0:01:29.658132
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:38:34.057486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.34
 ---- batch: 020 ----
mean loss: 64.79
 ---- batch: 030 ----
mean loss: 65.96
 ---- batch: 040 ----
mean loss: 66.23
train mean loss: 65.16
epoch train time: 0:00:00.689718
elapsed time: 0:01:30.348085
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:38:34.747425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.23
 ---- batch: 020 ----
mean loss: 65.54
 ---- batch: 030 ----
mean loss: 66.43
 ---- batch: 040 ----
mean loss: 62.98
train mean loss: 65.07
epoch train time: 0:00:00.678173
elapsed time: 0:01:31.026490
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:38:35.425844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.23
 ---- batch: 020 ----
mean loss: 65.70
 ---- batch: 030 ----
mean loss: 66.23
 ---- batch: 040 ----
mean loss: 63.26
train mean loss: 64.58
epoch train time: 0:00:00.669051
elapsed time: 0:01:31.695768
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:38:36.095102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.98
 ---- batch: 020 ----
mean loss: 67.41
 ---- batch: 030 ----
mean loss: 65.06
 ---- batch: 040 ----
mean loss: 61.93
train mean loss: 64.69
epoch train time: 0:00:00.685850
elapsed time: 0:01:32.381945
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:38:36.781299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.08
 ---- batch: 020 ----
mean loss: 63.75
 ---- batch: 030 ----
mean loss: 67.14
 ---- batch: 040 ----
mean loss: 64.82
train mean loss: 64.94
epoch train time: 0:00:00.696186
elapsed time: 0:01:33.078405
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:38:37.477774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.35
 ---- batch: 020 ----
mean loss: 64.44
 ---- batch: 030 ----
mean loss: 63.79
 ---- batch: 040 ----
mean loss: 61.30
train mean loss: 64.54
epoch train time: 0:00:00.697382
elapsed time: 0:01:33.776042
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:38:38.175415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.09
 ---- batch: 020 ----
mean loss: 63.02
 ---- batch: 030 ----
mean loss: 64.17
 ---- batch: 040 ----
mean loss: 67.14
train mean loss: 63.73
epoch train time: 0:00:00.684531
elapsed time: 0:01:34.460832
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:38:38.860172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.58
 ---- batch: 020 ----
mean loss: 63.53
 ---- batch: 030 ----
mean loss: 63.12
 ---- batch: 040 ----
mean loss: 61.51
train mean loss: 63.41
epoch train time: 0:00:00.646437
elapsed time: 0:01:35.107495
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:38:39.506838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.27
 ---- batch: 020 ----
mean loss: 61.34
 ---- batch: 030 ----
mean loss: 64.78
 ---- batch: 040 ----
mean loss: 63.68
train mean loss: 63.25
epoch train time: 0:00:00.653205
elapsed time: 0:01:35.760903
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:38:40.160270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.80
 ---- batch: 020 ----
mean loss: 62.51
 ---- batch: 030 ----
mean loss: 63.30
 ---- batch: 040 ----
mean loss: 61.39
train mean loss: 62.07
epoch train time: 0:00:00.682901
elapsed time: 0:01:36.444094
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:38:40.843436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.71
 ---- batch: 020 ----
mean loss: 62.49
 ---- batch: 030 ----
mean loss: 64.26
 ---- batch: 040 ----
mean loss: 61.81
train mean loss: 61.92
epoch train time: 0:00:00.706719
elapsed time: 0:01:37.151094
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:38:41.550444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.78
 ---- batch: 020 ----
mean loss: 63.25
 ---- batch: 030 ----
mean loss: 57.25
 ---- batch: 040 ----
mean loss: 62.34
train mean loss: 60.99
epoch train time: 0:00:00.691110
elapsed time: 0:01:37.842465
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:38:42.241833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.58
 ---- batch: 020 ----
mean loss: 61.95
 ---- batch: 030 ----
mean loss: 60.80
 ---- batch: 040 ----
mean loss: 58.46
train mean loss: 60.89
epoch train time: 0:00:00.663895
elapsed time: 0:01:38.506598
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:38:42.905928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.16
 ---- batch: 020 ----
mean loss: 59.92
 ---- batch: 030 ----
mean loss: 61.77
 ---- batch: 040 ----
mean loss: 59.40
train mean loss: 60.59
epoch train time: 0:00:00.651276
elapsed time: 0:01:39.158073
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:38:43.557406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.66
 ---- batch: 020 ----
mean loss: 59.15
 ---- batch: 030 ----
mean loss: 58.84
 ---- batch: 040 ----
mean loss: 59.80
train mean loss: 60.29
epoch train time: 0:00:00.662767
elapsed time: 0:01:39.821110
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:38:44.220473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.30
 ---- batch: 020 ----
mean loss: 58.59
 ---- batch: 030 ----
mean loss: 62.84
 ---- batch: 040 ----
mean loss: 59.15
train mean loss: 59.73
epoch train time: 0:00:00.698611
elapsed time: 0:01:40.519985
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:38:44.919360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.85
 ---- batch: 020 ----
mean loss: 58.61
 ---- batch: 030 ----
mean loss: 60.32
 ---- batch: 040 ----
mean loss: 59.05
train mean loss: 59.65
epoch train time: 0:00:00.688172
elapsed time: 0:01:41.208499
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:38:45.607820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.80
 ---- batch: 020 ----
mean loss: 56.18
 ---- batch: 030 ----
mean loss: 59.11
 ---- batch: 040 ----
mean loss: 59.58
train mean loss: 59.22
epoch train time: 0:00:00.687598
elapsed time: 0:01:41.896294
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:38:46.295626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.62
 ---- batch: 020 ----
mean loss: 59.48
 ---- batch: 030 ----
mean loss: 58.56
 ---- batch: 040 ----
mean loss: 59.72
train mean loss: 58.63
epoch train time: 0:00:00.655604
elapsed time: 0:01:42.552092
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:38:46.951432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.56
 ---- batch: 020 ----
mean loss: 58.98
 ---- batch: 030 ----
mean loss: 59.77
 ---- batch: 040 ----
mean loss: 55.10
train mean loss: 58.64
epoch train time: 0:00:00.658752
elapsed time: 0:01:43.211080
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:38:47.610421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.41
 ---- batch: 020 ----
mean loss: 60.69
 ---- batch: 030 ----
mean loss: 57.34
 ---- batch: 040 ----
mean loss: 60.09
train mean loss: 59.31
epoch train time: 0:00:00.704132
elapsed time: 0:01:43.915486
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:38:48.314827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.17
 ---- batch: 020 ----
mean loss: 59.46
 ---- batch: 030 ----
mean loss: 57.71
 ---- batch: 040 ----
mean loss: 59.52
train mean loss: 59.41
epoch train time: 0:00:00.695282
elapsed time: 0:01:44.611050
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:38:49.010394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.99
 ---- batch: 020 ----
mean loss: 57.21
 ---- batch: 030 ----
mean loss: 58.95
 ---- batch: 040 ----
mean loss: 55.56
train mean loss: 58.20
epoch train time: 0:00:00.686123
elapsed time: 0:01:45.297405
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:38:49.696739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.11
 ---- batch: 020 ----
mean loss: 56.76
 ---- batch: 030 ----
mean loss: 58.39
 ---- batch: 040 ----
mean loss: 56.94
train mean loss: 57.07
epoch train time: 0:00:00.658579
elapsed time: 0:01:45.956189
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:38:50.355523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.64
 ---- batch: 020 ----
mean loss: 58.78
 ---- batch: 030 ----
mean loss: 57.69
 ---- batch: 040 ----
mean loss: 56.13
train mean loss: 57.17
epoch train time: 0:00:00.657432
elapsed time: 0:01:46.613840
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:38:51.013215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.10
 ---- batch: 020 ----
mean loss: 57.41
 ---- batch: 030 ----
mean loss: 56.20
 ---- batch: 040 ----
mean loss: 56.96
train mean loss: 56.84
epoch train time: 0:00:00.668290
elapsed time: 0:01:47.282404
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:38:51.681760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.53
 ---- batch: 020 ----
mean loss: 59.20
 ---- batch: 030 ----
mean loss: 56.13
 ---- batch: 040 ----
mean loss: 55.22
train mean loss: 56.94
epoch train time: 0:00:00.707820
elapsed time: 0:01:47.990551
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:38:52.389894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.97
 ---- batch: 020 ----
mean loss: 55.84
 ---- batch: 030 ----
mean loss: 55.85
 ---- batch: 040 ----
mean loss: 56.38
train mean loss: 56.51
epoch train time: 0:00:00.695345
elapsed time: 0:01:48.686133
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:38:53.085498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.41
 ---- batch: 020 ----
mean loss: 59.89
 ---- batch: 030 ----
mean loss: 54.17
 ---- batch: 040 ----
mean loss: 52.99
train mean loss: 55.40
epoch train time: 0:00:00.689082
elapsed time: 0:01:49.375476
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:38:53.774886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.45
 ---- batch: 020 ----
mean loss: 55.45
 ---- batch: 030 ----
mean loss: 55.65
 ---- batch: 040 ----
mean loss: 54.44
train mean loss: 55.58
epoch train time: 0:00:00.665909
elapsed time: 0:01:50.041695
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:38:54.441048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.24
 ---- batch: 020 ----
mean loss: 51.98
 ---- batch: 030 ----
mean loss: 56.93
 ---- batch: 040 ----
mean loss: 56.89
train mean loss: 55.56
epoch train time: 0:00:00.674068
elapsed time: 0:01:50.715994
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:38:55.115332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.65
 ---- batch: 020 ----
mean loss: 57.25
 ---- batch: 030 ----
mean loss: 56.33
 ---- batch: 040 ----
mean loss: 55.70
train mean loss: 55.82
epoch train time: 0:00:00.702186
elapsed time: 0:01:51.418486
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:38:55.817847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.68
 ---- batch: 020 ----
mean loss: 57.99
 ---- batch: 030 ----
mean loss: 56.03
 ---- batch: 040 ----
mean loss: 52.22
train mean loss: 54.62
epoch train time: 0:00:00.700689
elapsed time: 0:01:52.119430
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:38:56.518785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.74
 ---- batch: 020 ----
mean loss: 55.12
 ---- batch: 030 ----
mean loss: 54.69
 ---- batch: 040 ----
mean loss: 56.01
train mean loss: 54.79
epoch train time: 0:00:00.699050
elapsed time: 0:01:52.818752
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:38:57.218089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.11
 ---- batch: 020 ----
mean loss: 56.30
 ---- batch: 030 ----
mean loss: 54.00
 ---- batch: 040 ----
mean loss: 52.47
train mean loss: 54.52
epoch train time: 0:00:00.676540
elapsed time: 0:01:53.495491
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:38:57.894832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.29
 ---- batch: 020 ----
mean loss: 52.57
 ---- batch: 030 ----
mean loss: 53.80
 ---- batch: 040 ----
mean loss: 55.31
train mean loss: 54.19
epoch train time: 0:00:00.668056
elapsed time: 0:01:54.163773
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:38:58.563130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.36
 ---- batch: 020 ----
mean loss: 53.36
 ---- batch: 030 ----
mean loss: 52.45
 ---- batch: 040 ----
mean loss: 52.63
train mean loss: 52.64
epoch train time: 0:00:00.686032
elapsed time: 0:01:54.850093
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:38:59.249459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.59
 ---- batch: 020 ----
mean loss: 52.15
 ---- batch: 030 ----
mean loss: 54.37
 ---- batch: 040 ----
mean loss: 51.01
train mean loss: 53.28
epoch train time: 0:00:00.720172
elapsed time: 0:01:55.570527
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:38:59.969866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.81
 ---- batch: 020 ----
mean loss: 55.92
 ---- batch: 030 ----
mean loss: 51.50
 ---- batch: 040 ----
mean loss: 51.05
train mean loss: 52.30
epoch train time: 0:00:00.703893
elapsed time: 0:01:56.274706
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:39:00.674010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.01
 ---- batch: 020 ----
mean loss: 52.92
 ---- batch: 030 ----
mean loss: 53.76
 ---- batch: 040 ----
mean loss: 51.99
train mean loss: 52.70
epoch train time: 0:00:00.687037
elapsed time: 0:01:56.961911
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:39:01.361242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.72
 ---- batch: 020 ----
mean loss: 51.45
 ---- batch: 030 ----
mean loss: 53.63
 ---- batch: 040 ----
mean loss: 53.63
train mean loss: 52.30
epoch train time: 0:00:00.657877
elapsed time: 0:01:57.619980
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:39:02.019311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.80
 ---- batch: 020 ----
mean loss: 52.48
 ---- batch: 030 ----
mean loss: 50.76
 ---- batch: 040 ----
mean loss: 51.78
train mean loss: 51.72
epoch train time: 0:00:00.654583
elapsed time: 0:01:58.274758
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:39:02.674091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.29
 ---- batch: 020 ----
mean loss: 52.76
 ---- batch: 030 ----
mean loss: 50.75
 ---- batch: 040 ----
mean loss: 52.61
train mean loss: 52.29
epoch train time: 0:00:00.701544
elapsed time: 0:01:58.976533
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:39:03.375887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.28
 ---- batch: 020 ----
mean loss: 52.35
 ---- batch: 030 ----
mean loss: 48.38
 ---- batch: 040 ----
mean loss: 51.48
train mean loss: 51.02
epoch train time: 0:00:00.686072
elapsed time: 0:01:59.662874
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:39:04.062215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.72
 ---- batch: 020 ----
mean loss: 51.42
 ---- batch: 030 ----
mean loss: 51.27
 ---- batch: 040 ----
mean loss: 50.81
train mean loss: 51.12
epoch train time: 0:00:00.697227
elapsed time: 0:02:00.360364
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:39:04.759718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.42
 ---- batch: 020 ----
mean loss: 51.54
 ---- batch: 030 ----
mean loss: 51.72
 ---- batch: 040 ----
mean loss: 50.96
train mean loss: 50.94
epoch train time: 0:00:00.723293
elapsed time: 0:02:01.083886
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:39:05.483225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.98
 ---- batch: 020 ----
mean loss: 50.79
 ---- batch: 030 ----
mean loss: 50.95
 ---- batch: 040 ----
mean loss: 50.89
train mean loss: 50.52
epoch train time: 0:00:00.688449
elapsed time: 0:02:01.772555
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:39:06.171888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.70
 ---- batch: 020 ----
mean loss: 49.68
 ---- batch: 030 ----
mean loss: 51.39
 ---- batch: 040 ----
mean loss: 50.48
train mean loss: 49.64
epoch train time: 0:00:00.681721
elapsed time: 0:02:02.454518
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:39:06.853874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.06
 ---- batch: 020 ----
mean loss: 50.36
 ---- batch: 030 ----
mean loss: 48.85
 ---- batch: 040 ----
mean loss: 48.47
train mean loss: 49.54
epoch train time: 0:00:00.698415
elapsed time: 0:02:03.153206
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:39:07.552563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.88
 ---- batch: 020 ----
mean loss: 50.30
 ---- batch: 030 ----
mean loss: 49.34
 ---- batch: 040 ----
mean loss: 52.87
train mean loss: 50.40
epoch train time: 0:00:00.711049
elapsed time: 0:02:03.864534
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:39:08.263873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.53
 ---- batch: 020 ----
mean loss: 48.03
 ---- batch: 030 ----
mean loss: 48.57
 ---- batch: 040 ----
mean loss: 50.69
train mean loss: 48.71
epoch train time: 0:00:00.673542
elapsed time: 0:02:04.538362
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:39:08.937707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.41
 ---- batch: 020 ----
mean loss: 47.73
 ---- batch: 030 ----
mean loss: 51.03
 ---- batch: 040 ----
mean loss: 51.66
train mean loss: 50.07
epoch train time: 0:00:00.647690
elapsed time: 0:02:05.186282
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:39:09.585619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.76
 ---- batch: 020 ----
mean loss: 49.71
 ---- batch: 030 ----
mean loss: 46.98
 ---- batch: 040 ----
mean loss: 52.16
train mean loss: 48.76
epoch train time: 0:00:00.669880
elapsed time: 0:02:05.856368
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:39:10.255703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.70
 ---- batch: 020 ----
mean loss: 47.21
 ---- batch: 030 ----
mean loss: 49.55
 ---- batch: 040 ----
mean loss: 49.41
train mean loss: 48.27
epoch train time: 0:00:00.705745
elapsed time: 0:02:06.562391
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:39:10.961736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.03
 ---- batch: 020 ----
mean loss: 47.55
 ---- batch: 030 ----
mean loss: 49.34
 ---- batch: 040 ----
mean loss: 47.99
train mean loss: 48.00
epoch train time: 0:00:00.695462
elapsed time: 0:02:07.258092
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:39:11.657432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.62
 ---- batch: 020 ----
mean loss: 46.70
 ---- batch: 030 ----
mean loss: 47.74
 ---- batch: 040 ----
mean loss: 49.04
train mean loss: 47.92
epoch train time: 0:00:00.688879
elapsed time: 0:02:07.947180
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:39:12.346513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.94
 ---- batch: 020 ----
mean loss: 47.04
 ---- batch: 030 ----
mean loss: 48.85
 ---- batch: 040 ----
mean loss: 43.66
train mean loss: 47.25
epoch train time: 0:00:00.658229
elapsed time: 0:02:08.605625
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:39:13.004974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.26
 ---- batch: 020 ----
mean loss: 46.50
 ---- batch: 030 ----
mean loss: 46.91
 ---- batch: 040 ----
mean loss: 46.70
train mean loss: 47.23
epoch train time: 0:00:00.669735
elapsed time: 0:02:09.275605
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:39:13.674955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.47
 ---- batch: 020 ----
mean loss: 46.27
 ---- batch: 030 ----
mean loss: 47.71
 ---- batch: 040 ----
mean loss: 43.81
train mean loss: 46.13
epoch train time: 0:00:00.684908
elapsed time: 0:02:09.960789
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:39:14.360147
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.82
 ---- batch: 020 ----
mean loss: 44.19
 ---- batch: 030 ----
mean loss: 49.14
 ---- batch: 040 ----
mean loss: 46.21
train mean loss: 46.04
epoch train time: 0:00:00.708551
elapsed time: 0:02:10.669605
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:39:15.068949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.91
 ---- batch: 020 ----
mean loss: 44.72
 ---- batch: 030 ----
mean loss: 45.50
 ---- batch: 040 ----
mean loss: 46.33
train mean loss: 45.82
epoch train time: 0:00:00.698065
elapsed time: 0:02:11.367947
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:39:15.767334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.65
 ---- batch: 020 ----
mean loss: 42.90
 ---- batch: 030 ----
mean loss: 47.37
 ---- batch: 040 ----
mean loss: 45.14
train mean loss: 45.42
epoch train time: 0:00:00.696404
elapsed time: 0:02:12.064643
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:39:16.463982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.93
 ---- batch: 020 ----
mean loss: 46.39
 ---- batch: 030 ----
mean loss: 45.68
 ---- batch: 040 ----
mean loss: 46.39
train mean loss: 45.54
epoch train time: 0:00:00.666755
elapsed time: 0:02:12.731634
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:39:17.130971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.29
 ---- batch: 020 ----
mean loss: 47.26
 ---- batch: 030 ----
mean loss: 45.58
 ---- batch: 040 ----
mean loss: 44.52
train mean loss: 45.47
epoch train time: 0:00:00.660409
elapsed time: 0:02:13.392362
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:39:17.791665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.00
 ---- batch: 020 ----
mean loss: 45.01
 ---- batch: 030 ----
mean loss: 45.32
 ---- batch: 040 ----
mean loss: 43.59
train mean loss: 44.62
epoch train time: 0:00:00.691776
elapsed time: 0:02:14.084362
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:39:18.483732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.68
 ---- batch: 020 ----
mean loss: 44.21
 ---- batch: 030 ----
mean loss: 46.37
 ---- batch: 040 ----
mean loss: 43.28
train mean loss: 44.42
epoch train time: 0:00:00.705443
elapsed time: 0:02:14.790112
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:39:19.189458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.53
 ---- batch: 020 ----
mean loss: 43.47
 ---- batch: 030 ----
mean loss: 42.73
 ---- batch: 040 ----
mean loss: 46.40
train mean loss: 43.59
epoch train time: 0:00:00.677640
elapsed time: 0:02:15.467973
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:39:19.867322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.29
 ---- batch: 020 ----
mean loss: 42.23
 ---- batch: 030 ----
mean loss: 42.02
 ---- batch: 040 ----
mean loss: 45.60
train mean loss: 43.83
epoch train time: 0:00:00.646094
elapsed time: 0:02:16.114327
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:39:20.513657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.53
 ---- batch: 020 ----
mean loss: 43.39
 ---- batch: 030 ----
mean loss: 41.18
 ---- batch: 040 ----
mean loss: 44.16
train mean loss: 43.27
epoch train time: 0:00:00.653266
elapsed time: 0:02:16.767792
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:39:21.167127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.13
 ---- batch: 020 ----
mean loss: 44.07
 ---- batch: 030 ----
mean loss: 43.13
 ---- batch: 040 ----
mean loss: 42.18
train mean loss: 43.01
epoch train time: 0:00:00.664742
elapsed time: 0:02:17.432777
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:39:21.832134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.90
 ---- batch: 020 ----
mean loss: 44.48
 ---- batch: 030 ----
mean loss: 40.94
 ---- batch: 040 ----
mean loss: 42.40
train mean loss: 42.25
epoch train time: 0:00:00.700423
elapsed time: 0:02:18.133465
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:39:22.532812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.75
 ---- batch: 020 ----
mean loss: 42.27
 ---- batch: 030 ----
mean loss: 43.14
 ---- batch: 040 ----
mean loss: 43.31
train mean loss: 42.90
epoch train time: 0:00:00.727301
elapsed time: 0:02:18.861030
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:39:23.260376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.41
 ---- batch: 020 ----
mean loss: 43.88
 ---- batch: 030 ----
mean loss: 40.41
 ---- batch: 040 ----
mean loss: 44.01
train mean loss: 42.92
epoch train time: 0:00:00.680267
elapsed time: 0:02:19.541510
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:39:23.940871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.18
 ---- batch: 020 ----
mean loss: 42.72
 ---- batch: 030 ----
mean loss: 41.60
 ---- batch: 040 ----
mean loss: 40.79
train mean loss: 41.51
epoch train time: 0:00:00.666941
elapsed time: 0:02:20.208684
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:39:24.608021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.79
 ---- batch: 020 ----
mean loss: 43.63
 ---- batch: 030 ----
mean loss: 39.29
 ---- batch: 040 ----
mean loss: 42.44
train mean loss: 40.93
epoch train time: 0:00:00.658115
elapsed time: 0:02:20.867008
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:39:25.266416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.29
 ---- batch: 020 ----
mean loss: 39.00
 ---- batch: 030 ----
mean loss: 44.87
 ---- batch: 040 ----
mean loss: 41.21
train mean loss: 41.62
epoch train time: 0:00:00.702818
elapsed time: 0:02:21.570143
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:39:25.969485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.64
 ---- batch: 020 ----
mean loss: 43.30
 ---- batch: 030 ----
mean loss: 43.38
 ---- batch: 040 ----
mean loss: 36.13
train mean loss: 41.60
epoch train time: 0:00:00.688504
elapsed time: 0:02:22.258893
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:39:26.658228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.47
 ---- batch: 020 ----
mean loss: 40.19
 ---- batch: 030 ----
mean loss: 40.45
 ---- batch: 040 ----
mean loss: 42.32
train mean loss: 40.77
epoch train time: 0:00:00.704919
elapsed time: 0:02:22.964026
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:39:27.363401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.45
 ---- batch: 020 ----
mean loss: 39.90
 ---- batch: 030 ----
mean loss: 41.94
 ---- batch: 040 ----
mean loss: 42.62
train mean loss: 40.64
epoch train time: 0:00:00.672151
elapsed time: 0:02:23.636422
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:39:28.035760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.74
 ---- batch: 020 ----
mean loss: 39.48
 ---- batch: 030 ----
mean loss: 41.54
 ---- batch: 040 ----
mean loss: 40.53
train mean loss: 39.89
epoch train time: 0:00:00.671000
elapsed time: 0:02:24.307638
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:39:28.706971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.20
 ---- batch: 020 ----
mean loss: 39.74
 ---- batch: 030 ----
mean loss: 37.85
 ---- batch: 040 ----
mean loss: 40.49
train mean loss: 39.45
epoch train time: 0:00:00.686696
elapsed time: 0:02:24.994591
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:39:29.393961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.25
 ---- batch: 020 ----
mean loss: 37.91
 ---- batch: 030 ----
mean loss: 40.94
 ---- batch: 040 ----
mean loss: 38.97
train mean loss: 39.43
epoch train time: 0:00:00.700308
elapsed time: 0:02:25.695199
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:39:30.094540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.87
 ---- batch: 020 ----
mean loss: 39.07
 ---- batch: 030 ----
mean loss: 39.98
 ---- batch: 040 ----
mean loss: 38.70
train mean loss: 39.60
epoch train time: 0:00:00.710683
elapsed time: 0:02:26.406173
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:39:30.805551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.22
 ---- batch: 020 ----
mean loss: 38.13
 ---- batch: 030 ----
mean loss: 40.25
 ---- batch: 040 ----
mean loss: 37.34
train mean loss: 38.73
epoch train time: 0:00:00.677054
elapsed time: 0:02:27.083583
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:39:31.482973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.99
 ---- batch: 020 ----
mean loss: 39.07
 ---- batch: 030 ----
mean loss: 39.01
 ---- batch: 040 ----
mean loss: 36.64
train mean loss: 38.52
epoch train time: 0:00:00.684944
elapsed time: 0:02:27.768798
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:39:32.168149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.79
 ---- batch: 020 ----
mean loss: 37.59
 ---- batch: 030 ----
mean loss: 39.89
 ---- batch: 040 ----
mean loss: 39.36
train mean loss: 38.60
epoch train time: 0:00:00.690264
elapsed time: 0:02:28.459320
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:39:32.858673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.26
 ---- batch: 020 ----
mean loss: 39.72
 ---- batch: 030 ----
mean loss: 38.17
 ---- batch: 040 ----
mean loss: 37.00
train mean loss: 38.15
epoch train time: 0:00:00.725338
elapsed time: 0:02:29.184923
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:39:33.584267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.35
 ---- batch: 020 ----
mean loss: 36.87
 ---- batch: 030 ----
mean loss: 38.19
 ---- batch: 040 ----
mean loss: 38.34
train mean loss: 37.46
epoch train time: 0:00:00.721482
elapsed time: 0:02:29.906667
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:39:34.306011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.30
 ---- batch: 020 ----
mean loss: 37.17
 ---- batch: 030 ----
mean loss: 37.82
 ---- batch: 040 ----
mean loss: 37.88
train mean loss: 37.57
epoch train time: 0:00:00.702184
elapsed time: 0:02:30.609088
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:39:35.008474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.00
 ---- batch: 020 ----
mean loss: 36.69
 ---- batch: 030 ----
mean loss: 37.43
 ---- batch: 040 ----
mean loss: 37.41
train mean loss: 36.95
epoch train time: 0:00:00.683625
elapsed time: 0:02:31.292993
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:39:35.692340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.68
 ---- batch: 020 ----
mean loss: 36.14
 ---- batch: 030 ----
mean loss: 36.77
 ---- batch: 040 ----
mean loss: 37.86
train mean loss: 36.80
epoch train time: 0:00:00.656891
elapsed time: 0:02:31.950120
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:39:36.349477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.49
 ---- batch: 020 ----
mean loss: 35.75
 ---- batch: 030 ----
mean loss: 36.20
 ---- batch: 040 ----
mean loss: 37.14
train mean loss: 35.55
epoch train time: 0:00:00.697913
elapsed time: 0:02:32.648417
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:39:37.047737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.40
 ---- batch: 020 ----
mean loss: 36.05
 ---- batch: 030 ----
mean loss: 36.12
 ---- batch: 040 ----
mean loss: 35.07
train mean loss: 35.28
epoch train time: 0:00:00.718669
elapsed time: 0:02:33.367307
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:39:37.766649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.74
 ---- batch: 020 ----
mean loss: 35.39
 ---- batch: 030 ----
mean loss: 34.67
 ---- batch: 040 ----
mean loss: 34.59
train mean loss: 34.99
epoch train time: 0:00:00.685444
elapsed time: 0:02:34.052967
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:39:38.452299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.85
 ---- batch: 020 ----
mean loss: 35.39
 ---- batch: 030 ----
mean loss: 35.16
 ---- batch: 040 ----
mean loss: 34.98
train mean loss: 35.20
epoch train time: 0:00:00.655963
elapsed time: 0:02:34.709193
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:39:39.108532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.20
 ---- batch: 020 ----
mean loss: 33.62
 ---- batch: 030 ----
mean loss: 36.63
 ---- batch: 040 ----
mean loss: 35.18
train mean loss: 35.37
epoch train time: 0:00:00.653895
elapsed time: 0:02:35.363331
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:39:39.762659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.81
 ---- batch: 020 ----
mean loss: 34.58
 ---- batch: 030 ----
mean loss: 36.04
 ---- batch: 040 ----
mean loss: 34.07
train mean loss: 35.18
epoch train time: 0:00:00.661946
elapsed time: 0:02:36.025517
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:39:40.424870
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.23
 ---- batch: 020 ----
mean loss: 34.38
 ---- batch: 030 ----
mean loss: 35.15
 ---- batch: 040 ----
mean loss: 34.29
train mean loss: 35.37
epoch train time: 0:00:00.702332
elapsed time: 0:02:36.728132
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:39:41.127478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.37
 ---- batch: 020 ----
mean loss: 35.07
 ---- batch: 030 ----
mean loss: 34.55
 ---- batch: 040 ----
mean loss: 33.97
train mean loss: 34.96
epoch train time: 0:00:00.721823
elapsed time: 0:02:37.450197
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:39:41.849554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.49
 ---- batch: 020 ----
mean loss: 36.50
 ---- batch: 030 ----
mean loss: 33.44
 ---- batch: 040 ----
mean loss: 36.43
train mean loss: 35.11
epoch train time: 0:00:00.673302
elapsed time: 0:02:38.123759
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:39:42.523112
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.14
 ---- batch: 020 ----
mean loss: 35.86
 ---- batch: 030 ----
mean loss: 35.71
 ---- batch: 040 ----
mean loss: 33.47
train mean loss: 35.01
epoch train time: 0:00:00.691879
elapsed time: 0:02:38.815860
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:39:43.215194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.71
 ---- batch: 020 ----
mean loss: 35.30
 ---- batch: 030 ----
mean loss: 34.16
 ---- batch: 040 ----
mean loss: 34.96
train mean loss: 35.17
epoch train time: 0:00:00.662244
elapsed time: 0:02:39.478303
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:39:43.877635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.19
 ---- batch: 020 ----
mean loss: 33.83
 ---- batch: 030 ----
mean loss: 34.03
 ---- batch: 040 ----
mean loss: 35.47
train mean loss: 35.28
epoch train time: 0:00:00.700909
elapsed time: 0:02:40.179450
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:39:44.578808
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.43
 ---- batch: 020 ----
mean loss: 33.16
 ---- batch: 030 ----
mean loss: 34.53
 ---- batch: 040 ----
mean loss: 35.95
train mean loss: 34.91
epoch train time: 0:00:00.713816
elapsed time: 0:02:40.893538
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:39:45.292895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.58
 ---- batch: 020 ----
mean loss: 35.60
 ---- batch: 030 ----
mean loss: 35.74
 ---- batch: 040 ----
mean loss: 35.54
train mean loss: 35.39
epoch train time: 0:00:00.690870
elapsed time: 0:02:41.584800
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:39:45.984160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.57
 ---- batch: 020 ----
mean loss: 33.83
 ---- batch: 030 ----
mean loss: 35.79
 ---- batch: 040 ----
mean loss: 35.36
train mean loss: 35.14
epoch train time: 0:00:00.655086
elapsed time: 0:02:42.240128
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:39:46.639461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.23
 ---- batch: 020 ----
mean loss: 35.12
 ---- batch: 030 ----
mean loss: 34.86
 ---- batch: 040 ----
mean loss: 33.73
train mean loss: 35.14
epoch train time: 0:00:00.689427
elapsed time: 0:02:42.929778
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:39:47.329114
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.69
 ---- batch: 020 ----
mean loss: 34.47
 ---- batch: 030 ----
mean loss: 35.46
 ---- batch: 040 ----
mean loss: 34.18
train mean loss: 34.83
epoch train time: 0:00:00.690241
elapsed time: 0:02:43.620300
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:39:48.019660
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.54
 ---- batch: 020 ----
mean loss: 36.40
 ---- batch: 030 ----
mean loss: 34.16
 ---- batch: 040 ----
mean loss: 35.47
train mean loss: 34.77
epoch train time: 0:00:00.715771
elapsed time: 0:02:44.336363
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:39:48.735730
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.46
 ---- batch: 020 ----
mean loss: 36.62
 ---- batch: 030 ----
mean loss: 35.01
 ---- batch: 040 ----
mean loss: 33.77
train mean loss: 35.15
epoch train time: 0:00:00.696960
elapsed time: 0:02:45.033578
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:39:49.432916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.80
 ---- batch: 020 ----
mean loss: 35.52
 ---- batch: 030 ----
mean loss: 35.83
 ---- batch: 040 ----
mean loss: 34.45
train mean loss: 34.88
epoch train time: 0:00:00.661038
elapsed time: 0:02:45.694861
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:39:50.094231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.07
 ---- batch: 020 ----
mean loss: 34.51
 ---- batch: 030 ----
mean loss: 35.55
 ---- batch: 040 ----
mean loss: 35.81
train mean loss: 34.96
epoch train time: 0:00:00.670094
elapsed time: 0:02:46.365214
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:39:50.764552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.07
 ---- batch: 020 ----
mean loss: 35.93
 ---- batch: 030 ----
mean loss: 33.54
 ---- batch: 040 ----
mean loss: 35.46
train mean loss: 34.62
epoch train time: 0:00:00.652346
elapsed time: 0:02:47.017849
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:39:51.417195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.68
 ---- batch: 020 ----
mean loss: 33.81
 ---- batch: 030 ----
mean loss: 34.43
 ---- batch: 040 ----
mean loss: 34.48
train mean loss: 34.36
epoch train time: 0:00:00.698545
elapsed time: 0:02:47.716682
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:39:52.116025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.66
 ---- batch: 020 ----
mean loss: 36.04
 ---- batch: 030 ----
mean loss: 33.01
 ---- batch: 040 ----
mean loss: 35.03
train mean loss: 34.84
epoch train time: 0:00:00.723949
elapsed time: 0:02:48.440911
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:39:52.840261
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.43
 ---- batch: 020 ----
mean loss: 34.68
 ---- batch: 030 ----
mean loss: 35.83
 ---- batch: 040 ----
mean loss: 34.29
train mean loss: 34.56
epoch train time: 0:00:00.703871
elapsed time: 0:02:49.145006
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:39:53.544352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.81
 ---- batch: 020 ----
mean loss: 35.78
 ---- batch: 030 ----
mean loss: 34.41
 ---- batch: 040 ----
mean loss: 35.22
train mean loss: 34.60
epoch train time: 0:00:00.690767
elapsed time: 0:02:49.836002
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:39:54.235342
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.22
 ---- batch: 020 ----
mean loss: 34.40
 ---- batch: 030 ----
mean loss: 35.28
 ---- batch: 040 ----
mean loss: 34.37
train mean loss: 34.50
epoch train time: 0:00:00.692231
elapsed time: 0:02:50.528459
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:39:54.927794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.23
 ---- batch: 020 ----
mean loss: 32.62
 ---- batch: 030 ----
mean loss: 36.36
 ---- batch: 040 ----
mean loss: 33.76
train mean loss: 34.21
epoch train time: 0:00:00.680912
elapsed time: 0:02:51.209624
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:39:55.608964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.09
 ---- batch: 020 ----
mean loss: 34.54
 ---- batch: 030 ----
mean loss: 34.58
 ---- batch: 040 ----
mean loss: 35.66
train mean loss: 34.51
epoch train time: 0:00:00.691085
elapsed time: 0:02:51.900996
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:39:56.300352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.18
 ---- batch: 020 ----
mean loss: 35.51
 ---- batch: 030 ----
mean loss: 33.65
 ---- batch: 040 ----
mean loss: 33.09
train mean loss: 34.58
epoch train time: 0:00:00.704551
elapsed time: 0:02:52.605772
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:39:57.005122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.73
 ---- batch: 020 ----
mean loss: 32.15
 ---- batch: 030 ----
mean loss: 35.38
 ---- batch: 040 ----
mean loss: 35.72
train mean loss: 34.56
epoch train time: 0:00:00.664461
elapsed time: 0:02:53.270467
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:39:57.669816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.17
 ---- batch: 020 ----
mean loss: 35.18
 ---- batch: 030 ----
mean loss: 34.96
 ---- batch: 040 ----
mean loss: 32.05
train mean loss: 34.34
epoch train time: 0:00:00.686918
elapsed time: 0:02:53.957630
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:39:58.356972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.77
 ---- batch: 020 ----
mean loss: 35.96
 ---- batch: 030 ----
mean loss: 33.54
 ---- batch: 040 ----
mean loss: 33.95
train mean loss: 34.58
epoch train time: 0:00:00.697067
elapsed time: 0:02:54.654963
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:39:59.054274
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.97
 ---- batch: 020 ----
mean loss: 34.05
 ---- batch: 030 ----
mean loss: 35.78
 ---- batch: 040 ----
mean loss: 34.10
train mean loss: 34.26
epoch train time: 0:00:00.714280
elapsed time: 0:02:55.369481
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:39:59.768822
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.81
 ---- batch: 020 ----
mean loss: 33.49
 ---- batch: 030 ----
mean loss: 32.56
 ---- batch: 040 ----
mean loss: 35.05
train mean loss: 34.21
epoch train time: 0:00:00.688824
elapsed time: 0:02:56.058563
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:40:00.457913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.00
 ---- batch: 020 ----
mean loss: 34.36
 ---- batch: 030 ----
mean loss: 34.48
 ---- batch: 040 ----
mean loss: 33.74
train mean loss: 34.23
epoch train time: 0:00:00.678148
elapsed time: 0:02:56.736931
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:40:01.136265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.86
 ---- batch: 020 ----
mean loss: 32.91
 ---- batch: 030 ----
mean loss: 34.29
 ---- batch: 040 ----
mean loss: 34.29
train mean loss: 34.35
epoch train time: 0:00:00.668840
elapsed time: 0:02:57.405993
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:40:01.805349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.09
 ---- batch: 020 ----
mean loss: 34.43
 ---- batch: 030 ----
mean loss: 33.41
 ---- batch: 040 ----
mean loss: 34.47
train mean loss: 34.07
epoch train time: 0:00:00.661198
elapsed time: 0:02:58.067439
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:40:02.466776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.92
 ---- batch: 020 ----
mean loss: 34.46
 ---- batch: 030 ----
mean loss: 35.46
 ---- batch: 040 ----
mean loss: 35.85
train mean loss: 34.44
epoch train time: 0:00:00.706435
elapsed time: 0:02:58.774118
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:40:03.173456
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.77
 ---- batch: 020 ----
mean loss: 35.38
 ---- batch: 030 ----
mean loss: 32.74
 ---- batch: 040 ----
mean loss: 33.82
train mean loss: 34.23
epoch train time: 0:00:00.708430
elapsed time: 0:02:59.482816
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:40:03.882201
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.37
 ---- batch: 020 ----
mean loss: 33.59
 ---- batch: 030 ----
mean loss: 34.04
 ---- batch: 040 ----
mean loss: 34.86
train mean loss: 33.95
epoch train time: 0:00:00.694174
elapsed time: 0:03:00.177288
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:40:04.576623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.02
 ---- batch: 020 ----
mean loss: 34.49
 ---- batch: 030 ----
mean loss: 33.08
 ---- batch: 040 ----
mean loss: 33.03
train mean loss: 34.03
epoch train time: 0:00:00.673985
elapsed time: 0:03:00.851493
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:40:05.250827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.75
 ---- batch: 020 ----
mean loss: 33.02
 ---- batch: 030 ----
mean loss: 33.95
 ---- batch: 040 ----
mean loss: 34.43
train mean loss: 34.35
epoch train time: 0:00:00.660144
elapsed time: 0:03:01.511854
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:40:05.911202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.04
 ---- batch: 020 ----
mean loss: 34.03
 ---- batch: 030 ----
mean loss: 35.04
 ---- batch: 040 ----
mean loss: 34.07
train mean loss: 34.13
epoch train time: 0:00:00.680082
elapsed time: 0:03:02.192281
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:40:06.591641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.04
 ---- batch: 020 ----
mean loss: 35.04
 ---- batch: 030 ----
mean loss: 34.22
 ---- batch: 040 ----
mean loss: 34.61
train mean loss: 33.85
epoch train time: 0:00:00.709896
elapsed time: 0:03:02.902434
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:40:07.301772
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.60
 ---- batch: 020 ----
mean loss: 33.17
 ---- batch: 030 ----
mean loss: 35.07
 ---- batch: 040 ----
mean loss: 35.16
train mean loss: 34.06
epoch train time: 0:00:00.704762
elapsed time: 0:03:03.607460
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:40:08.006804
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.16
 ---- batch: 020 ----
mean loss: 33.78
 ---- batch: 030 ----
mean loss: 33.80
 ---- batch: 040 ----
mean loss: 35.04
train mean loss: 34.02
epoch train time: 0:00:00.661699
elapsed time: 0:03:04.269385
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:40:08.668717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.60
 ---- batch: 020 ----
mean loss: 33.01
 ---- batch: 030 ----
mean loss: 33.95
 ---- batch: 040 ----
mean loss: 32.68
train mean loss: 34.07
epoch train time: 0:00:00.663005
elapsed time: 0:03:04.932600
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:40:09.331938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.03
 ---- batch: 020 ----
mean loss: 35.27
 ---- batch: 030 ----
mean loss: 33.88
 ---- batch: 040 ----
mean loss: 33.64
train mean loss: 34.07
epoch train time: 0:00:00.662672
elapsed time: 0:03:05.602205
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_1/checkpoint.pth.tar
**** end time: 2019-09-20 21:40:10.001487 ****
