Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_5', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5622
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 00:54:34.041332 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 00:54:34.059111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3187.75
 ---- batch: 020 ----
mean loss: 1817.12
 ---- batch: 030 ----
mean loss: 1429.84
 ---- batch: 040 ----
mean loss: 1335.13
 ---- batch: 050 ----
mean loss: 1234.26
 ---- batch: 060 ----
mean loss: 1148.58
 ---- batch: 070 ----
mean loss: 1124.03
 ---- batch: 080 ----
mean loss: 1119.74
 ---- batch: 090 ----
mean loss: 1122.69
train mean loss: 1475.53
epoch train time: 0:00:47.543571
elapsed time: 0:00:47.570288
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 00:55:21.611661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1075.37
 ---- batch: 020 ----
mean loss: 1050.33
 ---- batch: 030 ----
mean loss: 1026.17
 ---- batch: 040 ----
mean loss: 1035.11
 ---- batch: 050 ----
mean loss: 1035.42
 ---- batch: 060 ----
mean loss: 996.94
 ---- batch: 070 ----
mean loss: 1022.89
 ---- batch: 080 ----
mean loss: 1022.95
 ---- batch: 090 ----
mean loss: 1054.35
train mean loss: 1032.91
epoch train time: 0:00:16.995272
elapsed time: 0:01:04.566294
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 00:55:38.608194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1016.79
 ---- batch: 020 ----
mean loss: 1007.52
 ---- batch: 030 ----
mean loss: 1001.93
 ---- batch: 040 ----
mean loss: 1014.99
 ---- batch: 050 ----
mean loss: 1028.89
 ---- batch: 060 ----
mean loss: 993.40
 ---- batch: 070 ----
mean loss: 1006.20
 ---- batch: 080 ----
mean loss: 961.20
 ---- batch: 090 ----
mean loss: 1006.46
train mean loss: 1004.59
epoch train time: 0:00:16.933904
elapsed time: 0:01:21.501400
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 00:55:55.543278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1019.75
 ---- batch: 020 ----
mean loss: 977.81
 ---- batch: 030 ----
mean loss: 995.41
 ---- batch: 040 ----
mean loss: 999.52
 ---- batch: 050 ----
mean loss: 961.53
 ---- batch: 060 ----
mean loss: 983.41
 ---- batch: 070 ----
mean loss: 999.74
 ---- batch: 080 ----
mean loss: 1008.38
 ---- batch: 090 ----
mean loss: 975.28
train mean loss: 991.16
epoch train time: 0:00:16.926227
elapsed time: 0:01:38.428811
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 00:56:12.470710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1008.21
 ---- batch: 020 ----
mean loss: 976.17
 ---- batch: 030 ----
mean loss: 983.42
 ---- batch: 040 ----
mean loss: 990.17
 ---- batch: 050 ----
mean loss: 949.99
 ---- batch: 060 ----
mean loss: 964.06
 ---- batch: 070 ----
mean loss: 986.93
 ---- batch: 080 ----
mean loss: 983.38
 ---- batch: 090 ----
mean loss: 978.30
train mean loss: 977.81
epoch train time: 0:00:16.920175
elapsed time: 0:01:55.350223
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 00:56:29.392105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 975.55
 ---- batch: 020 ----
mean loss: 967.15
 ---- batch: 030 ----
mean loss: 987.36
 ---- batch: 040 ----
mean loss: 974.61
 ---- batch: 050 ----
mean loss: 965.08
 ---- batch: 060 ----
mean loss: 948.34
 ---- batch: 070 ----
mean loss: 982.27
 ---- batch: 080 ----
mean loss: 982.11
 ---- batch: 090 ----
mean loss: 952.83
train mean loss: 970.96
epoch train time: 0:00:16.892942
elapsed time: 0:02:12.244410
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 00:56:46.286433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 952.75
 ---- batch: 020 ----
mean loss: 975.89
 ---- batch: 030 ----
mean loss: 968.55
 ---- batch: 040 ----
mean loss: 985.71
 ---- batch: 050 ----
mean loss: 980.24
 ---- batch: 060 ----
mean loss: 966.31
 ---- batch: 070 ----
mean loss: 968.69
 ---- batch: 080 ----
mean loss: 972.09
 ---- batch: 090 ----
mean loss: 953.42
train mean loss: 969.16
epoch train time: 0:00:16.875330
elapsed time: 0:02:29.121093
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 00:57:03.163115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 955.42
 ---- batch: 020 ----
mean loss: 972.86
 ---- batch: 030 ----
mean loss: 975.81
 ---- batch: 040 ----
mean loss: 959.16
 ---- batch: 050 ----
mean loss: 945.88
 ---- batch: 060 ----
mean loss: 947.41
 ---- batch: 070 ----
mean loss: 964.38
 ---- batch: 080 ----
mean loss: 951.24
 ---- batch: 090 ----
mean loss: 923.86
train mean loss: 952.41
epoch train time: 0:00:16.871524
elapsed time: 0:02:45.994023
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 00:57:20.035898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 952.55
 ---- batch: 020 ----
mean loss: 939.81
 ---- batch: 030 ----
mean loss: 946.28
 ---- batch: 040 ----
mean loss: 989.33
 ---- batch: 050 ----
mean loss: 936.50
 ---- batch: 060 ----
mean loss: 931.66
 ---- batch: 070 ----
mean loss: 953.43
 ---- batch: 080 ----
mean loss: 931.75
 ---- batch: 090 ----
mean loss: 960.06
train mean loss: 948.82
epoch train time: 0:00:16.900831
elapsed time: 0:03:02.896211
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 00:57:36.938243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.70
 ---- batch: 020 ----
mean loss: 943.63
 ---- batch: 030 ----
mean loss: 923.21
 ---- batch: 040 ----
mean loss: 932.58
 ---- batch: 050 ----
mean loss: 959.62
 ---- batch: 060 ----
mean loss: 968.38
 ---- batch: 070 ----
mean loss: 953.57
 ---- batch: 080 ----
mean loss: 932.69
 ---- batch: 090 ----
mean loss: 939.83
train mean loss: 945.00
epoch train time: 0:00:16.919884
elapsed time: 0:03:19.817546
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 00:57:53.859497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.59
 ---- batch: 020 ----
mean loss: 962.18
 ---- batch: 030 ----
mean loss: 935.49
 ---- batch: 040 ----
mean loss: 940.15
 ---- batch: 050 ----
mean loss: 921.36
 ---- batch: 060 ----
mean loss: 936.59
 ---- batch: 070 ----
mean loss: 933.45
 ---- batch: 080 ----
mean loss: 930.69
 ---- batch: 090 ----
mean loss: 946.59
train mean loss: 935.19
epoch train time: 0:00:16.868590
elapsed time: 0:03:36.687438
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 00:58:10.729343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.23
 ---- batch: 020 ----
mean loss: 921.79
 ---- batch: 030 ----
mean loss: 931.52
 ---- batch: 040 ----
mean loss: 958.37
 ---- batch: 050 ----
mean loss: 945.52
 ---- batch: 060 ----
mean loss: 936.52
 ---- batch: 070 ----
mean loss: 941.39
 ---- batch: 080 ----
mean loss: 919.87
 ---- batch: 090 ----
mean loss: 911.01
train mean loss: 927.45
epoch train time: 0:00:16.884933
elapsed time: 0:03:53.573563
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 00:58:27.615476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.93
 ---- batch: 020 ----
mean loss: 930.93
 ---- batch: 030 ----
mean loss: 937.77
 ---- batch: 040 ----
mean loss: 905.44
 ---- batch: 050 ----
mean loss: 918.30
 ---- batch: 060 ----
mean loss: 930.65
 ---- batch: 070 ----
mean loss: 905.38
 ---- batch: 080 ----
mean loss: 910.92
 ---- batch: 090 ----
mean loss: 928.84
train mean loss: 922.12
epoch train time: 0:00:16.849489
elapsed time: 0:04:10.424254
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 00:58:44.466267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.69
 ---- batch: 020 ----
mean loss: 911.48
 ---- batch: 030 ----
mean loss: 905.67
 ---- batch: 040 ----
mean loss: 878.59
 ---- batch: 050 ----
mean loss: 921.38
 ---- batch: 060 ----
mean loss: 912.08
 ---- batch: 070 ----
mean loss: 934.27
 ---- batch: 080 ----
mean loss: 905.08
 ---- batch: 090 ----
mean loss: 911.69
train mean loss: 910.42
epoch train time: 0:00:16.870644
elapsed time: 0:04:27.296350
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 00:59:01.338242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.16
 ---- batch: 020 ----
mean loss: 902.94
 ---- batch: 030 ----
mean loss: 923.12
 ---- batch: 040 ----
mean loss: 889.25
 ---- batch: 050 ----
mean loss: 898.15
 ---- batch: 060 ----
mean loss: 888.52
 ---- batch: 070 ----
mean loss: 879.41
 ---- batch: 080 ----
mean loss: 901.93
 ---- batch: 090 ----
mean loss: 895.51
train mean loss: 899.39
epoch train time: 0:00:16.869005
elapsed time: 0:04:44.166519
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 00:59:18.208412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.38
 ---- batch: 020 ----
mean loss: 858.87
 ---- batch: 030 ----
mean loss: 841.15
 ---- batch: 040 ----
mean loss: 843.69
 ---- batch: 050 ----
mean loss: 818.79
 ---- batch: 060 ----
mean loss: 786.88
 ---- batch: 070 ----
mean loss: 756.91
 ---- batch: 080 ----
mean loss: 740.12
 ---- batch: 090 ----
mean loss: 742.37
train mean loss: 799.71
epoch train time: 0:00:16.830829
elapsed time: 0:05:00.998588
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 00:59:35.040492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 743.08
 ---- batch: 020 ----
mean loss: 693.63
 ---- batch: 030 ----
mean loss: 708.04
 ---- batch: 040 ----
mean loss: 691.54
 ---- batch: 050 ----
mean loss: 669.61
 ---- batch: 060 ----
mean loss: 674.92
 ---- batch: 070 ----
mean loss: 686.63
 ---- batch: 080 ----
mean loss: 687.05
 ---- batch: 090 ----
mean loss: 658.19
train mean loss: 688.27
epoch train time: 0:00:16.845410
elapsed time: 0:05:17.845234
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 00:59:51.887169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 645.68
 ---- batch: 020 ----
mean loss: 651.41
 ---- batch: 030 ----
mean loss: 645.44
 ---- batch: 040 ----
mean loss: 624.46
 ---- batch: 050 ----
mean loss: 614.94
 ---- batch: 060 ----
mean loss: 608.29
 ---- batch: 070 ----
mean loss: 612.17
 ---- batch: 080 ----
mean loss: 603.54
 ---- batch: 090 ----
mean loss: 599.24
train mean loss: 620.04
epoch train time: 0:00:16.828531
elapsed time: 0:05:34.675117
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 01:00:08.717004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.82
 ---- batch: 020 ----
mean loss: 587.19
 ---- batch: 030 ----
mean loss: 569.82
 ---- batch: 040 ----
mean loss: 568.64
 ---- batch: 050 ----
mean loss: 568.09
 ---- batch: 060 ----
mean loss: 548.45
 ---- batch: 070 ----
mean loss: 542.26
 ---- batch: 080 ----
mean loss: 530.64
 ---- batch: 090 ----
mean loss: 545.10
train mean loss: 560.87
epoch train time: 0:00:16.836903
elapsed time: 0:05:51.513296
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 01:00:25.555209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.21
 ---- batch: 020 ----
mean loss: 526.33
 ---- batch: 030 ----
mean loss: 519.44
 ---- batch: 040 ----
mean loss: 516.56
 ---- batch: 050 ----
mean loss: 518.32
 ---- batch: 060 ----
mean loss: 502.35
 ---- batch: 070 ----
mean loss: 502.10
 ---- batch: 080 ----
mean loss: 503.14
 ---- batch: 090 ----
mean loss: 502.37
train mean loss: 513.90
epoch train time: 0:00:16.904484
elapsed time: 0:06:08.419046
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 01:00:42.460969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 498.46
 ---- batch: 020 ----
mean loss: 499.23
 ---- batch: 030 ----
mean loss: 489.99
 ---- batch: 040 ----
mean loss: 488.94
 ---- batch: 050 ----
mean loss: 474.93
 ---- batch: 060 ----
mean loss: 470.14
 ---- batch: 070 ----
mean loss: 468.99
 ---- batch: 080 ----
mean loss: 471.83
 ---- batch: 090 ----
mean loss: 469.64
train mean loss: 480.03
epoch train time: 0:00:16.805707
elapsed time: 0:06:25.225962
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 01:00:59.267841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.63
 ---- batch: 020 ----
mean loss: 468.43
 ---- batch: 030 ----
mean loss: 466.44
 ---- batch: 040 ----
mean loss: 452.93
 ---- batch: 050 ----
mean loss: 455.64
 ---- batch: 060 ----
mean loss: 458.14
 ---- batch: 070 ----
mean loss: 445.65
 ---- batch: 080 ----
mean loss: 447.71
 ---- batch: 090 ----
mean loss: 452.52
train mean loss: 455.68
epoch train time: 0:00:16.810239
elapsed time: 0:06:42.037360
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 01:01:16.079110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.70
 ---- batch: 020 ----
mean loss: 441.81
 ---- batch: 030 ----
mean loss: 434.26
 ---- batch: 040 ----
mean loss: 438.22
 ---- batch: 050 ----
mean loss: 432.88
 ---- batch: 060 ----
mean loss: 434.72
 ---- batch: 070 ----
mean loss: 437.18
 ---- batch: 080 ----
mean loss: 440.82
 ---- batch: 090 ----
mean loss: 433.10
train mean loss: 438.45
epoch train time: 0:00:16.770221
elapsed time: 0:06:58.808744
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 01:01:32.850668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.23
 ---- batch: 020 ----
mean loss: 439.24
 ---- batch: 030 ----
mean loss: 415.50
 ---- batch: 040 ----
mean loss: 411.15
 ---- batch: 050 ----
mean loss: 423.21
 ---- batch: 060 ----
mean loss: 431.23
 ---- batch: 070 ----
mean loss: 418.42
 ---- batch: 080 ----
mean loss: 413.88
 ---- batch: 090 ----
mean loss: 408.19
train mean loss: 419.74
epoch train time: 0:00:16.807894
elapsed time: 0:07:15.617900
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 01:01:49.659838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.00
 ---- batch: 020 ----
mean loss: 423.25
 ---- batch: 030 ----
mean loss: 417.43
 ---- batch: 040 ----
mean loss: 409.60
 ---- batch: 050 ----
mean loss: 410.11
 ---- batch: 060 ----
mean loss: 412.78
 ---- batch: 070 ----
mean loss: 394.77
 ---- batch: 080 ----
mean loss: 395.53
 ---- batch: 090 ----
mean loss: 409.09
train mean loss: 408.61
epoch train time: 0:00:16.760986
elapsed time: 0:07:32.380155
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 01:02:06.422065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.25
 ---- batch: 020 ----
mean loss: 395.18
 ---- batch: 030 ----
mean loss: 393.62
 ---- batch: 040 ----
mean loss: 393.73
 ---- batch: 050 ----
mean loss: 395.13
 ---- batch: 060 ----
mean loss: 398.04
 ---- batch: 070 ----
mean loss: 387.63
 ---- batch: 080 ----
mean loss: 393.74
 ---- batch: 090 ----
mean loss: 389.78
train mean loss: 394.04
epoch train time: 0:00:16.769755
elapsed time: 0:07:49.151115
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 01:02:23.192984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.70
 ---- batch: 020 ----
mean loss: 395.75
 ---- batch: 030 ----
mean loss: 400.44
 ---- batch: 040 ----
mean loss: 395.22
 ---- batch: 050 ----
mean loss: 379.55
 ---- batch: 060 ----
mean loss: 381.93
 ---- batch: 070 ----
mean loss: 387.82
 ---- batch: 080 ----
mean loss: 382.44
 ---- batch: 090 ----
mean loss: 366.59
train mean loss: 386.98
epoch train time: 0:00:16.769581
elapsed time: 0:08:05.921882
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 01:02:39.963823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.85
 ---- batch: 020 ----
mean loss: 378.46
 ---- batch: 030 ----
mean loss: 375.67
 ---- batch: 040 ----
mean loss: 380.90
 ---- batch: 050 ----
mean loss: 388.85
 ---- batch: 060 ----
mean loss: 373.07
 ---- batch: 070 ----
mean loss: 365.33
 ---- batch: 080 ----
mean loss: 385.45
 ---- batch: 090 ----
mean loss: 369.71
train mean loss: 377.43
epoch train time: 0:00:16.820190
elapsed time: 0:08:22.743367
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 01:02:56.785268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.97
 ---- batch: 020 ----
mean loss: 358.89
 ---- batch: 030 ----
mean loss: 364.94
 ---- batch: 040 ----
mean loss: 374.58
 ---- batch: 050 ----
mean loss: 368.21
 ---- batch: 060 ----
mean loss: 366.90
 ---- batch: 070 ----
mean loss: 379.94
 ---- batch: 080 ----
mean loss: 369.91
 ---- batch: 090 ----
mean loss: 360.87
train mean loss: 367.58
epoch train time: 0:00:16.760738
elapsed time: 0:08:39.505356
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 01:03:13.547306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.76
 ---- batch: 020 ----
mean loss: 365.01
 ---- batch: 030 ----
mean loss: 354.92
 ---- batch: 040 ----
mean loss: 346.58
 ---- batch: 050 ----
mean loss: 356.41
 ---- batch: 060 ----
mean loss: 366.25
 ---- batch: 070 ----
mean loss: 353.54
 ---- batch: 080 ----
mean loss: 356.97
 ---- batch: 090 ----
mean loss: 346.55
train mean loss: 356.81
epoch train time: 0:00:16.797645
elapsed time: 0:08:56.304306
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 01:03:30.346188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.08
 ---- batch: 020 ----
mean loss: 357.46
 ---- batch: 030 ----
mean loss: 358.04
 ---- batch: 040 ----
mean loss: 354.71
 ---- batch: 050 ----
mean loss: 360.87
 ---- batch: 060 ----
mean loss: 354.40
 ---- batch: 070 ----
mean loss: 345.64
 ---- batch: 080 ----
mean loss: 346.07
 ---- batch: 090 ----
mean loss: 340.46
train mean loss: 350.70
epoch train time: 0:00:16.806264
elapsed time: 0:09:13.111743
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 01:03:47.153645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.36
 ---- batch: 020 ----
mean loss: 341.39
 ---- batch: 030 ----
mean loss: 342.10
 ---- batch: 040 ----
mean loss: 334.24
 ---- batch: 050 ----
mean loss: 330.56
 ---- batch: 060 ----
mean loss: 350.39
 ---- batch: 070 ----
mean loss: 342.19
 ---- batch: 080 ----
mean loss: 342.81
 ---- batch: 090 ----
mean loss: 344.78
train mean loss: 340.34
epoch train time: 0:00:16.862995
elapsed time: 0:09:29.975925
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 01:04:04.017863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.98
 ---- batch: 020 ----
mean loss: 354.24
 ---- batch: 030 ----
mean loss: 339.85
 ---- batch: 040 ----
mean loss: 335.24
 ---- batch: 050 ----
mean loss: 342.78
 ---- batch: 060 ----
mean loss: 337.66
 ---- batch: 070 ----
mean loss: 338.08
 ---- batch: 080 ----
mean loss: 325.81
 ---- batch: 090 ----
mean loss: 314.46
train mean loss: 335.90
epoch train time: 0:00:16.794483
elapsed time: 0:09:46.771634
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 01:04:20.813558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.81
 ---- batch: 020 ----
mean loss: 336.27
 ---- batch: 030 ----
mean loss: 333.56
 ---- batch: 040 ----
mean loss: 325.60
 ---- batch: 050 ----
mean loss: 332.42
 ---- batch: 060 ----
mean loss: 332.24
 ---- batch: 070 ----
mean loss: 321.57
 ---- batch: 080 ----
mean loss: 337.94
 ---- batch: 090 ----
mean loss: 329.40
train mean loss: 329.97
epoch train time: 0:00:16.797925
elapsed time: 0:10:03.570854
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 01:04:37.612802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.30
 ---- batch: 020 ----
mean loss: 330.05
 ---- batch: 030 ----
mean loss: 318.43
 ---- batch: 040 ----
mean loss: 320.90
 ---- batch: 050 ----
mean loss: 321.90
 ---- batch: 060 ----
mean loss: 321.87
 ---- batch: 070 ----
mean loss: 319.91
 ---- batch: 080 ----
mean loss: 312.64
 ---- batch: 090 ----
mean loss: 325.66
train mean loss: 321.76
epoch train time: 0:00:16.758115
elapsed time: 0:10:20.330226
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 01:04:54.372175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.98
 ---- batch: 020 ----
mean loss: 324.77
 ---- batch: 030 ----
mean loss: 318.90
 ---- batch: 040 ----
mean loss: 319.80
 ---- batch: 050 ----
mean loss: 310.60
 ---- batch: 060 ----
mean loss: 310.24
 ---- batch: 070 ----
mean loss: 310.08
 ---- batch: 080 ----
mean loss: 314.98
 ---- batch: 090 ----
mean loss: 310.37
train mean loss: 317.74
epoch train time: 0:00:16.794270
elapsed time: 0:10:37.125939
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 01:05:11.167919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.90
 ---- batch: 020 ----
mean loss: 316.22
 ---- batch: 030 ----
mean loss: 303.93
 ---- batch: 040 ----
mean loss: 322.17
 ---- batch: 050 ----
mean loss: 310.46
 ---- batch: 060 ----
mean loss: 308.98
 ---- batch: 070 ----
mean loss: 309.32
 ---- batch: 080 ----
mean loss: 308.78
 ---- batch: 090 ----
mean loss: 306.26
train mean loss: 312.06
epoch train time: 0:00:16.943576
elapsed time: 0:10:54.070816
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 01:05:28.112707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.23
 ---- batch: 020 ----
mean loss: 304.79
 ---- batch: 030 ----
mean loss: 308.40
 ---- batch: 040 ----
mean loss: 321.73
 ---- batch: 050 ----
mean loss: 308.46
 ---- batch: 060 ----
mean loss: 305.41
 ---- batch: 070 ----
mean loss: 312.33
 ---- batch: 080 ----
mean loss: 310.31
 ---- batch: 090 ----
mean loss: 301.40
train mean loss: 309.84
epoch train time: 0:00:16.745704
elapsed time: 0:11:10.817731
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 01:05:44.859616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.31
 ---- batch: 020 ----
mean loss: 301.03
 ---- batch: 030 ----
mean loss: 303.90
 ---- batch: 040 ----
mean loss: 302.42
 ---- batch: 050 ----
mean loss: 311.57
 ---- batch: 060 ----
mean loss: 301.04
 ---- batch: 070 ----
mean loss: 304.51
 ---- batch: 080 ----
mean loss: 307.10
 ---- batch: 090 ----
mean loss: 299.80
train mean loss: 302.79
epoch train time: 0:00:16.820417
elapsed time: 0:11:27.639341
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 01:06:01.681210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.62
 ---- batch: 020 ----
mean loss: 293.78
 ---- batch: 030 ----
mean loss: 297.40
 ---- batch: 040 ----
mean loss: 302.61
 ---- batch: 050 ----
mean loss: 306.33
 ---- batch: 060 ----
mean loss: 295.91
 ---- batch: 070 ----
mean loss: 290.42
 ---- batch: 080 ----
mean loss: 298.13
 ---- batch: 090 ----
mean loss: 298.42
train mean loss: 297.53
epoch train time: 0:00:16.768340
elapsed time: 0:11:44.408890
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 01:06:18.450832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.02
 ---- batch: 020 ----
mean loss: 297.56
 ---- batch: 030 ----
mean loss: 286.96
 ---- batch: 040 ----
mean loss: 296.39
 ---- batch: 050 ----
mean loss: 288.83
 ---- batch: 060 ----
mean loss: 298.85
 ---- batch: 070 ----
mean loss: 289.02
 ---- batch: 080 ----
mean loss: 301.19
 ---- batch: 090 ----
mean loss: 289.55
train mean loss: 294.03
epoch train time: 0:00:16.737443
elapsed time: 0:12:01.147636
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 01:06:35.189527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.29
 ---- batch: 020 ----
mean loss: 288.18
 ---- batch: 030 ----
mean loss: 292.64
 ---- batch: 040 ----
mean loss: 295.42
 ---- batch: 050 ----
mean loss: 292.37
 ---- batch: 060 ----
mean loss: 292.22
 ---- batch: 070 ----
mean loss: 287.85
 ---- batch: 080 ----
mean loss: 286.66
 ---- batch: 090 ----
mean loss: 290.63
train mean loss: 289.49
epoch train time: 0:00:16.749257
elapsed time: 0:12:17.898160
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 01:06:51.940054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.98
 ---- batch: 020 ----
mean loss: 281.41
 ---- batch: 030 ----
mean loss: 283.97
 ---- batch: 040 ----
mean loss: 285.79
 ---- batch: 050 ----
mean loss: 294.95
 ---- batch: 060 ----
mean loss: 293.62
 ---- batch: 070 ----
mean loss: 287.22
 ---- batch: 080 ----
mean loss: 292.72
 ---- batch: 090 ----
mean loss: 281.93
train mean loss: 287.96
epoch train time: 0:00:16.811884
elapsed time: 0:12:34.711231
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 01:07:08.753099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.62
 ---- batch: 020 ----
mean loss: 293.36
 ---- batch: 030 ----
mean loss: 279.62
 ---- batch: 040 ----
mean loss: 284.49
 ---- batch: 050 ----
mean loss: 279.34
 ---- batch: 060 ----
mean loss: 278.66
 ---- batch: 070 ----
mean loss: 281.39
 ---- batch: 080 ----
mean loss: 286.07
 ---- batch: 090 ----
mean loss: 284.36
train mean loss: 283.78
epoch train time: 0:00:16.799503
elapsed time: 0:12:51.511883
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 01:07:25.553796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.34
 ---- batch: 020 ----
mean loss: 273.37
 ---- batch: 030 ----
mean loss: 283.22
 ---- batch: 040 ----
mean loss: 273.54
 ---- batch: 050 ----
mean loss: 277.62
 ---- batch: 060 ----
mean loss: 286.03
 ---- batch: 070 ----
mean loss: 288.05
 ---- batch: 080 ----
mean loss: 280.37
 ---- batch: 090 ----
mean loss: 280.66
train mean loss: 281.70
epoch train time: 0:00:16.749181
elapsed time: 0:13:08.262307
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 01:07:42.304141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.70
 ---- batch: 020 ----
mean loss: 283.63
 ---- batch: 030 ----
mean loss: 271.58
 ---- batch: 040 ----
mean loss: 278.91
 ---- batch: 050 ----
mean loss: 278.02
 ---- batch: 060 ----
mean loss: 274.66
 ---- batch: 070 ----
mean loss: 281.40
 ---- batch: 080 ----
mean loss: 267.32
 ---- batch: 090 ----
mean loss: 280.85
train mean loss: 276.89
epoch train time: 0:00:16.814924
elapsed time: 0:13:25.078429
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 01:07:59.120319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.05
 ---- batch: 020 ----
mean loss: 283.97
 ---- batch: 030 ----
mean loss: 280.21
 ---- batch: 040 ----
mean loss: 276.49
 ---- batch: 050 ----
mean loss: 267.36
 ---- batch: 060 ----
mean loss: 270.30
 ---- batch: 070 ----
mean loss: 278.47
 ---- batch: 080 ----
mean loss: 269.31
 ---- batch: 090 ----
mean loss: 274.10
train mean loss: 276.47
epoch train time: 0:00:16.784889
elapsed time: 0:13:41.864531
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 01:08:15.906417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.42
 ---- batch: 020 ----
mean loss: 277.38
 ---- batch: 030 ----
mean loss: 270.69
 ---- batch: 040 ----
mean loss: 266.36
 ---- batch: 050 ----
mean loss: 273.44
 ---- batch: 060 ----
mean loss: 276.01
 ---- batch: 070 ----
mean loss: 270.13
 ---- batch: 080 ----
mean loss: 266.10
 ---- batch: 090 ----
mean loss: 269.87
train mean loss: 271.13
epoch train time: 0:00:16.756190
elapsed time: 0:13:58.621898
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 01:08:32.663823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.85
 ---- batch: 020 ----
mean loss: 262.79
 ---- batch: 030 ----
mean loss: 267.81
 ---- batch: 040 ----
mean loss: 264.75
 ---- batch: 050 ----
mean loss: 267.06
 ---- batch: 060 ----
mean loss: 269.33
 ---- batch: 070 ----
mean loss: 275.76
 ---- batch: 080 ----
mean loss: 276.65
 ---- batch: 090 ----
mean loss: 264.46
train mean loss: 268.77
epoch train time: 0:00:16.759235
elapsed time: 0:14:15.382375
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 01:08:49.424330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.14
 ---- batch: 020 ----
mean loss: 255.14
 ---- batch: 030 ----
mean loss: 274.24
 ---- batch: 040 ----
mean loss: 260.75
 ---- batch: 050 ----
mean loss: 267.65
 ---- batch: 060 ----
mean loss: 271.86
 ---- batch: 070 ----
mean loss: 263.51
 ---- batch: 080 ----
mean loss: 270.34
 ---- batch: 090 ----
mean loss: 261.09
train mean loss: 265.63
epoch train time: 0:00:16.820832
elapsed time: 0:14:32.204450
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 01:09:06.246387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.25
 ---- batch: 020 ----
mean loss: 264.84
 ---- batch: 030 ----
mean loss: 267.06
 ---- batch: 040 ----
mean loss: 259.16
 ---- batch: 050 ----
mean loss: 268.93
 ---- batch: 060 ----
mean loss: 265.62
 ---- batch: 070 ----
mean loss: 273.95
 ---- batch: 080 ----
mean loss: 258.14
 ---- batch: 090 ----
mean loss: 262.02
train mean loss: 264.25
epoch train time: 0:00:16.758543
elapsed time: 0:14:48.964234
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 01:09:23.006155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.95
 ---- batch: 020 ----
mean loss: 258.32
 ---- batch: 030 ----
mean loss: 264.43
 ---- batch: 040 ----
mean loss: 268.75
 ---- batch: 050 ----
mean loss: 261.51
 ---- batch: 060 ----
mean loss: 265.38
 ---- batch: 070 ----
mean loss: 253.11
 ---- batch: 080 ----
mean loss: 259.77
 ---- batch: 090 ----
mean loss: 260.25
train mean loss: 261.60
epoch train time: 0:00:16.769542
elapsed time: 0:15:05.735165
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 01:09:39.776967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.34
 ---- batch: 020 ----
mean loss: 257.66
 ---- batch: 030 ----
mean loss: 260.56
 ---- batch: 040 ----
mean loss: 251.78
 ---- batch: 050 ----
mean loss: 261.54
 ---- batch: 060 ----
mean loss: 262.96
 ---- batch: 070 ----
mean loss: 256.85
 ---- batch: 080 ----
mean loss: 247.67
 ---- batch: 090 ----
mean loss: 263.33
train mean loss: 258.07
epoch train time: 0:00:16.790523
elapsed time: 0:15:22.526791
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 01:09:56.568654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.39
 ---- batch: 020 ----
mean loss: 253.34
 ---- batch: 030 ----
mean loss: 263.24
 ---- batch: 040 ----
mean loss: 257.67
 ---- batch: 050 ----
mean loss: 256.73
 ---- batch: 060 ----
mean loss: 251.98
 ---- batch: 070 ----
mean loss: 254.86
 ---- batch: 080 ----
mean loss: 264.63
 ---- batch: 090 ----
mean loss: 248.00
train mean loss: 255.68
epoch train time: 0:00:16.799918
elapsed time: 0:15:39.327851
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 01:10:13.369772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.06
 ---- batch: 020 ----
mean loss: 258.87
 ---- batch: 030 ----
mean loss: 256.68
 ---- batch: 040 ----
mean loss: 257.12
 ---- batch: 050 ----
mean loss: 254.55
 ---- batch: 060 ----
mean loss: 256.22
 ---- batch: 070 ----
mean loss: 259.36
 ---- batch: 080 ----
mean loss: 247.49
 ---- batch: 090 ----
mean loss: 255.52
train mean loss: 256.24
epoch train time: 0:00:16.761502
elapsed time: 0:15:56.090568
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 01:10:30.132434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.54
 ---- batch: 020 ----
mean loss: 250.27
 ---- batch: 030 ----
mean loss: 251.07
 ---- batch: 040 ----
mean loss: 257.42
 ---- batch: 050 ----
mean loss: 251.13
 ---- batch: 060 ----
mean loss: 253.05
 ---- batch: 070 ----
mean loss: 260.34
 ---- batch: 080 ----
mean loss: 244.94
 ---- batch: 090 ----
mean loss: 241.46
train mean loss: 251.95
epoch train time: 0:00:16.762047
elapsed time: 0:16:12.853803
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 01:10:46.895754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.24
 ---- batch: 020 ----
mean loss: 249.61
 ---- batch: 030 ----
mean loss: 257.98
 ---- batch: 040 ----
mean loss: 255.79
 ---- batch: 050 ----
mean loss: 247.55
 ---- batch: 060 ----
mean loss: 246.05
 ---- batch: 070 ----
mean loss: 252.97
 ---- batch: 080 ----
mean loss: 250.01
 ---- batch: 090 ----
mean loss: 252.68
train mean loss: 251.97
epoch train time: 0:00:16.858522
elapsed time: 0:16:29.713647
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 01:11:03.755582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.01
 ---- batch: 020 ----
mean loss: 251.92
 ---- batch: 030 ----
mean loss: 251.70
 ---- batch: 040 ----
mean loss: 248.58
 ---- batch: 050 ----
mean loss: 251.76
 ---- batch: 060 ----
mean loss: 249.19
 ---- batch: 070 ----
mean loss: 237.96
 ---- batch: 080 ----
mean loss: 244.78
 ---- batch: 090 ----
mean loss: 246.51
train mean loss: 247.72
epoch train time: 0:00:16.802349
elapsed time: 0:16:46.517192
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 01:11:20.559105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.08
 ---- batch: 020 ----
mean loss: 248.08
 ---- batch: 030 ----
mean loss: 248.26
 ---- batch: 040 ----
mean loss: 244.00
 ---- batch: 050 ----
mean loss: 244.04
 ---- batch: 060 ----
mean loss: 245.47
 ---- batch: 070 ----
mean loss: 245.02
 ---- batch: 080 ----
mean loss: 247.68
 ---- batch: 090 ----
mean loss: 246.47
train mean loss: 246.64
epoch train time: 0:00:16.793280
elapsed time: 0:17:03.311714
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 01:11:37.353619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.33
 ---- batch: 020 ----
mean loss: 249.98
 ---- batch: 030 ----
mean loss: 239.01
 ---- batch: 040 ----
mean loss: 247.28
 ---- batch: 050 ----
mean loss: 249.23
 ---- batch: 060 ----
mean loss: 244.64
 ---- batch: 070 ----
mean loss: 240.67
 ---- batch: 080 ----
mean loss: 243.89
 ---- batch: 090 ----
mean loss: 242.29
train mean loss: 244.81
epoch train time: 0:00:16.782730
elapsed time: 0:17:20.095660
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 01:11:54.137559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.75
 ---- batch: 020 ----
mean loss: 245.24
 ---- batch: 030 ----
mean loss: 238.49
 ---- batch: 040 ----
mean loss: 246.84
 ---- batch: 050 ----
mean loss: 237.63
 ---- batch: 060 ----
mean loss: 238.67
 ---- batch: 070 ----
mean loss: 249.66
 ---- batch: 080 ----
mean loss: 247.15
 ---- batch: 090 ----
mean loss: 246.11
train mean loss: 242.86
epoch train time: 0:00:16.821775
elapsed time: 0:17:36.918684
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 01:12:10.960613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.82
 ---- batch: 020 ----
mean loss: 238.72
 ---- batch: 030 ----
mean loss: 235.36
 ---- batch: 040 ----
mean loss: 237.21
 ---- batch: 050 ----
mean loss: 236.95
 ---- batch: 060 ----
mean loss: 250.61
 ---- batch: 070 ----
mean loss: 238.46
 ---- batch: 080 ----
mean loss: 235.65
 ---- batch: 090 ----
mean loss: 249.63
train mean loss: 240.71
epoch train time: 0:00:16.802993
elapsed time: 0:17:53.722932
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 01:12:27.764870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.88
 ---- batch: 020 ----
mean loss: 237.06
 ---- batch: 030 ----
mean loss: 246.19
 ---- batch: 040 ----
mean loss: 238.11
 ---- batch: 050 ----
mean loss: 244.83
 ---- batch: 060 ----
mean loss: 240.78
 ---- batch: 070 ----
mean loss: 241.48
 ---- batch: 080 ----
mean loss: 243.70
 ---- batch: 090 ----
mean loss: 238.19
train mean loss: 241.19
epoch train time: 0:00:16.778361
elapsed time: 0:18:10.502564
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 01:12:44.544513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.20
 ---- batch: 020 ----
mean loss: 242.84
 ---- batch: 030 ----
mean loss: 234.43
 ---- batch: 040 ----
mean loss: 239.97
 ---- batch: 050 ----
mean loss: 249.77
 ---- batch: 060 ----
mean loss: 238.94
 ---- batch: 070 ----
mean loss: 236.44
 ---- batch: 080 ----
mean loss: 232.90
 ---- batch: 090 ----
mean loss: 233.64
train mean loss: 238.75
epoch train time: 0:00:16.765085
elapsed time: 0:18:27.268903
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 01:13:01.310784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.43
 ---- batch: 020 ----
mean loss: 233.10
 ---- batch: 030 ----
mean loss: 242.77
 ---- batch: 040 ----
mean loss: 241.98
 ---- batch: 050 ----
mean loss: 242.31
 ---- batch: 060 ----
mean loss: 241.66
 ---- batch: 070 ----
mean loss: 232.35
 ---- batch: 080 ----
mean loss: 232.65
 ---- batch: 090 ----
mean loss: 234.42
train mean loss: 238.27
epoch train time: 0:00:16.837321
elapsed time: 0:18:44.107438
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 01:13:18.149351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.22
 ---- batch: 020 ----
mean loss: 238.69
 ---- batch: 030 ----
mean loss: 237.06
 ---- batch: 040 ----
mean loss: 232.95
 ---- batch: 050 ----
mean loss: 235.53
 ---- batch: 060 ----
mean loss: 240.57
 ---- batch: 070 ----
mean loss: 233.87
 ---- batch: 080 ----
mean loss: 238.97
 ---- batch: 090 ----
mean loss: 235.03
train mean loss: 235.77
epoch train time: 0:00:17.011004
elapsed time: 0:19:01.119685
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 01:13:35.161589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.59
 ---- batch: 020 ----
mean loss: 232.83
 ---- batch: 030 ----
mean loss: 237.34
 ---- batch: 040 ----
mean loss: 243.05
 ---- batch: 050 ----
mean loss: 235.70
 ---- batch: 060 ----
mean loss: 239.03
 ---- batch: 070 ----
mean loss: 230.38
 ---- batch: 080 ----
mean loss: 229.72
 ---- batch: 090 ----
mean loss: 226.06
train mean loss: 234.37
epoch train time: 0:00:17.006795
elapsed time: 0:19:18.127718
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 01:13:52.169688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.88
 ---- batch: 020 ----
mean loss: 234.29
 ---- batch: 030 ----
mean loss: 229.53
 ---- batch: 040 ----
mean loss: 230.34
 ---- batch: 050 ----
mean loss: 231.14
 ---- batch: 060 ----
mean loss: 231.48
 ---- batch: 070 ----
mean loss: 236.35
 ---- batch: 080 ----
mean loss: 232.89
 ---- batch: 090 ----
mean loss: 227.88
train mean loss: 232.21
epoch train time: 0:00:17.028464
elapsed time: 0:19:35.157489
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 01:14:09.199365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.00
 ---- batch: 020 ----
mean loss: 232.29
 ---- batch: 030 ----
mean loss: 223.77
 ---- batch: 040 ----
mean loss: 227.34
 ---- batch: 050 ----
mean loss: 230.10
 ---- batch: 060 ----
mean loss: 227.97
 ---- batch: 070 ----
mean loss: 239.42
 ---- batch: 080 ----
mean loss: 232.49
 ---- batch: 090 ----
mean loss: 234.27
train mean loss: 231.32
epoch train time: 0:00:17.082647
elapsed time: 0:19:52.241454
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 01:14:26.283469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.03
 ---- batch: 020 ----
mean loss: 224.50
 ---- batch: 030 ----
mean loss: 227.53
 ---- batch: 040 ----
mean loss: 225.57
 ---- batch: 050 ----
mean loss: 224.91
 ---- batch: 060 ----
mean loss: 227.99
 ---- batch: 070 ----
mean loss: 221.31
 ---- batch: 080 ----
mean loss: 239.67
 ---- batch: 090 ----
mean loss: 231.97
train mean loss: 229.26
epoch train time: 0:00:17.082169
elapsed time: 0:20:09.325036
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 01:14:43.366955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.74
 ---- batch: 020 ----
mean loss: 223.02
 ---- batch: 030 ----
mean loss: 229.31
 ---- batch: 040 ----
mean loss: 235.71
 ---- batch: 050 ----
mean loss: 234.02
 ---- batch: 060 ----
mean loss: 227.55
 ---- batch: 070 ----
mean loss: 232.46
 ---- batch: 080 ----
mean loss: 230.44
 ---- batch: 090 ----
mean loss: 229.64
train mean loss: 229.41
epoch train time: 0:00:17.027191
elapsed time: 0:20:26.353559
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 01:15:00.395519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.78
 ---- batch: 020 ----
mean loss: 225.58
 ---- batch: 030 ----
mean loss: 235.45
 ---- batch: 040 ----
mean loss: 229.18
 ---- batch: 050 ----
mean loss: 230.03
 ---- batch: 060 ----
mean loss: 232.27
 ---- batch: 070 ----
mean loss: 222.39
 ---- batch: 080 ----
mean loss: 227.38
 ---- batch: 090 ----
mean loss: 226.28
train mean loss: 228.02
epoch train time: 0:00:17.012459
elapsed time: 0:20:43.367309
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 01:15:17.409173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.33
 ---- batch: 020 ----
mean loss: 213.90
 ---- batch: 030 ----
mean loss: 228.73
 ---- batch: 040 ----
mean loss: 226.46
 ---- batch: 050 ----
mean loss: 230.01
 ---- batch: 060 ----
mean loss: 225.88
 ---- batch: 070 ----
mean loss: 227.79
 ---- batch: 080 ----
mean loss: 234.71
 ---- batch: 090 ----
mean loss: 225.85
train mean loss: 225.96
epoch train time: 0:00:17.024765
elapsed time: 0:21:00.393281
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 01:15:34.435235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.38
 ---- batch: 020 ----
mean loss: 232.42
 ---- batch: 030 ----
mean loss: 230.83
 ---- batch: 040 ----
mean loss: 224.02
 ---- batch: 050 ----
mean loss: 218.49
 ---- batch: 060 ----
mean loss: 230.15
 ---- batch: 070 ----
mean loss: 226.37
 ---- batch: 080 ----
mean loss: 228.81
 ---- batch: 090 ----
mean loss: 225.57
train mean loss: 226.80
epoch train time: 0:00:17.005701
elapsed time: 0:21:17.400306
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 01:15:51.442271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.32
 ---- batch: 020 ----
mean loss: 231.21
 ---- batch: 030 ----
mean loss: 231.49
 ---- batch: 040 ----
mean loss: 230.20
 ---- batch: 050 ----
mean loss: 226.34
 ---- batch: 060 ----
mean loss: 220.38
 ---- batch: 070 ----
mean loss: 222.45
 ---- batch: 080 ----
mean loss: 219.50
 ---- batch: 090 ----
mean loss: 222.42
train mean loss: 225.78
epoch train time: 0:00:17.019441
elapsed time: 0:21:34.421076
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 01:16:08.463033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.36
 ---- batch: 020 ----
mean loss: 222.80
 ---- batch: 030 ----
mean loss: 227.55
 ---- batch: 040 ----
mean loss: 230.77
 ---- batch: 050 ----
mean loss: 218.91
 ---- batch: 060 ----
mean loss: 221.48
 ---- batch: 070 ----
mean loss: 224.60
 ---- batch: 080 ----
mean loss: 228.88
 ---- batch: 090 ----
mean loss: 223.27
train mean loss: 223.87
epoch train time: 0:00:17.088091
elapsed time: 0:21:51.510478
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 01:16:25.552349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.62
 ---- batch: 020 ----
mean loss: 224.91
 ---- batch: 030 ----
mean loss: 217.39
 ---- batch: 040 ----
mean loss: 225.44
 ---- batch: 050 ----
mean loss: 237.84
 ---- batch: 060 ----
mean loss: 223.77
 ---- batch: 070 ----
mean loss: 228.50
 ---- batch: 080 ----
mean loss: 227.05
 ---- batch: 090 ----
mean loss: 225.82
train mean loss: 226.24
epoch train time: 0:00:17.052891
elapsed time: 0:22:08.564613
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 01:16:42.606500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.61
 ---- batch: 020 ----
mean loss: 223.59
 ---- batch: 030 ----
mean loss: 221.70
 ---- batch: 040 ----
mean loss: 235.44
 ---- batch: 050 ----
mean loss: 218.44
 ---- batch: 060 ----
mean loss: 217.64
 ---- batch: 070 ----
mean loss: 224.95
 ---- batch: 080 ----
mean loss: 223.80
 ---- batch: 090 ----
mean loss: 221.37
train mean loss: 223.43
epoch train time: 0:00:17.017188
elapsed time: 0:22:25.583026
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 01:16:59.624936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.57
 ---- batch: 020 ----
mean loss: 219.34
 ---- batch: 030 ----
mean loss: 222.19
 ---- batch: 040 ----
mean loss: 234.31
 ---- batch: 050 ----
mean loss: 218.43
 ---- batch: 060 ----
mean loss: 214.27
 ---- batch: 070 ----
mean loss: 233.64
 ---- batch: 080 ----
mean loss: 217.62
 ---- batch: 090 ----
mean loss: 222.33
train mean loss: 221.33
epoch train time: 0:00:17.037956
elapsed time: 0:22:42.622354
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 01:17:16.664022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.54
 ---- batch: 020 ----
mean loss: 214.10
 ---- batch: 030 ----
mean loss: 219.01
 ---- batch: 040 ----
mean loss: 227.43
 ---- batch: 050 ----
mean loss: 219.67
 ---- batch: 060 ----
mean loss: 224.23
 ---- batch: 070 ----
mean loss: 211.69
 ---- batch: 080 ----
mean loss: 221.98
 ---- batch: 090 ----
mean loss: 220.33
train mean loss: 219.02
epoch train time: 0:00:17.002880
elapsed time: 0:22:59.626226
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 01:17:33.668099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.92
 ---- batch: 020 ----
mean loss: 212.20
 ---- batch: 030 ----
mean loss: 221.01
 ---- batch: 040 ----
mean loss: 215.85
 ---- batch: 050 ----
mean loss: 218.00
 ---- batch: 060 ----
mean loss: 217.73
 ---- batch: 070 ----
mean loss: 222.50
 ---- batch: 080 ----
mean loss: 218.87
 ---- batch: 090 ----
mean loss: 225.72
train mean loss: 219.14
epoch train time: 0:00:17.037072
elapsed time: 0:23:16.664586
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 01:17:50.706592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.82
 ---- batch: 020 ----
mean loss: 221.32
 ---- batch: 030 ----
mean loss: 224.09
 ---- batch: 040 ----
mean loss: 217.93
 ---- batch: 050 ----
mean loss: 224.33
 ---- batch: 060 ----
mean loss: 216.41
 ---- batch: 070 ----
mean loss: 224.08
 ---- batch: 080 ----
mean loss: 211.14
 ---- batch: 090 ----
mean loss: 218.75
train mean loss: 219.42
epoch train time: 0:00:17.104377
elapsed time: 0:23:33.770494
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 01:18:07.812396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.64
 ---- batch: 020 ----
mean loss: 218.80
 ---- batch: 030 ----
mean loss: 215.38
 ---- batch: 040 ----
mean loss: 215.18
 ---- batch: 050 ----
mean loss: 218.98
 ---- batch: 060 ----
mean loss: 224.96
 ---- batch: 070 ----
mean loss: 209.52
 ---- batch: 080 ----
mean loss: 224.53
 ---- batch: 090 ----
mean loss: 213.67
train mean loss: 217.23
epoch train time: 0:00:17.013825
elapsed time: 0:23:50.785553
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 01:18:24.827607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.65
 ---- batch: 020 ----
mean loss: 211.84
 ---- batch: 030 ----
mean loss: 219.97
 ---- batch: 040 ----
mean loss: 215.06
 ---- batch: 050 ----
mean loss: 217.13
 ---- batch: 060 ----
mean loss: 218.43
 ---- batch: 070 ----
mean loss: 217.68
 ---- batch: 080 ----
mean loss: 216.37
 ---- batch: 090 ----
mean loss: 216.46
train mean loss: 217.41
epoch train time: 0:00:17.026363
elapsed time: 0:24:07.813380
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 01:18:41.855298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.37
 ---- batch: 020 ----
mean loss: 212.03
 ---- batch: 030 ----
mean loss: 216.25
 ---- batch: 040 ----
mean loss: 213.24
 ---- batch: 050 ----
mean loss: 222.62
 ---- batch: 060 ----
mean loss: 217.08
 ---- batch: 070 ----
mean loss: 208.49
 ---- batch: 080 ----
mean loss: 218.69
 ---- batch: 090 ----
mean loss: 225.97
train mean loss: 216.44
epoch train time: 0:00:16.979805
elapsed time: 0:24:24.794504
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 01:18:58.836470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.45
 ---- batch: 020 ----
mean loss: 227.74
 ---- batch: 030 ----
mean loss: 216.90
 ---- batch: 040 ----
mean loss: 221.77
 ---- batch: 050 ----
mean loss: 214.05
 ---- batch: 060 ----
mean loss: 207.30
 ---- batch: 070 ----
mean loss: 207.55
 ---- batch: 080 ----
mean loss: 210.07
 ---- batch: 090 ----
mean loss: 224.45
train mean loss: 217.17
epoch train time: 0:00:17.030653
elapsed time: 0:24:41.826806
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 01:19:15.868719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.61
 ---- batch: 020 ----
mean loss: 211.54
 ---- batch: 030 ----
mean loss: 220.47
 ---- batch: 040 ----
mean loss: 218.41
 ---- batch: 050 ----
mean loss: 211.87
 ---- batch: 060 ----
mean loss: 221.19
 ---- batch: 070 ----
mean loss: 211.90
 ---- batch: 080 ----
mean loss: 215.52
 ---- batch: 090 ----
mean loss: 207.99
train mean loss: 215.12
epoch train time: 0:00:17.068247
elapsed time: 0:24:58.896317
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 01:19:32.938196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.36
 ---- batch: 020 ----
mean loss: 214.81
 ---- batch: 030 ----
mean loss: 216.65
 ---- batch: 040 ----
mean loss: 216.04
 ---- batch: 050 ----
mean loss: 202.91
 ---- batch: 060 ----
mean loss: 217.46
 ---- batch: 070 ----
mean loss: 220.36
 ---- batch: 080 ----
mean loss: 214.06
 ---- batch: 090 ----
mean loss: 215.29
train mean loss: 214.87
epoch train time: 0:00:17.004188
elapsed time: 0:25:15.901839
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 01:19:49.943711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.31
 ---- batch: 020 ----
mean loss: 213.62
 ---- batch: 030 ----
mean loss: 213.67
 ---- batch: 040 ----
mean loss: 218.43
 ---- batch: 050 ----
mean loss: 210.62
 ---- batch: 060 ----
mean loss: 220.79
 ---- batch: 070 ----
mean loss: 213.77
 ---- batch: 080 ----
mean loss: 214.69
 ---- batch: 090 ----
mean loss: 210.24
train mean loss: 215.04
epoch train time: 0:00:17.021458
elapsed time: 0:25:32.924528
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 01:20:06.966605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.48
 ---- batch: 020 ----
mean loss: 217.10
 ---- batch: 030 ----
mean loss: 216.37
 ---- batch: 040 ----
mean loss: 211.79
 ---- batch: 050 ----
mean loss: 215.26
 ---- batch: 060 ----
mean loss: 211.46
 ---- batch: 070 ----
mean loss: 211.52
 ---- batch: 080 ----
mean loss: 210.21
 ---- batch: 090 ----
mean loss: 215.18
train mean loss: 213.25
epoch train time: 0:00:17.003890
elapsed time: 0:25:49.929954
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 01:20:23.971844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.85
 ---- batch: 020 ----
mean loss: 211.30
 ---- batch: 030 ----
mean loss: 204.18
 ---- batch: 040 ----
mean loss: 213.09
 ---- batch: 050 ----
mean loss: 218.53
 ---- batch: 060 ----
mean loss: 219.25
 ---- batch: 070 ----
mean loss: 211.65
 ---- batch: 080 ----
mean loss: 221.51
 ---- batch: 090 ----
mean loss: 217.03
train mean loss: 214.08
epoch train time: 0:00:16.988090
elapsed time: 0:26:06.919317
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 01:20:40.961201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.55
 ---- batch: 020 ----
mean loss: 221.75
 ---- batch: 030 ----
mean loss: 213.20
 ---- batch: 040 ----
mean loss: 218.08
 ---- batch: 050 ----
mean loss: 222.58
 ---- batch: 060 ----
mean loss: 219.88
 ---- batch: 070 ----
mean loss: 217.76
 ---- batch: 080 ----
mean loss: 213.32
 ---- batch: 090 ----
mean loss: 209.40
train mean loss: 216.39
epoch train time: 0:00:17.031589
elapsed time: 0:26:23.952117
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 01:20:57.994086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.49
 ---- batch: 020 ----
mean loss: 220.91
 ---- batch: 030 ----
mean loss: 212.15
 ---- batch: 040 ----
mean loss: 212.14
 ---- batch: 050 ----
mean loss: 206.26
 ---- batch: 060 ----
mean loss: 211.69
 ---- batch: 070 ----
mean loss: 213.65
 ---- batch: 080 ----
mean loss: 218.30
 ---- batch: 090 ----
mean loss: 204.84
train mean loss: 212.40
epoch train time: 0:00:17.047401
elapsed time: 0:26:41.000842
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 01:21:15.042740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.34
 ---- batch: 020 ----
mean loss: 217.17
 ---- batch: 030 ----
mean loss: 212.92
 ---- batch: 040 ----
mean loss: 215.25
 ---- batch: 050 ----
mean loss: 209.76
 ---- batch: 060 ----
mean loss: 217.09
 ---- batch: 070 ----
mean loss: 219.34
 ---- batch: 080 ----
mean loss: 209.75
 ---- batch: 090 ----
mean loss: 204.18
train mean loss: 212.99
epoch train time: 0:00:17.143953
elapsed time: 0:26:58.146043
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 01:21:32.187938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.84
 ---- batch: 020 ----
mean loss: 204.63
 ---- batch: 030 ----
mean loss: 220.57
 ---- batch: 040 ----
mean loss: 201.39
 ---- batch: 050 ----
mean loss: 207.63
 ---- batch: 060 ----
mean loss: 220.90
 ---- batch: 070 ----
mean loss: 213.77
 ---- batch: 080 ----
mean loss: 209.39
 ---- batch: 090 ----
mean loss: 215.75
train mean loss: 211.51
epoch train time: 0:00:17.031425
elapsed time: 0:27:15.178707
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 01:21:49.220589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.38
 ---- batch: 020 ----
mean loss: 210.36
 ---- batch: 030 ----
mean loss: 214.33
 ---- batch: 040 ----
mean loss: 206.71
 ---- batch: 050 ----
mean loss: 214.08
 ---- batch: 060 ----
mean loss: 209.85
 ---- batch: 070 ----
mean loss: 206.67
 ---- batch: 080 ----
mean loss: 212.56
 ---- batch: 090 ----
mean loss: 213.21
train mean loss: 210.83
epoch train time: 0:00:17.032597
elapsed time: 0:27:32.212546
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 01:22:06.254443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.11
 ---- batch: 020 ----
mean loss: 205.24
 ---- batch: 030 ----
mean loss: 208.16
 ---- batch: 040 ----
mean loss: 217.35
 ---- batch: 050 ----
mean loss: 213.03
 ---- batch: 060 ----
mean loss: 204.89
 ---- batch: 070 ----
mean loss: 217.09
 ---- batch: 080 ----
mean loss: 205.05
 ---- batch: 090 ----
mean loss: 210.92
train mean loss: 211.10
epoch train time: 0:00:16.988496
elapsed time: 0:27:49.202224
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 01:22:23.244127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.76
 ---- batch: 020 ----
mean loss: 207.31
 ---- batch: 030 ----
mean loss: 210.25
 ---- batch: 040 ----
mean loss: 213.23
 ---- batch: 050 ----
mean loss: 210.09
 ---- batch: 060 ----
mean loss: 209.12
 ---- batch: 070 ----
mean loss: 219.58
 ---- batch: 080 ----
mean loss: 219.26
 ---- batch: 090 ----
mean loss: 217.98
train mean loss: 212.39
epoch train time: 0:00:17.041996
elapsed time: 0:28:06.245500
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 01:22:40.287459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.56
 ---- batch: 020 ----
mean loss: 216.20
 ---- batch: 030 ----
mean loss: 204.65
 ---- batch: 040 ----
mean loss: 206.96
 ---- batch: 050 ----
mean loss: 202.45
 ---- batch: 060 ----
mean loss: 213.65
 ---- batch: 070 ----
mean loss: 212.06
 ---- batch: 080 ----
mean loss: 216.99
 ---- batch: 090 ----
mean loss: 208.23
train mean loss: 210.28
epoch train time: 0:00:17.014782
elapsed time: 0:28:23.261575
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 01:22:57.303430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.23
 ---- batch: 020 ----
mean loss: 201.09
 ---- batch: 030 ----
mean loss: 206.06
 ---- batch: 040 ----
mean loss: 203.64
 ---- batch: 050 ----
mean loss: 212.70
 ---- batch: 060 ----
mean loss: 210.64
 ---- batch: 070 ----
mean loss: 204.63
 ---- batch: 080 ----
mean loss: 210.16
 ---- batch: 090 ----
mean loss: 208.71
train mean loss: 208.74
epoch train time: 0:00:17.058197
elapsed time: 0:28:40.320930
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 01:23:14.362905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.68
 ---- batch: 020 ----
mean loss: 207.08
 ---- batch: 030 ----
mean loss: 208.56
 ---- batch: 040 ----
mean loss: 208.87
 ---- batch: 050 ----
mean loss: 216.70
 ---- batch: 060 ----
mean loss: 201.48
 ---- batch: 070 ----
mean loss: 201.28
 ---- batch: 080 ----
mean loss: 209.71
 ---- batch: 090 ----
mean loss: 211.22
train mean loss: 208.67
epoch train time: 0:00:17.010129
elapsed time: 0:28:57.332352
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 01:23:31.374274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.45
 ---- batch: 020 ----
mean loss: 207.01
 ---- batch: 030 ----
mean loss: 209.32
 ---- batch: 040 ----
mean loss: 206.60
 ---- batch: 050 ----
mean loss: 212.59
 ---- batch: 060 ----
mean loss: 211.89
 ---- batch: 070 ----
mean loss: 203.03
 ---- batch: 080 ----
mean loss: 202.62
 ---- batch: 090 ----
mean loss: 207.83
train mean loss: 207.94
epoch train time: 0:00:17.027865
elapsed time: 0:29:14.361477
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 01:23:48.403357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.29
 ---- batch: 020 ----
mean loss: 201.70
 ---- batch: 030 ----
mean loss: 206.02
 ---- batch: 040 ----
mean loss: 206.10
 ---- batch: 050 ----
mean loss: 204.02
 ---- batch: 060 ----
mean loss: 209.37
 ---- batch: 070 ----
mean loss: 205.43
 ---- batch: 080 ----
mean loss: 204.68
 ---- batch: 090 ----
mean loss: 209.44
train mean loss: 206.82
epoch train time: 0:00:17.008243
elapsed time: 0:29:31.371050
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 01:24:05.412948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.78
 ---- batch: 020 ----
mean loss: 207.68
 ---- batch: 030 ----
mean loss: 204.00
 ---- batch: 040 ----
mean loss: 205.91
 ---- batch: 050 ----
mean loss: 208.09
 ---- batch: 060 ----
mean loss: 206.73
 ---- batch: 070 ----
mean loss: 213.22
 ---- batch: 080 ----
mean loss: 197.10
 ---- batch: 090 ----
mean loss: 207.77
train mean loss: 207.19
epoch train time: 0:00:17.019644
elapsed time: 0:29:48.391939
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 01:24:22.433855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.71
 ---- batch: 020 ----
mean loss: 213.24
 ---- batch: 030 ----
mean loss: 209.90
 ---- batch: 040 ----
mean loss: 211.56
 ---- batch: 050 ----
mean loss: 211.05
 ---- batch: 060 ----
mean loss: 210.27
 ---- batch: 070 ----
mean loss: 195.21
 ---- batch: 080 ----
mean loss: 200.19
 ---- batch: 090 ----
mean loss: 200.26
train mean loss: 207.06
epoch train time: 0:00:17.039168
elapsed time: 0:30:05.432358
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 01:24:39.474223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.83
 ---- batch: 020 ----
mean loss: 202.14
 ---- batch: 030 ----
mean loss: 214.18
 ---- batch: 040 ----
mean loss: 209.81
 ---- batch: 050 ----
mean loss: 201.50
 ---- batch: 060 ----
mean loss: 209.17
 ---- batch: 070 ----
mean loss: 208.26
 ---- batch: 080 ----
mean loss: 211.06
 ---- batch: 090 ----
mean loss: 211.87
train mean loss: 207.57
epoch train time: 0:00:17.016500
elapsed time: 0:30:22.450047
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 01:24:56.492069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.55
 ---- batch: 020 ----
mean loss: 208.72
 ---- batch: 030 ----
mean loss: 204.89
 ---- batch: 040 ----
mean loss: 199.40
 ---- batch: 050 ----
mean loss: 208.12
 ---- batch: 060 ----
mean loss: 208.35
 ---- batch: 070 ----
mean loss: 203.28
 ---- batch: 080 ----
mean loss: 207.66
 ---- batch: 090 ----
mean loss: 205.62
train mean loss: 205.60
epoch train time: 0:00:17.050004
elapsed time: 0:30:39.501714
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 01:25:13.543556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.06
 ---- batch: 020 ----
mean loss: 210.61
 ---- batch: 030 ----
mean loss: 202.52
 ---- batch: 040 ----
mean loss: 206.32
 ---- batch: 050 ----
mean loss: 199.79
 ---- batch: 060 ----
mean loss: 207.34
 ---- batch: 070 ----
mean loss: 205.68
 ---- batch: 080 ----
mean loss: 214.90
 ---- batch: 090 ----
mean loss: 208.21
train mean loss: 206.08
epoch train time: 0:00:17.043339
elapsed time: 0:30:56.546306
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 01:25:30.588181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.33
 ---- batch: 020 ----
mean loss: 206.17
 ---- batch: 030 ----
mean loss: 200.66
 ---- batch: 040 ----
mean loss: 204.92
 ---- batch: 050 ----
mean loss: 200.34
 ---- batch: 060 ----
mean loss: 209.72
 ---- batch: 070 ----
mean loss: 207.26
 ---- batch: 080 ----
mean loss: 205.49
 ---- batch: 090 ----
mean loss: 206.10
train mean loss: 206.10
epoch train time: 0:00:17.045113
elapsed time: 0:31:13.592740
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 01:25:47.634723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.66
 ---- batch: 020 ----
mean loss: 204.62
 ---- batch: 030 ----
mean loss: 208.48
 ---- batch: 040 ----
mean loss: 199.79
 ---- batch: 050 ----
mean loss: 202.96
 ---- batch: 060 ----
mean loss: 206.83
 ---- batch: 070 ----
mean loss: 207.41
 ---- batch: 080 ----
mean loss: 205.22
 ---- batch: 090 ----
mean loss: 206.17
train mean loss: 205.89
epoch train time: 0:00:17.058916
elapsed time: 0:31:30.653010
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 01:26:04.694884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.36
 ---- batch: 020 ----
mean loss: 204.43
 ---- batch: 030 ----
mean loss: 210.05
 ---- batch: 040 ----
mean loss: 204.20
 ---- batch: 050 ----
mean loss: 202.70
 ---- batch: 060 ----
mean loss: 210.89
 ---- batch: 070 ----
mean loss: 201.26
 ---- batch: 080 ----
mean loss: 201.94
 ---- batch: 090 ----
mean loss: 205.37
train mean loss: 206.22
epoch train time: 0:00:17.086042
elapsed time: 0:31:47.740466
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 01:26:21.782459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.35
 ---- batch: 020 ----
mean loss: 202.82
 ---- batch: 030 ----
mean loss: 211.76
 ---- batch: 040 ----
mean loss: 209.60
 ---- batch: 050 ----
mean loss: 208.14
 ---- batch: 060 ----
mean loss: 204.70
 ---- batch: 070 ----
mean loss: 209.38
 ---- batch: 080 ----
mean loss: 197.59
 ---- batch: 090 ----
mean loss: 199.59
train mean loss: 203.97
epoch train time: 0:00:17.198500
elapsed time: 0:32:04.940349
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 01:26:38.982242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.68
 ---- batch: 020 ----
mean loss: 206.80
 ---- batch: 030 ----
mean loss: 200.78
 ---- batch: 040 ----
mean loss: 201.68
 ---- batch: 050 ----
mean loss: 198.47
 ---- batch: 060 ----
mean loss: 207.50
 ---- batch: 070 ----
mean loss: 215.58
 ---- batch: 080 ----
mean loss: 209.76
 ---- batch: 090 ----
mean loss: 209.27
train mean loss: 205.48
epoch train time: 0:00:17.005781
elapsed time: 0:32:21.947375
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 01:26:55.989285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.50
 ---- batch: 020 ----
mean loss: 208.55
 ---- batch: 030 ----
mean loss: 203.38
 ---- batch: 040 ----
mean loss: 212.08
 ---- batch: 050 ----
mean loss: 207.42
 ---- batch: 060 ----
mean loss: 214.64
 ---- batch: 070 ----
mean loss: 196.61
 ---- batch: 080 ----
mean loss: 203.14
 ---- batch: 090 ----
mean loss: 202.35
train mean loss: 206.51
epoch train time: 0:00:17.039304
elapsed time: 0:32:38.987927
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 01:27:13.029851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.86
 ---- batch: 020 ----
mean loss: 207.25
 ---- batch: 030 ----
mean loss: 198.96
 ---- batch: 040 ----
mean loss: 203.86
 ---- batch: 050 ----
mean loss: 204.80
 ---- batch: 060 ----
mean loss: 206.51
 ---- batch: 070 ----
mean loss: 207.99
 ---- batch: 080 ----
mean loss: 193.70
 ---- batch: 090 ----
mean loss: 206.16
train mean loss: 203.96
epoch train time: 0:00:16.981388
elapsed time: 0:32:55.970583
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 01:27:30.012471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.07
 ---- batch: 020 ----
mean loss: 218.16
 ---- batch: 030 ----
mean loss: 215.93
 ---- batch: 040 ----
mean loss: 204.74
 ---- batch: 050 ----
mean loss: 213.68
 ---- batch: 060 ----
mean loss: 207.35
 ---- batch: 070 ----
mean loss: 208.12
 ---- batch: 080 ----
mean loss: 199.83
 ---- batch: 090 ----
mean loss: 201.37
train mean loss: 208.06
epoch train time: 0:00:16.998203
elapsed time: 0:33:12.970154
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 01:27:47.012053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.08
 ---- batch: 020 ----
mean loss: 196.73
 ---- batch: 030 ----
mean loss: 201.92
 ---- batch: 040 ----
mean loss: 199.65
 ---- batch: 050 ----
mean loss: 203.85
 ---- batch: 060 ----
mean loss: 207.22
 ---- batch: 070 ----
mean loss: 210.69
 ---- batch: 080 ----
mean loss: 210.23
 ---- batch: 090 ----
mean loss: 209.88
train mean loss: 203.71
epoch train time: 0:00:17.022565
elapsed time: 0:33:29.993979
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 01:28:04.035872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.01
 ---- batch: 020 ----
mean loss: 201.09
 ---- batch: 030 ----
mean loss: 207.06
 ---- batch: 040 ----
mean loss: 205.25
 ---- batch: 050 ----
mean loss: 201.69
 ---- batch: 060 ----
mean loss: 201.43
 ---- batch: 070 ----
mean loss: 219.01
 ---- batch: 080 ----
mean loss: 213.03
 ---- batch: 090 ----
mean loss: 220.47
train mean loss: 207.78
epoch train time: 0:00:17.050400
elapsed time: 0:33:47.045644
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 01:28:21.087538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.03
 ---- batch: 020 ----
mean loss: 206.24
 ---- batch: 030 ----
mean loss: 204.91
 ---- batch: 040 ----
mean loss: 207.99
 ---- batch: 050 ----
mean loss: 206.68
 ---- batch: 060 ----
mean loss: 202.67
 ---- batch: 070 ----
mean loss: 202.58
 ---- batch: 080 ----
mean loss: 204.74
 ---- batch: 090 ----
mean loss: 196.00
train mean loss: 203.47
epoch train time: 0:00:16.984479
elapsed time: 0:34:04.031367
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 01:28:38.073263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.50
 ---- batch: 020 ----
mean loss: 201.13
 ---- batch: 030 ----
mean loss: 199.70
 ---- batch: 040 ----
mean loss: 205.60
 ---- batch: 050 ----
mean loss: 200.72
 ---- batch: 060 ----
mean loss: 207.15
 ---- batch: 070 ----
mean loss: 201.59
 ---- batch: 080 ----
mean loss: 207.76
 ---- batch: 090 ----
mean loss: 201.54
train mean loss: 203.50
epoch train time: 0:00:17.009278
elapsed time: 0:34:21.041858
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 01:28:55.083739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.32
 ---- batch: 020 ----
mean loss: 203.59
 ---- batch: 030 ----
mean loss: 199.22
 ---- batch: 040 ----
mean loss: 209.09
 ---- batch: 050 ----
mean loss: 207.57
 ---- batch: 060 ----
mean loss: 201.25
 ---- batch: 070 ----
mean loss: 195.97
 ---- batch: 080 ----
mean loss: 206.53
 ---- batch: 090 ----
mean loss: 198.23
train mean loss: 203.06
epoch train time: 0:00:17.018323
elapsed time: 0:34:38.061412
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 01:29:12.103154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.33
 ---- batch: 020 ----
mean loss: 203.51
 ---- batch: 030 ----
mean loss: 210.82
 ---- batch: 040 ----
mean loss: 206.11
 ---- batch: 050 ----
mean loss: 214.53
 ---- batch: 060 ----
mean loss: 203.91
 ---- batch: 070 ----
mean loss: 204.40
 ---- batch: 080 ----
mean loss: 209.11
 ---- batch: 090 ----
mean loss: 201.00
train mean loss: 204.80
epoch train time: 0:00:17.028283
elapsed time: 0:34:55.090773
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 01:29:29.132684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.55
 ---- batch: 020 ----
mean loss: 203.17
 ---- batch: 030 ----
mean loss: 199.70
 ---- batch: 040 ----
mean loss: 205.91
 ---- batch: 050 ----
mean loss: 200.71
 ---- batch: 060 ----
mean loss: 200.28
 ---- batch: 070 ----
mean loss: 194.59
 ---- batch: 080 ----
mean loss: 199.42
 ---- batch: 090 ----
mean loss: 196.81
train mean loss: 200.65
epoch train time: 0:00:16.987788
elapsed time: 0:35:12.079841
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 01:29:46.121751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.08
 ---- batch: 020 ----
mean loss: 192.13
 ---- batch: 030 ----
mean loss: 195.90
 ---- batch: 040 ----
mean loss: 204.05
 ---- batch: 050 ----
mean loss: 203.60
 ---- batch: 060 ----
mean loss: 201.17
 ---- batch: 070 ----
mean loss: 204.67
 ---- batch: 080 ----
mean loss: 202.60
 ---- batch: 090 ----
mean loss: 199.82
train mean loss: 200.73
epoch train time: 0:00:17.006043
elapsed time: 0:35:29.087136
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 01:30:03.129000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.25
 ---- batch: 020 ----
mean loss: 197.88
 ---- batch: 030 ----
mean loss: 194.95
 ---- batch: 040 ----
mean loss: 203.97
 ---- batch: 050 ----
mean loss: 200.94
 ---- batch: 060 ----
mean loss: 203.97
 ---- batch: 070 ----
mean loss: 203.17
 ---- batch: 080 ----
mean loss: 202.22
 ---- batch: 090 ----
mean loss: 203.07
train mean loss: 200.74
epoch train time: 0:00:16.988833
elapsed time: 0:35:46.077135
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 01:30:20.119101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.95
 ---- batch: 020 ----
mean loss: 201.84
 ---- batch: 030 ----
mean loss: 197.68
 ---- batch: 040 ----
mean loss: 196.57
 ---- batch: 050 ----
mean loss: 201.55
 ---- batch: 060 ----
mean loss: 198.14
 ---- batch: 070 ----
mean loss: 201.87
 ---- batch: 080 ----
mean loss: 203.86
 ---- batch: 090 ----
mean loss: 206.68
train mean loss: 200.66
epoch train time: 0:00:17.017808
elapsed time: 0:36:03.096336
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 01:30:37.138357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.33
 ---- batch: 020 ----
mean loss: 201.60
 ---- batch: 030 ----
mean loss: 204.97
 ---- batch: 040 ----
mean loss: 205.54
 ---- batch: 050 ----
mean loss: 200.92
 ---- batch: 060 ----
mean loss: 199.23
 ---- batch: 070 ----
mean loss: 198.58
 ---- batch: 080 ----
mean loss: 201.44
 ---- batch: 090 ----
mean loss: 203.87
train mean loss: 201.07
epoch train time: 0:00:17.015907
elapsed time: 0:36:20.114015
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 01:30:54.155787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.95
 ---- batch: 020 ----
mean loss: 199.58
 ---- batch: 030 ----
mean loss: 199.38
 ---- batch: 040 ----
mean loss: 205.38
 ---- batch: 050 ----
mean loss: 200.76
 ---- batch: 060 ----
mean loss: 200.93
 ---- batch: 070 ----
mean loss: 194.64
 ---- batch: 080 ----
mean loss: 197.18
 ---- batch: 090 ----
mean loss: 208.01
train mean loss: 200.41
epoch train time: 0:00:17.026929
elapsed time: 0:36:37.142093
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 01:31:11.183987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.84
 ---- batch: 020 ----
mean loss: 196.65
 ---- batch: 030 ----
mean loss: 203.41
 ---- batch: 040 ----
mean loss: 203.90
 ---- batch: 050 ----
mean loss: 200.04
 ---- batch: 060 ----
mean loss: 195.29
 ---- batch: 070 ----
mean loss: 201.02
 ---- batch: 080 ----
mean loss: 203.75
 ---- batch: 090 ----
mean loss: 207.64
train mean loss: 201.79
epoch train time: 0:00:17.061773
elapsed time: 0:36:54.205258
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 01:31:28.247257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.79
 ---- batch: 020 ----
mean loss: 198.89
 ---- batch: 030 ----
mean loss: 198.30
 ---- batch: 040 ----
mean loss: 204.96
 ---- batch: 050 ----
mean loss: 200.58
 ---- batch: 060 ----
mean loss: 195.95
 ---- batch: 070 ----
mean loss: 203.40
 ---- batch: 080 ----
mean loss: 207.94
 ---- batch: 090 ----
mean loss: 203.14
train mean loss: 201.56
epoch train time: 0:00:17.092424
elapsed time: 0:37:11.298986
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 01:31:45.340864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.36
 ---- batch: 020 ----
mean loss: 197.44
 ---- batch: 030 ----
mean loss: 198.04
 ---- batch: 040 ----
mean loss: 202.73
 ---- batch: 050 ----
mean loss: 198.98
 ---- batch: 060 ----
mean loss: 198.48
 ---- batch: 070 ----
mean loss: 201.36
 ---- batch: 080 ----
mean loss: 200.02
 ---- batch: 090 ----
mean loss: 208.21
train mean loss: 200.20
epoch train time: 0:00:17.078521
elapsed time: 0:37:28.378706
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 01:32:02.420625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.09
 ---- batch: 020 ----
mean loss: 198.15
 ---- batch: 030 ----
mean loss: 207.88
 ---- batch: 040 ----
mean loss: 209.11
 ---- batch: 050 ----
mean loss: 201.69
 ---- batch: 060 ----
mean loss: 191.21
 ---- batch: 070 ----
mean loss: 201.95
 ---- batch: 080 ----
mean loss: 208.69
 ---- batch: 090 ----
mean loss: 199.40
train mean loss: 202.72
epoch train time: 0:00:17.015972
elapsed time: 0:37:45.395952
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 01:32:19.438024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.07
 ---- batch: 020 ----
mean loss: 199.83
 ---- batch: 030 ----
mean loss: 195.15
 ---- batch: 040 ----
mean loss: 195.84
 ---- batch: 050 ----
mean loss: 202.03
 ---- batch: 060 ----
mean loss: 200.45
 ---- batch: 070 ----
mean loss: 196.34
 ---- batch: 080 ----
mean loss: 206.21
 ---- batch: 090 ----
mean loss: 194.97
train mean loss: 199.29
epoch train time: 0:00:17.025262
elapsed time: 0:38:02.422665
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 01:32:36.464571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.84
 ---- batch: 020 ----
mean loss: 191.35
 ---- batch: 030 ----
mean loss: 195.82
 ---- batch: 040 ----
mean loss: 200.54
 ---- batch: 050 ----
mean loss: 202.37
 ---- batch: 060 ----
mean loss: 195.38
 ---- batch: 070 ----
mean loss: 198.16
 ---- batch: 080 ----
mean loss: 202.23
 ---- batch: 090 ----
mean loss: 202.85
train mean loss: 199.90
epoch train time: 0:00:17.012282
elapsed time: 0:38:19.436188
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 01:32:53.478150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.89
 ---- batch: 020 ----
mean loss: 200.31
 ---- batch: 030 ----
mean loss: 198.38
 ---- batch: 040 ----
mean loss: 198.34
 ---- batch: 050 ----
mean loss: 199.37
 ---- batch: 060 ----
mean loss: 208.59
 ---- batch: 070 ----
mean loss: 198.37
 ---- batch: 080 ----
mean loss: 194.45
 ---- batch: 090 ----
mean loss: 195.27
train mean loss: 198.79
epoch train time: 0:00:17.008057
elapsed time: 0:38:36.445529
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 01:33:10.487423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.10
 ---- batch: 020 ----
mean loss: 201.96
 ---- batch: 030 ----
mean loss: 199.65
 ---- batch: 040 ----
mean loss: 203.05
 ---- batch: 050 ----
mean loss: 195.95
 ---- batch: 060 ----
mean loss: 195.60
 ---- batch: 070 ----
mean loss: 194.93
 ---- batch: 080 ----
mean loss: 198.64
 ---- batch: 090 ----
mean loss: 204.76
train mean loss: 200.69
epoch train time: 0:00:17.074544
elapsed time: 0:38:53.521414
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 01:33:27.563343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.84
 ---- batch: 020 ----
mean loss: 196.82
 ---- batch: 030 ----
mean loss: 202.74
 ---- batch: 040 ----
mean loss: 199.66
 ---- batch: 050 ----
mean loss: 196.41
 ---- batch: 060 ----
mean loss: 200.02
 ---- batch: 070 ----
mean loss: 197.64
 ---- batch: 080 ----
mean loss: 197.33
 ---- batch: 090 ----
mean loss: 198.76
train mean loss: 198.97
epoch train time: 0:00:17.024080
elapsed time: 0:39:10.546757
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 01:33:44.588929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.25
 ---- batch: 020 ----
mean loss: 198.96
 ---- batch: 030 ----
mean loss: 199.08
 ---- batch: 040 ----
mean loss: 201.24
 ---- batch: 050 ----
mean loss: 194.96
 ---- batch: 060 ----
mean loss: 199.69
 ---- batch: 070 ----
mean loss: 205.06
 ---- batch: 080 ----
mean loss: 201.04
 ---- batch: 090 ----
mean loss: 194.31
train mean loss: 199.51
epoch train time: 0:00:17.014136
elapsed time: 0:39:27.562509
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 01:34:01.604409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.42
 ---- batch: 020 ----
mean loss: 208.52
 ---- batch: 030 ----
mean loss: 206.76
 ---- batch: 040 ----
mean loss: 196.28
 ---- batch: 050 ----
mean loss: 202.46
 ---- batch: 060 ----
mean loss: 195.93
 ---- batch: 070 ----
mean loss: 195.90
 ---- batch: 080 ----
mean loss: 198.29
 ---- batch: 090 ----
mean loss: 205.02
train mean loss: 201.41
epoch train time: 0:00:16.996357
elapsed time: 0:39:44.560170
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 01:34:18.602102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.33
 ---- batch: 020 ----
mean loss: 194.81
 ---- batch: 030 ----
mean loss: 197.86
 ---- batch: 040 ----
mean loss: 196.56
 ---- batch: 050 ----
mean loss: 193.10
 ---- batch: 060 ----
mean loss: 200.89
 ---- batch: 070 ----
mean loss: 201.36
 ---- batch: 080 ----
mean loss: 205.52
 ---- batch: 090 ----
mean loss: 192.69
train mean loss: 197.04
epoch train time: 0:00:16.961294
elapsed time: 0:40:01.522827
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 01:34:35.564728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.34
 ---- batch: 020 ----
mean loss: 196.25
 ---- batch: 030 ----
mean loss: 195.98
 ---- batch: 040 ----
mean loss: 199.75
 ---- batch: 050 ----
mean loss: 199.25
 ---- batch: 060 ----
mean loss: 197.11
 ---- batch: 070 ----
mean loss: 197.45
 ---- batch: 080 ----
mean loss: 198.22
 ---- batch: 090 ----
mean loss: 202.55
train mean loss: 197.58
epoch train time: 0:00:16.995997
elapsed time: 0:40:18.520017
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 01:34:52.561981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.14
 ---- batch: 020 ----
mean loss: 200.23
 ---- batch: 030 ----
mean loss: 206.04
 ---- batch: 040 ----
mean loss: 185.14
 ---- batch: 050 ----
mean loss: 204.06
 ---- batch: 060 ----
mean loss: 195.99
 ---- batch: 070 ----
mean loss: 199.43
 ---- batch: 080 ----
mean loss: 194.03
 ---- batch: 090 ----
mean loss: 201.77
train mean loss: 197.77
epoch train time: 0:00:16.990310
elapsed time: 0:40:35.511671
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 01:35:09.553568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.67
 ---- batch: 020 ----
mean loss: 194.04
 ---- batch: 030 ----
mean loss: 202.46
 ---- batch: 040 ----
mean loss: 203.68
 ---- batch: 050 ----
mean loss: 207.06
 ---- batch: 060 ----
mean loss: 203.29
 ---- batch: 070 ----
mean loss: 202.93
 ---- batch: 080 ----
mean loss: 211.03
 ---- batch: 090 ----
mean loss: 200.90
train mean loss: 202.63
epoch train time: 0:00:17.026753
elapsed time: 0:40:52.539660
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 01:35:26.581555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.86
 ---- batch: 020 ----
mean loss: 204.80
 ---- batch: 030 ----
mean loss: 192.22
 ---- batch: 040 ----
mean loss: 201.29
 ---- batch: 050 ----
mean loss: 193.12
 ---- batch: 060 ----
mean loss: 189.80
 ---- batch: 070 ----
mean loss: 197.50
 ---- batch: 080 ----
mean loss: 191.86
 ---- batch: 090 ----
mean loss: 192.80
train mean loss: 197.46
epoch train time: 0:00:17.029938
elapsed time: 0:41:09.570842
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 01:35:43.612695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.77
 ---- batch: 020 ----
mean loss: 205.74
 ---- batch: 030 ----
mean loss: 198.03
 ---- batch: 040 ----
mean loss: 197.07
 ---- batch: 050 ----
mean loss: 189.46
 ---- batch: 060 ----
mean loss: 202.80
 ---- batch: 070 ----
mean loss: 199.26
 ---- batch: 080 ----
mean loss: 194.78
 ---- batch: 090 ----
mean loss: 198.46
train mean loss: 197.58
epoch train time: 0:00:17.017130
elapsed time: 0:41:26.589292
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 01:36:00.631284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.13
 ---- batch: 020 ----
mean loss: 201.64
 ---- batch: 030 ----
mean loss: 192.93
 ---- batch: 040 ----
mean loss: 201.89
 ---- batch: 050 ----
mean loss: 202.40
 ---- batch: 060 ----
mean loss: 198.97
 ---- batch: 070 ----
mean loss: 192.51
 ---- batch: 080 ----
mean loss: 202.32
 ---- batch: 090 ----
mean loss: 196.50
train mean loss: 197.33
epoch train time: 0:00:16.989230
elapsed time: 0:41:43.579886
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 01:36:17.621801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.45
 ---- batch: 020 ----
mean loss: 200.50
 ---- batch: 030 ----
mean loss: 202.17
 ---- batch: 040 ----
mean loss: 199.42
 ---- batch: 050 ----
mean loss: 196.48
 ---- batch: 060 ----
mean loss: 196.12
 ---- batch: 070 ----
mean loss: 196.05
 ---- batch: 080 ----
mean loss: 191.83
 ---- batch: 090 ----
mean loss: 193.03
train mean loss: 196.84
epoch train time: 0:00:17.018687
elapsed time: 0:42:00.599794
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 01:36:34.641759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.61
 ---- batch: 020 ----
mean loss: 200.94
 ---- batch: 030 ----
mean loss: 200.09
 ---- batch: 040 ----
mean loss: 196.32
 ---- batch: 050 ----
mean loss: 196.81
 ---- batch: 060 ----
mean loss: 201.23
 ---- batch: 070 ----
mean loss: 191.01
 ---- batch: 080 ----
mean loss: 198.80
 ---- batch: 090 ----
mean loss: 200.19
train mean loss: 197.42
epoch train time: 0:00:17.040885
elapsed time: 0:42:17.641985
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 01:36:51.684024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.68
 ---- batch: 020 ----
mean loss: 197.38
 ---- batch: 030 ----
mean loss: 197.29
 ---- batch: 040 ----
mean loss: 198.48
 ---- batch: 050 ----
mean loss: 188.94
 ---- batch: 060 ----
mean loss: 192.00
 ---- batch: 070 ----
mean loss: 198.46
 ---- batch: 080 ----
mean loss: 200.04
 ---- batch: 090 ----
mean loss: 197.23
train mean loss: 196.54
epoch train time: 0:00:17.102026
elapsed time: 0:42:34.746186
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 01:37:08.787796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.37
 ---- batch: 020 ----
mean loss: 199.00
 ---- batch: 030 ----
mean loss: 194.38
 ---- batch: 040 ----
mean loss: 198.85
 ---- batch: 050 ----
mean loss: 197.58
 ---- batch: 060 ----
mean loss: 201.45
 ---- batch: 070 ----
mean loss: 197.42
 ---- batch: 080 ----
mean loss: 192.72
 ---- batch: 090 ----
mean loss: 196.33
train mean loss: 196.66
epoch train time: 0:00:17.006598
elapsed time: 0:42:51.753750
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 01:37:25.795660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.04
 ---- batch: 020 ----
mean loss: 201.21
 ---- batch: 030 ----
mean loss: 191.77
 ---- batch: 040 ----
mean loss: 202.93
 ---- batch: 050 ----
mean loss: 198.06
 ---- batch: 060 ----
mean loss: 202.12
 ---- batch: 070 ----
mean loss: 187.76
 ---- batch: 080 ----
mean loss: 203.97
 ---- batch: 090 ----
mean loss: 196.68
train mean loss: 198.44
epoch train time: 0:00:16.988232
elapsed time: 0:43:08.743213
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 01:37:42.785106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.71
 ---- batch: 020 ----
mean loss: 196.24
 ---- batch: 030 ----
mean loss: 192.88
 ---- batch: 040 ----
mean loss: 196.73
 ---- batch: 050 ----
mean loss: 195.45
 ---- batch: 060 ----
mean loss: 212.20
 ---- batch: 070 ----
mean loss: 202.72
 ---- batch: 080 ----
mean loss: 200.19
 ---- batch: 090 ----
mean loss: 204.54
train mean loss: 199.11
epoch train time: 0:00:16.989959
elapsed time: 0:43:25.734450
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 01:37:59.776317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.53
 ---- batch: 020 ----
mean loss: 200.39
 ---- batch: 030 ----
mean loss: 199.76
 ---- batch: 040 ----
mean loss: 190.38
 ---- batch: 050 ----
mean loss: 194.43
 ---- batch: 060 ----
mean loss: 195.20
 ---- batch: 070 ----
mean loss: 198.21
 ---- batch: 080 ----
mean loss: 190.86
 ---- batch: 090 ----
mean loss: 196.14
train mean loss: 195.83
epoch train time: 0:00:16.946628
elapsed time: 0:43:42.682300
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 01:38:16.724189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.57
 ---- batch: 020 ----
mean loss: 190.26
 ---- batch: 030 ----
mean loss: 201.03
 ---- batch: 040 ----
mean loss: 201.44
 ---- batch: 050 ----
mean loss: 193.39
 ---- batch: 060 ----
mean loss: 192.10
 ---- batch: 070 ----
mean loss: 192.76
 ---- batch: 080 ----
mean loss: 192.37
 ---- batch: 090 ----
mean loss: 194.28
train mean loss: 195.29
epoch train time: 0:00:17.034730
elapsed time: 0:43:59.718304
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 01:38:33.760221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.10
 ---- batch: 020 ----
mean loss: 196.46
 ---- batch: 030 ----
mean loss: 190.82
 ---- batch: 040 ----
mean loss: 194.05
 ---- batch: 050 ----
mean loss: 196.93
 ---- batch: 060 ----
mean loss: 207.65
 ---- batch: 070 ----
mean loss: 199.37
 ---- batch: 080 ----
mean loss: 197.04
 ---- batch: 090 ----
mean loss: 194.61
train mean loss: 196.53
epoch train time: 0:00:17.006222
elapsed time: 0:44:16.725816
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 01:38:50.767733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.51
 ---- batch: 020 ----
mean loss: 197.98
 ---- batch: 030 ----
mean loss: 197.36
 ---- batch: 040 ----
mean loss: 191.17
 ---- batch: 050 ----
mean loss: 189.22
 ---- batch: 060 ----
mean loss: 201.28
 ---- batch: 070 ----
mean loss: 195.48
 ---- batch: 080 ----
mean loss: 191.06
 ---- batch: 090 ----
mean loss: 191.80
train mean loss: 194.78
epoch train time: 0:00:17.019653
elapsed time: 0:44:33.746830
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 01:39:07.788855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.72
 ---- batch: 020 ----
mean loss: 189.91
 ---- batch: 030 ----
mean loss: 201.58
 ---- batch: 040 ----
mean loss: 204.07
 ---- batch: 050 ----
mean loss: 186.52
 ---- batch: 060 ----
mean loss: 196.46
 ---- batch: 070 ----
mean loss: 201.42
 ---- batch: 080 ----
mean loss: 214.13
 ---- batch: 090 ----
mean loss: 196.13
train mean loss: 198.02
epoch train time: 0:00:16.989421
elapsed time: 0:44:50.737638
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 01:39:24.779577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.60
 ---- batch: 020 ----
mean loss: 203.55
 ---- batch: 030 ----
mean loss: 195.38
 ---- batch: 040 ----
mean loss: 197.53
 ---- batch: 050 ----
mean loss: 199.50
 ---- batch: 060 ----
mean loss: 198.28
 ---- batch: 070 ----
mean loss: 191.92
 ---- batch: 080 ----
mean loss: 191.45
 ---- batch: 090 ----
mean loss: 193.49
train mean loss: 196.22
epoch train time: 0:00:17.022657
elapsed time: 0:45:07.761667
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 01:39:41.803548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.02
 ---- batch: 020 ----
mean loss: 193.98
 ---- batch: 030 ----
mean loss: 193.00
 ---- batch: 040 ----
mean loss: 188.17
 ---- batch: 050 ----
mean loss: 189.94
 ---- batch: 060 ----
mean loss: 192.04
 ---- batch: 070 ----
mean loss: 197.07
 ---- batch: 080 ----
mean loss: 200.14
 ---- batch: 090 ----
mean loss: 193.13
train mean loss: 194.77
epoch train time: 0:00:17.032091
elapsed time: 0:45:24.794958
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 01:39:58.836880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.78
 ---- batch: 020 ----
mean loss: 186.06
 ---- batch: 030 ----
mean loss: 197.43
 ---- batch: 040 ----
mean loss: 196.53
 ---- batch: 050 ----
mean loss: 199.44
 ---- batch: 060 ----
mean loss: 195.35
 ---- batch: 070 ----
mean loss: 199.09
 ---- batch: 080 ----
mean loss: 195.90
 ---- batch: 090 ----
mean loss: 194.61
train mean loss: 194.86
epoch train time: 0:00:16.960174
elapsed time: 0:45:41.756396
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 01:40:15.798281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.60
 ---- batch: 020 ----
mean loss: 195.17
 ---- batch: 030 ----
mean loss: 190.73
 ---- batch: 040 ----
mean loss: 199.52
 ---- batch: 050 ----
mean loss: 196.31
 ---- batch: 060 ----
mean loss: 193.26
 ---- batch: 070 ----
mean loss: 205.08
 ---- batch: 080 ----
mean loss: 192.82
 ---- batch: 090 ----
mean loss: 190.44
train mean loss: 194.67
epoch train time: 0:00:17.007589
elapsed time: 0:45:58.765198
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 01:40:32.807048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.03
 ---- batch: 020 ----
mean loss: 189.32
 ---- batch: 030 ----
mean loss: 196.94
 ---- batch: 040 ----
mean loss: 192.52
 ---- batch: 050 ----
mean loss: 193.77
 ---- batch: 060 ----
mean loss: 203.30
 ---- batch: 070 ----
mean loss: 200.73
 ---- batch: 080 ----
mean loss: 196.26
 ---- batch: 090 ----
mean loss: 195.66
train mean loss: 195.61
epoch train time: 0:00:16.998195
elapsed time: 0:46:15.764579
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 01:40:49.806457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.44
 ---- batch: 020 ----
mean loss: 204.71
 ---- batch: 030 ----
mean loss: 202.60
 ---- batch: 040 ----
mean loss: 205.25
 ---- batch: 050 ----
mean loss: 190.38
 ---- batch: 060 ----
mean loss: 196.15
 ---- batch: 070 ----
mean loss: 195.58
 ---- batch: 080 ----
mean loss: 197.98
 ---- batch: 090 ----
mean loss: 190.55
train mean loss: 196.94
epoch train time: 0:00:16.996295
elapsed time: 0:46:32.762129
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 01:41:06.804037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.19
 ---- batch: 020 ----
mean loss: 186.86
 ---- batch: 030 ----
mean loss: 191.65
 ---- batch: 040 ----
mean loss: 195.48
 ---- batch: 050 ----
mean loss: 194.71
 ---- batch: 060 ----
mean loss: 196.81
 ---- batch: 070 ----
mean loss: 196.00
 ---- batch: 080 ----
mean loss: 187.63
 ---- batch: 090 ----
mean loss: 195.73
train mean loss: 193.78
epoch train time: 0:00:17.023577
elapsed time: 0:46:49.786986
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 01:41:23.828913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.19
 ---- batch: 020 ----
mean loss: 189.80
 ---- batch: 030 ----
mean loss: 191.37
 ---- batch: 040 ----
mean loss: 188.54
 ---- batch: 050 ----
mean loss: 200.24
 ---- batch: 060 ----
mean loss: 195.56
 ---- batch: 070 ----
mean loss: 196.82
 ---- batch: 080 ----
mean loss: 194.12
 ---- batch: 090 ----
mean loss: 203.81
train mean loss: 194.70
epoch train time: 0:00:17.003926
elapsed time: 0:47:06.792252
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 01:41:40.834319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.89
 ---- batch: 020 ----
mean loss: 194.35
 ---- batch: 030 ----
mean loss: 188.25
 ---- batch: 040 ----
mean loss: 206.12
 ---- batch: 050 ----
mean loss: 193.86
 ---- batch: 060 ----
mean loss: 202.95
 ---- batch: 070 ----
mean loss: 191.25
 ---- batch: 080 ----
mean loss: 191.07
 ---- batch: 090 ----
mean loss: 196.74
train mean loss: 195.18
epoch train time: 0:00:17.040743
elapsed time: 0:47:23.834415
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 01:41:57.876286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.82
 ---- batch: 020 ----
mean loss: 195.19
 ---- batch: 030 ----
mean loss: 197.07
 ---- batch: 040 ----
mean loss: 193.17
 ---- batch: 050 ----
mean loss: 192.52
 ---- batch: 060 ----
mean loss: 191.84
 ---- batch: 070 ----
mean loss: 183.65
 ---- batch: 080 ----
mean loss: 202.19
 ---- batch: 090 ----
mean loss: 202.87
train mean loss: 195.18
epoch train time: 0:00:17.091143
elapsed time: 0:47:40.926764
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 01:42:14.968656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.82
 ---- batch: 020 ----
mean loss: 191.59
 ---- batch: 030 ----
mean loss: 197.58
 ---- batch: 040 ----
mean loss: 187.77
 ---- batch: 050 ----
mean loss: 194.00
 ---- batch: 060 ----
mean loss: 191.08
 ---- batch: 070 ----
mean loss: 197.93
 ---- batch: 080 ----
mean loss: 200.74
 ---- batch: 090 ----
mean loss: 194.14
train mean loss: 194.42
epoch train time: 0:00:17.001593
elapsed time: 0:47:57.929576
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 01:42:31.971468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.66
 ---- batch: 020 ----
mean loss: 195.70
 ---- batch: 030 ----
mean loss: 189.96
 ---- batch: 040 ----
mean loss: 198.31
 ---- batch: 050 ----
mean loss: 199.32
 ---- batch: 060 ----
mean loss: 206.10
 ---- batch: 070 ----
mean loss: 189.36
 ---- batch: 080 ----
mean loss: 197.56
 ---- batch: 090 ----
mean loss: 194.78
train mean loss: 196.99
epoch train time: 0:00:16.981957
elapsed time: 0:48:14.912847
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 01:42:48.954748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.60
 ---- batch: 020 ----
mean loss: 191.43
 ---- batch: 030 ----
mean loss: 188.73
 ---- batch: 040 ----
mean loss: 192.32
 ---- batch: 050 ----
mean loss: 192.64
 ---- batch: 060 ----
mean loss: 198.03
 ---- batch: 070 ----
mean loss: 196.51
 ---- batch: 080 ----
mean loss: 196.07
 ---- batch: 090 ----
mean loss: 189.27
train mean loss: 193.74
epoch train time: 0:00:17.012594
elapsed time: 0:48:31.926743
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 01:43:05.968639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.94
 ---- batch: 020 ----
mean loss: 193.45
 ---- batch: 030 ----
mean loss: 191.96
 ---- batch: 040 ----
mean loss: 193.68
 ---- batch: 050 ----
mean loss: 203.61
 ---- batch: 060 ----
mean loss: 192.14
 ---- batch: 070 ----
mean loss: 193.84
 ---- batch: 080 ----
mean loss: 184.19
 ---- batch: 090 ----
mean loss: 197.30
train mean loss: 195.08
epoch train time: 0:00:16.984099
elapsed time: 0:48:48.912142
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 01:43:22.954060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.39
 ---- batch: 020 ----
mean loss: 191.39
 ---- batch: 030 ----
mean loss: 187.96
 ---- batch: 040 ----
mean loss: 195.90
 ---- batch: 050 ----
mean loss: 192.43
 ---- batch: 060 ----
mean loss: 196.74
 ---- batch: 070 ----
mean loss: 199.57
 ---- batch: 080 ----
mean loss: 188.94
 ---- batch: 090 ----
mean loss: 198.85
train mean loss: 193.35
epoch train time: 0:00:17.091619
elapsed time: 0:49:06.005010
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 01:43:40.046909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.46
 ---- batch: 020 ----
mean loss: 190.94
 ---- batch: 030 ----
mean loss: 192.82
 ---- batch: 040 ----
mean loss: 196.15
 ---- batch: 050 ----
mean loss: 190.73
 ---- batch: 060 ----
mean loss: 190.59
 ---- batch: 070 ----
mean loss: 196.32
 ---- batch: 080 ----
mean loss: 192.52
 ---- batch: 090 ----
mean loss: 188.99
train mean loss: 193.42
epoch train time: 0:00:17.008753
elapsed time: 0:49:23.015076
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 01:43:57.057087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.46
 ---- batch: 020 ----
mean loss: 187.99
 ---- batch: 030 ----
mean loss: 196.50
 ---- batch: 040 ----
mean loss: 188.71
 ---- batch: 050 ----
mean loss: 189.83
 ---- batch: 060 ----
mean loss: 197.22
 ---- batch: 070 ----
mean loss: 196.90
 ---- batch: 080 ----
mean loss: 204.39
 ---- batch: 090 ----
mean loss: 190.08
train mean loss: 193.31
epoch train time: 0:00:17.015036
elapsed time: 0:49:40.031842
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 01:44:14.073383
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.89
 ---- batch: 020 ----
mean loss: 200.23
 ---- batch: 030 ----
mean loss: 200.35
 ---- batch: 040 ----
mean loss: 186.46
 ---- batch: 050 ----
mean loss: 190.46
 ---- batch: 060 ----
mean loss: 199.83
 ---- batch: 070 ----
mean loss: 191.01
 ---- batch: 080 ----
mean loss: 194.19
 ---- batch: 090 ----
mean loss: 192.46
train mean loss: 194.08
epoch train time: 0:00:17.032308
elapsed time: 0:49:57.065099
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 01:44:31.106990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.95
 ---- batch: 020 ----
mean loss: 188.73
 ---- batch: 030 ----
mean loss: 191.10
 ---- batch: 040 ----
mean loss: 194.38
 ---- batch: 050 ----
mean loss: 194.04
 ---- batch: 060 ----
mean loss: 196.60
 ---- batch: 070 ----
mean loss: 198.61
 ---- batch: 080 ----
mean loss: 186.05
 ---- batch: 090 ----
mean loss: 192.10
train mean loss: 193.07
epoch train time: 0:00:17.004285
elapsed time: 0:50:14.070592
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 01:44:48.112562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.82
 ---- batch: 020 ----
mean loss: 193.76
 ---- batch: 030 ----
mean loss: 191.70
 ---- batch: 040 ----
mean loss: 195.11
 ---- batch: 050 ----
mean loss: 197.20
 ---- batch: 060 ----
mean loss: 191.78
 ---- batch: 070 ----
mean loss: 185.58
 ---- batch: 080 ----
mean loss: 185.51
 ---- batch: 090 ----
mean loss: 196.84
train mean loss: 191.68
epoch train time: 0:00:17.018382
elapsed time: 0:50:31.090310
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 01:45:05.132277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.07
 ---- batch: 020 ----
mean loss: 195.64
 ---- batch: 030 ----
mean loss: 194.72
 ---- batch: 040 ----
mean loss: 187.49
 ---- batch: 050 ----
mean loss: 188.24
 ---- batch: 060 ----
mean loss: 193.17
 ---- batch: 070 ----
mean loss: 196.94
 ---- batch: 080 ----
mean loss: 188.82
 ---- batch: 090 ----
mean loss: 187.41
train mean loss: 191.83
epoch train time: 0:00:17.002627
elapsed time: 0:50:48.094250
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 01:45:22.136125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.06
 ---- batch: 020 ----
mean loss: 192.85
 ---- batch: 030 ----
mean loss: 194.79
 ---- batch: 040 ----
mean loss: 193.05
 ---- batch: 050 ----
mean loss: 189.21
 ---- batch: 060 ----
mean loss: 193.55
 ---- batch: 070 ----
mean loss: 191.42
 ---- batch: 080 ----
mean loss: 182.26
 ---- batch: 090 ----
mean loss: 192.13
train mean loss: 191.51
epoch train time: 0:00:17.027272
elapsed time: 0:51:05.122714
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 01:45:39.164633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.84
 ---- batch: 020 ----
mean loss: 190.91
 ---- batch: 030 ----
mean loss: 194.04
 ---- batch: 040 ----
mean loss: 187.37
 ---- batch: 050 ----
mean loss: 194.68
 ---- batch: 060 ----
mean loss: 193.35
 ---- batch: 070 ----
mean loss: 187.85
 ---- batch: 080 ----
mean loss: 192.75
 ---- batch: 090 ----
mean loss: 195.35
train mean loss: 192.21
epoch train time: 0:00:17.014017
elapsed time: 0:51:22.138061
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 01:45:56.179946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.15
 ---- batch: 020 ----
mean loss: 192.04
 ---- batch: 030 ----
mean loss: 192.72
 ---- batch: 040 ----
mean loss: 195.15
 ---- batch: 050 ----
mean loss: 191.84
 ---- batch: 060 ----
mean loss: 184.15
 ---- batch: 070 ----
mean loss: 188.24
 ---- batch: 080 ----
mean loss: 190.69
 ---- batch: 090 ----
mean loss: 189.16
train mean loss: 191.40
epoch train time: 0:00:17.013755
elapsed time: 0:51:39.153050
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 01:46:13.194989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.79
 ---- batch: 020 ----
mean loss: 195.67
 ---- batch: 030 ----
mean loss: 192.24
 ---- batch: 040 ----
mean loss: 191.73
 ---- batch: 050 ----
mean loss: 188.14
 ---- batch: 060 ----
mean loss: 193.82
 ---- batch: 070 ----
mean loss: 190.37
 ---- batch: 080 ----
mean loss: 191.48
 ---- batch: 090 ----
mean loss: 189.02
train mean loss: 191.63
epoch train time: 0:00:17.020114
elapsed time: 0:51:56.174502
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 01:46:30.216415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.98
 ---- batch: 020 ----
mean loss: 193.21
 ---- batch: 030 ----
mean loss: 198.46
 ---- batch: 040 ----
mean loss: 195.40
 ---- batch: 050 ----
mean loss: 187.03
 ---- batch: 060 ----
mean loss: 196.68
 ---- batch: 070 ----
mean loss: 188.84
 ---- batch: 080 ----
mean loss: 192.68
 ---- batch: 090 ----
mean loss: 184.49
train mean loss: 192.47
epoch train time: 0:00:16.971658
elapsed time: 0:52:13.147386
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 01:46:47.189291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.15
 ---- batch: 020 ----
mean loss: 193.00
 ---- batch: 030 ----
mean loss: 186.48
 ---- batch: 040 ----
mean loss: 195.87
 ---- batch: 050 ----
mean loss: 193.25
 ---- batch: 060 ----
mean loss: 191.90
 ---- batch: 070 ----
mean loss: 190.31
 ---- batch: 080 ----
mean loss: 195.21
 ---- batch: 090 ----
mean loss: 191.06
train mean loss: 192.13
epoch train time: 0:00:17.026858
elapsed time: 0:52:30.175534
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 01:47:04.217450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.29
 ---- batch: 020 ----
mean loss: 194.19
 ---- batch: 030 ----
mean loss: 188.99
 ---- batch: 040 ----
mean loss: 195.62
 ---- batch: 050 ----
mean loss: 191.20
 ---- batch: 060 ----
mean loss: 196.26
 ---- batch: 070 ----
mean loss: 189.73
 ---- batch: 080 ----
mean loss: 187.95
 ---- batch: 090 ----
mean loss: 192.85
train mean loss: 192.19
epoch train time: 0:00:17.130122
elapsed time: 0:52:47.306869
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 01:47:21.348740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.47
 ---- batch: 020 ----
mean loss: 187.30
 ---- batch: 030 ----
mean loss: 187.42
 ---- batch: 040 ----
mean loss: 200.31
 ---- batch: 050 ----
mean loss: 202.36
 ---- batch: 060 ----
mean loss: 195.47
 ---- batch: 070 ----
mean loss: 192.12
 ---- batch: 080 ----
mean loss: 188.61
 ---- batch: 090 ----
mean loss: 193.19
train mean loss: 193.23
epoch train time: 0:00:16.993430
elapsed time: 0:53:04.301444
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 01:47:38.343345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.87
 ---- batch: 020 ----
mean loss: 185.53
 ---- batch: 030 ----
mean loss: 194.80
 ---- batch: 040 ----
mean loss: 189.57
 ---- batch: 050 ----
mean loss: 188.16
 ---- batch: 060 ----
mean loss: 194.09
 ---- batch: 070 ----
mean loss: 196.21
 ---- batch: 080 ----
mean loss: 188.28
 ---- batch: 090 ----
mean loss: 197.87
train mean loss: 191.31
epoch train time: 0:00:16.997676
elapsed time: 0:53:21.300393
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 01:47:55.342410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.42
 ---- batch: 020 ----
mean loss: 198.57
 ---- batch: 030 ----
mean loss: 191.54
 ---- batch: 040 ----
mean loss: 191.29
 ---- batch: 050 ----
mean loss: 192.63
 ---- batch: 060 ----
mean loss: 191.62
 ---- batch: 070 ----
mean loss: 193.67
 ---- batch: 080 ----
mean loss: 193.46
 ---- batch: 090 ----
mean loss: 190.98
train mean loss: 192.61
epoch train time: 0:00:17.051664
elapsed time: 0:53:38.353473
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 01:48:12.395425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.09
 ---- batch: 020 ----
mean loss: 196.30
 ---- batch: 030 ----
mean loss: 186.46
 ---- batch: 040 ----
mean loss: 185.95
 ---- batch: 050 ----
mean loss: 191.34
 ---- batch: 060 ----
mean loss: 192.36
 ---- batch: 070 ----
mean loss: 194.16
 ---- batch: 080 ----
mean loss: 194.96
 ---- batch: 090 ----
mean loss: 191.60
train mean loss: 191.42
epoch train time: 0:00:16.990469
elapsed time: 0:53:55.345310
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 01:48:29.387170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.05
 ---- batch: 020 ----
mean loss: 192.49
 ---- batch: 030 ----
mean loss: 211.05
 ---- batch: 040 ----
mean loss: 194.13
 ---- batch: 050 ----
mean loss: 195.03
 ---- batch: 060 ----
mean loss: 181.85
 ---- batch: 070 ----
mean loss: 187.96
 ---- batch: 080 ----
mean loss: 199.24
 ---- batch: 090 ----
mean loss: 199.08
train mean loss: 194.23
epoch train time: 0:00:17.071553
elapsed time: 0:54:12.418067
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 01:48:46.459965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.79
 ---- batch: 020 ----
mean loss: 188.65
 ---- batch: 030 ----
mean loss: 196.64
 ---- batch: 040 ----
mean loss: 203.43
 ---- batch: 050 ----
mean loss: 199.96
 ---- batch: 060 ----
mean loss: 190.90
 ---- batch: 070 ----
mean loss: 192.11
 ---- batch: 080 ----
mean loss: 193.69
 ---- batch: 090 ----
mean loss: 182.71
train mean loss: 192.94
epoch train time: 0:00:17.059465
elapsed time: 0:54:29.478842
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 01:49:03.520753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.07
 ---- batch: 020 ----
mean loss: 194.08
 ---- batch: 030 ----
mean loss: 194.53
 ---- batch: 040 ----
mean loss: 185.76
 ---- batch: 050 ----
mean loss: 189.75
 ---- batch: 060 ----
mean loss: 198.61
 ---- batch: 070 ----
mean loss: 192.49
 ---- batch: 080 ----
mean loss: 187.51
 ---- batch: 090 ----
mean loss: 190.38
train mean loss: 191.52
epoch train time: 0:00:17.016827
elapsed time: 0:54:46.497001
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 01:49:20.538892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.61
 ---- batch: 020 ----
mean loss: 196.45
 ---- batch: 030 ----
mean loss: 188.46
 ---- batch: 040 ----
mean loss: 190.12
 ---- batch: 050 ----
mean loss: 192.41
 ---- batch: 060 ----
mean loss: 195.07
 ---- batch: 070 ----
mean loss: 185.39
 ---- batch: 080 ----
mean loss: 185.08
 ---- batch: 090 ----
mean loss: 190.19
train mean loss: 190.69
epoch train time: 0:00:17.051989
elapsed time: 0:55:03.550260
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 01:49:37.592166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.42
 ---- batch: 020 ----
mean loss: 185.33
 ---- batch: 030 ----
mean loss: 190.09
 ---- batch: 040 ----
mean loss: 189.73
 ---- batch: 050 ----
mean loss: 184.54
 ---- batch: 060 ----
mean loss: 181.07
 ---- batch: 070 ----
mean loss: 196.46
 ---- batch: 080 ----
mean loss: 196.05
 ---- batch: 090 ----
mean loss: 197.12
train mean loss: 190.30
epoch train time: 0:00:17.060357
elapsed time: 0:55:20.611873
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 01:49:54.653859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.17
 ---- batch: 020 ----
mean loss: 191.97
 ---- batch: 030 ----
mean loss: 191.36
 ---- batch: 040 ----
mean loss: 190.90
 ---- batch: 050 ----
mean loss: 184.06
 ---- batch: 060 ----
mean loss: 193.16
 ---- batch: 070 ----
mean loss: 192.20
 ---- batch: 080 ----
mean loss: 193.00
 ---- batch: 090 ----
mean loss: 188.25
train mean loss: 190.73
epoch train time: 0:00:17.099475
elapsed time: 0:55:37.712760
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 01:50:11.754625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.08
 ---- batch: 020 ----
mean loss: 194.43
 ---- batch: 030 ----
mean loss: 197.53
 ---- batch: 040 ----
mean loss: 187.14
 ---- batch: 050 ----
mean loss: 189.28
 ---- batch: 060 ----
mean loss: 192.66
 ---- batch: 070 ----
mean loss: 192.03
 ---- batch: 080 ----
mean loss: 190.51
 ---- batch: 090 ----
mean loss: 186.98
train mean loss: 190.34
epoch train time: 0:00:17.069604
elapsed time: 0:55:54.783606
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 01:50:28.825515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.12
 ---- batch: 020 ----
mean loss: 195.78
 ---- batch: 030 ----
mean loss: 191.51
 ---- batch: 040 ----
mean loss: 188.21
 ---- batch: 050 ----
mean loss: 188.75
 ---- batch: 060 ----
mean loss: 189.40
 ---- batch: 070 ----
mean loss: 188.97
 ---- batch: 080 ----
mean loss: 183.34
 ---- batch: 090 ----
mean loss: 193.93
train mean loss: 190.36
epoch train time: 0:00:17.050962
elapsed time: 0:56:11.835775
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 01:50:45.877724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.58
 ---- batch: 020 ----
mean loss: 192.08
 ---- batch: 030 ----
mean loss: 190.57
 ---- batch: 040 ----
mean loss: 187.90
 ---- batch: 050 ----
mean loss: 188.87
 ---- batch: 060 ----
mean loss: 195.15
 ---- batch: 070 ----
mean loss: 185.78
 ---- batch: 080 ----
mean loss: 188.41
 ---- batch: 090 ----
mean loss: 196.82
train mean loss: 190.21
epoch train time: 0:00:17.032062
elapsed time: 0:56:28.869149
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 01:51:02.911037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.17
 ---- batch: 020 ----
mean loss: 186.27
 ---- batch: 030 ----
mean loss: 187.95
 ---- batch: 040 ----
mean loss: 194.60
 ---- batch: 050 ----
mean loss: 196.67
 ---- batch: 060 ----
mean loss: 191.27
 ---- batch: 070 ----
mean loss: 190.99
 ---- batch: 080 ----
mean loss: 195.24
 ---- batch: 090 ----
mean loss: 186.86
train mean loss: 190.48
epoch train time: 0:00:17.080422
elapsed time: 0:56:45.950904
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 01:51:19.992813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.10
 ---- batch: 020 ----
mean loss: 183.07
 ---- batch: 030 ----
mean loss: 193.96
 ---- batch: 040 ----
mean loss: 188.78
 ---- batch: 050 ----
mean loss: 189.44
 ---- batch: 060 ----
mean loss: 192.92
 ---- batch: 070 ----
mean loss: 187.94
 ---- batch: 080 ----
mean loss: 189.63
 ---- batch: 090 ----
mean loss: 189.07
train mean loss: 189.14
epoch train time: 0:00:17.057875
elapsed time: 0:57:03.010083
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 01:51:37.051958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.97
 ---- batch: 020 ----
mean loss: 191.34
 ---- batch: 030 ----
mean loss: 181.68
 ---- batch: 040 ----
mean loss: 189.76
 ---- batch: 050 ----
mean loss: 192.99
 ---- batch: 060 ----
mean loss: 188.55
 ---- batch: 070 ----
mean loss: 193.03
 ---- batch: 080 ----
mean loss: 195.23
 ---- batch: 090 ----
mean loss: 196.88
train mean loss: 190.91
epoch train time: 0:00:17.056787
elapsed time: 0:57:20.068040
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 01:51:54.110063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 200.65
 ---- batch: 020 ----
mean loss: 188.06
 ---- batch: 030 ----
mean loss: 186.20
 ---- batch: 040 ----
mean loss: 193.88
 ---- batch: 050 ----
mean loss: 186.14
 ---- batch: 060 ----
mean loss: 179.90
 ---- batch: 070 ----
mean loss: 187.20
 ---- batch: 080 ----
mean loss: 190.66
 ---- batch: 090 ----
mean loss: 191.17
train mean loss: 189.42
epoch train time: 0:00:17.020359
elapsed time: 0:57:37.090516
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 01:52:11.132083
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.72
 ---- batch: 020 ----
mean loss: 182.41
 ---- batch: 030 ----
mean loss: 187.95
 ---- batch: 040 ----
mean loss: 180.72
 ---- batch: 050 ----
mean loss: 189.79
 ---- batch: 060 ----
mean loss: 186.16
 ---- batch: 070 ----
mean loss: 183.70
 ---- batch: 080 ----
mean loss: 189.38
 ---- batch: 090 ----
mean loss: 185.01
train mean loss: 186.65
epoch train time: 0:00:17.093367
elapsed time: 0:57:54.184928
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 01:52:28.226707
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.58
 ---- batch: 020 ----
mean loss: 190.15
 ---- batch: 030 ----
mean loss: 183.06
 ---- batch: 040 ----
mean loss: 188.01
 ---- batch: 050 ----
mean loss: 190.80
 ---- batch: 060 ----
mean loss: 178.58
 ---- batch: 070 ----
mean loss: 190.31
 ---- batch: 080 ----
mean loss: 187.36
 ---- batch: 090 ----
mean loss: 185.46
train mean loss: 186.61
epoch train time: 0:00:17.002609
elapsed time: 0:58:11.188709
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 01:52:45.230561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.50
 ---- batch: 020 ----
mean loss: 186.27
 ---- batch: 030 ----
mean loss: 193.02
 ---- batch: 040 ----
mean loss: 184.12
 ---- batch: 050 ----
mean loss: 183.19
 ---- batch: 060 ----
mean loss: 185.93
 ---- batch: 070 ----
mean loss: 184.64
 ---- batch: 080 ----
mean loss: 197.43
 ---- batch: 090 ----
mean loss: 181.78
train mean loss: 186.37
epoch train time: 0:00:17.001899
elapsed time: 0:58:28.191756
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 01:53:02.233653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.02
 ---- batch: 020 ----
mean loss: 187.58
 ---- batch: 030 ----
mean loss: 194.83
 ---- batch: 040 ----
mean loss: 176.84
 ---- batch: 050 ----
mean loss: 187.71
 ---- batch: 060 ----
mean loss: 187.81
 ---- batch: 070 ----
mean loss: 182.21
 ---- batch: 080 ----
mean loss: 195.88
 ---- batch: 090 ----
mean loss: 182.97
train mean loss: 186.45
epoch train time: 0:00:17.015700
elapsed time: 0:58:45.208705
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 01:53:19.250684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.18
 ---- batch: 020 ----
mean loss: 182.52
 ---- batch: 030 ----
mean loss: 183.51
 ---- batch: 040 ----
mean loss: 192.29
 ---- batch: 050 ----
mean loss: 184.98
 ---- batch: 060 ----
mean loss: 185.97
 ---- batch: 070 ----
mean loss: 193.78
 ---- batch: 080 ----
mean loss: 186.81
 ---- batch: 090 ----
mean loss: 187.22
train mean loss: 186.66
epoch train time: 0:00:17.016211
elapsed time: 0:59:02.226214
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 01:53:36.268096
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.31
 ---- batch: 020 ----
mean loss: 179.87
 ---- batch: 030 ----
mean loss: 188.19
 ---- batch: 040 ----
mean loss: 189.74
 ---- batch: 050 ----
mean loss: 189.14
 ---- batch: 060 ----
mean loss: 183.73
 ---- batch: 070 ----
mean loss: 186.80
 ---- batch: 080 ----
mean loss: 181.82
 ---- batch: 090 ----
mean loss: 187.95
train mean loss: 186.73
epoch train time: 0:00:17.089865
elapsed time: 0:59:19.317333
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 01:53:53.359209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.04
 ---- batch: 020 ----
mean loss: 187.81
 ---- batch: 030 ----
mean loss: 187.66
 ---- batch: 040 ----
mean loss: 196.23
 ---- batch: 050 ----
mean loss: 186.68
 ---- batch: 060 ----
mean loss: 183.44
 ---- batch: 070 ----
mean loss: 184.66
 ---- batch: 080 ----
mean loss: 190.06
 ---- batch: 090 ----
mean loss: 180.64
train mean loss: 186.74
epoch train time: 0:00:17.061521
elapsed time: 0:59:36.380064
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 01:54:10.421978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.86
 ---- batch: 020 ----
mean loss: 192.56
 ---- batch: 030 ----
mean loss: 188.62
 ---- batch: 040 ----
mean loss: 177.47
 ---- batch: 050 ----
mean loss: 193.37
 ---- batch: 060 ----
mean loss: 194.10
 ---- batch: 070 ----
mean loss: 183.64
 ---- batch: 080 ----
mean loss: 182.96
 ---- batch: 090 ----
mean loss: 183.08
train mean loss: 186.72
epoch train time: 0:00:17.033642
elapsed time: 0:59:53.414917
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 01:54:27.456815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.26
 ---- batch: 020 ----
mean loss: 189.00
 ---- batch: 030 ----
mean loss: 188.42
 ---- batch: 040 ----
mean loss: 185.29
 ---- batch: 050 ----
mean loss: 182.07
 ---- batch: 060 ----
mean loss: 192.18
 ---- batch: 070 ----
mean loss: 185.42
 ---- batch: 080 ----
mean loss: 185.98
 ---- batch: 090 ----
mean loss: 184.37
train mean loss: 186.36
epoch train time: 0:00:17.009414
elapsed time: 1:00:10.425574
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 01:54:44.467517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.02
 ---- batch: 020 ----
mean loss: 181.29
 ---- batch: 030 ----
mean loss: 188.81
 ---- batch: 040 ----
mean loss: 185.48
 ---- batch: 050 ----
mean loss: 184.54
 ---- batch: 060 ----
mean loss: 184.24
 ---- batch: 070 ----
mean loss: 190.23
 ---- batch: 080 ----
mean loss: 186.71
 ---- batch: 090 ----
mean loss: 189.47
train mean loss: 186.42
epoch train time: 0:00:17.089163
elapsed time: 1:00:27.516082
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 01:55:01.557980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.66
 ---- batch: 020 ----
mean loss: 185.26
 ---- batch: 030 ----
mean loss: 185.39
 ---- batch: 040 ----
mean loss: 193.41
 ---- batch: 050 ----
mean loss: 185.29
 ---- batch: 060 ----
mean loss: 179.23
 ---- batch: 070 ----
mean loss: 188.25
 ---- batch: 080 ----
mean loss: 188.68
 ---- batch: 090 ----
mean loss: 185.44
train mean loss: 186.55
epoch train time: 0:00:17.024231
elapsed time: 1:00:44.541514
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 01:55:18.583491
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.80
 ---- batch: 020 ----
mean loss: 189.32
 ---- batch: 030 ----
mean loss: 182.88
 ---- batch: 040 ----
mean loss: 189.83
 ---- batch: 050 ----
mean loss: 186.81
 ---- batch: 060 ----
mean loss: 190.63
 ---- batch: 070 ----
mean loss: 182.52
 ---- batch: 080 ----
mean loss: 185.84
 ---- batch: 090 ----
mean loss: 185.91
train mean loss: 186.20
epoch train time: 0:00:17.066400
elapsed time: 1:01:01.609288
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 01:55:35.651208
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.51
 ---- batch: 020 ----
mean loss: 173.99
 ---- batch: 030 ----
mean loss: 182.75
 ---- batch: 040 ----
mean loss: 188.66
 ---- batch: 050 ----
mean loss: 185.75
 ---- batch: 060 ----
mean loss: 189.70
 ---- batch: 070 ----
mean loss: 193.58
 ---- batch: 080 ----
mean loss: 195.14
 ---- batch: 090 ----
mean loss: 187.84
train mean loss: 186.73
epoch train time: 0:00:17.023626
elapsed time: 1:01:18.634284
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 01:55:52.676239
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.83
 ---- batch: 020 ----
mean loss: 189.05
 ---- batch: 030 ----
mean loss: 183.45
 ---- batch: 040 ----
mean loss: 184.27
 ---- batch: 050 ----
mean loss: 186.10
 ---- batch: 060 ----
mean loss: 188.42
 ---- batch: 070 ----
mean loss: 184.53
 ---- batch: 080 ----
mean loss: 188.60
 ---- batch: 090 ----
mean loss: 184.06
train mean loss: 186.70
epoch train time: 0:00:17.022305
elapsed time: 1:01:35.657907
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 01:56:09.699811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.06
 ---- batch: 020 ----
mean loss: 182.65
 ---- batch: 030 ----
mean loss: 191.36
 ---- batch: 040 ----
mean loss: 186.53
 ---- batch: 050 ----
mean loss: 186.91
 ---- batch: 060 ----
mean loss: 186.97
 ---- batch: 070 ----
mean loss: 183.94
 ---- batch: 080 ----
mean loss: 190.03
 ---- batch: 090 ----
mean loss: 183.92
train mean loss: 186.71
epoch train time: 0:00:17.014079
elapsed time: 1:01:52.673226
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 01:56:26.715140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.08
 ---- batch: 020 ----
mean loss: 184.82
 ---- batch: 030 ----
mean loss: 188.38
 ---- batch: 040 ----
mean loss: 187.31
 ---- batch: 050 ----
mean loss: 193.25
 ---- batch: 060 ----
mean loss: 182.69
 ---- batch: 070 ----
mean loss: 182.37
 ---- batch: 080 ----
mean loss: 190.71
 ---- batch: 090 ----
mean loss: 186.74
train mean loss: 186.69
epoch train time: 0:00:16.988525
elapsed time: 1:02:09.663036
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 01:56:43.704925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.41
 ---- batch: 020 ----
mean loss: 186.81
 ---- batch: 030 ----
mean loss: 189.89
 ---- batch: 040 ----
mean loss: 187.85
 ---- batch: 050 ----
mean loss: 185.02
 ---- batch: 060 ----
mean loss: 188.78
 ---- batch: 070 ----
mean loss: 181.28
 ---- batch: 080 ----
mean loss: 191.18
 ---- batch: 090 ----
mean loss: 188.53
train mean loss: 186.58
epoch train time: 0:00:17.014223
elapsed time: 1:02:26.678483
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 01:57:00.720374
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.03
 ---- batch: 020 ----
mean loss: 186.18
 ---- batch: 030 ----
mean loss: 188.96
 ---- batch: 040 ----
mean loss: 187.26
 ---- batch: 050 ----
mean loss: 191.23
 ---- batch: 060 ----
mean loss: 181.02
 ---- batch: 070 ----
mean loss: 181.20
 ---- batch: 080 ----
mean loss: 192.28
 ---- batch: 090 ----
mean loss: 183.62
train mean loss: 186.07
epoch train time: 0:00:17.040701
elapsed time: 1:02:43.720439
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 01:57:17.762417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.80
 ---- batch: 020 ----
mean loss: 182.45
 ---- batch: 030 ----
mean loss: 185.21
 ---- batch: 040 ----
mean loss: 187.56
 ---- batch: 050 ----
mean loss: 190.89
 ---- batch: 060 ----
mean loss: 183.46
 ---- batch: 070 ----
mean loss: 185.81
 ---- batch: 080 ----
mean loss: 189.60
 ---- batch: 090 ----
mean loss: 185.12
train mean loss: 186.26
epoch train time: 0:00:17.067267
elapsed time: 1:03:00.789051
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 01:57:34.830957
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.08
 ---- batch: 020 ----
mean loss: 184.90
 ---- batch: 030 ----
mean loss: 182.46
 ---- batch: 040 ----
mean loss: 188.04
 ---- batch: 050 ----
mean loss: 185.09
 ---- batch: 060 ----
mean loss: 194.83
 ---- batch: 070 ----
mean loss: 184.82
 ---- batch: 080 ----
mean loss: 191.93
 ---- batch: 090 ----
mean loss: 179.81
train mean loss: 186.26
epoch train time: 0:00:17.033461
elapsed time: 1:03:17.823778
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 01:57:51.865704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.49
 ---- batch: 020 ----
mean loss: 192.12
 ---- batch: 030 ----
mean loss: 184.96
 ---- batch: 040 ----
mean loss: 182.33
 ---- batch: 050 ----
mean loss: 182.02
 ---- batch: 060 ----
mean loss: 187.54
 ---- batch: 070 ----
mean loss: 183.44
 ---- batch: 080 ----
mean loss: 191.86
 ---- batch: 090 ----
mean loss: 188.45
train mean loss: 186.26
epoch train time: 0:00:16.991774
elapsed time: 1:03:34.816912
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 01:58:08.858823
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.35
 ---- batch: 020 ----
mean loss: 185.64
 ---- batch: 030 ----
mean loss: 185.96
 ---- batch: 040 ----
mean loss: 184.29
 ---- batch: 050 ----
mean loss: 188.72
 ---- batch: 060 ----
mean loss: 187.35
 ---- batch: 070 ----
mean loss: 185.74
 ---- batch: 080 ----
mean loss: 179.58
 ---- batch: 090 ----
mean loss: 192.24
train mean loss: 186.19
epoch train time: 0:00:16.987773
elapsed time: 1:03:51.805930
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 01:58:25.847825
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.04
 ---- batch: 020 ----
mean loss: 181.89
 ---- batch: 030 ----
mean loss: 184.17
 ---- batch: 040 ----
mean loss: 184.14
 ---- batch: 050 ----
mean loss: 183.56
 ---- batch: 060 ----
mean loss: 181.16
 ---- batch: 070 ----
mean loss: 187.45
 ---- batch: 080 ----
mean loss: 200.33
 ---- batch: 090 ----
mean loss: 191.11
train mean loss: 186.52
epoch train time: 0:00:16.967501
elapsed time: 1:04:08.774691
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 01:58:42.816621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.03
 ---- batch: 020 ----
mean loss: 189.00
 ---- batch: 030 ----
mean loss: 183.33
 ---- batch: 040 ----
mean loss: 188.67
 ---- batch: 050 ----
mean loss: 185.49
 ---- batch: 060 ----
mean loss: 184.46
 ---- batch: 070 ----
mean loss: 181.81
 ---- batch: 080 ----
mean loss: 183.45
 ---- batch: 090 ----
mean loss: 188.42
train mean loss: 186.13
epoch train time: 0:00:17.107353
elapsed time: 1:04:25.883278
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 01:58:59.925183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.16
 ---- batch: 020 ----
mean loss: 190.95
 ---- batch: 030 ----
mean loss: 187.42
 ---- batch: 040 ----
mean loss: 180.49
 ---- batch: 050 ----
mean loss: 182.26
 ---- batch: 060 ----
mean loss: 187.19
 ---- batch: 070 ----
mean loss: 181.18
 ---- batch: 080 ----
mean loss: 189.31
 ---- batch: 090 ----
mean loss: 193.96
train mean loss: 186.30
epoch train time: 0:00:17.084922
elapsed time: 1:04:42.969422
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 01:59:17.011149
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.46
 ---- batch: 020 ----
mean loss: 188.38
 ---- batch: 030 ----
mean loss: 184.31
 ---- batch: 040 ----
mean loss: 192.49
 ---- batch: 050 ----
mean loss: 194.12
 ---- batch: 060 ----
mean loss: 182.58
 ---- batch: 070 ----
mean loss: 185.45
 ---- batch: 080 ----
mean loss: 185.76
 ---- batch: 090 ----
mean loss: 182.64
train mean loss: 186.21
epoch train time: 0:00:17.097862
elapsed time: 1:05:00.068434
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 01:59:34.110367
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.27
 ---- batch: 020 ----
mean loss: 180.17
 ---- batch: 030 ----
mean loss: 189.71
 ---- batch: 040 ----
mean loss: 189.14
 ---- batch: 050 ----
mean loss: 180.96
 ---- batch: 060 ----
mean loss: 185.66
 ---- batch: 070 ----
mean loss: 189.93
 ---- batch: 080 ----
mean loss: 181.81
 ---- batch: 090 ----
mean loss: 188.31
train mean loss: 186.09
epoch train time: 0:00:17.086606
elapsed time: 1:05:17.156415
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 01:59:51.198451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.26
 ---- batch: 020 ----
mean loss: 184.52
 ---- batch: 030 ----
mean loss: 185.45
 ---- batch: 040 ----
mean loss: 192.73
 ---- batch: 050 ----
mean loss: 190.69
 ---- batch: 060 ----
mean loss: 184.09
 ---- batch: 070 ----
mean loss: 186.50
 ---- batch: 080 ----
mean loss: 180.08
 ---- batch: 090 ----
mean loss: 189.45
train mean loss: 186.32
epoch train time: 0:00:17.069888
elapsed time: 1:05:34.227842
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 02:00:08.269717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.69
 ---- batch: 020 ----
mean loss: 188.44
 ---- batch: 030 ----
mean loss: 189.13
 ---- batch: 040 ----
mean loss: 183.40
 ---- batch: 050 ----
mean loss: 183.57
 ---- batch: 060 ----
mean loss: 184.57
 ---- batch: 070 ----
mean loss: 188.37
 ---- batch: 080 ----
mean loss: 186.65
 ---- batch: 090 ----
mean loss: 185.22
train mean loss: 186.51
epoch train time: 0:00:17.030569
elapsed time: 1:05:51.259720
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 02:00:25.301645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.16
 ---- batch: 020 ----
mean loss: 184.70
 ---- batch: 030 ----
mean loss: 186.48
 ---- batch: 040 ----
mean loss: 188.08
 ---- batch: 050 ----
mean loss: 183.44
 ---- batch: 060 ----
mean loss: 179.80
 ---- batch: 070 ----
mean loss: 184.63
 ---- batch: 080 ----
mean loss: 188.93
 ---- batch: 090 ----
mean loss: 188.43
train mean loss: 186.14
epoch train time: 0:00:17.047344
elapsed time: 1:06:08.308423
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 02:00:42.350335
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.58
 ---- batch: 020 ----
mean loss: 183.78
 ---- batch: 030 ----
mean loss: 187.15
 ---- batch: 040 ----
mean loss: 182.81
 ---- batch: 050 ----
mean loss: 182.36
 ---- batch: 060 ----
mean loss: 189.90
 ---- batch: 070 ----
mean loss: 187.18
 ---- batch: 080 ----
mean loss: 182.49
 ---- batch: 090 ----
mean loss: 189.38
train mean loss: 185.86
epoch train time: 0:00:17.040775
elapsed time: 1:06:25.350568
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 02:00:59.392572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.46
 ---- batch: 020 ----
mean loss: 184.51
 ---- batch: 030 ----
mean loss: 182.60
 ---- batch: 040 ----
mean loss: 181.91
 ---- batch: 050 ----
mean loss: 187.53
 ---- batch: 060 ----
mean loss: 187.02
 ---- batch: 070 ----
mean loss: 181.40
 ---- batch: 080 ----
mean loss: 187.95
 ---- batch: 090 ----
mean loss: 187.34
train mean loss: 186.38
epoch train time: 0:00:17.043976
elapsed time: 1:06:42.396656
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 02:01:16.438047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.11
 ---- batch: 020 ----
mean loss: 188.37
 ---- batch: 030 ----
mean loss: 183.09
 ---- batch: 040 ----
mean loss: 190.03
 ---- batch: 050 ----
mean loss: 187.03
 ---- batch: 060 ----
mean loss: 187.62
 ---- batch: 070 ----
mean loss: 189.29
 ---- batch: 080 ----
mean loss: 185.73
 ---- batch: 090 ----
mean loss: 183.45
train mean loss: 185.95
epoch train time: 0:00:17.035094
elapsed time: 1:06:59.432510
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 02:01:33.474410
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.11
 ---- batch: 020 ----
mean loss: 186.85
 ---- batch: 030 ----
mean loss: 181.34
 ---- batch: 040 ----
mean loss: 182.44
 ---- batch: 050 ----
mean loss: 189.87
 ---- batch: 060 ----
mean loss: 188.71
 ---- batch: 070 ----
mean loss: 185.81
 ---- batch: 080 ----
mean loss: 186.90
 ---- batch: 090 ----
mean loss: 185.64
train mean loss: 185.98
epoch train time: 0:00:17.047214
elapsed time: 1:07:16.481057
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 02:01:50.522958
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.50
 ---- batch: 020 ----
mean loss: 186.01
 ---- batch: 030 ----
mean loss: 187.11
 ---- batch: 040 ----
mean loss: 189.68
 ---- batch: 050 ----
mean loss: 180.47
 ---- batch: 060 ----
mean loss: 180.52
 ---- batch: 070 ----
mean loss: 189.41
 ---- batch: 080 ----
mean loss: 188.36
 ---- batch: 090 ----
mean loss: 188.41
train mean loss: 186.01
epoch train time: 0:00:17.035237
elapsed time: 1:07:33.517684
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 02:02:07.559465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.01
 ---- batch: 020 ----
mean loss: 181.60
 ---- batch: 030 ----
mean loss: 195.71
 ---- batch: 040 ----
mean loss: 185.49
 ---- batch: 050 ----
mean loss: 184.14
 ---- batch: 060 ----
mean loss: 179.53
 ---- batch: 070 ----
mean loss: 186.81
 ---- batch: 080 ----
mean loss: 192.11
 ---- batch: 090 ----
mean loss: 185.95
train mean loss: 186.30
epoch train time: 0:00:17.075984
elapsed time: 1:07:50.594920
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 02:02:24.636811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.04
 ---- batch: 020 ----
mean loss: 186.72
 ---- batch: 030 ----
mean loss: 187.81
 ---- batch: 040 ----
mean loss: 183.75
 ---- batch: 050 ----
mean loss: 188.48
 ---- batch: 060 ----
mean loss: 181.92
 ---- batch: 070 ----
mean loss: 184.95
 ---- batch: 080 ----
mean loss: 183.52
 ---- batch: 090 ----
mean loss: 185.32
train mean loss: 186.10
epoch train time: 0:00:17.064593
elapsed time: 1:08:07.660784
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 02:02:41.702662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.30
 ---- batch: 020 ----
mean loss: 187.71
 ---- batch: 030 ----
mean loss: 185.71
 ---- batch: 040 ----
mean loss: 178.68
 ---- batch: 050 ----
mean loss: 187.69
 ---- batch: 060 ----
mean loss: 186.75
 ---- batch: 070 ----
mean loss: 182.61
 ---- batch: 080 ----
mean loss: 183.97
 ---- batch: 090 ----
mean loss: 189.83
train mean loss: 185.85
epoch train time: 0:00:17.198192
elapsed time: 1:08:24.860268
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 02:02:58.902259
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.38
 ---- batch: 020 ----
mean loss: 189.10
 ---- batch: 030 ----
mean loss: 187.35
 ---- batch: 040 ----
mean loss: 178.35
 ---- batch: 050 ----
mean loss: 185.41
 ---- batch: 060 ----
mean loss: 183.81
 ---- batch: 070 ----
mean loss: 189.54
 ---- batch: 080 ----
mean loss: 186.97
 ---- batch: 090 ----
mean loss: 182.42
train mean loss: 186.07
epoch train time: 0:00:17.092406
elapsed time: 1:08:41.954004
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 02:03:15.995883
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.13
 ---- batch: 020 ----
mean loss: 182.38
 ---- batch: 030 ----
mean loss: 188.04
 ---- batch: 040 ----
mean loss: 182.99
 ---- batch: 050 ----
mean loss: 185.22
 ---- batch: 060 ----
mean loss: 179.82
 ---- batch: 070 ----
mean loss: 187.63
 ---- batch: 080 ----
mean loss: 183.54
 ---- batch: 090 ----
mean loss: 190.62
train mean loss: 186.07
epoch train time: 0:00:17.072610
elapsed time: 1:08:59.027800
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 02:03:33.069724
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.65
 ---- batch: 020 ----
mean loss: 182.20
 ---- batch: 030 ----
mean loss: 189.05
 ---- batch: 040 ----
mean loss: 186.92
 ---- batch: 050 ----
mean loss: 181.55
 ---- batch: 060 ----
mean loss: 184.26
 ---- batch: 070 ----
mean loss: 182.36
 ---- batch: 080 ----
mean loss: 184.71
 ---- batch: 090 ----
mean loss: 191.85
train mean loss: 185.83
epoch train time: 0:00:17.095070
elapsed time: 1:09:16.124118
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 02:03:50.166019
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.73
 ---- batch: 020 ----
mean loss: 186.18
 ---- batch: 030 ----
mean loss: 186.56
 ---- batch: 040 ----
mean loss: 183.11
 ---- batch: 050 ----
mean loss: 182.76
 ---- batch: 060 ----
mean loss: 189.49
 ---- batch: 070 ----
mean loss: 191.62
 ---- batch: 080 ----
mean loss: 179.46
 ---- batch: 090 ----
mean loss: 190.63
train mean loss: 186.25
epoch train time: 0:00:17.133941
elapsed time: 1:09:33.259252
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 02:04:07.301142
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.80
 ---- batch: 020 ----
mean loss: 194.77
 ---- batch: 030 ----
mean loss: 184.94
 ---- batch: 040 ----
mean loss: 178.08
 ---- batch: 050 ----
mean loss: 186.84
 ---- batch: 060 ----
mean loss: 185.76
 ---- batch: 070 ----
mean loss: 186.89
 ---- batch: 080 ----
mean loss: 181.19
 ---- batch: 090 ----
mean loss: 187.93
train mean loss: 186.01
epoch train time: 0:00:17.037391
elapsed time: 1:09:50.297862
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 02:04:24.339734
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.65
 ---- batch: 020 ----
mean loss: 186.76
 ---- batch: 030 ----
mean loss: 182.64
 ---- batch: 040 ----
mean loss: 185.15
 ---- batch: 050 ----
mean loss: 191.21
 ---- batch: 060 ----
mean loss: 188.10
 ---- batch: 070 ----
mean loss: 183.08
 ---- batch: 080 ----
mean loss: 188.61
 ---- batch: 090 ----
mean loss: 190.00
train mean loss: 185.39
epoch train time: 0:00:17.033423
elapsed time: 1:10:07.332563
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 02:04:41.374493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.86
 ---- batch: 020 ----
mean loss: 186.24
 ---- batch: 030 ----
mean loss: 182.18
 ---- batch: 040 ----
mean loss: 190.79
 ---- batch: 050 ----
mean loss: 184.93
 ---- batch: 060 ----
mean loss: 181.68
 ---- batch: 070 ----
mean loss: 184.40
 ---- batch: 080 ----
mean loss: 180.83
 ---- batch: 090 ----
mean loss: 187.75
train mean loss: 185.99
epoch train time: 0:00:17.025543
elapsed time: 1:10:24.359429
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 02:04:58.401333
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.08
 ---- batch: 020 ----
mean loss: 184.79
 ---- batch: 030 ----
mean loss: 183.77
 ---- batch: 040 ----
mean loss: 190.94
 ---- batch: 050 ----
mean loss: 188.99
 ---- batch: 060 ----
mean loss: 188.72
 ---- batch: 070 ----
mean loss: 179.68
 ---- batch: 080 ----
mean loss: 179.15
 ---- batch: 090 ----
mean loss: 189.62
train mean loss: 185.61
epoch train time: 0:00:17.061891
elapsed time: 1:10:41.422534
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 02:05:15.464397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.11
 ---- batch: 020 ----
mean loss: 185.30
 ---- batch: 030 ----
mean loss: 184.13
 ---- batch: 040 ----
mean loss: 189.29
 ---- batch: 050 ----
mean loss: 181.51
 ---- batch: 060 ----
mean loss: 189.17
 ---- batch: 070 ----
mean loss: 197.99
 ---- batch: 080 ----
mean loss: 180.70
 ---- batch: 090 ----
mean loss: 182.72
train mean loss: 185.90
epoch train time: 0:00:17.069241
elapsed time: 1:10:58.492936
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 02:05:32.534883
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.76
 ---- batch: 020 ----
mean loss: 186.96
 ---- batch: 030 ----
mean loss: 185.56
 ---- batch: 040 ----
mean loss: 177.57
 ---- batch: 050 ----
mean loss: 191.28
 ---- batch: 060 ----
mean loss: 188.60
 ---- batch: 070 ----
mean loss: 183.94
 ---- batch: 080 ----
mean loss: 186.82
 ---- batch: 090 ----
mean loss: 189.19
train mean loss: 185.50
epoch train time: 0:00:17.035814
elapsed time: 1:11:15.540298
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_5/checkpoint.pth.tar
**** end time: 2019-09-25 02:05:49.581660 ****
