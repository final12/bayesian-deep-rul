Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 22944
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:32:51.485095 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:32:51.494650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4660.86
 ---- batch: 020 ----
mean loss: 4257.60
train mean loss: 4431.20
epoch train time: 0:00:07.618312
elapsed time: 0:00:07.633986
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:32:59.119119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3889.06
 ---- batch: 020 ----
mean loss: 3605.53
train mean loss: 3722.12
epoch train time: 0:00:00.351033
elapsed time: 0:00:07.985187
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:32:59.470368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3306.88
 ---- batch: 020 ----
mean loss: 3240.18
train mean loss: 3267.95
epoch train time: 0:00:00.362600
elapsed time: 0:00:08.347999
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:32:59.833165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3040.43
 ---- batch: 020 ----
mean loss: 2895.17
train mean loss: 2946.89
epoch train time: 0:00:00.350316
elapsed time: 0:00:08.698536
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:33:00.183725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2786.96
 ---- batch: 020 ----
mean loss: 2691.86
train mean loss: 2743.59
epoch train time: 0:00:00.346714
elapsed time: 0:00:09.045481
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:33:00.530639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2637.10
 ---- batch: 020 ----
mean loss: 2557.24
train mean loss: 2588.90
epoch train time: 0:00:00.359908
elapsed time: 0:00:09.405568
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:33:00.890723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2522.72
 ---- batch: 020 ----
mean loss: 2459.65
train mean loss: 2486.13
epoch train time: 0:00:00.346824
elapsed time: 0:00:09.752567
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:33:01.237725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2433.55
 ---- batch: 020 ----
mean loss: 2357.46
train mean loss: 2391.34
epoch train time: 0:00:00.347202
elapsed time: 0:00:10.099950
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:33:01.585142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2352.33
 ---- batch: 020 ----
mean loss: 2290.91
train mean loss: 2312.58
epoch train time: 0:00:00.358184
elapsed time: 0:00:10.458345
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:33:01.943503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2273.30
 ---- batch: 020 ----
mean loss: 2233.53
train mean loss: 2242.19
epoch train time: 0:00:00.345856
elapsed time: 0:00:10.804375
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:33:02.289531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2170.70
 ---- batch: 020 ----
mean loss: 2183.10
train mean loss: 2180.89
epoch train time: 0:00:00.341782
elapsed time: 0:00:11.146342
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:33:02.631513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2133.36
 ---- batch: 020 ----
mean loss: 2075.02
train mean loss: 2109.14
epoch train time: 0:00:00.353236
elapsed time: 0:00:11.499769
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:33:02.984960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2048.12
 ---- batch: 020 ----
mean loss: 2053.37
train mean loss: 2047.09
epoch train time: 0:00:00.349271
elapsed time: 0:00:11.849269
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:33:03.334422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1997.41
 ---- batch: 020 ----
mean loss: 1970.12
train mean loss: 1983.01
epoch train time: 0:00:00.345242
elapsed time: 0:00:12.194706
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:33:03.679863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1948.52
 ---- batch: 020 ----
mean loss: 1891.85
train mean loss: 1926.55
epoch train time: 0:00:00.360383
elapsed time: 0:00:12.555262
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:33:04.040418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1902.53
 ---- batch: 020 ----
mean loss: 1840.12
train mean loss: 1866.75
epoch train time: 0:00:00.332890
elapsed time: 0:00:12.888367
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:33:04.373523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1835.38
 ---- batch: 020 ----
mean loss: 1808.93
train mean loss: 1816.16
epoch train time: 0:00:00.343969
elapsed time: 0:00:13.232522
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:33:04.717700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1787.54
 ---- batch: 020 ----
mean loss: 1738.43
train mean loss: 1766.94
epoch train time: 0:00:00.362382
elapsed time: 0:00:13.595100
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:33:05.080266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1713.08
 ---- batch: 020 ----
mean loss: 1721.51
train mean loss: 1716.27
epoch train time: 0:00:00.347499
elapsed time: 0:00:13.942787
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:33:05.427942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1664.65
 ---- batch: 020 ----
mean loss: 1686.70
train mean loss: 1667.71
epoch train time: 0:00:00.345693
elapsed time: 0:00:14.288654
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:33:05.773839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1637.18
 ---- batch: 020 ----
mean loss: 1603.34
train mean loss: 1614.75
epoch train time: 0:00:00.338383
elapsed time: 0:00:14.627296
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:33:06.112454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1594.88
 ---- batch: 020 ----
mean loss: 1563.71
train mean loss: 1572.60
epoch train time: 0:00:00.335171
elapsed time: 0:00:14.962676
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:33:06.447884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1555.24
 ---- batch: 020 ----
mean loss: 1526.38
train mean loss: 1527.01
epoch train time: 0:00:00.351976
elapsed time: 0:00:15.314883
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:33:06.800039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1496.30
 ---- batch: 020 ----
mean loss: 1489.00
train mean loss: 1487.52
epoch train time: 0:00:00.347335
elapsed time: 0:00:15.662418
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:33:07.147573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1441.97
 ---- batch: 020 ----
mean loss: 1419.00
train mean loss: 1437.69
epoch train time: 0:00:00.339034
elapsed time: 0:00:16.001646
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:33:07.486833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1395.79
 ---- batch: 020 ----
mean loss: 1373.06
train mean loss: 1374.71
epoch train time: 0:00:00.351312
elapsed time: 0:00:16.353221
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:33:07.838418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1354.16
 ---- batch: 020 ----
mean loss: 1329.07
train mean loss: 1331.34
epoch train time: 0:00:00.339054
elapsed time: 0:00:16.692508
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:33:08.177663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1307.22
 ---- batch: 020 ----
mean loss: 1265.74
train mean loss: 1286.77
epoch train time: 0:00:00.340114
elapsed time: 0:00:17.032795
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:33:08.517960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1266.86
 ---- batch: 020 ----
mean loss: 1230.70
train mean loss: 1244.39
epoch train time: 0:00:00.344867
elapsed time: 0:00:17.377869
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:33:08.863043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1219.74
 ---- batch: 020 ----
mean loss: 1199.31
train mean loss: 1210.60
epoch train time: 0:00:00.342034
elapsed time: 0:00:17.720130
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:33:09.205287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1166.21
 ---- batch: 020 ----
mean loss: 1164.69
train mean loss: 1168.61
epoch train time: 0:00:00.347364
elapsed time: 0:00:18.067743
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:33:09.552920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1135.15
 ---- batch: 020 ----
mean loss: 1134.01
train mean loss: 1134.13
epoch train time: 0:00:00.349747
elapsed time: 0:00:18.417739
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:33:09.902907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1110.83
 ---- batch: 020 ----
mean loss: 1093.35
train mean loss: 1099.90
epoch train time: 0:00:00.346308
elapsed time: 0:00:18.764645
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:33:10.249841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1067.10
 ---- batch: 020 ----
mean loss: 1068.73
train mean loss: 1066.11
epoch train time: 0:00:00.340752
elapsed time: 0:00:19.105613
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:33:10.590774
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1034.89
 ---- batch: 020 ----
mean loss: 1028.21
train mean loss: 1032.09
epoch train time: 0:00:00.345566
elapsed time: 0:00:19.451366
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:33:10.936521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1010.27
 ---- batch: 020 ----
mean loss: 995.79
train mean loss: 1000.61
epoch train time: 0:00:00.349637
elapsed time: 0:00:19.801193
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:33:11.286345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.80
 ---- batch: 020 ----
mean loss: 950.83
train mean loss: 966.45
epoch train time: 0:00:00.345791
elapsed time: 0:00:20.147161
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:33:11.632313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 953.41
 ---- batch: 020 ----
mean loss: 922.39
train mean loss: 934.79
epoch train time: 0:00:00.355944
elapsed time: 0:00:20.503328
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:33:11.988489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 918.49
 ---- batch: 020 ----
mean loss: 890.67
train mean loss: 908.22
epoch train time: 0:00:00.346097
elapsed time: 0:00:20.849612
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:33:12.334776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.20
 ---- batch: 020 ----
mean loss: 879.35
train mean loss: 881.70
epoch train time: 0:00:00.345537
elapsed time: 0:00:21.195356
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:33:12.680515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.63
 ---- batch: 020 ----
mean loss: 839.56
train mean loss: 849.24
epoch train time: 0:00:00.365316
elapsed time: 0:00:21.560855
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:33:13.046010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.85
 ---- batch: 020 ----
mean loss: 826.78
train mean loss: 821.04
epoch train time: 0:00:00.347495
elapsed time: 0:00:21.908537
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:33:13.393709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 815.21
 ---- batch: 020 ----
mean loss: 775.92
train mean loss: 791.22
epoch train time: 0:00:00.346471
elapsed time: 0:00:22.255255
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:33:13.740415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.21
 ---- batch: 020 ----
mean loss: 762.42
train mean loss: 770.70
epoch train time: 0:00:00.341949
elapsed time: 0:00:22.597388
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:33:14.082546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 752.38
 ---- batch: 020 ----
mean loss: 743.12
train mean loss: 748.99
epoch train time: 0:00:00.337435
elapsed time: 0:00:22.935010
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:33:14.420165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 741.59
 ---- batch: 020 ----
mean loss: 712.01
train mean loss: 722.58
epoch train time: 0:00:00.346890
elapsed time: 0:00:23.282079
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:33:14.767237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.31
 ---- batch: 020 ----
mean loss: 693.05
train mean loss: 698.46
epoch train time: 0:00:00.360553
elapsed time: 0:00:23.642811
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:33:15.127966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.53
 ---- batch: 020 ----
mean loss: 668.84
train mean loss: 673.76
epoch train time: 0:00:00.348633
elapsed time: 0:00:23.991631
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:33:15.476803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 647.20
 ---- batch: 020 ----
mean loss: 659.32
train mean loss: 653.48
epoch train time: 0:00:00.348245
elapsed time: 0:00:24.340071
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:33:15.825228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.98
 ---- batch: 020 ----
mean loss: 630.57
train mean loss: 630.52
epoch train time: 0:00:00.338697
elapsed time: 0:00:24.678973
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:33:16.164126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.66
 ---- batch: 020 ----
mean loss: 608.21
train mean loss: 616.60
epoch train time: 0:00:00.336129
elapsed time: 0:00:25.015268
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:33:16.500421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 608.09
 ---- batch: 020 ----
mean loss: 580.26
train mean loss: 595.93
epoch train time: 0:00:00.356128
elapsed time: 0:00:25.371586
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:33:16.856760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 588.38
 ---- batch: 020 ----
mean loss: 569.63
train mean loss: 574.25
epoch train time: 0:00:00.348315
elapsed time: 0:00:25.720094
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:33:17.205261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 558.78
 ---- batch: 020 ----
mean loss: 555.50
train mean loss: 556.62
epoch train time: 0:00:00.344739
elapsed time: 0:00:26.065029
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:33:17.550223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.36
 ---- batch: 020 ----
mean loss: 540.73
train mean loss: 539.83
epoch train time: 0:00:00.344961
elapsed time: 0:00:26.410202
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:33:17.895358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 537.95
 ---- batch: 020 ----
mean loss: 514.52
train mean loss: 526.55
epoch train time: 0:00:00.333956
elapsed time: 0:00:26.744342
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:33:18.229494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.66
 ---- batch: 020 ----
mean loss: 501.67
train mean loss: 504.50
epoch train time: 0:00:00.334769
elapsed time: 0:00:27.079323
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:33:18.564493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.77
 ---- batch: 020 ----
mean loss: 491.57
train mean loss: 491.47
epoch train time: 0:00:00.344463
elapsed time: 0:00:27.423977
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:33:18.909133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.23
 ---- batch: 020 ----
mean loss: 472.43
train mean loss: 474.75
epoch train time: 0:00:00.344617
elapsed time: 0:00:27.768833
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:33:19.253989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.27
 ---- batch: 020 ----
mean loss: 457.43
train mean loss: 457.98
epoch train time: 0:00:00.344195
elapsed time: 0:00:28.113198
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:33:19.598351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.38
 ---- batch: 020 ----
mean loss: 442.18
train mean loss: 442.81
epoch train time: 0:00:00.344838
elapsed time: 0:00:28.458220
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:33:19.943369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.12
 ---- batch: 020 ----
mean loss: 434.24
train mean loss: 434.08
epoch train time: 0:00:00.333804
elapsed time: 0:00:28.792187
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:33:20.277342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.51
 ---- batch: 020 ----
mean loss: 412.36
train mean loss: 418.02
epoch train time: 0:00:00.334082
elapsed time: 0:00:29.126476
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:33:20.611630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.89
 ---- batch: 020 ----
mean loss: 399.77
train mean loss: 401.75
epoch train time: 0:00:00.349894
elapsed time: 0:00:29.476542
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:33:20.961696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.35
 ---- batch: 020 ----
mean loss: 389.66
train mean loss: 392.41
epoch train time: 0:00:00.339545
elapsed time: 0:00:29.816267
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:33:21.301457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.11
 ---- batch: 020 ----
mean loss: 377.42
train mean loss: 381.70
epoch train time: 0:00:00.334516
elapsed time: 0:00:30.150991
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:33:21.636148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.37
 ---- batch: 020 ----
mean loss: 366.54
train mean loss: 367.12
epoch train time: 0:00:00.341242
elapsed time: 0:00:30.492416
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:33:21.977570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.13
 ---- batch: 020 ----
mean loss: 352.56
train mean loss: 358.72
epoch train time: 0:00:00.337575
elapsed time: 0:00:30.830179
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:33:22.315333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.35
 ---- batch: 020 ----
mean loss: 347.74
train mean loss: 347.35
epoch train time: 0:00:00.331193
elapsed time: 0:00:31.161541
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:33:22.646693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.93
 ---- batch: 020 ----
mean loss: 332.48
train mean loss: 334.79
epoch train time: 0:00:00.343126
elapsed time: 0:00:31.504848
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:33:22.990004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.71
 ---- batch: 020 ----
mean loss: 325.40
train mean loss: 328.35
epoch train time: 0:00:00.336730
elapsed time: 0:00:31.841753
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:33:23.326917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.99
 ---- batch: 020 ----
mean loss: 317.19
train mean loss: 318.60
epoch train time: 0:00:00.333684
elapsed time: 0:00:32.175623
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:33:23.660777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.33
 ---- batch: 020 ----
mean loss: 302.87
train mean loss: 307.13
epoch train time: 0:00:00.364569
elapsed time: 0:00:32.540369
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:33:24.025541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.60
 ---- batch: 020 ----
mean loss: 298.12
train mean loss: 297.49
epoch train time: 0:00:00.343782
elapsed time: 0:00:32.884406
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:33:24.369565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.08
 ---- batch: 020 ----
mean loss: 287.16
train mean loss: 290.51
epoch train time: 0:00:00.344906
elapsed time: 0:00:33.229525
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:33:24.714681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.05
 ---- batch: 020 ----
mean loss: 276.08
train mean loss: 280.18
epoch train time: 0:00:00.354078
elapsed time: 0:00:33.583780
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:33:25.068937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.97
 ---- batch: 020 ----
mean loss: 271.77
train mean loss: 272.10
epoch train time: 0:00:00.345435
elapsed time: 0:00:33.929451
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:33:25.414609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.41
 ---- batch: 020 ----
mean loss: 268.15
train mean loss: 266.54
epoch train time: 0:00:00.359803
elapsed time: 0:00:34.289473
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:33:25.774632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.74
 ---- batch: 020 ----
mean loss: 256.02
train mean loss: 256.26
epoch train time: 0:00:00.349461
elapsed time: 0:00:34.639133
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:33:26.124305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.52
 ---- batch: 020 ----
mean loss: 249.60
train mean loss: 250.46
epoch train time: 0:00:00.343322
elapsed time: 0:00:34.982655
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:33:26.467813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.24
 ---- batch: 020 ----
mean loss: 244.43
train mean loss: 242.12
epoch train time: 0:00:00.357127
elapsed time: 0:00:35.340003
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:33:26.825169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.91
 ---- batch: 020 ----
mean loss: 236.35
train mean loss: 236.64
epoch train time: 0:00:00.347764
elapsed time: 0:00:35.687955
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:33:27.173113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.96
 ---- batch: 020 ----
mean loss: 231.79
train mean loss: 232.20
epoch train time: 0:00:00.344216
elapsed time: 0:00:36.032347
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:33:27.517502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.29
 ---- batch: 020 ----
mean loss: 220.60
train mean loss: 224.05
epoch train time: 0:00:00.349784
elapsed time: 0:00:36.382318
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:33:27.867502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.45
 ---- batch: 020 ----
mean loss: 212.99
train mean loss: 216.09
epoch train time: 0:00:00.341807
elapsed time: 0:00:36.724330
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:33:28.209487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.70
 ---- batch: 020 ----
mean loss: 211.99
train mean loss: 211.35
epoch train time: 0:00:00.342518
elapsed time: 0:00:37.067021
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:33:28.552175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.63
 ---- batch: 020 ----
mean loss: 210.13
train mean loss: 207.21
epoch train time: 0:00:00.373714
elapsed time: 0:00:37.440905
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:33:28.926076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.80
 ---- batch: 020 ----
mean loss: 198.60
train mean loss: 200.16
epoch train time: 0:00:00.343945
elapsed time: 0:00:37.785039
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:33:29.270208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.35
 ---- batch: 020 ----
mean loss: 194.94
train mean loss: 196.18
epoch train time: 0:00:00.349268
elapsed time: 0:00:38.134502
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:33:29.619659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.83
 ---- batch: 020 ----
mean loss: 188.54
train mean loss: 189.71
epoch train time: 0:00:00.350115
elapsed time: 0:00:38.484795
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:33:29.969950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.59
 ---- batch: 020 ----
mean loss: 185.46
train mean loss: 185.71
epoch train time: 0:00:00.332284
elapsed time: 0:00:38.817264
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:33:30.302421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.67
 ---- batch: 020 ----
mean loss: 180.74
train mean loss: 180.78
epoch train time: 0:00:00.336617
elapsed time: 0:00:39.154091
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:33:30.639262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.65
 ---- batch: 020 ----
mean loss: 177.38
train mean loss: 175.61
epoch train time: 0:00:00.353937
elapsed time: 0:00:39.508234
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:33:30.993392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.18
 ---- batch: 020 ----
mean loss: 170.13
train mean loss: 173.99
epoch train time: 0:00:00.341642
elapsed time: 0:00:39.850052
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:33:31.335208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.61
 ---- batch: 020 ----
mean loss: 165.86
train mean loss: 167.85
epoch train time: 0:00:00.344713
elapsed time: 0:00:40.194942
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:33:31.680099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.05
 ---- batch: 020 ----
mean loss: 163.26
train mean loss: 164.27
epoch train time: 0:00:00.354479
elapsed time: 0:00:40.549612
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:33:32.034768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.93
 ---- batch: 020 ----
mean loss: 160.66
train mean loss: 162.17
epoch train time: 0:00:00.340182
elapsed time: 0:00:40.889968
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:33:32.375125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.02
 ---- batch: 020 ----
mean loss: 160.56
train mean loss: 157.44
epoch train time: 0:00:00.347594
elapsed time: 0:00:41.237743
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:33:32.722898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.96
 ---- batch: 020 ----
mean loss: 153.60
train mean loss: 155.36
epoch train time: 0:00:00.349153
elapsed time: 0:00:41.587072
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:33:33.072261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.55
 ---- batch: 020 ----
mean loss: 148.89
train mean loss: 150.77
epoch train time: 0:00:00.345052
elapsed time: 0:00:41.932352
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:33:33.417549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.10
 ---- batch: 020 ----
mean loss: 146.54
train mean loss: 146.28
epoch train time: 0:00:00.348877
elapsed time: 0:00:42.281490
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:33:33.766658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.84
 ---- batch: 020 ----
mean loss: 140.44
train mean loss: 143.39
epoch train time: 0:00:00.342955
elapsed time: 0:00:42.624629
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:33:34.109801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.77
 ---- batch: 020 ----
mean loss: 142.44
train mean loss: 141.19
epoch train time: 0:00:00.332219
elapsed time: 0:00:42.957037
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:33:34.442191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.52
 ---- batch: 020 ----
mean loss: 137.31
train mean loss: 136.67
epoch train time: 0:00:00.364919
elapsed time: 0:00:43.322151
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:33:34.807321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.55
 ---- batch: 020 ----
mean loss: 137.84
train mean loss: 135.41
epoch train time: 0:00:00.347100
elapsed time: 0:00:43.669443
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:33:35.154600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.30
 ---- batch: 020 ----
mean loss: 133.60
train mean loss: 131.28
epoch train time: 0:00:00.342801
elapsed time: 0:00:44.012420
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:33:35.497577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.82
 ---- batch: 020 ----
mean loss: 129.93
train mean loss: 129.60
epoch train time: 0:00:00.351350
elapsed time: 0:00:44.363968
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:33:35.849106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.06
 ---- batch: 020 ----
mean loss: 128.44
train mean loss: 126.76
epoch train time: 0:00:00.345246
elapsed time: 0:00:44.709379
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:33:36.194534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.63
 ---- batch: 020 ----
mean loss: 122.70
train mean loss: 123.49
epoch train time: 0:00:00.340430
elapsed time: 0:00:45.049977
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:33:36.535129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.16
 ---- batch: 020 ----
mean loss: 124.86
train mean loss: 123.27
epoch train time: 0:00:00.341933
elapsed time: 0:00:45.392081
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:33:36.877233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.56
 ---- batch: 020 ----
mean loss: 120.21
train mean loss: 119.37
epoch train time: 0:00:00.341777
elapsed time: 0:00:45.734073
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:33:37.219226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.60
 ---- batch: 020 ----
mean loss: 114.63
train mean loss: 118.50
epoch train time: 0:00:00.342947
elapsed time: 0:00:46.077236
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:33:37.562396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.04
 ---- batch: 020 ----
mean loss: 117.54
train mean loss: 116.92
epoch train time: 0:00:00.349259
elapsed time: 0:00:46.426676
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:33:37.911832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.36
 ---- batch: 020 ----
mean loss: 117.79
train mean loss: 115.04
epoch train time: 0:00:00.347789
elapsed time: 0:00:46.774650
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:33:38.259807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.86
 ---- batch: 020 ----
mean loss: 110.92
train mean loss: 111.77
epoch train time: 0:00:00.348012
elapsed time: 0:00:47.122886
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:33:38.608039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.33
 ---- batch: 020 ----
mean loss: 113.86
train mean loss: 111.84
epoch train time: 0:00:00.359963
elapsed time: 0:00:47.483026
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:33:38.968183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.69
 ---- batch: 020 ----
mean loss: 107.35
train mean loss: 110.38
epoch train time: 0:00:00.342960
elapsed time: 0:00:47.826191
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:33:39.311360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.52
 ---- batch: 020 ----
mean loss: 108.33
train mean loss: 107.11
epoch train time: 0:00:00.342169
elapsed time: 0:00:48.168552
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:33:39.653719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.34
 ---- batch: 020 ----
mean loss: 106.56
train mean loss: 105.70
epoch train time: 0:00:00.348104
elapsed time: 0:00:48.516849
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:33:40.002016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.60
 ---- batch: 020 ----
mean loss: 106.95
train mean loss: 105.01
epoch train time: 0:00:00.337107
elapsed time: 0:00:48.854158
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:33:40.339344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.68
 ---- batch: 020 ----
mean loss: 102.66
train mean loss: 102.63
epoch train time: 0:00:00.337770
elapsed time: 0:00:49.192797
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:33:40.677962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.46
 ---- batch: 020 ----
mean loss: 101.46
train mean loss: 101.03
epoch train time: 0:00:00.348137
elapsed time: 0:00:49.541121
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:33:41.026291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.98
 ---- batch: 020 ----
mean loss: 102.06
train mean loss: 100.39
epoch train time: 0:00:00.346729
elapsed time: 0:00:49.888084
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:33:41.373255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.56
 ---- batch: 020 ----
mean loss: 96.17
train mean loss: 98.18
epoch train time: 0:00:00.348054
elapsed time: 0:00:50.236327
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:33:41.721480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.05
 ---- batch: 020 ----
mean loss: 98.46
train mean loss: 97.34
epoch train time: 0:00:00.350947
elapsed time: 0:00:50.587454
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:33:42.072608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.70
 ---- batch: 020 ----
mean loss: 95.39
train mean loss: 96.53
epoch train time: 0:00:00.343601
elapsed time: 0:00:50.931236
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:33:42.416393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.02
 ---- batch: 020 ----
mean loss: 94.81
train mean loss: 94.23
epoch train time: 0:00:00.346806
elapsed time: 0:00:51.278262
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:33:42.763397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.36
 ---- batch: 020 ----
mean loss: 91.95
train mean loss: 93.05
epoch train time: 0:00:00.340491
elapsed time: 0:00:51.618908
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:33:43.104063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.11
 ---- batch: 020 ----
mean loss: 91.74
train mean loss: 92.45
epoch train time: 0:00:00.334819
elapsed time: 0:00:51.953948
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:33:43.439104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.69
 ---- batch: 020 ----
mean loss: 92.96
train mean loss: 91.12
epoch train time: 0:00:00.346202
elapsed time: 0:00:52.300326
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:33:43.785481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.62
 ---- batch: 020 ----
mean loss: 91.72
train mean loss: 90.57
epoch train time: 0:00:00.341003
elapsed time: 0:00:52.641495
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:33:44.126648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.89
 ---- batch: 020 ----
mean loss: 89.59
train mean loss: 89.70
epoch train time: 0:00:00.333400
elapsed time: 0:00:52.975066
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:33:44.460221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.68
 ---- batch: 020 ----
mean loss: 89.20
train mean loss: 86.22
epoch train time: 0:00:00.334684
elapsed time: 0:00:53.309932
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:33:44.795091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.59
 ---- batch: 020 ----
mean loss: 88.21
train mean loss: 87.72
epoch train time: 0:00:00.353911
elapsed time: 0:00:53.664044
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:33:45.149196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.91
 ---- batch: 020 ----
mean loss: 84.87
train mean loss: 86.20
epoch train time: 0:00:00.343268
elapsed time: 0:00:54.007503
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:33:45.492674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.38
 ---- batch: 020 ----
mean loss: 89.70
train mean loss: 85.93
epoch train time: 0:00:00.344719
elapsed time: 0:00:54.352431
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:33:45.837602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.36
 ---- batch: 020 ----
mean loss: 84.03
train mean loss: 83.97
epoch train time: 0:00:00.350217
elapsed time: 0:00:54.702839
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:33:46.187990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.26
 ---- batch: 020 ----
mean loss: 86.08
train mean loss: 84.34
epoch train time: 0:00:00.349797
elapsed time: 0:00:55.052809
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:33:46.537966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.26
 ---- batch: 020 ----
mean loss: 82.83
train mean loss: 82.61
epoch train time: 0:00:00.367900
elapsed time: 0:00:55.420901
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:33:46.906071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.11
 ---- batch: 020 ----
mean loss: 81.52
train mean loss: 81.50
epoch train time: 0:00:00.344611
elapsed time: 0:00:55.765697
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:33:47.250850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.24
 ---- batch: 020 ----
mean loss: 84.22
train mean loss: 80.93
epoch train time: 0:00:00.343848
elapsed time: 0:00:56.109715
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:33:47.594869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.51
 ---- batch: 020 ----
mean loss: 79.11
train mean loss: 82.00
epoch train time: 0:00:00.360919
elapsed time: 0:00:56.470831
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:33:47.955996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.27
 ---- batch: 020 ----
mean loss: 79.11
train mean loss: 81.03
epoch train time: 0:00:00.351951
elapsed time: 0:00:56.822967
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:33:48.308124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.13
 ---- batch: 020 ----
mean loss: 77.70
train mean loss: 78.85
epoch train time: 0:00:00.350361
elapsed time: 0:00:57.173526
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:33:48.658682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.08
 ---- batch: 020 ----
mean loss: 81.50
train mean loss: 80.64
epoch train time: 0:00:00.358764
elapsed time: 0:00:57.532479
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:33:49.017648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.06
 ---- batch: 020 ----
mean loss: 77.26
train mean loss: 77.86
epoch train time: 0:00:00.343438
elapsed time: 0:00:57.876123
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:33:49.361292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.46
 ---- batch: 020 ----
mean loss: 77.13
train mean loss: 77.75
epoch train time: 0:00:00.342471
elapsed time: 0:00:58.218788
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:33:49.703947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.08
 ---- batch: 020 ----
mean loss: 74.33
train mean loss: 76.64
epoch train time: 0:00:00.351051
elapsed time: 0:00:58.570019
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:33:50.055178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.92
 ---- batch: 020 ----
mean loss: 75.38
train mean loss: 76.64
epoch train time: 0:00:00.339740
elapsed time: 0:00:58.910036
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:33:50.395173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.09
 ---- batch: 020 ----
mean loss: 74.30
train mean loss: 74.44
epoch train time: 0:00:00.336336
elapsed time: 0:00:59.246529
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:33:50.731717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.50
 ---- batch: 020 ----
mean loss: 75.54
train mean loss: 76.16
epoch train time: 0:00:00.360663
elapsed time: 0:00:59.607406
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:33:51.092593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.52
 ---- batch: 020 ----
mean loss: 75.56
train mean loss: 75.91
epoch train time: 0:00:00.342528
elapsed time: 0:00:59.950142
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:33:51.435313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.85
 ---- batch: 020 ----
mean loss: 75.72
train mean loss: 74.88
epoch train time: 0:00:00.350161
elapsed time: 0:01:00.300511
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:33:51.785668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.97
 ---- batch: 020 ----
mean loss: 72.81
train mean loss: 73.62
epoch train time: 0:00:00.354088
elapsed time: 0:01:00.654791
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:33:52.139962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.72
 ---- batch: 020 ----
mean loss: 71.83
train mean loss: 72.74
epoch train time: 0:00:00.346272
elapsed time: 0:01:01.001264
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:33:52.486419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.64
 ---- batch: 020 ----
mean loss: 73.28
train mean loss: 72.97
epoch train time: 0:00:00.340357
elapsed time: 0:01:01.341803
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:33:52.826960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.86
 ---- batch: 020 ----
mean loss: 71.71
train mean loss: 71.68
epoch train time: 0:00:00.337920
elapsed time: 0:01:01.679906
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:33:53.165061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.86
 ---- batch: 020 ----
mean loss: 71.36
train mean loss: 72.23
epoch train time: 0:00:00.336629
elapsed time: 0:01:02.016730
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:33:53.501881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.12
 ---- batch: 020 ----
mean loss: 69.50
train mean loss: 71.45
epoch train time: 0:00:00.340043
elapsed time: 0:01:02.356943
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:33:53.842097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.37
 ---- batch: 020 ----
mean loss: 70.99
train mean loss: 71.78
epoch train time: 0:00:00.344565
elapsed time: 0:01:02.701693
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:33:54.186852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.05
 ---- batch: 020 ----
mean loss: 71.00
train mean loss: 71.16
epoch train time: 0:00:00.342563
elapsed time: 0:01:03.044431
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:33:54.529580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.79
 ---- batch: 020 ----
mean loss: 69.88
train mean loss: 69.02
epoch train time: 0:00:00.362074
elapsed time: 0:01:03.406696
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:33:54.891870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.10
 ---- batch: 020 ----
mean loss: 72.68
train mean loss: 70.11
epoch train time: 0:00:00.341689
elapsed time: 0:01:03.748576
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:33:55.233729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.66
 ---- batch: 020 ----
mean loss: 68.02
train mean loss: 69.76
epoch train time: 0:00:00.342536
elapsed time: 0:01:04.091295
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:33:55.576454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.67
 ---- batch: 020 ----
mean loss: 69.06
train mean loss: 70.47
epoch train time: 0:00:00.362430
elapsed time: 0:01:04.453902
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:33:55.939058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.07
 ---- batch: 020 ----
mean loss: 67.85
train mean loss: 69.54
epoch train time: 0:00:00.341032
elapsed time: 0:01:04.795104
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:33:56.280267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.71
 ---- batch: 020 ----
mean loss: 64.16
train mean loss: 68.96
epoch train time: 0:00:00.337939
elapsed time: 0:01:05.133233
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:33:56.618410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.38
 ---- batch: 020 ----
mean loss: 65.62
train mean loss: 67.34
epoch train time: 0:00:00.347018
elapsed time: 0:01:05.480449
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:33:56.965605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.46
 ---- batch: 020 ----
mean loss: 68.03
train mean loss: 67.51
epoch train time: 0:00:00.343480
elapsed time: 0:01:05.824097
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:33:57.309276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.92
 ---- batch: 020 ----
mean loss: 65.16
train mean loss: 66.80
epoch train time: 0:00:00.342201
elapsed time: 0:01:06.166497
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:33:57.651654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.34
 ---- batch: 020 ----
mean loss: 67.08
train mean loss: 67.43
epoch train time: 0:00:00.350465
elapsed time: 0:01:06.517141
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:33:58.002325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.44
 ---- batch: 020 ----
mean loss: 67.16
train mean loss: 66.52
epoch train time: 0:00:00.343928
elapsed time: 0:01:06.861271
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:33:58.346427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.60
 ---- batch: 020 ----
mean loss: 65.76
train mean loss: 65.46
epoch train time: 0:00:00.347065
elapsed time: 0:01:07.208514
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:33:58.693674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.91
 ---- batch: 020 ----
mean loss: 67.26
train mean loss: 66.65
epoch train time: 0:00:00.347485
elapsed time: 0:01:07.556219
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:33:59.041352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.27
 ---- batch: 020 ----
mean loss: 66.61
train mean loss: 65.23
epoch train time: 0:00:00.345626
elapsed time: 0:01:07.902008
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:33:59.387161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.41
 ---- batch: 020 ----
mean loss: 67.19
train mean loss: 65.72
epoch train time: 0:00:00.346171
elapsed time: 0:01:08.248356
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:33:59.733514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.36
 ---- batch: 020 ----
mean loss: 62.52
train mean loss: 64.17
epoch train time: 0:00:00.345215
elapsed time: 0:01:08.593749
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:34:00.078905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.24
 ---- batch: 020 ----
mean loss: 64.86
train mean loss: 64.04
epoch train time: 0:00:00.333511
elapsed time: 0:01:08.927435
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:34:00.412607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.89
 ---- batch: 020 ----
mean loss: 66.81
train mean loss: 63.72
epoch train time: 0:00:00.336263
elapsed time: 0:01:09.263885
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:34:00.749037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.87
 ---- batch: 020 ----
mean loss: 64.54
train mean loss: 64.39
epoch train time: 0:00:00.348556
elapsed time: 0:01:09.612608
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:34:01.097780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.95
 ---- batch: 020 ----
mean loss: 63.57
train mean loss: 63.34
epoch train time: 0:00:00.339584
elapsed time: 0:01:09.952381
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:34:01.437570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.40
 ---- batch: 020 ----
mean loss: 62.99
train mean loss: 63.60
epoch train time: 0:00:00.353426
elapsed time: 0:01:10.306071
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:34:01.791271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.09
 ---- batch: 020 ----
mean loss: 66.24
train mean loss: 65.03
epoch train time: 0:00:00.340995
elapsed time: 0:01:10.647292
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:34:02.132449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.54
 ---- batch: 020 ----
mean loss: 61.58
train mean loss: 62.40
epoch train time: 0:00:00.333852
elapsed time: 0:01:10.981316
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:34:02.466469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.66
 ---- batch: 020 ----
mean loss: 60.17
train mean loss: 62.25
epoch train time: 0:00:00.336602
elapsed time: 0:01:11.318103
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:34:02.803258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.51
 ---- batch: 020 ----
mean loss: 61.94
train mean loss: 62.06
epoch train time: 0:00:00.354116
elapsed time: 0:01:11.672395
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:34:03.157551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.38
 ---- batch: 020 ----
mean loss: 63.99
train mean loss: 63.07
epoch train time: 0:00:00.346116
elapsed time: 0:01:12.018708
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:34:03.503898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.03
 ---- batch: 020 ----
mean loss: 62.55
train mean loss: 62.36
epoch train time: 0:00:00.346766
elapsed time: 0:01:12.365691
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:34:03.850846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.51
 ---- batch: 020 ----
mean loss: 61.77
train mean loss: 62.33
epoch train time: 0:00:00.345225
elapsed time: 0:01:12.711089
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:34:04.196245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.29
 ---- batch: 020 ----
mean loss: 60.15
train mean loss: 61.34
epoch train time: 0:00:00.343354
elapsed time: 0:01:13.054623
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:34:04.539779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.88
 ---- batch: 020 ----
mean loss: 61.44
train mean loss: 60.71
epoch train time: 0:00:00.343469
elapsed time: 0:01:13.398269
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:34:04.883428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.61
 ---- batch: 020 ----
mean loss: 59.56
train mean loss: 60.68
epoch train time: 0:00:00.349160
elapsed time: 0:01:13.747607
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:34:05.232764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.96
 ---- batch: 020 ----
mean loss: 58.32
train mean loss: 62.17
epoch train time: 0:00:00.350305
elapsed time: 0:01:14.098090
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:34:05.583258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.87
 ---- batch: 020 ----
mean loss: 62.53
train mean loss: 60.42
epoch train time: 0:00:00.356238
elapsed time: 0:01:14.454513
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:34:05.939666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.83
 ---- batch: 020 ----
mean loss: 60.89
train mean loss: 59.31
epoch train time: 0:00:00.342776
elapsed time: 0:01:14.797481
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:34:06.282638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.34
 ---- batch: 020 ----
mean loss: 61.42
train mean loss: 60.82
epoch train time: 0:00:00.336652
elapsed time: 0:01:15.134307
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:34:06.619459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.25
 ---- batch: 020 ----
mean loss: 61.39
train mean loss: 59.04
epoch train time: 0:00:00.349254
elapsed time: 0:01:15.483737
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:34:06.968912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.42
 ---- batch: 020 ----
mean loss: 59.58
train mean loss: 58.45
epoch train time: 0:00:00.354541
elapsed time: 0:01:15.838480
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:34:07.323656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.33
 ---- batch: 020 ----
mean loss: 59.37
train mean loss: 59.72
epoch train time: 0:00:00.348703
elapsed time: 0:01:16.187385
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:34:07.672545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.25
 ---- batch: 020 ----
mean loss: 61.07
train mean loss: 59.95
epoch train time: 0:00:00.359390
elapsed time: 0:01:16.546952
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:34:08.032120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.40
 ---- batch: 020 ----
mean loss: 59.89
train mean loss: 58.07
epoch train time: 0:00:00.347738
elapsed time: 0:01:16.894883
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:34:08.380044
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.13
 ---- batch: 020 ----
mean loss: 57.56
train mean loss: 58.61
epoch train time: 0:00:00.342184
elapsed time: 0:01:17.237302
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:34:08.722436
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.32
 ---- batch: 020 ----
mean loss: 60.34
train mean loss: 57.18
epoch train time: 0:00:00.357986
elapsed time: 0:01:17.595443
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:34:09.080600
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.17
 ---- batch: 020 ----
mean loss: 58.56
train mean loss: 57.93
epoch train time: 0:00:00.335007
elapsed time: 0:01:17.930626
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:34:09.415788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.36
 ---- batch: 020 ----
mean loss: 56.74
train mean loss: 56.74
epoch train time: 0:00:00.342053
elapsed time: 0:01:18.272866
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:34:09.758025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.88
 ---- batch: 020 ----
mean loss: 57.56
train mean loss: 58.27
epoch train time: 0:00:00.346836
elapsed time: 0:01:18.619884
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:34:10.105042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.18
 ---- batch: 020 ----
mean loss: 58.74
train mean loss: 57.60
epoch train time: 0:00:00.345823
elapsed time: 0:01:18.965881
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:34:10.451094
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.17
 ---- batch: 020 ----
mean loss: 56.94
train mean loss: 57.12
epoch train time: 0:00:00.377091
elapsed time: 0:01:19.343226
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:34:10.828393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.64
 ---- batch: 020 ----
mean loss: 58.25
train mean loss: 58.27
epoch train time: 0:00:00.363445
elapsed time: 0:01:19.706862
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:34:11.192021
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.50
 ---- batch: 020 ----
mean loss: 57.79
train mean loss: 58.17
epoch train time: 0:00:00.350668
elapsed time: 0:01:20.057707
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:34:11.542862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.45
 ---- batch: 020 ----
mean loss: 58.62
train mean loss: 56.87
epoch train time: 0:00:00.366553
elapsed time: 0:01:20.424474
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:34:11.909632
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.66
 ---- batch: 020 ----
mean loss: 55.92
train mean loss: 57.22
epoch train time: 0:00:00.345786
elapsed time: 0:01:20.770441
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:34:12.255599
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.47
 ---- batch: 020 ----
mean loss: 55.95
train mean loss: 57.54
epoch train time: 0:00:00.348303
elapsed time: 0:01:21.118919
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:34:12.604072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.24
 ---- batch: 020 ----
mean loss: 59.19
train mean loss: 57.11
epoch train time: 0:00:00.347478
elapsed time: 0:01:21.466589
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:34:12.951794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.16
 ---- batch: 020 ----
mean loss: 57.81
train mean loss: 56.64
epoch train time: 0:00:00.349683
elapsed time: 0:01:21.816502
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:34:13.301658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.48
 ---- batch: 020 ----
mean loss: 56.77
train mean loss: 57.31
epoch train time: 0:00:00.347151
elapsed time: 0:01:22.163835
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:34:13.648995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.03
 ---- batch: 020 ----
mean loss: 59.66
train mean loss: 57.55
epoch train time: 0:00:00.363967
elapsed time: 0:01:22.527985
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:34:14.013145
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.07
 ---- batch: 020 ----
mean loss: 58.05
train mean loss: 57.19
epoch train time: 0:00:00.350080
elapsed time: 0:01:22.878261
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:34:14.363417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.91
 ---- batch: 020 ----
mean loss: 58.36
train mean loss: 57.28
epoch train time: 0:00:00.353822
elapsed time: 0:01:23.232269
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:34:14.717453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.01
 ---- batch: 020 ----
mean loss: 59.59
train mean loss: 57.30
epoch train time: 0:00:00.363672
elapsed time: 0:01:23.596160
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:34:15.081317
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.95
 ---- batch: 020 ----
mean loss: 55.92
train mean loss: 57.16
epoch train time: 0:00:00.347982
elapsed time: 0:01:23.944320
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:34:15.429480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.45
 ---- batch: 020 ----
mean loss: 57.46
train mean loss: 58.12
epoch train time: 0:00:00.346140
elapsed time: 0:01:24.290644
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:34:15.775803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.67
 ---- batch: 020 ----
mean loss: 57.53
train mean loss: 57.82
epoch train time: 0:00:00.348134
elapsed time: 0:01:24.638960
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:34:16.124137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.46
 ---- batch: 020 ----
mean loss: 56.92
train mean loss: 56.58
epoch train time: 0:00:00.339049
elapsed time: 0:01:24.978232
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:34:16.463389
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.97
 ---- batch: 020 ----
mean loss: 58.93
train mean loss: 57.77
epoch train time: 0:00:00.340993
elapsed time: 0:01:25.319404
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:34:16.804562
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.12
 ---- batch: 020 ----
mean loss: 59.39
train mean loss: 56.99
epoch train time: 0:00:00.344845
elapsed time: 0:01:25.664430
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:34:17.149633
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.72
 ---- batch: 020 ----
mean loss: 58.98
train mean loss: 57.75
epoch train time: 0:00:00.349579
elapsed time: 0:01:26.014264
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:34:17.499423
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.59
 ---- batch: 020 ----
mean loss: 57.26
train mean loss: 57.53
epoch train time: 0:00:00.363221
elapsed time: 0:01:26.377670
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:34:17.862828
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 52.72
 ---- batch: 020 ----
mean loss: 59.61
train mean loss: 57.01
epoch train time: 0:00:00.335493
elapsed time: 0:01:26.713352
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:34:18.198516
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 54.36
 ---- batch: 020 ----
mean loss: 58.82
train mean loss: 56.46
epoch train time: 0:00:00.334747
elapsed time: 0:01:27.048279
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:34:18.533453
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.69
 ---- batch: 020 ----
mean loss: 57.26
train mean loss: 56.60
epoch train time: 0:00:00.336090
elapsed time: 0:01:27.384558
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:34:18.869711
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.08
 ---- batch: 020 ----
mean loss: 58.32
train mean loss: 56.62
epoch train time: 0:00:00.336636
elapsed time: 0:01:27.721368
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:34:19.206524
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.26
 ---- batch: 020 ----
mean loss: 55.75
train mean loss: 57.40
epoch train time: 0:00:00.336341
elapsed time: 0:01:28.057923
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:34:19.543083
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.54
 ---- batch: 020 ----
mean loss: 55.77
train mean loss: 57.44
epoch train time: 0:00:00.347389
elapsed time: 0:01:28.405514
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:34:19.890645
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.40
 ---- batch: 020 ----
mean loss: 57.27
train mean loss: 56.30
epoch train time: 0:00:00.345131
elapsed time: 0:01:28.750791
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:34:20.235942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.82
 ---- batch: 020 ----
mean loss: 56.96
train mean loss: 56.66
epoch train time: 0:00:00.343483
elapsed time: 0:01:29.094442
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:34:20.579595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.53
 ---- batch: 020 ----
mean loss: 57.81
train mean loss: 57.12
epoch train time: 0:00:00.343335
elapsed time: 0:01:29.437951
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:34:20.923107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.26
 ---- batch: 020 ----
mean loss: 57.46
train mean loss: 56.43
epoch train time: 0:00:00.345221
elapsed time: 0:01:29.783347
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:34:21.268502
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.54
 ---- batch: 020 ----
mean loss: 55.02
train mean loss: 57.48
epoch train time: 0:00:00.344760
elapsed time: 0:01:30.128304
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:34:21.613480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.56
 ---- batch: 020 ----
mean loss: 57.93
train mean loss: 56.28
epoch train time: 0:00:00.347346
elapsed time: 0:01:30.475845
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:34:21.961021
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.69
 ---- batch: 020 ----
mean loss: 57.33
train mean loss: 57.29
epoch train time: 0:00:00.346753
elapsed time: 0:01:30.822798
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:34:22.307958
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.21
 ---- batch: 020 ----
mean loss: 57.79
train mean loss: 56.71
epoch train time: 0:00:00.344645
elapsed time: 0:01:31.167628
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:34:22.652788
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.52
 ---- batch: 020 ----
mean loss: 57.22
train mean loss: 56.30
epoch train time: 0:00:00.354812
elapsed time: 0:01:31.522623
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:34:23.007777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.97
 ---- batch: 020 ----
mean loss: 57.43
train mean loss: 57.14
epoch train time: 0:00:00.354095
elapsed time: 0:01:31.876916
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:34:23.362090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.21
 ---- batch: 020 ----
mean loss: 58.22
train mean loss: 56.74
epoch train time: 0:00:00.350382
elapsed time: 0:01:32.227551
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:34:23.712750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.26
 ---- batch: 020 ----
mean loss: 56.76
train mean loss: 56.04
epoch train time: 0:00:00.360748
elapsed time: 0:01:32.588520
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:34:24.073677
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.96
 ---- batch: 020 ----
mean loss: 55.10
train mean loss: 56.80
epoch train time: 0:00:00.338618
elapsed time: 0:01:32.927324
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:34:24.412483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.84
 ---- batch: 020 ----
mean loss: 56.37
train mean loss: 56.88
epoch train time: 0:00:00.334124
elapsed time: 0:01:33.261628
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:34:24.746785
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 56.62
 ---- batch: 020 ----
mean loss: 56.51
train mean loss: 56.63
epoch train time: 0:00:00.348569
elapsed time: 0:01:33.610403
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:34:25.095575
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 55.20
 ---- batch: 020 ----
mean loss: 57.33
train mean loss: 55.99
epoch train time: 0:00:00.342106
elapsed time: 0:01:33.960542
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_6/checkpoint.pth.tar
**** end time: 2019-10-01 14:34:25.445654 ****
