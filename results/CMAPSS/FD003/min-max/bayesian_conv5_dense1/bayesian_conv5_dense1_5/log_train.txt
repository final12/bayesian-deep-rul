Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_5', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 29892
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-20 13:29:25.681949 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 31, 14]             200
           Sigmoid-2           [-1, 10, 31, 14]               0
    BayesianConv2d-3           [-1, 10, 30, 14]           2,000
           Sigmoid-4           [-1, 10, 30, 14]               0
    BayesianConv2d-5           [-1, 10, 31, 14]           2,000
           Sigmoid-6           [-1, 10, 31, 14]               0
    BayesianConv2d-7           [-1, 10, 30, 14]           2,000
           Sigmoid-8           [-1, 10, 30, 14]               0
    BayesianConv2d-9            [-1, 1, 30, 14]              60
         Softplus-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
   BayesianLinear-12                  [-1, 100]          84,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 90,460
Trainable params: 90,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 13:29:25.698220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2789.95
 ---- batch: 020 ----
mean loss: 1955.52
 ---- batch: 030 ----
mean loss: 1652.26
 ---- batch: 040 ----
mean loss: 1397.96
train mean loss: 1904.67
epoch train time: 0:00:19.829244
elapsed time: 0:00:19.852928
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 13:29:45.534911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1234.77
 ---- batch: 020 ----
mean loss: 1187.97
 ---- batch: 030 ----
mean loss: 1188.56
 ---- batch: 040 ----
mean loss: 1137.36
train mean loss: 1184.94
epoch train time: 0:00:07.961554
elapsed time: 0:00:27.814819
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 13:29:53.496939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1099.55
 ---- batch: 020 ----
mean loss: 1088.98
 ---- batch: 030 ----
mean loss: 1088.73
 ---- batch: 040 ----
mean loss: 1082.39
train mean loss: 1086.23
epoch train time: 0:00:07.930030
elapsed time: 0:00:35.745393
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 13:30:01.427494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1086.85
 ---- batch: 020 ----
mean loss: 1034.97
 ---- batch: 030 ----
mean loss: 1056.71
 ---- batch: 040 ----
mean loss: 1053.58
train mean loss: 1060.63
epoch train time: 0:00:07.655989
elapsed time: 0:00:43.401833
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 13:30:09.083957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1028.79
 ---- batch: 020 ----
mean loss: 1030.61
 ---- batch: 030 ----
mean loss: 1039.07
 ---- batch: 040 ----
mean loss: 1019.40
train mean loss: 1027.65
epoch train time: 0:00:07.654638
elapsed time: 0:00:51.056921
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 13:30:16.739014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1027.40
 ---- batch: 020 ----
mean loss: 1023.99
 ---- batch: 030 ----
mean loss: 1026.18
 ---- batch: 040 ----
mean loss: 1021.11
train mean loss: 1022.28
epoch train time: 0:00:07.682457
elapsed time: 0:00:58.739817
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 13:30:24.421997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 995.47
 ---- batch: 020 ----
mean loss: 1009.57
 ---- batch: 030 ----
mean loss: 993.78
 ---- batch: 040 ----
mean loss: 987.48
train mean loss: 996.22
epoch train time: 0:00:07.678702
elapsed time: 0:01:06.419070
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 13:30:32.101178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 987.50
 ---- batch: 020 ----
mean loss: 1008.67
 ---- batch: 030 ----
mean loss: 1011.52
 ---- batch: 040 ----
mean loss: 982.42
train mean loss: 996.58
epoch train time: 0:00:07.698131
elapsed time: 0:01:14.117615
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 13:30:39.799707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.91
 ---- batch: 020 ----
mean loss: 1003.24
 ---- batch: 030 ----
mean loss: 988.69
 ---- batch: 040 ----
mean loss: 979.80
train mean loss: 986.32
epoch train time: 0:00:07.560359
elapsed time: 0:01:21.678392
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 13:30:47.360509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.43
 ---- batch: 020 ----
mean loss: 981.59
 ---- batch: 030 ----
mean loss: 960.74
 ---- batch: 040 ----
mean loss: 968.01
train mean loss: 972.96
epoch train time: 0:00:07.577393
elapsed time: 0:01:29.256210
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 13:30:54.938371
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.51
 ---- batch: 020 ----
mean loss: 980.73
 ---- batch: 030 ----
mean loss: 946.23
 ---- batch: 040 ----
mean loss: 934.81
train mean loss: 947.89
epoch train time: 0:00:07.555774
elapsed time: 0:01:36.812425
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 13:31:02.494529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.63
 ---- batch: 020 ----
mean loss: 793.93
 ---- batch: 030 ----
mean loss: 701.77
 ---- batch: 040 ----
mean loss: 614.14
train mean loss: 729.52
epoch train time: 0:00:07.552570
elapsed time: 0:01:44.365483
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 13:31:10.047631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.00
 ---- batch: 020 ----
mean loss: 506.82
 ---- batch: 030 ----
mean loss: 493.71
 ---- batch: 040 ----
mean loss: 486.27
train mean loss: 504.85
epoch train time: 0:00:07.576558
elapsed time: 0:01:51.942503
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 13:31:17.624592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.94
 ---- batch: 020 ----
mean loss: 454.23
 ---- batch: 030 ----
mean loss: 447.96
 ---- batch: 040 ----
mean loss: 429.90
train mean loss: 453.95
epoch train time: 0:00:07.633985
elapsed time: 0:01:59.576883
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 13:31:25.258969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.27
 ---- batch: 020 ----
mean loss: 420.28
 ---- batch: 030 ----
mean loss: 419.32
 ---- batch: 040 ----
mean loss: 398.75
train mean loss: 416.85
epoch train time: 0:00:07.665199
elapsed time: 0:02:07.242494
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 13:31:32.924637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.18
 ---- batch: 020 ----
mean loss: 383.49
 ---- batch: 030 ----
mean loss: 389.98
 ---- batch: 040 ----
mean loss: 372.89
train mean loss: 385.63
epoch train time: 0:00:07.451899
elapsed time: 0:02:14.694820
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 13:31:40.376942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.47
 ---- batch: 020 ----
mean loss: 373.66
 ---- batch: 030 ----
mean loss: 369.48
 ---- batch: 040 ----
mean loss: 357.37
train mean loss: 369.59
epoch train time: 0:00:07.476177
elapsed time: 0:02:22.171429
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 13:31:47.853520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.72
 ---- batch: 020 ----
mean loss: 353.87
 ---- batch: 030 ----
mean loss: 357.57
 ---- batch: 040 ----
mean loss: 352.67
train mean loss: 353.21
epoch train time: 0:00:07.452800
elapsed time: 0:02:29.624625
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 13:31:55.306776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.38
 ---- batch: 020 ----
mean loss: 362.45
 ---- batch: 030 ----
mean loss: 340.30
 ---- batch: 040 ----
mean loss: 329.76
train mean loss: 341.37
epoch train time: 0:00:07.423241
elapsed time: 0:02:37.048328
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 13:32:02.730373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.61
 ---- batch: 020 ----
mean loss: 327.09
 ---- batch: 030 ----
mean loss: 332.46
 ---- batch: 040 ----
mean loss: 336.60
train mean loss: 332.76
epoch train time: 0:00:07.425611
elapsed time: 0:02:44.474281
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 13:32:10.156402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.43
 ---- batch: 020 ----
mean loss: 316.39
 ---- batch: 030 ----
mean loss: 325.19
 ---- batch: 040 ----
mean loss: 313.38
train mean loss: 317.01
epoch train time: 0:00:07.750683
elapsed time: 0:02:52.225393
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 13:32:17.907476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.18
 ---- batch: 020 ----
mean loss: 326.57
 ---- batch: 030 ----
mean loss: 305.75
 ---- batch: 040 ----
mean loss: 301.59
train mean loss: 309.87
epoch train time: 0:00:07.749144
elapsed time: 0:02:59.974922
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 13:32:25.656985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.14
 ---- batch: 020 ----
mean loss: 299.48
 ---- batch: 030 ----
mean loss: 290.09
 ---- batch: 040 ----
mean loss: 301.29
train mean loss: 299.28
epoch train time: 0:00:07.751539
elapsed time: 0:03:07.726810
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 13:32:33.408964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.72
 ---- batch: 020 ----
mean loss: 284.27
 ---- batch: 030 ----
mean loss: 287.13
 ---- batch: 040 ----
mean loss: 283.14
train mean loss: 282.76
epoch train time: 0:00:07.541477
elapsed time: 0:03:15.268791
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 13:32:40.950897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.55
 ---- batch: 020 ----
mean loss: 267.44
 ---- batch: 030 ----
mean loss: 263.36
 ---- batch: 040 ----
mean loss: 280.33
train mean loss: 269.80
epoch train time: 0:00:07.291352
elapsed time: 0:03:22.560592
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 13:32:48.242713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.17
 ---- batch: 020 ----
mean loss: 258.12
 ---- batch: 030 ----
mean loss: 257.76
 ---- batch: 040 ----
mean loss: 258.85
train mean loss: 261.71
epoch train time: 0:00:07.315847
elapsed time: 0:03:29.876838
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 13:32:55.558933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.78
 ---- batch: 020 ----
mean loss: 243.99
 ---- batch: 030 ----
mean loss: 257.03
 ---- batch: 040 ----
mean loss: 249.81
train mean loss: 252.09
epoch train time: 0:00:07.518842
elapsed time: 0:03:37.396065
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 13:33:03.078193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.57
 ---- batch: 020 ----
mean loss: 252.19
 ---- batch: 030 ----
mean loss: 243.20
 ---- batch: 040 ----
mean loss: 231.63
train mean loss: 241.18
epoch train time: 0:00:07.496621
elapsed time: 0:03:44.893264
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 13:33:10.575369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.16
 ---- batch: 020 ----
mean loss: 237.64
 ---- batch: 030 ----
mean loss: 237.75
 ---- batch: 040 ----
mean loss: 250.67
train mean loss: 239.85
epoch train time: 0:00:07.506415
elapsed time: 0:03:52.400072
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 13:33:18.082206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.58
 ---- batch: 020 ----
mean loss: 241.56
 ---- batch: 030 ----
mean loss: 221.92
 ---- batch: 040 ----
mean loss: 230.87
train mean loss: 231.34
epoch train time: 0:00:07.545798
elapsed time: 0:03:59.946313
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 13:33:25.628435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.97
 ---- batch: 020 ----
mean loss: 226.40
 ---- batch: 030 ----
mean loss: 226.16
 ---- batch: 040 ----
mean loss: 225.52
train mean loss: 226.52
epoch train time: 0:00:07.528030
elapsed time: 0:04:07.474761
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 13:33:33.156849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.22
 ---- batch: 020 ----
mean loss: 219.09
 ---- batch: 030 ----
mean loss: 219.50
 ---- batch: 040 ----
mean loss: 220.11
train mean loss: 223.72
epoch train time: 0:00:07.556003
elapsed time: 0:04:15.031164
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 13:33:40.713268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.05
 ---- batch: 020 ----
mean loss: 215.39
 ---- batch: 030 ----
mean loss: 219.05
 ---- batch: 040 ----
mean loss: 230.62
train mean loss: 222.64
epoch train time: 0:00:07.540003
elapsed time: 0:04:22.571595
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 13:33:48.253690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.53
 ---- batch: 020 ----
mean loss: 223.83
 ---- batch: 030 ----
mean loss: 215.75
 ---- batch: 040 ----
mean loss: 207.68
train mean loss: 215.59
epoch train time: 0:00:07.718750
elapsed time: 0:04:30.290788
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 13:33:55.972936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.53
 ---- batch: 020 ----
mean loss: 212.28
 ---- batch: 030 ----
mean loss: 206.74
 ---- batch: 040 ----
mean loss: 205.54
train mean loss: 208.68
epoch train time: 0:00:07.788166
elapsed time: 0:04:38.079429
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 13:34:03.761573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.36
 ---- batch: 020 ----
mean loss: 204.98
 ---- batch: 030 ----
mean loss: 200.87
 ---- batch: 040 ----
mean loss: 204.52
train mean loss: 204.01
epoch train time: 0:00:07.665609
elapsed time: 0:04:45.745546
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 13:34:11.427633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.96
 ---- batch: 020 ----
mean loss: 207.25
 ---- batch: 030 ----
mean loss: 202.81
 ---- batch: 040 ----
mean loss: 205.35
train mean loss: 201.34
epoch train time: 0:00:07.629393
elapsed time: 0:04:53.375400
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 13:34:19.057521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.33
 ---- batch: 020 ----
mean loss: 195.45
 ---- batch: 030 ----
mean loss: 202.30
 ---- batch: 040 ----
mean loss: 208.48
train mean loss: 203.12
epoch train time: 0:00:07.595237
elapsed time: 0:05:00.971047
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 13:34:26.653146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.71
 ---- batch: 020 ----
mean loss: 196.26
 ---- batch: 030 ----
mean loss: 201.28
 ---- batch: 040 ----
mean loss: 197.02
train mean loss: 198.43
epoch train time: 0:00:07.547464
elapsed time: 0:05:08.518911
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 13:34:34.201019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.90
 ---- batch: 020 ----
mean loss: 200.19
 ---- batch: 030 ----
mean loss: 201.29
 ---- batch: 040 ----
mean loss: 193.20
train mean loss: 197.03
epoch train time: 0:00:07.716196
elapsed time: 0:05:16.235579
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 13:34:41.917766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.32
 ---- batch: 020 ----
mean loss: 188.15
 ---- batch: 030 ----
mean loss: 197.12
 ---- batch: 040 ----
mean loss: 185.30
train mean loss: 191.63
epoch train time: 0:00:07.713431
elapsed time: 0:05:23.949582
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 13:34:49.631735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.19
 ---- batch: 020 ----
mean loss: 187.28
 ---- batch: 030 ----
mean loss: 198.44
 ---- batch: 040 ----
mean loss: 183.09
train mean loss: 188.22
epoch train time: 0:00:07.717242
elapsed time: 0:05:31.667243
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 13:34:57.349353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.13
 ---- batch: 020 ----
mean loss: 183.73
 ---- batch: 030 ----
mean loss: 183.99
 ---- batch: 040 ----
mean loss: 182.46
train mean loss: 184.56
epoch train time: 0:00:07.571286
elapsed time: 0:05:39.238946
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 13:35:04.921043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.83
 ---- batch: 020 ----
mean loss: 185.35
 ---- batch: 030 ----
mean loss: 178.94
 ---- batch: 040 ----
mean loss: 183.97
train mean loss: 184.96
epoch train time: 0:00:07.434938
elapsed time: 0:05:46.674310
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 13:35:12.356369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.86
 ---- batch: 020 ----
mean loss: 184.80
 ---- batch: 030 ----
mean loss: 177.76
 ---- batch: 040 ----
mean loss: 180.68
train mean loss: 181.62
epoch train time: 0:00:07.440216
elapsed time: 0:05:54.114921
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 13:35:19.797042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.93
 ---- batch: 020 ----
mean loss: 183.35
 ---- batch: 030 ----
mean loss: 180.09
 ---- batch: 040 ----
mean loss: 182.18
train mean loss: 182.03
epoch train time: 0:00:07.538863
elapsed time: 0:06:01.654163
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 13:35:27.336268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.23
 ---- batch: 020 ----
mean loss: 174.43
 ---- batch: 030 ----
mean loss: 183.07
 ---- batch: 040 ----
mean loss: 183.34
train mean loss: 180.21
epoch train time: 0:00:07.645486
elapsed time: 0:06:09.300068
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 13:35:34.982239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.88
 ---- batch: 020 ----
mean loss: 174.39
 ---- batch: 030 ----
mean loss: 180.39
 ---- batch: 040 ----
mean loss: 168.27
train mean loss: 174.00
epoch train time: 0:00:07.705900
elapsed time: 0:06:17.006476
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 13:35:42.688588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.10
 ---- batch: 020 ----
mean loss: 176.95
 ---- batch: 030 ----
mean loss: 178.15
 ---- batch: 040 ----
mean loss: 180.28
train mean loss: 177.53
epoch train time: 0:00:07.648613
elapsed time: 0:06:24.655462
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 13:35:50.337546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.95
 ---- batch: 020 ----
mean loss: 173.58
 ---- batch: 030 ----
mean loss: 166.82
 ---- batch: 040 ----
mean loss: 172.72
train mean loss: 173.27
epoch train time: 0:00:07.644168
elapsed time: 0:06:32.300020
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 13:35:57.982151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.80
 ---- batch: 020 ----
mean loss: 167.95
 ---- batch: 030 ----
mean loss: 170.89
 ---- batch: 040 ----
mean loss: 170.59
train mean loss: 169.69
epoch train time: 0:00:07.653404
elapsed time: 0:06:39.953869
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 13:36:05.635967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.03
 ---- batch: 020 ----
mean loss: 171.66
 ---- batch: 030 ----
mean loss: 169.98
 ---- batch: 040 ----
mean loss: 163.16
train mean loss: 167.61
epoch train time: 0:00:07.718790
elapsed time: 0:06:47.673066
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 13:36:13.355166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.48
 ---- batch: 020 ----
mean loss: 167.79
 ---- batch: 030 ----
mean loss: 164.95
 ---- batch: 040 ----
mean loss: 167.19
train mean loss: 167.96
epoch train time: 0:00:07.766735
elapsed time: 0:06:55.440251
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 13:36:21.122323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.42
 ---- batch: 020 ----
mean loss: 162.57
 ---- batch: 030 ----
mean loss: 163.17
 ---- batch: 040 ----
mean loss: 164.88
train mean loss: 164.79
epoch train time: 0:00:07.703134
elapsed time: 0:07:03.143827
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 13:36:28.825936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.47
 ---- batch: 020 ----
mean loss: 165.93
 ---- batch: 030 ----
mean loss: 158.95
 ---- batch: 040 ----
mean loss: 152.63
train mean loss: 160.51
epoch train time: 0:00:07.804183
elapsed time: 0:07:10.948433
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 13:36:36.630530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.79
 ---- batch: 020 ----
mean loss: 159.35
 ---- batch: 030 ----
mean loss: 162.11
 ---- batch: 040 ----
mean loss: 159.04
train mean loss: 161.71
epoch train time: 0:00:07.877229
elapsed time: 0:07:18.826078
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 13:36:44.508178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.36
 ---- batch: 020 ----
mean loss: 159.01
 ---- batch: 030 ----
mean loss: 156.40
 ---- batch: 040 ----
mean loss: 160.30
train mean loss: 159.44
epoch train time: 0:00:07.832244
elapsed time: 0:07:26.658765
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 13:36:52.340887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.43
 ---- batch: 020 ----
mean loss: 154.80
 ---- batch: 030 ----
mean loss: 156.16
 ---- batch: 040 ----
mean loss: 159.86
train mean loss: 157.70
epoch train time: 0:00:07.903184
elapsed time: 0:07:34.562464
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 13:37:00.244650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.28
 ---- batch: 020 ----
mean loss: 158.16
 ---- batch: 030 ----
mean loss: 159.01
 ---- batch: 040 ----
mean loss: 159.04
train mean loss: 157.28
epoch train time: 0:00:07.945963
elapsed time: 0:07:42.508920
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 13:37:08.191045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.68
 ---- batch: 020 ----
mean loss: 161.30
 ---- batch: 030 ----
mean loss: 153.23
 ---- batch: 040 ----
mean loss: 153.04
train mean loss: 155.46
epoch train time: 0:00:07.846003
elapsed time: 0:07:50.355372
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 13:37:16.037485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.30
 ---- batch: 020 ----
mean loss: 151.89
 ---- batch: 030 ----
mean loss: 161.06
 ---- batch: 040 ----
mean loss: 154.55
train mean loss: 155.68
epoch train time: 0:00:07.961855
elapsed time: 0:07:58.317670
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 13:37:23.999767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.58
 ---- batch: 020 ----
mean loss: 151.99
 ---- batch: 030 ----
mean loss: 150.52
 ---- batch: 040 ----
mean loss: 152.48
train mean loss: 151.73
epoch train time: 0:00:07.735913
elapsed time: 0:08:06.054031
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 13:37:31.736134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.51
 ---- batch: 020 ----
mean loss: 152.67
 ---- batch: 030 ----
mean loss: 151.92
 ---- batch: 040 ----
mean loss: 157.54
train mean loss: 153.05
epoch train time: 0:00:07.717915
elapsed time: 0:08:13.772366
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 13:37:39.454493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.54
 ---- batch: 020 ----
mean loss: 147.18
 ---- batch: 030 ----
mean loss: 155.24
 ---- batch: 040 ----
mean loss: 146.01
train mean loss: 148.99
epoch train time: 0:00:07.857027
elapsed time: 0:08:21.629822
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 13:37:47.311927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.28
 ---- batch: 020 ----
mean loss: 148.18
 ---- batch: 030 ----
mean loss: 153.27
 ---- batch: 040 ----
mean loss: 146.89
train mean loss: 151.85
epoch train time: 0:00:07.686630
elapsed time: 0:08:29.316874
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 13:37:54.998988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.33
 ---- batch: 020 ----
mean loss: 153.35
 ---- batch: 030 ----
mean loss: 146.56
 ---- batch: 040 ----
mean loss: 147.95
train mean loss: 150.34
epoch train time: 0:00:07.664133
elapsed time: 0:08:36.981462
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 13:38:02.663542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.12
 ---- batch: 020 ----
mean loss: 137.70
 ---- batch: 030 ----
mean loss: 150.95
 ---- batch: 040 ----
mean loss: 154.56
train mean loss: 147.04
epoch train time: 0:00:07.629473
elapsed time: 0:08:44.611320
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 13:38:10.293416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.42
 ---- batch: 020 ----
mean loss: 140.84
 ---- batch: 030 ----
mean loss: 146.97
 ---- batch: 040 ----
mean loss: 147.72
train mean loss: 144.91
epoch train time: 0:00:07.677558
elapsed time: 0:08:52.289366
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 13:38:17.971472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.49
 ---- batch: 020 ----
mean loss: 143.04
 ---- batch: 030 ----
mean loss: 145.18
 ---- batch: 040 ----
mean loss: 145.32
train mean loss: 145.49
epoch train time: 0:00:07.481180
elapsed time: 0:08:59.770980
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 13:38:25.453092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.68
 ---- batch: 020 ----
mean loss: 140.71
 ---- batch: 030 ----
mean loss: 142.65
 ---- batch: 040 ----
mean loss: 143.20
train mean loss: 143.21
epoch train time: 0:00:07.797532
elapsed time: 0:09:07.568937
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 13:38:33.251060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.34
 ---- batch: 020 ----
mean loss: 143.76
 ---- batch: 030 ----
mean loss: 141.67
 ---- batch: 040 ----
mean loss: 142.96
train mean loss: 143.90
epoch train time: 0:00:07.981340
elapsed time: 0:09:15.550719
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 13:38:41.232829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.68
 ---- batch: 020 ----
mean loss: 141.27
 ---- batch: 030 ----
mean loss: 140.38
 ---- batch: 040 ----
mean loss: 137.60
train mean loss: 140.23
epoch train time: 0:00:07.955737
elapsed time: 0:09:23.506871
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 13:38:49.188981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.92
 ---- batch: 020 ----
mean loss: 144.93
 ---- batch: 030 ----
mean loss: 139.29
 ---- batch: 040 ----
mean loss: 138.86
train mean loss: 139.70
epoch train time: 0:00:07.959572
elapsed time: 0:09:31.466880
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 13:38:57.148987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.78
 ---- batch: 020 ----
mean loss: 138.04
 ---- batch: 030 ----
mean loss: 136.07
 ---- batch: 040 ----
mean loss: 141.28
train mean loss: 139.38
epoch train time: 0:00:08.009092
elapsed time: 0:09:39.476410
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 13:39:05.158510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.57
 ---- batch: 020 ----
mean loss: 135.50
 ---- batch: 030 ----
mean loss: 135.15
 ---- batch: 040 ----
mean loss: 140.07
train mean loss: 137.30
epoch train time: 0:00:07.994644
elapsed time: 0:09:47.471493
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 13:39:13.153542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.94
 ---- batch: 020 ----
mean loss: 138.77
 ---- batch: 030 ----
mean loss: 134.98
 ---- batch: 040 ----
mean loss: 138.10
train mean loss: 137.68
epoch train time: 0:00:07.934415
elapsed time: 0:09:55.406275
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 13:39:21.088377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.83
 ---- batch: 020 ----
mean loss: 130.72
 ---- batch: 030 ----
mean loss: 132.48
 ---- batch: 040 ----
mean loss: 137.69
train mean loss: 134.52
epoch train time: 0:00:08.007642
elapsed time: 0:10:03.414338
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 13:39:29.096473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.37
 ---- batch: 020 ----
mean loss: 139.78
 ---- batch: 030 ----
mean loss: 133.48
 ---- batch: 040 ----
mean loss: 131.38
train mean loss: 134.17
epoch train time: 0:00:08.070266
elapsed time: 0:10:11.485152
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 13:39:37.167313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.32
 ---- batch: 020 ----
mean loss: 132.73
 ---- batch: 030 ----
mean loss: 134.55
 ---- batch: 040 ----
mean loss: 130.97
train mean loss: 133.62
epoch train time: 0:00:08.097988
elapsed time: 0:10:19.583614
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 13:39:45.265717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.77
 ---- batch: 020 ----
mean loss: 129.60
 ---- batch: 030 ----
mean loss: 134.87
 ---- batch: 040 ----
mean loss: 131.91
train mean loss: 133.19
epoch train time: 0:00:07.989793
elapsed time: 0:10:27.573832
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 13:39:53.255928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.66
 ---- batch: 020 ----
mean loss: 130.53
 ---- batch: 030 ----
mean loss: 133.35
 ---- batch: 040 ----
mean loss: 131.37
train mean loss: 132.29
epoch train time: 0:00:08.042879
elapsed time: 0:10:35.617182
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 13:40:01.299315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.74
 ---- batch: 020 ----
mean loss: 134.97
 ---- batch: 030 ----
mean loss: 134.05
 ---- batch: 040 ----
mean loss: 131.84
train mean loss: 131.77
epoch train time: 0:00:08.053282
elapsed time: 0:10:43.670986
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 13:40:09.353129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.73
 ---- batch: 020 ----
mean loss: 128.59
 ---- batch: 030 ----
mean loss: 126.16
 ---- batch: 040 ----
mean loss: 127.31
train mean loss: 128.94
epoch train time: 0:00:08.210097
elapsed time: 0:10:51.881600
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 13:40:17.563716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.50
 ---- batch: 020 ----
mean loss: 131.42
 ---- batch: 030 ----
mean loss: 129.39
 ---- batch: 040 ----
mean loss: 132.35
train mean loss: 129.12
epoch train time: 0:00:08.172174
elapsed time: 0:11:00.054266
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 13:40:25.736374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.59
 ---- batch: 020 ----
mean loss: 129.47
 ---- batch: 030 ----
mean loss: 126.26
 ---- batch: 040 ----
mean loss: 125.41
train mean loss: 127.75
epoch train time: 0:00:08.429242
elapsed time: 0:11:08.483989
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 13:40:34.166054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.30
 ---- batch: 020 ----
mean loss: 129.25
 ---- batch: 030 ----
mean loss: 126.33
 ---- batch: 040 ----
mean loss: 127.96
train mean loss: 127.97
epoch train time: 0:00:08.463278
elapsed time: 0:11:16.947744
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 13:40:42.629883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.11
 ---- batch: 020 ----
mean loss: 127.12
 ---- batch: 030 ----
mean loss: 127.29
 ---- batch: 040 ----
mean loss: 127.49
train mean loss: 127.72
epoch train time: 0:00:08.466024
elapsed time: 0:11:25.414368
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 13:40:51.096528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.04
 ---- batch: 020 ----
mean loss: 127.23
 ---- batch: 030 ----
mean loss: 129.89
 ---- batch: 040 ----
mean loss: 122.35
train mean loss: 126.78
epoch train time: 0:00:08.431321
elapsed time: 0:11:33.846194
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 13:40:59.528326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.57
 ---- batch: 020 ----
mean loss: 124.50
 ---- batch: 030 ----
mean loss: 123.35
 ---- batch: 040 ----
mean loss: 121.96
train mean loss: 125.01
epoch train time: 0:00:08.323637
elapsed time: 0:11:42.170320
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 13:41:07.852454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.71
 ---- batch: 020 ----
mean loss: 122.38
 ---- batch: 030 ----
mean loss: 123.13
 ---- batch: 040 ----
mean loss: 125.56
train mean loss: 123.94
epoch train time: 0:00:08.361887
elapsed time: 0:11:50.532727
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 13:41:16.214832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.30
 ---- batch: 020 ----
mean loss: 128.14
 ---- batch: 030 ----
mean loss: 119.02
 ---- batch: 040 ----
mean loss: 125.21
train mean loss: 123.74
epoch train time: 0:00:08.341768
elapsed time: 0:11:58.875043
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 13:41:24.557212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.38
 ---- batch: 020 ----
mean loss: 122.05
 ---- batch: 030 ----
mean loss: 118.39
 ---- batch: 040 ----
mean loss: 122.50
train mean loss: 121.35
epoch train time: 0:00:08.363321
elapsed time: 0:12:07.238899
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 13:41:32.920997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.31
 ---- batch: 020 ----
mean loss: 123.02
 ---- batch: 030 ----
mean loss: 122.34
 ---- batch: 040 ----
mean loss: 122.26
train mean loss: 122.16
epoch train time: 0:00:08.334879
elapsed time: 0:12:15.574265
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 13:41:41.256444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.60
 ---- batch: 020 ----
mean loss: 118.57
 ---- batch: 030 ----
mean loss: 123.26
 ---- batch: 040 ----
mean loss: 120.76
train mean loss: 121.49
epoch train time: 0:00:08.346549
elapsed time: 0:12:23.921389
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 13:41:49.603575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.38
 ---- batch: 020 ----
mean loss: 121.65
 ---- batch: 030 ----
mean loss: 114.84
 ---- batch: 040 ----
mean loss: 123.07
train mean loss: 119.31
epoch train time: 0:00:08.323032
elapsed time: 0:12:32.245017
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 13:41:57.927140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.74
 ---- batch: 020 ----
mean loss: 123.59
 ---- batch: 030 ----
mean loss: 119.54
 ---- batch: 040 ----
mean loss: 117.76
train mean loss: 119.54
epoch train time: 0:00:08.442287
elapsed time: 0:12:40.687780
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 13:42:06.369890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.26
 ---- batch: 020 ----
mean loss: 119.51
 ---- batch: 030 ----
mean loss: 114.33
 ---- batch: 040 ----
mean loss: 121.19
train mean loss: 119.91
epoch train time: 0:00:08.450025
elapsed time: 0:12:49.138312
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 13:42:14.820460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.06
 ---- batch: 020 ----
mean loss: 114.46
 ---- batch: 030 ----
mean loss: 115.42
 ---- batch: 040 ----
mean loss: 118.98
train mean loss: 117.62
epoch train time: 0:00:08.484585
elapsed time: 0:12:57.623414
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 13:42:23.305519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.02
 ---- batch: 020 ----
mean loss: 117.05
 ---- batch: 030 ----
mean loss: 118.48
 ---- batch: 040 ----
mean loss: 119.88
train mean loss: 116.97
epoch train time: 0:00:08.477743
elapsed time: 0:13:06.101622
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 13:42:31.783725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.23
 ---- batch: 020 ----
mean loss: 120.51
 ---- batch: 030 ----
mean loss: 112.89
 ---- batch: 040 ----
mean loss: 116.64
train mean loss: 116.77
epoch train time: 0:00:08.454497
elapsed time: 0:13:14.556581
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 13:42:40.238684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.33
 ---- batch: 020 ----
mean loss: 117.71
 ---- batch: 030 ----
mean loss: 115.50
 ---- batch: 040 ----
mean loss: 116.24
train mean loss: 115.34
epoch train time: 0:00:08.461032
elapsed time: 0:13:23.018060
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 13:42:48.700132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.20
 ---- batch: 020 ----
mean loss: 115.90
 ---- batch: 030 ----
mean loss: 114.43
 ---- batch: 040 ----
mean loss: 115.56
train mean loss: 115.43
epoch train time: 0:00:08.195873
elapsed time: 0:13:31.214316
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 13:42:56.896430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.42
 ---- batch: 020 ----
mean loss: 111.37
 ---- batch: 030 ----
mean loss: 113.61
 ---- batch: 040 ----
mean loss: 120.00
train mean loss: 114.12
epoch train time: 0:00:08.051503
elapsed time: 0:13:39.266239
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 13:43:04.948297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.17
 ---- batch: 020 ----
mean loss: 116.99
 ---- batch: 030 ----
mean loss: 115.13
 ---- batch: 040 ----
mean loss: 109.48
train mean loss: 114.03
epoch train time: 0:00:08.006291
elapsed time: 0:13:47.272909
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 13:43:12.955025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.15
 ---- batch: 020 ----
mean loss: 114.95
 ---- batch: 030 ----
mean loss: 109.83
 ---- batch: 040 ----
mean loss: 115.05
train mean loss: 112.01
epoch train time: 0:00:07.978173
elapsed time: 0:13:55.251503
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 13:43:20.933603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.04
 ---- batch: 020 ----
mean loss: 108.58
 ---- batch: 030 ----
mean loss: 110.78
 ---- batch: 040 ----
mean loss: 115.23
train mean loss: 112.94
epoch train time: 0:00:07.832416
elapsed time: 0:14:03.084360
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 13:43:28.766461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.22
 ---- batch: 020 ----
mean loss: 112.23
 ---- batch: 030 ----
mean loss: 109.37
 ---- batch: 040 ----
mean loss: 109.74
train mean loss: 111.12
epoch train time: 0:00:07.817857
elapsed time: 0:14:10.902738
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 13:43:36.584803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.68
 ---- batch: 020 ----
mean loss: 109.95
 ---- batch: 030 ----
mean loss: 111.39
 ---- batch: 040 ----
mean loss: 108.57
train mean loss: 110.45
epoch train time: 0:00:07.858188
elapsed time: 0:14:18.761334
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 13:43:44.443444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.22
 ---- batch: 020 ----
mean loss: 112.67
 ---- batch: 030 ----
mean loss: 107.57
 ---- batch: 040 ----
mean loss: 115.87
train mean loss: 110.88
epoch train time: 0:00:07.868630
elapsed time: 0:14:26.630376
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 13:43:52.312512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.73
 ---- batch: 020 ----
mean loss: 107.70
 ---- batch: 030 ----
mean loss: 111.51
 ---- batch: 040 ----
mean loss: 102.79
train mean loss: 109.29
epoch train time: 0:00:07.820296
elapsed time: 0:14:34.451136
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 13:44:00.133226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.04
 ---- batch: 020 ----
mean loss: 107.71
 ---- batch: 030 ----
mean loss: 111.77
 ---- batch: 040 ----
mean loss: 106.67
train mean loss: 108.41
epoch train time: 0:00:07.854805
elapsed time: 0:14:42.306346
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 13:44:07.988419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.16
 ---- batch: 020 ----
mean loss: 107.68
 ---- batch: 030 ----
mean loss: 110.35
 ---- batch: 040 ----
mean loss: 108.83
train mean loss: 108.95
epoch train time: 0:00:07.838718
elapsed time: 0:14:50.145467
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 13:44:15.827566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.16
 ---- batch: 020 ----
mean loss: 108.99
 ---- batch: 030 ----
mean loss: 111.03
 ---- batch: 040 ----
mean loss: 103.79
train mean loss: 108.23
epoch train time: 0:00:07.807261
elapsed time: 0:14:57.953272
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 13:44:23.635406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.02
 ---- batch: 020 ----
mean loss: 106.09
 ---- batch: 030 ----
mean loss: 106.84
 ---- batch: 040 ----
mean loss: 106.85
train mean loss: 107.26
epoch train time: 0:00:07.772440
elapsed time: 0:15:05.726202
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 13:44:31.408289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 107.19
 ---- batch: 020 ----
mean loss: 105.51
 ---- batch: 030 ----
mean loss: 105.31
 ---- batch: 040 ----
mean loss: 104.07
train mean loss: 105.61
epoch train time: 0:00:07.978824
elapsed time: 0:15:13.705447
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 13:44:39.387540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.40
 ---- batch: 020 ----
mean loss: 106.04
 ---- batch: 030 ----
mean loss: 108.82
 ---- batch: 040 ----
mean loss: 101.60
train mean loss: 105.79
epoch train time: 0:00:07.749177
elapsed time: 0:15:21.455052
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 13:44:47.137159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.97
 ---- batch: 020 ----
mean loss: 103.04
 ---- batch: 030 ----
mean loss: 106.64
 ---- batch: 040 ----
mean loss: 105.96
train mean loss: 105.47
epoch train time: 0:00:07.759646
elapsed time: 0:15:29.215119
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 13:44:54.897250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.34
 ---- batch: 020 ----
mean loss: 106.75
 ---- batch: 030 ----
mean loss: 102.90
 ---- batch: 040 ----
mean loss: 105.13
train mean loss: 104.73
epoch train time: 0:00:07.809903
elapsed time: 0:15:37.025478
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 13:45:02.707591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.41
 ---- batch: 020 ----
mean loss: 103.99
 ---- batch: 030 ----
mean loss: 105.71
 ---- batch: 040 ----
mean loss: 100.99
train mean loss: 104.12
epoch train time: 0:00:07.753868
elapsed time: 0:15:44.779784
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 13:45:10.461899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.93
 ---- batch: 020 ----
mean loss: 103.78
 ---- batch: 030 ----
mean loss: 106.00
 ---- batch: 040 ----
mean loss: 102.02
train mean loss: 104.30
epoch train time: 0:00:07.888320
elapsed time: 0:15:52.668541
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 13:45:18.350689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.16
 ---- batch: 020 ----
mean loss: 104.61
 ---- batch: 030 ----
mean loss: 112.94
 ---- batch: 040 ----
mean loss: 103.82
train mean loss: 105.14
epoch train time: 0:00:07.936049
elapsed time: 0:16:00.605045
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 13:45:26.287145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.57
 ---- batch: 020 ----
mean loss: 105.01
 ---- batch: 030 ----
mean loss: 101.66
 ---- batch: 040 ----
mean loss: 104.21
train mean loss: 102.93
epoch train time: 0:00:07.783531
elapsed time: 0:16:08.388995
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 13:45:34.071123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.14
 ---- batch: 020 ----
mean loss: 103.16
 ---- batch: 030 ----
mean loss: 105.30
 ---- batch: 040 ----
mean loss: 101.99
train mean loss: 102.85
epoch train time: 0:00:07.660853
elapsed time: 0:16:16.050307
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 13:45:41.732472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.51
 ---- batch: 020 ----
mean loss: 100.15
 ---- batch: 030 ----
mean loss: 105.03
 ---- batch: 040 ----
mean loss: 98.26
train mean loss: 100.88
epoch train time: 0:00:07.619108
elapsed time: 0:16:23.669889
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 13:45:49.351997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.36
 ---- batch: 020 ----
mean loss: 101.73
 ---- batch: 030 ----
mean loss: 103.42
 ---- batch: 040 ----
mean loss: 100.13
train mean loss: 101.67
epoch train time: 0:00:07.620986
elapsed time: 0:16:31.291281
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 13:45:56.973372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.37
 ---- batch: 020 ----
mean loss: 98.90
 ---- batch: 030 ----
mean loss: 103.91
 ---- batch: 040 ----
mean loss: 100.35
train mean loss: 100.18
epoch train time: 0:00:07.585616
elapsed time: 0:16:38.877322
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 13:46:04.559440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.78
 ---- batch: 020 ----
mean loss: 102.79
 ---- batch: 030 ----
mean loss: 99.37
 ---- batch: 040 ----
mean loss: 96.31
train mean loss: 99.54
epoch train time: 0:00:07.685165
elapsed time: 0:16:46.562986
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 13:46:12.244985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.52
 ---- batch: 020 ----
mean loss: 98.78
 ---- batch: 030 ----
mean loss: 99.67
 ---- batch: 040 ----
mean loss: 99.19
train mean loss: 100.57
epoch train time: 0:00:07.713413
elapsed time: 0:16:54.276728
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 13:46:19.958835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.76
 ---- batch: 020 ----
mean loss: 102.26
 ---- batch: 030 ----
mean loss: 102.51
 ---- batch: 040 ----
mean loss: 100.55
train mean loss: 100.33
epoch train time: 0:00:07.698015
elapsed time: 0:17:01.975150
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 13:46:27.657233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.32
 ---- batch: 020 ----
mean loss: 99.53
 ---- batch: 030 ----
mean loss: 101.14
 ---- batch: 040 ----
mean loss: 101.11
train mean loss: 100.60
epoch train time: 0:00:07.704472
elapsed time: 0:17:09.680041
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 13:46:35.362185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.55
 ---- batch: 020 ----
mean loss: 100.80
 ---- batch: 030 ----
mean loss: 97.27
 ---- batch: 040 ----
mean loss: 96.73
train mean loss: 98.72
epoch train time: 0:00:07.645193
elapsed time: 0:17:17.325718
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 13:46:43.007831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.29
 ---- batch: 020 ----
mean loss: 94.68
 ---- batch: 030 ----
mean loss: 97.80
 ---- batch: 040 ----
mean loss: 100.57
train mean loss: 98.93
epoch train time: 0:00:07.649855
elapsed time: 0:17:24.975993
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 13:46:50.658066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.00
 ---- batch: 020 ----
mean loss: 96.08
 ---- batch: 030 ----
mean loss: 97.81
 ---- batch: 040 ----
mean loss: 93.51
train mean loss: 97.18
epoch train time: 0:00:07.823292
elapsed time: 0:17:32.799627
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 13:46:58.481727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.80
 ---- batch: 020 ----
mean loss: 97.90
 ---- batch: 030 ----
mean loss: 97.29
 ---- batch: 040 ----
mean loss: 92.62
train mean loss: 95.41
epoch train time: 0:00:07.632309
elapsed time: 0:17:40.432375
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 13:47:06.114489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.81
 ---- batch: 020 ----
mean loss: 99.50
 ---- batch: 030 ----
mean loss: 96.81
 ---- batch: 040 ----
mean loss: 95.37
train mean loss: 97.57
epoch train time: 0:00:07.676315
elapsed time: 0:17:48.109251
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 13:47:13.791367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.37
 ---- batch: 020 ----
mean loss: 92.27
 ---- batch: 030 ----
mean loss: 94.55
 ---- batch: 040 ----
mean loss: 100.14
train mean loss: 95.44
epoch train time: 0:00:07.610824
elapsed time: 0:17:55.720593
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 13:47:21.402712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.75
 ---- batch: 020 ----
mean loss: 95.87
 ---- batch: 030 ----
mean loss: 94.35
 ---- batch: 040 ----
mean loss: 94.48
train mean loss: 95.45
epoch train time: 0:00:07.613031
elapsed time: 0:18:03.334104
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 13:47:29.016215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 94.73
 ---- batch: 020 ----
mean loss: 94.93
 ---- batch: 030 ----
mean loss: 100.76
 ---- batch: 040 ----
mean loss: 99.93
train mean loss: 97.34
epoch train time: 0:00:07.637126
elapsed time: 0:18:10.971680
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 13:47:36.653792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.35
 ---- batch: 020 ----
mean loss: 101.95
 ---- batch: 030 ----
mean loss: 94.91
 ---- batch: 040 ----
mean loss: 91.49
train mean loss: 95.53
epoch train time: 0:00:07.686154
elapsed time: 0:18:18.658266
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 13:47:44.340370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.11
 ---- batch: 020 ----
mean loss: 95.01
 ---- batch: 030 ----
mean loss: 97.39
 ---- batch: 040 ----
mean loss: 94.29
train mean loss: 95.63
epoch train time: 0:00:07.812923
elapsed time: 0:18:26.471672
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 13:47:52.153787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.71
 ---- batch: 020 ----
mean loss: 89.81
 ---- batch: 030 ----
mean loss: 94.48
 ---- batch: 040 ----
mean loss: 94.37
train mean loss: 93.11
epoch train time: 0:00:08.005602
elapsed time: 0:18:34.477724
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 13:48:00.159877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.01
 ---- batch: 020 ----
mean loss: 94.53
 ---- batch: 030 ----
mean loss: 94.53
 ---- batch: 040 ----
mean loss: 97.97
train mean loss: 94.09
epoch train time: 0:00:07.808369
elapsed time: 0:18:42.286559
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 13:48:07.968657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.95
 ---- batch: 020 ----
mean loss: 95.44
 ---- batch: 030 ----
mean loss: 96.02
 ---- batch: 040 ----
mean loss: 89.74
train mean loss: 92.52
epoch train time: 0:00:07.798384
elapsed time: 0:18:50.085429
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 13:48:15.767606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.44
 ---- batch: 020 ----
mean loss: 92.16
 ---- batch: 030 ----
mean loss: 92.06
 ---- batch: 040 ----
mean loss: 91.00
train mean loss: 91.81
epoch train time: 0:00:07.774935
elapsed time: 0:18:57.860897
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 13:48:23.543021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.05
 ---- batch: 020 ----
mean loss: 94.04
 ---- batch: 030 ----
mean loss: 94.17
 ---- batch: 040 ----
mean loss: 89.15
train mean loss: 92.54
epoch train time: 0:00:07.776172
elapsed time: 0:19:05.637521
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 13:48:31.319627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.89
 ---- batch: 020 ----
mean loss: 88.71
 ---- batch: 030 ----
mean loss: 93.01
 ---- batch: 040 ----
mean loss: 91.87
train mean loss: 91.79
epoch train time: 0:00:07.602422
elapsed time: 0:19:13.240395
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 13:48:38.922495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.66
 ---- batch: 020 ----
mean loss: 92.78
 ---- batch: 030 ----
mean loss: 90.84
 ---- batch: 040 ----
mean loss: 94.84
train mean loss: 91.26
epoch train time: 0:00:07.676181
elapsed time: 0:19:20.916979
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 13:48:46.599093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.72
 ---- batch: 020 ----
mean loss: 88.95
 ---- batch: 030 ----
mean loss: 90.73
 ---- batch: 040 ----
mean loss: 89.64
train mean loss: 90.51
epoch train time: 0:00:07.731720
elapsed time: 0:19:28.649113
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 13:48:54.331266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.30
 ---- batch: 020 ----
mean loss: 93.53
 ---- batch: 030 ----
mean loss: 90.77
 ---- batch: 040 ----
mean loss: 89.94
train mean loss: 90.56
epoch train time: 0:00:07.748613
elapsed time: 0:19:36.398263
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 13:49:02.080297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.33
 ---- batch: 020 ----
mean loss: 89.50
 ---- batch: 030 ----
mean loss: 90.84
 ---- batch: 040 ----
mean loss: 88.75
train mean loss: 90.73
epoch train time: 0:00:07.807932
elapsed time: 0:19:44.206572
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 13:49:09.888660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.06
 ---- batch: 020 ----
mean loss: 90.47
 ---- batch: 030 ----
mean loss: 88.11
 ---- batch: 040 ----
mean loss: 88.63
train mean loss: 89.77
epoch train time: 0:00:07.916231
elapsed time: 0:19:52.123192
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 13:49:17.805311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.93
 ---- batch: 020 ----
mean loss: 88.94
 ---- batch: 030 ----
mean loss: 89.67
 ---- batch: 040 ----
mean loss: 89.86
train mean loss: 89.93
epoch train time: 0:00:07.906581
elapsed time: 0:20:00.030169
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 13:49:25.712270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.73
 ---- batch: 020 ----
mean loss: 88.62
 ---- batch: 030 ----
mean loss: 87.80
 ---- batch: 040 ----
mean loss: 92.66
train mean loss: 88.75
epoch train time: 0:00:07.933684
elapsed time: 0:20:07.964252
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 13:49:33.646327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.13
 ---- batch: 020 ----
mean loss: 89.73
 ---- batch: 030 ----
mean loss: 84.18
 ---- batch: 040 ----
mean loss: 88.99
train mean loss: 88.03
epoch train time: 0:00:07.737609
elapsed time: 0:20:15.702353
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 13:49:41.384540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.24
 ---- batch: 020 ----
mean loss: 88.77
 ---- batch: 030 ----
mean loss: 87.85
 ---- batch: 040 ----
mean loss: 89.21
train mean loss: 88.46
epoch train time: 0:00:07.767108
elapsed time: 0:20:23.469940
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 13:49:49.152049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.75
 ---- batch: 020 ----
mean loss: 88.11
 ---- batch: 030 ----
mean loss: 94.03
 ---- batch: 040 ----
mean loss: 86.96
train mean loss: 88.04
epoch train time: 0:00:07.783339
elapsed time: 0:20:31.253735
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 13:49:56.935843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.31
 ---- batch: 020 ----
mean loss: 88.57
 ---- batch: 030 ----
mean loss: 85.03
 ---- batch: 040 ----
mean loss: 89.28
train mean loss: 86.98
epoch train time: 0:00:07.760300
elapsed time: 0:20:39.014487
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 13:50:04.696592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.76
 ---- batch: 020 ----
mean loss: 87.74
 ---- batch: 030 ----
mean loss: 88.19
 ---- batch: 040 ----
mean loss: 85.27
train mean loss: 86.71
epoch train time: 0:00:07.787098
elapsed time: 0:20:46.802013
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 13:50:12.484143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.56
 ---- batch: 020 ----
mean loss: 84.84
 ---- batch: 030 ----
mean loss: 87.66
 ---- batch: 040 ----
mean loss: 84.00
train mean loss: 85.89
epoch train time: 0:00:07.768576
elapsed time: 0:20:54.571020
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 13:50:20.253114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.15
 ---- batch: 020 ----
mean loss: 87.18
 ---- batch: 030 ----
mean loss: 84.71
 ---- batch: 040 ----
mean loss: 91.99
train mean loss: 87.34
epoch train time: 0:00:07.735846
elapsed time: 0:21:02.307313
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 13:50:27.989425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.88
 ---- batch: 020 ----
mean loss: 84.18
 ---- batch: 030 ----
mean loss: 85.92
 ---- batch: 040 ----
mean loss: 86.62
train mean loss: 85.43
epoch train time: 0:00:07.702867
elapsed time: 0:21:10.010616
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 13:50:35.692716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.61
 ---- batch: 020 ----
mean loss: 84.88
 ---- batch: 030 ----
mean loss: 84.03
 ---- batch: 040 ----
mean loss: 85.24
train mean loss: 84.81
epoch train time: 0:00:07.702070
elapsed time: 0:21:17.713056
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 13:50:43.395142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.23
 ---- batch: 020 ----
mean loss: 85.14
 ---- batch: 030 ----
mean loss: 82.84
 ---- batch: 040 ----
mean loss: 87.28
train mean loss: 85.02
epoch train time: 0:00:07.655012
elapsed time: 0:21:25.368511
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 13:50:51.050635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.20
 ---- batch: 020 ----
mean loss: 82.50
 ---- batch: 030 ----
mean loss: 88.08
 ---- batch: 040 ----
mean loss: 83.56
train mean loss: 83.91
epoch train time: 0:00:07.600350
elapsed time: 0:21:32.969261
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 13:50:58.651351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.73
 ---- batch: 020 ----
mean loss: 85.07
 ---- batch: 030 ----
mean loss: 84.09
 ---- batch: 040 ----
mean loss: 85.51
train mean loss: 83.83
epoch train time: 0:00:07.665318
elapsed time: 0:21:40.634950
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 13:51:06.317034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.98
 ---- batch: 020 ----
mean loss: 80.93
 ---- batch: 030 ----
mean loss: 83.24
 ---- batch: 040 ----
mean loss: 82.15
train mean loss: 83.24
epoch train time: 0:00:07.503003
elapsed time: 0:21:48.138399
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 13:51:13.820541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.84
 ---- batch: 020 ----
mean loss: 83.28
 ---- batch: 030 ----
mean loss: 83.88
 ---- batch: 040 ----
mean loss: 82.55
train mean loss: 83.29
epoch train time: 0:00:07.652108
elapsed time: 0:21:55.790948
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 13:51:21.473050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.36
 ---- batch: 020 ----
mean loss: 81.69
 ---- batch: 030 ----
mean loss: 83.11
 ---- batch: 040 ----
mean loss: 85.04
train mean loss: 82.66
epoch train time: 0:00:07.596166
elapsed time: 0:22:03.387594
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 13:51:29.069718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.18
 ---- batch: 020 ----
mean loss: 80.63
 ---- batch: 030 ----
mean loss: 88.69
 ---- batch: 040 ----
mean loss: 83.48
train mean loss: 83.89
epoch train time: 0:00:07.756679
elapsed time: 0:22:11.144736
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 13:51:36.826838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.57
 ---- batch: 020 ----
mean loss: 83.63
 ---- batch: 030 ----
mean loss: 89.19
 ---- batch: 040 ----
mean loss: 83.44
train mean loss: 85.28
epoch train time: 0:00:07.767910
elapsed time: 0:22:18.913055
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 13:51:44.595156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.02
 ---- batch: 020 ----
mean loss: 79.57
 ---- batch: 030 ----
mean loss: 82.08
 ---- batch: 040 ----
mean loss: 81.07
train mean loss: 81.74
epoch train time: 0:00:08.012603
elapsed time: 0:22:26.926120
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 13:51:52.608210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.02
 ---- batch: 020 ----
mean loss: 78.77
 ---- batch: 030 ----
mean loss: 84.60
 ---- batch: 040 ----
mean loss: 79.52
train mean loss: 81.40
epoch train time: 0:00:08.069338
elapsed time: 0:22:34.995878
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 13:52:00.677992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.37
 ---- batch: 020 ----
mean loss: 86.47
 ---- batch: 030 ----
mean loss: 82.43
 ---- batch: 040 ----
mean loss: 87.02
train mean loss: 85.13
epoch train time: 0:00:08.067102
elapsed time: 0:22:43.063442
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 13:52:08.745549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.64
 ---- batch: 020 ----
mean loss: 84.23
 ---- batch: 030 ----
mean loss: 81.64
 ---- batch: 040 ----
mean loss: 79.47
train mean loss: 81.28
epoch train time: 0:00:08.049281
elapsed time: 0:22:51.113268
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 13:52:16.795325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.52
 ---- batch: 020 ----
mean loss: 81.05
 ---- batch: 030 ----
mean loss: 80.32
 ---- batch: 040 ----
mean loss: 76.39
train mean loss: 80.02
epoch train time: 0:00:08.030263
elapsed time: 0:22:59.143904
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 13:52:24.825996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.65
 ---- batch: 020 ----
mean loss: 79.65
 ---- batch: 030 ----
mean loss: 81.33
 ---- batch: 040 ----
mean loss: 81.05
train mean loss: 80.40
epoch train time: 0:00:07.878425
elapsed time: 0:23:07.022729
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 13:52:32.704847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.98
 ---- batch: 020 ----
mean loss: 78.75
 ---- batch: 030 ----
mean loss: 80.47
 ---- batch: 040 ----
mean loss: 79.78
train mean loss: 79.15
epoch train time: 0:00:07.931664
elapsed time: 0:23:14.954816
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 13:52:40.636922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.07
 ---- batch: 020 ----
mean loss: 76.44
 ---- batch: 030 ----
mean loss: 77.55
 ---- batch: 040 ----
mean loss: 79.83
train mean loss: 79.22
epoch train time: 0:00:07.896227
elapsed time: 0:23:22.851499
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 13:52:48.533641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.95
 ---- batch: 020 ----
mean loss: 80.16
 ---- batch: 030 ----
mean loss: 78.60
 ---- batch: 040 ----
mean loss: 79.65
train mean loss: 80.10
epoch train time: 0:00:07.843892
elapsed time: 0:23:30.695868
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 13:52:56.377971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.08
 ---- batch: 020 ----
mean loss: 83.14
 ---- batch: 030 ----
mean loss: 83.24
 ---- batch: 040 ----
mean loss: 81.00
train mean loss: 81.56
epoch train time: 0:00:07.936128
elapsed time: 0:23:38.632409
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 13:53:04.314466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.80
 ---- batch: 020 ----
mean loss: 79.97
 ---- batch: 030 ----
mean loss: 76.40
 ---- batch: 040 ----
mean loss: 77.22
train mean loss: 77.66
epoch train time: 0:00:07.831710
elapsed time: 0:23:46.464573
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 13:53:12.146707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.50
 ---- batch: 020 ----
mean loss: 74.86
 ---- batch: 030 ----
mean loss: 75.78
 ---- batch: 040 ----
mean loss: 76.81
train mean loss: 76.45
epoch train time: 0:00:07.800703
elapsed time: 0:23:54.265735
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 13:53:19.947881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.67
 ---- batch: 020 ----
mean loss: 79.82
 ---- batch: 030 ----
mean loss: 76.20
 ---- batch: 040 ----
mean loss: 78.70
train mean loss: 77.29
epoch train time: 0:00:07.788737
elapsed time: 0:24:02.054958
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 13:53:27.737115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.61
 ---- batch: 020 ----
mean loss: 79.39
 ---- batch: 030 ----
mean loss: 77.94
 ---- batch: 040 ----
mean loss: 79.42
train mean loss: 78.31
epoch train time: 0:00:07.681220
elapsed time: 0:24:09.736638
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 13:53:35.418725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.64
 ---- batch: 020 ----
mean loss: 78.19
 ---- batch: 030 ----
mean loss: 72.98
 ---- batch: 040 ----
mean loss: 79.42
train mean loss: 76.70
epoch train time: 0:00:07.754403
elapsed time: 0:24:17.491480
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 13:53:43.173613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.54
 ---- batch: 020 ----
mean loss: 69.85
 ---- batch: 030 ----
mean loss: 78.17
 ---- batch: 040 ----
mean loss: 77.82
train mean loss: 75.76
epoch train time: 0:00:07.681329
elapsed time: 0:24:25.173278
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 13:53:50.855414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.05
 ---- batch: 020 ----
mean loss: 78.88
 ---- batch: 030 ----
mean loss: 76.41
 ---- batch: 040 ----
mean loss: 73.08
train mean loss: 75.85
epoch train time: 0:00:07.707842
elapsed time: 0:24:32.881560
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 13:53:58.563654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.20
 ---- batch: 020 ----
mean loss: 73.73
 ---- batch: 030 ----
mean loss: 76.46
 ---- batch: 040 ----
mean loss: 76.57
train mean loss: 75.74
epoch train time: 0:00:07.731994
elapsed time: 0:24:40.613977
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 13:54:06.296081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.70
 ---- batch: 020 ----
mean loss: 75.64
 ---- batch: 030 ----
mean loss: 75.46
 ---- batch: 040 ----
mean loss: 76.01
train mean loss: 75.30
epoch train time: 0:00:07.640339
elapsed time: 0:24:48.254797
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 13:54:13.936896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.74
 ---- batch: 020 ----
mean loss: 76.75
 ---- batch: 030 ----
mean loss: 74.26
 ---- batch: 040 ----
mean loss: 78.10
train mean loss: 76.08
epoch train time: 0:00:07.658094
elapsed time: 0:24:55.913361
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 13:54:21.595509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.94
 ---- batch: 020 ----
mean loss: 74.80
 ---- batch: 030 ----
mean loss: 74.48
 ---- batch: 040 ----
mean loss: 75.46
train mean loss: 74.75
epoch train time: 0:00:07.723846
elapsed time: 0:25:03.637670
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 13:54:29.319773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.06
 ---- batch: 020 ----
mean loss: 73.24
 ---- batch: 030 ----
mean loss: 77.31
 ---- batch: 040 ----
mean loss: 71.91
train mean loss: 74.16
epoch train time: 0:00:07.848085
elapsed time: 0:25:11.486176
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 13:54:37.168265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.49
 ---- batch: 020 ----
mean loss: 72.58
 ---- batch: 030 ----
mean loss: 75.87
 ---- batch: 040 ----
mean loss: 73.17
train mean loss: 73.97
epoch train time: 0:00:07.865958
elapsed time: 0:25:19.352565
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 13:54:45.034650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.02
 ---- batch: 020 ----
mean loss: 74.45
 ---- batch: 030 ----
mean loss: 75.25
 ---- batch: 040 ----
mean loss: 74.70
train mean loss: 74.17
epoch train time: 0:00:07.848547
elapsed time: 0:25:27.201524
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 13:54:52.883634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.15
 ---- batch: 020 ----
mean loss: 76.38
 ---- batch: 030 ----
mean loss: 70.97
 ---- batch: 040 ----
mean loss: 71.66
train mean loss: 72.76
epoch train time: 0:00:07.840077
elapsed time: 0:25:35.042070
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 13:55:00.724178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.07
 ---- batch: 020 ----
mean loss: 73.81
 ---- batch: 030 ----
mean loss: 76.96
 ---- batch: 040 ----
mean loss: 75.88
train mean loss: 74.67
epoch train time: 0:00:07.800001
elapsed time: 0:25:42.842511
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 13:55:08.524610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.32
 ---- batch: 020 ----
mean loss: 75.70
 ---- batch: 030 ----
mean loss: 69.96
 ---- batch: 040 ----
mean loss: 73.30
train mean loss: 73.26
epoch train time: 0:00:07.896166
elapsed time: 0:25:50.739112
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 13:55:16.421232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.05
 ---- batch: 020 ----
mean loss: 73.11
 ---- batch: 030 ----
mean loss: 74.69
 ---- batch: 040 ----
mean loss: 70.51
train mean loss: 73.11
epoch train time: 0:00:07.935827
elapsed time: 0:25:58.675386
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 13:55:24.357512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.93
 ---- batch: 020 ----
mean loss: 72.25
 ---- batch: 030 ----
mean loss: 71.62
 ---- batch: 040 ----
mean loss: 74.11
train mean loss: 72.42
epoch train time: 0:00:07.930406
elapsed time: 0:26:06.606252
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 13:55:32.288365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.00
 ---- batch: 020 ----
mean loss: 73.55
 ---- batch: 030 ----
mean loss: 73.34
 ---- batch: 040 ----
mean loss: 71.52
train mean loss: 72.25
epoch train time: 0:00:07.906445
elapsed time: 0:26:14.513119
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 13:55:40.195226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.01
 ---- batch: 020 ----
mean loss: 72.15
 ---- batch: 030 ----
mean loss: 70.40
 ---- batch: 040 ----
mean loss: 73.30
train mean loss: 71.99
epoch train time: 0:00:07.898944
elapsed time: 0:26:22.412479
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 13:55:48.094586
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.82
 ---- batch: 020 ----
mean loss: 70.18
 ---- batch: 030 ----
mean loss: 71.63
 ---- batch: 040 ----
mean loss: 71.91
train mean loss: 70.50
epoch train time: 0:00:07.921067
elapsed time: 0:26:30.334170
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 13:55:56.016200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.99
 ---- batch: 020 ----
mean loss: 73.75
 ---- batch: 030 ----
mean loss: 68.89
 ---- batch: 040 ----
mean loss: 69.61
train mean loss: 70.27
epoch train time: 0:00:07.912025
elapsed time: 0:26:38.246603
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 13:56:03.928713
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.17
 ---- batch: 020 ----
mean loss: 73.14
 ---- batch: 030 ----
mean loss: 68.46
 ---- batch: 040 ----
mean loss: 69.68
train mean loss: 70.79
epoch train time: 0:00:07.894251
elapsed time: 0:26:46.141354
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 13:56:11.823465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.80
 ---- batch: 020 ----
mean loss: 70.45
 ---- batch: 030 ----
mean loss: 71.63
 ---- batch: 040 ----
mean loss: 73.41
train mean loss: 70.87
epoch train time: 0:00:07.892528
elapsed time: 0:26:54.034412
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 13:56:19.716529
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.74
 ---- batch: 020 ----
mean loss: 71.37
 ---- batch: 030 ----
mean loss: 70.24
 ---- batch: 040 ----
mean loss: 70.37
train mean loss: 70.86
epoch train time: 0:00:07.879882
elapsed time: 0:27:01.914730
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 13:56:27.596815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.37
 ---- batch: 020 ----
mean loss: 71.12
 ---- batch: 030 ----
mean loss: 70.58
 ---- batch: 040 ----
mean loss: 67.44
train mean loss: 70.01
epoch train time: 0:00:07.909837
elapsed time: 0:27:09.824962
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 13:56:35.507068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.07
 ---- batch: 020 ----
mean loss: 68.74
 ---- batch: 030 ----
mean loss: 70.45
 ---- batch: 040 ----
mean loss: 68.75
train mean loss: 70.39
epoch train time: 0:00:07.860550
elapsed time: 0:27:17.685973
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 13:56:43.368037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.38
 ---- batch: 020 ----
mean loss: 73.36
 ---- batch: 030 ----
mean loss: 67.39
 ---- batch: 040 ----
mean loss: 70.38
train mean loss: 70.16
epoch train time: 0:00:07.868232
elapsed time: 0:27:25.554626
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 13:56:51.236724
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.03
 ---- batch: 020 ----
mean loss: 69.99
 ---- batch: 030 ----
mean loss: 68.15
 ---- batch: 040 ----
mean loss: 72.45
train mean loss: 69.80
epoch train time: 0:00:07.927022
elapsed time: 0:27:33.482051
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 13:56:59.164147
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.14
 ---- batch: 020 ----
mean loss: 73.25
 ---- batch: 030 ----
mean loss: 70.37
 ---- batch: 040 ----
mean loss: 66.56
train mean loss: 70.36
epoch train time: 0:00:08.078083
elapsed time: 0:27:41.560548
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 13:57:07.242649
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.59
 ---- batch: 020 ----
mean loss: 67.63
 ---- batch: 030 ----
mean loss: 68.83
 ---- batch: 040 ----
mean loss: 70.06
train mean loss: 69.76
epoch train time: 0:00:08.049426
elapsed time: 0:27:49.610488
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 13:57:15.292601
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.98
 ---- batch: 020 ----
mean loss: 69.36
 ---- batch: 030 ----
mean loss: 67.66
 ---- batch: 040 ----
mean loss: 70.78
train mean loss: 69.53
epoch train time: 0:00:08.058142
elapsed time: 0:27:57.669086
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 13:57:23.351182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.85
 ---- batch: 020 ----
mean loss: 66.85
 ---- batch: 030 ----
mean loss: 66.77
 ---- batch: 040 ----
mean loss: 75.44
train mean loss: 70.08
epoch train time: 0:00:08.056843
elapsed time: 0:28:05.726388
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 13:57:31.408643
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 71.02
 ---- batch: 020 ----
mean loss: 71.33
 ---- batch: 030 ----
mean loss: 68.74
 ---- batch: 040 ----
mean loss: 68.44
train mean loss: 69.89
epoch train time: 0:00:07.907719
elapsed time: 0:28:13.634689
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 13:57:39.316790
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.66
 ---- batch: 020 ----
mean loss: 70.95
 ---- batch: 030 ----
mean loss: 71.74
 ---- batch: 040 ----
mean loss: 69.79
train mean loss: 70.70
epoch train time: 0:00:07.905771
elapsed time: 0:28:21.540867
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 13:57:47.222965
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 73.79
 ---- batch: 020 ----
mean loss: 69.93
 ---- batch: 030 ----
mean loss: 69.34
 ---- batch: 040 ----
mean loss: 65.47
train mean loss: 69.98
epoch train time: 0:00:07.891105
elapsed time: 0:28:29.432425
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 13:57:55.114541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.16
 ---- batch: 020 ----
mean loss: 68.28
 ---- batch: 030 ----
mean loss: 73.25
 ---- batch: 040 ----
mean loss: 70.16
train mean loss: 70.33
epoch train time: 0:00:07.888449
elapsed time: 0:28:37.321329
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 13:58:03.003444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 65.41
 ---- batch: 020 ----
mean loss: 72.13
 ---- batch: 030 ----
mean loss: 70.09
 ---- batch: 040 ----
mean loss: 69.84
train mean loss: 69.86
epoch train time: 0:00:07.885212
elapsed time: 0:28:45.207006
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 13:58:10.889131
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.63
 ---- batch: 020 ----
mean loss: 69.84
 ---- batch: 030 ----
mean loss: 70.18
 ---- batch: 040 ----
mean loss: 69.17
train mean loss: 69.97
epoch train time: 0:00:07.898593
elapsed time: 0:28:53.106065
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 13:58:18.788176
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.42
 ---- batch: 020 ----
mean loss: 72.24
 ---- batch: 030 ----
mean loss: 71.34
 ---- batch: 040 ----
mean loss: 69.68
train mean loss: 70.63
epoch train time: 0:00:07.888018
elapsed time: 0:29:00.994586
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 13:58:26.676693
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.00
 ---- batch: 020 ----
mean loss: 68.89
 ---- batch: 030 ----
mean loss: 68.39
 ---- batch: 040 ----
mean loss: 72.38
train mean loss: 69.89
epoch train time: 0:00:07.906841
elapsed time: 0:29:08.901931
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 13:58:34.584035
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.05
 ---- batch: 020 ----
mean loss: 72.58
 ---- batch: 030 ----
mean loss: 70.63
 ---- batch: 040 ----
mean loss: 69.26
train mean loss: 70.14
epoch train time: 0:00:07.860249
elapsed time: 0:29:16.762624
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 13:58:42.444779
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.32
 ---- batch: 020 ----
mean loss: 69.23
 ---- batch: 030 ----
mean loss: 70.30
 ---- batch: 040 ----
mean loss: 68.77
train mean loss: 69.50
epoch train time: 0:00:07.942809
elapsed time: 0:29:24.705946
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 13:58:50.388040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.17
 ---- batch: 020 ----
mean loss: 70.42
 ---- batch: 030 ----
mean loss: 68.07
 ---- batch: 040 ----
mean loss: 70.54
train mean loss: 69.84
epoch train time: 0:00:07.932730
elapsed time: 0:29:32.639083
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 13:58:58.321175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.68
 ---- batch: 020 ----
mean loss: 68.29
 ---- batch: 030 ----
mean loss: 69.97
 ---- batch: 040 ----
mean loss: 71.54
train mean loss: 70.10
epoch train time: 0:00:07.973266
elapsed time: 0:29:40.612759
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 13:59:06.294937
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.73
 ---- batch: 020 ----
mean loss: 72.21
 ---- batch: 030 ----
mean loss: 67.42
 ---- batch: 040 ----
mean loss: 70.63
train mean loss: 69.81
epoch train time: 0:00:07.937399
elapsed time: 0:29:48.550670
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 13:59:14.232824
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.56
 ---- batch: 020 ----
mean loss: 70.10
 ---- batch: 030 ----
mean loss: 72.72
 ---- batch: 040 ----
mean loss: 66.68
train mean loss: 69.36
epoch train time: 0:00:07.913111
elapsed time: 0:29:56.464306
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 13:59:22.146423
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 72.09
 ---- batch: 020 ----
mean loss: 67.26
 ---- batch: 030 ----
mean loss: 70.88
 ---- batch: 040 ----
mean loss: 69.63
train mean loss: 69.82
epoch train time: 0:00:07.940769
elapsed time: 0:30:04.405535
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 13:59:30.087646
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.89
 ---- batch: 020 ----
mean loss: 70.45
 ---- batch: 030 ----
mean loss: 67.91
 ---- batch: 040 ----
mean loss: 69.75
train mean loss: 69.21
epoch train time: 0:00:07.977910
elapsed time: 0:30:12.383910
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 13:59:38.066017
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.64
 ---- batch: 020 ----
mean loss: 71.35
 ---- batch: 030 ----
mean loss: 71.42
 ---- batch: 040 ----
mean loss: 66.68
train mean loss: 70.01
epoch train time: 0:00:07.986366
elapsed time: 0:30:20.370719
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 13:59:46.052839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.95
 ---- batch: 020 ----
mean loss: 68.52
 ---- batch: 030 ----
mean loss: 69.55
 ---- batch: 040 ----
mean loss: 69.60
train mean loss: 70.04
epoch train time: 0:00:08.031191
elapsed time: 0:30:28.402360
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 13:59:54.084466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.08
 ---- batch: 020 ----
mean loss: 69.97
 ---- batch: 030 ----
mean loss: 70.60
 ---- batch: 040 ----
mean loss: 68.52
train mean loss: 69.52
epoch train time: 0:00:08.029986
elapsed time: 0:30:36.432800
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 14:00:02.114860
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.84
 ---- batch: 020 ----
mean loss: 70.67
 ---- batch: 030 ----
mean loss: 70.25
 ---- batch: 040 ----
mean loss: 68.57
train mean loss: 70.11
epoch train time: 0:00:08.041031
elapsed time: 0:30:44.474340
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 14:00:10.156336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.71
 ---- batch: 020 ----
mean loss: 69.54
 ---- batch: 030 ----
mean loss: 69.96
 ---- batch: 040 ----
mean loss: 69.28
train mean loss: 69.15
epoch train time: 0:00:08.048786
elapsed time: 0:30:52.523488
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 14:00:18.205592
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.34
 ---- batch: 020 ----
mean loss: 68.78
 ---- batch: 030 ----
mean loss: 67.39
 ---- batch: 040 ----
mean loss: 70.92
train mean loss: 69.18
epoch train time: 0:00:08.000016
elapsed time: 0:31:00.523904
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 14:00:26.205999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 69.10
 ---- batch: 020 ----
mean loss: 69.70
 ---- batch: 030 ----
mean loss: 69.71
 ---- batch: 040 ----
mean loss: 66.97
train mean loss: 68.93
epoch train time: 0:00:07.915857
elapsed time: 0:31:08.440207
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 14:00:34.122314
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.94
 ---- batch: 020 ----
mean loss: 65.85
 ---- batch: 030 ----
mean loss: 71.03
 ---- batch: 040 ----
mean loss: 68.90
train mean loss: 69.01
epoch train time: 0:00:08.079406
elapsed time: 0:31:16.520060
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 14:00:42.202202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.19
 ---- batch: 020 ----
mean loss: 68.52
 ---- batch: 030 ----
mean loss: 69.12
 ---- batch: 040 ----
mean loss: 68.59
train mean loss: 69.14
epoch train time: 0:00:08.057627
elapsed time: 0:31:24.578188
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 14:00:50.260299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.89
 ---- batch: 020 ----
mean loss: 68.52
 ---- batch: 030 ----
mean loss: 71.98
 ---- batch: 040 ----
mean loss: 69.05
train mean loss: 69.20
epoch train time: 0:00:08.061717
elapsed time: 0:31:32.640429
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 14:00:58.322535
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.40
 ---- batch: 020 ----
mean loss: 70.71
 ---- batch: 030 ----
mean loss: 66.65
 ---- batch: 040 ----
mean loss: 70.79
train mean loss: 69.42
epoch train time: 0:00:08.063157
elapsed time: 0:31:40.704115
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 14:01:06.386210
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.72
 ---- batch: 020 ----
mean loss: 69.25
 ---- batch: 030 ----
mean loss: 69.44
 ---- batch: 040 ----
mean loss: 70.66
train mean loss: 69.67
epoch train time: 0:00:08.076449
elapsed time: 0:31:48.780988
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 14:01:14.463102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.30
 ---- batch: 020 ----
mean loss: 68.24
 ---- batch: 030 ----
mean loss: 70.46
 ---- batch: 040 ----
mean loss: 70.16
train mean loss: 69.33
epoch train time: 0:00:07.997169
elapsed time: 0:31:56.778598
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 14:01:22.460716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 66.52
 ---- batch: 020 ----
mean loss: 71.01
 ---- batch: 030 ----
mean loss: 68.97
 ---- batch: 040 ----
mean loss: 70.20
train mean loss: 69.25
epoch train time: 0:00:07.950577
elapsed time: 0:32:04.729679
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 14:01:30.411839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.22
 ---- batch: 020 ----
mean loss: 70.53
 ---- batch: 030 ----
mean loss: 69.16
 ---- batch: 040 ----
mean loss: 67.68
train mean loss: 68.59
epoch train time: 0:00:07.919918
elapsed time: 0:32:12.650072
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 14:01:38.332134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 68.85
 ---- batch: 020 ----
mean loss: 71.32
 ---- batch: 030 ----
mean loss: 67.66
 ---- batch: 040 ----
mean loss: 69.63
train mean loss: 69.02
epoch train time: 0:00:07.979505
elapsed time: 0:32:20.630013
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 14:01:46.312119
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 67.96
 ---- batch: 020 ----
mean loss: 68.18
 ---- batch: 030 ----
mean loss: 71.28
 ---- batch: 040 ----
mean loss: 68.32
train mean loss: 68.76
epoch train time: 0:00:07.852769
elapsed time: 0:32:28.483222
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 14:01:54.165348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.18
 ---- batch: 020 ----
mean loss: 70.44
 ---- batch: 030 ----
mean loss: 66.41
 ---- batch: 040 ----
mean loss: 68.99
train mean loss: 69.34
epoch train time: 0:00:07.874691
elapsed time: 0:32:36.358389
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 14:02:02.040518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.10
 ---- batch: 020 ----
mean loss: 66.34
 ---- batch: 030 ----
mean loss: 72.13
 ---- batch: 040 ----
mean loss: 67.32
train mean loss: 68.89
epoch train time: 0:00:07.835269
elapsed time: 0:32:44.194156
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 14:02:09.876247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 70.90
 ---- batch: 020 ----
mean loss: 68.43
 ---- batch: 030 ----
mean loss: 67.50
 ---- batch: 040 ----
mean loss: 69.14
train mean loss: 69.22
epoch train time: 0:00:07.837857
elapsed time: 0:32:52.041218
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_5/checkpoint.pth.tar
**** end time: 2019-09-20 14:02:17.723184 ****
