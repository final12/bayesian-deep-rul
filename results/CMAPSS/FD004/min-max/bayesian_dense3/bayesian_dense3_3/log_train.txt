Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_3', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 14467
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 22:45:54.529413 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 22:45:54.539709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4629.81
 ---- batch: 020 ----
mean loss: 4371.12
 ---- batch: 030 ----
mean loss: 4084.32
 ---- batch: 040 ----
mean loss: 3821.05
 ---- batch: 050 ----
mean loss: 3595.95
 ---- batch: 060 ----
mean loss: 3375.18
 ---- batch: 070 ----
mean loss: 3220.66
 ---- batch: 080 ----
mean loss: 3068.29
 ---- batch: 090 ----
mean loss: 2928.96
 ---- batch: 100 ----
mean loss: 2843.00
 ---- batch: 110 ----
mean loss: 2769.27
train mean loss: 3496.99
epoch train time: 0:00:34.391701
elapsed time: 0:00:34.408228
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 22:46:28.937695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2642.05
 ---- batch: 020 ----
mean loss: 2544.20
 ---- batch: 030 ----
mean loss: 2498.33
 ---- batch: 040 ----
mean loss: 2449.03
 ---- batch: 050 ----
mean loss: 2422.65
 ---- batch: 060 ----
mean loss: 2360.89
 ---- batch: 070 ----
mean loss: 2303.66
 ---- batch: 080 ----
mean loss: 2291.86
 ---- batch: 090 ----
mean loss: 2234.82
 ---- batch: 100 ----
mean loss: 2183.29
 ---- batch: 110 ----
mean loss: 2141.94
train mean loss: 2364.84
epoch train time: 0:00:01.930654
elapsed time: 0:00:36.339164
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 22:46:30.868985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2118.43
 ---- batch: 020 ----
mean loss: 2094.88
 ---- batch: 030 ----
mean loss: 2075.37
 ---- batch: 040 ----
mean loss: 2042.72
 ---- batch: 050 ----
mean loss: 2000.33
 ---- batch: 060 ----
mean loss: 1967.35
 ---- batch: 070 ----
mean loss: 1961.85
 ---- batch: 080 ----
mean loss: 1915.09
 ---- batch: 090 ----
mean loss: 1875.63
 ---- batch: 100 ----
mean loss: 1879.12
 ---- batch: 110 ----
mean loss: 1807.82
train mean loss: 1972.53
epoch train time: 0:00:01.913037
elapsed time: 0:00:38.252806
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 22:46:32.782641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1792.44
 ---- batch: 020 ----
mean loss: 1740.72
 ---- batch: 030 ----
mean loss: 1763.18
 ---- batch: 040 ----
mean loss: 1740.71
 ---- batch: 050 ----
mean loss: 1688.51
 ---- batch: 060 ----
mean loss: 1685.06
 ---- batch: 070 ----
mean loss: 1661.81
 ---- batch: 080 ----
mean loss: 1661.18
 ---- batch: 090 ----
mean loss: 1615.05
 ---- batch: 100 ----
mean loss: 1611.69
 ---- batch: 110 ----
mean loss: 1579.04
train mean loss: 1681.99
epoch train time: 0:00:01.908645
elapsed time: 0:00:40.162078
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 22:46:34.691844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1580.98
 ---- batch: 020 ----
mean loss: 1542.33
 ---- batch: 030 ----
mean loss: 1525.12
 ---- batch: 040 ----
mean loss: 1498.32
 ---- batch: 050 ----
mean loss: 1492.15
 ---- batch: 060 ----
mean loss: 1462.72
 ---- batch: 070 ----
mean loss: 1435.80
 ---- batch: 080 ----
mean loss: 1426.32
 ---- batch: 090 ----
mean loss: 1419.58
 ---- batch: 100 ----
mean loss: 1416.20
 ---- batch: 110 ----
mean loss: 1399.58
train mean loss: 1469.67
epoch train time: 0:00:01.955789
elapsed time: 0:00:42.118429
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 22:46:36.648256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1378.28
 ---- batch: 020 ----
mean loss: 1373.40
 ---- batch: 030 ----
mean loss: 1344.82
 ---- batch: 040 ----
mean loss: 1346.79
 ---- batch: 050 ----
mean loss: 1306.12
 ---- batch: 060 ----
mean loss: 1303.45
 ---- batch: 070 ----
mean loss: 1277.49
 ---- batch: 080 ----
mean loss: 1268.04
 ---- batch: 090 ----
mean loss: 1262.83
 ---- batch: 100 ----
mean loss: 1267.68
 ---- batch: 110 ----
mean loss: 1241.14
train mean loss: 1304.91
epoch train time: 0:00:01.912262
elapsed time: 0:00:44.031307
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 22:46:38.561095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1241.26
 ---- batch: 020 ----
mean loss: 1210.42
 ---- batch: 030 ----
mean loss: 1202.62
 ---- batch: 040 ----
mean loss: 1203.90
 ---- batch: 050 ----
mean loss: 1203.53
 ---- batch: 060 ----
mean loss: 1160.88
 ---- batch: 070 ----
mean loss: 1160.33
 ---- batch: 080 ----
mean loss: 1161.50
 ---- batch: 090 ----
mean loss: 1163.71
 ---- batch: 100 ----
mean loss: 1140.23
 ---- batch: 110 ----
mean loss: 1141.98
train mean loss: 1179.89
epoch train time: 0:00:01.950658
elapsed time: 0:00:45.982527
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 22:46:40.512313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1115.49
 ---- batch: 020 ----
mean loss: 1119.73
 ---- batch: 030 ----
mean loss: 1093.66
 ---- batch: 040 ----
mean loss: 1088.62
 ---- batch: 050 ----
mean loss: 1096.59
 ---- batch: 060 ----
mean loss: 1101.55
 ---- batch: 070 ----
mean loss: 1073.26
 ---- batch: 080 ----
mean loss: 1062.64
 ---- batch: 090 ----
mean loss: 1076.96
 ---- batch: 100 ----
mean loss: 1078.41
 ---- batch: 110 ----
mean loss: 1039.41
train mean loss: 1085.16
epoch train time: 0:00:01.930444
elapsed time: 0:00:47.913566
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 22:46:42.443357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1030.27
 ---- batch: 020 ----
mean loss: 1038.57
 ---- batch: 030 ----
mean loss: 1017.57
 ---- batch: 040 ----
mean loss: 999.42
 ---- batch: 050 ----
mean loss: 1021.82
 ---- batch: 060 ----
mean loss: 1016.82
 ---- batch: 070 ----
mean loss: 1016.51
 ---- batch: 080 ----
mean loss: 998.91
 ---- batch: 090 ----
mean loss: 989.79
 ---- batch: 100 ----
mean loss: 1000.98
 ---- batch: 110 ----
mean loss: 994.29
train mean loss: 1010.14
epoch train time: 0:00:01.912496
elapsed time: 0:00:49.826635
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 22:46:44.356448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.52
 ---- batch: 020 ----
mean loss: 991.43
 ---- batch: 030 ----
mean loss: 957.03
 ---- batch: 040 ----
mean loss: 966.66
 ---- batch: 050 ----
mean loss: 954.45
 ---- batch: 060 ----
mean loss: 964.75
 ---- batch: 070 ----
mean loss: 957.83
 ---- batch: 080 ----
mean loss: 970.81
 ---- batch: 090 ----
mean loss: 947.41
 ---- batch: 100 ----
mean loss: 930.10
 ---- batch: 110 ----
mean loss: 954.66
train mean loss: 961.10
epoch train time: 0:00:01.935755
elapsed time: 0:00:51.763020
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 22:46:46.292794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.06
 ---- batch: 020 ----
mean loss: 930.05
 ---- batch: 030 ----
mean loss: 936.90
 ---- batch: 040 ----
mean loss: 941.59
 ---- batch: 050 ----
mean loss: 920.31
 ---- batch: 060 ----
mean loss: 918.88
 ---- batch: 070 ----
mean loss: 923.74
 ---- batch: 080 ----
mean loss: 914.20
 ---- batch: 090 ----
mean loss: 929.02
 ---- batch: 100 ----
mean loss: 913.32
 ---- batch: 110 ----
mean loss: 911.74
train mean loss: 924.97
epoch train time: 0:00:01.937444
elapsed time: 0:00:53.701033
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 22:46:48.230604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.82
 ---- batch: 020 ----
mean loss: 896.87
 ---- batch: 030 ----
mean loss: 908.22
 ---- batch: 040 ----
mean loss: 905.78
 ---- batch: 050 ----
mean loss: 897.46
 ---- batch: 060 ----
mean loss: 903.15
 ---- batch: 070 ----
mean loss: 902.09
 ---- batch: 080 ----
mean loss: 889.34
 ---- batch: 090 ----
mean loss: 894.41
 ---- batch: 100 ----
mean loss: 890.76
 ---- batch: 110 ----
mean loss: 869.99
train mean loss: 898.53
epoch train time: 0:00:01.933294
elapsed time: 0:00:55.634692
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 22:46:50.164491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.24
 ---- batch: 020 ----
mean loss: 902.28
 ---- batch: 030 ----
mean loss: 887.60
 ---- batch: 040 ----
mean loss: 883.30
 ---- batch: 050 ----
mean loss: 880.51
 ---- batch: 060 ----
mean loss: 865.14
 ---- batch: 070 ----
mean loss: 890.71
 ---- batch: 080 ----
mean loss: 861.64
 ---- batch: 090 ----
mean loss: 878.59
 ---- batch: 100 ----
mean loss: 892.28
 ---- batch: 110 ----
mean loss: 864.65
train mean loss: 883.08
epoch train time: 0:00:01.945983
elapsed time: 0:00:57.581244
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 22:46:52.111049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.00
 ---- batch: 020 ----
mean loss: 875.89
 ---- batch: 030 ----
mean loss: 864.54
 ---- batch: 040 ----
mean loss: 862.93
 ---- batch: 050 ----
mean loss: 864.34
 ---- batch: 060 ----
mean loss: 880.35
 ---- batch: 070 ----
mean loss: 877.15
 ---- batch: 080 ----
mean loss: 876.50
 ---- batch: 090 ----
mean loss: 869.42
 ---- batch: 100 ----
mean loss: 873.08
 ---- batch: 110 ----
mean loss: 875.66
train mean loss: 871.42
epoch train time: 0:00:01.925494
elapsed time: 0:00:59.507319
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 22:46:54.037128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.36
 ---- batch: 020 ----
mean loss: 866.21
 ---- batch: 030 ----
mean loss: 883.99
 ---- batch: 040 ----
mean loss: 880.92
 ---- batch: 050 ----
mean loss: 863.57
 ---- batch: 060 ----
mean loss: 860.58
 ---- batch: 070 ----
mean loss: 857.55
 ---- batch: 080 ----
mean loss: 853.50
 ---- batch: 090 ----
mean loss: 858.97
 ---- batch: 100 ----
mean loss: 861.47
 ---- batch: 110 ----
mean loss: 875.01
train mean loss: 865.57
epoch train time: 0:00:01.933088
elapsed time: 0:01:01.440984
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 22:46:55.970756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.65
 ---- batch: 020 ----
mean loss: 866.26
 ---- batch: 030 ----
mean loss: 864.14
 ---- batch: 040 ----
mean loss: 847.91
 ---- batch: 050 ----
mean loss: 854.15
 ---- batch: 060 ----
mean loss: 862.55
 ---- batch: 070 ----
mean loss: 879.04
 ---- batch: 080 ----
mean loss: 852.45
 ---- batch: 090 ----
mean loss: 856.56
 ---- batch: 100 ----
mean loss: 866.20
 ---- batch: 110 ----
mean loss: 851.56
train mean loss: 862.27
epoch train time: 0:00:01.897570
elapsed time: 0:01:03.339157
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 22:46:57.869041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.56
 ---- batch: 020 ----
mean loss: 829.32
 ---- batch: 030 ----
mean loss: 860.12
 ---- batch: 040 ----
mean loss: 879.43
 ---- batch: 050 ----
mean loss: 878.47
 ---- batch: 060 ----
mean loss: 873.48
 ---- batch: 070 ----
mean loss: 874.34
 ---- batch: 080 ----
mean loss: 862.93
 ---- batch: 090 ----
mean loss: 844.48
 ---- batch: 100 ----
mean loss: 853.25
 ---- batch: 110 ----
mean loss: 852.74
train mean loss: 860.60
epoch train time: 0:00:01.935469
elapsed time: 0:01:05.275325
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 22:46:59.805174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.24
 ---- batch: 020 ----
mean loss: 864.73
 ---- batch: 030 ----
mean loss: 848.44
 ---- batch: 040 ----
mean loss: 857.35
 ---- batch: 050 ----
mean loss: 874.62
 ---- batch: 060 ----
mean loss: 843.14
 ---- batch: 070 ----
mean loss: 873.38
 ---- batch: 080 ----
mean loss: 854.58
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 869.04
 ---- batch: 110 ----
mean loss: 865.73
train mean loss: 858.33
epoch train time: 0:00:01.894780
elapsed time: 0:01:07.170799
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 22:47:01.700622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.16
 ---- batch: 020 ----
mean loss: 867.19
 ---- batch: 030 ----
mean loss: 854.64
 ---- batch: 040 ----
mean loss: 832.56
 ---- batch: 050 ----
mean loss: 849.13
 ---- batch: 060 ----
mean loss: 863.05
 ---- batch: 070 ----
mean loss: 855.72
 ---- batch: 080 ----
mean loss: 859.72
 ---- batch: 090 ----
mean loss: 861.09
 ---- batch: 100 ----
mean loss: 845.49
 ---- batch: 110 ----
mean loss: 875.50
train mean loss: 855.68
epoch train time: 0:00:01.953874
elapsed time: 0:01:09.125269
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 22:47:03.655051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.39
 ---- batch: 020 ----
mean loss: 863.14
 ---- batch: 030 ----
mean loss: 868.11
 ---- batch: 040 ----
mean loss: 847.66
 ---- batch: 050 ----
mean loss: 839.17
 ---- batch: 060 ----
mean loss: 863.86
 ---- batch: 070 ----
mean loss: 854.98
 ---- batch: 080 ----
mean loss: 859.57
 ---- batch: 090 ----
mean loss: 848.39
 ---- batch: 100 ----
mean loss: 857.07
 ---- batch: 110 ----
mean loss: 857.80
train mean loss: 854.81
epoch train time: 0:00:01.964429
elapsed time: 0:01:11.090252
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 22:47:05.620069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 815.00
 ---- batch: 020 ----
mean loss: 889.40
 ---- batch: 030 ----
mean loss: 849.05
 ---- batch: 040 ----
mean loss: 882.01
 ---- batch: 050 ----
mean loss: 854.30
 ---- batch: 060 ----
mean loss: 869.45
 ---- batch: 070 ----
mean loss: 846.06
 ---- batch: 080 ----
mean loss: 869.89
 ---- batch: 090 ----
mean loss: 840.40
 ---- batch: 100 ----
mean loss: 833.28
 ---- batch: 110 ----
mean loss: 833.29
train mean loss: 852.17
epoch train time: 0:00:01.925874
elapsed time: 0:01:13.016702
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 22:47:07.546299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.63
 ---- batch: 020 ----
mean loss: 865.16
 ---- batch: 030 ----
mean loss: 837.65
 ---- batch: 040 ----
mean loss: 868.65
 ---- batch: 050 ----
mean loss: 857.16
 ---- batch: 060 ----
mean loss: 855.17
 ---- batch: 070 ----
mean loss: 864.26
 ---- batch: 080 ----
mean loss: 848.84
 ---- batch: 090 ----
mean loss: 846.42
 ---- batch: 100 ----
mean loss: 836.76
 ---- batch: 110 ----
mean loss: 861.63
train mean loss: 853.44
epoch train time: 0:00:01.933468
elapsed time: 0:01:14.950540
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 22:47:09.480435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.46
 ---- batch: 020 ----
mean loss: 840.14
 ---- batch: 030 ----
mean loss: 826.15
 ---- batch: 040 ----
mean loss: 853.73
 ---- batch: 050 ----
mean loss: 877.77
 ---- batch: 060 ----
mean loss: 846.05
 ---- batch: 070 ----
mean loss: 872.86
 ---- batch: 080 ----
mean loss: 839.25
 ---- batch: 090 ----
mean loss: 854.00
 ---- batch: 100 ----
mean loss: 868.02
 ---- batch: 110 ----
mean loss: 854.90
train mean loss: 851.99
epoch train time: 0:00:01.952252
elapsed time: 0:01:16.903455
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 22:47:11.433219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.32
 ---- batch: 020 ----
mean loss: 847.27
 ---- batch: 030 ----
mean loss: 864.22
 ---- batch: 040 ----
mean loss: 855.45
 ---- batch: 050 ----
mean loss: 875.60
 ---- batch: 060 ----
mean loss: 845.34
 ---- batch: 070 ----
mean loss: 842.98
 ---- batch: 080 ----
mean loss: 864.95
 ---- batch: 090 ----
mean loss: 848.79
 ---- batch: 100 ----
mean loss: 859.51
 ---- batch: 110 ----
mean loss: 837.71
train mean loss: 853.08
epoch train time: 0:00:01.916774
elapsed time: 0:01:18.820758
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 22:47:13.350514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.43
 ---- batch: 020 ----
mean loss: 851.17
 ---- batch: 030 ----
mean loss: 837.92
 ---- batch: 040 ----
mean loss: 858.41
 ---- batch: 050 ----
mean loss: 856.33
 ---- batch: 060 ----
mean loss: 859.97
 ---- batch: 070 ----
mean loss: 825.93
 ---- batch: 080 ----
mean loss: 852.09
 ---- batch: 090 ----
mean loss: 864.33
 ---- batch: 100 ----
mean loss: 844.67
 ---- batch: 110 ----
mean loss: 864.17
train mean loss: 851.65
epoch train time: 0:00:01.924501
elapsed time: 0:01:20.745797
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 22:47:15.275676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.24
 ---- batch: 020 ----
mean loss: 851.08
 ---- batch: 030 ----
mean loss: 848.99
 ---- batch: 040 ----
mean loss: 846.69
 ---- batch: 050 ----
mean loss: 843.67
 ---- batch: 060 ----
mean loss: 857.36
 ---- batch: 070 ----
mean loss: 869.31
 ---- batch: 080 ----
mean loss: 836.15
 ---- batch: 090 ----
mean loss: 858.33
 ---- batch: 100 ----
mean loss: 847.19
 ---- batch: 110 ----
mean loss: 846.66
train mean loss: 850.92
epoch train time: 0:00:01.905591
elapsed time: 0:01:22.652037
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 22:47:17.181834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.41
 ---- batch: 020 ----
mean loss: 862.68
 ---- batch: 030 ----
mean loss: 863.29
 ---- batch: 040 ----
mean loss: 850.61
 ---- batch: 050 ----
mean loss: 844.71
 ---- batch: 060 ----
mean loss: 830.77
 ---- batch: 070 ----
mean loss: 835.92
 ---- batch: 080 ----
mean loss: 866.05
 ---- batch: 090 ----
mean loss: 871.04
 ---- batch: 100 ----
mean loss: 841.27
 ---- batch: 110 ----
mean loss: 841.53
train mean loss: 850.61
epoch train time: 0:00:01.928637
elapsed time: 0:01:24.581251
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 22:47:19.111044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.14
 ---- batch: 020 ----
mean loss: 837.63
 ---- batch: 030 ----
mean loss: 855.48
 ---- batch: 040 ----
mean loss: 873.00
 ---- batch: 050 ----
mean loss: 843.65
 ---- batch: 060 ----
mean loss: 847.85
 ---- batch: 070 ----
mean loss: 838.68
 ---- batch: 080 ----
mean loss: 854.72
 ---- batch: 090 ----
mean loss: 858.21
 ---- batch: 100 ----
mean loss: 841.59
 ---- batch: 110 ----
mean loss: 850.46
train mean loss: 849.54
epoch train time: 0:00:01.902366
elapsed time: 0:01:26.484404
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 22:47:21.014223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 812.91
 ---- batch: 020 ----
mean loss: 844.33
 ---- batch: 030 ----
mean loss: 862.43
 ---- batch: 040 ----
mean loss: 866.77
 ---- batch: 050 ----
mean loss: 864.13
 ---- batch: 060 ----
mean loss: 846.18
 ---- batch: 070 ----
mean loss: 857.83
 ---- batch: 080 ----
mean loss: 850.54
 ---- batch: 090 ----
mean loss: 829.54
 ---- batch: 100 ----
mean loss: 863.94
 ---- batch: 110 ----
mean loss: 840.65
train mean loss: 849.42
epoch train time: 0:00:01.928908
elapsed time: 0:01:28.413904
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 22:47:22.943655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 866.74
 ---- batch: 020 ----
mean loss: 829.18
 ---- batch: 030 ----
mean loss: 838.19
 ---- batch: 040 ----
mean loss: 851.97
 ---- batch: 050 ----
mean loss: 854.42
 ---- batch: 060 ----
mean loss: 848.64
 ---- batch: 070 ----
mean loss: 852.66
 ---- batch: 080 ----
mean loss: 866.95
 ---- batch: 090 ----
mean loss: 847.71
 ---- batch: 100 ----
mean loss: 858.66
 ---- batch: 110 ----
mean loss: 847.48
train mean loss: 850.49
epoch train time: 0:00:01.932398
elapsed time: 0:01:30.346856
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 22:47:24.876658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.73
 ---- batch: 020 ----
mean loss: 844.56
 ---- batch: 030 ----
mean loss: 857.51
 ---- batch: 040 ----
mean loss: 864.79
 ---- batch: 050 ----
mean loss: 846.96
 ---- batch: 060 ----
mean loss: 841.13
 ---- batch: 070 ----
mean loss: 810.91
 ---- batch: 080 ----
mean loss: 857.83
 ---- batch: 090 ----
mean loss: 851.66
 ---- batch: 100 ----
mean loss: 859.88
 ---- batch: 110 ----
mean loss: 860.69
train mean loss: 848.97
epoch train time: 0:00:01.930377
elapsed time: 0:01:32.277840
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 22:47:26.807617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.08
 ---- batch: 020 ----
mean loss: 840.43
 ---- batch: 030 ----
mean loss: 869.05
 ---- batch: 040 ----
mean loss: 872.61
 ---- batch: 050 ----
mean loss: 822.32
 ---- batch: 060 ----
mean loss: 838.90
 ---- batch: 070 ----
mean loss: 859.55
 ---- batch: 080 ----
mean loss: 850.86
 ---- batch: 090 ----
mean loss: 849.82
 ---- batch: 100 ----
mean loss: 850.72
 ---- batch: 110 ----
mean loss: 834.49
train mean loss: 848.97
epoch train time: 0:00:01.938888
elapsed time: 0:01:34.217293
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 22:47:28.747063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.39
 ---- batch: 020 ----
mean loss: 839.01
 ---- batch: 030 ----
mean loss: 859.58
 ---- batch: 040 ----
mean loss: 866.15
 ---- batch: 050 ----
mean loss: 855.02
 ---- batch: 060 ----
mean loss: 861.51
 ---- batch: 070 ----
mean loss: 829.91
 ---- batch: 080 ----
mean loss: 854.07
 ---- batch: 090 ----
mean loss: 844.75
 ---- batch: 100 ----
mean loss: 855.43
 ---- batch: 110 ----
mean loss: 833.91
train mean loss: 847.88
epoch train time: 0:00:01.926944
elapsed time: 0:01:36.144791
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 22:47:30.674566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.48
 ---- batch: 020 ----
mean loss: 850.79
 ---- batch: 030 ----
mean loss: 844.96
 ---- batch: 040 ----
mean loss: 849.04
 ---- batch: 050 ----
mean loss: 830.82
 ---- batch: 060 ----
mean loss: 840.82
 ---- batch: 070 ----
mean loss: 847.99
 ---- batch: 080 ----
mean loss: 866.75
 ---- batch: 090 ----
mean loss: 860.22
 ---- batch: 100 ----
mean loss: 858.13
 ---- batch: 110 ----
mean loss: 860.50
train mean loss: 849.45
epoch train time: 0:00:01.912264
elapsed time: 0:01:38.057595
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 22:47:32.587172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.21
 ---- batch: 020 ----
mean loss: 818.98
 ---- batch: 030 ----
mean loss: 841.50
 ---- batch: 040 ----
mean loss: 855.16
 ---- batch: 050 ----
mean loss: 853.43
 ---- batch: 060 ----
mean loss: 858.76
 ---- batch: 070 ----
mean loss: 839.69
 ---- batch: 080 ----
mean loss: 854.68
 ---- batch: 090 ----
mean loss: 859.17
 ---- batch: 100 ----
mean loss: 854.80
 ---- batch: 110 ----
mean loss: 847.49
train mean loss: 848.28
epoch train time: 0:00:01.908278
elapsed time: 0:01:39.966259
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 22:47:34.496056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.81
 ---- batch: 020 ----
mean loss: 866.15
 ---- batch: 030 ----
mean loss: 839.62
 ---- batch: 040 ----
mean loss: 848.25
 ---- batch: 050 ----
mean loss: 842.77
 ---- batch: 060 ----
mean loss: 838.31
 ---- batch: 070 ----
mean loss: 845.47
 ---- batch: 080 ----
mean loss: 856.07
 ---- batch: 090 ----
mean loss: 836.85
 ---- batch: 100 ----
mean loss: 837.56
 ---- batch: 110 ----
mean loss: 838.03
train mean loss: 847.68
epoch train time: 0:00:01.937000
elapsed time: 0:01:41.903891
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 22:47:36.433832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.42
 ---- batch: 020 ----
mean loss: 847.81
 ---- batch: 030 ----
mean loss: 858.21
 ---- batch: 040 ----
mean loss: 846.26
 ---- batch: 050 ----
mean loss: 829.75
 ---- batch: 060 ----
mean loss: 853.63
 ---- batch: 070 ----
mean loss: 839.10
 ---- batch: 080 ----
mean loss: 830.88
 ---- batch: 090 ----
mean loss: 866.76
 ---- batch: 100 ----
mean loss: 854.68
 ---- batch: 110 ----
mean loss: 830.29
train mean loss: 847.53
epoch train time: 0:00:01.926276
elapsed time: 0:01:43.830872
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 22:47:38.360648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.94
 ---- batch: 020 ----
mean loss: 857.68
 ---- batch: 030 ----
mean loss: 834.23
 ---- batch: 040 ----
mean loss: 842.81
 ---- batch: 050 ----
mean loss: 871.41
 ---- batch: 060 ----
mean loss: 837.82
 ---- batch: 070 ----
mean loss: 843.51
 ---- batch: 080 ----
mean loss: 834.03
 ---- batch: 090 ----
mean loss: 831.95
 ---- batch: 100 ----
mean loss: 858.43
 ---- batch: 110 ----
mean loss: 860.21
train mean loss: 847.70
epoch train time: 0:00:01.888576
elapsed time: 0:01:45.720017
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 22:47:40.249921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.16
 ---- batch: 020 ----
mean loss: 838.77
 ---- batch: 030 ----
mean loss: 857.44
 ---- batch: 040 ----
mean loss: 865.43
 ---- batch: 050 ----
mean loss: 821.40
 ---- batch: 060 ----
mean loss: 835.06
 ---- batch: 070 ----
mean loss: 861.15
 ---- batch: 080 ----
mean loss: 856.48
 ---- batch: 090 ----
mean loss: 828.04
 ---- batch: 100 ----
mean loss: 848.35
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 847.16
epoch train time: 0:00:01.928115
elapsed time: 0:01:47.648819
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 22:47:42.178601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.88
 ---- batch: 020 ----
mean loss: 849.38
 ---- batch: 030 ----
mean loss: 844.84
 ---- batch: 040 ----
mean loss: 848.43
 ---- batch: 050 ----
mean loss: 835.45
 ---- batch: 060 ----
mean loss: 839.95
 ---- batch: 070 ----
mean loss: 843.72
 ---- batch: 080 ----
mean loss: 853.43
 ---- batch: 090 ----
mean loss: 849.01
 ---- batch: 100 ----
mean loss: 856.72
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 847.60
epoch train time: 0:00:01.962303
elapsed time: 0:01:49.611689
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 22:47:44.141468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.03
 ---- batch: 020 ----
mean loss: 858.48
 ---- batch: 030 ----
mean loss: 836.15
 ---- batch: 040 ----
mean loss: 858.48
 ---- batch: 050 ----
mean loss: 858.40
 ---- batch: 060 ----
mean loss: 845.38
 ---- batch: 070 ----
mean loss: 835.05
 ---- batch: 080 ----
mean loss: 848.97
 ---- batch: 090 ----
mean loss: 839.98
 ---- batch: 100 ----
mean loss: 844.85
 ---- batch: 110 ----
mean loss: 848.27
train mean loss: 846.62
epoch train time: 0:00:01.907034
elapsed time: 0:01:51.519299
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 22:47:46.049114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.17
 ---- batch: 020 ----
mean loss: 830.64
 ---- batch: 030 ----
mean loss: 842.77
 ---- batch: 040 ----
mean loss: 854.22
 ---- batch: 050 ----
mean loss: 839.28
 ---- batch: 060 ----
mean loss: 854.18
 ---- batch: 070 ----
mean loss: 847.94
 ---- batch: 080 ----
mean loss: 869.60
 ---- batch: 090 ----
mean loss: 858.23
 ---- batch: 100 ----
mean loss: 849.67
 ---- batch: 110 ----
mean loss: 826.56
train mean loss: 847.96
epoch train time: 0:00:01.943916
elapsed time: 0:01:53.463825
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 22:47:47.993715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.04
 ---- batch: 020 ----
mean loss: 837.97
 ---- batch: 030 ----
mean loss: 847.58
 ---- batch: 040 ----
mean loss: 839.48
 ---- batch: 050 ----
mean loss: 870.44
 ---- batch: 060 ----
mean loss: 832.69
 ---- batch: 070 ----
mean loss: 847.09
 ---- batch: 080 ----
mean loss: 858.35
 ---- batch: 090 ----
mean loss: 843.57
 ---- batch: 100 ----
mean loss: 843.14
 ---- batch: 110 ----
mean loss: 845.19
train mean loss: 846.47
epoch train time: 0:00:01.925033
elapsed time: 0:01:55.389569
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 22:47:49.919390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.80
 ---- batch: 020 ----
mean loss: 849.94
 ---- batch: 030 ----
mean loss: 855.36
 ---- batch: 040 ----
mean loss: 838.85
 ---- batch: 050 ----
mean loss: 848.08
 ---- batch: 060 ----
mean loss: 841.77
 ---- batch: 070 ----
mean loss: 813.87
 ---- batch: 080 ----
mean loss: 861.39
 ---- batch: 090 ----
mean loss: 846.24
 ---- batch: 100 ----
mean loss: 851.12
 ---- batch: 110 ----
mean loss: 853.71
train mean loss: 846.53
epoch train time: 0:00:01.929393
elapsed time: 0:01:57.319581
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 22:47:51.849361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.80
 ---- batch: 020 ----
mean loss: 837.57
 ---- batch: 030 ----
mean loss: 853.95
 ---- batch: 040 ----
mean loss: 858.84
 ---- batch: 050 ----
mean loss: 835.94
 ---- batch: 060 ----
mean loss: 866.59
 ---- batch: 070 ----
mean loss: 854.94
 ---- batch: 080 ----
mean loss: 836.89
 ---- batch: 090 ----
mean loss: 825.25
 ---- batch: 100 ----
mean loss: 833.90
 ---- batch: 110 ----
mean loss: 847.24
train mean loss: 846.51
epoch train time: 0:00:01.916910
elapsed time: 0:01:59.237052
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 22:47:53.766850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.79
 ---- batch: 020 ----
mean loss: 871.73
 ---- batch: 030 ----
mean loss: 861.52
 ---- batch: 040 ----
mean loss: 852.56
 ---- batch: 050 ----
mean loss: 834.80
 ---- batch: 060 ----
mean loss: 844.57
 ---- batch: 070 ----
mean loss: 841.00
 ---- batch: 080 ----
mean loss: 845.62
 ---- batch: 090 ----
mean loss: 842.26
 ---- batch: 100 ----
mean loss: 845.14
 ---- batch: 110 ----
mean loss: 855.07
train mean loss: 846.18
epoch train time: 0:00:01.927492
elapsed time: 0:02:01.165144
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 22:47:55.694955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.83
 ---- batch: 020 ----
mean loss: 827.00
 ---- batch: 030 ----
mean loss: 860.88
 ---- batch: 040 ----
mean loss: 854.11
 ---- batch: 050 ----
mean loss: 831.89
 ---- batch: 060 ----
mean loss: 827.89
 ---- batch: 070 ----
mean loss: 857.97
 ---- batch: 080 ----
mean loss: 820.92
 ---- batch: 090 ----
mean loss: 866.45
 ---- batch: 100 ----
mean loss: 836.82
 ---- batch: 110 ----
mean loss: 848.28
train mean loss: 843.07
epoch train time: 0:00:01.932007
elapsed time: 0:02:03.097762
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 22:47:57.627548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.94
 ---- batch: 020 ----
mean loss: 825.44
 ---- batch: 030 ----
mean loss: 807.43
 ---- batch: 040 ----
mean loss: 797.24
 ---- batch: 050 ----
mean loss: 791.39
 ---- batch: 060 ----
mean loss: 767.27
 ---- batch: 070 ----
mean loss: 744.47
 ---- batch: 080 ----
mean loss: 697.53
 ---- batch: 090 ----
mean loss: 658.89
 ---- batch: 100 ----
mean loss: 591.59
 ---- batch: 110 ----
mean loss: 561.27
train mean loss: 728.48
epoch train time: 0:00:01.948129
elapsed time: 0:02:05.046509
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 22:47:59.576279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.63
 ---- batch: 020 ----
mean loss: 482.26
 ---- batch: 030 ----
mean loss: 475.57
 ---- batch: 040 ----
mean loss: 454.22
 ---- batch: 050 ----
mean loss: 432.62
 ---- batch: 060 ----
mean loss: 445.51
 ---- batch: 070 ----
mean loss: 434.01
 ---- batch: 080 ----
mean loss: 424.05
 ---- batch: 090 ----
mean loss: 406.14
 ---- batch: 100 ----
mean loss: 414.03
 ---- batch: 110 ----
mean loss: 392.48
train mean loss: 442.52
epoch train time: 0:00:01.909482
elapsed time: 0:02:06.956534
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 22:48:01.486377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.18
 ---- batch: 020 ----
mean loss: 394.88
 ---- batch: 030 ----
mean loss: 375.67
 ---- batch: 040 ----
mean loss: 384.80
 ---- batch: 050 ----
mean loss: 377.10
 ---- batch: 060 ----
mean loss: 369.64
 ---- batch: 070 ----
mean loss: 362.37
 ---- batch: 080 ----
mean loss: 379.34
 ---- batch: 090 ----
mean loss: 366.18
 ---- batch: 100 ----
mean loss: 354.88
 ---- batch: 110 ----
mean loss: 346.13
train mean loss: 374.53
epoch train time: 0:00:01.933469
elapsed time: 0:02:08.890652
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 22:48:03.420476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.60
 ---- batch: 020 ----
mean loss: 347.44
 ---- batch: 030 ----
mean loss: 338.43
 ---- batch: 040 ----
mean loss: 356.81
 ---- batch: 050 ----
mean loss: 349.03
 ---- batch: 060 ----
mean loss: 345.32
 ---- batch: 070 ----
mean loss: 350.89
 ---- batch: 080 ----
mean loss: 337.20
 ---- batch: 090 ----
mean loss: 346.99
 ---- batch: 100 ----
mean loss: 335.02
 ---- batch: 110 ----
mean loss: 348.80
train mean loss: 346.56
epoch train time: 0:00:01.926125
elapsed time: 0:02:10.817402
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 22:48:05.347181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.01
 ---- batch: 020 ----
mean loss: 336.75
 ---- batch: 030 ----
mean loss: 347.72
 ---- batch: 040 ----
mean loss: 321.17
 ---- batch: 050 ----
mean loss: 312.79
 ---- batch: 060 ----
mean loss: 315.84
 ---- batch: 070 ----
mean loss: 324.33
 ---- batch: 080 ----
mean loss: 331.81
 ---- batch: 090 ----
mean loss: 315.48
 ---- batch: 100 ----
mean loss: 319.70
 ---- batch: 110 ----
mean loss: 316.07
train mean loss: 324.13
epoch train time: 0:00:01.949659
elapsed time: 0:02:12.767629
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 22:48:07.297406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.67
 ---- batch: 020 ----
mean loss: 315.72
 ---- batch: 030 ----
mean loss: 318.35
 ---- batch: 040 ----
mean loss: 305.45
 ---- batch: 050 ----
mean loss: 306.81
 ---- batch: 060 ----
mean loss: 300.41
 ---- batch: 070 ----
mean loss: 310.94
 ---- batch: 080 ----
mean loss: 306.72
 ---- batch: 090 ----
mean loss: 306.39
 ---- batch: 100 ----
mean loss: 309.35
 ---- batch: 110 ----
mean loss: 320.14
train mean loss: 310.46
epoch train time: 0:00:01.947270
elapsed time: 0:02:14.715500
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 22:48:09.245278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.08
 ---- batch: 020 ----
mean loss: 305.82
 ---- batch: 030 ----
mean loss: 302.75
 ---- batch: 040 ----
mean loss: 296.56
 ---- batch: 050 ----
mean loss: 307.64
 ---- batch: 060 ----
mean loss: 300.02
 ---- batch: 070 ----
mean loss: 295.25
 ---- batch: 080 ----
mean loss: 285.35
 ---- batch: 090 ----
mean loss: 296.67
 ---- batch: 100 ----
mean loss: 290.05
 ---- batch: 110 ----
mean loss: 300.48
train mean loss: 299.28
epoch train time: 0:00:01.950698
elapsed time: 0:02:16.666763
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 22:48:11.196551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.04
 ---- batch: 020 ----
mean loss: 286.96
 ---- batch: 030 ----
mean loss: 304.64
 ---- batch: 040 ----
mean loss: 291.00
 ---- batch: 050 ----
mean loss: 292.84
 ---- batch: 060 ----
mean loss: 282.50
 ---- batch: 070 ----
mean loss: 269.81
 ---- batch: 080 ----
mean loss: 293.48
 ---- batch: 090 ----
mean loss: 284.76
 ---- batch: 100 ----
mean loss: 296.48
 ---- batch: 110 ----
mean loss: 295.81
train mean loss: 290.36
epoch train time: 0:00:01.936574
elapsed time: 0:02:18.603940
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 22:48:13.133789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.70
 ---- batch: 020 ----
mean loss: 283.19
 ---- batch: 030 ----
mean loss: 293.72
 ---- batch: 040 ----
mean loss: 292.57
 ---- batch: 050 ----
mean loss: 285.09
 ---- batch: 060 ----
mean loss: 279.65
 ---- batch: 070 ----
mean loss: 273.48
 ---- batch: 080 ----
mean loss: 275.42
 ---- batch: 090 ----
mean loss: 278.57
 ---- batch: 100 ----
mean loss: 274.74
 ---- batch: 110 ----
mean loss: 284.53
train mean loss: 283.22
epoch train time: 0:00:01.935952
elapsed time: 0:02:20.540508
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 22:48:15.070012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.67
 ---- batch: 020 ----
mean loss: 275.43
 ---- batch: 030 ----
mean loss: 287.72
 ---- batch: 040 ----
mean loss: 278.85
 ---- batch: 050 ----
mean loss: 289.14
 ---- batch: 060 ----
mean loss: 283.25
 ---- batch: 070 ----
mean loss: 275.25
 ---- batch: 080 ----
mean loss: 273.78
 ---- batch: 090 ----
mean loss: 271.41
 ---- batch: 100 ----
mean loss: 268.60
 ---- batch: 110 ----
mean loss: 273.79
train mean loss: 277.97
epoch train time: 0:00:01.923129
elapsed time: 0:02:22.463973
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 22:48:16.993764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.43
 ---- batch: 020 ----
mean loss: 270.95
 ---- batch: 030 ----
mean loss: 266.30
 ---- batch: 040 ----
mean loss: 265.55
 ---- batch: 050 ----
mean loss: 283.84
 ---- batch: 060 ----
mean loss: 268.12
 ---- batch: 070 ----
mean loss: 272.75
 ---- batch: 080 ----
mean loss: 269.26
 ---- batch: 090 ----
mean loss: 265.87
 ---- batch: 100 ----
mean loss: 265.36
 ---- batch: 110 ----
mean loss: 274.77
train mean loss: 270.67
epoch train time: 0:00:01.941120
elapsed time: 0:02:24.405738
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 22:48:18.935591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.71
 ---- batch: 020 ----
mean loss: 276.17
 ---- batch: 030 ----
mean loss: 267.43
 ---- batch: 040 ----
mean loss: 259.37
 ---- batch: 050 ----
mean loss: 256.57
 ---- batch: 060 ----
mean loss: 261.75
 ---- batch: 070 ----
mean loss: 262.91
 ---- batch: 080 ----
mean loss: 271.62
 ---- batch: 090 ----
mean loss: 253.72
 ---- batch: 100 ----
mean loss: 276.82
 ---- batch: 110 ----
mean loss: 262.89
train mean loss: 264.48
epoch train time: 0:00:01.919896
elapsed time: 0:02:26.326362
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 22:48:20.856141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.47
 ---- batch: 020 ----
mean loss: 260.44
 ---- batch: 030 ----
mean loss: 266.92
 ---- batch: 040 ----
mean loss: 259.78
 ---- batch: 050 ----
mean loss: 253.83
 ---- batch: 060 ----
mean loss: 256.80
 ---- batch: 070 ----
mean loss: 250.31
 ---- batch: 080 ----
mean loss: 264.73
 ---- batch: 090 ----
mean loss: 264.95
 ---- batch: 100 ----
mean loss: 265.92
 ---- batch: 110 ----
mean loss: 270.12
train mean loss: 261.50
epoch train time: 0:00:01.972816
elapsed time: 0:02:28.299770
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 22:48:22.829593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.13
 ---- batch: 020 ----
mean loss: 248.19
 ---- batch: 030 ----
mean loss: 259.14
 ---- batch: 040 ----
mean loss: 259.06
 ---- batch: 050 ----
mean loss: 255.87
 ---- batch: 060 ----
mean loss: 253.99
 ---- batch: 070 ----
mean loss: 247.69
 ---- batch: 080 ----
mean loss: 258.47
 ---- batch: 090 ----
mean loss: 258.06
 ---- batch: 100 ----
mean loss: 253.10
 ---- batch: 110 ----
mean loss: 244.73
train mean loss: 254.48
epoch train time: 0:00:01.937279
elapsed time: 0:02:30.237660
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 22:48:24.767439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.13
 ---- batch: 020 ----
mean loss: 257.56
 ---- batch: 030 ----
mean loss: 247.90
 ---- batch: 040 ----
mean loss: 243.81
 ---- batch: 050 ----
mean loss: 254.19
 ---- batch: 060 ----
mean loss: 242.77
 ---- batch: 070 ----
mean loss: 260.31
 ---- batch: 080 ----
mean loss: 242.65
 ---- batch: 090 ----
mean loss: 258.10
 ---- batch: 100 ----
mean loss: 247.10
 ---- batch: 110 ----
mean loss: 259.90
train mean loss: 250.51
epoch train time: 0:00:01.935044
elapsed time: 0:02:32.173269
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 22:48:26.703056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.43
 ---- batch: 020 ----
mean loss: 235.97
 ---- batch: 030 ----
mean loss: 242.58
 ---- batch: 040 ----
mean loss: 250.68
 ---- batch: 050 ----
mean loss: 240.68
 ---- batch: 060 ----
mean loss: 244.86
 ---- batch: 070 ----
mean loss: 233.67
 ---- batch: 080 ----
mean loss: 257.61
 ---- batch: 090 ----
mean loss: 248.79
 ---- batch: 100 ----
mean loss: 245.13
 ---- batch: 110 ----
mean loss: 246.88
train mean loss: 245.43
epoch train time: 0:00:01.932759
elapsed time: 0:02:34.106606
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 22:48:28.636489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.87
 ---- batch: 020 ----
mean loss: 232.14
 ---- batch: 030 ----
mean loss: 238.08
 ---- batch: 040 ----
mean loss: 246.63
 ---- batch: 050 ----
mean loss: 232.63
 ---- batch: 060 ----
mean loss: 241.18
 ---- batch: 070 ----
mean loss: 249.17
 ---- batch: 080 ----
mean loss: 234.56
 ---- batch: 090 ----
mean loss: 241.81
 ---- batch: 100 ----
mean loss: 245.34
 ---- batch: 110 ----
mean loss: 255.97
train mean loss: 242.19
epoch train time: 0:00:01.954184
elapsed time: 0:02:36.061482
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 22:48:30.591299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.71
 ---- batch: 020 ----
mean loss: 240.70
 ---- batch: 030 ----
mean loss: 236.98
 ---- batch: 040 ----
mean loss: 229.22
 ---- batch: 050 ----
mean loss: 234.69
 ---- batch: 060 ----
mean loss: 238.26
 ---- batch: 070 ----
mean loss: 246.04
 ---- batch: 080 ----
mean loss: 247.31
 ---- batch: 090 ----
mean loss: 250.46
 ---- batch: 100 ----
mean loss: 234.45
 ---- batch: 110 ----
mean loss: 237.66
train mean loss: 239.33
epoch train time: 0:00:01.931452
elapsed time: 0:02:37.993564
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 22:48:32.523372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.14
 ---- batch: 020 ----
mean loss: 224.90
 ---- batch: 030 ----
mean loss: 238.23
 ---- batch: 040 ----
mean loss: 239.95
 ---- batch: 050 ----
mean loss: 240.81
 ---- batch: 060 ----
mean loss: 230.52
 ---- batch: 070 ----
mean loss: 237.84
 ---- batch: 080 ----
mean loss: 226.29
 ---- batch: 090 ----
mean loss: 227.29
 ---- batch: 100 ----
mean loss: 226.85
 ---- batch: 110 ----
mean loss: 240.22
train mean loss: 234.25
epoch train time: 0:00:01.947467
elapsed time: 0:02:39.941657
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 22:48:34.471454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.09
 ---- batch: 020 ----
mean loss: 231.72
 ---- batch: 030 ----
mean loss: 225.06
 ---- batch: 040 ----
mean loss: 233.05
 ---- batch: 050 ----
mean loss: 221.96
 ---- batch: 060 ----
mean loss: 233.29
 ---- batch: 070 ----
mean loss: 227.36
 ---- batch: 080 ----
mean loss: 230.87
 ---- batch: 090 ----
mean loss: 228.06
 ---- batch: 100 ----
mean loss: 229.87
 ---- batch: 110 ----
mean loss: 227.23
train mean loss: 229.52
epoch train time: 0:00:01.931834
elapsed time: 0:02:41.874066
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 22:48:36.403854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.01
 ---- batch: 020 ----
mean loss: 224.52
 ---- batch: 030 ----
mean loss: 214.19
 ---- batch: 040 ----
mean loss: 228.92
 ---- batch: 050 ----
mean loss: 225.81
 ---- batch: 060 ----
mean loss: 222.45
 ---- batch: 070 ----
mean loss: 231.06
 ---- batch: 080 ----
mean loss: 223.66
 ---- batch: 090 ----
mean loss: 235.04
 ---- batch: 100 ----
mean loss: 226.00
 ---- batch: 110 ----
mean loss: 237.97
train mean loss: 226.85
epoch train time: 0:00:01.919888
elapsed time: 0:02:43.794546
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 22:48:38.324300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.97
 ---- batch: 020 ----
mean loss: 228.80
 ---- batch: 030 ----
mean loss: 217.60
 ---- batch: 040 ----
mean loss: 223.76
 ---- batch: 050 ----
mean loss: 230.33
 ---- batch: 060 ----
mean loss: 233.67
 ---- batch: 070 ----
mean loss: 232.11
 ---- batch: 080 ----
mean loss: 221.39
 ---- batch: 090 ----
mean loss: 230.58
 ---- batch: 100 ----
mean loss: 216.12
 ---- batch: 110 ----
mean loss: 229.44
train mean loss: 225.69
epoch train time: 0:00:01.907324
elapsed time: 0:02:45.702474
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 22:48:40.232278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.76
 ---- batch: 020 ----
mean loss: 222.05
 ---- batch: 030 ----
mean loss: 222.44
 ---- batch: 040 ----
mean loss: 216.77
 ---- batch: 050 ----
mean loss: 220.60
 ---- batch: 060 ----
mean loss: 227.07
 ---- batch: 070 ----
mean loss: 216.69
 ---- batch: 080 ----
mean loss: 220.67
 ---- batch: 090 ----
mean loss: 219.30
 ---- batch: 100 ----
mean loss: 218.35
 ---- batch: 110 ----
mean loss: 227.59
train mean loss: 222.37
epoch train time: 0:00:01.903333
elapsed time: 0:02:47.606426
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 22:48:42.136215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.42
 ---- batch: 020 ----
mean loss: 218.81
 ---- batch: 030 ----
mean loss: 228.29
 ---- batch: 040 ----
mean loss: 217.53
 ---- batch: 050 ----
mean loss: 219.82
 ---- batch: 060 ----
mean loss: 215.79
 ---- batch: 070 ----
mean loss: 224.29
 ---- batch: 080 ----
mean loss: 213.32
 ---- batch: 090 ----
mean loss: 218.19
 ---- batch: 100 ----
mean loss: 213.17
 ---- batch: 110 ----
mean loss: 220.81
train mean loss: 218.07
epoch train time: 0:00:01.965574
elapsed time: 0:02:49.572566
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 22:48:44.102447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.89
 ---- batch: 020 ----
mean loss: 211.97
 ---- batch: 030 ----
mean loss: 213.45
 ---- batch: 040 ----
mean loss: 215.64
 ---- batch: 050 ----
mean loss: 203.56
 ---- batch: 060 ----
mean loss: 218.65
 ---- batch: 070 ----
mean loss: 221.95
 ---- batch: 080 ----
mean loss: 221.69
 ---- batch: 090 ----
mean loss: 211.92
 ---- batch: 100 ----
mean loss: 210.27
 ---- batch: 110 ----
mean loss: 216.03
train mean loss: 214.74
epoch train time: 0:00:01.927703
elapsed time: 0:02:51.500963
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 22:48:46.030795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.25
 ---- batch: 020 ----
mean loss: 210.27
 ---- batch: 030 ----
mean loss: 208.96
 ---- batch: 040 ----
mean loss: 212.72
 ---- batch: 050 ----
mean loss: 221.56
 ---- batch: 060 ----
mean loss: 211.47
 ---- batch: 070 ----
mean loss: 209.56
 ---- batch: 080 ----
mean loss: 209.00
 ---- batch: 090 ----
mean loss: 210.99
 ---- batch: 100 ----
mean loss: 214.03
 ---- batch: 110 ----
mean loss: 209.78
train mean loss: 211.46
epoch train time: 0:00:01.909283
elapsed time: 0:02:53.410839
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 22:48:47.940652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.73
 ---- batch: 020 ----
mean loss: 211.09
 ---- batch: 030 ----
mean loss: 204.33
 ---- batch: 040 ----
mean loss: 205.64
 ---- batch: 050 ----
mean loss: 205.47
 ---- batch: 060 ----
mean loss: 213.63
 ---- batch: 070 ----
mean loss: 206.59
 ---- batch: 080 ----
mean loss: 215.84
 ---- batch: 090 ----
mean loss: 207.35
 ---- batch: 100 ----
mean loss: 206.68
 ---- batch: 110 ----
mean loss: 204.94
train mean loss: 208.94
epoch train time: 0:00:01.897604
elapsed time: 0:02:55.309053
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 22:48:49.838853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.03
 ---- batch: 020 ----
mean loss: 205.85
 ---- batch: 030 ----
mean loss: 200.72
 ---- batch: 040 ----
mean loss: 204.69
 ---- batch: 050 ----
mean loss: 205.82
 ---- batch: 060 ----
mean loss: 209.94
 ---- batch: 070 ----
mean loss: 208.91
 ---- batch: 080 ----
mean loss: 211.03
 ---- batch: 090 ----
mean loss: 208.99
 ---- batch: 100 ----
mean loss: 203.83
 ---- batch: 110 ----
mean loss: 200.55
train mean loss: 206.51
epoch train time: 0:00:01.929008
elapsed time: 0:02:57.238646
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 22:48:51.768418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.75
 ---- batch: 020 ----
mean loss: 195.49
 ---- batch: 030 ----
mean loss: 211.77
 ---- batch: 040 ----
mean loss: 210.43
 ---- batch: 050 ----
mean loss: 209.26
 ---- batch: 060 ----
mean loss: 198.13
 ---- batch: 070 ----
mean loss: 204.16
 ---- batch: 080 ----
mean loss: 201.81
 ---- batch: 090 ----
mean loss: 208.48
 ---- batch: 100 ----
mean loss: 200.89
 ---- batch: 110 ----
mean loss: 211.09
train mean loss: 204.70
epoch train time: 0:00:01.941819
elapsed time: 0:02:59.181084
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 22:48:53.710886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.99
 ---- batch: 020 ----
mean loss: 199.62
 ---- batch: 030 ----
mean loss: 210.28
 ---- batch: 040 ----
mean loss: 206.35
 ---- batch: 050 ----
mean loss: 205.16
 ---- batch: 060 ----
mean loss: 200.18
 ---- batch: 070 ----
mean loss: 205.46
 ---- batch: 080 ----
mean loss: 202.36
 ---- batch: 090 ----
mean loss: 199.00
 ---- batch: 100 ----
mean loss: 189.67
 ---- batch: 110 ----
mean loss: 203.14
train mean loss: 201.99
epoch train time: 0:00:01.936473
elapsed time: 0:03:01.118132
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 22:48:55.647890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.62
 ---- batch: 020 ----
mean loss: 205.35
 ---- batch: 030 ----
mean loss: 194.75
 ---- batch: 040 ----
mean loss: 204.66
 ---- batch: 050 ----
mean loss: 206.13
 ---- batch: 060 ----
mean loss: 202.88
 ---- batch: 070 ----
mean loss: 196.41
 ---- batch: 080 ----
mean loss: 195.43
 ---- batch: 090 ----
mean loss: 202.78
 ---- batch: 100 ----
mean loss: 203.93
 ---- batch: 110 ----
mean loss: 192.05
train mean loss: 199.82
epoch train time: 0:00:01.905737
elapsed time: 0:03:03.024402
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 22:48:57.554159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.05
 ---- batch: 020 ----
mean loss: 194.55
 ---- batch: 030 ----
mean loss: 191.75
 ---- batch: 040 ----
mean loss: 198.40
 ---- batch: 050 ----
mean loss: 196.88
 ---- batch: 060 ----
mean loss: 207.43
 ---- batch: 070 ----
mean loss: 198.35
 ---- batch: 080 ----
mean loss: 197.99
 ---- batch: 090 ----
mean loss: 193.77
 ---- batch: 100 ----
mean loss: 192.94
 ---- batch: 110 ----
mean loss: 203.53
train mean loss: 197.09
epoch train time: 0:00:01.906146
elapsed time: 0:03:04.931097
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 22:48:59.460905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.74
 ---- batch: 020 ----
mean loss: 191.97
 ---- batch: 030 ----
mean loss: 188.77
 ---- batch: 040 ----
mean loss: 201.44
 ---- batch: 050 ----
mean loss: 192.92
 ---- batch: 060 ----
mean loss: 198.99
 ---- batch: 070 ----
mean loss: 196.50
 ---- batch: 080 ----
mean loss: 188.33
 ---- batch: 090 ----
mean loss: 200.16
 ---- batch: 100 ----
mean loss: 185.48
 ---- batch: 110 ----
mean loss: 204.59
train mean loss: 194.35
epoch train time: 0:00:01.888881
elapsed time: 0:03:06.820577
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 22:49:01.350353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.69
 ---- batch: 020 ----
mean loss: 193.30
 ---- batch: 030 ----
mean loss: 187.47
 ---- batch: 040 ----
mean loss: 185.54
 ---- batch: 050 ----
mean loss: 192.35
 ---- batch: 060 ----
mean loss: 198.55
 ---- batch: 070 ----
mean loss: 197.83
 ---- batch: 080 ----
mean loss: 195.25
 ---- batch: 090 ----
mean loss: 191.18
 ---- batch: 100 ----
mean loss: 195.60
 ---- batch: 110 ----
mean loss: 188.42
train mean loss: 192.32
epoch train time: 0:00:01.951863
elapsed time: 0:03:08.772994
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 22:49:03.302887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.62
 ---- batch: 020 ----
mean loss: 189.84
 ---- batch: 030 ----
mean loss: 187.17
 ---- batch: 040 ----
mean loss: 190.10
 ---- batch: 050 ----
mean loss: 191.63
 ---- batch: 060 ----
mean loss: 180.36
 ---- batch: 070 ----
mean loss: 189.43
 ---- batch: 080 ----
mean loss: 205.58
 ---- batch: 090 ----
mean loss: 196.31
 ---- batch: 100 ----
mean loss: 180.57
 ---- batch: 110 ----
mean loss: 198.33
train mean loss: 190.40
epoch train time: 0:00:01.966342
elapsed time: 0:03:10.739986
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 22:49:05.269773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.22
 ---- batch: 020 ----
mean loss: 192.60
 ---- batch: 030 ----
mean loss: 193.57
 ---- batch: 040 ----
mean loss: 196.69
 ---- batch: 050 ----
mean loss: 183.96
 ---- batch: 060 ----
mean loss: 188.53
 ---- batch: 070 ----
mean loss: 194.92
 ---- batch: 080 ----
mean loss: 186.93
 ---- batch: 090 ----
mean loss: 192.39
 ---- batch: 100 ----
mean loss: 187.46
 ---- batch: 110 ----
mean loss: 187.41
train mean loss: 189.89
epoch train time: 0:00:01.896593
elapsed time: 0:03:12.637129
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 22:49:07.166885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.02
 ---- batch: 020 ----
mean loss: 188.99
 ---- batch: 030 ----
mean loss: 185.12
 ---- batch: 040 ----
mean loss: 179.99
 ---- batch: 050 ----
mean loss: 189.93
 ---- batch: 060 ----
mean loss: 190.19
 ---- batch: 070 ----
mean loss: 186.43
 ---- batch: 080 ----
mean loss: 188.42
 ---- batch: 090 ----
mean loss: 184.96
 ---- batch: 100 ----
mean loss: 193.43
 ---- batch: 110 ----
mean loss: 186.86
train mean loss: 187.27
epoch train time: 0:00:01.910360
elapsed time: 0:03:14.548042
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 22:49:09.077858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.12
 ---- batch: 020 ----
mean loss: 185.37
 ---- batch: 030 ----
mean loss: 179.54
 ---- batch: 040 ----
mean loss: 182.29
 ---- batch: 050 ----
mean loss: 178.94
 ---- batch: 060 ----
mean loss: 182.89
 ---- batch: 070 ----
mean loss: 197.32
 ---- batch: 080 ----
mean loss: 192.84
 ---- batch: 090 ----
mean loss: 185.74
 ---- batch: 100 ----
mean loss: 193.58
 ---- batch: 110 ----
mean loss: 182.53
train mean loss: 185.86
epoch train time: 0:00:01.924681
elapsed time: 0:03:16.473330
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 22:49:11.003099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.17
 ---- batch: 020 ----
mean loss: 188.57
 ---- batch: 030 ----
mean loss: 172.54
 ---- batch: 040 ----
mean loss: 185.60
 ---- batch: 050 ----
mean loss: 192.23
 ---- batch: 060 ----
mean loss: 185.91
 ---- batch: 070 ----
mean loss: 186.62
 ---- batch: 080 ----
mean loss: 190.77
 ---- batch: 090 ----
mean loss: 189.73
 ---- batch: 100 ----
mean loss: 183.76
 ---- batch: 110 ----
mean loss: 181.72
train mean loss: 184.81
epoch train time: 0:00:01.928841
elapsed time: 0:03:18.402716
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 22:49:12.932564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.39
 ---- batch: 020 ----
mean loss: 191.35
 ---- batch: 030 ----
mean loss: 177.71
 ---- batch: 040 ----
mean loss: 177.46
 ---- batch: 050 ----
mean loss: 185.53
 ---- batch: 060 ----
mean loss: 178.55
 ---- batch: 070 ----
mean loss: 182.10
 ---- batch: 080 ----
mean loss: 178.88
 ---- batch: 090 ----
mean loss: 184.59
 ---- batch: 100 ----
mean loss: 185.92
 ---- batch: 110 ----
mean loss: 189.72
train mean loss: 181.94
epoch train time: 0:00:01.909156
elapsed time: 0:03:20.312503
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 22:49:14.842324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.27
 ---- batch: 020 ----
mean loss: 188.94
 ---- batch: 030 ----
mean loss: 178.31
 ---- batch: 040 ----
mean loss: 171.79
 ---- batch: 050 ----
mean loss: 182.76
 ---- batch: 060 ----
mean loss: 179.48
 ---- batch: 070 ----
mean loss: 183.51
 ---- batch: 080 ----
mean loss: 192.59
 ---- batch: 090 ----
mean loss: 179.14
 ---- batch: 100 ----
mean loss: 176.35
 ---- batch: 110 ----
mean loss: 184.40
train mean loss: 181.58
epoch train time: 0:00:01.938289
elapsed time: 0:03:22.251406
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 22:49:16.781200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.41
 ---- batch: 020 ----
mean loss: 176.68
 ---- batch: 030 ----
mean loss: 184.23
 ---- batch: 040 ----
mean loss: 179.56
 ---- batch: 050 ----
mean loss: 187.64
 ---- batch: 060 ----
mean loss: 176.04
 ---- batch: 070 ----
mean loss: 182.13
 ---- batch: 080 ----
mean loss: 180.02
 ---- batch: 090 ----
mean loss: 181.91
 ---- batch: 100 ----
mean loss: 186.51
 ---- batch: 110 ----
mean loss: 176.36
train mean loss: 179.72
epoch train time: 0:00:01.913894
elapsed time: 0:03:24.165939
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 22:49:18.695766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.21
 ---- batch: 020 ----
mean loss: 172.01
 ---- batch: 030 ----
mean loss: 178.84
 ---- batch: 040 ----
mean loss: 183.31
 ---- batch: 050 ----
mean loss: 169.25
 ---- batch: 060 ----
mean loss: 175.95
 ---- batch: 070 ----
mean loss: 183.61
 ---- batch: 080 ----
mean loss: 179.91
 ---- batch: 090 ----
mean loss: 177.00
 ---- batch: 100 ----
mean loss: 177.22
 ---- batch: 110 ----
mean loss: 179.30
train mean loss: 177.38
epoch train time: 0:00:01.957290
elapsed time: 0:03:26.123843
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 22:49:20.653635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.45
 ---- batch: 020 ----
mean loss: 172.59
 ---- batch: 030 ----
mean loss: 168.25
 ---- batch: 040 ----
mean loss: 178.92
 ---- batch: 050 ----
mean loss: 168.46
 ---- batch: 060 ----
mean loss: 188.67
 ---- batch: 070 ----
mean loss: 187.12
 ---- batch: 080 ----
mean loss: 181.48
 ---- batch: 090 ----
mean loss: 175.36
 ---- batch: 100 ----
mean loss: 176.70
 ---- batch: 110 ----
mean loss: 179.35
train mean loss: 177.88
epoch train time: 0:00:01.914310
elapsed time: 0:03:28.038700
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 22:49:22.568456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.90
 ---- batch: 020 ----
mean loss: 180.47
 ---- batch: 030 ----
mean loss: 178.92
 ---- batch: 040 ----
mean loss: 168.52
 ---- batch: 050 ----
mean loss: 174.45
 ---- batch: 060 ----
mean loss: 175.88
 ---- batch: 070 ----
mean loss: 175.60
 ---- batch: 080 ----
mean loss: 171.84
 ---- batch: 090 ----
mean loss: 178.08
 ---- batch: 100 ----
mean loss: 179.38
 ---- batch: 110 ----
mean loss: 180.01
train mean loss: 175.85
epoch train time: 0:00:01.936655
elapsed time: 0:03:29.975939
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 22:49:24.505746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.83
 ---- batch: 020 ----
mean loss: 176.27
 ---- batch: 030 ----
mean loss: 170.90
 ---- batch: 040 ----
mean loss: 177.18
 ---- batch: 050 ----
mean loss: 166.92
 ---- batch: 060 ----
mean loss: 174.72
 ---- batch: 070 ----
mean loss: 180.09
 ---- batch: 080 ----
mean loss: 177.31
 ---- batch: 090 ----
mean loss: 164.45
 ---- batch: 100 ----
mean loss: 174.93
 ---- batch: 110 ----
mean loss: 183.34
train mean loss: 174.33
epoch train time: 0:00:01.942903
elapsed time: 0:03:31.919428
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 22:49:26.449251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.63
 ---- batch: 020 ----
mean loss: 176.94
 ---- batch: 030 ----
mean loss: 170.05
 ---- batch: 040 ----
mean loss: 178.76
 ---- batch: 050 ----
mean loss: 169.78
 ---- batch: 060 ----
mean loss: 164.37
 ---- batch: 070 ----
mean loss: 176.50
 ---- batch: 080 ----
mean loss: 166.14
 ---- batch: 090 ----
mean loss: 175.21
 ---- batch: 100 ----
mean loss: 178.08
 ---- batch: 110 ----
mean loss: 177.04
train mean loss: 173.60
epoch train time: 0:00:01.933930
elapsed time: 0:03:33.853953
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 22:49:28.383728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.91
 ---- batch: 020 ----
mean loss: 177.57
 ---- batch: 030 ----
mean loss: 171.43
 ---- batch: 040 ----
mean loss: 167.95
 ---- batch: 050 ----
mean loss: 175.61
 ---- batch: 060 ----
mean loss: 170.71
 ---- batch: 070 ----
mean loss: 179.73
 ---- batch: 080 ----
mean loss: 178.74
 ---- batch: 090 ----
mean loss: 166.03
 ---- batch: 100 ----
mean loss: 172.11
 ---- batch: 110 ----
mean loss: 166.53
train mean loss: 172.02
epoch train time: 0:00:01.910385
elapsed time: 0:03:35.764908
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 22:49:30.294689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.26
 ---- batch: 020 ----
mean loss: 168.33
 ---- batch: 030 ----
mean loss: 173.86
 ---- batch: 040 ----
mean loss: 168.20
 ---- batch: 050 ----
mean loss: 174.61
 ---- batch: 060 ----
mean loss: 168.06
 ---- batch: 070 ----
mean loss: 172.38
 ---- batch: 080 ----
mean loss: 169.85
 ---- batch: 090 ----
mean loss: 169.95
 ---- batch: 100 ----
mean loss: 172.46
 ---- batch: 110 ----
mean loss: 169.99
train mean loss: 170.35
epoch train time: 0:00:01.915254
elapsed time: 0:03:37.680711
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 22:49:32.210501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.71
 ---- batch: 020 ----
mean loss: 174.00
 ---- batch: 030 ----
mean loss: 166.88
 ---- batch: 040 ----
mean loss: 176.21
 ---- batch: 050 ----
mean loss: 175.37
 ---- batch: 060 ----
mean loss: 164.59
 ---- batch: 070 ----
mean loss: 159.40
 ---- batch: 080 ----
mean loss: 160.89
 ---- batch: 090 ----
mean loss: 172.77
 ---- batch: 100 ----
mean loss: 174.03
 ---- batch: 110 ----
mean loss: 169.88
train mean loss: 169.73
epoch train time: 0:00:01.926193
elapsed time: 0:03:39.607462
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 22:49:34.137258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.68
 ---- batch: 020 ----
mean loss: 160.19
 ---- batch: 030 ----
mean loss: 152.56
 ---- batch: 040 ----
mean loss: 172.47
 ---- batch: 050 ----
mean loss: 179.21
 ---- batch: 060 ----
mean loss: 175.33
 ---- batch: 070 ----
mean loss: 173.36
 ---- batch: 080 ----
mean loss: 170.74
 ---- batch: 090 ----
mean loss: 163.98
 ---- batch: 100 ----
mean loss: 168.86
 ---- batch: 110 ----
mean loss: 175.78
train mean loss: 169.24
epoch train time: 0:00:01.911919
elapsed time: 0:03:41.519956
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 22:49:36.049463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.97
 ---- batch: 020 ----
mean loss: 160.69
 ---- batch: 030 ----
mean loss: 162.61
 ---- batch: 040 ----
mean loss: 162.50
 ---- batch: 050 ----
mean loss: 176.40
 ---- batch: 060 ----
mean loss: 163.75
 ---- batch: 070 ----
mean loss: 164.26
 ---- batch: 080 ----
mean loss: 176.85
 ---- batch: 090 ----
mean loss: 178.77
 ---- batch: 100 ----
mean loss: 175.21
 ---- batch: 110 ----
mean loss: 167.78
train mean loss: 168.09
epoch train time: 0:00:01.921318
elapsed time: 0:03:43.441588
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 22:49:37.971343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.07
 ---- batch: 020 ----
mean loss: 168.59
 ---- batch: 030 ----
mean loss: 164.23
 ---- batch: 040 ----
mean loss: 163.82
 ---- batch: 050 ----
mean loss: 170.57
 ---- batch: 060 ----
mean loss: 165.92
 ---- batch: 070 ----
mean loss: 167.59
 ---- batch: 080 ----
mean loss: 175.41
 ---- batch: 090 ----
mean loss: 167.38
 ---- batch: 100 ----
mean loss: 167.75
 ---- batch: 110 ----
mean loss: 165.86
train mean loss: 166.97
epoch train time: 0:00:01.901393
elapsed time: 0:03:45.343516
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 22:49:39.873315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.56
 ---- batch: 020 ----
mean loss: 171.65
 ---- batch: 030 ----
mean loss: 157.18
 ---- batch: 040 ----
mean loss: 167.62
 ---- batch: 050 ----
mean loss: 162.96
 ---- batch: 060 ----
mean loss: 165.99
 ---- batch: 070 ----
mean loss: 163.21
 ---- batch: 080 ----
mean loss: 161.12
 ---- batch: 090 ----
mean loss: 163.76
 ---- batch: 100 ----
mean loss: 172.60
 ---- batch: 110 ----
mean loss: 176.38
train mean loss: 166.80
epoch train time: 0:00:01.928560
elapsed time: 0:03:47.272676
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 22:49:41.802511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.07
 ---- batch: 020 ----
mean loss: 159.67
 ---- batch: 030 ----
mean loss: 165.87
 ---- batch: 040 ----
mean loss: 164.22
 ---- batch: 050 ----
mean loss: 164.58
 ---- batch: 060 ----
mean loss: 171.81
 ---- batch: 070 ----
mean loss: 164.10
 ---- batch: 080 ----
mean loss: 165.65
 ---- batch: 090 ----
mean loss: 164.27
 ---- batch: 100 ----
mean loss: 172.83
 ---- batch: 110 ----
mean loss: 165.78
train mean loss: 164.94
epoch train time: 0:00:01.936643
elapsed time: 0:03:49.209944
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 22:49:43.739719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.57
 ---- batch: 020 ----
mean loss: 163.03
 ---- batch: 030 ----
mean loss: 162.70
 ---- batch: 040 ----
mean loss: 157.47
 ---- batch: 050 ----
mean loss: 167.69
 ---- batch: 060 ----
mean loss: 157.63
 ---- batch: 070 ----
mean loss: 166.40
 ---- batch: 080 ----
mean loss: 167.59
 ---- batch: 090 ----
mean loss: 167.24
 ---- batch: 100 ----
mean loss: 158.10
 ---- batch: 110 ----
mean loss: 169.74
train mean loss: 164.00
epoch train time: 0:00:01.929633
elapsed time: 0:03:51.140153
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 22:49:45.670047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.59
 ---- batch: 020 ----
mean loss: 166.12
 ---- batch: 030 ----
mean loss: 165.66
 ---- batch: 040 ----
mean loss: 162.98
 ---- batch: 050 ----
mean loss: 161.37
 ---- batch: 060 ----
mean loss: 164.81
 ---- batch: 070 ----
mean loss: 158.88
 ---- batch: 080 ----
mean loss: 168.42
 ---- batch: 090 ----
mean loss: 160.46
 ---- batch: 100 ----
mean loss: 162.60
 ---- batch: 110 ----
mean loss: 162.30
train mean loss: 163.72
epoch train time: 0:00:01.920053
elapsed time: 0:03:53.060967
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 22:49:47.590781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.37
 ---- batch: 020 ----
mean loss: 158.44
 ---- batch: 030 ----
mean loss: 160.22
 ---- batch: 040 ----
mean loss: 163.95
 ---- batch: 050 ----
mean loss: 157.90
 ---- batch: 060 ----
mean loss: 167.74
 ---- batch: 070 ----
mean loss: 158.78
 ---- batch: 080 ----
mean loss: 159.87
 ---- batch: 090 ----
mean loss: 161.20
 ---- batch: 100 ----
mean loss: 163.70
 ---- batch: 110 ----
mean loss: 165.69
train mean loss: 162.05
epoch train time: 0:00:01.941082
elapsed time: 0:03:55.002688
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 22:49:49.532567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.01
 ---- batch: 020 ----
mean loss: 164.96
 ---- batch: 030 ----
mean loss: 166.10
 ---- batch: 040 ----
mean loss: 158.20
 ---- batch: 050 ----
mean loss: 164.25
 ---- batch: 060 ----
mean loss: 160.56
 ---- batch: 070 ----
mean loss: 161.24
 ---- batch: 080 ----
mean loss: 157.58
 ---- batch: 090 ----
mean loss: 160.71
 ---- batch: 100 ----
mean loss: 167.50
 ---- batch: 110 ----
mean loss: 171.45
train mean loss: 162.16
epoch train time: 0:00:01.954776
elapsed time: 0:03:56.958114
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 22:49:51.487898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.32
 ---- batch: 020 ----
mean loss: 160.00
 ---- batch: 030 ----
mean loss: 163.71
 ---- batch: 040 ----
mean loss: 156.28
 ---- batch: 050 ----
mean loss: 156.43
 ---- batch: 060 ----
mean loss: 163.66
 ---- batch: 070 ----
mean loss: 161.16
 ---- batch: 080 ----
mean loss: 155.00
 ---- batch: 090 ----
mean loss: 158.13
 ---- batch: 100 ----
mean loss: 165.72
 ---- batch: 110 ----
mean loss: 163.95
train mean loss: 160.86
epoch train time: 0:00:01.939552
elapsed time: 0:03:58.898431
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 22:49:53.428001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.57
 ---- batch: 020 ----
mean loss: 168.32
 ---- batch: 030 ----
mean loss: 153.58
 ---- batch: 040 ----
mean loss: 155.15
 ---- batch: 050 ----
mean loss: 161.10
 ---- batch: 060 ----
mean loss: 157.11
 ---- batch: 070 ----
mean loss: 153.19
 ---- batch: 080 ----
mean loss: 168.47
 ---- batch: 090 ----
mean loss: 166.26
 ---- batch: 100 ----
mean loss: 159.21
 ---- batch: 110 ----
mean loss: 165.93
train mean loss: 160.32
epoch train time: 0:00:01.950823
elapsed time: 0:04:00.849621
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 22:49:55.379441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.45
 ---- batch: 020 ----
mean loss: 163.91
 ---- batch: 030 ----
mean loss: 157.54
 ---- batch: 040 ----
mean loss: 153.64
 ---- batch: 050 ----
mean loss: 163.51
 ---- batch: 060 ----
mean loss: 159.33
 ---- batch: 070 ----
mean loss: 172.27
 ---- batch: 080 ----
mean loss: 164.51
 ---- batch: 090 ----
mean loss: 161.94
 ---- batch: 100 ----
mean loss: 147.88
 ---- batch: 110 ----
mean loss: 159.36
train mean loss: 160.00
epoch train time: 0:00:01.915118
elapsed time: 0:04:02.765339
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 22:49:57.295146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.45
 ---- batch: 020 ----
mean loss: 163.15
 ---- batch: 030 ----
mean loss: 151.76
 ---- batch: 040 ----
mean loss: 156.35
 ---- batch: 050 ----
mean loss: 164.67
 ---- batch: 060 ----
mean loss: 158.97
 ---- batch: 070 ----
mean loss: 164.15
 ---- batch: 080 ----
mean loss: 158.03
 ---- batch: 090 ----
mean loss: 162.43
 ---- batch: 100 ----
mean loss: 165.01
 ---- batch: 110 ----
mean loss: 158.72
train mean loss: 159.85
epoch train time: 0:00:01.950551
elapsed time: 0:04:04.716469
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 22:49:59.246237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.79
 ---- batch: 020 ----
mean loss: 150.13
 ---- batch: 030 ----
mean loss: 158.41
 ---- batch: 040 ----
mean loss: 151.96
 ---- batch: 050 ----
mean loss: 160.64
 ---- batch: 060 ----
mean loss: 158.99
 ---- batch: 070 ----
mean loss: 168.34
 ---- batch: 080 ----
mean loss: 163.07
 ---- batch: 090 ----
mean loss: 156.08
 ---- batch: 100 ----
mean loss: 156.89
 ---- batch: 110 ----
mean loss: 161.85
train mean loss: 157.73
epoch train time: 0:00:01.908059
elapsed time: 0:04:06.625080
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 22:50:01.154863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.71
 ---- batch: 020 ----
mean loss: 156.73
 ---- batch: 030 ----
mean loss: 153.91
 ---- batch: 040 ----
mean loss: 148.79
 ---- batch: 050 ----
mean loss: 161.93
 ---- batch: 060 ----
mean loss: 156.20
 ---- batch: 070 ----
mean loss: 158.94
 ---- batch: 080 ----
mean loss: 166.73
 ---- batch: 090 ----
mean loss: 152.92
 ---- batch: 100 ----
mean loss: 157.56
 ---- batch: 110 ----
mean loss: 159.97
train mean loss: 157.73
epoch train time: 0:00:01.925774
elapsed time: 0:04:08.551417
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 22:50:03.081209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.16
 ---- batch: 020 ----
mean loss: 150.83
 ---- batch: 030 ----
mean loss: 152.02
 ---- batch: 040 ----
mean loss: 158.03
 ---- batch: 050 ----
mean loss: 154.21
 ---- batch: 060 ----
mean loss: 162.48
 ---- batch: 070 ----
mean loss: 161.71
 ---- batch: 080 ----
mean loss: 167.19
 ---- batch: 090 ----
mean loss: 160.68
 ---- batch: 100 ----
mean loss: 147.68
 ---- batch: 110 ----
mean loss: 157.75
train mean loss: 157.19
epoch train time: 0:00:01.936882
elapsed time: 0:04:10.488886
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 22:50:05.018724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.88
 ---- batch: 020 ----
mean loss: 151.79
 ---- batch: 030 ----
mean loss: 153.95
 ---- batch: 040 ----
mean loss: 147.24
 ---- batch: 050 ----
mean loss: 158.21
 ---- batch: 060 ----
mean loss: 161.06
 ---- batch: 070 ----
mean loss: 147.83
 ---- batch: 080 ----
mean loss: 163.68
 ---- batch: 090 ----
mean loss: 158.19
 ---- batch: 100 ----
mean loss: 158.30
 ---- batch: 110 ----
mean loss: 166.52
train mean loss: 157.00
epoch train time: 0:00:01.940622
elapsed time: 0:04:12.430114
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 22:50:06.959885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.90
 ---- batch: 020 ----
mean loss: 160.79
 ---- batch: 030 ----
mean loss: 151.81
 ---- batch: 040 ----
mean loss: 156.87
 ---- batch: 050 ----
mean loss: 159.81
 ---- batch: 060 ----
mean loss: 169.37
 ---- batch: 070 ----
mean loss: 151.58
 ---- batch: 080 ----
mean loss: 149.94
 ---- batch: 090 ----
mean loss: 153.10
 ---- batch: 100 ----
mean loss: 158.15
 ---- batch: 110 ----
mean loss: 154.11
train mean loss: 155.93
epoch train time: 0:00:01.951437
elapsed time: 0:04:14.382176
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 22:50:08.912044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.23
 ---- batch: 020 ----
mean loss: 161.99
 ---- batch: 030 ----
mean loss: 154.86
 ---- batch: 040 ----
mean loss: 152.46
 ---- batch: 050 ----
mean loss: 158.23
 ---- batch: 060 ----
mean loss: 155.33
 ---- batch: 070 ----
mean loss: 154.83
 ---- batch: 080 ----
mean loss: 161.37
 ---- batch: 090 ----
mean loss: 153.91
 ---- batch: 100 ----
mean loss: 146.86
 ---- batch: 110 ----
mean loss: 149.15
train mean loss: 155.31
epoch train time: 0:00:01.953096
elapsed time: 0:04:16.335962
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 22:50:10.865810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.48
 ---- batch: 020 ----
mean loss: 152.48
 ---- batch: 030 ----
mean loss: 150.11
 ---- batch: 040 ----
mean loss: 153.90
 ---- batch: 050 ----
mean loss: 153.57
 ---- batch: 060 ----
mean loss: 159.32
 ---- batch: 070 ----
mean loss: 163.77
 ---- batch: 080 ----
mean loss: 163.48
 ---- batch: 090 ----
mean loss: 149.28
 ---- batch: 100 ----
mean loss: 161.81
 ---- batch: 110 ----
mean loss: 150.49
train mean loss: 155.27
epoch train time: 0:00:01.965623
elapsed time: 0:04:18.302289
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 22:50:12.832078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.75
 ---- batch: 020 ----
mean loss: 153.81
 ---- batch: 030 ----
mean loss: 155.13
 ---- batch: 040 ----
mean loss: 149.66
 ---- batch: 050 ----
mean loss: 149.08
 ---- batch: 060 ----
mean loss: 157.34
 ---- batch: 070 ----
mean loss: 155.57
 ---- batch: 080 ----
mean loss: 156.61
 ---- batch: 090 ----
mean loss: 160.52
 ---- batch: 100 ----
mean loss: 160.38
 ---- batch: 110 ----
mean loss: 153.21
train mean loss: 155.44
epoch train time: 0:00:02.016693
elapsed time: 0:04:20.319608
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 22:50:14.849395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.24
 ---- batch: 020 ----
mean loss: 147.34
 ---- batch: 030 ----
mean loss: 152.57
 ---- batch: 040 ----
mean loss: 151.84
 ---- batch: 050 ----
mean loss: 150.82
 ---- batch: 060 ----
mean loss: 153.71
 ---- batch: 070 ----
mean loss: 157.40
 ---- batch: 080 ----
mean loss: 156.79
 ---- batch: 090 ----
mean loss: 155.68
 ---- batch: 100 ----
mean loss: 155.18
 ---- batch: 110 ----
mean loss: 155.98
train mean loss: 153.08
epoch train time: 0:00:01.982716
elapsed time: 0:04:22.302922
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 22:50:16.832813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.35
 ---- batch: 020 ----
mean loss: 150.81
 ---- batch: 030 ----
mean loss: 146.76
 ---- batch: 040 ----
mean loss: 152.25
 ---- batch: 050 ----
mean loss: 154.48
 ---- batch: 060 ----
mean loss: 158.22
 ---- batch: 070 ----
mean loss: 151.82
 ---- batch: 080 ----
mean loss: 155.26
 ---- batch: 090 ----
mean loss: 158.50
 ---- batch: 100 ----
mean loss: 152.74
 ---- batch: 110 ----
mean loss: 154.12
train mean loss: 152.81
epoch train time: 0:00:02.003843
elapsed time: 0:04:24.307523
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 22:50:18.837333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.21
 ---- batch: 020 ----
mean loss: 141.84
 ---- batch: 030 ----
mean loss: 160.09
 ---- batch: 040 ----
mean loss: 152.56
 ---- batch: 050 ----
mean loss: 149.10
 ---- batch: 060 ----
mean loss: 150.66
 ---- batch: 070 ----
mean loss: 158.92
 ---- batch: 080 ----
mean loss: 144.55
 ---- batch: 090 ----
mean loss: 160.49
 ---- batch: 100 ----
mean loss: 150.68
 ---- batch: 110 ----
mean loss: 154.26
train mean loss: 152.50
epoch train time: 0:00:01.911462
elapsed time: 0:04:26.219619
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 22:50:20.749370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.50
 ---- batch: 020 ----
mean loss: 147.94
 ---- batch: 030 ----
mean loss: 150.13
 ---- batch: 040 ----
mean loss: 149.44
 ---- batch: 050 ----
mean loss: 143.11
 ---- batch: 060 ----
mean loss: 149.83
 ---- batch: 070 ----
mean loss: 158.35
 ---- batch: 080 ----
mean loss: 154.92
 ---- batch: 090 ----
mean loss: 149.80
 ---- batch: 100 ----
mean loss: 158.64
 ---- batch: 110 ----
mean loss: 155.94
train mean loss: 151.89
epoch train time: 0:00:01.976365
elapsed time: 0:04:28.196552
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 22:50:22.726331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.78
 ---- batch: 020 ----
mean loss: 156.47
 ---- batch: 030 ----
mean loss: 143.70
 ---- batch: 040 ----
mean loss: 149.29
 ---- batch: 050 ----
mean loss: 154.31
 ---- batch: 060 ----
mean loss: 153.09
 ---- batch: 070 ----
mean loss: 154.05
 ---- batch: 080 ----
mean loss: 156.00
 ---- batch: 090 ----
mean loss: 150.81
 ---- batch: 100 ----
mean loss: 153.00
 ---- batch: 110 ----
mean loss: 143.75
train mean loss: 151.29
epoch train time: 0:00:01.914881
elapsed time: 0:04:30.111985
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 22:50:24.641782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.33
 ---- batch: 020 ----
mean loss: 141.79
 ---- batch: 030 ----
mean loss: 147.48
 ---- batch: 040 ----
mean loss: 152.93
 ---- batch: 050 ----
mean loss: 148.39
 ---- batch: 060 ----
mean loss: 149.08
 ---- batch: 070 ----
mean loss: 150.35
 ---- batch: 080 ----
mean loss: 157.44
 ---- batch: 090 ----
mean loss: 161.31
 ---- batch: 100 ----
mean loss: 152.78
 ---- batch: 110 ----
mean loss: 144.28
train mean loss: 150.47
epoch train time: 0:00:01.935687
elapsed time: 0:04:32.048262
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 22:50:26.578057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.11
 ---- batch: 020 ----
mean loss: 142.72
 ---- batch: 030 ----
mean loss: 148.44
 ---- batch: 040 ----
mean loss: 150.61
 ---- batch: 050 ----
mean loss: 155.77
 ---- batch: 060 ----
mean loss: 159.48
 ---- batch: 070 ----
mean loss: 160.57
 ---- batch: 080 ----
mean loss: 150.18
 ---- batch: 090 ----
mean loss: 149.77
 ---- batch: 100 ----
mean loss: 147.65
 ---- batch: 110 ----
mean loss: 145.67
train mean loss: 150.87
epoch train time: 0:00:01.914810
elapsed time: 0:04:33.963633
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 22:50:28.493403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.94
 ---- batch: 020 ----
mean loss: 143.26
 ---- batch: 030 ----
mean loss: 149.74
 ---- batch: 040 ----
mean loss: 152.82
 ---- batch: 050 ----
mean loss: 140.74
 ---- batch: 060 ----
mean loss: 153.12
 ---- batch: 070 ----
mean loss: 156.07
 ---- batch: 080 ----
mean loss: 155.20
 ---- batch: 090 ----
mean loss: 148.85
 ---- batch: 100 ----
mean loss: 149.27
 ---- batch: 110 ----
mean loss: 158.33
train mean loss: 150.23
epoch train time: 0:00:01.923705
elapsed time: 0:04:35.887879
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 22:50:30.417745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.71
 ---- batch: 020 ----
mean loss: 144.53
 ---- batch: 030 ----
mean loss: 146.94
 ---- batch: 040 ----
mean loss: 152.28
 ---- batch: 050 ----
mean loss: 152.71
 ---- batch: 060 ----
mean loss: 145.72
 ---- batch: 070 ----
mean loss: 145.58
 ---- batch: 080 ----
mean loss: 156.66
 ---- batch: 090 ----
mean loss: 150.58
 ---- batch: 100 ----
mean loss: 150.07
 ---- batch: 110 ----
mean loss: 148.42
train mean loss: 149.74
epoch train time: 0:00:01.892976
elapsed time: 0:04:37.781784
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 22:50:32.311344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.89
 ---- batch: 020 ----
mean loss: 147.04
 ---- batch: 030 ----
mean loss: 146.88
 ---- batch: 040 ----
mean loss: 132.36
 ---- batch: 050 ----
mean loss: 162.23
 ---- batch: 060 ----
mean loss: 150.96
 ---- batch: 070 ----
mean loss: 147.85
 ---- batch: 080 ----
mean loss: 149.79
 ---- batch: 090 ----
mean loss: 151.08
 ---- batch: 100 ----
mean loss: 144.28
 ---- batch: 110 ----
mean loss: 151.46
train mean loss: 148.54
epoch train time: 0:00:01.910596
elapsed time: 0:04:39.692723
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 22:50:34.222482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.17
 ---- batch: 020 ----
mean loss: 137.25
 ---- batch: 030 ----
mean loss: 148.21
 ---- batch: 040 ----
mean loss: 153.17
 ---- batch: 050 ----
mean loss: 151.36
 ---- batch: 060 ----
mean loss: 147.32
 ---- batch: 070 ----
mean loss: 153.30
 ---- batch: 080 ----
mean loss: 151.89
 ---- batch: 090 ----
mean loss: 142.17
 ---- batch: 100 ----
mean loss: 157.28
 ---- batch: 110 ----
mean loss: 138.10
train mean loss: 148.16
epoch train time: 0:00:01.941443
elapsed time: 0:04:41.634691
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 22:50:36.164524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.60
 ---- batch: 020 ----
mean loss: 151.28
 ---- batch: 030 ----
mean loss: 145.23
 ---- batch: 040 ----
mean loss: 149.50
 ---- batch: 050 ----
mean loss: 150.18
 ---- batch: 060 ----
mean loss: 144.87
 ---- batch: 070 ----
mean loss: 142.76
 ---- batch: 080 ----
mean loss: 147.72
 ---- batch: 090 ----
mean loss: 146.41
 ---- batch: 100 ----
mean loss: 148.70
 ---- batch: 110 ----
mean loss: 152.97
train mean loss: 147.58
epoch train time: 0:00:01.918604
elapsed time: 0:04:43.553891
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 22:50:38.083657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.34
 ---- batch: 020 ----
mean loss: 144.53
 ---- batch: 030 ----
mean loss: 146.39
 ---- batch: 040 ----
mean loss: 154.64
 ---- batch: 050 ----
mean loss: 140.90
 ---- batch: 060 ----
mean loss: 141.68
 ---- batch: 070 ----
mean loss: 157.27
 ---- batch: 080 ----
mean loss: 153.75
 ---- batch: 090 ----
mean loss: 151.27
 ---- batch: 100 ----
mean loss: 142.96
 ---- batch: 110 ----
mean loss: 146.83
train mean loss: 147.72
epoch train time: 0:00:01.914893
elapsed time: 0:04:45.469333
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 22:50:39.999125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.55
 ---- batch: 020 ----
mean loss: 146.43
 ---- batch: 030 ----
mean loss: 148.91
 ---- batch: 040 ----
mean loss: 149.96
 ---- batch: 050 ----
mean loss: 143.12
 ---- batch: 060 ----
mean loss: 142.87
 ---- batch: 070 ----
mean loss: 148.49
 ---- batch: 080 ----
mean loss: 141.52
 ---- batch: 090 ----
mean loss: 145.71
 ---- batch: 100 ----
mean loss: 150.01
 ---- batch: 110 ----
mean loss: 152.62
train mean loss: 146.92
epoch train time: 0:00:01.934228
elapsed time: 0:04:47.404106
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 22:50:41.933875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.74
 ---- batch: 020 ----
mean loss: 146.66
 ---- batch: 030 ----
mean loss: 144.37
 ---- batch: 040 ----
mean loss: 146.00
 ---- batch: 050 ----
mean loss: 145.28
 ---- batch: 060 ----
mean loss: 151.07
 ---- batch: 070 ----
mean loss: 151.18
 ---- batch: 080 ----
mean loss: 140.16
 ---- batch: 090 ----
mean loss: 143.46
 ---- batch: 100 ----
mean loss: 145.07
 ---- batch: 110 ----
mean loss: 144.15
train mean loss: 145.86
epoch train time: 0:00:01.907132
elapsed time: 0:04:49.311788
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 22:50:43.841580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.74
 ---- batch: 020 ----
mean loss: 143.33
 ---- batch: 030 ----
mean loss: 141.29
 ---- batch: 040 ----
mean loss: 138.78
 ---- batch: 050 ----
mean loss: 145.93
 ---- batch: 060 ----
mean loss: 137.66
 ---- batch: 070 ----
mean loss: 150.78
 ---- batch: 080 ----
mean loss: 143.00
 ---- batch: 090 ----
mean loss: 154.24
 ---- batch: 100 ----
mean loss: 141.88
 ---- batch: 110 ----
mean loss: 152.29
train mean loss: 145.80
epoch train time: 0:00:01.929958
elapsed time: 0:04:51.242309
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 22:50:45.772097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.81
 ---- batch: 020 ----
mean loss: 147.03
 ---- batch: 030 ----
mean loss: 150.57
 ---- batch: 040 ----
mean loss: 143.28
 ---- batch: 050 ----
mean loss: 143.40
 ---- batch: 060 ----
mean loss: 149.47
 ---- batch: 070 ----
mean loss: 140.73
 ---- batch: 080 ----
mean loss: 141.67
 ---- batch: 090 ----
mean loss: 142.77
 ---- batch: 100 ----
mean loss: 150.33
 ---- batch: 110 ----
mean loss: 145.55
train mean loss: 145.27
epoch train time: 0:00:01.901215
elapsed time: 0:04:53.144114
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 22:50:47.673960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.87
 ---- batch: 020 ----
mean loss: 146.76
 ---- batch: 030 ----
mean loss: 150.74
 ---- batch: 040 ----
mean loss: 151.11
 ---- batch: 050 ----
mean loss: 142.16
 ---- batch: 060 ----
mean loss: 145.41
 ---- batch: 070 ----
mean loss: 143.89
 ---- batch: 080 ----
mean loss: 142.46
 ---- batch: 090 ----
mean loss: 145.44
 ---- batch: 100 ----
mean loss: 136.38
 ---- batch: 110 ----
mean loss: 148.88
train mean loss: 145.39
epoch train time: 0:00:01.942904
elapsed time: 0:04:55.087641
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 22:50:49.617411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.74
 ---- batch: 020 ----
mean loss: 141.37
 ---- batch: 030 ----
mean loss: 144.60
 ---- batch: 040 ----
mean loss: 146.23
 ---- batch: 050 ----
mean loss: 136.69
 ---- batch: 060 ----
mean loss: 140.71
 ---- batch: 070 ----
mean loss: 149.68
 ---- batch: 080 ----
mean loss: 145.67
 ---- batch: 090 ----
mean loss: 146.38
 ---- batch: 100 ----
mean loss: 141.86
 ---- batch: 110 ----
mean loss: 141.41
train mean loss: 144.62
epoch train time: 0:00:01.911534
elapsed time: 0:04:56.999814
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 22:50:51.529735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.99
 ---- batch: 020 ----
mean loss: 139.90
 ---- batch: 030 ----
mean loss: 141.83
 ---- batch: 040 ----
mean loss: 144.30
 ---- batch: 050 ----
mean loss: 143.64
 ---- batch: 060 ----
mean loss: 145.49
 ---- batch: 070 ----
mean loss: 137.23
 ---- batch: 080 ----
mean loss: 146.30
 ---- batch: 090 ----
mean loss: 137.97
 ---- batch: 100 ----
mean loss: 154.75
 ---- batch: 110 ----
mean loss: 155.27
train mean loss: 143.66
epoch train time: 0:00:01.933889
elapsed time: 0:04:58.934395
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 22:50:53.464160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.87
 ---- batch: 020 ----
mean loss: 145.50
 ---- batch: 030 ----
mean loss: 134.87
 ---- batch: 040 ----
mean loss: 154.74
 ---- batch: 050 ----
mean loss: 136.87
 ---- batch: 060 ----
mean loss: 146.44
 ---- batch: 070 ----
mean loss: 140.43
 ---- batch: 080 ----
mean loss: 143.77
 ---- batch: 090 ----
mean loss: 133.62
 ---- batch: 100 ----
mean loss: 151.90
 ---- batch: 110 ----
mean loss: 138.49
train mean loss: 143.33
epoch train time: 0:00:01.924995
elapsed time: 0:05:00.859955
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 22:50:55.389799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.07
 ---- batch: 020 ----
mean loss: 143.83
 ---- batch: 030 ----
mean loss: 147.66
 ---- batch: 040 ----
mean loss: 149.04
 ---- batch: 050 ----
mean loss: 148.10
 ---- batch: 060 ----
mean loss: 150.18
 ---- batch: 070 ----
mean loss: 140.79
 ---- batch: 080 ----
mean loss: 139.88
 ---- batch: 090 ----
mean loss: 143.65
 ---- batch: 100 ----
mean loss: 138.28
 ---- batch: 110 ----
mean loss: 146.86
train mean loss: 143.92
epoch train time: 0:00:01.983970
elapsed time: 0:05:02.844555
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 22:50:57.374384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.58
 ---- batch: 020 ----
mean loss: 140.45
 ---- batch: 030 ----
mean loss: 147.23
 ---- batch: 040 ----
mean loss: 144.87
 ---- batch: 050 ----
mean loss: 145.33
 ---- batch: 060 ----
mean loss: 137.99
 ---- batch: 070 ----
mean loss: 143.83
 ---- batch: 080 ----
mean loss: 145.89
 ---- batch: 090 ----
mean loss: 155.09
 ---- batch: 100 ----
mean loss: 139.23
 ---- batch: 110 ----
mean loss: 140.50
train mean loss: 143.26
epoch train time: 0:00:01.967672
elapsed time: 0:05:04.812886
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 22:50:59.342665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.78
 ---- batch: 020 ----
mean loss: 144.68
 ---- batch: 030 ----
mean loss: 141.00
 ---- batch: 040 ----
mean loss: 138.84
 ---- batch: 050 ----
mean loss: 148.44
 ---- batch: 060 ----
mean loss: 144.09
 ---- batch: 070 ----
mean loss: 138.49
 ---- batch: 080 ----
mean loss: 144.79
 ---- batch: 090 ----
mean loss: 142.07
 ---- batch: 100 ----
mean loss: 141.40
 ---- batch: 110 ----
mean loss: 141.26
train mean loss: 142.67
epoch train time: 0:00:01.958003
elapsed time: 0:05:06.771464
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 22:51:01.301301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.12
 ---- batch: 020 ----
mean loss: 144.26
 ---- batch: 030 ----
mean loss: 139.29
 ---- batch: 040 ----
mean loss: 146.55
 ---- batch: 050 ----
mean loss: 134.75
 ---- batch: 060 ----
mean loss: 144.11
 ---- batch: 070 ----
mean loss: 141.25
 ---- batch: 080 ----
mean loss: 139.74
 ---- batch: 090 ----
mean loss: 138.63
 ---- batch: 100 ----
mean loss: 149.02
 ---- batch: 110 ----
mean loss: 150.16
train mean loss: 142.03
epoch train time: 0:00:01.973141
elapsed time: 0:05:08.745213
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 22:51:03.275004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.79
 ---- batch: 020 ----
mean loss: 135.73
 ---- batch: 030 ----
mean loss: 136.08
 ---- batch: 040 ----
mean loss: 144.36
 ---- batch: 050 ----
mean loss: 147.57
 ---- batch: 060 ----
mean loss: 136.88
 ---- batch: 070 ----
mean loss: 143.69
 ---- batch: 080 ----
mean loss: 145.66
 ---- batch: 090 ----
mean loss: 149.34
 ---- batch: 100 ----
mean loss: 141.64
 ---- batch: 110 ----
mean loss: 133.49
train mean loss: 141.50
epoch train time: 0:00:01.955500
elapsed time: 0:05:10.701299
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 22:51:05.231090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.96
 ---- batch: 020 ----
mean loss: 135.33
 ---- batch: 030 ----
mean loss: 143.08
 ---- batch: 040 ----
mean loss: 143.06
 ---- batch: 050 ----
mean loss: 140.57
 ---- batch: 060 ----
mean loss: 139.74
 ---- batch: 070 ----
mean loss: 135.03
 ---- batch: 080 ----
mean loss: 140.25
 ---- batch: 090 ----
mean loss: 144.09
 ---- batch: 100 ----
mean loss: 145.47
 ---- batch: 110 ----
mean loss: 143.39
train mean loss: 140.81
epoch train time: 0:00:01.936330
elapsed time: 0:05:12.638197
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 22:51:07.168005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.20
 ---- batch: 020 ----
mean loss: 143.40
 ---- batch: 030 ----
mean loss: 144.06
 ---- batch: 040 ----
mean loss: 142.07
 ---- batch: 050 ----
mean loss: 141.65
 ---- batch: 060 ----
mean loss: 144.68
 ---- batch: 070 ----
mean loss: 139.15
 ---- batch: 080 ----
mean loss: 141.52
 ---- batch: 090 ----
mean loss: 142.97
 ---- batch: 100 ----
mean loss: 139.76
 ---- batch: 110 ----
mean loss: 136.35
train mean loss: 140.50
epoch train time: 0:00:01.937954
elapsed time: 0:05:14.576723
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 22:51:09.106534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.24
 ---- batch: 020 ----
mean loss: 138.27
 ---- batch: 030 ----
mean loss: 135.47
 ---- batch: 040 ----
mean loss: 138.23
 ---- batch: 050 ----
mean loss: 137.57
 ---- batch: 060 ----
mean loss: 141.41
 ---- batch: 070 ----
mean loss: 141.57
 ---- batch: 080 ----
mean loss: 140.34
 ---- batch: 090 ----
mean loss: 139.45
 ---- batch: 100 ----
mean loss: 139.09
 ---- batch: 110 ----
mean loss: 143.12
train mean loss: 139.59
epoch train time: 0:00:01.915697
elapsed time: 0:05:16.492991
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 22:51:11.022744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.57
 ---- batch: 020 ----
mean loss: 139.27
 ---- batch: 030 ----
mean loss: 133.09
 ---- batch: 040 ----
mean loss: 137.46
 ---- batch: 050 ----
mean loss: 142.94
 ---- batch: 060 ----
mean loss: 137.52
 ---- batch: 070 ----
mean loss: 140.03
 ---- batch: 080 ----
mean loss: 144.33
 ---- batch: 090 ----
mean loss: 145.87
 ---- batch: 100 ----
mean loss: 145.97
 ---- batch: 110 ----
mean loss: 133.61
train mean loss: 139.91
epoch train time: 0:00:01.912545
elapsed time: 0:05:18.406078
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 22:51:12.935851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.53
 ---- batch: 020 ----
mean loss: 140.32
 ---- batch: 030 ----
mean loss: 140.81
 ---- batch: 040 ----
mean loss: 132.00
 ---- batch: 050 ----
mean loss: 137.14
 ---- batch: 060 ----
mean loss: 135.17
 ---- batch: 070 ----
mean loss: 141.49
 ---- batch: 080 ----
mean loss: 145.82
 ---- batch: 090 ----
mean loss: 143.93
 ---- batch: 100 ----
mean loss: 144.54
 ---- batch: 110 ----
mean loss: 135.62
train mean loss: 139.06
epoch train time: 0:00:01.921033
elapsed time: 0:05:20.327923
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 22:51:14.857482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.66
 ---- batch: 020 ----
mean loss: 136.65
 ---- batch: 030 ----
mean loss: 130.71
 ---- batch: 040 ----
mean loss: 129.42
 ---- batch: 050 ----
mean loss: 131.77
 ---- batch: 060 ----
mean loss: 139.69
 ---- batch: 070 ----
mean loss: 142.62
 ---- batch: 080 ----
mean loss: 143.22
 ---- batch: 090 ----
mean loss: 145.64
 ---- batch: 100 ----
mean loss: 146.14
 ---- batch: 110 ----
mean loss: 138.05
train mean loss: 139.17
epoch train time: 0:00:01.955997
elapsed time: 0:05:22.284287
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 22:51:16.814146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.05
 ---- batch: 020 ----
mean loss: 130.91
 ---- batch: 030 ----
mean loss: 138.88
 ---- batch: 040 ----
mean loss: 135.75
 ---- batch: 050 ----
mean loss: 138.81
 ---- batch: 060 ----
mean loss: 143.71
 ---- batch: 070 ----
mean loss: 138.27
 ---- batch: 080 ----
mean loss: 146.00
 ---- batch: 090 ----
mean loss: 143.66
 ---- batch: 100 ----
mean loss: 136.05
 ---- batch: 110 ----
mean loss: 138.51
train mean loss: 138.87
epoch train time: 0:00:01.941813
elapsed time: 0:05:24.226756
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 22:51:18.756255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.64
 ---- batch: 020 ----
mean loss: 137.22
 ---- batch: 030 ----
mean loss: 138.41
 ---- batch: 040 ----
mean loss: 135.93
 ---- batch: 050 ----
mean loss: 131.85
 ---- batch: 060 ----
mean loss: 140.86
 ---- batch: 070 ----
mean loss: 143.88
 ---- batch: 080 ----
mean loss: 140.65
 ---- batch: 090 ----
mean loss: 141.27
 ---- batch: 100 ----
mean loss: 142.74
 ---- batch: 110 ----
mean loss: 136.36
train mean loss: 138.56
epoch train time: 0:00:01.942424
elapsed time: 0:05:26.169524
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 22:51:20.699322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.36
 ---- batch: 020 ----
mean loss: 133.35
 ---- batch: 030 ----
mean loss: 133.10
 ---- batch: 040 ----
mean loss: 130.17
 ---- batch: 050 ----
mean loss: 133.75
 ---- batch: 060 ----
mean loss: 133.58
 ---- batch: 070 ----
mean loss: 135.66
 ---- batch: 080 ----
mean loss: 146.25
 ---- batch: 090 ----
mean loss: 143.27
 ---- batch: 100 ----
mean loss: 143.53
 ---- batch: 110 ----
mean loss: 138.65
train mean loss: 137.94
epoch train time: 0:00:01.922050
elapsed time: 0:05:28.092168
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 22:51:22.621979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.18
 ---- batch: 020 ----
mean loss: 127.75
 ---- batch: 030 ----
mean loss: 132.10
 ---- batch: 040 ----
mean loss: 135.18
 ---- batch: 050 ----
mean loss: 137.00
 ---- batch: 060 ----
mean loss: 144.83
 ---- batch: 070 ----
mean loss: 144.26
 ---- batch: 080 ----
mean loss: 136.62
 ---- batch: 090 ----
mean loss: 136.49
 ---- batch: 100 ----
mean loss: 138.38
 ---- batch: 110 ----
mean loss: 140.26
train mean loss: 137.63
epoch train time: 0:00:01.935369
elapsed time: 0:05:30.028141
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 22:51:24.557986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.99
 ---- batch: 020 ----
mean loss: 132.18
 ---- batch: 030 ----
mean loss: 135.75
 ---- batch: 040 ----
mean loss: 133.43
 ---- batch: 050 ----
mean loss: 135.54
 ---- batch: 060 ----
mean loss: 140.12
 ---- batch: 070 ----
mean loss: 133.49
 ---- batch: 080 ----
mean loss: 134.70
 ---- batch: 090 ----
mean loss: 143.18
 ---- batch: 100 ----
mean loss: 140.26
 ---- batch: 110 ----
mean loss: 141.98
train mean loss: 136.81
epoch train time: 0:00:01.920908
elapsed time: 0:05:31.949674
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 22:51:26.479442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.03
 ---- batch: 020 ----
mean loss: 131.70
 ---- batch: 030 ----
mean loss: 138.59
 ---- batch: 040 ----
mean loss: 135.56
 ---- batch: 050 ----
mean loss: 134.32
 ---- batch: 060 ----
mean loss: 133.25
 ---- batch: 070 ----
mean loss: 142.23
 ---- batch: 080 ----
mean loss: 132.05
 ---- batch: 090 ----
mean loss: 142.50
 ---- batch: 100 ----
mean loss: 140.63
 ---- batch: 110 ----
mean loss: 141.86
train mean loss: 137.03
epoch train time: 0:00:01.959043
elapsed time: 0:05:33.909277
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 22:51:28.439092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.65
 ---- batch: 020 ----
mean loss: 132.66
 ---- batch: 030 ----
mean loss: 143.49
 ---- batch: 040 ----
mean loss: 143.58
 ---- batch: 050 ----
mean loss: 133.17
 ---- batch: 060 ----
mean loss: 133.98
 ---- batch: 070 ----
mean loss: 137.89
 ---- batch: 080 ----
mean loss: 141.76
 ---- batch: 090 ----
mean loss: 130.69
 ---- batch: 100 ----
mean loss: 142.30
 ---- batch: 110 ----
mean loss: 136.34
train mean loss: 137.20
epoch train time: 0:00:01.940673
elapsed time: 0:05:35.850530
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 22:51:30.380325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.26
 ---- batch: 020 ----
mean loss: 135.86
 ---- batch: 030 ----
mean loss: 134.36
 ---- batch: 040 ----
mean loss: 132.77
 ---- batch: 050 ----
mean loss: 130.90
 ---- batch: 060 ----
mean loss: 139.44
 ---- batch: 070 ----
mean loss: 126.34
 ---- batch: 080 ----
mean loss: 136.24
 ---- batch: 090 ----
mean loss: 143.38
 ---- batch: 100 ----
mean loss: 137.16
 ---- batch: 110 ----
mean loss: 146.18
train mean loss: 135.79
epoch train time: 0:00:01.956573
elapsed time: 0:05:37.807688
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 22:51:32.337485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.18
 ---- batch: 020 ----
mean loss: 132.79
 ---- batch: 030 ----
mean loss: 137.40
 ---- batch: 040 ----
mean loss: 132.33
 ---- batch: 050 ----
mean loss: 133.44
 ---- batch: 060 ----
mean loss: 130.63
 ---- batch: 070 ----
mean loss: 141.49
 ---- batch: 080 ----
mean loss: 134.86
 ---- batch: 090 ----
mean loss: 123.81
 ---- batch: 100 ----
mean loss: 140.97
 ---- batch: 110 ----
mean loss: 141.30
train mean loss: 135.38
epoch train time: 0:00:01.949087
elapsed time: 0:05:39.757400
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 22:51:34.287218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.87
 ---- batch: 020 ----
mean loss: 133.22
 ---- batch: 030 ----
mean loss: 130.06
 ---- batch: 040 ----
mean loss: 140.93
 ---- batch: 050 ----
mean loss: 137.79
 ---- batch: 060 ----
mean loss: 129.48
 ---- batch: 070 ----
mean loss: 133.40
 ---- batch: 080 ----
mean loss: 133.32
 ---- batch: 090 ----
mean loss: 134.67
 ---- batch: 100 ----
mean loss: 143.34
 ---- batch: 110 ----
mean loss: 134.08
train mean loss: 135.16
epoch train time: 0:00:01.941694
elapsed time: 0:05:41.699752
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 22:51:36.229535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.54
 ---- batch: 020 ----
mean loss: 133.99
 ---- batch: 030 ----
mean loss: 131.24
 ---- batch: 040 ----
mean loss: 140.11
 ---- batch: 050 ----
mean loss: 143.81
 ---- batch: 060 ----
mean loss: 133.05
 ---- batch: 070 ----
mean loss: 124.77
 ---- batch: 080 ----
mean loss: 142.98
 ---- batch: 090 ----
mean loss: 139.84
 ---- batch: 100 ----
mean loss: 128.12
 ---- batch: 110 ----
mean loss: 130.84
train mean loss: 134.72
epoch train time: 0:00:01.945435
elapsed time: 0:05:43.645744
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 22:51:38.175615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.02
 ---- batch: 020 ----
mean loss: 135.16
 ---- batch: 030 ----
mean loss: 129.69
 ---- batch: 040 ----
mean loss: 136.28
 ---- batch: 050 ----
mean loss: 141.46
 ---- batch: 060 ----
mean loss: 136.42
 ---- batch: 070 ----
mean loss: 132.11
 ---- batch: 080 ----
mean loss: 132.95
 ---- batch: 090 ----
mean loss: 133.63
 ---- batch: 100 ----
mean loss: 140.97
 ---- batch: 110 ----
mean loss: 134.27
train mean loss: 134.21
epoch train time: 0:00:01.950895
elapsed time: 0:05:45.597347
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 22:51:40.127162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.84
 ---- batch: 020 ----
mean loss: 137.49
 ---- batch: 030 ----
mean loss: 135.58
 ---- batch: 040 ----
mean loss: 127.54
 ---- batch: 050 ----
mean loss: 141.78
 ---- batch: 060 ----
mean loss: 135.78
 ---- batch: 070 ----
mean loss: 134.61
 ---- batch: 080 ----
mean loss: 121.71
 ---- batch: 090 ----
mean loss: 128.29
 ---- batch: 100 ----
mean loss: 141.72
 ---- batch: 110 ----
mean loss: 139.35
train mean loss: 134.23
epoch train time: 0:00:01.909497
elapsed time: 0:05:47.507450
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 22:51:42.037279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.52
 ---- batch: 020 ----
mean loss: 134.83
 ---- batch: 030 ----
mean loss: 133.83
 ---- batch: 040 ----
mean loss: 133.45
 ---- batch: 050 ----
mean loss: 135.16
 ---- batch: 060 ----
mean loss: 133.53
 ---- batch: 070 ----
mean loss: 132.37
 ---- batch: 080 ----
mean loss: 136.71
 ---- batch: 090 ----
mean loss: 134.06
 ---- batch: 100 ----
mean loss: 127.92
 ---- batch: 110 ----
mean loss: 132.73
train mean loss: 133.71
epoch train time: 0:00:01.921772
elapsed time: 0:05:49.429840
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 22:51:43.959633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.14
 ---- batch: 020 ----
mean loss: 139.21
 ---- batch: 030 ----
mean loss: 131.36
 ---- batch: 040 ----
mean loss: 130.25
 ---- batch: 050 ----
mean loss: 129.57
 ---- batch: 060 ----
mean loss: 135.02
 ---- batch: 070 ----
mean loss: 139.58
 ---- batch: 080 ----
mean loss: 134.80
 ---- batch: 090 ----
mean loss: 129.04
 ---- batch: 100 ----
mean loss: 135.57
 ---- batch: 110 ----
mean loss: 133.91
train mean loss: 133.54
epoch train time: 0:00:01.931177
elapsed time: 0:05:51.361603
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 22:51:45.891423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.77
 ---- batch: 020 ----
mean loss: 127.65
 ---- batch: 030 ----
mean loss: 134.66
 ---- batch: 040 ----
mean loss: 136.58
 ---- batch: 050 ----
mean loss: 143.87
 ---- batch: 060 ----
mean loss: 124.50
 ---- batch: 070 ----
mean loss: 133.74
 ---- batch: 080 ----
mean loss: 135.39
 ---- batch: 090 ----
mean loss: 139.98
 ---- batch: 100 ----
mean loss: 130.77
 ---- batch: 110 ----
mean loss: 122.50
train mean loss: 133.13
epoch train time: 0:00:01.944859
elapsed time: 0:05:53.307221
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 22:51:47.837167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.88
 ---- batch: 020 ----
mean loss: 134.35
 ---- batch: 030 ----
mean loss: 131.97
 ---- batch: 040 ----
mean loss: 130.19
 ---- batch: 050 ----
mean loss: 127.32
 ---- batch: 060 ----
mean loss: 134.68
 ---- batch: 070 ----
mean loss: 124.83
 ---- batch: 080 ----
mean loss: 142.02
 ---- batch: 090 ----
mean loss: 133.69
 ---- batch: 100 ----
mean loss: 149.56
 ---- batch: 110 ----
mean loss: 128.78
train mean loss: 133.38
epoch train time: 0:00:01.984176
elapsed time: 0:05:55.292130
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 22:51:49.821959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.04
 ---- batch: 020 ----
mean loss: 127.71
 ---- batch: 030 ----
mean loss: 129.27
 ---- batch: 040 ----
mean loss: 126.20
 ---- batch: 050 ----
mean loss: 128.78
 ---- batch: 060 ----
mean loss: 126.72
 ---- batch: 070 ----
mean loss: 133.40
 ---- batch: 080 ----
mean loss: 130.27
 ---- batch: 090 ----
mean loss: 140.55
 ---- batch: 100 ----
mean loss: 139.60
 ---- batch: 110 ----
mean loss: 141.30
train mean loss: 132.18
epoch train time: 0:00:01.915599
elapsed time: 0:05:57.208439
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 22:51:51.738225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.14
 ---- batch: 020 ----
mean loss: 134.49
 ---- batch: 030 ----
mean loss: 126.75
 ---- batch: 040 ----
mean loss: 135.98
 ---- batch: 050 ----
mean loss: 139.73
 ---- batch: 060 ----
mean loss: 124.67
 ---- batch: 070 ----
mean loss: 123.94
 ---- batch: 080 ----
mean loss: 133.82
 ---- batch: 090 ----
mean loss: 136.74
 ---- batch: 100 ----
mean loss: 130.17
 ---- batch: 110 ----
mean loss: 132.21
train mean loss: 131.42
epoch train time: 0:00:01.960399
elapsed time: 0:05:59.169457
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 22:51:53.699258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.82
 ---- batch: 020 ----
mean loss: 133.76
 ---- batch: 030 ----
mean loss: 119.85
 ---- batch: 040 ----
mean loss: 138.08
 ---- batch: 050 ----
mean loss: 133.41
 ---- batch: 060 ----
mean loss: 133.60
 ---- batch: 070 ----
mean loss: 131.17
 ---- batch: 080 ----
mean loss: 134.64
 ---- batch: 090 ----
mean loss: 135.03
 ---- batch: 100 ----
mean loss: 139.31
 ---- batch: 110 ----
mean loss: 123.45
train mean loss: 131.60
epoch train time: 0:00:01.949274
elapsed time: 0:06:01.119365
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 22:51:55.649173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.46
 ---- batch: 020 ----
mean loss: 127.62
 ---- batch: 030 ----
mean loss: 127.43
 ---- batch: 040 ----
mean loss: 129.28
 ---- batch: 050 ----
mean loss: 134.29
 ---- batch: 060 ----
mean loss: 135.55
 ---- batch: 070 ----
mean loss: 126.78
 ---- batch: 080 ----
mean loss: 133.12
 ---- batch: 090 ----
mean loss: 139.57
 ---- batch: 100 ----
mean loss: 128.49
 ---- batch: 110 ----
mean loss: 127.61
train mean loss: 131.36
epoch train time: 0:00:01.923356
elapsed time: 0:06:03.043368
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 22:51:57.573189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.78
 ---- batch: 020 ----
mean loss: 135.85
 ---- batch: 030 ----
mean loss: 134.02
 ---- batch: 040 ----
mean loss: 132.43
 ---- batch: 050 ----
mean loss: 130.25
 ---- batch: 060 ----
mean loss: 123.28
 ---- batch: 070 ----
mean loss: 136.86
 ---- batch: 080 ----
mean loss: 129.35
 ---- batch: 090 ----
mean loss: 134.35
 ---- batch: 100 ----
mean loss: 133.37
 ---- batch: 110 ----
mean loss: 127.74
train mean loss: 131.10
epoch train time: 0:00:01.963278
elapsed time: 0:06:05.007317
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 22:51:59.537137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.29
 ---- batch: 020 ----
mean loss: 133.10
 ---- batch: 030 ----
mean loss: 129.06
 ---- batch: 040 ----
mean loss: 124.56
 ---- batch: 050 ----
mean loss: 128.13
 ---- batch: 060 ----
mean loss: 132.01
 ---- batch: 070 ----
mean loss: 130.23
 ---- batch: 080 ----
mean loss: 129.95
 ---- batch: 090 ----
mean loss: 134.84
 ---- batch: 100 ----
mean loss: 133.28
 ---- batch: 110 ----
mean loss: 136.13
train mean loss: 130.77
epoch train time: 0:00:01.937580
elapsed time: 0:06:06.945494
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 22:52:01.475326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.87
 ---- batch: 020 ----
mean loss: 127.43
 ---- batch: 030 ----
mean loss: 132.61
 ---- batch: 040 ----
mean loss: 131.45
 ---- batch: 050 ----
mean loss: 127.25
 ---- batch: 060 ----
mean loss: 123.79
 ---- batch: 070 ----
mean loss: 132.97
 ---- batch: 080 ----
mean loss: 124.91
 ---- batch: 090 ----
mean loss: 138.66
 ---- batch: 100 ----
mean loss: 122.12
 ---- batch: 110 ----
mean loss: 138.93
train mean loss: 130.20
epoch train time: 0:00:01.933220
elapsed time: 0:06:08.879574
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 22:52:03.409098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.81
 ---- batch: 020 ----
mean loss: 129.09
 ---- batch: 030 ----
mean loss: 129.55
 ---- batch: 040 ----
mean loss: 129.04
 ---- batch: 050 ----
mean loss: 133.22
 ---- batch: 060 ----
mean loss: 128.73
 ---- batch: 070 ----
mean loss: 129.73
 ---- batch: 080 ----
mean loss: 128.55
 ---- batch: 090 ----
mean loss: 139.48
 ---- batch: 100 ----
mean loss: 133.83
 ---- batch: 110 ----
mean loss: 132.69
train mean loss: 130.37
epoch train time: 0:00:01.929194
elapsed time: 0:06:10.809076
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 22:52:05.338852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.57
 ---- batch: 020 ----
mean loss: 126.86
 ---- batch: 030 ----
mean loss: 128.29
 ---- batch: 040 ----
mean loss: 128.77
 ---- batch: 050 ----
mean loss: 126.80
 ---- batch: 060 ----
mean loss: 132.99
 ---- batch: 070 ----
mean loss: 130.75
 ---- batch: 080 ----
mean loss: 124.96
 ---- batch: 090 ----
mean loss: 129.46
 ---- batch: 100 ----
mean loss: 132.90
 ---- batch: 110 ----
mean loss: 127.36
train mean loss: 129.64
epoch train time: 0:00:01.970487
elapsed time: 0:06:12.780164
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 22:52:07.309970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.78
 ---- batch: 020 ----
mean loss: 132.44
 ---- batch: 030 ----
mean loss: 119.76
 ---- batch: 040 ----
mean loss: 140.63
 ---- batch: 050 ----
mean loss: 131.63
 ---- batch: 060 ----
mean loss: 128.70
 ---- batch: 070 ----
mean loss: 126.35
 ---- batch: 080 ----
mean loss: 130.33
 ---- batch: 090 ----
mean loss: 124.87
 ---- batch: 100 ----
mean loss: 129.52
 ---- batch: 110 ----
mean loss: 132.29
train mean loss: 129.46
epoch train time: 0:00:01.913490
elapsed time: 0:06:14.694227
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 22:52:09.223999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.82
 ---- batch: 020 ----
mean loss: 131.96
 ---- batch: 030 ----
mean loss: 131.30
 ---- batch: 040 ----
mean loss: 125.35
 ---- batch: 050 ----
mean loss: 128.08
 ---- batch: 060 ----
mean loss: 125.75
 ---- batch: 070 ----
mean loss: 125.64
 ---- batch: 080 ----
mean loss: 130.07
 ---- batch: 090 ----
mean loss: 133.74
 ---- batch: 100 ----
mean loss: 133.07
 ---- batch: 110 ----
mean loss: 127.11
train mean loss: 128.98
epoch train time: 0:00:01.911753
elapsed time: 0:06:16.606538
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 22:52:11.136328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.23
 ---- batch: 020 ----
mean loss: 134.47
 ---- batch: 030 ----
mean loss: 132.11
 ---- batch: 040 ----
mean loss: 121.57
 ---- batch: 050 ----
mean loss: 122.99
 ---- batch: 060 ----
mean loss: 130.78
 ---- batch: 070 ----
mean loss: 126.05
 ---- batch: 080 ----
mean loss: 130.16
 ---- batch: 090 ----
mean loss: 130.31
 ---- batch: 100 ----
mean loss: 123.84
 ---- batch: 110 ----
mean loss: 133.23
train mean loss: 128.66
epoch train time: 0:00:01.945077
elapsed time: 0:06:18.552231
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 22:52:13.082099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.82
 ---- batch: 020 ----
mean loss: 124.09
 ---- batch: 030 ----
mean loss: 127.70
 ---- batch: 040 ----
mean loss: 134.87
 ---- batch: 050 ----
mean loss: 127.67
 ---- batch: 060 ----
mean loss: 126.13
 ---- batch: 070 ----
mean loss: 133.71
 ---- batch: 080 ----
mean loss: 131.37
 ---- batch: 090 ----
mean loss: 126.07
 ---- batch: 100 ----
mean loss: 125.98
 ---- batch: 110 ----
mean loss: 135.54
train mean loss: 128.53
epoch train time: 0:00:01.920009
elapsed time: 0:06:20.472893
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 22:52:15.002671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.41
 ---- batch: 020 ----
mean loss: 127.72
 ---- batch: 030 ----
mean loss: 125.71
 ---- batch: 040 ----
mean loss: 122.98
 ---- batch: 050 ----
mean loss: 135.43
 ---- batch: 060 ----
mean loss: 130.63
 ---- batch: 070 ----
mean loss: 132.46
 ---- batch: 080 ----
mean loss: 129.90
 ---- batch: 090 ----
mean loss: 124.91
 ---- batch: 100 ----
mean loss: 131.13
 ---- batch: 110 ----
mean loss: 130.16
train mean loss: 128.00
epoch train time: 0:00:01.901724
elapsed time: 0:06:22.375156
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 22:52:16.904936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.40
 ---- batch: 020 ----
mean loss: 127.77
 ---- batch: 030 ----
mean loss: 136.62
 ---- batch: 040 ----
mean loss: 117.26
 ---- batch: 050 ----
mean loss: 126.53
 ---- batch: 060 ----
mean loss: 121.14
 ---- batch: 070 ----
mean loss: 125.19
 ---- batch: 080 ----
mean loss: 131.32
 ---- batch: 090 ----
mean loss: 133.29
 ---- batch: 100 ----
mean loss: 133.52
 ---- batch: 110 ----
mean loss: 133.78
train mean loss: 128.21
epoch train time: 0:00:01.912868
elapsed time: 0:06:24.288593
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 22:52:18.818430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.56
 ---- batch: 020 ----
mean loss: 133.21
 ---- batch: 030 ----
mean loss: 129.79
 ---- batch: 040 ----
mean loss: 131.53
 ---- batch: 050 ----
mean loss: 125.72
 ---- batch: 060 ----
mean loss: 130.17
 ---- batch: 070 ----
mean loss: 124.53
 ---- batch: 080 ----
mean loss: 124.59
 ---- batch: 090 ----
mean loss: 128.24
 ---- batch: 100 ----
mean loss: 130.51
 ---- batch: 110 ----
mean loss: 128.86
train mean loss: 128.80
epoch train time: 0:00:01.941930
elapsed time: 0:06:26.231150
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 22:52:20.760961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.21
 ---- batch: 020 ----
mean loss: 129.40
 ---- batch: 030 ----
mean loss: 121.51
 ---- batch: 040 ----
mean loss: 124.85
 ---- batch: 050 ----
mean loss: 122.82
 ---- batch: 060 ----
mean loss: 119.99
 ---- batch: 070 ----
mean loss: 132.76
 ---- batch: 080 ----
mean loss: 129.90
 ---- batch: 090 ----
mean loss: 135.88
 ---- batch: 100 ----
mean loss: 128.37
 ---- batch: 110 ----
mean loss: 128.74
train mean loss: 127.15
epoch train time: 0:00:01.929029
elapsed time: 0:06:28.160769
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 22:52:22.690549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.10
 ---- batch: 020 ----
mean loss: 124.96
 ---- batch: 030 ----
mean loss: 128.44
 ---- batch: 040 ----
mean loss: 122.27
 ---- batch: 050 ----
mean loss: 131.74
 ---- batch: 060 ----
mean loss: 125.33
 ---- batch: 070 ----
mean loss: 129.05
 ---- batch: 080 ----
mean loss: 126.43
 ---- batch: 090 ----
mean loss: 121.34
 ---- batch: 100 ----
mean loss: 131.27
 ---- batch: 110 ----
mean loss: 134.42
train mean loss: 127.25
epoch train time: 0:00:01.923593
elapsed time: 0:06:30.084905
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 22:52:24.614696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.52
 ---- batch: 020 ----
mean loss: 120.78
 ---- batch: 030 ----
mean loss: 126.51
 ---- batch: 040 ----
mean loss: 125.72
 ---- batch: 050 ----
mean loss: 131.08
 ---- batch: 060 ----
mean loss: 127.33
 ---- batch: 070 ----
mean loss: 131.02
 ---- batch: 080 ----
mean loss: 119.33
 ---- batch: 090 ----
mean loss: 127.38
 ---- batch: 100 ----
mean loss: 129.41
 ---- batch: 110 ----
mean loss: 134.44
train mean loss: 126.83
epoch train time: 0:00:01.921963
elapsed time: 0:06:32.007436
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 22:52:26.537200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.73
 ---- batch: 020 ----
mean loss: 132.34
 ---- batch: 030 ----
mean loss: 131.49
 ---- batch: 040 ----
mean loss: 122.20
 ---- batch: 050 ----
mean loss: 127.95
 ---- batch: 060 ----
mean loss: 126.93
 ---- batch: 070 ----
mean loss: 124.17
 ---- batch: 080 ----
mean loss: 128.83
 ---- batch: 090 ----
mean loss: 128.80
 ---- batch: 100 ----
mean loss: 121.03
 ---- batch: 110 ----
mean loss: 128.03
train mean loss: 126.69
epoch train time: 0:00:01.928921
elapsed time: 0:06:33.936937
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 22:52:28.466726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.60
 ---- batch: 020 ----
mean loss: 118.93
 ---- batch: 030 ----
mean loss: 120.69
 ---- batch: 040 ----
mean loss: 128.31
 ---- batch: 050 ----
mean loss: 127.41
 ---- batch: 060 ----
mean loss: 134.65
 ---- batch: 070 ----
mean loss: 123.84
 ---- batch: 080 ----
mean loss: 121.96
 ---- batch: 090 ----
mean loss: 133.15
 ---- batch: 100 ----
mean loss: 127.00
 ---- batch: 110 ----
mean loss: 126.22
train mean loss: 125.54
epoch train time: 0:00:01.930444
elapsed time: 0:06:35.867935
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 22:52:30.397708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.60
 ---- batch: 020 ----
mean loss: 129.14
 ---- batch: 030 ----
mean loss: 132.43
 ---- batch: 040 ----
mean loss: 119.15
 ---- batch: 050 ----
mean loss: 129.29
 ---- batch: 060 ----
mean loss: 127.09
 ---- batch: 070 ----
mean loss: 126.23
 ---- batch: 080 ----
mean loss: 125.50
 ---- batch: 090 ----
mean loss: 126.48
 ---- batch: 100 ----
mean loss: 126.14
 ---- batch: 110 ----
mean loss: 120.43
train mean loss: 125.85
epoch train time: 0:00:01.944755
elapsed time: 0:06:37.813248
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 22:52:32.343032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.11
 ---- batch: 020 ----
mean loss: 125.52
 ---- batch: 030 ----
mean loss: 139.47
 ---- batch: 040 ----
mean loss: 123.02
 ---- batch: 050 ----
mean loss: 117.22
 ---- batch: 060 ----
mean loss: 121.89
 ---- batch: 070 ----
mean loss: 121.57
 ---- batch: 080 ----
mean loss: 130.48
 ---- batch: 090 ----
mean loss: 129.01
 ---- batch: 100 ----
mean loss: 119.68
 ---- batch: 110 ----
mean loss: 125.58
train mean loss: 125.38
epoch train time: 0:00:01.938866
elapsed time: 0:06:39.752715
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 22:52:34.282495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.67
 ---- batch: 020 ----
mean loss: 111.97
 ---- batch: 030 ----
mean loss: 127.05
 ---- batch: 040 ----
mean loss: 121.05
 ---- batch: 050 ----
mean loss: 122.04
 ---- batch: 060 ----
mean loss: 131.11
 ---- batch: 070 ----
mean loss: 136.58
 ---- batch: 080 ----
mean loss: 124.86
 ---- batch: 090 ----
mean loss: 123.42
 ---- batch: 100 ----
mean loss: 126.83
 ---- batch: 110 ----
mean loss: 133.28
train mean loss: 125.27
epoch train time: 0:00:01.918668
elapsed time: 0:06:41.671944
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 22:52:36.201739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.33
 ---- batch: 020 ----
mean loss: 118.90
 ---- batch: 030 ----
mean loss: 125.29
 ---- batch: 040 ----
mean loss: 127.50
 ---- batch: 050 ----
mean loss: 124.78
 ---- batch: 060 ----
mean loss: 128.79
 ---- batch: 070 ----
mean loss: 123.02
 ---- batch: 080 ----
mean loss: 119.82
 ---- batch: 090 ----
mean loss: 125.10
 ---- batch: 100 ----
mean loss: 130.46
 ---- batch: 110 ----
mean loss: 123.58
train mean loss: 124.41
epoch train time: 0:00:01.942060
elapsed time: 0:06:43.614587
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 22:52:38.144404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.63
 ---- batch: 020 ----
mean loss: 120.22
 ---- batch: 030 ----
mean loss: 124.55
 ---- batch: 040 ----
mean loss: 123.55
 ---- batch: 050 ----
mean loss: 128.77
 ---- batch: 060 ----
mean loss: 130.19
 ---- batch: 070 ----
mean loss: 117.76
 ---- batch: 080 ----
mean loss: 126.81
 ---- batch: 090 ----
mean loss: 120.65
 ---- batch: 100 ----
mean loss: 125.42
 ---- batch: 110 ----
mean loss: 126.69
train mean loss: 124.77
epoch train time: 0:00:01.912866
elapsed time: 0:06:45.528030
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 22:52:40.057839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.91
 ---- batch: 020 ----
mean loss: 125.58
 ---- batch: 030 ----
mean loss: 118.36
 ---- batch: 040 ----
mean loss: 125.12
 ---- batch: 050 ----
mean loss: 122.14
 ---- batch: 060 ----
mean loss: 121.42
 ---- batch: 070 ----
mean loss: 119.52
 ---- batch: 080 ----
mean loss: 121.25
 ---- batch: 090 ----
mean loss: 121.97
 ---- batch: 100 ----
mean loss: 134.03
 ---- batch: 110 ----
mean loss: 134.27
train mean loss: 124.23
epoch train time: 0:00:01.931963
elapsed time: 0:06:47.460580
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 22:52:41.990385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.25
 ---- batch: 020 ----
mean loss: 132.79
 ---- batch: 030 ----
mean loss: 126.32
 ---- batch: 040 ----
mean loss: 123.10
 ---- batch: 050 ----
mean loss: 119.09
 ---- batch: 060 ----
mean loss: 117.39
 ---- batch: 070 ----
mean loss: 129.14
 ---- batch: 080 ----
mean loss: 122.52
 ---- batch: 090 ----
mean loss: 122.41
 ---- batch: 100 ----
mean loss: 129.37
 ---- batch: 110 ----
mean loss: 122.03
train mean loss: 123.94
epoch train time: 0:00:01.931907
elapsed time: 0:06:49.393132
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 22:52:43.922924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.82
 ---- batch: 020 ----
mean loss: 124.87
 ---- batch: 030 ----
mean loss: 128.08
 ---- batch: 040 ----
mean loss: 127.27
 ---- batch: 050 ----
mean loss: 115.05
 ---- batch: 060 ----
mean loss: 125.49
 ---- batch: 070 ----
mean loss: 126.18
 ---- batch: 080 ----
mean loss: 120.46
 ---- batch: 090 ----
mean loss: 125.82
 ---- batch: 100 ----
mean loss: 125.56
 ---- batch: 110 ----
mean loss: 126.73
train mean loss: 123.73
epoch train time: 0:00:01.919488
elapsed time: 0:06:51.313222
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 22:52:45.843017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.21
 ---- batch: 020 ----
mean loss: 119.99
 ---- batch: 030 ----
mean loss: 114.69
 ---- batch: 040 ----
mean loss: 132.83
 ---- batch: 050 ----
mean loss: 120.67
 ---- batch: 060 ----
mean loss: 117.06
 ---- batch: 070 ----
mean loss: 127.49
 ---- batch: 080 ----
mean loss: 122.89
 ---- batch: 090 ----
mean loss: 127.49
 ---- batch: 100 ----
mean loss: 119.44
 ---- batch: 110 ----
mean loss: 130.83
train mean loss: 123.44
epoch train time: 0:00:01.921087
elapsed time: 0:06:53.234907
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 22:52:47.764790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.93
 ---- batch: 020 ----
mean loss: 120.75
 ---- batch: 030 ----
mean loss: 122.31
 ---- batch: 040 ----
mean loss: 121.70
 ---- batch: 050 ----
mean loss: 120.71
 ---- batch: 060 ----
mean loss: 116.59
 ---- batch: 070 ----
mean loss: 136.66
 ---- batch: 080 ----
mean loss: 122.67
 ---- batch: 090 ----
mean loss: 127.49
 ---- batch: 100 ----
mean loss: 122.08
 ---- batch: 110 ----
mean loss: 121.12
train mean loss: 123.01
epoch train time: 0:00:01.927077
elapsed time: 0:06:55.162669
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 22:52:49.692476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.67
 ---- batch: 020 ----
mean loss: 118.02
 ---- batch: 030 ----
mean loss: 121.52
 ---- batch: 040 ----
mean loss: 120.91
 ---- batch: 050 ----
mean loss: 121.40
 ---- batch: 060 ----
mean loss: 129.80
 ---- batch: 070 ----
mean loss: 114.68
 ---- batch: 080 ----
mean loss: 126.18
 ---- batch: 090 ----
mean loss: 119.74
 ---- batch: 100 ----
mean loss: 123.54
 ---- batch: 110 ----
mean loss: 125.72
train mean loss: 122.80
epoch train time: 0:00:01.936460
elapsed time: 0:06:57.099739
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 22:52:51.629530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.49
 ---- batch: 020 ----
mean loss: 113.28
 ---- batch: 030 ----
mean loss: 126.19
 ---- batch: 040 ----
mean loss: 116.07
 ---- batch: 050 ----
mean loss: 123.00
 ---- batch: 060 ----
mean loss: 130.48
 ---- batch: 070 ----
mean loss: 125.80
 ---- batch: 080 ----
mean loss: 125.43
 ---- batch: 090 ----
mean loss: 121.36
 ---- batch: 100 ----
mean loss: 120.03
 ---- batch: 110 ----
mean loss: 127.36
train mean loss: 123.39
epoch train time: 0:00:01.950970
elapsed time: 0:06:59.051294
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 22:52:53.581082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.32
 ---- batch: 020 ----
mean loss: 114.90
 ---- batch: 030 ----
mean loss: 121.68
 ---- batch: 040 ----
mean loss: 118.02
 ---- batch: 050 ----
mean loss: 122.68
 ---- batch: 060 ----
mean loss: 125.93
 ---- batch: 070 ----
mean loss: 123.46
 ---- batch: 080 ----
mean loss: 117.64
 ---- batch: 090 ----
mean loss: 126.13
 ---- batch: 100 ----
mean loss: 125.98
 ---- batch: 110 ----
mean loss: 133.68
train mean loss: 122.75
epoch train time: 0:00:01.911381
elapsed time: 0:07:00.963266
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 22:52:55.493119
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.12
 ---- batch: 020 ----
mean loss: 116.06
 ---- batch: 030 ----
mean loss: 125.05
 ---- batch: 040 ----
mean loss: 117.90
 ---- batch: 050 ----
mean loss: 112.18
 ---- batch: 060 ----
mean loss: 117.27
 ---- batch: 070 ----
mean loss: 115.34
 ---- batch: 080 ----
mean loss: 116.25
 ---- batch: 090 ----
mean loss: 119.06
 ---- batch: 100 ----
mean loss: 120.17
 ---- batch: 110 ----
mean loss: 113.31
train mean loss: 117.42
epoch train time: 0:00:01.913782
elapsed time: 0:07:02.878011
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 22:52:57.407480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.94
 ---- batch: 020 ----
mean loss: 113.46
 ---- batch: 030 ----
mean loss: 109.21
 ---- batch: 040 ----
mean loss: 119.72
 ---- batch: 050 ----
mean loss: 115.08
 ---- batch: 060 ----
mean loss: 117.07
 ---- batch: 070 ----
mean loss: 110.14
 ---- batch: 080 ----
mean loss: 119.82
 ---- batch: 090 ----
mean loss: 121.38
 ---- batch: 100 ----
mean loss: 119.01
 ---- batch: 110 ----
mean loss: 115.52
train mean loss: 116.76
epoch train time: 0:00:01.918711
elapsed time: 0:07:04.796964
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 22:52:59.326755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.28
 ---- batch: 020 ----
mean loss: 117.70
 ---- batch: 030 ----
mean loss: 117.49
 ---- batch: 040 ----
mean loss: 114.85
 ---- batch: 050 ----
mean loss: 118.05
 ---- batch: 060 ----
mean loss: 118.57
 ---- batch: 070 ----
mean loss: 114.88
 ---- batch: 080 ----
mean loss: 120.67
 ---- batch: 090 ----
mean loss: 117.19
 ---- batch: 100 ----
mean loss: 115.25
 ---- batch: 110 ----
mean loss: 113.53
train mean loss: 116.56
epoch train time: 0:00:01.905700
elapsed time: 0:07:06.703256
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 22:53:01.232751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.04
 ---- batch: 020 ----
mean loss: 113.61
 ---- batch: 030 ----
mean loss: 115.27
 ---- batch: 040 ----
mean loss: 115.85
 ---- batch: 050 ----
mean loss: 121.89
 ---- batch: 060 ----
mean loss: 117.69
 ---- batch: 070 ----
mean loss: 123.21
 ---- batch: 080 ----
mean loss: 115.08
 ---- batch: 090 ----
mean loss: 117.83
 ---- batch: 100 ----
mean loss: 111.04
 ---- batch: 110 ----
mean loss: 112.64
train mean loss: 116.49
epoch train time: 0:00:01.912191
elapsed time: 0:07:08.615749
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 22:53:03.145574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.37
 ---- batch: 020 ----
mean loss: 110.14
 ---- batch: 030 ----
mean loss: 115.80
 ---- batch: 040 ----
mean loss: 115.35
 ---- batch: 050 ----
mean loss: 113.94
 ---- batch: 060 ----
mean loss: 120.47
 ---- batch: 070 ----
mean loss: 116.78
 ---- batch: 080 ----
mean loss: 123.00
 ---- batch: 090 ----
mean loss: 109.47
 ---- batch: 100 ----
mean loss: 114.12
 ---- batch: 110 ----
mean loss: 121.72
train mean loss: 116.36
epoch train time: 0:00:01.933246
elapsed time: 0:07:10.549610
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 22:53:05.079385
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.37
 ---- batch: 020 ----
mean loss: 113.25
 ---- batch: 030 ----
mean loss: 115.42
 ---- batch: 040 ----
mean loss: 120.10
 ---- batch: 050 ----
mean loss: 108.68
 ---- batch: 060 ----
mean loss: 118.13
 ---- batch: 070 ----
mean loss: 118.66
 ---- batch: 080 ----
mean loss: 119.40
 ---- batch: 090 ----
mean loss: 117.86
 ---- batch: 100 ----
mean loss: 109.20
 ---- batch: 110 ----
mean loss: 117.76
train mean loss: 116.41
epoch train time: 0:00:01.903120
elapsed time: 0:07:12.453288
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 22:53:06.983076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.16
 ---- batch: 020 ----
mean loss: 117.37
 ---- batch: 030 ----
mean loss: 117.34
 ---- batch: 040 ----
mean loss: 123.42
 ---- batch: 050 ----
mean loss: 113.46
 ---- batch: 060 ----
mean loss: 116.75
 ---- batch: 070 ----
mean loss: 108.08
 ---- batch: 080 ----
mean loss: 122.87
 ---- batch: 090 ----
mean loss: 115.41
 ---- batch: 100 ----
mean loss: 120.58
 ---- batch: 110 ----
mean loss: 114.11
train mean loss: 116.27
epoch train time: 0:00:01.921137
elapsed time: 0:07:14.375017
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 22:53:08.904826
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.15
 ---- batch: 020 ----
mean loss: 115.01
 ---- batch: 030 ----
mean loss: 111.23
 ---- batch: 040 ----
mean loss: 115.20
 ---- batch: 050 ----
mean loss: 119.98
 ---- batch: 060 ----
mean loss: 120.23
 ---- batch: 070 ----
mean loss: 123.13
 ---- batch: 080 ----
mean loss: 115.12
 ---- batch: 090 ----
mean loss: 117.12
 ---- batch: 100 ----
mean loss: 113.38
 ---- batch: 110 ----
mean loss: 119.88
train mean loss: 116.32
epoch train time: 0:00:01.920722
elapsed time: 0:07:16.296374
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 22:53:10.826165
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.69
 ---- batch: 020 ----
mean loss: 105.01
 ---- batch: 030 ----
mean loss: 121.77
 ---- batch: 040 ----
mean loss: 116.17
 ---- batch: 050 ----
mean loss: 114.11
 ---- batch: 060 ----
mean loss: 117.87
 ---- batch: 070 ----
mean loss: 119.40
 ---- batch: 080 ----
mean loss: 109.65
 ---- batch: 090 ----
mean loss: 119.80
 ---- batch: 100 ----
mean loss: 111.71
 ---- batch: 110 ----
mean loss: 127.20
train mean loss: 116.25
epoch train time: 0:00:01.938929
elapsed time: 0:07:18.235875
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 22:53:12.765689
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.60
 ---- batch: 020 ----
mean loss: 113.82
 ---- batch: 030 ----
mean loss: 114.07
 ---- batch: 040 ----
mean loss: 120.54
 ---- batch: 050 ----
mean loss: 118.46
 ---- batch: 060 ----
mean loss: 119.46
 ---- batch: 070 ----
mean loss: 108.36
 ---- batch: 080 ----
mean loss: 108.50
 ---- batch: 090 ----
mean loss: 114.71
 ---- batch: 100 ----
mean loss: 114.91
 ---- batch: 110 ----
mean loss: 114.14
train mean loss: 116.21
epoch train time: 0:00:01.963176
elapsed time: 0:07:20.199668
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 22:53:14.729503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.07
 ---- batch: 020 ----
mean loss: 121.65
 ---- batch: 030 ----
mean loss: 118.99
 ---- batch: 040 ----
mean loss: 114.12
 ---- batch: 050 ----
mean loss: 120.55
 ---- batch: 060 ----
mean loss: 113.24
 ---- batch: 070 ----
mean loss: 113.31
 ---- batch: 080 ----
mean loss: 112.20
 ---- batch: 090 ----
mean loss: 120.58
 ---- batch: 100 ----
mean loss: 110.22
 ---- batch: 110 ----
mean loss: 113.48
train mean loss: 116.26
epoch train time: 0:00:01.943070
elapsed time: 0:07:22.143430
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 22:53:16.673217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.95
 ---- batch: 020 ----
mean loss: 106.42
 ---- batch: 030 ----
mean loss: 118.17
 ---- batch: 040 ----
mean loss: 121.79
 ---- batch: 050 ----
mean loss: 119.21
 ---- batch: 060 ----
mean loss: 114.25
 ---- batch: 070 ----
mean loss: 116.15
 ---- batch: 080 ----
mean loss: 116.18
 ---- batch: 090 ----
mean loss: 115.27
 ---- batch: 100 ----
mean loss: 121.93
 ---- batch: 110 ----
mean loss: 115.30
train mean loss: 116.05
epoch train time: 0:00:01.956038
elapsed time: 0:07:24.100032
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 22:53:18.629897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.83
 ---- batch: 020 ----
mean loss: 116.62
 ---- batch: 030 ----
mean loss: 105.80
 ---- batch: 040 ----
mean loss: 126.11
 ---- batch: 050 ----
mean loss: 118.48
 ---- batch: 060 ----
mean loss: 121.16
 ---- batch: 070 ----
mean loss: 110.24
 ---- batch: 080 ----
mean loss: 112.84
 ---- batch: 090 ----
mean loss: 110.55
 ---- batch: 100 ----
mean loss: 116.28
 ---- batch: 110 ----
mean loss: 118.73
train mean loss: 115.94
epoch train time: 0:00:01.927533
elapsed time: 0:07:26.028285
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 22:53:20.558141
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.90
 ---- batch: 020 ----
mean loss: 115.67
 ---- batch: 030 ----
mean loss: 111.74
 ---- batch: 040 ----
mean loss: 113.52
 ---- batch: 050 ----
mean loss: 119.53
 ---- batch: 060 ----
mean loss: 113.84
 ---- batch: 070 ----
mean loss: 120.68
 ---- batch: 080 ----
mean loss: 115.17
 ---- batch: 090 ----
mean loss: 115.16
 ---- batch: 100 ----
mean loss: 118.43
 ---- batch: 110 ----
mean loss: 120.59
train mean loss: 116.07
epoch train time: 0:00:01.937482
elapsed time: 0:07:27.966398
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 22:53:22.496188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.25
 ---- batch: 020 ----
mean loss: 113.47
 ---- batch: 030 ----
mean loss: 116.10
 ---- batch: 040 ----
mean loss: 123.32
 ---- batch: 050 ----
mean loss: 112.12
 ---- batch: 060 ----
mean loss: 115.65
 ---- batch: 070 ----
mean loss: 121.68
 ---- batch: 080 ----
mean loss: 114.04
 ---- batch: 090 ----
mean loss: 109.43
 ---- batch: 100 ----
mean loss: 116.35
 ---- batch: 110 ----
mean loss: 113.34
train mean loss: 116.10
epoch train time: 0:00:01.947352
elapsed time: 0:07:29.914346
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 22:53:24.444122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.08
 ---- batch: 020 ----
mean loss: 117.60
 ---- batch: 030 ----
mean loss: 114.75
 ---- batch: 040 ----
mean loss: 118.68
 ---- batch: 050 ----
mean loss: 111.58
 ---- batch: 060 ----
mean loss: 116.60
 ---- batch: 070 ----
mean loss: 117.09
 ---- batch: 080 ----
mean loss: 117.39
 ---- batch: 090 ----
mean loss: 117.87
 ---- batch: 100 ----
mean loss: 114.26
 ---- batch: 110 ----
mean loss: 114.89
train mean loss: 115.96
epoch train time: 0:00:01.932453
elapsed time: 0:07:31.847363
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 22:53:26.377160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.24
 ---- batch: 020 ----
mean loss: 112.75
 ---- batch: 030 ----
mean loss: 119.13
 ---- batch: 040 ----
mean loss: 116.74
 ---- batch: 050 ----
mean loss: 114.76
 ---- batch: 060 ----
mean loss: 119.57
 ---- batch: 070 ----
mean loss: 115.52
 ---- batch: 080 ----
mean loss: 119.63
 ---- batch: 090 ----
mean loss: 115.12
 ---- batch: 100 ----
mean loss: 118.86
 ---- batch: 110 ----
mean loss: 114.47
train mean loss: 115.91
epoch train time: 0:00:01.914616
elapsed time: 0:07:33.762566
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 22:53:28.292361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.25
 ---- batch: 020 ----
mean loss: 117.92
 ---- batch: 030 ----
mean loss: 116.07
 ---- batch: 040 ----
mean loss: 116.80
 ---- batch: 050 ----
mean loss: 111.83
 ---- batch: 060 ----
mean loss: 115.89
 ---- batch: 070 ----
mean loss: 115.15
 ---- batch: 080 ----
mean loss: 114.39
 ---- batch: 090 ----
mean loss: 117.48
 ---- batch: 100 ----
mean loss: 119.43
 ---- batch: 110 ----
mean loss: 111.39
train mean loss: 115.97
epoch train time: 0:00:01.936409
elapsed time: 0:07:35.699687
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 22:53:30.229452
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.99
 ---- batch: 020 ----
mean loss: 116.98
 ---- batch: 030 ----
mean loss: 113.24
 ---- batch: 040 ----
mean loss: 109.67
 ---- batch: 050 ----
mean loss: 115.46
 ---- batch: 060 ----
mean loss: 109.80
 ---- batch: 070 ----
mean loss: 114.38
 ---- batch: 080 ----
mean loss: 124.31
 ---- batch: 090 ----
mean loss: 116.29
 ---- batch: 100 ----
mean loss: 118.56
 ---- batch: 110 ----
mean loss: 118.93
train mean loss: 115.89
epoch train time: 0:00:01.938685
elapsed time: 0:07:37.638919
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 22:53:32.168755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.14
 ---- batch: 020 ----
mean loss: 117.01
 ---- batch: 030 ----
mean loss: 119.60
 ---- batch: 040 ----
mean loss: 118.02
 ---- batch: 050 ----
mean loss: 120.52
 ---- batch: 060 ----
mean loss: 112.66
 ---- batch: 070 ----
mean loss: 113.14
 ---- batch: 080 ----
mean loss: 116.10
 ---- batch: 090 ----
mean loss: 116.83
 ---- batch: 100 ----
mean loss: 119.54
 ---- batch: 110 ----
mean loss: 115.11
train mean loss: 115.78
epoch train time: 0:00:01.907154
elapsed time: 0:07:39.546708
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 22:53:34.076498
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.85
 ---- batch: 020 ----
mean loss: 111.24
 ---- batch: 030 ----
mean loss: 120.90
 ---- batch: 040 ----
mean loss: 119.45
 ---- batch: 050 ----
mean loss: 110.39
 ---- batch: 060 ----
mean loss: 114.09
 ---- batch: 070 ----
mean loss: 119.36
 ---- batch: 080 ----
mean loss: 123.20
 ---- batch: 090 ----
mean loss: 113.47
 ---- batch: 100 ----
mean loss: 114.69
 ---- batch: 110 ----
mean loss: 117.74
train mean loss: 115.73
epoch train time: 0:00:01.945682
elapsed time: 0:07:41.492963
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 22:53:36.022745
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.89
 ---- batch: 020 ----
mean loss: 118.45
 ---- batch: 030 ----
mean loss: 121.93
 ---- batch: 040 ----
mean loss: 111.19
 ---- batch: 050 ----
mean loss: 115.72
 ---- batch: 060 ----
mean loss: 116.82
 ---- batch: 070 ----
mean loss: 114.08
 ---- batch: 080 ----
mean loss: 110.99
 ---- batch: 090 ----
mean loss: 110.94
 ---- batch: 100 ----
mean loss: 116.30
 ---- batch: 110 ----
mean loss: 114.50
train mean loss: 115.86
epoch train time: 0:00:01.940713
elapsed time: 0:07:43.434256
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 22:53:37.964098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.37
 ---- batch: 020 ----
mean loss: 116.52
 ---- batch: 030 ----
mean loss: 115.17
 ---- batch: 040 ----
mean loss: 110.81
 ---- batch: 050 ----
mean loss: 119.86
 ---- batch: 060 ----
mean loss: 120.87
 ---- batch: 070 ----
mean loss: 112.86
 ---- batch: 080 ----
mean loss: 118.98
 ---- batch: 090 ----
mean loss: 120.07
 ---- batch: 100 ----
mean loss: 110.19
 ---- batch: 110 ----
mean loss: 110.52
train mean loss: 115.76
epoch train time: 0:00:01.922305
elapsed time: 0:07:45.357196
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 22:53:39.887016
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.49
 ---- batch: 020 ----
mean loss: 119.16
 ---- batch: 030 ----
mean loss: 113.42
 ---- batch: 040 ----
mean loss: 111.35
 ---- batch: 050 ----
mean loss: 120.45
 ---- batch: 060 ----
mean loss: 115.79
 ---- batch: 070 ----
mean loss: 117.54
 ---- batch: 080 ----
mean loss: 120.06
 ---- batch: 090 ----
mean loss: 113.95
 ---- batch: 100 ----
mean loss: 109.01
 ---- batch: 110 ----
mean loss: 118.26
train mean loss: 115.66
epoch train time: 0:00:01.933777
elapsed time: 0:07:47.291598
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 22:53:41.821405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.99
 ---- batch: 020 ----
mean loss: 109.90
 ---- batch: 030 ----
mean loss: 119.77
 ---- batch: 040 ----
mean loss: 119.48
 ---- batch: 050 ----
mean loss: 107.47
 ---- batch: 060 ----
mean loss: 116.72
 ---- batch: 070 ----
mean loss: 112.04
 ---- batch: 080 ----
mean loss: 111.05
 ---- batch: 090 ----
mean loss: 120.19
 ---- batch: 100 ----
mean loss: 119.68
 ---- batch: 110 ----
mean loss: 118.39
train mean loss: 115.71
epoch train time: 0:00:01.979045
elapsed time: 0:07:49.271261
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 22:53:43.801048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.77
 ---- batch: 020 ----
mean loss: 125.12
 ---- batch: 030 ----
mean loss: 117.19
 ---- batch: 040 ----
mean loss: 115.16
 ---- batch: 050 ----
mean loss: 114.15
 ---- batch: 060 ----
mean loss: 106.72
 ---- batch: 070 ----
mean loss: 123.80
 ---- batch: 080 ----
mean loss: 112.07
 ---- batch: 090 ----
mean loss: 119.99
 ---- batch: 100 ----
mean loss: 115.13
 ---- batch: 110 ----
mean loss: 112.97
train mean loss: 115.68
epoch train time: 0:00:01.928495
elapsed time: 0:07:51.200345
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 22:53:45.730132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.54
 ---- batch: 020 ----
mean loss: 113.02
 ---- batch: 030 ----
mean loss: 116.94
 ---- batch: 040 ----
mean loss: 112.83
 ---- batch: 050 ----
mean loss: 113.37
 ---- batch: 060 ----
mean loss: 121.96
 ---- batch: 070 ----
mean loss: 120.78
 ---- batch: 080 ----
mean loss: 122.09
 ---- batch: 090 ----
mean loss: 114.86
 ---- batch: 100 ----
mean loss: 116.58
 ---- batch: 110 ----
mean loss: 114.34
train mean loss: 115.63
epoch train time: 0:00:01.906762
elapsed time: 0:07:53.107672
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 22:53:47.637521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.81
 ---- batch: 020 ----
mean loss: 111.97
 ---- batch: 030 ----
mean loss: 117.92
 ---- batch: 040 ----
mean loss: 119.17
 ---- batch: 050 ----
mean loss: 108.87
 ---- batch: 060 ----
mean loss: 111.71
 ---- batch: 070 ----
mean loss: 119.51
 ---- batch: 080 ----
mean loss: 121.99
 ---- batch: 090 ----
mean loss: 114.68
 ---- batch: 100 ----
mean loss: 116.72
 ---- batch: 110 ----
mean loss: 114.15
train mean loss: 115.65
epoch train time: 0:00:01.930972
elapsed time: 0:07:55.039265
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 22:53:49.569050
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.49
 ---- batch: 020 ----
mean loss: 119.32
 ---- batch: 030 ----
mean loss: 124.39
 ---- batch: 040 ----
mean loss: 120.94
 ---- batch: 050 ----
mean loss: 118.93
 ---- batch: 060 ----
mean loss: 115.56
 ---- batch: 070 ----
mean loss: 111.93
 ---- batch: 080 ----
mean loss: 106.93
 ---- batch: 090 ----
mean loss: 116.36
 ---- batch: 100 ----
mean loss: 117.51
 ---- batch: 110 ----
mean loss: 112.44
train mean loss: 115.57
epoch train time: 0:00:01.940177
elapsed time: 0:07:56.979995
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 22:53:51.509785
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.83
 ---- batch: 020 ----
mean loss: 113.39
 ---- batch: 030 ----
mean loss: 112.96
 ---- batch: 040 ----
mean loss: 122.89
 ---- batch: 050 ----
mean loss: 112.23
 ---- batch: 060 ----
mean loss: 113.51
 ---- batch: 070 ----
mean loss: 115.28
 ---- batch: 080 ----
mean loss: 118.57
 ---- batch: 090 ----
mean loss: 109.86
 ---- batch: 100 ----
mean loss: 115.00
 ---- batch: 110 ----
mean loss: 117.06
train mean loss: 115.53
epoch train time: 0:00:01.937270
elapsed time: 0:07:58.917870
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 22:53:53.447714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.23
 ---- batch: 020 ----
mean loss: 107.43
 ---- batch: 030 ----
mean loss: 113.10
 ---- batch: 040 ----
mean loss: 119.13
 ---- batch: 050 ----
mean loss: 117.03
 ---- batch: 060 ----
mean loss: 111.17
 ---- batch: 070 ----
mean loss: 114.75
 ---- batch: 080 ----
mean loss: 119.16
 ---- batch: 090 ----
mean loss: 116.47
 ---- batch: 100 ----
mean loss: 123.48
 ---- batch: 110 ----
mean loss: 118.14
train mean loss: 115.44
epoch train time: 0:00:01.939685
elapsed time: 0:08:00.858191
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 22:53:55.387985
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.91
 ---- batch: 020 ----
mean loss: 114.02
 ---- batch: 030 ----
mean loss: 121.57
 ---- batch: 040 ----
mean loss: 121.49
 ---- batch: 050 ----
mean loss: 105.97
 ---- batch: 060 ----
mean loss: 118.63
 ---- batch: 070 ----
mean loss: 118.71
 ---- batch: 080 ----
mean loss: 113.82
 ---- batch: 090 ----
mean loss: 118.05
 ---- batch: 100 ----
mean loss: 111.20
 ---- batch: 110 ----
mean loss: 111.65
train mean loss: 115.37
epoch train time: 0:00:01.932560
elapsed time: 0:08:02.791319
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 22:53:57.321103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.53
 ---- batch: 020 ----
mean loss: 108.86
 ---- batch: 030 ----
mean loss: 112.61
 ---- batch: 040 ----
mean loss: 117.49
 ---- batch: 050 ----
mean loss: 109.64
 ---- batch: 060 ----
mean loss: 119.93
 ---- batch: 070 ----
mean loss: 118.40
 ---- batch: 080 ----
mean loss: 117.50
 ---- batch: 090 ----
mean loss: 114.91
 ---- batch: 100 ----
mean loss: 121.58
 ---- batch: 110 ----
mean loss: 115.42
train mean loss: 115.41
epoch train time: 0:00:01.924372
elapsed time: 0:08:04.716523
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 22:53:59.246076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.17
 ---- batch: 020 ----
mean loss: 116.14
 ---- batch: 030 ----
mean loss: 108.98
 ---- batch: 040 ----
mean loss: 116.37
 ---- batch: 050 ----
mean loss: 113.50
 ---- batch: 060 ----
mean loss: 118.79
 ---- batch: 070 ----
mean loss: 115.33
 ---- batch: 080 ----
mean loss: 119.21
 ---- batch: 090 ----
mean loss: 116.36
 ---- batch: 100 ----
mean loss: 117.19
 ---- batch: 110 ----
mean loss: 111.89
train mean loss: 115.33
epoch train time: 0:00:01.923164
elapsed time: 0:08:06.640003
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 22:54:01.169835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.59
 ---- batch: 020 ----
mean loss: 109.54
 ---- batch: 030 ----
mean loss: 109.57
 ---- batch: 040 ----
mean loss: 125.54
 ---- batch: 050 ----
mean loss: 111.72
 ---- batch: 060 ----
mean loss: 119.32
 ---- batch: 070 ----
mean loss: 119.93
 ---- batch: 080 ----
mean loss: 111.76
 ---- batch: 090 ----
mean loss: 119.20
 ---- batch: 100 ----
mean loss: 117.58
 ---- batch: 110 ----
mean loss: 109.35
train mean loss: 115.32
epoch train time: 0:00:01.950447
elapsed time: 0:08:08.591118
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 22:54:03.120924
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.52
 ---- batch: 020 ----
mean loss: 112.79
 ---- batch: 030 ----
mean loss: 112.70
 ---- batch: 040 ----
mean loss: 125.93
 ---- batch: 050 ----
mean loss: 113.60
 ---- batch: 060 ----
mean loss: 124.23
 ---- batch: 070 ----
mean loss: 112.30
 ---- batch: 080 ----
mean loss: 116.16
 ---- batch: 090 ----
mean loss: 113.20
 ---- batch: 100 ----
mean loss: 114.92
 ---- batch: 110 ----
mean loss: 108.63
train mean loss: 115.23
epoch train time: 0:00:01.935310
elapsed time: 0:08:10.527008
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 22:54:05.056768
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.59
 ---- batch: 020 ----
mean loss: 122.67
 ---- batch: 030 ----
mean loss: 112.22
 ---- batch: 040 ----
mean loss: 114.50
 ---- batch: 050 ----
mean loss: 116.00
 ---- batch: 060 ----
mean loss: 116.13
 ---- batch: 070 ----
mean loss: 107.29
 ---- batch: 080 ----
mean loss: 115.23
 ---- batch: 090 ----
mean loss: 119.38
 ---- batch: 100 ----
mean loss: 113.71
 ---- batch: 110 ----
mean loss: 109.82
train mean loss: 115.19
epoch train time: 0:00:01.943152
elapsed time: 0:08:12.470705
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 22:54:07.000488
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.76
 ---- batch: 020 ----
mean loss: 114.08
 ---- batch: 030 ----
mean loss: 115.10
 ---- batch: 040 ----
mean loss: 116.21
 ---- batch: 050 ----
mean loss: 115.63
 ---- batch: 060 ----
mean loss: 117.67
 ---- batch: 070 ----
mean loss: 125.65
 ---- batch: 080 ----
mean loss: 109.50
 ---- batch: 090 ----
mean loss: 109.61
 ---- batch: 100 ----
mean loss: 117.01
 ---- batch: 110 ----
mean loss: 112.97
train mean loss: 115.17
epoch train time: 0:00:01.934501
elapsed time: 0:08:14.405784
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 22:54:08.935561
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.41
 ---- batch: 020 ----
mean loss: 116.30
 ---- batch: 030 ----
mean loss: 114.52
 ---- batch: 040 ----
mean loss: 121.87
 ---- batch: 050 ----
mean loss: 110.66
 ---- batch: 060 ----
mean loss: 121.24
 ---- batch: 070 ----
mean loss: 120.50
 ---- batch: 080 ----
mean loss: 120.41
 ---- batch: 090 ----
mean loss: 109.47
 ---- batch: 100 ----
mean loss: 114.23
 ---- batch: 110 ----
mean loss: 110.14
train mean loss: 115.21
epoch train time: 0:00:01.967113
elapsed time: 0:08:16.373452
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 22:54:10.903236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.34
 ---- batch: 020 ----
mean loss: 110.98
 ---- batch: 030 ----
mean loss: 114.83
 ---- batch: 040 ----
mean loss: 112.15
 ---- batch: 050 ----
mean loss: 112.77
 ---- batch: 060 ----
mean loss: 112.73
 ---- batch: 070 ----
mean loss: 117.34
 ---- batch: 080 ----
mean loss: 113.73
 ---- batch: 090 ----
mean loss: 126.70
 ---- batch: 100 ----
mean loss: 110.84
 ---- batch: 110 ----
mean loss: 118.60
train mean loss: 115.14
epoch train time: 0:00:01.914767
elapsed time: 0:08:18.288796
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 22:54:12.818567
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.00
 ---- batch: 020 ----
mean loss: 112.92
 ---- batch: 030 ----
mean loss: 109.50
 ---- batch: 040 ----
mean loss: 114.90
 ---- batch: 050 ----
mean loss: 117.23
 ---- batch: 060 ----
mean loss: 119.19
 ---- batch: 070 ----
mean loss: 117.41
 ---- batch: 080 ----
mean loss: 115.68
 ---- batch: 090 ----
mean loss: 116.26
 ---- batch: 100 ----
mean loss: 119.68
 ---- batch: 110 ----
mean loss: 111.31
train mean loss: 115.09
epoch train time: 0:00:01.943612
elapsed time: 0:08:20.232949
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 22:54:14.762719
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.37
 ---- batch: 020 ----
mean loss: 119.82
 ---- batch: 030 ----
mean loss: 120.91
 ---- batch: 040 ----
mean loss: 107.05
 ---- batch: 050 ----
mean loss: 112.45
 ---- batch: 060 ----
mean loss: 118.86
 ---- batch: 070 ----
mean loss: 117.69
 ---- batch: 080 ----
mean loss: 106.01
 ---- batch: 090 ----
mean loss: 111.43
 ---- batch: 100 ----
mean loss: 119.30
 ---- batch: 110 ----
mean loss: 116.90
train mean loss: 115.03
epoch train time: 0:00:01.945744
elapsed time: 0:08:22.179286
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 22:54:16.709057
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.79
 ---- batch: 020 ----
mean loss: 118.30
 ---- batch: 030 ----
mean loss: 109.44
 ---- batch: 040 ----
mean loss: 122.81
 ---- batch: 050 ----
mean loss: 116.35
 ---- batch: 060 ----
mean loss: 109.54
 ---- batch: 070 ----
mean loss: 118.48
 ---- batch: 080 ----
mean loss: 109.32
 ---- batch: 090 ----
mean loss: 113.69
 ---- batch: 100 ----
mean loss: 123.51
 ---- batch: 110 ----
mean loss: 111.74
train mean loss: 115.03
epoch train time: 0:00:01.923375
elapsed time: 0:08:24.103246
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 22:54:18.633075
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.72
 ---- batch: 020 ----
mean loss: 111.01
 ---- batch: 030 ----
mean loss: 108.04
 ---- batch: 040 ----
mean loss: 117.07
 ---- batch: 050 ----
mean loss: 117.21
 ---- batch: 060 ----
mean loss: 113.77
 ---- batch: 070 ----
mean loss: 118.07
 ---- batch: 080 ----
mean loss: 120.80
 ---- batch: 090 ----
mean loss: 111.58
 ---- batch: 100 ----
mean loss: 120.96
 ---- batch: 110 ----
mean loss: 117.89
train mean loss: 114.96
epoch train time: 0:00:01.936871
elapsed time: 0:08:26.040710
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 22:54:20.570484
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.77
 ---- batch: 020 ----
mean loss: 113.86
 ---- batch: 030 ----
mean loss: 118.21
 ---- batch: 040 ----
mean loss: 116.78
 ---- batch: 050 ----
mean loss: 107.46
 ---- batch: 060 ----
mean loss: 115.19
 ---- batch: 070 ----
mean loss: 115.63
 ---- batch: 080 ----
mean loss: 110.97
 ---- batch: 090 ----
mean loss: 111.98
 ---- batch: 100 ----
mean loss: 123.23
 ---- batch: 110 ----
mean loss: 119.25
train mean loss: 114.89
epoch train time: 0:00:01.920443
elapsed time: 0:08:27.961709
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 22:54:22.491614
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.37
 ---- batch: 020 ----
mean loss: 117.32
 ---- batch: 030 ----
mean loss: 110.94
 ---- batch: 040 ----
mean loss: 115.16
 ---- batch: 050 ----
mean loss: 118.49
 ---- batch: 060 ----
mean loss: 109.49
 ---- batch: 070 ----
mean loss: 121.54
 ---- batch: 080 ----
mean loss: 113.58
 ---- batch: 090 ----
mean loss: 114.13
 ---- batch: 100 ----
mean loss: 119.86
 ---- batch: 110 ----
mean loss: 110.09
train mean loss: 114.94
epoch train time: 0:00:01.924713
elapsed time: 0:08:29.887115
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 22:54:24.416913
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.16
 ---- batch: 020 ----
mean loss: 117.84
 ---- batch: 030 ----
mean loss: 113.19
 ---- batch: 040 ----
mean loss: 112.81
 ---- batch: 050 ----
mean loss: 115.74
 ---- batch: 060 ----
mean loss: 118.28
 ---- batch: 070 ----
mean loss: 115.55
 ---- batch: 080 ----
mean loss: 108.28
 ---- batch: 090 ----
mean loss: 115.50
 ---- batch: 100 ----
mean loss: 119.43
 ---- batch: 110 ----
mean loss: 124.69
train mean loss: 114.76
epoch train time: 0:00:01.930097
elapsed time: 0:08:31.817919
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 22:54:26.347736
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.56
 ---- batch: 020 ----
mean loss: 111.28
 ---- batch: 030 ----
mean loss: 114.98
 ---- batch: 040 ----
mean loss: 111.88
 ---- batch: 050 ----
mean loss: 114.68
 ---- batch: 060 ----
mean loss: 110.91
 ---- batch: 070 ----
mean loss: 114.02
 ---- batch: 080 ----
mean loss: 115.00
 ---- batch: 090 ----
mean loss: 118.03
 ---- batch: 100 ----
mean loss: 117.47
 ---- batch: 110 ----
mean loss: 118.26
train mean loss: 114.87
epoch train time: 0:00:01.930952
elapsed time: 0:08:33.749452
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 22:54:28.279225
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.94
 ---- batch: 020 ----
mean loss: 110.55
 ---- batch: 030 ----
mean loss: 113.37
 ---- batch: 040 ----
mean loss: 116.60
 ---- batch: 050 ----
mean loss: 113.56
 ---- batch: 060 ----
mean loss: 119.02
 ---- batch: 070 ----
mean loss: 114.34
 ---- batch: 080 ----
mean loss: 114.52
 ---- batch: 090 ----
mean loss: 115.71
 ---- batch: 100 ----
mean loss: 116.72
 ---- batch: 110 ----
mean loss: 114.88
train mean loss: 114.79
epoch train time: 0:00:01.930059
elapsed time: 0:08:35.687933
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_3/checkpoint.pth.tar
**** end time: 2019-09-26 22:54:30.217365 ****
