Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_4', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8633
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistDense3...
Done.
**** start time: 2019-09-20 23:43:05.297711 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
            Linear-2                  [-1, 100]          42,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 62,100
Trainable params: 62,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:43:05.301108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4733.70
 ---- batch: 020 ----
mean loss: 4627.24
 ---- batch: 030 ----
mean loss: 4580.46
 ---- batch: 040 ----
mean loss: 4430.11
train mean loss: 4575.63
epoch train time: 0:00:15.052788
elapsed time: 0:00:15.058204
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:43:20.355962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4373.32
 ---- batch: 020 ----
mean loss: 4247.94
 ---- batch: 030 ----
mean loss: 4168.73
 ---- batch: 040 ----
mean loss: 4204.63
train mean loss: 4237.27
epoch train time: 0:00:00.203204
elapsed time: 0:00:15.261542
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:43:20.559311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4029.83
 ---- batch: 020 ----
mean loss: 4006.63
 ---- batch: 030 ----
mean loss: 4012.47
 ---- batch: 040 ----
mean loss: 3894.05
train mean loss: 3971.37
epoch train time: 0:00:00.201785
elapsed time: 0:00:15.463461
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:43:20.761214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3786.89
 ---- batch: 020 ----
mean loss: 3844.15
 ---- batch: 030 ----
mean loss: 3587.92
 ---- batch: 040 ----
mean loss: 3663.07
train mean loss: 3720.94
epoch train time: 0:00:00.204725
elapsed time: 0:00:15.668305
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:43:20.966061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3585.81
 ---- batch: 020 ----
mean loss: 3485.72
 ---- batch: 030 ----
mean loss: 3462.87
 ---- batch: 040 ----
mean loss: 3376.93
train mean loss: 3468.99
epoch train time: 0:00:00.202121
elapsed time: 0:00:15.870549
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:43:21.168303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3294.72
 ---- batch: 020 ----
mean loss: 3308.37
 ---- batch: 030 ----
mean loss: 3206.74
 ---- batch: 040 ----
mean loss: 3139.89
train mean loss: 3232.56
epoch train time: 0:00:00.200066
elapsed time: 0:00:16.070751
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:43:21.368519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3072.55
 ---- batch: 020 ----
mean loss: 3095.54
 ---- batch: 030 ----
mean loss: 3020.51
 ---- batch: 040 ----
mean loss: 2894.44
train mean loss: 3015.47
epoch train time: 0:00:00.200412
elapsed time: 0:00:16.271305
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:43:21.569058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2904.10
 ---- batch: 020 ----
mean loss: 2794.88
 ---- batch: 030 ----
mean loss: 2828.27
 ---- batch: 040 ----
mean loss: 2747.08
train mean loss: 2814.73
epoch train time: 0:00:00.199300
elapsed time: 0:00:16.470751
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:43:21.768509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2760.05
 ---- batch: 020 ----
mean loss: 2634.66
 ---- batch: 030 ----
mean loss: 2602.34
 ---- batch: 040 ----
mean loss: 2553.70
train mean loss: 2632.19
epoch train time: 0:00:00.207448
elapsed time: 0:00:16.678326
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:43:21.976082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2517.98
 ---- batch: 020 ----
mean loss: 2473.54
 ---- batch: 030 ----
mean loss: 2451.28
 ---- batch: 040 ----
mean loss: 2449.09
train mean loss: 2465.88
epoch train time: 0:00:00.200656
elapsed time: 0:00:16.879117
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:43:22.176874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2359.60
 ---- batch: 020 ----
mean loss: 2340.43
 ---- batch: 030 ----
mean loss: 2316.98
 ---- batch: 040 ----
mean loss: 2231.53
train mean loss: 2314.15
epoch train time: 0:00:00.206633
elapsed time: 0:00:17.085888
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:43:22.383657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2263.97
 ---- batch: 020 ----
mean loss: 2169.66
 ---- batch: 030 ----
mean loss: 2140.60
 ---- batch: 040 ----
mean loss: 2130.41
train mean loss: 2173.48
epoch train time: 0:00:00.201583
elapsed time: 0:00:17.287607
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:43:22.585363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2080.11
 ---- batch: 020 ----
mean loss: 2067.82
 ---- batch: 030 ----
mean loss: 2019.13
 ---- batch: 040 ----
mean loss: 2023.04
train mean loss: 2043.65
epoch train time: 0:00:00.204170
elapsed time: 0:00:17.491902
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:43:22.789673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1963.29
 ---- batch: 020 ----
mean loss: 1939.00
 ---- batch: 030 ----
mean loss: 1903.54
 ---- batch: 040 ----
mean loss: 1894.54
train mean loss: 1923.94
epoch train time: 0:00:00.205109
elapsed time: 0:00:17.697165
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:43:22.994934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1860.77
 ---- batch: 020 ----
mean loss: 1815.32
 ---- batch: 030 ----
mean loss: 1787.11
 ---- batch: 040 ----
mean loss: 1789.96
train mean loss: 1807.32
epoch train time: 0:00:00.197038
elapsed time: 0:00:17.894343
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:43:23.192098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1722.36
 ---- batch: 020 ----
mean loss: 1735.10
 ---- batch: 030 ----
mean loss: 1704.39
 ---- batch: 040 ----
mean loss: 1665.65
train mean loss: 1702.86
epoch train time: 0:00:00.198805
elapsed time: 0:00:18.093281
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:43:23.391052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1619.09
 ---- batch: 020 ----
mean loss: 1609.83
 ---- batch: 030 ----
mean loss: 1597.23
 ---- batch: 040 ----
mean loss: 1584.44
train mean loss: 1602.59
epoch train time: 0:00:00.198871
elapsed time: 0:00:18.292289
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:43:23.590042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1533.13
 ---- batch: 020 ----
mean loss: 1520.19
 ---- batch: 030 ----
mean loss: 1498.97
 ---- batch: 040 ----
mean loss: 1478.94
train mean loss: 1505.55
epoch train time: 0:00:00.196402
elapsed time: 0:00:18.488812
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:43:23.786566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1465.57
 ---- batch: 020 ----
mean loss: 1402.48
 ---- batch: 030 ----
mean loss: 1427.73
 ---- batch: 040 ----
mean loss: 1389.35
train mean loss: 1417.36
epoch train time: 0:00:00.210471
elapsed time: 0:00:18.699428
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:43:23.997190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1352.96
 ---- batch: 020 ----
mean loss: 1355.21
 ---- batch: 030 ----
mean loss: 1350.42
 ---- batch: 040 ----
mean loss: 1296.04
train mean loss: 1338.28
epoch train time: 0:00:00.201841
elapsed time: 0:00:18.901400
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:43:24.199155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1283.14
 ---- batch: 020 ----
mean loss: 1279.26
 ---- batch: 030 ----
mean loss: 1255.60
 ---- batch: 040 ----
mean loss: 1240.43
train mean loss: 1262.60
epoch train time: 0:00:00.201374
elapsed time: 0:00:19.102900
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:43:24.400656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1221.06
 ---- batch: 020 ----
mean loss: 1187.21
 ---- batch: 030 ----
mean loss: 1187.49
 ---- batch: 040 ----
mean loss: 1182.17
train mean loss: 1191.76
epoch train time: 0:00:00.200901
elapsed time: 0:00:19.303926
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:43:24.601682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1156.00
 ---- batch: 020 ----
mean loss: 1141.21
 ---- batch: 030 ----
mean loss: 1123.01
 ---- batch: 040 ----
mean loss: 1099.10
train mean loss: 1128.36
epoch train time: 0:00:00.204557
elapsed time: 0:00:19.508605
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:43:24.806359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1095.36
 ---- batch: 020 ----
mean loss: 1043.31
 ---- batch: 030 ----
mean loss: 981.05
 ---- batch: 040 ----
mean loss: 947.71
train mean loss: 1010.00
epoch train time: 0:00:00.200487
elapsed time: 0:00:19.709214
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:43:25.006985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.42
 ---- batch: 020 ----
mean loss: 900.83
 ---- batch: 030 ----
mean loss: 890.03
 ---- batch: 040 ----
mean loss: 870.04
train mean loss: 896.98
epoch train time: 0:00:00.198112
elapsed time: 0:00:19.907503
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:43:25.205265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.82
 ---- batch: 020 ----
mean loss: 850.92
 ---- batch: 030 ----
mean loss: 817.66
 ---- batch: 040 ----
mean loss: 794.29
train mean loss: 824.82
epoch train time: 0:00:00.202384
elapsed time: 0:00:20.110017
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:43:25.407813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 792.66
 ---- batch: 020 ----
mean loss: 769.63
 ---- batch: 030 ----
mean loss: 740.18
 ---- batch: 040 ----
mean loss: 737.21
train mean loss: 757.61
epoch train time: 0:00:00.196139
elapsed time: 0:00:20.306312
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:43:25.604063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 724.00
 ---- batch: 020 ----
mean loss: 699.93
 ---- batch: 030 ----
mean loss: 697.00
 ---- batch: 040 ----
mean loss: 675.33
train mean loss: 695.57
epoch train time: 0:00:00.199352
elapsed time: 0:00:20.505781
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:43:25.803535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 667.88
 ---- batch: 020 ----
mean loss: 647.00
 ---- batch: 030 ----
mean loss: 631.09
 ---- batch: 040 ----
mean loss: 619.72
train mean loss: 638.94
epoch train time: 0:00:00.197034
elapsed time: 0:00:20.702940
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:43:26.000703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.67
 ---- batch: 020 ----
mean loss: 582.68
 ---- batch: 030 ----
mean loss: 591.18
 ---- batch: 040 ----
mean loss: 574.69
train mean loss: 586.57
epoch train time: 0:00:00.198685
elapsed time: 0:00:20.901761
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:43:26.199512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 543.89
 ---- batch: 020 ----
mean loss: 553.75
 ---- batch: 030 ----
mean loss: 541.29
 ---- batch: 040 ----
mean loss: 527.87
train mean loss: 538.82
epoch train time: 0:00:00.201511
elapsed time: 0:00:21.103386
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:43:26.401137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.03
 ---- batch: 020 ----
mean loss: 509.10
 ---- batch: 030 ----
mean loss: 491.68
 ---- batch: 040 ----
mean loss: 482.18
train mean loss: 496.40
epoch train time: 0:00:00.196510
elapsed time: 0:00:21.300074
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:43:26.597820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.59
 ---- batch: 020 ----
mean loss: 468.57
 ---- batch: 030 ----
mean loss: 447.27
 ---- batch: 040 ----
mean loss: 458.92
train mean loss: 457.52
epoch train time: 0:00:00.197537
elapsed time: 0:00:21.497724
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:43:26.795479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.81
 ---- batch: 020 ----
mean loss: 421.38
 ---- batch: 030 ----
mean loss: 416.28
 ---- batch: 040 ----
mean loss: 412.91
train mean loss: 421.07
epoch train time: 0:00:00.201820
elapsed time: 0:00:21.699679
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:43:26.997434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.95
 ---- batch: 020 ----
mean loss: 393.18
 ---- batch: 030 ----
mean loss: 385.69
 ---- batch: 040 ----
mean loss: 372.48
train mean loss: 388.37
epoch train time: 0:00:00.206735
elapsed time: 0:00:21.906540
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:43:27.204297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.26
 ---- batch: 020 ----
mean loss: 362.11
 ---- batch: 030 ----
mean loss: 356.31
 ---- batch: 040 ----
mean loss: 355.60
train mean loss: 358.46
epoch train time: 0:00:00.207884
elapsed time: 0:00:22.114553
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:43:27.412326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.37
 ---- batch: 020 ----
mean loss: 336.77
 ---- batch: 030 ----
mean loss: 329.84
 ---- batch: 040 ----
mean loss: 323.46
train mean loss: 331.33
epoch train time: 0:00:00.208975
elapsed time: 0:00:22.323675
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:43:27.621432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.63
 ---- batch: 020 ----
mean loss: 309.67
 ---- batch: 030 ----
mean loss: 302.69
 ---- batch: 040 ----
mean loss: 298.78
train mean loss: 305.72
epoch train time: 0:00:00.212027
elapsed time: 0:00:22.535834
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:43:27.833595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.67
 ---- batch: 020 ----
mean loss: 287.44
 ---- batch: 030 ----
mean loss: 277.92
 ---- batch: 040 ----
mean loss: 277.36
train mean loss: 283.22
epoch train time: 0:00:00.215691
elapsed time: 0:00:22.751665
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:43:28.049438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.84
 ---- batch: 020 ----
mean loss: 264.49
 ---- batch: 030 ----
mean loss: 259.93
 ---- batch: 040 ----
mean loss: 253.90
train mean loss: 262.27
epoch train time: 0:00:00.208138
elapsed time: 0:00:22.959946
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:43:28.257700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.15
 ---- batch: 020 ----
mean loss: 245.10
 ---- batch: 030 ----
mean loss: 243.46
 ---- batch: 040 ----
mean loss: 237.27
train mean loss: 243.50
epoch train time: 0:00:00.207261
elapsed time: 0:00:23.167328
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:43:28.465081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.43
 ---- batch: 020 ----
mean loss: 221.26
 ---- batch: 030 ----
mean loss: 229.60
 ---- batch: 040 ----
mean loss: 222.19
train mean loss: 226.01
epoch train time: 0:00:00.209095
elapsed time: 0:00:23.376562
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:43:28.674326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.94
 ---- batch: 020 ----
mean loss: 214.23
 ---- batch: 030 ----
mean loss: 210.45
 ---- batch: 040 ----
mean loss: 203.51
train mean loss: 210.60
epoch train time: 0:00:00.209616
elapsed time: 0:00:23.586314
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:43:28.884075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.12
 ---- batch: 020 ----
mean loss: 204.96
 ---- batch: 030 ----
mean loss: 191.55
 ---- batch: 040 ----
mean loss: 187.88
train mean loss: 195.97
epoch train time: 0:00:00.201357
elapsed time: 0:00:23.787801
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:43:29.085555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.98
 ---- batch: 020 ----
mean loss: 186.66
 ---- batch: 030 ----
mean loss: 183.20
 ---- batch: 040 ----
mean loss: 180.26
train mean loss: 183.91
epoch train time: 0:00:00.197598
elapsed time: 0:00:23.985519
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:43:29.283284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.47
 ---- batch: 020 ----
mean loss: 174.32
 ---- batch: 030 ----
mean loss: 171.25
 ---- batch: 040 ----
mean loss: 167.88
train mean loss: 171.92
epoch train time: 0:00:00.194417
elapsed time: 0:00:24.180070
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:43:29.477824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.85
 ---- batch: 020 ----
mean loss: 160.12
 ---- batch: 030 ----
mean loss: 163.73
 ---- batch: 040 ----
mean loss: 157.66
train mean loss: 160.58
epoch train time: 0:00:00.192981
elapsed time: 0:00:24.373171
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:43:29.670936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.20
 ---- batch: 020 ----
mean loss: 152.04
 ---- batch: 030 ----
mean loss: 147.79
 ---- batch: 040 ----
mean loss: 150.38
train mean loss: 150.80
epoch train time: 0:00:00.193077
elapsed time: 0:00:24.566376
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:43:29.864142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.41
 ---- batch: 020 ----
mean loss: 142.00
 ---- batch: 030 ----
mean loss: 140.41
 ---- batch: 040 ----
mean loss: 143.36
train mean loss: 141.84
epoch train time: 0:00:00.192718
elapsed time: 0:00:24.759229
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:43:30.056983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.69
 ---- batch: 020 ----
mean loss: 135.06
 ---- batch: 030 ----
mean loss: 131.12
 ---- batch: 040 ----
mean loss: 131.44
train mean loss: 134.20
epoch train time: 0:00:00.192807
elapsed time: 0:00:24.952151
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:43:30.249902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.51
 ---- batch: 020 ----
mean loss: 128.51
 ---- batch: 030 ----
mean loss: 125.45
 ---- batch: 040 ----
mean loss: 126.01
train mean loss: 127.05
epoch train time: 0:00:00.196122
elapsed time: 0:00:25.148384
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:43:30.446135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.11
 ---- batch: 020 ----
mean loss: 119.21
 ---- batch: 030 ----
mean loss: 119.66
 ---- batch: 040 ----
mean loss: 117.97
train mean loss: 120.12
epoch train time: 0:00:00.194699
elapsed time: 0:00:25.343224
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:43:30.640978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.19
 ---- batch: 020 ----
mean loss: 114.03
 ---- batch: 030 ----
mean loss: 115.79
 ---- batch: 040 ----
mean loss: 112.33
train mean loss: 115.00
epoch train time: 0:00:00.198969
elapsed time: 0:00:25.542306
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:43:30.840056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.59
 ---- batch: 020 ----
mean loss: 108.70
 ---- batch: 030 ----
mean loss: 108.13
 ---- batch: 040 ----
mean loss: 108.73
train mean loss: 109.01
epoch train time: 0:00:00.201622
elapsed time: 0:00:25.744072
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:43:31.041831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.23
 ---- batch: 020 ----
mean loss: 101.76
 ---- batch: 030 ----
mean loss: 105.49
 ---- batch: 040 ----
mean loss: 102.58
train mean loss: 104.23
epoch train time: 0:00:00.203126
elapsed time: 0:00:25.947325
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:43:31.245095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.29
 ---- batch: 020 ----
mean loss: 102.27
 ---- batch: 030 ----
mean loss: 101.75
 ---- batch: 040 ----
mean loss: 96.98
train mean loss: 100.67
epoch train time: 0:00:00.208568
elapsed time: 0:00:26.156029
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:43:31.453784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.75
 ---- batch: 020 ----
mean loss: 96.28
 ---- batch: 030 ----
mean loss: 94.12
 ---- batch: 040 ----
mean loss: 95.62
train mean loss: 96.33
epoch train time: 0:00:00.205711
elapsed time: 0:00:26.361863
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:43:31.659635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 93.29
 ---- batch: 020 ----
mean loss: 91.66
 ---- batch: 030 ----
mean loss: 93.31
 ---- batch: 040 ----
mean loss: 91.21
train mean loss: 92.41
epoch train time: 0:00:00.204951
elapsed time: 0:00:26.566974
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:43:31.864741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.08
 ---- batch: 020 ----
mean loss: 87.86
 ---- batch: 030 ----
mean loss: 87.58
 ---- batch: 040 ----
mean loss: 89.97
train mean loss: 88.90
epoch train time: 0:00:00.203514
elapsed time: 0:00:26.770656
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:43:32.068435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.44
 ---- batch: 020 ----
mean loss: 87.48
 ---- batch: 030 ----
mean loss: 84.96
 ---- batch: 040 ----
mean loss: 85.10
train mean loss: 86.57
epoch train time: 0:00:00.203415
elapsed time: 0:00:26.974225
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:43:32.271994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.03
 ---- batch: 020 ----
mean loss: 83.45
 ---- batch: 030 ----
mean loss: 85.08
 ---- batch: 040 ----
mean loss: 82.30
train mean loss: 83.50
epoch train time: 0:00:00.205199
elapsed time: 0:00:27.179561
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:43:32.477314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.88
 ---- batch: 020 ----
mean loss: 81.41
 ---- batch: 030 ----
mean loss: 79.03
 ---- batch: 040 ----
mean loss: 81.18
train mean loss: 81.18
epoch train time: 0:00:00.200689
elapsed time: 0:00:27.380368
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:43:32.678122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.36
 ---- batch: 020 ----
mean loss: 81.05
 ---- batch: 030 ----
mean loss: 80.72
 ---- batch: 040 ----
mean loss: 81.58
train mean loss: 79.85
epoch train time: 0:00:00.204067
elapsed time: 0:00:27.584579
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:43:32.882333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.29
 ---- batch: 020 ----
mean loss: 76.14
 ---- batch: 030 ----
mean loss: 80.00
 ---- batch: 040 ----
mean loss: 76.60
train mean loss: 77.42
epoch train time: 0:00:00.202834
elapsed time: 0:00:27.787536
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:43:33.085291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.33
 ---- batch: 020 ----
mean loss: 72.83
 ---- batch: 030 ----
mean loss: 74.91
 ---- batch: 040 ----
mean loss: 77.00
train mean loss: 74.72
epoch train time: 0:00:00.202200
elapsed time: 0:00:27.989856
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:43:33.287608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.17
 ---- batch: 020 ----
mean loss: 74.13
 ---- batch: 030 ----
mean loss: 71.57
 ---- batch: 040 ----
mean loss: 72.93
train mean loss: 73.32
epoch train time: 0:00:00.199422
elapsed time: 0:00:28.189392
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:43:33.487143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.84
 ---- batch: 020 ----
mean loss: 71.62
 ---- batch: 030 ----
mean loss: 71.78
 ---- batch: 040 ----
mean loss: 70.78
train mean loss: 71.55
epoch train time: 0:00:00.197103
elapsed time: 0:00:28.386638
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:43:33.684461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.69
 ---- batch: 020 ----
mean loss: 71.70
 ---- batch: 030 ----
mean loss: 72.09
 ---- batch: 040 ----
mean loss: 67.97
train mean loss: 70.55
epoch train time: 0:00:00.204303
elapsed time: 0:00:28.591143
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:43:33.888904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.55
 ---- batch: 020 ----
mean loss: 68.38
 ---- batch: 030 ----
mean loss: 70.49
 ---- batch: 040 ----
mean loss: 70.30
train mean loss: 70.44
epoch train time: 0:00:00.204990
elapsed time: 0:00:28.796271
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:43:34.094027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.40
 ---- batch: 020 ----
mean loss: 66.21
 ---- batch: 030 ----
mean loss: 64.83
 ---- batch: 040 ----
mean loss: 69.73
train mean loss: 67.67
epoch train time: 0:00:00.199544
elapsed time: 0:00:28.995937
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:43:34.293690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.67
 ---- batch: 020 ----
mean loss: 64.61
 ---- batch: 030 ----
mean loss: 66.19
 ---- batch: 040 ----
mean loss: 67.85
train mean loss: 66.73
epoch train time: 0:00:00.200605
elapsed time: 0:00:29.196659
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:43:34.494430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.87
 ---- batch: 020 ----
mean loss: 67.78
 ---- batch: 030 ----
mean loss: 67.05
 ---- batch: 040 ----
mean loss: 63.92
train mean loss: 65.72
epoch train time: 0:00:00.201390
elapsed time: 0:00:29.398216
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:43:34.696003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.35
 ---- batch: 020 ----
mean loss: 67.97
 ---- batch: 030 ----
mean loss: 64.52
 ---- batch: 040 ----
mean loss: 64.58
train mean loss: 64.58
epoch train time: 0:00:00.200345
elapsed time: 0:00:29.598749
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:43:34.896505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.33
 ---- batch: 020 ----
mean loss: 61.37
 ---- batch: 030 ----
mean loss: 64.76
 ---- batch: 040 ----
mean loss: 65.08
train mean loss: 63.79
epoch train time: 0:00:00.192957
elapsed time: 0:00:29.791840
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:43:35.089593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.55
 ---- batch: 020 ----
mean loss: 63.33
 ---- batch: 030 ----
mean loss: 63.41
 ---- batch: 040 ----
mean loss: 63.07
train mean loss: 63.16
epoch train time: 0:00:00.196072
elapsed time: 0:00:29.988059
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:43:35.285829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.86
 ---- batch: 020 ----
mean loss: 65.65
 ---- batch: 030 ----
mean loss: 63.08
 ---- batch: 040 ----
mean loss: 59.71
train mean loss: 63.20
epoch train time: 0:00:00.201462
elapsed time: 0:00:30.189656
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:43:35.487411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.50
 ---- batch: 020 ----
mean loss: 57.68
 ---- batch: 030 ----
mean loss: 62.64
 ---- batch: 040 ----
mean loss: 63.18
train mean loss: 61.40
epoch train time: 0:00:00.202439
elapsed time: 0:00:30.392212
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:43:35.689989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.22
 ---- batch: 020 ----
mean loss: 62.25
 ---- batch: 030 ----
mean loss: 61.67
 ---- batch: 040 ----
mean loss: 60.23
train mean loss: 61.02
epoch train time: 0:00:00.208018
elapsed time: 0:00:30.600384
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:43:35.898188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.90
 ---- batch: 020 ----
mean loss: 56.85
 ---- batch: 030 ----
mean loss: 62.59
 ---- batch: 040 ----
mean loss: 60.46
train mean loss: 60.11
epoch train time: 0:00:00.208019
elapsed time: 0:00:30.808585
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:43:36.106341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.36
 ---- batch: 020 ----
mean loss: 58.72
 ---- batch: 030 ----
mean loss: 60.90
 ---- batch: 040 ----
mean loss: 60.42
train mean loss: 60.19
epoch train time: 0:00:00.200865
elapsed time: 0:00:31.009577
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:43:36.307341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.31
 ---- batch: 020 ----
mean loss: 58.31
 ---- batch: 030 ----
mean loss: 59.45
 ---- batch: 040 ----
mean loss: 59.01
train mean loss: 59.54
epoch train time: 0:00:00.199874
elapsed time: 0:00:31.209583
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:43:36.507338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.50
 ---- batch: 020 ----
mean loss: 58.55
 ---- batch: 030 ----
mean loss: 59.92
 ---- batch: 040 ----
mean loss: 61.68
train mean loss: 59.20
epoch train time: 0:00:00.203160
elapsed time: 0:00:31.412867
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:43:36.710622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.11
 ---- batch: 020 ----
mean loss: 57.19
 ---- batch: 030 ----
mean loss: 56.87
 ---- batch: 040 ----
mean loss: 59.28
train mean loss: 58.64
epoch train time: 0:00:00.203720
elapsed time: 0:00:31.616708
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:43:36.914480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.49
 ---- batch: 020 ----
mean loss: 59.58
 ---- batch: 030 ----
mean loss: 58.91
 ---- batch: 040 ----
mean loss: 59.92
train mean loss: 58.51
epoch train time: 0:00:00.203429
elapsed time: 0:00:31.820277
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:43:37.118032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.03
 ---- batch: 020 ----
mean loss: 56.01
 ---- batch: 030 ----
mean loss: 57.27
 ---- batch: 040 ----
mean loss: 56.36
train mean loss: 57.29
epoch train time: 0:00:00.204599
elapsed time: 0:00:32.025009
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:43:37.322787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.07
 ---- batch: 020 ----
mean loss: 57.46
 ---- batch: 030 ----
mean loss: 55.48
 ---- batch: 040 ----
mean loss: 58.33
train mean loss: 56.79
epoch train time: 0:00:00.198394
elapsed time: 0:00:32.223550
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:43:37.521302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.89
 ---- batch: 020 ----
mean loss: 56.17
 ---- batch: 030 ----
mean loss: 55.05
 ---- batch: 040 ----
mean loss: 59.08
train mean loss: 56.50
epoch train time: 0:00:00.197882
elapsed time: 0:00:32.421546
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:43:37.719298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.07
 ---- batch: 020 ----
mean loss: 55.85
 ---- batch: 030 ----
mean loss: 56.53
 ---- batch: 040 ----
mean loss: 54.26
train mean loss: 55.95
epoch train time: 0:00:00.199131
elapsed time: 0:00:32.620792
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:43:37.918546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.54
 ---- batch: 020 ----
mean loss: 54.63
 ---- batch: 030 ----
mean loss: 53.35
 ---- batch: 040 ----
mean loss: 54.39
train mean loss: 55.15
epoch train time: 0:00:00.204732
elapsed time: 0:00:32.825674
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:43:38.123427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.42
 ---- batch: 020 ----
mean loss: 52.74
 ---- batch: 030 ----
mean loss: 55.60
 ---- batch: 040 ----
mean loss: 58.54
train mean loss: 55.18
epoch train time: 0:00:00.201206
elapsed time: 0:00:33.027016
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:43:38.324785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.20
 ---- batch: 020 ----
mean loss: 59.17
 ---- batch: 030 ----
mean loss: 53.23
 ---- batch: 040 ----
mean loss: 55.72
train mean loss: 55.79
epoch train time: 0:00:00.197096
elapsed time: 0:00:33.224276
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:43:38.522034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.72
 ---- batch: 020 ----
mean loss: 55.76
 ---- batch: 030 ----
mean loss: 52.93
 ---- batch: 040 ----
mean loss: 52.38
train mean loss: 54.04
epoch train time: 0:00:00.202088
elapsed time: 0:00:33.426482
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:43:38.724243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.65
 ---- batch: 020 ----
mean loss: 53.63
 ---- batch: 030 ----
mean loss: 54.66
 ---- batch: 040 ----
mean loss: 56.29
train mean loss: 54.30
epoch train time: 0:00:00.198166
elapsed time: 0:00:33.624808
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:43:38.922563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.53
 ---- batch: 020 ----
mean loss: 56.05
 ---- batch: 030 ----
mean loss: 52.35
 ---- batch: 040 ----
mean loss: 53.19
train mean loss: 53.93
epoch train time: 0:00:00.200343
elapsed time: 0:00:33.825269
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:43:39.123021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.83
 ---- batch: 020 ----
mean loss: 55.49
 ---- batch: 030 ----
mean loss: 52.39
 ---- batch: 040 ----
mean loss: 55.28
train mean loss: 53.69
epoch train time: 0:00:00.197584
elapsed time: 0:00:34.023004
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:43:39.320756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.19
 ---- batch: 020 ----
mean loss: 54.82
 ---- batch: 030 ----
mean loss: 53.79
 ---- batch: 040 ----
mean loss: 51.99
train mean loss: 53.70
epoch train time: 0:00:00.208305
elapsed time: 0:00:34.231427
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:43:39.529179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.35
 ---- batch: 020 ----
mean loss: 54.32
 ---- batch: 030 ----
mean loss: 49.16
 ---- batch: 040 ----
mean loss: 52.04
train mean loss: 52.46
epoch train time: 0:00:00.210465
elapsed time: 0:00:34.442036
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:43:39.739819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.68
 ---- batch: 020 ----
mean loss: 52.21
 ---- batch: 030 ----
mean loss: 48.70
 ---- batch: 040 ----
mean loss: 52.43
train mean loss: 52.05
epoch train time: 0:00:00.208073
elapsed time: 0:00:34.650284
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:43:39.948039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.20
 ---- batch: 020 ----
mean loss: 52.21
 ---- batch: 030 ----
mean loss: 51.56
 ---- batch: 040 ----
mean loss: 55.18
train mean loss: 52.97
epoch train time: 0:00:00.206124
elapsed time: 0:00:34.856540
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:43:40.154297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.83
 ---- batch: 020 ----
mean loss: 55.02
 ---- batch: 030 ----
mean loss: 56.52
 ---- batch: 040 ----
mean loss: 55.82
train mean loss: 55.51
epoch train time: 0:00:00.206179
elapsed time: 0:00:35.062845
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:43:40.360599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.94
 ---- batch: 020 ----
mean loss: 51.11
 ---- batch: 030 ----
mean loss: 52.06
 ---- batch: 040 ----
mean loss: 52.35
train mean loss: 51.79
epoch train time: 0:00:00.204715
elapsed time: 0:00:35.267682
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:43:40.565437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.96
 ---- batch: 020 ----
mean loss: 49.69
 ---- batch: 030 ----
mean loss: 50.78
 ---- batch: 040 ----
mean loss: 52.51
train mean loss: 51.25
epoch train time: 0:00:00.205632
elapsed time: 0:00:35.473434
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:43:40.771187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.96
 ---- batch: 020 ----
mean loss: 48.90
 ---- batch: 030 ----
mean loss: 52.60
 ---- batch: 040 ----
mean loss: 53.07
train mean loss: 51.18
epoch train time: 0:00:00.203355
elapsed time: 0:00:35.676908
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:43:40.974678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.37
 ---- batch: 020 ----
mean loss: 51.18
 ---- batch: 030 ----
mean loss: 50.78
 ---- batch: 040 ----
mean loss: 48.08
train mean loss: 50.28
epoch train time: 0:00:00.205074
elapsed time: 0:00:35.882113
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:43:41.179907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.37
 ---- batch: 020 ----
mean loss: 51.51
 ---- batch: 030 ----
mean loss: 47.37
 ---- batch: 040 ----
mean loss: 51.39
train mean loss: 50.44
epoch train time: 0:00:00.199753
elapsed time: 0:00:36.082025
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:43:41.379810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.89
 ---- batch: 020 ----
mean loss: 51.29
 ---- batch: 030 ----
mean loss: 49.63
 ---- batch: 040 ----
mean loss: 53.87
train mean loss: 50.77
epoch train time: 0:00:00.198577
elapsed time: 0:00:36.280795
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:43:41.578549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.05
 ---- batch: 020 ----
mean loss: 48.95
 ---- batch: 030 ----
mean loss: 51.27
 ---- batch: 040 ----
mean loss: 50.34
train mean loss: 49.93
epoch train time: 0:00:00.199237
elapsed time: 0:00:36.480163
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:43:41.777910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.23
 ---- batch: 020 ----
mean loss: 50.07
 ---- batch: 030 ----
mean loss: 49.21
 ---- batch: 040 ----
mean loss: 50.05
train mean loss: 50.20
epoch train time: 0:00:00.204552
elapsed time: 0:00:36.684830
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:43:41.982583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.74
 ---- batch: 020 ----
mean loss: 48.05
 ---- batch: 030 ----
mean loss: 48.26
 ---- batch: 040 ----
mean loss: 52.72
train mean loss: 49.53
epoch train time: 0:00:00.197719
elapsed time: 0:00:36.882666
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:43:42.180456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.57
 ---- batch: 020 ----
mean loss: 47.59
 ---- batch: 030 ----
mean loss: 50.56
 ---- batch: 040 ----
mean loss: 46.18
train mean loss: 49.03
epoch train time: 0:00:00.200995
elapsed time: 0:00:37.083814
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:43:42.381566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.68
 ---- batch: 020 ----
mean loss: 47.58
 ---- batch: 030 ----
mean loss: 50.15
 ---- batch: 040 ----
mean loss: 49.05
train mean loss: 48.67
epoch train time: 0:00:00.199254
elapsed time: 0:00:37.283185
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:43:42.580945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.56
 ---- batch: 020 ----
mean loss: 49.96
 ---- batch: 030 ----
mean loss: 48.18
 ---- batch: 040 ----
mean loss: 49.01
train mean loss: 49.49
epoch train time: 0:00:00.203095
elapsed time: 0:00:37.486420
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:43:42.784172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.94
 ---- batch: 020 ----
mean loss: 49.82
 ---- batch: 030 ----
mean loss: 48.78
 ---- batch: 040 ----
mean loss: 47.18
train mean loss: 48.95
epoch train time: 0:00:00.194614
elapsed time: 0:00:37.681189
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:43:42.978954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.43
 ---- batch: 020 ----
mean loss: 49.71
 ---- batch: 030 ----
mean loss: 47.81
 ---- batch: 040 ----
mean loss: 46.17
train mean loss: 48.07
epoch train time: 0:00:00.194413
elapsed time: 0:00:37.875729
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:43:43.173480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.76
 ---- batch: 020 ----
mean loss: 45.83
 ---- batch: 030 ----
mean loss: 49.59
 ---- batch: 040 ----
mean loss: 48.31
train mean loss: 48.02
epoch train time: 0:00:00.199879
elapsed time: 0:00:38.075741
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:43:43.373493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.55
 ---- batch: 020 ----
mean loss: 51.41
 ---- batch: 030 ----
mean loss: 50.66
 ---- batch: 040 ----
mean loss: 51.07
train mean loss: 50.69
epoch train time: 0:00:00.204330
elapsed time: 0:00:38.280188
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:43:43.577942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.19
 ---- batch: 020 ----
mean loss: 48.16
 ---- batch: 030 ----
mean loss: 48.04
 ---- batch: 040 ----
mean loss: 53.23
train mean loss: 49.07
epoch train time: 0:00:00.213280
elapsed time: 0:00:38.493608
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:43:43.791362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.34
 ---- batch: 020 ----
mean loss: 52.49
 ---- batch: 030 ----
mean loss: 47.28
 ---- batch: 040 ----
mean loss: 46.45
train mean loss: 49.99
epoch train time: 0:00:00.208543
elapsed time: 0:00:38.702271
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:43:44.000025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.49
 ---- batch: 020 ----
mean loss: 46.41
 ---- batch: 030 ----
mean loss: 47.05
 ---- batch: 040 ----
mean loss: 49.65
train mean loss: 47.94
epoch train time: 0:00:00.206634
elapsed time: 0:00:38.909036
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:43:44.206792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.34
 ---- batch: 020 ----
mean loss: 47.88
 ---- batch: 030 ----
mean loss: 49.37
 ---- batch: 040 ----
mean loss: 45.41
train mean loss: 48.05
epoch train time: 0:00:00.209385
elapsed time: 0:00:39.118554
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:43:44.416311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.10
 ---- batch: 020 ----
mean loss: 47.45
 ---- batch: 030 ----
mean loss: 47.96
 ---- batch: 040 ----
mean loss: 47.44
train mean loss: 46.97
epoch train time: 0:00:00.208045
elapsed time: 0:00:39.326773
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:43:44.624557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.81
 ---- batch: 020 ----
mean loss: 48.17
 ---- batch: 030 ----
mean loss: 43.83
 ---- batch: 040 ----
mean loss: 49.55
train mean loss: 46.75
epoch train time: 0:00:00.205996
elapsed time: 0:00:39.532921
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:43:44.830690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.49
 ---- batch: 020 ----
mean loss: 46.80
 ---- batch: 030 ----
mean loss: 46.84
 ---- batch: 040 ----
mean loss: 46.82
train mean loss: 46.76
epoch train time: 0:00:00.201846
elapsed time: 0:00:39.734911
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:43:45.032674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.28
 ---- batch: 020 ----
mean loss: 46.32
 ---- batch: 030 ----
mean loss: 46.79
 ---- batch: 040 ----
mean loss: 44.62
train mean loss: 46.18
epoch train time: 0:00:00.204440
elapsed time: 0:00:39.939502
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:43:45.237255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.80
 ---- batch: 020 ----
mean loss: 44.35
 ---- batch: 030 ----
mean loss: 45.16
 ---- batch: 040 ----
mean loss: 46.23
train mean loss: 45.77
epoch train time: 0:00:00.203199
elapsed time: 0:00:40.142818
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:43:45.440569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.79
 ---- batch: 020 ----
mean loss: 45.37
 ---- batch: 030 ----
mean loss: 49.42
 ---- batch: 040 ----
mean loss: 44.82
train mean loss: 45.88
epoch train time: 0:00:00.201893
elapsed time: 0:00:40.344823
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:43:45.642576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.62
 ---- batch: 020 ----
mean loss: 46.95
 ---- batch: 030 ----
mean loss: 46.29
 ---- batch: 040 ----
mean loss: 44.86
train mean loss: 46.74
epoch train time: 0:00:00.197059
elapsed time: 0:00:40.542012
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:43:45.839759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.35
 ---- batch: 020 ----
mean loss: 44.66
 ---- batch: 030 ----
mean loss: 45.10
 ---- batch: 040 ----
mean loss: 45.66
train mean loss: 45.90
epoch train time: 0:00:00.195032
elapsed time: 0:00:40.737153
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:43:46.034906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.28
 ---- batch: 020 ----
mean loss: 45.40
 ---- batch: 030 ----
mean loss: 44.97
 ---- batch: 040 ----
mean loss: 47.13
train mean loss: 45.49
epoch train time: 0:00:00.194652
elapsed time: 0:00:40.931918
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:43:46.229670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.65
 ---- batch: 020 ----
mean loss: 46.47
 ---- batch: 030 ----
mean loss: 47.79
 ---- batch: 040 ----
mean loss: 45.28
train mean loss: 46.55
epoch train time: 0:00:00.193699
elapsed time: 0:00:41.125732
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:43:46.423514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.32
 ---- batch: 020 ----
mean loss: 46.16
 ---- batch: 030 ----
mean loss: 43.39
 ---- batch: 040 ----
mean loss: 46.54
train mean loss: 45.40
epoch train time: 0:00:00.190842
elapsed time: 0:00:41.316719
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:43:46.614473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.82
 ---- batch: 020 ----
mean loss: 46.10
 ---- batch: 030 ----
mean loss: 43.70
 ---- batch: 040 ----
mean loss: 44.23
train mean loss: 45.69
epoch train time: 0:00:00.194125
elapsed time: 0:00:41.510961
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:43:46.808712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.98
 ---- batch: 020 ----
mean loss: 44.92
 ---- batch: 030 ----
mean loss: 44.51
 ---- batch: 040 ----
mean loss: 42.87
train mean loss: 44.80
epoch train time: 0:00:00.196469
elapsed time: 0:00:41.707554
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:43:47.005323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.23
 ---- batch: 020 ----
mean loss: 43.84
 ---- batch: 030 ----
mean loss: 44.93
 ---- batch: 040 ----
mean loss: 45.17
train mean loss: 44.12
epoch train time: 0:00:00.195296
elapsed time: 0:00:41.902982
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:43:47.200734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.26
 ---- batch: 020 ----
mean loss: 47.03
 ---- batch: 030 ----
mean loss: 45.31
 ---- batch: 040 ----
mean loss: 44.34
train mean loss: 45.55
epoch train time: 0:00:00.193571
elapsed time: 0:00:42.096668
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:43:47.394419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.10
 ---- batch: 020 ----
mean loss: 43.79
 ---- batch: 030 ----
mean loss: 44.14
 ---- batch: 040 ----
mean loss: 45.09
train mean loss: 43.96
epoch train time: 0:00:00.194713
elapsed time: 0:00:42.291494
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:43:47.589267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.24
 ---- batch: 020 ----
mean loss: 45.01
 ---- batch: 030 ----
mean loss: 44.34
 ---- batch: 040 ----
mean loss: 43.30
train mean loss: 43.89
epoch train time: 0:00:00.207428
elapsed time: 0:00:42.499075
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:43:47.796832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.33
 ---- batch: 020 ----
mean loss: 43.76
 ---- batch: 030 ----
mean loss: 46.14
 ---- batch: 040 ----
mean loss: 45.21
train mean loss: 44.56
epoch train time: 0:00:00.208847
elapsed time: 0:00:42.708051
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:43:48.005808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.91
 ---- batch: 020 ----
mean loss: 48.43
 ---- batch: 030 ----
mean loss: 45.64
 ---- batch: 040 ----
mean loss: 43.70
train mean loss: 45.24
epoch train time: 0:00:00.206812
elapsed time: 0:00:42.914991
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:43:48.212749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.79
 ---- batch: 020 ----
mean loss: 42.77
 ---- batch: 030 ----
mean loss: 43.07
 ---- batch: 040 ----
mean loss: 43.51
train mean loss: 43.49
epoch train time: 0:00:00.207819
elapsed time: 0:00:43.122954
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:43:48.420711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.75
 ---- batch: 020 ----
mean loss: 40.71
 ---- batch: 030 ----
mean loss: 42.75
 ---- batch: 040 ----
mean loss: 43.93
train mean loss: 43.02
epoch train time: 0:00:00.207341
elapsed time: 0:00:43.330448
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:43:48.628203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.12
 ---- batch: 020 ----
mean loss: 43.51
 ---- batch: 030 ----
mean loss: 44.95
 ---- batch: 040 ----
mean loss: 42.99
train mean loss: 43.07
epoch train time: 0:00:00.211129
elapsed time: 0:00:43.541700
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:43:48.839456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.25
 ---- batch: 020 ----
mean loss: 45.12
 ---- batch: 030 ----
mean loss: 44.10
 ---- batch: 040 ----
mean loss: 42.05
train mean loss: 43.10
epoch train time: 0:00:00.202246
elapsed time: 0:00:43.744077
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:43:49.041831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.26
 ---- batch: 020 ----
mean loss: 44.10
 ---- batch: 030 ----
mean loss: 43.15
 ---- batch: 040 ----
mean loss: 43.02
train mean loss: 43.03
epoch train time: 0:00:00.203809
elapsed time: 0:00:43.948010
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:43:49.245766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.05
 ---- batch: 020 ----
mean loss: 44.64
 ---- batch: 030 ----
mean loss: 42.70
 ---- batch: 040 ----
mean loss: 41.18
train mean loss: 42.50
epoch train time: 0:00:00.203776
elapsed time: 0:00:44.151909
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:43:49.449679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.56
 ---- batch: 020 ----
mean loss: 39.48
 ---- batch: 030 ----
mean loss: 42.75
 ---- batch: 040 ----
mean loss: 43.11
train mean loss: 42.20
epoch train time: 0:00:00.202285
elapsed time: 0:00:44.354328
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:43:49.652082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.02
 ---- batch: 020 ----
mean loss: 44.25
 ---- batch: 030 ----
mean loss: 41.92
 ---- batch: 040 ----
mean loss: 43.60
train mean loss: 42.91
epoch train time: 0:00:00.197171
elapsed time: 0:00:44.551615
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:43:49.849366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.29
 ---- batch: 020 ----
mean loss: 41.42
 ---- batch: 030 ----
mean loss: 43.02
 ---- batch: 040 ----
mean loss: 42.00
train mean loss: 42.63
epoch train time: 0:00:00.193183
elapsed time: 0:00:44.744925
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:43:50.042676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.68
 ---- batch: 020 ----
mean loss: 44.40
 ---- batch: 030 ----
mean loss: 40.83
 ---- batch: 040 ----
mean loss: 40.03
train mean loss: 41.52
epoch train time: 0:00:00.192817
elapsed time: 0:00:44.937863
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:43:50.235607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.80
 ---- batch: 020 ----
mean loss: 41.33
 ---- batch: 030 ----
mean loss: 42.18
 ---- batch: 040 ----
mean loss: 41.67
train mean loss: 41.83
epoch train time: 0:00:00.194416
elapsed time: 0:00:45.132383
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:43:50.430132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.74
 ---- batch: 020 ----
mean loss: 41.08
 ---- batch: 030 ----
mean loss: 42.46
 ---- batch: 040 ----
mean loss: 43.40
train mean loss: 41.94
epoch train time: 0:00:00.193200
elapsed time: 0:00:45.325693
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:43:50.623444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.55
 ---- batch: 020 ----
mean loss: 41.43
 ---- batch: 030 ----
mean loss: 40.50
 ---- batch: 040 ----
mean loss: 40.74
train mean loss: 40.76
epoch train time: 0:00:00.194847
elapsed time: 0:00:45.520658
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:43:50.818452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.93
 ---- batch: 020 ----
mean loss: 40.23
 ---- batch: 030 ----
mean loss: 41.46
 ---- batch: 040 ----
mean loss: 42.67
train mean loss: 41.31
epoch train time: 0:00:00.202212
elapsed time: 0:00:45.723051
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:43:51.020860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.36
 ---- batch: 020 ----
mean loss: 41.35
 ---- batch: 030 ----
mean loss: 38.70
 ---- batch: 040 ----
mean loss: 40.90
train mean loss: 41.12
epoch train time: 0:00:00.204344
elapsed time: 0:00:45.927573
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:43:51.225330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.74
 ---- batch: 020 ----
mean loss: 40.49
 ---- batch: 030 ----
mean loss: 40.76
 ---- batch: 040 ----
mean loss: 41.79
train mean loss: 41.11
epoch train time: 0:00:00.204372
elapsed time: 0:00:46.132107
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:43:51.429858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.52
 ---- batch: 020 ----
mean loss: 40.70
 ---- batch: 030 ----
mean loss: 42.50
 ---- batch: 040 ----
mean loss: 40.28
train mean loss: 41.11
epoch train time: 0:00:00.196784
elapsed time: 0:00:46.329067
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:43:51.626823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.48
 ---- batch: 020 ----
mean loss: 41.76
 ---- batch: 030 ----
mean loss: 42.61
 ---- batch: 040 ----
mean loss: 40.66
train mean loss: 41.00
epoch train time: 0:00:00.207768
elapsed time: 0:00:46.536953
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:43:51.834718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.21
 ---- batch: 020 ----
mean loss: 40.56
 ---- batch: 030 ----
mean loss: 40.18
 ---- batch: 040 ----
mean loss: 41.06
train mean loss: 40.01
epoch train time: 0:00:00.201235
elapsed time: 0:00:46.738319
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:43:52.036070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.80
 ---- batch: 020 ----
mean loss: 39.57
 ---- batch: 030 ----
mean loss: 40.30
 ---- batch: 040 ----
mean loss: 39.49
train mean loss: 39.85
epoch train time: 0:00:00.201896
elapsed time: 0:00:46.940341
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:43:52.238092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.20
 ---- batch: 020 ----
mean loss: 37.90
 ---- batch: 030 ----
mean loss: 41.20
 ---- batch: 040 ----
mean loss: 43.79
train mean loss: 40.53
epoch train time: 0:00:00.198786
elapsed time: 0:00:47.139260
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:43:52.437011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.46
 ---- batch: 020 ----
mean loss: 39.15
 ---- batch: 030 ----
mean loss: 39.37
 ---- batch: 040 ----
mean loss: 41.50
train mean loss: 39.97
epoch train time: 0:00:00.196988
elapsed time: 0:00:47.336371
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:43:52.634182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.62
 ---- batch: 020 ----
mean loss: 37.65
 ---- batch: 030 ----
mean loss: 39.82
 ---- batch: 040 ----
mean loss: 40.79
train mean loss: 39.68
epoch train time: 0:00:00.199708
elapsed time: 0:00:47.536258
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:43:52.834013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.11
 ---- batch: 020 ----
mean loss: 39.80
 ---- batch: 030 ----
mean loss: 38.64
 ---- batch: 040 ----
mean loss: 41.56
train mean loss: 39.32
epoch train time: 0:00:00.201180
elapsed time: 0:00:47.737559
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:43:53.035313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.72
 ---- batch: 020 ----
mean loss: 38.59
 ---- batch: 030 ----
mean loss: 38.75
 ---- batch: 040 ----
mean loss: 40.73
train mean loss: 39.20
epoch train time: 0:00:00.202618
elapsed time: 0:00:47.940297
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:43:53.238051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.80
 ---- batch: 020 ----
mean loss: 39.39
 ---- batch: 030 ----
mean loss: 39.99
 ---- batch: 040 ----
mean loss: 38.37
train mean loss: 39.13
epoch train time: 0:00:00.204540
elapsed time: 0:00:48.144956
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:43:53.442711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.88
 ---- batch: 020 ----
mean loss: 39.25
 ---- batch: 030 ----
mean loss: 38.92
 ---- batch: 040 ----
mean loss: 40.90
train mean loss: 39.70
epoch train time: 0:00:00.204999
elapsed time: 0:00:48.350075
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:43:53.647829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.80
 ---- batch: 020 ----
mean loss: 38.40
 ---- batch: 030 ----
mean loss: 39.63
 ---- batch: 040 ----
mean loss: 36.85
train mean loss: 38.78
epoch train time: 0:00:00.202108
elapsed time: 0:00:48.552299
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:43:53.850052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.81
 ---- batch: 020 ----
mean loss: 39.47
 ---- batch: 030 ----
mean loss: 38.34
 ---- batch: 040 ----
mean loss: 37.47
train mean loss: 38.47
epoch train time: 0:00:00.194069
elapsed time: 0:00:48.746482
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:43:54.044278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.74
 ---- batch: 020 ----
mean loss: 40.21
 ---- batch: 030 ----
mean loss: 43.32
 ---- batch: 040 ----
mean loss: 38.39
train mean loss: 40.04
epoch train time: 0:00:00.194954
elapsed time: 0:00:48.941647
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:43:54.239425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.71
 ---- batch: 020 ----
mean loss: 37.25
 ---- batch: 030 ----
mean loss: 40.63
 ---- batch: 040 ----
mean loss: 39.84
train mean loss: 38.57
epoch train time: 0:00:00.193433
elapsed time: 0:00:49.135220
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:43:54.432970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.90
 ---- batch: 020 ----
mean loss: 36.77
 ---- batch: 030 ----
mean loss: 38.63
 ---- batch: 040 ----
mean loss: 38.16
train mean loss: 38.04
epoch train time: 0:00:00.192110
elapsed time: 0:00:49.327471
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:43:54.625226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.35
 ---- batch: 020 ----
mean loss: 36.50
 ---- batch: 030 ----
mean loss: 39.63
 ---- batch: 040 ----
mean loss: 38.54
train mean loss: 37.91
epoch train time: 0:00:00.196017
elapsed time: 0:00:49.523621
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:43:54.821373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.56
 ---- batch: 020 ----
mean loss: 39.51
 ---- batch: 030 ----
mean loss: 38.67
 ---- batch: 040 ----
mean loss: 40.39
train mean loss: 39.19
epoch train time: 0:00:00.200124
elapsed time: 0:00:49.723860
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:43:55.021613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.44
 ---- batch: 020 ----
mean loss: 40.28
 ---- batch: 030 ----
mean loss: 38.47
 ---- batch: 040 ----
mean loss: 38.78
train mean loss: 38.67
epoch train time: 0:00:00.198135
elapsed time: 0:00:49.922138
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:43:55.219883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.43
 ---- batch: 020 ----
mean loss: 37.70
 ---- batch: 030 ----
mean loss: 37.89
 ---- batch: 040 ----
mean loss: 36.96
train mean loss: 37.63
epoch train time: 0:00:00.197465
elapsed time: 0:00:50.119714
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:43:55.417467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.97
 ---- batch: 020 ----
mean loss: 38.50
 ---- batch: 030 ----
mean loss: 38.13
 ---- batch: 040 ----
mean loss: 37.56
train mean loss: 38.18
epoch train time: 0:00:00.197868
elapsed time: 0:00:50.317701
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:43:55.615470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.44
 ---- batch: 020 ----
mean loss: 37.17
 ---- batch: 030 ----
mean loss: 37.24
 ---- batch: 040 ----
mean loss: 38.74
train mean loss: 37.15
epoch train time: 0:00:00.202710
elapsed time: 0:00:50.520554
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:43:55.818312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.34
 ---- batch: 020 ----
mean loss: 36.00
 ---- batch: 030 ----
mean loss: 35.14
 ---- batch: 040 ----
mean loss: 38.53
train mean loss: 36.87
epoch train time: 0:00:00.207877
elapsed time: 0:00:50.728596
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:43:56.026352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.08
 ---- batch: 020 ----
mean loss: 38.62
 ---- batch: 030 ----
mean loss: 36.22
 ---- batch: 040 ----
mean loss: 37.84
train mean loss: 37.81
epoch train time: 0:00:00.206836
elapsed time: 0:00:50.935557
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:43:56.233314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.17
 ---- batch: 020 ----
mean loss: 43.37
 ---- batch: 030 ----
mean loss: 36.88
 ---- batch: 040 ----
mean loss: 35.36
train mean loss: 39.18
epoch train time: 0:00:00.205445
elapsed time: 0:00:51.141128
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:43:56.438913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.55
 ---- batch: 020 ----
mean loss: 38.56
 ---- batch: 030 ----
mean loss: 36.33
 ---- batch: 040 ----
mean loss: 36.90
train mean loss: 37.08
epoch train time: 0:00:00.205680
elapsed time: 0:00:51.346968
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:43:56.644747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.16
 ---- batch: 020 ----
mean loss: 35.27
 ---- batch: 030 ----
mean loss: 35.62
 ---- batch: 040 ----
mean loss: 36.97
train mean loss: 36.35
epoch train time: 0:00:00.204774
elapsed time: 0:00:51.551891
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:43:56.849645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.83
 ---- batch: 020 ----
mean loss: 36.38
 ---- batch: 030 ----
mean loss: 34.15
 ---- batch: 040 ----
mean loss: 37.68
train mean loss: 36.32
epoch train time: 0:00:00.202037
elapsed time: 0:00:51.754056
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:43:57.051812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.99
 ---- batch: 020 ----
mean loss: 35.56
 ---- batch: 030 ----
mean loss: 39.67
 ---- batch: 040 ----
mean loss: 38.40
train mean loss: 37.28
epoch train time: 0:00:00.200825
elapsed time: 0:00:51.955006
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:43:57.252761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.00
 ---- batch: 020 ----
mean loss: 39.02
 ---- batch: 030 ----
mean loss: 35.86
 ---- batch: 040 ----
mean loss: 38.81
train mean loss: 36.97
epoch train time: 0:00:00.201653
elapsed time: 0:00:52.156792
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:43:57.454545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.51
 ---- batch: 020 ----
mean loss: 34.91
 ---- batch: 030 ----
mean loss: 37.84
 ---- batch: 040 ----
mean loss: 35.58
train mean loss: 36.28
epoch train time: 0:00:00.201743
elapsed time: 0:00:52.358651
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:43:57.656423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.74
 ---- batch: 020 ----
mean loss: 37.84
 ---- batch: 030 ----
mean loss: 36.78
 ---- batch: 040 ----
mean loss: 31.66
train mean loss: 36.20
epoch train time: 0:00:00.199437
elapsed time: 0:00:52.558225
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:43:57.855978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.35
 ---- batch: 020 ----
mean loss: 38.68
 ---- batch: 030 ----
mean loss: 37.47
 ---- batch: 040 ----
mean loss: 38.16
train mean loss: 37.03
epoch train time: 0:00:00.193836
elapsed time: 0:00:52.752179
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:43:58.049947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.09
 ---- batch: 020 ----
mean loss: 35.24
 ---- batch: 030 ----
mean loss: 35.83
 ---- batch: 040 ----
mean loss: 37.00
train mean loss: 35.62
epoch train time: 0:00:00.195420
elapsed time: 0:00:52.947734
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:43:58.245487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.21
 ---- batch: 020 ----
mean loss: 35.35
 ---- batch: 030 ----
mean loss: 37.05
 ---- batch: 040 ----
mean loss: 39.15
train mean loss: 36.32
epoch train time: 0:00:00.192632
elapsed time: 0:00:53.140485
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:43:58.438239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.80
 ---- batch: 020 ----
mean loss: 36.56
 ---- batch: 030 ----
mean loss: 34.18
 ---- batch: 040 ----
mean loss: 35.91
train mean loss: 35.49
epoch train time: 0:00:00.193822
elapsed time: 0:00:53.334423
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:43:58.632174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.45
 ---- batch: 020 ----
mean loss: 33.75
 ---- batch: 030 ----
mean loss: 36.04
 ---- batch: 040 ----
mean loss: 35.01
train mean loss: 34.96
epoch train time: 0:00:00.193458
elapsed time: 0:00:53.528001
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:43:58.825755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.81
 ---- batch: 020 ----
mean loss: 34.52
 ---- batch: 030 ----
mean loss: 36.37
 ---- batch: 040 ----
mean loss: 34.26
train mean loss: 35.67
epoch train time: 0:00:00.200614
elapsed time: 0:00:53.728742
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:43:59.026512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.78
 ---- batch: 020 ----
mean loss: 35.15
 ---- batch: 030 ----
mean loss: 38.66
 ---- batch: 040 ----
mean loss: 35.13
train mean loss: 36.29
epoch train time: 0:00:00.200413
elapsed time: 0:00:53.929292
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:43:59.227060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.26
 ---- batch: 020 ----
mean loss: 35.80
 ---- batch: 030 ----
mean loss: 34.96
 ---- batch: 040 ----
mean loss: 33.08
train mean loss: 35.25
epoch train time: 0:00:00.201063
elapsed time: 0:00:54.130486
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:43:59.428240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.43
 ---- batch: 020 ----
mean loss: 34.46
 ---- batch: 030 ----
mean loss: 36.98
 ---- batch: 040 ----
mean loss: 36.91
train mean loss: 35.58
epoch train time: 0:00:00.200674
elapsed time: 0:00:54.331285
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:43:59.629044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.22
 ---- batch: 020 ----
mean loss: 35.67
 ---- batch: 030 ----
mean loss: 34.65
 ---- batch: 040 ----
mean loss: 35.27
train mean loss: 34.95
epoch train time: 0:00:00.201652
elapsed time: 0:00:54.533060
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:43:59.830814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.19
 ---- batch: 020 ----
mean loss: 35.37
 ---- batch: 030 ----
mean loss: 34.26
 ---- batch: 040 ----
mean loss: 33.93
train mean loss: 34.69
epoch train time: 0:00:00.203896
elapsed time: 0:00:54.737076
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:44:00.034830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.60
 ---- batch: 020 ----
mean loss: 33.77
 ---- batch: 030 ----
mean loss: 35.78
 ---- batch: 040 ----
mean loss: 34.90
train mean loss: 34.37
epoch train time: 0:00:00.201866
elapsed time: 0:00:54.939071
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:44:00.236828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 33.27
 ---- batch: 020 ----
mean loss: 35.21
 ---- batch: 030 ----
mean loss: 35.07
 ---- batch: 040 ----
mean loss: 34.48
train mean loss: 34.59
epoch train time: 0:00:00.205141
elapsed time: 0:00:55.144353
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:44:00.442108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.77
 ---- batch: 020 ----
mean loss: 34.14
 ---- batch: 030 ----
mean loss: 35.37
 ---- batch: 040 ----
mean loss: 35.44
train mean loss: 34.96
epoch train time: 0:00:00.201914
elapsed time: 0:00:55.346386
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:44:00.644139
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.15
 ---- batch: 020 ----
mean loss: 32.58
 ---- batch: 030 ----
mean loss: 33.23
 ---- batch: 040 ----
mean loss: 33.68
train mean loss: 32.90
epoch train time: 0:00:00.207807
elapsed time: 0:00:55.554330
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:44:00.852078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.33
 ---- batch: 020 ----
mean loss: 33.51
 ---- batch: 030 ----
mean loss: 34.00
 ---- batch: 040 ----
mean loss: 31.97
train mean loss: 32.73
epoch train time: 0:00:00.204530
elapsed time: 0:00:55.758978
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:44:01.056742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.95
 ---- batch: 020 ----
mean loss: 33.43
 ---- batch: 030 ----
mean loss: 31.56
 ---- batch: 040 ----
mean loss: 31.77
train mean loss: 32.67
epoch train time: 0:00:00.203339
elapsed time: 0:00:55.962451
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:44:01.260209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.79
 ---- batch: 020 ----
mean loss: 33.00
 ---- batch: 030 ----
mean loss: 32.80
 ---- batch: 040 ----
mean loss: 33.51
train mean loss: 32.65
epoch train time: 0:00:00.202164
elapsed time: 0:00:56.164744
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:44:01.462500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.76
 ---- batch: 020 ----
mean loss: 32.63
 ---- batch: 030 ----
mean loss: 33.15
 ---- batch: 040 ----
mean loss: 32.34
train mean loss: 32.65
epoch train time: 0:00:00.204773
elapsed time: 0:00:56.369659
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:44:01.667431
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.80
 ---- batch: 020 ----
mean loss: 32.63
 ---- batch: 030 ----
mean loss: 33.10
 ---- batch: 040 ----
mean loss: 31.71
train mean loss: 32.68
epoch train time: 0:00:00.204182
elapsed time: 0:00:56.574024
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:44:01.871778
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.17
 ---- batch: 020 ----
mean loss: 31.67
 ---- batch: 030 ----
mean loss: 32.49
 ---- batch: 040 ----
mean loss: 32.48
train mean loss: 32.74
epoch train time: 0:00:00.199841
elapsed time: 0:00:56.773980
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:44:02.071731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.49
 ---- batch: 020 ----
mean loss: 32.57
 ---- batch: 030 ----
mean loss: 31.36
 ---- batch: 040 ----
mean loss: 31.54
train mean loss: 32.56
epoch train time: 0:00:00.199961
elapsed time: 0:00:56.974063
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:44:02.271815
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.64
 ---- batch: 020 ----
mean loss: 32.76
 ---- batch: 030 ----
mean loss: 31.37
 ---- batch: 040 ----
mean loss: 34.96
train mean loss: 32.59
epoch train time: 0:00:00.200255
elapsed time: 0:00:57.174433
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:44:02.472185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.32
 ---- batch: 020 ----
mean loss: 33.51
 ---- batch: 030 ----
mean loss: 32.65
 ---- batch: 040 ----
mean loss: 31.56
train mean loss: 32.56
epoch train time: 0:00:00.200769
elapsed time: 0:00:57.375319
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:44:02.673072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.74
 ---- batch: 020 ----
mean loss: 31.86
 ---- batch: 030 ----
mean loss: 31.90
 ---- batch: 040 ----
mean loss: 32.53
train mean loss: 32.56
epoch train time: 0:00:00.202560
elapsed time: 0:00:57.578000
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:44:02.875770
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.23
 ---- batch: 020 ----
mean loss: 31.60
 ---- batch: 030 ----
mean loss: 31.39
 ---- batch: 040 ----
mean loss: 34.05
train mean loss: 32.50
epoch train time: 0:00:00.197931
elapsed time: 0:00:57.776066
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:44:03.073819
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.99
 ---- batch: 020 ----
mean loss: 30.52
 ---- batch: 030 ----
mean loss: 31.27
 ---- batch: 040 ----
mean loss: 33.63
train mean loss: 32.52
epoch train time: 0:00:00.199179
elapsed time: 0:00:57.975394
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:44:03.273148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.06
 ---- batch: 020 ----
mean loss: 32.29
 ---- batch: 030 ----
mean loss: 32.88
 ---- batch: 040 ----
mean loss: 32.71
train mean loss: 32.51
epoch train time: 0:00:00.197631
elapsed time: 0:00:58.173139
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:44:03.470891
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.06
 ---- batch: 020 ----
mean loss: 31.74
 ---- batch: 030 ----
mean loss: 33.46
 ---- batch: 040 ----
mean loss: 32.86
train mean loss: 32.48
epoch train time: 0:00:00.198895
elapsed time: 0:00:58.372147
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:44:03.669898
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.95
 ---- batch: 020 ----
mean loss: 32.03
 ---- batch: 030 ----
mean loss: 32.58
 ---- batch: 040 ----
mean loss: 30.40
train mean loss: 32.50
epoch train time: 0:00:00.199901
elapsed time: 0:00:58.572163
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:44:03.869915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.55
 ---- batch: 020 ----
mean loss: 32.45
 ---- batch: 030 ----
mean loss: 32.42
 ---- batch: 040 ----
mean loss: 32.05
train mean loss: 32.47
epoch train time: 0:00:00.206785
elapsed time: 0:00:58.779071
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:44:04.076840
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 29.99
 ---- batch: 020 ----
mean loss: 34.14
 ---- batch: 030 ----
mean loss: 31.95
 ---- batch: 040 ----
mean loss: 33.36
train mean loss: 32.43
epoch train time: 0:00:00.206416
elapsed time: 0:00:58.985623
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:44:04.283379
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.12
 ---- batch: 020 ----
mean loss: 33.55
 ---- batch: 030 ----
mean loss: 31.87
 ---- batch: 040 ----
mean loss: 31.63
train mean loss: 32.50
epoch train time: 0:00:00.206564
elapsed time: 0:00:59.192308
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:44:04.490061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.97
 ---- batch: 020 ----
mean loss: 32.46
 ---- batch: 030 ----
mean loss: 33.19
 ---- batch: 040 ----
mean loss: 32.50
train mean loss: 32.44
epoch train time: 0:00:00.204903
elapsed time: 0:00:59.397328
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:44:04.695081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.46
 ---- batch: 020 ----
mean loss: 32.15
 ---- batch: 030 ----
mean loss: 32.77
 ---- batch: 040 ----
mean loss: 33.23
train mean loss: 32.46
epoch train time: 0:00:00.203865
elapsed time: 0:00:59.601314
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:44:04.899068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.36
 ---- batch: 020 ----
mean loss: 33.73
 ---- batch: 030 ----
mean loss: 32.19
 ---- batch: 040 ----
mean loss: 32.51
train mean loss: 32.53
epoch train time: 0:00:00.200621
elapsed time: 0:00:59.802057
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:44:05.099813
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.47
 ---- batch: 020 ----
mean loss: 31.73
 ---- batch: 030 ----
mean loss: 31.62
 ---- batch: 040 ----
mean loss: 33.36
train mean loss: 32.33
epoch train time: 0:00:00.201676
elapsed time: 0:01:00.003857
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:44:05.301611
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.56
 ---- batch: 020 ----
mean loss: 33.58
 ---- batch: 030 ----
mean loss: 31.89
 ---- batch: 040 ----
mean loss: 32.13
train mean loss: 32.46
epoch train time: 0:00:00.199982
elapsed time: 0:01:00.203960
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:44:05.501713
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.03
 ---- batch: 020 ----
mean loss: 32.97
 ---- batch: 030 ----
mean loss: 32.77
 ---- batch: 040 ----
mean loss: 31.45
train mean loss: 32.39
epoch train time: 0:00:00.202647
elapsed time: 0:01:00.406746
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:44:05.704539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.38
 ---- batch: 020 ----
mean loss: 32.72
 ---- batch: 030 ----
mean loss: 32.43
 ---- batch: 040 ----
mean loss: 32.66
train mean loss: 32.33
epoch train time: 0:00:00.203207
elapsed time: 0:01:00.610116
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:44:05.907873
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.42
 ---- batch: 020 ----
mean loss: 33.00
 ---- batch: 030 ----
mean loss: 32.47
 ---- batch: 040 ----
mean loss: 31.41
train mean loss: 32.22
epoch train time: 0:00:00.201652
elapsed time: 0:01:00.811889
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:44:06.109641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.02
 ---- batch: 020 ----
mean loss: 30.42
 ---- batch: 030 ----
mean loss: 33.23
 ---- batch: 040 ----
mean loss: 32.71
train mean loss: 32.31
epoch train time: 0:00:00.202058
elapsed time: 0:01:01.014062
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:44:06.311817
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.17
 ---- batch: 020 ----
mean loss: 33.34
 ---- batch: 030 ----
mean loss: 32.37
 ---- batch: 040 ----
mean loss: 33.30
train mean loss: 32.24
epoch train time: 0:00:00.200285
elapsed time: 0:01:01.214465
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:44:06.512231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.50
 ---- batch: 020 ----
mean loss: 32.04
 ---- batch: 030 ----
mean loss: 32.64
 ---- batch: 040 ----
mean loss: 31.70
train mean loss: 32.30
epoch train time: 0:00:00.198798
elapsed time: 0:01:01.413388
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:44:06.711138
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.62
 ---- batch: 020 ----
mean loss: 30.95
 ---- batch: 030 ----
mean loss: 31.79
 ---- batch: 040 ----
mean loss: 33.51
train mean loss: 32.40
epoch train time: 0:00:00.199165
elapsed time: 0:01:01.612696
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:44:06.910448
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.10
 ---- batch: 020 ----
mean loss: 33.18
 ---- batch: 030 ----
mean loss: 32.92
 ---- batch: 040 ----
mean loss: 30.21
train mean loss: 32.18
epoch train time: 0:00:00.196116
elapsed time: 0:01:01.808944
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:44:07.106698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.07
 ---- batch: 020 ----
mean loss: 34.32
 ---- batch: 030 ----
mean loss: 31.14
 ---- batch: 040 ----
mean loss: 31.23
train mean loss: 32.46
epoch train time: 0:00:00.200069
elapsed time: 0:01:02.009171
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:44:07.306932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.56
 ---- batch: 020 ----
mean loss: 31.82
 ---- batch: 030 ----
mean loss: 33.32
 ---- batch: 040 ----
mean loss: 32.57
train mean loss: 32.25
epoch train time: 0:00:00.200189
elapsed time: 0:01:02.209482
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:44:07.507247
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.30
 ---- batch: 020 ----
mean loss: 31.52
 ---- batch: 030 ----
mean loss: 30.46
 ---- batch: 040 ----
mean loss: 33.56
train mean loss: 32.19
epoch train time: 0:00:00.197034
elapsed time: 0:01:02.406643
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:44:07.704415
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.58
 ---- batch: 020 ----
mean loss: 32.09
 ---- batch: 030 ----
mean loss: 32.58
 ---- batch: 040 ----
mean loss: 31.84
train mean loss: 32.07
epoch train time: 0:00:00.198164
elapsed time: 0:01:02.604949
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:44:07.902721
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.43
 ---- batch: 020 ----
mean loss: 29.87
 ---- batch: 030 ----
mean loss: 32.94
 ---- batch: 040 ----
mean loss: 31.51
train mean loss: 32.13
epoch train time: 0:00:00.203274
elapsed time: 0:01:02.808376
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:44:08.106128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.80
 ---- batch: 020 ----
mean loss: 32.62
 ---- batch: 030 ----
mean loss: 31.39
 ---- batch: 040 ----
mean loss: 32.49
train mean loss: 32.15
epoch train time: 0:00:00.206082
elapsed time: 0:01:03.014588
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:44:08.312344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 30.51
 ---- batch: 020 ----
mean loss: 32.10
 ---- batch: 030 ----
mean loss: 33.37
 ---- batch: 040 ----
mean loss: 32.69
train mean loss: 32.07
epoch train time: 0:00:00.209243
elapsed time: 0:01:03.223970
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:44:08.521735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.44
 ---- batch: 020 ----
mean loss: 33.52
 ---- batch: 030 ----
mean loss: 30.63
 ---- batch: 040 ----
mean loss: 32.61
train mean loss: 32.15
epoch train time: 0:00:00.207602
elapsed time: 0:01:03.431708
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:44:08.729463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.74
 ---- batch: 020 ----
mean loss: 31.43
 ---- batch: 030 ----
mean loss: 32.20
 ---- batch: 040 ----
mean loss: 33.36
train mean loss: 32.03
epoch train time: 0:00:00.207409
elapsed time: 0:01:03.639241
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:44:08.937010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.53
 ---- batch: 020 ----
mean loss: 32.13
 ---- batch: 030 ----
mean loss: 32.04
 ---- batch: 040 ----
mean loss: 31.73
train mean loss: 32.05
epoch train time: 0:00:00.206811
elapsed time: 0:01:03.846206
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:44:09.143975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.55
 ---- batch: 020 ----
mean loss: 30.86
 ---- batch: 030 ----
mean loss: 33.03
 ---- batch: 040 ----
mean loss: 31.87
train mean loss: 32.02
epoch train time: 0:00:00.208444
elapsed time: 0:01:04.054790
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:44:09.352544
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.91
 ---- batch: 020 ----
mean loss: 32.00
 ---- batch: 030 ----
mean loss: 32.86
 ---- batch: 040 ----
mean loss: 31.45
train mean loss: 31.95
epoch train time: 0:00:00.202933
elapsed time: 0:01:04.257861
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:44:09.555614
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.37
 ---- batch: 020 ----
mean loss: 33.15
 ---- batch: 030 ----
mean loss: 32.18
 ---- batch: 040 ----
mean loss: 32.39
train mean loss: 31.97
epoch train time: 0:00:00.215435
elapsed time: 0:01:04.473419
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:44:09.771175
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.32
 ---- batch: 020 ----
mean loss: 30.48
 ---- batch: 030 ----
mean loss: 33.76
 ---- batch: 040 ----
mean loss: 33.46
train mean loss: 32.05
epoch train time: 0:00:00.205997
elapsed time: 0:01:04.679552
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:44:09.977307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.78
 ---- batch: 020 ----
mean loss: 32.59
 ---- batch: 030 ----
mean loss: 30.84
 ---- batch: 040 ----
mean loss: 32.42
train mean loss: 32.04
epoch train time: 0:00:00.210697
elapsed time: 0:01:04.890369
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:44:10.188121
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.30
 ---- batch: 020 ----
mean loss: 31.98
 ---- batch: 030 ----
mean loss: 32.47
 ---- batch: 040 ----
mean loss: 30.34
train mean loss: 32.05
epoch train time: 0:00:00.201025
elapsed time: 0:01:05.091512
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:44:10.389269
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.93
 ---- batch: 020 ----
mean loss: 32.52
 ---- batch: 030 ----
mean loss: 31.52
 ---- batch: 040 ----
mean loss: 31.45
train mean loss: 31.97
epoch train time: 0:00:00.199943
elapsed time: 0:01:05.294851
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_4/checkpoint.pth.tar
**** end time: 2019-09-20 23:44:10.592579 ****
