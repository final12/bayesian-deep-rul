Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5513
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-20 21:30:41.104320 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1             [-1, 8, 26, 1]           1,120
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
    BayesianConv2d-4            [-1, 14, 12, 1]             448
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
    BayesianLinear-8                    [-1, 1]             168
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 1,736
Trainable params: 1,736
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:30:41.113923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4466.15
 ---- batch: 020 ----
mean loss: 4408.43
 ---- batch: 030 ----
mean loss: 4411.32
 ---- batch: 040 ----
mean loss: 4285.31
train mean loss: 4378.90
epoch train time: 0:00:14.984926
elapsed time: 0:00:14.997265
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:30:56.101631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4228.27
 ---- batch: 020 ----
mean loss: 4093.87
 ---- batch: 030 ----
mean loss: 3990.94
 ---- batch: 040 ----
mean loss: 3985.93
train mean loss: 4059.08
epoch train time: 0:00:00.609563
elapsed time: 0:00:15.607779
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:30:56.712185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3781.24
 ---- batch: 020 ----
mean loss: 3735.35
 ---- batch: 030 ----
mean loss: 3692.82
 ---- batch: 040 ----
mean loss: 3559.20
train mean loss: 3672.51
epoch train time: 0:00:00.615131
elapsed time: 0:00:16.223124
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:30:57.327516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3428.59
 ---- batch: 020 ----
mean loss: 3442.45
 ---- batch: 030 ----
mean loss: 3180.31
 ---- batch: 040 ----
mean loss: 3233.41
train mean loss: 3319.44
epoch train time: 0:00:00.637378
elapsed time: 0:00:16.860715
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:30:57.965105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3144.14
 ---- batch: 020 ----
mean loss: 3032.82
 ---- batch: 030 ----
mean loss: 3010.98
 ---- batch: 040 ----
mean loss: 2923.91
train mean loss: 3018.32
epoch train time: 0:00:00.638759
elapsed time: 0:00:17.499702
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:30:58.604113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2833.90
 ---- batch: 020 ----
mean loss: 2842.71
 ---- batch: 030 ----
mean loss: 2741.57
 ---- batch: 040 ----
mean loss: 2684.81
train mean loss: 2769.38
epoch train time: 0:00:00.625194
elapsed time: 0:00:18.125136
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:30:59.229540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2624.75
 ---- batch: 020 ----
mean loss: 2626.84
 ---- batch: 030 ----
mean loss: 2554.60
 ---- batch: 040 ----
mean loss: 2459.75
train mean loss: 2561.51
epoch train time: 0:00:00.602555
elapsed time: 0:00:18.727957
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:30:59.832372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2451.90
 ---- batch: 020 ----
mean loss: 2357.59
 ---- batch: 030 ----
mean loss: 2378.81
 ---- batch: 040 ----
mean loss: 2287.85
train mean loss: 2365.74
epoch train time: 0:00:00.603511
elapsed time: 0:00:19.331681
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:31:00.436064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2308.22
 ---- batch: 020 ----
mean loss: 2188.19
 ---- batch: 030 ----
mean loss: 2159.28
 ---- batch: 040 ----
mean loss: 2098.07
train mean loss: 2182.11
epoch train time: 0:00:00.616800
elapsed time: 0:00:19.948704
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:31:01.053141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2076.32
 ---- batch: 020 ----
mean loss: 2030.73
 ---- batch: 030 ----
mean loss: 2001.32
 ---- batch: 040 ----
mean loss: 1998.76
train mean loss: 2021.08
epoch train time: 0:00:00.630077
elapsed time: 0:00:20.579047
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:31:01.683441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1916.17
 ---- batch: 020 ----
mean loss: 1907.62
 ---- batch: 030 ----
mean loss: 1874.22
 ---- batch: 040 ----
mean loss: 1811.26
train mean loss: 1877.82
epoch train time: 0:00:00.629497
elapsed time: 0:00:21.208766
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:31:02.313161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1822.20
 ---- batch: 020 ----
mean loss: 1741.34
 ---- batch: 030 ----
mean loss: 1719.83
 ---- batch: 040 ----
mean loss: 1700.78
train mean loss: 1743.08
epoch train time: 0:00:00.635554
elapsed time: 0:00:21.844586
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:31:02.948959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1665.51
 ---- batch: 020 ----
mean loss: 1651.77
 ---- batch: 030 ----
mean loss: 1608.04
 ---- batch: 040 ----
mean loss: 1628.27
train mean loss: 1634.31
epoch train time: 0:00:00.603377
elapsed time: 0:00:22.448150
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:31:03.552530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1559.18
 ---- batch: 020 ----
mean loss: 1544.20
 ---- batch: 030 ----
mean loss: 1513.60
 ---- batch: 040 ----
mean loss: 1500.38
train mean loss: 1528.65
epoch train time: 0:00:00.604962
elapsed time: 0:00:23.053295
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:31:04.157695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1482.61
 ---- batch: 020 ----
mean loss: 1448.66
 ---- batch: 030 ----
mean loss: 1415.72
 ---- batch: 040 ----
mean loss: 1412.16
train mean loss: 1435.76
epoch train time: 0:00:00.597161
elapsed time: 0:00:23.650729
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:31:04.755115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1359.51
 ---- batch: 020 ----
mean loss: 1370.06
 ---- batch: 030 ----
mean loss: 1344.42
 ---- batch: 040 ----
mean loss: 1329.82
train mean loss: 1347.76
epoch train time: 0:00:00.621076
elapsed time: 0:00:24.272009
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:31:05.376426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1284.02
 ---- batch: 020 ----
mean loss: 1292.18
 ---- batch: 030 ----
mean loss: 1272.53
 ---- batch: 040 ----
mean loss: 1265.29
train mean loss: 1278.38
epoch train time: 0:00:00.661595
elapsed time: 0:00:24.933901
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:31:06.038303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1227.09
 ---- batch: 020 ----
mean loss: 1218.62
 ---- batch: 030 ----
mean loss: 1200.83
 ---- batch: 040 ----
mean loss: 1194.87
train mean loss: 1208.91
epoch train time: 0:00:00.634521
elapsed time: 0:00:25.568695
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:31:06.673099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1178.66
 ---- batch: 020 ----
mean loss: 1130.33
 ---- batch: 030 ----
mean loss: 1158.59
 ---- batch: 040 ----
mean loss: 1142.95
train mean loss: 1149.79
epoch train time: 0:00:00.615576
elapsed time: 0:00:26.184528
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:31:07.288930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1100.44
 ---- batch: 020 ----
mean loss: 1108.89
 ---- batch: 030 ----
mean loss: 1097.80
 ---- batch: 040 ----
mean loss: 1063.73
train mean loss: 1092.24
epoch train time: 0:00:00.609335
elapsed time: 0:00:26.794079
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:31:07.898467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1059.89
 ---- batch: 020 ----
mean loss: 1048.97
 ---- batch: 030 ----
mean loss: 1039.90
 ---- batch: 040 ----
mean loss: 1030.64
train mean loss: 1042.19
epoch train time: 0:00:00.600094
elapsed time: 0:00:27.394367
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:31:08.498767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1018.62
 ---- batch: 020 ----
mean loss: 1001.25
 ---- batch: 030 ----
mean loss: 997.81
 ---- batch: 040 ----
mean loss: 985.97
train mean loss: 999.75
epoch train time: 0:00:00.610818
elapsed time: 0:00:28.005428
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:31:09.109799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.15
 ---- batch: 020 ----
mean loss: 974.09
 ---- batch: 030 ----
mean loss: 959.87
 ---- batch: 040 ----
mean loss: 935.69
train mean loss: 961.66
epoch train time: 0:00:00.619013
elapsed time: 0:00:28.624651
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:31:09.729042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 954.32
 ---- batch: 020 ----
mean loss: 933.24
 ---- batch: 030 ----
mean loss: 908.16
 ---- batch: 040 ----
mean loss: 906.74
train mean loss: 921.80
epoch train time: 0:00:00.631393
elapsed time: 0:00:29.256287
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:31:10.360679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.08
 ---- batch: 020 ----
mean loss: 903.98
 ---- batch: 030 ----
mean loss: 895.26
 ---- batch: 040 ----
mean loss: 872.78
train mean loss: 895.09
epoch train time: 0:00:00.610984
elapsed time: 0:00:29.867481
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:31:10.971868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.81
 ---- batch: 020 ----
mean loss: 886.11
 ---- batch: 030 ----
mean loss: 872.78
 ---- batch: 040 ----
mean loss: 853.93
train mean loss: 868.49
epoch train time: 0:00:00.596148
elapsed time: 0:00:30.463846
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:31:11.568249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 838.26
 ---- batch: 030 ----
mean loss: 851.44
 ---- batch: 040 ----
mean loss: 845.57
train mean loss: 847.18
epoch train time: 0:00:00.594488
elapsed time: 0:00:31.058554
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:31:12.162936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.53
 ---- batch: 020 ----
mean loss: 826.68
 ---- batch: 030 ----
mean loss: 824.46
 ---- batch: 040 ----
mean loss: 824.57
train mean loss: 824.15
epoch train time: 0:00:00.602619
elapsed time: 0:00:31.661374
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:31:12.765759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.11
 ---- batch: 020 ----
mean loss: 797.25
 ---- batch: 030 ----
mean loss: 795.63
 ---- batch: 040 ----
mean loss: 796.89
train mean loss: 802.00
epoch train time: 0:00:00.629063
elapsed time: 0:00:32.290643
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:31:13.395054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 794.64
 ---- batch: 020 ----
mean loss: 771.34
 ---- batch: 030 ----
mean loss: 795.72
 ---- batch: 040 ----
mean loss: 781.78
train mean loss: 787.56
epoch train time: 0:00:00.636870
elapsed time: 0:00:32.927758
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:31:14.032149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 781.40
 ---- batch: 020 ----
mean loss: 770.55
 ---- batch: 030 ----
mean loss: 781.65
 ---- batch: 040 ----
mean loss: 771.10
train mean loss: 775.50
epoch train time: 0:00:00.620659
elapsed time: 0:00:33.548607
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:31:14.653005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 776.98
 ---- batch: 020 ----
mean loss: 753.46
 ---- batch: 030 ----
mean loss: 768.35
 ---- batch: 040 ----
mean loss: 745.62
train mean loss: 760.33
epoch train time: 0:00:00.654279
elapsed time: 0:00:34.203113
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:31:15.307486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 773.82
 ---- batch: 020 ----
mean loss: 738.12
 ---- batch: 030 ----
mean loss: 730.79
 ---- batch: 040 ----
mean loss: 764.50
train mean loss: 749.93
epoch train time: 0:00:00.603496
elapsed time: 0:00:34.806784
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:31:15.911169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 744.32
 ---- batch: 020 ----
mean loss: 736.80
 ---- batch: 030 ----
mean loss: 744.20
 ---- batch: 040 ----
mean loss: 731.42
train mean loss: 738.58
epoch train time: 0:00:00.606536
elapsed time: 0:00:35.413540
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:31:16.517932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 735.38
 ---- batch: 020 ----
mean loss: 735.88
 ---- batch: 030 ----
mean loss: 718.88
 ---- batch: 040 ----
mean loss: 733.03
train mean loss: 731.98
epoch train time: 0:00:00.627782
elapsed time: 0:00:36.041598
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:31:17.145986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 727.79
 ---- batch: 020 ----
mean loss: 716.43
 ---- batch: 030 ----
mean loss: 711.54
 ---- batch: 040 ----
mean loss: 736.19
train mean loss: 721.61
epoch train time: 0:00:00.622639
elapsed time: 0:00:36.664446
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:31:17.768853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.47
 ---- batch: 020 ----
mean loss: 720.45
 ---- batch: 030 ----
mean loss: 722.08
 ---- batch: 040 ----
mean loss: 721.43
train mean loss: 718.70
epoch train time: 0:00:00.619297
elapsed time: 0:00:37.283986
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:31:18.388374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.75
 ---- batch: 020 ----
mean loss: 700.14
 ---- batch: 030 ----
mean loss: 707.77
 ---- batch: 040 ----
mean loss: 736.96
train mean loss: 714.41
epoch train time: 0:00:00.614551
elapsed time: 0:00:37.898738
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:31:19.003127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 710.27
 ---- batch: 020 ----
mean loss: 696.83
 ---- batch: 030 ----
mean loss: 729.18
 ---- batch: 040 ----
mean loss: 705.05
train mean loss: 707.67
epoch train time: 0:00:00.605578
elapsed time: 0:00:38.504514
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:31:19.608900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 717.58
 ---- batch: 020 ----
mean loss: 705.92
 ---- batch: 030 ----
mean loss: 701.57
 ---- batch: 040 ----
mean loss: 697.54
train mean loss: 703.36
epoch train time: 0:00:00.629013
elapsed time: 0:00:39.133747
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:31:20.238171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 688.56
 ---- batch: 020 ----
mean loss: 719.09
 ---- batch: 030 ----
mean loss: 704.94
 ---- batch: 040 ----
mean loss: 690.19
train mean loss: 699.59
epoch train time: 0:00:00.634209
elapsed time: 0:00:39.768247
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:31:20.872656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 713.50
 ---- batch: 020 ----
mean loss: 700.94
 ---- batch: 030 ----
mean loss: 687.12
 ---- batch: 040 ----
mean loss: 683.83
train mean loss: 696.99
epoch train time: 0:00:00.637146
elapsed time: 0:00:40.405678
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:31:21.510090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.93
 ---- batch: 020 ----
mean loss: 695.25
 ---- batch: 030 ----
mean loss: 695.40
 ---- batch: 040 ----
mean loss: 693.71
train mean loss: 693.44
epoch train time: 0:00:00.615924
elapsed time: 0:00:41.021958
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:31:22.126360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.99
 ---- batch: 020 ----
mean loss: 685.77
 ---- batch: 030 ----
mean loss: 694.03
 ---- batch: 040 ----
mean loss: 686.88
train mean loss: 693.76
epoch train time: 0:00:00.586744
elapsed time: 0:00:41.608920
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:31:22.713321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.34
 ---- batch: 020 ----
mean loss: 657.75
 ---- batch: 030 ----
mean loss: 692.67
 ---- batch: 040 ----
mean loss: 707.15
train mean loss: 687.85
epoch train time: 0:00:00.609198
elapsed time: 0:00:42.218325
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:31:23.322741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.26
 ---- batch: 020 ----
mean loss: 666.28
 ---- batch: 030 ----
mean loss: 686.73
 ---- batch: 040 ----
mean loss: 689.79
train mean loss: 686.76
epoch train time: 0:00:00.612184
elapsed time: 0:00:42.830746
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:31:23.935140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.32
 ---- batch: 020 ----
mean loss: 697.37
 ---- batch: 030 ----
mean loss: 675.85
 ---- batch: 040 ----
mean loss: 693.25
train mean loss: 685.37
epoch train time: 0:00:00.631820
elapsed time: 0:00:43.462779
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:31:24.567169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.05
 ---- batch: 020 ----
mean loss: 698.90
 ---- batch: 030 ----
mean loss: 683.20
 ---- batch: 040 ----
mean loss: 696.76
train mean loss: 686.39
epoch train time: 0:00:00.640352
elapsed time: 0:00:44.103338
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:31:25.207726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.54
 ---- batch: 020 ----
mean loss: 695.82
 ---- batch: 030 ----
mean loss: 675.87
 ---- batch: 040 ----
mean loss: 690.41
train mean loss: 684.51
epoch train time: 0:00:00.612985
elapsed time: 0:00:44.716559
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:31:25.820946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.18
 ---- batch: 020 ----
mean loss: 678.07
 ---- batch: 030 ----
mean loss: 698.58
 ---- batch: 040 ----
mean loss: 686.39
train mean loss: 682.30
epoch train time: 0:00:00.589313
elapsed time: 0:00:45.306085
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:31:26.410494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.88
 ---- batch: 020 ----
mean loss: 694.30
 ---- batch: 030 ----
mean loss: 673.33
 ---- batch: 040 ----
mean loss: 676.66
train mean loss: 680.06
epoch train time: 0:00:00.596986
elapsed time: 0:00:45.903341
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:31:27.007724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.12
 ---- batch: 020 ----
mean loss: 673.36
 ---- batch: 030 ----
mean loss: 682.25
 ---- batch: 040 ----
mean loss: 684.93
train mean loss: 677.76
epoch train time: 0:00:00.590304
elapsed time: 0:00:46.493839
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:31:27.598257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.11
 ---- batch: 020 ----
mean loss: 698.49
 ---- batch: 030 ----
mean loss: 651.46
 ---- batch: 040 ----
mean loss: 689.60
train mean loss: 678.78
epoch train time: 0:00:00.639119
elapsed time: 0:00:47.133189
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:31:28.237588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.87
 ---- batch: 020 ----
mean loss: 667.18
 ---- batch: 030 ----
mean loss: 680.38
 ---- batch: 040 ----
mean loss: 686.03
train mean loss: 679.99
epoch train time: 0:00:00.624409
elapsed time: 0:00:47.757857
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:31:28.862253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 678.36
 ---- batch: 020 ----
mean loss: 673.62
 ---- batch: 030 ----
mean loss: 685.99
 ---- batch: 040 ----
mean loss: 673.09
train mean loss: 677.70
epoch train time: 0:00:00.609714
elapsed time: 0:00:48.367774
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:31:29.472171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.39
 ---- batch: 020 ----
mean loss: 659.98
 ---- batch: 030 ----
mean loss: 660.46
 ---- batch: 040 ----
mean loss: 687.29
train mean loss: 673.65
epoch train time: 0:00:00.606113
elapsed time: 0:00:48.974180
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:31:30.078568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 697.42
 ---- batch: 020 ----
mean loss: 675.41
 ---- batch: 030 ----
mean loss: 681.55
 ---- batch: 040 ----
mean loss: 662.72
train mean loss: 676.58
epoch train time: 0:00:00.601704
elapsed time: 0:00:49.576103
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:31:30.680530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.13
 ---- batch: 020 ----
mean loss: 665.30
 ---- batch: 030 ----
mean loss: 670.14
 ---- batch: 040 ----
mean loss: 661.12
train mean loss: 670.65
epoch train time: 0:00:00.616794
elapsed time: 0:00:50.193167
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:31:31.297585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.06
 ---- batch: 020 ----
mean loss: 686.12
 ---- batch: 030 ----
mean loss: 658.29
 ---- batch: 040 ----
mean loss: 649.40
train mean loss: 663.18
epoch train time: 0:00:00.627739
elapsed time: 0:00:50.821147
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:31:31.925536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.31
 ---- batch: 020 ----
mean loss: 649.35
 ---- batch: 030 ----
mean loss: 610.04
 ---- batch: 040 ----
mean loss: 594.63
train mean loss: 617.51
epoch train time: 0:00:00.636821
elapsed time: 0:00:51.458179
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:31:32.562573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.78
 ---- batch: 020 ----
mean loss: 472.90
 ---- batch: 030 ----
mean loss: 449.35
 ---- batch: 040 ----
mean loss: 430.22
train mean loss: 467.26
epoch train time: 0:00:00.634818
elapsed time: 0:00:52.093230
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:31:33.197618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.95
 ---- batch: 020 ----
mean loss: 417.68
 ---- batch: 030 ----
mean loss: 407.18
 ---- batch: 040 ----
mean loss: 409.65
train mean loss: 411.38
epoch train time: 0:00:00.613230
elapsed time: 0:00:52.706686
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:31:33.811076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.77
 ---- batch: 020 ----
mean loss: 399.58
 ---- batch: 030 ----
mean loss: 393.37
 ---- batch: 040 ----
mean loss: 396.66
train mean loss: 397.08
epoch train time: 0:00:00.614895
elapsed time: 0:00:53.321800
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:31:34.426195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.95
 ---- batch: 020 ----
mean loss: 389.14
 ---- batch: 030 ----
mean loss: 388.18
 ---- batch: 040 ----
mean loss: 386.23
train mean loss: 386.47
epoch train time: 0:00:00.612613
elapsed time: 0:00:53.934643
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:31:35.039037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.63
 ---- batch: 020 ----
mean loss: 379.65
 ---- batch: 030 ----
mean loss: 376.68
 ---- batch: 040 ----
mean loss: 364.70
train mean loss: 377.28
epoch train time: 0:00:00.624304
elapsed time: 0:00:54.559230
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:31:35.663635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.28
 ---- batch: 020 ----
mean loss: 371.21
 ---- batch: 030 ----
mean loss: 366.60
 ---- batch: 040 ----
mean loss: 366.81
train mean loss: 369.24
epoch train time: 0:00:00.618996
elapsed time: 0:00:55.178485
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:31:36.282884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.77
 ---- batch: 020 ----
mean loss: 347.35
 ---- batch: 030 ----
mean loss: 364.67
 ---- batch: 040 ----
mean loss: 364.74
train mean loss: 362.34
epoch train time: 0:00:00.610134
elapsed time: 0:00:55.788831
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:31:36.893218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.89
 ---- batch: 020 ----
mean loss: 361.07
 ---- batch: 030 ----
mean loss: 350.64
 ---- batch: 040 ----
mean loss: 358.00
train mean loss: 357.26
epoch train time: 0:00:00.595779
elapsed time: 0:00:56.384844
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:31:37.489248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.68
 ---- batch: 020 ----
mean loss: 340.99
 ---- batch: 030 ----
mean loss: 358.13
 ---- batch: 040 ----
mean loss: 349.74
train mean loss: 351.16
epoch train time: 0:00:00.600404
elapsed time: 0:00:56.985515
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:31:38.089904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.19
 ---- batch: 020 ----
mean loss: 343.74
 ---- batch: 030 ----
mean loss: 337.35
 ---- batch: 040 ----
mean loss: 341.98
train mean loss: 345.72
epoch train time: 0:00:00.595202
elapsed time: 0:00:57.580915
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:31:38.685310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.58
 ---- batch: 020 ----
mean loss: 344.11
 ---- batch: 030 ----
mean loss: 346.27
 ---- batch: 040 ----
mean loss: 332.47
train mean loss: 340.94
epoch train time: 0:00:00.649045
elapsed time: 0:00:58.230205
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:31:39.334600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.84
 ---- batch: 020 ----
mean loss: 341.14
 ---- batch: 030 ----
mean loss: 339.98
 ---- batch: 040 ----
mean loss: 334.66
train mean loss: 337.94
epoch train time: 0:00:00.635712
elapsed time: 0:00:58.866143
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:31:39.970565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.38
 ---- batch: 020 ----
mean loss: 330.86
 ---- batch: 030 ----
mean loss: 329.30
 ---- batch: 040 ----
mean loss: 337.49
train mean loss: 332.41
epoch train time: 0:00:00.629461
elapsed time: 0:00:59.495840
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:31:40.600224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.57
 ---- batch: 020 ----
mean loss: 319.52
 ---- batch: 030 ----
mean loss: 329.67
 ---- batch: 040 ----
mean loss: 328.09
train mean loss: 329.18
epoch train time: 0:00:00.616672
elapsed time: 0:01:00.112696
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:31:41.217096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.19
 ---- batch: 020 ----
mean loss: 321.46
 ---- batch: 030 ----
mean loss: 314.77
 ---- batch: 040 ----
mean loss: 329.74
train mean loss: 324.51
epoch train time: 0:00:00.605209
elapsed time: 0:01:00.718128
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:31:41.822517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.32
 ---- batch: 020 ----
mean loss: 323.50
 ---- batch: 030 ----
mean loss: 316.96
 ---- batch: 040 ----
mean loss: 332.39
train mean loss: 321.61
epoch train time: 0:00:00.608223
elapsed time: 0:01:01.326575
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:31:42.430979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.87
 ---- batch: 020 ----
mean loss: 319.11
 ---- batch: 030 ----
mean loss: 314.43
 ---- batch: 040 ----
mean loss: 319.18
train mean loss: 317.95
epoch train time: 0:00:00.635055
elapsed time: 0:01:01.961871
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:31:43.066287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.84
 ---- batch: 020 ----
mean loss: 321.32
 ---- batch: 030 ----
mean loss: 310.91
 ---- batch: 040 ----
mean loss: 315.89
train mean loss: 316.31
epoch train time: 0:00:00.609689
elapsed time: 0:01:02.571789
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:31:43.676192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.00
 ---- batch: 020 ----
mean loss: 317.05
 ---- batch: 030 ----
mean loss: 303.85
 ---- batch: 040 ----
mean loss: 312.24
train mean loss: 312.86
epoch train time: 0:00:00.618565
elapsed time: 0:01:03.190567
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:31:44.294949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.71
 ---- batch: 020 ----
mean loss: 307.75
 ---- batch: 030 ----
mean loss: 317.15
 ---- batch: 040 ----
mean loss: 302.62
train mean loss: 310.56
epoch train time: 0:00:00.595420
elapsed time: 0:01:03.786190
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:31:44.890582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.22
 ---- batch: 020 ----
mean loss: 308.24
 ---- batch: 030 ----
mean loss: 313.03
 ---- batch: 040 ----
mean loss: 297.69
train mean loss: 306.60
epoch train time: 0:00:00.581795
elapsed time: 0:01:04.368245
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:31:45.472643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.42
 ---- batch: 020 ----
mean loss: 306.59
 ---- batch: 030 ----
mean loss: 300.83
 ---- batch: 040 ----
mean loss: 304.19
train mean loss: 303.88
epoch train time: 0:00:00.596523
elapsed time: 0:01:04.964971
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:31:46.069355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.46
 ---- batch: 020 ----
mean loss: 310.04
 ---- batch: 030 ----
mean loss: 300.83
 ---- batch: 040 ----
mean loss: 303.05
train mean loss: 301.86
epoch train time: 0:00:00.625906
elapsed time: 0:01:05.591120
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:31:46.695516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.79
 ---- batch: 020 ----
mean loss: 304.30
 ---- batch: 030 ----
mean loss: 292.60
 ---- batch: 040 ----
mean loss: 308.10
train mean loss: 300.32
epoch train time: 0:00:00.635695
elapsed time: 0:01:06.227053
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:31:47.331459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.79
 ---- batch: 020 ----
mean loss: 299.50
 ---- batch: 030 ----
mean loss: 294.44
 ---- batch: 040 ----
mean loss: 298.01
train mean loss: 297.68
epoch train time: 0:00:00.635083
elapsed time: 0:01:06.862472
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:31:47.966880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.00
 ---- batch: 020 ----
mean loss: 295.47
 ---- batch: 030 ----
mean loss: 297.83
 ---- batch: 040 ----
mean loss: 300.11
train mean loss: 294.92
epoch train time: 0:00:00.594073
elapsed time: 0:01:07.456795
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:31:48.561194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.23
 ---- batch: 020 ----
mean loss: 296.01
 ---- batch: 030 ----
mean loss: 283.27
 ---- batch: 040 ----
mean loss: 294.87
train mean loss: 292.80
epoch train time: 0:00:00.602337
elapsed time: 0:01:08.059351
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:31:49.163742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.99
 ---- batch: 020 ----
mean loss: 285.92
 ---- batch: 030 ----
mean loss: 299.68
 ---- batch: 040 ----
mean loss: 289.63
train mean loss: 291.52
epoch train time: 0:00:00.596715
elapsed time: 0:01:08.656324
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:31:49.760718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.26
 ---- batch: 020 ----
mean loss: 293.49
 ---- batch: 030 ----
mean loss: 292.95
 ---- batch: 040 ----
mean loss: 284.54
train mean loss: 289.19
epoch train time: 0:00:00.622828
elapsed time: 0:01:09.279366
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:31:50.383798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.43
 ---- batch: 020 ----
mean loss: 277.90
 ---- batch: 030 ----
mean loss: 288.97
 ---- batch: 040 ----
mean loss: 288.88
train mean loss: 286.40
epoch train time: 0:00:00.620229
elapsed time: 0:01:09.899884
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:31:51.004283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.73
 ---- batch: 020 ----
mean loss: 285.52
 ---- batch: 030 ----
mean loss: 283.50
 ---- batch: 040 ----
mean loss: 285.51
train mean loss: 284.75
epoch train time: 0:00:00.614566
elapsed time: 0:01:10.514672
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:31:51.619059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.54
 ---- batch: 020 ----
mean loss: 286.50
 ---- batch: 030 ----
mean loss: 283.43
 ---- batch: 040 ----
mean loss: 283.07
train mean loss: 283.22
epoch train time: 0:00:00.596085
elapsed time: 0:01:11.110968
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:31:52.215368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.61
 ---- batch: 020 ----
mean loss: 286.23
 ---- batch: 030 ----
mean loss: 277.80
 ---- batch: 040 ----
mean loss: 282.13
train mean loss: 281.51
epoch train time: 0:00:00.598497
elapsed time: 0:01:11.709695
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:31:52.814082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.15
 ---- batch: 020 ----
mean loss: 276.13
 ---- batch: 030 ----
mean loss: 272.76
 ---- batch: 040 ----
mean loss: 286.99
train mean loss: 279.74
epoch train time: 0:00:00.593672
elapsed time: 0:01:12.303561
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:31:53.407947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.61
 ---- batch: 020 ----
mean loss: 281.54
 ---- batch: 030 ----
mean loss: 281.10
 ---- batch: 040 ----
mean loss: 275.51
train mean loss: 277.31
epoch train time: 0:00:00.623000
elapsed time: 0:01:12.926778
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:31:54.031185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.41
 ---- batch: 020 ----
mean loss: 280.73
 ---- batch: 030 ----
mean loss: 267.39
 ---- batch: 040 ----
mean loss: 275.14
train mean loss: 276.55
epoch train time: 0:00:00.643327
elapsed time: 0:01:13.570372
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:31:54.674780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.49
 ---- batch: 020 ----
mean loss: 271.63
 ---- batch: 030 ----
mean loss: 267.06
 ---- batch: 040 ----
mean loss: 278.73
train mean loss: 273.66
epoch train time: 0:00:00.637184
elapsed time: 0:01:14.207778
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:31:55.312169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.44
 ---- batch: 020 ----
mean loss: 272.44
 ---- batch: 030 ----
mean loss: 275.29
 ---- batch: 040 ----
mean loss: 272.15
train mean loss: 273.01
epoch train time: 0:00:00.598900
elapsed time: 0:01:14.806867
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:31:55.911248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.16
 ---- batch: 020 ----
mean loss: 273.13
 ---- batch: 030 ----
mean loss: 275.34
 ---- batch: 040 ----
mean loss: 266.14
train mean loss: 270.82
epoch train time: 0:00:00.579388
elapsed time: 0:01:15.386451
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:31:56.490851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.51
 ---- batch: 020 ----
mean loss: 270.70
 ---- batch: 030 ----
mean loss: 264.09
 ---- batch: 040 ----
mean loss: 273.13
train mean loss: 269.30
epoch train time: 0:00:00.608872
elapsed time: 0:01:15.995546
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:31:57.099961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.22
 ---- batch: 020 ----
mean loss: 268.93
 ---- batch: 030 ----
mean loss: 274.26
 ---- batch: 040 ----
mean loss: 266.48
train mean loss: 268.22
epoch train time: 0:00:00.609978
elapsed time: 0:01:16.605799
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:31:57.710209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.78
 ---- batch: 020 ----
mean loss: 268.34
 ---- batch: 030 ----
mean loss: 268.63
 ---- batch: 040 ----
mean loss: 261.34
train mean loss: 266.51
epoch train time: 0:00:00.636275
elapsed time: 0:01:17.242293
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:31:58.346680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.10
 ---- batch: 020 ----
mean loss: 267.75
 ---- batch: 030 ----
mean loss: 267.56
 ---- batch: 040 ----
mean loss: 259.49
train mean loss: 264.12
epoch train time: 0:00:00.633078
elapsed time: 0:01:17.875620
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:31:58.980008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.11
 ---- batch: 020 ----
mean loss: 267.06
 ---- batch: 030 ----
mean loss: 262.48
 ---- batch: 040 ----
mean loss: 259.47
train mean loss: 263.08
epoch train time: 0:00:00.619823
elapsed time: 0:01:18.495634
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:31:59.600037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.96
 ---- batch: 020 ----
mean loss: 264.44
 ---- batch: 030 ----
mean loss: 250.08
 ---- batch: 040 ----
mean loss: 266.40
train mean loss: 262.12
epoch train time: 0:00:00.598332
elapsed time: 0:01:19.094232
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:32:00.198637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.89
 ---- batch: 020 ----
mean loss: 265.85
 ---- batch: 030 ----
mean loss: 259.28
 ---- batch: 040 ----
mean loss: 255.10
train mean loss: 261.18
epoch train time: 0:00:00.605727
elapsed time: 0:01:19.700296
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:32:00.804703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.42
 ---- batch: 020 ----
mean loss: 262.47
 ---- batch: 030 ----
mean loss: 251.70
 ---- batch: 040 ----
mean loss: 256.74
train mean loss: 258.69
epoch train time: 0:00:00.618611
elapsed time: 0:01:20.319175
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:32:01.423535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.66
 ---- batch: 020 ----
mean loss: 269.87
 ---- batch: 030 ----
mean loss: 255.41
 ---- batch: 040 ----
mean loss: 249.91
train mean loss: 257.04
epoch train time: 0:00:00.628887
elapsed time: 0:01:20.948273
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:32:02.052686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.89
 ---- batch: 020 ----
mean loss: 261.34
 ---- batch: 030 ----
mean loss: 251.74
 ---- batch: 040 ----
mean loss: 253.53
train mean loss: 257.05
epoch train time: 0:00:00.646769
elapsed time: 0:01:21.595279
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:32:02.699676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.86
 ---- batch: 020 ----
mean loss: 260.71
 ---- batch: 030 ----
mean loss: 258.96
 ---- batch: 040 ----
mean loss: 246.57
train mean loss: 254.72
epoch train time: 0:00:00.633793
elapsed time: 0:01:22.229283
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:32:03.333669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.36
 ---- batch: 020 ----
mean loss: 243.72
 ---- batch: 030 ----
mean loss: 257.99
 ---- batch: 040 ----
mean loss: 258.20
train mean loss: 252.26
epoch train time: 0:00:00.597809
elapsed time: 0:01:22.827300
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:32:03.931684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.09
 ---- batch: 020 ----
mean loss: 249.78
 ---- batch: 030 ----
mean loss: 251.67
 ---- batch: 040 ----
mean loss: 251.78
train mean loss: 251.41
epoch train time: 0:00:00.596253
elapsed time: 0:01:23.423755
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:32:04.528125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.88
 ---- batch: 020 ----
mean loss: 246.78
 ---- batch: 030 ----
mean loss: 252.66
 ---- batch: 040 ----
mean loss: 250.21
train mean loss: 250.67
epoch train time: 0:00:00.612071
elapsed time: 0:01:24.036028
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:32:05.140465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.02
 ---- batch: 020 ----
mean loss: 254.75
 ---- batch: 030 ----
mean loss: 249.69
 ---- batch: 040 ----
mean loss: 247.88
train mean loss: 249.85
epoch train time: 0:00:00.619425
elapsed time: 0:01:24.655742
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:32:05.760116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.86
 ---- batch: 020 ----
mean loss: 248.90
 ---- batch: 030 ----
mean loss: 243.25
 ---- batch: 040 ----
mean loss: 242.78
train mean loss: 248.95
epoch train time: 0:00:00.631890
elapsed time: 0:01:25.287830
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:32:06.392219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.63
 ---- batch: 020 ----
mean loss: 241.52
 ---- batch: 030 ----
mean loss: 249.03
 ---- batch: 040 ----
mean loss: 246.02
train mean loss: 246.22
epoch train time: 0:00:00.609386
elapsed time: 0:01:25.897410
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:32:07.001796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.20
 ---- batch: 020 ----
mean loss: 245.72
 ---- batch: 030 ----
mean loss: 240.81
 ---- batch: 040 ----
mean loss: 241.87
train mean loss: 245.24
epoch train time: 0:00:00.588381
elapsed time: 0:01:26.485989
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:32:07.590401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.55
 ---- batch: 020 ----
mean loss: 253.10
 ---- batch: 030 ----
mean loss: 243.54
 ---- batch: 040 ----
mean loss: 241.01
train mean loss: 244.18
epoch train time: 0:00:00.598125
elapsed time: 0:01:27.084331
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:32:08.188718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.15
 ---- batch: 020 ----
mean loss: 237.27
 ---- batch: 030 ----
mean loss: 244.16
 ---- batch: 040 ----
mean loss: 240.35
train mean loss: 243.17
epoch train time: 0:00:00.610794
elapsed time: 0:01:27.695361
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:32:08.799753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.66
 ---- batch: 020 ----
mean loss: 242.00
 ---- batch: 030 ----
mean loss: 237.05
 ---- batch: 040 ----
mean loss: 242.81
train mean loss: 242.22
epoch train time: 0:00:00.637517
elapsed time: 0:01:28.333128
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:32:09.437560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.87
 ---- batch: 020 ----
mean loss: 242.63
 ---- batch: 030 ----
mean loss: 242.16
 ---- batch: 040 ----
mean loss: 238.27
train mean loss: 240.55
epoch train time: 0:00:00.635697
elapsed time: 0:01:28.969074
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:32:10.073465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.74
 ---- batch: 020 ----
mean loss: 247.25
 ---- batch: 030 ----
mean loss: 235.51
 ---- batch: 040 ----
mean loss: 239.80
train mean loss: 240.19
epoch train time: 0:00:00.612460
elapsed time: 0:01:29.581750
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:32:10.686137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.25
 ---- batch: 020 ----
mean loss: 234.19
 ---- batch: 030 ----
mean loss: 240.10
 ---- batch: 040 ----
mean loss: 245.35
train mean loss: 238.22
epoch train time: 0:00:00.605736
elapsed time: 0:01:30.187675
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:32:11.292060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.48
 ---- batch: 020 ----
mean loss: 232.10
 ---- batch: 030 ----
mean loss: 238.52
 ---- batch: 040 ----
mean loss: 238.23
train mean loss: 237.18
epoch train time: 0:00:00.596734
elapsed time: 0:01:30.784634
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:32:11.889004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.71
 ---- batch: 020 ----
mean loss: 235.86
 ---- batch: 030 ----
mean loss: 235.56
 ---- batch: 040 ----
mean loss: 233.84
train mean loss: 236.73
epoch train time: 0:00:00.611062
elapsed time: 0:01:31.395897
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:32:12.500301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.16
 ---- batch: 020 ----
mean loss: 237.38
 ---- batch: 030 ----
mean loss: 238.35
 ---- batch: 040 ----
mean loss: 244.60
train mean loss: 235.37
epoch train time: 0:00:00.648789
elapsed time: 0:01:32.044917
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:32:13.149305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.74
 ---- batch: 020 ----
mean loss: 239.74
 ---- batch: 030 ----
mean loss: 239.62
 ---- batch: 040 ----
mean loss: 225.64
train mean loss: 235.27
epoch train time: 0:00:00.614439
elapsed time: 0:01:32.659660
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:32:13.764022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.96
 ---- batch: 020 ----
mean loss: 228.86
 ---- batch: 030 ----
mean loss: 236.12
 ---- batch: 040 ----
mean loss: 234.14
train mean loss: 234.31
epoch train time: 0:00:00.612702
elapsed time: 0:01:33.272519
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:32:14.376905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.31
 ---- batch: 020 ----
mean loss: 230.85
 ---- batch: 030 ----
mean loss: 223.50
 ---- batch: 040 ----
mean loss: 238.77
train mean loss: 232.57
epoch train time: 0:00:00.590115
elapsed time: 0:01:33.862823
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:32:14.967203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.17
 ---- batch: 020 ----
mean loss: 232.44
 ---- batch: 030 ----
mean loss: 234.73
 ---- batch: 040 ----
mean loss: 233.91
train mean loss: 231.73
epoch train time: 0:00:00.599947
elapsed time: 0:01:34.462946
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:32:15.567351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.29
 ---- batch: 020 ----
mean loss: 228.69
 ---- batch: 030 ----
mean loss: 232.25
 ---- batch: 040 ----
mean loss: 226.94
train mean loss: 230.19
epoch train time: 0:00:00.624819
elapsed time: 0:01:35.088027
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:32:16.192442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.76
 ---- batch: 020 ----
mean loss: 229.19
 ---- batch: 030 ----
mean loss: 223.69
 ---- batch: 040 ----
mean loss: 231.29
train mean loss: 229.12
epoch train time: 0:00:00.663889
elapsed time: 0:01:35.752232
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:32:16.856647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.55
 ---- batch: 020 ----
mean loss: 228.00
 ---- batch: 030 ----
mean loss: 232.69
 ---- batch: 040 ----
mean loss: 229.45
train mean loss: 228.66
epoch train time: 0:00:00.631426
elapsed time: 0:01:36.383910
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:32:17.488307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.33
 ---- batch: 020 ----
mean loss: 226.67
 ---- batch: 030 ----
mean loss: 233.12
 ---- batch: 040 ----
mean loss: 220.48
train mean loss: 227.87
epoch train time: 0:00:00.608533
elapsed time: 0:01:36.992652
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:32:18.097034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.08
 ---- batch: 020 ----
mean loss: 233.06
 ---- batch: 030 ----
mean loss: 220.79
 ---- batch: 040 ----
mean loss: 230.73
train mean loss: 226.62
epoch train time: 0:00:00.587292
elapsed time: 0:01:37.580156
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:32:18.684540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.98
 ---- batch: 020 ----
mean loss: 229.54
 ---- batch: 030 ----
mean loss: 221.60
 ---- batch: 040 ----
mean loss: 227.86
train mean loss: 225.51
epoch train time: 0:00:00.608484
elapsed time: 0:01:38.188841
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:32:19.293227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.80
 ---- batch: 020 ----
mean loss: 223.65
 ---- batch: 030 ----
mean loss: 223.38
 ---- batch: 040 ----
mean loss: 226.75
train mean loss: 224.63
epoch train time: 0:00:00.605382
elapsed time: 0:01:38.794518
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:32:19.898923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.35
 ---- batch: 020 ----
mean loss: 223.77
 ---- batch: 030 ----
mean loss: 221.91
 ---- batch: 040 ----
mean loss: 224.99
train mean loss: 223.96
epoch train time: 0:00:00.632269
elapsed time: 0:01:39.427031
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:32:20.531424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.30
 ---- batch: 020 ----
mean loss: 223.32
 ---- batch: 030 ----
mean loss: 229.60
 ---- batch: 040 ----
mean loss: 218.62
train mean loss: 223.32
epoch train time: 0:00:00.637764
elapsed time: 0:01:40.065041
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:32:21.169433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.39
 ---- batch: 020 ----
mean loss: 216.66
 ---- batch: 030 ----
mean loss: 223.48
 ---- batch: 040 ----
mean loss: 217.62
train mean loss: 221.91
epoch train time: 0:00:00.601264
elapsed time: 0:01:40.666545
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:32:21.770932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.84
 ---- batch: 020 ----
mean loss: 219.00
 ---- batch: 030 ----
mean loss: 217.57
 ---- batch: 040 ----
mean loss: 228.98
train mean loss: 220.54
epoch train time: 0:00:00.582407
elapsed time: 0:01:41.249179
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:32:22.353580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.00
 ---- batch: 020 ----
mean loss: 220.43
 ---- batch: 030 ----
mean loss: 220.89
 ---- batch: 040 ----
mean loss: 226.52
train mean loss: 220.88
epoch train time: 0:00:00.633758
elapsed time: 0:01:41.883217
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:32:22.987601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.94
 ---- batch: 020 ----
mean loss: 221.49
 ---- batch: 030 ----
mean loss: 217.22
 ---- batch: 040 ----
mean loss: 216.40
train mean loss: 219.45
epoch train time: 0:00:00.583092
elapsed time: 0:01:42.466524
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:32:23.570911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.49
 ---- batch: 020 ----
mean loss: 216.87
 ---- batch: 030 ----
mean loss: 221.21
 ---- batch: 040 ----
mean loss: 219.36
train mean loss: 219.30
epoch train time: 0:00:00.638129
elapsed time: 0:01:43.104861
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:32:24.209246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.04
 ---- batch: 020 ----
mean loss: 220.14
 ---- batch: 030 ----
mean loss: 217.08
 ---- batch: 040 ----
mean loss: 213.96
train mean loss: 217.78
epoch train time: 0:00:00.638377
elapsed time: 0:01:43.743506
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:32:24.847901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.56
 ---- batch: 020 ----
mean loss: 218.50
 ---- batch: 030 ----
mean loss: 216.14
 ---- batch: 040 ----
mean loss: 213.49
train mean loss: 216.73
epoch train time: 0:00:00.618238
elapsed time: 0:01:44.361956
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:32:25.466357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.61
 ---- batch: 020 ----
mean loss: 220.95
 ---- batch: 030 ----
mean loss: 213.29
 ---- batch: 040 ----
mean loss: 218.64
train mean loss: 215.92
epoch train time: 0:00:00.613318
elapsed time: 0:01:44.975565
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:32:26.079953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.96
 ---- batch: 020 ----
mean loss: 212.37
 ---- batch: 030 ----
mean loss: 216.90
 ---- batch: 040 ----
mean loss: 215.24
train mean loss: 216.10
epoch train time: 0:00:00.599469
elapsed time: 0:01:45.575280
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:32:26.679684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.97
 ---- batch: 020 ----
mean loss: 208.29
 ---- batch: 030 ----
mean loss: 218.45
 ---- batch: 040 ----
mean loss: 212.48
train mean loss: 214.89
epoch train time: 0:00:00.587583
elapsed time: 0:01:46.163114
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:32:27.267486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.16
 ---- batch: 020 ----
mean loss: 212.76
 ---- batch: 030 ----
mean loss: 213.68
 ---- batch: 040 ----
mean loss: 213.82
train mean loss: 214.36
epoch train time: 0:00:00.635202
elapsed time: 0:01:46.798534
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:32:27.902973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.62
 ---- batch: 020 ----
mean loss: 209.25
 ---- batch: 030 ----
mean loss: 221.03
 ---- batch: 040 ----
mean loss: 209.63
train mean loss: 212.80
epoch train time: 0:00:00.629133
elapsed time: 0:01:47.427969
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:32:28.532363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.28
 ---- batch: 020 ----
mean loss: 207.60
 ---- batch: 030 ----
mean loss: 217.07
 ---- batch: 040 ----
mean loss: 211.53
train mean loss: 212.34
epoch train time: 0:00:00.617391
elapsed time: 0:01:48.045574
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:32:29.149953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.66
 ---- batch: 020 ----
mean loss: 207.86
 ---- batch: 030 ----
mean loss: 204.19
 ---- batch: 040 ----
mean loss: 217.02
train mean loss: 211.25
epoch train time: 0:00:00.602944
elapsed time: 0:01:48.648703
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:32:29.753089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.27
 ---- batch: 020 ----
mean loss: 210.21
 ---- batch: 030 ----
mean loss: 206.99
 ---- batch: 040 ----
mean loss: 212.09
train mean loss: 211.70
epoch train time: 0:00:00.609047
elapsed time: 0:01:49.257951
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:32:30.362381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.78
 ---- batch: 020 ----
mean loss: 216.31
 ---- batch: 030 ----
mean loss: 207.88
 ---- batch: 040 ----
mean loss: 209.56
train mean loss: 210.04
epoch train time: 0:00:00.604698
elapsed time: 0:01:49.862909
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:32:30.967296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.41
 ---- batch: 020 ----
mean loss: 207.10
 ---- batch: 030 ----
mean loss: 213.96
 ---- batch: 040 ----
mean loss: 209.91
train mean loss: 209.33
epoch train time: 0:00:00.642596
elapsed time: 0:01:50.505730
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:32:31.610118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.35
 ---- batch: 020 ----
mean loss: 215.70
 ---- batch: 030 ----
mean loss: 207.61
 ---- batch: 040 ----
mean loss: 212.55
train mean loss: 208.88
epoch train time: 0:00:00.647697
elapsed time: 0:01:51.153639
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:32:32.258033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.40
 ---- batch: 020 ----
mean loss: 212.00
 ---- batch: 030 ----
mean loss: 212.64
 ---- batch: 040 ----
mean loss: 197.87
train mean loss: 208.62
epoch train time: 0:00:00.637078
elapsed time: 0:01:51.790959
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:32:32.895351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.95
 ---- batch: 020 ----
mean loss: 206.49
 ---- batch: 030 ----
mean loss: 210.21
 ---- batch: 040 ----
mean loss: 201.70
train mean loss: 206.95
epoch train time: 0:00:00.615426
elapsed time: 0:01:52.406603
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:32:33.510995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.31
 ---- batch: 020 ----
mean loss: 198.81
 ---- batch: 030 ----
mean loss: 202.13
 ---- batch: 040 ----
mean loss: 217.57
train mean loss: 206.25
epoch train time: 0:00:00.614884
elapsed time: 0:01:53.021688
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:32:34.126091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.72
 ---- batch: 020 ----
mean loss: 206.27
 ---- batch: 030 ----
mean loss: 201.89
 ---- batch: 040 ----
mean loss: 207.93
train mean loss: 206.00
epoch train time: 0:00:00.615566
elapsed time: 0:01:53.637511
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:32:34.741901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.76
 ---- batch: 020 ----
mean loss: 203.75
 ---- batch: 030 ----
mean loss: 200.51
 ---- batch: 040 ----
mean loss: 204.63
train mean loss: 204.88
epoch train time: 0:00:00.647024
elapsed time: 0:01:54.284764
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:32:35.389156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.83
 ---- batch: 020 ----
mean loss: 201.00
 ---- batch: 030 ----
mean loss: 206.27
 ---- batch: 040 ----
mean loss: 204.39
train mean loss: 204.42
epoch train time: 0:00:00.634854
elapsed time: 0:01:54.919839
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:32:36.024262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.51
 ---- batch: 020 ----
mean loss: 203.94
 ---- batch: 030 ----
mean loss: 207.27
 ---- batch: 040 ----
mean loss: 201.66
train mean loss: 203.10
epoch train time: 0:00:00.630272
elapsed time: 0:01:55.550372
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:32:36.654760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.30
 ---- batch: 020 ----
mean loss: 205.59
 ---- batch: 030 ----
mean loss: 205.37
 ---- batch: 040 ----
mean loss: 203.00
train mean loss: 202.97
epoch train time: 0:00:00.608064
elapsed time: 0:01:56.158638
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:32:37.263023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.23
 ---- batch: 020 ----
mean loss: 198.11
 ---- batch: 030 ----
mean loss: 202.18
 ---- batch: 040 ----
mean loss: 205.67
train mean loss: 202.25
epoch train time: 0:00:00.600022
elapsed time: 0:01:56.758849
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:32:37.863234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.24
 ---- batch: 020 ----
mean loss: 203.63
 ---- batch: 030 ----
mean loss: 208.29
 ---- batch: 040 ----
mean loss: 202.64
train mean loss: 202.13
epoch train time: 0:00:00.609149
elapsed time: 0:01:57.368218
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:32:38.472624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.58
 ---- batch: 020 ----
mean loss: 200.67
 ---- batch: 030 ----
mean loss: 204.20
 ---- batch: 040 ----
mean loss: 201.36
train mean loss: 201.06
epoch train time: 0:00:00.661482
elapsed time: 0:01:58.029963
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:32:39.134349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.96
 ---- batch: 020 ----
mean loss: 198.70
 ---- batch: 030 ----
mean loss: 201.14
 ---- batch: 040 ----
mean loss: 203.13
train mean loss: 200.75
epoch train time: 0:00:00.612104
elapsed time: 0:01:58.642311
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:32:39.746715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.89
 ---- batch: 020 ----
mean loss: 197.97
 ---- batch: 030 ----
mean loss: 206.26
 ---- batch: 040 ----
mean loss: 195.51
train mean loss: 199.85
epoch train time: 0:00:00.629973
elapsed time: 0:01:59.272511
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:32:40.376897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.43
 ---- batch: 020 ----
mean loss: 192.14
 ---- batch: 030 ----
mean loss: 199.96
 ---- batch: 040 ----
mean loss: 198.74
train mean loss: 199.90
epoch train time: 0:00:00.613131
elapsed time: 0:01:59.885830
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:32:40.990232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.97
 ---- batch: 020 ----
mean loss: 193.92
 ---- batch: 030 ----
mean loss: 204.74
 ---- batch: 040 ----
mean loss: 199.15
train mean loss: 198.59
epoch train time: 0:00:00.605192
elapsed time: 0:02:00.491222
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:32:41.595621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.85
 ---- batch: 020 ----
mean loss: 196.43
 ---- batch: 030 ----
mean loss: 192.91
 ---- batch: 040 ----
mean loss: 204.68
train mean loss: 197.70
epoch train time: 0:00:00.591495
elapsed time: 0:02:01.082960
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:32:42.187358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.02
 ---- batch: 020 ----
mean loss: 202.75
 ---- batch: 030 ----
mean loss: 193.46
 ---- batch: 040 ----
mean loss: 196.22
train mean loss: 197.59
epoch train time: 0:00:00.625390
elapsed time: 0:02:01.708645
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:32:42.813005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.42
 ---- batch: 020 ----
mean loss: 196.08
 ---- batch: 030 ----
mean loss: 198.21
 ---- batch: 040 ----
mean loss: 185.80
train mean loss: 196.41
epoch train time: 0:00:00.829973
elapsed time: 0:02:02.538805
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:32:43.643198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.20
 ---- batch: 020 ----
mean loss: 189.20
 ---- batch: 030 ----
mean loss: 195.76
 ---- batch: 040 ----
mean loss: 204.38
train mean loss: 195.97
epoch train time: 0:00:00.618729
elapsed time: 0:02:03.157762
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:32:44.262157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.80
 ---- batch: 020 ----
mean loss: 199.42
 ---- batch: 030 ----
mean loss: 193.24
 ---- batch: 040 ----
mean loss: 192.86
train mean loss: 195.88
epoch train time: 0:00:00.597440
elapsed time: 0:02:03.755417
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:32:44.859851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.79
 ---- batch: 020 ----
mean loss: 188.04
 ---- batch: 030 ----
mean loss: 191.37
 ---- batch: 040 ----
mean loss: 199.90
train mean loss: 195.10
epoch train time: 0:00:00.629122
elapsed time: 0:02:04.384784
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:32:45.489170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.28
 ---- batch: 020 ----
mean loss: 194.86
 ---- batch: 030 ----
mean loss: 192.00
 ---- batch: 040 ----
mean loss: 190.96
train mean loss: 193.85
epoch train time: 0:00:00.618768
elapsed time: 0:02:05.003820
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:32:46.108221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.28
 ---- batch: 020 ----
mean loss: 190.94
 ---- batch: 030 ----
mean loss: 190.90
 ---- batch: 040 ----
mean loss: 200.28
train mean loss: 193.52
epoch train time: 0:00:00.632123
elapsed time: 0:02:05.636222
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:32:46.740619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.73
 ---- batch: 020 ----
mean loss: 193.76
 ---- batch: 030 ----
mean loss: 191.85
 ---- batch: 040 ----
mean loss: 195.13
train mean loss: 192.87
epoch train time: 0:00:00.645750
elapsed time: 0:02:06.282211
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:32:47.386624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.43
 ---- batch: 020 ----
mean loss: 192.00
 ---- batch: 030 ----
mean loss: 192.03
 ---- batch: 040 ----
mean loss: 196.78
train mean loss: 192.45
epoch train time: 0:00:00.622532
elapsed time: 0:02:06.904960
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:32:48.009351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.59
 ---- batch: 020 ----
mean loss: 193.19
 ---- batch: 030 ----
mean loss: 189.36
 ---- batch: 040 ----
mean loss: 192.13
train mean loss: 191.37
epoch train time: 0:00:00.607826
elapsed time: 0:02:07.512979
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:32:48.617388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.83
 ---- batch: 020 ----
mean loss: 191.11
 ---- batch: 030 ----
mean loss: 188.02
 ---- batch: 040 ----
mean loss: 194.69
train mean loss: 191.31
epoch train time: 0:00:00.631025
elapsed time: 0:02:08.144215
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:32:49.248629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.93
 ---- batch: 020 ----
mean loss: 185.69
 ---- batch: 030 ----
mean loss: 190.87
 ---- batch: 040 ----
mean loss: 197.62
train mean loss: 190.71
epoch train time: 0:00:00.633679
elapsed time: 0:02:08.778149
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:32:49.882540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.94
 ---- batch: 020 ----
mean loss: 183.67
 ---- batch: 030 ----
mean loss: 190.94
 ---- batch: 040 ----
mean loss: 193.77
train mean loss: 190.31
epoch train time: 0:00:00.635606
elapsed time: 0:02:09.413960
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:32:50.518349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.31
 ---- batch: 020 ----
mean loss: 191.35
 ---- batch: 030 ----
mean loss: 189.34
 ---- batch: 040 ----
mean loss: 187.01
train mean loss: 189.07
epoch train time: 0:00:00.649874
elapsed time: 0:02:10.064081
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:32:51.168472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.88
 ---- batch: 020 ----
mean loss: 184.96
 ---- batch: 030 ----
mean loss: 191.26
 ---- batch: 040 ----
mean loss: 194.01
train mean loss: 189.32
epoch train time: 0:00:00.616817
elapsed time: 0:02:10.681108
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:32:51.785528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.07
 ---- batch: 020 ----
mean loss: 186.70
 ---- batch: 030 ----
mean loss: 186.56
 ---- batch: 040 ----
mean loss: 189.37
train mean loss: 188.03
epoch train time: 0:00:00.591475
elapsed time: 0:02:11.272816
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:32:52.377197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.97
 ---- batch: 020 ----
mean loss: 198.24
 ---- batch: 030 ----
mean loss: 183.65
 ---- batch: 040 ----
mean loss: 188.21
train mean loss: 188.21
epoch train time: 0:00:00.591368
elapsed time: 0:02:11.864370
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:32:52.968796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.98
 ---- batch: 020 ----
mean loss: 186.54
 ---- batch: 030 ----
mean loss: 194.16
 ---- batch: 040 ----
mean loss: 182.11
train mean loss: 187.23
epoch train time: 0:00:00.605140
elapsed time: 0:02:12.469753
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:32:53.574142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.92
 ---- batch: 020 ----
mean loss: 193.18
 ---- batch: 030 ----
mean loss: 189.98
 ---- batch: 040 ----
mean loss: 183.14
train mean loss: 186.63
epoch train time: 0:00:00.650448
elapsed time: 0:02:13.120442
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:32:54.224856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.85
 ---- batch: 020 ----
mean loss: 184.21
 ---- batch: 030 ----
mean loss: 184.16
 ---- batch: 040 ----
mean loss: 193.40
train mean loss: 186.43
epoch train time: 0:00:00.640796
elapsed time: 0:02:13.761526
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:32:54.865915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.21
 ---- batch: 020 ----
mean loss: 189.68
 ---- batch: 030 ----
mean loss: 184.86
 ---- batch: 040 ----
mean loss: 189.88
train mean loss: 186.14
epoch train time: 0:00:00.593658
elapsed time: 0:02:14.355378
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:32:55.459776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.62
 ---- batch: 020 ----
mean loss: 195.10
 ---- batch: 030 ----
mean loss: 183.96
 ---- batch: 040 ----
mean loss: 181.55
train mean loss: 185.24
epoch train time: 0:00:00.608019
elapsed time: 0:02:14.963626
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:32:56.068011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.81
 ---- batch: 020 ----
mean loss: 189.79
 ---- batch: 030 ----
mean loss: 185.85
 ---- batch: 040 ----
mean loss: 182.17
train mean loss: 184.58
epoch train time: 0:00:00.602889
elapsed time: 0:02:15.566708
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:32:56.671107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.24
 ---- batch: 020 ----
mean loss: 183.91
 ---- batch: 030 ----
mean loss: 180.56
 ---- batch: 040 ----
mean loss: 184.58
train mean loss: 184.63
epoch train time: 0:00:00.603985
elapsed time: 0:02:16.170917
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:32:57.275325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.67
 ---- batch: 020 ----
mean loss: 182.21
 ---- batch: 030 ----
mean loss: 185.95
 ---- batch: 040 ----
mean loss: 180.21
train mean loss: 183.82
epoch train time: 0:00:00.621340
elapsed time: 0:02:16.792482
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:32:57.896882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.56
 ---- batch: 020 ----
mean loss: 182.07
 ---- batch: 030 ----
mean loss: 185.77
 ---- batch: 040 ----
mean loss: 177.62
train mean loss: 183.18
epoch train time: 0:00:00.628013
elapsed time: 0:02:17.420716
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:32:58.525136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.25
 ---- batch: 020 ----
mean loss: 191.87
 ---- batch: 030 ----
mean loss: 178.65
 ---- batch: 040 ----
mean loss: 188.79
train mean loss: 182.75
epoch train time: 0:00:00.636226
elapsed time: 0:02:18.057168
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:32:59.161586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.49
 ---- batch: 020 ----
mean loss: 182.38
 ---- batch: 030 ----
mean loss: 179.19
 ---- batch: 040 ----
mean loss: 180.56
train mean loss: 182.81
epoch train time: 0:00:00.603491
elapsed time: 0:02:18.660886
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:32:59.765273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.99
 ---- batch: 020 ----
mean loss: 185.10
 ---- batch: 030 ----
mean loss: 183.67
 ---- batch: 040 ----
mean loss: 180.25
train mean loss: 182.00
epoch train time: 0:00:00.614652
elapsed time: 0:02:19.275789
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:33:00.380147
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.70
 ---- batch: 020 ----
mean loss: 187.70
 ---- batch: 030 ----
mean loss: 180.25
 ---- batch: 040 ----
mean loss: 178.81
train mean loss: 181.91
epoch train time: 0:00:00.618276
elapsed time: 0:02:19.894272
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:33:00.998658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.42
 ---- batch: 020 ----
mean loss: 186.72
 ---- batch: 030 ----
mean loss: 184.55
 ---- batch: 040 ----
mean loss: 177.49
train mean loss: 182.22
epoch train time: 0:00:00.617520
elapsed time: 0:02:20.512029
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:33:01.616437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.88
 ---- batch: 020 ----
mean loss: 181.41
 ---- batch: 030 ----
mean loss: 181.53
 ---- batch: 040 ----
mean loss: 185.70
train mean loss: 181.99
epoch train time: 0:00:00.634821
elapsed time: 0:02:21.147118
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:33:02.251510
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.71
 ---- batch: 020 ----
mean loss: 186.53
 ---- batch: 030 ----
mean loss: 181.83
 ---- batch: 040 ----
mean loss: 175.26
train mean loss: 181.76
epoch train time: 0:00:00.628181
elapsed time: 0:02:21.775525
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:33:02.879918
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.27
 ---- batch: 020 ----
mean loss: 180.87
 ---- batch: 030 ----
mean loss: 185.86
 ---- batch: 040 ----
mean loss: 180.56
train mean loss: 181.64
epoch train time: 0:00:00.607001
elapsed time: 0:02:22.382738
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:33:03.487127
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.56
 ---- batch: 020 ----
mean loss: 184.48
 ---- batch: 030 ----
mean loss: 183.95
 ---- batch: 040 ----
mean loss: 176.23
train mean loss: 181.84
epoch train time: 0:00:00.599859
elapsed time: 0:02:22.982782
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:33:04.087178
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.92
 ---- batch: 020 ----
mean loss: 187.22
 ---- batch: 030 ----
mean loss: 177.17
 ---- batch: 040 ----
mean loss: 180.28
train mean loss: 181.64
epoch train time: 0:00:00.603293
elapsed time: 0:02:23.586318
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:33:04.690709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.53
 ---- batch: 020 ----
mean loss: 184.37
 ---- batch: 030 ----
mean loss: 182.31
 ---- batch: 040 ----
mean loss: 183.94
train mean loss: 181.43
epoch train time: 0:00:00.637386
elapsed time: 0:02:24.224000
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:33:05.328452
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.23
 ---- batch: 020 ----
mean loss: 181.30
 ---- batch: 030 ----
mean loss: 182.97
 ---- batch: 040 ----
mean loss: 182.69
train mean loss: 181.18
epoch train time: 0:00:00.634225
elapsed time: 0:02:24.858531
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:33:05.962923
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.54
 ---- batch: 020 ----
mean loss: 182.86
 ---- batch: 030 ----
mean loss: 180.36
 ---- batch: 040 ----
mean loss: 183.70
train mean loss: 181.78
epoch train time: 0:00:00.612151
elapsed time: 0:02:25.470914
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:33:06.575327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.03
 ---- batch: 020 ----
mean loss: 184.21
 ---- batch: 030 ----
mean loss: 178.87
 ---- batch: 040 ----
mean loss: 177.97
train mean loss: 181.49
epoch train time: 0:00:00.617344
elapsed time: 0:02:26.088509
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:33:07.192911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.63
 ---- batch: 020 ----
mean loss: 179.28
 ---- batch: 030 ----
mean loss: 180.67
 ---- batch: 040 ----
mean loss: 181.85
train mean loss: 181.21
epoch train time: 0:00:00.611893
elapsed time: 0:02:26.700619
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:33:07.805005
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.21
 ---- batch: 020 ----
mean loss: 185.25
 ---- batch: 030 ----
mean loss: 179.28
 ---- batch: 040 ----
mean loss: 182.23
train mean loss: 181.10
epoch train time: 0:00:00.630006
elapsed time: 0:02:27.330833
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:33:08.435240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.83
 ---- batch: 020 ----
mean loss: 182.01
 ---- batch: 030 ----
mean loss: 177.49
 ---- batch: 040 ----
mean loss: 182.12
train mean loss: 181.51
epoch train time: 0:00:00.657093
elapsed time: 0:02:27.988193
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:33:09.092592
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.18
 ---- batch: 020 ----
mean loss: 178.98
 ---- batch: 030 ----
mean loss: 180.23
 ---- batch: 040 ----
mean loss: 178.23
train mean loss: 181.01
epoch train time: 0:00:00.630763
elapsed time: 0:02:28.619179
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:33:09.723570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.41
 ---- batch: 020 ----
mean loss: 179.29
 ---- batch: 030 ----
mean loss: 179.53
 ---- batch: 040 ----
mean loss: 182.46
train mean loss: 181.55
epoch train time: 0:00:00.612027
elapsed time: 0:02:29.231409
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:33:10.335812
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.70
 ---- batch: 020 ----
mean loss: 187.22
 ---- batch: 030 ----
mean loss: 179.15
 ---- batch: 040 ----
mean loss: 174.72
train mean loss: 181.40
epoch train time: 0:00:00.600721
elapsed time: 0:02:29.832365
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:33:10.936766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.43
 ---- batch: 020 ----
mean loss: 177.02
 ---- batch: 030 ----
mean loss: 179.74
 ---- batch: 040 ----
mean loss: 180.05
train mean loss: 181.12
epoch train time: 0:00:00.582882
elapsed time: 0:02:30.415449
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:33:11.519848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.77
 ---- batch: 020 ----
mean loss: 184.87
 ---- batch: 030 ----
mean loss: 184.60
 ---- batch: 040 ----
mean loss: 177.79
train mean loss: 181.01
epoch train time: 0:00:00.609710
elapsed time: 0:02:31.025468
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:33:12.129866
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.09
 ---- batch: 020 ----
mean loss: 188.36
 ---- batch: 030 ----
mean loss: 169.42
 ---- batch: 040 ----
mean loss: 177.91
train mean loss: 181.07
epoch train time: 0:00:00.644476
elapsed time: 0:02:31.670231
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:33:12.774684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.67
 ---- batch: 020 ----
mean loss: 182.18
 ---- batch: 030 ----
mean loss: 186.17
 ---- batch: 040 ----
mean loss: 180.69
train mean loss: 180.93
epoch train time: 0:00:00.639774
elapsed time: 0:02:32.310309
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:33:13.414697
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.53
 ---- batch: 020 ----
mean loss: 183.51
 ---- batch: 030 ----
mean loss: 177.49
 ---- batch: 040 ----
mean loss: 177.72
train mean loss: 180.60
epoch train time: 0:00:00.608475
elapsed time: 0:02:32.919011
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:33:14.023416
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.18
 ---- batch: 020 ----
mean loss: 181.82
 ---- batch: 030 ----
mean loss: 176.65
 ---- batch: 040 ----
mean loss: 179.97
train mean loss: 181.28
epoch train time: 0:00:00.591638
elapsed time: 0:02:33.510887
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:33:14.615290
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.53
 ---- batch: 020 ----
mean loss: 179.03
 ---- batch: 030 ----
mean loss: 179.80
 ---- batch: 040 ----
mean loss: 181.30
train mean loss: 180.81
epoch train time: 0:00:00.625339
elapsed time: 0:02:34.136456
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:33:15.240845
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.16
 ---- batch: 020 ----
mean loss: 177.90
 ---- batch: 030 ----
mean loss: 179.39
 ---- batch: 040 ----
mean loss: 178.78
train mean loss: 180.80
epoch train time: 0:00:00.605477
elapsed time: 0:02:34.742169
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:33:15.846565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.21
 ---- batch: 020 ----
mean loss: 183.63
 ---- batch: 030 ----
mean loss: 179.90
 ---- batch: 040 ----
mean loss: 176.41
train mean loss: 180.33
epoch train time: 0:00:00.642241
elapsed time: 0:02:35.384648
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:33:16.489040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.98
 ---- batch: 020 ----
mean loss: 176.82
 ---- batch: 030 ----
mean loss: 183.54
 ---- batch: 040 ----
mean loss: 181.98
train mean loss: 180.84
epoch train time: 0:00:00.637983
elapsed time: 0:02:36.022872
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:33:17.127276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.83
 ---- batch: 020 ----
mean loss: 184.37
 ---- batch: 030 ----
mean loss: 173.62
 ---- batch: 040 ----
mean loss: 185.56
train mean loss: 180.75
epoch train time: 0:00:00.615815
elapsed time: 0:02:36.638895
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:33:17.743282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.35
 ---- batch: 020 ----
mean loss: 179.54
 ---- batch: 030 ----
mean loss: 183.26
 ---- batch: 040 ----
mean loss: 185.14
train mean loss: 179.97
epoch train time: 0:00:00.598309
elapsed time: 0:02:37.237402
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:33:18.341792
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.12
 ---- batch: 020 ----
mean loss: 174.51
 ---- batch: 030 ----
mean loss: 185.86
 ---- batch: 040 ----
mean loss: 179.07
train mean loss: 180.48
epoch train time: 0:00:00.604504
elapsed time: 0:02:37.842091
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:33:18.946499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.41
 ---- batch: 020 ----
mean loss: 180.97
 ---- batch: 030 ----
mean loss: 186.71
 ---- batch: 040 ----
mean loss: 176.20
train mean loss: 180.52
epoch train time: 0:00:00.602085
elapsed time: 0:02:38.444413
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:33:19.548803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.46
 ---- batch: 020 ----
mean loss: 185.83
 ---- batch: 030 ----
mean loss: 182.91
 ---- batch: 040 ----
mean loss: 178.46
train mean loss: 180.72
epoch train time: 0:00:00.643149
elapsed time: 0:02:39.087847
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:33:20.192221
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.76
 ---- batch: 020 ----
mean loss: 183.79
 ---- batch: 030 ----
mean loss: 182.50
 ---- batch: 040 ----
mean loss: 175.78
train mean loss: 180.22
epoch train time: 0:00:00.633476
elapsed time: 0:02:39.721530
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:33:20.825918
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.76
 ---- batch: 020 ----
mean loss: 179.75
 ---- batch: 030 ----
mean loss: 177.62
 ---- batch: 040 ----
mean loss: 184.82
train mean loss: 180.10
epoch train time: 0:00:00.620734
elapsed time: 0:02:40.342453
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:33:21.446840
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.19
 ---- batch: 020 ----
mean loss: 174.21
 ---- batch: 030 ----
mean loss: 178.60
 ---- batch: 040 ----
mean loss: 177.90
train mean loss: 180.66
epoch train time: 0:00:00.589228
elapsed time: 0:02:40.931891
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:33:22.036278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.03
 ---- batch: 020 ----
mean loss: 176.76
 ---- batch: 030 ----
mean loss: 178.84
 ---- batch: 040 ----
mean loss: 180.83
train mean loss: 180.16
epoch train time: 0:00:00.591896
elapsed time: 0:02:41.523974
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:33:22.628368
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.95
 ---- batch: 020 ----
mean loss: 173.81
 ---- batch: 030 ----
mean loss: 181.03
 ---- batch: 040 ----
mean loss: 180.77
train mean loss: 179.83
epoch train time: 0:00:00.602233
elapsed time: 0:02:42.126446
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:33:23.230834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.02
 ---- batch: 020 ----
mean loss: 172.10
 ---- batch: 030 ----
mean loss: 181.80
 ---- batch: 040 ----
mean loss: 185.00
train mean loss: 179.79
epoch train time: 0:00:00.627418
elapsed time: 0:02:42.754099
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:33:23.858487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.38
 ---- batch: 020 ----
mean loss: 186.47
 ---- batch: 030 ----
mean loss: 174.85
 ---- batch: 040 ----
mean loss: 184.08
train mean loss: 180.31
epoch train time: 0:00:00.623763
elapsed time: 0:02:43.378092
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:33:24.482504
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.34
 ---- batch: 020 ----
mean loss: 184.65
 ---- batch: 030 ----
mean loss: 182.70
 ---- batch: 040 ----
mean loss: 179.82
train mean loss: 180.18
epoch train time: 0:00:00.645873
elapsed time: 0:02:44.024286
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:33:25.128686
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.42
 ---- batch: 020 ----
mean loss: 182.65
 ---- batch: 030 ----
mean loss: 182.61
 ---- batch: 040 ----
mean loss: 177.89
train mean loss: 179.96
epoch train time: 0:00:00.611339
elapsed time: 0:02:44.635833
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:33:25.740221
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.14
 ---- batch: 020 ----
mean loss: 180.39
 ---- batch: 030 ----
mean loss: 177.26
 ---- batch: 040 ----
mean loss: 183.73
train mean loss: 179.77
epoch train time: 0:00:00.592442
elapsed time: 0:02:45.228500
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:33:26.332904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.81
 ---- batch: 020 ----
mean loss: 182.17
 ---- batch: 030 ----
mean loss: 182.94
 ---- batch: 040 ----
mean loss: 177.41
train mean loss: 179.64
epoch train time: 0:00:00.598324
elapsed time: 0:02:45.827032
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:33:26.931419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.84
 ---- batch: 020 ----
mean loss: 183.66
 ---- batch: 030 ----
mean loss: 179.71
 ---- batch: 040 ----
mean loss: 181.30
train mean loss: 179.84
epoch train time: 0:00:00.610692
elapsed time: 0:02:46.437934
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:33:27.542355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.17
 ---- batch: 020 ----
mean loss: 181.37
 ---- batch: 030 ----
mean loss: 186.68
 ---- batch: 040 ----
mean loss: 173.76
train mean loss: 180.11
epoch train time: 0:00:00.623074
elapsed time: 0:02:47.061293
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:33:28.165693
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.93
 ---- batch: 020 ----
mean loss: 179.77
 ---- batch: 030 ----
mean loss: 175.27
 ---- batch: 040 ----
mean loss: 177.05
train mean loss: 180.28
epoch train time: 0:00:00.638942
elapsed time: 0:02:47.700460
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:33:28.804879
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.78
 ---- batch: 020 ----
mean loss: 178.35
 ---- batch: 030 ----
mean loss: 189.05
 ---- batch: 040 ----
mean loss: 172.52
train mean loss: 179.08
epoch train time: 0:00:00.613545
elapsed time: 0:02:48.314222
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:33:29.418606
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.05
 ---- batch: 020 ----
mean loss: 181.90
 ---- batch: 030 ----
mean loss: 178.61
 ---- batch: 040 ----
mean loss: 174.79
train mean loss: 179.31
epoch train time: 0:00:00.580966
elapsed time: 0:02:48.898775
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_9/checkpoint.pth.tar
**** end time: 2019-09-20 21:33:30.003127 ****
