Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.25/bayesian_dense3_0.25_4', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 23569
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:45:06.429128 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:45:06.438293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4558.37
train mean loss: 4536.61
epoch train time: 0:00:03.931662
elapsed time: 0:00:03.947145
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:45:10.376313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4301.46
train mean loss: 4271.80
epoch train time: 0:00:00.179370
elapsed time: 0:00:04.126636
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:45:10.555818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3999.51
train mean loss: 3996.51
epoch train time: 0:00:00.188093
elapsed time: 0:00:04.314869
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:45:10.744050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3734.91
train mean loss: 3747.37
epoch train time: 0:00:00.175891
elapsed time: 0:00:04.490896
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:45:10.920070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3543.15
train mean loss: 3517.81
epoch train time: 0:00:00.173554
elapsed time: 0:00:04.664578
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:45:11.093816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3345.56
train mean loss: 3350.11
epoch train time: 0:00:00.170573
elapsed time: 0:00:04.835340
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:45:11.264512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3180.94
train mean loss: 3185.69
epoch train time: 0:00:00.167621
elapsed time: 0:00:05.003085
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:45:11.432329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3068.78
train mean loss: 3059.66
epoch train time: 0:00:00.174079
elapsed time: 0:00:05.177357
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:45:11.606530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2942.64
train mean loss: 2935.50
epoch train time: 0:00:00.177769
elapsed time: 0:00:05.355253
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:45:11.784425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2834.06
train mean loss: 2843.72
epoch train time: 0:00:00.189691
elapsed time: 0:00:05.545069
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:45:11.974242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2742.27
train mean loss: 2750.10
epoch train time: 0:00:00.173431
elapsed time: 0:00:05.718647
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:45:12.147843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2679.39
train mean loss: 2667.54
epoch train time: 0:00:00.169374
elapsed time: 0:00:05.888172
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:45:12.317342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2598.27
train mean loss: 2591.23
epoch train time: 0:00:00.187205
elapsed time: 0:00:06.075523
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:45:12.504695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2542.18
train mean loss: 2531.28
epoch train time: 0:00:00.171336
elapsed time: 0:00:06.246989
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:45:12.676160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2471.84
train mean loss: 2477.05
epoch train time: 0:00:00.192523
elapsed time: 0:00:06.439650
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:45:12.868831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2418.86
train mean loss: 2418.13
epoch train time: 0:00:00.177332
elapsed time: 0:00:06.617124
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:45:13.046299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2384.37
train mean loss: 2372.77
epoch train time: 0:00:00.174329
elapsed time: 0:00:06.791592
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:45:13.220780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2324.56
train mean loss: 2324.80
epoch train time: 0:00:00.173548
elapsed time: 0:00:06.965277
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:45:13.394448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2278.94
train mean loss: 2271.08
epoch train time: 0:00:00.173703
elapsed time: 0:00:07.139100
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:45:13.568271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2251.76
train mean loss: 2243.26
epoch train time: 0:00:00.183028
elapsed time: 0:00:07.322251
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:45:13.751423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2193.33
train mean loss: 2208.54
epoch train time: 0:00:00.181462
elapsed time: 0:00:07.503843
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:45:13.933016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2170.60
train mean loss: 2165.50
epoch train time: 0:00:00.169857
elapsed time: 0:00:07.673863
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:45:14.103035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2147.37
train mean loss: 2137.35
epoch train time: 0:00:00.168139
elapsed time: 0:00:07.842161
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:45:14.271337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2096.71
train mean loss: 2098.24
epoch train time: 0:00:00.166383
elapsed time: 0:00:08.008705
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:45:14.437887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2057.37
train mean loss: 2056.26
epoch train time: 0:00:00.172656
elapsed time: 0:00:08.181511
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:45:14.610683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2025.87
train mean loss: 2021.72
epoch train time: 0:00:00.171631
elapsed time: 0:00:08.353351
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:45:14.782537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2002.14
train mean loss: 1995.62
epoch train time: 0:00:00.170711
elapsed time: 0:00:08.524218
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:45:14.953400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1974.32
train mean loss: 1971.77
epoch train time: 0:00:00.173069
elapsed time: 0:00:08.698347
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:45:15.127530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1936.48
train mean loss: 1941.90
epoch train time: 0:00:00.167441
elapsed time: 0:00:08.865915
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:45:15.295099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1894.76
train mean loss: 1909.71
epoch train time: 0:00:00.166916
elapsed time: 0:00:09.032966
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:45:15.462135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1872.22
train mean loss: 1879.85
epoch train time: 0:00:00.166167
elapsed time: 0:00:09.199242
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:45:15.628408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1843.68
train mean loss: 1845.00
epoch train time: 0:00:00.169480
elapsed time: 0:00:09.368844
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:45:15.798017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1829.74
train mean loss: 1831.37
epoch train time: 0:00:00.180896
elapsed time: 0:00:09.549880
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:45:15.979066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1793.54
train mean loss: 1790.26
epoch train time: 0:00:00.173163
elapsed time: 0:00:09.723207
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:45:16.152397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1765.62
train mean loss: 1765.84
epoch train time: 0:00:00.167919
elapsed time: 0:00:09.891276
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:45:16.320474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1735.11
train mean loss: 1740.05
epoch train time: 0:00:00.171159
elapsed time: 0:00:10.062591
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:45:16.491769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1720.56
train mean loss: 1721.36
epoch train time: 0:00:00.171400
elapsed time: 0:00:10.234122
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:45:16.663293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1688.15
train mean loss: 1693.71
epoch train time: 0:00:00.181049
elapsed time: 0:00:10.415296
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:45:16.844498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1665.11
train mean loss: 1664.55
epoch train time: 0:00:00.172982
elapsed time: 0:00:10.588483
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:45:17.017656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1640.46
train mean loss: 1652.32
epoch train time: 0:00:00.178395
elapsed time: 0:00:10.767036
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:45:17.196207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1625.58
train mean loss: 1630.41
epoch train time: 0:00:00.172414
elapsed time: 0:00:10.939579
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:45:17.368752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1597.49
train mean loss: 1598.36
epoch train time: 0:00:00.172653
elapsed time: 0:00:11.112361
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:45:17.541536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1582.94
train mean loss: 1585.52
epoch train time: 0:00:00.183309
elapsed time: 0:00:11.295810
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:45:17.724987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1559.24
train mean loss: 1552.23
epoch train time: 0:00:00.177294
elapsed time: 0:00:11.473241
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:45:17.902462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1526.48
train mean loss: 1534.97
epoch train time: 0:00:00.171992
elapsed time: 0:00:11.645411
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:45:18.074588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1516.18
train mean loss: 1521.31
epoch train time: 0:00:00.175404
elapsed time: 0:00:11.820944
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:45:18.250114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1498.12
train mean loss: 1498.95
epoch train time: 0:00:00.169628
elapsed time: 0:00:11.990695
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:45:18.419865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1478.60
train mean loss: 1476.37
epoch train time: 0:00:00.168911
elapsed time: 0:00:12.159721
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:45:18.588890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1454.92
train mean loss: 1461.32
epoch train time: 0:00:00.175212
elapsed time: 0:00:12.335054
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:45:18.764226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1450.76
train mean loss: 1447.25
epoch train time: 0:00:00.184867
elapsed time: 0:00:12.520121
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:45:18.949308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1414.93
train mean loss: 1423.19
epoch train time: 0:00:00.171016
elapsed time: 0:00:12.691288
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:45:19.120464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1399.62
train mean loss: 1404.84
epoch train time: 0:00:00.173144
elapsed time: 0:00:12.864562
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:45:19.293734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1389.10
train mean loss: 1383.92
epoch train time: 0:00:00.172581
elapsed time: 0:00:13.037277
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:45:19.466449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1372.58
train mean loss: 1370.28
epoch train time: 0:00:00.170891
elapsed time: 0:00:13.208293
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:45:19.637482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1349.39
train mean loss: 1348.84
epoch train time: 0:00:00.185331
elapsed time: 0:00:13.393777
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:45:19.822953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1329.68
train mean loss: 1335.05
epoch train time: 0:00:00.179825
elapsed time: 0:00:13.573732
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:45:20.002904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1312.53
train mean loss: 1314.04
epoch train time: 0:00:00.177153
elapsed time: 0:00:13.751010
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:45:20.180184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1305.90
train mean loss: 1301.38
epoch train time: 0:00:00.171782
elapsed time: 0:00:13.922912
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:45:20.352088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1291.07
train mean loss: 1285.95
epoch train time: 0:00:00.171470
elapsed time: 0:00:14.094509
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:45:20.523696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1263.44
train mean loss: 1266.50
epoch train time: 0:00:00.175454
elapsed time: 0:00:14.270102
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:45:20.699275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1248.38
train mean loss: 1250.99
epoch train time: 0:00:00.176662
elapsed time: 0:00:14.446891
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:45:20.876069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1236.26
train mean loss: 1236.57
epoch train time: 0:00:00.174250
elapsed time: 0:00:14.621273
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:45:21.050446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1217.95
train mean loss: 1227.24
epoch train time: 0:00:00.174449
elapsed time: 0:00:14.795862
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:45:21.225035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1210.54
train mean loss: 1210.89
epoch train time: 0:00:00.171277
elapsed time: 0:00:14.967266
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:45:21.396438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1177.82
train mean loss: 1177.19
epoch train time: 0:00:00.168311
elapsed time: 0:00:15.135698
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:45:21.564871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1157.29
train mean loss: 1159.96
epoch train time: 0:00:00.181808
elapsed time: 0:00:15.317652
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:45:21.746823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1134.60
train mean loss: 1130.20
epoch train time: 0:00:00.169715
elapsed time: 0:00:15.487504
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:45:21.916674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1096.74
train mean loss: 1092.01
epoch train time: 0:00:00.169440
elapsed time: 0:00:15.657067
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:45:22.086237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1054.12
train mean loss: 1055.15
epoch train time: 0:00:00.171356
elapsed time: 0:00:15.828545
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:45:22.257725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1029.40
train mean loss: 1027.97
epoch train time: 0:00:00.172854
elapsed time: 0:00:16.001528
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:45:22.430698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1009.91
train mean loss: 1008.23
epoch train time: 0:00:00.167562
elapsed time: 0:00:16.169211
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:45:22.598379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 986.93
train mean loss: 992.85
epoch train time: 0:00:00.182421
elapsed time: 0:00:16.351752
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:45:22.780937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 976.74
train mean loss: 974.54
epoch train time: 0:00:00.176031
elapsed time: 0:00:16.527925
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:45:22.957098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.86
train mean loss: 954.18
epoch train time: 0:00:00.177885
elapsed time: 0:00:16.705940
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:45:23.135119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.55
train mean loss: 941.20
epoch train time: 0:00:00.176611
elapsed time: 0:00:16.882701
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:45:23.311873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.58
train mean loss: 921.55
epoch train time: 0:00:00.172379
elapsed time: 0:00:17.055197
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:45:23.484367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.44
train mean loss: 908.17
epoch train time: 0:00:00.173979
elapsed time: 0:00:17.229331
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:45:23.658515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.57
train mean loss: 881.73
epoch train time: 0:00:00.174463
elapsed time: 0:00:17.403926
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:45:23.833095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.08
train mean loss: 878.75
epoch train time: 0:00:00.171486
elapsed time: 0:00:17.575528
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:45:24.004714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.92
train mean loss: 854.14
epoch train time: 0:00:00.174036
elapsed time: 0:00:17.749700
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:45:24.178870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.53
train mean loss: 843.57
epoch train time: 0:00:00.174131
elapsed time: 0:00:17.924000
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:45:24.353192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 833.83
train mean loss: 831.14
epoch train time: 0:00:00.177576
elapsed time: 0:00:18.101720
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:45:24.530892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 819.09
train mean loss: 815.51
epoch train time: 0:00:00.173796
elapsed time: 0:00:18.275708
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:45:24.704923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 802.70
train mean loss: 797.87
epoch train time: 0:00:00.175671
elapsed time: 0:00:18.451548
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:45:24.880721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 782.72
train mean loss: 790.40
epoch train time: 0:00:00.171204
elapsed time: 0:00:18.622877
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:45:25.052050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 774.88
train mean loss: 776.73
epoch train time: 0:00:00.172190
elapsed time: 0:00:18.795191
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:45:25.224363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 754.95
train mean loss: 757.80
epoch train time: 0:00:00.172334
elapsed time: 0:00:18.967684
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:45:25.396857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 739.47
train mean loss: 735.84
epoch train time: 0:00:00.170393
elapsed time: 0:00:19.138202
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:45:25.567374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.56
train mean loss: 735.91
epoch train time: 0:00:00.169143
elapsed time: 0:00:19.307477
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:45:25.736653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.91
train mean loss: 721.99
epoch train time: 0:00:00.186585
elapsed time: 0:00:19.494200
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:45:25.923389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.22
train mean loss: 701.72
epoch train time: 0:00:00.174090
elapsed time: 0:00:19.668429
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:45:26.097599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.99
train mean loss: 691.99
epoch train time: 0:00:00.175394
elapsed time: 0:00:19.843944
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:45:26.273119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 688.01
train mean loss: 685.09
epoch train time: 0:00:00.178903
elapsed time: 0:00:20.022980
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:45:26.452155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 672.12
train mean loss: 670.64
epoch train time: 0:00:00.172589
elapsed time: 0:00:20.195688
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:45:26.624855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 659.97
train mean loss: 660.38
epoch train time: 0:00:00.178191
elapsed time: 0:00:20.374008
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:45:26.803185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 652.39
train mean loss: 646.59
epoch train time: 0:00:00.181677
elapsed time: 0:00:20.555819
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:45:26.985001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 640.08
train mean loss: 638.42
epoch train time: 0:00:00.175611
elapsed time: 0:00:20.731568
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:45:27.160743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 622.43
train mean loss: 630.80
epoch train time: 0:00:00.174756
elapsed time: 0:00:20.906455
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:45:27.335627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 618.86
train mean loss: 622.43
epoch train time: 0:00:00.177180
elapsed time: 0:00:21.083779
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:45:27.512951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 607.76
train mean loss: 604.18
epoch train time: 0:00:00.179519
elapsed time: 0:00:21.263445
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:45:27.692619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.90
train mean loss: 596.34
epoch train time: 0:00:00.177042
elapsed time: 0:00:21.440616
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:45:27.869833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.44
train mean loss: 580.89
epoch train time: 0:00:00.175245
elapsed time: 0:00:21.616042
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:45:28.045222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 574.75
train mean loss: 572.77
epoch train time: 0:00:00.171528
elapsed time: 0:00:21.787701
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:45:28.216873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 565.41
train mean loss: 569.21
epoch train time: 0:00:00.171788
elapsed time: 0:00:21.959633
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:45:28.388808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 561.11
train mean loss: 558.00
epoch train time: 0:00:00.169637
elapsed time: 0:00:22.129399
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:45:28.558570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.99
train mean loss: 544.96
epoch train time: 0:00:00.176576
elapsed time: 0:00:22.306104
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:45:28.735292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 541.51
train mean loss: 543.53
epoch train time: 0:00:00.174968
elapsed time: 0:00:22.481240
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:45:28.910418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.01
train mean loss: 527.23
epoch train time: 0:00:00.170673
elapsed time: 0:00:22.652043
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:45:29.081215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.26
train mean loss: 522.44
epoch train time: 0:00:00.169750
elapsed time: 0:00:22.821921
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:45:29.251111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.33
train mean loss: 510.06
epoch train time: 0:00:00.169613
elapsed time: 0:00:22.991677
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:45:29.420852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.27
train mean loss: 502.00
epoch train time: 0:00:00.177414
elapsed time: 0:00:23.169226
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:45:29.598401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.23
train mean loss: 493.37
epoch train time: 0:00:00.179932
elapsed time: 0:00:23.349292
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:45:29.778465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.39
train mean loss: 482.52
epoch train time: 0:00:00.178615
elapsed time: 0:00:23.528041
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:45:29.957223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.14
train mean loss: 473.80
epoch train time: 0:00:00.171566
elapsed time: 0:00:23.699744
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:45:30.128918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.97
train mean loss: 463.36
epoch train time: 0:00:00.175547
elapsed time: 0:00:23.875420
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:45:30.304595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.13
train mean loss: 453.29
epoch train time: 0:00:00.172773
elapsed time: 0:00:24.048337
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:45:30.477520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.67
train mean loss: 448.54
epoch train time: 0:00:00.174242
elapsed time: 0:00:24.222716
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:45:30.651892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.66
train mean loss: 437.62
epoch train time: 0:00:00.181896
elapsed time: 0:00:24.404756
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:45:30.833941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.42
train mean loss: 431.57
epoch train time: 0:00:00.175523
elapsed time: 0:00:24.580416
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:45:31.009589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.38
train mean loss: 422.29
epoch train time: 0:00:00.174768
elapsed time: 0:00:24.755308
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:45:31.184494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.03
train mean loss: 415.64
epoch train time: 0:00:00.167831
elapsed time: 0:00:24.923291
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:45:31.352463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.20
train mean loss: 405.30
epoch train time: 0:00:00.170099
elapsed time: 0:00:25.093546
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:45:31.522720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.66
train mean loss: 400.40
epoch train time: 0:00:00.176905
elapsed time: 0:00:25.270584
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:45:31.699772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.46
train mean loss: 395.32
epoch train time: 0:00:00.183720
elapsed time: 0:00:25.454467
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:45:31.883642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.10
train mean loss: 385.18
epoch train time: 0:00:00.179058
elapsed time: 0:00:25.633696
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:45:32.062885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.92
train mean loss: 384.76
epoch train time: 0:00:00.177175
elapsed time: 0:00:25.811010
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:45:32.240186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.42
train mean loss: 379.33
epoch train time: 0:00:00.172669
elapsed time: 0:00:25.983837
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:45:32.413020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.09
train mean loss: 371.71
epoch train time: 0:00:00.176771
elapsed time: 0:00:26.160763
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:45:32.589937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.12
train mean loss: 361.50
epoch train time: 0:00:00.189335
elapsed time: 0:00:26.350250
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:45:32.779471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.75
train mean loss: 364.60
epoch train time: 0:00:00.188759
elapsed time: 0:00:26.539190
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:45:32.968362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.68
train mean loss: 350.75
epoch train time: 0:00:00.172893
elapsed time: 0:00:26.712210
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:45:33.141384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.78
train mean loss: 344.94
epoch train time: 0:00:00.175350
elapsed time: 0:00:26.887701
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:45:33.316878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.35
train mean loss: 341.48
epoch train time: 0:00:00.175606
elapsed time: 0:00:27.063441
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:45:33.492618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.47
train mean loss: 333.96
epoch train time: 0:00:00.178548
elapsed time: 0:00:27.242135
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:45:33.671313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.97
train mean loss: 329.17
epoch train time: 0:00:00.176302
elapsed time: 0:00:27.418573
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:45:33.847749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.37
train mean loss: 322.25
epoch train time: 0:00:00.180577
elapsed time: 0:00:27.599284
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:45:34.028457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.74
train mean loss: 320.27
epoch train time: 0:00:00.180207
elapsed time: 0:00:27.779626
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:45:34.208803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.08
train mean loss: 311.77
epoch train time: 0:00:00.180993
elapsed time: 0:00:27.960778
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:45:34.389953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.31
train mean loss: 310.54
epoch train time: 0:00:00.179701
elapsed time: 0:00:28.140608
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:45:34.569807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.90
train mean loss: 304.60
epoch train time: 0:00:00.177183
elapsed time: 0:00:28.317953
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:45:34.747127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.54
train mean loss: 295.70
epoch train time: 0:00:00.191437
elapsed time: 0:00:28.509515
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:45:34.938687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.37
train mean loss: 295.63
epoch train time: 0:00:00.176663
elapsed time: 0:00:28.686306
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:45:35.115493
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.81
train mean loss: 288.25
epoch train time: 0:00:00.173788
elapsed time: 0:00:28.860865
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:45:35.290081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.69
train mean loss: 289.72
epoch train time: 0:00:00.171967
elapsed time: 0:00:29.033007
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:45:35.462181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.34
train mean loss: 278.48
epoch train time: 0:00:00.178647
elapsed time: 0:00:29.211789
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:45:35.640979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.18
train mean loss: 277.79
epoch train time: 0:00:00.186698
elapsed time: 0:00:29.398635
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:45:35.827811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.20
train mean loss: 272.52
epoch train time: 0:00:00.174900
elapsed time: 0:00:29.573667
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:45:36.002839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.35
train mean loss: 266.37
epoch train time: 0:00:00.173768
elapsed time: 0:00:29.747558
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:45:36.176736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.77
train mean loss: 264.74
epoch train time: 0:00:00.169407
elapsed time: 0:00:29.917110
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:45:36.346276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.37
train mean loss: 258.99
epoch train time: 0:00:00.172323
elapsed time: 0:00:30.089564
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:45:36.518766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.57
train mean loss: 255.58
epoch train time: 0:00:00.174084
elapsed time: 0:00:30.263838
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:45:36.693027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.05
train mean loss: 251.90
epoch train time: 0:00:00.179794
elapsed time: 0:00:30.443781
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:45:36.872954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.23
train mean loss: 248.92
epoch train time: 0:00:00.178235
elapsed time: 0:00:30.622154
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:45:37.051329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.16
train mean loss: 245.16
epoch train time: 0:00:00.174858
elapsed time: 0:00:30.797143
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:45:37.226315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.06
train mean loss: 238.90
epoch train time: 0:00:00.174274
elapsed time: 0:00:30.971553
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:45:37.400736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.58
train mean loss: 235.45
epoch train time: 0:00:00.191078
elapsed time: 0:00:31.162770
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:45:37.591942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.91
train mean loss: 233.13
epoch train time: 0:00:00.177877
elapsed time: 0:00:31.340797
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:45:37.770003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.64
train mean loss: 228.71
epoch train time: 0:00:00.191737
elapsed time: 0:00:31.532732
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:45:37.961909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.98
train mean loss: 227.06
epoch train time: 0:00:00.180455
elapsed time: 0:00:31.713319
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:45:38.142492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.33
train mean loss: 224.19
epoch train time: 0:00:00.177145
elapsed time: 0:00:31.890589
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:45:38.319762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.75
train mean loss: 217.50
epoch train time: 0:00:00.176383
elapsed time: 0:00:32.067101
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:45:38.496275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.84
train mean loss: 214.97
epoch train time: 0:00:00.178566
elapsed time: 0:00:32.245797
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:45:38.674973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.29
train mean loss: 211.49
epoch train time: 0:00:00.183221
elapsed time: 0:00:32.429155
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:45:38.858333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.92
train mean loss: 212.89
epoch train time: 0:00:00.181769
elapsed time: 0:00:32.611056
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:45:39.040245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.75
train mean loss: 207.63
epoch train time: 0:00:00.175166
elapsed time: 0:00:32.786367
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:45:39.215542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.07
train mean loss: 206.73
epoch train time: 0:00:00.169649
elapsed time: 0:00:32.956149
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:45:39.385322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.95
train mean loss: 202.40
epoch train time: 0:00:00.172823
elapsed time: 0:00:33.129097
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:45:39.558271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.60
train mean loss: 199.03
epoch train time: 0:00:00.172337
elapsed time: 0:00:33.301575
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:45:39.730761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.18
train mean loss: 195.80
epoch train time: 0:00:00.191767
elapsed time: 0:00:33.493484
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:45:39.922660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.62
train mean loss: 192.10
epoch train time: 0:00:00.176214
elapsed time: 0:00:33.669827
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:45:40.098999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.24
train mean loss: 192.00
epoch train time: 0:00:00.173801
elapsed time: 0:00:33.843751
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:45:40.272923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.50
train mean loss: 188.12
epoch train time: 0:00:00.173921
elapsed time: 0:00:34.017797
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:45:40.446970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.53
train mean loss: 187.78
epoch train time: 0:00:00.172824
elapsed time: 0:00:34.190758
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:45:40.619972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.91
train mean loss: 184.73
epoch train time: 0:00:00.178367
elapsed time: 0:00:34.369337
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:45:40.798506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.40
train mean loss: 179.87
epoch train time: 0:00:00.181960
elapsed time: 0:00:34.551425
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:45:40.980602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.50
train mean loss: 178.91
epoch train time: 0:00:00.181123
elapsed time: 0:00:34.732700
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:45:41.161894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.45
train mean loss: 176.69
epoch train time: 0:00:00.180039
elapsed time: 0:00:34.912905
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:45:41.342082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.86
train mean loss: 175.46
epoch train time: 0:00:00.178060
elapsed time: 0:00:35.091111
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:45:41.520286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.39
train mean loss: 172.27
epoch train time: 0:00:00.182090
elapsed time: 0:00:35.273334
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:45:41.702509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.96
train mean loss: 168.22
epoch train time: 0:00:00.193547
elapsed time: 0:00:35.467015
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:45:41.896194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.25
train mean loss: 169.93
epoch train time: 0:00:00.179016
elapsed time: 0:00:35.646167
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:45:42.075344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.85
train mean loss: 164.50
epoch train time: 0:00:00.180035
elapsed time: 0:00:35.826329
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:45:42.255529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.68
train mean loss: 164.21
epoch train time: 0:00:00.181663
elapsed time: 0:00:36.008181
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:45:42.437405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.48
train mean loss: 162.85
epoch train time: 0:00:00.182053
elapsed time: 0:00:36.190410
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:45:42.619582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.16
train mean loss: 159.37
epoch train time: 0:00:00.188295
elapsed time: 0:00:36.378832
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:45:42.808005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.33
train mean loss: 155.80
epoch train time: 0:00:00.176644
elapsed time: 0:00:36.555602
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:45:42.984783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.49
train mean loss: 154.69
epoch train time: 0:00:00.176198
elapsed time: 0:00:36.731954
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:45:43.161129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.01
train mean loss: 151.77
epoch train time: 0:00:00.173515
elapsed time: 0:00:36.905640
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:45:43.334849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.90
train mean loss: 152.15
epoch train time: 0:00:00.174254
elapsed time: 0:00:37.080058
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:45:43.509233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.76
train mean loss: 150.88
epoch train time: 0:00:00.179189
elapsed time: 0:00:37.259382
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:45:43.688556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.24
train mean loss: 148.30
epoch train time: 0:00:00.183960
elapsed time: 0:00:37.443479
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:45:43.872653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.91
train mean loss: 146.06
epoch train time: 0:00:00.176584
elapsed time: 0:00:37.620201
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:45:44.049375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.11
train mean loss: 146.75
epoch train time: 0:00:00.168547
elapsed time: 0:00:37.788871
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:45:44.218041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.86
train mean loss: 145.21
epoch train time: 0:00:00.172050
elapsed time: 0:00:37.961043
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:45:44.390230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.17
train mean loss: 142.81
epoch train time: 0:00:00.167489
elapsed time: 0:00:38.128695
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:45:44.557870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.66
train mean loss: 140.86
epoch train time: 0:00:00.171910
elapsed time: 0:00:38.300737
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:45:44.729910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.05
train mean loss: 138.48
epoch train time: 0:00:00.174967
elapsed time: 0:00:38.475831
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:45:44.905001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.40
train mean loss: 137.38
epoch train time: 0:00:00.176178
elapsed time: 0:00:38.652136
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:45:45.081320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.36
train mean loss: 136.67
epoch train time: 0:00:00.173416
elapsed time: 0:00:38.825700
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:45:45.254875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.87
train mean loss: 131.79
epoch train time: 0:00:00.174662
elapsed time: 0:00:39.000494
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:45:45.429683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.95
train mean loss: 131.69
epoch train time: 0:00:00.177854
elapsed time: 0:00:39.178492
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:45:45.607667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.10
train mean loss: 131.37
epoch train time: 0:00:00.177651
elapsed time: 0:00:39.356285
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:45:45.785481
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.10
train mean loss: 129.54
epoch train time: 0:00:00.181294
elapsed time: 0:00:39.537726
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:45:45.966896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.14
train mean loss: 132.32
epoch train time: 0:00:00.175752
elapsed time: 0:00:39.713615
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:45:46.142789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.03
train mean loss: 129.69
epoch train time: 0:00:00.174801
elapsed time: 0:00:39.888560
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:45:46.317751
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.15
train mean loss: 130.52
epoch train time: 0:00:00.173250
elapsed time: 0:00:40.061956
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:45:46.491138
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.61
train mean loss: 130.70
epoch train time: 0:00:00.169486
elapsed time: 0:00:40.231580
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:45:46.660755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.45
train mean loss: 130.42
epoch train time: 0:00:00.186205
elapsed time: 0:00:40.417916
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:45:46.847091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.40
train mean loss: 131.56
epoch train time: 0:00:00.173252
elapsed time: 0:00:40.591295
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:45:47.020470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.18
train mean loss: 134.20
epoch train time: 0:00:00.176539
elapsed time: 0:00:40.767986
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:45:47.197161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.89
train mean loss: 128.80
epoch train time: 0:00:00.175304
elapsed time: 0:00:40.943421
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:45:47.372597
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.95
train mean loss: 132.51
epoch train time: 0:00:00.174562
elapsed time: 0:00:41.118121
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:45:47.547298
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.75
train mean loss: 129.51
epoch train time: 0:00:00.177797
elapsed time: 0:00:41.296078
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:45:47.725267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.06
train mean loss: 129.59
epoch train time: 0:00:00.183187
elapsed time: 0:00:41.479417
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:45:47.908594
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.26
train mean loss: 129.32
epoch train time: 0:00:00.180595
elapsed time: 0:00:41.660154
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:45:48.089332
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.30
train mean loss: 130.21
epoch train time: 0:00:00.179927
elapsed time: 0:00:41.840221
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:45:48.269401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.93
train mean loss: 130.92
epoch train time: 0:00:00.181399
elapsed time: 0:00:42.021757
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:45:48.450932
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.23
train mean loss: 131.38
epoch train time: 0:00:00.176670
elapsed time: 0:00:42.198561
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:45:48.627750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.09
train mean loss: 129.34
epoch train time: 0:00:00.197019
elapsed time: 0:00:42.395758
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:45:48.824952
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.18
train mean loss: 127.01
epoch train time: 0:00:00.184976
elapsed time: 0:00:42.580886
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:45:49.010060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.24
train mean loss: 129.61
epoch train time: 0:00:00.180900
elapsed time: 0:00:42.761918
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:45:49.191092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.17
train mean loss: 130.43
epoch train time: 0:00:00.178606
elapsed time: 0:00:42.940679
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:45:49.369856
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.37
train mean loss: 130.73
epoch train time: 0:00:00.206589
elapsed time: 0:00:43.147417
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:45:49.576650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.84
train mean loss: 129.37
epoch train time: 0:00:00.179822
elapsed time: 0:00:43.327430
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:45:49.756605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.80
train mean loss: 128.81
epoch train time: 0:00:00.183381
elapsed time: 0:00:43.510940
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:45:49.940120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.05
train mean loss: 126.96
epoch train time: 0:00:00.178105
elapsed time: 0:00:43.689208
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:45:50.118381
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.30
train mean loss: 127.61
epoch train time: 0:00:00.201083
elapsed time: 0:00:43.890418
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:45:50.319606
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.34
train mean loss: 129.37
epoch train time: 0:00:00.178364
elapsed time: 0:00:44.068958
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:45:50.498133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.90
train mean loss: 128.03
epoch train time: 0:00:00.172680
elapsed time: 0:00:44.241765
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:45:50.670940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.40
train mean loss: 130.23
epoch train time: 0:00:00.177327
elapsed time: 0:00:44.419220
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:45:50.848394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.39
train mean loss: 129.27
epoch train time: 0:00:00.174420
elapsed time: 0:00:44.593766
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:45:51.022955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.50
train mean loss: 128.21
epoch train time: 0:00:00.175128
elapsed time: 0:00:44.769041
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:45:51.198229
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.47
train mean loss: 125.83
epoch train time: 0:00:00.174594
elapsed time: 0:00:44.943808
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:45:51.372985
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.27
train mean loss: 127.06
epoch train time: 0:00:00.169416
elapsed time: 0:00:45.113366
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:45:51.542530
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.18
train mean loss: 126.21
epoch train time: 0:00:00.173099
elapsed time: 0:00:45.286591
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:45:51.715766
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.61
train mean loss: 128.31
epoch train time: 0:00:00.177740
elapsed time: 0:00:45.464457
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:45:51.893627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.39
train mean loss: 128.27
epoch train time: 0:00:00.171868
elapsed time: 0:00:45.636445
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:45:52.065631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.00
train mean loss: 126.61
epoch train time: 0:00:00.174511
elapsed time: 0:00:45.811098
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:45:52.240272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.96
train mean loss: 126.71
epoch train time: 0:00:00.175145
elapsed time: 0:00:45.986380
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:45:52.415568
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.19
train mean loss: 127.11
epoch train time: 0:00:00.171741
elapsed time: 0:00:46.158262
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:45:52.587440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.27
train mean loss: 127.70
epoch train time: 0:00:00.183771
elapsed time: 0:00:46.342176
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:45:52.771351
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.20
train mean loss: 126.70
epoch train time: 0:00:00.177216
elapsed time: 0:00:46.519524
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:45:52.948708
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.11
train mean loss: 124.00
epoch train time: 0:00:00.193285
elapsed time: 0:00:46.712968
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:45:53.142140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.06
train mean loss: 125.80
epoch train time: 0:00:00.176317
elapsed time: 0:00:46.889445
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:45:53.318615
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.24
train mean loss: 127.49
epoch train time: 0:00:00.175442
elapsed time: 0:00:47.065015
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:45:53.494186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.91
train mean loss: 127.53
epoch train time: 0:00:00.174033
elapsed time: 0:00:47.239185
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:45:53.668359
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.55
train mean loss: 124.46
epoch train time: 0:00:00.178557
elapsed time: 0:00:47.417876
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:45:53.847055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.57
train mean loss: 125.24
epoch train time: 0:00:00.176803
elapsed time: 0:00:47.594808
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:45:54.023980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.84
train mean loss: 125.53
epoch train time: 0:00:00.174419
elapsed time: 0:00:47.769351
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:45:54.198552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.00
train mean loss: 124.71
epoch train time: 0:00:00.181336
elapsed time: 0:00:47.958378
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.25/bayesian_dense3_0.25_4/checkpoint.pth.tar
**** end time: 2019-10-01 14:45:54.387525 ****
