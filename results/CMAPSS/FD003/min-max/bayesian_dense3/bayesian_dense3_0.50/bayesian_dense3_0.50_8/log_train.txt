Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 23106
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-10-01 14:36:43.153574 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-10-01 14:36:43.163279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4449.62
 ---- batch: 020 ----
mean loss: 4129.06
train mean loss: 4266.69
epoch train time: 0:00:07.917182
elapsed time: 0:00:07.933673
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-10-01 14:36:51.087297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3780.43
 ---- batch: 020 ----
mean loss: 3507.49
train mean loss: 3619.58
epoch train time: 0:00:00.360784
elapsed time: 0:00:08.294672
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-10-01 14:36:51.448325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3183.44
 ---- batch: 020 ----
mean loss: 3104.35
train mean loss: 3136.10
epoch train time: 0:00:00.368693
elapsed time: 0:00:08.663570
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-10-01 14:36:51.817205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2907.99
 ---- batch: 020 ----
mean loss: 2782.04
train mean loss: 2823.02
epoch train time: 0:00:00.352588
elapsed time: 0:00:09.016334
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-10-01 14:36:52.169979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2672.63
 ---- batch: 020 ----
mean loss: 2582.56
train mean loss: 2635.01
epoch train time: 0:00:00.350739
elapsed time: 0:00:09.367256
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-10-01 14:36:52.520908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2521.13
 ---- batch: 020 ----
mean loss: 2438.76
train mean loss: 2468.24
epoch train time: 0:00:00.359885
elapsed time: 0:00:09.727354
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-10-01 14:36:52.881049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2403.50
 ---- batch: 020 ----
mean loss: 2328.52
train mean loss: 2362.36
epoch train time: 0:00:00.356319
elapsed time: 0:00:10.083928
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-10-01 14:36:53.237566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2290.52
 ---- batch: 020 ----
mean loss: 2221.77
train mean loss: 2248.10
epoch train time: 0:00:00.352851
elapsed time: 0:00:10.436960
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-10-01 14:36:53.590592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2219.63
 ---- batch: 020 ----
mean loss: 2159.22
train mean loss: 2179.02
epoch train time: 0:00:00.360064
elapsed time: 0:00:10.797204
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-10-01 14:36:53.950837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2140.98
 ---- batch: 020 ----
mean loss: 2087.11
train mean loss: 2102.89
epoch train time: 0:00:00.351093
elapsed time: 0:00:11.148490
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-10-01 14:36:54.302143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2025.88
 ---- batch: 020 ----
mean loss: 2036.47
train mean loss: 2034.99
epoch train time: 0:00:00.361185
elapsed time: 0:00:11.509875
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-10-01 14:36:54.663514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1995.68
 ---- batch: 020 ----
mean loss: 1927.16
train mean loss: 1966.46
epoch train time: 0:00:00.361536
elapsed time: 0:00:11.871616
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-10-01 14:36:55.025260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1908.65
 ---- batch: 020 ----
mean loss: 1907.76
train mean loss: 1904.97
epoch train time: 0:00:00.344988
elapsed time: 0:00:12.216834
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-10-01 14:36:55.370470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1848.29
 ---- batch: 020 ----
mean loss: 1824.51
train mean loss: 1836.92
epoch train time: 0:00:00.339788
elapsed time: 0:00:12.556833
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-10-01 14:36:55.710475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1798.04
 ---- batch: 020 ----
mean loss: 1756.88
train mean loss: 1786.40
epoch train time: 0:00:00.359009
elapsed time: 0:00:12.916042
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-10-01 14:36:56.069680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1756.92
 ---- batch: 020 ----
mean loss: 1696.59
train mean loss: 1721.87
epoch train time: 0:00:00.354164
elapsed time: 0:00:13.270386
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-10-01 14:36:56.424016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1691.22
 ---- batch: 020 ----
mean loss: 1646.80
train mean loss: 1666.65
epoch train time: 0:00:00.357205
elapsed time: 0:00:13.627779
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-10-01 14:36:56.781424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1633.52
 ---- batch: 020 ----
mean loss: 1608.75
train mean loss: 1623.71
epoch train time: 0:00:00.345217
elapsed time: 0:00:13.973182
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-10-01 14:36:57.126822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1564.77
 ---- batch: 020 ----
mean loss: 1561.90
train mean loss: 1562.52
epoch train time: 0:00:00.343581
elapsed time: 0:00:14.316948
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-10-01 14:36:57.470598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1518.13
 ---- batch: 020 ----
mean loss: 1541.75
train mean loss: 1523.01
epoch train time: 0:00:00.351756
elapsed time: 0:00:14.668952
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-10-01 14:36:57.822595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1488.10
 ---- batch: 020 ----
mean loss: 1455.55
train mean loss: 1464.62
epoch train time: 0:00:00.348925
elapsed time: 0:00:15.018061
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-10-01 14:36:58.171713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1431.30
 ---- batch: 020 ----
mean loss: 1407.28
train mean loss: 1414.58
epoch train time: 0:00:00.346822
elapsed time: 0:00:15.365087
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-10-01 14:36:58.518728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1381.06
 ---- batch: 020 ----
mean loss: 1346.15
train mean loss: 1350.09
epoch train time: 0:00:00.349065
elapsed time: 0:00:15.714356
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-10-01 14:36:58.868033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1310.49
 ---- batch: 020 ----
mean loss: 1297.52
train mean loss: 1296.31
epoch train time: 0:00:00.344115
elapsed time: 0:00:16.058690
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-10-01 14:36:59.212320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1246.24
 ---- batch: 020 ----
mean loss: 1235.89
train mean loss: 1249.12
epoch train time: 0:00:00.336125
elapsed time: 0:00:16.395121
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-10-01 14:36:59.548793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1215.53
 ---- batch: 020 ----
mean loss: 1211.10
train mean loss: 1204.07
epoch train time: 0:00:00.351509
elapsed time: 0:00:16.746842
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-10-01 14:36:59.900486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1178.76
 ---- batch: 020 ----
mean loss: 1151.94
train mean loss: 1154.46
epoch train time: 0:00:00.345191
elapsed time: 0:00:17.092241
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-10-01 14:37:00.245889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1145.52
 ---- batch: 020 ----
mean loss: 1109.17
train mean loss: 1125.43
epoch train time: 0:00:00.343339
elapsed time: 0:00:17.435785
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-10-01 14:37:00.589419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1104.17
 ---- batch: 020 ----
mean loss: 1079.74
train mean loss: 1090.00
epoch train time: 0:00:00.345956
elapsed time: 0:00:17.781925
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-10-01 14:37:00.935558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1065.59
 ---- batch: 020 ----
mean loss: 1039.23
train mean loss: 1055.19
epoch train time: 0:00:00.349949
elapsed time: 0:00:18.132076
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-10-01 14:37:01.285729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1009.66
 ---- batch: 020 ----
mean loss: 1014.06
train mean loss: 1013.69
epoch train time: 0:00:00.353357
elapsed time: 0:00:18.485650
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-10-01 14:37:01.639289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 981.94
 ---- batch: 020 ----
mean loss: 968.34
train mean loss: 973.14
epoch train time: 0:00:00.361678
elapsed time: 0:00:18.847549
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-10-01 14:37:02.001183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.75
 ---- batch: 020 ----
mean loss: 938.62
train mean loss: 938.49
epoch train time: 0:00:00.345447
elapsed time: 0:00:19.193172
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-10-01 14:37:02.346820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.11
 ---- batch: 020 ----
mean loss: 908.97
train mean loss: 908.95
epoch train time: 0:00:00.336042
elapsed time: 0:00:19.529405
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-10-01 14:37:02.683039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.87
 ---- batch: 020 ----
mean loss: 877.59
train mean loss: 879.93
epoch train time: 0:00:00.346971
elapsed time: 0:00:19.876653
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-10-01 14:37:03.030287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.45
 ---- batch: 020 ----
mean loss: 849.27
train mean loss: 854.60
epoch train time: 0:00:00.339339
elapsed time: 0:00:20.216164
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-10-01 14:37:03.369828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.11
 ---- batch: 020 ----
mean loss: 808.95
train mean loss: 817.68
epoch train time: 0:00:00.353085
elapsed time: 0:00:20.569487
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-10-01 14:37:03.723125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.89
 ---- batch: 020 ----
mean loss: 779.35
train mean loss: 789.85
epoch train time: 0:00:00.347629
elapsed time: 0:00:20.917296
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-10-01 14:37:04.070930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.05
 ---- batch: 020 ----
mean loss: 750.97
train mean loss: 764.30
epoch train time: 0:00:00.346115
elapsed time: 0:00:21.263586
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-10-01 14:37:04.417221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.58
 ---- batch: 020 ----
mean loss: 728.97
train mean loss: 738.31
epoch train time: 0:00:00.344836
elapsed time: 0:00:21.608607
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-10-01 14:37:04.762245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 729.01
 ---- batch: 020 ----
mean loss: 701.35
train mean loss: 715.08
epoch train time: 0:00:00.343607
elapsed time: 0:00:21.952414
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-10-01 14:37:05.106061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 688.22
 ---- batch: 020 ----
mean loss: 686.64
train mean loss: 685.43
epoch train time: 0:00:00.341195
elapsed time: 0:00:22.293803
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-10-01 14:37:05.447455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 679.99
 ---- batch: 020 ----
mean loss: 652.66
train mean loss: 663.41
epoch train time: 0:00:00.343139
elapsed time: 0:00:22.637246
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-10-01 14:37:05.790929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.01
 ---- batch: 020 ----
mean loss: 633.59
train mean loss: 641.38
epoch train time: 0:00:00.347815
elapsed time: 0:00:22.985294
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-10-01 14:37:06.138928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.62
 ---- batch: 020 ----
mean loss: 610.77
train mean loss: 621.26
epoch train time: 0:00:00.333811
elapsed time: 0:00:23.319274
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-10-01 14:37:06.472903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.47
 ---- batch: 020 ----
mean loss: 581.51
train mean loss: 594.27
epoch train time: 0:00:00.336239
elapsed time: 0:00:23.655709
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-10-01 14:37:06.809358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 586.33
 ---- batch: 020 ----
mean loss: 572.11
train mean loss: 575.66
epoch train time: 0:00:00.349262
elapsed time: 0:00:24.005171
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-10-01 14:37:07.158809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 565.29
 ---- batch: 020 ----
mean loss: 548.86
train mean loss: 555.75
epoch train time: 0:00:00.348175
elapsed time: 0:00:24.353551
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-10-01 14:37:07.507205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 531.44
 ---- batch: 020 ----
mean loss: 543.04
train mean loss: 539.47
epoch train time: 0:00:00.344947
elapsed time: 0:00:24.698699
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-10-01 14:37:07.852336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.09
 ---- batch: 020 ----
mean loss: 513.43
train mean loss: 513.61
epoch train time: 0:00:00.347873
elapsed time: 0:00:25.046755
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-10-01 14:37:08.200390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.39
 ---- batch: 020 ----
mean loss: 495.30
train mean loss: 505.33
epoch train time: 0:00:00.352975
elapsed time: 0:00:25.399911
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-10-01 14:37:08.553547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.36
 ---- batch: 020 ----
mean loss: 478.13
train mean loss: 488.72
epoch train time: 0:00:00.357885
elapsed time: 0:00:25.758021
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-10-01 14:37:08.911678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.52
 ---- batch: 020 ----
mean loss: 463.14
train mean loss: 467.33
epoch train time: 0:00:00.342368
elapsed time: 0:00:26.100589
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-10-01 14:37:09.254223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.89
 ---- batch: 020 ----
mean loss: 453.04
train mean loss: 452.15
epoch train time: 0:00:00.343670
elapsed time: 0:00:26.444454
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-10-01 14:37:09.598090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.32
 ---- batch: 020 ----
mean loss: 438.73
train mean loss: 438.46
epoch train time: 0:00:00.353971
elapsed time: 0:00:26.798607
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-10-01 14:37:09.952253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.98
 ---- batch: 020 ----
mean loss: 417.49
train mean loss: 424.55
epoch train time: 0:00:00.342729
elapsed time: 0:00:27.141538
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-10-01 14:37:10.295188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.09
 ---- batch: 020 ----
mean loss: 411.61
train mean loss: 411.73
epoch train time: 0:00:00.342084
elapsed time: 0:00:27.483811
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-10-01 14:37:10.637469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.49
 ---- batch: 020 ----
mean loss: 394.68
train mean loss: 396.95
epoch train time: 0:00:00.354625
elapsed time: 0:00:27.838638
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-10-01 14:37:10.992283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.80
 ---- batch: 020 ----
mean loss: 385.13
train mean loss: 386.53
epoch train time: 0:00:00.342770
elapsed time: 0:00:28.181700
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-10-01 14:37:11.335388
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.53
 ---- batch: 020 ----
mean loss: 370.86
train mean loss: 374.48
epoch train time: 0:00:00.349050
elapsed time: 0:00:28.530980
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-10-01 14:37:11.684617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.89
 ---- batch: 020 ----
mean loss: 355.65
train mean loss: 358.28
epoch train time: 0:00:00.350889
elapsed time: 0:00:28.882096
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-10-01 14:37:12.035768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.59
 ---- batch: 020 ----
mean loss: 353.31
train mean loss: 352.45
epoch train time: 0:00:00.333588
elapsed time: 0:00:29.215987
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-10-01 14:37:12.369621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.46
 ---- batch: 020 ----
mean loss: 327.97
train mean loss: 335.55
epoch train time: 0:00:00.337901
elapsed time: 0:00:29.554091
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-10-01 14:37:12.707725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.76
 ---- batch: 020 ----
mean loss: 326.92
train mean loss: 327.27
epoch train time: 0:00:00.345682
elapsed time: 0:00:29.899977
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-10-01 14:37:13.053615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.35
 ---- batch: 020 ----
mean loss: 317.82
train mean loss: 319.45
epoch train time: 0:00:00.347982
elapsed time: 0:00:30.248142
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-10-01 14:37:13.401799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.94
 ---- batch: 020 ----
mean loss: 312.77
train mean loss: 310.71
epoch train time: 0:00:00.353284
elapsed time: 0:00:30.601639
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-10-01 14:37:13.755346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.16
 ---- batch: 020 ----
mean loss: 294.83
train mean loss: 299.97
epoch train time: 0:00:00.337796
elapsed time: 0:00:30.939772
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-10-01 14:37:14.093415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.35
 ---- batch: 020 ----
mean loss: 283.93
train mean loss: 289.98
epoch train time: 0:00:00.337667
elapsed time: 0:00:31.277618
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-10-01 14:37:14.431250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.40
 ---- batch: 020 ----
mean loss: 281.95
train mean loss: 281.83
epoch train time: 0:00:00.360762
elapsed time: 0:00:31.638558
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-10-01 14:37:14.792195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.31
 ---- batch: 020 ----
mean loss: 272.61
train mean loss: 272.89
epoch train time: 0:00:00.370987
elapsed time: 0:00:32.009730
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-10-01 14:37:15.163366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.54
 ---- batch: 020 ----
mean loss: 266.79
train mean loss: 266.72
epoch train time: 0:00:00.336353
elapsed time: 0:00:32.346252
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-10-01 14:37:15.499881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.99
 ---- batch: 020 ----
mean loss: 259.72
train mean loss: 259.03
epoch train time: 0:00:00.357846
elapsed time: 0:00:32.704347
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-10-01 14:37:15.858013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.93
 ---- batch: 020 ----
mean loss: 250.42
train mean loss: 251.67
epoch train time: 0:00:00.348113
elapsed time: 0:00:33.052687
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-10-01 14:37:16.206327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.82
 ---- batch: 020 ----
mean loss: 242.85
train mean loss: 242.81
epoch train time: 0:00:00.353212
elapsed time: 0:00:33.406128
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-10-01 14:37:16.559767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.31
 ---- batch: 020 ----
mean loss: 237.29
train mean loss: 241.11
epoch train time: 0:00:00.359849
elapsed time: 0:00:33.766161
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-10-01 14:37:16.919808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.68
 ---- batch: 020 ----
mean loss: 234.80
train mean loss: 232.97
epoch train time: 0:00:00.352460
elapsed time: 0:00:34.118832
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-10-01 14:37:17.272467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.67
 ---- batch: 020 ----
mean loss: 224.80
train mean loss: 227.13
epoch train time: 0:00:00.354134
elapsed time: 0:00:34.473214
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-10-01 14:37:17.626880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.71
 ---- batch: 020 ----
mean loss: 221.75
train mean loss: 221.26
epoch train time: 0:00:00.367620
elapsed time: 0:00:34.841044
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-10-01 14:37:17.994678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.36
 ---- batch: 020 ----
mean loss: 214.48
train mean loss: 212.29
epoch train time: 0:00:00.348121
elapsed time: 0:00:35.189405
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-10-01 14:37:18.343043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.51
 ---- batch: 020 ----
mean loss: 212.11
train mean loss: 211.51
epoch train time: 0:00:00.348249
elapsed time: 0:00:35.537866
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-10-01 14:37:18.691501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.11
 ---- batch: 020 ----
mean loss: 204.99
train mean loss: 202.96
epoch train time: 0:00:00.346178
elapsed time: 0:00:35.884249
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-10-01 14:37:19.037886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.42
 ---- batch: 020 ----
mean loss: 195.66
train mean loss: 197.66
epoch train time: 0:00:00.340180
elapsed time: 0:00:36.224619
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-10-01 14:37:19.378265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.87
 ---- batch: 020 ----
mean loss: 196.80
train mean loss: 195.50
epoch train time: 0:00:00.339117
elapsed time: 0:00:36.563954
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-10-01 14:37:19.717620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.27
 ---- batch: 020 ----
mean loss: 187.16
train mean loss: 189.95
epoch train time: 0:00:00.347302
elapsed time: 0:00:36.911479
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-10-01 14:37:20.065118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.59
 ---- batch: 020 ----
mean loss: 186.46
train mean loss: 186.14
epoch train time: 0:00:00.343042
elapsed time: 0:00:37.254723
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-10-01 14:37:20.408363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.08
 ---- batch: 020 ----
mean loss: 183.27
train mean loss: 179.48
epoch train time: 0:00:00.359930
elapsed time: 0:00:37.614855
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-10-01 14:37:20.768500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.39
 ---- batch: 020 ----
mean loss: 178.33
train mean loss: 177.07
epoch train time: 0:00:00.362251
elapsed time: 0:00:37.977301
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-10-01 14:37:21.130940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.61
 ---- batch: 020 ----
mean loss: 170.10
train mean loss: 172.70
epoch train time: 0:00:00.350502
elapsed time: 0:00:38.327987
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-10-01 14:37:21.481640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.29
 ---- batch: 020 ----
mean loss: 168.72
train mean loss: 171.23
epoch train time: 0:00:00.345893
elapsed time: 0:00:38.674078
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-10-01 14:37:21.827711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.73
 ---- batch: 020 ----
mean loss: 165.22
train mean loss: 166.97
epoch train time: 0:00:00.340965
elapsed time: 0:00:39.015222
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-10-01 14:37:22.168855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.75
 ---- batch: 020 ----
mean loss: 163.84
train mean loss: 163.20
epoch train time: 0:00:00.342398
elapsed time: 0:00:39.357892
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-10-01 14:37:22.511555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.53
 ---- batch: 020 ----
mean loss: 160.44
train mean loss: 159.74
epoch train time: 0:00:00.344436
elapsed time: 0:00:39.702552
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-10-01 14:37:22.856187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.76
 ---- batch: 020 ----
mean loss: 154.05
train mean loss: 154.95
epoch train time: 0:00:00.334717
elapsed time: 0:00:40.037434
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-10-01 14:37:23.191064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.50
 ---- batch: 020 ----
mean loss: 150.43
train mean loss: 154.86
epoch train time: 0:00:00.333900
elapsed time: 0:00:40.371501
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-10-01 14:37:23.525133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.53
 ---- batch: 020 ----
mean loss: 146.19
train mean loss: 149.04
epoch train time: 0:00:00.339553
elapsed time: 0:00:40.711230
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-10-01 14:37:23.864865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.34
 ---- batch: 020 ----
mean loss: 145.25
train mean loss: 147.33
epoch train time: 0:00:00.335854
elapsed time: 0:00:41.047257
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-10-01 14:37:24.200907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.24
 ---- batch: 020 ----
mean loss: 140.51
train mean loss: 144.59
epoch train time: 0:00:00.336777
elapsed time: 0:00:41.384246
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-10-01 14:37:24.537892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.09
 ---- batch: 020 ----
mean loss: 147.01
train mean loss: 143.44
epoch train time: 0:00:00.339708
elapsed time: 0:00:41.724206
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-10-01 14:37:24.877846
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.67
 ---- batch: 020 ----
mean loss: 135.91
train mean loss: 140.18
epoch train time: 0:00:00.339755
elapsed time: 0:00:42.064135
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-10-01 14:37:25.217783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.80
 ---- batch: 020 ----
mean loss: 136.52
train mean loss: 138.14
epoch train time: 0:00:00.339365
elapsed time: 0:00:42.403734
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-10-01 14:37:25.557367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.66
 ---- batch: 020 ----
mean loss: 136.24
train mean loss: 135.59
epoch train time: 0:00:00.356041
elapsed time: 0:00:42.759973
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-10-01 14:37:25.913608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.52
 ---- batch: 020 ----
mean loss: 132.86
train mean loss: 133.83
epoch train time: 0:00:00.340648
elapsed time: 0:00:43.100800
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-10-01 14:37:26.254436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.18
 ---- batch: 020 ----
mean loss: 131.73
train mean loss: 132.81
epoch train time: 0:00:00.335485
elapsed time: 0:00:43.436514
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-10-01 14:37:26.590168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.79
 ---- batch: 020 ----
mean loss: 129.35
train mean loss: 129.65
epoch train time: 0:00:00.360257
elapsed time: 0:00:43.797025
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-10-01 14:37:26.950695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.72
 ---- batch: 020 ----
mean loss: 128.26
train mean loss: 125.85
epoch train time: 0:00:00.345802
elapsed time: 0:00:44.143035
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-10-01 14:37:27.296671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.10
 ---- batch: 020 ----
mean loss: 126.12
train mean loss: 126.56
epoch train time: 0:00:00.343467
elapsed time: 0:00:44.486694
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-10-01 14:37:27.640340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.22
 ---- batch: 020 ----
mean loss: 127.61
train mean loss: 124.77
epoch train time: 0:00:00.361264
elapsed time: 0:00:44.848239
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-10-01 14:37:28.001853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.66
 ---- batch: 020 ----
mean loss: 123.32
train mean loss: 121.71
epoch train time: 0:00:00.346019
elapsed time: 0:00:45.194412
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-10-01 14:37:28.348047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.35
 ---- batch: 020 ----
mean loss: 118.78
train mean loss: 120.77
epoch train time: 0:00:00.340535
elapsed time: 0:00:45.535197
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-10-01 14:37:28.688827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.92
 ---- batch: 020 ----
mean loss: 121.76
train mean loss: 121.29
epoch train time: 0:00:00.355010
elapsed time: 0:00:45.890380
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-10-01 14:37:29.044016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.00
 ---- batch: 020 ----
mean loss: 116.52
train mean loss: 116.08
epoch train time: 0:00:00.346239
elapsed time: 0:00:46.236801
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-10-01 14:37:29.390445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.10
 ---- batch: 020 ----
mean loss: 110.10
train mean loss: 114.31
epoch train time: 0:00:00.347546
elapsed time: 0:00:46.584537
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-10-01 14:37:29.738173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.30
 ---- batch: 020 ----
mean loss: 113.45
train mean loss: 113.55
epoch train time: 0:00:00.344219
elapsed time: 0:00:46.928939
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-10-01 14:37:30.082575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.94
 ---- batch: 020 ----
mean loss: 115.03
train mean loss: 111.64
epoch train time: 0:00:00.342515
elapsed time: 0:00:47.271628
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-10-01 14:37:30.425272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.06
 ---- batch: 020 ----
mean loss: 109.56
train mean loss: 111.58
epoch train time: 0:00:00.348301
elapsed time: 0:00:47.620133
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-10-01 14:37:30.773790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.17
 ---- batch: 020 ----
mean loss: 112.72
train mean loss: 111.08
epoch train time: 0:00:00.349971
elapsed time: 0:00:47.970319
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-10-01 14:37:31.123951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.72
 ---- batch: 020 ----
mean loss: 104.16
train mean loss: 109.43
epoch train time: 0:00:00.347828
elapsed time: 0:00:48.318336
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-10-01 14:37:31.472009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.44
 ---- batch: 020 ----
mean loss: 107.88
train mean loss: 107.78
epoch train time: 0:00:00.351119
elapsed time: 0:00:48.669689
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-10-01 14:37:31.823327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.56
 ---- batch: 020 ----
mean loss: 107.83
train mean loss: 105.38
epoch train time: 0:00:00.355590
elapsed time: 0:00:49.025470
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-10-01 14:37:32.179124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.40
 ---- batch: 020 ----
mean loss: 107.21
train mean loss: 107.41
epoch train time: 0:00:00.347469
elapsed time: 0:00:49.373136
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-10-01 14:37:32.526782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.58
 ---- batch: 020 ----
mean loss: 104.13
train mean loss: 103.35
epoch train time: 0:00:00.367786
elapsed time: 0:00:49.741127
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-10-01 14:37:32.894761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.32
 ---- batch: 020 ----
mean loss: 103.85
train mean loss: 102.29
epoch train time: 0:00:00.348882
elapsed time: 0:00:50.090203
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-10-01 14:37:33.243839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.99
 ---- batch: 020 ----
mean loss: 100.37
train mean loss: 100.77
epoch train time: 0:00:00.343631
elapsed time: 0:00:50.434029
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-10-01 14:37:33.587665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.14
 ---- batch: 020 ----
mean loss: 96.75
train mean loss: 100.42
epoch train time: 0:00:00.347742
elapsed time: 0:00:50.781953
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-10-01 14:37:33.935589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 100.11
 ---- batch: 020 ----
mean loss: 101.43
train mean loss: 100.81
epoch train time: 0:00:00.357653
elapsed time: 0:00:51.139786
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-10-01 14:37:34.293421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.15
 ---- batch: 020 ----
mean loss: 97.15
train mean loss: 96.77
epoch train time: 0:00:00.351894
elapsed time: 0:00:51.491881
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-10-01 14:37:34.645516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.85
 ---- batch: 020 ----
mean loss: 96.23
train mean loss: 97.76
epoch train time: 0:00:00.372687
elapsed time: 0:00:51.864784
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-10-01 14:37:35.018402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.82
 ---- batch: 020 ----
mean loss: 95.31
train mean loss: 95.68
epoch train time: 0:00:00.347451
elapsed time: 0:00:52.212412
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-10-01 14:37:35.366047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.52
 ---- batch: 020 ----
mean loss: 96.31
train mean loss: 96.28
epoch train time: 0:00:00.344994
elapsed time: 0:00:52.557582
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-10-01 14:37:35.711229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.01
 ---- batch: 020 ----
mean loss: 92.95
train mean loss: 93.65
epoch train time: 0:00:00.344733
elapsed time: 0:00:52.902537
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-10-01 14:37:36.056185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.65
 ---- batch: 020 ----
mean loss: 94.53
train mean loss: 93.09
epoch train time: 0:00:00.352407
elapsed time: 0:00:53.255149
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-10-01 14:37:36.408798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.65
 ---- batch: 020 ----
mean loss: 93.20
train mean loss: 92.10
epoch train time: 0:00:00.344990
elapsed time: 0:00:53.600361
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-10-01 14:37:36.754003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.30
 ---- batch: 020 ----
mean loss: 96.58
train mean loss: 92.99
epoch train time: 0:00:00.348274
elapsed time: 0:00:53.948835
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-10-01 14:37:37.102468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.61
 ---- batch: 020 ----
mean loss: 91.38
train mean loss: 90.88
epoch train time: 0:00:00.349192
elapsed time: 0:00:54.298219
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-10-01 14:37:37.451909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.41
 ---- batch: 020 ----
mean loss: 92.58
train mean loss: 91.76
epoch train time: 0:00:00.346361
elapsed time: 0:00:54.644821
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-10-01 14:37:37.798479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.36
 ---- batch: 020 ----
mean loss: 94.44
train mean loss: 89.65
epoch train time: 0:00:00.346897
elapsed time: 0:00:54.991917
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-10-01 14:37:38.145550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.24
 ---- batch: 020 ----
mean loss: 90.51
train mean loss: 90.40
epoch train time: 0:00:00.339892
elapsed time: 0:00:55.332078
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-10-01 14:37:38.485724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.35
 ---- batch: 020 ----
mean loss: 86.45
train mean loss: 88.47
epoch train time: 0:00:00.350010
elapsed time: 0:00:55.682323
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-10-01 14:37:38.835991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.94
 ---- batch: 020 ----
mean loss: 86.47
train mean loss: 87.70
epoch train time: 0:00:00.338140
elapsed time: 0:00:56.020675
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-10-01 14:37:39.174319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.26
 ---- batch: 020 ----
mean loss: 84.10
train mean loss: 83.75
epoch train time: 0:00:00.341004
elapsed time: 0:00:56.361867
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-10-01 14:37:39.515517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.45
 ---- batch: 020 ----
mean loss: 86.89
train mean loss: 85.63
epoch train time: 0:00:00.347817
elapsed time: 0:00:56.709891
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-10-01 14:37:39.863528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.36
 ---- batch: 020 ----
mean loss: 81.89
train mean loss: 85.68
epoch train time: 0:00:00.343988
elapsed time: 0:00:57.054078
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-10-01 14:37:40.207710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.89
 ---- batch: 020 ----
mean loss: 84.09
train mean loss: 86.15
epoch train time: 0:00:00.342808
elapsed time: 0:00:57.397073
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-10-01 14:37:40.550719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.07
 ---- batch: 020 ----
mean loss: 81.70
train mean loss: 85.40
epoch train time: 0:00:00.366622
elapsed time: 0:00:57.763892
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-10-01 14:37:40.917526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.19
 ---- batch: 020 ----
mean loss: 87.42
train mean loss: 87.04
epoch train time: 0:00:00.344278
elapsed time: 0:00:58.108361
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-10-01 14:37:41.262002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.63
 ---- batch: 020 ----
mean loss: 83.37
train mean loss: 84.66
epoch train time: 0:00:00.346014
elapsed time: 0:00:58.454577
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-10-01 14:37:41.608213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 86.02
 ---- batch: 020 ----
mean loss: 81.06
train mean loss: 82.01
epoch train time: 0:00:00.348415
elapsed time: 0:00:58.803174
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-10-01 14:37:41.956811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.55
 ---- batch: 020 ----
mean loss: 78.11
train mean loss: 81.37
epoch train time: 0:00:00.348735
elapsed time: 0:00:59.152089
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-10-01 14:37:42.305777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.34
 ---- batch: 020 ----
mean loss: 81.11
train mean loss: 82.03
epoch train time: 0:00:00.346110
elapsed time: 0:00:59.498482
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-10-01 14:37:42.652095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.18
 ---- batch: 020 ----
mean loss: 78.52
train mean loss: 80.55
epoch train time: 0:00:00.350772
elapsed time: 0:00:59.849407
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-10-01 14:37:43.003041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.71
 ---- batch: 020 ----
mean loss: 80.51
train mean loss: 81.16
epoch train time: 0:00:00.344240
elapsed time: 0:01:00.193838
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-10-01 14:37:43.347488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.90
 ---- batch: 020 ----
mean loss: 80.86
train mean loss: 80.79
epoch train time: 0:00:00.344809
elapsed time: 0:01:00.538843
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-10-01 14:37:43.692478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.49
 ---- batch: 020 ----
mean loss: 80.95
train mean loss: 79.93
epoch train time: 0:00:00.347250
elapsed time: 0:01:00.886290
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-10-01 14:37:44.039922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 81.93
 ---- batch: 020 ----
mean loss: 78.61
train mean loss: 80.25
epoch train time: 0:00:00.342362
elapsed time: 0:01:01.228833
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-10-01 14:37:44.382547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.76
 ---- batch: 020 ----
mean loss: 78.72
train mean loss: 79.50
epoch train time: 0:00:00.344113
elapsed time: 0:01:01.573198
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-10-01 14:37:44.726826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.16
 ---- batch: 020 ----
mean loss: 79.99
train mean loss: 78.09
epoch train time: 0:00:00.353961
elapsed time: 0:01:01.927322
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-10-01 14:37:45.080959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.07
 ---- batch: 020 ----
mean loss: 79.96
train mean loss: 77.62
epoch train time: 0:00:00.344163
elapsed time: 0:01:02.271660
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-10-01 14:37:45.425302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.41
 ---- batch: 020 ----
mean loss: 75.87
train mean loss: 76.78
epoch train time: 0:00:00.349195
elapsed time: 0:01:02.621050
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-10-01 14:37:45.774716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.89
 ---- batch: 020 ----
mean loss: 75.90
train mean loss: 76.58
epoch train time: 0:00:00.350450
elapsed time: 0:01:02.971706
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-10-01 14:37:46.125339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.69
 ---- batch: 020 ----
mean loss: 77.48
train mean loss: 77.04
epoch train time: 0:00:00.353433
elapsed time: 0:01:03.325316
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-10-01 14:37:46.478964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.47
 ---- batch: 020 ----
mean loss: 76.27
train mean loss: 76.95
epoch train time: 0:00:00.361322
elapsed time: 0:01:03.686833
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-10-01 14:37:46.840470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.27
 ---- batch: 020 ----
mean loss: 76.89
train mean loss: 75.84
epoch train time: 0:00:00.345383
elapsed time: 0:01:04.032406
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-10-01 14:37:47.186042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.55
 ---- batch: 020 ----
mean loss: 77.68
train mean loss: 74.56
epoch train time: 0:00:00.344143
elapsed time: 0:01:04.376760
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-10-01 14:37:47.530394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.57
 ---- batch: 020 ----
mean loss: 71.95
train mean loss: 73.08
epoch train time: 0:00:00.344155
elapsed time: 0:01:04.721091
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-10-01 14:37:47.874726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.01
 ---- batch: 020 ----
mean loss: 75.34
train mean loss: 76.22
epoch train time: 0:00:00.344577
elapsed time: 0:01:05.065935
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-10-01 14:37:48.219570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.66
 ---- batch: 020 ----
mean loss: 73.93
train mean loss: 74.09
epoch train time: 0:00:00.353214
elapsed time: 0:01:05.419340
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-10-01 14:37:48.573012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.22
 ---- batch: 020 ----
mean loss: 69.22
train mean loss: 73.13
epoch train time: 0:00:00.359632
elapsed time: 0:01:05.779192
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-10-01 14:37:48.932828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.12
 ---- batch: 020 ----
mean loss: 73.49
train mean loss: 73.47
epoch train time: 0:00:00.348795
elapsed time: 0:01:06.128163
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-10-01 14:37:49.281815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.61
 ---- batch: 020 ----
mean loss: 72.13
train mean loss: 72.36
epoch train time: 0:00:00.349019
elapsed time: 0:01:06.477376
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-10-01 14:37:49.631025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.18
 ---- batch: 020 ----
mean loss: 71.30
train mean loss: 72.32
epoch train time: 0:00:00.356189
elapsed time: 0:01:06.833827
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-10-01 14:37:49.987474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.93
 ---- batch: 020 ----
mean loss: 73.75
train mean loss: 73.98
epoch train time: 0:00:00.340810
elapsed time: 0:01:07.174861
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-10-01 14:37:50.328500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.31
 ---- batch: 020 ----
mean loss: 71.18
train mean loss: 71.30
epoch train time: 0:00:00.344011
elapsed time: 0:01:07.519054
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-10-01 14:37:50.672689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.63
 ---- batch: 020 ----
mean loss: 71.15
train mean loss: 70.37
epoch train time: 0:00:00.357218
elapsed time: 0:01:07.876486
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-10-01 14:37:51.030121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.84
 ---- batch: 020 ----
mean loss: 69.99
train mean loss: 70.52
epoch train time: 0:00:00.344927
elapsed time: 0:01:08.221617
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-10-01 14:37:51.375229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.24
 ---- batch: 020 ----
mean loss: 69.32
train mean loss: 70.04
epoch train time: 0:00:00.347882
elapsed time: 0:01:08.569659
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-10-01 14:37:51.723294
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.62
 ---- batch: 020 ----
mean loss: 72.92
train mean loss: 71.51
epoch train time: 0:00:00.363109
elapsed time: 0:01:08.932959
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-10-01 14:37:52.086598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.07
 ---- batch: 020 ----
mean loss: 67.90
train mean loss: 69.98
epoch train time: 0:00:00.355945
elapsed time: 0:01:09.289147
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-10-01 14:37:52.442818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.41
 ---- batch: 020 ----
mean loss: 70.13
train mean loss: 68.97
epoch train time: 0:00:00.381222
elapsed time: 0:01:09.670639
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-10-01 14:37:52.824290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.87
 ---- batch: 020 ----
mean loss: 70.57
train mean loss: 69.04
epoch train time: 0:00:00.342319
elapsed time: 0:01:10.013148
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-10-01 14:37:53.166816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.81
 ---- batch: 020 ----
mean loss: 68.93
train mean loss: 69.81
epoch train time: 0:00:00.338017
elapsed time: 0:01:10.351395
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-10-01 14:37:53.505068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.27
 ---- batch: 020 ----
mean loss: 68.01
train mean loss: 66.99
epoch train time: 0:00:00.343459
elapsed time: 0:01:10.695081
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-10-01 14:37:53.848718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.63
 ---- batch: 020 ----
mean loss: 67.58
train mean loss: 67.06
epoch train time: 0:00:00.340678
elapsed time: 0:01:11.035998
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-10-01 14:37:54.189665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.25
 ---- batch: 020 ----
mean loss: 69.36
train mean loss: 69.00
epoch train time: 0:00:00.339836
elapsed time: 0:01:11.376038
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-10-01 14:37:54.529669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.90
 ---- batch: 020 ----
mean loss: 66.78
train mean loss: 67.94
epoch train time: 0:00:00.356631
elapsed time: 0:01:11.732863
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-10-01 14:37:54.886508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.68
 ---- batch: 020 ----
mean loss: 65.69
train mean loss: 66.92
epoch train time: 0:00:00.342600
elapsed time: 0:01:12.075662
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-10-01 14:37:55.229307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.33
 ---- batch: 020 ----
mean loss: 68.35
train mean loss: 67.73
epoch train time: 0:00:00.343672
elapsed time: 0:01:12.419517
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-10-01 14:37:55.573182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.41
 ---- batch: 020 ----
mean loss: 67.81
train mean loss: 66.06
epoch train time: 0:00:00.349881
elapsed time: 0:01:12.769617
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-10-01 14:37:55.923278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.57
 ---- batch: 020 ----
mean loss: 64.83
train mean loss: 65.25
epoch train time: 0:00:00.344625
elapsed time: 0:01:13.114442
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-10-01 14:37:56.268076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.78
 ---- batch: 020 ----
mean loss: 64.78
train mean loss: 66.52
epoch train time: 0:00:00.342051
elapsed time: 0:01:13.456712
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-10-01 14:37:56.610347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.10
 ---- batch: 020 ----
mean loss: 67.18
train mean loss: 66.14
epoch train time: 0:00:00.344608
elapsed time: 0:01:13.801522
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-10-01 14:37:56.955172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.74
 ---- batch: 020 ----
mean loss: 65.90
train mean loss: 65.43
epoch train time: 0:00:00.345905
elapsed time: 0:01:14.147690
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-10-01 14:37:57.301401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.61
 ---- batch: 020 ----
mean loss: 64.86
train mean loss: 65.49
epoch train time: 0:00:00.342227
elapsed time: 0:01:14.490205
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-10-01 14:37:57.643843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.32
 ---- batch: 020 ----
mean loss: 61.95
train mean loss: 65.77
epoch train time: 0:00:00.358848
elapsed time: 0:01:14.849275
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-10-01 14:37:58.002915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.71
 ---- batch: 020 ----
mean loss: 66.67
train mean loss: 65.87
epoch train time: 0:00:00.345717
elapsed time: 0:01:15.195180
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-10-01 14:37:58.348816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.65
 ---- batch: 020 ----
mean loss: 64.91
train mean loss: 64.70
epoch train time: 0:00:00.351835
elapsed time: 0:01:15.547238
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-10-01 14:37:58.700871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.50
 ---- batch: 020 ----
mean loss: 65.05
train mean loss: 64.78
epoch train time: 0:00:00.358701
elapsed time: 0:01:15.906131
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-10-01 14:37:59.059803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.68
 ---- batch: 020 ----
mean loss: 63.80
train mean loss: 62.95
epoch train time: 0:00:00.342172
elapsed time: 0:01:16.248522
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-10-01 14:37:59.402177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.70
 ---- batch: 020 ----
mean loss: 63.25
train mean loss: 63.16
epoch train time: 0:00:00.349145
elapsed time: 0:01:16.597880
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-10-01 14:37:59.751529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.05
 ---- batch: 020 ----
mean loss: 63.92
train mean loss: 64.08
epoch train time: 0:00:00.346055
elapsed time: 0:01:16.944162
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-10-01 14:38:00.097821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.97
 ---- batch: 020 ----
mean loss: 65.30
train mean loss: 63.30
epoch train time: 0:00:00.342788
elapsed time: 0:01:17.287147
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-10-01 14:38:00.440779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.14
 ---- batch: 020 ----
mean loss: 63.58
train mean loss: 62.10
epoch train time: 0:00:00.345759
elapsed time: 0:01:17.633082
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-10-01 14:38:00.786720
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.05
 ---- batch: 020 ----
mean loss: 61.04
train mean loss: 61.70
epoch train time: 0:00:00.339934
elapsed time: 0:01:17.973229
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-10-01 14:38:01.126841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.89
 ---- batch: 020 ----
mean loss: 65.52
train mean loss: 61.92
epoch train time: 0:00:00.335796
elapsed time: 0:01:18.309199
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-10-01 14:38:01.462839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.98
 ---- batch: 020 ----
mean loss: 62.35
train mean loss: 61.10
epoch train time: 0:00:00.334972
elapsed time: 0:01:18.644374
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-10-01 14:38:01.798007
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.77
 ---- batch: 020 ----
mean loss: 62.34
train mean loss: 62.13
epoch train time: 0:00:00.345434
elapsed time: 0:01:18.990016
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-10-01 14:38:02.143655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.02
 ---- batch: 020 ----
mean loss: 62.00
train mean loss: 61.45
epoch train time: 0:00:00.338883
elapsed time: 0:01:19.329089
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-10-01 14:38:02.482722
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.09
 ---- batch: 020 ----
mean loss: 62.31
train mean loss: 61.06
epoch train time: 0:00:00.352195
elapsed time: 0:01:19.681557
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-10-01 14:38:02.835214
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.37
 ---- batch: 020 ----
mean loss: 62.77
train mean loss: 61.79
epoch train time: 0:00:00.345817
elapsed time: 0:01:20.027612
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-10-01 14:38:03.181250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.65
 ---- batch: 020 ----
mean loss: 60.88
train mean loss: 61.31
epoch train time: 0:00:00.344461
elapsed time: 0:01:20.372311
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-10-01 14:38:03.525951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.33
 ---- batch: 020 ----
mean loss: 60.95
train mean loss: 61.39
epoch train time: 0:00:00.363158
elapsed time: 0:01:20.735678
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-10-01 14:38:03.889315
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.77
 ---- batch: 020 ----
mean loss: 64.18
train mean loss: 60.97
epoch train time: 0:00:00.383471
elapsed time: 0:01:21.119340
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-10-01 14:38:04.272994
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.31
 ---- batch: 020 ----
mean loss: 60.19
train mean loss: 60.46
epoch train time: 0:00:00.356513
elapsed time: 0:01:21.476048
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-10-01 14:38:04.629678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.43
 ---- batch: 020 ----
mean loss: 60.94
train mean loss: 61.82
epoch train time: 0:00:00.349629
elapsed time: 0:01:21.825867
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-10-01 14:38:04.979503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.70
 ---- batch: 020 ----
mean loss: 63.18
train mean loss: 61.33
epoch train time: 0:00:00.354401
elapsed time: 0:01:22.180456
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-10-01 14:38:05.334123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.99
 ---- batch: 020 ----
mean loss: 63.11
train mean loss: 61.01
epoch train time: 0:00:00.346157
elapsed time: 0:01:22.526817
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-10-01 14:38:05.680487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.49
 ---- batch: 020 ----
mean loss: 59.91
train mean loss: 60.31
epoch train time: 0:00:00.347928
elapsed time: 0:01:22.875000
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-10-01 14:38:06.028630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.40
 ---- batch: 020 ----
mean loss: 62.97
train mean loss: 60.67
epoch train time: 0:00:00.334011
elapsed time: 0:01:23.209195
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-10-01 14:38:06.362842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.31
 ---- batch: 020 ----
mean loss: 62.04
train mean loss: 61.40
epoch train time: 0:00:00.335856
elapsed time: 0:01:23.545250
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-10-01 14:38:06.698884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.06
 ---- batch: 020 ----
mean loss: 59.69
train mean loss: 60.94
epoch train time: 0:00:00.352660
elapsed time: 0:01:23.898084
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-10-01 14:38:07.051717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.00
 ---- batch: 020 ----
mean loss: 63.19
train mean loss: 61.11
epoch train time: 0:00:00.348284
elapsed time: 0:01:24.246544
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-10-01 14:38:07.400207
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.43
 ---- batch: 020 ----
mean loss: 61.50
train mean loss: 62.22
epoch train time: 0:00:00.345208
elapsed time: 0:01:24.591955
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-10-01 14:38:07.745586
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.93
 ---- batch: 020 ----
mean loss: 61.52
train mean loss: 61.16
epoch train time: 0:00:00.348947
elapsed time: 0:01:24.941083
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-10-01 14:38:08.094725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.60
 ---- batch: 020 ----
mean loss: 61.60
train mean loss: 62.26
epoch train time: 0:00:00.350683
elapsed time: 0:01:25.291949
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-10-01 14:38:08.445582
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.29
 ---- batch: 020 ----
mean loss: 62.59
train mean loss: 61.90
epoch train time: 0:00:00.367264
elapsed time: 0:01:25.659392
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-10-01 14:38:08.813043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.30
 ---- batch: 020 ----
mean loss: 62.21
train mean loss: 62.03
epoch train time: 0:00:00.350617
elapsed time: 0:01:26.010223
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-10-01 14:38:09.163861
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.04
 ---- batch: 020 ----
mean loss: 62.26
train mean loss: 61.13
epoch train time: 0:00:00.349615
elapsed time: 0:01:26.360057
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-10-01 14:38:09.513716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.45
 ---- batch: 020 ----
mean loss: 60.31
train mean loss: 60.50
epoch train time: 0:00:00.346707
elapsed time: 0:01:26.706982
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-10-01 14:38:09.860615
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.86
 ---- batch: 020 ----
mean loss: 59.64
train mean loss: 60.27
epoch train time: 0:00:00.340789
elapsed time: 0:01:27.047978
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-10-01 14:38:10.201614
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.07
 ---- batch: 020 ----
mean loss: 63.00
train mean loss: 60.98
epoch train time: 0:00:00.343299
elapsed time: 0:01:27.391470
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-10-01 14:38:10.545118
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.91
 ---- batch: 020 ----
mean loss: 62.61
train mean loss: 60.49
epoch train time: 0:00:00.341105
elapsed time: 0:01:27.732757
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-10-01 14:38:10.886385
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.30
 ---- batch: 020 ----
mean loss: 63.18
train mean loss: 60.78
epoch train time: 0:00:00.329416
elapsed time: 0:01:28.062338
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-10-01 14:38:11.216022
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.01
 ---- batch: 020 ----
mean loss: 64.35
train mean loss: 61.61
epoch train time: 0:00:00.331103
elapsed time: 0:01:28.393658
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-10-01 14:38:11.547287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.90
 ---- batch: 020 ----
mean loss: 58.78
train mean loss: 61.31
epoch train time: 0:00:00.365298
elapsed time: 0:01:28.759163
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-10-01 14:38:11.912830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.68
 ---- batch: 020 ----
mean loss: 59.47
train mean loss: 61.33
epoch train time: 0:00:00.343699
elapsed time: 0:01:29.103098
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-10-01 14:38:12.256709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.37
 ---- batch: 020 ----
mean loss: 60.29
train mean loss: 59.56
epoch train time: 0:00:00.348129
elapsed time: 0:01:29.451382
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-10-01 14:38:12.605043
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.61
 ---- batch: 020 ----
mean loss: 60.14
train mean loss: 60.08
epoch train time: 0:00:00.352648
elapsed time: 0:01:29.804249
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-10-01 14:38:12.957884
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.11
 ---- batch: 020 ----
mean loss: 59.57
train mean loss: 59.81
epoch train time: 0:00:00.347700
elapsed time: 0:01:30.152124
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-10-01 14:38:13.305777
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 61.01
 ---- batch: 020 ----
mean loss: 61.21
train mean loss: 61.31
epoch train time: 0:00:00.348838
elapsed time: 0:01:30.501163
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-10-01 14:38:13.654801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.63
 ---- batch: 020 ----
mean loss: 58.03
train mean loss: 60.31
epoch train time: 0:00:00.365267
elapsed time: 0:01:30.866613
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-10-01 14:38:14.020248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 58.01
 ---- batch: 020 ----
mean loss: 62.84
train mean loss: 59.80
epoch train time: 0:00:00.349482
elapsed time: 0:01:31.216296
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-10-01 14:38:14.369942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 63.04
 ---- batch: 020 ----
mean loss: 59.31
train mean loss: 61.35
epoch train time: 0:00:00.360979
elapsed time: 0:01:31.577469
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-10-01 14:38:14.731110
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.78
 ---- batch: 020 ----
mean loss: 61.94
train mean loss: 60.58
epoch train time: 0:00:00.358264
elapsed time: 0:01:31.935932
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-10-01 14:38:15.089570
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.42
 ---- batch: 020 ----
mean loss: 61.79
train mean loss: 60.81
epoch train time: 0:00:00.339899
elapsed time: 0:01:32.276007
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-10-01 14:38:15.429650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.32
 ---- batch: 020 ----
mean loss: 61.65
train mean loss: 61.13
epoch train time: 0:00:00.350778
elapsed time: 0:01:32.626969
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-10-01 14:38:15.780622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.28
 ---- batch: 020 ----
mean loss: 61.83
train mean loss: 59.71
epoch train time: 0:00:00.346215
elapsed time: 0:01:32.973410
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-10-01 14:38:16.127078
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 59.76
 ---- batch: 020 ----
mean loss: 62.96
train mean loss: 61.49
epoch train time: 0:00:00.343139
elapsed time: 0:01:33.316772
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-10-01 14:38:16.470418
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 62.75
 ---- batch: 020 ----
mean loss: 58.95
train mean loss: 61.05
epoch train time: 0:00:00.346460
elapsed time: 0:01:33.663422
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-10-01 14:38:16.817058
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.93
 ---- batch: 020 ----
mean loss: 59.91
train mean loss: 60.34
epoch train time: 0:00:00.342342
elapsed time: 0:01:34.005941
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-10-01 14:38:17.159593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 60.90
 ---- batch: 020 ----
mean loss: 60.50
train mean loss: 60.37
epoch train time: 0:00:00.345252
elapsed time: 0:01:34.351392
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-10-01 14:38:17.505032
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 57.67
 ---- batch: 020 ----
mean loss: 62.33
train mean loss: 60.86
epoch train time: 0:00:00.361019
elapsed time: 0:01:34.720627
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_0.50/bayesian_dense3_0.50_8/checkpoint.pth.tar
**** end time: 2019-10-01 14:38:17.874217 ****
