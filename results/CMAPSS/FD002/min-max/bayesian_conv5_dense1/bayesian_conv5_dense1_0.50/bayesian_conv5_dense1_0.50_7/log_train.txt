Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.5, resume=False, step_size=200, visualize_step=50)
pid: 12990
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 11:01:27.158298 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 11:01:27.176350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2667.53
 ---- batch: 020 ----
mean loss: 1788.33
 ---- batch: 030 ----
mean loss: 1407.88
 ---- batch: 040 ----
mean loss: 1322.47
train mean loss: 1697.49
epoch train time: 0:00:23.666803
elapsed time: 0:00:23.693562
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 11:01:50.851952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1207.22
 ---- batch: 020 ----
mean loss: 1178.05
 ---- batch: 030 ----
mean loss: 1163.67
 ---- batch: 040 ----
mean loss: 1093.57
train mean loss: 1148.43
epoch train time: 0:00:08.406251
elapsed time: 0:00:32.100732
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 11:01:59.259331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1090.17
 ---- batch: 020 ----
mean loss: 1069.32
 ---- batch: 030 ----
mean loss: 1078.33
 ---- batch: 040 ----
mean loss: 1071.19
train mean loss: 1072.94
epoch train time: 0:00:08.369664
elapsed time: 0:00:40.471592
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 11:02:07.630189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1057.04
 ---- batch: 020 ----
mean loss: 1033.35
 ---- batch: 030 ----
mean loss: 1043.27
 ---- batch: 040 ----
mean loss: 1040.91
train mean loss: 1046.28
epoch train time: 0:00:08.374805
elapsed time: 0:00:48.847602
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 11:02:16.006201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1054.74
 ---- batch: 020 ----
mean loss: 1016.01
 ---- batch: 030 ----
mean loss: 1041.06
 ---- batch: 040 ----
mean loss: 1015.88
train mean loss: 1032.38
epoch train time: 0:00:08.357246
elapsed time: 0:00:57.205994
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 11:02:24.364663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1031.73
 ---- batch: 020 ----
mean loss: 1032.81
 ---- batch: 030 ----
mean loss: 1016.96
 ---- batch: 040 ----
mean loss: 1010.31
train mean loss: 1021.00
epoch train time: 0:00:08.376251
elapsed time: 0:01:05.583444
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 11:02:32.742090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1025.80
 ---- batch: 020 ----
mean loss: 1008.54
 ---- batch: 030 ----
mean loss: 1008.72
 ---- batch: 040 ----
mean loss: 1013.34
train mean loss: 1015.92
epoch train time: 0:00:08.367775
elapsed time: 0:01:13.952491
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 11:02:41.111085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1013.39
 ---- batch: 020 ----
mean loss: 1000.82
 ---- batch: 030 ----
mean loss: 1003.44
 ---- batch: 040 ----
mean loss: 1025.84
train mean loss: 1008.93
epoch train time: 0:00:08.381723
elapsed time: 0:01:22.335470
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 11:02:49.494132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1029.84
 ---- batch: 020 ----
mean loss: 1010.52
 ---- batch: 030 ----
mean loss: 983.88
 ---- batch: 040 ----
mean loss: 1002.38
train mean loss: 1005.67
epoch train time: 0:00:08.366669
elapsed time: 0:01:30.703406
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 11:02:57.862028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 987.15
 ---- batch: 020 ----
mean loss: 982.63
 ---- batch: 030 ----
mean loss: 998.03
 ---- batch: 040 ----
mean loss: 1008.78
train mean loss: 997.35
epoch train time: 0:00:08.361438
elapsed time: 0:01:39.066046
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 11:03:06.224657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 995.97
 ---- batch: 020 ----
mean loss: 995.68
 ---- batch: 030 ----
mean loss: 995.84
 ---- batch: 040 ----
mean loss: 993.59
train mean loss: 994.55
epoch train time: 0:00:08.377780
elapsed time: 0:01:47.445123
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 11:03:14.603731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.13
 ---- batch: 020 ----
mean loss: 996.31
 ---- batch: 030 ----
mean loss: 1000.88
 ---- batch: 040 ----
mean loss: 995.17
train mean loss: 991.25
epoch train time: 0:00:08.391065
elapsed time: 0:01:55.837382
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 11:03:22.995961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 998.04
 ---- batch: 020 ----
mean loss: 980.91
 ---- batch: 030 ----
mean loss: 958.58
 ---- batch: 040 ----
mean loss: 990.53
train mean loss: 982.19
epoch train time: 0:00:08.354713
elapsed time: 0:02:04.193175
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 11:03:31.351744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.56
 ---- batch: 020 ----
mean loss: 975.37
 ---- batch: 030 ----
mean loss: 977.92
 ---- batch: 040 ----
mean loss: 977.52
train mean loss: 976.80
epoch train time: 0:00:08.374065
elapsed time: 0:02:12.568377
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 11:03:39.726986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.31
 ---- batch: 020 ----
mean loss: 985.73
 ---- batch: 030 ----
mean loss: 971.17
 ---- batch: 040 ----
mean loss: 970.36
train mean loss: 973.44
epoch train time: 0:00:08.386313
elapsed time: 0:02:20.955981
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 11:03:48.114613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.55
 ---- batch: 020 ----
mean loss: 957.10
 ---- batch: 030 ----
mean loss: 982.70
 ---- batch: 040 ----
mean loss: 954.29
train mean loss: 965.10
epoch train time: 0:00:08.372165
elapsed time: 0:02:29.329381
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 11:03:56.487932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.03
 ---- batch: 020 ----
mean loss: 975.12
 ---- batch: 030 ----
mean loss: 945.58
 ---- batch: 040 ----
mean loss: 967.38
train mean loss: 956.20
epoch train time: 0:00:08.404133
elapsed time: 0:02:37.734648
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 11:04:04.893267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 947.16
 ---- batch: 020 ----
mean loss: 947.75
 ---- batch: 030 ----
mean loss: 945.76
 ---- batch: 040 ----
mean loss: 948.08
train mean loss: 946.93
epoch train time: 0:00:08.359785
elapsed time: 0:02:46.095714
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 11:04:13.254346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 937.78
 ---- batch: 020 ----
mean loss: 945.89
 ---- batch: 030 ----
mean loss: 932.16
 ---- batch: 040 ----
mean loss: 959.53
train mean loss: 941.43
epoch train time: 0:00:08.349999
elapsed time: 0:02:54.446939
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 11:04:21.605600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.19
 ---- batch: 020 ----
mean loss: 909.27
 ---- batch: 030 ----
mean loss: 938.65
 ---- batch: 040 ----
mean loss: 951.45
train mean loss: 926.69
epoch train time: 0:00:08.351379
elapsed time: 0:03:02.799566
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 11:04:29.958192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.00
 ---- batch: 020 ----
mean loss: 918.76
 ---- batch: 030 ----
mean loss: 895.04
 ---- batch: 040 ----
mean loss: 907.44
train mean loss: 902.01
epoch train time: 0:00:08.346861
elapsed time: 0:03:11.147663
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 11:04:38.306262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.31
 ---- batch: 020 ----
mean loss: 844.82
 ---- batch: 030 ----
mean loss: 841.45
 ---- batch: 040 ----
mean loss: 820.61
train mean loss: 836.38
epoch train time: 0:00:08.337358
elapsed time: 0:03:19.486178
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 11:04:46.644699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 787.50
 ---- batch: 020 ----
mean loss: 794.60
 ---- batch: 030 ----
mean loss: 786.08
 ---- batch: 040 ----
mean loss: 791.14
train mean loss: 785.55
epoch train time: 0:00:08.347533
elapsed time: 0:03:27.834830
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 11:04:54.993446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 771.52
 ---- batch: 020 ----
mean loss: 742.82
 ---- batch: 030 ----
mean loss: 738.15
 ---- batch: 040 ----
mean loss: 750.87
train mean loss: 751.36
epoch train time: 0:00:08.379664
elapsed time: 0:03:36.215807
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 11:05:03.374392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.34
 ---- batch: 020 ----
mean loss: 725.57
 ---- batch: 030 ----
mean loss: 725.44
 ---- batch: 040 ----
mean loss: 723.84
train mean loss: 726.37
epoch train time: 0:00:08.346445
elapsed time: 0:03:44.563455
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 11:05:11.722085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.36
 ---- batch: 020 ----
mean loss: 696.39
 ---- batch: 030 ----
mean loss: 694.47
 ---- batch: 040 ----
mean loss: 677.26
train mean loss: 695.14
epoch train time: 0:00:08.374912
elapsed time: 0:03:52.939615
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 11:05:20.098222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.71
 ---- batch: 020 ----
mean loss: 688.02
 ---- batch: 030 ----
mean loss: 676.72
 ---- batch: 040 ----
mean loss: 672.01
train mean loss: 677.71
epoch train time: 0:00:08.341183
elapsed time: 0:04:01.281989
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 11:05:28.440593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 660.96
 ---- batch: 020 ----
mean loss: 653.35
 ---- batch: 030 ----
mean loss: 639.29
 ---- batch: 040 ----
mean loss: 645.91
train mean loss: 648.97
epoch train time: 0:00:08.379731
elapsed time: 0:04:09.662914
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 11:05:36.821535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.80
 ---- batch: 020 ----
mean loss: 629.06
 ---- batch: 030 ----
mean loss: 614.98
 ---- batch: 040 ----
mean loss: 617.39
train mean loss: 621.89
epoch train time: 0:00:08.366790
elapsed time: 0:04:18.031327
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 11:05:45.190190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 625.07
 ---- batch: 020 ----
mean loss: 613.96
 ---- batch: 030 ----
mean loss: 590.83
 ---- batch: 040 ----
mean loss: 595.05
train mean loss: 602.35
epoch train time: 0:00:08.374642
elapsed time: 0:04:26.407450
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 11:05:53.566073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 579.25
 ---- batch: 020 ----
mean loss: 589.98
 ---- batch: 030 ----
mean loss: 581.86
 ---- batch: 040 ----
mean loss: 570.99
train mean loss: 579.53
epoch train time: 0:00:08.372563
elapsed time: 0:04:34.781202
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 11:06:01.939994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 564.17
 ---- batch: 020 ----
mean loss: 561.17
 ---- batch: 030 ----
mean loss: 549.50
 ---- batch: 040 ----
mean loss: 545.40
train mean loss: 554.63
epoch train time: 0:00:08.353873
elapsed time: 0:04:43.136480
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 11:06:10.295118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.38
 ---- batch: 020 ----
mean loss: 519.01
 ---- batch: 030 ----
mean loss: 542.18
 ---- batch: 040 ----
mean loss: 530.94
train mean loss: 534.10
epoch train time: 0:00:08.343301
elapsed time: 0:04:51.481027
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 11:06:18.639635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 515.60
 ---- batch: 020 ----
mean loss: 533.80
 ---- batch: 030 ----
mean loss: 498.69
 ---- batch: 040 ----
mean loss: 502.32
train mean loss: 512.16
epoch train time: 0:00:08.367293
elapsed time: 0:04:59.849515
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 11:06:27.008151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.44
 ---- batch: 020 ----
mean loss: 499.30
 ---- batch: 030 ----
mean loss: 491.69
 ---- batch: 040 ----
mean loss: 501.11
train mean loss: 497.93
epoch train time: 0:00:08.405448
elapsed time: 0:05:08.256193
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 11:06:35.414849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.22
 ---- batch: 020 ----
mean loss: 493.19
 ---- batch: 030 ----
mean loss: 510.12
 ---- batch: 040 ----
mean loss: 484.55
train mean loss: 493.03
epoch train time: 0:00:08.415972
elapsed time: 0:05:16.673794
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 11:06:43.832460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.50
 ---- batch: 020 ----
mean loss: 476.14
 ---- batch: 030 ----
mean loss: 478.98
 ---- batch: 040 ----
mean loss: 476.87
train mean loss: 480.33
epoch train time: 0:00:08.338884
elapsed time: 0:05:25.013937
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 11:06:52.172551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.99
 ---- batch: 020 ----
mean loss: 470.35
 ---- batch: 030 ----
mean loss: 474.46
 ---- batch: 040 ----
mean loss: 462.01
train mean loss: 469.88
epoch train time: 0:00:08.334399
elapsed time: 0:05:33.349568
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 11:07:00.508190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 486.23
 ---- batch: 020 ----
mean loss: 448.22
 ---- batch: 030 ----
mean loss: 462.03
 ---- batch: 040 ----
mean loss: 465.07
train mean loss: 465.17
epoch train time: 0:00:08.345644
elapsed time: 0:05:41.696399
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 11:07:08.855039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.63
 ---- batch: 020 ----
mean loss: 462.50
 ---- batch: 030 ----
mean loss: 450.55
 ---- batch: 040 ----
mean loss: 444.39
train mean loss: 451.04
epoch train time: 0:00:08.379256
elapsed time: 0:05:50.076861
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 11:07:17.235477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.38
 ---- batch: 020 ----
mean loss: 440.44
 ---- batch: 030 ----
mean loss: 437.60
 ---- batch: 040 ----
mean loss: 442.12
train mean loss: 444.99
epoch train time: 0:00:08.385967
elapsed time: 0:05:58.464009
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 11:07:25.622624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.02
 ---- batch: 020 ----
mean loss: 442.56
 ---- batch: 030 ----
mean loss: 443.09
 ---- batch: 040 ----
mean loss: 424.45
train mean loss: 437.18
epoch train time: 0:00:08.347843
elapsed time: 0:06:06.813080
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 11:07:33.971688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.32
 ---- batch: 020 ----
mean loss: 427.71
 ---- batch: 030 ----
mean loss: 440.52
 ---- batch: 040 ----
mean loss: 440.21
train mean loss: 433.66
epoch train time: 0:00:08.355751
elapsed time: 0:06:15.169997
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 11:07:42.328629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.46
 ---- batch: 020 ----
mean loss: 432.47
 ---- batch: 030 ----
mean loss: 414.13
 ---- batch: 040 ----
mean loss: 426.80
train mean loss: 423.17
epoch train time: 0:00:08.381114
elapsed time: 0:06:23.552404
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 11:07:50.710937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.41
 ---- batch: 020 ----
mean loss: 404.12
 ---- batch: 030 ----
mean loss: 419.91
 ---- batch: 040 ----
mean loss: 412.26
train mean loss: 413.63
epoch train time: 0:00:08.393418
elapsed time: 0:06:31.947074
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 11:07:59.105736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.12
 ---- batch: 020 ----
mean loss: 422.06
 ---- batch: 030 ----
mean loss: 405.77
 ---- batch: 040 ----
mean loss: 402.83
train mean loss: 411.46
epoch train time: 0:00:08.407934
elapsed time: 0:06:40.356239
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 11:08:07.514910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.76
 ---- batch: 020 ----
mean loss: 400.72
 ---- batch: 030 ----
mean loss: 400.87
 ---- batch: 040 ----
mean loss: 403.06
train mean loss: 401.70
epoch train time: 0:00:08.400943
elapsed time: 0:06:48.758430
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 11:08:15.917061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.60
 ---- batch: 020 ----
mean loss: 388.83
 ---- batch: 030 ----
mean loss: 400.87
 ---- batch: 040 ----
mean loss: 403.33
train mean loss: 397.29
epoch train time: 0:00:08.398886
elapsed time: 0:06:57.158588
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 11:08:24.317288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.32
 ---- batch: 020 ----
mean loss: 390.09
 ---- batch: 030 ----
mean loss: 375.35
 ---- batch: 040 ----
mean loss: 386.60
train mean loss: 389.02
epoch train time: 0:00:08.376581
elapsed time: 0:07:05.536507
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 11:08:32.695143
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.04
 ---- batch: 020 ----
mean loss: 395.83
 ---- batch: 030 ----
mean loss: 393.60
 ---- batch: 040 ----
mean loss: 387.28
train mean loss: 390.45
epoch train time: 0:00:08.383591
elapsed time: 0:07:13.921367
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 11:08:41.079970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.69
 ---- batch: 020 ----
mean loss: 378.36
 ---- batch: 030 ----
mean loss: 392.47
 ---- batch: 040 ----
mean loss: 373.84
train mean loss: 380.48
epoch train time: 0:00:08.389526
elapsed time: 0:07:22.312099
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 11:08:49.470713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.92
 ---- batch: 020 ----
mean loss: 377.50
 ---- batch: 030 ----
mean loss: 370.87
 ---- batch: 040 ----
mean loss: 375.48
train mean loss: 374.14
epoch train time: 0:00:08.379702
elapsed time: 0:07:30.693082
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 11:08:57.851770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.99
 ---- batch: 020 ----
mean loss: 381.38
 ---- batch: 030 ----
mean loss: 375.16
 ---- batch: 040 ----
mean loss: 367.14
train mean loss: 372.17
epoch train time: 0:00:08.390819
elapsed time: 0:07:39.085185
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 11:09:06.243790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.05
 ---- batch: 020 ----
mean loss: 360.94
 ---- batch: 030 ----
mean loss: 368.24
 ---- batch: 040 ----
mean loss: 362.36
train mean loss: 365.83
epoch train time: 0:00:08.406527
elapsed time: 0:07:47.492926
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 11:09:14.651532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.01
 ---- batch: 020 ----
mean loss: 368.91
 ---- batch: 030 ----
mean loss: 359.77
 ---- batch: 040 ----
mean loss: 369.40
train mean loss: 364.52
epoch train time: 0:00:08.365149
elapsed time: 0:07:55.859267
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 11:09:23.017887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.01
 ---- batch: 020 ----
mean loss: 357.61
 ---- batch: 030 ----
mean loss: 349.39
 ---- batch: 040 ----
mean loss: 375.10
train mean loss: 358.68
epoch train time: 0:00:08.468707
elapsed time: 0:08:04.329304
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 11:09:31.488100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.11
 ---- batch: 020 ----
mean loss: 355.62
 ---- batch: 030 ----
mean loss: 353.54
 ---- batch: 040 ----
mean loss: 348.73
train mean loss: 352.50
epoch train time: 0:00:08.630748
elapsed time: 0:08:12.961439
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 11:09:40.120050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.67
 ---- batch: 020 ----
mean loss: 350.62
 ---- batch: 030 ----
mean loss: 347.52
 ---- batch: 040 ----
mean loss: 344.19
train mean loss: 349.64
epoch train time: 0:00:08.494570
elapsed time: 0:08:21.457329
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 11:09:48.615988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.01
 ---- batch: 020 ----
mean loss: 341.48
 ---- batch: 030 ----
mean loss: 341.90
 ---- batch: 040 ----
mean loss: 344.03
train mean loss: 345.38
epoch train time: 0:00:08.662451
elapsed time: 0:08:30.121029
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 11:09:57.279644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.08
 ---- batch: 020 ----
mean loss: 346.57
 ---- batch: 030 ----
mean loss: 347.65
 ---- batch: 040 ----
mean loss: 342.93
train mean loss: 343.89
epoch train time: 0:00:08.720400
elapsed time: 0:08:38.842700
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 11:10:06.001387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.74
 ---- batch: 020 ----
mean loss: 355.40
 ---- batch: 030 ----
mean loss: 329.02
 ---- batch: 040 ----
mean loss: 346.63
train mean loss: 340.86
epoch train time: 0:00:08.653735
elapsed time: 0:08:47.497898
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 11:10:14.656501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.65
 ---- batch: 020 ----
mean loss: 328.43
 ---- batch: 030 ----
mean loss: 354.03
 ---- batch: 040 ----
mean loss: 328.37
train mean loss: 336.96
epoch train time: 0:00:08.624787
elapsed time: 0:08:56.124066
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 11:10:23.282723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.52
 ---- batch: 020 ----
mean loss: 335.70
 ---- batch: 030 ----
mean loss: 339.93
 ---- batch: 040 ----
mean loss: 329.34
train mean loss: 335.92
epoch train time: 0:00:08.645323
elapsed time: 0:09:04.770721
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 11:10:31.929330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.05
 ---- batch: 020 ----
mean loss: 324.32
 ---- batch: 030 ----
mean loss: 332.48
 ---- batch: 040 ----
mean loss: 347.34
train mean loss: 331.79
epoch train time: 0:00:08.627977
elapsed time: 0:09:13.400047
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 11:10:40.558656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.72
 ---- batch: 020 ----
mean loss: 332.23
 ---- batch: 030 ----
mean loss: 315.16
 ---- batch: 040 ----
mean loss: 320.59
train mean loss: 328.56
epoch train time: 0:00:08.659555
elapsed time: 0:09:22.060913
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 11:10:49.219603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.93
 ---- batch: 020 ----
mean loss: 322.32
 ---- batch: 030 ----
mean loss: 328.49
 ---- batch: 040 ----
mean loss: 325.07
train mean loss: 324.87
epoch train time: 0:00:08.674922
elapsed time: 0:09:30.737181
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 11:10:57.895815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.20
 ---- batch: 020 ----
mean loss: 323.45
 ---- batch: 030 ----
mean loss: 305.36
 ---- batch: 040 ----
mean loss: 327.57
train mean loss: 319.76
epoch train time: 0:00:08.643343
elapsed time: 0:09:39.381729
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 11:11:06.540347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.05
 ---- batch: 020 ----
mean loss: 324.68
 ---- batch: 030 ----
mean loss: 330.12
 ---- batch: 040 ----
mean loss: 324.06
train mean loss: 321.56
epoch train time: 0:00:08.696021
elapsed time: 0:09:48.079169
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 11:11:15.237833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.90
 ---- batch: 020 ----
mean loss: 319.11
 ---- batch: 030 ----
mean loss: 313.71
 ---- batch: 040 ----
mean loss: 314.27
train mean loss: 315.03
epoch train time: 0:00:08.700204
elapsed time: 0:09:56.780801
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 11:11:23.939447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.98
 ---- batch: 020 ----
mean loss: 322.48
 ---- batch: 030 ----
mean loss: 322.68
 ---- batch: 040 ----
mean loss: 324.85
train mean loss: 320.41
epoch train time: 0:00:08.696314
elapsed time: 0:10:05.478387
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 11:11:32.637001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.18
 ---- batch: 020 ----
mean loss: 308.63
 ---- batch: 030 ----
mean loss: 313.10
 ---- batch: 040 ----
mean loss: 309.68
train mean loss: 310.23
epoch train time: 0:00:08.724639
elapsed time: 0:10:14.204236
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 11:11:41.362866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.92
 ---- batch: 020 ----
mean loss: 303.15
 ---- batch: 030 ----
mean loss: 323.16
 ---- batch: 040 ----
mean loss: 312.92
train mean loss: 310.87
epoch train time: 0:00:08.644935
elapsed time: 0:10:22.850463
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 11:11:50.009099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.88
 ---- batch: 020 ----
mean loss: 310.58
 ---- batch: 030 ----
mean loss: 305.33
 ---- batch: 040 ----
mean loss: 300.04
train mean loss: 307.23
epoch train time: 0:00:08.415191
elapsed time: 0:10:31.266872
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 11:11:58.425487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.80
 ---- batch: 020 ----
mean loss: 305.08
 ---- batch: 030 ----
mean loss: 313.24
 ---- batch: 040 ----
mean loss: 297.02
train mean loss: 304.28
epoch train time: 0:00:08.447610
elapsed time: 0:10:39.715823
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 11:12:06.874573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.91
 ---- batch: 020 ----
mean loss: 295.06
 ---- batch: 030 ----
mean loss: 307.54
 ---- batch: 040 ----
mean loss: 310.34
train mean loss: 305.10
epoch train time: 0:00:08.382917
elapsed time: 0:10:48.100048
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 11:12:15.258694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.95
 ---- batch: 020 ----
mean loss: 299.77
 ---- batch: 030 ----
mean loss: 301.93
 ---- batch: 040 ----
mean loss: 304.13
train mean loss: 302.18
epoch train time: 0:00:08.335635
elapsed time: 0:10:56.436922
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 11:12:23.595539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.06
 ---- batch: 020 ----
mean loss: 297.00
 ---- batch: 030 ----
mean loss: 301.56
 ---- batch: 040 ----
mean loss: 300.17
train mean loss: 302.93
epoch train time: 0:00:08.374335
elapsed time: 0:11:04.812436
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 11:12:31.971061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.60
 ---- batch: 020 ----
mean loss: 304.27
 ---- batch: 030 ----
mean loss: 304.59
 ---- batch: 040 ----
mean loss: 293.87
train mean loss: 300.51
epoch train time: 0:00:08.370894
elapsed time: 0:11:13.184520
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 11:12:40.343181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.35
 ---- batch: 020 ----
mean loss: 302.67
 ---- batch: 030 ----
mean loss: 305.18
 ---- batch: 040 ----
mean loss: 292.38
train mean loss: 299.82
epoch train time: 0:00:08.327920
elapsed time: 0:11:21.513668
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 11:12:48.672295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.60
 ---- batch: 020 ----
mean loss: 289.54
 ---- batch: 030 ----
mean loss: 294.73
 ---- batch: 040 ----
mean loss: 297.16
train mean loss: 295.44
epoch train time: 0:00:08.322478
elapsed time: 0:11:29.837312
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 11:12:56.995918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.95
 ---- batch: 020 ----
mean loss: 291.14
 ---- batch: 030 ----
mean loss: 284.20
 ---- batch: 040 ----
mean loss: 287.15
train mean loss: 291.57
epoch train time: 0:00:08.314262
elapsed time: 0:11:38.152729
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 11:13:05.311332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.00
 ---- batch: 020 ----
mean loss: 296.79
 ---- batch: 030 ----
mean loss: 282.55
 ---- batch: 040 ----
mean loss: 298.99
train mean loss: 292.87
epoch train time: 0:00:08.330471
elapsed time: 0:11:46.484368
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 11:13:13.643011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.14
 ---- batch: 020 ----
mean loss: 290.80
 ---- batch: 030 ----
mean loss: 292.89
 ---- batch: 040 ----
mean loss: 289.27
train mean loss: 290.37
epoch train time: 0:00:08.361306
elapsed time: 0:11:54.846865
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 11:13:22.005566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.70
 ---- batch: 020 ----
mean loss: 290.61
 ---- batch: 030 ----
mean loss: 296.41
 ---- batch: 040 ----
mean loss: 281.24
train mean loss: 292.58
epoch train time: 0:00:08.337450
elapsed time: 0:12:03.185558
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 11:13:30.344166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.10
 ---- batch: 020 ----
mean loss: 285.70
 ---- batch: 030 ----
mean loss: 290.06
 ---- batch: 040 ----
mean loss: 287.45
train mean loss: 286.46
epoch train time: 0:00:08.540341
elapsed time: 0:12:11.727140
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 11:13:38.885771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.26
 ---- batch: 020 ----
mean loss: 278.73
 ---- batch: 030 ----
mean loss: 287.91
 ---- batch: 040 ----
mean loss: 278.13
train mean loss: 283.38
epoch train time: 0:00:08.384244
elapsed time: 0:12:20.112596
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 11:13:47.271130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.55
 ---- batch: 020 ----
mean loss: 282.79
 ---- batch: 030 ----
mean loss: 288.19
 ---- batch: 040 ----
mean loss: 281.34
train mean loss: 285.63
epoch train time: 0:00:08.449668
elapsed time: 0:12:28.563395
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 11:13:55.722018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.95
 ---- batch: 020 ----
mean loss: 290.19
 ---- batch: 030 ----
mean loss: 277.08
 ---- batch: 040 ----
mean loss: 277.20
train mean loss: 282.98
epoch train time: 0:00:08.448499
elapsed time: 0:12:37.013294
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 11:14:04.171889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.03
 ---- batch: 020 ----
mean loss: 281.60
 ---- batch: 030 ----
mean loss: 279.74
 ---- batch: 040 ----
mean loss: 274.93
train mean loss: 280.77
epoch train time: 0:00:08.442286
elapsed time: 0:12:45.456768
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 11:14:12.615431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.56
 ---- batch: 020 ----
mean loss: 273.74
 ---- batch: 030 ----
mean loss: 277.46
 ---- batch: 040 ----
mean loss: 280.59
train mean loss: 280.16
epoch train time: 0:00:08.568245
elapsed time: 0:12:54.026255
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 11:14:21.184868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.25
 ---- batch: 020 ----
mean loss: 273.46
 ---- batch: 030 ----
mean loss: 269.98
 ---- batch: 040 ----
mean loss: 284.37
train mean loss: 277.48
epoch train time: 0:00:08.517745
elapsed time: 0:13:02.545241
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 11:14:29.703902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.19
 ---- batch: 020 ----
mean loss: 281.85
 ---- batch: 030 ----
mean loss: 280.13
 ---- batch: 040 ----
mean loss: 276.28
train mean loss: 275.82
epoch train time: 0:00:08.465508
elapsed time: 0:13:11.012061
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 11:14:38.170720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.22
 ---- batch: 020 ----
mean loss: 271.19
 ---- batch: 030 ----
mean loss: 280.65
 ---- batch: 040 ----
mean loss: 280.58
train mean loss: 275.05
epoch train time: 0:00:08.459612
elapsed time: 0:13:19.472910
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 11:14:46.631547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.13
 ---- batch: 020 ----
mean loss: 278.38
 ---- batch: 030 ----
mean loss: 275.88
 ---- batch: 040 ----
mean loss: 272.59
train mean loss: 275.29
epoch train time: 0:00:08.422937
elapsed time: 0:13:27.897403
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 11:14:55.055946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.06
 ---- batch: 020 ----
mean loss: 279.97
 ---- batch: 030 ----
mean loss: 265.40
 ---- batch: 040 ----
mean loss: 271.62
train mean loss: 273.12
epoch train time: 0:00:08.436015
elapsed time: 0:13:36.334575
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 11:15:03.493195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.25
 ---- batch: 020 ----
mean loss: 268.48
 ---- batch: 030 ----
mean loss: 269.82
 ---- batch: 040 ----
mean loss: 265.47
train mean loss: 270.00
epoch train time: 0:00:08.484497
elapsed time: 0:13:44.820282
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 11:15:11.979036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.72
 ---- batch: 020 ----
mean loss: 263.01
 ---- batch: 030 ----
mean loss: 265.97
 ---- batch: 040 ----
mean loss: 272.34
train mean loss: 270.68
epoch train time: 0:00:08.488766
elapsed time: 0:13:53.310528
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 11:15:20.469122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.64
 ---- batch: 020 ----
mean loss: 271.34
 ---- batch: 030 ----
mean loss: 269.51
 ---- batch: 040 ----
mean loss: 280.95
train mean loss: 271.89
epoch train time: 0:00:08.516599
elapsed time: 0:14:01.828305
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 11:15:28.986947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.31
 ---- batch: 020 ----
mean loss: 269.27
 ---- batch: 030 ----
mean loss: 274.32
 ---- batch: 040 ----
mean loss: 263.77
train mean loss: 268.37
epoch train time: 0:00:08.512689
elapsed time: 0:14:10.342248
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 11:15:37.500833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.19
 ---- batch: 020 ----
mean loss: 262.03
 ---- batch: 030 ----
mean loss: 278.50
 ---- batch: 040 ----
mean loss: 270.24
train mean loss: 266.53
epoch train time: 0:00:08.505623
elapsed time: 0:14:18.849078
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 11:15:46.007678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.91
 ---- batch: 020 ----
mean loss: 266.75
 ---- batch: 030 ----
mean loss: 260.50
 ---- batch: 040 ----
mean loss: 268.92
train mean loss: 266.39
epoch train time: 0:00:08.475453
elapsed time: 0:14:27.325774
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 11:15:54.484417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.75
 ---- batch: 020 ----
mean loss: 269.77
 ---- batch: 030 ----
mean loss: 263.12
 ---- batch: 040 ----
mean loss: 252.78
train mean loss: 263.58
epoch train time: 0:00:08.483036
elapsed time: 0:14:35.810158
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 11:16:02.968899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.93
 ---- batch: 020 ----
mean loss: 268.00
 ---- batch: 030 ----
mean loss: 262.22
 ---- batch: 040 ----
mean loss: 259.63
train mean loss: 262.11
epoch train time: 0:00:08.450832
elapsed time: 0:14:44.262398
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 11:16:11.421060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.53
 ---- batch: 020 ----
mean loss: 262.12
 ---- batch: 030 ----
mean loss: 257.71
 ---- batch: 040 ----
mean loss: 260.54
train mean loss: 261.39
epoch train time: 0:00:08.475635
elapsed time: 0:14:52.739462
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 11:16:19.898084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.28
 ---- batch: 020 ----
mean loss: 266.36
 ---- batch: 030 ----
mean loss: 265.04
 ---- batch: 040 ----
mean loss: 263.52
train mean loss: 261.91
epoch train time: 0:00:08.497665
elapsed time: 0:15:01.238334
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 11:16:28.396959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.22
 ---- batch: 020 ----
mean loss: 268.49
 ---- batch: 030 ----
mean loss: 263.12
 ---- batch: 040 ----
mean loss: 265.16
train mean loss: 264.59
epoch train time: 0:00:08.469409
elapsed time: 0:15:09.708950
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 11:16:36.867579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.58
 ---- batch: 020 ----
mean loss: 261.18
 ---- batch: 030 ----
mean loss: 254.32
 ---- batch: 040 ----
mean loss: 251.03
train mean loss: 256.35
epoch train time: 0:00:08.475467
elapsed time: 0:15:18.185894
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 11:16:45.344332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.85
 ---- batch: 020 ----
mean loss: 253.94
 ---- batch: 030 ----
mean loss: 261.16
 ---- batch: 040 ----
mean loss: 255.79
train mean loss: 257.29
epoch train time: 0:00:08.470801
elapsed time: 0:15:26.657818
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 11:16:53.816438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.34
 ---- batch: 020 ----
mean loss: 255.30
 ---- batch: 030 ----
mean loss: 261.31
 ---- batch: 040 ----
mean loss: 256.55
train mean loss: 258.50
epoch train time: 0:00:08.612905
elapsed time: 0:15:35.272850
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 11:17:02.431479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.60
 ---- batch: 020 ----
mean loss: 257.08
 ---- batch: 030 ----
mean loss: 245.56
 ---- batch: 040 ----
mean loss: 258.70
train mean loss: 255.53
epoch train time: 0:00:08.493062
elapsed time: 0:15:43.767271
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 11:17:10.925883
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.74
 ---- batch: 020 ----
mean loss: 257.47
 ---- batch: 030 ----
mean loss: 256.63
 ---- batch: 040 ----
mean loss: 257.60
train mean loss: 254.57
epoch train time: 0:00:08.425457
elapsed time: 0:15:52.193919
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 11:17:19.352546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.51
 ---- batch: 020 ----
mean loss: 253.98
 ---- batch: 030 ----
mean loss: 253.79
 ---- batch: 040 ----
mean loss: 255.37
train mean loss: 253.96
epoch train time: 0:00:08.523998
elapsed time: 0:16:00.719140
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 11:17:27.877815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.57
 ---- batch: 020 ----
mean loss: 253.71
 ---- batch: 030 ----
mean loss: 252.61
 ---- batch: 040 ----
mean loss: 251.31
train mean loss: 253.81
epoch train time: 0:00:08.485310
elapsed time: 0:16:09.205740
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 11:17:36.364373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.29
 ---- batch: 020 ----
mean loss: 244.53
 ---- batch: 030 ----
mean loss: 248.96
 ---- batch: 040 ----
mean loss: 262.39
train mean loss: 251.21
epoch train time: 0:00:08.439354
elapsed time: 0:16:17.646333
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 11:17:44.804936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.27
 ---- batch: 020 ----
mean loss: 253.83
 ---- batch: 030 ----
mean loss: 249.28
 ---- batch: 040 ----
mean loss: 246.30
train mean loss: 249.69
epoch train time: 0:00:08.415181
elapsed time: 0:16:26.062705
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 11:17:53.221344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.46
 ---- batch: 020 ----
mean loss: 257.02
 ---- batch: 030 ----
mean loss: 253.02
 ---- batch: 040 ----
mean loss: 257.08
train mean loss: 255.00
epoch train time: 0:00:08.447649
elapsed time: 0:16:34.511583
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 11:18:01.670169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.96
 ---- batch: 020 ----
mean loss: 247.12
 ---- batch: 030 ----
mean loss: 246.96
 ---- batch: 040 ----
mean loss: 253.09
train mean loss: 252.81
epoch train time: 0:00:08.446392
elapsed time: 0:16:42.959173
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 11:18:10.117820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.25
 ---- batch: 020 ----
mean loss: 249.26
 ---- batch: 030 ----
mean loss: 250.73
 ---- batch: 040 ----
mean loss: 255.03
train mean loss: 251.81
epoch train time: 0:00:08.404156
elapsed time: 0:16:51.364543
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 11:18:18.523175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.04
 ---- batch: 020 ----
mean loss: 252.84
 ---- batch: 030 ----
mean loss: 250.98
 ---- batch: 040 ----
mean loss: 242.81
train mean loss: 247.24
epoch train time: 0:00:08.455162
elapsed time: 0:16:59.821009
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 11:18:26.979605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.43
 ---- batch: 020 ----
mean loss: 249.15
 ---- batch: 030 ----
mean loss: 246.68
 ---- batch: 040 ----
mean loss: 251.53
train mean loss: 247.01
epoch train time: 0:00:08.437822
elapsed time: 0:17:08.260124
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 11:18:35.418812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.49
 ---- batch: 020 ----
mean loss: 243.20
 ---- batch: 030 ----
mean loss: 249.34
 ---- batch: 040 ----
mean loss: 249.75
train mean loss: 247.01
epoch train time: 0:00:08.401325
elapsed time: 0:17:16.662712
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 11:18:43.821318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.19
 ---- batch: 020 ----
mean loss: 243.73
 ---- batch: 030 ----
mean loss: 248.80
 ---- batch: 040 ----
mean loss: 244.15
train mean loss: 245.61
epoch train time: 0:00:08.349026
elapsed time: 0:17:25.012910
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 11:18:52.171535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.77
 ---- batch: 020 ----
mean loss: 249.68
 ---- batch: 030 ----
mean loss: 246.61
 ---- batch: 040 ----
mean loss: 235.51
train mean loss: 244.16
epoch train time: 0:00:08.406630
elapsed time: 0:17:33.420809
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 11:19:00.579435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.77
 ---- batch: 020 ----
mean loss: 240.60
 ---- batch: 030 ----
mean loss: 244.88
 ---- batch: 040 ----
mean loss: 243.43
train mean loss: 242.73
epoch train time: 0:00:08.446320
elapsed time: 0:17:41.868347
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 11:19:09.026950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.74
 ---- batch: 020 ----
mean loss: 243.95
 ---- batch: 030 ----
mean loss: 235.33
 ---- batch: 040 ----
mean loss: 245.77
train mean loss: 240.31
epoch train time: 0:00:08.473949
elapsed time: 0:17:50.343700
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 11:19:17.502451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.40
 ---- batch: 020 ----
mean loss: 235.00
 ---- batch: 030 ----
mean loss: 241.54
 ---- batch: 040 ----
mean loss: 247.81
train mean loss: 241.66
epoch train time: 0:00:08.449985
elapsed time: 0:17:58.795088
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 11:19:25.953752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.74
 ---- batch: 020 ----
mean loss: 235.78
 ---- batch: 030 ----
mean loss: 247.34
 ---- batch: 040 ----
mean loss: 232.29
train mean loss: 239.63
epoch train time: 0:00:08.376558
elapsed time: 0:18:07.173228
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 11:19:34.331675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.01
 ---- batch: 020 ----
mean loss: 252.83
 ---- batch: 030 ----
mean loss: 235.00
 ---- batch: 040 ----
mean loss: 247.47
train mean loss: 243.92
epoch train time: 0:00:08.384832
elapsed time: 0:18:15.559217
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 11:19:42.717741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.91
 ---- batch: 020 ----
mean loss: 240.04
 ---- batch: 030 ----
mean loss: 237.14
 ---- batch: 040 ----
mean loss: 243.24
train mean loss: 242.21
epoch train time: 0:00:08.422736
elapsed time: 0:18:23.983042
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 11:19:51.141696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.54
 ---- batch: 020 ----
mean loss: 236.61
 ---- batch: 030 ----
mean loss: 234.10
 ---- batch: 040 ----
mean loss: 238.45
train mean loss: 237.48
epoch train time: 0:00:08.424784
elapsed time: 0:18:32.409228
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 11:19:59.567864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.63
 ---- batch: 020 ----
mean loss: 238.58
 ---- batch: 030 ----
mean loss: 235.89
 ---- batch: 040 ----
mean loss: 238.45
train mean loss: 239.46
epoch train time: 0:00:08.369337
elapsed time: 0:18:40.779981
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 11:20:07.938622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.40
 ---- batch: 020 ----
mean loss: 239.60
 ---- batch: 030 ----
mean loss: 242.25
 ---- batch: 040 ----
mean loss: 234.97
train mean loss: 240.51
epoch train time: 0:00:08.392183
elapsed time: 0:18:49.173485
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 11:20:16.332101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.84
 ---- batch: 020 ----
mean loss: 238.97
 ---- batch: 030 ----
mean loss: 235.16
 ---- batch: 040 ----
mean loss: 240.99
train mean loss: 236.27
epoch train time: 0:00:08.388323
elapsed time: 0:18:57.563066
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 11:20:24.721715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.71
 ---- batch: 020 ----
mean loss: 231.68
 ---- batch: 030 ----
mean loss: 234.23
 ---- batch: 040 ----
mean loss: 236.12
train mean loss: 236.04
epoch train time: 0:00:08.406932
elapsed time: 0:19:05.971208
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 11:20:33.129862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.95
 ---- batch: 020 ----
mean loss: 232.20
 ---- batch: 030 ----
mean loss: 230.41
 ---- batch: 040 ----
mean loss: 239.13
train mean loss: 234.85
epoch train time: 0:00:08.352091
elapsed time: 0:19:14.324606
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 11:20:41.483227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.49
 ---- batch: 020 ----
mean loss: 232.97
 ---- batch: 030 ----
mean loss: 244.62
 ---- batch: 040 ----
mean loss: 237.84
train mean loss: 234.29
epoch train time: 0:00:08.357429
elapsed time: 0:19:22.683264
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 11:20:49.841895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.02
 ---- batch: 020 ----
mean loss: 237.38
 ---- batch: 030 ----
mean loss: 229.54
 ---- batch: 040 ----
mean loss: 231.36
train mean loss: 232.96
epoch train time: 0:00:08.404415
elapsed time: 0:19:31.088909
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 11:20:58.247541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.26
 ---- batch: 020 ----
mean loss: 245.09
 ---- batch: 030 ----
mean loss: 238.51
 ---- batch: 040 ----
mean loss: 231.81
train mean loss: 235.57
epoch train time: 0:00:08.473665
elapsed time: 0:19:39.563943
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 11:21:06.722617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.60
 ---- batch: 020 ----
mean loss: 234.21
 ---- batch: 030 ----
mean loss: 239.45
 ---- batch: 040 ----
mean loss: 230.78
train mean loss: 235.05
epoch train time: 0:00:08.526792
elapsed time: 0:19:48.091982
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 11:21:15.250630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.22
 ---- batch: 020 ----
mean loss: 231.43
 ---- batch: 030 ----
mean loss: 225.68
 ---- batch: 040 ----
mean loss: 244.01
train mean loss: 232.73
epoch train time: 0:00:08.499451
elapsed time: 0:19:56.592686
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 11:21:23.751286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.01
 ---- batch: 020 ----
mean loss: 229.20
 ---- batch: 030 ----
mean loss: 230.85
 ---- batch: 040 ----
mean loss: 241.69
train mean loss: 234.36
epoch train time: 0:00:08.492495
elapsed time: 0:20:05.086430
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 11:21:32.245097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.18
 ---- batch: 020 ----
mean loss: 227.11
 ---- batch: 030 ----
mean loss: 227.45
 ---- batch: 040 ----
mean loss: 234.17
train mean loss: 231.69
epoch train time: 0:00:08.469605
elapsed time: 0:20:13.557326
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 11:21:40.715923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.42
 ---- batch: 020 ----
mean loss: 230.75
 ---- batch: 030 ----
mean loss: 231.90
 ---- batch: 040 ----
mean loss: 233.59
train mean loss: 231.17
epoch train time: 0:00:08.489129
elapsed time: 0:20:22.047637
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 11:21:49.206300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.99
 ---- batch: 020 ----
mean loss: 225.55
 ---- batch: 030 ----
mean loss: 232.35
 ---- batch: 040 ----
mean loss: 232.85
train mean loss: 229.47
epoch train time: 0:00:08.522875
elapsed time: 0:20:30.571786
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 11:21:57.730422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.17
 ---- batch: 020 ----
mean loss: 227.80
 ---- batch: 030 ----
mean loss: 226.99
 ---- batch: 040 ----
mean loss: 237.22
train mean loss: 230.22
epoch train time: 0:00:08.568807
elapsed time: 0:20:39.141908
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 11:22:06.300532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.18
 ---- batch: 020 ----
mean loss: 221.81
 ---- batch: 030 ----
mean loss: 235.03
 ---- batch: 040 ----
mean loss: 226.12
train mean loss: 229.76
epoch train time: 0:00:08.541940
elapsed time: 0:20:47.685103
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 11:22:14.843794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.71
 ---- batch: 020 ----
mean loss: 236.53
 ---- batch: 030 ----
mean loss: 229.69
 ---- batch: 040 ----
mean loss: 219.39
train mean loss: 228.10
epoch train time: 0:00:08.555520
elapsed time: 0:20:56.241931
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 11:22:23.400583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.50
 ---- batch: 020 ----
mean loss: 229.95
 ---- batch: 030 ----
mean loss: 230.49
 ---- batch: 040 ----
mean loss: 236.54
train mean loss: 229.57
epoch train time: 0:00:08.466891
elapsed time: 0:21:04.710088
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 11:22:31.868722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.94
 ---- batch: 020 ----
mean loss: 221.27
 ---- batch: 030 ----
mean loss: 228.38
 ---- batch: 040 ----
mean loss: 228.02
train mean loss: 228.05
epoch train time: 0:00:08.553686
elapsed time: 0:21:13.265228
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 11:22:40.423634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.75
 ---- batch: 020 ----
mean loss: 228.97
 ---- batch: 030 ----
mean loss: 224.26
 ---- batch: 040 ----
mean loss: 228.30
train mean loss: 227.11
epoch train time: 0:00:08.502156
elapsed time: 0:21:21.768460
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 11:22:48.927085
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.56
 ---- batch: 020 ----
mean loss: 234.80
 ---- batch: 030 ----
mean loss: 229.45
 ---- batch: 040 ----
mean loss: 221.89
train mean loss: 228.03
epoch train time: 0:00:08.567753
elapsed time: 0:21:30.337523
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 11:22:57.496170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.48
 ---- batch: 020 ----
mean loss: 226.96
 ---- batch: 030 ----
mean loss: 223.80
 ---- batch: 040 ----
mean loss: 227.63
train mean loss: 228.25
epoch train time: 0:00:08.523847
elapsed time: 0:21:38.862644
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 11:23:06.021319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.35
 ---- batch: 020 ----
mean loss: 228.30
 ---- batch: 030 ----
mean loss: 223.78
 ---- batch: 040 ----
mean loss: 229.79
train mean loss: 225.55
epoch train time: 0:00:08.555332
elapsed time: 0:21:47.419328
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 11:23:14.577920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.22
 ---- batch: 020 ----
mean loss: 224.70
 ---- batch: 030 ----
mean loss: 228.07
 ---- batch: 040 ----
mean loss: 218.51
train mean loss: 224.98
epoch train time: 0:00:08.481776
elapsed time: 0:21:55.902236
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 11:23:23.060820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.43
 ---- batch: 020 ----
mean loss: 234.54
 ---- batch: 030 ----
mean loss: 222.50
 ---- batch: 040 ----
mean loss: 217.94
train mean loss: 224.99
epoch train time: 0:00:08.670748
elapsed time: 0:22:04.574306
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 11:23:31.732899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.05
 ---- batch: 020 ----
mean loss: 223.31
 ---- batch: 030 ----
mean loss: 220.85
 ---- batch: 040 ----
mean loss: 230.22
train mean loss: 224.97
epoch train time: 0:00:08.488292
elapsed time: 0:22:13.063833
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 11:23:40.222526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.19
 ---- batch: 020 ----
mean loss: 222.06
 ---- batch: 030 ----
mean loss: 221.29
 ---- batch: 040 ----
mean loss: 223.91
train mean loss: 224.59
epoch train time: 0:00:08.469762
elapsed time: 0:22:21.534922
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 11:23:48.693518
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.78
 ---- batch: 020 ----
mean loss: 224.48
 ---- batch: 030 ----
mean loss: 218.49
 ---- batch: 040 ----
mean loss: 226.26
train mean loss: 224.71
epoch train time: 0:00:08.469291
elapsed time: 0:22:30.005485
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 11:23:57.164098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.88
 ---- batch: 020 ----
mean loss: 217.94
 ---- batch: 030 ----
mean loss: 220.88
 ---- batch: 040 ----
mean loss: 228.79
train mean loss: 222.50
epoch train time: 0:00:08.551313
elapsed time: 0:22:38.558085
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 11:24:05.716706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.83
 ---- batch: 020 ----
mean loss: 225.57
 ---- batch: 030 ----
mean loss: 225.82
 ---- batch: 040 ----
mean loss: 215.99
train mean loss: 223.23
epoch train time: 0:00:08.562034
elapsed time: 0:22:47.121445
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 11:24:14.280067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.44
 ---- batch: 020 ----
mean loss: 225.89
 ---- batch: 030 ----
mean loss: 221.76
 ---- batch: 040 ----
mean loss: 221.60
train mean loss: 224.03
epoch train time: 0:00:08.395391
elapsed time: 0:22:55.518073
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 11:24:22.676682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.30
 ---- batch: 020 ----
mean loss: 219.60
 ---- batch: 030 ----
mean loss: 224.06
 ---- batch: 040 ----
mean loss: 222.77
train mean loss: 221.35
epoch train time: 0:00:08.438159
elapsed time: 0:23:03.957508
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 11:24:31.116118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.98
 ---- batch: 020 ----
mean loss: 214.70
 ---- batch: 030 ----
mean loss: 226.15
 ---- batch: 040 ----
mean loss: 226.44
train mean loss: 221.92
epoch train time: 0:00:08.407418
elapsed time: 0:23:12.366176
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 11:24:39.524802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.18
 ---- batch: 020 ----
mean loss: 224.90
 ---- batch: 030 ----
mean loss: 231.92
 ---- batch: 040 ----
mean loss: 229.67
train mean loss: 226.01
epoch train time: 0:00:08.435831
elapsed time: 0:23:20.803227
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 11:24:47.961867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.49
 ---- batch: 020 ----
mean loss: 227.94
 ---- batch: 030 ----
mean loss: 218.19
 ---- batch: 040 ----
mean loss: 222.86
train mean loss: 222.48
epoch train time: 0:00:08.537752
elapsed time: 0:23:29.342181
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 11:24:56.500795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.09
 ---- batch: 020 ----
mean loss: 212.75
 ---- batch: 030 ----
mean loss: 217.59
 ---- batch: 040 ----
mean loss: 229.52
train mean loss: 219.85
epoch train time: 0:00:08.456278
elapsed time: 0:23:37.799774
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 11:25:04.958462
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.94
 ---- batch: 020 ----
mean loss: 224.15
 ---- batch: 030 ----
mean loss: 223.86
 ---- batch: 040 ----
mean loss: 216.05
train mean loss: 221.08
epoch train time: 0:00:08.466802
elapsed time: 0:23:46.267850
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 11:25:13.426451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.68
 ---- batch: 020 ----
mean loss: 225.33
 ---- batch: 030 ----
mean loss: 212.47
 ---- batch: 040 ----
mean loss: 219.10
train mean loss: 220.67
epoch train time: 0:00:08.484161
elapsed time: 0:23:54.753240
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 11:25:21.911928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.41
 ---- batch: 020 ----
mean loss: 220.46
 ---- batch: 030 ----
mean loss: 225.56
 ---- batch: 040 ----
mean loss: 215.72
train mean loss: 220.91
epoch train time: 0:00:08.511377
elapsed time: 0:24:03.265901
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 11:25:30.424532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.33
 ---- batch: 020 ----
mean loss: 210.42
 ---- batch: 030 ----
mean loss: 220.90
 ---- batch: 040 ----
mean loss: 222.49
train mean loss: 217.72
epoch train time: 0:00:08.503699
elapsed time: 0:24:11.771264
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 11:25:38.929920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.39
 ---- batch: 020 ----
mean loss: 219.64
 ---- batch: 030 ----
mean loss: 210.13
 ---- batch: 040 ----
mean loss: 223.73
train mean loss: 219.14
epoch train time: 0:00:08.492764
elapsed time: 0:24:20.265273
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 11:25:47.423935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.19
 ---- batch: 020 ----
mean loss: 214.65
 ---- batch: 030 ----
mean loss: 218.29
 ---- batch: 040 ----
mean loss: 225.53
train mean loss: 218.96
epoch train time: 0:00:08.390940
elapsed time: 0:24:28.657476
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 11:25:55.816105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.23
 ---- batch: 020 ----
mean loss: 216.34
 ---- batch: 030 ----
mean loss: 220.80
 ---- batch: 040 ----
mean loss: 219.71
train mean loss: 217.69
epoch train time: 0:00:08.375557
elapsed time: 0:24:37.034181
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 11:26:04.192789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.95
 ---- batch: 020 ----
mean loss: 220.30
 ---- batch: 030 ----
mean loss: 212.61
 ---- batch: 040 ----
mean loss: 215.83
train mean loss: 218.65
epoch train time: 0:00:08.406554
elapsed time: 0:24:45.442161
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 11:26:12.600542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.64
 ---- batch: 020 ----
mean loss: 228.29
 ---- batch: 030 ----
mean loss: 217.68
 ---- batch: 040 ----
mean loss: 223.25
train mean loss: 220.66
epoch train time: 0:00:08.428555
elapsed time: 0:24:53.871709
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 11:26:21.030340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.80
 ---- batch: 020 ----
mean loss: 214.27
 ---- batch: 030 ----
mean loss: 221.77
 ---- batch: 040 ----
mean loss: 221.83
train mean loss: 217.20
epoch train time: 0:00:08.483737
elapsed time: 0:25:02.356742
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 11:26:29.515377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.99
 ---- batch: 020 ----
mean loss: 221.60
 ---- batch: 030 ----
mean loss: 217.16
 ---- batch: 040 ----
mean loss: 220.03
train mean loss: 218.72
epoch train time: 0:00:08.513395
elapsed time: 0:25:10.871576
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 11:26:38.030277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.07
 ---- batch: 020 ----
mean loss: 211.21
 ---- batch: 030 ----
mean loss: 206.77
 ---- batch: 040 ----
mean loss: 216.72
train mean loss: 214.55
epoch train time: 0:00:08.467519
elapsed time: 0:25:19.340349
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 11:26:46.498975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.89
 ---- batch: 020 ----
mean loss: 216.23
 ---- batch: 030 ----
mean loss: 216.43
 ---- batch: 040 ----
mean loss: 213.64
train mean loss: 217.40
epoch train time: 0:00:08.492775
elapsed time: 0:25:27.834376
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 11:26:54.993006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.47
 ---- batch: 020 ----
mean loss: 210.40
 ---- batch: 030 ----
mean loss: 219.60
 ---- batch: 040 ----
mean loss: 220.24
train mean loss: 215.57
epoch train time: 0:00:08.467107
elapsed time: 0:25:36.302746
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 11:27:03.461411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.28
 ---- batch: 020 ----
mean loss: 219.50
 ---- batch: 030 ----
mean loss: 222.84
 ---- batch: 040 ----
mean loss: 205.43
train mean loss: 214.80
epoch train time: 0:00:08.494187
elapsed time: 0:25:44.798189
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 11:27:11.956728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.02
 ---- batch: 020 ----
mean loss: 210.24
 ---- batch: 030 ----
mean loss: 215.23
 ---- batch: 040 ----
mean loss: 214.01
train mean loss: 213.35
epoch train time: 0:00:08.524904
elapsed time: 0:25:53.324300
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 11:27:20.482922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.16
 ---- batch: 020 ----
mean loss: 210.85
 ---- batch: 030 ----
mean loss: 212.47
 ---- batch: 040 ----
mean loss: 215.68
train mean loss: 214.08
epoch train time: 0:00:08.523822
elapsed time: 0:26:01.849349
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 11:27:29.008032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.44
 ---- batch: 020 ----
mean loss: 219.92
 ---- batch: 030 ----
mean loss: 218.52
 ---- batch: 040 ----
mean loss: 206.51
train mean loss: 213.34
epoch train time: 0:00:08.497315
elapsed time: 0:26:10.348003
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 11:27:37.506624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.49
 ---- batch: 020 ----
mean loss: 220.67
 ---- batch: 030 ----
mean loss: 215.26
 ---- batch: 040 ----
mean loss: 213.89
train mean loss: 215.09
epoch train time: 0:00:08.484670
elapsed time: 0:26:18.833934
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 11:27:45.992576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.39
 ---- batch: 020 ----
mean loss: 217.22
 ---- batch: 030 ----
mean loss: 214.94
 ---- batch: 040 ----
mean loss: 212.63
train mean loss: 215.15
epoch train time: 0:00:08.484746
elapsed time: 0:26:27.319926
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 11:27:54.478567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.46
 ---- batch: 020 ----
mean loss: 225.87
 ---- batch: 030 ----
mean loss: 214.75
 ---- batch: 040 ----
mean loss: 220.30
train mean loss: 216.71
epoch train time: 0:00:08.483413
elapsed time: 0:26:35.804532
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 11:28:02.963173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.12
 ---- batch: 020 ----
mean loss: 217.09
 ---- batch: 030 ----
mean loss: 219.42
 ---- batch: 040 ----
mean loss: 210.60
train mean loss: 215.45
epoch train time: 0:00:08.471237
elapsed time: 0:26:44.277011
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 11:28:11.435626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.32
 ---- batch: 020 ----
mean loss: 215.33
 ---- batch: 030 ----
mean loss: 208.95
 ---- batch: 040 ----
mean loss: 208.75
train mean loss: 212.34
epoch train time: 0:00:08.467607
elapsed time: 0:26:52.745930
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 11:28:19.904559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.65
 ---- batch: 020 ----
mean loss: 218.27
 ---- batch: 030 ----
mean loss: 217.62
 ---- batch: 040 ----
mean loss: 216.03
train mean loss: 213.27
epoch train time: 0:00:08.461072
elapsed time: 0:27:01.208258
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 11:28:28.366870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.21
 ---- batch: 020 ----
mean loss: 215.47
 ---- batch: 030 ----
mean loss: 207.46
 ---- batch: 040 ----
mean loss: 209.08
train mean loss: 212.90
epoch train time: 0:00:08.496308
elapsed time: 0:27:09.705898
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 11:28:36.864524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.97
 ---- batch: 020 ----
mean loss: 218.29
 ---- batch: 030 ----
mean loss: 212.47
 ---- batch: 040 ----
mean loss: 208.20
train mean loss: 211.16
epoch train time: 0:00:08.493058
elapsed time: 0:27:18.200266
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 11:28:45.358941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.48
 ---- batch: 020 ----
mean loss: 214.41
 ---- batch: 030 ----
mean loss: 216.00
 ---- batch: 040 ----
mean loss: 207.68
train mean loss: 212.05
epoch train time: 0:00:08.505803
elapsed time: 0:27:26.707398
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 11:28:53.866008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.99
 ---- batch: 020 ----
mean loss: 205.17
 ---- batch: 030 ----
mean loss: 211.32
 ---- batch: 040 ----
mean loss: 214.64
train mean loss: 212.66
epoch train time: 0:00:08.482023
elapsed time: 0:27:35.190693
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 11:29:02.349375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.23
 ---- batch: 020 ----
mean loss: 212.75
 ---- batch: 030 ----
mean loss: 211.86
 ---- batch: 040 ----
mean loss: 209.00
train mean loss: 211.17
epoch train time: 0:00:08.507495
elapsed time: 0:27:43.699503
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 11:29:10.858153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.21
 ---- batch: 020 ----
mean loss: 204.18
 ---- batch: 030 ----
mean loss: 212.22
 ---- batch: 040 ----
mean loss: 216.50
train mean loss: 211.29
epoch train time: 0:00:08.487247
elapsed time: 0:27:52.188058
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 11:29:19.346700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.03
 ---- batch: 020 ----
mean loss: 213.77
 ---- batch: 030 ----
mean loss: 208.65
 ---- batch: 040 ----
mean loss: 209.99
train mean loss: 211.63
epoch train time: 0:00:08.463392
elapsed time: 0:28:00.652715
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 11:29:27.811357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.55
 ---- batch: 020 ----
mean loss: 212.36
 ---- batch: 030 ----
mean loss: 206.11
 ---- batch: 040 ----
mean loss: 218.55
train mean loss: 211.00
epoch train time: 0:00:08.520038
elapsed time: 0:28:09.174007
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 11:29:36.332706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.07
 ---- batch: 020 ----
mean loss: 215.39
 ---- batch: 030 ----
mean loss: 207.97
 ---- batch: 040 ----
mean loss: 210.20
train mean loss: 209.74
epoch train time: 0:00:08.483018
elapsed time: 0:28:17.658329
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 11:29:44.817007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.98
 ---- batch: 020 ----
mean loss: 209.59
 ---- batch: 030 ----
mean loss: 213.47
 ---- batch: 040 ----
mean loss: 211.48
train mean loss: 210.36
epoch train time: 0:00:08.482972
elapsed time: 0:28:26.142560
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 11:29:53.301214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.50
 ---- batch: 020 ----
mean loss: 220.49
 ---- batch: 030 ----
mean loss: 208.75
 ---- batch: 040 ----
mean loss: 215.07
train mean loss: 212.11
epoch train time: 0:00:08.492194
elapsed time: 0:28:34.636045
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 11:30:01.794709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.16
 ---- batch: 020 ----
mean loss: 209.99
 ---- batch: 030 ----
mean loss: 208.09
 ---- batch: 040 ----
mean loss: 211.14
train mean loss: 207.47
epoch train time: 0:00:08.482667
elapsed time: 0:28:43.120225
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 11:30:10.278616
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 213.62
 ---- batch: 020 ----
mean loss: 200.99
 ---- batch: 030 ----
mean loss: 211.24
 ---- batch: 040 ----
mean loss: 209.35
train mean loss: 208.20
epoch train time: 0:00:08.497247
elapsed time: 0:28:51.618493
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 11:30:18.777144
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.54
 ---- batch: 020 ----
mean loss: 203.49
 ---- batch: 030 ----
mean loss: 210.87
 ---- batch: 040 ----
mean loss: 197.64
train mean loss: 206.78
epoch train time: 0:00:08.497034
elapsed time: 0:29:00.116830
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 11:30:27.275446
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.77
 ---- batch: 020 ----
mean loss: 206.43
 ---- batch: 030 ----
mean loss: 211.29
 ---- batch: 040 ----
mean loss: 211.79
train mean loss: 206.86
epoch train time: 0:00:08.495698
elapsed time: 0:29:08.613784
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 11:30:35.772428
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.07
 ---- batch: 020 ----
mean loss: 207.85
 ---- batch: 030 ----
mean loss: 201.02
 ---- batch: 040 ----
mean loss: 210.00
train mean loss: 207.63
epoch train time: 0:00:08.504691
elapsed time: 0:29:17.119752
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 11:30:44.278395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.21
 ---- batch: 020 ----
mean loss: 205.06
 ---- batch: 030 ----
mean loss: 206.06
 ---- batch: 040 ----
mean loss: 207.13
train mean loss: 207.75
epoch train time: 0:00:08.484297
elapsed time: 0:29:25.605373
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 11:30:52.764034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.12
 ---- batch: 020 ----
mean loss: 201.83
 ---- batch: 030 ----
mean loss: 205.82
 ---- batch: 040 ----
mean loss: 207.59
train mean loss: 206.96
epoch train time: 0:00:08.471448
elapsed time: 0:29:34.078124
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 11:31:01.236747
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.91
 ---- batch: 020 ----
mean loss: 212.27
 ---- batch: 030 ----
mean loss: 206.58
 ---- batch: 040 ----
mean loss: 205.40
train mean loss: 207.12
epoch train time: 0:00:08.507385
elapsed time: 0:29:42.586710
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 11:31:09.745341
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.19
 ---- batch: 020 ----
mean loss: 210.27
 ---- batch: 030 ----
mean loss: 204.58
 ---- batch: 040 ----
mean loss: 209.99
train mean loss: 207.22
epoch train time: 0:00:08.519690
elapsed time: 0:29:51.107660
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 11:31:18.266331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.49
 ---- batch: 020 ----
mean loss: 213.27
 ---- batch: 030 ----
mean loss: 207.80
 ---- batch: 040 ----
mean loss: 201.43
train mean loss: 207.13
epoch train time: 0:00:08.483295
elapsed time: 0:29:59.592270
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 11:31:26.750954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.27
 ---- batch: 020 ----
mean loss: 211.85
 ---- batch: 030 ----
mean loss: 210.15
 ---- batch: 040 ----
mean loss: 198.90
train mean loss: 207.51
epoch train time: 0:00:08.491677
elapsed time: 0:30:08.085177
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 11:31:35.243793
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.16
 ---- batch: 020 ----
mean loss: 199.37
 ---- batch: 030 ----
mean loss: 209.54
 ---- batch: 040 ----
mean loss: 205.82
train mean loss: 207.44
epoch train time: 0:00:08.502748
elapsed time: 0:30:16.589290
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 11:31:43.747921
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.97
 ---- batch: 020 ----
mean loss: 208.49
 ---- batch: 030 ----
mean loss: 206.15
 ---- batch: 040 ----
mean loss: 203.06
train mean loss: 206.82
epoch train time: 0:00:08.506865
elapsed time: 0:30:25.097412
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 11:31:52.256032
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.92
 ---- batch: 020 ----
mean loss: 207.49
 ---- batch: 030 ----
mean loss: 202.40
 ---- batch: 040 ----
mean loss: 210.36
train mean loss: 206.91
epoch train time: 0:00:08.477268
elapsed time: 0:30:33.575877
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 11:32:00.734506
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.25
 ---- batch: 020 ----
mean loss: 202.31
 ---- batch: 030 ----
mean loss: 206.10
 ---- batch: 040 ----
mean loss: 208.86
train mean loss: 206.31
epoch train time: 0:00:08.472288
elapsed time: 0:30:42.049417
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 11:32:09.208068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.27
 ---- batch: 020 ----
mean loss: 202.89
 ---- batch: 030 ----
mean loss: 205.23
 ---- batch: 040 ----
mean loss: 205.10
train mean loss: 207.05
epoch train time: 0:00:08.479737
elapsed time: 0:30:50.530548
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 11:32:17.689161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.31
 ---- batch: 020 ----
mean loss: 201.56
 ---- batch: 030 ----
mean loss: 208.17
 ---- batch: 040 ----
mean loss: 210.52
train mean loss: 206.74
epoch train time: 0:00:08.468619
elapsed time: 0:30:59.000485
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 11:32:26.159100
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.01
 ---- batch: 020 ----
mean loss: 207.81
 ---- batch: 030 ----
mean loss: 211.84
 ---- batch: 040 ----
mean loss: 204.84
train mean loss: 207.31
epoch train time: 0:00:08.554750
elapsed time: 0:31:07.556428
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 11:32:34.715039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.90
 ---- batch: 020 ----
mean loss: 213.12
 ---- batch: 030 ----
mean loss: 209.33
 ---- batch: 040 ----
mean loss: 206.98
train mean loss: 207.20
epoch train time: 0:00:08.492121
elapsed time: 0:31:16.049764
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 11:32:43.208400
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.14
 ---- batch: 020 ----
mean loss: 208.20
 ---- batch: 030 ----
mean loss: 202.05
 ---- batch: 040 ----
mean loss: 211.98
train mean loss: 206.75
epoch train time: 0:00:08.464494
elapsed time: 0:31:24.515674
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 11:32:51.674325
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.32
 ---- batch: 020 ----
mean loss: 203.62
 ---- batch: 030 ----
mean loss: 203.34
 ---- batch: 040 ----
mean loss: 208.06
train mean loss: 207.09
epoch train time: 0:00:08.477797
elapsed time: 0:31:32.994674
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 11:33:00.153308
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.76
 ---- batch: 020 ----
mean loss: 202.39
 ---- batch: 030 ----
mean loss: 201.31
 ---- batch: 040 ----
mean loss: 211.73
train mean loss: 206.96
epoch train time: 0:00:08.478263
elapsed time: 0:31:41.474129
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 11:33:08.632725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.22
 ---- batch: 020 ----
mean loss: 208.05
 ---- batch: 030 ----
mean loss: 206.43
 ---- batch: 040 ----
mean loss: 207.11
train mean loss: 207.08
epoch train time: 0:00:08.470770
elapsed time: 0:31:49.946053
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 11:33:17.104640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 209.21
 ---- batch: 020 ----
mean loss: 205.97
 ---- batch: 030 ----
mean loss: 208.00
 ---- batch: 040 ----
mean loss: 205.62
train mean loss: 206.79
epoch train time: 0:00:08.479406
elapsed time: 0:31:58.426647
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 11:33:25.585246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.81
 ---- batch: 020 ----
mean loss: 213.94
 ---- batch: 030 ----
mean loss: 205.93
 ---- batch: 040 ----
mean loss: 199.08
train mean loss: 206.95
epoch train time: 0:00:08.479343
elapsed time: 0:32:06.907166
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 11:33:34.065821
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.76
 ---- batch: 020 ----
mean loss: 204.58
 ---- batch: 030 ----
mean loss: 210.71
 ---- batch: 040 ----
mean loss: 211.22
train mean loss: 207.59
epoch train time: 0:00:08.476124
elapsed time: 0:32:15.384550
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 11:33:42.543197
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.10
 ---- batch: 020 ----
mean loss: 208.76
 ---- batch: 030 ----
mean loss: 205.18
 ---- batch: 040 ----
mean loss: 207.67
train mean loss: 207.05
epoch train time: 0:00:08.480830
elapsed time: 0:32:23.866720
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 11:33:51.025360
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.86
 ---- batch: 020 ----
mean loss: 212.25
 ---- batch: 030 ----
mean loss: 207.39
 ---- batch: 040 ----
mean loss: 203.86
train mean loss: 206.81
epoch train time: 0:00:08.470475
elapsed time: 0:32:32.338377
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 11:33:59.496989
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.13
 ---- batch: 020 ----
mean loss: 204.13
 ---- batch: 030 ----
mean loss: 216.98
 ---- batch: 040 ----
mean loss: 202.21
train mean loss: 206.70
epoch train time: 0:00:08.476801
elapsed time: 0:32:40.816419
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 11:34:07.975028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.59
 ---- batch: 020 ----
mean loss: 202.33
 ---- batch: 030 ----
mean loss: 209.12
 ---- batch: 040 ----
mean loss: 206.87
train mean loss: 206.63
epoch train time: 0:00:08.465567
elapsed time: 0:32:49.283231
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 11:34:16.441888
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.88
 ---- batch: 020 ----
mean loss: 206.41
 ---- batch: 030 ----
mean loss: 203.38
 ---- batch: 040 ----
mean loss: 204.77
train mean loss: 207.12
epoch train time: 0:00:08.469987
elapsed time: 0:32:57.754462
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 11:34:24.913055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.18
 ---- batch: 020 ----
mean loss: 208.49
 ---- batch: 030 ----
mean loss: 205.11
 ---- batch: 040 ----
mean loss: 210.96
train mean loss: 206.48
epoch train time: 0:00:08.468837
elapsed time: 0:33:06.224466
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 11:34:33.383108
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 207.62
 ---- batch: 020 ----
mean loss: 204.95
 ---- batch: 030 ----
mean loss: 206.04
 ---- batch: 040 ----
mean loss: 202.73
train mean loss: 206.24
epoch train time: 0:00:08.497711
elapsed time: 0:33:14.723650
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 11:34:41.882024
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.85
 ---- batch: 020 ----
mean loss: 211.09
 ---- batch: 030 ----
mean loss: 205.42
 ---- batch: 040 ----
mean loss: 208.03
train mean loss: 206.75
epoch train time: 0:00:08.468374
elapsed time: 0:33:23.192952
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 11:34:50.351548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.36
 ---- batch: 020 ----
mean loss: 204.91
 ---- batch: 030 ----
mean loss: 206.96
 ---- batch: 040 ----
mean loss: 199.94
train mean loss: 205.70
epoch train time: 0:00:08.464080
elapsed time: 0:33:31.658224
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 11:34:58.816839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.95
 ---- batch: 020 ----
mean loss: 202.14
 ---- batch: 030 ----
mean loss: 208.55
 ---- batch: 040 ----
mean loss: 203.58
train mean loss: 206.32
epoch train time: 0:00:08.454137
elapsed time: 0:33:40.113498
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 11:35:07.272129
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 212.06
 ---- batch: 020 ----
mean loss: 212.63
 ---- batch: 030 ----
mean loss: 203.17
 ---- batch: 040 ----
mean loss: 199.57
train mean loss: 206.65
epoch train time: 0:00:08.467300
elapsed time: 0:33:48.582044
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 11:35:15.740699
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.17
 ---- batch: 020 ----
mean loss: 208.25
 ---- batch: 030 ----
mean loss: 205.01
 ---- batch: 040 ----
mean loss: 211.10
train mean loss: 206.22
epoch train time: 0:00:08.471646
elapsed time: 0:33:57.054922
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 11:35:24.213554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 206.32
 ---- batch: 020 ----
mean loss: 207.91
 ---- batch: 030 ----
mean loss: 205.57
 ---- batch: 040 ----
mean loss: 209.26
train mean loss: 206.40
epoch train time: 0:00:08.491148
elapsed time: 0:34:05.547384
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 11:35:32.706028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 205.86
 ---- batch: 020 ----
mean loss: 210.83
 ---- batch: 030 ----
mean loss: 204.99
 ---- batch: 040 ----
mean loss: 199.92
train mean loss: 206.61
epoch train time: 0:00:08.451390
elapsed time: 0:34:13.999983
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 11:35:41.158618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.27
 ---- batch: 020 ----
mean loss: 209.64
 ---- batch: 030 ----
mean loss: 207.43
 ---- batch: 040 ----
mean loss: 201.65
train mean loss: 205.88
epoch train time: 0:00:08.456236
elapsed time: 0:34:22.457487
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 11:35:49.616111
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.73
 ---- batch: 020 ----
mean loss: 212.78
 ---- batch: 030 ----
mean loss: 206.66
 ---- batch: 040 ----
mean loss: 201.77
train mean loss: 205.83
epoch train time: 0:00:08.462795
elapsed time: 0:34:30.921505
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 11:35:58.080116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.07
 ---- batch: 020 ----
mean loss: 205.56
 ---- batch: 030 ----
mean loss: 202.99
 ---- batch: 040 ----
mean loss: 214.32
train mean loss: 206.40
epoch train time: 0:00:08.482558
elapsed time: 0:34:39.405360
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 11:36:06.564041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.30
 ---- batch: 020 ----
mean loss: 209.03
 ---- batch: 030 ----
mean loss: 204.37
 ---- batch: 040 ----
mean loss: 212.93
train mean loss: 205.44
epoch train time: 0:00:08.463675
elapsed time: 0:34:47.870318
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 11:36:15.029009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 210.30
 ---- batch: 020 ----
mean loss: 202.68
 ---- batch: 030 ----
mean loss: 203.10
 ---- batch: 040 ----
mean loss: 206.27
train mean loss: 206.02
epoch train time: 0:00:08.465575
elapsed time: 0:34:56.337284
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 11:36:23.495933
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.02
 ---- batch: 020 ----
mean loss: 208.89
 ---- batch: 030 ----
mean loss: 207.80
 ---- batch: 040 ----
mean loss: 205.73
train mean loss: 206.62
epoch train time: 0:00:08.489236
elapsed time: 0:35:04.827872
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 11:36:31.986455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 211.29
 ---- batch: 020 ----
mean loss: 203.71
 ---- batch: 030 ----
mean loss: 200.75
 ---- batch: 040 ----
mean loss: 206.53
train mean loss: 206.56
epoch train time: 0:00:08.491345
elapsed time: 0:35:13.320412
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 11:36:40.479036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 208.17
 ---- batch: 020 ----
mean loss: 207.29
 ---- batch: 030 ----
mean loss: 201.66
 ---- batch: 040 ----
mean loss: 207.62
train mean loss: 206.19
epoch train time: 0:00:08.466704
elapsed time: 0:35:21.788401
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 11:36:48.947163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 204.93
 ---- batch: 020 ----
mean loss: 208.01
 ---- batch: 030 ----
mean loss: 201.97
 ---- batch: 040 ----
mean loss: 204.58
train mean loss: 206.63
epoch train time: 0:00:08.489918
elapsed time: 0:35:30.289321
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.50/bayesian_conv5_dense1_0.50_7/checkpoint.pth.tar
**** end time: 2019-09-25 11:36:57.447636 ****
