Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_7', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 23753
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-25 22:50:54.149291 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 8, 16, 11]             560
           Sigmoid-2            [-1, 8, 16, 11]               0
         AvgPool2d-3             [-1, 8, 8, 11]               0
            Conv2d-4            [-1, 14, 7, 11]             224
           Sigmoid-5            [-1, 14, 7, 11]               0
         AvgPool2d-6            [-1, 14, 3, 11]               0
           Flatten-7                  [-1, 462]               0
            Linear-8                    [-1, 1]             462
================================================================
Total params: 1,246
Trainable params: 1,246
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 22:50:54.154900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4287.29
 ---- batch: 020 ----
mean loss: 4021.80
 ---- batch: 030 ----
mean loss: 3889.65
 ---- batch: 040 ----
mean loss: 3635.21
 ---- batch: 050 ----
mean loss: 3344.81
 ---- batch: 060 ----
mean loss: 3192.35
 ---- batch: 070 ----
mean loss: 2891.82
 ---- batch: 080 ----
mean loss: 2694.85
 ---- batch: 090 ----
mean loss: 2456.48
train mean loss: 3309.67
epoch train time: 0:00:33.336737
elapsed time: 0:00:33.343681
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 22:51:27.493030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2059.32
 ---- batch: 020 ----
mean loss: 1917.07
 ---- batch: 030 ----
mean loss: 1728.11
 ---- batch: 040 ----
mean loss: 1572.54
 ---- batch: 050 ----
mean loss: 1403.71
 ---- batch: 060 ----
mean loss: 1313.40
 ---- batch: 070 ----
mean loss: 1216.18
 ---- batch: 080 ----
mean loss: 1136.90
 ---- batch: 090 ----
mean loss: 1100.35
train mean loss: 1465.11
epoch train time: 0:00:00.714749
elapsed time: 0:00:34.058593
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 22:51:28.207957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1005.17
 ---- batch: 020 ----
mean loss: 962.91
 ---- batch: 030 ----
mean loss: 950.00
 ---- batch: 040 ----
mean loss: 946.04
 ---- batch: 050 ----
mean loss: 931.23
 ---- batch: 060 ----
mean loss: 899.58
 ---- batch: 070 ----
mean loss: 906.96
 ---- batch: 080 ----
mean loss: 877.08
 ---- batch: 090 ----
mean loss: 892.76
train mean loss: 927.30
epoch train time: 0:00:00.689492
elapsed time: 0:00:34.748257
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 22:51:28.897601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.90
 ---- batch: 020 ----
mean loss: 873.87
 ---- batch: 030 ----
mean loss: 879.59
 ---- batch: 040 ----
mean loss: 886.92
 ---- batch: 050 ----
mean loss: 868.35
 ---- batch: 060 ----
mean loss: 872.31
 ---- batch: 070 ----
mean loss: 891.23
 ---- batch: 080 ----
mean loss: 885.34
 ---- batch: 090 ----
mean loss: 870.31
train mean loss: 880.46
epoch train time: 0:00:00.682667
elapsed time: 0:00:35.431095
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 22:51:29.580440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.14
 ---- batch: 020 ----
mean loss: 863.85
 ---- batch: 030 ----
mean loss: 880.90
 ---- batch: 040 ----
mean loss: 880.67
 ---- batch: 050 ----
mean loss: 860.82
 ---- batch: 060 ----
mean loss: 874.45
 ---- batch: 070 ----
mean loss: 889.98
 ---- batch: 080 ----
mean loss: 882.70
 ---- batch: 090 ----
mean loss: 865.45
train mean loss: 874.78
epoch train time: 0:00:00.677005
elapsed time: 0:00:36.108257
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 22:51:30.257624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 878.31
 ---- batch: 020 ----
mean loss: 873.84
 ---- batch: 030 ----
mean loss: 884.00
 ---- batch: 040 ----
mean loss: 873.29
 ---- batch: 050 ----
mean loss: 869.01
 ---- batch: 060 ----
mean loss: 848.15
 ---- batch: 070 ----
mean loss: 885.02
 ---- batch: 080 ----
mean loss: 874.74
 ---- batch: 090 ----
mean loss: 862.94
train mean loss: 872.37
epoch train time: 0:00:00.679541
elapsed time: 0:00:36.787967
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 22:51:30.937328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.94
 ---- batch: 020 ----
mean loss: 872.77
 ---- batch: 030 ----
mean loss: 880.06
 ---- batch: 040 ----
mean loss: 879.92
 ---- batch: 050 ----
mean loss: 877.61
 ---- batch: 060 ----
mean loss: 867.12
 ---- batch: 070 ----
mean loss: 864.48
 ---- batch: 080 ----
mean loss: 863.75
 ---- batch: 090 ----
mean loss: 860.01
train mean loss: 868.84
epoch train time: 0:00:00.675035
elapsed time: 0:00:37.463197
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 22:51:31.612537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.92
 ---- batch: 020 ----
mean loss: 870.52
 ---- batch: 030 ----
mean loss: 902.70
 ---- batch: 040 ----
mean loss: 867.21
 ---- batch: 050 ----
mean loss: 860.87
 ---- batch: 060 ----
mean loss: 856.93
 ---- batch: 070 ----
mean loss: 878.37
 ---- batch: 080 ----
mean loss: 859.44
 ---- batch: 090 ----
mean loss: 841.00
train mean loss: 864.01
epoch train time: 0:00:00.689180
elapsed time: 0:00:38.152527
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 22:51:32.301912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.49
 ---- batch: 020 ----
mean loss: 855.16
 ---- batch: 030 ----
mean loss: 864.27
 ---- batch: 040 ----
mean loss: 887.35
 ---- batch: 050 ----
mean loss: 844.65
 ---- batch: 060 ----
mean loss: 853.22
 ---- batch: 070 ----
mean loss: 851.92
 ---- batch: 080 ----
mean loss: 845.20
 ---- batch: 090 ----
mean loss: 870.42
train mean loss: 859.32
epoch train time: 0:00:00.681818
elapsed time: 0:00:38.834564
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 22:51:32.983935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.64
 ---- batch: 020 ----
mean loss: 859.52
 ---- batch: 030 ----
mean loss: 831.87
 ---- batch: 040 ----
mean loss: 851.62
 ---- batch: 050 ----
mean loss: 866.93
 ---- batch: 060 ----
mean loss: 868.86
 ---- batch: 070 ----
mean loss: 856.25
 ---- batch: 080 ----
mean loss: 842.25
 ---- batch: 090 ----
mean loss: 845.77
train mean loss: 853.68
epoch train time: 0:00:00.673349
elapsed time: 0:00:39.508087
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 22:51:33.657429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.08
 ---- batch: 020 ----
mean loss: 859.32
 ---- batch: 030 ----
mean loss: 849.97
 ---- batch: 040 ----
mean loss: 852.20
 ---- batch: 050 ----
mean loss: 840.55
 ---- batch: 060 ----
mean loss: 850.49
 ---- batch: 070 ----
mean loss: 848.70
 ---- batch: 080 ----
mean loss: 833.52
 ---- batch: 090 ----
mean loss: 847.57
train mean loss: 845.61
epoch train time: 0:00:00.681763
elapsed time: 0:00:40.189999
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 22:51:34.339342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.72
 ---- batch: 020 ----
mean loss: 830.26
 ---- batch: 030 ----
mean loss: 841.00
 ---- batch: 040 ----
mean loss: 860.40
 ---- batch: 050 ----
mean loss: 855.16
 ---- batch: 060 ----
mean loss: 841.18
 ---- batch: 070 ----
mean loss: 850.85
 ---- batch: 080 ----
mean loss: 832.17
 ---- batch: 090 ----
mean loss: 824.35
train mean loss: 836.11
epoch train time: 0:00:00.682393
elapsed time: 0:00:40.872563
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 22:51:35.021950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.19
 ---- batch: 020 ----
mean loss: 833.23
 ---- batch: 030 ----
mean loss: 834.84
 ---- batch: 040 ----
mean loss: 819.47
 ---- batch: 050 ----
mean loss: 836.68
 ---- batch: 060 ----
mean loss: 833.91
 ---- batch: 070 ----
mean loss: 808.55
 ---- batch: 080 ----
mean loss: 823.53
 ---- batch: 090 ----
mean loss: 838.82
train mean loss: 828.43
epoch train time: 0:00:00.674245
elapsed time: 0:00:41.547000
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 22:51:35.696392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.94
 ---- batch: 020 ----
mean loss: 826.32
 ---- batch: 030 ----
mean loss: 821.75
 ---- batch: 040 ----
mean loss: 798.54
 ---- batch: 050 ----
mean loss: 826.62
 ---- batch: 060 ----
mean loss: 811.67
 ---- batch: 070 ----
mean loss: 838.50
 ---- batch: 080 ----
mean loss: 816.22
 ---- batch: 090 ----
mean loss: 816.24
train mean loss: 820.25
epoch train time: 0:00:00.677336
elapsed time: 0:00:42.224555
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 22:51:36.373889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.61
 ---- batch: 020 ----
mean loss: 815.07
 ---- batch: 030 ----
mean loss: 818.41
 ---- batch: 040 ----
mean loss: 810.68
 ---- batch: 050 ----
mean loss: 805.52
 ---- batch: 060 ----
mean loss: 801.82
 ---- batch: 070 ----
mean loss: 803.26
 ---- batch: 080 ----
mean loss: 822.06
 ---- batch: 090 ----
mean loss: 812.06
train mean loss: 813.41
epoch train time: 0:00:00.681188
elapsed time: 0:00:42.905908
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 22:51:37.055252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 802.32
 ---- batch: 020 ----
mean loss: 812.25
 ---- batch: 030 ----
mean loss: 801.22
 ---- batch: 040 ----
mean loss: 817.72
 ---- batch: 050 ----
mean loss: 814.30
 ---- batch: 060 ----
mean loss: 797.45
 ---- batch: 070 ----
mean loss: 788.40
 ---- batch: 080 ----
mean loss: 799.85
 ---- batch: 090 ----
mean loss: 807.59
train mean loss: 805.13
epoch train time: 0:00:00.673854
elapsed time: 0:00:43.579925
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 22:51:37.729271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.75
 ---- batch: 020 ----
mean loss: 782.56
 ---- batch: 030 ----
mean loss: 796.52
 ---- batch: 040 ----
mean loss: 805.09
 ---- batch: 050 ----
mean loss: 783.15
 ---- batch: 060 ----
mean loss: 803.69
 ---- batch: 070 ----
mean loss: 809.09
 ---- batch: 080 ----
mean loss: 813.82
 ---- batch: 090 ----
mean loss: 779.84
train mean loss: 798.38
epoch train time: 0:00:00.681378
elapsed time: 0:00:44.261466
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 22:51:38.410810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.04
 ---- batch: 020 ----
mean loss: 787.25
 ---- batch: 030 ----
mean loss: 804.53
 ---- batch: 040 ----
mean loss: 799.76
 ---- batch: 050 ----
mean loss: 776.11
 ---- batch: 060 ----
mean loss: 775.26
 ---- batch: 070 ----
mean loss: 790.73
 ---- batch: 080 ----
mean loss: 786.28
 ---- batch: 090 ----
mean loss: 785.99
train mean loss: 790.42
epoch train time: 0:00:00.682177
elapsed time: 0:00:44.943794
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 22:51:39.093136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 792.51
 ---- batch: 020 ----
mean loss: 793.90
 ---- batch: 030 ----
mean loss: 770.20
 ---- batch: 040 ----
mean loss: 786.83
 ---- batch: 050 ----
mean loss: 784.34
 ---- batch: 060 ----
mean loss: 781.98
 ---- batch: 070 ----
mean loss: 765.67
 ---- batch: 080 ----
mean loss: 788.75
 ---- batch: 090 ----
mean loss: 788.21
train mean loss: 782.70
epoch train time: 0:00:00.683289
elapsed time: 0:00:45.627282
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 22:51:39.776647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 777.19
 ---- batch: 020 ----
mean loss: 787.40
 ---- batch: 030 ----
mean loss: 778.83
 ---- batch: 040 ----
mean loss: 781.73
 ---- batch: 050 ----
mean loss: 764.58
 ---- batch: 060 ----
mean loss: 775.87
 ---- batch: 070 ----
mean loss: 777.79
 ---- batch: 080 ----
mean loss: 763.67
 ---- batch: 090 ----
mean loss: 765.74
train mean loss: 774.99
epoch train time: 0:00:00.678939
elapsed time: 0:00:46.306427
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 22:51:40.455786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 773.86
 ---- batch: 020 ----
mean loss: 766.09
 ---- batch: 030 ----
mean loss: 760.29
 ---- batch: 040 ----
mean loss: 768.37
 ---- batch: 050 ----
mean loss: 777.04
 ---- batch: 060 ----
mean loss: 762.46
 ---- batch: 070 ----
mean loss: 752.34
 ---- batch: 080 ----
mean loss: 772.96
 ---- batch: 090 ----
mean loss: 768.53
train mean loss: 765.72
epoch train time: 0:00:00.683825
elapsed time: 0:00:46.990427
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 22:51:41.139793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.92
 ---- batch: 020 ----
mean loss: 768.37
 ---- batch: 030 ----
mean loss: 753.00
 ---- batch: 040 ----
mean loss: 743.31
 ---- batch: 050 ----
mean loss: 750.77
 ---- batch: 060 ----
mean loss: 770.49
 ---- batch: 070 ----
mean loss: 757.74
 ---- batch: 080 ----
mean loss: 755.42
 ---- batch: 090 ----
mean loss: 759.72
train mean loss: 757.80
epoch train time: 0:00:00.686695
elapsed time: 0:00:47.677300
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 22:51:41.826649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.80
 ---- batch: 020 ----
mean loss: 745.41
 ---- batch: 030 ----
mean loss: 737.10
 ---- batch: 040 ----
mean loss: 744.77
 ---- batch: 050 ----
mean loss: 748.64
 ---- batch: 060 ----
mean loss: 747.47
 ---- batch: 070 ----
mean loss: 748.82
 ---- batch: 080 ----
mean loss: 751.95
 ---- batch: 090 ----
mean loss: 752.05
train mean loss: 748.38
epoch train time: 0:00:00.685712
elapsed time: 0:00:48.363219
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 22:51:42.512563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 756.11
 ---- batch: 020 ----
mean loss: 734.66
 ---- batch: 030 ----
mean loss: 740.22
 ---- batch: 040 ----
mean loss: 729.51
 ---- batch: 050 ----
mean loss: 736.92
 ---- batch: 060 ----
mean loss: 728.05
 ---- batch: 070 ----
mean loss: 743.62
 ---- batch: 080 ----
mean loss: 745.69
 ---- batch: 090 ----
mean loss: 734.45
train mean loss: 739.89
epoch train time: 0:00:00.677113
elapsed time: 0:00:49.040515
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 22:51:43.189855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.96
 ---- batch: 020 ----
mean loss: 744.17
 ---- batch: 030 ----
mean loss: 735.42
 ---- batch: 040 ----
mean loss: 736.12
 ---- batch: 050 ----
mean loss: 735.66
 ---- batch: 060 ----
mean loss: 728.66
 ---- batch: 070 ----
mean loss: 724.70
 ---- batch: 080 ----
mean loss: 721.87
 ---- batch: 090 ----
mean loss: 731.47
train mean loss: 729.77
epoch train time: 0:00:00.670340
elapsed time: 0:00:49.711034
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 22:51:43.860379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 716.53
 ---- batch: 020 ----
mean loss: 719.02
 ---- batch: 030 ----
mean loss: 715.41
 ---- batch: 040 ----
mean loss: 711.39
 ---- batch: 050 ----
mean loss: 714.02
 ---- batch: 060 ----
mean loss: 738.97
 ---- batch: 070 ----
mean loss: 723.92
 ---- batch: 080 ----
mean loss: 719.03
 ---- batch: 090 ----
mean loss: 713.02
train mean loss: 719.83
epoch train time: 0:00:00.678462
elapsed time: 0:00:50.389654
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 22:51:44.538996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 718.10
 ---- batch: 020 ----
mean loss: 719.40
 ---- batch: 030 ----
mean loss: 706.71
 ---- batch: 040 ----
mean loss: 708.90
 ---- batch: 050 ----
mean loss: 697.09
 ---- batch: 060 ----
mean loss: 708.48
 ---- batch: 070 ----
mean loss: 713.14
 ---- batch: 080 ----
mean loss: 715.57
 ---- batch: 090 ----
mean loss: 712.95
train mean loss: 709.84
epoch train time: 0:00:00.674982
elapsed time: 0:00:51.064795
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 22:51:45.214154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 716.25
 ---- batch: 020 ----
mean loss: 698.77
 ---- batch: 030 ----
mean loss: 701.06
 ---- batch: 040 ----
mean loss: 709.53
 ---- batch: 050 ----
mean loss: 703.76
 ---- batch: 060 ----
mean loss: 691.42
 ---- batch: 070 ----
mean loss: 686.67
 ---- batch: 080 ----
mean loss: 712.52
 ---- batch: 090 ----
mean loss: 683.14
train mean loss: 699.44
epoch train time: 0:00:00.674602
elapsed time: 0:00:51.739575
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 22:51:45.888927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.81
 ---- batch: 020 ----
mean loss: 688.99
 ---- batch: 030 ----
mean loss: 680.55
 ---- batch: 040 ----
mean loss: 691.17
 ---- batch: 050 ----
mean loss: 695.19
 ---- batch: 060 ----
mean loss: 695.70
 ---- batch: 070 ----
mean loss: 704.74
 ---- batch: 080 ----
mean loss: 683.19
 ---- batch: 090 ----
mean loss: 675.17
train mean loss: 689.87
epoch train time: 0:00:00.668360
elapsed time: 0:00:52.408087
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 22:51:46.557426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.78
 ---- batch: 020 ----
mean loss: 687.88
 ---- batch: 030 ----
mean loss: 679.46
 ---- batch: 040 ----
mean loss: 678.56
 ---- batch: 050 ----
mean loss: 683.90
 ---- batch: 060 ----
mean loss: 690.09
 ---- batch: 070 ----
mean loss: 674.71
 ---- batch: 080 ----
mean loss: 675.75
 ---- batch: 090 ----
mean loss: 653.68
train mean loss: 678.95
epoch train time: 0:00:00.672553
elapsed time: 0:00:53.080787
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 22:51:47.230127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.74
 ---- batch: 020 ----
mean loss: 670.62
 ---- batch: 030 ----
mean loss: 667.68
 ---- batch: 040 ----
mean loss: 674.19
 ---- batch: 050 ----
mean loss: 685.92
 ---- batch: 060 ----
mean loss: 661.71
 ---- batch: 070 ----
mean loss: 678.96
 ---- batch: 080 ----
mean loss: 665.15
 ---- batch: 090 ----
mean loss: 654.07
train mean loss: 668.72
epoch train time: 0:00:00.669223
elapsed time: 0:00:53.750155
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 22:51:47.899498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 666.48
 ---- batch: 020 ----
mean loss: 668.37
 ---- batch: 030 ----
mean loss: 662.12
 ---- batch: 040 ----
mean loss: 651.03
 ---- batch: 050 ----
mean loss: 650.89
 ---- batch: 060 ----
mean loss: 665.41
 ---- batch: 070 ----
mean loss: 642.96
 ---- batch: 080 ----
mean loss: 671.69
 ---- batch: 090 ----
mean loss: 651.74
train mean loss: 658.54
epoch train time: 0:00:00.667450
elapsed time: 0:00:54.417750
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 22:51:48.567098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 653.88
 ---- batch: 020 ----
mean loss: 655.95
 ---- batch: 030 ----
mean loss: 651.34
 ---- batch: 040 ----
mean loss: 649.08
 ---- batch: 050 ----
mean loss: 641.33
 ---- batch: 060 ----
mean loss: 644.05
 ---- batch: 070 ----
mean loss: 647.08
 ---- batch: 080 ----
mean loss: 646.27
 ---- batch: 090 ----
mean loss: 641.46
train mean loss: 648.81
epoch train time: 0:00:00.685316
elapsed time: 0:00:55.103216
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 22:51:49.252556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 646.36
 ---- batch: 020 ----
mean loss: 646.52
 ---- batch: 030 ----
mean loss: 643.96
 ---- batch: 040 ----
mean loss: 639.24
 ---- batch: 050 ----
mean loss: 645.57
 ---- batch: 060 ----
mean loss: 636.21
 ---- batch: 070 ----
mean loss: 636.27
 ---- batch: 080 ----
mean loss: 623.91
 ---- batch: 090 ----
mean loss: 630.17
train mean loss: 638.46
epoch train time: 0:00:00.685752
elapsed time: 0:00:55.789131
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 22:51:49.938475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.27
 ---- batch: 020 ----
mean loss: 631.56
 ---- batch: 030 ----
mean loss: 622.35
 ---- batch: 040 ----
mean loss: 628.25
 ---- batch: 050 ----
mean loss: 625.49
 ---- batch: 060 ----
mean loss: 633.05
 ---- batch: 070 ----
mean loss: 630.11
 ---- batch: 080 ----
mean loss: 621.40
 ---- batch: 090 ----
mean loss: 636.60
train mean loss: 628.64
epoch train time: 0:00:00.676236
elapsed time: 0:00:56.465546
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 22:51:50.614897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 623.36
 ---- batch: 020 ----
mean loss: 620.95
 ---- batch: 030 ----
mean loss: 627.92
 ---- batch: 040 ----
mean loss: 627.37
 ---- batch: 050 ----
mean loss: 623.40
 ---- batch: 060 ----
mean loss: 603.43
 ---- batch: 070 ----
mean loss: 607.49
 ---- batch: 080 ----
mean loss: 609.74
 ---- batch: 090 ----
mean loss: 631.78
train mean loss: 618.61
epoch train time: 0:00:00.688368
elapsed time: 0:00:57.154152
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 22:51:51.303512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 623.98
 ---- batch: 020 ----
mean loss: 613.37
 ---- batch: 030 ----
mean loss: 607.89
 ---- batch: 040 ----
mean loss: 619.81
 ---- batch: 050 ----
mean loss: 615.40
 ---- batch: 060 ----
mean loss: 603.19
 ---- batch: 070 ----
mean loss: 610.73
 ---- batch: 080 ----
mean loss: 588.67
 ---- batch: 090 ----
mean loss: 596.73
train mean loss: 609.22
epoch train time: 0:00:00.684586
elapsed time: 0:00:57.838898
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 22:51:51.988238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 595.01
 ---- batch: 020 ----
mean loss: 609.42
 ---- batch: 030 ----
mean loss: 598.50
 ---- batch: 040 ----
mean loss: 611.13
 ---- batch: 050 ----
mean loss: 596.77
 ---- batch: 060 ----
mean loss: 597.81
 ---- batch: 070 ----
mean loss: 601.18
 ---- batch: 080 ----
mean loss: 601.51
 ---- batch: 090 ----
mean loss: 584.57
train mean loss: 599.88
epoch train time: 0:00:00.684482
elapsed time: 0:00:58.523522
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 22:51:52.672877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.56
 ---- batch: 020 ----
mean loss: 592.16
 ---- batch: 030 ----
mean loss: 589.18
 ---- batch: 040 ----
mean loss: 594.47
 ---- batch: 050 ----
mean loss: 585.90
 ---- batch: 060 ----
mean loss: 594.48
 ---- batch: 070 ----
mean loss: 596.94
 ---- batch: 080 ----
mean loss: 586.19
 ---- batch: 090 ----
mean loss: 587.31
train mean loss: 591.33
epoch train time: 0:00:00.679582
elapsed time: 0:00:59.203287
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 22:51:53.352660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 589.34
 ---- batch: 020 ----
mean loss: 576.29
 ---- batch: 030 ----
mean loss: 577.92
 ---- batch: 040 ----
mean loss: 590.91
 ---- batch: 050 ----
mean loss: 584.63
 ---- batch: 060 ----
mean loss: 581.70
 ---- batch: 070 ----
mean loss: 576.96
 ---- batch: 080 ----
mean loss: 574.56
 ---- batch: 090 ----
mean loss: 583.23
train mean loss: 582.15
epoch train time: 0:00:00.685955
elapsed time: 0:00:59.889423
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 22:51:54.038767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 585.82
 ---- batch: 020 ----
mean loss: 578.73
 ---- batch: 030 ----
mean loss: 570.94
 ---- batch: 040 ----
mean loss: 569.83
 ---- batch: 050 ----
mean loss: 573.50
 ---- batch: 060 ----
mean loss: 577.70
 ---- batch: 070 ----
mean loss: 575.86
 ---- batch: 080 ----
mean loss: 569.45
 ---- batch: 090 ----
mean loss: 561.87
train mean loss: 573.62
epoch train time: 0:00:00.677416
elapsed time: 0:01:00.566990
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 22:51:54.716332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 560.70
 ---- batch: 020 ----
mean loss: 568.90
 ---- batch: 030 ----
mean loss: 568.99
 ---- batch: 040 ----
mean loss: 568.60
 ---- batch: 050 ----
mean loss: 556.91
 ---- batch: 060 ----
mean loss: 575.65
 ---- batch: 070 ----
mean loss: 559.08
 ---- batch: 080 ----
mean loss: 557.39
 ---- batch: 090 ----
mean loss: 567.99
train mean loss: 565.35
epoch train time: 0:00:00.672128
elapsed time: 0:01:01.239259
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 22:51:55.388600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 555.10
 ---- batch: 020 ----
mean loss: 549.39
 ---- batch: 030 ----
mean loss: 560.56
 ---- batch: 040 ----
mean loss: 539.81
 ---- batch: 050 ----
mean loss: 555.78
 ---- batch: 060 ----
mean loss: 559.32
 ---- batch: 070 ----
mean loss: 555.93
 ---- batch: 080 ----
mean loss: 565.04
 ---- batch: 090 ----
mean loss: 560.78
train mean loss: 557.38
epoch train time: 0:00:00.679797
elapsed time: 0:01:01.919217
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 22:51:56.068585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 553.14
 ---- batch: 020 ----
mean loss: 556.33
 ---- batch: 030 ----
mean loss: 555.82
 ---- batch: 040 ----
mean loss: 551.56
 ---- batch: 050 ----
mean loss: 544.64
 ---- batch: 060 ----
mean loss: 545.88
 ---- batch: 070 ----
mean loss: 544.99
 ---- batch: 080 ----
mean loss: 546.28
 ---- batch: 090 ----
mean loss: 548.53
train mean loss: 549.22
epoch train time: 0:00:00.674241
elapsed time: 0:01:02.593636
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 22:51:56.742981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 542.84
 ---- batch: 020 ----
mean loss: 535.81
 ---- batch: 030 ----
mean loss: 561.24
 ---- batch: 040 ----
mean loss: 537.32
 ---- batch: 050 ----
mean loss: 535.52
 ---- batch: 060 ----
mean loss: 541.99
 ---- batch: 070 ----
mean loss: 544.14
 ---- batch: 080 ----
mean loss: 529.49
 ---- batch: 090 ----
mean loss: 542.84
train mean loss: 540.99
epoch train time: 0:00:00.674053
elapsed time: 0:01:03.267868
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 22:51:57.417212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 535.54
 ---- batch: 020 ----
mean loss: 541.43
 ---- batch: 030 ----
mean loss: 531.98
 ---- batch: 040 ----
mean loss: 533.54
 ---- batch: 050 ----
mean loss: 524.91
 ---- batch: 060 ----
mean loss: 522.31
 ---- batch: 070 ----
mean loss: 532.82
 ---- batch: 080 ----
mean loss: 514.34
 ---- batch: 090 ----
mean loss: 519.20
train mean loss: 527.67
epoch train time: 0:00:00.679220
elapsed time: 0:01:03.947286
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 22:51:58.096629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.55
 ---- batch: 020 ----
mean loss: 512.87
 ---- batch: 030 ----
mean loss: 514.10
 ---- batch: 040 ----
mean loss: 501.93
 ---- batch: 050 ----
mean loss: 508.65
 ---- batch: 060 ----
mean loss: 506.02
 ---- batch: 070 ----
mean loss: 502.83
 ---- batch: 080 ----
mean loss: 482.55
 ---- batch: 090 ----
mean loss: 490.67
train mean loss: 503.72
epoch train time: 0:00:00.672428
elapsed time: 0:01:04.619863
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 22:51:58.769205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.37
 ---- batch: 020 ----
mean loss: 485.54
 ---- batch: 030 ----
mean loss: 486.99
 ---- batch: 040 ----
mean loss: 473.80
 ---- batch: 050 ----
mean loss: 498.81
 ---- batch: 060 ----
mean loss: 469.52
 ---- batch: 070 ----
mean loss: 479.11
 ---- batch: 080 ----
mean loss: 468.05
 ---- batch: 090 ----
mean loss: 476.87
train mean loss: 480.96
epoch train time: 0:00:00.680818
elapsed time: 0:01:05.300917
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 22:51:59.450274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.11
 ---- batch: 020 ----
mean loss: 460.71
 ---- batch: 030 ----
mean loss: 462.26
 ---- batch: 040 ----
mean loss: 450.39
 ---- batch: 050 ----
mean loss: 462.10
 ---- batch: 060 ----
mean loss: 456.33
 ---- batch: 070 ----
mean loss: 462.05
 ---- batch: 080 ----
mean loss: 462.50
 ---- batch: 090 ----
mean loss: 458.07
train mean loss: 459.55
epoch train time: 0:00:00.679046
elapsed time: 0:01:05.980121
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 22:52:00.129461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.99
 ---- batch: 020 ----
mean loss: 437.50
 ---- batch: 030 ----
mean loss: 445.50
 ---- batch: 040 ----
mean loss: 446.32
 ---- batch: 050 ----
mean loss: 429.06
 ---- batch: 060 ----
mean loss: 443.63
 ---- batch: 070 ----
mean loss: 428.07
 ---- batch: 080 ----
mean loss: 434.69
 ---- batch: 090 ----
mean loss: 430.68
train mean loss: 437.39
epoch train time: 0:00:00.676936
elapsed time: 0:01:06.657201
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 22:52:00.806567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.31
 ---- batch: 020 ----
mean loss: 420.32
 ---- batch: 030 ----
mean loss: 426.02
 ---- batch: 040 ----
mean loss: 415.32
 ---- batch: 050 ----
mean loss: 411.00
 ---- batch: 060 ----
mean loss: 414.51
 ---- batch: 070 ----
mean loss: 411.11
 ---- batch: 080 ----
mean loss: 403.19
 ---- batch: 090 ----
mean loss: 406.55
train mean loss: 414.26
epoch train time: 0:00:00.683075
elapsed time: 0:01:07.340452
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 22:52:01.489813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.25
 ---- batch: 020 ----
mean loss: 396.15
 ---- batch: 030 ----
mean loss: 398.14
 ---- batch: 040 ----
mean loss: 397.23
 ---- batch: 050 ----
mean loss: 381.20
 ---- batch: 060 ----
mean loss: 383.54
 ---- batch: 070 ----
mean loss: 382.48
 ---- batch: 080 ----
mean loss: 392.01
 ---- batch: 090 ----
mean loss: 391.81
train mean loss: 391.18
epoch train time: 0:00:00.681422
elapsed time: 0:01:08.022029
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 22:52:02.171365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.15
 ---- batch: 020 ----
mean loss: 377.57
 ---- batch: 030 ----
mean loss: 375.08
 ---- batch: 040 ----
mean loss: 364.93
 ---- batch: 050 ----
mean loss: 378.93
 ---- batch: 060 ----
mean loss: 378.83
 ---- batch: 070 ----
mean loss: 359.40
 ---- batch: 080 ----
mean loss: 368.02
 ---- batch: 090 ----
mean loss: 368.03
train mean loss: 371.49
epoch train time: 0:00:00.670876
elapsed time: 0:01:08.693062
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 22:52:02.842415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.32
 ---- batch: 020 ----
mean loss: 361.07
 ---- batch: 030 ----
mean loss: 374.96
 ---- batch: 040 ----
mean loss: 357.25
 ---- batch: 050 ----
mean loss: 356.31
 ---- batch: 060 ----
mean loss: 359.77
 ---- batch: 070 ----
mean loss: 348.26
 ---- batch: 080 ----
mean loss: 363.90
 ---- batch: 090 ----
mean loss: 341.07
train mean loss: 358.40
epoch train time: 0:00:00.669633
elapsed time: 0:01:09.362851
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 22:52:03.512193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.39
 ---- batch: 020 ----
mean loss: 346.64
 ---- batch: 030 ----
mean loss: 356.04
 ---- batch: 040 ----
mean loss: 340.73
 ---- batch: 050 ----
mean loss: 350.05
 ---- batch: 060 ----
mean loss: 354.14
 ---- batch: 070 ----
mean loss: 354.61
 ---- batch: 080 ----
mean loss: 335.80
 ---- batch: 090 ----
mean loss: 349.81
train mean loss: 348.72
epoch train time: 0:00:00.677067
elapsed time: 0:01:10.040074
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 22:52:04.189412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.29
 ---- batch: 020 ----
mean loss: 341.97
 ---- batch: 030 ----
mean loss: 342.44
 ---- batch: 040 ----
mean loss: 343.86
 ---- batch: 050 ----
mean loss: 342.41
 ---- batch: 060 ----
mean loss: 341.92
 ---- batch: 070 ----
mean loss: 351.27
 ---- batch: 080 ----
mean loss: 321.12
 ---- batch: 090 ----
mean loss: 335.20
train mean loss: 340.98
epoch train time: 0:00:00.679467
elapsed time: 0:01:10.719691
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 22:52:04.869042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.56
 ---- batch: 020 ----
mean loss: 330.14
 ---- batch: 030 ----
mean loss: 348.37
 ---- batch: 040 ----
mean loss: 335.27
 ---- batch: 050 ----
mean loss: 338.72
 ---- batch: 060 ----
mean loss: 333.96
 ---- batch: 070 ----
mean loss: 332.97
 ---- batch: 080 ----
mean loss: 329.64
 ---- batch: 090 ----
mean loss: 327.42
train mean loss: 334.51
epoch train time: 0:00:00.661710
elapsed time: 0:01:11.381595
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 22:52:05.530956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.11
 ---- batch: 020 ----
mean loss: 338.57
 ---- batch: 030 ----
mean loss: 335.41
 ---- batch: 040 ----
mean loss: 324.28
 ---- batch: 050 ----
mean loss: 338.71
 ---- batch: 060 ----
mean loss: 329.41
 ---- batch: 070 ----
mean loss: 317.48
 ---- batch: 080 ----
mean loss: 324.92
 ---- batch: 090 ----
mean loss: 328.28
train mean loss: 328.81
epoch train time: 0:00:00.674745
elapsed time: 0:01:12.056509
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 22:52:06.205881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.00
 ---- batch: 020 ----
mean loss: 316.89
 ---- batch: 030 ----
mean loss: 326.78
 ---- batch: 040 ----
mean loss: 324.97
 ---- batch: 050 ----
mean loss: 317.41
 ---- batch: 060 ----
mean loss: 319.32
 ---- batch: 070 ----
mean loss: 325.38
 ---- batch: 080 ----
mean loss: 327.80
 ---- batch: 090 ----
mean loss: 327.84
train mean loss: 323.72
epoch train time: 0:00:00.679022
elapsed time: 0:01:12.735719
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 22:52:06.885060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.01
 ---- batch: 020 ----
mean loss: 320.98
 ---- batch: 030 ----
mean loss: 318.34
 ---- batch: 040 ----
mean loss: 318.65
 ---- batch: 050 ----
mean loss: 317.66
 ---- batch: 060 ----
mean loss: 324.39
 ---- batch: 070 ----
mean loss: 314.59
 ---- batch: 080 ----
mean loss: 317.54
 ---- batch: 090 ----
mean loss: 320.51
train mean loss: 319.48
epoch train time: 0:00:00.661164
elapsed time: 0:01:13.397022
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 22:52:07.546361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.65
 ---- batch: 020 ----
mean loss: 310.51
 ---- batch: 030 ----
mean loss: 306.54
 ---- batch: 040 ----
mean loss: 320.86
 ---- batch: 050 ----
mean loss: 310.21
 ---- batch: 060 ----
mean loss: 306.73
 ---- batch: 070 ----
mean loss: 327.25
 ---- batch: 080 ----
mean loss: 322.25
 ---- batch: 090 ----
mean loss: 321.41
train mean loss: 314.40
epoch train time: 0:00:00.670851
elapsed time: 0:01:14.068016
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 22:52:08.217360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.93
 ---- batch: 020 ----
mean loss: 305.83
 ---- batch: 030 ----
mean loss: 310.09
 ---- batch: 040 ----
mean loss: 296.44
 ---- batch: 050 ----
mean loss: 309.07
 ---- batch: 060 ----
mean loss: 317.57
 ---- batch: 070 ----
mean loss: 308.66
 ---- batch: 080 ----
mean loss: 307.02
 ---- batch: 090 ----
mean loss: 321.39
train mean loss: 310.25
epoch train time: 0:00:00.674977
elapsed time: 0:01:14.743145
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 22:52:08.892489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.39
 ---- batch: 020 ----
mean loss: 298.87
 ---- batch: 030 ----
mean loss: 309.32
 ---- batch: 040 ----
mean loss: 301.34
 ---- batch: 050 ----
mean loss: 309.92
 ---- batch: 060 ----
mean loss: 307.43
 ---- batch: 070 ----
mean loss: 306.75
 ---- batch: 080 ----
mean loss: 308.76
 ---- batch: 090 ----
mean loss: 303.94
train mean loss: 306.32
epoch train time: 0:00:00.663033
elapsed time: 0:01:15.406325
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 22:52:09.555666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.62
 ---- batch: 020 ----
mean loss: 302.67
 ---- batch: 030 ----
mean loss: 303.62
 ---- batch: 040 ----
mean loss: 302.98
 ---- batch: 050 ----
mean loss: 314.13
 ---- batch: 060 ----
mean loss: 296.82
 ---- batch: 070 ----
mean loss: 297.43
 ---- batch: 080 ----
mean loss: 300.97
 ---- batch: 090 ----
mean loss: 296.85
train mean loss: 302.27
epoch train time: 0:00:00.676123
elapsed time: 0:01:16.082610
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 22:52:10.231968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.65
 ---- batch: 020 ----
mean loss: 303.39
 ---- batch: 030 ----
mean loss: 300.18
 ---- batch: 040 ----
mean loss: 304.23
 ---- batch: 050 ----
mean loss: 305.60
 ---- batch: 060 ----
mean loss: 296.24
 ---- batch: 070 ----
mean loss: 287.07
 ---- batch: 080 ----
mean loss: 296.98
 ---- batch: 090 ----
mean loss: 296.74
train mean loss: 299.19
epoch train time: 0:00:00.679379
elapsed time: 0:01:16.762164
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 22:52:10.911507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.32
 ---- batch: 020 ----
mean loss: 297.16
 ---- batch: 030 ----
mean loss: 301.83
 ---- batch: 040 ----
mean loss: 290.02
 ---- batch: 050 ----
mean loss: 292.66
 ---- batch: 060 ----
mean loss: 300.98
 ---- batch: 070 ----
mean loss: 288.16
 ---- batch: 080 ----
mean loss: 296.23
 ---- batch: 090 ----
mean loss: 296.48
train mean loss: 294.75
epoch train time: 0:00:00.675503
elapsed time: 0:01:17.437816
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 22:52:11.587175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.24
 ---- batch: 020 ----
mean loss: 287.07
 ---- batch: 030 ----
mean loss: 303.45
 ---- batch: 040 ----
mean loss: 302.21
 ---- batch: 050 ----
mean loss: 296.23
 ---- batch: 060 ----
mean loss: 289.42
 ---- batch: 070 ----
mean loss: 286.61
 ---- batch: 080 ----
mean loss: 287.03
 ---- batch: 090 ----
mean loss: 288.14
train mean loss: 292.06
epoch train time: 0:00:00.843763
elapsed time: 0:01:18.281769
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 22:52:12.431173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.18
 ---- batch: 020 ----
mean loss: 288.18
 ---- batch: 030 ----
mean loss: 279.54
 ---- batch: 040 ----
mean loss: 279.52
 ---- batch: 050 ----
mean loss: 291.38
 ---- batch: 060 ----
mean loss: 296.08
 ---- batch: 070 ----
mean loss: 299.16
 ---- batch: 080 ----
mean loss: 294.15
 ---- batch: 090 ----
mean loss: 281.32
train mean loss: 288.66
epoch train time: 0:00:00.679821
elapsed time: 0:01:18.961801
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 22:52:13.111144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.71
 ---- batch: 020 ----
mean loss: 291.45
 ---- batch: 030 ----
mean loss: 282.26
 ---- batch: 040 ----
mean loss: 283.10
 ---- batch: 050 ----
mean loss: 282.67
 ---- batch: 060 ----
mean loss: 285.15
 ---- batch: 070 ----
mean loss: 289.25
 ---- batch: 080 ----
mean loss: 279.78
 ---- batch: 090 ----
mean loss: 288.67
train mean loss: 286.23
epoch train time: 0:00:00.667195
elapsed time: 0:01:19.629156
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 22:52:13.778502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.66
 ---- batch: 020 ----
mean loss: 276.78
 ---- batch: 030 ----
mean loss: 280.89
 ---- batch: 040 ----
mean loss: 285.79
 ---- batch: 050 ----
mean loss: 281.54
 ---- batch: 060 ----
mean loss: 274.78
 ---- batch: 070 ----
mean loss: 272.50
 ---- batch: 080 ----
mean loss: 296.34
 ---- batch: 090 ----
mean loss: 289.53
train mean loss: 283.56
epoch train time: 0:00:00.677178
elapsed time: 0:01:20.306484
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 22:52:14.455837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.78
 ---- batch: 020 ----
mean loss: 278.39
 ---- batch: 030 ----
mean loss: 288.57
 ---- batch: 040 ----
mean loss: 290.98
 ---- batch: 050 ----
mean loss: 285.49
 ---- batch: 060 ----
mean loss: 275.05
 ---- batch: 070 ----
mean loss: 281.40
 ---- batch: 080 ----
mean loss: 273.42
 ---- batch: 090 ----
mean loss: 283.27
train mean loss: 280.97
epoch train time: 0:00:00.673960
elapsed time: 0:01:20.980610
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 22:52:15.129955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.88
 ---- batch: 020 ----
mean loss: 272.20
 ---- batch: 030 ----
mean loss: 286.65
 ---- batch: 040 ----
mean loss: 278.13
 ---- batch: 050 ----
mean loss: 285.18
 ---- batch: 060 ----
mean loss: 286.57
 ---- batch: 070 ----
mean loss: 269.81
 ---- batch: 080 ----
mean loss: 276.87
 ---- batch: 090 ----
mean loss: 277.01
train mean loss: 278.68
epoch train time: 0:00:00.672058
elapsed time: 0:01:21.652820
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 22:52:15.802167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.81
 ---- batch: 020 ----
mean loss: 266.82
 ---- batch: 030 ----
mean loss: 272.13
 ---- batch: 040 ----
mean loss: 281.17
 ---- batch: 050 ----
mean loss: 279.70
 ---- batch: 060 ----
mean loss: 278.69
 ---- batch: 070 ----
mean loss: 277.99
 ---- batch: 080 ----
mean loss: 280.73
 ---- batch: 090 ----
mean loss: 277.72
train mean loss: 276.48
epoch train time: 0:00:00.682369
elapsed time: 0:01:22.335340
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 22:52:16.484681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.89
 ---- batch: 020 ----
mean loss: 274.54
 ---- batch: 030 ----
mean loss: 285.13
 ---- batch: 040 ----
mean loss: 271.07
 ---- batch: 050 ----
mean loss: 272.41
 ---- batch: 060 ----
mean loss: 274.49
 ---- batch: 070 ----
mean loss: 272.72
 ---- batch: 080 ----
mean loss: 280.43
 ---- batch: 090 ----
mean loss: 272.78
train mean loss: 274.24
epoch train time: 0:00:00.676599
elapsed time: 0:01:23.012084
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 22:52:17.161444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.92
 ---- batch: 020 ----
mean loss: 273.13
 ---- batch: 030 ----
mean loss: 282.67
 ---- batch: 040 ----
mean loss: 269.75
 ---- batch: 050 ----
mean loss: 268.84
 ---- batch: 060 ----
mean loss: 273.80
 ---- batch: 070 ----
mean loss: 271.44
 ---- batch: 080 ----
mean loss: 270.57
 ---- batch: 090 ----
mean loss: 271.48
train mean loss: 272.58
epoch train time: 0:00:00.679786
elapsed time: 0:01:23.692082
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 22:52:17.841451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.13
 ---- batch: 020 ----
mean loss: 274.44
 ---- batch: 030 ----
mean loss: 271.17
 ---- batch: 040 ----
mean loss: 272.94
 ---- batch: 050 ----
mean loss: 262.96
 ---- batch: 060 ----
mean loss: 272.73
 ---- batch: 070 ----
mean loss: 277.41
 ---- batch: 080 ----
mean loss: 273.86
 ---- batch: 090 ----
mean loss: 274.81
train mean loss: 271.34
epoch train time: 0:00:00.675057
elapsed time: 0:01:24.367314
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 22:52:18.516657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.43
 ---- batch: 020 ----
mean loss: 268.43
 ---- batch: 030 ----
mean loss: 253.53
 ---- batch: 040 ----
mean loss: 272.00
 ---- batch: 050 ----
mean loss: 276.51
 ---- batch: 060 ----
mean loss: 262.95
 ---- batch: 070 ----
mean loss: 272.42
 ---- batch: 080 ----
mean loss: 275.48
 ---- batch: 090 ----
mean loss: 267.27
train mean loss: 269.66
epoch train time: 0:00:00.673826
elapsed time: 0:01:25.041305
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 22:52:19.190650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.06
 ---- batch: 020 ----
mean loss: 273.00
 ---- batch: 030 ----
mean loss: 261.02
 ---- batch: 040 ----
mean loss: 280.38
 ---- batch: 050 ----
mean loss: 263.40
 ---- batch: 060 ----
mean loss: 265.51
 ---- batch: 070 ----
mean loss: 268.43
 ---- batch: 080 ----
mean loss: 269.49
 ---- batch: 090 ----
mean loss: 267.62
train mean loss: 268.04
epoch train time: 0:00:00.672328
elapsed time: 0:01:25.713787
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 22:52:19.863128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.66
 ---- batch: 020 ----
mean loss: 264.29
 ---- batch: 030 ----
mean loss: 263.66
 ---- batch: 040 ----
mean loss: 279.89
 ---- batch: 050 ----
mean loss: 268.65
 ---- batch: 060 ----
mean loss: 259.90
 ---- batch: 070 ----
mean loss: 279.93
 ---- batch: 080 ----
mean loss: 261.88
 ---- batch: 090 ----
mean loss: 269.44
train mean loss: 266.95
epoch train time: 0:00:00.672680
elapsed time: 0:01:26.386610
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 22:52:20.535953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.49
 ---- batch: 020 ----
mean loss: 264.33
 ---- batch: 030 ----
mean loss: 265.73
 ---- batch: 040 ----
mean loss: 272.52
 ---- batch: 050 ----
mean loss: 260.49
 ---- batch: 060 ----
mean loss: 276.02
 ---- batch: 070 ----
mean loss: 253.54
 ---- batch: 080 ----
mean loss: 266.72
 ---- batch: 090 ----
mean loss: 266.50
train mean loss: 265.28
epoch train time: 0:00:00.674888
elapsed time: 0:01:27.061707
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 22:52:21.211067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.10
 ---- batch: 020 ----
mean loss: 262.09
 ---- batch: 030 ----
mean loss: 263.39
 ---- batch: 040 ----
mean loss: 262.53
 ---- batch: 050 ----
mean loss: 263.53
 ---- batch: 060 ----
mean loss: 262.46
 ---- batch: 070 ----
mean loss: 262.28
 ---- batch: 080 ----
mean loss: 266.46
 ---- batch: 090 ----
mean loss: 267.05
train mean loss: 264.21
epoch train time: 0:00:00.671971
elapsed time: 0:01:27.733843
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 22:52:21.883201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.34
 ---- batch: 020 ----
mean loss: 269.33
 ---- batch: 030 ----
mean loss: 269.49
 ---- batch: 040 ----
mean loss: 260.03
 ---- batch: 050 ----
mean loss: 273.41
 ---- batch: 060 ----
mean loss: 257.84
 ---- batch: 070 ----
mean loss: 261.97
 ---- batch: 080 ----
mean loss: 251.29
 ---- batch: 090 ----
mean loss: 260.46
train mean loss: 262.59
epoch train time: 0:00:00.671142
elapsed time: 0:01:28.405201
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 22:52:22.554534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.08
 ---- batch: 020 ----
mean loss: 258.35
 ---- batch: 030 ----
mean loss: 256.36
 ---- batch: 040 ----
mean loss: 259.61
 ---- batch: 050 ----
mean loss: 262.44
 ---- batch: 060 ----
mean loss: 269.79
 ---- batch: 070 ----
mean loss: 255.68
 ---- batch: 080 ----
mean loss: 270.04
 ---- batch: 090 ----
mean loss: 260.88
train mean loss: 261.57
epoch train time: 0:00:00.675211
elapsed time: 0:01:29.080550
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 22:52:23.229892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.75
 ---- batch: 020 ----
mean loss: 252.72
 ---- batch: 030 ----
mean loss: 263.13
 ---- batch: 040 ----
mean loss: 259.81
 ---- batch: 050 ----
mean loss: 263.80
 ---- batch: 060 ----
mean loss: 261.49
 ---- batch: 070 ----
mean loss: 259.34
 ---- batch: 080 ----
mean loss: 258.88
 ---- batch: 090 ----
mean loss: 259.59
train mean loss: 260.85
epoch train time: 0:00:00.673174
elapsed time: 0:01:29.753872
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 22:52:23.903214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.07
 ---- batch: 020 ----
mean loss: 256.67
 ---- batch: 030 ----
mean loss: 261.35
 ---- batch: 040 ----
mean loss: 248.29
 ---- batch: 050 ----
mean loss: 265.55
 ---- batch: 060 ----
mean loss: 262.54
 ---- batch: 070 ----
mean loss: 256.81
 ---- batch: 080 ----
mean loss: 258.83
 ---- batch: 090 ----
mean loss: 268.19
train mean loss: 259.41
epoch train time: 0:00:00.671899
elapsed time: 0:01:30.425963
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 22:52:24.575306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.53
 ---- batch: 020 ----
mean loss: 265.17
 ---- batch: 030 ----
mean loss: 254.83
 ---- batch: 040 ----
mean loss: 263.03
 ---- batch: 050 ----
mean loss: 258.10
 ---- batch: 060 ----
mean loss: 249.23
 ---- batch: 070 ----
mean loss: 247.60
 ---- batch: 080 ----
mean loss: 253.36
 ---- batch: 090 ----
mean loss: 271.84
train mean loss: 259.24
epoch train time: 0:00:00.669981
elapsed time: 0:01:31.096094
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 22:52:25.245437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.75
 ---- batch: 020 ----
mean loss: 257.80
 ---- batch: 030 ----
mean loss: 258.59
 ---- batch: 040 ----
mean loss: 263.36
 ---- batch: 050 ----
mean loss: 256.30
 ---- batch: 060 ----
mean loss: 258.81
 ---- batch: 070 ----
mean loss: 255.51
 ---- batch: 080 ----
mean loss: 258.29
 ---- batch: 090 ----
mean loss: 250.66
train mean loss: 257.89
epoch train time: 0:00:00.666552
elapsed time: 0:01:31.762792
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 22:52:25.912141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.15
 ---- batch: 020 ----
mean loss: 259.44
 ---- batch: 030 ----
mean loss: 262.04
 ---- batch: 040 ----
mean loss: 262.10
 ---- batch: 050 ----
mean loss: 245.67
 ---- batch: 060 ----
mean loss: 262.83
 ---- batch: 070 ----
mean loss: 254.18
 ---- batch: 080 ----
mean loss: 262.85
 ---- batch: 090 ----
mean loss: 253.59
train mean loss: 257.17
epoch train time: 0:00:00.673399
elapsed time: 0:01:32.436363
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 22:52:26.585707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.19
 ---- batch: 020 ----
mean loss: 259.27
 ---- batch: 030 ----
mean loss: 257.72
 ---- batch: 040 ----
mean loss: 253.46
 ---- batch: 050 ----
mean loss: 252.87
 ---- batch: 060 ----
mean loss: 262.38
 ---- batch: 070 ----
mean loss: 248.32
 ---- batch: 080 ----
mean loss: 254.15
 ---- batch: 090 ----
mean loss: 252.74
train mean loss: 256.40
epoch train time: 0:00:00.671251
elapsed time: 0:01:33.107765
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 22:52:27.257127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.89
 ---- batch: 020 ----
mean loss: 261.99
 ---- batch: 030 ----
mean loss: 258.23
 ---- batch: 040 ----
mean loss: 254.38
 ---- batch: 050 ----
mean loss: 258.39
 ---- batch: 060 ----
mean loss: 252.44
 ---- batch: 070 ----
mean loss: 251.81
 ---- batch: 080 ----
mean loss: 254.39
 ---- batch: 090 ----
mean loss: 258.09
train mean loss: 255.66
epoch train time: 0:00:00.679363
elapsed time: 0:01:33.787292
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 22:52:27.936631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.54
 ---- batch: 020 ----
mean loss: 253.15
 ---- batch: 030 ----
mean loss: 243.33
 ---- batch: 040 ----
mean loss: 257.84
 ---- batch: 050 ----
mean loss: 261.18
 ---- batch: 060 ----
mean loss: 268.15
 ---- batch: 070 ----
mean loss: 255.00
 ---- batch: 080 ----
mean loss: 253.13
 ---- batch: 090 ----
mean loss: 251.46
train mean loss: 254.78
epoch train time: 0:00:00.666689
elapsed time: 0:01:34.454135
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 22:52:28.603481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.63
 ---- batch: 020 ----
mean loss: 257.35
 ---- batch: 030 ----
mean loss: 253.14
 ---- batch: 040 ----
mean loss: 251.25
 ---- batch: 050 ----
mean loss: 260.74
 ---- batch: 060 ----
mean loss: 264.53
 ---- batch: 070 ----
mean loss: 251.90
 ---- batch: 080 ----
mean loss: 252.69
 ---- batch: 090 ----
mean loss: 248.96
train mean loss: 254.15
epoch train time: 0:00:00.679636
elapsed time: 0:01:35.133977
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 22:52:29.283321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.32
 ---- batch: 020 ----
mean loss: 259.07
 ---- batch: 030 ----
mean loss: 250.37
 ---- batch: 040 ----
mean loss: 254.86
 ---- batch: 050 ----
mean loss: 253.15
 ---- batch: 060 ----
mean loss: 255.21
 ---- batch: 070 ----
mean loss: 250.83
 ---- batch: 080 ----
mean loss: 256.05
 ---- batch: 090 ----
mean loss: 256.64
train mean loss: 253.93
epoch train time: 0:00:00.673586
elapsed time: 0:01:35.807712
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 22:52:29.957056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.06
 ---- batch: 020 ----
mean loss: 259.88
 ---- batch: 030 ----
mean loss: 254.85
 ---- batch: 040 ----
mean loss: 256.73
 ---- batch: 050 ----
mean loss: 250.18
 ---- batch: 060 ----
mean loss: 256.40
 ---- batch: 070 ----
mean loss: 259.60
 ---- batch: 080 ----
mean loss: 258.31
 ---- batch: 090 ----
mean loss: 242.28
train mean loss: 253.63
epoch train time: 0:00:00.672435
elapsed time: 0:01:36.480296
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 22:52:30.629664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.36
 ---- batch: 020 ----
mean loss: 250.31
 ---- batch: 030 ----
mean loss: 262.14
 ---- batch: 040 ----
mean loss: 243.51
 ---- batch: 050 ----
mean loss: 247.71
 ---- batch: 060 ----
mean loss: 260.38
 ---- batch: 070 ----
mean loss: 252.42
 ---- batch: 080 ----
mean loss: 249.18
 ---- batch: 090 ----
mean loss: 256.70
train mean loss: 252.42
epoch train time: 0:00:00.672050
elapsed time: 0:01:37.152528
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 22:52:31.301889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.41
 ---- batch: 020 ----
mean loss: 246.57
 ---- batch: 030 ----
mean loss: 252.57
 ---- batch: 040 ----
mean loss: 254.62
 ---- batch: 050 ----
mean loss: 251.05
 ---- batch: 060 ----
mean loss: 251.88
 ---- batch: 070 ----
mean loss: 246.53
 ---- batch: 080 ----
mean loss: 250.74
 ---- batch: 090 ----
mean loss: 253.08
train mean loss: 251.83
epoch train time: 0:00:00.670466
elapsed time: 0:01:37.823156
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 22:52:31.972515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.14
 ---- batch: 020 ----
mean loss: 246.97
 ---- batch: 030 ----
mean loss: 251.59
 ---- batch: 040 ----
mean loss: 251.80
 ---- batch: 050 ----
mean loss: 250.26
 ---- batch: 060 ----
mean loss: 251.98
 ---- batch: 070 ----
mean loss: 255.91
 ---- batch: 080 ----
mean loss: 246.25
 ---- batch: 090 ----
mean loss: 255.41
train mean loss: 251.74
epoch train time: 0:00:00.666420
elapsed time: 0:01:38.489771
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 22:52:32.639111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.18
 ---- batch: 020 ----
mean loss: 247.93
 ---- batch: 030 ----
mean loss: 248.40
 ---- batch: 040 ----
mean loss: 252.62
 ---- batch: 050 ----
mean loss: 250.83
 ---- batch: 060 ----
mean loss: 244.98
 ---- batch: 070 ----
mean loss: 251.92
 ---- batch: 080 ----
mean loss: 255.16
 ---- batch: 090 ----
mean loss: 259.13
train mean loss: 250.85
epoch train time: 0:00:00.677156
elapsed time: 0:01:39.167072
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 22:52:33.316422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.58
 ---- batch: 020 ----
mean loss: 256.20
 ---- batch: 030 ----
mean loss: 246.53
 ---- batch: 040 ----
mean loss: 252.21
 ---- batch: 050 ----
mean loss: 244.82
 ---- batch: 060 ----
mean loss: 252.14
 ---- batch: 070 ----
mean loss: 257.78
 ---- batch: 080 ----
mean loss: 251.83
 ---- batch: 090 ----
mean loss: 246.14
train mean loss: 250.50
epoch train time: 0:00:00.673306
elapsed time: 0:01:39.840538
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 22:52:33.989884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.48
 ---- batch: 020 ----
mean loss: 242.30
 ---- batch: 030 ----
mean loss: 245.80
 ---- batch: 040 ----
mean loss: 248.71
 ---- batch: 050 ----
mean loss: 254.85
 ---- batch: 060 ----
mean loss: 252.24
 ---- batch: 070 ----
mean loss: 247.74
 ---- batch: 080 ----
mean loss: 250.71
 ---- batch: 090 ----
mean loss: 251.54
train mean loss: 250.13
epoch train time: 0:00:00.672113
elapsed time: 0:01:40.512800
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 22:52:34.662202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.30
 ---- batch: 020 ----
mean loss: 246.82
 ---- batch: 030 ----
mean loss: 247.19
 ---- batch: 040 ----
mean loss: 248.70
 ---- batch: 050 ----
mean loss: 261.91
 ---- batch: 060 ----
mean loss: 243.92
 ---- batch: 070 ----
mean loss: 241.05
 ---- batch: 080 ----
mean loss: 248.30
 ---- batch: 090 ----
mean loss: 247.70
train mean loss: 249.59
epoch train time: 0:00:00.674839
elapsed time: 0:01:41.187847
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 22:52:35.337189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.82
 ---- batch: 020 ----
mean loss: 250.86
 ---- batch: 030 ----
mean loss: 248.41
 ---- batch: 040 ----
mean loss: 248.35
 ---- batch: 050 ----
mean loss: 252.46
 ---- batch: 060 ----
mean loss: 254.26
 ---- batch: 070 ----
mean loss: 246.02
 ---- batch: 080 ----
mean loss: 243.24
 ---- batch: 090 ----
mean loss: 247.51
train mean loss: 249.09
epoch train time: 0:00:00.677712
elapsed time: 0:01:41.865703
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 22:52:36.015047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.09
 ---- batch: 020 ----
mean loss: 243.23
 ---- batch: 030 ----
mean loss: 246.04
 ---- batch: 040 ----
mean loss: 249.51
 ---- batch: 050 ----
mean loss: 244.48
 ---- batch: 060 ----
mean loss: 253.53
 ---- batch: 070 ----
mean loss: 248.07
 ---- batch: 080 ----
mean loss: 250.24
 ---- batch: 090 ----
mean loss: 251.95
train mean loss: 248.61
epoch train time: 0:00:00.675328
elapsed time: 0:01:42.541184
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 22:52:36.690524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.89
 ---- batch: 020 ----
mean loss: 250.80
 ---- batch: 030 ----
mean loss: 242.22
 ---- batch: 040 ----
mean loss: 249.31
 ---- batch: 050 ----
mean loss: 242.59
 ---- batch: 060 ----
mean loss: 249.86
 ---- batch: 070 ----
mean loss: 254.25
 ---- batch: 080 ----
mean loss: 242.46
 ---- batch: 090 ----
mean loss: 248.35
train mean loss: 248.33
epoch train time: 0:00:00.671793
elapsed time: 0:01:43.213122
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 22:52:37.362481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.15
 ---- batch: 020 ----
mean loss: 250.44
 ---- batch: 030 ----
mean loss: 244.32
 ---- batch: 040 ----
mean loss: 246.90
 ---- batch: 050 ----
mean loss: 247.67
 ---- batch: 060 ----
mean loss: 254.77
 ---- batch: 070 ----
mean loss: 237.75
 ---- batch: 080 ----
mean loss: 246.74
 ---- batch: 090 ----
mean loss: 243.26
train mean loss: 247.75
epoch train time: 0:00:00.673879
elapsed time: 0:01:43.887172
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 22:52:38.036517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.82
 ---- batch: 020 ----
mean loss: 244.57
 ---- batch: 030 ----
mean loss: 254.30
 ---- batch: 040 ----
mean loss: 245.51
 ---- batch: 050 ----
mean loss: 239.59
 ---- batch: 060 ----
mean loss: 251.04
 ---- batch: 070 ----
mean loss: 244.97
 ---- batch: 080 ----
mean loss: 248.56
 ---- batch: 090 ----
mean loss: 253.06
train mean loss: 247.16
epoch train time: 0:00:00.663963
elapsed time: 0:01:44.551323
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 22:52:38.700683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.64
 ---- batch: 020 ----
mean loss: 250.64
 ---- batch: 030 ----
mean loss: 249.01
 ---- batch: 040 ----
mean loss: 238.60
 ---- batch: 050 ----
mean loss: 249.92
 ---- batch: 060 ----
mean loss: 248.01
 ---- batch: 070 ----
mean loss: 243.43
 ---- batch: 080 ----
mean loss: 245.26
 ---- batch: 090 ----
mean loss: 244.46
train mean loss: 246.67
epoch train time: 0:00:00.677660
elapsed time: 0:01:45.229168
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 22:52:39.378499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.99
 ---- batch: 020 ----
mean loss: 246.39
 ---- batch: 030 ----
mean loss: 245.65
 ---- batch: 040 ----
mean loss: 249.43
 ---- batch: 050 ----
mean loss: 241.67
 ---- batch: 060 ----
mean loss: 248.20
 ---- batch: 070 ----
mean loss: 249.04
 ---- batch: 080 ----
mean loss: 252.21
 ---- batch: 090 ----
mean loss: 245.22
train mean loss: 246.00
epoch train time: 0:00:00.683847
elapsed time: 0:01:45.913155
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 22:52:40.062506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.03
 ---- batch: 020 ----
mean loss: 246.18
 ---- batch: 030 ----
mean loss: 241.01
 ---- batch: 040 ----
mean loss: 246.03
 ---- batch: 050 ----
mean loss: 238.26
 ---- batch: 060 ----
mean loss: 249.75
 ---- batch: 070 ----
mean loss: 246.75
 ---- batch: 080 ----
mean loss: 246.42
 ---- batch: 090 ----
mean loss: 245.54
train mean loss: 245.66
epoch train time: 0:00:00.676731
elapsed time: 0:01:46.590118
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 22:52:40.739460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 251.17
 ---- batch: 020 ----
mean loss: 243.75
 ---- batch: 030 ----
mean loss: 246.33
 ---- batch: 040 ----
mean loss: 239.51
 ---- batch: 050 ----
mean loss: 246.28
 ---- batch: 060 ----
mean loss: 250.52
 ---- batch: 070 ----
mean loss: 250.41
 ---- batch: 080 ----
mean loss: 244.20
 ---- batch: 090 ----
mean loss: 237.66
train mean loss: 246.14
epoch train time: 0:00:00.679566
elapsed time: 0:01:47.269822
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 22:52:41.419162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.27
 ---- batch: 020 ----
mean loss: 245.24
 ---- batch: 030 ----
mean loss: 250.62
 ---- batch: 040 ----
mean loss: 241.97
 ---- batch: 050 ----
mean loss: 242.73
 ---- batch: 060 ----
mean loss: 252.24
 ---- batch: 070 ----
mean loss: 242.37
 ---- batch: 080 ----
mean loss: 241.49
 ---- batch: 090 ----
mean loss: 243.75
train mean loss: 245.21
epoch train time: 0:00:00.670336
elapsed time: 0:01:47.940297
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 22:52:42.089691
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.46
 ---- batch: 020 ----
mean loss: 244.68
 ---- batch: 030 ----
mean loss: 247.74
 ---- batch: 040 ----
mean loss: 251.68
 ---- batch: 050 ----
mean loss: 248.80
 ---- batch: 060 ----
mean loss: 248.93
 ---- batch: 070 ----
mean loss: 250.45
 ---- batch: 080 ----
mean loss: 241.00
 ---- batch: 090 ----
mean loss: 236.99
train mean loss: 244.71
epoch train time: 0:00:00.673059
elapsed time: 0:01:48.613586
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 22:52:42.762971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.57
 ---- batch: 020 ----
mean loss: 245.91
 ---- batch: 030 ----
mean loss: 237.62
 ---- batch: 040 ----
mean loss: 240.65
 ---- batch: 050 ----
mean loss: 238.97
 ---- batch: 060 ----
mean loss: 250.50
 ---- batch: 070 ----
mean loss: 258.00
 ---- batch: 080 ----
mean loss: 245.49
 ---- batch: 090 ----
mean loss: 244.08
train mean loss: 244.82
epoch train time: 0:00:00.668473
elapsed time: 0:01:49.282242
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 22:52:43.431601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.62
 ---- batch: 020 ----
mean loss: 237.14
 ---- batch: 030 ----
mean loss: 243.35
 ---- batch: 040 ----
mean loss: 247.95
 ---- batch: 050 ----
mean loss: 241.58
 ---- batch: 060 ----
mean loss: 249.69
 ---- batch: 070 ----
mean loss: 236.48
 ---- batch: 080 ----
mean loss: 248.41
 ---- batch: 090 ----
mean loss: 244.90
train mean loss: 244.36
epoch train time: 0:00:00.678095
elapsed time: 0:01:49.960515
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 22:52:44.109857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.37
 ---- batch: 020 ----
mean loss: 244.81
 ---- batch: 030 ----
mean loss: 238.39
 ---- batch: 040 ----
mean loss: 243.28
 ---- batch: 050 ----
mean loss: 238.42
 ---- batch: 060 ----
mean loss: 246.19
 ---- batch: 070 ----
mean loss: 245.76
 ---- batch: 080 ----
mean loss: 235.53
 ---- batch: 090 ----
mean loss: 245.11
train mean loss: 243.49
epoch train time: 0:00:00.677866
elapsed time: 0:01:50.638530
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 22:52:44.787874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.99
 ---- batch: 020 ----
mean loss: 244.42
 ---- batch: 030 ----
mean loss: 238.40
 ---- batch: 040 ----
mean loss: 232.64
 ---- batch: 050 ----
mean loss: 255.00
 ---- batch: 060 ----
mean loss: 243.56
 ---- batch: 070 ----
mean loss: 252.70
 ---- batch: 080 ----
mean loss: 238.87
 ---- batch: 090 ----
mean loss: 238.11
train mean loss: 243.55
epoch train time: 0:00:00.672872
elapsed time: 0:01:51.311546
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 22:52:45.460907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.60
 ---- batch: 020 ----
mean loss: 232.17
 ---- batch: 030 ----
mean loss: 244.24
 ---- batch: 040 ----
mean loss: 240.45
 ---- batch: 050 ----
mean loss: 242.43
 ---- batch: 060 ----
mean loss: 246.41
 ---- batch: 070 ----
mean loss: 254.23
 ---- batch: 080 ----
mean loss: 252.96
 ---- batch: 090 ----
mean loss: 250.01
train mean loss: 242.49
epoch train time: 0:00:00.674121
elapsed time: 0:01:51.985829
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 22:52:46.135220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.89
 ---- batch: 020 ----
mean loss: 236.63
 ---- batch: 030 ----
mean loss: 243.11
 ---- batch: 040 ----
mean loss: 241.69
 ---- batch: 050 ----
mean loss: 241.37
 ---- batch: 060 ----
mean loss: 237.00
 ---- batch: 070 ----
mean loss: 246.65
 ---- batch: 080 ----
mean loss: 245.73
 ---- batch: 090 ----
mean loss: 249.20
train mean loss: 242.75
epoch train time: 0:00:00.685499
elapsed time: 0:01:52.671538
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 22:52:46.820906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.94
 ---- batch: 020 ----
mean loss: 244.84
 ---- batch: 030 ----
mean loss: 244.21
 ---- batch: 040 ----
mean loss: 248.98
 ---- batch: 050 ----
mean loss: 247.43
 ---- batch: 060 ----
mean loss: 242.49
 ---- batch: 070 ----
mean loss: 241.15
 ---- batch: 080 ----
mean loss: 246.11
 ---- batch: 090 ----
mean loss: 228.96
train mean loss: 242.53
epoch train time: 0:00:00.667170
elapsed time: 0:01:53.338879
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 22:52:47.488237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.74
 ---- batch: 020 ----
mean loss: 241.16
 ---- batch: 030 ----
mean loss: 238.56
 ---- batch: 040 ----
mean loss: 240.95
 ---- batch: 050 ----
mean loss: 233.37
 ---- batch: 060 ----
mean loss: 244.85
 ---- batch: 070 ----
mean loss: 242.40
 ---- batch: 080 ----
mean loss: 249.74
 ---- batch: 090 ----
mean loss: 241.22
train mean loss: 242.12
epoch train time: 0:00:00.676446
elapsed time: 0:01:54.015487
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 22:52:48.164843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.59
 ---- batch: 020 ----
mean loss: 241.14
 ---- batch: 030 ----
mean loss: 240.51
 ---- batch: 040 ----
mean loss: 248.06
 ---- batch: 050 ----
mean loss: 240.05
 ---- batch: 060 ----
mean loss: 243.57
 ---- batch: 070 ----
mean loss: 237.10
 ---- batch: 080 ----
mean loss: 245.86
 ---- batch: 090 ----
mean loss: 241.73
train mean loss: 241.52
epoch train time: 0:00:00.676302
elapsed time: 0:01:54.691959
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 22:52:48.841304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.30
 ---- batch: 020 ----
mean loss: 243.05
 ---- batch: 030 ----
mean loss: 237.76
 ---- batch: 040 ----
mean loss: 241.93
 ---- batch: 050 ----
mean loss: 241.86
 ---- batch: 060 ----
mean loss: 241.54
 ---- batch: 070 ----
mean loss: 243.57
 ---- batch: 080 ----
mean loss: 248.48
 ---- batch: 090 ----
mean loss: 242.60
train mean loss: 241.17
epoch train time: 0:00:00.674625
elapsed time: 0:01:55.366734
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 22:52:49.516077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.42
 ---- batch: 020 ----
mean loss: 244.74
 ---- batch: 030 ----
mean loss: 236.22
 ---- batch: 040 ----
mean loss: 246.70
 ---- batch: 050 ----
mean loss: 243.84
 ---- batch: 060 ----
mean loss: 240.87
 ---- batch: 070 ----
mean loss: 231.08
 ---- batch: 080 ----
mean loss: 242.04
 ---- batch: 090 ----
mean loss: 237.63
train mean loss: 240.92
epoch train time: 0:00:00.674382
elapsed time: 0:01:56.041264
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 22:52:50.190606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.58
 ---- batch: 020 ----
mean loss: 233.12
 ---- batch: 030 ----
mean loss: 236.91
 ---- batch: 040 ----
mean loss: 243.39
 ---- batch: 050 ----
mean loss: 239.99
 ---- batch: 060 ----
mean loss: 241.19
 ---- batch: 070 ----
mean loss: 241.23
 ---- batch: 080 ----
mean loss: 244.63
 ---- batch: 090 ----
mean loss: 239.82
train mean loss: 240.67
epoch train time: 0:00:00.679505
elapsed time: 0:01:56.720927
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 22:52:50.870280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.02
 ---- batch: 020 ----
mean loss: 234.22
 ---- batch: 030 ----
mean loss: 236.62
 ---- batch: 040 ----
mean loss: 244.28
 ---- batch: 050 ----
mean loss: 239.38
 ---- batch: 060 ----
mean loss: 249.67
 ---- batch: 070 ----
mean loss: 239.67
 ---- batch: 080 ----
mean loss: 241.57
 ---- batch: 090 ----
mean loss: 245.59
train mean loss: 240.20
epoch train time: 0:00:00.670087
elapsed time: 0:01:57.391199
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 22:52:51.540542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.49
 ---- batch: 020 ----
mean loss: 238.85
 ---- batch: 030 ----
mean loss: 228.64
 ---- batch: 040 ----
mean loss: 233.94
 ---- batch: 050 ----
mean loss: 244.32
 ---- batch: 060 ----
mean loss: 241.43
 ---- batch: 070 ----
mean loss: 239.88
 ---- batch: 080 ----
mean loss: 247.66
 ---- batch: 090 ----
mean loss: 246.54
train mean loss: 239.89
epoch train time: 0:00:00.670637
elapsed time: 0:01:58.062004
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 22:52:52.211382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.49
 ---- batch: 020 ----
mean loss: 235.89
 ---- batch: 030 ----
mean loss: 244.16
 ---- batch: 040 ----
mean loss: 239.89
 ---- batch: 050 ----
mean loss: 240.78
 ---- batch: 060 ----
mean loss: 236.94
 ---- batch: 070 ----
mean loss: 238.63
 ---- batch: 080 ----
mean loss: 244.91
 ---- batch: 090 ----
mean loss: 247.96
train mean loss: 239.97
epoch train time: 0:00:00.671575
elapsed time: 0:01:58.733792
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 22:52:52.883131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.06
 ---- batch: 020 ----
mean loss: 233.31
 ---- batch: 030 ----
mean loss: 236.94
 ---- batch: 040 ----
mean loss: 244.60
 ---- batch: 050 ----
mean loss: 241.31
 ---- batch: 060 ----
mean loss: 242.66
 ---- batch: 070 ----
mean loss: 229.25
 ---- batch: 080 ----
mean loss: 238.45
 ---- batch: 090 ----
mean loss: 248.05
train mean loss: 239.72
epoch train time: 0:00:00.667204
elapsed time: 0:01:59.401138
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 22:52:53.550479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.82
 ---- batch: 020 ----
mean loss: 229.95
 ---- batch: 030 ----
mean loss: 236.58
 ---- batch: 040 ----
mean loss: 236.53
 ---- batch: 050 ----
mean loss: 236.62
 ---- batch: 060 ----
mean loss: 237.23
 ---- batch: 070 ----
mean loss: 244.46
 ---- batch: 080 ----
mean loss: 243.20
 ---- batch: 090 ----
mean loss: 247.99
train mean loss: 239.10
epoch train time: 0:00:00.670905
elapsed time: 0:02:00.072203
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 22:52:54.221544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.16
 ---- batch: 020 ----
mean loss: 237.65
 ---- batch: 030 ----
mean loss: 233.55
 ---- batch: 040 ----
mean loss: 237.58
 ---- batch: 050 ----
mean loss: 233.80
 ---- batch: 060 ----
mean loss: 233.01
 ---- batch: 070 ----
mean loss: 242.84
 ---- batch: 080 ----
mean loss: 248.22
 ---- batch: 090 ----
mean loss: 241.47
train mean loss: 239.04
epoch train time: 0:00:00.672223
elapsed time: 0:02:00.744572
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 22:52:54.893921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.83
 ---- batch: 020 ----
mean loss: 240.00
 ---- batch: 030 ----
mean loss: 238.99
 ---- batch: 040 ----
mean loss: 244.57
 ---- batch: 050 ----
mean loss: 237.79
 ---- batch: 060 ----
mean loss: 235.95
 ---- batch: 070 ----
mean loss: 238.44
 ---- batch: 080 ----
mean loss: 237.07
 ---- batch: 090 ----
mean loss: 237.64
train mean loss: 238.72
epoch train time: 0:00:00.671470
elapsed time: 0:02:01.416210
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 22:52:55.565554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.80
 ---- batch: 020 ----
mean loss: 234.64
 ---- batch: 030 ----
mean loss: 235.23
 ---- batch: 040 ----
mean loss: 239.23
 ---- batch: 050 ----
mean loss: 238.58
 ---- batch: 060 ----
mean loss: 234.46
 ---- batch: 070 ----
mean loss: 239.49
 ---- batch: 080 ----
mean loss: 248.60
 ---- batch: 090 ----
mean loss: 236.39
train mean loss: 238.71
epoch train time: 0:00:00.670018
elapsed time: 0:02:02.086371
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 22:52:56.235709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.80
 ---- batch: 020 ----
mean loss: 235.73
 ---- batch: 030 ----
mean loss: 233.35
 ---- batch: 040 ----
mean loss: 232.05
 ---- batch: 050 ----
mean loss: 240.40
 ---- batch: 060 ----
mean loss: 241.32
 ---- batch: 070 ----
mean loss: 235.45
 ---- batch: 080 ----
mean loss: 242.98
 ---- batch: 090 ----
mean loss: 238.09
train mean loss: 238.28
epoch train time: 0:00:00.676914
elapsed time: 0:02:02.763431
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 22:52:56.912791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.24
 ---- batch: 020 ----
mean loss: 225.68
 ---- batch: 030 ----
mean loss: 234.94
 ---- batch: 040 ----
mean loss: 239.48
 ---- batch: 050 ----
mean loss: 240.67
 ---- batch: 060 ----
mean loss: 231.62
 ---- batch: 070 ----
mean loss: 246.65
 ---- batch: 080 ----
mean loss: 243.31
 ---- batch: 090 ----
mean loss: 236.59
train mean loss: 238.52
epoch train time: 0:00:00.677731
elapsed time: 0:02:03.441330
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 22:52:57.590692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.54
 ---- batch: 020 ----
mean loss: 237.26
 ---- batch: 030 ----
mean loss: 237.96
 ---- batch: 040 ----
mean loss: 236.08
 ---- batch: 050 ----
mean loss: 233.94
 ---- batch: 060 ----
mean loss: 246.74
 ---- batch: 070 ----
mean loss: 240.45
 ---- batch: 080 ----
mean loss: 235.45
 ---- batch: 090 ----
mean loss: 236.46
train mean loss: 237.96
epoch train time: 0:00:00.675452
elapsed time: 0:02:04.116953
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 22:52:58.266297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.37
 ---- batch: 020 ----
mean loss: 236.12
 ---- batch: 030 ----
mean loss: 231.83
 ---- batch: 040 ----
mean loss: 240.10
 ---- batch: 050 ----
mean loss: 235.62
 ---- batch: 060 ----
mean loss: 237.23
 ---- batch: 070 ----
mean loss: 236.34
 ---- batch: 080 ----
mean loss: 237.91
 ---- batch: 090 ----
mean loss: 239.03
train mean loss: 237.51
epoch train time: 0:00:00.678120
elapsed time: 0:02:04.795218
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 22:52:58.944559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.04
 ---- batch: 020 ----
mean loss: 236.24
 ---- batch: 030 ----
mean loss: 240.10
 ---- batch: 040 ----
mean loss: 232.65
 ---- batch: 050 ----
mean loss: 235.46
 ---- batch: 060 ----
mean loss: 234.01
 ---- batch: 070 ----
mean loss: 231.59
 ---- batch: 080 ----
mean loss: 237.74
 ---- batch: 090 ----
mean loss: 244.16
train mean loss: 237.15
epoch train time: 0:00:00.670560
elapsed time: 0:02:05.465951
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 22:52:59.615304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.18
 ---- batch: 020 ----
mean loss: 237.98
 ---- batch: 030 ----
mean loss: 237.95
 ---- batch: 040 ----
mean loss: 243.01
 ---- batch: 050 ----
mean loss: 235.21
 ---- batch: 060 ----
mean loss: 241.30
 ---- batch: 070 ----
mean loss: 242.36
 ---- batch: 080 ----
mean loss: 235.37
 ---- batch: 090 ----
mean loss: 230.79
train mean loss: 237.07
epoch train time: 0:00:00.678857
elapsed time: 0:02:06.144968
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 22:53:00.294310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.88
 ---- batch: 020 ----
mean loss: 237.67
 ---- batch: 030 ----
mean loss: 240.01
 ---- batch: 040 ----
mean loss: 234.55
 ---- batch: 050 ----
mean loss: 244.19
 ---- batch: 060 ----
mean loss: 233.26
 ---- batch: 070 ----
mean loss: 233.61
 ---- batch: 080 ----
mean loss: 236.01
 ---- batch: 090 ----
mean loss: 243.72
train mean loss: 236.99
epoch train time: 0:00:00.677413
elapsed time: 0:02:06.822529
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 22:53:00.971888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.52
 ---- batch: 020 ----
mean loss: 234.31
 ---- batch: 030 ----
mean loss: 241.95
 ---- batch: 040 ----
mean loss: 228.74
 ---- batch: 050 ----
mean loss: 232.95
 ---- batch: 060 ----
mean loss: 237.50
 ---- batch: 070 ----
mean loss: 237.98
 ---- batch: 080 ----
mean loss: 248.43
 ---- batch: 090 ----
mean loss: 231.60
train mean loss: 236.10
epoch train time: 0:00:00.676973
elapsed time: 0:02:07.499679
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 22:53:01.649024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.86
 ---- batch: 020 ----
mean loss: 233.99
 ---- batch: 030 ----
mean loss: 234.87
 ---- batch: 040 ----
mean loss: 238.78
 ---- batch: 050 ----
mean loss: 239.35
 ---- batch: 060 ----
mean loss: 238.18
 ---- batch: 070 ----
mean loss: 227.65
 ---- batch: 080 ----
mean loss: 235.39
 ---- batch: 090 ----
mean loss: 245.11
train mean loss: 236.64
epoch train time: 0:00:00.671580
elapsed time: 0:02:08.171404
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 22:53:02.320761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.08
 ---- batch: 020 ----
mean loss: 239.31
 ---- batch: 030 ----
mean loss: 241.42
 ---- batch: 040 ----
mean loss: 227.88
 ---- batch: 050 ----
mean loss: 239.36
 ---- batch: 060 ----
mean loss: 234.76
 ---- batch: 070 ----
mean loss: 236.40
 ---- batch: 080 ----
mean loss: 233.23
 ---- batch: 090 ----
mean loss: 236.27
train mean loss: 236.18
epoch train time: 0:00:00.673670
elapsed time: 0:02:08.845249
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 22:53:02.994592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.82
 ---- batch: 020 ----
mean loss: 234.95
 ---- batch: 030 ----
mean loss: 238.75
 ---- batch: 040 ----
mean loss: 236.86
 ---- batch: 050 ----
mean loss: 241.92
 ---- batch: 060 ----
mean loss: 236.09
 ---- batch: 070 ----
mean loss: 233.44
 ---- batch: 080 ----
mean loss: 234.13
 ---- batch: 090 ----
mean loss: 230.88
train mean loss: 235.87
epoch train time: 0:00:00.666047
elapsed time: 0:02:09.511474
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 22:53:03.660813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.28
 ---- batch: 020 ----
mean loss: 239.95
 ---- batch: 030 ----
mean loss: 227.14
 ---- batch: 040 ----
mean loss: 238.76
 ---- batch: 050 ----
mean loss: 231.89
 ---- batch: 060 ----
mean loss: 229.50
 ---- batch: 070 ----
mean loss: 236.99
 ---- batch: 080 ----
mean loss: 233.24
 ---- batch: 090 ----
mean loss: 231.86
train mean loss: 235.61
epoch train time: 0:00:00.679553
elapsed time: 0:02:10.191177
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 22:53:04.340519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.97
 ---- batch: 020 ----
mean loss: 244.78
 ---- batch: 030 ----
mean loss: 234.07
 ---- batch: 040 ----
mean loss: 234.24
 ---- batch: 050 ----
mean loss: 225.50
 ---- batch: 060 ----
mean loss: 242.14
 ---- batch: 070 ----
mean loss: 238.57
 ---- batch: 080 ----
mean loss: 233.21
 ---- batch: 090 ----
mean loss: 235.46
train mean loss: 235.22
epoch train time: 0:00:00.672033
elapsed time: 0:02:10.863355
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 22:53:05.012712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.70
 ---- batch: 020 ----
mean loss: 236.55
 ---- batch: 030 ----
mean loss: 229.04
 ---- batch: 040 ----
mean loss: 239.19
 ---- batch: 050 ----
mean loss: 236.50
 ---- batch: 060 ----
mean loss: 239.42
 ---- batch: 070 ----
mean loss: 230.61
 ---- batch: 080 ----
mean loss: 242.00
 ---- batch: 090 ----
mean loss: 234.08
train mean loss: 234.87
epoch train time: 0:00:00.681876
elapsed time: 0:02:11.545407
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 22:53:05.694762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.77
 ---- batch: 020 ----
mean loss: 235.99
 ---- batch: 030 ----
mean loss: 236.36
 ---- batch: 040 ----
mean loss: 234.36
 ---- batch: 050 ----
mean loss: 231.83
 ---- batch: 060 ----
mean loss: 234.60
 ---- batch: 070 ----
mean loss: 232.44
 ---- batch: 080 ----
mean loss: 233.86
 ---- batch: 090 ----
mean loss: 234.27
train mean loss: 235.04
epoch train time: 0:00:00.677657
elapsed time: 0:02:12.223222
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 22:53:06.372566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.73
 ---- batch: 020 ----
mean loss: 243.96
 ---- batch: 030 ----
mean loss: 234.00
 ---- batch: 040 ----
mean loss: 235.76
 ---- batch: 050 ----
mean loss: 233.03
 ---- batch: 060 ----
mean loss: 235.64
 ---- batch: 070 ----
mean loss: 224.39
 ---- batch: 080 ----
mean loss: 238.59
 ---- batch: 090 ----
mean loss: 238.26
train mean loss: 235.03
epoch train time: 0:00:00.683034
elapsed time: 0:02:12.906401
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 22:53:07.055743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.63
 ---- batch: 020 ----
mean loss: 232.43
 ---- batch: 030 ----
mean loss: 232.32
 ---- batch: 040 ----
mean loss: 239.81
 ---- batch: 050 ----
mean loss: 230.62
 ---- batch: 060 ----
mean loss: 229.70
 ---- batch: 070 ----
mean loss: 234.88
 ---- batch: 080 ----
mean loss: 237.42
 ---- batch: 090 ----
mean loss: 235.27
train mean loss: 234.34
epoch train time: 0:00:00.673786
elapsed time: 0:02:13.580375
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 22:53:07.729725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.32
 ---- batch: 020 ----
mean loss: 233.68
 ---- batch: 030 ----
mean loss: 232.35
 ---- batch: 040 ----
mean loss: 234.41
 ---- batch: 050 ----
mean loss: 238.76
 ---- batch: 060 ----
mean loss: 244.43
 ---- batch: 070 ----
mean loss: 236.84
 ---- batch: 080 ----
mean loss: 228.54
 ---- batch: 090 ----
mean loss: 237.22
train mean loss: 234.55
epoch train time: 0:00:00.671518
elapsed time: 0:02:14.252081
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 22:53:08.401432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.38
 ---- batch: 020 ----
mean loss: 238.79
 ---- batch: 030 ----
mean loss: 230.19
 ---- batch: 040 ----
mean loss: 237.94
 ---- batch: 050 ----
mean loss: 224.39
 ---- batch: 060 ----
mean loss: 234.67
 ---- batch: 070 ----
mean loss: 229.56
 ---- batch: 080 ----
mean loss: 238.37
 ---- batch: 090 ----
mean loss: 231.73
train mean loss: 234.10
epoch train time: 0:00:00.685473
elapsed time: 0:02:14.937710
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 22:53:09.087054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.65
 ---- batch: 020 ----
mean loss: 235.25
 ---- batch: 030 ----
mean loss: 234.18
 ---- batch: 040 ----
mean loss: 232.03
 ---- batch: 050 ----
mean loss: 229.79
 ---- batch: 060 ----
mean loss: 236.73
 ---- batch: 070 ----
mean loss: 231.88
 ---- batch: 080 ----
mean loss: 236.24
 ---- batch: 090 ----
mean loss: 244.39
train mean loss: 233.95
epoch train time: 0:00:00.671357
elapsed time: 0:02:15.609218
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 22:53:09.758575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.18
 ---- batch: 020 ----
mean loss: 242.54
 ---- batch: 030 ----
mean loss: 238.18
 ---- batch: 040 ----
mean loss: 228.08
 ---- batch: 050 ----
mean loss: 234.67
 ---- batch: 060 ----
mean loss: 230.10
 ---- batch: 070 ----
mean loss: 232.73
 ---- batch: 080 ----
mean loss: 229.97
 ---- batch: 090 ----
mean loss: 235.70
train mean loss: 233.34
epoch train time: 0:00:00.667876
elapsed time: 0:02:16.277272
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 22:53:10.426616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.76
 ---- batch: 020 ----
mean loss: 226.18
 ---- batch: 030 ----
mean loss: 240.40
 ---- batch: 040 ----
mean loss: 243.61
 ---- batch: 050 ----
mean loss: 234.13
 ---- batch: 060 ----
mean loss: 230.69
 ---- batch: 070 ----
mean loss: 230.66
 ---- batch: 080 ----
mean loss: 230.19
 ---- batch: 090 ----
mean loss: 227.90
train mean loss: 233.68
epoch train time: 0:00:00.672373
elapsed time: 0:02:16.949794
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 22:53:11.099139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.61
 ---- batch: 020 ----
mean loss: 230.77
 ---- batch: 030 ----
mean loss: 228.00
 ---- batch: 040 ----
mean loss: 237.49
 ---- batch: 050 ----
mean loss: 232.97
 ---- batch: 060 ----
mean loss: 237.54
 ---- batch: 070 ----
mean loss: 231.61
 ---- batch: 080 ----
mean loss: 237.77
 ---- batch: 090 ----
mean loss: 231.99
train mean loss: 233.26
epoch train time: 0:00:00.673981
elapsed time: 0:02:17.623924
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 22:53:11.773264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.62
 ---- batch: 020 ----
mean loss: 235.17
 ---- batch: 030 ----
mean loss: 233.48
 ---- batch: 040 ----
mean loss: 224.80
 ---- batch: 050 ----
mean loss: 232.13
 ---- batch: 060 ----
mean loss: 242.78
 ---- batch: 070 ----
mean loss: 234.27
 ---- batch: 080 ----
mean loss: 231.38
 ---- batch: 090 ----
mean loss: 235.86
train mean loss: 233.26
epoch train time: 0:00:00.692617
elapsed time: 0:02:18.316696
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 22:53:12.466041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.52
 ---- batch: 020 ----
mean loss: 224.40
 ---- batch: 030 ----
mean loss: 236.21
 ---- batch: 040 ----
mean loss: 240.62
 ---- batch: 050 ----
mean loss: 223.79
 ---- batch: 060 ----
mean loss: 236.14
 ---- batch: 070 ----
mean loss: 237.77
 ---- batch: 080 ----
mean loss: 244.51
 ---- batch: 090 ----
mean loss: 228.78
train mean loss: 233.11
epoch train time: 0:00:00.680957
elapsed time: 0:02:18.997805
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 22:53:13.147163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.79
 ---- batch: 020 ----
mean loss: 239.09
 ---- batch: 030 ----
mean loss: 230.60
 ---- batch: 040 ----
mean loss: 232.42
 ---- batch: 050 ----
mean loss: 238.83
 ---- batch: 060 ----
mean loss: 230.89
 ---- batch: 070 ----
mean loss: 231.14
 ---- batch: 080 ----
mean loss: 228.89
 ---- batch: 090 ----
mean loss: 230.92
train mean loss: 232.76
epoch train time: 0:00:00.677765
elapsed time: 0:02:19.675737
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 22:53:13.825087
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.53
 ---- batch: 020 ----
mean loss: 226.24
 ---- batch: 030 ----
mean loss: 233.76
 ---- batch: 040 ----
mean loss: 231.41
 ---- batch: 050 ----
mean loss: 226.61
 ---- batch: 060 ----
mean loss: 230.92
 ---- batch: 070 ----
mean loss: 230.68
 ---- batch: 080 ----
mean loss: 239.63
 ---- batch: 090 ----
mean loss: 229.24
train mean loss: 232.90
epoch train time: 0:00:00.670351
elapsed time: 0:02:20.346273
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 22:53:14.495632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.37
 ---- batch: 020 ----
mean loss: 226.38
 ---- batch: 030 ----
mean loss: 239.79
 ---- batch: 040 ----
mean loss: 236.37
 ---- batch: 050 ----
mean loss: 236.42
 ---- batch: 060 ----
mean loss: 228.87
 ---- batch: 070 ----
mean loss: 232.07
 ---- batch: 080 ----
mean loss: 228.98
 ---- batch: 090 ----
mean loss: 233.07
train mean loss: 232.34
epoch train time: 0:00:00.670943
elapsed time: 0:02:21.017402
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 22:53:15.166748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.71
 ---- batch: 020 ----
mean loss: 230.03
 ---- batch: 030 ----
mean loss: 228.42
 ---- batch: 040 ----
mean loss: 231.53
 ---- batch: 050 ----
mean loss: 229.96
 ---- batch: 060 ----
mean loss: 231.48
 ---- batch: 070 ----
mean loss: 238.26
 ---- batch: 080 ----
mean loss: 233.58
 ---- batch: 090 ----
mean loss: 233.76
train mean loss: 232.40
epoch train time: 0:00:00.668647
elapsed time: 0:02:21.686231
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 22:53:15.835589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.85
 ---- batch: 020 ----
mean loss: 225.79
 ---- batch: 030 ----
mean loss: 239.51
 ---- batch: 040 ----
mean loss: 227.64
 ---- batch: 050 ----
mean loss: 227.21
 ---- batch: 060 ----
mean loss: 237.91
 ---- batch: 070 ----
mean loss: 238.44
 ---- batch: 080 ----
mean loss: 228.86
 ---- batch: 090 ----
mean loss: 229.47
train mean loss: 231.90
epoch train time: 0:00:00.681851
elapsed time: 0:02:22.368248
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 22:53:16.517591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.33
 ---- batch: 020 ----
mean loss: 235.69
 ---- batch: 030 ----
mean loss: 242.83
 ---- batch: 040 ----
mean loss: 233.70
 ---- batch: 050 ----
mean loss: 226.08
 ---- batch: 060 ----
mean loss: 237.65
 ---- batch: 070 ----
mean loss: 230.24
 ---- batch: 080 ----
mean loss: 232.87
 ---- batch: 090 ----
mean loss: 225.02
train mean loss: 231.54
epoch train time: 0:00:00.674799
elapsed time: 0:02:23.043191
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 22:53:17.192539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.46
 ---- batch: 020 ----
mean loss: 223.32
 ---- batch: 030 ----
mean loss: 232.55
 ---- batch: 040 ----
mean loss: 232.72
 ---- batch: 050 ----
mean loss: 233.19
 ---- batch: 060 ----
mean loss: 235.23
 ---- batch: 070 ----
mean loss: 234.07
 ---- batch: 080 ----
mean loss: 229.00
 ---- batch: 090 ----
mean loss: 233.45
train mean loss: 231.73
epoch train time: 0:00:00.671521
elapsed time: 0:02:23.714860
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 22:53:17.864216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.84
 ---- batch: 020 ----
mean loss: 229.58
 ---- batch: 030 ----
mean loss: 229.99
 ---- batch: 040 ----
mean loss: 225.81
 ---- batch: 050 ----
mean loss: 239.72
 ---- batch: 060 ----
mean loss: 236.41
 ---- batch: 070 ----
mean loss: 229.96
 ---- batch: 080 ----
mean loss: 225.16
 ---- batch: 090 ----
mean loss: 237.89
train mean loss: 231.31
epoch train time: 0:00:00.674741
elapsed time: 0:02:24.389769
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 22:53:18.539110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.47
 ---- batch: 020 ----
mean loss: 228.13
 ---- batch: 030 ----
mean loss: 225.95
 ---- batch: 040 ----
mean loss: 238.43
 ---- batch: 050 ----
mean loss: 233.07
 ---- batch: 060 ----
mean loss: 242.45
 ---- batch: 070 ----
mean loss: 223.18
 ---- batch: 080 ----
mean loss: 222.57
 ---- batch: 090 ----
mean loss: 234.61
train mean loss: 231.27
epoch train time: 0:00:00.671366
elapsed time: 0:02:25.061321
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 22:53:19.210667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.15
 ---- batch: 020 ----
mean loss: 227.58
 ---- batch: 030 ----
mean loss: 234.06
 ---- batch: 040 ----
mean loss: 228.14
 ---- batch: 050 ----
mean loss: 232.09
 ---- batch: 060 ----
mean loss: 227.01
 ---- batch: 070 ----
mean loss: 222.28
 ---- batch: 080 ----
mean loss: 232.69
 ---- batch: 090 ----
mean loss: 237.78
train mean loss: 230.93
epoch train time: 0:00:00.670571
elapsed time: 0:02:25.732047
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 22:53:19.881403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.85
 ---- batch: 020 ----
mean loss: 229.06
 ---- batch: 030 ----
mean loss: 236.70
 ---- batch: 040 ----
mean loss: 222.60
 ---- batch: 050 ----
mean loss: 231.40
 ---- batch: 060 ----
mean loss: 226.87
 ---- batch: 070 ----
mean loss: 234.30
 ---- batch: 080 ----
mean loss: 227.76
 ---- batch: 090 ----
mean loss: 230.63
train mean loss: 230.92
epoch train time: 0:00:00.677555
elapsed time: 0:02:26.409769
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 22:53:20.559112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.98
 ---- batch: 020 ----
mean loss: 231.43
 ---- batch: 030 ----
mean loss: 222.42
 ---- batch: 040 ----
mean loss: 227.62
 ---- batch: 050 ----
mean loss: 231.43
 ---- batch: 060 ----
mean loss: 236.21
 ---- batch: 070 ----
mean loss: 226.19
 ---- batch: 080 ----
mean loss: 236.09
 ---- batch: 090 ----
mean loss: 228.58
train mean loss: 230.78
epoch train time: 0:00:00.687562
elapsed time: 0:02:27.097483
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 22:53:21.246829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.99
 ---- batch: 020 ----
mean loss: 227.68
 ---- batch: 030 ----
mean loss: 225.69
 ---- batch: 040 ----
mean loss: 232.95
 ---- batch: 050 ----
mean loss: 227.88
 ---- batch: 060 ----
mean loss: 233.70
 ---- batch: 070 ----
mean loss: 236.06
 ---- batch: 080 ----
mean loss: 231.92
 ---- batch: 090 ----
mean loss: 227.64
train mean loss: 230.42
epoch train time: 0:00:00.679229
elapsed time: 0:02:27.776879
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 22:53:21.926225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.83
 ---- batch: 020 ----
mean loss: 230.36
 ---- batch: 030 ----
mean loss: 227.50
 ---- batch: 040 ----
mean loss: 232.58
 ---- batch: 050 ----
mean loss: 236.74
 ---- batch: 060 ----
mean loss: 230.30
 ---- batch: 070 ----
mean loss: 233.13
 ---- batch: 080 ----
mean loss: 220.22
 ---- batch: 090 ----
mean loss: 227.59
train mean loss: 229.96
epoch train time: 0:00:00.684637
elapsed time: 0:02:28.461671
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 22:53:22.611014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.79
 ---- batch: 020 ----
mean loss: 226.35
 ---- batch: 030 ----
mean loss: 221.56
 ---- batch: 040 ----
mean loss: 230.57
 ---- batch: 050 ----
mean loss: 230.25
 ---- batch: 060 ----
mean loss: 233.47
 ---- batch: 070 ----
mean loss: 237.69
 ---- batch: 080 ----
mean loss: 226.50
 ---- batch: 090 ----
mean loss: 234.26
train mean loss: 230.11
epoch train time: 0:00:00.683016
elapsed time: 0:02:29.144841
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 22:53:23.294204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.51
 ---- batch: 020 ----
mean loss: 217.67
 ---- batch: 030 ----
mean loss: 233.84
 ---- batch: 040 ----
mean loss: 236.73
 ---- batch: 050 ----
mean loss: 225.33
 ---- batch: 060 ----
mean loss: 223.49
 ---- batch: 070 ----
mean loss: 229.49
 ---- batch: 080 ----
mean loss: 233.43
 ---- batch: 090 ----
mean loss: 228.03
train mean loss: 229.85
epoch train time: 0:00:00.680030
elapsed time: 0:02:29.825040
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 22:53:23.974384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.93
 ---- batch: 020 ----
mean loss: 223.92
 ---- batch: 030 ----
mean loss: 237.75
 ---- batch: 040 ----
mean loss: 222.41
 ---- batch: 050 ----
mean loss: 223.56
 ---- batch: 060 ----
mean loss: 234.34
 ---- batch: 070 ----
mean loss: 232.33
 ---- batch: 080 ----
mean loss: 232.71
 ---- batch: 090 ----
mean loss: 229.42
train mean loss: 229.67
epoch train time: 0:00:00.665380
elapsed time: 0:02:30.490617
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 22:53:24.639946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.36
 ---- batch: 020 ----
mean loss: 231.77
 ---- batch: 030 ----
mean loss: 242.45
 ---- batch: 040 ----
mean loss: 221.43
 ---- batch: 050 ----
mean loss: 224.01
 ---- batch: 060 ----
mean loss: 234.12
 ---- batch: 070 ----
mean loss: 222.03
 ---- batch: 080 ----
mean loss: 231.80
 ---- batch: 090 ----
mean loss: 230.14
train mean loss: 229.33
epoch train time: 0:00:00.680724
elapsed time: 0:02:31.171502
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 22:53:25.320858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.21
 ---- batch: 020 ----
mean loss: 223.98
 ---- batch: 030 ----
mean loss: 229.11
 ---- batch: 040 ----
mean loss: 233.53
 ---- batch: 050 ----
mean loss: 230.27
 ---- batch: 060 ----
mean loss: 231.41
 ---- batch: 070 ----
mean loss: 233.09
 ---- batch: 080 ----
mean loss: 224.19
 ---- batch: 090 ----
mean loss: 227.82
train mean loss: 229.07
epoch train time: 0:00:00.677774
elapsed time: 0:02:31.849450
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 22:53:25.998821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.75
 ---- batch: 020 ----
mean loss: 228.16
 ---- batch: 030 ----
mean loss: 227.19
 ---- batch: 040 ----
mean loss: 230.15
 ---- batch: 050 ----
mean loss: 233.73
 ---- batch: 060 ----
mean loss: 228.71
 ---- batch: 070 ----
mean loss: 224.97
 ---- batch: 080 ----
mean loss: 220.45
 ---- batch: 090 ----
mean loss: 237.17
train mean loss: 228.87
epoch train time: 0:00:00.686968
elapsed time: 0:02:32.536596
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 22:53:26.685938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.06
 ---- batch: 020 ----
mean loss: 233.80
 ---- batch: 030 ----
mean loss: 227.58
 ---- batch: 040 ----
mean loss: 224.14
 ---- batch: 050 ----
mean loss: 222.44
 ---- batch: 060 ----
mean loss: 230.53
 ---- batch: 070 ----
mean loss: 231.97
 ---- batch: 080 ----
mean loss: 228.45
 ---- batch: 090 ----
mean loss: 225.22
train mean loss: 228.53
epoch train time: 0:00:00.688140
elapsed time: 0:02:33.224892
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 22:53:27.374238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.43
 ---- batch: 020 ----
mean loss: 232.47
 ---- batch: 030 ----
mean loss: 231.91
 ---- batch: 040 ----
mean loss: 231.84
 ---- batch: 050 ----
mean loss: 220.95
 ---- batch: 060 ----
mean loss: 232.69
 ---- batch: 070 ----
mean loss: 224.31
 ---- batch: 080 ----
mean loss: 221.16
 ---- batch: 090 ----
mean loss: 227.41
train mean loss: 228.64
epoch train time: 0:00:00.686113
elapsed time: 0:02:33.911193
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 22:53:28.060574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.06
 ---- batch: 020 ----
mean loss: 229.71
 ---- batch: 030 ----
mean loss: 231.30
 ---- batch: 040 ----
mean loss: 226.81
 ---- batch: 050 ----
mean loss: 232.90
 ---- batch: 060 ----
mean loss: 229.14
 ---- batch: 070 ----
mean loss: 222.49
 ---- batch: 080 ----
mean loss: 229.07
 ---- batch: 090 ----
mean loss: 226.66
train mean loss: 228.52
epoch train time: 0:00:00.668256
elapsed time: 0:02:34.579665
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 22:53:28.729007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.03
 ---- batch: 020 ----
mean loss: 229.63
 ---- batch: 030 ----
mean loss: 228.29
 ---- batch: 040 ----
mean loss: 236.25
 ---- batch: 050 ----
mean loss: 225.82
 ---- batch: 060 ----
mean loss: 223.34
 ---- batch: 070 ----
mean loss: 224.88
 ---- batch: 080 ----
mean loss: 229.11
 ---- batch: 090 ----
mean loss: 224.95
train mean loss: 228.56
epoch train time: 0:00:00.685796
elapsed time: 0:02:35.265610
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 22:53:29.414951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.72
 ---- batch: 020 ----
mean loss: 232.85
 ---- batch: 030 ----
mean loss: 226.37
 ---- batch: 040 ----
mean loss: 231.50
 ---- batch: 050 ----
mean loss: 220.18
 ---- batch: 060 ----
mean loss: 226.86
 ---- batch: 070 ----
mean loss: 231.18
 ---- batch: 080 ----
mean loss: 228.09
 ---- batch: 090 ----
mean loss: 227.20
train mean loss: 228.19
epoch train time: 0:00:00.686205
elapsed time: 0:02:35.951959
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 22:53:30.101301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.43
 ---- batch: 020 ----
mean loss: 227.11
 ---- batch: 030 ----
mean loss: 235.28
 ---- batch: 040 ----
mean loss: 227.37
 ---- batch: 050 ----
mean loss: 225.20
 ---- batch: 060 ----
mean loss: 233.26
 ---- batch: 070 ----
mean loss: 226.22
 ---- batch: 080 ----
mean loss: 231.87
 ---- batch: 090 ----
mean loss: 214.71
train mean loss: 227.66
epoch train time: 0:00:00.677585
elapsed time: 0:02:36.629727
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 22:53:30.779071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.83
 ---- batch: 020 ----
mean loss: 227.22
 ---- batch: 030 ----
mean loss: 223.92
 ---- batch: 040 ----
mean loss: 233.57
 ---- batch: 050 ----
mean loss: 229.10
 ---- batch: 060 ----
mean loss: 228.73
 ---- batch: 070 ----
mean loss: 228.80
 ---- batch: 080 ----
mean loss: 229.40
 ---- batch: 090 ----
mean loss: 221.97
train mean loss: 227.88
epoch train time: 0:00:00.682392
elapsed time: 0:02:37.312309
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 22:53:31.462145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.95
 ---- batch: 020 ----
mean loss: 225.87
 ---- batch: 030 ----
mean loss: 226.97
 ---- batch: 040 ----
mean loss: 229.57
 ---- batch: 050 ----
mean loss: 231.42
 ---- batch: 060 ----
mean loss: 236.92
 ---- batch: 070 ----
mean loss: 224.42
 ---- batch: 080 ----
mean loss: 222.65
 ---- batch: 090 ----
mean loss: 222.21
train mean loss: 227.90
epoch train time: 0:00:00.686877
elapsed time: 0:02:37.999828
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 22:53:32.149178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.96
 ---- batch: 020 ----
mean loss: 220.19
 ---- batch: 030 ----
mean loss: 225.44
 ---- batch: 040 ----
mean loss: 234.86
 ---- batch: 050 ----
mean loss: 240.09
 ---- batch: 060 ----
mean loss: 226.32
 ---- batch: 070 ----
mean loss: 225.84
 ---- batch: 080 ----
mean loss: 224.76
 ---- batch: 090 ----
mean loss: 227.99
train mean loss: 227.52
epoch train time: 0:00:00.687557
elapsed time: 0:02:38.687541
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 22:53:32.836884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.36
 ---- batch: 020 ----
mean loss: 223.73
 ---- batch: 030 ----
mean loss: 228.77
 ---- batch: 040 ----
mean loss: 228.27
 ---- batch: 050 ----
mean loss: 221.15
 ---- batch: 060 ----
mean loss: 230.41
 ---- batch: 070 ----
mean loss: 230.85
 ---- batch: 080 ----
mean loss: 224.72
 ---- batch: 090 ----
mean loss: 230.62
train mean loss: 227.46
epoch train time: 0:00:00.680086
elapsed time: 0:02:39.367791
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 22:53:33.517162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.12
 ---- batch: 020 ----
mean loss: 228.67
 ---- batch: 030 ----
mean loss: 224.96
 ---- batch: 040 ----
mean loss: 224.37
 ---- batch: 050 ----
mean loss: 229.70
 ---- batch: 060 ----
mean loss: 227.85
 ---- batch: 070 ----
mean loss: 225.89
 ---- batch: 080 ----
mean loss: 227.11
 ---- batch: 090 ----
mean loss: 229.12
train mean loss: 226.90
epoch train time: 0:00:00.677399
elapsed time: 0:02:40.045370
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 22:53:34.194713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.22
 ---- batch: 020 ----
mean loss: 229.42
 ---- batch: 030 ----
mean loss: 222.96
 ---- batch: 040 ----
mean loss: 223.75
 ---- batch: 050 ----
mean loss: 231.59
 ---- batch: 060 ----
mean loss: 222.70
 ---- batch: 070 ----
mean loss: 230.25
 ---- batch: 080 ----
mean loss: 228.33
 ---- batch: 090 ----
mean loss: 229.56
train mean loss: 227.20
epoch train time: 0:00:00.680607
elapsed time: 0:02:40.726192
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 22:53:34.875530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.52
 ---- batch: 020 ----
mean loss: 225.87
 ---- batch: 030 ----
mean loss: 238.13
 ---- batch: 040 ----
mean loss: 220.28
 ---- batch: 050 ----
mean loss: 228.92
 ---- batch: 060 ----
mean loss: 218.52
 ---- batch: 070 ----
mean loss: 219.64
 ---- batch: 080 ----
mean loss: 231.25
 ---- batch: 090 ----
mean loss: 231.74
train mean loss: 226.67
epoch train time: 0:00:00.673731
elapsed time: 0:02:41.400066
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 22:53:35.549406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.27
 ---- batch: 020 ----
mean loss: 226.73
 ---- batch: 030 ----
mean loss: 229.82
 ---- batch: 040 ----
mean loss: 229.53
 ---- batch: 050 ----
mean loss: 230.82
 ---- batch: 060 ----
mean loss: 223.70
 ---- batch: 070 ----
mean loss: 225.64
 ---- batch: 080 ----
mean loss: 234.45
 ---- batch: 090 ----
mean loss: 217.74
train mean loss: 226.87
epoch train time: 0:00:00.684277
elapsed time: 0:02:42.084493
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 22:53:36.233840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.84
 ---- batch: 020 ----
mean loss: 228.88
 ---- batch: 030 ----
mean loss: 230.21
 ---- batch: 040 ----
mean loss: 220.51
 ---- batch: 050 ----
mean loss: 229.40
 ---- batch: 060 ----
mean loss: 225.67
 ---- batch: 070 ----
mean loss: 228.46
 ---- batch: 080 ----
mean loss: 223.89
 ---- batch: 090 ----
mean loss: 223.77
train mean loss: 226.35
epoch train time: 0:00:00.687425
elapsed time: 0:02:42.772084
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 22:53:36.921428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.60
 ---- batch: 020 ----
mean loss: 226.63
 ---- batch: 030 ----
mean loss: 221.18
 ---- batch: 040 ----
mean loss: 229.09
 ---- batch: 050 ----
mean loss: 226.32
 ---- batch: 060 ----
mean loss: 230.15
 ---- batch: 070 ----
mean loss: 221.60
 ---- batch: 080 ----
mean loss: 224.64
 ---- batch: 090 ----
mean loss: 224.60
train mean loss: 225.94
epoch train time: 0:00:00.680076
elapsed time: 0:02:43.452347
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 22:53:37.601696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.62
 ---- batch: 020 ----
mean loss: 219.46
 ---- batch: 030 ----
mean loss: 225.17
 ---- batch: 040 ----
mean loss: 227.09
 ---- batch: 050 ----
mean loss: 226.62
 ---- batch: 060 ----
mean loss: 215.28
 ---- batch: 070 ----
mean loss: 231.11
 ---- batch: 080 ----
mean loss: 231.07
 ---- batch: 090 ----
mean loss: 229.28
train mean loss: 226.18
epoch train time: 0:00:00.683484
elapsed time: 0:02:44.135992
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 22:53:38.285355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.11
 ---- batch: 020 ----
mean loss: 224.86
 ---- batch: 030 ----
mean loss: 223.96
 ---- batch: 040 ----
mean loss: 225.94
 ---- batch: 050 ----
mean loss: 220.47
 ---- batch: 060 ----
mean loss: 230.09
 ---- batch: 070 ----
mean loss: 231.91
 ---- batch: 080 ----
mean loss: 224.13
 ---- batch: 090 ----
mean loss: 225.29
train mean loss: 226.18
epoch train time: 0:00:00.679306
elapsed time: 0:02:44.815470
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 22:53:38.964812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.59
 ---- batch: 020 ----
mean loss: 224.02
 ---- batch: 030 ----
mean loss: 232.69
 ---- batch: 040 ----
mean loss: 224.08
 ---- batch: 050 ----
mean loss: 222.37
 ---- batch: 060 ----
mean loss: 230.79
 ---- batch: 070 ----
mean loss: 230.25
 ---- batch: 080 ----
mean loss: 225.03
 ---- batch: 090 ----
mean loss: 221.78
train mean loss: 226.29
epoch train time: 0:00:00.688657
elapsed time: 0:02:45.504378
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 22:53:39.653726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.47
 ---- batch: 020 ----
mean loss: 226.56
 ---- batch: 030 ----
mean loss: 220.36
 ---- batch: 040 ----
mean loss: 227.79
 ---- batch: 050 ----
mean loss: 224.39
 ---- batch: 060 ----
mean loss: 221.98
 ---- batch: 070 ----
mean loss: 224.79
 ---- batch: 080 ----
mean loss: 222.10
 ---- batch: 090 ----
mean loss: 233.64
train mean loss: 225.45
epoch train time: 0:00:00.680363
elapsed time: 0:02:46.184906
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 22:53:40.334253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.89
 ---- batch: 020 ----
mean loss: 229.70
 ---- batch: 030 ----
mean loss: 225.46
 ---- batch: 040 ----
mean loss: 224.75
 ---- batch: 050 ----
mean loss: 224.59
 ---- batch: 060 ----
mean loss: 228.28
 ---- batch: 070 ----
mean loss: 220.07
 ---- batch: 080 ----
mean loss: 221.40
 ---- batch: 090 ----
mean loss: 232.19
train mean loss: 225.52
epoch train time: 0:00:00.684301
elapsed time: 0:02:46.869374
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 22:53:41.018723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.53
 ---- batch: 020 ----
mean loss: 219.71
 ---- batch: 030 ----
mean loss: 222.85
 ---- batch: 040 ----
mean loss: 231.68
 ---- batch: 050 ----
mean loss: 233.82
 ---- batch: 060 ----
mean loss: 225.53
 ---- batch: 070 ----
mean loss: 228.27
 ---- batch: 080 ----
mean loss: 229.47
 ---- batch: 090 ----
mean loss: 217.80
train mean loss: 225.50
epoch train time: 0:00:00.681989
elapsed time: 0:02:47.551536
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 22:53:41.700882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.08
 ---- batch: 020 ----
mean loss: 214.37
 ---- batch: 030 ----
mean loss: 229.63
 ---- batch: 040 ----
mean loss: 224.13
 ---- batch: 050 ----
mean loss: 227.48
 ---- batch: 060 ----
mean loss: 228.22
 ---- batch: 070 ----
mean loss: 225.21
 ---- batch: 080 ----
mean loss: 231.08
 ---- batch: 090 ----
mean loss: 220.08
train mean loss: 224.95
epoch train time: 0:00:00.682881
elapsed time: 0:02:48.234582
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 22:53:42.383928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.77
 ---- batch: 020 ----
mean loss: 226.14
 ---- batch: 030 ----
mean loss: 218.06
 ---- batch: 040 ----
mean loss: 225.86
 ---- batch: 050 ----
mean loss: 226.00
 ---- batch: 060 ----
mean loss: 219.43
 ---- batch: 070 ----
mean loss: 226.98
 ---- batch: 080 ----
mean loss: 227.71
 ---- batch: 090 ----
mean loss: 229.03
train mean loss: 225.48
epoch train time: 0:00:00.679729
elapsed time: 0:02:48.914469
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 22:53:43.063824
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.01
 ---- batch: 020 ----
mean loss: 219.73
 ---- batch: 030 ----
mean loss: 222.17
 ---- batch: 040 ----
mean loss: 235.15
 ---- batch: 050 ----
mean loss: 225.80
 ---- batch: 060 ----
mean loss: 213.86
 ---- batch: 070 ----
mean loss: 223.79
 ---- batch: 080 ----
mean loss: 226.71
 ---- batch: 090 ----
mean loss: 228.13
train mean loss: 224.68
epoch train time: 0:00:00.679368
elapsed time: 0:02:49.594021
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 22:53:43.743384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.33
 ---- batch: 020 ----
mean loss: 222.31
 ---- batch: 030 ----
mean loss: 227.94
 ---- batch: 040 ----
mean loss: 217.76
 ---- batch: 050 ----
mean loss: 228.63
 ---- batch: 060 ----
mean loss: 218.37
 ---- batch: 070 ----
mean loss: 223.64
 ---- batch: 080 ----
mean loss: 227.74
 ---- batch: 090 ----
mean loss: 221.52
train mean loss: 224.57
epoch train time: 0:00:00.688319
elapsed time: 0:02:50.282547
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 22:53:44.431896
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.22
 ---- batch: 020 ----
mean loss: 226.35
 ---- batch: 030 ----
mean loss: 217.23
 ---- batch: 040 ----
mean loss: 225.39
 ---- batch: 050 ----
mean loss: 225.07
 ---- batch: 060 ----
mean loss: 218.26
 ---- batch: 070 ----
mean loss: 234.98
 ---- batch: 080 ----
mean loss: 223.56
 ---- batch: 090 ----
mean loss: 224.91
train mean loss: 224.47
epoch train time: 0:00:00.671953
elapsed time: 0:02:50.954664
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 22:53:45.104010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.89
 ---- batch: 020 ----
mean loss: 228.31
 ---- batch: 030 ----
mean loss: 225.57
 ---- batch: 040 ----
mean loss: 222.40
 ---- batch: 050 ----
mean loss: 224.93
 ---- batch: 060 ----
mean loss: 226.86
 ---- batch: 070 ----
mean loss: 219.75
 ---- batch: 080 ----
mean loss: 232.62
 ---- batch: 090 ----
mean loss: 220.46
train mean loss: 224.34
epoch train time: 0:00:00.670745
elapsed time: 0:02:51.625570
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 22:53:45.774915
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.46
 ---- batch: 020 ----
mean loss: 225.92
 ---- batch: 030 ----
mean loss: 233.46
 ---- batch: 040 ----
mean loss: 212.26
 ---- batch: 050 ----
mean loss: 226.59
 ---- batch: 060 ----
mean loss: 223.63
 ---- batch: 070 ----
mean loss: 224.28
 ---- batch: 080 ----
mean loss: 236.46
 ---- batch: 090 ----
mean loss: 222.04
train mean loss: 224.36
epoch train time: 0:00:00.689761
elapsed time: 0:02:52.315486
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 22:53:46.464830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.14
 ---- batch: 020 ----
mean loss: 218.67
 ---- batch: 030 ----
mean loss: 220.18
 ---- batch: 040 ----
mean loss: 232.98
 ---- batch: 050 ----
mean loss: 223.17
 ---- batch: 060 ----
mean loss: 224.88
 ---- batch: 070 ----
mean loss: 230.24
 ---- batch: 080 ----
mean loss: 230.70
 ---- batch: 090 ----
mean loss: 221.63
train mean loss: 224.59
epoch train time: 0:00:00.686206
elapsed time: 0:02:53.001875
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 22:53:47.151251
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.74
 ---- batch: 020 ----
mean loss: 217.98
 ---- batch: 030 ----
mean loss: 222.86
 ---- batch: 040 ----
mean loss: 232.47
 ---- batch: 050 ----
mean loss: 223.33
 ---- batch: 060 ----
mean loss: 216.97
 ---- batch: 070 ----
mean loss: 227.79
 ---- batch: 080 ----
mean loss: 226.05
 ---- batch: 090 ----
mean loss: 225.31
train mean loss: 224.55
epoch train time: 0:00:00.686829
elapsed time: 0:02:53.688891
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 22:53:47.838234
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 220.35
 ---- batch: 020 ----
mean loss: 227.77
 ---- batch: 030 ----
mean loss: 221.80
 ---- batch: 040 ----
mean loss: 236.35
 ---- batch: 050 ----
mean loss: 227.68
 ---- batch: 060 ----
mean loss: 219.72
 ---- batch: 070 ----
mean loss: 221.50
 ---- batch: 080 ----
mean loss: 224.48
 ---- batch: 090 ----
mean loss: 220.70
train mean loss: 224.43
epoch train time: 0:00:00.678283
elapsed time: 0:02:54.367323
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 22:53:48.516682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.67
 ---- batch: 020 ----
mean loss: 228.46
 ---- batch: 030 ----
mean loss: 223.19
 ---- batch: 040 ----
mean loss: 221.01
 ---- batch: 050 ----
mean loss: 230.15
 ---- batch: 060 ----
mean loss: 229.80
 ---- batch: 070 ----
mean loss: 224.19
 ---- batch: 080 ----
mean loss: 224.02
 ---- batch: 090 ----
mean loss: 221.04
train mean loss: 224.44
epoch train time: 0:00:00.678099
elapsed time: 0:02:55.045590
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 22:53:49.194933
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.04
 ---- batch: 020 ----
mean loss: 227.38
 ---- batch: 030 ----
mean loss: 226.66
 ---- batch: 040 ----
mean loss: 221.90
 ---- batch: 050 ----
mean loss: 217.64
 ---- batch: 060 ----
mean loss: 233.66
 ---- batch: 070 ----
mean loss: 221.72
 ---- batch: 080 ----
mean loss: 222.70
 ---- batch: 090 ----
mean loss: 220.58
train mean loss: 224.21
epoch train time: 0:00:00.686562
elapsed time: 0:02:55.732305
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 22:53:49.881673
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.09
 ---- batch: 020 ----
mean loss: 223.77
 ---- batch: 030 ----
mean loss: 223.41
 ---- batch: 040 ----
mean loss: 223.04
 ---- batch: 050 ----
mean loss: 223.04
 ---- batch: 060 ----
mean loss: 223.92
 ---- batch: 070 ----
mean loss: 227.86
 ---- batch: 080 ----
mean loss: 222.74
 ---- batch: 090 ----
mean loss: 223.45
train mean loss: 224.65
epoch train time: 0:00:00.676530
elapsed time: 0:02:56.409045
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 22:53:50.558388
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.29
 ---- batch: 020 ----
mean loss: 226.62
 ---- batch: 030 ----
mean loss: 224.56
 ---- batch: 040 ----
mean loss: 228.46
 ---- batch: 050 ----
mean loss: 221.19
 ---- batch: 060 ----
mean loss: 218.18
 ---- batch: 070 ----
mean loss: 226.62
 ---- batch: 080 ----
mean loss: 224.75
 ---- batch: 090 ----
mean loss: 224.54
train mean loss: 224.50
epoch train time: 0:00:00.684840
elapsed time: 0:02:57.094061
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 22:53:51.243419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.22
 ---- batch: 020 ----
mean loss: 224.93
 ---- batch: 030 ----
mean loss: 226.24
 ---- batch: 040 ----
mean loss: 226.08
 ---- batch: 050 ----
mean loss: 231.01
 ---- batch: 060 ----
mean loss: 227.16
 ---- batch: 070 ----
mean loss: 218.85
 ---- batch: 080 ----
mean loss: 220.81
 ---- batch: 090 ----
mean loss: 220.65
train mean loss: 224.21
epoch train time: 0:00:00.685947
elapsed time: 0:02:57.780180
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 22:53:51.929525
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 217.70
 ---- batch: 020 ----
mean loss: 210.24
 ---- batch: 030 ----
mean loss: 221.40
 ---- batch: 040 ----
mean loss: 221.91
 ---- batch: 050 ----
mean loss: 225.14
 ---- batch: 060 ----
mean loss: 233.06
 ---- batch: 070 ----
mean loss: 231.09
 ---- batch: 080 ----
mean loss: 232.53
 ---- batch: 090 ----
mean loss: 226.93
train mean loss: 224.70
epoch train time: 0:00:00.690467
elapsed time: 0:02:58.470807
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 22:53:52.620153
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.53
 ---- batch: 020 ----
mean loss: 225.70
 ---- batch: 030 ----
mean loss: 224.26
 ---- batch: 040 ----
mean loss: 223.84
 ---- batch: 050 ----
mean loss: 221.72
 ---- batch: 060 ----
mean loss: 229.68
 ---- batch: 070 ----
mean loss: 220.91
 ---- batch: 080 ----
mean loss: 222.75
 ---- batch: 090 ----
mean loss: 220.20
train mean loss: 224.35
epoch train time: 0:00:00.682743
elapsed time: 0:02:59.153726
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 22:53:53.303068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.93
 ---- batch: 020 ----
mean loss: 223.22
 ---- batch: 030 ----
mean loss: 228.10
 ---- batch: 040 ----
mean loss: 220.90
 ---- batch: 050 ----
mean loss: 222.15
 ---- batch: 060 ----
mean loss: 226.25
 ---- batch: 070 ----
mean loss: 218.16
 ---- batch: 080 ----
mean loss: 229.42
 ---- batch: 090 ----
mean loss: 223.46
train mean loss: 224.16
epoch train time: 0:00:00.682248
elapsed time: 0:02:59.836121
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 22:53:53.985480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.84
 ---- batch: 020 ----
mean loss: 227.07
 ---- batch: 030 ----
mean loss: 227.33
 ---- batch: 040 ----
mean loss: 220.49
 ---- batch: 050 ----
mean loss: 227.90
 ---- batch: 060 ----
mean loss: 219.91
 ---- batch: 070 ----
mean loss: 217.70
 ---- batch: 080 ----
mean loss: 233.82
 ---- batch: 090 ----
mean loss: 223.73
train mean loss: 224.35
epoch train time: 0:00:00.676939
elapsed time: 0:03:00.513273
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 22:53:54.662628
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.27
 ---- batch: 020 ----
mean loss: 227.02
 ---- batch: 030 ----
mean loss: 226.93
 ---- batch: 040 ----
mean loss: 224.41
 ---- batch: 050 ----
mean loss: 221.04
 ---- batch: 060 ----
mean loss: 224.36
 ---- batch: 070 ----
mean loss: 221.24
 ---- batch: 080 ----
mean loss: 228.64
 ---- batch: 090 ----
mean loss: 224.25
train mean loss: 224.48
epoch train time: 0:00:00.679669
elapsed time: 0:03:01.193109
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 22:53:55.342461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.82
 ---- batch: 020 ----
mean loss: 220.28
 ---- batch: 030 ----
mean loss: 231.62
 ---- batch: 040 ----
mean loss: 227.83
 ---- batch: 050 ----
mean loss: 233.01
 ---- batch: 060 ----
mean loss: 215.75
 ---- batch: 070 ----
mean loss: 219.29
 ---- batch: 080 ----
mean loss: 232.11
 ---- batch: 090 ----
mean loss: 223.36
train mean loss: 224.13
epoch train time: 0:00:00.680303
elapsed time: 0:03:01.873572
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 22:53:56.022931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.83
 ---- batch: 020 ----
mean loss: 219.75
 ---- batch: 030 ----
mean loss: 226.75
 ---- batch: 040 ----
mean loss: 222.46
 ---- batch: 050 ----
mean loss: 227.21
 ---- batch: 060 ----
mean loss: 222.01
 ---- batch: 070 ----
mean loss: 224.45
 ---- batch: 080 ----
mean loss: 230.74
 ---- batch: 090 ----
mean loss: 222.77
train mean loss: 224.46
epoch train time: 0:00:00.676533
elapsed time: 0:03:02.550284
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 22:53:56.699630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 214.50
 ---- batch: 020 ----
mean loss: 221.95
 ---- batch: 030 ----
mean loss: 223.97
 ---- batch: 040 ----
mean loss: 225.01
 ---- batch: 050 ----
mean loss: 225.31
 ---- batch: 060 ----
mean loss: 231.97
 ---- batch: 070 ----
mean loss: 224.90
 ---- batch: 080 ----
mean loss: 233.23
 ---- batch: 090 ----
mean loss: 214.09
train mean loss: 224.15
epoch train time: 0:00:00.676091
elapsed time: 0:03:03.226573
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 22:53:57.375916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.66
 ---- batch: 020 ----
mean loss: 224.54
 ---- batch: 030 ----
mean loss: 222.02
 ---- batch: 040 ----
mean loss: 219.97
 ---- batch: 050 ----
mean loss: 220.05
 ---- batch: 060 ----
mean loss: 225.05
 ---- batch: 070 ----
mean loss: 223.57
 ---- batch: 080 ----
mean loss: 235.95
 ---- batch: 090 ----
mean loss: 226.08
train mean loss: 224.20
epoch train time: 0:00:00.683211
elapsed time: 0:03:03.909934
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 22:53:58.059278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 225.64
 ---- batch: 020 ----
mean loss: 222.32
 ---- batch: 030 ----
mean loss: 228.26
 ---- batch: 040 ----
mean loss: 223.72
 ---- batch: 050 ----
mean loss: 226.95
 ---- batch: 060 ----
mean loss: 220.67
 ---- batch: 070 ----
mean loss: 226.00
 ---- batch: 080 ----
mean loss: 214.10
 ---- batch: 090 ----
mean loss: 233.75
train mean loss: 224.19
epoch train time: 0:00:00.682726
elapsed time: 0:03:04.592821
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 22:53:58.742186
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.87
 ---- batch: 020 ----
mean loss: 217.97
 ---- batch: 030 ----
mean loss: 222.31
 ---- batch: 040 ----
mean loss: 224.52
 ---- batch: 050 ----
mean loss: 225.16
 ---- batch: 060 ----
mean loss: 221.25
 ---- batch: 070 ----
mean loss: 224.66
 ---- batch: 080 ----
mean loss: 235.93
 ---- batch: 090 ----
mean loss: 227.06
train mean loss: 224.14
epoch train time: 0:00:00.676959
elapsed time: 0:03:05.269955
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 22:53:59.419327
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.18
 ---- batch: 020 ----
mean loss: 228.46
 ---- batch: 030 ----
mean loss: 220.64
 ---- batch: 040 ----
mean loss: 222.79
 ---- batch: 050 ----
mean loss: 226.61
 ---- batch: 060 ----
mean loss: 223.11
 ---- batch: 070 ----
mean loss: 224.35
 ---- batch: 080 ----
mean loss: 220.68
 ---- batch: 090 ----
mean loss: 224.76
train mean loss: 224.13
epoch train time: 0:00:00.684064
elapsed time: 0:03:05.954197
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 22:54:00.103548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.93
 ---- batch: 020 ----
mean loss: 230.42
 ---- batch: 030 ----
mean loss: 224.99
 ---- batch: 040 ----
mean loss: 218.72
 ---- batch: 050 ----
mean loss: 221.26
 ---- batch: 060 ----
mean loss: 224.62
 ---- batch: 070 ----
mean loss: 217.43
 ---- batch: 080 ----
mean loss: 226.56
 ---- batch: 090 ----
mean loss: 229.99
train mean loss: 224.30
epoch train time: 0:00:00.681481
elapsed time: 0:03:06.635839
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 22:54:00.785214
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.38
 ---- batch: 020 ----
mean loss: 228.01
 ---- batch: 030 ----
mean loss: 221.85
 ---- batch: 040 ----
mean loss: 226.79
 ---- batch: 050 ----
mean loss: 231.57
 ---- batch: 060 ----
mean loss: 219.75
 ---- batch: 070 ----
mean loss: 225.19
 ---- batch: 080 ----
mean loss: 223.19
 ---- batch: 090 ----
mean loss: 222.80
train mean loss: 224.13
epoch train time: 0:00:00.679181
elapsed time: 0:03:07.315208
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 22:54:01.464554
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 236.43
 ---- batch: 020 ----
mean loss: 218.86
 ---- batch: 030 ----
mean loss: 227.84
 ---- batch: 040 ----
mean loss: 224.08
 ---- batch: 050 ----
mean loss: 219.01
 ---- batch: 060 ----
mean loss: 224.63
 ---- batch: 070 ----
mean loss: 228.03
 ---- batch: 080 ----
mean loss: 218.84
 ---- batch: 090 ----
mean loss: 223.03
train mean loss: 224.08
epoch train time: 0:00:00.689136
elapsed time: 0:03:08.004495
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 22:54:02.153837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.07
 ---- batch: 020 ----
mean loss: 225.16
 ---- batch: 030 ----
mean loss: 220.37
 ---- batch: 040 ----
mean loss: 229.74
 ---- batch: 050 ----
mean loss: 228.92
 ---- batch: 060 ----
mean loss: 218.68
 ---- batch: 070 ----
mean loss: 227.57
 ---- batch: 080 ----
mean loss: 218.20
 ---- batch: 090 ----
mean loss: 232.61
train mean loss: 224.00
epoch train time: 0:00:00.698279
elapsed time: 0:03:08.702927
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 22:54:02.852288
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 228.00
 ---- batch: 020 ----
mean loss: 225.24
 ---- batch: 030 ----
mean loss: 230.64
 ---- batch: 040 ----
mean loss: 220.14
 ---- batch: 050 ----
mean loss: 217.13
 ---- batch: 060 ----
mean loss: 218.84
 ---- batch: 070 ----
mean loss: 224.00
 ---- batch: 080 ----
mean loss: 229.08
 ---- batch: 090 ----
mean loss: 225.76
train mean loss: 224.23
epoch train time: 0:00:00.702289
elapsed time: 0:03:09.405404
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 22:54:03.554827
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.15
 ---- batch: 020 ----
mean loss: 221.20
 ---- batch: 030 ----
mean loss: 223.44
 ---- batch: 040 ----
mean loss: 222.53
 ---- batch: 050 ----
mean loss: 219.60
 ---- batch: 060 ----
mean loss: 217.32
 ---- batch: 070 ----
mean loss: 221.54
 ---- batch: 080 ----
mean loss: 224.11
 ---- batch: 090 ----
mean loss: 230.90
train mean loss: 223.92
epoch train time: 0:00:00.702198
elapsed time: 0:03:10.107833
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 22:54:04.257195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 232.99
 ---- batch: 020 ----
mean loss: 225.87
 ---- batch: 030 ----
mean loss: 227.70
 ---- batch: 040 ----
mean loss: 220.48
 ---- batch: 050 ----
mean loss: 217.93
 ---- batch: 060 ----
mean loss: 230.16
 ---- batch: 070 ----
mean loss: 220.44
 ---- batch: 080 ----
mean loss: 220.37
 ---- batch: 090 ----
mean loss: 226.96
train mean loss: 224.01
epoch train time: 0:00:00.698235
elapsed time: 0:03:10.806260
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 22:54:04.955606
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.79
 ---- batch: 020 ----
mean loss: 222.90
 ---- batch: 030 ----
mean loss: 220.93
 ---- batch: 040 ----
mean loss: 219.97
 ---- batch: 050 ----
mean loss: 225.88
 ---- batch: 060 ----
mean loss: 223.64
 ---- batch: 070 ----
mean loss: 218.12
 ---- batch: 080 ----
mean loss: 226.50
 ---- batch: 090 ----
mean loss: 225.52
train mean loss: 224.10
epoch train time: 0:00:00.695685
elapsed time: 0:03:11.502111
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 22:54:05.651444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.11
 ---- batch: 020 ----
mean loss: 224.27
 ---- batch: 030 ----
mean loss: 222.35
 ---- batch: 040 ----
mean loss: 230.90
 ---- batch: 050 ----
mean loss: 227.88
 ---- batch: 060 ----
mean loss: 221.61
 ---- batch: 070 ----
mean loss: 228.21
 ---- batch: 080 ----
mean loss: 220.93
 ---- batch: 090 ----
mean loss: 221.08
train mean loss: 224.11
epoch train time: 0:00:00.704081
elapsed time: 0:03:12.206345
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 22:54:06.355693
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.12
 ---- batch: 020 ----
mean loss: 221.92
 ---- batch: 030 ----
mean loss: 221.65
 ---- batch: 040 ----
mean loss: 219.38
 ---- batch: 050 ----
mean loss: 229.67
 ---- batch: 060 ----
mean loss: 232.99
 ---- batch: 070 ----
mean loss: 220.83
 ---- batch: 080 ----
mean loss: 224.59
 ---- batch: 090 ----
mean loss: 222.49
train mean loss: 223.99
epoch train time: 0:00:00.709993
elapsed time: 0:03:12.916501
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 22:54:07.065847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.17
 ---- batch: 020 ----
mean loss: 226.76
 ---- batch: 030 ----
mean loss: 223.27
 ---- batch: 040 ----
mean loss: 230.32
 ---- batch: 050 ----
mean loss: 224.43
 ---- batch: 060 ----
mean loss: 215.55
 ---- batch: 070 ----
mean loss: 230.87
 ---- batch: 080 ----
mean loss: 222.97
 ---- batch: 090 ----
mean loss: 223.43
train mean loss: 223.88
epoch train time: 0:00:00.682573
elapsed time: 0:03:13.599233
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 22:54:07.748578
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.21
 ---- batch: 020 ----
mean loss: 217.43
 ---- batch: 030 ----
mean loss: 228.48
 ---- batch: 040 ----
mean loss: 227.35
 ---- batch: 050 ----
mean loss: 222.73
 ---- batch: 060 ----
mean loss: 215.83
 ---- batch: 070 ----
mean loss: 224.53
 ---- batch: 080 ----
mean loss: 226.93
 ---- batch: 090 ----
mean loss: 223.61
train mean loss: 224.16
epoch train time: 0:00:00.678646
elapsed time: 0:03:14.278045
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 22:54:08.427388
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 230.61
 ---- batch: 020 ----
mean loss: 222.43
 ---- batch: 030 ----
mean loss: 226.32
 ---- batch: 040 ----
mean loss: 221.92
 ---- batch: 050 ----
mean loss: 226.27
 ---- batch: 060 ----
mean loss: 215.87
 ---- batch: 070 ----
mean loss: 221.76
 ---- batch: 080 ----
mean loss: 223.68
 ---- batch: 090 ----
mean loss: 226.23
train mean loss: 224.22
epoch train time: 0:00:00.675250
elapsed time: 0:03:14.953445
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 22:54:09.102819
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.74
 ---- batch: 020 ----
mean loss: 225.21
 ---- batch: 030 ----
mean loss: 221.30
 ---- batch: 040 ----
mean loss: 215.97
 ---- batch: 050 ----
mean loss: 221.55
 ---- batch: 060 ----
mean loss: 232.19
 ---- batch: 070 ----
mean loss: 219.86
 ---- batch: 080 ----
mean loss: 225.13
 ---- batch: 090 ----
mean loss: 229.71
train mean loss: 224.10
epoch train time: 0:00:00.677077
elapsed time: 0:03:15.630754
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 22:54:09.780110
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 226.77
 ---- batch: 020 ----
mean loss: 221.14
 ---- batch: 030 ----
mean loss: 226.18
 ---- batch: 040 ----
mean loss: 222.88
 ---- batch: 050 ----
mean loss: 224.00
 ---- batch: 060 ----
mean loss: 218.76
 ---- batch: 070 ----
mean loss: 225.80
 ---- batch: 080 ----
mean loss: 227.53
 ---- batch: 090 ----
mean loss: 219.58
train mean loss: 224.18
epoch train time: 0:00:00.682398
elapsed time: 0:03:16.313313
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 22:54:10.462655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 227.01
 ---- batch: 020 ----
mean loss: 223.63
 ---- batch: 030 ----
mean loss: 226.73
 ---- batch: 040 ----
mean loss: 216.86
 ---- batch: 050 ----
mean loss: 223.86
 ---- batch: 060 ----
mean loss: 216.55
 ---- batch: 070 ----
mean loss: 227.87
 ---- batch: 080 ----
mean loss: 219.84
 ---- batch: 090 ----
mean loss: 228.60
train mean loss: 224.22
epoch train time: 0:00:00.682205
elapsed time: 0:03:16.995673
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 22:54:11.145037
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.33
 ---- batch: 020 ----
mean loss: 218.62
 ---- batch: 030 ----
mean loss: 227.26
 ---- batch: 040 ----
mean loss: 217.71
 ---- batch: 050 ----
mean loss: 221.84
 ---- batch: 060 ----
mean loss: 221.13
 ---- batch: 070 ----
mean loss: 225.31
 ---- batch: 080 ----
mean loss: 222.16
 ---- batch: 090 ----
mean loss: 232.29
train mean loss: 223.97
epoch train time: 0:00:00.685954
elapsed time: 0:03:17.681801
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 22:54:11.831146
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 222.39
 ---- batch: 020 ----
mean loss: 224.50
 ---- batch: 030 ----
mean loss: 222.35
 ---- batch: 040 ----
mean loss: 216.21
 ---- batch: 050 ----
mean loss: 219.96
 ---- batch: 060 ----
mean loss: 229.80
 ---- batch: 070 ----
mean loss: 229.57
 ---- batch: 080 ----
mean loss: 217.71
 ---- batch: 090 ----
mean loss: 231.37
train mean loss: 223.95
epoch train time: 0:00:00.678177
elapsed time: 0:03:18.360164
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 22:54:12.509515
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.88
 ---- batch: 020 ----
mean loss: 233.43
 ---- batch: 030 ----
mean loss: 219.19
 ---- batch: 040 ----
mean loss: 222.45
 ---- batch: 050 ----
mean loss: 230.23
 ---- batch: 060 ----
mean loss: 218.41
 ---- batch: 070 ----
mean loss: 222.35
 ---- batch: 080 ----
mean loss: 219.68
 ---- batch: 090 ----
mean loss: 226.28
train mean loss: 223.82
epoch train time: 0:00:00.686515
elapsed time: 0:03:19.046843
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 22:54:13.196189
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 219.60
 ---- batch: 020 ----
mean loss: 223.91
 ---- batch: 030 ----
mean loss: 222.83
 ---- batch: 040 ----
mean loss: 226.62
 ---- batch: 050 ----
mean loss: 223.40
 ---- batch: 060 ----
mean loss: 223.81
 ---- batch: 070 ----
mean loss: 219.28
 ---- batch: 080 ----
mean loss: 228.77
 ---- batch: 090 ----
mean loss: 228.17
train mean loss: 223.88
epoch train time: 0:00:00.691211
elapsed time: 0:03:19.738207
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 22:54:13.887576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 231.60
 ---- batch: 020 ----
mean loss: 226.64
 ---- batch: 030 ----
mean loss: 219.65
 ---- batch: 040 ----
mean loss: 229.50
 ---- batch: 050 ----
mean loss: 225.30
 ---- batch: 060 ----
mean loss: 214.23
 ---- batch: 070 ----
mean loss: 219.89
 ---- batch: 080 ----
mean loss: 219.80
 ---- batch: 090 ----
mean loss: 227.87
train mean loss: 223.93
epoch train time: 0:00:00.674382
elapsed time: 0:03:20.412777
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 22:54:14.562134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 224.26
 ---- batch: 020 ----
mean loss: 221.92
 ---- batch: 030 ----
mean loss: 224.00
 ---- batch: 040 ----
mean loss: 230.53
 ---- batch: 050 ----
mean loss: 226.25
 ---- batch: 060 ----
mean loss: 228.17
 ---- batch: 070 ----
mean loss: 213.32
 ---- batch: 080 ----
mean loss: 216.52
 ---- batch: 090 ----
mean loss: 228.72
train mean loss: 223.75
epoch train time: 0:00:00.689756
elapsed time: 0:03:21.102697
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 22:54:15.252041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 221.46
 ---- batch: 020 ----
mean loss: 226.68
 ---- batch: 030 ----
mean loss: 219.80
 ---- batch: 040 ----
mean loss: 229.13
 ---- batch: 050 ----
mean loss: 217.64
 ---- batch: 060 ----
mean loss: 225.48
 ---- batch: 070 ----
mean loss: 232.07
 ---- batch: 080 ----
mean loss: 219.01
 ---- batch: 090 ----
mean loss: 220.43
train mean loss: 223.80
epoch train time: 0:00:00.690449
elapsed time: 0:03:21.793323
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 22:54:15.942668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 216.72
 ---- batch: 020 ----
mean loss: 228.34
 ---- batch: 030 ----
mean loss: 219.71
 ---- batch: 040 ----
mean loss: 216.80
 ---- batch: 050 ----
mean loss: 233.35
 ---- batch: 060 ----
mean loss: 224.72
 ---- batch: 070 ----
mean loss: 218.89
 ---- batch: 080 ----
mean loss: 228.16
 ---- batch: 090 ----
mean loss: 229.82
train mean loss: 223.65
epoch train time: 0:00:00.680702
elapsed time: 0:03:22.476427
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_7/checkpoint.pth.tar
**** end time: 2019-09-25 22:54:16.625735 ****
