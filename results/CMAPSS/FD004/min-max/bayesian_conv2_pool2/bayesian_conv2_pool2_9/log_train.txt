Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_9', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 13868
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-26 22:08:14.467481 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1            [-1, 8, 11, 11]           1,120
           Sigmoid-2            [-1, 8, 11, 11]               0
         AvgPool2d-3             [-1, 8, 5, 11]               0
    BayesianConv2d-4            [-1, 14, 4, 11]             448
           Sigmoid-5            [-1, 14, 4, 11]               0
         AvgPool2d-6            [-1, 14, 2, 11]               0
           Flatten-7                  [-1, 308]               0
    BayesianLinear-8                    [-1, 1]             616
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 2,184
Trainable params: 2,184
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 22:08:14.479935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4678.96
 ---- batch: 020 ----
mean loss: 4530.19
 ---- batch: 030 ----
mean loss: 4371.90
 ---- batch: 040 ----
mean loss: 4183.81
 ---- batch: 050 ----
mean loss: 4014.88
 ---- batch: 060 ----
mean loss: 3809.70
 ---- batch: 070 ----
mean loss: 3658.94
 ---- batch: 080 ----
mean loss: 3468.15
 ---- batch: 090 ----
mean loss: 3286.48
 ---- batch: 100 ----
mean loss: 3141.37
 ---- batch: 110 ----
mean loss: 2974.57
train mean loss: 3802.39
epoch train time: 0:00:35.447157
elapsed time: 0:00:35.462925
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 22:08:49.930451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2721.00
 ---- batch: 020 ----
mean loss: 2534.26
 ---- batch: 030 ----
mean loss: 2406.20
 ---- batch: 040 ----
mean loss: 2256.26
 ---- batch: 050 ----
mean loss: 2132.47
 ---- batch: 060 ----
mean loss: 1984.41
 ---- batch: 070 ----
mean loss: 1850.91
 ---- batch: 080 ----
mean loss: 1738.93
 ---- batch: 090 ----
mean loss: 1641.63
 ---- batch: 100 ----
mean loss: 1534.88
 ---- batch: 110 ----
mean loss: 1436.94
train mean loss: 2005.46
epoch train time: 0:00:02.558334
elapsed time: 0:00:38.021487
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 22:08:52.489221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1344.67
 ---- batch: 020 ----
mean loss: 1284.14
 ---- batch: 030 ----
mean loss: 1227.36
 ---- batch: 040 ----
mean loss: 1182.80
 ---- batch: 050 ----
mean loss: 1122.02
 ---- batch: 060 ----
mean loss: 1066.06
 ---- batch: 070 ----
mean loss: 1068.02
 ---- batch: 080 ----
mean loss: 1023.86
 ---- batch: 090 ----
mean loss: 992.71
 ---- batch: 100 ----
mean loss: 973.12
 ---- batch: 110 ----
mean loss: 952.07
train mean loss: 1107.48
epoch train time: 0:00:02.534830
elapsed time: 0:00:40.556784
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 22:08:55.024559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.63
 ---- batch: 020 ----
mean loss: 922.51
 ---- batch: 030 ----
mean loss: 918.54
 ---- batch: 040 ----
mean loss: 899.33
 ---- batch: 050 ----
mean loss: 880.52
 ---- batch: 060 ----
mean loss: 881.26
 ---- batch: 070 ----
mean loss: 885.34
 ---- batch: 080 ----
mean loss: 851.95
 ---- batch: 090 ----
mean loss: 891.60
 ---- batch: 100 ----
mean loss: 883.83
 ---- batch: 110 ----
mean loss: 866.49
train mean loss: 892.30
epoch train time: 0:00:02.503475
elapsed time: 0:00:43.060767
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 22:08:57.528517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.85
 ---- batch: 020 ----
mean loss: 860.99
 ---- batch: 030 ----
mean loss: 872.97
 ---- batch: 040 ----
mean loss: 875.50
 ---- batch: 050 ----
mean loss: 869.95
 ---- batch: 060 ----
mean loss: 853.87
 ---- batch: 070 ----
mean loss: 858.81
 ---- batch: 080 ----
mean loss: 851.11
 ---- batch: 090 ----
mean loss: 858.49
 ---- batch: 100 ----
mean loss: 869.35
 ---- batch: 110 ----
mean loss: 863.59
train mean loss: 863.36
epoch train time: 0:00:02.516117
elapsed time: 0:00:45.577315
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 22:09:00.045030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.32
 ---- batch: 020 ----
mean loss: 851.75
 ---- batch: 030 ----
mean loss: 863.22
 ---- batch: 040 ----
mean loss: 835.57
 ---- batch: 050 ----
mean loss: 849.37
 ---- batch: 060 ----
mean loss: 872.74
 ---- batch: 070 ----
mean loss: 845.99
 ---- batch: 080 ----
mean loss: 870.40
 ---- batch: 090 ----
mean loss: 846.40
 ---- batch: 100 ----
mean loss: 860.34
 ---- batch: 110 ----
mean loss: 859.48
train mean loss: 856.10
epoch train time: 0:00:02.508218
elapsed time: 0:00:48.086202
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 22:09:02.553969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.37
 ---- batch: 020 ----
mean loss: 843.68
 ---- batch: 030 ----
mean loss: 843.55
 ---- batch: 040 ----
mean loss: 854.89
 ---- batch: 050 ----
mean loss: 825.39
 ---- batch: 060 ----
mean loss: 852.30
 ---- batch: 070 ----
mean loss: 845.06
 ---- batch: 080 ----
mean loss: 864.42
 ---- batch: 090 ----
mean loss: 859.29
 ---- batch: 100 ----
mean loss: 869.38
 ---- batch: 110 ----
mean loss: 847.65
train mean loss: 851.83
epoch train time: 0:00:02.517029
elapsed time: 0:00:50.603704
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 22:09:05.071413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.95
 ---- batch: 020 ----
mean loss: 831.53
 ---- batch: 030 ----
mean loss: 862.79
 ---- batch: 040 ----
mean loss: 848.22
 ---- batch: 050 ----
mean loss: 850.08
 ---- batch: 060 ----
mean loss: 857.51
 ---- batch: 070 ----
mean loss: 822.58
 ---- batch: 080 ----
mean loss: 847.03
 ---- batch: 090 ----
mean loss: 854.46
 ---- batch: 100 ----
mean loss: 855.14
 ---- batch: 110 ----
mean loss: 846.51
train mean loss: 846.27
epoch train time: 0:00:02.515566
elapsed time: 0:00:53.119759
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 22:09:07.587483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.35
 ---- batch: 020 ----
mean loss: 851.54
 ---- batch: 030 ----
mean loss: 828.60
 ---- batch: 040 ----
mean loss: 828.32
 ---- batch: 050 ----
mean loss: 847.26
 ---- batch: 060 ----
mean loss: 829.68
 ---- batch: 070 ----
mean loss: 850.81
 ---- batch: 080 ----
mean loss: 836.84
 ---- batch: 090 ----
mean loss: 834.74
 ---- batch: 100 ----
mean loss: 856.62
 ---- batch: 110 ----
mean loss: 847.74
train mean loss: 841.96
epoch train time: 0:00:02.511835
elapsed time: 0:00:55.632011
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 22:09:10.099735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.75
 ---- batch: 020 ----
mean loss: 843.81
 ---- batch: 030 ----
mean loss: 816.82
 ---- batch: 040 ----
mean loss: 846.00
 ---- batch: 050 ----
mean loss: 829.64
 ---- batch: 060 ----
mean loss: 836.14
 ---- batch: 070 ----
mean loss: 832.30
 ---- batch: 080 ----
mean loss: 863.34
 ---- batch: 090 ----
mean loss: 824.82
 ---- batch: 100 ----
mean loss: 818.05
 ---- batch: 110 ----
mean loss: 839.35
train mean loss: 834.89
epoch train time: 0:00:02.505979
elapsed time: 0:00:58.138433
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 22:09:12.606136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.08
 ---- batch: 020 ----
mean loss: 817.90
 ---- batch: 030 ----
mean loss: 828.67
 ---- batch: 040 ----
mean loss: 849.09
 ---- batch: 050 ----
mean loss: 822.13
 ---- batch: 060 ----
mean loss: 823.97
 ---- batch: 070 ----
mean loss: 832.85
 ---- batch: 080 ----
mean loss: 823.43
 ---- batch: 090 ----
mean loss: 848.06
 ---- batch: 100 ----
mean loss: 826.53
 ---- batch: 110 ----
mean loss: 823.75
train mean loss: 829.84
epoch train time: 0:00:02.526139
elapsed time: 0:01:00.664969
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 22:09:15.132598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.84
 ---- batch: 020 ----
mean loss: 804.29
 ---- batch: 030 ----
mean loss: 839.60
 ---- batch: 040 ----
mean loss: 833.13
 ---- batch: 050 ----
mean loss: 828.58
 ---- batch: 060 ----
mean loss: 817.34
 ---- batch: 070 ----
mean loss: 817.95
 ---- batch: 080 ----
mean loss: 815.65
 ---- batch: 090 ----
mean loss: 829.43
 ---- batch: 100 ----
mean loss: 819.84
 ---- batch: 110 ----
mean loss: 793.86
train mean loss: 823.90
epoch train time: 0:00:02.511060
elapsed time: 0:01:03.176357
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 22:09:17.644082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.97
 ---- batch: 020 ----
mean loss: 824.43
 ---- batch: 030 ----
mean loss: 825.23
 ---- batch: 040 ----
mean loss: 816.39
 ---- batch: 050 ----
mean loss: 816.59
 ---- batch: 060 ----
mean loss: 812.64
 ---- batch: 070 ----
mean loss: 830.18
 ---- batch: 080 ----
mean loss: 792.39
 ---- batch: 090 ----
mean loss: 814.09
 ---- batch: 100 ----
mean loss: 828.73
 ---- batch: 110 ----
mean loss: 803.82
train mean loss: 818.77
epoch train time: 0:00:02.523097
elapsed time: 0:01:05.699876
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 22:09:20.167622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 801.40
 ---- batch: 020 ----
mean loss: 817.34
 ---- batch: 030 ----
mean loss: 811.39
 ---- batch: 040 ----
mean loss: 796.32
 ---- batch: 050 ----
mean loss: 803.02
 ---- batch: 060 ----
mean loss: 818.75
 ---- batch: 070 ----
mean loss: 819.85
 ---- batch: 080 ----
mean loss: 819.16
 ---- batch: 090 ----
mean loss: 802.09
 ---- batch: 100 ----
mean loss: 827.71
 ---- batch: 110 ----
mean loss: 824.68
train mean loss: 813.75
epoch train time: 0:00:02.510161
elapsed time: 0:01:08.210479
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 22:09:22.678182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.59
 ---- batch: 020 ----
mean loss: 802.33
 ---- batch: 030 ----
mean loss: 816.52
 ---- batch: 040 ----
mean loss: 821.32
 ---- batch: 050 ----
mean loss: 812.71
 ---- batch: 060 ----
mean loss: 800.37
 ---- batch: 070 ----
mean loss: 800.99
 ---- batch: 080 ----
mean loss: 796.35
 ---- batch: 090 ----
mean loss: 804.25
 ---- batch: 100 ----
mean loss: 804.87
 ---- batch: 110 ----
mean loss: 822.49
train mean loss: 807.71
epoch train time: 0:00:02.530722
elapsed time: 0:01:10.741603
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 22:09:25.209317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 806.21
 ---- batch: 020 ----
mean loss: 809.98
 ---- batch: 030 ----
mean loss: 798.09
 ---- batch: 040 ----
mean loss: 787.15
 ---- batch: 050 ----
mean loss: 799.85
 ---- batch: 060 ----
mean loss: 809.91
 ---- batch: 070 ----
mean loss: 810.29
 ---- batch: 080 ----
mean loss: 788.53
 ---- batch: 090 ----
mean loss: 788.82
 ---- batch: 100 ----
mean loss: 813.31
 ---- batch: 110 ----
mean loss: 790.23
train mean loss: 801.03
epoch train time: 0:00:02.528464
elapsed time: 0:01:13.270492
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 22:09:27.738219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 792.43
 ---- batch: 020 ----
mean loss: 765.36
 ---- batch: 030 ----
mean loss: 796.97
 ---- batch: 040 ----
mean loss: 815.72
 ---- batch: 050 ----
mean loss: 816.69
 ---- batch: 060 ----
mean loss: 810.66
 ---- batch: 070 ----
mean loss: 807.82
 ---- batch: 080 ----
mean loss: 794.36
 ---- batch: 090 ----
mean loss: 780.61
 ---- batch: 100 ----
mean loss: 787.74
 ---- batch: 110 ----
mean loss: 785.81
train mean loss: 796.01
epoch train time: 0:00:02.526968
elapsed time: 0:01:15.797912
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 22:09:30.265686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 772.71
 ---- batch: 020 ----
mean loss: 799.49
 ---- batch: 030 ----
mean loss: 778.66
 ---- batch: 040 ----
mean loss: 796.71
 ---- batch: 050 ----
mean loss: 804.75
 ---- batch: 060 ----
mean loss: 774.62
 ---- batch: 070 ----
mean loss: 800.12
 ---- batch: 080 ----
mean loss: 786.33
 ---- batch: 090 ----
mean loss: 785.55
 ---- batch: 100 ----
mean loss: 802.78
 ---- batch: 110 ----
mean loss: 796.30
train mean loss: 790.33
epoch train time: 0:00:02.545345
elapsed time: 0:01:18.343789
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 22:09:32.811507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.17
 ---- batch: 020 ----
mean loss: 797.54
 ---- batch: 030 ----
mean loss: 790.80
 ---- batch: 040 ----
mean loss: 767.80
 ---- batch: 050 ----
mean loss: 775.00
 ---- batch: 060 ----
mean loss: 788.63
 ---- batch: 070 ----
mean loss: 784.03
 ---- batch: 080 ----
mean loss: 786.02
 ---- batch: 090 ----
mean loss: 793.12
 ---- batch: 100 ----
mean loss: 783.93
 ---- batch: 110 ----
mean loss: 802.35
train mean loss: 785.86
epoch train time: 0:00:02.542158
elapsed time: 0:01:20.886353
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 22:09:35.354088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 779.84
 ---- batch: 020 ----
mean loss: 791.38
 ---- batch: 030 ----
mean loss: 794.73
 ---- batch: 040 ----
mean loss: 778.75
 ---- batch: 050 ----
mean loss: 770.88
 ---- batch: 060 ----
mean loss: 790.84
 ---- batch: 070 ----
mean loss: 784.57
 ---- batch: 080 ----
mean loss: 785.14
 ---- batch: 090 ----
mean loss: 774.92
 ---- batch: 100 ----
mean loss: 786.30
 ---- batch: 110 ----
mean loss: 784.47
train mean loss: 782.82
epoch train time: 0:00:02.517152
elapsed time: 0:01:23.403939
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 22:09:37.871668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 749.74
 ---- batch: 020 ----
mean loss: 812.40
 ---- batch: 030 ----
mean loss: 780.75
 ---- batch: 040 ----
mean loss: 802.08
 ---- batch: 050 ----
mean loss: 782.21
 ---- batch: 060 ----
mean loss: 783.76
 ---- batch: 070 ----
mean loss: 772.39
 ---- batch: 080 ----
mean loss: 781.66
 ---- batch: 090 ----
mean loss: 762.49
 ---- batch: 100 ----
mean loss: 764.78
 ---- batch: 110 ----
mean loss: 756.89
train mean loss: 777.02
epoch train time: 0:00:02.509453
elapsed time: 0:01:25.913805
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 22:09:40.381434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 764.80
 ---- batch: 020 ----
mean loss: 789.44
 ---- batch: 030 ----
mean loss: 767.76
 ---- batch: 040 ----
mean loss: 792.22
 ---- batch: 050 ----
mean loss: 776.70
 ---- batch: 060 ----
mean loss: 769.63
 ---- batch: 070 ----
mean loss: 778.04
 ---- batch: 080 ----
mean loss: 770.03
 ---- batch: 090 ----
mean loss: 762.96
 ---- batch: 100 ----
mean loss: 760.94
 ---- batch: 110 ----
mean loss: 776.25
train mean loss: 772.82
epoch train time: 0:00:02.514073
elapsed time: 0:01:28.428231
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 22:09:42.895963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.50
 ---- batch: 020 ----
mean loss: 757.38
 ---- batch: 030 ----
mean loss: 747.05
 ---- batch: 040 ----
mean loss: 776.81
 ---- batch: 050 ----
mean loss: 783.83
 ---- batch: 060 ----
mean loss: 765.92
 ---- batch: 070 ----
mean loss: 787.50
 ---- batch: 080 ----
mean loss: 758.97
 ---- batch: 090 ----
mean loss: 759.56
 ---- batch: 100 ----
mean loss: 791.44
 ---- batch: 110 ----
mean loss: 758.70
train mean loss: 767.84
epoch train time: 0:00:02.514153
elapsed time: 0:01:30.942811
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 22:09:45.410533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 745.65
 ---- batch: 020 ----
mean loss: 760.64
 ---- batch: 030 ----
mean loss: 777.22
 ---- batch: 040 ----
mean loss: 759.67
 ---- batch: 050 ----
mean loss: 776.12
 ---- batch: 060 ----
mean loss: 757.80
 ---- batch: 070 ----
mean loss: 759.73
 ---- batch: 080 ----
mean loss: 769.34
 ---- batch: 090 ----
mean loss: 760.82
 ---- batch: 100 ----
mean loss: 769.12
 ---- batch: 110 ----
mean loss: 751.09
train mean loss: 762.62
epoch train time: 0:00:02.516984
elapsed time: 0:01:33.460211
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 22:09:47.927916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.08
 ---- batch: 020 ----
mean loss: 756.64
 ---- batch: 030 ----
mean loss: 745.40
 ---- batch: 040 ----
mean loss: 765.18
 ---- batch: 050 ----
mean loss: 759.57
 ---- batch: 060 ----
mean loss: 765.57
 ---- batch: 070 ----
mean loss: 741.05
 ---- batch: 080 ----
mean loss: 760.40
 ---- batch: 090 ----
mean loss: 767.61
 ---- batch: 100 ----
mean loss: 743.33
 ---- batch: 110 ----
mean loss: 764.69
train mean loss: 757.28
epoch train time: 0:00:02.520656
elapsed time: 0:01:35.981251
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 22:09:50.448966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 758.30
 ---- batch: 020 ----
mean loss: 753.24
 ---- batch: 030 ----
mean loss: 746.63
 ---- batch: 040 ----
mean loss: 752.95
 ---- batch: 050 ----
mean loss: 756.05
 ---- batch: 060 ----
mean loss: 761.28
 ---- batch: 070 ----
mean loss: 765.76
 ---- batch: 080 ----
mean loss: 737.89
 ---- batch: 090 ----
mean loss: 761.48
 ---- batch: 100 ----
mean loss: 745.76
 ---- batch: 110 ----
mean loss: 747.48
train mean loss: 752.54
epoch train time: 0:00:02.520218
elapsed time: 0:01:38.501882
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 22:09:52.969702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 752.17
 ---- batch: 020 ----
mean loss: 753.87
 ---- batch: 030 ----
mean loss: 759.24
 ---- batch: 040 ----
mean loss: 755.04
 ---- batch: 050 ----
mean loss: 741.97
 ---- batch: 060 ----
mean loss: 732.71
 ---- batch: 070 ----
mean loss: 731.22
 ---- batch: 080 ----
mean loss: 765.88
 ---- batch: 090 ----
mean loss: 758.02
 ---- batch: 100 ----
mean loss: 733.80
 ---- batch: 110 ----
mean loss: 735.63
train mean loss: 746.73
epoch train time: 0:00:02.518142
elapsed time: 0:01:41.020619
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 22:09:55.488385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 740.37
 ---- batch: 020 ----
mean loss: 731.79
 ---- batch: 030 ----
mean loss: 759.76
 ---- batch: 040 ----
mean loss: 757.85
 ---- batch: 050 ----
mean loss: 737.40
 ---- batch: 060 ----
mean loss: 739.74
 ---- batch: 070 ----
mean loss: 736.03
 ---- batch: 080 ----
mean loss: 738.93
 ---- batch: 090 ----
mean loss: 746.82
 ---- batch: 100 ----
mean loss: 732.52
 ---- batch: 110 ----
mean loss: 738.33
train mean loss: 740.90
epoch train time: 0:00:02.499605
elapsed time: 0:01:43.520685
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 22:09:57.988403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.10
 ---- batch: 020 ----
mean loss: 733.24
 ---- batch: 030 ----
mean loss: 748.78
 ---- batch: 040 ----
mean loss: 758.34
 ---- batch: 050 ----
mean loss: 754.37
 ---- batch: 060 ----
mean loss: 726.68
 ---- batch: 070 ----
mean loss: 737.85
 ---- batch: 080 ----
mean loss: 736.50
 ---- batch: 090 ----
mean loss: 720.29
 ---- batch: 100 ----
mean loss: 738.94
 ---- batch: 110 ----
mean loss: 731.10
train mean loss: 735.40
epoch train time: 0:00:02.514622
elapsed time: 0:01:46.035766
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 22:10:00.503481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 736.07
 ---- batch: 020 ----
mean loss: 716.00
 ---- batch: 030 ----
mean loss: 726.29
 ---- batch: 040 ----
mean loss: 724.31
 ---- batch: 050 ----
mean loss: 738.02
 ---- batch: 060 ----
mean loss: 726.05
 ---- batch: 070 ----
mean loss: 727.04
 ---- batch: 080 ----
mean loss: 738.11
 ---- batch: 090 ----
mean loss: 730.13
 ---- batch: 100 ----
mean loss: 734.45
 ---- batch: 110 ----
mean loss: 728.21
train mean loss: 728.91
epoch train time: 0:00:02.524982
elapsed time: 0:01:48.561175
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 22:10:03.028886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 727.64
 ---- batch: 020 ----
mean loss: 719.16
 ---- batch: 030 ----
mean loss: 729.10
 ---- batch: 040 ----
mean loss: 730.92
 ---- batch: 050 ----
mean loss: 725.94
 ---- batch: 060 ----
mean loss: 724.03
 ---- batch: 070 ----
mean loss: 689.72
 ---- batch: 080 ----
mean loss: 731.93
 ---- batch: 090 ----
mean loss: 725.56
 ---- batch: 100 ----
mean loss: 722.80
 ---- batch: 110 ----
mean loss: 727.85
train mean loss: 723.20
epoch train time: 0:00:02.511807
elapsed time: 0:01:51.073379
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 22:10:05.541097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 723.85
 ---- batch: 020 ----
mean loss: 702.83
 ---- batch: 030 ----
mean loss: 732.38
 ---- batch: 040 ----
mean loss: 736.03
 ---- batch: 050 ----
mean loss: 701.05
 ---- batch: 060 ----
mean loss: 721.75
 ---- batch: 070 ----
mean loss: 716.83
 ---- batch: 080 ----
mean loss: 715.13
 ---- batch: 090 ----
mean loss: 713.69
 ---- batch: 100 ----
mean loss: 716.03
 ---- batch: 110 ----
mean loss: 706.77
train mean loss: 716.82
epoch train time: 0:00:02.520050
elapsed time: 0:01:53.593837
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 22:10:08.061555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.91
 ---- batch: 020 ----
mean loss: 703.29
 ---- batch: 030 ----
mean loss: 723.16
 ---- batch: 040 ----
mean loss: 725.25
 ---- batch: 050 ----
mean loss: 712.00
 ---- batch: 060 ----
mean loss: 719.68
 ---- batch: 070 ----
mean loss: 683.40
 ---- batch: 080 ----
mean loss: 718.19
 ---- batch: 090 ----
mean loss: 705.67
 ---- batch: 100 ----
mean loss: 717.89
 ---- batch: 110 ----
mean loss: 698.76
train mean loss: 709.76
epoch train time: 0:00:02.511621
elapsed time: 0:01:56.105891
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 22:10:10.573665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.21
 ---- batch: 020 ----
mean loss: 707.18
 ---- batch: 030 ----
mean loss: 698.70
 ---- batch: 040 ----
mean loss: 706.12
 ---- batch: 050 ----
mean loss: 690.59
 ---- batch: 060 ----
mean loss: 695.14
 ---- batch: 070 ----
mean loss: 712.71
 ---- batch: 080 ----
mean loss: 712.32
 ---- batch: 090 ----
mean loss: 710.70
 ---- batch: 100 ----
mean loss: 712.64
 ---- batch: 110 ----
mean loss: 701.87
train mean loss: 703.83
epoch train time: 0:00:02.575014
elapsed time: 0:01:58.681390
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 22:10:13.149121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 697.26
 ---- batch: 020 ----
mean loss: 675.65
 ---- batch: 030 ----
mean loss: 694.17
 ---- batch: 040 ----
mean loss: 711.45
 ---- batch: 050 ----
mean loss: 697.57
 ---- batch: 060 ----
mean loss: 705.92
 ---- batch: 070 ----
mean loss: 684.82
 ---- batch: 080 ----
mean loss: 703.90
 ---- batch: 090 ----
mean loss: 703.96
 ---- batch: 100 ----
mean loss: 702.92
 ---- batch: 110 ----
mean loss: 697.67
train mean loss: 697.19
epoch train time: 0:00:02.575407
elapsed time: 0:02:01.257228
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 22:10:15.724839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 720.02
 ---- batch: 020 ----
mean loss: 700.89
 ---- batch: 030 ----
mean loss: 688.87
 ---- batch: 040 ----
mean loss: 685.27
 ---- batch: 050 ----
mean loss: 689.76
 ---- batch: 060 ----
mean loss: 683.87
 ---- batch: 070 ----
mean loss: 686.21
 ---- batch: 080 ----
mean loss: 688.04
 ---- batch: 090 ----
mean loss: 673.01
 ---- batch: 100 ----
mean loss: 685.51
 ---- batch: 110 ----
mean loss: 682.46
train mean loss: 689.40
epoch train time: 0:00:02.523531
elapsed time: 0:02:03.781110
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 22:10:18.248837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 695.81
 ---- batch: 020 ----
mean loss: 684.52
 ---- batch: 030 ----
mean loss: 692.25
 ---- batch: 040 ----
mean loss: 686.07
 ---- batch: 050 ----
mean loss: 673.66
 ---- batch: 060 ----
mean loss: 691.18
 ---- batch: 070 ----
mean loss: 675.79
 ---- batch: 080 ----
mean loss: 674.19
 ---- batch: 090 ----
mean loss: 693.38
 ---- batch: 100 ----
mean loss: 686.10
 ---- batch: 110 ----
mean loss: 664.16
train mean loss: 683.40
epoch train time: 0:00:02.511195
elapsed time: 0:02:06.292757
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 22:10:20.760471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.61
 ---- batch: 020 ----
mean loss: 686.10
 ---- batch: 030 ----
mean loss: 681.89
 ---- batch: 040 ----
mean loss: 679.43
 ---- batch: 050 ----
mean loss: 694.80
 ---- batch: 060 ----
mean loss: 663.37
 ---- batch: 070 ----
mean loss: 668.43
 ---- batch: 080 ----
mean loss: 668.59
 ---- batch: 090 ----
mean loss: 664.49
 ---- batch: 100 ----
mean loss: 673.88
 ---- batch: 110 ----
mean loss: 688.73
train mean loss: 677.03
epoch train time: 0:00:02.535515
elapsed time: 0:02:08.828681
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 22:10:23.296398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 682.79
 ---- batch: 020 ----
mean loss: 675.03
 ---- batch: 030 ----
mean loss: 675.79
 ---- batch: 040 ----
mean loss: 673.08
 ---- batch: 050 ----
mean loss: 649.27
 ---- batch: 060 ----
mean loss: 664.53
 ---- batch: 070 ----
mean loss: 668.59
 ---- batch: 080 ----
mean loss: 677.76
 ---- batch: 090 ----
mean loss: 663.50
 ---- batch: 100 ----
mean loss: 662.45
 ---- batch: 110 ----
mean loss: 672.04
train mean loss: 669.90
epoch train time: 0:00:02.531807
elapsed time: 0:02:11.360945
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 22:10:25.828747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 666.50
 ---- batch: 020 ----
mean loss: 669.06
 ---- batch: 030 ----
mean loss: 653.10
 ---- batch: 040 ----
mean loss: 662.68
 ---- batch: 050 ----
mean loss: 665.45
 ---- batch: 060 ----
mean loss: 659.81
 ---- batch: 070 ----
mean loss: 660.99
 ---- batch: 080 ----
mean loss: 671.86
 ---- batch: 090 ----
mean loss: 658.44
 ---- batch: 100 ----
mean loss: 668.64
 ---- batch: 110 ----
mean loss: 666.57
train mean loss: 663.09
epoch train time: 0:00:02.514378
elapsed time: 0:02:13.875857
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 22:10:28.343626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 652.34
 ---- batch: 020 ----
mean loss: 670.65
 ---- batch: 030 ----
mean loss: 646.14
 ---- batch: 040 ----
mean loss: 666.13
 ---- batch: 050 ----
mean loss: 660.94
 ---- batch: 060 ----
mean loss: 658.53
 ---- batch: 070 ----
mean loss: 658.00
 ---- batch: 080 ----
mean loss: 662.19
 ---- batch: 090 ----
mean loss: 642.01
 ---- batch: 100 ----
mean loss: 650.45
 ---- batch: 110 ----
mean loss: 657.92
train mean loss: 656.37
epoch train time: 0:00:02.524657
elapsed time: 0:02:16.400993
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 22:10:30.868718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 659.36
 ---- batch: 020 ----
mean loss: 642.44
 ---- batch: 030 ----
mean loss: 652.13
 ---- batch: 040 ----
mean loss: 651.61
 ---- batch: 050 ----
mean loss: 639.96
 ---- batch: 060 ----
mean loss: 640.53
 ---- batch: 070 ----
mean loss: 662.62
 ---- batch: 080 ----
mean loss: 660.71
 ---- batch: 090 ----
mean loss: 650.21
 ---- batch: 100 ----
mean loss: 645.97
 ---- batch: 110 ----
mean loss: 633.72
train mean loss: 648.24
epoch train time: 0:00:02.498801
elapsed time: 0:02:18.900237
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 22:10:33.368005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 648.08
 ---- batch: 020 ----
mean loss: 639.78
 ---- batch: 030 ----
mean loss: 646.51
 ---- batch: 040 ----
mean loss: 649.36
 ---- batch: 050 ----
mean loss: 647.61
 ---- batch: 060 ----
mean loss: 641.12
 ---- batch: 070 ----
mean loss: 645.66
 ---- batch: 080 ----
mean loss: 639.89
 ---- batch: 090 ----
mean loss: 639.19
 ---- batch: 100 ----
mean loss: 623.82
 ---- batch: 110 ----
mean loss: 642.17
train mean loss: 642.11
epoch train time: 0:00:02.534831
elapsed time: 0:02:21.435557
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 22:10:35.903286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 631.01
 ---- batch: 020 ----
mean loss: 626.35
 ---- batch: 030 ----
mean loss: 644.24
 ---- batch: 040 ----
mean loss: 634.98
 ---- batch: 050 ----
mean loss: 636.24
 ---- batch: 060 ----
mean loss: 636.30
 ---- batch: 070 ----
mean loss: 619.37
 ---- batch: 080 ----
mean loss: 644.66
 ---- batch: 090 ----
mean loss: 629.98
 ---- batch: 100 ----
mean loss: 636.32
 ---- batch: 110 ----
mean loss: 629.69
train mean loss: 634.02
epoch train time: 0:00:02.487673
elapsed time: 0:02:23.923653
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 22:10:38.391420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.83
 ---- batch: 020 ----
mean loss: 627.94
 ---- batch: 030 ----
mean loss: 625.42
 ---- batch: 040 ----
mean loss: 632.13
 ---- batch: 050 ----
mean loss: 623.87
 ---- batch: 060 ----
mean loss: 651.74
 ---- batch: 070 ----
mean loss: 633.79
 ---- batch: 080 ----
mean loss: 619.72
 ---- batch: 090 ----
mean loss: 619.30
 ---- batch: 100 ----
mean loss: 604.69
 ---- batch: 110 ----
mean loss: 623.52
train mean loss: 626.07
epoch train time: 0:00:02.514284
elapsed time: 0:02:26.438386
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 22:10:40.906100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 606.98
 ---- batch: 020 ----
mean loss: 637.78
 ---- batch: 030 ----
mean loss: 629.92
 ---- batch: 040 ----
mean loss: 625.11
 ---- batch: 050 ----
mean loss: 611.95
 ---- batch: 060 ----
mean loss: 611.81
 ---- batch: 070 ----
mean loss: 610.24
 ---- batch: 080 ----
mean loss: 619.58
 ---- batch: 090 ----
mean loss: 607.85
 ---- batch: 100 ----
mean loss: 620.78
 ---- batch: 110 ----
mean loss: 623.61
train mean loss: 618.20
epoch train time: 0:00:02.501824
elapsed time: 0:02:28.940640
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 22:10:43.408373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 610.13
 ---- batch: 020 ----
mean loss: 612.57
 ---- batch: 030 ----
mean loss: 615.15
 ---- batch: 040 ----
mean loss: 608.47
 ---- batch: 050 ----
mean loss: 608.66
 ---- batch: 060 ----
mean loss: 596.52
 ---- batch: 070 ----
mean loss: 625.71
 ---- batch: 080 ----
mean loss: 600.19
 ---- batch: 090 ----
mean loss: 615.95
 ---- batch: 100 ----
mean loss: 601.18
 ---- batch: 110 ----
mean loss: 613.08
train mean loss: 609.81
epoch train time: 0:00:02.516649
elapsed time: 0:02:31.457738
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 22:10:45.925445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.04
 ---- batch: 020 ----
mean loss: 611.96
 ---- batch: 030 ----
mean loss: 597.34
 ---- batch: 040 ----
mean loss: 590.77
 ---- batch: 050 ----
mean loss: 608.61
 ---- batch: 060 ----
mean loss: 596.71
 ---- batch: 070 ----
mean loss: 617.90
 ---- batch: 080 ----
mean loss: 601.74
 ---- batch: 090 ----
mean loss: 608.01
 ---- batch: 100 ----
mean loss: 582.30
 ---- batch: 110 ----
mean loss: 595.71
train mean loss: 601.37
epoch train time: 0:00:02.510653
elapsed time: 0:02:33.968837
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 22:10:48.436567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 598.06
 ---- batch: 020 ----
mean loss: 593.42
 ---- batch: 030 ----
mean loss: 603.63
 ---- batch: 040 ----
mean loss: 584.80
 ---- batch: 050 ----
mean loss: 584.56
 ---- batch: 060 ----
mean loss: 600.27
 ---- batch: 070 ----
mean loss: 604.55
 ---- batch: 080 ----
mean loss: 593.53
 ---- batch: 090 ----
mean loss: 582.01
 ---- batch: 100 ----
mean loss: 589.13
 ---- batch: 110 ----
mean loss: 583.95
train mean loss: 592.85
epoch train time: 0:00:02.516625
elapsed time: 0:02:36.485865
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 22:10:50.953559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 591.79
 ---- batch: 020 ----
mean loss: 591.48
 ---- batch: 030 ----
mean loss: 587.64
 ---- batch: 040 ----
mean loss: 589.90
 ---- batch: 050 ----
mean loss: 575.04
 ---- batch: 060 ----
mean loss: 582.60
 ---- batch: 070 ----
mean loss: 575.53
 ---- batch: 080 ----
mean loss: 585.15
 ---- batch: 090 ----
mean loss: 586.01
 ---- batch: 100 ----
mean loss: 571.06
 ---- batch: 110 ----
mean loss: 570.32
train mean loss: 582.90
epoch train time: 0:00:02.484018
elapsed time: 0:02:38.970293
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 22:10:53.438018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 576.33
 ---- batch: 020 ----
mean loss: 586.37
 ---- batch: 030 ----
mean loss: 571.48
 ---- batch: 040 ----
mean loss: 579.42
 ---- batch: 050 ----
mean loss: 574.48
 ---- batch: 060 ----
mean loss: 572.83
 ---- batch: 070 ----
mean loss: 571.76
 ---- batch: 080 ----
mean loss: 570.71
 ---- batch: 090 ----
mean loss: 561.83
 ---- batch: 100 ----
mean loss: 555.75
 ---- batch: 110 ----
mean loss: 557.71
train mean loss: 570.73
epoch train time: 0:00:02.521590
elapsed time: 0:02:41.492289
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 22:10:55.959997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 563.79
 ---- batch: 020 ----
mean loss: 565.13
 ---- batch: 030 ----
mean loss: 573.75
 ---- batch: 040 ----
mean loss: 552.40
 ---- batch: 050 ----
mean loss: 547.11
 ---- batch: 060 ----
mean loss: 560.30
 ---- batch: 070 ----
mean loss: 551.96
 ---- batch: 080 ----
mean loss: 540.64
 ---- batch: 090 ----
mean loss: 539.41
 ---- batch: 100 ----
mean loss: 557.79
 ---- batch: 110 ----
mean loss: 549.68
train mean loss: 554.58
epoch train time: 0:00:02.524916
elapsed time: 0:02:44.017588
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 22:10:58.485344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.30
 ---- batch: 020 ----
mean loss: 546.15
 ---- batch: 030 ----
mean loss: 534.02
 ---- batch: 040 ----
mean loss: 526.22
 ---- batch: 050 ----
mean loss: 523.30
 ---- batch: 060 ----
mean loss: 534.52
 ---- batch: 070 ----
mean loss: 534.81
 ---- batch: 080 ----
mean loss: 537.10
 ---- batch: 090 ----
mean loss: 520.50
 ---- batch: 100 ----
mean loss: 517.98
 ---- batch: 110 ----
mean loss: 536.03
train mean loss: 532.13
epoch train time: 0:00:02.521748
elapsed time: 0:02:46.539799
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 22:11:01.007525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.38
 ---- batch: 020 ----
mean loss: 519.36
 ---- batch: 030 ----
mean loss: 520.21
 ---- batch: 040 ----
mean loss: 504.10
 ---- batch: 050 ----
mean loss: 499.59
 ---- batch: 060 ----
mean loss: 517.30
 ---- batch: 070 ----
mean loss: 514.77
 ---- batch: 080 ----
mean loss: 513.40
 ---- batch: 090 ----
mean loss: 494.74
 ---- batch: 100 ----
mean loss: 507.35
 ---- batch: 110 ----
mean loss: 499.01
train mean loss: 509.87
epoch train time: 0:00:02.509151
elapsed time: 0:02:49.049453
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 22:11:03.517186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.33
 ---- batch: 020 ----
mean loss: 499.73
 ---- batch: 030 ----
mean loss: 502.78
 ---- batch: 040 ----
mean loss: 502.75
 ---- batch: 050 ----
mean loss: 496.69
 ---- batch: 060 ----
mean loss: 472.99
 ---- batch: 070 ----
mean loss: 484.43
 ---- batch: 080 ----
mean loss: 478.76
 ---- batch: 090 ----
mean loss: 478.30
 ---- batch: 100 ----
mean loss: 493.67
 ---- batch: 110 ----
mean loss: 482.81
train mean loss: 490.46
epoch train time: 0:00:02.525659
elapsed time: 0:02:51.575558
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 22:11:06.043278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.37
 ---- batch: 020 ----
mean loss: 478.07
 ---- batch: 030 ----
mean loss: 489.45
 ---- batch: 040 ----
mean loss: 469.28
 ---- batch: 050 ----
mean loss: 472.86
 ---- batch: 060 ----
mean loss: 463.54
 ---- batch: 070 ----
mean loss: 456.36
 ---- batch: 080 ----
mean loss: 465.15
 ---- batch: 090 ----
mean loss: 470.80
 ---- batch: 100 ----
mean loss: 458.59
 ---- batch: 110 ----
mean loss: 466.10
train mean loss: 471.21
epoch train time: 0:00:02.513574
elapsed time: 0:02:54.089642
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 22:11:08.557384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.95
 ---- batch: 020 ----
mean loss: 460.83
 ---- batch: 030 ----
mean loss: 463.19
 ---- batch: 040 ----
mean loss: 453.50
 ---- batch: 050 ----
mean loss: 466.10
 ---- batch: 060 ----
mean loss: 466.18
 ---- batch: 070 ----
mean loss: 438.75
 ---- batch: 080 ----
mean loss: 437.58
 ---- batch: 090 ----
mean loss: 445.51
 ---- batch: 100 ----
mean loss: 446.73
 ---- batch: 110 ----
mean loss: 444.44
train mean loss: 453.53
epoch train time: 0:00:02.537366
elapsed time: 0:02:56.627606
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 22:11:11.095195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.76
 ---- batch: 020 ----
mean loss: 449.25
 ---- batch: 030 ----
mean loss: 429.63
 ---- batch: 040 ----
mean loss: 438.17
 ---- batch: 050 ----
mean loss: 434.08
 ---- batch: 060 ----
mean loss: 432.45
 ---- batch: 070 ----
mean loss: 435.02
 ---- batch: 080 ----
mean loss: 420.07
 ---- batch: 090 ----
mean loss: 444.62
 ---- batch: 100 ----
mean loss: 443.09
 ---- batch: 110 ----
mean loss: 437.79
train mean loss: 437.78
epoch train time: 0:00:02.529063
elapsed time: 0:02:59.156953
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 22:11:13.624665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 425.32
 ---- batch: 020 ----
mean loss: 429.82
 ---- batch: 030 ----
mean loss: 426.71
 ---- batch: 040 ----
mean loss: 428.47
 ---- batch: 050 ----
mean loss: 422.48
 ---- batch: 060 ----
mean loss: 406.85
 ---- batch: 070 ----
mean loss: 418.80
 ---- batch: 080 ----
mean loss: 424.01
 ---- batch: 090 ----
mean loss: 416.61
 ---- batch: 100 ----
mean loss: 418.40
 ---- batch: 110 ----
mean loss: 421.66
train mean loss: 421.65
epoch train time: 0:00:02.535800
elapsed time: 0:03:01.693198
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 22:11:16.161002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.92
 ---- batch: 020 ----
mean loss: 415.34
 ---- batch: 030 ----
mean loss: 419.98
 ---- batch: 040 ----
mean loss: 407.76
 ---- batch: 050 ----
mean loss: 400.09
 ---- batch: 060 ----
mean loss: 407.32
 ---- batch: 070 ----
mean loss: 392.43
 ---- batch: 080 ----
mean loss: 403.92
 ---- batch: 090 ----
mean loss: 415.45
 ---- batch: 100 ----
mean loss: 403.85
 ---- batch: 110 ----
mean loss: 400.97
train mean loss: 407.47
epoch train time: 0:00:02.513022
elapsed time: 0:03:04.206760
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 22:11:18.674469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.14
 ---- batch: 020 ----
mean loss: 396.96
 ---- batch: 030 ----
mean loss: 401.32
 ---- batch: 040 ----
mean loss: 397.74
 ---- batch: 050 ----
mean loss: 407.76
 ---- batch: 060 ----
mean loss: 386.15
 ---- batch: 070 ----
mean loss: 387.21
 ---- batch: 080 ----
mean loss: 393.60
 ---- batch: 090 ----
mean loss: 386.79
 ---- batch: 100 ----
mean loss: 392.67
 ---- batch: 110 ----
mean loss: 388.78
train mean loss: 394.69
epoch train time: 0:00:02.528249
elapsed time: 0:03:06.735423
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 22:11:21.203152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.99
 ---- batch: 020 ----
mean loss: 389.63
 ---- batch: 030 ----
mean loss: 389.21
 ---- batch: 040 ----
mean loss: 378.28
 ---- batch: 050 ----
mean loss: 386.44
 ---- batch: 060 ----
mean loss: 387.35
 ---- batch: 070 ----
mean loss: 389.16
 ---- batch: 080 ----
mean loss: 378.08
 ---- batch: 090 ----
mean loss: 386.97
 ---- batch: 100 ----
mean loss: 373.02
 ---- batch: 110 ----
mean loss: 386.25
train mean loss: 383.24
epoch train time: 0:00:02.555396
elapsed time: 0:03:09.291262
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 22:11:23.758996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.19
 ---- batch: 020 ----
mean loss: 367.83
 ---- batch: 030 ----
mean loss: 369.00
 ---- batch: 040 ----
mean loss: 383.20
 ---- batch: 050 ----
mean loss: 377.27
 ---- batch: 060 ----
mean loss: 373.41
 ---- batch: 070 ----
mean loss: 356.57
 ---- batch: 080 ----
mean loss: 370.81
 ---- batch: 090 ----
mean loss: 372.54
 ---- batch: 100 ----
mean loss: 369.48
 ---- batch: 110 ----
mean loss: 373.66
train mean loss: 372.39
epoch train time: 0:00:02.544457
elapsed time: 0:03:11.836157
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 22:11:26.303873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.19
 ---- batch: 020 ----
mean loss: 356.48
 ---- batch: 030 ----
mean loss: 359.45
 ---- batch: 040 ----
mean loss: 367.03
 ---- batch: 050 ----
mean loss: 354.60
 ---- batch: 060 ----
mean loss: 360.45
 ---- batch: 070 ----
mean loss: 372.31
 ---- batch: 080 ----
mean loss: 357.06
 ---- batch: 090 ----
mean loss: 355.49
 ---- batch: 100 ----
mean loss: 364.12
 ---- batch: 110 ----
mean loss: 375.28
train mean loss: 362.12
epoch train time: 0:00:02.522302
elapsed time: 0:03:14.358878
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 22:11:28.826595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.45
 ---- batch: 020 ----
mean loss: 358.00
 ---- batch: 030 ----
mean loss: 355.03
 ---- batch: 040 ----
mean loss: 338.09
 ---- batch: 050 ----
mean loss: 348.74
 ---- batch: 060 ----
mean loss: 353.39
 ---- batch: 070 ----
mean loss: 356.05
 ---- batch: 080 ----
mean loss: 354.03
 ---- batch: 090 ----
mean loss: 367.16
 ---- batch: 100 ----
mean loss: 342.19
 ---- batch: 110 ----
mean loss: 343.76
train mean loss: 352.34
epoch train time: 0:00:02.502401
elapsed time: 0:03:16.861679
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 22:11:31.329393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.18
 ---- batch: 020 ----
mean loss: 341.37
 ---- batch: 030 ----
mean loss: 342.00
 ---- batch: 040 ----
mean loss: 348.77
 ---- batch: 050 ----
mean loss: 352.63
 ---- batch: 060 ----
mean loss: 337.61
 ---- batch: 070 ----
mean loss: 340.95
 ---- batch: 080 ----
mean loss: 338.82
 ---- batch: 090 ----
mean loss: 335.32
 ---- batch: 100 ----
mean loss: 331.87
 ---- batch: 110 ----
mean loss: 340.95
train mean loss: 342.50
epoch train time: 0:00:02.524739
elapsed time: 0:03:19.386840
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 22:11:33.854565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.33
 ---- batch: 020 ----
mean loss: 337.74
 ---- batch: 030 ----
mean loss: 334.58
 ---- batch: 040 ----
mean loss: 335.48
 ---- batch: 050 ----
mean loss: 329.64
 ---- batch: 060 ----
mean loss: 341.17
 ---- batch: 070 ----
mean loss: 334.12
 ---- batch: 080 ----
mean loss: 334.07
 ---- batch: 090 ----
mean loss: 328.36
 ---- batch: 100 ----
mean loss: 330.15
 ---- batch: 110 ----
mean loss: 321.88
train mean loss: 333.01
epoch train time: 0:00:02.523660
elapsed time: 0:03:21.910931
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 22:11:36.378644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.61
 ---- batch: 020 ----
mean loss: 328.42
 ---- batch: 030 ----
mean loss: 319.49
 ---- batch: 040 ----
mean loss: 328.93
 ---- batch: 050 ----
mean loss: 323.06
 ---- batch: 060 ----
mean loss: 318.11
 ---- batch: 070 ----
mean loss: 317.26
 ---- batch: 080 ----
mean loss: 316.15
 ---- batch: 090 ----
mean loss: 322.29
 ---- batch: 100 ----
mean loss: 319.67
 ---- batch: 110 ----
mean loss: 327.96
train mean loss: 323.48
epoch train time: 0:00:02.529775
elapsed time: 0:03:24.441131
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 22:11:38.908838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.43
 ---- batch: 020 ----
mean loss: 322.26
 ---- batch: 030 ----
mean loss: 312.15
 ---- batch: 040 ----
mean loss: 315.98
 ---- batch: 050 ----
mean loss: 317.13
 ---- batch: 060 ----
mean loss: 322.53
 ---- batch: 070 ----
mean loss: 317.72
 ---- batch: 080 ----
mean loss: 307.44
 ---- batch: 090 ----
mean loss: 309.85
 ---- batch: 100 ----
mean loss: 302.74
 ---- batch: 110 ----
mean loss: 318.58
train mean loss: 314.04
epoch train time: 0:00:02.516026
elapsed time: 0:03:26.957559
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 22:11:41.425275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.09
 ---- batch: 020 ----
mean loss: 302.75
 ---- batch: 030 ----
mean loss: 305.40
 ---- batch: 040 ----
mean loss: 298.60
 ---- batch: 050 ----
mean loss: 300.27
 ---- batch: 060 ----
mean loss: 317.06
 ---- batch: 070 ----
mean loss: 308.01
 ---- batch: 080 ----
mean loss: 300.43
 ---- batch: 090 ----
mean loss: 307.12
 ---- batch: 100 ----
mean loss: 292.20
 ---- batch: 110 ----
mean loss: 307.71
train mean loss: 304.90
epoch train time: 0:00:02.518623
elapsed time: 0:03:29.476596
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 22:11:43.944337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.20
 ---- batch: 020 ----
mean loss: 302.23
 ---- batch: 030 ----
mean loss: 304.37
 ---- batch: 040 ----
mean loss: 302.20
 ---- batch: 050 ----
mean loss: 294.32
 ---- batch: 060 ----
mean loss: 285.01
 ---- batch: 070 ----
mean loss: 300.59
 ---- batch: 080 ----
mean loss: 295.63
 ---- batch: 090 ----
mean loss: 295.38
 ---- batch: 100 ----
mean loss: 294.84
 ---- batch: 110 ----
mean loss: 295.11
train mean loss: 296.68
epoch train time: 0:00:02.499554
elapsed time: 0:03:31.976578
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 22:11:46.444287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.60
 ---- batch: 020 ----
mean loss: 285.59
 ---- batch: 030 ----
mean loss: 291.82
 ---- batch: 040 ----
mean loss: 285.18
 ---- batch: 050 ----
mean loss: 272.94
 ---- batch: 060 ----
mean loss: 286.07
 ---- batch: 070 ----
mean loss: 294.30
 ---- batch: 080 ----
mean loss: 297.72
 ---- batch: 090 ----
mean loss: 289.99
 ---- batch: 100 ----
mean loss: 292.10
 ---- batch: 110 ----
mean loss: 283.72
train mean loss: 288.67
epoch train time: 0:00:02.529286
elapsed time: 0:03:34.506399
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 22:11:48.974198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 281.08
 ---- batch: 020 ----
mean loss: 282.98
 ---- batch: 030 ----
mean loss: 281.42
 ---- batch: 040 ----
mean loss: 288.06
 ---- batch: 050 ----
mean loss: 288.29
 ---- batch: 060 ----
mean loss: 279.54
 ---- batch: 070 ----
mean loss: 281.58
 ---- batch: 080 ----
mean loss: 285.69
 ---- batch: 090 ----
mean loss: 282.00
 ---- batch: 100 ----
mean loss: 281.12
 ---- batch: 110 ----
mean loss: 277.49
train mean loss: 281.99
epoch train time: 0:00:02.539440
elapsed time: 0:03:37.046323
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 22:11:51.514021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.49
 ---- batch: 020 ----
mean loss: 282.92
 ---- batch: 030 ----
mean loss: 270.11
 ---- batch: 040 ----
mean loss: 270.46
 ---- batch: 050 ----
mean loss: 268.49
 ---- batch: 060 ----
mean loss: 281.87
 ---- batch: 070 ----
mean loss: 274.05
 ---- batch: 080 ----
mean loss: 279.77
 ---- batch: 090 ----
mean loss: 273.36
 ---- batch: 100 ----
mean loss: 271.86
 ---- batch: 110 ----
mean loss: 273.48
train mean loss: 275.09
epoch train time: 0:00:02.546743
elapsed time: 0:03:39.593442
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 22:11:54.061137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.81
 ---- batch: 020 ----
mean loss: 269.51
 ---- batch: 030 ----
mean loss: 264.56
 ---- batch: 040 ----
mean loss: 267.45
 ---- batch: 050 ----
mean loss: 262.51
 ---- batch: 060 ----
mean loss: 279.81
 ---- batch: 070 ----
mean loss: 275.33
 ---- batch: 080 ----
mean loss: 271.74
 ---- batch: 090 ----
mean loss: 266.32
 ---- batch: 100 ----
mean loss: 272.87
 ---- batch: 110 ----
mean loss: 263.40
train mean loss: 270.19
epoch train time: 0:00:02.517819
elapsed time: 0:03:42.111644
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 22:11:56.579340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.83
 ---- batch: 020 ----
mean loss: 269.78
 ---- batch: 030 ----
mean loss: 277.19
 ---- batch: 040 ----
mean loss: 269.46
 ---- batch: 050 ----
mean loss: 271.86
 ---- batch: 060 ----
mean loss: 260.50
 ---- batch: 070 ----
mean loss: 257.58
 ---- batch: 080 ----
mean loss: 260.47
 ---- batch: 090 ----
mean loss: 261.74
 ---- batch: 100 ----
mean loss: 260.76
 ---- batch: 110 ----
mean loss: 261.26
train mean loss: 265.01
epoch train time: 0:00:02.563839
elapsed time: 0:03:44.675874
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 22:11:59.143597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.08
 ---- batch: 020 ----
mean loss: 257.49
 ---- batch: 030 ----
mean loss: 263.90
 ---- batch: 040 ----
mean loss: 265.44
 ---- batch: 050 ----
mean loss: 260.58
 ---- batch: 060 ----
mean loss: 263.49
 ---- batch: 070 ----
mean loss: 260.14
 ---- batch: 080 ----
mean loss: 258.99
 ---- batch: 090 ----
mean loss: 259.48
 ---- batch: 100 ----
mean loss: 255.69
 ---- batch: 110 ----
mean loss: 261.12
train mean loss: 260.26
epoch train time: 0:00:02.547214
elapsed time: 0:03:47.223567
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 22:12:01.691287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.91
 ---- batch: 020 ----
mean loss: 265.30
 ---- batch: 030 ----
mean loss: 253.14
 ---- batch: 040 ----
mean loss: 257.98
 ---- batch: 050 ----
mean loss: 263.56
 ---- batch: 060 ----
mean loss: 259.25
 ---- batch: 070 ----
mean loss: 249.79
 ---- batch: 080 ----
mean loss: 252.08
 ---- batch: 090 ----
mean loss: 263.22
 ---- batch: 100 ----
mean loss: 252.84
 ---- batch: 110 ----
mean loss: 249.45
train mean loss: 256.75
epoch train time: 0:00:02.530081
elapsed time: 0:03:49.754070
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 22:12:04.221796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.50
 ---- batch: 020 ----
mean loss: 256.10
 ---- batch: 030 ----
mean loss: 247.68
 ---- batch: 040 ----
mean loss: 252.35
 ---- batch: 050 ----
mean loss: 258.75
 ---- batch: 060 ----
mean loss: 259.37
 ---- batch: 070 ----
mean loss: 253.97
 ---- batch: 080 ----
mean loss: 258.52
 ---- batch: 090 ----
mean loss: 249.31
 ---- batch: 100 ----
mean loss: 250.05
 ---- batch: 110 ----
mean loss: 248.12
train mean loss: 252.87
epoch train time: 0:00:02.535730
elapsed time: 0:03:52.290235
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 22:12:06.757976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.66
 ---- batch: 020 ----
mean loss: 252.50
 ---- batch: 030 ----
mean loss: 247.54
 ---- batch: 040 ----
mean loss: 255.32
 ---- batch: 050 ----
mean loss: 250.56
 ---- batch: 060 ----
mean loss: 248.54
 ---- batch: 070 ----
mean loss: 249.12
 ---- batch: 080 ----
mean loss: 244.04
 ---- batch: 090 ----
mean loss: 261.02
 ---- batch: 100 ----
mean loss: 237.10
 ---- batch: 110 ----
mean loss: 257.00
train mean loss: 249.88
epoch train time: 0:00:02.539929
elapsed time: 0:03:54.830647
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 22:12:09.298361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.22
 ---- batch: 020 ----
mean loss: 253.13
 ---- batch: 030 ----
mean loss: 241.83
 ---- batch: 040 ----
mean loss: 242.43
 ---- batch: 050 ----
mean loss: 252.69
 ---- batch: 060 ----
mean loss: 247.45
 ---- batch: 070 ----
mean loss: 255.22
 ---- batch: 080 ----
mean loss: 242.01
 ---- batch: 090 ----
mean loss: 247.81
 ---- batch: 100 ----
mean loss: 250.36
 ---- batch: 110 ----
mean loss: 240.30
train mean loss: 247.15
epoch train time: 0:00:02.557319
elapsed time: 0:03:57.388381
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 22:12:11.856097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 238.33
 ---- batch: 020 ----
mean loss: 248.49
 ---- batch: 030 ----
mean loss: 243.25
 ---- batch: 040 ----
mean loss: 242.94
 ---- batch: 050 ----
mean loss: 245.02
 ---- batch: 060 ----
mean loss: 241.68
 ---- batch: 070 ----
mean loss: 241.90
 ---- batch: 080 ----
mean loss: 256.45
 ---- batch: 090 ----
mean loss: 247.75
 ---- batch: 100 ----
mean loss: 236.30
 ---- batch: 110 ----
mean loss: 250.30
train mean loss: 244.89
epoch train time: 0:00:02.547945
elapsed time: 0:03:59.936744
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 22:12:14.404453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.67
 ---- batch: 020 ----
mean loss: 249.48
 ---- batch: 030 ----
mean loss: 247.89
 ---- batch: 040 ----
mean loss: 248.14
 ---- batch: 050 ----
mean loss: 240.48
 ---- batch: 060 ----
mean loss: 241.33
 ---- batch: 070 ----
mean loss: 243.48
 ---- batch: 080 ----
mean loss: 236.81
 ---- batch: 090 ----
mean loss: 240.42
 ---- batch: 100 ----
mean loss: 244.14
 ---- batch: 110 ----
mean loss: 240.52
train mean loss: 242.61
epoch train time: 0:00:02.559739
elapsed time: 0:04:02.496963
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 22:12:16.964682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.49
 ---- batch: 020 ----
mean loss: 246.01
 ---- batch: 030 ----
mean loss: 238.63
 ---- batch: 040 ----
mean loss: 234.68
 ---- batch: 050 ----
mean loss: 244.70
 ---- batch: 060 ----
mean loss: 241.64
 ---- batch: 070 ----
mean loss: 229.75
 ---- batch: 080 ----
mean loss: 239.59
 ---- batch: 090 ----
mean loss: 241.50
 ---- batch: 100 ----
mean loss: 245.63
 ---- batch: 110 ----
mean loss: 240.14
train mean loss: 240.70
epoch train time: 0:00:02.525020
elapsed time: 0:04:05.022400
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 22:12:19.490123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.49
 ---- batch: 020 ----
mean loss: 244.00
 ---- batch: 030 ----
mean loss: 237.09
 ---- batch: 040 ----
mean loss: 234.37
 ---- batch: 050 ----
mean loss: 233.36
 ---- batch: 060 ----
mean loss: 240.07
 ---- batch: 070 ----
mean loss: 246.27
 ---- batch: 080 ----
mean loss: 245.56
 ---- batch: 090 ----
mean loss: 240.29
 ---- batch: 100 ----
mean loss: 241.01
 ---- batch: 110 ----
mean loss: 232.34
train mean loss: 238.86
epoch train time: 0:00:02.533648
elapsed time: 0:04:07.556529
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 22:12:22.024349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.51
 ---- batch: 020 ----
mean loss: 244.90
 ---- batch: 030 ----
mean loss: 230.01
 ---- batch: 040 ----
mean loss: 243.31
 ---- batch: 050 ----
mean loss: 240.95
 ---- batch: 060 ----
mean loss: 234.72
 ---- batch: 070 ----
mean loss: 239.63
 ---- batch: 080 ----
mean loss: 241.21
 ---- batch: 090 ----
mean loss: 239.88
 ---- batch: 100 ----
mean loss: 236.24
 ---- batch: 110 ----
mean loss: 231.84
train mean loss: 237.29
epoch train time: 0:00:02.558767
elapsed time: 0:04:10.115843
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 22:12:24.583596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.32
 ---- batch: 020 ----
mean loss: 244.97
 ---- batch: 030 ----
mean loss: 233.62
 ---- batch: 040 ----
mean loss: 234.47
 ---- batch: 050 ----
mean loss: 237.58
 ---- batch: 060 ----
mean loss: 232.30
 ---- batch: 070 ----
mean loss: 229.65
 ---- batch: 080 ----
mean loss: 232.41
 ---- batch: 090 ----
mean loss: 239.20
 ---- batch: 100 ----
mean loss: 235.76
 ---- batch: 110 ----
mean loss: 240.70
train mean loss: 235.95
epoch train time: 0:00:02.535871
elapsed time: 0:04:12.652178
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 22:12:27.119894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.99
 ---- batch: 020 ----
mean loss: 235.66
 ---- batch: 030 ----
mean loss: 235.03
 ---- batch: 040 ----
mean loss: 225.17
 ---- batch: 050 ----
mean loss: 234.51
 ---- batch: 060 ----
mean loss: 233.91
 ---- batch: 070 ----
mean loss: 234.51
 ---- batch: 080 ----
mean loss: 243.48
 ---- batch: 090 ----
mean loss: 234.21
 ---- batch: 100 ----
mean loss: 227.09
 ---- batch: 110 ----
mean loss: 232.99
train mean loss: 234.37
epoch train time: 0:00:02.540008
elapsed time: 0:04:15.192605
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 22:12:29.660357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.86
 ---- batch: 020 ----
mean loss: 231.45
 ---- batch: 030 ----
mean loss: 237.26
 ---- batch: 040 ----
mean loss: 237.60
 ---- batch: 050 ----
mean loss: 238.16
 ---- batch: 060 ----
mean loss: 228.97
 ---- batch: 070 ----
mean loss: 231.71
 ---- batch: 080 ----
mean loss: 238.81
 ---- batch: 090 ----
mean loss: 238.49
 ---- batch: 100 ----
mean loss: 234.12
 ---- batch: 110 ----
mean loss: 233.97
train mean loss: 233.33
epoch train time: 0:00:02.512509
elapsed time: 0:04:17.705569
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 22:12:32.173275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.34
 ---- batch: 020 ----
mean loss: 231.46
 ---- batch: 030 ----
mean loss: 238.57
 ---- batch: 040 ----
mean loss: 239.14
 ---- batch: 050 ----
mean loss: 223.66
 ---- batch: 060 ----
mean loss: 227.90
 ---- batch: 070 ----
mean loss: 234.53
 ---- batch: 080 ----
mean loss: 232.01
 ---- batch: 090 ----
mean loss: 234.05
 ---- batch: 100 ----
mean loss: 230.31
 ---- batch: 110 ----
mean loss: 226.59
train mean loss: 232.37
epoch train time: 0:00:02.532693
elapsed time: 0:04:20.238665
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 22:12:34.706372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.12
 ---- batch: 020 ----
mean loss: 229.95
 ---- batch: 030 ----
mean loss: 222.03
 ---- batch: 040 ----
mean loss: 232.04
 ---- batch: 050 ----
mean loss: 227.20
 ---- batch: 060 ----
mean loss: 238.81
 ---- batch: 070 ----
mean loss: 236.24
 ---- batch: 080 ----
mean loss: 236.43
 ---- batch: 090 ----
mean loss: 230.34
 ---- batch: 100 ----
mean loss: 230.62
 ---- batch: 110 ----
mean loss: 233.50
train mean loss: 231.31
epoch train time: 0:00:02.561517
elapsed time: 0:04:22.800608
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 22:12:37.268350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.38
 ---- batch: 020 ----
mean loss: 238.26
 ---- batch: 030 ----
mean loss: 236.80
 ---- batch: 040 ----
mean loss: 227.19
 ---- batch: 050 ----
mean loss: 229.14
 ---- batch: 060 ----
mean loss: 229.92
 ---- batch: 070 ----
mean loss: 230.62
 ---- batch: 080 ----
mean loss: 225.78
 ---- batch: 090 ----
mean loss: 228.56
 ---- batch: 100 ----
mean loss: 220.08
 ---- batch: 110 ----
mean loss: 236.33
train mean loss: 230.27
epoch train time: 0:00:02.513018
elapsed time: 0:04:25.314083
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 22:12:39.781841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.15
 ---- batch: 020 ----
mean loss: 229.45
 ---- batch: 030 ----
mean loss: 230.73
 ---- batch: 040 ----
mean loss: 238.62
 ---- batch: 050 ----
mean loss: 223.26
 ---- batch: 060 ----
mean loss: 226.17
 ---- batch: 070 ----
mean loss: 233.78
 ---- batch: 080 ----
mean loss: 227.15
 ---- batch: 090 ----
mean loss: 224.00
 ---- batch: 100 ----
mean loss: 225.37
 ---- batch: 110 ----
mean loss: 234.25
train mean loss: 229.48
epoch train time: 0:00:02.532122
elapsed time: 0:04:27.846663
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 22:12:42.314369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.28
 ---- batch: 020 ----
mean loss: 232.11
 ---- batch: 030 ----
mean loss: 230.56
 ---- batch: 040 ----
mean loss: 235.60
 ---- batch: 050 ----
mean loss: 224.36
 ---- batch: 060 ----
mean loss: 218.81
 ---- batch: 070 ----
mean loss: 228.94
 ---- batch: 080 ----
mean loss: 223.77
 ---- batch: 090 ----
mean loss: 235.89
 ---- batch: 100 ----
mean loss: 230.24
 ---- batch: 110 ----
mean loss: 223.29
train mean loss: 228.34
epoch train time: 0:00:02.523865
elapsed time: 0:04:30.370955
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 22:12:44.838718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.97
 ---- batch: 020 ----
mean loss: 238.16
 ---- batch: 030 ----
mean loss: 224.88
 ---- batch: 040 ----
mean loss: 226.88
 ---- batch: 050 ----
mean loss: 224.96
 ---- batch: 060 ----
mean loss: 225.52
 ---- batch: 070 ----
mean loss: 232.86
 ---- batch: 080 ----
mean loss: 234.85
 ---- batch: 090 ----
mean loss: 225.34
 ---- batch: 100 ----
mean loss: 225.70
 ---- batch: 110 ----
mean loss: 224.48
train mean loss: 227.92
epoch train time: 0:00:02.524297
elapsed time: 0:04:32.895713
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 22:12:47.363424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.59
 ---- batch: 020 ----
mean loss: 231.73
 ---- batch: 030 ----
mean loss: 231.46
 ---- batch: 040 ----
mean loss: 223.83
 ---- batch: 050 ----
mean loss: 228.08
 ---- batch: 060 ----
mean loss: 228.27
 ---- batch: 070 ----
mean loss: 230.88
 ---- batch: 080 ----
mean loss: 225.35
 ---- batch: 090 ----
mean loss: 230.17
 ---- batch: 100 ----
mean loss: 226.30
 ---- batch: 110 ----
mean loss: 220.40
train mean loss: 227.13
epoch train time: 0:00:02.531568
elapsed time: 0:04:35.427694
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 22:12:49.895403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.63
 ---- batch: 020 ----
mean loss: 226.76
 ---- batch: 030 ----
mean loss: 227.99
 ---- batch: 040 ----
mean loss: 231.38
 ---- batch: 050 ----
mean loss: 234.77
 ---- batch: 060 ----
mean loss: 221.33
 ---- batch: 070 ----
mean loss: 217.45
 ---- batch: 080 ----
mean loss: 221.40
 ---- batch: 090 ----
mean loss: 227.04
 ---- batch: 100 ----
mean loss: 226.48
 ---- batch: 110 ----
mean loss: 225.02
train mean loss: 226.43
epoch train time: 0:00:02.513995
elapsed time: 0:04:37.942096
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 22:12:52.409834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.11
 ---- batch: 020 ----
mean loss: 218.59
 ---- batch: 030 ----
mean loss: 207.89
 ---- batch: 040 ----
mean loss: 230.05
 ---- batch: 050 ----
mean loss: 239.58
 ---- batch: 060 ----
mean loss: 231.09
 ---- batch: 070 ----
mean loss: 224.53
 ---- batch: 080 ----
mean loss: 225.99
 ---- batch: 090 ----
mean loss: 225.57
 ---- batch: 100 ----
mean loss: 224.95
 ---- batch: 110 ----
mean loss: 231.33
train mean loss: 225.73
epoch train time: 0:00:02.528831
elapsed time: 0:04:40.471381
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 22:12:54.939119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.77
 ---- batch: 020 ----
mean loss: 218.10
 ---- batch: 030 ----
mean loss: 224.20
 ---- batch: 040 ----
mean loss: 223.99
 ---- batch: 050 ----
mean loss: 234.76
 ---- batch: 060 ----
mean loss: 220.24
 ---- batch: 070 ----
mean loss: 224.71
 ---- batch: 080 ----
mean loss: 231.61
 ---- batch: 090 ----
mean loss: 228.37
 ---- batch: 100 ----
mean loss: 228.05
 ---- batch: 110 ----
mean loss: 225.60
train mean loss: 225.06
epoch train time: 0:00:02.509310
elapsed time: 0:04:42.981147
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 22:12:57.448707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.68
 ---- batch: 020 ----
mean loss: 225.30
 ---- batch: 030 ----
mean loss: 225.76
 ---- batch: 040 ----
mean loss: 215.78
 ---- batch: 050 ----
mean loss: 223.12
 ---- batch: 060 ----
mean loss: 228.81
 ---- batch: 070 ----
mean loss: 224.15
 ---- batch: 080 ----
mean loss: 236.38
 ---- batch: 090 ----
mean loss: 225.73
 ---- batch: 100 ----
mean loss: 221.98
 ---- batch: 110 ----
mean loss: 223.88
train mean loss: 224.41
epoch train time: 0:00:02.522480
elapsed time: 0:04:45.503913
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 22:12:59.971644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 232.52
 ---- batch: 020 ----
mean loss: 231.94
 ---- batch: 030 ----
mean loss: 215.34
 ---- batch: 040 ----
mean loss: 230.67
 ---- batch: 050 ----
mean loss: 216.30
 ---- batch: 060 ----
mean loss: 225.31
 ---- batch: 070 ----
mean loss: 215.63
 ---- batch: 080 ----
mean loss: 212.94
 ---- batch: 090 ----
mean loss: 223.62
 ---- batch: 100 ----
mean loss: 226.23
 ---- batch: 110 ----
mean loss: 231.63
train mean loss: 223.75
epoch train time: 0:00:02.503042
elapsed time: 0:04:48.007376
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 22:13:02.475091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.18
 ---- batch: 020 ----
mean loss: 222.87
 ---- batch: 030 ----
mean loss: 232.86
 ---- batch: 040 ----
mean loss: 220.01
 ---- batch: 050 ----
mean loss: 224.36
 ---- batch: 060 ----
mean loss: 225.24
 ---- batch: 070 ----
mean loss: 222.45
 ---- batch: 080 ----
mean loss: 222.99
 ---- batch: 090 ----
mean loss: 219.63
 ---- batch: 100 ----
mean loss: 228.85
 ---- batch: 110 ----
mean loss: 220.54
train mean loss: 223.23
epoch train time: 0:00:02.570987
elapsed time: 0:04:50.578777
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 22:13:05.046527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.23
 ---- batch: 020 ----
mean loss: 221.59
 ---- batch: 030 ----
mean loss: 224.61
 ---- batch: 040 ----
mean loss: 218.40
 ---- batch: 050 ----
mean loss: 225.60
 ---- batch: 060 ----
mean loss: 218.05
 ---- batch: 070 ----
mean loss: 221.91
 ---- batch: 080 ----
mean loss: 226.24
 ---- batch: 090 ----
mean loss: 225.69
 ---- batch: 100 ----
mean loss: 215.42
 ---- batch: 110 ----
mean loss: 226.10
train mean loss: 222.67
epoch train time: 0:00:02.530003
elapsed time: 0:04:53.109291
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 22:13:07.577021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.78
 ---- batch: 020 ----
mean loss: 219.93
 ---- batch: 030 ----
mean loss: 229.47
 ---- batch: 040 ----
mean loss: 222.32
 ---- batch: 050 ----
mean loss: 220.21
 ---- batch: 060 ----
mean loss: 226.29
 ---- batch: 070 ----
mean loss: 216.34
 ---- batch: 080 ----
mean loss: 227.33
 ---- batch: 090 ----
mean loss: 214.57
 ---- batch: 100 ----
mean loss: 219.18
 ---- batch: 110 ----
mean loss: 219.86
train mean loss: 222.18
epoch train time: 0:00:02.528241
elapsed time: 0:04:55.637964
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 22:13:10.105708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.21
 ---- batch: 020 ----
mean loss: 214.94
 ---- batch: 030 ----
mean loss: 223.12
 ---- batch: 040 ----
mean loss: 219.81
 ---- batch: 050 ----
mean loss: 214.50
 ---- batch: 060 ----
mean loss: 225.15
 ---- batch: 070 ----
mean loss: 221.76
 ---- batch: 080 ----
mean loss: 219.20
 ---- batch: 090 ----
mean loss: 216.98
 ---- batch: 100 ----
mean loss: 223.45
 ---- batch: 110 ----
mean loss: 229.11
train mean loss: 221.69
epoch train time: 0:00:02.533577
elapsed time: 0:04:58.172004
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 22:13:12.639749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.02
 ---- batch: 020 ----
mean loss: 222.14
 ---- batch: 030 ----
mean loss: 220.10
 ---- batch: 040 ----
mean loss: 219.79
 ---- batch: 050 ----
mean loss: 219.85
 ---- batch: 060 ----
mean loss: 215.19
 ---- batch: 070 ----
mean loss: 225.97
 ---- batch: 080 ----
mean loss: 214.59
 ---- batch: 090 ----
mean loss: 220.48
 ---- batch: 100 ----
mean loss: 227.04
 ---- batch: 110 ----
mean loss: 232.26
train mean loss: 221.18
epoch train time: 0:00:02.524444
elapsed time: 0:05:00.696878
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 22:13:15.164588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.63
 ---- batch: 020 ----
mean loss: 218.50
 ---- batch: 030 ----
mean loss: 220.33
 ---- batch: 040 ----
mean loss: 215.07
 ---- batch: 050 ----
mean loss: 214.61
 ---- batch: 060 ----
mean loss: 226.00
 ---- batch: 070 ----
mean loss: 220.66
 ---- batch: 080 ----
mean loss: 218.11
 ---- batch: 090 ----
mean loss: 219.53
 ---- batch: 100 ----
mean loss: 230.19
 ---- batch: 110 ----
mean loss: 218.22
train mean loss: 220.53
epoch train time: 0:00:02.522792
elapsed time: 0:05:03.220219
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 22:13:17.687856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.89
 ---- batch: 020 ----
mean loss: 231.27
 ---- batch: 030 ----
mean loss: 214.29
 ---- batch: 040 ----
mean loss: 219.38
 ---- batch: 050 ----
mean loss: 221.42
 ---- batch: 060 ----
mean loss: 218.49
 ---- batch: 070 ----
mean loss: 215.62
 ---- batch: 080 ----
mean loss: 226.79
 ---- batch: 090 ----
mean loss: 225.03
 ---- batch: 100 ----
mean loss: 211.31
 ---- batch: 110 ----
mean loss: 224.17
train mean loss: 220.23
epoch train time: 0:00:02.526594
elapsed time: 0:05:05.747139
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 22:13:20.214842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.71
 ---- batch: 020 ----
mean loss: 220.32
 ---- batch: 030 ----
mean loss: 224.46
 ---- batch: 040 ----
mean loss: 214.09
 ---- batch: 050 ----
mean loss: 230.08
 ---- batch: 060 ----
mean loss: 213.40
 ---- batch: 070 ----
mean loss: 228.52
 ---- batch: 080 ----
mean loss: 219.39
 ---- batch: 090 ----
mean loss: 225.38
 ---- batch: 100 ----
mean loss: 207.31
 ---- batch: 110 ----
mean loss: 212.88
train mean loss: 219.64
epoch train time: 0:00:02.494103
elapsed time: 0:05:08.241654
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 22:13:22.709362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.02
 ---- batch: 020 ----
mean loss: 225.13
 ---- batch: 030 ----
mean loss: 208.95
 ---- batch: 040 ----
mean loss: 216.08
 ---- batch: 050 ----
mean loss: 222.43
 ---- batch: 060 ----
mean loss: 216.69
 ---- batch: 070 ----
mean loss: 218.54
 ---- batch: 080 ----
mean loss: 219.54
 ---- batch: 090 ----
mean loss: 224.90
 ---- batch: 100 ----
mean loss: 224.13
 ---- batch: 110 ----
mean loss: 220.44
train mean loss: 219.20
epoch train time: 0:00:02.522754
elapsed time: 0:05:10.764799
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 22:13:25.232510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.49
 ---- batch: 020 ----
mean loss: 209.39
 ---- batch: 030 ----
mean loss: 224.79
 ---- batch: 040 ----
mean loss: 210.89
 ---- batch: 050 ----
mean loss: 223.13
 ---- batch: 060 ----
mean loss: 215.40
 ---- batch: 070 ----
mean loss: 226.32
 ---- batch: 080 ----
mean loss: 220.36
 ---- batch: 090 ----
mean loss: 213.87
 ---- batch: 100 ----
mean loss: 220.86
 ---- batch: 110 ----
mean loss: 223.93
train mean loss: 218.67
epoch train time: 0:00:02.506761
elapsed time: 0:05:13.271987
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 22:13:27.739735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.72
 ---- batch: 020 ----
mean loss: 219.14
 ---- batch: 030 ----
mean loss: 214.73
 ---- batch: 040 ----
mean loss: 208.76
 ---- batch: 050 ----
mean loss: 227.04
 ---- batch: 060 ----
mean loss: 216.61
 ---- batch: 070 ----
mean loss: 217.42
 ---- batch: 080 ----
mean loss: 226.31
 ---- batch: 090 ----
mean loss: 213.53
 ---- batch: 100 ----
mean loss: 211.97
 ---- batch: 110 ----
mean loss: 224.01
train mean loss: 218.19
epoch train time: 0:00:02.521228
elapsed time: 0:05:15.793730
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 22:13:30.261484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.09
 ---- batch: 020 ----
mean loss: 218.49
 ---- batch: 030 ----
mean loss: 221.37
 ---- batch: 040 ----
mean loss: 218.67
 ---- batch: 050 ----
mean loss: 207.77
 ---- batch: 060 ----
mean loss: 221.26
 ---- batch: 070 ----
mean loss: 217.44
 ---- batch: 080 ----
mean loss: 223.19
 ---- batch: 090 ----
mean loss: 226.08
 ---- batch: 100 ----
mean loss: 208.36
 ---- batch: 110 ----
mean loss: 219.22
train mean loss: 218.05
epoch train time: 0:00:02.537308
elapsed time: 0:05:18.331592
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 22:13:32.799312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.36
 ---- batch: 020 ----
mean loss: 205.92
 ---- batch: 030 ----
mean loss: 218.03
 ---- batch: 040 ----
mean loss: 206.95
 ---- batch: 050 ----
mean loss: 218.00
 ---- batch: 060 ----
mean loss: 223.00
 ---- batch: 070 ----
mean loss: 219.21
 ---- batch: 080 ----
mean loss: 222.40
 ---- batch: 090 ----
mean loss: 218.02
 ---- batch: 100 ----
mean loss: 218.64
 ---- batch: 110 ----
mean loss: 223.90
train mean loss: 217.39
epoch train time: 0:00:02.509033
elapsed time: 0:05:20.841060
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 22:13:35.308769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.07
 ---- batch: 020 ----
mean loss: 227.66
 ---- batch: 030 ----
mean loss: 213.78
 ---- batch: 040 ----
mean loss: 217.89
 ---- batch: 050 ----
mean loss: 224.28
 ---- batch: 060 ----
mean loss: 232.61
 ---- batch: 070 ----
mean loss: 221.66
 ---- batch: 080 ----
mean loss: 209.33
 ---- batch: 090 ----
mean loss: 214.09
 ---- batch: 100 ----
mean loss: 215.83
 ---- batch: 110 ----
mean loss: 209.86
train mean loss: 217.71
epoch train time: 0:00:02.528592
elapsed time: 0:05:23.370062
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 22:13:37.837804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.78
 ---- batch: 020 ----
mean loss: 225.07
 ---- batch: 030 ----
mean loss: 217.99
 ---- batch: 040 ----
mean loss: 207.07
 ---- batch: 050 ----
mean loss: 217.30
 ---- batch: 060 ----
mean loss: 215.57
 ---- batch: 070 ----
mean loss: 213.05
 ---- batch: 080 ----
mean loss: 222.75
 ---- batch: 090 ----
mean loss: 220.23
 ---- batch: 100 ----
mean loss: 209.73
 ---- batch: 110 ----
mean loss: 211.75
train mean loss: 216.71
epoch train time: 0:00:02.523166
elapsed time: 0:05:25.893734
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 22:13:40.361463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.54
 ---- batch: 020 ----
mean loss: 215.66
 ---- batch: 030 ----
mean loss: 211.32
 ---- batch: 040 ----
mean loss: 213.68
 ---- batch: 050 ----
mean loss: 213.99
 ---- batch: 060 ----
mean loss: 216.90
 ---- batch: 070 ----
mean loss: 221.19
 ---- batch: 080 ----
mean loss: 219.46
 ---- batch: 090 ----
mean loss: 216.89
 ---- batch: 100 ----
mean loss: 220.86
 ---- batch: 110 ----
mean loss: 215.01
train mean loss: 216.32
epoch train time: 0:00:02.500490
elapsed time: 0:05:28.394731
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 22:13:42.862509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.47
 ---- batch: 020 ----
mean loss: 218.87
 ---- batch: 030 ----
mean loss: 218.37
 ---- batch: 040 ----
mean loss: 214.02
 ---- batch: 050 ----
mean loss: 211.32
 ---- batch: 060 ----
mean loss: 213.72
 ---- batch: 070 ----
mean loss: 213.45
 ---- batch: 080 ----
mean loss: 226.90
 ---- batch: 090 ----
mean loss: 214.04
 ---- batch: 100 ----
mean loss: 214.72
 ---- batch: 110 ----
mean loss: 208.24
train mean loss: 215.85
epoch train time: 0:00:02.540048
elapsed time: 0:05:30.935382
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 22:13:45.403134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.62
 ---- batch: 020 ----
mean loss: 212.81
 ---- batch: 030 ----
mean loss: 218.43
 ---- batch: 040 ----
mean loss: 211.37
 ---- batch: 050 ----
mean loss: 215.56
 ---- batch: 060 ----
mean loss: 210.47
 ---- batch: 070 ----
mean loss: 212.55
 ---- batch: 080 ----
mean loss: 222.50
 ---- batch: 090 ----
mean loss: 222.16
 ---- batch: 100 ----
mean loss: 217.06
 ---- batch: 110 ----
mean loss: 214.15
train mean loss: 215.34
epoch train time: 0:00:02.509305
elapsed time: 0:05:33.445151
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 22:13:47.912870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.86
 ---- batch: 020 ----
mean loss: 209.33
 ---- batch: 030 ----
mean loss: 210.28
 ---- batch: 040 ----
mean loss: 219.36
 ---- batch: 050 ----
mean loss: 218.32
 ---- batch: 060 ----
mean loss: 226.56
 ---- batch: 070 ----
mean loss: 213.00
 ---- batch: 080 ----
mean loss: 214.63
 ---- batch: 090 ----
mean loss: 224.47
 ---- batch: 100 ----
mean loss: 209.27
 ---- batch: 110 ----
mean loss: 215.31
train mean loss: 215.22
epoch train time: 0:00:02.532096
elapsed time: 0:05:35.977658
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 22:13:50.445364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.58
 ---- batch: 020 ----
mean loss: 201.63
 ---- batch: 030 ----
mean loss: 218.23
 ---- batch: 040 ----
mean loss: 215.78
 ---- batch: 050 ----
mean loss: 210.39
 ---- batch: 060 ----
mean loss: 213.27
 ---- batch: 070 ----
mean loss: 219.89
 ---- batch: 080 ----
mean loss: 204.27
 ---- batch: 090 ----
mean loss: 223.86
 ---- batch: 100 ----
mean loss: 214.13
 ---- batch: 110 ----
mean loss: 218.50
train mean loss: 214.83
epoch train time: 0:00:02.516702
elapsed time: 0:05:38.494768
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 22:13:52.962471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.39
 ---- batch: 020 ----
mean loss: 215.07
 ---- batch: 030 ----
mean loss: 211.52
 ---- batch: 040 ----
mean loss: 219.77
 ---- batch: 050 ----
mean loss: 206.83
 ---- batch: 060 ----
mean loss: 213.93
 ---- batch: 070 ----
mean loss: 212.46
 ---- batch: 080 ----
mean loss: 212.98
 ---- batch: 090 ----
mean loss: 213.29
 ---- batch: 100 ----
mean loss: 220.24
 ---- batch: 110 ----
mean loss: 217.40
train mean loss: 214.34
epoch train time: 0:00:02.528796
elapsed time: 0:05:41.023964
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 22:13:55.491687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.52
 ---- batch: 020 ----
mean loss: 220.41
 ---- batch: 030 ----
mean loss: 209.89
 ---- batch: 040 ----
mean loss: 217.09
 ---- batch: 050 ----
mean loss: 220.74
 ---- batch: 060 ----
mean loss: 213.87
 ---- batch: 070 ----
mean loss: 216.36
 ---- batch: 080 ----
mean loss: 217.40
 ---- batch: 090 ----
mean loss: 198.80
 ---- batch: 100 ----
mean loss: 220.24
 ---- batch: 110 ----
mean loss: 205.69
train mean loss: 214.01
epoch train time: 0:00:02.557206
elapsed time: 0:05:43.581671
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 22:13:58.049380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.10
 ---- batch: 020 ----
mean loss: 205.45
 ---- batch: 030 ----
mean loss: 213.48
 ---- batch: 040 ----
mean loss: 216.63
 ---- batch: 050 ----
mean loss: 213.27
 ---- batch: 060 ----
mean loss: 218.80
 ---- batch: 070 ----
mean loss: 212.99
 ---- batch: 080 ----
mean loss: 214.97
 ---- batch: 090 ----
mean loss: 217.52
 ---- batch: 100 ----
mean loss: 213.64
 ---- batch: 110 ----
mean loss: 204.97
train mean loss: 213.39
epoch train time: 0:00:02.564289
elapsed time: 0:05:46.146413
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 22:14:00.614113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.13
 ---- batch: 020 ----
mean loss: 211.02
 ---- batch: 030 ----
mean loss: 213.17
 ---- batch: 040 ----
mean loss: 221.84
 ---- batch: 050 ----
mean loss: 221.20
 ---- batch: 060 ----
mean loss: 215.85
 ---- batch: 070 ----
mean loss: 216.44
 ---- batch: 080 ----
mean loss: 208.55
 ---- batch: 090 ----
mean loss: 212.85
 ---- batch: 100 ----
mean loss: 202.39
 ---- batch: 110 ----
mean loss: 208.56
train mean loss: 213.20
epoch train time: 0:00:02.564772
elapsed time: 0:05:48.711589
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 22:14:03.179395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.64
 ---- batch: 020 ----
mean loss: 208.82
 ---- batch: 030 ----
mean loss: 215.76
 ---- batch: 040 ----
mean loss: 219.68
 ---- batch: 050 ----
mean loss: 207.14
 ---- batch: 060 ----
mean loss: 214.21
 ---- batch: 070 ----
mean loss: 220.92
 ---- batch: 080 ----
mean loss: 217.30
 ---- batch: 090 ----
mean loss: 208.70
 ---- batch: 100 ----
mean loss: 203.44
 ---- batch: 110 ----
mean loss: 216.25
train mean loss: 212.99
epoch train time: 0:00:02.541990
elapsed time: 0:05:51.254073
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 22:14:05.721809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.05
 ---- batch: 020 ----
mean loss: 207.11
 ---- batch: 030 ----
mean loss: 206.70
 ---- batch: 040 ----
mean loss: 216.47
 ---- batch: 050 ----
mean loss: 212.63
 ---- batch: 060 ----
mean loss: 209.13
 ---- batch: 070 ----
mean loss: 213.26
 ---- batch: 080 ----
mean loss: 217.90
 ---- batch: 090 ----
mean loss: 213.07
 ---- batch: 100 ----
mean loss: 214.97
 ---- batch: 110 ----
mean loss: 211.97
train mean loss: 212.70
epoch train time: 0:00:02.515856
elapsed time: 0:05:53.770482
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 22:14:08.238105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.98
 ---- batch: 020 ----
mean loss: 211.01
 ---- batch: 030 ----
mean loss: 213.11
 ---- batch: 040 ----
mean loss: 199.01
 ---- batch: 050 ----
mean loss: 224.19
 ---- batch: 060 ----
mean loss: 213.12
 ---- batch: 070 ----
mean loss: 209.10
 ---- batch: 080 ----
mean loss: 212.11
 ---- batch: 090 ----
mean loss: 213.10
 ---- batch: 100 ----
mean loss: 210.98
 ---- batch: 110 ----
mean loss: 211.77
train mean loss: 212.13
epoch train time: 0:00:02.526912
elapsed time: 0:05:56.297712
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 22:14:10.765449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.70
 ---- batch: 020 ----
mean loss: 206.33
 ---- batch: 030 ----
mean loss: 208.35
 ---- batch: 040 ----
mean loss: 210.44
 ---- batch: 050 ----
mean loss: 222.05
 ---- batch: 060 ----
mean loss: 212.28
 ---- batch: 070 ----
mean loss: 214.42
 ---- batch: 080 ----
mean loss: 213.21
 ---- batch: 090 ----
mean loss: 202.59
 ---- batch: 100 ----
mean loss: 218.82
 ---- batch: 110 ----
mean loss: 202.97
train mean loss: 211.91
epoch train time: 0:00:02.523588
elapsed time: 0:05:58.821746
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 22:14:13.289445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.76
 ---- batch: 020 ----
mean loss: 220.89
 ---- batch: 030 ----
mean loss: 211.57
 ---- batch: 040 ----
mean loss: 212.13
 ---- batch: 050 ----
mean loss: 214.22
 ---- batch: 060 ----
mean loss: 213.09
 ---- batch: 070 ----
mean loss: 202.55
 ---- batch: 080 ----
mean loss: 212.55
 ---- batch: 090 ----
mean loss: 209.75
 ---- batch: 100 ----
mean loss: 212.22
 ---- batch: 110 ----
mean loss: 213.14
train mean loss: 211.60
epoch train time: 0:00:02.505140
elapsed time: 0:06:01.327292
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 22:14:15.794992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.73
 ---- batch: 020 ----
mean loss: 210.27
 ---- batch: 030 ----
mean loss: 211.14
 ---- batch: 040 ----
mean loss: 220.59
 ---- batch: 050 ----
mean loss: 202.82
 ---- batch: 060 ----
mean loss: 209.88
 ---- batch: 070 ----
mean loss: 219.01
 ---- batch: 080 ----
mean loss: 212.68
 ---- batch: 090 ----
mean loss: 214.49
 ---- batch: 100 ----
mean loss: 210.02
 ---- batch: 110 ----
mean loss: 207.16
train mean loss: 211.22
epoch train time: 0:00:02.517410
elapsed time: 0:06:03.845107
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 22:14:18.312830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.14
 ---- batch: 020 ----
mean loss: 212.71
 ---- batch: 030 ----
mean loss: 214.91
 ---- batch: 040 ----
mean loss: 211.68
 ---- batch: 050 ----
mean loss: 212.13
 ---- batch: 060 ----
mean loss: 208.33
 ---- batch: 070 ----
mean loss: 211.39
 ---- batch: 080 ----
mean loss: 210.80
 ---- batch: 090 ----
mean loss: 204.59
 ---- batch: 100 ----
mean loss: 214.47
 ---- batch: 110 ----
mean loss: 207.50
train mean loss: 211.04
epoch train time: 0:00:02.507304
elapsed time: 0:06:06.352864
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 22:14:20.820601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.77
 ---- batch: 020 ----
mean loss: 217.74
 ---- batch: 030 ----
mean loss: 213.31
 ---- batch: 040 ----
mean loss: 212.76
 ---- batch: 050 ----
mean loss: 207.77
 ---- batch: 060 ----
mean loss: 212.00
 ---- batch: 070 ----
mean loss: 216.77
 ---- batch: 080 ----
mean loss: 206.15
 ---- batch: 090 ----
mean loss: 198.84
 ---- batch: 100 ----
mean loss: 208.76
 ---- batch: 110 ----
mean loss: 207.92
train mean loss: 210.42
epoch train time: 0:00:02.514245
elapsed time: 0:06:08.867618
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 22:14:23.335392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.03
 ---- batch: 020 ----
mean loss: 208.53
 ---- batch: 030 ----
mean loss: 211.61
 ---- batch: 040 ----
mean loss: 204.38
 ---- batch: 050 ----
mean loss: 209.54
 ---- batch: 060 ----
mean loss: 204.23
 ---- batch: 070 ----
mean loss: 209.94
 ---- batch: 080 ----
mean loss: 207.94
 ---- batch: 090 ----
mean loss: 217.99
 ---- batch: 100 ----
mean loss: 199.84
 ---- batch: 110 ----
mean loss: 215.31
train mean loss: 210.31
epoch train time: 0:00:02.525589
elapsed time: 0:06:11.393697
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 22:14:25.861397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.12
 ---- batch: 020 ----
mean loss: 211.87
 ---- batch: 030 ----
mean loss: 217.92
 ---- batch: 040 ----
mean loss: 211.63
 ---- batch: 050 ----
mean loss: 207.68
 ---- batch: 060 ----
mean loss: 213.16
 ---- batch: 070 ----
mean loss: 209.74
 ---- batch: 080 ----
mean loss: 204.62
 ---- batch: 090 ----
mean loss: 206.49
 ---- batch: 100 ----
mean loss: 210.45
 ---- batch: 110 ----
mean loss: 205.74
train mean loss: 210.03
epoch train time: 0:00:02.514751
elapsed time: 0:06:13.908839
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 22:14:28.376664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.88
 ---- batch: 020 ----
mean loss: 215.24
 ---- batch: 030 ----
mean loss: 218.14
 ---- batch: 040 ----
mean loss: 213.09
 ---- batch: 050 ----
mean loss: 205.09
 ---- batch: 060 ----
mean loss: 206.12
 ---- batch: 070 ----
mean loss: 207.12
 ---- batch: 080 ----
mean loss: 207.14
 ---- batch: 090 ----
mean loss: 215.88
 ---- batch: 100 ----
mean loss: 204.57
 ---- batch: 110 ----
mean loss: 208.44
train mean loss: 209.78
epoch train time: 0:00:02.545197
elapsed time: 0:06:16.454582
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 22:14:30.922331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.41
 ---- batch: 020 ----
mean loss: 214.66
 ---- batch: 030 ----
mean loss: 209.75
 ---- batch: 040 ----
mean loss: 210.09
 ---- batch: 050 ----
mean loss: 201.81
 ---- batch: 060 ----
mean loss: 203.76
 ---- batch: 070 ----
mean loss: 212.52
 ---- batch: 080 ----
mean loss: 207.25
 ---- batch: 090 ----
mean loss: 211.20
 ---- batch: 100 ----
mean loss: 203.73
 ---- batch: 110 ----
mean loss: 203.76
train mean loss: 209.37
epoch train time: 0:00:02.506052
elapsed time: 0:06:18.961159
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 22:14:33.428927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.07
 ---- batch: 020 ----
mean loss: 212.35
 ---- batch: 030 ----
mean loss: 210.73
 ---- batch: 040 ----
mean loss: 211.45
 ---- batch: 050 ----
mean loss: 214.08
 ---- batch: 060 ----
mean loss: 213.46
 ---- batch: 070 ----
mean loss: 199.21
 ---- batch: 080 ----
mean loss: 210.86
 ---- batch: 090 ----
mean loss: 199.37
 ---- batch: 100 ----
mean loss: 215.17
 ---- batch: 110 ----
mean loss: 212.66
train mean loss: 209.03
epoch train time: 0:00:02.519598
elapsed time: 0:06:21.481219
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 22:14:35.948924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.03
 ---- batch: 020 ----
mean loss: 215.73
 ---- batch: 030 ----
mean loss: 203.40
 ---- batch: 040 ----
mean loss: 216.81
 ---- batch: 050 ----
mean loss: 198.40
 ---- batch: 060 ----
mean loss: 214.76
 ---- batch: 070 ----
mean loss: 198.56
 ---- batch: 080 ----
mean loss: 209.81
 ---- batch: 090 ----
mean loss: 203.41
 ---- batch: 100 ----
mean loss: 213.84
 ---- batch: 110 ----
mean loss: 203.26
train mean loss: 208.53
epoch train time: 0:00:02.512482
elapsed time: 0:06:23.994101
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 22:14:38.461828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.83
 ---- batch: 020 ----
mean loss: 218.17
 ---- batch: 030 ----
mean loss: 212.21
 ---- batch: 040 ----
mean loss: 213.22
 ---- batch: 050 ----
mean loss: 211.00
 ---- batch: 060 ----
mean loss: 210.68
 ---- batch: 070 ----
mean loss: 206.33
 ---- batch: 080 ----
mean loss: 201.12
 ---- batch: 090 ----
mean loss: 208.17
 ---- batch: 100 ----
mean loss: 197.44
 ---- batch: 110 ----
mean loss: 210.13
train mean loss: 208.54
epoch train time: 0:00:02.537141
elapsed time: 0:06:26.531668
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 22:14:40.999442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.53
 ---- batch: 020 ----
mean loss: 208.26
 ---- batch: 030 ----
mean loss: 217.46
 ---- batch: 040 ----
mean loss: 212.76
 ---- batch: 050 ----
mean loss: 213.88
 ---- batch: 060 ----
mean loss: 199.99
 ---- batch: 070 ----
mean loss: 212.90
 ---- batch: 080 ----
mean loss: 208.38
 ---- batch: 090 ----
mean loss: 216.37
 ---- batch: 100 ----
mean loss: 205.08
 ---- batch: 110 ----
mean loss: 198.51
train mean loss: 208.28
epoch train time: 0:00:02.510697
elapsed time: 0:06:29.042837
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 22:14:43.510550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.77
 ---- batch: 020 ----
mean loss: 218.67
 ---- batch: 030 ----
mean loss: 208.39
 ---- batch: 040 ----
mean loss: 202.00
 ---- batch: 050 ----
mean loss: 202.02
 ---- batch: 060 ----
mean loss: 210.68
 ---- batch: 070 ----
mean loss: 206.18
 ---- batch: 080 ----
mean loss: 212.65
 ---- batch: 090 ----
mean loss: 207.72
 ---- batch: 100 ----
mean loss: 206.15
 ---- batch: 110 ----
mean loss: 201.86
train mean loss: 207.85
epoch train time: 0:00:02.524641
elapsed time: 0:06:31.567882
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 22:14:46.035586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.67
 ---- batch: 020 ----
mean loss: 217.07
 ---- batch: 030 ----
mean loss: 205.19
 ---- batch: 040 ----
mean loss: 207.22
 ---- batch: 050 ----
mean loss: 202.68
 ---- batch: 060 ----
mean loss: 206.25
 ---- batch: 070 ----
mean loss: 204.82
 ---- batch: 080 ----
mean loss: 205.02
 ---- batch: 090 ----
mean loss: 199.49
 ---- batch: 100 ----
mean loss: 213.89
 ---- batch: 110 ----
mean loss: 217.40
train mean loss: 207.72
epoch train time: 0:00:02.513679
elapsed time: 0:06:34.081941
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 22:14:48.549668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.22
 ---- batch: 020 ----
mean loss: 199.79
 ---- batch: 030 ----
mean loss: 198.80
 ---- batch: 040 ----
mean loss: 206.45
 ---- batch: 050 ----
mean loss: 215.42
 ---- batch: 060 ----
mean loss: 201.21
 ---- batch: 070 ----
mean loss: 209.95
 ---- batch: 080 ----
mean loss: 219.43
 ---- batch: 090 ----
mean loss: 210.57
 ---- batch: 100 ----
mean loss: 211.42
 ---- batch: 110 ----
mean loss: 199.78
train mean loss: 207.51
epoch train time: 0:00:02.539598
elapsed time: 0:06:36.621969
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 22:14:51.089727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.44
 ---- batch: 020 ----
mean loss: 206.64
 ---- batch: 030 ----
mean loss: 208.43
 ---- batch: 040 ----
mean loss: 213.83
 ---- batch: 050 ----
mean loss: 204.50
 ---- batch: 060 ----
mean loss: 208.74
 ---- batch: 070 ----
mean loss: 203.27
 ---- batch: 080 ----
mean loss: 216.07
 ---- batch: 090 ----
mean loss: 202.49
 ---- batch: 100 ----
mean loss: 210.27
 ---- batch: 110 ----
mean loss: 203.18
train mean loss: 207.18
epoch train time: 0:00:02.505048
elapsed time: 0:06:39.127489
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 22:14:53.595250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.81
 ---- batch: 020 ----
mean loss: 210.67
 ---- batch: 030 ----
mean loss: 216.83
 ---- batch: 040 ----
mean loss: 201.82
 ---- batch: 050 ----
mean loss: 213.13
 ---- batch: 060 ----
mean loss: 210.80
 ---- batch: 070 ----
mean loss: 205.41
 ---- batch: 080 ----
mean loss: 206.26
 ---- batch: 090 ----
mean loss: 205.67
 ---- batch: 100 ----
mean loss: 204.07
 ---- batch: 110 ----
mean loss: 206.82
train mean loss: 206.80
epoch train time: 0:00:02.512433
elapsed time: 0:06:41.640405
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 22:14:56.108130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.44
 ---- batch: 020 ----
mean loss: 210.85
 ---- batch: 030 ----
mean loss: 201.02
 ---- batch: 040 ----
mean loss: 209.75
 ---- batch: 050 ----
mean loss: 201.85
 ---- batch: 060 ----
mean loss: 206.69
 ---- batch: 070 ----
mean loss: 211.35
 ---- batch: 080 ----
mean loss: 204.43
 ---- batch: 090 ----
mean loss: 207.93
 ---- batch: 100 ----
mean loss: 200.35
 ---- batch: 110 ----
mean loss: 207.90
train mean loss: 206.77
epoch train time: 0:00:02.490775
elapsed time: 0:06:44.131604
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 22:14:58.599314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.28
 ---- batch: 020 ----
mean loss: 210.95
 ---- batch: 030 ----
mean loss: 205.50
 ---- batch: 040 ----
mean loss: 202.07
 ---- batch: 050 ----
mean loss: 215.42
 ---- batch: 060 ----
mean loss: 202.95
 ---- batch: 070 ----
mean loss: 207.96
 ---- batch: 080 ----
mean loss: 205.26
 ---- batch: 090 ----
mean loss: 203.96
 ---- batch: 100 ----
mean loss: 215.48
 ---- batch: 110 ----
mean loss: 195.71
train mean loss: 206.35
epoch train time: 0:00:02.537767
elapsed time: 0:06:46.669762
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 22:15:01.137488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.49
 ---- batch: 020 ----
mean loss: 209.11
 ---- batch: 030 ----
mean loss: 212.63
 ---- batch: 040 ----
mean loss: 200.53
 ---- batch: 050 ----
mean loss: 209.48
 ---- batch: 060 ----
mean loss: 196.94
 ---- batch: 070 ----
mean loss: 201.90
 ---- batch: 080 ----
mean loss: 215.34
 ---- batch: 090 ----
mean loss: 211.14
 ---- batch: 100 ----
mean loss: 210.34
 ---- batch: 110 ----
mean loss: 198.07
train mean loss: 206.26
epoch train time: 0:00:02.501587
elapsed time: 0:06:49.171890
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 22:15:03.639498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.73
 ---- batch: 020 ----
mean loss: 202.12
 ---- batch: 030 ----
mean loss: 199.31
 ---- batch: 040 ----
mean loss: 191.79
 ---- batch: 050 ----
mean loss: 204.99
 ---- batch: 060 ----
mean loss: 208.26
 ---- batch: 070 ----
mean loss: 211.55
 ---- batch: 080 ----
mean loss: 211.73
 ---- batch: 090 ----
mean loss: 209.66
 ---- batch: 100 ----
mean loss: 209.62
 ---- batch: 110 ----
mean loss: 204.34
train mean loss: 205.94
epoch train time: 0:00:02.542860
elapsed time: 0:06:51.715062
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 22:15:06.182782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.87
 ---- batch: 020 ----
mean loss: 200.99
 ---- batch: 030 ----
mean loss: 212.48
 ---- batch: 040 ----
mean loss: 200.57
 ---- batch: 050 ----
mean loss: 206.75
 ---- batch: 060 ----
mean loss: 212.21
 ---- batch: 070 ----
mean loss: 200.92
 ---- batch: 080 ----
mean loss: 212.48
 ---- batch: 090 ----
mean loss: 206.57
 ---- batch: 100 ----
mean loss: 199.54
 ---- batch: 110 ----
mean loss: 202.85
train mean loss: 205.86
epoch train time: 0:00:02.537229
elapsed time: 0:06:54.252773
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 22:15:08.720511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.90
 ---- batch: 020 ----
mean loss: 200.66
 ---- batch: 030 ----
mean loss: 209.22
 ---- batch: 040 ----
mean loss: 207.49
 ---- batch: 050 ----
mean loss: 204.30
 ---- batch: 060 ----
mean loss: 209.75
 ---- batch: 070 ----
mean loss: 205.28
 ---- batch: 080 ----
mean loss: 203.86
 ---- batch: 090 ----
mean loss: 205.37
 ---- batch: 100 ----
mean loss: 210.64
 ---- batch: 110 ----
mean loss: 198.31
train mean loss: 205.60
epoch train time: 0:00:02.502290
elapsed time: 0:06:56.755494
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 22:15:11.223201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.40
 ---- batch: 020 ----
mean loss: 204.04
 ---- batch: 030 ----
mean loss: 204.20
 ---- batch: 040 ----
mean loss: 197.18
 ---- batch: 050 ----
mean loss: 206.43
 ---- batch: 060 ----
mean loss: 201.45
 ---- batch: 070 ----
mean loss: 198.50
 ---- batch: 080 ----
mean loss: 213.26
 ---- batch: 090 ----
mean loss: 210.75
 ---- batch: 100 ----
mean loss: 204.19
 ---- batch: 110 ----
mean loss: 198.98
train mean loss: 205.14
epoch train time: 0:00:02.526897
elapsed time: 0:06:59.282841
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 22:15:13.750563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.60
 ---- batch: 020 ----
mean loss: 193.52
 ---- batch: 030 ----
mean loss: 207.45
 ---- batch: 040 ----
mean loss: 200.51
 ---- batch: 050 ----
mean loss: 212.84
 ---- batch: 060 ----
mean loss: 208.75
 ---- batch: 070 ----
mean loss: 209.78
 ---- batch: 080 ----
mean loss: 203.72
 ---- batch: 090 ----
mean loss: 200.68
 ---- batch: 100 ----
mean loss: 204.46
 ---- batch: 110 ----
mean loss: 206.90
train mean loss: 204.97
epoch train time: 0:00:02.559972
elapsed time: 0:07:01.843248
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 22:15:16.310968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.42
 ---- batch: 020 ----
mean loss: 200.40
 ---- batch: 030 ----
mean loss: 207.28
 ---- batch: 040 ----
mean loss: 198.81
 ---- batch: 050 ----
mean loss: 205.35
 ---- batch: 060 ----
mean loss: 202.01
 ---- batch: 070 ----
mean loss: 207.24
 ---- batch: 080 ----
mean loss: 204.67
 ---- batch: 090 ----
mean loss: 209.71
 ---- batch: 100 ----
mean loss: 200.83
 ---- batch: 110 ----
mean loss: 210.45
train mean loss: 204.57
epoch train time: 0:00:02.595871
elapsed time: 0:07:04.439570
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 22:15:18.907293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.08
 ---- batch: 020 ----
mean loss: 199.87
 ---- batch: 030 ----
mean loss: 202.06
 ---- batch: 040 ----
mean loss: 207.39
 ---- batch: 050 ----
mean loss: 198.30
 ---- batch: 060 ----
mean loss: 201.73
 ---- batch: 070 ----
mean loss: 210.68
 ---- batch: 080 ----
mean loss: 201.69
 ---- batch: 090 ----
mean loss: 208.42
 ---- batch: 100 ----
mean loss: 206.01
 ---- batch: 110 ----
mean loss: 207.96
train mean loss: 204.63
epoch train time: 0:00:02.566936
elapsed time: 0:07:07.006927
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 22:15:21.474640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.55
 ---- batch: 020 ----
mean loss: 202.29
 ---- batch: 030 ----
mean loss: 210.08
 ---- batch: 040 ----
mean loss: 217.40
 ---- batch: 050 ----
mean loss: 202.23
 ---- batch: 060 ----
mean loss: 205.50
 ---- batch: 070 ----
mean loss: 205.25
 ---- batch: 080 ----
mean loss: 199.25
 ---- batch: 090 ----
mean loss: 197.12
 ---- batch: 100 ----
mean loss: 204.28
 ---- batch: 110 ----
mean loss: 199.03
train mean loss: 204.53
epoch train time: 0:00:02.531551
elapsed time: 0:07:09.538895
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 22:15:24.006605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.52
 ---- batch: 020 ----
mean loss: 202.60
 ---- batch: 030 ----
mean loss: 206.26
 ---- batch: 040 ----
mean loss: 202.38
 ---- batch: 050 ----
mean loss: 194.42
 ---- batch: 060 ----
mean loss: 211.58
 ---- batch: 070 ----
mean loss: 196.05
 ---- batch: 080 ----
mean loss: 206.16
 ---- batch: 090 ----
mean loss: 204.32
 ---- batch: 100 ----
mean loss: 209.38
 ---- batch: 110 ----
mean loss: 210.07
train mean loss: 204.27
epoch train time: 0:00:02.525907
elapsed time: 0:07:12.065209
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 22:15:26.532977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.26
 ---- batch: 020 ----
mean loss: 201.84
 ---- batch: 030 ----
mean loss: 206.60
 ---- batch: 040 ----
mean loss: 196.03
 ---- batch: 050 ----
mean loss: 200.88
 ---- batch: 060 ----
mean loss: 202.67
 ---- batch: 070 ----
mean loss: 204.54
 ---- batch: 080 ----
mean loss: 200.63
 ---- batch: 090 ----
mean loss: 197.42
 ---- batch: 100 ----
mean loss: 206.70
 ---- batch: 110 ----
mean loss: 213.80
train mean loss: 203.85
epoch train time: 0:00:02.533626
elapsed time: 0:07:14.599341
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 22:15:29.067151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.84
 ---- batch: 020 ----
mean loss: 200.84
 ---- batch: 030 ----
mean loss: 200.92
 ---- batch: 040 ----
mean loss: 205.88
 ---- batch: 050 ----
mean loss: 206.66
 ---- batch: 060 ----
mean loss: 198.78
 ---- batch: 070 ----
mean loss: 210.36
 ---- batch: 080 ----
mean loss: 201.81
 ---- batch: 090 ----
mean loss: 198.48
 ---- batch: 100 ----
mean loss: 213.93
 ---- batch: 110 ----
mean loss: 199.14
train mean loss: 203.85
epoch train time: 0:00:02.528359
elapsed time: 0:07:17.128213
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 22:15:31.595934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.51
 ---- batch: 020 ----
mean loss: 201.47
 ---- batch: 030 ----
mean loss: 197.46
 ---- batch: 040 ----
mean loss: 205.62
 ---- batch: 050 ----
mean loss: 220.42
 ---- batch: 060 ----
mean loss: 197.07
 ---- batch: 070 ----
mean loss: 198.68
 ---- batch: 080 ----
mean loss: 212.08
 ---- batch: 090 ----
mean loss: 208.09
 ---- batch: 100 ----
mean loss: 191.85
 ---- batch: 110 ----
mean loss: 199.08
train mean loss: 203.38
epoch train time: 0:00:02.533443
elapsed time: 0:07:19.662076
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 22:15:34.129805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.84
 ---- batch: 020 ----
mean loss: 205.41
 ---- batch: 030 ----
mean loss: 207.01
 ---- batch: 040 ----
mean loss: 205.19
 ---- batch: 050 ----
mean loss: 213.06
 ---- batch: 060 ----
mean loss: 203.82
 ---- batch: 070 ----
mean loss: 199.83
 ---- batch: 080 ----
mean loss: 201.52
 ---- batch: 090 ----
mean loss: 203.38
 ---- batch: 100 ----
mean loss: 209.38
 ---- batch: 110 ----
mean loss: 199.52
train mean loss: 203.29
epoch train time: 0:00:02.501696
elapsed time: 0:07:22.164212
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 22:15:36.631944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.93
 ---- batch: 020 ----
mean loss: 211.04
 ---- batch: 030 ----
mean loss: 207.47
 ---- batch: 040 ----
mean loss: 194.78
 ---- batch: 050 ----
mean loss: 214.44
 ---- batch: 060 ----
mean loss: 204.23
 ---- batch: 070 ----
mean loss: 193.98
 ---- batch: 080 ----
mean loss: 186.80
 ---- batch: 090 ----
mean loss: 200.13
 ---- batch: 100 ----
mean loss: 205.90
 ---- batch: 110 ----
mean loss: 208.30
train mean loss: 203.27
epoch train time: 0:00:02.551471
elapsed time: 0:07:24.716200
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 22:15:39.183963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.89
 ---- batch: 020 ----
mean loss: 206.22
 ---- batch: 030 ----
mean loss: 210.33
 ---- batch: 040 ----
mean loss: 210.76
 ---- batch: 050 ----
mean loss: 201.75
 ---- batch: 060 ----
mean loss: 201.27
 ---- batch: 070 ----
mean loss: 202.87
 ---- batch: 080 ----
mean loss: 202.83
 ---- batch: 090 ----
mean loss: 195.06
 ---- batch: 100 ----
mean loss: 193.64
 ---- batch: 110 ----
mean loss: 201.10
train mean loss: 202.79
epoch train time: 0:00:02.534366
elapsed time: 0:07:27.251026
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 22:15:41.718750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.05
 ---- batch: 020 ----
mean loss: 207.57
 ---- batch: 030 ----
mean loss: 199.93
 ---- batch: 040 ----
mean loss: 202.25
 ---- batch: 050 ----
mean loss: 207.12
 ---- batch: 060 ----
mean loss: 199.67
 ---- batch: 070 ----
mean loss: 216.29
 ---- batch: 080 ----
mean loss: 198.73
 ---- batch: 090 ----
mean loss: 194.09
 ---- batch: 100 ----
mean loss: 201.08
 ---- batch: 110 ----
mean loss: 202.34
train mean loss: 202.76
epoch train time: 0:00:02.504724
elapsed time: 0:07:29.756167
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 22:15:44.223881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.91
 ---- batch: 020 ----
mean loss: 195.88
 ---- batch: 030 ----
mean loss: 206.77
 ---- batch: 040 ----
mean loss: 201.32
 ---- batch: 050 ----
mean loss: 214.86
 ---- batch: 060 ----
mean loss: 201.84
 ---- batch: 070 ----
mean loss: 200.84
 ---- batch: 080 ----
mean loss: 206.11
 ---- batch: 090 ----
mean loss: 205.18
 ---- batch: 100 ----
mean loss: 197.15
 ---- batch: 110 ----
mean loss: 193.01
train mean loss: 202.58
epoch train time: 0:00:02.514597
elapsed time: 0:07:32.271173
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 22:15:46.738895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.31
 ---- batch: 020 ----
mean loss: 204.35
 ---- batch: 030 ----
mean loss: 199.60
 ---- batch: 040 ----
mean loss: 198.11
 ---- batch: 050 ----
mean loss: 193.80
 ---- batch: 060 ----
mean loss: 203.40
 ---- batch: 070 ----
mean loss: 188.85
 ---- batch: 080 ----
mean loss: 213.86
 ---- batch: 090 ----
mean loss: 206.54
 ---- batch: 100 ----
mean loss: 217.60
 ---- batch: 110 ----
mean loss: 197.31
train mean loss: 202.46
epoch train time: 0:00:02.523596
elapsed time: 0:07:34.795179
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 22:15:49.262906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.59
 ---- batch: 020 ----
mean loss: 201.62
 ---- batch: 030 ----
mean loss: 199.26
 ---- batch: 040 ----
mean loss: 195.81
 ---- batch: 050 ----
mean loss: 202.93
 ---- batch: 060 ----
mean loss: 199.29
 ---- batch: 070 ----
mean loss: 200.13
 ---- batch: 080 ----
mean loss: 198.04
 ---- batch: 090 ----
mean loss: 208.87
 ---- batch: 100 ----
mean loss: 207.79
 ---- batch: 110 ----
mean loss: 206.84
train mean loss: 202.06
epoch train time: 0:00:02.528159
elapsed time: 0:07:37.323774
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 22:15:51.791504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.67
 ---- batch: 020 ----
mean loss: 209.79
 ---- batch: 030 ----
mean loss: 202.91
 ---- batch: 040 ----
mean loss: 211.78
 ---- batch: 050 ----
mean loss: 213.75
 ---- batch: 060 ----
mean loss: 196.35
 ---- batch: 070 ----
mean loss: 190.51
 ---- batch: 080 ----
mean loss: 201.63
 ---- batch: 090 ----
mean loss: 194.68
 ---- batch: 100 ----
mean loss: 197.42
 ---- batch: 110 ----
mean loss: 197.11
train mean loss: 202.03
epoch train time: 0:00:02.512636
elapsed time: 0:07:39.836863
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 22:15:54.304630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.90
 ---- batch: 020 ----
mean loss: 202.34
 ---- batch: 030 ----
mean loss: 189.90
 ---- batch: 040 ----
mean loss: 210.26
 ---- batch: 050 ----
mean loss: 206.66
 ---- batch: 060 ----
mean loss: 206.56
 ---- batch: 070 ----
mean loss: 201.14
 ---- batch: 080 ----
mean loss: 207.02
 ---- batch: 090 ----
mean loss: 197.40
 ---- batch: 100 ----
mean loss: 203.40
 ---- batch: 110 ----
mean loss: 196.32
train mean loss: 201.84
epoch train time: 0:00:02.522133
elapsed time: 0:07:42.359505
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 22:15:56.827438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.95
 ---- batch: 020 ----
mean loss: 208.20
 ---- batch: 030 ----
mean loss: 206.24
 ---- batch: 040 ----
mean loss: 197.74
 ---- batch: 050 ----
mean loss: 196.99
 ---- batch: 060 ----
mean loss: 205.82
 ---- batch: 070 ----
mean loss: 199.69
 ---- batch: 080 ----
mean loss: 201.77
 ---- batch: 090 ----
mean loss: 204.63
 ---- batch: 100 ----
mean loss: 196.32
 ---- batch: 110 ----
mean loss: 196.65
train mean loss: 201.53
epoch train time: 0:00:02.563790
elapsed time: 0:07:44.923924
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 22:15:59.391665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.62
 ---- batch: 020 ----
mean loss: 211.35
 ---- batch: 030 ----
mean loss: 207.73
 ---- batch: 040 ----
mean loss: 198.96
 ---- batch: 050 ----
mean loss: 203.45
 ---- batch: 060 ----
mean loss: 194.40
 ---- batch: 070 ----
mean loss: 206.50
 ---- batch: 080 ----
mean loss: 198.45
 ---- batch: 090 ----
mean loss: 202.04
 ---- batch: 100 ----
mean loss: 200.49
 ---- batch: 110 ----
mean loss: 199.90
train mean loss: 201.28
epoch train time: 0:00:02.524550
elapsed time: 0:07:47.448911
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 22:16:01.916625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.99
 ---- batch: 020 ----
mean loss: 207.41
 ---- batch: 030 ----
mean loss: 200.24
 ---- batch: 040 ----
mean loss: 194.53
 ---- batch: 050 ----
mean loss: 202.33
 ---- batch: 060 ----
mean loss: 202.97
 ---- batch: 070 ----
mean loss: 190.61
 ---- batch: 080 ----
mean loss: 205.16
 ---- batch: 090 ----
mean loss: 209.56
 ---- batch: 100 ----
mean loss: 197.81
 ---- batch: 110 ----
mean loss: 205.16
train mean loss: 201.11
epoch train time: 0:00:02.522671
elapsed time: 0:07:49.971997
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 22:16:04.439734
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.81
 ---- batch: 020 ----
mean loss: 199.79
 ---- batch: 030 ----
mean loss: 207.29
 ---- batch: 040 ----
mean loss: 201.86
 ---- batch: 050 ----
mean loss: 192.16
 ---- batch: 060 ----
mean loss: 200.61
 ---- batch: 070 ----
mean loss: 203.50
 ---- batch: 080 ----
mean loss: 200.77
 ---- batch: 090 ----
mean loss: 206.31
 ---- batch: 100 ----
mean loss: 191.80
 ---- batch: 110 ----
mean loss: 204.24
train mean loss: 200.97
epoch train time: 0:00:02.550116
elapsed time: 0:07:52.522702
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 22:16:06.990297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.05
 ---- batch: 020 ----
mean loss: 208.01
 ---- batch: 030 ----
mean loss: 199.78
 ---- batch: 040 ----
mean loss: 201.68
 ---- batch: 050 ----
mean loss: 205.78
 ---- batch: 060 ----
mean loss: 200.33
 ---- batch: 070 ----
mean loss: 198.21
 ---- batch: 080 ----
mean loss: 192.43
 ---- batch: 090 ----
mean loss: 198.25
 ---- batch: 100 ----
mean loss: 203.76
 ---- batch: 110 ----
mean loss: 199.53
train mean loss: 200.72
epoch train time: 0:00:02.521098
elapsed time: 0:07:55.044078
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 22:16:09.511825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.12
 ---- batch: 020 ----
mean loss: 202.87
 ---- batch: 030 ----
mean loss: 198.90
 ---- batch: 040 ----
mean loss: 198.38
 ---- batch: 050 ----
mean loss: 194.87
 ---- batch: 060 ----
mean loss: 203.24
 ---- batch: 070 ----
mean loss: 204.36
 ---- batch: 080 ----
mean loss: 195.14
 ---- batch: 090 ----
mean loss: 205.04
 ---- batch: 100 ----
mean loss: 198.48
 ---- batch: 110 ----
mean loss: 195.40
train mean loss: 200.64
epoch train time: 0:00:02.515633
elapsed time: 0:07:57.560172
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 22:16:12.027912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.95
 ---- batch: 020 ----
mean loss: 199.33
 ---- batch: 030 ----
mean loss: 187.69
 ---- batch: 040 ----
mean loss: 211.49
 ---- batch: 050 ----
mean loss: 205.03
 ---- batch: 060 ----
mean loss: 200.05
 ---- batch: 070 ----
mean loss: 203.68
 ---- batch: 080 ----
mean loss: 200.68
 ---- batch: 090 ----
mean loss: 195.97
 ---- batch: 100 ----
mean loss: 200.78
 ---- batch: 110 ----
mean loss: 195.92
train mean loss: 200.38
epoch train time: 0:00:02.526018
elapsed time: 0:08:00.086643
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 22:16:14.554394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.42
 ---- batch: 020 ----
mean loss: 209.16
 ---- batch: 030 ----
mean loss: 205.96
 ---- batch: 040 ----
mean loss: 191.98
 ---- batch: 050 ----
mean loss: 197.89
 ---- batch: 060 ----
mean loss: 199.61
 ---- batch: 070 ----
mean loss: 192.90
 ---- batch: 080 ----
mean loss: 203.56
 ---- batch: 090 ----
mean loss: 200.00
 ---- batch: 100 ----
mean loss: 201.20
 ---- batch: 110 ----
mean loss: 191.10
train mean loss: 200.23
epoch train time: 0:00:02.527509
elapsed time: 0:08:02.614607
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 22:16:17.082314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.02
 ---- batch: 020 ----
mean loss: 206.55
 ---- batch: 030 ----
mean loss: 200.14
 ---- batch: 040 ----
mean loss: 195.73
 ---- batch: 050 ----
mean loss: 192.67
 ---- batch: 060 ----
mean loss: 202.15
 ---- batch: 070 ----
mean loss: 199.43
 ---- batch: 080 ----
mean loss: 196.30
 ---- batch: 090 ----
mean loss: 207.62
 ---- batch: 100 ----
mean loss: 195.63
 ---- batch: 110 ----
mean loss: 206.27
train mean loss: 200.13
epoch train time: 0:00:02.532857
elapsed time: 0:08:05.147859
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 22:16:19.615557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.72
 ---- batch: 020 ----
mean loss: 192.02
 ---- batch: 030 ----
mean loss: 202.97
 ---- batch: 040 ----
mean loss: 205.04
 ---- batch: 050 ----
mean loss: 207.71
 ---- batch: 060 ----
mean loss: 194.29
 ---- batch: 070 ----
mean loss: 206.26
 ---- batch: 080 ----
mean loss: 200.34
 ---- batch: 090 ----
mean loss: 195.35
 ---- batch: 100 ----
mean loss: 193.65
 ---- batch: 110 ----
mean loss: 209.72
train mean loss: 200.08
epoch train time: 0:00:02.541278
elapsed time: 0:08:07.689569
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 22:16:22.157340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.88
 ---- batch: 020 ----
mean loss: 207.52
 ---- batch: 030 ----
mean loss: 195.38
 ---- batch: 040 ----
mean loss: 191.72
 ---- batch: 050 ----
mean loss: 208.10
 ---- batch: 060 ----
mean loss: 204.35
 ---- batch: 070 ----
mean loss: 201.97
 ---- batch: 080 ----
mean loss: 200.26
 ---- batch: 090 ----
mean loss: 191.75
 ---- batch: 100 ----
mean loss: 200.05
 ---- batch: 110 ----
mean loss: 200.97
train mean loss: 199.64
epoch train time: 0:00:02.529478
elapsed time: 0:08:10.219530
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 22:16:24.687240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.17
 ---- batch: 020 ----
mean loss: 201.96
 ---- batch: 030 ----
mean loss: 212.62
 ---- batch: 040 ----
mean loss: 189.74
 ---- batch: 050 ----
mean loss: 196.39
 ---- batch: 060 ----
mean loss: 188.94
 ---- batch: 070 ----
mean loss: 194.33
 ---- batch: 080 ----
mean loss: 203.45
 ---- batch: 090 ----
mean loss: 202.56
 ---- batch: 100 ----
mean loss: 203.11
 ---- batch: 110 ----
mean loss: 201.50
train mean loss: 199.59
epoch train time: 0:00:02.524742
elapsed time: 0:08:12.744696
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 22:16:27.212452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.52
 ---- batch: 020 ----
mean loss: 206.28
 ---- batch: 030 ----
mean loss: 198.49
 ---- batch: 040 ----
mean loss: 205.23
 ---- batch: 050 ----
mean loss: 195.37
 ---- batch: 060 ----
mean loss: 203.68
 ---- batch: 070 ----
mean loss: 201.34
 ---- batch: 080 ----
mean loss: 192.78
 ---- batch: 090 ----
mean loss: 200.44
 ---- batch: 100 ----
mean loss: 195.58
 ---- batch: 110 ----
mean loss: 191.50
train mean loss: 199.44
epoch train time: 0:00:02.529375
elapsed time: 0:08:15.274542
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 22:16:29.742254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.43
 ---- batch: 020 ----
mean loss: 201.81
 ---- batch: 030 ----
mean loss: 190.88
 ---- batch: 040 ----
mean loss: 204.20
 ---- batch: 050 ----
mean loss: 188.43
 ---- batch: 060 ----
mean loss: 198.25
 ---- batch: 070 ----
mean loss: 196.81
 ---- batch: 080 ----
mean loss: 202.10
 ---- batch: 090 ----
mean loss: 211.21
 ---- batch: 100 ----
mean loss: 204.25
 ---- batch: 110 ----
mean loss: 195.84
train mean loss: 199.18
epoch train time: 0:00:02.528600
elapsed time: 0:08:17.803575
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 22:16:32.271304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.59
 ---- batch: 020 ----
mean loss: 201.70
 ---- batch: 030 ----
mean loss: 201.85
 ---- batch: 040 ----
mean loss: 195.01
 ---- batch: 050 ----
mean loss: 204.84
 ---- batch: 060 ----
mean loss: 205.60
 ---- batch: 070 ----
mean loss: 197.41
 ---- batch: 080 ----
mean loss: 199.23
 ---- batch: 090 ----
mean loss: 191.64
 ---- batch: 100 ----
mean loss: 195.32
 ---- batch: 110 ----
mean loss: 203.40
train mean loss: 198.86
epoch train time: 0:00:02.533964
elapsed time: 0:08:20.338045
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 22:16:34.805950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.85
 ---- batch: 020 ----
mean loss: 189.91
 ---- batch: 030 ----
mean loss: 205.11
 ---- batch: 040 ----
mean loss: 198.28
 ---- batch: 050 ----
mean loss: 201.33
 ---- batch: 060 ----
mean loss: 199.22
 ---- batch: 070 ----
mean loss: 202.14
 ---- batch: 080 ----
mean loss: 193.60
 ---- batch: 090 ----
mean loss: 196.59
 ---- batch: 100 ----
mean loss: 202.83
 ---- batch: 110 ----
mean loss: 203.84
train mean loss: 199.04
epoch train time: 0:00:02.539804
elapsed time: 0:08:22.878470
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 22:16:37.346201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.04
 ---- batch: 020 ----
mean loss: 207.21
 ---- batch: 030 ----
mean loss: 203.79
 ---- batch: 040 ----
mean loss: 195.66
 ---- batch: 050 ----
mean loss: 197.80
 ---- batch: 060 ----
mean loss: 194.67
 ---- batch: 070 ----
mean loss: 201.35
 ---- batch: 080 ----
mean loss: 197.83
 ---- batch: 090 ----
mean loss: 190.20
 ---- batch: 100 ----
mean loss: 191.11
 ---- batch: 110 ----
mean loss: 200.79
train mean loss: 198.66
epoch train time: 0:00:02.570385
elapsed time: 0:08:25.449325
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 22:16:39.917036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.61
 ---- batch: 020 ----
mean loss: 191.80
 ---- batch: 030 ----
mean loss: 198.69
 ---- batch: 040 ----
mean loss: 201.73
 ---- batch: 050 ----
mean loss: 202.33
 ---- batch: 060 ----
mean loss: 209.73
 ---- batch: 070 ----
mean loss: 196.18
 ---- batch: 080 ----
mean loss: 191.98
 ---- batch: 090 ----
mean loss: 205.16
 ---- batch: 100 ----
mean loss: 200.25
 ---- batch: 110 ----
mean loss: 197.38
train mean loss: 198.71
epoch train time: 0:00:02.548050
elapsed time: 0:08:27.997775
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 22:16:42.465483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.03
 ---- batch: 020 ----
mean loss: 199.51
 ---- batch: 030 ----
mean loss: 209.18
 ---- batch: 040 ----
mean loss: 192.79
 ---- batch: 050 ----
mean loss: 201.97
 ---- batch: 060 ----
mean loss: 195.69
 ---- batch: 070 ----
mean loss: 200.55
 ---- batch: 080 ----
mean loss: 194.05
 ---- batch: 090 ----
mean loss: 196.65
 ---- batch: 100 ----
mean loss: 199.59
 ---- batch: 110 ----
mean loss: 193.69
train mean loss: 198.45
epoch train time: 0:00:02.533103
elapsed time: 0:08:30.531278
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 22:16:44.999013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.87
 ---- batch: 020 ----
mean loss: 195.51
 ---- batch: 030 ----
mean loss: 209.96
 ---- batch: 040 ----
mean loss: 201.09
 ---- batch: 050 ----
mean loss: 197.42
 ---- batch: 060 ----
mean loss: 198.84
 ---- batch: 070 ----
mean loss: 197.09
 ---- batch: 080 ----
mean loss: 206.26
 ---- batch: 090 ----
mean loss: 198.05
 ---- batch: 100 ----
mean loss: 191.69
 ---- batch: 110 ----
mean loss: 188.26
train mean loss: 198.33
epoch train time: 0:00:02.536190
elapsed time: 0:08:33.067897
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 22:16:47.535645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.29
 ---- batch: 020 ----
mean loss: 193.96
 ---- batch: 030 ----
mean loss: 206.54
 ---- batch: 040 ----
mean loss: 195.32
 ---- batch: 050 ----
mean loss: 192.72
 ---- batch: 060 ----
mean loss: 202.98
 ---- batch: 070 ----
mean loss: 207.23
 ---- batch: 080 ----
mean loss: 196.18
 ---- batch: 090 ----
mean loss: 193.83
 ---- batch: 100 ----
mean loss: 200.41
 ---- batch: 110 ----
mean loss: 203.53
train mean loss: 198.27
epoch train time: 0:00:02.530285
elapsed time: 0:08:35.598700
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 22:16:50.066427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.67
 ---- batch: 020 ----
mean loss: 191.07
 ---- batch: 030 ----
mean loss: 198.15
 ---- batch: 040 ----
mean loss: 199.37
 ---- batch: 050 ----
mean loss: 204.94
 ---- batch: 060 ----
mean loss: 202.25
 ---- batch: 070 ----
mean loss: 194.62
 ---- batch: 080 ----
mean loss: 192.78
 ---- batch: 090 ----
mean loss: 200.19
 ---- batch: 100 ----
mean loss: 201.76
 ---- batch: 110 ----
mean loss: 191.82
train mean loss: 197.97
epoch train time: 0:00:02.528161
elapsed time: 0:08:38.127285
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 22:16:52.595069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.31
 ---- batch: 020 ----
mean loss: 193.90
 ---- batch: 030 ----
mean loss: 194.08
 ---- batch: 040 ----
mean loss: 198.12
 ---- batch: 050 ----
mean loss: 201.70
 ---- batch: 060 ----
mean loss: 208.64
 ---- batch: 070 ----
mean loss: 190.63
 ---- batch: 080 ----
mean loss: 198.78
 ---- batch: 090 ----
mean loss: 195.59
 ---- batch: 100 ----
mean loss: 191.00
 ---- batch: 110 ----
mean loss: 198.58
train mean loss: 197.82
epoch train time: 0:00:02.528459
elapsed time: 0:08:40.656243
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 22:16:55.123964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.26
 ---- batch: 020 ----
mean loss: 200.57
 ---- batch: 030 ----
mean loss: 195.33
 ---- batch: 040 ----
mean loss: 201.43
 ---- batch: 050 ----
mean loss: 205.62
 ---- batch: 060 ----
mean loss: 190.51
 ---- batch: 070 ----
mean loss: 189.52
 ---- batch: 080 ----
mean loss: 197.07
 ---- batch: 090 ----
mean loss: 189.05
 ---- batch: 100 ----
mean loss: 204.19
 ---- batch: 110 ----
mean loss: 204.54
train mean loss: 197.67
epoch train time: 0:00:02.517293
elapsed time: 0:08:43.173990
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 22:16:57.641788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.35
 ---- batch: 020 ----
mean loss: 205.16
 ---- batch: 030 ----
mean loss: 203.92
 ---- batch: 040 ----
mean loss: 197.18
 ---- batch: 050 ----
mean loss: 191.26
 ---- batch: 060 ----
mean loss: 195.26
 ---- batch: 070 ----
mean loss: 203.75
 ---- batch: 080 ----
mean loss: 193.12
 ---- batch: 090 ----
mean loss: 197.05
 ---- batch: 100 ----
mean loss: 203.18
 ---- batch: 110 ----
mean loss: 189.67
train mean loss: 197.43
epoch train time: 0:00:02.529763
elapsed time: 0:08:45.704253
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 22:17:00.172001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.97
 ---- batch: 020 ----
mean loss: 204.66
 ---- batch: 030 ----
mean loss: 205.22
 ---- batch: 040 ----
mean loss: 199.85
 ---- batch: 050 ----
mean loss: 187.61
 ---- batch: 060 ----
mean loss: 192.57
 ---- batch: 070 ----
mean loss: 197.48
 ---- batch: 080 ----
mean loss: 195.12
 ---- batch: 090 ----
mean loss: 201.57
 ---- batch: 100 ----
mean loss: 194.65
 ---- batch: 110 ----
mean loss: 201.96
train mean loss: 197.33
epoch train time: 0:00:02.531503
elapsed time: 0:08:48.236357
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 22:17:02.704220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.86
 ---- batch: 020 ----
mean loss: 199.54
 ---- batch: 030 ----
mean loss: 193.04
 ---- batch: 040 ----
mean loss: 206.37
 ---- batch: 050 ----
mean loss: 193.33
 ---- batch: 060 ----
mean loss: 196.25
 ---- batch: 070 ----
mean loss: 196.66
 ---- batch: 080 ----
mean loss: 196.02
 ---- batch: 090 ----
mean loss: 193.55
 ---- batch: 100 ----
mean loss: 185.61
 ---- batch: 110 ----
mean loss: 205.28
train mean loss: 197.10
epoch train time: 0:00:02.524547
elapsed time: 0:08:50.761466
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 22:17:05.229245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.17
 ---- batch: 020 ----
mean loss: 194.23
 ---- batch: 030 ----
mean loss: 204.06
 ---- batch: 040 ----
mean loss: 189.24
 ---- batch: 050 ----
mean loss: 203.02
 ---- batch: 060 ----
mean loss: 190.66
 ---- batch: 070 ----
mean loss: 211.07
 ---- batch: 080 ----
mean loss: 201.62
 ---- batch: 090 ----
mean loss: 195.31
 ---- batch: 100 ----
mean loss: 191.13
 ---- batch: 110 ----
mean loss: 191.25
train mean loss: 197.03
epoch train time: 0:00:02.517160
elapsed time: 0:08:53.279114
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 22:17:07.746828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.41
 ---- batch: 020 ----
mean loss: 194.57
 ---- batch: 030 ----
mean loss: 197.96
 ---- batch: 040 ----
mean loss: 197.13
 ---- batch: 050 ----
mean loss: 193.40
 ---- batch: 060 ----
mean loss: 205.01
 ---- batch: 070 ----
mean loss: 190.28
 ---- batch: 080 ----
mean loss: 192.03
 ---- batch: 090 ----
mean loss: 198.22
 ---- batch: 100 ----
mean loss: 185.06
 ---- batch: 110 ----
mean loss: 203.61
train mean loss: 196.91
epoch train time: 0:00:02.539699
elapsed time: 0:08:55.819219
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 22:17:10.286959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.86
 ---- batch: 020 ----
mean loss: 188.27
 ---- batch: 030 ----
mean loss: 206.15
 ---- batch: 040 ----
mean loss: 188.68
 ---- batch: 050 ----
mean loss: 200.66
 ---- batch: 060 ----
mean loss: 209.02
 ---- batch: 070 ----
mean loss: 197.48
 ---- batch: 080 ----
mean loss: 197.59
 ---- batch: 090 ----
mean loss: 188.41
 ---- batch: 100 ----
mean loss: 194.25
 ---- batch: 110 ----
mean loss: 197.59
train mean loss: 196.80
epoch train time: 0:00:02.508401
elapsed time: 0:08:58.328121
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 22:17:12.795858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.34
 ---- batch: 020 ----
mean loss: 191.22
 ---- batch: 030 ----
mean loss: 197.55
 ---- batch: 040 ----
mean loss: 193.54
 ---- batch: 050 ----
mean loss: 198.21
 ---- batch: 060 ----
mean loss: 199.59
 ---- batch: 070 ----
mean loss: 198.27
 ---- batch: 080 ----
mean loss: 189.67
 ---- batch: 090 ----
mean loss: 196.32
 ---- batch: 100 ----
mean loss: 198.60
 ---- batch: 110 ----
mean loss: 204.61
train mean loss: 196.78
epoch train time: 0:00:02.531149
elapsed time: 0:09:00.859707
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 22:17:15.327432
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.57
 ---- batch: 020 ----
mean loss: 195.27
 ---- batch: 030 ----
mean loss: 205.36
 ---- batch: 040 ----
mean loss: 196.59
 ---- batch: 050 ----
mean loss: 190.65
 ---- batch: 060 ----
mean loss: 195.45
 ---- batch: 070 ----
mean loss: 193.27
 ---- batch: 080 ----
mean loss: 198.76
 ---- batch: 090 ----
mean loss: 196.80
 ---- batch: 100 ----
mean loss: 190.01
 ---- batch: 110 ----
mean loss: 195.62
train mean loss: 196.14
epoch train time: 0:00:02.498065
elapsed time: 0:09:03.358421
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 22:17:17.825960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.64
 ---- batch: 020 ----
mean loss: 191.22
 ---- batch: 030 ----
mean loss: 188.50
 ---- batch: 040 ----
mean loss: 199.72
 ---- batch: 050 ----
mean loss: 197.35
 ---- batch: 060 ----
mean loss: 194.82
 ---- batch: 070 ----
mean loss: 190.95
 ---- batch: 080 ----
mean loss: 205.70
 ---- batch: 090 ----
mean loss: 201.40
 ---- batch: 100 ----
mean loss: 196.67
 ---- batch: 110 ----
mean loss: 188.62
train mean loss: 196.15
epoch train time: 0:00:02.503949
elapsed time: 0:09:05.862641
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 22:17:20.330372
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.66
 ---- batch: 020 ----
mean loss: 197.84
 ---- batch: 030 ----
mean loss: 198.57
 ---- batch: 040 ----
mean loss: 198.78
 ---- batch: 050 ----
mean loss: 200.25
 ---- batch: 060 ----
mean loss: 194.59
 ---- batch: 070 ----
mean loss: 192.19
 ---- batch: 080 ----
mean loss: 205.48
 ---- batch: 090 ----
mean loss: 195.64
 ---- batch: 100 ----
mean loss: 193.34
 ---- batch: 110 ----
mean loss: 188.01
train mean loss: 196.09
epoch train time: 0:00:02.517313
elapsed time: 0:09:08.380410
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 22:17:22.848172
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.02
 ---- batch: 020 ----
mean loss: 196.28
 ---- batch: 030 ----
mean loss: 190.87
 ---- batch: 040 ----
mean loss: 194.94
 ---- batch: 050 ----
mean loss: 199.81
 ---- batch: 060 ----
mean loss: 195.65
 ---- batch: 070 ----
mean loss: 200.15
 ---- batch: 080 ----
mean loss: 202.33
 ---- batch: 090 ----
mean loss: 199.21
 ---- batch: 100 ----
mean loss: 190.11
 ---- batch: 110 ----
mean loss: 191.09
train mean loss: 196.06
epoch train time: 0:00:02.533869
elapsed time: 0:09:10.914736
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 22:17:25.382462
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.26
 ---- batch: 020 ----
mean loss: 193.94
 ---- batch: 030 ----
mean loss: 198.06
 ---- batch: 040 ----
mean loss: 191.99
 ---- batch: 050 ----
mean loss: 196.09
 ---- batch: 060 ----
mean loss: 200.39
 ---- batch: 070 ----
mean loss: 196.01
 ---- batch: 080 ----
mean loss: 202.74
 ---- batch: 090 ----
mean loss: 193.41
 ---- batch: 100 ----
mean loss: 188.70
 ---- batch: 110 ----
mean loss: 203.69
train mean loss: 196.04
epoch train time: 0:00:02.521074
elapsed time: 0:09:13.436253
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 22:17:27.903978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.32
 ---- batch: 020 ----
mean loss: 194.24
 ---- batch: 030 ----
mean loss: 196.73
 ---- batch: 040 ----
mean loss: 198.20
 ---- batch: 050 ----
mean loss: 189.23
 ---- batch: 060 ----
mean loss: 196.96
 ---- batch: 070 ----
mean loss: 198.78
 ---- batch: 080 ----
mean loss: 200.66
 ---- batch: 090 ----
mean loss: 193.64
 ---- batch: 100 ----
mean loss: 187.08
 ---- batch: 110 ----
mean loss: 201.80
train mean loss: 196.08
epoch train time: 0:00:02.521112
elapsed time: 0:09:15.957798
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 22:17:30.425548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.14
 ---- batch: 020 ----
mean loss: 196.74
 ---- batch: 030 ----
mean loss: 197.55
 ---- batch: 040 ----
mean loss: 207.34
 ---- batch: 050 ----
mean loss: 185.60
 ---- batch: 060 ----
mean loss: 197.41
 ---- batch: 070 ----
mean loss: 186.81
 ---- batch: 080 ----
mean loss: 198.95
 ---- batch: 090 ----
mean loss: 190.47
 ---- batch: 100 ----
mean loss: 207.62
 ---- batch: 110 ----
mean loss: 193.89
train mean loss: 196.14
epoch train time: 0:00:02.536452
elapsed time: 0:09:18.494701
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 22:17:32.962480
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.91
 ---- batch: 020 ----
mean loss: 196.92
 ---- batch: 030 ----
mean loss: 188.10
 ---- batch: 040 ----
mean loss: 190.72
 ---- batch: 050 ----
mean loss: 197.90
 ---- batch: 060 ----
mean loss: 206.33
 ---- batch: 070 ----
mean loss: 202.92
 ---- batch: 080 ----
mean loss: 202.25
 ---- batch: 090 ----
mean loss: 193.12
 ---- batch: 100 ----
mean loss: 190.31
 ---- batch: 110 ----
mean loss: 200.57
train mean loss: 196.14
epoch train time: 0:00:02.540094
elapsed time: 0:09:21.035271
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 22:17:35.502996
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.51
 ---- batch: 020 ----
mean loss: 179.60
 ---- batch: 030 ----
mean loss: 199.64
 ---- batch: 040 ----
mean loss: 189.98
 ---- batch: 050 ----
mean loss: 194.09
 ---- batch: 060 ----
mean loss: 204.54
 ---- batch: 070 ----
mean loss: 196.28
 ---- batch: 080 ----
mean loss: 195.11
 ---- batch: 090 ----
mean loss: 195.73
 ---- batch: 100 ----
mean loss: 197.05
 ---- batch: 110 ----
mean loss: 208.58
train mean loss: 196.01
epoch train time: 0:00:02.499533
elapsed time: 0:09:23.535229
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 22:17:38.002959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.32
 ---- batch: 020 ----
mean loss: 191.94
 ---- batch: 030 ----
mean loss: 194.86
 ---- batch: 040 ----
mean loss: 202.99
 ---- batch: 050 ----
mean loss: 200.47
 ---- batch: 060 ----
mean loss: 199.31
 ---- batch: 070 ----
mean loss: 186.59
 ---- batch: 080 ----
mean loss: 190.95
 ---- batch: 090 ----
mean loss: 189.08
 ---- batch: 100 ----
mean loss: 192.04
 ---- batch: 110 ----
mean loss: 198.09
train mean loss: 196.00
epoch train time: 0:00:02.545741
elapsed time: 0:09:26.081438
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 22:17:40.549231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.96
 ---- batch: 020 ----
mean loss: 201.66
 ---- batch: 030 ----
mean loss: 196.80
 ---- batch: 040 ----
mean loss: 198.17
 ---- batch: 050 ----
mean loss: 203.90
 ---- batch: 060 ----
mean loss: 197.47
 ---- batch: 070 ----
mean loss: 187.63
 ---- batch: 080 ----
mean loss: 190.39
 ---- batch: 090 ----
mean loss: 203.91
 ---- batch: 100 ----
mean loss: 187.19
 ---- batch: 110 ----
mean loss: 191.94
train mean loss: 195.96
epoch train time: 0:00:02.581995
elapsed time: 0:09:28.663927
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 22:17:43.131647
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.62
 ---- batch: 020 ----
mean loss: 192.92
 ---- batch: 030 ----
mean loss: 198.39
 ---- batch: 040 ----
mean loss: 200.75
 ---- batch: 050 ----
mean loss: 200.03
 ---- batch: 060 ----
mean loss: 197.44
 ---- batch: 070 ----
mean loss: 200.65
 ---- batch: 080 ----
mean loss: 191.49
 ---- batch: 090 ----
mean loss: 195.33
 ---- batch: 100 ----
mean loss: 194.31
 ---- batch: 110 ----
mean loss: 195.78
train mean loss: 195.91
epoch train time: 0:00:02.562480
elapsed time: 0:09:31.226839
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 22:17:45.694565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 202.98
 ---- batch: 020 ----
mean loss: 192.70
 ---- batch: 030 ----
mean loss: 185.42
 ---- batch: 040 ----
mean loss: 199.08
 ---- batch: 050 ----
mean loss: 197.89
 ---- batch: 060 ----
mean loss: 201.92
 ---- batch: 070 ----
mean loss: 187.38
 ---- batch: 080 ----
mean loss: 195.39
 ---- batch: 090 ----
mean loss: 187.93
 ---- batch: 100 ----
mean loss: 200.63
 ---- batch: 110 ----
mean loss: 205.00
train mean loss: 195.86
epoch train time: 0:00:02.526069
elapsed time: 0:09:33.753333
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 22:17:48.221059
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.01
 ---- batch: 020 ----
mean loss: 197.51
 ---- batch: 030 ----
mean loss: 196.71
 ---- batch: 040 ----
mean loss: 185.69
 ---- batch: 050 ----
mean loss: 200.64
 ---- batch: 060 ----
mean loss: 191.92
 ---- batch: 070 ----
mean loss: 205.34
 ---- batch: 080 ----
mean loss: 196.52
 ---- batch: 090 ----
mean loss: 190.61
 ---- batch: 100 ----
mean loss: 197.44
 ---- batch: 110 ----
mean loss: 201.84
train mean loss: 195.99
epoch train time: 0:00:02.542688
elapsed time: 0:09:36.296446
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 22:17:50.764171
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 203.37
 ---- batch: 020 ----
mean loss: 203.67
 ---- batch: 030 ----
mean loss: 197.89
 ---- batch: 040 ----
mean loss: 203.75
 ---- batch: 050 ----
mean loss: 190.59
 ---- batch: 060 ----
mean loss: 191.41
 ---- batch: 070 ----
mean loss: 202.75
 ---- batch: 080 ----
mean loss: 187.24
 ---- batch: 090 ----
mean loss: 182.64
 ---- batch: 100 ----
mean loss: 193.51
 ---- batch: 110 ----
mean loss: 195.54
train mean loss: 195.97
epoch train time: 0:00:02.534557
elapsed time: 0:09:38.831489
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 22:17:53.299232
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 201.11
 ---- batch: 020 ----
mean loss: 194.98
 ---- batch: 030 ----
mean loss: 196.46
 ---- batch: 040 ----
mean loss: 199.14
 ---- batch: 050 ----
mean loss: 195.27
 ---- batch: 060 ----
mean loss: 195.11
 ---- batch: 070 ----
mean loss: 190.23
 ---- batch: 080 ----
mean loss: 200.18
 ---- batch: 090 ----
mean loss: 197.28
 ---- batch: 100 ----
mean loss: 196.46
 ---- batch: 110 ----
mean loss: 194.10
train mean loss: 195.94
epoch train time: 0:00:02.532817
elapsed time: 0:09:41.364745
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 22:17:55.832487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.26
 ---- batch: 020 ----
mean loss: 192.81
 ---- batch: 030 ----
mean loss: 195.53
 ---- batch: 040 ----
mean loss: 195.87
 ---- batch: 050 ----
mean loss: 191.58
 ---- batch: 060 ----
mean loss: 197.34
 ---- batch: 070 ----
mean loss: 195.45
 ---- batch: 080 ----
mean loss: 199.59
 ---- batch: 090 ----
mean loss: 196.04
 ---- batch: 100 ----
mean loss: 204.47
 ---- batch: 110 ----
mean loss: 196.70
train mean loss: 195.96
epoch train time: 0:00:02.540143
elapsed time: 0:09:43.905344
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 22:17:58.373115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.01
 ---- batch: 020 ----
mean loss: 202.38
 ---- batch: 030 ----
mean loss: 201.29
 ---- batch: 040 ----
mean loss: 199.94
 ---- batch: 050 ----
mean loss: 194.90
 ---- batch: 060 ----
mean loss: 199.45
 ---- batch: 070 ----
mean loss: 196.79
 ---- batch: 080 ----
mean loss: 184.02
 ---- batch: 090 ----
mean loss: 193.32
 ---- batch: 100 ----
mean loss: 193.69
 ---- batch: 110 ----
mean loss: 191.51
train mean loss: 195.82
epoch train time: 0:00:02.521522
elapsed time: 0:09:46.427386
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 22:18:00.895098
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.13
 ---- batch: 020 ----
mean loss: 201.22
 ---- batch: 030 ----
mean loss: 191.39
 ---- batch: 040 ----
mean loss: 194.11
 ---- batch: 050 ----
mean loss: 198.88
 ---- batch: 060 ----
mean loss: 186.92
 ---- batch: 070 ----
mean loss: 195.19
 ---- batch: 080 ----
mean loss: 201.57
 ---- batch: 090 ----
mean loss: 197.08
 ---- batch: 100 ----
mean loss: 196.62
 ---- batch: 110 ----
mean loss: 196.42
train mean loss: 195.86
epoch train time: 0:00:02.518372
elapsed time: 0:09:48.946229
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 22:18:03.413974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.43
 ---- batch: 020 ----
mean loss: 199.95
 ---- batch: 030 ----
mean loss: 198.13
 ---- batch: 040 ----
mean loss: 194.89
 ---- batch: 050 ----
mean loss: 201.57
 ---- batch: 060 ----
mean loss: 190.50
 ---- batch: 070 ----
mean loss: 197.65
 ---- batch: 080 ----
mean loss: 197.58
 ---- batch: 090 ----
mean loss: 196.71
 ---- batch: 100 ----
mean loss: 200.31
 ---- batch: 110 ----
mean loss: 192.60
train mean loss: 195.81
epoch train time: 0:00:02.547770
elapsed time: 0:09:51.494446
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 22:18:05.962168
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.46
 ---- batch: 020 ----
mean loss: 189.96
 ---- batch: 030 ----
mean loss: 200.79
 ---- batch: 040 ----
mean loss: 204.14
 ---- batch: 050 ----
mean loss: 188.06
 ---- batch: 060 ----
mean loss: 191.96
 ---- batch: 070 ----
mean loss: 201.08
 ---- batch: 080 ----
mean loss: 200.48
 ---- batch: 090 ----
mean loss: 193.28
 ---- batch: 100 ----
mean loss: 195.54
 ---- batch: 110 ----
mean loss: 198.90
train mean loss: 195.81
epoch train time: 0:00:02.518263
elapsed time: 0:09:54.013126
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 22:18:08.480847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.87
 ---- batch: 020 ----
mean loss: 203.24
 ---- batch: 030 ----
mean loss: 209.13
 ---- batch: 040 ----
mean loss: 187.92
 ---- batch: 050 ----
mean loss: 192.75
 ---- batch: 060 ----
mean loss: 193.33
 ---- batch: 070 ----
mean loss: 195.21
 ---- batch: 080 ----
mean loss: 192.38
 ---- batch: 090 ----
mean loss: 190.61
 ---- batch: 100 ----
mean loss: 198.79
 ---- batch: 110 ----
mean loss: 191.17
train mean loss: 195.86
epoch train time: 0:00:02.561100
elapsed time: 0:09:56.574652
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 22:18:11.042386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.29
 ---- batch: 020 ----
mean loss: 195.44
 ---- batch: 030 ----
mean loss: 193.30
 ---- batch: 040 ----
mean loss: 202.34
 ---- batch: 050 ----
mean loss: 203.73
 ---- batch: 060 ----
mean loss: 192.75
 ---- batch: 070 ----
mean loss: 191.80
 ---- batch: 080 ----
mean loss: 191.35
 ---- batch: 090 ----
mean loss: 199.05
 ---- batch: 100 ----
mean loss: 198.47
 ---- batch: 110 ----
mean loss: 191.61
train mean loss: 195.82
epoch train time: 0:00:02.540700
elapsed time: 0:09:59.115792
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 22:18:13.583536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.85
 ---- batch: 020 ----
mean loss: 196.62
 ---- batch: 030 ----
mean loss: 193.27
 ---- batch: 040 ----
mean loss: 196.62
 ---- batch: 050 ----
mean loss: 194.23
 ---- batch: 060 ----
mean loss: 209.58
 ---- batch: 070 ----
mean loss: 191.75
 ---- batch: 080 ----
mean loss: 201.62
 ---- batch: 090 ----
mean loss: 198.60
 ---- batch: 100 ----
mean loss: 183.32
 ---- batch: 110 ----
mean loss: 193.50
train mean loss: 195.78
epoch train time: 0:00:02.507059
elapsed time: 0:10:01.623291
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 22:18:16.091016
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.61
 ---- batch: 020 ----
mean loss: 192.29
 ---- batch: 030 ----
mean loss: 199.20
 ---- batch: 040 ----
mean loss: 203.60
 ---- batch: 050 ----
mean loss: 185.87
 ---- batch: 060 ----
mean loss: 198.68
 ---- batch: 070 ----
mean loss: 189.34
 ---- batch: 080 ----
mean loss: 190.14
 ---- batch: 090 ----
mean loss: 203.69
 ---- batch: 100 ----
mean loss: 199.55
 ---- batch: 110 ----
mean loss: 190.37
train mean loss: 195.86
epoch train time: 0:00:02.538959
elapsed time: 0:10:04.162674
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 22:18:18.630394
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.92
 ---- batch: 020 ----
mean loss: 200.60
 ---- batch: 030 ----
mean loss: 197.61
 ---- batch: 040 ----
mean loss: 192.30
 ---- batch: 050 ----
mean loss: 197.07
 ---- batch: 060 ----
mean loss: 184.29
 ---- batch: 070 ----
mean loss: 200.70
 ---- batch: 080 ----
mean loss: 191.71
 ---- batch: 090 ----
mean loss: 202.19
 ---- batch: 100 ----
mean loss: 200.75
 ---- batch: 110 ----
mean loss: 197.57
train mean loss: 195.71
epoch train time: 0:00:02.522844
elapsed time: 0:10:06.686008
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 22:18:21.153807
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.08
 ---- batch: 020 ----
mean loss: 194.76
 ---- batch: 030 ----
mean loss: 198.11
 ---- batch: 040 ----
mean loss: 191.92
 ---- batch: 050 ----
mean loss: 193.24
 ---- batch: 060 ----
mean loss: 199.54
 ---- batch: 070 ----
mean loss: 204.23
 ---- batch: 080 ----
mean loss: 202.18
 ---- batch: 090 ----
mean loss: 196.32
 ---- batch: 100 ----
mean loss: 195.57
 ---- batch: 110 ----
mean loss: 191.87
train mean loss: 195.71
epoch train time: 0:00:02.547171
elapsed time: 0:10:09.233687
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 22:18:23.701405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.14
 ---- batch: 020 ----
mean loss: 191.74
 ---- batch: 030 ----
mean loss: 190.09
 ---- batch: 040 ----
mean loss: 198.25
 ---- batch: 050 ----
mean loss: 199.61
 ---- batch: 060 ----
mean loss: 196.98
 ---- batch: 070 ----
mean loss: 202.79
 ---- batch: 080 ----
mean loss: 196.49
 ---- batch: 090 ----
mean loss: 190.94
 ---- batch: 100 ----
mean loss: 193.02
 ---- batch: 110 ----
mean loss: 194.02
train mean loss: 195.73
epoch train time: 0:00:02.518836
elapsed time: 0:10:11.752926
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 22:18:26.220630
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.59
 ---- batch: 020 ----
mean loss: 199.07
 ---- batch: 030 ----
mean loss: 206.50
 ---- batch: 040 ----
mean loss: 198.23
 ---- batch: 050 ----
mean loss: 193.49
 ---- batch: 060 ----
mean loss: 193.56
 ---- batch: 070 ----
mean loss: 193.07
 ---- batch: 080 ----
mean loss: 184.84
 ---- batch: 090 ----
mean loss: 200.68
 ---- batch: 100 ----
mean loss: 198.26
 ---- batch: 110 ----
mean loss: 197.17
train mean loss: 195.69
epoch train time: 0:00:02.489299
elapsed time: 0:10:14.242623
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 22:18:28.710353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 199.67
 ---- batch: 020 ----
mean loss: 190.59
 ---- batch: 030 ----
mean loss: 188.42
 ---- batch: 040 ----
mean loss: 207.77
 ---- batch: 050 ----
mean loss: 189.03
 ---- batch: 060 ----
mean loss: 194.92
 ---- batch: 070 ----
mean loss: 196.52
 ---- batch: 080 ----
mean loss: 198.31
 ---- batch: 090 ----
mean loss: 194.27
 ---- batch: 100 ----
mean loss: 190.12
 ---- batch: 110 ----
mean loss: 199.98
train mean loss: 195.76
epoch train time: 0:00:02.507617
elapsed time: 0:10:16.750668
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 22:18:31.218389
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.34
 ---- batch: 020 ----
mean loss: 184.09
 ---- batch: 030 ----
mean loss: 188.71
 ---- batch: 040 ----
mean loss: 200.71
 ---- batch: 050 ----
mean loss: 198.40
 ---- batch: 060 ----
mean loss: 189.67
 ---- batch: 070 ----
mean loss: 199.64
 ---- batch: 080 ----
mean loss: 205.78
 ---- batch: 090 ----
mean loss: 192.95
 ---- batch: 100 ----
mean loss: 203.87
 ---- batch: 110 ----
mean loss: 196.69
train mean loss: 195.66
epoch train time: 0:00:02.539056
elapsed time: 0:10:19.290148
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 22:18:33.757857
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.33
 ---- batch: 020 ----
mean loss: 191.75
 ---- batch: 030 ----
mean loss: 200.78
 ---- batch: 040 ----
mean loss: 199.44
 ---- batch: 050 ----
mean loss: 187.11
 ---- batch: 060 ----
mean loss: 201.21
 ---- batch: 070 ----
mean loss: 197.76
 ---- batch: 080 ----
mean loss: 201.04
 ---- batch: 090 ----
mean loss: 194.35
 ---- batch: 100 ----
mean loss: 192.79
 ---- batch: 110 ----
mean loss: 194.97
train mean loss: 195.64
epoch train time: 0:00:02.503038
elapsed time: 0:10:21.793594
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 22:18:36.261306
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.14
 ---- batch: 020 ----
mean loss: 186.71
 ---- batch: 030 ----
mean loss: 193.67
 ---- batch: 040 ----
mean loss: 198.07
 ---- batch: 050 ----
mean loss: 194.99
 ---- batch: 060 ----
mean loss: 207.10
 ---- batch: 070 ----
mean loss: 196.71
 ---- batch: 080 ----
mean loss: 196.20
 ---- batch: 090 ----
mean loss: 193.15
 ---- batch: 100 ----
mean loss: 200.64
 ---- batch: 110 ----
mean loss: 192.69
train mean loss: 195.63
epoch train time: 0:00:02.523245
elapsed time: 0:10:24.317454
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 22:18:38.784983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.52
 ---- batch: 020 ----
mean loss: 194.00
 ---- batch: 030 ----
mean loss: 184.28
 ---- batch: 040 ----
mean loss: 197.55
 ---- batch: 050 ----
mean loss: 190.70
 ---- batch: 060 ----
mean loss: 199.08
 ---- batch: 070 ----
mean loss: 192.75
 ---- batch: 080 ----
mean loss: 204.04
 ---- batch: 090 ----
mean loss: 201.34
 ---- batch: 100 ----
mean loss: 196.16
 ---- batch: 110 ----
mean loss: 193.79
train mean loss: 195.67
epoch train time: 0:00:02.539392
elapsed time: 0:10:26.857068
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 22:18:41.324794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.10
 ---- batch: 020 ----
mean loss: 188.88
 ---- batch: 030 ----
mean loss: 193.78
 ---- batch: 040 ----
mean loss: 201.44
 ---- batch: 050 ----
mean loss: 188.70
 ---- batch: 060 ----
mean loss: 202.05
 ---- batch: 070 ----
mean loss: 199.45
 ---- batch: 080 ----
mean loss: 188.02
 ---- batch: 090 ----
mean loss: 195.51
 ---- batch: 100 ----
mean loss: 199.46
 ---- batch: 110 ----
mean loss: 194.19
train mean loss: 195.65
epoch train time: 0:00:02.547045
elapsed time: 0:10:29.404536
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 22:18:43.872265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.50
 ---- batch: 020 ----
mean loss: 196.86
 ---- batch: 030 ----
mean loss: 190.93
 ---- batch: 040 ----
mean loss: 201.55
 ---- batch: 050 ----
mean loss: 195.15
 ---- batch: 060 ----
mean loss: 209.10
 ---- batch: 070 ----
mean loss: 197.98
 ---- batch: 080 ----
mean loss: 188.97
 ---- batch: 090 ----
mean loss: 196.29
 ---- batch: 100 ----
mean loss: 189.49
 ---- batch: 110 ----
mean loss: 192.82
train mean loss: 195.66
epoch train time: 0:00:02.518173
elapsed time: 0:10:31.923130
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 22:18:46.390691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.52
 ---- batch: 020 ----
mean loss: 206.56
 ---- batch: 030 ----
mean loss: 195.56
 ---- batch: 040 ----
mean loss: 191.22
 ---- batch: 050 ----
mean loss: 195.20
 ---- batch: 060 ----
mean loss: 198.91
 ---- batch: 070 ----
mean loss: 185.28
 ---- batch: 080 ----
mean loss: 189.96
 ---- batch: 090 ----
mean loss: 200.07
 ---- batch: 100 ----
mean loss: 199.16
 ---- batch: 110 ----
mean loss: 190.27
train mean loss: 195.55
epoch train time: 0:00:02.541830
elapsed time: 0:10:34.465235
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 22:18:48.932985
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 194.99
 ---- batch: 020 ----
mean loss: 200.69
 ---- batch: 030 ----
mean loss: 194.09
 ---- batch: 040 ----
mean loss: 193.75
 ---- batch: 050 ----
mean loss: 192.45
 ---- batch: 060 ----
mean loss: 197.41
 ---- batch: 070 ----
mean loss: 205.66
 ---- batch: 080 ----
mean loss: 191.25
 ---- batch: 090 ----
mean loss: 189.43
 ---- batch: 100 ----
mean loss: 200.94
 ---- batch: 110 ----
mean loss: 192.76
train mean loss: 195.55
epoch train time: 0:00:02.522605
elapsed time: 0:10:36.988314
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 22:18:51.456016
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.97
 ---- batch: 020 ----
mean loss: 197.05
 ---- batch: 030 ----
mean loss: 197.86
 ---- batch: 040 ----
mean loss: 202.08
 ---- batch: 050 ----
mean loss: 189.34
 ---- batch: 060 ----
mean loss: 203.77
 ---- batch: 070 ----
mean loss: 198.26
 ---- batch: 080 ----
mean loss: 206.32
 ---- batch: 090 ----
mean loss: 189.00
 ---- batch: 100 ----
mean loss: 193.97
 ---- batch: 110 ----
mean loss: 187.30
train mean loss: 195.57
epoch train time: 0:00:02.520653
elapsed time: 0:10:39.509407
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 22:18:53.977163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.03
 ---- batch: 020 ----
mean loss: 190.49
 ---- batch: 030 ----
mean loss: 199.51
 ---- batch: 040 ----
mean loss: 186.30
 ---- batch: 050 ----
mean loss: 195.68
 ---- batch: 060 ----
mean loss: 197.46
 ---- batch: 070 ----
mean loss: 198.46
 ---- batch: 080 ----
mean loss: 195.36
 ---- batch: 090 ----
mean loss: 206.69
 ---- batch: 100 ----
mean loss: 193.01
 ---- batch: 110 ----
mean loss: 199.44
train mean loss: 195.61
epoch train time: 0:00:02.513602
elapsed time: 0:10:42.023452
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 22:18:56.491177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.51
 ---- batch: 020 ----
mean loss: 196.89
 ---- batch: 030 ----
mean loss: 191.85
 ---- batch: 040 ----
mean loss: 192.19
 ---- batch: 050 ----
mean loss: 199.35
 ---- batch: 060 ----
mean loss: 203.13
 ---- batch: 070 ----
mean loss: 197.03
 ---- batch: 080 ----
mean loss: 197.28
 ---- batch: 090 ----
mean loss: 198.83
 ---- batch: 100 ----
mean loss: 200.17
 ---- batch: 110 ----
mean loss: 189.36
train mean loss: 195.50
epoch train time: 0:00:02.500805
elapsed time: 0:10:44.524696
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 22:18:58.992397
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.66
 ---- batch: 020 ----
mean loss: 211.85
 ---- batch: 030 ----
mean loss: 203.28
 ---- batch: 040 ----
mean loss: 185.64
 ---- batch: 050 ----
mean loss: 192.00
 ---- batch: 060 ----
mean loss: 195.07
 ---- batch: 070 ----
mean loss: 198.74
 ---- batch: 080 ----
mean loss: 180.97
 ---- batch: 090 ----
mean loss: 195.60
 ---- batch: 100 ----
mean loss: 200.48
 ---- batch: 110 ----
mean loss: 194.33
train mean loss: 195.51
epoch train time: 0:00:02.530474
elapsed time: 0:10:47.055582
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 22:19:01.523337
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.88
 ---- batch: 020 ----
mean loss: 200.22
 ---- batch: 030 ----
mean loss: 191.53
 ---- batch: 040 ----
mean loss: 204.42
 ---- batch: 050 ----
mean loss: 200.49
 ---- batch: 060 ----
mean loss: 192.77
 ---- batch: 070 ----
mean loss: 200.56
 ---- batch: 080 ----
mean loss: 189.38
 ---- batch: 090 ----
mean loss: 187.92
 ---- batch: 100 ----
mean loss: 198.97
 ---- batch: 110 ----
mean loss: 192.87
train mean loss: 195.50
epoch train time: 0:00:02.510130
elapsed time: 0:10:49.566172
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 22:19:04.033911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.25
 ---- batch: 020 ----
mean loss: 194.09
 ---- batch: 030 ----
mean loss: 197.14
 ---- batch: 040 ----
mean loss: 195.82
 ---- batch: 050 ----
mean loss: 195.26
 ---- batch: 060 ----
mean loss: 190.19
 ---- batch: 070 ----
mean loss: 199.07
 ---- batch: 080 ----
mean loss: 203.91
 ---- batch: 090 ----
mean loss: 193.12
 ---- batch: 100 ----
mean loss: 198.64
 ---- batch: 110 ----
mean loss: 196.92
train mean loss: 195.48
epoch train time: 0:00:02.560803
elapsed time: 0:10:52.127424
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 22:19:06.595182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.22
 ---- batch: 020 ----
mean loss: 191.62
 ---- batch: 030 ----
mean loss: 196.88
 ---- batch: 040 ----
mean loss: 195.48
 ---- batch: 050 ----
mean loss: 194.03
 ---- batch: 060 ----
mean loss: 198.60
 ---- batch: 070 ----
mean loss: 199.91
 ---- batch: 080 ----
mean loss: 191.16
 ---- batch: 090 ----
mean loss: 195.94
 ---- batch: 100 ----
mean loss: 198.56
 ---- batch: 110 ----
mean loss: 201.46
train mean loss: 195.52
epoch train time: 0:00:02.586418
elapsed time: 0:10:54.714322
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 22:19:09.182055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.92
 ---- batch: 020 ----
mean loss: 194.86
 ---- batch: 030 ----
mean loss: 195.89
 ---- batch: 040 ----
mean loss: 194.17
 ---- batch: 050 ----
mean loss: 197.35
 ---- batch: 060 ----
mean loss: 192.25
 ---- batch: 070 ----
mean loss: 196.94
 ---- batch: 080 ----
mean loss: 193.87
 ---- batch: 090 ----
mean loss: 192.84
 ---- batch: 100 ----
mean loss: 196.07
 ---- batch: 110 ----
mean loss: 196.18
train mean loss: 195.53
epoch train time: 0:00:02.544853
elapsed time: 0:10:57.259633
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 22:19:11.727349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.40
 ---- batch: 020 ----
mean loss: 199.97
 ---- batch: 030 ----
mean loss: 198.93
 ---- batch: 040 ----
mean loss: 197.56
 ---- batch: 050 ----
mean loss: 191.37
 ---- batch: 060 ----
mean loss: 197.35
 ---- batch: 070 ----
mean loss: 199.28
 ---- batch: 080 ----
mean loss: 191.19
 ---- batch: 090 ----
mean loss: 194.72
 ---- batch: 100 ----
mean loss: 198.95
 ---- batch: 110 ----
mean loss: 203.46
train mean loss: 195.45
epoch train time: 0:00:02.578555
elapsed time: 0:10:59.838673
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 22:19:14.306378
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 197.70
 ---- batch: 020 ----
mean loss: 195.47
 ---- batch: 030 ----
mean loss: 193.84
 ---- batch: 040 ----
mean loss: 194.37
 ---- batch: 050 ----
mean loss: 190.89
 ---- batch: 060 ----
mean loss: 190.21
 ---- batch: 070 ----
mean loss: 192.50
 ---- batch: 080 ----
mean loss: 196.76
 ---- batch: 090 ----
mean loss: 199.32
 ---- batch: 100 ----
mean loss: 201.95
 ---- batch: 110 ----
mean loss: 197.83
train mean loss: 195.34
epoch train time: 0:00:02.553628
elapsed time: 0:11:02.392694
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 22:19:16.860430
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.19
 ---- batch: 020 ----
mean loss: 197.67
 ---- batch: 030 ----
mean loss: 192.27
 ---- batch: 040 ----
mean loss: 194.53
 ---- batch: 050 ----
mean loss: 189.57
 ---- batch: 060 ----
mean loss: 201.18
 ---- batch: 070 ----
mean loss: 196.64
 ---- batch: 080 ----
mean loss: 196.55
 ---- batch: 090 ----
mean loss: 196.97
 ---- batch: 100 ----
mean loss: 199.84
 ---- batch: 110 ----
mean loss: 195.93
train mean loss: 195.42
epoch train time: 0:00:02.530533
elapsed time: 0:11:04.927842
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_9/checkpoint.pth.tar
**** end time: 2019-09-26 22:19:19.395340 ****
