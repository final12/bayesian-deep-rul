Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_6', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31779
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:47:08.431771 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:47:08.436852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4198.60
 ---- batch: 020 ----
mean loss: 4032.62
 ---- batch: 030 ----
mean loss: 4083.23
train mean loss: 4099.80
epoch train time: 0:00:12.592565
elapsed time: 0:00:12.599002
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:47:21.030814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4025.99
 ---- batch: 020 ----
mean loss: 3945.50
 ---- batch: 030 ----
mean loss: 3923.63
train mean loss: 3959.14
epoch train time: 0:00:00.175779
elapsed time: 0:00:12.774912
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:47:21.206745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3836.88
 ---- batch: 020 ----
mean loss: 3759.95
 ---- batch: 030 ----
mean loss: 3706.18
train mean loss: 3745.30
epoch train time: 0:00:00.171102
elapsed time: 0:00:12.946182
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:47:21.378003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3556.51
 ---- batch: 020 ----
mean loss: 3457.51
 ---- batch: 030 ----
mean loss: 3420.50
train mean loss: 3468.00
epoch train time: 0:00:00.171558
elapsed time: 0:00:13.117904
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:47:21.549729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3239.86
 ---- batch: 020 ----
mean loss: 3159.95
 ---- batch: 030 ----
mean loss: 3208.25
train mean loss: 3185.41
epoch train time: 0:00:00.175162
elapsed time: 0:00:13.293256
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:47:21.725110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3018.40
 ---- batch: 020 ----
mean loss: 2933.47
 ---- batch: 030 ----
mean loss: 2903.51
train mean loss: 2937.33
epoch train time: 0:00:00.173315
elapsed time: 0:00:13.466744
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:47:21.898566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2812.77
 ---- batch: 020 ----
mean loss: 2723.90
 ---- batch: 030 ----
mean loss: 2659.77
train mean loss: 2718.19
epoch train time: 0:00:00.171550
elapsed time: 0:00:13.638452
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:47:22.070273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2587.56
 ---- batch: 020 ----
mean loss: 2556.34
 ---- batch: 030 ----
mean loss: 2458.27
train mean loss: 2522.28
epoch train time: 0:00:00.168502
elapsed time: 0:00:13.807103
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:47:22.238942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2403.77
 ---- batch: 020 ----
mean loss: 2358.24
 ---- batch: 030 ----
mean loss: 2318.97
train mean loss: 2348.98
epoch train time: 0:00:00.172387
elapsed time: 0:00:13.979663
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:47:22.411488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2259.90
 ---- batch: 020 ----
mean loss: 2193.79
 ---- batch: 030 ----
mean loss: 2162.86
train mean loss: 2191.39
epoch train time: 0:00:00.169601
elapsed time: 0:00:14.149406
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:47:22.581227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2085.25
 ---- batch: 020 ----
mean loss: 2072.33
 ---- batch: 030 ----
mean loss: 2012.13
train mean loss: 2045.69
epoch train time: 0:00:00.170503
elapsed time: 0:00:14.320067
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:47:22.751887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1953.55
 ---- batch: 020 ----
mean loss: 1935.69
 ---- batch: 030 ----
mean loss: 1865.43
train mean loss: 1915.77
epoch train time: 0:00:00.170016
elapsed time: 0:00:14.490233
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:47:22.922055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1843.99
 ---- batch: 020 ----
mean loss: 1789.10
 ---- batch: 030 ----
mean loss: 1787.54
train mean loss: 1792.91
epoch train time: 0:00:00.171030
elapsed time: 0:00:14.661404
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:47:23.093224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1749.91
 ---- batch: 020 ----
mean loss: 1696.15
 ---- batch: 030 ----
mean loss: 1637.44
train mean loss: 1680.41
epoch train time: 0:00:00.167249
elapsed time: 0:00:14.828792
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:47:23.260610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1608.53
 ---- batch: 020 ----
mean loss: 1587.58
 ---- batch: 030 ----
mean loss: 1558.03
train mean loss: 1577.96
epoch train time: 0:00:00.168558
elapsed time: 0:00:14.997500
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:47:23.429319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1507.72
 ---- batch: 020 ----
mean loss: 1482.11
 ---- batch: 030 ----
mean loss: 1470.32
train mean loss: 1481.42
epoch train time: 0:00:00.170935
elapsed time: 0:00:15.168594
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:47:23.600415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1427.32
 ---- batch: 020 ----
mean loss: 1391.74
 ---- batch: 030 ----
mean loss: 1374.07
train mean loss: 1392.06
epoch train time: 0:00:00.175004
elapsed time: 0:00:15.343772
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:47:23.775595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1341.53
 ---- batch: 020 ----
mean loss: 1318.47
 ---- batch: 030 ----
mean loss: 1274.92
train mean loss: 1310.94
epoch train time: 0:00:00.180998
elapsed time: 0:00:15.524949
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:47:23.956796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1266.08
 ---- batch: 020 ----
mean loss: 1244.34
 ---- batch: 030 ----
mean loss: 1211.25
train mean loss: 1234.61
epoch train time: 0:00:00.183914
elapsed time: 0:00:15.709027
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:47:24.140844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1193.46
 ---- batch: 020 ----
mean loss: 1165.27
 ---- batch: 030 ----
mean loss: 1146.65
train mean loss: 1164.06
epoch train time: 0:00:00.167181
elapsed time: 0:00:15.876341
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:47:24.308160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1110.29
 ---- batch: 020 ----
mean loss: 1103.22
 ---- batch: 030 ----
mean loss: 1085.04
train mean loss: 1100.17
epoch train time: 0:00:00.171629
elapsed time: 0:00:16.048138
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:47:24.479990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1053.20
 ---- batch: 020 ----
mean loss: 1064.58
 ---- batch: 030 ----
mean loss: 1028.61
train mean loss: 1040.05
epoch train time: 0:00:00.172425
elapsed time: 0:00:16.220736
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:47:24.652557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1012.96
 ---- batch: 020 ----
mean loss: 987.71
 ---- batch: 030 ----
mean loss: 977.11
train mean loss: 984.84
epoch train time: 0:00:00.175173
elapsed time: 0:00:16.396056
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:47:24.827902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 940.38
 ---- batch: 020 ----
mean loss: 950.62
 ---- batch: 030 ----
mean loss: 933.99
train mean loss: 933.41
epoch train time: 0:00:00.174275
elapsed time: 0:00:16.570531
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:47:25.002365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.87
 ---- batch: 020 ----
mean loss: 903.08
 ---- batch: 030 ----
mean loss: 869.65
train mean loss: 886.78
epoch train time: 0:00:00.172036
elapsed time: 0:00:16.742729
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:47:25.174550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.53
 ---- batch: 020 ----
mean loss: 862.00
 ---- batch: 030 ----
mean loss: 839.88
train mean loss: 844.57
epoch train time: 0:00:00.167610
elapsed time: 0:00:16.910483
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:47:25.342303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 821.33
 ---- batch: 020 ----
mean loss: 811.71
 ---- batch: 030 ----
mean loss: 781.65
train mean loss: 805.56
epoch train time: 0:00:00.170305
elapsed time: 0:00:17.080932
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:47:25.512754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 784.69
 ---- batch: 020 ----
mean loss: 771.55
 ---- batch: 030 ----
mean loss: 753.11
train mean loss: 768.06
epoch train time: 0:00:00.176508
elapsed time: 0:00:17.257583
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:47:25.689402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 739.38
 ---- batch: 020 ----
mean loss: 751.83
 ---- batch: 030 ----
mean loss: 726.65
train mean loss: 734.12
epoch train time: 0:00:00.183092
elapsed time: 0:00:17.440815
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:47:25.872683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 712.45
 ---- batch: 020 ----
mean loss: 703.19
 ---- batch: 030 ----
mean loss: 697.43
train mean loss: 704.16
epoch train time: 0:00:00.181938
elapsed time: 0:00:17.622948
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:47:26.054770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.58
 ---- batch: 020 ----
mean loss: 669.02
 ---- batch: 030 ----
mean loss: 684.53
train mean loss: 676.27
epoch train time: 0:00:00.179247
elapsed time: 0:00:17.802338
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:47:26.234177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 660.38
 ---- batch: 020 ----
mean loss: 656.39
 ---- batch: 030 ----
mean loss: 640.60
train mean loss: 649.35
epoch train time: 0:00:00.173213
elapsed time: 0:00:17.975726
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:47:26.407587
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.65
 ---- batch: 020 ----
mean loss: 624.78
 ---- batch: 030 ----
mean loss: 612.08
train mean loss: 626.21
epoch train time: 0:00:00.179433
elapsed time: 0:00:18.155342
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:47:26.587177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 612.47
 ---- batch: 020 ----
mean loss: 613.49
 ---- batch: 030 ----
mean loss: 594.28
train mean loss: 604.73
epoch train time: 0:00:00.182392
elapsed time: 0:00:18.337921
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:47:26.769747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 586.29
 ---- batch: 020 ----
mean loss: 593.04
 ---- batch: 030 ----
mean loss: 578.37
train mean loss: 584.79
epoch train time: 0:00:00.189737
elapsed time: 0:00:18.527805
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:47:26.959628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 569.10
 ---- batch: 020 ----
mean loss: 564.15
 ---- batch: 030 ----
mean loss: 573.04
train mean loss: 567.13
epoch train time: 0:00:00.181972
elapsed time: 0:00:18.709958
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:47:27.141793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.29
 ---- batch: 020 ----
mean loss: 552.97
 ---- batch: 030 ----
mean loss: 546.71
train mean loss: 549.55
epoch train time: 0:00:00.178812
elapsed time: 0:00:18.888937
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:47:27.320773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 541.37
 ---- batch: 020 ----
mean loss: 531.11
 ---- batch: 030 ----
mean loss: 535.35
train mean loss: 534.57
epoch train time: 0:00:00.180974
elapsed time: 0:00:19.070091
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:47:27.501919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 528.72
 ---- batch: 020 ----
mean loss: 521.89
 ---- batch: 030 ----
mean loss: 515.36
train mean loss: 520.97
epoch train time: 0:00:00.177341
elapsed time: 0:00:19.247578
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:47:27.679420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.33
 ---- batch: 020 ----
mean loss: 507.38
 ---- batch: 030 ----
mean loss: 508.27
train mean loss: 508.00
epoch train time: 0:00:00.181273
elapsed time: 0:00:19.429019
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:47:27.860842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.78
 ---- batch: 020 ----
mean loss: 493.38
 ---- batch: 030 ----
mean loss: 494.67
train mean loss: 496.20
epoch train time: 0:00:00.182517
elapsed time: 0:00:19.611689
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:47:28.043527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.39
 ---- batch: 020 ----
mean loss: 478.97
 ---- batch: 030 ----
mean loss: 486.80
train mean loss: 485.19
epoch train time: 0:00:00.178820
elapsed time: 0:00:19.790676
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:47:28.222517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.44
 ---- batch: 020 ----
mean loss: 473.62
 ---- batch: 030 ----
mean loss: 470.85
train mean loss: 475.08
epoch train time: 0:00:00.178326
elapsed time: 0:00:19.969179
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:47:28.401008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.33
 ---- batch: 020 ----
mean loss: 474.42
 ---- batch: 030 ----
mean loss: 464.95
train mean loss: 466.30
epoch train time: 0:00:00.182584
elapsed time: 0:00:20.151936
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:47:28.583765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.04
 ---- batch: 020 ----
mean loss: 453.32
 ---- batch: 030 ----
mean loss: 460.51
train mean loss: 457.61
epoch train time: 0:00:00.183360
elapsed time: 0:00:20.335440
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:47:28.767309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.23
 ---- batch: 020 ----
mean loss: 448.15
 ---- batch: 030 ----
mean loss: 446.22
train mean loss: 450.12
epoch train time: 0:00:00.190209
elapsed time: 0:00:20.525861
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:47:28.957688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.48
 ---- batch: 020 ----
mean loss: 443.47
 ---- batch: 030 ----
mean loss: 442.00
train mean loss: 442.28
epoch train time: 0:00:00.181139
elapsed time: 0:00:20.707160
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:47:29.138979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.99
 ---- batch: 020 ----
mean loss: 440.86
 ---- batch: 030 ----
mean loss: 433.05
train mean loss: 435.67
epoch train time: 0:00:00.179174
elapsed time: 0:00:20.886474
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:47:29.318295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.37
 ---- batch: 020 ----
mean loss: 429.51
 ---- batch: 030 ----
mean loss: 423.57
train mean loss: 429.08
epoch train time: 0:00:00.180344
elapsed time: 0:00:21.066977
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:47:29.498801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.75
 ---- batch: 020 ----
mean loss: 423.54
 ---- batch: 030 ----
mean loss: 419.97
train mean loss: 423.79
epoch train time: 0:00:00.183086
elapsed time: 0:00:21.250222
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:47:29.682045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.15
 ---- batch: 020 ----
mean loss: 416.83
 ---- batch: 030 ----
mean loss: 420.22
train mean loss: 418.15
epoch train time: 0:00:00.185255
elapsed time: 0:00:21.435620
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:47:29.867443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.40
 ---- batch: 020 ----
mean loss: 405.48
 ---- batch: 030 ----
mean loss: 417.47
train mean loss: 413.40
epoch train time: 0:00:00.179459
elapsed time: 0:00:21.615223
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:47:30.047044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.71
 ---- batch: 020 ----
mean loss: 412.45
 ---- batch: 030 ----
mean loss: 400.97
train mean loss: 408.56
epoch train time: 0:00:00.176755
elapsed time: 0:00:21.792120
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:47:30.223951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.98
 ---- batch: 020 ----
mean loss: 402.41
 ---- batch: 030 ----
mean loss: 406.90
train mean loss: 404.65
epoch train time: 0:00:00.180746
elapsed time: 0:00:21.973015
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:47:30.404835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.10
 ---- batch: 020 ----
mean loss: 404.04
 ---- batch: 030 ----
mean loss: 394.60
train mean loss: 400.93
epoch train time: 0:00:00.176566
elapsed time: 0:00:22.149734
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:47:30.581555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.68
 ---- batch: 020 ----
mean loss: 396.53
 ---- batch: 030 ----
mean loss: 399.70
train mean loss: 397.57
epoch train time: 0:00:00.189453
elapsed time: 0:00:22.339367
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:47:30.771204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.19
 ---- batch: 020 ----
mean loss: 402.16
 ---- batch: 030 ----
mean loss: 391.49
train mean loss: 394.97
epoch train time: 0:00:00.189174
elapsed time: 0:00:22.528743
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:47:30.960561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.85
 ---- batch: 020 ----
mean loss: 391.93
 ---- batch: 030 ----
mean loss: 392.59
train mean loss: 391.97
epoch train time: 0:00:00.179616
elapsed time: 0:00:22.708499
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:47:31.140338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.66
 ---- batch: 020 ----
mean loss: 398.99
 ---- batch: 030 ----
mean loss: 389.84
train mean loss: 389.57
epoch train time: 0:00:00.177346
elapsed time: 0:00:22.886010
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:47:31.317831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.59
 ---- batch: 020 ----
mean loss: 390.38
 ---- batch: 030 ----
mean loss: 382.29
train mean loss: 387.55
epoch train time: 0:00:00.178584
elapsed time: 0:00:23.064736
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:47:31.496559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.33
 ---- batch: 020 ----
mean loss: 378.97
 ---- batch: 030 ----
mean loss: 382.55
train mean loss: 386.12
epoch train time: 0:00:00.180290
elapsed time: 0:00:23.245180
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:47:31.677003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.26
 ---- batch: 020 ----
mean loss: 382.70
 ---- batch: 030 ----
mean loss: 390.22
train mean loss: 384.10
epoch train time: 0:00:00.190789
elapsed time: 0:00:23.436109
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:47:31.867938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.96
 ---- batch: 020 ----
mean loss: 385.95
 ---- batch: 030 ----
mean loss: 378.89
train mean loss: 382.61
epoch train time: 0:00:00.177115
elapsed time: 0:00:23.613371
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:47:32.045206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.69
 ---- batch: 020 ----
mean loss: 382.90
 ---- batch: 030 ----
mean loss: 379.89
train mean loss: 380.89
epoch train time: 0:00:00.175292
elapsed time: 0:00:23.788819
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:47:32.220648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.30
 ---- batch: 020 ----
mean loss: 376.16
 ---- batch: 030 ----
mean loss: 379.27
train mean loss: 379.97
epoch train time: 0:00:00.177400
elapsed time: 0:00:23.966369
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:47:32.398190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.18
 ---- batch: 020 ----
mean loss: 374.30
 ---- batch: 030 ----
mean loss: 377.31
train mean loss: 378.77
epoch train time: 0:00:00.188592
elapsed time: 0:00:24.155111
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:47:32.586941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.10
 ---- batch: 020 ----
mean loss: 374.54
 ---- batch: 030 ----
mean loss: 374.65
train mean loss: 377.46
epoch train time: 0:00:00.192323
elapsed time: 0:00:24.347586
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:47:32.779418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.58
 ---- batch: 020 ----
mean loss: 373.18
 ---- batch: 030 ----
mean loss: 377.83
train mean loss: 376.79
epoch train time: 0:00:00.180125
elapsed time: 0:00:24.527863
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:47:32.959685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.55
 ---- batch: 020 ----
mean loss: 378.97
 ---- batch: 030 ----
mean loss: 375.33
train mean loss: 375.43
epoch train time: 0:00:00.181703
elapsed time: 0:00:24.709709
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:47:33.141544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.92
 ---- batch: 020 ----
mean loss: 375.04
 ---- batch: 030 ----
mean loss: 374.58
train mean loss: 374.98
epoch train time: 0:00:00.181546
elapsed time: 0:00:24.891467
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:47:33.323305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.08
 ---- batch: 020 ----
mean loss: 365.97
 ---- batch: 030 ----
mean loss: 379.09
train mean loss: 373.87
epoch train time: 0:00:00.178383
elapsed time: 0:00:25.070012
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:47:33.501834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.71
 ---- batch: 020 ----
mean loss: 371.92
 ---- batch: 030 ----
mean loss: 367.56
train mean loss: 373.56
epoch train time: 0:00:00.178994
elapsed time: 0:00:25.249163
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:47:33.681003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.39
 ---- batch: 020 ----
mean loss: 373.93
 ---- batch: 030 ----
mean loss: 369.27
train mean loss: 372.56
epoch train time: 0:00:00.181741
elapsed time: 0:00:25.431064
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:47:33.862884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.00
 ---- batch: 020 ----
mean loss: 379.90
 ---- batch: 030 ----
mean loss: 368.50
train mean loss: 371.98
epoch train time: 0:00:00.177948
elapsed time: 0:00:25.609152
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:47:34.041003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.00
 ---- batch: 020 ----
mean loss: 367.83
 ---- batch: 030 ----
mean loss: 374.42
train mean loss: 371.75
epoch train time: 0:00:00.179420
elapsed time: 0:00:25.788743
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:47:34.220570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.78
 ---- batch: 020 ----
mean loss: 375.05
 ---- batch: 030 ----
mean loss: 371.94
train mean loss: 371.23
epoch train time: 0:00:00.177764
elapsed time: 0:00:25.966673
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:47:34.398495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.10
 ---- batch: 020 ----
mean loss: 366.23
 ---- batch: 030 ----
mean loss: 371.13
train mean loss: 370.97
epoch train time: 0:00:00.175935
elapsed time: 0:00:26.142749
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:47:34.574569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.69
 ---- batch: 020 ----
mean loss: 373.54
 ---- batch: 030 ----
mean loss: 373.62
train mean loss: 370.17
epoch train time: 0:00:00.173964
elapsed time: 0:00:26.316883
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:47:34.748705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.49
 ---- batch: 020 ----
mean loss: 364.03
 ---- batch: 030 ----
mean loss: 366.02
train mean loss: 370.21
epoch train time: 0:00:00.180838
elapsed time: 0:00:26.497887
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:47:34.929711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.52
 ---- batch: 020 ----
mean loss: 369.73
 ---- batch: 030 ----
mean loss: 372.72
train mean loss: 369.82
epoch train time: 0:00:00.175297
elapsed time: 0:00:26.673325
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:47:35.105142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.92
 ---- batch: 020 ----
mean loss: 368.27
 ---- batch: 030 ----
mean loss: 380.38
train mean loss: 368.94
epoch train time: 0:00:00.173885
elapsed time: 0:00:26.847346
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:47:35.279166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.89
 ---- batch: 020 ----
mean loss: 370.72
 ---- batch: 030 ----
mean loss: 368.63
train mean loss: 368.85
epoch train time: 0:00:00.175088
elapsed time: 0:00:27.022589
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:47:35.454409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.00
 ---- batch: 020 ----
mean loss: 362.79
 ---- batch: 030 ----
mean loss: 369.97
train mean loss: 368.68
epoch train time: 0:00:00.173451
elapsed time: 0:00:27.196189
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:47:35.628012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.52
 ---- batch: 020 ----
mean loss: 371.61
 ---- batch: 030 ----
mean loss: 366.37
train mean loss: 368.17
epoch train time: 0:00:00.177664
elapsed time: 0:00:27.374017
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:47:35.805838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.18
 ---- batch: 020 ----
mean loss: 363.87
 ---- batch: 030 ----
mean loss: 368.05
train mean loss: 368.21
epoch train time: 0:00:00.176007
elapsed time: 0:00:27.550176
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:47:35.982017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.48
 ---- batch: 020 ----
mean loss: 373.15
 ---- batch: 030 ----
mean loss: 366.36
train mean loss: 367.51
epoch train time: 0:00:00.173450
elapsed time: 0:00:27.723788
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:47:36.155610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.80
 ---- batch: 020 ----
mean loss: 368.27
 ---- batch: 030 ----
mean loss: 364.11
train mean loss: 367.30
epoch train time: 0:00:00.172133
elapsed time: 0:00:27.896062
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:47:36.327884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.09
 ---- batch: 020 ----
mean loss: 377.33
 ---- batch: 030 ----
mean loss: 359.01
train mean loss: 367.02
epoch train time: 0:00:00.172471
elapsed time: 0:00:28.068683
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:47:36.500520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.54
 ---- batch: 020 ----
mean loss: 362.62
 ---- batch: 030 ----
mean loss: 361.99
train mean loss: 366.94
epoch train time: 0:00:00.170632
elapsed time: 0:00:28.239476
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:47:36.671297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.30
 ---- batch: 020 ----
mean loss: 363.00
 ---- batch: 030 ----
mean loss: 370.40
train mean loss: 366.41
epoch train time: 0:00:00.177224
elapsed time: 0:00:28.416849
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:47:36.848672
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.12
 ---- batch: 020 ----
mean loss: 369.04
 ---- batch: 030 ----
mean loss: 369.62
train mean loss: 366.40
epoch train time: 0:00:00.178370
elapsed time: 0:00:28.595367
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:47:37.027190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.84
 ---- batch: 020 ----
mean loss: 365.85
 ---- batch: 030 ----
mean loss: 369.01
train mean loss: 365.83
epoch train time: 0:00:00.172796
elapsed time: 0:00:28.768316
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:47:37.200139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.99
 ---- batch: 020 ----
mean loss: 366.14
 ---- batch: 030 ----
mean loss: 362.72
train mean loss: 365.79
epoch train time: 0:00:00.176310
elapsed time: 0:00:28.944784
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:47:37.376603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.47
 ---- batch: 020 ----
mean loss: 364.17
 ---- batch: 030 ----
mean loss: 366.59
train mean loss: 365.69
epoch train time: 0:00:00.174917
elapsed time: 0:00:29.119853
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:47:37.551676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.24
 ---- batch: 020 ----
mean loss: 359.57
 ---- batch: 030 ----
mean loss: 373.09
train mean loss: 365.71
epoch train time: 0:00:00.174770
elapsed time: 0:00:29.294772
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:47:37.726594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.25
 ---- batch: 020 ----
mean loss: 362.54
 ---- batch: 030 ----
mean loss: 370.50
train mean loss: 364.94
epoch train time: 0:00:00.176093
elapsed time: 0:00:29.471012
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:47:37.902834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.82
 ---- batch: 020 ----
mean loss: 364.35
 ---- batch: 030 ----
mean loss: 369.46
train mean loss: 364.87
epoch train time: 0:00:00.174154
elapsed time: 0:00:29.645316
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:47:38.077139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.07
 ---- batch: 020 ----
mean loss: 362.45
 ---- batch: 030 ----
mean loss: 361.28
train mean loss: 364.86
epoch train time: 0:00:00.171175
elapsed time: 0:00:29.816632
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:47:38.248469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.75
 ---- batch: 020 ----
mean loss: 363.38
 ---- batch: 030 ----
mean loss: 361.79
train mean loss: 364.89
epoch train time: 0:00:00.171128
elapsed time: 0:00:29.987918
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:47:38.419756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.40
 ---- batch: 020 ----
mean loss: 360.84
 ---- batch: 030 ----
mean loss: 366.43
train mean loss: 364.18
epoch train time: 0:00:00.171826
elapsed time: 0:00:30.159919
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:47:38.591739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.90
 ---- batch: 020 ----
mean loss: 367.02
 ---- batch: 030 ----
mean loss: 358.84
train mean loss: 364.54
epoch train time: 0:00:00.172954
elapsed time: 0:00:30.333015
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:47:38.764835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.13
 ---- batch: 020 ----
mean loss: 368.87
 ---- batch: 030 ----
mean loss: 364.36
train mean loss: 364.04
epoch train time: 0:00:00.183248
elapsed time: 0:00:30.516408
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:47:38.948231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.13
 ---- batch: 020 ----
mean loss: 364.54
 ---- batch: 030 ----
mean loss: 364.29
train mean loss: 363.49
epoch train time: 0:00:00.180519
elapsed time: 0:00:30.697095
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:47:39.128917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.70
 ---- batch: 020 ----
mean loss: 369.29
 ---- batch: 030 ----
mean loss: 364.15
train mean loss: 363.75
epoch train time: 0:00:00.174512
elapsed time: 0:00:30.871764
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:47:39.303584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.79
 ---- batch: 020 ----
mean loss: 369.83
 ---- batch: 030 ----
mean loss: 365.47
train mean loss: 363.30
epoch train time: 0:00:00.173454
elapsed time: 0:00:31.045360
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:47:39.477181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.35
 ---- batch: 020 ----
mean loss: 363.82
 ---- batch: 030 ----
mean loss: 369.27
train mean loss: 363.30
epoch train time: 0:00:00.174179
elapsed time: 0:00:31.219682
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:47:39.651519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.78
 ---- batch: 020 ----
mean loss: 365.75
 ---- batch: 030 ----
mean loss: 365.23
train mean loss: 363.13
epoch train time: 0:00:00.173018
elapsed time: 0:00:31.392876
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:47:39.824697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.11
 ---- batch: 020 ----
mean loss: 355.41
 ---- batch: 030 ----
mean loss: 366.84
train mean loss: 362.90
epoch train time: 0:00:00.178079
elapsed time: 0:00:31.571097
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:47:40.002931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.75
 ---- batch: 020 ----
mean loss: 369.70
 ---- batch: 030 ----
mean loss: 358.16
train mean loss: 362.98
epoch train time: 0:00:00.171899
elapsed time: 0:00:31.743149
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:47:40.174968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.80
 ---- batch: 020 ----
mean loss: 356.72
 ---- batch: 030 ----
mean loss: 364.93
train mean loss: 362.78
epoch train time: 0:00:00.172509
elapsed time: 0:00:31.915797
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:47:40.347617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.90
 ---- batch: 020 ----
mean loss: 361.79
 ---- batch: 030 ----
mean loss: 363.16
train mean loss: 362.03
epoch train time: 0:00:00.175418
elapsed time: 0:00:32.091354
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:47:40.523174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.70
 ---- batch: 020 ----
mean loss: 361.31
 ---- batch: 030 ----
mean loss: 364.98
train mean loss: 362.08
epoch train time: 0:00:00.173091
elapsed time: 0:00:32.264589
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:47:40.696427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.14
 ---- batch: 020 ----
mean loss: 358.99
 ---- batch: 030 ----
mean loss: 360.58
train mean loss: 362.55
epoch train time: 0:00:00.175609
elapsed time: 0:00:32.440358
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:47:40.872179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.27
 ---- batch: 020 ----
mean loss: 361.30
 ---- batch: 030 ----
mean loss: 365.55
train mean loss: 361.76
epoch train time: 0:00:00.178080
elapsed time: 0:00:32.618582
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:47:41.050404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.40
 ---- batch: 020 ----
mean loss: 364.40
 ---- batch: 030 ----
mean loss: 369.74
train mean loss: 361.72
epoch train time: 0:00:00.175524
elapsed time: 0:00:32.794258
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:47:41.226078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.51
 ---- batch: 020 ----
mean loss: 362.65
 ---- batch: 030 ----
mean loss: 360.20
train mean loss: 362.02
epoch train time: 0:00:00.173151
elapsed time: 0:00:32.967550
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:47:41.399370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.10
 ---- batch: 020 ----
mean loss: 362.43
 ---- batch: 030 ----
mean loss: 362.97
train mean loss: 361.16
epoch train time: 0:00:00.174436
elapsed time: 0:00:33.142150
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:47:41.573972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.85
 ---- batch: 020 ----
mean loss: 364.08
 ---- batch: 030 ----
mean loss: 359.67
train mean loss: 360.70
epoch train time: 0:00:00.172841
elapsed time: 0:00:33.315131
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:47:41.746950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.17
 ---- batch: 020 ----
mean loss: 364.77
 ---- batch: 030 ----
mean loss: 360.14
train mean loss: 361.19
epoch train time: 0:00:00.182424
elapsed time: 0:00:33.497710
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:47:41.929530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.23
 ---- batch: 020 ----
mean loss: 358.30
 ---- batch: 030 ----
mean loss: 359.53
train mean loss: 360.90
epoch train time: 0:00:00.174786
elapsed time: 0:00:33.672634
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:47:42.104463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.33
 ---- batch: 020 ----
mean loss: 357.59
 ---- batch: 030 ----
mean loss: 358.48
train mean loss: 360.86
epoch train time: 0:00:00.172327
elapsed time: 0:00:33.845107
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:47:42.276926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.28
 ---- batch: 020 ----
mean loss: 358.42
 ---- batch: 030 ----
mean loss: 367.47
train mean loss: 360.39
epoch train time: 0:00:00.175015
elapsed time: 0:00:34.020267
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:47:42.452089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.85
 ---- batch: 020 ----
mean loss: 358.72
 ---- batch: 030 ----
mean loss: 364.26
train mean loss: 360.53
epoch train time: 0:00:00.173726
elapsed time: 0:00:34.194139
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:47:42.625965
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.67
 ---- batch: 020 ----
mean loss: 362.14
 ---- batch: 030 ----
mean loss: 358.94
train mean loss: 360.12
epoch train time: 0:00:00.175829
elapsed time: 0:00:34.370118
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:47:42.801938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.31
 ---- batch: 020 ----
mean loss: 354.58
 ---- batch: 030 ----
mean loss: 363.02
train mean loss: 359.84
epoch train time: 0:00:00.176959
elapsed time: 0:00:34.547221
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:47:42.979043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.72
 ---- batch: 020 ----
mean loss: 357.88
 ---- batch: 030 ----
mean loss: 351.67
train mean loss: 360.23
epoch train time: 0:00:00.171596
elapsed time: 0:00:34.718960
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:47:43.150803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.79
 ---- batch: 020 ----
mean loss: 365.76
 ---- batch: 030 ----
mean loss: 360.46
train mean loss: 359.71
epoch train time: 0:00:00.173686
elapsed time: 0:00:34.892839
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:47:43.324649
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.76
 ---- batch: 020 ----
mean loss: 358.55
 ---- batch: 030 ----
mean loss: 364.22
train mean loss: 359.66
epoch train time: 0:00:00.176553
elapsed time: 0:00:35.069538
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:47:43.501356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.67
 ---- batch: 020 ----
mean loss: 362.43
 ---- batch: 030 ----
mean loss: 360.66
train mean loss: 359.27
epoch train time: 0:00:00.174663
elapsed time: 0:00:35.244367
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:47:43.676186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.88
 ---- batch: 020 ----
mean loss: 361.00
 ---- batch: 030 ----
mean loss: 361.32
train mean loss: 359.12
epoch train time: 0:00:00.182995
elapsed time: 0:00:35.427512
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:47:43.859364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.97
 ---- batch: 020 ----
mean loss: 358.96
 ---- batch: 030 ----
mean loss: 369.63
train mean loss: 359.44
epoch train time: 0:00:00.178260
elapsed time: 0:00:35.605949
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:47:44.037770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.30
 ---- batch: 020 ----
mean loss: 371.52
 ---- batch: 030 ----
mean loss: 352.22
train mean loss: 359.04
epoch train time: 0:00:00.172171
elapsed time: 0:00:35.778277
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:47:44.210098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.46
 ---- batch: 020 ----
mean loss: 363.20
 ---- batch: 030 ----
mean loss: 366.40
train mean loss: 358.91
epoch train time: 0:00:00.171661
elapsed time: 0:00:35.950083
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:47:44.381912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.92
 ---- batch: 020 ----
mean loss: 356.86
 ---- batch: 030 ----
mean loss: 352.15
train mean loss: 358.70
epoch train time: 0:00:00.173645
elapsed time: 0:00:36.123893
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:47:44.555714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.61
 ---- batch: 020 ----
mean loss: 355.49
 ---- batch: 030 ----
mean loss: 365.00
train mean loss: 358.42
epoch train time: 0:00:00.172710
elapsed time: 0:00:36.296742
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:47:44.728562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.76
 ---- batch: 020 ----
mean loss: 360.84
 ---- batch: 030 ----
mean loss: 365.90
train mean loss: 358.59
epoch train time: 0:00:00.172676
elapsed time: 0:00:36.469558
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:47:44.901381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.99
 ---- batch: 020 ----
mean loss: 354.25
 ---- batch: 030 ----
mean loss: 354.23
train mean loss: 358.44
epoch train time: 0:00:00.175469
elapsed time: 0:00:36.645169
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:47:45.076989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.50
 ---- batch: 020 ----
mean loss: 356.76
 ---- batch: 030 ----
mean loss: 359.97
train mean loss: 358.21
epoch train time: 0:00:00.173105
elapsed time: 0:00:36.818427
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:47:45.250249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.41
 ---- batch: 020 ----
mean loss: 354.91
 ---- batch: 030 ----
mean loss: 361.21
train mean loss: 357.68
epoch train time: 0:00:00.171015
elapsed time: 0:00:36.989611
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:47:45.421461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.86
 ---- batch: 020 ----
mean loss: 354.71
 ---- batch: 030 ----
mean loss: 350.97
train mean loss: 357.99
epoch train time: 0:00:00.175232
elapsed time: 0:00:37.165013
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:47:45.596832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.18
 ---- batch: 020 ----
mean loss: 360.05
 ---- batch: 030 ----
mean loss: 352.83
train mean loss: 358.02
epoch train time: 0:00:00.171608
elapsed time: 0:00:37.336761
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:47:45.768583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.86
 ---- batch: 020 ----
mean loss: 356.28
 ---- batch: 030 ----
mean loss: 357.19
train mean loss: 357.87
epoch train time: 0:00:00.176414
elapsed time: 0:00:37.513318
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:47:45.945142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.31
 ---- batch: 020 ----
mean loss: 362.46
 ---- batch: 030 ----
mean loss: 356.90
train mean loss: 357.47
epoch train time: 0:00:00.180087
elapsed time: 0:00:37.693552
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:47:46.125374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.83
 ---- batch: 020 ----
mean loss: 364.53
 ---- batch: 030 ----
mean loss: 353.15
train mean loss: 357.24
epoch train time: 0:00:00.174326
elapsed time: 0:00:37.868022
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:47:46.299843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.04
 ---- batch: 020 ----
mean loss: 357.17
 ---- batch: 030 ----
mean loss: 357.03
train mean loss: 356.99
epoch train time: 0:00:00.174181
elapsed time: 0:00:38.042349
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:47:46.474171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.54
 ---- batch: 020 ----
mean loss: 358.74
 ---- batch: 030 ----
mean loss: 354.71
train mean loss: 357.05
epoch train time: 0:00:00.174326
elapsed time: 0:00:38.216819
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:47:46.648684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.94
 ---- batch: 020 ----
mean loss: 357.81
 ---- batch: 030 ----
mean loss: 358.00
train mean loss: 356.73
epoch train time: 0:00:00.176906
elapsed time: 0:00:38.393934
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:47:46.825756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.82
 ---- batch: 020 ----
mean loss: 352.41
 ---- batch: 030 ----
mean loss: 356.55
train mean loss: 356.48
epoch train time: 0:00:00.176078
elapsed time: 0:00:38.570159
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:47:47.001997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.70
 ---- batch: 020 ----
mean loss: 353.31
 ---- batch: 030 ----
mean loss: 360.54
train mean loss: 356.44
epoch train time: 0:00:00.172934
elapsed time: 0:00:38.743262
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:47:47.175072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.54
 ---- batch: 020 ----
mean loss: 356.73
 ---- batch: 030 ----
mean loss: 363.07
train mean loss: 356.28
epoch train time: 0:00:00.170999
elapsed time: 0:00:38.914390
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:47:47.346228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.76
 ---- batch: 020 ----
mean loss: 361.54
 ---- batch: 030 ----
mean loss: 350.42
train mean loss: 356.22
epoch train time: 0:00:00.169638
elapsed time: 0:00:39.084186
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:47:47.516007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.99
 ---- batch: 020 ----
mean loss: 362.98
 ---- batch: 030 ----
mean loss: 352.58
train mean loss: 356.09
epoch train time: 0:00:00.168248
elapsed time: 0:00:39.252579
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:47:47.684401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.45
 ---- batch: 020 ----
mean loss: 354.39
 ---- batch: 030 ----
mean loss: 353.17
train mean loss: 356.36
epoch train time: 0:00:00.187270
elapsed time: 0:00:39.440031
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:47:47.871894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.52
 ---- batch: 020 ----
mean loss: 354.71
 ---- batch: 030 ----
mean loss: 360.32
train mean loss: 355.75
epoch train time: 0:00:00.176658
elapsed time: 0:00:39.616883
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:47:48.048702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.30
 ---- batch: 020 ----
mean loss: 358.19
 ---- batch: 030 ----
mean loss: 359.53
train mean loss: 355.57
epoch train time: 0:00:00.170821
elapsed time: 0:00:39.787848
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:47:48.219669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.60
 ---- batch: 020 ----
mean loss: 352.96
 ---- batch: 030 ----
mean loss: 353.19
train mean loss: 355.83
epoch train time: 0:00:00.168103
elapsed time: 0:00:39.956090
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:47:48.387910
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.82
 ---- batch: 020 ----
mean loss: 355.95
 ---- batch: 030 ----
mean loss: 351.55
train mean loss: 355.36
epoch train time: 0:00:00.170711
elapsed time: 0:00:40.126952
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:47:48.558818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.57
 ---- batch: 020 ----
mean loss: 359.85
 ---- batch: 030 ----
mean loss: 357.22
train mean loss: 355.37
epoch train time: 0:00:00.176671
elapsed time: 0:00:40.303809
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:47:48.735629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.84
 ---- batch: 020 ----
mean loss: 350.65
 ---- batch: 030 ----
mean loss: 355.47
train mean loss: 355.03
epoch train time: 0:00:00.171687
elapsed time: 0:00:40.475646
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:47:48.907470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.75
 ---- batch: 020 ----
mean loss: 358.31
 ---- batch: 030 ----
mean loss: 351.67
train mean loss: 355.10
epoch train time: 0:00:00.176707
elapsed time: 0:00:40.652497
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:47:49.084317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.82
 ---- batch: 020 ----
mean loss: 365.09
 ---- batch: 030 ----
mean loss: 347.77
train mean loss: 354.33
epoch train time: 0:00:00.169743
elapsed time: 0:00:40.822379
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:47:49.254197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.04
 ---- batch: 020 ----
mean loss: 351.15
 ---- batch: 030 ----
mean loss: 359.19
train mean loss: 354.54
epoch train time: 0:00:00.174448
elapsed time: 0:00:40.996963
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:47:49.428783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.39
 ---- batch: 020 ----
mean loss: 357.89
 ---- batch: 030 ----
mean loss: 357.90
train mean loss: 354.70
epoch train time: 0:00:00.174458
elapsed time: 0:00:41.171559
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:47:49.603381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.30
 ---- batch: 020 ----
mean loss: 356.78
 ---- batch: 030 ----
mean loss: 353.86
train mean loss: 354.56
epoch train time: 0:00:00.175306
elapsed time: 0:00:41.347020
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:47:49.778843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.72
 ---- batch: 020 ----
mean loss: 360.66
 ---- batch: 030 ----
mean loss: 352.24
train mean loss: 354.21
epoch train time: 0:00:00.183353
elapsed time: 0:00:41.530515
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:47:49.962335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.53
 ---- batch: 020 ----
mean loss: 359.81
 ---- batch: 030 ----
mean loss: 348.75
train mean loss: 353.66
epoch train time: 0:00:00.172882
elapsed time: 0:00:41.703541
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:47:50.135362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.76
 ---- batch: 020 ----
mean loss: 353.49
 ---- batch: 030 ----
mean loss: 354.85
train mean loss: 353.86
epoch train time: 0:00:00.172086
elapsed time: 0:00:41.875765
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:47:50.307601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.61
 ---- batch: 020 ----
mean loss: 346.54
 ---- batch: 030 ----
mean loss: 362.44
train mean loss: 354.23
epoch train time: 0:00:00.170032
elapsed time: 0:00:42.045957
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:47:50.477778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.45
 ---- batch: 020 ----
mean loss: 352.10
 ---- batch: 030 ----
mean loss: 354.05
train mean loss: 353.79
epoch train time: 0:00:00.175503
elapsed time: 0:00:42.221611
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:47:50.653443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.47
 ---- batch: 020 ----
mean loss: 356.75
 ---- batch: 030 ----
mean loss: 350.64
train mean loss: 353.52
epoch train time: 0:00:00.193113
elapsed time: 0:00:42.414878
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:47:50.846702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.83
 ---- batch: 020 ----
mean loss: 353.05
 ---- batch: 030 ----
mean loss: 351.59
train mean loss: 353.30
epoch train time: 0:00:00.177525
elapsed time: 0:00:42.592549
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:47:51.024399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.13
 ---- batch: 020 ----
mean loss: 359.82
 ---- batch: 030 ----
mean loss: 360.53
train mean loss: 353.15
epoch train time: 0:00:00.175469
elapsed time: 0:00:42.768191
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:47:51.200010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.54
 ---- batch: 020 ----
mean loss: 348.04
 ---- batch: 030 ----
mean loss: 354.91
train mean loss: 353.53
epoch train time: 0:00:00.173687
elapsed time: 0:00:42.942051
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:47:51.373941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.19
 ---- batch: 020 ----
mean loss: 356.41
 ---- batch: 030 ----
mean loss: 349.67
train mean loss: 353.16
epoch train time: 0:00:00.173551
elapsed time: 0:00:43.115826
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:47:51.547650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.09
 ---- batch: 020 ----
mean loss: 356.11
 ---- batch: 030 ----
mean loss: 353.79
train mean loss: 352.76
epoch train time: 0:00:00.171244
elapsed time: 0:00:43.287248
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:47:51.719068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.18
 ---- batch: 020 ----
mean loss: 355.07
 ---- batch: 030 ----
mean loss: 350.21
train mean loss: 352.67
epoch train time: 0:00:00.175715
elapsed time: 0:00:43.463123
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:47:51.894945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.83
 ---- batch: 020 ----
mean loss: 347.36
 ---- batch: 030 ----
mean loss: 358.67
train mean loss: 352.43
epoch train time: 0:00:00.179363
elapsed time: 0:00:43.642671
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:47:52.074491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.64
 ---- batch: 020 ----
mean loss: 351.07
 ---- batch: 030 ----
mean loss: 346.20
train mean loss: 352.25
epoch train time: 0:00:00.178345
elapsed time: 0:00:43.821214
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:47:52.253052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.00
 ---- batch: 020 ----
mean loss: 358.69
 ---- batch: 030 ----
mean loss: 346.04
train mean loss: 352.46
epoch train time: 0:00:00.182041
elapsed time: 0:00:44.003432
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:47:52.435257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.71
 ---- batch: 020 ----
mean loss: 351.95
 ---- batch: 030 ----
mean loss: 355.95
train mean loss: 351.97
epoch train time: 0:00:00.177011
elapsed time: 0:00:44.180590
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:47:52.612430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.59
 ---- batch: 020 ----
mean loss: 356.56
 ---- batch: 030 ----
mean loss: 346.56
train mean loss: 352.27
epoch train time: 0:00:00.176380
elapsed time: 0:00:44.357128
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:47:52.788946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.30
 ---- batch: 020 ----
mean loss: 350.19
 ---- batch: 030 ----
mean loss: 356.46
train mean loss: 351.70
epoch train time: 0:00:00.175510
elapsed time: 0:00:44.532776
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:47:52.964612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.64
 ---- batch: 020 ----
mean loss: 348.87
 ---- batch: 030 ----
mean loss: 352.50
train mean loss: 351.86
epoch train time: 0:00:00.178158
elapsed time: 0:00:44.711088
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:47:53.142919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.63
 ---- batch: 020 ----
mean loss: 351.61
 ---- batch: 030 ----
mean loss: 353.60
train mean loss: 351.73
epoch train time: 0:00:00.170570
elapsed time: 0:00:44.881821
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:47:53.313680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.50
 ---- batch: 020 ----
mean loss: 346.74
 ---- batch: 030 ----
mean loss: 351.03
train mean loss: 351.51
epoch train time: 0:00:00.169085
elapsed time: 0:00:45.051117
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:47:53.482967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.28
 ---- batch: 020 ----
mean loss: 357.21
 ---- batch: 030 ----
mean loss: 342.39
train mean loss: 351.51
epoch train time: 0:00:00.169447
elapsed time: 0:00:45.220755
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:47:53.652577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.31
 ---- batch: 020 ----
mean loss: 355.85
 ---- batch: 030 ----
mean loss: 347.18
train mean loss: 351.14
epoch train time: 0:00:00.171354
elapsed time: 0:00:45.392315
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:47:53.824136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.79
 ---- batch: 020 ----
mean loss: 357.78
 ---- batch: 030 ----
mean loss: 350.78
train mean loss: 351.10
epoch train time: 0:00:00.176369
elapsed time: 0:00:45.568827
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:47:54.000648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.50
 ---- batch: 020 ----
mean loss: 344.76
 ---- batch: 030 ----
mean loss: 356.27
train mean loss: 350.77
epoch train time: 0:00:00.175367
elapsed time: 0:00:45.744337
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:47:54.176158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.16
 ---- batch: 020 ----
mean loss: 353.83
 ---- batch: 030 ----
mean loss: 354.69
train mean loss: 350.48
epoch train time: 0:00:00.170271
elapsed time: 0:00:45.914760
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:47:54.346582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.23
 ---- batch: 020 ----
mean loss: 354.13
 ---- batch: 030 ----
mean loss: 350.87
train mean loss: 350.54
epoch train time: 0:00:00.177185
elapsed time: 0:00:46.092087
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:47:54.523919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.26
 ---- batch: 020 ----
mean loss: 355.97
 ---- batch: 030 ----
mean loss: 347.29
train mean loss: 350.15
epoch train time: 0:00:00.172166
elapsed time: 0:00:46.264404
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:47:54.696226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.54
 ---- batch: 020 ----
mean loss: 354.30
 ---- batch: 030 ----
mean loss: 345.14
train mean loss: 350.04
epoch train time: 0:00:00.179442
elapsed time: 0:00:46.443992
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:47:54.875819
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.92
 ---- batch: 020 ----
mean loss: 349.56
 ---- batch: 030 ----
mean loss: 348.04
train mean loss: 350.10
epoch train time: 0:00:00.177001
elapsed time: 0:00:46.621142
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:47:55.052990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.32
 ---- batch: 020 ----
mean loss: 353.20
 ---- batch: 030 ----
mean loss: 349.03
train mean loss: 349.90
epoch train time: 0:00:00.179918
elapsed time: 0:00:46.801229
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:47:55.233106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.37
 ---- batch: 020 ----
mean loss: 344.78
 ---- batch: 030 ----
mean loss: 349.40
train mean loss: 349.61
epoch train time: 0:00:00.178230
elapsed time: 0:00:46.979658
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:47:55.411478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.95
 ---- batch: 020 ----
mean loss: 349.35
 ---- batch: 030 ----
mean loss: 346.66
train mean loss: 349.21
epoch train time: 0:00:00.176002
elapsed time: 0:00:47.155813
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:47:55.587648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.48
 ---- batch: 020 ----
mean loss: 353.41
 ---- batch: 030 ----
mean loss: 355.37
train mean loss: 349.35
epoch train time: 0:00:00.180691
elapsed time: 0:00:47.336701
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:47:55.768550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.20
 ---- batch: 020 ----
mean loss: 348.03
 ---- batch: 030 ----
mean loss: 348.62
train mean loss: 349.23
epoch train time: 0:00:00.183374
elapsed time: 0:00:47.520245
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:47:55.952067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.83
 ---- batch: 020 ----
mean loss: 349.71
 ---- batch: 030 ----
mean loss: 351.45
train mean loss: 348.58
epoch train time: 0:00:00.175608
elapsed time: 0:00:47.695995
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:47:56.127815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.36
 ---- batch: 020 ----
mean loss: 357.03
 ---- batch: 030 ----
mean loss: 344.91
train mean loss: 349.09
epoch train time: 0:00:00.173515
elapsed time: 0:00:47.869650
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:47:56.301471
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.54
 ---- batch: 020 ----
mean loss: 344.95
 ---- batch: 030 ----
mean loss: 351.24
train mean loss: 348.79
epoch train time: 0:00:00.174354
elapsed time: 0:00:48.044176
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:47:56.475996
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.61
 ---- batch: 020 ----
mean loss: 350.06
 ---- batch: 030 ----
mean loss: 353.87
train mean loss: 348.86
epoch train time: 0:00:00.174597
elapsed time: 0:00:48.218912
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:47:56.650732
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.20
 ---- batch: 020 ----
mean loss: 351.56
 ---- batch: 030 ----
mean loss: 342.17
train mean loss: 348.95
epoch train time: 0:00:00.194388
elapsed time: 0:00:48.413442
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:47:56.845264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.08
 ---- batch: 020 ----
mean loss: 344.68
 ---- batch: 030 ----
mean loss: 355.93
train mean loss: 348.76
epoch train time: 0:00:00.180772
elapsed time: 0:00:48.594370
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:47:57.026193
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.33
 ---- batch: 020 ----
mean loss: 348.04
 ---- batch: 030 ----
mean loss: 352.01
train mean loss: 348.46
epoch train time: 0:00:00.179926
elapsed time: 0:00:48.774478
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:47:57.206300
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.79
 ---- batch: 020 ----
mean loss: 347.55
 ---- batch: 030 ----
mean loss: 346.29
train mean loss: 348.76
epoch train time: 0:00:00.182077
elapsed time: 0:00:48.956700
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:47:57.388540
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.75
 ---- batch: 020 ----
mean loss: 349.55
 ---- batch: 030 ----
mean loss: 354.93
train mean loss: 348.52
epoch train time: 0:00:00.176814
elapsed time: 0:00:49.133711
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:47:57.565539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.98
 ---- batch: 020 ----
mean loss: 342.29
 ---- batch: 030 ----
mean loss: 349.99
train mean loss: 348.89
epoch train time: 0:00:00.176824
elapsed time: 0:00:49.310701
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:47:57.742520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.20
 ---- batch: 020 ----
mean loss: 356.01
 ---- batch: 030 ----
mean loss: 350.05
train mean loss: 348.46
epoch train time: 0:00:00.185841
elapsed time: 0:00:49.496696
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:47:57.928548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.00
 ---- batch: 020 ----
mean loss: 346.13
 ---- batch: 030 ----
mean loss: 352.07
train mean loss: 349.21
epoch train time: 0:00:00.175124
elapsed time: 0:00:49.671993
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:47:58.103833
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.38
 ---- batch: 020 ----
mean loss: 348.10
 ---- batch: 030 ----
mean loss: 355.15
train mean loss: 348.33
epoch train time: 0:00:00.174390
elapsed time: 0:00:49.846554
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:47:58.278373
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.43
 ---- batch: 020 ----
mean loss: 355.63
 ---- batch: 030 ----
mean loss: 345.71
train mean loss: 348.23
epoch train time: 0:00:00.173262
elapsed time: 0:00:50.019954
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:47:58.451776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.87
 ---- batch: 020 ----
mean loss: 346.52
 ---- batch: 030 ----
mean loss: 344.73
train mean loss: 348.12
epoch train time: 0:00:00.172341
elapsed time: 0:00:50.192451
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:47:58.624271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.24
 ---- batch: 020 ----
mean loss: 345.01
 ---- batch: 030 ----
mean loss: 349.86
train mean loss: 348.98
epoch train time: 0:00:00.189918
elapsed time: 0:00:50.382571
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:47:58.814427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.55
 ---- batch: 020 ----
mean loss: 344.13
 ---- batch: 030 ----
mean loss: 349.33
train mean loss: 348.42
epoch train time: 0:00:00.175864
elapsed time: 0:00:50.558612
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:47:58.990442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.41
 ---- batch: 020 ----
mean loss: 349.87
 ---- batch: 030 ----
mean loss: 345.97
train mean loss: 348.16
epoch train time: 0:00:00.174744
elapsed time: 0:00:50.733508
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:47:59.165329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.53
 ---- batch: 020 ----
mean loss: 345.99
 ---- batch: 030 ----
mean loss: 351.17
train mean loss: 348.84
epoch train time: 0:00:00.172591
elapsed time: 0:00:50.906288
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:47:59.338107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.06
 ---- batch: 020 ----
mean loss: 348.39
 ---- batch: 030 ----
mean loss: 345.21
train mean loss: 348.67
epoch train time: 0:00:00.171223
elapsed time: 0:00:51.078277
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:47:59.510113
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.41
 ---- batch: 020 ----
mean loss: 351.16
 ---- batch: 030 ----
mean loss: 343.33
train mean loss: 348.42
epoch train time: 0:00:00.178177
elapsed time: 0:00:51.256610
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:47:59.688441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.95
 ---- batch: 020 ----
mean loss: 344.28
 ---- batch: 030 ----
mean loss: 346.93
train mean loss: 348.31
epoch train time: 0:00:00.180202
elapsed time: 0:00:51.436962
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:47:59.868797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.72
 ---- batch: 020 ----
mean loss: 346.81
 ---- batch: 030 ----
mean loss: 345.07
train mean loss: 348.43
epoch train time: 0:00:00.174789
elapsed time: 0:00:51.611910
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:48:00.043730
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.69
 ---- batch: 020 ----
mean loss: 335.56
 ---- batch: 030 ----
mean loss: 351.67
train mean loss: 348.19
epoch train time: 0:00:00.172572
elapsed time: 0:00:51.784651
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:48:00.216472
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.09
 ---- batch: 020 ----
mean loss: 346.21
 ---- batch: 030 ----
mean loss: 349.26
train mean loss: 348.27
epoch train time: 0:00:00.172330
elapsed time: 0:00:51.957120
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:48:00.388940
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.72
 ---- batch: 020 ----
mean loss: 348.89
 ---- batch: 030 ----
mean loss: 350.75
train mean loss: 348.47
epoch train time: 0:00:00.170064
elapsed time: 0:00:52.127330
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:48:00.559148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.10
 ---- batch: 020 ----
mean loss: 347.16
 ---- batch: 030 ----
mean loss: 344.80
train mean loss: 348.41
epoch train time: 0:00:00.173895
elapsed time: 0:00:52.301366
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:48:00.733188
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.98
 ---- batch: 020 ----
mean loss: 343.78
 ---- batch: 030 ----
mean loss: 346.30
train mean loss: 348.12
epoch train time: 0:00:00.178475
elapsed time: 0:00:52.479986
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:48:00.911808
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.09
 ---- batch: 020 ----
mean loss: 341.93
 ---- batch: 030 ----
mean loss: 351.77
train mean loss: 348.13
epoch train time: 0:00:00.177112
elapsed time: 0:00:52.657244
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:48:01.089068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.20
 ---- batch: 020 ----
mean loss: 357.43
 ---- batch: 030 ----
mean loss: 349.65
train mean loss: 348.12
epoch train time: 0:00:00.180787
elapsed time: 0:00:52.838181
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:48:01.270003
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.45
 ---- batch: 020 ----
mean loss: 350.68
 ---- batch: 030 ----
mean loss: 348.63
train mean loss: 348.79
epoch train time: 0:00:00.177502
elapsed time: 0:00:53.015823
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:48:01.447657
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.08
 ---- batch: 020 ----
mean loss: 355.38
 ---- batch: 030 ----
mean loss: 340.25
train mean loss: 347.78
epoch train time: 0:00:00.176516
elapsed time: 0:00:53.192513
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:48:01.624334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.31
 ---- batch: 020 ----
mean loss: 338.77
 ---- batch: 030 ----
mean loss: 356.01
train mean loss: 348.04
epoch train time: 0:00:00.181840
elapsed time: 0:00:53.374508
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:48:01.806328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.64
 ---- batch: 020 ----
mean loss: 342.08
 ---- batch: 030 ----
mean loss: 355.68
train mean loss: 348.30
epoch train time: 0:00:00.174099
elapsed time: 0:00:53.548757
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:48:01.980627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.62
 ---- batch: 020 ----
mean loss: 351.82
 ---- batch: 030 ----
mean loss: 351.27
train mean loss: 348.04
epoch train time: 0:00:00.171044
elapsed time: 0:00:53.720030
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:48:02.151887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.85
 ---- batch: 020 ----
mean loss: 347.94
 ---- batch: 030 ----
mean loss: 349.89
train mean loss: 348.39
epoch train time: 0:00:00.169918
elapsed time: 0:00:53.890127
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:48:02.321947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 338.96
 ---- batch: 020 ----
mean loss: 347.14
 ---- batch: 030 ----
mean loss: 352.35
train mean loss: 347.87
epoch train time: 0:00:00.172645
elapsed time: 0:00:54.062911
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:48:02.494731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.47
 ---- batch: 020 ----
mean loss: 350.86
 ---- batch: 030 ----
mean loss: 342.64
train mean loss: 348.23
epoch train time: 0:00:00.172919
elapsed time: 0:00:54.235984
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:48:02.667803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.10
 ---- batch: 020 ----
mean loss: 353.70
 ---- batch: 030 ----
mean loss: 346.76
train mean loss: 348.11
epoch train time: 0:00:00.178747
elapsed time: 0:00:54.414878
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:48:02.846717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.40
 ---- batch: 020 ----
mean loss: 354.49
 ---- batch: 030 ----
mean loss: 337.11
train mean loss: 348.16
epoch train time: 0:00:00.171045
elapsed time: 0:00:54.586106
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:48:03.017929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.03
 ---- batch: 020 ----
mean loss: 353.52
 ---- batch: 030 ----
mean loss: 346.40
train mean loss: 348.13
epoch train time: 0:00:00.174166
elapsed time: 0:00:54.760433
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:48:03.192254
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.86
 ---- batch: 020 ----
mean loss: 342.90
 ---- batch: 030 ----
mean loss: 346.25
train mean loss: 347.61
epoch train time: 0:00:00.172169
elapsed time: 0:00:54.932739
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:48:03.364560
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.44
 ---- batch: 020 ----
mean loss: 347.48
 ---- batch: 030 ----
mean loss: 347.13
train mean loss: 347.87
epoch train time: 0:00:00.170363
elapsed time: 0:00:55.103242
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:48:03.535062
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.45
 ---- batch: 020 ----
mean loss: 343.87
 ---- batch: 030 ----
mean loss: 348.19
train mean loss: 348.34
epoch train time: 0:00:00.172463
elapsed time: 0:00:55.275877
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:48:03.707707
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.56
 ---- batch: 020 ----
mean loss: 353.59
 ---- batch: 030 ----
mean loss: 345.99
train mean loss: 347.69
epoch train time: 0:00:00.172450
elapsed time: 0:00:55.448479
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:48:03.880299
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.28
 ---- batch: 020 ----
mean loss: 350.27
 ---- batch: 030 ----
mean loss: 340.86
train mean loss: 348.14
epoch train time: 0:00:00.177960
elapsed time: 0:00:55.626580
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:48:04.058399
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.58
 ---- batch: 020 ----
mean loss: 344.59
 ---- batch: 030 ----
mean loss: 348.10
train mean loss: 348.06
epoch train time: 0:00:00.175106
elapsed time: 0:00:55.801826
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:48:04.233675
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.06
 ---- batch: 020 ----
mean loss: 359.24
 ---- batch: 030 ----
mean loss: 343.20
train mean loss: 348.11
epoch train time: 0:00:00.176564
elapsed time: 0:00:55.978559
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:48:04.410407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.31
 ---- batch: 020 ----
mean loss: 349.51
 ---- batch: 030 ----
mean loss: 342.26
train mean loss: 348.12
epoch train time: 0:00:00.174725
elapsed time: 0:00:56.153453
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:48:04.585274
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.45
 ---- batch: 020 ----
mean loss: 346.02
 ---- batch: 030 ----
mean loss: 353.67
train mean loss: 347.92
epoch train time: 0:00:00.172771
elapsed time: 0:00:56.326371
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:48:04.758191
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.20
 ---- batch: 020 ----
mean loss: 342.72
 ---- batch: 030 ----
mean loss: 349.65
train mean loss: 347.69
epoch train time: 0:00:00.174309
elapsed time: 0:00:56.502967
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_6/checkpoint.pth.tar
**** end time: 2019-09-27 16:48:04.934756 ****
