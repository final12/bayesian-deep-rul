Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.25, resume=False, step_size=200, visualize_step=50)
pid: 14530
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 13:07:16.636171 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 13:07:16.653569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2300.96
 ---- batch: 020 ----
mean loss: 1357.95
train mean loss: 1722.77
epoch train time: 0:00:11.869202
elapsed time: 0:00:11.895030
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 13:07:28.531240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1195.19
 ---- batch: 020 ----
mean loss: 1091.79
train mean loss: 1123.77
epoch train time: 0:00:04.300543
elapsed time: 0:00:16.196058
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 13:07:32.832358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1064.19
 ---- batch: 020 ----
mean loss: 1018.87
train mean loss: 1038.87
epoch train time: 0:00:04.330309
elapsed time: 0:00:20.527006
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 13:07:37.163298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1014.49
 ---- batch: 020 ----
mean loss: 1010.89
train mean loss: 1020.66
epoch train time: 0:00:04.275140
elapsed time: 0:00:24.802703
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 13:07:41.439052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1002.77
 ---- batch: 020 ----
mean loss: 1000.68
train mean loss: 1001.84
epoch train time: 0:00:04.285985
elapsed time: 0:00:29.089618
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 13:07:45.725913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 994.55
 ---- batch: 020 ----
mean loss: 963.24
train mean loss: 977.86
epoch train time: 0:00:04.320527
elapsed time: 0:00:33.410718
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 13:07:50.046999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 968.56
 ---- batch: 020 ----
mean loss: 978.53
train mean loss: 976.89
epoch train time: 0:00:04.281872
elapsed time: 0:00:37.693360
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 13:07:54.329706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 972.86
 ---- batch: 020 ----
mean loss: 958.40
train mean loss: 967.14
epoch train time: 0:00:04.287410
elapsed time: 0:00:41.981401
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 13:07:58.617710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.67
 ---- batch: 020 ----
mean loss: 958.55
train mean loss: 961.74
epoch train time: 0:00:04.307477
elapsed time: 0:00:46.289540
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 13:08:02.925833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 958.88
 ---- batch: 020 ----
mean loss: 947.23
train mean loss: 956.06
epoch train time: 0:00:04.295641
elapsed time: 0:00:50.585807
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 13:08:07.222118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 959.38
 ---- batch: 020 ----
mean loss: 932.99
train mean loss: 949.26
epoch train time: 0:00:04.316148
elapsed time: 0:00:54.902534
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 13:08:11.538826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 957.89
 ---- batch: 020 ----
mean loss: 941.76
train mean loss: 945.24
epoch train time: 0:00:04.282681
elapsed time: 0:00:59.185851
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 13:08:15.822177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.57
 ---- batch: 020 ----
mean loss: 943.48
train mean loss: 942.19
epoch train time: 0:00:04.273554
elapsed time: 0:01:03.459993
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 13:08:20.096302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.35
 ---- batch: 020 ----
mean loss: 944.16
train mean loss: 936.12
epoch train time: 0:00:04.258696
elapsed time: 0:01:07.719324
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 13:08:24.355616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 958.66
 ---- batch: 020 ----
mean loss: 939.31
train mean loss: 943.82
epoch train time: 0:00:04.287994
elapsed time: 0:01:12.008017
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 13:08:28.644349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.09
 ---- batch: 020 ----
mean loss: 921.87
train mean loss: 922.70
epoch train time: 0:00:04.287525
elapsed time: 0:01:16.296141
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 13:08:32.932455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.93
 ---- batch: 020 ----
mean loss: 944.71
train mean loss: 932.58
epoch train time: 0:00:04.238746
elapsed time: 0:01:20.535575
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 13:08:37.171900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 933.37
 ---- batch: 020 ----
mean loss: 925.45
train mean loss: 930.37
epoch train time: 0:00:04.311362
elapsed time: 0:01:24.847544
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 13:08:41.483840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.51
 ---- batch: 020 ----
mean loss: 937.11
train mean loss: 926.57
epoch train time: 0:00:04.277539
elapsed time: 0:01:29.125721
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 13:08:45.762009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.68
 ---- batch: 020 ----
mean loss: 921.67
train mean loss: 932.67
epoch train time: 0:00:04.315496
elapsed time: 0:01:33.441951
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 13:08:50.078262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.76
 ---- batch: 020 ----
mean loss: 933.44
train mean loss: 925.26
epoch train time: 0:00:04.312931
elapsed time: 0:01:37.755488
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 13:08:54.391783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 927.63
 ---- batch: 020 ----
mean loss: 914.06
train mean loss: 926.44
epoch train time: 0:00:04.234395
elapsed time: 0:01:41.990479
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 13:08:58.626771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.44
 ---- batch: 020 ----
mean loss: 925.06
train mean loss: 924.63
epoch train time: 0:00:04.236315
elapsed time: 0:01:46.227399
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 13:09:02.863699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 918.38
 ---- batch: 020 ----
mean loss: 919.36
train mean loss: 920.74
epoch train time: 0:00:04.254067
elapsed time: 0:01:50.482027
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 13:09:07.118323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.91
 ---- batch: 020 ----
mean loss: 927.07
train mean loss: 923.07
epoch train time: 0:00:04.249270
elapsed time: 0:01:54.731857
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 13:09:11.368151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 913.65
 ---- batch: 020 ----
mean loss: 917.46
train mean loss: 913.59
epoch train time: 0:00:04.264047
elapsed time: 0:01:58.996472
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 13:09:15.632767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.65
 ---- batch: 020 ----
mean loss: 930.90
train mean loss: 924.37
epoch train time: 0:00:04.258344
elapsed time: 0:02:03.255366
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 13:09:19.891655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.07
 ---- batch: 020 ----
mean loss: 907.42
train mean loss: 907.08
epoch train time: 0:00:04.211222
elapsed time: 0:02:07.467161
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 13:09:24.103452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 910.17
 ---- batch: 020 ----
mean loss: 922.10
train mean loss: 915.93
epoch train time: 0:00:04.221929
elapsed time: 0:02:11.689692
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 13:09:28.326008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.16
 ---- batch: 020 ----
mean loss: 911.88
train mean loss: 916.54
epoch train time: 0:00:04.281081
elapsed time: 0:02:15.971383
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 13:09:32.607674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.23
 ---- batch: 020 ----
mean loss: 891.05
train mean loss: 907.81
epoch train time: 0:00:04.296934
elapsed time: 0:02:20.268994
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 13:09:36.905313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 901.96
 ---- batch: 020 ----
mean loss: 925.10
train mean loss: 914.09
epoch train time: 0:00:04.248132
elapsed time: 0:02:24.517844
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 13:09:41.154145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.73
 ---- batch: 020 ----
mean loss: 898.36
train mean loss: 907.72
epoch train time: 0:00:04.256633
elapsed time: 0:02:28.775074
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 13:09:45.411372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.46
 ---- batch: 020 ----
mean loss: 911.46
train mean loss: 904.99
epoch train time: 0:00:04.252809
elapsed time: 0:02:33.028470
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 13:09:49.664767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.31
 ---- batch: 020 ----
mean loss: 901.50
train mean loss: 906.06
epoch train time: 0:00:04.289879
elapsed time: 0:02:37.318955
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 13:09:53.955266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.40
 ---- batch: 020 ----
mean loss: 908.09
train mean loss: 903.05
epoch train time: 0:00:04.283735
elapsed time: 0:02:41.603507
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 13:09:58.239808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.95
 ---- batch: 020 ----
mean loss: 904.00
train mean loss: 906.58
epoch train time: 0:00:04.319117
elapsed time: 0:02:45.923218
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 13:10:02.559530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.10
 ---- batch: 020 ----
mean loss: 908.75
train mean loss: 905.31
epoch train time: 0:00:04.320805
elapsed time: 0:02:50.244636
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 13:10:06.880934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 921.87
 ---- batch: 020 ----
mean loss: 888.84
train mean loss: 903.69
epoch train time: 0:00:04.253146
elapsed time: 0:02:54.498345
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 13:10:11.134655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.07
 ---- batch: 020 ----
mean loss: 910.57
train mean loss: 896.66
epoch train time: 0:00:04.238336
elapsed time: 0:02:58.737269
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 13:10:15.373565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 912.39
 ---- batch: 020 ----
mean loss: 877.53
train mean loss: 894.15
epoch train time: 0:00:04.287334
elapsed time: 0:03:03.025249
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 13:10:19.661555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 896.31
 ---- batch: 020 ----
mean loss: 895.45
train mean loss: 895.83
epoch train time: 0:00:04.236118
elapsed time: 0:03:07.261937
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 13:10:23.898247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 877.98
 ---- batch: 020 ----
mean loss: 897.94
train mean loss: 896.28
epoch train time: 0:00:04.232413
elapsed time: 0:03:11.494964
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 13:10:28.131262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 890.14
 ---- batch: 020 ----
mean loss: 885.68
train mean loss: 894.92
epoch train time: 0:00:04.299504
elapsed time: 0:03:15.795076
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 13:10:32.431405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.18
 ---- batch: 020 ----
mean loss: 877.03
train mean loss: 884.45
epoch train time: 0:00:04.259559
elapsed time: 0:03:20.055243
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 13:10:36.691607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.76
 ---- batch: 020 ----
mean loss: 889.49
train mean loss: 884.45
epoch train time: 0:00:04.252514
elapsed time: 0:03:24.308426
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 13:10:40.944760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.17
 ---- batch: 020 ----
mean loss: 893.84
train mean loss: 889.71
epoch train time: 0:00:04.289271
elapsed time: 0:03:28.598304
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 13:10:45.234591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.88
 ---- batch: 020 ----
mean loss: 874.38
train mean loss: 883.74
epoch train time: 0:00:04.249609
elapsed time: 0:03:32.848470
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 13:10:49.484755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.72
 ---- batch: 020 ----
mean loss: 892.75
train mean loss: 885.94
epoch train time: 0:00:04.304724
elapsed time: 0:03:37.154006
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 13:10:53.790303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.96
 ---- batch: 020 ----
mean loss: 865.05
train mean loss: 880.14
epoch train time: 0:00:04.371122
elapsed time: 0:03:41.525748
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 13:10:58.162036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.07
 ---- batch: 020 ----
mean loss: 868.92
train mean loss: 877.66
epoch train time: 0:00:04.345089
elapsed time: 0:03:45.871439
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 13:11:02.507737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.24
 ---- batch: 020 ----
mean loss: 870.51
train mean loss: 878.33
epoch train time: 0:00:04.338544
elapsed time: 0:03:50.210595
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 13:11:06.846890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.88
 ---- batch: 020 ----
mean loss: 881.11
train mean loss: 876.92
epoch train time: 0:00:04.351124
elapsed time: 0:03:54.562682
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 13:11:11.198983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 869.05
 ---- batch: 020 ----
mean loss: 867.85
train mean loss: 867.70
epoch train time: 0:00:04.371809
elapsed time: 0:03:58.935082
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 13:11:15.571372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.96
 ---- batch: 020 ----
mean loss: 879.46
train mean loss: 858.95
epoch train time: 0:00:04.356496
elapsed time: 0:04:03.292471
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 13:11:19.928769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.72
 ---- batch: 020 ----
mean loss: 871.45
train mean loss: 863.91
epoch train time: 0:00:04.333886
elapsed time: 0:04:07.626934
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 13:11:24.263225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.50
 ---- batch: 020 ----
mean loss: 838.61
train mean loss: 839.52
epoch train time: 0:00:04.313480
elapsed time: 0:04:11.941062
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 13:11:28.577361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 811.95
 ---- batch: 020 ----
mean loss: 836.07
train mean loss: 819.44
epoch train time: 0:00:04.226695
elapsed time: 0:04:16.168345
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 13:11:32.804638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 784.87
 ---- batch: 020 ----
mean loss: 796.75
train mean loss: 786.23
epoch train time: 0:00:04.273261
elapsed time: 0:04:20.442138
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 13:11:37.078442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 754.55
 ---- batch: 020 ----
mean loss: 744.38
train mean loss: 740.14
epoch train time: 0:00:04.296477
elapsed time: 0:04:24.739184
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 13:11:41.375497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 702.08
 ---- batch: 020 ----
mean loss: 699.90
train mean loss: 699.55
epoch train time: 0:00:04.258584
elapsed time: 0:04:28.998399
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 13:11:45.634689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.18
 ---- batch: 020 ----
mean loss: 672.74
train mean loss: 683.17
epoch train time: 0:00:04.311960
elapsed time: 0:04:33.310913
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 13:11:49.947212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 665.28
 ---- batch: 020 ----
mean loss: 668.80
train mean loss: 661.24
epoch train time: 0:00:04.274867
elapsed time: 0:04:37.586381
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 13:11:54.222685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.54
 ---- batch: 020 ----
mean loss: 651.59
train mean loss: 652.88
epoch train time: 0:00:04.293934
elapsed time: 0:04:41.881174
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 13:11:58.517478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 636.34
 ---- batch: 020 ----
mean loss: 630.53
train mean loss: 628.97
epoch train time: 0:00:04.267463
elapsed time: 0:04:46.149193
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 13:12:02.785486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 621.06
 ---- batch: 020 ----
mean loss: 597.37
train mean loss: 610.32
epoch train time: 0:00:04.231181
elapsed time: 0:04:50.381024
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 13:12:07.017368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 586.20
 ---- batch: 020 ----
mean loss: 604.03
train mean loss: 592.82
epoch train time: 0:00:04.250548
elapsed time: 0:04:54.632212
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 13:12:11.268509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 580.33
 ---- batch: 020 ----
mean loss: 564.32
train mean loss: 575.65
epoch train time: 0:00:04.284606
elapsed time: 0:04:58.917468
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 13:12:15.553803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 575.93
 ---- batch: 020 ----
mean loss: 550.94
train mean loss: 562.69
epoch train time: 0:00:04.256122
elapsed time: 0:05:03.174212
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 13:12:19.810517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 547.38
 ---- batch: 020 ----
mean loss: 551.59
train mean loss: 546.43
epoch train time: 0:00:04.206756
elapsed time: 0:05:07.381551
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 13:12:24.017850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 535.87
 ---- batch: 020 ----
mean loss: 545.19
train mean loss: 542.74
epoch train time: 0:00:04.237458
elapsed time: 0:05:11.619618
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 13:12:28.255921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.57
 ---- batch: 020 ----
mean loss: 520.38
train mean loss: 520.87
epoch train time: 0:00:04.329396
elapsed time: 0:05:15.949648
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 13:12:32.585943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.01
 ---- batch: 020 ----
mean loss: 501.65
train mean loss: 510.67
epoch train time: 0:00:04.294239
elapsed time: 0:05:20.244454
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 13:12:36.880750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.43
 ---- batch: 020 ----
mean loss: 504.46
train mean loss: 500.30
epoch train time: 0:00:04.287217
elapsed time: 0:05:24.532355
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 13:12:41.168654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.18
 ---- batch: 020 ----
mean loss: 488.92
train mean loss: 487.21
epoch train time: 0:00:04.279918
elapsed time: 0:05:28.812908
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 13:12:45.449229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.90
 ---- batch: 020 ----
mean loss: 478.91
train mean loss: 476.82
epoch train time: 0:00:04.285576
elapsed time: 0:05:33.099212
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 13:12:49.735511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.92
 ---- batch: 020 ----
mean loss: 477.91
train mean loss: 472.35
epoch train time: 0:00:04.301616
elapsed time: 0:05:37.401477
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 13:12:54.037768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.48
 ---- batch: 020 ----
mean loss: 467.23
train mean loss: 457.29
epoch train time: 0:00:04.291936
elapsed time: 0:05:41.694023
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 13:12:58.330356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.61
 ---- batch: 020 ----
mean loss: 441.12
train mean loss: 451.89
epoch train time: 0:00:04.312043
elapsed time: 0:05:46.006704
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 13:13:02.643012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.74
 ---- batch: 020 ----
mean loss: 438.73
train mean loss: 449.99
epoch train time: 0:00:04.277461
elapsed time: 0:05:50.284753
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 13:13:06.921057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.35
 ---- batch: 020 ----
mean loss: 442.93
train mean loss: 446.43
epoch train time: 0:00:04.250532
elapsed time: 0:05:54.535846
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 13:13:11.172141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.70
 ---- batch: 020 ----
mean loss: 447.85
train mean loss: 437.60
epoch train time: 0:00:04.281206
elapsed time: 0:05:58.817625
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 13:13:15.453975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.49
 ---- batch: 020 ----
mean loss: 433.40
train mean loss: 430.96
epoch train time: 0:00:04.264937
elapsed time: 0:06:03.083204
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 13:13:19.719504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 432.56
 ---- batch: 020 ----
mean loss: 426.42
train mean loss: 431.31
epoch train time: 0:00:04.248145
elapsed time: 0:06:07.331914
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 13:13:23.968206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.75
 ---- batch: 020 ----
mean loss: 428.70
train mean loss: 426.39
epoch train time: 0:00:04.267213
elapsed time: 0:06:11.599787
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 13:13:28.236098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.75
 ---- batch: 020 ----
mean loss: 432.74
train mean loss: 423.84
epoch train time: 0:00:04.325333
elapsed time: 0:06:15.925792
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 13:13:32.562151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.33
 ---- batch: 020 ----
mean loss: 417.78
train mean loss: 416.81
epoch train time: 0:00:04.252398
elapsed time: 0:06:20.178821
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 13:13:36.815128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.00
 ---- batch: 020 ----
mean loss: 410.19
train mean loss: 416.82
epoch train time: 0:00:04.293071
elapsed time: 0:06:24.472463
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 13:13:41.108762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.84
 ---- batch: 020 ----
mean loss: 409.45
train mean loss: 409.77
epoch train time: 0:00:04.293738
elapsed time: 0:06:28.766764
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 13:13:45.403073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.13
 ---- batch: 020 ----
mean loss: 389.71
train mean loss: 406.76
epoch train time: 0:00:04.247628
elapsed time: 0:06:33.015025
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 13:13:49.651315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.07
 ---- batch: 020 ----
mean loss: 405.14
train mean loss: 403.11
epoch train time: 0:00:04.351899
elapsed time: 0:06:37.367545
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 13:13:54.003864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.01
 ---- batch: 020 ----
mean loss: 402.83
train mean loss: 395.80
epoch train time: 0:00:04.273488
elapsed time: 0:06:41.641662
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 13:13:58.277971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.76
 ---- batch: 020 ----
mean loss: 384.56
train mean loss: 391.59
epoch train time: 0:00:04.315904
elapsed time: 0:06:45.958333
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 13:14:02.594630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.47
 ---- batch: 020 ----
mean loss: 389.04
train mean loss: 392.35
epoch train time: 0:00:04.270013
elapsed time: 0:06:50.228961
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 13:14:06.865256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.13
 ---- batch: 020 ----
mean loss: 386.79
train mean loss: 388.70
epoch train time: 0:00:04.286135
elapsed time: 0:06:54.515722
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 13:14:11.152021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.04
 ---- batch: 020 ----
mean loss: 393.32
train mean loss: 385.93
epoch train time: 0:00:04.304710
elapsed time: 0:06:58.821043
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 13:14:15.457336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.17
 ---- batch: 020 ----
mean loss: 391.50
train mean loss: 386.44
epoch train time: 0:00:04.300791
elapsed time: 0:07:03.122480
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 13:14:19.758779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.06
 ---- batch: 020 ----
mean loss: 384.83
train mean loss: 381.51
epoch train time: 0:00:04.282566
elapsed time: 0:07:07.405640
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 13:14:24.041944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.39
 ---- batch: 020 ----
mean loss: 371.65
train mean loss: 378.76
epoch train time: 0:00:04.266793
elapsed time: 0:07:11.673135
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 13:14:28.309430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.66
 ---- batch: 020 ----
mean loss: 372.23
train mean loss: 376.44
epoch train time: 0:00:04.269014
elapsed time: 0:07:15.942694
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 13:14:32.579008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.81
 ---- batch: 020 ----
mean loss: 377.44
train mean loss: 371.53
epoch train time: 0:00:04.235613
elapsed time: 0:07:20.178875
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 13:14:36.815206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.81
 ---- batch: 020 ----
mean loss: 370.02
train mean loss: 367.89
epoch train time: 0:00:04.197571
elapsed time: 0:07:24.377012
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 13:14:41.013298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.83
 ---- batch: 020 ----
mean loss: 367.45
train mean loss: 367.74
epoch train time: 0:00:04.181882
elapsed time: 0:07:28.559470
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 13:14:45.195761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.35
 ---- batch: 020 ----
mean loss: 358.47
train mean loss: 363.79
epoch train time: 0:00:04.202787
elapsed time: 0:07:32.762831
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 13:14:49.399148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.79
 ---- batch: 020 ----
mean loss: 361.02
train mean loss: 365.89
epoch train time: 0:00:04.220012
elapsed time: 0:07:36.983487
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 13:14:53.619786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.21
 ---- batch: 020 ----
mean loss: 358.46
train mean loss: 357.87
epoch train time: 0:00:04.253279
elapsed time: 0:07:41.237383
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 13:14:57.873752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.17
 ---- batch: 020 ----
mean loss: 352.06
train mean loss: 352.62
epoch train time: 0:00:04.227591
elapsed time: 0:07:45.465701
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 13:15:02.101922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.00
 ---- batch: 020 ----
mean loss: 353.75
train mean loss: 361.31
epoch train time: 0:00:04.239405
elapsed time: 0:07:49.705663
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 13:15:06.341958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.98
 ---- batch: 020 ----
mean loss: 360.05
train mean loss: 362.56
epoch train time: 0:00:04.240004
elapsed time: 0:07:53.946286
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 13:15:10.582574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.46
 ---- batch: 020 ----
mean loss: 355.59
train mean loss: 351.27
epoch train time: 0:00:04.214155
elapsed time: 0:07:58.160963
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 13:15:14.797248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.99
 ---- batch: 020 ----
mean loss: 342.48
train mean loss: 342.51
epoch train time: 0:00:04.205141
elapsed time: 0:08:02.366682
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 13:15:19.002969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.00
 ---- batch: 020 ----
mean loss: 339.92
train mean loss: 347.04
epoch train time: 0:00:04.183055
elapsed time: 0:08:06.550279
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 13:15:23.186616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.28
 ---- batch: 020 ----
mean loss: 346.01
train mean loss: 345.15
epoch train time: 0:00:04.187287
elapsed time: 0:08:10.738158
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 13:15:27.374442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.09
 ---- batch: 020 ----
mean loss: 343.84
train mean loss: 344.31
epoch train time: 0:00:04.241363
elapsed time: 0:08:14.980086
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 13:15:31.616379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.67
 ---- batch: 020 ----
mean loss: 339.98
train mean loss: 345.28
epoch train time: 0:00:04.264100
elapsed time: 0:08:19.244817
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 13:15:35.881120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.94
 ---- batch: 020 ----
mean loss: 343.45
train mean loss: 342.05
epoch train time: 0:00:04.279714
elapsed time: 0:08:23.525213
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 13:15:40.161551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.47
 ---- batch: 020 ----
mean loss: 347.67
train mean loss: 337.33
epoch train time: 0:00:04.234418
elapsed time: 0:08:27.760234
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 13:15:44.396524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.64
 ---- batch: 020 ----
mean loss: 328.99
train mean loss: 331.26
epoch train time: 0:00:04.228458
elapsed time: 0:08:31.989264
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 13:15:48.625552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.24
 ---- batch: 020 ----
mean loss: 331.12
train mean loss: 333.83
epoch train time: 0:00:04.191573
elapsed time: 0:08:36.181411
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 13:15:52.817733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.44
 ---- batch: 020 ----
mean loss: 326.16
train mean loss: 335.15
epoch train time: 0:00:04.165989
elapsed time: 0:08:40.348060
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 13:15:56.984390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.27
 ---- batch: 020 ----
mean loss: 329.66
train mean loss: 328.52
epoch train time: 0:00:04.200152
elapsed time: 0:08:44.548806
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 13:16:01.185096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.25
 ---- batch: 020 ----
mean loss: 326.31
train mean loss: 329.25
epoch train time: 0:00:04.295708
elapsed time: 0:08:48.845087
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 13:16:05.481399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.59
 ---- batch: 020 ----
mean loss: 324.43
train mean loss: 325.03
epoch train time: 0:00:04.302188
elapsed time: 0:08:53.147927
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 13:16:09.784233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.05
 ---- batch: 020 ----
mean loss: 320.24
train mean loss: 324.03
epoch train time: 0:00:04.304721
elapsed time: 0:08:57.453277
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 13:16:14.089583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.39
 ---- batch: 020 ----
mean loss: 316.26
train mean loss: 320.61
epoch train time: 0:00:04.284712
elapsed time: 0:09:01.738569
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 13:16:18.374893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.10
 ---- batch: 020 ----
mean loss: 326.56
train mean loss: 324.17
epoch train time: 0:00:04.274540
elapsed time: 0:09:06.013724
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 13:16:22.650030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.78
 ---- batch: 020 ----
mean loss: 328.32
train mean loss: 326.47
epoch train time: 0:00:04.228308
elapsed time: 0:09:10.242689
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 13:16:26.878916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.73
 ---- batch: 020 ----
mean loss: 318.45
train mean loss: 321.78
epoch train time: 0:00:04.219379
elapsed time: 0:09:14.462603
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 13:16:31.098904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.39
 ---- batch: 020 ----
mean loss: 324.93
train mean loss: 319.67
epoch train time: 0:00:04.258360
elapsed time: 0:09:18.721594
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 13:16:35.357892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.44
 ---- batch: 020 ----
mean loss: 306.42
train mean loss: 313.65
epoch train time: 0:00:04.292122
elapsed time: 0:09:23.014304
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 13:16:39.650590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.45
 ---- batch: 020 ----
mean loss: 307.09
train mean loss: 309.45
epoch train time: 0:00:04.292957
elapsed time: 0:09:27.308097
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 13:16:43.944403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.73
 ---- batch: 020 ----
mean loss: 318.64
train mean loss: 316.13
epoch train time: 0:00:04.278971
elapsed time: 0:09:31.587674
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 13:16:48.223972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.70
 ---- batch: 020 ----
mean loss: 317.95
train mean loss: 313.96
epoch train time: 0:00:04.295575
elapsed time: 0:09:35.883805
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 13:16:52.520102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.13
 ---- batch: 020 ----
mean loss: 304.24
train mean loss: 309.52
epoch train time: 0:00:04.280812
elapsed time: 0:09:40.165168
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 13:16:56.801449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.15
 ---- batch: 020 ----
mean loss: 312.25
train mean loss: 308.94
epoch train time: 0:00:04.211833
elapsed time: 0:09:44.377712
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 13:17:01.014018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.18
 ---- batch: 020 ----
mean loss: 319.25
train mean loss: 312.21
epoch train time: 0:00:04.321781
elapsed time: 0:09:48.700086
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 13:17:05.336376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.12
 ---- batch: 020 ----
mean loss: 301.68
train mean loss: 308.31
epoch train time: 0:00:04.407365
elapsed time: 0:09:53.108118
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 13:17:09.744404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.09
 ---- batch: 020 ----
mean loss: 297.83
train mean loss: 306.34
epoch train time: 0:00:04.362333
elapsed time: 0:09:57.470985
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 13:17:14.107288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.19
 ---- batch: 020 ----
mean loss: 300.88
train mean loss: 302.36
epoch train time: 0:00:04.271700
elapsed time: 0:10:01.743308
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 13:17:18.379596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.72
 ---- batch: 020 ----
mean loss: 316.23
train mean loss: 307.80
epoch train time: 0:00:04.201369
elapsed time: 0:10:05.945348
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 13:17:22.581668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.86
 ---- batch: 020 ----
mean loss: 298.51
train mean loss: 302.90
epoch train time: 0:00:04.217862
elapsed time: 0:10:10.163809
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 13:17:26.800105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.91
 ---- batch: 020 ----
mean loss: 316.35
train mean loss: 307.23
epoch train time: 0:00:04.206663
elapsed time: 0:10:14.371018
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 13:17:31.007330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.45
 ---- batch: 020 ----
mean loss: 298.08
train mean loss: 305.61
epoch train time: 0:00:04.211240
elapsed time: 0:10:18.582840
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 13:17:35.219137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.02
 ---- batch: 020 ----
mean loss: 295.97
train mean loss: 296.74
epoch train time: 0:00:04.211396
elapsed time: 0:10:22.794789
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 13:17:39.431080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.57
 ---- batch: 020 ----
mean loss: 299.19
train mean loss: 295.55
epoch train time: 0:00:04.271439
elapsed time: 0:10:27.066760
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 13:17:43.703051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.42
 ---- batch: 020 ----
mean loss: 308.69
train mean loss: 300.24
epoch train time: 0:00:04.251837
elapsed time: 0:10:31.319230
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 13:17:47.955550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.39
 ---- batch: 020 ----
mean loss: 300.84
train mean loss: 295.89
epoch train time: 0:00:04.243898
elapsed time: 0:10:35.563758
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 13:17:52.200066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.35
 ---- batch: 020 ----
mean loss: 287.41
train mean loss: 294.67
epoch train time: 0:00:04.239404
elapsed time: 0:10:39.803804
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 13:17:56.440146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 291.04
 ---- batch: 020 ----
mean loss: 296.22
train mean loss: 293.59
epoch train time: 0:00:04.236902
elapsed time: 0:10:44.041526
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 13:18:00.677787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.55
 ---- batch: 020 ----
mean loss: 289.44
train mean loss: 293.68
epoch train time: 0:00:04.232945
elapsed time: 0:10:48.274973
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 13:18:04.911283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.64
 ---- batch: 020 ----
mean loss: 297.06
train mean loss: 289.13
epoch train time: 0:00:04.219714
elapsed time: 0:10:52.495257
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 13:18:09.131551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.40
 ---- batch: 020 ----
mean loss: 291.13
train mean loss: 291.11
epoch train time: 0:00:04.218289
elapsed time: 0:10:56.714136
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 13:18:13.350434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.49
 ---- batch: 020 ----
mean loss: 290.18
train mean loss: 288.50
epoch train time: 0:00:04.233902
elapsed time: 0:11:00.948583
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 13:18:17.584886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.76
 ---- batch: 020 ----
mean loss: 285.10
train mean loss: 288.35
epoch train time: 0:00:04.288037
elapsed time: 0:11:05.237263
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 13:18:21.873565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.42
 ---- batch: 020 ----
mean loss: 286.16
train mean loss: 290.67
epoch train time: 0:00:04.339366
elapsed time: 0:11:09.577218
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 13:18:26.213575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.52
 ---- batch: 020 ----
mean loss: 286.19
train mean loss: 287.16
epoch train time: 0:00:04.263634
elapsed time: 0:11:13.841610
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 13:18:30.477901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.14
 ---- batch: 020 ----
mean loss: 288.77
train mean loss: 287.58
epoch train time: 0:00:04.399953
elapsed time: 0:11:18.242203
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 13:18:34.878500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.81
 ---- batch: 020 ----
mean loss: 289.13
train mean loss: 286.07
epoch train time: 0:00:04.371033
elapsed time: 0:11:22.613888
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 13:18:39.250179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.27
 ---- batch: 020 ----
mean loss: 284.44
train mean loss: 283.04
epoch train time: 0:00:04.392021
elapsed time: 0:11:27.006537
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 13:18:43.642901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.79
 ---- batch: 020 ----
mean loss: 280.75
train mean loss: 283.34
epoch train time: 0:00:04.369278
elapsed time: 0:11:31.376473
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 13:18:48.012757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.21
 ---- batch: 020 ----
mean loss: 280.62
train mean loss: 284.99
epoch train time: 0:00:04.362259
elapsed time: 0:11:35.739364
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 13:18:52.375663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.61
 ---- batch: 020 ----
mean loss: 292.61
train mean loss: 285.42
epoch train time: 0:00:04.414076
elapsed time: 0:11:40.154074
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 13:18:56.790364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.68
 ---- batch: 020 ----
mean loss: 273.14
train mean loss: 280.52
epoch train time: 0:00:04.396331
elapsed time: 0:11:44.551047
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 13:19:01.187341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.54
 ---- batch: 020 ----
mean loss: 272.68
train mean loss: 278.00
epoch train time: 0:00:04.392330
elapsed time: 0:11:48.943959
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 13:19:05.580246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.46
 ---- batch: 020 ----
mean loss: 277.14
train mean loss: 279.43
epoch train time: 0:00:04.312918
elapsed time: 0:11:53.257419
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 13:19:09.893727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.56
 ---- batch: 020 ----
mean loss: 287.32
train mean loss: 281.44
epoch train time: 0:00:04.197836
elapsed time: 0:11:57.455839
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 13:19:14.092153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.25
 ---- batch: 020 ----
mean loss: 289.25
train mean loss: 280.50
epoch train time: 0:00:04.214900
elapsed time: 0:12:01.671293
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 13:19:18.307577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.16
 ---- batch: 020 ----
mean loss: 273.77
train mean loss: 277.12
epoch train time: 0:00:04.201089
elapsed time: 0:12:05.872886
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 13:19:22.509185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.74
 ---- batch: 020 ----
mean loss: 276.85
train mean loss: 275.23
epoch train time: 0:00:04.197780
elapsed time: 0:12:10.071201
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 13:19:26.707528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.89
 ---- batch: 020 ----
mean loss: 270.11
train mean loss: 274.32
epoch train time: 0:00:04.207513
elapsed time: 0:12:14.279385
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 13:19:30.915679
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.77
 ---- batch: 020 ----
mean loss: 278.99
train mean loss: 275.75
epoch train time: 0:00:04.247191
elapsed time: 0:12:18.527104
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 13:19:35.163387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.32
 ---- batch: 020 ----
mean loss: 277.60
train mean loss: 275.09
epoch train time: 0:00:04.218975
elapsed time: 0:12:22.746617
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 13:19:39.382913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.61
 ---- batch: 020 ----
mean loss: 270.24
train mean loss: 269.96
epoch train time: 0:00:04.225468
elapsed time: 0:12:26.972687
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 13:19:43.609042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.33
 ---- batch: 020 ----
mean loss: 274.88
train mean loss: 276.23
epoch train time: 0:00:04.223587
elapsed time: 0:12:31.196993
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 13:19:47.833222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.15
 ---- batch: 020 ----
mean loss: 275.92
train mean loss: 274.73
epoch train time: 0:00:04.219667
elapsed time: 0:12:35.417128
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 13:19:52.053463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.91
 ---- batch: 020 ----
mean loss: 271.41
train mean loss: 272.82
epoch train time: 0:00:04.251230
elapsed time: 0:12:39.669011
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 13:19:56.305343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.83
 ---- batch: 020 ----
mean loss: 265.08
train mean loss: 272.43
epoch train time: 0:00:04.207316
elapsed time: 0:12:43.876938
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 13:20:00.513250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.34
 ---- batch: 020 ----
mean loss: 270.86
train mean loss: 272.26
epoch train time: 0:00:04.248144
elapsed time: 0:12:48.125634
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 13:20:04.761967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.64
 ---- batch: 020 ----
mean loss: 262.91
train mean loss: 267.28
epoch train time: 0:00:04.273753
elapsed time: 0:12:52.400025
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 13:20:09.036324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.45
 ---- batch: 020 ----
mean loss: 274.37
train mean loss: 271.11
epoch train time: 0:00:04.244567
elapsed time: 0:12:56.645182
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 13:20:13.281478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.16
 ---- batch: 020 ----
mean loss: 269.71
train mean loss: 273.18
epoch train time: 0:00:04.194705
elapsed time: 0:13:00.840427
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 13:20:17.476723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.83
 ---- batch: 020 ----
mean loss: 261.24
train mean loss: 269.75
epoch train time: 0:00:04.208671
elapsed time: 0:13:05.049700
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 13:20:21.685999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.21
 ---- batch: 020 ----
mean loss: 266.72
train mean loss: 267.02
epoch train time: 0:00:04.445091
elapsed time: 0:13:09.495442
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 13:20:26.131743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.50
 ---- batch: 020 ----
mean loss: 269.16
train mean loss: 267.72
epoch train time: 0:00:04.447417
elapsed time: 0:13:13.943506
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 13:20:30.579831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.40
 ---- batch: 020 ----
mean loss: 266.61
train mean loss: 264.92
epoch train time: 0:00:04.464060
elapsed time: 0:13:18.408533
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 13:20:35.044850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.64
 ---- batch: 020 ----
mean loss: 274.71
train mean loss: 269.02
epoch train time: 0:00:04.458719
elapsed time: 0:13:22.867912
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 13:20:39.504207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 273.43
 ---- batch: 020 ----
mean loss: 260.25
train mean loss: 265.61
epoch train time: 0:00:04.463729
elapsed time: 0:13:27.332289
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 13:20:43.968582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.98
 ---- batch: 020 ----
mean loss: 268.41
train mean loss: 263.73
epoch train time: 0:00:04.468185
elapsed time: 0:13:31.801062
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 13:20:48.437353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.32
 ---- batch: 020 ----
mean loss: 267.30
train mean loss: 260.35
epoch train time: 0:00:04.464163
elapsed time: 0:13:36.265921
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 13:20:52.902255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.18
 ---- batch: 020 ----
mean loss: 259.97
train mean loss: 264.25
epoch train time: 0:00:04.460110
elapsed time: 0:13:40.726689
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 13:20:57.362993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.82
 ---- batch: 020 ----
mean loss: 266.09
train mean loss: 269.81
epoch train time: 0:00:04.401397
elapsed time: 0:13:45.128736
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 13:21:01.765024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.32
 ---- batch: 020 ----
mean loss: 263.30
train mean loss: 264.32
epoch train time: 0:00:04.387131
elapsed time: 0:13:49.516479
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 13:21:06.152776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.45
 ---- batch: 020 ----
mean loss: 265.55
train mean loss: 262.03
epoch train time: 0:00:04.233964
elapsed time: 0:13:53.751031
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 13:21:10.387324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.48
 ---- batch: 020 ----
mean loss: 264.28
train mean loss: 262.74
epoch train time: 0:00:04.235688
elapsed time: 0:13:57.987272
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 13:21:14.623573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.01
 ---- batch: 020 ----
mean loss: 264.20
train mean loss: 267.57
epoch train time: 0:00:04.245567
elapsed time: 0:14:02.233391
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 13:21:18.869708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.32
 ---- batch: 020 ----
mean loss: 255.14
train mean loss: 259.48
epoch train time: 0:00:04.230111
elapsed time: 0:14:06.464105
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 13:21:23.100426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 260.23
 ---- batch: 020 ----
mean loss: 257.96
train mean loss: 259.11
epoch train time: 0:00:04.197943
elapsed time: 0:14:10.662630
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 13:21:27.298926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.74
 ---- batch: 020 ----
mean loss: 257.61
train mean loss: 255.91
epoch train time: 0:00:04.212027
elapsed time: 0:14:14.875339
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 13:21:31.511656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.04
 ---- batch: 020 ----
mean loss: 250.29
train mean loss: 255.84
epoch train time: 0:00:04.272152
elapsed time: 0:14:19.148395
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 13:21:35.784749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.80
 ---- batch: 020 ----
mean loss: 261.80
train mean loss: 259.56
epoch train time: 0:00:04.261140
elapsed time: 0:14:23.410144
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 13:21:40.046460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.78
 ---- batch: 020 ----
mean loss: 259.92
train mean loss: 259.98
epoch train time: 0:00:04.224337
elapsed time: 0:14:27.635061
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 13:21:44.271361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.40
 ---- batch: 020 ----
mean loss: 264.31
train mean loss: 258.60
epoch train time: 0:00:04.220955
elapsed time: 0:14:31.856702
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 13:21:48.492925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.06
 ---- batch: 020 ----
mean loss: 261.02
train mean loss: 256.17
epoch train time: 0:00:04.227266
elapsed time: 0:14:36.084497
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 13:21:52.720791
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.69
 ---- batch: 020 ----
mean loss: 253.73
train mean loss: 253.70
epoch train time: 0:00:04.281459
elapsed time: 0:14:40.366517
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 13:21:57.002824
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.43
 ---- batch: 020 ----
mean loss: 251.15
train mean loss: 252.24
epoch train time: 0:00:04.251214
elapsed time: 0:14:44.618290
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 13:22:01.254582
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 244.78
 ---- batch: 020 ----
mean loss: 267.03
train mean loss: 255.89
epoch train time: 0:00:04.272465
elapsed time: 0:14:48.891481
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 13:22:05.527786
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.57
 ---- batch: 020 ----
mean loss: 247.68
train mean loss: 254.09
epoch train time: 0:00:04.284020
elapsed time: 0:14:53.176067
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 13:22:09.812352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.79
 ---- batch: 020 ----
mean loss: 256.58
train mean loss: 254.22
epoch train time: 0:00:04.395885
elapsed time: 0:14:57.572585
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 13:22:14.208872
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.96
 ---- batch: 020 ----
mean loss: 252.54
train mean loss: 255.21
epoch train time: 0:00:04.279626
elapsed time: 0:15:01.852763
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 13:22:18.489055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.80
 ---- batch: 020 ----
mean loss: 254.23
train mean loss: 253.90
epoch train time: 0:00:04.218018
elapsed time: 0:15:06.071324
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 13:22:22.707607
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.82
 ---- batch: 020 ----
mean loss: 255.22
train mean loss: 255.84
epoch train time: 0:00:04.180705
elapsed time: 0:15:10.252547
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 13:22:26.888834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.70
 ---- batch: 020 ----
mean loss: 248.61
train mean loss: 253.28
epoch train time: 0:00:04.194220
elapsed time: 0:15:14.447295
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 13:22:31.083577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.34
 ---- batch: 020 ----
mean loss: 251.50
train mean loss: 254.48
epoch train time: 0:00:04.200971
elapsed time: 0:15:18.648786
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 13:22:35.285086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.40
 ---- batch: 020 ----
mean loss: 251.65
train mean loss: 253.42
epoch train time: 0:00:04.195455
elapsed time: 0:15:22.844900
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 13:22:39.481207
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 258.07
 ---- batch: 020 ----
mean loss: 254.73
train mean loss: 255.21
epoch train time: 0:00:04.248715
elapsed time: 0:15:27.094220
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 13:22:43.730588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.80
 ---- batch: 020 ----
mean loss: 253.08
train mean loss: 253.47
epoch train time: 0:00:04.235176
elapsed time: 0:15:31.330024
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 13:22:47.966305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 247.50
 ---- batch: 020 ----
mean loss: 261.82
train mean loss: 254.13
epoch train time: 0:00:04.194798
elapsed time: 0:15:35.525356
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 13:22:52.161698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.02
 ---- batch: 020 ----
mean loss: 250.06
train mean loss: 253.50
epoch train time: 0:00:04.200670
elapsed time: 0:15:39.726639
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 13:22:56.362934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.02
 ---- batch: 020 ----
mean loss: 254.75
train mean loss: 254.45
epoch train time: 0:00:04.198485
elapsed time: 0:15:43.925678
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 13:23:00.561973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.77
 ---- batch: 020 ----
mean loss: 253.34
train mean loss: 253.59
epoch train time: 0:00:04.245758
elapsed time: 0:15:48.172010
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 13:23:04.808305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 257.88
 ---- batch: 020 ----
mean loss: 254.99
train mean loss: 256.07
epoch train time: 0:00:04.224044
elapsed time: 0:15:52.396642
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 13:23:09.032980
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.63
 ---- batch: 020 ----
mean loss: 254.39
train mean loss: 254.04
epoch train time: 0:00:04.269076
elapsed time: 0:15:56.666542
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 13:23:13.302889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.79
 ---- batch: 020 ----
mean loss: 250.66
train mean loss: 254.15
epoch train time: 0:00:04.231979
elapsed time: 0:16:00.899113
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 13:23:17.535409
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.10
 ---- batch: 020 ----
mean loss: 254.05
train mean loss: 252.44
epoch train time: 0:00:04.192229
elapsed time: 0:16:05.091885
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 13:23:21.728177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.07
 ---- batch: 020 ----
mean loss: 255.81
train mean loss: 253.63
epoch train time: 0:00:04.221147
elapsed time: 0:16:09.313681
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 13:23:25.950026
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.43
 ---- batch: 020 ----
mean loss: 253.74
train mean loss: 254.34
epoch train time: 0:00:04.176916
elapsed time: 0:16:13.491200
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 13:23:30.127499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.49
 ---- batch: 020 ----
mean loss: 250.45
train mean loss: 252.33
epoch train time: 0:00:04.226143
elapsed time: 0:16:17.717870
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 13:23:34.354163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.62
 ---- batch: 020 ----
mean loss: 250.77
train mean loss: 251.40
epoch train time: 0:00:04.160698
elapsed time: 0:16:21.879106
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 13:23:38.515396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.41
 ---- batch: 020 ----
mean loss: 255.37
train mean loss: 254.42
epoch train time: 0:00:04.141328
elapsed time: 0:16:26.020980
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 13:23:42.657272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 256.46
 ---- batch: 020 ----
mean loss: 251.94
train mean loss: 254.84
epoch train time: 0:00:04.163807
elapsed time: 0:16:30.185325
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 13:23:46.821635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.49
 ---- batch: 020 ----
mean loss: 253.18
train mean loss: 254.80
epoch train time: 0:00:04.167532
elapsed time: 0:16:34.353398
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 13:23:50.989728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.32
 ---- batch: 020 ----
mean loss: 265.25
train mean loss: 255.97
epoch train time: 0:00:04.163140
elapsed time: 0:16:38.517126
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 13:23:55.153417
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.28
 ---- batch: 020 ----
mean loss: 253.53
train mean loss: 252.76
epoch train time: 0:00:04.183351
elapsed time: 0:16:42.701022
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 13:23:59.337318
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.65
 ---- batch: 020 ----
mean loss: 253.53
train mean loss: 253.20
epoch train time: 0:00:04.148109
elapsed time: 0:16:46.849796
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 13:24:03.486018
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.07
 ---- batch: 020 ----
mean loss: 250.96
train mean loss: 252.38
epoch train time: 0:00:04.155223
elapsed time: 0:16:51.005505
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 13:24:07.641798
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 259.33
 ---- batch: 020 ----
mean loss: 242.76
train mean loss: 251.34
epoch train time: 0:00:04.195640
elapsed time: 0:16:55.201779
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 13:24:11.838091
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 262.11
 ---- batch: 020 ----
mean loss: 243.39
train mean loss: 252.77
epoch train time: 0:00:04.166050
elapsed time: 0:16:59.368361
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 13:24:16.004646
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.90
 ---- batch: 020 ----
mean loss: 252.22
train mean loss: 252.47
epoch train time: 0:00:04.194787
elapsed time: 0:17:03.563755
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 13:24:20.200061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 243.90
 ---- batch: 020 ----
mean loss: 256.27
train mean loss: 252.66
epoch train time: 0:00:04.177771
elapsed time: 0:17:07.742113
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 13:24:24.378410
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.46
 ---- batch: 020 ----
mean loss: 254.17
train mean loss: 252.18
epoch train time: 0:00:04.258238
elapsed time: 0:17:12.000931
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 13:24:28.637248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.19
 ---- batch: 020 ----
mean loss: 251.87
train mean loss: 253.02
epoch train time: 0:00:04.226561
elapsed time: 0:17:16.228149
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 13:24:32.864396
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.03
 ---- batch: 020 ----
mean loss: 252.36
train mean loss: 252.36
epoch train time: 0:00:04.214922
elapsed time: 0:17:20.443565
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 13:24:37.079858
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.57
 ---- batch: 020 ----
mean loss: 258.41
train mean loss: 254.03
epoch train time: 0:00:04.222656
elapsed time: 0:17:24.666916
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 13:24:41.303203
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 255.94
 ---- batch: 020 ----
mean loss: 249.89
train mean loss: 251.07
epoch train time: 0:00:04.248211
elapsed time: 0:17:28.915778
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 13:24:45.552070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 254.90
 ---- batch: 020 ----
mean loss: 249.26
train mean loss: 252.03
epoch train time: 0:00:04.224265
elapsed time: 0:17:33.140711
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 13:24:49.777031
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 250.76
 ---- batch: 020 ----
mean loss: 249.03
train mean loss: 249.25
epoch train time: 0:00:04.237207
elapsed time: 0:17:37.378610
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 13:24:54.014942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 251.49
 ---- batch: 020 ----
mean loss: 254.69
train mean loss: 253.74
epoch train time: 0:00:04.244716
elapsed time: 0:17:41.623940
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 13:24:58.260303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.58
 ---- batch: 020 ----
mean loss: 246.29
train mean loss: 248.78
epoch train time: 0:00:04.249745
elapsed time: 0:17:45.874305
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 13:25:02.510591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 252.51
 ---- batch: 020 ----
mean loss: 254.23
train mean loss: 250.67
epoch train time: 0:00:04.258236
elapsed time: 0:17:50.133245
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 13:25:06.769572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 253.76
 ---- batch: 020 ----
mean loss: 246.10
train mean loss: 251.25
epoch train time: 0:00:04.235515
elapsed time: 0:17:54.379067
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.25/bayesian_conv5_dense1_0.25_1/checkpoint.pth.tar
**** end time: 2019-09-25 13:25:11.015258 ****
