Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_1', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 20089
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianDense3...
Done.
**** start time: 2019-09-25 19:30:35.988781 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 480]               0
    BayesianLinear-2                  [-1, 100]          96,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 136,200
Trainable params: 136,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 19:30:35.998842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3568.43
 ---- batch: 020 ----
mean loss: 3209.16
 ---- batch: 030 ----
mean loss: 3009.67
 ---- batch: 040 ----
mean loss: 2753.18
 ---- batch: 050 ----
mean loss: 2533.85
 ---- batch: 060 ----
mean loss: 2442.05
 ---- batch: 070 ----
mean loss: 2263.88
 ---- batch: 080 ----
mean loss: 2192.20
 ---- batch: 090 ----
mean loss: 2089.64
train mean loss: 2632.56
epoch train time: 0:00:35.235619
elapsed time: 0:00:35.252429
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 19:31:11.241260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1919.47
 ---- batch: 020 ----
mean loss: 1909.26
 ---- batch: 030 ----
mean loss: 1846.30
 ---- batch: 040 ----
mean loss: 1796.54
 ---- batch: 050 ----
mean loss: 1713.21
 ---- batch: 060 ----
mean loss: 1697.94
 ---- batch: 070 ----
mean loss: 1665.57
 ---- batch: 080 ----
mean loss: 1625.38
 ---- batch: 090 ----
mean loss: 1616.38
train mean loss: 1743.22
epoch train time: 0:00:01.629118
elapsed time: 0:00:36.882054
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 19:31:12.871106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1541.30
 ---- batch: 020 ----
mean loss: 1496.03
 ---- batch: 030 ----
mean loss: 1501.32
 ---- batch: 040 ----
mean loss: 1485.21
 ---- batch: 050 ----
mean loss: 1467.99
 ---- batch: 060 ----
mean loss: 1429.98
 ---- batch: 070 ----
mean loss: 1432.63
 ---- batch: 080 ----
mean loss: 1392.70
 ---- batch: 090 ----
mean loss: 1371.34
train mean loss: 1452.93
epoch train time: 0:00:01.602008
elapsed time: 0:00:38.484664
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 19:31:14.473771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1360.95
 ---- batch: 020 ----
mean loss: 1311.01
 ---- batch: 030 ----
mean loss: 1294.72
 ---- batch: 040 ----
mean loss: 1309.96
 ---- batch: 050 ----
mean loss: 1280.11
 ---- batch: 060 ----
mean loss: 1275.37
 ---- batch: 070 ----
mean loss: 1242.99
 ---- batch: 080 ----
mean loss: 1237.81
 ---- batch: 090 ----
mean loss: 1234.38
train mean loss: 1279.06
epoch train time: 0:00:01.638639
elapsed time: 0:00:40.123948
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 19:31:16.113030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1202.11
 ---- batch: 020 ----
mean loss: 1187.91
 ---- batch: 030 ----
mean loss: 1177.00
 ---- batch: 040 ----
mean loss: 1161.80
 ---- batch: 050 ----
mean loss: 1161.38
 ---- batch: 060 ----
mean loss: 1136.95
 ---- batch: 070 ----
mean loss: 1151.24
 ---- batch: 080 ----
mean loss: 1144.77
 ---- batch: 090 ----
mean loss: 1112.55
train mean loss: 1154.51
epoch train time: 0:00:01.625435
elapsed time: 0:00:41.750002
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 19:31:17.739101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1095.79
 ---- batch: 020 ----
mean loss: 1076.37
 ---- batch: 030 ----
mean loss: 1092.54
 ---- batch: 040 ----
mean loss: 1067.11
 ---- batch: 050 ----
mean loss: 1084.33
 ---- batch: 060 ----
mean loss: 1028.35
 ---- batch: 070 ----
mean loss: 1062.98
 ---- batch: 080 ----
mean loss: 1059.82
 ---- batch: 090 ----
mean loss: 1025.32
train mean loss: 1063.61
epoch train time: 0:00:01.634005
elapsed time: 0:00:43.384809
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 19:31:19.373944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1014.16
 ---- batch: 020 ----
mean loss: 1029.77
 ---- batch: 030 ----
mean loss: 996.18
 ---- batch: 040 ----
mean loss: 1019.66
 ---- batch: 050 ----
mean loss: 1003.06
 ---- batch: 060 ----
mean loss: 999.47
 ---- batch: 070 ----
mean loss: 985.65
 ---- batch: 080 ----
mean loss: 980.82
 ---- batch: 090 ----
mean loss: 979.12
train mean loss: 998.33
epoch train time: 0:00:01.639425
elapsed time: 0:00:45.025020
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 19:31:21.014160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.36
 ---- batch: 020 ----
mean loss: 972.88
 ---- batch: 030 ----
mean loss: 996.56
 ---- batch: 040 ----
mean loss: 948.53
 ---- batch: 050 ----
mean loss: 961.53
 ---- batch: 060 ----
mean loss: 952.08
 ---- batch: 070 ----
mean loss: 948.84
 ---- batch: 080 ----
mean loss: 944.83
 ---- batch: 090 ----
mean loss: 920.62
train mean loss: 953.71
epoch train time: 0:00:01.594276
elapsed time: 0:00:46.619969
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 19:31:22.609068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.57
 ---- batch: 020 ----
mean loss: 919.32
 ---- batch: 030 ----
mean loss: 933.53
 ---- batch: 040 ----
mean loss: 949.16
 ---- batch: 050 ----
mean loss: 922.18
 ---- batch: 060 ----
mean loss: 908.03
 ---- batch: 070 ----
mean loss: 920.77
 ---- batch: 080 ----
mean loss: 903.94
 ---- batch: 090 ----
mean loss: 929.47
train mean loss: 924.50
epoch train time: 0:00:01.619183
elapsed time: 0:00:48.239868
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 19:31:24.228961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 915.13
 ---- batch: 020 ----
mean loss: 914.44
 ---- batch: 030 ----
mean loss: 898.63
 ---- batch: 040 ----
mean loss: 901.03
 ---- batch: 050 ----
mean loss: 918.24
 ---- batch: 060 ----
mean loss: 915.49
 ---- batch: 070 ----
mean loss: 912.60
 ---- batch: 080 ----
mean loss: 894.93
 ---- batch: 090 ----
mean loss: 898.29
train mean loss: 907.49
epoch train time: 0:00:01.630973
elapsed time: 0:00:49.871501
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 19:31:25.860585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 898.05
 ---- batch: 020 ----
mean loss: 909.25
 ---- batch: 030 ----
mean loss: 901.11
 ---- batch: 040 ----
mean loss: 897.71
 ---- batch: 050 ----
mean loss: 892.92
 ---- batch: 060 ----
mean loss: 901.95
 ---- batch: 070 ----
mean loss: 896.42
 ---- batch: 080 ----
mean loss: 879.28
 ---- batch: 090 ----
mean loss: 899.76
train mean loss: 895.60
epoch train time: 0:00:01.627402
elapsed time: 0:00:51.499531
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 19:31:27.488639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.46
 ---- batch: 020 ----
mean loss: 888.24
 ---- batch: 030 ----
mean loss: 894.94
 ---- batch: 040 ----
mean loss: 908.27
 ---- batch: 050 ----
mean loss: 910.05
 ---- batch: 060 ----
mean loss: 896.26
 ---- batch: 070 ----
mean loss: 909.61
 ---- batch: 080 ----
mean loss: 885.29
 ---- batch: 090 ----
mean loss: 880.68
train mean loss: 890.70
epoch train time: 0:00:01.638980
elapsed time: 0:00:53.139143
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 19:31:29.128109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.76
 ---- batch: 020 ----
mean loss: 893.32
 ---- batch: 030 ----
mean loss: 890.44
 ---- batch: 040 ----
mean loss: 869.17
 ---- batch: 050 ----
mean loss: 895.64
 ---- batch: 060 ----
mean loss: 897.64
 ---- batch: 070 ----
mean loss: 867.59
 ---- batch: 080 ----
mean loss: 886.75
 ---- batch: 090 ----
mean loss: 897.62
train mean loss: 887.53
epoch train time: 0:00:01.652950
elapsed time: 0:00:54.792723
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 19:31:30.781889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.75
 ---- batch: 020 ----
mean loss: 884.09
 ---- batch: 030 ----
mean loss: 881.64
 ---- batch: 040 ----
mean loss: 860.64
 ---- batch: 050 ----
mean loss: 892.28
 ---- batch: 060 ----
mean loss: 883.83
 ---- batch: 070 ----
mean loss: 901.12
 ---- batch: 080 ----
mean loss: 884.53
 ---- batch: 090 ----
mean loss: 886.16
train mean loss: 885.16
epoch train time: 0:00:01.644298
elapsed time: 0:00:56.437703
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 19:31:32.426812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 895.71
 ---- batch: 020 ----
mean loss: 885.71
 ---- batch: 030 ----
mean loss: 884.87
 ---- batch: 040 ----
mean loss: 883.71
 ---- batch: 050 ----
mean loss: 873.49
 ---- batch: 060 ----
mean loss: 866.70
 ---- batch: 070 ----
mean loss: 873.68
 ---- batch: 080 ----
mean loss: 895.24
 ---- batch: 090 ----
mean loss: 885.23
train mean loss: 883.61
epoch train time: 0:00:01.611075
elapsed time: 0:00:58.049428
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 19:31:34.038513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.20
 ---- batch: 020 ----
mean loss: 884.95
 ---- batch: 030 ----
mean loss: 878.18
 ---- batch: 040 ----
mean loss: 895.51
 ---- batch: 050 ----
mean loss: 888.30
 ---- batch: 060 ----
mean loss: 877.47
 ---- batch: 070 ----
mean loss: 863.11
 ---- batch: 080 ----
mean loss: 879.08
 ---- batch: 090 ----
mean loss: 888.50
train mean loss: 882.01
epoch train time: 0:00:01.627221
elapsed time: 0:00:59.677334
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 19:31:35.666432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.33
 ---- batch: 020 ----
mean loss: 858.14
 ---- batch: 030 ----
mean loss: 875.86
 ---- batch: 040 ----
mean loss: 883.47
 ---- batch: 050 ----
mean loss: 863.43
 ---- batch: 060 ----
mean loss: 892.75
 ---- batch: 070 ----
mean loss: 900.31
 ---- batch: 080 ----
mean loss: 900.74
 ---- batch: 090 ----
mean loss: 872.12
train mean loss: 882.46
epoch train time: 0:00:01.609710
elapsed time: 0:01:01.287720
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 19:31:37.276795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.82
 ---- batch: 020 ----
mean loss: 873.17
 ---- batch: 030 ----
mean loss: 898.43
 ---- batch: 040 ----
mean loss: 890.59
 ---- batch: 050 ----
mean loss: 871.58
 ---- batch: 060 ----
mean loss: 863.60
 ---- batch: 070 ----
mean loss: 881.32
 ---- batch: 080 ----
mean loss: 879.70
 ---- batch: 090 ----
mean loss: 875.98
train mean loss: 881.30
epoch train time: 0:00:01.656880
elapsed time: 0:01:02.945258
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 19:31:38.934331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.85
 ---- batch: 020 ----
mean loss: 888.98
 ---- batch: 030 ----
mean loss: 864.30
 ---- batch: 040 ----
mean loss: 887.03
 ---- batch: 050 ----
mean loss: 882.16
 ---- batch: 060 ----
mean loss: 878.83
 ---- batch: 070 ----
mean loss: 862.09
 ---- batch: 080 ----
mean loss: 896.61
 ---- batch: 090 ----
mean loss: 888.25
train mean loss: 880.95
epoch train time: 0:00:01.683066
elapsed time: 0:01:04.629006
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 19:31:40.618105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.43
 ---- batch: 020 ----
mean loss: 894.47
 ---- batch: 030 ----
mean loss: 882.11
 ---- batch: 040 ----
mean loss: 890.02
 ---- batch: 050 ----
mean loss: 873.41
 ---- batch: 060 ----
mean loss: 885.55
 ---- batch: 070 ----
mean loss: 891.77
 ---- batch: 080 ----
mean loss: 869.81
 ---- batch: 090 ----
mean loss: 874.91
train mean loss: 881.82
epoch train time: 0:00:01.647406
elapsed time: 0:01:06.277130
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 19:31:42.266215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 886.63
 ---- batch: 020 ----
mean loss: 872.40
 ---- batch: 030 ----
mean loss: 870.34
 ---- batch: 040 ----
mean loss: 881.29
 ---- batch: 050 ----
mean loss: 899.44
 ---- batch: 060 ----
mean loss: 877.27
 ---- batch: 070 ----
mean loss: 862.77
 ---- batch: 080 ----
mean loss: 894.94
 ---- batch: 090 ----
mean loss: 883.86
train mean loss: 879.57
epoch train time: 0:00:01.620968
elapsed time: 0:01:07.898843
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 19:31:43.887929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 876.64
 ---- batch: 020 ----
mean loss: 892.55
 ---- batch: 030 ----
mean loss: 869.48
 ---- batch: 040 ----
mean loss: 858.13
 ---- batch: 050 ----
mean loss: 870.41
 ---- batch: 060 ----
mean loss: 894.85
 ---- batch: 070 ----
mean loss: 883.63
 ---- batch: 080 ----
mean loss: 877.82
 ---- batch: 090 ----
mean loss: 881.09
train mean loss: 879.33
epoch train time: 0:00:01.646114
elapsed time: 0:01:09.545638
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 19:31:45.534562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.66
 ---- batch: 020 ----
mean loss: 873.35
 ---- batch: 030 ----
mean loss: 868.10
 ---- batch: 040 ----
mean loss: 873.33
 ---- batch: 050 ----
mean loss: 886.61
 ---- batch: 060 ----
mean loss: 880.53
 ---- batch: 070 ----
mean loss: 880.59
 ---- batch: 080 ----
mean loss: 882.63
 ---- batch: 090 ----
mean loss: 890.29
train mean loss: 879.87
epoch train time: 0:00:01.593431
elapsed time: 0:01:11.139567
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 19:31:47.128685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 893.76
 ---- batch: 020 ----
mean loss: 874.66
 ---- batch: 030 ----
mean loss: 880.57
 ---- batch: 040 ----
mean loss: 864.36
 ---- batch: 050 ----
mean loss: 878.52
 ---- batch: 060 ----
mean loss: 870.21
 ---- batch: 070 ----
mean loss: 886.81
 ---- batch: 080 ----
mean loss: 887.00
 ---- batch: 090 ----
mean loss: 878.52
train mean loss: 881.25
epoch train time: 0:00:01.635615
elapsed time: 0:01:12.775911
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 19:31:48.765049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.91
 ---- batch: 020 ----
mean loss: 895.48
 ---- batch: 030 ----
mean loss: 887.99
 ---- batch: 040 ----
mean loss: 878.83
 ---- batch: 050 ----
mean loss: 886.46
 ---- batch: 060 ----
mean loss: 884.94
 ---- batch: 070 ----
mean loss: 876.10
 ---- batch: 080 ----
mean loss: 866.57
 ---- batch: 090 ----
mean loss: 885.56
train mean loss: 878.97
epoch train time: 0:00:01.622356
elapsed time: 0:01:14.399025
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 19:31:50.388124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.13
 ---- batch: 020 ----
mean loss: 872.65
 ---- batch: 030 ----
mean loss: 866.70
 ---- batch: 040 ----
mean loss: 874.14
 ---- batch: 050 ----
mean loss: 869.69
 ---- batch: 060 ----
mean loss: 904.79
 ---- batch: 070 ----
mean loss: 885.71
 ---- batch: 080 ----
mean loss: 882.13
 ---- batch: 090 ----
mean loss: 872.21
train mean loss: 878.47
epoch train time: 0:00:01.646263
elapsed time: 0:01:16.046128
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 19:31:52.035220
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.70
 ---- batch: 020 ----
mean loss: 875.11
 ---- batch: 030 ----
mean loss: 875.76
 ---- batch: 040 ----
mean loss: 874.08
 ---- batch: 050 ----
mean loss: 869.14
 ---- batch: 060 ----
mean loss: 873.06
 ---- batch: 070 ----
mean loss: 881.69
 ---- batch: 080 ----
mean loss: 890.08
 ---- batch: 090 ----
mean loss: 891.38
train mean loss: 878.20
epoch train time: 0:00:01.649280
elapsed time: 0:01:17.696101
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 19:31:53.685196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.32
 ---- batch: 020 ----
mean loss: 872.25
 ---- batch: 030 ----
mean loss: 879.02
 ---- batch: 040 ----
mean loss: 888.70
 ---- batch: 050 ----
mean loss: 877.93
 ---- batch: 060 ----
mean loss: 874.49
 ---- batch: 070 ----
mean loss: 867.13
 ---- batch: 080 ----
mean loss: 894.13
 ---- batch: 090 ----
mean loss: 864.91
train mean loss: 877.85
epoch train time: 0:00:01.679727
elapsed time: 0:01:19.376465
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 19:31:55.365563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.89
 ---- batch: 020 ----
mean loss: 876.65
 ---- batch: 030 ----
mean loss: 864.60
 ---- batch: 040 ----
mean loss: 877.21
 ---- batch: 050 ----
mean loss: 884.14
 ---- batch: 060 ----
mean loss: 887.49
 ---- batch: 070 ----
mean loss: 890.76
 ---- batch: 080 ----
mean loss: 868.61
 ---- batch: 090 ----
mean loss: 863.12
train mean loss: 878.58
epoch train time: 0:00:01.644845
elapsed time: 0:01:21.021975
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 19:31:57.011047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.44
 ---- batch: 020 ----
mean loss: 887.63
 ---- batch: 030 ----
mean loss: 873.07
 ---- batch: 040 ----
mean loss: 878.84
 ---- batch: 050 ----
mean loss: 876.61
 ---- batch: 060 ----
mean loss: 880.37
 ---- batch: 070 ----
mean loss: 878.58
 ---- batch: 080 ----
mean loss: 872.84
 ---- batch: 090 ----
mean loss: 858.55
train mean loss: 877.04
epoch train time: 0:00:01.665416
elapsed time: 0:01:22.688095
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 19:31:58.677200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 879.23
 ---- batch: 020 ----
mean loss: 875.18
 ---- batch: 030 ----
mean loss: 871.54
 ---- batch: 040 ----
mean loss: 881.55
 ---- batch: 050 ----
mean loss: 898.40
 ---- batch: 060 ----
mean loss: 868.71
 ---- batch: 070 ----
mean loss: 894.29
 ---- batch: 080 ----
mean loss: 871.79
 ---- batch: 090 ----
mean loss: 863.04
train mean loss: 878.11
epoch train time: 0:00:01.629627
elapsed time: 0:01:24.318414
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 19:32:00.307536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 885.39
 ---- batch: 020 ----
mean loss: 882.41
 ---- batch: 030 ----
mean loss: 878.24
 ---- batch: 040 ----
mean loss: 871.81
 ---- batch: 050 ----
mean loss: 878.93
 ---- batch: 060 ----
mean loss: 890.10
 ---- batch: 070 ----
mean loss: 865.34
 ---- batch: 080 ----
mean loss: 886.27
 ---- batch: 090 ----
mean loss: 872.65
train mean loss: 878.13
epoch train time: 0:00:01.634234
elapsed time: 0:01:25.953316
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 19:32:01.942400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 882.21
 ---- batch: 020 ----
mean loss: 880.39
 ---- batch: 030 ----
mean loss: 879.35
 ---- batch: 040 ----
mean loss: 873.37
 ---- batch: 050 ----
mean loss: 872.42
 ---- batch: 060 ----
mean loss: 873.14
 ---- batch: 070 ----
mean loss: 871.32
 ---- batch: 080 ----
mean loss: 880.61
 ---- batch: 090 ----
mean loss: 879.45
train mean loss: 879.15
epoch train time: 0:00:01.625297
elapsed time: 0:01:27.579239
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 19:32:03.568324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 870.69
 ---- batch: 020 ----
mean loss: 886.36
 ---- batch: 030 ----
mean loss: 866.30
 ---- batch: 040 ----
mean loss: 882.26
 ---- batch: 050 ----
mean loss: 891.65
 ---- batch: 060 ----
mean loss: 883.73
 ---- batch: 070 ----
mean loss: 886.44
 ---- batch: 080 ----
mean loss: 856.59
 ---- batch: 090 ----
mean loss: 877.52
train mean loss: 877.34
epoch train time: 0:00:01.673884
elapsed time: 0:01:29.253843
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 19:32:05.242962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 868.48
 ---- batch: 020 ----
mean loss: 874.73
 ---- batch: 030 ----
mean loss: 862.54
 ---- batch: 040 ----
mean loss: 880.80
 ---- batch: 050 ----
mean loss: 873.32
 ---- batch: 060 ----
mean loss: 892.80
 ---- batch: 070 ----
mean loss: 881.39
 ---- batch: 080 ----
mean loss: 877.84
 ---- batch: 090 ----
mean loss: 888.57
train mean loss: 878.29
epoch train time: 0:00:01.655474
elapsed time: 0:01:30.910002
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 19:32:06.899096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 873.95
 ---- batch: 020 ----
mean loss: 870.94
 ---- batch: 030 ----
mean loss: 875.42
 ---- batch: 040 ----
mean loss: 882.80
 ---- batch: 050 ----
mean loss: 882.98
 ---- batch: 060 ----
mean loss: 852.03
 ---- batch: 070 ----
mean loss: 856.23
 ---- batch: 080 ----
mean loss: 853.74
 ---- batch: 090 ----
mean loss: 880.25
train mean loss: 867.32
epoch train time: 0:00:01.661271
elapsed time: 0:01:32.571962
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 19:32:08.561051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 826.06
 ---- batch: 020 ----
mean loss: 798.59
 ---- batch: 030 ----
mean loss: 762.35
 ---- batch: 040 ----
mean loss: 718.76
 ---- batch: 050 ----
mean loss: 645.36
 ---- batch: 060 ----
mean loss: 578.94
 ---- batch: 070 ----
mean loss: 537.67
 ---- batch: 080 ----
mean loss: 506.00
 ---- batch: 090 ----
mean loss: 490.69
train mean loss: 640.75
epoch train time: 0:00:01.641727
elapsed time: 0:01:34.214334
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 19:32:10.203417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 461.55
 ---- batch: 020 ----
mean loss: 455.23
 ---- batch: 030 ----
mean loss: 436.76
 ---- batch: 040 ----
mean loss: 433.66
 ---- batch: 050 ----
mean loss: 418.77
 ---- batch: 060 ----
mean loss: 414.68
 ---- batch: 070 ----
mean loss: 417.80
 ---- batch: 080 ----
mean loss: 403.62
 ---- batch: 090 ----
mean loss: 387.41
train mean loss: 422.89
epoch train time: 0:00:01.642544
elapsed time: 0:01:35.857560
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 19:32:11.846665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.91
 ---- batch: 020 ----
mean loss: 380.96
 ---- batch: 030 ----
mean loss: 369.91
 ---- batch: 040 ----
mean loss: 385.60
 ---- batch: 050 ----
mean loss: 368.76
 ---- batch: 060 ----
mean loss: 369.02
 ---- batch: 070 ----
mean loss: 368.99
 ---- batch: 080 ----
mean loss: 360.26
 ---- batch: 090 ----
mean loss: 353.49
train mean loss: 369.95
epoch train time: 0:00:01.609996
elapsed time: 0:01:37.468212
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 19:32:13.457311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.65
 ---- batch: 020 ----
mean loss: 348.27
 ---- batch: 030 ----
mean loss: 345.16
 ---- batch: 040 ----
mean loss: 352.33
 ---- batch: 050 ----
mean loss: 342.83
 ---- batch: 060 ----
mean loss: 344.22
 ---- batch: 070 ----
mean loss: 334.84
 ---- batch: 080 ----
mean loss: 331.54
 ---- batch: 090 ----
mean loss: 330.06
train mean loss: 341.76
epoch train time: 0:00:01.591129
elapsed time: 0:01:39.059978
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 19:32:15.049064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.81
 ---- batch: 020 ----
mean loss: 336.41
 ---- batch: 030 ----
mean loss: 325.51
 ---- batch: 040 ----
mean loss: 333.21
 ---- batch: 050 ----
mean loss: 316.66
 ---- batch: 060 ----
mean loss: 317.60
 ---- batch: 070 ----
mean loss: 317.80
 ---- batch: 080 ----
mean loss: 324.15
 ---- batch: 090 ----
mean loss: 311.10
train mean loss: 322.88
epoch train time: 0:00:01.603598
elapsed time: 0:01:40.664291
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 19:32:16.653413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.37
 ---- batch: 020 ----
mean loss: 307.98
 ---- batch: 030 ----
mean loss: 310.02
 ---- batch: 040 ----
mean loss: 317.84
 ---- batch: 050 ----
mean loss: 313.98
 ---- batch: 060 ----
mean loss: 310.55
 ---- batch: 070 ----
mean loss: 305.44
 ---- batch: 080 ----
mean loss: 311.82
 ---- batch: 090 ----
mean loss: 314.73
train mean loss: 309.92
epoch train time: 0:00:01.589101
elapsed time: 0:01:42.254052
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 19:32:18.243269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.53
 ---- batch: 020 ----
mean loss: 293.16
 ---- batch: 030 ----
mean loss: 298.39
 ---- batch: 040 ----
mean loss: 299.54
 ---- batch: 050 ----
mean loss: 304.35
 ---- batch: 060 ----
mean loss: 301.49
 ---- batch: 070 ----
mean loss: 298.68
 ---- batch: 080 ----
mean loss: 302.55
 ---- batch: 090 ----
mean loss: 295.11
train mean loss: 299.20
epoch train time: 0:00:01.678142
elapsed time: 0:01:43.933064
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 19:32:19.922198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.37
 ---- batch: 020 ----
mean loss: 300.61
 ---- batch: 030 ----
mean loss: 292.52
 ---- batch: 040 ----
mean loss: 298.17
 ---- batch: 050 ----
mean loss: 287.47
 ---- batch: 060 ----
mean loss: 287.38
 ---- batch: 070 ----
mean loss: 286.08
 ---- batch: 080 ----
mean loss: 284.74
 ---- batch: 090 ----
mean loss: 293.25
train mean loss: 290.74
epoch train time: 0:00:01.655655
elapsed time: 0:01:45.589449
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 19:32:21.578360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.41
 ---- batch: 020 ----
mean loss: 270.69
 ---- batch: 030 ----
mean loss: 296.41
 ---- batch: 040 ----
mean loss: 278.61
 ---- batch: 050 ----
mean loss: 275.62
 ---- batch: 060 ----
mean loss: 288.50
 ---- batch: 070 ----
mean loss: 291.23
 ---- batch: 080 ----
mean loss: 282.78
 ---- batch: 090 ----
mean loss: 273.94
train mean loss: 282.79
epoch train time: 0:00:01.652557
elapsed time: 0:01:47.242459
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 19:32:23.231534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.99
 ---- batch: 020 ----
mean loss: 281.09
 ---- batch: 030 ----
mean loss: 273.19
 ---- batch: 040 ----
mean loss: 276.85
 ---- batch: 050 ----
mean loss: 275.31
 ---- batch: 060 ----
mean loss: 272.87
 ---- batch: 070 ----
mean loss: 273.27
 ---- batch: 080 ----
mean loss: 277.42
 ---- batch: 090 ----
mean loss: 280.60
train mean loss: 275.30
epoch train time: 0:00:01.658098
elapsed time: 0:01:48.901244
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 19:32:24.890391
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.98
 ---- batch: 020 ----
mean loss: 272.05
 ---- batch: 030 ----
mean loss: 273.57
 ---- batch: 040 ----
mean loss: 273.17
 ---- batch: 050 ----
mean loss: 266.96
 ---- batch: 060 ----
mean loss: 279.01
 ---- batch: 070 ----
mean loss: 265.46
 ---- batch: 080 ----
mean loss: 263.62
 ---- batch: 090 ----
mean loss: 268.06
train mean loss: 271.66
epoch train time: 0:00:01.668794
elapsed time: 0:01:50.570854
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 19:32:26.560031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.55
 ---- batch: 020 ----
mean loss: 269.05
 ---- batch: 030 ----
mean loss: 268.11
 ---- batch: 040 ----
mean loss: 260.54
 ---- batch: 050 ----
mean loss: 274.64
 ---- batch: 060 ----
mean loss: 263.73
 ---- batch: 070 ----
mean loss: 260.28
 ---- batch: 080 ----
mean loss: 263.15
 ---- batch: 090 ----
mean loss: 265.24
train mean loss: 265.83
epoch train time: 0:00:01.638438
elapsed time: 0:01:52.209968
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 19:32:28.199050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.72
 ---- batch: 020 ----
mean loss: 257.31
 ---- batch: 030 ----
mean loss: 258.99
 ---- batch: 040 ----
mean loss: 252.90
 ---- batch: 050 ----
mean loss: 259.28
 ---- batch: 060 ----
mean loss: 261.32
 ---- batch: 070 ----
mean loss: 268.62
 ---- batch: 080 ----
mean loss: 268.13
 ---- batch: 090 ----
mean loss: 256.28
train mean loss: 261.63
epoch train time: 0:00:01.644401
elapsed time: 0:01:53.855044
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 19:32:29.844130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.04
 ---- batch: 020 ----
mean loss: 248.04
 ---- batch: 030 ----
mean loss: 267.92
 ---- batch: 040 ----
mean loss: 254.66
 ---- batch: 050 ----
mean loss: 260.68
 ---- batch: 060 ----
mean loss: 262.63
 ---- batch: 070 ----
mean loss: 256.57
 ---- batch: 080 ----
mean loss: 260.98
 ---- batch: 090 ----
mean loss: 251.49
train mean loss: 257.72
epoch train time: 0:00:01.629075
elapsed time: 0:01:55.484792
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 19:32:31.473903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.39
 ---- batch: 020 ----
mean loss: 256.55
 ---- batch: 030 ----
mean loss: 260.89
 ---- batch: 040 ----
mean loss: 248.21
 ---- batch: 050 ----
mean loss: 255.32
 ---- batch: 060 ----
mean loss: 248.56
 ---- batch: 070 ----
mean loss: 257.48
 ---- batch: 080 ----
mean loss: 251.75
 ---- batch: 090 ----
mean loss: 251.56
train mean loss: 254.03
epoch train time: 0:00:01.612940
elapsed time: 0:01:57.098507
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 19:32:33.087647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 255.05
 ---- batch: 020 ----
mean loss: 242.02
 ---- batch: 030 ----
mean loss: 252.11
 ---- batch: 040 ----
mean loss: 260.33
 ---- batch: 050 ----
mean loss: 247.12
 ---- batch: 060 ----
mean loss: 248.40
 ---- batch: 070 ----
mean loss: 247.78
 ---- batch: 080 ----
mean loss: 252.66
 ---- batch: 090 ----
mean loss: 252.43
train mean loss: 250.33
epoch train time: 0:00:01.672148
elapsed time: 0:01:58.771384
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 19:32:34.760570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.67
 ---- batch: 020 ----
mean loss: 244.84
 ---- batch: 030 ----
mean loss: 250.63
 ---- batch: 040 ----
mean loss: 237.96
 ---- batch: 050 ----
mean loss: 248.78
 ---- batch: 060 ----
mean loss: 251.10
 ---- batch: 070 ----
mean loss: 237.34
 ---- batch: 080 ----
mean loss: 245.62
 ---- batch: 090 ----
mean loss: 259.02
train mean loss: 246.75
epoch train time: 0:00:01.653392
elapsed time: 0:02:00.425491
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 19:32:36.414595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.06
 ---- batch: 020 ----
mean loss: 240.95
 ---- batch: 030 ----
mean loss: 254.81
 ---- batch: 040 ----
mean loss: 241.80
 ---- batch: 050 ----
mean loss: 241.34
 ---- batch: 060 ----
mean loss: 240.98
 ---- batch: 070 ----
mean loss: 238.04
 ---- batch: 080 ----
mean loss: 256.11
 ---- batch: 090 ----
mean loss: 237.90
train mean loss: 244.20
epoch train time: 0:00:01.659800
elapsed time: 0:02:02.085976
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 19:32:38.075060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.87
 ---- batch: 020 ----
mean loss: 238.51
 ---- batch: 030 ----
mean loss: 243.15
 ---- batch: 040 ----
mean loss: 245.35
 ---- batch: 050 ----
mean loss: 241.42
 ---- batch: 060 ----
mean loss: 247.68
 ---- batch: 070 ----
mean loss: 245.31
 ---- batch: 080 ----
mean loss: 229.64
 ---- batch: 090 ----
mean loss: 242.07
train mean loss: 240.62
epoch train time: 0:00:01.589042
elapsed time: 0:02:03.675756
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 19:32:39.664830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.72
 ---- batch: 020 ----
mean loss: 237.49
 ---- batch: 030 ----
mean loss: 239.30
 ---- batch: 040 ----
mean loss: 242.20
 ---- batch: 050 ----
mean loss: 233.76
 ---- batch: 060 ----
mean loss: 240.18
 ---- batch: 070 ----
mean loss: 246.01
 ---- batch: 080 ----
mean loss: 231.31
 ---- batch: 090 ----
mean loss: 230.28
train mean loss: 238.11
epoch train time: 0:00:01.678549
elapsed time: 0:02:05.354902
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 19:32:41.343979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.09
 ---- batch: 020 ----
mean loss: 234.60
 ---- batch: 030 ----
mean loss: 241.94
 ---- batch: 040 ----
mean loss: 241.68
 ---- batch: 050 ----
mean loss: 230.50
 ---- batch: 060 ----
mean loss: 230.25
 ---- batch: 070 ----
mean loss: 235.36
 ---- batch: 080 ----
mean loss: 231.22
 ---- batch: 090 ----
mean loss: 238.66
train mean loss: 235.72
epoch train time: 0:00:01.620948
elapsed time: 0:02:06.976491
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 19:32:42.965575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.25
 ---- batch: 020 ----
mean loss: 229.33
 ---- batch: 030 ----
mean loss: 229.88
 ---- batch: 040 ----
mean loss: 227.90
 ---- batch: 050 ----
mean loss: 236.50
 ---- batch: 060 ----
mean loss: 239.36
 ---- batch: 070 ----
mean loss: 224.27
 ---- batch: 080 ----
mean loss: 226.49
 ---- batch: 090 ----
mean loss: 227.57
train mean loss: 229.97
epoch train time: 0:00:01.631904
elapsed time: 0:02:08.609043
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 19:32:44.598121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.96
 ---- batch: 020 ----
mean loss: 225.50
 ---- batch: 030 ----
mean loss: 231.38
 ---- batch: 040 ----
mean loss: 230.19
 ---- batch: 050 ----
mean loss: 224.82
 ---- batch: 060 ----
mean loss: 226.08
 ---- batch: 070 ----
mean loss: 225.49
 ---- batch: 080 ----
mean loss: 233.79
 ---- batch: 090 ----
mean loss: 233.00
train mean loss: 229.31
epoch train time: 0:00:01.656624
elapsed time: 0:02:10.266346
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 19:32:46.255437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.20
 ---- batch: 020 ----
mean loss: 236.57
 ---- batch: 030 ----
mean loss: 217.53
 ---- batch: 040 ----
mean loss: 226.05
 ---- batch: 050 ----
mean loss: 227.65
 ---- batch: 060 ----
mean loss: 228.66
 ---- batch: 070 ----
mean loss: 225.17
 ---- batch: 080 ----
mean loss: 227.78
 ---- batch: 090 ----
mean loss: 225.29
train mean loss: 227.15
epoch train time: 0:00:01.645652
elapsed time: 0:02:11.912707
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 19:32:47.901817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.80
 ---- batch: 020 ----
mean loss: 220.68
 ---- batch: 030 ----
mean loss: 219.88
 ---- batch: 040 ----
mean loss: 220.22
 ---- batch: 050 ----
mean loss: 220.85
 ---- batch: 060 ----
mean loss: 221.33
 ---- batch: 070 ----
mean loss: 229.70
 ---- batch: 080 ----
mean loss: 231.28
 ---- batch: 090 ----
mean loss: 227.48
train mean loss: 223.64
epoch train time: 0:00:01.660948
elapsed time: 0:02:13.574446
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 19:32:49.563609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.83
 ---- batch: 020 ----
mean loss: 216.50
 ---- batch: 030 ----
mean loss: 212.19
 ---- batch: 040 ----
mean loss: 216.19
 ---- batch: 050 ----
mean loss: 219.20
 ---- batch: 060 ----
mean loss: 232.00
 ---- batch: 070 ----
mean loss: 217.12
 ---- batch: 080 ----
mean loss: 219.57
 ---- batch: 090 ----
mean loss: 232.84
train mean loss: 220.94
epoch train time: 0:00:01.641179
elapsed time: 0:02:15.216437
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 19:32:51.205584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.46
 ---- batch: 020 ----
mean loss: 207.40
 ---- batch: 030 ----
mean loss: 225.48
 ---- batch: 040 ----
mean loss: 212.19
 ---- batch: 050 ----
mean loss: 221.02
 ---- batch: 060 ----
mean loss: 224.23
 ---- batch: 070 ----
mean loss: 224.74
 ---- batch: 080 ----
mean loss: 225.86
 ---- batch: 090 ----
mean loss: 216.75
train mean loss: 219.56
epoch train time: 0:00:01.650511
elapsed time: 0:02:16.867659
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 19:32:52.856746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.83
 ---- batch: 020 ----
mean loss: 219.91
 ---- batch: 030 ----
mean loss: 212.06
 ---- batch: 040 ----
mean loss: 217.99
 ---- batch: 050 ----
mean loss: 223.96
 ---- batch: 060 ----
mean loss: 212.49
 ---- batch: 070 ----
mean loss: 210.51
 ---- batch: 080 ----
mean loss: 216.74
 ---- batch: 090 ----
mean loss: 214.05
train mean loss: 216.16
epoch train time: 0:00:01.592146
elapsed time: 0:02:18.460460
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 19:32:54.449542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.18
 ---- batch: 020 ----
mean loss: 211.59
 ---- batch: 030 ----
mean loss: 217.34
 ---- batch: 040 ----
mean loss: 221.50
 ---- batch: 050 ----
mean loss: 220.63
 ---- batch: 060 ----
mean loss: 215.62
 ---- batch: 070 ----
mean loss: 209.04
 ---- batch: 080 ----
mean loss: 207.19
 ---- batch: 090 ----
mean loss: 210.53
train mean loss: 214.62
epoch train time: 0:00:01.625021
elapsed time: 0:02:20.086087
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 19:32:56.075206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.87
 ---- batch: 020 ----
mean loss: 212.02
 ---- batch: 030 ----
mean loss: 209.47
 ---- batch: 040 ----
mean loss: 212.69
 ---- batch: 050 ----
mean loss: 215.77
 ---- batch: 060 ----
mean loss: 217.26
 ---- batch: 070 ----
mean loss: 209.59
 ---- batch: 080 ----
mean loss: 225.34
 ---- batch: 090 ----
mean loss: 210.44
train mean loss: 212.51
epoch train time: 0:00:01.671972
elapsed time: 0:02:21.758741
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 19:32:57.747841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.70
 ---- batch: 020 ----
mean loss: 203.90
 ---- batch: 030 ----
mean loss: 219.40
 ---- batch: 040 ----
mean loss: 218.50
 ---- batch: 050 ----
mean loss: 212.82
 ---- batch: 060 ----
mean loss: 212.04
 ---- batch: 070 ----
mean loss: 206.48
 ---- batch: 080 ----
mean loss: 208.33
 ---- batch: 090 ----
mean loss: 202.16
train mean loss: 209.91
epoch train time: 0:00:01.670515
elapsed time: 0:02:23.429881
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 19:32:59.418971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.75
 ---- batch: 020 ----
mean loss: 211.61
 ---- batch: 030 ----
mean loss: 203.98
 ---- batch: 040 ----
mean loss: 204.05
 ---- batch: 050 ----
mean loss: 205.48
 ---- batch: 060 ----
mean loss: 209.59
 ---- batch: 070 ----
mean loss: 204.62
 ---- batch: 080 ----
mean loss: 212.96
 ---- batch: 090 ----
mean loss: 203.76
train mean loss: 207.52
epoch train time: 0:00:01.647088
elapsed time: 0:02:25.077631
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 19:33:01.066737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.99
 ---- batch: 020 ----
mean loss: 206.75
 ---- batch: 030 ----
mean loss: 198.65
 ---- batch: 040 ----
mean loss: 204.52
 ---- batch: 050 ----
mean loss: 201.49
 ---- batch: 060 ----
mean loss: 202.65
 ---- batch: 070 ----
mean loss: 212.96
 ---- batch: 080 ----
mean loss: 205.60
 ---- batch: 090 ----
mean loss: 211.13
train mean loss: 206.19
epoch train time: 0:00:01.636671
elapsed time: 0:02:26.715073
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 19:33:02.704225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.06
 ---- batch: 020 ----
mean loss: 197.08
 ---- batch: 030 ----
mean loss: 197.16
 ---- batch: 040 ----
mean loss: 201.84
 ---- batch: 050 ----
mean loss: 203.85
 ---- batch: 060 ----
mean loss: 201.12
 ---- batch: 070 ----
mean loss: 197.69
 ---- batch: 080 ----
mean loss: 207.87
 ---- batch: 090 ----
mean loss: 209.09
train mean loss: 203.11
epoch train time: 0:00:01.593139
elapsed time: 0:02:28.308922
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 19:33:04.298004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.16
 ---- batch: 020 ----
mean loss: 194.77
 ---- batch: 030 ----
mean loss: 200.02
 ---- batch: 040 ----
mean loss: 210.90
 ---- batch: 050 ----
mean loss: 208.14
 ---- batch: 060 ----
mean loss: 198.20
 ---- batch: 070 ----
mean loss: 200.17
 ---- batch: 080 ----
mean loss: 199.14
 ---- batch: 090 ----
mean loss: 204.47
train mean loss: 201.09
epoch train time: 0:00:01.593100
elapsed time: 0:02:29.902703
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 19:33:05.891790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.97
 ---- batch: 020 ----
mean loss: 199.53
 ---- batch: 030 ----
mean loss: 204.28
 ---- batch: 040 ----
mean loss: 201.39
 ---- batch: 050 ----
mean loss: 197.68
 ---- batch: 060 ----
mean loss: 202.58
 ---- batch: 070 ----
mean loss: 202.23
 ---- batch: 080 ----
mean loss: 200.95
 ---- batch: 090 ----
mean loss: 200.68
train mean loss: 200.65
epoch train time: 0:00:01.622675
elapsed time: 0:02:31.526031
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 19:33:07.515137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.62
 ---- batch: 020 ----
mean loss: 185.47
 ---- batch: 030 ----
mean loss: 199.10
 ---- batch: 040 ----
mean loss: 197.31
 ---- batch: 050 ----
mean loss: 207.98
 ---- batch: 060 ----
mean loss: 199.02
 ---- batch: 070 ----
mean loss: 200.01
 ---- batch: 080 ----
mean loss: 208.09
 ---- batch: 090 ----
mean loss: 196.62
train mean loss: 197.29
epoch train time: 0:00:01.629786
elapsed time: 0:02:33.156617
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 19:33:09.145744
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.80
 ---- batch: 020 ----
mean loss: 191.30
 ---- batch: 030 ----
mean loss: 203.63
 ---- batch: 040 ----
mean loss: 197.14
 ---- batch: 050 ----
mean loss: 197.33
 ---- batch: 060 ----
mean loss: 202.09
 ---- batch: 070 ----
mean loss: 192.14
 ---- batch: 080 ----
mean loss: 197.97
 ---- batch: 090 ----
mean loss: 198.16
train mean loss: 197.02
epoch train time: 0:00:01.662767
elapsed time: 0:02:34.820103
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 19:33:10.809187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.60
 ---- batch: 020 ----
mean loss: 192.51
 ---- batch: 030 ----
mean loss: 190.63
 ---- batch: 040 ----
mean loss: 195.00
 ---- batch: 050 ----
mean loss: 193.92
 ---- batch: 060 ----
mean loss: 198.26
 ---- batch: 070 ----
mean loss: 191.00
 ---- batch: 080 ----
mean loss: 193.80
 ---- batch: 090 ----
mean loss: 197.12
train mean loss: 193.97
epoch train time: 0:00:01.646630
elapsed time: 0:02:36.467367
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 19:33:12.456453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.76
 ---- batch: 020 ----
mean loss: 187.59
 ---- batch: 030 ----
mean loss: 191.51
 ---- batch: 040 ----
mean loss: 197.49
 ---- batch: 050 ----
mean loss: 189.59
 ---- batch: 060 ----
mean loss: 188.02
 ---- batch: 070 ----
mean loss: 195.96
 ---- batch: 080 ----
mean loss: 197.23
 ---- batch: 090 ----
mean loss: 194.66
train mean loss: 192.46
epoch train time: 0:00:01.629461
elapsed time: 0:02:38.097488
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 19:33:14.086586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.47
 ---- batch: 020 ----
mean loss: 191.43
 ---- batch: 030 ----
mean loss: 183.71
 ---- batch: 040 ----
mean loss: 185.10
 ---- batch: 050 ----
mean loss: 193.69
 ---- batch: 060 ----
mean loss: 188.15
 ---- batch: 070 ----
mean loss: 196.86
 ---- batch: 080 ----
mean loss: 193.37
 ---- batch: 090 ----
mean loss: 198.01
train mean loss: 191.68
epoch train time: 0:00:01.619496
elapsed time: 0:02:39.717678
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 19:33:15.706785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.46
 ---- batch: 020 ----
mean loss: 193.28
 ---- batch: 030 ----
mean loss: 188.95
 ---- batch: 040 ----
mean loss: 192.91
 ---- batch: 050 ----
mean loss: 182.97
 ---- batch: 060 ----
mean loss: 192.16
 ---- batch: 070 ----
mean loss: 193.45
 ---- batch: 080 ----
mean loss: 191.63
 ---- batch: 090 ----
mean loss: 187.11
train mean loss: 189.82
epoch train time: 0:00:01.665302
elapsed time: 0:02:41.383634
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 19:33:17.372773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.71
 ---- batch: 020 ----
mean loss: 187.70
 ---- batch: 030 ----
mean loss: 179.47
 ---- batch: 040 ----
mean loss: 198.61
 ---- batch: 050 ----
mean loss: 187.08
 ---- batch: 060 ----
mean loss: 184.96
 ---- batch: 070 ----
mean loss: 196.83
 ---- batch: 080 ----
mean loss: 190.30
 ---- batch: 090 ----
mean loss: 186.31
train mean loss: 187.43
epoch train time: 0:00:01.617319
elapsed time: 0:02:43.001661
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 19:33:18.990765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.34
 ---- batch: 020 ----
mean loss: 179.61
 ---- batch: 030 ----
mean loss: 179.20
 ---- batch: 040 ----
mean loss: 195.70
 ---- batch: 050 ----
mean loss: 188.37
 ---- batch: 060 ----
mean loss: 190.13
 ---- batch: 070 ----
mean loss: 185.14
 ---- batch: 080 ----
mean loss: 186.65
 ---- batch: 090 ----
mean loss: 186.52
train mean loss: 186.45
epoch train time: 0:00:01.628545
elapsed time: 0:02:44.630855
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 19:33:20.619930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.59
 ---- batch: 020 ----
mean loss: 177.89
 ---- batch: 030 ----
mean loss: 179.86
 ---- batch: 040 ----
mean loss: 181.73
 ---- batch: 050 ----
mean loss: 187.87
 ---- batch: 060 ----
mean loss: 183.59
 ---- batch: 070 ----
mean loss: 188.71
 ---- batch: 080 ----
mean loss: 183.99
 ---- batch: 090 ----
mean loss: 191.27
train mean loss: 184.76
epoch train time: 0:00:01.599684
elapsed time: 0:02:46.231196
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 19:33:22.220297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.01
 ---- batch: 020 ----
mean loss: 184.12
 ---- batch: 030 ----
mean loss: 181.68
 ---- batch: 040 ----
mean loss: 180.22
 ---- batch: 050 ----
mean loss: 189.12
 ---- batch: 060 ----
mean loss: 183.44
 ---- batch: 070 ----
mean loss: 184.57
 ---- batch: 080 ----
mean loss: 182.33
 ---- batch: 090 ----
mean loss: 185.20
train mean loss: 183.67
epoch train time: 0:00:01.612320
elapsed time: 0:02:47.844259
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 19:33:23.833342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.48
 ---- batch: 020 ----
mean loss: 180.67
 ---- batch: 030 ----
mean loss: 174.93
 ---- batch: 040 ----
mean loss: 181.08
 ---- batch: 050 ----
mean loss: 187.73
 ---- batch: 060 ----
mean loss: 183.04
 ---- batch: 070 ----
mean loss: 175.52
 ---- batch: 080 ----
mean loss: 192.77
 ---- batch: 090 ----
mean loss: 178.76
train mean loss: 181.71
epoch train time: 0:00:01.651374
elapsed time: 0:02:49.496235
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 19:33:25.485345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.24
 ---- batch: 020 ----
mean loss: 174.38
 ---- batch: 030 ----
mean loss: 182.29
 ---- batch: 040 ----
mean loss: 177.44
 ---- batch: 050 ----
mean loss: 184.79
 ---- batch: 060 ----
mean loss: 182.92
 ---- batch: 070 ----
mean loss: 177.54
 ---- batch: 080 ----
mean loss: 183.35
 ---- batch: 090 ----
mean loss: 185.93
train mean loss: 180.67
epoch train time: 0:00:01.660713
elapsed time: 0:02:51.157601
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 19:33:27.146704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.77
 ---- batch: 020 ----
mean loss: 179.95
 ---- batch: 030 ----
mean loss: 175.69
 ---- batch: 040 ----
mean loss: 176.68
 ---- batch: 050 ----
mean loss: 185.79
 ---- batch: 060 ----
mean loss: 182.74
 ---- batch: 070 ----
mean loss: 169.84
 ---- batch: 080 ----
mean loss: 180.88
 ---- batch: 090 ----
mean loss: 188.84
train mean loss: 179.53
epoch train time: 0:00:01.651178
elapsed time: 0:02:52.809535
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 19:33:28.798611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.65
 ---- batch: 020 ----
mean loss: 183.99
 ---- batch: 030 ----
mean loss: 173.62
 ---- batch: 040 ----
mean loss: 180.69
 ---- batch: 050 ----
mean loss: 175.85
 ---- batch: 060 ----
mean loss: 172.65
 ---- batch: 070 ----
mean loss: 165.66
 ---- batch: 080 ----
mean loss: 181.92
 ---- batch: 090 ----
mean loss: 182.14
train mean loss: 177.62
epoch train time: 0:00:01.638542
elapsed time: 0:02:54.448707
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 19:33:30.437634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.07
 ---- batch: 020 ----
mean loss: 169.23
 ---- batch: 030 ----
mean loss: 185.14
 ---- batch: 040 ----
mean loss: 184.23
 ---- batch: 050 ----
mean loss: 170.62
 ---- batch: 060 ----
mean loss: 172.86
 ---- batch: 070 ----
mean loss: 172.25
 ---- batch: 080 ----
mean loss: 180.30
 ---- batch: 090 ----
mean loss: 171.22
train mean loss: 175.91
epoch train time: 0:00:01.649656
elapsed time: 0:02:56.098954
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 19:33:32.088064
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.44
 ---- batch: 020 ----
mean loss: 167.84
 ---- batch: 030 ----
mean loss: 175.64
 ---- batch: 040 ----
mean loss: 173.90
 ---- batch: 050 ----
mean loss: 164.67
 ---- batch: 060 ----
mean loss: 181.00
 ---- batch: 070 ----
mean loss: 176.60
 ---- batch: 080 ----
mean loss: 178.01
 ---- batch: 090 ----
mean loss: 183.05
train mean loss: 174.86
epoch train time: 0:00:01.666813
elapsed time: 0:02:57.766476
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 19:33:33.755574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.77
 ---- batch: 020 ----
mean loss: 172.85
 ---- batch: 030 ----
mean loss: 166.03
 ---- batch: 040 ----
mean loss: 176.01
 ---- batch: 050 ----
mean loss: 167.65
 ---- batch: 060 ----
mean loss: 179.87
 ---- batch: 070 ----
mean loss: 173.64
 ---- batch: 080 ----
mean loss: 175.24
 ---- batch: 090 ----
mean loss: 171.86
train mean loss: 173.48
epoch train time: 0:00:01.640411
elapsed time: 0:02:59.407567
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 19:33:35.396653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.93
 ---- batch: 020 ----
mean loss: 171.17
 ---- batch: 030 ----
mean loss: 169.97
 ---- batch: 040 ----
mean loss: 173.26
 ---- batch: 050 ----
mean loss: 176.36
 ---- batch: 060 ----
mean loss: 172.76
 ---- batch: 070 ----
mean loss: 177.21
 ---- batch: 080 ----
mean loss: 174.72
 ---- batch: 090 ----
mean loss: 173.03
train mean loss: 172.37
epoch train time: 0:00:01.625845
elapsed time: 0:03:01.034126
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 19:33:37.023236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.95
 ---- batch: 020 ----
mean loss: 168.56
 ---- batch: 030 ----
mean loss: 163.41
 ---- batch: 040 ----
mean loss: 166.20
 ---- batch: 050 ----
mean loss: 171.69
 ---- batch: 060 ----
mean loss: 181.43
 ---- batch: 070 ----
mean loss: 167.72
 ---- batch: 080 ----
mean loss: 177.25
 ---- batch: 090 ----
mean loss: 173.59
train mean loss: 171.07
epoch train time: 0:00:01.603613
elapsed time: 0:03:02.638478
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 19:33:38.627586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.57
 ---- batch: 020 ----
mean loss: 167.51
 ---- batch: 030 ----
mean loss: 170.85
 ---- batch: 040 ----
mean loss: 166.22
 ---- batch: 050 ----
mean loss: 167.74
 ---- batch: 060 ----
mean loss: 173.08
 ---- batch: 070 ----
mean loss: 169.56
 ---- batch: 080 ----
mean loss: 173.31
 ---- batch: 090 ----
mean loss: 168.26
train mean loss: 169.76
epoch train time: 0:00:01.628091
elapsed time: 0:03:04.267197
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 19:33:40.256299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.43
 ---- batch: 020 ----
mean loss: 168.26
 ---- batch: 030 ----
mean loss: 167.24
 ---- batch: 040 ----
mean loss: 167.00
 ---- batch: 050 ----
mean loss: 163.46
 ---- batch: 060 ----
mean loss: 174.42
 ---- batch: 070 ----
mean loss: 167.07
 ---- batch: 080 ----
mean loss: 178.69
 ---- batch: 090 ----
mean loss: 168.02
train mean loss: 169.24
epoch train time: 0:00:01.627395
elapsed time: 0:03:05.895281
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 19:33:41.884389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.12
 ---- batch: 020 ----
mean loss: 167.02
 ---- batch: 030 ----
mean loss: 169.54
 ---- batch: 040 ----
mean loss: 169.01
 ---- batch: 050 ----
mean loss: 168.49
 ---- batch: 060 ----
mean loss: 174.18
 ---- batch: 070 ----
mean loss: 172.33
 ---- batch: 080 ----
mean loss: 173.01
 ---- batch: 090 ----
mean loss: 162.92
train mean loss: 167.85
epoch train time: 0:00:01.628461
elapsed time: 0:03:07.524381
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 19:33:43.513454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.43
 ---- batch: 020 ----
mean loss: 154.81
 ---- batch: 030 ----
mean loss: 173.37
 ---- batch: 040 ----
mean loss: 161.23
 ---- batch: 050 ----
mean loss: 162.22
 ---- batch: 060 ----
mean loss: 177.54
 ---- batch: 070 ----
mean loss: 169.55
 ---- batch: 080 ----
mean loss: 166.27
 ---- batch: 090 ----
mean loss: 173.42
train mean loss: 166.95
epoch train time: 0:00:01.628796
elapsed time: 0:03:09.153791
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 19:33:45.142875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.98
 ---- batch: 020 ----
mean loss: 164.37
 ---- batch: 030 ----
mean loss: 163.74
 ---- batch: 040 ----
mean loss: 165.69
 ---- batch: 050 ----
mean loss: 167.41
 ---- batch: 060 ----
mean loss: 170.68
 ---- batch: 070 ----
mean loss: 163.11
 ---- batch: 080 ----
mean loss: 168.11
 ---- batch: 090 ----
mean loss: 165.32
train mean loss: 165.60
epoch train time: 0:00:01.604861
elapsed time: 0:03:10.759454
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 19:33:46.748553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.54
 ---- batch: 020 ----
mean loss: 155.22
 ---- batch: 030 ----
mean loss: 161.26
 ---- batch: 040 ----
mean loss: 166.07
 ---- batch: 050 ----
mean loss: 165.34
 ---- batch: 060 ----
mean loss: 169.63
 ---- batch: 070 ----
mean loss: 172.59
 ---- batch: 080 ----
mean loss: 168.85
 ---- batch: 090 ----
mean loss: 161.65
train mean loss: 164.75
epoch train time: 0:00:01.599669
elapsed time: 0:03:12.359809
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 19:33:48.348893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.70
 ---- batch: 020 ----
mean loss: 160.97
 ---- batch: 030 ----
mean loss: 165.37
 ---- batch: 040 ----
mean loss: 163.50
 ---- batch: 050 ----
mean loss: 159.66
 ---- batch: 060 ----
mean loss: 159.27
 ---- batch: 070 ----
mean loss: 169.35
 ---- batch: 080 ----
mean loss: 167.67
 ---- batch: 090 ----
mean loss: 167.75
train mean loss: 163.96
epoch train time: 0:00:01.608102
elapsed time: 0:03:13.968557
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 19:33:49.957693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.91
 ---- batch: 020 ----
mean loss: 160.66
 ---- batch: 030 ----
mean loss: 159.15
 ---- batch: 040 ----
mean loss: 161.96
 ---- batch: 050 ----
mean loss: 163.54
 ---- batch: 060 ----
mean loss: 167.23
 ---- batch: 070 ----
mean loss: 162.84
 ---- batch: 080 ----
mean loss: 163.41
 ---- batch: 090 ----
mean loss: 157.37
train mean loss: 161.74
epoch train time: 0:00:01.608030
elapsed time: 0:03:15.577300
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 19:33:51.566405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.84
 ---- batch: 020 ----
mean loss: 149.81
 ---- batch: 030 ----
mean loss: 154.79
 ---- batch: 040 ----
mean loss: 154.37
 ---- batch: 050 ----
mean loss: 165.11
 ---- batch: 060 ----
mean loss: 168.82
 ---- batch: 070 ----
mean loss: 157.78
 ---- batch: 080 ----
mean loss: 168.47
 ---- batch: 090 ----
mean loss: 162.55
train mean loss: 161.49
epoch train time: 0:00:01.584947
elapsed time: 0:03:17.162940
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 19:33:53.152039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.44
 ---- batch: 020 ----
mean loss: 156.83
 ---- batch: 030 ----
mean loss: 156.80
 ---- batch: 040 ----
mean loss: 157.56
 ---- batch: 050 ----
mean loss: 165.05
 ---- batch: 060 ----
mean loss: 157.94
 ---- batch: 070 ----
mean loss: 161.31
 ---- batch: 080 ----
mean loss: 164.69
 ---- batch: 090 ----
mean loss: 167.03
train mean loss: 161.06
epoch train time: 0:00:01.625870
elapsed time: 0:03:18.789470
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 19:33:54.778566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.49
 ---- batch: 020 ----
mean loss: 155.15
 ---- batch: 030 ----
mean loss: 158.08
 ---- batch: 040 ----
mean loss: 157.85
 ---- batch: 050 ----
mean loss: 170.17
 ---- batch: 060 ----
mean loss: 166.54
 ---- batch: 070 ----
mean loss: 158.40
 ---- batch: 080 ----
mean loss: 155.43
 ---- batch: 090 ----
mean loss: 158.93
train mean loss: 160.07
epoch train time: 0:00:01.611890
elapsed time: 0:03:20.402009
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 19:33:56.391186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.15
 ---- batch: 020 ----
mean loss: 157.66
 ---- batch: 030 ----
mean loss: 156.33
 ---- batch: 040 ----
mean loss: 154.78
 ---- batch: 050 ----
mean loss: 156.34
 ---- batch: 060 ----
mean loss: 162.55
 ---- batch: 070 ----
mean loss: 159.94
 ---- batch: 080 ----
mean loss: 156.63
 ---- batch: 090 ----
mean loss: 165.98
train mean loss: 158.25
epoch train time: 0:00:01.620378
elapsed time: 0:03:22.023142
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 19:33:58.012244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.55
 ---- batch: 020 ----
mean loss: 156.28
 ---- batch: 030 ----
mean loss: 149.60
 ---- batch: 040 ----
mean loss: 157.41
 ---- batch: 050 ----
mean loss: 156.63
 ---- batch: 060 ----
mean loss: 160.24
 ---- batch: 070 ----
mean loss: 163.28
 ---- batch: 080 ----
mean loss: 147.73
 ---- batch: 090 ----
mean loss: 161.16
train mean loss: 157.18
epoch train time: 0:00:01.578705
elapsed time: 0:03:23.602531
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 19:33:59.591621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.04
 ---- batch: 020 ----
mean loss: 160.30
 ---- batch: 030 ----
mean loss: 154.15
 ---- batch: 040 ----
mean loss: 162.00
 ---- batch: 050 ----
mean loss: 158.38
 ---- batch: 060 ----
mean loss: 163.71
 ---- batch: 070 ----
mean loss: 152.10
 ---- batch: 080 ----
mean loss: 154.11
 ---- batch: 090 ----
mean loss: 150.69
train mean loss: 157.08
epoch train time: 0:00:01.583947
elapsed time: 0:03:25.187183
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 19:34:01.176275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.48
 ---- batch: 020 ----
mean loss: 150.23
 ---- batch: 030 ----
mean loss: 158.33
 ---- batch: 040 ----
mean loss: 155.91
 ---- batch: 050 ----
mean loss: 152.67
 ---- batch: 060 ----
mean loss: 158.65
 ---- batch: 070 ----
mean loss: 159.44
 ---- batch: 080 ----
mean loss: 157.96
 ---- batch: 090 ----
mean loss: 156.52
train mean loss: 156.22
epoch train time: 0:00:01.613493
elapsed time: 0:03:26.801354
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 19:34:02.790460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.82
 ---- batch: 020 ----
mean loss: 156.20
 ---- batch: 030 ----
mean loss: 150.00
 ---- batch: 040 ----
mean loss: 156.27
 ---- batch: 050 ----
mean loss: 156.12
 ---- batch: 060 ----
mean loss: 157.10
 ---- batch: 070 ----
mean loss: 155.41
 ---- batch: 080 ----
mean loss: 157.11
 ---- batch: 090 ----
mean loss: 159.29
train mean loss: 155.00
epoch train time: 0:00:01.625413
elapsed time: 0:03:28.427574
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 19:34:04.416499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.90
 ---- batch: 020 ----
mean loss: 150.94
 ---- batch: 030 ----
mean loss: 153.31
 ---- batch: 040 ----
mean loss: 157.27
 ---- batch: 050 ----
mean loss: 158.09
 ---- batch: 060 ----
mean loss: 150.71
 ---- batch: 070 ----
mean loss: 159.95
 ---- batch: 080 ----
mean loss: 160.78
 ---- batch: 090 ----
mean loss: 157.36
train mean loss: 154.35
epoch train time: 0:00:01.625419
elapsed time: 0:03:30.053475
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 19:34:06.042564
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.06
 ---- batch: 020 ----
mean loss: 160.61
 ---- batch: 030 ----
mean loss: 148.76
 ---- batch: 040 ----
mean loss: 150.71
 ---- batch: 050 ----
mean loss: 149.43
 ---- batch: 060 ----
mean loss: 153.48
 ---- batch: 070 ----
mean loss: 155.90
 ---- batch: 080 ----
mean loss: 151.51
 ---- batch: 090 ----
mean loss: 152.09
train mean loss: 153.26
epoch train time: 0:00:01.641097
elapsed time: 0:03:31.695289
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 19:34:07.684382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.32
 ---- batch: 020 ----
mean loss: 153.51
 ---- batch: 030 ----
mean loss: 150.52
 ---- batch: 040 ----
mean loss: 149.74
 ---- batch: 050 ----
mean loss: 146.77
 ---- batch: 060 ----
mean loss: 156.83
 ---- batch: 070 ----
mean loss: 153.26
 ---- batch: 080 ----
mean loss: 156.80
 ---- batch: 090 ----
mean loss: 152.98
train mean loss: 152.93
epoch train time: 0:00:01.632738
elapsed time: 0:03:33.328724
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 19:34:09.317838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.40
 ---- batch: 020 ----
mean loss: 150.10
 ---- batch: 030 ----
mean loss: 149.74
 ---- batch: 040 ----
mean loss: 147.36
 ---- batch: 050 ----
mean loss: 155.34
 ---- batch: 060 ----
mean loss: 156.71
 ---- batch: 070 ----
mean loss: 153.17
 ---- batch: 080 ----
mean loss: 152.01
 ---- batch: 090 ----
mean loss: 151.39
train mean loss: 151.79
epoch train time: 0:00:01.624005
elapsed time: 0:03:34.953397
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 19:34:10.942490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.22
 ---- batch: 020 ----
mean loss: 146.48
 ---- batch: 030 ----
mean loss: 151.77
 ---- batch: 040 ----
mean loss: 153.84
 ---- batch: 050 ----
mean loss: 153.15
 ---- batch: 060 ----
mean loss: 152.79
 ---- batch: 070 ----
mean loss: 154.17
 ---- batch: 080 ----
mean loss: 146.46
 ---- batch: 090 ----
mean loss: 152.05
train mean loss: 150.19
epoch train time: 0:00:01.625993
elapsed time: 0:03:36.580015
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 19:34:12.569092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.02
 ---- batch: 020 ----
mean loss: 150.13
 ---- batch: 030 ----
mean loss: 148.63
 ---- batch: 040 ----
mean loss: 147.23
 ---- batch: 050 ----
mean loss: 150.07
 ---- batch: 060 ----
mean loss: 152.84
 ---- batch: 070 ----
mean loss: 156.49
 ---- batch: 080 ----
mean loss: 148.03
 ---- batch: 090 ----
mean loss: 150.41
train mean loss: 150.02
epoch train time: 0:00:01.625425
elapsed time: 0:03:38.206062
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 19:34:14.195142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.75
 ---- batch: 020 ----
mean loss: 146.07
 ---- batch: 030 ----
mean loss: 143.37
 ---- batch: 040 ----
mean loss: 151.42
 ---- batch: 050 ----
mean loss: 153.66
 ---- batch: 060 ----
mean loss: 151.65
 ---- batch: 070 ----
mean loss: 143.69
 ---- batch: 080 ----
mean loss: 152.04
 ---- batch: 090 ----
mean loss: 147.41
train mean loss: 149.07
epoch train time: 0:00:01.615420
elapsed time: 0:03:39.822203
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 19:34:15.811306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.61
 ---- batch: 020 ----
mean loss: 145.27
 ---- batch: 030 ----
mean loss: 145.53
 ---- batch: 040 ----
mean loss: 149.97
 ---- batch: 050 ----
mean loss: 146.10
 ---- batch: 060 ----
mean loss: 151.78
 ---- batch: 070 ----
mean loss: 155.92
 ---- batch: 080 ----
mean loss: 142.19
 ---- batch: 090 ----
mean loss: 152.34
train mean loss: 148.21
epoch train time: 0:00:01.673810
elapsed time: 0:03:41.496763
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 19:34:17.485864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.70
 ---- batch: 020 ----
mean loss: 142.09
 ---- batch: 030 ----
mean loss: 145.81
 ---- batch: 040 ----
mean loss: 137.13
 ---- batch: 050 ----
mean loss: 149.81
 ---- batch: 060 ----
mean loss: 147.42
 ---- batch: 070 ----
mean loss: 155.29
 ---- batch: 080 ----
mean loss: 152.45
 ---- batch: 090 ----
mean loss: 155.14
train mean loss: 147.34
epoch train time: 0:00:01.659759
elapsed time: 0:03:43.157234
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 19:34:19.146304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.47
 ---- batch: 020 ----
mean loss: 138.16
 ---- batch: 030 ----
mean loss: 145.97
 ---- batch: 040 ----
mean loss: 147.71
 ---- batch: 050 ----
mean loss: 149.72
 ---- batch: 060 ----
mean loss: 146.04
 ---- batch: 070 ----
mean loss: 148.35
 ---- batch: 080 ----
mean loss: 152.42
 ---- batch: 090 ----
mean loss: 157.49
train mean loss: 146.97
epoch train time: 0:00:01.614499
elapsed time: 0:03:44.772414
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 19:34:20.761501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.71
 ---- batch: 020 ----
mean loss: 139.54
 ---- batch: 030 ----
mean loss: 144.67
 ---- batch: 040 ----
mean loss: 144.40
 ---- batch: 050 ----
mean loss: 145.73
 ---- batch: 060 ----
mean loss: 141.71
 ---- batch: 070 ----
mean loss: 147.30
 ---- batch: 080 ----
mean loss: 152.20
 ---- batch: 090 ----
mean loss: 155.27
train mean loss: 146.20
epoch train time: 0:00:01.622423
elapsed time: 0:03:46.395502
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 19:34:22.384584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.04
 ---- batch: 020 ----
mean loss: 143.74
 ---- batch: 030 ----
mean loss: 144.91
 ---- batch: 040 ----
mean loss: 146.47
 ---- batch: 050 ----
mean loss: 150.82
 ---- batch: 060 ----
mean loss: 147.13
 ---- batch: 070 ----
mean loss: 143.46
 ---- batch: 080 ----
mean loss: 144.81
 ---- batch: 090 ----
mean loss: 142.26
train mean loss: 144.91
epoch train time: 0:00:01.655766
elapsed time: 0:03:48.051913
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 19:34:24.040982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.84
 ---- batch: 020 ----
mean loss: 144.31
 ---- batch: 030 ----
mean loss: 140.86
 ---- batch: 040 ----
mean loss: 144.42
 ---- batch: 050 ----
mean loss: 150.27
 ---- batch: 060 ----
mean loss: 146.28
 ---- batch: 070 ----
mean loss: 145.63
 ---- batch: 080 ----
mean loss: 146.17
 ---- batch: 090 ----
mean loss: 146.92
train mean loss: 145.17
epoch train time: 0:00:01.644987
elapsed time: 0:03:49.697584
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 19:34:25.686669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.80
 ---- batch: 020 ----
mean loss: 140.17
 ---- batch: 030 ----
mean loss: 140.68
 ---- batch: 040 ----
mean loss: 145.60
 ---- batch: 050 ----
mean loss: 143.59
 ---- batch: 060 ----
mean loss: 140.25
 ---- batch: 070 ----
mean loss: 143.24
 ---- batch: 080 ----
mean loss: 152.79
 ---- batch: 090 ----
mean loss: 143.17
train mean loss: 143.53
epoch train time: 0:00:01.603630
elapsed time: 0:03:51.301901
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 19:34:27.291007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.56
 ---- batch: 020 ----
mean loss: 140.57
 ---- batch: 030 ----
mean loss: 141.06
 ---- batch: 040 ----
mean loss: 143.76
 ---- batch: 050 ----
mean loss: 145.39
 ---- batch: 060 ----
mean loss: 143.12
 ---- batch: 070 ----
mean loss: 147.75
 ---- batch: 080 ----
mean loss: 148.09
 ---- batch: 090 ----
mean loss: 146.26
train mean loss: 143.43
epoch train time: 0:00:01.629202
elapsed time: 0:03:52.931792
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 19:34:28.920878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.98
 ---- batch: 020 ----
mean loss: 141.03
 ---- batch: 030 ----
mean loss: 143.12
 ---- batch: 040 ----
mean loss: 141.44
 ---- batch: 050 ----
mean loss: 148.41
 ---- batch: 060 ----
mean loss: 143.36
 ---- batch: 070 ----
mean loss: 141.04
 ---- batch: 080 ----
mean loss: 142.52
 ---- batch: 090 ----
mean loss: 141.07
train mean loss: 142.97
epoch train time: 0:00:01.601372
elapsed time: 0:03:54.533807
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 19:34:30.522890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.27
 ---- batch: 020 ----
mean loss: 132.31
 ---- batch: 030 ----
mean loss: 139.04
 ---- batch: 040 ----
mean loss: 143.81
 ---- batch: 050 ----
mean loss: 147.07
 ---- batch: 060 ----
mean loss: 140.15
 ---- batch: 070 ----
mean loss: 143.91
 ---- batch: 080 ----
mean loss: 153.07
 ---- batch: 090 ----
mean loss: 145.03
train mean loss: 141.86
epoch train time: 0:00:01.602781
elapsed time: 0:03:56.137232
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 19:34:32.126330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.68
 ---- batch: 020 ----
mean loss: 141.25
 ---- batch: 030 ----
mean loss: 135.96
 ---- batch: 040 ----
mean loss: 141.46
 ---- batch: 050 ----
mean loss: 145.13
 ---- batch: 060 ----
mean loss: 150.38
 ---- batch: 070 ----
mean loss: 142.69
 ---- batch: 080 ----
mean loss: 137.99
 ---- batch: 090 ----
mean loss: 139.19
train mean loss: 141.39
epoch train time: 0:00:01.646954
elapsed time: 0:03:57.784941
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 19:34:33.774042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.90
 ---- batch: 020 ----
mean loss: 140.86
 ---- batch: 030 ----
mean loss: 136.66
 ---- batch: 040 ----
mean loss: 133.34
 ---- batch: 050 ----
mean loss: 142.05
 ---- batch: 060 ----
mean loss: 139.81
 ---- batch: 070 ----
mean loss: 135.70
 ---- batch: 080 ----
mean loss: 138.64
 ---- batch: 090 ----
mean loss: 153.67
train mean loss: 140.15
epoch train time: 0:00:01.629454
elapsed time: 0:03:59.415088
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 19:34:35.404177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.72
 ---- batch: 020 ----
mean loss: 136.06
 ---- batch: 030 ----
mean loss: 138.27
 ---- batch: 040 ----
mean loss: 139.36
 ---- batch: 050 ----
mean loss: 136.09
 ---- batch: 060 ----
mean loss: 141.14
 ---- batch: 070 ----
mean loss: 141.88
 ---- batch: 080 ----
mean loss: 142.05
 ---- batch: 090 ----
mean loss: 145.87
train mean loss: 139.64
epoch train time: 0:00:01.602575
elapsed time: 0:04:01.018519
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 19:34:37.007432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.12
 ---- batch: 020 ----
mean loss: 137.29
 ---- batch: 030 ----
mean loss: 135.01
 ---- batch: 040 ----
mean loss: 143.12
 ---- batch: 050 ----
mean loss: 142.94
 ---- batch: 060 ----
mean loss: 137.45
 ---- batch: 070 ----
mean loss: 142.33
 ---- batch: 080 ----
mean loss: 138.08
 ---- batch: 090 ----
mean loss: 146.82
train mean loss: 139.60
epoch train time: 0:00:01.614273
elapsed time: 0:04:02.633278
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 19:34:38.622452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.63
 ---- batch: 020 ----
mean loss: 130.64
 ---- batch: 030 ----
mean loss: 134.76
 ---- batch: 040 ----
mean loss: 134.94
 ---- batch: 050 ----
mean loss: 135.08
 ---- batch: 060 ----
mean loss: 135.84
 ---- batch: 070 ----
mean loss: 142.81
 ---- batch: 080 ----
mean loss: 145.94
 ---- batch: 090 ----
mean loss: 145.73
train mean loss: 138.11
epoch train time: 0:00:01.660680
elapsed time: 0:04:04.294707
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 19:34:40.283812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.55
 ---- batch: 020 ----
mean loss: 135.01
 ---- batch: 030 ----
mean loss: 130.63
 ---- batch: 040 ----
mean loss: 135.43
 ---- batch: 050 ----
mean loss: 136.58
 ---- batch: 060 ----
mean loss: 134.02
 ---- batch: 070 ----
mean loss: 143.62
 ---- batch: 080 ----
mean loss: 148.24
 ---- batch: 090 ----
mean loss: 143.43
train mean loss: 138.28
epoch train time: 0:00:01.617809
elapsed time: 0:04:05.913225
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 19:34:41.902303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.63
 ---- batch: 020 ----
mean loss: 135.46
 ---- batch: 030 ----
mean loss: 133.81
 ---- batch: 040 ----
mean loss: 132.41
 ---- batch: 050 ----
mean loss: 132.76
 ---- batch: 060 ----
mean loss: 137.59
 ---- batch: 070 ----
mean loss: 143.05
 ---- batch: 080 ----
mean loss: 135.48
 ---- batch: 090 ----
mean loss: 144.53
train mean loss: 136.77
epoch train time: 0:00:01.615469
elapsed time: 0:04:07.529354
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 19:34:43.518449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.27
 ---- batch: 020 ----
mean loss: 129.82
 ---- batch: 030 ----
mean loss: 139.50
 ---- batch: 040 ----
mean loss: 133.14
 ---- batch: 050 ----
mean loss: 136.91
 ---- batch: 060 ----
mean loss: 130.61
 ---- batch: 070 ----
mean loss: 138.52
 ---- batch: 080 ----
mean loss: 144.48
 ---- batch: 090 ----
mean loss: 135.78
train mean loss: 136.84
epoch train time: 0:00:01.612844
elapsed time: 0:04:09.142874
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 19:34:45.131962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.73
 ---- batch: 020 ----
mean loss: 127.89
 ---- batch: 030 ----
mean loss: 132.53
 ---- batch: 040 ----
mean loss: 132.01
 ---- batch: 050 ----
mean loss: 130.40
 ---- batch: 060 ----
mean loss: 139.44
 ---- batch: 070 ----
mean loss: 141.94
 ---- batch: 080 ----
mean loss: 139.44
 ---- batch: 090 ----
mean loss: 137.77
train mean loss: 136.08
epoch train time: 0:00:01.651701
elapsed time: 0:04:10.795271
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 19:34:46.784347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.62
 ---- batch: 020 ----
mean loss: 129.01
 ---- batch: 030 ----
mean loss: 130.06
 ---- batch: 040 ----
mean loss: 134.35
 ---- batch: 050 ----
mean loss: 130.03
 ---- batch: 060 ----
mean loss: 136.28
 ---- batch: 070 ----
mean loss: 143.11
 ---- batch: 080 ----
mean loss: 139.31
 ---- batch: 090 ----
mean loss: 143.69
train mean loss: 136.11
epoch train time: 0:00:01.649829
elapsed time: 0:04:12.445787
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 19:34:48.434962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.71
 ---- batch: 020 ----
mean loss: 135.33
 ---- batch: 030 ----
mean loss: 133.11
 ---- batch: 040 ----
mean loss: 134.32
 ---- batch: 050 ----
mean loss: 132.61
 ---- batch: 060 ----
mean loss: 137.98
 ---- batch: 070 ----
mean loss: 138.48
 ---- batch: 080 ----
mean loss: 137.44
 ---- batch: 090 ----
mean loss: 133.75
train mean loss: 135.01
epoch train time: 0:00:01.648904
elapsed time: 0:04:14.095419
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 19:34:50.084500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.90
 ---- batch: 020 ----
mean loss: 124.41
 ---- batch: 030 ----
mean loss: 134.79
 ---- batch: 040 ----
mean loss: 134.33
 ---- batch: 050 ----
mean loss: 130.36
 ---- batch: 060 ----
mean loss: 133.98
 ---- batch: 070 ----
mean loss: 132.50
 ---- batch: 080 ----
mean loss: 138.04
 ---- batch: 090 ----
mean loss: 143.68
train mean loss: 134.34
epoch train time: 0:00:01.683311
elapsed time: 0:04:15.779456
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 19:34:51.768652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.47
 ---- batch: 020 ----
mean loss: 127.17
 ---- batch: 030 ----
mean loss: 129.62
 ---- batch: 040 ----
mean loss: 132.08
 ---- batch: 050 ----
mean loss: 132.48
 ---- batch: 060 ----
mean loss: 137.35
 ---- batch: 070 ----
mean loss: 136.47
 ---- batch: 080 ----
mean loss: 139.22
 ---- batch: 090 ----
mean loss: 137.42
train mean loss: 133.91
epoch train time: 0:00:01.647669
elapsed time: 0:04:17.428027
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 19:34:53.417124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.05
 ---- batch: 020 ----
mean loss: 126.67
 ---- batch: 030 ----
mean loss: 128.71
 ---- batch: 040 ----
mean loss: 132.16
 ---- batch: 050 ----
mean loss: 131.94
 ---- batch: 060 ----
mean loss: 141.02
 ---- batch: 070 ----
mean loss: 134.77
 ---- batch: 080 ----
mean loss: 136.08
 ---- batch: 090 ----
mean loss: 134.33
train mean loss: 132.77
epoch train time: 0:00:01.659654
elapsed time: 0:04:19.088334
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 19:34:55.077449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.27
 ---- batch: 020 ----
mean loss: 126.90
 ---- batch: 030 ----
mean loss: 134.10
 ---- batch: 040 ----
mean loss: 132.77
 ---- batch: 050 ----
mean loss: 132.53
 ---- batch: 060 ----
mean loss: 128.29
 ---- batch: 070 ----
mean loss: 127.36
 ---- batch: 080 ----
mean loss: 142.27
 ---- batch: 090 ----
mean loss: 140.21
train mean loss: 132.42
epoch train time: 0:00:01.631240
elapsed time: 0:04:20.720275
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 19:34:56.709365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.10
 ---- batch: 020 ----
mean loss: 124.28
 ---- batch: 030 ----
mean loss: 134.26
 ---- batch: 040 ----
mean loss: 131.63
 ---- batch: 050 ----
mean loss: 130.79
 ---- batch: 060 ----
mean loss: 135.39
 ---- batch: 070 ----
mean loss: 131.91
 ---- batch: 080 ----
mean loss: 139.69
 ---- batch: 090 ----
mean loss: 133.98
train mean loss: 131.49
epoch train time: 0:00:01.636763
elapsed time: 0:04:22.357682
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 19:34:58.346768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.96
 ---- batch: 020 ----
mean loss: 129.48
 ---- batch: 030 ----
mean loss: 128.15
 ---- batch: 040 ----
mean loss: 133.73
 ---- batch: 050 ----
mean loss: 130.52
 ---- batch: 060 ----
mean loss: 130.17
 ---- batch: 070 ----
mean loss: 128.66
 ---- batch: 080 ----
mean loss: 136.07
 ---- batch: 090 ----
mean loss: 131.91
train mean loss: 130.91
epoch train time: 0:00:01.612219
elapsed time: 0:04:23.970553
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 19:34:59.959673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.24
 ---- batch: 020 ----
mean loss: 127.19
 ---- batch: 030 ----
mean loss: 132.70
 ---- batch: 040 ----
mean loss: 121.71
 ---- batch: 050 ----
mean loss: 134.66
 ---- batch: 060 ----
mean loss: 125.55
 ---- batch: 070 ----
mean loss: 137.33
 ---- batch: 080 ----
mean loss: 132.50
 ---- batch: 090 ----
mean loss: 136.32
train mean loss: 131.16
epoch train time: 0:00:01.650518
elapsed time: 0:04:25.621761
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 19:35:01.610853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.92
 ---- batch: 020 ----
mean loss: 129.36
 ---- batch: 030 ----
mean loss: 129.03
 ---- batch: 040 ----
mean loss: 128.90
 ---- batch: 050 ----
mean loss: 134.46
 ---- batch: 060 ----
mean loss: 132.20
 ---- batch: 070 ----
mean loss: 129.45
 ---- batch: 080 ----
mean loss: 129.68
 ---- batch: 090 ----
mean loss: 129.03
train mean loss: 130.27
epoch train time: 0:00:01.651334
elapsed time: 0:04:27.273726
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 19:35:03.262809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.28
 ---- batch: 020 ----
mean loss: 130.79
 ---- batch: 030 ----
mean loss: 122.71
 ---- batch: 040 ----
mean loss: 133.24
 ---- batch: 050 ----
mean loss: 127.75
 ---- batch: 060 ----
mean loss: 126.47
 ---- batch: 070 ----
mean loss: 133.00
 ---- batch: 080 ----
mean loss: 127.57
 ---- batch: 090 ----
mean loss: 127.03
train mean loss: 129.73
epoch train time: 0:00:01.655994
elapsed time: 0:04:28.930469
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 19:35:04.919642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.52
 ---- batch: 020 ----
mean loss: 130.54
 ---- batch: 030 ----
mean loss: 133.04
 ---- batch: 040 ----
mean loss: 126.35
 ---- batch: 050 ----
mean loss: 126.83
 ---- batch: 060 ----
mean loss: 134.32
 ---- batch: 070 ----
mean loss: 130.04
 ---- batch: 080 ----
mean loss: 126.33
 ---- batch: 090 ----
mean loss: 132.18
train mean loss: 129.02
epoch train time: 0:00:01.628766
elapsed time: 0:04:30.559966
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 19:35:06.549058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.37
 ---- batch: 020 ----
mean loss: 130.09
 ---- batch: 030 ----
mean loss: 124.55
 ---- batch: 040 ----
mean loss: 125.87
 ---- batch: 050 ----
mean loss: 134.25
 ---- batch: 060 ----
mean loss: 131.73
 ---- batch: 070 ----
mean loss: 125.49
 ---- batch: 080 ----
mean loss: 133.12
 ---- batch: 090 ----
mean loss: 130.38
train mean loss: 129.10
epoch train time: 0:00:01.671579
elapsed time: 0:04:32.232258
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 19:35:08.221405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.36
 ---- batch: 020 ----
mean loss: 121.42
 ---- batch: 030 ----
mean loss: 128.05
 ---- batch: 040 ----
mean loss: 127.51
 ---- batch: 050 ----
mean loss: 129.19
 ---- batch: 060 ----
mean loss: 133.23
 ---- batch: 070 ----
mean loss: 124.79
 ---- batch: 080 ----
mean loss: 126.69
 ---- batch: 090 ----
mean loss: 129.35
train mean loss: 128.13
epoch train time: 0:00:01.655281
elapsed time: 0:04:33.888372
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 19:35:09.877469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.59
 ---- batch: 020 ----
mean loss: 128.65
 ---- batch: 030 ----
mean loss: 126.30
 ---- batch: 040 ----
mean loss: 128.35
 ---- batch: 050 ----
mean loss: 122.56
 ---- batch: 060 ----
mean loss: 130.52
 ---- batch: 070 ----
mean loss: 124.98
 ---- batch: 080 ----
mean loss: 134.07
 ---- batch: 090 ----
mean loss: 130.23
train mean loss: 127.44
epoch train time: 0:00:01.608261
elapsed time: 0:04:35.497330
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 19:35:11.486411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.33
 ---- batch: 020 ----
mean loss: 121.61
 ---- batch: 030 ----
mean loss: 124.49
 ---- batch: 040 ----
mean loss: 129.59
 ---- batch: 050 ----
mean loss: 124.87
 ---- batch: 060 ----
mean loss: 125.48
 ---- batch: 070 ----
mean loss: 133.45
 ---- batch: 080 ----
mean loss: 134.97
 ---- batch: 090 ----
mean loss: 131.66
train mean loss: 127.77
epoch train time: 0:00:01.637515
elapsed time: 0:04:37.135729
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 19:35:13.124644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.27
 ---- batch: 020 ----
mean loss: 122.56
 ---- batch: 030 ----
mean loss: 121.55
 ---- batch: 040 ----
mean loss: 125.70
 ---- batch: 050 ----
mean loss: 122.62
 ---- batch: 060 ----
mean loss: 133.43
 ---- batch: 070 ----
mean loss: 134.41
 ---- batch: 080 ----
mean loss: 127.98
 ---- batch: 090 ----
mean loss: 133.33
train mean loss: 126.65
epoch train time: 0:00:01.613708
elapsed time: 0:04:38.749964
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 19:35:14.739055
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.39
 ---- batch: 020 ----
mean loss: 128.39
 ---- batch: 030 ----
mean loss: 127.94
 ---- batch: 040 ----
mean loss: 130.58
 ---- batch: 050 ----
mean loss: 120.92
 ---- batch: 060 ----
mean loss: 124.18
 ---- batch: 070 ----
mean loss: 119.73
 ---- batch: 080 ----
mean loss: 130.81
 ---- batch: 090 ----
mean loss: 125.23
train mean loss: 126.29
epoch train time: 0:00:01.636150
elapsed time: 0:04:40.386834
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 19:35:16.375982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.62
 ---- batch: 020 ----
mean loss: 124.16
 ---- batch: 030 ----
mean loss: 125.38
 ---- batch: 040 ----
mean loss: 125.95
 ---- batch: 050 ----
mean loss: 130.79
 ---- batch: 060 ----
mean loss: 126.06
 ---- batch: 070 ----
mean loss: 122.74
 ---- batch: 080 ----
mean loss: 126.78
 ---- batch: 090 ----
mean loss: 128.20
train mean loss: 126.05
epoch train time: 0:00:01.630798
elapsed time: 0:04:42.018379
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 19:35:18.007454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.52
 ---- batch: 020 ----
mean loss: 126.41
 ---- batch: 030 ----
mean loss: 132.33
 ---- batch: 040 ----
mean loss: 119.56
 ---- batch: 050 ----
mean loss: 125.07
 ---- batch: 060 ----
mean loss: 125.59
 ---- batch: 070 ----
mean loss: 128.93
 ---- batch: 080 ----
mean loss: 127.43
 ---- batch: 090 ----
mean loss: 129.03
train mean loss: 125.23
epoch train time: 0:00:01.637216
elapsed time: 0:04:43.656230
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 19:35:19.645318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.22
 ---- batch: 020 ----
mean loss: 121.57
 ---- batch: 030 ----
mean loss: 124.95
 ---- batch: 040 ----
mean loss: 124.98
 ---- batch: 050 ----
mean loss: 120.63
 ---- batch: 060 ----
mean loss: 124.26
 ---- batch: 070 ----
mean loss: 125.36
 ---- batch: 080 ----
mean loss: 124.49
 ---- batch: 090 ----
mean loss: 131.18
train mean loss: 124.82
epoch train time: 0:00:01.646457
elapsed time: 0:04:45.303353
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 19:35:21.292471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.80
 ---- batch: 020 ----
mean loss: 120.79
 ---- batch: 030 ----
mean loss: 119.45
 ---- batch: 040 ----
mean loss: 128.34
 ---- batch: 050 ----
mean loss: 130.73
 ---- batch: 060 ----
mean loss: 132.50
 ---- batch: 070 ----
mean loss: 126.01
 ---- batch: 080 ----
mean loss: 121.94
 ---- batch: 090 ----
mean loss: 121.05
train mean loss: 124.33
epoch train time: 0:00:01.650557
elapsed time: 0:04:46.954661
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 19:35:22.943760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.84
 ---- batch: 020 ----
mean loss: 127.58
 ---- batch: 030 ----
mean loss: 125.02
 ---- batch: 040 ----
mean loss: 122.86
 ---- batch: 050 ----
mean loss: 127.08
 ---- batch: 060 ----
mean loss: 125.93
 ---- batch: 070 ----
mean loss: 125.25
 ---- batch: 080 ----
mean loss: 119.24
 ---- batch: 090 ----
mean loss: 124.55
train mean loss: 123.75
epoch train time: 0:00:01.689376
elapsed time: 0:04:48.644807
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 19:35:24.633926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.09
 ---- batch: 020 ----
mean loss: 118.08
 ---- batch: 030 ----
mean loss: 129.17
 ---- batch: 040 ----
mean loss: 123.74
 ---- batch: 050 ----
mean loss: 116.96
 ---- batch: 060 ----
mean loss: 119.85
 ---- batch: 070 ----
mean loss: 126.87
 ---- batch: 080 ----
mean loss: 132.33
 ---- batch: 090 ----
mean loss: 125.75
train mean loss: 123.48
epoch train time: 0:00:01.655246
elapsed time: 0:04:50.300704
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 19:35:26.289840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.79
 ---- batch: 020 ----
mean loss: 120.90
 ---- batch: 030 ----
mean loss: 119.81
 ---- batch: 040 ----
mean loss: 117.71
 ---- batch: 050 ----
mean loss: 127.15
 ---- batch: 060 ----
mean loss: 122.98
 ---- batch: 070 ----
mean loss: 122.65
 ---- batch: 080 ----
mean loss: 127.04
 ---- batch: 090 ----
mean loss: 125.73
train mean loss: 123.29
epoch train time: 0:00:01.630248
elapsed time: 0:04:51.931701
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 19:35:27.920783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.74
 ---- batch: 020 ----
mean loss: 121.60
 ---- batch: 030 ----
mean loss: 125.54
 ---- batch: 040 ----
mean loss: 121.12
 ---- batch: 050 ----
mean loss: 121.04
 ---- batch: 060 ----
mean loss: 117.41
 ---- batch: 070 ----
mean loss: 119.35
 ---- batch: 080 ----
mean loss: 124.85
 ---- batch: 090 ----
mean loss: 123.99
train mean loss: 122.30
epoch train time: 0:00:01.629468
elapsed time: 0:04:53.561862
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 19:35:29.551076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.24
 ---- batch: 020 ----
mean loss: 118.66
 ---- batch: 030 ----
mean loss: 120.39
 ---- batch: 040 ----
mean loss: 125.89
 ---- batch: 050 ----
mean loss: 127.29
 ---- batch: 060 ----
mean loss: 118.61
 ---- batch: 070 ----
mean loss: 127.45
 ---- batch: 080 ----
mean loss: 126.51
 ---- batch: 090 ----
mean loss: 123.88
train mean loss: 122.27
epoch train time: 0:00:01.614942
elapsed time: 0:04:55.177525
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 19:35:31.166603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.73
 ---- batch: 020 ----
mean loss: 118.21
 ---- batch: 030 ----
mean loss: 120.25
 ---- batch: 040 ----
mean loss: 117.36
 ---- batch: 050 ----
mean loss: 117.88
 ---- batch: 060 ----
mean loss: 124.10
 ---- batch: 070 ----
mean loss: 124.05
 ---- batch: 080 ----
mean loss: 127.08
 ---- batch: 090 ----
mean loss: 118.99
train mean loss: 120.89
epoch train time: 0:00:01.651083
elapsed time: 0:04:56.829253
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 19:35:32.818327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.77
 ---- batch: 020 ----
mean loss: 119.10
 ---- batch: 030 ----
mean loss: 118.75
 ---- batch: 040 ----
mean loss: 118.55
 ---- batch: 050 ----
mean loss: 115.54
 ---- batch: 060 ----
mean loss: 118.91
 ---- batch: 070 ----
mean loss: 122.74
 ---- batch: 080 ----
mean loss: 124.04
 ---- batch: 090 ----
mean loss: 123.27
train mean loss: 120.46
epoch train time: 0:00:01.643447
elapsed time: 0:04:58.473338
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 19:35:34.462411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.12
 ---- batch: 020 ----
mean loss: 122.89
 ---- batch: 030 ----
mean loss: 122.38
 ---- batch: 040 ----
mean loss: 123.52
 ---- batch: 050 ----
mean loss: 118.82
 ---- batch: 060 ----
mean loss: 121.40
 ---- batch: 070 ----
mean loss: 124.73
 ---- batch: 080 ----
mean loss: 124.17
 ---- batch: 090 ----
mean loss: 120.88
train mean loss: 120.86
epoch train time: 0:00:01.647661
elapsed time: 0:05:00.121666
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 19:35:36.110842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.58
 ---- batch: 020 ----
mean loss: 115.52
 ---- batch: 030 ----
mean loss: 121.56
 ---- batch: 040 ----
mean loss: 118.71
 ---- batch: 050 ----
mean loss: 117.55
 ---- batch: 060 ----
mean loss: 121.82
 ---- batch: 070 ----
mean loss: 120.52
 ---- batch: 080 ----
mean loss: 117.80
 ---- batch: 090 ----
mean loss: 123.77
train mean loss: 120.30
epoch train time: 0:00:01.679705
elapsed time: 0:05:01.802160
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 19:35:37.791331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.13
 ---- batch: 020 ----
mean loss: 115.85
 ---- batch: 030 ----
mean loss: 117.93
 ---- batch: 040 ----
mean loss: 111.86
 ---- batch: 050 ----
mean loss: 118.70
 ---- batch: 060 ----
mean loss: 123.90
 ---- batch: 070 ----
mean loss: 127.49
 ---- batch: 080 ----
mean loss: 118.87
 ---- batch: 090 ----
mean loss: 123.45
train mean loss: 120.11
epoch train time: 0:00:01.645784
elapsed time: 0:05:03.448693
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 19:35:39.437823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.08
 ---- batch: 020 ----
mean loss: 120.79
 ---- batch: 030 ----
mean loss: 115.28
 ---- batch: 040 ----
mean loss: 123.26
 ---- batch: 050 ----
mean loss: 119.01
 ---- batch: 060 ----
mean loss: 116.81
 ---- batch: 070 ----
mean loss: 120.94
 ---- batch: 080 ----
mean loss: 118.29
 ---- batch: 090 ----
mean loss: 115.22
train mean loss: 119.05
epoch train time: 0:00:01.646335
elapsed time: 0:05:05.095751
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 19:35:41.084889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.27
 ---- batch: 020 ----
mean loss: 118.87
 ---- batch: 030 ----
mean loss: 118.85
 ---- batch: 040 ----
mean loss: 117.06
 ---- batch: 050 ----
mean loss: 116.83
 ---- batch: 060 ----
mean loss: 117.49
 ---- batch: 070 ----
mean loss: 113.77
 ---- batch: 080 ----
mean loss: 118.25
 ---- batch: 090 ----
mean loss: 122.63
train mean loss: 118.32
epoch train time: 0:00:01.609737
elapsed time: 0:05:06.706208
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 19:35:42.695310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.32
 ---- batch: 020 ----
mean loss: 115.07
 ---- batch: 030 ----
mean loss: 122.44
 ---- batch: 040 ----
mean loss: 109.13
 ---- batch: 050 ----
mean loss: 118.48
 ---- batch: 060 ----
mean loss: 113.57
 ---- batch: 070 ----
mean loss: 122.95
 ---- batch: 080 ----
mean loss: 126.56
 ---- batch: 090 ----
mean loss: 117.33
train mean loss: 118.51
epoch train time: 0:00:01.634278
elapsed time: 0:05:08.341137
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 19:35:44.330309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.55
 ---- batch: 020 ----
mean loss: 116.92
 ---- batch: 030 ----
mean loss: 110.61
 ---- batch: 040 ----
mean loss: 113.71
 ---- batch: 050 ----
mean loss: 117.81
 ---- batch: 060 ----
mean loss: 127.55
 ---- batch: 070 ----
mean loss: 121.44
 ---- batch: 080 ----
mean loss: 126.36
 ---- batch: 090 ----
mean loss: 116.24
train mean loss: 118.46
epoch train time: 0:00:01.627510
elapsed time: 0:05:09.969417
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 19:35:45.958510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.73
 ---- batch: 020 ----
mean loss: 112.78
 ---- batch: 030 ----
mean loss: 111.12
 ---- batch: 040 ----
mean loss: 117.42
 ---- batch: 050 ----
mean loss: 112.45
 ---- batch: 060 ----
mean loss: 119.65
 ---- batch: 070 ----
mean loss: 122.20
 ---- batch: 080 ----
mean loss: 119.71
 ---- batch: 090 ----
mean loss: 120.61
train mean loss: 117.13
epoch train time: 0:00:01.623818
elapsed time: 0:05:11.593850
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 19:35:47.582934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.20
 ---- batch: 020 ----
mean loss: 115.48
 ---- batch: 030 ----
mean loss: 116.31
 ---- batch: 040 ----
mean loss: 119.05
 ---- batch: 050 ----
mean loss: 120.57
 ---- batch: 060 ----
mean loss: 114.95
 ---- batch: 070 ----
mean loss: 119.12
 ---- batch: 080 ----
mean loss: 117.42
 ---- batch: 090 ----
mean loss: 114.43
train mean loss: 116.95
epoch train time: 0:00:01.664034
elapsed time: 0:05:13.258526
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 19:35:49.247599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.97
 ---- batch: 020 ----
mean loss: 109.21
 ---- batch: 030 ----
mean loss: 116.80
 ---- batch: 040 ----
mean loss: 111.80
 ---- batch: 050 ----
mean loss: 115.08
 ---- batch: 060 ----
mean loss: 117.31
 ---- batch: 070 ----
mean loss: 117.17
 ---- batch: 080 ----
mean loss: 117.93
 ---- batch: 090 ----
mean loss: 125.31
train mean loss: 116.27
epoch train time: 0:00:01.632907
elapsed time: 0:05:14.892064
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 19:35:50.881144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.46
 ---- batch: 020 ----
mean loss: 113.34
 ---- batch: 030 ----
mean loss: 122.00
 ---- batch: 040 ----
mean loss: 116.39
 ---- batch: 050 ----
mean loss: 112.66
 ---- batch: 060 ----
mean loss: 113.24
 ---- batch: 070 ----
mean loss: 116.83
 ---- batch: 080 ----
mean loss: 118.10
 ---- batch: 090 ----
mean loss: 121.06
train mean loss: 116.37
epoch train time: 0:00:01.641284
elapsed time: 0:05:16.533993
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 19:35:52.523073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.59
 ---- batch: 020 ----
mean loss: 110.76
 ---- batch: 030 ----
mean loss: 116.65
 ---- batch: 040 ----
mean loss: 114.96
 ---- batch: 050 ----
mean loss: 114.82
 ---- batch: 060 ----
mean loss: 125.16
 ---- batch: 070 ----
mean loss: 119.11
 ---- batch: 080 ----
mean loss: 115.65
 ---- batch: 090 ----
mean loss: 119.89
train mean loss: 116.78
epoch train time: 0:00:01.639555
elapsed time: 0:05:18.174457
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 19:35:54.163309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.93
 ---- batch: 020 ----
mean loss: 115.64
 ---- batch: 030 ----
mean loss: 120.08
 ---- batch: 040 ----
mean loss: 113.83
 ---- batch: 050 ----
mean loss: 110.55
 ---- batch: 060 ----
mean loss: 117.93
 ---- batch: 070 ----
mean loss: 115.39
 ---- batch: 080 ----
mean loss: 114.79
 ---- batch: 090 ----
mean loss: 119.10
train mean loss: 115.70
epoch train time: 0:00:01.651021
elapsed time: 0:05:19.825946
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 19:35:55.815036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.53
 ---- batch: 020 ----
mean loss: 109.77
 ---- batch: 030 ----
mean loss: 114.09
 ---- batch: 040 ----
mean loss: 118.24
 ---- batch: 050 ----
mean loss: 113.55
 ---- batch: 060 ----
mean loss: 123.98
 ---- batch: 070 ----
mean loss: 117.38
 ---- batch: 080 ----
mean loss: 117.93
 ---- batch: 090 ----
mean loss: 115.67
train mean loss: 115.62
epoch train time: 0:00:01.627758
elapsed time: 0:05:21.454474
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 19:35:57.443655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.91
 ---- batch: 020 ----
mean loss: 112.66
 ---- batch: 030 ----
mean loss: 109.66
 ---- batch: 040 ----
mean loss: 113.30
 ---- batch: 050 ----
mean loss: 118.02
 ---- batch: 060 ----
mean loss: 115.51
 ---- batch: 070 ----
mean loss: 117.41
 ---- batch: 080 ----
mean loss: 114.79
 ---- batch: 090 ----
mean loss: 122.10
train mean loss: 115.37
epoch train time: 0:00:01.620178
elapsed time: 0:05:23.075518
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 19:35:59.064606
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.44
 ---- batch: 020 ----
mean loss: 109.67
 ---- batch: 030 ----
mean loss: 114.32
 ---- batch: 040 ----
mean loss: 111.54
 ---- batch: 050 ----
mean loss: 113.39
 ---- batch: 060 ----
mean loss: 118.82
 ---- batch: 070 ----
mean loss: 118.86
 ---- batch: 080 ----
mean loss: 112.81
 ---- batch: 090 ----
mean loss: 110.08
train mean loss: 114.03
epoch train time: 0:00:01.669719
elapsed time: 0:05:24.745929
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 19:36:00.735027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.88
 ---- batch: 020 ----
mean loss: 114.25
 ---- batch: 030 ----
mean loss: 109.09
 ---- batch: 040 ----
mean loss: 113.57
 ---- batch: 050 ----
mean loss: 113.57
 ---- batch: 060 ----
mean loss: 119.21
 ---- batch: 070 ----
mean loss: 116.31
 ---- batch: 080 ----
mean loss: 114.23
 ---- batch: 090 ----
mean loss: 117.42
train mean loss: 114.16
epoch train time: 0:00:01.652351
elapsed time: 0:05:26.398953
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 19:36:02.388049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.29
 ---- batch: 020 ----
mean loss: 106.80
 ---- batch: 030 ----
mean loss: 113.78
 ---- batch: 040 ----
mean loss: 110.22
 ---- batch: 050 ----
mean loss: 111.76
 ---- batch: 060 ----
mean loss: 114.90
 ---- batch: 070 ----
mean loss: 116.21
 ---- batch: 080 ----
mean loss: 118.48
 ---- batch: 090 ----
mean loss: 113.85
train mean loss: 113.22
epoch train time: 0:00:01.645711
elapsed time: 0:05:28.045348
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 19:36:04.034432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.76
 ---- batch: 020 ----
mean loss: 111.51
 ---- batch: 030 ----
mean loss: 110.35
 ---- batch: 040 ----
mean loss: 108.85
 ---- batch: 050 ----
mean loss: 114.67
 ---- batch: 060 ----
mean loss: 111.46
 ---- batch: 070 ----
mean loss: 115.18
 ---- batch: 080 ----
mean loss: 114.77
 ---- batch: 090 ----
mean loss: 116.29
train mean loss: 113.09
epoch train time: 0:00:01.652980
elapsed time: 0:05:29.698967
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 19:36:05.688100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.92
 ---- batch: 020 ----
mean loss: 108.15
 ---- batch: 030 ----
mean loss: 108.31
 ---- batch: 040 ----
mean loss: 112.31
 ---- batch: 050 ----
mean loss: 112.26
 ---- batch: 060 ----
mean loss: 116.77
 ---- batch: 070 ----
mean loss: 113.18
 ---- batch: 080 ----
mean loss: 117.20
 ---- batch: 090 ----
mean loss: 113.34
train mean loss: 112.39
epoch train time: 0:00:01.621901
elapsed time: 0:05:31.321564
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 19:36:07.310745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.81
 ---- batch: 020 ----
mean loss: 113.33
 ---- batch: 030 ----
mean loss: 110.81
 ---- batch: 040 ----
mean loss: 115.46
 ---- batch: 050 ----
mean loss: 113.64
 ---- batch: 060 ----
mean loss: 111.43
 ---- batch: 070 ----
mean loss: 107.73
 ---- batch: 080 ----
mean loss: 113.66
 ---- batch: 090 ----
mean loss: 109.17
train mean loss: 112.07
epoch train time: 0:00:01.659829
elapsed time: 0:05:32.982129
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 19:36:08.971258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.50
 ---- batch: 020 ----
mean loss: 111.88
 ---- batch: 030 ----
mean loss: 107.52
 ---- batch: 040 ----
mean loss: 111.61
 ---- batch: 050 ----
mean loss: 112.09
 ---- batch: 060 ----
mean loss: 110.91
 ---- batch: 070 ----
mean loss: 115.79
 ---- batch: 080 ----
mean loss: 112.32
 ---- batch: 090 ----
mean loss: 109.77
train mean loss: 111.90
epoch train time: 0:00:01.650382
elapsed time: 0:05:34.633198
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 19:36:10.622288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.87
 ---- batch: 020 ----
mean loss: 112.61
 ---- batch: 030 ----
mean loss: 104.84
 ---- batch: 040 ----
mean loss: 116.77
 ---- batch: 050 ----
mean loss: 113.19
 ---- batch: 060 ----
mean loss: 114.46
 ---- batch: 070 ----
mean loss: 111.03
 ---- batch: 080 ----
mean loss: 108.46
 ---- batch: 090 ----
mean loss: 112.26
train mean loss: 111.93
epoch train time: 0:00:01.674745
elapsed time: 0:05:36.308663
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 19:36:12.297781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.31
 ---- batch: 020 ----
mean loss: 109.32
 ---- batch: 030 ----
mean loss: 105.68
 ---- batch: 040 ----
mean loss: 108.85
 ---- batch: 050 ----
mean loss: 118.28
 ---- batch: 060 ----
mean loss: 114.46
 ---- batch: 070 ----
mean loss: 110.96
 ---- batch: 080 ----
mean loss: 111.97
 ---- batch: 090 ----
mean loss: 112.77
train mean loss: 110.97
epoch train time: 0:00:01.652715
elapsed time: 0:05:37.962112
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 19:36:13.951262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.26
 ---- batch: 020 ----
mean loss: 105.73
 ---- batch: 030 ----
mean loss: 111.45
 ---- batch: 040 ----
mean loss: 113.09
 ---- batch: 050 ----
mean loss: 107.45
 ---- batch: 060 ----
mean loss: 114.07
 ---- batch: 070 ----
mean loss: 113.42
 ---- batch: 080 ----
mean loss: 106.39
 ---- batch: 090 ----
mean loss: 117.43
train mean loss: 111.14
epoch train time: 0:00:01.649856
elapsed time: 0:05:39.612774
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 19:36:15.601887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.48
 ---- batch: 020 ----
mean loss: 111.44
 ---- batch: 030 ----
mean loss: 107.72
 ---- batch: 040 ----
mean loss: 108.72
 ---- batch: 050 ----
mean loss: 112.83
 ---- batch: 060 ----
mean loss: 111.89
 ---- batch: 070 ----
mean loss: 109.22
 ---- batch: 080 ----
mean loss: 112.86
 ---- batch: 090 ----
mean loss: 112.73
train mean loss: 110.44
epoch train time: 0:00:01.606908
elapsed time: 0:05:41.220356
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 19:36:17.209460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.59
 ---- batch: 020 ----
mean loss: 106.49
 ---- batch: 030 ----
mean loss: 102.31
 ---- batch: 040 ----
mean loss: 105.31
 ---- batch: 050 ----
mean loss: 111.57
 ---- batch: 060 ----
mean loss: 113.30
 ---- batch: 070 ----
mean loss: 112.58
 ---- batch: 080 ----
mean loss: 115.52
 ---- batch: 090 ----
mean loss: 113.04
train mean loss: 109.53
epoch train time: 0:00:01.633263
elapsed time: 0:05:42.854368
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 19:36:18.843529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.29
 ---- batch: 020 ----
mean loss: 109.34
 ---- batch: 030 ----
mean loss: 112.07
 ---- batch: 040 ----
mean loss: 106.52
 ---- batch: 050 ----
mean loss: 107.91
 ---- batch: 060 ----
mean loss: 108.28
 ---- batch: 070 ----
mean loss: 109.22
 ---- batch: 080 ----
mean loss: 115.24
 ---- batch: 090 ----
mean loss: 115.82
train mean loss: 109.74
epoch train time: 0:00:01.613871
elapsed time: 0:05:44.469026
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 19:36:20.458116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.42
 ---- batch: 020 ----
mean loss: 107.96
 ---- batch: 030 ----
mean loss: 107.10
 ---- batch: 040 ----
mean loss: 110.95
 ---- batch: 050 ----
mean loss: 110.60
 ---- batch: 060 ----
mean loss: 110.75
 ---- batch: 070 ----
mean loss: 114.42
 ---- batch: 080 ----
mean loss: 112.84
 ---- batch: 090 ----
mean loss: 105.04
train mean loss: 109.40
epoch train time: 0:00:01.614622
elapsed time: 0:05:46.084270
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 19:36:22.073357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.71
 ---- batch: 020 ----
mean loss: 106.72
 ---- batch: 030 ----
mean loss: 106.36
 ---- batch: 040 ----
mean loss: 107.95
 ---- batch: 050 ----
mean loss: 109.80
 ---- batch: 060 ----
mean loss: 107.54
 ---- batch: 070 ----
mean loss: 114.90
 ---- batch: 080 ----
mean loss: 110.15
 ---- batch: 090 ----
mean loss: 112.38
train mean loss: 108.96
epoch train time: 0:00:01.628605
elapsed time: 0:05:47.713584
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 19:36:23.702675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.75
 ---- batch: 020 ----
mean loss: 105.43
 ---- batch: 030 ----
mean loss: 105.74
 ---- batch: 040 ----
mean loss: 107.53
 ---- batch: 050 ----
mean loss: 109.75
 ---- batch: 060 ----
mean loss: 109.80
 ---- batch: 070 ----
mean loss: 106.42
 ---- batch: 080 ----
mean loss: 104.01
 ---- batch: 090 ----
mean loss: 115.73
train mean loss: 108.26
epoch train time: 0:00:01.653094
elapsed time: 0:05:49.367415
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 19:36:25.356609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.87
 ---- batch: 020 ----
mean loss: 100.81
 ---- batch: 030 ----
mean loss: 108.46
 ---- batch: 040 ----
mean loss: 106.85
 ---- batch: 050 ----
mean loss: 108.79
 ---- batch: 060 ----
mean loss: 102.72
 ---- batch: 070 ----
mean loss: 108.31
 ---- batch: 080 ----
mean loss: 115.32
 ---- batch: 090 ----
mean loss: 117.51
train mean loss: 108.25
epoch train time: 0:00:01.630971
elapsed time: 0:05:50.999166
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 19:36:26.988282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.34
 ---- batch: 020 ----
mean loss: 106.68
 ---- batch: 030 ----
mean loss: 103.23
 ---- batch: 040 ----
mean loss: 107.62
 ---- batch: 050 ----
mean loss: 110.41
 ---- batch: 060 ----
mean loss: 110.76
 ---- batch: 070 ----
mean loss: 109.59
 ---- batch: 080 ----
mean loss: 113.67
 ---- batch: 090 ----
mean loss: 108.45
train mean loss: 108.35
epoch train time: 0:00:01.623912
elapsed time: 0:05:52.623771
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 19:36:28.612857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.59
 ---- batch: 020 ----
mean loss: 108.19
 ---- batch: 030 ----
mean loss: 111.53
 ---- batch: 040 ----
mean loss: 105.95
 ---- batch: 050 ----
mean loss: 106.61
 ---- batch: 060 ----
mean loss: 106.97
 ---- batch: 070 ----
mean loss: 110.43
 ---- batch: 080 ----
mean loss: 110.96
 ---- batch: 090 ----
mean loss: 107.55
train mean loss: 107.52
epoch train time: 0:00:01.657335
elapsed time: 0:05:54.281770
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 19:36:30.270943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.79
 ---- batch: 020 ----
mean loss: 104.64
 ---- batch: 030 ----
mean loss: 103.56
 ---- batch: 040 ----
mean loss: 106.72
 ---- batch: 050 ----
mean loss: 102.36
 ---- batch: 060 ----
mean loss: 107.70
 ---- batch: 070 ----
mean loss: 109.93
 ---- batch: 080 ----
mean loss: 104.20
 ---- batch: 090 ----
mean loss: 115.77
train mean loss: 107.03
epoch train time: 0:00:01.644223
elapsed time: 0:05:55.926806
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 19:36:31.915904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.44
 ---- batch: 020 ----
mean loss: 107.11
 ---- batch: 030 ----
mean loss: 103.49
 ---- batch: 040 ----
mean loss: 102.34
 ---- batch: 050 ----
mean loss: 103.90
 ---- batch: 060 ----
mean loss: 107.47
 ---- batch: 070 ----
mean loss: 107.18
 ---- batch: 080 ----
mean loss: 114.97
 ---- batch: 090 ----
mean loss: 116.05
train mean loss: 107.42
epoch train time: 0:00:01.617825
elapsed time: 0:05:57.545307
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 19:36:33.534393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.55
 ---- batch: 020 ----
mean loss: 101.80
 ---- batch: 030 ----
mean loss: 105.86
 ---- batch: 040 ----
mean loss: 105.04
 ---- batch: 050 ----
mean loss: 111.07
 ---- batch: 060 ----
mean loss: 107.20
 ---- batch: 070 ----
mean loss: 109.04
 ---- batch: 080 ----
mean loss: 111.70
 ---- batch: 090 ----
mean loss: 109.68
train mean loss: 106.66
epoch train time: 0:00:01.641727
elapsed time: 0:05:59.187703
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 19:36:35.176806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 97.20
 ---- batch: 020 ----
mean loss: 102.83
 ---- batch: 030 ----
mean loss: 104.82
 ---- batch: 040 ----
mean loss: 108.53
 ---- batch: 050 ----
mean loss: 106.09
 ---- batch: 060 ----
mean loss: 109.12
 ---- batch: 070 ----
mean loss: 104.32
 ---- batch: 080 ----
mean loss: 107.95
 ---- batch: 090 ----
mean loss: 113.67
train mean loss: 106.74
epoch train time: 0:00:01.584847
elapsed time: 0:06:00.773236
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 19:36:36.762355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.40
 ---- batch: 020 ----
mean loss: 104.93
 ---- batch: 030 ----
mean loss: 100.02
 ---- batch: 040 ----
mean loss: 101.33
 ---- batch: 050 ----
mean loss: 104.91
 ---- batch: 060 ----
mean loss: 107.19
 ---- batch: 070 ----
mean loss: 112.29
 ---- batch: 080 ----
mean loss: 115.61
 ---- batch: 090 ----
mean loss: 109.34
train mean loss: 106.59
epoch train time: 0:00:01.604523
elapsed time: 0:06:02.378510
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 19:36:38.367588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.66
 ---- batch: 020 ----
mean loss: 101.71
 ---- batch: 030 ----
mean loss: 95.62
 ---- batch: 040 ----
mean loss: 106.63
 ---- batch: 050 ----
mean loss: 99.66
 ---- batch: 060 ----
mean loss: 97.72
 ---- batch: 070 ----
mean loss: 100.21
 ---- batch: 080 ----
mean loss: 100.39
 ---- batch: 090 ----
mean loss: 99.90
train mean loss: 101.08
epoch train time: 0:00:01.628821
elapsed time: 0:06:04.008254
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 19:36:39.997088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 103.32
 ---- batch: 020 ----
mean loss: 97.68
 ---- batch: 030 ----
mean loss: 99.74
 ---- batch: 040 ----
mean loss: 97.52
 ---- batch: 050 ----
mean loss: 100.43
 ---- batch: 060 ----
mean loss: 105.33
 ---- batch: 070 ----
mean loss: 97.22
 ---- batch: 080 ----
mean loss: 98.93
 ---- batch: 090 ----
mean loss: 97.88
train mean loss: 100.13
epoch train time: 0:00:01.647795
elapsed time: 0:06:05.656511
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 19:36:41.645619
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.76
 ---- batch: 020 ----
mean loss: 99.44
 ---- batch: 030 ----
mean loss: 95.39
 ---- batch: 040 ----
mean loss: 102.37
 ---- batch: 050 ----
mean loss: 102.16
 ---- batch: 060 ----
mean loss: 95.35
 ---- batch: 070 ----
mean loss: 98.31
 ---- batch: 080 ----
mean loss: 104.84
 ---- batch: 090 ----
mean loss: 100.00
train mean loss: 99.88
epoch train time: 0:00:01.615604
elapsed time: 0:06:07.272732
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 19:36:43.261865
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.85
 ---- batch: 020 ----
mean loss: 99.23
 ---- batch: 030 ----
mean loss: 106.71
 ---- batch: 040 ----
mean loss: 91.95
 ---- batch: 050 ----
mean loss: 106.70
 ---- batch: 060 ----
mean loss: 96.95
 ---- batch: 070 ----
mean loss: 98.60
 ---- batch: 080 ----
mean loss: 100.24
 ---- batch: 090 ----
mean loss: 100.19
train mean loss: 99.60
epoch train time: 0:00:01.613102
elapsed time: 0:06:08.886518
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 19:36:44.875610
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.38
 ---- batch: 020 ----
mean loss: 101.56
 ---- batch: 030 ----
mean loss: 98.81
 ---- batch: 040 ----
mean loss: 96.23
 ---- batch: 050 ----
mean loss: 102.19
 ---- batch: 060 ----
mean loss: 97.97
 ---- batch: 070 ----
mean loss: 99.33
 ---- batch: 080 ----
mean loss: 103.66
 ---- batch: 090 ----
mean loss: 97.95
train mean loss: 99.51
epoch train time: 0:00:01.613989
elapsed time: 0:06:10.501176
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 19:36:46.490264
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.44
 ---- batch: 020 ----
mean loss: 94.48
 ---- batch: 030 ----
mean loss: 104.83
 ---- batch: 040 ----
mean loss: 99.89
 ---- batch: 050 ----
mean loss: 104.67
 ---- batch: 060 ----
mean loss: 96.97
 ---- batch: 070 ----
mean loss: 101.92
 ---- batch: 080 ----
mean loss: 98.19
 ---- batch: 090 ----
mean loss: 102.23
train mean loss: 99.53
epoch train time: 0:00:01.596397
elapsed time: 0:06:12.098265
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 19:36:48.087369
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.04
 ---- batch: 020 ----
mean loss: 98.20
 ---- batch: 030 ----
mean loss: 98.38
 ---- batch: 040 ----
mean loss: 102.44
 ---- batch: 050 ----
mean loss: 99.30
 ---- batch: 060 ----
mean loss: 97.38
 ---- batch: 070 ----
mean loss: 97.60
 ---- batch: 080 ----
mean loss: 100.17
 ---- batch: 090 ----
mean loss: 99.75
train mean loss: 99.68
epoch train time: 0:00:01.634720
elapsed time: 0:06:13.733690
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 19:36:49.722781
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.84
 ---- batch: 020 ----
mean loss: 104.10
 ---- batch: 030 ----
mean loss: 97.10
 ---- batch: 040 ----
mean loss: 99.82
 ---- batch: 050 ----
mean loss: 106.72
 ---- batch: 060 ----
mean loss: 92.17
 ---- batch: 070 ----
mean loss: 95.39
 ---- batch: 080 ----
mean loss: 103.24
 ---- batch: 090 ----
mean loss: 96.01
train mean loss: 99.48
epoch train time: 0:00:01.645487
elapsed time: 0:06:15.379835
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 19:36:51.368939
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.38
 ---- batch: 020 ----
mean loss: 100.55
 ---- batch: 030 ----
mean loss: 98.64
 ---- batch: 040 ----
mean loss: 97.01
 ---- batch: 050 ----
mean loss: 105.03
 ---- batch: 060 ----
mean loss: 99.99
 ---- batch: 070 ----
mean loss: 96.49
 ---- batch: 080 ----
mean loss: 94.89
 ---- batch: 090 ----
mean loss: 99.30
train mean loss: 99.50
epoch train time: 0:00:01.639637
elapsed time: 0:06:17.020219
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 19:36:53.009305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.79
 ---- batch: 020 ----
mean loss: 98.15
 ---- batch: 030 ----
mean loss: 101.20
 ---- batch: 040 ----
mean loss: 101.88
 ---- batch: 050 ----
mean loss: 96.35
 ---- batch: 060 ----
mean loss: 100.13
 ---- batch: 070 ----
mean loss: 99.65
 ---- batch: 080 ----
mean loss: 102.85
 ---- batch: 090 ----
mean loss: 98.58
train mean loss: 99.22
epoch train time: 0:00:01.658074
elapsed time: 0:06:18.679012
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 19:36:54.668122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.90
 ---- batch: 020 ----
mean loss: 95.20
 ---- batch: 030 ----
mean loss: 98.77
 ---- batch: 040 ----
mean loss: 102.13
 ---- batch: 050 ----
mean loss: 98.62
 ---- batch: 060 ----
mean loss: 104.07
 ---- batch: 070 ----
mean loss: 100.66
 ---- batch: 080 ----
mean loss: 97.99
 ---- batch: 090 ----
mean loss: 95.76
train mean loss: 99.11
epoch train time: 0:00:01.653695
elapsed time: 0:06:20.333437
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 19:36:56.322612
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.45
 ---- batch: 020 ----
mean loss: 96.28
 ---- batch: 030 ----
mean loss: 100.92
 ---- batch: 040 ----
mean loss: 104.07
 ---- batch: 050 ----
mean loss: 99.89
 ---- batch: 060 ----
mean loss: 97.09
 ---- batch: 070 ----
mean loss: 96.96
 ---- batch: 080 ----
mean loss: 98.10
 ---- batch: 090 ----
mean loss: 98.57
train mean loss: 99.31
epoch train time: 0:00:01.676211
elapsed time: 0:06:22.010476
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 19:36:57.999568
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.91
 ---- batch: 020 ----
mean loss: 96.19
 ---- batch: 030 ----
mean loss: 96.33
 ---- batch: 040 ----
mean loss: 101.26
 ---- batch: 050 ----
mean loss: 103.44
 ---- batch: 060 ----
mean loss: 98.87
 ---- batch: 070 ----
mean loss: 98.59
 ---- batch: 080 ----
mean loss: 101.12
 ---- batch: 090 ----
mean loss: 94.34
train mean loss: 99.34
epoch train time: 0:00:01.626528
elapsed time: 0:06:23.637605
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 19:36:59.626678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.05
 ---- batch: 020 ----
mean loss: 93.85
 ---- batch: 030 ----
mean loss: 99.96
 ---- batch: 040 ----
mean loss: 92.84
 ---- batch: 050 ----
mean loss: 98.85
 ---- batch: 060 ----
mean loss: 99.62
 ---- batch: 070 ----
mean loss: 101.15
 ---- batch: 080 ----
mean loss: 104.96
 ---- batch: 090 ----
mean loss: 102.65
train mean loss: 99.18
epoch train time: 0:00:01.651647
elapsed time: 0:06:25.289870
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 19:37:01.278953
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 102.07
 ---- batch: 020 ----
mean loss: 98.27
 ---- batch: 030 ----
mean loss: 96.07
 ---- batch: 040 ----
mean loss: 97.09
 ---- batch: 050 ----
mean loss: 98.51
 ---- batch: 060 ----
mean loss: 97.64
 ---- batch: 070 ----
mean loss: 99.70
 ---- batch: 080 ----
mean loss: 102.73
 ---- batch: 090 ----
mean loss: 98.93
train mean loss: 99.05
epoch train time: 0:00:01.648135
elapsed time: 0:06:26.938678
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 19:37:02.927789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.19
 ---- batch: 020 ----
mean loss: 99.51
 ---- batch: 030 ----
mean loss: 97.28
 ---- batch: 040 ----
mean loss: 102.77
 ---- batch: 050 ----
mean loss: 98.40
 ---- batch: 060 ----
mean loss: 96.69
 ---- batch: 070 ----
mean loss: 96.08
 ---- batch: 080 ----
mean loss: 101.53
 ---- batch: 090 ----
mean loss: 98.32
train mean loss: 98.95
epoch train time: 0:00:01.635990
elapsed time: 0:06:28.575422
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 19:37:04.564505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.50
 ---- batch: 020 ----
mean loss: 100.55
 ---- batch: 030 ----
mean loss: 99.67
 ---- batch: 040 ----
mean loss: 97.99
 ---- batch: 050 ----
mean loss: 98.79
 ---- batch: 060 ----
mean loss: 95.26
 ---- batch: 070 ----
mean loss: 101.24
 ---- batch: 080 ----
mean loss: 102.94
 ---- batch: 090 ----
mean loss: 100.31
train mean loss: 98.97
epoch train time: 0:00:01.626741
elapsed time: 0:06:30.202808
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 19:37:06.191918
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.19
 ---- batch: 020 ----
mean loss: 95.52
 ---- batch: 030 ----
mean loss: 95.73
 ---- batch: 040 ----
mean loss: 99.60
 ---- batch: 050 ----
mean loss: 98.20
 ---- batch: 060 ----
mean loss: 104.59
 ---- batch: 070 ----
mean loss: 98.30
 ---- batch: 080 ----
mean loss: 101.23
 ---- batch: 090 ----
mean loss: 103.58
train mean loss: 98.96
epoch train time: 0:00:01.642187
elapsed time: 0:06:31.845750
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 19:37:07.834870
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.94
 ---- batch: 020 ----
mean loss: 96.53
 ---- batch: 030 ----
mean loss: 101.36
 ---- batch: 040 ----
mean loss: 105.67
 ---- batch: 050 ----
mean loss: 101.17
 ---- batch: 060 ----
mean loss: 90.12
 ---- batch: 070 ----
mean loss: 97.30
 ---- batch: 080 ----
mean loss: 103.32
 ---- batch: 090 ----
mean loss: 98.47
train mean loss: 98.92
epoch train time: 0:00:01.619788
elapsed time: 0:06:33.466202
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 19:37:09.455289
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.17
 ---- batch: 020 ----
mean loss: 97.09
 ---- batch: 030 ----
mean loss: 94.82
 ---- batch: 040 ----
mean loss: 100.77
 ---- batch: 050 ----
mean loss: 98.70
 ---- batch: 060 ----
mean loss: 97.35
 ---- batch: 070 ----
mean loss: 97.23
 ---- batch: 080 ----
mean loss: 104.02
 ---- batch: 090 ----
mean loss: 98.37
train mean loss: 98.81
epoch train time: 0:00:01.639897
elapsed time: 0:06:35.106769
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 19:37:11.095850
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.94
 ---- batch: 020 ----
mean loss: 96.44
 ---- batch: 030 ----
mean loss: 97.99
 ---- batch: 040 ----
mean loss: 98.18
 ---- batch: 050 ----
mean loss: 100.24
 ---- batch: 060 ----
mean loss: 101.69
 ---- batch: 070 ----
mean loss: 97.55
 ---- batch: 080 ----
mean loss: 100.56
 ---- batch: 090 ----
mean loss: 96.58
train mean loss: 98.87
epoch train time: 0:00:01.637534
elapsed time: 0:06:36.744986
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 19:37:12.734074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.76
 ---- batch: 020 ----
mean loss: 99.18
 ---- batch: 030 ----
mean loss: 98.88
 ---- batch: 040 ----
mean loss: 95.32
 ---- batch: 050 ----
mean loss: 95.88
 ---- batch: 060 ----
mean loss: 104.97
 ---- batch: 070 ----
mean loss: 98.21
 ---- batch: 080 ----
mean loss: 102.71
 ---- batch: 090 ----
mean loss: 98.43
train mean loss: 98.85
epoch train time: 0:00:01.675019
elapsed time: 0:06:38.420770
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 19:37:14.409701
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.25
 ---- batch: 020 ----
mean loss: 100.57
 ---- batch: 030 ----
mean loss: 103.38
 ---- batch: 040 ----
mean loss: 97.66
 ---- batch: 050 ----
mean loss: 98.68
 ---- batch: 060 ----
mean loss: 97.11
 ---- batch: 070 ----
mean loss: 99.03
 ---- batch: 080 ----
mean loss: 96.50
 ---- batch: 090 ----
mean loss: 105.13
train mean loss: 98.81
epoch train time: 0:00:01.666661
elapsed time: 0:06:40.087989
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 19:37:16.077086
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.53
 ---- batch: 020 ----
mean loss: 93.08
 ---- batch: 030 ----
mean loss: 100.28
 ---- batch: 040 ----
mean loss: 95.68
 ---- batch: 050 ----
mean loss: 96.72
 ---- batch: 060 ----
mean loss: 96.09
 ---- batch: 070 ----
mean loss: 100.11
 ---- batch: 080 ----
mean loss: 106.20
 ---- batch: 090 ----
mean loss: 100.28
train mean loss: 98.77
epoch train time: 0:00:01.611438
elapsed time: 0:06:41.700074
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 19:37:17.689164
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.15
 ---- batch: 020 ----
mean loss: 102.41
 ---- batch: 030 ----
mean loss: 96.99
 ---- batch: 040 ----
mean loss: 100.65
 ---- batch: 050 ----
mean loss: 96.80
 ---- batch: 060 ----
mean loss: 97.41
 ---- batch: 070 ----
mean loss: 94.10
 ---- batch: 080 ----
mean loss: 101.60
 ---- batch: 090 ----
mean loss: 97.77
train mean loss: 98.65
epoch train time: 0:00:01.628574
elapsed time: 0:06:43.329307
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 19:37:19.318398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.28
 ---- batch: 020 ----
mean loss: 101.67
 ---- batch: 030 ----
mean loss: 98.46
 ---- batch: 040 ----
mean loss: 96.28
 ---- batch: 050 ----
mean loss: 95.87
 ---- batch: 060 ----
mean loss: 100.22
 ---- batch: 070 ----
mean loss: 96.25
 ---- batch: 080 ----
mean loss: 103.02
 ---- batch: 090 ----
mean loss: 101.18
train mean loss: 98.61
epoch train time: 0:00:01.618662
elapsed time: 0:06:44.948683
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 19:37:20.937810
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.20
 ---- batch: 020 ----
mean loss: 102.87
 ---- batch: 030 ----
mean loss: 99.89
 ---- batch: 040 ----
mean loss: 105.26
 ---- batch: 050 ----
mean loss: 99.80
 ---- batch: 060 ----
mean loss: 95.71
 ---- batch: 070 ----
mean loss: 96.69
 ---- batch: 080 ----
mean loss: 95.92
 ---- batch: 090 ----
mean loss: 96.15
train mean loss: 98.68
epoch train time: 0:00:01.606553
elapsed time: 0:06:46.555960
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 19:37:22.545061
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.74
 ---- batch: 020 ----
mean loss: 96.32
 ---- batch: 030 ----
mean loss: 100.64
 ---- batch: 040 ----
mean loss: 104.04
 ---- batch: 050 ----
mean loss: 100.02
 ---- batch: 060 ----
mean loss: 96.33
 ---- batch: 070 ----
mean loss: 95.45
 ---- batch: 080 ----
mean loss: 95.17
 ---- batch: 090 ----
mean loss: 108.79
train mean loss: 98.32
epoch train time: 0:00:01.624694
elapsed time: 0:06:48.181294
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 19:37:24.170387
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.90
 ---- batch: 020 ----
mean loss: 96.41
 ---- batch: 030 ----
mean loss: 98.12
 ---- batch: 040 ----
mean loss: 99.11
 ---- batch: 050 ----
mean loss: 101.83
 ---- batch: 060 ----
mean loss: 99.36
 ---- batch: 070 ----
mean loss: 99.40
 ---- batch: 080 ----
mean loss: 93.67
 ---- batch: 090 ----
mean loss: 103.73
train mean loss: 98.70
epoch train time: 0:00:01.663693
elapsed time: 0:06:49.845685
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 19:37:25.834785
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.74
 ---- batch: 020 ----
mean loss: 97.34
 ---- batch: 030 ----
mean loss: 101.45
 ---- batch: 040 ----
mean loss: 92.66
 ---- batch: 050 ----
mean loss: 97.95
 ---- batch: 060 ----
mean loss: 99.67
 ---- batch: 070 ----
mean loss: 99.06
 ---- batch: 080 ----
mean loss: 102.74
 ---- batch: 090 ----
mean loss: 95.93
train mean loss: 98.58
epoch train time: 0:00:01.638008
elapsed time: 0:06:51.484589
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 19:37:27.473760
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.47
 ---- batch: 020 ----
mean loss: 101.21
 ---- batch: 030 ----
mean loss: 99.66
 ---- batch: 040 ----
mean loss: 99.97
 ---- batch: 050 ----
mean loss: 96.13
 ---- batch: 060 ----
mean loss: 95.62
 ---- batch: 070 ----
mean loss: 97.67
 ---- batch: 080 ----
mean loss: 100.03
 ---- batch: 090 ----
mean loss: 98.98
train mean loss: 98.36
epoch train time: 0:00:01.644420
elapsed time: 0:06:53.129745
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 19:37:29.118829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 99.69
 ---- batch: 020 ----
mean loss: 92.93
 ---- batch: 030 ----
mean loss: 100.45
 ---- batch: 040 ----
mean loss: 92.34
 ---- batch: 050 ----
mean loss: 100.78
 ---- batch: 060 ----
mean loss: 100.18
 ---- batch: 070 ----
mean loss: 102.34
 ---- batch: 080 ----
mean loss: 98.45
 ---- batch: 090 ----
mean loss: 99.20
train mean loss: 98.40
epoch train time: 0:00:01.659330
elapsed time: 0:06:54.789715
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 19:37:30.778799
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.58
 ---- batch: 020 ----
mean loss: 96.27
 ---- batch: 030 ----
mean loss: 98.77
 ---- batch: 040 ----
mean loss: 96.89
 ---- batch: 050 ----
mean loss: 97.73
 ---- batch: 060 ----
mean loss: 94.19
 ---- batch: 070 ----
mean loss: 93.92
 ---- batch: 080 ----
mean loss: 103.81
 ---- batch: 090 ----
mean loss: 100.74
train mean loss: 98.51
epoch train time: 0:00:01.641922
elapsed time: 0:06:56.432634
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 19:37:32.421478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.89
 ---- batch: 020 ----
mean loss: 95.68
 ---- batch: 030 ----
mean loss: 95.00
 ---- batch: 040 ----
mean loss: 103.29
 ---- batch: 050 ----
mean loss: 99.20
 ---- batch: 060 ----
mean loss: 98.30
 ---- batch: 070 ----
mean loss: 98.03
 ---- batch: 080 ----
mean loss: 100.65
 ---- batch: 090 ----
mean loss: 100.13
train mean loss: 98.22
epoch train time: 0:00:01.632484
elapsed time: 0:06:58.065561
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 19:37:34.054689
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.81
 ---- batch: 020 ----
mean loss: 99.12
 ---- batch: 030 ----
mean loss: 96.73
 ---- batch: 040 ----
mean loss: 96.55
 ---- batch: 050 ----
mean loss: 97.69
 ---- batch: 060 ----
mean loss: 101.66
 ---- batch: 070 ----
mean loss: 98.92
 ---- batch: 080 ----
mean loss: 98.90
 ---- batch: 090 ----
mean loss: 99.10
train mean loss: 98.22
epoch train time: 0:00:01.611991
elapsed time: 0:06:59.678310
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 19:37:35.667399
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 97.63
 ---- batch: 020 ----
mean loss: 98.70
 ---- batch: 030 ----
mean loss: 100.18
 ---- batch: 040 ----
mean loss: 96.55
 ---- batch: 050 ----
mean loss: 99.59
 ---- batch: 060 ----
mean loss: 97.15
 ---- batch: 070 ----
mean loss: 97.75
 ---- batch: 080 ----
mean loss: 97.28
 ---- batch: 090 ----
mean loss: 99.47
train mean loss: 98.16
epoch train time: 0:00:01.626186
elapsed time: 0:07:01.305221
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 19:37:37.294297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.04
 ---- batch: 020 ----
mean loss: 95.52
 ---- batch: 030 ----
mean loss: 107.41
 ---- batch: 040 ----
mean loss: 97.57
 ---- batch: 050 ----
mean loss: 98.47
 ---- batch: 060 ----
mean loss: 95.79
 ---- batch: 070 ----
mean loss: 96.42
 ---- batch: 080 ----
mean loss: 101.91
 ---- batch: 090 ----
mean loss: 97.92
train mean loss: 98.18
epoch train time: 0:00:01.580967
elapsed time: 0:07:02.886867
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 19:37:38.875954
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.49
 ---- batch: 020 ----
mean loss: 98.02
 ---- batch: 030 ----
mean loss: 97.98
 ---- batch: 040 ----
mean loss: 96.36
 ---- batch: 050 ----
mean loss: 97.28
 ---- batch: 060 ----
mean loss: 97.48
 ---- batch: 070 ----
mean loss: 102.96
 ---- batch: 080 ----
mean loss: 97.39
 ---- batch: 090 ----
mean loss: 96.69
train mean loss: 98.17
epoch train time: 0:00:01.604171
elapsed time: 0:07:04.491749
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 19:37:40.480830
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.05
 ---- batch: 020 ----
mean loss: 99.85
 ---- batch: 030 ----
mean loss: 97.94
 ---- batch: 040 ----
mean loss: 93.81
 ---- batch: 050 ----
mean loss: 102.30
 ---- batch: 060 ----
mean loss: 96.11
 ---- batch: 070 ----
mean loss: 97.87
 ---- batch: 080 ----
mean loss: 98.25
 ---- batch: 090 ----
mean loss: 99.86
train mean loss: 98.11
epoch train time: 0:00:01.663501
elapsed time: 0:07:06.155920
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 19:37:42.145033
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 101.85
 ---- batch: 020 ----
mean loss: 99.42
 ---- batch: 030 ----
mean loss: 93.99
 ---- batch: 040 ----
mean loss: 95.27
 ---- batch: 050 ----
mean loss: 97.17
 ---- batch: 060 ----
mean loss: 100.07
 ---- batch: 070 ----
mean loss: 102.08
 ---- batch: 080 ----
mean loss: 94.66
 ---- batch: 090 ----
mean loss: 94.72
train mean loss: 98.13
epoch train time: 0:00:01.622261
elapsed time: 0:07:07.778869
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 19:37:43.767976
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.68
 ---- batch: 020 ----
mean loss: 96.60
 ---- batch: 030 ----
mean loss: 100.60
 ---- batch: 040 ----
mean loss: 94.51
 ---- batch: 050 ----
mean loss: 101.17
 ---- batch: 060 ----
mean loss: 92.12
 ---- batch: 070 ----
mean loss: 100.70
 ---- batch: 080 ----
mean loss: 97.52
 ---- batch: 090 ----
mean loss: 95.79
train mean loss: 98.28
epoch train time: 0:00:01.654619
elapsed time: 0:07:09.434208
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 19:37:45.423341
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 106.78
 ---- batch: 020 ----
mean loss: 93.71
 ---- batch: 030 ----
mean loss: 96.34
 ---- batch: 040 ----
mean loss: 93.81
 ---- batch: 050 ----
mean loss: 92.65
 ---- batch: 060 ----
mean loss: 94.24
 ---- batch: 070 ----
mean loss: 102.18
 ---- batch: 080 ----
mean loss: 105.28
 ---- batch: 090 ----
mean loss: 98.86
train mean loss: 98.01
epoch train time: 0:00:01.647374
elapsed time: 0:07:11.082327
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 19:37:47.071407
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 96.94
 ---- batch: 020 ----
mean loss: 102.23
 ---- batch: 030 ----
mean loss: 96.78
 ---- batch: 040 ----
mean loss: 94.62
 ---- batch: 050 ----
mean loss: 95.25
 ---- batch: 060 ----
mean loss: 100.50
 ---- batch: 070 ----
mean loss: 98.58
 ---- batch: 080 ----
mean loss: 94.09
 ---- batch: 090 ----
mean loss: 100.68
train mean loss: 97.92
epoch train time: 0:00:01.608411
elapsed time: 0:07:12.691447
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 19:37:48.680577
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.38
 ---- batch: 020 ----
mean loss: 103.05
 ---- batch: 030 ----
mean loss: 95.72
 ---- batch: 040 ----
mean loss: 96.35
 ---- batch: 050 ----
mean loss: 94.50
 ---- batch: 060 ----
mean loss: 96.18
 ---- batch: 070 ----
mean loss: 100.77
 ---- batch: 080 ----
mean loss: 98.38
 ---- batch: 090 ----
mean loss: 101.19
train mean loss: 98.09
epoch train time: 0:00:01.631193
elapsed time: 0:07:14.323380
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 19:37:50.312528
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 92.69
 ---- batch: 020 ----
mean loss: 99.37
 ---- batch: 030 ----
mean loss: 97.62
 ---- batch: 040 ----
mean loss: 97.06
 ---- batch: 050 ----
mean loss: 100.60
 ---- batch: 060 ----
mean loss: 97.60
 ---- batch: 070 ----
mean loss: 98.48
 ---- batch: 080 ----
mean loss: 100.14
 ---- batch: 090 ----
mean loss: 100.53
train mean loss: 97.81
epoch train time: 0:00:01.600055
elapsed time: 0:07:15.924223
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 19:37:51.913301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 98.83
 ---- batch: 020 ----
mean loss: 92.98
 ---- batch: 030 ----
mean loss: 94.22
 ---- batch: 040 ----
mean loss: 98.11
 ---- batch: 050 ----
mean loss: 94.82
 ---- batch: 060 ----
mean loss: 101.77
 ---- batch: 070 ----
mean loss: 99.19
 ---- batch: 080 ----
mean loss: 98.02
 ---- batch: 090 ----
mean loss: 103.21
train mean loss: 97.88
epoch train time: 0:00:01.597650
elapsed time: 0:07:17.522499
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 19:37:53.511626
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 95.83
 ---- batch: 020 ----
mean loss: 97.10
 ---- batch: 030 ----
mean loss: 93.67
 ---- batch: 040 ----
mean loss: 100.55
 ---- batch: 050 ----
mean loss: 102.65
 ---- batch: 060 ----
mean loss: 101.12
 ---- batch: 070 ----
mean loss: 94.62
 ---- batch: 080 ----
mean loss: 96.38
 ---- batch: 090 ----
mean loss: 99.59
train mean loss: 97.88
epoch train time: 0:00:01.621739
elapsed time: 0:07:19.144978
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 19:37:55.134066
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 100.27
 ---- batch: 020 ----
mean loss: 94.80
 ---- batch: 030 ----
mean loss: 94.00
 ---- batch: 040 ----
mean loss: 97.05
 ---- batch: 050 ----
mean loss: 101.57
 ---- batch: 060 ----
mean loss: 102.06
 ---- batch: 070 ----
mean loss: 100.24
 ---- batch: 080 ----
mean loss: 94.59
 ---- batch: 090 ----
mean loss: 95.19
train mean loss: 97.72
epoch train time: 0:00:01.579232
elapsed time: 0:07:20.724884
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 19:37:56.713966
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 94.18
 ---- batch: 020 ----
mean loss: 100.16
 ---- batch: 030 ----
mean loss: 98.43
 ---- batch: 040 ----
mean loss: 92.40
 ---- batch: 050 ----
mean loss: 95.72
 ---- batch: 060 ----
mean loss: 99.51
 ---- batch: 070 ----
mean loss: 96.59
 ---- batch: 080 ----
mean loss: 103.53
 ---- batch: 090 ----
mean loss: 100.81
train mean loss: 97.74
epoch train time: 0:00:01.607617
elapsed time: 0:07:22.341485
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_dense3/bayesian_dense3_1/checkpoint.pth.tar
**** end time: 2019-09-25 19:37:58.330285 ****
