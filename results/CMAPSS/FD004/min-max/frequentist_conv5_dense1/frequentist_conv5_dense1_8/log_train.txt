Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 16650
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-27 01:05:59.439900 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 01:05:59.449148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4578.15
 ---- batch: 020 ----
mean loss: 2498.69
 ---- batch: 030 ----
mean loss: 1385.60
 ---- batch: 040 ----
mean loss: 1315.44
 ---- batch: 050 ----
mean loss: 1157.16
 ---- batch: 060 ----
mean loss: 1096.23
 ---- batch: 070 ----
mean loss: 1064.65
 ---- batch: 080 ----
mean loss: 1029.12
 ---- batch: 090 ----
mean loss: 977.42
 ---- batch: 100 ----
mean loss: 962.47
 ---- batch: 110 ----
mean loss: 930.77
train mean loss: 1528.65
epoch train time: 0:00:34.593620
elapsed time: 0:00:34.605470
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 01:06:34.045412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.68
 ---- batch: 020 ----
mean loss: 885.60
 ---- batch: 030 ----
mean loss: 838.13
 ---- batch: 040 ----
mean loss: 833.73
 ---- batch: 050 ----
mean loss: 801.63
 ---- batch: 060 ----
mean loss: 794.77
 ---- batch: 070 ----
mean loss: 799.56
 ---- batch: 080 ----
mean loss: 776.70
 ---- batch: 090 ----
mean loss: 780.89
 ---- batch: 100 ----
mean loss: 779.93
 ---- batch: 110 ----
mean loss: 785.43
train mean loss: 814.72
epoch train time: 0:00:02.221615
elapsed time: 0:00:36.827234
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 01:06:36.267192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.20
 ---- batch: 020 ----
mean loss: 746.36
 ---- batch: 030 ----
mean loss: 727.69
 ---- batch: 040 ----
mean loss: 710.44
 ---- batch: 050 ----
mean loss: 715.05
 ---- batch: 060 ----
mean loss: 702.38
 ---- batch: 070 ----
mean loss: 728.55
 ---- batch: 080 ----
mean loss: 697.92
 ---- batch: 090 ----
mean loss: 697.24
 ---- batch: 100 ----
mean loss: 691.77
 ---- batch: 110 ----
mean loss: 677.82
train mean loss: 709.98
epoch train time: 0:00:02.137945
elapsed time: 0:00:38.965328
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 01:06:38.405277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.85
 ---- batch: 020 ----
mean loss: 683.47
 ---- batch: 030 ----
mean loss: 664.80
 ---- batch: 040 ----
mean loss: 654.54
 ---- batch: 050 ----
mean loss: 663.50
 ---- batch: 060 ----
mean loss: 642.14
 ---- batch: 070 ----
mean loss: 665.42
 ---- batch: 080 ----
mean loss: 641.39
 ---- batch: 090 ----
mean loss: 643.81
 ---- batch: 100 ----
mean loss: 638.51
 ---- batch: 110 ----
mean loss: 630.90
train mean loss: 656.05
epoch train time: 0:00:02.135970
elapsed time: 0:00:41.101445
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 01:06:40.541401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 620.25
 ---- batch: 020 ----
mean loss: 623.77
 ---- batch: 030 ----
mean loss: 639.53
 ---- batch: 040 ----
mean loss: 626.13
 ---- batch: 050 ----
mean loss: 619.99
 ---- batch: 060 ----
mean loss: 621.32
 ---- batch: 070 ----
mean loss: 611.64
 ---- batch: 080 ----
mean loss: 607.19
 ---- batch: 090 ----
mean loss: 603.46
 ---- batch: 100 ----
mean loss: 614.99
 ---- batch: 110 ----
mean loss: 609.47
train mean loss: 617.36
epoch train time: 0:00:02.138343
elapsed time: 0:00:43.239954
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 01:06:42.679929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.88
 ---- batch: 020 ----
mean loss: 607.96
 ---- batch: 030 ----
mean loss: 590.93
 ---- batch: 040 ----
mean loss: 599.13
 ---- batch: 050 ----
mean loss: 596.68
 ---- batch: 060 ----
mean loss: 595.36
 ---- batch: 070 ----
mean loss: 590.68
 ---- batch: 080 ----
mean loss: 599.69
 ---- batch: 090 ----
mean loss: 569.07
 ---- batch: 100 ----
mean loss: 571.85
 ---- batch: 110 ----
mean loss: 575.97
train mean loss: 590.78
epoch train time: 0:00:02.145299
elapsed time: 0:00:45.385441
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 01:06:44.825427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 560.31
 ---- batch: 020 ----
mean loss: 588.25
 ---- batch: 030 ----
mean loss: 575.57
 ---- batch: 040 ----
mean loss: 569.48
 ---- batch: 050 ----
mean loss: 560.41
 ---- batch: 060 ----
mean loss: 561.52
 ---- batch: 070 ----
mean loss: 557.38
 ---- batch: 080 ----
mean loss: 558.97
 ---- batch: 090 ----
mean loss: 555.12
 ---- batch: 100 ----
mean loss: 563.75
 ---- batch: 110 ----
mean loss: 565.57
train mean loss: 564.38
epoch train time: 0:00:02.140906
elapsed time: 0:00:47.526539
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 01:06:46.966495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 548.05
 ---- batch: 020 ----
mean loss: 534.18
 ---- batch: 030 ----
mean loss: 554.02
 ---- batch: 040 ----
mean loss: 553.41
 ---- batch: 050 ----
mean loss: 548.38
 ---- batch: 060 ----
mean loss: 539.85
 ---- batch: 070 ----
mean loss: 537.78
 ---- batch: 080 ----
mean loss: 525.99
 ---- batch: 090 ----
mean loss: 530.16
 ---- batch: 100 ----
mean loss: 568.13
 ---- batch: 110 ----
mean loss: 572.78
train mean loss: 546.13
epoch train time: 0:00:02.144803
elapsed time: 0:00:49.671503
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 01:06:49.111456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 540.02
 ---- batch: 020 ----
mean loss: 541.44
 ---- batch: 030 ----
mean loss: 534.12
 ---- batch: 040 ----
mean loss: 517.26
 ---- batch: 050 ----
mean loss: 533.40
 ---- batch: 060 ----
mean loss: 539.80
 ---- batch: 070 ----
mean loss: 515.60
 ---- batch: 080 ----
mean loss: 527.81
 ---- batch: 090 ----
mean loss: 525.19
 ---- batch: 100 ----
mean loss: 546.39
 ---- batch: 110 ----
mean loss: 536.28
train mean loss: 532.63
epoch train time: 0:00:02.140589
elapsed time: 0:00:51.812255
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 01:06:51.252206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.90
 ---- batch: 020 ----
mean loss: 519.12
 ---- batch: 030 ----
mean loss: 509.10
 ---- batch: 040 ----
mean loss: 530.95
 ---- batch: 050 ----
mean loss: 528.59
 ---- batch: 060 ----
mean loss: 534.19
 ---- batch: 070 ----
mean loss: 529.03
 ---- batch: 080 ----
mean loss: 553.70
 ---- batch: 090 ----
mean loss: 507.48
 ---- batch: 100 ----
mean loss: 517.04
 ---- batch: 110 ----
mean loss: 538.51
train mean loss: 526.76
epoch train time: 0:00:02.136787
elapsed time: 0:00:53.949192
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 01:06:53.389181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.59
 ---- batch: 020 ----
mean loss: 513.17
 ---- batch: 030 ----
mean loss: 520.64
 ---- batch: 040 ----
mean loss: 525.39
 ---- batch: 050 ----
mean loss: 506.44
 ---- batch: 060 ----
mean loss: 531.05
 ---- batch: 070 ----
mean loss: 529.29
 ---- batch: 080 ----
mean loss: 510.70
 ---- batch: 090 ----
mean loss: 515.08
 ---- batch: 100 ----
mean loss: 517.04
 ---- batch: 110 ----
mean loss: 500.66
train mean loss: 517.50
epoch train time: 0:00:02.144091
elapsed time: 0:00:56.093481
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 01:06:55.533438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 525.62
 ---- batch: 020 ----
mean loss: 516.52
 ---- batch: 030 ----
mean loss: 536.53
 ---- batch: 040 ----
mean loss: 539.50
 ---- batch: 050 ----
mean loss: 514.32
 ---- batch: 060 ----
mean loss: 505.67
 ---- batch: 070 ----
mean loss: 510.72
 ---- batch: 080 ----
mean loss: 512.45
 ---- batch: 090 ----
mean loss: 519.45
 ---- batch: 100 ----
mean loss: 513.69
 ---- batch: 110 ----
mean loss: 518.07
train mean loss: 519.20
epoch train time: 0:00:02.146677
elapsed time: 0:00:58.240330
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 01:06:57.680281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.09
 ---- batch: 020 ----
mean loss: 519.52
 ---- batch: 030 ----
mean loss: 518.80
 ---- batch: 040 ----
mean loss: 512.21
 ---- batch: 050 ----
mean loss: 503.46
 ---- batch: 060 ----
mean loss: 512.34
 ---- batch: 070 ----
mean loss: 496.35
 ---- batch: 080 ----
mean loss: 519.22
 ---- batch: 090 ----
mean loss: 510.84
 ---- batch: 100 ----
mean loss: 516.77
 ---- batch: 110 ----
mean loss: 503.98
train mean loss: 512.06
epoch train time: 0:00:02.147633
elapsed time: 0:01:00.388128
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 01:06:59.828098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.93
 ---- batch: 020 ----
mean loss: 510.37
 ---- batch: 030 ----
mean loss: 497.49
 ---- batch: 040 ----
mean loss: 503.50
 ---- batch: 050 ----
mean loss: 518.18
 ---- batch: 060 ----
mean loss: 503.35
 ---- batch: 070 ----
mean loss: 507.13
 ---- batch: 080 ----
mean loss: 514.83
 ---- batch: 090 ----
mean loss: 508.97
 ---- batch: 100 ----
mean loss: 513.95
 ---- batch: 110 ----
mean loss: 494.94
train mean loss: 506.87
epoch train time: 0:00:02.144295
elapsed time: 0:01:02.532587
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 01:07:01.972541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.86
 ---- batch: 020 ----
mean loss: 516.00
 ---- batch: 030 ----
mean loss: 513.52
 ---- batch: 040 ----
mean loss: 511.57
 ---- batch: 050 ----
mean loss: 510.36
 ---- batch: 060 ----
mean loss: 507.64
 ---- batch: 070 ----
mean loss: 503.17
 ---- batch: 080 ----
mean loss: 524.99
 ---- batch: 090 ----
mean loss: 521.52
 ---- batch: 100 ----
mean loss: 519.99
 ---- batch: 110 ----
mean loss: 540.16
train mean loss: 514.57
epoch train time: 0:00:02.147482
elapsed time: 0:01:04.680224
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 01:07:04.120178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.26
 ---- batch: 020 ----
mean loss: 517.08
 ---- batch: 030 ----
mean loss: 504.05
 ---- batch: 040 ----
mean loss: 503.30
 ---- batch: 050 ----
mean loss: 499.60
 ---- batch: 060 ----
mean loss: 526.06
 ---- batch: 070 ----
mean loss: 500.20
 ---- batch: 080 ----
mean loss: 483.16
 ---- batch: 090 ----
mean loss: 511.53
 ---- batch: 100 ----
mean loss: 504.80
 ---- batch: 110 ----
mean loss: 506.92
train mean loss: 506.13
epoch train time: 0:00:02.144894
elapsed time: 0:01:06.825275
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 01:07:06.265263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.27
 ---- batch: 020 ----
mean loss: 468.49
 ---- batch: 030 ----
mean loss: 496.49
 ---- batch: 040 ----
mean loss: 494.70
 ---- batch: 050 ----
mean loss: 523.04
 ---- batch: 060 ----
mean loss: 506.34
 ---- batch: 070 ----
mean loss: 498.79
 ---- batch: 080 ----
mean loss: 514.41
 ---- batch: 090 ----
mean loss: 500.17
 ---- batch: 100 ----
mean loss: 507.75
 ---- batch: 110 ----
mean loss: 496.90
train mean loss: 500.14
epoch train time: 0:00:02.149503
elapsed time: 0:01:08.974978
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 01:07:08.414939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.41
 ---- batch: 020 ----
mean loss: 510.71
 ---- batch: 030 ----
mean loss: 488.19
 ---- batch: 040 ----
mean loss: 505.43
 ---- batch: 050 ----
mean loss: 504.65
 ---- batch: 060 ----
mean loss: 490.27
 ---- batch: 070 ----
mean loss: 508.37
 ---- batch: 080 ----
mean loss: 495.19
 ---- batch: 090 ----
mean loss: 486.48
 ---- batch: 100 ----
mean loss: 523.56
 ---- batch: 110 ----
mean loss: 500.63
train mean loss: 499.87
epoch train time: 0:00:02.144762
elapsed time: 0:01:11.119898
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 01:07:10.559855
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.68
 ---- batch: 020 ----
mean loss: 505.14
 ---- batch: 030 ----
mean loss: 504.18
 ---- batch: 040 ----
mean loss: 491.35
 ---- batch: 050 ----
mean loss: 500.48
 ---- batch: 060 ----
mean loss: 507.63
 ---- batch: 070 ----
mean loss: 495.60
 ---- batch: 080 ----
mean loss: 515.91
 ---- batch: 090 ----
mean loss: 487.56
 ---- batch: 100 ----
mean loss: 507.48
 ---- batch: 110 ----
mean loss: 503.95
train mean loss: 501.09
epoch train time: 0:00:02.137451
elapsed time: 0:01:13.257534
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 01:07:12.697593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.40
 ---- batch: 020 ----
mean loss: 510.66
 ---- batch: 030 ----
mean loss: 521.19
 ---- batch: 040 ----
mean loss: 509.32
 ---- batch: 050 ----
mean loss: 498.40
 ---- batch: 060 ----
mean loss: 514.50
 ---- batch: 070 ----
mean loss: 491.50
 ---- batch: 080 ----
mean loss: 487.68
 ---- batch: 090 ----
mean loss: 490.82
 ---- batch: 100 ----
mean loss: 502.32
 ---- batch: 110 ----
mean loss: 487.73
train mean loss: 501.95
epoch train time: 0:00:02.145230
elapsed time: 0:01:15.403032
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 01:07:14.842987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.88
 ---- batch: 020 ----
mean loss: 505.51
 ---- batch: 030 ----
mean loss: 500.97
 ---- batch: 040 ----
mean loss: 493.41
 ---- batch: 050 ----
mean loss: 503.79
 ---- batch: 060 ----
mean loss: 508.89
 ---- batch: 070 ----
mean loss: 507.10
 ---- batch: 080 ----
mean loss: 506.87
 ---- batch: 090 ----
mean loss: 486.41
 ---- batch: 100 ----
mean loss: 494.01
 ---- batch: 110 ----
mean loss: 476.26
train mean loss: 498.22
epoch train time: 0:00:02.143277
elapsed time: 0:01:17.546463
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 01:07:16.986418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.95
 ---- batch: 020 ----
mean loss: 505.25
 ---- batch: 030 ----
mean loss: 488.81
 ---- batch: 040 ----
mean loss: 501.64
 ---- batch: 050 ----
mean loss: 498.02
 ---- batch: 060 ----
mean loss: 496.26
 ---- batch: 070 ----
mean loss: 499.76
 ---- batch: 080 ----
mean loss: 484.26
 ---- batch: 090 ----
mean loss: 498.47
 ---- batch: 100 ----
mean loss: 499.39
 ---- batch: 110 ----
mean loss: 495.81
train mean loss: 496.35
epoch train time: 0:00:02.141743
elapsed time: 0:01:19.688378
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 01:07:19.128325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.45
 ---- batch: 020 ----
mean loss: 493.32
 ---- batch: 030 ----
mean loss: 491.62
 ---- batch: 040 ----
mean loss: 482.58
 ---- batch: 050 ----
mean loss: 494.24
 ---- batch: 060 ----
mean loss: 502.23
 ---- batch: 070 ----
mean loss: 495.55
 ---- batch: 080 ----
mean loss: 491.87
 ---- batch: 090 ----
mean loss: 484.22
 ---- batch: 100 ----
mean loss: 488.71
 ---- batch: 110 ----
mean loss: 488.05
train mean loss: 490.82
epoch train time: 0:00:02.151160
elapsed time: 0:01:21.839709
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 01:07:21.279683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.80
 ---- batch: 020 ----
mean loss: 491.47
 ---- batch: 030 ----
mean loss: 505.99
 ---- batch: 040 ----
mean loss: 479.64
 ---- batch: 050 ----
mean loss: 514.50
 ---- batch: 060 ----
mean loss: 503.82
 ---- batch: 070 ----
mean loss: 491.60
 ---- batch: 080 ----
mean loss: 497.26
 ---- batch: 090 ----
mean loss: 479.60
 ---- batch: 100 ----
mean loss: 474.33
 ---- batch: 110 ----
mean loss: 483.42
train mean loss: 493.12
epoch train time: 0:00:02.145127
elapsed time: 0:01:23.985011
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 01:07:23.424973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.99
 ---- batch: 020 ----
mean loss: 488.38
 ---- batch: 030 ----
mean loss: 493.88
 ---- batch: 040 ----
mean loss: 510.45
 ---- batch: 050 ----
mean loss: 481.68
 ---- batch: 060 ----
mean loss: 503.90
 ---- batch: 070 ----
mean loss: 501.64
 ---- batch: 080 ----
mean loss: 503.10
 ---- batch: 090 ----
mean loss: 498.95
 ---- batch: 100 ----
mean loss: 496.26
 ---- batch: 110 ----
mean loss: 504.48
train mean loss: 496.27
epoch train time: 0:00:02.143575
elapsed time: 0:01:26.128764
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 01:07:25.568732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.82
 ---- batch: 020 ----
mean loss: 498.11
 ---- batch: 030 ----
mean loss: 492.26
 ---- batch: 040 ----
mean loss: 492.72
 ---- batch: 050 ----
mean loss: 478.84
 ---- batch: 060 ----
mean loss: 512.74
 ---- batch: 070 ----
mean loss: 499.12
 ---- batch: 080 ----
mean loss: 486.47
 ---- batch: 090 ----
mean loss: 463.90
 ---- batch: 100 ----
mean loss: 501.32
 ---- batch: 110 ----
mean loss: 496.32
train mean loss: 492.16
epoch train time: 0:00:02.146267
elapsed time: 0:01:28.275265
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 01:07:27.715244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.29
 ---- batch: 020 ----
mean loss: 492.63
 ---- batch: 030 ----
mean loss: 485.19
 ---- batch: 040 ----
mean loss: 500.08
 ---- batch: 050 ----
mean loss: 473.31
 ---- batch: 060 ----
mean loss: 470.38
 ---- batch: 070 ----
mean loss: 472.16
 ---- batch: 080 ----
mean loss: 492.47
 ---- batch: 090 ----
mean loss: 499.17
 ---- batch: 100 ----
mean loss: 482.64
 ---- batch: 110 ----
mean loss: 492.99
train mean loss: 486.33
epoch train time: 0:00:02.146315
elapsed time: 0:01:30.421811
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 01:07:29.861783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.72
 ---- batch: 020 ----
mean loss: 499.12
 ---- batch: 030 ----
mean loss: 497.56
 ---- batch: 040 ----
mean loss: 497.02
 ---- batch: 050 ----
mean loss: 482.67
 ---- batch: 060 ----
mean loss: 476.72
 ---- batch: 070 ----
mean loss: 476.95
 ---- batch: 080 ----
mean loss: 482.99
 ---- batch: 090 ----
mean loss: 497.70
 ---- batch: 100 ----
mean loss: 473.26
 ---- batch: 110 ----
mean loss: 480.49
train mean loss: 486.93
epoch train time: 0:00:02.152382
elapsed time: 0:01:32.574385
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 01:07:32.014344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.59
 ---- batch: 020 ----
mean loss: 488.02
 ---- batch: 030 ----
mean loss: 479.90
 ---- batch: 040 ----
mean loss: 488.96
 ---- batch: 050 ----
mean loss: 515.67
 ---- batch: 060 ----
mean loss: 485.96
 ---- batch: 070 ----
mean loss: 483.64
 ---- batch: 080 ----
mean loss: 503.29
 ---- batch: 090 ----
mean loss: 488.13
 ---- batch: 100 ----
mean loss: 493.52
 ---- batch: 110 ----
mean loss: 505.06
train mean loss: 492.39
epoch train time: 0:00:02.147743
elapsed time: 0:01:34.722322
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 01:07:34.162278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.81
 ---- batch: 020 ----
mean loss: 472.78
 ---- batch: 030 ----
mean loss: 472.31
 ---- batch: 040 ----
mean loss: 494.27
 ---- batch: 050 ----
mean loss: 487.68
 ---- batch: 060 ----
mean loss: 499.28
 ---- batch: 070 ----
mean loss: 475.54
 ---- batch: 080 ----
mean loss: 484.77
 ---- batch: 090 ----
mean loss: 486.21
 ---- batch: 100 ----
mean loss: 503.08
 ---- batch: 110 ----
mean loss: 510.12
train mean loss: 489.28
epoch train time: 0:00:02.146773
elapsed time: 0:01:36.869266
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 01:07:36.309222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.06
 ---- batch: 020 ----
mean loss: 482.05
 ---- batch: 030 ----
mean loss: 493.99
 ---- batch: 040 ----
mean loss: 477.33
 ---- batch: 050 ----
mean loss: 485.38
 ---- batch: 060 ----
mean loss: 501.34
 ---- batch: 070 ----
mean loss: 477.62
 ---- batch: 080 ----
mean loss: 510.92
 ---- batch: 090 ----
mean loss: 495.18
 ---- batch: 100 ----
mean loss: 499.76
 ---- batch: 110 ----
mean loss: 490.41
train mean loss: 490.10
epoch train time: 0:00:02.146431
elapsed time: 0:01:39.015851
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 01:07:38.455804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.40
 ---- batch: 020 ----
mean loss: 510.65
 ---- batch: 030 ----
mean loss: 494.34
 ---- batch: 040 ----
mean loss: 483.96
 ---- batch: 050 ----
mean loss: 479.07
 ---- batch: 060 ----
mean loss: 491.73
 ---- batch: 070 ----
mean loss: 463.60
 ---- batch: 080 ----
mean loss: 484.33
 ---- batch: 090 ----
mean loss: 482.00
 ---- batch: 100 ----
mean loss: 499.27
 ---- batch: 110 ----
mean loss: 486.47
train mean loss: 486.75
epoch train time: 0:00:02.137428
elapsed time: 0:01:41.153432
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 01:07:40.593405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.40
 ---- batch: 020 ----
mean loss: 486.58
 ---- batch: 030 ----
mean loss: 489.73
 ---- batch: 040 ----
mean loss: 480.10
 ---- batch: 050 ----
mean loss: 486.93
 ---- batch: 060 ----
mean loss: 489.56
 ---- batch: 070 ----
mean loss: 486.93
 ---- batch: 080 ----
mean loss: 494.81
 ---- batch: 090 ----
mean loss: 486.72
 ---- batch: 100 ----
mean loss: 492.14
 ---- batch: 110 ----
mean loss: 475.13
train mean loss: 486.18
epoch train time: 0:00:02.144479
elapsed time: 0:01:43.298112
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 01:07:42.738066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.18
 ---- batch: 020 ----
mean loss: 492.99
 ---- batch: 030 ----
mean loss: 499.80
 ---- batch: 040 ----
mean loss: 478.84
 ---- batch: 050 ----
mean loss: 479.36
 ---- batch: 060 ----
mean loss: 478.11
 ---- batch: 070 ----
mean loss: 491.43
 ---- batch: 080 ----
mean loss: 480.40
 ---- batch: 090 ----
mean loss: 483.47
 ---- batch: 100 ----
mean loss: 483.56
 ---- batch: 110 ----
mean loss: 487.30
train mean loss: 486.83
epoch train time: 0:00:02.143776
elapsed time: 0:01:45.442047
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 01:07:44.882005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.31
 ---- batch: 020 ----
mean loss: 468.39
 ---- batch: 030 ----
mean loss: 482.08
 ---- batch: 040 ----
mean loss: 495.91
 ---- batch: 050 ----
mean loss: 480.33
 ---- batch: 060 ----
mean loss: 484.04
 ---- batch: 070 ----
mean loss: 472.32
 ---- batch: 080 ----
mean loss: 512.13
 ---- batch: 090 ----
mean loss: 486.57
 ---- batch: 100 ----
mean loss: 485.92
 ---- batch: 110 ----
mean loss: 496.21
train mean loss: 486.86
epoch train time: 0:00:02.139082
elapsed time: 0:01:47.581302
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 01:07:47.021256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.35
 ---- batch: 020 ----
mean loss: 480.10
 ---- batch: 030 ----
mean loss: 473.08
 ---- batch: 040 ----
mean loss: 479.12
 ---- batch: 050 ----
mean loss: 477.48
 ---- batch: 060 ----
mean loss: 477.89
 ---- batch: 070 ----
mean loss: 476.84
 ---- batch: 080 ----
mean loss: 487.86
 ---- batch: 090 ----
mean loss: 465.78
 ---- batch: 100 ----
mean loss: 497.84
 ---- batch: 110 ----
mean loss: 479.35
train mean loss: 481.10
epoch train time: 0:00:02.147482
elapsed time: 0:01:49.728945
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 01:07:49.168900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.04
 ---- batch: 020 ----
mean loss: 476.39
 ---- batch: 030 ----
mean loss: 484.43
 ---- batch: 040 ----
mean loss: 479.99
 ---- batch: 050 ----
mean loss: 489.43
 ---- batch: 060 ----
mean loss: 482.47
 ---- batch: 070 ----
mean loss: 474.21
 ---- batch: 080 ----
mean loss: 479.56
 ---- batch: 090 ----
mean loss: 478.70
 ---- batch: 100 ----
mean loss: 475.56
 ---- batch: 110 ----
mean loss: 494.87
train mean loss: 482.48
epoch train time: 0:00:02.143122
elapsed time: 0:01:51.872223
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 01:07:51.312179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.01
 ---- batch: 020 ----
mean loss: 493.40
 ---- batch: 030 ----
mean loss: 486.76
 ---- batch: 040 ----
mean loss: 471.88
 ---- batch: 050 ----
mean loss: 501.34
 ---- batch: 060 ----
mean loss: 478.14
 ---- batch: 070 ----
mean loss: 480.98
 ---- batch: 080 ----
mean loss: 475.36
 ---- batch: 090 ----
mean loss: 478.72
 ---- batch: 100 ----
mean loss: 475.97
 ---- batch: 110 ----
mean loss: 501.96
train mean loss: 484.38
epoch train time: 0:00:02.147723
elapsed time: 0:01:54.020120
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 01:07:53.460073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.97
 ---- batch: 020 ----
mean loss: 492.01
 ---- batch: 030 ----
mean loss: 477.57
 ---- batch: 040 ----
mean loss: 485.99
 ---- batch: 050 ----
mean loss: 474.73
 ---- batch: 060 ----
mean loss: 481.29
 ---- batch: 070 ----
mean loss: 467.27
 ---- batch: 080 ----
mean loss: 484.73
 ---- batch: 090 ----
mean loss: 484.58
 ---- batch: 100 ----
mean loss: 490.43
 ---- batch: 110 ----
mean loss: 477.65
train mean loss: 481.21
epoch train time: 0:00:02.147890
elapsed time: 0:01:56.168173
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 01:07:55.608148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.34
 ---- batch: 020 ----
mean loss: 479.58
 ---- batch: 030 ----
mean loss: 491.89
 ---- batch: 040 ----
mean loss: 470.62
 ---- batch: 050 ----
mean loss: 476.89
 ---- batch: 060 ----
mean loss: 481.74
 ---- batch: 070 ----
mean loss: 483.34
 ---- batch: 080 ----
mean loss: 502.56
 ---- batch: 090 ----
mean loss: 501.46
 ---- batch: 100 ----
mean loss: 496.69
 ---- batch: 110 ----
mean loss: 476.04
train mean loss: 483.73
epoch train time: 0:00:02.152499
elapsed time: 0:01:58.320845
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 01:07:57.760801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.46
 ---- batch: 020 ----
mean loss: 471.61
 ---- batch: 030 ----
mean loss: 471.89
 ---- batch: 040 ----
mean loss: 479.14
 ---- batch: 050 ----
mean loss: 480.91
 ---- batch: 060 ----
mean loss: 476.56
 ---- batch: 070 ----
mean loss: 485.16
 ---- batch: 080 ----
mean loss: 476.89
 ---- batch: 090 ----
mean loss: 485.27
 ---- batch: 100 ----
mean loss: 466.26
 ---- batch: 110 ----
mean loss: 475.50
train mean loss: 477.87
epoch train time: 0:00:02.144099
elapsed time: 0:02:00.465136
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 01:07:59.905091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.33
 ---- batch: 020 ----
mean loss: 473.22
 ---- batch: 030 ----
mean loss: 468.93
 ---- batch: 040 ----
mean loss: 472.82
 ---- batch: 050 ----
mean loss: 485.55
 ---- batch: 060 ----
mean loss: 467.87
 ---- batch: 070 ----
mean loss: 479.86
 ---- batch: 080 ----
mean loss: 488.96
 ---- batch: 090 ----
mean loss: 463.21
 ---- batch: 100 ----
mean loss: 476.83
 ---- batch: 110 ----
mean loss: 468.37
train mean loss: 474.70
epoch train time: 0:00:02.141781
elapsed time: 0:02:02.607074
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 01:08:02.047029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.17
 ---- batch: 020 ----
mean loss: 470.39
 ---- batch: 030 ----
mean loss: 471.22
 ---- batch: 040 ----
mean loss: 482.16
 ---- batch: 050 ----
mean loss: 484.23
 ---- batch: 060 ----
mean loss: 469.05
 ---- batch: 070 ----
mean loss: 480.04
 ---- batch: 080 ----
mean loss: 468.08
 ---- batch: 090 ----
mean loss: 475.18
 ---- batch: 100 ----
mean loss: 467.93
 ---- batch: 110 ----
mean loss: 492.72
train mean loss: 476.63
epoch train time: 0:00:02.146058
elapsed time: 0:02:04.753305
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 01:08:04.193282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.86
 ---- batch: 020 ----
mean loss: 477.53
 ---- batch: 030 ----
mean loss: 473.02
 ---- batch: 040 ----
mean loss: 477.37
 ---- batch: 050 ----
mean loss: 472.40
 ---- batch: 060 ----
mean loss: 460.07
 ---- batch: 070 ----
mean loss: 455.33
 ---- batch: 080 ----
mean loss: 478.89
 ---- batch: 090 ----
mean loss: 469.40
 ---- batch: 100 ----
mean loss: 467.59
 ---- batch: 110 ----
mean loss: 472.69
train mean loss: 472.85
epoch train time: 0:00:02.145919
elapsed time: 0:02:06.899401
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 01:08:06.339357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.18
 ---- batch: 020 ----
mean loss: 481.84
 ---- batch: 030 ----
mean loss: 467.71
 ---- batch: 040 ----
mean loss: 476.94
 ---- batch: 050 ----
mean loss: 468.19
 ---- batch: 060 ----
mean loss: 496.33
 ---- batch: 070 ----
mean loss: 458.27
 ---- batch: 080 ----
mean loss: 461.41
 ---- batch: 090 ----
mean loss: 471.30
 ---- batch: 100 ----
mean loss: 458.62
 ---- batch: 110 ----
mean loss: 464.03
train mean loss: 470.81
epoch train time: 0:00:02.143353
elapsed time: 0:02:09.042908
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 01:08:08.482862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.81
 ---- batch: 020 ----
mean loss: 466.89
 ---- batch: 030 ----
mean loss: 481.76
 ---- batch: 040 ----
mean loss: 471.18
 ---- batch: 050 ----
mean loss: 470.92
 ---- batch: 060 ----
mean loss: 448.36
 ---- batch: 070 ----
mean loss: 470.34
 ---- batch: 080 ----
mean loss: 463.35
 ---- batch: 090 ----
mean loss: 454.70
 ---- batch: 100 ----
mean loss: 468.86
 ---- batch: 110 ----
mean loss: 475.12
train mean loss: 467.34
epoch train time: 0:00:02.137689
elapsed time: 0:02:11.180775
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 01:08:10.620748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.03
 ---- batch: 020 ----
mean loss: 459.58
 ---- batch: 030 ----
mean loss: 452.83
 ---- batch: 040 ----
mean loss: 471.14
 ---- batch: 050 ----
mean loss: 471.54
 ---- batch: 060 ----
mean loss: 474.60
 ---- batch: 070 ----
mean loss: 483.86
 ---- batch: 080 ----
mean loss: 460.27
 ---- batch: 090 ----
mean loss: 465.55
 ---- batch: 100 ----
mean loss: 460.56
 ---- batch: 110 ----
mean loss: 468.12
train mean loss: 466.89
epoch train time: 0:00:02.144982
elapsed time: 0:02:13.325981
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 01:08:12.765948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 443.41
 ---- batch: 020 ----
mean loss: 482.30
 ---- batch: 030 ----
mean loss: 474.25
 ---- batch: 040 ----
mean loss: 464.09
 ---- batch: 050 ----
mean loss: 461.79
 ---- batch: 060 ----
mean loss: 469.47
 ---- batch: 070 ----
mean loss: 474.06
 ---- batch: 080 ----
mean loss: 457.85
 ---- batch: 090 ----
mean loss: 480.67
 ---- batch: 100 ----
mean loss: 450.23
 ---- batch: 110 ----
mean loss: 457.37
train mean loss: 465.21
epoch train time: 0:00:02.141286
elapsed time: 0:02:15.467459
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 01:08:14.907413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.52
 ---- batch: 020 ----
mean loss: 465.01
 ---- batch: 030 ----
mean loss: 476.80
 ---- batch: 040 ----
mean loss: 467.08
 ---- batch: 050 ----
mean loss: 463.58
 ---- batch: 060 ----
mean loss: 468.75
 ---- batch: 070 ----
mean loss: 469.39
 ---- batch: 080 ----
mean loss: 465.30
 ---- batch: 090 ----
mean loss: 467.80
 ---- batch: 100 ----
mean loss: 460.12
 ---- batch: 110 ----
mean loss: 454.69
train mean loss: 466.12
epoch train time: 0:00:02.148161
elapsed time: 0:02:17.615772
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 01:08:17.055731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.70
 ---- batch: 020 ----
mean loss: 466.56
 ---- batch: 030 ----
mean loss: 457.59
 ---- batch: 040 ----
mean loss: 471.85
 ---- batch: 050 ----
mean loss: 467.96
 ---- batch: 060 ----
mean loss: 437.78
 ---- batch: 070 ----
mean loss: 447.71
 ---- batch: 080 ----
mean loss: 455.97
 ---- batch: 090 ----
mean loss: 474.02
 ---- batch: 100 ----
mean loss: 462.92
 ---- batch: 110 ----
mean loss: 436.17
train mean loss: 459.50
epoch train time: 0:00:02.148115
elapsed time: 0:02:19.764068
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 01:08:19.204039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.26
 ---- batch: 020 ----
mean loss: 449.99
 ---- batch: 030 ----
mean loss: 440.64
 ---- batch: 040 ----
mean loss: 449.78
 ---- batch: 050 ----
mean loss: 450.76
 ---- batch: 060 ----
mean loss: 445.50
 ---- batch: 070 ----
mean loss: 462.64
 ---- batch: 080 ----
mean loss: 452.50
 ---- batch: 090 ----
mean loss: 448.18
 ---- batch: 100 ----
mean loss: 443.33
 ---- batch: 110 ----
mean loss: 463.64
train mean loss: 451.54
epoch train time: 0:00:02.146920
elapsed time: 0:02:21.911157
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 01:08:21.351111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.29
 ---- batch: 020 ----
mean loss: 448.37
 ---- batch: 030 ----
mean loss: 438.30
 ---- batch: 040 ----
mean loss: 443.34
 ---- batch: 050 ----
mean loss: 434.46
 ---- batch: 060 ----
mean loss: 434.60
 ---- batch: 070 ----
mean loss: 441.47
 ---- batch: 080 ----
mean loss: 433.96
 ---- batch: 090 ----
mean loss: 445.17
 ---- batch: 100 ----
mean loss: 445.11
 ---- batch: 110 ----
mean loss: 430.25
train mean loss: 438.77
epoch train time: 0:00:02.140799
elapsed time: 0:02:24.052106
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 01:08:23.492079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.37
 ---- batch: 020 ----
mean loss: 428.36
 ---- batch: 030 ----
mean loss: 432.71
 ---- batch: 040 ----
mean loss: 423.56
 ---- batch: 050 ----
mean loss: 429.79
 ---- batch: 060 ----
mean loss: 431.79
 ---- batch: 070 ----
mean loss: 442.17
 ---- batch: 080 ----
mean loss: 431.27
 ---- batch: 090 ----
mean loss: 434.10
 ---- batch: 100 ----
mean loss: 419.94
 ---- batch: 110 ----
mean loss: 434.57
train mean loss: 432.28
epoch train time: 0:00:02.145285
elapsed time: 0:02:26.197571
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 01:08:25.637525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.12
 ---- batch: 020 ----
mean loss: 431.01
 ---- batch: 030 ----
mean loss: 433.45
 ---- batch: 040 ----
mean loss: 435.31
 ---- batch: 050 ----
mean loss: 433.03
 ---- batch: 060 ----
mean loss: 425.02
 ---- batch: 070 ----
mean loss: 426.65
 ---- batch: 080 ----
mean loss: 421.19
 ---- batch: 090 ----
mean loss: 422.30
 ---- batch: 100 ----
mean loss: 420.11
 ---- batch: 110 ----
mean loss: 416.34
train mean loss: 426.09
epoch train time: 0:00:02.145688
elapsed time: 0:02:28.343413
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 01:08:27.783384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.56
 ---- batch: 020 ----
mean loss: 412.45
 ---- batch: 030 ----
mean loss: 433.35
 ---- batch: 040 ----
mean loss: 425.25
 ---- batch: 050 ----
mean loss: 427.59
 ---- batch: 060 ----
mean loss: 405.86
 ---- batch: 070 ----
mean loss: 414.83
 ---- batch: 080 ----
mean loss: 426.29
 ---- batch: 090 ----
mean loss: 416.53
 ---- batch: 100 ----
mean loss: 412.43
 ---- batch: 110 ----
mean loss: 423.31
train mean loss: 419.98
epoch train time: 0:00:02.141545
elapsed time: 0:02:30.485125
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 01:08:29.925078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.14
 ---- batch: 020 ----
mean loss: 415.63
 ---- batch: 030 ----
mean loss: 425.64
 ---- batch: 040 ----
mean loss: 411.26
 ---- batch: 050 ----
mean loss: 416.70
 ---- batch: 060 ----
mean loss: 411.20
 ---- batch: 070 ----
mean loss: 417.06
 ---- batch: 080 ----
mean loss: 415.15
 ---- batch: 090 ----
mean loss: 411.67
 ---- batch: 100 ----
mean loss: 411.27
 ---- batch: 110 ----
mean loss: 417.02
train mean loss: 415.23
epoch train time: 0:00:02.151220
elapsed time: 0:02:32.636515
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 01:08:32.076498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.51
 ---- batch: 020 ----
mean loss: 408.22
 ---- batch: 030 ----
mean loss: 416.79
 ---- batch: 040 ----
mean loss: 403.36
 ---- batch: 050 ----
mean loss: 428.65
 ---- batch: 060 ----
mean loss: 414.95
 ---- batch: 070 ----
mean loss: 407.65
 ---- batch: 080 ----
mean loss: 395.58
 ---- batch: 090 ----
mean loss: 414.92
 ---- batch: 100 ----
mean loss: 399.24
 ---- batch: 110 ----
mean loss: 406.67
train mean loss: 409.77
epoch train time: 0:00:02.160512
elapsed time: 0:02:34.797216
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 01:08:34.237177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.34
 ---- batch: 020 ----
mean loss: 416.21
 ---- batch: 030 ----
mean loss: 413.84
 ---- batch: 040 ----
mean loss: 399.17
 ---- batch: 050 ----
mean loss: 410.03
 ---- batch: 060 ----
mean loss: 409.13
 ---- batch: 070 ----
mean loss: 404.54
 ---- batch: 080 ----
mean loss: 409.76
 ---- batch: 090 ----
mean loss: 421.02
 ---- batch: 100 ----
mean loss: 400.93
 ---- batch: 110 ----
mean loss: 429.16
train mean loss: 411.12
epoch train time: 0:00:02.148335
elapsed time: 0:02:36.945735
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 01:08:36.385692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.67
 ---- batch: 020 ----
mean loss: 430.14
 ---- batch: 030 ----
mean loss: 423.92
 ---- batch: 040 ----
mean loss: 407.17
 ---- batch: 050 ----
mean loss: 402.59
 ---- batch: 060 ----
mean loss: 391.23
 ---- batch: 070 ----
mean loss: 408.03
 ---- batch: 080 ----
mean loss: 408.82
 ---- batch: 090 ----
mean loss: 407.25
 ---- batch: 100 ----
mean loss: 415.43
 ---- batch: 110 ----
mean loss: 409.74
train mean loss: 412.39
epoch train time: 0:00:02.149385
elapsed time: 0:02:39.095293
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 01:08:38.535268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.82
 ---- batch: 020 ----
mean loss: 404.16
 ---- batch: 030 ----
mean loss: 403.82
 ---- batch: 040 ----
mean loss: 398.25
 ---- batch: 050 ----
mean loss: 404.09
 ---- batch: 060 ----
mean loss: 401.71
 ---- batch: 070 ----
mean loss: 393.52
 ---- batch: 080 ----
mean loss: 404.41
 ---- batch: 090 ----
mean loss: 403.84
 ---- batch: 100 ----
mean loss: 409.35
 ---- batch: 110 ----
mean loss: 412.82
train mean loss: 404.57
epoch train time: 0:00:02.146469
elapsed time: 0:02:41.241948
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 01:08:40.681906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.87
 ---- batch: 020 ----
mean loss: 399.15
 ---- batch: 030 ----
mean loss: 405.89
 ---- batch: 040 ----
mean loss: 399.37
 ---- batch: 050 ----
mean loss: 404.09
 ---- batch: 060 ----
mean loss: 403.00
 ---- batch: 070 ----
mean loss: 401.09
 ---- batch: 080 ----
mean loss: 404.85
 ---- batch: 090 ----
mean loss: 401.37
 ---- batch: 100 ----
mean loss: 391.03
 ---- batch: 110 ----
mean loss: 415.93
train mean loss: 405.54
epoch train time: 0:00:02.141173
elapsed time: 0:02:43.383332
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 01:08:42.823302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.82
 ---- batch: 020 ----
mean loss: 398.31
 ---- batch: 030 ----
mean loss: 400.27
 ---- batch: 040 ----
mean loss: 387.85
 ---- batch: 050 ----
mean loss: 399.22
 ---- batch: 060 ----
mean loss: 396.48
 ---- batch: 070 ----
mean loss: 406.77
 ---- batch: 080 ----
mean loss: 404.45
 ---- batch: 090 ----
mean loss: 399.37
 ---- batch: 100 ----
mean loss: 398.83
 ---- batch: 110 ----
mean loss: 416.04
train mean loss: 400.44
epoch train time: 0:00:02.152672
elapsed time: 0:02:45.536223
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 01:08:44.976206
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.43
 ---- batch: 020 ----
mean loss: 401.90
 ---- batch: 030 ----
mean loss: 393.97
 ---- batch: 040 ----
mean loss: 399.62
 ---- batch: 050 ----
mean loss: 398.93
 ---- batch: 060 ----
mean loss: 399.73
 ---- batch: 070 ----
mean loss: 394.12
 ---- batch: 080 ----
mean loss: 411.51
 ---- batch: 090 ----
mean loss: 413.32
 ---- batch: 100 ----
mean loss: 392.28
 ---- batch: 110 ----
mean loss: 397.41
train mean loss: 401.30
epoch train time: 0:00:02.142419
elapsed time: 0:02:47.678832
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 01:08:47.118801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.32
 ---- batch: 020 ----
mean loss: 394.86
 ---- batch: 030 ----
mean loss: 397.35
 ---- batch: 040 ----
mean loss: 399.36
 ---- batch: 050 ----
mean loss: 396.05
 ---- batch: 060 ----
mean loss: 398.57
 ---- batch: 070 ----
mean loss: 398.13
 ---- batch: 080 ----
mean loss: 390.28
 ---- batch: 090 ----
mean loss: 392.06
 ---- batch: 100 ----
mean loss: 398.36
 ---- batch: 110 ----
mean loss: 417.50
train mean loss: 398.30
epoch train time: 0:00:02.140610
elapsed time: 0:02:49.819642
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 01:08:49.259593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.95
 ---- batch: 020 ----
mean loss: 406.06
 ---- batch: 030 ----
mean loss: 403.29
 ---- batch: 040 ----
mean loss: 394.94
 ---- batch: 050 ----
mean loss: 402.97
 ---- batch: 060 ----
mean loss: 402.38
 ---- batch: 070 ----
mean loss: 399.53
 ---- batch: 080 ----
mean loss: 395.08
 ---- batch: 090 ----
mean loss: 396.37
 ---- batch: 100 ----
mean loss: 403.71
 ---- batch: 110 ----
mean loss: 393.51
train mean loss: 400.07
epoch train time: 0:00:02.141272
elapsed time: 0:02:51.961078
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 01:08:51.401056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.37
 ---- batch: 020 ----
mean loss: 386.98
 ---- batch: 030 ----
mean loss: 391.69
 ---- batch: 040 ----
mean loss: 411.66
 ---- batch: 050 ----
mean loss: 411.76
 ---- batch: 060 ----
mean loss: 393.82
 ---- batch: 070 ----
mean loss: 406.65
 ---- batch: 080 ----
mean loss: 405.27
 ---- batch: 090 ----
mean loss: 379.42
 ---- batch: 100 ----
mean loss: 390.05
 ---- batch: 110 ----
mean loss: 389.60
train mean loss: 397.46
epoch train time: 0:00:02.141276
elapsed time: 0:02:54.102560
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 01:08:53.542537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.90
 ---- batch: 020 ----
mean loss: 396.78
 ---- batch: 030 ----
mean loss: 395.01
 ---- batch: 040 ----
mean loss: 404.19
 ---- batch: 050 ----
mean loss: 378.63
 ---- batch: 060 ----
mean loss: 409.26
 ---- batch: 070 ----
mean loss: 393.98
 ---- batch: 080 ----
mean loss: 398.51
 ---- batch: 090 ----
mean loss: 396.64
 ---- batch: 100 ----
mean loss: 386.46
 ---- batch: 110 ----
mean loss: 384.12
train mean loss: 394.61
epoch train time: 0:00:02.152317
elapsed time: 0:02:56.255068
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 01:08:55.695022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.70
 ---- batch: 020 ----
mean loss: 379.57
 ---- batch: 030 ----
mean loss: 383.73
 ---- batch: 040 ----
mean loss: 397.24
 ---- batch: 050 ----
mean loss: 390.61
 ---- batch: 060 ----
mean loss: 389.91
 ---- batch: 070 ----
mean loss: 383.50
 ---- batch: 080 ----
mean loss: 395.27
 ---- batch: 090 ----
mean loss: 416.04
 ---- batch: 100 ----
mean loss: 401.62
 ---- batch: 110 ----
mean loss: 416.17
train mean loss: 395.55
epoch train time: 0:00:02.149076
elapsed time: 0:02:58.404301
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 01:08:57.844256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.95
 ---- batch: 020 ----
mean loss: 397.78
 ---- batch: 030 ----
mean loss: 397.00
 ---- batch: 040 ----
mean loss: 408.80
 ---- batch: 050 ----
mean loss: 423.38
 ---- batch: 060 ----
mean loss: 407.77
 ---- batch: 070 ----
mean loss: 406.59
 ---- batch: 080 ----
mean loss: 389.40
 ---- batch: 090 ----
mean loss: 394.17
 ---- batch: 100 ----
mean loss: 386.70
 ---- batch: 110 ----
mean loss: 397.09
train mean loss: 400.27
epoch train time: 0:00:02.142867
elapsed time: 0:03:00.547334
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 01:08:59.987289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.91
 ---- batch: 020 ----
mean loss: 396.77
 ---- batch: 030 ----
mean loss: 398.67
 ---- batch: 040 ----
mean loss: 396.87
 ---- batch: 050 ----
mean loss: 406.59
 ---- batch: 060 ----
mean loss: 391.96
 ---- batch: 070 ----
mean loss: 385.31
 ---- batch: 080 ----
mean loss: 401.61
 ---- batch: 090 ----
mean loss: 392.38
 ---- batch: 100 ----
mean loss: 387.65
 ---- batch: 110 ----
mean loss: 404.52
train mean loss: 396.68
epoch train time: 0:00:02.146141
elapsed time: 0:03:02.693638
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 01:09:02.133620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.02
 ---- batch: 020 ----
mean loss: 397.67
 ---- batch: 030 ----
mean loss: 392.53
 ---- batch: 040 ----
mean loss: 399.63
 ---- batch: 050 ----
mean loss: 383.58
 ---- batch: 060 ----
mean loss: 383.59
 ---- batch: 070 ----
mean loss: 391.96
 ---- batch: 080 ----
mean loss: 387.10
 ---- batch: 090 ----
mean loss: 389.17
 ---- batch: 100 ----
mean loss: 386.88
 ---- batch: 110 ----
mean loss: 383.79
train mean loss: 390.16
epoch train time: 0:00:02.149034
elapsed time: 0:03:04.842859
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 01:09:04.282814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.00
 ---- batch: 020 ----
mean loss: 389.40
 ---- batch: 030 ----
mean loss: 393.48
 ---- batch: 040 ----
mean loss: 383.75
 ---- batch: 050 ----
mean loss: 374.16
 ---- batch: 060 ----
mean loss: 403.01
 ---- batch: 070 ----
mean loss: 394.64
 ---- batch: 080 ----
mean loss: 406.74
 ---- batch: 090 ----
mean loss: 390.18
 ---- batch: 100 ----
mean loss: 382.73
 ---- batch: 110 ----
mean loss: 392.66
train mean loss: 390.79
epoch train time: 0:00:02.148879
elapsed time: 0:03:06.991940
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 01:09:06.431926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.92
 ---- batch: 020 ----
mean loss: 386.05
 ---- batch: 030 ----
mean loss: 378.34
 ---- batch: 040 ----
mean loss: 399.81
 ---- batch: 050 ----
mean loss: 391.68
 ---- batch: 060 ----
mean loss: 387.85
 ---- batch: 070 ----
mean loss: 378.83
 ---- batch: 080 ----
mean loss: 393.86
 ---- batch: 090 ----
mean loss: 389.62
 ---- batch: 100 ----
mean loss: 383.34
 ---- batch: 110 ----
mean loss: 385.17
train mean loss: 387.79
epoch train time: 0:00:02.148117
elapsed time: 0:03:09.140244
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 01:09:08.580200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.42
 ---- batch: 020 ----
mean loss: 387.79
 ---- batch: 030 ----
mean loss: 381.85
 ---- batch: 040 ----
mean loss: 369.15
 ---- batch: 050 ----
mean loss: 380.28
 ---- batch: 060 ----
mean loss: 389.12
 ---- batch: 070 ----
mean loss: 384.19
 ---- batch: 080 ----
mean loss: 388.51
 ---- batch: 090 ----
mean loss: 387.12
 ---- batch: 100 ----
mean loss: 383.85
 ---- batch: 110 ----
mean loss: 388.58
train mean loss: 385.09
epoch train time: 0:00:02.144715
elapsed time: 0:03:11.285119
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 01:09:10.725075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.12
 ---- batch: 020 ----
mean loss: 374.60
 ---- batch: 030 ----
mean loss: 379.58
 ---- batch: 040 ----
mean loss: 369.70
 ---- batch: 050 ----
mean loss: 381.00
 ---- batch: 060 ----
mean loss: 389.49
 ---- batch: 070 ----
mean loss: 376.18
 ---- batch: 080 ----
mean loss: 392.68
 ---- batch: 090 ----
mean loss: 385.35
 ---- batch: 100 ----
mean loss: 384.89
 ---- batch: 110 ----
mean loss: 376.72
train mean loss: 381.96
epoch train time: 0:00:02.146218
elapsed time: 0:03:13.431488
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 01:09:12.871450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.79
 ---- batch: 020 ----
mean loss: 387.12
 ---- batch: 030 ----
mean loss: 382.90
 ---- batch: 040 ----
mean loss: 377.89
 ---- batch: 050 ----
mean loss: 383.85
 ---- batch: 060 ----
mean loss: 364.14
 ---- batch: 070 ----
mean loss: 372.27
 ---- batch: 080 ----
mean loss: 377.04
 ---- batch: 090 ----
mean loss: 379.99
 ---- batch: 100 ----
mean loss: 391.08
 ---- batch: 110 ----
mean loss: 385.85
train mean loss: 379.23
epoch train time: 0:00:02.138850
elapsed time: 0:03:15.570492
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 01:09:15.010440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.58
 ---- batch: 020 ----
mean loss: 381.60
 ---- batch: 030 ----
mean loss: 384.32
 ---- batch: 040 ----
mean loss: 384.50
 ---- batch: 050 ----
mean loss: 370.11
 ---- batch: 060 ----
mean loss: 382.12
 ---- batch: 070 ----
mean loss: 383.29
 ---- batch: 080 ----
mean loss: 381.36
 ---- batch: 090 ----
mean loss: 370.92
 ---- batch: 100 ----
mean loss: 359.81
 ---- batch: 110 ----
mean loss: 386.00
train mean loss: 377.52
epoch train time: 0:00:02.139109
elapsed time: 0:03:17.709790
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 01:09:17.149741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.47
 ---- batch: 020 ----
mean loss: 382.54
 ---- batch: 030 ----
mean loss: 384.37
 ---- batch: 040 ----
mean loss: 385.31
 ---- batch: 050 ----
mean loss: 365.42
 ---- batch: 060 ----
mean loss: 384.02
 ---- batch: 070 ----
mean loss: 378.45
 ---- batch: 080 ----
mean loss: 365.25
 ---- batch: 090 ----
mean loss: 376.07
 ---- batch: 100 ----
mean loss: 391.85
 ---- batch: 110 ----
mean loss: 370.79
train mean loss: 379.13
epoch train time: 0:00:02.146075
elapsed time: 0:03:19.856054
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 01:09:19.296011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.27
 ---- batch: 020 ----
mean loss: 369.60
 ---- batch: 030 ----
mean loss: 367.28
 ---- batch: 040 ----
mean loss: 377.64
 ---- batch: 050 ----
mean loss: 383.37
 ---- batch: 060 ----
mean loss: 372.14
 ---- batch: 070 ----
mean loss: 374.93
 ---- batch: 080 ----
mean loss: 375.42
 ---- batch: 090 ----
mean loss: 371.11
 ---- batch: 100 ----
mean loss: 365.34
 ---- batch: 110 ----
mean loss: 379.84
train mean loss: 374.34
epoch train time: 0:00:02.140007
elapsed time: 0:03:21.996221
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 01:09:21.436195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.04
 ---- batch: 020 ----
mean loss: 367.02
 ---- batch: 030 ----
mean loss: 364.20
 ---- batch: 040 ----
mean loss: 385.26
 ---- batch: 050 ----
mean loss: 373.14
 ---- batch: 060 ----
mean loss: 379.93
 ---- batch: 070 ----
mean loss: 365.85
 ---- batch: 080 ----
mean loss: 371.87
 ---- batch: 090 ----
mean loss: 369.95
 ---- batch: 100 ----
mean loss: 364.54
 ---- batch: 110 ----
mean loss: 380.71
train mean loss: 371.79
epoch train time: 0:00:02.143742
elapsed time: 0:03:24.140136
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 01:09:23.580093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.89
 ---- batch: 020 ----
mean loss: 374.04
 ---- batch: 030 ----
mean loss: 364.02
 ---- batch: 040 ----
mean loss: 371.76
 ---- batch: 050 ----
mean loss: 373.57
 ---- batch: 060 ----
mean loss: 376.90
 ---- batch: 070 ----
mean loss: 376.47
 ---- batch: 080 ----
mean loss: 374.74
 ---- batch: 090 ----
mean loss: 364.76
 ---- batch: 100 ----
mean loss: 375.70
 ---- batch: 110 ----
mean loss: 371.16
train mean loss: 371.37
epoch train time: 0:00:02.139098
elapsed time: 0:03:26.279391
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 01:09:25.719343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.47
 ---- batch: 020 ----
mean loss: 369.01
 ---- batch: 030 ----
mean loss: 366.42
 ---- batch: 040 ----
mean loss: 371.21
 ---- batch: 050 ----
mean loss: 365.32
 ---- batch: 060 ----
mean loss: 364.63
 ---- batch: 070 ----
mean loss: 372.47
 ---- batch: 080 ----
mean loss: 378.07
 ---- batch: 090 ----
mean loss: 362.14
 ---- batch: 100 ----
mean loss: 368.92
 ---- batch: 110 ----
mean loss: 391.88
train mean loss: 371.13
epoch train time: 0:00:02.137303
elapsed time: 0:03:28.416877
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 01:09:27.856838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.81
 ---- batch: 020 ----
mean loss: 365.65
 ---- batch: 030 ----
mean loss: 368.16
 ---- batch: 040 ----
mean loss: 359.08
 ---- batch: 050 ----
mean loss: 362.79
 ---- batch: 060 ----
mean loss: 374.29
 ---- batch: 070 ----
mean loss: 372.09
 ---- batch: 080 ----
mean loss: 359.40
 ---- batch: 090 ----
mean loss: 365.98
 ---- batch: 100 ----
mean loss: 368.48
 ---- batch: 110 ----
mean loss: 355.64
train mean loss: 364.28
epoch train time: 0:00:02.148356
elapsed time: 0:03:30.565400
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 01:09:30.005351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.70
 ---- batch: 020 ----
mean loss: 369.76
 ---- batch: 030 ----
mean loss: 368.27
 ---- batch: 040 ----
mean loss: 379.90
 ---- batch: 050 ----
mean loss: 362.54
 ---- batch: 060 ----
mean loss: 357.01
 ---- batch: 070 ----
mean loss: 361.74
 ---- batch: 080 ----
mean loss: 364.76
 ---- batch: 090 ----
mean loss: 374.85
 ---- batch: 100 ----
mean loss: 371.12
 ---- batch: 110 ----
mean loss: 376.05
train mean loss: 368.29
epoch train time: 0:00:02.154166
elapsed time: 0:03:32.719722
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 01:09:32.159692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.11
 ---- batch: 020 ----
mean loss: 364.64
 ---- batch: 030 ----
mean loss: 360.39
 ---- batch: 040 ----
mean loss: 365.26
 ---- batch: 050 ----
mean loss: 361.13
 ---- batch: 060 ----
mean loss: 351.86
 ---- batch: 070 ----
mean loss: 364.22
 ---- batch: 080 ----
mean loss: 378.61
 ---- batch: 090 ----
mean loss: 370.97
 ---- batch: 100 ----
mean loss: 381.88
 ---- batch: 110 ----
mean loss: 378.44
train mean loss: 367.19
epoch train time: 0:00:02.146149
elapsed time: 0:03:34.866049
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 01:09:34.306042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.39
 ---- batch: 020 ----
mean loss: 367.66
 ---- batch: 030 ----
mean loss: 370.69
 ---- batch: 040 ----
mean loss: 379.01
 ---- batch: 050 ----
mean loss: 364.46
 ---- batch: 060 ----
mean loss: 365.19
 ---- batch: 070 ----
mean loss: 364.36
 ---- batch: 080 ----
mean loss: 378.28
 ---- batch: 090 ----
mean loss: 351.07
 ---- batch: 100 ----
mean loss: 349.78
 ---- batch: 110 ----
mean loss: 365.33
train mean loss: 365.16
epoch train time: 0:00:02.142679
elapsed time: 0:03:37.008920
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 01:09:36.448872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.70
 ---- batch: 020 ----
mean loss: 367.65
 ---- batch: 030 ----
mean loss: 365.47
 ---- batch: 040 ----
mean loss: 360.51
 ---- batch: 050 ----
mean loss: 364.85
 ---- batch: 060 ----
mean loss: 358.62
 ---- batch: 070 ----
mean loss: 355.95
 ---- batch: 080 ----
mean loss: 359.53
 ---- batch: 090 ----
mean loss: 367.26
 ---- batch: 100 ----
mean loss: 362.99
 ---- batch: 110 ----
mean loss: 366.63
train mean loss: 361.71
epoch train time: 0:00:02.139571
elapsed time: 0:03:39.148645
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 01:09:38.588600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.67
 ---- batch: 020 ----
mean loss: 361.97
 ---- batch: 030 ----
mean loss: 361.89
 ---- batch: 040 ----
mean loss: 350.19
 ---- batch: 050 ----
mean loss: 358.09
 ---- batch: 060 ----
mean loss: 370.54
 ---- batch: 070 ----
mean loss: 353.89
 ---- batch: 080 ----
mean loss: 374.27
 ---- batch: 090 ----
mean loss: 367.00
 ---- batch: 100 ----
mean loss: 356.14
 ---- batch: 110 ----
mean loss: 361.06
train mean loss: 361.19
epoch train time: 0:00:02.137166
elapsed time: 0:03:41.285999
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 01:09:40.725961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.94
 ---- batch: 020 ----
mean loss: 357.73
 ---- batch: 030 ----
mean loss: 360.22
 ---- batch: 040 ----
mean loss: 360.14
 ---- batch: 050 ----
mean loss: 357.17
 ---- batch: 060 ----
mean loss: 345.69
 ---- batch: 070 ----
mean loss: 352.35
 ---- batch: 080 ----
mean loss: 358.15
 ---- batch: 090 ----
mean loss: 355.19
 ---- batch: 100 ----
mean loss: 368.08
 ---- batch: 110 ----
mean loss: 359.58
train mean loss: 357.57
epoch train time: 0:00:02.142642
elapsed time: 0:03:43.428820
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 01:09:42.868798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.42
 ---- batch: 020 ----
mean loss: 363.27
 ---- batch: 030 ----
mean loss: 356.20
 ---- batch: 040 ----
mean loss: 358.95
 ---- batch: 050 ----
mean loss: 358.16
 ---- batch: 060 ----
mean loss: 346.64
 ---- batch: 070 ----
mean loss: 350.38
 ---- batch: 080 ----
mean loss: 358.02
 ---- batch: 090 ----
mean loss: 349.22
 ---- batch: 100 ----
mean loss: 348.88
 ---- batch: 110 ----
mean loss: 350.68
train mean loss: 354.77
epoch train time: 0:00:02.152092
elapsed time: 0:03:45.581091
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 01:09:45.021041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.08
 ---- batch: 020 ----
mean loss: 354.39
 ---- batch: 030 ----
mean loss: 343.02
 ---- batch: 040 ----
mean loss: 355.58
 ---- batch: 050 ----
mean loss: 349.29
 ---- batch: 060 ----
mean loss: 356.43
 ---- batch: 070 ----
mean loss: 366.00
 ---- batch: 080 ----
mean loss: 359.98
 ---- batch: 090 ----
mean loss: 356.99
 ---- batch: 100 ----
mean loss: 351.63
 ---- batch: 110 ----
mean loss: 366.85
train mean loss: 356.08
epoch train time: 0:00:02.145068
elapsed time: 0:03:47.726308
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 01:09:47.166274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.93
 ---- batch: 020 ----
mean loss: 358.06
 ---- batch: 030 ----
mean loss: 351.56
 ---- batch: 040 ----
mean loss: 340.90
 ---- batch: 050 ----
mean loss: 363.46
 ---- batch: 060 ----
mean loss: 361.11
 ---- batch: 070 ----
mean loss: 357.26
 ---- batch: 080 ----
mean loss: 349.72
 ---- batch: 090 ----
mean loss: 356.78
 ---- batch: 100 ----
mean loss: 349.66
 ---- batch: 110 ----
mean loss: 348.34
train mean loss: 353.68
epoch train time: 0:00:02.138618
elapsed time: 0:03:49.865104
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 01:09:49.305075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.08
 ---- batch: 020 ----
mean loss: 362.30
 ---- batch: 030 ----
mean loss: 347.26
 ---- batch: 040 ----
mean loss: 365.86
 ---- batch: 050 ----
mean loss: 343.36
 ---- batch: 060 ----
mean loss: 356.98
 ---- batch: 070 ----
mean loss: 369.42
 ---- batch: 080 ----
mean loss: 339.07
 ---- batch: 090 ----
mean loss: 347.99
 ---- batch: 100 ----
mean loss: 346.66
 ---- batch: 110 ----
mean loss: 357.56
train mean loss: 354.20
epoch train time: 0:00:02.135689
elapsed time: 0:03:52.000991
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 01:09:51.440946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.97
 ---- batch: 020 ----
mean loss: 365.43
 ---- batch: 030 ----
mean loss: 353.93
 ---- batch: 040 ----
mean loss: 351.55
 ---- batch: 050 ----
mean loss: 350.57
 ---- batch: 060 ----
mean loss: 343.34
 ---- batch: 070 ----
mean loss: 343.41
 ---- batch: 080 ----
mean loss: 353.91
 ---- batch: 090 ----
mean loss: 350.39
 ---- batch: 100 ----
mean loss: 351.16
 ---- batch: 110 ----
mean loss: 347.52
train mean loss: 351.52
epoch train time: 0:00:02.140613
elapsed time: 0:03:54.141774
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 01:09:53.581729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.70
 ---- batch: 020 ----
mean loss: 356.84
 ---- batch: 030 ----
mean loss: 361.17
 ---- batch: 040 ----
mean loss: 346.44
 ---- batch: 050 ----
mean loss: 351.53
 ---- batch: 060 ----
mean loss: 352.09
 ---- batch: 070 ----
mean loss: 355.43
 ---- batch: 080 ----
mean loss: 342.82
 ---- batch: 090 ----
mean loss: 350.66
 ---- batch: 100 ----
mean loss: 347.28
 ---- batch: 110 ----
mean loss: 356.71
train mean loss: 352.04
epoch train time: 0:00:02.150435
elapsed time: 0:03:56.292380
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 01:09:55.732336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.58
 ---- batch: 020 ----
mean loss: 352.72
 ---- batch: 030 ----
mean loss: 355.88
 ---- batch: 040 ----
mean loss: 342.27
 ---- batch: 050 ----
mean loss: 354.10
 ---- batch: 060 ----
mean loss: 338.01
 ---- batch: 070 ----
mean loss: 360.45
 ---- batch: 080 ----
mean loss: 351.31
 ---- batch: 090 ----
mean loss: 361.69
 ---- batch: 100 ----
mean loss: 346.61
 ---- batch: 110 ----
mean loss: 350.79
train mean loss: 351.21
epoch train time: 0:00:02.145022
elapsed time: 0:03:58.437565
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 01:09:57.877526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.92
 ---- batch: 020 ----
mean loss: 356.33
 ---- batch: 030 ----
mean loss: 347.58
 ---- batch: 040 ----
mean loss: 356.25
 ---- batch: 050 ----
mean loss: 358.36
 ---- batch: 060 ----
mean loss: 337.93
 ---- batch: 070 ----
mean loss: 348.03
 ---- batch: 080 ----
mean loss: 351.45
 ---- batch: 090 ----
mean loss: 349.60
 ---- batch: 100 ----
mean loss: 349.60
 ---- batch: 110 ----
mean loss: 350.48
train mean loss: 349.44
epoch train time: 0:00:02.157340
elapsed time: 0:04:00.595070
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 01:10:00.035025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.41
 ---- batch: 020 ----
mean loss: 332.79
 ---- batch: 030 ----
mean loss: 332.38
 ---- batch: 040 ----
mean loss: 352.73
 ---- batch: 050 ----
mean loss: 363.60
 ---- batch: 060 ----
mean loss: 356.02
 ---- batch: 070 ----
mean loss: 350.19
 ---- batch: 080 ----
mean loss: 356.11
 ---- batch: 090 ----
mean loss: 339.34
 ---- batch: 100 ----
mean loss: 353.16
 ---- batch: 110 ----
mean loss: 347.02
train mean loss: 346.68
epoch train time: 0:00:02.153251
elapsed time: 0:04:02.748590
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 01:10:02.188604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.93
 ---- batch: 020 ----
mean loss: 344.98
 ---- batch: 030 ----
mean loss: 346.97
 ---- batch: 040 ----
mean loss: 345.61
 ---- batch: 050 ----
mean loss: 354.13
 ---- batch: 060 ----
mean loss: 344.91
 ---- batch: 070 ----
mean loss: 340.12
 ---- batch: 080 ----
mean loss: 360.55
 ---- batch: 090 ----
mean loss: 354.59
 ---- batch: 100 ----
mean loss: 359.45
 ---- batch: 110 ----
mean loss: 366.28
train mean loss: 350.59
epoch train time: 0:00:02.154960
elapsed time: 0:04:04.903801
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 01:10:04.343754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.04
 ---- batch: 020 ----
mean loss: 341.23
 ---- batch: 030 ----
mean loss: 345.98
 ---- batch: 040 ----
mean loss: 343.14
 ---- batch: 050 ----
mean loss: 346.53
 ---- batch: 060 ----
mean loss: 351.39
 ---- batch: 070 ----
mean loss: 342.44
 ---- batch: 080 ----
mean loss: 356.03
 ---- batch: 090 ----
mean loss: 350.13
 ---- batch: 100 ----
mean loss: 339.64
 ---- batch: 110 ----
mean loss: 344.52
train mean loss: 346.73
epoch train time: 0:00:02.147411
elapsed time: 0:04:07.051367
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 01:10:06.491320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.45
 ---- batch: 020 ----
mean loss: 346.92
 ---- batch: 030 ----
mean loss: 343.03
 ---- batch: 040 ----
mean loss: 342.16
 ---- batch: 050 ----
mean loss: 335.41
 ---- batch: 060 ----
mean loss: 343.35
 ---- batch: 070 ----
mean loss: 335.55
 ---- batch: 080 ----
mean loss: 344.88
 ---- batch: 090 ----
mean loss: 350.41
 ---- batch: 100 ----
mean loss: 347.55
 ---- batch: 110 ----
mean loss: 354.13
train mean loss: 345.66
epoch train time: 0:00:02.156505
elapsed time: 0:04:09.208023
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 01:10:08.647977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.89
 ---- batch: 020 ----
mean loss: 348.54
 ---- batch: 030 ----
mean loss: 353.17
 ---- batch: 040 ----
mean loss: 352.80
 ---- batch: 050 ----
mean loss: 334.59
 ---- batch: 060 ----
mean loss: 349.63
 ---- batch: 070 ----
mean loss: 332.38
 ---- batch: 080 ----
mean loss: 342.49
 ---- batch: 090 ----
mean loss: 335.97
 ---- batch: 100 ----
mean loss: 351.22
 ---- batch: 110 ----
mean loss: 343.55
train mean loss: 344.42
epoch train time: 0:00:02.147200
elapsed time: 0:04:11.355385
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 01:10:10.795338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.43
 ---- batch: 020 ----
mean loss: 341.38
 ---- batch: 030 ----
mean loss: 352.89
 ---- batch: 040 ----
mean loss: 343.65
 ---- batch: 050 ----
mean loss: 346.26
 ---- batch: 060 ----
mean loss: 337.46
 ---- batch: 070 ----
mean loss: 346.43
 ---- batch: 080 ----
mean loss: 340.27
 ---- batch: 090 ----
mean loss: 349.39
 ---- batch: 100 ----
mean loss: 342.64
 ---- batch: 110 ----
mean loss: 341.53
train mean loss: 345.47
epoch train time: 0:00:02.143912
elapsed time: 0:04:13.499486
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 01:10:12.939451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.22
 ---- batch: 020 ----
mean loss: 338.10
 ---- batch: 030 ----
mean loss: 345.69
 ---- batch: 040 ----
mean loss: 342.50
 ---- batch: 050 ----
mean loss: 352.66
 ---- batch: 060 ----
mean loss: 347.01
 ---- batch: 070 ----
mean loss: 333.74
 ---- batch: 080 ----
mean loss: 354.89
 ---- batch: 090 ----
mean loss: 330.31
 ---- batch: 100 ----
mean loss: 350.15
 ---- batch: 110 ----
mean loss: 335.69
train mean loss: 342.20
epoch train time: 0:00:02.143970
elapsed time: 0:04:15.643615
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 01:10:15.083585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.73
 ---- batch: 020 ----
mean loss: 339.71
 ---- batch: 030 ----
mean loss: 343.85
 ---- batch: 040 ----
mean loss: 348.37
 ---- batch: 050 ----
mean loss: 327.73
 ---- batch: 060 ----
mean loss: 350.79
 ---- batch: 070 ----
mean loss: 335.48
 ---- batch: 080 ----
mean loss: 319.64
 ---- batch: 090 ----
mean loss: 343.14
 ---- batch: 100 ----
mean loss: 343.29
 ---- batch: 110 ----
mean loss: 349.86
train mean loss: 340.38
epoch train time: 0:00:02.140935
elapsed time: 0:04:17.784731
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 01:10:17.224718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.25
 ---- batch: 020 ----
mean loss: 342.76
 ---- batch: 030 ----
mean loss: 335.25
 ---- batch: 040 ----
mean loss: 339.12
 ---- batch: 050 ----
mean loss: 342.87
 ---- batch: 060 ----
mean loss: 346.17
 ---- batch: 070 ----
mean loss: 348.41
 ---- batch: 080 ----
mean loss: 339.03
 ---- batch: 090 ----
mean loss: 342.06
 ---- batch: 100 ----
mean loss: 353.19
 ---- batch: 110 ----
mean loss: 338.40
train mean loss: 341.04
epoch train time: 0:00:02.143906
elapsed time: 0:04:19.928848
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 01:10:19.368804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.32
 ---- batch: 020 ----
mean loss: 341.31
 ---- batch: 030 ----
mean loss: 339.54
 ---- batch: 040 ----
mean loss: 353.13
 ---- batch: 050 ----
mean loss: 346.45
 ---- batch: 060 ----
mean loss: 346.67
 ---- batch: 070 ----
mean loss: 340.34
 ---- batch: 080 ----
mean loss: 338.49
 ---- batch: 090 ----
mean loss: 335.21
 ---- batch: 100 ----
mean loss: 346.84
 ---- batch: 110 ----
mean loss: 337.09
train mean loss: 342.01
epoch train time: 0:00:02.141163
elapsed time: 0:04:22.070177
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 01:10:21.510115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.32
 ---- batch: 020 ----
mean loss: 352.07
 ---- batch: 030 ----
mean loss: 342.56
 ---- batch: 040 ----
mean loss: 348.32
 ---- batch: 050 ----
mean loss: 347.55
 ---- batch: 060 ----
mean loss: 335.19
 ---- batch: 070 ----
mean loss: 330.56
 ---- batch: 080 ----
mean loss: 340.02
 ---- batch: 090 ----
mean loss: 349.89
 ---- batch: 100 ----
mean loss: 333.76
 ---- batch: 110 ----
mean loss: 344.59
train mean loss: 341.93
epoch train time: 0:00:02.136043
elapsed time: 0:04:24.206349
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 01:10:23.646301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.38
 ---- batch: 020 ----
mean loss: 342.38
 ---- batch: 030 ----
mean loss: 343.14
 ---- batch: 040 ----
mean loss: 344.24
 ---- batch: 050 ----
mean loss: 345.17
 ---- batch: 060 ----
mean loss: 346.63
 ---- batch: 070 ----
mean loss: 327.69
 ---- batch: 080 ----
mean loss: 336.84
 ---- batch: 090 ----
mean loss: 331.75
 ---- batch: 100 ----
mean loss: 324.68
 ---- batch: 110 ----
mean loss: 334.10
train mean loss: 337.67
epoch train time: 0:00:02.146403
elapsed time: 0:04:26.352908
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 01:10:25.792863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.67
 ---- batch: 020 ----
mean loss: 335.66
 ---- batch: 030 ----
mean loss: 332.24
 ---- batch: 040 ----
mean loss: 333.49
 ---- batch: 050 ----
mean loss: 353.02
 ---- batch: 060 ----
mean loss: 331.03
 ---- batch: 070 ----
mean loss: 340.93
 ---- batch: 080 ----
mean loss: 348.24
 ---- batch: 090 ----
mean loss: 347.49
 ---- batch: 100 ----
mean loss: 335.88
 ---- batch: 110 ----
mean loss: 342.48
train mean loss: 338.96
epoch train time: 0:00:02.146196
elapsed time: 0:04:28.499261
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 01:10:27.939212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.06
 ---- batch: 020 ----
mean loss: 331.26
 ---- batch: 030 ----
mean loss: 338.46
 ---- batch: 040 ----
mean loss: 333.76
 ---- batch: 050 ----
mean loss: 328.25
 ---- batch: 060 ----
mean loss: 345.91
 ---- batch: 070 ----
mean loss: 345.04
 ---- batch: 080 ----
mean loss: 339.46
 ---- batch: 090 ----
mean loss: 328.04
 ---- batch: 100 ----
mean loss: 341.21
 ---- batch: 110 ----
mean loss: 330.08
train mean loss: 336.17
epoch train time: 0:00:02.145256
elapsed time: 0:04:30.644699
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 01:10:30.084693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.26
 ---- batch: 020 ----
mean loss: 335.09
 ---- batch: 030 ----
mean loss: 330.28
 ---- batch: 040 ----
mean loss: 321.29
 ---- batch: 050 ----
mean loss: 340.49
 ---- batch: 060 ----
mean loss: 336.37
 ---- batch: 070 ----
mean loss: 329.64
 ---- batch: 080 ----
mean loss: 337.60
 ---- batch: 090 ----
mean loss: 340.89
 ---- batch: 100 ----
mean loss: 333.15
 ---- batch: 110 ----
mean loss: 323.78
train mean loss: 333.20
epoch train time: 0:00:02.150220
elapsed time: 0:04:32.795109
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 01:10:32.235060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.19
 ---- batch: 020 ----
mean loss: 328.86
 ---- batch: 030 ----
mean loss: 342.52
 ---- batch: 040 ----
mean loss: 339.93
 ---- batch: 050 ----
mean loss: 327.60
 ---- batch: 060 ----
mean loss: 343.35
 ---- batch: 070 ----
mean loss: 336.38
 ---- batch: 080 ----
mean loss: 338.98
 ---- batch: 090 ----
mean loss: 348.98
 ---- batch: 100 ----
mean loss: 336.65
 ---- batch: 110 ----
mean loss: 332.08
train mean loss: 337.26
epoch train time: 0:00:02.139027
elapsed time: 0:04:34.934305
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 01:10:34.374290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.09
 ---- batch: 020 ----
mean loss: 321.04
 ---- batch: 030 ----
mean loss: 339.05
 ---- batch: 040 ----
mean loss: 327.94
 ---- batch: 050 ----
mean loss: 343.16
 ---- batch: 060 ----
mean loss: 339.33
 ---- batch: 070 ----
mean loss: 325.23
 ---- batch: 080 ----
mean loss: 335.93
 ---- batch: 090 ----
mean loss: 337.54
 ---- batch: 100 ----
mean loss: 330.96
 ---- batch: 110 ----
mean loss: 339.79
train mean loss: 333.78
epoch train time: 0:00:02.139902
elapsed time: 0:04:37.074395
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 01:10:36.514348
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.76
 ---- batch: 020 ----
mean loss: 337.29
 ---- batch: 030 ----
mean loss: 327.70
 ---- batch: 040 ----
mean loss: 327.84
 ---- batch: 050 ----
mean loss: 350.36
 ---- batch: 060 ----
mean loss: 353.78
 ---- batch: 070 ----
mean loss: 338.05
 ---- batch: 080 ----
mean loss: 333.44
 ---- batch: 090 ----
mean loss: 330.72
 ---- batch: 100 ----
mean loss: 335.68
 ---- batch: 110 ----
mean loss: 328.88
train mean loss: 336.10
epoch train time: 0:00:02.139194
elapsed time: 0:04:39.213770
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 01:10:38.653737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.04
 ---- batch: 020 ----
mean loss: 343.27
 ---- batch: 030 ----
mean loss: 337.13
 ---- batch: 040 ----
mean loss: 330.99
 ---- batch: 050 ----
mean loss: 340.39
 ---- batch: 060 ----
mean loss: 321.11
 ---- batch: 070 ----
mean loss: 332.10
 ---- batch: 080 ----
mean loss: 337.55
 ---- batch: 090 ----
mean loss: 336.01
 ---- batch: 100 ----
mean loss: 334.30
 ---- batch: 110 ----
mean loss: 321.47
train mean loss: 333.76
epoch train time: 0:00:02.141724
elapsed time: 0:04:41.355670
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 01:10:40.795621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.17
 ---- batch: 020 ----
mean loss: 324.60
 ---- batch: 030 ----
mean loss: 333.06
 ---- batch: 040 ----
mean loss: 337.13
 ---- batch: 050 ----
mean loss: 332.65
 ---- batch: 060 ----
mean loss: 332.38
 ---- batch: 070 ----
mean loss: 334.94
 ---- batch: 080 ----
mean loss: 333.61
 ---- batch: 090 ----
mean loss: 328.64
 ---- batch: 100 ----
mean loss: 331.89
 ---- batch: 110 ----
mean loss: 336.77
train mean loss: 332.32
epoch train time: 0:00:02.139076
elapsed time: 0:04:43.494898
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 01:10:42.934853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.30
 ---- batch: 020 ----
mean loss: 337.48
 ---- batch: 030 ----
mean loss: 337.40
 ---- batch: 040 ----
mean loss: 331.04
 ---- batch: 050 ----
mean loss: 324.38
 ---- batch: 060 ----
mean loss: 330.47
 ---- batch: 070 ----
mean loss: 331.70
 ---- batch: 080 ----
mean loss: 342.30
 ---- batch: 090 ----
mean loss: 325.47
 ---- batch: 100 ----
mean loss: 335.13
 ---- batch: 110 ----
mean loss: 325.77
train mean loss: 333.83
epoch train time: 0:00:02.139597
elapsed time: 0:04:45.634645
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 01:10:45.074615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.37
 ---- batch: 020 ----
mean loss: 337.10
 ---- batch: 030 ----
mean loss: 332.90
 ---- batch: 040 ----
mean loss: 324.78
 ---- batch: 050 ----
mean loss: 330.42
 ---- batch: 060 ----
mean loss: 323.49
 ---- batch: 070 ----
mean loss: 325.95
 ---- batch: 080 ----
mean loss: 345.69
 ---- batch: 090 ----
mean loss: 341.59
 ---- batch: 100 ----
mean loss: 335.00
 ---- batch: 110 ----
mean loss: 331.64
train mean loss: 332.84
epoch train time: 0:00:02.140957
elapsed time: 0:04:47.775769
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 01:10:47.215721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.76
 ---- batch: 020 ----
mean loss: 336.01
 ---- batch: 030 ----
mean loss: 322.41
 ---- batch: 040 ----
mean loss: 324.89
 ---- batch: 050 ----
mean loss: 327.63
 ---- batch: 060 ----
mean loss: 345.22
 ---- batch: 070 ----
mean loss: 334.58
 ---- batch: 080 ----
mean loss: 327.77
 ---- batch: 090 ----
mean loss: 335.36
 ---- batch: 100 ----
mean loss: 334.09
 ---- batch: 110 ----
mean loss: 342.84
train mean loss: 333.22
epoch train time: 0:00:02.140571
elapsed time: 0:04:49.916487
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 01:10:49.356441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.52
 ---- batch: 020 ----
mean loss: 309.28
 ---- batch: 030 ----
mean loss: 329.22
 ---- batch: 040 ----
mean loss: 333.06
 ---- batch: 050 ----
mean loss: 328.47
 ---- batch: 060 ----
mean loss: 327.76
 ---- batch: 070 ----
mean loss: 328.19
 ---- batch: 080 ----
mean loss: 325.27
 ---- batch: 090 ----
mean loss: 334.02
 ---- batch: 100 ----
mean loss: 342.47
 ---- batch: 110 ----
mean loss: 337.26
train mean loss: 330.13
epoch train time: 0:00:02.136870
elapsed time: 0:04:52.053501
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 01:10:51.493480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.51
 ---- batch: 020 ----
mean loss: 327.34
 ---- batch: 030 ----
mean loss: 329.18
 ---- batch: 040 ----
mean loss: 328.85
 ---- batch: 050 ----
mean loss: 323.76
 ---- batch: 060 ----
mean loss: 333.58
 ---- batch: 070 ----
mean loss: 323.59
 ---- batch: 080 ----
mean loss: 326.74
 ---- batch: 090 ----
mean loss: 333.28
 ---- batch: 100 ----
mean loss: 328.43
 ---- batch: 110 ----
mean loss: 329.18
train mean loss: 328.44
epoch train time: 0:00:02.135099
elapsed time: 0:04:54.188826
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 01:10:53.628778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.52
 ---- batch: 020 ----
mean loss: 328.68
 ---- batch: 030 ----
mean loss: 316.82
 ---- batch: 040 ----
mean loss: 324.29
 ---- batch: 050 ----
mean loss: 330.80
 ---- batch: 060 ----
mean loss: 340.44
 ---- batch: 070 ----
mean loss: 336.62
 ---- batch: 080 ----
mean loss: 343.67
 ---- batch: 090 ----
mean loss: 322.54
 ---- batch: 100 ----
mean loss: 337.63
 ---- batch: 110 ----
mean loss: 324.12
train mean loss: 331.00
epoch train time: 0:00:02.151298
elapsed time: 0:04:56.340277
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 01:10:55.780232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.12
 ---- batch: 020 ----
mean loss: 335.05
 ---- batch: 030 ----
mean loss: 318.75
 ---- batch: 040 ----
mean loss: 328.09
 ---- batch: 050 ----
mean loss: 335.00
 ---- batch: 060 ----
mean loss: 334.28
 ---- batch: 070 ----
mean loss: 333.04
 ---- batch: 080 ----
mean loss: 336.69
 ---- batch: 090 ----
mean loss: 350.25
 ---- batch: 100 ----
mean loss: 332.53
 ---- batch: 110 ----
mean loss: 325.09
train mean loss: 331.85
epoch train time: 0:00:02.143356
elapsed time: 0:04:58.483788
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 01:10:57.923739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.58
 ---- batch: 020 ----
mean loss: 316.94
 ---- batch: 030 ----
mean loss: 328.82
 ---- batch: 040 ----
mean loss: 336.18
 ---- batch: 050 ----
mean loss: 333.86
 ---- batch: 060 ----
mean loss: 323.66
 ---- batch: 070 ----
mean loss: 330.61
 ---- batch: 080 ----
mean loss: 323.88
 ---- batch: 090 ----
mean loss: 331.41
 ---- batch: 100 ----
mean loss: 312.41
 ---- batch: 110 ----
mean loss: 320.30
train mean loss: 326.30
epoch train time: 0:00:02.143475
elapsed time: 0:05:00.627429
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 01:11:00.067399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.97
 ---- batch: 020 ----
mean loss: 319.93
 ---- batch: 030 ----
mean loss: 329.42
 ---- batch: 040 ----
mean loss: 331.20
 ---- batch: 050 ----
mean loss: 327.47
 ---- batch: 060 ----
mean loss: 328.43
 ---- batch: 070 ----
mean loss: 324.83
 ---- batch: 080 ----
mean loss: 334.89
 ---- batch: 090 ----
mean loss: 323.58
 ---- batch: 100 ----
mean loss: 323.30
 ---- batch: 110 ----
mean loss: 332.25
train mean loss: 327.46
epoch train time: 0:00:02.144832
elapsed time: 0:05:02.772426
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 01:11:02.212379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.70
 ---- batch: 020 ----
mean loss: 331.42
 ---- batch: 030 ----
mean loss: 326.72
 ---- batch: 040 ----
mean loss: 325.12
 ---- batch: 050 ----
mean loss: 331.85
 ---- batch: 060 ----
mean loss: 326.04
 ---- batch: 070 ----
mean loss: 323.58
 ---- batch: 080 ----
mean loss: 326.46
 ---- batch: 090 ----
mean loss: 322.05
 ---- batch: 100 ----
mean loss: 325.31
 ---- batch: 110 ----
mean loss: 319.81
train mean loss: 326.52
epoch train time: 0:00:02.140390
elapsed time: 0:05:04.912976
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 01:11:04.352916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.15
 ---- batch: 020 ----
mean loss: 322.31
 ---- batch: 030 ----
mean loss: 324.44
 ---- batch: 040 ----
mean loss: 321.70
 ---- batch: 050 ----
mean loss: 341.57
 ---- batch: 060 ----
mean loss: 330.34
 ---- batch: 070 ----
mean loss: 321.72
 ---- batch: 080 ----
mean loss: 332.47
 ---- batch: 090 ----
mean loss: 330.20
 ---- batch: 100 ----
mean loss: 323.05
 ---- batch: 110 ----
mean loss: 323.51
train mean loss: 327.63
epoch train time: 0:00:02.140078
elapsed time: 0:05:07.053206
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 01:11:06.493172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.21
 ---- batch: 020 ----
mean loss: 312.08
 ---- batch: 030 ----
mean loss: 329.69
 ---- batch: 040 ----
mean loss: 326.88
 ---- batch: 050 ----
mean loss: 326.26
 ---- batch: 060 ----
mean loss: 332.74
 ---- batch: 070 ----
mean loss: 326.58
 ---- batch: 080 ----
mean loss: 328.01
 ---- batch: 090 ----
mean loss: 307.69
 ---- batch: 100 ----
mean loss: 329.82
 ---- batch: 110 ----
mean loss: 329.14
train mean loss: 325.81
epoch train time: 0:00:02.142625
elapsed time: 0:05:09.195991
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 01:11:08.635942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.26
 ---- batch: 020 ----
mean loss: 331.48
 ---- batch: 030 ----
mean loss: 323.58
 ---- batch: 040 ----
mean loss: 326.69
 ---- batch: 050 ----
mean loss: 322.41
 ---- batch: 060 ----
mean loss: 329.66
 ---- batch: 070 ----
mean loss: 315.72
 ---- batch: 080 ----
mean loss: 314.91
 ---- batch: 090 ----
mean loss: 323.50
 ---- batch: 100 ----
mean loss: 324.91
 ---- batch: 110 ----
mean loss: 324.19
train mean loss: 323.71
epoch train time: 0:00:02.145814
elapsed time: 0:05:11.341954
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 01:11:10.781915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.61
 ---- batch: 020 ----
mean loss: 316.53
 ---- batch: 030 ----
mean loss: 321.02
 ---- batch: 040 ----
mean loss: 328.31
 ---- batch: 050 ----
mean loss: 313.03
 ---- batch: 060 ----
mean loss: 317.94
 ---- batch: 070 ----
mean loss: 332.54
 ---- batch: 080 ----
mean loss: 325.47
 ---- batch: 090 ----
mean loss: 334.44
 ---- batch: 100 ----
mean loss: 326.21
 ---- batch: 110 ----
mean loss: 327.66
train mean loss: 323.86
epoch train time: 0:00:02.141650
elapsed time: 0:05:13.483767
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 01:11:12.923743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.48
 ---- batch: 020 ----
mean loss: 325.48
 ---- batch: 030 ----
mean loss: 330.44
 ---- batch: 040 ----
mean loss: 323.53
 ---- batch: 050 ----
mean loss: 328.56
 ---- batch: 060 ----
mean loss: 319.71
 ---- batch: 070 ----
mean loss: 332.47
 ---- batch: 080 ----
mean loss: 325.99
 ---- batch: 090 ----
mean loss: 325.24
 ---- batch: 100 ----
mean loss: 316.88
 ---- batch: 110 ----
mean loss: 323.29
train mean loss: 325.41
epoch train time: 0:00:02.143930
elapsed time: 0:05:15.627892
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 01:11:15.067847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.57
 ---- batch: 020 ----
mean loss: 327.27
 ---- batch: 030 ----
mean loss: 329.97
 ---- batch: 040 ----
mean loss: 331.02
 ---- batch: 050 ----
mean loss: 326.41
 ---- batch: 060 ----
mean loss: 328.35
 ---- batch: 070 ----
mean loss: 318.58
 ---- batch: 080 ----
mean loss: 321.97
 ---- batch: 090 ----
mean loss: 312.27
 ---- batch: 100 ----
mean loss: 315.46
 ---- batch: 110 ----
mean loss: 319.83
train mean loss: 322.01
epoch train time: 0:00:02.143790
elapsed time: 0:05:17.771842
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 01:11:17.211795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.11
 ---- batch: 020 ----
mean loss: 323.70
 ---- batch: 030 ----
mean loss: 322.67
 ---- batch: 040 ----
mean loss: 327.27
 ---- batch: 050 ----
mean loss: 328.62
 ---- batch: 060 ----
mean loss: 318.13
 ---- batch: 070 ----
mean loss: 320.34
 ---- batch: 080 ----
mean loss: 319.09
 ---- batch: 090 ----
mean loss: 326.80
 ---- batch: 100 ----
mean loss: 314.47
 ---- batch: 110 ----
mean loss: 322.41
train mean loss: 323.02
epoch train time: 0:00:02.143795
elapsed time: 0:05:19.915789
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 01:11:19.355742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.38
 ---- batch: 020 ----
mean loss: 321.47
 ---- batch: 030 ----
mean loss: 334.67
 ---- batch: 040 ----
mean loss: 323.63
 ---- batch: 050 ----
mean loss: 320.51
 ---- batch: 060 ----
mean loss: 319.28
 ---- batch: 070 ----
mean loss: 322.37
 ---- batch: 080 ----
mean loss: 312.58
 ---- batch: 090 ----
mean loss: 323.55
 ---- batch: 100 ----
mean loss: 325.26
 ---- batch: 110 ----
mean loss: 322.13
train mean loss: 322.50
epoch train time: 0:00:02.146208
elapsed time: 0:05:22.062143
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 01:11:21.502109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.67
 ---- batch: 020 ----
mean loss: 321.65
 ---- batch: 030 ----
mean loss: 329.67
 ---- batch: 040 ----
mean loss: 319.47
 ---- batch: 050 ----
mean loss: 309.74
 ---- batch: 060 ----
mean loss: 330.24
 ---- batch: 070 ----
mean loss: 324.69
 ---- batch: 080 ----
mean loss: 316.37
 ---- batch: 090 ----
mean loss: 325.79
 ---- batch: 100 ----
mean loss: 314.41
 ---- batch: 110 ----
mean loss: 317.48
train mean loss: 320.04
epoch train time: 0:00:02.135354
elapsed time: 0:05:24.197658
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 01:11:23.637626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.26
 ---- batch: 020 ----
mean loss: 329.03
 ---- batch: 030 ----
mean loss: 330.54
 ---- batch: 040 ----
mean loss: 328.60
 ---- batch: 050 ----
mean loss: 317.51
 ---- batch: 060 ----
mean loss: 322.85
 ---- batch: 070 ----
mean loss: 330.50
 ---- batch: 080 ----
mean loss: 321.20
 ---- batch: 090 ----
mean loss: 317.12
 ---- batch: 100 ----
mean loss: 311.56
 ---- batch: 110 ----
mean loss: 318.17
train mean loss: 323.26
epoch train time: 0:00:02.137427
elapsed time: 0:05:26.335258
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 01:11:25.775210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.12
 ---- batch: 020 ----
mean loss: 324.36
 ---- batch: 030 ----
mean loss: 326.94
 ---- batch: 040 ----
mean loss: 325.38
 ---- batch: 050 ----
mean loss: 322.76
 ---- batch: 060 ----
mean loss: 320.51
 ---- batch: 070 ----
mean loss: 306.26
 ---- batch: 080 ----
mean loss: 327.62
 ---- batch: 090 ----
mean loss: 305.77
 ---- batch: 100 ----
mean loss: 327.28
 ---- batch: 110 ----
mean loss: 319.76
train mean loss: 319.92
epoch train time: 0:00:02.147161
elapsed time: 0:05:28.482570
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 01:11:27.922541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.32
 ---- batch: 020 ----
mean loss: 323.57
 ---- batch: 030 ----
mean loss: 308.16
 ---- batch: 040 ----
mean loss: 320.33
 ---- batch: 050 ----
mean loss: 312.37
 ---- batch: 060 ----
mean loss: 320.40
 ---- batch: 070 ----
mean loss: 304.47
 ---- batch: 080 ----
mean loss: 325.73
 ---- batch: 090 ----
mean loss: 322.28
 ---- batch: 100 ----
mean loss: 324.12
 ---- batch: 110 ----
mean loss: 312.32
train mean loss: 318.32
epoch train time: 0:00:02.137950
elapsed time: 0:05:30.620705
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 01:11:30.060661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.73
 ---- batch: 020 ----
mean loss: 328.67
 ---- batch: 030 ----
mean loss: 334.93
 ---- batch: 040 ----
mean loss: 324.63
 ---- batch: 050 ----
mean loss: 329.15
 ---- batch: 060 ----
mean loss: 334.67
 ---- batch: 070 ----
mean loss: 319.66
 ---- batch: 080 ----
mean loss: 324.39
 ---- batch: 090 ----
mean loss: 323.84
 ---- batch: 100 ----
mean loss: 305.07
 ---- batch: 110 ----
mean loss: 316.10
train mean loss: 323.49
epoch train time: 0:00:02.139007
elapsed time: 0:05:32.759895
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 01:11:32.199848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.30
 ---- batch: 020 ----
mean loss: 324.96
 ---- batch: 030 ----
mean loss: 325.37
 ---- batch: 040 ----
mean loss: 323.50
 ---- batch: 050 ----
mean loss: 327.39
 ---- batch: 060 ----
mean loss: 304.83
 ---- batch: 070 ----
mean loss: 318.09
 ---- batch: 080 ----
mean loss: 326.82
 ---- batch: 090 ----
mean loss: 326.13
 ---- batch: 100 ----
mean loss: 320.77
 ---- batch: 110 ----
mean loss: 310.31
train mean loss: 320.44
epoch train time: 0:00:02.142741
elapsed time: 0:05:34.902793
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 01:11:34.342777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.24
 ---- batch: 020 ----
mean loss: 319.94
 ---- batch: 030 ----
mean loss: 320.76
 ---- batch: 040 ----
mean loss: 323.34
 ---- batch: 050 ----
mean loss: 317.88
 ---- batch: 060 ----
mean loss: 313.71
 ---- batch: 070 ----
mean loss: 323.66
 ---- batch: 080 ----
mean loss: 320.19
 ---- batch: 090 ----
mean loss: 317.70
 ---- batch: 100 ----
mean loss: 304.51
 ---- batch: 110 ----
mean loss: 308.99
train mean loss: 318.33
epoch train time: 0:00:02.141578
elapsed time: 0:05:37.044565
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 01:11:36.484519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.22
 ---- batch: 020 ----
mean loss: 329.88
 ---- batch: 030 ----
mean loss: 333.59
 ---- batch: 040 ----
mean loss: 315.85
 ---- batch: 050 ----
mean loss: 315.75
 ---- batch: 060 ----
mean loss: 323.94
 ---- batch: 070 ----
mean loss: 320.59
 ---- batch: 080 ----
mean loss: 318.48
 ---- batch: 090 ----
mean loss: 309.39
 ---- batch: 100 ----
mean loss: 320.47
 ---- batch: 110 ----
mean loss: 323.12
train mean loss: 320.27
epoch train time: 0:00:02.142205
elapsed time: 0:05:39.186923
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 01:11:38.626873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.44
 ---- batch: 020 ----
mean loss: 315.26
 ---- batch: 030 ----
mean loss: 310.05
 ---- batch: 040 ----
mean loss: 315.85
 ---- batch: 050 ----
mean loss: 313.41
 ---- batch: 060 ----
mean loss: 311.95
 ---- batch: 070 ----
mean loss: 318.36
 ---- batch: 080 ----
mean loss: 328.56
 ---- batch: 090 ----
mean loss: 326.01
 ---- batch: 100 ----
mean loss: 318.69
 ---- batch: 110 ----
mean loss: 311.61
train mean loss: 316.52
epoch train time: 0:00:02.139044
elapsed time: 0:05:41.326122
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 01:11:40.766107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.36
 ---- batch: 020 ----
mean loss: 323.87
 ---- batch: 030 ----
mean loss: 321.60
 ---- batch: 040 ----
mean loss: 318.38
 ---- batch: 050 ----
mean loss: 313.26
 ---- batch: 060 ----
mean loss: 322.30
 ---- batch: 070 ----
mean loss: 319.51
 ---- batch: 080 ----
mean loss: 327.82
 ---- batch: 090 ----
mean loss: 312.07
 ---- batch: 100 ----
mean loss: 316.43
 ---- batch: 110 ----
mean loss: 313.52
train mean loss: 319.12
epoch train time: 0:00:02.140137
elapsed time: 0:05:43.466500
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 01:11:42.906454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.50
 ---- batch: 020 ----
mean loss: 326.65
 ---- batch: 030 ----
mean loss: 321.66
 ---- batch: 040 ----
mean loss: 314.95
 ---- batch: 050 ----
mean loss: 325.80
 ---- batch: 060 ----
mean loss: 322.52
 ---- batch: 070 ----
mean loss: 310.76
 ---- batch: 080 ----
mean loss: 314.71
 ---- batch: 090 ----
mean loss: 306.82
 ---- batch: 100 ----
mean loss: 308.53
 ---- batch: 110 ----
mean loss: 313.30
train mean loss: 315.81
epoch train time: 0:00:02.141329
elapsed time: 0:05:45.607979
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 01:11:45.047932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.67
 ---- batch: 020 ----
mean loss: 324.69
 ---- batch: 030 ----
mean loss: 308.64
 ---- batch: 040 ----
mean loss: 309.41
 ---- batch: 050 ----
mean loss: 309.44
 ---- batch: 060 ----
mean loss: 315.99
 ---- batch: 070 ----
mean loss: 319.51
 ---- batch: 080 ----
mean loss: 315.92
 ---- batch: 090 ----
mean loss: 314.66
 ---- batch: 100 ----
mean loss: 313.91
 ---- batch: 110 ----
mean loss: 312.68
train mean loss: 316.16
epoch train time: 0:00:02.146358
elapsed time: 0:05:47.754489
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 01:11:47.194442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.85
 ---- batch: 020 ----
mean loss: 312.06
 ---- batch: 030 ----
mean loss: 308.69
 ---- batch: 040 ----
mean loss: 316.81
 ---- batch: 050 ----
mean loss: 323.96
 ---- batch: 060 ----
mean loss: 314.62
 ---- batch: 070 ----
mean loss: 318.40
 ---- batch: 080 ----
mean loss: 322.98
 ---- batch: 090 ----
mean loss: 315.98
 ---- batch: 100 ----
mean loss: 315.36
 ---- batch: 110 ----
mean loss: 300.62
train mean loss: 314.90
epoch train time: 0:00:02.147799
elapsed time: 0:05:49.902470
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 01:11:49.342434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.43
 ---- batch: 020 ----
mean loss: 320.51
 ---- batch: 030 ----
mean loss: 310.98
 ---- batch: 040 ----
mean loss: 314.89
 ---- batch: 050 ----
mean loss: 312.20
 ---- batch: 060 ----
mean loss: 311.47
 ---- batch: 070 ----
mean loss: 318.57
 ---- batch: 080 ----
mean loss: 323.86
 ---- batch: 090 ----
mean loss: 308.26
 ---- batch: 100 ----
mean loss: 314.75
 ---- batch: 110 ----
mean loss: 312.44
train mean loss: 314.27
epoch train time: 0:00:02.149327
elapsed time: 0:05:52.051973
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 01:11:51.491913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.15
 ---- batch: 020 ----
mean loss: 310.01
 ---- batch: 030 ----
mean loss: 326.94
 ---- batch: 040 ----
mean loss: 324.72
 ---- batch: 050 ----
mean loss: 305.95
 ---- batch: 060 ----
mean loss: 313.17
 ---- batch: 070 ----
mean loss: 316.83
 ---- batch: 080 ----
mean loss: 312.55
 ---- batch: 090 ----
mean loss: 312.43
 ---- batch: 100 ----
mean loss: 314.16
 ---- batch: 110 ----
mean loss: 322.27
train mean loss: 316.24
epoch train time: 0:00:02.141207
elapsed time: 0:05:54.193318
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 01:11:53.633270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.62
 ---- batch: 020 ----
mean loss: 312.65
 ---- batch: 030 ----
mean loss: 308.44
 ---- batch: 040 ----
mean loss: 315.55
 ---- batch: 050 ----
mean loss: 317.21
 ---- batch: 060 ----
mean loss: 324.64
 ---- batch: 070 ----
mean loss: 304.69
 ---- batch: 080 ----
mean loss: 318.24
 ---- batch: 090 ----
mean loss: 314.79
 ---- batch: 100 ----
mean loss: 306.57
 ---- batch: 110 ----
mean loss: 318.36
train mean loss: 314.14
epoch train time: 0:00:02.144043
elapsed time: 0:05:56.337507
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 01:11:55.777477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.63
 ---- batch: 020 ----
mean loss: 312.14
 ---- batch: 030 ----
mean loss: 321.93
 ---- batch: 040 ----
mean loss: 317.93
 ---- batch: 050 ----
mean loss: 309.49
 ---- batch: 060 ----
mean loss: 315.83
 ---- batch: 070 ----
mean loss: 319.18
 ---- batch: 080 ----
mean loss: 317.54
 ---- batch: 090 ----
mean loss: 325.66
 ---- batch: 100 ----
mean loss: 326.74
 ---- batch: 110 ----
mean loss: 301.72
train mean loss: 316.51
epoch train time: 0:00:02.134035
elapsed time: 0:05:58.471715
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 01:11:57.911668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.48
 ---- batch: 020 ----
mean loss: 323.36
 ---- batch: 030 ----
mean loss: 303.59
 ---- batch: 040 ----
mean loss: 301.65
 ---- batch: 050 ----
mean loss: 310.03
 ---- batch: 060 ----
mean loss: 312.10
 ---- batch: 070 ----
mean loss: 320.22
 ---- batch: 080 ----
mean loss: 326.18
 ---- batch: 090 ----
mean loss: 316.65
 ---- batch: 100 ----
mean loss: 305.12
 ---- batch: 110 ----
mean loss: 301.98
train mean loss: 313.09
epoch train time: 0:00:02.137459
elapsed time: 0:06:00.609352
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 01:12:00.049303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.64
 ---- batch: 020 ----
mean loss: 306.97
 ---- batch: 030 ----
mean loss: 319.83
 ---- batch: 040 ----
mean loss: 307.43
 ---- batch: 050 ----
mean loss: 315.77
 ---- batch: 060 ----
mean loss: 311.49
 ---- batch: 070 ----
mean loss: 308.48
 ---- batch: 080 ----
mean loss: 317.14
 ---- batch: 090 ----
mean loss: 304.88
 ---- batch: 100 ----
mean loss: 316.80
 ---- batch: 110 ----
mean loss: 317.74
train mean loss: 313.07
epoch train time: 0:00:02.137235
elapsed time: 0:06:02.746736
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 01:12:02.186713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.27
 ---- batch: 020 ----
mean loss: 313.30
 ---- batch: 030 ----
mean loss: 317.98
 ---- batch: 040 ----
mean loss: 302.86
 ---- batch: 050 ----
mean loss: 319.64
 ---- batch: 060 ----
mean loss: 314.23
 ---- batch: 070 ----
mean loss: 314.35
 ---- batch: 080 ----
mean loss: 305.40
 ---- batch: 090 ----
mean loss: 319.04
 ---- batch: 100 ----
mean loss: 308.20
 ---- batch: 110 ----
mean loss: 312.54
train mean loss: 312.25
epoch train time: 0:00:02.139010
elapsed time: 0:06:04.885922
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 01:12:04.325871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.51
 ---- batch: 020 ----
mean loss: 307.13
 ---- batch: 030 ----
mean loss: 304.77
 ---- batch: 040 ----
mean loss: 312.52
 ---- batch: 050 ----
mean loss: 307.84
 ---- batch: 060 ----
mean loss: 309.48
 ---- batch: 070 ----
mean loss: 316.45
 ---- batch: 080 ----
mean loss: 304.92
 ---- batch: 090 ----
mean loss: 314.11
 ---- batch: 100 ----
mean loss: 314.28
 ---- batch: 110 ----
mean loss: 322.53
train mean loss: 311.46
epoch train time: 0:00:02.142247
elapsed time: 0:06:07.028318
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 01:12:06.468274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.37
 ---- batch: 020 ----
mean loss: 305.53
 ---- batch: 030 ----
mean loss: 321.89
 ---- batch: 040 ----
mean loss: 318.72
 ---- batch: 050 ----
mean loss: 310.24
 ---- batch: 060 ----
mean loss: 306.61
 ---- batch: 070 ----
mean loss: 307.32
 ---- batch: 080 ----
mean loss: 305.15
 ---- batch: 090 ----
mean loss: 304.34
 ---- batch: 100 ----
mean loss: 313.35
 ---- batch: 110 ----
mean loss: 304.95
train mean loss: 309.64
epoch train time: 0:00:02.141014
elapsed time: 0:06:09.169481
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 01:12:08.609448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.06
 ---- batch: 020 ----
mean loss: 307.03
 ---- batch: 030 ----
mean loss: 315.27
 ---- batch: 040 ----
mean loss: 313.22
 ---- batch: 050 ----
mean loss: 304.35
 ---- batch: 060 ----
mean loss: 319.71
 ---- batch: 070 ----
mean loss: 297.63
 ---- batch: 080 ----
mean loss: 316.04
 ---- batch: 090 ----
mean loss: 328.44
 ---- batch: 100 ----
mean loss: 323.30
 ---- batch: 110 ----
mean loss: 307.52
train mean loss: 312.64
epoch train time: 0:00:02.140001
elapsed time: 0:06:11.309642
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 01:12:10.749596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.92
 ---- batch: 020 ----
mean loss: 307.23
 ---- batch: 030 ----
mean loss: 308.00
 ---- batch: 040 ----
mean loss: 302.20
 ---- batch: 050 ----
mean loss: 328.48
 ---- batch: 060 ----
mean loss: 321.90
 ---- batch: 070 ----
mean loss: 305.35
 ---- batch: 080 ----
mean loss: 308.37
 ---- batch: 090 ----
mean loss: 305.73
 ---- batch: 100 ----
mean loss: 320.88
 ---- batch: 110 ----
mean loss: 320.56
train mean loss: 314.26
epoch train time: 0:00:02.140375
elapsed time: 0:06:13.450189
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 01:12:12.890142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.81
 ---- batch: 020 ----
mean loss: 303.58
 ---- batch: 030 ----
mean loss: 306.83
 ---- batch: 040 ----
mean loss: 304.05
 ---- batch: 050 ----
mean loss: 302.53
 ---- batch: 060 ----
mean loss: 300.26
 ---- batch: 070 ----
mean loss: 322.10
 ---- batch: 080 ----
mean loss: 296.80
 ---- batch: 090 ----
mean loss: 302.63
 ---- batch: 100 ----
mean loss: 321.97
 ---- batch: 110 ----
mean loss: 314.82
train mean loss: 308.15
epoch train time: 0:00:02.141182
elapsed time: 0:06:15.591527
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 01:12:15.031482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.64
 ---- batch: 020 ----
mean loss: 300.15
 ---- batch: 030 ----
mean loss: 308.99
 ---- batch: 040 ----
mean loss: 307.50
 ---- batch: 050 ----
mean loss: 325.95
 ---- batch: 060 ----
mean loss: 304.26
 ---- batch: 070 ----
mean loss: 308.44
 ---- batch: 080 ----
mean loss: 321.35
 ---- batch: 090 ----
mean loss: 319.11
 ---- batch: 100 ----
mean loss: 300.95
 ---- batch: 110 ----
mean loss: 304.83
train mean loss: 309.77
epoch train time: 0:00:02.143775
elapsed time: 0:06:17.735458
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 01:12:17.175420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.89
 ---- batch: 020 ----
mean loss: 320.54
 ---- batch: 030 ----
mean loss: 313.18
 ---- batch: 040 ----
mean loss: 302.06
 ---- batch: 050 ----
mean loss: 310.95
 ---- batch: 060 ----
mean loss: 312.87
 ---- batch: 070 ----
mean loss: 301.98
 ---- batch: 080 ----
mean loss: 316.60
 ---- batch: 090 ----
mean loss: 303.76
 ---- batch: 100 ----
mean loss: 313.17
 ---- batch: 110 ----
mean loss: 308.02
train mean loss: 308.57
epoch train time: 0:00:02.149329
elapsed time: 0:06:19.884946
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 01:12:19.324897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.81
 ---- batch: 020 ----
mean loss: 305.89
 ---- batch: 030 ----
mean loss: 313.05
 ---- batch: 040 ----
mean loss: 304.61
 ---- batch: 050 ----
mean loss: 325.74
 ---- batch: 060 ----
mean loss: 318.60
 ---- batch: 070 ----
mean loss: 314.30
 ---- batch: 080 ----
mean loss: 293.68
 ---- batch: 090 ----
mean loss: 312.25
 ---- batch: 100 ----
mean loss: 318.24
 ---- batch: 110 ----
mean loss: 315.16
train mean loss: 312.84
epoch train time: 0:00:02.143873
elapsed time: 0:06:22.028973
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 01:12:21.468929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.32
 ---- batch: 020 ----
mean loss: 316.47
 ---- batch: 030 ----
mean loss: 312.76
 ---- batch: 040 ----
mean loss: 313.58
 ---- batch: 050 ----
mean loss: 307.12
 ---- batch: 060 ----
mean loss: 299.65
 ---- batch: 070 ----
mean loss: 308.86
 ---- batch: 080 ----
mean loss: 309.50
 ---- batch: 090 ----
mean loss: 305.76
 ---- batch: 100 ----
mean loss: 301.34
 ---- batch: 110 ----
mean loss: 316.87
train mean loss: 308.98
epoch train time: 0:00:02.146756
elapsed time: 0:06:24.175883
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 01:12:23.615871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.66
 ---- batch: 020 ----
mean loss: 324.39
 ---- batch: 030 ----
mean loss: 319.32
 ---- batch: 040 ----
mean loss: 325.59
 ---- batch: 050 ----
mean loss: 320.25
 ---- batch: 060 ----
mean loss: 309.64
 ---- batch: 070 ----
mean loss: 324.90
 ---- batch: 080 ----
mean loss: 302.35
 ---- batch: 090 ----
mean loss: 295.23
 ---- batch: 100 ----
mean loss: 311.68
 ---- batch: 110 ----
mean loss: 304.88
train mean loss: 313.37
epoch train time: 0:00:02.142464
elapsed time: 0:06:26.318533
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 01:12:25.758487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.79
 ---- batch: 020 ----
mean loss: 299.06
 ---- batch: 030 ----
mean loss: 311.15
 ---- batch: 040 ----
mean loss: 303.61
 ---- batch: 050 ----
mean loss: 315.89
 ---- batch: 060 ----
mean loss: 311.90
 ---- batch: 070 ----
mean loss: 301.57
 ---- batch: 080 ----
mean loss: 308.80
 ---- batch: 090 ----
mean loss: 308.59
 ---- batch: 100 ----
mean loss: 300.10
 ---- batch: 110 ----
mean loss: 299.72
train mean loss: 306.88
epoch train time: 0:00:02.141889
elapsed time: 0:06:28.460571
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 01:12:27.900522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.28
 ---- batch: 020 ----
mean loss: 314.61
 ---- batch: 030 ----
mean loss: 304.20
 ---- batch: 040 ----
mean loss: 306.86
 ---- batch: 050 ----
mean loss: 296.12
 ---- batch: 060 ----
mean loss: 316.46
 ---- batch: 070 ----
mean loss: 297.93
 ---- batch: 080 ----
mean loss: 323.07
 ---- batch: 090 ----
mean loss: 310.54
 ---- batch: 100 ----
mean loss: 320.48
 ---- batch: 110 ----
mean loss: 308.97
train mean loss: 309.95
epoch train time: 0:00:02.139338
elapsed time: 0:06:30.600057
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 01:12:30.040011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.23
 ---- batch: 020 ----
mean loss: 308.61
 ---- batch: 030 ----
mean loss: 304.92
 ---- batch: 040 ----
mean loss: 295.29
 ---- batch: 050 ----
mean loss: 306.12
 ---- batch: 060 ----
mean loss: 302.63
 ---- batch: 070 ----
mean loss: 311.61
 ---- batch: 080 ----
mean loss: 300.04
 ---- batch: 090 ----
mean loss: 314.84
 ---- batch: 100 ----
mean loss: 311.27
 ---- batch: 110 ----
mean loss: 313.05
train mean loss: 306.87
epoch train time: 0:00:02.143024
elapsed time: 0:06:32.743231
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 01:12:32.183182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.01
 ---- batch: 020 ----
mean loss: 305.37
 ---- batch: 030 ----
mean loss: 301.00
 ---- batch: 040 ----
mean loss: 312.82
 ---- batch: 050 ----
mean loss: 313.99
 ---- batch: 060 ----
mean loss: 299.82
 ---- batch: 070 ----
mean loss: 292.51
 ---- batch: 080 ----
mean loss: 316.03
 ---- batch: 090 ----
mean loss: 307.70
 ---- batch: 100 ----
mean loss: 312.48
 ---- batch: 110 ----
mean loss: 313.75
train mean loss: 307.79
epoch train time: 0:00:02.139217
elapsed time: 0:06:34.882598
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 01:12:34.322549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.02
 ---- batch: 020 ----
mean loss: 302.09
 ---- batch: 030 ----
mean loss: 296.89
 ---- batch: 040 ----
mean loss: 314.42
 ---- batch: 050 ----
mean loss: 304.13
 ---- batch: 060 ----
mean loss: 303.43
 ---- batch: 070 ----
mean loss: 309.35
 ---- batch: 080 ----
mean loss: 311.96
 ---- batch: 090 ----
mean loss: 311.63
 ---- batch: 100 ----
mean loss: 304.89
 ---- batch: 110 ----
mean loss: 299.24
train mean loss: 305.44
epoch train time: 0:00:02.134519
elapsed time: 0:06:37.017264
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 01:12:36.457213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.48
 ---- batch: 020 ----
mean loss: 311.46
 ---- batch: 030 ----
mean loss: 302.46
 ---- batch: 040 ----
mean loss: 299.32
 ---- batch: 050 ----
mean loss: 300.32
 ---- batch: 060 ----
mean loss: 319.14
 ---- batch: 070 ----
mean loss: 301.79
 ---- batch: 080 ----
mean loss: 303.23
 ---- batch: 090 ----
mean loss: 307.50
 ---- batch: 100 ----
mean loss: 302.95
 ---- batch: 110 ----
mean loss: 299.09
train mean loss: 305.21
epoch train time: 0:00:02.140855
elapsed time: 0:06:39.158268
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 01:12:38.598239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.86
 ---- batch: 020 ----
mean loss: 314.37
 ---- batch: 030 ----
mean loss: 310.71
 ---- batch: 040 ----
mean loss: 303.81
 ---- batch: 050 ----
mean loss: 300.11
 ---- batch: 060 ----
mean loss: 300.50
 ---- batch: 070 ----
mean loss: 316.91
 ---- batch: 080 ----
mean loss: 309.72
 ---- batch: 090 ----
mean loss: 310.98
 ---- batch: 100 ----
mean loss: 310.26
 ---- batch: 110 ----
mean loss: 296.46
train mean loss: 306.31
epoch train time: 0:00:02.141556
elapsed time: 0:06:41.299991
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 01:12:40.739945
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.06
 ---- batch: 020 ----
mean loss: 305.41
 ---- batch: 030 ----
mean loss: 308.44
 ---- batch: 040 ----
mean loss: 301.27
 ---- batch: 050 ----
mean loss: 302.46
 ---- batch: 060 ----
mean loss: 303.08
 ---- batch: 070 ----
mean loss: 298.71
 ---- batch: 080 ----
mean loss: 304.24
 ---- batch: 090 ----
mean loss: 314.11
 ---- batch: 100 ----
mean loss: 299.59
 ---- batch: 110 ----
mean loss: 305.64
train mean loss: 304.19
epoch train time: 0:00:02.146063
elapsed time: 0:06:43.446207
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 01:12:42.886165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.57
 ---- batch: 020 ----
mean loss: 300.85
 ---- batch: 030 ----
mean loss: 310.97
 ---- batch: 040 ----
mean loss: 299.59
 ---- batch: 050 ----
mean loss: 294.83
 ---- batch: 060 ----
mean loss: 300.22
 ---- batch: 070 ----
mean loss: 310.36
 ---- batch: 080 ----
mean loss: 299.46
 ---- batch: 090 ----
mean loss: 313.97
 ---- batch: 100 ----
mean loss: 297.43
 ---- batch: 110 ----
mean loss: 308.10
train mean loss: 303.02
epoch train time: 0:00:02.137742
elapsed time: 0:06:45.584122
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 01:12:45.024065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.48
 ---- batch: 020 ----
mean loss: 310.09
 ---- batch: 030 ----
mean loss: 311.53
 ---- batch: 040 ----
mean loss: 304.77
 ---- batch: 050 ----
mean loss: 306.47
 ---- batch: 060 ----
mean loss: 302.79
 ---- batch: 070 ----
mean loss: 301.34
 ---- batch: 080 ----
mean loss: 299.31
 ---- batch: 090 ----
mean loss: 312.90
 ---- batch: 100 ----
mean loss: 311.40
 ---- batch: 110 ----
mean loss: 312.02
train mean loss: 306.17
epoch train time: 0:00:02.143101
elapsed time: 0:06:47.727368
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 01:12:47.167320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.95
 ---- batch: 020 ----
mean loss: 309.62
 ---- batch: 030 ----
mean loss: 303.29
 ---- batch: 040 ----
mean loss: 317.44
 ---- batch: 050 ----
mean loss: 310.82
 ---- batch: 060 ----
mean loss: 304.76
 ---- batch: 070 ----
mean loss: 315.28
 ---- batch: 080 ----
mean loss: 302.47
 ---- batch: 090 ----
mean loss: 304.06
 ---- batch: 100 ----
mean loss: 311.20
 ---- batch: 110 ----
mean loss: 299.09
train mean loss: 308.35
epoch train time: 0:00:02.139806
elapsed time: 0:06:49.867321
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 01:12:49.307272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.16
 ---- batch: 020 ----
mean loss: 309.16
 ---- batch: 030 ----
mean loss: 299.30
 ---- batch: 040 ----
mean loss: 315.55
 ---- batch: 050 ----
mean loss: 304.07
 ---- batch: 060 ----
mean loss: 296.72
 ---- batch: 070 ----
mean loss: 302.87
 ---- batch: 080 ----
mean loss: 308.65
 ---- batch: 090 ----
mean loss: 294.55
 ---- batch: 100 ----
mean loss: 296.39
 ---- batch: 110 ----
mean loss: 308.29
train mean loss: 304.01
epoch train time: 0:00:02.139711
elapsed time: 0:06:52.007186
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 01:12:51.447165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.03
 ---- batch: 020 ----
mean loss: 308.80
 ---- batch: 030 ----
mean loss: 301.58
 ---- batch: 040 ----
mean loss: 300.21
 ---- batch: 050 ----
mean loss: 301.32
 ---- batch: 060 ----
mean loss: 304.82
 ---- batch: 070 ----
mean loss: 293.49
 ---- batch: 080 ----
mean loss: 300.33
 ---- batch: 090 ----
mean loss: 313.10
 ---- batch: 100 ----
mean loss: 304.88
 ---- batch: 110 ----
mean loss: 289.06
train mean loss: 302.34
epoch train time: 0:00:02.146698
elapsed time: 0:06:54.154066
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 01:12:53.594019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.87
 ---- batch: 020 ----
mean loss: 318.96
 ---- batch: 030 ----
mean loss: 310.30
 ---- batch: 040 ----
mean loss: 292.31
 ---- batch: 050 ----
mean loss: 295.85
 ---- batch: 060 ----
mean loss: 303.67
 ---- batch: 070 ----
mean loss: 305.33
 ---- batch: 080 ----
mean loss: 299.48
 ---- batch: 090 ----
mean loss: 319.90
 ---- batch: 100 ----
mean loss: 305.41
 ---- batch: 110 ----
mean loss: 304.86
train mean loss: 305.81
epoch train time: 0:00:02.142961
elapsed time: 0:06:56.297185
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 01:12:55.737139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.67
 ---- batch: 020 ----
mean loss: 296.54
 ---- batch: 030 ----
mean loss: 311.85
 ---- batch: 040 ----
mean loss: 301.73
 ---- batch: 050 ----
mean loss: 309.58
 ---- batch: 060 ----
mean loss: 304.65
 ---- batch: 070 ----
mean loss: 308.41
 ---- batch: 080 ----
mean loss: 312.99
 ---- batch: 090 ----
mean loss: 303.26
 ---- batch: 100 ----
mean loss: 298.86
 ---- batch: 110 ----
mean loss: 306.08
train mean loss: 303.98
epoch train time: 0:00:02.146800
elapsed time: 0:06:58.444145
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 01:12:57.884097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.68
 ---- batch: 020 ----
mean loss: 300.09
 ---- batch: 030 ----
mean loss: 298.40
 ---- batch: 040 ----
mean loss: 298.69
 ---- batch: 050 ----
mean loss: 306.77
 ---- batch: 060 ----
mean loss: 310.58
 ---- batch: 070 ----
mean loss: 309.38
 ---- batch: 080 ----
mean loss: 310.96
 ---- batch: 090 ----
mean loss: 307.87
 ---- batch: 100 ----
mean loss: 302.09
 ---- batch: 110 ----
mean loss: 304.00
train mean loss: 304.61
epoch train time: 0:00:02.145612
elapsed time: 0:07:00.589926
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 01:13:00.029898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.44
 ---- batch: 020 ----
mean loss: 304.41
 ---- batch: 030 ----
mean loss: 308.44
 ---- batch: 040 ----
mean loss: 302.32
 ---- batch: 050 ----
mean loss: 303.86
 ---- batch: 060 ----
mean loss: 294.27
 ---- batch: 070 ----
mean loss: 305.93
 ---- batch: 080 ----
mean loss: 308.68
 ---- batch: 090 ----
mean loss: 301.74
 ---- batch: 100 ----
mean loss: 310.33
 ---- batch: 110 ----
mean loss: 308.41
train mean loss: 303.41
epoch train time: 0:00:02.145868
elapsed time: 0:07:02.735967
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 01:13:02.175931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.92
 ---- batch: 020 ----
mean loss: 308.31
 ---- batch: 030 ----
mean loss: 311.40
 ---- batch: 040 ----
mean loss: 306.90
 ---- batch: 050 ----
mean loss: 298.52
 ---- batch: 060 ----
mean loss: 306.76
 ---- batch: 070 ----
mean loss: 298.68
 ---- batch: 080 ----
mean loss: 299.33
 ---- batch: 090 ----
mean loss: 299.33
 ---- batch: 100 ----
mean loss: 296.82
 ---- batch: 110 ----
mean loss: 297.35
train mean loss: 303.45
epoch train time: 0:00:02.138934
elapsed time: 0:07:04.875069
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 01:13:04.315037
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.46
 ---- batch: 020 ----
mean loss: 310.87
 ---- batch: 030 ----
mean loss: 294.28
 ---- batch: 040 ----
mean loss: 304.67
 ---- batch: 050 ----
mean loss: 301.62
 ---- batch: 060 ----
mean loss: 309.30
 ---- batch: 070 ----
mean loss: 304.69
 ---- batch: 080 ----
mean loss: 303.59
 ---- batch: 090 ----
mean loss: 307.10
 ---- batch: 100 ----
mean loss: 305.89
 ---- batch: 110 ----
mean loss: 305.77
train mean loss: 304.07
epoch train time: 0:00:02.146538
elapsed time: 0:07:07.021812
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 01:13:06.461788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.16
 ---- batch: 020 ----
mean loss: 317.61
 ---- batch: 030 ----
mean loss: 299.47
 ---- batch: 040 ----
mean loss: 298.58
 ---- batch: 050 ----
mean loss: 306.70
 ---- batch: 060 ----
mean loss: 302.52
 ---- batch: 070 ----
mean loss: 300.55
 ---- batch: 080 ----
mean loss: 295.00
 ---- batch: 090 ----
mean loss: 292.08
 ---- batch: 100 ----
mean loss: 298.69
 ---- batch: 110 ----
mean loss: 309.48
train mean loss: 301.06
epoch train time: 0:00:02.142342
elapsed time: 0:07:09.164347
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 01:13:08.604314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.98
 ---- batch: 020 ----
mean loss: 300.43
 ---- batch: 030 ----
mean loss: 306.36
 ---- batch: 040 ----
mean loss: 303.12
 ---- batch: 050 ----
mean loss: 294.19
 ---- batch: 060 ----
mean loss: 303.64
 ---- batch: 070 ----
mean loss: 312.20
 ---- batch: 080 ----
mean loss: 300.01
 ---- batch: 090 ----
mean loss: 292.72
 ---- batch: 100 ----
mean loss: 302.80
 ---- batch: 110 ----
mean loss: 300.77
train mean loss: 302.50
epoch train time: 0:00:02.151063
elapsed time: 0:07:11.315588
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 01:13:10.755556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.98
 ---- batch: 020 ----
mean loss: 301.69
 ---- batch: 030 ----
mean loss: 312.80
 ---- batch: 040 ----
mean loss: 290.16
 ---- batch: 050 ----
mean loss: 300.95
 ---- batch: 060 ----
mean loss: 305.68
 ---- batch: 070 ----
mean loss: 306.43
 ---- batch: 080 ----
mean loss: 296.56
 ---- batch: 090 ----
mean loss: 298.68
 ---- batch: 100 ----
mean loss: 306.64
 ---- batch: 110 ----
mean loss: 316.92
train mean loss: 304.45
epoch train time: 0:00:02.143394
elapsed time: 0:07:13.459145
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 01:13:12.899097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.73
 ---- batch: 020 ----
mean loss: 300.08
 ---- batch: 030 ----
mean loss: 300.80
 ---- batch: 040 ----
mean loss: 311.06
 ---- batch: 050 ----
mean loss: 305.93
 ---- batch: 060 ----
mean loss: 313.70
 ---- batch: 070 ----
mean loss: 294.99
 ---- batch: 080 ----
mean loss: 305.41
 ---- batch: 090 ----
mean loss: 318.88
 ---- batch: 100 ----
mean loss: 303.29
 ---- batch: 110 ----
mean loss: 293.88
train mean loss: 303.95
epoch train time: 0:00:02.139725
elapsed time: 0:07:15.599023
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 01:13:15.038974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.45
 ---- batch: 020 ----
mean loss: 303.80
 ---- batch: 030 ----
mean loss: 304.30
 ---- batch: 040 ----
mean loss: 288.16
 ---- batch: 050 ----
mean loss: 294.64
 ---- batch: 060 ----
mean loss: 300.85
 ---- batch: 070 ----
mean loss: 309.07
 ---- batch: 080 ----
mean loss: 301.62
 ---- batch: 090 ----
mean loss: 306.32
 ---- batch: 100 ----
mean loss: 295.21
 ---- batch: 110 ----
mean loss: 296.29
train mean loss: 300.39
epoch train time: 0:00:02.142743
elapsed time: 0:07:17.741914
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 01:13:17.181867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.32
 ---- batch: 020 ----
mean loss: 303.50
 ---- batch: 030 ----
mean loss: 316.22
 ---- batch: 040 ----
mean loss: 304.83
 ---- batch: 050 ----
mean loss: 297.92
 ---- batch: 060 ----
mean loss: 295.07
 ---- batch: 070 ----
mean loss: 289.91
 ---- batch: 080 ----
mean loss: 305.39
 ---- batch: 090 ----
mean loss: 305.86
 ---- batch: 100 ----
mean loss: 292.11
 ---- batch: 110 ----
mean loss: 293.63
train mean loss: 300.40
epoch train time: 0:00:02.146981
elapsed time: 0:07:19.889039
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 01:13:19.329009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.52
 ---- batch: 020 ----
mean loss: 287.44
 ---- batch: 030 ----
mean loss: 300.36
 ---- batch: 040 ----
mean loss: 298.62
 ---- batch: 050 ----
mean loss: 289.79
 ---- batch: 060 ----
mean loss: 303.02
 ---- batch: 070 ----
mean loss: 311.04
 ---- batch: 080 ----
mean loss: 308.82
 ---- batch: 090 ----
mean loss: 299.06
 ---- batch: 100 ----
mean loss: 297.10
 ---- batch: 110 ----
mean loss: 308.24
train mean loss: 299.24
epoch train time: 0:00:02.138552
elapsed time: 0:07:22.027755
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 01:13:21.467707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.13
 ---- batch: 020 ----
mean loss: 294.96
 ---- batch: 030 ----
mean loss: 286.08
 ---- batch: 040 ----
mean loss: 297.88
 ---- batch: 050 ----
mean loss: 306.24
 ---- batch: 060 ----
mean loss: 302.72
 ---- batch: 070 ----
mean loss: 303.63
 ---- batch: 080 ----
mean loss: 309.21
 ---- batch: 090 ----
mean loss: 298.34
 ---- batch: 100 ----
mean loss: 291.87
 ---- batch: 110 ----
mean loss: 297.58
train mean loss: 299.04
epoch train time: 0:00:02.139637
elapsed time: 0:07:24.167542
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 01:13:23.607495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.08
 ---- batch: 020 ----
mean loss: 290.00
 ---- batch: 030 ----
mean loss: 297.57
 ---- batch: 040 ----
mean loss: 305.72
 ---- batch: 050 ----
mean loss: 311.15
 ---- batch: 060 ----
mean loss: 313.13
 ---- batch: 070 ----
mean loss: 288.87
 ---- batch: 080 ----
mean loss: 307.20
 ---- batch: 090 ----
mean loss: 303.03
 ---- batch: 100 ----
mean loss: 294.43
 ---- batch: 110 ----
mean loss: 302.51
train mean loss: 301.73
epoch train time: 0:00:02.143132
elapsed time: 0:07:26.310830
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 01:13:25.750785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.58
 ---- batch: 020 ----
mean loss: 297.23
 ---- batch: 030 ----
mean loss: 302.41
 ---- batch: 040 ----
mean loss: 304.73
 ---- batch: 050 ----
mean loss: 301.55
 ---- batch: 060 ----
mean loss: 284.52
 ---- batch: 070 ----
mean loss: 298.05
 ---- batch: 080 ----
mean loss: 304.69
 ---- batch: 090 ----
mean loss: 296.00
 ---- batch: 100 ----
mean loss: 300.88
 ---- batch: 110 ----
mean loss: 307.78
train mean loss: 300.57
epoch train time: 0:00:02.142865
elapsed time: 0:07:28.453848
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 01:13:27.893798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 297.65
 ---- batch: 020 ----
mean loss: 311.03
 ---- batch: 030 ----
mean loss: 301.71
 ---- batch: 040 ----
mean loss: 291.32
 ---- batch: 050 ----
mean loss: 287.18
 ---- batch: 060 ----
mean loss: 302.83
 ---- batch: 070 ----
mean loss: 306.14
 ---- batch: 080 ----
mean loss: 282.97
 ---- batch: 090 ----
mean loss: 304.81
 ---- batch: 100 ----
mean loss: 293.92
 ---- batch: 110 ----
mean loss: 300.72
train mean loss: 297.98
epoch train time: 0:00:02.149585
elapsed time: 0:07:30.603599
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 01:13:30.043555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.26
 ---- batch: 020 ----
mean loss: 301.76
 ---- batch: 030 ----
mean loss: 312.56
 ---- batch: 040 ----
mean loss: 298.32
 ---- batch: 050 ----
mean loss: 303.06
 ---- batch: 060 ----
mean loss: 293.93
 ---- batch: 070 ----
mean loss: 293.43
 ---- batch: 080 ----
mean loss: 303.84
 ---- batch: 090 ----
mean loss: 297.92
 ---- batch: 100 ----
mean loss: 286.80
 ---- batch: 110 ----
mean loss: 299.30
train mean loss: 299.30
epoch train time: 0:00:02.147871
elapsed time: 0:07:32.751626
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 01:13:32.191581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.67
 ---- batch: 020 ----
mean loss: 300.30
 ---- batch: 030 ----
mean loss: 300.14
 ---- batch: 040 ----
mean loss: 314.70
 ---- batch: 050 ----
mean loss: 290.03
 ---- batch: 060 ----
mean loss: 291.28
 ---- batch: 070 ----
mean loss: 301.01
 ---- batch: 080 ----
mean loss: 300.51
 ---- batch: 090 ----
mean loss: 299.35
 ---- batch: 100 ----
mean loss: 301.22
 ---- batch: 110 ----
mean loss: 301.20
train mean loss: 299.75
epoch train time: 0:00:02.146400
elapsed time: 0:07:34.898185
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 01:13:34.338156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.45
 ---- batch: 020 ----
mean loss: 302.53
 ---- batch: 030 ----
mean loss: 304.04
 ---- batch: 040 ----
mean loss: 293.23
 ---- batch: 050 ----
mean loss: 305.53
 ---- batch: 060 ----
mean loss: 292.60
 ---- batch: 070 ----
mean loss: 311.38
 ---- batch: 080 ----
mean loss: 307.30
 ---- batch: 090 ----
mean loss: 302.23
 ---- batch: 100 ----
mean loss: 307.15
 ---- batch: 110 ----
mean loss: 291.07
train mean loss: 300.88
epoch train time: 0:00:02.147445
elapsed time: 0:07:37.045816
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 01:13:36.485769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.05
 ---- batch: 020 ----
mean loss: 292.17
 ---- batch: 030 ----
mean loss: 289.89
 ---- batch: 040 ----
mean loss: 298.15
 ---- batch: 050 ----
mean loss: 302.86
 ---- batch: 060 ----
mean loss: 309.12
 ---- batch: 070 ----
mean loss: 289.30
 ---- batch: 080 ----
mean loss: 296.17
 ---- batch: 090 ----
mean loss: 303.99
 ---- batch: 100 ----
mean loss: 287.70
 ---- batch: 110 ----
mean loss: 302.33
train mean loss: 297.28
epoch train time: 0:00:02.159371
elapsed time: 0:07:39.205338
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 01:13:38.645290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.09
 ---- batch: 020 ----
mean loss: 288.91
 ---- batch: 030 ----
mean loss: 310.06
 ---- batch: 040 ----
mean loss: 295.07
 ---- batch: 050 ----
mean loss: 293.47
 ---- batch: 060 ----
mean loss: 312.76
 ---- batch: 070 ----
mean loss: 302.98
 ---- batch: 080 ----
mean loss: 300.68
 ---- batch: 090 ----
mean loss: 298.63
 ---- batch: 100 ----
mean loss: 295.46
 ---- batch: 110 ----
mean loss: 300.31
train mean loss: 299.34
epoch train time: 0:00:02.147641
elapsed time: 0:07:41.353133
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 01:13:40.793104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.13
 ---- batch: 020 ----
mean loss: 297.66
 ---- batch: 030 ----
mean loss: 305.41
 ---- batch: 040 ----
mean loss: 303.78
 ---- batch: 050 ----
mean loss: 298.24
 ---- batch: 060 ----
mean loss: 308.96
 ---- batch: 070 ----
mean loss: 294.99
 ---- batch: 080 ----
mean loss: 291.03
 ---- batch: 090 ----
mean loss: 298.03
 ---- batch: 100 ----
mean loss: 286.97
 ---- batch: 110 ----
mean loss: 307.95
train mean loss: 299.09
epoch train time: 0:00:02.147633
elapsed time: 0:07:43.500927
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 01:13:42.940892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.12
 ---- batch: 020 ----
mean loss: 286.06
 ---- batch: 030 ----
mean loss: 291.52
 ---- batch: 040 ----
mean loss: 292.26
 ---- batch: 050 ----
mean loss: 283.98
 ---- batch: 060 ----
mean loss: 304.35
 ---- batch: 070 ----
mean loss: 290.95
 ---- batch: 080 ----
mean loss: 290.37
 ---- batch: 090 ----
mean loss: 293.69
 ---- batch: 100 ----
mean loss: 287.15
 ---- batch: 110 ----
mean loss: 294.18
train mean loss: 290.69
epoch train time: 0:00:02.139915
elapsed time: 0:07:45.641032
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 01:13:45.080973
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.52
 ---- batch: 020 ----
mean loss: 294.10
 ---- batch: 030 ----
mean loss: 283.73
 ---- batch: 040 ----
mean loss: 296.22
 ---- batch: 050 ----
mean loss: 287.53
 ---- batch: 060 ----
mean loss: 291.85
 ---- batch: 070 ----
mean loss: 299.22
 ---- batch: 080 ----
mean loss: 296.28
 ---- batch: 090 ----
mean loss: 299.05
 ---- batch: 100 ----
mean loss: 282.31
 ---- batch: 110 ----
mean loss: 285.53
train mean loss: 292.25
epoch train time: 0:00:02.142016
elapsed time: 0:07:47.783207
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 01:13:47.223183
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.00
 ---- batch: 020 ----
mean loss: 285.42
 ---- batch: 030 ----
mean loss: 293.25
 ---- batch: 040 ----
mean loss: 282.87
 ---- batch: 050 ----
mean loss: 288.10
 ---- batch: 060 ----
mean loss: 292.71
 ---- batch: 070 ----
mean loss: 284.98
 ---- batch: 080 ----
mean loss: 296.54
 ---- batch: 090 ----
mean loss: 290.08
 ---- batch: 100 ----
mean loss: 296.14
 ---- batch: 110 ----
mean loss: 281.28
train mean loss: 289.11
epoch train time: 0:00:02.145227
elapsed time: 0:07:49.928624
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 01:13:49.368588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 286.00
 ---- batch: 020 ----
mean loss: 292.76
 ---- batch: 030 ----
mean loss: 294.40
 ---- batch: 040 ----
mean loss: 288.42
 ---- batch: 050 ----
mean loss: 287.13
 ---- batch: 060 ----
mean loss: 287.37
 ---- batch: 070 ----
mean loss: 293.06
 ---- batch: 080 ----
mean loss: 290.09
 ---- batch: 090 ----
mean loss: 294.10
 ---- batch: 100 ----
mean loss: 283.56
 ---- batch: 110 ----
mean loss: 282.88
train mean loss: 288.32
epoch train time: 0:00:02.139909
elapsed time: 0:07:52.068707
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 01:13:51.508675
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.50
 ---- batch: 020 ----
mean loss: 286.05
 ---- batch: 030 ----
mean loss: 287.59
 ---- batch: 040 ----
mean loss: 295.60
 ---- batch: 050 ----
mean loss: 294.80
 ---- batch: 060 ----
mean loss: 291.79
 ---- batch: 070 ----
mean loss: 297.16
 ---- batch: 080 ----
mean loss: 287.02
 ---- batch: 090 ----
mean loss: 283.87
 ---- batch: 100 ----
mean loss: 275.93
 ---- batch: 110 ----
mean loss: 292.52
train mean loss: 289.38
epoch train time: 0:00:02.151262
elapsed time: 0:07:54.220150
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 01:13:53.660107
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.25
 ---- batch: 020 ----
mean loss: 295.72
 ---- batch: 030 ----
mean loss: 288.44
 ---- batch: 040 ----
mean loss: 292.57
 ---- batch: 050 ----
mean loss: 281.42
 ---- batch: 060 ----
mean loss: 295.14
 ---- batch: 070 ----
mean loss: 291.52
 ---- batch: 080 ----
mean loss: 294.48
 ---- batch: 090 ----
mean loss: 283.34
 ---- batch: 100 ----
mean loss: 282.17
 ---- batch: 110 ----
mean loss: 293.78
train mean loss: 290.27
epoch train time: 0:00:02.153450
elapsed time: 0:07:56.373824
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 01:13:55.813795
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.98
 ---- batch: 020 ----
mean loss: 301.81
 ---- batch: 030 ----
mean loss: 285.47
 ---- batch: 040 ----
mean loss: 297.42
 ---- batch: 050 ----
mean loss: 282.03
 ---- batch: 060 ----
mean loss: 289.91
 ---- batch: 070 ----
mean loss: 277.54
 ---- batch: 080 ----
mean loss: 285.94
 ---- batch: 090 ----
mean loss: 283.05
 ---- batch: 100 ----
mean loss: 301.68
 ---- batch: 110 ----
mean loss: 286.94
train mean loss: 288.56
epoch train time: 0:00:02.142471
elapsed time: 0:07:58.516471
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 01:13:57.956426
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.14
 ---- batch: 020 ----
mean loss: 287.67
 ---- batch: 030 ----
mean loss: 285.36
 ---- batch: 040 ----
mean loss: 292.30
 ---- batch: 050 ----
mean loss: 292.16
 ---- batch: 060 ----
mean loss: 295.98
 ---- batch: 070 ----
mean loss: 299.77
 ---- batch: 080 ----
mean loss: 290.37
 ---- batch: 090 ----
mean loss: 286.97
 ---- batch: 100 ----
mean loss: 285.94
 ---- batch: 110 ----
mean loss: 289.79
train mean loss: 290.09
epoch train time: 0:00:02.139880
elapsed time: 0:08:00.656518
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 01:14:00.096474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 293.54
 ---- batch: 020 ----
mean loss: 281.67
 ---- batch: 030 ----
mean loss: 291.41
 ---- batch: 040 ----
mean loss: 285.37
 ---- batch: 050 ----
mean loss: 283.91
 ---- batch: 060 ----
mean loss: 287.41
 ---- batch: 070 ----
mean loss: 288.34
 ---- batch: 080 ----
mean loss: 289.56
 ---- batch: 090 ----
mean loss: 295.58
 ---- batch: 100 ----
mean loss: 290.83
 ---- batch: 110 ----
mean loss: 300.99
train mean loss: 289.71
epoch train time: 0:00:02.143664
elapsed time: 0:08:02.800335
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 01:14:02.240310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 292.82
 ---- batch: 020 ----
mean loss: 290.47
 ---- batch: 030 ----
mean loss: 289.17
 ---- batch: 040 ----
mean loss: 293.42
 ---- batch: 050 ----
mean loss: 291.07
 ---- batch: 060 ----
mean loss: 293.31
 ---- batch: 070 ----
mean loss: 284.29
 ---- batch: 080 ----
mean loss: 282.62
 ---- batch: 090 ----
mean loss: 276.58
 ---- batch: 100 ----
mean loss: 284.63
 ---- batch: 110 ----
mean loss: 292.73
train mean loss: 288.87
epoch train time: 0:00:02.138159
elapsed time: 0:08:04.938666
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 01:14:04.378618
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.46
 ---- batch: 020 ----
mean loss: 291.70
 ---- batch: 030 ----
mean loss: 298.72
 ---- batch: 040 ----
mean loss: 293.75
 ---- batch: 050 ----
mean loss: 296.97
 ---- batch: 060 ----
mean loss: 292.17
 ---- batch: 070 ----
mean loss: 279.70
 ---- batch: 080 ----
mean loss: 285.38
 ---- batch: 090 ----
mean loss: 288.56
 ---- batch: 100 ----
mean loss: 286.61
 ---- batch: 110 ----
mean loss: 288.78
train mean loss: 290.25
epoch train time: 0:00:02.145665
elapsed time: 0:08:07.084501
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 01:14:06.524470
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.74
 ---- batch: 020 ----
mean loss: 289.35
 ---- batch: 030 ----
mean loss: 289.41
 ---- batch: 040 ----
mean loss: 287.90
 ---- batch: 050 ----
mean loss: 295.71
 ---- batch: 060 ----
mean loss: 296.23
 ---- batch: 070 ----
mean loss: 291.49
 ---- batch: 080 ----
mean loss: 285.52
 ---- batch: 090 ----
mean loss: 282.36
 ---- batch: 100 ----
mean loss: 291.35
 ---- batch: 110 ----
mean loss: 286.53
train mean loss: 288.76
epoch train time: 0:00:02.137882
elapsed time: 0:08:09.222553
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 01:14:08.662514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.98
 ---- batch: 020 ----
mean loss: 283.77
 ---- batch: 030 ----
mean loss: 285.18
 ---- batch: 040 ----
mean loss: 299.90
 ---- batch: 050 ----
mean loss: 281.79
 ---- batch: 060 ----
mean loss: 292.11
 ---- batch: 070 ----
mean loss: 282.25
 ---- batch: 080 ----
mean loss: 291.57
 ---- batch: 090 ----
mean loss: 277.90
 ---- batch: 100 ----
mean loss: 291.67
 ---- batch: 110 ----
mean loss: 297.92
train mean loss: 288.45
epoch train time: 0:00:02.141432
elapsed time: 0:08:11.364161
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 01:14:10.804130
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.05
 ---- batch: 020 ----
mean loss: 294.46
 ---- batch: 030 ----
mean loss: 292.16
 ---- batch: 040 ----
mean loss: 280.81
 ---- batch: 050 ----
mean loss: 289.29
 ---- batch: 060 ----
mean loss: 292.23
 ---- batch: 070 ----
mean loss: 303.60
 ---- batch: 080 ----
mean loss: 289.03
 ---- batch: 090 ----
mean loss: 286.85
 ---- batch: 100 ----
mean loss: 290.51
 ---- batch: 110 ----
mean loss: 289.57
train mean loss: 290.12
epoch train time: 0:00:02.143061
elapsed time: 0:08:13.507383
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 01:14:12.947335
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.25
 ---- batch: 020 ----
mean loss: 297.60
 ---- batch: 030 ----
mean loss: 295.66
 ---- batch: 040 ----
mean loss: 298.43
 ---- batch: 050 ----
mean loss: 293.77
 ---- batch: 060 ----
mean loss: 284.60
 ---- batch: 070 ----
mean loss: 291.60
 ---- batch: 080 ----
mean loss: 284.01
 ---- batch: 090 ----
mean loss: 273.69
 ---- batch: 100 ----
mean loss: 281.84
 ---- batch: 110 ----
mean loss: 294.11
train mean loss: 289.16
epoch train time: 0:00:02.139170
elapsed time: 0:08:15.646728
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 01:14:15.086683
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.75
 ---- batch: 020 ----
mean loss: 287.82
 ---- batch: 030 ----
mean loss: 290.49
 ---- batch: 040 ----
mean loss: 289.33
 ---- batch: 050 ----
mean loss: 289.41
 ---- batch: 060 ----
mean loss: 279.83
 ---- batch: 070 ----
mean loss: 285.37
 ---- batch: 080 ----
mean loss: 287.56
 ---- batch: 090 ----
mean loss: 289.82
 ---- batch: 100 ----
mean loss: 298.61
 ---- batch: 110 ----
mean loss: 286.80
train mean loss: 288.25
epoch train time: 0:00:02.137492
elapsed time: 0:08:17.784404
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 01:14:17.224377
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.76
 ---- batch: 020 ----
mean loss: 275.83
 ---- batch: 030 ----
mean loss: 288.18
 ---- batch: 040 ----
mean loss: 287.70
 ---- batch: 050 ----
mean loss: 288.94
 ---- batch: 060 ----
mean loss: 291.67
 ---- batch: 070 ----
mean loss: 294.80
 ---- batch: 080 ----
mean loss: 282.53
 ---- batch: 090 ----
mean loss: 285.86
 ---- batch: 100 ----
mean loss: 296.48
 ---- batch: 110 ----
mean loss: 296.92
train mean loss: 288.77
epoch train time: 0:00:02.140003
elapsed time: 0:08:19.924573
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 01:14:19.364523
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.90
 ---- batch: 020 ----
mean loss: 291.79
 ---- batch: 030 ----
mean loss: 290.35
 ---- batch: 040 ----
mean loss: 294.68
 ---- batch: 050 ----
mean loss: 288.08
 ---- batch: 060 ----
mean loss: 292.40
 ---- batch: 070 ----
mean loss: 288.98
 ---- batch: 080 ----
mean loss: 286.29
 ---- batch: 090 ----
mean loss: 288.88
 ---- batch: 100 ----
mean loss: 286.53
 ---- batch: 110 ----
mean loss: 289.12
train mean loss: 289.15
epoch train time: 0:00:02.138371
elapsed time: 0:08:22.063089
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 01:14:21.503039
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.13
 ---- batch: 020 ----
mean loss: 290.78
 ---- batch: 030 ----
mean loss: 293.44
 ---- batch: 040 ----
mean loss: 288.00
 ---- batch: 050 ----
mean loss: 294.39
 ---- batch: 060 ----
mean loss: 286.95
 ---- batch: 070 ----
mean loss: 290.18
 ---- batch: 080 ----
mean loss: 294.99
 ---- batch: 090 ----
mean loss: 286.72
 ---- batch: 100 ----
mean loss: 289.45
 ---- batch: 110 ----
mean loss: 286.25
train mean loss: 289.61
epoch train time: 0:00:02.139405
elapsed time: 0:08:24.202642
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 01:14:23.642593
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.57
 ---- batch: 020 ----
mean loss: 298.65
 ---- batch: 030 ----
mean loss: 292.80
 ---- batch: 040 ----
mean loss: 285.09
 ---- batch: 050 ----
mean loss: 295.46
 ---- batch: 060 ----
mean loss: 285.38
 ---- batch: 070 ----
mean loss: 288.88
 ---- batch: 080 ----
mean loss: 289.19
 ---- batch: 090 ----
mean loss: 294.92
 ---- batch: 100 ----
mean loss: 289.60
 ---- batch: 110 ----
mean loss: 279.53
train mean loss: 289.02
epoch train time: 0:00:02.137092
elapsed time: 0:08:26.339885
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 01:14:25.779855
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.57
 ---- batch: 020 ----
mean loss: 290.34
 ---- batch: 030 ----
mean loss: 290.25
 ---- batch: 040 ----
mean loss: 291.68
 ---- batch: 050 ----
mean loss: 281.57
 ---- batch: 060 ----
mean loss: 278.42
 ---- batch: 070 ----
mean loss: 289.53
 ---- batch: 080 ----
mean loss: 294.67
 ---- batch: 090 ----
mean loss: 292.25
 ---- batch: 100 ----
mean loss: 287.17
 ---- batch: 110 ----
mean loss: 287.59
train mean loss: 288.36
epoch train time: 0:00:02.146001
elapsed time: 0:08:28.486075
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 01:14:27.926029
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 285.39
 ---- batch: 020 ----
mean loss: 290.42
 ---- batch: 030 ----
mean loss: 293.46
 ---- batch: 040 ----
mean loss: 280.22
 ---- batch: 050 ----
mean loss: 286.13
 ---- batch: 060 ----
mean loss: 293.19
 ---- batch: 070 ----
mean loss: 283.43
 ---- batch: 080 ----
mean loss: 282.59
 ---- batch: 090 ----
mean loss: 281.32
 ---- batch: 100 ----
mean loss: 289.54
 ---- batch: 110 ----
mean loss: 280.44
train mean loss: 286.00
epoch train time: 0:00:02.152974
elapsed time: 0:08:30.639210
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 01:14:30.079182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.36
 ---- batch: 020 ----
mean loss: 278.60
 ---- batch: 030 ----
mean loss: 278.10
 ---- batch: 040 ----
mean loss: 294.33
 ---- batch: 050 ----
mean loss: 288.94
 ---- batch: 060 ----
mean loss: 283.71
 ---- batch: 070 ----
mean loss: 290.37
 ---- batch: 080 ----
mean loss: 286.88
 ---- batch: 090 ----
mean loss: 283.23
 ---- batch: 100 ----
mean loss: 290.22
 ---- batch: 110 ----
mean loss: 282.48
train mean loss: 286.76
epoch train time: 0:00:02.144303
elapsed time: 0:08:32.783688
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 01:14:32.223654
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.37
 ---- batch: 020 ----
mean loss: 292.75
 ---- batch: 030 ----
mean loss: 295.99
 ---- batch: 040 ----
mean loss: 294.22
 ---- batch: 050 ----
mean loss: 292.24
 ---- batch: 060 ----
mean loss: 296.22
 ---- batch: 070 ----
mean loss: 293.31
 ---- batch: 080 ----
mean loss: 290.85
 ---- batch: 090 ----
mean loss: 289.88
 ---- batch: 100 ----
mean loss: 278.40
 ---- batch: 110 ----
mean loss: 281.93
train mean loss: 290.18
epoch train time: 0:00:02.139614
elapsed time: 0:08:34.923462
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 01:14:34.363422
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.37
 ---- batch: 020 ----
mean loss: 280.12
 ---- batch: 030 ----
mean loss: 289.77
 ---- batch: 040 ----
mean loss: 298.86
 ---- batch: 050 ----
mean loss: 279.97
 ---- batch: 060 ----
mean loss: 293.14
 ---- batch: 070 ----
mean loss: 284.25
 ---- batch: 080 ----
mean loss: 282.08
 ---- batch: 090 ----
mean loss: 290.79
 ---- batch: 100 ----
mean loss: 293.12
 ---- batch: 110 ----
mean loss: 287.07
train mean loss: 288.70
epoch train time: 0:00:02.143903
elapsed time: 0:08:37.067528
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 01:14:36.507482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.03
 ---- batch: 020 ----
mean loss: 296.13
 ---- batch: 030 ----
mean loss: 284.47
 ---- batch: 040 ----
mean loss: 280.26
 ---- batch: 050 ----
mean loss: 290.68
 ---- batch: 060 ----
mean loss: 273.30
 ---- batch: 070 ----
mean loss: 299.44
 ---- batch: 080 ----
mean loss: 288.29
 ---- batch: 090 ----
mean loss: 299.85
 ---- batch: 100 ----
mean loss: 293.45
 ---- batch: 110 ----
mean loss: 290.46
train mean loss: 288.61
epoch train time: 0:00:02.145204
elapsed time: 0:08:39.212944
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 01:14:38.652931
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 273.76
 ---- batch: 020 ----
mean loss: 282.60
 ---- batch: 030 ----
mean loss: 288.05
 ---- batch: 040 ----
mean loss: 281.58
 ---- batch: 050 ----
mean loss: 280.66
 ---- batch: 060 ----
mean loss: 291.20
 ---- batch: 070 ----
mean loss: 301.81
 ---- batch: 080 ----
mean loss: 301.30
 ---- batch: 090 ----
mean loss: 287.73
 ---- batch: 100 ----
mean loss: 287.48
 ---- batch: 110 ----
mean loss: 276.90
train mean loss: 286.29
epoch train time: 0:00:02.150826
elapsed time: 0:08:41.363956
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 01:14:40.803909
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.57
 ---- batch: 020 ----
mean loss: 288.78
 ---- batch: 030 ----
mean loss: 284.33
 ---- batch: 040 ----
mean loss: 290.45
 ---- batch: 050 ----
mean loss: 287.96
 ---- batch: 060 ----
mean loss: 296.77
 ---- batch: 070 ----
mean loss: 294.26
 ---- batch: 080 ----
mean loss: 293.75
 ---- batch: 090 ----
mean loss: 281.55
 ---- batch: 100 ----
mean loss: 286.11
 ---- batch: 110 ----
mean loss: 285.57
train mean loss: 289.30
epoch train time: 0:00:02.146771
elapsed time: 0:08:43.510881
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 01:14:42.950837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.12
 ---- batch: 020 ----
mean loss: 294.75
 ---- batch: 030 ----
mean loss: 298.41
 ---- batch: 040 ----
mean loss: 288.82
 ---- batch: 050 ----
mean loss: 293.94
 ---- batch: 060 ----
mean loss: 284.79
 ---- batch: 070 ----
mean loss: 294.51
 ---- batch: 080 ----
mean loss: 282.75
 ---- batch: 090 ----
mean loss: 275.82
 ---- batch: 100 ----
mean loss: 283.14
 ---- batch: 110 ----
mean loss: 284.13
train mean loss: 288.11
epoch train time: 0:00:02.145840
elapsed time: 0:08:45.656877
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 01:14:45.096849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.99
 ---- batch: 020 ----
mean loss: 290.88
 ---- batch: 030 ----
mean loss: 291.11
 ---- batch: 040 ----
mean loss: 297.49
 ---- batch: 050 ----
mean loss: 285.93
 ---- batch: 060 ----
mean loss: 285.07
 ---- batch: 070 ----
mean loss: 286.53
 ---- batch: 080 ----
mean loss: 290.71
 ---- batch: 090 ----
mean loss: 292.34
 ---- batch: 100 ----
mean loss: 285.17
 ---- batch: 110 ----
mean loss: 298.84
train mean loss: 290.66
epoch train time: 0:00:02.143010
elapsed time: 0:08:47.800090
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 01:14:47.240046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.17
 ---- batch: 020 ----
mean loss: 282.76
 ---- batch: 030 ----
mean loss: 284.31
 ---- batch: 040 ----
mean loss: 293.21
 ---- batch: 050 ----
mean loss: 298.61
 ---- batch: 060 ----
mean loss: 285.48
 ---- batch: 070 ----
mean loss: 282.27
 ---- batch: 080 ----
mean loss: 298.75
 ---- batch: 090 ----
mean loss: 284.43
 ---- batch: 100 ----
mean loss: 293.17
 ---- batch: 110 ----
mean loss: 291.27
train mean loss: 288.42
epoch train time: 0:00:02.147472
elapsed time: 0:08:49.947720
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 01:14:49.387676
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.36
 ---- batch: 020 ----
mean loss: 282.66
 ---- batch: 030 ----
mean loss: 284.92
 ---- batch: 040 ----
mean loss: 286.86
 ---- batch: 050 ----
mean loss: 286.07
 ---- batch: 060 ----
mean loss: 284.85
 ---- batch: 070 ----
mean loss: 287.13
 ---- batch: 080 ----
mean loss: 289.57
 ---- batch: 090 ----
mean loss: 290.85
 ---- batch: 100 ----
mean loss: 284.79
 ---- batch: 110 ----
mean loss: 294.38
train mean loss: 287.81
epoch train time: 0:00:02.146667
elapsed time: 0:08:52.094546
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 01:14:51.534502
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 288.48
 ---- batch: 020 ----
mean loss: 282.52
 ---- batch: 030 ----
mean loss: 282.35
 ---- batch: 040 ----
mean loss: 290.31
 ---- batch: 050 ----
mean loss: 287.39
 ---- batch: 060 ----
mean loss: 294.99
 ---- batch: 070 ----
mean loss: 294.16
 ---- batch: 080 ----
mean loss: 289.92
 ---- batch: 090 ----
mean loss: 277.24
 ---- batch: 100 ----
mean loss: 296.37
 ---- batch: 110 ----
mean loss: 287.23
train mean loss: 288.08
epoch train time: 0:00:02.155497
elapsed time: 0:08:54.250229
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 01:14:53.690179
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.94
 ---- batch: 020 ----
mean loss: 283.14
 ---- batch: 030 ----
mean loss: 276.82
 ---- batch: 040 ----
mean loss: 285.88
 ---- batch: 050 ----
mean loss: 285.96
 ---- batch: 060 ----
mean loss: 293.76
 ---- batch: 070 ----
mean loss: 290.67
 ---- batch: 080 ----
mean loss: 288.93
 ---- batch: 090 ----
mean loss: 293.11
 ---- batch: 100 ----
mean loss: 290.17
 ---- batch: 110 ----
mean loss: 288.62
train mean loss: 288.18
epoch train time: 0:00:02.146320
elapsed time: 0:08:56.396709
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 01:14:55.836662
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 287.70
 ---- batch: 020 ----
mean loss: 275.73
 ---- batch: 030 ----
mean loss: 284.84
 ---- batch: 040 ----
mean loss: 287.27
 ---- batch: 050 ----
mean loss: 293.20
 ---- batch: 060 ----
mean loss: 290.45
 ---- batch: 070 ----
mean loss: 286.81
 ---- batch: 080 ----
mean loss: 279.14
 ---- batch: 090 ----
mean loss: 289.16
 ---- batch: 100 ----
mean loss: 285.90
 ---- batch: 110 ----
mean loss: 290.09
train mean loss: 286.28
epoch train time: 0:00:02.146051
elapsed time: 0:08:58.542968
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 01:14:57.982999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 290.05
 ---- batch: 020 ----
mean loss: 296.78
 ---- batch: 030 ----
mean loss: 288.25
 ---- batch: 040 ----
mean loss: 289.78
 ---- batch: 050 ----
mean loss: 285.18
 ---- batch: 060 ----
mean loss: 296.54
 ---- batch: 070 ----
mean loss: 292.23
 ---- batch: 080 ----
mean loss: 282.07
 ---- batch: 090 ----
mean loss: 289.44
 ---- batch: 100 ----
mean loss: 281.28
 ---- batch: 110 ----
mean loss: 288.46
train mean loss: 289.03
epoch train time: 0:00:02.148831
elapsed time: 0:09:00.692057
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 01:15:00.132028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 294.22
 ---- batch: 020 ----
mean loss: 295.87
 ---- batch: 030 ----
mean loss: 277.46
 ---- batch: 040 ----
mean loss: 280.91
 ---- batch: 050 ----
mean loss: 289.94
 ---- batch: 060 ----
mean loss: 287.72
 ---- batch: 070 ----
mean loss: 284.79
 ---- batch: 080 ----
mean loss: 284.70
 ---- batch: 090 ----
mean loss: 289.77
 ---- batch: 100 ----
mean loss: 294.31
 ---- batch: 110 ----
mean loss: 289.18
train mean loss: 287.90
epoch train time: 0:00:02.142541
elapsed time: 0:09:02.834791
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 01:15:02.274748
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.47
 ---- batch: 020 ----
mean loss: 290.43
 ---- batch: 030 ----
mean loss: 289.27
 ---- batch: 040 ----
mean loss: 293.09
 ---- batch: 050 ----
mean loss: 291.42
 ---- batch: 060 ----
mean loss: 282.59
 ---- batch: 070 ----
mean loss: 299.73
 ---- batch: 080 ----
mean loss: 283.14
 ---- batch: 090 ----
mean loss: 277.17
 ---- batch: 100 ----
mean loss: 292.77
 ---- batch: 110 ----
mean loss: 282.22
train mean loss: 287.61
epoch train time: 0:00:02.144049
elapsed time: 0:09:04.979028
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 01:15:04.418999
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 280.92
 ---- batch: 020 ----
mean loss: 281.06
 ---- batch: 030 ----
mean loss: 283.66
 ---- batch: 040 ----
mean loss: 291.50
 ---- batch: 050 ----
mean loss: 291.07
 ---- batch: 060 ----
mean loss: 298.33
 ---- batch: 070 ----
mean loss: 289.96
 ---- batch: 080 ----
mean loss: 292.05
 ---- batch: 090 ----
mean loss: 287.19
 ---- batch: 100 ----
mean loss: 285.05
 ---- batch: 110 ----
mean loss: 278.98
train mean loss: 287.22
epoch train time: 0:00:02.155031
elapsed time: 0:09:07.134237
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 01:15:06.574191
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 282.08
 ---- batch: 020 ----
mean loss: 280.77
 ---- batch: 030 ----
mean loss: 281.96
 ---- batch: 040 ----
mean loss: 281.56
 ---- batch: 050 ----
mean loss: 277.13
 ---- batch: 060 ----
mean loss: 283.45
 ---- batch: 070 ----
mean loss: 293.44
 ---- batch: 080 ----
mean loss: 288.37
 ---- batch: 090 ----
mean loss: 304.32
 ---- batch: 100 ----
mean loss: 281.36
 ---- batch: 110 ----
mean loss: 295.28
train mean loss: 286.53
epoch train time: 0:00:02.146735
elapsed time: 0:09:09.281127
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 01:15:08.721081
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.86
 ---- batch: 020 ----
mean loss: 286.29
 ---- batch: 030 ----
mean loss: 284.37
 ---- batch: 040 ----
mean loss: 279.72
 ---- batch: 050 ----
mean loss: 291.27
 ---- batch: 060 ----
mean loss: 285.69
 ---- batch: 070 ----
mean loss: 290.15
 ---- batch: 080 ----
mean loss: 278.99
 ---- batch: 090 ----
mean loss: 293.49
 ---- batch: 100 ----
mean loss: 292.54
 ---- batch: 110 ----
mean loss: 287.02
train mean loss: 286.39
epoch train time: 0:00:02.153951
elapsed time: 0:09:11.435289
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 01:15:10.875248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 289.74
 ---- batch: 020 ----
mean loss: 294.97
 ---- batch: 030 ----
mean loss: 298.37
 ---- batch: 040 ----
mean loss: 286.00
 ---- batch: 050 ----
mean loss: 287.82
 ---- batch: 060 ----
mean loss: 288.04
 ---- batch: 070 ----
mean loss: 295.56
 ---- batch: 080 ----
mean loss: 276.82
 ---- batch: 090 ----
mean loss: 288.11
 ---- batch: 100 ----
mean loss: 281.30
 ---- batch: 110 ----
mean loss: 280.32
train mean loss: 287.43
epoch train time: 0:00:02.154037
elapsed time: 0:09:13.589501
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 01:15:13.029482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 275.64
 ---- batch: 020 ----
mean loss: 296.81
 ---- batch: 030 ----
mean loss: 283.06
 ---- batch: 040 ----
mean loss: 300.29
 ---- batch: 050 ----
mean loss: 287.65
 ---- batch: 060 ----
mean loss: 291.57
 ---- batch: 070 ----
mean loss: 295.24
 ---- batch: 080 ----
mean loss: 285.73
 ---- batch: 090 ----
mean loss: 280.70
 ---- batch: 100 ----
mean loss: 285.20
 ---- batch: 110 ----
mean loss: 284.68
train mean loss: 287.74
epoch train time: 0:00:02.163840
elapsed time: 0:09:15.753529
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 01:15:15.193487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 278.37
 ---- batch: 020 ----
mean loss: 281.55
 ---- batch: 030 ----
mean loss: 284.62
 ---- batch: 040 ----
mean loss: 286.95
 ---- batch: 050 ----
mean loss: 294.07
 ---- batch: 060 ----
mean loss: 286.03
 ---- batch: 070 ----
mean loss: 294.29
 ---- batch: 080 ----
mean loss: 288.43
 ---- batch: 090 ----
mean loss: 284.43
 ---- batch: 100 ----
mean loss: 291.10
 ---- batch: 110 ----
mean loss: 282.23
train mean loss: 286.52
epoch train time: 0:00:02.154372
elapsed time: 0:09:17.908071
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 01:15:17.348028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.21
 ---- batch: 020 ----
mean loss: 284.56
 ---- batch: 030 ----
mean loss: 291.08
 ---- batch: 040 ----
mean loss: 289.36
 ---- batch: 050 ----
mean loss: 275.65
 ---- batch: 060 ----
mean loss: 296.05
 ---- batch: 070 ----
mean loss: 287.38
 ---- batch: 080 ----
mean loss: 282.50
 ---- batch: 090 ----
mean loss: 286.49
 ---- batch: 100 ----
mean loss: 280.29
 ---- batch: 110 ----
mean loss: 296.18
train mean loss: 286.47
epoch train time: 0:00:02.152933
elapsed time: 0:09:20.061182
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 01:15:19.501135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 291.60
 ---- batch: 020 ----
mean loss: 279.80
 ---- batch: 030 ----
mean loss: 280.49
 ---- batch: 040 ----
mean loss: 288.28
 ---- batch: 050 ----
mean loss: 294.53
 ---- batch: 060 ----
mean loss: 283.03
 ---- batch: 070 ----
mean loss: 287.46
 ---- batch: 080 ----
mean loss: 285.64
 ---- batch: 090 ----
mean loss: 285.35
 ---- batch: 100 ----
mean loss: 285.13
 ---- batch: 110 ----
mean loss: 287.02
train mean loss: 286.77
epoch train time: 0:00:02.142788
elapsed time: 0:09:22.204132
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 01:15:21.644090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 271.56
 ---- batch: 020 ----
mean loss: 293.74
 ---- batch: 030 ----
mean loss: 289.87
 ---- batch: 040 ----
mean loss: 294.28
 ---- batch: 050 ----
mean loss: 283.13
 ---- batch: 060 ----
mean loss: 290.85
 ---- batch: 070 ----
mean loss: 286.12
 ---- batch: 080 ----
mean loss: 289.40
 ---- batch: 090 ----
mean loss: 278.04
 ---- batch: 100 ----
mean loss: 287.43
 ---- batch: 110 ----
mean loss: 291.97
train mean loss: 286.57
epoch train time: 0:00:02.153607
elapsed time: 0:09:24.357897
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 01:15:23.797850
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 283.65
 ---- batch: 020 ----
mean loss: 285.10
 ---- batch: 030 ----
mean loss: 290.22
 ---- batch: 040 ----
mean loss: 287.62
 ---- batch: 050 ----
mean loss: 283.45
 ---- batch: 060 ----
mean loss: 286.23
 ---- batch: 070 ----
mean loss: 292.20
 ---- batch: 080 ----
mean loss: 288.02
 ---- batch: 090 ----
mean loss: 290.12
 ---- batch: 100 ----
mean loss: 298.03
 ---- batch: 110 ----
mean loss: 283.82
train mean loss: 288.08
epoch train time: 0:00:02.144531
elapsed time: 0:09:26.502597
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 01:15:25.942571
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 284.13
 ---- batch: 020 ----
mean loss: 285.15
 ---- batch: 030 ----
mean loss: 285.53
 ---- batch: 040 ----
mean loss: 281.03
 ---- batch: 050 ----
mean loss: 277.28
 ---- batch: 060 ----
mean loss: 295.48
 ---- batch: 070 ----
mean loss: 288.93
 ---- batch: 080 ----
mean loss: 290.40
 ---- batch: 090 ----
mean loss: 282.05
 ---- batch: 100 ----
mean loss: 295.79
 ---- batch: 110 ----
mean loss: 280.92
train mean loss: 285.91
epoch train time: 0:00:02.150059
elapsed time: 0:09:28.656413
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_8/checkpoint.pth.tar
**** end time: 2019-09-27 01:15:28.096331 ****
