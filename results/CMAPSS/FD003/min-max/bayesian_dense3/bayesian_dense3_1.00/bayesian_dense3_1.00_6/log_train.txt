Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6067
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-09-20 21:53:42.825578 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:53:42.835278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4715.06
 ---- batch: 020 ----
mean loss: 4514.40
 ---- batch: 030 ----
mean loss: 4257.25
 ---- batch: 040 ----
mean loss: 3914.29
train mean loss: 4307.92
epoch train time: 0:00:15.016759
elapsed time: 0:00:15.032752
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:53:57.858378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3645.23
 ---- batch: 020 ----
mean loss: 3411.37
 ---- batch: 030 ----
mean loss: 3243.48
 ---- batch: 040 ----
mean loss: 3182.70
train mean loss: 3347.58
epoch train time: 0:00:00.682090
elapsed time: 0:00:15.715076
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:53:58.540754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2963.42
 ---- batch: 020 ----
mean loss: 2888.53
 ---- batch: 030 ----
mean loss: 2860.57
 ---- batch: 040 ----
mean loss: 2745.95
train mean loss: 2848.68
epoch train time: 0:00:00.654587
elapsed time: 0:00:16.369889
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:53:59.195543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2646.08
 ---- batch: 020 ----
mean loss: 2662.03
 ---- batch: 030 ----
mean loss: 2474.09
 ---- batch: 040 ----
mean loss: 2514.58
train mean loss: 2573.70
epoch train time: 0:00:00.656763
elapsed time: 0:00:17.026884
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:53:59.852539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2460.53
 ---- batch: 020 ----
mean loss: 2381.35
 ---- batch: 030 ----
mean loss: 2365.52
 ---- batch: 040 ----
mean loss: 2310.38
train mean loss: 2373.30
epoch train time: 0:00:00.688261
elapsed time: 0:00:17.715453
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:54:00.541105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2263.13
 ---- batch: 020 ----
mean loss: 2275.13
 ---- batch: 030 ----
mean loss: 2208.72
 ---- batch: 040 ----
mean loss: 2164.64
train mean loss: 2224.25
epoch train time: 0:00:00.710805
elapsed time: 0:00:18.426496
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:54:01.252154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2120.60
 ---- batch: 020 ----
mean loss: 2157.04
 ---- batch: 030 ----
mean loss: 2090.66
 ---- batch: 040 ----
mean loss: 2015.09
train mean loss: 2093.03
epoch train time: 0:00:00.708860
elapsed time: 0:00:19.135619
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:54:01.961275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2034.19
 ---- batch: 020 ----
mean loss: 1963.52
 ---- batch: 030 ----
mean loss: 1986.06
 ---- batch: 040 ----
mean loss: 1943.16
train mean loss: 1979.14
epoch train time: 0:00:00.671997
elapsed time: 0:00:19.807924
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:54:02.633576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1955.01
 ---- batch: 020 ----
mean loss: 1875.56
 ---- batch: 030 ----
mean loss: 1850.06
 ---- batch: 040 ----
mean loss: 1825.63
train mean loss: 1873.68
epoch train time: 0:00:00.683242
elapsed time: 0:00:20.491408
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:54:03.317066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1806.60
 ---- batch: 020 ----
mean loss: 1775.93
 ---- batch: 030 ----
mean loss: 1763.07
 ---- batch: 040 ----
mean loss: 1774.49
train mean loss: 1775.60
epoch train time: 0:00:00.688021
elapsed time: 0:00:21.179644
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:54:04.005288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1712.17
 ---- batch: 020 ----
mean loss: 1705.30
 ---- batch: 030 ----
mean loss: 1685.84
 ---- batch: 040 ----
mean loss: 1620.13
train mean loss: 1682.12
epoch train time: 0:00:00.695874
elapsed time: 0:00:21.875760
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:54:04.701420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1642.06
 ---- batch: 020 ----
mean loss: 1563.48
 ---- batch: 030 ----
mean loss: 1549.50
 ---- batch: 040 ----
mean loss: 1528.25
train mean loss: 1568.26
epoch train time: 0:00:00.720922
elapsed time: 0:00:22.596974
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:54:05.422609
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1497.06
 ---- batch: 020 ----
mean loss: 1478.31
 ---- batch: 030 ----
mean loss: 1459.15
 ---- batch: 040 ----
mean loss: 1453.68
train mean loss: 1470.59
epoch train time: 0:00:00.700685
elapsed time: 0:00:23.297906
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:54:06.123584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1416.27
 ---- batch: 020 ----
mean loss: 1391.35
 ---- batch: 030 ----
mean loss: 1372.20
 ---- batch: 040 ----
mean loss: 1365.47
train mean loss: 1387.12
epoch train time: 0:00:00.686402
elapsed time: 0:00:23.984565
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:54:06.810246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1351.26
 ---- batch: 020 ----
mean loss: 1316.04
 ---- batch: 030 ----
mean loss: 1296.56
 ---- batch: 040 ----
mean loss: 1297.48
train mean loss: 1310.98
epoch train time: 0:00:00.699636
elapsed time: 0:00:24.684483
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:54:07.510152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1245.54
 ---- batch: 020 ----
mean loss: 1262.82
 ---- batch: 030 ----
mean loss: 1230.37
 ---- batch: 040 ----
mean loss: 1204.56
train mean loss: 1233.35
epoch train time: 0:00:00.682804
elapsed time: 0:00:25.367543
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:54:08.193197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1176.10
 ---- batch: 020 ----
mean loss: 1169.09
 ---- batch: 030 ----
mean loss: 1156.19
 ---- batch: 040 ----
mean loss: 1152.70
train mean loss: 1164.02
epoch train time: 0:00:00.702398
elapsed time: 0:00:26.070204
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:54:08.895870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1110.54
 ---- batch: 020 ----
mean loss: 1105.69
 ---- batch: 030 ----
mean loss: 1093.04
 ---- batch: 040 ----
mean loss: 1068.66
train mean loss: 1092.08
epoch train time: 0:00:00.723192
elapsed time: 0:00:26.793667
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:54:09.619343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1067.80
 ---- batch: 020 ----
mean loss: 1022.24
 ---- batch: 030 ----
mean loss: 1044.26
 ---- batch: 040 ----
mean loss: 1011.72
train mean loss: 1033.38
epoch train time: 0:00:00.681756
elapsed time: 0:00:27.475672
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:54:10.301347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.98
 ---- batch: 020 ----
mean loss: 989.77
 ---- batch: 030 ----
mean loss: 980.16
 ---- batch: 040 ----
mean loss: 945.01
train mean loss: 976.33
epoch train time: 0:00:00.671327
elapsed time: 0:00:28.147260
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:54:10.972982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.44
 ---- batch: 020 ----
mean loss: 931.85
 ---- batch: 030 ----
mean loss: 918.79
 ---- batch: 040 ----
mean loss: 900.53
train mean loss: 920.22
epoch train time: 0:00:00.671387
elapsed time: 0:00:28.819010
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:54:11.644670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 887.50
 ---- batch: 020 ----
mean loss: 864.71
 ---- batch: 030 ----
mean loss: 865.04
 ---- batch: 040 ----
mean loss: 857.04
train mean loss: 866.61
epoch train time: 0:00:00.721202
elapsed time: 0:00:29.540450
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:54:12.366104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.66
 ---- batch: 020 ----
mean loss: 821.80
 ---- batch: 030 ----
mean loss: 816.60
 ---- batch: 040 ----
mean loss: 785.13
train mean loss: 813.48
epoch train time: 0:00:00.710680
elapsed time: 0:00:30.251402
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:54:13.077072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 788.62
 ---- batch: 020 ----
mean loss: 788.34
 ---- batch: 030 ----
mean loss: 760.77
 ---- batch: 040 ----
mean loss: 738.33
train mean loss: 766.21
epoch train time: 0:00:00.679150
elapsed time: 0:00:30.930780
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:54:13.756448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 746.17
 ---- batch: 020 ----
mean loss: 717.37
 ---- batch: 030 ----
mean loss: 718.44
 ---- batch: 040 ----
mean loss: 705.16
train mean loss: 720.61
epoch train time: 0:00:00.651289
elapsed time: 0:00:31.582287
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:54:14.407930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 705.35
 ---- batch: 020 ----
mean loss: 693.30
 ---- batch: 030 ----
mean loss: 673.33
 ---- batch: 040 ----
mean loss: 656.53
train mean loss: 678.49
epoch train time: 0:00:00.677153
elapsed time: 0:00:32.259686
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:54:15.085372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.40
 ---- batch: 020 ----
mean loss: 648.53
 ---- batch: 030 ----
mean loss: 623.80
 ---- batch: 040 ----
mean loss: 628.24
train mean loss: 640.67
epoch train time: 0:00:00.702243
elapsed time: 0:00:32.962245
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:54:15.787906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.92
 ---- batch: 020 ----
mean loss: 603.11
 ---- batch: 030 ----
mean loss: 608.42
 ---- batch: 040 ----
mean loss: 592.65
train mean loss: 604.38
epoch train time: 0:00:00.694248
elapsed time: 0:00:33.656763
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:54:16.482414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 590.10
 ---- batch: 020 ----
mean loss: 574.12
 ---- batch: 030 ----
mean loss: 562.30
 ---- batch: 040 ----
mean loss: 554.98
train mean loss: 568.44
epoch train time: 0:00:00.692737
elapsed time: 0:00:34.349726
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:54:17.175378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 546.85
 ---- batch: 020 ----
mean loss: 529.56
 ---- batch: 030 ----
mean loss: 536.79
 ---- batch: 040 ----
mean loss: 532.02
train mean loss: 534.31
epoch train time: 0:00:00.663134
elapsed time: 0:00:35.013118
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:54:17.838765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.49
 ---- batch: 020 ----
mean loss: 513.06
 ---- batch: 030 ----
mean loss: 503.98
 ---- batch: 040 ----
mean loss: 487.50
train mean loss: 500.12
epoch train time: 0:00:00.665324
elapsed time: 0:00:35.678641
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:54:18.504313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.53
 ---- batch: 020 ----
mean loss: 480.26
 ---- batch: 030 ----
mean loss: 473.40
 ---- batch: 040 ----
mean loss: 464.08
train mean loss: 472.03
epoch train time: 0:00:00.690992
elapsed time: 0:00:36.369921
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:54:19.195596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.17
 ---- batch: 020 ----
mean loss: 455.37
 ---- batch: 030 ----
mean loss: 438.06
 ---- batch: 040 ----
mean loss: 450.98
train mean loss: 445.87
epoch train time: 0:00:00.714667
elapsed time: 0:00:37.084880
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:54:19.910538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.49
 ---- batch: 020 ----
mean loss: 416.17
 ---- batch: 030 ----
mean loss: 417.84
 ---- batch: 040 ----
mean loss: 415.12
train mean loss: 420.51
epoch train time: 0:00:00.711536
elapsed time: 0:00:37.796675
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:54:20.622361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.45
 ---- batch: 020 ----
mean loss: 402.86
 ---- batch: 030 ----
mean loss: 393.14
 ---- batch: 040 ----
mean loss: 387.59
train mean loss: 396.51
epoch train time: 0:00:00.672815
elapsed time: 0:00:38.469777
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:54:21.295432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.25
 ---- batch: 020 ----
mean loss: 382.80
 ---- batch: 030 ----
mean loss: 378.41
 ---- batch: 040 ----
mean loss: 371.38
train mean loss: 376.79
epoch train time: 0:00:00.688349
elapsed time: 0:00:39.158371
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:54:21.984041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.45
 ---- batch: 020 ----
mean loss: 355.04
 ---- batch: 030 ----
mean loss: 353.86
 ---- batch: 040 ----
mean loss: 346.67
train mean loss: 351.76
epoch train time: 0:00:00.676843
elapsed time: 0:00:39.835479
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:54:22.661149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.24
 ---- batch: 020 ----
mean loss: 337.55
 ---- batch: 030 ----
mean loss: 330.89
 ---- batch: 040 ----
mean loss: 328.12
train mean loss: 334.21
epoch train time: 0:00:00.714711
elapsed time: 0:00:40.550464
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:54:23.376188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.31
 ---- batch: 020 ----
mean loss: 318.76
 ---- batch: 030 ----
mean loss: 312.34
 ---- batch: 040 ----
mean loss: 311.46
train mean loss: 315.45
epoch train time: 0:00:00.700808
elapsed time: 0:00:41.251592
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:54:24.077247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.42
 ---- batch: 020 ----
mean loss: 300.90
 ---- batch: 030 ----
mean loss: 295.86
 ---- batch: 040 ----
mean loss: 290.87
train mean loss: 298.22
epoch train time: 0:00:00.679312
elapsed time: 0:00:41.931141
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:54:24.756812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.79
 ---- batch: 020 ----
mean loss: 283.26
 ---- batch: 030 ----
mean loss: 282.71
 ---- batch: 040 ----
mean loss: 277.06
train mean loss: 282.13
epoch train time: 0:00:00.678700
elapsed time: 0:00:42.610075
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:54:25.435727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.60
 ---- batch: 020 ----
mean loss: 264.76
 ---- batch: 030 ----
mean loss: 270.97
 ---- batch: 040 ----
mean loss: 265.35
train mean loss: 267.87
epoch train time: 0:00:00.671203
elapsed time: 0:00:43.281484
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:54:26.107129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.60
 ---- batch: 020 ----
mean loss: 255.12
 ---- batch: 030 ----
mean loss: 258.30
 ---- batch: 040 ----
mean loss: 248.34
train mean loss: 254.84
epoch train time: 0:00:00.676693
elapsed time: 0:00:43.958479
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:54:26.784166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.30
 ---- batch: 020 ----
mean loss: 251.07
 ---- batch: 030 ----
mean loss: 233.13
 ---- batch: 040 ----
mean loss: 233.78
train mean loss: 240.05
epoch train time: 0:00:00.698703
elapsed time: 0:00:44.657469
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:54:27.483149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.92
 ---- batch: 020 ----
mean loss: 228.36
 ---- batch: 030 ----
mean loss: 225.06
 ---- batch: 040 ----
mean loss: 226.48
train mean loss: 227.97
epoch train time: 0:00:00.706151
elapsed time: 0:00:45.363891
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:54:28.189556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.56
 ---- batch: 020 ----
mean loss: 220.53
 ---- batch: 030 ----
mean loss: 214.33
 ---- batch: 040 ----
mean loss: 211.41
train mean loss: 216.55
epoch train time: 0:00:00.680113
elapsed time: 0:00:46.044257
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:54:28.869947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.81
 ---- batch: 020 ----
mean loss: 207.86
 ---- batch: 030 ----
mean loss: 212.43
 ---- batch: 040 ----
mean loss: 205.17
train mean loss: 207.17
epoch train time: 0:00:00.658206
elapsed time: 0:00:46.702776
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:54:29.528459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.68
 ---- batch: 020 ----
mean loss: 198.03
 ---- batch: 030 ----
mean loss: 190.05
 ---- batch: 040 ----
mean loss: 197.97
train mean loss: 196.54
epoch train time: 0:00:00.665715
elapsed time: 0:00:47.368755
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:54:30.194411
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.97
 ---- batch: 020 ----
mean loss: 189.72
 ---- batch: 030 ----
mean loss: 185.84
 ---- batch: 040 ----
mean loss: 191.49
train mean loss: 188.29
epoch train time: 0:00:00.708819
elapsed time: 0:00:48.077828
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:54:30.903496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.33
 ---- batch: 020 ----
mean loss: 183.08
 ---- batch: 030 ----
mean loss: 177.93
 ---- batch: 040 ----
mean loss: 177.31
train mean loss: 180.56
epoch train time: 0:00:00.690528
elapsed time: 0:00:48.768658
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:54:31.594303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.43
 ---- batch: 020 ----
mean loss: 172.79
 ---- batch: 030 ----
mean loss: 168.66
 ---- batch: 040 ----
mean loss: 171.95
train mean loss: 171.24
epoch train time: 0:00:00.684918
elapsed time: 0:00:49.453804
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:54:32.279457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.97
 ---- batch: 020 ----
mean loss: 163.11
 ---- batch: 030 ----
mean loss: 161.32
 ---- batch: 040 ----
mean loss: 162.42
train mean loss: 163.17
epoch train time: 0:00:00.675963
elapsed time: 0:00:50.129969
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:54:32.955616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.19
 ---- batch: 020 ----
mean loss: 158.61
 ---- batch: 030 ----
mean loss: 159.19
 ---- batch: 040 ----
mean loss: 155.81
train mean loss: 159.47
epoch train time: 0:00:00.662549
elapsed time: 0:00:50.792744
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:54:33.618392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.66
 ---- batch: 020 ----
mean loss: 151.94
 ---- batch: 030 ----
mean loss: 147.26
 ---- batch: 040 ----
mean loss: 149.94
train mean loss: 150.92
epoch train time: 0:00:00.684163
elapsed time: 0:00:51.477159
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:54:34.302832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.14
 ---- batch: 020 ----
mean loss: 144.62
 ---- batch: 030 ----
mean loss: 146.31
 ---- batch: 040 ----
mean loss: 143.79
train mean loss: 145.82
epoch train time: 0:00:00.700497
elapsed time: 0:00:52.177910
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:54:35.003586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.13
 ---- batch: 020 ----
mean loss: 142.63
 ---- batch: 030 ----
mean loss: 137.43
 ---- batch: 040 ----
mean loss: 137.06
train mean loss: 140.00
epoch train time: 0:00:00.702922
elapsed time: 0:00:52.881079
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:54:35.706733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.14
 ---- batch: 020 ----
mean loss: 137.27
 ---- batch: 030 ----
mean loss: 131.96
 ---- batch: 040 ----
mean loss: 133.22
train mean loss: 135.48
epoch train time: 0:00:00.658134
elapsed time: 0:00:53.539433
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:54:36.365083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.25
 ---- batch: 020 ----
mean loss: 130.42
 ---- batch: 030 ----
mean loss: 130.05
 ---- batch: 040 ----
mean loss: 128.77
train mean loss: 130.53
epoch train time: 0:00:00.663774
elapsed time: 0:00:54.203427
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:54:37.029077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.72
 ---- batch: 020 ----
mean loss: 125.42
 ---- batch: 030 ----
mean loss: 125.97
 ---- batch: 040 ----
mean loss: 128.44
train mean loss: 126.38
epoch train time: 0:00:00.683884
elapsed time: 0:00:54.887586
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:54:37.713243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.92
 ---- batch: 020 ----
mean loss: 124.91
 ---- batch: 030 ----
mean loss: 121.20
 ---- batch: 040 ----
mean loss: 118.65
train mean loss: 123.18
epoch train time: 0:00:00.729852
elapsed time: 0:00:55.617683
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:54:38.443355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.55
 ---- batch: 020 ----
mean loss: 117.18
 ---- batch: 030 ----
mean loss: 121.37
 ---- batch: 040 ----
mean loss: 118.49
train mean loss: 118.02
epoch train time: 0:00:00.705262
elapsed time: 0:00:56.323198
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:54:39.148886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.32
 ---- batch: 020 ----
mean loss: 116.43
 ---- batch: 030 ----
mean loss: 114.54
 ---- batch: 040 ----
mean loss: 116.94
train mean loss: 116.17
epoch train time: 0:00:00.668669
elapsed time: 0:00:56.992148
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:54:39.817817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 109.89
 ---- batch: 020 ----
mean loss: 114.27
 ---- batch: 030 ----
mean loss: 112.95
 ---- batch: 040 ----
mean loss: 110.14
train mean loss: 111.36
epoch train time: 0:00:00.661585
elapsed time: 0:00:57.654023
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:54:40.479685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.01
 ---- batch: 020 ----
mean loss: 106.57
 ---- batch: 030 ----
mean loss: 116.27
 ---- batch: 040 ----
mean loss: 103.35
train mean loss: 109.08
epoch train time: 0:00:00.675536
elapsed time: 0:00:58.329825
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:54:41.155482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.48
 ---- batch: 020 ----
mean loss: 105.48
 ---- batch: 030 ----
mean loss: 109.45
 ---- batch: 040 ----
mean loss: 107.42
train mean loss: 106.97
epoch train time: 0:00:00.706935
elapsed time: 0:00:59.037012
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:54:41.862674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 104.87
 ---- batch: 020 ----
mean loss: 106.36
 ---- batch: 030 ----
mean loss: 101.20
 ---- batch: 040 ----
mean loss: 105.06
train mean loss: 104.75
epoch train time: 0:00:00.718207
elapsed time: 0:00:59.755481
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:54:42.581145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 99.85
 ---- batch: 020 ----
mean loss: 102.11
 ---- batch: 030 ----
mean loss: 100.77
 ---- batch: 040 ----
mean loss: 101.93
train mean loss: 101.16
epoch train time: 0:00:00.687904
elapsed time: 0:01:00.443603
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:54:43.269282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.22
 ---- batch: 020 ----
mean loss: 100.16
 ---- batch: 030 ----
mean loss: 101.38
 ---- batch: 040 ----
mean loss: 96.27
train mean loss: 99.21
epoch train time: 0:00:00.669224
elapsed time: 0:01:01.113140
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:54:43.938823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.28
 ---- batch: 020 ----
mean loss: 97.94
 ---- batch: 030 ----
mean loss: 97.57
 ---- batch: 040 ----
mean loss: 98.93
train mean loss: 98.54
epoch train time: 0:00:00.644381
elapsed time: 0:01:01.757767
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:54:44.583428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.04
 ---- batch: 020 ----
mean loss: 94.05
 ---- batch: 030 ----
mean loss: 93.24
 ---- batch: 040 ----
mean loss: 95.48
train mean loss: 95.15
epoch train time: 0:00:00.675643
elapsed time: 0:01:02.433656
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:54:45.259310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.55
 ---- batch: 020 ----
mean loss: 92.12
 ---- batch: 030 ----
mean loss: 93.05
 ---- batch: 040 ----
mean loss: 93.37
train mean loss: 92.79
epoch train time: 0:00:00.709630
elapsed time: 0:01:03.143521
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:54:45.969201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 92.68
 ---- batch: 020 ----
mean loss: 91.29
 ---- batch: 030 ----
mean loss: 93.50
 ---- batch: 040 ----
mean loss: 89.46
train mean loss: 91.51
epoch train time: 0:00:00.716103
elapsed time: 0:01:03.859965
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:54:46.685637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 89.81
 ---- batch: 020 ----
mean loss: 95.10
 ---- batch: 030 ----
mean loss: 87.79
 ---- batch: 040 ----
mean loss: 91.04
train mean loss: 90.44
epoch train time: 0:00:00.696887
elapsed time: 0:01:04.557097
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:54:47.382755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.15
 ---- batch: 020 ----
mean loss: 85.28
 ---- batch: 030 ----
mean loss: 87.26
 ---- batch: 040 ----
mean loss: 88.82
train mean loss: 87.39
epoch train time: 0:00:00.664035
elapsed time: 0:01:05.221372
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:54:48.047050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.88
 ---- batch: 020 ----
mean loss: 86.02
 ---- batch: 030 ----
mean loss: 86.06
 ---- batch: 040 ----
mean loss: 85.89
train mean loss: 86.23
epoch train time: 0:00:00.654439
elapsed time: 0:01:05.876104
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:54:48.701748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.05
 ---- batch: 020 ----
mean loss: 85.45
 ---- batch: 030 ----
mean loss: 84.21
 ---- batch: 040 ----
mean loss: 80.88
train mean loss: 84.07
epoch train time: 0:00:00.690179
elapsed time: 0:01:06.566546
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:54:49.392225
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.04
 ---- batch: 020 ----
mean loss: 78.00
 ---- batch: 030 ----
mean loss: 83.83
 ---- batch: 040 ----
mean loss: 86.21
train mean loss: 83.24
epoch train time: 0:00:00.706981
elapsed time: 0:01:07.273833
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:54:50.099491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.40
 ---- batch: 020 ----
mean loss: 84.09
 ---- batch: 030 ----
mean loss: 87.45
 ---- batch: 040 ----
mean loss: 80.68
train mean loss: 83.51
epoch train time: 0:00:00.689013
elapsed time: 0:01:07.963071
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:54:50.788723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.33
 ---- batch: 020 ----
mean loss: 79.28
 ---- batch: 030 ----
mean loss: 83.41
 ---- batch: 040 ----
mean loss: 82.23
train mean loss: 82.13
epoch train time: 0:00:00.676748
elapsed time: 0:01:08.640061
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:54:51.465740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.45
 ---- batch: 020 ----
mean loss: 81.80
 ---- batch: 030 ----
mean loss: 81.29
 ---- batch: 040 ----
mean loss: 82.51
train mean loss: 81.44
epoch train time: 0:00:00.691249
elapsed time: 0:01:09.331551
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:54:52.157197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.76
 ---- batch: 020 ----
mean loss: 76.42
 ---- batch: 030 ----
mean loss: 77.79
 ---- batch: 040 ----
mean loss: 78.71
train mean loss: 78.33
epoch train time: 0:00:00.670412
elapsed time: 0:01:10.002216
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:54:52.827873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.85
 ---- batch: 020 ----
mean loss: 77.43
 ---- batch: 030 ----
mean loss: 78.10
 ---- batch: 040 ----
mean loss: 77.90
train mean loss: 77.30
epoch train time: 0:00:00.718339
elapsed time: 0:01:10.720787
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:54:53.546442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.91
 ---- batch: 020 ----
mean loss: 77.43
 ---- batch: 030 ----
mean loss: 75.15
 ---- batch: 040 ----
mean loss: 78.09
train mean loss: 77.52
epoch train time: 0:00:00.712689
elapsed time: 0:01:11.433694
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:54:54.259359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.93
 ---- batch: 020 ----
mean loss: 77.72
 ---- batch: 030 ----
mean loss: 77.44
 ---- batch: 040 ----
mean loss: 79.44
train mean loss: 77.16
epoch train time: 0:00:00.662213
elapsed time: 0:01:12.096173
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:54:54.921842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.50
 ---- batch: 020 ----
mean loss: 74.95
 ---- batch: 030 ----
mean loss: 75.12
 ---- batch: 040 ----
mean loss: 74.61
train mean loss: 75.53
epoch train time: 0:00:00.664857
elapsed time: 0:01:12.761281
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:54:55.586943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.38
 ---- batch: 020 ----
mean loss: 74.71
 ---- batch: 030 ----
mean loss: 74.77
 ---- batch: 040 ----
mean loss: 75.27
train mean loss: 75.48
epoch train time: 0:00:00.664820
elapsed time: 0:01:13.426348
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:54:56.252004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.60
 ---- batch: 020 ----
mean loss: 75.02
 ---- batch: 030 ----
mean loss: 71.58
 ---- batch: 040 ----
mean loss: 76.12
train mean loss: 74.45
epoch train time: 0:00:00.709022
elapsed time: 0:01:14.135625
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:54:56.961280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.60
 ---- batch: 020 ----
mean loss: 72.18
 ---- batch: 030 ----
mean loss: 72.77
 ---- batch: 040 ----
mean loss: 71.44
train mean loss: 72.51
epoch train time: 0:00:00.687717
elapsed time: 0:01:14.823579
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:54:57.649233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.78
 ---- batch: 020 ----
mean loss: 72.49
 ---- batch: 030 ----
mean loss: 71.05
 ---- batch: 040 ----
mean loss: 71.37
train mean loss: 72.78
epoch train time: 0:00:00.672869
elapsed time: 0:01:15.496669
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:54:58.322377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.24
 ---- batch: 020 ----
mean loss: 72.43
 ---- batch: 030 ----
mean loss: 72.99
 ---- batch: 040 ----
mean loss: 73.77
train mean loss: 72.30
epoch train time: 0:00:00.672342
elapsed time: 0:01:16.169295
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:54:58.994950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.82
 ---- batch: 020 ----
mean loss: 74.68
 ---- batch: 030 ----
mean loss: 68.09
 ---- batch: 040 ----
mean loss: 73.51
train mean loss: 71.33
epoch train time: 0:00:00.668256
elapsed time: 0:01:16.837780
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:54:59.663435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.50
 ---- batch: 020 ----
mean loss: 72.48
 ---- batch: 030 ----
mean loss: 69.83
 ---- batch: 040 ----
mean loss: 69.34
train mean loss: 71.11
epoch train time: 0:00:00.677916
elapsed time: 0:01:17.515983
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:55:00.341654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.96
 ---- batch: 020 ----
mean loss: 69.22
 ---- batch: 030 ----
mean loss: 70.25
 ---- batch: 040 ----
mean loss: 70.80
train mean loss: 69.63
epoch train time: 0:00:00.689422
elapsed time: 0:01:18.205688
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:55:01.031374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.29
 ---- batch: 020 ----
mean loss: 71.70
 ---- batch: 030 ----
mean loss: 67.31
 ---- batch: 040 ----
mean loss: 70.44
train mean loss: 69.81
epoch train time: 0:00:00.698502
elapsed time: 0:01:18.904494
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:55:01.730172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.25
 ---- batch: 020 ----
mean loss: 69.13
 ---- batch: 030 ----
mean loss: 67.21
 ---- batch: 040 ----
mean loss: 70.23
train mean loss: 68.55
epoch train time: 0:00:00.657039
elapsed time: 0:01:19.561761
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:55:02.387465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.54
 ---- batch: 020 ----
mean loss: 69.79
 ---- batch: 030 ----
mean loss: 68.96
 ---- batch: 040 ----
mean loss: 67.34
train mean loss: 69.11
epoch train time: 0:00:00.655440
elapsed time: 0:01:20.217458
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:55:03.043106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.63
 ---- batch: 020 ----
mean loss: 71.33
 ---- batch: 030 ----
mean loss: 65.65
 ---- batch: 040 ----
mean loss: 67.00
train mean loss: 68.06
epoch train time: 0:00:00.667247
elapsed time: 0:01:20.885017
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:55:03.710683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.75
 ---- batch: 020 ----
mean loss: 66.18
 ---- batch: 030 ----
mean loss: 66.47
 ---- batch: 040 ----
mean loss: 69.15
train mean loss: 67.97
epoch train time: 0:00:00.708554
elapsed time: 0:01:21.593828
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:55:04.419502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.83
 ---- batch: 020 ----
mean loss: 68.12
 ---- batch: 030 ----
mean loss: 65.99
 ---- batch: 040 ----
mean loss: 68.04
train mean loss: 67.01
epoch train time: 0:00:00.714959
elapsed time: 0:01:22.309032
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:55:05.134684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.05
 ---- batch: 020 ----
mean loss: 69.57
 ---- batch: 030 ----
mean loss: 65.89
 ---- batch: 040 ----
mean loss: 62.24
train mean loss: 66.45
epoch train time: 0:00:00.703389
elapsed time: 0:01:23.012659
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:55:05.838319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.56
 ---- batch: 020 ----
mean loss: 64.83
 ---- batch: 030 ----
mean loss: 67.78
 ---- batch: 040 ----
mean loss: 66.69
train mean loss: 66.09
epoch train time: 0:00:00.652445
elapsed time: 0:01:23.665331
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:55:06.491033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.77
 ---- batch: 020 ----
mean loss: 63.48
 ---- batch: 030 ----
mean loss: 64.57
 ---- batch: 040 ----
mean loss: 67.56
train mean loss: 66.12
epoch train time: 0:00:00.660881
elapsed time: 0:01:24.326464
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:55:07.152113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.08
 ---- batch: 020 ----
mean loss: 62.98
 ---- batch: 030 ----
mean loss: 65.87
 ---- batch: 040 ----
mean loss: 69.23
train mean loss: 65.54
epoch train time: 0:00:00.692109
elapsed time: 0:01:25.018859
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:55:07.844551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.90
 ---- batch: 020 ----
mean loss: 66.97
 ---- batch: 030 ----
mean loss: 64.04
 ---- batch: 040 ----
mean loss: 62.46
train mean loss: 64.86
epoch train time: 0:00:00.701487
elapsed time: 0:01:25.720686
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:55:08.546352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.71
 ---- batch: 020 ----
mean loss: 66.43
 ---- batch: 030 ----
mean loss: 62.58
 ---- batch: 040 ----
mean loss: 68.99
train mean loss: 65.62
epoch train time: 0:00:00.694938
elapsed time: 0:01:26.415885
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:55:09.241553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.51
 ---- batch: 020 ----
mean loss: 66.27
 ---- batch: 030 ----
mean loss: 63.70
 ---- batch: 040 ----
mean loss: 66.14
train mean loss: 65.23
epoch train time: 0:00:00.669562
elapsed time: 0:01:27.085740
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:55:09.911410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.71
 ---- batch: 020 ----
mean loss: 60.71
 ---- batch: 030 ----
mean loss: 62.79
 ---- batch: 040 ----
mean loss: 64.90
train mean loss: 63.09
epoch train time: 0:00:00.659593
elapsed time: 0:01:27.745625
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:55:10.571246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.86
 ---- batch: 020 ----
mean loss: 62.30
 ---- batch: 030 ----
mean loss: 64.49
 ---- batch: 040 ----
mean loss: 62.73
train mean loss: 63.45
epoch train time: 0:00:00.679715
elapsed time: 0:01:28.425547
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:55:11.251209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.00
 ---- batch: 020 ----
mean loss: 60.75
 ---- batch: 030 ----
mean loss: 62.56
 ---- batch: 040 ----
mean loss: 66.62
train mean loss: 62.73
epoch train time: 0:00:00.697333
elapsed time: 0:01:29.123154
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:55:11.948821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.12
 ---- batch: 020 ----
mean loss: 62.73
 ---- batch: 030 ----
mean loss: 64.65
 ---- batch: 040 ----
mean loss: 61.72
train mean loss: 63.55
epoch train time: 0:00:00.686588
elapsed time: 0:01:29.809980
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:55:12.635695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.85
 ---- batch: 020 ----
mean loss: 63.63
 ---- batch: 030 ----
mean loss: 63.88
 ---- batch: 040 ----
mean loss: 61.95
train mean loss: 62.59
epoch train time: 0:00:00.682528
elapsed time: 0:01:30.492802
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:55:13.318472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.97
 ---- batch: 020 ----
mean loss: 63.19
 ---- batch: 030 ----
mean loss: 61.93
 ---- batch: 040 ----
mean loss: 61.28
train mean loss: 61.98
epoch train time: 0:00:00.690369
elapsed time: 0:01:31.183466
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:55:14.009127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.68
 ---- batch: 020 ----
mean loss: 63.84
 ---- batch: 030 ----
mean loss: 61.29
 ---- batch: 040 ----
mean loss: 59.36
train mean loss: 61.73
epoch train time: 0:00:00.648743
elapsed time: 0:01:31.832444
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:55:14.658094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.83
 ---- batch: 020 ----
mean loss: 62.23
 ---- batch: 030 ----
mean loss: 61.12
 ---- batch: 040 ----
mean loss: 58.80
train mean loss: 61.09
epoch train time: 0:00:00.703033
elapsed time: 0:01:32.535744
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:55:15.361408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.38
 ---- batch: 020 ----
mean loss: 60.05
 ---- batch: 030 ----
mean loss: 62.63
 ---- batch: 040 ----
mean loss: 62.15
train mean loss: 61.28
epoch train time: 0:00:00.719097
elapsed time: 0:01:33.255109
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:55:16.080767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.52
 ---- batch: 020 ----
mean loss: 60.06
 ---- batch: 030 ----
mean loss: 61.11
 ---- batch: 040 ----
mean loss: 59.80
train mean loss: 61.23
epoch train time: 0:00:00.675055
elapsed time: 0:01:33.930373
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:55:16.756036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.53
 ---- batch: 020 ----
mean loss: 60.27
 ---- batch: 030 ----
mean loss: 60.00
 ---- batch: 040 ----
mean loss: 62.28
train mean loss: 60.12
epoch train time: 0:00:00.663599
elapsed time: 0:01:34.594205
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:55:17.419881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.52
 ---- batch: 020 ----
mean loss: 60.70
 ---- batch: 030 ----
mean loss: 59.96
 ---- batch: 040 ----
mean loss: 58.17
train mean loss: 60.10
epoch train time: 0:00:00.656655
elapsed time: 0:01:35.251100
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:55:18.076752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.39
 ---- batch: 020 ----
mean loss: 60.10
 ---- batch: 030 ----
mean loss: 61.18
 ---- batch: 040 ----
mean loss: 58.88
train mean loss: 60.04
epoch train time: 0:00:00.668339
elapsed time: 0:01:35.919715
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:55:18.745376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.00
 ---- batch: 020 ----
mean loss: 61.54
 ---- batch: 030 ----
mean loss: 60.96
 ---- batch: 040 ----
mean loss: 56.90
train mean loss: 59.52
epoch train time: 0:00:00.719991
elapsed time: 0:01:36.639956
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:55:19.465619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.03
 ---- batch: 020 ----
mean loss: 59.77
 ---- batch: 030 ----
mean loss: 61.13
 ---- batch: 040 ----
mean loss: 59.23
train mean loss: 59.20
epoch train time: 0:00:00.694793
elapsed time: 0:01:37.335001
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:55:20.160721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.54
 ---- batch: 020 ----
mean loss: 59.89
 ---- batch: 030 ----
mean loss: 55.53
 ---- batch: 040 ----
mean loss: 60.45
train mean loss: 58.33
epoch train time: 0:00:00.668280
elapsed time: 0:01:38.003560
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:55:20.829228
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.61
 ---- batch: 020 ----
mean loss: 59.60
 ---- batch: 030 ----
mean loss: 58.71
 ---- batch: 040 ----
mean loss: 58.80
train mean loss: 59.11
epoch train time: 0:00:00.660641
elapsed time: 0:01:38.664455
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:55:21.490109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.91
 ---- batch: 020 ----
mean loss: 58.70
 ---- batch: 030 ----
mean loss: 57.18
 ---- batch: 040 ----
mean loss: 57.17
train mean loss: 58.37
epoch train time: 0:00:00.676501
elapsed time: 0:01:39.341174
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:55:22.166831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.04
 ---- batch: 020 ----
mean loss: 57.26
 ---- batch: 030 ----
mean loss: 57.21
 ---- batch: 040 ----
mean loss: 58.15
train mean loss: 58.11
epoch train time: 0:00:00.697804
elapsed time: 0:01:40.039222
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:55:22.864888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.77
 ---- batch: 020 ----
mean loss: 56.10
 ---- batch: 030 ----
mean loss: 60.20
 ---- batch: 040 ----
mean loss: 57.86
train mean loss: 57.06
epoch train time: 0:00:00.684749
elapsed time: 0:01:40.724223
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:55:23.549885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.86
 ---- batch: 020 ----
mean loss: 56.60
 ---- batch: 030 ----
mean loss: 57.13
 ---- batch: 040 ----
mean loss: 56.90
train mean loss: 56.96
epoch train time: 0:00:00.685713
elapsed time: 0:01:41.410186
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:55:24.235810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.93
 ---- batch: 020 ----
mean loss: 55.38
 ---- batch: 030 ----
mean loss: 57.50
 ---- batch: 040 ----
mean loss: 57.17
train mean loss: 57.59
epoch train time: 0:00:00.679314
elapsed time: 0:01:42.089733
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:55:24.915390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.15
 ---- batch: 020 ----
mean loss: 57.57
 ---- batch: 030 ----
mean loss: 56.84
 ---- batch: 040 ----
mean loss: 57.32
train mean loss: 56.76
epoch train time: 0:00:00.662030
elapsed time: 0:01:42.751999
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:55:25.577652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.56
 ---- batch: 020 ----
mean loss: 55.98
 ---- batch: 030 ----
mean loss: 58.06
 ---- batch: 040 ----
mean loss: 53.37
train mean loss: 56.69
epoch train time: 0:00:00.671464
elapsed time: 0:01:43.423699
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:55:26.249357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.88
 ---- batch: 020 ----
mean loss: 58.19
 ---- batch: 030 ----
mean loss: 55.42
 ---- batch: 040 ----
mean loss: 56.74
train mean loss: 57.06
epoch train time: 0:00:00.694258
elapsed time: 0:01:44.118200
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:55:26.943856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.05
 ---- batch: 020 ----
mean loss: 56.04
 ---- batch: 030 ----
mean loss: 54.29
 ---- batch: 040 ----
mean loss: 55.71
train mean loss: 56.02
epoch train time: 0:00:00.697565
elapsed time: 0:01:44.815994
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:55:27.641648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.20
 ---- batch: 020 ----
mean loss: 55.09
 ---- batch: 030 ----
mean loss: 55.15
 ---- batch: 040 ----
mean loss: 54.43
train mean loss: 56.03
epoch train time: 0:00:00.665949
elapsed time: 0:01:45.482151
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:55:28.307800
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.11
 ---- batch: 020 ----
mean loss: 53.88
 ---- batch: 030 ----
mean loss: 57.43
 ---- batch: 040 ----
mean loss: 55.28
train mean loss: 54.87
epoch train time: 0:00:00.653207
elapsed time: 0:01:46.135632
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:55:28.961312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.68
 ---- batch: 020 ----
mean loss: 57.37
 ---- batch: 030 ----
mean loss: 56.45
 ---- batch: 040 ----
mean loss: 54.74
train mean loss: 55.59
epoch train time: 0:00:00.637314
elapsed time: 0:01:46.773175
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:55:29.598828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.08
 ---- batch: 020 ----
mean loss: 54.93
 ---- batch: 030 ----
mean loss: 53.46
 ---- batch: 040 ----
mean loss: 55.37
train mean loss: 54.48
epoch train time: 0:00:00.689915
elapsed time: 0:01:47.463328
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:55:30.288982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.62
 ---- batch: 020 ----
mean loss: 56.03
 ---- batch: 030 ----
mean loss: 54.24
 ---- batch: 040 ----
mean loss: 53.05
train mean loss: 54.72
epoch train time: 0:00:00.701093
elapsed time: 0:01:48.164704
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:55:30.990358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.17
 ---- batch: 020 ----
mean loss: 54.29
 ---- batch: 030 ----
mean loss: 54.29
 ---- batch: 040 ----
mean loss: 54.73
train mean loss: 54.50
epoch train time: 0:00:00.693221
elapsed time: 0:01:48.858137
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:55:31.683804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.93
 ---- batch: 020 ----
mean loss: 58.95
 ---- batch: 030 ----
mean loss: 53.46
 ---- batch: 040 ----
mean loss: 52.48
train mean loss: 54.27
epoch train time: 0:00:00.663418
elapsed time: 0:01:49.521797
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:55:32.347450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.80
 ---- batch: 020 ----
mean loss: 53.67
 ---- batch: 030 ----
mean loss: 53.21
 ---- batch: 040 ----
mean loss: 54.24
train mean loss: 54.11
epoch train time: 0:00:00.676921
elapsed time: 0:01:50.198974
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:55:33.024684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.15
 ---- batch: 020 ----
mean loss: 50.23
 ---- batch: 030 ----
mean loss: 53.79
 ---- batch: 040 ----
mean loss: 54.85
train mean loss: 53.16
epoch train time: 0:00:00.689684
elapsed time: 0:01:50.888971
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:55:33.714654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.41
 ---- batch: 020 ----
mean loss: 54.38
 ---- batch: 030 ----
mean loss: 53.94
 ---- batch: 040 ----
mean loss: 52.88
train mean loss: 53.37
epoch train time: 0:00:00.734704
elapsed time: 0:01:51.623997
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:55:34.449656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.42
 ---- batch: 020 ----
mean loss: 57.14
 ---- batch: 030 ----
mean loss: 53.68
 ---- batch: 040 ----
mean loss: 51.27
train mean loss: 52.93
epoch train time: 0:00:00.709369
elapsed time: 0:01:52.333627
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:55:35.159309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.84
 ---- batch: 020 ----
mean loss: 53.78
 ---- batch: 030 ----
mean loss: 52.31
 ---- batch: 040 ----
mean loss: 53.19
train mean loss: 52.78
epoch train time: 0:00:00.684795
elapsed time: 0:01:53.018665
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:55:35.844332
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.98
 ---- batch: 020 ----
mean loss: 53.33
 ---- batch: 030 ----
mean loss: 54.52
 ---- batch: 040 ----
mean loss: 51.28
train mean loss: 52.84
epoch train time: 0:00:00.658563
elapsed time: 0:01:53.677488
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:55:36.503137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.47
 ---- batch: 020 ----
mean loss: 49.22
 ---- batch: 030 ----
mean loss: 51.10
 ---- batch: 040 ----
mean loss: 53.80
train mean loss: 52.15
epoch train time: 0:00:00.671107
elapsed time: 0:01:54.348809
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:55:37.174490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.29
 ---- batch: 020 ----
mean loss: 52.67
 ---- batch: 030 ----
mean loss: 50.00
 ---- batch: 040 ----
mean loss: 53.51
train mean loss: 51.69
epoch train time: 0:00:00.699522
elapsed time: 0:01:55.048586
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:55:37.874242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.04
 ---- batch: 020 ----
mean loss: 49.31
 ---- batch: 030 ----
mean loss: 52.20
 ---- batch: 040 ----
mean loss: 49.91
train mean loss: 51.51
epoch train time: 0:00:00.690119
elapsed time: 0:01:55.738956
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:55:38.564614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.34
 ---- batch: 020 ----
mean loss: 54.60
 ---- batch: 030 ----
mean loss: 52.45
 ---- batch: 040 ----
mean loss: 50.31
train mean loss: 51.59
epoch train time: 0:00:00.684670
elapsed time: 0:01:56.423897
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:55:39.249539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.74
 ---- batch: 020 ----
mean loss: 49.93
 ---- batch: 030 ----
mean loss: 51.22
 ---- batch: 040 ----
mean loss: 50.20
train mean loss: 50.58
epoch train time: 0:00:00.663884
elapsed time: 0:01:57.087994
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:55:39.913641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.66
 ---- batch: 020 ----
mean loss: 51.31
 ---- batch: 030 ----
mean loss: 50.70
 ---- batch: 040 ----
mean loss: 51.70
train mean loss: 50.64
epoch train time: 0:00:00.664893
elapsed time: 0:01:57.753109
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:55:40.578765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.67
 ---- batch: 020 ----
mean loss: 49.13
 ---- batch: 030 ----
mean loss: 49.25
 ---- batch: 040 ----
mean loss: 51.54
train mean loss: 50.21
epoch train time: 0:00:00.687986
elapsed time: 0:01:58.441396
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:55:41.267069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.73
 ---- batch: 020 ----
mean loss: 50.84
 ---- batch: 030 ----
mean loss: 49.58
 ---- batch: 040 ----
mean loss: 50.57
train mean loss: 50.43
epoch train time: 0:00:00.709511
elapsed time: 0:01:59.151241
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:55:41.976902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.51
 ---- batch: 020 ----
mean loss: 51.22
 ---- batch: 030 ----
mean loss: 47.40
 ---- batch: 040 ----
mean loss: 49.22
train mean loss: 49.60
epoch train time: 0:00:00.701895
elapsed time: 0:01:59.853363
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:55:42.679019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.48
 ---- batch: 020 ----
mean loss: 50.12
 ---- batch: 030 ----
mean loss: 49.59
 ---- batch: 040 ----
mean loss: 49.48
train mean loss: 49.61
epoch train time: 0:00:00.668839
elapsed time: 0:02:00.522441
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:55:43.348105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.45
 ---- batch: 020 ----
mean loss: 51.33
 ---- batch: 030 ----
mean loss: 50.75
 ---- batch: 040 ----
mean loss: 49.34
train mean loss: 49.62
epoch train time: 0:00:00.674405
elapsed time: 0:02:01.197104
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:55:44.022813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.29
 ---- batch: 020 ----
mean loss: 50.02
 ---- batch: 030 ----
mean loss: 48.85
 ---- batch: 040 ----
mean loss: 49.50
train mean loss: 48.94
epoch train time: 0:00:00.665517
elapsed time: 0:02:01.862924
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:55:44.688583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.04
 ---- batch: 020 ----
mean loss: 49.46
 ---- batch: 030 ----
mean loss: 50.09
 ---- batch: 040 ----
mean loss: 49.33
train mean loss: 48.51
epoch train time: 0:00:00.701754
elapsed time: 0:02:02.564918
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:55:45.390585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.00
 ---- batch: 020 ----
mean loss: 49.78
 ---- batch: 030 ----
mean loss: 48.47
 ---- batch: 040 ----
mean loss: 48.03
train mean loss: 48.27
epoch train time: 0:00:00.700906
elapsed time: 0:02:03.266128
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:55:46.091783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.93
 ---- batch: 020 ----
mean loss: 48.14
 ---- batch: 030 ----
mean loss: 47.15
 ---- batch: 040 ----
mean loss: 51.06
train mean loss: 48.56
epoch train time: 0:00:00.666888
elapsed time: 0:02:03.933266
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:55:46.758933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.73
 ---- batch: 020 ----
mean loss: 47.80
 ---- batch: 030 ----
mean loss: 46.98
 ---- batch: 040 ----
mean loss: 49.32
train mean loss: 47.77
epoch train time: 0:00:00.662033
elapsed time: 0:02:04.595559
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:55:47.421223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.87
 ---- batch: 020 ----
mean loss: 46.02
 ---- batch: 030 ----
mean loss: 49.04
 ---- batch: 040 ----
mean loss: 50.30
train mean loss: 48.08
epoch train time: 0:00:00.710666
elapsed time: 0:02:05.306459
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:55:48.132110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.37
 ---- batch: 020 ----
mean loss: 48.43
 ---- batch: 030 ----
mean loss: 45.78
 ---- batch: 040 ----
mean loss: 50.58
train mean loss: 47.78
epoch train time: 0:00:00.686213
elapsed time: 0:02:05.992965
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:55:48.818639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.56
 ---- batch: 020 ----
mean loss: 46.19
 ---- batch: 030 ----
mean loss: 47.91
 ---- batch: 040 ----
mean loss: 47.95
train mean loss: 46.71
epoch train time: 0:00:00.687202
elapsed time: 0:02:06.680444
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:55:49.506105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.19
 ---- batch: 020 ----
mean loss: 47.45
 ---- batch: 030 ----
mean loss: 48.90
 ---- batch: 040 ----
mean loss: 47.11
train mean loss: 47.14
epoch train time: 0:00:00.710034
elapsed time: 0:02:07.390696
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:55:50.216347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.38
 ---- batch: 020 ----
mean loss: 44.80
 ---- batch: 030 ----
mean loss: 47.32
 ---- batch: 040 ----
mean loss: 46.04
train mean loss: 46.75
epoch train time: 0:00:00.669134
elapsed time: 0:02:08.060073
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:55:50.885726
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.78
 ---- batch: 020 ----
mean loss: 45.08
 ---- batch: 030 ----
mean loss: 47.39
 ---- batch: 040 ----
mean loss: 43.57
train mean loss: 45.77
epoch train time: 0:00:00.669722
elapsed time: 0:02:08.730017
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:55:51.555688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.31
 ---- batch: 020 ----
mean loss: 45.27
 ---- batch: 030 ----
mean loss: 46.44
 ---- batch: 040 ----
mean loss: 45.44
train mean loss: 46.22
epoch train time: 0:00:00.670876
elapsed time: 0:02:09.401186
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:55:52.226850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.59
 ---- batch: 020 ----
mean loss: 45.50
 ---- batch: 030 ----
mean loss: 47.99
 ---- batch: 040 ----
mean loss: 43.35
train mean loss: 45.64
epoch train time: 0:00:00.702250
elapsed time: 0:02:10.103714
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:55:52.929369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.87
 ---- batch: 020 ----
mean loss: 42.20
 ---- batch: 030 ----
mean loss: 47.71
 ---- batch: 040 ----
mean loss: 46.46
train mean loss: 45.24
epoch train time: 0:00:00.681650
elapsed time: 0:02:10.785603
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:55:53.611277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.36
 ---- batch: 020 ----
mean loss: 44.90
 ---- batch: 030 ----
mean loss: 43.91
 ---- batch: 040 ----
mean loss: 44.94
train mean loss: 44.86
epoch train time: 0:00:00.689890
elapsed time: 0:02:11.475722
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:55:54.301382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.58
 ---- batch: 020 ----
mean loss: 43.30
 ---- batch: 030 ----
mean loss: 47.66
 ---- batch: 040 ----
mean loss: 43.91
train mean loss: 45.05
epoch train time: 0:00:00.686625
elapsed time: 0:02:12.162563
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:55:54.988430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.66
 ---- batch: 020 ----
mean loss: 46.63
 ---- batch: 030 ----
mean loss: 45.21
 ---- batch: 040 ----
mean loss: 46.11
train mean loss: 45.60
epoch train time: 0:00:00.647155
elapsed time: 0:02:12.810167
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:55:55.635818
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.30
 ---- batch: 020 ----
mean loss: 47.40
 ---- batch: 030 ----
mean loss: 44.76
 ---- batch: 040 ----
mean loss: 43.18
train mean loss: 44.94
epoch train time: 0:00:00.711568
elapsed time: 0:02:13.522002
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:55:56.347619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.02
 ---- batch: 020 ----
mean loss: 45.43
 ---- batch: 030 ----
mean loss: 44.11
 ---- batch: 040 ----
mean loss: 42.27
train mean loss: 44.40
epoch train time: 0:00:00.700808
elapsed time: 0:02:14.223023
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:55:57.048712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.93
 ---- batch: 020 ----
mean loss: 44.70
 ---- batch: 030 ----
mean loss: 44.70
 ---- batch: 040 ----
mean loss: 42.87
train mean loss: 44.12
epoch train time: 0:00:00.712098
elapsed time: 0:02:14.935385
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:55:57.761091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.12
 ---- batch: 020 ----
mean loss: 43.77
 ---- batch: 030 ----
mean loss: 43.89
 ---- batch: 040 ----
mean loss: 46.08
train mean loss: 43.68
epoch train time: 0:00:00.657120
elapsed time: 0:02:15.592760
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:55:58.418408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.11
 ---- batch: 020 ----
mean loss: 41.49
 ---- batch: 030 ----
mean loss: 41.63
 ---- batch: 040 ----
mean loss: 44.93
train mean loss: 43.26
epoch train time: 0:00:00.655237
elapsed time: 0:02:16.248201
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:55:59.073847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.01
 ---- batch: 020 ----
mean loss: 44.27
 ---- batch: 030 ----
mean loss: 40.47
 ---- batch: 040 ----
mean loss: 43.89
train mean loss: 43.16
epoch train time: 0:00:00.664923
elapsed time: 0:02:16.913373
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:55:59.739023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.31
 ---- batch: 020 ----
mean loss: 44.12
 ---- batch: 030 ----
mean loss: 43.26
 ---- batch: 040 ----
mean loss: 39.64
train mean loss: 42.30
epoch train time: 0:00:00.704721
elapsed time: 0:02:17.618367
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:56:00.444021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.24
 ---- batch: 020 ----
mean loss: 45.16
 ---- batch: 030 ----
mean loss: 40.84
 ---- batch: 040 ----
mean loss: 41.88
train mean loss: 42.23
epoch train time: 0:00:00.703567
elapsed time: 0:02:18.322169
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:56:01.147844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.13
 ---- batch: 020 ----
mean loss: 41.65
 ---- batch: 030 ----
mean loss: 41.24
 ---- batch: 040 ----
mean loss: 43.88
train mean loss: 42.46
epoch train time: 0:00:00.678778
elapsed time: 0:02:19.001210
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:56:01.826885
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.79
 ---- batch: 020 ----
mean loss: 41.85
 ---- batch: 030 ----
mean loss: 40.46
 ---- batch: 040 ----
mean loss: 43.69
train mean loss: 41.80
epoch train time: 0:00:00.663087
elapsed time: 0:02:19.664530
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:56:02.490177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.50
 ---- batch: 020 ----
mean loss: 43.23
 ---- batch: 030 ----
mean loss: 41.56
 ---- batch: 040 ----
mean loss: 42.14
train mean loss: 42.25
epoch train time: 0:00:00.674748
elapsed time: 0:02:20.339480
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:56:03.165123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.40
 ---- batch: 020 ----
mean loss: 43.29
 ---- batch: 030 ----
mean loss: 39.58
 ---- batch: 040 ----
mean loss: 43.40
train mean loss: 41.25
epoch train time: 0:00:00.703870
elapsed time: 0:02:21.043602
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:56:03.869257
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.69
 ---- batch: 020 ----
mean loss: 39.68
 ---- batch: 030 ----
mean loss: 44.73
 ---- batch: 040 ----
mean loss: 41.47
train mean loss: 41.83
epoch train time: 0:00:00.689750
elapsed time: 0:02:21.733592
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:56:04.559256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.46
 ---- batch: 020 ----
mean loss: 44.30
 ---- batch: 030 ----
mean loss: 43.18
 ---- batch: 040 ----
mean loss: 36.83
train mean loss: 41.88
epoch train time: 0:00:00.691098
elapsed time: 0:02:22.424919
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:56:05.250599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.63
 ---- batch: 020 ----
mean loss: 39.78
 ---- batch: 030 ----
mean loss: 40.78
 ---- batch: 040 ----
mean loss: 43.36
train mean loss: 41.08
epoch train time: 0:00:00.675644
elapsed time: 0:02:23.100813
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:56:05.926494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.32
 ---- batch: 020 ----
mean loss: 41.38
 ---- batch: 030 ----
mean loss: 41.03
 ---- batch: 040 ----
mean loss: 41.82
train mean loss: 40.85
epoch train time: 0:00:00.670519
elapsed time: 0:02:23.771597
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:56:06.597265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.84
 ---- batch: 020 ----
mean loss: 40.43
 ---- batch: 030 ----
mean loss: 42.91
 ---- batch: 040 ----
mean loss: 42.45
train mean loss: 41.03
epoch train time: 0:00:00.693506
elapsed time: 0:02:24.465395
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:56:07.291071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.82
 ---- batch: 020 ----
mean loss: 41.17
 ---- batch: 030 ----
mean loss: 38.52
 ---- batch: 040 ----
mean loss: 40.46
train mean loss: 40.06
epoch train time: 0:00:00.707955
elapsed time: 0:02:25.173607
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:56:07.999263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.38
 ---- batch: 020 ----
mean loss: 38.49
 ---- batch: 030 ----
mean loss: 42.10
 ---- batch: 040 ----
mean loss: 38.76
train mean loss: 39.80
epoch train time: 0:00:00.676269
elapsed time: 0:02:25.850114
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:56:08.675768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.71
 ---- batch: 020 ----
mean loss: 38.32
 ---- batch: 030 ----
mean loss: 39.83
 ---- batch: 040 ----
mean loss: 38.51
train mean loss: 39.43
epoch train time: 0:00:00.661061
elapsed time: 0:02:26.511378
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:56:09.337025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.10
 ---- batch: 020 ----
mean loss: 38.70
 ---- batch: 030 ----
mean loss: 41.48
 ---- batch: 040 ----
mean loss: 38.81
train mean loss: 39.71
epoch train time: 0:00:00.643637
elapsed time: 0:02:27.155255
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:56:09.980928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.53
 ---- batch: 020 ----
mean loss: 40.56
 ---- batch: 030 ----
mean loss: 39.23
 ---- batch: 040 ----
mean loss: 37.78
train mean loss: 39.62
epoch train time: 0:00:00.636063
elapsed time: 0:02:27.791559
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:56:10.617222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.64
 ---- batch: 020 ----
mean loss: 39.43
 ---- batch: 030 ----
mean loss: 41.00
 ---- batch: 040 ----
mean loss: 40.06
train mean loss: 40.13
epoch train time: 0:00:00.676398
elapsed time: 0:02:28.468206
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:56:11.293891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.57
 ---- batch: 020 ----
mean loss: 40.03
 ---- batch: 030 ----
mean loss: 38.34
 ---- batch: 040 ----
mean loss: 38.27
train mean loss: 38.88
epoch train time: 0:00:00.695561
elapsed time: 0:02:29.164057
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:56:11.989714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.25
 ---- batch: 020 ----
mean loss: 39.05
 ---- batch: 030 ----
mean loss: 37.34
 ---- batch: 040 ----
mean loss: 38.09
train mean loss: 38.02
epoch train time: 0:00:00.686229
elapsed time: 0:02:29.850509
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:56:12.676193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.73
 ---- batch: 020 ----
mean loss: 37.40
 ---- batch: 030 ----
mean loss: 38.61
 ---- batch: 040 ----
mean loss: 38.01
train mean loss: 37.95
epoch train time: 0:00:00.689298
elapsed time: 0:02:30.540045
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:56:13.365694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.38
 ---- batch: 020 ----
mean loss: 37.41
 ---- batch: 030 ----
mean loss: 38.31
 ---- batch: 040 ----
mean loss: 39.29
train mean loss: 38.16
epoch train time: 0:00:00.670888
elapsed time: 0:02:31.211178
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:56:14.036839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.57
 ---- batch: 020 ----
mean loss: 37.37
 ---- batch: 030 ----
mean loss: 37.97
 ---- batch: 040 ----
mean loss: 37.61
train mean loss: 37.68
epoch train time: 0:00:00.670610
elapsed time: 0:02:31.882067
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:56:14.707728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.64
 ---- batch: 020 ----
mean loss: 36.67
 ---- batch: 030 ----
mean loss: 36.21
 ---- batch: 040 ----
mean loss: 37.89
train mean loss: 36.44
epoch train time: 0:00:00.702293
elapsed time: 0:02:32.584693
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:56:15.410329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.48
 ---- batch: 020 ----
mean loss: 37.59
 ---- batch: 030 ----
mean loss: 36.05
 ---- batch: 040 ----
mean loss: 36.97
train mean loss: 36.53
epoch train time: 0:00:00.684482
elapsed time: 0:02:33.269408
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:56:16.095059
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.95
 ---- batch: 020 ----
mean loss: 36.89
 ---- batch: 030 ----
mean loss: 36.21
 ---- batch: 040 ----
mean loss: 35.51
train mean loss: 36.67
epoch train time: 0:00:00.655977
elapsed time: 0:02:33.925650
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:56:16.751318
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.71
 ---- batch: 020 ----
mean loss: 36.48
 ---- batch: 030 ----
mean loss: 36.21
 ---- batch: 040 ----
mean loss: 37.16
train mean loss: 36.30
epoch train time: 0:00:00.651119
elapsed time: 0:02:34.577006
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:56:17.402656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.51
 ---- batch: 020 ----
mean loss: 34.89
 ---- batch: 030 ----
mean loss: 36.36
 ---- batch: 040 ----
mean loss: 36.61
train mean loss: 36.29
epoch train time: 0:00:00.673584
elapsed time: 0:02:35.250852
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:56:18.076530
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.00
 ---- batch: 020 ----
mean loss: 35.33
 ---- batch: 030 ----
mean loss: 36.35
 ---- batch: 040 ----
mean loss: 35.03
train mean loss: 36.33
epoch train time: 0:00:00.694368
elapsed time: 0:02:35.945574
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:56:18.771254
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.04
 ---- batch: 020 ----
mean loss: 35.56
 ---- batch: 030 ----
mean loss: 35.19
 ---- batch: 040 ----
mean loss: 36.81
train mean loss: 36.43
epoch train time: 0:00:00.691381
elapsed time: 0:02:36.637245
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:56:19.462927
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.92
 ---- batch: 020 ----
mean loss: 36.42
 ---- batch: 030 ----
mean loss: 35.19
 ---- batch: 040 ----
mean loss: 35.92
train mean loss: 36.49
epoch train time: 0:00:00.693283
elapsed time: 0:02:37.330842
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:56:20.156497
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.13
 ---- batch: 020 ----
mean loss: 36.62
 ---- batch: 030 ----
mean loss: 34.14
 ---- batch: 040 ----
mean loss: 38.44
train mean loss: 36.24
epoch train time: 0:00:00.652595
elapsed time: 0:02:37.983672
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:56:20.809336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.82
 ---- batch: 020 ----
mean loss: 36.19
 ---- batch: 030 ----
mean loss: 37.06
 ---- batch: 040 ----
mean loss: 34.95
train mean loss: 36.43
epoch train time: 0:00:00.667877
elapsed time: 0:02:38.651779
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:56:21.477430
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.48
 ---- batch: 020 ----
mean loss: 35.72
 ---- batch: 030 ----
mean loss: 34.89
 ---- batch: 040 ----
mean loss: 35.83
train mean loss: 36.20
epoch train time: 0:00:00.675580
elapsed time: 0:02:39.327623
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:56:22.153276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.33
 ---- batch: 020 ----
mean loss: 36.08
 ---- batch: 030 ----
mean loss: 34.83
 ---- batch: 040 ----
mean loss: 36.61
train mean loss: 36.19
epoch train time: 0:00:00.690960
elapsed time: 0:02:40.018833
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:56:22.844501
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.76
 ---- batch: 020 ----
mean loss: 33.85
 ---- batch: 030 ----
mean loss: 35.44
 ---- batch: 040 ----
mean loss: 37.76
train mean loss: 36.06
epoch train time: 0:00:00.696301
elapsed time: 0:02:40.715367
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:56:23.541036
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.50
 ---- batch: 020 ----
mean loss: 36.04
 ---- batch: 030 ----
mean loss: 37.03
 ---- batch: 040 ----
mean loss: 36.05
train mean loss: 36.13
epoch train time: 0:00:00.677308
elapsed time: 0:02:41.392908
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:56:24.218576
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.10
 ---- batch: 020 ----
mean loss: 34.71
 ---- batch: 030 ----
mean loss: 36.87
 ---- batch: 040 ----
mean loss: 36.68
train mean loss: 36.01
epoch train time: 0:00:00.666338
elapsed time: 0:02:42.059475
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:56:24.885123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.31
 ---- batch: 020 ----
mean loss: 35.80
 ---- batch: 030 ----
mean loss: 35.30
 ---- batch: 040 ----
mean loss: 34.05
train mean loss: 35.73
epoch train time: 0:00:00.654958
elapsed time: 0:02:42.714669
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:56:25.540357
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.66
 ---- batch: 020 ----
mean loss: 35.47
 ---- batch: 030 ----
mean loss: 37.18
 ---- batch: 040 ----
mean loss: 35.32
train mean loss: 36.16
epoch train time: 0:00:00.709531
elapsed time: 0:02:43.424483
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:56:26.250160
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.49
 ---- batch: 020 ----
mean loss: 37.42
 ---- batch: 030 ----
mean loss: 35.38
 ---- batch: 040 ----
mean loss: 36.97
train mean loss: 35.92
epoch train time: 0:00:00.706005
elapsed time: 0:02:44.130737
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:56:26.956443
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.17
 ---- batch: 020 ----
mean loss: 37.23
 ---- batch: 030 ----
mean loss: 36.28
 ---- batch: 040 ----
mean loss: 34.38
train mean loss: 36.16
epoch train time: 0:00:00.656447
elapsed time: 0:02:44.787469
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:56:27.613155
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.99
 ---- batch: 020 ----
mean loss: 36.78
 ---- batch: 030 ----
mean loss: 37.82
 ---- batch: 040 ----
mean loss: 35.45
train mean loss: 35.95
epoch train time: 0:00:00.654739
elapsed time: 0:02:45.442462
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:56:28.268112
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.20
 ---- batch: 020 ----
mean loss: 35.54
 ---- batch: 030 ----
mean loss: 36.24
 ---- batch: 040 ----
mean loss: 37.44
train mean loss: 36.00
epoch train time: 0:00:00.658915
elapsed time: 0:02:46.101621
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:56:28.927309
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.08
 ---- batch: 020 ----
mean loss: 38.01
 ---- batch: 030 ----
mean loss: 34.54
 ---- batch: 040 ----
mean loss: 35.57
train mean loss: 35.84
epoch train time: 0:00:00.689867
elapsed time: 0:02:46.791789
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:56:29.617462
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.25
 ---- batch: 020 ----
mean loss: 34.80
 ---- batch: 030 ----
mean loss: 35.69
 ---- batch: 040 ----
mean loss: 37.09
train mean loss: 35.72
epoch train time: 0:00:00.699333
elapsed time: 0:02:47.491411
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:56:30.317082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.87
 ---- batch: 020 ----
mean loss: 36.68
 ---- batch: 030 ----
mean loss: 35.26
 ---- batch: 040 ----
mean loss: 35.49
train mean loss: 35.81
epoch train time: 0:00:00.700964
elapsed time: 0:02:48.192607
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:56:31.018260
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.89
 ---- batch: 020 ----
mean loss: 35.34
 ---- batch: 030 ----
mean loss: 35.91
 ---- batch: 040 ----
mean loss: 35.34
train mean loss: 35.69
epoch train time: 0:00:00.676381
elapsed time: 0:02:48.869193
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:56:31.694841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.98
 ---- batch: 020 ----
mean loss: 36.57
 ---- batch: 030 ----
mean loss: 36.41
 ---- batch: 040 ----
mean loss: 36.40
train mean loss: 35.88
epoch train time: 0:00:00.656104
elapsed time: 0:02:49.525513
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:56:32.351170
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.70
 ---- batch: 020 ----
mean loss: 35.56
 ---- batch: 030 ----
mean loss: 36.92
 ---- batch: 040 ----
mean loss: 34.48
train mean loss: 35.32
epoch train time: 0:00:00.698190
elapsed time: 0:02:50.223998
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:56:33.049661
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.44
 ---- batch: 020 ----
mean loss: 33.18
 ---- batch: 030 ----
mean loss: 37.89
 ---- batch: 040 ----
mean loss: 35.98
train mean loss: 35.54
epoch train time: 0:00:00.726359
elapsed time: 0:02:50.950685
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:56:33.776352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.28
 ---- batch: 020 ----
mean loss: 35.33
 ---- batch: 030 ----
mean loss: 35.23
 ---- batch: 040 ----
mean loss: 37.18
train mean loss: 35.44
epoch train time: 0:00:00.711006
elapsed time: 0:02:51.661951
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:56:34.487604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.86
 ---- batch: 020 ----
mean loss: 36.32
 ---- batch: 030 ----
mean loss: 35.57
 ---- batch: 040 ----
mean loss: 34.05
train mean loss: 35.45
epoch train time: 0:00:00.688402
elapsed time: 0:02:52.350567
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:56:35.176249
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.82
 ---- batch: 020 ----
mean loss: 33.35
 ---- batch: 030 ----
mean loss: 35.97
 ---- batch: 040 ----
mean loss: 36.80
train mean loss: 35.68
epoch train time: 0:00:00.675071
elapsed time: 0:02:53.025917
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:56:35.851565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.10
 ---- batch: 020 ----
mean loss: 36.26
 ---- batch: 030 ----
mean loss: 35.76
 ---- batch: 040 ----
mean loss: 33.42
train mean loss: 35.36
epoch train time: 0:00:00.672384
elapsed time: 0:02:53.698525
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:56:36.524181
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.15
 ---- batch: 020 ----
mean loss: 37.24
 ---- batch: 030 ----
mean loss: 34.83
 ---- batch: 040 ----
mean loss: 34.79
train mean loss: 35.79
epoch train time: 0:00:00.702370
elapsed time: 0:02:54.401199
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:56:37.226842
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.11
 ---- batch: 020 ----
mean loss: 35.72
 ---- batch: 030 ----
mean loss: 35.42
 ---- batch: 040 ----
mean loss: 35.61
train mean loss: 35.56
epoch train time: 0:00:00.697246
elapsed time: 0:02:55.098708
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:56:37.924363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.56
 ---- batch: 020 ----
mean loss: 35.02
 ---- batch: 030 ----
mean loss: 33.82
 ---- batch: 040 ----
mean loss: 36.98
train mean loss: 35.50
epoch train time: 0:00:00.665202
elapsed time: 0:02:55.764140
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:56:38.589794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.79
 ---- batch: 020 ----
mean loss: 34.68
 ---- batch: 030 ----
mean loss: 36.15
 ---- batch: 040 ----
mean loss: 35.02
train mean loss: 35.07
epoch train time: 0:00:00.689001
elapsed time: 0:02:56.453351
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:56:39.279015
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.91
 ---- batch: 020 ----
mean loss: 32.99
 ---- batch: 030 ----
mean loss: 36.56
 ---- batch: 040 ----
mean loss: 35.92
train mean loss: 35.67
epoch train time: 0:00:00.679293
elapsed time: 0:02:57.132926
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:56:39.958579
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.95
 ---- batch: 020 ----
mean loss: 36.02
 ---- batch: 030 ----
mean loss: 34.72
 ---- batch: 040 ----
mean loss: 35.84
train mean loss: 35.42
epoch train time: 0:00:00.714988
elapsed time: 0:02:57.848159
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:56:40.673813
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.62
 ---- batch: 020 ----
mean loss: 36.12
 ---- batch: 030 ----
mean loss: 36.55
 ---- batch: 040 ----
mean loss: 36.43
train mean loss: 35.40
epoch train time: 0:00:00.712295
elapsed time: 0:02:58.560711
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:56:41.386386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.24
 ---- batch: 020 ----
mean loss: 36.24
 ---- batch: 030 ----
mean loss: 33.03
 ---- batch: 040 ----
mean loss: 35.38
train mean loss: 35.39
epoch train time: 0:00:00.676104
elapsed time: 0:02:59.237053
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:56:42.062704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.44
 ---- batch: 020 ----
mean loss: 34.02
 ---- batch: 030 ----
mean loss: 34.63
 ---- batch: 040 ----
mean loss: 36.10
train mean loss: 34.85
epoch train time: 0:00:00.652294
elapsed time: 0:02:59.889622
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:56:42.715360
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.72
 ---- batch: 020 ----
mean loss: 34.97
 ---- batch: 030 ----
mean loss: 34.94
 ---- batch: 040 ----
mean loss: 34.54
train mean loss: 35.26
epoch train time: 0:00:00.666413
elapsed time: 0:03:00.556334
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:56:43.381994
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.88
 ---- batch: 020 ----
mean loss: 33.85
 ---- batch: 030 ----
mean loss: 35.81
 ---- batch: 040 ----
mean loss: 35.29
train mean loss: 35.40
epoch train time: 0:00:00.676137
elapsed time: 0:03:01.232745
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:56:44.058415
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.69
 ---- batch: 020 ----
mean loss: 35.98
 ---- batch: 030 ----
mean loss: 36.04
 ---- batch: 040 ----
mean loss: 34.41
train mean loss: 35.16
epoch train time: 0:00:00.685112
elapsed time: 0:03:01.918147
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:56:44.743805
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.25
 ---- batch: 020 ----
mean loss: 36.22
 ---- batch: 030 ----
mean loss: 35.88
 ---- batch: 040 ----
mean loss: 34.78
train mean loss: 35.09
epoch train time: 0:00:00.697816
elapsed time: 0:03:02.616200
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:56:45.441866
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.46
 ---- batch: 020 ----
mean loss: 34.34
 ---- batch: 030 ----
mean loss: 35.97
 ---- batch: 040 ----
mean loss: 37.11
train mean loss: 35.17
epoch train time: 0:00:00.655912
elapsed time: 0:03:03.272378
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:56:46.098060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.41
 ---- batch: 020 ----
mean loss: 36.50
 ---- batch: 030 ----
mean loss: 34.87
 ---- batch: 040 ----
mean loss: 34.84
train mean loss: 35.15
epoch train time: 0:00:00.676683
elapsed time: 0:03:03.949381
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:56:46.775042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.49
 ---- batch: 020 ----
mean loss: 35.59
 ---- batch: 030 ----
mean loss: 35.77
 ---- batch: 040 ----
mean loss: 33.64
train mean loss: 35.37
epoch train time: 0:00:00.697510
elapsed time: 0:03:04.647142
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:56:47.472816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.08
 ---- batch: 020 ----
mean loss: 35.56
 ---- batch: 030 ----
mean loss: 34.48
 ---- batch: 040 ----
mean loss: 35.04
train mean loss: 35.23
epoch train time: 0:00:00.740555
elapsed time: 0:03:05.395118
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_6/checkpoint.pth.tar
**** end time: 2019-09-20 21:56:48.220716 ****
