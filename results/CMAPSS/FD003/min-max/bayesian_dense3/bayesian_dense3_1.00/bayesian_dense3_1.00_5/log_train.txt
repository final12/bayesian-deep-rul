Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_5', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5989
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianDense3...
Done.
**** start time: 2019-09-20 21:50:21.665707 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
    BayesianLinear-2                  [-1, 100]          84,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 124,200
Trainable params: 124,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:50:21.675274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4651.53
 ---- batch: 020 ----
mean loss: 4447.11
 ---- batch: 030 ----
mean loss: 4240.67
 ---- batch: 040 ----
mean loss: 3946.77
train mean loss: 4283.51
epoch train time: 0:00:14.976803
elapsed time: 0:00:14.991990
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:50:36.657731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3699.43
 ---- batch: 020 ----
mean loss: 3485.33
 ---- batch: 030 ----
mean loss: 3328.95
 ---- batch: 040 ----
mean loss: 3267.75
train mean loss: 3423.99
epoch train time: 0:00:00.665288
elapsed time: 0:00:15.657450
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:50:37.323254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3017.72
 ---- batch: 020 ----
mean loss: 2951.26
 ---- batch: 030 ----
mean loss: 2914.19
 ---- batch: 040 ----
mean loss: 2791.95
train mean loss: 2903.16
epoch train time: 0:00:00.661104
elapsed time: 0:00:16.318828
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:50:37.984630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2673.37
 ---- batch: 020 ----
mean loss: 2705.30
 ---- batch: 030 ----
mean loss: 2514.54
 ---- batch: 040 ----
mean loss: 2566.07
train mean loss: 2616.58
epoch train time: 0:00:00.683148
elapsed time: 0:00:17.002230
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:50:38.668023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2520.07
 ---- batch: 020 ----
mean loss: 2453.23
 ---- batch: 030 ----
mean loss: 2446.34
 ---- batch: 040 ----
mean loss: 2380.88
train mean loss: 2444.11
epoch train time: 0:00:00.708134
elapsed time: 0:00:17.710616
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:50:39.376421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2326.69
 ---- batch: 020 ----
mean loss: 2359.84
 ---- batch: 030 ----
mean loss: 2289.27
 ---- batch: 040 ----
mean loss: 2244.21
train mean loss: 2302.46
epoch train time: 0:00:00.710957
elapsed time: 0:00:18.421818
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:50:40.087593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2197.63
 ---- batch: 020 ----
mean loss: 2227.96
 ---- batch: 030 ----
mean loss: 2183.01
 ---- batch: 040 ----
mean loss: 2088.49
train mean loss: 2172.35
epoch train time: 0:00:00.648146
elapsed time: 0:00:19.070526
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:50:40.736311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2107.59
 ---- batch: 020 ----
mean loss: 2034.22
 ---- batch: 030 ----
mean loss: 2060.98
 ---- batch: 040 ----
mean loss: 2004.43
train mean loss: 2050.46
epoch train time: 0:00:00.668713
elapsed time: 0:00:19.739486
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:50:41.405291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2029.09
 ---- batch: 020 ----
mean loss: 1942.53
 ---- batch: 030 ----
mean loss: 1911.41
 ---- batch: 040 ----
mean loss: 1883.79
train mean loss: 1937.70
epoch train time: 0:00:00.679166
elapsed time: 0:00:20.418911
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:50:42.084694
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1874.89
 ---- batch: 020 ----
mean loss: 1831.23
 ---- batch: 030 ----
mean loss: 1830.91
 ---- batch: 040 ----
mean loss: 1837.12
train mean loss: 1839.36
epoch train time: 0:00:00.679644
elapsed time: 0:00:21.098879
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:50:42.764664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1771.71
 ---- batch: 020 ----
mean loss: 1774.48
 ---- batch: 030 ----
mean loss: 1751.68
 ---- batch: 040 ----
mean loss: 1688.39
train mean loss: 1746.94
epoch train time: 0:00:00.692349
elapsed time: 0:00:21.791486
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:50:43.457284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1717.45
 ---- batch: 020 ----
mean loss: 1649.62
 ---- batch: 030 ----
mean loss: 1640.24
 ---- batch: 040 ----
mean loss: 1637.01
train mean loss: 1659.59
epoch train time: 0:00:00.663982
elapsed time: 0:00:22.455761
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:50:44.121542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1598.99
 ---- batch: 020 ----
mean loss: 1583.03
 ---- batch: 030 ----
mean loss: 1553.89
 ---- batch: 040 ----
mean loss: 1561.92
train mean loss: 1573.39
epoch train time: 0:00:00.643469
elapsed time: 0:00:23.099452
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:50:44.765235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1521.98
 ---- batch: 020 ----
mean loss: 1502.89
 ---- batch: 030 ----
mean loss: 1492.00
 ---- batch: 040 ----
mean loss: 1480.64
train mean loss: 1498.77
epoch train time: 0:00:00.655458
elapsed time: 0:00:23.755184
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:50:45.421011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1451.96
 ---- batch: 020 ----
mean loss: 1425.74
 ---- batch: 030 ----
mean loss: 1406.61
 ---- batch: 040 ----
mean loss: 1412.25
train mean loss: 1420.02
epoch train time: 0:00:00.671699
elapsed time: 0:00:24.427192
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:50:46.092975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1366.52
 ---- batch: 020 ----
mean loss: 1378.40
 ---- batch: 030 ----
mean loss: 1353.77
 ---- batch: 040 ----
mean loss: 1309.43
train mean loss: 1346.73
epoch train time: 0:00:00.693504
elapsed time: 0:00:25.120986
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:50:46.786772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1250.59
 ---- batch: 020 ----
mean loss: 1237.76
 ---- batch: 030 ----
mean loss: 1224.87
 ---- batch: 040 ----
mean loss: 1222.72
train mean loss: 1232.97
epoch train time: 0:00:00.701904
elapsed time: 0:00:25.823163
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:50:47.488944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1170.68
 ---- batch: 020 ----
mean loss: 1167.19
 ---- batch: 030 ----
mean loss: 1149.90
 ---- batch: 040 ----
mean loss: 1138.01
train mean loss: 1153.82
epoch train time: 0:00:00.684219
elapsed time: 0:00:26.507604
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:50:48.173413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1129.59
 ---- batch: 020 ----
mean loss: 1070.00
 ---- batch: 030 ----
mean loss: 1104.15
 ---- batch: 040 ----
mean loss: 1063.48
train mean loss: 1087.97
epoch train time: 0:00:00.662922
elapsed time: 0:00:27.170796
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:50:48.836581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1035.08
 ---- batch: 020 ----
mean loss: 1031.71
 ---- batch: 030 ----
mean loss: 1028.52
 ---- batch: 040 ----
mean loss: 991.95
train mean loss: 1021.79
epoch train time: 0:00:00.663001
elapsed time: 0:00:27.834112
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:50:49.499867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 973.28
 ---- batch: 020 ----
mean loss: 978.79
 ---- batch: 030 ----
mean loss: 958.62
 ---- batch: 040 ----
mean loss: 939.36
train mean loss: 960.37
epoch train time: 0:00:00.693770
elapsed time: 0:00:28.528127
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:50:50.193919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.08
 ---- batch: 020 ----
mean loss: 900.15
 ---- batch: 030 ----
mean loss: 902.78
 ---- batch: 040 ----
mean loss: 896.40
train mean loss: 905.72
epoch train time: 0:00:00.684612
elapsed time: 0:00:29.212990
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:50:50.878773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 872.13
 ---- batch: 020 ----
mean loss: 854.68
 ---- batch: 030 ----
mean loss: 849.40
 ---- batch: 040 ----
mean loss: 823.48
train mean loss: 847.48
epoch train time: 0:00:00.674770
elapsed time: 0:00:29.887989
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:50:51.553771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.47
 ---- batch: 020 ----
mean loss: 821.45
 ---- batch: 030 ----
mean loss: 791.02
 ---- batch: 040 ----
mean loss: 781.56
train mean loss: 800.12
epoch train time: 0:00:00.667251
elapsed time: 0:00:30.555471
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:50:52.221263
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.20
 ---- batch: 020 ----
mean loss: 750.91
 ---- batch: 030 ----
mean loss: 749.82
 ---- batch: 040 ----
mean loss: 732.16
train mean loss: 752.26
epoch train time: 0:00:00.671150
elapsed time: 0:00:31.226845
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:50:52.892624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 732.66
 ---- batch: 020 ----
mean loss: 727.86
 ---- batch: 030 ----
mean loss: 696.80
 ---- batch: 040 ----
mean loss: 683.36
train mean loss: 706.52
epoch train time: 0:00:00.692074
elapsed time: 0:00:31.919157
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:50:53.584952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 689.09
 ---- batch: 020 ----
mean loss: 671.56
 ---- batch: 030 ----
mean loss: 648.63
 ---- batch: 040 ----
mean loss: 652.72
train mean loss: 663.90
epoch train time: 0:00:00.688895
elapsed time: 0:00:32.608297
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:50:54.274090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 643.36
 ---- batch: 020 ----
mean loss: 628.35
 ---- batch: 030 ----
mean loss: 631.09
 ---- batch: 040 ----
mean loss: 614.72
train mean loss: 626.40
epoch train time: 0:00:00.690727
elapsed time: 0:00:33.299313
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:50:54.965108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 604.90
 ---- batch: 020 ----
mean loss: 587.19
 ---- batch: 030 ----
mean loss: 583.13
 ---- batch: 040 ----
mean loss: 570.95
train mean loss: 585.23
epoch train time: 0:00:00.652293
elapsed time: 0:00:33.951914
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:50:55.617692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 559.60
 ---- batch: 020 ----
mean loss: 545.28
 ---- batch: 030 ----
mean loss: 553.03
 ---- batch: 040 ----
mean loss: 540.92
train mean loss: 548.34
epoch train time: 0:00:00.668674
elapsed time: 0:00:34.620809
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:50:56.286653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.45
 ---- batch: 020 ----
mean loss: 535.20
 ---- batch: 030 ----
mean loss: 520.94
 ---- batch: 040 ----
mean loss: 506.31
train mean loss: 519.06
epoch train time: 0:00:00.681129
elapsed time: 0:00:35.302244
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:50:56.968026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.82
 ---- batch: 020 ----
mean loss: 498.17
 ---- batch: 030 ----
mean loss: 483.18
 ---- batch: 040 ----
mean loss: 479.86
train mean loss: 488.25
epoch train time: 0:00:00.707316
elapsed time: 0:00:36.009806
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:50:57.675588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.19
 ---- batch: 020 ----
mean loss: 471.26
 ---- batch: 030 ----
mean loss: 451.27
 ---- batch: 040 ----
mean loss: 466.79
train mean loss: 460.91
epoch train time: 0:00:00.698400
elapsed time: 0:00:36.708450
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:50:58.374246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 450.86
 ---- batch: 020 ----
mean loss: 432.43
 ---- batch: 030 ----
mean loss: 427.57
 ---- batch: 040 ----
mean loss: 427.08
train mean loss: 433.69
epoch train time: 0:00:00.682200
elapsed time: 0:00:37.390889
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:50:59.056699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.95
 ---- batch: 020 ----
mean loss: 413.47
 ---- batch: 030 ----
mean loss: 403.63
 ---- batch: 040 ----
mean loss: 394.54
train mean loss: 408.44
epoch train time: 0:00:00.668513
elapsed time: 0:00:38.059647
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:50:59.725425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.90
 ---- batch: 020 ----
mean loss: 390.60
 ---- batch: 030 ----
mean loss: 380.91
 ---- batch: 040 ----
mean loss: 385.90
train mean loss: 385.12
epoch train time: 0:00:00.664743
elapsed time: 0:00:38.724624
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:51:00.390405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.36
 ---- batch: 020 ----
mean loss: 367.67
 ---- batch: 030 ----
mean loss: 357.32
 ---- batch: 040 ----
mean loss: 353.89
train mean loss: 361.28
epoch train time: 0:00:00.706441
elapsed time: 0:00:39.431296
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:51:01.097096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.11
 ---- batch: 020 ----
mean loss: 343.20
 ---- batch: 030 ----
mean loss: 338.22
 ---- batch: 040 ----
mean loss: 335.76
train mean loss: 341.34
epoch train time: 0:00:00.687221
elapsed time: 0:00:40.118840
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:51:01.784698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 328.98
 ---- batch: 020 ----
mean loss: 325.47
 ---- batch: 030 ----
mean loss: 317.13
 ---- batch: 040 ----
mean loss: 321.58
train mean loss: 323.42
epoch train time: 0:00:00.696995
elapsed time: 0:00:40.816172
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:51:02.481963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.37
 ---- batch: 020 ----
mean loss: 308.72
 ---- batch: 030 ----
mean loss: 306.36
 ---- batch: 040 ----
mean loss: 294.58
train mean loss: 305.53
epoch train time: 0:00:00.696252
elapsed time: 0:00:41.512648
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:51:03.178442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 298.43
 ---- batch: 020 ----
mean loss: 290.51
 ---- batch: 030 ----
mean loss: 285.97
 ---- batch: 040 ----
mean loss: 280.06
train mean loss: 288.25
epoch train time: 0:00:00.663719
elapsed time: 0:00:42.176657
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:51:03.842434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 278.86
 ---- batch: 020 ----
mean loss: 267.04
 ---- batch: 030 ----
mean loss: 278.37
 ---- batch: 040 ----
mean loss: 273.02
train mean loss: 274.15
epoch train time: 0:00:00.681053
elapsed time: 0:00:42.858008
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:51:04.523812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.00
 ---- batch: 020 ----
mean loss: 260.53
 ---- batch: 030 ----
mean loss: 255.23
 ---- batch: 040 ----
mean loss: 254.01
train mean loss: 257.81
epoch train time: 0:00:00.709603
elapsed time: 0:00:43.567922
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:51:05.233738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.19
 ---- batch: 020 ----
mean loss: 258.14
 ---- batch: 030 ----
mean loss: 241.69
 ---- batch: 040 ----
mean loss: 235.68
train mean loss: 244.66
epoch train time: 0:00:00.691474
elapsed time: 0:00:44.259642
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:51:05.925420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.86
 ---- batch: 020 ----
mean loss: 235.14
 ---- batch: 030 ----
mean loss: 232.40
 ---- batch: 040 ----
mean loss: 227.82
train mean loss: 232.51
epoch train time: 0:00:00.648938
elapsed time: 0:00:44.908787
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:51:06.574569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.24
 ---- batch: 020 ----
mean loss: 223.68
 ---- batch: 030 ----
mean loss: 218.88
 ---- batch: 040 ----
mean loss: 215.48
train mean loss: 221.16
epoch train time: 0:00:00.662947
elapsed time: 0:00:45.571941
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:51:07.237715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.39
 ---- batch: 020 ----
mean loss: 205.72
 ---- batch: 030 ----
mean loss: 214.02
 ---- batch: 040 ----
mean loss: 206.53
train mean loss: 207.82
epoch train time: 0:00:00.662433
elapsed time: 0:00:46.234615
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:51:07.900418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.36
 ---- batch: 020 ----
mean loss: 197.88
 ---- batch: 030 ----
mean loss: 192.44
 ---- batch: 040 ----
mean loss: 197.04
train mean loss: 196.72
epoch train time: 0:00:00.703554
elapsed time: 0:00:46.938446
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:51:08.604232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.53
 ---- batch: 020 ----
mean loss: 190.69
 ---- batch: 030 ----
mean loss: 186.53
 ---- batch: 040 ----
mean loss: 192.34
train mean loss: 190.82
epoch train time: 0:00:00.703624
elapsed time: 0:00:47.642352
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:51:09.308139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.01
 ---- batch: 020 ----
mean loss: 182.01
 ---- batch: 030 ----
mean loss: 179.64
 ---- batch: 040 ----
mean loss: 179.65
train mean loss: 182.04
epoch train time: 0:00:00.685189
elapsed time: 0:00:48.327792
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:51:09.993579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.44
 ---- batch: 020 ----
mean loss: 174.20
 ---- batch: 030 ----
mean loss: 170.77
 ---- batch: 040 ----
mean loss: 170.76
train mean loss: 173.58
epoch train time: 0:00:00.667248
elapsed time: 0:00:48.995265
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:51:10.661044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.71
 ---- batch: 020 ----
mean loss: 164.92
 ---- batch: 030 ----
mean loss: 168.04
 ---- batch: 040 ----
mean loss: 163.62
train mean loss: 166.28
epoch train time: 0:00:00.652529
elapsed time: 0:00:49.647998
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:51:11.313776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.60
 ---- batch: 020 ----
mean loss: 157.18
 ---- batch: 030 ----
mean loss: 158.90
 ---- batch: 040 ----
mean loss: 156.53
train mean loss: 159.12
epoch train time: 0:00:00.686633
elapsed time: 0:00:50.334879
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:51:12.000666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.55
 ---- batch: 020 ----
mean loss: 152.62
 ---- batch: 030 ----
mean loss: 153.22
 ---- batch: 040 ----
mean loss: 153.10
train mean loss: 153.68
epoch train time: 0:00:00.687855
elapsed time: 0:00:51.023001
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:51:12.688917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.16
 ---- batch: 020 ----
mean loss: 144.18
 ---- batch: 030 ----
mean loss: 149.60
 ---- batch: 040 ----
mean loss: 145.16
train mean loss: 146.38
epoch train time: 0:00:00.690603
elapsed time: 0:00:51.713962
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:51:13.379792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.63
 ---- batch: 020 ----
mean loss: 148.77
 ---- batch: 030 ----
mean loss: 139.66
 ---- batch: 040 ----
mean loss: 137.21
train mean loss: 142.47
epoch train time: 0:00:00.662799
elapsed time: 0:00:52.377052
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:51:14.042832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.34
 ---- batch: 020 ----
mean loss: 135.06
 ---- batch: 030 ----
mean loss: 129.18
 ---- batch: 040 ----
mean loss: 136.85
train mean loss: 135.01
epoch train time: 0:00:00.659242
elapsed time: 0:00:53.036516
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:51:14.702334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.26
 ---- batch: 020 ----
mean loss: 132.25
 ---- batch: 030 ----
mean loss: 130.90
 ---- batch: 040 ----
mean loss: 131.88
train mean loss: 131.41
epoch train time: 0:00:00.682949
elapsed time: 0:00:53.719783
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:51:15.385570
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.53
 ---- batch: 020 ----
mean loss: 126.25
 ---- batch: 030 ----
mean loss: 128.42
 ---- batch: 040 ----
mean loss: 127.80
train mean loss: 127.41
epoch train time: 0:00:00.705709
elapsed time: 0:00:54.425809
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:51:16.091591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.85
 ---- batch: 020 ----
mean loss: 124.46
 ---- batch: 030 ----
mean loss: 120.86
 ---- batch: 040 ----
mean loss: 119.68
train mean loss: 122.78
epoch train time: 0:00:00.695478
elapsed time: 0:00:55.121525
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:51:16.787315
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.97
 ---- batch: 020 ----
mean loss: 118.88
 ---- batch: 030 ----
mean loss: 121.90
 ---- batch: 040 ----
mean loss: 118.91
train mean loss: 119.71
epoch train time: 0:00:00.646321
elapsed time: 0:00:55.768075
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:51:17.433866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.35
 ---- batch: 020 ----
mean loss: 119.47
 ---- batch: 030 ----
mean loss: 116.38
 ---- batch: 040 ----
mean loss: 117.31
train mean loss: 117.72
epoch train time: 0:00:00.644289
elapsed time: 0:00:56.412618
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:51:18.078420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.47
 ---- batch: 020 ----
mean loss: 113.07
 ---- batch: 030 ----
mean loss: 111.81
 ---- batch: 040 ----
mean loss: 115.03
train mean loss: 112.12
epoch train time: 0:00:00.652302
elapsed time: 0:00:57.065188
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:51:18.730974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.68
 ---- batch: 020 ----
mean loss: 107.34
 ---- batch: 030 ----
mean loss: 112.87
 ---- batch: 040 ----
mean loss: 104.52
train mean loss: 108.79
epoch train time: 0:00:00.687683
elapsed time: 0:00:57.753192
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:51:19.418994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 105.27
 ---- batch: 020 ----
mean loss: 105.43
 ---- batch: 030 ----
mean loss: 107.94
 ---- batch: 040 ----
mean loss: 106.21
train mean loss: 106.27
epoch train time: 0:00:00.682425
elapsed time: 0:00:58.435864
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:51:20.101646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.61
 ---- batch: 020 ----
mean loss: 105.78
 ---- batch: 030 ----
mean loss: 98.82
 ---- batch: 040 ----
mean loss: 104.69
train mean loss: 103.27
epoch train time: 0:00:00.686017
elapsed time: 0:00:59.122108
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:51:20.787908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 103.06
 ---- batch: 020 ----
mean loss: 101.96
 ---- batch: 030 ----
mean loss: 101.16
 ---- batch: 040 ----
mean loss: 99.24
train mean loss: 101.41
epoch train time: 0:00:00.668174
elapsed time: 0:00:59.790539
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:51:21.456336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 102.55
 ---- batch: 020 ----
mean loss: 101.35
 ---- batch: 030 ----
mean loss: 103.06
 ---- batch: 040 ----
mean loss: 96.50
train mean loss: 100.07
epoch train time: 0:00:00.663317
elapsed time: 0:01:00.454242
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:51:22.120068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.54
 ---- batch: 020 ----
mean loss: 98.13
 ---- batch: 030 ----
mean loss: 96.18
 ---- batch: 040 ----
mean loss: 96.58
train mean loss: 96.92
epoch train time: 0:00:00.696615
elapsed time: 0:01:01.151215
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:51:22.817018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.11
 ---- batch: 020 ----
mean loss: 94.77
 ---- batch: 030 ----
mean loss: 92.81
 ---- batch: 040 ----
mean loss: 95.41
train mean loss: 94.84
epoch train time: 0:00:00.700410
elapsed time: 0:01:01.851989
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:51:23.517781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 96.12
 ---- batch: 020 ----
mean loss: 90.64
 ---- batch: 030 ----
mean loss: 92.24
 ---- batch: 040 ----
mean loss: 94.78
train mean loss: 93.38
epoch train time: 0:00:00.702178
elapsed time: 0:01:02.554441
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:51:24.220226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 91.29
 ---- batch: 020 ----
mean loss: 91.29
 ---- batch: 030 ----
mean loss: 92.06
 ---- batch: 040 ----
mean loss: 90.20
train mean loss: 91.28
epoch train time: 0:00:00.663824
elapsed time: 0:01:03.218518
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:51:24.884295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 87.60
 ---- batch: 020 ----
mean loss: 95.25
 ---- batch: 030 ----
mean loss: 87.18
 ---- batch: 040 ----
mean loss: 89.60
train mean loss: 89.27
epoch train time: 0:00:00.676024
elapsed time: 0:01:03.894813
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:51:25.560596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.18
 ---- batch: 020 ----
mean loss: 84.61
 ---- batch: 030 ----
mean loss: 88.82
 ---- batch: 040 ----
mean loss: 89.78
train mean loss: 88.00
epoch train time: 0:00:00.686992
elapsed time: 0:01:04.582063
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:51:26.247872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 85.95
 ---- batch: 020 ----
mean loss: 86.91
 ---- batch: 030 ----
mean loss: 87.01
 ---- batch: 040 ----
mean loss: 86.75
train mean loss: 86.86
epoch train time: 0:00:00.713852
elapsed time: 0:01:05.296191
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:51:26.961998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.36
 ---- batch: 020 ----
mean loss: 85.59
 ---- batch: 030 ----
mean loss: 83.89
 ---- batch: 040 ----
mean loss: 83.47
train mean loss: 84.92
epoch train time: 0:00:00.712502
elapsed time: 0:01:06.008964
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:51:27.674745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.17
 ---- batch: 020 ----
mean loss: 80.29
 ---- batch: 030 ----
mean loss: 84.32
 ---- batch: 040 ----
mean loss: 86.28
train mean loss: 83.69
epoch train time: 0:00:00.672358
elapsed time: 0:01:06.681546
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:51:28.347322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 82.76
 ---- batch: 020 ----
mean loss: 84.84
 ---- batch: 030 ----
mean loss: 82.46
 ---- batch: 040 ----
mean loss: 80.25
train mean loss: 82.23
epoch train time: 0:00:00.693535
elapsed time: 0:01:07.375354
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:51:29.041135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.17
 ---- batch: 020 ----
mean loss: 79.15
 ---- batch: 030 ----
mean loss: 83.13
 ---- batch: 040 ----
mean loss: 82.86
train mean loss: 81.53
epoch train time: 0:00:00.669855
elapsed time: 0:01:08.045500
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:51:29.711300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.86
 ---- batch: 020 ----
mean loss: 79.13
 ---- batch: 030 ----
mean loss: 80.92
 ---- batch: 040 ----
mean loss: 83.47
train mean loss: 81.04
epoch train time: 0:00:00.699962
elapsed time: 0:01:08.745742
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:51:30.411541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.97
 ---- batch: 020 ----
mean loss: 77.98
 ---- batch: 030 ----
mean loss: 77.99
 ---- batch: 040 ----
mean loss: 79.12
train mean loss: 79.92
epoch train time: 0:00:00.702226
elapsed time: 0:01:09.448282
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:51:31.114066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.79
 ---- batch: 020 ----
mean loss: 78.84
 ---- batch: 030 ----
mean loss: 78.70
 ---- batch: 040 ----
mean loss: 80.41
train mean loss: 78.38
epoch train time: 0:00:00.679524
elapsed time: 0:01:10.128025
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:51:31.793834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 80.40
 ---- batch: 020 ----
mean loss: 75.33
 ---- batch: 030 ----
mean loss: 77.08
 ---- batch: 040 ----
mean loss: 75.94
train mean loss: 77.03
epoch train time: 0:00:00.663126
elapsed time: 0:01:10.791400
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:51:32.457185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 75.33
 ---- batch: 020 ----
mean loss: 78.94
 ---- batch: 030 ----
mean loss: 74.06
 ---- batch: 040 ----
mean loss: 79.41
train mean loss: 76.48
epoch train time: 0:00:00.691196
elapsed time: 0:01:11.482842
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:51:33.148618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.93
 ---- batch: 020 ----
mean loss: 76.04
 ---- batch: 030 ----
mean loss: 76.39
 ---- batch: 040 ----
mean loss: 73.92
train mean loss: 76.25
epoch train time: 0:00:00.696937
elapsed time: 0:01:12.180029
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:51:33.845843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 78.95
 ---- batch: 020 ----
mean loss: 75.76
 ---- batch: 030 ----
mean loss: 75.21
 ---- batch: 040 ----
mean loss: 76.09
train mean loss: 76.59
epoch train time: 0:00:00.686325
elapsed time: 0:01:12.866629
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:51:34.532472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.02
 ---- batch: 020 ----
mean loss: 74.94
 ---- batch: 030 ----
mean loss: 72.46
 ---- batch: 040 ----
mean loss: 76.05
train mean loss: 74.89
epoch train time: 0:00:00.695421
elapsed time: 0:01:13.562351
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:51:35.228149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 74.67
 ---- batch: 020 ----
mean loss: 74.31
 ---- batch: 030 ----
mean loss: 75.39
 ---- batch: 040 ----
mean loss: 70.91
train mean loss: 73.99
epoch train time: 0:00:00.669319
elapsed time: 0:01:14.231957
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:51:35.897768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.98
 ---- batch: 020 ----
mean loss: 73.33
 ---- batch: 030 ----
mean loss: 70.24
 ---- batch: 040 ----
mean loss: 71.94
train mean loss: 72.90
epoch train time: 0:00:00.643943
elapsed time: 0:01:14.876146
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:51:36.541922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 73.07
 ---- batch: 020 ----
mean loss: 71.61
 ---- batch: 030 ----
mean loss: 74.54
 ---- batch: 040 ----
mean loss: 74.02
train mean loss: 73.44
epoch train time: 0:00:00.720799
elapsed time: 0:01:15.597175
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:51:37.262960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 70.57
 ---- batch: 020 ----
mean loss: 74.83
 ---- batch: 030 ----
mean loss: 68.21
 ---- batch: 040 ----
mean loss: 75.07
train mean loss: 72.60
epoch train time: 0:00:00.704890
elapsed time: 0:01:16.302395
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:51:37.968199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.31
 ---- batch: 020 ----
mean loss: 72.82
 ---- batch: 030 ----
mean loss: 71.12
 ---- batch: 040 ----
mean loss: 70.90
train mean loss: 72.18
epoch train time: 0:00:00.699085
elapsed time: 0:01:17.001746
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:51:38.667533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.17
 ---- batch: 020 ----
mean loss: 70.29
 ---- batch: 030 ----
mean loss: 72.64
 ---- batch: 040 ----
mean loss: 71.87
train mean loss: 71.43
epoch train time: 0:00:00.672425
elapsed time: 0:01:17.674408
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:51:39.340200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.58
 ---- batch: 020 ----
mean loss: 71.84
 ---- batch: 030 ----
mean loss: 68.24
 ---- batch: 040 ----
mean loss: 69.77
train mean loss: 70.27
epoch train time: 0:00:00.666283
elapsed time: 0:01:18.340962
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:51:40.006765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.19
 ---- batch: 020 ----
mean loss: 73.04
 ---- batch: 030 ----
mean loss: 68.31
 ---- batch: 040 ----
mean loss: 70.55
train mean loss: 70.02
epoch train time: 0:00:00.670218
elapsed time: 0:01:19.011482
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:51:40.677267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.85
 ---- batch: 020 ----
mean loss: 68.72
 ---- batch: 030 ----
mean loss: 70.81
 ---- batch: 040 ----
mean loss: 69.69
train mean loss: 69.85
epoch train time: 0:00:00.678971
elapsed time: 0:01:19.690728
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:51:41.356536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.61
 ---- batch: 020 ----
mean loss: 72.05
 ---- batch: 030 ----
mean loss: 65.55
 ---- batch: 040 ----
mean loss: 69.31
train mean loss: 69.64
epoch train time: 0:00:00.697346
elapsed time: 0:01:20.388377
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:51:42.054209
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.87
 ---- batch: 020 ----
mean loss: 66.22
 ---- batch: 030 ----
mean loss: 65.56
 ---- batch: 040 ----
mean loss: 68.28
train mean loss: 67.03
epoch train time: 0:00:00.703660
elapsed time: 0:01:21.092314
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:51:42.758116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.49
 ---- batch: 020 ----
mean loss: 68.09
 ---- batch: 030 ----
mean loss: 67.47
 ---- batch: 040 ----
mean loss: 69.84
train mean loss: 68.27
epoch train time: 0:00:00.651722
elapsed time: 0:01:21.744278
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:51:43.410076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.36
 ---- batch: 020 ----
mean loss: 70.22
 ---- batch: 030 ----
mean loss: 66.25
 ---- batch: 040 ----
mean loss: 64.22
train mean loss: 67.17
epoch train time: 0:00:00.656556
elapsed time: 0:01:22.401092
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:51:44.066875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.00
 ---- batch: 020 ----
mean loss: 64.95
 ---- batch: 030 ----
mean loss: 68.50
 ---- batch: 040 ----
mean loss: 67.89
train mean loss: 66.35
epoch train time: 0:00:00.670106
elapsed time: 0:01:23.071482
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:51:44.737265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.19
 ---- batch: 020 ----
mean loss: 64.46
 ---- batch: 030 ----
mean loss: 63.94
 ---- batch: 040 ----
mean loss: 67.11
train mean loss: 66.38
epoch train time: 0:00:00.685503
elapsed time: 0:01:23.757298
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:51:45.423098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.52
 ---- batch: 020 ----
mean loss: 64.69
 ---- batch: 030 ----
mean loss: 66.01
 ---- batch: 040 ----
mean loss: 67.93
train mean loss: 66.00
epoch train time: 0:00:00.691176
elapsed time: 0:01:24.448729
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:51:46.114523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.25
 ---- batch: 020 ----
mean loss: 69.54
 ---- batch: 030 ----
mean loss: 66.53
 ---- batch: 040 ----
mean loss: 63.95
train mean loss: 66.64
epoch train time: 0:00:00.663874
elapsed time: 0:01:25.112870
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:51:46.778692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 62.37
 ---- batch: 020 ----
mean loss: 67.71
 ---- batch: 030 ----
mean loss: 63.37
 ---- batch: 040 ----
mean loss: 68.93
train mean loss: 65.84
epoch train time: 0:00:00.669065
elapsed time: 0:01:25.782194
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:51:47.447975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.07
 ---- batch: 020 ----
mean loss: 66.90
 ---- batch: 030 ----
mean loss: 64.05
 ---- batch: 040 ----
mean loss: 68.33
train mean loss: 65.99
epoch train time: 0:00:00.664917
elapsed time: 0:01:26.447323
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:51:48.113104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.11
 ---- batch: 020 ----
mean loss: 61.63
 ---- batch: 030 ----
mean loss: 64.04
 ---- batch: 040 ----
mean loss: 66.02
train mean loss: 64.20
epoch train time: 0:00:00.688484
elapsed time: 0:01:27.136094
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:51:48.801849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.25
 ---- batch: 020 ----
mean loss: 62.28
 ---- batch: 030 ----
mean loss: 65.53
 ---- batch: 040 ----
mean loss: 63.39
train mean loss: 64.04
epoch train time: 0:00:00.713667
elapsed time: 0:01:27.850033
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:51:49.515822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.07
 ---- batch: 020 ----
mean loss: 61.91
 ---- batch: 030 ----
mean loss: 61.43
 ---- batch: 040 ----
mean loss: 68.66
train mean loss: 63.50
epoch train time: 0:00:00.688081
elapsed time: 0:01:28.538333
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:51:50.204114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.73
 ---- batch: 020 ----
mean loss: 62.37
 ---- batch: 030 ----
mean loss: 65.35
 ---- batch: 040 ----
mean loss: 60.46
train mean loss: 64.17
epoch train time: 0:00:00.685809
elapsed time: 0:01:29.224381
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:51:50.890175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.48
 ---- batch: 020 ----
mean loss: 62.16
 ---- batch: 030 ----
mean loss: 64.54
 ---- batch: 040 ----
mean loss: 63.91
train mean loss: 63.64
epoch train time: 0:00:00.665699
elapsed time: 0:01:29.890318
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:51:51.556114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 65.71
 ---- batch: 020 ----
mean loss: 63.87
 ---- batch: 030 ----
mean loss: 62.29
 ---- batch: 040 ----
mean loss: 61.48
train mean loss: 63.09
epoch train time: 0:00:00.692696
elapsed time: 0:01:30.583292
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:51:52.249083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.67
 ---- batch: 020 ----
mean loss: 64.07
 ---- batch: 030 ----
mean loss: 63.06
 ---- batch: 040 ----
mean loss: 59.54
train mean loss: 62.68
epoch train time: 0:00:00.708897
elapsed time: 0:01:31.292437
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:51:52.958224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.71
 ---- batch: 020 ----
mean loss: 64.79
 ---- batch: 030 ----
mean loss: 63.07
 ---- batch: 040 ----
mean loss: 60.14
train mean loss: 62.51
epoch train time: 0:00:00.697520
elapsed time: 0:01:31.990210
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:51:53.656001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 61.60
 ---- batch: 020 ----
mean loss: 60.71
 ---- batch: 030 ----
mean loss: 64.98
 ---- batch: 040 ----
mean loss: 60.97
train mean loss: 62.42
epoch train time: 0:00:00.668156
elapsed time: 0:01:32.658592
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:51:54.324405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 66.90
 ---- batch: 020 ----
mean loss: 61.68
 ---- batch: 030 ----
mean loss: 61.84
 ---- batch: 040 ----
mean loss: 59.70
train mean loss: 62.35
epoch train time: 0:00:00.672399
elapsed time: 0:01:33.331237
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:51:54.997014
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.85
 ---- batch: 020 ----
mean loss: 60.08
 ---- batch: 030 ----
mean loss: 61.92
 ---- batch: 040 ----
mean loss: 62.45
train mean loss: 60.88
epoch train time: 0:00:00.646924
elapsed time: 0:01:33.978432
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:51:55.644216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.46
 ---- batch: 020 ----
mean loss: 61.43
 ---- batch: 030 ----
mean loss: 59.98
 ---- batch: 040 ----
mean loss: 58.37
train mean loss: 60.69
epoch train time: 0:00:00.704809
elapsed time: 0:01:34.683529
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:51:56.349316
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.43
 ---- batch: 020 ----
mean loss: 60.40
 ---- batch: 030 ----
mean loss: 62.30
 ---- batch: 040 ----
mean loss: 60.42
train mean loss: 60.78
epoch train time: 0:00:00.707152
elapsed time: 0:01:35.390945
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:51:57.056731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.77
 ---- batch: 020 ----
mean loss: 60.30
 ---- batch: 030 ----
mean loss: 62.43
 ---- batch: 040 ----
mean loss: 58.67
train mean loss: 60.37
epoch train time: 0:00:00.692708
elapsed time: 0:01:36.083891
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:51:57.749716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.63
 ---- batch: 020 ----
mean loss: 59.49
 ---- batch: 030 ----
mean loss: 61.48
 ---- batch: 040 ----
mean loss: 60.03
train mean loss: 59.47
epoch train time: 0:00:00.660719
elapsed time: 0:01:36.744872
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:51:58.410652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.99
 ---- batch: 020 ----
mean loss: 61.75
 ---- batch: 030 ----
mean loss: 55.44
 ---- batch: 040 ----
mean loss: 61.61
train mean loss: 59.38
epoch train time: 0:00:00.661389
elapsed time: 0:01:37.406476
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:51:59.072272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.91
 ---- batch: 020 ----
mean loss: 60.33
 ---- batch: 030 ----
mean loss: 60.13
 ---- batch: 040 ----
mean loss: 59.64
train mean loss: 59.77
epoch train time: 0:00:00.677511
elapsed time: 0:01:38.084254
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:51:59.750057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.20
 ---- batch: 020 ----
mean loss: 58.88
 ---- batch: 030 ----
mean loss: 59.60
 ---- batch: 040 ----
mean loss: 56.38
train mean loss: 58.33
epoch train time: 0:00:00.708416
elapsed time: 0:01:38.792923
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:52:00.458707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.51
 ---- batch: 020 ----
mean loss: 56.93
 ---- batch: 030 ----
mean loss: 58.32
 ---- batch: 040 ----
mean loss: 59.12
train mean loss: 58.33
epoch train time: 0:00:00.704423
elapsed time: 0:01:39.497567
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:52:01.163353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.45
 ---- batch: 020 ----
mean loss: 58.05
 ---- batch: 030 ----
mean loss: 61.88
 ---- batch: 040 ----
mean loss: 58.67
train mean loss: 58.44
epoch train time: 0:00:00.698563
elapsed time: 0:01:40.196403
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:52:01.862237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.77
 ---- batch: 020 ----
mean loss: 57.03
 ---- batch: 030 ----
mean loss: 58.08
 ---- batch: 040 ----
mean loss: 57.26
train mean loss: 58.10
epoch train time: 0:00:00.663689
elapsed time: 0:01:40.860463
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:52:02.526224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.64
 ---- batch: 020 ----
mean loss: 56.37
 ---- batch: 030 ----
mean loss: 59.41
 ---- batch: 040 ----
mean loss: 57.90
train mean loss: 58.43
epoch train time: 0:00:00.683360
elapsed time: 0:01:41.544057
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:52:03.209847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.21
 ---- batch: 020 ----
mean loss: 57.69
 ---- batch: 030 ----
mean loss: 56.34
 ---- batch: 040 ----
mean loss: 58.23
train mean loss: 57.04
epoch train time: 0:00:00.685618
elapsed time: 0:01:42.229976
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:52:03.895764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.59
 ---- batch: 020 ----
mean loss: 56.09
 ---- batch: 030 ----
mean loss: 58.79
 ---- batch: 040 ----
mean loss: 53.47
train mean loss: 57.12
epoch train time: 0:00:00.702059
elapsed time: 0:01:42.932285
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:52:04.598075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.70
 ---- batch: 020 ----
mean loss: 58.29
 ---- batch: 030 ----
mean loss: 56.07
 ---- batch: 040 ----
mean loss: 56.73
train mean loss: 57.55
epoch train time: 0:00:00.715560
elapsed time: 0:01:43.648107
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:52:05.313917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.26
 ---- batch: 020 ----
mean loss: 55.84
 ---- batch: 030 ----
mean loss: 55.24
 ---- batch: 040 ----
mean loss: 56.05
train mean loss: 56.64
epoch train time: 0:00:00.697036
elapsed time: 0:01:44.345394
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:52:06.011178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 56.40
 ---- batch: 020 ----
mean loss: 55.21
 ---- batch: 030 ----
mean loss: 56.56
 ---- batch: 040 ----
mean loss: 55.31
train mean loss: 56.35
epoch train time: 0:00:00.664783
elapsed time: 0:01:45.010391
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:52:06.676164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.95
 ---- batch: 020 ----
mean loss: 56.19
 ---- batch: 030 ----
mean loss: 58.22
 ---- batch: 040 ----
mean loss: 55.73
train mean loss: 55.62
epoch train time: 0:00:00.659529
elapsed time: 0:01:45.670115
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:52:07.335902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.43
 ---- batch: 020 ----
mean loss: 57.46
 ---- batch: 030 ----
mean loss: 56.75
 ---- batch: 040 ----
mean loss: 55.24
train mean loss: 56.07
epoch train time: 0:00:00.670284
elapsed time: 0:01:46.340653
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:52:08.006449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 57.69
 ---- batch: 020 ----
mean loss: 54.79
 ---- batch: 030 ----
mean loss: 53.90
 ---- batch: 040 ----
mean loss: 56.65
train mean loss: 55.59
epoch train time: 0:00:00.695067
elapsed time: 0:01:47.035988
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:52:08.701777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.89
 ---- batch: 020 ----
mean loss: 55.74
 ---- batch: 030 ----
mean loss: 55.23
 ---- batch: 040 ----
mean loss: 53.16
train mean loss: 54.94
epoch train time: 0:00:00.723484
elapsed time: 0:01:47.759773
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:52:09.425558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.39
 ---- batch: 020 ----
mean loss: 55.37
 ---- batch: 030 ----
mean loss: 55.23
 ---- batch: 040 ----
mean loss: 55.64
train mean loss: 55.42
epoch train time: 0:00:00.675462
elapsed time: 0:01:48.435481
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:52:10.101258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.86
 ---- batch: 020 ----
mean loss: 60.90
 ---- batch: 030 ----
mean loss: 53.46
 ---- batch: 040 ----
mean loss: 53.03
train mean loss: 54.85
epoch train time: 0:00:00.666917
elapsed time: 0:01:49.102607
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:52:10.768400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.26
 ---- batch: 020 ----
mean loss: 54.49
 ---- batch: 030 ----
mean loss: 53.96
 ---- batch: 040 ----
mean loss: 53.82
train mean loss: 54.61
epoch train time: 0:00:00.653523
elapsed time: 0:01:49.756399
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:52:11.422185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.87
 ---- batch: 020 ----
mean loss: 51.60
 ---- batch: 030 ----
mean loss: 55.08
 ---- batch: 040 ----
mean loss: 56.81
train mean loss: 54.30
epoch train time: 0:00:00.664807
elapsed time: 0:01:50.421457
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:52:12.087247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.38
 ---- batch: 020 ----
mean loss: 54.29
 ---- batch: 030 ----
mean loss: 54.19
 ---- batch: 040 ----
mean loss: 54.31
train mean loss: 53.69
epoch train time: 0:00:00.688790
elapsed time: 0:01:51.110519
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:52:12.776314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.48
 ---- batch: 020 ----
mean loss: 56.65
 ---- batch: 030 ----
mean loss: 54.15
 ---- batch: 040 ----
mean loss: 51.48
train mean loss: 53.26
epoch train time: 0:00:00.681290
elapsed time: 0:01:51.792089
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:52:13.457898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.84
 ---- batch: 020 ----
mean loss: 54.05
 ---- batch: 030 ----
mean loss: 54.11
 ---- batch: 040 ----
mean loss: 54.44
train mean loss: 53.66
epoch train time: 0:00:00.672427
elapsed time: 0:01:52.464791
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:52:14.130584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.45
 ---- batch: 020 ----
mean loss: 54.60
 ---- batch: 030 ----
mean loss: 53.35
 ---- batch: 040 ----
mean loss: 52.60
train mean loss: 53.55
epoch train time: 0:00:00.659118
elapsed time: 0:01:53.124183
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:52:14.789990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.29
 ---- batch: 020 ----
mean loss: 50.17
 ---- batch: 030 ----
mean loss: 52.20
 ---- batch: 040 ----
mean loss: 53.66
train mean loss: 52.58
epoch train time: 0:00:00.679205
elapsed time: 0:01:53.803632
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:52:15.469431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.98
 ---- batch: 020 ----
mean loss: 53.39
 ---- batch: 030 ----
mean loss: 50.78
 ---- batch: 040 ----
mean loss: 53.71
train mean loss: 52.32
epoch train time: 0:00:00.689902
elapsed time: 0:01:54.493782
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:52:16.159571
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.92
 ---- batch: 020 ----
mean loss: 50.42
 ---- batch: 030 ----
mean loss: 53.34
 ---- batch: 040 ----
mean loss: 51.19
train mean loss: 52.50
epoch train time: 0:00:00.706723
elapsed time: 0:01:55.200743
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:52:16.866530
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.20
 ---- batch: 020 ----
mean loss: 55.01
 ---- batch: 030 ----
mean loss: 51.08
 ---- batch: 040 ----
mean loss: 50.68
train mean loss: 51.64
epoch train time: 0:00:00.700602
elapsed time: 0:01:55.901625
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:52:17.567372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.24
 ---- batch: 020 ----
mean loss: 51.13
 ---- batch: 030 ----
mean loss: 52.45
 ---- batch: 040 ----
mean loss: 53.05
train mean loss: 52.22
epoch train time: 0:00:00.680348
elapsed time: 0:01:56.582157
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:52:18.247950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.07
 ---- batch: 020 ----
mean loss: 51.52
 ---- batch: 030 ----
mean loss: 51.08
 ---- batch: 040 ----
mean loss: 53.36
train mean loss: 51.52
epoch train time: 0:00:00.660966
elapsed time: 0:01:57.243377
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:52:18.909159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.91
 ---- batch: 020 ----
mean loss: 51.34
 ---- batch: 030 ----
mean loss: 50.07
 ---- batch: 040 ----
mean loss: 52.43
train mean loss: 51.43
epoch train time: 0:00:00.653242
elapsed time: 0:01:57.896842
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:52:19.562620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.74
 ---- batch: 020 ----
mean loss: 51.25
 ---- batch: 030 ----
mean loss: 50.50
 ---- batch: 040 ----
mean loss: 51.42
train mean loss: 51.50
epoch train time: 0:00:00.678628
elapsed time: 0:01:58.575740
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:52:20.241529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.65
 ---- batch: 020 ----
mean loss: 52.10
 ---- batch: 030 ----
mean loss: 46.98
 ---- batch: 040 ----
mean loss: 50.73
train mean loss: 50.40
epoch train time: 0:00:00.707206
elapsed time: 0:01:59.283219
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:52:20.949004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.28
 ---- batch: 020 ----
mean loss: 51.32
 ---- batch: 030 ----
mean loss: 50.26
 ---- batch: 040 ----
mean loss: 49.93
train mean loss: 50.37
epoch train time: 0:00:00.678548
elapsed time: 0:01:59.962005
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:52:21.627789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.29
 ---- batch: 020 ----
mean loss: 51.48
 ---- batch: 030 ----
mean loss: 52.65
 ---- batch: 040 ----
mean loss: 49.65
train mean loss: 50.47
epoch train time: 0:00:00.677130
elapsed time: 0:02:00.639373
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:52:22.305175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.63
 ---- batch: 020 ----
mean loss: 50.35
 ---- batch: 030 ----
mean loss: 50.49
 ---- batch: 040 ----
mean loss: 50.70
train mean loss: 50.05
epoch train time: 0:00:00.671288
elapsed time: 0:02:01.310941
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:52:22.976738
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.05
 ---- batch: 020 ----
mean loss: 49.55
 ---- batch: 030 ----
mean loss: 49.70
 ---- batch: 040 ----
mean loss: 49.32
train mean loss: 48.73
epoch train time: 0:00:00.660336
elapsed time: 0:02:01.971536
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:52:23.637336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.95
 ---- batch: 020 ----
mean loss: 49.42
 ---- batch: 030 ----
mean loss: 50.61
 ---- batch: 040 ----
mean loss: 48.18
train mean loss: 49.11
epoch train time: 0:00:00.699629
elapsed time: 0:02:02.671428
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:52:24.337213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.60
 ---- batch: 020 ----
mean loss: 48.72
 ---- batch: 030 ----
mean loss: 48.22
 ---- batch: 040 ----
mean loss: 52.90
train mean loss: 49.47
epoch train time: 0:00:00.706328
elapsed time: 0:02:03.378021
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:52:25.043840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.10
 ---- batch: 020 ----
mean loss: 48.65
 ---- batch: 030 ----
mean loss: 48.99
 ---- batch: 040 ----
mean loss: 49.89
train mean loss: 48.86
epoch train time: 0:00:00.698101
elapsed time: 0:02:04.076444
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:52:25.742240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.66
 ---- batch: 020 ----
mean loss: 47.25
 ---- batch: 030 ----
mean loss: 51.17
 ---- batch: 040 ----
mean loss: 51.40
train mean loss: 49.30
epoch train time: 0:00:00.682394
elapsed time: 0:02:04.759082
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:52:26.424869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.79
 ---- batch: 020 ----
mean loss: 48.69
 ---- batch: 030 ----
mean loss: 47.42
 ---- batch: 040 ----
mean loss: 50.96
train mean loss: 48.51
epoch train time: 0:00:00.661586
elapsed time: 0:02:05.420894
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:52:27.086684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.90
 ---- batch: 020 ----
mean loss: 46.53
 ---- batch: 030 ----
mean loss: 48.67
 ---- batch: 040 ----
mean loss: 48.49
train mean loss: 47.46
epoch train time: 0:00:00.651636
elapsed time: 0:02:06.072779
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:52:27.738589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.50
 ---- batch: 020 ----
mean loss: 47.56
 ---- batch: 030 ----
mean loss: 49.85
 ---- batch: 040 ----
mean loss: 47.81
train mean loss: 47.72
epoch train time: 0:00:00.681542
elapsed time: 0:02:06.754601
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:52:28.420410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.48
 ---- batch: 020 ----
mean loss: 45.50
 ---- batch: 030 ----
mean loss: 47.69
 ---- batch: 040 ----
mean loss: 47.37
train mean loss: 47.66
epoch train time: 0:00:00.695429
elapsed time: 0:02:07.450357
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:52:29.116142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.84
 ---- batch: 020 ----
mean loss: 46.67
 ---- batch: 030 ----
mean loss: 47.53
 ---- batch: 040 ----
mean loss: 44.88
train mean loss: 46.80
epoch train time: 0:00:00.678138
elapsed time: 0:02:08.128748
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:52:29.794594
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.67
 ---- batch: 020 ----
mean loss: 46.15
 ---- batch: 030 ----
mean loss: 46.15
 ---- batch: 040 ----
mean loss: 46.58
train mean loss: 47.03
epoch train time: 0:00:00.668726
elapsed time: 0:02:08.797761
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:52:30.463541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.18
 ---- batch: 020 ----
mean loss: 46.63
 ---- batch: 030 ----
mean loss: 49.03
 ---- batch: 040 ----
mean loss: 43.44
train mean loss: 46.58
epoch train time: 0:00:00.649520
elapsed time: 0:02:09.447489
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:52:31.113267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.42
 ---- batch: 020 ----
mean loss: 44.23
 ---- batch: 030 ----
mean loss: 48.55
 ---- batch: 040 ----
mean loss: 47.08
train mean loss: 46.16
epoch train time: 0:00:00.643764
elapsed time: 0:02:10.091482
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:52:31.757289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.71
 ---- batch: 020 ----
mean loss: 45.44
 ---- batch: 030 ----
mean loss: 45.61
 ---- batch: 040 ----
mean loss: 45.66
train mean loss: 46.09
epoch train time: 0:00:00.666436
elapsed time: 0:02:10.758200
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:52:32.424017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.04
 ---- batch: 020 ----
mean loss: 44.13
 ---- batch: 030 ----
mean loss: 47.90
 ---- batch: 040 ----
mean loss: 45.42
train mean loss: 45.89
epoch train time: 0:00:00.699754
elapsed time: 0:02:11.458254
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:52:33.124076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.19
 ---- batch: 020 ----
mean loss: 47.81
 ---- batch: 030 ----
mean loss: 46.18
 ---- batch: 040 ----
mean loss: 48.36
train mean loss: 46.69
epoch train time: 0:00:00.714218
elapsed time: 0:02:12.172782
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:52:33.838576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.39
 ---- batch: 020 ----
mean loss: 48.40
 ---- batch: 030 ----
mean loss: 46.42
 ---- batch: 040 ----
mean loss: 44.65
train mean loss: 46.41
epoch train time: 0:00:00.686714
elapsed time: 0:02:12.859767
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:52:34.525512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.63
 ---- batch: 020 ----
mean loss: 45.46
 ---- batch: 030 ----
mean loss: 45.37
 ---- batch: 040 ----
mean loss: 43.39
train mean loss: 44.83
epoch train time: 0:00:00.680129
elapsed time: 0:02:13.540071
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:52:35.205850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.10
 ---- batch: 020 ----
mean loss: 45.70
 ---- batch: 030 ----
mean loss: 45.18
 ---- batch: 040 ----
mean loss: 43.42
train mean loss: 44.82
epoch train time: 0:00:00.664152
elapsed time: 0:02:14.204498
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:52:35.870320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.53
 ---- batch: 020 ----
mean loss: 44.49
 ---- batch: 030 ----
mean loss: 44.80
 ---- batch: 040 ----
mean loss: 47.71
train mean loss: 44.56
epoch train time: 0:00:00.687076
elapsed time: 0:02:14.891861
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:52:36.557648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.59
 ---- batch: 020 ----
mean loss: 42.75
 ---- batch: 030 ----
mean loss: 42.94
 ---- batch: 040 ----
mean loss: 46.23
train mean loss: 44.44
epoch train time: 0:00:00.701465
elapsed time: 0:02:15.593571
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:52:37.259369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.93
 ---- batch: 020 ----
mean loss: 43.87
 ---- batch: 030 ----
mean loss: 42.94
 ---- batch: 040 ----
mean loss: 43.89
train mean loss: 44.16
epoch train time: 0:00:00.697795
elapsed time: 0:02:16.291629
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:52:37.957414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.17
 ---- batch: 020 ----
mean loss: 45.59
 ---- batch: 030 ----
mean loss: 44.81
 ---- batch: 040 ----
mean loss: 41.33
train mean loss: 43.67
epoch train time: 0:00:00.668521
elapsed time: 0:02:16.960387
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:52:38.626167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.54
 ---- batch: 020 ----
mean loss: 44.36
 ---- batch: 030 ----
mean loss: 41.67
 ---- batch: 040 ----
mean loss: 43.87
train mean loss: 43.09
epoch train time: 0:00:00.647801
elapsed time: 0:02:17.608390
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:52:39.274164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.01
 ---- batch: 020 ----
mean loss: 41.89
 ---- batch: 030 ----
mean loss: 41.96
 ---- batch: 040 ----
mean loss: 44.83
train mean loss: 43.60
epoch train time: 0:00:00.656888
elapsed time: 0:02:18.265499
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:52:39.931293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.21
 ---- batch: 020 ----
mean loss: 43.83
 ---- batch: 030 ----
mean loss: 40.88
 ---- batch: 040 ----
mean loss: 44.05
train mean loss: 43.10
epoch train time: 0:00:00.677582
elapsed time: 0:02:18.943344
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:52:40.609138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.54
 ---- batch: 020 ----
mean loss: 43.91
 ---- batch: 030 ----
mean loss: 42.40
 ---- batch: 040 ----
mean loss: 44.05
train mean loss: 43.08
epoch train time: 0:00:00.693901
elapsed time: 0:02:19.637559
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:52:41.303375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.09
 ---- batch: 020 ----
mean loss: 44.47
 ---- batch: 030 ----
mean loss: 41.05
 ---- batch: 040 ----
mean loss: 43.16
train mean loss: 42.26
epoch train time: 0:00:00.687718
elapsed time: 0:02:20.325532
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:52:41.991321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.72
 ---- batch: 020 ----
mean loss: 41.18
 ---- batch: 030 ----
mean loss: 45.39
 ---- batch: 040 ----
mean loss: 41.53
train mean loss: 42.59
epoch train time: 0:00:00.663629
elapsed time: 0:02:20.989418
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:52:42.655192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.18
 ---- batch: 020 ----
mean loss: 44.06
 ---- batch: 030 ----
mean loss: 44.47
 ---- batch: 040 ----
mean loss: 37.51
train mean loss: 42.59
epoch train time: 0:00:00.662231
elapsed time: 0:02:21.651881
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:52:43.317675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.62
 ---- batch: 020 ----
mean loss: 41.55
 ---- batch: 030 ----
mean loss: 41.57
 ---- batch: 040 ----
mean loss: 42.58
train mean loss: 41.71
epoch train time: 0:00:00.667289
elapsed time: 0:02:22.319417
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:52:43.985198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.38
 ---- batch: 020 ----
mean loss: 40.73
 ---- batch: 030 ----
mean loss: 41.61
 ---- batch: 040 ----
mean loss: 43.64
train mean loss: 41.34
epoch train time: 0:00:00.682680
elapsed time: 0:02:23.002375
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:52:44.668175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.99
 ---- batch: 020 ----
mean loss: 40.51
 ---- batch: 030 ----
mean loss: 43.61
 ---- batch: 040 ----
mean loss: 41.05
train mean loss: 41.07
epoch train time: 0:00:00.681620
elapsed time: 0:02:23.684264
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:52:45.350042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.84
 ---- batch: 020 ----
mean loss: 42.08
 ---- batch: 030 ----
mean loss: 39.28
 ---- batch: 040 ----
mean loss: 41.75
train mean loss: 40.71
epoch train time: 0:00:00.702821
elapsed time: 0:02:24.387355
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:52:46.053137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.18
 ---- batch: 020 ----
mean loss: 40.19
 ---- batch: 030 ----
mean loss: 41.92
 ---- batch: 040 ----
mean loss: 40.89
train mean loss: 40.64
epoch train time: 0:00:00.682182
elapsed time: 0:02:25.069758
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:52:46.735536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.77
 ---- batch: 020 ----
mean loss: 40.00
 ---- batch: 030 ----
mean loss: 40.44
 ---- batch: 040 ----
mean loss: 40.11
train mean loss: 40.58
epoch train time: 0:00:00.660547
elapsed time: 0:02:25.730518
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:52:47.396317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.39
 ---- batch: 020 ----
mean loss: 40.35
 ---- batch: 030 ----
mean loss: 41.31
 ---- batch: 040 ----
mean loss: 40.28
train mean loss: 40.77
epoch train time: 0:00:00.670546
elapsed time: 0:02:26.401402
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:52:48.067191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.19
 ---- batch: 020 ----
mean loss: 41.15
 ---- batch: 030 ----
mean loss: 39.67
 ---- batch: 040 ----
mean loss: 38.19
train mean loss: 39.92
epoch train time: 0:00:00.665375
elapsed time: 0:02:27.067013
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:52:48.732809
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.56
 ---- batch: 020 ----
mean loss: 38.54
 ---- batch: 030 ----
mean loss: 41.53
 ---- batch: 040 ----
mean loss: 40.12
train mean loss: 40.18
epoch train time: 0:00:00.681571
elapsed time: 0:02:27.748835
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:52:49.414623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.46
 ---- batch: 020 ----
mean loss: 40.29
 ---- batch: 030 ----
mean loss: 38.73
 ---- batch: 040 ----
mean loss: 38.53
train mean loss: 39.16
epoch train time: 0:00:00.681840
elapsed time: 0:02:28.430920
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:52:50.096705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.55
 ---- batch: 020 ----
mean loss: 39.58
 ---- batch: 030 ----
mean loss: 39.35
 ---- batch: 040 ----
mean loss: 39.39
train mean loss: 39.30
epoch train time: 0:00:00.670018
elapsed time: 0:02:29.101165
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:52:50.766947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.88
 ---- batch: 020 ----
mean loss: 38.06
 ---- batch: 030 ----
mean loss: 38.73
 ---- batch: 040 ----
mean loss: 39.24
train mean loss: 38.33
epoch train time: 0:00:00.665006
elapsed time: 0:02:29.766382
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:52:51.432162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.06
 ---- batch: 020 ----
mean loss: 37.22
 ---- batch: 030 ----
mean loss: 39.86
 ---- batch: 040 ----
mean loss: 39.09
train mean loss: 38.29
epoch train time: 0:00:00.668692
elapsed time: 0:02:30.435281
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:52:52.101059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.97
 ---- batch: 020 ----
mean loss: 38.49
 ---- batch: 030 ----
mean loss: 39.64
 ---- batch: 040 ----
mean loss: 40.13
train mean loss: 39.09
epoch train time: 0:00:00.685345
elapsed time: 0:02:31.120872
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:52:52.786678
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.82
 ---- batch: 020 ----
mean loss: 38.09
 ---- batch: 030 ----
mean loss: 37.49
 ---- batch: 040 ----
mean loss: 38.14
train mean loss: 37.29
epoch train time: 0:00:00.690649
elapsed time: 0:02:31.811817
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:52:53.477564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.78
 ---- batch: 020 ----
mean loss: 38.58
 ---- batch: 030 ----
mean loss: 37.19
 ---- batch: 040 ----
mean loss: 37.21
train mean loss: 37.24
epoch train time: 0:00:00.692676
elapsed time: 0:02:32.504687
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:52:54.170496
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.01
 ---- batch: 020 ----
mean loss: 37.92
 ---- batch: 030 ----
mean loss: 36.16
 ---- batch: 040 ----
mean loss: 35.90
train mean loss: 36.82
epoch train time: 0:00:00.677631
elapsed time: 0:02:33.182572
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:52:54.848351
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.51
 ---- batch: 020 ----
mean loss: 37.49
 ---- batch: 030 ----
mean loss: 37.61
 ---- batch: 040 ----
mean loss: 37.83
train mean loss: 37.13
epoch train time: 0:00:00.665661
elapsed time: 0:02:33.848468
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:52:55.514250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.38
 ---- batch: 020 ----
mean loss: 36.64
 ---- batch: 030 ----
mean loss: 36.81
 ---- batch: 040 ----
mean loss: 37.59
train mean loss: 37.01
epoch train time: 0:00:00.675564
elapsed time: 0:02:34.524275
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:52:56.190046
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.35
 ---- batch: 020 ----
mean loss: 35.80
 ---- batch: 030 ----
mean loss: 36.73
 ---- batch: 040 ----
mean loss: 36.22
train mean loss: 36.68
epoch train time: 0:00:00.711130
elapsed time: 0:02:35.235654
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:52:56.901455
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.91
 ---- batch: 020 ----
mean loss: 35.32
 ---- batch: 030 ----
mean loss: 36.61
 ---- batch: 040 ----
mean loss: 37.07
train mean loss: 36.75
epoch train time: 0:00:00.692071
elapsed time: 0:02:35.928051
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:52:57.593844
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.58
 ---- batch: 020 ----
mean loss: 37.00
 ---- batch: 030 ----
mean loss: 34.55
 ---- batch: 040 ----
mean loss: 36.41
train mean loss: 36.84
epoch train time: 0:00:00.694807
elapsed time: 0:02:36.623098
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:52:58.288881
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.87
 ---- batch: 020 ----
mean loss: 37.88
 ---- batch: 030 ----
mean loss: 35.14
 ---- batch: 040 ----
mean loss: 38.88
train mean loss: 37.07
epoch train time: 0:00:00.670826
elapsed time: 0:02:37.294161
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:52:58.959950
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.41
 ---- batch: 020 ----
mean loss: 37.40
 ---- batch: 030 ----
mean loss: 37.10
 ---- batch: 040 ----
mean loss: 35.79
train mean loss: 36.89
epoch train time: 0:00:00.659074
elapsed time: 0:02:37.953469
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:52:59.619254
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.89
 ---- batch: 020 ----
mean loss: 37.24
 ---- batch: 030 ----
mean loss: 35.19
 ---- batch: 040 ----
mean loss: 37.05
train mean loss: 36.76
epoch train time: 0:00:00.671293
elapsed time: 0:02:38.624975
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:53:00.290755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.32
 ---- batch: 020 ----
mean loss: 36.73
 ---- batch: 030 ----
mean loss: 35.37
 ---- batch: 040 ----
mean loss: 36.67
train mean loss: 36.73
epoch train time: 0:00:00.704458
elapsed time: 0:02:39.329679
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:53:00.995466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.17
 ---- batch: 020 ----
mean loss: 34.60
 ---- batch: 030 ----
mean loss: 36.14
 ---- batch: 040 ----
mean loss: 37.44
train mean loss: 36.49
epoch train time: 0:00:00.695015
elapsed time: 0:02:40.024954
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:53:01.690739
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.08
 ---- batch: 020 ----
mean loss: 36.29
 ---- batch: 030 ----
mean loss: 36.84
 ---- batch: 040 ----
mean loss: 36.86
train mean loss: 36.65
epoch train time: 0:00:00.683526
elapsed time: 0:02:40.708767
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:53:02.374569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.35
 ---- batch: 020 ----
mean loss: 35.19
 ---- batch: 030 ----
mean loss: 37.16
 ---- batch: 040 ----
mean loss: 38.28
train mean loss: 36.67
epoch train time: 0:00:00.673754
elapsed time: 0:02:41.382783
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:53:03.048574
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 38.91
 ---- batch: 020 ----
mean loss: 36.96
 ---- batch: 030 ----
mean loss: 35.78
 ---- batch: 040 ----
mean loss: 35.45
train mean loss: 36.82
epoch train time: 0:00:00.648955
elapsed time: 0:02:42.031997
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:53:03.697811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.36
 ---- batch: 020 ----
mean loss: 35.98
 ---- batch: 030 ----
mean loss: 37.94
 ---- batch: 040 ----
mean loss: 35.32
train mean loss: 36.72
epoch train time: 0:00:00.661105
elapsed time: 0:02:42.693354
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:53:04.359132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.86
 ---- batch: 020 ----
mean loss: 38.71
 ---- batch: 030 ----
mean loss: 35.70
 ---- batch: 040 ----
mean loss: 36.95
train mean loss: 36.77
epoch train time: 0:00:00.707955
elapsed time: 0:02:43.401557
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:53:05.067384
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.38
 ---- batch: 020 ----
mean loss: 37.82
 ---- batch: 030 ----
mean loss: 35.92
 ---- batch: 040 ----
mean loss: 34.43
train mean loss: 36.30
epoch train time: 0:00:00.694197
elapsed time: 0:02:44.096029
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:53:05.761813
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.24
 ---- batch: 020 ----
mean loss: 38.41
 ---- batch: 030 ----
mean loss: 36.91
 ---- batch: 040 ----
mean loss: 35.37
train mean loss: 36.34
epoch train time: 0:00:00.688327
elapsed time: 0:02:44.784588
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:53:06.450391
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.61
 ---- batch: 020 ----
mean loss: 35.26
 ---- batch: 030 ----
mean loss: 36.96
 ---- batch: 040 ----
mean loss: 37.44
train mean loss: 36.25
epoch train time: 0:00:00.673504
elapsed time: 0:02:45.458343
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:53:07.124128
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.40
 ---- batch: 020 ----
mean loss: 38.18
 ---- batch: 030 ----
mean loss: 35.38
 ---- batch: 040 ----
mean loss: 36.44
train mean loss: 36.39
epoch train time: 0:00:00.679640
elapsed time: 0:02:46.138221
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:53:07.804024
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.54
 ---- batch: 020 ----
mean loss: 34.75
 ---- batch: 030 ----
mean loss: 36.76
 ---- batch: 040 ----
mean loss: 37.88
train mean loss: 36.31
epoch train time: 0:00:00.663633
elapsed time: 0:02:46.802094
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:53:08.467897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.73
 ---- batch: 020 ----
mean loss: 37.16
 ---- batch: 030 ----
mean loss: 35.27
 ---- batch: 040 ----
mean loss: 36.38
train mean loss: 36.48
epoch train time: 0:00:00.703732
elapsed time: 0:02:47.506098
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:53:09.171897
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.61
 ---- batch: 020 ----
mean loss: 36.93
 ---- batch: 030 ----
mean loss: 37.24
 ---- batch: 040 ----
mean loss: 35.69
train mean loss: 36.42
epoch train time: 0:00:00.707474
elapsed time: 0:02:48.213864
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:53:09.879667
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.30
 ---- batch: 020 ----
mean loss: 36.71
 ---- batch: 030 ----
mean loss: 36.01
 ---- batch: 040 ----
mean loss: 36.56
train mean loss: 36.21
epoch train time: 0:00:00.703783
elapsed time: 0:02:48.917915
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:53:10.583704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.66
 ---- batch: 020 ----
mean loss: 36.12
 ---- batch: 030 ----
mean loss: 37.28
 ---- batch: 040 ----
mean loss: 36.07
train mean loss: 36.13
epoch train time: 0:00:00.679783
elapsed time: 0:02:49.597950
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:53:11.263730
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.36
 ---- batch: 020 ----
mean loss: 34.90
 ---- batch: 030 ----
mean loss: 38.36
 ---- batch: 040 ----
mean loss: 36.00
train mean loss: 36.29
epoch train time: 0:00:00.672584
elapsed time: 0:02:50.270762
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:53:11.936545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.52
 ---- batch: 020 ----
mean loss: 36.74
 ---- batch: 030 ----
mean loss: 35.75
 ---- batch: 040 ----
mean loss: 37.60
train mean loss: 36.21
epoch train time: 0:00:00.676614
elapsed time: 0:02:50.947646
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:53:12.613433
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.84
 ---- batch: 020 ----
mean loss: 36.87
 ---- batch: 030 ----
mean loss: 35.73
 ---- batch: 040 ----
mean loss: 34.85
train mean loss: 36.11
epoch train time: 0:00:00.693183
elapsed time: 0:02:51.641073
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:53:13.306860
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.38
 ---- batch: 020 ----
mean loss: 33.28
 ---- batch: 030 ----
mean loss: 36.84
 ---- batch: 040 ----
mean loss: 37.33
train mean loss: 36.19
epoch train time: 0:00:00.682134
elapsed time: 0:02:52.323441
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:53:13.989237
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.35
 ---- batch: 020 ----
mean loss: 36.85
 ---- batch: 030 ----
mean loss: 36.58
 ---- batch: 040 ----
mean loss: 34.76
train mean loss: 36.20
epoch train time: 0:00:00.676559
elapsed time: 0:02:53.000233
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:53:14.666013
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.42
 ---- batch: 020 ----
mean loss: 37.28
 ---- batch: 030 ----
mean loss: 35.48
 ---- batch: 040 ----
mean loss: 35.48
train mean loss: 36.16
epoch train time: 0:00:00.685705
elapsed time: 0:02:53.686180
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:53:15.351923
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.30
 ---- batch: 020 ----
mean loss: 36.08
 ---- batch: 030 ----
mean loss: 36.98
 ---- batch: 040 ----
mean loss: 35.33
train mean loss: 36.09
epoch train time: 0:00:00.666175
elapsed time: 0:02:54.352580
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:53:16.018363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.38
 ---- batch: 020 ----
mean loss: 35.02
 ---- batch: 030 ----
mean loss: 35.40
 ---- batch: 040 ----
mean loss: 36.15
train mean loss: 35.94
epoch train time: 0:00:00.667510
elapsed time: 0:02:55.020333
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:53:16.686124
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.49
 ---- batch: 020 ----
mean loss: 36.40
 ---- batch: 030 ----
mean loss: 36.89
 ---- batch: 040 ----
mean loss: 35.74
train mean loss: 36.07
epoch train time: 0:00:00.702620
elapsed time: 0:02:55.723210
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:53:17.388990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.31
 ---- batch: 020 ----
mean loss: 33.64
 ---- batch: 030 ----
mean loss: 36.85
 ---- batch: 040 ----
mean loss: 35.81
train mean loss: 35.90
epoch train time: 0:00:00.693246
elapsed time: 0:02:56.416737
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:53:18.082521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.98
 ---- batch: 020 ----
mean loss: 36.77
 ---- batch: 030 ----
mean loss: 35.81
 ---- batch: 040 ----
mean loss: 36.19
train mean loss: 35.86
epoch train time: 0:00:00.705413
elapsed time: 0:02:57.122427
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:53:18.788243
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.08
 ---- batch: 020 ----
mean loss: 37.16
 ---- batch: 030 ----
mean loss: 37.38
 ---- batch: 040 ----
mean loss: 35.88
train mean loss: 35.95
epoch train time: 0:00:00.671887
elapsed time: 0:02:57.794562
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:53:19.460342
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.36
 ---- batch: 020 ----
mean loss: 35.98
 ---- batch: 030 ----
mean loss: 33.61
 ---- batch: 040 ----
mean loss: 36.05
train mean loss: 35.75
epoch train time: 0:00:00.685107
elapsed time: 0:02:58.479897
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:53:20.145680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.89
 ---- batch: 020 ----
mean loss: 34.70
 ---- batch: 030 ----
mean loss: 35.43
 ---- batch: 040 ----
mean loss: 36.37
train mean loss: 35.66
epoch train time: 0:00:00.667606
elapsed time: 0:02:59.147785
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:53:20.813571
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 37.86
 ---- batch: 020 ----
mean loss: 35.65
 ---- batch: 030 ----
mean loss: 34.81
 ---- batch: 040 ----
mean loss: 35.86
train mean loss: 35.73
epoch train time: 0:00:00.685536
elapsed time: 0:02:59.833558
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:53:21.499353
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.62
 ---- batch: 020 ----
mean loss: 33.82
 ---- batch: 030 ----
mean loss: 36.02
 ---- batch: 040 ----
mean loss: 36.35
train mean loss: 35.84
epoch train time: 0:00:00.688369
elapsed time: 0:03:00.522196
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:53:22.187994
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.97
 ---- batch: 020 ----
mean loss: 35.94
 ---- batch: 030 ----
mean loss: 36.78
 ---- batch: 040 ----
mean loss: 35.28
train mean loss: 35.53
epoch train time: 0:00:00.693309
elapsed time: 0:03:01.215797
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:53:22.881587
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.95
 ---- batch: 020 ----
mean loss: 36.92
 ---- batch: 030 ----
mean loss: 36.40
 ---- batch: 040 ----
mean loss: 36.58
train mean loss: 35.87
epoch train time: 0:00:00.645502
elapsed time: 0:03:01.861509
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:53:23.527284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.24
 ---- batch: 020 ----
mean loss: 34.27
 ---- batch: 030 ----
mean loss: 37.52
 ---- batch: 040 ----
mean loss: 37.35
train mean loss: 35.80
epoch train time: 0:00:00.657804
elapsed time: 0:03:02.519512
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:53:24.185303
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.29
 ---- batch: 020 ----
mean loss: 36.48
 ---- batch: 030 ----
mean loss: 35.20
 ---- batch: 040 ----
mean loss: 35.78
train mean loss: 35.45
epoch train time: 0:00:00.660667
elapsed time: 0:03:03.180414
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:53:24.846200
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.67
 ---- batch: 020 ----
mean loss: 35.53
 ---- batch: 030 ----
mean loss: 35.58
 ---- batch: 040 ----
mean loss: 34.25
train mean loss: 35.76
epoch train time: 0:00:00.696048
elapsed time: 0:03:03.876717
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:53:25.542507
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.48
 ---- batch: 020 ----
mean loss: 36.48
 ---- batch: 030 ----
mean loss: 34.96
 ---- batch: 040 ----
mean loss: 34.83
train mean loss: 35.63
epoch train time: 0:00:00.689148
elapsed time: 0:03:04.573431
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_dense3/bayesian_dense3_1.00/bayesian_dense3_1.00_5/checkpoint.pth.tar
**** end time: 2019-09-20 21:53:26.239157 ****
