Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_3', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31610
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:43:30.387589 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:43:30.392768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4232.64
 ---- batch: 020 ----
mean loss: 4066.63
 ---- batch: 030 ----
mean loss: 4119.86
train mean loss: 4135.33
epoch train time: 0:00:12.800556
elapsed time: 0:00:12.807165
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:43:43.194794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4069.20
 ---- batch: 020 ----
mean loss: 3995.16
 ---- batch: 030 ----
mean loss: 3981.90
train mean loss: 4011.76
epoch train time: 0:00:00.181067
elapsed time: 0:00:12.988362
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:43:43.376016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3910.66
 ---- batch: 020 ----
mean loss: 3844.51
 ---- batch: 030 ----
mean loss: 3801.11
train mean loss: 3832.07
epoch train time: 0:00:00.177446
elapsed time: 0:00:13.165988
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:43:43.553653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3661.91
 ---- batch: 020 ----
mean loss: 3566.28
 ---- batch: 030 ----
mean loss: 3530.75
train mean loss: 3576.38
epoch train time: 0:00:00.178123
elapsed time: 0:00:13.344303
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:43:43.731975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3343.85
 ---- batch: 020 ----
mean loss: 3258.73
 ---- batch: 030 ----
mean loss: 3303.61
train mean loss: 3283.28
epoch train time: 0:00:00.181862
elapsed time: 0:00:13.526351
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:43:43.913994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3102.56
 ---- batch: 020 ----
mean loss: 3010.65
 ---- batch: 030 ----
mean loss: 2976.01
train mean loss: 3013.82
epoch train time: 0:00:00.179869
elapsed time: 0:00:13.706365
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:43:44.094004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2877.11
 ---- batch: 020 ----
mean loss: 2783.29
 ---- batch: 030 ----
mean loss: 2714.50
train mean loss: 2776.58
epoch train time: 0:00:00.174046
elapsed time: 0:00:13.880549
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:43:44.268190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2636.70
 ---- batch: 020 ----
mean loss: 2602.25
 ---- batch: 030 ----
mean loss: 2500.49
train mean loss: 2567.27
epoch train time: 0:00:00.173263
elapsed time: 0:00:14.053956
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:43:44.441634
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2441.85
 ---- batch: 020 ----
mean loss: 2393.80
 ---- batch: 030 ----
mean loss: 2351.98
train mean loss: 2383.94
epoch train time: 0:00:00.170714
elapsed time: 0:00:14.224881
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:43:44.612540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2289.50
 ---- batch: 020 ----
mean loss: 2221.19
 ---- batch: 030 ----
mean loss: 2188.76
train mean loss: 2218.49
epoch train time: 0:00:00.174533
elapsed time: 0:00:14.399593
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:43:44.787242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2107.91
 ---- batch: 020 ----
mean loss: 2093.58
 ---- batch: 030 ----
mean loss: 2031.74
train mean loss: 2066.47
epoch train time: 0:00:00.184391
elapsed time: 0:00:14.584142
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:43:44.971783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1970.89
 ---- batch: 020 ----
mean loss: 1951.79
 ---- batch: 030 ----
mean loss: 1879.97
train mean loss: 1931.47
epoch train time: 0:00:00.178542
elapsed time: 0:00:14.762826
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:43:45.150466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1856.97
 ---- batch: 020 ----
mean loss: 1800.82
 ---- batch: 030 ----
mean loss: 1798.34
train mean loss: 1804.45
epoch train time: 0:00:00.176481
elapsed time: 0:00:14.939446
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:43:45.327084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1759.41
 ---- batch: 020 ----
mean loss: 1704.46
 ---- batch: 030 ----
mean loss: 1644.82
train mean loss: 1688.57
epoch train time: 0:00:00.174358
elapsed time: 0:00:15.113943
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:43:45.501645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1614.81
 ---- batch: 020 ----
mean loss: 1593.18
 ---- batch: 030 ----
mean loss: 1562.87
train mean loss: 1583.35
epoch train time: 0:00:00.172503
elapsed time: 0:00:15.286692
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:43:45.674333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1511.53
 ---- batch: 020 ----
mean loss: 1485.28
 ---- batch: 030 ----
mean loss: 1473.10
train mean loss: 1484.55
epoch train time: 0:00:00.174497
elapsed time: 0:00:15.461369
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:43:45.849011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1429.01
 ---- batch: 020 ----
mean loss: 1393.20
 ---- batch: 030 ----
mean loss: 1374.97
train mean loss: 1393.33
epoch train time: 0:00:00.179807
elapsed time: 0:00:15.641334
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:43:46.028972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1341.84
 ---- batch: 020 ----
mean loss: 1318.29
 ---- batch: 030 ----
mean loss: 1274.20
train mean loss: 1310.70
epoch train time: 0:00:00.174629
elapsed time: 0:00:15.816101
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:43:46.203741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1264.89
 ---- batch: 020 ----
mean loss: 1243.06
 ---- batch: 030 ----
mean loss: 1209.54
train mean loss: 1233.12
epoch train time: 0:00:00.175007
elapsed time: 0:00:15.991247
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:43:46.378884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1191.46
 ---- batch: 020 ----
mean loss: 1162.77
 ---- batch: 030 ----
mean loss: 1143.85
train mean loss: 1161.55
epoch train time: 0:00:00.175414
elapsed time: 0:00:16.166796
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:43:46.554434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1107.04
 ---- batch: 020 ----
mean loss: 1100.09
 ---- batch: 030 ----
mean loss: 1081.50
train mean loss: 1096.84
epoch train time: 0:00:00.175038
elapsed time: 0:00:16.341991
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:43:46.729682
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1049.23
 ---- batch: 020 ----
mean loss: 1060.84
 ---- batch: 030 ----
mean loss: 1024.49
train mean loss: 1036.05
epoch train time: 0:00:00.182121
elapsed time: 0:00:16.524304
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:43:46.911950
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1008.64
 ---- batch: 020 ----
mean loss: 983.09
 ---- batch: 030 ----
mean loss: 972.38
train mean loss: 980.29
epoch train time: 0:00:00.177634
elapsed time: 0:00:16.702107
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:43:47.089762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 935.48
 ---- batch: 020 ----
mean loss: 945.54
 ---- batch: 030 ----
mean loss: 928.96
train mean loss: 928.43
epoch train time: 0:00:00.177904
elapsed time: 0:00:16.880166
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:43:47.267823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 905.80
 ---- batch: 020 ----
mean loss: 897.55
 ---- batch: 030 ----
mean loss: 864.35
train mean loss: 881.43
epoch train time: 0:00:00.176165
elapsed time: 0:00:17.056524
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:43:47.444164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.01
 ---- batch: 020 ----
mean loss: 856.54
 ---- batch: 030 ----
mean loss: 834.02
train mean loss: 838.95
epoch train time: 0:00:00.172415
elapsed time: 0:00:17.229081
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:43:47.616718
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 815.59
 ---- batch: 020 ----
mean loss: 805.80
 ---- batch: 030 ----
mean loss: 775.82
train mean loss: 799.69
epoch train time: 0:00:00.181573
elapsed time: 0:00:17.410806
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:43:47.798459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 778.69
 ---- batch: 020 ----
mean loss: 765.68
 ---- batch: 030 ----
mean loss: 746.86
train mean loss: 762.02
epoch train time: 0:00:00.180596
elapsed time: 0:00:17.591601
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:43:47.979239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.09
 ---- batch: 020 ----
mean loss: 745.55
 ---- batch: 030 ----
mean loss: 720.47
train mean loss: 727.92
epoch train time: 0:00:00.176343
elapsed time: 0:00:17.768082
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:43:48.155722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.02
 ---- batch: 020 ----
mean loss: 697.05
 ---- batch: 030 ----
mean loss: 691.11
train mean loss: 697.82
epoch train time: 0:00:00.170695
elapsed time: 0:00:17.938916
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:43:48.326568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.94
 ---- batch: 020 ----
mean loss: 662.76
 ---- batch: 030 ----
mean loss: 678.19
train mean loss: 669.79
epoch train time: 0:00:00.173022
elapsed time: 0:00:18.112089
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:43:48.499741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 653.93
 ---- batch: 020 ----
mean loss: 649.58
 ---- batch: 030 ----
mean loss: 634.16
train mean loss: 642.76
epoch train time: 0:00:00.170554
elapsed time: 0:00:18.282793
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:43:48.670431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 630.86
 ---- batch: 020 ----
mean loss: 617.97
 ---- batch: 030 ----
mean loss: 605.56
train mean loss: 619.48
epoch train time: 0:00:00.184268
elapsed time: 0:00:18.467202
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:43:48.854858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.47
 ---- batch: 020 ----
mean loss: 606.61
 ---- batch: 030 ----
mean loss: 587.56
train mean loss: 597.86
epoch train time: 0:00:00.180391
elapsed time: 0:00:18.647754
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:43:49.035394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 579.39
 ---- batch: 020 ----
mean loss: 585.84
 ---- batch: 030 ----
mean loss: 571.52
train mean loss: 577.79
epoch train time: 0:00:00.181654
elapsed time: 0:00:18.829563
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:43:49.217214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.17
 ---- batch: 020 ----
mean loss: 557.34
 ---- batch: 030 ----
mean loss: 565.58
train mean loss: 559.98
epoch train time: 0:00:00.176928
elapsed time: 0:00:19.006649
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:43:49.394307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.09
 ---- batch: 020 ----
mean loss: 545.56
 ---- batch: 030 ----
mean loss: 539.22
train mean loss: 542.29
epoch train time: 0:00:00.183653
elapsed time: 0:00:19.190464
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:43:49.578102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 534.10
 ---- batch: 020 ----
mean loss: 523.66
 ---- batch: 030 ----
mean loss: 528.01
train mean loss: 527.16
epoch train time: 0:00:00.179481
elapsed time: 0:00:19.370106
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:43:49.757747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.06
 ---- batch: 020 ----
mean loss: 514.19
 ---- batch: 030 ----
mean loss: 508.19
train mean loss: 513.47
epoch train time: 0:00:00.187451
elapsed time: 0:00:19.557711
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:43:49.945349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.16
 ---- batch: 020 ----
mean loss: 499.76
 ---- batch: 030 ----
mean loss: 500.12
train mean loss: 500.37
epoch train time: 0:00:00.177136
elapsed time: 0:00:19.735022
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:43:50.122661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.20
 ---- batch: 020 ----
mean loss: 485.78
 ---- batch: 030 ----
mean loss: 486.60
train mean loss: 488.48
epoch train time: 0:00:00.173422
elapsed time: 0:00:19.908581
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:43:50.296219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.73
 ---- batch: 020 ----
mean loss: 471.07
 ---- batch: 030 ----
mean loss: 478.62
train mean loss: 477.38
epoch train time: 0:00:00.173029
elapsed time: 0:00:20.081761
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:43:50.469398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.02
 ---- batch: 020 ----
mean loss: 465.61
 ---- batch: 030 ----
mean loss: 462.49
train mean loss: 467.14
epoch train time: 0:00:00.171184
elapsed time: 0:00:20.253080
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:43:50.640717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.38
 ---- batch: 020 ----
mean loss: 465.23
 ---- batch: 030 ----
mean loss: 457.49
train mean loss: 458.23
epoch train time: 0:00:00.175808
elapsed time: 0:00:20.429029
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:43:50.816670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.82
 ---- batch: 020 ----
mean loss: 445.14
 ---- batch: 030 ----
mean loss: 452.35
train mean loss: 449.42
epoch train time: 0:00:00.182006
elapsed time: 0:00:20.611177
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:43:50.998833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.82
 ---- batch: 020 ----
mean loss: 440.12
 ---- batch: 030 ----
mean loss: 437.64
train mean loss: 441.85
epoch train time: 0:00:00.179750
elapsed time: 0:00:20.791097
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:43:51.178736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.13
 ---- batch: 020 ----
mean loss: 435.57
 ---- batch: 030 ----
mean loss: 433.69
train mean loss: 434.02
epoch train time: 0:00:00.174636
elapsed time: 0:00:20.965873
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:43:51.353511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.14
 ---- batch: 020 ----
mean loss: 432.45
 ---- batch: 030 ----
mean loss: 425.12
train mean loss: 427.53
epoch train time: 0:00:00.176932
elapsed time: 0:00:21.143495
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:43:51.531144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.01
 ---- batch: 020 ----
mean loss: 421.23
 ---- batch: 030 ----
mean loss: 415.96
train mean loss: 421.14
epoch train time: 0:00:00.173470
elapsed time: 0:00:21.317155
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:43:51.704811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.04
 ---- batch: 020 ----
mean loss: 416.09
 ---- batch: 030 ----
mean loss: 412.29
train mean loss: 416.20
epoch train time: 0:00:00.186362
elapsed time: 0:00:21.503675
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:43:51.891314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.73
 ---- batch: 020 ----
mean loss: 410.33
 ---- batch: 030 ----
mean loss: 412.53
train mean loss: 411.09
epoch train time: 0:00:00.179951
elapsed time: 0:00:21.683765
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:43:52.071414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.75
 ---- batch: 020 ----
mean loss: 398.35
 ---- batch: 030 ----
mean loss: 411.18
train mean loss: 406.87
epoch train time: 0:00:00.177994
elapsed time: 0:00:21.861925
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:43:52.249579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.44
 ---- batch: 020 ----
mean loss: 406.22
 ---- batch: 030 ----
mean loss: 395.44
train mean loss: 402.50
epoch train time: 0:00:00.177074
elapsed time: 0:00:22.039154
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:43:52.426810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.36
 ---- batch: 020 ----
mean loss: 396.07
 ---- batch: 030 ----
mean loss: 401.65
train mean loss: 399.05
epoch train time: 0:00:00.176358
elapsed time: 0:00:22.215670
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:43:52.603308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.02
 ---- batch: 020 ----
mean loss: 399.23
 ---- batch: 030 ----
mean loss: 388.99
train mean loss: 395.73
epoch train time: 0:00:00.180896
elapsed time: 0:00:22.396707
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:43:52.784345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.78
 ---- batch: 020 ----
mean loss: 391.52
 ---- batch: 030 ----
mean loss: 394.92
train mean loss: 392.73
epoch train time: 0:00:00.179554
elapsed time: 0:00:22.576405
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:43:52.964045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.50
 ---- batch: 020 ----
mean loss: 397.45
 ---- batch: 030 ----
mean loss: 387.26
train mean loss: 390.47
epoch train time: 0:00:00.179598
elapsed time: 0:00:22.756166
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:43:53.143820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.45
 ---- batch: 020 ----
mean loss: 387.74
 ---- batch: 030 ----
mean loss: 388.36
train mean loss: 387.73
epoch train time: 0:00:00.179196
elapsed time: 0:00:22.935526
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:43:53.323167
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.73
 ---- batch: 020 ----
mean loss: 394.99
 ---- batch: 030 ----
mean loss: 385.79
train mean loss: 385.61
epoch train time: 0:00:00.178467
elapsed time: 0:00:23.114142
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:43:53.501782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.95
 ---- batch: 020 ----
mean loss: 386.53
 ---- batch: 030 ----
mean loss: 378.51
train mean loss: 383.84
epoch train time: 0:00:00.181188
elapsed time: 0:00:23.295473
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:43:53.683126
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.97
 ---- batch: 020 ----
mean loss: 375.55
 ---- batch: 030 ----
mean loss: 378.91
train mean loss: 382.63
epoch train time: 0:00:00.183825
elapsed time: 0:00:23.479452
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:43:53.867091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.96
 ---- batch: 020 ----
mean loss: 379.55
 ---- batch: 030 ----
mean loss: 386.67
train mean loss: 380.82
epoch train time: 0:00:00.177880
elapsed time: 0:00:23.657474
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:43:54.045112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.78
 ---- batch: 020 ----
mean loss: 382.92
 ---- batch: 030 ----
mean loss: 375.76
train mean loss: 379.51
epoch train time: 0:00:00.177463
elapsed time: 0:00:23.835083
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:43:54.222741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.79
 ---- batch: 020 ----
mean loss: 379.82
 ---- batch: 030 ----
mean loss: 377.06
train mean loss: 377.97
epoch train time: 0:00:00.181792
elapsed time: 0:00:24.017035
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:43:54.404673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.56
 ---- batch: 020 ----
mean loss: 373.36
 ---- batch: 030 ----
mean loss: 376.46
train mean loss: 377.20
epoch train time: 0:00:00.180906
elapsed time: 0:00:24.198108
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:43:54.585772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.56
 ---- batch: 020 ----
mean loss: 371.62
 ---- batch: 030 ----
mean loss: 374.65
train mean loss: 376.15
epoch train time: 0:00:00.182199
elapsed time: 0:00:24.380477
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:43:54.768115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.66
 ---- batch: 020 ----
mean loss: 371.84
 ---- batch: 030 ----
mean loss: 372.13
train mean loss: 374.98
epoch train time: 0:00:00.173031
elapsed time: 0:00:24.553654
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:43:54.941290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.18
 ---- batch: 020 ----
mean loss: 370.69
 ---- batch: 030 ----
mean loss: 375.55
train mean loss: 374.42
epoch train time: 0:00:00.175390
elapsed time: 0:00:24.729196
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:43:55.116833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.18
 ---- batch: 020 ----
mean loss: 376.63
 ---- batch: 030 ----
mean loss: 373.22
train mean loss: 373.17
epoch train time: 0:00:00.172633
elapsed time: 0:00:24.901966
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:43:55.289623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.84
 ---- batch: 020 ----
mean loss: 372.71
 ---- batch: 030 ----
mean loss: 372.41
train mean loss: 372.81
epoch train time: 0:00:00.174473
elapsed time: 0:00:25.076600
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:43:55.464240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.07
 ---- batch: 020 ----
mean loss: 363.83
 ---- batch: 030 ----
mean loss: 377.03
train mean loss: 371.79
epoch train time: 0:00:00.176324
elapsed time: 0:00:25.253064
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:43:55.640702
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.73
 ---- batch: 020 ----
mean loss: 369.90
 ---- batch: 030 ----
mean loss: 365.44
train mean loss: 371.58
epoch train time: 0:00:00.179627
elapsed time: 0:00:25.432841
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:43:55.820483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.57
 ---- batch: 020 ----
mean loss: 372.02
 ---- batch: 030 ----
mean loss: 367.29
train mean loss: 370.64
epoch train time: 0:00:00.178970
elapsed time: 0:00:25.611961
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:43:55.999601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.19
 ---- batch: 020 ----
mean loss: 378.06
 ---- batch: 030 ----
mean loss: 366.63
train mean loss: 370.14
epoch train time: 0:00:00.179459
elapsed time: 0:00:25.791558
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:43:56.179196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.20
 ---- batch: 020 ----
mean loss: 366.03
 ---- batch: 030 ----
mean loss: 372.62
train mean loss: 369.97
epoch train time: 0:00:00.178004
elapsed time: 0:00:25.969700
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:43:56.357339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.97
 ---- batch: 020 ----
mean loss: 373.31
 ---- batch: 030 ----
mean loss: 370.30
train mean loss: 369.51
epoch train time: 0:00:00.176903
elapsed time: 0:00:26.147525
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:43:56.535236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.38
 ---- batch: 020 ----
mean loss: 364.65
 ---- batch: 030 ----
mean loss: 369.43
train mean loss: 369.30
epoch train time: 0:00:00.177709
elapsed time: 0:00:26.325448
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:43:56.713086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.99
 ---- batch: 020 ----
mean loss: 371.96
 ---- batch: 030 ----
mean loss: 372.03
train mean loss: 368.56
epoch train time: 0:00:00.175890
elapsed time: 0:00:26.501488
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:43:56.889127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.97
 ---- batch: 020 ----
mean loss: 362.44
 ---- batch: 030 ----
mean loss: 364.52
train mean loss: 368.65
epoch train time: 0:00:00.176018
elapsed time: 0:00:26.677645
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:43:57.065282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.92
 ---- batch: 020 ----
mean loss: 368.36
 ---- batch: 030 ----
mean loss: 371.15
train mean loss: 368.30
epoch train time: 0:00:00.173176
elapsed time: 0:00:26.850964
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:43:57.238603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.40
 ---- batch: 020 ----
mean loss: 366.91
 ---- batch: 030 ----
mean loss: 378.79
train mean loss: 367.47
epoch train time: 0:00:00.175990
elapsed time: 0:00:27.027101
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:43:57.414739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.34
 ---- batch: 020 ----
mean loss: 369.26
 ---- batch: 030 ----
mean loss: 367.33
train mean loss: 367.42
epoch train time: 0:00:00.174799
elapsed time: 0:00:27.202059
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:43:57.589701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.63
 ---- batch: 020 ----
mean loss: 361.30
 ---- batch: 030 ----
mean loss: 368.61
train mean loss: 367.28
epoch train time: 0:00:00.179009
elapsed time: 0:00:27.381208
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:43:57.768845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.21
 ---- batch: 020 ----
mean loss: 370.08
 ---- batch: 030 ----
mean loss: 365.04
train mean loss: 366.80
epoch train time: 0:00:00.177269
elapsed time: 0:00:27.558674
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:43:57.946314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.74
 ---- batch: 020 ----
mean loss: 362.59
 ---- batch: 030 ----
mean loss: 366.76
train mean loss: 366.87
epoch train time: 0:00:00.176169
elapsed time: 0:00:27.734984
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:43:58.122622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.12
 ---- batch: 020 ----
mean loss: 371.88
 ---- batch: 030 ----
mean loss: 364.98
train mean loss: 366.21
epoch train time: 0:00:00.179710
elapsed time: 0:00:27.914844
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:43:58.302484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.56
 ---- batch: 020 ----
mean loss: 366.98
 ---- batch: 030 ----
mean loss: 362.82
train mean loss: 366.03
epoch train time: 0:00:00.178275
elapsed time: 0:00:28.093259
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:43:58.480898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.80
 ---- batch: 020 ----
mean loss: 376.14
 ---- batch: 030 ----
mean loss: 357.72
train mean loss: 365.78
epoch train time: 0:00:00.176037
elapsed time: 0:00:28.269435
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:43:58.657073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.28
 ---- batch: 020 ----
mean loss: 361.44
 ---- batch: 030 ----
mean loss: 360.80
train mean loss: 365.71
epoch train time: 0:00:00.178587
elapsed time: 0:00:28.448172
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:43:58.835811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.10
 ---- batch: 020 ----
mean loss: 361.81
 ---- batch: 030 ----
mean loss: 369.20
train mean loss: 365.22
epoch train time: 0:00:00.180570
elapsed time: 0:00:28.628887
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:43:59.016557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.89
 ---- batch: 020 ----
mean loss: 367.79
 ---- batch: 030 ----
mean loss: 368.54
train mean loss: 365.23
epoch train time: 0:00:00.182091
elapsed time: 0:00:28.811158
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:43:59.198797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.72
 ---- batch: 020 ----
mean loss: 364.71
 ---- batch: 030 ----
mean loss: 367.78
train mean loss: 364.68
epoch train time: 0:00:00.177538
elapsed time: 0:00:28.988850
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:43:59.376492
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.80
 ---- batch: 020 ----
mean loss: 365.06
 ---- batch: 030 ----
mean loss: 361.56
train mean loss: 364.66
epoch train time: 0:00:00.182204
elapsed time: 0:00:29.171213
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:43:59.558853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.36
 ---- batch: 020 ----
mean loss: 363.08
 ---- batch: 030 ----
mean loss: 365.48
train mean loss: 364.57
epoch train time: 0:00:00.186170
elapsed time: 0:00:29.357533
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:43:59.745176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.13
 ---- batch: 020 ----
mean loss: 358.42
 ---- batch: 030 ----
mean loss: 371.96
train mean loss: 364.62
epoch train time: 0:00:00.179123
elapsed time: 0:00:29.536799
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:43:59.924438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.20
 ---- batch: 020 ----
mean loss: 361.44
 ---- batch: 030 ----
mean loss: 369.47
train mean loss: 363.87
epoch train time: 0:00:00.174724
elapsed time: 0:00:29.711662
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:44:00.099301
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.84
 ---- batch: 020 ----
mean loss: 363.31
 ---- batch: 030 ----
mean loss: 368.38
train mean loss: 363.81
epoch train time: 0:00:00.175016
elapsed time: 0:00:29.886818
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:44:00.274457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.94
 ---- batch: 020 ----
mean loss: 361.48
 ---- batch: 030 ----
mean loss: 360.23
train mean loss: 363.82
epoch train time: 0:00:00.175027
elapsed time: 0:00:30.061982
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:44:00.449651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.73
 ---- batch: 020 ----
mean loss: 362.50
 ---- batch: 030 ----
mean loss: 360.71
train mean loss: 363.86
epoch train time: 0:00:00.176000
elapsed time: 0:00:30.238152
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:44:00.625806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.35
 ---- batch: 020 ----
mean loss: 359.86
 ---- batch: 030 ----
mean loss: 365.43
train mean loss: 363.17
epoch train time: 0:00:00.175906
elapsed time: 0:00:30.414239
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:44:00.801892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.91
 ---- batch: 020 ----
mean loss: 366.09
 ---- batch: 030 ----
mean loss: 357.83
train mean loss: 363.55
epoch train time: 0:00:00.180985
elapsed time: 0:00:30.595378
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:44:00.983025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.13
 ---- batch: 020 ----
mean loss: 367.86
 ---- batch: 030 ----
mean loss: 363.43
train mean loss: 363.06
epoch train time: 0:00:00.177045
elapsed time: 0:00:30.772571
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:44:01.160208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.05
 ---- batch: 020 ----
mean loss: 363.63
 ---- batch: 030 ----
mean loss: 363.37
train mean loss: 362.53
epoch train time: 0:00:00.173837
elapsed time: 0:00:30.946562
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:44:01.334205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.67
 ---- batch: 020 ----
mean loss: 368.38
 ---- batch: 030 ----
mean loss: 363.19
train mean loss: 362.80
epoch train time: 0:00:00.175047
elapsed time: 0:00:31.121765
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:44:01.509401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.87
 ---- batch: 020 ----
mean loss: 368.84
 ---- batch: 030 ----
mean loss: 364.55
train mean loss: 362.36
epoch train time: 0:00:00.177139
elapsed time: 0:00:31.299039
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:44:01.686709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.54
 ---- batch: 020 ----
mean loss: 362.85
 ---- batch: 030 ----
mean loss: 368.31
train mean loss: 362.36
epoch train time: 0:00:00.182303
elapsed time: 0:00:31.481511
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:44:01.869149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.85
 ---- batch: 020 ----
mean loss: 364.80
 ---- batch: 030 ----
mean loss: 364.39
train mean loss: 362.22
epoch train time: 0:00:00.175351
elapsed time: 0:00:31.657029
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:44:02.044657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.15
 ---- batch: 020 ----
mean loss: 354.53
 ---- batch: 030 ----
mean loss: 365.92
train mean loss: 362.01
epoch train time: 0:00:00.174222
elapsed time: 0:00:31.831381
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:44:02.219018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.90
 ---- batch: 020 ----
mean loss: 368.72
 ---- batch: 030 ----
mean loss: 357.31
train mean loss: 362.08
epoch train time: 0:00:00.173777
elapsed time: 0:00:32.005293
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:44:02.392929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.88
 ---- batch: 020 ----
mean loss: 355.88
 ---- batch: 030 ----
mean loss: 364.02
train mean loss: 361.89
epoch train time: 0:00:00.180829
elapsed time: 0:00:32.186276
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:44:02.573964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.03
 ---- batch: 020 ----
mean loss: 360.92
 ---- batch: 030 ----
mean loss: 362.31
train mean loss: 361.16
epoch train time: 0:00:00.178879
elapsed time: 0:00:32.365346
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:44:02.752983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.86
 ---- batch: 020 ----
mean loss: 360.46
 ---- batch: 030 ----
mean loss: 364.06
train mean loss: 361.22
epoch train time: 0:00:00.183092
elapsed time: 0:00:32.548577
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:44:02.936215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.20
 ---- batch: 020 ----
mean loss: 358.20
 ---- batch: 030 ----
mean loss: 359.70
train mean loss: 361.70
epoch train time: 0:00:00.175932
elapsed time: 0:00:32.724654
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:44:03.112291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.45
 ---- batch: 020 ----
mean loss: 360.48
 ---- batch: 030 ----
mean loss: 364.67
train mean loss: 360.91
epoch train time: 0:00:00.175225
elapsed time: 0:00:32.900064
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:44:03.287719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.62
 ---- batch: 020 ----
mean loss: 363.58
 ---- batch: 030 ----
mean loss: 368.83
train mean loss: 360.88
epoch train time: 0:00:00.179100
elapsed time: 0:00:33.079323
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:44:03.466963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.75
 ---- batch: 020 ----
mean loss: 361.77
 ---- batch: 030 ----
mean loss: 359.31
train mean loss: 361.20
epoch train time: 0:00:00.179788
elapsed time: 0:00:33.259252
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:44:03.646888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.32
 ---- batch: 020 ----
mean loss: 361.59
 ---- batch: 030 ----
mean loss: 362.18
train mean loss: 360.34
epoch train time: 0:00:00.184223
elapsed time: 0:00:33.443621
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:44:03.831295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.08
 ---- batch: 020 ----
mean loss: 363.24
 ---- batch: 030 ----
mean loss: 358.83
train mean loss: 359.89
epoch train time: 0:00:00.179864
elapsed time: 0:00:33.623660
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:44:04.011299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.36
 ---- batch: 020 ----
mean loss: 363.97
 ---- batch: 030 ----
mean loss: 359.32
train mean loss: 360.39
epoch train time: 0:00:00.173532
elapsed time: 0:00:33.797366
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:44:04.185005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.48
 ---- batch: 020 ----
mean loss: 357.46
 ---- batch: 030 ----
mean loss: 358.75
train mean loss: 360.11
epoch train time: 0:00:00.175003
elapsed time: 0:00:33.972522
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:44:04.360158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.55
 ---- batch: 020 ----
mean loss: 356.85
 ---- batch: 030 ----
mean loss: 357.65
train mean loss: 360.08
epoch train time: 0:00:00.179216
elapsed time: 0:00:34.151899
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:44:04.539537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.54
 ---- batch: 020 ----
mean loss: 357.61
 ---- batch: 030 ----
mean loss: 366.66
train mean loss: 359.62
epoch train time: 0:00:00.179437
elapsed time: 0:00:34.331483
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:44:04.719131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.07
 ---- batch: 020 ----
mean loss: 357.93
 ---- batch: 030 ----
mean loss: 363.51
train mean loss: 359.76
epoch train time: 0:00:00.183490
elapsed time: 0:00:34.515168
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:44:04.902806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.01
 ---- batch: 020 ----
mean loss: 361.34
 ---- batch: 030 ----
mean loss: 358.10
train mean loss: 359.36
epoch train time: 0:00:00.178732
elapsed time: 0:00:34.694119
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:44:05.081762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.51
 ---- batch: 020 ----
mean loss: 353.82
 ---- batch: 030 ----
mean loss: 362.31
train mean loss: 359.09
epoch train time: 0:00:00.179565
elapsed time: 0:00:34.873851
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:44:05.261503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.92
 ---- batch: 020 ----
mean loss: 357.17
 ---- batch: 030 ----
mean loss: 350.93
train mean loss: 359.48
epoch train time: 0:00:00.179126
elapsed time: 0:00:35.053155
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:44:05.440827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.00
 ---- batch: 020 ----
mean loss: 365.00
 ---- batch: 030 ----
mean loss: 359.76
train mean loss: 358.97
epoch train time: 0:00:00.179004
elapsed time: 0:00:35.232349
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:44:05.619978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.02
 ---- batch: 020 ----
mean loss: 357.76
 ---- batch: 030 ----
mean loss: 363.50
train mean loss: 358.92
epoch train time: 0:00:00.184371
elapsed time: 0:00:35.416942
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:44:05.804596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.94
 ---- batch: 020 ----
mean loss: 361.72
 ---- batch: 030 ----
mean loss: 359.98
train mean loss: 358.54
epoch train time: 0:00:00.182625
elapsed time: 0:00:35.599722
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:44:05.987360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.14
 ---- batch: 020 ----
mean loss: 360.26
 ---- batch: 030 ----
mean loss: 360.59
train mean loss: 358.40
epoch train time: 0:00:00.177085
elapsed time: 0:00:35.776947
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:44:06.164617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.26
 ---- batch: 020 ----
mean loss: 358.30
 ---- batch: 030 ----
mean loss: 368.81
train mean loss: 358.72
epoch train time: 0:00:00.173293
elapsed time: 0:00:35.950412
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:44:06.338067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.65
 ---- batch: 020 ----
mean loss: 370.73
 ---- batch: 030 ----
mean loss: 351.53
train mean loss: 358.32
epoch train time: 0:00:00.176137
elapsed time: 0:00:36.126925
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:44:06.514580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.69
 ---- batch: 020 ----
mean loss: 362.46
 ---- batch: 030 ----
mean loss: 365.77
train mean loss: 358.21
epoch train time: 0:00:00.175249
elapsed time: 0:00:36.302332
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:44:06.689974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.22
 ---- batch: 020 ----
mean loss: 356.19
 ---- batch: 030 ----
mean loss: 351.48
train mean loss: 358.00
epoch train time: 0:00:00.175505
elapsed time: 0:00:36.477979
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:44:06.865640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.83
 ---- batch: 020 ----
mean loss: 354.81
 ---- batch: 030 ----
mean loss: 364.38
train mean loss: 357.72
epoch train time: 0:00:00.173362
elapsed time: 0:00:36.651503
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:44:07.039141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.06
 ---- batch: 020 ----
mean loss: 360.22
 ---- batch: 030 ----
mean loss: 365.20
train mean loss: 357.90
epoch train time: 0:00:00.175039
elapsed time: 0:00:36.826680
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:44:07.214319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.31
 ---- batch: 020 ----
mean loss: 353.59
 ---- batch: 030 ----
mean loss: 353.50
train mean loss: 357.75
epoch train time: 0:00:00.173713
elapsed time: 0:00:37.000546
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:44:07.388185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.84
 ---- batch: 020 ----
mean loss: 356.05
 ---- batch: 030 ----
mean loss: 359.28
train mean loss: 357.53
epoch train time: 0:00:00.176429
elapsed time: 0:00:37.177123
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:44:07.564761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.68
 ---- batch: 020 ----
mean loss: 354.19
 ---- batch: 030 ----
mean loss: 360.65
train mean loss: 357.00
epoch train time: 0:00:00.179835
elapsed time: 0:00:37.357097
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:44:07.744736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.16
 ---- batch: 020 ----
mean loss: 354.06
 ---- batch: 030 ----
mean loss: 350.33
train mean loss: 357.32
epoch train time: 0:00:00.176629
elapsed time: 0:00:37.533878
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:44:07.921520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.51
 ---- batch: 020 ----
mean loss: 359.37
 ---- batch: 030 ----
mean loss: 352.22
train mean loss: 357.35
epoch train time: 0:00:00.173458
elapsed time: 0:00:37.707481
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:44:08.095119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.16
 ---- batch: 020 ----
mean loss: 355.68
 ---- batch: 030 ----
mean loss: 356.53
train mean loss: 357.21
epoch train time: 0:00:00.174256
elapsed time: 0:00:37.881884
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:44:08.269538
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.62
 ---- batch: 020 ----
mean loss: 361.88
 ---- batch: 030 ----
mean loss: 356.21
train mean loss: 356.82
epoch train time: 0:00:00.175220
elapsed time: 0:00:38.057288
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:44:08.444952
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.14
 ---- batch: 020 ----
mean loss: 363.88
 ---- batch: 030 ----
mean loss: 352.52
train mean loss: 356.59
epoch train time: 0:00:00.174193
elapsed time: 0:00:38.231655
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:44:08.619308
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.37
 ---- batch: 020 ----
mean loss: 356.50
 ---- batch: 030 ----
mean loss: 356.37
train mean loss: 356.34
epoch train time: 0:00:00.193792
elapsed time: 0:00:38.425612
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:44:08.813261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.97
 ---- batch: 020 ----
mean loss: 358.05
 ---- batch: 030 ----
mean loss: 354.04
train mean loss: 356.40
epoch train time: 0:00:00.181948
elapsed time: 0:00:38.607712
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:44:08.995354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.34
 ---- batch: 020 ----
mean loss: 357.17
 ---- batch: 030 ----
mean loss: 357.30
train mean loss: 356.08
epoch train time: 0:00:00.178325
elapsed time: 0:00:38.786182
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:44:09.173822
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.18
 ---- batch: 020 ----
mean loss: 351.73
 ---- batch: 030 ----
mean loss: 355.91
train mean loss: 355.83
epoch train time: 0:00:00.175197
elapsed time: 0:00:38.961518
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:44:09.349158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.06
 ---- batch: 020 ----
mean loss: 352.67
 ---- batch: 030 ----
mean loss: 359.92
train mean loss: 355.80
epoch train time: 0:00:00.176053
elapsed time: 0:00:39.137727
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:44:09.525356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.00
 ---- batch: 020 ----
mean loss: 356.13
 ---- batch: 030 ----
mean loss: 362.32
train mean loss: 355.65
epoch train time: 0:00:00.177735
elapsed time: 0:00:39.315590
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:44:09.703229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.06
 ---- batch: 020 ----
mean loss: 360.95
 ---- batch: 030 ----
mean loss: 349.77
train mean loss: 355.58
epoch train time: 0:00:00.178956
elapsed time: 0:00:39.494686
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:44:09.882325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.39
 ---- batch: 020 ----
mean loss: 362.31
 ---- batch: 030 ----
mean loss: 351.97
train mean loss: 355.46
epoch train time: 0:00:00.175334
elapsed time: 0:00:39.670174
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:44:10.057817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.76
 ---- batch: 020 ----
mean loss: 353.84
 ---- batch: 030 ----
mean loss: 352.60
train mean loss: 355.74
epoch train time: 0:00:00.176216
elapsed time: 0:00:39.846537
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:44:10.234177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.85
 ---- batch: 020 ----
mean loss: 354.14
 ---- batch: 030 ----
mean loss: 359.72
train mean loss: 355.12
epoch train time: 0:00:00.179451
elapsed time: 0:00:40.026132
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:44:10.413771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.70
 ---- batch: 020 ----
mean loss: 357.59
 ---- batch: 030 ----
mean loss: 358.86
train mean loss: 354.95
epoch train time: 0:00:00.173752
elapsed time: 0:00:40.200044
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:44:10.587715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.06
 ---- batch: 020 ----
mean loss: 352.31
 ---- batch: 030 ----
mean loss: 352.59
train mean loss: 355.21
epoch train time: 0:00:00.177136
elapsed time: 0:00:40.377353
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:44:10.765022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.28
 ---- batch: 020 ----
mean loss: 355.27
 ---- batch: 030 ----
mean loss: 350.91
train mean loss: 354.75
epoch train time: 0:00:00.173674
elapsed time: 0:00:40.551193
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:44:10.938851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.01
 ---- batch: 020 ----
mean loss: 359.18
 ---- batch: 030 ----
mean loss: 356.56
train mean loss: 354.76
epoch train time: 0:00:00.180831
elapsed time: 0:00:40.732188
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:44:11.119844
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.31
 ---- batch: 020 ----
mean loss: 350.05
 ---- batch: 030 ----
mean loss: 354.79
train mean loss: 354.42
epoch train time: 0:00:00.179211
elapsed time: 0:00:40.911556
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:44:11.299193
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.09
 ---- batch: 020 ----
mean loss: 357.74
 ---- batch: 030 ----
mean loss: 351.05
train mean loss: 354.50
epoch train time: 0:00:00.174903
elapsed time: 0:00:41.086599
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:44:11.474237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.27
 ---- batch: 020 ----
mean loss: 364.47
 ---- batch: 030 ----
mean loss: 347.15
train mean loss: 353.73
epoch train time: 0:00:00.173683
elapsed time: 0:00:41.260419
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:44:11.648058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.41
 ---- batch: 020 ----
mean loss: 350.59
 ---- batch: 030 ----
mean loss: 358.64
train mean loss: 353.94
epoch train time: 0:00:00.175655
elapsed time: 0:00:41.436216
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:44:11.823853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.82
 ---- batch: 020 ----
mean loss: 357.22
 ---- batch: 030 ----
mean loss: 357.35
train mean loss: 354.10
epoch train time: 0:00:00.174490
elapsed time: 0:00:41.610883
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:44:11.998523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.64
 ---- batch: 020 ----
mean loss: 356.14
 ---- batch: 030 ----
mean loss: 353.30
train mean loss: 353.96
epoch train time: 0:00:00.175574
elapsed time: 0:00:41.786598
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:44:12.174234
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.15
 ---- batch: 020 ----
mean loss: 360.03
 ---- batch: 030 ----
mean loss: 351.61
train mean loss: 353.61
epoch train time: 0:00:00.176110
elapsed time: 0:00:41.962858
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:44:12.350498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.87
 ---- batch: 020 ----
mean loss: 359.25
 ---- batch: 030 ----
mean loss: 348.21
train mean loss: 353.07
epoch train time: 0:00:00.176281
elapsed time: 0:00:42.139280
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:44:12.526930
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.21
 ---- batch: 020 ----
mean loss: 352.89
 ---- batch: 030 ----
mean loss: 354.21
train mean loss: 353.27
epoch train time: 0:00:00.178122
elapsed time: 0:00:42.317577
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:44:12.705242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.95
 ---- batch: 020 ----
mean loss: 346.00
 ---- batch: 030 ----
mean loss: 361.85
train mean loss: 353.64
epoch train time: 0:00:00.187182
elapsed time: 0:00:42.504992
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:44:12.892631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.81
 ---- batch: 020 ----
mean loss: 351.55
 ---- batch: 030 ----
mean loss: 353.47
train mean loss: 353.20
epoch train time: 0:00:00.174291
elapsed time: 0:00:42.679431
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:44:13.067077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.80
 ---- batch: 020 ----
mean loss: 356.17
 ---- batch: 030 ----
mean loss: 350.13
train mean loss: 352.93
epoch train time: 0:00:00.173403
elapsed time: 0:00:42.852979
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:44:13.240617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.21
 ---- batch: 020 ----
mean loss: 352.56
 ---- batch: 030 ----
mean loss: 350.90
train mean loss: 352.72
epoch train time: 0:00:00.173306
elapsed time: 0:00:43.026474
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:44:13.414124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.62
 ---- batch: 020 ----
mean loss: 359.26
 ---- batch: 030 ----
mean loss: 359.85
train mean loss: 352.57
epoch train time: 0:00:00.173325
elapsed time: 0:00:43.199948
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:44:13.587593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.97
 ---- batch: 020 ----
mean loss: 347.45
 ---- batch: 030 ----
mean loss: 354.30
train mean loss: 352.95
epoch train time: 0:00:00.175976
elapsed time: 0:00:43.376066
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:44:13.763706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.59
 ---- batch: 020 ----
mean loss: 355.82
 ---- batch: 030 ----
mean loss: 349.09
train mean loss: 352.58
epoch train time: 0:00:00.177779
elapsed time: 0:00:43.554035
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:44:13.941665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.53
 ---- batch: 020 ----
mean loss: 355.52
 ---- batch: 030 ----
mean loss: 353.17
train mean loss: 352.18
epoch train time: 0:00:00.171631
elapsed time: 0:00:43.725823
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:44:14.113460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.54
 ---- batch: 020 ----
mean loss: 354.54
 ---- batch: 030 ----
mean loss: 349.64
train mean loss: 352.09
epoch train time: 0:00:00.173306
elapsed time: 0:00:43.899265
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:44:14.286902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.36
 ---- batch: 020 ----
mean loss: 346.77
 ---- batch: 030 ----
mean loss: 358.04
train mean loss: 351.85
epoch train time: 0:00:00.172167
elapsed time: 0:00:44.071565
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:44:14.459200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.09
 ---- batch: 020 ----
mean loss: 350.41
 ---- batch: 030 ----
mean loss: 345.71
train mean loss: 351.68
epoch train time: 0:00:00.173426
elapsed time: 0:00:44.245124
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:44:14.632759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.32
 ---- batch: 020 ----
mean loss: 358.13
 ---- batch: 030 ----
mean loss: 345.52
train mean loss: 351.89
epoch train time: 0:00:00.172502
elapsed time: 0:00:44.417779
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:44:14.805442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.23
 ---- batch: 020 ----
mean loss: 351.38
 ---- batch: 030 ----
mean loss: 355.31
train mean loss: 351.40
epoch train time: 0:00:00.182333
elapsed time: 0:00:44.600287
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:44:14.987927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.05
 ---- batch: 020 ----
mean loss: 355.98
 ---- batch: 030 ----
mean loss: 346.01
train mean loss: 351.70
epoch train time: 0:00:00.178332
elapsed time: 0:00:44.778774
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:44:15.166413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.79
 ---- batch: 020 ----
mean loss: 349.62
 ---- batch: 030 ----
mean loss: 355.88
train mean loss: 351.13
epoch train time: 0:00:00.175149
elapsed time: 0:00:44.954083
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:44:15.341724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.06
 ---- batch: 020 ----
mean loss: 348.32
 ---- batch: 030 ----
mean loss: 352.00
train mean loss: 351.28
epoch train time: 0:00:00.176988
elapsed time: 0:00:45.131213
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:44:15.518851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.03
 ---- batch: 020 ----
mean loss: 351.15
 ---- batch: 030 ----
mean loss: 352.98
train mean loss: 351.16
epoch train time: 0:00:00.175943
elapsed time: 0:00:45.307310
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:44:15.694968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.98
 ---- batch: 020 ----
mean loss: 346.11
 ---- batch: 030 ----
mean loss: 350.47
train mean loss: 350.95
epoch train time: 0:00:00.177494
elapsed time: 0:00:45.484974
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:44:15.872636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.88
 ---- batch: 020 ----
mean loss: 356.60
 ---- batch: 030 ----
mean loss: 341.77
train mean loss: 350.94
epoch train time: 0:00:00.179852
elapsed time: 0:00:45.665021
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:44:16.052660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.67
 ---- batch: 020 ----
mean loss: 355.26
 ---- batch: 030 ----
mean loss: 346.76
train mean loss: 350.57
epoch train time: 0:00:00.181183
elapsed time: 0:00:45.846361
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:44:16.234000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.21
 ---- batch: 020 ----
mean loss: 357.18
 ---- batch: 030 ----
mean loss: 350.21
train mean loss: 350.55
epoch train time: 0:00:00.178512
elapsed time: 0:00:46.025012
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:44:16.412651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.91
 ---- batch: 020 ----
mean loss: 344.19
 ---- batch: 030 ----
mean loss: 355.74
train mean loss: 350.21
epoch train time: 0:00:00.177996
elapsed time: 0:00:46.203764
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:44:16.591417
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.61
 ---- batch: 020 ----
mean loss: 353.28
 ---- batch: 030 ----
mean loss: 354.09
train mean loss: 349.92
epoch train time: 0:00:00.177867
elapsed time: 0:00:46.381789
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:44:16.769428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.72
 ---- batch: 020 ----
mean loss: 353.54
 ---- batch: 030 ----
mean loss: 350.32
train mean loss: 349.98
epoch train time: 0:00:00.181271
elapsed time: 0:00:46.563204
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:44:16.950849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.66
 ---- batch: 020 ----
mean loss: 355.56
 ---- batch: 030 ----
mean loss: 346.66
train mean loss: 349.59
epoch train time: 0:00:00.176968
elapsed time: 0:00:46.740323
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:44:17.127963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.95
 ---- batch: 020 ----
mean loss: 353.77
 ---- batch: 030 ----
mean loss: 344.65
train mean loss: 349.48
epoch train time: 0:00:00.176034
elapsed time: 0:00:46.916496
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:44:17.304135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.38
 ---- batch: 020 ----
mean loss: 349.00
 ---- batch: 030 ----
mean loss: 347.51
train mean loss: 349.54
epoch train time: 0:00:00.173757
elapsed time: 0:00:47.090411
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:44:17.478051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.78
 ---- batch: 020 ----
mean loss: 352.67
 ---- batch: 030 ----
mean loss: 348.42
train mean loss: 349.34
epoch train time: 0:00:00.173710
elapsed time: 0:00:47.264262
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:44:17.651899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.83
 ---- batch: 020 ----
mean loss: 344.15
 ---- batch: 030 ----
mean loss: 348.84
train mean loss: 349.06
epoch train time: 0:00:00.175989
elapsed time: 0:00:47.440388
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:44:17.828026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.39
 ---- batch: 020 ----
mean loss: 348.80
 ---- batch: 030 ----
mean loss: 346.13
train mean loss: 348.65
epoch train time: 0:00:00.180523
elapsed time: 0:00:47.621049
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:44:18.008688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.85
 ---- batch: 020 ----
mean loss: 352.87
 ---- batch: 030 ----
mean loss: 354.87
train mean loss: 348.79
epoch train time: 0:00:00.175637
elapsed time: 0:00:47.796824
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:44:18.184478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.66
 ---- batch: 020 ----
mean loss: 347.50
 ---- batch: 030 ----
mean loss: 348.12
train mean loss: 348.67
epoch train time: 0:00:00.171168
elapsed time: 0:00:47.968147
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:44:18.355785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.23
 ---- batch: 020 ----
mean loss: 349.21
 ---- batch: 030 ----
mean loss: 350.88
train mean loss: 348.02
epoch train time: 0:00:00.175260
elapsed time: 0:00:48.143544
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:44:18.531182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.79
 ---- batch: 020 ----
mean loss: 356.52
 ---- batch: 030 ----
mean loss: 344.36
train mean loss: 348.54
epoch train time: 0:00:00.173886
elapsed time: 0:00:48.317582
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:44:18.705223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.93
 ---- batch: 020 ----
mean loss: 344.44
 ---- batch: 030 ----
mean loss: 350.75
train mean loss: 348.24
epoch train time: 0:00:00.172620
elapsed time: 0:00:48.490353
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:44:18.878034
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.01
 ---- batch: 020 ----
mean loss: 349.49
 ---- batch: 030 ----
mean loss: 353.35
train mean loss: 348.30
epoch train time: 0:00:00.176141
elapsed time: 0:00:48.666677
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:44:19.054316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.60
 ---- batch: 020 ----
mean loss: 351.09
 ---- batch: 030 ----
mean loss: 341.62
train mean loss: 348.39
epoch train time: 0:00:00.172122
elapsed time: 0:00:48.838960
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:44:19.226639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.44
 ---- batch: 020 ----
mean loss: 344.18
 ---- batch: 030 ----
mean loss: 355.38
train mean loss: 348.21
epoch train time: 0:00:00.172096
elapsed time: 0:00:49.011237
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:44:19.398891
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.79
 ---- batch: 020 ----
mean loss: 347.49
 ---- batch: 030 ----
mean loss: 351.45
train mean loss: 347.91
epoch train time: 0:00:00.176134
elapsed time: 0:00:49.187526
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:44:19.575180
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.21
 ---- batch: 020 ----
mean loss: 346.96
 ---- batch: 030 ----
mean loss: 345.79
train mean loss: 348.21
epoch train time: 0:00:00.179897
elapsed time: 0:00:49.367671
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:44:19.755329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.21
 ---- batch: 020 ----
mean loss: 349.03
 ---- batch: 030 ----
mean loss: 354.39
train mean loss: 347.97
epoch train time: 0:00:00.179819
elapsed time: 0:00:49.547654
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:44:19.935294
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.38
 ---- batch: 020 ----
mean loss: 341.73
 ---- batch: 030 ----
mean loss: 349.48
train mean loss: 348.34
epoch train time: 0:00:00.179099
elapsed time: 0:00:49.726894
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:44:20.114532
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.69
 ---- batch: 020 ----
mean loss: 355.39
 ---- batch: 030 ----
mean loss: 349.59
train mean loss: 347.90
epoch train time: 0:00:00.175334
elapsed time: 0:00:49.902370
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:44:20.290009
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.37
 ---- batch: 020 ----
mean loss: 345.63
 ---- batch: 030 ----
mean loss: 351.47
train mean loss: 348.66
epoch train time: 0:00:00.173824
elapsed time: 0:00:50.076343
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:44:20.464000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.81
 ---- batch: 020 ----
mean loss: 347.58
 ---- batch: 030 ----
mean loss: 354.58
train mean loss: 347.78
epoch train time: 0:00:00.174997
elapsed time: 0:00:50.251498
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:44:20.639152
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.82
 ---- batch: 020 ----
mean loss: 355.13
 ---- batch: 030 ----
mean loss: 345.09
train mean loss: 347.68
epoch train time: 0:00:00.176050
elapsed time: 0:00:50.427707
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:44:20.815347
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.31
 ---- batch: 020 ----
mean loss: 345.97
 ---- batch: 030 ----
mean loss: 344.27
train mean loss: 347.56
epoch train time: 0:00:00.173366
elapsed time: 0:00:50.601222
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:44:20.988863
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.70
 ---- batch: 020 ----
mean loss: 344.42
 ---- batch: 030 ----
mean loss: 349.30
train mean loss: 348.43
epoch train time: 0:00:00.178414
elapsed time: 0:00:50.779777
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:44:21.167415
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.04
 ---- batch: 020 ----
mean loss: 343.70
 ---- batch: 030 ----
mean loss: 348.70
train mean loss: 347.88
epoch train time: 0:00:00.175714
elapsed time: 0:00:50.955629
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:44:21.343268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.80
 ---- batch: 020 ----
mean loss: 349.31
 ---- batch: 030 ----
mean loss: 345.52
train mean loss: 347.61
epoch train time: 0:00:00.176171
elapsed time: 0:00:51.131949
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:44:21.519587
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.93
 ---- batch: 020 ----
mean loss: 345.50
 ---- batch: 030 ----
mean loss: 350.70
train mean loss: 348.30
epoch train time: 0:00:00.177437
elapsed time: 0:00:51.309540
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:44:21.697185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.54
 ---- batch: 020 ----
mean loss: 347.84
 ---- batch: 030 ----
mean loss: 344.62
train mean loss: 348.11
epoch train time: 0:00:00.180921
elapsed time: 0:00:51.490610
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:44:21.878248
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.87
 ---- batch: 020 ----
mean loss: 350.58
 ---- batch: 030 ----
mean loss: 342.73
train mean loss: 347.88
epoch train time: 0:00:00.177112
elapsed time: 0:00:51.667875
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:44:22.055531
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.34
 ---- batch: 020 ----
mean loss: 343.66
 ---- batch: 030 ----
mean loss: 346.51
train mean loss: 347.76
epoch train time: 0:00:00.174201
elapsed time: 0:00:51.842254
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:44:22.229892
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.21
 ---- batch: 020 ----
mean loss: 346.21
 ---- batch: 030 ----
mean loss: 344.52
train mean loss: 347.88
epoch train time: 0:00:00.172983
elapsed time: 0:00:52.015385
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:44:22.403025
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.14
 ---- batch: 020 ----
mean loss: 335.00
 ---- batch: 030 ----
mean loss: 351.10
train mean loss: 347.64
epoch train time: 0:00:00.173892
elapsed time: 0:00:52.189417
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:44:22.577055
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.64
 ---- batch: 020 ----
mean loss: 345.65
 ---- batch: 030 ----
mean loss: 348.58
train mean loss: 347.72
epoch train time: 0:00:00.180037
elapsed time: 0:00:52.369601
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:44:22.757246
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.18
 ---- batch: 020 ----
mean loss: 348.29
 ---- batch: 030 ----
mean loss: 350.18
train mean loss: 347.92
epoch train time: 0:00:00.180615
elapsed time: 0:00:52.550365
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:44:22.938022
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.57
 ---- batch: 020 ----
mean loss: 346.59
 ---- batch: 030 ----
mean loss: 344.22
train mean loss: 347.87
epoch train time: 0:00:00.175380
elapsed time: 0:00:52.725903
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:44:23.113539
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.52
 ---- batch: 020 ----
mean loss: 343.21
 ---- batch: 030 ----
mean loss: 345.74
train mean loss: 347.57
epoch train time: 0:00:00.173404
elapsed time: 0:00:52.899439
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:44:23.287076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.44
 ---- batch: 020 ----
mean loss: 341.41
 ---- batch: 030 ----
mean loss: 351.26
train mean loss: 347.58
epoch train time: 0:00:00.170460
elapsed time: 0:00:53.070070
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:44:23.457712
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.64
 ---- batch: 020 ----
mean loss: 356.85
 ---- batch: 030 ----
mean loss: 349.14
train mean loss: 347.57
epoch train time: 0:00:00.172874
elapsed time: 0:00:53.243086
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:44:23.630724
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 343.84
 ---- batch: 020 ----
mean loss: 350.12
 ---- batch: 030 ----
mean loss: 348.19
train mean loss: 348.25
epoch train time: 0:00:00.190359
elapsed time: 0:00:53.433587
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:44:23.821227
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.49
 ---- batch: 020 ----
mean loss: 354.80
 ---- batch: 030 ----
mean loss: 339.74
train mean loss: 347.23
epoch train time: 0:00:00.177055
elapsed time: 0:00:53.610783
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:44:23.998421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.71
 ---- batch: 020 ----
mean loss: 338.32
 ---- batch: 030 ----
mean loss: 355.41
train mean loss: 347.50
epoch train time: 0:00:00.178779
elapsed time: 0:00:53.789701
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:44:24.177339
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.01
 ---- batch: 020 ----
mean loss: 341.57
 ---- batch: 030 ----
mean loss: 355.20
train mean loss: 347.75
epoch train time: 0:00:00.174414
elapsed time: 0:00:53.964254
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:44:24.351919
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.03
 ---- batch: 020 ----
mean loss: 351.30
 ---- batch: 030 ----
mean loss: 350.73
train mean loss: 347.50
epoch train time: 0:00:00.183774
elapsed time: 0:00:54.148197
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:44:24.535836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.23
 ---- batch: 020 ----
mean loss: 347.47
 ---- batch: 030 ----
mean loss: 349.30
train mean loss: 347.84
epoch train time: 0:00:00.176456
elapsed time: 0:00:54.324794
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:44:24.712432
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 338.42
 ---- batch: 020 ----
mean loss: 346.59
 ---- batch: 030 ----
mean loss: 351.78
train mean loss: 347.32
epoch train time: 0:00:00.174317
elapsed time: 0:00:54.499250
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:44:24.886890
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.97
 ---- batch: 020 ----
mean loss: 350.26
 ---- batch: 030 ----
mean loss: 342.17
train mean loss: 347.69
epoch train time: 0:00:00.175198
elapsed time: 0:00:54.674591
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:44:25.062229
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.57
 ---- batch: 020 ----
mean loss: 353.14
 ---- batch: 030 ----
mean loss: 346.23
train mean loss: 347.57
epoch train time: 0:00:00.175584
elapsed time: 0:00:54.850315
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:44:25.237972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.79
 ---- batch: 020 ----
mean loss: 354.01
 ---- batch: 030 ----
mean loss: 336.57
train mean loss: 347.62
epoch train time: 0:00:00.172570
elapsed time: 0:00:55.023043
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:44:25.410699
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.56
 ---- batch: 020 ----
mean loss: 352.91
 ---- batch: 030 ----
mean loss: 345.87
train mean loss: 347.58
epoch train time: 0:00:00.175892
elapsed time: 0:00:55.199092
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:44:25.586731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 356.30
 ---- batch: 020 ----
mean loss: 342.22
 ---- batch: 030 ----
mean loss: 345.83
train mean loss: 347.06
epoch train time: 0:00:00.174674
elapsed time: 0:00:55.373925
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:44:25.761565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.85
 ---- batch: 020 ----
mean loss: 347.07
 ---- batch: 030 ----
mean loss: 346.53
train mean loss: 347.32
epoch train time: 0:00:00.180463
elapsed time: 0:00:55.554530
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:44:25.942166
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.85
 ---- batch: 020 ----
mean loss: 343.28
 ---- batch: 030 ----
mean loss: 347.62
train mean loss: 347.80
epoch train time: 0:00:00.172922
elapsed time: 0:00:55.727623
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:44:26.115266
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.01
 ---- batch: 020 ----
mean loss: 353.02
 ---- batch: 030 ----
mean loss: 345.51
train mean loss: 347.15
epoch train time: 0:00:00.172922
elapsed time: 0:00:55.900686
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:44:26.288324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.74
 ---- batch: 020 ----
mean loss: 349.76
 ---- batch: 030 ----
mean loss: 340.21
train mean loss: 347.60
epoch train time: 0:00:00.169671
elapsed time: 0:00:56.070496
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:44:26.458134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.01
 ---- batch: 020 ----
mean loss: 344.01
 ---- batch: 030 ----
mean loss: 347.53
train mean loss: 347.52
epoch train time: 0:00:00.173058
elapsed time: 0:00:56.244314
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:44:26.631977
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.55
 ---- batch: 020 ----
mean loss: 358.64
 ---- batch: 030 ----
mean loss: 342.69
train mean loss: 347.56
epoch train time: 0:00:00.174560
elapsed time: 0:00:56.419037
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:44:26.806676
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.73
 ---- batch: 020 ----
mean loss: 349.06
 ---- batch: 030 ----
mean loss: 341.64
train mean loss: 347.57
epoch train time: 0:00:00.185769
elapsed time: 0:00:56.604949
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:44:26.992588
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.83
 ---- batch: 020 ----
mean loss: 345.57
 ---- batch: 030 ----
mean loss: 353.08
train mean loss: 347.37
epoch train time: 0:00:00.178056
elapsed time: 0:00:56.783146
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:44:27.170796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.65
 ---- batch: 020 ----
mean loss: 342.26
 ---- batch: 030 ----
mean loss: 349.06
train mean loss: 347.15
epoch train time: 0:00:00.176990
elapsed time: 0:00:56.962437
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_3/checkpoint.pth.tar
**** end time: 2019-09-27 16:44:27.350043 ****
