Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 7952
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-20 23:26:41.300061 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:26:41.304935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4757.00
 ---- batch: 020 ----
mean loss: 4724.62
 ---- batch: 030 ----
mean loss: 4753.46
 ---- batch: 040 ----
mean loss: 4649.71
train mean loss: 4711.48
epoch train time: 0:00:14.721502
elapsed time: 0:00:14.727568
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:26:56.027669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4621.73
 ---- batch: 020 ----
mean loss: 4499.23
 ---- batch: 030 ----
mean loss: 4412.21
 ---- batch: 040 ----
mean loss: 4437.32
train mean loss: 4479.15
epoch train time: 0:00:00.208960
elapsed time: 0:00:14.936646
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:26:56.236760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4225.39
 ---- batch: 020 ----
mean loss: 4170.26
 ---- batch: 030 ----
mean loss: 4140.74
 ---- batch: 040 ----
mean loss: 3983.62
train mean loss: 4110.17
epoch train time: 0:00:00.205513
elapsed time: 0:00:15.142291
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:26:56.442396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3831.81
 ---- batch: 020 ----
mean loss: 3857.96
 ---- batch: 030 ----
mean loss: 3574.49
 ---- batch: 040 ----
mean loss: 3626.43
train mean loss: 3719.41
epoch train time: 0:00:00.204099
elapsed time: 0:00:15.347126
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:26:56.647238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3523.15
 ---- batch: 020 ----
mean loss: 3407.13
 ---- batch: 030 ----
mean loss: 3369.98
 ---- batch: 040 ----
mean loss: 3270.90
train mean loss: 3381.96
epoch train time: 0:00:00.204877
elapsed time: 0:00:15.552128
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:26:56.852231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3174.28
 ---- batch: 020 ----
mean loss: 3176.42
 ---- batch: 030 ----
mean loss: 3067.01
 ---- batch: 040 ----
mean loss: 2991.50
train mean loss: 3095.90
epoch train time: 0:00:00.208123
elapsed time: 0:00:15.760433
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:26:57.060557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2912.99
 ---- batch: 020 ----
mean loss: 2925.54
 ---- batch: 030 ----
mean loss: 2844.29
 ---- batch: 040 ----
mean loss: 2716.54
train mean loss: 2843.49
epoch train time: 0:00:00.204212
elapsed time: 0:00:15.964788
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:26:57.264895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2715.70
 ---- batch: 020 ----
mean loss: 2604.80
 ---- batch: 030 ----
mean loss: 2630.02
 ---- batch: 040 ----
mean loss: 2545.51
train mean loss: 2619.25
epoch train time: 0:00:00.209230
elapsed time: 0:00:16.174140
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:26:57.474254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2548.78
 ---- batch: 020 ----
mean loss: 2424.50
 ---- batch: 030 ----
mean loss: 2386.20
 ---- batch: 040 ----
mean loss: 2335.67
train mean loss: 2417.72
epoch train time: 0:00:00.212956
elapsed time: 0:00:16.387251
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:26:57.687379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2294.83
 ---- batch: 020 ----
mean loss: 2246.21
 ---- batch: 030 ----
mean loss: 2220.43
 ---- batch: 040 ----
mean loss: 2212.73
train mean loss: 2236.03
epoch train time: 0:00:00.210830
elapsed time: 0:00:16.598229
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:26:57.898335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2121.95
 ---- batch: 020 ----
mean loss: 2100.99
 ---- batch: 030 ----
mean loss: 2072.93
 ---- batch: 040 ----
mean loss: 1990.22
train mean loss: 2072.32
epoch train time: 0:00:00.210072
elapsed time: 0:00:16.808445
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:26:58.108551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2011.10
 ---- batch: 020 ----
mean loss: 1921.31
 ---- batch: 030 ----
mean loss: 1892.89
 ---- batch: 040 ----
mean loss: 1879.12
train mean loss: 1922.82
epoch train time: 0:00:00.210683
elapsed time: 0:00:17.019260
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:26:58.319367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1826.08
 ---- batch: 020 ----
mean loss: 1808.93
 ---- batch: 030 ----
mean loss: 1764.21
 ---- batch: 040 ----
mean loss: 1764.44
train mean loss: 1786.87
epoch train time: 0:00:00.211142
elapsed time: 0:00:17.230544
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:26:58.530651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1703.25
 ---- batch: 020 ----
mean loss: 1680.75
 ---- batch: 030 ----
mean loss: 1649.11
 ---- batch: 040 ----
mean loss: 1635.34
train mean loss: 1665.51
epoch train time: 0:00:00.212500
elapsed time: 0:00:17.443181
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:26:58.743288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1604.51
 ---- batch: 020 ----
mean loss: 1561.40
 ---- batch: 030 ----
mean loss: 1534.84
 ---- batch: 040 ----
mean loss: 1532.35
train mean loss: 1552.89
epoch train time: 0:00:00.211245
elapsed time: 0:00:17.654569
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:26:58.954675
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1470.38
 ---- batch: 020 ----
mean loss: 1481.56
 ---- batch: 030 ----
mean loss: 1452.01
 ---- batch: 040 ----
mean loss: 1418.97
train mean loss: 1452.33
epoch train time: 0:00:00.206375
elapsed time: 0:00:17.861065
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:26:59.161170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1378.85
 ---- batch: 020 ----
mean loss: 1368.42
 ---- batch: 030 ----
mean loss: 1359.03
 ---- batch: 040 ----
mean loss: 1343.45
train mean loss: 1362.28
epoch train time: 0:00:00.208414
elapsed time: 0:00:18.069618
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:26:59.369722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1301.30
 ---- batch: 020 ----
mean loss: 1291.49
 ---- batch: 030 ----
mean loss: 1272.71
 ---- batch: 040 ----
mean loss: 1258.32
train mean loss: 1279.55
epoch train time: 0:00:00.204776
elapsed time: 0:00:18.274509
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:26:59.574610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1245.65
 ---- batch: 020 ----
mean loss: 1192.98
 ---- batch: 030 ----
mean loss: 1212.79
 ---- batch: 040 ----
mean loss: 1183.08
train mean loss: 1205.32
epoch train time: 0:00:00.215288
elapsed time: 0:00:18.489913
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:26:59.790016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1150.87
 ---- batch: 020 ----
mean loss: 1154.01
 ---- batch: 030 ----
mean loss: 1149.97
 ---- batch: 040 ----
mean loss: 1107.32
train mean loss: 1140.55
epoch train time: 0:00:00.215622
elapsed time: 0:00:18.705653
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:27:00.005769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1098.81
 ---- batch: 020 ----
mean loss: 1091.24
 ---- batch: 030 ----
mean loss: 1075.92
 ---- batch: 040 ----
mean loss: 1062.63
train mean loss: 1080.56
epoch train time: 0:00:00.204633
elapsed time: 0:00:18.910416
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:27:00.210540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1051.09
 ---- batch: 020 ----
mean loss: 1023.99
 ---- batch: 030 ----
mean loss: 1021.71
 ---- batch: 040 ----
mean loss: 1020.11
train mean loss: 1027.07
epoch train time: 0:00:00.205801
elapsed time: 0:00:19.116378
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:27:00.416485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1000.49
 ---- batch: 020 ----
mean loss: 992.67
 ---- batch: 030 ----
mean loss: 974.79
 ---- batch: 040 ----
mean loss: 956.42
train mean loss: 980.20
epoch train time: 0:00:00.203731
elapsed time: 0:00:19.320238
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:27:00.620342
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 965.30
 ---- batch: 020 ----
mean loss: 951.62
 ---- batch: 030 ----
mean loss: 932.26
 ---- batch: 040 ----
mean loss: 916.55
train mean loss: 938.34
epoch train time: 0:00:00.203627
elapsed time: 0:00:19.523982
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:27:00.824091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 925.86
 ---- batch: 020 ----
mean loss: 907.40
 ---- batch: 030 ----
mean loss: 902.87
 ---- batch: 040 ----
mean loss: 875.40
train mean loss: 901.63
epoch train time: 0:00:00.202860
elapsed time: 0:00:19.726963
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:27:01.027066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.90
 ---- batch: 020 ----
mean loss: 886.55
 ---- batch: 030 ----
mean loss: 865.80
 ---- batch: 040 ----
mean loss: 854.74
train mean loss: 868.44
epoch train time: 0:00:00.198374
elapsed time: 0:00:19.925450
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:27:01.225552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.09
 ---- batch: 020 ----
mean loss: 831.05
 ---- batch: 030 ----
mean loss: 838.14
 ---- batch: 040 ----
mean loss: 835.67
train mean loss: 839.73
epoch train time: 0:00:00.199940
elapsed time: 0:00:20.125510
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:27:01.425616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 822.44
 ---- batch: 020 ----
mean loss: 820.37
 ---- batch: 030 ----
mean loss: 811.04
 ---- batch: 040 ----
mean loss: 811.59
train mean loss: 814.92
epoch train time: 0:00:00.209721
elapsed time: 0:00:20.335385
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:27:01.635490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.51
 ---- batch: 020 ----
mean loss: 783.98
 ---- batch: 030 ----
mean loss: 789.92
 ---- batch: 040 ----
mean loss: 786.95
train mean loss: 793.06
epoch train time: 0:00:00.210981
elapsed time: 0:00:20.546489
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:27:01.846610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 781.00
 ---- batch: 020 ----
mean loss: 763.10
 ---- batch: 030 ----
mean loss: 784.50
 ---- batch: 040 ----
mean loss: 765.68
train mean loss: 774.39
epoch train time: 0:00:00.209278
elapsed time: 0:00:20.755906
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:27:02.056012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.68
 ---- batch: 020 ----
mean loss: 753.27
 ---- batch: 030 ----
mean loss: 762.11
 ---- batch: 040 ----
mean loss: 754.87
train mean loss: 757.29
epoch train time: 0:00:00.207888
elapsed time: 0:00:20.963946
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:27:02.264068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 760.43
 ---- batch: 020 ----
mean loss: 735.04
 ---- batch: 030 ----
mean loss: 750.97
 ---- batch: 040 ----
mean loss: 731.66
train mean loss: 743.00
epoch train time: 0:00:00.206091
elapsed time: 0:00:21.170171
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:27:02.470284
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 757.25
 ---- batch: 020 ----
mean loss: 715.73
 ---- batch: 030 ----
mean loss: 717.89
 ---- batch: 040 ----
mean loss: 740.60
train mean loss: 730.76
epoch train time: 0:00:00.210138
elapsed time: 0:00:21.380442
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:27:02.680562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 722.02
 ---- batch: 020 ----
mean loss: 722.09
 ---- batch: 030 ----
mean loss: 725.94
 ---- batch: 040 ----
mean loss: 715.30
train mean loss: 720.18
epoch train time: 0:00:00.211044
elapsed time: 0:00:21.591623
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:27:02.891729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.28
 ---- batch: 020 ----
mean loss: 713.74
 ---- batch: 030 ----
mean loss: 699.51
 ---- batch: 040 ----
mean loss: 714.45
train mean loss: 711.71
epoch train time: 0:00:00.206686
elapsed time: 0:00:21.798433
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:27:03.098539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 708.48
 ---- batch: 020 ----
mean loss: 694.88
 ---- batch: 030 ----
mean loss: 697.91
 ---- batch: 040 ----
mean loss: 719.66
train mean loss: 703.31
epoch train time: 0:00:00.208801
elapsed time: 0:00:22.007356
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:27:03.307458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 697.73
 ---- batch: 020 ----
mean loss: 704.45
 ---- batch: 030 ----
mean loss: 698.71
 ---- batch: 040 ----
mean loss: 699.96
train mean loss: 697.37
epoch train time: 0:00:00.201743
elapsed time: 0:00:22.209223
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:27:03.509328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.72
 ---- batch: 020 ----
mean loss: 683.96
 ---- batch: 030 ----
mean loss: 683.39
 ---- batch: 040 ----
mean loss: 709.93
train mean loss: 692.06
epoch train time: 0:00:00.207416
elapsed time: 0:00:22.416786
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:27:03.716899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 691.52
 ---- batch: 020 ----
mean loss: 674.77
 ---- batch: 030 ----
mean loss: 706.76
 ---- batch: 040 ----
mean loss: 684.28
train mean loss: 687.14
epoch train time: 0:00:00.208759
elapsed time: 0:00:22.625674
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:27:03.925779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 696.64
 ---- batch: 020 ----
mean loss: 685.01
 ---- batch: 030 ----
mean loss: 682.72
 ---- batch: 040 ----
mean loss: 675.56
train mean loss: 682.93
epoch train time: 0:00:00.195703
elapsed time: 0:00:22.821494
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:27:04.121596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.61
 ---- batch: 020 ----
mean loss: 697.15
 ---- batch: 030 ----
mean loss: 682.61
 ---- batch: 040 ----
mean loss: 673.34
train mean loss: 680.10
epoch train time: 0:00:00.197915
elapsed time: 0:00:23.019584
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:27:04.319687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.51
 ---- batch: 020 ----
mean loss: 679.00
 ---- batch: 030 ----
mean loss: 671.28
 ---- batch: 040 ----
mean loss: 664.24
train mean loss: 677.55
epoch train time: 0:00:00.194990
elapsed time: 0:00:23.214690
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:27:04.514790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 675.56
 ---- batch: 020 ----
mean loss: 675.77
 ---- batch: 030 ----
mean loss: 678.69
 ---- batch: 040 ----
mean loss: 677.41
train mean loss: 675.31
epoch train time: 0:00:00.197194
elapsed time: 0:00:23.412007
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:27:04.712108
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.92
 ---- batch: 020 ----
mean loss: 662.49
 ---- batch: 030 ----
mean loss: 674.00
 ---- batch: 040 ----
mean loss: 667.37
train mean loss: 674.07
epoch train time: 0:00:00.205460
elapsed time: 0:00:23.617600
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:27:04.917703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 674.42
 ---- batch: 020 ----
mean loss: 646.57
 ---- batch: 030 ----
mean loss: 676.63
 ---- batch: 040 ----
mean loss: 686.90
train mean loss: 671.92
epoch train time: 0:00:00.199953
elapsed time: 0:00:23.817694
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:27:05.117866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.09
 ---- batch: 020 ----
mean loss: 653.49
 ---- batch: 030 ----
mean loss: 670.74
 ---- batch: 040 ----
mean loss: 676.48
train mean loss: 670.92
epoch train time: 0:00:00.207727
elapsed time: 0:00:24.025610
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:27:05.325716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 663.11
 ---- batch: 020 ----
mean loss: 678.90
 ---- batch: 030 ----
mean loss: 659.62
 ---- batch: 040 ----
mean loss: 685.29
train mean loss: 669.62
epoch train time: 0:00:00.209005
elapsed time: 0:00:24.234790
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:27:05.534895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 666.29
 ---- batch: 020 ----
mean loss: 679.74
 ---- batch: 030 ----
mean loss: 665.77
 ---- batch: 040 ----
mean loss: 673.53
train mean loss: 668.72
epoch train time: 0:00:00.211592
elapsed time: 0:00:24.446504
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:27:05.746622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 670.23
 ---- batch: 020 ----
mean loss: 678.70
 ---- batch: 030 ----
mean loss: 656.30
 ---- batch: 040 ----
mean loss: 673.65
train mean loss: 667.65
epoch train time: 0:00:00.208769
elapsed time: 0:00:24.655406
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:27:05.955510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 656.15
 ---- batch: 020 ----
mean loss: 660.11
 ---- batch: 030 ----
mean loss: 680.58
 ---- batch: 040 ----
mean loss: 676.61
train mean loss: 667.17
epoch train time: 0:00:00.207007
elapsed time: 0:00:24.862547
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:27:06.162650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 662.31
 ---- batch: 020 ----
mean loss: 677.18
 ---- batch: 030 ----
mean loss: 656.46
 ---- batch: 040 ----
mean loss: 662.02
train mean loss: 665.09
epoch train time: 0:00:00.202417
elapsed time: 0:00:25.065080
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:27:06.365182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 643.41
 ---- batch: 020 ----
mean loss: 641.97
 ---- batch: 030 ----
mean loss: 636.29
 ---- batch: 040 ----
mean loss: 600.74
train mean loss: 627.27
epoch train time: 0:00:00.206061
elapsed time: 0:00:25.271282
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:27:06.571387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.44
 ---- batch: 020 ----
mean loss: 473.71
 ---- batch: 030 ----
mean loss: 440.95
 ---- batch: 040 ----
mean loss: 426.04
train mean loss: 465.80
epoch train time: 0:00:00.214086
elapsed time: 0:00:25.485508
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:27:06.785615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.63
 ---- batch: 020 ----
mean loss: 423.88
 ---- batch: 030 ----
mean loss: 419.37
 ---- batch: 040 ----
mean loss: 414.03
train mean loss: 419.66
epoch train time: 0:00:00.211848
elapsed time: 0:00:25.697480
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:27:06.997586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.93
 ---- batch: 020 ----
mean loss: 410.46
 ---- batch: 030 ----
mean loss: 407.99
 ---- batch: 040 ----
mean loss: 401.74
train mean loss: 407.31
epoch train time: 0:00:00.205304
elapsed time: 0:00:25.902903
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:27:07.203033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.73
 ---- batch: 020 ----
mean loss: 391.80
 ---- batch: 030 ----
mean loss: 397.81
 ---- batch: 040 ----
mean loss: 392.88
train mean loss: 397.37
epoch train time: 0:00:00.214657
elapsed time: 0:00:26.117705
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:27:07.417808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.63
 ---- batch: 020 ----
mean loss: 383.89
 ---- batch: 030 ----
mean loss: 388.54
 ---- batch: 040 ----
mean loss: 391.62
train mean loss: 389.35
epoch train time: 0:00:00.206478
elapsed time: 0:00:26.324315
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:27:07.624540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.34
 ---- batch: 020 ----
mean loss: 380.51
 ---- batch: 030 ----
mean loss: 381.73
 ---- batch: 040 ----
mean loss: 378.07
train mean loss: 382.24
epoch train time: 0:00:00.204600
elapsed time: 0:00:26.529157
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:27:07.829262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.86
 ---- batch: 020 ----
mean loss: 381.82
 ---- batch: 030 ----
mean loss: 376.32
 ---- batch: 040 ----
mean loss: 375.37
train mean loss: 376.15
epoch train time: 0:00:00.203651
elapsed time: 0:00:26.732925
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:27:08.033027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.87
 ---- batch: 020 ----
mean loss: 375.46
 ---- batch: 030 ----
mean loss: 371.43
 ---- batch: 040 ----
mean loss: 370.37
train mean loss: 370.37
epoch train time: 0:00:00.203761
elapsed time: 0:00:26.936804
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:27:08.236907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.87
 ---- batch: 020 ----
mean loss: 364.65
 ---- batch: 030 ----
mean loss: 370.76
 ---- batch: 040 ----
mean loss: 359.56
train mean loss: 365.76
epoch train time: 0:00:00.201042
elapsed time: 0:00:27.137960
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:27:08.438060
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.15
 ---- batch: 020 ----
mean loss: 366.46
 ---- batch: 030 ----
mean loss: 357.10
 ---- batch: 040 ----
mean loss: 365.01
train mean loss: 361.46
epoch train time: 0:00:00.203394
elapsed time: 0:00:27.341471
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:27:08.641576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.46
 ---- batch: 020 ----
mean loss: 359.60
 ---- batch: 030 ----
mean loss: 355.10
 ---- batch: 040 ----
mean loss: 355.83
train mean loss: 357.35
epoch train time: 0:00:00.206125
elapsed time: 0:00:27.547772
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:27:08.847877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.98
 ---- batch: 020 ----
mean loss: 357.44
 ---- batch: 030 ----
mean loss: 357.61
 ---- batch: 040 ----
mean loss: 352.35
train mean loss: 353.78
epoch train time: 0:00:00.207333
elapsed time: 0:00:27.755228
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:27:09.055334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.48
 ---- batch: 020 ----
mean loss: 352.78
 ---- batch: 030 ----
mean loss: 354.44
 ---- batch: 040 ----
mean loss: 338.14
train mean loss: 350.53
epoch train time: 0:00:00.210472
elapsed time: 0:00:27.965829
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:27:09.265937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.78
 ---- batch: 020 ----
mean loss: 349.21
 ---- batch: 030 ----
mean loss: 349.12
 ---- batch: 040 ----
mean loss: 344.72
train mean loss: 348.18
epoch train time: 0:00:00.210537
elapsed time: 0:00:28.176491
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:27:09.476627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.38
 ---- batch: 020 ----
mean loss: 329.86
 ---- batch: 030 ----
mean loss: 349.45
 ---- batch: 040 ----
mean loss: 348.14
train mean loss: 345.47
epoch train time: 0:00:00.210646
elapsed time: 0:00:28.387297
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:27:09.687422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.53
 ---- batch: 020 ----
mean loss: 344.31
 ---- batch: 030 ----
mean loss: 339.22
 ---- batch: 040 ----
mean loss: 343.98
train mean loss: 343.16
epoch train time: 0:00:00.208286
elapsed time: 0:00:28.595721
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:27:09.895851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.99
 ---- batch: 020 ----
mean loss: 329.59
 ---- batch: 030 ----
mean loss: 344.50
 ---- batch: 040 ----
mean loss: 346.79
train mean loss: 341.23
epoch train time: 0:00:00.207302
elapsed time: 0:00:28.803170
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:27:10.103289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.01
 ---- batch: 020 ----
mean loss: 338.08
 ---- batch: 030 ----
mean loss: 330.59
 ---- batch: 040 ----
mean loss: 335.74
train mean loss: 339.64
epoch train time: 0:00:00.206665
elapsed time: 0:00:29.009987
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:27:10.310094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.29
 ---- batch: 020 ----
mean loss: 341.08
 ---- batch: 030 ----
mean loss: 342.62
 ---- batch: 040 ----
mean loss: 331.11
train mean loss: 337.88
epoch train time: 0:00:00.207807
elapsed time: 0:00:29.217918
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:27:10.518040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.10
 ---- batch: 020 ----
mean loss: 340.39
 ---- batch: 030 ----
mean loss: 338.66
 ---- batch: 040 ----
mean loss: 333.59
train mean loss: 336.67
epoch train time: 0:00:00.207152
elapsed time: 0:00:29.425208
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:27:10.725310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.32
 ---- batch: 020 ----
mean loss: 338.28
 ---- batch: 030 ----
mean loss: 333.39
 ---- batch: 040 ----
mean loss: 337.81
train mean loss: 335.04
epoch train time: 0:00:00.210894
elapsed time: 0:00:29.636225
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:27:10.936328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.92
 ---- batch: 020 ----
mean loss: 325.49
 ---- batch: 030 ----
mean loss: 333.97
 ---- batch: 040 ----
mean loss: 334.55
train mean loss: 334.06
epoch train time: 0:00:00.201144
elapsed time: 0:00:29.837481
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:27:11.137597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.90
 ---- batch: 020 ----
mean loss: 332.51
 ---- batch: 030 ----
mean loss: 322.01
 ---- batch: 040 ----
mean loss: 339.63
train mean loss: 333.67
epoch train time: 0:00:00.195403
elapsed time: 0:00:30.033009
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:27:11.333109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.18
 ---- batch: 020 ----
mean loss: 333.55
 ---- batch: 030 ----
mean loss: 327.57
 ---- batch: 040 ----
mean loss: 341.08
train mean loss: 331.88
epoch train time: 0:00:00.195818
elapsed time: 0:00:30.228941
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:27:11.529045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.41
 ---- batch: 020 ----
mean loss: 331.20
 ---- batch: 030 ----
mean loss: 331.31
 ---- batch: 040 ----
mean loss: 333.06
train mean loss: 331.39
epoch train time: 0:00:00.199898
elapsed time: 0:00:30.429750
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:27:11.729863
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.27
 ---- batch: 020 ----
mean loss: 335.60
 ---- batch: 030 ----
mean loss: 324.65
 ---- batch: 040 ----
mean loss: 331.55
train mean loss: 330.53
epoch train time: 0:00:00.206415
elapsed time: 0:00:30.636292
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:27:11.936414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.68
 ---- batch: 020 ----
mean loss: 330.32
 ---- batch: 030 ----
mean loss: 326.05
 ---- batch: 040 ----
mean loss: 331.88
train mean loss: 329.71
epoch train time: 0:00:00.199338
elapsed time: 0:00:30.835807
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:27:12.135914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 330.02
 ---- batch: 020 ----
mean loss: 326.86
 ---- batch: 030 ----
mean loss: 337.31
 ---- batch: 040 ----
mean loss: 320.27
train mean loss: 329.14
epoch train time: 0:00:00.195375
elapsed time: 0:00:31.031298
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:27:12.331399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.21
 ---- batch: 020 ----
mean loss: 330.31
 ---- batch: 030 ----
mean loss: 336.61
 ---- batch: 040 ----
mean loss: 317.28
train mean loss: 328.46
epoch train time: 0:00:00.197188
elapsed time: 0:00:31.228602
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:27:12.528730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.17
 ---- batch: 020 ----
mean loss: 324.05
 ---- batch: 030 ----
mean loss: 324.32
 ---- batch: 040 ----
mean loss: 330.06
train mean loss: 327.49
epoch train time: 0:00:00.203893
elapsed time: 0:00:31.432643
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:27:12.732748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.70
 ---- batch: 020 ----
mean loss: 335.65
 ---- batch: 030 ----
mean loss: 323.09
 ---- batch: 040 ----
mean loss: 331.64
train mean loss: 327.51
epoch train time: 0:00:00.207879
elapsed time: 0:00:31.640673
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:27:12.940793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.32
 ---- batch: 020 ----
mean loss: 330.36
 ---- batch: 030 ----
mean loss: 317.88
 ---- batch: 040 ----
mean loss: 333.69
train mean loss: 326.54
epoch train time: 0:00:00.206271
elapsed time: 0:00:31.847096
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:27:13.147200
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.17
 ---- batch: 020 ----
mean loss: 327.75
 ---- batch: 030 ----
mean loss: 323.33
 ---- batch: 040 ----
mean loss: 328.61
train mean loss: 326.23
epoch train time: 0:00:00.205522
elapsed time: 0:00:32.052745
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:27:13.352849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.59
 ---- batch: 020 ----
mean loss: 325.70
 ---- batch: 030 ----
mean loss: 329.58
 ---- batch: 040 ----
mean loss: 332.08
train mean loss: 325.95
epoch train time: 0:00:00.213334
elapsed time: 0:00:32.266205
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:27:13.566326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.21
 ---- batch: 020 ----
mean loss: 329.26
 ---- batch: 030 ----
mean loss: 317.30
 ---- batch: 040 ----
mean loss: 327.67
train mean loss: 325.76
epoch train time: 0:00:00.207257
elapsed time: 0:00:32.473596
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:27:13.773698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.13
 ---- batch: 020 ----
mean loss: 316.66
 ---- batch: 030 ----
mean loss: 334.31
 ---- batch: 040 ----
mean loss: 325.37
train mean loss: 325.23
epoch train time: 0:00:00.210576
elapsed time: 0:00:32.684289
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:27:13.984429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.35
 ---- batch: 020 ----
mean loss: 327.78
 ---- batch: 030 ----
mean loss: 328.26
 ---- batch: 040 ----
mean loss: 320.75
train mean loss: 324.39
epoch train time: 0:00:00.210445
elapsed time: 0:00:32.894909
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:27:14.195015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.47
 ---- batch: 020 ----
mean loss: 319.55
 ---- batch: 030 ----
mean loss: 325.18
 ---- batch: 040 ----
mean loss: 325.50
train mean loss: 324.17
epoch train time: 0:00:00.208683
elapsed time: 0:00:33.103713
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:27:14.403828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.48
 ---- batch: 020 ----
mean loss: 322.07
 ---- batch: 030 ----
mean loss: 323.56
 ---- batch: 040 ----
mean loss: 326.74
train mean loss: 323.61
epoch train time: 0:00:00.202616
elapsed time: 0:00:33.306501
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:27:14.606603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.99
 ---- batch: 020 ----
mean loss: 328.14
 ---- batch: 030 ----
mean loss: 323.55
 ---- batch: 040 ----
mean loss: 324.41
train mean loss: 323.33
epoch train time: 0:00:00.203486
elapsed time: 0:00:33.510109
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:27:14.810214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.58
 ---- batch: 020 ----
mean loss: 328.93
 ---- batch: 030 ----
mean loss: 322.26
 ---- batch: 040 ----
mean loss: 321.65
train mean loss: 323.41
epoch train time: 0:00:00.214564
elapsed time: 0:00:33.724798
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:27:15.024949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.43
 ---- batch: 020 ----
mean loss: 322.18
 ---- batch: 030 ----
mean loss: 312.20
 ---- batch: 040 ----
mean loss: 329.68
train mean loss: 322.68
epoch train time: 0:00:00.202043
elapsed time: 0:00:33.927019
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:27:15.227122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.93
 ---- batch: 020 ----
mean loss: 328.23
 ---- batch: 030 ----
mean loss: 327.40
 ---- batch: 040 ----
mean loss: 317.28
train mean loss: 322.35
epoch train time: 0:00:00.202291
elapsed time: 0:00:34.129427
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:27:15.429531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.29
 ---- batch: 020 ----
mean loss: 324.48
 ---- batch: 030 ----
mean loss: 311.59
 ---- batch: 040 ----
mean loss: 322.98
train mean loss: 322.15
epoch train time: 0:00:00.200478
elapsed time: 0:00:34.330040
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:27:15.630150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.85
 ---- batch: 020 ----
mean loss: 321.09
 ---- batch: 030 ----
mean loss: 317.75
 ---- batch: 040 ----
mean loss: 329.63
train mean loss: 321.57
epoch train time: 0:00:00.201574
elapsed time: 0:00:34.531736
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:27:15.831840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.20
 ---- batch: 020 ----
mean loss: 321.70
 ---- batch: 030 ----
mean loss: 323.66
 ---- batch: 040 ----
mean loss: 321.72
train mean loss: 321.15
epoch train time: 0:00:00.200200
elapsed time: 0:00:34.732081
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:27:16.032183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.11
 ---- batch: 020 ----
mean loss: 321.56
 ---- batch: 030 ----
mean loss: 327.10
 ---- batch: 040 ----
mean loss: 316.50
train mean loss: 320.94
epoch train time: 0:00:00.198188
elapsed time: 0:00:34.930386
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:27:16.230489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.87
 ---- batch: 020 ----
mean loss: 322.69
 ---- batch: 030 ----
mean loss: 315.85
 ---- batch: 040 ----
mean loss: 327.67
train mean loss: 320.59
epoch train time: 0:00:00.200980
elapsed time: 0:00:35.131514
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:27:16.431624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.53
 ---- batch: 020 ----
mean loss: 323.05
 ---- batch: 030 ----
mean loss: 326.70
 ---- batch: 040 ----
mean loss: 317.16
train mean loss: 320.76
epoch train time: 0:00:00.206936
elapsed time: 0:00:35.338643
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:27:16.638764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.32
 ---- batch: 020 ----
mean loss: 322.03
 ---- batch: 030 ----
mean loss: 322.35
 ---- batch: 040 ----
mean loss: 313.91
train mean loss: 320.48
epoch train time: 0:00:00.212050
elapsed time: 0:00:35.550827
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:27:16.850932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.44
 ---- batch: 020 ----
mean loss: 328.50
 ---- batch: 030 ----
mean loss: 325.59
 ---- batch: 040 ----
mean loss: 308.38
train mean loss: 319.75
epoch train time: 0:00:00.212674
elapsed time: 0:00:35.763624
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:27:17.063769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.96
 ---- batch: 020 ----
mean loss: 326.33
 ---- batch: 030 ----
mean loss: 317.29
 ---- batch: 040 ----
mean loss: 316.75
train mean loss: 319.92
epoch train time: 0:00:00.210068
elapsed time: 0:00:35.973874
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:27:17.273992
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.74
 ---- batch: 020 ----
mean loss: 321.14
 ---- batch: 030 ----
mean loss: 309.40
 ---- batch: 040 ----
mean loss: 318.21
train mean loss: 319.70
epoch train time: 0:00:00.210511
elapsed time: 0:00:36.184536
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:27:17.484655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.76
 ---- batch: 020 ----
mean loss: 324.58
 ---- batch: 030 ----
mean loss: 318.55
 ---- batch: 040 ----
mean loss: 316.16
train mean loss: 319.35
epoch train time: 0:00:00.210825
elapsed time: 0:00:36.395498
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:27:17.695619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.69
 ---- batch: 020 ----
mean loss: 323.35
 ---- batch: 030 ----
mean loss: 312.17
 ---- batch: 040 ----
mean loss: 317.03
train mean loss: 318.81
epoch train time: 0:00:00.215176
elapsed time: 0:00:36.610830
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:27:17.910937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.19
 ---- batch: 020 ----
mean loss: 332.09
 ---- batch: 030 ----
mean loss: 315.99
 ---- batch: 040 ----
mean loss: 310.58
train mean loss: 318.44
epoch train time: 0:00:00.210607
elapsed time: 0:00:36.821563
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:27:18.121669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.12
 ---- batch: 020 ----
mean loss: 325.25
 ---- batch: 030 ----
mean loss: 311.37
 ---- batch: 040 ----
mean loss: 312.45
train mean loss: 318.57
epoch train time: 0:00:00.204453
elapsed time: 0:00:37.026154
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:27:18.326266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.87
 ---- batch: 020 ----
mean loss: 325.01
 ---- batch: 030 ----
mean loss: 322.55
 ---- batch: 040 ----
mean loss: 312.66
train mean loss: 318.63
epoch train time: 0:00:00.200329
elapsed time: 0:00:37.226609
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:27:18.526713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.82
 ---- batch: 020 ----
mean loss: 304.16
 ---- batch: 030 ----
mean loss: 326.21
 ---- batch: 040 ----
mean loss: 326.44
train mean loss: 317.58
epoch train time: 0:00:00.206402
elapsed time: 0:00:37.433125
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:27:18.733227
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.08
 ---- batch: 020 ----
mean loss: 310.74
 ---- batch: 030 ----
mean loss: 318.98
 ---- batch: 040 ----
mean loss: 319.15
train mean loss: 317.60
epoch train time: 0:00:00.201820
elapsed time: 0:00:37.635076
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:27:18.935178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.14
 ---- batch: 020 ----
mean loss: 312.89
 ---- batch: 030 ----
mean loss: 317.82
 ---- batch: 040 ----
mean loss: 322.02
train mean loss: 317.58
epoch train time: 0:00:00.205956
elapsed time: 0:00:37.841147
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:27:19.141250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.67
 ---- batch: 020 ----
mean loss: 325.58
 ---- batch: 030 ----
mean loss: 318.55
 ---- batch: 040 ----
mean loss: 313.33
train mean loss: 317.08
epoch train time: 0:00:00.203498
elapsed time: 0:00:38.044767
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:27:19.344873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.68
 ---- batch: 020 ----
mean loss: 317.49
 ---- batch: 030 ----
mean loss: 313.21
 ---- batch: 040 ----
mean loss: 312.85
train mean loss: 317.39
epoch train time: 0:00:00.202970
elapsed time: 0:00:38.247857
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:27:19.547960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.47
 ---- batch: 020 ----
mean loss: 314.21
 ---- batch: 030 ----
mean loss: 319.68
 ---- batch: 040 ----
mean loss: 315.70
train mean loss: 316.86
epoch train time: 0:00:00.205726
elapsed time: 0:00:38.453733
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:27:19.753835
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.58
 ---- batch: 020 ----
mean loss: 317.19
 ---- batch: 030 ----
mean loss: 311.68
 ---- batch: 040 ----
mean loss: 312.99
train mean loss: 316.78
epoch train time: 0:00:00.202891
elapsed time: 0:00:38.656774
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:27:19.956908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.01
 ---- batch: 020 ----
mean loss: 325.24
 ---- batch: 030 ----
mean loss: 316.46
 ---- batch: 040 ----
mean loss: 312.74
train mean loss: 316.29
epoch train time: 0:00:00.209539
elapsed time: 0:00:38.866461
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:27:20.166565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.44
 ---- batch: 020 ----
mean loss: 309.37
 ---- batch: 030 ----
mean loss: 315.42
 ---- batch: 040 ----
mean loss: 315.95
train mean loss: 316.54
epoch train time: 0:00:00.211888
elapsed time: 0:00:39.078487
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:27:20.378592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.13
 ---- batch: 020 ----
mean loss: 315.82
 ---- batch: 030 ----
mean loss: 313.25
 ---- batch: 040 ----
mean loss: 319.99
train mean loss: 315.86
epoch train time: 0:00:00.210400
elapsed time: 0:00:39.289029
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:27:20.589134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.10
 ---- batch: 020 ----
mean loss: 316.57
 ---- batch: 030 ----
mean loss: 315.17
 ---- batch: 040 ----
mean loss: 313.74
train mean loss: 315.82
epoch train time: 0:00:00.211860
elapsed time: 0:00:39.501017
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:27:20.801137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.80
 ---- batch: 020 ----
mean loss: 323.66
 ---- batch: 030 ----
mean loss: 315.01
 ---- batch: 040 ----
mean loss: 311.88
train mean loss: 315.47
epoch train time: 0:00:00.209430
elapsed time: 0:00:39.710588
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:27:21.010693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.49
 ---- batch: 020 ----
mean loss: 311.89
 ---- batch: 030 ----
mean loss: 317.20
 ---- batch: 040 ----
mean loss: 324.45
train mean loss: 315.27
epoch train time: 0:00:00.211622
elapsed time: 0:00:39.922334
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:27:21.222439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 323.82
 ---- batch: 020 ----
mean loss: 310.75
 ---- batch: 030 ----
mean loss: 313.42
 ---- batch: 040 ----
mean loss: 316.23
train mean loss: 315.22
epoch train time: 0:00:00.208511
elapsed time: 0:00:40.130969
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:27:21.431073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.95
 ---- batch: 020 ----
mean loss: 312.52
 ---- batch: 030 ----
mean loss: 314.65
 ---- batch: 040 ----
mean loss: 312.81
train mean loss: 315.05
epoch train time: 0:00:00.210985
elapsed time: 0:00:40.342076
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:27:21.642182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.41
 ---- batch: 020 ----
mean loss: 319.99
 ---- batch: 030 ----
mean loss: 312.57
 ---- batch: 040 ----
mean loss: 329.98
train mean loss: 314.88
epoch train time: 0:00:00.209291
elapsed time: 0:00:40.551491
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:27:21.851602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 320.04
 ---- batch: 020 ----
mean loss: 309.31
 ---- batch: 030 ----
mean loss: 320.34
 ---- batch: 040 ----
mean loss: 308.59
train mean loss: 314.63
epoch train time: 0:00:00.206435
elapsed time: 0:00:40.758062
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:27:22.058158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.78
 ---- batch: 020 ----
mean loss: 307.64
 ---- batch: 030 ----
mean loss: 318.59
 ---- batch: 040 ----
mean loss: 316.17
train mean loss: 315.03
epoch train time: 0:00:00.201489
elapsed time: 0:00:40.959660
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:27:22.259763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.77
 ---- batch: 020 ----
mean loss: 310.50
 ---- batch: 030 ----
mean loss: 307.01
 ---- batch: 040 ----
mean loss: 316.45
train mean loss: 314.23
epoch train time: 0:00:00.199559
elapsed time: 0:00:41.159339
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:27:22.459458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.32
 ---- batch: 020 ----
mean loss: 315.36
 ---- batch: 030 ----
mean loss: 322.03
 ---- batch: 040 ----
mean loss: 316.60
train mean loss: 314.05
epoch train time: 0:00:00.200039
elapsed time: 0:00:41.359510
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:27:22.659611
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.52
 ---- batch: 020 ----
mean loss: 308.39
 ---- batch: 030 ----
mean loss: 316.77
 ---- batch: 040 ----
mean loss: 309.60
train mean loss: 314.04
epoch train time: 0:00:00.201718
elapsed time: 0:00:41.561344
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:27:22.861449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.38
 ---- batch: 020 ----
mean loss: 313.88
 ---- batch: 030 ----
mean loss: 308.83
 ---- batch: 040 ----
mean loss: 316.44
train mean loss: 313.64
epoch train time: 0:00:00.202566
elapsed time: 0:00:41.764029
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:27:23.064132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.69
 ---- batch: 020 ----
mean loss: 313.07
 ---- batch: 030 ----
mean loss: 320.03
 ---- batch: 040 ----
mean loss: 318.24
train mean loss: 313.76
epoch train time: 0:00:00.201042
elapsed time: 0:00:41.965186
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:27:23.265288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.07
 ---- batch: 020 ----
mean loss: 312.33
 ---- batch: 030 ----
mean loss: 319.66
 ---- batch: 040 ----
mean loss: 305.71
train mean loss: 313.43
epoch train time: 0:00:00.200377
elapsed time: 0:00:42.165682
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:27:23.465794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.63
 ---- batch: 020 ----
mean loss: 316.94
 ---- batch: 030 ----
mean loss: 309.31
 ---- batch: 040 ----
mean loss: 314.69
train mean loss: 313.33
epoch train time: 0:00:00.200703
elapsed time: 0:00:42.366517
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:27:23.666622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.26
 ---- batch: 020 ----
mean loss: 313.97
 ---- batch: 030 ----
mean loss: 310.13
 ---- batch: 040 ----
mean loss: 317.41
train mean loss: 313.25
epoch train time: 0:00:00.206160
elapsed time: 0:00:42.572808
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:27:23.872929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.56
 ---- batch: 020 ----
mean loss: 309.68
 ---- batch: 030 ----
mean loss: 312.87
 ---- batch: 040 ----
mean loss: 314.72
train mean loss: 312.87
epoch train time: 0:00:00.204787
elapsed time: 0:00:42.777742
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:27:24.077850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.56
 ---- batch: 020 ----
mean loss: 318.89
 ---- batch: 030 ----
mean loss: 313.86
 ---- batch: 040 ----
mean loss: 308.38
train mean loss: 312.95
epoch train time: 0:00:00.205357
elapsed time: 0:00:42.983253
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:27:24.283357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.51
 ---- batch: 020 ----
mean loss: 309.46
 ---- batch: 030 ----
mean loss: 316.95
 ---- batch: 040 ----
mean loss: 307.23
train mean loss: 312.36
epoch train time: 0:00:00.214696
elapsed time: 0:00:43.198066
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:27:24.498169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.56
 ---- batch: 020 ----
mean loss: 308.05
 ---- batch: 030 ----
mean loss: 314.52
 ---- batch: 040 ----
mean loss: 310.18
train mean loss: 312.31
epoch train time: 0:00:00.210478
elapsed time: 0:00:43.408668
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:27:24.708799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.33
 ---- batch: 020 ----
mean loss: 312.95
 ---- batch: 030 ----
mean loss: 310.06
 ---- batch: 040 ----
mean loss: 318.93
train mean loss: 312.00
epoch train time: 0:00:00.207143
elapsed time: 0:00:43.615962
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:27:24.916066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.85
 ---- batch: 020 ----
mean loss: 310.88
 ---- batch: 030 ----
mean loss: 313.59
 ---- batch: 040 ----
mean loss: 315.64
train mean loss: 312.29
epoch train time: 0:00:00.205814
elapsed time: 0:00:43.821930
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:27:25.122033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.62
 ---- batch: 020 ----
mean loss: 315.01
 ---- batch: 030 ----
mean loss: 306.45
 ---- batch: 040 ----
mean loss: 306.59
train mean loss: 311.98
epoch train time: 0:00:00.206153
elapsed time: 0:00:44.028211
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:27:25.328330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.68
 ---- batch: 020 ----
mean loss: 307.74
 ---- batch: 030 ----
mean loss: 312.39
 ---- batch: 040 ----
mean loss: 316.63
train mean loss: 311.94
epoch train time: 0:00:00.203368
elapsed time: 0:00:44.231745
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:27:25.531850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.39
 ---- batch: 020 ----
mean loss: 312.09
 ---- batch: 030 ----
mean loss: 311.33
 ---- batch: 040 ----
mean loss: 309.95
train mean loss: 311.45
epoch train time: 0:00:00.198920
elapsed time: 0:00:44.430792
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:27:25.730896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.34
 ---- batch: 020 ----
mean loss: 319.47
 ---- batch: 030 ----
mean loss: 310.17
 ---- batch: 040 ----
mean loss: 306.36
train mean loss: 311.43
epoch train time: 0:00:00.211741
elapsed time: 0:00:44.642649
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:27:25.942751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.31
 ---- batch: 020 ----
mean loss: 317.22
 ---- batch: 030 ----
mean loss: 308.36
 ---- batch: 040 ----
mean loss: 314.38
train mean loss: 311.19
epoch train time: 0:00:00.202167
elapsed time: 0:00:44.844931
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:27:26.145033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.59
 ---- batch: 020 ----
mean loss: 311.80
 ---- batch: 030 ----
mean loss: 311.07
 ---- batch: 040 ----
mean loss: 308.95
train mean loss: 311.66
epoch train time: 0:00:00.202217
elapsed time: 0:00:45.047339
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:27:26.347444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 315.30
 ---- batch: 020 ----
mean loss: 297.80
 ---- batch: 030 ----
mean loss: 317.72
 ---- batch: 040 ----
mean loss: 312.12
train mean loss: 311.05
epoch train time: 0:00:00.197297
elapsed time: 0:00:45.244763
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:27:26.544856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.41
 ---- batch: 020 ----
mean loss: 308.60
 ---- batch: 030 ----
mean loss: 313.16
 ---- batch: 040 ----
mean loss: 310.16
train mean loss: 311.02
epoch train time: 0:00:00.216938
elapsed time: 0:00:45.461828
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:27:26.761947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.84
 ---- batch: 020 ----
mean loss: 307.19
 ---- batch: 030 ----
mean loss: 319.24
 ---- batch: 040 ----
mean loss: 308.15
train mean loss: 310.76
epoch train time: 0:00:00.202694
elapsed time: 0:00:45.664655
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:27:26.964756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.08
 ---- batch: 020 ----
mean loss: 301.95
 ---- batch: 030 ----
mean loss: 318.90
 ---- batch: 040 ----
mean loss: 310.67
train mean loss: 310.42
epoch train time: 0:00:00.201225
elapsed time: 0:00:45.865998
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:27:27.166117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.70
 ---- batch: 020 ----
mean loss: 307.56
 ---- batch: 030 ----
mean loss: 307.01
 ---- batch: 040 ----
mean loss: 309.64
train mean loss: 310.61
epoch train time: 0:00:00.201826
elapsed time: 0:00:46.067955
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:27:27.368057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.13
 ---- batch: 020 ----
mean loss: 310.68
 ---- batch: 030 ----
mean loss: 303.20
 ---- batch: 040 ----
mean loss: 314.72
train mean loss: 310.45
epoch train time: 0:00:00.207330
elapsed time: 0:00:46.275405
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:27:27.575511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.59
 ---- batch: 020 ----
mean loss: 313.52
 ---- batch: 030 ----
mean loss: 304.44
 ---- batch: 040 ----
mean loss: 318.21
train mean loss: 310.05
epoch train time: 0:00:00.209083
elapsed time: 0:00:46.484610
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:27:27.784713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.09
 ---- batch: 020 ----
mean loss: 306.24
 ---- batch: 030 ----
mean loss: 317.14
 ---- batch: 040 ----
mean loss: 309.46
train mean loss: 309.83
epoch train time: 0:00:00.206569
elapsed time: 0:00:46.691295
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:27:27.991399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.71
 ---- batch: 020 ----
mean loss: 319.52
 ---- batch: 030 ----
mean loss: 307.71
 ---- batch: 040 ----
mean loss: 308.88
train mean loss: 309.25
epoch train time: 0:00:00.204493
elapsed time: 0:00:46.895913
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:27:28.196018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.71
 ---- batch: 020 ----
mean loss: 313.19
 ---- batch: 030 ----
mean loss: 312.52
 ---- batch: 040 ----
mean loss: 297.73
train mean loss: 309.81
epoch train time: 0:00:00.205903
elapsed time: 0:00:47.101967
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:27:28.402075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.82
 ---- batch: 020 ----
mean loss: 313.21
 ---- batch: 030 ----
mean loss: 306.69
 ---- batch: 040 ----
mean loss: 307.78
train mean loss: 309.29
epoch train time: 0:00:00.205894
elapsed time: 0:00:47.308001
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:27:28.608107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.36
 ---- batch: 020 ----
mean loss: 298.46
 ---- batch: 030 ----
mean loss: 306.61
 ---- batch: 040 ----
mean loss: 324.48
train mean loss: 309.23
epoch train time: 0:00:00.210966
elapsed time: 0:00:47.519118
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:27:28.819232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.71
 ---- batch: 020 ----
mean loss: 311.65
 ---- batch: 030 ----
mean loss: 305.97
 ---- batch: 040 ----
mean loss: 306.87
train mean loss: 308.74
epoch train time: 0:00:00.208009
elapsed time: 0:00:47.727260
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:27:29.027366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.91
 ---- batch: 020 ----
mean loss: 308.01
 ---- batch: 030 ----
mean loss: 303.82
 ---- batch: 040 ----
mean loss: 307.15
train mean loss: 308.65
epoch train time: 0:00:00.211407
elapsed time: 0:00:47.938812
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:27:29.238919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.89
 ---- batch: 020 ----
mean loss: 300.54
 ---- batch: 030 ----
mean loss: 310.39
 ---- batch: 040 ----
mean loss: 309.54
train mean loss: 308.85
epoch train time: 0:00:00.207004
elapsed time: 0:00:48.145944
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:27:29.446050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.23
 ---- batch: 020 ----
mean loss: 308.99
 ---- batch: 030 ----
mean loss: 308.47
 ---- batch: 040 ----
mean loss: 311.18
train mean loss: 308.23
epoch train time: 0:00:00.202594
elapsed time: 0:00:48.348661
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:27:29.648784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.98
 ---- batch: 020 ----
mean loss: 310.54
 ---- batch: 030 ----
mean loss: 315.46
 ---- batch: 040 ----
mean loss: 308.31
train mean loss: 308.23
epoch train time: 0:00:00.206708
elapsed time: 0:00:48.555511
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:27:29.855620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.71
 ---- batch: 020 ----
mean loss: 308.19
 ---- batch: 030 ----
mean loss: 304.93
 ---- batch: 040 ----
mean loss: 313.31
train mean loss: 308.50
epoch train time: 0:00:00.217498
elapsed time: 0:00:48.773131
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:27:30.073241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.11
 ---- batch: 020 ----
mean loss: 301.42
 ---- batch: 030 ----
mean loss: 316.43
 ---- batch: 040 ----
mean loss: 309.45
train mean loss: 308.16
epoch train time: 0:00:00.202008
elapsed time: 0:00:48.975264
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:27:30.275367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.43
 ---- batch: 020 ----
mean loss: 313.16
 ---- batch: 030 ----
mean loss: 312.81
 ---- batch: 040 ----
mean loss: 304.89
train mean loss: 307.20
epoch train time: 0:00:00.205188
elapsed time: 0:00:49.180584
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:27:30.480689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.05
 ---- batch: 020 ----
mean loss: 307.27
 ---- batch: 030 ----
mean loss: 307.29
 ---- batch: 040 ----
mean loss: 312.21
train mean loss: 307.78
epoch train time: 0:00:00.205655
elapsed time: 0:00:49.386355
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:27:30.686457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.09
 ---- batch: 020 ----
mean loss: 310.82
 ---- batch: 030 ----
mean loss: 312.70
 ---- batch: 040 ----
mean loss: 297.94
train mean loss: 307.95
epoch train time: 0:00:00.204680
elapsed time: 0:00:49.591156
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:27:30.891259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.96
 ---- batch: 020 ----
mean loss: 305.23
 ---- batch: 030 ----
mean loss: 307.92
 ---- batch: 040 ----
mean loss: 302.87
train mean loss: 307.90
epoch train time: 0:00:00.203027
elapsed time: 0:00:49.794299
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:27:31.094401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.88
 ---- batch: 020 ----
mean loss: 303.30
 ---- batch: 030 ----
mean loss: 309.38
 ---- batch: 040 ----
mean loss: 309.29
train mean loss: 307.52
epoch train time: 0:00:00.203462
elapsed time: 0:00:49.997877
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:27:31.297994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.48
 ---- batch: 020 ----
mean loss: 306.90
 ---- batch: 030 ----
mean loss: 308.44
 ---- batch: 040 ----
mean loss: 313.47
train mean loss: 306.80
epoch train time: 0:00:00.204528
elapsed time: 0:00:50.202537
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:27:31.502642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.37
 ---- batch: 020 ----
mean loss: 308.32
 ---- batch: 030 ----
mean loss: 304.57
 ---- batch: 040 ----
mean loss: 306.22
train mean loss: 307.12
epoch train time: 0:00:00.211802
elapsed time: 0:00:50.414471
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:27:31.714583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.33
 ---- batch: 020 ----
mean loss: 309.61
 ---- batch: 030 ----
mean loss: 311.01
 ---- batch: 040 ----
mean loss: 295.81
train mean loss: 307.02
epoch train time: 0:00:00.210843
elapsed time: 0:00:50.625441
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:27:31.925546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.85
 ---- batch: 020 ----
mean loss: 296.67
 ---- batch: 030 ----
mean loss: 312.11
 ---- batch: 040 ----
mean loss: 317.37
train mean loss: 306.83
epoch train time: 0:00:00.210032
elapsed time: 0:00:50.835595
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:27:32.135717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.78
 ---- batch: 020 ----
mean loss: 312.18
 ---- batch: 030 ----
mean loss: 306.16
 ---- batch: 040 ----
mean loss: 300.68
train mean loss: 306.84
epoch train time: 0:00:00.209725
elapsed time: 0:00:51.045462
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:27:32.345568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.49
 ---- batch: 020 ----
mean loss: 296.98
 ---- batch: 030 ----
mean loss: 306.61
 ---- batch: 040 ----
mean loss: 311.82
train mean loss: 306.37
epoch train time: 0:00:00.209073
elapsed time: 0:00:51.254658
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:27:32.554790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.33
 ---- batch: 020 ----
mean loss: 309.63
 ---- batch: 030 ----
mean loss: 302.11
 ---- batch: 040 ----
mean loss: 304.04
train mean loss: 306.38
epoch train time: 0:00:00.208732
elapsed time: 0:00:51.463540
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:27:32.763645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 311.91
 ---- batch: 020 ----
mean loss: 298.73
 ---- batch: 030 ----
mean loss: 303.48
 ---- batch: 040 ----
mean loss: 312.90
train mean loss: 305.50
epoch train time: 0:00:00.207809
elapsed time: 0:00:51.671467
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:27:32.971585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.51
 ---- batch: 020 ----
mean loss: 302.93
 ---- batch: 030 ----
mean loss: 307.35
 ---- batch: 040 ----
mean loss: 310.42
train mean loss: 305.43
epoch train time: 0:00:00.208529
elapsed time: 0:00:51.880131
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:27:33.180246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.15
 ---- batch: 020 ----
mean loss: 307.32
 ---- batch: 030 ----
mean loss: 300.37
 ---- batch: 040 ----
mean loss: 307.16
train mean loss: 305.60
epoch train time: 0:00:00.209825
elapsed time: 0:00:52.090083
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:27:33.390186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.43
 ---- batch: 020 ----
mean loss: 311.74
 ---- batch: 030 ----
mean loss: 300.56
 ---- batch: 040 ----
mean loss: 303.44
train mean loss: 305.54
epoch train time: 0:00:00.206170
elapsed time: 0:00:52.296420
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:27:33.596544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.16
 ---- batch: 020 ----
mean loss: 311.23
 ---- batch: 030 ----
mean loss: 299.06
 ---- batch: 040 ----
mean loss: 304.66
train mean loss: 305.31
epoch train time: 0:00:00.204443
elapsed time: 0:00:52.501013
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:27:33.801119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 305.44
 ---- batch: 020 ----
mean loss: 300.02
 ---- batch: 030 ----
mean loss: 307.53
 ---- batch: 040 ----
mean loss: 310.54
train mean loss: 305.14
epoch train time: 0:00:00.198975
elapsed time: 0:00:52.700108
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:27:34.000211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 306.14
 ---- batch: 020 ----
mean loss: 303.16
 ---- batch: 030 ----
mean loss: 297.28
 ---- batch: 040 ----
mean loss: 315.78
train mean loss: 304.98
epoch train time: 0:00:00.203933
elapsed time: 0:00:52.904179
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:27:34.204280
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.46
 ---- batch: 020 ----
mean loss: 309.79
 ---- batch: 030 ----
mean loss: 307.57
 ---- batch: 040 ----
mean loss: 301.01
train mean loss: 305.07
epoch train time: 0:00:00.204132
elapsed time: 0:00:53.108458
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:27:34.408584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.94
 ---- batch: 020 ----
mean loss: 298.57
 ---- batch: 030 ----
mean loss: 309.88
 ---- batch: 040 ----
mean loss: 308.44
train mean loss: 305.01
epoch train time: 0:00:00.205760
elapsed time: 0:00:53.314391
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:27:34.614509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.09
 ---- batch: 020 ----
mean loss: 304.60
 ---- batch: 030 ----
mean loss: 306.18
 ---- batch: 040 ----
mean loss: 299.50
train mean loss: 304.48
epoch train time: 0:00:00.201678
elapsed time: 0:00:53.516205
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:27:34.816310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 292.09
 ---- batch: 020 ----
mean loss: 314.95
 ---- batch: 030 ----
mean loss: 296.99
 ---- batch: 040 ----
mean loss: 309.35
train mean loss: 304.83
epoch train time: 0:00:00.200183
elapsed time: 0:00:53.716520
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:27:35.016637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 303.95
 ---- batch: 020 ----
mean loss: 308.64
 ---- batch: 030 ----
mean loss: 308.39
 ---- batch: 040 ----
mean loss: 293.50
train mean loss: 304.06
epoch train time: 0:00:00.200722
elapsed time: 0:00:53.917396
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:27:35.217515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 301.02
 ---- batch: 020 ----
mean loss: 302.81
 ---- batch: 030 ----
mean loss: 303.26
 ---- batch: 040 ----
mean loss: 308.30
train mean loss: 303.93
epoch train time: 0:00:00.200486
elapsed time: 0:00:54.118022
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:27:35.418125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.36
 ---- batch: 020 ----
mean loss: 301.55
 ---- batch: 030 ----
mean loss: 298.81
 ---- batch: 040 ----
mean loss: 312.75
train mean loss: 304.20
epoch train time: 0:00:00.205305
elapsed time: 0:00:54.323447
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:27:35.623552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.93
 ---- batch: 020 ----
mean loss: 309.11
 ---- batch: 030 ----
mean loss: 305.22
 ---- batch: 040 ----
mean loss: 309.26
train mean loss: 304.08
epoch train time: 0:00:00.224721
elapsed time: 0:00:54.548287
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:27:35.848413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 299.66
 ---- batch: 020 ----
mean loss: 316.03
 ---- batch: 030 ----
mean loss: 301.92
 ---- batch: 040 ----
mean loss: 299.23
train mean loss: 303.33
epoch train time: 0:00:00.209610
elapsed time: 0:00:54.758038
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:27:36.058141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.86
 ---- batch: 020 ----
mean loss: 303.77
 ---- batch: 030 ----
mean loss: 302.90
 ---- batch: 040 ----
mean loss: 305.07
train mean loss: 303.24
epoch train time: 0:00:00.209516
elapsed time: 0:00:54.967674
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:27:36.267778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 302.23
 ---- batch: 020 ----
mean loss: 302.60
 ---- batch: 030 ----
mean loss: 299.19
 ---- batch: 040 ----
mean loss: 306.27
train mean loss: 303.19
epoch train time: 0:00:00.209084
elapsed time: 0:00:55.176895
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:27:36.477001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.83
 ---- batch: 020 ----
mean loss: 304.56
 ---- batch: 030 ----
mean loss: 308.25
 ---- batch: 040 ----
mean loss: 302.14
train mean loss: 303.12
epoch train time: 0:00:00.210156
elapsed time: 0:00:55.387193
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:27:36.687297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 310.48
 ---- batch: 020 ----
mean loss: 300.30
 ---- batch: 030 ----
mean loss: 299.10
 ---- batch: 040 ----
mean loss: 300.65
train mean loss: 302.77
epoch train time: 0:00:00.209506
elapsed time: 0:00:55.596823
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:27:36.896931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 296.77
 ---- batch: 020 ----
mean loss: 313.40
 ---- batch: 030 ----
mean loss: 297.42
 ---- batch: 040 ----
mean loss: 307.30
train mean loss: 302.49
epoch train time: 0:00:00.208093
elapsed time: 0:00:55.805042
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:27:37.105175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.49
 ---- batch: 020 ----
mean loss: 299.27
 ---- batch: 030 ----
mean loss: 300.13
 ---- batch: 040 ----
mean loss: 305.87
train mean loss: 302.67
epoch train time: 0:00:00.209791
elapsed time: 0:00:56.014985
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:27:37.315092
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.64
 ---- batch: 020 ----
mean loss: 305.27
 ---- batch: 030 ----
mean loss: 300.54
 ---- batch: 040 ----
mean loss: 303.39
train mean loss: 302.60
epoch train time: 0:00:00.209849
elapsed time: 0:00:56.224969
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:27:37.525065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.08
 ---- batch: 020 ----
mean loss: 307.81
 ---- batch: 030 ----
mean loss: 301.19
 ---- batch: 040 ----
mean loss: 304.06
train mean loss: 302.19
epoch train time: 0:00:00.206879
elapsed time: 0:00:56.431961
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:27:37.732059
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.72
 ---- batch: 020 ----
mean loss: 311.04
 ---- batch: 030 ----
mean loss: 306.17
 ---- batch: 040 ----
mean loss: 299.72
train mean loss: 302.39
epoch train time: 0:00:00.202780
elapsed time: 0:00:56.634857
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:27:37.934963
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.38
 ---- batch: 020 ----
mean loss: 303.80
 ---- batch: 030 ----
mean loss: 295.46
 ---- batch: 040 ----
mean loss: 310.77
train mean loss: 302.60
epoch train time: 0:00:00.204136
elapsed time: 0:00:56.839111
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:27:38.139213
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.08
 ---- batch: 020 ----
mean loss: 305.36
 ---- batch: 030 ----
mean loss: 304.22
 ---- batch: 040 ----
mean loss: 294.85
train mean loss: 302.27
epoch train time: 0:00:00.198749
elapsed time: 0:00:57.037977
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:27:38.338109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.61
 ---- batch: 020 ----
mean loss: 300.32
 ---- batch: 030 ----
mean loss: 308.97
 ---- batch: 040 ----
mean loss: 300.75
train mean loss: 302.16
epoch train time: 0:00:00.202169
elapsed time: 0:00:57.240291
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:27:38.540413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.63
 ---- batch: 020 ----
mean loss: 299.20
 ---- batch: 030 ----
mean loss: 307.57
 ---- batch: 040 ----
mean loss: 297.41
train mean loss: 302.63
epoch train time: 0:00:00.203283
elapsed time: 0:00:57.443734
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:27:38.743839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.81
 ---- batch: 020 ----
mean loss: 310.05
 ---- batch: 030 ----
mean loss: 298.04
 ---- batch: 040 ----
mean loss: 303.44
train mean loss: 302.18
epoch train time: 0:00:00.201355
elapsed time: 0:00:57.645240
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:27:38.945354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.49
 ---- batch: 020 ----
mean loss: 307.54
 ---- batch: 030 ----
mean loss: 299.29
 ---- batch: 040 ----
mean loss: 305.98
train mean loss: 302.49
epoch train time: 0:00:00.196512
elapsed time: 0:00:57.841895
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:27:39.142031
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.52
 ---- batch: 020 ----
mean loss: 300.30
 ---- batch: 030 ----
mean loss: 297.31
 ---- batch: 040 ----
mean loss: 310.84
train mean loss: 302.31
epoch train time: 0:00:00.197034
elapsed time: 0:00:58.039077
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:27:39.339195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.28
 ---- batch: 020 ----
mean loss: 301.81
 ---- batch: 030 ----
mean loss: 303.03
 ---- batch: 040 ----
mean loss: 307.06
train mean loss: 302.17
epoch train time: 0:00:00.200033
elapsed time: 0:00:58.239248
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:27:39.539362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.35
 ---- batch: 020 ----
mean loss: 299.97
 ---- batch: 030 ----
mean loss: 299.81
 ---- batch: 040 ----
mean loss: 306.95
train mean loss: 302.63
epoch train time: 0:00:00.204179
elapsed time: 0:00:58.443572
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:27:39.743688
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.68
 ---- batch: 020 ----
mean loss: 304.22
 ---- batch: 030 ----
mean loss: 301.33
 ---- batch: 040 ----
mean loss: 301.95
train mean loss: 302.14
epoch train time: 0:00:00.214010
elapsed time: 0:00:58.657717
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:27:39.957837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.03
 ---- batch: 020 ----
mean loss: 296.24
 ---- batch: 030 ----
mean loss: 302.47
 ---- batch: 040 ----
mean loss: 308.19
train mean loss: 302.32
epoch train time: 0:00:00.211346
elapsed time: 0:00:58.869197
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:27:40.169300
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.12
 ---- batch: 020 ----
mean loss: 309.59
 ---- batch: 030 ----
mean loss: 295.71
 ---- batch: 040 ----
mean loss: 299.88
train mean loss: 302.34
epoch train time: 0:00:00.211656
elapsed time: 0:00:59.080972
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:27:40.381076
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.54
 ---- batch: 020 ----
mean loss: 297.52
 ---- batch: 030 ----
mean loss: 303.80
 ---- batch: 040 ----
mean loss: 299.65
train mean loss: 302.19
epoch train time: 0:00:00.211220
elapsed time: 0:00:59.292312
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:27:40.592442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.11
 ---- batch: 020 ----
mean loss: 302.74
 ---- batch: 030 ----
mean loss: 295.76
 ---- batch: 040 ----
mean loss: 306.13
train mean loss: 302.48
epoch train time: 0:00:00.212913
elapsed time: 0:00:59.505390
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:27:40.805497
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.85
 ---- batch: 020 ----
mean loss: 308.78
 ---- batch: 030 ----
mean loss: 301.00
 ---- batch: 040 ----
mean loss: 295.80
train mean loss: 302.07
epoch train time: 0:00:00.211448
elapsed time: 0:00:59.716967
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:27:41.017073
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.44
 ---- batch: 020 ----
mean loss: 297.77
 ---- batch: 030 ----
mean loss: 296.37
 ---- batch: 040 ----
mean loss: 302.51
train mean loss: 302.27
epoch train time: 0:00:00.207905
elapsed time: 0:00:59.924997
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:27:41.225103
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.14
 ---- batch: 020 ----
mean loss: 302.97
 ---- batch: 030 ----
mean loss: 301.73
 ---- batch: 040 ----
mean loss: 304.91
train mean loss: 302.12
epoch train time: 0:00:00.208251
elapsed time: 0:01:00.133372
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:27:41.433476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.56
 ---- batch: 020 ----
mean loss: 308.33
 ---- batch: 030 ----
mean loss: 297.01
 ---- batch: 040 ----
mean loss: 288.74
train mean loss: 302.71
epoch train time: 0:00:00.209650
elapsed time: 0:01:00.343144
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:27:41.643249
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.76
 ---- batch: 020 ----
mean loss: 301.32
 ---- batch: 030 ----
mean loss: 307.22
 ---- batch: 040 ----
mean loss: 301.22
train mean loss: 302.41
epoch train time: 0:00:00.219367
elapsed time: 0:01:00.562631
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:27:41.862737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.27
 ---- batch: 020 ----
mean loss: 305.67
 ---- batch: 030 ----
mean loss: 294.96
 ---- batch: 040 ----
mean loss: 302.24
train mean loss: 302.11
epoch train time: 0:00:00.199962
elapsed time: 0:01:00.762716
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:27:42.062836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 302.54
 ---- batch: 020 ----
mean loss: 303.80
 ---- batch: 030 ----
mean loss: 302.88
 ---- batch: 040 ----
mean loss: 299.43
train mean loss: 302.26
epoch train time: 0:00:00.197854
elapsed time: 0:01:00.960701
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:27:42.260801
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.65
 ---- batch: 020 ----
mean loss: 290.64
 ---- batch: 030 ----
mean loss: 302.97
 ---- batch: 040 ----
mean loss: 306.87
train mean loss: 302.15
epoch train time: 0:00:00.196114
elapsed time: 0:01:01.156927
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:27:42.457028
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.50
 ---- batch: 020 ----
mean loss: 297.62
 ---- batch: 030 ----
mean loss: 306.57
 ---- batch: 040 ----
mean loss: 299.98
train mean loss: 301.87
epoch train time: 0:00:00.200045
elapsed time: 0:01:01.357154
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:27:42.657257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.98
 ---- batch: 020 ----
mean loss: 302.88
 ---- batch: 030 ----
mean loss: 299.22
 ---- batch: 040 ----
mean loss: 298.88
train mean loss: 301.90
epoch train time: 0:00:00.203832
elapsed time: 0:01:01.561103
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:27:42.861207
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.16
 ---- batch: 020 ----
mean loss: 297.19
 ---- batch: 030 ----
mean loss: 304.66
 ---- batch: 040 ----
mean loss: 303.71
train mean loss: 301.93
epoch train time: 0:00:00.201311
elapsed time: 0:01:01.762544
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:27:43.062656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 303.86
 ---- batch: 020 ----
mean loss: 308.49
 ---- batch: 030 ----
mean loss: 294.63
 ---- batch: 040 ----
mean loss: 302.64
train mean loss: 301.89
epoch train time: 0:00:00.200825
elapsed time: 0:01:01.963494
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:27:43.263615
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 296.18
 ---- batch: 020 ----
mean loss: 305.17
 ---- batch: 030 ----
mean loss: 305.40
 ---- batch: 040 ----
mean loss: 307.32
train mean loss: 301.48
epoch train time: 0:00:00.200242
elapsed time: 0:01:02.163870
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:27:43.463972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.46
 ---- batch: 020 ----
mean loss: 297.72
 ---- batch: 030 ----
mean loss: 307.61
 ---- batch: 040 ----
mean loss: 303.31
train mean loss: 301.88
epoch train time: 0:00:00.201956
elapsed time: 0:01:02.365947
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:27:43.666053
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.61
 ---- batch: 020 ----
mean loss: 302.82
 ---- batch: 030 ----
mean loss: 306.35
 ---- batch: 040 ----
mean loss: 304.99
train mean loss: 302.02
epoch train time: 0:00:00.216755
elapsed time: 0:01:02.582825
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:27:43.882933
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.58
 ---- batch: 020 ----
mean loss: 308.21
 ---- batch: 030 ----
mean loss: 301.14
 ---- batch: 040 ----
mean loss: 300.14
train mean loss: 301.96
epoch train time: 0:00:00.207899
elapsed time: 0:01:02.790863
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:27:44.090990
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.06
 ---- batch: 020 ----
mean loss: 307.27
 ---- batch: 030 ----
mean loss: 305.71
 ---- batch: 040 ----
mean loss: 294.46
train mean loss: 301.87
epoch train time: 0:00:00.212232
elapsed time: 0:01:03.003260
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:27:44.303366
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.23
 ---- batch: 020 ----
mean loss: 303.92
 ---- batch: 030 ----
mean loss: 301.21
 ---- batch: 040 ----
mean loss: 301.21
train mean loss: 301.57
epoch train time: 0:00:00.211274
elapsed time: 0:01:03.214669
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:27:44.514796
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.62
 ---- batch: 020 ----
mean loss: 294.23
 ---- batch: 030 ----
mean loss: 305.02
 ---- batch: 040 ----
mean loss: 295.49
train mean loss: 301.96
epoch train time: 0:00:00.211808
elapsed time: 0:01:03.426623
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:27:44.726729
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.89
 ---- batch: 020 ----
mean loss: 297.34
 ---- batch: 030 ----
mean loss: 303.14
 ---- batch: 040 ----
mean loss: 299.61
train mean loss: 301.60
epoch train time: 0:00:00.207764
elapsed time: 0:01:03.634507
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:27:44.934610
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.41
 ---- batch: 020 ----
mean loss: 298.33
 ---- batch: 030 ----
mean loss: 299.25
 ---- batch: 040 ----
mean loss: 296.85
train mean loss: 301.87
epoch train time: 0:00:00.207213
elapsed time: 0:01:03.841841
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:27:45.141992
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.23
 ---- batch: 020 ----
mean loss: 302.44
 ---- batch: 030 ----
mean loss: 298.72
 ---- batch: 040 ----
mean loss: 308.39
train mean loss: 301.58
epoch train time: 0:00:00.209118
elapsed time: 0:01:04.051128
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:27:45.351241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 298.82
 ---- batch: 020 ----
mean loss: 305.79
 ---- batch: 030 ----
mean loss: 303.24
 ---- batch: 040 ----
mean loss: 303.82
train mean loss: 301.55
epoch train time: 0:00:00.208236
elapsed time: 0:01:04.259491
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:27:45.559595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 295.34
 ---- batch: 020 ----
mean loss: 304.74
 ---- batch: 030 ----
mean loss: 305.59
 ---- batch: 040 ----
mean loss: 299.86
train mean loss: 301.98
epoch train time: 0:00:00.208490
elapsed time: 0:01:04.468113
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:27:45.768229
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.30
 ---- batch: 020 ----
mean loss: 304.89
 ---- batch: 030 ----
mean loss: 305.30
 ---- batch: 040 ----
mean loss: 296.50
train mean loss: 301.81
epoch train time: 0:00:00.202145
elapsed time: 0:01:04.670390
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:27:45.970493
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.20
 ---- batch: 020 ----
mean loss: 300.93
 ---- batch: 030 ----
mean loss: 299.62
 ---- batch: 040 ----
mean loss: 305.17
train mean loss: 301.60
epoch train time: 0:00:00.198703
elapsed time: 0:01:04.869235
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:27:46.169336
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 297.15
 ---- batch: 020 ----
mean loss: 303.59
 ---- batch: 030 ----
mean loss: 305.57
 ---- batch: 040 ----
mean loss: 303.82
train mean loss: 301.56
epoch train time: 0:00:00.197960
elapsed time: 0:01:05.067309
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:27:46.367411
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.55
 ---- batch: 020 ----
mean loss: 301.58
 ---- batch: 030 ----
mean loss: 300.76
 ---- batch: 040 ----
mean loss: 302.77
train mean loss: 301.49
epoch train time: 0:00:00.198540
elapsed time: 0:01:05.265969
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:27:46.566088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.06
 ---- batch: 020 ----
mean loss: 301.83
 ---- batch: 030 ----
mean loss: 307.32
 ---- batch: 040 ----
mean loss: 295.26
train mean loss: 301.99
epoch train time: 0:00:00.204924
elapsed time: 0:01:05.471029
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:27:46.771135
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 299.93
 ---- batch: 020 ----
mean loss: 307.14
 ---- batch: 030 ----
mean loss: 300.30
 ---- batch: 040 ----
mean loss: 297.20
train mean loss: 302.21
epoch train time: 0:00:00.205322
elapsed time: 0:01:05.676495
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:27:46.976614
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.92
 ---- batch: 020 ----
mean loss: 295.54
 ---- batch: 030 ----
mean loss: 310.39
 ---- batch: 040 ----
mean loss: 295.17
train mean loss: 301.56
epoch train time: 0:00:00.205238
elapsed time: 0:01:05.881867
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:27:47.181998
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 301.61
 ---- batch: 020 ----
mean loss: 308.17
 ---- batch: 030 ----
mean loss: 299.95
 ---- batch: 040 ----
mean loss: 296.27
train mean loss: 301.54
epoch train time: 0:00:00.202777
elapsed time: 0:01:06.086761
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_2/checkpoint.pth.tar
**** end time: 2019-09-20 23:27:47.386836 ****
