Namespace(batch_size=512, dataset='CMAPSS/FD001', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 31452
use_cuda: True
Dataset: CMAPSS/FD001
Building FrequentistConv2Pool2...
Done.
**** start time: 2019-09-27 16:39:51.326993 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 8, 26, 1]             560
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
            Conv2d-4            [-1, 14, 12, 1]             224
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
            Linear-8                    [-1, 1]              84
================================================================
Total params: 868
Trainable params: 868
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 16:39:51.332277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4131.94
 ---- batch: 020 ----
mean loss: 3967.70
 ---- batch: 030 ----
mean loss: 4015.47
train mean loss: 4032.71
epoch train time: 0:00:12.607427
elapsed time: 0:00:12.614054
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 16:40:03.941091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3950.44
 ---- batch: 020 ----
mean loss: 3862.75
 ---- batch: 030 ----
mean loss: 3831.98
train mean loss: 3873.58
epoch train time: 0:00:00.173935
elapsed time: 0:00:12.788196
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 16:40:04.115250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3732.26
 ---- batch: 020 ----
mean loss: 3649.08
 ---- batch: 030 ----
mean loss: 3591.39
train mean loss: 3634.49
epoch train time: 0:00:00.175540
elapsed time: 0:00:12.963884
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 16:40:04.290936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3441.20
 ---- batch: 020 ----
mean loss: 3342.29
 ---- batch: 030 ----
mean loss: 3304.25
train mean loss: 3352.23
epoch train time: 0:00:00.171942
elapsed time: 0:00:13.135971
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 16:40:04.463010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3127.40
 ---- batch: 020 ----
mean loss: 3049.71
 ---- batch: 030 ----
mean loss: 3097.99
train mean loss: 3075.14
epoch train time: 0:00:00.172007
elapsed time: 0:00:13.308110
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 16:40:04.635164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2914.96
 ---- batch: 020 ----
mean loss: 2834.51
 ---- batch: 030 ----
mean loss: 2806.45
train mean loss: 2838.38
epoch train time: 0:00:00.174421
elapsed time: 0:00:13.482707
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 16:40:04.809751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2720.83
 ---- batch: 020 ----
mean loss: 2635.33
 ---- batch: 030 ----
mean loss: 2574.53
train mean loss: 2630.35
epoch train time: 0:00:00.185252
elapsed time: 0:00:13.668097
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 16:40:04.995155
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2506.06
 ---- batch: 020 ----
mean loss: 2476.99
 ---- batch: 030 ----
mean loss: 2382.12
train mean loss: 2443.82
epoch train time: 0:00:00.184865
elapsed time: 0:00:13.853152
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 16:40:05.180226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2330.70
 ---- batch: 020 ----
mean loss: 2287.00
 ---- batch: 030 ----
mean loss: 2249.83
train mean loss: 2278.29
epoch train time: 0:00:00.178364
elapsed time: 0:00:14.031722
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 16:40:05.358779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2193.63
 ---- batch: 020 ----
mean loss: 2129.50
 ---- batch: 030 ----
mean loss: 2099.54
train mean loss: 2127.26
epoch train time: 0:00:00.172282
elapsed time: 0:00:14.204180
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 16:40:05.531222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2025.13
 ---- batch: 020 ----
mean loss: 2013.38
 ---- batch: 030 ----
mean loss: 1954.77
train mean loss: 1987.26
epoch train time: 0:00:00.177620
elapsed time: 0:00:14.381935
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 16:40:05.708976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1898.43
 ---- batch: 020 ----
mean loss: 1881.43
 ---- batch: 030 ----
mean loss: 1813.34
train mean loss: 1862.18
epoch train time: 0:00:00.175681
elapsed time: 0:00:14.557754
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 16:40:05.884796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1792.99
 ---- batch: 020 ----
mean loss: 1739.88
 ---- batch: 030 ----
mean loss: 1738.74
train mean loss: 1743.68
epoch train time: 0:00:00.171253
elapsed time: 0:00:14.729146
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 16:40:06.056188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1702.45
 ---- batch: 020 ----
mean loss: 1650.45
 ---- batch: 030 ----
mean loss: 1593.25
train mean loss: 1635.05
epoch train time: 0:00:00.167799
elapsed time: 0:00:14.897093
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 16:40:06.224154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1565.47
 ---- batch: 020 ----
mean loss: 1545.41
 ---- batch: 030 ----
mean loss: 1516.86
train mean loss: 1536.07
epoch train time: 0:00:00.170729
elapsed time: 0:00:15.067989
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 16:40:06.395031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1468.27
 ---- batch: 020 ----
mean loss: 1443.30
 ---- batch: 030 ----
mean loss: 1432.07
train mean loss: 1442.73
epoch train time: 0:00:00.163160
elapsed time: 0:00:15.231305
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 16:40:06.558347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1390.75
 ---- batch: 020 ----
mean loss: 1355.90
 ---- batch: 030 ----
mean loss: 1338.89
train mean loss: 1356.31
epoch train time: 0:00:00.174276
elapsed time: 0:00:15.405733
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 16:40:06.732775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1307.24
 ---- batch: 020 ----
mean loss: 1285.33
 ---- batch: 030 ----
mean loss: 1243.13
train mean loss: 1277.88
epoch train time: 0:00:00.173162
elapsed time: 0:00:15.579041
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 16:40:06.906083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1234.51
 ---- batch: 020 ----
mean loss: 1213.47
 ---- batch: 030 ----
mean loss: 1181.46
train mean loss: 1204.04
epoch train time: 0:00:00.169228
elapsed time: 0:00:15.748407
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 16:40:07.075448
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1164.19
 ---- batch: 020 ----
mean loss: 1136.97
 ---- batch: 030 ----
mean loss: 1118.98
train mean loss: 1135.81
epoch train time: 0:00:00.164410
elapsed time: 0:00:15.912951
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 16:40:07.240002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1083.90
 ---- batch: 020 ----
mean loss: 1077.05
 ---- batch: 030 ----
mean loss: 1059.30
train mean loss: 1074.04
epoch train time: 0:00:00.173131
elapsed time: 0:00:16.086226
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 16:40:07.413269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1028.71
 ---- batch: 020 ----
mean loss: 1039.66
 ---- batch: 030 ----
mean loss: 1004.89
train mean loss: 1015.92
epoch train time: 0:00:00.165420
elapsed time: 0:00:16.251783
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 16:40:07.578825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 989.79
 ---- batch: 020 ----
mean loss: 965.26
 ---- batch: 030 ----
mean loss: 955.19
train mean loss: 962.56
epoch train time: 0:00:00.168858
elapsed time: 0:00:16.420783
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 16:40:07.747832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 919.68
 ---- batch: 020 ----
mean loss: 929.72
 ---- batch: 030 ----
mean loss: 913.12
train mean loss: 912.86
epoch train time: 0:00:00.179586
elapsed time: 0:00:16.600516
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 16:40:07.927567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.00
 ---- batch: 020 ----
mean loss: 883.72
 ---- batch: 030 ----
mean loss: 851.29
train mean loss: 867.82
epoch train time: 0:00:00.169118
elapsed time: 0:00:16.769785
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 16:40:08.096830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 828.66
 ---- batch: 020 ----
mean loss: 844.20
 ---- batch: 030 ----
mean loss: 822.66
train mean loss: 827.09
epoch train time: 0:00:00.165308
elapsed time: 0:00:16.935238
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 16:40:08.262310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 804.67
 ---- batch: 020 ----
mean loss: 795.31
 ---- batch: 030 ----
mean loss: 766.33
train mean loss: 789.44
epoch train time: 0:00:00.173471
elapsed time: 0:00:17.108876
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 16:40:08.435934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 769.34
 ---- batch: 020 ----
mean loss: 756.49
 ---- batch: 030 ----
mean loss: 738.83
train mean loss: 753.28
epoch train time: 0:00:00.167005
elapsed time: 0:00:17.276047
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 16:40:08.603090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 725.63
 ---- batch: 020 ----
mean loss: 737.75
 ---- batch: 030 ----
mean loss: 713.46
train mean loss: 720.57
epoch train time: 0:00:00.179084
elapsed time: 0:00:17.455270
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 16:40:08.782310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 699.70
 ---- batch: 020 ----
mean loss: 690.72
 ---- batch: 030 ----
mean loss: 685.28
train mean loss: 691.76
epoch train time: 0:00:00.183012
elapsed time: 0:00:17.638426
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 16:40:08.965484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 669.93
 ---- batch: 020 ----
mean loss: 658.01
 ---- batch: 030 ----
mean loss: 672.87
train mean loss: 664.92
epoch train time: 0:00:00.177566
elapsed time: 0:00:17.816149
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 16:40:09.143192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.74
 ---- batch: 020 ----
mean loss: 645.90
 ---- batch: 030 ----
mean loss: 630.54
train mean loss: 639.04
epoch train time: 0:00:00.175301
elapsed time: 0:00:17.991591
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 16:40:09.318632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 627.94
 ---- batch: 020 ----
mean loss: 615.43
 ---- batch: 030 ----
mean loss: 603.11
train mean loss: 616.85
epoch train time: 0:00:00.182848
elapsed time: 0:00:18.174579
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 16:40:09.501644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.69
 ---- batch: 020 ----
mean loss: 604.52
 ---- batch: 030 ----
mean loss: 586.22
train mean loss: 596.26
epoch train time: 0:00:00.175284
elapsed time: 0:00:18.350025
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 16:40:09.677068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.36
 ---- batch: 020 ----
mean loss: 585.43
 ---- batch: 030 ----
mean loss: 570.85
train mean loss: 577.15
epoch train time: 0:00:00.178158
elapsed time: 0:00:18.528324
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 16:40:09.855366
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.08
 ---- batch: 020 ----
mean loss: 557.32
 ---- batch: 030 ----
mean loss: 566.07
train mean loss: 560.27
epoch train time: 0:00:00.175630
elapsed time: 0:00:18.704095
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 16:40:10.031136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.88
 ---- batch: 020 ----
mean loss: 546.91
 ---- batch: 030 ----
mean loss: 540.76
train mean loss: 543.43
epoch train time: 0:00:00.174340
elapsed time: 0:00:18.878575
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 16:40:10.205639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 535.67
 ---- batch: 020 ----
mean loss: 525.73
 ---- batch: 030 ----
mean loss: 530.11
train mean loss: 529.14
epoch train time: 0:00:00.172199
elapsed time: 0:00:19.050936
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 16:40:10.377979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 523.46
 ---- batch: 020 ----
mean loss: 517.10
 ---- batch: 030 ----
mean loss: 510.70
train mean loss: 516.19
epoch train time: 0:00:00.172790
elapsed time: 0:00:19.223884
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 16:40:10.550926
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.87
 ---- batch: 020 ----
mean loss: 503.47
 ---- batch: 030 ----
mean loss: 504.14
train mean loss: 503.82
epoch train time: 0:00:00.174371
elapsed time: 0:00:19.398430
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 16:40:10.725523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.91
 ---- batch: 020 ----
mean loss: 490.02
 ---- batch: 030 ----
mean loss: 491.09
train mean loss: 492.56
epoch train time: 0:00:00.179793
elapsed time: 0:00:19.578416
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 16:40:10.905461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.22
 ---- batch: 020 ----
mean loss: 475.87
 ---- batch: 030 ----
mean loss: 483.62
train mean loss: 482.08
epoch train time: 0:00:00.178394
elapsed time: 0:00:19.756956
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 16:40:11.083999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.32
 ---- batch: 020 ----
mean loss: 470.94
 ---- batch: 030 ----
mean loss: 468.69
train mean loss: 472.44
epoch train time: 0:00:00.177043
elapsed time: 0:00:19.934140
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 16:40:11.261183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.02
 ---- batch: 020 ----
mean loss: 472.42
 ---- batch: 030 ----
mean loss: 462.79
train mean loss: 464.11
epoch train time: 0:00:00.173889
elapsed time: 0:00:20.108200
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 16:40:11.435246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 456.04
 ---- batch: 020 ----
mean loss: 451.61
 ---- batch: 030 ----
mean loss: 458.89
train mean loss: 455.86
epoch train time: 0:00:00.175012
elapsed time: 0:00:20.283356
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 16:40:11.610428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 449.68
 ---- batch: 020 ----
mean loss: 446.92
 ---- batch: 030 ----
mean loss: 444.99
train mean loss: 448.75
epoch train time: 0:00:00.177263
elapsed time: 0:00:20.460815
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 16:40:11.787867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.53
 ---- batch: 020 ----
mean loss: 442.17
 ---- batch: 030 ----
mean loss: 441.01
train mean loss: 441.26
epoch train time: 0:00:00.184473
elapsed time: 0:00:20.645443
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 16:40:11.972486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 438.21
 ---- batch: 020 ----
mean loss: 440.19
 ---- batch: 030 ----
mean loss: 432.11
train mean loss: 434.94
epoch train time: 0:00:00.175527
elapsed time: 0:00:20.821113
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 16:40:12.148156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.83
 ---- batch: 020 ----
mean loss: 429.17
 ---- batch: 030 ----
mean loss: 423.05
train mean loss: 428.58
epoch train time: 0:00:00.175849
elapsed time: 0:00:20.997118
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 16:40:12.324160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.43
 ---- batch: 020 ----
mean loss: 423.13
 ---- batch: 030 ----
mean loss: 419.85
train mean loss: 423.50
epoch train time: 0:00:00.172096
elapsed time: 0:00:21.169351
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 16:40:12.496392
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.98
 ---- batch: 020 ----
mean loss: 416.60
 ---- batch: 030 ----
mean loss: 420.20
train mean loss: 418.00
epoch train time: 0:00:00.172388
elapsed time: 0:00:21.341891
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 16:40:12.668973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.30
 ---- batch: 020 ----
mean loss: 405.50
 ---- batch: 030 ----
mean loss: 417.44
train mean loss: 413.35
epoch train time: 0:00:00.173929
elapsed time: 0:00:21.516010
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 16:40:12.843054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.59
 ---- batch: 020 ----
mean loss: 412.64
 ---- batch: 030 ----
mean loss: 400.81
train mean loss: 408.60
epoch train time: 0:00:00.176400
elapsed time: 0:00:21.692558
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 16:40:13.019599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.09
 ---- batch: 020 ----
mean loss: 402.55
 ---- batch: 030 ----
mean loss: 407.03
train mean loss: 404.75
epoch train time: 0:00:00.177755
elapsed time: 0:00:21.870483
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 16:40:13.197589
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.18
 ---- batch: 020 ----
mean loss: 404.17
 ---- batch: 030 ----
mean loss: 394.82
train mean loss: 401.09
epoch train time: 0:00:00.194837
elapsed time: 0:00:22.065574
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 16:40:13.392625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.79
 ---- batch: 020 ----
mean loss: 396.75
 ---- batch: 030 ----
mean loss: 400.09
train mean loss: 397.80
epoch train time: 0:00:00.185377
elapsed time: 0:00:22.251127
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 16:40:13.578177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.46
 ---- batch: 020 ----
mean loss: 402.42
 ---- batch: 030 ----
mean loss: 391.76
train mean loss: 395.27
epoch train time: 0:00:00.181642
elapsed time: 0:00:22.432930
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 16:40:13.759979
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.27
 ---- batch: 020 ----
mean loss: 392.34
 ---- batch: 030 ----
mean loss: 392.85
train mean loss: 392.33
epoch train time: 0:00:00.176613
elapsed time: 0:00:22.609704
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 16:40:13.936753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.95
 ---- batch: 020 ----
mean loss: 399.45
 ---- batch: 030 ----
mean loss: 390.29
train mean loss: 389.98
epoch train time: 0:00:00.184134
elapsed time: 0:00:22.793992
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 16:40:14.121036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.05
 ---- batch: 020 ----
mean loss: 390.81
 ---- batch: 030 ----
mean loss: 382.80
train mean loss: 388.01
epoch train time: 0:00:00.182380
elapsed time: 0:00:22.976514
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 16:40:14.303558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.81
 ---- batch: 020 ----
mean loss: 379.42
 ---- batch: 030 ----
mean loss: 383.07
train mean loss: 386.63
epoch train time: 0:00:00.182626
elapsed time: 0:00:23.159289
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 16:40:14.486334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.77
 ---- batch: 020 ----
mean loss: 383.25
 ---- batch: 030 ----
mean loss: 390.83
train mean loss: 384.65
epoch train time: 0:00:00.182175
elapsed time: 0:00:23.341616
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 16:40:14.668678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.55
 ---- batch: 020 ----
mean loss: 386.50
 ---- batch: 030 ----
mean loss: 379.47
train mean loss: 383.19
epoch train time: 0:00:00.195478
elapsed time: 0:00:23.537255
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 16:40:14.864313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.27
 ---- batch: 020 ----
mean loss: 383.54
 ---- batch: 030 ----
mean loss: 380.45
train mean loss: 381.51
epoch train time: 0:00:00.181084
elapsed time: 0:00:23.718496
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 16:40:15.045540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.03
 ---- batch: 020 ----
mean loss: 376.82
 ---- batch: 030 ----
mean loss: 379.88
train mean loss: 380.61
epoch train time: 0:00:00.182394
elapsed time: 0:00:23.901034
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 16:40:15.228078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.83
 ---- batch: 020 ----
mean loss: 375.00
 ---- batch: 030 ----
mean loss: 377.90
train mean loss: 379.44
epoch train time: 0:00:00.180162
elapsed time: 0:00:24.081337
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 16:40:15.408378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.70
 ---- batch: 020 ----
mean loss: 375.28
 ---- batch: 030 ----
mean loss: 375.33
train mean loss: 378.15
epoch train time: 0:00:00.178749
elapsed time: 0:00:24.260229
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 16:40:15.587290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.16
 ---- batch: 020 ----
mean loss: 373.97
 ---- batch: 030 ----
mean loss: 378.61
train mean loss: 377.50
epoch train time: 0:00:00.186487
elapsed time: 0:00:24.446894
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 16:40:15.773935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.32
 ---- batch: 020 ----
mean loss: 379.74
 ---- batch: 030 ----
mean loss: 376.02
train mean loss: 376.15
epoch train time: 0:00:00.180715
elapsed time: 0:00:24.627747
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 16:40:15.954788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.63
 ---- batch: 020 ----
mean loss: 375.80
 ---- batch: 030 ----
mean loss: 375.35
train mean loss: 375.72
epoch train time: 0:00:00.177917
elapsed time: 0:00:24.805806
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 16:40:16.132864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.86
 ---- batch: 020 ----
mean loss: 366.65
 ---- batch: 030 ----
mean loss: 379.86
train mean loss: 374.62
epoch train time: 0:00:00.179559
elapsed time: 0:00:24.985521
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 16:40:16.312577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.49
 ---- batch: 020 ----
mean loss: 372.66
 ---- batch: 030 ----
mean loss: 368.31
train mean loss: 374.33
epoch train time: 0:00:00.175836
elapsed time: 0:00:25.161511
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 16:40:16.488555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.19
 ---- batch: 020 ----
mean loss: 374.63
 ---- batch: 030 ----
mean loss: 370.16
train mean loss: 373.33
epoch train time: 0:00:00.177354
elapsed time: 0:00:25.339032
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 16:40:16.666089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.76
 ---- batch: 020 ----
mean loss: 380.76
 ---- batch: 030 ----
mean loss: 369.27
train mean loss: 372.76
epoch train time: 0:00:00.178115
elapsed time: 0:00:25.517302
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 16:40:16.844345
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.78
 ---- batch: 020 ----
mean loss: 368.60
 ---- batch: 030 ----
mean loss: 375.21
train mean loss: 372.55
epoch train time: 0:00:00.177456
elapsed time: 0:00:25.694900
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 16:40:17.021963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.50
 ---- batch: 020 ----
mean loss: 375.88
 ---- batch: 030 ----
mean loss: 372.75
train mean loss: 372.02
epoch train time: 0:00:00.179172
elapsed time: 0:00:25.874251
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 16:40:17.201292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.93
 ---- batch: 020 ----
mean loss: 366.97
 ---- batch: 030 ----
mean loss: 371.97
train mean loss: 371.76
epoch train time: 0:00:00.179752
elapsed time: 0:00:26.054143
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 16:40:17.381187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.53
 ---- batch: 020 ----
mean loss: 374.30
 ---- batch: 030 ----
mean loss: 374.44
train mean loss: 370.97
epoch train time: 0:00:00.176643
elapsed time: 0:00:26.230948
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 16:40:17.557990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.34
 ---- batch: 020 ----
mean loss: 364.75
 ---- batch: 030 ----
mean loss: 366.76
train mean loss: 371.02
epoch train time: 0:00:00.179238
elapsed time: 0:00:26.410330
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 16:40:17.737373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.33
 ---- batch: 020 ----
mean loss: 370.52
 ---- batch: 030 ----
mean loss: 373.54
train mean loss: 370.62
epoch train time: 0:00:00.178758
elapsed time: 0:00:26.589230
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 16:40:17.916270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.63
 ---- batch: 020 ----
mean loss: 369.04
 ---- batch: 030 ----
mean loss: 381.27
train mean loss: 369.74
epoch train time: 0:00:00.170834
elapsed time: 0:00:26.760202
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 16:40:18.087261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.65
 ---- batch: 020 ----
mean loss: 371.58
 ---- batch: 030 ----
mean loss: 369.35
train mean loss: 369.66
epoch train time: 0:00:00.172410
elapsed time: 0:00:26.932770
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 16:40:18.259811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.74
 ---- batch: 020 ----
mean loss: 363.66
 ---- batch: 030 ----
mean loss: 370.75
train mean loss: 369.48
epoch train time: 0:00:00.172338
elapsed time: 0:00:27.105248
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 16:40:18.432290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.31
 ---- batch: 020 ----
mean loss: 372.44
 ---- batch: 030 ----
mean loss: 367.16
train mean loss: 368.96
epoch train time: 0:00:00.175854
elapsed time: 0:00:27.281241
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 16:40:18.608283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.01
 ---- batch: 020 ----
mean loss: 364.64
 ---- batch: 030 ----
mean loss: 368.84
train mean loss: 369.00
epoch train time: 0:00:00.172270
elapsed time: 0:00:27.453653
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 16:40:18.780696
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.19
 ---- batch: 020 ----
mean loss: 373.92
 ---- batch: 030 ----
mean loss: 367.23
train mean loss: 368.30
epoch train time: 0:00:00.174965
elapsed time: 0:00:27.628761
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 16:40:18.955804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.54
 ---- batch: 020 ----
mean loss: 369.12
 ---- batch: 030 ----
mean loss: 364.84
train mean loss: 368.08
epoch train time: 0:00:00.175143
elapsed time: 0:00:27.804058
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 16:40:19.131103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.91
 ---- batch: 020 ----
mean loss: 378.14
 ---- batch: 030 ----
mean loss: 359.79
train mean loss: 367.80
epoch train time: 0:00:00.176724
elapsed time: 0:00:27.980952
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 16:40:19.308025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 372.30
 ---- batch: 020 ----
mean loss: 363.38
 ---- batch: 030 ----
mean loss: 362.76
train mean loss: 367.72
epoch train time: 0:00:00.173334
elapsed time: 0:00:28.154504
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 16:40:19.481545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.00
 ---- batch: 020 ----
mean loss: 363.81
 ---- batch: 030 ----
mean loss: 371.22
train mean loss: 367.18
epoch train time: 0:00:00.176623
elapsed time: 0:00:28.331265
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 16:40:19.658306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.92
 ---- batch: 020 ----
mean loss: 369.80
 ---- batch: 030 ----
mean loss: 370.38
train mean loss: 367.17
epoch train time: 0:00:00.181409
elapsed time: 0:00:28.512818
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 16:40:19.839860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.56
 ---- batch: 020 ----
mean loss: 366.63
 ---- batch: 030 ----
mean loss: 369.77
train mean loss: 366.59
epoch train time: 0:00:00.173816
elapsed time: 0:00:28.686776
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 16:40:20.013816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.75
 ---- batch: 020 ----
mean loss: 366.94
 ---- batch: 030 ----
mean loss: 363.46
train mean loss: 366.55
epoch train time: 0:00:00.172184
elapsed time: 0:00:28.859126
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 16:40:20.186169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.25
 ---- batch: 020 ----
mean loss: 364.85
 ---- batch: 030 ----
mean loss: 367.36
train mean loss: 366.44
epoch train time: 0:00:00.177241
elapsed time: 0:00:29.036520
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 16:40:20.363560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.88
 ---- batch: 020 ----
mean loss: 360.39
 ---- batch: 030 ----
mean loss: 373.86
train mean loss: 366.46
epoch train time: 0:00:00.176548
elapsed time: 0:00:29.213202
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 16:40:20.540269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.99
 ---- batch: 020 ----
mean loss: 363.27
 ---- batch: 030 ----
mean loss: 371.24
train mean loss: 365.68
epoch train time: 0:00:00.175031
elapsed time: 0:00:29.388402
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 16:40:20.715454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 362.51
 ---- batch: 020 ----
mean loss: 365.07
 ---- batch: 030 ----
mean loss: 370.26
train mean loss: 365.60
epoch train time: 0:00:00.178306
elapsed time: 0:00:29.566863
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 16:40:20.893908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.79
 ---- batch: 020 ----
mean loss: 363.23
 ---- batch: 030 ----
mean loss: 362.01
train mean loss: 365.59
epoch train time: 0:00:00.179386
elapsed time: 0:00:29.746394
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 16:40:21.073439
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.48
 ---- batch: 020 ----
mean loss: 364.01
 ---- batch: 030 ----
mean loss: 362.59
train mean loss: 365.62
epoch train time: 0:00:00.180159
elapsed time: 0:00:29.926732
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 16:40:21.253788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.15
 ---- batch: 020 ----
mean loss: 361.58
 ---- batch: 030 ----
mean loss: 367.07
train mean loss: 364.90
epoch train time: 0:00:00.181302
elapsed time: 0:00:30.108210
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 16:40:21.435256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.60
 ---- batch: 020 ----
mean loss: 367.78
 ---- batch: 030 ----
mean loss: 359.50
train mean loss: 365.26
epoch train time: 0:00:00.178232
elapsed time: 0:00:30.286590
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 16:40:21.613664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.85
 ---- batch: 020 ----
mean loss: 369.59
 ---- batch: 030 ----
mean loss: 365.08
train mean loss: 364.75
epoch train time: 0:00:00.182900
elapsed time: 0:00:30.469666
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 16:40:21.796710
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.90
 ---- batch: 020 ----
mean loss: 365.25
 ---- batch: 030 ----
mean loss: 364.94
train mean loss: 364.20
epoch train time: 0:00:00.177041
elapsed time: 0:00:30.646871
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 16:40:21.973914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.35
 ---- batch: 020 ----
mean loss: 369.97
 ---- batch: 030 ----
mean loss: 364.92
train mean loss: 364.44
epoch train time: 0:00:00.175515
elapsed time: 0:00:30.822528
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 16:40:22.149569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.44
 ---- batch: 020 ----
mean loss: 370.63
 ---- batch: 030 ----
mean loss: 366.13
train mean loss: 363.99
epoch train time: 0:00:00.176393
elapsed time: 0:00:30.999484
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 16:40:22.326537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.95
 ---- batch: 020 ----
mean loss: 364.54
 ---- batch: 030 ----
mean loss: 370.04
train mean loss: 363.98
epoch train time: 0:00:00.179032
elapsed time: 0:00:31.178733
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 16:40:22.505808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.48
 ---- batch: 020 ----
mean loss: 366.46
 ---- batch: 030 ----
mean loss: 365.85
train mean loss: 363.81
epoch train time: 0:00:00.181172
elapsed time: 0:00:31.360091
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 16:40:22.687124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.82
 ---- batch: 020 ----
mean loss: 356.09
 ---- batch: 030 ----
mean loss: 367.48
train mean loss: 363.58
epoch train time: 0:00:00.179331
elapsed time: 0:00:31.539559
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 16:40:22.866622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.43
 ---- batch: 020 ----
mean loss: 370.41
 ---- batch: 030 ----
mean loss: 358.77
train mean loss: 363.65
epoch train time: 0:00:00.176392
elapsed time: 0:00:31.716129
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 16:40:23.043171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 368.45
 ---- batch: 020 ----
mean loss: 357.39
 ---- batch: 030 ----
mean loss: 365.60
train mean loss: 363.44
epoch train time: 0:00:00.178898
elapsed time: 0:00:31.895172
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 16:40:23.222213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.64
 ---- batch: 020 ----
mean loss: 362.37
 ---- batch: 030 ----
mean loss: 363.83
train mean loss: 362.69
epoch train time: 0:00:00.176110
elapsed time: 0:00:32.071430
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 16:40:23.398474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.31
 ---- batch: 020 ----
mean loss: 361.95
 ---- batch: 030 ----
mean loss: 365.69
train mean loss: 362.73
epoch train time: 0:00:00.178185
elapsed time: 0:00:32.249759
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 16:40:23.576801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.77
 ---- batch: 020 ----
mean loss: 359.65
 ---- batch: 030 ----
mean loss: 361.27
train mean loss: 363.20
epoch train time: 0:00:00.179932
elapsed time: 0:00:32.429833
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 16:40:23.756875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.92
 ---- batch: 020 ----
mean loss: 361.93
 ---- batch: 030 ----
mean loss: 366.20
train mean loss: 362.39
epoch train time: 0:00:00.185169
elapsed time: 0:00:32.615146
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 16:40:23.942187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.07
 ---- batch: 020 ----
mean loss: 365.02
 ---- batch: 030 ----
mean loss: 370.39
train mean loss: 362.35
epoch train time: 0:00:00.184179
elapsed time: 0:00:32.799466
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 16:40:24.126514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.14
 ---- batch: 020 ----
mean loss: 363.25
 ---- batch: 030 ----
mean loss: 360.88
train mean loss: 362.66
epoch train time: 0:00:00.182587
elapsed time: 0:00:32.982200
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 16:40:24.309243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.77
 ---- batch: 020 ----
mean loss: 363.06
 ---- batch: 030 ----
mean loss: 363.63
train mean loss: 361.79
epoch train time: 0:00:00.193834
elapsed time: 0:00:33.176176
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 16:40:24.503217
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.49
 ---- batch: 020 ----
mean loss: 364.71
 ---- batch: 030 ----
mean loss: 360.30
train mean loss: 361.32
epoch train time: 0:00:00.178428
elapsed time: 0:00:33.354755
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 16:40:24.681799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.76
 ---- batch: 020 ----
mean loss: 365.35
 ---- batch: 030 ----
mean loss: 360.80
train mean loss: 361.81
epoch train time: 0:00:00.181262
elapsed time: 0:00:33.536157
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 16:40:24.863196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.88
 ---- batch: 020 ----
mean loss: 358.94
 ---- batch: 030 ----
mean loss: 360.08
train mean loss: 361.51
epoch train time: 0:00:00.184134
elapsed time: 0:00:33.720437
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 16:40:25.047488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.96
 ---- batch: 020 ----
mean loss: 358.17
 ---- batch: 030 ----
mean loss: 359.14
train mean loss: 361.47
epoch train time: 0:00:00.181163
elapsed time: 0:00:33.901751
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 16:40:25.228793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.93
 ---- batch: 020 ----
mean loss: 359.00
 ---- batch: 030 ----
mean loss: 368.06
train mean loss: 360.99
epoch train time: 0:00:00.181212
elapsed time: 0:00:34.083151
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 16:40:25.410216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.50
 ---- batch: 020 ----
mean loss: 359.34
 ---- batch: 030 ----
mean loss: 364.89
train mean loss: 361.13
epoch train time: 0:00:00.181380
elapsed time: 0:00:34.264712
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 16:40:25.591760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.28
 ---- batch: 020 ----
mean loss: 362.74
 ---- batch: 030 ----
mean loss: 359.54
train mean loss: 360.71
epoch train time: 0:00:00.183295
elapsed time: 0:00:34.448215
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 16:40:25.775261
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 364.85
 ---- batch: 020 ----
mean loss: 355.21
 ---- batch: 030 ----
mean loss: 363.64
train mean loss: 360.43
epoch train time: 0:00:00.176518
elapsed time: 0:00:34.624876
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 16:40:25.951918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.25
 ---- batch: 020 ----
mean loss: 358.51
 ---- batch: 030 ----
mean loss: 352.26
train mean loss: 360.82
epoch train time: 0:00:00.177782
elapsed time: 0:00:34.802801
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 16:40:26.129848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.32
 ---- batch: 020 ----
mean loss: 366.38
 ---- batch: 030 ----
mean loss: 361.03
train mean loss: 360.29
epoch train time: 0:00:00.180167
elapsed time: 0:00:34.983143
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 16:40:26.310186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.35
 ---- batch: 020 ----
mean loss: 359.18
 ---- batch: 030 ----
mean loss: 364.72
train mean loss: 360.23
epoch train time: 0:00:00.187176
elapsed time: 0:00:35.170487
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 16:40:26.497547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.23
 ---- batch: 020 ----
mean loss: 363.03
 ---- batch: 030 ----
mean loss: 361.25
train mean loss: 359.83
epoch train time: 0:00:00.178817
elapsed time: 0:00:35.349463
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 16:40:26.676505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.55
 ---- batch: 020 ----
mean loss: 361.57
 ---- batch: 030 ----
mean loss: 361.82
train mean loss: 359.68
epoch train time: 0:00:00.179221
elapsed time: 0:00:35.528824
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 16:40:26.855884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.51
 ---- batch: 020 ----
mean loss: 359.55
 ---- batch: 030 ----
mean loss: 370.20
train mean loss: 360.00
epoch train time: 0:00:00.192500
elapsed time: 0:00:35.721532
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 16:40:27.048581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.80
 ---- batch: 020 ----
mean loss: 372.03
 ---- batch: 030 ----
mean loss: 352.83
train mean loss: 359.60
epoch train time: 0:00:00.176167
elapsed time: 0:00:35.897845
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 16:40:27.224889
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.01
 ---- batch: 020 ----
mean loss: 363.72
 ---- batch: 030 ----
mean loss: 366.95
train mean loss: 359.46
epoch train time: 0:00:00.174855
elapsed time: 0:00:36.072841
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 16:40:27.399882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 365.49
 ---- batch: 020 ----
mean loss: 357.38
 ---- batch: 030 ----
mean loss: 352.69
train mean loss: 359.25
epoch train time: 0:00:00.175589
elapsed time: 0:00:36.248571
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 16:40:27.575613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.20
 ---- batch: 020 ----
mean loss: 355.97
 ---- batch: 030 ----
mean loss: 365.57
train mean loss: 358.96
epoch train time: 0:00:00.176920
elapsed time: 0:00:36.425633
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 16:40:27.752699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.29
 ---- batch: 020 ----
mean loss: 361.32
 ---- batch: 030 ----
mean loss: 366.45
train mean loss: 359.14
epoch train time: 0:00:00.183986
elapsed time: 0:00:36.609785
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 16:40:27.936831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.52
 ---- batch: 020 ----
mean loss: 354.74
 ---- batch: 030 ----
mean loss: 354.79
train mean loss: 358.97
epoch train time: 0:00:00.181535
elapsed time: 0:00:36.791465
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 16:40:28.118508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.02
 ---- batch: 020 ----
mean loss: 357.27
 ---- batch: 030 ----
mean loss: 360.50
train mean loss: 358.74
epoch train time: 0:00:00.180024
elapsed time: 0:00:36.971631
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 16:40:28.298674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.95
 ---- batch: 020 ----
mean loss: 355.44
 ---- batch: 030 ----
mean loss: 361.71
train mean loss: 358.21
epoch train time: 0:00:00.180328
elapsed time: 0:00:37.152163
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 16:40:28.479219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.41
 ---- batch: 020 ----
mean loss: 355.16
 ---- batch: 030 ----
mean loss: 351.51
train mean loss: 358.51
epoch train time: 0:00:00.182572
elapsed time: 0:00:37.334895
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 16:40:28.661940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.71
 ---- batch: 020 ----
mean loss: 360.54
 ---- batch: 030 ----
mean loss: 353.35
train mean loss: 358.54
epoch train time: 0:00:00.183635
elapsed time: 0:00:37.518694
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 16:40:28.845740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.38
 ---- batch: 020 ----
mean loss: 356.75
 ---- batch: 030 ----
mean loss: 357.72
train mean loss: 358.39
epoch train time: 0:00:00.176879
elapsed time: 0:00:37.695720
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 16:40:29.022765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.83
 ---- batch: 020 ----
mean loss: 362.96
 ---- batch: 030 ----
mean loss: 357.43
train mean loss: 357.98
epoch train time: 0:00:00.178406
elapsed time: 0:00:37.874296
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 16:40:29.201337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.29
 ---- batch: 020 ----
mean loss: 365.03
 ---- batch: 030 ----
mean loss: 353.67
train mean loss: 357.75
epoch train time: 0:00:00.176411
elapsed time: 0:00:38.050849
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 16:40:29.377891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.51
 ---- batch: 020 ----
mean loss: 357.69
 ---- batch: 030 ----
mean loss: 357.57
train mean loss: 357.49
epoch train time: 0:00:00.179437
elapsed time: 0:00:38.230452
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 16:40:29.557527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.00
 ---- batch: 020 ----
mean loss: 359.31
 ---- batch: 030 ----
mean loss: 355.17
train mean loss: 357.55
epoch train time: 0:00:00.177819
elapsed time: 0:00:38.408459
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 16:40:29.735503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.42
 ---- batch: 020 ----
mean loss: 358.28
 ---- batch: 030 ----
mean loss: 358.53
train mean loss: 357.22
epoch train time: 0:00:00.178980
elapsed time: 0:00:38.587580
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 16:40:29.914621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.25
 ---- batch: 020 ----
mean loss: 352.92
 ---- batch: 030 ----
mean loss: 357.12
train mean loss: 356.97
epoch train time: 0:00:00.177824
elapsed time: 0:00:38.765553
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 16:40:30.092622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.16
 ---- batch: 020 ----
mean loss: 353.78
 ---- batch: 030 ----
mean loss: 361.06
train mean loss: 356.93
epoch train time: 0:00:00.178549
elapsed time: 0:00:38.944282
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 16:40:30.271331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.09
 ---- batch: 020 ----
mean loss: 357.19
 ---- batch: 030 ----
mean loss: 363.54
train mean loss: 356.77
epoch train time: 0:00:00.178242
elapsed time: 0:00:39.122692
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 16:40:30.449737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.25
 ---- batch: 020 ----
mean loss: 361.97
 ---- batch: 030 ----
mean loss: 350.96
train mean loss: 356.70
epoch train time: 0:00:00.180366
elapsed time: 0:00:39.303233
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 16:40:30.630275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.42
 ---- batch: 020 ----
mean loss: 363.44
 ---- batch: 030 ----
mean loss: 353.10
train mean loss: 356.57
epoch train time: 0:00:00.179068
elapsed time: 0:00:39.482452
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 16:40:30.809496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.92
 ---- batch: 020 ----
mean loss: 354.84
 ---- batch: 030 ----
mean loss: 353.66
train mean loss: 356.84
epoch train time: 0:00:00.178881
elapsed time: 0:00:39.661486
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 16:40:30.988528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.95
 ---- batch: 020 ----
mean loss: 355.22
 ---- batch: 030 ----
mean loss: 360.81
train mean loss: 356.22
epoch train time: 0:00:00.178812
elapsed time: 0:00:39.840437
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 16:40:31.167479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.80
 ---- batch: 020 ----
mean loss: 358.61
 ---- batch: 030 ----
mean loss: 360.01
train mean loss: 356.04
epoch train time: 0:00:00.175368
elapsed time: 0:00:40.015962
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 16:40:31.343004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 357.01
 ---- batch: 020 ----
mean loss: 353.42
 ---- batch: 030 ----
mean loss: 353.76
train mean loss: 356.29
epoch train time: 0:00:00.175244
elapsed time: 0:00:40.191360
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 16:40:31.518399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 359.26
 ---- batch: 020 ----
mean loss: 356.38
 ---- batch: 030 ----
mean loss: 352.07
train mean loss: 355.83
epoch train time: 0:00:00.176653
elapsed time: 0:00:40.368202
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 16:40:31.695277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.03
 ---- batch: 020 ----
mean loss: 360.30
 ---- batch: 030 ----
mean loss: 357.74
train mean loss: 355.83
epoch train time: 0:00:00.183145
elapsed time: 0:00:40.551522
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 16:40:31.878565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.26
 ---- batch: 020 ----
mean loss: 351.10
 ---- batch: 030 ----
mean loss: 355.96
train mean loss: 355.49
epoch train time: 0:00:00.178966
elapsed time: 0:00:40.730651
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 16:40:32.057712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.19
 ---- batch: 020 ----
mean loss: 358.78
 ---- batch: 030 ----
mean loss: 352.14
train mean loss: 355.56
epoch train time: 0:00:00.178939
elapsed time: 0:00:40.909760
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 16:40:32.236804
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.28
 ---- batch: 020 ----
mean loss: 365.48
 ---- batch: 030 ----
mean loss: 348.19
train mean loss: 354.78
epoch train time: 0:00:00.178969
elapsed time: 0:00:41.088872
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 16:40:32.415915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.51
 ---- batch: 020 ----
mean loss: 351.61
 ---- batch: 030 ----
mean loss: 359.61
train mean loss: 354.99
epoch train time: 0:00:00.176816
elapsed time: 0:00:41.265836
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 16:40:32.592906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.78
 ---- batch: 020 ----
mean loss: 358.44
 ---- batch: 030 ----
mean loss: 358.29
train mean loss: 355.14
epoch train time: 0:00:00.173957
elapsed time: 0:00:41.439961
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 16:40:32.767002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.68
 ---- batch: 020 ----
mean loss: 357.30
 ---- batch: 030 ----
mean loss: 354.35
train mean loss: 354.99
epoch train time: 0:00:00.181256
elapsed time: 0:00:41.621354
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 16:40:32.948395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.18
 ---- batch: 020 ----
mean loss: 361.05
 ---- batch: 030 ----
mean loss: 352.68
train mean loss: 354.65
epoch train time: 0:00:00.181378
elapsed time: 0:00:41.802885
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 16:40:33.129924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.01
 ---- batch: 020 ----
mean loss: 360.18
 ---- batch: 030 ----
mean loss: 349.19
train mean loss: 354.10
epoch train time: 0:00:00.179146
elapsed time: 0:00:41.982169
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 16:40:33.309212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 356.22
 ---- batch: 020 ----
mean loss: 353.87
 ---- batch: 030 ----
mean loss: 355.34
train mean loss: 354.30
epoch train time: 0:00:00.177564
elapsed time: 0:00:42.159875
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 16:40:33.486927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.05
 ---- batch: 020 ----
mean loss: 346.96
 ---- batch: 030 ----
mean loss: 362.82
train mean loss: 354.66
epoch train time: 0:00:00.183249
elapsed time: 0:00:42.343289
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 16:40:33.670330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.94
 ---- batch: 020 ----
mean loss: 352.48
 ---- batch: 030 ----
mean loss: 354.49
train mean loss: 354.22
epoch train time: 0:00:00.180261
elapsed time: 0:00:42.523694
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 16:40:33.850737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.90
 ---- batch: 020 ----
mean loss: 357.22
 ---- batch: 030 ----
mean loss: 351.05
train mean loss: 353.94
epoch train time: 0:00:00.179753
elapsed time: 0:00:42.703588
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 16:40:34.030645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.27
 ---- batch: 020 ----
mean loss: 353.43
 ---- batch: 030 ----
mean loss: 352.06
train mean loss: 353.72
epoch train time: 0:00:00.175506
elapsed time: 0:00:42.879246
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 16:40:34.206287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.54
 ---- batch: 020 ----
mean loss: 360.26
 ---- batch: 030 ----
mean loss: 360.94
train mean loss: 353.57
epoch train time: 0:00:00.178846
elapsed time: 0:00:43.058231
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 16:40:34.385274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.94
 ---- batch: 020 ----
mean loss: 348.43
 ---- batch: 030 ----
mean loss: 355.38
train mean loss: 353.95
epoch train time: 0:00:00.179065
elapsed time: 0:00:43.237456
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 16:40:34.564500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.64
 ---- batch: 020 ----
mean loss: 356.82
 ---- batch: 030 ----
mean loss: 350.05
train mean loss: 353.57
epoch train time: 0:00:00.181967
elapsed time: 0:00:43.419581
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 16:40:34.746613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.56
 ---- batch: 020 ----
mean loss: 356.50
 ---- batch: 030 ----
mean loss: 354.15
train mean loss: 353.18
epoch train time: 0:00:00.192397
elapsed time: 0:00:43.612117
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 16:40:34.939168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.58
 ---- batch: 020 ----
mean loss: 355.48
 ---- batch: 030 ----
mean loss: 350.60
train mean loss: 353.08
epoch train time: 0:00:00.181459
elapsed time: 0:00:43.793739
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 16:40:35.120810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.23
 ---- batch: 020 ----
mean loss: 347.78
 ---- batch: 030 ----
mean loss: 359.05
train mean loss: 352.84
epoch train time: 0:00:00.176204
elapsed time: 0:00:43.970110
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 16:40:35.297149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.04
 ---- batch: 020 ----
mean loss: 351.49
 ---- batch: 030 ----
mean loss: 346.64
train mean loss: 352.66
epoch train time: 0:00:00.175861
elapsed time: 0:00:44.146108
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 16:40:35.473149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.39
 ---- batch: 020 ----
mean loss: 359.13
 ---- batch: 030 ----
mean loss: 346.45
train mean loss: 352.86
epoch train time: 0:00:00.178354
elapsed time: 0:00:44.324599
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 16:40:35.651640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.05
 ---- batch: 020 ----
mean loss: 352.34
 ---- batch: 030 ----
mean loss: 356.39
train mean loss: 352.38
epoch train time: 0:00:00.180411
elapsed time: 0:00:44.505177
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 16:40:35.832251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.97
 ---- batch: 020 ----
mean loss: 356.96
 ---- batch: 030 ----
mean loss: 346.97
train mean loss: 352.68
epoch train time: 0:00:00.177275
elapsed time: 0:00:44.682681
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 16:40:36.009729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.63
 ---- batch: 020 ----
mean loss: 350.64
 ---- batch: 030 ----
mean loss: 356.90
train mean loss: 352.10
epoch train time: 0:00:00.177156
elapsed time: 0:00:44.860012
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 16:40:36.187054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.02
 ---- batch: 020 ----
mean loss: 349.28
 ---- batch: 030 ----
mean loss: 352.87
train mean loss: 352.26
epoch train time: 0:00:00.174575
elapsed time: 0:00:45.034757
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 16:40:36.361801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.04
 ---- batch: 020 ----
mean loss: 351.96
 ---- batch: 030 ----
mean loss: 354.05
train mean loss: 352.13
epoch train time: 0:00:00.175018
elapsed time: 0:00:45.209930
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 16:40:36.536995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.90
 ---- batch: 020 ----
mean loss: 347.11
 ---- batch: 030 ----
mean loss: 351.44
train mean loss: 351.91
epoch train time: 0:00:00.181842
elapsed time: 0:00:45.391949
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 16:40:36.719002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 355.60
 ---- batch: 020 ----
mean loss: 357.54
 ---- batch: 030 ----
mean loss: 342.85
train mean loss: 351.91
epoch train time: 0:00:00.188344
elapsed time: 0:00:45.580444
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 16:40:36.907525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.72
 ---- batch: 020 ----
mean loss: 356.22
 ---- batch: 030 ----
mean loss: 347.53
train mean loss: 351.53
epoch train time: 0:00:00.181169
elapsed time: 0:00:45.761790
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 16:40:37.088832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.26
 ---- batch: 020 ----
mean loss: 358.15
 ---- batch: 030 ----
mean loss: 351.14
train mean loss: 351.49
epoch train time: 0:00:00.177014
elapsed time: 0:00:45.938947
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 16:40:37.265990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.85
 ---- batch: 020 ----
mean loss: 345.17
 ---- batch: 030 ----
mean loss: 356.66
train mean loss: 351.15
epoch train time: 0:00:00.177947
elapsed time: 0:00:46.117043
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 16:40:37.444115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.59
 ---- batch: 020 ----
mean loss: 354.21
 ---- batch: 030 ----
mean loss: 355.01
train mean loss: 350.87
epoch train time: 0:00:00.175935
elapsed time: 0:00:46.293150
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 16:40:37.620191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.62
 ---- batch: 020 ----
mean loss: 354.50
 ---- batch: 030 ----
mean loss: 351.21
train mean loss: 350.92
epoch train time: 0:00:00.175703
elapsed time: 0:00:46.469006
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 16:40:37.796049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.69
 ---- batch: 020 ----
mean loss: 356.29
 ---- batch: 030 ----
mean loss: 347.69
train mean loss: 350.53
epoch train time: 0:00:00.176179
elapsed time: 0:00:46.645328
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 16:40:37.972373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.94
 ---- batch: 020 ----
mean loss: 354.70
 ---- batch: 030 ----
mean loss: 345.49
train mean loss: 350.42
epoch train time: 0:00:00.178359
elapsed time: 0:00:46.823839
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 16:40:38.150899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.35
 ---- batch: 020 ----
mean loss: 349.93
 ---- batch: 030 ----
mean loss: 348.36
train mean loss: 350.48
epoch train time: 0:00:00.174464
elapsed time: 0:00:46.998463
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 16:40:38.325507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 346.67
 ---- batch: 020 ----
mean loss: 353.62
 ---- batch: 030 ----
mean loss: 349.42
train mean loss: 350.27
epoch train time: 0:00:00.171902
elapsed time: 0:00:47.170512
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 16:40:38.497555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.78
 ---- batch: 020 ----
mean loss: 345.10
 ---- batch: 030 ----
mean loss: 349.78
train mean loss: 349.99
epoch train time: 0:00:00.172830
elapsed time: 0:00:47.343485
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 16:40:38.670528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 354.34
 ---- batch: 020 ----
mean loss: 349.73
 ---- batch: 030 ----
mean loss: 347.00
train mean loss: 349.58
epoch train time: 0:00:00.171998
elapsed time: 0:00:47.515623
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 16:40:38.842665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.76
 ---- batch: 020 ----
mean loss: 353.81
 ---- batch: 030 ----
mean loss: 355.77
train mean loss: 349.72
epoch train time: 0:00:00.171313
elapsed time: 0:00:47.687079
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 16:40:39.014122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.59
 ---- batch: 020 ----
mean loss: 348.33
 ---- batch: 030 ----
mean loss: 349.01
train mean loss: 349.60
epoch train time: 0:00:00.171689
elapsed time: 0:00:47.858913
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 16:40:39.185955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.19
 ---- batch: 020 ----
mean loss: 350.06
 ---- batch: 030 ----
mean loss: 351.82
train mean loss: 348.94
epoch train time: 0:00:00.170767
elapsed time: 0:00:48.029824
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 16:40:39.356867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.71
 ---- batch: 020 ----
mean loss: 357.39
 ---- batch: 030 ----
mean loss: 345.28
train mean loss: 349.45
epoch train time: 0:00:00.171603
elapsed time: 0:00:48.201567
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 16:40:39.528610
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.89
 ---- batch: 020 ----
mean loss: 345.32
 ---- batch: 030 ----
mean loss: 351.57
train mean loss: 349.15
epoch train time: 0:00:00.172443
elapsed time: 0:00:48.374185
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 16:40:39.701220
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.99
 ---- batch: 020 ----
mean loss: 350.42
 ---- batch: 030 ----
mean loss: 354.18
train mean loss: 349.22
epoch train time: 0:00:00.178820
elapsed time: 0:00:48.553148
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 16:40:39.880208
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.58
 ---- batch: 020 ----
mean loss: 351.85
 ---- batch: 030 ----
mean loss: 342.58
train mean loss: 349.31
epoch train time: 0:00:00.171041
elapsed time: 0:00:48.724346
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 16:40:40.051388
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.49
 ---- batch: 020 ----
mean loss: 345.01
 ---- batch: 030 ----
mean loss: 356.27
train mean loss: 349.12
epoch train time: 0:00:00.170357
elapsed time: 0:00:48.894846
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 16:40:40.221889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.71
 ---- batch: 020 ----
mean loss: 348.34
 ---- batch: 030 ----
mean loss: 352.38
train mean loss: 348.82
epoch train time: 0:00:00.171506
elapsed time: 0:00:49.066494
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 16:40:40.393537
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.15
 ---- batch: 020 ----
mean loss: 347.94
 ---- batch: 030 ----
mean loss: 346.61
train mean loss: 349.11
epoch train time: 0:00:00.175196
elapsed time: 0:00:49.241845
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 16:40:40.568890
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.10
 ---- batch: 020 ----
mean loss: 349.91
 ---- batch: 030 ----
mean loss: 355.23
train mean loss: 348.88
epoch train time: 0:00:00.171337
elapsed time: 0:00:49.413327
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 16:40:40.740370
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.45
 ---- batch: 020 ----
mean loss: 342.63
 ---- batch: 030 ----
mean loss: 350.32
train mean loss: 349.24
epoch train time: 0:00:00.172365
elapsed time: 0:00:49.585842
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 16:40:40.912885
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.53
 ---- batch: 020 ----
mean loss: 356.34
 ---- batch: 030 ----
mean loss: 350.42
train mean loss: 348.82
epoch train time: 0:00:00.171296
elapsed time: 0:00:49.757279
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 16:40:41.084322
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.39
 ---- batch: 020 ----
mean loss: 346.56
 ---- batch: 030 ----
mean loss: 352.34
train mean loss: 349.56
epoch train time: 0:00:00.170761
elapsed time: 0:00:49.928195
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 16:40:41.255257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.71
 ---- batch: 020 ----
mean loss: 348.47
 ---- batch: 030 ----
mean loss: 355.53
train mean loss: 348.69
epoch train time: 0:00:00.169331
elapsed time: 0:00:50.097687
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 16:40:41.424729
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.79
 ---- batch: 020 ----
mean loss: 356.04
 ---- batch: 030 ----
mean loss: 346.03
train mean loss: 348.58
epoch train time: 0:00:00.170669
elapsed time: 0:00:50.268498
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 16:40:41.595541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.26
 ---- batch: 020 ----
mean loss: 346.89
 ---- batch: 030 ----
mean loss: 344.99
train mean loss: 348.48
epoch train time: 0:00:00.172937
elapsed time: 0:00:50.441582
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 16:40:41.768626
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.63
 ---- batch: 020 ----
mean loss: 345.42
 ---- batch: 030 ----
mean loss: 350.12
train mean loss: 349.33
epoch train time: 0:00:00.177229
elapsed time: 0:00:50.618965
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 16:40:41.946010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.96
 ---- batch: 020 ----
mean loss: 344.44
 ---- batch: 030 ----
mean loss: 349.69
train mean loss: 348.78
epoch train time: 0:00:00.172256
elapsed time: 0:00:50.791366
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 16:40:42.118437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.83
 ---- batch: 020 ----
mean loss: 350.27
 ---- batch: 030 ----
mean loss: 346.28
train mean loss: 348.51
epoch train time: 0:00:00.172498
elapsed time: 0:00:50.964037
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 16:40:42.291080
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.91
 ---- batch: 020 ----
mean loss: 346.30
 ---- batch: 030 ----
mean loss: 351.50
train mean loss: 349.20
epoch train time: 0:00:00.174961
elapsed time: 0:00:51.139150
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 16:40:42.466195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.41
 ---- batch: 020 ----
mean loss: 348.73
 ---- batch: 030 ----
mean loss: 345.60
train mean loss: 349.02
epoch train time: 0:00:00.172023
elapsed time: 0:00:51.311319
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 16:40:42.638362
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.70
 ---- batch: 020 ----
mean loss: 351.54
 ---- batch: 030 ----
mean loss: 343.76
train mean loss: 348.77
epoch train time: 0:00:00.176622
elapsed time: 0:00:51.488089
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 16:40:42.815133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.28
 ---- batch: 020 ----
mean loss: 344.73
 ---- batch: 030 ----
mean loss: 347.18
train mean loss: 348.66
epoch train time: 0:00:00.172109
elapsed time: 0:00:51.660353
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 16:40:42.987424
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.13
 ---- batch: 020 ----
mean loss: 347.16
 ---- batch: 030 ----
mean loss: 345.43
train mean loss: 348.78
epoch train time: 0:00:00.170052
elapsed time: 0:00:51.830573
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 16:40:43.157674
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.06
 ---- batch: 020 ----
mean loss: 335.94
 ---- batch: 030 ----
mean loss: 352.06
train mean loss: 348.54
epoch train time: 0:00:00.168973
elapsed time: 0:00:51.999747
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 16:40:43.326789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.42
 ---- batch: 020 ----
mean loss: 346.57
 ---- batch: 030 ----
mean loss: 349.65
train mean loss: 348.63
epoch train time: 0:00:00.171217
elapsed time: 0:00:52.171106
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 16:40:43.498148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.04
 ---- batch: 020 ----
mean loss: 349.25
 ---- batch: 030 ----
mean loss: 351.10
train mean loss: 348.83
epoch train time: 0:00:00.168632
elapsed time: 0:00:52.339879
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 16:40:43.666929
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.42
 ---- batch: 020 ----
mean loss: 347.59
 ---- batch: 030 ----
mean loss: 345.13
train mean loss: 348.76
epoch train time: 0:00:00.173058
elapsed time: 0:00:52.513084
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 16:40:43.840126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.26
 ---- batch: 020 ----
mean loss: 344.23
 ---- batch: 030 ----
mean loss: 346.64
train mean loss: 348.48
epoch train time: 0:00:00.172269
elapsed time: 0:00:52.685493
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 16:40:44.012536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.50
 ---- batch: 020 ----
mean loss: 342.29
 ---- batch: 030 ----
mean loss: 352.07
train mean loss: 348.49
epoch train time: 0:00:00.170296
elapsed time: 0:00:52.855931
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 16:40:44.182972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 341.56
 ---- batch: 020 ----
mean loss: 357.81
 ---- batch: 030 ----
mean loss: 349.94
train mean loss: 348.47
epoch train time: 0:00:00.169724
elapsed time: 0:00:53.025792
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 16:40:44.352911
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 344.82
 ---- batch: 020 ----
mean loss: 351.09
 ---- batch: 030 ----
mean loss: 348.88
train mean loss: 349.14
epoch train time: 0:00:00.168409
elapsed time: 0:00:53.194418
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 16:40:44.521479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.48
 ---- batch: 020 ----
mean loss: 355.72
 ---- batch: 030 ----
mean loss: 340.63
train mean loss: 348.13
epoch train time: 0:00:00.171983
elapsed time: 0:00:53.366569
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 16:40:44.693668
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 349.65
 ---- batch: 020 ----
mean loss: 339.12
 ---- batch: 030 ----
mean loss: 356.38
train mean loss: 348.40
epoch train time: 0:00:00.176120
elapsed time: 0:00:53.542902
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 16:40:44.869944
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.95
 ---- batch: 020 ----
mean loss: 342.45
 ---- batch: 030 ----
mean loss: 356.06
train mean loss: 348.65
epoch train time: 0:00:00.174519
elapsed time: 0:00:53.717561
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 16:40:45.044603
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.03
 ---- batch: 020 ----
mean loss: 352.11
 ---- batch: 030 ----
mean loss: 351.62
train mean loss: 348.40
epoch train time: 0:00:00.173206
elapsed time: 0:00:53.890924
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 16:40:45.217955
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.25
 ---- batch: 020 ----
mean loss: 348.26
 ---- batch: 030 ----
mean loss: 350.29
train mean loss: 348.74
epoch train time: 0:00:00.171920
elapsed time: 0:00:54.062983
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 16:40:45.390026
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 339.34
 ---- batch: 020 ----
mean loss: 347.52
 ---- batch: 030 ----
mean loss: 352.62
train mean loss: 348.22
epoch train time: 0:00:00.170427
elapsed time: 0:00:54.233551
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 16:40:45.560591
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.82
 ---- batch: 020 ----
mean loss: 351.23
 ---- batch: 030 ----
mean loss: 343.01
train mean loss: 348.58
epoch train time: 0:00:00.171213
elapsed time: 0:00:54.404903
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 16:40:45.731944
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 348.43
 ---- batch: 020 ----
mean loss: 354.04
 ---- batch: 030 ----
mean loss: 347.16
train mean loss: 348.46
epoch train time: 0:00:00.174542
elapsed time: 0:00:54.579584
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 16:40:45.906644
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.81
 ---- batch: 020 ----
mean loss: 354.82
 ---- batch: 030 ----
mean loss: 337.44
train mean loss: 348.51
epoch train time: 0:00:00.176663
elapsed time: 0:00:54.756415
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 16:40:46.083461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 340.38
 ---- batch: 020 ----
mean loss: 353.88
 ---- batch: 030 ----
mean loss: 346.72
train mean loss: 348.48
epoch train time: 0:00:00.179933
elapsed time: 0:00:54.936509
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 16:40:46.263553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 357.21
 ---- batch: 020 ----
mean loss: 343.37
 ---- batch: 030 ----
mean loss: 346.52
train mean loss: 347.96
epoch train time: 0:00:00.181620
elapsed time: 0:00:55.118271
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 16:40:46.445313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 354.76
 ---- batch: 020 ----
mean loss: 347.85
 ---- batch: 030 ----
mean loss: 347.47
train mean loss: 348.22
epoch train time: 0:00:00.169666
elapsed time: 0:00:55.288079
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 16:40:46.615122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 352.81
 ---- batch: 020 ----
mean loss: 344.24
 ---- batch: 030 ----
mean loss: 348.55
train mean loss: 348.69
epoch train time: 0:00:00.170892
elapsed time: 0:00:55.459115
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 16:40:46.786174
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 346.88
 ---- batch: 020 ----
mean loss: 353.92
 ---- batch: 030 ----
mean loss: 346.37
train mean loss: 348.04
epoch train time: 0:00:00.184114
elapsed time: 0:00:55.643414
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 16:40:46.970469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.64
 ---- batch: 020 ----
mean loss: 350.56
 ---- batch: 030 ----
mean loss: 341.23
train mean loss: 348.49
epoch train time: 0:00:00.174849
elapsed time: 0:00:55.818427
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 16:40:47.145472
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 351.92
 ---- batch: 020 ----
mean loss: 344.98
 ---- batch: 030 ----
mean loss: 348.43
train mean loss: 348.41
epoch train time: 0:00:00.186550
elapsed time: 0:00:56.005119
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 16:40:47.332159
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 342.42
 ---- batch: 020 ----
mean loss: 359.58
 ---- batch: 030 ----
mean loss: 343.56
train mean loss: 348.45
epoch train time: 0:00:00.179165
elapsed time: 0:00:56.184423
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 16:40:47.511466
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 345.63
 ---- batch: 020 ----
mean loss: 349.84
 ---- batch: 030 ----
mean loss: 342.67
train mean loss: 348.46
epoch train time: 0:00:00.172529
elapsed time: 0:00:56.357093
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 16:40:47.684163
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 347.80
 ---- batch: 020 ----
mean loss: 346.35
 ---- batch: 030 ----
mean loss: 354.05
train mean loss: 348.26
epoch train time: 0:00:00.179187
elapsed time: 0:00:56.536479
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 16:40:47.863521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 350.63
 ---- batch: 020 ----
mean loss: 343.03
 ---- batch: 030 ----
mean loss: 350.03
train mean loss: 348.04
epoch train time: 0:00:00.181402
elapsed time: 0:00:56.720370
checkpoint saved in file: log/CMAPSS/FD001/min-max/frequentist_conv2_pool2/frequentist_conv2_pool2_0/checkpoint.pth.tar
**** end time: 2019-09-27 16:40:48.047379 ****
