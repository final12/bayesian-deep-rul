Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_8', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 8859
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistDense3...
Done.
**** start time: 2019-09-20 23:48:30.134952 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 420]               0
            Linear-2                  [-1, 100]          42,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 62,100
Trainable params: 62,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 23:48:30.138202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4743.07
 ---- batch: 020 ----
mean loss: 4637.06
 ---- batch: 030 ----
mean loss: 4594.40
 ---- batch: 040 ----
mean loss: 4448.79
train mean loss: 4589.13
epoch train time: 0:00:14.831160
elapsed time: 0:00:14.836463
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 23:48:44.971490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4396.66
 ---- batch: 020 ----
mean loss: 4275.58
 ---- batch: 030 ----
mean loss: 4201.69
 ---- batch: 040 ----
mean loss: 4244.85
train mean loss: 4269.20
epoch train time: 0:00:00.213583
elapsed time: 0:00:15.050200
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 23:48:45.185244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4077.07
 ---- batch: 020 ----
mean loss: 4058.30
 ---- batch: 030 ----
mean loss: 4067.70
 ---- batch: 040 ----
mean loss: 3947.38
train mean loss: 4023.19
epoch train time: 0:00:00.206579
elapsed time: 0:00:15.256946
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 23:48:45.391942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3837.04
 ---- batch: 020 ----
mean loss: 3894.61
 ---- batch: 030 ----
mean loss: 3638.14
 ---- batch: 040 ----
mean loss: 3719.90
train mean loss: 3773.45
epoch train time: 0:00:00.210273
elapsed time: 0:00:15.467348
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 23:48:45.602347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3647.35
 ---- batch: 020 ----
mean loss: 3550.11
 ---- batch: 030 ----
mean loss: 3528.49
 ---- batch: 040 ----
mean loss: 3441.73
train mean loss: 3533.10
epoch train time: 0:00:00.206587
elapsed time: 0:00:15.674064
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 23:48:45.809059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3359.94
 ---- batch: 020 ----
mean loss: 3376.46
 ---- batch: 030 ----
mean loss: 3275.44
 ---- batch: 040 ----
mean loss: 3209.41
train mean loss: 3300.67
epoch train time: 0:00:00.205716
elapsed time: 0:00:15.879916
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 23:48:46.014922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3141.72
 ---- batch: 020 ----
mean loss: 3164.92
 ---- batch: 030 ----
mean loss: 3090.11
 ---- batch: 040 ----
mean loss: 2963.38
train mean loss: 3084.88
epoch train time: 0:00:00.205214
elapsed time: 0:00:16.085274
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 23:48:46.220270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2974.78
 ---- batch: 020 ----
mean loss: 2861.58
 ---- batch: 030 ----
mean loss: 2892.48
 ---- batch: 040 ----
mean loss: 2807.80
train mean loss: 2879.88
epoch train time: 0:00:00.203687
elapsed time: 0:00:16.289086
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 23:48:46.424083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2817.46
 ---- batch: 020 ----
mean loss: 2686.05
 ---- batch: 030 ----
mean loss: 2650.01
 ---- batch: 040 ----
mean loss: 2597.79
train mean loss: 2681.74
epoch train time: 0:00:00.203706
elapsed time: 0:00:16.492913
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 23:48:46.627907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2555.35
 ---- batch: 020 ----
mean loss: 2503.97
 ---- batch: 030 ----
mean loss: 2475.80
 ---- batch: 040 ----
mean loss: 2468.33
train mean loss: 2492.92
epoch train time: 0:00:00.204530
elapsed time: 0:00:16.697584
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 23:48:46.832578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2371.89
 ---- batch: 020 ----
mean loss: 2346.74
 ---- batch: 030 ----
mean loss: 2317.52
 ---- batch: 040 ----
mean loss: 2227.05
train mean loss: 2317.00
epoch train time: 0:00:00.200615
elapsed time: 0:00:16.898314
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 23:48:47.033306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2253.38
 ---- batch: 020 ----
mean loss: 2155.05
 ---- batch: 030 ----
mean loss: 2120.79
 ---- batch: 040 ----
mean loss: 2104.94
train mean loss: 2155.06
epoch train time: 0:00:00.194963
elapsed time: 0:00:17.093391
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 23:48:47.228403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2048.62
 ---- batch: 020 ----
mean loss: 2031.90
 ---- batch: 030 ----
mean loss: 1979.48
 ---- batch: 040 ----
mean loss: 1976.54
train mean loss: 2004.37
epoch train time: 0:00:00.195068
elapsed time: 0:00:17.288588
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 23:48:47.423580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1908.84
 ---- batch: 020 ----
mean loss: 1879.55
 ---- batch: 030 ----
mean loss: 1840.69
 ---- batch: 040 ----
mean loss: 1827.73
train mean loss: 1862.43
epoch train time: 0:00:00.195718
elapsed time: 0:00:17.484416
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 23:48:47.619423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1790.82
 ---- batch: 020 ----
mean loss: 1744.34
 ---- batch: 030 ----
mean loss: 1713.65
 ---- batch: 040 ----
mean loss: 1713.41
train mean loss: 1734.45
epoch train time: 0:00:00.198532
elapsed time: 0:00:17.683077
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 23:48:47.818068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1645.26
 ---- batch: 020 ----
mean loss: 1654.98
 ---- batch: 030 ----
mean loss: 1623.31
 ---- batch: 040 ----
mean loss: 1584.56
train mean loss: 1622.97
epoch train time: 0:00:00.197320
elapsed time: 0:00:17.880509
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 23:48:48.015501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1539.46
 ---- batch: 020 ----
mean loss: 1530.30
 ---- batch: 030 ----
mean loss: 1518.33
 ---- batch: 040 ----
mean loss: 1505.46
train mean loss: 1523.28
epoch train time: 0:00:00.196787
elapsed time: 0:00:18.077427
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 23:48:48.212446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1456.89
 ---- batch: 020 ----
mean loss: 1443.07
 ---- batch: 030 ----
mean loss: 1421.58
 ---- batch: 040 ----
mean loss: 1402.07
train mean loss: 1428.73
epoch train time: 0:00:00.195213
elapsed time: 0:00:18.272778
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 23:48:48.407770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1387.27
 ---- batch: 020 ----
mean loss: 1325.42
 ---- batch: 030 ----
mean loss: 1346.46
 ---- batch: 040 ----
mean loss: 1308.22
train mean loss: 1337.80
epoch train time: 0:00:00.193158
elapsed time: 0:00:18.466060
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 23:48:48.601050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1270.30
 ---- batch: 020 ----
mean loss: 1270.55
 ---- batch: 030 ----
mean loss: 1263.61
 ---- batch: 040 ----
mean loss: 1211.08
train mean loss: 1253.36
epoch train time: 0:00:00.198698
elapsed time: 0:00:18.664875
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 23:48:48.799871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1197.87
 ---- batch: 020 ----
mean loss: 1190.95
 ---- batch: 030 ----
mean loss: 1168.52
 ---- batch: 040 ----
mean loss: 1153.02
train mean loss: 1175.50
epoch train time: 0:00:00.206749
elapsed time: 0:00:18.871751
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 23:48:49.006745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1135.35
 ---- batch: 020 ----
mean loss: 1103.19
 ---- batch: 030 ----
mean loss: 1101.56
 ---- batch: 040 ----
mean loss: 1095.30
train mean loss: 1106.10
epoch train time: 0:00:00.204539
elapsed time: 0:00:19.076409
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 23:48:49.211403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1071.07
 ---- batch: 020 ----
mean loss: 1057.69
 ---- batch: 030 ----
mean loss: 1037.97
 ---- batch: 040 ----
mean loss: 983.85
train mean loss: 1030.78
epoch train time: 0:00:00.204129
elapsed time: 0:00:19.280658
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 23:48:49.415652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 911.72
 ---- batch: 020 ----
mean loss: 901.30
 ---- batch: 030 ----
mean loss: 862.50
 ---- batch: 040 ----
mean loss: 840.02
train mean loss: 874.23
epoch train time: 0:00:00.201291
elapsed time: 0:00:19.482069
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 23:48:49.617062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.23
 ---- batch: 020 ----
mean loss: 800.94
 ---- batch: 030 ----
mean loss: 790.98
 ---- batch: 040 ----
mean loss: 772.10
train mean loss: 796.94
epoch train time: 0:00:00.205538
elapsed time: 0:00:19.687738
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 23:48:49.822783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 759.23
 ---- batch: 020 ----
mean loss: 753.07
 ---- batch: 030 ----
mean loss: 723.07
 ---- batch: 040 ----
mean loss: 701.68
train mean loss: 729.89
epoch train time: 0:00:00.207752
elapsed time: 0:00:19.895661
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 23:48:50.030670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 700.07
 ---- batch: 020 ----
mean loss: 678.19
 ---- batch: 030 ----
mean loss: 651.39
 ---- batch: 040 ----
mean loss: 649.47
train mean loss: 667.73
epoch train time: 0:00:00.210342
elapsed time: 0:00:20.106144
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 23:48:50.241142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 637.25
 ---- batch: 020 ----
mean loss: 615.16
 ---- batch: 030 ----
mean loss: 612.97
 ---- batch: 040 ----
mean loss: 593.15
train mean loss: 611.47
epoch train time: 0:00:00.208618
elapsed time: 0:00:20.314884
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 23:48:50.449880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 586.10
 ---- batch: 020 ----
mean loss: 567.23
 ---- batch: 030 ----
mean loss: 554.21
 ---- batch: 040 ----
mean loss: 543.80
train mean loss: 560.70
epoch train time: 0:00:00.210309
elapsed time: 0:00:20.525324
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 23:48:50.660322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 531.66
 ---- batch: 020 ----
mean loss: 510.18
 ---- batch: 030 ----
mean loss: 517.47
 ---- batch: 040 ----
mean loss: 502.35
train mean loss: 513.36
epoch train time: 0:00:00.207859
elapsed time: 0:00:20.733307
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 23:48:50.868304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.42
 ---- batch: 020 ----
mean loss: 483.02
 ---- batch: 030 ----
mean loss: 472.06
 ---- batch: 040 ----
mean loss: 460.23
train mean loss: 470.14
epoch train time: 0:00:00.202823
elapsed time: 0:00:20.936249
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 23:48:51.071250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.15
 ---- batch: 020 ----
mean loss: 443.17
 ---- batch: 030 ----
mean loss: 427.55
 ---- batch: 040 ----
mean loss: 418.88
train mean loss: 431.92
epoch train time: 0:00:00.198442
elapsed time: 0:00:21.134828
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 23:48:51.269821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.26
 ---- batch: 020 ----
mean loss: 406.41
 ---- batch: 030 ----
mean loss: 387.23
 ---- batch: 040 ----
mean loss: 397.44
train mean loss: 396.65
epoch train time: 0:00:00.196325
elapsed time: 0:00:21.331265
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 23:48:51.466255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.04
 ---- batch: 020 ----
mean loss: 364.55
 ---- batch: 030 ----
mean loss: 359.58
 ---- batch: 040 ----
mean loss: 356.70
train mean loss: 364.06
epoch train time: 0:00:00.195644
elapsed time: 0:00:21.527044
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 23:48:51.662042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.46
 ---- batch: 020 ----
mean loss: 339.14
 ---- batch: 030 ----
mean loss: 332.19
 ---- batch: 040 ----
mean loss: 320.41
train mean loss: 334.76
epoch train time: 0:00:00.202441
elapsed time: 0:00:21.729602
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 23:48:51.864593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 313.44
 ---- batch: 020 ----
mean loss: 311.59
 ---- batch: 030 ----
mean loss: 306.77
 ---- batch: 040 ----
mean loss: 305.39
train mean loss: 308.17
epoch train time: 0:00:00.194513
elapsed time: 0:00:21.924229
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 23:48:52.059235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 294.14
 ---- batch: 020 ----
mean loss: 289.20
 ---- batch: 030 ----
mean loss: 282.52
 ---- batch: 040 ----
mean loss: 277.22
train mean loss: 284.25
epoch train time: 0:00:00.199436
elapsed time: 0:00:22.123811
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 23:48:52.258828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.93
 ---- batch: 020 ----
mean loss: 264.47
 ---- batch: 030 ----
mean loss: 258.92
 ---- batch: 040 ----
mean loss: 255.10
train mean loss: 261.37
epoch train time: 0:00:00.197916
elapsed time: 0:00:22.321911
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 23:48:52.456932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.57
 ---- batch: 020 ----
mean loss: 245.23
 ---- batch: 030 ----
mean loss: 237.69
 ---- batch: 040 ----
mean loss: 236.85
train mean loss: 241.77
epoch train time: 0:00:00.193823
elapsed time: 0:00:22.515874
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 23:48:52.650865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.33
 ---- batch: 020 ----
mean loss: 225.56
 ---- batch: 030 ----
mean loss: 221.89
 ---- batch: 040 ----
mean loss: 216.68
train mean loss: 223.63
epoch train time: 0:00:00.200622
elapsed time: 0:00:22.716610
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 23:48:52.851605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.74
 ---- batch: 020 ----
mean loss: 208.70
 ---- batch: 030 ----
mean loss: 208.39
 ---- batch: 040 ----
mean loss: 203.09
train mean loss: 207.81
epoch train time: 0:00:00.208848
elapsed time: 0:00:22.925579
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 23:48:53.060575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.48
 ---- batch: 020 ----
mean loss: 188.55
 ---- batch: 030 ----
mean loss: 195.79
 ---- batch: 040 ----
mean loss: 189.28
train mean loss: 192.75
epoch train time: 0:00:00.207200
elapsed time: 0:00:23.132910
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 23:48:53.267916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.35
 ---- batch: 020 ----
mean loss: 182.80
 ---- batch: 030 ----
mean loss: 179.21
 ---- batch: 040 ----
mean loss: 173.62
train mean loss: 179.55
epoch train time: 0:00:00.208157
elapsed time: 0:00:23.341192
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 23:48:53.476199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.94
 ---- batch: 020 ----
mean loss: 174.42
 ---- batch: 030 ----
mean loss: 163.35
 ---- batch: 040 ----
mean loss: 160.35
train mean loss: 167.24
epoch train time: 0:00:00.204261
elapsed time: 0:00:23.545586
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 23:48:53.680578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.41
 ---- batch: 020 ----
mean loss: 159.66
 ---- batch: 030 ----
mean loss: 157.08
 ---- batch: 040 ----
mean loss: 154.05
train mean loss: 157.42
epoch train time: 0:00:00.204176
elapsed time: 0:00:23.749884
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 23:48:53.884879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.55
 ---- batch: 020 ----
mean loss: 149.18
 ---- batch: 030 ----
mean loss: 146.63
 ---- batch: 040 ----
mean loss: 144.11
train mean loss: 147.36
epoch train time: 0:00:00.204479
elapsed time: 0:00:23.954482
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 23:48:54.089477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.94
 ---- batch: 020 ----
mean loss: 138.01
 ---- batch: 030 ----
mean loss: 140.72
 ---- batch: 040 ----
mean loss: 136.61
train mean loss: 138.41
epoch train time: 0:00:00.206005
elapsed time: 0:00:24.160609
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 23:48:54.295603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.72
 ---- batch: 020 ----
mean loss: 130.78
 ---- batch: 030 ----
mean loss: 128.04
 ---- batch: 040 ----
mean loss: 129.95
train mean loss: 130.10
epoch train time: 0:00:00.204219
elapsed time: 0:00:24.364943
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 23:48:54.499935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.94
 ---- batch: 020 ----
mean loss: 122.97
 ---- batch: 030 ----
mean loss: 121.71
 ---- batch: 040 ----
mean loss: 124.85
train mean loss: 122.98
epoch train time: 0:00:00.208099
elapsed time: 0:00:24.573174
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 23:48:54.708199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.82
 ---- batch: 020 ----
mean loss: 117.69
 ---- batch: 030 ----
mean loss: 112.72
 ---- batch: 040 ----
mean loss: 114.55
train mean loss: 116.60
epoch train time: 0:00:00.218554
elapsed time: 0:00:24.791879
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 23:48:54.926872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.30
 ---- batch: 020 ----
mean loss: 113.03
 ---- batch: 030 ----
mean loss: 109.64
 ---- batch: 040 ----
mean loss: 110.23
train mean loss: 111.06
epoch train time: 0:00:00.198606
elapsed time: 0:00:24.990620
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 23:48:55.125612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 108.83
 ---- batch: 020 ----
mean loss: 104.84
 ---- batch: 030 ----
mean loss: 104.88
 ---- batch: 040 ----
mean loss: 103.43
train mean loss: 105.47
epoch train time: 0:00:00.197742
elapsed time: 0:00:25.188474
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 23:48:55.323473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 101.19
 ---- batch: 020 ----
mean loss: 100.16
 ---- batch: 030 ----
mean loss: 102.30
 ---- batch: 040 ----
mean loss: 99.43
train mean loss: 101.24
epoch train time: 0:00:00.193481
elapsed time: 0:00:25.382075
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 23:48:55.517067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 98.70
 ---- batch: 020 ----
mean loss: 96.09
 ---- batch: 030 ----
mean loss: 95.80
 ---- batch: 040 ----
mean loss: 96.72
train mean loss: 96.61
epoch train time: 0:00:00.193308
elapsed time: 0:00:25.575507
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 23:48:55.710499
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 95.05
 ---- batch: 020 ----
mean loss: 90.54
 ---- batch: 030 ----
mean loss: 94.12
 ---- batch: 040 ----
mean loss: 91.36
train mean loss: 92.92
epoch train time: 0:00:00.194554
elapsed time: 0:00:25.770179
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 23:48:55.905172
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 90.29
 ---- batch: 020 ----
mean loss: 91.22
 ---- batch: 030 ----
mean loss: 92.08
 ---- batch: 040 ----
mean loss: 86.92
train mean loss: 90.35
epoch train time: 0:00:00.191752
elapsed time: 0:00:25.962046
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 23:48:56.097038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 88.31
 ---- batch: 020 ----
mean loss: 85.80
 ---- batch: 030 ----
mean loss: 84.49
 ---- batch: 040 ----
mean loss: 86.61
train mean loss: 86.50
epoch train time: 0:00:00.196182
elapsed time: 0:00:26.158338
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 23:48:56.293338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 84.78
 ---- batch: 020 ----
mean loss: 82.78
 ---- batch: 030 ----
mean loss: 84.92
 ---- batch: 040 ----
mean loss: 82.66
train mean loss: 83.82
epoch train time: 0:00:00.201337
elapsed time: 0:00:26.359801
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 23:48:56.494789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 83.83
 ---- batch: 020 ----
mean loss: 79.46
 ---- batch: 030 ----
mean loss: 78.82
 ---- batch: 040 ----
mean loss: 82.30
train mean loss: 80.68
epoch train time: 0:00:00.199205
elapsed time: 0:00:26.559122
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 23:48:56.694127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 79.40
 ---- batch: 020 ----
mean loss: 79.98
 ---- batch: 030 ----
mean loss: 77.49
 ---- batch: 040 ----
mean loss: 77.57
train mean loss: 78.94
epoch train time: 0:00:00.204299
elapsed time: 0:00:26.763548
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 23:48:56.898540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 77.06
 ---- batch: 020 ----
mean loss: 76.75
 ---- batch: 030 ----
mean loss: 78.26
 ---- batch: 040 ----
mean loss: 75.78
train mean loss: 76.77
epoch train time: 0:00:00.208662
elapsed time: 0:00:26.972332
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 23:48:57.107347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 76.44
 ---- batch: 020 ----
mean loss: 75.40
 ---- batch: 030 ----
mean loss: 73.00
 ---- batch: 040 ----
mean loss: 74.96
train mean loss: 75.03
epoch train time: 0:00:00.210496
elapsed time: 0:00:27.182972
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 23:48:57.317968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 71.49
 ---- batch: 020 ----
mean loss: 74.92
 ---- batch: 030 ----
mean loss: 75.29
 ---- batch: 040 ----
mean loss: 76.16
train mean loss: 74.14
epoch train time: 0:00:00.205782
elapsed time: 0:00:27.388874
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 23:48:57.523879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 72.99
 ---- batch: 020 ----
mean loss: 71.09
 ---- batch: 030 ----
mean loss: 74.98
 ---- batch: 040 ----
mean loss: 71.57
train mean loss: 72.35
epoch train time: 0:00:00.206033
elapsed time: 0:00:27.595036
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 23:48:57.730029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 68.40
 ---- batch: 020 ----
mean loss: 68.45
 ---- batch: 030 ----
mean loss: 69.72
 ---- batch: 040 ----
mean loss: 71.83
train mean loss: 69.86
epoch train time: 0:00:00.205920
elapsed time: 0:00:27.801072
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 23:48:57.936063
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 69.53
 ---- batch: 020 ----
mean loss: 69.70
 ---- batch: 030 ----
mean loss: 66.90
 ---- batch: 040 ----
mean loss: 68.45
train mean loss: 68.81
epoch train time: 0:00:00.202229
elapsed time: 0:00:28.003416
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 23:48:58.138410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.63
 ---- batch: 020 ----
mean loss: 66.82
 ---- batch: 030 ----
mean loss: 68.16
 ---- batch: 040 ----
mean loss: 66.48
train mean loss: 67.35
epoch train time: 0:00:00.210150
elapsed time: 0:00:28.213687
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 23:48:58.348680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.19
 ---- batch: 020 ----
mean loss: 67.90
 ---- batch: 030 ----
mean loss: 68.59
 ---- batch: 040 ----
mean loss: 64.08
train mean loss: 66.66
epoch train time: 0:00:00.206673
elapsed time: 0:00:28.420477
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 23:48:58.555471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 67.60
 ---- batch: 020 ----
mean loss: 64.39
 ---- batch: 030 ----
mean loss: 66.48
 ---- batch: 040 ----
mean loss: 67.30
train mean loss: 66.65
epoch train time: 0:00:00.205566
elapsed time: 0:00:28.626162
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 23:48:58.761171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.83
 ---- batch: 020 ----
mean loss: 62.51
 ---- batch: 030 ----
mean loss: 61.98
 ---- batch: 040 ----
mean loss: 66.48
train mean loss: 64.33
epoch train time: 0:00:00.217803
elapsed time: 0:00:28.844095
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 23:48:58.979088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 64.74
 ---- batch: 020 ----
mean loss: 61.49
 ---- batch: 030 ----
mean loss: 63.20
 ---- batch: 040 ----
mean loss: 64.66
train mean loss: 63.68
epoch train time: 0:00:00.198117
elapsed time: 0:00:29.042337
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 23:48:59.177327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.66
 ---- batch: 020 ----
mean loss: 65.15
 ---- batch: 030 ----
mean loss: 64.07
 ---- batch: 040 ----
mean loss: 61.43
train mean loss: 62.90
epoch train time: 0:00:00.200236
elapsed time: 0:00:29.242696
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 23:48:59.377687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.60
 ---- batch: 020 ----
mean loss: 65.20
 ---- batch: 030 ----
mean loss: 62.44
 ---- batch: 040 ----
mean loss: 61.85
train mean loss: 61.94
epoch train time: 0:00:00.195721
elapsed time: 0:00:29.438530
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 23:48:59.573550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.88
 ---- batch: 020 ----
mean loss: 58.83
 ---- batch: 030 ----
mean loss: 62.07
 ---- batch: 040 ----
mean loss: 63.11
train mean loss: 61.37
epoch train time: 0:00:00.196003
elapsed time: 0:00:29.634670
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 23:48:59.769657
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.68
 ---- batch: 020 ----
mean loss: 61.38
 ---- batch: 030 ----
mean loss: 61.03
 ---- batch: 040 ----
mean loss: 60.08
train mean loss: 60.89
epoch train time: 0:00:00.201485
elapsed time: 0:00:29.836277
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 23:48:59.971267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 63.97
 ---- batch: 020 ----
mean loss: 63.78
 ---- batch: 030 ----
mean loss: 61.34
 ---- batch: 040 ----
mean loss: 57.69
train mean loss: 61.31
epoch train time: 0:00:00.201064
elapsed time: 0:00:30.037470
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 23:49:00.172464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 60.28
 ---- batch: 020 ----
mean loss: 55.54
 ---- batch: 030 ----
mean loss: 61.11
 ---- batch: 040 ----
mean loss: 61.31
train mean loss: 59.54
epoch train time: 0:00:00.204470
elapsed time: 0:00:30.242060
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 23:49:00.377051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.33
 ---- batch: 020 ----
mean loss: 60.80
 ---- batch: 030 ----
mean loss: 59.38
 ---- batch: 040 ----
mean loss: 58.14
train mean loss: 59.14
epoch train time: 0:00:00.193549
elapsed time: 0:00:30.435728
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 23:49:00.570719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.10
 ---- batch: 020 ----
mean loss: 55.11
 ---- batch: 030 ----
mean loss: 61.04
 ---- batch: 040 ----
mean loss: 58.68
train mean loss: 58.43
epoch train time: 0:00:00.199529
elapsed time: 0:00:30.635366
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 23:49:00.770358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 58.57
 ---- batch: 020 ----
mean loss: 57.02
 ---- batch: 030 ----
mean loss: 59.47
 ---- batch: 040 ----
mean loss: 59.03
train mean loss: 58.57
epoch train time: 0:00:00.201071
elapsed time: 0:00:30.836546
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 23:49:00.971536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.68
 ---- batch: 020 ----
mean loss: 55.92
 ---- batch: 030 ----
mean loss: 58.13
 ---- batch: 040 ----
mean loss: 57.76
train mean loss: 57.92
epoch train time: 0:00:00.204064
elapsed time: 0:00:31.040740
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 23:49:01.175751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.13
 ---- batch: 020 ----
mean loss: 57.12
 ---- batch: 030 ----
mean loss: 58.62
 ---- batch: 040 ----
mean loss: 60.28
train mean loss: 57.77
epoch train time: 0:00:00.208040
elapsed time: 0:00:31.248917
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 23:49:01.383913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.89
 ---- batch: 020 ----
mean loss: 55.91
 ---- batch: 030 ----
mean loss: 55.86
 ---- batch: 040 ----
mean loss: 57.98
train mean loss: 57.44
epoch train time: 0:00:00.207716
elapsed time: 0:00:31.456769
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 23:49:01.591789
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.70
 ---- batch: 020 ----
mean loss: 58.45
 ---- batch: 030 ----
mean loss: 57.94
 ---- batch: 040 ----
mean loss: 58.53
train mean loss: 57.47
epoch train time: 0:00:00.209857
elapsed time: 0:00:31.666785
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 23:49:01.801792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 59.08
 ---- batch: 020 ----
mean loss: 54.74
 ---- batch: 030 ----
mean loss: 56.32
 ---- batch: 040 ----
mean loss: 55.13
train mean loss: 56.16
epoch train time: 0:00:00.207709
elapsed time: 0:00:31.874624
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 23:49:02.009617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.52
 ---- batch: 020 ----
mean loss: 56.04
 ---- batch: 030 ----
mean loss: 54.39
 ---- batch: 040 ----
mean loss: 57.66
train mean loss: 55.96
epoch train time: 0:00:00.206765
elapsed time: 0:00:32.081532
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 23:49:02.216542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.89
 ---- batch: 020 ----
mean loss: 55.43
 ---- batch: 030 ----
mean loss: 54.83
 ---- batch: 040 ----
mean loss: 59.07
train mean loss: 55.97
epoch train time: 0:00:00.205346
elapsed time: 0:00:32.287041
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 23:49:02.422039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.64
 ---- batch: 020 ----
mean loss: 54.72
 ---- batch: 030 ----
mean loss: 55.89
 ---- batch: 040 ----
mean loss: 53.89
train mean loss: 55.26
epoch train time: 0:00:00.206167
elapsed time: 0:00:32.493339
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 23:49:02.628335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.91
 ---- batch: 020 ----
mean loss: 53.87
 ---- batch: 030 ----
mean loss: 52.49
 ---- batch: 040 ----
mean loss: 53.84
train mean loss: 54.47
epoch train time: 0:00:00.209554
elapsed time: 0:00:32.703034
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 23:49:02.838030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.81
 ---- batch: 020 ----
mean loss: 52.22
 ---- batch: 030 ----
mean loss: 54.45
 ---- batch: 040 ----
mean loss: 58.42
train mean loss: 54.67
epoch train time: 0:00:00.199702
elapsed time: 0:00:32.902865
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 23:49:03.037870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.18
 ---- batch: 020 ----
mean loss: 58.04
 ---- batch: 030 ----
mean loss: 52.50
 ---- batch: 040 ----
mean loss: 55.04
train mean loss: 55.19
epoch train time: 0:00:00.198424
elapsed time: 0:00:33.101451
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 23:49:03.236445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.06
 ---- batch: 020 ----
mean loss: 54.92
 ---- batch: 030 ----
mean loss: 52.59
 ---- batch: 040 ----
mean loss: 52.05
train mean loss: 53.56
epoch train time: 0:00:00.202813
elapsed time: 0:00:33.304376
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 23:49:03.439376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.15
 ---- batch: 020 ----
mean loss: 53.43
 ---- batch: 030 ----
mean loss: 54.53
 ---- batch: 040 ----
mean loss: 55.69
train mean loss: 53.91
epoch train time: 0:00:00.203064
elapsed time: 0:00:33.507559
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 23:49:03.642551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.06
 ---- batch: 020 ----
mean loss: 55.46
 ---- batch: 030 ----
mean loss: 52.38
 ---- batch: 040 ----
mean loss: 52.95
train mean loss: 53.58
epoch train time: 0:00:00.207130
elapsed time: 0:00:33.714813
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 23:49:03.849810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.64
 ---- batch: 020 ----
mean loss: 55.81
 ---- batch: 030 ----
mean loss: 52.18
 ---- batch: 040 ----
mean loss: 55.14
train mean loss: 53.57
epoch train time: 0:00:00.199245
elapsed time: 0:00:33.914179
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 23:49:04.049173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 54.76
 ---- batch: 020 ----
mean loss: 55.05
 ---- batch: 030 ----
mean loss: 54.25
 ---- batch: 040 ----
mean loss: 51.90
train mean loss: 53.94
epoch train time: 0:00:00.202166
elapsed time: 0:00:34.116476
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 23:49:04.251467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.48
 ---- batch: 020 ----
mean loss: 54.18
 ---- batch: 030 ----
mean loss: 48.84
 ---- batch: 040 ----
mean loss: 52.35
train mean loss: 52.19
epoch train time: 0:00:00.199641
elapsed time: 0:00:34.316224
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 23:49:04.451215
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 52.33
 ---- batch: 020 ----
mean loss: 51.94
 ---- batch: 030 ----
mean loss: 48.93
 ---- batch: 040 ----
mean loss: 52.21
train mean loss: 51.91
epoch train time: 0:00:00.198179
elapsed time: 0:00:34.514511
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 23:49:04.649517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.10
 ---- batch: 020 ----
mean loss: 52.01
 ---- batch: 030 ----
mean loss: 52.43
 ---- batch: 040 ----
mean loss: 53.78
train mean loss: 52.77
epoch train time: 0:00:00.213655
elapsed time: 0:00:34.728309
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 23:49:04.863305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 53.00
 ---- batch: 020 ----
mean loss: 55.16
 ---- batch: 030 ----
mean loss: 55.61
 ---- batch: 040 ----
mean loss: 54.89
train mean loss: 54.83
epoch train time: 0:00:00.217597
elapsed time: 0:00:34.946076
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 23:49:05.081086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.31
 ---- batch: 020 ----
mean loss: 51.04
 ---- batch: 030 ----
mean loss: 52.53
 ---- batch: 040 ----
mean loss: 52.13
train mean loss: 51.62
epoch train time: 0:00:00.209157
elapsed time: 0:00:35.155367
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 23:49:05.290361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.79
 ---- batch: 020 ----
mean loss: 49.76
 ---- batch: 030 ----
mean loss: 50.86
 ---- batch: 040 ----
mean loss: 52.48
train mean loss: 51.27
epoch train time: 0:00:00.208136
elapsed time: 0:00:35.363620
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 23:49:05.498615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.77
 ---- batch: 020 ----
mean loss: 49.02
 ---- batch: 030 ----
mean loss: 52.35
 ---- batch: 040 ----
mean loss: 53.57
train mean loss: 51.24
epoch train time: 0:00:00.210128
elapsed time: 0:00:35.573870
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 23:49:05.708867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.29
 ---- batch: 020 ----
mean loss: 51.40
 ---- batch: 030 ----
mean loss: 50.75
 ---- batch: 040 ----
mean loss: 48.52
train mean loss: 50.44
epoch train time: 0:00:00.204695
elapsed time: 0:00:35.778701
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 23:49:05.913695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.90
 ---- batch: 020 ----
mean loss: 51.45
 ---- batch: 030 ----
mean loss: 47.39
 ---- batch: 040 ----
mean loss: 51.41
train mean loss: 50.39
epoch train time: 0:00:00.202839
elapsed time: 0:00:35.981667
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 23:49:06.116662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.63
 ---- batch: 020 ----
mean loss: 51.37
 ---- batch: 030 ----
mean loss: 49.84
 ---- batch: 040 ----
mean loss: 54.79
train mean loss: 51.03
epoch train time: 0:00:00.205538
elapsed time: 0:00:36.187340
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 23:49:06.322333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.87
 ---- batch: 020 ----
mean loss: 48.94
 ---- batch: 030 ----
mean loss: 50.82
 ---- batch: 040 ----
mean loss: 50.60
train mean loss: 50.14
epoch train time: 0:00:00.205389
elapsed time: 0:00:36.392873
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 23:49:06.527859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.22
 ---- batch: 020 ----
mean loss: 50.25
 ---- batch: 030 ----
mean loss: 49.46
 ---- batch: 040 ----
mean loss: 49.88
train mean loss: 50.27
epoch train time: 0:00:00.205140
elapsed time: 0:00:36.598118
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 23:49:06.733110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.57
 ---- batch: 020 ----
mean loss: 48.04
 ---- batch: 030 ----
mean loss: 49.02
 ---- batch: 040 ----
mean loss: 53.18
train mean loss: 49.84
epoch train time: 0:00:00.203714
elapsed time: 0:00:36.801946
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 23:49:06.936938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.78
 ---- batch: 020 ----
mean loss: 47.62
 ---- batch: 030 ----
mean loss: 50.84
 ---- batch: 040 ----
mean loss: 46.59
train mean loss: 49.18
epoch train time: 0:00:00.204326
elapsed time: 0:00:37.006387
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 23:49:07.141394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.82
 ---- batch: 020 ----
mean loss: 48.09
 ---- batch: 030 ----
mean loss: 50.52
 ---- batch: 040 ----
mean loss: 48.87
train mean loss: 48.95
epoch train time: 0:00:00.199322
elapsed time: 0:00:37.205853
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 23:49:07.340860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 51.13
 ---- batch: 020 ----
mean loss: 50.09
 ---- batch: 030 ----
mean loss: 48.69
 ---- batch: 040 ----
mean loss: 49.44
train mean loss: 49.92
epoch train time: 0:00:00.201540
elapsed time: 0:00:37.407517
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 23:49:07.542508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.96
 ---- batch: 020 ----
mean loss: 50.36
 ---- batch: 030 ----
mean loss: 48.65
 ---- batch: 040 ----
mean loss: 47.77
train mean loss: 49.42
epoch train time: 0:00:00.211873
elapsed time: 0:00:37.619506
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 23:49:07.754520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.58
 ---- batch: 020 ----
mean loss: 49.81
 ---- batch: 030 ----
mean loss: 48.03
 ---- batch: 040 ----
mean loss: 46.38
train mean loss: 48.29
epoch train time: 0:00:00.204402
elapsed time: 0:00:37.824044
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 23:49:07.959038
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.74
 ---- batch: 020 ----
mean loss: 46.19
 ---- batch: 030 ----
mean loss: 50.27
 ---- batch: 040 ----
mean loss: 48.65
train mean loss: 48.34
epoch train time: 0:00:00.202852
elapsed time: 0:00:38.027047
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 23:49:08.162044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 50.19
 ---- batch: 020 ----
mean loss: 51.73
 ---- batch: 030 ----
mean loss: 51.27
 ---- batch: 040 ----
mean loss: 50.84
train mean loss: 50.91
epoch train time: 0:00:00.202091
elapsed time: 0:00:38.229296
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 23:49:08.364335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.23
 ---- batch: 020 ----
mean loss: 48.68
 ---- batch: 030 ----
mean loss: 48.04
 ---- batch: 040 ----
mean loss: 53.91
train mean loss: 49.44
epoch train time: 0:00:00.202644
elapsed time: 0:00:38.432147
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 23:49:08.567153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 55.40
 ---- batch: 020 ----
mean loss: 52.52
 ---- batch: 030 ----
mean loss: 48.19
 ---- batch: 040 ----
mean loss: 46.75
train mean loss: 50.52
epoch train time: 0:00:00.219036
elapsed time: 0:00:38.651321
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 23:49:08.786330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.63
 ---- batch: 020 ----
mean loss: 46.79
 ---- batch: 030 ----
mean loss: 47.70
 ---- batch: 040 ----
mean loss: 49.28
train mean loss: 48.11
epoch train time: 0:00:00.213029
elapsed time: 0:00:38.864487
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 23:49:08.999483
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.76
 ---- batch: 020 ----
mean loss: 48.29
 ---- batch: 030 ----
mean loss: 49.61
 ---- batch: 040 ----
mean loss: 46.08
train mean loss: 48.16
epoch train time: 0:00:00.206908
elapsed time: 0:00:39.071516
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 23:49:09.206525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.56
 ---- batch: 020 ----
mean loss: 48.01
 ---- batch: 030 ----
mean loss: 48.50
 ---- batch: 040 ----
mean loss: 47.54
train mean loss: 47.33
epoch train time: 0:00:00.207729
elapsed time: 0:00:39.279376
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 23:49:09.414369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.86
 ---- batch: 020 ----
mean loss: 48.64
 ---- batch: 030 ----
mean loss: 44.44
 ---- batch: 040 ----
mean loss: 49.99
train mean loss: 47.03
epoch train time: 0:00:00.208486
elapsed time: 0:00:39.487978
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 23:49:09.622987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.59
 ---- batch: 020 ----
mean loss: 47.48
 ---- batch: 030 ----
mean loss: 46.86
 ---- batch: 040 ----
mean loss: 47.01
train mean loss: 46.97
epoch train time: 0:00:00.210549
elapsed time: 0:00:39.698673
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 23:49:09.833664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.35
 ---- batch: 020 ----
mean loss: 46.83
 ---- batch: 030 ----
mean loss: 47.49
 ---- batch: 040 ----
mean loss: 45.05
train mean loss: 46.60
epoch train time: 0:00:00.202940
elapsed time: 0:00:39.901730
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 23:49:10.036722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 47.83
 ---- batch: 020 ----
mean loss: 44.59
 ---- batch: 030 ----
mean loss: 45.02
 ---- batch: 040 ----
mean loss: 46.46
train mean loss: 46.17
epoch train time: 0:00:00.204410
elapsed time: 0:00:40.106272
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 23:49:10.241275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.99
 ---- batch: 020 ----
mean loss: 45.41
 ---- batch: 030 ----
mean loss: 49.72
 ---- batch: 040 ----
mean loss: 45.32
train mean loss: 46.20
epoch train time: 0:00:00.201964
elapsed time: 0:00:40.308371
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 23:49:10.443361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.63
 ---- batch: 020 ----
mean loss: 48.51
 ---- batch: 030 ----
mean loss: 46.87
 ---- batch: 040 ----
mean loss: 45.67
train mean loss: 47.37
epoch train time: 0:00:00.197801
elapsed time: 0:00:40.506301
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 23:49:10.641288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 48.62
 ---- batch: 020 ----
mean loss: 45.28
 ---- batch: 030 ----
mean loss: 45.45
 ---- batch: 040 ----
mean loss: 46.08
train mean loss: 46.53
epoch train time: 0:00:00.204777
elapsed time: 0:00:40.711184
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 23:49:10.846175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.29
 ---- batch: 020 ----
mean loss: 45.77
 ---- batch: 030 ----
mean loss: 45.54
 ---- batch: 040 ----
mean loss: 47.33
train mean loss: 45.82
epoch train time: 0:00:00.197201
elapsed time: 0:00:40.908494
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 23:49:11.043484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.93
 ---- batch: 020 ----
mean loss: 46.55
 ---- batch: 030 ----
mean loss: 47.86
 ---- batch: 040 ----
mean loss: 45.35
train mean loss: 46.80
epoch train time: 0:00:00.195849
elapsed time: 0:00:41.104468
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 23:49:11.239459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.04
 ---- batch: 020 ----
mean loss: 46.37
 ---- batch: 030 ----
mean loss: 43.66
 ---- batch: 040 ----
mean loss: 46.77
train mean loss: 45.73
epoch train time: 0:00:00.195147
elapsed time: 0:00:41.299722
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 23:49:11.434711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 49.87
 ---- batch: 020 ----
mean loss: 47.41
 ---- batch: 030 ----
mean loss: 44.17
 ---- batch: 040 ----
mean loss: 44.98
train mean loss: 46.52
epoch train time: 0:00:00.198413
elapsed time: 0:00:41.498246
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 23:49:11.633237
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.67
 ---- batch: 020 ----
mean loss: 46.27
 ---- batch: 030 ----
mean loss: 45.41
 ---- batch: 040 ----
mean loss: 43.22
train mean loss: 45.59
epoch train time: 0:00:00.199509
elapsed time: 0:00:41.697883
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 23:49:11.832875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.00
 ---- batch: 020 ----
mean loss: 44.48
 ---- batch: 030 ----
mean loss: 46.32
 ---- batch: 040 ----
mean loss: 44.94
train mean loss: 44.55
epoch train time: 0:00:00.195303
elapsed time: 0:00:41.893305
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 23:49:12.028311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.27
 ---- batch: 020 ----
mean loss: 46.96
 ---- batch: 030 ----
mean loss: 45.80
 ---- batch: 040 ----
mean loss: 44.99
train mean loss: 45.76
epoch train time: 0:00:00.197137
elapsed time: 0:00:42.090582
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 23:49:12.225572
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.84
 ---- batch: 020 ----
mean loss: 44.60
 ---- batch: 030 ----
mean loss: 44.67
 ---- batch: 040 ----
mean loss: 45.72
train mean loss: 44.61
epoch train time: 0:00:00.198725
elapsed time: 0:00:42.289420
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 23:49:12.424436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.18
 ---- batch: 020 ----
mean loss: 45.71
 ---- batch: 030 ----
mean loss: 44.60
 ---- batch: 040 ----
mean loss: 43.70
train mean loss: 44.49
epoch train time: 0:00:00.203002
elapsed time: 0:00:42.492565
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 23:49:12.627558
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.67
 ---- batch: 020 ----
mean loss: 44.30
 ---- batch: 030 ----
mean loss: 47.06
 ---- batch: 040 ----
mean loss: 45.30
train mean loss: 45.08
epoch train time: 0:00:00.204281
elapsed time: 0:00:42.696965
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 23:49:12.831961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 45.00
 ---- batch: 020 ----
mean loss: 49.02
 ---- batch: 030 ----
mean loss: 45.64
 ---- batch: 040 ----
mean loss: 44.25
train mean loss: 45.81
epoch train time: 0:00:00.204909
elapsed time: 0:00:42.902015
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 23:49:13.037011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 44.77
 ---- batch: 020 ----
mean loss: 42.86
 ---- batch: 030 ----
mean loss: 43.25
 ---- batch: 040 ----
mean loss: 44.00
train mean loss: 43.92
epoch train time: 0:00:00.205984
elapsed time: 0:00:43.108119
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 23:49:13.243130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.78
 ---- batch: 020 ----
mean loss: 41.20
 ---- batch: 030 ----
mean loss: 43.56
 ---- batch: 040 ----
mean loss: 44.68
train mean loss: 43.55
epoch train time: 0:00:00.204735
elapsed time: 0:00:43.312989
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 23:49:13.447982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.67
 ---- batch: 020 ----
mean loss: 44.70
 ---- batch: 030 ----
mean loss: 45.34
 ---- batch: 040 ----
mean loss: 43.79
train mean loss: 43.75
epoch train time: 0:00:00.205019
elapsed time: 0:00:43.518140
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 23:49:13.653131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.22
 ---- batch: 020 ----
mean loss: 46.20
 ---- batch: 030 ----
mean loss: 44.34
 ---- batch: 040 ----
mean loss: 42.72
train mean loss: 43.56
epoch train time: 0:00:00.205112
elapsed time: 0:00:43.723367
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 23:49:13.858360
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.52
 ---- batch: 020 ----
mean loss: 45.01
 ---- batch: 030 ----
mean loss: 44.06
 ---- batch: 040 ----
mean loss: 43.56
train mean loss: 43.95
epoch train time: 0:00:00.204733
elapsed time: 0:00:43.928253
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 23:49:14.063260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.92
 ---- batch: 020 ----
mean loss: 45.38
 ---- batch: 030 ----
mean loss: 43.43
 ---- batch: 040 ----
mean loss: 42.01
train mean loss: 43.26
epoch train time: 0:00:00.203540
elapsed time: 0:00:44.131940
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 23:49:14.266960
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.47
 ---- batch: 020 ----
mean loss: 40.45
 ---- batch: 030 ----
mean loss: 43.50
 ---- batch: 040 ----
mean loss: 43.67
train mean loss: 43.01
epoch train time: 0:00:00.208730
elapsed time: 0:00:44.340822
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 23:49:14.475840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.77
 ---- batch: 020 ----
mean loss: 45.58
 ---- batch: 030 ----
mean loss: 43.54
 ---- batch: 040 ----
mean loss: 44.75
train mean loss: 44.07
epoch train time: 0:00:00.198435
elapsed time: 0:00:44.539408
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 23:49:14.674412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 46.09
 ---- batch: 020 ----
mean loss: 42.16
 ---- batch: 030 ----
mean loss: 43.87
 ---- batch: 040 ----
mean loss: 42.17
train mean loss: 43.46
epoch train time: 0:00:00.199077
elapsed time: 0:00:44.738610
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 23:49:14.873605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.97
 ---- batch: 020 ----
mean loss: 45.39
 ---- batch: 030 ----
mean loss: 41.57
 ---- batch: 040 ----
mean loss: 40.80
train mean loss: 42.26
epoch train time: 0:00:00.198499
elapsed time: 0:00:44.937234
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 23:49:15.072219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 43.57
 ---- batch: 020 ----
mean loss: 42.23
 ---- batch: 030 ----
mean loss: 42.73
 ---- batch: 040 ----
mean loss: 42.60
train mean loss: 42.62
epoch train time: 0:00:00.197966
elapsed time: 0:00:45.135299
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 23:49:15.270288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.23
 ---- batch: 020 ----
mean loss: 42.71
 ---- batch: 030 ----
mean loss: 43.17
 ---- batch: 040 ----
mean loss: 44.24
train mean loss: 42.87
epoch train time: 0:00:00.193173
elapsed time: 0:00:45.328582
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 23:49:15.463577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.57
 ---- batch: 020 ----
mean loss: 42.33
 ---- batch: 030 ----
mean loss: 41.44
 ---- batch: 040 ----
mean loss: 41.72
train mean loss: 41.70
epoch train time: 0:00:00.195942
elapsed time: 0:00:45.524663
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 23:49:15.659686
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.47
 ---- batch: 020 ----
mean loss: 41.50
 ---- batch: 030 ----
mean loss: 42.89
 ---- batch: 040 ----
mean loss: 42.90
train mean loss: 42.17
epoch train time: 0:00:00.207888
elapsed time: 0:00:45.732710
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 23:49:15.867719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.59
 ---- batch: 020 ----
mean loss: 42.58
 ---- batch: 030 ----
mean loss: 39.41
 ---- batch: 040 ----
mean loss: 41.02
train mean loss: 41.67
epoch train time: 0:00:00.195700
elapsed time: 0:00:45.928537
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 23:49:16.063526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.75
 ---- batch: 020 ----
mean loss: 41.30
 ---- batch: 030 ----
mean loss: 41.41
 ---- batch: 040 ----
mean loss: 42.32
train mean loss: 41.92
epoch train time: 0:00:00.196909
elapsed time: 0:00:46.125601
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 23:49:16.260592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.56
 ---- batch: 020 ----
mean loss: 41.83
 ---- batch: 030 ----
mean loss: 43.19
 ---- batch: 040 ----
mean loss: 40.72
train mean loss: 41.77
epoch train time: 0:00:00.195637
elapsed time: 0:00:46.321348
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 23:49:16.456349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.68
 ---- batch: 020 ----
mean loss: 41.81
 ---- batch: 030 ----
mean loss: 42.67
 ---- batch: 040 ----
mean loss: 41.21
train mean loss: 41.48
epoch train time: 0:00:00.203986
elapsed time: 0:00:46.525480
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 23:49:16.660476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.14
 ---- batch: 020 ----
mean loss: 42.18
 ---- batch: 030 ----
mean loss: 41.34
 ---- batch: 040 ----
mean loss: 41.66
train mean loss: 41.10
epoch train time: 0:00:00.211841
elapsed time: 0:00:46.737476
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 23:49:16.872487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.00
 ---- batch: 020 ----
mean loss: 40.71
 ---- batch: 030 ----
mean loss: 41.22
 ---- batch: 040 ----
mean loss: 40.17
train mean loss: 40.77
epoch train time: 0:00:00.207764
elapsed time: 0:00:46.945385
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 23:49:17.080380
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.52
 ---- batch: 020 ----
mean loss: 39.46
 ---- batch: 030 ----
mean loss: 41.59
 ---- batch: 040 ----
mean loss: 45.03
train mean loss: 41.42
epoch train time: 0:00:00.208124
elapsed time: 0:00:47.153633
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 23:49:17.288643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.39
 ---- batch: 020 ----
mean loss: 39.88
 ---- batch: 030 ----
mean loss: 39.98
 ---- batch: 040 ----
mean loss: 41.87
train mean loss: 40.93
epoch train time: 0:00:00.206910
elapsed time: 0:00:47.360675
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 23:49:17.495669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.75
 ---- batch: 020 ----
mean loss: 38.31
 ---- batch: 030 ----
mean loss: 41.16
 ---- batch: 040 ----
mean loss: 41.98
train mean loss: 40.55
epoch train time: 0:00:00.206884
elapsed time: 0:00:47.567678
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 23:49:17.702671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.60
 ---- batch: 020 ----
mean loss: 40.86
 ---- batch: 030 ----
mean loss: 38.91
 ---- batch: 040 ----
mean loss: 42.30
train mean loss: 40.18
epoch train time: 0:00:00.207484
elapsed time: 0:00:47.775280
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 23:49:17.910285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.16
 ---- batch: 020 ----
mean loss: 38.98
 ---- batch: 030 ----
mean loss: 40.35
 ---- batch: 040 ----
mean loss: 41.18
train mean loss: 39.91
epoch train time: 0:00:00.203554
elapsed time: 0:00:47.978986
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 23:49:18.113982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.93
 ---- batch: 020 ----
mean loss: 40.49
 ---- batch: 030 ----
mean loss: 41.40
 ---- batch: 040 ----
mean loss: 39.48
train mean loss: 40.01
epoch train time: 0:00:00.204983
elapsed time: 0:00:48.184115
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 23:49:18.319109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 41.04
 ---- batch: 020 ----
mean loss: 41.03
 ---- batch: 030 ----
mean loss: 39.93
 ---- batch: 040 ----
mean loss: 41.91
train mean loss: 40.83
epoch train time: 0:00:00.200319
elapsed time: 0:00:48.384578
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 23:49:18.519586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.59
 ---- batch: 020 ----
mean loss: 40.03
 ---- batch: 030 ----
mean loss: 40.45
 ---- batch: 040 ----
mean loss: 37.88
train mean loss: 39.88
epoch train time: 0:00:00.203536
elapsed time: 0:00:48.588244
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 23:49:18.723238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.56
 ---- batch: 020 ----
mean loss: 41.57
 ---- batch: 030 ----
mean loss: 39.59
 ---- batch: 040 ----
mean loss: 37.96
train mean loss: 39.59
epoch train time: 0:00:00.202695
elapsed time: 0:00:48.791057
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 23:49:18.926051
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.58
 ---- batch: 020 ----
mean loss: 40.68
 ---- batch: 030 ----
mean loss: 44.81
 ---- batch: 040 ----
mean loss: 39.36
train mean loss: 41.01
epoch train time: 0:00:00.200026
elapsed time: 0:00:48.991198
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 23:49:19.126191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.94
 ---- batch: 020 ----
mean loss: 38.06
 ---- batch: 030 ----
mean loss: 42.23
 ---- batch: 040 ----
mean loss: 41.31
train mean loss: 39.91
epoch train time: 0:00:00.201553
elapsed time: 0:00:49.192866
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 23:49:19.327874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.44
 ---- batch: 020 ----
mean loss: 38.20
 ---- batch: 030 ----
mean loss: 38.88
 ---- batch: 040 ----
mean loss: 39.52
train mean loss: 39.18
epoch train time: 0:00:00.197976
elapsed time: 0:00:49.390971
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 23:49:19.525963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.60
 ---- batch: 020 ----
mean loss: 38.39
 ---- batch: 030 ----
mean loss: 40.96
 ---- batch: 040 ----
mean loss: 39.38
train mean loss: 39.25
epoch train time: 0:00:00.203982
elapsed time: 0:00:49.595065
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 23:49:19.730068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.30
 ---- batch: 020 ----
mean loss: 40.63
 ---- batch: 030 ----
mean loss: 39.56
 ---- batch: 040 ----
mean loss: 42.43
train mean loss: 40.39
epoch train time: 0:00:00.201176
elapsed time: 0:00:49.796363
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 23:49:19.931355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.20
 ---- batch: 020 ----
mean loss: 41.41
 ---- batch: 030 ----
mean loss: 40.27
 ---- batch: 040 ----
mean loss: 40.09
train mean loss: 39.94
epoch train time: 0:00:00.199043
elapsed time: 0:00:49.995575
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 23:49:20.130561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.09
 ---- batch: 020 ----
mean loss: 38.95
 ---- batch: 030 ----
mean loss: 38.67
 ---- batch: 040 ----
mean loss: 38.19
train mean loss: 38.86
epoch train time: 0:00:00.199203
elapsed time: 0:00:50.194884
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 23:49:20.329876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 40.14
 ---- batch: 020 ----
mean loss: 39.54
 ---- batch: 030 ----
mean loss: 39.00
 ---- batch: 040 ----
mean loss: 39.57
train mean loss: 39.45
epoch train time: 0:00:00.199395
elapsed time: 0:00:50.394395
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 23:49:20.529402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.19
 ---- batch: 020 ----
mean loss: 38.79
 ---- batch: 030 ----
mean loss: 38.63
 ---- batch: 040 ----
mean loss: 40.40
train mean loss: 38.58
epoch train time: 0:00:00.209068
elapsed time: 0:00:50.603593
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 23:49:20.738617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.74
 ---- batch: 020 ----
mean loss: 37.11
 ---- batch: 030 ----
mean loss: 36.22
 ---- batch: 040 ----
mean loss: 39.24
train mean loss: 38.23
epoch train time: 0:00:00.211080
elapsed time: 0:00:50.814832
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 23:49:20.949845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.28
 ---- batch: 020 ----
mean loss: 40.16
 ---- batch: 030 ----
mean loss: 37.03
 ---- batch: 040 ----
mean loss: 39.19
train mean loss: 39.08
epoch train time: 0:00:00.207224
elapsed time: 0:00:51.022194
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 23:49:21.157190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 42.98
 ---- batch: 020 ----
mean loss: 45.87
 ---- batch: 030 ----
mean loss: 38.47
 ---- batch: 040 ----
mean loss: 36.10
train mean loss: 40.78
epoch train time: 0:00:00.206547
elapsed time: 0:00:51.228910
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 23:49:21.363919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.29
 ---- batch: 020 ----
mean loss: 39.74
 ---- batch: 030 ----
mean loss: 37.81
 ---- batch: 040 ----
mean loss: 37.42
train mean loss: 38.31
epoch train time: 0:00:00.206689
elapsed time: 0:00:51.435783
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 23:49:21.570796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.14
 ---- batch: 020 ----
mean loss: 36.57
 ---- batch: 030 ----
mean loss: 37.09
 ---- batch: 040 ----
mean loss: 38.42
train mean loss: 37.68
epoch train time: 0:00:00.207339
elapsed time: 0:00:51.643292
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 23:49:21.778302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.31
 ---- batch: 020 ----
mean loss: 37.50
 ---- batch: 030 ----
mean loss: 35.11
 ---- batch: 040 ----
mean loss: 39.11
train mean loss: 37.61
epoch train time: 0:00:00.206364
elapsed time: 0:00:51.849792
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 23:49:21.984801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.30
 ---- batch: 020 ----
mean loss: 37.99
 ---- batch: 030 ----
mean loss: 42.75
 ---- batch: 040 ----
mean loss: 39.26
train mean loss: 39.08
epoch train time: 0:00:00.200552
elapsed time: 0:00:52.050493
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 23:49:22.185489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.32
 ---- batch: 020 ----
mean loss: 40.78
 ---- batch: 030 ----
mean loss: 36.68
 ---- batch: 040 ----
mean loss: 40.49
train mean loss: 38.51
epoch train time: 0:00:00.202379
elapsed time: 0:00:52.252990
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 23:49:22.387983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.79
 ---- batch: 020 ----
mean loss: 36.09
 ---- batch: 030 ----
mean loss: 40.38
 ---- batch: 040 ----
mean loss: 37.15
train mean loss: 37.85
epoch train time: 0:00:00.200772
elapsed time: 0:00:52.453915
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 23:49:22.588907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.09
 ---- batch: 020 ----
mean loss: 38.97
 ---- batch: 030 ----
mean loss: 38.35
 ---- batch: 040 ----
mean loss: 32.82
train mean loss: 37.46
epoch train time: 0:00:00.204403
elapsed time: 0:00:52.658428
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 23:49:22.793418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.59
 ---- batch: 020 ----
mean loss: 40.31
 ---- batch: 030 ----
mean loss: 38.46
 ---- batch: 040 ----
mean loss: 39.53
train mean loss: 38.53
epoch train time: 0:00:00.199160
elapsed time: 0:00:52.857715
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 23:49:22.992729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.61
 ---- batch: 020 ----
mean loss: 37.29
 ---- batch: 030 ----
mean loss: 37.76
 ---- batch: 040 ----
mean loss: 38.48
train mean loss: 37.35
epoch train time: 0:00:00.195423
elapsed time: 0:00:53.053274
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 23:49:23.188279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.55
 ---- batch: 020 ----
mean loss: 36.93
 ---- batch: 030 ----
mean loss: 39.14
 ---- batch: 040 ----
mean loss: 40.07
train mean loss: 37.86
epoch train time: 0:00:00.197364
elapsed time: 0:00:53.250776
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 23:49:23.385777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.57
 ---- batch: 020 ----
mean loss: 38.17
 ---- batch: 030 ----
mean loss: 35.30
 ---- batch: 040 ----
mean loss: 37.32
train mean loss: 36.76
epoch train time: 0:00:00.196073
elapsed time: 0:00:53.446973
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 23:49:23.581995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.85
 ---- batch: 020 ----
mean loss: 35.33
 ---- batch: 030 ----
mean loss: 37.23
 ---- batch: 040 ----
mean loss: 36.66
train mean loss: 36.40
epoch train time: 0:00:00.205678
elapsed time: 0:00:53.652794
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 23:49:23.787795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.00
 ---- batch: 020 ----
mean loss: 36.17
 ---- batch: 030 ----
mean loss: 38.48
 ---- batch: 040 ----
mean loss: 35.75
train mean loss: 37.52
epoch train time: 0:00:00.199990
elapsed time: 0:00:53.852934
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 23:49:23.987976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 39.59
 ---- batch: 020 ----
mean loss: 36.54
 ---- batch: 030 ----
mean loss: 40.06
 ---- batch: 040 ----
mean loss: 36.67
train mean loss: 37.85
epoch train time: 0:00:00.198874
elapsed time: 0:00:54.051974
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 23:49:24.186967
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.48
 ---- batch: 020 ----
mean loss: 38.91
 ---- batch: 030 ----
mean loss: 35.71
 ---- batch: 040 ----
mean loss: 35.50
train mean loss: 37.03
epoch train time: 0:00:00.198411
elapsed time: 0:00:54.250513
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 23:49:24.385535
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.05
 ---- batch: 020 ----
mean loss: 35.63
 ---- batch: 030 ----
mean loss: 38.11
 ---- batch: 040 ----
mean loss: 38.08
train mean loss: 36.93
epoch train time: 0:00:00.195973
elapsed time: 0:00:54.446656
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 23:49:24.581647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 37.31
 ---- batch: 020 ----
mean loss: 37.68
 ---- batch: 030 ----
mean loss: 36.66
 ---- batch: 040 ----
mean loss: 37.57
train mean loss: 36.99
epoch train time: 0:00:00.204973
elapsed time: 0:00:54.651748
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 23:49:24.786743
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 38.13
 ---- batch: 020 ----
mean loss: 36.03
 ---- batch: 030 ----
mean loss: 35.75
 ---- batch: 040 ----
mean loss: 35.33
train mean loss: 36.12
epoch train time: 0:00:00.207686
elapsed time: 0:00:54.859570
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 23:49:24.994565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 35.31
 ---- batch: 020 ----
mean loss: 34.98
 ---- batch: 030 ----
mean loss: 36.76
 ---- batch: 040 ----
mean loss: 36.40
train mean loss: 35.71
epoch train time: 0:00:00.204224
elapsed time: 0:00:55.063919
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 23:49:25.198914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 34.46
 ---- batch: 020 ----
mean loss: 36.23
 ---- batch: 030 ----
mean loss: 36.29
 ---- batch: 040 ----
mean loss: 36.67
train mean loss: 36.08
epoch train time: 0:00:00.207044
elapsed time: 0:00:55.271098
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 23:49:25.406092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 36.45
 ---- batch: 020 ----
mean loss: 35.33
 ---- batch: 030 ----
mean loss: 37.29
 ---- batch: 040 ----
mean loss: 37.42
train mean loss: 36.43
epoch train time: 0:00:00.206436
elapsed time: 0:00:55.477661
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 23:49:25.612659
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.81
 ---- batch: 020 ----
mean loss: 34.29
 ---- batch: 030 ----
mean loss: 34.69
 ---- batch: 040 ----
mean loss: 35.88
train mean loss: 34.59
epoch train time: 0:00:00.208852
elapsed time: 0:00:55.686648
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 23:49:25.821637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.32
 ---- batch: 020 ----
mean loss: 35.31
 ---- batch: 030 ----
mean loss: 35.08
 ---- batch: 040 ----
mean loss: 33.33
train mean loss: 34.33
epoch train time: 0:00:00.207989
elapsed time: 0:00:55.894753
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 23:49:26.029759
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.77
 ---- batch: 020 ----
mean loss: 34.72
 ---- batch: 030 ----
mean loss: 33.19
 ---- batch: 040 ----
mean loss: 33.16
train mean loss: 34.26
epoch train time: 0:00:00.207344
elapsed time: 0:00:56.102229
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 23:49:26.237233
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.57
 ---- batch: 020 ----
mean loss: 34.23
 ---- batch: 030 ----
mean loss: 34.47
 ---- batch: 040 ----
mean loss: 35.32
train mean loss: 34.26
epoch train time: 0:00:00.207339
elapsed time: 0:00:56.309704
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 23:49:26.444698
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.31
 ---- batch: 020 ----
mean loss: 34.19
 ---- batch: 030 ----
mean loss: 34.82
 ---- batch: 040 ----
mean loss: 34.23
train mean loss: 34.29
epoch train time: 0:00:00.207793
elapsed time: 0:00:56.517614
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 23:49:26.652605
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.57
 ---- batch: 020 ----
mean loss: 33.46
 ---- batch: 030 ----
mean loss: 35.07
 ---- batch: 040 ----
mean loss: 33.71
train mean loss: 34.30
epoch train time: 0:00:00.208811
elapsed time: 0:00:56.726551
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 23:49:26.861571
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.65
 ---- batch: 020 ----
mean loss: 33.04
 ---- batch: 030 ----
mean loss: 33.90
 ---- batch: 040 ----
mean loss: 34.17
train mean loss: 34.38
epoch train time: 0:00:00.210674
elapsed time: 0:00:56.937370
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 23:49:27.072363
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.20
 ---- batch: 020 ----
mean loss: 34.03
 ---- batch: 030 ----
mean loss: 32.77
 ---- batch: 040 ----
mean loss: 33.50
train mean loss: 34.18
epoch train time: 0:00:00.200891
elapsed time: 0:00:57.138397
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 23:49:27.273390
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.46
 ---- batch: 020 ----
mean loss: 34.53
 ---- batch: 030 ----
mean loss: 32.73
 ---- batch: 040 ----
mean loss: 36.45
train mean loss: 34.23
epoch train time: 0:00:00.201142
elapsed time: 0:00:57.339650
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 23:49:27.474655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.78
 ---- batch: 020 ----
mean loss: 34.89
 ---- batch: 030 ----
mean loss: 34.74
 ---- batch: 040 ----
mean loss: 32.93
train mean loss: 34.23
epoch train time: 0:00:00.200408
elapsed time: 0:00:57.540190
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 23:49:27.675185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.79
 ---- batch: 020 ----
mean loss: 33.35
 ---- batch: 030 ----
mean loss: 33.56
 ---- batch: 040 ----
mean loss: 34.06
train mean loss: 34.22
epoch train time: 0:00:00.199248
elapsed time: 0:00:57.739551
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 23:49:27.874541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.61
 ---- batch: 020 ----
mean loss: 33.37
 ---- batch: 030 ----
mean loss: 33.26
 ---- batch: 040 ----
mean loss: 35.54
train mean loss: 34.17
epoch train time: 0:00:00.203763
elapsed time: 0:00:57.943423
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 23:49:28.078415
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.81
 ---- batch: 020 ----
mean loss: 32.32
 ---- batch: 030 ----
mean loss: 33.05
 ---- batch: 040 ----
mean loss: 34.72
train mean loss: 34.17
epoch train time: 0:00:00.202728
elapsed time: 0:00:58.146262
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 23:49:28.281279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.58
 ---- batch: 020 ----
mean loss: 34.05
 ---- batch: 030 ----
mean loss: 34.28
 ---- batch: 040 ----
mean loss: 34.64
train mean loss: 34.12
epoch train time: 0:00:00.197917
elapsed time: 0:00:58.344331
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 23:49:28.479322
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.70
 ---- batch: 020 ----
mean loss: 33.23
 ---- batch: 030 ----
mean loss: 34.82
 ---- batch: 040 ----
mean loss: 35.01
train mean loss: 34.10
epoch train time: 0:00:00.199087
elapsed time: 0:00:58.543530
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 23:49:28.678521
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 36.56
 ---- batch: 020 ----
mean loss: 34.31
 ---- batch: 030 ----
mean loss: 33.67
 ---- batch: 040 ----
mean loss: 31.87
train mean loss: 34.15
epoch train time: 0:00:00.206148
elapsed time: 0:00:58.749866
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 23:49:28.884871
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.72
 ---- batch: 020 ----
mean loss: 34.25
 ---- batch: 030 ----
mean loss: 34.91
 ---- batch: 040 ----
mean loss: 33.38
train mean loss: 34.12
epoch train time: 0:00:00.212703
elapsed time: 0:00:58.962697
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 23:49:29.097691
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 31.83
 ---- batch: 020 ----
mean loss: 35.60
 ---- batch: 030 ----
mean loss: 33.60
 ---- batch: 040 ----
mean loss: 34.84
train mean loss: 34.06
epoch train time: 0:00:00.207684
elapsed time: 0:00:59.170502
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 23:49:29.305512
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.84
 ---- batch: 020 ----
mean loss: 35.18
 ---- batch: 030 ----
mean loss: 33.84
 ---- batch: 040 ----
mean loss: 33.07
train mean loss: 34.16
epoch train time: 0:00:00.207166
elapsed time: 0:00:59.377807
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 23:49:29.512803
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.91
 ---- batch: 020 ----
mean loss: 34.31
 ---- batch: 030 ----
mean loss: 35.21
 ---- batch: 040 ----
mean loss: 33.81
train mean loss: 34.10
epoch train time: 0:00:00.205921
elapsed time: 0:00:59.583918
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 23:49:29.718914
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.40
 ---- batch: 020 ----
mean loss: 33.44
 ---- batch: 030 ----
mean loss: 34.44
 ---- batch: 040 ----
mean loss: 34.94
train mean loss: 34.16
epoch train time: 0:00:00.214140
elapsed time: 0:00:59.798181
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 23:49:29.933176
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.30
 ---- batch: 020 ----
mean loss: 35.01
 ---- batch: 030 ----
mean loss: 33.47
 ---- batch: 040 ----
mean loss: 34.47
train mean loss: 34.17
epoch train time: 0:00:00.208386
elapsed time: 0:01:00.006740
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 23:49:30.141762
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.13
 ---- batch: 020 ----
mean loss: 33.12
 ---- batch: 030 ----
mean loss: 33.67
 ---- batch: 040 ----
mean loss: 34.88
train mean loss: 33.97
epoch train time: 0:00:00.208566
elapsed time: 0:01:00.215454
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 23:49:30.350449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.07
 ---- batch: 020 ----
mean loss: 35.14
 ---- batch: 030 ----
mean loss: 33.53
 ---- batch: 040 ----
mean loss: 33.90
train mean loss: 34.09
epoch train time: 0:00:00.207696
elapsed time: 0:01:00.423269
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 23:49:30.558263
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.84
 ---- batch: 020 ----
mean loss: 34.39
 ---- batch: 030 ----
mean loss: 34.03
 ---- batch: 040 ----
mean loss: 33.53
train mean loss: 34.04
epoch train time: 0:00:00.208804
elapsed time: 0:01:00.632691
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 23:49:30.767693
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.07
 ---- batch: 020 ----
mean loss: 33.83
 ---- batch: 030 ----
mean loss: 34.50
 ---- batch: 040 ----
mean loss: 34.23
train mean loss: 33.98
epoch train time: 0:00:00.207810
elapsed time: 0:01:00.840629
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 23:49:30.975623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.28
 ---- batch: 020 ----
mean loss: 34.45
 ---- batch: 030 ----
mean loss: 34.44
 ---- batch: 040 ----
mean loss: 32.80
train mean loss: 33.89
epoch train time: 0:00:00.204944
elapsed time: 0:01:01.045691
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 23:49:31.180684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.71
 ---- batch: 020 ----
mean loss: 31.63
 ---- batch: 030 ----
mean loss: 35.45
 ---- batch: 040 ----
mean loss: 34.37
train mean loss: 33.95
epoch train time: 0:00:00.202708
elapsed time: 0:01:01.248563
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 23:49:31.383572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.09
 ---- batch: 020 ----
mean loss: 34.97
 ---- batch: 030 ----
mean loss: 34.02
 ---- batch: 040 ----
mean loss: 35.02
train mean loss: 33.87
epoch train time: 0:00:00.200243
elapsed time: 0:01:01.448962
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 23:49:31.583956
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.61
 ---- batch: 020 ----
mean loss: 34.05
 ---- batch: 030 ----
mean loss: 33.98
 ---- batch: 040 ----
mean loss: 32.73
train mean loss: 33.96
epoch train time: 0:00:00.210279
elapsed time: 0:01:01.659356
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 23:49:31.794376
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.24
 ---- batch: 020 ----
mean loss: 32.31
 ---- batch: 030 ----
mean loss: 33.90
 ---- batch: 040 ----
mean loss: 35.34
train mean loss: 34.12
epoch train time: 0:00:00.201768
elapsed time: 0:01:01.861274
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 23:49:31.996267
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.02
 ---- batch: 020 ----
mean loss: 34.49
 ---- batch: 030 ----
mean loss: 34.33
 ---- batch: 040 ----
mean loss: 32.19
train mean loss: 33.88
epoch train time: 0:00:00.202478
elapsed time: 0:01:02.063868
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 23:49:32.198860
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.47
 ---- batch: 020 ----
mean loss: 36.13
 ---- batch: 030 ----
mean loss: 32.82
 ---- batch: 040 ----
mean loss: 32.63
train mean loss: 34.14
epoch train time: 0:00:00.201606
elapsed time: 0:01:02.265596
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 23:49:32.400580
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.36
 ---- batch: 020 ----
mean loss: 33.56
 ---- batch: 030 ----
mean loss: 34.31
 ---- batch: 040 ----
mean loss: 34.64
train mean loss: 33.94
epoch train time: 0:00:00.202760
elapsed time: 0:01:02.468482
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 23:49:32.603477
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.23
 ---- batch: 020 ----
mean loss: 33.11
 ---- batch: 030 ----
mean loss: 32.17
 ---- batch: 040 ----
mean loss: 35.05
train mean loss: 33.86
epoch train time: 0:00:00.210492
elapsed time: 0:01:02.679098
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 23:49:32.814106
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.43
 ---- batch: 020 ----
mean loss: 33.59
 ---- batch: 030 ----
mean loss: 34.31
 ---- batch: 040 ----
mean loss: 33.29
train mean loss: 33.74
epoch train time: 0:00:00.203738
elapsed time: 0:01:02.882968
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 23:49:33.017962
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.45
 ---- batch: 020 ----
mean loss: 31.98
 ---- batch: 030 ----
mean loss: 34.35
 ---- batch: 040 ----
mean loss: 33.51
train mean loss: 33.79
epoch train time: 0:00:00.209629
elapsed time: 0:01:03.092717
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 23:49:33.227714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.50
 ---- batch: 020 ----
mean loss: 34.38
 ---- batch: 030 ----
mean loss: 33.02
 ---- batch: 040 ----
mean loss: 34.23
train mean loss: 33.84
epoch train time: 0:00:00.206847
elapsed time: 0:01:03.299686
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 23:49:33.434680
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 32.88
 ---- batch: 020 ----
mean loss: 33.57
 ---- batch: 030 ----
mean loss: 35.27
 ---- batch: 040 ----
mean loss: 34.01
train mean loss: 33.73
epoch train time: 0:00:00.207555
elapsed time: 0:01:03.507375
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 23:49:33.642370
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.65
 ---- batch: 020 ----
mean loss: 34.98
 ---- batch: 030 ----
mean loss: 31.67
 ---- batch: 040 ----
mean loss: 34.49
train mean loss: 33.84
epoch train time: 0:00:00.205904
elapsed time: 0:01:03.713417
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 23:49:33.848437
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.64
 ---- batch: 020 ----
mean loss: 33.22
 ---- batch: 030 ----
mean loss: 34.03
 ---- batch: 040 ----
mean loss: 34.78
train mean loss: 33.71
epoch train time: 0:00:00.217902
elapsed time: 0:01:03.931462
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 23:49:34.066458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 35.28
 ---- batch: 020 ----
mean loss: 33.63
 ---- batch: 030 ----
mean loss: 33.78
 ---- batch: 040 ----
mean loss: 33.29
train mean loss: 33.71
epoch train time: 0:00:00.207197
elapsed time: 0:01:04.138779
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 23:49:34.273793
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.17
 ---- batch: 020 ----
mean loss: 32.26
 ---- batch: 030 ----
mean loss: 34.80
 ---- batch: 040 ----
mean loss: 33.65
train mean loss: 33.71
epoch train time: 0:00:00.205691
elapsed time: 0:01:04.344626
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 23:49:34.479621
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.34
 ---- batch: 020 ----
mean loss: 34.06
 ---- batch: 030 ----
mean loss: 34.38
 ---- batch: 040 ----
mean loss: 33.30
train mean loss: 33.60
epoch train time: 0:00:00.205638
elapsed time: 0:01:04.550402
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 23:49:34.685398
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.09
 ---- batch: 020 ----
mean loss: 34.69
 ---- batch: 030 ----
mean loss: 34.01
 ---- batch: 040 ----
mean loss: 34.01
train mean loss: 33.67
epoch train time: 0:00:00.211091
elapsed time: 0:01:04.761624
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 23:49:34.896625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.03
 ---- batch: 020 ----
mean loss: 32.21
 ---- batch: 030 ----
mean loss: 35.05
 ---- batch: 040 ----
mean loss: 35.48
train mean loss: 33.74
epoch train time: 0:00:00.207686
elapsed time: 0:01:04.969453
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 23:49:35.104449
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.57
 ---- batch: 020 ----
mean loss: 34.09
 ---- batch: 030 ----
mean loss: 32.48
 ---- batch: 040 ----
mean loss: 34.21
train mean loss: 33.69
epoch train time: 0:00:00.201329
elapsed time: 0:01:05.170904
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 23:49:35.305894
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 33.89
 ---- batch: 020 ----
mean loss: 33.29
 ---- batch: 030 ----
mean loss: 34.10
 ---- batch: 040 ----
mean loss: 32.52
train mean loss: 33.73
epoch train time: 0:00:00.195476
elapsed time: 0:01:05.366509
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 23:49:35.501503
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 34.83
 ---- batch: 020 ----
mean loss: 34.11
 ---- batch: 030 ----
mean loss: 33.30
 ---- batch: 040 ----
mean loss: 32.62
train mean loss: 33.61
epoch train time: 0:00:00.198708
elapsed time: 0:01:05.568373
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_dense3/frequentist_dense3_8/checkpoint.pth.tar
**** end time: 2019-09-20 23:49:35.703339 ****
