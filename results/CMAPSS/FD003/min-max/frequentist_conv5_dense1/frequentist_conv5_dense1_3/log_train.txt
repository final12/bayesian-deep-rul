Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_3', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 6841
use_cuda: True
Dataset: CMAPSS/FD003
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-20 22:30:10.739364 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 31, 14]             100
              Tanh-2           [-1, 10, 31, 14]               0
            Conv2d-3           [-1, 10, 30, 14]           1,000
              Tanh-4           [-1, 10, 30, 14]               0
            Conv2d-5           [-1, 10, 31, 14]           1,000
              Tanh-6           [-1, 10, 31, 14]               0
            Conv2d-7           [-1, 10, 30, 14]           1,000
              Tanh-8           [-1, 10, 30, 14]               0
            Conv2d-9            [-1, 1, 30, 14]              30
             Tanh-10            [-1, 1, 30, 14]               0
          Flatten-11                  [-1, 420]               0
          Dropout-12                  [-1, 420]               0
           Linear-13                  [-1, 100]          42,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 45,230
Trainable params: 45,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 22:30:10.747249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3872.02
 ---- batch: 020 ----
mean loss: 1394.73
 ---- batch: 030 ----
mean loss: 416.46
 ---- batch: 040 ----
mean loss: 441.49
train mean loss: 1452.31
epoch train time: 0:00:15.221019
elapsed time: 0:00:15.231350
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 22:30:25.970752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 349.88
 ---- batch: 020 ----
mean loss: 346.62
 ---- batch: 030 ----
mean loss: 323.48
 ---- batch: 040 ----
mean loss: 313.57
train mean loss: 333.67
epoch train time: 0:00:01.823027
elapsed time: 0:00:17.054515
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 22:30:27.793933
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.79
 ---- batch: 020 ----
mean loss: 302.02
 ---- batch: 030 ----
mean loss: 289.88
 ---- batch: 040 ----
mean loss: 280.03
train mean loss: 291.61
epoch train time: 0:00:01.691837
elapsed time: 0:00:18.746512
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 22:30:29.485923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 254.96
 ---- batch: 020 ----
mean loss: 246.71
 ---- batch: 030 ----
mean loss: 239.32
 ---- batch: 040 ----
mean loss: 237.60
train mean loss: 243.79
epoch train time: 0:00:01.732559
elapsed time: 0:00:20.479198
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 22:30:31.218607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.27
 ---- batch: 020 ----
mean loss: 234.23
 ---- batch: 030 ----
mean loss: 226.45
 ---- batch: 040 ----
mean loss: 213.09
train mean loss: 224.57
epoch train time: 0:00:01.719310
elapsed time: 0:00:22.198653
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 22:30:32.938091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.80
 ---- batch: 020 ----
mean loss: 212.12
 ---- batch: 030 ----
mean loss: 214.32
 ---- batch: 040 ----
mean loss: 213.83
train mean loss: 214.28
epoch train time: 0:00:01.675473
elapsed time: 0:00:23.874289
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 22:30:34.613699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.49
 ---- batch: 020 ----
mean loss: 222.23
 ---- batch: 030 ----
mean loss: 212.52
 ---- batch: 040 ----
mean loss: 208.16
train mean loss: 212.97
epoch train time: 0:00:01.788119
elapsed time: 0:00:25.662533
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 22:30:36.401998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.07
 ---- batch: 020 ----
mean loss: 214.21
 ---- batch: 030 ----
mean loss: 204.35
 ---- batch: 040 ----
mean loss: 198.35
train mean loss: 207.30
epoch train time: 0:00:01.674110
elapsed time: 0:00:27.336833
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 22:30:38.076245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.37
 ---- batch: 020 ----
mean loss: 204.01
 ---- batch: 030 ----
mean loss: 196.33
 ---- batch: 040 ----
mean loss: 206.58
train mean loss: 202.99
epoch train time: 0:00:01.774363
elapsed time: 0:00:29.111356
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 22:30:39.850788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.76
 ---- batch: 020 ----
mean loss: 203.36
 ---- batch: 030 ----
mean loss: 201.45
 ---- batch: 040 ----
mean loss: 203.75
train mean loss: 203.94
epoch train time: 0:00:01.676782
elapsed time: 0:00:30.788324
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 22:30:41.527754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.71
 ---- batch: 020 ----
mean loss: 205.33
 ---- batch: 030 ----
mean loss: 196.31
 ---- batch: 040 ----
mean loss: 200.44
train mean loss: 202.49
epoch train time: 0:00:01.674368
elapsed time: 0:00:32.462847
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 22:30:43.202290
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.59
 ---- batch: 020 ----
mean loss: 203.41
 ---- batch: 030 ----
mean loss: 196.96
 ---- batch: 040 ----
mean loss: 198.02
train mean loss: 200.73
epoch train time: 0:00:01.784498
elapsed time: 0:00:34.247502
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 22:30:44.986908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.18
 ---- batch: 020 ----
mean loss: 196.38
 ---- batch: 030 ----
mean loss: 194.47
 ---- batch: 040 ----
mean loss: 196.05
train mean loss: 196.96
epoch train time: 0:00:01.677568
elapsed time: 0:00:35.925202
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 22:30:46.664619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.74
 ---- batch: 020 ----
mean loss: 197.16
 ---- batch: 030 ----
mean loss: 198.44
 ---- batch: 040 ----
mean loss: 198.38
train mean loss: 199.79
epoch train time: 0:00:01.774080
elapsed time: 0:00:37.699432
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 22:30:48.438841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.04
 ---- batch: 020 ----
mean loss: 202.31
 ---- batch: 030 ----
mean loss: 194.03
 ---- batch: 040 ----
mean loss: 194.15
train mean loss: 197.26
epoch train time: 0:00:01.675495
elapsed time: 0:00:39.375069
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 22:30:50.114480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.69
 ---- batch: 020 ----
mean loss: 191.69
 ---- batch: 030 ----
mean loss: 194.47
 ---- batch: 040 ----
mean loss: 195.19
train mean loss: 195.53
epoch train time: 0:00:01.722946
elapsed time: 0:00:41.098149
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 22:30:51.837557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.25
 ---- batch: 020 ----
mean loss: 196.78
 ---- batch: 030 ----
mean loss: 193.71
 ---- batch: 040 ----
mean loss: 190.45
train mean loss: 194.37
epoch train time: 0:00:01.736195
elapsed time: 0:00:42.834510
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 22:30:53.573935
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.35
 ---- batch: 020 ----
mean loss: 193.54
 ---- batch: 030 ----
mean loss: 198.93
 ---- batch: 040 ----
mean loss: 194.52
train mean loss: 195.60
epoch train time: 0:00:01.678371
elapsed time: 0:00:44.513050
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 22:30:55.252466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.30
 ---- batch: 020 ----
mean loss: 203.57
 ---- batch: 030 ----
mean loss: 204.94
 ---- batch: 040 ----
mean loss: 186.14
train mean loss: 196.53
epoch train time: 0:00:01.789578
elapsed time: 0:00:46.302772
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 22:30:57.042184
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.62
 ---- batch: 020 ----
mean loss: 197.29
 ---- batch: 030 ----
mean loss: 199.23
 ---- batch: 040 ----
mean loss: 194.13
train mean loss: 196.16
epoch train time: 0:00:01.682889
elapsed time: 0:00:47.985809
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 22:30:58.725223
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.59
 ---- batch: 020 ----
mean loss: 192.34
 ---- batch: 030 ----
mean loss: 196.10
 ---- batch: 040 ----
mean loss: 184.31
train mean loss: 191.85
epoch train time: 0:00:01.785640
elapsed time: 0:00:49.771615
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 22:31:00.511028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.20
 ---- batch: 020 ----
mean loss: 202.83
 ---- batch: 030 ----
mean loss: 183.61
 ---- batch: 040 ----
mean loss: 193.66
train mean loss: 193.34
epoch train time: 0:00:01.682954
elapsed time: 0:00:51.454723
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 22:31:02.194149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.92
 ---- batch: 020 ----
mean loss: 194.12
 ---- batch: 030 ----
mean loss: 188.22
 ---- batch: 040 ----
mean loss: 188.43
train mean loss: 192.95
epoch train time: 0:00:01.675583
elapsed time: 0:00:53.130456
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 22:31:03.869896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.47
 ---- batch: 020 ----
mean loss: 185.77
 ---- batch: 030 ----
mean loss: 194.86
 ---- batch: 040 ----
mean loss: 196.28
train mean loss: 192.03
epoch train time: 0:00:01.794799
elapsed time: 0:00:54.925418
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 22:31:05.664827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.36
 ---- batch: 020 ----
mean loss: 197.01
 ---- batch: 030 ----
mean loss: 196.30
 ---- batch: 040 ----
mean loss: 201.32
train mean loss: 195.47
epoch train time: 0:00:01.685466
elapsed time: 0:00:56.611025
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 22:31:07.350437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.77
 ---- batch: 020 ----
mean loss: 197.22
 ---- batch: 030 ----
mean loss: 190.39
 ---- batch: 040 ----
mean loss: 191.03
train mean loss: 192.83
epoch train time: 0:00:01.781679
elapsed time: 0:00:58.392828
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 22:31:09.132238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.78
 ---- batch: 020 ----
mean loss: 189.06
 ---- batch: 030 ----
mean loss: 199.20
 ---- batch: 040 ----
mean loss: 191.49
train mean loss: 191.38
epoch train time: 0:00:01.683205
elapsed time: 0:01:00.076180
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 22:31:10.815603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.47
 ---- batch: 020 ----
mean loss: 196.55
 ---- batch: 030 ----
mean loss: 194.96
 ---- batch: 040 ----
mean loss: 193.53
train mean loss: 194.85
epoch train time: 0:00:01.779919
elapsed time: 0:01:01.856235
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 22:31:12.595643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.30
 ---- batch: 020 ----
mean loss: 184.68
 ---- batch: 030 ----
mean loss: 193.52
 ---- batch: 040 ----
mean loss: 201.98
train mean loss: 193.65
epoch train time: 0:00:01.691233
elapsed time: 0:01:03.547608
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 22:31:14.287022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.38
 ---- batch: 020 ----
mean loss: 202.47
 ---- batch: 030 ----
mean loss: 184.09
 ---- batch: 040 ----
mean loss: 188.86
train mean loss: 192.28
epoch train time: 0:00:01.679873
elapsed time: 0:01:05.227616
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 22:31:15.967025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.18
 ---- batch: 020 ----
mean loss: 194.21
 ---- batch: 030 ----
mean loss: 196.77
 ---- batch: 040 ----
mean loss: 189.06
train mean loss: 195.44
epoch train time: 0:00:01.786679
elapsed time: 0:01:07.014426
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 22:31:17.753870
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.20
 ---- batch: 020 ----
mean loss: 189.71
 ---- batch: 030 ----
mean loss: 190.05
 ---- batch: 040 ----
mean loss: 188.67
train mean loss: 191.09
epoch train time: 0:00:01.681079
elapsed time: 0:01:08.695688
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 22:31:19.435101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.30
 ---- batch: 020 ----
mean loss: 183.62
 ---- batch: 030 ----
mean loss: 191.07
 ---- batch: 040 ----
mean loss: 189.74
train mean loss: 192.62
epoch train time: 0:00:01.784379
elapsed time: 0:01:10.480206
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 22:31:21.219663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.98
 ---- batch: 020 ----
mean loss: 194.05
 ---- batch: 030 ----
mean loss: 186.65
 ---- batch: 040 ----
mean loss: 185.65
train mean loss: 189.06
epoch train time: 0:00:01.694411
elapsed time: 0:01:12.174793
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 22:31:22.914216
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.35
 ---- batch: 020 ----
mean loss: 194.03
 ---- batch: 030 ----
mean loss: 185.76
 ---- batch: 040 ----
mean loss: 190.69
train mean loss: 190.19
epoch train time: 0:00:01.698561
elapsed time: 0:01:13.873534
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 22:31:24.612964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.27
 ---- batch: 020 ----
mean loss: 183.04
 ---- batch: 030 ----
mean loss: 186.75
 ---- batch: 040 ----
mean loss: 187.55
train mean loss: 188.29
epoch train time: 0:00:01.771904
elapsed time: 0:01:15.645632
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 22:31:26.385042
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.73
 ---- batch: 020 ----
mean loss: 190.90
 ---- batch: 030 ----
mean loss: 188.66
 ---- batch: 040 ----
mean loss: 188.66
train mean loss: 187.82
epoch train time: 0:00:01.683533
elapsed time: 0:01:17.329306
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 22:31:28.068716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.04
 ---- batch: 020 ----
mean loss: 186.53
 ---- batch: 030 ----
mean loss: 184.95
 ---- batch: 040 ----
mean loss: 189.03
train mean loss: 187.94
epoch train time: 0:00:01.794307
elapsed time: 0:01:19.123740
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 22:31:29.863151
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.93
 ---- batch: 020 ----
mean loss: 185.50
 ---- batch: 030 ----
mean loss: 189.75
 ---- batch: 040 ----
mean loss: 194.60
train mean loss: 189.70
epoch train time: 0:00:01.688069
elapsed time: 0:01:20.811960
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 22:31:31.551370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.13
 ---- batch: 020 ----
mean loss: 185.45
 ---- batch: 030 ----
mean loss: 186.84
 ---- batch: 040 ----
mean loss: 185.18
train mean loss: 187.17
epoch train time: 0:00:01.794585
elapsed time: 0:01:22.606669
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 22:31:33.346076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.97
 ---- batch: 020 ----
mean loss: 186.71
 ---- batch: 030 ----
mean loss: 188.48
 ---- batch: 040 ----
mean loss: 190.07
train mean loss: 188.02
epoch train time: 0:00:01.686841
elapsed time: 0:01:24.293657
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 22:31:35.033080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.82
 ---- batch: 020 ----
mean loss: 186.10
 ---- batch: 030 ----
mean loss: 195.99
 ---- batch: 040 ----
mean loss: 182.67
train mean loss: 189.25
epoch train time: 0:00:01.680933
elapsed time: 0:01:25.974778
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 22:31:36.714190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.99
 ---- batch: 020 ----
mean loss: 186.23
 ---- batch: 030 ----
mean loss: 188.61
 ---- batch: 040 ----
mean loss: 183.85
train mean loss: 186.78
epoch train time: 0:00:01.788571
elapsed time: 0:01:27.763477
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 22:31:38.502915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.18
 ---- batch: 020 ----
mean loss: 185.21
 ---- batch: 030 ----
mean loss: 179.11
 ---- batch: 040 ----
mean loss: 188.08
train mean loss: 186.25
epoch train time: 0:00:01.687634
elapsed time: 0:01:29.451289
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 22:31:40.190701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.82
 ---- batch: 020 ----
mean loss: 184.99
 ---- batch: 030 ----
mean loss: 178.67
 ---- batch: 040 ----
mean loss: 176.15
train mean loss: 181.18
epoch train time: 0:00:01.787590
elapsed time: 0:01:31.239025
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 22:31:41.978431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.69
 ---- batch: 020 ----
mean loss: 181.05
 ---- batch: 030 ----
mean loss: 177.60
 ---- batch: 040 ----
mean loss: 180.21
train mean loss: 180.14
epoch train time: 0:00:01.681020
elapsed time: 0:01:32.920209
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 22:31:43.659637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.00
 ---- batch: 020 ----
mean loss: 174.75
 ---- batch: 030 ----
mean loss: 182.21
 ---- batch: 040 ----
mean loss: 173.59
train mean loss: 178.50
epoch train time: 0:00:01.784448
elapsed time: 0:01:34.704833
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 22:31:45.444277
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.79
 ---- batch: 020 ----
mean loss: 181.29
 ---- batch: 030 ----
mean loss: 186.53
 ---- batch: 040 ----
mean loss: 174.63
train mean loss: 180.82
epoch train time: 0:00:01.692736
elapsed time: 0:01:36.397763
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 22:31:47.137176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.80
 ---- batch: 020 ----
mean loss: 182.89
 ---- batch: 030 ----
mean loss: 174.31
 ---- batch: 040 ----
mean loss: 177.85
train mean loss: 177.60
epoch train time: 0:00:01.688552
elapsed time: 0:01:38.086460
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 22:31:48.825875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.46
 ---- batch: 020 ----
mean loss: 176.92
 ---- batch: 030 ----
mean loss: 167.90
 ---- batch: 040 ----
mean loss: 178.78
train mean loss: 175.58
epoch train time: 0:00:01.791193
elapsed time: 0:01:39.877782
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 22:31:50.617189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.46
 ---- batch: 020 ----
mean loss: 173.04
 ---- batch: 030 ----
mean loss: 172.21
 ---- batch: 040 ----
mean loss: 173.43
train mean loss: 174.24
epoch train time: 0:00:01.678781
elapsed time: 0:01:41.556711
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 22:31:52.296122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.79
 ---- batch: 020 ----
mean loss: 172.63
 ---- batch: 030 ----
mean loss: 180.22
 ---- batch: 040 ----
mean loss: 165.72
train mean loss: 173.28
epoch train time: 0:00:01.788904
elapsed time: 0:01:43.345749
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 22:31:54.085161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.87
 ---- batch: 020 ----
mean loss: 163.98
 ---- batch: 030 ----
mean loss: 170.22
 ---- batch: 040 ----
mean loss: 165.86
train mean loss: 169.53
epoch train time: 0:00:01.695979
elapsed time: 0:01:45.041881
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 22:31:55.781297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.47
 ---- batch: 020 ----
mean loss: 167.00
 ---- batch: 030 ----
mean loss: 166.20
 ---- batch: 040 ----
mean loss: 173.08
train mean loss: 168.90
epoch train time: 0:00:01.678689
elapsed time: 0:01:46.720705
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 22:31:57.460127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.90
 ---- batch: 020 ----
mean loss: 172.31
 ---- batch: 030 ----
mean loss: 172.05
 ---- batch: 040 ----
mean loss: 162.90
train mean loss: 168.65
epoch train time: 0:00:01.803716
elapsed time: 0:01:48.524597
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 22:31:59.264006
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.03
 ---- batch: 020 ----
mean loss: 170.88
 ---- batch: 030 ----
mean loss: 167.04
 ---- batch: 040 ----
mean loss: 158.46
train mean loss: 165.51
epoch train time: 0:00:01.686552
elapsed time: 0:01:50.211284
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 22:32:00.950715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.41
 ---- batch: 020 ----
mean loss: 156.71
 ---- batch: 030 ----
mean loss: 161.16
 ---- batch: 040 ----
mean loss: 171.16
train mean loss: 164.31
epoch train time: 0:00:01.785917
elapsed time: 0:01:51.997349
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 22:32:02.736756
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.81
 ---- batch: 020 ----
mean loss: 156.88
 ---- batch: 030 ----
mean loss: 161.32
 ---- batch: 040 ----
mean loss: 160.21
train mean loss: 161.14
epoch train time: 0:00:01.679596
elapsed time: 0:01:53.677096
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 22:32:04.416527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.25
 ---- batch: 020 ----
mean loss: 161.95
 ---- batch: 030 ----
mean loss: 161.27
 ---- batch: 040 ----
mean loss: 160.85
train mean loss: 159.69
epoch train time: 0:00:01.790578
elapsed time: 0:01:55.467821
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 22:32:06.207231
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.03
 ---- batch: 020 ----
mean loss: 166.75
 ---- batch: 030 ----
mean loss: 147.99
 ---- batch: 040 ----
mean loss: 159.58
train mean loss: 157.86
epoch train time: 0:00:01.698052
elapsed time: 0:01:57.166039
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 22:32:07.905473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.37
 ---- batch: 020 ----
mean loss: 154.09
 ---- batch: 030 ----
mean loss: 156.61
 ---- batch: 040 ----
mean loss: 161.04
train mean loss: 157.48
epoch train time: 0:00:01.679560
elapsed time: 0:01:58.845761
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 22:32:09.585173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.97
 ---- batch: 020 ----
mean loss: 158.04
 ---- batch: 030 ----
mean loss: 153.00
 ---- batch: 040 ----
mean loss: 159.30
train mean loss: 157.90
epoch train time: 0:00:01.782767
elapsed time: 0:02:00.628672
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 22:32:11.368089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.11
 ---- batch: 020 ----
mean loss: 156.31
 ---- batch: 030 ----
mean loss: 156.19
 ---- batch: 040 ----
mean loss: 161.56
train mean loss: 157.71
epoch train time: 0:00:01.677043
elapsed time: 0:02:02.305855
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 22:32:13.045278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.57
 ---- batch: 020 ----
mean loss: 156.55
 ---- batch: 030 ----
mean loss: 157.02
 ---- batch: 040 ----
mean loss: 155.50
train mean loss: 155.77
epoch train time: 0:00:01.788179
elapsed time: 0:02:04.094181
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 22:32:14.833599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.07
 ---- batch: 020 ----
mean loss: 157.98
 ---- batch: 030 ----
mean loss: 157.83
 ---- batch: 040 ----
mean loss: 150.03
train mean loss: 157.63
epoch train time: 0:00:01.690711
elapsed time: 0:02:05.785053
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 22:32:16.524481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.10
 ---- batch: 020 ----
mean loss: 158.48
 ---- batch: 030 ----
mean loss: 152.44
 ---- batch: 040 ----
mean loss: 155.96
train mean loss: 155.44
epoch train time: 0:00:01.783890
elapsed time: 0:02:07.569109
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 22:32:18.308520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.71
 ---- batch: 020 ----
mean loss: 155.11
 ---- batch: 030 ----
mean loss: 157.73
 ---- batch: 040 ----
mean loss: 160.23
train mean loss: 156.82
epoch train time: 0:00:01.683690
elapsed time: 0:02:09.252985
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 22:32:19.992414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.12
 ---- batch: 020 ----
mean loss: 151.72
 ---- batch: 030 ----
mean loss: 156.30
 ---- batch: 040 ----
mean loss: 152.24
train mean loss: 152.31
epoch train time: 0:00:01.681042
elapsed time: 0:02:10.934203
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 22:32:21.673617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.59
 ---- batch: 020 ----
mean loss: 154.18
 ---- batch: 030 ----
mean loss: 152.40
 ---- batch: 040 ----
mean loss: 148.03
train mean loss: 151.91
epoch train time: 0:00:01.794207
elapsed time: 0:02:12.728539
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 22:32:23.467946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.99
 ---- batch: 020 ----
mean loss: 146.74
 ---- batch: 030 ----
mean loss: 148.28
 ---- batch: 040 ----
mean loss: 150.88
train mean loss: 150.64
epoch train time: 0:00:01.684244
elapsed time: 0:02:14.412950
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 22:32:25.152429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.50
 ---- batch: 020 ----
mean loss: 160.26
 ---- batch: 030 ----
mean loss: 155.78
 ---- batch: 040 ----
mean loss: 149.25
train mean loss: 155.76
epoch train time: 0:00:01.790199
elapsed time: 0:02:16.203339
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 22:32:26.942747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.95
 ---- batch: 020 ----
mean loss: 150.79
 ---- batch: 030 ----
mean loss: 147.74
 ---- batch: 040 ----
mean loss: 142.61
train mean loss: 147.93
epoch train time: 0:00:01.691116
elapsed time: 0:02:17.894617
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 22:32:28.634027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.99
 ---- batch: 020 ----
mean loss: 151.29
 ---- batch: 030 ----
mean loss: 143.70
 ---- batch: 040 ----
mean loss: 148.90
train mean loss: 148.99
epoch train time: 0:00:01.748980
elapsed time: 0:02:19.643727
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 22:32:30.383137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.27
 ---- batch: 020 ----
mean loss: 143.70
 ---- batch: 030 ----
mean loss: 147.64
 ---- batch: 040 ----
mean loss: 142.46
train mean loss: 147.83
epoch train time: 0:00:01.732350
elapsed time: 0:02:21.376233
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 22:32:32.115643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.42
 ---- batch: 020 ----
mean loss: 148.51
 ---- batch: 030 ----
mean loss: 146.62
 ---- batch: 040 ----
mean loss: 144.52
train mean loss: 147.05
epoch train time: 0:00:01.683949
elapsed time: 0:02:23.060334
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 22:32:33.799762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.36
 ---- batch: 020 ----
mean loss: 149.40
 ---- batch: 030 ----
mean loss: 145.81
 ---- batch: 040 ----
mean loss: 144.59
train mean loss: 146.42
epoch train time: 0:00:01.798020
elapsed time: 0:02:24.858506
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 22:32:35.597911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.80
 ---- batch: 020 ----
mean loss: 142.61
 ---- batch: 030 ----
mean loss: 147.06
 ---- batch: 040 ----
mean loss: 140.83
train mean loss: 143.43
epoch train time: 0:00:01.694208
elapsed time: 0:02:26.552955
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 22:32:37.292422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.38
 ---- batch: 020 ----
mean loss: 148.20
 ---- batch: 030 ----
mean loss: 141.91
 ---- batch: 040 ----
mean loss: 141.16
train mean loss: 143.01
epoch train time: 0:00:01.793848
elapsed time: 0:02:28.346989
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 22:32:39.086407
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.15
 ---- batch: 020 ----
mean loss: 151.91
 ---- batch: 030 ----
mean loss: 148.80
 ---- batch: 040 ----
mean loss: 148.32
train mean loss: 150.59
epoch train time: 0:00:01.691998
elapsed time: 0:02:30.039138
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 22:32:40.778574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.92
 ---- batch: 020 ----
mean loss: 137.37
 ---- batch: 030 ----
mean loss: 143.14
 ---- batch: 040 ----
mean loss: 142.34
train mean loss: 139.66
epoch train time: 0:00:01.676121
elapsed time: 0:02:31.715434
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 22:32:42.454845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.76
 ---- batch: 020 ----
mean loss: 137.02
 ---- batch: 030 ----
mean loss: 141.93
 ---- batch: 040 ----
mean loss: 140.70
train mean loss: 141.35
epoch train time: 0:00:01.798616
elapsed time: 0:02:33.514204
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 22:32:44.253617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.91
 ---- batch: 020 ----
mean loss: 146.53
 ---- batch: 030 ----
mean loss: 142.83
 ---- batch: 040 ----
mean loss: 136.57
train mean loss: 141.31
epoch train time: 0:00:01.681646
elapsed time: 0:02:35.195988
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 22:32:45.935412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.21
 ---- batch: 020 ----
mean loss: 143.33
 ---- batch: 030 ----
mean loss: 135.32
 ---- batch: 040 ----
mean loss: 139.62
train mean loss: 139.96
epoch train time: 0:00:01.800330
elapsed time: 0:02:36.996460
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 22:32:47.735887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.74
 ---- batch: 020 ----
mean loss: 140.09
 ---- batch: 030 ----
mean loss: 138.48
 ---- batch: 040 ----
mean loss: 141.21
train mean loss: 139.33
epoch train time: 0:00:01.685604
elapsed time: 0:02:38.682215
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 22:32:49.421625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.80
 ---- batch: 020 ----
mean loss: 135.53
 ---- batch: 030 ----
mean loss: 138.80
 ---- batch: 040 ----
mean loss: 141.46
train mean loss: 139.43
epoch train time: 0:00:01.786557
elapsed time: 0:02:40.468916
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 22:32:51.208341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.87
 ---- batch: 020 ----
mean loss: 134.68
 ---- batch: 030 ----
mean loss: 137.05
 ---- batch: 040 ----
mean loss: 141.57
train mean loss: 136.35
epoch train time: 0:00:01.692438
elapsed time: 0:02:42.161511
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 22:32:52.900923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.75
 ---- batch: 020 ----
mean loss: 142.17
 ---- batch: 030 ----
mean loss: 141.01
 ---- batch: 040 ----
mean loss: 134.95
train mean loss: 142.57
epoch train time: 0:00:01.682554
elapsed time: 0:02:43.844197
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 22:32:54.583608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.07
 ---- batch: 020 ----
mean loss: 135.55
 ---- batch: 030 ----
mean loss: 135.93
 ---- batch: 040 ----
mean loss: 132.99
train mean loss: 135.25
epoch train time: 0:00:01.793189
elapsed time: 0:02:45.637511
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 22:32:56.376918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.46
 ---- batch: 020 ----
mean loss: 132.69
 ---- batch: 030 ----
mean loss: 134.58
 ---- batch: 040 ----
mean loss: 128.25
train mean loss: 133.78
epoch train time: 0:00:01.691719
elapsed time: 0:02:47.329370
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 22:32:58.068785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.69
 ---- batch: 020 ----
mean loss: 134.61
 ---- batch: 030 ----
mean loss: 137.87
 ---- batch: 040 ----
mean loss: 134.98
train mean loss: 135.15
epoch train time: 0:00:01.798184
elapsed time: 0:02:49.127687
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 22:32:59.867112
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.04
 ---- batch: 020 ----
mean loss: 134.83
 ---- batch: 030 ----
mean loss: 129.99
 ---- batch: 040 ----
mean loss: 133.41
train mean loss: 132.56
epoch train time: 0:00:01.685732
elapsed time: 0:02:50.813565
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 22:33:01.552975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.25
 ---- batch: 020 ----
mean loss: 131.30
 ---- batch: 030 ----
mean loss: 131.38
 ---- batch: 040 ----
mean loss: 141.39
train mean loss: 135.54
epoch train time: 0:00:01.776392
elapsed time: 0:02:52.590080
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 22:33:03.329502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.04
 ---- batch: 020 ----
mean loss: 133.24
 ---- batch: 030 ----
mean loss: 130.78
 ---- batch: 040 ----
mean loss: 139.72
train mean loss: 134.18
epoch train time: 0:00:01.684597
elapsed time: 0:02:54.274864
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 22:33:05.014286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.00
 ---- batch: 020 ----
mean loss: 136.95
 ---- batch: 030 ----
mean loss: 137.54
 ---- batch: 040 ----
mean loss: 135.83
train mean loss: 136.22
epoch train time: 0:00:01.680889
elapsed time: 0:02:55.955918
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 22:33:06.695357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.34
 ---- batch: 020 ----
mean loss: 131.95
 ---- batch: 030 ----
mean loss: 129.53
 ---- batch: 040 ----
mean loss: 130.78
train mean loss: 131.01
epoch train time: 0:00:01.791904
elapsed time: 0:02:57.747988
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 22:33:08.487396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.66
 ---- batch: 020 ----
mean loss: 129.37
 ---- batch: 030 ----
mean loss: 125.67
 ---- batch: 040 ----
mean loss: 126.68
train mean loss: 129.37
epoch train time: 0:00:01.685912
elapsed time: 0:02:59.434036
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 22:33:10.173447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.13
 ---- batch: 020 ----
mean loss: 133.31
 ---- batch: 030 ----
mean loss: 130.48
 ---- batch: 040 ----
mean loss: 140.92
train mean loss: 135.96
epoch train time: 0:00:01.785020
elapsed time: 0:03:01.219182
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 22:33:11.958588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.82
 ---- batch: 020 ----
mean loss: 127.74
 ---- batch: 030 ----
mean loss: 131.78
 ---- batch: 040 ----
mean loss: 128.80
train mean loss: 130.74
epoch train time: 0:00:01.691393
elapsed time: 0:03:02.910739
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 22:33:13.650166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.17
 ---- batch: 020 ----
mean loss: 131.26
 ---- batch: 030 ----
mean loss: 133.08
 ---- batch: 040 ----
mean loss: 135.89
train mean loss: 132.20
epoch train time: 0:00:01.781676
elapsed time: 0:03:04.692572
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 22:33:15.431975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.53
 ---- batch: 020 ----
mean loss: 130.69
 ---- batch: 030 ----
mean loss: 127.19
 ---- batch: 040 ----
mean loss: 132.13
train mean loss: 132.29
epoch train time: 0:00:01.690209
elapsed time: 0:03:06.382954
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 22:33:17.122370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.89
 ---- batch: 020 ----
mean loss: 132.73
 ---- batch: 030 ----
mean loss: 129.93
 ---- batch: 040 ----
mean loss: 126.39
train mean loss: 129.72
epoch train time: 0:00:01.689202
elapsed time: 0:03:08.072324
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 22:33:18.811745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.29
 ---- batch: 020 ----
mean loss: 130.15
 ---- batch: 030 ----
mean loss: 128.89
 ---- batch: 040 ----
mean loss: 122.65
train mean loss: 127.36
epoch train time: 0:00:01.789851
elapsed time: 0:03:09.862321
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 22:33:20.601741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.86
 ---- batch: 020 ----
mean loss: 127.32
 ---- batch: 030 ----
mean loss: 129.83
 ---- batch: 040 ----
mean loss: 125.92
train mean loss: 127.04
epoch train time: 0:00:01.686826
elapsed time: 0:03:11.549292
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 22:33:22.288717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.18
 ---- batch: 020 ----
mean loss: 125.81
 ---- batch: 030 ----
mean loss: 123.53
 ---- batch: 040 ----
mean loss: 124.56
train mean loss: 125.82
epoch train time: 0:00:01.785955
elapsed time: 0:03:13.335427
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 22:33:24.074837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.55
 ---- batch: 020 ----
mean loss: 128.86
 ---- batch: 030 ----
mean loss: 129.68
 ---- batch: 040 ----
mean loss: 130.65
train mean loss: 129.15
epoch train time: 0:00:01.688338
elapsed time: 0:03:15.023904
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 22:33:25.763335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.78
 ---- batch: 020 ----
mean loss: 125.53
 ---- batch: 030 ----
mean loss: 126.37
 ---- batch: 040 ----
mean loss: 126.06
train mean loss: 125.66
epoch train time: 0:00:01.673196
elapsed time: 0:03:16.697275
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 22:33:27.436715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 127.06
 ---- batch: 020 ----
mean loss: 128.39
 ---- batch: 030 ----
mean loss: 127.44
 ---- batch: 040 ----
mean loss: 127.02
train mean loss: 127.70
epoch train time: 0:00:01.784901
elapsed time: 0:03:18.482399
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 22:33:29.221803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.20
 ---- batch: 020 ----
mean loss: 126.11
 ---- batch: 030 ----
mean loss: 123.25
 ---- batch: 040 ----
mean loss: 122.75
train mean loss: 124.53
epoch train time: 0:00:01.683705
elapsed time: 0:03:20.166238
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 22:33:30.905654
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.52
 ---- batch: 020 ----
mean loss: 122.31
 ---- batch: 030 ----
mean loss: 122.75
 ---- batch: 040 ----
mean loss: 135.35
train mean loss: 127.31
epoch train time: 0:00:01.779779
elapsed time: 0:03:21.946177
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 22:33:32.685585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.51
 ---- batch: 020 ----
mean loss: 123.56
 ---- batch: 030 ----
mean loss: 132.75
 ---- batch: 040 ----
mean loss: 128.79
train mean loss: 129.60
epoch train time: 0:00:01.688547
elapsed time: 0:03:23.634882
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 22:33:34.374292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.74
 ---- batch: 020 ----
mean loss: 122.70
 ---- batch: 030 ----
mean loss: 124.24
 ---- batch: 040 ----
mean loss: 120.56
train mean loss: 125.49
epoch train time: 0:00:01.783871
elapsed time: 0:03:25.418882
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 22:33:36.158314
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.33
 ---- batch: 020 ----
mean loss: 123.35
 ---- batch: 030 ----
mean loss: 126.12
 ---- batch: 040 ----
mean loss: 120.82
train mean loss: 124.91
epoch train time: 0:00:01.705202
elapsed time: 0:03:27.124273
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 22:33:37.863701
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.00
 ---- batch: 020 ----
mean loss: 118.72
 ---- batch: 030 ----
mean loss: 123.83
 ---- batch: 040 ----
mean loss: 122.03
train mean loss: 122.79
epoch train time: 0:00:01.677611
elapsed time: 0:03:28.802044
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 22:33:39.541454
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.75
 ---- batch: 020 ----
mean loss: 126.86
 ---- batch: 030 ----
mean loss: 123.15
 ---- batch: 040 ----
mean loss: 125.37
train mean loss: 125.13
epoch train time: 0:00:01.802142
elapsed time: 0:03:30.604327
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 22:33:41.343751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.03
 ---- batch: 020 ----
mean loss: 119.50
 ---- batch: 030 ----
mean loss: 129.77
 ---- batch: 040 ----
mean loss: 124.06
train mean loss: 124.56
epoch train time: 0:00:01.690340
elapsed time: 0:03:32.294816
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 22:33:43.034224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.31
 ---- batch: 020 ----
mean loss: 121.03
 ---- batch: 030 ----
mean loss: 127.63
 ---- batch: 040 ----
mean loss: 121.97
train mean loss: 122.74
epoch train time: 0:00:01.784598
elapsed time: 0:03:34.079574
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 22:33:44.818983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.22
 ---- batch: 020 ----
mean loss: 122.86
 ---- batch: 030 ----
mean loss: 122.56
 ---- batch: 040 ----
mean loss: 120.84
train mean loss: 122.03
epoch train time: 0:00:01.678861
elapsed time: 0:03:35.758578
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 22:33:46.498008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.04
 ---- batch: 020 ----
mean loss: 125.96
 ---- batch: 030 ----
mean loss: 119.25
 ---- batch: 040 ----
mean loss: 123.25
train mean loss: 121.79
epoch train time: 0:00:01.783363
elapsed time: 0:03:37.542088
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 22:33:48.281524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.86
 ---- batch: 020 ----
mean loss: 122.47
 ---- batch: 030 ----
mean loss: 121.57
 ---- batch: 040 ----
mean loss: 122.11
train mean loss: 122.67
epoch train time: 0:00:01.689689
elapsed time: 0:03:39.231948
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 22:33:49.971362
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.30
 ---- batch: 020 ----
mean loss: 124.31
 ---- batch: 030 ----
mean loss: 122.58
 ---- batch: 040 ----
mean loss: 123.67
train mean loss: 124.45
epoch train time: 0:00:01.680168
elapsed time: 0:03:40.912290
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 22:33:51.651699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.84
 ---- batch: 020 ----
mean loss: 118.35
 ---- batch: 030 ----
mean loss: 121.84
 ---- batch: 040 ----
mean loss: 124.46
train mean loss: 121.46
epoch train time: 0:00:01.788534
elapsed time: 0:03:42.700954
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 22:33:53.440367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.30
 ---- batch: 020 ----
mean loss: 125.97
 ---- batch: 030 ----
mean loss: 114.61
 ---- batch: 040 ----
mean loss: 132.06
train mean loss: 125.42
epoch train time: 0:00:01.685219
elapsed time: 0:03:44.386307
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 22:33:55.125719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.61
 ---- batch: 020 ----
mean loss: 136.28
 ---- batch: 030 ----
mean loss: 123.97
 ---- batch: 040 ----
mean loss: 119.73
train mean loss: 125.94
epoch train time: 0:00:01.800545
elapsed time: 0:03:46.187011
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 22:33:56.926420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.29
 ---- batch: 020 ----
mean loss: 124.84
 ---- batch: 030 ----
mean loss: 121.91
 ---- batch: 040 ----
mean loss: 117.78
train mean loss: 121.82
epoch train time: 0:00:01.689533
elapsed time: 0:03:47.876705
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 22:33:58.616120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.42
 ---- batch: 020 ----
mean loss: 119.49
 ---- batch: 030 ----
mean loss: 115.82
 ---- batch: 040 ----
mean loss: 117.34
train mean loss: 118.34
epoch train time: 0:00:01.736020
elapsed time: 0:03:49.612915
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 22:34:00.352341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.72
 ---- batch: 020 ----
mean loss: 120.34
 ---- batch: 030 ----
mean loss: 121.31
 ---- batch: 040 ----
mean loss: 121.99
train mean loss: 119.97
epoch train time: 0:00:01.740956
elapsed time: 0:03:51.354020
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 22:34:02.093433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.85
 ---- batch: 020 ----
mean loss: 124.15
 ---- batch: 030 ----
mean loss: 126.67
 ---- batch: 040 ----
mean loss: 117.14
train mean loss: 121.91
epoch train time: 0:00:01.683666
elapsed time: 0:03:53.037840
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 22:34:03.777240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.12
 ---- batch: 020 ----
mean loss: 120.32
 ---- batch: 030 ----
mean loss: 120.65
 ---- batch: 040 ----
mean loss: 123.39
train mean loss: 121.02
epoch train time: 0:00:01.803674
elapsed time: 0:03:54.841691
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 22:34:05.581103
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.70
 ---- batch: 020 ----
mean loss: 125.62
 ---- batch: 030 ----
mean loss: 121.61
 ---- batch: 040 ----
mean loss: 121.50
train mean loss: 121.38
epoch train time: 0:00:01.687893
elapsed time: 0:03:56.529724
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 22:34:07.269135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.40
 ---- batch: 020 ----
mean loss: 122.06
 ---- batch: 030 ----
mean loss: 119.35
 ---- batch: 040 ----
mean loss: 114.51
train mean loss: 118.70
epoch train time: 0:00:01.791492
elapsed time: 0:03:58.321348
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 22:34:09.060807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.48
 ---- batch: 020 ----
mean loss: 121.29
 ---- batch: 030 ----
mean loss: 123.93
 ---- batch: 040 ----
mean loss: 122.55
train mean loss: 122.32
epoch train time: 0:00:01.694526
elapsed time: 0:04:00.016076
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 22:34:10.755490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.27
 ---- batch: 020 ----
mean loss: 116.48
 ---- batch: 030 ----
mean loss: 120.77
 ---- batch: 040 ----
mean loss: 119.63
train mean loss: 119.06
epoch train time: 0:00:01.676882
elapsed time: 0:04:01.693105
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 22:34:12.432515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.93
 ---- batch: 020 ----
mean loss: 117.30
 ---- batch: 030 ----
mean loss: 118.77
 ---- batch: 040 ----
mean loss: 116.60
train mean loss: 117.80
epoch train time: 0:00:01.780548
elapsed time: 0:04:03.473799
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 22:34:14.213221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.82
 ---- batch: 020 ----
mean loss: 119.16
 ---- batch: 030 ----
mean loss: 117.88
 ---- batch: 040 ----
mean loss: 116.18
train mean loss: 118.91
epoch train time: 0:00:01.679759
elapsed time: 0:04:05.153703
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 22:34:15.893113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.90
 ---- batch: 020 ----
mean loss: 124.28
 ---- batch: 030 ----
mean loss: 121.04
 ---- batch: 040 ----
mean loss: 119.16
train mean loss: 122.81
epoch train time: 0:00:01.803769
elapsed time: 0:04:06.957594
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 22:34:17.696998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.75
 ---- batch: 020 ----
mean loss: 115.49
 ---- batch: 030 ----
mean loss: 114.97
 ---- batch: 040 ----
mean loss: 114.93
train mean loss: 115.32
epoch train time: 0:00:01.688051
elapsed time: 0:04:08.645791
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 22:34:19.385205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.93
 ---- batch: 020 ----
mean loss: 119.03
 ---- batch: 030 ----
mean loss: 114.89
 ---- batch: 040 ----
mean loss: 117.67
train mean loss: 118.20
epoch train time: 0:00:01.788132
elapsed time: 0:04:10.434066
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 22:34:21.173489
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.81
 ---- batch: 020 ----
mean loss: 113.71
 ---- batch: 030 ----
mean loss: 130.39
 ---- batch: 040 ----
mean loss: 119.78
train mean loss: 121.21
epoch train time: 0:00:01.690829
elapsed time: 0:04:12.125073
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 22:34:22.864505
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.82
 ---- batch: 020 ----
mean loss: 123.90
 ---- batch: 030 ----
mean loss: 117.77
 ---- batch: 040 ----
mean loss: 117.21
train mean loss: 118.99
epoch train time: 0:00:01.673218
elapsed time: 0:04:13.798462
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 22:34:24.537872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.87
 ---- batch: 020 ----
mean loss: 121.75
 ---- batch: 030 ----
mean loss: 121.26
 ---- batch: 040 ----
mean loss: 114.87
train mean loss: 119.61
epoch train time: 0:00:01.791758
elapsed time: 0:04:15.590349
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 22:34:26.329769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.83
 ---- batch: 020 ----
mean loss: 120.57
 ---- batch: 030 ----
mean loss: 116.28
 ---- batch: 040 ----
mean loss: 123.30
train mean loss: 120.81
epoch train time: 0:00:01.684139
elapsed time: 0:04:17.274648
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 22:34:28.014058
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.20
 ---- batch: 020 ----
mean loss: 120.02
 ---- batch: 030 ----
mean loss: 114.71
 ---- batch: 040 ----
mean loss: 119.84
train mean loss: 118.12
epoch train time: 0:00:01.793496
elapsed time: 0:04:19.068294
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 22:34:29.807720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.66
 ---- batch: 020 ----
mean loss: 117.98
 ---- batch: 030 ----
mean loss: 119.06
 ---- batch: 040 ----
mean loss: 113.44
train mean loss: 116.07
epoch train time: 0:00:01.682241
elapsed time: 0:04:20.750689
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 22:34:31.490101
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.95
 ---- batch: 020 ----
mean loss: 116.52
 ---- batch: 030 ----
mean loss: 119.45
 ---- batch: 040 ----
mean loss: 119.42
train mean loss: 117.55
epoch train time: 0:00:01.690865
elapsed time: 0:04:22.441696
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 22:34:33.181119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.44
 ---- batch: 020 ----
mean loss: 113.55
 ---- batch: 030 ----
mean loss: 116.18
 ---- batch: 040 ----
mean loss: 111.89
train mean loss: 115.85
epoch train time: 0:00:01.725450
elapsed time: 0:04:24.167320
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 22:34:34.906753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.95
 ---- batch: 020 ----
mean loss: 113.80
 ---- batch: 030 ----
mean loss: 116.34
 ---- batch: 040 ----
mean loss: 108.92
train mean loss: 114.35
epoch train time: 0:00:01.676407
elapsed time: 0:04:25.843890
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 22:34:36.583303
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.66
 ---- batch: 020 ----
mean loss: 120.53
 ---- batch: 030 ----
mean loss: 116.53
 ---- batch: 040 ----
mean loss: 124.33
train mean loss: 119.39
epoch train time: 0:00:01.786821
elapsed time: 0:04:27.630836
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 22:34:38.370240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.73
 ---- batch: 020 ----
mean loss: 117.69
 ---- batch: 030 ----
mean loss: 118.84
 ---- batch: 040 ----
mean loss: 112.66
train mean loss: 118.43
epoch train time: 0:00:01.686743
elapsed time: 0:04:29.317708
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 22:34:40.057134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.55
 ---- batch: 020 ----
mean loss: 116.59
 ---- batch: 030 ----
mean loss: 119.01
 ---- batch: 040 ----
mean loss: 116.19
train mean loss: 119.35
epoch train time: 0:00:01.779939
elapsed time: 0:04:31.097833
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 22:34:41.837233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.14
 ---- batch: 020 ----
mean loss: 126.54
 ---- batch: 030 ----
mean loss: 120.20
 ---- batch: 040 ----
mean loss: 121.26
train mean loss: 123.28
epoch train time: 0:00:01.682488
elapsed time: 0:04:32.780450
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 22:34:43.519860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.24
 ---- batch: 020 ----
mean loss: 120.22
 ---- batch: 030 ----
mean loss: 119.97
 ---- batch: 040 ----
mean loss: 118.62
train mean loss: 119.88
epoch train time: 0:00:01.777772
elapsed time: 0:04:34.558367
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 22:34:45.297786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.75
 ---- batch: 020 ----
mean loss: 113.50
 ---- batch: 030 ----
mean loss: 119.35
 ---- batch: 040 ----
mean loss: 121.27
train mean loss: 118.69
epoch train time: 0:00:01.699909
elapsed time: 0:04:36.258421
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 22:34:46.997832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.54
 ---- batch: 020 ----
mean loss: 124.43
 ---- batch: 030 ----
mean loss: 115.97
 ---- batch: 040 ----
mean loss: 117.57
train mean loss: 119.27
epoch train time: 0:00:01.682005
elapsed time: 0:04:37.940565
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 22:34:48.679975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.26
 ---- batch: 020 ----
mean loss: 115.20
 ---- batch: 030 ----
mean loss: 110.60
 ---- batch: 040 ----
mean loss: 120.55
train mean loss: 115.58
epoch train time: 0:00:01.800570
elapsed time: 0:04:39.741277
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 22:34:50.480707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.21
 ---- batch: 020 ----
mean loss: 125.27
 ---- batch: 030 ----
mean loss: 123.21
 ---- batch: 040 ----
mean loss: 120.35
train mean loss: 120.70
epoch train time: 0:00:01.687245
elapsed time: 0:04:41.428678
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 22:34:52.168092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.14
 ---- batch: 020 ----
mean loss: 119.48
 ---- batch: 030 ----
mean loss: 123.65
 ---- batch: 040 ----
mean loss: 117.90
train mean loss: 119.85
epoch train time: 0:00:01.781980
elapsed time: 0:04:43.210798
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 22:34:53.950203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.52
 ---- batch: 020 ----
mean loss: 118.99
 ---- batch: 030 ----
mean loss: 117.31
 ---- batch: 040 ----
mean loss: 117.72
train mean loss: 117.51
epoch train time: 0:00:01.688050
elapsed time: 0:04:44.898984
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 22:34:55.638399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.80
 ---- batch: 020 ----
mean loss: 121.26
 ---- batch: 030 ----
mean loss: 118.07
 ---- batch: 040 ----
mean loss: 124.92
train mean loss: 120.84
epoch train time: 0:00:01.733912
elapsed time: 0:04:46.633043
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 22:34:57.372456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.39
 ---- batch: 020 ----
mean loss: 118.58
 ---- batch: 030 ----
mean loss: 116.00
 ---- batch: 040 ----
mean loss: 117.01
train mean loss: 118.60
epoch train time: 0:00:01.742607
elapsed time: 0:04:48.375838
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 22:34:59.115251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.26
 ---- batch: 020 ----
mean loss: 117.96
 ---- batch: 030 ----
mean loss: 118.15
 ---- batch: 040 ----
mean loss: 120.14
train mean loss: 117.65
epoch train time: 0:00:01.680060
elapsed time: 0:04:50.056046
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 22:35:00.795471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.43
 ---- batch: 020 ----
mean loss: 115.62
 ---- batch: 030 ----
mean loss: 112.58
 ---- batch: 040 ----
mean loss: 116.96
train mean loss: 114.74
epoch train time: 0:00:01.789925
elapsed time: 0:04:51.846118
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 22:35:02.585528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.89
 ---- batch: 020 ----
mean loss: 116.84
 ---- batch: 030 ----
mean loss: 116.97
 ---- batch: 040 ----
mean loss: 118.72
train mean loss: 117.38
epoch train time: 0:00:01.681936
elapsed time: 0:04:53.528190
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 22:35:04.267601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.17
 ---- batch: 020 ----
mean loss: 122.25
 ---- batch: 030 ----
mean loss: 118.94
 ---- batch: 040 ----
mean loss: 116.87
train mean loss: 118.89
epoch train time: 0:00:01.785307
elapsed time: 0:04:55.313627
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 22:35:06.053035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.88
 ---- batch: 020 ----
mean loss: 115.09
 ---- batch: 030 ----
mean loss: 115.25
 ---- batch: 040 ----
mean loss: 117.37
train mean loss: 116.65
epoch train time: 0:00:01.691396
elapsed time: 0:04:57.005169
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 22:35:07.744593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.64
 ---- batch: 020 ----
mean loss: 118.34
 ---- batch: 030 ----
mean loss: 116.54
 ---- batch: 040 ----
mean loss: 117.95
train mean loss: 116.73
epoch train time: 0:00:01.778276
elapsed time: 0:04:58.783603
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 22:35:09.523026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.68
 ---- batch: 020 ----
mean loss: 119.84
 ---- batch: 030 ----
mean loss: 116.46
 ---- batch: 040 ----
mean loss: 115.02
train mean loss: 117.62
epoch train time: 0:00:01.689444
elapsed time: 0:05:00.473192
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 22:35:11.212615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.34
 ---- batch: 020 ----
mean loss: 118.07
 ---- batch: 030 ----
mean loss: 118.86
 ---- batch: 040 ----
mean loss: 113.37
train mean loss: 116.44
epoch train time: 0:00:01.679630
elapsed time: 0:05:02.153002
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 22:35:12.892433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.12
 ---- batch: 020 ----
mean loss: 118.94
 ---- batch: 030 ----
mean loss: 114.57
 ---- batch: 040 ----
mean loss: 113.32
train mean loss: 115.76
epoch train time: 0:00:01.783094
elapsed time: 0:05:03.936245
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 22:35:14.675652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.26
 ---- batch: 020 ----
mean loss: 113.97
 ---- batch: 030 ----
mean loss: 116.31
 ---- batch: 040 ----
mean loss: 119.95
train mean loss: 117.31
epoch train time: 0:00:01.686456
elapsed time: 0:05:05.622834
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 22:35:16.362247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.19
 ---- batch: 020 ----
mean loss: 119.92
 ---- batch: 030 ----
mean loss: 119.17
 ---- batch: 040 ----
mean loss: 114.78
train mean loss: 119.03
epoch train time: 0:00:01.783550
elapsed time: 0:05:07.406508
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 22:35:18.145913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.74
 ---- batch: 020 ----
mean loss: 120.14
 ---- batch: 030 ----
mean loss: 118.26
 ---- batch: 040 ----
mean loss: 114.86
train mean loss: 116.28
epoch train time: 0:00:01.686703
elapsed time: 0:05:09.093354
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 22:35:19.832794
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.38
 ---- batch: 020 ----
mean loss: 117.38
 ---- batch: 030 ----
mean loss: 116.39
 ---- batch: 040 ----
mean loss: 114.76
train mean loss: 116.88
epoch train time: 0:00:01.674064
elapsed time: 0:05:10.767606
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 22:35:21.507015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.03
 ---- batch: 020 ----
mean loss: 115.86
 ---- batch: 030 ----
mean loss: 114.77
 ---- batch: 040 ----
mean loss: 118.55
train mean loss: 116.56
epoch train time: 0:00:01.788912
elapsed time: 0:05:12.556684
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 22:35:23.296099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.42
 ---- batch: 020 ----
mean loss: 118.21
 ---- batch: 030 ----
mean loss: 120.30
 ---- batch: 040 ----
mean loss: 114.97
train mean loss: 115.87
epoch train time: 0:00:01.683112
elapsed time: 0:05:14.239951
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 22:35:24.979352
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.72
 ---- batch: 020 ----
mean loss: 113.07
 ---- batch: 030 ----
mean loss: 116.30
 ---- batch: 040 ----
mean loss: 111.55
train mean loss: 115.04
epoch train time: 0:00:01.781268
elapsed time: 0:05:16.021345
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 22:35:26.760757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.16
 ---- batch: 020 ----
mean loss: 114.42
 ---- batch: 030 ----
mean loss: 115.00
 ---- batch: 040 ----
mean loss: 115.29
train mean loss: 115.31
epoch train time: 0:00:01.686183
elapsed time: 0:05:17.707685
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 22:35:28.447096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.79
 ---- batch: 020 ----
mean loss: 115.31
 ---- batch: 030 ----
mean loss: 113.89
 ---- batch: 040 ----
mean loss: 112.84
train mean loss: 113.52
epoch train time: 0:00:01.784767
elapsed time: 0:05:19.492593
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 22:35:30.232018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.98
 ---- batch: 020 ----
mean loss: 115.32
 ---- batch: 030 ----
mean loss: 117.15
 ---- batch: 040 ----
mean loss: 116.07
train mean loss: 116.08
epoch train time: 0:00:01.702194
elapsed time: 0:05:21.194946
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 22:35:31.934370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.95
 ---- batch: 020 ----
mean loss: 113.29
 ---- batch: 030 ----
mean loss: 113.92
 ---- batch: 040 ----
mean loss: 113.96
train mean loss: 114.48
epoch train time: 0:00:01.675583
elapsed time: 0:05:22.870687
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 22:35:33.610095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.37
 ---- batch: 020 ----
mean loss: 117.34
 ---- batch: 030 ----
mean loss: 117.70
 ---- batch: 040 ----
mean loss: 115.03
train mean loss: 116.29
epoch train time: 0:00:01.785461
elapsed time: 0:05:24.656271
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 22:35:35.395678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.15
 ---- batch: 020 ----
mean loss: 116.65
 ---- batch: 030 ----
mean loss: 115.01
 ---- batch: 040 ----
mean loss: 113.01
train mean loss: 114.57
epoch train time: 0:00:01.683489
elapsed time: 0:05:26.339924
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 22:35:37.079350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.13
 ---- batch: 020 ----
mean loss: 113.81
 ---- batch: 030 ----
mean loss: 115.55
 ---- batch: 040 ----
mean loss: 115.94
train mean loss: 115.30
epoch train time: 0:00:01.787469
elapsed time: 0:05:28.127544
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 22:35:38.866958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.50
 ---- batch: 020 ----
mean loss: 116.46
 ---- batch: 030 ----
mean loss: 116.85
 ---- batch: 040 ----
mean loss: 118.99
train mean loss: 118.44
epoch train time: 0:00:01.692714
elapsed time: 0:05:29.820398
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 22:35:40.559831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.76
 ---- batch: 020 ----
mean loss: 116.15
 ---- batch: 030 ----
mean loss: 120.93
 ---- batch: 040 ----
mean loss: 116.15
train mean loss: 115.97
epoch train time: 0:00:01.677465
elapsed time: 0:05:31.498009
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 22:35:42.237415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.99
 ---- batch: 020 ----
mean loss: 117.41
 ---- batch: 030 ----
mean loss: 113.27
 ---- batch: 040 ----
mean loss: 118.21
train mean loss: 115.48
epoch train time: 0:00:01.790515
elapsed time: 0:05:33.288662
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 22:35:44.028076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.23
 ---- batch: 020 ----
mean loss: 113.42
 ---- batch: 030 ----
mean loss: 121.98
 ---- batch: 040 ----
mean loss: 114.95
train mean loss: 115.45
epoch train time: 0:00:01.681250
elapsed time: 0:05:34.970083
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 22:35:45.709496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.43
 ---- batch: 020 ----
mean loss: 115.40
 ---- batch: 030 ----
mean loss: 113.83
 ---- batch: 040 ----
mean loss: 115.73
train mean loss: 114.00
epoch train time: 0:00:01.785985
elapsed time: 0:05:36.756201
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 22:35:47.495640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.76
 ---- batch: 020 ----
mean loss: 114.56
 ---- batch: 030 ----
mean loss: 112.44
 ---- batch: 040 ----
mean loss: 120.13
train mean loss: 116.51
epoch train time: 0:00:01.685921
elapsed time: 0:05:38.442317
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 22:35:49.181727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.62
 ---- batch: 020 ----
mean loss: 114.96
 ---- batch: 030 ----
mean loss: 115.36
 ---- batch: 040 ----
mean loss: 116.44
train mean loss: 114.74
epoch train time: 0:00:01.781655
elapsed time: 0:05:40.224099
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 22:35:50.963520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 112.11
 ---- batch: 020 ----
mean loss: 113.87
 ---- batch: 030 ----
mean loss: 111.67
 ---- batch: 040 ----
mean loss: 113.45
train mean loss: 113.02
epoch train time: 0:00:01.696001
elapsed time: 0:05:41.920246
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 22:35:52.659659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.97
 ---- batch: 020 ----
mean loss: 121.90
 ---- batch: 030 ----
mean loss: 123.04
 ---- batch: 040 ----
mean loss: 120.78
train mean loss: 119.49
epoch train time: 0:00:01.677442
elapsed time: 0:05:43.597827
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 22:35:54.337238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 120.49
 ---- batch: 020 ----
mean loss: 116.74
 ---- batch: 030 ----
mean loss: 116.45
 ---- batch: 040 ----
mean loss: 116.98
train mean loss: 117.93
epoch train time: 0:00:01.776622
elapsed time: 0:05:45.374592
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 22:35:56.114000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 116.77
 ---- batch: 020 ----
mean loss: 110.76
 ---- batch: 030 ----
mean loss: 111.12
 ---- batch: 040 ----
mean loss: 116.76
train mean loss: 114.15
epoch train time: 0:00:01.682660
elapsed time: 0:05:47.057392
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 22:35:57.796814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.19
 ---- batch: 020 ----
mean loss: 113.32
 ---- batch: 030 ----
mean loss: 114.52
 ---- batch: 040 ----
mean loss: 113.68
train mean loss: 113.59
epoch train time: 0:00:01.784918
elapsed time: 0:05:48.842445
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 22:35:59.581854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.53
 ---- batch: 020 ----
mean loss: 115.79
 ---- batch: 030 ----
mean loss: 112.02
 ---- batch: 040 ----
mean loss: 109.02
train mean loss: 111.63
epoch train time: 0:00:01.691609
elapsed time: 0:05:50.534183
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 22:36:01.273591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 106.68
 ---- batch: 020 ----
mean loss: 115.97
 ---- batch: 030 ----
mean loss: 112.14
 ---- batch: 040 ----
mean loss: 112.70
train mean loss: 111.69
epoch train time: 0:00:01.780153
elapsed time: 0:05:52.314478
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 22:36:03.053888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 111.97
 ---- batch: 020 ----
mean loss: 114.25
 ---- batch: 030 ----
mean loss: 110.54
 ---- batch: 040 ----
mean loss: 114.95
train mean loss: 112.88
epoch train time: 0:00:01.689694
elapsed time: 0:05:54.004304
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 22:36:04.743716
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 115.68
 ---- batch: 020 ----
mean loss: 113.75
 ---- batch: 030 ----
mean loss: 113.59
 ---- batch: 040 ----
mean loss: 113.67
train mean loss: 114.29
epoch train time: 0:00:01.683846
elapsed time: 0:05:55.688292
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 22:36:06.427704
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 114.66
 ---- batch: 020 ----
mean loss: 117.74
 ---- batch: 030 ----
mean loss: 116.80
 ---- batch: 040 ----
mean loss: 113.72
train mean loss: 115.50
epoch train time: 0:00:01.711473
elapsed time: 0:05:57.399898
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 22:36:08.139306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 110.44
 ---- batch: 020 ----
mean loss: 113.47
 ---- batch: 030 ----
mean loss: 117.82
 ---- batch: 040 ----
mean loss: 113.36
train mean loss: 114.54
epoch train time: 0:00:01.682480
elapsed time: 0:05:59.082511
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 22:36:09.821923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.34
 ---- batch: 020 ----
mean loss: 112.52
 ---- batch: 030 ----
mean loss: 119.11
 ---- batch: 040 ----
mean loss: 115.75
train mean loss: 116.65
epoch train time: 0:00:01.793865
elapsed time: 0:06:00.876525
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 22:36:11.615968
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.03
 ---- batch: 020 ----
mean loss: 113.18
 ---- batch: 030 ----
mean loss: 110.68
 ---- batch: 040 ----
mean loss: 111.80
train mean loss: 112.62
epoch train time: 0:00:01.694729
elapsed time: 0:06:02.571446
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 22:36:13.310849
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.94
 ---- batch: 020 ----
mean loss: 110.42
 ---- batch: 030 ----
mean loss: 113.56
 ---- batch: 040 ----
mean loss: 108.14
train mean loss: 111.00
epoch train time: 0:00:01.678128
elapsed time: 0:06:04.249715
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 22:36:14.989122
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.59
 ---- batch: 020 ----
mean loss: 114.60
 ---- batch: 030 ----
mean loss: 110.13
 ---- batch: 040 ----
mean loss: 107.13
train mean loss: 110.45
epoch train time: 0:00:01.778701
elapsed time: 0:06:06.028560
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 22:36:16.767970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.14
 ---- batch: 020 ----
mean loss: 113.81
 ---- batch: 030 ----
mean loss: 109.29
 ---- batch: 040 ----
mean loss: 114.44
train mean loss: 111.90
epoch train time: 0:00:01.687008
elapsed time: 0:06:07.715721
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 22:36:18.455147
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.96
 ---- batch: 020 ----
mean loss: 108.80
 ---- batch: 030 ----
mean loss: 109.00
 ---- batch: 040 ----
mean loss: 107.44
train mean loss: 109.81
epoch train time: 0:00:01.786768
elapsed time: 0:06:09.502654
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 22:36:20.242068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.17
 ---- batch: 020 ----
mean loss: 110.54
 ---- batch: 030 ----
mean loss: 111.80
 ---- batch: 040 ----
mean loss: 107.86
train mean loss: 109.88
epoch train time: 0:00:01.695049
elapsed time: 0:06:11.197861
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 22:36:21.937268
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.96
 ---- batch: 020 ----
mean loss: 111.29
 ---- batch: 030 ----
mean loss: 107.57
 ---- batch: 040 ----
mean loss: 109.96
train mean loss: 110.54
epoch train time: 0:00:01.784012
elapsed time: 0:06:12.981996
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 22:36:23.721405
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.61
 ---- batch: 020 ----
mean loss: 110.94
 ---- batch: 030 ----
mean loss: 108.92
 ---- batch: 040 ----
mean loss: 112.04
train mean loss: 110.48
epoch train time: 0:00:01.683316
elapsed time: 0:06:14.665451
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 22:36:25.404877
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.88
 ---- batch: 020 ----
mean loss: 114.78
 ---- batch: 030 ----
mean loss: 109.07
 ---- batch: 040 ----
mean loss: 114.62
train mean loss: 111.68
epoch train time: 0:00:01.681443
elapsed time: 0:06:16.347057
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 22:36:27.086465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.67
 ---- batch: 020 ----
mean loss: 113.82
 ---- batch: 030 ----
mean loss: 111.47
 ---- batch: 040 ----
mean loss: 110.16
train mean loss: 111.33
epoch train time: 0:00:01.794006
elapsed time: 0:06:18.141217
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 22:36:28.880625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.18
 ---- batch: 020 ----
mean loss: 112.92
 ---- batch: 030 ----
mean loss: 107.66
 ---- batch: 040 ----
mean loss: 109.89
train mean loss: 110.86
epoch train time: 0:00:01.690178
elapsed time: 0:06:19.831526
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 22:36:30.570938
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.24
 ---- batch: 020 ----
mean loss: 107.05
 ---- batch: 030 ----
mean loss: 110.86
 ---- batch: 040 ----
mean loss: 110.24
train mean loss: 110.23
epoch train time: 0:00:01.786650
elapsed time: 0:06:21.618303
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 22:36:32.357723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.46
 ---- batch: 020 ----
mean loss: 104.85
 ---- batch: 030 ----
mean loss: 113.12
 ---- batch: 040 ----
mean loss: 112.69
train mean loss: 110.51
epoch train time: 0:00:01.696565
elapsed time: 0:06:23.315016
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 22:36:34.054427
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.49
 ---- batch: 020 ----
mean loss: 112.00
 ---- batch: 030 ----
mean loss: 110.07
 ---- batch: 040 ----
mean loss: 108.72
train mean loss: 109.83
epoch train time: 0:00:01.782560
elapsed time: 0:06:25.097704
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 22:36:35.837126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.30
 ---- batch: 020 ----
mean loss: 109.65
 ---- batch: 030 ----
mean loss: 110.50
 ---- batch: 040 ----
mean loss: 109.35
train mean loss: 110.34
epoch train time: 0:00:01.690520
elapsed time: 0:06:26.788379
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 22:36:37.527794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.26
 ---- batch: 020 ----
mean loss: 112.25
 ---- batch: 030 ----
mean loss: 111.45
 ---- batch: 040 ----
mean loss: 106.26
train mean loss: 110.43
epoch train time: 0:00:01.791323
elapsed time: 0:06:28.579845
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 22:36:39.319265
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.35
 ---- batch: 020 ----
mean loss: 108.32
 ---- batch: 030 ----
mean loss: 109.36
 ---- batch: 040 ----
mean loss: 109.69
train mean loss: 109.46
epoch train time: 0:00:01.698846
elapsed time: 0:06:30.278859
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 22:36:41.018273
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.01
 ---- batch: 020 ----
mean loss: 111.66
 ---- batch: 030 ----
mean loss: 107.73
 ---- batch: 040 ----
mean loss: 110.70
train mean loss: 110.00
epoch train time: 0:00:01.756944
elapsed time: 0:06:32.035961
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 22:36:42.775368
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.74
 ---- batch: 020 ----
mean loss: 108.27
 ---- batch: 030 ----
mean loss: 111.70
 ---- batch: 040 ----
mean loss: 111.79
train mean loss: 110.51
epoch train time: 0:00:01.716785
elapsed time: 0:06:33.752873
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 22:36:44.492282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.76
 ---- batch: 020 ----
mean loss: 110.74
 ---- batch: 030 ----
mean loss: 110.11
 ---- batch: 040 ----
mean loss: 114.57
train mean loss: 110.93
epoch train time: 0:00:01.682795
elapsed time: 0:06:35.435829
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 22:36:46.175238
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.20
 ---- batch: 020 ----
mean loss: 109.67
 ---- batch: 030 ----
mean loss: 107.39
 ---- batch: 040 ----
mean loss: 111.92
train mean loss: 109.94
epoch train time: 0:00:01.785535
elapsed time: 0:06:37.221490
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 22:36:47.960906
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.67
 ---- batch: 020 ----
mean loss: 109.46
 ---- batch: 030 ----
mean loss: 107.53
 ---- batch: 040 ----
mean loss: 108.61
train mean loss: 110.02
epoch train time: 0:00:01.689640
elapsed time: 0:06:38.911265
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 22:36:49.650675
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.27
 ---- batch: 020 ----
mean loss: 110.53
 ---- batch: 030 ----
mean loss: 111.82
 ---- batch: 040 ----
mean loss: 108.65
train mean loss: 110.77
epoch train time: 0:00:01.789369
elapsed time: 0:06:40.700773
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 22:36:51.440181
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.74
 ---- batch: 020 ----
mean loss: 112.36
 ---- batch: 030 ----
mean loss: 110.05
 ---- batch: 040 ----
mean loss: 110.55
train mean loss: 111.21
epoch train time: 0:00:01.684372
elapsed time: 0:06:42.385282
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 22:36:53.124709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.18
 ---- batch: 020 ----
mean loss: 110.05
 ---- batch: 030 ----
mean loss: 111.59
 ---- batch: 040 ----
mean loss: 113.04
train mean loss: 110.86
epoch train time: 0:00:01.792290
elapsed time: 0:06:44.177731
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 22:36:54.917140
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.61
 ---- batch: 020 ----
mean loss: 108.66
 ---- batch: 030 ----
mean loss: 112.24
 ---- batch: 040 ----
mean loss: 112.49
train mean loss: 111.29
epoch train time: 0:00:01.682864
elapsed time: 0:06:45.860729
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 22:36:56.600156
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.42
 ---- batch: 020 ----
mean loss: 110.89
 ---- batch: 030 ----
mean loss: 109.68
 ---- batch: 040 ----
mean loss: 108.86
train mean loss: 110.04
epoch train time: 0:00:01.792201
elapsed time: 0:06:47.653123
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 22:36:58.392534
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.87
 ---- batch: 020 ----
mean loss: 106.96
 ---- batch: 030 ----
mean loss: 111.55
 ---- batch: 040 ----
mean loss: 111.47
train mean loss: 108.95
epoch train time: 0:00:01.679411
elapsed time: 0:06:49.332664
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 22:37:00.072071
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.87
 ---- batch: 020 ----
mean loss: 109.85
 ---- batch: 030 ----
mean loss: 110.11
 ---- batch: 040 ----
mean loss: 110.52
train mean loss: 109.57
epoch train time: 0:00:01.780753
elapsed time: 0:06:51.113556
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 22:37:01.852970
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.48
 ---- batch: 020 ----
mean loss: 111.69
 ---- batch: 030 ----
mean loss: 110.72
 ---- batch: 040 ----
mean loss: 108.25
train mean loss: 109.49
epoch train time: 0:00:01.691538
elapsed time: 0:06:52.805266
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 22:37:03.544694
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.54
 ---- batch: 020 ----
mean loss: 107.64
 ---- batch: 030 ----
mean loss: 113.38
 ---- batch: 040 ----
mean loss: 112.43
train mean loss: 110.20
epoch train time: 0:00:01.786738
elapsed time: 0:06:54.592147
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 22:37:05.331555
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.49
 ---- batch: 020 ----
mean loss: 113.33
 ---- batch: 030 ----
mean loss: 109.34
 ---- batch: 040 ----
mean loss: 108.25
train mean loss: 110.59
epoch train time: 0:00:01.694329
elapsed time: 0:06:56.286632
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 22:37:07.026065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.36
 ---- batch: 020 ----
mean loss: 108.72
 ---- batch: 030 ----
mean loss: 111.10
 ---- batch: 040 ----
mean loss: 109.75
train mean loss: 109.54
epoch train time: 0:00:01.749390
elapsed time: 0:06:58.036199
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 22:37:08.775606
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.62
 ---- batch: 020 ----
mean loss: 113.62
 ---- batch: 030 ----
mean loss: 109.40
 ---- batch: 040 ----
mean loss: 108.81
train mean loss: 111.07
epoch train time: 0:00:01.728379
elapsed time: 0:06:59.764710
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 22:37:10.504120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.87
 ---- batch: 020 ----
mean loss: 111.24
 ---- batch: 030 ----
mean loss: 108.04
 ---- batch: 040 ----
mean loss: 111.81
train mean loss: 110.21
epoch train time: 0:00:01.674587
elapsed time: 0:07:01.439430
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 22:37:12.178839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.25
 ---- batch: 020 ----
mean loss: 107.77
 ---- batch: 030 ----
mean loss: 110.46
 ---- batch: 040 ----
mean loss: 108.32
train mean loss: 109.22
epoch train time: 0:00:01.774658
elapsed time: 0:07:03.214268
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 22:37:13.953692
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.07
 ---- batch: 020 ----
mean loss: 108.13
 ---- batch: 030 ----
mean loss: 108.52
 ---- batch: 040 ----
mean loss: 111.87
train mean loss: 111.03
epoch train time: 0:00:01.692012
elapsed time: 0:07:04.906449
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 22:37:15.645859
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.03
 ---- batch: 020 ----
mean loss: 109.33
 ---- batch: 030 ----
mean loss: 112.62
 ---- batch: 040 ----
mean loss: 110.45
train mean loss: 110.05
epoch train time: 0:00:01.714935
elapsed time: 0:07:06.621509
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 22:37:17.360917
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.08
 ---- batch: 020 ----
mean loss: 110.22
 ---- batch: 030 ----
mean loss: 110.17
 ---- batch: 040 ----
mean loss: 110.24
train mean loss: 110.72
epoch train time: 0:00:01.688308
elapsed time: 0:07:08.309963
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 22:37:19.049371
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.99
 ---- batch: 020 ----
mean loss: 110.50
 ---- batch: 030 ----
mean loss: 106.23
 ---- batch: 040 ----
mean loss: 112.03
train mean loss: 109.39
epoch train time: 0:00:01.791656
elapsed time: 0:07:10.101753
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 22:37:20.841161
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.15
 ---- batch: 020 ----
mean loss: 111.13
 ---- batch: 030 ----
mean loss: 110.64
 ---- batch: 040 ----
mean loss: 110.88
train mean loss: 109.72
epoch train time: 0:00:01.682300
elapsed time: 0:07:11.784181
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 22:37:22.523592
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.01
 ---- batch: 020 ----
mean loss: 110.87
 ---- batch: 030 ----
mean loss: 112.80
 ---- batch: 040 ----
mean loss: 108.09
train mean loss: 110.40
epoch train time: 0:00:01.671663
elapsed time: 0:07:13.456002
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 22:37:24.195442
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.88
 ---- batch: 020 ----
mean loss: 109.01
 ---- batch: 030 ----
mean loss: 111.92
 ---- batch: 040 ----
mean loss: 108.93
train mean loss: 110.15
epoch train time: 0:00:01.671927
elapsed time: 0:07:15.128090
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 22:37:25.867501
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.94
 ---- batch: 020 ----
mean loss: 112.26
 ---- batch: 030 ----
mean loss: 110.15
 ---- batch: 040 ----
mean loss: 109.16
train mean loss: 111.07
epoch train time: 0:00:01.659144
elapsed time: 0:07:16.787386
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 22:37:27.526811
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.30
 ---- batch: 020 ----
mean loss: 113.82
 ---- batch: 030 ----
mean loss: 108.38
 ---- batch: 040 ----
mean loss: 109.77
train mean loss: 110.03
epoch train time: 0:00:01.674701
elapsed time: 0:07:18.462229
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 22:37:29.201639
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 109.56
 ---- batch: 020 ----
mean loss: 112.53
 ---- batch: 030 ----
mean loss: 111.93
 ---- batch: 040 ----
mean loss: 107.88
train mean loss: 110.27
epoch train time: 0:00:01.657878
elapsed time: 0:07:20.120239
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 22:37:30.859650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.26
 ---- batch: 020 ----
mean loss: 111.62
 ---- batch: 030 ----
mean loss: 108.59
 ---- batch: 040 ----
mean loss: 107.71
train mean loss: 109.26
epoch train time: 0:00:01.654697
elapsed time: 0:07:21.775067
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 22:37:32.514478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 108.79
 ---- batch: 020 ----
mean loss: 110.44
 ---- batch: 030 ----
mean loss: 111.87
 ---- batch: 040 ----
mean loss: 106.84
train mean loss: 110.04
epoch train time: 0:00:01.652856
elapsed time: 0:07:23.428074
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 22:37:34.167502
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.30
 ---- batch: 020 ----
mean loss: 107.26
 ---- batch: 030 ----
mean loss: 109.79
 ---- batch: 040 ----
mean loss: 106.75
train mean loss: 109.58
epoch train time: 0:00:01.651975
elapsed time: 0:07:25.083533
checkpoint saved in file: log/CMAPSS/FD003/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_3/checkpoint.pth.tar
**** end time: 2019-09-20 22:37:35.822913 ****
