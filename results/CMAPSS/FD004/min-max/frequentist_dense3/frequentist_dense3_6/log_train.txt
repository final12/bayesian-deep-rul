Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_6', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 18244
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:22:48.817282 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:22:48.821098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4916.89
 ---- batch: 020 ----
mean loss: 4796.89
 ---- batch: 030 ----
mean loss: 4684.62
 ---- batch: 040 ----
mean loss: 4572.00
 ---- batch: 050 ----
mean loss: 4499.90
 ---- batch: 060 ----
mean loss: 4390.18
 ---- batch: 070 ----
mean loss: 4352.03
 ---- batch: 080 ----
mean loss: 4276.51
 ---- batch: 090 ----
mean loss: 4204.57
 ---- batch: 100 ----
mean loss: 4170.62
 ---- batch: 110 ----
mean loss: 4130.82
train mean loss: 4443.58
epoch train time: 0:00:33.056053
elapsed time: 0:00:33.062017
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:23:21.879355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4008.39
 ---- batch: 020 ----
mean loss: 3915.06
 ---- batch: 030 ----
mean loss: 3886.10
 ---- batch: 040 ----
mean loss: 3820.22
 ---- batch: 050 ----
mean loss: 3798.29
 ---- batch: 060 ----
mean loss: 3706.98
 ---- batch: 070 ----
mean loss: 3614.85
 ---- batch: 080 ----
mean loss: 3601.60
 ---- batch: 090 ----
mean loss: 3514.61
 ---- batch: 100 ----
mean loss: 3425.90
 ---- batch: 110 ----
mean loss: 3342.63
train mean loss: 3686.36
epoch train time: 0:00:00.584761
elapsed time: 0:00:33.646922
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:23:22.464254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3318.19
 ---- batch: 020 ----
mean loss: 3260.29
 ---- batch: 030 ----
mean loss: 3231.73
 ---- batch: 040 ----
mean loss: 3172.37
 ---- batch: 050 ----
mean loss: 3109.81
 ---- batch: 060 ----
mean loss: 3046.24
 ---- batch: 070 ----
mean loss: 3017.42
 ---- batch: 080 ----
mean loss: 2931.32
 ---- batch: 090 ----
mean loss: 2870.18
 ---- batch: 100 ----
mean loss: 2864.18
 ---- batch: 110 ----
mean loss: 2739.72
train mean loss: 3044.76
epoch train time: 0:00:00.569408
elapsed time: 0:00:34.216466
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:23:23.033811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2723.38
 ---- batch: 020 ----
mean loss: 2639.07
 ---- batch: 030 ----
mean loss: 2665.79
 ---- batch: 040 ----
mean loss: 2624.89
 ---- batch: 050 ----
mean loss: 2542.10
 ---- batch: 060 ----
mean loss: 2529.93
 ---- batch: 070 ----
mean loss: 2480.28
 ---- batch: 080 ----
mean loss: 2473.96
 ---- batch: 090 ----
mean loss: 2378.97
 ---- batch: 100 ----
mean loss: 2359.71
 ---- batch: 110 ----
mean loss: 2317.62
train mean loss: 2515.27
epoch train time: 0:00:00.558503
elapsed time: 0:00:34.775116
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:23:23.592464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2302.15
 ---- batch: 020 ----
mean loss: 2238.33
 ---- batch: 030 ----
mean loss: 2187.31
 ---- batch: 040 ----
mean loss: 2148.12
 ---- batch: 050 ----
mean loss: 2126.14
 ---- batch: 060 ----
mean loss: 2077.96
 ---- batch: 070 ----
mean loss: 2025.81
 ---- batch: 080 ----
mean loss: 1989.47
 ---- batch: 090 ----
mean loss: 1980.81
 ---- batch: 100 ----
mean loss: 1964.19
 ---- batch: 110 ----
mean loss: 1942.45
train mean loss: 2083.92
epoch train time: 0:00:00.576716
elapsed time: 0:00:35.351986
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:23:24.169317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1898.49
 ---- batch: 020 ----
mean loss: 1865.04
 ---- batch: 030 ----
mean loss: 1830.68
 ---- batch: 040 ----
mean loss: 1832.67
 ---- batch: 050 ----
mean loss: 1757.83
 ---- batch: 060 ----
mean loss: 1732.20
 ---- batch: 070 ----
mean loss: 1698.45
 ---- batch: 080 ----
mean loss: 1670.43
 ---- batch: 090 ----
mean loss: 1659.62
 ---- batch: 100 ----
mean loss: 1653.52
 ---- batch: 110 ----
mean loss: 1615.47
train mean loss: 1743.42
epoch train time: 0:00:00.566193
elapsed time: 0:00:35.918317
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:23:24.735668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1593.69
 ---- batch: 020 ----
mean loss: 1562.67
 ---- batch: 030 ----
mean loss: 1532.83
 ---- batch: 040 ----
mean loss: 1523.87
 ---- batch: 050 ----
mean loss: 1535.59
 ---- batch: 060 ----
mean loss: 1456.09
 ---- batch: 070 ----
mean loss: 1450.12
 ---- batch: 080 ----
mean loss: 1430.71
 ---- batch: 090 ----
mean loss: 1426.90
 ---- batch: 100 ----
mean loss: 1393.26
 ---- batch: 110 ----
mean loss: 1409.92
train mean loss: 1480.43
epoch train time: 0:00:00.560317
elapsed time: 0:00:36.478794
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:23:25.296149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1349.65
 ---- batch: 020 ----
mean loss: 1358.75
 ---- batch: 030 ----
mean loss: 1321.12
 ---- batch: 040 ----
mean loss: 1302.90
 ---- batch: 050 ----
mean loss: 1301.76
 ---- batch: 060 ----
mean loss: 1286.69
 ---- batch: 070 ----
mean loss: 1267.64
 ---- batch: 080 ----
mean loss: 1256.84
 ---- batch: 090 ----
mean loss: 1260.54
 ---- batch: 100 ----
mean loss: 1246.85
 ---- batch: 110 ----
mean loss: 1194.72
train mean loss: 1284.42
epoch train time: 0:00:00.553732
elapsed time: 0:00:37.032696
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:23:25.850022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1185.42
 ---- batch: 020 ----
mean loss: 1192.04
 ---- batch: 030 ----
mean loss: 1161.46
 ---- batch: 040 ----
mean loss: 1143.66
 ---- batch: 050 ----
mean loss: 1148.24
 ---- batch: 060 ----
mean loss: 1144.52
 ---- batch: 070 ----
mean loss: 1133.60
 ---- batch: 080 ----
mean loss: 1116.24
 ---- batch: 090 ----
mean loss: 1102.88
 ---- batch: 100 ----
mean loss: 1105.98
 ---- batch: 110 ----
mean loss: 1098.84
train mean loss: 1137.11
epoch train time: 0:00:00.559265
elapsed time: 0:00:37.592090
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:23:26.409421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1075.26
 ---- batch: 020 ----
mean loss: 1075.06
 ---- batch: 030 ----
mean loss: 1047.40
 ---- batch: 040 ----
mean loss: 1043.00
 ---- batch: 050 ----
mean loss: 1036.70
 ---- batch: 060 ----
mean loss: 1037.83
 ---- batch: 070 ----
mean loss: 1032.64
 ---- batch: 080 ----
mean loss: 1026.23
 ---- batch: 090 ----
mean loss: 1014.86
 ---- batch: 100 ----
mean loss: 992.58
 ---- batch: 110 ----
mean loss: 1008.95
train mean loss: 1034.49
epoch train time: 0:00:00.563316
elapsed time: 0:00:38.155546
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:23:26.972878
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 988.19
 ---- batch: 020 ----
mean loss: 977.50
 ---- batch: 030 ----
mean loss: 979.52
 ---- batch: 040 ----
mean loss: 977.46
 ---- batch: 050 ----
mean loss: 965.45
 ---- batch: 060 ----
mean loss: 960.17
 ---- batch: 070 ----
mean loss: 946.06
 ---- batch: 080 ----
mean loss: 946.52
 ---- batch: 090 ----
mean loss: 951.86
 ---- batch: 100 ----
mean loss: 945.35
 ---- batch: 110 ----
mean loss: 930.69
train mean loss: 960.07
epoch train time: 0:00:00.573181
elapsed time: 0:00:38.728884
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:23:27.546266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 942.04
 ---- batch: 020 ----
mean loss: 918.07
 ---- batch: 030 ----
mean loss: 922.78
 ---- batch: 040 ----
mean loss: 912.26
 ---- batch: 050 ----
mean loss: 901.36
 ---- batch: 060 ----
mean loss: 905.38
 ---- batch: 070 ----
mean loss: 908.52
 ---- batch: 080 ----
mean loss: 898.75
 ---- batch: 090 ----
mean loss: 904.10
 ---- batch: 100 ----
mean loss: 897.46
 ---- batch: 110 ----
mean loss: 879.67
train mean loss: 908.23
epoch train time: 0:00:00.580219
elapsed time: 0:00:39.309294
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:23:28.126670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.00
 ---- batch: 020 ----
mean loss: 895.26
 ---- batch: 030 ----
mean loss: 889.96
 ---- batch: 040 ----
mean loss: 879.34
 ---- batch: 050 ----
mean loss: 875.21
 ---- batch: 060 ----
mean loss: 862.41
 ---- batch: 070 ----
mean loss: 885.99
 ---- batch: 080 ----
mean loss: 857.93
 ---- batch: 090 ----
mean loss: 868.67
 ---- batch: 100 ----
mean loss: 880.69
 ---- batch: 110 ----
mean loss: 855.16
train mean loss: 877.40
epoch train time: 0:00:00.554802
elapsed time: 0:00:39.864275
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:23:28.681624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.76
 ---- batch: 020 ----
mean loss: 862.10
 ---- batch: 030 ----
mean loss: 859.70
 ---- batch: 040 ----
mean loss: 851.03
 ---- batch: 050 ----
mean loss: 854.58
 ---- batch: 060 ----
mean loss: 868.87
 ---- batch: 070 ----
mean loss: 861.04
 ---- batch: 080 ----
mean loss: 864.74
 ---- batch: 090 ----
mean loss: 852.10
 ---- batch: 100 ----
mean loss: 862.74
 ---- batch: 110 ----
mean loss: 862.34
train mean loss: 859.98
epoch train time: 0:00:00.560239
elapsed time: 0:00:40.424681
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:23:29.242010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.40
 ---- batch: 020 ----
mean loss: 850.09
 ---- batch: 030 ----
mean loss: 866.93
 ---- batch: 040 ----
mean loss: 864.30
 ---- batch: 050 ----
mean loss: 852.62
 ---- batch: 060 ----
mean loss: 843.39
 ---- batch: 070 ----
mean loss: 844.32
 ---- batch: 080 ----
mean loss: 839.74
 ---- batch: 090 ----
mean loss: 848.80
 ---- batch: 100 ----
mean loss: 842.60
 ---- batch: 110 ----
mean loss: 864.27
train mean loss: 851.30
epoch train time: 0:00:00.567888
elapsed time: 0:00:40.992730
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:23:29.810071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.13
 ---- batch: 020 ----
mean loss: 854.88
 ---- batch: 030 ----
mean loss: 845.01
 ---- batch: 040 ----
mean loss: 832.62
 ---- batch: 050 ----
mean loss: 843.56
 ---- batch: 060 ----
mean loss: 851.59
 ---- batch: 070 ----
mean loss: 860.99
 ---- batch: 080 ----
mean loss: 835.13
 ---- batch: 090 ----
mean loss: 839.97
 ---- batch: 100 ----
mean loss: 855.79
 ---- batch: 110 ----
mean loss: 834.31
train mean loss: 847.19
epoch train time: 0:00:00.569101
elapsed time: 0:00:41.561991
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:23:30.379321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.18
 ---- batch: 020 ----
mean loss: 816.82
 ---- batch: 030 ----
mean loss: 843.41
 ---- batch: 040 ----
mean loss: 864.07
 ---- batch: 050 ----
mean loss: 865.63
 ---- batch: 060 ----
mean loss: 859.94
 ---- batch: 070 ----
mean loss: 855.40
 ---- batch: 080 ----
mean loss: 845.73
 ---- batch: 090 ----
mean loss: 831.10
 ---- batch: 100 ----
mean loss: 837.68
 ---- batch: 110 ----
mean loss: 837.27
train mean loss: 845.26
epoch train time: 0:00:00.565510
elapsed time: 0:00:42.127647
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:23:30.944982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 823.45
 ---- batch: 020 ----
mean loss: 848.46
 ---- batch: 030 ----
mean loss: 832.05
 ---- batch: 040 ----
mean loss: 845.60
 ---- batch: 050 ----
mean loss: 860.77
 ---- batch: 060 ----
mean loss: 823.40
 ---- batch: 070 ----
mean loss: 858.73
 ---- batch: 080 ----
mean loss: 840.58
 ---- batch: 090 ----
mean loss: 840.99
 ---- batch: 100 ----
mean loss: 860.16
 ---- batch: 110 ----
mean loss: 856.35
train mean loss: 844.34
epoch train time: 0:00:00.562285
elapsed time: 0:00:42.690072
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:23:31.507440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.37
 ---- batch: 020 ----
mean loss: 855.48
 ---- batch: 030 ----
mean loss: 842.81
 ---- batch: 040 ----
mean loss: 821.38
 ---- batch: 050 ----
mean loss: 834.98
 ---- batch: 060 ----
mean loss: 852.01
 ---- batch: 070 ----
mean loss: 841.16
 ---- batch: 080 ----
mean loss: 841.19
 ---- batch: 090 ----
mean loss: 852.40
 ---- batch: 100 ----
mean loss: 839.93
 ---- batch: 110 ----
mean loss: 862.97
train mean loss: 843.97
epoch train time: 0:00:00.568288
elapsed time: 0:00:43.258535
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:23:32.075866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.71
 ---- batch: 020 ----
mean loss: 852.67
 ---- batch: 030 ----
mean loss: 854.68
 ---- batch: 040 ----
mean loss: 838.23
 ---- batch: 050 ----
mean loss: 828.07
 ---- batch: 060 ----
mean loss: 851.58
 ---- batch: 070 ----
mean loss: 842.82
 ---- batch: 080 ----
mean loss: 849.05
 ---- batch: 090 ----
mean loss: 837.62
 ---- batch: 100 ----
mean loss: 845.75
 ---- batch: 110 ----
mean loss: 850.97
train mean loss: 843.94
epoch train time: 0:00:00.564674
elapsed time: 0:00:43.823344
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:23:32.640674
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 807.33
 ---- batch: 020 ----
mean loss: 881.35
 ---- batch: 030 ----
mean loss: 843.18
 ---- batch: 040 ----
mean loss: 870.07
 ---- batch: 050 ----
mean loss: 847.85
 ---- batch: 060 ----
mean loss: 859.64
 ---- batch: 070 ----
mean loss: 840.69
 ---- batch: 080 ----
mean loss: 857.06
 ---- batch: 090 ----
mean loss: 829.69
 ---- batch: 100 ----
mean loss: 825.85
 ---- batch: 110 ----
mean loss: 827.43
train mean loss: 843.97
epoch train time: 0:00:00.561455
elapsed time: 0:00:44.384935
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:23:33.202264
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.43
 ---- batch: 020 ----
mean loss: 856.29
 ---- batch: 030 ----
mean loss: 828.55
 ---- batch: 040 ----
mean loss: 859.15
 ---- batch: 050 ----
mean loss: 850.46
 ---- batch: 060 ----
mean loss: 841.69
 ---- batch: 070 ----
mean loss: 853.68
 ---- batch: 080 ----
mean loss: 837.44
 ---- batch: 090 ----
mean loss: 835.54
 ---- batch: 100 ----
mean loss: 831.79
 ---- batch: 110 ----
mean loss: 853.08
train mean loss: 843.97
epoch train time: 0:00:00.552958
elapsed time: 0:00:44.938031
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:23:33.755363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.41
 ---- batch: 020 ----
mean loss: 834.43
 ---- batch: 030 ----
mean loss: 818.79
 ---- batch: 040 ----
mean loss: 844.22
 ---- batch: 050 ----
mean loss: 871.60
 ---- batch: 060 ----
mean loss: 835.55
 ---- batch: 070 ----
mean loss: 864.85
 ---- batch: 080 ----
mean loss: 832.25
 ---- batch: 090 ----
mean loss: 846.19
 ---- batch: 100 ----
mean loss: 862.13
 ---- batch: 110 ----
mean loss: 844.14
train mean loss: 844.01
epoch train time: 0:00:00.559226
elapsed time: 0:00:45.497410
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:23:34.314735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.32
 ---- batch: 020 ----
mean loss: 841.57
 ---- batch: 030 ----
mean loss: 856.96
 ---- batch: 040 ----
mean loss: 844.32
 ---- batch: 050 ----
mean loss: 860.73
 ---- batch: 060 ----
mean loss: 834.29
 ---- batch: 070 ----
mean loss: 833.48
 ---- batch: 080 ----
mean loss: 858.26
 ---- batch: 090 ----
mean loss: 841.80
 ---- batch: 100 ----
mean loss: 851.81
 ---- batch: 110 ----
mean loss: 829.19
train mean loss: 843.99
epoch train time: 0:00:00.554446
elapsed time: 0:00:46.052005
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:23:34.869354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.90
 ---- batch: 020 ----
mean loss: 842.32
 ---- batch: 030 ----
mean loss: 827.27
 ---- batch: 040 ----
mean loss: 849.86
 ---- batch: 050 ----
mean loss: 846.89
 ---- batch: 060 ----
mean loss: 858.72
 ---- batch: 070 ----
mean loss: 818.85
 ---- batch: 080 ----
mean loss: 846.70
 ---- batch: 090 ----
mean loss: 854.57
 ---- batch: 100 ----
mean loss: 835.44
 ---- batch: 110 ----
mean loss: 854.74
train mean loss: 844.00
epoch train time: 0:00:00.574750
elapsed time: 0:00:46.626921
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:23:35.444251
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.74
 ---- batch: 020 ----
mean loss: 845.48
 ---- batch: 030 ----
mean loss: 843.53
 ---- batch: 040 ----
mean loss: 840.51
 ---- batch: 050 ----
mean loss: 834.65
 ---- batch: 060 ----
mean loss: 852.36
 ---- batch: 070 ----
mean loss: 864.92
 ---- batch: 080 ----
mean loss: 830.98
 ---- batch: 090 ----
mean loss: 848.37
 ---- batch: 100 ----
mean loss: 838.74
 ---- batch: 110 ----
mean loss: 838.41
train mean loss: 843.96
epoch train time: 0:00:00.560778
elapsed time: 0:00:47.187839
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:23:36.005171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.07
 ---- batch: 020 ----
mean loss: 852.37
 ---- batch: 030 ----
mean loss: 858.97
 ---- batch: 040 ----
mean loss: 845.35
 ---- batch: 050 ----
mean loss: 839.65
 ---- batch: 060 ----
mean loss: 819.39
 ---- batch: 070 ----
mean loss: 828.46
 ---- batch: 080 ----
mean loss: 860.91
 ---- batch: 090 ----
mean loss: 863.15
 ---- batch: 100 ----
mean loss: 836.78
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.562541
elapsed time: 0:00:47.750516
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:23:36.567862
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.65
 ---- batch: 030 ----
mean loss: 854.14
 ---- batch: 040 ----
mean loss: 865.58
 ---- batch: 050 ----
mean loss: 838.80
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.96
 ---- batch: 080 ----
mean loss: 849.24
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.79
 ---- batch: 110 ----
mean loss: 847.17
train mean loss: 843.85
epoch train time: 0:00:00.562785
elapsed time: 0:00:48.313493
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:23:37.130849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.58
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.25
 ---- batch: 040 ----
mean loss: 861.77
 ---- batch: 050 ----
mean loss: 858.51
 ---- batch: 060 ----
mean loss: 838.93
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.46
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.95
epoch train time: 0:00:00.563190
elapsed time: 0:00:48.876851
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:23:37.694181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.47
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.57
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.557215
elapsed time: 0:00:49.434213
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:23:38.251539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.23
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.44
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.62
 ---- batch: 090 ----
mean loss: 844.56
 ---- batch: 100 ----
mean loss: 855.07
 ---- batch: 110 ----
mean loss: 856.52
train mean loss: 844.02
epoch train time: 0:00:00.557068
elapsed time: 0:00:49.991430
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:23:38.808780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.90
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.11
 ---- batch: 050 ----
mean loss: 818.05
 ---- batch: 060 ----
mean loss: 833.14
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.88
 ---- batch: 090 ----
mean loss: 843.00
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.563680
elapsed time: 0:00:50.555283
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:23:39.372614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.31
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.57
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.01
train mean loss: 843.87
epoch train time: 0:00:00.561651
elapsed time: 0:00:51.117077
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:23:39.934408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.59
 ---- batch: 020 ----
mean loss: 848.84
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.45
 ---- batch: 050 ----
mean loss: 824.86
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.43
 ---- batch: 080 ----
mean loss: 858.19
 ---- batch: 090 ----
mean loss: 855.88
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.84
train mean loss: 843.97
epoch train time: 0:00:00.560895
elapsed time: 0:00:51.678131
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:23:40.495496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.72
 ---- batch: 030 ----
mean loss: 838.89
 ---- batch: 040 ----
mean loss: 851.11
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.73
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.570503
elapsed time: 0:00:52.248804
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:23:41.066135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.11
 ---- batch: 020 ----
mean loss: 861.62
 ---- batch: 030 ----
mean loss: 838.13
 ---- batch: 040 ----
mean loss: 845.02
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.19
 ---- batch: 090 ----
mean loss: 833.51
 ---- batch: 100 ----
mean loss: 834.02
 ---- batch: 110 ----
mean loss: 834.95
train mean loss: 844.04
epoch train time: 0:00:00.579168
elapsed time: 0:00:52.828121
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:23:41.645450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.52
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.74
 ---- batch: 060 ----
mean loss: 850.22
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.12
 ---- batch: 090 ----
mean loss: 864.27
 ---- batch: 100 ----
mean loss: 852.67
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.96
epoch train time: 0:00:00.564870
elapsed time: 0:00:53.393127
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:23:42.210465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.71
 ---- batch: 030 ----
mean loss: 831.00
 ---- batch: 040 ----
mean loss: 838.39
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.25
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.75
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.10
 ---- batch: 110 ----
mean loss: 855.26
train mean loss: 843.95
epoch train time: 0:00:00.568248
elapsed time: 0:00:53.961535
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:23:42.778866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.39
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.65
 ---- batch: 050 ----
mean loss: 817.85
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.03
 ---- batch: 080 ----
mean loss: 854.71
 ---- batch: 090 ----
mean loss: 824.24
 ---- batch: 100 ----
mean loss: 842.72
 ---- batch: 110 ----
mean loss: 844.57
train mean loss: 844.10
epoch train time: 0:00:00.573899
elapsed time: 0:00:54.535591
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:23:43.352959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.51
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.47
 ---- batch: 040 ----
mean loss: 845.67
 ---- batch: 050 ----
mean loss: 833.51
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.36
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.95
train mean loss: 844.01
epoch train time: 0:00:00.574642
elapsed time: 0:00:55.110409
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:23:43.927741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.24
 ---- batch: 030 ----
mean loss: 833.33
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.67
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.563603
elapsed time: 0:00:55.674158
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:23:44.491491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.51
 ---- batch: 020 ----
mean loss: 825.59
 ---- batch: 030 ----
mean loss: 839.15
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.24
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.48
 ---- batch: 090 ----
mean loss: 853.98
 ---- batch: 100 ----
mean loss: 844.05
 ---- batch: 110 ----
mean loss: 822.75
train mean loss: 843.97
epoch train time: 0:00:00.568929
elapsed time: 0:00:56.243223
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:23:45.060552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.68
 ---- batch: 030 ----
mean loss: 846.89
 ---- batch: 040 ----
mean loss: 835.59
 ---- batch: 050 ----
mean loss: 868.33
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.96
 ---- batch: 090 ----
mean loss: 841.47
 ---- batch: 100 ----
mean loss: 842.00
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.567488
elapsed time: 0:00:56.810847
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:23:45.628177
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.80
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.95
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 809.18
 ---- batch: 080 ----
mean loss: 856.45
 ---- batch: 090 ----
mean loss: 845.24
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.570548
elapsed time: 0:00:57.381530
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:23:46.198880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.82
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.10
 ---- batch: 050 ----
mean loss: 833.76
 ---- batch: 060 ----
mean loss: 865.05
 ---- batch: 070 ----
mean loss: 852.63
 ---- batch: 080 ----
mean loss: 833.16
 ---- batch: 090 ----
mean loss: 822.32
 ---- batch: 100 ----
mean loss: 830.72
 ---- batch: 110 ----
mean loss: 843.91
train mean loss: 844.05
epoch train time: 0:00:00.567704
elapsed time: 0:00:57.949417
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:23:46.766745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 816.96
 ---- batch: 020 ----
mean loss: 869.94
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.32
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.35
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.572606
elapsed time: 0:00:58.522166
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:23:47.339501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.24
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.66
 ---- batch: 060 ----
mean loss: 825.04
 ---- batch: 070 ----
mean loss: 862.11
 ---- batch: 080 ----
mean loss: 821.69
 ---- batch: 090 ----
mean loss: 868.29
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.32
train mean loss: 844.02
epoch train time: 0:00:00.560941
elapsed time: 0:00:59.083279
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:23:47.900615
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.91
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.45
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.58
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.89
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.03
epoch train time: 0:00:00.571594
elapsed time: 0:00:59.655019
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:23:48.472354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.86
 ---- batch: 020 ----
mean loss: 843.80
 ---- batch: 030 ----
mean loss: 851.08
 ---- batch: 040 ----
mean loss: 856.67
 ---- batch: 050 ----
mean loss: 832.85
 ---- batch: 060 ----
mean loss: 851.56
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.22
 ---- batch: 090 ----
mean loss: 833.16
 ---- batch: 100 ----
mean loss: 851.10
 ---- batch: 110 ----
mean loss: 824.82
train mean loss: 844.03
epoch train time: 0:00:00.565127
elapsed time: 0:01:00.220287
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:23:49.037642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.34
 ---- batch: 030 ----
mean loss: 854.57
 ---- batch: 040 ----
mean loss: 833.63
 ---- batch: 050 ----
mean loss: 834.70
 ---- batch: 060 ----
mean loss: 846.26
 ---- batch: 070 ----
mean loss: 825.67
 ---- batch: 080 ----
mean loss: 834.82
 ---- batch: 090 ----
mean loss: 816.54
 ---- batch: 100 ----
mean loss: 793.04
 ---- batch: 110 ----
mean loss: 788.65
train mean loss: 828.99
epoch train time: 0:00:00.565010
elapsed time: 0:01:00.785461
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:23:49.602817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 766.88
 ---- batch: 020 ----
mean loss: 768.60
 ---- batch: 030 ----
mean loss: 767.40
 ---- batch: 040 ----
mean loss: 750.30
 ---- batch: 050 ----
mean loss: 723.93
 ---- batch: 060 ----
mean loss: 709.77
 ---- batch: 070 ----
mean loss: 708.45
 ---- batch: 080 ----
mean loss: 680.35
 ---- batch: 090 ----
mean loss: 639.50
 ---- batch: 100 ----
mean loss: 572.75
 ---- batch: 110 ----
mean loss: 525.79
train mean loss: 686.79
epoch train time: 0:00:00.564878
elapsed time: 0:01:01.350523
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:23:50.167868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 463.60
 ---- batch: 020 ----
mean loss: 430.65
 ---- batch: 030 ----
mean loss: 420.66
 ---- batch: 040 ----
mean loss: 380.57
 ---- batch: 050 ----
mean loss: 365.46
 ---- batch: 060 ----
mean loss: 358.32
 ---- batch: 070 ----
mean loss: 350.27
 ---- batch: 080 ----
mean loss: 357.08
 ---- batch: 090 ----
mean loss: 332.31
 ---- batch: 100 ----
mean loss: 331.63
 ---- batch: 110 ----
mean loss: 325.68
train mean loss: 372.53
epoch train time: 0:00:00.568558
elapsed time: 0:01:01.919232
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:23:50.736565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.22
 ---- batch: 020 ----
mean loss: 311.31
 ---- batch: 030 ----
mean loss: 306.72
 ---- batch: 040 ----
mean loss: 297.80
 ---- batch: 050 ----
mean loss: 287.50
 ---- batch: 060 ----
mean loss: 281.05
 ---- batch: 070 ----
mean loss: 287.31
 ---- batch: 080 ----
mean loss: 281.52
 ---- batch: 090 ----
mean loss: 276.48
 ---- batch: 100 ----
mean loss: 279.11
 ---- batch: 110 ----
mean loss: 283.17
train mean loss: 291.42
epoch train time: 0:00:00.563386
elapsed time: 0:01:02.482758
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:23:51.300093
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.23
 ---- batch: 020 ----
mean loss: 271.50
 ---- batch: 030 ----
mean loss: 264.29
 ---- batch: 040 ----
mean loss: 258.35
 ---- batch: 050 ----
mean loss: 264.08
 ---- batch: 060 ----
mean loss: 261.76
 ---- batch: 070 ----
mean loss: 258.58
 ---- batch: 080 ----
mean loss: 255.60
 ---- batch: 090 ----
mean loss: 260.00
 ---- batch: 100 ----
mean loss: 257.34
 ---- batch: 110 ----
mean loss: 259.79
train mean loss: 261.90
epoch train time: 0:00:00.574174
elapsed time: 0:01:03.057078
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:23:51.874433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.48
 ---- batch: 020 ----
mean loss: 241.04
 ---- batch: 030 ----
mean loss: 253.82
 ---- batch: 040 ----
mean loss: 247.68
 ---- batch: 050 ----
mean loss: 243.24
 ---- batch: 060 ----
mean loss: 233.49
 ---- batch: 070 ----
mean loss: 227.18
 ---- batch: 080 ----
mean loss: 240.00
 ---- batch: 090 ----
mean loss: 238.28
 ---- batch: 100 ----
mean loss: 247.81
 ---- batch: 110 ----
mean loss: 244.45
train mean loss: 242.53
epoch train time: 0:00:00.563592
elapsed time: 0:01:03.620832
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:23:52.438163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.83
 ---- batch: 020 ----
mean loss: 233.03
 ---- batch: 030 ----
mean loss: 241.77
 ---- batch: 040 ----
mean loss: 235.19
 ---- batch: 050 ----
mean loss: 233.26
 ---- batch: 060 ----
mean loss: 227.30
 ---- batch: 070 ----
mean loss: 226.25
 ---- batch: 080 ----
mean loss: 225.54
 ---- batch: 090 ----
mean loss: 230.27
 ---- batch: 100 ----
mean loss: 226.82
 ---- batch: 110 ----
mean loss: 228.43
train mean loss: 231.68
epoch train time: 0:00:00.567126
elapsed time: 0:01:04.188094
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:23:53.005433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.83
 ---- batch: 020 ----
mean loss: 222.76
 ---- batch: 030 ----
mean loss: 233.62
 ---- batch: 040 ----
mean loss: 219.65
 ---- batch: 050 ----
mean loss: 229.01
 ---- batch: 060 ----
mean loss: 227.90
 ---- batch: 070 ----
mean loss: 225.53
 ---- batch: 080 ----
mean loss: 214.62
 ---- batch: 090 ----
mean loss: 224.01
 ---- batch: 100 ----
mean loss: 216.43
 ---- batch: 110 ----
mean loss: 218.86
train mean loss: 223.37
epoch train time: 0:00:00.561792
elapsed time: 0:01:04.750029
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:23:53.567377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.60
 ---- batch: 020 ----
mean loss: 211.98
 ---- batch: 030 ----
mean loss: 215.92
 ---- batch: 040 ----
mean loss: 210.73
 ---- batch: 050 ----
mean loss: 222.82
 ---- batch: 060 ----
mean loss: 215.76
 ---- batch: 070 ----
mean loss: 222.04
 ---- batch: 080 ----
mean loss: 212.15
 ---- batch: 090 ----
mean loss: 215.93
 ---- batch: 100 ----
mean loss: 213.38
 ---- batch: 110 ----
mean loss: 224.72
train mean loss: 216.96
epoch train time: 0:00:00.570810
elapsed time: 0:01:05.321011
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:23:54.138373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.12
 ---- batch: 020 ----
mean loss: 216.46
 ---- batch: 030 ----
mean loss: 212.84
 ---- batch: 040 ----
mean loss: 210.86
 ---- batch: 050 ----
mean loss: 204.52
 ---- batch: 060 ----
mean loss: 202.22
 ---- batch: 070 ----
mean loss: 204.50
 ---- batch: 080 ----
mean loss: 216.25
 ---- batch: 090 ----
mean loss: 207.20
 ---- batch: 100 ----
mean loss: 218.85
 ---- batch: 110 ----
mean loss: 207.42
train mean loss: 209.71
epoch train time: 0:00:00.563840
elapsed time: 0:01:05.885022
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:23:54.702354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.05
 ---- batch: 020 ----
mean loss: 199.46
 ---- batch: 030 ----
mean loss: 205.63
 ---- batch: 040 ----
mean loss: 212.87
 ---- batch: 050 ----
mean loss: 203.19
 ---- batch: 060 ----
mean loss: 201.39
 ---- batch: 070 ----
mean loss: 194.98
 ---- batch: 080 ----
mean loss: 206.75
 ---- batch: 090 ----
mean loss: 211.02
 ---- batch: 100 ----
mean loss: 209.89
 ---- batch: 110 ----
mean loss: 209.89
train mean loss: 205.08
epoch train time: 0:00:00.568475
elapsed time: 0:01:06.453685
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:23:55.271016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.00
 ---- batch: 020 ----
mean loss: 193.64
 ---- batch: 030 ----
mean loss: 207.26
 ---- batch: 040 ----
mean loss: 204.08
 ---- batch: 050 ----
mean loss: 203.79
 ---- batch: 060 ----
mean loss: 203.11
 ---- batch: 070 ----
mean loss: 199.42
 ---- batch: 080 ----
mean loss: 204.31
 ---- batch: 090 ----
mean loss: 205.78
 ---- batch: 100 ----
mean loss: 200.55
 ---- batch: 110 ----
mean loss: 195.36
train mean loss: 201.66
epoch train time: 0:00:00.568382
elapsed time: 0:01:07.022202
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:23:55.839537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.92
 ---- batch: 020 ----
mean loss: 206.74
 ---- batch: 030 ----
mean loss: 201.36
 ---- batch: 040 ----
mean loss: 188.42
 ---- batch: 050 ----
mean loss: 204.65
 ---- batch: 060 ----
mean loss: 196.51
 ---- batch: 070 ----
mean loss: 204.21
 ---- batch: 080 ----
mean loss: 192.36
 ---- batch: 090 ----
mean loss: 204.58
 ---- batch: 100 ----
mean loss: 190.77
 ---- batch: 110 ----
mean loss: 202.65
train mean loss: 198.73
epoch train time: 0:00:00.562620
elapsed time: 0:01:07.584964
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:23:56.402298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.79
 ---- batch: 020 ----
mean loss: 186.30
 ---- batch: 030 ----
mean loss: 195.02
 ---- batch: 040 ----
mean loss: 196.79
 ---- batch: 050 ----
mean loss: 187.48
 ---- batch: 060 ----
mean loss: 196.04
 ---- batch: 070 ----
mean loss: 179.32
 ---- batch: 080 ----
mean loss: 205.45
 ---- batch: 090 ----
mean loss: 195.99
 ---- batch: 100 ----
mean loss: 195.27
 ---- batch: 110 ----
mean loss: 195.25
train mean loss: 194.35
epoch train time: 0:00:00.570536
elapsed time: 0:01:08.155640
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:23:56.972972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.91
 ---- batch: 020 ----
mean loss: 190.36
 ---- batch: 030 ----
mean loss: 192.35
 ---- batch: 040 ----
mean loss: 194.02
 ---- batch: 050 ----
mean loss: 188.52
 ---- batch: 060 ----
mean loss: 189.12
 ---- batch: 070 ----
mean loss: 196.47
 ---- batch: 080 ----
mean loss: 191.75
 ---- batch: 090 ----
mean loss: 184.89
 ---- batch: 100 ----
mean loss: 191.21
 ---- batch: 110 ----
mean loss: 202.60
train mean loss: 191.89
epoch train time: 0:00:00.569773
elapsed time: 0:01:08.725566
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:23:57.542928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.44
 ---- batch: 020 ----
mean loss: 191.88
 ---- batch: 030 ----
mean loss: 184.67
 ---- batch: 040 ----
mean loss: 176.90
 ---- batch: 050 ----
mean loss: 191.72
 ---- batch: 060 ----
mean loss: 194.01
 ---- batch: 070 ----
mean loss: 194.55
 ---- batch: 080 ----
mean loss: 195.35
 ---- batch: 090 ----
mean loss: 196.85
 ---- batch: 100 ----
mean loss: 191.77
 ---- batch: 110 ----
mean loss: 185.62
train mean loss: 190.11
epoch train time: 0:00:00.585515
elapsed time: 0:01:09.311248
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:23:58.128578
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.21
 ---- batch: 020 ----
mean loss: 179.69
 ---- batch: 030 ----
mean loss: 193.61
 ---- batch: 040 ----
mean loss: 192.65
 ---- batch: 050 ----
mean loss: 198.15
 ---- batch: 060 ----
mean loss: 194.46
 ---- batch: 070 ----
mean loss: 197.05
 ---- batch: 080 ----
mean loss: 189.77
 ---- batch: 090 ----
mean loss: 181.28
 ---- batch: 100 ----
mean loss: 183.89
 ---- batch: 110 ----
mean loss: 195.52
train mean loss: 190.92
epoch train time: 0:00:00.568026
elapsed time: 0:01:09.879412
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:23:58.696741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.74
 ---- batch: 020 ----
mean loss: 182.04
 ---- batch: 030 ----
mean loss: 182.66
 ---- batch: 040 ----
mean loss: 184.63
 ---- batch: 050 ----
mean loss: 178.09
 ---- batch: 060 ----
mean loss: 194.30
 ---- batch: 070 ----
mean loss: 186.75
 ---- batch: 080 ----
mean loss: 195.08
 ---- batch: 090 ----
mean loss: 187.58
 ---- batch: 100 ----
mean loss: 189.89
 ---- batch: 110 ----
mean loss: 187.94
train mean loss: 186.61
epoch train time: 0:00:00.577498
elapsed time: 0:01:10.457045
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:23:59.274376
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.55
 ---- batch: 020 ----
mean loss: 180.61
 ---- batch: 030 ----
mean loss: 175.00
 ---- batch: 040 ----
mean loss: 186.80
 ---- batch: 050 ----
mean loss: 186.88
 ---- batch: 060 ----
mean loss: 179.41
 ---- batch: 070 ----
mean loss: 191.73
 ---- batch: 080 ----
mean loss: 187.71
 ---- batch: 090 ----
mean loss: 196.45
 ---- batch: 100 ----
mean loss: 186.63
 ---- batch: 110 ----
mean loss: 188.42
train mean loss: 186.10
epoch train time: 0:00:00.579559
elapsed time: 0:01:11.036740
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:23:59.854086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.18
 ---- batch: 020 ----
mean loss: 185.80
 ---- batch: 030 ----
mean loss: 175.94
 ---- batch: 040 ----
mean loss: 182.98
 ---- batch: 050 ----
mean loss: 186.73
 ---- batch: 060 ----
mean loss: 192.68
 ---- batch: 070 ----
mean loss: 186.42
 ---- batch: 080 ----
mean loss: 186.00
 ---- batch: 090 ----
mean loss: 192.96
 ---- batch: 100 ----
mean loss: 173.83
 ---- batch: 110 ----
mean loss: 189.66
train mean loss: 185.26
epoch train time: 0:00:00.568424
elapsed time: 0:01:11.605318
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:24:00.422663
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.09
 ---- batch: 020 ----
mean loss: 185.37
 ---- batch: 030 ----
mean loss: 188.91
 ---- batch: 040 ----
mean loss: 177.12
 ---- batch: 050 ----
mean loss: 185.39
 ---- batch: 060 ----
mean loss: 190.85
 ---- batch: 070 ----
mean loss: 178.49
 ---- batch: 080 ----
mean loss: 180.74
 ---- batch: 090 ----
mean loss: 180.68
 ---- batch: 100 ----
mean loss: 179.20
 ---- batch: 110 ----
mean loss: 191.54
train mean loss: 184.02
epoch train time: 0:00:00.572476
elapsed time: 0:01:12.177951
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:24:00.995296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.03
 ---- batch: 020 ----
mean loss: 183.14
 ---- batch: 030 ----
mean loss: 193.82
 ---- batch: 040 ----
mean loss: 183.13
 ---- batch: 050 ----
mean loss: 181.14
 ---- batch: 060 ----
mean loss: 175.61
 ---- batch: 070 ----
mean loss: 186.00
 ---- batch: 080 ----
mean loss: 182.03
 ---- batch: 090 ----
mean loss: 188.10
 ---- batch: 100 ----
mean loss: 179.71
 ---- batch: 110 ----
mean loss: 186.39
train mean loss: 182.87
epoch train time: 0:00:00.570241
elapsed time: 0:01:12.748371
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:24:01.565709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.44
 ---- batch: 020 ----
mean loss: 172.95
 ---- batch: 030 ----
mean loss: 184.75
 ---- batch: 040 ----
mean loss: 183.46
 ---- batch: 050 ----
mean loss: 169.24
 ---- batch: 060 ----
mean loss: 182.51
 ---- batch: 070 ----
mean loss: 184.07
 ---- batch: 080 ----
mean loss: 192.44
 ---- batch: 090 ----
mean loss: 180.59
 ---- batch: 100 ----
mean loss: 182.74
 ---- batch: 110 ----
mean loss: 184.03
train mean loss: 181.88
epoch train time: 0:00:00.572678
elapsed time: 0:01:13.321194
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:24:02.138527
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.50
 ---- batch: 020 ----
mean loss: 178.32
 ---- batch: 030 ----
mean loss: 183.06
 ---- batch: 040 ----
mean loss: 183.65
 ---- batch: 050 ----
mean loss: 186.03
 ---- batch: 060 ----
mean loss: 177.02
 ---- batch: 070 ----
mean loss: 183.57
 ---- batch: 080 ----
mean loss: 177.49
 ---- batch: 090 ----
mean loss: 175.86
 ---- batch: 100 ----
mean loss: 188.03
 ---- batch: 110 ----
mean loss: 179.42
train mean loss: 180.95
epoch train time: 0:00:00.570523
elapsed time: 0:01:13.891860
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:24:02.709194
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.05
 ---- batch: 020 ----
mean loss: 181.91
 ---- batch: 030 ----
mean loss: 178.10
 ---- batch: 040 ----
mean loss: 173.80
 ---- batch: 050 ----
mean loss: 177.79
 ---- batch: 060 ----
mean loss: 186.77
 ---- batch: 070 ----
mean loss: 175.99
 ---- batch: 080 ----
mean loss: 183.33
 ---- batch: 090 ----
mean loss: 183.76
 ---- batch: 100 ----
mean loss: 183.86
 ---- batch: 110 ----
mean loss: 177.31
train mean loss: 180.05
epoch train time: 0:00:00.579664
elapsed time: 0:01:14.471669
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:24:03.288995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.89
 ---- batch: 020 ----
mean loss: 176.72
 ---- batch: 030 ----
mean loss: 175.49
 ---- batch: 040 ----
mean loss: 174.50
 ---- batch: 050 ----
mean loss: 178.27
 ---- batch: 060 ----
mean loss: 179.65
 ---- batch: 070 ----
mean loss: 182.29
 ---- batch: 080 ----
mean loss: 185.99
 ---- batch: 090 ----
mean loss: 177.76
 ---- batch: 100 ----
mean loss: 178.48
 ---- batch: 110 ----
mean loss: 181.29
train mean loss: 179.77
epoch train time: 0:00:00.594855
elapsed time: 0:01:15.066681
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:24:03.884029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.76
 ---- batch: 020 ----
mean loss: 175.42
 ---- batch: 030 ----
mean loss: 185.69
 ---- batch: 040 ----
mean loss: 181.29
 ---- batch: 050 ----
mean loss: 187.67
 ---- batch: 060 ----
mean loss: 171.89
 ---- batch: 070 ----
mean loss: 175.34
 ---- batch: 080 ----
mean loss: 178.13
 ---- batch: 090 ----
mean loss: 180.00
 ---- batch: 100 ----
mean loss: 175.08
 ---- batch: 110 ----
mean loss: 184.47
train mean loss: 179.13
epoch train time: 0:00:00.563631
elapsed time: 0:01:15.630469
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:24:04.447816
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.87
 ---- batch: 020 ----
mean loss: 171.07
 ---- batch: 030 ----
mean loss: 180.37
 ---- batch: 040 ----
mean loss: 180.97
 ---- batch: 050 ----
mean loss: 182.52
 ---- batch: 060 ----
mean loss: 181.66
 ---- batch: 070 ----
mean loss: 178.24
 ---- batch: 080 ----
mean loss: 174.17
 ---- batch: 090 ----
mean loss: 177.84
 ---- batch: 100 ----
mean loss: 169.71
 ---- batch: 110 ----
mean loss: 180.90
train mean loss: 178.12
epoch train time: 0:00:00.564008
elapsed time: 0:01:16.194643
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:24:05.012003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.99
 ---- batch: 020 ----
mean loss: 180.01
 ---- batch: 030 ----
mean loss: 172.30
 ---- batch: 040 ----
mean loss: 174.10
 ---- batch: 050 ----
mean loss: 180.16
 ---- batch: 060 ----
mean loss: 178.64
 ---- batch: 070 ----
mean loss: 178.99
 ---- batch: 080 ----
mean loss: 178.53
 ---- batch: 090 ----
mean loss: 177.60
 ---- batch: 100 ----
mean loss: 182.41
 ---- batch: 110 ----
mean loss: 174.14
train mean loss: 177.77
epoch train time: 0:00:00.570040
elapsed time: 0:01:16.764851
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:24:05.582183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.80
 ---- batch: 020 ----
mean loss: 172.59
 ---- batch: 030 ----
mean loss: 169.45
 ---- batch: 040 ----
mean loss: 177.70
 ---- batch: 050 ----
mean loss: 177.66
 ---- batch: 060 ----
mean loss: 185.94
 ---- batch: 070 ----
mean loss: 175.76
 ---- batch: 080 ----
mean loss: 180.92
 ---- batch: 090 ----
mean loss: 177.77
 ---- batch: 100 ----
mean loss: 175.78
 ---- batch: 110 ----
mean loss: 180.31
train mean loss: 176.37
epoch train time: 0:00:00.568126
elapsed time: 0:01:17.333119
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:24:06.150450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.54
 ---- batch: 020 ----
mean loss: 178.80
 ---- batch: 030 ----
mean loss: 169.07
 ---- batch: 040 ----
mean loss: 181.13
 ---- batch: 050 ----
mean loss: 176.08
 ---- batch: 060 ----
mean loss: 176.87
 ---- batch: 070 ----
mean loss: 178.06
 ---- batch: 080 ----
mean loss: 171.74
 ---- batch: 090 ----
mean loss: 179.11
 ---- batch: 100 ----
mean loss: 169.02
 ---- batch: 110 ----
mean loss: 192.79
train mean loss: 176.23
epoch train time: 0:00:00.580070
elapsed time: 0:01:17.913326
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:24:06.730677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.26
 ---- batch: 020 ----
mean loss: 177.92
 ---- batch: 030 ----
mean loss: 171.13
 ---- batch: 040 ----
mean loss: 173.85
 ---- batch: 050 ----
mean loss: 172.32
 ---- batch: 060 ----
mean loss: 176.80
 ---- batch: 070 ----
mean loss: 182.64
 ---- batch: 080 ----
mean loss: 171.64
 ---- batch: 090 ----
mean loss: 178.29
 ---- batch: 100 ----
mean loss: 178.12
 ---- batch: 110 ----
mean loss: 174.10
train mean loss: 175.35
epoch train time: 0:00:00.565763
elapsed time: 0:01:18.479245
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:24:07.296576
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.84
 ---- batch: 020 ----
mean loss: 172.45
 ---- batch: 030 ----
mean loss: 172.88
 ---- batch: 040 ----
mean loss: 172.98
 ---- batch: 050 ----
mean loss: 175.44
 ---- batch: 060 ----
mean loss: 170.61
 ---- batch: 070 ----
mean loss: 171.50
 ---- batch: 080 ----
mean loss: 191.33
 ---- batch: 090 ----
mean loss: 173.99
 ---- batch: 100 ----
mean loss: 165.15
 ---- batch: 110 ----
mean loss: 184.13
train mean loss: 174.65
epoch train time: 0:00:00.574873
elapsed time: 0:01:19.054254
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:24:07.871585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.68
 ---- batch: 020 ----
mean loss: 180.44
 ---- batch: 030 ----
mean loss: 179.33
 ---- batch: 040 ----
mean loss: 177.17
 ---- batch: 050 ----
mean loss: 167.54
 ---- batch: 060 ----
mean loss: 176.15
 ---- batch: 070 ----
mean loss: 181.76
 ---- batch: 080 ----
mean loss: 176.13
 ---- batch: 090 ----
mean loss: 177.16
 ---- batch: 100 ----
mean loss: 171.49
 ---- batch: 110 ----
mean loss: 171.79
train mean loss: 175.16
epoch train time: 0:00:00.557268
elapsed time: 0:01:19.611684
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:24:08.429032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.82
 ---- batch: 020 ----
mean loss: 171.02
 ---- batch: 030 ----
mean loss: 173.99
 ---- batch: 040 ----
mean loss: 168.04
 ---- batch: 050 ----
mean loss: 178.41
 ---- batch: 060 ----
mean loss: 171.95
 ---- batch: 070 ----
mean loss: 173.37
 ---- batch: 080 ----
mean loss: 174.43
 ---- batch: 090 ----
mean loss: 173.04
 ---- batch: 100 ----
mean loss: 174.04
 ---- batch: 110 ----
mean loss: 174.59
train mean loss: 173.30
epoch train time: 0:00:00.563505
elapsed time: 0:01:20.175355
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:24:08.992684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.85
 ---- batch: 020 ----
mean loss: 174.98
 ---- batch: 030 ----
mean loss: 169.48
 ---- batch: 040 ----
mean loss: 166.93
 ---- batch: 050 ----
mean loss: 164.88
 ---- batch: 060 ----
mean loss: 174.40
 ---- batch: 070 ----
mean loss: 182.06
 ---- batch: 080 ----
mean loss: 179.70
 ---- batch: 090 ----
mean loss: 173.22
 ---- batch: 100 ----
mean loss: 182.15
 ---- batch: 110 ----
mean loss: 170.90
train mean loss: 173.75
epoch train time: 0:00:00.566099
elapsed time: 0:01:20.741606
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:24:09.558944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.46
 ---- batch: 020 ----
mean loss: 179.69
 ---- batch: 030 ----
mean loss: 161.18
 ---- batch: 040 ----
mean loss: 175.46
 ---- batch: 050 ----
mean loss: 174.48
 ---- batch: 060 ----
mean loss: 170.89
 ---- batch: 070 ----
mean loss: 169.29
 ---- batch: 080 ----
mean loss: 180.26
 ---- batch: 090 ----
mean loss: 176.17
 ---- batch: 100 ----
mean loss: 174.74
 ---- batch: 110 ----
mean loss: 176.28
train mean loss: 173.15
epoch train time: 0:00:00.572513
elapsed time: 0:01:21.314281
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:24:10.131617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.19
 ---- batch: 020 ----
mean loss: 180.41
 ---- batch: 030 ----
mean loss: 163.45
 ---- batch: 040 ----
mean loss: 167.94
 ---- batch: 050 ----
mean loss: 177.38
 ---- batch: 060 ----
mean loss: 172.27
 ---- batch: 070 ----
mean loss: 170.03
 ---- batch: 080 ----
mean loss: 168.22
 ---- batch: 090 ----
mean loss: 176.27
 ---- batch: 100 ----
mean loss: 175.83
 ---- batch: 110 ----
mean loss: 180.16
train mean loss: 171.84
epoch train time: 0:00:00.569314
elapsed time: 0:01:21.883757
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:24:10.701090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.17
 ---- batch: 020 ----
mean loss: 176.22
 ---- batch: 030 ----
mean loss: 167.91
 ---- batch: 040 ----
mean loss: 163.73
 ---- batch: 050 ----
mean loss: 172.17
 ---- batch: 060 ----
mean loss: 172.38
 ---- batch: 070 ----
mean loss: 173.55
 ---- batch: 080 ----
mean loss: 181.18
 ---- batch: 090 ----
mean loss: 172.23
 ---- batch: 100 ----
mean loss: 164.49
 ---- batch: 110 ----
mean loss: 179.14
train mean loss: 172.12
epoch train time: 0:00:00.580642
elapsed time: 0:01:22.464582
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:24:11.281957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.13
 ---- batch: 020 ----
mean loss: 164.01
 ---- batch: 030 ----
mean loss: 174.12
 ---- batch: 040 ----
mean loss: 167.97
 ---- batch: 050 ----
mean loss: 172.02
 ---- batch: 060 ----
mean loss: 163.80
 ---- batch: 070 ----
mean loss: 178.06
 ---- batch: 080 ----
mean loss: 173.91
 ---- batch: 090 ----
mean loss: 174.01
 ---- batch: 100 ----
mean loss: 178.11
 ---- batch: 110 ----
mean loss: 168.11
train mean loss: 170.14
epoch train time: 0:00:00.596973
elapsed time: 0:01:23.061744
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:24:11.879077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.31
 ---- batch: 020 ----
mean loss: 167.65
 ---- batch: 030 ----
mean loss: 168.74
 ---- batch: 040 ----
mean loss: 175.85
 ---- batch: 050 ----
mean loss: 162.24
 ---- batch: 060 ----
mean loss: 168.33
 ---- batch: 070 ----
mean loss: 174.37
 ---- batch: 080 ----
mean loss: 175.90
 ---- batch: 090 ----
mean loss: 168.95
 ---- batch: 100 ----
mean loss: 170.30
 ---- batch: 110 ----
mean loss: 172.92
train mean loss: 170.23
epoch train time: 0:00:00.572100
elapsed time: 0:01:23.633994
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:24:12.451357
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.01
 ---- batch: 020 ----
mean loss: 167.60
 ---- batch: 030 ----
mean loss: 161.39
 ---- batch: 040 ----
mean loss: 164.81
 ---- batch: 050 ----
mean loss: 162.75
 ---- batch: 060 ----
mean loss: 175.41
 ---- batch: 070 ----
mean loss: 180.73
 ---- batch: 080 ----
mean loss: 172.51
 ---- batch: 090 ----
mean loss: 165.44
 ---- batch: 100 ----
mean loss: 174.33
 ---- batch: 110 ----
mean loss: 171.97
train mean loss: 169.88
epoch train time: 0:00:00.570269
elapsed time: 0:01:24.204431
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:24:13.021761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.78
 ---- batch: 020 ----
mean loss: 170.26
 ---- batch: 030 ----
mean loss: 169.98
 ---- batch: 040 ----
mean loss: 162.63
 ---- batch: 050 ----
mean loss: 168.68
 ---- batch: 060 ----
mean loss: 171.35
 ---- batch: 070 ----
mean loss: 168.05
 ---- batch: 080 ----
mean loss: 168.26
 ---- batch: 090 ----
mean loss: 172.35
 ---- batch: 100 ----
mean loss: 171.82
 ---- batch: 110 ----
mean loss: 172.85
train mean loss: 169.30
epoch train time: 0:00:00.561538
elapsed time: 0:01:24.766120
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:24:13.583449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.35
 ---- batch: 020 ----
mean loss: 175.31
 ---- batch: 030 ----
mean loss: 160.90
 ---- batch: 040 ----
mean loss: 172.96
 ---- batch: 050 ----
mean loss: 161.23
 ---- batch: 060 ----
mean loss: 168.46
 ---- batch: 070 ----
mean loss: 174.63
 ---- batch: 080 ----
mean loss: 176.41
 ---- batch: 090 ----
mean loss: 166.92
 ---- batch: 100 ----
mean loss: 170.05
 ---- batch: 110 ----
mean loss: 170.50
train mean loss: 169.55
epoch train time: 0:00:00.563075
elapsed time: 0:01:25.329329
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:24:14.146660
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.34
 ---- batch: 020 ----
mean loss: 173.10
 ---- batch: 030 ----
mean loss: 167.42
 ---- batch: 040 ----
mean loss: 172.09
 ---- batch: 050 ----
mean loss: 169.60
 ---- batch: 060 ----
mean loss: 158.23
 ---- batch: 070 ----
mean loss: 169.75
 ---- batch: 080 ----
mean loss: 161.17
 ---- batch: 090 ----
mean loss: 170.81
 ---- batch: 100 ----
mean loss: 170.71
 ---- batch: 110 ----
mean loss: 172.21
train mean loss: 168.43
epoch train time: 0:00:00.571831
elapsed time: 0:01:25.901297
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:24:14.718627
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.18
 ---- batch: 020 ----
mean loss: 173.26
 ---- batch: 030 ----
mean loss: 170.82
 ---- batch: 040 ----
mean loss: 162.01
 ---- batch: 050 ----
mean loss: 163.23
 ---- batch: 060 ----
mean loss: 171.00
 ---- batch: 070 ----
mean loss: 174.42
 ---- batch: 080 ----
mean loss: 174.60
 ---- batch: 090 ----
mean loss: 162.53
 ---- batch: 100 ----
mean loss: 167.69
 ---- batch: 110 ----
mean loss: 164.15
train mean loss: 168.05
epoch train time: 0:00:00.564506
elapsed time: 0:01:26.465940
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:24:15.283286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.87
 ---- batch: 020 ----
mean loss: 167.95
 ---- batch: 030 ----
mean loss: 172.67
 ---- batch: 040 ----
mean loss: 164.22
 ---- batch: 050 ----
mean loss: 169.76
 ---- batch: 060 ----
mean loss: 168.87
 ---- batch: 070 ----
mean loss: 165.03
 ---- batch: 080 ----
mean loss: 168.55
 ---- batch: 090 ----
mean loss: 165.73
 ---- batch: 100 ----
mean loss: 165.96
 ---- batch: 110 ----
mean loss: 168.77
train mean loss: 167.79
epoch train time: 0:00:00.567959
elapsed time: 0:01:27.034059
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:24:15.851436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.29
 ---- batch: 020 ----
mean loss: 168.92
 ---- batch: 030 ----
mean loss: 166.72
 ---- batch: 040 ----
mean loss: 174.92
 ---- batch: 050 ----
mean loss: 174.23
 ---- batch: 060 ----
mean loss: 163.81
 ---- batch: 070 ----
mean loss: 162.00
 ---- batch: 080 ----
mean loss: 161.80
 ---- batch: 090 ----
mean loss: 170.09
 ---- batch: 100 ----
mean loss: 165.68
 ---- batch: 110 ----
mean loss: 165.43
train mean loss: 167.92
epoch train time: 0:00:00.581551
elapsed time: 0:01:27.615793
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:24:16.433163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.84
 ---- batch: 020 ----
mean loss: 155.98
 ---- batch: 030 ----
mean loss: 154.27
 ---- batch: 040 ----
mean loss: 170.98
 ---- batch: 050 ----
mean loss: 174.20
 ---- batch: 060 ----
mean loss: 171.44
 ---- batch: 070 ----
mean loss: 169.62
 ---- batch: 080 ----
mean loss: 167.31
 ---- batch: 090 ----
mean loss: 163.91
 ---- batch: 100 ----
mean loss: 169.00
 ---- batch: 110 ----
mean loss: 173.50
train mean loss: 167.18
epoch train time: 0:00:00.573642
elapsed time: 0:01:28.189613
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:24:17.006944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.79
 ---- batch: 020 ----
mean loss: 162.28
 ---- batch: 030 ----
mean loss: 165.77
 ---- batch: 040 ----
mean loss: 166.07
 ---- batch: 050 ----
mean loss: 173.25
 ---- batch: 060 ----
mean loss: 162.19
 ---- batch: 070 ----
mean loss: 162.54
 ---- batch: 080 ----
mean loss: 172.01
 ---- batch: 090 ----
mean loss: 174.45
 ---- batch: 100 ----
mean loss: 171.96
 ---- batch: 110 ----
mean loss: 163.54
train mean loss: 166.67
epoch train time: 0:00:00.568906
elapsed time: 0:01:28.758656
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:24:17.575988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.49
 ---- batch: 020 ----
mean loss: 164.83
 ---- batch: 030 ----
mean loss: 164.87
 ---- batch: 040 ----
mean loss: 164.26
 ---- batch: 050 ----
mean loss: 167.37
 ---- batch: 060 ----
mean loss: 171.56
 ---- batch: 070 ----
mean loss: 167.84
 ---- batch: 080 ----
mean loss: 177.53
 ---- batch: 090 ----
mean loss: 164.05
 ---- batch: 100 ----
mean loss: 168.66
 ---- batch: 110 ----
mean loss: 164.63
train mean loss: 166.42
epoch train time: 0:00:00.564699
elapsed time: 0:01:29.323489
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:24:18.140817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.23
 ---- batch: 020 ----
mean loss: 168.37
 ---- batch: 030 ----
mean loss: 157.08
 ---- batch: 040 ----
mean loss: 168.14
 ---- batch: 050 ----
mean loss: 163.85
 ---- batch: 060 ----
mean loss: 164.22
 ---- batch: 070 ----
mean loss: 158.60
 ---- batch: 080 ----
mean loss: 157.19
 ---- batch: 090 ----
mean loss: 166.28
 ---- batch: 100 ----
mean loss: 173.48
 ---- batch: 110 ----
mean loss: 174.21
train mean loss: 165.89
epoch train time: 0:00:00.563208
elapsed time: 0:01:29.886846
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:24:18.704176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.80
 ---- batch: 020 ----
mean loss: 160.17
 ---- batch: 030 ----
mean loss: 169.20
 ---- batch: 040 ----
mean loss: 159.62
 ---- batch: 050 ----
mean loss: 161.33
 ---- batch: 060 ----
mean loss: 170.64
 ---- batch: 070 ----
mean loss: 165.28
 ---- batch: 080 ----
mean loss: 166.28
 ---- batch: 090 ----
mean loss: 161.76
 ---- batch: 100 ----
mean loss: 173.16
 ---- batch: 110 ----
mean loss: 167.69
train mean loss: 164.81
epoch train time: 0:00:00.561936
elapsed time: 0:01:30.448917
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:24:19.266246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.37
 ---- batch: 020 ----
mean loss: 165.45
 ---- batch: 030 ----
mean loss: 162.92
 ---- batch: 040 ----
mean loss: 159.15
 ---- batch: 050 ----
mean loss: 167.83
 ---- batch: 060 ----
mean loss: 159.03
 ---- batch: 070 ----
mean loss: 164.08
 ---- batch: 080 ----
mean loss: 170.34
 ---- batch: 090 ----
mean loss: 166.57
 ---- batch: 100 ----
mean loss: 154.64
 ---- batch: 110 ----
mean loss: 166.56
train mean loss: 164.30
epoch train time: 0:00:00.564049
elapsed time: 0:01:31.013119
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:24:19.830470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.94
 ---- batch: 020 ----
mean loss: 164.24
 ---- batch: 030 ----
mean loss: 162.18
 ---- batch: 040 ----
mean loss: 164.95
 ---- batch: 050 ----
mean loss: 162.71
 ---- batch: 060 ----
mean loss: 170.73
 ---- batch: 070 ----
mean loss: 162.18
 ---- batch: 080 ----
mean loss: 172.31
 ---- batch: 090 ----
mean loss: 165.17
 ---- batch: 100 ----
mean loss: 165.71
 ---- batch: 110 ----
mean loss: 162.50
train mean loss: 165.47
epoch train time: 0:00:00.582024
elapsed time: 0:01:31.595303
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:24:20.412636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.61
 ---- batch: 020 ----
mean loss: 159.51
 ---- batch: 030 ----
mean loss: 161.08
 ---- batch: 040 ----
mean loss: 165.08
 ---- batch: 050 ----
mean loss: 157.87
 ---- batch: 060 ----
mean loss: 171.13
 ---- batch: 070 ----
mean loss: 163.30
 ---- batch: 080 ----
mean loss: 162.57
 ---- batch: 090 ----
mean loss: 157.14
 ---- batch: 100 ----
mean loss: 167.96
 ---- batch: 110 ----
mean loss: 172.32
train mean loss: 164.27
epoch train time: 0:00:00.586409
elapsed time: 0:01:32.181856
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:24:20.999190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.89
 ---- batch: 020 ----
mean loss: 161.94
 ---- batch: 030 ----
mean loss: 167.14
 ---- batch: 040 ----
mean loss: 161.72
 ---- batch: 050 ----
mean loss: 161.16
 ---- batch: 060 ----
mean loss: 162.66
 ---- batch: 070 ----
mean loss: 158.30
 ---- batch: 080 ----
mean loss: 161.13
 ---- batch: 090 ----
mean loss: 166.45
 ---- batch: 100 ----
mean loss: 169.87
 ---- batch: 110 ----
mean loss: 176.06
train mean loss: 163.34
epoch train time: 0:00:00.575927
elapsed time: 0:01:32.757951
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:24:21.575288
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.54
 ---- batch: 020 ----
mean loss: 161.94
 ---- batch: 030 ----
mean loss: 162.10
 ---- batch: 040 ----
mean loss: 158.65
 ---- batch: 050 ----
mean loss: 159.12
 ---- batch: 060 ----
mean loss: 166.38
 ---- batch: 070 ----
mean loss: 162.91
 ---- batch: 080 ----
mean loss: 162.77
 ---- batch: 090 ----
mean loss: 164.66
 ---- batch: 100 ----
mean loss: 170.62
 ---- batch: 110 ----
mean loss: 163.93
train mean loss: 163.87
epoch train time: 0:00:00.583138
elapsed time: 0:01:33.341246
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:24:22.158568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.48
 ---- batch: 020 ----
mean loss: 168.14
 ---- batch: 030 ----
mean loss: 158.58
 ---- batch: 040 ----
mean loss: 158.80
 ---- batch: 050 ----
mean loss: 162.56
 ---- batch: 060 ----
mean loss: 159.70
 ---- batch: 070 ----
mean loss: 160.90
 ---- batch: 080 ----
mean loss: 169.95
 ---- batch: 090 ----
mean loss: 169.26
 ---- batch: 100 ----
mean loss: 161.16
 ---- batch: 110 ----
mean loss: 166.35
train mean loss: 162.87
epoch train time: 0:00:00.563373
elapsed time: 0:01:33.904768
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:24:22.722102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.79
 ---- batch: 020 ----
mean loss: 165.63
 ---- batch: 030 ----
mean loss: 161.64
 ---- batch: 040 ----
mean loss: 160.13
 ---- batch: 050 ----
mean loss: 166.64
 ---- batch: 060 ----
mean loss: 159.17
 ---- batch: 070 ----
mean loss: 171.00
 ---- batch: 080 ----
mean loss: 169.07
 ---- batch: 090 ----
mean loss: 168.76
 ---- batch: 100 ----
mean loss: 155.68
 ---- batch: 110 ----
mean loss: 164.50
train mean loss: 164.00
epoch train time: 0:00:00.576589
elapsed time: 0:01:34.481519
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:24:23.298867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.25
 ---- batch: 020 ----
mean loss: 163.68
 ---- batch: 030 ----
mean loss: 150.79
 ---- batch: 040 ----
mean loss: 156.62
 ---- batch: 050 ----
mean loss: 164.38
 ---- batch: 060 ----
mean loss: 157.95
 ---- batch: 070 ----
mean loss: 167.70
 ---- batch: 080 ----
mean loss: 166.00
 ---- batch: 090 ----
mean loss: 167.52
 ---- batch: 100 ----
mean loss: 171.00
 ---- batch: 110 ----
mean loss: 163.69
train mean loss: 162.90
epoch train time: 0:00:00.568916
elapsed time: 0:01:35.050603
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:24:23.867947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.07
 ---- batch: 020 ----
mean loss: 154.56
 ---- batch: 030 ----
mean loss: 163.38
 ---- batch: 040 ----
mean loss: 156.57
 ---- batch: 050 ----
mean loss: 162.11
 ---- batch: 060 ----
mean loss: 159.67
 ---- batch: 070 ----
mean loss: 169.50
 ---- batch: 080 ----
mean loss: 165.61
 ---- batch: 090 ----
mean loss: 157.92
 ---- batch: 100 ----
mean loss: 164.24
 ---- batch: 110 ----
mean loss: 164.52
train mean loss: 161.60
epoch train time: 0:00:00.567807
elapsed time: 0:01:35.618561
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:24:24.435893
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.25
 ---- batch: 020 ----
mean loss: 158.89
 ---- batch: 030 ----
mean loss: 156.93
 ---- batch: 040 ----
mean loss: 151.67
 ---- batch: 050 ----
mean loss: 165.59
 ---- batch: 060 ----
mean loss: 160.67
 ---- batch: 070 ----
mean loss: 162.42
 ---- batch: 080 ----
mean loss: 166.13
 ---- batch: 090 ----
mean loss: 158.53
 ---- batch: 100 ----
mean loss: 161.83
 ---- batch: 110 ----
mean loss: 166.47
train mean loss: 161.26
epoch train time: 0:00:00.569528
elapsed time: 0:01:36.188227
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:24:25.005567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.58
 ---- batch: 020 ----
mean loss: 153.36
 ---- batch: 030 ----
mean loss: 158.91
 ---- batch: 040 ----
mean loss: 162.99
 ---- batch: 050 ----
mean loss: 155.67
 ---- batch: 060 ----
mean loss: 159.80
 ---- batch: 070 ----
mean loss: 163.33
 ---- batch: 080 ----
mean loss: 169.98
 ---- batch: 090 ----
mean loss: 169.99
 ---- batch: 100 ----
mean loss: 157.20
 ---- batch: 110 ----
mean loss: 165.21
train mean loss: 161.76
epoch train time: 0:00:00.559329
elapsed time: 0:01:36.747714
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:24:25.565043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.90
 ---- batch: 020 ----
mean loss: 154.23
 ---- batch: 030 ----
mean loss: 161.48
 ---- batch: 040 ----
mean loss: 151.72
 ---- batch: 050 ----
mean loss: 163.43
 ---- batch: 060 ----
mean loss: 159.28
 ---- batch: 070 ----
mean loss: 153.32
 ---- batch: 080 ----
mean loss: 168.39
 ---- batch: 090 ----
mean loss: 162.80
 ---- batch: 100 ----
mean loss: 162.89
 ---- batch: 110 ----
mean loss: 169.42
train mean loss: 160.99
epoch train time: 0:00:00.586705
elapsed time: 0:01:37.334555
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:24:26.151891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.38
 ---- batch: 020 ----
mean loss: 171.53
 ---- batch: 030 ----
mean loss: 157.80
 ---- batch: 040 ----
mean loss: 161.72
 ---- batch: 050 ----
mean loss: 165.72
 ---- batch: 060 ----
mean loss: 171.65
 ---- batch: 070 ----
mean loss: 155.00
 ---- batch: 080 ----
mean loss: 153.53
 ---- batch: 090 ----
mean loss: 161.40
 ---- batch: 100 ----
mean loss: 163.56
 ---- batch: 110 ----
mean loss: 157.38
train mean loss: 161.65
epoch train time: 0:00:00.575804
elapsed time: 0:01:37.910503
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:24:26.727842
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.85
 ---- batch: 020 ----
mean loss: 163.52
 ---- batch: 030 ----
mean loss: 157.32
 ---- batch: 040 ----
mean loss: 153.47
 ---- batch: 050 ----
mean loss: 161.49
 ---- batch: 060 ----
mean loss: 159.00
 ---- batch: 070 ----
mean loss: 161.32
 ---- batch: 080 ----
mean loss: 164.73
 ---- batch: 090 ----
mean loss: 160.27
 ---- batch: 100 ----
mean loss: 153.41
 ---- batch: 110 ----
mean loss: 153.46
train mean loss: 159.62
epoch train time: 0:00:00.562873
elapsed time: 0:01:38.473518
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:24:27.290920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.20
 ---- batch: 020 ----
mean loss: 159.55
 ---- batch: 030 ----
mean loss: 157.72
 ---- batch: 040 ----
mean loss: 161.08
 ---- batch: 050 ----
mean loss: 161.20
 ---- batch: 060 ----
mean loss: 164.74
 ---- batch: 070 ----
mean loss: 169.33
 ---- batch: 080 ----
mean loss: 166.70
 ---- batch: 090 ----
mean loss: 156.32
 ---- batch: 100 ----
mean loss: 163.51
 ---- batch: 110 ----
mean loss: 155.56
train mean loss: 160.79
epoch train time: 0:00:00.564880
elapsed time: 0:01:39.038667
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:24:27.855999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.03
 ---- batch: 020 ----
mean loss: 161.10
 ---- batch: 030 ----
mean loss: 159.44
 ---- batch: 040 ----
mean loss: 156.98
 ---- batch: 050 ----
mean loss: 158.81
 ---- batch: 060 ----
mean loss: 158.87
 ---- batch: 070 ----
mean loss: 162.22
 ---- batch: 080 ----
mean loss: 161.42
 ---- batch: 090 ----
mean loss: 161.61
 ---- batch: 100 ----
mean loss: 159.89
 ---- batch: 110 ----
mean loss: 156.57
train mean loss: 160.12
epoch train time: 0:00:00.567475
elapsed time: 0:01:39.606290
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:24:28.423628
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.77
 ---- batch: 020 ----
mean loss: 156.76
 ---- batch: 030 ----
mean loss: 157.34
 ---- batch: 040 ----
mean loss: 156.35
 ---- batch: 050 ----
mean loss: 161.40
 ---- batch: 060 ----
mean loss: 157.41
 ---- batch: 070 ----
mean loss: 162.31
 ---- batch: 080 ----
mean loss: 165.80
 ---- batch: 090 ----
mean loss: 163.58
 ---- batch: 100 ----
mean loss: 157.24
 ---- batch: 110 ----
mean loss: 160.06
train mean loss: 159.60
epoch train time: 0:00:00.577550
elapsed time: 0:01:40.183992
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:24:29.001325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.64
 ---- batch: 020 ----
mean loss: 151.29
 ---- batch: 030 ----
mean loss: 156.51
 ---- batch: 040 ----
mean loss: 157.60
 ---- batch: 050 ----
mean loss: 162.80
 ---- batch: 060 ----
mean loss: 160.16
 ---- batch: 070 ----
mean loss: 161.81
 ---- batch: 080 ----
mean loss: 159.47
 ---- batch: 090 ----
mean loss: 165.95
 ---- batch: 100 ----
mean loss: 159.80
 ---- batch: 110 ----
mean loss: 166.29
train mean loss: 159.69
epoch train time: 0:00:00.578579
elapsed time: 0:01:40.762710
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:24:29.580040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.51
 ---- batch: 020 ----
mean loss: 149.38
 ---- batch: 030 ----
mean loss: 164.75
 ---- batch: 040 ----
mean loss: 159.60
 ---- batch: 050 ----
mean loss: 156.59
 ---- batch: 060 ----
mean loss: 157.39
 ---- batch: 070 ----
mean loss: 165.29
 ---- batch: 080 ----
mean loss: 154.28
 ---- batch: 090 ----
mean loss: 167.10
 ---- batch: 100 ----
mean loss: 157.66
 ---- batch: 110 ----
mean loss: 158.23
train mean loss: 158.80
epoch train time: 0:00:00.560659
elapsed time: 0:01:41.323502
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:24:30.140834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.20
 ---- batch: 020 ----
mean loss: 153.06
 ---- batch: 030 ----
mean loss: 156.66
 ---- batch: 040 ----
mean loss: 154.42
 ---- batch: 050 ----
mean loss: 146.16
 ---- batch: 060 ----
mean loss: 156.21
 ---- batch: 070 ----
mean loss: 159.22
 ---- batch: 080 ----
mean loss: 163.55
 ---- batch: 090 ----
mean loss: 161.54
 ---- batch: 100 ----
mean loss: 169.05
 ---- batch: 110 ----
mean loss: 162.55
train mean loss: 158.27
epoch train time: 0:00:00.558456
elapsed time: 0:01:41.882094
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:24:30.699425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.47
 ---- batch: 020 ----
mean loss: 165.47
 ---- batch: 030 ----
mean loss: 152.13
 ---- batch: 040 ----
mean loss: 159.43
 ---- batch: 050 ----
mean loss: 163.90
 ---- batch: 060 ----
mean loss: 163.33
 ---- batch: 070 ----
mean loss: 164.32
 ---- batch: 080 ----
mean loss: 166.11
 ---- batch: 090 ----
mean loss: 154.20
 ---- batch: 100 ----
mean loss: 164.35
 ---- batch: 110 ----
mean loss: 152.18
train mean loss: 160.12
epoch train time: 0:00:00.571173
elapsed time: 0:01:42.453412
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:24:31.270769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.58
 ---- batch: 020 ----
mean loss: 152.17
 ---- batch: 030 ----
mean loss: 154.37
 ---- batch: 040 ----
mean loss: 164.51
 ---- batch: 050 ----
mean loss: 156.15
 ---- batch: 060 ----
mean loss: 155.69
 ---- batch: 070 ----
mean loss: 156.39
 ---- batch: 080 ----
mean loss: 163.47
 ---- batch: 090 ----
mean loss: 165.78
 ---- batch: 100 ----
mean loss: 156.46
 ---- batch: 110 ----
mean loss: 150.77
train mean loss: 157.29
epoch train time: 0:00:00.566297
elapsed time: 0:01:43.019889
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:24:31.837224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.89
 ---- batch: 020 ----
mean loss: 145.70
 ---- batch: 030 ----
mean loss: 159.17
 ---- batch: 040 ----
mean loss: 156.26
 ---- batch: 050 ----
mean loss: 164.68
 ---- batch: 060 ----
mean loss: 161.42
 ---- batch: 070 ----
mean loss: 165.97
 ---- batch: 080 ----
mean loss: 155.62
 ---- batch: 090 ----
mean loss: 158.50
 ---- batch: 100 ----
mean loss: 156.64
 ---- batch: 110 ----
mean loss: 153.42
train mean loss: 157.69
epoch train time: 0:00:00.563979
elapsed time: 0:01:43.584003
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:24:32.401331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.81
 ---- batch: 020 ----
mean loss: 150.36
 ---- batch: 030 ----
mean loss: 159.58
 ---- batch: 040 ----
mean loss: 164.25
 ---- batch: 050 ----
mean loss: 153.22
 ---- batch: 060 ----
mean loss: 161.43
 ---- batch: 070 ----
mean loss: 165.76
 ---- batch: 080 ----
mean loss: 163.41
 ---- batch: 090 ----
mean loss: 154.98
 ---- batch: 100 ----
mean loss: 156.79
 ---- batch: 110 ----
mean loss: 158.43
train mean loss: 158.51
epoch train time: 0:00:00.570754
elapsed time: 0:01:44.154901
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:24:32.972250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.55
 ---- batch: 020 ----
mean loss: 151.56
 ---- batch: 030 ----
mean loss: 157.15
 ---- batch: 040 ----
mean loss: 158.25
 ---- batch: 050 ----
mean loss: 159.06
 ---- batch: 060 ----
mean loss: 152.75
 ---- batch: 070 ----
mean loss: 156.46
 ---- batch: 080 ----
mean loss: 161.03
 ---- batch: 090 ----
mean loss: 160.94
 ---- batch: 100 ----
mean loss: 160.07
 ---- batch: 110 ----
mean loss: 158.50
train mean loss: 157.67
epoch train time: 0:00:00.567843
elapsed time: 0:01:44.722925
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:24:33.540248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.70
 ---- batch: 020 ----
mean loss: 154.24
 ---- batch: 030 ----
mean loss: 153.59
 ---- batch: 040 ----
mean loss: 140.35
 ---- batch: 050 ----
mean loss: 170.11
 ---- batch: 060 ----
mean loss: 161.72
 ---- batch: 070 ----
mean loss: 156.95
 ---- batch: 080 ----
mean loss: 157.24
 ---- batch: 090 ----
mean loss: 157.17
 ---- batch: 100 ----
mean loss: 156.33
 ---- batch: 110 ----
mean loss: 158.09
train mean loss: 156.93
epoch train time: 0:00:00.564090
elapsed time: 0:01:45.287179
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:24:34.104510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.81
 ---- batch: 020 ----
mean loss: 145.21
 ---- batch: 030 ----
mean loss: 154.63
 ---- batch: 040 ----
mean loss: 154.13
 ---- batch: 050 ----
mean loss: 156.81
 ---- batch: 060 ----
mean loss: 155.08
 ---- batch: 070 ----
mean loss: 158.68
 ---- batch: 080 ----
mean loss: 165.56
 ---- batch: 090 ----
mean loss: 152.76
 ---- batch: 100 ----
mean loss: 167.15
 ---- batch: 110 ----
mean loss: 151.27
train mean loss: 156.99
epoch train time: 0:00:00.563927
elapsed time: 0:01:45.851259
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:24:34.668599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.23
 ---- batch: 020 ----
mean loss: 162.70
 ---- batch: 030 ----
mean loss: 154.31
 ---- batch: 040 ----
mean loss: 153.98
 ---- batch: 050 ----
mean loss: 157.99
 ---- batch: 060 ----
mean loss: 157.53
 ---- batch: 070 ----
mean loss: 146.91
 ---- batch: 080 ----
mean loss: 154.71
 ---- batch: 090 ----
mean loss: 158.47
 ---- batch: 100 ----
mean loss: 156.61
 ---- batch: 110 ----
mean loss: 162.70
train mean loss: 156.52
epoch train time: 0:00:00.566724
elapsed time: 0:01:46.418136
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:24:35.235467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.15
 ---- batch: 020 ----
mean loss: 151.36
 ---- batch: 030 ----
mean loss: 156.67
 ---- batch: 040 ----
mean loss: 162.46
 ---- batch: 050 ----
mean loss: 152.54
 ---- batch: 060 ----
mean loss: 151.63
 ---- batch: 070 ----
mean loss: 162.14
 ---- batch: 080 ----
mean loss: 158.76
 ---- batch: 090 ----
mean loss: 163.85
 ---- batch: 100 ----
mean loss: 152.24
 ---- batch: 110 ----
mean loss: 153.61
train mean loss: 156.06
epoch train time: 0:00:00.566464
elapsed time: 0:01:46.984749
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:24:35.802090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.69
 ---- batch: 020 ----
mean loss: 155.09
 ---- batch: 030 ----
mean loss: 159.50
 ---- batch: 040 ----
mean loss: 157.82
 ---- batch: 050 ----
mean loss: 153.09
 ---- batch: 060 ----
mean loss: 154.01
 ---- batch: 070 ----
mean loss: 156.63
 ---- batch: 080 ----
mean loss: 152.47
 ---- batch: 090 ----
mean loss: 153.42
 ---- batch: 100 ----
mean loss: 158.65
 ---- batch: 110 ----
mean loss: 159.30
train mean loss: 155.98
epoch train time: 0:00:00.577914
elapsed time: 0:01:47.562807
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:24:36.380140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.71
 ---- batch: 020 ----
mean loss: 158.31
 ---- batch: 030 ----
mean loss: 154.04
 ---- batch: 040 ----
mean loss: 157.23
 ---- batch: 050 ----
mean loss: 152.89
 ---- batch: 060 ----
mean loss: 160.21
 ---- batch: 070 ----
mean loss: 158.03
 ---- batch: 080 ----
mean loss: 150.40
 ---- batch: 090 ----
mean loss: 147.25
 ---- batch: 100 ----
mean loss: 152.63
 ---- batch: 110 ----
mean loss: 156.04
train mean loss: 154.83
epoch train time: 0:00:00.578621
elapsed time: 0:01:48.141570
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:24:36.958902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.92
 ---- batch: 020 ----
mean loss: 147.97
 ---- batch: 030 ----
mean loss: 152.95
 ---- batch: 040 ----
mean loss: 152.29
 ---- batch: 050 ----
mean loss: 155.57
 ---- batch: 060 ----
mean loss: 147.68
 ---- batch: 070 ----
mean loss: 154.66
 ---- batch: 080 ----
mean loss: 153.60
 ---- batch: 090 ----
mean loss: 168.63
 ---- batch: 100 ----
mean loss: 148.29
 ---- batch: 110 ----
mean loss: 159.56
train mean loss: 155.13
epoch train time: 0:00:00.564236
elapsed time: 0:01:48.705942
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:24:37.523272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.19
 ---- batch: 020 ----
mean loss: 158.01
 ---- batch: 030 ----
mean loss: 164.44
 ---- batch: 040 ----
mean loss: 157.90
 ---- batch: 050 ----
mean loss: 157.56
 ---- batch: 060 ----
mean loss: 155.15
 ---- batch: 070 ----
mean loss: 152.60
 ---- batch: 080 ----
mean loss: 147.95
 ---- batch: 090 ----
mean loss: 150.76
 ---- batch: 100 ----
mean loss: 162.93
 ---- batch: 110 ----
mean loss: 155.15
train mean loss: 155.88
epoch train time: 0:00:00.579829
elapsed time: 0:01:49.285908
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:24:38.103238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.14
 ---- batch: 020 ----
mean loss: 157.62
 ---- batch: 030 ----
mean loss: 160.55
 ---- batch: 040 ----
mean loss: 161.31
 ---- batch: 050 ----
mean loss: 146.29
 ---- batch: 060 ----
mean loss: 157.98
 ---- batch: 070 ----
mean loss: 152.58
 ---- batch: 080 ----
mean loss: 152.36
 ---- batch: 090 ----
mean loss: 154.36
 ---- batch: 100 ----
mean loss: 149.54
 ---- batch: 110 ----
mean loss: 159.63
train mean loss: 155.34
epoch train time: 0:00:00.570978
elapsed time: 0:01:49.857023
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:24:38.674373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.21
 ---- batch: 020 ----
mean loss: 155.71
 ---- batch: 030 ----
mean loss: 155.68
 ---- batch: 040 ----
mean loss: 152.42
 ---- batch: 050 ----
mean loss: 149.39
 ---- batch: 060 ----
mean loss: 152.25
 ---- batch: 070 ----
mean loss: 159.93
 ---- batch: 080 ----
mean loss: 153.54
 ---- batch: 090 ----
mean loss: 155.56
 ---- batch: 100 ----
mean loss: 150.90
 ---- batch: 110 ----
mean loss: 153.43
train mean loss: 154.61
epoch train time: 0:00:00.570227
elapsed time: 0:01:50.427408
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:24:39.244739
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.07
 ---- batch: 020 ----
mean loss: 155.09
 ---- batch: 030 ----
mean loss: 152.62
 ---- batch: 040 ----
mean loss: 152.67
 ---- batch: 050 ----
mean loss: 157.14
 ---- batch: 060 ----
mean loss: 158.08
 ---- batch: 070 ----
mean loss: 149.01
 ---- batch: 080 ----
mean loss: 155.54
 ---- batch: 090 ----
mean loss: 145.40
 ---- batch: 100 ----
mean loss: 161.81
 ---- batch: 110 ----
mean loss: 161.30
train mean loss: 153.84
epoch train time: 0:00:00.569438
elapsed time: 0:01:50.996997
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:24:39.814328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.57
 ---- batch: 020 ----
mean loss: 159.43
 ---- batch: 030 ----
mean loss: 149.76
 ---- batch: 040 ----
mean loss: 159.97
 ---- batch: 050 ----
mean loss: 144.91
 ---- batch: 060 ----
mean loss: 158.50
 ---- batch: 070 ----
mean loss: 149.42
 ---- batch: 080 ----
mean loss: 152.71
 ---- batch: 090 ----
mean loss: 154.12
 ---- batch: 100 ----
mean loss: 157.45
 ---- batch: 110 ----
mean loss: 151.18
train mean loss: 154.42
epoch train time: 0:00:00.565225
elapsed time: 0:01:51.562363
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:24:40.379728
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.73
 ---- batch: 020 ----
mean loss: 154.83
 ---- batch: 030 ----
mean loss: 155.79
 ---- batch: 040 ----
mean loss: 161.69
 ---- batch: 050 ----
mean loss: 159.52
 ---- batch: 060 ----
mean loss: 154.95
 ---- batch: 070 ----
mean loss: 153.13
 ---- batch: 080 ----
mean loss: 148.44
 ---- batch: 090 ----
mean loss: 154.39
 ---- batch: 100 ----
mean loss: 144.94
 ---- batch: 110 ----
mean loss: 159.65
train mean loss: 154.19
epoch train time: 0:00:00.568330
elapsed time: 0:01:52.130864
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:24:40.948213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.93
 ---- batch: 020 ----
mean loss: 151.78
 ---- batch: 030 ----
mean loss: 156.94
 ---- batch: 040 ----
mean loss: 159.28
 ---- batch: 050 ----
mean loss: 156.63
 ---- batch: 060 ----
mean loss: 147.57
 ---- batch: 070 ----
mean loss: 153.22
 ---- batch: 080 ----
mean loss: 156.61
 ---- batch: 090 ----
mean loss: 163.05
 ---- batch: 100 ----
mean loss: 151.62
 ---- batch: 110 ----
mean loss: 150.55
train mean loss: 153.94
epoch train time: 0:00:00.572054
elapsed time: 0:01:52.703067
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:24:41.520418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.92
 ---- batch: 020 ----
mean loss: 154.52
 ---- batch: 030 ----
mean loss: 153.20
 ---- batch: 040 ----
mean loss: 148.29
 ---- batch: 050 ----
mean loss: 150.27
 ---- batch: 060 ----
mean loss: 153.51
 ---- batch: 070 ----
mean loss: 152.45
 ---- batch: 080 ----
mean loss: 156.54
 ---- batch: 090 ----
mean loss: 157.83
 ---- batch: 100 ----
mean loss: 154.39
 ---- batch: 110 ----
mean loss: 149.37
train mean loss: 153.51
epoch train time: 0:00:00.570435
elapsed time: 0:01:53.273681
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:24:42.091029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.93
 ---- batch: 020 ----
mean loss: 158.34
 ---- batch: 030 ----
mean loss: 150.59
 ---- batch: 040 ----
mean loss: 150.92
 ---- batch: 050 ----
mean loss: 150.64
 ---- batch: 060 ----
mean loss: 153.31
 ---- batch: 070 ----
mean loss: 150.39
 ---- batch: 080 ----
mean loss: 155.94
 ---- batch: 090 ----
mean loss: 148.45
 ---- batch: 100 ----
mean loss: 160.54
 ---- batch: 110 ----
mean loss: 164.24
train mean loss: 154.36
epoch train time: 0:00:00.570925
elapsed time: 0:01:53.844776
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:24:42.662106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.53
 ---- batch: 020 ----
mean loss: 145.98
 ---- batch: 030 ----
mean loss: 150.78
 ---- batch: 040 ----
mean loss: 158.73
 ---- batch: 050 ----
mean loss: 154.19
 ---- batch: 060 ----
mean loss: 147.08
 ---- batch: 070 ----
mean loss: 160.22
 ---- batch: 080 ----
mean loss: 160.35
 ---- batch: 090 ----
mean loss: 160.40
 ---- batch: 100 ----
mean loss: 157.26
 ---- batch: 110 ----
mean loss: 142.59
train mean loss: 153.50
epoch train time: 0:00:00.570037
elapsed time: 0:01:54.414959
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:24:43.232287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.95
 ---- batch: 020 ----
mean loss: 145.00
 ---- batch: 030 ----
mean loss: 161.33
 ---- batch: 040 ----
mean loss: 154.57
 ---- batch: 050 ----
mean loss: 151.06
 ---- batch: 060 ----
mean loss: 154.53
 ---- batch: 070 ----
mean loss: 147.91
 ---- batch: 080 ----
mean loss: 152.98
 ---- batch: 090 ----
mean loss: 153.50
 ---- batch: 100 ----
mean loss: 155.42
 ---- batch: 110 ----
mean loss: 152.07
train mean loss: 152.37
epoch train time: 0:00:00.567694
elapsed time: 0:01:54.982785
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:24:43.800114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.37
 ---- batch: 020 ----
mean loss: 156.77
 ---- batch: 030 ----
mean loss: 154.86
 ---- batch: 040 ----
mean loss: 151.67
 ---- batch: 050 ----
mean loss: 157.88
 ---- batch: 060 ----
mean loss: 152.19
 ---- batch: 070 ----
mean loss: 147.82
 ---- batch: 080 ----
mean loss: 155.26
 ---- batch: 090 ----
mean loss: 152.00
 ---- batch: 100 ----
mean loss: 148.98
 ---- batch: 110 ----
mean loss: 153.32
train mean loss: 152.34
epoch train time: 0:00:00.566418
elapsed time: 0:01:55.549344
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:24:44.366676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.85
 ---- batch: 020 ----
mean loss: 151.85
 ---- batch: 030 ----
mean loss: 146.58
 ---- batch: 040 ----
mean loss: 153.01
 ---- batch: 050 ----
mean loss: 144.23
 ---- batch: 060 ----
mean loss: 153.83
 ---- batch: 070 ----
mean loss: 157.79
 ---- batch: 080 ----
mean loss: 147.86
 ---- batch: 090 ----
mean loss: 147.84
 ---- batch: 100 ----
mean loss: 150.77
 ---- batch: 110 ----
mean loss: 158.00
train mean loss: 151.72
epoch train time: 0:00:00.561834
elapsed time: 0:01:56.111312
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:24:44.928640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.65
 ---- batch: 020 ----
mean loss: 152.24
 ---- batch: 030 ----
mean loss: 148.23
 ---- batch: 040 ----
mean loss: 147.92
 ---- batch: 050 ----
mean loss: 154.25
 ---- batch: 060 ----
mean loss: 149.62
 ---- batch: 070 ----
mean loss: 153.32
 ---- batch: 080 ----
mean loss: 151.20
 ---- batch: 090 ----
mean loss: 157.42
 ---- batch: 100 ----
mean loss: 159.35
 ---- batch: 110 ----
mean loss: 147.65
train mean loss: 152.16
epoch train time: 0:00:00.564400
elapsed time: 0:01:56.675876
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:24:45.493205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.58
 ---- batch: 020 ----
mean loss: 154.34
 ---- batch: 030 ----
mean loss: 155.34
 ---- batch: 040 ----
mean loss: 149.40
 ---- batch: 050 ----
mean loss: 149.47
 ---- batch: 060 ----
mean loss: 147.13
 ---- batch: 070 ----
mean loss: 152.12
 ---- batch: 080 ----
mean loss: 160.07
 ---- batch: 090 ----
mean loss: 152.45
 ---- batch: 100 ----
mean loss: 160.01
 ---- batch: 110 ----
mean loss: 149.85
train mean loss: 151.88
epoch train time: 0:00:00.568452
elapsed time: 0:01:57.244478
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:24:46.061817
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.72
 ---- batch: 020 ----
mean loss: 148.85
 ---- batch: 030 ----
mean loss: 148.10
 ---- batch: 040 ----
mean loss: 140.07
 ---- batch: 050 ----
mean loss: 147.03
 ---- batch: 060 ----
mean loss: 154.32
 ---- batch: 070 ----
mean loss: 156.70
 ---- batch: 080 ----
mean loss: 154.29
 ---- batch: 090 ----
mean loss: 153.33
 ---- batch: 100 ----
mean loss: 158.01
 ---- batch: 110 ----
mean loss: 152.40
train mean loss: 151.88
epoch train time: 0:00:00.570491
elapsed time: 0:01:57.815113
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:24:46.632445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.35
 ---- batch: 020 ----
mean loss: 145.15
 ---- batch: 030 ----
mean loss: 153.21
 ---- batch: 040 ----
mean loss: 145.80
 ---- batch: 050 ----
mean loss: 150.35
 ---- batch: 060 ----
mean loss: 159.49
 ---- batch: 070 ----
mean loss: 148.68
 ---- batch: 080 ----
mean loss: 158.63
 ---- batch: 090 ----
mean loss: 156.27
 ---- batch: 100 ----
mean loss: 145.55
 ---- batch: 110 ----
mean loss: 153.72
train mean loss: 151.30
epoch train time: 0:00:00.564776
elapsed time: 0:01:58.380042
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:24:47.197373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.49
 ---- batch: 020 ----
mean loss: 145.32
 ---- batch: 030 ----
mean loss: 155.93
 ---- batch: 040 ----
mean loss: 148.25
 ---- batch: 050 ----
mean loss: 146.82
 ---- batch: 060 ----
mean loss: 150.11
 ---- batch: 070 ----
mean loss: 156.79
 ---- batch: 080 ----
mean loss: 153.30
 ---- batch: 090 ----
mean loss: 155.91
 ---- batch: 100 ----
mean loss: 157.54
 ---- batch: 110 ----
mean loss: 146.10
train mean loss: 151.92
epoch train time: 0:00:00.569204
elapsed time: 0:01:58.949380
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:24:47.766708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.86
 ---- batch: 020 ----
mean loss: 147.33
 ---- batch: 030 ----
mean loss: 148.93
 ---- batch: 040 ----
mean loss: 142.69
 ---- batch: 050 ----
mean loss: 150.78
 ---- batch: 060 ----
mean loss: 147.40
 ---- batch: 070 ----
mean loss: 147.39
 ---- batch: 080 ----
mean loss: 155.80
 ---- batch: 090 ----
mean loss: 151.51
 ---- batch: 100 ----
mean loss: 154.06
 ---- batch: 110 ----
mean loss: 150.10
train mean loss: 150.66
epoch train time: 0:00:00.568571
elapsed time: 0:01:59.518085
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:24:48.335414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.97
 ---- batch: 020 ----
mean loss: 139.52
 ---- batch: 030 ----
mean loss: 148.57
 ---- batch: 040 ----
mean loss: 151.08
 ---- batch: 050 ----
mean loss: 154.29
 ---- batch: 060 ----
mean loss: 154.62
 ---- batch: 070 ----
mean loss: 156.11
 ---- batch: 080 ----
mean loss: 149.00
 ---- batch: 090 ----
mean loss: 145.38
 ---- batch: 100 ----
mean loss: 155.22
 ---- batch: 110 ----
mean loss: 151.90
train mean loss: 150.97
epoch train time: 0:00:00.571556
elapsed time: 0:02:00.089775
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:24:48.907113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.47
 ---- batch: 020 ----
mean loss: 145.11
 ---- batch: 030 ----
mean loss: 148.66
 ---- batch: 040 ----
mean loss: 152.70
 ---- batch: 050 ----
mean loss: 152.48
 ---- batch: 060 ----
mean loss: 149.69
 ---- batch: 070 ----
mean loss: 151.26
 ---- batch: 080 ----
mean loss: 146.90
 ---- batch: 090 ----
mean loss: 157.05
 ---- batch: 100 ----
mean loss: 147.15
 ---- batch: 110 ----
mean loss: 155.71
train mean loss: 150.16
epoch train time: 0:00:00.564604
elapsed time: 0:02:00.654519
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:24:49.471848
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.25
 ---- batch: 020 ----
mean loss: 145.29
 ---- batch: 030 ----
mean loss: 150.12
 ---- batch: 040 ----
mean loss: 149.70
 ---- batch: 050 ----
mean loss: 148.70
 ---- batch: 060 ----
mean loss: 149.97
 ---- batch: 070 ----
mean loss: 157.99
 ---- batch: 080 ----
mean loss: 150.01
 ---- batch: 090 ----
mean loss: 151.00
 ---- batch: 100 ----
mean loss: 154.68
 ---- batch: 110 ----
mean loss: 157.94
train mean loss: 151.29
epoch train time: 0:00:00.572519
elapsed time: 0:02:01.227171
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:24:50.044536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.07
 ---- batch: 020 ----
mean loss: 145.74
 ---- batch: 030 ----
mean loss: 159.27
 ---- batch: 040 ----
mean loss: 159.52
 ---- batch: 050 ----
mean loss: 152.21
 ---- batch: 060 ----
mean loss: 153.59
 ---- batch: 070 ----
mean loss: 157.89
 ---- batch: 080 ----
mean loss: 153.64
 ---- batch: 090 ----
mean loss: 145.54
 ---- batch: 100 ----
mean loss: 154.55
 ---- batch: 110 ----
mean loss: 148.78
train mean loss: 152.63
epoch train time: 0:00:00.571759
elapsed time: 0:02:01.799101
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:24:50.616442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.43
 ---- batch: 020 ----
mean loss: 145.73
 ---- batch: 030 ----
mean loss: 152.05
 ---- batch: 040 ----
mean loss: 145.96
 ---- batch: 050 ----
mean loss: 138.61
 ---- batch: 060 ----
mean loss: 155.26
 ---- batch: 070 ----
mean loss: 141.07
 ---- batch: 080 ----
mean loss: 148.67
 ---- batch: 090 ----
mean loss: 160.71
 ---- batch: 100 ----
mean loss: 152.47
 ---- batch: 110 ----
mean loss: 160.36
train mean loss: 149.73
epoch train time: 0:00:00.573905
elapsed time: 0:02:02.373161
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:24:51.190491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.70
 ---- batch: 020 ----
mean loss: 146.57
 ---- batch: 030 ----
mean loss: 153.16
 ---- batch: 040 ----
mean loss: 146.38
 ---- batch: 050 ----
mean loss: 148.74
 ---- batch: 060 ----
mean loss: 144.45
 ---- batch: 070 ----
mean loss: 151.89
 ---- batch: 080 ----
mean loss: 150.56
 ---- batch: 090 ----
mean loss: 140.46
 ---- batch: 100 ----
mean loss: 156.85
 ---- batch: 110 ----
mean loss: 153.69
train mean loss: 149.87
epoch train time: 0:00:00.568330
elapsed time: 0:02:02.941647
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:24:51.758981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.02
 ---- batch: 020 ----
mean loss: 147.41
 ---- batch: 030 ----
mean loss: 142.63
 ---- batch: 040 ----
mean loss: 157.92
 ---- batch: 050 ----
mean loss: 150.86
 ---- batch: 060 ----
mean loss: 148.14
 ---- batch: 070 ----
mean loss: 149.75
 ---- batch: 080 ----
mean loss: 151.02
 ---- batch: 090 ----
mean loss: 146.67
 ---- batch: 100 ----
mean loss: 158.74
 ---- batch: 110 ----
mean loss: 141.27
train mean loss: 149.73
epoch train time: 0:00:00.566081
elapsed time: 0:02:03.507864
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:24:52.325192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.58
 ---- batch: 020 ----
mean loss: 146.87
 ---- batch: 030 ----
mean loss: 145.20
 ---- batch: 040 ----
mean loss: 157.73
 ---- batch: 050 ----
mean loss: 160.56
 ---- batch: 060 ----
mean loss: 146.00
 ---- batch: 070 ----
mean loss: 140.27
 ---- batch: 080 ----
mean loss: 157.73
 ---- batch: 090 ----
mean loss: 156.79
 ---- batch: 100 ----
mean loss: 139.88
 ---- batch: 110 ----
mean loss: 146.84
train mean loss: 149.53
epoch train time: 0:00:00.571561
elapsed time: 0:02:04.079587
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:24:52.896941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.10
 ---- batch: 020 ----
mean loss: 149.64
 ---- batch: 030 ----
mean loss: 147.59
 ---- batch: 040 ----
mean loss: 151.37
 ---- batch: 050 ----
mean loss: 155.52
 ---- batch: 060 ----
mean loss: 151.52
 ---- batch: 070 ----
mean loss: 150.03
 ---- batch: 080 ----
mean loss: 150.16
 ---- batch: 090 ----
mean loss: 148.55
 ---- batch: 100 ----
mean loss: 153.89
 ---- batch: 110 ----
mean loss: 147.45
train mean loss: 149.51
epoch train time: 0:00:00.567211
elapsed time: 0:02:04.646956
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:24:53.464317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.37
 ---- batch: 020 ----
mean loss: 153.68
 ---- batch: 030 ----
mean loss: 149.04
 ---- batch: 040 ----
mean loss: 144.36
 ---- batch: 050 ----
mean loss: 158.74
 ---- batch: 060 ----
mean loss: 150.10
 ---- batch: 070 ----
mean loss: 147.35
 ---- batch: 080 ----
mean loss: 131.38
 ---- batch: 090 ----
mean loss: 142.54
 ---- batch: 100 ----
mean loss: 154.50
 ---- batch: 110 ----
mean loss: 153.73
train mean loss: 148.83
epoch train time: 0:00:00.568653
elapsed time: 0:02:05.215818
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:24:54.033146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.94
 ---- batch: 020 ----
mean loss: 146.19
 ---- batch: 030 ----
mean loss: 154.31
 ---- batch: 040 ----
mean loss: 153.23
 ---- batch: 050 ----
mean loss: 152.39
 ---- batch: 060 ----
mean loss: 147.02
 ---- batch: 070 ----
mean loss: 149.86
 ---- batch: 080 ----
mean loss: 149.00
 ---- batch: 090 ----
mean loss: 146.64
 ---- batch: 100 ----
mean loss: 143.90
 ---- batch: 110 ----
mean loss: 148.63
train mean loss: 149.30
epoch train time: 0:00:00.568307
elapsed time: 0:02:05.784265
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:24:54.601598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.72
 ---- batch: 020 ----
mean loss: 154.27
 ---- batch: 030 ----
mean loss: 146.43
 ---- batch: 040 ----
mean loss: 148.06
 ---- batch: 050 ----
mean loss: 150.59
 ---- batch: 060 ----
mean loss: 143.71
 ---- batch: 070 ----
mean loss: 157.19
 ---- batch: 080 ----
mean loss: 147.93
 ---- batch: 090 ----
mean loss: 140.36
 ---- batch: 100 ----
mean loss: 152.84
 ---- batch: 110 ----
mean loss: 147.07
train mean loss: 148.59
epoch train time: 0:00:00.583899
elapsed time: 0:02:06.368316
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:24:55.185687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.67
 ---- batch: 020 ----
mean loss: 141.70
 ---- batch: 030 ----
mean loss: 146.07
 ---- batch: 040 ----
mean loss: 146.84
 ---- batch: 050 ----
mean loss: 163.43
 ---- batch: 060 ----
mean loss: 145.95
 ---- batch: 070 ----
mean loss: 151.41
 ---- batch: 080 ----
mean loss: 149.60
 ---- batch: 090 ----
mean loss: 152.91
 ---- batch: 100 ----
mean loss: 142.84
 ---- batch: 110 ----
mean loss: 138.44
train mean loss: 148.46
epoch train time: 0:00:00.580411
elapsed time: 0:02:06.948910
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:24:55.766241
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.09
 ---- batch: 020 ----
mean loss: 147.35
 ---- batch: 030 ----
mean loss: 142.55
 ---- batch: 040 ----
mean loss: 143.65
 ---- batch: 050 ----
mean loss: 145.27
 ---- batch: 060 ----
mean loss: 147.12
 ---- batch: 070 ----
mean loss: 138.19
 ---- batch: 080 ----
mean loss: 158.69
 ---- batch: 090 ----
mean loss: 152.21
 ---- batch: 100 ----
mean loss: 161.88
 ---- batch: 110 ----
mean loss: 142.50
train mean loss: 147.70
epoch train time: 0:00:00.599112
elapsed time: 0:02:07.548166
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:24:56.365498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.49
 ---- batch: 020 ----
mean loss: 140.08
 ---- batch: 030 ----
mean loss: 144.10
 ---- batch: 040 ----
mean loss: 141.99
 ---- batch: 050 ----
mean loss: 147.02
 ---- batch: 060 ----
mean loss: 144.94
 ---- batch: 070 ----
mean loss: 146.25
 ---- batch: 080 ----
mean loss: 142.98
 ---- batch: 090 ----
mean loss: 162.47
 ---- batch: 100 ----
mean loss: 154.66
 ---- batch: 110 ----
mean loss: 155.74
train mean loss: 147.82
epoch train time: 0:00:00.589562
elapsed time: 0:02:08.137907
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:24:56.955246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.24
 ---- batch: 020 ----
mean loss: 146.85
 ---- batch: 030 ----
mean loss: 142.70
 ---- batch: 040 ----
mean loss: 162.18
 ---- batch: 050 ----
mean loss: 158.05
 ---- batch: 060 ----
mean loss: 142.53
 ---- batch: 070 ----
mean loss: 136.71
 ---- batch: 080 ----
mean loss: 146.61
 ---- batch: 090 ----
mean loss: 150.21
 ---- batch: 100 ----
mean loss: 145.66
 ---- batch: 110 ----
mean loss: 151.64
train mean loss: 147.53
epoch train time: 0:00:00.588983
elapsed time: 0:02:08.727032
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:24:57.544396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.68
 ---- batch: 020 ----
mean loss: 147.33
 ---- batch: 030 ----
mean loss: 139.12
 ---- batch: 040 ----
mean loss: 153.38
 ---- batch: 050 ----
mean loss: 154.49
 ---- batch: 060 ----
mean loss: 149.48
 ---- batch: 070 ----
mean loss: 145.04
 ---- batch: 080 ----
mean loss: 158.48
 ---- batch: 090 ----
mean loss: 145.58
 ---- batch: 100 ----
mean loss: 153.03
 ---- batch: 110 ----
mean loss: 140.97
train mean loss: 147.88
epoch train time: 0:00:00.585333
elapsed time: 0:02:09.312551
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:24:58.129881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.04
 ---- batch: 020 ----
mean loss: 147.32
 ---- batch: 030 ----
mean loss: 148.77
 ---- batch: 040 ----
mean loss: 146.89
 ---- batch: 050 ----
mean loss: 148.38
 ---- batch: 060 ----
mean loss: 153.07
 ---- batch: 070 ----
mean loss: 144.99
 ---- batch: 080 ----
mean loss: 149.60
 ---- batch: 090 ----
mean loss: 156.01
 ---- batch: 100 ----
mean loss: 140.88
 ---- batch: 110 ----
mean loss: 144.92
train mean loss: 148.12
epoch train time: 0:00:00.577698
elapsed time: 0:02:09.890405
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:24:58.707736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.41
 ---- batch: 020 ----
mean loss: 154.44
 ---- batch: 030 ----
mean loss: 156.49
 ---- batch: 040 ----
mean loss: 144.51
 ---- batch: 050 ----
mean loss: 144.15
 ---- batch: 060 ----
mean loss: 142.71
 ---- batch: 070 ----
mean loss: 155.38
 ---- batch: 080 ----
mean loss: 143.51
 ---- batch: 090 ----
mean loss: 148.95
 ---- batch: 100 ----
mean loss: 151.38
 ---- batch: 110 ----
mean loss: 143.77
train mean loss: 147.53
epoch train time: 0:00:00.583575
elapsed time: 0:02:10.474116
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:24:59.291445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.03
 ---- batch: 020 ----
mean loss: 153.09
 ---- batch: 030 ----
mean loss: 146.37
 ---- batch: 040 ----
mean loss: 141.21
 ---- batch: 050 ----
mean loss: 147.84
 ---- batch: 060 ----
mean loss: 144.87
 ---- batch: 070 ----
mean loss: 142.90
 ---- batch: 080 ----
mean loss: 150.17
 ---- batch: 090 ----
mean loss: 150.70
 ---- batch: 100 ----
mean loss: 144.46
 ---- batch: 110 ----
mean loss: 155.40
train mean loss: 147.27
epoch train time: 0:00:00.584277
elapsed time: 0:02:11.058541
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:24:59.875898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.35
 ---- batch: 020 ----
mean loss: 143.93
 ---- batch: 030 ----
mean loss: 150.48
 ---- batch: 040 ----
mean loss: 147.37
 ---- batch: 050 ----
mean loss: 139.48
 ---- batch: 060 ----
mean loss: 146.67
 ---- batch: 070 ----
mean loss: 150.68
 ---- batch: 080 ----
mean loss: 137.99
 ---- batch: 090 ----
mean loss: 155.47
 ---- batch: 100 ----
mean loss: 138.37
 ---- batch: 110 ----
mean loss: 153.90
train mean loss: 146.80
epoch train time: 0:00:00.575022
elapsed time: 0:02:11.633760
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:25:00.451086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.10
 ---- batch: 020 ----
mean loss: 148.40
 ---- batch: 030 ----
mean loss: 143.10
 ---- batch: 040 ----
mean loss: 146.36
 ---- batch: 050 ----
mean loss: 154.00
 ---- batch: 060 ----
mean loss: 146.99
 ---- batch: 070 ----
mean loss: 146.69
 ---- batch: 080 ----
mean loss: 143.26
 ---- batch: 090 ----
mean loss: 152.60
 ---- batch: 100 ----
mean loss: 153.00
 ---- batch: 110 ----
mean loss: 147.89
train mean loss: 147.26
epoch train time: 0:00:00.562212
elapsed time: 0:02:12.196119
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:25:01.013464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.45
 ---- batch: 020 ----
mean loss: 143.19
 ---- batch: 030 ----
mean loss: 145.94
 ---- batch: 040 ----
mean loss: 145.80
 ---- batch: 050 ----
mean loss: 144.21
 ---- batch: 060 ----
mean loss: 147.03
 ---- batch: 070 ----
mean loss: 153.95
 ---- batch: 080 ----
mean loss: 147.63
 ---- batch: 090 ----
mean loss: 148.76
 ---- batch: 100 ----
mean loss: 145.81
 ---- batch: 110 ----
mean loss: 143.24
train mean loss: 147.01
epoch train time: 0:00:00.564852
elapsed time: 0:02:12.761122
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:25:01.578477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.21
 ---- batch: 020 ----
mean loss: 146.42
 ---- batch: 030 ----
mean loss: 139.42
 ---- batch: 040 ----
mean loss: 155.17
 ---- batch: 050 ----
mean loss: 146.08
 ---- batch: 060 ----
mean loss: 148.56
 ---- batch: 070 ----
mean loss: 145.68
 ---- batch: 080 ----
mean loss: 146.65
 ---- batch: 090 ----
mean loss: 142.19
 ---- batch: 100 ----
mean loss: 145.47
 ---- batch: 110 ----
mean loss: 146.67
train mean loss: 146.22
epoch train time: 0:00:00.561736
elapsed time: 0:02:13.323019
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:25:02.140364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.84
 ---- batch: 020 ----
mean loss: 150.91
 ---- batch: 030 ----
mean loss: 151.53
 ---- batch: 040 ----
mean loss: 140.11
 ---- batch: 050 ----
mean loss: 144.66
 ---- batch: 060 ----
mean loss: 145.44
 ---- batch: 070 ----
mean loss: 140.42
 ---- batch: 080 ----
mean loss: 146.84
 ---- batch: 090 ----
mean loss: 147.73
 ---- batch: 100 ----
mean loss: 151.41
 ---- batch: 110 ----
mean loss: 148.39
train mean loss: 146.75
epoch train time: 0:00:00.559321
elapsed time: 0:02:13.882490
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:25:02.699820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.70
 ---- batch: 020 ----
mean loss: 149.99
 ---- batch: 030 ----
mean loss: 147.99
 ---- batch: 040 ----
mean loss: 138.62
 ---- batch: 050 ----
mean loss: 145.43
 ---- batch: 060 ----
mean loss: 150.47
 ---- batch: 070 ----
mean loss: 138.61
 ---- batch: 080 ----
mean loss: 146.88
 ---- batch: 090 ----
mean loss: 148.74
 ---- batch: 100 ----
mean loss: 141.10
 ---- batch: 110 ----
mean loss: 149.73
train mean loss: 145.71
epoch train time: 0:00:00.564062
elapsed time: 0:02:14.446697
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:25:03.264029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.11
 ---- batch: 020 ----
mean loss: 142.85
 ---- batch: 030 ----
mean loss: 139.18
 ---- batch: 040 ----
mean loss: 151.31
 ---- batch: 050 ----
mean loss: 144.91
 ---- batch: 060 ----
mean loss: 145.35
 ---- batch: 070 ----
mean loss: 152.58
 ---- batch: 080 ----
mean loss: 149.69
 ---- batch: 090 ----
mean loss: 146.87
 ---- batch: 100 ----
mean loss: 147.02
 ---- batch: 110 ----
mean loss: 153.33
train mean loss: 146.51
epoch train time: 0:00:00.558990
elapsed time: 0:02:15.005832
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:25:03.823163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.32
 ---- batch: 020 ----
mean loss: 150.50
 ---- batch: 030 ----
mean loss: 137.03
 ---- batch: 040 ----
mean loss: 140.29
 ---- batch: 050 ----
mean loss: 155.06
 ---- batch: 060 ----
mean loss: 149.29
 ---- batch: 070 ----
mean loss: 151.65
 ---- batch: 080 ----
mean loss: 146.89
 ---- batch: 090 ----
mean loss: 141.82
 ---- batch: 100 ----
mean loss: 150.10
 ---- batch: 110 ----
mean loss: 144.80
train mean loss: 146.11
epoch train time: 0:00:00.559791
elapsed time: 0:02:15.565760
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:25:04.383090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.50
 ---- batch: 020 ----
mean loss: 146.49
 ---- batch: 030 ----
mean loss: 151.02
 ---- batch: 040 ----
mean loss: 131.57
 ---- batch: 050 ----
mean loss: 146.45
 ---- batch: 060 ----
mean loss: 136.10
 ---- batch: 070 ----
mean loss: 145.59
 ---- batch: 080 ----
mean loss: 150.71
 ---- batch: 090 ----
mean loss: 148.77
 ---- batch: 100 ----
mean loss: 152.17
 ---- batch: 110 ----
mean loss: 153.48
train mean loss: 145.82
epoch train time: 0:00:00.569273
elapsed time: 0:02:16.135172
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:25:04.952521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.51
 ---- batch: 020 ----
mean loss: 149.57
 ---- batch: 030 ----
mean loss: 149.60
 ---- batch: 040 ----
mean loss: 146.91
 ---- batch: 050 ----
mean loss: 145.83
 ---- batch: 060 ----
mean loss: 150.97
 ---- batch: 070 ----
mean loss: 143.51
 ---- batch: 080 ----
mean loss: 142.00
 ---- batch: 090 ----
mean loss: 144.18
 ---- batch: 100 ----
mean loss: 148.19
 ---- batch: 110 ----
mean loss: 144.60
train mean loss: 146.70
epoch train time: 0:00:00.558341
elapsed time: 0:02:16.693664
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:25:05.511002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.61
 ---- batch: 020 ----
mean loss: 147.41
 ---- batch: 030 ----
mean loss: 134.25
 ---- batch: 040 ----
mean loss: 145.97
 ---- batch: 050 ----
mean loss: 139.17
 ---- batch: 060 ----
mean loss: 139.09
 ---- batch: 070 ----
mean loss: 151.57
 ---- batch: 080 ----
mean loss: 146.02
 ---- batch: 090 ----
mean loss: 158.06
 ---- batch: 100 ----
mean loss: 145.38
 ---- batch: 110 ----
mean loss: 146.39
train mean loss: 145.55
epoch train time: 0:00:00.567382
elapsed time: 0:02:17.261213
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:25:06.078543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.87
 ---- batch: 020 ----
mean loss: 146.75
 ---- batch: 030 ----
mean loss: 142.71
 ---- batch: 040 ----
mean loss: 144.95
 ---- batch: 050 ----
mean loss: 148.75
 ---- batch: 060 ----
mean loss: 144.68
 ---- batch: 070 ----
mean loss: 144.96
 ---- batch: 080 ----
mean loss: 145.05
 ---- batch: 090 ----
mean loss: 140.55
 ---- batch: 100 ----
mean loss: 146.28
 ---- batch: 110 ----
mean loss: 154.55
train mean loss: 145.32
epoch train time: 0:00:00.565108
elapsed time: 0:02:17.826456
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:25:06.643792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.93
 ---- batch: 020 ----
mean loss: 141.81
 ---- batch: 030 ----
mean loss: 151.29
 ---- batch: 040 ----
mean loss: 144.55
 ---- batch: 050 ----
mean loss: 146.28
 ---- batch: 060 ----
mean loss: 148.53
 ---- batch: 070 ----
mean loss: 150.18
 ---- batch: 080 ----
mean loss: 137.05
 ---- batch: 090 ----
mean loss: 140.66
 ---- batch: 100 ----
mean loss: 148.14
 ---- batch: 110 ----
mean loss: 150.88
train mean loss: 145.58
epoch train time: 0:00:00.566948
elapsed time: 0:02:18.393546
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:25:07.210875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.38
 ---- batch: 020 ----
mean loss: 151.08
 ---- batch: 030 ----
mean loss: 152.06
 ---- batch: 040 ----
mean loss: 143.19
 ---- batch: 050 ----
mean loss: 143.21
 ---- batch: 060 ----
mean loss: 144.17
 ---- batch: 070 ----
mean loss: 143.64
 ---- batch: 080 ----
mean loss: 146.88
 ---- batch: 090 ----
mean loss: 143.74
 ---- batch: 100 ----
mean loss: 143.00
 ---- batch: 110 ----
mean loss: 144.50
train mean loss: 145.49
epoch train time: 0:00:00.559075
elapsed time: 0:02:18.952760
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:25:07.770091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.52
 ---- batch: 020 ----
mean loss: 137.83
 ---- batch: 030 ----
mean loss: 139.84
 ---- batch: 040 ----
mean loss: 151.36
 ---- batch: 050 ----
mean loss: 149.41
 ---- batch: 060 ----
mean loss: 153.33
 ---- batch: 070 ----
mean loss: 135.11
 ---- batch: 080 ----
mean loss: 140.82
 ---- batch: 090 ----
mean loss: 153.83
 ---- batch: 100 ----
mean loss: 150.00
 ---- batch: 110 ----
mean loss: 146.20
train mean loss: 145.02
epoch train time: 0:00:00.561231
elapsed time: 0:02:19.514145
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:25:08.331516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.28
 ---- batch: 020 ----
mean loss: 144.84
 ---- batch: 030 ----
mean loss: 151.49
 ---- batch: 040 ----
mean loss: 138.35
 ---- batch: 050 ----
mean loss: 148.71
 ---- batch: 060 ----
mean loss: 144.76
 ---- batch: 070 ----
mean loss: 145.63
 ---- batch: 080 ----
mean loss: 144.22
 ---- batch: 090 ----
mean loss: 152.47
 ---- batch: 100 ----
mean loss: 146.56
 ---- batch: 110 ----
mean loss: 137.46
train mean loss: 145.20
epoch train time: 0:00:00.564266
elapsed time: 0:02:20.078588
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:25:08.895919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.05
 ---- batch: 020 ----
mean loss: 143.81
 ---- batch: 030 ----
mean loss: 155.83
 ---- batch: 040 ----
mean loss: 142.53
 ---- batch: 050 ----
mean loss: 137.70
 ---- batch: 060 ----
mean loss: 142.51
 ---- batch: 070 ----
mean loss: 139.65
 ---- batch: 080 ----
mean loss: 153.48
 ---- batch: 090 ----
mean loss: 146.79
 ---- batch: 100 ----
mean loss: 141.19
 ---- batch: 110 ----
mean loss: 143.70
train mean loss: 144.11
epoch train time: 0:00:00.563672
elapsed time: 0:02:20.642410
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:25:09.459740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.66
 ---- batch: 020 ----
mean loss: 136.55
 ---- batch: 030 ----
mean loss: 144.53
 ---- batch: 040 ----
mean loss: 139.40
 ---- batch: 050 ----
mean loss: 139.34
 ---- batch: 060 ----
mean loss: 148.00
 ---- batch: 070 ----
mean loss: 155.04
 ---- batch: 080 ----
mean loss: 143.46
 ---- batch: 090 ----
mean loss: 148.00
 ---- batch: 100 ----
mean loss: 149.17
 ---- batch: 110 ----
mean loss: 152.47
train mean loss: 144.98
epoch train time: 0:00:00.573361
elapsed time: 0:02:21.215906
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:25:10.033239
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.60
 ---- batch: 020 ----
mean loss: 133.23
 ---- batch: 030 ----
mean loss: 143.44
 ---- batch: 040 ----
mean loss: 142.69
 ---- batch: 050 ----
mean loss: 145.40
 ---- batch: 060 ----
mean loss: 145.20
 ---- batch: 070 ----
mean loss: 141.46
 ---- batch: 080 ----
mean loss: 141.74
 ---- batch: 090 ----
mean loss: 145.54
 ---- batch: 100 ----
mean loss: 149.28
 ---- batch: 110 ----
mean loss: 146.48
train mean loss: 143.56
epoch train time: 0:00:00.565932
elapsed time: 0:02:21.781973
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:25:10.599302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.01
 ---- batch: 020 ----
mean loss: 141.45
 ---- batch: 030 ----
mean loss: 144.21
 ---- batch: 040 ----
mean loss: 143.38
 ---- batch: 050 ----
mean loss: 144.70
 ---- batch: 060 ----
mean loss: 145.37
 ---- batch: 070 ----
mean loss: 139.09
 ---- batch: 080 ----
mean loss: 143.16
 ---- batch: 090 ----
mean loss: 143.08
 ---- batch: 100 ----
mean loss: 142.75
 ---- batch: 110 ----
mean loss: 146.49
train mean loss: 143.83
epoch train time: 0:00:00.563409
elapsed time: 0:02:22.345513
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:25:11.162841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.89
 ---- batch: 020 ----
mean loss: 144.53
 ---- batch: 030 ----
mean loss: 141.30
 ---- batch: 040 ----
mean loss: 148.63
 ---- batch: 050 ----
mean loss: 145.28
 ---- batch: 060 ----
mean loss: 139.45
 ---- batch: 070 ----
mean loss: 137.44
 ---- batch: 080 ----
mean loss: 140.42
 ---- batch: 090 ----
mean loss: 141.10
 ---- batch: 100 ----
mean loss: 151.63
 ---- batch: 110 ----
mean loss: 152.95
train mean loss: 144.18
epoch train time: 0:00:00.558009
elapsed time: 0:02:22.903659
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:25:11.720993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.43
 ---- batch: 020 ----
mean loss: 150.10
 ---- batch: 030 ----
mean loss: 144.78
 ---- batch: 040 ----
mean loss: 143.47
 ---- batch: 050 ----
mean loss: 139.30
 ---- batch: 060 ----
mean loss: 137.26
 ---- batch: 070 ----
mean loss: 150.43
 ---- batch: 080 ----
mean loss: 142.27
 ---- batch: 090 ----
mean loss: 144.94
 ---- batch: 100 ----
mean loss: 146.80
 ---- batch: 110 ----
mean loss: 140.60
train mean loss: 143.51
epoch train time: 0:00:00.555564
elapsed time: 0:02:23.459362
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:25:12.276715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.20
 ---- batch: 020 ----
mean loss: 148.84
 ---- batch: 030 ----
mean loss: 152.10
 ---- batch: 040 ----
mean loss: 153.09
 ---- batch: 050 ----
mean loss: 131.60
 ---- batch: 060 ----
mean loss: 139.22
 ---- batch: 070 ----
mean loss: 142.03
 ---- batch: 080 ----
mean loss: 137.33
 ---- batch: 090 ----
mean loss: 150.82
 ---- batch: 100 ----
mean loss: 140.45
 ---- batch: 110 ----
mean loss: 145.19
train mean loss: 143.74
epoch train time: 0:00:00.571108
elapsed time: 0:02:24.030628
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:25:12.847956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.95
 ---- batch: 020 ----
mean loss: 138.33
 ---- batch: 030 ----
mean loss: 140.13
 ---- batch: 040 ----
mean loss: 150.69
 ---- batch: 050 ----
mean loss: 136.21
 ---- batch: 060 ----
mean loss: 138.18
 ---- batch: 070 ----
mean loss: 149.62
 ---- batch: 080 ----
mean loss: 145.60
 ---- batch: 090 ----
mean loss: 145.88
 ---- batch: 100 ----
mean loss: 139.34
 ---- batch: 110 ----
mean loss: 147.78
train mean loss: 143.78
epoch train time: 0:00:00.561427
elapsed time: 0:02:24.592205
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:25:13.409539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.86
 ---- batch: 020 ----
mean loss: 141.58
 ---- batch: 030 ----
mean loss: 145.00
 ---- batch: 040 ----
mean loss: 135.11
 ---- batch: 050 ----
mean loss: 144.34
 ---- batch: 060 ----
mean loss: 136.41
 ---- batch: 070 ----
mean loss: 156.33
 ---- batch: 080 ----
mean loss: 149.95
 ---- batch: 090 ----
mean loss: 146.49
 ---- batch: 100 ----
mean loss: 144.27
 ---- batch: 110 ----
mean loss: 141.76
train mean loss: 143.50
epoch train time: 0:00:00.559573
elapsed time: 0:02:25.151916
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:25:13.969247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.47
 ---- batch: 020 ----
mean loss: 139.19
 ---- batch: 030 ----
mean loss: 142.85
 ---- batch: 040 ----
mean loss: 142.48
 ---- batch: 050 ----
mean loss: 139.97
 ---- batch: 060 ----
mean loss: 148.68
 ---- batch: 070 ----
mean loss: 138.46
 ---- batch: 080 ----
mean loss: 142.87
 ---- batch: 090 ----
mean loss: 140.32
 ---- batch: 100 ----
mean loss: 136.29
 ---- batch: 110 ----
mean loss: 150.94
train mean loss: 142.96
epoch train time: 0:00:00.554033
elapsed time: 0:02:25.706110
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:25:14.523455
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.35
 ---- batch: 020 ----
mean loss: 135.84
 ---- batch: 030 ----
mean loss: 150.29
 ---- batch: 040 ----
mean loss: 133.04
 ---- batch: 050 ----
mean loss: 145.41
 ---- batch: 060 ----
mean loss: 149.68
 ---- batch: 070 ----
mean loss: 144.77
 ---- batch: 080 ----
mean loss: 144.21
 ---- batch: 090 ----
mean loss: 139.85
 ---- batch: 100 ----
mean loss: 139.51
 ---- batch: 110 ----
mean loss: 145.71
train mean loss: 143.13
epoch train time: 0:00:00.556825
elapsed time: 0:02:26.263097
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:25:15.080444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.04
 ---- batch: 020 ----
mean loss: 133.93
 ---- batch: 030 ----
mean loss: 136.74
 ---- batch: 040 ----
mean loss: 139.86
 ---- batch: 050 ----
mean loss: 142.74
 ---- batch: 060 ----
mean loss: 145.91
 ---- batch: 070 ----
mean loss: 145.12
 ---- batch: 080 ----
mean loss: 138.89
 ---- batch: 090 ----
mean loss: 150.91
 ---- batch: 100 ----
mean loss: 144.30
 ---- batch: 110 ----
mean loss: 155.47
train mean loss: 143.03
epoch train time: 0:00:00.573349
elapsed time: 0:02:26.836602
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:25:15.653934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.89
 ---- batch: 020 ----
mean loss: 140.93
 ---- batch: 030 ----
mean loss: 142.99
 ---- batch: 040 ----
mean loss: 135.72
 ---- batch: 050 ----
mean loss: 133.53
 ---- batch: 060 ----
mean loss: 137.69
 ---- batch: 070 ----
mean loss: 131.33
 ---- batch: 080 ----
mean loss: 137.81
 ---- batch: 090 ----
mean loss: 138.38
 ---- batch: 100 ----
mean loss: 141.32
 ---- batch: 110 ----
mean loss: 135.10
train mean loss: 137.53
epoch train time: 0:00:00.572152
elapsed time: 0:02:27.408908
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:25:16.226278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 144.94
 ---- batch: 020 ----
mean loss: 135.08
 ---- batch: 030 ----
mean loss: 135.05
 ---- batch: 040 ----
mean loss: 138.95
 ---- batch: 050 ----
mean loss: 136.90
 ---- batch: 060 ----
mean loss: 137.27
 ---- batch: 070 ----
mean loss: 126.78
 ---- batch: 080 ----
mean loss: 142.26
 ---- batch: 090 ----
mean loss: 136.94
 ---- batch: 100 ----
mean loss: 138.32
 ---- batch: 110 ----
mean loss: 131.29
train mean loss: 136.65
epoch train time: 0:00:00.566779
elapsed time: 0:02:27.975861
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:25:16.793194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.64
 ---- batch: 020 ----
mean loss: 134.26
 ---- batch: 030 ----
mean loss: 140.31
 ---- batch: 040 ----
mean loss: 133.38
 ---- batch: 050 ----
mean loss: 140.16
 ---- batch: 060 ----
mean loss: 135.84
 ---- batch: 070 ----
mean loss: 134.15
 ---- batch: 080 ----
mean loss: 142.64
 ---- batch: 090 ----
mean loss: 137.50
 ---- batch: 100 ----
mean loss: 133.23
 ---- batch: 110 ----
mean loss: 137.42
train mean loss: 136.29
epoch train time: 0:00:00.563430
elapsed time: 0:02:28.539431
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:25:17.356761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 142.61
 ---- batch: 020 ----
mean loss: 130.69
 ---- batch: 030 ----
mean loss: 134.13
 ---- batch: 040 ----
mean loss: 133.49
 ---- batch: 050 ----
mean loss: 142.50
 ---- batch: 060 ----
mean loss: 135.70
 ---- batch: 070 ----
mean loss: 138.48
 ---- batch: 080 ----
mean loss: 136.35
 ---- batch: 090 ----
mean loss: 144.82
 ---- batch: 100 ----
mean loss: 130.97
 ---- batch: 110 ----
mean loss: 131.45
train mean loss: 136.14
epoch train time: 0:00:00.566232
elapsed time: 0:02:29.105798
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:25:17.923126
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 142.47
 ---- batch: 020 ----
mean loss: 133.80
 ---- batch: 030 ----
mean loss: 134.28
 ---- batch: 040 ----
mean loss: 135.01
 ---- batch: 050 ----
mean loss: 130.17
 ---- batch: 060 ----
mean loss: 137.16
 ---- batch: 070 ----
mean loss: 139.48
 ---- batch: 080 ----
mean loss: 143.39
 ---- batch: 090 ----
mean loss: 131.41
 ---- batch: 100 ----
mean loss: 133.26
 ---- batch: 110 ----
mean loss: 138.92
train mean loss: 136.21
epoch train time: 0:00:00.556610
elapsed time: 0:02:29.662541
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:25:18.479887
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.37
 ---- batch: 020 ----
mean loss: 134.26
 ---- batch: 030 ----
mean loss: 134.41
 ---- batch: 040 ----
mean loss: 140.91
 ---- batch: 050 ----
mean loss: 129.60
 ---- batch: 060 ----
mean loss: 143.32
 ---- batch: 070 ----
mean loss: 137.90
 ---- batch: 080 ----
mean loss: 137.94
 ---- batch: 090 ----
mean loss: 137.59
 ---- batch: 100 ----
mean loss: 128.33
 ---- batch: 110 ----
mean loss: 135.88
train mean loss: 136.11
epoch train time: 0:00:00.560053
elapsed time: 0:02:30.222751
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:25:19.040082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.53
 ---- batch: 020 ----
mean loss: 139.29
 ---- batch: 030 ----
mean loss: 134.61
 ---- batch: 040 ----
mean loss: 146.52
 ---- batch: 050 ----
mean loss: 130.49
 ---- batch: 060 ----
mean loss: 135.99
 ---- batch: 070 ----
mean loss: 128.27
 ---- batch: 080 ----
mean loss: 138.42
 ---- batch: 090 ----
mean loss: 137.39
 ---- batch: 100 ----
mean loss: 141.45
 ---- batch: 110 ----
mean loss: 132.60
train mean loss: 136.00
epoch train time: 0:00:00.552818
elapsed time: 0:02:30.775725
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:25:19.593072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.40
 ---- batch: 020 ----
mean loss: 132.09
 ---- batch: 030 ----
mean loss: 134.10
 ---- batch: 040 ----
mean loss: 135.15
 ---- batch: 050 ----
mean loss: 139.30
 ---- batch: 060 ----
mean loss: 142.49
 ---- batch: 070 ----
mean loss: 140.75
 ---- batch: 080 ----
mean loss: 133.80
 ---- batch: 090 ----
mean loss: 138.78
 ---- batch: 100 ----
mean loss: 133.15
 ---- batch: 110 ----
mean loss: 135.16
train mean loss: 136.09
epoch train time: 0:00:00.557545
elapsed time: 0:02:31.333421
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:25:20.150750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.85
 ---- batch: 020 ----
mean loss: 127.80
 ---- batch: 030 ----
mean loss: 137.77
 ---- batch: 040 ----
mean loss: 134.14
 ---- batch: 050 ----
mean loss: 130.05
 ---- batch: 060 ----
mean loss: 141.16
 ---- batch: 070 ----
mean loss: 138.79
 ---- batch: 080 ----
mean loss: 131.51
 ---- batch: 090 ----
mean loss: 137.50
 ---- batch: 100 ----
mean loss: 136.80
 ---- batch: 110 ----
mean loss: 146.64
train mean loss: 135.88
epoch train time: 0:00:00.558219
elapsed time: 0:02:31.891777
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:25:20.709105
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 142.49
 ---- batch: 020 ----
mean loss: 131.96
 ---- batch: 030 ----
mean loss: 140.07
 ---- batch: 040 ----
mean loss: 142.12
 ---- batch: 050 ----
mean loss: 136.74
 ---- batch: 060 ----
mean loss: 136.28
 ---- batch: 070 ----
mean loss: 130.18
 ---- batch: 080 ----
mean loss: 130.46
 ---- batch: 090 ----
mean loss: 131.27
 ---- batch: 100 ----
mean loss: 134.27
 ---- batch: 110 ----
mean loss: 134.38
train mean loss: 135.90
epoch train time: 0:00:00.559334
elapsed time: 0:02:32.451274
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:25:21.268604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 140.48
 ---- batch: 020 ----
mean loss: 142.48
 ---- batch: 030 ----
mean loss: 138.74
 ---- batch: 040 ----
mean loss: 135.70
 ---- batch: 050 ----
mean loss: 137.35
 ---- batch: 060 ----
mean loss: 131.31
 ---- batch: 070 ----
mean loss: 130.67
 ---- batch: 080 ----
mean loss: 130.73
 ---- batch: 090 ----
mean loss: 142.37
 ---- batch: 100 ----
mean loss: 130.04
 ---- batch: 110 ----
mean loss: 134.49
train mean loss: 135.97
epoch train time: 0:00:00.560822
elapsed time: 0:02:33.012232
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:25:21.829564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.14
 ---- batch: 020 ----
mean loss: 133.71
 ---- batch: 030 ----
mean loss: 137.49
 ---- batch: 040 ----
mean loss: 136.57
 ---- batch: 050 ----
mean loss: 139.92
 ---- batch: 060 ----
mean loss: 138.52
 ---- batch: 070 ----
mean loss: 136.79
 ---- batch: 080 ----
mean loss: 134.83
 ---- batch: 090 ----
mean loss: 130.63
 ---- batch: 100 ----
mean loss: 142.17
 ---- batch: 110 ----
mean loss: 133.75
train mean loss: 135.95
epoch train time: 0:00:00.558588
elapsed time: 0:02:33.570956
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:25:22.388287
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 140.02
 ---- batch: 020 ----
mean loss: 132.12
 ---- batch: 030 ----
mean loss: 125.00
 ---- batch: 040 ----
mean loss: 141.38
 ---- batch: 050 ----
mean loss: 138.68
 ---- batch: 060 ----
mean loss: 140.59
 ---- batch: 070 ----
mean loss: 128.82
 ---- batch: 080 ----
mean loss: 138.40
 ---- batch: 090 ----
mean loss: 129.53
 ---- batch: 100 ----
mean loss: 139.08
 ---- batch: 110 ----
mean loss: 142.09
train mean loss: 135.88
epoch train time: 0:00:00.575309
elapsed time: 0:02:34.146398
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:25:22.963726
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.57
 ---- batch: 020 ----
mean loss: 131.28
 ---- batch: 030 ----
mean loss: 133.34
 ---- batch: 040 ----
mean loss: 132.60
 ---- batch: 050 ----
mean loss: 138.57
 ---- batch: 060 ----
mean loss: 130.12
 ---- batch: 070 ----
mean loss: 148.17
 ---- batch: 080 ----
mean loss: 132.80
 ---- batch: 090 ----
mean loss: 135.03
 ---- batch: 100 ----
mean loss: 140.21
 ---- batch: 110 ----
mean loss: 140.89
train mean loss: 135.91
epoch train time: 0:00:00.562015
elapsed time: 0:02:34.708547
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:25:23.525917
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.87
 ---- batch: 020 ----
mean loss: 132.22
 ---- batch: 030 ----
mean loss: 141.15
 ---- batch: 040 ----
mean loss: 140.25
 ---- batch: 050 ----
mean loss: 131.93
 ---- batch: 060 ----
mean loss: 133.94
 ---- batch: 070 ----
mean loss: 141.05
 ---- batch: 080 ----
mean loss: 131.79
 ---- batch: 090 ----
mean loss: 127.83
 ---- batch: 100 ----
mean loss: 135.11
 ---- batch: 110 ----
mean loss: 136.65
train mean loss: 135.85
epoch train time: 0:00:00.563135
elapsed time: 0:02:35.271856
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:25:24.089217
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 145.14
 ---- batch: 020 ----
mean loss: 134.61
 ---- batch: 030 ----
mean loss: 133.99
 ---- batch: 040 ----
mean loss: 135.44
 ---- batch: 050 ----
mean loss: 131.81
 ---- batch: 060 ----
mean loss: 136.62
 ---- batch: 070 ----
mean loss: 130.70
 ---- batch: 080 ----
mean loss: 141.78
 ---- batch: 090 ----
mean loss: 136.47
 ---- batch: 100 ----
mean loss: 135.72
 ---- batch: 110 ----
mean loss: 133.82
train mean loss: 135.76
epoch train time: 0:00:00.562879
elapsed time: 0:02:35.834918
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:25:24.652250
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.24
 ---- batch: 020 ----
mean loss: 132.72
 ---- batch: 030 ----
mean loss: 136.81
 ---- batch: 040 ----
mean loss: 134.11
 ---- batch: 050 ----
mean loss: 132.91
 ---- batch: 060 ----
mean loss: 140.50
 ---- batch: 070 ----
mean loss: 135.94
 ---- batch: 080 ----
mean loss: 136.29
 ---- batch: 090 ----
mean loss: 137.18
 ---- batch: 100 ----
mean loss: 141.23
 ---- batch: 110 ----
mean loss: 133.30
train mean loss: 135.85
epoch train time: 0:00:00.575714
elapsed time: 0:02:36.410771
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:25:25.228101
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.93
 ---- batch: 020 ----
mean loss: 136.43
 ---- batch: 030 ----
mean loss: 136.76
 ---- batch: 040 ----
mean loss: 134.99
 ---- batch: 050 ----
mean loss: 135.26
 ---- batch: 060 ----
mean loss: 136.12
 ---- batch: 070 ----
mean loss: 131.59
 ---- batch: 080 ----
mean loss: 132.96
 ---- batch: 090 ----
mean loss: 136.53
 ---- batch: 100 ----
mean loss: 141.75
 ---- batch: 110 ----
mean loss: 133.53
train mean loss: 135.80
epoch train time: 0:00:00.579903
elapsed time: 0:02:36.990833
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:25:25.808173
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 140.22
 ---- batch: 020 ----
mean loss: 136.46
 ---- batch: 030 ----
mean loss: 130.35
 ---- batch: 040 ----
mean loss: 135.52
 ---- batch: 050 ----
mean loss: 132.37
 ---- batch: 060 ----
mean loss: 129.13
 ---- batch: 070 ----
mean loss: 136.58
 ---- batch: 080 ----
mean loss: 144.60
 ---- batch: 090 ----
mean loss: 134.76
 ---- batch: 100 ----
mean loss: 138.85
 ---- batch: 110 ----
mean loss: 134.75
train mean loss: 135.81
epoch train time: 0:00:00.572258
elapsed time: 0:02:37.563235
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:25:26.380565
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.15
 ---- batch: 020 ----
mean loss: 136.53
 ---- batch: 030 ----
mean loss: 138.44
 ---- batch: 040 ----
mean loss: 135.61
 ---- batch: 050 ----
mean loss: 139.56
 ---- batch: 060 ----
mean loss: 134.59
 ---- batch: 070 ----
mean loss: 127.95
 ---- batch: 080 ----
mean loss: 138.05
 ---- batch: 090 ----
mean loss: 136.17
 ---- batch: 100 ----
mean loss: 138.46
 ---- batch: 110 ----
mean loss: 140.91
train mean loss: 135.81
epoch train time: 0:00:00.568841
elapsed time: 0:02:38.132282
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:25:26.949664
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.13
 ---- batch: 020 ----
mean loss: 133.31
 ---- batch: 030 ----
mean loss: 138.12
 ---- batch: 040 ----
mean loss: 139.33
 ---- batch: 050 ----
mean loss: 131.30
 ---- batch: 060 ----
mean loss: 133.69
 ---- batch: 070 ----
mean loss: 141.33
 ---- batch: 080 ----
mean loss: 143.49
 ---- batch: 090 ----
mean loss: 135.12
 ---- batch: 100 ----
mean loss: 133.18
 ---- batch: 110 ----
mean loss: 132.92
train mean loss: 135.71
epoch train time: 0:00:00.559482
elapsed time: 0:02:38.691952
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:25:27.509283
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.68
 ---- batch: 020 ----
mean loss: 136.71
 ---- batch: 030 ----
mean loss: 141.90
 ---- batch: 040 ----
mean loss: 129.80
 ---- batch: 050 ----
mean loss: 135.20
 ---- batch: 060 ----
mean loss: 137.65
 ---- batch: 070 ----
mean loss: 134.32
 ---- batch: 080 ----
mean loss: 132.64
 ---- batch: 090 ----
mean loss: 131.70
 ---- batch: 100 ----
mean loss: 137.79
 ---- batch: 110 ----
mean loss: 133.50
train mean loss: 135.70
epoch train time: 0:00:00.563189
elapsed time: 0:02:39.255276
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:25:28.072622
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.59
 ---- batch: 020 ----
mean loss: 133.73
 ---- batch: 030 ----
mean loss: 129.03
 ---- batch: 040 ----
mean loss: 135.00
 ---- batch: 050 ----
mean loss: 141.01
 ---- batch: 060 ----
mean loss: 136.76
 ---- batch: 070 ----
mean loss: 136.29
 ---- batch: 080 ----
mean loss: 135.40
 ---- batch: 090 ----
mean loss: 137.97
 ---- batch: 100 ----
mean loss: 138.70
 ---- batch: 110 ----
mean loss: 135.12
train mean loss: 135.68
epoch train time: 0:00:00.552667
elapsed time: 0:02:39.808090
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:25:28.625421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.52
 ---- batch: 020 ----
mean loss: 134.90
 ---- batch: 030 ----
mean loss: 132.57
 ---- batch: 040 ----
mean loss: 132.59
 ---- batch: 050 ----
mean loss: 140.70
 ---- batch: 060 ----
mean loss: 140.26
 ---- batch: 070 ----
mean loss: 136.07
 ---- batch: 080 ----
mean loss: 143.85
 ---- batch: 090 ----
mean loss: 137.17
 ---- batch: 100 ----
mean loss: 125.21
 ---- batch: 110 ----
mean loss: 136.48
train mean loss: 135.69
epoch train time: 0:00:00.573175
elapsed time: 0:02:40.381405
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:25:29.198737
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.58
 ---- batch: 020 ----
mean loss: 131.05
 ---- batch: 030 ----
mean loss: 137.46
 ---- batch: 040 ----
mean loss: 142.65
 ---- batch: 050 ----
mean loss: 128.61
 ---- batch: 060 ----
mean loss: 140.01
 ---- batch: 070 ----
mean loss: 130.28
 ---- batch: 080 ----
mean loss: 127.53
 ---- batch: 090 ----
mean loss: 142.59
 ---- batch: 100 ----
mean loss: 139.06
 ---- batch: 110 ----
mean loss: 133.83
train mean loss: 135.69
epoch train time: 0:00:00.559992
elapsed time: 0:02:40.941561
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:25:29.758895
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.55
 ---- batch: 020 ----
mean loss: 144.66
 ---- batch: 030 ----
mean loss: 136.19
 ---- batch: 040 ----
mean loss: 132.32
 ---- batch: 050 ----
mean loss: 133.86
 ---- batch: 060 ----
mean loss: 127.65
 ---- batch: 070 ----
mean loss: 145.84
 ---- batch: 080 ----
mean loss: 132.65
 ---- batch: 090 ----
mean loss: 141.03
 ---- batch: 100 ----
mean loss: 135.32
 ---- batch: 110 ----
mean loss: 136.84
train mean loss: 135.47
epoch train time: 0:00:00.562315
elapsed time: 0:02:41.504015
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:25:30.321346
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.91
 ---- batch: 020 ----
mean loss: 133.65
 ---- batch: 030 ----
mean loss: 136.77
 ---- batch: 040 ----
mean loss: 132.33
 ---- batch: 050 ----
mean loss: 134.69
 ---- batch: 060 ----
mean loss: 138.08
 ---- batch: 070 ----
mean loss: 143.79
 ---- batch: 080 ----
mean loss: 141.93
 ---- batch: 090 ----
mean loss: 134.50
 ---- batch: 100 ----
mean loss: 137.20
 ---- batch: 110 ----
mean loss: 137.44
train mean loss: 135.64
epoch train time: 0:00:00.561071
elapsed time: 0:02:42.065224
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:25:30.882572
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.48
 ---- batch: 020 ----
mean loss: 131.15
 ---- batch: 030 ----
mean loss: 131.48
 ---- batch: 040 ----
mean loss: 141.32
 ---- batch: 050 ----
mean loss: 136.10
 ---- batch: 060 ----
mean loss: 133.81
 ---- batch: 070 ----
mean loss: 141.67
 ---- batch: 080 ----
mean loss: 136.96
 ---- batch: 090 ----
mean loss: 135.58
 ---- batch: 100 ----
mean loss: 134.12
 ---- batch: 110 ----
mean loss: 133.17
train mean loss: 135.46
epoch train time: 0:00:00.562922
elapsed time: 0:02:42.628300
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:25:31.445655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.55
 ---- batch: 020 ----
mean loss: 137.69
 ---- batch: 030 ----
mean loss: 142.89
 ---- batch: 040 ----
mean loss: 138.35
 ---- batch: 050 ----
mean loss: 134.75
 ---- batch: 060 ----
mean loss: 141.18
 ---- batch: 070 ----
mean loss: 133.71
 ---- batch: 080 ----
mean loss: 124.38
 ---- batch: 090 ----
mean loss: 137.99
 ---- batch: 100 ----
mean loss: 136.33
 ---- batch: 110 ----
mean loss: 135.66
train mean loss: 135.50
epoch train time: 0:00:00.561485
elapsed time: 0:02:43.189946
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:25:32.007279
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.75
 ---- batch: 020 ----
mean loss: 133.85
 ---- batch: 030 ----
mean loss: 130.10
 ---- batch: 040 ----
mean loss: 147.74
 ---- batch: 050 ----
mean loss: 134.28
 ---- batch: 060 ----
mean loss: 137.41
 ---- batch: 070 ----
mean loss: 130.17
 ---- batch: 080 ----
mean loss: 136.77
 ---- batch: 090 ----
mean loss: 134.75
 ---- batch: 100 ----
mean loss: 133.25
 ---- batch: 110 ----
mean loss: 135.30
train mean loss: 135.69
epoch train time: 0:00:00.555015
elapsed time: 0:02:43.745099
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:25:32.562429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.16
 ---- batch: 020 ----
mean loss: 127.20
 ---- batch: 030 ----
mean loss: 132.67
 ---- batch: 040 ----
mean loss: 144.00
 ---- batch: 050 ----
mean loss: 137.70
 ---- batch: 060 ----
mean loss: 126.80
 ---- batch: 070 ----
mean loss: 137.26
 ---- batch: 080 ----
mean loss: 137.92
 ---- batch: 090 ----
mean loss: 136.70
 ---- batch: 100 ----
mean loss: 145.55
 ---- batch: 110 ----
mean loss: 139.20
train mean loss: 135.47
epoch train time: 0:00:00.568622
elapsed time: 0:02:44.313877
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:25:33.131207
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.30
 ---- batch: 020 ----
mean loss: 136.44
 ---- batch: 030 ----
mean loss: 136.91
 ---- batch: 040 ----
mean loss: 137.94
 ---- batch: 050 ----
mean loss: 128.31
 ---- batch: 060 ----
mean loss: 140.14
 ---- batch: 070 ----
mean loss: 139.81
 ---- batch: 080 ----
mean loss: 138.33
 ---- batch: 090 ----
mean loss: 136.58
 ---- batch: 100 ----
mean loss: 129.71
 ---- batch: 110 ----
mean loss: 134.61
train mean loss: 135.44
epoch train time: 0:00:00.559015
elapsed time: 0:02:44.873032
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:25:33.690364
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.00
 ---- batch: 020 ----
mean loss: 131.08
 ---- batch: 030 ----
mean loss: 134.09
 ---- batch: 040 ----
mean loss: 139.51
 ---- batch: 050 ----
mean loss: 130.01
 ---- batch: 060 ----
mean loss: 139.05
 ---- batch: 070 ----
mean loss: 137.71
 ---- batch: 080 ----
mean loss: 137.38
 ---- batch: 090 ----
mean loss: 136.74
 ---- batch: 100 ----
mean loss: 138.29
 ---- batch: 110 ----
mean loss: 135.50
train mean loss: 135.45
epoch train time: 0:00:00.555786
elapsed time: 0:02:45.428971
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:25:34.246310
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.38
 ---- batch: 020 ----
mean loss: 133.38
 ---- batch: 030 ----
mean loss: 128.29
 ---- batch: 040 ----
mean loss: 136.53
 ---- batch: 050 ----
mean loss: 132.21
 ---- batch: 060 ----
mean loss: 135.10
 ---- batch: 070 ----
mean loss: 137.36
 ---- batch: 080 ----
mean loss: 140.22
 ---- batch: 090 ----
mean loss: 140.10
 ---- batch: 100 ----
mean loss: 136.02
 ---- batch: 110 ----
mean loss: 135.65
train mean loss: 135.33
epoch train time: 0:00:00.551248
elapsed time: 0:02:45.980429
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:25:34.797773
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.77
 ---- batch: 020 ----
mean loss: 130.09
 ---- batch: 030 ----
mean loss: 127.98
 ---- batch: 040 ----
mean loss: 145.32
 ---- batch: 050 ----
mean loss: 133.21
 ---- batch: 060 ----
mean loss: 144.82
 ---- batch: 070 ----
mean loss: 135.64
 ---- batch: 080 ----
mean loss: 131.32
 ---- batch: 090 ----
mean loss: 136.13
 ---- batch: 100 ----
mean loss: 135.83
 ---- batch: 110 ----
mean loss: 131.64
train mean loss: 135.35
epoch train time: 0:00:00.564817
elapsed time: 0:02:46.545407
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:25:35.362742
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.13
 ---- batch: 020 ----
mean loss: 136.22
 ---- batch: 030 ----
mean loss: 134.38
 ---- batch: 040 ----
mean loss: 142.26
 ---- batch: 050 ----
mean loss: 135.41
 ---- batch: 060 ----
mean loss: 146.09
 ---- batch: 070 ----
mean loss: 134.68
 ---- batch: 080 ----
mean loss: 133.57
 ---- batch: 090 ----
mean loss: 129.13
 ---- batch: 100 ----
mean loss: 136.25
 ---- batch: 110 ----
mean loss: 130.39
train mean loss: 135.37
epoch train time: 0:00:00.562243
elapsed time: 0:02:47.107793
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:25:35.925123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.01
 ---- batch: 020 ----
mean loss: 144.58
 ---- batch: 030 ----
mean loss: 130.49
 ---- batch: 040 ----
mean loss: 135.06
 ---- batch: 050 ----
mean loss: 132.02
 ---- batch: 060 ----
mean loss: 137.41
 ---- batch: 070 ----
mean loss: 128.74
 ---- batch: 080 ----
mean loss: 133.98
 ---- batch: 090 ----
mean loss: 140.14
 ---- batch: 100 ----
mean loss: 134.80
 ---- batch: 110 ----
mean loss: 132.44
train mean loss: 135.34
epoch train time: 0:00:00.558777
elapsed time: 0:02:47.666719
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:25:36.484047
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.06
 ---- batch: 020 ----
mean loss: 143.01
 ---- batch: 030 ----
mean loss: 133.75
 ---- batch: 040 ----
mean loss: 134.43
 ---- batch: 050 ----
mean loss: 134.31
 ---- batch: 060 ----
mean loss: 136.45
 ---- batch: 070 ----
mean loss: 141.58
 ---- batch: 080 ----
mean loss: 125.90
 ---- batch: 090 ----
mean loss: 130.55
 ---- batch: 100 ----
mean loss: 138.70
 ---- batch: 110 ----
mean loss: 133.53
train mean loss: 135.32
epoch train time: 0:00:00.561941
elapsed time: 0:02:48.228802
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:25:37.046134
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.00
 ---- batch: 020 ----
mean loss: 134.48
 ---- batch: 030 ----
mean loss: 137.76
 ---- batch: 040 ----
mean loss: 138.82
 ---- batch: 050 ----
mean loss: 136.46
 ---- batch: 060 ----
mean loss: 146.77
 ---- batch: 070 ----
mean loss: 139.89
 ---- batch: 080 ----
mean loss: 139.45
 ---- batch: 090 ----
mean loss: 128.53
 ---- batch: 100 ----
mean loss: 133.59
 ---- batch: 110 ----
mean loss: 128.73
train mean loss: 135.35
epoch train time: 0:00:00.551218
elapsed time: 0:02:48.780156
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:25:37.597486
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.09
 ---- batch: 020 ----
mean loss: 132.22
 ---- batch: 030 ----
mean loss: 134.97
 ---- batch: 040 ----
mean loss: 132.00
 ---- batch: 050 ----
mean loss: 131.66
 ---- batch: 060 ----
mean loss: 136.57
 ---- batch: 070 ----
mean loss: 134.27
 ---- batch: 080 ----
mean loss: 139.15
 ---- batch: 090 ----
mean loss: 149.68
 ---- batch: 100 ----
mean loss: 127.74
 ---- batch: 110 ----
mean loss: 138.09
train mean loss: 135.36
epoch train time: 0:00:00.583200
elapsed time: 0:02:49.363491
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:25:38.180840
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.91
 ---- batch: 020 ----
mean loss: 131.65
 ---- batch: 030 ----
mean loss: 128.94
 ---- batch: 040 ----
mean loss: 133.15
 ---- batch: 050 ----
mean loss: 139.29
 ---- batch: 060 ----
mean loss: 142.03
 ---- batch: 070 ----
mean loss: 136.94
 ---- batch: 080 ----
mean loss: 130.96
 ---- batch: 090 ----
mean loss: 137.68
 ---- batch: 100 ----
mean loss: 144.28
 ---- batch: 110 ----
mean loss: 132.97
train mean loss: 135.20
epoch train time: 0:00:00.560396
elapsed time: 0:02:49.924067
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:25:38.741419
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.57
 ---- batch: 020 ----
mean loss: 139.52
 ---- batch: 030 ----
mean loss: 139.15
 ---- batch: 040 ----
mean loss: 128.10
 ---- batch: 050 ----
mean loss: 132.12
 ---- batch: 060 ----
mean loss: 138.73
 ---- batch: 070 ----
mean loss: 136.58
 ---- batch: 080 ----
mean loss: 122.31
 ---- batch: 090 ----
mean loss: 131.59
 ---- batch: 100 ----
mean loss: 145.04
 ---- batch: 110 ----
mean loss: 142.58
train mean loss: 135.32
epoch train time: 0:00:00.578123
elapsed time: 0:02:50.502358
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:25:39.319688
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.92
 ---- batch: 020 ----
mean loss: 143.27
 ---- batch: 030 ----
mean loss: 127.97
 ---- batch: 040 ----
mean loss: 140.68
 ---- batch: 050 ----
mean loss: 139.31
 ---- batch: 060 ----
mean loss: 131.98
 ---- batch: 070 ----
mean loss: 144.17
 ---- batch: 080 ----
mean loss: 125.58
 ---- batch: 090 ----
mean loss: 129.13
 ---- batch: 100 ----
mean loss: 138.77
 ---- batch: 110 ----
mean loss: 132.00
train mean loss: 135.27
epoch train time: 0:00:00.569505
elapsed time: 0:02:51.072014
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:25:39.889344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.25
 ---- batch: 020 ----
mean loss: 130.14
 ---- batch: 030 ----
mean loss: 134.29
 ---- batch: 040 ----
mean loss: 135.10
 ---- batch: 050 ----
mean loss: 135.58
 ---- batch: 060 ----
mean loss: 135.29
 ---- batch: 070 ----
mean loss: 140.69
 ---- batch: 080 ----
mean loss: 138.01
 ---- batch: 090 ----
mean loss: 131.74
 ---- batch: 100 ----
mean loss: 138.01
 ---- batch: 110 ----
mean loss: 138.66
train mean loss: 135.25
epoch train time: 0:00:00.560220
elapsed time: 0:02:51.632393
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:25:40.449728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.42
 ---- batch: 020 ----
mean loss: 134.98
 ---- batch: 030 ----
mean loss: 137.58
 ---- batch: 040 ----
mean loss: 132.93
 ---- batch: 050 ----
mean loss: 131.96
 ---- batch: 060 ----
mean loss: 135.16
 ---- batch: 070 ----
mean loss: 133.18
 ---- batch: 080 ----
mean loss: 132.51
 ---- batch: 090 ----
mean loss: 133.79
 ---- batch: 100 ----
mean loss: 140.24
 ---- batch: 110 ----
mean loss: 140.34
train mean loss: 135.10
epoch train time: 0:00:00.565725
elapsed time: 0:02:52.198259
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:25:41.015604
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 137.00
 ---- batch: 020 ----
mean loss: 137.18
 ---- batch: 030 ----
mean loss: 133.21
 ---- batch: 040 ----
mean loss: 130.04
 ---- batch: 050 ----
mean loss: 136.86
 ---- batch: 060 ----
mean loss: 135.66
 ---- batch: 070 ----
mean loss: 135.98
 ---- batch: 080 ----
mean loss: 131.39
 ---- batch: 090 ----
mean loss: 134.23
 ---- batch: 100 ----
mean loss: 138.58
 ---- batch: 110 ----
mean loss: 134.64
train mean loss: 135.19
epoch train time: 0:00:00.573173
elapsed time: 0:02:52.771585
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:25:41.588947
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.96
 ---- batch: 020 ----
mean loss: 132.52
 ---- batch: 030 ----
mean loss: 140.73
 ---- batch: 040 ----
mean loss: 132.08
 ---- batch: 050 ----
mean loss: 132.19
 ---- batch: 060 ----
mean loss: 134.31
 ---- batch: 070 ----
mean loss: 136.66
 ---- batch: 080 ----
mean loss: 133.75
 ---- batch: 090 ----
mean loss: 136.73
 ---- batch: 100 ----
mean loss: 139.76
 ---- batch: 110 ----
mean loss: 143.80
train mean loss: 135.06
epoch train time: 0:00:00.567791
elapsed time: 0:02:53.339545
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:25:42.156894
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.15
 ---- batch: 020 ----
mean loss: 133.67
 ---- batch: 030 ----
mean loss: 130.62
 ---- batch: 040 ----
mean loss: 136.94
 ---- batch: 050 ----
mean loss: 132.44
 ---- batch: 060 ----
mean loss: 133.79
 ---- batch: 070 ----
mean loss: 133.13
 ---- batch: 080 ----
mean loss: 134.72
 ---- batch: 090 ----
mean loss: 138.00
 ---- batch: 100 ----
mean loss: 141.24
 ---- batch: 110 ----
mean loss: 137.37
train mean loss: 135.06
epoch train time: 0:00:00.564330
elapsed time: 0:02:53.904031
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:25:42.721360
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.25
 ---- batch: 020 ----
mean loss: 132.95
 ---- batch: 030 ----
mean loss: 127.43
 ---- batch: 040 ----
mean loss: 135.85
 ---- batch: 050 ----
mean loss: 132.84
 ---- batch: 060 ----
mean loss: 140.33
 ---- batch: 070 ----
mean loss: 133.29
 ---- batch: 080 ----
mean loss: 137.20
 ---- batch: 090 ----
mean loss: 139.51
 ---- batch: 100 ----
mean loss: 138.01
 ---- batch: 110 ----
mean loss: 135.87
train mean loss: 135.11
epoch train time: 0:00:00.571879
elapsed time: 0:02:54.479282
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_6/checkpoint.pth.tar
**** end time: 2019-09-27 02:25:43.296581 ****
