Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.125/bayesian_conv5_dense1_0.125_6', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=0.125, resume=False, step_size=200, visualize_step=50)
pid: 17629
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-25 16:45:53.481727 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 16:45:53.500046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2020.34
train mean loss: 1969.68
epoch train time: 0:00:06.023803
elapsed time: 0:00:06.050036
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 16:45:59.531805
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1379.48
train mean loss: 1367.59
epoch train time: 0:00:02.125691
elapsed time: 0:00:08.176029
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 16:46:01.657876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1247.11
train mean loss: 1238.50
epoch train time: 0:00:02.116752
elapsed time: 0:00:10.293758
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 16:46:03.775585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1168.88
train mean loss: 1155.92
epoch train time: 0:00:02.108864
elapsed time: 0:00:12.402948
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 16:46:05.884813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1111.02
train mean loss: 1122.72
epoch train time: 0:00:02.115850
elapsed time: 0:00:14.519209
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 16:46:08.001028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1096.26
train mean loss: 1098.40
epoch train time: 0:00:02.117977
elapsed time: 0:00:16.637451
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 16:46:10.119252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1065.45
train mean loss: 1067.19
epoch train time: 0:00:02.113607
elapsed time: 0:00:18.751303
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 16:46:12.233106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1056.91
train mean loss: 1047.43
epoch train time: 0:00:02.118623
elapsed time: 0:00:20.870191
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 16:46:14.352013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1028.75
train mean loss: 1033.19
epoch train time: 0:00:02.116675
elapsed time: 0:00:22.987161
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 16:46:16.468964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1019.16
train mean loss: 1026.74
epoch train time: 0:00:02.124341
elapsed time: 0:00:25.111762
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 16:46:18.593562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1002.47
train mean loss: 1005.86
epoch train time: 0:00:02.117251
elapsed time: 0:00:27.229330
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 16:46:20.711150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1003.67
train mean loss: 1000.21
epoch train time: 0:00:02.120933
elapsed time: 0:00:29.350580
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 16:46:22.832405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1017.25
train mean loss: 1000.13
epoch train time: 0:00:02.131896
elapsed time: 0:00:31.482840
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 16:46:24.964651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 999.60
train mean loss: 1001.96
epoch train time: 0:00:02.112394
elapsed time: 0:00:33.595498
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 16:46:27.077296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 992.24
train mean loss: 988.69
epoch train time: 0:00:02.115997
elapsed time: 0:00:35.711791
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 16:46:29.193598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1014.37
train mean loss: 1002.52
epoch train time: 0:00:02.116541
elapsed time: 0:00:37.828608
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 16:46:31.310412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.95
train mean loss: 989.44
epoch train time: 0:00:02.122269
elapsed time: 0:00:39.951214
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 16:46:33.433017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1004.92
train mean loss: 991.53
epoch train time: 0:00:02.118437
elapsed time: 0:00:42.069936
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 16:46:35.551751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 980.64
train mean loss: 977.63
epoch train time: 0:00:02.128001
elapsed time: 0:00:44.198263
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 16:46:37.680070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 979.71
train mean loss: 980.20
epoch train time: 0:00:02.119758
elapsed time: 0:00:46.318291
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 16:46:39.800107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 959.88
train mean loss: 963.20
epoch train time: 0:00:02.116521
elapsed time: 0:00:48.435088
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 16:46:41.916925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 985.55
train mean loss: 985.93
epoch train time: 0:00:02.120125
elapsed time: 0:00:50.555526
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 16:46:44.037333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 976.07
train mean loss: 980.30
epoch train time: 0:00:02.114287
elapsed time: 0:00:52.670090
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 16:46:46.151908
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.41
train mean loss: 961.18
epoch train time: 0:00:02.118431
elapsed time: 0:00:54.788807
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 16:46:48.270625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 974.48
train mean loss: 972.31
epoch train time: 0:00:02.108129
elapsed time: 0:00:56.897214
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 16:46:50.379017
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 964.13
train mean loss: 965.42
epoch train time: 0:00:02.111495
elapsed time: 0:00:59.008974
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 16:46:52.490773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.17
train mean loss: 962.03
epoch train time: 0:00:02.115082
elapsed time: 0:01:01.124323
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 16:46:54.606133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 971.54
train mean loss: 966.77
epoch train time: 0:00:02.114340
elapsed time: 0:01:03.238983
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 16:46:56.720798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 956.91
train mean loss: 963.02
epoch train time: 0:00:02.112242
elapsed time: 0:01:05.351629
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 16:46:58.833451
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 965.93
train mean loss: 967.84
epoch train time: 0:00:02.115414
elapsed time: 0:01:07.467336
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 16:47:00.949140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 943.55
train mean loss: 947.54
epoch train time: 0:00:02.120167
elapsed time: 0:01:09.587803
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 16:47:03.069631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 953.78
train mean loss: 960.95
epoch train time: 0:00:02.128043
elapsed time: 0:01:11.716128
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 16:47:05.197944
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.01
train mean loss: 954.11
epoch train time: 0:00:02.120316
elapsed time: 0:01:13.836712
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 16:47:07.318511
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.21
train mean loss: 958.26
epoch train time: 0:00:02.120691
elapsed time: 0:01:15.957669
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 16:47:09.439470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 961.21
train mean loss: 967.97
epoch train time: 0:00:02.120315
elapsed time: 0:01:18.078270
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 16:47:11.560072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 967.66
train mean loss: 966.62
epoch train time: 0:00:02.121080
elapsed time: 0:01:20.199739
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 16:47:13.681548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 950.43
train mean loss: 950.07
epoch train time: 0:00:02.123298
elapsed time: 0:01:22.323332
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 16:47:15.805144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.94
train mean loss: 949.56
epoch train time: 0:00:02.114126
elapsed time: 0:01:24.437803
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 16:47:17.919635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.24
train mean loss: 941.22
epoch train time: 0:00:02.123761
elapsed time: 0:01:26.561896
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 16:47:20.043708
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 962.42
train mean loss: 959.14
epoch train time: 0:00:02.114165
elapsed time: 0:01:28.676331
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 16:47:22.158162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.42
train mean loss: 954.23
epoch train time: 0:00:02.111553
elapsed time: 0:01:30.788194
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 16:47:24.269998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.51
train mean loss: 953.74
epoch train time: 0:00:02.111152
elapsed time: 0:01:32.899632
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 16:47:26.381433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 951.06
train mean loss: 952.38
epoch train time: 0:00:02.109388
elapsed time: 0:01:35.009284
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 16:47:28.491084
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.22
train mean loss: 942.30
epoch train time: 0:00:02.118740
elapsed time: 0:01:37.128342
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 16:47:30.610154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.13
train mean loss: 945.65
epoch train time: 0:00:02.116320
elapsed time: 0:01:39.244954
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 16:47:32.726790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.99
train mean loss: 944.81
epoch train time: 0:00:02.130032
elapsed time: 0:01:41.375298
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 16:47:34.857118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 946.64
train mean loss: 942.70
epoch train time: 0:00:02.116862
elapsed time: 0:01:43.492486
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 16:47:36.974300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.71
train mean loss: 946.90
epoch train time: 0:00:02.118509
elapsed time: 0:01:45.611268
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 16:47:39.093068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.77
train mean loss: 942.29
epoch train time: 0:00:02.110777
elapsed time: 0:01:47.722441
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 16:47:41.204246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.44
train mean loss: 945.68
epoch train time: 0:00:02.117107
elapsed time: 0:01:49.839812
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 16:47:43.321631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.44
train mean loss: 928.39
epoch train time: 0:00:02.116862
elapsed time: 0:01:51.956946
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 16:47:45.438753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 934.46
train mean loss: 940.93
epoch train time: 0:00:02.110710
elapsed time: 0:01:54.067951
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 16:47:47.549753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.20
train mean loss: 936.11
epoch train time: 0:00:02.113735
elapsed time: 0:01:56.181961
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 16:47:49.663777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.78
train mean loss: 944.20
epoch train time: 0:00:02.116089
elapsed time: 0:01:58.298343
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 16:47:51.780148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.97
train mean loss: 928.24
epoch train time: 0:00:02.118010
elapsed time: 0:02:00.416633
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 16:47:53.898432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 939.69
train mean loss: 937.81
epoch train time: 0:00:02.114546
elapsed time: 0:02:02.531460
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 16:47:56.013260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 932.31
train mean loss: 931.38
epoch train time: 0:00:02.117312
elapsed time: 0:02:04.649056
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 16:47:58.130867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.75
train mean loss: 930.14
epoch train time: 0:00:02.113402
elapsed time: 0:02:06.762722
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 16:48:00.244575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.07
train mean loss: 928.17
epoch train time: 0:00:02.116517
elapsed time: 0:02:08.879629
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 16:48:02.361490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 938.65
train mean loss: 926.63
epoch train time: 0:00:02.120351
elapsed time: 0:02:11.000319
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 16:48:04.482125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.57
train mean loss: 923.77
epoch train time: 0:00:02.112951
elapsed time: 0:02:13.113561
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 16:48:06.595361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 926.05
train mean loss: 921.25
epoch train time: 0:00:02.122392
elapsed time: 0:02:15.236227
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 16:48:08.718027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 931.05
train mean loss: 925.88
epoch train time: 0:00:02.112347
elapsed time: 0:02:17.348851
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 16:48:10.830655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.01
train mean loss: 925.97
epoch train time: 0:00:02.113717
elapsed time: 0:02:19.462874
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 16:48:12.944677
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 936.36
train mean loss: 934.92
epoch train time: 0:00:02.120436
elapsed time: 0:02:21.583572
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 16:48:15.065374
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 929.97
train mean loss: 926.62
epoch train time: 0:00:02.119526
elapsed time: 0:02:23.703380
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 16:48:17.185186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 923.83
train mean loss: 926.69
epoch train time: 0:00:02.124723
elapsed time: 0:02:25.828368
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 16:48:19.310169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.62
train mean loss: 926.44
epoch train time: 0:00:02.107045
elapsed time: 0:02:27.935691
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 16:48:21.417490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.31
train mean loss: 908.97
epoch train time: 0:00:02.112478
elapsed time: 0:02:30.048436
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 16:48:23.530238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 914.35
train mean loss: 922.92
epoch train time: 0:00:02.125790
elapsed time: 0:02:32.174497
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 16:48:25.656310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.29
train mean loss: 910.25
epoch train time: 0:00:02.134070
elapsed time: 0:02:34.308855
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 16:48:27.790655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 908.85
train mean loss: 910.21
epoch train time: 0:00:02.142525
elapsed time: 0:02:36.451646
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 16:48:29.933447
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 891.87
train mean loss: 907.23
epoch train time: 0:00:02.137033
elapsed time: 0:02:38.588963
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 16:48:32.070764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.62
train mean loss: 905.65
epoch train time: 0:00:02.126089
elapsed time: 0:02:40.715325
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 16:48:34.197125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 904.75
train mean loss: 901.49
epoch train time: 0:00:02.114260
elapsed time: 0:02:42.829852
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 16:48:36.311661
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 906.32
train mean loss: 900.79
epoch train time: 0:00:02.115536
elapsed time: 0:02:44.945664
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 16:48:38.427490
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 899.45
train mean loss: 897.72
epoch train time: 0:00:02.112137
elapsed time: 0:02:47.058094
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 16:48:40.539911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.85
train mean loss: 896.68
epoch train time: 0:00:02.122455
elapsed time: 0:02:49.180853
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 16:48:42.662646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 888.67
train mean loss: 887.42
epoch train time: 0:00:02.130186
elapsed time: 0:02:51.311313
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 16:48:44.793120
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.08
train mean loss: 888.68
epoch train time: 0:00:02.115544
elapsed time: 0:02:53.427155
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 16:48:46.908953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 875.59
train mean loss: 879.10
epoch train time: 0:00:02.110742
elapsed time: 0:02:55.538276
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 16:48:49.020135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.86
train mean loss: 860.83
epoch train time: 0:00:02.119799
elapsed time: 0:02:57.658479
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 16:48:51.140283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.55
train mean loss: 870.70
epoch train time: 0:00:02.134926
elapsed time: 0:02:59.793666
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 16:48:53.275478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.96
train mean loss: 850.01
epoch train time: 0:00:02.125098
elapsed time: 0:03:01.919022
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 16:48:55.400821
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.40
train mean loss: 831.91
epoch train time: 0:00:02.121116
elapsed time: 0:03:04.040404
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 16:48:57.522205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 814.54
train mean loss: 814.62
epoch train time: 0:00:02.131077
elapsed time: 0:03:06.171817
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 16:48:59.653640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 803.22
train mean loss: 795.57
epoch train time: 0:00:02.129004
elapsed time: 0:03:08.301125
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 16:49:01.782929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 789.44
train mean loss: 780.02
epoch train time: 0:00:02.127955
elapsed time: 0:03:10.429367
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 16:49:03.911169
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 748.23
train mean loss: 747.44
epoch train time: 0:00:02.123532
elapsed time: 0:03:12.553175
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 16:49:06.034973
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 748.30
train mean loss: 743.14
epoch train time: 0:00:02.119835
elapsed time: 0:03:14.673271
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 16:49:08.155075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 726.80
train mean loss: 734.08
epoch train time: 0:00:02.131674
elapsed time: 0:03:16.805276
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 16:49:10.287081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 723.35
train mean loss: 715.15
epoch train time: 0:00:02.130775
elapsed time: 0:03:18.936362
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 16:49:12.418164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.42
train mean loss: 719.80
epoch train time: 0:00:02.121715
elapsed time: 0:03:21.058343
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 16:49:14.540175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.50
train mean loss: 705.85
epoch train time: 0:00:02.122032
elapsed time: 0:03:23.180683
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 16:49:16.662480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.29
train mean loss: 704.73
epoch train time: 0:00:02.117124
elapsed time: 0:03:25.298068
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 16:49:18.779887
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 692.37
train mean loss: 690.88
epoch train time: 0:00:02.124460
elapsed time: 0:03:27.422826
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 16:49:20.904638
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.05
train mean loss: 672.34
epoch train time: 0:00:02.129269
elapsed time: 0:03:29.552379
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 16:49:23.034199
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 668.09
train mean loss: 674.35
epoch train time: 0:00:02.124642
elapsed time: 0:03:31.677326
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 16:49:25.159141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 659.11
train mean loss: 656.17
epoch train time: 0:00:02.120597
elapsed time: 0:03:33.798224
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 16:49:27.280047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.27
train mean loss: 662.00
epoch train time: 0:00:02.131290
elapsed time: 0:03:35.929840
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 16:49:29.411646
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 658.98
train mean loss: 658.17
epoch train time: 0:00:02.121375
elapsed time: 0:03:38.051477
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 16:49:31.533296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 649.02
train mean loss: 646.00
epoch train time: 0:00:02.124908
elapsed time: 0:03:40.176836
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 16:49:33.658656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 638.46
train mean loss: 638.40
epoch train time: 0:00:02.121507
elapsed time: 0:03:42.298636
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 16:49:35.780438
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 638.99
train mean loss: 645.23
epoch train time: 0:00:02.118236
elapsed time: 0:03:44.417154
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 16:49:37.898966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 626.62
train mean loss: 628.15
epoch train time: 0:00:02.121379
elapsed time: 0:03:46.538816
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 16:49:40.020624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 623.03
train mean loss: 621.69
epoch train time: 0:00:02.126324
elapsed time: 0:03:48.665418
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 16:49:42.147222
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 613.88
train mean loss: 612.65
epoch train time: 0:00:02.124831
elapsed time: 0:03:50.790566
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 16:49:44.272340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 609.82
train mean loss: 611.93
epoch train time: 0:00:02.121396
elapsed time: 0:03:52.912193
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 16:49:46.393995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 605.92
train mean loss: 608.14
epoch train time: 0:00:02.125310
elapsed time: 0:03:55.037769
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 16:49:48.519569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 608.84
train mean loss: 605.98
epoch train time: 0:00:02.120196
elapsed time: 0:03:57.158221
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 16:49:50.640020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 603.09
train mean loss: 598.51
epoch train time: 0:00:02.134347
elapsed time: 0:03:59.292854
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 16:49:52.774662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 596.89
train mean loss: 596.48
epoch train time: 0:00:02.119689
elapsed time: 0:04:01.412840
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 16:49:54.894656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 581.33
train mean loss: 582.06
epoch train time: 0:00:02.125938
elapsed time: 0:04:03.539077
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 16:49:57.020912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 574.08
train mean loss: 575.71
epoch train time: 0:00:02.127643
elapsed time: 0:04:05.667020
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 16:49:59.148823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 571.21
train mean loss: 576.01
epoch train time: 0:00:02.127469
elapsed time: 0:04:07.794795
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 16:50:01.276604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 573.76
train mean loss: 570.15
epoch train time: 0:00:02.129852
elapsed time: 0:04:09.924930
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 16:50:03.406737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.59
train mean loss: 563.79
epoch train time: 0:00:02.124209
elapsed time: 0:04:12.049410
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 16:50:05.531214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.61
train mean loss: 561.52
epoch train time: 0:00:02.120279
elapsed time: 0:04:14.169955
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 16:50:07.651759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 573.87
train mean loss: 566.37
epoch train time: 0:00:02.130466
elapsed time: 0:04:16.300705
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 16:50:09.782508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 549.19
train mean loss: 545.55
epoch train time: 0:00:02.122148
elapsed time: 0:04:18.423125
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 16:50:11.904927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.57
train mean loss: 545.97
epoch train time: 0:00:02.114510
elapsed time: 0:04:20.537925
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 16:50:14.019723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 545.08
train mean loss: 544.85
epoch train time: 0:00:02.133952
elapsed time: 0:04:22.672132
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 16:50:16.153931
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 533.92
train mean loss: 534.52
epoch train time: 0:00:02.119737
elapsed time: 0:04:24.792122
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 16:50:18.273925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.37
train mean loss: 533.09
epoch train time: 0:00:02.122507
elapsed time: 0:04:26.914897
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 16:50:20.396698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.92
train mean loss: 529.78
epoch train time: 0:00:02.128189
elapsed time: 0:04:29.043363
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 16:50:22.525165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 538.24
train mean loss: 531.70
epoch train time: 0:00:02.130571
elapsed time: 0:04:31.174291
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 16:50:24.656095
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 521.41
train mean loss: 520.09
epoch train time: 0:00:02.117091
elapsed time: 0:04:33.291711
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 16:50:26.773491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.28
train mean loss: 522.78
epoch train time: 0:00:02.118548
elapsed time: 0:04:35.410516
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 16:50:28.892318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.68
train mean loss: 518.15
epoch train time: 0:00:02.130356
elapsed time: 0:04:37.541155
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 16:50:31.022999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.77
train mean loss: 508.61
epoch train time: 0:00:02.137613
elapsed time: 0:04:39.679120
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 16:50:33.160924
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.54
train mean loss: 503.00
epoch train time: 0:00:02.133102
elapsed time: 0:04:41.812479
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 16:50:35.294305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.46
train mean loss: 501.16
epoch train time: 0:00:02.116533
elapsed time: 0:04:43.929297
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 16:50:37.411115
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.16
train mean loss: 496.43
epoch train time: 0:00:02.122880
elapsed time: 0:04:46.052465
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 16:50:39.534269
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.27
train mean loss: 494.94
epoch train time: 0:00:02.113255
elapsed time: 0:04:48.166002
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 16:50:41.647801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.20
train mean loss: 483.73
epoch train time: 0:00:02.123759
elapsed time: 0:04:50.290035
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 16:50:43.771845
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.57
train mean loss: 479.13
epoch train time: 0:00:02.122865
elapsed time: 0:04:52.413168
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 16:50:45.894969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 480.75
train mean loss: 484.37
epoch train time: 0:00:02.115923
elapsed time: 0:04:54.529417
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 16:50:48.011219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.34
train mean loss: 478.72
epoch train time: 0:00:02.109451
elapsed time: 0:04:56.639153
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 16:50:50.120962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.48
train mean loss: 483.24
epoch train time: 0:00:02.114127
elapsed time: 0:04:58.753561
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 16:50:52.235364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.89
train mean loss: 471.18
epoch train time: 0:00:02.120099
elapsed time: 0:05:00.873922
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 16:50:54.355724
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.25
train mean loss: 470.29
epoch train time: 0:00:02.112795
elapsed time: 0:05:02.986982
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 16:50:56.468782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.08
train mean loss: 468.62
epoch train time: 0:00:02.113943
elapsed time: 0:05:05.101187
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 16:50:58.582999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.41
train mean loss: 467.25
epoch train time: 0:00:02.113530
elapsed time: 0:05:07.215058
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 16:51:00.696859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 455.57
train mean loss: 454.48
epoch train time: 0:00:02.117750
elapsed time: 0:05:09.333097
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 16:51:02.814898
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.01
train mean loss: 452.30
epoch train time: 0:00:02.123140
elapsed time: 0:05:11.456510
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 16:51:04.938326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.20
train mean loss: 452.97
epoch train time: 0:00:02.111846
elapsed time: 0:05:13.568637
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 16:51:07.050443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 444.30
train mean loss: 445.83
epoch train time: 0:00:02.126822
elapsed time: 0:05:15.695726
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 16:51:09.177543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.12
train mean loss: 447.52
epoch train time: 0:00:02.121764
elapsed time: 0:05:17.817793
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 16:51:11.299597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.78
train mean loss: 437.98
epoch train time: 0:00:02.131363
elapsed time: 0:05:19.949497
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 16:51:13.431268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.91
train mean loss: 441.97
epoch train time: 0:00:02.129554
elapsed time: 0:05:22.079277
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 16:51:15.561104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.40
train mean loss: 446.57
epoch train time: 0:00:02.126832
elapsed time: 0:05:24.206415
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 16:51:17.688219
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.82
train mean loss: 440.64
epoch train time: 0:00:02.135048
elapsed time: 0:05:26.341730
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 16:51:19.823537
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 435.77
train mean loss: 432.67
epoch train time: 0:00:02.116166
elapsed time: 0:05:28.458239
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 16:51:21.940054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.94
train mean loss: 435.25
epoch train time: 0:00:02.118945
elapsed time: 0:05:30.577474
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 16:51:24.059279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.65
train mean loss: 430.64
epoch train time: 0:00:02.126221
elapsed time: 0:05:32.704003
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 16:51:26.185812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.79
train mean loss: 436.76
epoch train time: 0:00:02.110237
elapsed time: 0:05:34.814504
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 16:51:28.296319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 429.64
train mean loss: 427.73
epoch train time: 0:00:02.119915
elapsed time: 0:05:36.934705
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 16:51:30.416508
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.54
train mean loss: 429.74
epoch train time: 0:00:02.118905
elapsed time: 0:05:39.053935
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 16:51:32.535745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.60
train mean loss: 434.54
epoch train time: 0:00:02.127357
elapsed time: 0:05:41.181562
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 16:51:34.663364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.30
train mean loss: 433.10
epoch train time: 0:00:02.120242
elapsed time: 0:05:43.302083
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 16:51:36.783884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.71
train mean loss: 419.67
epoch train time: 0:00:02.113009
elapsed time: 0:05:45.415382
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 16:51:38.897183
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.64
train mean loss: 421.92
epoch train time: 0:00:02.124016
elapsed time: 0:05:47.539675
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 16:51:41.021480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.78
train mean loss: 425.67
epoch train time: 0:00:02.125240
elapsed time: 0:05:49.665187
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 16:51:43.146989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.78
train mean loss: 422.59
epoch train time: 0:00:02.115276
elapsed time: 0:05:51.780726
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 16:51:45.262524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 419.78
train mean loss: 420.59
epoch train time: 0:00:02.125308
elapsed time: 0:05:53.906282
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 16:51:47.388113
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.21
train mean loss: 421.68
epoch train time: 0:00:02.121079
elapsed time: 0:05:56.027661
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 16:51:49.509469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.01
train mean loss: 412.56
epoch train time: 0:00:02.117571
elapsed time: 0:05:58.145540
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 16:51:51.627356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.61
train mean loss: 421.72
epoch train time: 0:00:02.122801
elapsed time: 0:06:00.268651
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 16:51:53.750475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.82
train mean loss: 417.30
epoch train time: 0:00:02.128264
elapsed time: 0:06:02.397302
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 16:51:55.879109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 415.34
train mean loss: 414.71
epoch train time: 0:00:02.125850
elapsed time: 0:06:04.523454
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 16:51:58.005256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.60
train mean loss: 408.57
epoch train time: 0:00:02.119646
elapsed time: 0:06:06.643363
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 16:52:00.125168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.93
train mean loss: 402.67
epoch train time: 0:00:02.124334
elapsed time: 0:06:08.767982
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 16:52:02.249785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.32
train mean loss: 405.82
epoch train time: 0:00:02.122763
elapsed time: 0:06:10.891010
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 16:52:04.372814
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.21
train mean loss: 406.20
epoch train time: 0:00:02.121686
elapsed time: 0:06:13.012998
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 16:52:06.494769
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.04
train mean loss: 400.44
epoch train time: 0:00:02.124435
elapsed time: 0:06:15.137677
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 16:52:08.619479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.68
train mean loss: 404.54
epoch train time: 0:00:02.123131
elapsed time: 0:06:17.261078
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 16:52:10.742877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.73
train mean loss: 403.63
epoch train time: 0:00:02.116359
elapsed time: 0:06:19.377700
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 16:52:12.859512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.28
train mean loss: 399.91
epoch train time: 0:00:02.119493
elapsed time: 0:06:21.497523
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 16:52:14.979379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.77
train mean loss: 394.87
epoch train time: 0:00:02.118458
elapsed time: 0:06:23.616333
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 16:52:17.098136
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.89
train mean loss: 395.74
epoch train time: 0:00:02.123998
elapsed time: 0:06:25.740618
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 16:52:19.222420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.61
train mean loss: 399.85
epoch train time: 0:00:02.111720
elapsed time: 0:06:27.852612
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 16:52:21.334430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.23
train mean loss: 396.96
epoch train time: 0:00:02.117910
elapsed time: 0:06:29.970809
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 16:52:23.452613
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.86
train mean loss: 393.66
epoch train time: 0:00:02.120369
elapsed time: 0:06:32.091440
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 16:52:25.573246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.51
train mean loss: 389.31
epoch train time: 0:00:02.114380
elapsed time: 0:06:34.206115
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 16:52:27.687899
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.91
train mean loss: 393.90
epoch train time: 0:00:02.122112
elapsed time: 0:06:36.328499
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 16:52:29.810300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.11
train mean loss: 393.69
epoch train time: 0:00:02.122307
elapsed time: 0:06:38.451071
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 16:52:31.932881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.84
train mean loss: 387.03
epoch train time: 0:00:02.126547
elapsed time: 0:06:40.577890
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 16:52:34.059722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.68
train mean loss: 378.09
epoch train time: 0:00:02.114858
elapsed time: 0:06:42.693031
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 16:52:36.174830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.07
train mean loss: 381.81
epoch train time: 0:00:02.120994
elapsed time: 0:06:44.814343
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 16:52:38.296161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.97
train mean loss: 385.45
epoch train time: 0:00:02.120643
elapsed time: 0:06:46.935269
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 16:52:40.417071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.89
train mean loss: 383.85
epoch train time: 0:00:02.123687
elapsed time: 0:06:49.059218
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 16:52:42.541024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.64
train mean loss: 380.29
epoch train time: 0:00:02.122677
elapsed time: 0:06:51.182164
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 16:52:44.663980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.16
train mean loss: 367.14
epoch train time: 0:00:02.113122
elapsed time: 0:06:53.295617
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 16:52:46.777427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.53
train mean loss: 379.97
epoch train time: 0:00:02.112228
elapsed time: 0:06:55.408117
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 16:52:48.889921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.82
train mean loss: 380.37
epoch train time: 0:00:02.134904
elapsed time: 0:06:57.543301
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 16:52:51.025160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.86
train mean loss: 377.66
epoch train time: 0:00:02.121096
elapsed time: 0:06:59.664722
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 16:52:53.146542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.03
train mean loss: 371.05
epoch train time: 0:00:02.114786
elapsed time: 0:07:01.779796
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 16:52:55.261620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.24
train mean loss: 370.54
epoch train time: 0:00:02.115518
elapsed time: 0:07:03.895599
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 16:52:57.377416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.49
train mean loss: 368.34
epoch train time: 0:00:02.120951
elapsed time: 0:07:06.016842
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 16:52:59.498644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.27
train mean loss: 369.34
epoch train time: 0:00:02.133011
elapsed time: 0:07:08.150132
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 16:53:01.631946
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.64
train mean loss: 372.93
epoch train time: 0:00:02.129325
elapsed time: 0:07:10.279767
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 16:53:03.761569
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.69
train mean loss: 367.20
epoch train time: 0:00:02.124699
elapsed time: 0:07:12.404791
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 16:53:05.886563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.23
train mean loss: 360.10
epoch train time: 0:00:02.122026
elapsed time: 0:07:14.527066
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 16:53:08.008880
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.98
train mean loss: 368.02
epoch train time: 0:00:02.120855
elapsed time: 0:07:16.648236
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 16:53:10.130041
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.73
train mean loss: 370.97
epoch train time: 0:00:02.124461
elapsed time: 0:07:18.772963
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 16:53:12.254785
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.31
train mean loss: 361.58
epoch train time: 0:00:02.125158
elapsed time: 0:07:20.898402
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 16:53:14.380206
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.79
train mean loss: 364.79
epoch train time: 0:00:02.135221
elapsed time: 0:07:23.033899
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 16:53:16.515706
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.85
train mean loss: 369.56
epoch train time: 0:00:02.123273
elapsed time: 0:07:25.157437
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 16:53:18.639240
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.00
train mean loss: 369.21
epoch train time: 0:00:02.129811
elapsed time: 0:07:27.287577
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 16:53:20.769401
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.14
train mean loss: 373.79
epoch train time: 0:00:02.128614
elapsed time: 0:07:29.416484
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 16:53:22.898289
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.63
train mean loss: 365.92
epoch train time: 0:00:02.120736
elapsed time: 0:07:31.537494
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 16:53:25.019297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.61
train mean loss: 365.88
epoch train time: 0:00:02.123144
elapsed time: 0:07:33.660916
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 16:53:27.142717
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.99
train mean loss: 372.47
epoch train time: 0:00:02.116067
elapsed time: 0:07:35.777283
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 16:53:29.259088
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.60
train mean loss: 368.88
epoch train time: 0:00:02.120053
elapsed time: 0:07:37.897629
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 16:53:31.379452
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.16
train mean loss: 357.21
epoch train time: 0:00:02.128245
elapsed time: 0:07:40.026157
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 16:53:33.507959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.74
train mean loss: 367.01
epoch train time: 0:00:02.139610
elapsed time: 0:07:42.166043
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 16:53:35.647857
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.42
train mean loss: 368.27
epoch train time: 0:00:02.152587
elapsed time: 0:07:44.318919
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 16:53:37.800735
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.86
train mean loss: 362.96
epoch train time: 0:00:02.147947
elapsed time: 0:07:46.467238
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 16:53:39.949068
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.68
train mean loss: 366.22
epoch train time: 0:00:02.136737
elapsed time: 0:07:48.604284
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 16:53:42.086090
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.97
train mean loss: 359.24
epoch train time: 0:00:02.136392
elapsed time: 0:07:50.740963
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 16:53:44.222763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.71
train mean loss: 363.75
epoch train time: 0:00:02.120068
elapsed time: 0:07:52.861318
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 16:53:46.343125
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.17
train mean loss: 366.24
epoch train time: 0:00:02.115818
elapsed time: 0:07:54.977415
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 16:53:48.459236
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.68
train mean loss: 362.13
epoch train time: 0:00:02.117788
elapsed time: 0:07:57.095529
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 16:53:50.577346
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.54
train mean loss: 366.61
epoch train time: 0:00:02.128659
elapsed time: 0:07:59.224524
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 16:53:52.706334
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.29
train mean loss: 364.36
epoch train time: 0:00:02.132946
elapsed time: 0:08:01.357766
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 16:53:54.839583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.40
train mean loss: 366.01
epoch train time: 0:00:02.128554
elapsed time: 0:08:03.486660
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 16:53:56.968463
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.19
train mean loss: 366.21
epoch train time: 0:00:02.124587
elapsed time: 0:08:05.611545
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 16:53:59.093349
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.48
train mean loss: 359.87
epoch train time: 0:00:02.138849
elapsed time: 0:08:07.750666
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 16:54:01.232490
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.12
train mean loss: 363.52
epoch train time: 0:00:02.121704
elapsed time: 0:08:09.872672
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 16:54:03.354469
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.10
train mean loss: 370.61
epoch train time: 0:00:02.122197
elapsed time: 0:08:11.995164
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 16:54:05.476983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.34
train mean loss: 364.09
epoch train time: 0:00:02.120286
elapsed time: 0:08:14.115718
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 16:54:07.597520
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.70
train mean loss: 365.02
epoch train time: 0:00:02.121342
elapsed time: 0:08:16.237397
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 16:54:09.719216
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.03
train mean loss: 362.79
epoch train time: 0:00:02.113563
elapsed time: 0:08:18.351301
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 16:54:11.833115
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.36
train mean loss: 363.02
epoch train time: 0:00:02.120560
elapsed time: 0:08:20.472235
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 16:54:13.954010
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 370.19
train mean loss: 365.86
epoch train time: 0:00:02.126443
elapsed time: 0:08:22.598988
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 16:54:16.080807
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 353.79
train mean loss: 362.11
epoch train time: 0:00:02.118301
elapsed time: 0:08:24.717595
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 16:54:18.199420
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 372.15
train mean loss: 370.41
epoch train time: 0:00:02.125075
elapsed time: 0:08:26.842954
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 16:54:20.324753
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.52
train mean loss: 364.34
epoch train time: 0:00:02.121830
elapsed time: 0:08:28.965044
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 16:54:22.446853
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 361.09
train mean loss: 363.67
epoch train time: 0:00:02.121383
elapsed time: 0:08:31.086700
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 16:54:24.568504
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.00
train mean loss: 363.64
epoch train time: 0:00:02.115954
elapsed time: 0:08:33.202948
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 16:54:26.684761
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.69
train mean loss: 362.23
epoch train time: 0:00:02.121392
elapsed time: 0:08:35.324650
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 16:54:28.806499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.68
train mean loss: 365.19
epoch train time: 0:00:02.120740
elapsed time: 0:08:37.445710
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 16:54:30.927511
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.67
train mean loss: 366.36
epoch train time: 0:00:02.126076
elapsed time: 0:08:39.572073
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 16:54:33.053875
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.31
train mean loss: 360.50
epoch train time: 0:00:02.120382
elapsed time: 0:08:41.692737
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 16:54:35.174541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.90
train mean loss: 362.56
epoch train time: 0:00:02.116656
elapsed time: 0:08:43.809646
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 16:54:37.291445
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.00
train mean loss: 364.05
epoch train time: 0:00:02.121348
elapsed time: 0:08:45.931267
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 16:54:39.413065
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 358.75
train mean loss: 359.59
epoch train time: 0:00:02.114691
elapsed time: 0:08:48.046279
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 16:54:41.528079
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.51
train mean loss: 362.09
epoch train time: 0:00:02.116428
elapsed time: 0:08:50.162994
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 16:54:43.644797
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.06
train mean loss: 364.42
epoch train time: 0:00:02.119968
elapsed time: 0:08:52.283229
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 16:54:45.765072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.56
train mean loss: 362.15
epoch train time: 0:00:02.119116
elapsed time: 0:08:54.411868
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_0.125/bayesian_conv5_dense1_0.125_6/checkpoint.pth.tar
**** end time: 2019-09-25 16:54:47.893642 ****
