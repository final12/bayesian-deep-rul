Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_5', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 22060
use_cuda: True
Dataset: CMAPSS/FD002
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-25 21:32:50.748399 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 21, 24]             100
              Tanh-2           [-1, 10, 21, 24]               0
            Conv2d-3           [-1, 10, 20, 24]           1,000
              Tanh-4           [-1, 10, 20, 24]               0
            Conv2d-5           [-1, 10, 21, 24]           1,000
              Tanh-6           [-1, 10, 21, 24]               0
            Conv2d-7           [-1, 10, 20, 24]           1,000
              Tanh-8           [-1, 10, 20, 24]               0
            Conv2d-9            [-1, 1, 20, 24]              30
             Tanh-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
          Dropout-12                  [-1, 480]               0
           Linear-13                  [-1, 100]          48,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 51,230
Trainable params: 51,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-25 21:32:50.757298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3713.69
 ---- batch: 020 ----
mean loss: 1571.39
 ---- batch: 030 ----
mean loss: 1227.17
 ---- batch: 040 ----
mean loss: 1074.73
 ---- batch: 050 ----
mean loss: 1014.27
 ---- batch: 060 ----
mean loss: 986.73
 ---- batch: 070 ----
mean loss: 951.54
 ---- batch: 080 ----
mean loss: 937.90
 ---- batch: 090 ----
mean loss: 929.81
train mean loss: 1348.78
epoch train time: 0:00:34.896252
elapsed time: 0:00:34.908329
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-25 21:33:25.656776
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 889.52
 ---- batch: 020 ----
mean loss: 873.05
 ---- batch: 030 ----
mean loss: 853.87
 ---- batch: 040 ----
mean loss: 840.88
 ---- batch: 050 ----
mean loss: 840.12
 ---- batch: 060 ----
mean loss: 822.63
 ---- batch: 070 ----
mean loss: 794.60
 ---- batch: 080 ----
mean loss: 810.44
 ---- batch: 090 ----
mean loss: 802.82
train mean loss: 831.35
epoch train time: 0:00:02.423623
elapsed time: 0:00:37.332125
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-25 21:33:28.080585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 753.78
 ---- batch: 020 ----
mean loss: 734.40
 ---- batch: 030 ----
mean loss: 731.86
 ---- batch: 040 ----
mean loss: 744.18
 ---- batch: 050 ----
mean loss: 719.27
 ---- batch: 060 ----
mean loss: 696.99
 ---- batch: 070 ----
mean loss: 698.75
 ---- batch: 080 ----
mean loss: 682.65
 ---- batch: 090 ----
mean loss: 671.25
train mean loss: 711.76
epoch train time: 0:00:02.357421
elapsed time: 0:00:39.689727
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-25 21:33:30.438190
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.56
 ---- batch: 020 ----
mean loss: 647.40
 ---- batch: 030 ----
mean loss: 639.57
 ---- batch: 040 ----
mean loss: 620.93
 ---- batch: 050 ----
mean loss: 623.29
 ---- batch: 060 ----
mean loss: 628.45
 ---- batch: 070 ----
mean loss: 612.74
 ---- batch: 080 ----
mean loss: 597.13
 ---- batch: 090 ----
mean loss: 590.41
train mean loss: 623.79
epoch train time: 0:00:02.364292
elapsed time: 0:00:42.054210
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-25 21:33:32.802695
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 590.12
 ---- batch: 020 ----
mean loss: 588.48
 ---- batch: 030 ----
mean loss: 595.46
 ---- batch: 040 ----
mean loss: 576.04
 ---- batch: 050 ----
mean loss: 588.95
 ---- batch: 060 ----
mean loss: 581.57
 ---- batch: 070 ----
mean loss: 573.30
 ---- batch: 080 ----
mean loss: 572.60
 ---- batch: 090 ----
mean loss: 564.21
train mean loss: 578.52
epoch train time: 0:00:02.355591
elapsed time: 0:00:44.410013
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-25 21:33:35.158467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 562.21
 ---- batch: 020 ----
mean loss: 555.00
 ---- batch: 030 ----
mean loss: 558.59
 ---- batch: 040 ----
mean loss: 555.15
 ---- batch: 050 ----
mean loss: 567.57
 ---- batch: 060 ----
mean loss: 531.25
 ---- batch: 070 ----
mean loss: 525.28
 ---- batch: 080 ----
mean loss: 526.44
 ---- batch: 090 ----
mean loss: 531.49
train mean loss: 544.37
epoch train time: 0:00:02.360968
elapsed time: 0:00:46.771156
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-25 21:33:37.519612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.99
 ---- batch: 020 ----
mean loss: 511.23
 ---- batch: 030 ----
mean loss: 505.57
 ---- batch: 040 ----
mean loss: 510.58
 ---- batch: 050 ----
mean loss: 488.88
 ---- batch: 060 ----
mean loss: 518.57
 ---- batch: 070 ----
mean loss: 493.64
 ---- batch: 080 ----
mean loss: 487.50
 ---- batch: 090 ----
mean loss: 497.52
train mean loss: 500.78
epoch train time: 0:00:02.355973
elapsed time: 0:00:49.127338
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-25 21:33:39.875806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.47
 ---- batch: 020 ----
mean loss: 501.00
 ---- batch: 030 ----
mean loss: 456.45
 ---- batch: 040 ----
mean loss: 489.75
 ---- batch: 050 ----
mean loss: 479.80
 ---- batch: 060 ----
mean loss: 475.03
 ---- batch: 070 ----
mean loss: 482.82
 ---- batch: 080 ----
mean loss: 460.62
 ---- batch: 090 ----
mean loss: 461.85
train mean loss: 474.36
epoch train time: 0:00:02.351285
elapsed time: 0:00:51.478823
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-25 21:33:42.227279
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.50
 ---- batch: 020 ----
mean loss: 449.60
 ---- batch: 030 ----
mean loss: 458.31
 ---- batch: 040 ----
mean loss: 453.57
 ---- batch: 050 ----
mean loss: 441.32
 ---- batch: 060 ----
mean loss: 446.35
 ---- batch: 070 ----
mean loss: 460.73
 ---- batch: 080 ----
mean loss: 471.18
 ---- batch: 090 ----
mean loss: 458.80
train mean loss: 454.48
epoch train time: 0:00:02.363393
elapsed time: 0:00:53.842395
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-25 21:33:44.590854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.64
 ---- batch: 020 ----
mean loss: 448.16
 ---- batch: 030 ----
mean loss: 422.08
 ---- batch: 040 ----
mean loss: 444.16
 ---- batch: 050 ----
mean loss: 442.06
 ---- batch: 060 ----
mean loss: 446.50
 ---- batch: 070 ----
mean loss: 438.00
 ---- batch: 080 ----
mean loss: 447.54
 ---- batch: 090 ----
mean loss: 448.16
train mean loss: 445.91
epoch train time: 0:00:02.352027
elapsed time: 0:00:56.194615
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-25 21:33:46.943071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 457.12
 ---- batch: 020 ----
mean loss: 462.21
 ---- batch: 030 ----
mean loss: 448.24
 ---- batch: 040 ----
mean loss: 447.60
 ---- batch: 050 ----
mean loss: 455.84
 ---- batch: 060 ----
mean loss: 443.62
 ---- batch: 070 ----
mean loss: 446.09
 ---- batch: 080 ----
mean loss: 439.48
 ---- batch: 090 ----
mean loss: 434.60
train mean loss: 447.65
epoch train time: 0:00:02.355569
elapsed time: 0:00:58.550361
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-25 21:33:49.298833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.20
 ---- batch: 020 ----
mean loss: 446.54
 ---- batch: 030 ----
mean loss: 446.28
 ---- batch: 040 ----
mean loss: 429.59
 ---- batch: 050 ----
mean loss: 422.98
 ---- batch: 060 ----
mean loss: 420.29
 ---- batch: 070 ----
mean loss: 431.02
 ---- batch: 080 ----
mean loss: 424.08
 ---- batch: 090 ----
mean loss: 423.13
train mean loss: 431.55
epoch train time: 0:00:02.340136
elapsed time: 0:01:00.890690
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-25 21:33:51.639149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.87
 ---- batch: 020 ----
mean loss: 428.17
 ---- batch: 030 ----
mean loss: 438.05
 ---- batch: 040 ----
mean loss: 423.69
 ---- batch: 050 ----
mean loss: 452.32
 ---- batch: 060 ----
mean loss: 440.14
 ---- batch: 070 ----
mean loss: 429.39
 ---- batch: 080 ----
mean loss: 445.90
 ---- batch: 090 ----
mean loss: 444.60
train mean loss: 435.61
epoch train time: 0:00:02.346791
elapsed time: 0:01:03.237684
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-25 21:33:53.986159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.64
 ---- batch: 020 ----
mean loss: 457.73
 ---- batch: 030 ----
mean loss: 434.78
 ---- batch: 040 ----
mean loss: 431.56
 ---- batch: 050 ----
mean loss: 415.57
 ---- batch: 060 ----
mean loss: 423.63
 ---- batch: 070 ----
mean loss: 411.19
 ---- batch: 080 ----
mean loss: 427.95
 ---- batch: 090 ----
mean loss: 443.13
train mean loss: 432.17
epoch train time: 0:00:02.348091
elapsed time: 0:01:05.585986
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-25 21:33:56.334444
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.16
 ---- batch: 020 ----
mean loss: 417.12
 ---- batch: 030 ----
mean loss: 436.75
 ---- batch: 040 ----
mean loss: 438.50
 ---- batch: 050 ----
mean loss: 417.04
 ---- batch: 060 ----
mean loss: 431.78
 ---- batch: 070 ----
mean loss: 425.50
 ---- batch: 080 ----
mean loss: 420.51
 ---- batch: 090 ----
mean loss: 423.97
train mean loss: 426.69
epoch train time: 0:00:02.345704
elapsed time: 0:01:07.931865
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-25 21:33:58.680358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.03
 ---- batch: 020 ----
mean loss: 429.23
 ---- batch: 030 ----
mean loss: 421.20
 ---- batch: 040 ----
mean loss: 413.50
 ---- batch: 050 ----
mean loss: 414.67
 ---- batch: 060 ----
mean loss: 417.33
 ---- batch: 070 ----
mean loss: 417.90
 ---- batch: 080 ----
mean loss: 406.67
 ---- batch: 090 ----
mean loss: 434.04
train mean loss: 420.47
epoch train time: 0:00:02.338629
elapsed time: 0:01:10.270705
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-25 21:34:01.019173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 433.45
 ---- batch: 020 ----
mean loss: 431.40
 ---- batch: 030 ----
mean loss: 415.54
 ---- batch: 040 ----
mean loss: 403.22
 ---- batch: 050 ----
mean loss: 417.61
 ---- batch: 060 ----
mean loss: 409.25
 ---- batch: 070 ----
mean loss: 412.56
 ---- batch: 080 ----
mean loss: 436.26
 ---- batch: 090 ----
mean loss: 437.56
train mean loss: 423.57
epoch train time: 0:00:02.337274
elapsed time: 0:01:12.608185
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-25 21:34:03.356642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 426.21
 ---- batch: 020 ----
mean loss: 415.77
 ---- batch: 030 ----
mean loss: 409.50
 ---- batch: 040 ----
mean loss: 407.15
 ---- batch: 050 ----
mean loss: 398.96
 ---- batch: 060 ----
mean loss: 440.26
 ---- batch: 070 ----
mean loss: 428.53
 ---- batch: 080 ----
mean loss: 440.43
 ---- batch: 090 ----
mean loss: 434.60
train mean loss: 422.18
epoch train time: 0:00:02.340574
elapsed time: 0:01:14.948959
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-25 21:34:05.697423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.97
 ---- batch: 020 ----
mean loss: 420.09
 ---- batch: 030 ----
mean loss: 412.18
 ---- batch: 040 ----
mean loss: 413.33
 ---- batch: 050 ----
mean loss: 414.28
 ---- batch: 060 ----
mean loss: 409.09
 ---- batch: 070 ----
mean loss: 411.75
 ---- batch: 080 ----
mean loss: 420.90
 ---- batch: 090 ----
mean loss: 419.15
train mean loss: 414.77
epoch train time: 0:00:02.355498
elapsed time: 0:01:17.304652
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-25 21:34:08.053138
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.86
 ---- batch: 020 ----
mean loss: 406.44
 ---- batch: 030 ----
mean loss: 424.06
 ---- batch: 040 ----
mean loss: 418.92
 ---- batch: 050 ----
mean loss: 417.78
 ---- batch: 060 ----
mean loss: 414.42
 ---- batch: 070 ----
mean loss: 407.53
 ---- batch: 080 ----
mean loss: 414.35
 ---- batch: 090 ----
mean loss: 426.93
train mean loss: 416.71
epoch train time: 0:00:02.332302
elapsed time: 0:01:19.637163
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-25 21:34:10.385640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.62
 ---- batch: 020 ----
mean loss: 413.72
 ---- batch: 030 ----
mean loss: 397.19
 ---- batch: 040 ----
mean loss: 419.40
 ---- batch: 050 ----
mean loss: 410.13
 ---- batch: 060 ----
mean loss: 449.41
 ---- batch: 070 ----
mean loss: 422.26
 ---- batch: 080 ----
mean loss: 414.94
 ---- batch: 090 ----
mean loss: 412.13
train mean loss: 417.84
epoch train time: 0:00:02.341962
elapsed time: 0:01:21.979325
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-25 21:34:12.727781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.37
 ---- batch: 020 ----
mean loss: 400.26
 ---- batch: 030 ----
mean loss: 415.39
 ---- batch: 040 ----
mean loss: 408.70
 ---- batch: 050 ----
mean loss: 406.80
 ---- batch: 060 ----
mean loss: 423.08
 ---- batch: 070 ----
mean loss: 408.70
 ---- batch: 080 ----
mean loss: 413.89
 ---- batch: 090 ----
mean loss: 425.07
train mean loss: 412.15
epoch train time: 0:00:02.345792
elapsed time: 0:01:24.325331
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-25 21:34:15.073795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.25
 ---- batch: 020 ----
mean loss: 413.53
 ---- batch: 030 ----
mean loss: 424.21
 ---- batch: 040 ----
mean loss: 408.41
 ---- batch: 050 ----
mean loss: 417.01
 ---- batch: 060 ----
mean loss: 414.01
 ---- batch: 070 ----
mean loss: 408.88
 ---- batch: 080 ----
mean loss: 428.13
 ---- batch: 090 ----
mean loss: 398.87
train mean loss: 414.25
epoch train time: 0:00:02.329729
elapsed time: 0:01:26.655244
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-25 21:34:17.403705
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 424.42
 ---- batch: 020 ----
mean loss: 405.13
 ---- batch: 030 ----
mean loss: 420.43
 ---- batch: 040 ----
mean loss: 392.25
 ---- batch: 050 ----
mean loss: 414.53
 ---- batch: 060 ----
mean loss: 403.09
 ---- batch: 070 ----
mean loss: 413.28
 ---- batch: 080 ----
mean loss: 416.13
 ---- batch: 090 ----
mean loss: 415.19
train mean loss: 412.11
epoch train time: 0:00:02.337003
elapsed time: 0:01:28.992444
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-25 21:34:19.740901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 418.90
 ---- batch: 020 ----
mean loss: 416.10
 ---- batch: 030 ----
mean loss: 412.92
 ---- batch: 040 ----
mean loss: 402.86
 ---- batch: 050 ----
mean loss: 399.67
 ---- batch: 060 ----
mean loss: 411.33
 ---- batch: 070 ----
mean loss: 393.95
 ---- batch: 080 ----
mean loss: 410.84
 ---- batch: 090 ----
mean loss: 414.82
train mean loss: 408.78
epoch train time: 0:00:02.341679
elapsed time: 0:01:31.334304
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-25 21:34:22.082768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.44
 ---- batch: 020 ----
mean loss: 415.82
 ---- batch: 030 ----
mean loss: 406.45
 ---- batch: 040 ----
mean loss: 417.71
 ---- batch: 050 ----
mean loss: 402.94
 ---- batch: 060 ----
mean loss: 417.78
 ---- batch: 070 ----
mean loss: 402.00
 ---- batch: 080 ----
mean loss: 416.58
 ---- batch: 090 ----
mean loss: 402.77
train mean loss: 408.93
epoch train time: 0:00:02.334249
elapsed time: 0:01:33.668741
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-25 21:34:24.417224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.06
 ---- batch: 020 ----
mean loss: 420.34
 ---- batch: 030 ----
mean loss: 416.63
 ---- batch: 040 ----
mean loss: 416.94
 ---- batch: 050 ----
mean loss: 426.59
 ---- batch: 060 ----
mean loss: 417.40
 ---- batch: 070 ----
mean loss: 390.37
 ---- batch: 080 ----
mean loss: 404.33
 ---- batch: 090 ----
mean loss: 400.02
train mean loss: 410.95
epoch train time: 0:00:02.334719
elapsed time: 0:01:36.003667
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-25 21:34:26.752161
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 427.36
 ---- batch: 020 ----
mean loss: 395.57
 ---- batch: 030 ----
mean loss: 410.57
 ---- batch: 040 ----
mean loss: 418.98
 ---- batch: 050 ----
mean loss: 410.70
 ---- batch: 060 ----
mean loss: 401.72
 ---- batch: 070 ----
mean loss: 398.07
 ---- batch: 080 ----
mean loss: 398.92
 ---- batch: 090 ----
mean loss: 394.07
train mean loss: 405.82
epoch train time: 0:00:02.332328
elapsed time: 0:01:38.336229
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-25 21:34:29.084699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.36
 ---- batch: 020 ----
mean loss: 401.48
 ---- batch: 030 ----
mean loss: 385.86
 ---- batch: 040 ----
mean loss: 417.13
 ---- batch: 050 ----
mean loss: 400.70
 ---- batch: 060 ----
mean loss: 400.93
 ---- batch: 070 ----
mean loss: 407.56
 ---- batch: 080 ----
mean loss: 395.95
 ---- batch: 090 ----
mean loss: 395.70
train mean loss: 400.53
epoch train time: 0:00:02.336940
elapsed time: 0:01:40.673360
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-25 21:34:31.421834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 436.83
 ---- batch: 020 ----
mean loss: 414.74
 ---- batch: 030 ----
mean loss: 395.55
 ---- batch: 040 ----
mean loss: 393.64
 ---- batch: 050 ----
mean loss: 418.91
 ---- batch: 060 ----
mean loss: 424.00
 ---- batch: 070 ----
mean loss: 416.98
 ---- batch: 080 ----
mean loss: 430.17
 ---- batch: 090 ----
mean loss: 398.73
train mean loss: 413.16
epoch train time: 0:00:02.339257
elapsed time: 0:01:43.012815
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-25 21:34:33.761272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.42
 ---- batch: 020 ----
mean loss: 395.13
 ---- batch: 030 ----
mean loss: 405.14
 ---- batch: 040 ----
mean loss: 435.80
 ---- batch: 050 ----
mean loss: 418.77
 ---- batch: 060 ----
mean loss: 403.80
 ---- batch: 070 ----
mean loss: 411.10
 ---- batch: 080 ----
mean loss: 401.80
 ---- batch: 090 ----
mean loss: 403.58
train mean loss: 408.94
epoch train time: 0:00:02.326661
elapsed time: 0:01:45.339670
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-25 21:34:36.088146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.81
 ---- batch: 020 ----
mean loss: 406.31
 ---- batch: 030 ----
mean loss: 405.01
 ---- batch: 040 ----
mean loss: 387.84
 ---- batch: 050 ----
mean loss: 408.58
 ---- batch: 060 ----
mean loss: 412.93
 ---- batch: 070 ----
mean loss: 408.59
 ---- batch: 080 ----
mean loss: 410.17
 ---- batch: 090 ----
mean loss: 414.69
train mean loss: 407.25
epoch train time: 0:00:02.327224
elapsed time: 0:01:47.667135
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-25 21:34:38.415592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.90
 ---- batch: 020 ----
mean loss: 405.73
 ---- batch: 030 ----
mean loss: 408.21
 ---- batch: 040 ----
mean loss: 408.80
 ---- batch: 050 ----
mean loss: 413.28
 ---- batch: 060 ----
mean loss: 412.95
 ---- batch: 070 ----
mean loss: 407.53
 ---- batch: 080 ----
mean loss: 404.40
 ---- batch: 090 ----
mean loss: 388.37
train mean loss: 405.16
epoch train time: 0:00:02.340685
elapsed time: 0:01:50.007997
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-25 21:34:40.756453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.04
 ---- batch: 020 ----
mean loss: 407.40
 ---- batch: 030 ----
mean loss: 400.54
 ---- batch: 040 ----
mean loss: 392.43
 ---- batch: 050 ----
mean loss: 400.79
 ---- batch: 060 ----
mean loss: 398.78
 ---- batch: 070 ----
mean loss: 398.33
 ---- batch: 080 ----
mean loss: 406.07
 ---- batch: 090 ----
mean loss: 411.77
train mean loss: 403.48
epoch train time: 0:00:02.332267
elapsed time: 0:01:52.340471
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-25 21:34:43.088951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.36
 ---- batch: 020 ----
mean loss: 412.15
 ---- batch: 030 ----
mean loss: 407.73
 ---- batch: 040 ----
mean loss: 397.68
 ---- batch: 050 ----
mean loss: 400.31
 ---- batch: 060 ----
mean loss: 399.93
 ---- batch: 070 ----
mean loss: 409.60
 ---- batch: 080 ----
mean loss: 407.64
 ---- batch: 090 ----
mean loss: 403.03
train mean loss: 403.27
epoch train time: 0:00:02.328788
elapsed time: 0:01:54.669464
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-25 21:34:45.417923
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.98
 ---- batch: 020 ----
mean loss: 406.36
 ---- batch: 030 ----
mean loss: 399.56
 ---- batch: 040 ----
mean loss: 397.56
 ---- batch: 050 ----
mean loss: 408.86
 ---- batch: 060 ----
mean loss: 384.73
 ---- batch: 070 ----
mean loss: 397.96
 ---- batch: 080 ----
mean loss: 397.82
 ---- batch: 090 ----
mean loss: 396.68
train mean loss: 399.54
epoch train time: 0:00:02.353588
elapsed time: 0:01:57.023271
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-25 21:34:47.771782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.27
 ---- batch: 020 ----
mean loss: 399.40
 ---- batch: 030 ----
mean loss: 392.34
 ---- batch: 040 ----
mean loss: 400.95
 ---- batch: 050 ----
mean loss: 404.95
 ---- batch: 060 ----
mean loss: 413.25
 ---- batch: 070 ----
mean loss: 411.92
 ---- batch: 080 ----
mean loss: 394.95
 ---- batch: 090 ----
mean loss: 398.31
train mean loss: 402.82
epoch train time: 0:00:02.330553
elapsed time: 0:01:59.354060
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-25 21:34:50.102520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.76
 ---- batch: 020 ----
mean loss: 399.18
 ---- batch: 030 ----
mean loss: 399.30
 ---- batch: 040 ----
mean loss: 399.22
 ---- batch: 050 ----
mean loss: 401.36
 ---- batch: 060 ----
mean loss: 411.29
 ---- batch: 070 ----
mean loss: 429.31
 ---- batch: 080 ----
mean loss: 426.74
 ---- batch: 090 ----
mean loss: 409.00
train mean loss: 407.25
epoch train time: 0:00:02.333771
elapsed time: 0:02:01.688033
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-25 21:34:52.436494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.19
 ---- batch: 020 ----
mean loss: 403.07
 ---- batch: 030 ----
mean loss: 396.10
 ---- batch: 040 ----
mean loss: 381.23
 ---- batch: 050 ----
mean loss: 400.21
 ---- batch: 060 ----
mean loss: 408.95
 ---- batch: 070 ----
mean loss: 409.97
 ---- batch: 080 ----
mean loss: 407.41
 ---- batch: 090 ----
mean loss: 400.05
train mean loss: 401.13
epoch train time: 0:00:02.325760
elapsed time: 0:02:04.013979
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-25 21:34:54.762446
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.39
 ---- batch: 020 ----
mean loss: 412.47
 ---- batch: 030 ----
mean loss: 410.48
 ---- batch: 040 ----
mean loss: 408.45
 ---- batch: 050 ----
mean loss: 402.69
 ---- batch: 060 ----
mean loss: 398.67
 ---- batch: 070 ----
mean loss: 403.56
 ---- batch: 080 ----
mean loss: 406.10
 ---- batch: 090 ----
mean loss: 404.06
train mean loss: 407.55
epoch train time: 0:00:02.327120
elapsed time: 0:02:06.341304
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-25 21:34:57.089766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.69
 ---- batch: 020 ----
mean loss: 428.55
 ---- batch: 030 ----
mean loss: 405.66
 ---- batch: 040 ----
mean loss: 407.09
 ---- batch: 050 ----
mean loss: 408.46
 ---- batch: 060 ----
mean loss: 401.67
 ---- batch: 070 ----
mean loss: 388.41
 ---- batch: 080 ----
mean loss: 405.21
 ---- batch: 090 ----
mean loss: 394.40
train mean loss: 406.00
epoch train time: 0:00:02.327961
elapsed time: 0:02:08.669443
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-25 21:34:59.417916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.39
 ---- batch: 020 ----
mean loss: 396.78
 ---- batch: 030 ----
mean loss: 407.05
 ---- batch: 040 ----
mean loss: 403.33
 ---- batch: 050 ----
mean loss: 413.74
 ---- batch: 060 ----
mean loss: 405.28
 ---- batch: 070 ----
mean loss: 388.31
 ---- batch: 080 ----
mean loss: 401.35
 ---- batch: 090 ----
mean loss: 398.55
train mean loss: 400.32
epoch train time: 0:00:02.338689
elapsed time: 0:02:11.008420
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-25 21:35:01.756929
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.09
 ---- batch: 020 ----
mean loss: 402.39
 ---- batch: 030 ----
mean loss: 421.70
 ---- batch: 040 ----
mean loss: 406.84
 ---- batch: 050 ----
mean loss: 402.15
 ---- batch: 060 ----
mean loss: 389.41
 ---- batch: 070 ----
mean loss: 395.41
 ---- batch: 080 ----
mean loss: 414.27
 ---- batch: 090 ----
mean loss: 398.98
train mean loss: 403.95
epoch train time: 0:00:02.336869
elapsed time: 0:02:13.345525
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-25 21:35:04.093984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.20
 ---- batch: 020 ----
mean loss: 406.20
 ---- batch: 030 ----
mean loss: 398.80
 ---- batch: 040 ----
mean loss: 396.93
 ---- batch: 050 ----
mean loss: 392.99
 ---- batch: 060 ----
mean loss: 388.77
 ---- batch: 070 ----
mean loss: 403.41
 ---- batch: 080 ----
mean loss: 390.63
 ---- batch: 090 ----
mean loss: 409.87
train mean loss: 399.75
epoch train time: 0:00:02.325761
elapsed time: 0:02:15.671459
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-25 21:35:06.419915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.44
 ---- batch: 020 ----
mean loss: 393.76
 ---- batch: 030 ----
mean loss: 406.08
 ---- batch: 040 ----
mean loss: 394.37
 ---- batch: 050 ----
mean loss: 390.16
 ---- batch: 060 ----
mean loss: 396.18
 ---- batch: 070 ----
mean loss: 400.42
 ---- batch: 080 ----
mean loss: 419.84
 ---- batch: 090 ----
mean loss: 404.63
train mean loss: 401.20
epoch train time: 0:00:02.329033
elapsed time: 0:02:18.000690
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-25 21:35:08.749149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.05
 ---- batch: 020 ----
mean loss: 415.08
 ---- batch: 030 ----
mean loss: 419.81
 ---- batch: 040 ----
mean loss: 398.69
 ---- batch: 050 ----
mean loss: 408.53
 ---- batch: 060 ----
mean loss: 405.83
 ---- batch: 070 ----
mean loss: 401.03
 ---- batch: 080 ----
mean loss: 398.57
 ---- batch: 090 ----
mean loss: 400.95
train mean loss: 406.31
epoch train time: 0:00:02.343275
elapsed time: 0:02:20.344149
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-25 21:35:11.092608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.52
 ---- batch: 020 ----
mean loss: 404.78
 ---- batch: 030 ----
mean loss: 415.03
 ---- batch: 040 ----
mean loss: 395.80
 ---- batch: 050 ----
mean loss: 403.20
 ---- batch: 060 ----
mean loss: 388.48
 ---- batch: 070 ----
mean loss: 390.38
 ---- batch: 080 ----
mean loss: 401.33
 ---- batch: 090 ----
mean loss: 411.28
train mean loss: 400.90
epoch train time: 0:00:02.340191
elapsed time: 0:02:22.684562
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-25 21:35:13.433046
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.11
 ---- batch: 020 ----
mean loss: 396.23
 ---- batch: 030 ----
mean loss: 402.05
 ---- batch: 040 ----
mean loss: 396.73
 ---- batch: 050 ----
mean loss: 396.90
 ---- batch: 060 ----
mean loss: 396.86
 ---- batch: 070 ----
mean loss: 399.45
 ---- batch: 080 ----
mean loss: 387.08
 ---- batch: 090 ----
mean loss: 397.95
train mean loss: 396.45
epoch train time: 0:00:02.330161
elapsed time: 0:02:25.014933
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-25 21:35:15.763390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.39
 ---- batch: 020 ----
mean loss: 389.12
 ---- batch: 030 ----
mean loss: 392.88
 ---- batch: 040 ----
mean loss: 389.18
 ---- batch: 050 ----
mean loss: 399.33
 ---- batch: 060 ----
mean loss: 389.81
 ---- batch: 070 ----
mean loss: 410.31
 ---- batch: 080 ----
mean loss: 400.71
 ---- batch: 090 ----
mean loss: 398.58
train mean loss: 397.29
epoch train time: 0:00:02.326736
elapsed time: 0:02:27.341851
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-25 21:35:18.090328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.70
 ---- batch: 020 ----
mean loss: 381.09
 ---- batch: 030 ----
mean loss: 402.12
 ---- batch: 040 ----
mean loss: 405.69
 ---- batch: 050 ----
mean loss: 399.66
 ---- batch: 060 ----
mean loss: 408.15
 ---- batch: 070 ----
mean loss: 399.29
 ---- batch: 080 ----
mean loss: 390.86
 ---- batch: 090 ----
mean loss: 398.47
train mean loss: 396.75
epoch train time: 0:00:02.323878
elapsed time: 0:02:29.665929
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-25 21:35:20.414397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.52
 ---- batch: 020 ----
mean loss: 389.86
 ---- batch: 030 ----
mean loss: 404.94
 ---- batch: 040 ----
mean loss: 404.29
 ---- batch: 050 ----
mean loss: 393.51
 ---- batch: 060 ----
mean loss: 396.84
 ---- batch: 070 ----
mean loss: 391.07
 ---- batch: 080 ----
mean loss: 405.92
 ---- batch: 090 ----
mean loss: 398.47
train mean loss: 396.87
epoch train time: 0:00:02.336949
elapsed time: 0:02:32.003106
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-25 21:35:22.751586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.46
 ---- batch: 020 ----
mean loss: 395.24
 ---- batch: 030 ----
mean loss: 385.13
 ---- batch: 040 ----
mean loss: 407.12
 ---- batch: 050 ----
mean loss: 387.74
 ---- batch: 060 ----
mean loss: 398.70
 ---- batch: 070 ----
mean loss: 389.59
 ---- batch: 080 ----
mean loss: 398.10
 ---- batch: 090 ----
mean loss: 396.51
train mean loss: 395.28
epoch train time: 0:00:02.329726
elapsed time: 0:02:34.333042
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-25 21:35:25.081500
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.89
 ---- batch: 020 ----
mean loss: 389.04
 ---- batch: 030 ----
mean loss: 393.61
 ---- batch: 040 ----
mean loss: 392.67
 ---- batch: 050 ----
mean loss: 411.67
 ---- batch: 060 ----
mean loss: 403.64
 ---- batch: 070 ----
mean loss: 377.00
 ---- batch: 080 ----
mean loss: 403.65
 ---- batch: 090 ----
mean loss: 382.31
train mean loss: 394.29
epoch train time: 0:00:02.332092
elapsed time: 0:02:36.665397
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-25 21:35:27.413876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.09
 ---- batch: 020 ----
mean loss: 388.13
 ---- batch: 030 ----
mean loss: 404.03
 ---- batch: 040 ----
mean loss: 386.37
 ---- batch: 050 ----
mean loss: 381.38
 ---- batch: 060 ----
mean loss: 388.12
 ---- batch: 070 ----
mean loss: 390.21
 ---- batch: 080 ----
mean loss: 407.87
 ---- batch: 090 ----
mean loss: 386.56
train mean loss: 392.29
epoch train time: 0:00:02.339647
elapsed time: 0:02:39.005288
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-25 21:35:29.753757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 402.98
 ---- batch: 020 ----
mean loss: 398.30
 ---- batch: 030 ----
mean loss: 399.52
 ---- batch: 040 ----
mean loss: 394.14
 ---- batch: 050 ----
mean loss: 393.79
 ---- batch: 060 ----
mean loss: 400.75
 ---- batch: 070 ----
mean loss: 399.09
 ---- batch: 080 ----
mean loss: 393.86
 ---- batch: 090 ----
mean loss: 407.41
train mean loss: 399.46
epoch train time: 0:00:02.346035
elapsed time: 0:02:41.351550
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-25 21:35:32.100007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.27
 ---- batch: 020 ----
mean loss: 383.85
 ---- batch: 030 ----
mean loss: 392.98
 ---- batch: 040 ----
mean loss: 410.26
 ---- batch: 050 ----
mean loss: 388.12
 ---- batch: 060 ----
mean loss: 393.54
 ---- batch: 070 ----
mean loss: 399.29
 ---- batch: 080 ----
mean loss: 372.23
 ---- batch: 090 ----
mean loss: 379.61
train mean loss: 391.36
epoch train time: 0:00:02.340053
elapsed time: 0:02:43.691793
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-25 21:35:34.440265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.52
 ---- batch: 020 ----
mean loss: 414.80
 ---- batch: 030 ----
mean loss: 412.08
 ---- batch: 040 ----
mean loss: 404.26
 ---- batch: 050 ----
mean loss: 401.66
 ---- batch: 060 ----
mean loss: 400.12
 ---- batch: 070 ----
mean loss: 391.83
 ---- batch: 080 ----
mean loss: 400.35
 ---- batch: 090 ----
mean loss: 385.96
train mean loss: 399.46
epoch train time: 0:00:02.334275
elapsed time: 0:02:46.026280
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-25 21:35:36.774742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.44
 ---- batch: 020 ----
mean loss: 398.59
 ---- batch: 030 ----
mean loss: 389.14
 ---- batch: 040 ----
mean loss: 383.71
 ---- batch: 050 ----
mean loss: 393.94
 ---- batch: 060 ----
mean loss: 388.02
 ---- batch: 070 ----
mean loss: 384.02
 ---- batch: 080 ----
mean loss: 397.77
 ---- batch: 090 ----
mean loss: 397.40
train mean loss: 390.94
epoch train time: 0:00:02.331963
elapsed time: 0:02:48.358428
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-25 21:35:39.106901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.84
 ---- batch: 020 ----
mean loss: 415.83
 ---- batch: 030 ----
mean loss: 402.05
 ---- batch: 040 ----
mean loss: 398.06
 ---- batch: 050 ----
mean loss: 387.26
 ---- batch: 060 ----
mean loss: 401.03
 ---- batch: 070 ----
mean loss: 401.94
 ---- batch: 080 ----
mean loss: 409.19
 ---- batch: 090 ----
mean loss: 416.09
train mean loss: 405.11
epoch train time: 0:00:02.324290
elapsed time: 0:02:50.682924
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-25 21:35:41.431382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.45
 ---- batch: 020 ----
mean loss: 386.42
 ---- batch: 030 ----
mean loss: 404.02
 ---- batch: 040 ----
mean loss: 417.52
 ---- batch: 050 ----
mean loss: 402.11
 ---- batch: 060 ----
mean loss: 394.16
 ---- batch: 070 ----
mean loss: 396.73
 ---- batch: 080 ----
mean loss: 392.45
 ---- batch: 090 ----
mean loss: 408.92
train mean loss: 402.11
epoch train time: 0:00:02.338248
elapsed time: 0:02:53.021344
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-25 21:35:43.769825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.61
 ---- batch: 020 ----
mean loss: 395.93
 ---- batch: 030 ----
mean loss: 380.92
 ---- batch: 040 ----
mean loss: 399.20
 ---- batch: 050 ----
mean loss: 384.14
 ---- batch: 060 ----
mean loss: 381.81
 ---- batch: 070 ----
mean loss: 389.02
 ---- batch: 080 ----
mean loss: 410.89
 ---- batch: 090 ----
mean loss: 394.24
train mean loss: 390.88
epoch train time: 0:00:02.331925
elapsed time: 0:02:55.353488
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-25 21:35:46.101961
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.28
 ---- batch: 020 ----
mean loss: 404.16
 ---- batch: 030 ----
mean loss: 387.38
 ---- batch: 040 ----
mean loss: 389.79
 ---- batch: 050 ----
mean loss: 394.25
 ---- batch: 060 ----
mean loss: 403.37
 ---- batch: 070 ----
mean loss: 391.82
 ---- batch: 080 ----
mean loss: 405.73
 ---- batch: 090 ----
mean loss: 405.66
train mean loss: 398.47
epoch train time: 0:00:02.331533
elapsed time: 0:02:57.685247
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-25 21:35:48.433720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.84
 ---- batch: 020 ----
mean loss: 390.43
 ---- batch: 030 ----
mean loss: 393.26
 ---- batch: 040 ----
mean loss: 386.00
 ---- batch: 050 ----
mean loss: 405.26
 ---- batch: 060 ----
mean loss: 385.41
 ---- batch: 070 ----
mean loss: 410.54
 ---- batch: 080 ----
mean loss: 427.49
 ---- batch: 090 ----
mean loss: 405.24
train mean loss: 401.54
epoch train time: 0:00:02.329644
elapsed time: 0:03:00.015110
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-25 21:35:50.763569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 409.97
 ---- batch: 020 ----
mean loss: 393.89
 ---- batch: 030 ----
mean loss: 388.07
 ---- batch: 040 ----
mean loss: 395.23
 ---- batch: 050 ----
mean loss: 423.51
 ---- batch: 060 ----
mean loss: 395.64
 ---- batch: 070 ----
mean loss: 383.34
 ---- batch: 080 ----
mean loss: 386.32
 ---- batch: 090 ----
mean loss: 386.15
train mean loss: 394.91
epoch train time: 0:00:02.336555
elapsed time: 0:03:02.351862
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-25 21:35:53.100341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.20
 ---- batch: 020 ----
mean loss: 403.78
 ---- batch: 030 ----
mean loss: 405.32
 ---- batch: 040 ----
mean loss: 403.02
 ---- batch: 050 ----
mean loss: 397.81
 ---- batch: 060 ----
mean loss: 391.99
 ---- batch: 070 ----
mean loss: 392.88
 ---- batch: 080 ----
mean loss: 399.29
 ---- batch: 090 ----
mean loss: 397.46
train mean loss: 399.18
epoch train time: 0:00:02.328969
elapsed time: 0:03:04.681100
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-25 21:35:55.429556
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.72
 ---- batch: 020 ----
mean loss: 394.30
 ---- batch: 030 ----
mean loss: 383.14
 ---- batch: 040 ----
mean loss: 398.05
 ---- batch: 050 ----
mean loss: 397.04
 ---- batch: 060 ----
mean loss: 390.55
 ---- batch: 070 ----
mean loss: 378.57
 ---- batch: 080 ----
mean loss: 391.57
 ---- batch: 090 ----
mean loss: 383.47
train mean loss: 388.66
epoch train time: 0:00:02.334099
elapsed time: 0:03:07.015392
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-25 21:35:57.763851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.25
 ---- batch: 020 ----
mean loss: 403.61
 ---- batch: 030 ----
mean loss: 410.04
 ---- batch: 040 ----
mean loss: 398.21
 ---- batch: 050 ----
mean loss: 403.84
 ---- batch: 060 ----
mean loss: 402.21
 ---- batch: 070 ----
mean loss: 389.90
 ---- batch: 080 ----
mean loss: 381.87
 ---- batch: 090 ----
mean loss: 380.04
train mean loss: 394.16
epoch train time: 0:00:02.339585
elapsed time: 0:03:09.355158
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-25 21:36:00.103625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.97
 ---- batch: 020 ----
mean loss: 391.08
 ---- batch: 030 ----
mean loss: 390.50
 ---- batch: 040 ----
mean loss: 388.55
 ---- batch: 050 ----
mean loss: 390.04
 ---- batch: 060 ----
mean loss: 390.39
 ---- batch: 070 ----
mean loss: 395.10
 ---- batch: 080 ----
mean loss: 412.28
 ---- batch: 090 ----
mean loss: 393.98
train mean loss: 393.93
epoch train time: 0:00:02.330373
elapsed time: 0:03:11.685770
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-25 21:36:02.434226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.03
 ---- batch: 020 ----
mean loss: 393.57
 ---- batch: 030 ----
mean loss: 382.41
 ---- batch: 040 ----
mean loss: 398.34
 ---- batch: 050 ----
mean loss: 401.42
 ---- batch: 060 ----
mean loss: 397.76
 ---- batch: 070 ----
mean loss: 406.78
 ---- batch: 080 ----
mean loss: 384.75
 ---- batch: 090 ----
mean loss: 394.04
train mean loss: 396.00
epoch train time: 0:00:02.335355
elapsed time: 0:03:14.021352
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-25 21:36:04.769812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.33
 ---- batch: 020 ----
mean loss: 384.35
 ---- batch: 030 ----
mean loss: 391.63
 ---- batch: 040 ----
mean loss: 397.18
 ---- batch: 050 ----
mean loss: 402.01
 ---- batch: 060 ----
mean loss: 392.52
 ---- batch: 070 ----
mean loss: 383.12
 ---- batch: 080 ----
mean loss: 400.63
 ---- batch: 090 ----
mean loss: 403.24
train mean loss: 394.67
epoch train time: 0:00:02.338346
elapsed time: 0:03:16.359915
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-25 21:36:07.108386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.83
 ---- batch: 020 ----
mean loss: 384.22
 ---- batch: 030 ----
mean loss: 405.18
 ---- batch: 040 ----
mean loss: 402.18
 ---- batch: 050 ----
mean loss: 405.61
 ---- batch: 060 ----
mean loss: 388.67
 ---- batch: 070 ----
mean loss: 381.18
 ---- batch: 080 ----
mean loss: 390.67
 ---- batch: 090 ----
mean loss: 390.45
train mean loss: 394.13
epoch train time: 0:00:02.325335
elapsed time: 0:03:18.685445
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-25 21:36:09.433925
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.33
 ---- batch: 020 ----
mean loss: 396.26
 ---- batch: 030 ----
mean loss: 416.39
 ---- batch: 040 ----
mean loss: 409.18
 ---- batch: 050 ----
mean loss: 393.28
 ---- batch: 060 ----
mean loss: 397.45
 ---- batch: 070 ----
mean loss: 381.57
 ---- batch: 080 ----
mean loss: 391.49
 ---- batch: 090 ----
mean loss: 401.44
train mean loss: 397.88
epoch train time: 0:00:02.333892
elapsed time: 0:03:21.019581
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-25 21:36:11.768050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.62
 ---- batch: 020 ----
mean loss: 380.12
 ---- batch: 030 ----
mean loss: 396.12
 ---- batch: 040 ----
mean loss: 381.71
 ---- batch: 050 ----
mean loss: 408.97
 ---- batch: 060 ----
mean loss: 397.47
 ---- batch: 070 ----
mean loss: 389.43
 ---- batch: 080 ----
mean loss: 397.63
 ---- batch: 090 ----
mean loss: 399.63
train mean loss: 393.86
epoch train time: 0:00:02.336438
elapsed time: 0:03:23.356207
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-25 21:36:14.104678
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.26
 ---- batch: 020 ----
mean loss: 390.28
 ---- batch: 030 ----
mean loss: 410.59
 ---- batch: 040 ----
mean loss: 416.15
 ---- batch: 050 ----
mean loss: 398.34
 ---- batch: 060 ----
mean loss: 397.34
 ---- batch: 070 ----
mean loss: 396.63
 ---- batch: 080 ----
mean loss: 381.74
 ---- batch: 090 ----
mean loss: 396.13
train mean loss: 396.92
epoch train time: 0:00:02.327758
elapsed time: 0:03:25.684183
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-25 21:36:16.432666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.92
 ---- batch: 020 ----
mean loss: 383.15
 ---- batch: 030 ----
mean loss: 393.14
 ---- batch: 040 ----
mean loss: 380.24
 ---- batch: 050 ----
mean loss: 385.91
 ---- batch: 060 ----
mean loss: 391.89
 ---- batch: 070 ----
mean loss: 397.32
 ---- batch: 080 ----
mean loss: 415.59
 ---- batch: 090 ----
mean loss: 409.77
train mean loss: 393.79
epoch train time: 0:00:02.329300
elapsed time: 0:03:28.013686
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-25 21:36:18.762145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.66
 ---- batch: 020 ----
mean loss: 391.13
 ---- batch: 030 ----
mean loss: 394.48
 ---- batch: 040 ----
mean loss: 391.20
 ---- batch: 050 ----
mean loss: 384.24
 ---- batch: 060 ----
mean loss: 382.76
 ---- batch: 070 ----
mean loss: 400.04
 ---- batch: 080 ----
mean loss: 396.69
 ---- batch: 090 ----
mean loss: 396.38
train mean loss: 391.16
epoch train time: 0:00:02.335078
elapsed time: 0:03:30.348944
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-25 21:36:21.097402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.28
 ---- batch: 020 ----
mean loss: 390.84
 ---- batch: 030 ----
mean loss: 388.88
 ---- batch: 040 ----
mean loss: 404.38
 ---- batch: 050 ----
mean loss: 423.86
 ---- batch: 060 ----
mean loss: 405.71
 ---- batch: 070 ----
mean loss: 400.41
 ---- batch: 080 ----
mean loss: 415.62
 ---- batch: 090 ----
mean loss: 401.56
train mean loss: 405.26
epoch train time: 0:00:02.330306
elapsed time: 0:03:32.679425
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-25 21:36:23.427881
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.56
 ---- batch: 020 ----
mean loss: 385.20
 ---- batch: 030 ----
mean loss: 387.97
 ---- batch: 040 ----
mean loss: 402.16
 ---- batch: 050 ----
mean loss: 412.27
 ---- batch: 060 ----
mean loss: 399.59
 ---- batch: 070 ----
mean loss: 391.74
 ---- batch: 080 ----
mean loss: 396.86
 ---- batch: 090 ----
mean loss: 397.40
train mean loss: 396.64
epoch train time: 0:00:02.335869
elapsed time: 0:03:35.015477
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-25 21:36:25.763939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.23
 ---- batch: 020 ----
mean loss: 398.67
 ---- batch: 030 ----
mean loss: 386.85
 ---- batch: 040 ----
mean loss: 412.01
 ---- batch: 050 ----
mean loss: 395.75
 ---- batch: 060 ----
mean loss: 387.35
 ---- batch: 070 ----
mean loss: 406.17
 ---- batch: 080 ----
mean loss: 390.93
 ---- batch: 090 ----
mean loss: 384.00
train mean loss: 394.28
epoch train time: 0:00:02.331360
elapsed time: 0:03:37.347026
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-25 21:36:28.095504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.56
 ---- batch: 020 ----
mean loss: 383.44
 ---- batch: 030 ----
mean loss: 385.13
 ---- batch: 040 ----
mean loss: 395.30
 ---- batch: 050 ----
mean loss: 374.19
 ---- batch: 060 ----
mean loss: 394.86
 ---- batch: 070 ----
mean loss: 390.80
 ---- batch: 080 ----
mean loss: 409.55
 ---- batch: 090 ----
mean loss: 415.92
train mean loss: 393.96
epoch train time: 0:00:02.332700
elapsed time: 0:03:39.679925
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-25 21:36:30.428393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.15
 ---- batch: 020 ----
mean loss: 382.40
 ---- batch: 030 ----
mean loss: 392.78
 ---- batch: 040 ----
mean loss: 389.64
 ---- batch: 050 ----
mean loss: 396.42
 ---- batch: 060 ----
mean loss: 391.66
 ---- batch: 070 ----
mean loss: 395.16
 ---- batch: 080 ----
mean loss: 398.09
 ---- batch: 090 ----
mean loss: 398.38
train mean loss: 393.92
epoch train time: 0:00:02.326704
elapsed time: 0:03:42.006865
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-25 21:36:32.755326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.86
 ---- batch: 020 ----
mean loss: 391.55
 ---- batch: 030 ----
mean loss: 395.27
 ---- batch: 040 ----
mean loss: 380.72
 ---- batch: 050 ----
mean loss: 398.00
 ---- batch: 060 ----
mean loss: 384.36
 ---- batch: 070 ----
mean loss: 399.13
 ---- batch: 080 ----
mean loss: 391.01
 ---- batch: 090 ----
mean loss: 398.91
train mean loss: 391.84
epoch train time: 0:00:02.330617
elapsed time: 0:03:44.337670
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-25 21:36:35.086129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.97
 ---- batch: 020 ----
mean loss: 389.50
 ---- batch: 030 ----
mean loss: 373.46
 ---- batch: 040 ----
mean loss: 391.47
 ---- batch: 050 ----
mean loss: 387.43
 ---- batch: 060 ----
mean loss: 391.37
 ---- batch: 070 ----
mean loss: 384.65
 ---- batch: 080 ----
mean loss: 391.50
 ---- batch: 090 ----
mean loss: 388.94
train mean loss: 389.18
epoch train time: 0:00:02.323020
elapsed time: 0:03:46.660889
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-25 21:36:37.409341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.58
 ---- batch: 020 ----
mean loss: 383.50
 ---- batch: 030 ----
mean loss: 377.07
 ---- batch: 040 ----
mean loss: 386.13
 ---- batch: 050 ----
mean loss: 388.64
 ---- batch: 060 ----
mean loss: 402.01
 ---- batch: 070 ----
mean loss: 407.03
 ---- batch: 080 ----
mean loss: 393.75
 ---- batch: 090 ----
mean loss: 389.00
train mean loss: 391.50
epoch train time: 0:00:02.324380
elapsed time: 0:03:48.985450
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-25 21:36:39.733918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.48
 ---- batch: 020 ----
mean loss: 382.97
 ---- batch: 030 ----
mean loss: 397.05
 ---- batch: 040 ----
mean loss: 394.63
 ---- batch: 050 ----
mean loss: 398.06
 ---- batch: 060 ----
mean loss: 408.45
 ---- batch: 070 ----
mean loss: 387.49
 ---- batch: 080 ----
mean loss: 378.18
 ---- batch: 090 ----
mean loss: 394.16
train mean loss: 392.63
epoch train time: 0:00:02.323777
elapsed time: 0:03:51.309454
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-25 21:36:42.057911
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.22
 ---- batch: 020 ----
mean loss: 403.83
 ---- batch: 030 ----
mean loss: 394.05
 ---- batch: 040 ----
mean loss: 392.03
 ---- batch: 050 ----
mean loss: 391.57
 ---- batch: 060 ----
mean loss: 377.82
 ---- batch: 070 ----
mean loss: 385.92
 ---- batch: 080 ----
mean loss: 379.94
 ---- batch: 090 ----
mean loss: 398.09
train mean loss: 392.57
epoch train time: 0:00:02.332560
elapsed time: 0:03:53.642220
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-25 21:36:44.390683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.18
 ---- batch: 020 ----
mean loss: 375.47
 ---- batch: 030 ----
mean loss: 386.70
 ---- batch: 040 ----
mean loss: 384.70
 ---- batch: 050 ----
mean loss: 394.71
 ---- batch: 060 ----
mean loss: 403.31
 ---- batch: 070 ----
mean loss: 387.03
 ---- batch: 080 ----
mean loss: 396.41
 ---- batch: 090 ----
mean loss: 381.71
train mean loss: 390.20
epoch train time: 0:00:02.333476
elapsed time: 0:03:55.975890
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-25 21:36:46.724400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.67
 ---- batch: 020 ----
mean loss: 389.68
 ---- batch: 030 ----
mean loss: 398.49
 ---- batch: 040 ----
mean loss: 393.65
 ---- batch: 050 ----
mean loss: 371.63
 ---- batch: 060 ----
mean loss: 398.55
 ---- batch: 070 ----
mean loss: 387.32
 ---- batch: 080 ----
mean loss: 400.13
 ---- batch: 090 ----
mean loss: 377.69
train mean loss: 390.58
epoch train time: 0:00:02.332253
elapsed time: 0:03:58.308374
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-25 21:36:49.056847
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.12
 ---- batch: 020 ----
mean loss: 400.45
 ---- batch: 030 ----
mean loss: 396.17
 ---- batch: 040 ----
mean loss: 391.65
 ---- batch: 050 ----
mean loss: 363.13
 ---- batch: 060 ----
mean loss: 391.31
 ---- batch: 070 ----
mean loss: 385.11
 ---- batch: 080 ----
mean loss: 380.74
 ---- batch: 090 ----
mean loss: 381.95
train mean loss: 388.87
epoch train time: 0:00:02.332180
elapsed time: 0:04:00.640740
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-25 21:36:51.389196
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.53
 ---- batch: 020 ----
mean loss: 408.68
 ---- batch: 030 ----
mean loss: 391.74
 ---- batch: 040 ----
mean loss: 380.80
 ---- batch: 050 ----
mean loss: 379.92
 ---- batch: 060 ----
mean loss: 388.81
 ---- batch: 070 ----
mean loss: 396.78
 ---- batch: 080 ----
mean loss: 382.74
 ---- batch: 090 ----
mean loss: 392.69
train mean loss: 393.35
epoch train time: 0:00:02.329902
elapsed time: 0:04:02.970823
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-25 21:36:53.719281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.24
 ---- batch: 020 ----
mean loss: 400.62
 ---- batch: 030 ----
mean loss: 388.07
 ---- batch: 040 ----
mean loss: 391.83
 ---- batch: 050 ----
mean loss: 397.22
 ---- batch: 060 ----
mean loss: 406.49
 ---- batch: 070 ----
mean loss: 398.31
 ---- batch: 080 ----
mean loss: 403.07
 ---- batch: 090 ----
mean loss: 387.65
train mean loss: 395.65
epoch train time: 0:00:02.323502
elapsed time: 0:04:05.294519
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-25 21:36:56.042987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.28
 ---- batch: 020 ----
mean loss: 392.75
 ---- batch: 030 ----
mean loss: 399.98
 ---- batch: 040 ----
mean loss: 404.24
 ---- batch: 050 ----
mean loss: 401.80
 ---- batch: 060 ----
mean loss: 408.59
 ---- batch: 070 ----
mean loss: 385.05
 ---- batch: 080 ----
mean loss: 379.62
 ---- batch: 090 ----
mean loss: 379.61
train mean loss: 391.61
epoch train time: 0:00:02.330834
elapsed time: 0:04:07.625550
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-25 21:36:58.374024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.30
 ---- batch: 020 ----
mean loss: 390.95
 ---- batch: 030 ----
mean loss: 413.63
 ---- batch: 040 ----
mean loss: 416.37
 ---- batch: 050 ----
mean loss: 401.35
 ---- batch: 060 ----
mean loss: 408.46
 ---- batch: 070 ----
mean loss: 396.60
 ---- batch: 080 ----
mean loss: 395.52
 ---- batch: 090 ----
mean loss: 377.39
train mean loss: 398.94
epoch train time: 0:00:02.330715
elapsed time: 0:04:09.956459
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-25 21:37:00.704941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.47
 ---- batch: 020 ----
mean loss: 382.29
 ---- batch: 030 ----
mean loss: 385.84
 ---- batch: 040 ----
mean loss: 393.69
 ---- batch: 050 ----
mean loss: 385.39
 ---- batch: 060 ----
mean loss: 399.38
 ---- batch: 070 ----
mean loss: 393.18
 ---- batch: 080 ----
mean loss: 392.80
 ---- batch: 090 ----
mean loss: 369.63
train mean loss: 388.82
epoch train time: 0:00:02.328189
elapsed time: 0:04:12.284859
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-25 21:37:03.033335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.22
 ---- batch: 020 ----
mean loss: 404.29
 ---- batch: 030 ----
mean loss: 395.71
 ---- batch: 040 ----
mean loss: 393.04
 ---- batch: 050 ----
mean loss: 377.88
 ---- batch: 060 ----
mean loss: 399.22
 ---- batch: 070 ----
mean loss: 392.08
 ---- batch: 080 ----
mean loss: 388.20
 ---- batch: 090 ----
mean loss: 390.29
train mean loss: 392.09
epoch train time: 0:00:02.330249
elapsed time: 0:04:14.615299
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-25 21:37:05.363757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.59
 ---- batch: 020 ----
mean loss: 382.11
 ---- batch: 030 ----
mean loss: 386.79
 ---- batch: 040 ----
mean loss: 374.58
 ---- batch: 050 ----
mean loss: 383.06
 ---- batch: 060 ----
mean loss: 387.67
 ---- batch: 070 ----
mean loss: 383.29
 ---- batch: 080 ----
mean loss: 385.78
 ---- batch: 090 ----
mean loss: 401.18
train mean loss: 385.02
epoch train time: 0:00:02.324687
elapsed time: 0:04:16.940161
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-25 21:37:07.688618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.81
 ---- batch: 020 ----
mean loss: 381.57
 ---- batch: 030 ----
mean loss: 386.69
 ---- batch: 040 ----
mean loss: 388.17
 ---- batch: 050 ----
mean loss: 373.86
 ---- batch: 060 ----
mean loss: 380.84
 ---- batch: 070 ----
mean loss: 398.62
 ---- batch: 080 ----
mean loss: 389.12
 ---- batch: 090 ----
mean loss: 386.38
train mean loss: 386.35
epoch train time: 0:00:02.320606
elapsed time: 0:04:19.260952
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-25 21:37:10.009410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.70
 ---- batch: 020 ----
mean loss: 388.32
 ---- batch: 030 ----
mean loss: 381.36
 ---- batch: 040 ----
mean loss: 384.03
 ---- batch: 050 ----
mean loss: 382.47
 ---- batch: 060 ----
mean loss: 387.19
 ---- batch: 070 ----
mean loss: 406.38
 ---- batch: 080 ----
mean loss: 393.72
 ---- batch: 090 ----
mean loss: 397.88
train mean loss: 390.81
epoch train time: 0:00:02.327738
elapsed time: 0:04:21.588871
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-25 21:37:12.337328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.81
 ---- batch: 020 ----
mean loss: 381.11
 ---- batch: 030 ----
mean loss: 375.22
 ---- batch: 040 ----
mean loss: 372.18
 ---- batch: 050 ----
mean loss: 380.05
 ---- batch: 060 ----
mean loss: 392.82
 ---- batch: 070 ----
mean loss: 387.78
 ---- batch: 080 ----
mean loss: 390.75
 ---- batch: 090 ----
mean loss: 411.18
train mean loss: 388.29
epoch train time: 0:00:02.333167
elapsed time: 0:04:23.922219
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-25 21:37:14.670676
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.20
 ---- batch: 020 ----
mean loss: 383.85
 ---- batch: 030 ----
mean loss: 382.38
 ---- batch: 040 ----
mean loss: 401.82
 ---- batch: 050 ----
mean loss: 407.62
 ---- batch: 060 ----
mean loss: 385.35
 ---- batch: 070 ----
mean loss: 409.84
 ---- batch: 080 ----
mean loss: 382.00
 ---- batch: 090 ----
mean loss: 384.32
train mean loss: 392.36
epoch train time: 0:00:02.320447
elapsed time: 0:04:26.242865
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-25 21:37:16.991326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.03
 ---- batch: 020 ----
mean loss: 377.62
 ---- batch: 030 ----
mean loss: 382.36
 ---- batch: 040 ----
mean loss: 380.26
 ---- batch: 050 ----
mean loss: 414.09
 ---- batch: 060 ----
mean loss: 381.16
 ---- batch: 070 ----
mean loss: 382.33
 ---- batch: 080 ----
mean loss: 415.58
 ---- batch: 090 ----
mean loss: 410.73
train mean loss: 394.81
epoch train time: 0:00:02.335423
elapsed time: 0:04:28.578471
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-25 21:37:19.326927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.30
 ---- batch: 020 ----
mean loss: 384.45
 ---- batch: 030 ----
mean loss: 383.88
 ---- batch: 040 ----
mean loss: 400.34
 ---- batch: 050 ----
mean loss: 386.62
 ---- batch: 060 ----
mean loss: 384.17
 ---- batch: 070 ----
mean loss: 371.81
 ---- batch: 080 ----
mean loss: 391.90
 ---- batch: 090 ----
mean loss: 382.54
train mean loss: 387.96
epoch train time: 0:00:02.330955
elapsed time: 0:04:30.909627
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-25 21:37:21.658117
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.92
 ---- batch: 020 ----
mean loss: 390.93
 ---- batch: 030 ----
mean loss: 398.76
 ---- batch: 040 ----
mean loss: 398.41
 ---- batch: 050 ----
mean loss: 390.58
 ---- batch: 060 ----
mean loss: 393.43
 ---- batch: 070 ----
mean loss: 380.81
 ---- batch: 080 ----
mean loss: 380.82
 ---- batch: 090 ----
mean loss: 394.17
train mean loss: 391.70
epoch train time: 0:00:02.325955
elapsed time: 0:04:33.235797
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-25 21:37:23.984254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.74
 ---- batch: 020 ----
mean loss: 372.22
 ---- batch: 030 ----
mean loss: 382.74
 ---- batch: 040 ----
mean loss: 395.05
 ---- batch: 050 ----
mean loss: 390.14
 ---- batch: 060 ----
mean loss: 380.95
 ---- batch: 070 ----
mean loss: 388.37
 ---- batch: 080 ----
mean loss: 370.96
 ---- batch: 090 ----
mean loss: 397.22
train mean loss: 386.36
epoch train time: 0:00:02.338261
elapsed time: 0:04:35.574252
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-25 21:37:26.322711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.58
 ---- batch: 020 ----
mean loss: 392.97
 ---- batch: 030 ----
mean loss: 392.84
 ---- batch: 040 ----
mean loss: 387.70
 ---- batch: 050 ----
mean loss: 393.98
 ---- batch: 060 ----
mean loss: 388.71
 ---- batch: 070 ----
mean loss: 380.56
 ---- batch: 080 ----
mean loss: 405.66
 ---- batch: 090 ----
mean loss: 412.90
train mean loss: 393.53
epoch train time: 0:00:02.337519
elapsed time: 0:04:37.911958
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-25 21:37:28.660416
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.01
 ---- batch: 020 ----
mean loss: 388.05
 ---- batch: 030 ----
mean loss: 403.70
 ---- batch: 040 ----
mean loss: 382.53
 ---- batch: 050 ----
mean loss: 381.49
 ---- batch: 060 ----
mean loss: 392.87
 ---- batch: 070 ----
mean loss: 385.78
 ---- batch: 080 ----
mean loss: 387.93
 ---- batch: 090 ----
mean loss: 397.50
train mean loss: 389.29
epoch train time: 0:00:02.323697
elapsed time: 0:04:40.235838
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-25 21:37:30.984297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 400.32
 ---- batch: 020 ----
mean loss: 400.89
 ---- batch: 030 ----
mean loss: 390.24
 ---- batch: 040 ----
mean loss: 381.02
 ---- batch: 050 ----
mean loss: 393.01
 ---- batch: 060 ----
mean loss: 390.41
 ---- batch: 070 ----
mean loss: 379.98
 ---- batch: 080 ----
mean loss: 396.61
 ---- batch: 090 ----
mean loss: 381.51
train mean loss: 389.10
epoch train time: 0:00:02.331811
elapsed time: 0:04:42.567845
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-25 21:37:33.316299
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.24
 ---- batch: 020 ----
mean loss: 386.18
 ---- batch: 030 ----
mean loss: 385.69
 ---- batch: 040 ----
mean loss: 389.73
 ---- batch: 050 ----
mean loss: 381.34
 ---- batch: 060 ----
mean loss: 381.23
 ---- batch: 070 ----
mean loss: 393.82
 ---- batch: 080 ----
mean loss: 393.14
 ---- batch: 090 ----
mean loss: 370.69
train mean loss: 383.81
epoch train time: 0:00:02.335751
elapsed time: 0:04:44.903778
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-25 21:37:35.652243
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.98
 ---- batch: 020 ----
mean loss: 391.12
 ---- batch: 030 ----
mean loss: 385.05
 ---- batch: 040 ----
mean loss: 383.68
 ---- batch: 050 ----
mean loss: 393.61
 ---- batch: 060 ----
mean loss: 389.42
 ---- batch: 070 ----
mean loss: 385.40
 ---- batch: 080 ----
mean loss: 377.58
 ---- batch: 090 ----
mean loss: 384.34
train mean loss: 386.22
epoch train time: 0:00:02.326510
elapsed time: 0:04:47.230542
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-25 21:37:37.979035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.57
 ---- batch: 020 ----
mean loss: 377.39
 ---- batch: 030 ----
mean loss: 390.70
 ---- batch: 040 ----
mean loss: 396.33
 ---- batch: 050 ----
mean loss: 401.05
 ---- batch: 060 ----
mean loss: 399.75
 ---- batch: 070 ----
mean loss: 391.76
 ---- batch: 080 ----
mean loss: 391.84
 ---- batch: 090 ----
mean loss: 389.69
train mean loss: 394.51
epoch train time: 0:00:02.333239
elapsed time: 0:04:49.564006
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-25 21:37:40.312463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.56
 ---- batch: 020 ----
mean loss: 395.59
 ---- batch: 030 ----
mean loss: 395.24
 ---- batch: 040 ----
mean loss: 385.11
 ---- batch: 050 ----
mean loss: 392.35
 ---- batch: 060 ----
mean loss: 394.66
 ---- batch: 070 ----
mean loss: 391.52
 ---- batch: 080 ----
mean loss: 390.48
 ---- batch: 090 ----
mean loss: 395.47
train mean loss: 393.17
epoch train time: 0:00:02.328336
elapsed time: 0:04:51.892539
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-25 21:37:42.641015
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.96
 ---- batch: 020 ----
mean loss: 384.86
 ---- batch: 030 ----
mean loss: 386.77
 ---- batch: 040 ----
mean loss: 416.57
 ---- batch: 050 ----
mean loss: 408.34
 ---- batch: 060 ----
mean loss: 403.06
 ---- batch: 070 ----
mean loss: 389.49
 ---- batch: 080 ----
mean loss: 386.91
 ---- batch: 090 ----
mean loss: 383.81
train mean loss: 392.11
epoch train time: 0:00:02.332320
elapsed time: 0:04:54.225073
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-25 21:37:44.973531
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.85
 ---- batch: 020 ----
mean loss: 390.62
 ---- batch: 030 ----
mean loss: 393.38
 ---- batch: 040 ----
mean loss: 387.91
 ---- batch: 050 ----
mean loss: 397.88
 ---- batch: 060 ----
mean loss: 414.27
 ---- batch: 070 ----
mean loss: 399.93
 ---- batch: 080 ----
mean loss: 403.43
 ---- batch: 090 ----
mean loss: 395.32
train mean loss: 394.78
epoch train time: 0:00:02.329416
elapsed time: 0:04:56.554674
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-25 21:37:47.303162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.24
 ---- batch: 020 ----
mean loss: 382.60
 ---- batch: 030 ----
mean loss: 389.19
 ---- batch: 040 ----
mean loss: 397.52
 ---- batch: 050 ----
mean loss: 393.57
 ---- batch: 060 ----
mean loss: 397.47
 ---- batch: 070 ----
mean loss: 381.88
 ---- batch: 080 ----
mean loss: 394.04
 ---- batch: 090 ----
mean loss: 403.46
train mean loss: 391.88
epoch train time: 0:00:02.343185
elapsed time: 0:04:58.898083
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-25 21:37:49.646547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.47
 ---- batch: 020 ----
mean loss: 389.17
 ---- batch: 030 ----
mean loss: 382.66
 ---- batch: 040 ----
mean loss: 392.14
 ---- batch: 050 ----
mean loss: 369.10
 ---- batch: 060 ----
mean loss: 384.36
 ---- batch: 070 ----
mean loss: 390.44
 ---- batch: 080 ----
mean loss: 369.04
 ---- batch: 090 ----
mean loss: 378.32
train mean loss: 382.32
epoch train time: 0:00:02.336154
elapsed time: 0:05:01.234431
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-25 21:37:51.982890
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 393.76
 ---- batch: 020 ----
mean loss: 379.10
 ---- batch: 030 ----
mean loss: 379.18
 ---- batch: 040 ----
mean loss: 375.80
 ---- batch: 050 ----
mean loss: 396.25
 ---- batch: 060 ----
mean loss: 379.73
 ---- batch: 070 ----
mean loss: 398.62
 ---- batch: 080 ----
mean loss: 372.23
 ---- batch: 090 ----
mean loss: 375.95
train mean loss: 386.26
epoch train time: 0:00:02.331976
elapsed time: 0:05:03.566612
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-25 21:37:54.315069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.80
 ---- batch: 020 ----
mean loss: 383.54
 ---- batch: 030 ----
mean loss: 381.63
 ---- batch: 040 ----
mean loss: 389.35
 ---- batch: 050 ----
mean loss: 396.45
 ---- batch: 060 ----
mean loss: 413.84
 ---- batch: 070 ----
mean loss: 415.27
 ---- batch: 080 ----
mean loss: 403.32
 ---- batch: 090 ----
mean loss: 402.46
train mean loss: 394.57
epoch train time: 0:00:02.332829
elapsed time: 0:05:05.899620
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-25 21:37:56.648097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.30
 ---- batch: 020 ----
mean loss: 382.22
 ---- batch: 030 ----
mean loss: 383.07
 ---- batch: 040 ----
mean loss: 378.20
 ---- batch: 050 ----
mean loss: 391.70
 ---- batch: 060 ----
mean loss: 381.51
 ---- batch: 070 ----
mean loss: 383.89
 ---- batch: 080 ----
mean loss: 397.47
 ---- batch: 090 ----
mean loss: 388.16
train mean loss: 388.29
epoch train time: 0:00:02.329189
elapsed time: 0:05:08.229036
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-25 21:37:58.977507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.56
 ---- batch: 020 ----
mean loss: 413.98
 ---- batch: 030 ----
mean loss: 400.14
 ---- batch: 040 ----
mean loss: 391.75
 ---- batch: 050 ----
mean loss: 394.56
 ---- batch: 060 ----
mean loss: 376.90
 ---- batch: 070 ----
mean loss: 390.71
 ---- batch: 080 ----
mean loss: 387.23
 ---- batch: 090 ----
mean loss: 385.75
train mean loss: 395.74
epoch train time: 0:00:02.328738
elapsed time: 0:05:10.557967
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-25 21:38:01.306432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.92
 ---- batch: 020 ----
mean loss: 389.79
 ---- batch: 030 ----
mean loss: 383.22
 ---- batch: 040 ----
mean loss: 384.60
 ---- batch: 050 ----
mean loss: 383.89
 ---- batch: 060 ----
mean loss: 376.11
 ---- batch: 070 ----
mean loss: 389.84
 ---- batch: 080 ----
mean loss: 390.26
 ---- batch: 090 ----
mean loss: 376.97
train mean loss: 385.75
epoch train time: 0:00:02.330578
elapsed time: 0:05:12.888731
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-25 21:38:03.637189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.79
 ---- batch: 020 ----
mean loss: 400.79
 ---- batch: 030 ----
mean loss: 393.23
 ---- batch: 040 ----
mean loss: 394.94
 ---- batch: 050 ----
mean loss: 393.83
 ---- batch: 060 ----
mean loss: 398.36
 ---- batch: 070 ----
mean loss: 385.41
 ---- batch: 080 ----
mean loss: 385.20
 ---- batch: 090 ----
mean loss: 385.10
train mean loss: 391.27
epoch train time: 0:00:02.326278
elapsed time: 0:05:15.215183
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-25 21:38:05.963653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 396.29
 ---- batch: 020 ----
mean loss: 388.93
 ---- batch: 030 ----
mean loss: 379.92
 ---- batch: 040 ----
mean loss: 391.44
 ---- batch: 050 ----
mean loss: 420.16
 ---- batch: 060 ----
mean loss: 401.84
 ---- batch: 070 ----
mean loss: 391.92
 ---- batch: 080 ----
mean loss: 416.37
 ---- batch: 090 ----
mean loss: 405.52
train mean loss: 397.66
epoch train time: 0:00:02.320755
elapsed time: 0:05:17.536158
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-25 21:38:08.284642
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.41
 ---- batch: 020 ----
mean loss: 393.07
 ---- batch: 030 ----
mean loss: 379.49
 ---- batch: 040 ----
mean loss: 402.45
 ---- batch: 050 ----
mean loss: 385.52
 ---- batch: 060 ----
mean loss: 395.58
 ---- batch: 070 ----
mean loss: 383.44
 ---- batch: 080 ----
mean loss: 381.98
 ---- batch: 090 ----
mean loss: 388.43
train mean loss: 389.34
epoch train time: 0:00:02.330673
elapsed time: 0:05:19.867054
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-25 21:38:10.615516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.75
 ---- batch: 020 ----
mean loss: 370.58
 ---- batch: 030 ----
mean loss: 393.19
 ---- batch: 040 ----
mean loss: 390.80
 ---- batch: 050 ----
mean loss: 396.93
 ---- batch: 060 ----
mean loss: 385.49
 ---- batch: 070 ----
mean loss: 389.38
 ---- batch: 080 ----
mean loss: 380.66
 ---- batch: 090 ----
mean loss: 385.42
train mean loss: 386.70
epoch train time: 0:00:02.332559
elapsed time: 0:05:22.199786
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-25 21:38:12.948265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.28
 ---- batch: 020 ----
mean loss: 393.28
 ---- batch: 030 ----
mean loss: 384.81
 ---- batch: 040 ----
mean loss: 397.27
 ---- batch: 050 ----
mean loss: 382.66
 ---- batch: 060 ----
mean loss: 370.53
 ---- batch: 070 ----
mean loss: 385.19
 ---- batch: 080 ----
mean loss: 385.54
 ---- batch: 090 ----
mean loss: 404.44
train mean loss: 388.89
epoch train time: 0:00:02.333517
elapsed time: 0:05:24.533527
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-25 21:38:15.281986
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.77
 ---- batch: 020 ----
mean loss: 390.57
 ---- batch: 030 ----
mean loss: 373.93
 ---- batch: 040 ----
mean loss: 390.46
 ---- batch: 050 ----
mean loss: 396.99
 ---- batch: 060 ----
mean loss: 388.74
 ---- batch: 070 ----
mean loss: 379.19
 ---- batch: 080 ----
mean loss: 386.80
 ---- batch: 090 ----
mean loss: 394.36
train mean loss: 387.37
epoch train time: 0:00:02.326550
elapsed time: 0:05:26.860254
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-25 21:38:17.608714
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.54
 ---- batch: 020 ----
mean loss: 390.69
 ---- batch: 030 ----
mean loss: 378.20
 ---- batch: 040 ----
mean loss: 385.45
 ---- batch: 050 ----
mean loss: 384.88
 ---- batch: 060 ----
mean loss: 382.95
 ---- batch: 070 ----
mean loss: 383.57
 ---- batch: 080 ----
mean loss: 396.51
 ---- batch: 090 ----
mean loss: 396.27
train mean loss: 387.45
epoch train time: 0:00:02.329008
elapsed time: 0:05:29.189478
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-25 21:38:19.937934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.35
 ---- batch: 020 ----
mean loss: 401.65
 ---- batch: 030 ----
mean loss: 394.73
 ---- batch: 040 ----
mean loss: 385.85
 ---- batch: 050 ----
mean loss: 386.21
 ---- batch: 060 ----
mean loss: 381.40
 ---- batch: 070 ----
mean loss: 369.31
 ---- batch: 080 ----
mean loss: 380.29
 ---- batch: 090 ----
mean loss: 389.02
train mean loss: 388.48
epoch train time: 0:00:02.322705
elapsed time: 0:05:31.512363
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-25 21:38:22.260820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.18
 ---- batch: 020 ----
mean loss: 385.05
 ---- batch: 030 ----
mean loss: 383.88
 ---- batch: 040 ----
mean loss: 387.36
 ---- batch: 050 ----
mean loss: 384.71
 ---- batch: 060 ----
mean loss: 382.19
 ---- batch: 070 ----
mean loss: 389.08
 ---- batch: 080 ----
mean loss: 409.62
 ---- batch: 090 ----
mean loss: 402.88
train mean loss: 391.07
epoch train time: 0:00:02.326069
elapsed time: 0:05:33.838604
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-25 21:38:24.587066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 404.31
 ---- batch: 020 ----
mean loss: 405.74
 ---- batch: 030 ----
mean loss: 377.63
 ---- batch: 040 ----
mean loss: 382.39
 ---- batch: 050 ----
mean loss: 389.08
 ---- batch: 060 ----
mean loss: 374.00
 ---- batch: 070 ----
mean loss: 391.08
 ---- batch: 080 ----
mean loss: 387.52
 ---- batch: 090 ----
mean loss: 388.14
train mean loss: 387.72
epoch train time: 0:00:02.328731
elapsed time: 0:05:36.167525
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-25 21:38:26.915983
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.10
 ---- batch: 020 ----
mean loss: 384.87
 ---- batch: 030 ----
mean loss: 382.80
 ---- batch: 040 ----
mean loss: 390.27
 ---- batch: 050 ----
mean loss: 384.61
 ---- batch: 060 ----
mean loss: 405.61
 ---- batch: 070 ----
mean loss: 390.93
 ---- batch: 080 ----
mean loss: 382.90
 ---- batch: 090 ----
mean loss: 396.74
train mean loss: 388.20
epoch train time: 0:00:02.328590
elapsed time: 0:05:38.496310
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-25 21:38:29.244781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.77
 ---- batch: 020 ----
mean loss: 388.69
 ---- batch: 030 ----
mean loss: 400.92
 ---- batch: 040 ----
mean loss: 397.56
 ---- batch: 050 ----
mean loss: 405.39
 ---- batch: 060 ----
mean loss: 384.39
 ---- batch: 070 ----
mean loss: 383.03
 ---- batch: 080 ----
mean loss: 390.94
 ---- batch: 090 ----
mean loss: 383.09
train mean loss: 390.67
epoch train time: 0:00:02.323804
elapsed time: 0:05:40.820327
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-25 21:38:31.568802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.71
 ---- batch: 020 ----
mean loss: 384.12
 ---- batch: 030 ----
mean loss: 382.43
 ---- batch: 040 ----
mean loss: 385.64
 ---- batch: 050 ----
mean loss: 390.69
 ---- batch: 060 ----
mean loss: 383.01
 ---- batch: 070 ----
mean loss: 375.95
 ---- batch: 080 ----
mean loss: 384.32
 ---- batch: 090 ----
mean loss: 385.28
train mean loss: 383.07
epoch train time: 0:00:02.337477
elapsed time: 0:05:43.158004
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-25 21:38:33.906461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.83
 ---- batch: 020 ----
mean loss: 377.05
 ---- batch: 030 ----
mean loss: 381.35
 ---- batch: 040 ----
mean loss: 398.75
 ---- batch: 050 ----
mean loss: 394.66
 ---- batch: 060 ----
mean loss: 391.18
 ---- batch: 070 ----
mean loss: 393.71
 ---- batch: 080 ----
mean loss: 386.35
 ---- batch: 090 ----
mean loss: 404.79
train mean loss: 392.78
epoch train time: 0:00:02.340554
elapsed time: 0:05:45.498801
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-25 21:38:36.247296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.10
 ---- batch: 020 ----
mean loss: 396.29
 ---- batch: 030 ----
mean loss: 379.22
 ---- batch: 040 ----
mean loss: 382.46
 ---- batch: 050 ----
mean loss: 377.54
 ---- batch: 060 ----
mean loss: 392.08
 ---- batch: 070 ----
mean loss: 377.50
 ---- batch: 080 ----
mean loss: 379.49
 ---- batch: 090 ----
mean loss: 382.80
train mean loss: 386.14
epoch train time: 0:00:02.329620
elapsed time: 0:05:47.828659
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-25 21:38:38.577132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.05
 ---- batch: 020 ----
mean loss: 385.35
 ---- batch: 030 ----
mean loss: 381.13
 ---- batch: 040 ----
mean loss: 386.30
 ---- batch: 050 ----
mean loss: 397.06
 ---- batch: 060 ----
mean loss: 383.17
 ---- batch: 070 ----
mean loss: 384.71
 ---- batch: 080 ----
mean loss: 389.90
 ---- batch: 090 ----
mean loss: 400.50
train mean loss: 388.25
epoch train time: 0:00:02.322864
elapsed time: 0:05:50.151719
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-25 21:38:40.900191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.86
 ---- batch: 020 ----
mean loss: 375.10
 ---- batch: 030 ----
mean loss: 377.56
 ---- batch: 040 ----
mean loss: 381.67
 ---- batch: 050 ----
mean loss: 382.65
 ---- batch: 060 ----
mean loss: 375.01
 ---- batch: 070 ----
mean loss: 388.69
 ---- batch: 080 ----
mean loss: 387.22
 ---- batch: 090 ----
mean loss: 384.99
train mean loss: 383.01
epoch train time: 0:00:02.333966
elapsed time: 0:05:52.485877
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-25 21:38:43.234338
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.51
 ---- batch: 020 ----
mean loss: 386.10
 ---- batch: 030 ----
mean loss: 381.15
 ---- batch: 040 ----
mean loss: 388.18
 ---- batch: 050 ----
mean loss: 380.22
 ---- batch: 060 ----
mean loss: 390.62
 ---- batch: 070 ----
mean loss: 390.61
 ---- batch: 080 ----
mean loss: 373.25
 ---- batch: 090 ----
mean loss: 382.94
train mean loss: 384.86
epoch train time: 0:00:02.327158
elapsed time: 0:05:54.813236
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-25 21:38:45.561700
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.86
 ---- batch: 020 ----
mean loss: 381.45
 ---- batch: 030 ----
mean loss: 385.89
 ---- batch: 040 ----
mean loss: 366.41
 ---- batch: 050 ----
mean loss: 391.23
 ---- batch: 060 ----
mean loss: 392.82
 ---- batch: 070 ----
mean loss: 411.06
 ---- batch: 080 ----
mean loss: 399.48
 ---- batch: 090 ----
mean loss: 392.32
train mean loss: 387.71
epoch train time: 0:00:02.334152
elapsed time: 0:05:57.147573
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-25 21:38:47.896054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.04
 ---- batch: 020 ----
mean loss: 386.41
 ---- batch: 030 ----
mean loss: 390.54
 ---- batch: 040 ----
mean loss: 387.06
 ---- batch: 050 ----
mean loss: 385.38
 ---- batch: 060 ----
mean loss: 375.56
 ---- batch: 070 ----
mean loss: 407.11
 ---- batch: 080 ----
mean loss: 390.84
 ---- batch: 090 ----
mean loss: 387.86
train mean loss: 387.29
epoch train time: 0:00:02.328831
elapsed time: 0:05:59.476621
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-25 21:38:50.225078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.39
 ---- batch: 020 ----
mean loss: 384.13
 ---- batch: 030 ----
mean loss: 394.00
 ---- batch: 040 ----
mean loss: 376.64
 ---- batch: 050 ----
mean loss: 389.20
 ---- batch: 060 ----
mean loss: 382.41
 ---- batch: 070 ----
mean loss: 378.23
 ---- batch: 080 ----
mean loss: 383.03
 ---- batch: 090 ----
mean loss: 404.70
train mean loss: 386.67
epoch train time: 0:00:02.336234
elapsed time: 0:06:01.813044
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-25 21:38:52.561504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.25
 ---- batch: 020 ----
mean loss: 403.79
 ---- batch: 030 ----
mean loss: 396.17
 ---- batch: 040 ----
mean loss: 375.22
 ---- batch: 050 ----
mean loss: 393.02
 ---- batch: 060 ----
mean loss: 393.71
 ---- batch: 070 ----
mean loss: 384.24
 ---- batch: 080 ----
mean loss: 394.58
 ---- batch: 090 ----
mean loss: 388.74
train mean loss: 390.20
epoch train time: 0:00:02.329702
elapsed time: 0:06:04.142930
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-25 21:38:54.891387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.50
 ---- batch: 020 ----
mean loss: 385.76
 ---- batch: 030 ----
mean loss: 373.95
 ---- batch: 040 ----
mean loss: 383.17
 ---- batch: 050 ----
mean loss: 397.37
 ---- batch: 060 ----
mean loss: 387.63
 ---- batch: 070 ----
mean loss: 396.44
 ---- batch: 080 ----
mean loss: 388.50
 ---- batch: 090 ----
mean loss: 376.18
train mean loss: 388.01
epoch train time: 0:00:02.334787
elapsed time: 0:06:06.477898
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-25 21:38:57.226355
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.69
 ---- batch: 020 ----
mean loss: 393.34
 ---- batch: 030 ----
mean loss: 368.80
 ---- batch: 040 ----
mean loss: 384.78
 ---- batch: 050 ----
mean loss: 369.76
 ---- batch: 060 ----
mean loss: 376.83
 ---- batch: 070 ----
mean loss: 392.98
 ---- batch: 080 ----
mean loss: 401.46
 ---- batch: 090 ----
mean loss: 394.05
train mean loss: 386.15
epoch train time: 0:00:02.334729
elapsed time: 0:06:08.812808
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-25 21:38:59.561265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.70
 ---- batch: 020 ----
mean loss: 390.28
 ---- batch: 030 ----
mean loss: 393.26
 ---- batch: 040 ----
mean loss: 391.10
 ---- batch: 050 ----
mean loss: 392.79
 ---- batch: 060 ----
mean loss: 405.59
 ---- batch: 070 ----
mean loss: 399.77
 ---- batch: 080 ----
mean loss: 371.63
 ---- batch: 090 ----
mean loss: 394.99
train mean loss: 391.92
epoch train time: 0:00:02.327996
elapsed time: 0:06:11.141015
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-25 21:39:01.889495
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.60
 ---- batch: 020 ----
mean loss: 398.84
 ---- batch: 030 ----
mean loss: 373.16
 ---- batch: 040 ----
mean loss: 373.43
 ---- batch: 050 ----
mean loss: 386.44
 ---- batch: 060 ----
mean loss: 388.25
 ---- batch: 070 ----
mean loss: 375.77
 ---- batch: 080 ----
mean loss: 381.20
 ---- batch: 090 ----
mean loss: 382.44
train mean loss: 382.33
epoch train time: 0:00:02.335233
elapsed time: 0:06:13.476452
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-25 21:39:04.224921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 405.71
 ---- batch: 020 ----
mean loss: 407.05
 ---- batch: 030 ----
mean loss: 395.03
 ---- batch: 040 ----
mean loss: 383.64
 ---- batch: 050 ----
mean loss: 388.05
 ---- batch: 060 ----
mean loss: 376.04
 ---- batch: 070 ----
mean loss: 383.88
 ---- batch: 080 ----
mean loss: 406.95
 ---- batch: 090 ----
mean loss: 388.39
train mean loss: 391.54
epoch train time: 0:00:02.335984
elapsed time: 0:06:15.812648
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-25 21:39:06.561122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.04
 ---- batch: 020 ----
mean loss: 402.49
 ---- batch: 030 ----
mean loss: 388.94
 ---- batch: 040 ----
mean loss: 389.27
 ---- batch: 050 ----
mean loss: 378.42
 ---- batch: 060 ----
mean loss: 385.30
 ---- batch: 070 ----
mean loss: 364.68
 ---- batch: 080 ----
mean loss: 388.22
 ---- batch: 090 ----
mean loss: 380.88
train mean loss: 383.95
epoch train time: 0:00:02.330676
elapsed time: 0:06:18.143552
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-25 21:39:08.892013
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.78
 ---- batch: 020 ----
mean loss: 379.79
 ---- batch: 030 ----
mean loss: 379.57
 ---- batch: 040 ----
mean loss: 383.52
 ---- batch: 050 ----
mean loss: 373.20
 ---- batch: 060 ----
mean loss: 377.62
 ---- batch: 070 ----
mean loss: 398.88
 ---- batch: 080 ----
mean loss: 396.27
 ---- batch: 090 ----
mean loss: 401.96
train mean loss: 386.33
epoch train time: 0:00:02.330162
elapsed time: 0:06:20.473921
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-25 21:39:11.222365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.97
 ---- batch: 020 ----
mean loss: 401.30
 ---- batch: 030 ----
mean loss: 387.44
 ---- batch: 040 ----
mean loss: 402.49
 ---- batch: 050 ----
mean loss: 384.21
 ---- batch: 060 ----
mean loss: 389.45
 ---- batch: 070 ----
mean loss: 379.76
 ---- batch: 080 ----
mean loss: 388.87
 ---- batch: 090 ----
mean loss: 386.46
train mean loss: 390.27
epoch train time: 0:00:02.334480
elapsed time: 0:06:22.808561
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-25 21:39:13.557027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.37
 ---- batch: 020 ----
mean loss: 384.68
 ---- batch: 030 ----
mean loss: 377.16
 ---- batch: 040 ----
mean loss: 393.61
 ---- batch: 050 ----
mean loss: 382.67
 ---- batch: 060 ----
mean loss: 388.62
 ---- batch: 070 ----
mean loss: 376.33
 ---- batch: 080 ----
mean loss: 376.74
 ---- batch: 090 ----
mean loss: 377.69
train mean loss: 382.38
epoch train time: 0:00:02.332762
elapsed time: 0:06:25.141526
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-25 21:39:15.889987
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.57
 ---- batch: 020 ----
mean loss: 394.64
 ---- batch: 030 ----
mean loss: 384.86
 ---- batch: 040 ----
mean loss: 390.90
 ---- batch: 050 ----
mean loss: 380.50
 ---- batch: 060 ----
mean loss: 400.25
 ---- batch: 070 ----
mean loss: 384.55
 ---- batch: 080 ----
mean loss: 383.30
 ---- batch: 090 ----
mean loss: 389.78
train mean loss: 386.43
epoch train time: 0:00:02.327891
elapsed time: 0:06:27.469596
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-25 21:39:18.218054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.68
 ---- batch: 020 ----
mean loss: 392.53
 ---- batch: 030 ----
mean loss: 383.71
 ---- batch: 040 ----
mean loss: 378.79
 ---- batch: 050 ----
mean loss: 388.13
 ---- batch: 060 ----
mean loss: 386.31
 ---- batch: 070 ----
mean loss: 395.29
 ---- batch: 080 ----
mean loss: 384.76
 ---- batch: 090 ----
mean loss: 380.20
train mean loss: 385.57
epoch train time: 0:00:02.330731
elapsed time: 0:06:29.800508
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-25 21:39:20.548981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 406.57
 ---- batch: 020 ----
mean loss: 397.16
 ---- batch: 030 ----
mean loss: 391.34
 ---- batch: 040 ----
mean loss: 386.29
 ---- batch: 050 ----
mean loss: 383.27
 ---- batch: 060 ----
mean loss: 377.34
 ---- batch: 070 ----
mean loss: 367.11
 ---- batch: 080 ----
mean loss: 387.71
 ---- batch: 090 ----
mean loss: 378.92
train mean loss: 386.48
epoch train time: 0:00:02.330384
elapsed time: 0:06:32.131107
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-25 21:39:22.879595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.73
 ---- batch: 020 ----
mean loss: 368.48
 ---- batch: 030 ----
mean loss: 374.03
 ---- batch: 040 ----
mean loss: 384.11
 ---- batch: 050 ----
mean loss: 387.85
 ---- batch: 060 ----
mean loss: 394.59
 ---- batch: 070 ----
mean loss: 378.27
 ---- batch: 080 ----
mean loss: 397.96
 ---- batch: 090 ----
mean loss: 379.29
train mean loss: 383.16
epoch train time: 0:00:02.329271
elapsed time: 0:06:34.460623
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-25 21:39:25.209082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.48
 ---- batch: 020 ----
mean loss: 399.50
 ---- batch: 030 ----
mean loss: 385.26
 ---- batch: 040 ----
mean loss: 385.83
 ---- batch: 050 ----
mean loss: 392.95
 ---- batch: 060 ----
mean loss: 391.97
 ---- batch: 070 ----
mean loss: 391.14
 ---- batch: 080 ----
mean loss: 372.41
 ---- batch: 090 ----
mean loss: 380.64
train mean loss: 388.06
epoch train time: 0:00:02.322745
elapsed time: 0:06:36.783544
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-25 21:39:27.532018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 397.76
 ---- batch: 020 ----
mean loss: 390.20
 ---- batch: 030 ----
mean loss: 384.87
 ---- batch: 040 ----
mean loss: 400.16
 ---- batch: 050 ----
mean loss: 362.30
 ---- batch: 060 ----
mean loss: 375.74
 ---- batch: 070 ----
mean loss: 399.54
 ---- batch: 080 ----
mean loss: 423.49
 ---- batch: 090 ----
mean loss: 381.31
train mean loss: 389.65
epoch train time: 0:00:02.333720
elapsed time: 0:06:39.117465
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-25 21:39:29.865939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.18
 ---- batch: 020 ----
mean loss: 398.21
 ---- batch: 030 ----
mean loss: 374.76
 ---- batch: 040 ----
mean loss: 379.40
 ---- batch: 050 ----
mean loss: 382.08
 ---- batch: 060 ----
mean loss: 383.53
 ---- batch: 070 ----
mean loss: 382.69
 ---- batch: 080 ----
mean loss: 384.32
 ---- batch: 090 ----
mean loss: 385.27
train mean loss: 384.74
epoch train time: 0:00:02.329045
elapsed time: 0:06:41.446755
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-25 21:39:32.195232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.91
 ---- batch: 020 ----
mean loss: 401.74
 ---- batch: 030 ----
mean loss: 412.88
 ---- batch: 040 ----
mean loss: 387.29
 ---- batch: 050 ----
mean loss: 382.67
 ---- batch: 060 ----
mean loss: 387.41
 ---- batch: 070 ----
mean loss: 381.37
 ---- batch: 080 ----
mean loss: 394.70
 ---- batch: 090 ----
mean loss: 381.60
train mean loss: 391.67
epoch train time: 0:00:02.327298
elapsed time: 0:06:43.774255
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-25 21:39:34.522723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 408.31
 ---- batch: 020 ----
mean loss: 384.44
 ---- batch: 030 ----
mean loss: 381.54
 ---- batch: 040 ----
mean loss: 391.04
 ---- batch: 050 ----
mean loss: 393.66
 ---- batch: 060 ----
mean loss: 382.95
 ---- batch: 070 ----
mean loss: 378.96
 ---- batch: 080 ----
mean loss: 382.27
 ---- batch: 090 ----
mean loss: 381.78
train mean loss: 386.73
epoch train time: 0:00:02.330277
elapsed time: 0:06:46.104748
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-25 21:39:36.853205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.12
 ---- batch: 020 ----
mean loss: 382.54
 ---- batch: 030 ----
mean loss: 381.18
 ---- batch: 040 ----
mean loss: 396.55
 ---- batch: 050 ----
mean loss: 379.91
 ---- batch: 060 ----
mean loss: 374.88
 ---- batch: 070 ----
mean loss: 379.09
 ---- batch: 080 ----
mean loss: 375.77
 ---- batch: 090 ----
mean loss: 382.03
train mean loss: 381.27
epoch train time: 0:00:02.324461
elapsed time: 0:06:48.429388
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-25 21:39:39.177849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.79
 ---- batch: 020 ----
mean loss: 385.29
 ---- batch: 030 ----
mean loss: 393.70
 ---- batch: 040 ----
mean loss: 384.75
 ---- batch: 050 ----
mean loss: 373.28
 ---- batch: 060 ----
mean loss: 380.77
 ---- batch: 070 ----
mean loss: 393.02
 ---- batch: 080 ----
mean loss: 380.62
 ---- batch: 090 ----
mean loss: 374.34
train mean loss: 382.86
epoch train time: 0:00:02.330575
elapsed time: 0:06:50.760142
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-25 21:39:41.508617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.51
 ---- batch: 020 ----
mean loss: 380.96
 ---- batch: 030 ----
mean loss: 394.02
 ---- batch: 040 ----
mean loss: 389.86
 ---- batch: 050 ----
mean loss: 377.29
 ---- batch: 060 ----
mean loss: 398.84
 ---- batch: 070 ----
mean loss: 388.39
 ---- batch: 080 ----
mean loss: 382.34
 ---- batch: 090 ----
mean loss: 366.84
train mean loss: 384.55
epoch train time: 0:00:02.325194
elapsed time: 0:06:53.085544
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-25 21:39:43.834008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.30
 ---- batch: 020 ----
mean loss: 377.18
 ---- batch: 030 ----
mean loss: 388.04
 ---- batch: 040 ----
mean loss: 403.56
 ---- batch: 050 ----
mean loss: 396.99
 ---- batch: 060 ----
mean loss: 396.81
 ---- batch: 070 ----
mean loss: 390.83
 ---- batch: 080 ----
mean loss: 375.94
 ---- batch: 090 ----
mean loss: 379.85
train mean loss: 387.73
epoch train time: 0:00:02.331463
elapsed time: 0:06:55.417194
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-25 21:39:46.165687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.82
 ---- batch: 020 ----
mean loss: 374.86
 ---- batch: 030 ----
mean loss: 382.63
 ---- batch: 040 ----
mean loss: 368.78
 ---- batch: 050 ----
mean loss: 384.92
 ---- batch: 060 ----
mean loss: 386.26
 ---- batch: 070 ----
mean loss: 379.54
 ---- batch: 080 ----
mean loss: 388.70
 ---- batch: 090 ----
mean loss: 391.40
train mean loss: 382.99
epoch train time: 0:00:02.335144
elapsed time: 0:06:57.752552
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-25 21:39:48.501010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.22
 ---- batch: 020 ----
mean loss: 386.32
 ---- batch: 030 ----
mean loss: 377.94
 ---- batch: 040 ----
mean loss: 403.35
 ---- batch: 050 ----
mean loss: 390.20
 ---- batch: 060 ----
mean loss: 388.83
 ---- batch: 070 ----
mean loss: 390.20
 ---- batch: 080 ----
mean loss: 376.21
 ---- batch: 090 ----
mean loss: 384.22
train mean loss: 385.90
epoch train time: 0:00:02.326180
elapsed time: 0:07:00.078927
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-25 21:39:50.827415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.48
 ---- batch: 020 ----
mean loss: 390.83
 ---- batch: 030 ----
mean loss: 379.85
 ---- batch: 040 ----
mean loss: 382.22
 ---- batch: 050 ----
mean loss: 392.58
 ---- batch: 060 ----
mean loss: 403.74
 ---- batch: 070 ----
mean loss: 392.71
 ---- batch: 080 ----
mean loss: 386.54
 ---- batch: 090 ----
mean loss: 390.69
train mean loss: 389.52
epoch train time: 0:00:02.334256
elapsed time: 0:07:02.413433
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-25 21:39:53.161902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.59
 ---- batch: 020 ----
mean loss: 371.27
 ---- batch: 030 ----
mean loss: 387.16
 ---- batch: 040 ----
mean loss: 378.39
 ---- batch: 050 ----
mean loss: 385.72
 ---- batch: 060 ----
mean loss: 403.24
 ---- batch: 070 ----
mean loss: 402.87
 ---- batch: 080 ----
mean loss: 397.92
 ---- batch: 090 ----
mean loss: 399.09
train mean loss: 390.70
epoch train time: 0:00:02.334619
elapsed time: 0:07:04.748249
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-25 21:39:55.496706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.17
 ---- batch: 020 ----
mean loss: 392.92
 ---- batch: 030 ----
mean loss: 389.09
 ---- batch: 040 ----
mean loss: 378.19
 ---- batch: 050 ----
mean loss: 391.16
 ---- batch: 060 ----
mean loss: 380.10
 ---- batch: 070 ----
mean loss: 378.60
 ---- batch: 080 ----
mean loss: 373.71
 ---- batch: 090 ----
mean loss: 384.23
train mean loss: 384.43
epoch train time: 0:00:02.329904
elapsed time: 0:07:07.078334
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-25 21:39:57.826791
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 398.93
 ---- batch: 020 ----
mean loss: 381.32
 ---- batch: 030 ----
mean loss: 373.44
 ---- batch: 040 ----
mean loss: 381.91
 ---- batch: 050 ----
mean loss: 378.97
 ---- batch: 060 ----
mean loss: 391.81
 ---- batch: 070 ----
mean loss: 390.39
 ---- batch: 080 ----
mean loss: 379.01
 ---- batch: 090 ----
mean loss: 381.18
train mean loss: 383.38
epoch train time: 0:00:02.326317
elapsed time: 0:07:09.404828
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-25 21:40:00.153379
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.69
 ---- batch: 020 ----
mean loss: 382.95
 ---- batch: 030 ----
mean loss: 387.12
 ---- batch: 040 ----
mean loss: 387.79
 ---- batch: 050 ----
mean loss: 398.65
 ---- batch: 060 ----
mean loss: 382.33
 ---- batch: 070 ----
mean loss: 383.70
 ---- batch: 080 ----
mean loss: 363.44
 ---- batch: 090 ----
mean loss: 377.09
train mean loss: 383.88
epoch train time: 0:00:02.325655
elapsed time: 0:07:11.730754
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-25 21:40:02.479208
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.89
 ---- batch: 020 ----
mean loss: 481.29
 ---- batch: 030 ----
mean loss: 410.40
 ---- batch: 040 ----
mean loss: 403.72
 ---- batch: 050 ----
mean loss: 397.27
 ---- batch: 060 ----
mean loss: 385.49
 ---- batch: 070 ----
mean loss: 389.23
 ---- batch: 080 ----
mean loss: 383.92
 ---- batch: 090 ----
mean loss: 409.98
train mean loss: 412.30
epoch train time: 0:00:02.321020
elapsed time: 0:07:14.051949
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-25 21:40:04.800415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.87
 ---- batch: 020 ----
mean loss: 399.82
 ---- batch: 030 ----
mean loss: 382.09
 ---- batch: 040 ----
mean loss: 383.63
 ---- batch: 050 ----
mean loss: 376.09
 ---- batch: 060 ----
mean loss: 370.64
 ---- batch: 070 ----
mean loss: 387.00
 ---- batch: 080 ----
mean loss: 388.68
 ---- batch: 090 ----
mean loss: 385.41
train mean loss: 388.12
epoch train time: 0:00:02.330055
elapsed time: 0:07:16.382196
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-25 21:40:07.130656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 389.12
 ---- batch: 020 ----
mean loss: 389.81
 ---- batch: 030 ----
mean loss: 390.32
 ---- batch: 040 ----
mean loss: 386.24
 ---- batch: 050 ----
mean loss: 373.18
 ---- batch: 060 ----
mean loss: 385.02
 ---- batch: 070 ----
mean loss: 392.35
 ---- batch: 080 ----
mean loss: 400.46
 ---- batch: 090 ----
mean loss: 389.46
train mean loss: 388.02
epoch train time: 0:00:02.329745
elapsed time: 0:07:18.712142
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-25 21:40:09.460583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.89
 ---- batch: 020 ----
mean loss: 383.96
 ---- batch: 030 ----
mean loss: 393.08
 ---- batch: 040 ----
mean loss: 373.57
 ---- batch: 050 ----
mean loss: 381.89
 ---- batch: 060 ----
mean loss: 383.51
 ---- batch: 070 ----
mean loss: 386.42
 ---- batch: 080 ----
mean loss: 383.37
 ---- batch: 090 ----
mean loss: 380.37
train mean loss: 384.26
epoch train time: 0:00:02.338511
elapsed time: 0:07:21.050818
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-25 21:40:11.799274
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.06
 ---- batch: 020 ----
mean loss: 389.09
 ---- batch: 030 ----
mean loss: 382.91
 ---- batch: 040 ----
mean loss: 393.12
 ---- batch: 050 ----
mean loss: 397.93
 ---- batch: 060 ----
mean loss: 380.81
 ---- batch: 070 ----
mean loss: 387.52
 ---- batch: 080 ----
mean loss: 369.94
 ---- batch: 090 ----
mean loss: 373.36
train mean loss: 384.99
epoch train time: 0:00:02.329825
elapsed time: 0:07:23.380819
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-25 21:40:14.129276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.86
 ---- batch: 020 ----
mean loss: 385.89
 ---- batch: 030 ----
mean loss: 391.24
 ---- batch: 040 ----
mean loss: 385.66
 ---- batch: 050 ----
mean loss: 374.03
 ---- batch: 060 ----
mean loss: 382.14
 ---- batch: 070 ----
mean loss: 377.17
 ---- batch: 080 ----
mean loss: 374.54
 ---- batch: 090 ----
mean loss: 395.71
train mean loss: 380.92
epoch train time: 0:00:02.331909
elapsed time: 0:07:25.712913
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-25 21:40:16.461393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 373.95
 ---- batch: 020 ----
mean loss: 396.97
 ---- batch: 030 ----
mean loss: 393.57
 ---- batch: 040 ----
mean loss: 379.51
 ---- batch: 050 ----
mean loss: 378.99
 ---- batch: 060 ----
mean loss: 385.89
 ---- batch: 070 ----
mean loss: 375.48
 ---- batch: 080 ----
mean loss: 384.31
 ---- batch: 090 ----
mean loss: 395.78
train mean loss: 385.76
epoch train time: 0:00:02.339405
elapsed time: 0:07:28.052540
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-25 21:40:18.801000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.59
 ---- batch: 020 ----
mean loss: 392.37
 ---- batch: 030 ----
mean loss: 382.34
 ---- batch: 040 ----
mean loss: 387.96
 ---- batch: 050 ----
mean loss: 377.25
 ---- batch: 060 ----
mean loss: 378.55
 ---- batch: 070 ----
mean loss: 394.26
 ---- batch: 080 ----
mean loss: 367.16
 ---- batch: 090 ----
mean loss: 381.17
train mean loss: 383.06
epoch train time: 0:00:02.329243
elapsed time: 0:07:30.381969
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-25 21:40:21.130431
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.21
 ---- batch: 020 ----
mean loss: 385.56
 ---- batch: 030 ----
mean loss: 382.97
 ---- batch: 040 ----
mean loss: 384.21
 ---- batch: 050 ----
mean loss: 385.69
 ---- batch: 060 ----
mean loss: 373.45
 ---- batch: 070 ----
mean loss: 390.59
 ---- batch: 080 ----
mean loss: 376.30
 ---- batch: 090 ----
mean loss: 371.84
train mean loss: 380.75
epoch train time: 0:00:02.336341
elapsed time: 0:07:32.718584
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-25 21:40:23.467176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.53
 ---- batch: 020 ----
mean loss: 367.62
 ---- batch: 030 ----
mean loss: 377.64
 ---- batch: 040 ----
mean loss: 395.53
 ---- batch: 050 ----
mean loss: 385.63
 ---- batch: 060 ----
mean loss: 376.23
 ---- batch: 070 ----
mean loss: 394.77
 ---- batch: 080 ----
mean loss: 377.47
 ---- batch: 090 ----
mean loss: 383.86
train mean loss: 382.99
epoch train time: 0:00:02.327189
elapsed time: 0:07:35.046102
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-25 21:40:25.794563
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.35
 ---- batch: 020 ----
mean loss: 394.82
 ---- batch: 030 ----
mean loss: 377.52
 ---- batch: 040 ----
mean loss: 385.46
 ---- batch: 050 ----
mean loss: 367.08
 ---- batch: 060 ----
mean loss: 367.37
 ---- batch: 070 ----
mean loss: 387.80
 ---- batch: 080 ----
mean loss: 387.14
 ---- batch: 090 ----
mean loss: 382.43
train mean loss: 381.59
epoch train time: 0:00:02.327534
elapsed time: 0:07:37.373831
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-25 21:40:28.122328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.66
 ---- batch: 020 ----
mean loss: 381.19
 ---- batch: 030 ----
mean loss: 392.02
 ---- batch: 040 ----
mean loss: 381.48
 ---- batch: 050 ----
mean loss: 363.80
 ---- batch: 060 ----
mean loss: 395.90
 ---- batch: 070 ----
mean loss: 376.92
 ---- batch: 080 ----
mean loss: 377.09
 ---- batch: 090 ----
mean loss: 370.98
train mean loss: 379.93
epoch train time: 0:00:02.323588
elapsed time: 0:07:39.697629
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-25 21:40:30.446105
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.20
 ---- batch: 020 ----
mean loss: 396.17
 ---- batch: 030 ----
mean loss: 370.56
 ---- batch: 040 ----
mean loss: 381.18
 ---- batch: 050 ----
mean loss: 370.08
 ---- batch: 060 ----
mean loss: 387.42
 ---- batch: 070 ----
mean loss: 382.02
 ---- batch: 080 ----
mean loss: 374.92
 ---- batch: 090 ----
mean loss: 384.90
train mean loss: 380.39
epoch train time: 0:00:02.336175
elapsed time: 0:07:42.034007
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-25 21:40:32.782475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.29
 ---- batch: 020 ----
mean loss: 373.88
 ---- batch: 030 ----
mean loss: 373.81
 ---- batch: 040 ----
mean loss: 386.54
 ---- batch: 050 ----
mean loss: 394.44
 ---- batch: 060 ----
mean loss: 411.79
 ---- batch: 070 ----
mean loss: 385.00
 ---- batch: 080 ----
mean loss: 368.45
 ---- batch: 090 ----
mean loss: 379.77
train mean loss: 383.36
epoch train time: 0:00:02.325733
elapsed time: 0:07:44.359926
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-25 21:40:35.108406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.12
 ---- batch: 020 ----
mean loss: 372.12
 ---- batch: 030 ----
mean loss: 372.92
 ---- batch: 040 ----
mean loss: 394.09
 ---- batch: 050 ----
mean loss: 388.31
 ---- batch: 060 ----
mean loss: 369.20
 ---- batch: 070 ----
mean loss: 387.99
 ---- batch: 080 ----
mean loss: 388.57
 ---- batch: 090 ----
mean loss: 391.92
train mean loss: 381.84
epoch train time: 0:00:02.330923
elapsed time: 0:07:46.691046
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-25 21:40:37.439503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 401.93
 ---- batch: 020 ----
mean loss: 387.14
 ---- batch: 030 ----
mean loss: 379.72
 ---- batch: 040 ----
mean loss: 387.46
 ---- batch: 050 ----
mean loss: 370.39
 ---- batch: 060 ----
mean loss: 379.35
 ---- batch: 070 ----
mean loss: 382.53
 ---- batch: 080 ----
mean loss: 384.39
 ---- batch: 090 ----
mean loss: 387.66
train mean loss: 385.66
epoch train time: 0:00:02.331231
elapsed time: 0:07:49.022495
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-25 21:40:39.770953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.43
 ---- batch: 020 ----
mean loss: 383.41
 ---- batch: 030 ----
mean loss: 364.23
 ---- batch: 040 ----
mean loss: 377.24
 ---- batch: 050 ----
mean loss: 386.38
 ---- batch: 060 ----
mean loss: 383.09
 ---- batch: 070 ----
mean loss: 384.88
 ---- batch: 080 ----
mean loss: 367.34
 ---- batch: 090 ----
mean loss: 385.43
train mean loss: 379.14
epoch train time: 0:00:02.329873
elapsed time: 0:07:51.352568
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-25 21:40:42.101061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.52
 ---- batch: 020 ----
mean loss: 395.04
 ---- batch: 030 ----
mean loss: 384.78
 ---- batch: 040 ----
mean loss: 380.34
 ---- batch: 050 ----
mean loss: 370.04
 ---- batch: 060 ----
mean loss: 378.56
 ---- batch: 070 ----
mean loss: 382.91
 ---- batch: 080 ----
mean loss: 386.72
 ---- batch: 090 ----
mean loss: 388.95
train mean loss: 382.74
epoch train time: 0:00:02.332983
elapsed time: 0:07:53.685763
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-25 21:40:44.434250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 388.63
 ---- batch: 020 ----
mean loss: 383.34
 ---- batch: 030 ----
mean loss: 381.53
 ---- batch: 040 ----
mean loss: 368.32
 ---- batch: 050 ----
mean loss: 372.33
 ---- batch: 060 ----
mean loss: 372.92
 ---- batch: 070 ----
mean loss: 370.23
 ---- batch: 080 ----
mean loss: 385.46
 ---- batch: 090 ----
mean loss: 390.31
train mean loss: 380.20
epoch train time: 0:00:02.337721
elapsed time: 0:07:56.023699
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-25 21:40:46.772156
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 376.13
 ---- batch: 020 ----
mean loss: 378.73
 ---- batch: 030 ----
mean loss: 387.20
 ---- batch: 040 ----
mean loss: 374.35
 ---- batch: 050 ----
mean loss: 394.05
 ---- batch: 060 ----
mean loss: 388.92
 ---- batch: 070 ----
mean loss: 372.26
 ---- batch: 080 ----
mean loss: 385.81
 ---- batch: 090 ----
mean loss: 375.76
train mean loss: 380.90
epoch train time: 0:00:02.326457
elapsed time: 0:07:58.350334
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-25 21:40:49.098790
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.91
 ---- batch: 020 ----
mean loss: 388.71
 ---- batch: 030 ----
mean loss: 384.54
 ---- batch: 040 ----
mean loss: 375.09
 ---- batch: 050 ----
mean loss: 372.57
 ---- batch: 060 ----
mean loss: 382.64
 ---- batch: 070 ----
mean loss: 390.93
 ---- batch: 080 ----
mean loss: 377.57
 ---- batch: 090 ----
mean loss: 387.22
train mean loss: 382.17
epoch train time: 0:00:02.326370
elapsed time: 0:08:00.676871
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-25 21:40:51.425381
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.91
 ---- batch: 020 ----
mean loss: 371.70
 ---- batch: 030 ----
mean loss: 375.65
 ---- batch: 040 ----
mean loss: 379.43
 ---- batch: 050 ----
mean loss: 383.34
 ---- batch: 060 ----
mean loss: 396.37
 ---- batch: 070 ----
mean loss: 380.16
 ---- batch: 080 ----
mean loss: 373.77
 ---- batch: 090 ----
mean loss: 398.35
train mean loss: 382.81
epoch train time: 0:00:02.326077
elapsed time: 0:08:03.003188
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-25 21:40:53.751652
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 390.97
 ---- batch: 020 ----
mean loss: 370.51
 ---- batch: 030 ----
mean loss: 379.28
 ---- batch: 040 ----
mean loss: 388.37
 ---- batch: 050 ----
mean loss: 377.56
 ---- batch: 060 ----
mean loss: 369.30
 ---- batch: 070 ----
mean loss: 386.85
 ---- batch: 080 ----
mean loss: 390.68
 ---- batch: 090 ----
mean loss: 380.52
train mean loss: 380.51
epoch train time: 0:00:02.322093
elapsed time: 0:08:05.325471
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-25 21:40:56.073939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 379.05
 ---- batch: 020 ----
mean loss: 382.82
 ---- batch: 030 ----
mean loss: 374.57
 ---- batch: 040 ----
mean loss: 372.92
 ---- batch: 050 ----
mean loss: 385.70
 ---- batch: 060 ----
mean loss: 390.23
 ---- batch: 070 ----
mean loss: 384.62
 ---- batch: 080 ----
mean loss: 378.79
 ---- batch: 090 ----
mean loss: 364.03
train mean loss: 378.22
epoch train time: 0:00:02.322107
elapsed time: 0:08:07.647778
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-25 21:40:58.396235
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.10
 ---- batch: 020 ----
mean loss: 379.21
 ---- batch: 030 ----
mean loss: 377.51
 ---- batch: 040 ----
mean loss: 374.59
 ---- batch: 050 ----
mean loss: 386.71
 ---- batch: 060 ----
mean loss: 394.04
 ---- batch: 070 ----
mean loss: 379.99
 ---- batch: 080 ----
mean loss: 379.50
 ---- batch: 090 ----
mean loss: 380.97
train mean loss: 380.31
epoch train time: 0:00:02.328022
elapsed time: 0:08:09.975978
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-25 21:41:00.724443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 380.41
 ---- batch: 020 ----
mean loss: 399.03
 ---- batch: 030 ----
mean loss: 391.68
 ---- batch: 040 ----
mean loss: 392.73
 ---- batch: 050 ----
mean loss: 386.00
 ---- batch: 060 ----
mean loss: 395.28
 ---- batch: 070 ----
mean loss: 404.61
 ---- batch: 080 ----
mean loss: 386.15
 ---- batch: 090 ----
mean loss: 375.75
train mean loss: 388.45
epoch train time: 0:00:02.326362
elapsed time: 0:08:12.302527
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-25 21:41:03.051000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.59
 ---- batch: 020 ----
mean loss: 386.57
 ---- batch: 030 ----
mean loss: 381.55
 ---- batch: 040 ----
mean loss: 383.24
 ---- batch: 050 ----
mean loss: 373.44
 ---- batch: 060 ----
mean loss: 375.05
 ---- batch: 070 ----
mean loss: 384.94
 ---- batch: 080 ----
mean loss: 373.98
 ---- batch: 090 ----
mean loss: 387.84
train mean loss: 378.98
epoch train time: 0:00:02.326398
elapsed time: 0:08:14.629122
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-25 21:41:05.377580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.06
 ---- batch: 020 ----
mean loss: 382.55
 ---- batch: 030 ----
mean loss: 374.76
 ---- batch: 040 ----
mean loss: 385.09
 ---- batch: 050 ----
mean loss: 378.36
 ---- batch: 060 ----
mean loss: 380.32
 ---- batch: 070 ----
mean loss: 384.51
 ---- batch: 080 ----
mean loss: 399.71
 ---- batch: 090 ----
mean loss: 379.83
train mean loss: 384.68
epoch train time: 0:00:02.333826
elapsed time: 0:08:16.963154
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-25 21:41:07.711622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.41
 ---- batch: 020 ----
mean loss: 390.96
 ---- batch: 030 ----
mean loss: 395.77
 ---- batch: 040 ----
mean loss: 372.75
 ---- batch: 050 ----
mean loss: 382.87
 ---- batch: 060 ----
mean loss: 380.82
 ---- batch: 070 ----
mean loss: 385.44
 ---- batch: 080 ----
mean loss: 375.18
 ---- batch: 090 ----
mean loss: 381.24
train mean loss: 383.58
epoch train time: 0:00:02.330641
elapsed time: 0:08:19.293983
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-25 21:41:10.042464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 392.04
 ---- batch: 020 ----
mean loss: 385.74
 ---- batch: 030 ----
mean loss: 375.99
 ---- batch: 040 ----
mean loss: 385.51
 ---- batch: 050 ----
mean loss: 386.14
 ---- batch: 060 ----
mean loss: 369.24
 ---- batch: 070 ----
mean loss: 376.43
 ---- batch: 080 ----
mean loss: 389.71
 ---- batch: 090 ----
mean loss: 410.77
train mean loss: 387.71
epoch train time: 0:00:02.325725
elapsed time: 0:08:21.619932
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-25 21:41:12.368395
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 381.86
 ---- batch: 020 ----
mean loss: 373.91
 ---- batch: 030 ----
mean loss: 376.61
 ---- batch: 040 ----
mean loss: 380.46
 ---- batch: 050 ----
mean loss: 383.46
 ---- batch: 060 ----
mean loss: 368.78
 ---- batch: 070 ----
mean loss: 368.09
 ---- batch: 080 ----
mean loss: 375.60
 ---- batch: 090 ----
mean loss: 373.31
train mean loss: 375.91
epoch train time: 0:00:02.325016
elapsed time: 0:08:23.945171
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-25 21:41:14.693631
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.13
 ---- batch: 020 ----
mean loss: 380.11
 ---- batch: 030 ----
mean loss: 380.31
 ---- batch: 040 ----
mean loss: 359.31
 ---- batch: 050 ----
mean loss: 386.66
 ---- batch: 060 ----
mean loss: 366.42
 ---- batch: 070 ----
mean loss: 361.53
 ---- batch: 080 ----
mean loss: 364.28
 ---- batch: 090 ----
mean loss: 366.82
train mean loss: 372.19
epoch train time: 0:00:02.324016
elapsed time: 0:08:26.269376
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-25 21:41:17.017835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 380.14
 ---- batch: 020 ----
mean loss: 362.40
 ---- batch: 030 ----
mean loss: 363.21
 ---- batch: 040 ----
mean loss: 362.38
 ---- batch: 050 ----
mean loss: 372.68
 ---- batch: 060 ----
mean loss: 355.93
 ---- batch: 070 ----
mean loss: 383.04
 ---- batch: 080 ----
mean loss: 374.41
 ---- batch: 090 ----
mean loss: 376.02
train mean loss: 369.49
epoch train time: 0:00:02.329582
elapsed time: 0:08:28.599127
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-25 21:41:19.347581
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 375.02
 ---- batch: 020 ----
mean loss: 373.48
 ---- batch: 030 ----
mean loss: 366.55
 ---- batch: 040 ----
mean loss: 363.35
 ---- batch: 050 ----
mean loss: 368.11
 ---- batch: 060 ----
mean loss: 367.91
 ---- batch: 070 ----
mean loss: 371.52
 ---- batch: 080 ----
mean loss: 385.92
 ---- batch: 090 ----
mean loss: 371.24
train mean loss: 370.75
epoch train time: 0:00:02.329895
elapsed time: 0:08:30.929273
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-25 21:41:21.677740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.49
 ---- batch: 020 ----
mean loss: 367.68
 ---- batch: 030 ----
mean loss: 377.21
 ---- batch: 040 ----
mean loss: 358.16
 ---- batch: 050 ----
mean loss: 369.73
 ---- batch: 060 ----
mean loss: 369.02
 ---- batch: 070 ----
mean loss: 368.87
 ---- batch: 080 ----
mean loss: 379.71
 ---- batch: 090 ----
mean loss: 370.37
train mean loss: 370.46
epoch train time: 0:00:02.338226
elapsed time: 0:08:33.267704
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-25 21:41:24.016162
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.71
 ---- batch: 020 ----
mean loss: 370.73
 ---- batch: 030 ----
mean loss: 361.86
 ---- batch: 040 ----
mean loss: 380.81
 ---- batch: 050 ----
mean loss: 372.11
 ---- batch: 060 ----
mean loss: 373.07
 ---- batch: 070 ----
mean loss: 373.13
 ---- batch: 080 ----
mean loss: 381.92
 ---- batch: 090 ----
mean loss: 368.89
train mean loss: 372.88
epoch train time: 0:00:02.342588
elapsed time: 0:08:35.610483
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-25 21:41:26.358964
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 380.02
 ---- batch: 020 ----
mean loss: 353.35
 ---- batch: 030 ----
mean loss: 379.86
 ---- batch: 040 ----
mean loss: 375.41
 ---- batch: 050 ----
mean loss: 367.00
 ---- batch: 060 ----
mean loss: 360.63
 ---- batch: 070 ----
mean loss: 372.62
 ---- batch: 080 ----
mean loss: 380.12
 ---- batch: 090 ----
mean loss: 372.20
train mean loss: 372.50
epoch train time: 0:00:02.328323
elapsed time: 0:08:37.939022
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-25 21:41:28.687478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 361.48
 ---- batch: 020 ----
mean loss: 376.32
 ---- batch: 030 ----
mean loss: 364.55
 ---- batch: 040 ----
mean loss: 377.37
 ---- batch: 050 ----
mean loss: 366.58
 ---- batch: 060 ----
mean loss: 377.45
 ---- batch: 070 ----
mean loss: 374.76
 ---- batch: 080 ----
mean loss: 382.24
 ---- batch: 090 ----
mean loss: 376.47
train mean loss: 372.96
epoch train time: 0:00:02.333038
elapsed time: 0:08:40.272248
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-25 21:41:31.020709
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.11
 ---- batch: 020 ----
mean loss: 368.24
 ---- batch: 030 ----
mean loss: 375.18
 ---- batch: 040 ----
mean loss: 369.55
 ---- batch: 050 ----
mean loss: 368.67
 ---- batch: 060 ----
mean loss: 373.54
 ---- batch: 070 ----
mean loss: 370.11
 ---- batch: 080 ----
mean loss: 374.80
 ---- batch: 090 ----
mean loss: 370.85
train mean loss: 371.19
epoch train time: 0:00:02.328117
elapsed time: 0:08:42.600561
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-25 21:41:33.349022
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 388.30
 ---- batch: 020 ----
mean loss: 369.47
 ---- batch: 030 ----
mean loss: 369.91
 ---- batch: 040 ----
mean loss: 367.71
 ---- batch: 050 ----
mean loss: 362.02
 ---- batch: 060 ----
mean loss: 380.73
 ---- batch: 070 ----
mean loss: 385.69
 ---- batch: 080 ----
mean loss: 371.85
 ---- batch: 090 ----
mean loss: 366.28
train mean loss: 372.82
epoch train time: 0:00:02.331164
elapsed time: 0:08:44.931909
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-25 21:41:35.680386
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 386.54
 ---- batch: 020 ----
mean loss: 369.83
 ---- batch: 030 ----
mean loss: 371.97
 ---- batch: 040 ----
mean loss: 369.55
 ---- batch: 050 ----
mean loss: 358.19
 ---- batch: 060 ----
mean loss: 372.27
 ---- batch: 070 ----
mean loss: 366.41
 ---- batch: 080 ----
mean loss: 361.99
 ---- batch: 090 ----
mean loss: 368.11
train mean loss: 369.87
epoch train time: 0:00:02.339741
elapsed time: 0:08:47.271868
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-25 21:41:38.020330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.19
 ---- batch: 020 ----
mean loss: 366.58
 ---- batch: 030 ----
mean loss: 377.06
 ---- batch: 040 ----
mean loss: 389.72
 ---- batch: 050 ----
mean loss: 367.93
 ---- batch: 060 ----
mean loss: 371.51
 ---- batch: 070 ----
mean loss: 378.32
 ---- batch: 080 ----
mean loss: 366.70
 ---- batch: 090 ----
mean loss: 371.07
train mean loss: 373.16
epoch train time: 0:00:02.331793
elapsed time: 0:08:49.603848
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-25 21:41:40.352305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 377.78
 ---- batch: 020 ----
mean loss: 368.66
 ---- batch: 030 ----
mean loss: 369.79
 ---- batch: 040 ----
mean loss: 367.18
 ---- batch: 050 ----
mean loss: 374.20
 ---- batch: 060 ----
mean loss: 369.83
 ---- batch: 070 ----
mean loss: 365.97
 ---- batch: 080 ----
mean loss: 370.97
 ---- batch: 090 ----
mean loss: 369.22
train mean loss: 370.61
epoch train time: 0:00:02.335246
elapsed time: 0:08:51.939290
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-25 21:41:42.687752
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.07
 ---- batch: 020 ----
mean loss: 363.53
 ---- batch: 030 ----
mean loss: 379.76
 ---- batch: 040 ----
mean loss: 374.26
 ---- batch: 050 ----
mean loss: 381.35
 ---- batch: 060 ----
mean loss: 373.52
 ---- batch: 070 ----
mean loss: 379.94
 ---- batch: 080 ----
mean loss: 386.45
 ---- batch: 090 ----
mean loss: 373.14
train mean loss: 376.00
epoch train time: 0:00:02.343322
elapsed time: 0:08:54.282797
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-25 21:41:45.031276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.79
 ---- batch: 020 ----
mean loss: 363.84
 ---- batch: 030 ----
mean loss: 365.73
 ---- batch: 040 ----
mean loss: 368.67
 ---- batch: 050 ----
mean loss: 370.95
 ---- batch: 060 ----
mean loss: 375.41
 ---- batch: 070 ----
mean loss: 370.98
 ---- batch: 080 ----
mean loss: 354.36
 ---- batch: 090 ----
mean loss: 366.03
train mean loss: 367.70
epoch train time: 0:00:02.330010
elapsed time: 0:08:56.613022
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-25 21:41:47.361479
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 378.50
 ---- batch: 020 ----
mean loss: 366.40
 ---- batch: 030 ----
mean loss: 371.22
 ---- batch: 040 ----
mean loss: 374.42
 ---- batch: 050 ----
mean loss: 375.61
 ---- batch: 060 ----
mean loss: 374.62
 ---- batch: 070 ----
mean loss: 360.07
 ---- batch: 080 ----
mean loss: 383.10
 ---- batch: 090 ----
mean loss: 373.95
train mean loss: 372.00
epoch train time: 0:00:02.325360
elapsed time: 0:08:58.938592
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-25 21:41:49.687048
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.97
 ---- batch: 020 ----
mean loss: 376.62
 ---- batch: 030 ----
mean loss: 371.92
 ---- batch: 040 ----
mean loss: 368.05
 ---- batch: 050 ----
mean loss: 363.44
 ---- batch: 060 ----
mean loss: 368.16
 ---- batch: 070 ----
mean loss: 365.01
 ---- batch: 080 ----
mean loss: 371.17
 ---- batch: 090 ----
mean loss: 372.38
train mean loss: 369.21
epoch train time: 0:00:02.334002
elapsed time: 0:09:01.272770
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-25 21:41:52.021226
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.09
 ---- batch: 020 ----
mean loss: 367.43
 ---- batch: 030 ----
mean loss: 372.03
 ---- batch: 040 ----
mean loss: 362.95
 ---- batch: 050 ----
mean loss: 369.26
 ---- batch: 060 ----
mean loss: 365.54
 ---- batch: 070 ----
mean loss: 364.84
 ---- batch: 080 ----
mean loss: 375.26
 ---- batch: 090 ----
mean loss: 370.43
train mean loss: 368.71
epoch train time: 0:00:02.322449
elapsed time: 0:09:03.595407
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-25 21:41:54.343870
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.51
 ---- batch: 020 ----
mean loss: 373.02
 ---- batch: 030 ----
mean loss: 376.24
 ---- batch: 040 ----
mean loss: 363.75
 ---- batch: 050 ----
mean loss: 371.51
 ---- batch: 060 ----
mean loss: 369.39
 ---- batch: 070 ----
mean loss: 368.02
 ---- batch: 080 ----
mean loss: 373.85
 ---- batch: 090 ----
mean loss: 372.62
train mean loss: 369.31
epoch train time: 0:00:02.337339
elapsed time: 0:09:05.932938
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-25 21:41:56.681403
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.20
 ---- batch: 020 ----
mean loss: 365.85
 ---- batch: 030 ----
mean loss: 380.02
 ---- batch: 040 ----
mean loss: 356.84
 ---- batch: 050 ----
mean loss: 381.03
 ---- batch: 060 ----
mean loss: 384.26
 ---- batch: 070 ----
mean loss: 375.40
 ---- batch: 080 ----
mean loss: 374.29
 ---- batch: 090 ----
mean loss: 370.90
train mean loss: 372.82
epoch train time: 0:00:02.333889
elapsed time: 0:09:08.267020
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-25 21:41:59.015483
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 359.13
 ---- batch: 020 ----
mean loss: 372.88
 ---- batch: 030 ----
mean loss: 370.09
 ---- batch: 040 ----
mean loss: 374.57
 ---- batch: 050 ----
mean loss: 374.70
 ---- batch: 060 ----
mean loss: 372.80
 ---- batch: 070 ----
mean loss: 375.59
 ---- batch: 080 ----
mean loss: 379.63
 ---- batch: 090 ----
mean loss: 367.38
train mean loss: 371.55
epoch train time: 0:00:02.335969
elapsed time: 0:09:10.603205
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-25 21:42:01.351683
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.96
 ---- batch: 020 ----
mean loss: 372.58
 ---- batch: 030 ----
mean loss: 365.70
 ---- batch: 040 ----
mean loss: 365.14
 ---- batch: 050 ----
mean loss: 365.14
 ---- batch: 060 ----
mean loss: 380.91
 ---- batch: 070 ----
mean loss: 360.47
 ---- batch: 080 ----
mean loss: 384.91
 ---- batch: 090 ----
mean loss: 368.18
train mean loss: 369.69
epoch train time: 0:00:02.332745
elapsed time: 0:09:12.936220
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-25 21:42:03.684723
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.52
 ---- batch: 020 ----
mean loss: 370.17
 ---- batch: 030 ----
mean loss: 362.29
 ---- batch: 040 ----
mean loss: 372.30
 ---- batch: 050 ----
mean loss: 375.36
 ---- batch: 060 ----
mean loss: 371.81
 ---- batch: 070 ----
mean loss: 364.82
 ---- batch: 080 ----
mean loss: 359.47
 ---- batch: 090 ----
mean loss: 385.96
train mean loss: 370.62
epoch train time: 0:00:02.337726
elapsed time: 0:09:15.274173
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-25 21:42:06.022629
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 363.00
 ---- batch: 020 ----
mean loss: 371.10
 ---- batch: 030 ----
mean loss: 368.70
 ---- batch: 040 ----
mean loss: 364.84
 ---- batch: 050 ----
mean loss: 358.39
 ---- batch: 060 ----
mean loss: 365.22
 ---- batch: 070 ----
mean loss: 371.33
 ---- batch: 080 ----
mean loss: 382.28
 ---- batch: 090 ----
mean loss: 367.82
train mean loss: 367.87
epoch train time: 0:00:02.322855
elapsed time: 0:09:17.597238
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-25 21:42:08.345700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.61
 ---- batch: 020 ----
mean loss: 365.37
 ---- batch: 030 ----
mean loss: 365.56
 ---- batch: 040 ----
mean loss: 369.35
 ---- batch: 050 ----
mean loss: 376.04
 ---- batch: 060 ----
mean loss: 367.12
 ---- batch: 070 ----
mean loss: 368.86
 ---- batch: 080 ----
mean loss: 372.48
 ---- batch: 090 ----
mean loss: 372.05
train mean loss: 370.53
epoch train time: 0:00:02.336280
elapsed time: 0:09:19.933713
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-25 21:42:10.682185
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.26
 ---- batch: 020 ----
mean loss: 365.51
 ---- batch: 030 ----
mean loss: 377.97
 ---- batch: 040 ----
mean loss: 363.25
 ---- batch: 050 ----
mean loss: 367.52
 ---- batch: 060 ----
mean loss: 378.19
 ---- batch: 070 ----
mean loss: 373.61
 ---- batch: 080 ----
mean loss: 371.17
 ---- batch: 090 ----
mean loss: 388.99
train mean loss: 372.16
epoch train time: 0:00:02.328178
elapsed time: 0:09:22.262097
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-25 21:42:13.010557
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 371.02
 ---- batch: 020 ----
mean loss: 375.87
 ---- batch: 030 ----
mean loss: 368.87
 ---- batch: 040 ----
mean loss: 371.13
 ---- batch: 050 ----
mean loss: 362.97
 ---- batch: 060 ----
mean loss: 375.18
 ---- batch: 070 ----
mean loss: 378.88
 ---- batch: 080 ----
mean loss: 367.05
 ---- batch: 090 ----
mean loss: 366.63
train mean loss: 371.42
epoch train time: 0:00:02.341383
elapsed time: 0:09:24.603688
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-25 21:42:15.352148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.92
 ---- batch: 020 ----
mean loss: 364.42
 ---- batch: 030 ----
mean loss: 375.80
 ---- batch: 040 ----
mean loss: 367.91
 ---- batch: 050 ----
mean loss: 369.69
 ---- batch: 060 ----
mean loss: 384.50
 ---- batch: 070 ----
mean loss: 378.49
 ---- batch: 080 ----
mean loss: 354.13
 ---- batch: 090 ----
mean loss: 362.69
train mean loss: 369.71
epoch train time: 0:00:02.325743
elapsed time: 0:09:26.929611
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-25 21:42:17.678070
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.07
 ---- batch: 020 ----
mean loss: 377.45
 ---- batch: 030 ----
mean loss: 369.75
 ---- batch: 040 ----
mean loss: 377.87
 ---- batch: 050 ----
mean loss: 366.74
 ---- batch: 060 ----
mean loss: 359.36
 ---- batch: 070 ----
mean loss: 373.72
 ---- batch: 080 ----
mean loss: 365.24
 ---- batch: 090 ----
mean loss: 376.80
train mean loss: 370.03
epoch train time: 0:00:02.332668
elapsed time: 0:09:29.262481
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-25 21:42:20.010959
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.58
 ---- batch: 020 ----
mean loss: 370.00
 ---- batch: 030 ----
mean loss: 377.41
 ---- batch: 040 ----
mean loss: 364.75
 ---- batch: 050 ----
mean loss: 363.01
 ---- batch: 060 ----
mean loss: 358.82
 ---- batch: 070 ----
mean loss: 358.37
 ---- batch: 080 ----
mean loss: 369.42
 ---- batch: 090 ----
mean loss: 367.91
train mean loss: 366.78
epoch train time: 0:00:02.333287
elapsed time: 0:09:31.595991
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-25 21:42:22.344465
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 365.65
 ---- batch: 020 ----
mean loss: 359.25
 ---- batch: 030 ----
mean loss: 372.04
 ---- batch: 040 ----
mean loss: 358.87
 ---- batch: 050 ----
mean loss: 372.25
 ---- batch: 060 ----
mean loss: 366.12
 ---- batch: 070 ----
mean loss: 362.22
 ---- batch: 080 ----
mean loss: 368.74
 ---- batch: 090 ----
mean loss: 370.91
train mean loss: 367.01
epoch train time: 0:00:02.330944
elapsed time: 0:09:33.927150
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-25 21:42:24.675609
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 376.93
 ---- batch: 020 ----
mean loss: 375.48
 ---- batch: 030 ----
mean loss: 370.59
 ---- batch: 040 ----
mean loss: 354.61
 ---- batch: 050 ----
mean loss: 357.76
 ---- batch: 060 ----
mean loss: 369.02
 ---- batch: 070 ----
mean loss: 362.19
 ---- batch: 080 ----
mean loss: 367.91
 ---- batch: 090 ----
mean loss: 378.21
train mean loss: 368.15
epoch train time: 0:00:02.331529
elapsed time: 0:09:36.258865
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-25 21:42:27.007320
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.25
 ---- batch: 020 ----
mean loss: 366.76
 ---- batch: 030 ----
mean loss: 364.79
 ---- batch: 040 ----
mean loss: 369.20
 ---- batch: 050 ----
mean loss: 370.97
 ---- batch: 060 ----
mean loss: 366.11
 ---- batch: 070 ----
mean loss: 376.28
 ---- batch: 080 ----
mean loss: 373.85
 ---- batch: 090 ----
mean loss: 370.54
train mean loss: 370.09
epoch train time: 0:00:02.331518
elapsed time: 0:09:38.590613
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-25 21:42:29.339074
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 368.07
 ---- batch: 020 ----
mean loss: 375.16
 ---- batch: 030 ----
mean loss: 362.75
 ---- batch: 040 ----
mean loss: 365.71
 ---- batch: 050 ----
mean loss: 368.82
 ---- batch: 060 ----
mean loss: 378.47
 ---- batch: 070 ----
mean loss: 373.05
 ---- batch: 080 ----
mean loss: 369.25
 ---- batch: 090 ----
mean loss: 375.39
train mean loss: 369.69
epoch train time: 0:00:02.329551
elapsed time: 0:09:40.920381
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-25 21:42:31.668837
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.05
 ---- batch: 020 ----
mean loss: 367.47
 ---- batch: 030 ----
mean loss: 374.48
 ---- batch: 040 ----
mean loss: 375.52
 ---- batch: 050 ----
mean loss: 368.90
 ---- batch: 060 ----
mean loss: 381.28
 ---- batch: 070 ----
mean loss: 362.18
 ---- batch: 080 ----
mean loss: 369.39
 ---- batch: 090 ----
mean loss: 388.12
train mean loss: 372.43
epoch train time: 0:00:02.319333
elapsed time: 0:09:43.239886
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-25 21:42:33.988344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 364.81
 ---- batch: 020 ----
mean loss: 366.44
 ---- batch: 030 ----
mean loss: 372.67
 ---- batch: 040 ----
mean loss: 377.46
 ---- batch: 050 ----
mean loss: 373.78
 ---- batch: 060 ----
mean loss: 353.84
 ---- batch: 070 ----
mean loss: 373.18
 ---- batch: 080 ----
mean loss: 365.60
 ---- batch: 090 ----
mean loss: 359.61
train mean loss: 368.09
epoch train time: 0:00:02.326464
elapsed time: 0:09:45.566566
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-25 21:42:36.315042
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 369.43
 ---- batch: 020 ----
mean loss: 372.26
 ---- batch: 030 ----
mean loss: 364.59
 ---- batch: 040 ----
mean loss: 367.27
 ---- batch: 050 ----
mean loss: 371.12
 ---- batch: 060 ----
mean loss: 357.45
 ---- batch: 070 ----
mean loss: 358.68
 ---- batch: 080 ----
mean loss: 373.53
 ---- batch: 090 ----
mean loss: 372.31
train mean loss: 367.12
epoch train time: 0:00:02.333760
elapsed time: 0:09:47.900527
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-25 21:42:38.649002
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 376.55
 ---- batch: 020 ----
mean loss: 373.78
 ---- batch: 030 ----
mean loss: 376.84
 ---- batch: 040 ----
mean loss: 375.46
 ---- batch: 050 ----
mean loss: 368.61
 ---- batch: 060 ----
mean loss: 358.20
 ---- batch: 070 ----
mean loss: 381.26
 ---- batch: 080 ----
mean loss: 362.24
 ---- batch: 090 ----
mean loss: 369.81
train mean loss: 371.09
epoch train time: 0:00:02.334140
elapsed time: 0:09:50.234866
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-25 21:42:40.983330
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.62
 ---- batch: 020 ----
mean loss: 371.76
 ---- batch: 030 ----
mean loss: 371.26
 ---- batch: 040 ----
mean loss: 372.43
 ---- batch: 050 ----
mean loss: 373.07
 ---- batch: 060 ----
mean loss: 365.98
 ---- batch: 070 ----
mean loss: 357.42
 ---- batch: 080 ----
mean loss: 375.68
 ---- batch: 090 ----
mean loss: 378.88
train mean loss: 370.23
epoch train time: 0:00:02.330667
elapsed time: 0:09:52.565729
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-25 21:42:43.314192
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.56
 ---- batch: 020 ----
mean loss: 373.04
 ---- batch: 030 ----
mean loss: 365.31
 ---- batch: 040 ----
mean loss: 374.98
 ---- batch: 050 ----
mean loss: 369.43
 ---- batch: 060 ----
mean loss: 369.75
 ---- batch: 070 ----
mean loss: 370.18
 ---- batch: 080 ----
mean loss: 386.20
 ---- batch: 090 ----
mean loss: 360.76
train mean loss: 370.57
epoch train time: 0:00:02.347002
elapsed time: 0:09:54.912931
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-25 21:42:45.661413
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 373.25
 ---- batch: 020 ----
mean loss: 366.13
 ---- batch: 030 ----
mean loss: 369.77
 ---- batch: 040 ----
mean loss: 368.99
 ---- batch: 050 ----
mean loss: 370.53
 ---- batch: 060 ----
mean loss: 365.07
 ---- batch: 070 ----
mean loss: 365.19
 ---- batch: 080 ----
mean loss: 366.78
 ---- batch: 090 ----
mean loss: 384.79
train mean loss: 369.54
epoch train time: 0:00:02.328455
elapsed time: 0:09:57.241606
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-25 21:42:47.990063
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 376.32
 ---- batch: 020 ----
mean loss: 366.29
 ---- batch: 030 ----
mean loss: 368.79
 ---- batch: 040 ----
mean loss: 366.59
 ---- batch: 050 ----
mean loss: 364.57
 ---- batch: 060 ----
mean loss: 369.39
 ---- batch: 070 ----
mean loss: 360.32
 ---- batch: 080 ----
mean loss: 357.34
 ---- batch: 090 ----
mean loss: 377.57
train mean loss: 367.49
epoch train time: 0:00:02.329438
elapsed time: 0:09:59.571223
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-25 21:42:50.319684
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 377.28
 ---- batch: 020 ----
mean loss: 374.72
 ---- batch: 030 ----
mean loss: 371.88
 ---- batch: 040 ----
mean loss: 362.69
 ---- batch: 050 ----
mean loss: 364.27
 ---- batch: 060 ----
mean loss: 376.43
 ---- batch: 070 ----
mean loss: 371.86
 ---- batch: 080 ----
mean loss: 365.08
 ---- batch: 090 ----
mean loss: 370.58
train mean loss: 370.10
epoch train time: 0:00:02.324224
elapsed time: 0:10:01.895633
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-25 21:42:52.644087
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 366.79
 ---- batch: 020 ----
mean loss: 375.12
 ---- batch: 030 ----
mean loss: 359.66
 ---- batch: 040 ----
mean loss: 367.09
 ---- batch: 050 ----
mean loss: 370.31
 ---- batch: 060 ----
mean loss: 360.49
 ---- batch: 070 ----
mean loss: 375.74
 ---- batch: 080 ----
mean loss: 382.93
 ---- batch: 090 ----
mean loss: 379.38
train mean loss: 370.61
epoch train time: 0:00:02.332340
elapsed time: 0:10:04.228153
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-25 21:42:54.976651
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 355.73
 ---- batch: 020 ----
mean loss: 364.35
 ---- batch: 030 ----
mean loss: 371.00
 ---- batch: 040 ----
mean loss: 379.08
 ---- batch: 050 ----
mean loss: 367.97
 ---- batch: 060 ----
mean loss: 368.87
 ---- batch: 070 ----
mean loss: 364.84
 ---- batch: 080 ----
mean loss: 363.58
 ---- batch: 090 ----
mean loss: 379.20
train mean loss: 368.82
epoch train time: 0:00:02.335859
elapsed time: 0:10:06.564225
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-25 21:42:57.312679
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 367.10
 ---- batch: 020 ----
mean loss: 377.45
 ---- batch: 030 ----
mean loss: 366.29
 ---- batch: 040 ----
mean loss: 369.97
 ---- batch: 050 ----
mean loss: 377.07
 ---- batch: 060 ----
mean loss: 357.23
 ---- batch: 070 ----
mean loss: 366.76
 ---- batch: 080 ----
mean loss: 368.65
 ---- batch: 090 ----
mean loss: 385.81
train mean loss: 370.37
epoch train time: 0:00:02.336099
elapsed time: 0:10:08.900518
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-25 21:42:59.648981
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 360.28
 ---- batch: 020 ----
mean loss: 378.71
 ---- batch: 030 ----
mean loss: 376.27
 ---- batch: 040 ----
mean loss: 380.71
 ---- batch: 050 ----
mean loss: 378.22
 ---- batch: 060 ----
mean loss: 368.64
 ---- batch: 070 ----
mean loss: 362.14
 ---- batch: 080 ----
mean loss: 366.58
 ---- batch: 090 ----
mean loss: 373.64
train mean loss: 371.73
epoch train time: 0:00:02.339513
elapsed time: 0:10:11.240212
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-25 21:43:01.988683
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 376.70
 ---- batch: 020 ----
mean loss: 372.77
 ---- batch: 030 ----
mean loss: 363.65
 ---- batch: 040 ----
mean loss: 370.62
 ---- batch: 050 ----
mean loss: 359.43
 ---- batch: 060 ----
mean loss: 368.64
 ---- batch: 070 ----
mean loss: 372.05
 ---- batch: 080 ----
mean loss: 361.93
 ---- batch: 090 ----
mean loss: 366.12
train mean loss: 368.27
epoch train time: 0:00:02.342252
elapsed time: 0:10:13.582690
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-25 21:43:04.331148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 362.84
 ---- batch: 020 ----
mean loss: 373.19
 ---- batch: 030 ----
mean loss: 367.56
 ---- batch: 040 ----
mean loss: 361.11
 ---- batch: 050 ----
mean loss: 381.05
 ---- batch: 060 ----
mean loss: 376.26
 ---- batch: 070 ----
mean loss: 369.16
 ---- batch: 080 ----
mean loss: 358.77
 ---- batch: 090 ----
mean loss: 369.73
train mean loss: 368.06
epoch train time: 0:00:02.333538
elapsed time: 0:10:15.935354
checkpoint saved in file: log/CMAPSS/FD002/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_5/checkpoint.pth.tar
**** end time: 2019-09-25 21:43:06.683785 ****
