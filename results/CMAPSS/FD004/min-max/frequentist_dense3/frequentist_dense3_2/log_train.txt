Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_2', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 17947
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistDense3...
Done.
**** start time: 2019-09-27 02:10:11.878980 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
            Linear-2                  [-1, 100]          36,000
           Sigmoid-3                  [-1, 100]               0
            Linear-4                  [-1, 100]          10,000
           Sigmoid-5                  [-1, 100]               0
            Linear-6                  [-1, 100]          10,000
           Sigmoid-7                  [-1, 100]               0
            Linear-8                    [-1, 1]             100
================================================================
Total params: 56,100
Trainable params: 56,100
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-27 02:10:11.882224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4951.64
 ---- batch: 020 ----
mean loss: 4826.77
 ---- batch: 030 ----
mean loss: 4716.16
 ---- batch: 040 ----
mean loss: 4610.25
 ---- batch: 050 ----
mean loss: 4546.19
 ---- batch: 060 ----
mean loss: 4446.76
 ---- batch: 070 ----
mean loss: 4419.71
 ---- batch: 080 ----
mean loss: 4353.04
 ---- batch: 090 ----
mean loss: 4286.92
 ---- batch: 100 ----
mean loss: 4256.66
 ---- batch: 110 ----
mean loss: 4221.30
train mean loss: 4502.73
epoch train time: 0:00:32.728976
elapsed time: 0:00:32.734389
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-27 02:10:44.613420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4105.65
 ---- batch: 020 ----
mean loss: 4017.63
 ---- batch: 030 ----
mean loss: 3993.12
 ---- batch: 040 ----
mean loss: 3927.88
 ---- batch: 050 ----
mean loss: 3907.42
 ---- batch: 060 ----
mean loss: 3819.36
 ---- batch: 070 ----
mean loss: 3731.62
 ---- batch: 080 ----
mean loss: 3724.81
 ---- batch: 090 ----
mean loss: 3640.12
 ---- batch: 100 ----
mean loss: 3553.63
 ---- batch: 110 ----
mean loss: 3475.06
train mean loss: 3801.73
epoch train time: 0:00:00.552573
elapsed time: 0:00:33.287103
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-27 02:10:45.166133
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3461.20
 ---- batch: 020 ----
mean loss: 3409.18
 ---- batch: 030 ----
mean loss: 3386.49
 ---- batch: 040 ----
mean loss: 3330.40
 ---- batch: 050 ----
mean loss: 3271.66
 ---- batch: 060 ----
mean loss: 3211.67
 ---- batch: 070 ----
mean loss: 3187.72
 ---- batch: 080 ----
mean loss: 3105.41
 ---- batch: 090 ----
mean loss: 3048.02
 ---- batch: 100 ----
mean loss: 3048.55
 ---- batch: 110 ----
mean loss: 2923.11
train mean loss: 3211.11
epoch train time: 0:00:00.550463
elapsed time: 0:00:33.837700
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-27 02:10:45.716750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2913.18
 ---- batch: 020 ----
mean loss: 2829.73
 ---- batch: 030 ----
mean loss: 2864.10
 ---- batch: 040 ----
mean loss: 2827.63
 ---- batch: 050 ----
mean loss: 2745.06
 ---- batch: 060 ----
mean loss: 2737.56
 ---- batch: 070 ----
mean loss: 2690.22
 ---- batch: 080 ----
mean loss: 2689.08
 ---- batch: 090 ----
mean loss: 2588.81
 ---- batch: 100 ----
mean loss: 2571.27
 ---- batch: 110 ----
mean loss: 2530.92
train mean loss: 2720.22
epoch train time: 0:00:00.545509
elapsed time: 0:00:34.383383
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-27 02:10:46.262426
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2518.74
 ---- batch: 020 ----
mean loss: 2453.23
 ---- batch: 030 ----
mean loss: 2400.73
 ---- batch: 040 ----
mean loss: 2360.35
 ---- batch: 050 ----
mean loss: 2339.22
 ---- batch: 060 ----
mean loss: 2291.09
 ---- batch: 070 ----
mean loss: 2236.05
 ---- batch: 080 ----
mean loss: 2197.36
 ---- batch: 090 ----
mean loss: 2188.27
 ---- batch: 100 ----
mean loss: 2169.40
 ---- batch: 110 ----
mean loss: 2146.57
train mean loss: 2294.40
epoch train time: 0:00:00.552042
elapsed time: 0:00:34.935572
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-27 02:10:46.814596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2101.50
 ---- batch: 020 ----
mean loss: 2065.13
 ---- batch: 030 ----
mean loss: 2027.74
 ---- batch: 040 ----
mean loss: 2033.70
 ---- batch: 050 ----
mean loss: 1951.43
 ---- batch: 060 ----
mean loss: 1923.53
 ---- batch: 070 ----
mean loss: 1890.19
 ---- batch: 080 ----
mean loss: 1857.13
 ---- batch: 090 ----
mean loss: 1846.33
 ---- batch: 100 ----
mean loss: 1837.72
 ---- batch: 110 ----
mean loss: 1794.23
train mean loss: 1935.24
epoch train time: 0:00:00.548370
elapsed time: 0:00:35.484070
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-27 02:10:47.363094
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1768.90
 ---- batch: 020 ----
mean loss: 1735.51
 ---- batch: 030 ----
mean loss: 1700.78
 ---- batch: 040 ----
mean loss: 1687.76
 ---- batch: 050 ----
mean loss: 1701.43
 ---- batch: 060 ----
mean loss: 1607.42
 ---- batch: 070 ----
mean loss: 1599.93
 ---- batch: 080 ----
mean loss: 1574.11
 ---- batch: 090 ----
mean loss: 1569.10
 ---- batch: 100 ----
mean loss: 1529.28
 ---- batch: 110 ----
mean loss: 1548.29
train mean loss: 1634.95
epoch train time: 0:00:00.555725
elapsed time: 0:00:36.039926
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-27 02:10:47.918951
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1480.68
 ---- batch: 020 ----
mean loss: 1491.43
 ---- batch: 030 ----
mean loss: 1444.09
 ---- batch: 040 ----
mean loss: 1425.55
 ---- batch: 050 ----
mean loss: 1423.53
 ---- batch: 060 ----
mean loss: 1406.48
 ---- batch: 070 ----
mean loss: 1388.82
 ---- batch: 080 ----
mean loss: 1375.30
 ---- batch: 090 ----
mean loss: 1378.91
 ---- batch: 100 ----
mean loss: 1362.87
 ---- batch: 110 ----
mean loss: 1303.90
train mean loss: 1405.60
epoch train time: 0:00:00.548311
elapsed time: 0:00:36.588383
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-27 02:10:48.467410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1293.53
 ---- batch: 020 ----
mean loss: 1300.93
 ---- batch: 030 ----
mean loss: 1268.86
 ---- batch: 040 ----
mean loss: 1248.76
 ---- batch: 050 ----
mean loss: 1251.40
 ---- batch: 060 ----
mean loss: 1249.61
 ---- batch: 070 ----
mean loss: 1232.39
 ---- batch: 080 ----
mean loss: 1215.33
 ---- batch: 090 ----
mean loss: 1199.85
 ---- batch: 100 ----
mean loss: 1198.53
 ---- batch: 110 ----
mean loss: 1191.71
train mean loss: 1238.38
epoch train time: 0:00:00.560971
elapsed time: 0:00:37.149486
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-27 02:10:49.028541
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1166.77
 ---- batch: 020 ----
mean loss: 1163.32
 ---- batch: 030 ----
mean loss: 1135.92
 ---- batch: 040 ----
mean loss: 1123.99
 ---- batch: 050 ----
mean loss: 1119.89
 ---- batch: 060 ----
mean loss: 1119.19
 ---- batch: 070 ----
mean loss: 1114.06
 ---- batch: 080 ----
mean loss: 1098.74
 ---- batch: 090 ----
mean loss: 1093.72
 ---- batch: 100 ----
mean loss: 1068.42
 ---- batch: 110 ----
mean loss: 1083.32
train mean loss: 1115.95
epoch train time: 0:00:00.550637
elapsed time: 0:00:37.700280
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-27 02:10:49.579305
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1059.35
 ---- batch: 020 ----
mean loss: 1050.64
 ---- batch: 030 ----
mean loss: 1050.58
 ---- batch: 040 ----
mean loss: 1044.20
 ---- batch: 050 ----
mean loss: 1035.51
 ---- batch: 060 ----
mean loss: 1027.51
 ---- batch: 070 ----
mean loss: 1008.98
 ---- batch: 080 ----
mean loss: 1013.12
 ---- batch: 090 ----
mean loss: 1015.24
 ---- batch: 100 ----
mean loss: 1012.20
 ---- batch: 110 ----
mean loss: 991.30
train mean loss: 1027.22
epoch train time: 0:00:00.577425
elapsed time: 0:00:38.277840
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-27 02:10:50.156868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 997.61
 ---- batch: 020 ----
mean loss: 982.36
 ---- batch: 030 ----
mean loss: 977.35
 ---- batch: 040 ----
mean loss: 966.53
 ---- batch: 050 ----
mean loss: 955.18
 ---- batch: 060 ----
mean loss: 963.00
 ---- batch: 070 ----
mean loss: 965.82
 ---- batch: 080 ----
mean loss: 954.83
 ---- batch: 090 ----
mean loss: 955.74
 ---- batch: 100 ----
mean loss: 950.58
 ---- batch: 110 ----
mean loss: 937.30
train mean loss: 964.17
epoch train time: 0:00:00.564834
elapsed time: 0:00:38.842807
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-27 02:10:50.721833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 953.11
 ---- batch: 020 ----
mean loss: 942.88
 ---- batch: 030 ----
mean loss: 935.57
 ---- batch: 040 ----
mean loss: 926.57
 ---- batch: 050 ----
mean loss: 918.19
 ---- batch: 060 ----
mean loss: 902.62
 ---- batch: 070 ----
mean loss: 927.90
 ---- batch: 080 ----
mean loss: 903.53
 ---- batch: 090 ----
mean loss: 908.80
 ---- batch: 100 ----
mean loss: 919.53
 ---- batch: 110 ----
mean loss: 895.17
train mean loss: 920.51
epoch train time: 0:00:00.553381
elapsed time: 0:00:39.396323
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-27 02:10:51.275350
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 892.17
 ---- batch: 020 ----
mean loss: 893.58
 ---- batch: 030 ----
mean loss: 895.28
 ---- batch: 040 ----
mean loss: 888.40
 ---- batch: 050 ----
mean loss: 889.98
 ---- batch: 060 ----
mean loss: 900.03
 ---- batch: 070 ----
mean loss: 892.60
 ---- batch: 080 ----
mean loss: 891.64
 ---- batch: 090 ----
mean loss: 880.67
 ---- batch: 100 ----
mean loss: 888.34
 ---- batch: 110 ----
mean loss: 888.09
train mean loss: 891.35
epoch train time: 0:00:00.552876
elapsed time: 0:00:39.949358
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-27 02:10:51.828394
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 881.28
 ---- batch: 020 ----
mean loss: 875.21
 ---- batch: 030 ----
mean loss: 891.02
 ---- batch: 040 ----
mean loss: 888.35
 ---- batch: 050 ----
mean loss: 872.75
 ---- batch: 060 ----
mean loss: 865.64
 ---- batch: 070 ----
mean loss: 864.32
 ---- batch: 080 ----
mean loss: 860.06
 ---- batch: 090 ----
mean loss: 868.66
 ---- batch: 100 ----
mean loss: 859.90
 ---- batch: 110 ----
mean loss: 878.36
train mean loss: 872.26
epoch train time: 0:00:00.559015
elapsed time: 0:00:40.508515
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-27 02:10:52.387543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 871.58
 ---- batch: 020 ----
mean loss: 867.99
 ---- batch: 030 ----
mean loss: 859.37
 ---- batch: 040 ----
mean loss: 848.44
 ---- batch: 050 ----
mean loss: 858.38
 ---- batch: 060 ----
mean loss: 863.10
 ---- batch: 070 ----
mean loss: 873.12
 ---- batch: 080 ----
mean loss: 849.87
 ---- batch: 090 ----
mean loss: 852.81
 ---- batch: 100 ----
mean loss: 865.15
 ---- batch: 110 ----
mean loss: 846.35
train mean loss: 860.40
epoch train time: 0:00:00.554618
elapsed time: 0:00:41.063283
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-27 02:10:52.942328
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.37
 ---- batch: 020 ----
mean loss: 829.51
 ---- batch: 030 ----
mean loss: 853.99
 ---- batch: 040 ----
mean loss: 871.24
 ---- batch: 050 ----
mean loss: 870.46
 ---- batch: 060 ----
mean loss: 867.23
 ---- batch: 070 ----
mean loss: 861.05
 ---- batch: 080 ----
mean loss: 852.98
 ---- batch: 090 ----
mean loss: 839.63
 ---- batch: 100 ----
mean loss: 843.17
 ---- batch: 110 ----
mean loss: 845.03
train mean loss: 853.16
epoch train time: 0:00:00.557802
elapsed time: 0:00:41.621239
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-27 02:10:53.500266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.10
 ---- batch: 020 ----
mean loss: 853.78
 ---- batch: 030 ----
mean loss: 837.69
 ---- batch: 040 ----
mean loss: 849.91
 ---- batch: 050 ----
mean loss: 863.05
 ---- batch: 060 ----
mean loss: 830.00
 ---- batch: 070 ----
mean loss: 862.20
 ---- batch: 080 ----
mean loss: 844.92
 ---- batch: 090 ----
mean loss: 845.32
 ---- batch: 100 ----
mean loss: 862.88
 ---- batch: 110 ----
mean loss: 858.24
train mean loss: 848.78
epoch train time: 0:00:00.556861
elapsed time: 0:00:42.178246
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-27 02:10:54.057275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 851.68
 ---- batch: 020 ----
mean loss: 858.10
 ---- batch: 030 ----
mean loss: 845.31
 ---- batch: 040 ----
mean loss: 826.37
 ---- batch: 050 ----
mean loss: 837.59
 ---- batch: 060 ----
mean loss: 853.15
 ---- batch: 070 ----
mean loss: 843.07
 ---- batch: 080 ----
mean loss: 844.28
 ---- batch: 090 ----
mean loss: 853.67
 ---- batch: 100 ----
mean loss: 841.83
 ---- batch: 110 ----
mean loss: 863.94
train mean loss: 846.33
epoch train time: 0:00:00.555662
elapsed time: 0:00:42.734045
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-27 02:10:54.613072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.18
 ---- batch: 020 ----
mean loss: 853.80
 ---- batch: 030 ----
mean loss: 855.46
 ---- batch: 040 ----
mean loss: 840.12
 ---- batch: 050 ----
mean loss: 829.55
 ---- batch: 060 ----
mean loss: 852.23
 ---- batch: 070 ----
mean loss: 843.87
 ---- batch: 080 ----
mean loss: 849.23
 ---- batch: 090 ----
mean loss: 839.23
 ---- batch: 100 ----
mean loss: 847.14
 ---- batch: 110 ----
mean loss: 852.10
train mean loss: 845.12
epoch train time: 0:00:00.557091
elapsed time: 0:00:43.291271
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-27 02:10:55.170297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 809.13
 ---- batch: 020 ----
mean loss: 880.47
 ---- batch: 030 ----
mean loss: 843.77
 ---- batch: 040 ----
mean loss: 869.50
 ---- batch: 050 ----
mean loss: 848.09
 ---- batch: 060 ----
mean loss: 859.93
 ---- batch: 070 ----
mean loss: 840.83
 ---- batch: 080 ----
mean loss: 857.68
 ---- batch: 090 ----
mean loss: 831.18
 ---- batch: 100 ----
mean loss: 826.80
 ---- batch: 110 ----
mean loss: 828.44
train mean loss: 844.51
epoch train time: 0:00:00.548229
elapsed time: 0:00:43.839663
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-27 02:10:55.718687
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 841.31
 ---- batch: 020 ----
mean loss: 856.59
 ---- batch: 030 ----
mean loss: 828.49
 ---- batch: 040 ----
mean loss: 858.93
 ---- batch: 050 ----
mean loss: 850.72
 ---- batch: 060 ----
mean loss: 841.81
 ---- batch: 070 ----
mean loss: 854.08
 ---- batch: 080 ----
mean loss: 837.92
 ---- batch: 090 ----
mean loss: 835.50
 ---- batch: 100 ----
mean loss: 832.02
 ---- batch: 110 ----
mean loss: 853.16
train mean loss: 844.20
epoch train time: 0:00:00.546554
elapsed time: 0:00:44.386363
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-27 02:10:56.265406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 824.65
 ---- batch: 020 ----
mean loss: 834.96
 ---- batch: 030 ----
mean loss: 819.38
 ---- batch: 040 ----
mean loss: 844.27
 ---- batch: 050 ----
mean loss: 871.30
 ---- batch: 060 ----
mean loss: 835.81
 ---- batch: 070 ----
mean loss: 864.69
 ---- batch: 080 ----
mean loss: 832.34
 ---- batch: 090 ----
mean loss: 846.11
 ---- batch: 100 ----
mean loss: 861.93
 ---- batch: 110 ----
mean loss: 844.32
train mean loss: 844.11
epoch train time: 0:00:00.547495
elapsed time: 0:00:44.934010
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-27 02:10:56.813028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.58
 ---- batch: 020 ----
mean loss: 841.72
 ---- batch: 030 ----
mean loss: 856.79
 ---- batch: 040 ----
mean loss: 844.14
 ---- batch: 050 ----
mean loss: 860.66
 ---- batch: 060 ----
mean loss: 834.29
 ---- batch: 070 ----
mean loss: 833.58
 ---- batch: 080 ----
mean loss: 858.24
 ---- batch: 090 ----
mean loss: 841.93
 ---- batch: 100 ----
mean loss: 851.76
 ---- batch: 110 ----
mean loss: 829.38
train mean loss: 844.03
epoch train time: 0:00:00.549141
elapsed time: 0:00:45.483280
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-27 02:10:57.362320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.91
 ---- batch: 020 ----
mean loss: 842.29
 ---- batch: 030 ----
mean loss: 827.43
 ---- batch: 040 ----
mean loss: 849.84
 ---- batch: 050 ----
mean loss: 846.95
 ---- batch: 060 ----
mean loss: 858.61
 ---- batch: 070 ----
mean loss: 818.99
 ---- batch: 080 ----
mean loss: 846.59
 ---- batch: 090 ----
mean loss: 854.55
 ---- batch: 100 ----
mean loss: 835.62
 ---- batch: 110 ----
mean loss: 854.63
train mean loss: 844.01
epoch train time: 0:00:00.550583
elapsed time: 0:00:46.034013
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-27 02:10:57.913039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.82
 ---- batch: 020 ----
mean loss: 845.54
 ---- batch: 030 ----
mean loss: 843.55
 ---- batch: 040 ----
mean loss: 840.51
 ---- batch: 050 ----
mean loss: 834.66
 ---- batch: 060 ----
mean loss: 852.33
 ---- batch: 070 ----
mean loss: 864.82
 ---- batch: 080 ----
mean loss: 830.97
 ---- batch: 090 ----
mean loss: 848.36
 ---- batch: 100 ----
mean loss: 838.77
 ---- batch: 110 ----
mean loss: 838.40
train mean loss: 843.97
epoch train time: 0:00:00.547993
elapsed time: 0:00:46.582137
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-27 02:10:58.461162
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.06
 ---- batch: 020 ----
mean loss: 852.36
 ---- batch: 030 ----
mean loss: 858.96
 ---- batch: 040 ----
mean loss: 845.33
 ---- batch: 050 ----
mean loss: 839.66
 ---- batch: 060 ----
mean loss: 819.43
 ---- batch: 070 ----
mean loss: 828.53
 ---- batch: 080 ----
mean loss: 860.83
 ---- batch: 090 ----
mean loss: 863.11
 ---- batch: 100 ----
mean loss: 836.79
 ---- batch: 110 ----
mean loss: 837.83
train mean loss: 843.92
epoch train time: 0:00:00.561982
elapsed time: 0:00:47.144248
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-27 02:10:59.023275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.02
 ---- batch: 020 ----
mean loss: 834.67
 ---- batch: 030 ----
mean loss: 854.12
 ---- batch: 040 ----
mean loss: 865.54
 ---- batch: 050 ----
mean loss: 838.79
 ---- batch: 060 ----
mean loss: 839.51
 ---- batch: 070 ----
mean loss: 829.96
 ---- batch: 080 ----
mean loss: 849.24
 ---- batch: 090 ----
mean loss: 852.49
 ---- batch: 100 ----
mean loss: 835.79
 ---- batch: 110 ----
mean loss: 847.18
train mean loss: 843.84
epoch train time: 0:00:00.555840
elapsed time: 0:00:47.700223
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-27 02:10:59.579253
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 808.63
 ---- batch: 020 ----
mean loss: 839.61
 ---- batch: 030 ----
mean loss: 857.26
 ---- batch: 040 ----
mean loss: 861.71
 ---- batch: 050 ----
mean loss: 858.47
 ---- batch: 060 ----
mean loss: 838.94
 ---- batch: 070 ----
mean loss: 851.71
 ---- batch: 080 ----
mean loss: 844.45
 ---- batch: 090 ----
mean loss: 822.49
 ---- batch: 100 ----
mean loss: 855.87
 ---- batch: 110 ----
mean loss: 837.78
train mean loss: 843.94
epoch train time: 0:00:00.565710
elapsed time: 0:00:48.266093
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-27 02:11:00.145144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 860.03
 ---- batch: 020 ----
mean loss: 823.18
 ---- batch: 030 ----
mean loss: 834.62
 ---- batch: 040 ----
mean loss: 843.74
 ---- batch: 050 ----
mean loss: 847.57
 ---- batch: 060 ----
mean loss: 842.48
 ---- batch: 070 ----
mean loss: 847.41
 ---- batch: 080 ----
mean loss: 859.33
 ---- batch: 090 ----
mean loss: 843.55
 ---- batch: 100 ----
mean loss: 848.51
 ---- batch: 110 ----
mean loss: 841.92
train mean loss: 843.91
epoch train time: 0:00:00.552859
elapsed time: 0:00:48.819111
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-27 02:11:00.698140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 834.91
 ---- batch: 020 ----
mean loss: 839.22
 ---- batch: 030 ----
mean loss: 847.48
 ---- batch: 040 ----
mean loss: 860.20
 ---- batch: 050 ----
mean loss: 844.45
 ---- batch: 060 ----
mean loss: 840.43
 ---- batch: 070 ----
mean loss: 807.10
 ---- batch: 080 ----
mean loss: 851.61
 ---- batch: 090 ----
mean loss: 844.55
 ---- batch: 100 ----
mean loss: 855.05
 ---- batch: 110 ----
mean loss: 856.51
train mean loss: 844.02
epoch train time: 0:00:00.557725
elapsed time: 0:00:49.376973
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-27 02:11:01.255999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.92
 ---- batch: 020 ----
mean loss: 833.89
 ---- batch: 030 ----
mean loss: 864.36
 ---- batch: 040 ----
mean loss: 867.12
 ---- batch: 050 ----
mean loss: 818.03
 ---- batch: 060 ----
mean loss: 833.12
 ---- batch: 070 ----
mean loss: 855.68
 ---- batch: 080 ----
mean loss: 846.87
 ---- batch: 090 ----
mean loss: 842.99
 ---- batch: 100 ----
mean loss: 845.87
 ---- batch: 110 ----
mean loss: 831.11
train mean loss: 844.04
epoch train time: 0:00:00.555359
elapsed time: 0:00:49.932473
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-27 02:11:01.811509
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 829.77
 ---- batch: 020 ----
mean loss: 832.53
 ---- batch: 030 ----
mean loss: 855.60
 ---- batch: 040 ----
mean loss: 864.32
 ---- batch: 050 ----
mean loss: 850.39
 ---- batch: 060 ----
mean loss: 854.95
 ---- batch: 070 ----
mean loss: 825.55
 ---- batch: 080 ----
mean loss: 851.27
 ---- batch: 090 ----
mean loss: 841.01
 ---- batch: 100 ----
mean loss: 851.35
 ---- batch: 110 ----
mean loss: 831.00
train mean loss: 843.87
epoch train time: 0:00:00.553681
elapsed time: 0:00:50.486297
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-27 02:11:02.365340
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.60
 ---- batch: 020 ----
mean loss: 848.83
 ---- batch: 030 ----
mean loss: 839.32
 ---- batch: 040 ----
mean loss: 843.44
 ---- batch: 050 ----
mean loss: 824.87
 ---- batch: 060 ----
mean loss: 836.67
 ---- batch: 070 ----
mean loss: 843.42
 ---- batch: 080 ----
mean loss: 858.18
 ---- batch: 090 ----
mean loss: 855.87
 ---- batch: 100 ----
mean loss: 850.60
 ---- batch: 110 ----
mean loss: 855.83
train mean loss: 843.97
epoch train time: 0:00:00.559273
elapsed time: 0:00:51.045724
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-27 02:11:02.924757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.51
 ---- batch: 020 ----
mean loss: 814.73
 ---- batch: 030 ----
mean loss: 838.88
 ---- batch: 040 ----
mean loss: 851.10
 ---- batch: 050 ----
mean loss: 848.75
 ---- batch: 060 ----
mean loss: 852.61
 ---- batch: 070 ----
mean loss: 834.72
 ---- batch: 080 ----
mean loss: 850.42
 ---- batch: 090 ----
mean loss: 854.91
 ---- batch: 100 ----
mean loss: 853.50
 ---- batch: 110 ----
mean loss: 840.83
train mean loss: 844.05
epoch train time: 0:00:00.555456
elapsed time: 0:00:51.601319
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-27 02:11:03.480364
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.12
 ---- batch: 020 ----
mean loss: 861.64
 ---- batch: 030 ----
mean loss: 838.11
 ---- batch: 040 ----
mean loss: 845.01
 ---- batch: 050 ----
mean loss: 836.21
 ---- batch: 060 ----
mean loss: 835.64
 ---- batch: 070 ----
mean loss: 842.72
 ---- batch: 080 ----
mean loss: 851.18
 ---- batch: 090 ----
mean loss: 833.50
 ---- batch: 100 ----
mean loss: 834.01
 ---- batch: 110 ----
mean loss: 834.96
train mean loss: 844.04
epoch train time: 0:00:00.563865
elapsed time: 0:00:52.165367
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-27 02:11:04.044398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.26
 ---- batch: 020 ----
mean loss: 843.11
 ---- batch: 030 ----
mean loss: 855.51
 ---- batch: 040 ----
mean loss: 842.87
 ---- batch: 050 ----
mean loss: 826.75
 ---- batch: 060 ----
mean loss: 850.21
 ---- batch: 070 ----
mean loss: 835.43
 ---- batch: 080 ----
mean loss: 827.14
 ---- batch: 090 ----
mean loss: 864.25
 ---- batch: 100 ----
mean loss: 852.65
 ---- batch: 110 ----
mean loss: 827.14
train mean loss: 843.95
epoch train time: 0:00:00.552395
elapsed time: 0:00:52.717915
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-27 02:11:04.596970
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.02
 ---- batch: 020 ----
mean loss: 855.72
 ---- batch: 030 ----
mean loss: 830.98
 ---- batch: 040 ----
mean loss: 838.38
 ---- batch: 050 ----
mean loss: 868.38
 ---- batch: 060 ----
mean loss: 833.24
 ---- batch: 070 ----
mean loss: 839.23
 ---- batch: 080 ----
mean loss: 829.76
 ---- batch: 090 ----
mean loss: 826.33
 ---- batch: 100 ----
mean loss: 855.06
 ---- batch: 110 ----
mean loss: 855.24
train mean loss: 843.94
epoch train time: 0:00:00.552333
elapsed time: 0:00:53.270411
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-27 02:11:05.149432
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.59
 ---- batch: 020 ----
mean loss: 836.38
 ---- batch: 030 ----
mean loss: 854.91
 ---- batch: 040 ----
mean loss: 859.66
 ---- batch: 050 ----
mean loss: 817.83
 ---- batch: 060 ----
mean loss: 832.36
 ---- batch: 070 ----
mean loss: 858.04
 ---- batch: 080 ----
mean loss: 854.70
 ---- batch: 090 ----
mean loss: 824.21
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 844.56
train mean loss: 844.09
epoch train time: 0:00:00.553668
elapsed time: 0:00:53.824274
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-27 02:11:05.703302
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.50
 ---- batch: 020 ----
mean loss: 844.67
 ---- batch: 030 ----
mean loss: 842.46
 ---- batch: 040 ----
mean loss: 845.66
 ---- batch: 050 ----
mean loss: 833.50
 ---- batch: 060 ----
mean loss: 835.28
 ---- batch: 070 ----
mean loss: 840.01
 ---- batch: 080 ----
mean loss: 852.60
 ---- batch: 090 ----
mean loss: 845.35
 ---- batch: 100 ----
mean loss: 850.67
 ---- batch: 110 ----
mean loss: 849.94
train mean loss: 844.01
epoch train time: 0:00:00.545975
elapsed time: 0:00:54.370401
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-27 02:11:06.249421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.99
 ---- batch: 020 ----
mean loss: 855.22
 ---- batch: 030 ----
mean loss: 833.32
 ---- batch: 040 ----
mean loss: 858.46
 ---- batch: 050 ----
mean loss: 855.47
 ---- batch: 060 ----
mean loss: 845.19
 ---- batch: 070 ----
mean loss: 832.65
 ---- batch: 080 ----
mean loss: 846.02
 ---- batch: 090 ----
mean loss: 835.69
 ---- batch: 100 ----
mean loss: 840.27
 ---- batch: 110 ----
mean loss: 845.14
train mean loss: 843.93
epoch train time: 0:00:00.567041
elapsed time: 0:00:54.937579
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-27 02:11:06.816617
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.49
 ---- batch: 020 ----
mean loss: 825.58
 ---- batch: 030 ----
mean loss: 839.14
 ---- batch: 040 ----
mean loss: 848.79
 ---- batch: 050 ----
mean loss: 836.38
 ---- batch: 060 ----
mean loss: 850.23
 ---- batch: 070 ----
mean loss: 845.20
 ---- batch: 080 ----
mean loss: 866.50
 ---- batch: 090 ----
mean loss: 853.99
 ---- batch: 100 ----
mean loss: 844.02
 ---- batch: 110 ----
mean loss: 822.70
train mean loss: 843.96
epoch train time: 0:00:00.557465
elapsed time: 0:00:55.495188
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-27 02:11:07.374238
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.68
 ---- batch: 020 ----
mean loss: 835.69
 ---- batch: 030 ----
mean loss: 846.90
 ---- batch: 040 ----
mean loss: 835.58
 ---- batch: 050 ----
mean loss: 868.27
 ---- batch: 060 ----
mean loss: 830.60
 ---- batch: 070 ----
mean loss: 843.45
 ---- batch: 080 ----
mean loss: 856.95
 ---- batch: 090 ----
mean loss: 841.46
 ---- batch: 100 ----
mean loss: 841.99
 ---- batch: 110 ----
mean loss: 840.71
train mean loss: 844.08
epoch train time: 0:00:00.553866
elapsed time: 0:00:56.049208
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-27 02:11:07.928247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.32
 ---- batch: 020 ----
mean loss: 848.50
 ---- batch: 030 ----
mean loss: 852.79
 ---- batch: 040 ----
mean loss: 836.35
 ---- batch: 050 ----
mean loss: 845.94
 ---- batch: 060 ----
mean loss: 841.48
 ---- batch: 070 ----
mean loss: 809.14
 ---- batch: 080 ----
mean loss: 856.46
 ---- batch: 090 ----
mean loss: 845.25
 ---- batch: 100 ----
mean loss: 849.24
 ---- batch: 110 ----
mean loss: 851.07
train mean loss: 844.06
epoch train time: 0:00:00.548679
elapsed time: 0:00:56.598028
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-27 02:11:08.477068
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.24
 ---- batch: 020 ----
mean loss: 835.80
 ---- batch: 030 ----
mean loss: 850.60
 ---- batch: 040 ----
mean loss: 858.14
 ---- batch: 050 ----
mean loss: 833.71
 ---- batch: 060 ----
mean loss: 865.10
 ---- batch: 070 ----
mean loss: 852.62
 ---- batch: 080 ----
mean loss: 833.09
 ---- batch: 090 ----
mean loss: 822.29
 ---- batch: 100 ----
mean loss: 830.74
 ---- batch: 110 ----
mean loss: 843.89
train mean loss: 844.04
epoch train time: 0:00:00.554624
elapsed time: 0:00:57.152801
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-27 02:11:09.031832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.01
 ---- batch: 020 ----
mean loss: 869.89
 ---- batch: 030 ----
mean loss: 857.71
 ---- batch: 040 ----
mean loss: 850.94
 ---- batch: 050 ----
mean loss: 831.70
 ---- batch: 060 ----
mean loss: 842.31
 ---- batch: 070 ----
mean loss: 837.32
 ---- batch: 080 ----
mean loss: 844.26
 ---- batch: 090 ----
mean loss: 840.97
 ---- batch: 100 ----
mean loss: 843.34
 ---- batch: 110 ----
mean loss: 853.45
train mean loss: 843.94
epoch train time: 0:00:00.545554
elapsed time: 0:00:57.698488
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-27 02:11:09.577513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 844.23
 ---- batch: 020 ----
mean loss: 826.17
 ---- batch: 030 ----
mean loss: 857.23
 ---- batch: 040 ----
mean loss: 855.75
 ---- batch: 050 ----
mean loss: 829.65
 ---- batch: 060 ----
mean loss: 825.05
 ---- batch: 070 ----
mean loss: 862.07
 ---- batch: 080 ----
mean loss: 821.71
 ---- batch: 090 ----
mean loss: 868.25
 ---- batch: 100 ----
mean loss: 840.49
 ---- batch: 110 ----
mean loss: 854.33
train mean loss: 844.02
epoch train time: 0:00:00.556806
elapsed time: 0:00:58.255424
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-27 02:11:10.134456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.59
 ---- batch: 020 ----
mean loss: 839.90
 ---- batch: 030 ----
mean loss: 832.06
 ---- batch: 040 ----
mean loss: 833.44
 ---- batch: 050 ----
mean loss: 843.63
 ---- batch: 060 ----
mean loss: 841.47
 ---- batch: 070 ----
mean loss: 865.28
 ---- batch: 080 ----
mean loss: 850.59
 ---- batch: 090 ----
mean loss: 864.03
 ---- batch: 100 ----
mean loss: 829.86
 ---- batch: 110 ----
mean loss: 840.02
train mean loss: 844.02
epoch train time: 0:00:00.548803
elapsed time: 0:00:58.804380
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-27 02:11:10.683425
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 835.87
 ---- batch: 020 ----
mean loss: 843.81
 ---- batch: 030 ----
mean loss: 851.01
 ---- batch: 040 ----
mean loss: 856.66
 ---- batch: 050 ----
mean loss: 832.84
 ---- batch: 060 ----
mean loss: 851.55
 ---- batch: 070 ----
mean loss: 866.04
 ---- batch: 080 ----
mean loss: 833.21
 ---- batch: 090 ----
mean loss: 833.15
 ---- batch: 100 ----
mean loss: 851.12
 ---- batch: 110 ----
mean loss: 824.81
train mean loss: 844.02
epoch train time: 0:00:00.558140
elapsed time: 0:00:59.362692
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-27 02:11:11.241723
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.72
 ---- batch: 020 ----
mean loss: 849.35
 ---- batch: 030 ----
mean loss: 854.58
 ---- batch: 040 ----
mean loss: 833.74
 ---- batch: 050 ----
mean loss: 834.99
 ---- batch: 060 ----
mean loss: 847.39
 ---- batch: 070 ----
mean loss: 828.86
 ---- batch: 080 ----
mean loss: 845.62
 ---- batch: 090 ----
mean loss: 847.91
 ---- batch: 100 ----
mean loss: 839.44
 ---- batch: 110 ----
mean loss: 846.27
train mean loss: 844.00
epoch train time: 0:00:00.556409
elapsed time: 0:00:59.919243
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-27 02:11:11.798319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 839.21
 ---- batch: 020 ----
mean loss: 846.50
 ---- batch: 030 ----
mean loss: 862.86
 ---- batch: 040 ----
mean loss: 852.98
 ---- batch: 050 ----
mean loss: 837.18
 ---- batch: 060 ----
mean loss: 840.36
 ---- batch: 070 ----
mean loss: 860.77
 ---- batch: 080 ----
mean loss: 855.69
 ---- batch: 090 ----
mean loss: 833.00
 ---- batch: 100 ----
mean loss: 817.31
 ---- batch: 110 ----
mean loss: 839.15
train mean loss: 843.95
epoch train time: 0:00:00.558366
elapsed time: 0:01:00.477796
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-27 02:11:12.356824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.07
 ---- batch: 020 ----
mean loss: 842.83
 ---- batch: 030 ----
mean loss: 842.87
 ---- batch: 040 ----
mean loss: 838.17
 ---- batch: 050 ----
mean loss: 844.72
 ---- batch: 060 ----
mean loss: 863.76
 ---- batch: 070 ----
mean loss: 842.70
 ---- batch: 080 ----
mean loss: 808.24
 ---- batch: 090 ----
mean loss: 833.55
 ---- batch: 100 ----
mean loss: 859.01
 ---- batch: 110 ----
mean loss: 849.06
train mean loss: 844.05
epoch train time: 0:00:00.550793
elapsed time: 0:01:01.028725
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-27 02:11:12.907752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.97
 ---- batch: 020 ----
mean loss: 835.15
 ---- batch: 030 ----
mean loss: 837.94
 ---- batch: 040 ----
mean loss: 824.12
 ---- batch: 050 ----
mean loss: 839.35
 ---- batch: 060 ----
mean loss: 857.77
 ---- batch: 070 ----
mean loss: 822.83
 ---- batch: 080 ----
mean loss: 857.42
 ---- batch: 090 ----
mean loss: 835.62
 ---- batch: 100 ----
mean loss: 851.20
 ---- batch: 110 ----
mean loss: 863.65
train mean loss: 844.00
epoch train time: 0:00:00.555400
elapsed time: 0:01:01.584270
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-27 02:11:13.463296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.57
 ---- batch: 020 ----
mean loss: 858.19
 ---- batch: 030 ----
mean loss: 852.89
 ---- batch: 040 ----
mean loss: 839.76
 ---- batch: 050 ----
mean loss: 830.17
 ---- batch: 060 ----
mean loss: 841.44
 ---- batch: 070 ----
mean loss: 855.69
 ---- batch: 080 ----
mean loss: 849.65
 ---- batch: 090 ----
mean loss: 832.65
 ---- batch: 100 ----
mean loss: 836.73
 ---- batch: 110 ----
mean loss: 842.08
train mean loss: 843.93
epoch train time: 0:00:00.559989
elapsed time: 0:01:02.144385
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-27 02:11:14.023412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.38
 ---- batch: 020 ----
mean loss: 853.11
 ---- batch: 030 ----
mean loss: 847.68
 ---- batch: 040 ----
mean loss: 843.46
 ---- batch: 050 ----
mean loss: 859.03
 ---- batch: 060 ----
mean loss: 816.25
 ---- batch: 070 ----
mean loss: 860.16
 ---- batch: 080 ----
mean loss: 813.18
 ---- batch: 090 ----
mean loss: 834.16
 ---- batch: 100 ----
mean loss: 842.73
 ---- batch: 110 ----
mean loss: 857.17
train mean loss: 843.89
epoch train time: 0:00:00.547173
elapsed time: 0:01:02.691688
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-27 02:11:14.570750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.12
 ---- batch: 020 ----
mean loss: 837.08
 ---- batch: 030 ----
mean loss: 843.34
 ---- batch: 040 ----
mean loss: 852.88
 ---- batch: 050 ----
mean loss: 846.10
 ---- batch: 060 ----
mean loss: 837.07
 ---- batch: 070 ----
mean loss: 843.34
 ---- batch: 080 ----
mean loss: 827.01
 ---- batch: 090 ----
mean loss: 848.79
 ---- batch: 100 ----
mean loss: 843.75
 ---- batch: 110 ----
mean loss: 850.03
train mean loss: 843.99
epoch train time: 0:00:00.559424
elapsed time: 0:01:03.251282
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-27 02:11:15.130310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.50
 ---- batch: 020 ----
mean loss: 850.58
 ---- batch: 030 ----
mean loss: 833.68
 ---- batch: 040 ----
mean loss: 829.32
 ---- batch: 050 ----
mean loss: 839.98
 ---- batch: 060 ----
mean loss: 842.15
 ---- batch: 070 ----
mean loss: 844.12
 ---- batch: 080 ----
mean loss: 847.44
 ---- batch: 090 ----
mean loss: 835.43
 ---- batch: 100 ----
mean loss: 836.30
 ---- batch: 110 ----
mean loss: 848.08
train mean loss: 843.99
epoch train time: 0:00:00.556350
elapsed time: 0:01:03.807805
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-27 02:11:15.686828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.15
 ---- batch: 020 ----
mean loss: 851.73
 ---- batch: 030 ----
mean loss: 823.89
 ---- batch: 040 ----
mean loss: 839.99
 ---- batch: 050 ----
mean loss: 834.94
 ---- batch: 060 ----
mean loss: 839.89
 ---- batch: 070 ----
mean loss: 838.58
 ---- batch: 080 ----
mean loss: 813.63
 ---- batch: 090 ----
mean loss: 881.84
 ---- batch: 100 ----
mean loss: 868.32
 ---- batch: 110 ----
mean loss: 850.99
train mean loss: 844.12
epoch train time: 0:00:00.565987
elapsed time: 0:01:04.373949
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-27 02:11:16.252989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.26
 ---- batch: 020 ----
mean loss: 822.48
 ---- batch: 030 ----
mean loss: 845.69
 ---- batch: 040 ----
mean loss: 846.27
 ---- batch: 050 ----
mean loss: 841.19
 ---- batch: 060 ----
mean loss: 837.61
 ---- batch: 070 ----
mean loss: 865.36
 ---- batch: 080 ----
mean loss: 840.56
 ---- batch: 090 ----
mean loss: 837.30
 ---- batch: 100 ----
mean loss: 838.93
 ---- batch: 110 ----
mean loss: 858.55
train mean loss: 844.02
epoch train time: 0:00:00.555729
elapsed time: 0:01:04.929828
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-27 02:11:16.808857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.98
 ---- batch: 020 ----
mean loss: 852.26
 ---- batch: 030 ----
mean loss: 859.63
 ---- batch: 040 ----
mean loss: 834.39
 ---- batch: 050 ----
mean loss: 841.75
 ---- batch: 060 ----
mean loss: 838.75
 ---- batch: 070 ----
mean loss: 856.25
 ---- batch: 080 ----
mean loss: 828.24
 ---- batch: 090 ----
mean loss: 855.48
 ---- batch: 100 ----
mean loss: 836.84
 ---- batch: 110 ----
mean loss: 822.14
train mean loss: 843.98
epoch train time: 0:00:00.561197
elapsed time: 0:01:05.491163
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-27 02:11:17.370189
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.44
 ---- batch: 020 ----
mean loss: 841.00
 ---- batch: 030 ----
mean loss: 855.26
 ---- batch: 040 ----
mean loss: 856.40
 ---- batch: 050 ----
mean loss: 832.74
 ---- batch: 060 ----
mean loss: 812.66
 ---- batch: 070 ----
mean loss: 800.15
 ---- batch: 080 ----
mean loss: 809.77
 ---- batch: 090 ----
mean loss: 768.55
 ---- batch: 100 ----
mean loss: 775.03
 ---- batch: 110 ----
mean loss: 767.85
train mean loss: 814.07
epoch train time: 0:00:00.562205
elapsed time: 0:01:06.053501
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-27 02:11:17.932528
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 738.90
 ---- batch: 020 ----
mean loss: 744.13
 ---- batch: 030 ----
mean loss: 743.00
 ---- batch: 040 ----
mean loss: 730.84
 ---- batch: 050 ----
mean loss: 686.22
 ---- batch: 060 ----
mean loss: 638.65
 ---- batch: 070 ----
mean loss: 531.49
 ---- batch: 080 ----
mean loss: 454.49
 ---- batch: 090 ----
mean loss: 421.26
 ---- batch: 100 ----
mean loss: 393.16
 ---- batch: 110 ----
mean loss: 389.06
train mean loss: 581.76
epoch train time: 0:00:00.559905
elapsed time: 0:01:06.613584
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-27 02:11:18.492626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 375.62
 ---- batch: 020 ----
mean loss: 360.12
 ---- batch: 030 ----
mean loss: 347.57
 ---- batch: 040 ----
mean loss: 353.98
 ---- batch: 050 ----
mean loss: 330.55
 ---- batch: 060 ----
mean loss: 327.35
 ---- batch: 070 ----
mean loss: 305.74
 ---- batch: 080 ----
mean loss: 315.50
 ---- batch: 090 ----
mean loss: 311.99
 ---- batch: 100 ----
mean loss: 303.38
 ---- batch: 110 ----
mean loss: 299.73
train mean loss: 329.06
epoch train time: 0:00:00.572304
elapsed time: 0:01:07.186041
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-27 02:11:19.065070
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.30
 ---- batch: 020 ----
mean loss: 278.23
 ---- batch: 030 ----
mean loss: 271.64
 ---- batch: 040 ----
mean loss: 284.66
 ---- batch: 050 ----
mean loss: 264.37
 ---- batch: 060 ----
mean loss: 265.99
 ---- batch: 070 ----
mean loss: 270.99
 ---- batch: 080 ----
mean loss: 262.28
 ---- batch: 090 ----
mean loss: 267.89
 ---- batch: 100 ----
mean loss: 271.25
 ---- batch: 110 ----
mean loss: 276.37
train mean loss: 272.27
epoch train time: 0:00:00.554269
elapsed time: 0:01:07.740476
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-27 02:11:19.619506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 257.59
 ---- batch: 020 ----
mean loss: 256.89
 ---- batch: 030 ----
mean loss: 250.13
 ---- batch: 040 ----
mean loss: 234.76
 ---- batch: 050 ----
mean loss: 249.74
 ---- batch: 060 ----
mean loss: 251.25
 ---- batch: 070 ----
mean loss: 247.15
 ---- batch: 080 ----
mean loss: 255.19
 ---- batch: 090 ----
mean loss: 259.36
 ---- batch: 100 ----
mean loss: 246.52
 ---- batch: 110 ----
mean loss: 241.14
train mean loss: 249.83
epoch train time: 0:00:00.564367
elapsed time: 0:01:08.304980
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-27 02:11:20.184012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.02
 ---- batch: 020 ----
mean loss: 229.00
 ---- batch: 030 ----
mean loss: 245.37
 ---- batch: 040 ----
mean loss: 243.43
 ---- batch: 050 ----
mean loss: 245.99
 ---- batch: 060 ----
mean loss: 235.17
 ---- batch: 070 ----
mean loss: 244.37
 ---- batch: 080 ----
mean loss: 228.84
 ---- batch: 090 ----
mean loss: 227.60
 ---- batch: 100 ----
mean loss: 226.83
 ---- batch: 110 ----
mean loss: 239.76
train mean loss: 237.40
epoch train time: 0:00:00.557868
elapsed time: 0:01:08.862990
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-27 02:11:20.742019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.43
 ---- batch: 020 ----
mean loss: 225.79
 ---- batch: 030 ----
mean loss: 227.72
 ---- batch: 040 ----
mean loss: 224.70
 ---- batch: 050 ----
mean loss: 216.57
 ---- batch: 060 ----
mean loss: 230.05
 ---- batch: 070 ----
mean loss: 229.91
 ---- batch: 080 ----
mean loss: 232.19
 ---- batch: 090 ----
mean loss: 226.50
 ---- batch: 100 ----
mean loss: 223.67
 ---- batch: 110 ----
mean loss: 218.47
train mean loss: 225.83
epoch train time: 0:00:00.547479
elapsed time: 0:01:09.410604
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-27 02:11:21.289662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.50
 ---- batch: 020 ----
mean loss: 218.34
 ---- batch: 030 ----
mean loss: 207.93
 ---- batch: 040 ----
mean loss: 222.30
 ---- batch: 050 ----
mean loss: 217.92
 ---- batch: 060 ----
mean loss: 216.00
 ---- batch: 070 ----
mean loss: 223.62
 ---- batch: 080 ----
mean loss: 222.88
 ---- batch: 090 ----
mean loss: 226.66
 ---- batch: 100 ----
mean loss: 218.69
 ---- batch: 110 ----
mean loss: 224.98
train mean loss: 220.38
epoch train time: 0:00:00.541311
elapsed time: 0:01:09.952081
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-27 02:11:21.831134
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.18
 ---- batch: 020 ----
mean loss: 214.39
 ---- batch: 030 ----
mean loss: 203.10
 ---- batch: 040 ----
mean loss: 214.15
 ---- batch: 050 ----
mean loss: 219.38
 ---- batch: 060 ----
mean loss: 220.20
 ---- batch: 070 ----
mean loss: 216.02
 ---- batch: 080 ----
mean loss: 212.18
 ---- batch: 090 ----
mean loss: 216.22
 ---- batch: 100 ----
mean loss: 204.13
 ---- batch: 110 ----
mean loss: 217.97
train mean loss: 213.58
epoch train time: 0:00:00.561217
elapsed time: 0:01:10.513467
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-27 02:11:22.392494
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.57
 ---- batch: 020 ----
mean loss: 210.23
 ---- batch: 030 ----
mean loss: 208.77
 ---- batch: 040 ----
mean loss: 201.64
 ---- batch: 050 ----
mean loss: 209.89
 ---- batch: 060 ----
mean loss: 210.88
 ---- batch: 070 ----
mean loss: 202.93
 ---- batch: 080 ----
mean loss: 203.00
 ---- batch: 090 ----
mean loss: 205.24
 ---- batch: 100 ----
mean loss: 202.96
 ---- batch: 110 ----
mean loss: 213.47
train mean loss: 207.60
epoch train time: 0:00:00.568535
elapsed time: 0:01:11.082139
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-27 02:11:22.961166
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.66
 ---- batch: 020 ----
mean loss: 205.95
 ---- batch: 030 ----
mean loss: 213.81
 ---- batch: 040 ----
mean loss: 200.94
 ---- batch: 050 ----
mean loss: 198.77
 ---- batch: 060 ----
mean loss: 197.04
 ---- batch: 070 ----
mean loss: 206.10
 ---- batch: 080 ----
mean loss: 202.68
 ---- batch: 090 ----
mean loss: 207.35
 ---- batch: 100 ----
mean loss: 198.94
 ---- batch: 110 ----
mean loss: 201.52
train mean loss: 202.24
epoch train time: 0:00:00.546193
elapsed time: 0:01:11.628457
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-27 02:11:23.507480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.75
 ---- batch: 020 ----
mean loss: 188.99
 ---- batch: 030 ----
mean loss: 199.51
 ---- batch: 040 ----
mean loss: 197.36
 ---- batch: 050 ----
mean loss: 185.49
 ---- batch: 060 ----
mean loss: 199.73
 ---- batch: 070 ----
mean loss: 203.88
 ---- batch: 080 ----
mean loss: 206.92
 ---- batch: 090 ----
mean loss: 198.76
 ---- batch: 100 ----
mean loss: 200.20
 ---- batch: 110 ----
mean loss: 198.49
train mean loss: 198.18
epoch train time: 0:00:00.558876
elapsed time: 0:01:12.187472
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-27 02:11:24.066504
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.60
 ---- batch: 020 ----
mean loss: 193.40
 ---- batch: 030 ----
mean loss: 194.51
 ---- batch: 040 ----
mean loss: 195.23
 ---- batch: 050 ----
mean loss: 202.09
 ---- batch: 060 ----
mean loss: 192.05
 ---- batch: 070 ----
mean loss: 198.94
 ---- batch: 080 ----
mean loss: 189.82
 ---- batch: 090 ----
mean loss: 193.09
 ---- batch: 100 ----
mean loss: 198.98
 ---- batch: 110 ----
mean loss: 191.09
train mean loss: 194.58
epoch train time: 0:00:00.557005
elapsed time: 0:01:12.744615
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-27 02:11:24.623640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.23
 ---- batch: 020 ----
mean loss: 192.74
 ---- batch: 030 ----
mean loss: 189.13
 ---- batch: 040 ----
mean loss: 185.90
 ---- batch: 050 ----
mean loss: 187.54
 ---- batch: 060 ----
mean loss: 199.35
 ---- batch: 070 ----
mean loss: 189.03
 ---- batch: 080 ----
mean loss: 195.98
 ---- batch: 090 ----
mean loss: 195.57
 ---- batch: 100 ----
mean loss: 193.53
 ---- batch: 110 ----
mean loss: 187.46
train mean loss: 191.92
epoch train time: 0:00:00.562395
elapsed time: 0:01:13.307137
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-27 02:11:25.186160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.88
 ---- batch: 020 ----
mean loss: 190.41
 ---- batch: 030 ----
mean loss: 187.12
 ---- batch: 040 ----
mean loss: 182.06
 ---- batch: 050 ----
mean loss: 189.93
 ---- batch: 060 ----
mean loss: 191.12
 ---- batch: 070 ----
mean loss: 190.71
 ---- batch: 080 ----
mean loss: 193.15
 ---- batch: 090 ----
mean loss: 187.65
 ---- batch: 100 ----
mean loss: 188.00
 ---- batch: 110 ----
mean loss: 189.11
train mean loss: 189.45
epoch train time: 0:00:00.549655
elapsed time: 0:01:13.856922
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-27 02:11:25.735966
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.49
 ---- batch: 020 ----
mean loss: 185.09
 ---- batch: 030 ----
mean loss: 194.94
 ---- batch: 040 ----
mean loss: 191.78
 ---- batch: 050 ----
mean loss: 193.91
 ---- batch: 060 ----
mean loss: 178.00
 ---- batch: 070 ----
mean loss: 183.98
 ---- batch: 080 ----
mean loss: 188.54
 ---- batch: 090 ----
mean loss: 188.56
 ---- batch: 100 ----
mean loss: 181.86
 ---- batch: 110 ----
mean loss: 192.26
train mean loss: 187.42
epoch train time: 0:00:00.554842
elapsed time: 0:01:14.411911
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-27 02:11:26.290943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.84
 ---- batch: 020 ----
mean loss: 176.48
 ---- batch: 030 ----
mean loss: 187.92
 ---- batch: 040 ----
mean loss: 189.75
 ---- batch: 050 ----
mean loss: 190.12
 ---- batch: 060 ----
mean loss: 190.74
 ---- batch: 070 ----
mean loss: 190.95
 ---- batch: 080 ----
mean loss: 184.77
 ---- batch: 090 ----
mean loss: 186.08
 ---- batch: 100 ----
mean loss: 173.53
 ---- batch: 110 ----
mean loss: 190.69
train mean loss: 186.03
epoch train time: 0:00:00.562297
elapsed time: 0:01:14.974345
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-27 02:11:26.853389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.53
 ---- batch: 020 ----
mean loss: 187.24
 ---- batch: 030 ----
mean loss: 176.99
 ---- batch: 040 ----
mean loss: 181.80
 ---- batch: 050 ----
mean loss: 186.59
 ---- batch: 060 ----
mean loss: 186.42
 ---- batch: 070 ----
mean loss: 182.30
 ---- batch: 080 ----
mean loss: 184.52
 ---- batch: 090 ----
mean loss: 185.53
 ---- batch: 100 ----
mean loss: 188.64
 ---- batch: 110 ----
mean loss: 178.84
train mean loss: 184.25
epoch train time: 0:00:00.543452
elapsed time: 0:01:15.517959
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-27 02:11:27.396982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.92
 ---- batch: 020 ----
mean loss: 180.17
 ---- batch: 030 ----
mean loss: 172.21
 ---- batch: 040 ----
mean loss: 184.59
 ---- batch: 050 ----
mean loss: 184.21
 ---- batch: 060 ----
mean loss: 191.86
 ---- batch: 070 ----
mean loss: 180.98
 ---- batch: 080 ----
mean loss: 187.61
 ---- batch: 090 ----
mean loss: 182.29
 ---- batch: 100 ----
mean loss: 179.42
 ---- batch: 110 ----
mean loss: 181.72
train mean loss: 181.93
epoch train time: 0:00:00.552724
elapsed time: 0:01:16.070815
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-27 02:11:27.949849
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.46
 ---- batch: 020 ----
mean loss: 181.21
 ---- batch: 030 ----
mean loss: 174.13
 ---- batch: 040 ----
mean loss: 185.97
 ---- batch: 050 ----
mean loss: 182.50
 ---- batch: 060 ----
mean loss: 182.90
 ---- batch: 070 ----
mean loss: 180.40
 ---- batch: 080 ----
mean loss: 179.12
 ---- batch: 090 ----
mean loss: 184.68
 ---- batch: 100 ----
mean loss: 173.79
 ---- batch: 110 ----
mean loss: 197.32
train mean loss: 181.14
epoch train time: 0:00:00.545215
elapsed time: 0:01:16.616183
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-27 02:11:28.495213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.67
 ---- batch: 020 ----
mean loss: 182.99
 ---- batch: 030 ----
mean loss: 177.11
 ---- batch: 040 ----
mean loss: 178.44
 ---- batch: 050 ----
mean loss: 179.54
 ---- batch: 060 ----
mean loss: 183.18
 ---- batch: 070 ----
mean loss: 186.18
 ---- batch: 080 ----
mean loss: 176.97
 ---- batch: 090 ----
mean loss: 184.67
 ---- batch: 100 ----
mean loss: 184.05
 ---- batch: 110 ----
mean loss: 177.63
train mean loss: 180.46
epoch train time: 0:00:00.558719
elapsed time: 0:01:17.175039
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-27 02:11:29.054083
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.80
 ---- batch: 020 ----
mean loss: 176.44
 ---- batch: 030 ----
mean loss: 177.34
 ---- batch: 040 ----
mean loss: 176.89
 ---- batch: 050 ----
mean loss: 181.27
 ---- batch: 060 ----
mean loss: 173.24
 ---- batch: 070 ----
mean loss: 177.73
 ---- batch: 080 ----
mean loss: 193.51
 ---- batch: 090 ----
mean loss: 179.27
 ---- batch: 100 ----
mean loss: 169.91
 ---- batch: 110 ----
mean loss: 187.55
train mean loss: 178.90
epoch train time: 0:00:00.546826
elapsed time: 0:01:17.722010
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-27 02:11:29.601035
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.77
 ---- batch: 020 ----
mean loss: 181.71
 ---- batch: 030 ----
mean loss: 181.78
 ---- batch: 040 ----
mean loss: 183.93
 ---- batch: 050 ----
mean loss: 172.73
 ---- batch: 060 ----
mean loss: 181.86
 ---- batch: 070 ----
mean loss: 187.99
 ---- batch: 080 ----
mean loss: 178.90
 ---- batch: 090 ----
mean loss: 179.60
 ---- batch: 100 ----
mean loss: 175.32
 ---- batch: 110 ----
mean loss: 179.96
train mean loss: 179.51
epoch train time: 0:00:00.561940
elapsed time: 0:01:18.284128
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-27 02:11:30.163154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.00
 ---- batch: 020 ----
mean loss: 175.86
 ---- batch: 030 ----
mean loss: 177.43
 ---- batch: 040 ----
mean loss: 171.91
 ---- batch: 050 ----
mean loss: 179.08
 ---- batch: 060 ----
mean loss: 176.19
 ---- batch: 070 ----
mean loss: 173.80
 ---- batch: 080 ----
mean loss: 177.80
 ---- batch: 090 ----
mean loss: 177.42
 ---- batch: 100 ----
mean loss: 177.73
 ---- batch: 110 ----
mean loss: 180.54
train mean loss: 177.18
epoch train time: 0:00:00.556312
elapsed time: 0:01:18.840587
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-27 02:11:30.719658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.55
 ---- batch: 020 ----
mean loss: 180.41
 ---- batch: 030 ----
mean loss: 174.30
 ---- batch: 040 ----
mean loss: 169.86
 ---- batch: 050 ----
mean loss: 169.30
 ---- batch: 060 ----
mean loss: 179.37
 ---- batch: 070 ----
mean loss: 185.45
 ---- batch: 080 ----
mean loss: 185.16
 ---- batch: 090 ----
mean loss: 173.95
 ---- batch: 100 ----
mean loss: 183.48
 ---- batch: 110 ----
mean loss: 174.21
train mean loss: 177.38
epoch train time: 0:00:00.551568
elapsed time: 0:01:19.392332
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-27 02:11:31.271359
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.81
 ---- batch: 020 ----
mean loss: 181.38
 ---- batch: 030 ----
mean loss: 162.86
 ---- batch: 040 ----
mean loss: 178.54
 ---- batch: 050 ----
mean loss: 176.97
 ---- batch: 060 ----
mean loss: 174.98
 ---- batch: 070 ----
mean loss: 173.91
 ---- batch: 080 ----
mean loss: 182.60
 ---- batch: 090 ----
mean loss: 179.81
 ---- batch: 100 ----
mean loss: 182.51
 ---- batch: 110 ----
mean loss: 179.74
train mean loss: 176.25
epoch train time: 0:00:00.547549
elapsed time: 0:01:19.940016
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-27 02:11:31.819044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.51
 ---- batch: 020 ----
mean loss: 182.15
 ---- batch: 030 ----
mean loss: 167.78
 ---- batch: 040 ----
mean loss: 170.12
 ---- batch: 050 ----
mean loss: 180.44
 ---- batch: 060 ----
mean loss: 173.57
 ---- batch: 070 ----
mean loss: 173.91
 ---- batch: 080 ----
mean loss: 171.62
 ---- batch: 090 ----
mean loss: 174.60
 ---- batch: 100 ----
mean loss: 178.50
 ---- batch: 110 ----
mean loss: 184.17
train mean loss: 174.77
epoch train time: 0:00:00.548749
elapsed time: 0:01:20.488897
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-27 02:11:32.367936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.87
 ---- batch: 020 ----
mean loss: 179.84
 ---- batch: 030 ----
mean loss: 173.09
 ---- batch: 040 ----
mean loss: 163.90
 ---- batch: 050 ----
mean loss: 173.37
 ---- batch: 060 ----
mean loss: 172.73
 ---- batch: 070 ----
mean loss: 174.48
 ---- batch: 080 ----
mean loss: 182.24
 ---- batch: 090 ----
mean loss: 176.81
 ---- batch: 100 ----
mean loss: 169.34
 ---- batch: 110 ----
mean loss: 180.68
train mean loss: 174.36
epoch train time: 0:00:00.560377
elapsed time: 0:01:21.049415
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-27 02:11:32.928469
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.98
 ---- batch: 020 ----
mean loss: 166.41
 ---- batch: 030 ----
mean loss: 177.09
 ---- batch: 040 ----
mean loss: 173.13
 ---- batch: 050 ----
mean loss: 175.68
 ---- batch: 060 ----
mean loss: 169.98
 ---- batch: 070 ----
mean loss: 182.09
 ---- batch: 080 ----
mean loss: 174.87
 ---- batch: 090 ----
mean loss: 175.04
 ---- batch: 100 ----
mean loss: 179.61
 ---- batch: 110 ----
mean loss: 170.55
train mean loss: 173.16
epoch train time: 0:00:00.547100
elapsed time: 0:01:21.596706
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-27 02:11:33.475730
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.40
 ---- batch: 020 ----
mean loss: 171.43
 ---- batch: 030 ----
mean loss: 173.84
 ---- batch: 040 ----
mean loss: 176.20
 ---- batch: 050 ----
mean loss: 168.09
 ---- batch: 060 ----
mean loss: 168.53
 ---- batch: 070 ----
mean loss: 172.73
 ---- batch: 080 ----
mean loss: 177.54
 ---- batch: 090 ----
mean loss: 172.29
 ---- batch: 100 ----
mean loss: 173.38
 ---- batch: 110 ----
mean loss: 173.73
train mean loss: 172.40
epoch train time: 0:00:00.554830
elapsed time: 0:01:22.151667
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-27 02:11:34.030692
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.87
 ---- batch: 020 ----
mean loss: 166.60
 ---- batch: 030 ----
mean loss: 163.91
 ---- batch: 040 ----
mean loss: 170.44
 ---- batch: 050 ----
mean loss: 165.65
 ---- batch: 060 ----
mean loss: 179.92
 ---- batch: 070 ----
mean loss: 183.45
 ---- batch: 080 ----
mean loss: 175.50
 ---- batch: 090 ----
mean loss: 166.68
 ---- batch: 100 ----
mean loss: 173.64
 ---- batch: 110 ----
mean loss: 176.17
train mean loss: 172.30
epoch train time: 0:00:00.548649
elapsed time: 0:01:22.700443
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-27 02:11:34.579525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.82
 ---- batch: 020 ----
mean loss: 171.54
 ---- batch: 030 ----
mean loss: 174.14
 ---- batch: 040 ----
mean loss: 163.59
 ---- batch: 050 ----
mean loss: 170.71
 ---- batch: 060 ----
mean loss: 176.56
 ---- batch: 070 ----
mean loss: 170.71
 ---- batch: 080 ----
mean loss: 169.06
 ---- batch: 090 ----
mean loss: 174.51
 ---- batch: 100 ----
mean loss: 172.62
 ---- batch: 110 ----
mean loss: 177.64
train mean loss: 171.57
epoch train time: 0:00:00.551733
elapsed time: 0:01:23.252362
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-27 02:11:35.131387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.64
 ---- batch: 020 ----
mean loss: 175.53
 ---- batch: 030 ----
mean loss: 166.13
 ---- batch: 040 ----
mean loss: 174.63
 ---- batch: 050 ----
mean loss: 162.48
 ---- batch: 060 ----
mean loss: 172.18
 ---- batch: 070 ----
mean loss: 177.69
 ---- batch: 080 ----
mean loss: 178.50
 ---- batch: 090 ----
mean loss: 168.05
 ---- batch: 100 ----
mean loss: 170.34
 ---- batch: 110 ----
mean loss: 173.10
train mean loss: 171.52
epoch train time: 0:00:00.552314
elapsed time: 0:01:23.804804
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-27 02:11:35.683830
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.45
 ---- batch: 020 ----
mean loss: 175.27
 ---- batch: 030 ----
mean loss: 167.79
 ---- batch: 040 ----
mean loss: 174.38
 ---- batch: 050 ----
mean loss: 171.16
 ---- batch: 060 ----
mean loss: 157.76
 ---- batch: 070 ----
mean loss: 172.87
 ---- batch: 080 ----
mean loss: 161.79
 ---- batch: 090 ----
mean loss: 176.13
 ---- batch: 100 ----
mean loss: 170.91
 ---- batch: 110 ----
mean loss: 172.81
train mean loss: 170.08
epoch train time: 0:00:00.575566
elapsed time: 0:01:24.380502
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-27 02:11:36.259529
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.72
 ---- batch: 020 ----
mean loss: 174.65
 ---- batch: 030 ----
mean loss: 171.07
 ---- batch: 040 ----
mean loss: 164.38
 ---- batch: 050 ----
mean loss: 167.31
 ---- batch: 060 ----
mean loss: 173.86
 ---- batch: 070 ----
mean loss: 175.39
 ---- batch: 080 ----
mean loss: 178.08
 ---- batch: 090 ----
mean loss: 165.95
 ---- batch: 100 ----
mean loss: 170.81
 ---- batch: 110 ----
mean loss: 167.72
train mean loss: 170.34
epoch train time: 0:00:00.562895
elapsed time: 0:01:24.943543
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-27 02:11:36.822585
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.57
 ---- batch: 020 ----
mean loss: 168.78
 ---- batch: 030 ----
mean loss: 173.95
 ---- batch: 040 ----
mean loss: 165.41
 ---- batch: 050 ----
mean loss: 171.85
 ---- batch: 060 ----
mean loss: 170.17
 ---- batch: 070 ----
mean loss: 166.40
 ---- batch: 080 ----
mean loss: 171.32
 ---- batch: 090 ----
mean loss: 166.39
 ---- batch: 100 ----
mean loss: 169.90
 ---- batch: 110 ----
mean loss: 170.42
train mean loss: 169.44
epoch train time: 0:00:00.554813
elapsed time: 0:01:25.498534
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-27 02:11:37.377561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.76
 ---- batch: 020 ----
mean loss: 170.42
 ---- batch: 030 ----
mean loss: 166.57
 ---- batch: 040 ----
mean loss: 174.86
 ---- batch: 050 ----
mean loss: 174.60
 ---- batch: 060 ----
mean loss: 165.07
 ---- batch: 070 ----
mean loss: 163.59
 ---- batch: 080 ----
mean loss: 163.71
 ---- batch: 090 ----
mean loss: 171.25
 ---- batch: 100 ----
mean loss: 169.26
 ---- batch: 110 ----
mean loss: 170.25
train mean loss: 169.55
epoch train time: 0:00:00.564336
elapsed time: 0:01:26.063009
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-27 02:11:37.942036
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.52
 ---- batch: 020 ----
mean loss: 158.14
 ---- batch: 030 ----
mean loss: 153.86
 ---- batch: 040 ----
mean loss: 173.47
 ---- batch: 050 ----
mean loss: 178.57
 ---- batch: 060 ----
mean loss: 171.03
 ---- batch: 070 ----
mean loss: 171.73
 ---- batch: 080 ----
mean loss: 171.72
 ---- batch: 090 ----
mean loss: 164.50
 ---- batch: 100 ----
mean loss: 173.49
 ---- batch: 110 ----
mean loss: 171.09
train mean loss: 168.90
epoch train time: 0:00:00.551790
elapsed time: 0:01:26.614948
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-27 02:11:38.493990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.04
 ---- batch: 020 ----
mean loss: 163.61
 ---- batch: 030 ----
mean loss: 163.23
 ---- batch: 040 ----
mean loss: 170.28
 ---- batch: 050 ----
mean loss: 177.70
 ---- batch: 060 ----
mean loss: 162.41
 ---- batch: 070 ----
mean loss: 163.61
 ---- batch: 080 ----
mean loss: 176.47
 ---- batch: 090 ----
mean loss: 174.56
 ---- batch: 100 ----
mean loss: 172.73
 ---- batch: 110 ----
mean loss: 167.85
train mean loss: 168.37
epoch train time: 0:00:00.552236
elapsed time: 0:01:27.167335
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-27 02:11:39.046363
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.26
 ---- batch: 020 ----
mean loss: 165.91
 ---- batch: 030 ----
mean loss: 166.10
 ---- batch: 040 ----
mean loss: 161.62
 ---- batch: 050 ----
mean loss: 169.46
 ---- batch: 060 ----
mean loss: 170.47
 ---- batch: 070 ----
mean loss: 168.84
 ---- batch: 080 ----
mean loss: 174.68
 ---- batch: 090 ----
mean loss: 169.92
 ---- batch: 100 ----
mean loss: 169.29
 ---- batch: 110 ----
mean loss: 164.90
train mean loss: 167.21
epoch train time: 0:00:00.556897
elapsed time: 0:01:27.724366
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-27 02:11:39.603396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.39
 ---- batch: 020 ----
mean loss: 171.51
 ---- batch: 030 ----
mean loss: 158.11
 ---- batch: 040 ----
mean loss: 170.66
 ---- batch: 050 ----
mean loss: 162.74
 ---- batch: 060 ----
mean loss: 166.99
 ---- batch: 070 ----
mean loss: 161.54
 ---- batch: 080 ----
mean loss: 158.32
 ---- batch: 090 ----
mean loss: 166.24
 ---- batch: 100 ----
mean loss: 174.42
 ---- batch: 110 ----
mean loss: 176.46
train mean loss: 166.99
epoch train time: 0:00:00.548976
elapsed time: 0:01:28.273499
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-27 02:11:40.152559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.72
 ---- batch: 020 ----
mean loss: 161.80
 ---- batch: 030 ----
mean loss: 172.07
 ---- batch: 040 ----
mean loss: 158.54
 ---- batch: 050 ----
mean loss: 165.25
 ---- batch: 060 ----
mean loss: 176.27
 ---- batch: 070 ----
mean loss: 165.44
 ---- batch: 080 ----
mean loss: 166.06
 ---- batch: 090 ----
mean loss: 164.36
 ---- batch: 100 ----
mean loss: 171.68
 ---- batch: 110 ----
mean loss: 167.19
train mean loss: 166.29
epoch train time: 0:00:00.553877
elapsed time: 0:01:28.827557
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-27 02:11:40.706629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.46
 ---- batch: 020 ----
mean loss: 166.30
 ---- batch: 030 ----
mean loss: 165.79
 ---- batch: 040 ----
mean loss: 162.76
 ---- batch: 050 ----
mean loss: 171.24
 ---- batch: 060 ----
mean loss: 159.23
 ---- batch: 070 ----
mean loss: 165.11
 ---- batch: 080 ----
mean loss: 171.68
 ---- batch: 090 ----
mean loss: 164.41
 ---- batch: 100 ----
mean loss: 158.19
 ---- batch: 110 ----
mean loss: 169.30
train mean loss: 165.89
epoch train time: 0:00:00.560021
elapsed time: 0:01:29.387760
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-27 02:11:41.266787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.54
 ---- batch: 020 ----
mean loss: 166.77
 ---- batch: 030 ----
mean loss: 164.09
 ---- batch: 040 ----
mean loss: 165.69
 ---- batch: 050 ----
mean loss: 164.74
 ---- batch: 060 ----
mean loss: 166.48
 ---- batch: 070 ----
mean loss: 164.55
 ---- batch: 080 ----
mean loss: 173.74
 ---- batch: 090 ----
mean loss: 165.21
 ---- batch: 100 ----
mean loss: 167.56
 ---- batch: 110 ----
mean loss: 163.35
train mean loss: 166.37
epoch train time: 0:00:00.562620
elapsed time: 0:01:29.950549
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-27 02:11:41.829588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.68
 ---- batch: 020 ----
mean loss: 162.35
 ---- batch: 030 ----
mean loss: 163.09
 ---- batch: 040 ----
mean loss: 167.58
 ---- batch: 050 ----
mean loss: 158.91
 ---- batch: 060 ----
mean loss: 167.86
 ---- batch: 070 ----
mean loss: 161.25
 ---- batch: 080 ----
mean loss: 163.32
 ---- batch: 090 ----
mean loss: 161.93
 ---- batch: 100 ----
mean loss: 169.28
 ---- batch: 110 ----
mean loss: 173.12
train mean loss: 165.52
epoch train time: 0:00:00.544732
elapsed time: 0:01:30.495439
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-27 02:11:42.374486
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.60
 ---- batch: 020 ----
mean loss: 161.90
 ---- batch: 030 ----
mean loss: 166.94
 ---- batch: 040 ----
mean loss: 164.06
 ---- batch: 050 ----
mean loss: 163.37
 ---- batch: 060 ----
mean loss: 164.99
 ---- batch: 070 ----
mean loss: 163.29
 ---- batch: 080 ----
mean loss: 161.95
 ---- batch: 090 ----
mean loss: 166.29
 ---- batch: 100 ----
mean loss: 171.33
 ---- batch: 110 ----
mean loss: 176.42
train mean loss: 164.89
epoch train time: 0:00:00.553035
elapsed time: 0:01:31.048625
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-27 02:11:42.927650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.16
 ---- batch: 020 ----
mean loss: 162.53
 ---- batch: 030 ----
mean loss: 161.42
 ---- batch: 040 ----
mean loss: 159.10
 ---- batch: 050 ----
mean loss: 156.02
 ---- batch: 060 ----
mean loss: 165.75
 ---- batch: 070 ----
mean loss: 165.90
 ---- batch: 080 ----
mean loss: 162.03
 ---- batch: 090 ----
mean loss: 167.47
 ---- batch: 100 ----
mean loss: 168.75
 ---- batch: 110 ----
mean loss: 165.86
train mean loss: 164.26
epoch train time: 0:00:00.553598
elapsed time: 0:01:31.602379
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-27 02:11:43.481396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.17
 ---- batch: 020 ----
mean loss: 168.06
 ---- batch: 030 ----
mean loss: 161.43
 ---- batch: 040 ----
mean loss: 159.53
 ---- batch: 050 ----
mean loss: 165.81
 ---- batch: 060 ----
mean loss: 164.75
 ---- batch: 070 ----
mean loss: 159.77
 ---- batch: 080 ----
mean loss: 166.52
 ---- batch: 090 ----
mean loss: 173.85
 ---- batch: 100 ----
mean loss: 158.19
 ---- batch: 110 ----
mean loss: 166.35
train mean loss: 163.87
epoch train time: 0:00:00.551443
elapsed time: 0:01:32.153945
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-27 02:11:44.032996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.96
 ---- batch: 020 ----
mean loss: 168.63
 ---- batch: 030 ----
mean loss: 161.42
 ---- batch: 040 ----
mean loss: 158.95
 ---- batch: 050 ----
mean loss: 167.60
 ---- batch: 060 ----
mean loss: 160.47
 ---- batch: 070 ----
mean loss: 172.04
 ---- batch: 080 ----
mean loss: 167.97
 ---- batch: 090 ----
mean loss: 166.41
 ---- batch: 100 ----
mean loss: 157.15
 ---- batch: 110 ----
mean loss: 165.75
train mean loss: 164.17
epoch train time: 0:00:00.548362
elapsed time: 0:01:32.702474
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-27 02:11:44.581555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.60
 ---- batch: 020 ----
mean loss: 165.89
 ---- batch: 030 ----
mean loss: 153.24
 ---- batch: 040 ----
mean loss: 158.17
 ---- batch: 050 ----
mean loss: 164.08
 ---- batch: 060 ----
mean loss: 158.66
 ---- batch: 070 ----
mean loss: 171.20
 ---- batch: 080 ----
mean loss: 164.89
 ---- batch: 090 ----
mean loss: 166.93
 ---- batch: 100 ----
mean loss: 166.82
 ---- batch: 110 ----
mean loss: 161.87
train mean loss: 163.00
epoch train time: 0:00:00.557314
elapsed time: 0:01:33.259976
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-27 02:11:45.139002
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.05
 ---- batch: 020 ----
mean loss: 155.16
 ---- batch: 030 ----
mean loss: 163.34
 ---- batch: 040 ----
mean loss: 157.86
 ---- batch: 050 ----
mean loss: 165.65
 ---- batch: 060 ----
mean loss: 158.88
 ---- batch: 070 ----
mean loss: 171.92
 ---- batch: 080 ----
mean loss: 166.96
 ---- batch: 090 ----
mean loss: 158.64
 ---- batch: 100 ----
mean loss: 161.00
 ---- batch: 110 ----
mean loss: 168.01
train mean loss: 162.23
epoch train time: 0:00:00.542968
elapsed time: 0:01:33.803112
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-27 02:11:45.682137
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.04
 ---- batch: 020 ----
mean loss: 161.62
 ---- batch: 030 ----
mean loss: 159.22
 ---- batch: 040 ----
mean loss: 153.32
 ---- batch: 050 ----
mean loss: 165.57
 ---- batch: 060 ----
mean loss: 162.39
 ---- batch: 070 ----
mean loss: 163.49
 ---- batch: 080 ----
mean loss: 166.06
 ---- batch: 090 ----
mean loss: 158.64
 ---- batch: 100 ----
mean loss: 160.40
 ---- batch: 110 ----
mean loss: 167.35
train mean loss: 161.90
epoch train time: 0:00:00.561901
elapsed time: 0:01:34.365160
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-27 02:11:46.244188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.78
 ---- batch: 020 ----
mean loss: 158.69
 ---- batch: 030 ----
mean loss: 158.54
 ---- batch: 040 ----
mean loss: 162.31
 ---- batch: 050 ----
mean loss: 158.01
 ---- batch: 060 ----
mean loss: 162.84
 ---- batch: 070 ----
mean loss: 163.32
 ---- batch: 080 ----
mean loss: 169.42
 ---- batch: 090 ----
mean loss: 170.62
 ---- batch: 100 ----
mean loss: 153.58
 ---- batch: 110 ----
mean loss: 167.53
train mean loss: 162.55
epoch train time: 0:00:00.548413
elapsed time: 0:01:34.913707
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-27 02:11:46.792735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.40
 ---- batch: 020 ----
mean loss: 152.08
 ---- batch: 030 ----
mean loss: 160.27
 ---- batch: 040 ----
mean loss: 151.12
 ---- batch: 050 ----
mean loss: 164.08
 ---- batch: 060 ----
mean loss: 159.88
 ---- batch: 070 ----
mean loss: 154.33
 ---- batch: 080 ----
mean loss: 169.51
 ---- batch: 090 ----
mean loss: 165.13
 ---- batch: 100 ----
mean loss: 162.39
 ---- batch: 110 ----
mean loss: 170.42
train mean loss: 161.44
epoch train time: 0:00:00.553418
elapsed time: 0:01:35.467340
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-27 02:11:47.346485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.46
 ---- batch: 020 ----
mean loss: 165.75
 ---- batch: 030 ----
mean loss: 155.23
 ---- batch: 040 ----
mean loss: 161.31
 ---- batch: 050 ----
mean loss: 165.07
 ---- batch: 060 ----
mean loss: 172.09
 ---- batch: 070 ----
mean loss: 160.40
 ---- batch: 080 ----
mean loss: 152.95
 ---- batch: 090 ----
mean loss: 161.44
 ---- batch: 100 ----
mean loss: 162.71
 ---- batch: 110 ----
mean loss: 160.67
train mean loss: 161.52
epoch train time: 0:00:00.553145
elapsed time: 0:01:36.020752
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-27 02:11:47.899777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.46
 ---- batch: 020 ----
mean loss: 164.20
 ---- batch: 030 ----
mean loss: 160.61
 ---- batch: 040 ----
mean loss: 156.36
 ---- batch: 050 ----
mean loss: 163.16
 ---- batch: 060 ----
mean loss: 158.08
 ---- batch: 070 ----
mean loss: 162.76
 ---- batch: 080 ----
mean loss: 165.18
 ---- batch: 090 ----
mean loss: 160.74
 ---- batch: 100 ----
mean loss: 153.63
 ---- batch: 110 ----
mean loss: 152.82
train mean loss: 160.22
epoch train time: 0:00:00.550500
elapsed time: 0:01:36.571382
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-27 02:11:48.450424
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.52
 ---- batch: 020 ----
mean loss: 164.32
 ---- batch: 030 ----
mean loss: 158.45
 ---- batch: 040 ----
mean loss: 161.60
 ---- batch: 050 ----
mean loss: 160.45
 ---- batch: 060 ----
mean loss: 161.27
 ---- batch: 070 ----
mean loss: 168.30
 ---- batch: 080 ----
mean loss: 167.65
 ---- batch: 090 ----
mean loss: 156.32
 ---- batch: 100 ----
mean loss: 164.03
 ---- batch: 110 ----
mean loss: 159.01
train mean loss: 161.29
epoch train time: 0:00:00.563443
elapsed time: 0:01:37.134973
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-27 02:11:49.013999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.79
 ---- batch: 020 ----
mean loss: 160.58
 ---- batch: 030 ----
mean loss: 159.61
 ---- batch: 040 ----
mean loss: 156.16
 ---- batch: 050 ----
mean loss: 156.96
 ---- batch: 060 ----
mean loss: 162.32
 ---- batch: 070 ----
mean loss: 160.57
 ---- batch: 080 ----
mean loss: 162.12
 ---- batch: 090 ----
mean loss: 158.19
 ---- batch: 100 ----
mean loss: 160.91
 ---- batch: 110 ----
mean loss: 156.06
train mean loss: 160.04
epoch train time: 0:00:00.557623
elapsed time: 0:01:37.692724
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-27 02:11:49.571753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.26
 ---- batch: 020 ----
mean loss: 155.95
 ---- batch: 030 ----
mean loss: 156.62
 ---- batch: 040 ----
mean loss: 157.21
 ---- batch: 050 ----
mean loss: 161.06
 ---- batch: 060 ----
mean loss: 159.97
 ---- batch: 070 ----
mean loss: 159.27
 ---- batch: 080 ----
mean loss: 165.47
 ---- batch: 090 ----
mean loss: 161.55
 ---- batch: 100 ----
mean loss: 161.10
 ---- batch: 110 ----
mean loss: 164.79
train mean loss: 159.77
epoch train time: 0:00:00.554487
elapsed time: 0:01:38.247361
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-27 02:11:50.126384
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.03
 ---- batch: 020 ----
mean loss: 154.77
 ---- batch: 030 ----
mean loss: 153.70
 ---- batch: 040 ----
mean loss: 157.77
 ---- batch: 050 ----
mean loss: 165.03
 ---- batch: 060 ----
mean loss: 164.74
 ---- batch: 070 ----
mean loss: 158.81
 ---- batch: 080 ----
mean loss: 159.98
 ---- batch: 090 ----
mean loss: 164.42
 ---- batch: 100 ----
mean loss: 160.49
 ---- batch: 110 ----
mean loss: 162.93
train mean loss: 160.04
epoch train time: 0:00:00.558124
elapsed time: 0:01:38.805613
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-27 02:11:50.684640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.02
 ---- batch: 020 ----
mean loss: 146.49
 ---- batch: 030 ----
mean loss: 167.56
 ---- batch: 040 ----
mean loss: 159.54
 ---- batch: 050 ----
mean loss: 157.35
 ---- batch: 060 ----
mean loss: 155.66
 ---- batch: 070 ----
mean loss: 163.55
 ---- batch: 080 ----
mean loss: 154.34
 ---- batch: 090 ----
mean loss: 167.43
 ---- batch: 100 ----
mean loss: 157.92
 ---- batch: 110 ----
mean loss: 159.42
train mean loss: 158.79
epoch train time: 0:00:00.548308
elapsed time: 0:01:39.354084
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-27 02:11:51.233192
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.93
 ---- batch: 020 ----
mean loss: 153.20
 ---- batch: 030 ----
mean loss: 158.88
 ---- batch: 040 ----
mean loss: 152.76
 ---- batch: 050 ----
mean loss: 149.22
 ---- batch: 060 ----
mean loss: 155.62
 ---- batch: 070 ----
mean loss: 161.87
 ---- batch: 080 ----
mean loss: 161.36
 ---- batch: 090 ----
mean loss: 159.80
 ---- batch: 100 ----
mean loss: 169.16
 ---- batch: 110 ----
mean loss: 162.16
train mean loss: 157.94
epoch train time: 0:00:00.563732
elapsed time: 0:01:39.918088
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-27 02:11:51.797140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.80
 ---- batch: 020 ----
mean loss: 165.02
 ---- batch: 030 ----
mean loss: 154.86
 ---- batch: 040 ----
mean loss: 158.24
 ---- batch: 050 ----
mean loss: 166.08
 ---- batch: 060 ----
mean loss: 161.69
 ---- batch: 070 ----
mean loss: 166.01
 ---- batch: 080 ----
mean loss: 163.82
 ---- batch: 090 ----
mean loss: 156.94
 ---- batch: 100 ----
mean loss: 163.64
 ---- batch: 110 ----
mean loss: 149.98
train mean loss: 160.30
epoch train time: 0:00:00.562371
elapsed time: 0:01:40.480614
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-27 02:11:52.359640
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.27
 ---- batch: 020 ----
mean loss: 150.64
 ---- batch: 030 ----
mean loss: 154.13
 ---- batch: 040 ----
mean loss: 160.63
 ---- batch: 050 ----
mean loss: 154.83
 ---- batch: 060 ----
mean loss: 157.90
 ---- batch: 070 ----
mean loss: 159.59
 ---- batch: 080 ----
mean loss: 163.86
 ---- batch: 090 ----
mean loss: 166.64
 ---- batch: 100 ----
mean loss: 162.66
 ---- batch: 110 ----
mean loss: 151.62
train mean loss: 157.87
epoch train time: 0:00:00.556038
elapsed time: 0:01:41.036783
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-27 02:11:52.915808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.82
 ---- batch: 020 ----
mean loss: 146.23
 ---- batch: 030 ----
mean loss: 158.80
 ---- batch: 040 ----
mean loss: 155.74
 ---- batch: 050 ----
mean loss: 164.58
 ---- batch: 060 ----
mean loss: 161.20
 ---- batch: 070 ----
mean loss: 168.10
 ---- batch: 080 ----
mean loss: 153.67
 ---- batch: 090 ----
mean loss: 157.82
 ---- batch: 100 ----
mean loss: 157.75
 ---- batch: 110 ----
mean loss: 154.10
train mean loss: 157.58
epoch train time: 0:00:00.547898
elapsed time: 0:01:41.584811
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-27 02:11:53.463837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.48
 ---- batch: 020 ----
mean loss: 147.48
 ---- batch: 030 ----
mean loss: 160.45
 ---- batch: 040 ----
mean loss: 161.29
 ---- batch: 050 ----
mean loss: 152.74
 ---- batch: 060 ----
mean loss: 160.44
 ---- batch: 070 ----
mean loss: 164.69
 ---- batch: 080 ----
mean loss: 159.32
 ---- batch: 090 ----
mean loss: 153.49
 ---- batch: 100 ----
mean loss: 158.47
 ---- batch: 110 ----
mean loss: 162.27
train mean loss: 157.74
epoch train time: 0:00:00.559966
elapsed time: 0:01:42.144911
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-27 02:11:54.023949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.18
 ---- batch: 020 ----
mean loss: 151.05
 ---- batch: 030 ----
mean loss: 155.40
 ---- batch: 040 ----
mean loss: 160.50
 ---- batch: 050 ----
mean loss: 157.73
 ---- batch: 060 ----
mean loss: 155.77
 ---- batch: 070 ----
mean loss: 155.94
 ---- batch: 080 ----
mean loss: 161.71
 ---- batch: 090 ----
mean loss: 161.85
 ---- batch: 100 ----
mean loss: 157.87
 ---- batch: 110 ----
mean loss: 156.28
train mean loss: 157.30
epoch train time: 0:00:00.554638
elapsed time: 0:01:42.699708
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-27 02:11:54.578746
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 155.64
 ---- batch: 020 ----
mean loss: 154.83
 ---- batch: 030 ----
mean loss: 157.65
 ---- batch: 040 ----
mean loss: 142.85
 ---- batch: 050 ----
mean loss: 169.35
 ---- batch: 060 ----
mean loss: 158.80
 ---- batch: 070 ----
mean loss: 159.95
 ---- batch: 080 ----
mean loss: 156.18
 ---- batch: 090 ----
mean loss: 161.27
 ---- batch: 100 ----
mean loss: 154.43
 ---- batch: 110 ----
mean loss: 160.50
train mean loss: 157.54
epoch train time: 0:00:00.559861
elapsed time: 0:01:43.259712
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-27 02:11:55.138736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.11
 ---- batch: 020 ----
mean loss: 146.68
 ---- batch: 030 ----
mean loss: 155.30
 ---- batch: 040 ----
mean loss: 155.90
 ---- batch: 050 ----
mean loss: 156.05
 ---- batch: 060 ----
mean loss: 155.18
 ---- batch: 070 ----
mean loss: 162.12
 ---- batch: 080 ----
mean loss: 160.86
 ---- batch: 090 ----
mean loss: 154.27
 ---- batch: 100 ----
mean loss: 167.06
 ---- batch: 110 ----
mean loss: 147.93
train mean loss: 156.63
epoch train time: 0:00:00.544910
elapsed time: 0:01:43.804761
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-27 02:11:55.683785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.20
 ---- batch: 020 ----
mean loss: 163.45
 ---- batch: 030 ----
mean loss: 153.34
 ---- batch: 040 ----
mean loss: 156.46
 ---- batch: 050 ----
mean loss: 159.18
 ---- batch: 060 ----
mean loss: 158.62
 ---- batch: 070 ----
mean loss: 149.42
 ---- batch: 080 ----
mean loss: 152.00
 ---- batch: 090 ----
mean loss: 156.90
 ---- batch: 100 ----
mean loss: 156.38
 ---- batch: 110 ----
mean loss: 160.66
train mean loss: 156.40
epoch train time: 0:00:00.558719
elapsed time: 0:01:44.363610
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-27 02:11:56.242637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.98
 ---- batch: 020 ----
mean loss: 151.87
 ---- batch: 030 ----
mean loss: 155.12
 ---- batch: 040 ----
mean loss: 159.98
 ---- batch: 050 ----
mean loss: 147.27
 ---- batch: 060 ----
mean loss: 153.33
 ---- batch: 070 ----
mean loss: 162.13
 ---- batch: 080 ----
mean loss: 160.52
 ---- batch: 090 ----
mean loss: 164.06
 ---- batch: 100 ----
mean loss: 155.24
 ---- batch: 110 ----
mean loss: 152.98
train mean loss: 155.85
epoch train time: 0:00:00.554266
elapsed time: 0:01:44.918014
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-27 02:11:56.797040
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 159.01
 ---- batch: 020 ----
mean loss: 152.33
 ---- batch: 030 ----
mean loss: 158.82
 ---- batch: 040 ----
mean loss: 153.80
 ---- batch: 050 ----
mean loss: 152.80
 ---- batch: 060 ----
mean loss: 154.12
 ---- batch: 070 ----
mean loss: 157.86
 ---- batch: 080 ----
mean loss: 152.89
 ---- batch: 090 ----
mean loss: 151.09
 ---- batch: 100 ----
mean loss: 160.30
 ---- batch: 110 ----
mean loss: 160.17
train mean loss: 155.75
epoch train time: 0:00:00.548723
elapsed time: 0:01:45.466897
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-27 02:11:57.345920
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.28
 ---- batch: 020 ----
mean loss: 153.59
 ---- batch: 030 ----
mean loss: 153.07
 ---- batch: 040 ----
mean loss: 155.16
 ---- batch: 050 ----
mean loss: 154.38
 ---- batch: 060 ----
mean loss: 160.80
 ---- batch: 070 ----
mean loss: 160.43
 ---- batch: 080 ----
mean loss: 147.09
 ---- batch: 090 ----
mean loss: 147.35
 ---- batch: 100 ----
mean loss: 155.63
 ---- batch: 110 ----
mean loss: 153.02
train mean loss: 154.43
epoch train time: 0:00:00.555894
elapsed time: 0:01:46.022922
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-27 02:11:57.901948
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.77
 ---- batch: 020 ----
mean loss: 145.62
 ---- batch: 030 ----
mean loss: 150.25
 ---- batch: 040 ----
mean loss: 153.28
 ---- batch: 050 ----
mean loss: 157.15
 ---- batch: 060 ----
mean loss: 147.61
 ---- batch: 070 ----
mean loss: 157.20
 ---- batch: 080 ----
mean loss: 153.42
 ---- batch: 090 ----
mean loss: 163.39
 ---- batch: 100 ----
mean loss: 153.33
 ---- batch: 110 ----
mean loss: 162.24
train mean loss: 155.15
epoch train time: 0:00:00.560324
elapsed time: 0:01:46.583379
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-27 02:11:58.462423
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.16
 ---- batch: 020 ----
mean loss: 157.82
 ---- batch: 030 ----
mean loss: 159.23
 ---- batch: 040 ----
mean loss: 156.47
 ---- batch: 050 ----
mean loss: 159.24
 ---- batch: 060 ----
mean loss: 154.62
 ---- batch: 070 ----
mean loss: 153.94
 ---- batch: 080 ----
mean loss: 151.00
 ---- batch: 090 ----
mean loss: 153.44
 ---- batch: 100 ----
mean loss: 159.34
 ---- batch: 110 ----
mean loss: 151.98
train mean loss: 155.33
epoch train time: 0:00:00.551083
elapsed time: 0:01:47.134613
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-27 02:11:59.013666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.82
 ---- batch: 020 ----
mean loss: 156.60
 ---- batch: 030 ----
mean loss: 157.70
 ---- batch: 040 ----
mean loss: 160.25
 ---- batch: 050 ----
mean loss: 148.13
 ---- batch: 060 ----
mean loss: 155.11
 ---- batch: 070 ----
mean loss: 152.24
 ---- batch: 080 ----
mean loss: 154.36
 ---- batch: 090 ----
mean loss: 153.61
 ---- batch: 100 ----
mean loss: 152.66
 ---- batch: 110 ----
mean loss: 159.37
train mean loss: 154.29
epoch train time: 0:00:00.551090
elapsed time: 0:01:47.685873
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-27 02:11:59.564916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.24
 ---- batch: 020 ----
mean loss: 152.90
 ---- batch: 030 ----
mean loss: 155.14
 ---- batch: 040 ----
mean loss: 149.82
 ---- batch: 050 ----
mean loss: 148.58
 ---- batch: 060 ----
mean loss: 155.08
 ---- batch: 070 ----
mean loss: 156.95
 ---- batch: 080 ----
mean loss: 150.53
 ---- batch: 090 ----
mean loss: 157.84
 ---- batch: 100 ----
mean loss: 148.03
 ---- batch: 110 ----
mean loss: 150.84
train mean loss: 153.70
epoch train time: 0:00:00.560849
elapsed time: 0:01:48.246890
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-27 02:12:00.125916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.75
 ---- batch: 020 ----
mean loss: 151.42
 ---- batch: 030 ----
mean loss: 147.10
 ---- batch: 040 ----
mean loss: 154.71
 ---- batch: 050 ----
mean loss: 159.39
 ---- batch: 060 ----
mean loss: 157.65
 ---- batch: 070 ----
mean loss: 145.62
 ---- batch: 080 ----
mean loss: 154.84
 ---- batch: 090 ----
mean loss: 145.20
 ---- batch: 100 ----
mean loss: 160.58
 ---- batch: 110 ----
mean loss: 163.48
train mean loss: 153.02
epoch train time: 0:00:00.564974
elapsed time: 0:01:48.811999
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-27 02:12:00.691027
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 166.24
 ---- batch: 020 ----
mean loss: 157.24
 ---- batch: 030 ----
mean loss: 146.31
 ---- batch: 040 ----
mean loss: 161.83
 ---- batch: 050 ----
mean loss: 147.24
 ---- batch: 060 ----
mean loss: 157.54
 ---- batch: 070 ----
mean loss: 148.56
 ---- batch: 080 ----
mean loss: 152.79
 ---- batch: 090 ----
mean loss: 150.40
 ---- batch: 100 ----
mean loss: 159.32
 ---- batch: 110 ----
mean loss: 150.13
train mean loss: 153.97
epoch train time: 0:00:00.557112
elapsed time: 0:01:49.369245
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-27 02:12:01.248271
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.27
 ---- batch: 020 ----
mean loss: 153.97
 ---- batch: 030 ----
mean loss: 155.47
 ---- batch: 040 ----
mean loss: 157.69
 ---- batch: 050 ----
mean loss: 156.83
 ---- batch: 060 ----
mean loss: 160.49
 ---- batch: 070 ----
mean loss: 148.73
 ---- batch: 080 ----
mean loss: 146.52
 ---- batch: 090 ----
mean loss: 155.35
 ---- batch: 100 ----
mean loss: 144.10
 ---- batch: 110 ----
mean loss: 157.22
train mean loss: 153.17
epoch train time: 0:00:00.561978
elapsed time: 0:01:49.931362
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-27 02:12:01.810385
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.58
 ---- batch: 020 ----
mean loss: 147.43
 ---- batch: 030 ----
mean loss: 153.44
 ---- batch: 040 ----
mean loss: 159.62
 ---- batch: 050 ----
mean loss: 152.04
 ---- batch: 060 ----
mean loss: 146.96
 ---- batch: 070 ----
mean loss: 152.72
 ---- batch: 080 ----
mean loss: 159.49
 ---- batch: 090 ----
mean loss: 164.33
 ---- batch: 100 ----
mean loss: 153.58
 ---- batch: 110 ----
mean loss: 145.85
train mean loss: 152.80
epoch train time: 0:00:00.552289
elapsed time: 0:01:50.483781
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-27 02:12:02.362825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.68
 ---- batch: 020 ----
mean loss: 156.66
 ---- batch: 030 ----
mean loss: 151.57
 ---- batch: 040 ----
mean loss: 148.68
 ---- batch: 050 ----
mean loss: 154.10
 ---- batch: 060 ----
mean loss: 151.68
 ---- batch: 070 ----
mean loss: 153.61
 ---- batch: 080 ----
mean loss: 157.17
 ---- batch: 090 ----
mean loss: 155.58
 ---- batch: 100 ----
mean loss: 152.20
 ---- batch: 110 ----
mean loss: 148.91
train mean loss: 152.85
epoch train time: 0:00:00.554437
elapsed time: 0:01:51.038373
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-27 02:12:02.917402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.67
 ---- batch: 020 ----
mean loss: 155.79
 ---- batch: 030 ----
mean loss: 148.89
 ---- batch: 040 ----
mean loss: 149.28
 ---- batch: 050 ----
mean loss: 143.23
 ---- batch: 060 ----
mean loss: 154.11
 ---- batch: 070 ----
mean loss: 148.33
 ---- batch: 080 ----
mean loss: 154.27
 ---- batch: 090 ----
mean loss: 147.45
 ---- batch: 100 ----
mean loss: 157.74
 ---- batch: 110 ----
mean loss: 163.05
train mean loss: 151.93
epoch train time: 0:00:00.560885
elapsed time: 0:01:51.599409
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-27 02:12:03.478437
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.60
 ---- batch: 020 ----
mean loss: 145.40
 ---- batch: 030 ----
mean loss: 147.71
 ---- batch: 040 ----
mean loss: 154.70
 ---- batch: 050 ----
mean loss: 152.59
 ---- batch: 060 ----
mean loss: 147.25
 ---- batch: 070 ----
mean loss: 159.80
 ---- batch: 080 ----
mean loss: 162.98
 ---- batch: 090 ----
mean loss: 161.66
 ---- batch: 100 ----
mean loss: 156.22
 ---- batch: 110 ----
mean loss: 143.46
train mean loss: 152.82
epoch train time: 0:00:00.559754
elapsed time: 0:01:52.159299
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-27 02:12:04.038327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.01
 ---- batch: 020 ----
mean loss: 144.90
 ---- batch: 030 ----
mean loss: 157.47
 ---- batch: 040 ----
mean loss: 156.85
 ---- batch: 050 ----
mean loss: 151.91
 ---- batch: 060 ----
mean loss: 151.44
 ---- batch: 070 ----
mean loss: 146.21
 ---- batch: 080 ----
mean loss: 149.78
 ---- batch: 090 ----
mean loss: 152.27
 ---- batch: 100 ----
mean loss: 153.38
 ---- batch: 110 ----
mean loss: 153.91
train mean loss: 151.57
epoch train time: 0:00:00.553746
elapsed time: 0:01:52.713180
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-27 02:12:04.592212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.55
 ---- batch: 020 ----
mean loss: 156.65
 ---- batch: 030 ----
mean loss: 153.84
 ---- batch: 040 ----
mean loss: 153.54
 ---- batch: 050 ----
mean loss: 158.37
 ---- batch: 060 ----
mean loss: 149.31
 ---- batch: 070 ----
mean loss: 149.64
 ---- batch: 080 ----
mean loss: 154.03
 ---- batch: 090 ----
mean loss: 153.80
 ---- batch: 100 ----
mean loss: 148.64
 ---- batch: 110 ----
mean loss: 148.00
train mean loss: 151.57
epoch train time: 0:00:00.550259
elapsed time: 0:01:53.263576
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-27 02:12:05.142601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.75
 ---- batch: 020 ----
mean loss: 148.62
 ---- batch: 030 ----
mean loss: 143.11
 ---- batch: 040 ----
mean loss: 152.72
 ---- batch: 050 ----
mean loss: 148.06
 ---- batch: 060 ----
mean loss: 150.69
 ---- batch: 070 ----
mean loss: 156.55
 ---- batch: 080 ----
mean loss: 149.71
 ---- batch: 090 ----
mean loss: 146.19
 ---- batch: 100 ----
mean loss: 148.88
 ---- batch: 110 ----
mean loss: 156.52
train mean loss: 150.62
epoch train time: 0:00:00.554595
elapsed time: 0:01:53.818312
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-27 02:12:05.697358
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.16
 ---- batch: 020 ----
mean loss: 152.61
 ---- batch: 030 ----
mean loss: 147.64
 ---- batch: 040 ----
mean loss: 146.89
 ---- batch: 050 ----
mean loss: 152.54
 ---- batch: 060 ----
mean loss: 148.35
 ---- batch: 070 ----
mean loss: 152.04
 ---- batch: 080 ----
mean loss: 147.81
 ---- batch: 090 ----
mean loss: 156.50
 ---- batch: 100 ----
mean loss: 159.78
 ---- batch: 110 ----
mean loss: 144.08
train mean loss: 150.63
epoch train time: 0:00:00.544881
elapsed time: 0:01:54.363343
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-27 02:12:06.242367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.51
 ---- batch: 020 ----
mean loss: 154.14
 ---- batch: 030 ----
mean loss: 151.26
 ---- batch: 040 ----
mean loss: 144.91
 ---- batch: 050 ----
mean loss: 146.64
 ---- batch: 060 ----
mean loss: 145.06
 ---- batch: 070 ----
mean loss: 152.23
 ---- batch: 080 ----
mean loss: 159.60
 ---- batch: 090 ----
mean loss: 150.93
 ---- batch: 100 ----
mean loss: 157.47
 ---- batch: 110 ----
mean loss: 148.09
train mean loss: 150.06
epoch train time: 0:00:00.566217
elapsed time: 0:01:54.929707
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-27 02:12:06.808725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.81
 ---- batch: 020 ----
mean loss: 143.88
 ---- batch: 030 ----
mean loss: 143.07
 ---- batch: 040 ----
mean loss: 139.20
 ---- batch: 050 ----
mean loss: 145.08
 ---- batch: 060 ----
mean loss: 152.96
 ---- batch: 070 ----
mean loss: 156.62
 ---- batch: 080 ----
mean loss: 154.71
 ---- batch: 090 ----
mean loss: 154.31
 ---- batch: 100 ----
mean loss: 155.96
 ---- batch: 110 ----
mean loss: 151.79
train mean loss: 150.37
epoch train time: 0:00:00.554231
elapsed time: 0:01:55.484062
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-27 02:12:07.363088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.70
 ---- batch: 020 ----
mean loss: 144.93
 ---- batch: 030 ----
mean loss: 153.57
 ---- batch: 040 ----
mean loss: 145.73
 ---- batch: 050 ----
mean loss: 152.52
 ---- batch: 060 ----
mean loss: 160.60
 ---- batch: 070 ----
mean loss: 146.61
 ---- batch: 080 ----
mean loss: 155.64
 ---- batch: 090 ----
mean loss: 152.07
 ---- batch: 100 ----
mean loss: 144.78
 ---- batch: 110 ----
mean loss: 146.26
train mean loss: 149.99
epoch train time: 0:00:00.583888
elapsed time: 0:01:56.068120
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-27 02:12:07.947146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.90
 ---- batch: 020 ----
mean loss: 146.02
 ---- batch: 030 ----
mean loss: 151.67
 ---- batch: 040 ----
mean loss: 146.53
 ---- batch: 050 ----
mean loss: 144.14
 ---- batch: 060 ----
mean loss: 155.23
 ---- batch: 070 ----
mean loss: 152.92
 ---- batch: 080 ----
mean loss: 148.30
 ---- batch: 090 ----
mean loss: 151.48
 ---- batch: 100 ----
mean loss: 156.61
 ---- batch: 110 ----
mean loss: 151.33
train mean loss: 150.66
epoch train time: 0:00:00.570733
elapsed time: 0:01:56.638989
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-27 02:12:08.518016
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.47
 ---- batch: 020 ----
mean loss: 145.51
 ---- batch: 030 ----
mean loss: 146.51
 ---- batch: 040 ----
mean loss: 144.08
 ---- batch: 050 ----
mean loss: 147.61
 ---- batch: 060 ----
mean loss: 148.46
 ---- batch: 070 ----
mean loss: 148.74
 ---- batch: 080 ----
mean loss: 153.46
 ---- batch: 090 ----
mean loss: 152.95
 ---- batch: 100 ----
mean loss: 151.63
 ---- batch: 110 ----
mean loss: 150.27
train mean loss: 149.45
epoch train time: 0:00:00.568767
elapsed time: 0:01:57.207892
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-27 02:12:09.086917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.65
 ---- batch: 020 ----
mean loss: 136.57
 ---- batch: 030 ----
mean loss: 145.52
 ---- batch: 040 ----
mean loss: 152.45
 ---- batch: 050 ----
mean loss: 157.45
 ---- batch: 060 ----
mean loss: 152.74
 ---- batch: 070 ----
mean loss: 155.22
 ---- batch: 080 ----
mean loss: 147.70
 ---- batch: 090 ----
mean loss: 144.79
 ---- batch: 100 ----
mean loss: 152.30
 ---- batch: 110 ----
mean loss: 151.74
train mean loss: 149.69
epoch train time: 0:00:00.562602
elapsed time: 0:01:57.770622
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-27 02:12:09.649689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.87
 ---- batch: 020 ----
mean loss: 145.28
 ---- batch: 030 ----
mean loss: 147.24
 ---- batch: 040 ----
mean loss: 149.27
 ---- batch: 050 ----
mean loss: 149.10
 ---- batch: 060 ----
mean loss: 150.74
 ---- batch: 070 ----
mean loss: 146.59
 ---- batch: 080 ----
mean loss: 146.73
 ---- batch: 090 ----
mean loss: 153.10
 ---- batch: 100 ----
mean loss: 143.50
 ---- batch: 110 ----
mean loss: 152.37
train mean loss: 148.34
epoch train time: 0:00:00.574395
elapsed time: 0:01:58.345186
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-27 02:12:10.224210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.84
 ---- batch: 020 ----
mean loss: 141.42
 ---- batch: 030 ----
mean loss: 151.71
 ---- batch: 040 ----
mean loss: 146.68
 ---- batch: 050 ----
mean loss: 143.59
 ---- batch: 060 ----
mean loss: 145.36
 ---- batch: 070 ----
mean loss: 162.55
 ---- batch: 080 ----
mean loss: 151.08
 ---- batch: 090 ----
mean loss: 151.87
 ---- batch: 100 ----
mean loss: 145.49
 ---- batch: 110 ----
mean loss: 153.34
train mean loss: 149.10
epoch train time: 0:00:00.570795
elapsed time: 0:01:58.916113
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-27 02:12:10.795175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.18
 ---- batch: 020 ----
mean loss: 147.34
 ---- batch: 030 ----
mean loss: 153.65
 ---- batch: 040 ----
mean loss: 161.01
 ---- batch: 050 ----
mean loss: 153.82
 ---- batch: 060 ----
mean loss: 149.07
 ---- batch: 070 ----
mean loss: 150.98
 ---- batch: 080 ----
mean loss: 148.80
 ---- batch: 090 ----
mean loss: 141.03
 ---- batch: 100 ----
mean loss: 152.87
 ---- batch: 110 ----
mean loss: 149.33
train mean loss: 149.93
epoch train time: 0:00:00.568786
elapsed time: 0:01:59.485099
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-27 02:12:11.364124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.91
 ---- batch: 020 ----
mean loss: 145.39
 ---- batch: 030 ----
mean loss: 153.00
 ---- batch: 040 ----
mean loss: 144.43
 ---- batch: 050 ----
mean loss: 140.30
 ---- batch: 060 ----
mean loss: 154.23
 ---- batch: 070 ----
mean loss: 139.90
 ---- batch: 080 ----
mean loss: 146.01
 ---- batch: 090 ----
mean loss: 156.95
 ---- batch: 100 ----
mean loss: 148.34
 ---- batch: 110 ----
mean loss: 155.53
train mean loss: 147.93
epoch train time: 0:00:00.568027
elapsed time: 0:02:00.053257
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-27 02:12:11.932281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.23
 ---- batch: 020 ----
mean loss: 142.79
 ---- batch: 030 ----
mean loss: 146.84
 ---- batch: 040 ----
mean loss: 146.91
 ---- batch: 050 ----
mean loss: 147.23
 ---- batch: 060 ----
mean loss: 142.70
 ---- batch: 070 ----
mean loss: 150.22
 ---- batch: 080 ----
mean loss: 143.64
 ---- batch: 090 ----
mean loss: 142.17
 ---- batch: 100 ----
mean loss: 152.33
 ---- batch: 110 ----
mean loss: 153.34
train mean loss: 147.85
epoch train time: 0:00:00.565922
elapsed time: 0:02:00.619306
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-27 02:12:12.498331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.83
 ---- batch: 020 ----
mean loss: 149.76
 ---- batch: 030 ----
mean loss: 144.40
 ---- batch: 040 ----
mean loss: 155.21
 ---- batch: 050 ----
mean loss: 148.32
 ---- batch: 060 ----
mean loss: 143.61
 ---- batch: 070 ----
mean loss: 146.63
 ---- batch: 080 ----
mean loss: 147.50
 ---- batch: 090 ----
mean loss: 146.29
 ---- batch: 100 ----
mean loss: 156.33
 ---- batch: 110 ----
mean loss: 144.77
train mean loss: 148.43
epoch train time: 0:00:00.568108
elapsed time: 0:02:01.187562
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-27 02:12:13.066635
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.38
 ---- batch: 020 ----
mean loss: 144.26
 ---- batch: 030 ----
mean loss: 140.78
 ---- batch: 040 ----
mean loss: 152.54
 ---- batch: 050 ----
mean loss: 155.04
 ---- batch: 060 ----
mean loss: 141.97
 ---- batch: 070 ----
mean loss: 140.90
 ---- batch: 080 ----
mean loss: 157.07
 ---- batch: 090 ----
mean loss: 156.26
 ---- batch: 100 ----
mean loss: 140.57
 ---- batch: 110 ----
mean loss: 146.48
train mean loss: 147.32
epoch train time: 0:00:00.566914
elapsed time: 0:02:01.754669
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-27 02:12:13.633697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.74
 ---- batch: 020 ----
mean loss: 148.25
 ---- batch: 030 ----
mean loss: 142.46
 ---- batch: 040 ----
mean loss: 149.33
 ---- batch: 050 ----
mean loss: 155.00
 ---- batch: 060 ----
mean loss: 151.33
 ---- batch: 070 ----
mean loss: 144.77
 ---- batch: 080 ----
mean loss: 146.49
 ---- batch: 090 ----
mean loss: 145.95
 ---- batch: 100 ----
mean loss: 151.01
 ---- batch: 110 ----
mean loss: 147.44
train mean loss: 147.36
epoch train time: 0:00:00.568645
elapsed time: 0:02:02.323447
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-27 02:12:14.202472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.79
 ---- batch: 020 ----
mean loss: 151.74
 ---- batch: 030 ----
mean loss: 145.67
 ---- batch: 040 ----
mean loss: 138.28
 ---- batch: 050 ----
mean loss: 155.79
 ---- batch: 060 ----
mean loss: 147.63
 ---- batch: 070 ----
mean loss: 147.98
 ---- batch: 080 ----
mean loss: 133.09
 ---- batch: 090 ----
mean loss: 145.68
 ---- batch: 100 ----
mean loss: 153.59
 ---- batch: 110 ----
mean loss: 150.56
train mean loss: 146.89
epoch train time: 0:00:00.568869
elapsed time: 0:02:02.892452
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-27 02:12:14.771480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.89
 ---- batch: 020 ----
mean loss: 144.01
 ---- batch: 030 ----
mean loss: 150.74
 ---- batch: 040 ----
mean loss: 150.48
 ---- batch: 050 ----
mean loss: 149.42
 ---- batch: 060 ----
mean loss: 145.63
 ---- batch: 070 ----
mean loss: 142.31
 ---- batch: 080 ----
mean loss: 147.01
 ---- batch: 090 ----
mean loss: 146.91
 ---- batch: 100 ----
mean loss: 140.48
 ---- batch: 110 ----
mean loss: 145.52
train mean loss: 146.27
epoch train time: 0:00:00.564480
elapsed time: 0:02:03.457068
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-27 02:12:15.336096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.08
 ---- batch: 020 ----
mean loss: 152.63
 ---- batch: 030 ----
mean loss: 150.06
 ---- batch: 040 ----
mean loss: 143.75
 ---- batch: 050 ----
mean loss: 148.36
 ---- batch: 060 ----
mean loss: 143.52
 ---- batch: 070 ----
mean loss: 155.35
 ---- batch: 080 ----
mean loss: 146.64
 ---- batch: 090 ----
mean loss: 138.08
 ---- batch: 100 ----
mean loss: 149.02
 ---- batch: 110 ----
mean loss: 147.14
train mean loss: 147.03
epoch train time: 0:00:00.570840
elapsed time: 0:02:04.028045
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-27 02:12:15.907073
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 148.32
 ---- batch: 020 ----
mean loss: 138.22
 ---- batch: 030 ----
mean loss: 147.32
 ---- batch: 040 ----
mean loss: 144.00
 ---- batch: 050 ----
mean loss: 159.97
 ---- batch: 060 ----
mean loss: 143.24
 ---- batch: 070 ----
mean loss: 149.33
 ---- batch: 080 ----
mean loss: 147.53
 ---- batch: 090 ----
mean loss: 151.25
 ---- batch: 100 ----
mean loss: 143.39
 ---- batch: 110 ----
mean loss: 137.26
train mean loss: 146.70
epoch train time: 0:00:00.577401
elapsed time: 0:02:04.605577
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-27 02:12:16.484604
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.76
 ---- batch: 020 ----
mean loss: 148.71
 ---- batch: 030 ----
mean loss: 139.15
 ---- batch: 040 ----
mean loss: 143.70
 ---- batch: 050 ----
mean loss: 138.00
 ---- batch: 060 ----
mean loss: 147.47
 ---- batch: 070 ----
mean loss: 138.73
 ---- batch: 080 ----
mean loss: 155.94
 ---- batch: 090 ----
mean loss: 147.64
 ---- batch: 100 ----
mean loss: 160.71
 ---- batch: 110 ----
mean loss: 143.51
train mean loss: 145.73
epoch train time: 0:00:00.583530
elapsed time: 0:02:05.189243
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-27 02:12:17.068292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.39
 ---- batch: 020 ----
mean loss: 140.07
 ---- batch: 030 ----
mean loss: 142.03
 ---- batch: 040 ----
mean loss: 139.75
 ---- batch: 050 ----
mean loss: 147.85
 ---- batch: 060 ----
mean loss: 142.53
 ---- batch: 070 ----
mean loss: 146.03
 ---- batch: 080 ----
mean loss: 139.15
 ---- batch: 090 ----
mean loss: 158.21
 ---- batch: 100 ----
mean loss: 153.57
 ---- batch: 110 ----
mean loss: 152.55
train mean loss: 145.75
epoch train time: 0:00:00.572239
elapsed time: 0:02:05.761641
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-27 02:12:17.640669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.94
 ---- batch: 020 ----
mean loss: 146.00
 ---- batch: 030 ----
mean loss: 139.65
 ---- batch: 040 ----
mean loss: 154.28
 ---- batch: 050 ----
mean loss: 152.08
 ---- batch: 060 ----
mean loss: 138.90
 ---- batch: 070 ----
mean loss: 137.46
 ---- batch: 080 ----
mean loss: 149.10
 ---- batch: 090 ----
mean loss: 150.03
 ---- batch: 100 ----
mean loss: 142.30
 ---- batch: 110 ----
mean loss: 146.94
train mean loss: 145.12
epoch train time: 0:00:00.562092
elapsed time: 0:02:06.323865
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-27 02:12:18.202891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.89
 ---- batch: 020 ----
mean loss: 142.87
 ---- batch: 030 ----
mean loss: 136.33
 ---- batch: 040 ----
mean loss: 149.11
 ---- batch: 050 ----
mean loss: 149.91
 ---- batch: 060 ----
mean loss: 145.92
 ---- batch: 070 ----
mean loss: 147.28
 ---- batch: 080 ----
mean loss: 151.57
 ---- batch: 090 ----
mean loss: 145.81
 ---- batch: 100 ----
mean loss: 153.36
 ---- batch: 110 ----
mean loss: 140.41
train mean loss: 145.45
epoch train time: 0:00:00.548208
elapsed time: 0:02:06.872204
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-27 02:12:18.751230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.82
 ---- batch: 020 ----
mean loss: 144.08
 ---- batch: 030 ----
mean loss: 147.51
 ---- batch: 040 ----
mean loss: 143.89
 ---- batch: 050 ----
mean loss: 144.70
 ---- batch: 060 ----
mean loss: 148.59
 ---- batch: 070 ----
mean loss: 140.04
 ---- batch: 080 ----
mean loss: 148.75
 ---- batch: 090 ----
mean loss: 151.97
 ---- batch: 100 ----
mean loss: 136.50
 ---- batch: 110 ----
mean loss: 140.67
train mean loss: 145.04
epoch train time: 0:00:00.557089
elapsed time: 0:02:07.429431
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-27 02:12:19.308460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.98
 ---- batch: 020 ----
mean loss: 152.61
 ---- batch: 030 ----
mean loss: 152.78
 ---- batch: 040 ----
mean loss: 141.30
 ---- batch: 050 ----
mean loss: 139.65
 ---- batch: 060 ----
mean loss: 137.23
 ---- batch: 070 ----
mean loss: 155.25
 ---- batch: 080 ----
mean loss: 142.47
 ---- batch: 090 ----
mean loss: 145.36
 ---- batch: 100 ----
mean loss: 147.72
 ---- batch: 110 ----
mean loss: 145.72
train mean loss: 145.02
epoch train time: 0:00:00.556578
elapsed time: 0:02:07.986146
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-27 02:12:19.865174
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.38
 ---- batch: 020 ----
mean loss: 143.46
 ---- batch: 030 ----
mean loss: 145.20
 ---- batch: 040 ----
mean loss: 140.50
 ---- batch: 050 ----
mean loss: 145.01
 ---- batch: 060 ----
mean loss: 147.84
 ---- batch: 070 ----
mean loss: 140.39
 ---- batch: 080 ----
mean loss: 148.52
 ---- batch: 090 ----
mean loss: 149.21
 ---- batch: 100 ----
mean loss: 148.45
 ---- batch: 110 ----
mean loss: 151.24
train mean loss: 145.47
epoch train time: 0:00:00.554129
elapsed time: 0:02:08.540407
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-27 02:12:20.419461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.55
 ---- batch: 020 ----
mean loss: 140.98
 ---- batch: 030 ----
mean loss: 145.31
 ---- batch: 040 ----
mean loss: 145.72
 ---- batch: 050 ----
mean loss: 139.24
 ---- batch: 060 ----
mean loss: 143.52
 ---- batch: 070 ----
mean loss: 145.59
 ---- batch: 080 ----
mean loss: 136.36
 ---- batch: 090 ----
mean loss: 151.22
 ---- batch: 100 ----
mean loss: 138.10
 ---- batch: 110 ----
mean loss: 152.27
train mean loss: 144.03
epoch train time: 0:00:00.554928
elapsed time: 0:02:09.095524
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-27 02:12:20.974561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.65
 ---- batch: 020 ----
mean loss: 148.40
 ---- batch: 030 ----
mean loss: 140.33
 ---- batch: 040 ----
mean loss: 147.18
 ---- batch: 050 ----
mean loss: 148.66
 ---- batch: 060 ----
mean loss: 140.15
 ---- batch: 070 ----
mean loss: 144.99
 ---- batch: 080 ----
mean loss: 140.92
 ---- batch: 090 ----
mean loss: 149.71
 ---- batch: 100 ----
mean loss: 147.05
 ---- batch: 110 ----
mean loss: 147.80
train mean loss: 144.59
epoch train time: 0:00:00.552928
elapsed time: 0:02:09.648611
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-27 02:12:21.527653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.94
 ---- batch: 020 ----
mean loss: 142.51
 ---- batch: 030 ----
mean loss: 141.21
 ---- batch: 040 ----
mean loss: 143.05
 ---- batch: 050 ----
mean loss: 141.23
 ---- batch: 060 ----
mean loss: 142.58
 ---- batch: 070 ----
mean loss: 153.18
 ---- batch: 080 ----
mean loss: 144.93
 ---- batch: 090 ----
mean loss: 145.53
 ---- batch: 100 ----
mean loss: 143.66
 ---- batch: 110 ----
mean loss: 142.80
train mean loss: 144.62
epoch train time: 0:00:00.560234
elapsed time: 0:02:10.209000
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-27 02:12:22.088026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.95
 ---- batch: 020 ----
mean loss: 144.75
 ---- batch: 030 ----
mean loss: 137.38
 ---- batch: 040 ----
mean loss: 150.97
 ---- batch: 050 ----
mean loss: 146.69
 ---- batch: 060 ----
mean loss: 143.18
 ---- batch: 070 ----
mean loss: 142.91
 ---- batch: 080 ----
mean loss: 148.09
 ---- batch: 090 ----
mean loss: 137.28
 ---- batch: 100 ----
mean loss: 145.44
 ---- batch: 110 ----
mean loss: 142.23
train mean loss: 143.94
epoch train time: 0:00:00.558165
elapsed time: 0:02:10.767295
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-27 02:12:22.646320
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.49
 ---- batch: 020 ----
mean loss: 148.20
 ---- batch: 030 ----
mean loss: 146.63
 ---- batch: 040 ----
mean loss: 139.48
 ---- batch: 050 ----
mean loss: 143.44
 ---- batch: 060 ----
mean loss: 143.35
 ---- batch: 070 ----
mean loss: 134.54
 ---- batch: 080 ----
mean loss: 145.79
 ---- batch: 090 ----
mean loss: 148.45
 ---- batch: 100 ----
mean loss: 147.24
 ---- batch: 110 ----
mean loss: 146.71
train mean loss: 144.72
epoch train time: 0:00:00.568938
elapsed time: 0:02:11.336381
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-27 02:12:23.215415
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.98
 ---- batch: 020 ----
mean loss: 148.43
 ---- batch: 030 ----
mean loss: 146.22
 ---- batch: 040 ----
mean loss: 137.86
 ---- batch: 050 ----
mean loss: 139.07
 ---- batch: 060 ----
mean loss: 146.85
 ---- batch: 070 ----
mean loss: 137.24
 ---- batch: 080 ----
mean loss: 140.56
 ---- batch: 090 ----
mean loss: 148.72
 ---- batch: 100 ----
mean loss: 144.67
 ---- batch: 110 ----
mean loss: 148.12
train mean loss: 143.77
epoch train time: 0:00:00.553259
elapsed time: 0:02:11.889776
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-27 02:12:23.768801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.15
 ---- batch: 020 ----
mean loss: 139.21
 ---- batch: 030 ----
mean loss: 140.40
 ---- batch: 040 ----
mean loss: 146.23
 ---- batch: 050 ----
mean loss: 148.08
 ---- batch: 060 ----
mean loss: 136.57
 ---- batch: 070 ----
mean loss: 143.73
 ---- batch: 080 ----
mean loss: 149.00
 ---- batch: 090 ----
mean loss: 141.45
 ---- batch: 100 ----
mean loss: 141.75
 ---- batch: 110 ----
mean loss: 149.73
train mean loss: 143.32
epoch train time: 0:00:00.550984
elapsed time: 0:02:12.440889
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-27 02:12:24.319939
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.96
 ---- batch: 020 ----
mean loss: 145.24
 ---- batch: 030 ----
mean loss: 138.49
 ---- batch: 040 ----
mean loss: 139.72
 ---- batch: 050 ----
mean loss: 152.45
 ---- batch: 060 ----
mean loss: 145.25
 ---- batch: 070 ----
mean loss: 146.79
 ---- batch: 080 ----
mean loss: 145.76
 ---- batch: 090 ----
mean loss: 138.89
 ---- batch: 100 ----
mean loss: 142.16
 ---- batch: 110 ----
mean loss: 142.28
train mean loss: 143.09
epoch train time: 0:00:00.557055
elapsed time: 0:02:12.998146
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-27 02:12:24.877170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.63
 ---- batch: 020 ----
mean loss: 140.85
 ---- batch: 030 ----
mean loss: 147.97
 ---- batch: 040 ----
mean loss: 131.27
 ---- batch: 050 ----
mean loss: 141.01
 ---- batch: 060 ----
mean loss: 138.10
 ---- batch: 070 ----
mean loss: 144.20
 ---- batch: 080 ----
mean loss: 148.27
 ---- batch: 090 ----
mean loss: 149.90
 ---- batch: 100 ----
mean loss: 150.21
 ---- batch: 110 ----
mean loss: 147.14
train mean loss: 143.68
epoch train time: 0:00:00.548250
elapsed time: 0:02:13.546545
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-27 02:12:25.425568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.86
 ---- batch: 020 ----
mean loss: 145.39
 ---- batch: 030 ----
mean loss: 142.13
 ---- batch: 040 ----
mean loss: 145.02
 ---- batch: 050 ----
mean loss: 146.24
 ---- batch: 060 ----
mean loss: 147.11
 ---- batch: 070 ----
mean loss: 140.62
 ---- batch: 080 ----
mean loss: 137.89
 ---- batch: 090 ----
mean loss: 144.69
 ---- batch: 100 ----
mean loss: 151.21
 ---- batch: 110 ----
mean loss: 140.76
train mean loss: 144.14
epoch train time: 0:00:00.555426
elapsed time: 0:02:14.102106
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-27 02:12:25.981135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 140.46
 ---- batch: 020 ----
mean loss: 143.78
 ---- batch: 030 ----
mean loss: 134.24
 ---- batch: 040 ----
mean loss: 145.73
 ---- batch: 050 ----
mean loss: 136.91
 ---- batch: 060 ----
mean loss: 133.67
 ---- batch: 070 ----
mean loss: 148.41
 ---- batch: 080 ----
mean loss: 146.42
 ---- batch: 090 ----
mean loss: 152.92
 ---- batch: 100 ----
mean loss: 144.24
 ---- batch: 110 ----
mean loss: 142.18
train mean loss: 142.96
epoch train time: 0:00:00.565224
elapsed time: 0:02:14.667468
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-27 02:12:26.546496
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.25
 ---- batch: 020 ----
mean loss: 143.40
 ---- batch: 030 ----
mean loss: 140.23
 ---- batch: 040 ----
mean loss: 139.84
 ---- batch: 050 ----
mean loss: 147.84
 ---- batch: 060 ----
mean loss: 139.47
 ---- batch: 070 ----
mean loss: 141.89
 ---- batch: 080 ----
mean loss: 145.02
 ---- batch: 090 ----
mean loss: 135.01
 ---- batch: 100 ----
mean loss: 144.81
 ---- batch: 110 ----
mean loss: 148.38
train mean loss: 142.03
epoch train time: 0:00:00.573710
elapsed time: 0:02:15.241312
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-27 02:12:27.120337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.94
 ---- batch: 020 ----
mean loss: 139.22
 ---- batch: 030 ----
mean loss: 154.21
 ---- batch: 040 ----
mean loss: 142.84
 ---- batch: 050 ----
mean loss: 141.69
 ---- batch: 060 ----
mean loss: 141.69
 ---- batch: 070 ----
mean loss: 143.39
 ---- batch: 080 ----
mean loss: 139.91
 ---- batch: 090 ----
mean loss: 139.46
 ---- batch: 100 ----
mean loss: 144.53
 ---- batch: 110 ----
mean loss: 149.19
train mean loss: 143.26
epoch train time: 0:00:00.558210
elapsed time: 0:02:15.799681
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-27 02:12:27.678707
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.46
 ---- batch: 020 ----
mean loss: 149.57
 ---- batch: 030 ----
mean loss: 153.80
 ---- batch: 040 ----
mean loss: 139.13
 ---- batch: 050 ----
mean loss: 142.77
 ---- batch: 060 ----
mean loss: 141.18
 ---- batch: 070 ----
mean loss: 139.23
 ---- batch: 080 ----
mean loss: 142.80
 ---- batch: 090 ----
mean loss: 137.77
 ---- batch: 100 ----
mean loss: 140.61
 ---- batch: 110 ----
mean loss: 141.20
train mean loss: 142.29
epoch train time: 0:00:00.564710
elapsed time: 0:02:16.364524
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-27 02:12:28.243569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.63
 ---- batch: 020 ----
mean loss: 135.12
 ---- batch: 030 ----
mean loss: 134.31
 ---- batch: 040 ----
mean loss: 148.60
 ---- batch: 050 ----
mean loss: 147.20
 ---- batch: 060 ----
mean loss: 149.69
 ---- batch: 070 ----
mean loss: 134.98
 ---- batch: 080 ----
mean loss: 139.00
 ---- batch: 090 ----
mean loss: 146.27
 ---- batch: 100 ----
mean loss: 148.08
 ---- batch: 110 ----
mean loss: 148.88
train mean loss: 141.92
epoch train time: 0:00:00.549180
elapsed time: 0:02:16.913877
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-27 02:12:28.792902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.62
 ---- batch: 020 ----
mean loss: 145.52
 ---- batch: 030 ----
mean loss: 147.10
 ---- batch: 040 ----
mean loss: 132.07
 ---- batch: 050 ----
mean loss: 144.38
 ---- batch: 060 ----
mean loss: 140.22
 ---- batch: 070 ----
mean loss: 142.74
 ---- batch: 080 ----
mean loss: 139.82
 ---- batch: 090 ----
mean loss: 148.79
 ---- batch: 100 ----
mean loss: 145.45
 ---- batch: 110 ----
mean loss: 135.34
train mean loss: 141.71
epoch train time: 0:00:00.550580
elapsed time: 0:02:17.464586
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-27 02:12:29.343614
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 142.08
 ---- batch: 020 ----
mean loss: 141.64
 ---- batch: 030 ----
mean loss: 151.97
 ---- batch: 040 ----
mean loss: 141.42
 ---- batch: 050 ----
mean loss: 138.78
 ---- batch: 060 ----
mean loss: 143.43
 ---- batch: 070 ----
mean loss: 137.00
 ---- batch: 080 ----
mean loss: 145.83
 ---- batch: 090 ----
mean loss: 141.97
 ---- batch: 100 ----
mean loss: 137.10
 ---- batch: 110 ----
mean loss: 138.04
train mean loss: 141.71
epoch train time: 0:00:00.563814
elapsed time: 0:02:18.028530
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-27 02:12:29.907554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.30
 ---- batch: 020 ----
mean loss: 135.91
 ---- batch: 030 ----
mean loss: 139.04
 ---- batch: 040 ----
mean loss: 135.20
 ---- batch: 050 ----
mean loss: 138.96
 ---- batch: 060 ----
mean loss: 147.87
 ---- batch: 070 ----
mean loss: 151.13
 ---- batch: 080 ----
mean loss: 138.50
 ---- batch: 090 ----
mean loss: 140.49
 ---- batch: 100 ----
mean loss: 144.92
 ---- batch: 110 ----
mean loss: 145.81
train mean loss: 141.40
epoch train time: 0:00:00.553258
elapsed time: 0:02:18.581918
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-27 02:12:30.460956
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.43
 ---- batch: 020 ----
mean loss: 131.82
 ---- batch: 030 ----
mean loss: 141.65
 ---- batch: 040 ----
mean loss: 140.33
 ---- batch: 050 ----
mean loss: 141.88
 ---- batch: 060 ----
mean loss: 146.86
 ---- batch: 070 ----
mean loss: 139.74
 ---- batch: 080 ----
mean loss: 138.34
 ---- batch: 090 ----
mean loss: 142.88
 ---- batch: 100 ----
mean loss: 145.88
 ---- batch: 110 ----
mean loss: 140.56
train mean loss: 140.80
epoch train time: 0:00:00.552022
elapsed time: 0:02:19.134081
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-27 02:12:31.013106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.90
 ---- batch: 020 ----
mean loss: 143.58
 ---- batch: 030 ----
mean loss: 144.17
 ---- batch: 040 ----
mean loss: 138.53
 ---- batch: 050 ----
mean loss: 141.63
 ---- batch: 060 ----
mean loss: 146.97
 ---- batch: 070 ----
mean loss: 134.64
 ---- batch: 080 ----
mean loss: 138.50
 ---- batch: 090 ----
mean loss: 138.75
 ---- batch: 100 ----
mean loss: 135.71
 ---- batch: 110 ----
mean loss: 143.96
train mean loss: 140.97
epoch train time: 0:00:00.556470
elapsed time: 0:02:19.690703
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-27 02:12:31.569750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.04
 ---- batch: 020 ----
mean loss: 139.02
 ---- batch: 030 ----
mean loss: 137.57
 ---- batch: 040 ----
mean loss: 146.29
 ---- batch: 050 ----
mean loss: 142.02
 ---- batch: 060 ----
mean loss: 134.79
 ---- batch: 070 ----
mean loss: 137.88
 ---- batch: 080 ----
mean loss: 135.95
 ---- batch: 090 ----
mean loss: 136.68
 ---- batch: 100 ----
mean loss: 149.70
 ---- batch: 110 ----
mean loss: 150.68
train mean loss: 140.92
epoch train time: 0:00:00.552330
elapsed time: 0:02:20.243205
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-27 02:12:32.122232
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.32
 ---- batch: 020 ----
mean loss: 151.86
 ---- batch: 030 ----
mean loss: 144.04
 ---- batch: 040 ----
mean loss: 138.76
 ---- batch: 050 ----
mean loss: 137.73
 ---- batch: 060 ----
mean loss: 133.75
 ---- batch: 070 ----
mean loss: 147.29
 ---- batch: 080 ----
mean loss: 136.61
 ---- batch: 090 ----
mean loss: 141.20
 ---- batch: 100 ----
mean loss: 144.61
 ---- batch: 110 ----
mean loss: 134.67
train mean loss: 140.50
epoch train time: 0:00:00.552035
elapsed time: 0:02:20.795376
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-27 02:12:32.674403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.40
 ---- batch: 020 ----
mean loss: 144.51
 ---- batch: 030 ----
mean loss: 147.53
 ---- batch: 040 ----
mean loss: 141.79
 ---- batch: 050 ----
mean loss: 133.45
 ---- batch: 060 ----
mean loss: 136.98
 ---- batch: 070 ----
mean loss: 142.60
 ---- batch: 080 ----
mean loss: 139.18
 ---- batch: 090 ----
mean loss: 143.11
 ---- batch: 100 ----
mean loss: 137.71
 ---- batch: 110 ----
mean loss: 142.06
train mean loss: 140.24
epoch train time: 0:00:00.562837
elapsed time: 0:02:21.358347
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-27 02:12:33.237373
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 141.42
 ---- batch: 020 ----
mean loss: 136.67
 ---- batch: 030 ----
mean loss: 139.78
 ---- batch: 040 ----
mean loss: 150.49
 ---- batch: 050 ----
mean loss: 134.43
 ---- batch: 060 ----
mean loss: 134.90
 ---- batch: 070 ----
mean loss: 144.93
 ---- batch: 080 ----
mean loss: 141.51
 ---- batch: 090 ----
mean loss: 143.09
 ---- batch: 100 ----
mean loss: 132.87
 ---- batch: 110 ----
mean loss: 143.72
train mean loss: 140.67
epoch train time: 0:00:00.557444
elapsed time: 0:02:21.915929
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-27 02:12:33.794963
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.39
 ---- batch: 020 ----
mean loss: 138.48
 ---- batch: 030 ----
mean loss: 142.75
 ---- batch: 040 ----
mean loss: 134.05
 ---- batch: 050 ----
mean loss: 140.50
 ---- batch: 060 ----
mean loss: 132.32
 ---- batch: 070 ----
mean loss: 158.03
 ---- batch: 080 ----
mean loss: 143.68
 ---- batch: 090 ----
mean loss: 142.34
 ---- batch: 100 ----
mean loss: 142.57
 ---- batch: 110 ----
mean loss: 137.22
train mean loss: 140.30
epoch train time: 0:00:00.571178
elapsed time: 0:02:22.487270
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-27 02:12:34.366311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.76
 ---- batch: 020 ----
mean loss: 136.33
 ---- batch: 030 ----
mean loss: 139.34
 ---- batch: 040 ----
mean loss: 138.19
 ---- batch: 050 ----
mean loss: 137.26
 ---- batch: 060 ----
mean loss: 149.14
 ---- batch: 070 ----
mean loss: 131.40
 ---- batch: 080 ----
mean loss: 142.11
 ---- batch: 090 ----
mean loss: 137.55
 ---- batch: 100 ----
mean loss: 137.60
 ---- batch: 110 ----
mean loss: 146.44
train mean loss: 140.22
epoch train time: 0:00:00.562906
elapsed time: 0:02:23.050323
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-27 02:12:34.929349
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.24
 ---- batch: 020 ----
mean loss: 134.57
 ---- batch: 030 ----
mean loss: 147.98
 ---- batch: 040 ----
mean loss: 132.70
 ---- batch: 050 ----
mean loss: 137.68
 ---- batch: 060 ----
mean loss: 145.11
 ---- batch: 070 ----
mean loss: 144.24
 ---- batch: 080 ----
mean loss: 141.71
 ---- batch: 090 ----
mean loss: 134.93
 ---- batch: 100 ----
mean loss: 136.31
 ---- batch: 110 ----
mean loss: 144.03
train mean loss: 140.13
epoch train time: 0:00:00.563338
elapsed time: 0:02:23.613791
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-27 02:12:35.492834
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.25
 ---- batch: 020 ----
mean loss: 134.53
 ---- batch: 030 ----
mean loss: 139.76
 ---- batch: 040 ----
mean loss: 132.03
 ---- batch: 050 ----
mean loss: 139.27
 ---- batch: 060 ----
mean loss: 139.95
 ---- batch: 070 ----
mean loss: 141.05
 ---- batch: 080 ----
mean loss: 135.13
 ---- batch: 090 ----
mean loss: 146.29
 ---- batch: 100 ----
mean loss: 143.75
 ---- batch: 110 ----
mean loss: 148.81
train mean loss: 140.14
epoch train time: 0:00:00.550922
elapsed time: 0:02:24.164858
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-27 02:12:36.043883
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.06
 ---- batch: 020 ----
mean loss: 135.74
 ---- batch: 030 ----
mean loss: 141.64
 ---- batch: 040 ----
mean loss: 132.92
 ---- batch: 050 ----
mean loss: 131.86
 ---- batch: 060 ----
mean loss: 134.20
 ---- batch: 070 ----
mean loss: 124.49
 ---- batch: 080 ----
mean loss: 133.67
 ---- batch: 090 ----
mean loss: 135.56
 ---- batch: 100 ----
mean loss: 137.75
 ---- batch: 110 ----
mean loss: 132.85
train mean loss: 133.74
epoch train time: 0:00:00.547266
elapsed time: 0:02:24.712266
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-27 02:12:36.591284
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 140.62
 ---- batch: 020 ----
mean loss: 130.85
 ---- batch: 030 ----
mean loss: 125.26
 ---- batch: 040 ----
mean loss: 132.94
 ---- batch: 050 ----
mean loss: 133.74
 ---- batch: 060 ----
mean loss: 137.66
 ---- batch: 070 ----
mean loss: 128.52
 ---- batch: 080 ----
mean loss: 137.36
 ---- batch: 090 ----
mean loss: 134.67
 ---- batch: 100 ----
mean loss: 132.58
 ---- batch: 110 ----
mean loss: 128.33
train mean loss: 132.95
epoch train time: 0:00:00.556678
elapsed time: 0:02:25.269097
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-27 02:12:37.148123
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.89
 ---- batch: 020 ----
mean loss: 132.17
 ---- batch: 030 ----
mean loss: 137.43
 ---- batch: 040 ----
mean loss: 130.14
 ---- batch: 050 ----
mean loss: 133.94
 ---- batch: 060 ----
mean loss: 134.91
 ---- batch: 070 ----
mean loss: 128.66
 ---- batch: 080 ----
mean loss: 136.48
 ---- batch: 090 ----
mean loss: 132.29
 ---- batch: 100 ----
mean loss: 131.07
 ---- batch: 110 ----
mean loss: 131.54
train mean loss: 132.62
epoch train time: 0:00:00.557047
elapsed time: 0:02:25.826279
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-27 02:12:37.705307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.51
 ---- batch: 020 ----
mean loss: 125.68
 ---- batch: 030 ----
mean loss: 126.87
 ---- batch: 040 ----
mean loss: 128.89
 ---- batch: 050 ----
mean loss: 138.92
 ---- batch: 060 ----
mean loss: 134.04
 ---- batch: 070 ----
mean loss: 139.63
 ---- batch: 080 ----
mean loss: 130.56
 ---- batch: 090 ----
mean loss: 140.71
 ---- batch: 100 ----
mean loss: 129.39
 ---- batch: 110 ----
mean loss: 128.24
train mean loss: 132.52
epoch train time: 0:00:00.555079
elapsed time: 0:02:26.381488
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-27 02:12:38.260514
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 141.00
 ---- batch: 020 ----
mean loss: 132.99
 ---- batch: 030 ----
mean loss: 131.00
 ---- batch: 040 ----
mean loss: 127.10
 ---- batch: 050 ----
mean loss: 129.18
 ---- batch: 060 ----
mean loss: 134.38
 ---- batch: 070 ----
mean loss: 134.73
 ---- batch: 080 ----
mean loss: 138.13
 ---- batch: 090 ----
mean loss: 126.86
 ---- batch: 100 ----
mean loss: 129.83
 ---- batch: 110 ----
mean loss: 133.63
train mean loss: 132.53
epoch train time: 0:00:00.570540
elapsed time: 0:02:26.952163
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-27 02:12:38.831210
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.13
 ---- batch: 020 ----
mean loss: 129.82
 ---- batch: 030 ----
mean loss: 134.56
 ---- batch: 040 ----
mean loss: 137.00
 ---- batch: 050 ----
mean loss: 125.47
 ---- batch: 060 ----
mean loss: 135.82
 ---- batch: 070 ----
mean loss: 134.67
 ---- batch: 080 ----
mean loss: 135.39
 ---- batch: 090 ----
mean loss: 132.58
 ---- batch: 100 ----
mean loss: 125.44
 ---- batch: 110 ----
mean loss: 131.78
train mean loss: 132.48
epoch train time: 0:00:00.552239
elapsed time: 0:02:27.504559
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-27 02:12:39.383587
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.20
 ---- batch: 020 ----
mean loss: 131.26
 ---- batch: 030 ----
mean loss: 131.62
 ---- batch: 040 ----
mean loss: 142.37
 ---- batch: 050 ----
mean loss: 129.85
 ---- batch: 060 ----
mean loss: 132.91
 ---- batch: 070 ----
mean loss: 124.54
 ---- batch: 080 ----
mean loss: 134.18
 ---- batch: 090 ----
mean loss: 132.90
 ---- batch: 100 ----
mean loss: 137.16
 ---- batch: 110 ----
mean loss: 131.94
train mean loss: 132.37
epoch train time: 0:00:00.549407
elapsed time: 0:02:28.054105
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-27 02:12:39.933133
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.15
 ---- batch: 020 ----
mean loss: 129.81
 ---- batch: 030 ----
mean loss: 133.84
 ---- batch: 040 ----
mean loss: 130.43
 ---- batch: 050 ----
mean loss: 135.98
 ---- batch: 060 ----
mean loss: 139.68
 ---- batch: 070 ----
mean loss: 138.20
 ---- batch: 080 ----
mean loss: 132.87
 ---- batch: 090 ----
mean loss: 132.27
 ---- batch: 100 ----
mean loss: 127.14
 ---- batch: 110 ----
mean loss: 130.39
train mean loss: 132.36
epoch train time: 0:00:00.549281
elapsed time: 0:02:28.603527
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-27 02:12:40.482553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.86
 ---- batch: 020 ----
mean loss: 123.39
 ---- batch: 030 ----
mean loss: 136.62
 ---- batch: 040 ----
mean loss: 129.98
 ---- batch: 050 ----
mean loss: 129.70
 ---- batch: 060 ----
mean loss: 137.47
 ---- batch: 070 ----
mean loss: 133.68
 ---- batch: 080 ----
mean loss: 128.71
 ---- batch: 090 ----
mean loss: 133.82
 ---- batch: 100 ----
mean loss: 132.24
 ---- batch: 110 ----
mean loss: 142.28
train mean loss: 132.25
epoch train time: 0:00:00.555946
elapsed time: 0:02:29.159609
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-27 02:12:41.038637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.98
 ---- batch: 020 ----
mean loss: 127.20
 ---- batch: 030 ----
mean loss: 132.96
 ---- batch: 040 ----
mean loss: 137.76
 ---- batch: 050 ----
mean loss: 134.72
 ---- batch: 060 ----
mean loss: 128.99
 ---- batch: 070 ----
mean loss: 129.64
 ---- batch: 080 ----
mean loss: 127.55
 ---- batch: 090 ----
mean loss: 134.09
 ---- batch: 100 ----
mean loss: 129.57
 ---- batch: 110 ----
mean loss: 130.87
train mean loss: 132.25
epoch train time: 0:00:00.559200
elapsed time: 0:02:29.718947
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-27 02:12:41.597972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.98
 ---- batch: 020 ----
mean loss: 135.86
 ---- batch: 030 ----
mean loss: 134.38
 ---- batch: 040 ----
mean loss: 132.69
 ---- batch: 050 ----
mean loss: 136.66
 ---- batch: 060 ----
mean loss: 132.26
 ---- batch: 070 ----
mean loss: 129.50
 ---- batch: 080 ----
mean loss: 127.56
 ---- batch: 090 ----
mean loss: 135.76
 ---- batch: 100 ----
mean loss: 124.09
 ---- batch: 110 ----
mean loss: 130.78
train mean loss: 132.28
epoch train time: 0:00:00.551516
elapsed time: 0:02:30.270589
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-27 02:12:42.149636
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.83
 ---- batch: 020 ----
mean loss: 130.19
 ---- batch: 030 ----
mean loss: 135.07
 ---- batch: 040 ----
mean loss: 135.98
 ---- batch: 050 ----
mean loss: 133.35
 ---- batch: 060 ----
mean loss: 130.08
 ---- batch: 070 ----
mean loss: 132.50
 ---- batch: 080 ----
mean loss: 133.38
 ---- batch: 090 ----
mean loss: 132.36
 ---- batch: 100 ----
mean loss: 135.77
 ---- batch: 110 ----
mean loss: 129.44
train mean loss: 132.16
epoch train time: 0:00:00.548923
elapsed time: 0:02:30.819679
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-27 02:12:42.698725
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.80
 ---- batch: 020 ----
mean loss: 131.89
 ---- batch: 030 ----
mean loss: 121.57
 ---- batch: 040 ----
mean loss: 139.93
 ---- batch: 050 ----
mean loss: 133.28
 ---- batch: 060 ----
mean loss: 136.47
 ---- batch: 070 ----
mean loss: 126.80
 ---- batch: 080 ----
mean loss: 133.05
 ---- batch: 090 ----
mean loss: 123.23
 ---- batch: 100 ----
mean loss: 135.35
 ---- batch: 110 ----
mean loss: 138.50
train mean loss: 132.11
epoch train time: 0:00:00.559190
elapsed time: 0:02:31.379025
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-27 02:12:43.258054
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.43
 ---- batch: 020 ----
mean loss: 131.88
 ---- batch: 030 ----
mean loss: 126.22
 ---- batch: 040 ----
mean loss: 130.13
 ---- batch: 050 ----
mean loss: 134.80
 ---- batch: 060 ----
mean loss: 127.09
 ---- batch: 070 ----
mean loss: 139.20
 ---- batch: 080 ----
mean loss: 132.76
 ---- batch: 090 ----
mean loss: 130.09
 ---- batch: 100 ----
mean loss: 133.65
 ---- batch: 110 ----
mean loss: 136.90
train mean loss: 132.18
epoch train time: 0:00:00.560084
elapsed time: 0:02:31.939245
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-27 02:12:43.818272
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 135.77
 ---- batch: 020 ----
mean loss: 129.89
 ---- batch: 030 ----
mean loss: 138.54
 ---- batch: 040 ----
mean loss: 135.01
 ---- batch: 050 ----
mean loss: 129.69
 ---- batch: 060 ----
mean loss: 126.71
 ---- batch: 070 ----
mean loss: 138.10
 ---- batch: 080 ----
mean loss: 128.75
 ---- batch: 090 ----
mean loss: 120.58
 ---- batch: 100 ----
mean loss: 131.82
 ---- batch: 110 ----
mean loss: 136.01
train mean loss: 132.13
epoch train time: 0:00:00.557294
elapsed time: 0:02:32.496721
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-27 02:12:44.375816
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 139.68
 ---- batch: 020 ----
mean loss: 134.69
 ---- batch: 030 ----
mean loss: 129.68
 ---- batch: 040 ----
mean loss: 129.77
 ---- batch: 050 ----
mean loss: 132.01
 ---- batch: 060 ----
mean loss: 129.75
 ---- batch: 070 ----
mean loss: 128.93
 ---- batch: 080 ----
mean loss: 134.51
 ---- batch: 090 ----
mean loss: 135.34
 ---- batch: 100 ----
mean loss: 129.80
 ---- batch: 110 ----
mean loss: 131.14
train mean loss: 132.11
epoch train time: 0:00:00.555436
elapsed time: 0:02:33.052369
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-27 02:12:44.931393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.21
 ---- batch: 020 ----
mean loss: 130.49
 ---- batch: 030 ----
mean loss: 134.46
 ---- batch: 040 ----
mean loss: 131.47
 ---- batch: 050 ----
mean loss: 130.93
 ---- batch: 060 ----
mean loss: 131.34
 ---- batch: 070 ----
mean loss: 129.74
 ---- batch: 080 ----
mean loss: 136.96
 ---- batch: 090 ----
mean loss: 131.07
 ---- batch: 100 ----
mean loss: 135.59
 ---- batch: 110 ----
mean loss: 132.82
train mean loss: 132.09
epoch train time: 0:00:00.548418
elapsed time: 0:02:33.600922
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-27 02:12:45.479945
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.30
 ---- batch: 020 ----
mean loss: 135.94
 ---- batch: 030 ----
mean loss: 135.03
 ---- batch: 040 ----
mean loss: 133.84
 ---- batch: 050 ----
mean loss: 127.23
 ---- batch: 060 ----
mean loss: 131.16
 ---- batch: 070 ----
mean loss: 131.49
 ---- batch: 080 ----
mean loss: 127.01
 ---- batch: 090 ----
mean loss: 130.51
 ---- batch: 100 ----
mean loss: 140.52
 ---- batch: 110 ----
mean loss: 128.67
train mean loss: 132.07
epoch train time: 0:00:00.547362
elapsed time: 0:02:34.148409
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-27 02:12:46.027468
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 134.44
 ---- batch: 020 ----
mean loss: 128.16
 ---- batch: 030 ----
mean loss: 128.37
 ---- batch: 040 ----
mean loss: 130.10
 ---- batch: 050 ----
mean loss: 131.50
 ---- batch: 060 ----
mean loss: 124.51
 ---- batch: 070 ----
mean loss: 132.25
 ---- batch: 080 ----
mean loss: 137.49
 ---- batch: 090 ----
mean loss: 131.19
 ---- batch: 100 ----
mean loss: 134.18
 ---- batch: 110 ----
mean loss: 140.06
train mean loss: 132.05
epoch train time: 0:00:00.556982
elapsed time: 0:02:34.705557
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-27 02:12:46.584583
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.96
 ---- batch: 020 ----
mean loss: 134.93
 ---- batch: 030 ----
mean loss: 135.71
 ---- batch: 040 ----
mean loss: 131.54
 ---- batch: 050 ----
mean loss: 136.67
 ---- batch: 060 ----
mean loss: 129.51
 ---- batch: 070 ----
mean loss: 126.76
 ---- batch: 080 ----
mean loss: 130.01
 ---- batch: 090 ----
mean loss: 130.95
 ---- batch: 100 ----
mean loss: 135.91
 ---- batch: 110 ----
mean loss: 132.32
train mean loss: 131.96
epoch train time: 0:00:00.555236
elapsed time: 0:02:35.260958
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-27 02:12:47.139982
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.23
 ---- batch: 020 ----
mean loss: 128.40
 ---- batch: 030 ----
mean loss: 137.35
 ---- batch: 040 ----
mean loss: 132.33
 ---- batch: 050 ----
mean loss: 129.20
 ---- batch: 060 ----
mean loss: 130.26
 ---- batch: 070 ----
mean loss: 139.64
 ---- batch: 080 ----
mean loss: 141.49
 ---- batch: 090 ----
mean loss: 125.81
 ---- batch: 100 ----
mean loss: 127.50
 ---- batch: 110 ----
mean loss: 135.89
train mean loss: 132.07
epoch train time: 0:00:00.548613
elapsed time: 0:02:35.809702
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-27 02:12:47.688728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.53
 ---- batch: 020 ----
mean loss: 132.85
 ---- batch: 030 ----
mean loss: 140.48
 ---- batch: 040 ----
mean loss: 126.61
 ---- batch: 050 ----
mean loss: 129.97
 ---- batch: 060 ----
mean loss: 129.93
 ---- batch: 070 ----
mean loss: 135.33
 ---- batch: 080 ----
mean loss: 127.59
 ---- batch: 090 ----
mean loss: 129.20
 ---- batch: 100 ----
mean loss: 132.32
 ---- batch: 110 ----
mean loss: 131.23
train mean loss: 132.00
epoch train time: 0:00:00.566120
elapsed time: 0:02:36.375986
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-27 02:12:48.255014
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.45
 ---- batch: 020 ----
mean loss: 131.28
 ---- batch: 030 ----
mean loss: 128.19
 ---- batch: 040 ----
mean loss: 131.71
 ---- batch: 050 ----
mean loss: 136.57
 ---- batch: 060 ----
mean loss: 131.85
 ---- batch: 070 ----
mean loss: 131.72
 ---- batch: 080 ----
mean loss: 133.15
 ---- batch: 090 ----
mean loss: 134.53
 ---- batch: 100 ----
mean loss: 131.38
 ---- batch: 110 ----
mean loss: 130.35
train mean loss: 131.96
epoch train time: 0:00:00.558065
elapsed time: 0:02:36.934204
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-27 02:12:48.813231
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.94
 ---- batch: 020 ----
mean loss: 135.20
 ---- batch: 030 ----
mean loss: 129.96
 ---- batch: 040 ----
mean loss: 131.83
 ---- batch: 050 ----
mean loss: 132.82
 ---- batch: 060 ----
mean loss: 132.14
 ---- batch: 070 ----
mean loss: 133.91
 ---- batch: 080 ----
mean loss: 140.67
 ---- batch: 090 ----
mean loss: 132.91
 ---- batch: 100 ----
mean loss: 125.04
 ---- batch: 110 ----
mean loss: 131.36
train mean loss: 131.95
epoch train time: 0:00:00.559683
elapsed time: 0:02:37.494035
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-27 02:12:49.373060
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.56
 ---- batch: 020 ----
mean loss: 127.75
 ---- batch: 030 ----
mean loss: 132.78
 ---- batch: 040 ----
mean loss: 131.83
 ---- batch: 050 ----
mean loss: 124.80
 ---- batch: 060 ----
mean loss: 136.10
 ---- batch: 070 ----
mean loss: 129.87
 ---- batch: 080 ----
mean loss: 125.52
 ---- batch: 090 ----
mean loss: 136.50
 ---- batch: 100 ----
mean loss: 140.97
 ---- batch: 110 ----
mean loss: 129.04
train mean loss: 131.97
epoch train time: 0:00:00.566465
elapsed time: 0:02:38.060646
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-27 02:12:49.939688
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.27
 ---- batch: 020 ----
mean loss: 141.52
 ---- batch: 030 ----
mean loss: 132.71
 ---- batch: 040 ----
mean loss: 127.55
 ---- batch: 050 ----
mean loss: 129.46
 ---- batch: 060 ----
mean loss: 124.47
 ---- batch: 070 ----
mean loss: 143.03
 ---- batch: 080 ----
mean loss: 124.69
 ---- batch: 090 ----
mean loss: 133.81
 ---- batch: 100 ----
mean loss: 132.77
 ---- batch: 110 ----
mean loss: 135.90
train mean loss: 131.76
epoch train time: 0:00:00.548810
elapsed time: 0:02:38.609601
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-27 02:12:50.488625
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.02
 ---- batch: 020 ----
mean loss: 130.16
 ---- batch: 030 ----
mean loss: 130.82
 ---- batch: 040 ----
mean loss: 124.38
 ---- batch: 050 ----
mean loss: 126.85
 ---- batch: 060 ----
mean loss: 136.71
 ---- batch: 070 ----
mean loss: 141.14
 ---- batch: 080 ----
mean loss: 138.05
 ---- batch: 090 ----
mean loss: 133.09
 ---- batch: 100 ----
mean loss: 134.08
 ---- batch: 110 ----
mean loss: 131.71
train mean loss: 131.83
epoch train time: 0:00:00.554066
elapsed time: 0:02:39.163815
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-27 02:12:51.042841
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.89
 ---- batch: 020 ----
mean loss: 129.84
 ---- batch: 030 ----
mean loss: 129.34
 ---- batch: 040 ----
mean loss: 138.40
 ---- batch: 050 ----
mean loss: 134.06
 ---- batch: 060 ----
mean loss: 129.83
 ---- batch: 070 ----
mean loss: 137.49
 ---- batch: 080 ----
mean loss: 133.78
 ---- batch: 090 ----
mean loss: 128.85
 ---- batch: 100 ----
mean loss: 130.88
 ---- batch: 110 ----
mean loss: 131.02
train mean loss: 131.75
epoch train time: 0:00:00.556701
elapsed time: 0:02:39.720649
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-27 02:12:51.599682
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.74
 ---- batch: 020 ----
mean loss: 134.78
 ---- batch: 030 ----
mean loss: 139.68
 ---- batch: 040 ----
mean loss: 135.53
 ---- batch: 050 ----
mean loss: 129.52
 ---- batch: 060 ----
mean loss: 138.05
 ---- batch: 070 ----
mean loss: 128.62
 ---- batch: 080 ----
mean loss: 120.43
 ---- batch: 090 ----
mean loss: 133.44
 ---- batch: 100 ----
mean loss: 133.34
 ---- batch: 110 ----
mean loss: 133.26
train mean loss: 131.71
epoch train time: 0:00:00.558023
elapsed time: 0:02:40.278810
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-27 02:12:52.157835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 136.51
 ---- batch: 020 ----
mean loss: 127.72
 ---- batch: 030 ----
mean loss: 126.26
 ---- batch: 040 ----
mean loss: 144.03
 ---- batch: 050 ----
mean loss: 127.90
 ---- batch: 060 ----
mean loss: 132.48
 ---- batch: 070 ----
mean loss: 130.56
 ---- batch: 080 ----
mean loss: 132.30
 ---- batch: 090 ----
mean loss: 131.22
 ---- batch: 100 ----
mean loss: 130.94
 ---- batch: 110 ----
mean loss: 131.21
train mean loss: 132.02
epoch train time: 0:00:00.551745
elapsed time: 0:02:40.830702
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-27 02:12:52.709728
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 125.13
 ---- batch: 020 ----
mean loss: 126.16
 ---- batch: 030 ----
mean loss: 128.45
 ---- batch: 040 ----
mean loss: 137.83
 ---- batch: 050 ----
mean loss: 132.63
 ---- batch: 060 ----
mean loss: 123.52
 ---- batch: 070 ----
mean loss: 134.50
 ---- batch: 080 ----
mean loss: 137.94
 ---- batch: 090 ----
mean loss: 127.81
 ---- batch: 100 ----
mean loss: 142.63
 ---- batch: 110 ----
mean loss: 136.24
train mean loss: 131.70
epoch train time: 0:00:00.557668
elapsed time: 0:02:41.388535
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-27 02:12:53.267578
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.52
 ---- batch: 020 ----
mean loss: 129.26
 ---- batch: 030 ----
mean loss: 133.31
 ---- batch: 040 ----
mean loss: 132.47
 ---- batch: 050 ----
mean loss: 125.32
 ---- batch: 060 ----
mean loss: 138.30
 ---- batch: 070 ----
mean loss: 134.75
 ---- batch: 080 ----
mean loss: 138.41
 ---- batch: 090 ----
mean loss: 134.41
 ---- batch: 100 ----
mean loss: 124.80
 ---- batch: 110 ----
mean loss: 128.30
train mean loss: 131.74
epoch train time: 0:00:00.563183
elapsed time: 0:02:41.951903
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-27 02:12:53.830972
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.67
 ---- batch: 020 ----
mean loss: 126.72
 ---- batch: 030 ----
mean loss: 130.65
 ---- batch: 040 ----
mean loss: 135.33
 ---- batch: 050 ----
mean loss: 129.31
 ---- batch: 060 ----
mean loss: 135.82
 ---- batch: 070 ----
mean loss: 135.97
 ---- batch: 080 ----
mean loss: 133.25
 ---- batch: 090 ----
mean loss: 130.29
 ---- batch: 100 ----
mean loss: 134.15
 ---- batch: 110 ----
mean loss: 127.31
train mean loss: 131.71
epoch train time: 0:00:00.554451
elapsed time: 0:02:42.506554
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-27 02:12:54.385573
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.67
 ---- batch: 020 ----
mean loss: 131.71
 ---- batch: 030 ----
mean loss: 126.23
 ---- batch: 040 ----
mean loss: 130.89
 ---- batch: 050 ----
mean loss: 127.45
 ---- batch: 060 ----
mean loss: 136.17
 ---- batch: 070 ----
mean loss: 131.30
 ---- batch: 080 ----
mean loss: 133.62
 ---- batch: 090 ----
mean loss: 134.63
 ---- batch: 100 ----
mean loss: 134.68
 ---- batch: 110 ----
mean loss: 132.57
train mean loss: 131.61
epoch train time: 0:00:00.558088
elapsed time: 0:02:43.064766
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-27 02:12:54.943794
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 131.12
 ---- batch: 020 ----
mean loss: 125.68
 ---- batch: 030 ----
mean loss: 123.11
 ---- batch: 040 ----
mean loss: 142.10
 ---- batch: 050 ----
mean loss: 126.47
 ---- batch: 060 ----
mean loss: 136.27
 ---- batch: 070 ----
mean loss: 136.33
 ---- batch: 080 ----
mean loss: 128.04
 ---- batch: 090 ----
mean loss: 134.23
 ---- batch: 100 ----
mean loss: 134.25
 ---- batch: 110 ----
mean loss: 127.11
train mean loss: 131.67
epoch train time: 0:00:00.553411
elapsed time: 0:02:43.618312
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-27 02:12:55.497337
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.82
 ---- batch: 020 ----
mean loss: 131.34
 ---- batch: 030 ----
mean loss: 131.64
 ---- batch: 040 ----
mean loss: 133.85
 ---- batch: 050 ----
mean loss: 133.03
 ---- batch: 060 ----
mean loss: 143.01
 ---- batch: 070 ----
mean loss: 131.81
 ---- batch: 080 ----
mean loss: 126.98
 ---- batch: 090 ----
mean loss: 125.42
 ---- batch: 100 ----
mean loss: 132.76
 ---- batch: 110 ----
mean loss: 126.94
train mean loss: 131.56
epoch train time: 0:00:00.560208
elapsed time: 0:02:44.178674
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-27 02:12:56.057703
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 133.84
 ---- batch: 020 ----
mean loss: 140.82
 ---- batch: 030 ----
mean loss: 128.43
 ---- batch: 040 ----
mean loss: 129.46
 ---- batch: 050 ----
mean loss: 127.44
 ---- batch: 060 ----
mean loss: 138.89
 ---- batch: 070 ----
mean loss: 125.09
 ---- batch: 080 ----
mean loss: 131.97
 ---- batch: 090 ----
mean loss: 132.32
 ---- batch: 100 ----
mean loss: 130.78
 ---- batch: 110 ----
mean loss: 127.16
train mean loss: 131.58
epoch train time: 0:00:00.561080
elapsed time: 0:02:44.739890
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-27 02:12:56.618917
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 132.91
 ---- batch: 020 ----
mean loss: 136.95
 ---- batch: 030 ----
mean loss: 128.25
 ---- batch: 040 ----
mean loss: 132.55
 ---- batch: 050 ----
mean loss: 132.39
 ---- batch: 060 ----
mean loss: 132.86
 ---- batch: 070 ----
mean loss: 138.60
 ---- batch: 080 ----
mean loss: 121.44
 ---- batch: 090 ----
mean loss: 126.04
 ---- batch: 100 ----
mean loss: 136.87
 ---- batch: 110 ----
mean loss: 129.74
train mean loss: 131.46
epoch train time: 0:00:00.567807
elapsed time: 0:02:45.307832
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-27 02:12:57.186910
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.64
 ---- batch: 020 ----
mean loss: 134.39
 ---- batch: 030 ----
mean loss: 135.13
 ---- batch: 040 ----
mean loss: 134.07
 ---- batch: 050 ----
mean loss: 133.58
 ---- batch: 060 ----
mean loss: 141.36
 ---- batch: 070 ----
mean loss: 134.37
 ---- batch: 080 ----
mean loss: 135.49
 ---- batch: 090 ----
mean loss: 127.23
 ---- batch: 100 ----
mean loss: 128.85
 ---- batch: 110 ----
mean loss: 120.46
train mean loss: 131.59
epoch train time: 0:00:00.571516
elapsed time: 0:02:45.879584
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-27 02:12:57.758613
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.20
 ---- batch: 020 ----
mean loss: 128.73
 ---- batch: 030 ----
mean loss: 132.56
 ---- batch: 040 ----
mean loss: 128.70
 ---- batch: 050 ----
mean loss: 127.85
 ---- batch: 060 ----
mean loss: 126.34
 ---- batch: 070 ----
mean loss: 133.58
 ---- batch: 080 ----
mean loss: 133.16
 ---- batch: 090 ----
mean loss: 149.21
 ---- batch: 100 ----
mean loss: 126.99
 ---- batch: 110 ----
mean loss: 133.78
train mean loss: 131.55
epoch train time: 0:00:00.554094
elapsed time: 0:02:46.433811
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-27 02:12:58.312836
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.23
 ---- batch: 020 ----
mean loss: 129.73
 ---- batch: 030 ----
mean loss: 127.48
 ---- batch: 040 ----
mean loss: 127.76
 ---- batch: 050 ----
mean loss: 135.47
 ---- batch: 060 ----
mean loss: 135.36
 ---- batch: 070 ----
mean loss: 134.31
 ---- batch: 080 ----
mean loss: 131.13
 ---- batch: 090 ----
mean loss: 132.89
 ---- batch: 100 ----
mean loss: 134.13
 ---- batch: 110 ----
mean loss: 130.03
train mean loss: 131.45
epoch train time: 0:00:00.568505
elapsed time: 0:02:47.002447
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-27 02:12:58.881472
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.01
 ---- batch: 020 ----
mean loss: 137.69
 ---- batch: 030 ----
mean loss: 137.60
 ---- batch: 040 ----
mean loss: 124.42
 ---- batch: 050 ----
mean loss: 127.56
 ---- batch: 060 ----
mean loss: 137.19
 ---- batch: 070 ----
mean loss: 133.38
 ---- batch: 080 ----
mean loss: 119.36
 ---- batch: 090 ----
mean loss: 128.91
 ---- batch: 100 ----
mean loss: 137.71
 ---- batch: 110 ----
mean loss: 135.25
train mean loss: 131.57
epoch train time: 0:00:00.557140
elapsed time: 0:02:47.559732
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-27 02:12:59.438760
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 123.03
 ---- batch: 020 ----
mean loss: 139.90
 ---- batch: 030 ----
mean loss: 128.61
 ---- batch: 040 ----
mean loss: 136.86
 ---- batch: 050 ----
mean loss: 136.71
 ---- batch: 060 ----
mean loss: 127.51
 ---- batch: 070 ----
mean loss: 136.06
 ---- batch: 080 ----
mean loss: 122.42
 ---- batch: 090 ----
mean loss: 128.55
 ---- batch: 100 ----
mean loss: 136.18
 ---- batch: 110 ----
mean loss: 129.42
train mean loss: 131.39
epoch train time: 0:00:00.563566
elapsed time: 0:02:48.123430
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-27 02:13:00.002478
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 128.67
 ---- batch: 020 ----
mean loss: 126.04
 ---- batch: 030 ----
mean loss: 128.48
 ---- batch: 040 ----
mean loss: 136.16
 ---- batch: 050 ----
mean loss: 133.87
 ---- batch: 060 ----
mean loss: 129.32
 ---- batch: 070 ----
mean loss: 134.99
 ---- batch: 080 ----
mean loss: 134.01
 ---- batch: 090 ----
mean loss: 127.74
 ---- batch: 100 ----
mean loss: 137.02
 ---- batch: 110 ----
mean loss: 131.37
train mean loss: 131.37
epoch train time: 0:00:00.547548
elapsed time: 0:02:48.671135
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-27 02:13:00.550162
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 130.80
 ---- batch: 020 ----
mean loss: 128.13
 ---- batch: 030 ----
mean loss: 136.20
 ---- batch: 040 ----
mean loss: 131.00
 ---- batch: 050 ----
mean loss: 127.22
 ---- batch: 060 ----
mean loss: 131.31
 ---- batch: 070 ----
mean loss: 132.02
 ---- batch: 080 ----
mean loss: 127.18
 ---- batch: 090 ----
mean loss: 131.60
 ---- batch: 100 ----
mean loss: 136.91
 ---- batch: 110 ----
mean loss: 136.83
train mean loss: 131.38
epoch train time: 0:00:00.557332
elapsed time: 0:02:49.228599
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-27 02:13:01.107653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 138.04
 ---- batch: 020 ----
mean loss: 133.12
 ---- batch: 030 ----
mean loss: 127.42
 ---- batch: 040 ----
mean loss: 127.74
 ---- batch: 050 ----
mean loss: 133.14
 ---- batch: 060 ----
mean loss: 128.29
 ---- batch: 070 ----
mean loss: 134.35
 ---- batch: 080 ----
mean loss: 130.72
 ---- batch: 090 ----
mean loss: 130.39
 ---- batch: 100 ----
mean loss: 132.82
 ---- batch: 110 ----
mean loss: 126.18
train mean loss: 131.36
epoch train time: 0:00:00.555978
elapsed time: 0:02:49.784737
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-27 02:13:01.663763
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.41
 ---- batch: 020 ----
mean loss: 130.57
 ---- batch: 030 ----
mean loss: 130.27
 ---- batch: 040 ----
mean loss: 129.97
 ---- batch: 050 ----
mean loss: 130.57
 ---- batch: 060 ----
mean loss: 132.01
 ---- batch: 070 ----
mean loss: 134.28
 ---- batch: 080 ----
mean loss: 127.24
 ---- batch: 090 ----
mean loss: 132.72
 ---- batch: 100 ----
mean loss: 134.72
 ---- batch: 110 ----
mean loss: 141.31
train mean loss: 131.26
epoch train time: 0:00:00.554284
elapsed time: 0:02:50.339157
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-27 02:13:02.218182
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 127.99
 ---- batch: 020 ----
mean loss: 129.25
 ---- batch: 030 ----
mean loss: 130.95
 ---- batch: 040 ----
mean loss: 130.14
 ---- batch: 050 ----
mean loss: 128.43
 ---- batch: 060 ----
mean loss: 130.17
 ---- batch: 070 ----
mean loss: 130.02
 ---- batch: 080 ----
mean loss: 130.43
 ---- batch: 090 ----
mean loss: 136.76
 ---- batch: 100 ----
mean loss: 137.27
 ---- batch: 110 ----
mean loss: 131.65
train mean loss: 131.31
epoch train time: 0:00:00.560554
elapsed time: 0:02:50.899839
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-27 02:13:02.778863
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 129.66
 ---- batch: 020 ----
mean loss: 131.02
 ---- batch: 030 ----
mean loss: 126.08
 ---- batch: 040 ----
mean loss: 129.39
 ---- batch: 050 ----
mean loss: 129.91
 ---- batch: 060 ----
mean loss: 134.23
 ---- batch: 070 ----
mean loss: 127.42
 ---- batch: 080 ----
mean loss: 133.59
 ---- batch: 090 ----
mean loss: 133.20
 ---- batch: 100 ----
mean loss: 134.88
 ---- batch: 110 ----
mean loss: 134.94
train mean loss: 131.31
epoch train time: 0:00:00.559864
elapsed time: 0:02:51.463049
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_dense3/frequentist_dense3_2/checkpoint.pth.tar
**** end time: 2019-09-27 02:13:03.342045 ****
