Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 27345
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-26 01:46:30.689774 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 16, 24]             200
           Sigmoid-2           [-1, 10, 16, 24]               0
    BayesianConv2d-3           [-1, 10, 15, 24]           2,000
           Sigmoid-4           [-1, 10, 15, 24]               0
    BayesianConv2d-5           [-1, 10, 16, 24]           2,000
           Sigmoid-6           [-1, 10, 16, 24]               0
    BayesianConv2d-7           [-1, 10, 15, 24]           2,000
           Sigmoid-8           [-1, 10, 15, 24]               0
    BayesianConv2d-9            [-1, 1, 15, 24]              60
         Softplus-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
   BayesianLinear-12                  [-1, 100]          72,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 78,460
Trainable params: 78,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 01:46:30.709191
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3254.87
 ---- batch: 020 ----
mean loss: 1294.77
 ---- batch: 030 ----
mean loss: 1142.32
 ---- batch: 040 ----
mean loss: 1096.00
 ---- batch: 050 ----
mean loss: 1080.38
 ---- batch: 060 ----
mean loss: 1038.88
 ---- batch: 070 ----
mean loss: 1031.13
 ---- batch: 080 ----
mean loss: 1014.41
 ---- batch: 090 ----
mean loss: 1003.62
 ---- batch: 100 ----
mean loss: 981.06
 ---- batch: 110 ----
mean loss: 959.41
train mean loss: 1255.91
epoch train time: 0:00:46.097921
elapsed time: 0:00:46.125901
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 01:47:16.815722
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1000.37
 ---- batch: 020 ----
mean loss: 985.80
 ---- batch: 030 ----
mean loss: 945.99
 ---- batch: 040 ----
mean loss: 947.01
 ---- batch: 050 ----
mean loss: 944.69
 ---- batch: 060 ----
mean loss: 920.71
 ---- batch: 070 ----
mean loss: 976.64
 ---- batch: 080 ----
mean loss: 919.66
 ---- batch: 090 ----
mean loss: 948.05
 ---- batch: 100 ----
mean loss: 958.60
 ---- batch: 110 ----
mean loss: 937.09
train mean loss: 952.64
epoch train time: 0:00:15.627925
elapsed time: 0:01:01.754299
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 01:47:32.444598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 916.59
 ---- batch: 020 ----
mean loss: 934.47
 ---- batch: 030 ----
mean loss: 918.08
 ---- batch: 040 ----
mean loss: 925.68
 ---- batch: 050 ----
mean loss: 896.62
 ---- batch: 060 ----
mean loss: 901.84
 ---- batch: 070 ----
mean loss: 920.08
 ---- batch: 080 ----
mean loss: 920.54
 ---- batch: 090 ----
mean loss: 917.89
 ---- batch: 100 ----
mean loss: 908.67
 ---- batch: 110 ----
mean loss: 923.59
train mean loss: 915.62
epoch train time: 0:00:15.654142
elapsed time: 0:01:17.409387
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 01:47:48.099665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 922.78
 ---- batch: 020 ----
mean loss: 923.04
 ---- batch: 030 ----
mean loss: 915.75
 ---- batch: 040 ----
mean loss: 901.43
 ---- batch: 050 ----
mean loss: 898.66
 ---- batch: 060 ----
mean loss: 894.32
 ---- batch: 070 ----
mean loss: 907.94
 ---- batch: 080 ----
mean loss: 855.38
 ---- batch: 090 ----
mean loss: 914.23
 ---- batch: 100 ----
mean loss: 919.50
 ---- batch: 110 ----
mean loss: 907.91
train mean loss: 904.67
epoch train time: 0:00:15.855747
elapsed time: 0:01:33.266267
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 01:48:03.956779
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.84
 ---- batch: 020 ----
mean loss: 906.78
 ---- batch: 030 ----
mean loss: 895.83
 ---- batch: 040 ----
mean loss: 904.88
 ---- batch: 050 ----
mean loss: 890.03
 ---- batch: 060 ----
mean loss: 881.59
 ---- batch: 070 ----
mean loss: 881.83
 ---- batch: 080 ----
mean loss: 878.75
 ---- batch: 090 ----
mean loss: 893.63
 ---- batch: 100 ----
mean loss: 901.26
 ---- batch: 110 ----
mean loss: 887.02
train mean loss: 891.39
epoch train time: 0:00:16.040303
elapsed time: 0:01:49.308047
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 01:48:19.998610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 884.91
 ---- batch: 020 ----
mean loss: 880.16
 ---- batch: 030 ----
mean loss: 896.48
 ---- batch: 040 ----
mean loss: 867.67
 ---- batch: 050 ----
mean loss: 886.49
 ---- batch: 060 ----
mean loss: 903.23
 ---- batch: 070 ----
mean loss: 874.78
 ---- batch: 080 ----
mean loss: 900.35
 ---- batch: 090 ----
mean loss: 879.94
 ---- batch: 100 ----
mean loss: 877.98
 ---- batch: 110 ----
mean loss: 878.95
train mean loss: 884.90
epoch train time: 0:00:15.849182
elapsed time: 0:02:05.158728
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 01:48:35.849460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.78
 ---- batch: 020 ----
mean loss: 862.99
 ---- batch: 030 ----
mean loss: 869.60
 ---- batch: 040 ----
mean loss: 876.93
 ---- batch: 050 ----
mean loss: 853.73
 ---- batch: 060 ----
mean loss: 887.88
 ---- batch: 070 ----
mean loss: 871.38
 ---- batch: 080 ----
mean loss: 891.01
 ---- batch: 090 ----
mean loss: 885.46
 ---- batch: 100 ----
mean loss: 892.05
 ---- batch: 110 ----
mean loss: 874.80
train mean loss: 877.96
epoch train time: 0:00:16.014550
elapsed time: 0:02:21.174745
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 01:48:51.865034
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 862.43
 ---- batch: 020 ----
mean loss: 857.98
 ---- batch: 030 ----
mean loss: 887.33
 ---- batch: 040 ----
mean loss: 867.94
 ---- batch: 050 ----
mean loss: 861.98
 ---- batch: 060 ----
mean loss: 882.37
 ---- batch: 070 ----
mean loss: 838.75
 ---- batch: 080 ----
mean loss: 859.99
 ---- batch: 090 ----
mean loss: 863.85
 ---- batch: 100 ----
mean loss: 871.60
 ---- batch: 110 ----
mean loss: 876.61
train mean loss: 866.23
epoch train time: 0:00:15.813496
elapsed time: 0:02:36.989155
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 01:49:07.679648
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 883.35
 ---- batch: 020 ----
mean loss: 880.00
 ---- batch: 030 ----
mean loss: 851.14
 ---- batch: 040 ----
mean loss: 860.78
 ---- batch: 050 ----
mean loss: 868.46
 ---- batch: 060 ----
mean loss: 862.33
 ---- batch: 070 ----
mean loss: 878.47
 ---- batch: 080 ----
mean loss: 857.43
 ---- batch: 090 ----
mean loss: 857.74
 ---- batch: 100 ----
mean loss: 888.22
 ---- batch: 110 ----
mean loss: 870.12
train mean loss: 868.22
epoch train time: 0:00:16.037244
elapsed time: 0:02:53.027741
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 01:49:23.718260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.82
 ---- batch: 020 ----
mean loss: 882.06
 ---- batch: 030 ----
mean loss: 836.01
 ---- batch: 040 ----
mean loss: 873.36
 ---- batch: 050 ----
mean loss: 852.06
 ---- batch: 060 ----
mean loss: 862.51
 ---- batch: 070 ----
mean loss: 849.07
 ---- batch: 080 ----
mean loss: 887.79
 ---- batch: 090 ----
mean loss: 850.64
 ---- batch: 100 ----
mean loss: 838.88
 ---- batch: 110 ----
mean loss: 855.67
train mean loss: 857.54
epoch train time: 0:00:15.899942
elapsed time: 0:03:08.929269
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 01:49:39.619581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.17
 ---- batch: 020 ----
mean loss: 836.29
 ---- batch: 030 ----
mean loss: 850.89
 ---- batch: 040 ----
mean loss: 869.87
 ---- batch: 050 ----
mean loss: 844.07
 ---- batch: 060 ----
mean loss: 839.38
 ---- batch: 070 ----
mean loss: 853.58
 ---- batch: 080 ----
mean loss: 843.95
 ---- batch: 090 ----
mean loss: 862.07
 ---- batch: 100 ----
mean loss: 839.39
 ---- batch: 110 ----
mean loss: 841.78
train mean loss: 849.18
epoch train time: 0:00:16.091355
elapsed time: 0:03:25.021724
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 01:49:55.712160
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.60
 ---- batch: 020 ----
mean loss: 820.97
 ---- batch: 030 ----
mean loss: 853.49
 ---- batch: 040 ----
mean loss: 853.10
 ---- batch: 050 ----
mean loss: 850.14
 ---- batch: 060 ----
mean loss: 839.98
 ---- batch: 070 ----
mean loss: 837.93
 ---- batch: 080 ----
mean loss: 821.58
 ---- batch: 090 ----
mean loss: 836.44
 ---- batch: 100 ----
mean loss: 835.07
 ---- batch: 110 ----
mean loss: 802.98
train mean loss: 839.40
epoch train time: 0:00:15.914604
elapsed time: 0:03:40.937362
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 01:50:11.627711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.19
 ---- batch: 020 ----
mean loss: 836.57
 ---- batch: 030 ----
mean loss: 832.67
 ---- batch: 040 ----
mean loss: 829.04
 ---- batch: 050 ----
mean loss: 826.71
 ---- batch: 060 ----
mean loss: 815.77
 ---- batch: 070 ----
mean loss: 830.26
 ---- batch: 080 ----
mean loss: 797.00
 ---- batch: 090 ----
mean loss: 814.03
 ---- batch: 100 ----
mean loss: 827.77
 ---- batch: 110 ----
mean loss: 795.24
train mean loss: 822.76
epoch train time: 0:00:15.989059
elapsed time: 0:03:56.927584
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 01:50:27.618127
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 785.06
 ---- batch: 020 ----
mean loss: 813.14
 ---- batch: 030 ----
mean loss: 795.27
 ---- batch: 040 ----
mean loss: 790.35
 ---- batch: 050 ----
mean loss: 795.08
 ---- batch: 060 ----
mean loss: 804.79
 ---- batch: 070 ----
mean loss: 804.38
 ---- batch: 080 ----
mean loss: 786.05
 ---- batch: 090 ----
mean loss: 786.87
 ---- batch: 100 ----
mean loss: 792.86
 ---- batch: 110 ----
mean loss: 803.79
train mean loss: 796.83
epoch train time: 0:00:15.904881
elapsed time: 0:04:12.833666
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 01:50:43.523934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 780.56
 ---- batch: 020 ----
mean loss: 779.56
 ---- batch: 030 ----
mean loss: 773.27
 ---- batch: 040 ----
mean loss: 781.26
 ---- batch: 050 ----
mean loss: 770.13
 ---- batch: 060 ----
mean loss: 767.05
 ---- batch: 070 ----
mean loss: 765.75
 ---- batch: 080 ----
mean loss: 752.48
 ---- batch: 090 ----
mean loss: 759.61
 ---- batch: 100 ----
mean loss: 757.89
 ---- batch: 110 ----
mean loss: 773.91
train mean loss: 768.79
epoch train time: 0:00:15.944432
elapsed time: 0:04:28.779182
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 01:50:59.469559
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 756.15
 ---- batch: 020 ----
mean loss: 758.32
 ---- batch: 030 ----
mean loss: 746.46
 ---- batch: 040 ----
mean loss: 746.78
 ---- batch: 050 ----
mean loss: 746.53
 ---- batch: 060 ----
mean loss: 758.45
 ---- batch: 070 ----
mean loss: 738.27
 ---- batch: 080 ----
mean loss: 724.20
 ---- batch: 090 ----
mean loss: 722.95
 ---- batch: 100 ----
mean loss: 725.50
 ---- batch: 110 ----
mean loss: 716.66
train mean loss: 740.46
epoch train time: 0:00:16.006424
elapsed time: 0:04:44.786602
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 01:51:15.476915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.90
 ---- batch: 020 ----
mean loss: 681.92
 ---- batch: 030 ----
mean loss: 699.42
 ---- batch: 040 ----
mean loss: 715.59
 ---- batch: 050 ----
mean loss: 709.74
 ---- batch: 060 ----
mean loss: 702.01
 ---- batch: 070 ----
mean loss: 692.00
 ---- batch: 080 ----
mean loss: 675.22
 ---- batch: 090 ----
mean loss: 663.18
 ---- batch: 100 ----
mean loss: 658.66
 ---- batch: 110 ----
mean loss: 664.63
train mean loss: 687.34
epoch train time: 0:00:15.840740
elapsed time: 0:05:00.628484
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 01:51:31.318884
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 643.99
 ---- batch: 020 ----
mean loss: 655.60
 ---- batch: 030 ----
mean loss: 627.69
 ---- batch: 040 ----
mean loss: 645.85
 ---- batch: 050 ----
mean loss: 638.70
 ---- batch: 060 ----
mean loss: 630.44
 ---- batch: 070 ----
mean loss: 625.30
 ---- batch: 080 ----
mean loss: 606.99
 ---- batch: 090 ----
mean loss: 597.27
 ---- batch: 100 ----
mean loss: 604.04
 ---- batch: 110 ----
mean loss: 590.01
train mean loss: 622.14
epoch train time: 0:00:16.112213
elapsed time: 0:05:16.741761
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 01:51:47.432052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 580.97
 ---- batch: 020 ----
mean loss: 585.35
 ---- batch: 030 ----
mean loss: 578.88
 ---- batch: 040 ----
mean loss: 556.44
 ---- batch: 050 ----
mean loss: 569.08
 ---- batch: 060 ----
mean loss: 554.17
 ---- batch: 070 ----
mean loss: 565.32
 ---- batch: 080 ----
mean loss: 572.37
 ---- batch: 090 ----
mean loss: 541.21
 ---- batch: 100 ----
mean loss: 566.90
 ---- batch: 110 ----
mean loss: 569.67
train mean loss: 566.28
epoch train time: 0:00:15.885917
elapsed time: 0:05:32.628807
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 01:52:03.319399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 551.04
 ---- batch: 020 ----
mean loss: 530.65
 ---- batch: 030 ----
mean loss: 549.68
 ---- batch: 040 ----
mean loss: 548.58
 ---- batch: 050 ----
mean loss: 524.97
 ---- batch: 060 ----
mean loss: 538.12
 ---- batch: 070 ----
mean loss: 533.28
 ---- batch: 080 ----
mean loss: 528.22
 ---- batch: 090 ----
mean loss: 517.59
 ---- batch: 100 ----
mean loss: 525.05
 ---- batch: 110 ----
mean loss: 518.10
train mean loss: 532.76
epoch train time: 0:00:16.078180
elapsed time: 0:05:48.708272
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 01:52:19.398552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.39
 ---- batch: 020 ----
mean loss: 507.66
 ---- batch: 030 ----
mean loss: 505.17
 ---- batch: 040 ----
mean loss: 512.55
 ---- batch: 050 ----
mean loss: 509.12
 ---- batch: 060 ----
mean loss: 491.35
 ---- batch: 070 ----
mean loss: 495.53
 ---- batch: 080 ----
mean loss: 495.44
 ---- batch: 090 ----
mean loss: 488.72
 ---- batch: 100 ----
mean loss: 467.95
 ---- batch: 110 ----
mean loss: 476.66
train mean loss: 494.03
epoch train time: 0:00:15.955426
elapsed time: 0:06:04.664658
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 01:52:35.355012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 466.53
 ---- batch: 020 ----
mean loss: 491.05
 ---- batch: 030 ----
mean loss: 483.44
 ---- batch: 040 ----
mean loss: 476.94
 ---- batch: 050 ----
mean loss: 483.38
 ---- batch: 060 ----
mean loss: 470.34
 ---- batch: 070 ----
mean loss: 468.56
 ---- batch: 080 ----
mean loss: 456.47
 ---- batch: 090 ----
mean loss: 451.42
 ---- batch: 100 ----
mean loss: 457.47
 ---- batch: 110 ----
mean loss: 446.76
train mean loss: 468.26
epoch train time: 0:00:16.096028
elapsed time: 0:06:20.761673
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 01:52:51.452045
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 445.99
 ---- batch: 020 ----
mean loss: 441.90
 ---- batch: 030 ----
mean loss: 452.31
 ---- batch: 040 ----
mean loss: 440.31
 ---- batch: 050 ----
mean loss: 441.26
 ---- batch: 060 ----
mean loss: 444.02
 ---- batch: 070 ----
mean loss: 450.34
 ---- batch: 080 ----
mean loss: 445.18
 ---- batch: 090 ----
mean loss: 429.96
 ---- batch: 100 ----
mean loss: 451.44
 ---- batch: 110 ----
mean loss: 437.36
train mean loss: 443.99
epoch train time: 0:00:15.907690
elapsed time: 0:06:36.670496
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 01:53:07.360888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.51
 ---- batch: 020 ----
mean loss: 435.66
 ---- batch: 030 ----
mean loss: 435.72
 ---- batch: 040 ----
mean loss: 428.08
 ---- batch: 050 ----
mean loss: 434.84
 ---- batch: 060 ----
mean loss: 429.90
 ---- batch: 070 ----
mean loss: 428.29
 ---- batch: 080 ----
mean loss: 426.05
 ---- batch: 090 ----
mean loss: 416.84
 ---- batch: 100 ----
mean loss: 426.58
 ---- batch: 110 ----
mean loss: 417.46
train mean loss: 426.92
epoch train time: 0:00:16.161110
elapsed time: 0:06:52.832755
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 01:53:23.523029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 417.36
 ---- batch: 020 ----
mean loss: 412.43
 ---- batch: 030 ----
mean loss: 400.39
 ---- batch: 040 ----
mean loss: 411.50
 ---- batch: 050 ----
mean loss: 402.23
 ---- batch: 060 ----
mean loss: 400.44
 ---- batch: 070 ----
mean loss: 406.35
 ---- batch: 080 ----
mean loss: 405.30
 ---- batch: 090 ----
mean loss: 415.36
 ---- batch: 100 ----
mean loss: 405.79
 ---- batch: 110 ----
mean loss: 413.24
train mean loss: 407.65
epoch train time: 0:00:15.984527
elapsed time: 0:07:08.818488
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 01:53:39.508949
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 395.39
 ---- batch: 020 ----
mean loss: 410.40
 ---- batch: 030 ----
mean loss: 390.06
 ---- batch: 040 ----
mean loss: 397.33
 ---- batch: 050 ----
mean loss: 401.15
 ---- batch: 060 ----
mean loss: 391.27
 ---- batch: 070 ----
mean loss: 386.34
 ---- batch: 080 ----
mean loss: 399.41
 ---- batch: 090 ----
mean loss: 381.36
 ---- batch: 100 ----
mean loss: 384.38
 ---- batch: 110 ----
mean loss: 385.55
train mean loss: 392.65
epoch train time: 0:00:16.115441
elapsed time: 0:07:24.935226
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 01:53:55.625761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.31
 ---- batch: 020 ----
mean loss: 384.60
 ---- batch: 030 ----
mean loss: 385.17
 ---- batch: 040 ----
mean loss: 387.71
 ---- batch: 050 ----
mean loss: 375.66
 ---- batch: 060 ----
mean loss: 371.34
 ---- batch: 070 ----
mean loss: 376.22
 ---- batch: 080 ----
mean loss: 386.99
 ---- batch: 090 ----
mean loss: 384.85
 ---- batch: 100 ----
mean loss: 373.04
 ---- batch: 110 ----
mean loss: 373.05
train mean loss: 381.01
epoch train time: 0:00:15.968686
elapsed time: 0:07:40.905062
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 01:54:11.595590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.52
 ---- batch: 020 ----
mean loss: 371.74
 ---- batch: 030 ----
mean loss: 379.03
 ---- batch: 040 ----
mean loss: 370.92
 ---- batch: 050 ----
mean loss: 367.29
 ---- batch: 060 ----
mean loss: 363.60
 ---- batch: 070 ----
mean loss: 364.40
 ---- batch: 080 ----
mean loss: 368.54
 ---- batch: 090 ----
mean loss: 359.97
 ---- batch: 100 ----
mean loss: 364.90
 ---- batch: 110 ----
mean loss: 369.89
train mean loss: 367.75
epoch train time: 0:00:16.107642
elapsed time: 0:07:57.014075
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 01:54:27.704560
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.72
 ---- batch: 020 ----
mean loss: 363.87
 ---- batch: 030 ----
mean loss: 355.17
 ---- batch: 040 ----
mean loss: 367.19
 ---- batch: 050 ----
mean loss: 367.65
 ---- batch: 060 ----
mean loss: 358.11
 ---- batch: 070 ----
mean loss: 361.87
 ---- batch: 080 ----
mean loss: 366.10
 ---- batch: 090 ----
mean loss: 358.53
 ---- batch: 100 ----
mean loss: 359.90
 ---- batch: 110 ----
mean loss: 362.11
train mean loss: 361.06
epoch train time: 0:00:15.893464
elapsed time: 0:08:12.909159
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 01:54:43.599995
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 351.90
 ---- batch: 020 ----
mean loss: 356.64
 ---- batch: 030 ----
mean loss: 351.35
 ---- batch: 040 ----
mean loss: 360.61
 ---- batch: 050 ----
mean loss: 358.91
 ---- batch: 060 ----
mean loss: 342.51
 ---- batch: 070 ----
mean loss: 352.62
 ---- batch: 080 ----
mean loss: 340.55
 ---- batch: 090 ----
mean loss: 342.81
 ---- batch: 100 ----
mean loss: 354.74
 ---- batch: 110 ----
mean loss: 357.16
train mean loss: 351.27
epoch train time: 0:00:15.977743
elapsed time: 0:08:28.888366
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 01:54:59.578747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 341.01
 ---- batch: 020 ----
mean loss: 349.33
 ---- batch: 030 ----
mean loss: 349.19
 ---- batch: 040 ----
mean loss: 342.42
 ---- batch: 050 ----
mean loss: 343.82
 ---- batch: 060 ----
mean loss: 352.95
 ---- batch: 070 ----
mean loss: 328.30
 ---- batch: 080 ----
mean loss: 349.54
 ---- batch: 090 ----
mean loss: 347.21
 ---- batch: 100 ----
mean loss: 340.77
 ---- batch: 110 ----
mean loss: 350.81
train mean loss: 345.20
epoch train time: 0:00:15.784601
elapsed time: 0:08:44.674146
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 01:55:15.364442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 343.79
 ---- batch: 020 ----
mean loss: 351.08
 ---- batch: 030 ----
mean loss: 345.24
 ---- batch: 040 ----
mean loss: 331.53
 ---- batch: 050 ----
mean loss: 332.20
 ---- batch: 060 ----
mean loss: 351.00
 ---- batch: 070 ----
mean loss: 321.77
 ---- batch: 080 ----
mean loss: 336.85
 ---- batch: 090 ----
mean loss: 336.60
 ---- batch: 100 ----
mean loss: 334.60
 ---- batch: 110 ----
mean loss: 340.77
train mean loss: 338.66
epoch train time: 0:00:16.123039
elapsed time: 0:09:00.798152
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 01:55:31.488427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.83
 ---- batch: 020 ----
mean loss: 331.30
 ---- batch: 030 ----
mean loss: 333.28
 ---- batch: 040 ----
mean loss: 326.11
 ---- batch: 050 ----
mean loss: 329.80
 ---- batch: 060 ----
mean loss: 338.70
 ---- batch: 070 ----
mean loss: 326.57
 ---- batch: 080 ----
mean loss: 346.12
 ---- batch: 090 ----
mean loss: 329.41
 ---- batch: 100 ----
mean loss: 333.78
 ---- batch: 110 ----
mean loss: 321.41
train mean loss: 331.66
epoch train time: 0:00:15.960909
elapsed time: 0:09:16.760053
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 01:55:47.450325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 331.42
 ---- batch: 020 ----
mean loss: 326.80
 ---- batch: 030 ----
mean loss: 325.32
 ---- batch: 040 ----
mean loss: 324.26
 ---- batch: 050 ----
mean loss: 322.19
 ---- batch: 060 ----
mean loss: 315.89
 ---- batch: 070 ----
mean loss: 339.50
 ---- batch: 080 ----
mean loss: 324.51
 ---- batch: 090 ----
mean loss: 331.43
 ---- batch: 100 ----
mean loss: 313.40
 ---- batch: 110 ----
mean loss: 323.01
train mean loss: 325.37
epoch train time: 0:00:16.028927
elapsed time: 0:09:32.789898
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 01:56:03.480477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.86
 ---- batch: 020 ----
mean loss: 314.65
 ---- batch: 030 ----
mean loss: 321.60
 ---- batch: 040 ----
mean loss: 321.28
 ---- batch: 050 ----
mean loss: 318.12
 ---- batch: 060 ----
mean loss: 320.58
 ---- batch: 070 ----
mean loss: 315.50
 ---- batch: 080 ----
mean loss: 325.03
 ---- batch: 090 ----
mean loss: 317.42
 ---- batch: 100 ----
mean loss: 319.74
 ---- batch: 110 ----
mean loss: 327.02
train mean loss: 319.91
epoch train time: 0:00:15.971251
elapsed time: 0:09:48.762429
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 01:56:19.452732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.83
 ---- batch: 020 ----
mean loss: 313.41
 ---- batch: 030 ----
mean loss: 314.80
 ---- batch: 040 ----
mean loss: 303.37
 ---- batch: 050 ----
mean loss: 313.65
 ---- batch: 060 ----
mean loss: 311.75
 ---- batch: 070 ----
mean loss: 315.65
 ---- batch: 080 ----
mean loss: 314.20
 ---- batch: 090 ----
mean loss: 309.57
 ---- batch: 100 ----
mean loss: 321.80
 ---- batch: 110 ----
mean loss: 310.58
train mean loss: 314.29
epoch train time: 0:00:15.914261
elapsed time: 0:10:04.677856
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 01:56:35.368413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.18
 ---- batch: 020 ----
mean loss: 308.99
 ---- batch: 030 ----
mean loss: 322.59
 ---- batch: 040 ----
mean loss: 304.70
 ---- batch: 050 ----
mean loss: 310.63
 ---- batch: 060 ----
mean loss: 317.90
 ---- batch: 070 ----
mean loss: 318.71
 ---- batch: 080 ----
mean loss: 308.17
 ---- batch: 090 ----
mean loss: 305.50
 ---- batch: 100 ----
mean loss: 305.42
 ---- batch: 110 ----
mean loss: 301.72
train mean loss: 310.06
epoch train time: 0:00:16.092704
elapsed time: 0:10:20.771797
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 01:56:51.462080
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.04
 ---- batch: 020 ----
mean loss: 301.13
 ---- batch: 030 ----
mean loss: 314.51
 ---- batch: 040 ----
mean loss: 308.19
 ---- batch: 050 ----
mean loss: 305.38
 ---- batch: 060 ----
mean loss: 309.30
 ---- batch: 070 ----
mean loss: 295.75
 ---- batch: 080 ----
mean loss: 308.23
 ---- batch: 090 ----
mean loss: 292.66
 ---- batch: 100 ----
mean loss: 304.83
 ---- batch: 110 ----
mean loss: 312.87
train mean loss: 304.16
epoch train time: 0:00:15.923053
elapsed time: 0:10:36.695945
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 01:57:07.386289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.44
 ---- batch: 020 ----
mean loss: 310.49
 ---- batch: 030 ----
mean loss: 300.91
 ---- batch: 040 ----
mean loss: 306.50
 ---- batch: 050 ----
mean loss: 298.92
 ---- batch: 060 ----
mean loss: 298.94
 ---- batch: 070 ----
mean loss: 289.71
 ---- batch: 080 ----
mean loss: 314.56
 ---- batch: 090 ----
mean loss: 288.63
 ---- batch: 100 ----
mean loss: 298.73
 ---- batch: 110 ----
mean loss: 302.48
train mean loss: 301.33
epoch train time: 0:00:16.092835
elapsed time: 0:10:52.789940
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 01:57:23.480291
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.89
 ---- batch: 020 ----
mean loss: 305.01
 ---- batch: 030 ----
mean loss: 294.67
 ---- batch: 040 ----
mean loss: 300.46
 ---- batch: 050 ----
mean loss: 290.11
 ---- batch: 060 ----
mean loss: 297.20
 ---- batch: 070 ----
mean loss: 300.52
 ---- batch: 080 ----
mean loss: 296.99
 ---- batch: 090 ----
mean loss: 312.89
 ---- batch: 100 ----
mean loss: 306.26
 ---- batch: 110 ----
mean loss: 301.97
train mean loss: 300.08
epoch train time: 0:00:15.924044
elapsed time: 0:11:08.714998
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 01:57:39.405276
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 293.49
 ---- batch: 020 ----
mean loss: 298.60
 ---- batch: 030 ----
mean loss: 296.60
 ---- batch: 040 ----
mean loss: 307.10
 ---- batch: 050 ----
mean loss: 294.85
 ---- batch: 060 ----
mean loss: 296.93
 ---- batch: 070 ----
mean loss: 290.65
 ---- batch: 080 ----
mean loss: 291.80
 ---- batch: 090 ----
mean loss: 292.31
 ---- batch: 100 ----
mean loss: 290.76
 ---- batch: 110 ----
mean loss: 290.94
train mean loss: 294.96
epoch train time: 0:00:16.164573
elapsed time: 0:11:24.880802
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 01:57:55.571420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.43
 ---- batch: 020 ----
mean loss: 289.81
 ---- batch: 030 ----
mean loss: 302.16
 ---- batch: 040 ----
mean loss: 300.25
 ---- batch: 050 ----
mean loss: 289.78
 ---- batch: 060 ----
mean loss: 288.34
 ---- batch: 070 ----
mean loss: 299.87
 ---- batch: 080 ----
mean loss: 292.87
 ---- batch: 090 ----
mean loss: 292.14
 ---- batch: 100 ----
mean loss: 289.14
 ---- batch: 110 ----
mean loss: 291.67
train mean loss: 292.13
epoch train time: 0:00:15.906134
elapsed time: 0:11:40.788273
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 01:58:11.478616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.68
 ---- batch: 020 ----
mean loss: 287.41
 ---- batch: 030 ----
mean loss: 297.35
 ---- batch: 040 ----
mean loss: 295.75
 ---- batch: 050 ----
mean loss: 285.97
 ---- batch: 060 ----
mean loss: 292.43
 ---- batch: 070 ----
mean loss: 290.77
 ---- batch: 080 ----
mean loss: 284.47
 ---- batch: 090 ----
mean loss: 287.51
 ---- batch: 100 ----
mean loss: 288.99
 ---- batch: 110 ----
mean loss: 296.61
train mean loss: 290.38
epoch train time: 0:00:16.047249
elapsed time: 0:11:56.836628
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 01:58:27.527074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.58
 ---- batch: 020 ----
mean loss: 282.13
 ---- batch: 030 ----
mean loss: 288.61
 ---- batch: 040 ----
mean loss: 291.66
 ---- batch: 050 ----
mean loss: 280.80
 ---- batch: 060 ----
mean loss: 286.96
 ---- batch: 070 ----
mean loss: 284.27
 ---- batch: 080 ----
mean loss: 290.02
 ---- batch: 090 ----
mean loss: 280.96
 ---- batch: 100 ----
mean loss: 284.40
 ---- batch: 110 ----
mean loss: 285.52
train mean loss: 285.45
epoch train time: 0:00:15.915637
elapsed time: 0:12:12.753342
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 01:58:43.443810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.40
 ---- batch: 020 ----
mean loss: 286.82
 ---- batch: 030 ----
mean loss: 276.06
 ---- batch: 040 ----
mean loss: 280.53
 ---- batch: 050 ----
mean loss: 287.42
 ---- batch: 060 ----
mean loss: 296.19
 ---- batch: 070 ----
mean loss: 278.29
 ---- batch: 080 ----
mean loss: 274.48
 ---- batch: 090 ----
mean loss: 292.81
 ---- batch: 100 ----
mean loss: 282.64
 ---- batch: 110 ----
mean loss: 270.16
train mean loss: 282.03
epoch train time: 0:00:16.022693
elapsed time: 0:12:28.777358
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 01:58:59.467896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.20
 ---- batch: 020 ----
mean loss: 287.25
 ---- batch: 030 ----
mean loss: 285.17
 ---- batch: 040 ----
mean loss: 286.61
 ---- batch: 050 ----
mean loss: 274.53
 ---- batch: 060 ----
mean loss: 278.27
 ---- batch: 070 ----
mean loss: 293.08
 ---- batch: 080 ----
mean loss: 291.75
 ---- batch: 090 ----
mean loss: 285.54
 ---- batch: 100 ----
mean loss: 291.31
 ---- batch: 110 ----
mean loss: 277.52
train mean loss: 285.14
epoch train time: 0:00:15.921934
elapsed time: 0:12:44.700855
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 01:59:15.391972
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.39
 ---- batch: 020 ----
mean loss: 279.07
 ---- batch: 030 ----
mean loss: 279.47
 ---- batch: 040 ----
mean loss: 271.97
 ---- batch: 050 ----
mean loss: 282.93
 ---- batch: 060 ----
mean loss: 270.92
 ---- batch: 070 ----
mean loss: 282.80
 ---- batch: 080 ----
mean loss: 278.30
 ---- batch: 090 ----
mean loss: 275.07
 ---- batch: 100 ----
mean loss: 279.95
 ---- batch: 110 ----
mean loss: 278.60
train mean loss: 278.17
epoch train time: 0:00:16.045141
elapsed time: 0:13:00.748125
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 01:59:31.439140
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.58
 ---- batch: 020 ----
mean loss: 286.44
 ---- batch: 030 ----
mean loss: 271.59
 ---- batch: 040 ----
mean loss: 280.24
 ---- batch: 050 ----
mean loss: 276.91
 ---- batch: 060 ----
mean loss: 285.84
 ---- batch: 070 ----
mean loss: 280.12
 ---- batch: 080 ----
mean loss: 283.69
 ---- batch: 090 ----
mean loss: 277.69
 ---- batch: 100 ----
mean loss: 258.41
 ---- batch: 110 ----
mean loss: 268.37
train mean loss: 276.35
epoch train time: 0:00:15.931166
elapsed time: 0:13:16.681007
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 01:59:47.371321
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.63
 ---- batch: 020 ----
mean loss: 276.56
 ---- batch: 030 ----
mean loss: 273.02
 ---- batch: 040 ----
mean loss: 280.74
 ---- batch: 050 ----
mean loss: 268.05
 ---- batch: 060 ----
mean loss: 274.22
 ---- batch: 070 ----
mean loss: 273.48
 ---- batch: 080 ----
mean loss: 286.77
 ---- batch: 090 ----
mean loss: 271.91
 ---- batch: 100 ----
mean loss: 269.63
 ---- batch: 110 ----
mean loss: 274.03
train mean loss: 275.28
epoch train time: 0:00:16.154212
elapsed time: 0:13:32.836254
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 02:00:03.526595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 282.27
 ---- batch: 020 ----
mean loss: 275.94
 ---- batch: 030 ----
mean loss: 270.71
 ---- batch: 040 ----
mean loss: 283.77
 ---- batch: 050 ----
mean loss: 269.80
 ---- batch: 060 ----
mean loss: 269.60
 ---- batch: 070 ----
mean loss: 263.57
 ---- batch: 080 ----
mean loss: 268.20
 ---- batch: 090 ----
mean loss: 269.44
 ---- batch: 100 ----
mean loss: 267.50
 ---- batch: 110 ----
mean loss: 266.23
train mean loss: 271.71
epoch train time: 0:00:15.958018
elapsed time: 0:13:48.795292
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 02:00:19.485542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.80
 ---- batch: 020 ----
mean loss: 266.24
 ---- batch: 030 ----
mean loss: 264.47
 ---- batch: 040 ----
mean loss: 270.74
 ---- batch: 050 ----
mean loss: 272.20
 ---- batch: 060 ----
mean loss: 267.22
 ---- batch: 070 ----
mean loss: 267.49
 ---- batch: 080 ----
mean loss: 262.55
 ---- batch: 090 ----
mean loss: 282.09
 ---- batch: 100 ----
mean loss: 270.51
 ---- batch: 110 ----
mean loss: 281.33
train mean loss: 270.60
epoch train time: 0:00:16.010474
elapsed time: 0:14:04.806889
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 02:00:35.497272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.88
 ---- batch: 020 ----
mean loss: 269.91
 ---- batch: 030 ----
mean loss: 277.59
 ---- batch: 040 ----
mean loss: 259.93
 ---- batch: 050 ----
mean loss: 266.36
 ---- batch: 060 ----
mean loss: 265.44
 ---- batch: 070 ----
mean loss: 272.74
 ---- batch: 080 ----
mean loss: 267.06
 ---- batch: 090 ----
mean loss: 265.69
 ---- batch: 100 ----
mean loss: 270.31
 ---- batch: 110 ----
mean loss: 262.46
train mean loss: 266.96
epoch train time: 0:00:15.993458
elapsed time: 0:14:20.801442
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 02:00:51.491712
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.18
 ---- batch: 020 ----
mean loss: 270.09
 ---- batch: 030 ----
mean loss: 264.57
 ---- batch: 040 ----
mean loss: 270.06
 ---- batch: 050 ----
mean loss: 258.22
 ---- batch: 060 ----
mean loss: 259.87
 ---- batch: 070 ----
mean loss: 262.56
 ---- batch: 080 ----
mean loss: 265.73
 ---- batch: 090 ----
mean loss: 259.92
 ---- batch: 100 ----
mean loss: 262.14
 ---- batch: 110 ----
mean loss: 271.82
train mean loss: 265.37
epoch train time: 0:00:15.926507
elapsed time: 0:14:36.729008
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 02:01:07.419292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 263.59
 ---- batch: 020 ----
mean loss: 263.10
 ---- batch: 030 ----
mean loss: 262.34
 ---- batch: 040 ----
mean loss: 260.01
 ---- batch: 050 ----
mean loss: 269.49
 ---- batch: 060 ----
mean loss: 266.98
 ---- batch: 070 ----
mean loss: 263.96
 ---- batch: 080 ----
mean loss: 257.22
 ---- batch: 090 ----
mean loss: 258.44
 ---- batch: 100 ----
mean loss: 262.51
 ---- batch: 110 ----
mean loss: 263.75
train mean loss: 263.21
epoch train time: 0:00:16.029250
elapsed time: 0:14:52.759194
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 02:01:23.449445
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.80
 ---- batch: 020 ----
mean loss: 258.38
 ---- batch: 030 ----
mean loss: 270.66
 ---- batch: 040 ----
mean loss: 259.17
 ---- batch: 050 ----
mean loss: 260.77
 ---- batch: 060 ----
mean loss: 253.59
 ---- batch: 070 ----
mean loss: 249.28
 ---- batch: 080 ----
mean loss: 258.18
 ---- batch: 090 ----
mean loss: 256.23
 ---- batch: 100 ----
mean loss: 267.78
 ---- batch: 110 ----
mean loss: 271.19
train mean loss: 261.22
epoch train time: 0:00:15.805037
elapsed time: 0:15:08.565167
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 02:01:39.255502
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 264.65
 ---- batch: 020 ----
mean loss: 254.48
 ---- batch: 030 ----
mean loss: 271.63
 ---- batch: 040 ----
mean loss: 261.81
 ---- batch: 050 ----
mean loss: 261.88
 ---- batch: 060 ----
mean loss: 255.93
 ---- batch: 070 ----
mean loss: 257.15
 ---- batch: 080 ----
mean loss: 255.41
 ---- batch: 090 ----
mean loss: 263.39
 ---- batch: 100 ----
mean loss: 259.07
 ---- batch: 110 ----
mean loss: 261.25
train mean loss: 260.59
epoch train time: 0:00:15.570961
elapsed time: 0:15:24.137232
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 02:01:54.827546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.94
 ---- batch: 020 ----
mean loss: 262.87
 ---- batch: 030 ----
mean loss: 273.82
 ---- batch: 040 ----
mean loss: 253.94
 ---- batch: 050 ----
mean loss: 266.65
 ---- batch: 060 ----
mean loss: 264.56
 ---- batch: 070 ----
mean loss: 259.35
 ---- batch: 080 ----
mean loss: 254.56
 ---- batch: 090 ----
mean loss: 259.70
 ---- batch: 100 ----
mean loss: 254.67
 ---- batch: 110 ----
mean loss: 260.90
train mean loss: 261.56
epoch train time: 0:00:15.731776
elapsed time: 0:15:39.869997
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 02:02:10.560286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.19
 ---- batch: 020 ----
mean loss: 255.62
 ---- batch: 030 ----
mean loss: 255.39
 ---- batch: 040 ----
mean loss: 249.51
 ---- batch: 050 ----
mean loss: 258.63
 ---- batch: 060 ----
mean loss: 252.62
 ---- batch: 070 ----
mean loss: 256.03
 ---- batch: 080 ----
mean loss: 256.03
 ---- batch: 090 ----
mean loss: 254.90
 ---- batch: 100 ----
mean loss: 255.65
 ---- batch: 110 ----
mean loss: 267.77
train mean loss: 256.91
epoch train time: 0:00:15.647142
elapsed time: 0:15:55.518042
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 02:02:26.208344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.71
 ---- batch: 020 ----
mean loss: 262.70
 ---- batch: 030 ----
mean loss: 258.31
 ---- batch: 040 ----
mean loss: 252.68
 ---- batch: 050 ----
mean loss: 247.99
 ---- batch: 060 ----
mean loss: 247.79
 ---- batch: 070 ----
mean loss: 253.23
 ---- batch: 080 ----
mean loss: 263.37
 ---- batch: 090 ----
mean loss: 252.00
 ---- batch: 100 ----
mean loss: 259.79
 ---- batch: 110 ----
mean loss: 251.58
train mean loss: 254.27
epoch train time: 0:00:15.711346
elapsed time: 0:16:11.230367
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 02:02:41.920626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.04
 ---- batch: 020 ----
mean loss: 253.06
 ---- batch: 030 ----
mean loss: 256.61
 ---- batch: 040 ----
mean loss: 250.56
 ---- batch: 050 ----
mean loss: 240.28
 ---- batch: 060 ----
mean loss: 254.44
 ---- batch: 070 ----
mean loss: 243.93
 ---- batch: 080 ----
mean loss: 264.33
 ---- batch: 090 ----
mean loss: 263.22
 ---- batch: 100 ----
mean loss: 256.11
 ---- batch: 110 ----
mean loss: 265.75
train mean loss: 255.11
epoch train time: 0:00:15.650632
elapsed time: 0:16:26.881945
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 02:02:57.572282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.08
 ---- batch: 020 ----
mean loss: 249.53
 ---- batch: 030 ----
mean loss: 254.00
 ---- batch: 040 ----
mean loss: 253.18
 ---- batch: 050 ----
mean loss: 254.52
 ---- batch: 060 ----
mean loss: 253.97
 ---- batch: 070 ----
mean loss: 248.71
 ---- batch: 080 ----
mean loss: 257.15
 ---- batch: 090 ----
mean loss: 256.18
 ---- batch: 100 ----
mean loss: 248.44
 ---- batch: 110 ----
mean loss: 251.25
train mean loss: 252.80
epoch train time: 0:00:15.710208
elapsed time: 0:16:42.593132
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 02:03:13.283414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 244.73
 ---- batch: 020 ----
mean loss: 251.18
 ---- batch: 030 ----
mean loss: 246.30
 ---- batch: 040 ----
mean loss: 237.75
 ---- batch: 050 ----
mean loss: 248.45
 ---- batch: 060 ----
mean loss: 249.95
 ---- batch: 070 ----
mean loss: 261.79
 ---- batch: 080 ----
mean loss: 247.75
 ---- batch: 090 ----
mean loss: 254.04
 ---- batch: 100 ----
mean loss: 241.77
 ---- batch: 110 ----
mean loss: 256.69
train mean loss: 248.79
epoch train time: 0:00:15.686498
elapsed time: 0:16:58.280623
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 02:03:28.970928
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.08
 ---- batch: 020 ----
mean loss: 240.30
 ---- batch: 030 ----
mean loss: 242.66
 ---- batch: 040 ----
mean loss: 254.50
 ---- batch: 050 ----
mean loss: 244.86
 ---- batch: 060 ----
mean loss: 252.39
 ---- batch: 070 ----
mean loss: 237.51
 ---- batch: 080 ----
mean loss: 254.71
 ---- batch: 090 ----
mean loss: 252.08
 ---- batch: 100 ----
mean loss: 249.08
 ---- batch: 110 ----
mean loss: 250.93
train mean loss: 248.15
epoch train time: 0:00:15.670188
elapsed time: 0:17:13.951805
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 02:03:44.642106
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.45
 ---- batch: 020 ----
mean loss: 241.54
 ---- batch: 030 ----
mean loss: 247.21
 ---- batch: 040 ----
mean loss: 247.99
 ---- batch: 050 ----
mean loss: 239.39
 ---- batch: 060 ----
mean loss: 246.50
 ---- batch: 070 ----
mean loss: 253.48
 ---- batch: 080 ----
mean loss: 243.57
 ---- batch: 090 ----
mean loss: 241.28
 ---- batch: 100 ----
mean loss: 247.63
 ---- batch: 110 ----
mean loss: 258.01
train mean loss: 246.62
epoch train time: 0:00:15.665208
elapsed time: 0:17:29.618052
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 02:04:00.308450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.15
 ---- batch: 020 ----
mean loss: 248.66
 ---- batch: 030 ----
mean loss: 244.05
 ---- batch: 040 ----
mean loss: 230.27
 ---- batch: 050 ----
mean loss: 245.01
 ---- batch: 060 ----
mean loss: 246.51
 ---- batch: 070 ----
mean loss: 249.37
 ---- batch: 080 ----
mean loss: 248.47
 ---- batch: 090 ----
mean loss: 251.62
 ---- batch: 100 ----
mean loss: 245.22
 ---- batch: 110 ----
mean loss: 241.86
train mean loss: 245.24
epoch train time: 0:00:15.609919
elapsed time: 0:17:45.229041
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 02:04:15.919334
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.29
 ---- batch: 020 ----
mean loss: 235.28
 ---- batch: 030 ----
mean loss: 246.21
 ---- batch: 040 ----
mean loss: 247.59
 ---- batch: 050 ----
mean loss: 249.03
 ---- batch: 060 ----
mean loss: 240.89
 ---- batch: 070 ----
mean loss: 246.56
 ---- batch: 080 ----
mean loss: 242.19
 ---- batch: 090 ----
mean loss: 237.47
 ---- batch: 100 ----
mean loss: 239.05
 ---- batch: 110 ----
mean loss: 246.49
train mean loss: 243.96
epoch train time: 0:00:15.637822
elapsed time: 0:18:00.867864
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 02:04:31.558185
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.33
 ---- batch: 020 ----
mean loss: 245.84
 ---- batch: 030 ----
mean loss: 242.26
 ---- batch: 040 ----
mean loss: 240.54
 ---- batch: 050 ----
mean loss: 234.92
 ---- batch: 060 ----
mean loss: 246.76
 ---- batch: 070 ----
mean loss: 235.43
 ---- batch: 080 ----
mean loss: 249.07
 ---- batch: 090 ----
mean loss: 241.89
 ---- batch: 100 ----
mean loss: 247.43
 ---- batch: 110 ----
mean loss: 244.75
train mean loss: 243.16
epoch train time: 0:00:15.669115
elapsed time: 0:18:16.538089
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 02:04:47.228433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.37
 ---- batch: 020 ----
mean loss: 238.78
 ---- batch: 030 ----
mean loss: 233.70
 ---- batch: 040 ----
mean loss: 243.65
 ---- batch: 050 ----
mean loss: 255.00
 ---- batch: 060 ----
mean loss: 232.27
 ---- batch: 070 ----
mean loss: 242.90
 ---- batch: 080 ----
mean loss: 233.19
 ---- batch: 090 ----
mean loss: 244.36
 ---- batch: 100 ----
mean loss: 238.53
 ---- batch: 110 ----
mean loss: 244.13
train mean loss: 241.08
epoch train time: 0:00:15.669346
elapsed time: 0:18:32.208503
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 02:05:02.898856
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.21
 ---- batch: 020 ----
mean loss: 242.46
 ---- batch: 030 ----
mean loss: 232.45
 ---- batch: 040 ----
mean loss: 245.34
 ---- batch: 050 ----
mean loss: 246.76
 ---- batch: 060 ----
mean loss: 249.97
 ---- batch: 070 ----
mean loss: 238.13
 ---- batch: 080 ----
mean loss: 233.38
 ---- batch: 090 ----
mean loss: 240.86
 ---- batch: 100 ----
mean loss: 232.66
 ---- batch: 110 ----
mean loss: 242.27
train mean loss: 240.23
epoch train time: 0:00:15.703700
elapsed time: 0:18:47.913242
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 02:05:18.603597
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.29
 ---- batch: 020 ----
mean loss: 235.64
 ---- batch: 030 ----
mean loss: 243.00
 ---- batch: 040 ----
mean loss: 228.50
 ---- batch: 050 ----
mean loss: 241.43
 ---- batch: 060 ----
mean loss: 242.83
 ---- batch: 070 ----
mean loss: 234.01
 ---- batch: 080 ----
mean loss: 233.70
 ---- batch: 090 ----
mean loss: 236.77
 ---- batch: 100 ----
mean loss: 231.37
 ---- batch: 110 ----
mean loss: 240.42
train mean loss: 237.30
epoch train time: 0:00:15.617330
elapsed time: 0:19:03.531618
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 02:05:34.221957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.75
 ---- batch: 020 ----
mean loss: 241.54
 ---- batch: 030 ----
mean loss: 244.64
 ---- batch: 040 ----
mean loss: 239.61
 ---- batch: 050 ----
mean loss: 226.45
 ---- batch: 060 ----
mean loss: 228.21
 ---- batch: 070 ----
mean loss: 236.22
 ---- batch: 080 ----
mean loss: 237.52
 ---- batch: 090 ----
mean loss: 241.20
 ---- batch: 100 ----
mean loss: 235.56
 ---- batch: 110 ----
mean loss: 239.69
train mean loss: 237.03
epoch train time: 0:00:15.622524
elapsed time: 0:19:19.155176
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 02:05:49.845449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.74
 ---- batch: 020 ----
mean loss: 229.99
 ---- batch: 030 ----
mean loss: 241.97
 ---- batch: 040 ----
mean loss: 233.97
 ---- batch: 050 ----
mean loss: 218.38
 ---- batch: 060 ----
mean loss: 233.58
 ---- batch: 070 ----
mean loss: 237.58
 ---- batch: 080 ----
mean loss: 244.19
 ---- batch: 090 ----
mean loss: 232.47
 ---- batch: 100 ----
mean loss: 232.39
 ---- batch: 110 ----
mean loss: 241.38
train mean loss: 234.71
epoch train time: 0:00:15.651075
elapsed time: 0:19:34.807216
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 02:06:05.497517
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.77
 ---- batch: 020 ----
mean loss: 231.21
 ---- batch: 030 ----
mean loss: 234.40
 ---- batch: 040 ----
mean loss: 236.73
 ---- batch: 050 ----
mean loss: 241.05
 ---- batch: 060 ----
mean loss: 231.51
 ---- batch: 070 ----
mean loss: 225.11
 ---- batch: 080 ----
mean loss: 231.70
 ---- batch: 090 ----
mean loss: 227.12
 ---- batch: 100 ----
mean loss: 235.69
 ---- batch: 110 ----
mean loss: 228.70
train mean loss: 231.98
epoch train time: 0:00:15.665318
elapsed time: 0:19:50.473560
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 02:06:21.163831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.37
 ---- batch: 020 ----
mean loss: 234.56
 ---- batch: 030 ----
mean loss: 225.75
 ---- batch: 040 ----
mean loss: 225.39
 ---- batch: 050 ----
mean loss: 224.96
 ---- batch: 060 ----
mean loss: 238.11
 ---- batch: 070 ----
mean loss: 228.51
 ---- batch: 080 ----
mean loss: 241.85
 ---- batch: 090 ----
mean loss: 229.17
 ---- batch: 100 ----
mean loss: 229.41
 ---- batch: 110 ----
mean loss: 225.28
train mean loss: 230.37
epoch train time: 0:00:15.549456
elapsed time: 0:20:06.024020
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 02:06:36.714309
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.01
 ---- batch: 020 ----
mean loss: 231.93
 ---- batch: 030 ----
mean loss: 236.53
 ---- batch: 040 ----
mean loss: 230.93
 ---- batch: 050 ----
mean loss: 221.62
 ---- batch: 060 ----
mean loss: 232.14
 ---- batch: 070 ----
mean loss: 232.58
 ---- batch: 080 ----
mean loss: 232.16
 ---- batch: 090 ----
mean loss: 229.00
 ---- batch: 100 ----
mean loss: 228.16
 ---- batch: 110 ----
mean loss: 228.98
train mean loss: 230.89
epoch train time: 0:00:15.566478
elapsed time: 0:20:21.591466
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 02:06:52.281853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.03
 ---- batch: 020 ----
mean loss: 223.36
 ---- batch: 030 ----
mean loss: 235.38
 ---- batch: 040 ----
mean loss: 225.19
 ---- batch: 050 ----
mean loss: 237.48
 ---- batch: 060 ----
mean loss: 221.82
 ---- batch: 070 ----
mean loss: 223.96
 ---- batch: 080 ----
mean loss: 226.38
 ---- batch: 090 ----
mean loss: 228.54
 ---- batch: 100 ----
mean loss: 229.58
 ---- batch: 110 ----
mean loss: 230.91
train mean loss: 228.41
epoch train time: 0:00:15.574439
elapsed time: 0:20:37.167101
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 02:07:07.857393
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.49
 ---- batch: 020 ----
mean loss: 222.07
 ---- batch: 030 ----
mean loss: 226.70
 ---- batch: 040 ----
mean loss: 230.21
 ---- batch: 050 ----
mean loss: 229.17
 ---- batch: 060 ----
mean loss: 231.95
 ---- batch: 070 ----
mean loss: 229.97
 ---- batch: 080 ----
mean loss: 222.33
 ---- batch: 090 ----
mean loss: 220.21
 ---- batch: 100 ----
mean loss: 212.73
 ---- batch: 110 ----
mean loss: 231.72
train mean loss: 226.13
epoch train time: 0:00:15.632539
elapsed time: 0:20:52.800637
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 02:07:23.490919
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.69
 ---- batch: 020 ----
mean loss: 232.76
 ---- batch: 030 ----
mean loss: 225.22
 ---- batch: 040 ----
mean loss: 225.76
 ---- batch: 050 ----
mean loss: 228.03
 ---- batch: 060 ----
mean loss: 231.02
 ---- batch: 070 ----
mean loss: 220.63
 ---- batch: 080 ----
mean loss: 223.13
 ---- batch: 090 ----
mean loss: 227.88
 ---- batch: 100 ----
mean loss: 224.78
 ---- batch: 110 ----
mean loss: 224.03
train mean loss: 226.50
epoch train time: 0:00:15.599904
elapsed time: 0:21:08.401591
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 02:07:39.091895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.40
 ---- batch: 020 ----
mean loss: 220.63
 ---- batch: 030 ----
mean loss: 217.49
 ---- batch: 040 ----
mean loss: 222.88
 ---- batch: 050 ----
mean loss: 228.62
 ---- batch: 060 ----
mean loss: 234.43
 ---- batch: 070 ----
mean loss: 230.75
 ---- batch: 080 ----
mean loss: 229.68
 ---- batch: 090 ----
mean loss: 220.79
 ---- batch: 100 ----
mean loss: 215.95
 ---- batch: 110 ----
mean loss: 224.49
train mean loss: 224.32
epoch train time: 0:00:15.635611
elapsed time: 0:21:24.038184
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 02:07:54.728573
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.10
 ---- batch: 020 ----
mean loss: 222.30
 ---- batch: 030 ----
mean loss: 214.92
 ---- batch: 040 ----
mean loss: 226.44
 ---- batch: 050 ----
mean loss: 223.37
 ---- batch: 060 ----
mean loss: 221.29
 ---- batch: 070 ----
mean loss: 224.72
 ---- batch: 080 ----
mean loss: 212.90
 ---- batch: 090 ----
mean loss: 229.83
 ---- batch: 100 ----
mean loss: 212.87
 ---- batch: 110 ----
mean loss: 230.50
train mean loss: 221.32
epoch train time: 0:00:15.587067
elapsed time: 0:21:39.626308
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 02:08:10.316591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.00
 ---- batch: 020 ----
mean loss: 225.82
 ---- batch: 030 ----
mean loss: 213.17
 ---- batch: 040 ----
mean loss: 223.14
 ---- batch: 050 ----
mean loss: 224.87
 ---- batch: 060 ----
mean loss: 220.00
 ---- batch: 070 ----
mean loss: 229.48
 ---- batch: 080 ----
mean loss: 217.06
 ---- batch: 090 ----
mean loss: 229.13
 ---- batch: 100 ----
mean loss: 222.86
 ---- batch: 110 ----
mean loss: 215.84
train mean loss: 221.47
epoch train time: 0:00:15.613919
elapsed time: 0:21:55.241225
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 02:08:25.931568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.93
 ---- batch: 020 ----
mean loss: 216.26
 ---- batch: 030 ----
mean loss: 214.01
 ---- batch: 040 ----
mean loss: 219.28
 ---- batch: 050 ----
mean loss: 220.25
 ---- batch: 060 ----
mean loss: 213.72
 ---- batch: 070 ----
mean loss: 215.62
 ---- batch: 080 ----
mean loss: 236.71
 ---- batch: 090 ----
mean loss: 222.25
 ---- batch: 100 ----
mean loss: 209.89
 ---- batch: 110 ----
mean loss: 223.77
train mean loss: 218.68
epoch train time: 0:00:15.561744
elapsed time: 0:22:10.803893
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 02:08:41.494201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.39
 ---- batch: 020 ----
mean loss: 225.17
 ---- batch: 030 ----
mean loss: 227.79
 ---- batch: 040 ----
mean loss: 224.24
 ---- batch: 050 ----
mean loss: 216.25
 ---- batch: 060 ----
mean loss: 219.04
 ---- batch: 070 ----
mean loss: 222.33
 ---- batch: 080 ----
mean loss: 212.06
 ---- batch: 090 ----
mean loss: 216.10
 ---- batch: 100 ----
mean loss: 216.82
 ---- batch: 110 ----
mean loss: 212.18
train mean loss: 218.45
epoch train time: 0:00:15.517026
elapsed time: 0:22:26.321994
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 02:08:57.012389
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.80
 ---- batch: 020 ----
mean loss: 219.15
 ---- batch: 030 ----
mean loss: 212.63
 ---- batch: 040 ----
mean loss: 208.97
 ---- batch: 050 ----
mean loss: 217.48
 ---- batch: 060 ----
mean loss: 214.98
 ---- batch: 070 ----
mean loss: 212.41
 ---- batch: 080 ----
mean loss: 213.28
 ---- batch: 090 ----
mean loss: 216.13
 ---- batch: 100 ----
mean loss: 219.51
 ---- batch: 110 ----
mean loss: 221.03
train mean loss: 215.80
epoch train time: 0:00:15.596225
elapsed time: 0:22:41.919296
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 02:09:12.609641
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.37
 ---- batch: 020 ----
mean loss: 218.88
 ---- batch: 030 ----
mean loss: 217.97
 ---- batch: 040 ----
mean loss: 213.36
 ---- batch: 050 ----
mean loss: 210.20
 ---- batch: 060 ----
mean loss: 214.63
 ---- batch: 070 ----
mean loss: 218.67
 ---- batch: 080 ----
mean loss: 220.28
 ---- batch: 090 ----
mean loss: 212.53
 ---- batch: 100 ----
mean loss: 221.30
 ---- batch: 110 ----
mean loss: 211.74
train mean loss: 216.11
epoch train time: 0:00:15.616935
elapsed time: 0:22:57.537225
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 02:09:28.227630
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.28
 ---- batch: 020 ----
mean loss: 218.96
 ---- batch: 030 ----
mean loss: 203.50
 ---- batch: 040 ----
mean loss: 216.18
 ---- batch: 050 ----
mean loss: 215.09
 ---- batch: 060 ----
mean loss: 211.49
 ---- batch: 070 ----
mean loss: 216.16
 ---- batch: 080 ----
mean loss: 218.35
 ---- batch: 090 ----
mean loss: 216.03
 ---- batch: 100 ----
mean loss: 211.42
 ---- batch: 110 ----
mean loss: 213.67
train mean loss: 213.21
epoch train time: 0:00:15.636127
elapsed time: 0:23:13.174473
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 02:09:43.864740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.92
 ---- batch: 020 ----
mean loss: 223.28
 ---- batch: 030 ----
mean loss: 207.62
 ---- batch: 040 ----
mean loss: 206.93
 ---- batch: 050 ----
mean loss: 215.41
 ---- batch: 060 ----
mean loss: 212.26
 ---- batch: 070 ----
mean loss: 202.84
 ---- batch: 080 ----
mean loss: 208.23
 ---- batch: 090 ----
mean loss: 220.09
 ---- batch: 100 ----
mean loss: 220.35
 ---- batch: 110 ----
mean loss: 216.11
train mean loss: 212.64
epoch train time: 0:00:15.604794
elapsed time: 0:23:28.780259
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 02:09:59.470605
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.46
 ---- batch: 020 ----
mean loss: 212.33
 ---- batch: 030 ----
mean loss: 208.80
 ---- batch: 040 ----
mean loss: 202.65
 ---- batch: 050 ----
mean loss: 208.77
 ---- batch: 060 ----
mean loss: 209.53
 ---- batch: 070 ----
mean loss: 212.18
 ---- batch: 080 ----
mean loss: 223.29
 ---- batch: 090 ----
mean loss: 209.05
 ---- batch: 100 ----
mean loss: 204.55
 ---- batch: 110 ----
mean loss: 212.76
train mean loss: 210.82
epoch train time: 0:00:15.604124
elapsed time: 0:23:44.385407
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 02:10:15.075742
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.79
 ---- batch: 020 ----
mean loss: 205.14
 ---- batch: 030 ----
mean loss: 216.84
 ---- batch: 040 ----
mean loss: 214.13
 ---- batch: 050 ----
mean loss: 216.37
 ---- batch: 060 ----
mean loss: 208.48
 ---- batch: 070 ----
mean loss: 210.09
 ---- batch: 080 ----
mean loss: 210.30
 ---- batch: 090 ----
mean loss: 218.87
 ---- batch: 100 ----
mean loss: 217.94
 ---- batch: 110 ----
mean loss: 217.53
train mean loss: 212.09
epoch train time: 0:00:15.647055
elapsed time: 0:24:00.033470
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 02:10:30.723751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.57
 ---- batch: 020 ----
mean loss: 215.98
 ---- batch: 030 ----
mean loss: 214.80
 ---- batch: 040 ----
mean loss: 218.74
 ---- batch: 050 ----
mean loss: 202.70
 ---- batch: 060 ----
mean loss: 204.11
 ---- batch: 070 ----
mean loss: 208.40
 ---- batch: 080 ----
mean loss: 208.52
 ---- batch: 090 ----
mean loss: 207.05
 ---- batch: 100 ----
mean loss: 201.78
 ---- batch: 110 ----
mean loss: 205.91
train mean loss: 209.38
epoch train time: 0:00:15.605682
elapsed time: 0:24:15.640211
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 02:10:46.330625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.12
 ---- batch: 020 ----
mean loss: 204.57
 ---- batch: 030 ----
mean loss: 200.15
 ---- batch: 040 ----
mean loss: 206.42
 ---- batch: 050 ----
mean loss: 201.43
 ---- batch: 060 ----
mean loss: 214.35
 ---- batch: 070 ----
mean loss: 218.50
 ---- batch: 080 ----
mean loss: 211.83
 ---- batch: 090 ----
mean loss: 203.96
 ---- batch: 100 ----
mean loss: 214.32
 ---- batch: 110 ----
mean loss: 206.23
train mean loss: 207.90
epoch train time: 0:00:15.607129
elapsed time: 0:24:31.248418
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 02:11:01.938733
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.36
 ---- batch: 020 ----
mean loss: 210.97
 ---- batch: 030 ----
mean loss: 209.20
 ---- batch: 040 ----
mean loss: 204.65
 ---- batch: 050 ----
mean loss: 201.29
 ---- batch: 060 ----
mean loss: 208.57
 ---- batch: 070 ----
mean loss: 204.80
 ---- batch: 080 ----
mean loss: 200.50
 ---- batch: 090 ----
mean loss: 205.14
 ---- batch: 100 ----
mean loss: 201.92
 ---- batch: 110 ----
mean loss: 209.71
train mean loss: 205.64
epoch train time: 0:00:15.584521
elapsed time: 0:24:46.834001
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 02:11:17.524312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.10
 ---- batch: 020 ----
mean loss: 210.16
 ---- batch: 030 ----
mean loss: 202.57
 ---- batch: 040 ----
mean loss: 209.47
 ---- batch: 050 ----
mean loss: 198.27
 ---- batch: 060 ----
mean loss: 203.60
 ---- batch: 070 ----
mean loss: 213.40
 ---- batch: 080 ----
mean loss: 215.46
 ---- batch: 090 ----
mean loss: 207.90
 ---- batch: 100 ----
mean loss: 205.55
 ---- batch: 110 ----
mean loss: 209.78
train mean loss: 207.75
epoch train time: 0:00:15.612494
elapsed time: 0:25:02.447509
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 02:11:33.137826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.87
 ---- batch: 020 ----
mean loss: 213.54
 ---- batch: 030 ----
mean loss: 206.62
 ---- batch: 040 ----
mean loss: 207.73
 ---- batch: 050 ----
mean loss: 204.14
 ---- batch: 060 ----
mean loss: 195.64
 ---- batch: 070 ----
mean loss: 205.05
 ---- batch: 080 ----
mean loss: 192.77
 ---- batch: 090 ----
mean loss: 207.99
 ---- batch: 100 ----
mean loss: 201.60
 ---- batch: 110 ----
mean loss: 202.54
train mean loss: 203.96
epoch train time: 0:00:15.601992
elapsed time: 0:25:18.050340
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 02:11:48.740598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.63
 ---- batch: 020 ----
mean loss: 208.92
 ---- batch: 030 ----
mean loss: 211.66
 ---- batch: 040 ----
mean loss: 201.92
 ---- batch: 050 ----
mean loss: 200.51
 ---- batch: 060 ----
mean loss: 217.75
 ---- batch: 070 ----
mean loss: 217.87
 ---- batch: 080 ----
mean loss: 208.72
 ---- batch: 090 ----
mean loss: 197.19
 ---- batch: 100 ----
mean loss: 201.98
 ---- batch: 110 ----
mean loss: 197.78
train mean loss: 205.99
epoch train time: 0:00:15.594867
elapsed time: 0:25:33.646168
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 02:12:04.336481
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.87
 ---- batch: 020 ----
mean loss: 203.20
 ---- batch: 030 ----
mean loss: 210.49
 ---- batch: 040 ----
mean loss: 198.03
 ---- batch: 050 ----
mean loss: 204.09
 ---- batch: 060 ----
mean loss: 198.39
 ---- batch: 070 ----
mean loss: 206.02
 ---- batch: 080 ----
mean loss: 203.01
 ---- batch: 090 ----
mean loss: 202.31
 ---- batch: 100 ----
mean loss: 201.08
 ---- batch: 110 ----
mean loss: 196.38
train mean loss: 201.99
epoch train time: 0:00:15.601013
elapsed time: 0:25:49.248113
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 02:12:19.938460
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.39
 ---- batch: 020 ----
mean loss: 200.40
 ---- batch: 030 ----
mean loss: 199.85
 ---- batch: 040 ----
mean loss: 210.34
 ---- batch: 050 ----
mean loss: 208.11
 ---- batch: 060 ----
mean loss: 196.93
 ---- batch: 070 ----
mean loss: 191.51
 ---- batch: 080 ----
mean loss: 196.70
 ---- batch: 090 ----
mean loss: 203.58
 ---- batch: 100 ----
mean loss: 199.97
 ---- batch: 110 ----
mean loss: 200.44
train mean loss: 201.53
epoch train time: 0:00:15.607801
elapsed time: 0:26:04.857004
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 02:12:35.547341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.31
 ---- batch: 020 ----
mean loss: 194.39
 ---- batch: 030 ----
mean loss: 185.47
 ---- batch: 040 ----
mean loss: 206.15
 ---- batch: 050 ----
mean loss: 207.98
 ---- batch: 060 ----
mean loss: 206.75
 ---- batch: 070 ----
mean loss: 209.83
 ---- batch: 080 ----
mean loss: 202.64
 ---- batch: 090 ----
mean loss: 199.18
 ---- batch: 100 ----
mean loss: 200.03
 ---- batch: 110 ----
mean loss: 204.88
train mean loss: 201.39
epoch train time: 0:00:15.642855
elapsed time: 0:26:20.500836
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 02:12:51.191142
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.83
 ---- batch: 020 ----
mean loss: 200.47
 ---- batch: 030 ----
mean loss: 200.91
 ---- batch: 040 ----
mean loss: 200.26
 ---- batch: 050 ----
mean loss: 210.31
 ---- batch: 060 ----
mean loss: 193.00
 ---- batch: 070 ----
mean loss: 196.53
 ---- batch: 080 ----
mean loss: 208.80
 ---- batch: 090 ----
mean loss: 204.89
 ---- batch: 100 ----
mean loss: 210.41
 ---- batch: 110 ----
mean loss: 196.61
train mean loss: 201.17
epoch train time: 0:00:15.591789
elapsed time: 0:26:36.093655
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 02:13:06.783927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.73
 ---- batch: 020 ----
mean loss: 202.35
 ---- batch: 030 ----
mean loss: 197.54
 ---- batch: 040 ----
mean loss: 192.81
 ---- batch: 050 ----
mean loss: 202.19
 ---- batch: 060 ----
mean loss: 204.76
 ---- batch: 070 ----
mean loss: 205.56
 ---- batch: 080 ----
mean loss: 211.32
 ---- batch: 090 ----
mean loss: 197.93
 ---- batch: 100 ----
mean loss: 202.98
 ---- batch: 110 ----
mean loss: 195.06
train mean loss: 200.47
epoch train time: 0:00:15.628897
elapsed time: 0:26:51.723486
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 02:13:22.413765
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.75
 ---- batch: 020 ----
mean loss: 200.71
 ---- batch: 030 ----
mean loss: 187.80
 ---- batch: 040 ----
mean loss: 205.71
 ---- batch: 050 ----
mean loss: 196.17
 ---- batch: 060 ----
mean loss: 202.34
 ---- batch: 070 ----
mean loss: 193.71
 ---- batch: 080 ----
mean loss: 190.79
 ---- batch: 090 ----
mean loss: 201.84
 ---- batch: 100 ----
mean loss: 206.13
 ---- batch: 110 ----
mean loss: 206.86
train mean loss: 199.52
epoch train time: 0:00:15.601336
elapsed time: 0:27:07.325797
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 02:13:38.016077
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.71
 ---- batch: 020 ----
mean loss: 194.65
 ---- batch: 030 ----
mean loss: 206.07
 ---- batch: 040 ----
mean loss: 194.28
 ---- batch: 050 ----
mean loss: 198.79
 ---- batch: 060 ----
mean loss: 200.79
 ---- batch: 070 ----
mean loss: 196.30
 ---- batch: 080 ----
mean loss: 197.06
 ---- batch: 090 ----
mean loss: 193.25
 ---- batch: 100 ----
mean loss: 200.16
 ---- batch: 110 ----
mean loss: 201.29
train mean loss: 197.61
epoch train time: 0:00:15.606406
elapsed time: 0:27:22.933177
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 02:13:53.623466
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.76
 ---- batch: 020 ----
mean loss: 197.63
 ---- batch: 030 ----
mean loss: 199.78
 ---- batch: 040 ----
mean loss: 188.83
 ---- batch: 050 ----
mean loss: 201.15
 ---- batch: 060 ----
mean loss: 191.21
 ---- batch: 070 ----
mean loss: 197.30
 ---- batch: 080 ----
mean loss: 199.86
 ---- batch: 090 ----
mean loss: 198.06
 ---- batch: 100 ----
mean loss: 191.36
 ---- batch: 110 ----
mean loss: 198.17
train mean loss: 197.07
epoch train time: 0:00:15.661358
elapsed time: 0:27:38.595488
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 02:14:09.285785
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.46
 ---- batch: 020 ----
mean loss: 198.00
 ---- batch: 030 ----
mean loss: 199.21
 ---- batch: 040 ----
mean loss: 200.99
 ---- batch: 050 ----
mean loss: 195.84
 ---- batch: 060 ----
mean loss: 202.86
 ---- batch: 070 ----
mean loss: 192.08
 ---- batch: 080 ----
mean loss: 200.68
 ---- batch: 090 ----
mean loss: 190.41
 ---- batch: 100 ----
mean loss: 196.78
 ---- batch: 110 ----
mean loss: 193.70
train mean loss: 197.60
epoch train time: 0:00:15.639562
elapsed time: 0:27:54.236033
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 02:14:24.926333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.34
 ---- batch: 020 ----
mean loss: 192.46
 ---- batch: 030 ----
mean loss: 202.04
 ---- batch: 040 ----
mean loss: 195.12
 ---- batch: 050 ----
mean loss: 188.71
 ---- batch: 060 ----
mean loss: 198.41
 ---- batch: 070 ----
mean loss: 190.39
 ---- batch: 080 ----
mean loss: 192.93
 ---- batch: 090 ----
mean loss: 190.31
 ---- batch: 100 ----
mean loss: 198.95
 ---- batch: 110 ----
mean loss: 202.23
train mean loss: 195.51
epoch train time: 0:00:15.603434
elapsed time: 0:28:09.840310
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 02:14:40.530626
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.24
 ---- batch: 020 ----
mean loss: 196.46
 ---- batch: 030 ----
mean loss: 197.00
 ---- batch: 040 ----
mean loss: 192.14
 ---- batch: 050 ----
mean loss: 195.41
 ---- batch: 060 ----
mean loss: 191.86
 ---- batch: 070 ----
mean loss: 194.05
 ---- batch: 080 ----
mean loss: 191.28
 ---- batch: 090 ----
mean loss: 192.91
 ---- batch: 100 ----
mean loss: 203.11
 ---- batch: 110 ----
mean loss: 210.98
train mean loss: 195.20
epoch train time: 0:00:15.569296
elapsed time: 0:28:25.410589
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 02:14:56.101033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.72
 ---- batch: 020 ----
mean loss: 199.02
 ---- batch: 030 ----
mean loss: 197.93
 ---- batch: 040 ----
mean loss: 190.91
 ---- batch: 050 ----
mean loss: 189.89
 ---- batch: 060 ----
mean loss: 203.26
 ---- batch: 070 ----
mean loss: 198.89
 ---- batch: 080 ----
mean loss: 188.89
 ---- batch: 090 ----
mean loss: 192.70
 ---- batch: 100 ----
mean loss: 198.88
 ---- batch: 110 ----
mean loss: 190.03
train mean loss: 195.64
epoch train time: 0:00:15.598695
elapsed time: 0:28:41.010618
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 02:15:11.700927
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.04
 ---- batch: 020 ----
mean loss: 207.84
 ---- batch: 030 ----
mean loss: 189.93
 ---- batch: 040 ----
mean loss: 194.07
 ---- batch: 050 ----
mean loss: 200.23
 ---- batch: 060 ----
mean loss: 195.98
 ---- batch: 070 ----
mean loss: 187.35
 ---- batch: 080 ----
mean loss: 200.49
 ---- batch: 090 ----
mean loss: 201.59
 ---- batch: 100 ----
mean loss: 185.25
 ---- batch: 110 ----
mean loss: 195.94
train mean loss: 195.14
epoch train time: 0:00:15.639002
elapsed time: 0:28:56.650576
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 02:15:27.340916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.54
 ---- batch: 020 ----
mean loss: 195.77
 ---- batch: 030 ----
mean loss: 201.28
 ---- batch: 040 ----
mean loss: 195.75
 ---- batch: 050 ----
mean loss: 202.23
 ---- batch: 060 ----
mean loss: 184.56
 ---- batch: 070 ----
mean loss: 198.49
 ---- batch: 080 ----
mean loss: 199.70
 ---- batch: 090 ----
mean loss: 197.44
 ---- batch: 100 ----
mean loss: 181.83
 ---- batch: 110 ----
mean loss: 199.47
train mean loss: 195.63
epoch train time: 0:00:15.576080
elapsed time: 0:29:12.227696
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 02:15:42.918005
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.27
 ---- batch: 020 ----
mean loss: 200.01
 ---- batch: 030 ----
mean loss: 182.97
 ---- batch: 040 ----
mean loss: 192.99
 ---- batch: 050 ----
mean loss: 194.65
 ---- batch: 060 ----
mean loss: 193.89
 ---- batch: 070 ----
mean loss: 198.96
 ---- batch: 080 ----
mean loss: 199.45
 ---- batch: 090 ----
mean loss: 205.87
 ---- batch: 100 ----
mean loss: 200.05
 ---- batch: 110 ----
mean loss: 192.93
train mean loss: 195.42
epoch train time: 0:00:15.556313
elapsed time: 0:29:27.784987
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 02:15:58.475406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.66
 ---- batch: 020 ----
mean loss: 187.09
 ---- batch: 030 ----
mean loss: 197.07
 ---- batch: 040 ----
mean loss: 191.00
 ---- batch: 050 ----
mean loss: 188.42
 ---- batch: 060 ----
mean loss: 194.88
 ---- batch: 070 ----
mean loss: 202.41
 ---- batch: 080 ----
mean loss: 194.90
 ---- batch: 090 ----
mean loss: 184.20
 ---- batch: 100 ----
mean loss: 191.67
 ---- batch: 110 ----
mean loss: 196.09
train mean loss: 192.50
epoch train time: 0:00:15.573716
elapsed time: 0:29:43.359816
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 02:16:14.050176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.52
 ---- batch: 020 ----
mean loss: 192.88
 ---- batch: 030 ----
mean loss: 190.96
 ---- batch: 040 ----
mean loss: 184.08
 ---- batch: 050 ----
mean loss: 200.05
 ---- batch: 060 ----
mean loss: 190.00
 ---- batch: 070 ----
mean loss: 192.00
 ---- batch: 080 ----
mean loss: 199.81
 ---- batch: 090 ----
mean loss: 187.09
 ---- batch: 100 ----
mean loss: 185.90
 ---- batch: 110 ----
mean loss: 199.39
train mean loss: 192.60
epoch train time: 0:00:15.534705
elapsed time: 0:29:58.895520
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 02:16:29.585832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.42
 ---- batch: 020 ----
mean loss: 187.75
 ---- batch: 030 ----
mean loss: 194.56
 ---- batch: 040 ----
mean loss: 194.08
 ---- batch: 050 ----
mean loss: 185.73
 ---- batch: 060 ----
mean loss: 194.40
 ---- batch: 070 ----
mean loss: 197.23
 ---- batch: 080 ----
mean loss: 197.41
 ---- batch: 090 ----
mean loss: 199.20
 ---- batch: 100 ----
mean loss: 190.23
 ---- batch: 110 ----
mean loss: 198.14
train mean loss: 193.95
epoch train time: 0:00:15.577965
elapsed time: 0:30:14.474481
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 02:16:45.164771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.08
 ---- batch: 020 ----
mean loss: 181.99
 ---- batch: 030 ----
mean loss: 192.72
 ---- batch: 040 ----
mean loss: 181.51
 ---- batch: 050 ----
mean loss: 191.37
 ---- batch: 060 ----
mean loss: 194.67
 ---- batch: 070 ----
mean loss: 184.48
 ---- batch: 080 ----
mean loss: 196.17
 ---- batch: 090 ----
mean loss: 195.85
 ---- batch: 100 ----
mean loss: 190.95
 ---- batch: 110 ----
mean loss: 197.66
train mean loss: 191.33
epoch train time: 0:00:15.518296
elapsed time: 0:30:29.993727
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 02:17:00.684020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.20
 ---- batch: 020 ----
mean loss: 199.74
 ---- batch: 030 ----
mean loss: 183.19
 ---- batch: 040 ----
mean loss: 194.06
 ---- batch: 050 ----
mean loss: 203.86
 ---- batch: 060 ----
mean loss: 204.49
 ---- batch: 070 ----
mean loss: 191.67
 ---- batch: 080 ----
mean loss: 183.06
 ---- batch: 090 ----
mean loss: 186.75
 ---- batch: 100 ----
mean loss: 187.28
 ---- batch: 110 ----
mean loss: 191.59
train mean loss: 192.16
epoch train time: 0:00:15.572745
elapsed time: 0:30:45.567473
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 02:17:16.257815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.51
 ---- batch: 020 ----
mean loss: 199.15
 ---- batch: 030 ----
mean loss: 187.32
 ---- batch: 040 ----
mean loss: 185.57
 ---- batch: 050 ----
mean loss: 192.49
 ---- batch: 060 ----
mean loss: 185.44
 ---- batch: 070 ----
mean loss: 186.07
 ---- batch: 080 ----
mean loss: 200.56
 ---- batch: 090 ----
mean loss: 196.40
 ---- batch: 100 ----
mean loss: 181.58
 ---- batch: 110 ----
mean loss: 185.39
train mean loss: 191.11
epoch train time: 0:00:15.590917
elapsed time: 0:31:01.159429
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 02:17:31.849812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.67
 ---- batch: 020 ----
mean loss: 187.00
 ---- batch: 030 ----
mean loss: 189.71
 ---- batch: 040 ----
mean loss: 194.92
 ---- batch: 050 ----
mean loss: 187.50
 ---- batch: 060 ----
mean loss: 191.36
 ---- batch: 070 ----
mean loss: 207.86
 ---- batch: 080 ----
mean loss: 197.74
 ---- batch: 090 ----
mean loss: 187.34
 ---- batch: 100 ----
mean loss: 197.98
 ---- batch: 110 ----
mean loss: 189.10
train mean loss: 192.27
epoch train time: 0:00:15.623346
elapsed time: 0:31:16.783801
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 02:17:47.474118
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.12
 ---- batch: 020 ----
mean loss: 193.17
 ---- batch: 030 ----
mean loss: 188.11
 ---- batch: 040 ----
mean loss: 187.12
 ---- batch: 050 ----
mean loss: 184.61
 ---- batch: 060 ----
mean loss: 193.92
 ---- batch: 070 ----
mean loss: 190.18
 ---- batch: 080 ----
mean loss: 196.19
 ---- batch: 090 ----
mean loss: 186.04
 ---- batch: 100 ----
mean loss: 191.72
 ---- batch: 110 ----
mean loss: 194.06
train mean loss: 190.97
epoch train time: 0:00:15.670725
elapsed time: 0:31:32.455467
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 02:18:03.145891
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.55
 ---- batch: 020 ----
mean loss: 187.80
 ---- batch: 030 ----
mean loss: 187.80
 ---- batch: 040 ----
mean loss: 184.87
 ---- batch: 050 ----
mean loss: 187.64
 ---- batch: 060 ----
mean loss: 186.93
 ---- batch: 070 ----
mean loss: 185.60
 ---- batch: 080 ----
mean loss: 196.45
 ---- batch: 090 ----
mean loss: 199.26
 ---- batch: 100 ----
mean loss: 197.80
 ---- batch: 110 ----
mean loss: 189.92
train mean loss: 190.12
epoch train time: 0:00:15.598382
elapsed time: 0:31:48.054997
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 02:18:18.745319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.96
 ---- batch: 020 ----
mean loss: 188.12
 ---- batch: 030 ----
mean loss: 185.78
 ---- batch: 040 ----
mean loss: 186.90
 ---- batch: 050 ----
mean loss: 191.73
 ---- batch: 060 ----
mean loss: 198.56
 ---- batch: 070 ----
mean loss: 187.83
 ---- batch: 080 ----
mean loss: 188.22
 ---- batch: 090 ----
mean loss: 195.40
 ---- batch: 100 ----
mean loss: 188.44
 ---- batch: 110 ----
mean loss: 189.88
train mean loss: 189.13
epoch train time: 0:00:15.661711
elapsed time: 0:32:03.717702
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 02:18:34.407985
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.71
 ---- batch: 020 ----
mean loss: 177.81
 ---- batch: 030 ----
mean loss: 193.86
 ---- batch: 040 ----
mean loss: 187.49
 ---- batch: 050 ----
mean loss: 185.58
 ---- batch: 060 ----
mean loss: 182.58
 ---- batch: 070 ----
mean loss: 194.81
 ---- batch: 080 ----
mean loss: 178.78
 ---- batch: 090 ----
mean loss: 193.85
 ---- batch: 100 ----
mean loss: 188.44
 ---- batch: 110 ----
mean loss: 192.12
train mean loss: 187.86
epoch train time: 0:00:15.624679
elapsed time: 0:32:19.343364
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 02:18:50.033699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.80
 ---- batch: 020 ----
mean loss: 189.19
 ---- batch: 030 ----
mean loss: 184.98
 ---- batch: 040 ----
mean loss: 186.69
 ---- batch: 050 ----
mean loss: 179.66
 ---- batch: 060 ----
mean loss: 186.47
 ---- batch: 070 ----
mean loss: 191.10
 ---- batch: 080 ----
mean loss: 189.26
 ---- batch: 090 ----
mean loss: 189.15
 ---- batch: 100 ----
mean loss: 191.32
 ---- batch: 110 ----
mean loss: 189.51
train mean loss: 187.89
epoch train time: 0:00:15.659895
elapsed time: 0:32:35.004266
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 02:19:05.694568
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.10
 ---- batch: 020 ----
mean loss: 194.65
 ---- batch: 030 ----
mean loss: 182.22
 ---- batch: 040 ----
mean loss: 190.92
 ---- batch: 050 ----
mean loss: 195.60
 ---- batch: 060 ----
mean loss: 192.07
 ---- batch: 070 ----
mean loss: 194.80
 ---- batch: 080 ----
mean loss: 194.35
 ---- batch: 090 ----
mean loss: 182.06
 ---- batch: 100 ----
mean loss: 191.53
 ---- batch: 110 ----
mean loss: 179.14
train mean loss: 189.95
epoch train time: 0:00:15.678734
elapsed time: 0:32:50.684015
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 02:19:21.374313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.16
 ---- batch: 020 ----
mean loss: 189.57
 ---- batch: 030 ----
mean loss: 187.94
 ---- batch: 040 ----
mean loss: 190.52
 ---- batch: 050 ----
mean loss: 186.73
 ---- batch: 060 ----
mean loss: 188.19
 ---- batch: 070 ----
mean loss: 186.23
 ---- batch: 080 ----
mean loss: 189.22
 ---- batch: 090 ----
mean loss: 195.63
 ---- batch: 100 ----
mean loss: 190.16
 ---- batch: 110 ----
mean loss: 178.53
train mean loss: 188.44
epoch train time: 0:00:15.661648
elapsed time: 0:33:06.346549
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 02:19:37.036868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.73
 ---- batch: 020 ----
mean loss: 179.08
 ---- batch: 030 ----
mean loss: 188.60
 ---- batch: 040 ----
mean loss: 192.50
 ---- batch: 050 ----
mean loss: 196.21
 ---- batch: 060 ----
mean loss: 189.48
 ---- batch: 070 ----
mean loss: 193.67
 ---- batch: 080 ----
mean loss: 181.51
 ---- batch: 090 ----
mean loss: 187.26
 ---- batch: 100 ----
mean loss: 181.83
 ---- batch: 110 ----
mean loss: 187.79
train mean loss: 187.89
epoch train time: 0:00:15.624991
elapsed time: 0:33:21.972503
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 02:19:52.662798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.28
 ---- batch: 020 ----
mean loss: 180.32
 ---- batch: 030 ----
mean loss: 186.29
 ---- batch: 040 ----
mean loss: 196.59
 ---- batch: 050 ----
mean loss: 181.71
 ---- batch: 060 ----
mean loss: 189.10
 ---- batch: 070 ----
mean loss: 192.64
 ---- batch: 080 ----
mean loss: 193.04
 ---- batch: 090 ----
mean loss: 180.81
 ---- batch: 100 ----
mean loss: 182.86
 ---- batch: 110 ----
mean loss: 191.81
train mean loss: 187.22
epoch train time: 0:00:15.640816
elapsed time: 0:33:37.614275
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 02:20:08.304729
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.44
 ---- batch: 020 ----
mean loss: 185.72
 ---- batch: 030 ----
mean loss: 187.53
 ---- batch: 040 ----
mean loss: 191.66
 ---- batch: 050 ----
mean loss: 185.04
 ---- batch: 060 ----
mean loss: 190.79
 ---- batch: 070 ----
mean loss: 185.91
 ---- batch: 080 ----
mean loss: 191.11
 ---- batch: 090 ----
mean loss: 186.66
 ---- batch: 100 ----
mean loss: 179.82
 ---- batch: 110 ----
mean loss: 183.80
train mean loss: 186.95
epoch train time: 0:00:15.627521
elapsed time: 0:33:53.243177
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 02:20:23.933436
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.42
 ---- batch: 020 ----
mean loss: 189.88
 ---- batch: 030 ----
mean loss: 187.80
 ---- batch: 040 ----
mean loss: 170.84
 ---- batch: 050 ----
mean loss: 202.97
 ---- batch: 060 ----
mean loss: 187.66
 ---- batch: 070 ----
mean loss: 193.28
 ---- batch: 080 ----
mean loss: 186.33
 ---- batch: 090 ----
mean loss: 190.53
 ---- batch: 100 ----
mean loss: 180.77
 ---- batch: 110 ----
mean loss: 186.94
train mean loss: 187.96
epoch train time: 0:00:15.576349
elapsed time: 0:34:08.820533
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 02:20:39.510843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.48
 ---- batch: 020 ----
mean loss: 178.68
 ---- batch: 030 ----
mean loss: 184.77
 ---- batch: 040 ----
mean loss: 188.66
 ---- batch: 050 ----
mean loss: 190.88
 ---- batch: 060 ----
mean loss: 187.90
 ---- batch: 070 ----
mean loss: 187.48
 ---- batch: 080 ----
mean loss: 186.03
 ---- batch: 090 ----
mean loss: 175.25
 ---- batch: 100 ----
mean loss: 192.15
 ---- batch: 110 ----
mean loss: 178.93
train mean loss: 185.87
epoch train time: 0:00:15.564061
elapsed time: 0:34:24.385492
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 02:20:55.075798
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.75
 ---- batch: 020 ----
mean loss: 195.42
 ---- batch: 030 ----
mean loss: 187.99
 ---- batch: 040 ----
mean loss: 186.24
 ---- batch: 050 ----
mean loss: 188.01
 ---- batch: 060 ----
mean loss: 191.80
 ---- batch: 070 ----
mean loss: 181.48
 ---- batch: 080 ----
mean loss: 182.40
 ---- batch: 090 ----
mean loss: 180.68
 ---- batch: 100 ----
mean loss: 181.31
 ---- batch: 110 ----
mean loss: 188.67
train mean loss: 186.19
epoch train time: 0:00:15.563865
elapsed time: 0:34:39.950313
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 02:21:10.640554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.55
 ---- batch: 020 ----
mean loss: 186.51
 ---- batch: 030 ----
mean loss: 184.51
 ---- batch: 040 ----
mean loss: 192.18
 ---- batch: 050 ----
mean loss: 179.22
 ---- batch: 060 ----
mean loss: 182.92
 ---- batch: 070 ----
mean loss: 191.52
 ---- batch: 080 ----
mean loss: 187.42
 ---- batch: 090 ----
mean loss: 186.04
 ---- batch: 100 ----
mean loss: 185.10
 ---- batch: 110 ----
mean loss: 184.46
train mean loss: 185.41
epoch train time: 0:00:15.562894
elapsed time: 0:34:55.514142
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 02:21:26.204434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.53
 ---- batch: 020 ----
mean loss: 185.82
 ---- batch: 030 ----
mean loss: 186.40
 ---- batch: 040 ----
mean loss: 185.11
 ---- batch: 050 ----
mean loss: 185.85
 ---- batch: 060 ----
mean loss: 180.45
 ---- batch: 070 ----
mean loss: 185.89
 ---- batch: 080 ----
mean loss: 180.34
 ---- batch: 090 ----
mean loss: 179.53
 ---- batch: 100 ----
mean loss: 191.95
 ---- batch: 110 ----
mean loss: 186.74
train mean loss: 184.67
epoch train time: 0:00:15.571364
elapsed time: 0:35:11.086464
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 02:21:41.776761
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.15
 ---- batch: 020 ----
mean loss: 185.01
 ---- batch: 030 ----
mean loss: 187.84
 ---- batch: 040 ----
mean loss: 187.15
 ---- batch: 050 ----
mean loss: 183.58
 ---- batch: 060 ----
mean loss: 185.86
 ---- batch: 070 ----
mean loss: 191.93
 ---- batch: 080 ----
mean loss: 180.46
 ---- batch: 090 ----
mean loss: 179.53
 ---- batch: 100 ----
mean loss: 181.27
 ---- batch: 110 ----
mean loss: 178.62
train mean loss: 184.35
epoch train time: 0:00:15.598695
elapsed time: 0:35:26.686145
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 02:21:57.376399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.71
 ---- batch: 020 ----
mean loss: 185.78
 ---- batch: 030 ----
mean loss: 185.52
 ---- batch: 040 ----
mean loss: 184.06
 ---- batch: 050 ----
mean loss: 192.86
 ---- batch: 060 ----
mean loss: 182.69
 ---- batch: 070 ----
mean loss: 181.12
 ---- batch: 080 ----
mean loss: 181.32
 ---- batch: 090 ----
mean loss: 196.55
 ---- batch: 100 ----
mean loss: 178.54
 ---- batch: 110 ----
mean loss: 192.67
train mean loss: 187.66
epoch train time: 0:00:15.576094
elapsed time: 0:35:42.263036
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 02:22:12.953304
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.78
 ---- batch: 020 ----
mean loss: 194.57
 ---- batch: 030 ----
mean loss: 194.55
 ---- batch: 040 ----
mean loss: 185.16
 ---- batch: 050 ----
mean loss: 185.12
 ---- batch: 060 ----
mean loss: 185.57
 ---- batch: 070 ----
mean loss: 181.05
 ---- batch: 080 ----
mean loss: 177.62
 ---- batch: 090 ----
mean loss: 180.05
 ---- batch: 100 ----
mean loss: 184.95
 ---- batch: 110 ----
mean loss: 180.67
train mean loss: 184.61
epoch train time: 0:00:15.547916
elapsed time: 0:35:57.811911
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 02:22:28.502204
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.99
 ---- batch: 020 ----
mean loss: 198.16
 ---- batch: 030 ----
mean loss: 185.35
 ---- batch: 040 ----
mean loss: 185.54
 ---- batch: 050 ----
mean loss: 176.63
 ---- batch: 060 ----
mean loss: 187.24
 ---- batch: 070 ----
mean loss: 180.07
 ---- batch: 080 ----
mean loss: 179.20
 ---- batch: 090 ----
mean loss: 184.71
 ---- batch: 100 ----
mean loss: 177.85
 ---- batch: 110 ----
mean loss: 186.05
train mean loss: 184.00
epoch train time: 0:00:15.589187
elapsed time: 0:36:13.402096
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 02:22:44.092480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.08
 ---- batch: 020 ----
mean loss: 186.73
 ---- batch: 030 ----
mean loss: 186.20
 ---- batch: 040 ----
mean loss: 179.99
 ---- batch: 050 ----
mean loss: 175.58
 ---- batch: 060 ----
mean loss: 180.62
 ---- batch: 070 ----
mean loss: 186.55
 ---- batch: 080 ----
mean loss: 189.77
 ---- batch: 090 ----
mean loss: 193.48
 ---- batch: 100 ----
mean loss: 178.45
 ---- batch: 110 ----
mean loss: 178.53
train mean loss: 184.57
epoch train time: 0:00:15.576392
elapsed time: 0:36:28.979565
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 02:22:59.669903
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.89
 ---- batch: 020 ----
mean loss: 186.99
 ---- batch: 030 ----
mean loss: 183.90
 ---- batch: 040 ----
mean loss: 186.15
 ---- batch: 050 ----
mean loss: 184.46
 ---- batch: 060 ----
mean loss: 188.16
 ---- batch: 070 ----
mean loss: 180.07
 ---- batch: 080 ----
mean loss: 181.78
 ---- batch: 090 ----
mean loss: 171.52
 ---- batch: 100 ----
mean loss: 190.98
 ---- batch: 110 ----
mean loss: 191.26
train mean loss: 183.61
epoch train time: 0:00:15.566825
elapsed time: 0:36:44.547431
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 02:23:15.237784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.02
 ---- batch: 020 ----
mean loss: 193.43
 ---- batch: 030 ----
mean loss: 175.54
 ---- batch: 040 ----
mean loss: 193.71
 ---- batch: 050 ----
mean loss: 175.91
 ---- batch: 060 ----
mean loss: 185.41
 ---- batch: 070 ----
mean loss: 174.22
 ---- batch: 080 ----
mean loss: 182.95
 ---- batch: 090 ----
mean loss: 175.75
 ---- batch: 100 ----
mean loss: 186.51
 ---- batch: 110 ----
mean loss: 179.40
train mean loss: 182.98
epoch train time: 0:00:15.582770
elapsed time: 0:37:00.131262
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 02:23:30.821515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.25
 ---- batch: 020 ----
mean loss: 185.09
 ---- batch: 030 ----
mean loss: 184.38
 ---- batch: 040 ----
mean loss: 188.19
 ---- batch: 050 ----
mean loss: 190.73
 ---- batch: 060 ----
mean loss: 190.08
 ---- batch: 070 ----
mean loss: 177.39
 ---- batch: 080 ----
mean loss: 173.63
 ---- batch: 090 ----
mean loss: 181.35
 ---- batch: 100 ----
mean loss: 172.31
 ---- batch: 110 ----
mean loss: 185.63
train mean loss: 182.40
epoch train time: 0:00:15.577316
elapsed time: 0:37:15.709429
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 02:23:46.399717
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.55
 ---- batch: 020 ----
mean loss: 183.84
 ---- batch: 030 ----
mean loss: 188.54
 ---- batch: 040 ----
mean loss: 192.58
 ---- batch: 050 ----
mean loss: 186.40
 ---- batch: 060 ----
mean loss: 173.40
 ---- batch: 070 ----
mean loss: 186.41
 ---- batch: 080 ----
mean loss: 184.01
 ---- batch: 090 ----
mean loss: 191.36
 ---- batch: 100 ----
mean loss: 186.88
 ---- batch: 110 ----
mean loss: 176.92
train mean loss: 184.43
epoch train time: 0:00:15.566889
elapsed time: 0:37:31.277373
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 02:24:01.967736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.52
 ---- batch: 020 ----
mean loss: 184.25
 ---- batch: 030 ----
mean loss: 182.80
 ---- batch: 040 ----
mean loss: 181.95
 ---- batch: 050 ----
mean loss: 181.34
 ---- batch: 060 ----
mean loss: 181.77
 ---- batch: 070 ----
mean loss: 182.87
 ---- batch: 080 ----
mean loss: 183.82
 ---- batch: 090 ----
mean loss: 186.19
 ---- batch: 100 ----
mean loss: 179.64
 ---- batch: 110 ----
mean loss: 176.63
train mean loss: 182.75
epoch train time: 0:00:15.526471
elapsed time: 0:37:46.804820
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 02:24:17.495130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.83
 ---- batch: 020 ----
mean loss: 192.52
 ---- batch: 030 ----
mean loss: 178.85
 ---- batch: 040 ----
mean loss: 186.02
 ---- batch: 050 ----
mean loss: 181.91
 ---- batch: 060 ----
mean loss: 185.90
 ---- batch: 070 ----
mean loss: 180.27
 ---- batch: 080 ----
mean loss: 180.39
 ---- batch: 090 ----
mean loss: 170.04
 ---- batch: 100 ----
mean loss: 188.58
 ---- batch: 110 ----
mean loss: 197.64
train mean loss: 183.74
epoch train time: 0:00:15.528538
elapsed time: 0:38:02.334388
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 02:24:33.024697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.33
 ---- batch: 020 ----
mean loss: 174.70
 ---- batch: 030 ----
mean loss: 175.12
 ---- batch: 040 ----
mean loss: 180.64
 ---- batch: 050 ----
mean loss: 186.34
 ---- batch: 060 ----
mean loss: 175.97
 ---- batch: 070 ----
mean loss: 185.30
 ---- batch: 080 ----
mean loss: 189.34
 ---- batch: 090 ----
mean loss: 188.03
 ---- batch: 100 ----
mean loss: 183.29
 ---- batch: 110 ----
mean loss: 177.18
train mean loss: 182.25
epoch train time: 0:00:15.595298
elapsed time: 0:38:17.930544
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 02:24:48.620811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.87
 ---- batch: 020 ----
mean loss: 182.53
 ---- batch: 030 ----
mean loss: 185.03
 ---- batch: 040 ----
mean loss: 189.12
 ---- batch: 050 ----
mean loss: 182.67
 ---- batch: 060 ----
mean loss: 179.55
 ---- batch: 070 ----
mean loss: 177.12
 ---- batch: 080 ----
mean loss: 186.31
 ---- batch: 090 ----
mean loss: 181.23
 ---- batch: 100 ----
mean loss: 185.34
 ---- batch: 110 ----
mean loss: 180.99
train mean loss: 182.71
epoch train time: 0:00:15.557902
elapsed time: 0:38:33.489303
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 02:25:04.179599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.24
 ---- batch: 020 ----
mean loss: 185.31
 ---- batch: 030 ----
mean loss: 186.95
 ---- batch: 040 ----
mean loss: 179.01
 ---- batch: 050 ----
mean loss: 186.47
 ---- batch: 060 ----
mean loss: 183.41
 ---- batch: 070 ----
mean loss: 181.31
 ---- batch: 080 ----
mean loss: 181.71
 ---- batch: 090 ----
mean loss: 183.00
 ---- batch: 100 ----
mean loss: 182.79
 ---- batch: 110 ----
mean loss: 182.68
train mean loss: 182.22
epoch train time: 0:00:15.636424
elapsed time: 0:38:49.126695
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 02:25:19.817090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.46
 ---- batch: 020 ----
mean loss: 186.68
 ---- batch: 030 ----
mean loss: 178.49
 ---- batch: 040 ----
mean loss: 179.33
 ---- batch: 050 ----
mean loss: 173.88
 ---- batch: 060 ----
mean loss: 179.09
 ---- batch: 070 ----
mean loss: 185.68
 ---- batch: 080 ----
mean loss: 180.77
 ---- batch: 090 ----
mean loss: 182.50
 ---- batch: 100 ----
mean loss: 178.05
 ---- batch: 110 ----
mean loss: 184.68
train mean loss: 181.88
epoch train time: 0:00:15.569715
elapsed time: 0:39:04.697430
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 02:25:35.387664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.50
 ---- batch: 020 ----
mean loss: 185.55
 ---- batch: 030 ----
mean loss: 177.25
 ---- batch: 040 ----
mean loss: 178.84
 ---- batch: 050 ----
mean loss: 185.78
 ---- batch: 060 ----
mean loss: 179.36
 ---- batch: 070 ----
mean loss: 180.67
 ---- batch: 080 ----
mean loss: 185.59
 ---- batch: 090 ----
mean loss: 186.71
 ---- batch: 100 ----
mean loss: 193.56
 ---- batch: 110 ----
mean loss: 173.78
train mean loss: 182.71
epoch train time: 0:00:15.535227
elapsed time: 0:39:20.233602
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 02:25:50.924098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.48
 ---- batch: 020 ----
mean loss: 184.98
 ---- batch: 030 ----
mean loss: 184.50
 ---- batch: 040 ----
mean loss: 178.58
 ---- batch: 050 ----
mean loss: 181.36
 ---- batch: 060 ----
mean loss: 178.74
 ---- batch: 070 ----
mean loss: 184.43
 ---- batch: 080 ----
mean loss: 186.65
 ---- batch: 090 ----
mean loss: 183.31
 ---- batch: 100 ----
mean loss: 191.92
 ---- batch: 110 ----
mean loss: 180.04
train mean loss: 182.81
epoch train time: 0:00:15.599340
elapsed time: 0:39:35.834321
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 02:26:06.524547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.78
 ---- batch: 020 ----
mean loss: 174.76
 ---- batch: 030 ----
mean loss: 173.72
 ---- batch: 040 ----
mean loss: 173.74
 ---- batch: 050 ----
mean loss: 176.79
 ---- batch: 060 ----
mean loss: 186.71
 ---- batch: 070 ----
mean loss: 190.70
 ---- batch: 080 ----
mean loss: 184.09
 ---- batch: 090 ----
mean loss: 180.10
 ---- batch: 100 ----
mean loss: 184.99
 ---- batch: 110 ----
mean loss: 177.25
train mean loss: 181.34
epoch train time: 0:00:15.575899
elapsed time: 0:39:51.411107
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 02:26:22.101382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.61
 ---- batch: 020 ----
mean loss: 178.75
 ---- batch: 030 ----
mean loss: 184.66
 ---- batch: 040 ----
mean loss: 176.49
 ---- batch: 050 ----
mean loss: 180.64
 ---- batch: 060 ----
mean loss: 193.42
 ---- batch: 070 ----
mean loss: 180.94
 ---- batch: 080 ----
mean loss: 188.30
 ---- batch: 090 ----
mean loss: 183.80
 ---- batch: 100 ----
mean loss: 175.25
 ---- batch: 110 ----
mean loss: 179.24
train mean loss: 182.19
epoch train time: 0:00:15.504882
elapsed time: 0:40:06.916915
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 02:26:37.607187
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.45
 ---- batch: 020 ----
mean loss: 177.98
 ---- batch: 030 ----
mean loss: 186.98
 ---- batch: 040 ----
mean loss: 183.28
 ---- batch: 050 ----
mean loss: 176.31
 ---- batch: 060 ----
mean loss: 186.84
 ---- batch: 070 ----
mean loss: 184.40
 ---- batch: 080 ----
mean loss: 182.48
 ---- batch: 090 ----
mean loss: 178.88
 ---- batch: 100 ----
mean loss: 182.22
 ---- batch: 110 ----
mean loss: 175.84
train mean loss: 182.28
epoch train time: 0:00:15.511466
elapsed time: 0:40:22.429303
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 02:26:53.119732
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.79
 ---- batch: 020 ----
mean loss: 179.51
 ---- batch: 030 ----
mean loss: 179.51
 ---- batch: 040 ----
mean loss: 177.79
 ---- batch: 050 ----
mean loss: 187.96
 ---- batch: 060 ----
mean loss: 180.69
 ---- batch: 070 ----
mean loss: 177.53
 ---- batch: 080 ----
mean loss: 187.79
 ---- batch: 090 ----
mean loss: 184.40
 ---- batch: 100 ----
mean loss: 179.44
 ---- batch: 110 ----
mean loss: 175.48
train mean loss: 182.37
epoch train time: 0:00:15.472654
elapsed time: 0:40:37.903047
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 02:27:08.593498
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.72
 ---- batch: 020 ----
mean loss: 174.70
 ---- batch: 030 ----
mean loss: 181.83
 ---- batch: 040 ----
mean loss: 175.82
 ---- batch: 050 ----
mean loss: 180.97
 ---- batch: 060 ----
mean loss: 184.50
 ---- batch: 070 ----
mean loss: 183.63
 ---- batch: 080 ----
mean loss: 175.40
 ---- batch: 090 ----
mean loss: 171.99
 ---- batch: 100 ----
mean loss: 178.29
 ---- batch: 110 ----
mean loss: 184.45
train mean loss: 179.93
epoch train time: 0:00:15.523416
elapsed time: 0:40:53.427572
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 02:27:24.117922
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.37
 ---- batch: 020 ----
mean loss: 178.92
 ---- batch: 030 ----
mean loss: 181.98
 ---- batch: 040 ----
mean loss: 177.74
 ---- batch: 050 ----
mean loss: 184.69
 ---- batch: 060 ----
mean loss: 181.28
 ---- batch: 070 ----
mean loss: 183.45
 ---- batch: 080 ----
mean loss: 174.80
 ---- batch: 090 ----
mean loss: 184.40
 ---- batch: 100 ----
mean loss: 181.61
 ---- batch: 110 ----
mean loss: 186.34
train mean loss: 181.17
epoch train time: 0:00:15.504164
elapsed time: 0:41:08.932772
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 02:27:39.623044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.32
 ---- batch: 020 ----
mean loss: 176.93
 ---- batch: 030 ----
mean loss: 174.94
 ---- batch: 040 ----
mean loss: 177.81
 ---- batch: 050 ----
mean loss: 175.19
 ---- batch: 060 ----
mean loss: 179.86
 ---- batch: 070 ----
mean loss: 187.79
 ---- batch: 080 ----
mean loss: 176.57
 ---- batch: 090 ----
mean loss: 185.21
 ---- batch: 100 ----
mean loss: 179.91
 ---- batch: 110 ----
mean loss: 189.06
train mean loss: 180.78
epoch train time: 0:00:15.618786
elapsed time: 0:41:24.552578
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 02:27:55.242897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.25
 ---- batch: 020 ----
mean loss: 173.50
 ---- batch: 030 ----
mean loss: 191.71
 ---- batch: 040 ----
mean loss: 189.47
 ---- batch: 050 ----
mean loss: 175.44
 ---- batch: 060 ----
mean loss: 176.46
 ---- batch: 070 ----
mean loss: 176.79
 ---- batch: 080 ----
mean loss: 176.45
 ---- batch: 090 ----
mean loss: 173.96
 ---- batch: 100 ----
mean loss: 182.53
 ---- batch: 110 ----
mean loss: 181.39
train mean loss: 180.33
epoch train time: 0:00:15.610217
elapsed time: 0:41:40.163863
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 02:28:10.854170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.62
 ---- batch: 020 ----
mean loss: 180.19
 ---- batch: 030 ----
mean loss: 181.80
 ---- batch: 040 ----
mean loss: 179.14
 ---- batch: 050 ----
mean loss: 174.45
 ---- batch: 060 ----
mean loss: 186.59
 ---- batch: 070 ----
mean loss: 173.47
 ---- batch: 080 ----
mean loss: 174.57
 ---- batch: 090 ----
mean loss: 186.25
 ---- batch: 100 ----
mean loss: 189.08
 ---- batch: 110 ----
mean loss: 185.59
train mean loss: 181.50
epoch train time: 0:00:15.542132
elapsed time: 0:41:55.706828
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 02:28:26.397081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.94
 ---- batch: 020 ----
mean loss: 175.43
 ---- batch: 030 ----
mean loss: 182.02
 ---- batch: 040 ----
mean loss: 174.77
 ---- batch: 050 ----
mean loss: 179.10
 ---- batch: 060 ----
mean loss: 175.82
 ---- batch: 070 ----
mean loss: 187.52
 ---- batch: 080 ----
mean loss: 180.31
 ---- batch: 090 ----
mean loss: 174.65
 ---- batch: 100 ----
mean loss: 185.78
 ---- batch: 110 ----
mean loss: 186.71
train mean loss: 181.19
epoch train time: 0:00:15.511549
elapsed time: 0:42:11.219278
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 02:28:41.909596
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.86
 ---- batch: 020 ----
mean loss: 177.08
 ---- batch: 030 ----
mean loss: 175.34
 ---- batch: 040 ----
mean loss: 186.28
 ---- batch: 050 ----
mean loss: 178.09
 ---- batch: 060 ----
mean loss: 173.97
 ---- batch: 070 ----
mean loss: 183.20
 ---- batch: 080 ----
mean loss: 177.01
 ---- batch: 090 ----
mean loss: 174.33
 ---- batch: 100 ----
mean loss: 184.67
 ---- batch: 110 ----
mean loss: 174.75
train mean loss: 179.31
epoch train time: 0:00:15.605869
elapsed time: 0:42:26.826131
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 02:28:57.516441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.65
 ---- batch: 020 ----
mean loss: 181.85
 ---- batch: 030 ----
mean loss: 176.82
 ---- batch: 040 ----
mean loss: 187.77
 ---- batch: 050 ----
mean loss: 195.43
 ---- batch: 060 ----
mean loss: 174.00
 ---- batch: 070 ----
mean loss: 173.42
 ---- batch: 080 ----
mean loss: 187.39
 ---- batch: 090 ----
mean loss: 182.90
 ---- batch: 100 ----
mean loss: 168.19
 ---- batch: 110 ----
mean loss: 170.83
train mean loss: 180.50
epoch train time: 0:00:15.645656
elapsed time: 0:42:42.472771
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 02:29:13.163163
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.17
 ---- batch: 020 ----
mean loss: 183.94
 ---- batch: 030 ----
mean loss: 182.18
 ---- batch: 040 ----
mean loss: 188.12
 ---- batch: 050 ----
mean loss: 192.28
 ---- batch: 060 ----
mean loss: 176.62
 ---- batch: 070 ----
mean loss: 177.80
 ---- batch: 080 ----
mean loss: 173.88
 ---- batch: 090 ----
mean loss: 180.60
 ---- batch: 100 ----
mean loss: 190.12
 ---- batch: 110 ----
mean loss: 177.15
train mean loss: 181.18
epoch train time: 0:00:15.633960
elapsed time: 0:42:58.107804
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 02:29:28.798102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.75
 ---- batch: 020 ----
mean loss: 183.42
 ---- batch: 030 ----
mean loss: 180.93
 ---- batch: 040 ----
mean loss: 170.55
 ---- batch: 050 ----
mean loss: 189.91
 ---- batch: 060 ----
mean loss: 180.36
 ---- batch: 070 ----
mean loss: 176.29
 ---- batch: 080 ----
mean loss: 162.29
 ---- batch: 090 ----
mean loss: 175.11
 ---- batch: 100 ----
mean loss: 183.16
 ---- batch: 110 ----
mean loss: 186.41
train mean loss: 179.40
epoch train time: 0:00:15.644456
elapsed time: 0:43:13.753212
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 02:29:44.443480
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.04
 ---- batch: 020 ----
mean loss: 184.40
 ---- batch: 030 ----
mean loss: 187.58
 ---- batch: 040 ----
mean loss: 186.21
 ---- batch: 050 ----
mean loss: 181.96
 ---- batch: 060 ----
mean loss: 179.12
 ---- batch: 070 ----
mean loss: 174.59
 ---- batch: 080 ----
mean loss: 181.11
 ---- batch: 090 ----
mean loss: 173.04
 ---- batch: 100 ----
mean loss: 169.34
 ---- batch: 110 ----
mean loss: 178.63
train mean loss: 180.41
epoch train time: 0:00:15.618643
elapsed time: 0:43:29.372858
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 02:30:00.063512
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.66
 ---- batch: 020 ----
mean loss: 187.42
 ---- batch: 030 ----
mean loss: 177.54
 ---- batch: 040 ----
mean loss: 180.78
 ---- batch: 050 ----
mean loss: 182.16
 ---- batch: 060 ----
mean loss: 180.61
 ---- batch: 070 ----
mean loss: 194.28
 ---- batch: 080 ----
mean loss: 177.93
 ---- batch: 090 ----
mean loss: 174.26
 ---- batch: 100 ----
mean loss: 183.67
 ---- batch: 110 ----
mean loss: 183.11
train mean loss: 181.40
epoch train time: 0:00:15.561279
elapsed time: 0:43:44.935424
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 02:30:15.625755
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.62
 ---- batch: 020 ----
mean loss: 172.82
 ---- batch: 030 ----
mean loss: 183.06
 ---- batch: 040 ----
mean loss: 178.90
 ---- batch: 050 ----
mean loss: 190.09
 ---- batch: 060 ----
mean loss: 177.03
 ---- batch: 070 ----
mean loss: 176.19
 ---- batch: 080 ----
mean loss: 180.33
 ---- batch: 090 ----
mean loss: 182.92
 ---- batch: 100 ----
mean loss: 172.18
 ---- batch: 110 ----
mean loss: 167.59
train mean loss: 179.26
epoch train time: 0:00:15.566288
elapsed time: 0:44:00.502753
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 02:30:31.193052
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.18
 ---- batch: 020 ----
mean loss: 180.79
 ---- batch: 030 ----
mean loss: 178.40
 ---- batch: 040 ----
mean loss: 178.49
 ---- batch: 050 ----
mean loss: 173.25
 ---- batch: 060 ----
mean loss: 176.77
 ---- batch: 070 ----
mean loss: 164.49
 ---- batch: 080 ----
mean loss: 188.52
 ---- batch: 090 ----
mean loss: 182.72
 ---- batch: 100 ----
mean loss: 194.31
 ---- batch: 110 ----
mean loss: 174.84
train mean loss: 179.22
epoch train time: 0:00:15.551531
elapsed time: 0:44:16.055276
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 02:30:46.745651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.29
 ---- batch: 020 ----
mean loss: 179.49
 ---- batch: 030 ----
mean loss: 172.16
 ---- batch: 040 ----
mean loss: 171.42
 ---- batch: 050 ----
mean loss: 184.04
 ---- batch: 060 ----
mean loss: 181.57
 ---- batch: 070 ----
mean loss: 182.51
 ---- batch: 080 ----
mean loss: 172.39
 ---- batch: 090 ----
mean loss: 192.06
 ---- batch: 100 ----
mean loss: 182.33
 ---- batch: 110 ----
mean loss: 188.19
train mean loss: 180.65
epoch train time: 0:00:15.567846
elapsed time: 0:44:31.624164
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 02:31:02.314479
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.57
 ---- batch: 020 ----
mean loss: 186.63
 ---- batch: 030 ----
mean loss: 180.01
 ---- batch: 040 ----
mean loss: 189.15
 ---- batch: 050 ----
mean loss: 188.27
 ---- batch: 060 ----
mean loss: 177.17
 ---- batch: 070 ----
mean loss: 172.34
 ---- batch: 080 ----
mean loss: 179.79
 ---- batch: 090 ----
mean loss: 172.65
 ---- batch: 100 ----
mean loss: 173.72
 ---- batch: 110 ----
mean loss: 176.25
train mean loss: 180.18
epoch train time: 0:00:15.585234
elapsed time: 0:44:47.210438
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 02:31:17.900754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.98
 ---- batch: 020 ----
mean loss: 180.42
 ---- batch: 030 ----
mean loss: 169.03
 ---- batch: 040 ----
mean loss: 186.42
 ---- batch: 050 ----
mean loss: 184.34
 ---- batch: 060 ----
mean loss: 182.77
 ---- batch: 070 ----
mean loss: 177.35
 ---- batch: 080 ----
mean loss: 180.24
 ---- batch: 090 ----
mean loss: 176.68
 ---- batch: 100 ----
mean loss: 177.69
 ---- batch: 110 ----
mean loss: 167.28
train mean loss: 178.06
epoch train time: 0:00:15.583229
elapsed time: 0:45:02.794629
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 02:31:33.484895
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.25
 ---- batch: 020 ----
mean loss: 183.34
 ---- batch: 030 ----
mean loss: 186.42
 ---- batch: 040 ----
mean loss: 178.87
 ---- batch: 050 ----
mean loss: 177.46
 ---- batch: 060 ----
mean loss: 184.86
 ---- batch: 070 ----
mean loss: 173.18
 ---- batch: 080 ----
mean loss: 184.26
 ---- batch: 090 ----
mean loss: 181.56
 ---- batch: 100 ----
mean loss: 170.09
 ---- batch: 110 ----
mean loss: 169.46
train mean loss: 179.16
epoch train time: 0:00:15.555395
elapsed time: 0:45:18.350965
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 02:31:49.041249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.30
 ---- batch: 020 ----
mean loss: 188.90
 ---- batch: 030 ----
mean loss: 185.76
 ---- batch: 040 ----
mean loss: 180.68
 ---- batch: 050 ----
mean loss: 175.67
 ---- batch: 060 ----
mean loss: 171.48
 ---- batch: 070 ----
mean loss: 184.58
 ---- batch: 080 ----
mean loss: 176.95
 ---- batch: 090 ----
mean loss: 177.95
 ---- batch: 100 ----
mean loss: 181.54
 ---- batch: 110 ----
mean loss: 175.86
train mean loss: 178.77
epoch train time: 0:00:15.588893
elapsed time: 0:45:33.940837
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 02:32:04.631121
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.05
 ---- batch: 020 ----
mean loss: 184.89
 ---- batch: 030 ----
mean loss: 189.42
 ---- batch: 040 ----
mean loss: 174.15
 ---- batch: 050 ----
mean loss: 175.19
 ---- batch: 060 ----
mean loss: 181.12
 ---- batch: 070 ----
mean loss: 172.66
 ---- batch: 080 ----
mean loss: 177.49
 ---- batch: 090 ----
mean loss: 182.56
 ---- batch: 100 ----
mean loss: 184.00
 ---- batch: 110 ----
mean loss: 181.65
train mean loss: 180.03
epoch train time: 0:00:15.605870
elapsed time: 0:45:49.547650
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 02:32:20.238102
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 172.62
 ---- batch: 020 ----
mean loss: 174.87
 ---- batch: 030 ----
mean loss: 182.85
 ---- batch: 040 ----
mean loss: 179.68
 ---- batch: 050 ----
mean loss: 166.57
 ---- batch: 060 ----
mean loss: 175.34
 ---- batch: 070 ----
mean loss: 183.70
 ---- batch: 080 ----
mean loss: 176.14
 ---- batch: 090 ----
mean loss: 185.63
 ---- batch: 100 ----
mean loss: 172.51
 ---- batch: 110 ----
mean loss: 187.78
train mean loss: 178.32
epoch train time: 0:00:15.596792
elapsed time: 0:46:05.146299
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 02:32:35.836335
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.77
 ---- batch: 020 ----
mean loss: 184.66
 ---- batch: 030 ----
mean loss: 177.28
 ---- batch: 040 ----
mean loss: 177.54
 ---- batch: 050 ----
mean loss: 188.40
 ---- batch: 060 ----
mean loss: 178.04
 ---- batch: 070 ----
mean loss: 176.31
 ---- batch: 080 ----
mean loss: 173.15
 ---- batch: 090 ----
mean loss: 175.70
 ---- batch: 100 ----
mean loss: 178.39
 ---- batch: 110 ----
mean loss: 179.58
train mean loss: 178.68
epoch train time: 0:00:15.588867
elapsed time: 0:46:20.735932
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 02:32:51.426242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.05
 ---- batch: 020 ----
mean loss: 180.87
 ---- batch: 030 ----
mean loss: 176.03
 ---- batch: 040 ----
mean loss: 177.01
 ---- batch: 050 ----
mean loss: 177.40
 ---- batch: 060 ----
mean loss: 176.75
 ---- batch: 070 ----
mean loss: 180.46
 ---- batch: 080 ----
mean loss: 177.48
 ---- batch: 090 ----
mean loss: 179.06
 ---- batch: 100 ----
mean loss: 179.77
 ---- batch: 110 ----
mean loss: 172.34
train mean loss: 178.96
epoch train time: 0:00:15.627649
elapsed time: 0:46:36.364573
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 02:33:07.054872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.21
 ---- batch: 020 ----
mean loss: 179.56
 ---- batch: 030 ----
mean loss: 168.93
 ---- batch: 040 ----
mean loss: 192.76
 ---- batch: 050 ----
mean loss: 181.10
 ---- batch: 060 ----
mean loss: 176.62
 ---- batch: 070 ----
mean loss: 175.36
 ---- batch: 080 ----
mean loss: 179.07
 ---- batch: 090 ----
mean loss: 173.82
 ---- batch: 100 ----
mean loss: 174.40
 ---- batch: 110 ----
mean loss: 179.03
train mean loss: 178.47
epoch train time: 0:00:15.661430
elapsed time: 0:46:52.026896
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 02:33:22.717202
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.18
 ---- batch: 020 ----
mean loss: 185.58
 ---- batch: 030 ----
mean loss: 181.93
 ---- batch: 040 ----
mean loss: 172.79
 ---- batch: 050 ----
mean loss: 178.32
 ---- batch: 060 ----
mean loss: 175.34
 ---- batch: 070 ----
mean loss: 172.17
 ---- batch: 080 ----
mean loss: 179.50
 ---- batch: 090 ----
mean loss: 184.39
 ---- batch: 100 ----
mean loss: 180.14
 ---- batch: 110 ----
mean loss: 168.64
train mean loss: 178.34
epoch train time: 0:00:15.600011
elapsed time: 0:47:07.627867
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 02:33:38.318132
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.98
 ---- batch: 020 ----
mean loss: 188.46
 ---- batch: 030 ----
mean loss: 185.53
 ---- batch: 040 ----
mean loss: 175.21
 ---- batch: 050 ----
mean loss: 168.92
 ---- batch: 060 ----
mean loss: 179.55
 ---- batch: 070 ----
mean loss: 172.22
 ---- batch: 080 ----
mean loss: 172.00
 ---- batch: 090 ----
mean loss: 180.32
 ---- batch: 100 ----
mean loss: 173.20
 ---- batch: 110 ----
mean loss: 179.46
train mean loss: 177.53
epoch train time: 0:00:15.565563
elapsed time: 0:47:23.194393
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 02:33:53.884721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.87
 ---- batch: 020 ----
mean loss: 171.89
 ---- batch: 030 ----
mean loss: 177.86
 ---- batch: 040 ----
mean loss: 179.54
 ---- batch: 050 ----
mean loss: 178.87
 ---- batch: 060 ----
mean loss: 170.88
 ---- batch: 070 ----
mean loss: 185.49
 ---- batch: 080 ----
mean loss: 179.63
 ---- batch: 090 ----
mean loss: 178.76
 ---- batch: 100 ----
mean loss: 178.60
 ---- batch: 110 ----
mean loss: 183.82
train mean loss: 177.77
epoch train time: 0:00:15.588527
elapsed time: 0:47:38.784469
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 02:34:09.474760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.06
 ---- batch: 020 ----
mean loss: 180.14
 ---- batch: 030 ----
mean loss: 170.71
 ---- batch: 040 ----
mean loss: 168.43
 ---- batch: 050 ----
mean loss: 186.87
 ---- batch: 060 ----
mean loss: 184.48
 ---- batch: 070 ----
mean loss: 181.90
 ---- batch: 080 ----
mean loss: 182.72
 ---- batch: 090 ----
mean loss: 173.07
 ---- batch: 100 ----
mean loss: 180.23
 ---- batch: 110 ----
mean loss: 177.10
train mean loss: 177.89
epoch train time: 0:00:15.638311
elapsed time: 0:47:54.423751
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 02:34:25.114082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 168.35
 ---- batch: 020 ----
mean loss: 178.23
 ---- batch: 030 ----
mean loss: 187.97
 ---- batch: 040 ----
mean loss: 171.27
 ---- batch: 050 ----
mean loss: 177.15
 ---- batch: 060 ----
mean loss: 167.45
 ---- batch: 070 ----
mean loss: 174.91
 ---- batch: 080 ----
mean loss: 176.52
 ---- batch: 090 ----
mean loss: 179.00
 ---- batch: 100 ----
mean loss: 183.25
 ---- batch: 110 ----
mean loss: 184.52
train mean loss: 177.40
epoch train time: 0:00:15.617556
elapsed time: 0:48:10.042470
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 02:34:40.732876
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 180.62
 ---- batch: 020 ----
mean loss: 181.50
 ---- batch: 030 ----
mean loss: 175.40
 ---- batch: 040 ----
mean loss: 182.59
 ---- batch: 050 ----
mean loss: 181.04
 ---- batch: 060 ----
mean loss: 188.59
 ---- batch: 070 ----
mean loss: 176.42
 ---- batch: 080 ----
mean loss: 175.32
 ---- batch: 090 ----
mean loss: 173.93
 ---- batch: 100 ----
mean loss: 177.87
 ---- batch: 110 ----
mean loss: 168.07
train mean loss: 178.60
epoch train time: 0:00:15.627245
elapsed time: 0:48:25.670803
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 02:34:56.361086
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.34
 ---- batch: 020 ----
mean loss: 179.69
 ---- batch: 030 ----
mean loss: 171.80
 ---- batch: 040 ----
mean loss: 178.23
 ---- batch: 050 ----
mean loss: 171.74
 ---- batch: 060 ----
mean loss: 174.15
 ---- batch: 070 ----
mean loss: 181.94
 ---- batch: 080 ----
mean loss: 178.93
 ---- batch: 090 ----
mean loss: 191.19
 ---- batch: 100 ----
mean loss: 176.15
 ---- batch: 110 ----
mean loss: 173.01
train mean loss: 177.33
epoch train time: 0:00:15.614021
elapsed time: 0:48:41.285787
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 02:35:11.976076
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.83
 ---- batch: 020 ----
mean loss: 180.80
 ---- batch: 030 ----
mean loss: 178.93
 ---- batch: 040 ----
mean loss: 173.49
 ---- batch: 050 ----
mean loss: 183.51
 ---- batch: 060 ----
mean loss: 188.65
 ---- batch: 070 ----
mean loss: 177.50
 ---- batch: 080 ----
mean loss: 177.51
 ---- batch: 090 ----
mean loss: 163.31
 ---- batch: 100 ----
mean loss: 171.77
 ---- batch: 110 ----
mean loss: 181.02
train mean loss: 177.05
epoch train time: 0:00:15.612574
elapsed time: 0:48:56.899307
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 02:35:27.589600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.04
 ---- batch: 020 ----
mean loss: 171.24
 ---- batch: 030 ----
mean loss: 189.61
 ---- batch: 040 ----
mean loss: 184.51
 ---- batch: 050 ----
mean loss: 181.65
 ---- batch: 060 ----
mean loss: 177.30
 ---- batch: 070 ----
mean loss: 178.46
 ---- batch: 080 ----
mean loss: 166.62
 ---- batch: 090 ----
mean loss: 173.64
 ---- batch: 100 ----
mean loss: 175.37
 ---- batch: 110 ----
mean loss: 178.47
train mean loss: 177.25
epoch train time: 0:00:15.497280
elapsed time: 0:49:12.397560
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 02:35:43.087865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.54
 ---- batch: 020 ----
mean loss: 182.51
 ---- batch: 030 ----
mean loss: 184.57
 ---- batch: 040 ----
mean loss: 172.19
 ---- batch: 050 ----
mean loss: 175.15
 ---- batch: 060 ----
mean loss: 177.46
 ---- batch: 070 ----
mean loss: 175.51
 ---- batch: 080 ----
mean loss: 172.60
 ---- batch: 090 ----
mean loss: 170.72
 ---- batch: 100 ----
mean loss: 165.07
 ---- batch: 110 ----
mean loss: 173.22
train mean loss: 175.71
epoch train time: 0:00:15.501953
elapsed time: 0:49:27.900478
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 02:35:58.590754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.54
 ---- batch: 020 ----
mean loss: 191.09
 ---- batch: 030 ----
mean loss: 190.02
 ---- batch: 040 ----
mean loss: 180.47
 ---- batch: 050 ----
mean loss: 181.72
 ---- batch: 060 ----
mean loss: 180.96
 ---- batch: 070 ----
mean loss: 167.62
 ---- batch: 080 ----
mean loss: 171.05
 ---- batch: 090 ----
mean loss: 180.51
 ---- batch: 100 ----
mean loss: 178.53
 ---- batch: 110 ----
mean loss: 177.56
train mean loss: 179.33
epoch train time: 0:00:15.504465
elapsed time: 0:49:43.405880
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 02:36:14.096152
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.59
 ---- batch: 020 ----
mean loss: 183.85
 ---- batch: 030 ----
mean loss: 184.66
 ---- batch: 040 ----
mean loss: 165.29
 ---- batch: 050 ----
mean loss: 178.28
 ---- batch: 060 ----
mean loss: 178.52
 ---- batch: 070 ----
mean loss: 175.67
 ---- batch: 080 ----
mean loss: 171.25
 ---- batch: 090 ----
mean loss: 182.09
 ---- batch: 100 ----
mean loss: 177.82
 ---- batch: 110 ----
mean loss: 171.63
train mean loss: 177.55
epoch train time: 0:00:15.491321
elapsed time: 0:49:58.898226
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 02:36:29.588583
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.71
 ---- batch: 020 ----
mean loss: 182.31
 ---- batch: 030 ----
mean loss: 195.80
 ---- batch: 040 ----
mean loss: 191.13
 ---- batch: 050 ----
mean loss: 182.52
 ---- batch: 060 ----
mean loss: 176.60
 ---- batch: 070 ----
mean loss: 172.89
 ---- batch: 080 ----
mean loss: 186.33
 ---- batch: 090 ----
mean loss: 174.44
 ---- batch: 100 ----
mean loss: 165.82
 ---- batch: 110 ----
mean loss: 173.21
train mean loss: 179.79
epoch train time: 0:00:15.496099
elapsed time: 0:50:14.395206
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 02:36:45.085482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 165.41
 ---- batch: 020 ----
mean loss: 172.24
 ---- batch: 030 ----
mean loss: 180.61
 ---- batch: 040 ----
mean loss: 172.73
 ---- batch: 050 ----
mean loss: 175.35
 ---- batch: 060 ----
mean loss: 182.81
 ---- batch: 070 ----
mean loss: 182.08
 ---- batch: 080 ----
mean loss: 173.19
 ---- batch: 090 ----
mean loss: 173.72
 ---- batch: 100 ----
mean loss: 177.27
 ---- batch: 110 ----
mean loss: 182.41
train mean loss: 176.50
epoch train time: 0:00:15.454229
elapsed time: 0:50:29.850277
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 02:37:00.540562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 175.17
 ---- batch: 020 ----
mean loss: 170.29
 ---- batch: 030 ----
mean loss: 176.57
 ---- batch: 040 ----
mean loss: 176.76
 ---- batch: 050 ----
mean loss: 182.48
 ---- batch: 060 ----
mean loss: 182.45
 ---- batch: 070 ----
mean loss: 182.31
 ---- batch: 080 ----
mean loss: 181.85
 ---- batch: 090 ----
mean loss: 187.85
 ---- batch: 100 ----
mean loss: 180.94
 ---- batch: 110 ----
mean loss: 177.90
train mean loss: 179.52
epoch train time: 0:00:15.539773
elapsed time: 0:50:45.391015
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 02:37:16.081289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.19
 ---- batch: 020 ----
mean loss: 170.88
 ---- batch: 030 ----
mean loss: 174.36
 ---- batch: 040 ----
mean loss: 173.95
 ---- batch: 050 ----
mean loss: 179.56
 ---- batch: 060 ----
mean loss: 184.45
 ---- batch: 070 ----
mean loss: 169.05
 ---- batch: 080 ----
mean loss: 177.46
 ---- batch: 090 ----
mean loss: 167.74
 ---- batch: 100 ----
mean loss: 173.32
 ---- batch: 110 ----
mean loss: 179.70
train mean loss: 175.85
epoch train time: 0:00:15.609734
elapsed time: 0:51:01.001785
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 02:37:31.692135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 178.16
 ---- batch: 020 ----
mean loss: 184.00
 ---- batch: 030 ----
mean loss: 182.21
 ---- batch: 040 ----
mean loss: 182.34
 ---- batch: 050 ----
mean loss: 178.23
 ---- batch: 060 ----
mean loss: 166.26
 ---- batch: 070 ----
mean loss: 168.12
 ---- batch: 080 ----
mean loss: 173.89
 ---- batch: 090 ----
mean loss: 167.04
 ---- batch: 100 ----
mean loss: 184.83
 ---- batch: 110 ----
mean loss: 179.68
train mean loss: 176.75
epoch train time: 0:00:15.663123
elapsed time: 0:51:16.665924
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 02:37:47.356240
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.30
 ---- batch: 020 ----
mean loss: 189.71
 ---- batch: 030 ----
mean loss: 181.06
 ---- batch: 040 ----
mean loss: 176.08
 ---- batch: 050 ----
mean loss: 168.64
 ---- batch: 060 ----
mean loss: 171.81
 ---- batch: 070 ----
mean loss: 180.13
 ---- batch: 080 ----
mean loss: 170.46
 ---- batch: 090 ----
mean loss: 178.65
 ---- batch: 100 ----
mean loss: 177.06
 ---- batch: 110 ----
mean loss: 165.80
train mean loss: 175.42
epoch train time: 0:00:15.589788
elapsed time: 0:51:32.256680
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 02:38:02.947059
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.19
 ---- batch: 020 ----
mean loss: 181.90
 ---- batch: 030 ----
mean loss: 187.45
 ---- batch: 040 ----
mean loss: 182.35
 ---- batch: 050 ----
mean loss: 167.08
 ---- batch: 060 ----
mean loss: 172.76
 ---- batch: 070 ----
mean loss: 176.10
 ---- batch: 080 ----
mean loss: 176.03
 ---- batch: 090 ----
mean loss: 179.51
 ---- batch: 100 ----
mean loss: 169.76
 ---- batch: 110 ----
mean loss: 180.37
train mean loss: 176.48
epoch train time: 0:00:15.773664
elapsed time: 0:51:48.031468
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 02:38:18.721852
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.14
 ---- batch: 020 ----
mean loss: 174.29
 ---- batch: 030 ----
mean loss: 176.96
 ---- batch: 040 ----
mean loss: 188.60
 ---- batch: 050 ----
mean loss: 169.90
 ---- batch: 060 ----
mean loss: 168.26
 ---- batch: 070 ----
mean loss: 178.41
 ---- batch: 080 ----
mean loss: 178.66
 ---- batch: 090 ----
mean loss: 172.81
 ---- batch: 100 ----
mean loss: 166.59
 ---- batch: 110 ----
mean loss: 179.58
train mean loss: 175.64
epoch train time: 0:00:15.741745
elapsed time: 0:52:03.774267
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 02:38:34.464579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.41
 ---- batch: 020 ----
mean loss: 177.25
 ---- batch: 030 ----
mean loss: 179.46
 ---- batch: 040 ----
mean loss: 164.48
 ---- batch: 050 ----
mean loss: 180.82
 ---- batch: 060 ----
mean loss: 170.04
 ---- batch: 070 ----
mean loss: 190.10
 ---- batch: 080 ----
mean loss: 181.16
 ---- batch: 090 ----
mean loss: 175.07
 ---- batch: 100 ----
mean loss: 173.99
 ---- batch: 110 ----
mean loss: 171.75
train mean loss: 176.12
epoch train time: 0:00:15.547417
elapsed time: 0:52:19.322566
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 02:38:50.012873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.03
 ---- batch: 020 ----
mean loss: 171.67
 ---- batch: 030 ----
mean loss: 172.54
 ---- batch: 040 ----
mean loss: 178.31
 ---- batch: 050 ----
mean loss: 176.87
 ---- batch: 060 ----
mean loss: 186.49
 ---- batch: 070 ----
mean loss: 172.49
 ---- batch: 080 ----
mean loss: 176.04
 ---- batch: 090 ----
mean loss: 170.04
 ---- batch: 100 ----
mean loss: 169.59
 ---- batch: 110 ----
mean loss: 177.54
train mean loss: 175.70
epoch train time: 0:00:15.615069
elapsed time: 0:52:34.938563
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 02:39:05.628882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.32
 ---- batch: 020 ----
mean loss: 169.30
 ---- batch: 030 ----
mean loss: 188.68
 ---- batch: 040 ----
mean loss: 165.83
 ---- batch: 050 ----
mean loss: 177.52
 ---- batch: 060 ----
mean loss: 191.27
 ---- batch: 070 ----
mean loss: 185.19
 ---- batch: 080 ----
mean loss: 175.61
 ---- batch: 090 ----
mean loss: 171.76
 ---- batch: 100 ----
mean loss: 179.45
 ---- batch: 110 ----
mean loss: 178.25
train mean loss: 178.35
epoch train time: 0:00:15.501519
elapsed time: 0:52:50.441039
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 02:39:21.131506
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.76
 ---- batch: 020 ----
mean loss: 174.32
 ---- batch: 030 ----
mean loss: 175.29
 ---- batch: 040 ----
mean loss: 174.49
 ---- batch: 050 ----
mean loss: 174.46
 ---- batch: 060 ----
mean loss: 181.68
 ---- batch: 070 ----
mean loss: 175.06
 ---- batch: 080 ----
mean loss: 166.65
 ---- batch: 090 ----
mean loss: 175.90
 ---- batch: 100 ----
mean loss: 181.01
 ---- batch: 110 ----
mean loss: 187.59
train mean loss: 176.45
epoch train time: 0:00:15.473350
elapsed time: 0:53:05.915506
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 02:39:36.605975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.54
 ---- batch: 020 ----
mean loss: 172.58
 ---- batch: 030 ----
mean loss: 182.54
 ---- batch: 040 ----
mean loss: 170.37
 ---- batch: 050 ----
mean loss: 165.47
 ---- batch: 060 ----
mean loss: 175.09
 ---- batch: 070 ----
mean loss: 169.26
 ---- batch: 080 ----
mean loss: 173.66
 ---- batch: 090 ----
mean loss: 172.48
 ---- batch: 100 ----
mean loss: 172.53
 ---- batch: 110 ----
mean loss: 168.80
train mean loss: 172.35
epoch train time: 0:00:15.511992
elapsed time: 0:53:21.429200
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 02:39:52.119177
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.61
 ---- batch: 020 ----
mean loss: 166.31
 ---- batch: 030 ----
mean loss: 165.41
 ---- batch: 040 ----
mean loss: 175.90
 ---- batch: 050 ----
mean loss: 174.44
 ---- batch: 060 ----
mean loss: 172.82
 ---- batch: 070 ----
mean loss: 164.90
 ---- batch: 080 ----
mean loss: 180.97
 ---- batch: 090 ----
mean loss: 176.74
 ---- batch: 100 ----
mean loss: 172.94
 ---- batch: 110 ----
mean loss: 166.64
train mean loss: 172.14
epoch train time: 0:00:15.457954
elapsed time: 0:53:36.887801
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 02:40:07.578168
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.96
 ---- batch: 020 ----
mean loss: 173.61
 ---- batch: 030 ----
mean loss: 179.69
 ---- batch: 040 ----
mean loss: 172.09
 ---- batch: 050 ----
mean loss: 174.74
 ---- batch: 060 ----
mean loss: 177.98
 ---- batch: 070 ----
mean loss: 167.34
 ---- batch: 080 ----
mean loss: 177.51
 ---- batch: 090 ----
mean loss: 168.10
 ---- batch: 100 ----
mean loss: 171.20
 ---- batch: 110 ----
mean loss: 168.19
train mean loss: 172.30
epoch train time: 0:00:15.465503
elapsed time: 0:53:52.354387
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 02:40:23.044731
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.11
 ---- batch: 020 ----
mean loss: 168.72
 ---- batch: 030 ----
mean loss: 167.19
 ---- batch: 040 ----
mean loss: 169.96
 ---- batch: 050 ----
mean loss: 174.49
 ---- batch: 060 ----
mean loss: 174.37
 ---- batch: 070 ----
mean loss: 177.10
 ---- batch: 080 ----
mean loss: 176.16
 ---- batch: 090 ----
mean loss: 175.36
 ---- batch: 100 ----
mean loss: 164.21
 ---- batch: 110 ----
mean loss: 170.76
train mean loss: 172.11
epoch train time: 0:00:15.475974
elapsed time: 0:54:07.831318
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 02:40:38.521623
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.24
 ---- batch: 020 ----
mean loss: 169.09
 ---- batch: 030 ----
mean loss: 173.99
 ---- batch: 040 ----
mean loss: 170.54
 ---- batch: 050 ----
mean loss: 171.99
 ---- batch: 060 ----
mean loss: 175.06
 ---- batch: 070 ----
mean loss: 173.72
 ---- batch: 080 ----
mean loss: 178.15
 ---- batch: 090 ----
mean loss: 163.56
 ---- batch: 100 ----
mean loss: 166.20
 ---- batch: 110 ----
mean loss: 176.42
train mean loss: 172.22
epoch train time: 0:00:15.497869
elapsed time: 0:54:23.330111
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 02:40:54.020365
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.68
 ---- batch: 020 ----
mean loss: 168.69
 ---- batch: 030 ----
mean loss: 173.06
 ---- batch: 040 ----
mean loss: 174.40
 ---- batch: 050 ----
mean loss: 165.64
 ---- batch: 060 ----
mean loss: 177.14
 ---- batch: 070 ----
mean loss: 174.24
 ---- batch: 080 ----
mean loss: 177.40
 ---- batch: 090 ----
mean loss: 169.20
 ---- batch: 100 ----
mean loss: 163.89
 ---- batch: 110 ----
mean loss: 177.39
train mean loss: 172.14
epoch train time: 0:00:15.515094
elapsed time: 0:54:38.846103
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 02:41:09.536487
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.84
 ---- batch: 020 ----
mean loss: 175.90
 ---- batch: 030 ----
mean loss: 173.23
 ---- batch: 040 ----
mean loss: 183.59
 ---- batch: 050 ----
mean loss: 162.91
 ---- batch: 060 ----
mean loss: 173.38
 ---- batch: 070 ----
mean loss: 164.04
 ---- batch: 080 ----
mean loss: 179.71
 ---- batch: 090 ----
mean loss: 166.97
 ---- batch: 100 ----
mean loss: 178.30
 ---- batch: 110 ----
mean loss: 169.71
train mean loss: 172.04
epoch train time: 0:00:15.532581
elapsed time: 0:54:54.379721
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 02:41:25.070102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.83
 ---- batch: 020 ----
mean loss: 171.32
 ---- batch: 030 ----
mean loss: 168.63
 ---- batch: 040 ----
mean loss: 167.62
 ---- batch: 050 ----
mean loss: 172.31
 ---- batch: 060 ----
mean loss: 180.26
 ---- batch: 070 ----
mean loss: 178.27
 ---- batch: 080 ----
mean loss: 172.93
 ---- batch: 090 ----
mean loss: 173.11
 ---- batch: 100 ----
mean loss: 166.47
 ---- batch: 110 ----
mean loss: 176.27
train mean loss: 172.19
epoch train time: 0:00:15.509304
elapsed time: 0:55:09.890046
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 02:41:40.580318
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.47
 ---- batch: 020 ----
mean loss: 157.79
 ---- batch: 030 ----
mean loss: 178.39
 ---- batch: 040 ----
mean loss: 167.64
 ---- batch: 050 ----
mean loss: 167.28
 ---- batch: 060 ----
mean loss: 176.18
 ---- batch: 070 ----
mean loss: 173.93
 ---- batch: 080 ----
mean loss: 170.48
 ---- batch: 090 ----
mean loss: 175.43
 ---- batch: 100 ----
mean loss: 170.34
 ---- batch: 110 ----
mean loss: 183.01
train mean loss: 172.04
epoch train time: 0:00:15.523222
elapsed time: 0:55:25.414168
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 02:41:56.104482
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.62
 ---- batch: 020 ----
mean loss: 171.02
 ---- batch: 030 ----
mean loss: 166.83
 ---- batch: 040 ----
mean loss: 175.85
 ---- batch: 050 ----
mean loss: 176.28
 ---- batch: 060 ----
mean loss: 173.50
 ---- batch: 070 ----
mean loss: 166.96
 ---- batch: 080 ----
mean loss: 167.70
 ---- batch: 090 ----
mean loss: 166.75
 ---- batch: 100 ----
mean loss: 169.79
 ---- batch: 110 ----
mean loss: 168.00
train mean loss: 172.01
epoch train time: 0:00:15.512245
elapsed time: 0:55:40.927374
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 02:42:11.617707
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.37
 ---- batch: 020 ----
mean loss: 174.06
 ---- batch: 030 ----
mean loss: 173.28
 ---- batch: 040 ----
mean loss: 175.32
 ---- batch: 050 ----
mean loss: 173.35
 ---- batch: 060 ----
mean loss: 174.71
 ---- batch: 070 ----
mean loss: 165.57
 ---- batch: 080 ----
mean loss: 170.43
 ---- batch: 090 ----
mean loss: 176.05
 ---- batch: 100 ----
mean loss: 165.19
 ---- batch: 110 ----
mean loss: 172.73
train mean loss: 172.32
epoch train time: 0:00:15.553819
elapsed time: 0:55:56.482184
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 02:42:27.172473
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.21
 ---- batch: 020 ----
mean loss: 169.79
 ---- batch: 030 ----
mean loss: 170.50
 ---- batch: 040 ----
mean loss: 179.43
 ---- batch: 050 ----
mean loss: 175.40
 ---- batch: 060 ----
mean loss: 172.94
 ---- batch: 070 ----
mean loss: 175.01
 ---- batch: 080 ----
mean loss: 169.93
 ---- batch: 090 ----
mean loss: 170.64
 ---- batch: 100 ----
mean loss: 173.81
 ---- batch: 110 ----
mean loss: 168.46
train mean loss: 171.95
epoch train time: 0:00:15.525622
elapsed time: 0:56:12.008816
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 02:42:42.699323
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.55
 ---- batch: 020 ----
mean loss: 166.09
 ---- batch: 030 ----
mean loss: 161.58
 ---- batch: 040 ----
mean loss: 181.26
 ---- batch: 050 ----
mean loss: 173.49
 ---- batch: 060 ----
mean loss: 178.52
 ---- batch: 070 ----
mean loss: 165.32
 ---- batch: 080 ----
mean loss: 168.55
 ---- batch: 090 ----
mean loss: 163.70
 ---- batch: 100 ----
mean loss: 180.93
 ---- batch: 110 ----
mean loss: 177.27
train mean loss: 171.89
epoch train time: 0:00:15.525019
elapsed time: 0:56:27.534986
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 02:42:58.225276
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.28
 ---- batch: 020 ----
mean loss: 175.91
 ---- batch: 030 ----
mean loss: 168.25
 ---- batch: 040 ----
mean loss: 162.96
 ---- batch: 050 ----
mean loss: 177.02
 ---- batch: 060 ----
mean loss: 169.53
 ---- batch: 070 ----
mean loss: 180.84
 ---- batch: 080 ----
mean loss: 172.95
 ---- batch: 090 ----
mean loss: 169.28
 ---- batch: 100 ----
mean loss: 174.54
 ---- batch: 110 ----
mean loss: 174.84
train mean loss: 172.11
epoch train time: 0:00:15.514498
elapsed time: 0:56:43.050446
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 02:43:13.740756
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.03
 ---- batch: 020 ----
mean loss: 177.29
 ---- batch: 030 ----
mean loss: 177.03
 ---- batch: 040 ----
mean loss: 175.98
 ---- batch: 050 ----
mean loss: 165.35
 ---- batch: 060 ----
mean loss: 169.75
 ---- batch: 070 ----
mean loss: 177.19
 ---- batch: 080 ----
mean loss: 163.81
 ---- batch: 090 ----
mean loss: 161.69
 ---- batch: 100 ----
mean loss: 170.13
 ---- batch: 110 ----
mean loss: 172.67
train mean loss: 172.08
epoch train time: 0:00:15.678624
elapsed time: 0:56:58.730066
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 02:43:29.420328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.17
 ---- batch: 020 ----
mean loss: 174.04
 ---- batch: 030 ----
mean loss: 171.58
 ---- batch: 040 ----
mean loss: 171.79
 ---- batch: 050 ----
mean loss: 168.28
 ---- batch: 060 ----
mean loss: 169.18
 ---- batch: 070 ----
mean loss: 164.26
 ---- batch: 080 ----
mean loss: 173.83
 ---- batch: 090 ----
mean loss: 175.08
 ---- batch: 100 ----
mean loss: 174.09
 ---- batch: 110 ----
mean loss: 170.64
train mean loss: 171.86
epoch train time: 0:00:15.578160
elapsed time: 0:57:14.309273
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 02:43:44.999563
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.82
 ---- batch: 020 ----
mean loss: 167.41
 ---- batch: 030 ----
mean loss: 175.89
 ---- batch: 040 ----
mean loss: 169.31
 ---- batch: 050 ----
mean loss: 172.56
 ---- batch: 060 ----
mean loss: 174.03
 ---- batch: 070 ----
mean loss: 172.99
 ---- batch: 080 ----
mean loss: 174.21
 ---- batch: 090 ----
mean loss: 174.70
 ---- batch: 100 ----
mean loss: 179.45
 ---- batch: 110 ----
mean loss: 166.57
train mean loss: 171.99
epoch train time: 0:00:15.527691
elapsed time: 0:57:29.837904
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 02:44:00.528324
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.34
 ---- batch: 020 ----
mean loss: 176.04
 ---- batch: 030 ----
mean loss: 179.22
 ---- batch: 040 ----
mean loss: 171.88
 ---- batch: 050 ----
mean loss: 170.04
 ---- batch: 060 ----
mean loss: 173.76
 ---- batch: 070 ----
mean loss: 167.46
 ---- batch: 080 ----
mean loss: 163.78
 ---- batch: 090 ----
mean loss: 166.52
 ---- batch: 100 ----
mean loss: 177.50
 ---- batch: 110 ----
mean loss: 169.56
train mean loss: 171.82
epoch train time: 0:00:15.491672
elapsed time: 0:57:45.330685
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 02:44:16.020978
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.17
 ---- batch: 020 ----
mean loss: 172.90
 ---- batch: 030 ----
mean loss: 166.86
 ---- batch: 040 ----
mean loss: 170.52
 ---- batch: 050 ----
mean loss: 176.43
 ---- batch: 060 ----
mean loss: 166.52
 ---- batch: 070 ----
mean loss: 168.23
 ---- batch: 080 ----
mean loss: 182.37
 ---- batch: 090 ----
mean loss: 173.28
 ---- batch: 100 ----
mean loss: 171.59
 ---- batch: 110 ----
mean loss: 175.00
train mean loss: 172.25
epoch train time: 0:00:15.502776
elapsed time: 0:58:00.834401
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 02:44:31.524716
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.77
 ---- batch: 020 ----
mean loss: 173.22
 ---- batch: 030 ----
mean loss: 176.68
 ---- batch: 040 ----
mean loss: 174.10
 ---- batch: 050 ----
mean loss: 180.37
 ---- batch: 060 ----
mean loss: 169.56
 ---- batch: 070 ----
mean loss: 168.40
 ---- batch: 080 ----
mean loss: 169.23
 ---- batch: 090 ----
mean loss: 170.60
 ---- batch: 100 ----
mean loss: 174.71
 ---- batch: 110 ----
mean loss: 171.95
train mean loss: 172.00
epoch train time: 0:00:15.517934
elapsed time: 0:58:16.353367
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 02:44:47.043635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.57
 ---- batch: 020 ----
mean loss: 168.98
 ---- batch: 030 ----
mean loss: 175.69
 ---- batch: 040 ----
mean loss: 178.89
 ---- batch: 050 ----
mean loss: 165.91
 ---- batch: 060 ----
mean loss: 170.24
 ---- batch: 070 ----
mean loss: 177.77
 ---- batch: 080 ----
mean loss: 178.86
 ---- batch: 090 ----
mean loss: 165.85
 ---- batch: 100 ----
mean loss: 168.89
 ---- batch: 110 ----
mean loss: 173.12
train mean loss: 171.90
epoch train time: 0:00:15.518320
elapsed time: 0:58:31.872577
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 02:45:02.562848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.15
 ---- batch: 020 ----
mean loss: 172.93
 ---- batch: 030 ----
mean loss: 181.66
 ---- batch: 040 ----
mean loss: 169.96
 ---- batch: 050 ----
mean loss: 173.58
 ---- batch: 060 ----
mean loss: 172.86
 ---- batch: 070 ----
mean loss: 169.32
 ---- batch: 080 ----
mean loss: 168.31
 ---- batch: 090 ----
mean loss: 169.15
 ---- batch: 100 ----
mean loss: 175.57
 ---- batch: 110 ----
mean loss: 167.47
train mean loss: 172.19
epoch train time: 0:00:15.493908
elapsed time: 0:58:47.367415
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 02:45:18.057740
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.26
 ---- batch: 020 ----
mean loss: 169.45
 ---- batch: 030 ----
mean loss: 166.25
 ---- batch: 040 ----
mean loss: 170.42
 ---- batch: 050 ----
mean loss: 180.74
 ---- batch: 060 ----
mean loss: 171.51
 ---- batch: 070 ----
mean loss: 172.50
 ---- batch: 080 ----
mean loss: 170.63
 ---- batch: 090 ----
mean loss: 177.74
 ---- batch: 100 ----
mean loss: 173.13
 ---- batch: 110 ----
mean loss: 165.09
train mean loss: 171.79
epoch train time: 0:00:15.544975
elapsed time: 0:59:02.913341
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 02:45:33.603626
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 165.11
 ---- batch: 020 ----
mean loss: 176.18
 ---- batch: 030 ----
mean loss: 173.38
 ---- batch: 040 ----
mean loss: 169.36
 ---- batch: 050 ----
mean loss: 173.13
 ---- batch: 060 ----
mean loss: 178.65
 ---- batch: 070 ----
mean loss: 171.02
 ---- batch: 080 ----
mean loss: 177.11
 ---- batch: 090 ----
mean loss: 173.63
 ---- batch: 100 ----
mean loss: 162.41
 ---- batch: 110 ----
mean loss: 169.83
train mean loss: 172.09
epoch train time: 0:00:15.550080
elapsed time: 0:59:18.464350
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 02:45:49.154640
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.72
 ---- batch: 020 ----
mean loss: 165.84
 ---- batch: 030 ----
mean loss: 179.64
 ---- batch: 040 ----
mean loss: 178.53
 ---- batch: 050 ----
mean loss: 165.26
 ---- batch: 060 ----
mean loss: 178.71
 ---- batch: 070 ----
mean loss: 164.70
 ---- batch: 080 ----
mean loss: 165.62
 ---- batch: 090 ----
mean loss: 177.42
 ---- batch: 100 ----
mean loss: 176.03
 ---- batch: 110 ----
mean loss: 172.07
train mean loss: 172.18
epoch train time: 0:00:15.594959
elapsed time: 0:59:34.060333
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 02:46:04.750635
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.04
 ---- batch: 020 ----
mean loss: 180.06
 ---- batch: 030 ----
mean loss: 174.59
 ---- batch: 040 ----
mean loss: 166.26
 ---- batch: 050 ----
mean loss: 174.59
 ---- batch: 060 ----
mean loss: 158.02
 ---- batch: 070 ----
mean loss: 181.06
 ---- batch: 080 ----
mean loss: 166.48
 ---- batch: 090 ----
mean loss: 176.68
 ---- batch: 100 ----
mean loss: 173.20
 ---- batch: 110 ----
mean loss: 172.61
train mean loss: 171.72
epoch train time: 0:00:15.567870
elapsed time: 0:59:49.629132
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 02:46:20.319412
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.61
 ---- batch: 020 ----
mean loss: 171.16
 ---- batch: 030 ----
mean loss: 175.48
 ---- batch: 040 ----
mean loss: 166.41
 ---- batch: 050 ----
mean loss: 170.60
 ---- batch: 060 ----
mean loss: 173.61
 ---- batch: 070 ----
mean loss: 182.31
 ---- batch: 080 ----
mean loss: 178.47
 ---- batch: 090 ----
mean loss: 170.68
 ---- batch: 100 ----
mean loss: 171.00
 ---- batch: 110 ----
mean loss: 165.47
train mean loss: 171.65
epoch train time: 0:00:15.595069
elapsed time: 1:00:05.225211
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 02:46:35.915541
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.85
 ---- batch: 020 ----
mean loss: 168.33
 ---- batch: 030 ----
mean loss: 166.45
 ---- batch: 040 ----
mean loss: 177.31
 ---- batch: 050 ----
mean loss: 173.76
 ---- batch: 060 ----
mean loss: 173.68
 ---- batch: 070 ----
mean loss: 175.70
 ---- batch: 080 ----
mean loss: 175.05
 ---- batch: 090 ----
mean loss: 168.67
 ---- batch: 100 ----
mean loss: 168.09
 ---- batch: 110 ----
mean loss: 171.36
train mean loss: 171.82
epoch train time: 0:00:15.569241
elapsed time: 1:00:20.795346
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 02:46:51.485697
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.28
 ---- batch: 020 ----
mean loss: 177.76
 ---- batch: 030 ----
mean loss: 181.21
 ---- batch: 040 ----
mean loss: 176.07
 ---- batch: 050 ----
mean loss: 171.88
 ---- batch: 060 ----
mean loss: 174.62
 ---- batch: 070 ----
mean loss: 169.76
 ---- batch: 080 ----
mean loss: 161.82
 ---- batch: 090 ----
mean loss: 174.88
 ---- batch: 100 ----
mean loss: 170.98
 ---- batch: 110 ----
mean loss: 168.08
train mean loss: 171.87
epoch train time: 0:00:15.567837
elapsed time: 1:00:36.364116
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 02:47:07.054421
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.01
 ---- batch: 020 ----
mean loss: 164.02
 ---- batch: 030 ----
mean loss: 166.67
 ---- batch: 040 ----
mean loss: 181.39
 ---- batch: 050 ----
mean loss: 169.10
 ---- batch: 060 ----
mean loss: 172.46
 ---- batch: 070 ----
mean loss: 173.20
 ---- batch: 080 ----
mean loss: 171.28
 ---- batch: 090 ----
mean loss: 173.88
 ---- batch: 100 ----
mean loss: 164.98
 ---- batch: 110 ----
mean loss: 175.93
train mean loss: 171.74
epoch train time: 0:00:15.600996
elapsed time: 1:00:51.966113
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 02:47:22.656439
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 168.26
 ---- batch: 020 ----
mean loss: 162.21
 ---- batch: 030 ----
mean loss: 165.08
 ---- batch: 040 ----
mean loss: 180.74
 ---- batch: 050 ----
mean loss: 178.06
 ---- batch: 060 ----
mean loss: 163.51
 ---- batch: 070 ----
mean loss: 170.90
 ---- batch: 080 ----
mean loss: 179.98
 ---- batch: 090 ----
mean loss: 168.24
 ---- batch: 100 ----
mean loss: 177.99
 ---- batch: 110 ----
mean loss: 176.27
train mean loss: 171.81
epoch train time: 0:00:15.656256
elapsed time: 1:01:07.623236
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 02:47:38.313564
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.85
 ---- batch: 020 ----
mean loss: 171.26
 ---- batch: 030 ----
mean loss: 172.67
 ---- batch: 040 ----
mean loss: 175.81
 ---- batch: 050 ----
mean loss: 162.92
 ---- batch: 060 ----
mean loss: 180.16
 ---- batch: 070 ----
mean loss: 172.90
 ---- batch: 080 ----
mean loss: 175.04
 ---- batch: 090 ----
mean loss: 173.70
 ---- batch: 100 ----
mean loss: 167.64
 ---- batch: 110 ----
mean loss: 168.24
train mean loss: 171.82
epoch train time: 0:00:15.577683
elapsed time: 1:01:23.201961
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 02:47:53.892474
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.19
 ---- batch: 020 ----
mean loss: 167.66
 ---- batch: 030 ----
mean loss: 174.53
 ---- batch: 040 ----
mean loss: 175.21
 ---- batch: 050 ----
mean loss: 167.37
 ---- batch: 060 ----
mean loss: 176.80
 ---- batch: 070 ----
mean loss: 175.91
 ---- batch: 080 ----
mean loss: 172.90
 ---- batch: 090 ----
mean loss: 168.66
 ---- batch: 100 ----
mean loss: 178.33
 ---- batch: 110 ----
mean loss: 168.76
train mean loss: 172.28
epoch train time: 0:00:15.554197
elapsed time: 1:01:38.758062
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 02:48:09.448101
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.99
 ---- batch: 020 ----
mean loss: 169.89
 ---- batch: 030 ----
mean loss: 162.21
 ---- batch: 040 ----
mean loss: 172.38
 ---- batch: 050 ----
mean loss: 168.88
 ---- batch: 060 ----
mean loss: 177.45
 ---- batch: 070 ----
mean loss: 168.97
 ---- batch: 080 ----
mean loss: 176.96
 ---- batch: 090 ----
mean loss: 178.31
 ---- batch: 100 ----
mean loss: 172.45
 ---- batch: 110 ----
mean loss: 173.13
train mean loss: 171.98
epoch train time: 0:00:15.546620
elapsed time: 1:01:54.305448
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 02:48:24.995738
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.65
 ---- batch: 020 ----
mean loss: 163.31
 ---- batch: 030 ----
mean loss: 167.20
 ---- batch: 040 ----
mean loss: 180.45
 ---- batch: 050 ----
mean loss: 167.97
 ---- batch: 060 ----
mean loss: 178.11
 ---- batch: 070 ----
mean loss: 173.75
 ---- batch: 080 ----
mean loss: 167.22
 ---- batch: 090 ----
mean loss: 170.05
 ---- batch: 100 ----
mean loss: 169.44
 ---- batch: 110 ----
mean loss: 170.26
train mean loss: 171.62
epoch train time: 0:00:15.656200
elapsed time: 1:02:09.962631
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 02:48:40.652925
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.46
 ---- batch: 020 ----
mean loss: 174.18
 ---- batch: 030 ----
mean loss: 168.26
 ---- batch: 040 ----
mean loss: 176.33
 ---- batch: 050 ----
mean loss: 172.30
 ---- batch: 060 ----
mean loss: 183.15
 ---- batch: 070 ----
mean loss: 170.24
 ---- batch: 080 ----
mean loss: 168.66
 ---- batch: 090 ----
mean loss: 165.43
 ---- batch: 100 ----
mean loss: 170.01
 ---- batch: 110 ----
mean loss: 167.24
train mean loss: 171.61
epoch train time: 0:00:15.568294
elapsed time: 1:02:25.531782
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 02:48:56.222099
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.07
 ---- batch: 020 ----
mean loss: 183.93
 ---- batch: 030 ----
mean loss: 168.65
 ---- batch: 040 ----
mean loss: 168.89
 ---- batch: 050 ----
mean loss: 172.31
 ---- batch: 060 ----
mean loss: 176.07
 ---- batch: 070 ----
mean loss: 165.41
 ---- batch: 080 ----
mean loss: 167.41
 ---- batch: 090 ----
mean loss: 173.76
 ---- batch: 100 ----
mean loss: 168.27
 ---- batch: 110 ----
mean loss: 165.96
train mean loss: 171.38
epoch train time: 0:00:15.563351
elapsed time: 1:02:41.096138
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 02:49:11.786429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.30
 ---- batch: 020 ----
mean loss: 174.22
 ---- batch: 030 ----
mean loss: 172.94
 ---- batch: 040 ----
mean loss: 175.80
 ---- batch: 050 ----
mean loss: 170.58
 ---- batch: 060 ----
mean loss: 173.80
 ---- batch: 070 ----
mean loss: 184.62
 ---- batch: 080 ----
mean loss: 162.01
 ---- batch: 090 ----
mean loss: 161.87
 ---- batch: 100 ----
mean loss: 176.37
 ---- batch: 110 ----
mean loss: 166.02
train mean loss: 171.52
epoch train time: 0:00:15.581962
elapsed time: 1:02:56.678957
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 02:49:27.369278
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 163.89
 ---- batch: 020 ----
mean loss: 175.77
 ---- batch: 030 ----
mean loss: 175.81
 ---- batch: 040 ----
mean loss: 171.53
 ---- batch: 050 ----
mean loss: 168.93
 ---- batch: 060 ----
mean loss: 182.11
 ---- batch: 070 ----
mean loss: 176.58
 ---- batch: 080 ----
mean loss: 178.68
 ---- batch: 090 ----
mean loss: 164.47
 ---- batch: 100 ----
mean loss: 168.21
 ---- batch: 110 ----
mean loss: 162.20
train mean loss: 171.71
epoch train time: 0:00:15.609919
elapsed time: 1:03:12.289924
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 02:49:42.980297
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 164.87
 ---- batch: 020 ----
mean loss: 166.12
 ---- batch: 030 ----
mean loss: 174.36
 ---- batch: 040 ----
mean loss: 164.43
 ---- batch: 050 ----
mean loss: 169.88
 ---- batch: 060 ----
mean loss: 173.02
 ---- batch: 070 ----
mean loss: 178.29
 ---- batch: 080 ----
mean loss: 169.78
 ---- batch: 090 ----
mean loss: 185.10
 ---- batch: 100 ----
mean loss: 168.01
 ---- batch: 110 ----
mean loss: 173.95
train mean loss: 171.77
epoch train time: 0:00:15.580310
elapsed time: 1:03:27.871298
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 02:49:58.561595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.83
 ---- batch: 020 ----
mean loss: 167.99
 ---- batch: 030 ----
mean loss: 170.45
 ---- batch: 040 ----
mean loss: 168.15
 ---- batch: 050 ----
mean loss: 177.49
 ---- batch: 060 ----
mean loss: 179.13
 ---- batch: 070 ----
mean loss: 175.18
 ---- batch: 080 ----
mean loss: 168.46
 ---- batch: 090 ----
mean loss: 170.07
 ---- batch: 100 ----
mean loss: 180.19
 ---- batch: 110 ----
mean loss: 166.18
train mean loss: 171.38
epoch train time: 0:00:15.573032
elapsed time: 1:03:43.445304
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 02:50:14.135589
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.91
 ---- batch: 020 ----
mean loss: 181.87
 ---- batch: 030 ----
mean loss: 181.39
 ---- batch: 040 ----
mean loss: 168.08
 ---- batch: 050 ----
mean loss: 166.19
 ---- batch: 060 ----
mean loss: 171.73
 ---- batch: 070 ----
mean loss: 172.62
 ---- batch: 080 ----
mean loss: 156.41
 ---- batch: 090 ----
mean loss: 168.42
 ---- batch: 100 ----
mean loss: 176.52
 ---- batch: 110 ----
mean loss: 172.76
train mean loss: 171.77
epoch train time: 0:00:15.599077
elapsed time: 1:03:59.045335
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 02:50:29.735624
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 167.13
 ---- batch: 020 ----
mean loss: 181.05
 ---- batch: 030 ----
mean loss: 164.45
 ---- batch: 040 ----
mean loss: 183.09
 ---- batch: 050 ----
mean loss: 176.34
 ---- batch: 060 ----
mean loss: 169.61
 ---- batch: 070 ----
mean loss: 177.45
 ---- batch: 080 ----
mean loss: 163.65
 ---- batch: 090 ----
mean loss: 163.26
 ---- batch: 100 ----
mean loss: 176.84
 ---- batch: 110 ----
mean loss: 167.93
train mean loss: 171.69
epoch train time: 0:00:15.601314
elapsed time: 1:04:14.647634
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 02:50:45.337984
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 166.69
 ---- batch: 020 ----
mean loss: 170.77
 ---- batch: 030 ----
mean loss: 169.07
 ---- batch: 040 ----
mean loss: 169.42
 ---- batch: 050 ----
mean loss: 171.02
 ---- batch: 060 ----
mean loss: 170.69
 ---- batch: 070 ----
mean loss: 178.66
 ---- batch: 080 ----
mean loss: 177.17
 ---- batch: 090 ----
mean loss: 167.47
 ---- batch: 100 ----
mean loss: 174.33
 ---- batch: 110 ----
mean loss: 175.64
train mean loss: 171.51
epoch train time: 0:00:15.589505
elapsed time: 1:04:30.238199
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 02:51:00.928499
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 173.33
 ---- batch: 020 ----
mean loss: 170.74
 ---- batch: 030 ----
mean loss: 176.31
 ---- batch: 040 ----
mean loss: 172.55
 ---- batch: 050 ----
mean loss: 166.19
 ---- batch: 060 ----
mean loss: 177.04
 ---- batch: 070 ----
mean loss: 170.93
 ---- batch: 080 ----
mean loss: 164.70
 ---- batch: 090 ----
mean loss: 168.86
 ---- batch: 100 ----
mean loss: 176.83
 ---- batch: 110 ----
mean loss: 176.67
train mean loss: 171.87
epoch train time: 0:00:15.575216
elapsed time: 1:04:45.814313
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 02:51:16.504655
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.31
 ---- batch: 020 ----
mean loss: 174.53
 ---- batch: 030 ----
mean loss: 170.70
 ---- batch: 040 ----
mean loss: 170.80
 ---- batch: 050 ----
mean loss: 172.45
 ---- batch: 060 ----
mean loss: 170.08
 ---- batch: 070 ----
mean loss: 172.08
 ---- batch: 080 ----
mean loss: 169.46
 ---- batch: 090 ----
mean loss: 170.01
 ---- batch: 100 ----
mean loss: 171.23
 ---- batch: 110 ----
mean loss: 170.23
train mean loss: 171.92
epoch train time: 0:00:15.635777
elapsed time: 1:05:01.450989
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 02:51:32.141270
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 162.86
 ---- batch: 020 ----
mean loss: 174.89
 ---- batch: 030 ----
mean loss: 172.65
 ---- batch: 040 ----
mean loss: 169.03
 ---- batch: 050 ----
mean loss: 168.01
 ---- batch: 060 ----
mean loss: 172.93
 ---- batch: 070 ----
mean loss: 172.83
 ---- batch: 080 ----
mean loss: 167.57
 ---- batch: 090 ----
mean loss: 171.23
 ---- batch: 100 ----
mean loss: 175.89
 ---- batch: 110 ----
mean loss: 181.30
train mean loss: 171.38
epoch train time: 0:00:15.580009
elapsed time: 1:05:17.031964
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 02:51:47.722243
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 170.57
 ---- batch: 020 ----
mean loss: 169.96
 ---- batch: 030 ----
mean loss: 167.57
 ---- batch: 040 ----
mean loss: 171.52
 ---- batch: 050 ----
mean loss: 172.11
 ---- batch: 060 ----
mean loss: 168.62
 ---- batch: 070 ----
mean loss: 166.96
 ---- batch: 080 ----
mean loss: 172.17
 ---- batch: 090 ----
mean loss: 176.88
 ---- batch: 100 ----
mean loss: 181.09
 ---- batch: 110 ----
mean loss: 171.65
train mean loss: 171.64
epoch train time: 0:00:15.592300
elapsed time: 1:05:32.625193
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 02:52:03.315495
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 169.29
 ---- batch: 020 ----
mean loss: 170.77
 ---- batch: 030 ----
mean loss: 165.04
 ---- batch: 040 ----
mean loss: 172.14
 ---- batch: 050 ----
mean loss: 166.32
 ---- batch: 060 ----
mean loss: 175.22
 ---- batch: 070 ----
mean loss: 169.97
 ---- batch: 080 ----
mean loss: 176.18
 ---- batch: 090 ----
mean loss: 174.49
 ---- batch: 100 ----
mean loss: 175.66
 ---- batch: 110 ----
mean loss: 173.39
train mean loss: 171.44
epoch train time: 0:00:15.599449
elapsed time: 1:05:48.235518
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_2/checkpoint.pth.tar
**** end time: 2019-09-26 02:52:18.925310 ****
