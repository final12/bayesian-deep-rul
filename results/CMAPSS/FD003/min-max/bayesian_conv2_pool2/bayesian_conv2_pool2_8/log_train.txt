Namespace(batch_size=512, dataset='CMAPSS/FD003', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD003/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_8', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv2_pool2', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 5418
use_cuda: True
Dataset: CMAPSS/FD003
Building BayesianConv2Pool2...
Done.
**** start time: 2019-09-20 21:27:37.781781 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1             [-1, 8, 26, 1]           1,120
           Sigmoid-2             [-1, 8, 26, 1]               0
         AvgPool2d-3             [-1, 8, 13, 1]               0
    BayesianConv2d-4            [-1, 14, 12, 1]             448
           Sigmoid-5            [-1, 14, 12, 1]               0
         AvgPool2d-6             [-1, 14, 6, 1]               0
           Flatten-7                   [-1, 84]               0
    BayesianLinear-8                    [-1, 1]             168
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 1,736
Trainable params: 1,736
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-20 21:27:37.791900
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4259.13
 ---- batch: 020 ----
mean loss: 4179.18
 ---- batch: 030 ----
mean loss: 4164.26
 ---- batch: 040 ----
mean loss: 4024.44
train mean loss: 4140.18
epoch train time: 0:00:14.997098
elapsed time: 0:00:15.010224
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-20 21:27:52.792043
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3936.04
 ---- batch: 020 ----
mean loss: 3767.84
 ---- batch: 030 ----
mean loss: 3651.18
 ---- batch: 040 ----
mean loss: 3638.90
train mean loss: 3732.13
epoch train time: 0:00:00.612051
elapsed time: 0:00:15.622469
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-20 21:27:53.404347
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3433.63
 ---- batch: 020 ----
mean loss: 3374.71
 ---- batch: 030 ----
mean loss: 3341.84
 ---- batch: 040 ----
mean loss: 3209.36
train mean loss: 3322.00
epoch train time: 0:00:00.596518
elapsed time: 0:00:16.219250
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-20 21:27:54.001097
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3086.04
 ---- batch: 020 ----
mean loss: 3102.53
 ---- batch: 030 ----
mean loss: 2880.25
 ---- batch: 040 ----
mean loss: 2918.08
train mean loss: 2995.60
epoch train time: 0:00:00.589981
elapsed time: 0:00:16.809421
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-20 21:27:54.591270
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2838.24
 ---- batch: 020 ----
mean loss: 2749.83
 ---- batch: 030 ----
mean loss: 2726.61
 ---- batch: 040 ----
mean loss: 2657.92
train mean loss: 2736.11
epoch train time: 0:00:00.597900
elapsed time: 0:00:17.407555
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-20 21:27:55.189435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2578.70
 ---- batch: 020 ----
mean loss: 2588.92
 ---- batch: 030 ----
mean loss: 2513.06
 ---- batch: 040 ----
mean loss: 2445.64
train mean loss: 2526.87
epoch train time: 0:00:00.608702
elapsed time: 0:00:18.016554
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-20 21:27:55.798419
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2393.51
 ---- batch: 020 ----
mean loss: 2411.16
 ---- batch: 030 ----
mean loss: 2342.20
 ---- batch: 040 ----
mean loss: 2241.80
train mean loss: 2342.24
epoch train time: 0:00:00.610860
elapsed time: 0:00:18.627629
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-20 21:27:56.409473
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2252.82
 ---- batch: 020 ----
mean loss: 2146.33
 ---- batch: 030 ----
mean loss: 2178.60
 ---- batch: 040 ----
mean loss: 2114.52
train mean loss: 2169.21
epoch train time: 0:00:00.618959
elapsed time: 0:00:19.246804
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-20 21:27:57.028667
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2118.82
 ---- batch: 020 ----
mean loss: 2002.77
 ---- batch: 030 ----
mean loss: 1973.68
 ---- batch: 040 ----
mean loss: 1934.21
train mean loss: 2001.90
epoch train time: 0:00:00.591372
elapsed time: 0:00:19.838388
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-20 21:27:57.620236
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1893.62
 ---- batch: 020 ----
mean loss: 1864.32
 ---- batch: 030 ----
mean loss: 1840.10
 ---- batch: 040 ----
mean loss: 1836.41
train mean loss: 1852.32
epoch train time: 0:00:00.615544
elapsed time: 0:00:20.454119
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-20 21:27:58.235962
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1762.97
 ---- batch: 020 ----
mean loss: 1758.29
 ---- batch: 030 ----
mean loss: 1733.64
 ---- batch: 040 ----
mean loss: 1660.16
train mean loss: 1729.74
epoch train time: 0:00:00.604041
elapsed time: 0:00:21.058349
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-20 21:27:58.840198
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1674.03
 ---- batch: 020 ----
mean loss: 1600.59
 ---- batch: 030 ----
mean loss: 1591.50
 ---- batch: 040 ----
mean loss: 1584.93
train mean loss: 1610.24
epoch train time: 0:00:00.629002
elapsed time: 0:00:21.687603
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-20 21:27:59.469476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1535.28
 ---- batch: 020 ----
mean loss: 1518.28
 ---- batch: 030 ----
mean loss: 1487.31
 ---- batch: 040 ----
mean loss: 1495.54
train mean loss: 1506.77
epoch train time: 0:00:00.626888
elapsed time: 0:00:22.314781
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-20 21:28:00.096645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1445.90
 ---- batch: 020 ----
mean loss: 1422.61
 ---- batch: 030 ----
mean loss: 1405.40
 ---- batch: 040 ----
mean loss: 1392.99
train mean loss: 1415.67
epoch train time: 0:00:00.615559
elapsed time: 0:00:22.930568
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-20 21:28:00.712443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1362.17
 ---- batch: 020 ----
mean loss: 1336.80
 ---- batch: 030 ----
mean loss: 1314.86
 ---- batch: 040 ----
mean loss: 1303.74
train mean loss: 1326.18
epoch train time: 0:00:00.612002
elapsed time: 0:00:23.542885
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-20 21:28:01.324736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1273.83
 ---- batch: 020 ----
mean loss: 1273.60
 ---- batch: 030 ----
mean loss: 1259.71
 ---- batch: 040 ----
mean loss: 1226.66
train mean loss: 1256.39
epoch train time: 0:00:00.588665
elapsed time: 0:00:24.131739
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-20 21:28:01.913600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1199.64
 ---- batch: 020 ----
mean loss: 1189.71
 ---- batch: 030 ----
mean loss: 1199.47
 ---- batch: 040 ----
mean loss: 1168.76
train mean loss: 1188.71
epoch train time: 0:00:00.572631
elapsed time: 0:00:24.704605
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-20 21:28:02.486452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1141.71
 ---- batch: 020 ----
mean loss: 1132.76
 ---- batch: 030 ----
mean loss: 1120.89
 ---- batch: 040 ----
mean loss: 1110.62
train mean loss: 1125.91
epoch train time: 0:00:00.618491
elapsed time: 0:00:25.323328
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-20 21:28:03.105179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1100.16
 ---- batch: 020 ----
mean loss: 1063.45
 ---- batch: 030 ----
mean loss: 1076.26
 ---- batch: 040 ----
mean loss: 1062.08
train mean loss: 1073.51
epoch train time: 0:00:00.643149
elapsed time: 0:00:25.966732
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-20 21:28:03.748633
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1033.07
 ---- batch: 020 ----
mean loss: 1027.40
 ---- batch: 030 ----
mean loss: 1032.20
 ---- batch: 040 ----
mean loss: 1004.19
train mean loss: 1024.38
epoch train time: 0:00:00.630647
elapsed time: 0:00:26.597628
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-20 21:28:04.379507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 999.01
 ---- batch: 020 ----
mean loss: 994.50
 ---- batch: 030 ----
mean loss: 976.27
 ---- batch: 040 ----
mean loss: 969.97
train mean loss: 984.75
epoch train time: 0:00:00.607630
elapsed time: 0:00:27.205477
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-20 21:28:04.987324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 960.46
 ---- batch: 020 ----
mean loss: 936.57
 ---- batch: 030 ----
mean loss: 934.55
 ---- batch: 040 ----
mean loss: 937.73
train mean loss: 940.56
epoch train time: 0:00:00.613760
elapsed time: 0:00:27.819421
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-20 21:28:05.601262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 924.69
 ---- batch: 020 ----
mean loss: 912.92
 ---- batch: 030 ----
mean loss: 906.85
 ---- batch: 040 ----
mean loss: 887.67
train mean loss: 908.16
epoch train time: 0:00:00.591715
elapsed time: 0:00:28.411336
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-20 21:28:06.193182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 903.46
 ---- batch: 020 ----
mean loss: 892.24
 ---- batch: 030 ----
mean loss: 869.98
 ---- batch: 040 ----
mean loss: 865.68
train mean loss: 880.28
epoch train time: 0:00:00.605573
elapsed time: 0:00:29.017128
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-20 21:28:06.798982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 865.38
 ---- batch: 020 ----
mean loss: 864.03
 ---- batch: 030 ----
mean loss: 856.17
 ---- batch: 040 ----
mean loss: 821.79
train mean loss: 850.48
epoch train time: 0:00:00.641436
elapsed time: 0:00:29.658794
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-20 21:28:07.440671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.72
 ---- batch: 020 ----
mean loss: 838.03
 ---- batch: 030 ----
mean loss: 823.38
 ---- batch: 040 ----
mean loss: 818.47
train mean loss: 826.50
epoch train time: 0:00:00.621550
elapsed time: 0:00:30.280594
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-20 21:28:08.062452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 817.33
 ---- batch: 020 ----
mean loss: 794.26
 ---- batch: 030 ----
mean loss: 810.76
 ---- batch: 040 ----
mean loss: 808.52
train mean loss: 807.45
epoch train time: 0:00:00.591545
elapsed time: 0:00:30.872329
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-20 21:28:08.654188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 790.19
 ---- batch: 020 ----
mean loss: 787.16
 ---- batch: 030 ----
mean loss: 777.58
 ---- batch: 040 ----
mean loss: 784.73
train mean loss: 784.19
epoch train time: 0:00:00.601034
elapsed time: 0:00:31.473554
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-20 21:28:09.255405
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 791.21
 ---- batch: 020 ----
mean loss: 757.50
 ---- batch: 030 ----
mean loss: 772.19
 ---- batch: 040 ----
mean loss: 766.15
train mean loss: 771.84
epoch train time: 0:00:00.594283
elapsed time: 0:00:32.068054
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-20 21:28:09.849953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 762.58
 ---- batch: 020 ----
mean loss: 742.28
 ---- batch: 030 ----
mean loss: 771.28
 ---- batch: 040 ----
mean loss: 750.77
train mean loss: 757.20
epoch train time: 0:00:00.601756
elapsed time: 0:00:32.670102
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-20 21:28:10.451971
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 751.48
 ---- batch: 020 ----
mean loss: 741.92
 ---- batch: 030 ----
mean loss: 746.18
 ---- batch: 040 ----
mean loss: 736.71
train mean loss: 743.49
epoch train time: 0:00:00.632762
elapsed time: 0:00:33.303138
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-20 21:28:11.085041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 748.86
 ---- batch: 020 ----
mean loss: 719.15
 ---- batch: 030 ----
mean loss: 738.74
 ---- batch: 040 ----
mean loss: 726.01
train mean loss: 732.57
epoch train time: 0:00:00.620462
elapsed time: 0:00:33.923923
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-20 21:28:11.705801
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 754.16
 ---- batch: 020 ----
mean loss: 703.33
 ---- batch: 030 ----
mean loss: 705.57
 ---- batch: 040 ----
mean loss: 736.08
train mean loss: 722.62
epoch train time: 0:00:00.614765
elapsed time: 0:00:34.538941
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-20 21:28:12.320797
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 722.35
 ---- batch: 020 ----
mean loss: 721.47
 ---- batch: 030 ----
mean loss: 719.83
 ---- batch: 040 ----
mean loss: 714.60
train mean loss: 718.30
epoch train time: 0:00:00.596857
elapsed time: 0:00:35.136010
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-20 21:28:12.917854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 711.91
 ---- batch: 020 ----
mean loss: 715.16
 ---- batch: 030 ----
mean loss: 697.67
 ---- batch: 040 ----
mean loss: 713.59
train mean loss: 711.47
epoch train time: 0:00:00.583483
elapsed time: 0:00:35.719674
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-20 21:28:13.501514
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 710.49
 ---- batch: 020 ----
mean loss: 691.40
 ---- batch: 030 ----
mean loss: 701.27
 ---- batch: 040 ----
mean loss: 724.87
train mean loss: 704.76
epoch train time: 0:00:00.594172
elapsed time: 0:00:36.314071
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-20 21:28:14.095921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 702.52
 ---- batch: 020 ----
mean loss: 707.28
 ---- batch: 030 ----
mean loss: 701.64
 ---- batch: 040 ----
mean loss: 700.61
train mean loss: 700.75
epoch train time: 0:00:00.613911
elapsed time: 0:00:36.928205
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-20 21:28:14.710062
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 693.42
 ---- batch: 020 ----
mean loss: 684.85
 ---- batch: 030 ----
mean loss: 686.12
 ---- batch: 040 ----
mean loss: 715.47
train mean loss: 694.92
epoch train time: 0:00:00.625488
elapsed time: 0:00:37.553906
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-20 21:28:15.335750
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 706.06
 ---- batch: 020 ----
mean loss: 682.74
 ---- batch: 030 ----
mean loss: 714.01
 ---- batch: 040 ----
mean loss: 693.88
train mean loss: 696.02
epoch train time: 0:00:00.604697
elapsed time: 0:00:38.158791
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-20 21:28:15.940664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 701.57
 ---- batch: 020 ----
mean loss: 688.30
 ---- batch: 030 ----
mean loss: 690.68
 ---- batch: 040 ----
mean loss: 685.28
train mean loss: 689.71
epoch train time: 0:00:00.590625
elapsed time: 0:00:38.749635
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-20 21:28:16.531484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 676.82
 ---- batch: 020 ----
mean loss: 705.26
 ---- batch: 030 ----
mean loss: 689.83
 ---- batch: 040 ----
mean loss: 679.23
train mean loss: 687.49
epoch train time: 0:00:00.593104
elapsed time: 0:00:39.342934
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-20 21:28:17.124781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 707.95
 ---- batch: 020 ----
mean loss: 688.70
 ---- batch: 030 ----
mean loss: 676.47
 ---- batch: 040 ----
mean loss: 670.22
train mean loss: 685.98
epoch train time: 0:00:00.584951
elapsed time: 0:00:39.928123
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-20 21:28:17.709978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.95
 ---- batch: 020 ----
mean loss: 682.53
 ---- batch: 030 ----
mean loss: 685.58
 ---- batch: 040 ----
mean loss: 689.07
train mean loss: 683.69
epoch train time: 0:00:00.634387
elapsed time: 0:00:40.562754
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-20 21:28:18.344636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 698.55
 ---- batch: 020 ----
mean loss: 670.64
 ---- batch: 030 ----
mean loss: 681.54
 ---- batch: 040 ----
mean loss: 672.83
train mean loss: 681.26
epoch train time: 0:00:00.640077
elapsed time: 0:00:41.203112
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-20 21:28:18.984953
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 681.79
 ---- batch: 020 ----
mean loss: 652.26
 ---- batch: 030 ----
mean loss: 687.25
 ---- batch: 040 ----
mean loss: 697.41
train mean loss: 681.13
epoch train time: 0:00:00.621371
elapsed time: 0:00:41.824681
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-20 21:28:19.606551
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 694.36
 ---- batch: 020 ----
mean loss: 659.72
 ---- batch: 030 ----
mean loss: 680.63
 ---- batch: 040 ----
mean loss: 685.46
train mean loss: 679.12
epoch train time: 0:00:00.582656
elapsed time: 0:00:42.407591
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-20 21:28:20.189450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 671.08
 ---- batch: 020 ----
mean loss: 686.02
 ---- batch: 030 ----
mean loss: 668.33
 ---- batch: 040 ----
mean loss: 697.65
train mean loss: 678.58
epoch train time: 0:00:00.588090
elapsed time: 0:00:42.995893
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-20 21:28:20.777740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.95
 ---- batch: 020 ----
mean loss: 687.39
 ---- batch: 030 ----
mean loss: 673.88
 ---- batch: 040 ----
mean loss: 680.81
train mean loss: 677.40
epoch train time: 0:00:00.594190
elapsed time: 0:00:43.590275
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-20 21:28:21.372123
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 679.14
 ---- batch: 020 ----
mean loss: 688.94
 ---- batch: 030 ----
mean loss: 661.10
 ---- batch: 040 ----
mean loss: 681.16
train mean loss: 675.30
epoch train time: 0:00:00.616444
elapsed time: 0:00:44.206931
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-20 21:28:21.988782
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 665.83
 ---- batch: 020 ----
mean loss: 666.85
 ---- batch: 030 ----
mean loss: 685.71
 ---- batch: 040 ----
mean loss: 682.94
train mean loss: 674.43
epoch train time: 0:00:00.610620
elapsed time: 0:00:44.817757
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-20 21:28:22.599607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 673.70
 ---- batch: 020 ----
mean loss: 688.34
 ---- batch: 030 ----
mean loss: 664.87
 ---- batch: 040 ----
mean loss: 670.46
train mean loss: 674.38
epoch train time: 0:00:00.607899
elapsed time: 0:00:45.425860
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-20 21:28:23.207703
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 659.26
 ---- batch: 020 ----
mean loss: 665.85
 ---- batch: 030 ----
mean loss: 677.96
 ---- batch: 040 ----
mean loss: 678.45
train mean loss: 671.64
epoch train time: 0:00:00.602193
elapsed time: 0:00:46.028271
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-20 21:28:23.810139
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 680.76
 ---- batch: 020 ----
mean loss: 690.67
 ---- batch: 030 ----
mean loss: 645.53
 ---- batch: 040 ----
mean loss: 682.92
train mean loss: 673.73
epoch train time: 0:00:00.584998
elapsed time: 0:00:46.613475
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-20 21:28:24.395317
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.27
 ---- batch: 020 ----
mean loss: 662.59
 ---- batch: 030 ----
mean loss: 672.09
 ---- batch: 040 ----
mean loss: 679.86
train mean loss: 673.52
epoch train time: 0:00:00.593653
elapsed time: 0:00:47.207309
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-20 21:28:24.989168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 677.24
 ---- batch: 020 ----
mean loss: 663.07
 ---- batch: 030 ----
mean loss: 677.18
 ---- batch: 040 ----
mean loss: 670.00
train mean loss: 672.66
epoch train time: 0:00:00.620602
elapsed time: 0:00:47.828216
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-20 21:28:25.610071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.16
 ---- batch: 020 ----
mean loss: 653.84
 ---- batch: 030 ----
mean loss: 659.90
 ---- batch: 040 ----
mean loss: 687.76
train mean loss: 670.63
epoch train time: 0:00:00.647204
elapsed time: 0:00:48.475642
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-20 21:28:26.257522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 688.68
 ---- batch: 020 ----
mean loss: 669.63
 ---- batch: 030 ----
mean loss: 674.59
 ---- batch: 040 ----
mean loss: 653.63
train mean loss: 668.93
epoch train time: 0:00:00.632123
elapsed time: 0:00:49.108003
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-20 21:28:26.889850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 685.56
 ---- batch: 020 ----
mean loss: 670.02
 ---- batch: 030 ----
mean loss: 665.97
 ---- batch: 040 ----
mean loss: 655.18
train mean loss: 667.80
epoch train time: 0:00:00.600318
elapsed time: 0:00:49.708536
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-20 21:28:27.490386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 665.03
 ---- batch: 020 ----
mean loss: 684.26
 ---- batch: 030 ----
mean loss: 652.84
 ---- batch: 040 ----
mean loss: 650.17
train mean loss: 663.38
epoch train time: 0:00:00.592181
elapsed time: 0:00:50.300909
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-20 21:28:28.082906
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 650.36
 ---- batch: 020 ----
mean loss: 673.02
 ---- batch: 030 ----
mean loss: 646.53
 ---- batch: 040 ----
mean loss: 662.77
train mean loss: 655.57
epoch train time: 0:00:00.582927
elapsed time: 0:00:50.884199
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-20 21:28:28.666074
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 643.87
 ---- batch: 020 ----
mean loss: 626.79
 ---- batch: 030 ----
mean loss: 610.85
 ---- batch: 040 ----
mean loss: 569.80
train mean loss: 610.11
epoch train time: 0:00:00.638575
elapsed time: 0:00:51.523007
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-20 21:28:29.304858
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 529.46
 ---- batch: 020 ----
mean loss: 508.10
 ---- batch: 030 ----
mean loss: 468.62
 ---- batch: 040 ----
mean loss: 454.49
train mean loss: 487.07
epoch train time: 0:00:00.626738
elapsed time: 0:00:52.149950
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-20 21:28:29.931810
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 437.42
 ---- batch: 020 ----
mean loss: 429.01
 ---- batch: 030 ----
mean loss: 414.27
 ---- batch: 040 ----
mean loss: 411.44
train mean loss: 420.64
epoch train time: 0:00:00.620529
elapsed time: 0:00:52.770730
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-20 21:28:30.552602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.14
 ---- batch: 020 ----
mean loss: 393.91
 ---- batch: 030 ----
mean loss: 391.16
 ---- batch: 040 ----
mean loss: 385.44
train mean loss: 390.19
epoch train time: 0:00:00.587921
elapsed time: 0:00:53.358893
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-20 21:28:31.140748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 387.52
 ---- batch: 020 ----
mean loss: 379.69
 ---- batch: 030 ----
mean loss: 380.16
 ---- batch: 040 ----
mean loss: 364.21
train mean loss: 377.06
epoch train time: 0:00:00.592103
elapsed time: 0:00:53.951201
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-20 21:28:31.733047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 369.84
 ---- batch: 020 ----
mean loss: 371.21
 ---- batch: 030 ----
mean loss: 367.42
 ---- batch: 040 ----
mean loss: 362.02
train mean loss: 368.49
epoch train time: 0:00:00.589175
elapsed time: 0:00:54.540610
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-20 21:28:32.322476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.75
 ---- batch: 020 ----
mean loss: 348.42
 ---- batch: 030 ----
mean loss: 362.47
 ---- batch: 040 ----
mean loss: 360.90
train mean loss: 361.55
epoch train time: 0:00:00.627253
elapsed time: 0:00:55.168122
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-20 21:28:32.949975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 358.02
 ---- batch: 020 ----
mean loss: 355.21
 ---- batch: 030 ----
mean loss: 349.93
 ---- batch: 040 ----
mean loss: 351.58
train mean loss: 352.99
epoch train time: 0:00:00.621728
elapsed time: 0:00:55.790076
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-20 21:28:33.571936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.86
 ---- batch: 020 ----
mean loss: 338.39
 ---- batch: 030 ----
mean loss: 352.73
 ---- batch: 040 ----
mean loss: 350.58
train mean loss: 348.78
epoch train time: 0:00:00.625996
elapsed time: 0:00:56.416319
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-20 21:28:34.198173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.28
 ---- batch: 020 ----
mean loss: 345.25
 ---- batch: 030 ----
mean loss: 334.93
 ---- batch: 040 ----
mean loss: 337.32
train mean loss: 343.55
epoch train time: 0:00:00.599200
elapsed time: 0:00:57.015731
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-20 21:28:34.797618
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 340.78
 ---- batch: 020 ----
mean loss: 340.38
 ---- batch: 030 ----
mean loss: 341.32
 ---- batch: 040 ----
mean loss: 329.62
train mean loss: 337.03
epoch train time: 0:00:00.599297
elapsed time: 0:00:57.615254
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-20 21:28:35.397098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 333.84
 ---- batch: 020 ----
mean loss: 336.34
 ---- batch: 030 ----
mean loss: 335.91
 ---- batch: 040 ----
mean loss: 330.21
train mean loss: 333.65
epoch train time: 0:00:00.598384
elapsed time: 0:00:58.213824
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-20 21:28:35.995671
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.97
 ---- batch: 020 ----
mean loss: 332.51
 ---- batch: 030 ----
mean loss: 325.45
 ---- batch: 040 ----
mean loss: 332.03
train mean loss: 329.27
epoch train time: 0:00:00.617433
elapsed time: 0:00:58.831488
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-20 21:28:36.613318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.48
 ---- batch: 020 ----
mean loss: 316.72
 ---- batch: 030 ----
mean loss: 326.02
 ---- batch: 040 ----
mean loss: 323.53
train mean loss: 324.43
epoch train time: 0:00:00.611131
elapsed time: 0:00:59.442855
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-20 21:28:37.224711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 322.45
 ---- batch: 020 ----
mean loss: 316.93
 ---- batch: 030 ----
mean loss: 310.10
 ---- batch: 040 ----
mean loss: 326.28
train mean loss: 320.67
epoch train time: 0:00:00.632660
elapsed time: 0:01:00.075723
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-20 21:28:37.857574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 312.02
 ---- batch: 020 ----
mean loss: 318.11
 ---- batch: 030 ----
mean loss: 311.81
 ---- batch: 040 ----
mean loss: 327.99
train mean loss: 316.60
epoch train time: 0:00:00.596800
elapsed time: 0:01:00.672707
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-20 21:28:38.454562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.22
 ---- batch: 020 ----
mean loss: 313.16
 ---- batch: 030 ----
mean loss: 310.76
 ---- batch: 040 ----
mean loss: 315.30
train mean loss: 313.01
epoch train time: 0:00:00.604357
elapsed time: 0:01:01.277264
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-20 21:28:39.059129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.11
 ---- batch: 020 ----
mean loss: 317.05
 ---- batch: 030 ----
mean loss: 305.42
 ---- batch: 040 ----
mean loss: 309.98
train mean loss: 310.97
epoch train time: 0:00:00.594731
elapsed time: 0:01:01.872205
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-20 21:28:39.654047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.21
 ---- batch: 020 ----
mean loss: 311.95
 ---- batch: 030 ----
mean loss: 303.63
 ---- batch: 040 ----
mean loss: 305.35
train mean loss: 308.31
epoch train time: 0:00:00.630895
elapsed time: 0:01:02.503302
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-20 21:28:40.285149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.81
 ---- batch: 020 ----
mean loss: 303.62
 ---- batch: 030 ----
mean loss: 313.67
 ---- batch: 040 ----
mean loss: 294.05
train mean loss: 305.05
epoch train time: 0:00:00.631271
elapsed time: 0:01:03.134781
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-20 21:28:40.916645
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 308.28
 ---- batch: 020 ----
mean loss: 304.87
 ---- batch: 030 ----
mean loss: 309.07
 ---- batch: 040 ----
mean loss: 293.34
train mean loss: 303.01
epoch train time: 0:00:00.633995
elapsed time: 0:01:03.769011
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-20 21:28:41.550864
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 307.32
 ---- batch: 020 ----
mean loss: 302.34
 ---- batch: 030 ----
mean loss: 296.06
 ---- batch: 040 ----
mean loss: 300.34
train mean loss: 300.07
epoch train time: 0:00:00.608035
elapsed time: 0:01:04.377255
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-20 21:28:42.159096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.39
 ---- batch: 020 ----
mean loss: 306.60
 ---- batch: 030 ----
mean loss: 296.97
 ---- batch: 040 ----
mean loss: 298.79
train mean loss: 298.33
epoch train time: 0:00:00.613441
elapsed time: 0:01:04.990920
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-20 21:28:42.772815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 289.82
 ---- batch: 020 ----
mean loss: 299.54
 ---- batch: 030 ----
mean loss: 286.29
 ---- batch: 040 ----
mean loss: 301.51
train mean loss: 294.56
epoch train time: 0:00:00.590972
elapsed time: 0:01:05.582161
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-20 21:28:43.364009
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 295.31
 ---- batch: 020 ----
mean loss: 293.58
 ---- batch: 030 ----
mean loss: 289.34
 ---- batch: 040 ----
mean loss: 293.63
train mean loss: 292.75
epoch train time: 0:00:00.607108
elapsed time: 0:01:06.189511
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-20 21:28:43.971368
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 284.14
 ---- batch: 020 ----
mean loss: 290.91
 ---- batch: 030 ----
mean loss: 294.89
 ---- batch: 040 ----
mean loss: 298.24
train mean loss: 291.54
epoch train time: 0:00:00.625236
elapsed time: 0:01:06.814972
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-20 21:28:44.596832
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 288.36
 ---- batch: 020 ----
mean loss: 292.39
 ---- batch: 030 ----
mean loss: 281.36
 ---- batch: 040 ----
mean loss: 291.01
train mean loss: 289.47
epoch train time: 0:00:00.632931
elapsed time: 0:01:07.448124
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-20 21:28:45.229974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.68
 ---- batch: 020 ----
mean loss: 280.58
 ---- batch: 030 ----
mean loss: 297.38
 ---- batch: 040 ----
mean loss: 286.33
train mean loss: 287.31
epoch train time: 0:00:00.604533
elapsed time: 0:01:08.052844
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-20 21:28:45.834689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 285.08
 ---- batch: 020 ----
mean loss: 288.00
 ---- batch: 030 ----
mean loss: 289.26
 ---- batch: 040 ----
mean loss: 278.42
train mean loss: 285.08
epoch train time: 0:00:00.593930
elapsed time: 0:01:08.646964
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-20 21:28:46.428820
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 287.90
 ---- batch: 020 ----
mean loss: 276.09
 ---- batch: 030 ----
mean loss: 285.39
 ---- batch: 040 ----
mean loss: 285.20
train mean loss: 283.54
epoch train time: 0:00:00.598770
elapsed time: 0:01:09.245931
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-20 21:28:47.027777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.84
 ---- batch: 020 ----
mean loss: 282.11
 ---- batch: 030 ----
mean loss: 278.43
 ---- batch: 040 ----
mean loss: 283.76
train mean loss: 281.49
epoch train time: 0:00:00.605319
elapsed time: 0:01:09.851480
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-20 21:28:47.633344
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.26
 ---- batch: 020 ----
mean loss: 282.80
 ---- batch: 030 ----
mean loss: 279.15
 ---- batch: 040 ----
mean loss: 278.85
train mean loss: 279.81
epoch train time: 0:00:00.637503
elapsed time: 0:01:10.489223
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-20 21:28:48.271091
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.48
 ---- batch: 020 ----
mean loss: 281.88
 ---- batch: 030 ----
mean loss: 275.59
 ---- batch: 040 ----
mean loss: 279.23
train mean loss: 277.66
epoch train time: 0:00:00.633416
elapsed time: 0:01:11.122890
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-20 21:28:48.904752
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 280.92
 ---- batch: 020 ----
mean loss: 276.16
 ---- batch: 030 ----
mean loss: 268.13
 ---- batch: 040 ----
mean loss: 282.06
train mean loss: 276.93
epoch train time: 0:00:00.591822
elapsed time: 0:01:11.715028
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-20 21:28:49.496872
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.50
 ---- batch: 020 ----
mean loss: 280.34
 ---- batch: 030 ----
mean loss: 279.04
 ---- batch: 040 ----
mean loss: 273.30
train mean loss: 275.75
epoch train time: 0:00:00.595045
elapsed time: 0:01:12.310296
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-20 21:28:50.092168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 279.88
 ---- batch: 020 ----
mean loss: 277.87
 ---- batch: 030 ----
mean loss: 262.39
 ---- batch: 040 ----
mean loss: 272.06
train mean loss: 273.27
epoch train time: 0:00:00.596431
elapsed time: 0:01:12.906971
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-20 21:28:50.688843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 270.58
 ---- batch: 020 ----
mean loss: 270.03
 ---- batch: 030 ----
mean loss: 265.29
 ---- batch: 040 ----
mean loss: 277.27
train mean loss: 271.10
epoch train time: 0:00:00.604265
elapsed time: 0:01:13.511478
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-20 21:28:51.293351
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 269.08
 ---- batch: 020 ----
mean loss: 270.35
 ---- batch: 030 ----
mean loss: 270.50
 ---- batch: 040 ----
mean loss: 269.52
train mean loss: 269.80
epoch train time: 0:00:00.614243
elapsed time: 0:01:14.125980
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-20 21:28:51.907838
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.20
 ---- batch: 020 ----
mean loss: 269.45
 ---- batch: 030 ----
mean loss: 272.22
 ---- batch: 040 ----
mean loss: 262.28
train mean loss: 267.35
epoch train time: 0:00:00.628848
elapsed time: 0:01:14.755049
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-20 21:28:52.536901
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 267.27
 ---- batch: 020 ----
mean loss: 268.37
 ---- batch: 030 ----
mean loss: 262.33
 ---- batch: 040 ----
mean loss: 270.45
train mean loss: 266.50
epoch train time: 0:00:00.614348
elapsed time: 0:01:15.369589
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-20 21:28:53.151434
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.59
 ---- batch: 020 ----
mean loss: 267.49
 ---- batch: 030 ----
mean loss: 271.37
 ---- batch: 040 ----
mean loss: 266.03
train mean loss: 265.68
epoch train time: 0:00:00.592001
elapsed time: 0:01:15.961795
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-20 21:28:53.743659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 265.39
 ---- batch: 020 ----
mean loss: 264.28
 ---- batch: 030 ----
mean loss: 266.50
 ---- batch: 040 ----
mean loss: 258.14
train mean loss: 263.69
epoch train time: 0:00:00.591993
elapsed time: 0:01:16.554043
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-20 21:28:54.335888
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.55
 ---- batch: 020 ----
mean loss: 265.79
 ---- batch: 030 ----
mean loss: 263.81
 ---- batch: 040 ----
mean loss: 256.44
train mean loss: 261.73
epoch train time: 0:00:00.602097
elapsed time: 0:01:17.156335
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-20 21:28:54.938181
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.22
 ---- batch: 020 ----
mean loss: 266.27
 ---- batch: 030 ----
mean loss: 260.62
 ---- batch: 040 ----
mean loss: 256.45
train mean loss: 260.81
epoch train time: 0:00:00.615560
elapsed time: 0:01:17.772131
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-20 21:28:55.553999
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 262.41
 ---- batch: 020 ----
mean loss: 262.02
 ---- batch: 030 ----
mean loss: 247.12
 ---- batch: 040 ----
mean loss: 259.35
train mean loss: 258.58
epoch train time: 0:00:00.629322
elapsed time: 0:01:18.401697
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-20 21:28:56.183552
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.26
 ---- batch: 020 ----
mean loss: 262.15
 ---- batch: 030 ----
mean loss: 253.60
 ---- batch: 040 ----
mean loss: 253.42
train mean loss: 257.28
epoch train time: 0:00:00.609641
elapsed time: 0:01:19.011600
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-20 21:28:56.793456
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.99
 ---- batch: 020 ----
mean loss: 260.01
 ---- batch: 030 ----
mean loss: 247.96
 ---- batch: 040 ----
mean loss: 253.23
train mean loss: 255.76
epoch train time: 0:00:00.592186
elapsed time: 0:01:19.604025
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-20 21:28:57.385857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.69
 ---- batch: 020 ----
mean loss: 267.91
 ---- batch: 030 ----
mean loss: 254.76
 ---- batch: 040 ----
mean loss: 247.41
train mean loss: 255.31
epoch train time: 0:00:00.592873
elapsed time: 0:01:20.197069
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-20 21:28:57.978913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.12
 ---- batch: 020 ----
mean loss: 257.19
 ---- batch: 030 ----
mean loss: 247.27
 ---- batch: 040 ----
mean loss: 252.78
train mean loss: 253.96
epoch train time: 0:00:00.589360
elapsed time: 0:01:20.786667
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-20 21:28:58.568534
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.16
 ---- batch: 020 ----
mean loss: 256.78
 ---- batch: 030 ----
mean loss: 256.05
 ---- batch: 040 ----
mean loss: 244.60
train mean loss: 252.08
epoch train time: 0:00:00.628320
elapsed time: 0:01:21.415206
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-20 21:28:59.197067
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.85
 ---- batch: 020 ----
mean loss: 241.07
 ---- batch: 030 ----
mean loss: 255.37
 ---- batch: 040 ----
mean loss: 256.27
train mean loss: 250.12
epoch train time: 0:00:00.616611
elapsed time: 0:01:22.032052
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-20 21:28:59.813904
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.37
 ---- batch: 020 ----
mean loss: 247.27
 ---- batch: 030 ----
mean loss: 251.58
 ---- batch: 040 ----
mean loss: 249.91
train mean loss: 249.78
epoch train time: 0:00:00.604183
elapsed time: 0:01:22.636440
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-20 21:29:00.418285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 247.45
 ---- batch: 020 ----
mean loss: 244.04
 ---- batch: 030 ----
mean loss: 248.71
 ---- batch: 040 ----
mean loss: 247.54
train mean loss: 247.41
epoch train time: 0:00:00.605081
elapsed time: 0:01:23.241738
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-20 21:29:01.023600
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 243.23
 ---- batch: 020 ----
mean loss: 252.26
 ---- batch: 030 ----
mean loss: 248.31
 ---- batch: 040 ----
mean loss: 244.01
train mean loss: 247.17
epoch train time: 0:00:00.593129
elapsed time: 0:01:23.835067
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-20 21:29:01.616912
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 249.87
 ---- batch: 020 ----
mean loss: 245.26
 ---- batch: 030 ----
mean loss: 240.64
 ---- batch: 040 ----
mean loss: 241.41
train mean loss: 246.03
epoch train time: 0:00:00.602245
elapsed time: 0:01:24.437520
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-20 21:29:02.219410
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.09
 ---- batch: 020 ----
mean loss: 241.89
 ---- batch: 030 ----
mean loss: 246.69
 ---- batch: 040 ----
mean loss: 243.01
train mean loss: 244.73
epoch train time: 0:00:00.633655
elapsed time: 0:01:25.071464
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-20 21:29:02.853331
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 252.18
 ---- batch: 020 ----
mean loss: 245.21
 ---- batch: 030 ----
mean loss: 239.14
 ---- batch: 040 ----
mean loss: 239.72
train mean loss: 243.46
epoch train time: 0:00:00.607630
elapsed time: 0:01:25.679329
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-20 21:29:03.461178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.67
 ---- batch: 020 ----
mean loss: 250.20
 ---- batch: 030 ----
mean loss: 242.12
 ---- batch: 040 ----
mean loss: 238.01
train mean loss: 241.88
epoch train time: 0:00:00.629675
elapsed time: 0:01:26.309225
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-20 21:29:04.091072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.53
 ---- batch: 020 ----
mean loss: 236.01
 ---- batch: 030 ----
mean loss: 242.07
 ---- batch: 040 ----
mean loss: 239.20
train mean loss: 241.29
epoch train time: 0:00:00.591851
elapsed time: 0:01:26.901280
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-20 21:29:04.683130
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 245.32
 ---- batch: 020 ----
mean loss: 240.88
 ---- batch: 030 ----
mean loss: 235.43
 ---- batch: 040 ----
mean loss: 240.74
train mean loss: 240.25
epoch train time: 0:00:00.610423
elapsed time: 0:01:27.511918
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-20 21:29:05.293783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.34
 ---- batch: 020 ----
mean loss: 240.18
 ---- batch: 030 ----
mean loss: 240.57
 ---- batch: 040 ----
mean loss: 235.42
train mean loss: 238.53
epoch train time: 0:00:00.610257
elapsed time: 0:01:28.122401
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-20 21:29:05.904247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.60
 ---- batch: 020 ----
mean loss: 244.14
 ---- batch: 030 ----
mean loss: 232.68
 ---- batch: 040 ----
mean loss: 237.72
train mean loss: 237.61
epoch train time: 0:00:00.626595
elapsed time: 0:01:28.749222
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-20 21:29:06.531071
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.88
 ---- batch: 020 ----
mean loss: 233.74
 ---- batch: 030 ----
mean loss: 238.85
 ---- batch: 040 ----
mean loss: 243.50
train mean loss: 236.58
epoch train time: 0:00:00.638911
elapsed time: 0:01:29.388385
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-20 21:29:07.170233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.50
 ---- batch: 020 ----
mean loss: 229.31
 ---- batch: 030 ----
mean loss: 237.22
 ---- batch: 040 ----
mean loss: 237.72
train mean loss: 235.84
epoch train time: 0:00:00.627275
elapsed time: 0:01:30.015921
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-20 21:29:07.797767
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.71
 ---- batch: 020 ----
mean loss: 233.99
 ---- batch: 030 ----
mean loss: 234.75
 ---- batch: 040 ----
mean loss: 230.84
train mean loss: 234.62
epoch train time: 0:00:00.609448
elapsed time: 0:01:30.625573
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-20 21:29:08.407420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.56
 ---- batch: 020 ----
mean loss: 234.89
 ---- batch: 030 ----
mean loss: 238.88
 ---- batch: 040 ----
mean loss: 241.06
train mean loss: 233.91
epoch train time: 0:00:00.627290
elapsed time: 0:01:31.253049
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-20 21:29:09.034896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.34
 ---- batch: 020 ----
mean loss: 234.77
 ---- batch: 030 ----
mean loss: 237.94
 ---- batch: 040 ----
mean loss: 223.02
train mean loss: 232.39
epoch train time: 0:00:00.603600
elapsed time: 0:01:31.856871
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-20 21:29:09.638688
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 231.58
 ---- batch: 020 ----
mean loss: 226.80
 ---- batch: 030 ----
mean loss: 232.83
 ---- batch: 040 ----
mean loss: 231.30
train mean loss: 231.55
epoch train time: 0:00:00.627624
elapsed time: 0:01:32.484678
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-20 21:29:10.266546
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 235.27
 ---- batch: 020 ----
mean loss: 228.43
 ---- batch: 030 ----
mean loss: 221.01
 ---- batch: 040 ----
mean loss: 235.92
train mean loss: 230.26
epoch train time: 0:00:00.629097
elapsed time: 0:01:33.114035
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-20 21:29:10.895937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.32
 ---- batch: 020 ----
mean loss: 228.87
 ---- batch: 030 ----
mean loss: 233.71
 ---- batch: 040 ----
mean loss: 233.22
train mean loss: 230.24
epoch train time: 0:00:00.633360
elapsed time: 0:01:33.747654
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-20 21:29:11.529501
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 229.05
 ---- batch: 020 ----
mean loss: 227.20
 ---- batch: 030 ----
mean loss: 229.93
 ---- batch: 040 ----
mean loss: 226.09
train mean loss: 228.56
epoch train time: 0:00:00.611516
elapsed time: 0:01:34.359362
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-20 21:29:12.141207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.49
 ---- batch: 020 ----
mean loss: 226.68
 ---- batch: 030 ----
mean loss: 221.01
 ---- batch: 040 ----
mean loss: 230.08
train mean loss: 226.93
epoch train time: 0:00:00.590666
elapsed time: 0:01:34.950227
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-20 21:29:12.732099
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.11
 ---- batch: 020 ----
mean loss: 224.99
 ---- batch: 030 ----
mean loss: 232.74
 ---- batch: 040 ----
mean loss: 228.29
train mean loss: 227.17
epoch train time: 0:00:00.616154
elapsed time: 0:01:35.566669
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-20 21:29:13.348554
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 230.49
 ---- batch: 020 ----
mean loss: 223.66
 ---- batch: 030 ----
mean loss: 231.40
 ---- batch: 040 ----
mean loss: 219.01
train mean loss: 225.88
epoch train time: 0:00:00.642503
elapsed time: 0:01:36.209406
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-20 21:29:13.991254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.32
 ---- batch: 020 ----
mean loss: 230.77
 ---- batch: 030 ----
mean loss: 219.80
 ---- batch: 040 ----
mean loss: 227.29
train mean loss: 225.08
epoch train time: 0:00:00.616168
elapsed time: 0:01:36.825830
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-20 21:29:14.607711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 222.90
 ---- batch: 020 ----
mean loss: 228.95
 ---- batch: 030 ----
mean loss: 219.59
 ---- batch: 040 ----
mean loss: 227.31
train mean loss: 224.30
epoch train time: 0:00:00.619615
elapsed time: 0:01:37.445659
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-20 21:29:15.227516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.49
 ---- batch: 020 ----
mean loss: 221.51
 ---- batch: 030 ----
mean loss: 221.25
 ---- batch: 040 ----
mean loss: 224.66
train mean loss: 223.06
epoch train time: 0:00:00.595913
elapsed time: 0:01:38.041787
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-20 21:29:15.823644
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.34
 ---- batch: 020 ----
mean loss: 221.52
 ---- batch: 030 ----
mean loss: 222.26
 ---- batch: 040 ----
mean loss: 222.36
train mean loss: 221.91
epoch train time: 0:00:00.598929
elapsed time: 0:01:38.640913
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-20 21:29:16.422759
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.47
 ---- batch: 020 ----
mean loss: 222.76
 ---- batch: 030 ----
mean loss: 226.35
 ---- batch: 040 ----
mean loss: 215.34
train mean loss: 221.03
epoch train time: 0:00:00.593322
elapsed time: 0:01:39.234424
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-20 21:29:17.016272
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.14
 ---- batch: 020 ----
mean loss: 215.24
 ---- batch: 030 ----
mean loss: 222.26
 ---- batch: 040 ----
mean loss: 215.48
train mean loss: 220.25
epoch train time: 0:00:00.629275
elapsed time: 0:01:39.863987
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-20 21:29:17.645861
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.50
 ---- batch: 020 ----
mean loss: 217.35
 ---- batch: 030 ----
mean loss: 215.90
 ---- batch: 040 ----
mean loss: 227.00
train mean loss: 219.37
epoch train time: 0:00:00.631208
elapsed time: 0:01:40.495449
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-20 21:29:18.277341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.64
 ---- batch: 020 ----
mean loss: 217.37
 ---- batch: 030 ----
mean loss: 218.89
 ---- batch: 040 ----
mean loss: 223.74
train mean loss: 218.43
epoch train time: 0:00:00.619567
elapsed time: 0:01:41.115246
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-20 21:29:18.897090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.73
 ---- batch: 020 ----
mean loss: 220.24
 ---- batch: 030 ----
mean loss: 215.24
 ---- batch: 040 ----
mean loss: 214.37
train mean loss: 218.08
epoch train time: 0:00:00.589256
elapsed time: 0:01:41.704679
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-20 21:29:19.486521
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.26
 ---- batch: 020 ----
mean loss: 214.65
 ---- batch: 030 ----
mean loss: 219.84
 ---- batch: 040 ----
mean loss: 217.19
train mean loss: 217.14
epoch train time: 0:00:00.589309
elapsed time: 0:01:42.294167
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-20 21:29:20.076023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.46
 ---- batch: 020 ----
mean loss: 218.00
 ---- batch: 030 ----
mean loss: 215.35
 ---- batch: 040 ----
mean loss: 212.23
train mean loss: 215.73
epoch train time: 0:00:00.577726
elapsed time: 0:01:42.872099
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-20 21:29:20.653978
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.12
 ---- batch: 020 ----
mean loss: 218.01
 ---- batch: 030 ----
mean loss: 212.85
 ---- batch: 040 ----
mean loss: 211.65
train mean loss: 215.37
epoch train time: 0:00:00.619289
elapsed time: 0:01:43.491624
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-20 21:29:21.273491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.97
 ---- batch: 020 ----
mean loss: 218.60
 ---- batch: 030 ----
mean loss: 211.19
 ---- batch: 040 ----
mean loss: 217.06
train mean loss: 214.22
epoch train time: 0:00:00.627804
elapsed time: 0:01:44.119715
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-20 21:29:21.901569
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.63
 ---- batch: 020 ----
mean loss: 211.08
 ---- batch: 030 ----
mean loss: 215.72
 ---- batch: 040 ----
mean loss: 213.61
train mean loss: 214.42
epoch train time: 0:00:00.616522
elapsed time: 0:01:44.736456
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-20 21:29:22.518485
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.48
 ---- batch: 020 ----
mean loss: 206.66
 ---- batch: 030 ----
mean loss: 215.76
 ---- batch: 040 ----
mean loss: 211.73
train mean loss: 213.28
epoch train time: 0:00:00.612662
elapsed time: 0:01:45.349513
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-20 21:29:23.131343
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.46
 ---- batch: 020 ----
mean loss: 210.38
 ---- batch: 030 ----
mean loss: 211.56
 ---- batch: 040 ----
mean loss: 211.32
train mean loss: 211.94
epoch train time: 0:00:00.599267
elapsed time: 0:01:45.948962
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-20 21:29:23.730827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.26
 ---- batch: 020 ----
mean loss: 208.20
 ---- batch: 030 ----
mean loss: 217.84
 ---- batch: 040 ----
mean loss: 208.10
train mean loss: 210.81
epoch train time: 0:00:00.589747
elapsed time: 0:01:46.538946
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-20 21:29:24.320799
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.36
 ---- batch: 020 ----
mean loss: 206.00
 ---- batch: 030 ----
mean loss: 216.24
 ---- batch: 040 ----
mean loss: 209.97
train mean loss: 210.81
epoch train time: 0:00:00.616249
elapsed time: 0:01:47.155418
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-20 21:29:24.937285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.33
 ---- batch: 020 ----
mean loss: 207.25
 ---- batch: 030 ----
mean loss: 203.37
 ---- batch: 040 ----
mean loss: 215.42
train mean loss: 209.92
epoch train time: 0:00:00.632839
elapsed time: 0:01:47.788487
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-20 21:29:25.570339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.03
 ---- batch: 020 ----
mean loss: 207.15
 ---- batch: 030 ----
mean loss: 204.90
 ---- batch: 040 ----
mean loss: 210.63
train mean loss: 209.10
epoch train time: 0:00:00.631089
elapsed time: 0:01:48.419822
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-20 21:29:26.201683
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.67
 ---- batch: 020 ----
mean loss: 213.76
 ---- batch: 030 ----
mean loss: 206.24
 ---- batch: 040 ----
mean loss: 208.73
train mean loss: 208.10
epoch train time: 0:00:00.594052
elapsed time: 0:01:49.014071
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-20 21:29:26.795915
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.46
 ---- batch: 020 ----
mean loss: 205.71
 ---- batch: 030 ----
mean loss: 212.68
 ---- batch: 040 ----
mean loss: 207.33
train mean loss: 207.84
epoch train time: 0:00:00.592755
elapsed time: 0:01:49.607021
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-20 21:29:27.388865
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.36
 ---- batch: 020 ----
mean loss: 213.90
 ---- batch: 030 ----
mean loss: 205.06
 ---- batch: 040 ----
mean loss: 209.09
train mean loss: 206.63
epoch train time: 0:00:00.597828
elapsed time: 0:01:50.205052
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-20 21:29:27.986897
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.63
 ---- batch: 020 ----
mean loss: 209.48
 ---- batch: 030 ----
mean loss: 210.14
 ---- batch: 040 ----
mean loss: 195.36
train mean loss: 206.06
epoch train time: 0:00:00.618997
elapsed time: 0:01:50.824279
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-20 21:29:28.606131
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.78
 ---- batch: 020 ----
mean loss: 205.61
 ---- batch: 030 ----
mean loss: 208.15
 ---- batch: 040 ----
mean loss: 200.85
train mean loss: 205.41
epoch train time: 0:00:00.629305
elapsed time: 0:01:51.453843
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-20 21:29:29.235745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.72
 ---- batch: 020 ----
mean loss: 197.02
 ---- batch: 030 ----
mean loss: 201.13
 ---- batch: 040 ----
mean loss: 216.52
train mean loss: 204.96
epoch train time: 0:00:00.645753
elapsed time: 0:01:52.099890
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-20 21:29:29.881748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.91
 ---- batch: 020 ----
mean loss: 204.77
 ---- batch: 030 ----
mean loss: 199.78
 ---- batch: 040 ----
mean loss: 205.41
train mean loss: 204.05
epoch train time: 0:00:00.598507
elapsed time: 0:01:52.698625
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-20 21:29:30.480474
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.31
 ---- batch: 020 ----
mean loss: 202.85
 ---- batch: 030 ----
mean loss: 198.90
 ---- batch: 040 ----
mean loss: 203.09
train mean loss: 203.51
epoch train time: 0:00:00.602902
elapsed time: 0:01:53.301821
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-20 21:29:31.083669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.37
 ---- batch: 020 ----
mean loss: 198.89
 ---- batch: 030 ----
mean loss: 204.78
 ---- batch: 040 ----
mean loss: 203.14
train mean loss: 202.76
epoch train time: 0:00:00.593339
elapsed time: 0:01:53.895354
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-20 21:29:31.677214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.17
 ---- batch: 020 ----
mean loss: 202.18
 ---- batch: 030 ----
mean loss: 206.16
 ---- batch: 040 ----
mean loss: 200.55
train mean loss: 202.02
epoch train time: 0:00:00.622970
elapsed time: 0:01:54.518549
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-20 21:29:32.300427
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.12
 ---- batch: 020 ----
mean loss: 204.17
 ---- batch: 030 ----
mean loss: 204.80
 ---- batch: 040 ----
mean loss: 200.05
train mean loss: 201.08
epoch train time: 0:00:00.650606
elapsed time: 0:01:55.169423
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-20 21:29:32.951273
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.30
 ---- batch: 020 ----
mean loss: 197.10
 ---- batch: 030 ----
mean loss: 201.21
 ---- batch: 040 ----
mean loss: 204.55
train mean loss: 201.09
epoch train time: 0:00:00.627697
elapsed time: 0:01:55.797330
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-20 21:29:33.579207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.87
 ---- batch: 020 ----
mean loss: 200.79
 ---- batch: 030 ----
mean loss: 205.37
 ---- batch: 040 ----
mean loss: 199.99
train mean loss: 199.68
epoch train time: 0:00:00.597553
elapsed time: 0:01:56.395097
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-20 21:29:34.176942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.02
 ---- batch: 020 ----
mean loss: 197.58
 ---- batch: 030 ----
mean loss: 203.01
 ---- batch: 040 ----
mean loss: 198.56
train mean loss: 198.68
epoch train time: 0:00:00.578875
elapsed time: 0:01:56.974164
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-20 21:29:34.756020
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.79
 ---- batch: 020 ----
mean loss: 197.64
 ---- batch: 030 ----
mean loss: 199.62
 ---- batch: 040 ----
mean loss: 201.30
train mean loss: 199.24
epoch train time: 0:00:00.603291
elapsed time: 0:01:57.577645
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-20 21:29:35.359487
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.16
 ---- batch: 020 ----
mean loss: 196.17
 ---- batch: 030 ----
mean loss: 206.09
 ---- batch: 040 ----
mean loss: 193.05
train mean loss: 198.21
epoch train time: 0:00:00.615199
elapsed time: 0:01:58.193046
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-20 21:29:35.974916
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.69
 ---- batch: 020 ----
mean loss: 190.31
 ---- batch: 030 ----
mean loss: 197.53
 ---- batch: 040 ----
mean loss: 196.60
train mean loss: 197.85
epoch train time: 0:00:00.634452
elapsed time: 0:01:58.827744
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-20 21:29:36.609599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.99
 ---- batch: 020 ----
mean loss: 192.87
 ---- batch: 030 ----
mean loss: 203.75
 ---- batch: 040 ----
mean loss: 197.22
train mean loss: 197.18
epoch train time: 0:00:00.634672
elapsed time: 0:01:59.462734
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-20 21:29:37.244607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.99
 ---- batch: 020 ----
mean loss: 195.06
 ---- batch: 030 ----
mean loss: 191.27
 ---- batch: 040 ----
mean loss: 203.48
train mean loss: 196.15
epoch train time: 0:00:00.605803
elapsed time: 0:02:00.068751
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-20 21:29:37.850631
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.14
 ---- batch: 020 ----
mean loss: 200.73
 ---- batch: 030 ----
mean loss: 192.28
 ---- batch: 040 ----
mean loss: 194.86
train mean loss: 195.74
epoch train time: 0:00:00.606243
elapsed time: 0:02:00.675246
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-20 21:29:38.457075
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.80
 ---- batch: 020 ----
mean loss: 194.23
 ---- batch: 030 ----
mean loss: 197.76
 ---- batch: 040 ----
mean loss: 184.95
train mean loss: 195.11
epoch train time: 0:00:00.604956
elapsed time: 0:02:01.280401
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-20 21:29:39.062267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.97
 ---- batch: 020 ----
mean loss: 188.37
 ---- batch: 030 ----
mean loss: 193.91
 ---- batch: 040 ----
mean loss: 201.74
train mean loss: 194.21
epoch train time: 0:00:00.612498
elapsed time: 0:02:01.893124
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-20 21:29:39.674977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.68
 ---- batch: 020 ----
mean loss: 196.91
 ---- batch: 030 ----
mean loss: 191.62
 ---- batch: 040 ----
mean loss: 190.18
train mean loss: 193.62
epoch train time: 0:00:00.628983
elapsed time: 0:02:02.522346
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-20 21:29:40.304203
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.97
 ---- batch: 020 ----
mean loss: 186.37
 ---- batch: 030 ----
mean loss: 190.11
 ---- batch: 040 ----
mean loss: 198.08
train mean loss: 193.57
epoch train time: 0:00:00.639282
elapsed time: 0:02:03.161867
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-20 21:29:40.943719
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.92
 ---- batch: 020 ----
mean loss: 193.51
 ---- batch: 030 ----
mean loss: 190.44
 ---- batch: 040 ----
mean loss: 189.68
train mean loss: 192.72
epoch train time: 0:00:00.594772
elapsed time: 0:02:03.756851
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-20 21:29:41.538751
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.11
 ---- batch: 020 ----
mean loss: 188.55
 ---- batch: 030 ----
mean loss: 190.63
 ---- batch: 040 ----
mean loss: 198.24
train mean loss: 192.13
epoch train time: 0:00:00.604643
elapsed time: 0:02:04.361738
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-20 21:29:42.143584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.33
 ---- batch: 020 ----
mean loss: 193.29
 ---- batch: 030 ----
mean loss: 190.99
 ---- batch: 040 ----
mean loss: 193.14
train mean loss: 191.55
epoch train time: 0:00:00.583004
elapsed time: 0:02:04.944923
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-20 21:29:42.726766
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.60
 ---- batch: 020 ----
mean loss: 190.13
 ---- batch: 030 ----
mean loss: 189.67
 ---- batch: 040 ----
mean loss: 195.08
train mean loss: 190.84
epoch train time: 0:00:00.614843
elapsed time: 0:02:05.560015
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-20 21:29:43.341896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.06
 ---- batch: 020 ----
mean loss: 192.03
 ---- batch: 030 ----
mean loss: 188.34
 ---- batch: 040 ----
mean loss: 190.77
train mean loss: 190.14
epoch train time: 0:00:00.613149
elapsed time: 0:02:06.173402
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-20 21:29:43.955268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.31
 ---- batch: 020 ----
mean loss: 188.96
 ---- batch: 030 ----
mean loss: 186.62
 ---- batch: 040 ----
mean loss: 192.90
train mean loss: 189.95
epoch train time: 0:00:00.605877
elapsed time: 0:02:06.779508
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-20 21:29:44.561361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.16
 ---- batch: 020 ----
mean loss: 185.57
 ---- batch: 030 ----
mean loss: 189.24
 ---- batch: 040 ----
mean loss: 195.10
train mean loss: 189.50
epoch train time: 0:00:00.593798
elapsed time: 0:02:07.373490
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-20 21:29:45.155330
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.30
 ---- batch: 020 ----
mean loss: 182.84
 ---- batch: 030 ----
mean loss: 189.49
 ---- batch: 040 ----
mean loss: 192.92
train mean loss: 189.06
epoch train time: 0:00:00.586088
elapsed time: 0:02:07.959814
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-20 21:29:45.741664
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.94
 ---- batch: 020 ----
mean loss: 189.61
 ---- batch: 030 ----
mean loss: 189.00
 ---- batch: 040 ----
mean loss: 186.66
train mean loss: 188.35
epoch train time: 0:00:00.590867
elapsed time: 0:02:08.550866
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-20 21:29:46.332749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.01
 ---- batch: 020 ----
mean loss: 184.26
 ---- batch: 030 ----
mean loss: 189.68
 ---- batch: 040 ----
mean loss: 192.12
train mean loss: 187.93
epoch train time: 0:00:00.612048
elapsed time: 0:02:09.163170
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-20 21:29:46.945019
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.30
 ---- batch: 020 ----
mean loss: 186.27
 ---- batch: 030 ----
mean loss: 185.14
 ---- batch: 040 ----
mean loss: 188.47
train mean loss: 187.20
epoch train time: 0:00:00.625187
elapsed time: 0:02:09.788572
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-20 21:29:47.570422
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.96
 ---- batch: 020 ----
mean loss: 197.30
 ---- batch: 030 ----
mean loss: 182.87
 ---- batch: 040 ----
mean loss: 187.35
train mean loss: 187.15
epoch train time: 0:00:00.623058
elapsed time: 0:02:10.411889
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-20 21:29:48.193754
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 183.83
 ---- batch: 020 ----
mean loss: 185.90
 ---- batch: 030 ----
mean loss: 192.33
 ---- batch: 040 ----
mean loss: 181.83
train mean loss: 186.02
epoch train time: 0:00:00.617530
elapsed time: 0:02:11.029669
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-20 21:29:48.811519
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 179.73
 ---- batch: 020 ----
mean loss: 191.67
 ---- batch: 030 ----
mean loss: 188.84
 ---- batch: 040 ----
mean loss: 182.40
train mean loss: 185.48
epoch train time: 0:00:00.578414
elapsed time: 0:02:11.608265
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-20 21:29:49.390104
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.03
 ---- batch: 020 ----
mean loss: 182.26
 ---- batch: 030 ----
mean loss: 182.72
 ---- batch: 040 ----
mean loss: 192.46
train mean loss: 185.16
epoch train time: 0:00:00.585909
elapsed time: 0:02:12.194367
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-20 21:29:49.976211
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.28
 ---- batch: 020 ----
mean loss: 188.26
 ---- batch: 030 ----
mean loss: 183.49
 ---- batch: 040 ----
mean loss: 188.50
train mean loss: 184.60
epoch train time: 0:00:00.593278
elapsed time: 0:02:12.787856
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-20 21:29:50.569709
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.41
 ---- batch: 020 ----
mean loss: 193.85
 ---- batch: 030 ----
mean loss: 182.38
 ---- batch: 040 ----
mean loss: 180.54
train mean loss: 183.97
epoch train time: 0:00:00.615443
elapsed time: 0:02:13.403507
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-20 21:29:51.185356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.28
 ---- batch: 020 ----
mean loss: 188.84
 ---- batch: 030 ----
mean loss: 183.65
 ---- batch: 040 ----
mean loss: 181.55
train mean loss: 183.58
epoch train time: 0:00:00.609717
elapsed time: 0:02:14.013422
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-20 21:29:51.795283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.35
 ---- batch: 020 ----
mean loss: 182.65
 ---- batch: 030 ----
mean loss: 178.75
 ---- batch: 040 ----
mean loss: 183.19
train mean loss: 182.98
epoch train time: 0:00:00.606342
elapsed time: 0:02:14.619979
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-20 21:29:52.401833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.60
 ---- batch: 020 ----
mean loss: 181.72
 ---- batch: 030 ----
mean loss: 184.23
 ---- batch: 040 ----
mean loss: 178.95
train mean loss: 182.63
epoch train time: 0:00:00.585160
elapsed time: 0:02:15.205325
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-20 21:29:52.987165
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.45
 ---- batch: 020 ----
mean loss: 180.32
 ---- batch: 030 ----
mean loss: 184.55
 ---- batch: 040 ----
mean loss: 176.07
train mean loss: 182.04
epoch train time: 0:00:00.587291
elapsed time: 0:02:15.792805
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-20 21:29:53.574665
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 174.06
 ---- batch: 020 ----
mean loss: 190.80
 ---- batch: 030 ----
mean loss: 177.65
 ---- batch: 040 ----
mean loss: 187.53
train mean loss: 181.54
epoch train time: 0:00:00.599526
elapsed time: 0:02:16.392541
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-20 21:29:54.174398
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.77
 ---- batch: 020 ----
mean loss: 181.65
 ---- batch: 030 ----
mean loss: 177.10
 ---- batch: 040 ----
mean loss: 178.98
train mean loss: 181.37
epoch train time: 0:00:00.614900
elapsed time: 0:02:17.007691
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-20 21:29:54.789546
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.48
 ---- batch: 020 ----
mean loss: 184.13
 ---- batch: 030 ----
mean loss: 183.13
 ---- batch: 040 ----
mean loss: 179.06
train mean loss: 180.95
epoch train time: 0:00:00.634724
elapsed time: 0:02:17.642713
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-20 21:29:55.424548
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.70
 ---- batch: 020 ----
mean loss: 186.11
 ---- batch: 030 ----
mean loss: 179.44
 ---- batch: 040 ----
mean loss: 177.85
train mean loss: 180.65
epoch train time: 0:00:00.627075
elapsed time: 0:02:18.269978
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-20 21:29:56.051832
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.95
 ---- batch: 020 ----
mean loss: 184.72
 ---- batch: 030 ----
mean loss: 183.11
 ---- batch: 040 ----
mean loss: 176.17
train mean loss: 180.65
epoch train time: 0:00:00.598532
elapsed time: 0:02:18.868700
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-20 21:29:56.650553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.60
 ---- batch: 020 ----
mean loss: 179.17
 ---- batch: 030 ----
mean loss: 180.96
 ---- batch: 040 ----
mean loss: 183.93
train mean loss: 180.43
epoch train time: 0:00:00.581694
elapsed time: 0:02:19.450657
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-20 21:29:57.232553
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.97
 ---- batch: 020 ----
mean loss: 185.28
 ---- batch: 030 ----
mean loss: 180.90
 ---- batch: 040 ----
mean loss: 174.50
train mean loss: 180.54
epoch train time: 0:00:00.608074
elapsed time: 0:02:20.058999
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-20 21:29:57.840859
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.12
 ---- batch: 020 ----
mean loss: 179.93
 ---- batch: 030 ----
mean loss: 184.64
 ---- batch: 040 ----
mean loss: 179.64
train mean loss: 180.41
epoch train time: 0:00:00.609024
elapsed time: 0:02:20.668245
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-20 21:29:58.450093
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.49
 ---- batch: 020 ----
mean loss: 183.75
 ---- batch: 030 ----
mean loss: 183.02
 ---- batch: 040 ----
mean loss: 175.34
train mean loss: 180.88
epoch train time: 0:00:00.636183
elapsed time: 0:02:21.304640
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-20 21:29:59.086492
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.93
 ---- batch: 020 ----
mean loss: 186.61
 ---- batch: 030 ----
mean loss: 175.66
 ---- batch: 040 ----
mean loss: 179.63
train mean loss: 180.66
epoch train time: 0:00:00.622929
elapsed time: 0:02:21.927794
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-20 21:29:59.709641
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.34
 ---- batch: 020 ----
mean loss: 183.88
 ---- batch: 030 ----
mean loss: 182.04
 ---- batch: 040 ----
mean loss: 182.64
train mean loss: 180.67
epoch train time: 0:00:00.602565
elapsed time: 0:02:22.530594
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-20 21:30:00.312508
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.51
 ---- batch: 020 ----
mean loss: 180.51
 ---- batch: 030 ----
mean loss: 181.98
 ---- batch: 040 ----
mean loss: 181.31
train mean loss: 180.28
epoch train time: 0:00:00.616867
elapsed time: 0:02:23.147797
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-20 21:30:00.929750
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.23
 ---- batch: 020 ----
mean loss: 181.52
 ---- batch: 030 ----
mean loss: 178.11
 ---- batch: 040 ----
mean loss: 182.50
train mean loss: 180.30
epoch train time: 0:00:00.623901
elapsed time: 0:02:23.772012
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-20 21:30:01.553878
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.44
 ---- batch: 020 ----
mean loss: 183.20
 ---- batch: 030 ----
mean loss: 177.91
 ---- batch: 040 ----
mean loss: 176.96
train mean loss: 180.59
epoch train time: 0:00:00.632765
elapsed time: 0:02:24.405065
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-20 21:30:02.186937
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.69
 ---- batch: 020 ----
mean loss: 178.50
 ---- batch: 030 ----
mean loss: 179.28
 ---- batch: 040 ----
mean loss: 180.26
train mean loss: 180.07
epoch train time: 0:00:00.614098
elapsed time: 0:02:25.019435
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-20 21:30:02.801294
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.09
 ---- batch: 020 ----
mean loss: 183.03
 ---- batch: 030 ----
mean loss: 178.50
 ---- batch: 040 ----
mean loss: 182.08
train mean loss: 180.13
epoch train time: 0:00:00.625162
elapsed time: 0:02:25.644819
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-20 21:30:03.426666
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.71
 ---- batch: 020 ----
mean loss: 180.15
 ---- batch: 030 ----
mean loss: 176.62
 ---- batch: 040 ----
mean loss: 179.61
train mean loss: 179.95
epoch train time: 0:00:00.616627
elapsed time: 0:02:26.261629
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-20 21:30:04.043490
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.83
 ---- batch: 020 ----
mean loss: 178.57
 ---- batch: 030 ----
mean loss: 179.65
 ---- batch: 040 ----
mean loss: 177.10
train mean loss: 180.11
epoch train time: 0:00:00.595315
elapsed time: 0:02:26.857152
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-20 21:30:04.639030
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.56
 ---- batch: 020 ----
mean loss: 178.08
 ---- batch: 030 ----
mean loss: 177.31
 ---- batch: 040 ----
mean loss: 181.18
train mean loss: 180.09
epoch train time: 0:00:00.594703
elapsed time: 0:02:27.452092
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-20 21:30:05.233937
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.99
 ---- batch: 020 ----
mean loss: 185.41
 ---- batch: 030 ----
mean loss: 177.48
 ---- batch: 040 ----
mean loss: 173.83
train mean loss: 179.92
epoch train time: 0:00:00.612869
elapsed time: 0:02:28.065173
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-20 21:30:05.847020
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.44
 ---- batch: 020 ----
mean loss: 176.56
 ---- batch: 030 ----
mean loss: 178.66
 ---- batch: 040 ----
mean loss: 178.62
train mean loss: 180.14
epoch train time: 0:00:00.609418
elapsed time: 0:02:28.674806
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-20 21:30:06.456671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.22
 ---- batch: 020 ----
mean loss: 183.46
 ---- batch: 030 ----
mean loss: 183.88
 ---- batch: 040 ----
mean loss: 177.36
train mean loss: 180.03
epoch train time: 0:00:00.629432
elapsed time: 0:02:29.304455
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-20 21:30:07.086307
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.10
 ---- batch: 020 ----
mean loss: 187.08
 ---- batch: 030 ----
mean loss: 169.45
 ---- batch: 040 ----
mean loss: 176.76
train mean loss: 180.13
epoch train time: 0:00:00.592157
elapsed time: 0:02:29.896819
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-20 21:30:07.678700
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 174.01
 ---- batch: 020 ----
mean loss: 181.58
 ---- batch: 030 ----
mean loss: 185.06
 ---- batch: 040 ----
mean loss: 179.48
train mean loss: 180.03
epoch train time: 0:00:00.606462
elapsed time: 0:02:30.503530
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-20 21:30:08.285429
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.27
 ---- batch: 020 ----
mean loss: 182.53
 ---- batch: 030 ----
mean loss: 176.40
 ---- batch: 040 ----
mean loss: 177.28
train mean loss: 179.89
epoch train time: 0:00:00.601744
elapsed time: 0:02:31.105575
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-20 21:30:08.887440
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.64
 ---- batch: 020 ----
mean loss: 179.64
 ---- batch: 030 ----
mean loss: 175.19
 ---- batch: 040 ----
mean loss: 178.03
train mean loss: 179.92
epoch train time: 0:00:00.611013
elapsed time: 0:02:31.716813
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-20 21:30:09.498660
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.64
 ---- batch: 020 ----
mean loss: 177.79
 ---- batch: 030 ----
mean loss: 178.67
 ---- batch: 040 ----
mean loss: 180.02
train mean loss: 179.69
epoch train time: 0:00:00.639266
elapsed time: 0:02:32.356283
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-20 21:30:10.138132
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.81
 ---- batch: 020 ----
mean loss: 176.01
 ---- batch: 030 ----
mean loss: 179.05
 ---- batch: 040 ----
mean loss: 178.33
train mean loss: 179.64
epoch train time: 0:00:00.624073
elapsed time: 0:02:32.980582
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-20 21:30:10.762438
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.04
 ---- batch: 020 ----
mean loss: 182.52
 ---- batch: 030 ----
mean loss: 179.19
 ---- batch: 040 ----
mean loss: 175.95
train mean loss: 179.66
epoch train time: 0:00:00.587677
elapsed time: 0:02:33.568452
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-20 21:30:11.350305
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.18
 ---- batch: 020 ----
mean loss: 175.50
 ---- batch: 030 ----
mean loss: 181.90
 ---- batch: 040 ----
mean loss: 180.40
train mean loss: 179.28
epoch train time: 0:00:00.580338
elapsed time: 0:02:34.149003
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-20 21:30:11.930857
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.39
 ---- batch: 020 ----
mean loss: 182.93
 ---- batch: 030 ----
mean loss: 172.47
 ---- batch: 040 ----
mean loss: 183.73
train mean loss: 179.30
epoch train time: 0:00:00.578607
elapsed time: 0:02:34.727806
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-20 21:30:12.509650
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.29
 ---- batch: 020 ----
mean loss: 178.91
 ---- batch: 030 ----
mean loss: 182.66
 ---- batch: 040 ----
mean loss: 184.05
train mean loss: 179.21
epoch train time: 0:00:00.623501
elapsed time: 0:02:35.351518
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-20 21:30:13.133369
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.11
 ---- batch: 020 ----
mean loss: 173.08
 ---- batch: 030 ----
mean loss: 185.35
 ---- batch: 040 ----
mean loss: 178.50
train mean loss: 179.56
epoch train time: 0:00:00.633117
elapsed time: 0:02:35.984903
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-20 21:30:13.766746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.25
 ---- batch: 020 ----
mean loss: 180.27
 ---- batch: 030 ----
mean loss: 185.29
 ---- batch: 040 ----
mean loss: 175.75
train mean loss: 179.58
epoch train time: 0:00:00.643765
elapsed time: 0:02:36.628898
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-20 21:30:14.410755
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.27
 ---- batch: 020 ----
mean loss: 184.14
 ---- batch: 030 ----
mean loss: 182.53
 ---- batch: 040 ----
mean loss: 176.64
train mean loss: 179.44
epoch train time: 0:00:00.607009
elapsed time: 0:02:37.236170
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-20 21:30:15.018003
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.29
 ---- batch: 020 ----
mean loss: 182.74
 ---- batch: 030 ----
mean loss: 181.08
 ---- batch: 040 ----
mean loss: 175.59
train mean loss: 179.16
epoch train time: 0:00:00.601848
elapsed time: 0:02:37.838237
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-20 21:30:15.620118
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 175.95
 ---- batch: 020 ----
mean loss: 178.47
 ---- batch: 030 ----
mean loss: 176.20
 ---- batch: 040 ----
mean loss: 183.36
train mean loss: 178.92
epoch train time: 0:00:00.594834
elapsed time: 0:02:38.433287
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-20 21:30:16.215129
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.02
 ---- batch: 020 ----
mean loss: 172.26
 ---- batch: 030 ----
mean loss: 177.24
 ---- batch: 040 ----
mean loss: 176.37
train mean loss: 179.18
epoch train time: 0:00:00.604935
elapsed time: 0:02:39.038431
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-20 21:30:16.820282
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.44
 ---- batch: 020 ----
mean loss: 175.52
 ---- batch: 030 ----
mean loss: 178.04
 ---- batch: 040 ----
mean loss: 179.80
train mean loss: 178.92
epoch train time: 0:00:00.629257
elapsed time: 0:02:39.667925
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-20 21:30:17.449789
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.46
 ---- batch: 020 ----
mean loss: 172.70
 ---- batch: 030 ----
mean loss: 179.87
 ---- batch: 040 ----
mean loss: 179.12
train mean loss: 178.68
epoch train time: 0:00:00.644588
elapsed time: 0:02:40.312782
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-20 21:30:18.094637
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 179.57
 ---- batch: 020 ----
mean loss: 170.80
 ---- batch: 030 ----
mean loss: 180.20
 ---- batch: 040 ----
mean loss: 183.27
train mean loss: 178.55
epoch train time: 0:00:00.601995
elapsed time: 0:02:40.915057
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-20 21:30:18.696934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.16
 ---- batch: 020 ----
mean loss: 184.61
 ---- batch: 030 ----
mean loss: 172.96
 ---- batch: 040 ----
mean loss: 182.45
train mean loss: 178.58
epoch train time: 0:00:00.593283
elapsed time: 0:02:41.508568
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-20 21:30:19.290438
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 171.13
 ---- batch: 020 ----
mean loss: 182.84
 ---- batch: 030 ----
mean loss: 181.62
 ---- batch: 040 ----
mean loss: 178.80
train mean loss: 178.80
epoch train time: 0:00:00.585726
elapsed time: 0:02:42.094516
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-20 21:30:19.876355
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 178.03
 ---- batch: 020 ----
mean loss: 181.62
 ---- batch: 030 ----
mean loss: 181.13
 ---- batch: 040 ----
mean loss: 176.74
train mean loss: 178.59
epoch train time: 0:00:00.594557
elapsed time: 0:02:42.689277
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-20 21:30:20.471137
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 177.32
 ---- batch: 020 ----
mean loss: 178.80
 ---- batch: 030 ----
mean loss: 175.97
 ---- batch: 040 ----
mean loss: 183.49
train mean loss: 178.48
epoch train time: 0:00:00.617841
elapsed time: 0:02:43.307367
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-20 21:30:21.089216
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.46
 ---- batch: 020 ----
mean loss: 180.43
 ---- batch: 030 ----
mean loss: 181.54
 ---- batch: 040 ----
mean loss: 176.46
train mean loss: 178.34
epoch train time: 0:00:00.607600
elapsed time: 0:02:43.915234
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-20 21:30:21.697085
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 172.79
 ---- batch: 020 ----
mean loss: 182.86
 ---- batch: 030 ----
mean loss: 177.14
 ---- batch: 040 ----
mean loss: 180.47
train mean loss: 178.51
epoch train time: 0:00:00.618615
elapsed time: 0:02:44.534092
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-20 21:30:22.315956
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.57
 ---- batch: 020 ----
mean loss: 178.93
 ---- batch: 030 ----
mean loss: 185.19
 ---- batch: 040 ----
mean loss: 173.11
train mean loss: 178.80
epoch train time: 0:00:00.617668
elapsed time: 0:02:45.151961
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-20 21:30:22.933829
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.23
 ---- batch: 020 ----
mean loss: 179.67
 ---- batch: 030 ----
mean loss: 174.15
 ---- batch: 040 ----
mean loss: 175.57
train mean loss: 178.98
epoch train time: 0:00:00.596795
elapsed time: 0:02:45.748978
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-20 21:30:23.530839
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 176.92
 ---- batch: 020 ----
mean loss: 177.51
 ---- batch: 030 ----
mean loss: 187.98
 ---- batch: 040 ----
mean loss: 171.25
train mean loss: 178.16
epoch train time: 0:00:00.614251
elapsed time: 0:02:46.363472
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-20 21:30:24.145326
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.37
 ---- batch: 020 ----
mean loss: 181.16
 ---- batch: 030 ----
mean loss: 177.51
 ---- batch: 040 ----
mean loss: 173.49
train mean loss: 178.07
epoch train time: 0:00:00.619821
elapsed time: 0:02:46.987061
checkpoint saved in file: log/CMAPSS/FD003/min-max/bayesian_conv2_pool2/bayesian_conv2_pool2_8/checkpoint.pth.tar
**** end time: 2019-09-20 21:30:24.768860 ****
