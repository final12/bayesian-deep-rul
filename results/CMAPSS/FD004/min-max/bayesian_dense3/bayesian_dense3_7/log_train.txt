Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_7', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_dense3', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 15028
use_cuda: True
Dataset: CMAPSS/FD004
Building BayesianDense3...
Done.
**** start time: 2019-09-26 23:21:33.774741 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
           Flatten-1                  [-1, 360]               0
    BayesianLinear-2                  [-1, 100]          72,000
           Sigmoid-3                  [-1, 100]               0
    BayesianLinear-4                  [-1, 100]          20,000
           Sigmoid-5                  [-1, 100]               0
    BayesianLinear-6                  [-1, 100]          20,000
           Sigmoid-7                  [-1, 100]               0
    BayesianLinear-8                    [-1, 1]             200
          Softplus-9                    [-1, 1]               0
================================================================
Total params: 112,200
Trainable params: 112,200
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 23:21:33.784119
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4860.20
 ---- batch: 020 ----
mean loss: 4649.35
 ---- batch: 030 ----
mean loss: 4407.83
 ---- batch: 040 ----
mean loss: 4137.52
 ---- batch: 050 ----
mean loss: 3937.54
 ---- batch: 060 ----
mean loss: 3718.29
 ---- batch: 070 ----
mean loss: 3582.70
 ---- batch: 080 ----
mean loss: 3431.73
 ---- batch: 090 ----
mean loss: 3309.04
 ---- batch: 100 ----
mean loss: 3240.27
 ---- batch: 110 ----
mean loss: 3174.45
train mean loss: 3838.69
epoch train time: 0:00:34.577414
elapsed time: 0:00:34.593205
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 23:22:08.367991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 3041.61
 ---- batch: 020 ----
mean loss: 2945.46
 ---- batch: 030 ----
mean loss: 2907.61
 ---- batch: 040 ----
mean loss: 2852.17
 ---- batch: 050 ----
mean loss: 2827.32
 ---- batch: 060 ----
mean loss: 2762.84
 ---- batch: 070 ----
mean loss: 2696.51
 ---- batch: 080 ----
mean loss: 2694.10
 ---- batch: 090 ----
mean loss: 2638.36
 ---- batch: 100 ----
mean loss: 2572.64
 ---- batch: 110 ----
mean loss: 2503.97
train mean loss: 2762.19
epoch train time: 0:00:01.909491
elapsed time: 0:00:36.502942
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 23:22:10.278098
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2507.07
 ---- batch: 020 ----
mean loss: 2468.63
 ---- batch: 030 ----
mean loss: 2451.79
 ---- batch: 040 ----
mean loss: 2414.85
 ---- batch: 050 ----
mean loss: 2374.25
 ---- batch: 060 ----
mean loss: 2331.61
 ---- batch: 070 ----
mean loss: 2328.57
 ---- batch: 080 ----
mean loss: 2267.76
 ---- batch: 090 ----
mean loss: 2231.57
 ---- batch: 100 ----
mean loss: 2233.25
 ---- batch: 110 ----
mean loss: 2140.55
train mean loss: 2337.57
epoch train time: 0:00:01.867197
elapsed time: 0:00:38.370742
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 23:22:12.145868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2141.16
 ---- batch: 020 ----
mean loss: 2082.39
 ---- batch: 030 ----
mean loss: 2118.53
 ---- batch: 040 ----
mean loss: 2087.24
 ---- batch: 050 ----
mean loss: 2023.62
 ---- batch: 060 ----
mean loss: 2026.24
 ---- batch: 070 ----
mean loss: 2000.85
 ---- batch: 080 ----
mean loss: 2001.34
 ---- batch: 090 ----
mean loss: 1938.40
 ---- batch: 100 ----
mean loss: 1933.13
 ---- batch: 110 ----
mean loss: 1908.34
train mean loss: 2019.90
epoch train time: 0:00:01.921142
elapsed time: 0:00:40.292525
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 23:22:14.067668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1908.86
 ---- batch: 020 ----
mean loss: 1859.66
 ---- batch: 030 ----
mean loss: 1829.22
 ---- batch: 040 ----
mean loss: 1797.38
 ---- batch: 050 ----
mean loss: 1785.62
 ---- batch: 060 ----
mean loss: 1750.00
 ---- batch: 070 ----
mean loss: 1711.14
 ---- batch: 080 ----
mean loss: 1689.10
 ---- batch: 090 ----
mean loss: 1691.74
 ---- batch: 100 ----
mean loss: 1687.86
 ---- batch: 110 ----
mean loss: 1675.65
train mean loss: 1758.46
epoch train time: 0:00:01.868792
elapsed time: 0:00:42.161921
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 23:22:15.937024
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1643.78
 ---- batch: 020 ----
mean loss: 1624.04
 ---- batch: 030 ----
mean loss: 1604.84
 ---- batch: 040 ----
mean loss: 1608.37
 ---- batch: 050 ----
mean loss: 1549.62
 ---- batch: 060 ----
mean loss: 1538.18
 ---- batch: 070 ----
mean loss: 1512.86
 ---- batch: 080 ----
mean loss: 1502.76
 ---- batch: 090 ----
mean loss: 1494.78
 ---- batch: 100 ----
mean loss: 1493.68
 ---- batch: 110 ----
mean loss: 1469.40
train mean loss: 1547.39
epoch train time: 0:00:01.865393
elapsed time: 0:00:44.027885
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 23:22:17.803033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1453.32
 ---- batch: 020 ----
mean loss: 1430.74
 ---- batch: 030 ----
mean loss: 1414.83
 ---- batch: 040 ----
mean loss: 1413.58
 ---- batch: 050 ----
mean loss: 1429.95
 ---- batch: 060 ----
mean loss: 1357.11
 ---- batch: 070 ----
mean loss: 1362.35
 ---- batch: 080 ----
mean loss: 1353.70
 ---- batch: 090 ----
mean loss: 1346.87
 ---- batch: 100 ----
mean loss: 1321.84
 ---- batch: 110 ----
mean loss: 1337.14
train mean loss: 1381.88
epoch train time: 0:00:01.869900
elapsed time: 0:00:45.898444
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 23:22:19.673684
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1294.24
 ---- batch: 020 ----
mean loss: 1302.45
 ---- batch: 030 ----
mean loss: 1273.87
 ---- batch: 040 ----
mean loss: 1257.38
 ---- batch: 050 ----
mean loss: 1261.93
 ---- batch: 060 ----
mean loss: 1252.64
 ---- batch: 070 ----
mean loss: 1238.19
 ---- batch: 080 ----
mean loss: 1231.89
 ---- batch: 090 ----
mean loss: 1239.60
 ---- batch: 100 ----
mean loss: 1228.93
 ---- batch: 110 ----
mean loss: 1186.57
train mean loss: 1250.62
epoch train time: 0:00:01.900634
elapsed time: 0:00:47.799781
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 23:22:21.574875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1177.28
 ---- batch: 020 ----
mean loss: 1197.17
 ---- batch: 030 ----
mean loss: 1162.39
 ---- batch: 040 ----
mean loss: 1149.86
 ---- batch: 050 ----
mean loss: 1157.14
 ---- batch: 060 ----
mean loss: 1150.00
 ---- batch: 070 ----
mean loss: 1149.96
 ---- batch: 080 ----
mean loss: 1134.54
 ---- batch: 090 ----
mean loss: 1120.91
 ---- batch: 100 ----
mean loss: 1128.64
 ---- batch: 110 ----
mean loss: 1120.97
train mean loss: 1147.87
epoch train time: 0:00:01.876379
elapsed time: 0:00:49.676687
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 23:22:23.451802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1100.23
 ---- batch: 020 ----
mean loss: 1094.58
 ---- batch: 030 ----
mean loss: 1078.58
 ---- batch: 040 ----
mean loss: 1072.43
 ---- batch: 050 ----
mean loss: 1066.19
 ---- batch: 060 ----
mean loss: 1071.49
 ---- batch: 070 ----
mean loss: 1068.81
 ---- batch: 080 ----
mean loss: 1064.25
 ---- batch: 090 ----
mean loss: 1053.09
 ---- batch: 100 ----
mean loss: 1028.07
 ---- batch: 110 ----
mean loss: 1047.78
train mean loss: 1066.99
epoch train time: 0:00:01.894231
elapsed time: 0:00:51.571499
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 23:22:25.346607
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1027.09
 ---- batch: 020 ----
mean loss: 1019.54
 ---- batch: 030 ----
mean loss: 1014.94
 ---- batch: 040 ----
mean loss: 1017.81
 ---- batch: 050 ----
mean loss: 1007.30
 ---- batch: 060 ----
mean loss: 1004.70
 ---- batch: 070 ----
mean loss: 989.90
 ---- batch: 080 ----
mean loss: 993.59
 ---- batch: 090 ----
mean loss: 998.04
 ---- batch: 100 ----
mean loss: 994.21
 ---- batch: 110 ----
mean loss: 980.54
train mean loss: 1003.96
epoch train time: 0:00:01.872330
elapsed time: 0:00:53.444385
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 23:22:27.219311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 988.16
 ---- batch: 020 ----
mean loss: 970.57
 ---- batch: 030 ----
mean loss: 973.63
 ---- batch: 040 ----
mean loss: 959.62
 ---- batch: 050 ----
mean loss: 944.63
 ---- batch: 060 ----
mean loss: 953.05
 ---- batch: 070 ----
mean loss: 957.49
 ---- batch: 080 ----
mean loss: 949.93
 ---- batch: 090 ----
mean loss: 952.07
 ---- batch: 100 ----
mean loss: 948.68
 ---- batch: 110 ----
mean loss: 936.63
train mean loss: 957.70
epoch train time: 0:00:01.871557
elapsed time: 0:00:55.316371
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 23:22:29.091468
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 948.21
 ---- batch: 020 ----
mean loss: 943.95
 ---- batch: 030 ----
mean loss: 938.27
 ---- batch: 040 ----
mean loss: 928.26
 ---- batch: 050 ----
mean loss: 922.88
 ---- batch: 060 ----
mean loss: 906.65
 ---- batch: 070 ----
mean loss: 930.69
 ---- batch: 080 ----
mean loss: 907.47
 ---- batch: 090 ----
mean loss: 912.94
 ---- batch: 100 ----
mean loss: 925.36
 ---- batch: 110 ----
mean loss: 902.24
train mean loss: 923.57
epoch train time: 0:00:01.910246
elapsed time: 0:00:57.227188
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 23:22:31.002318
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 902.35
 ---- batch: 020 ----
mean loss: 900.91
 ---- batch: 030 ----
mean loss: 903.64
 ---- batch: 040 ----
mean loss: 896.67
 ---- batch: 050 ----
mean loss: 900.71
 ---- batch: 060 ----
mean loss: 912.40
 ---- batch: 070 ----
mean loss: 901.26
 ---- batch: 080 ----
mean loss: 899.39
 ---- batch: 090 ----
mean loss: 883.39
 ---- batch: 100 ----
mean loss: 901.01
 ---- batch: 110 ----
mean loss: 900.56
train mean loss: 900.61
epoch train time: 0:00:01.866873
elapsed time: 0:00:59.094672
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 23:22:32.869777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 894.22
 ---- batch: 020 ----
mean loss: 883.98
 ---- batch: 030 ----
mean loss: 905.04
 ---- batch: 040 ----
mean loss: 898.68
 ---- batch: 050 ----
mean loss: 883.89
 ---- batch: 060 ----
mean loss: 876.82
 ---- batch: 070 ----
mean loss: 875.55
 ---- batch: 080 ----
mean loss: 871.49
 ---- batch: 090 ----
mean loss: 880.80
 ---- batch: 100 ----
mean loss: 875.60
 ---- batch: 110 ----
mean loss: 890.35
train mean loss: 884.24
epoch train time: 0:00:01.862981
elapsed time: 0:01:00.958327
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 23:22:34.733536
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 880.68
 ---- batch: 020 ----
mean loss: 883.47
 ---- batch: 030 ----
mean loss: 869.86
 ---- batch: 040 ----
mean loss: 861.34
 ---- batch: 050 ----
mean loss: 873.09
 ---- batch: 060 ----
mean loss: 876.61
 ---- batch: 070 ----
mean loss: 883.76
 ---- batch: 080 ----
mean loss: 858.45
 ---- batch: 090 ----
mean loss: 864.28
 ---- batch: 100 ----
mean loss: 879.07
 ---- batch: 110 ----
mean loss: 857.18
train mean loss: 872.51
epoch train time: 0:00:01.886873
elapsed time: 0:01:02.845865
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 23:22:36.620984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 861.96
 ---- batch: 020 ----
mean loss: 838.72
 ---- batch: 030 ----
mean loss: 862.77
 ---- batch: 040 ----
mean loss: 878.65
 ---- batch: 050 ----
mean loss: 881.73
 ---- batch: 060 ----
mean loss: 874.02
 ---- batch: 070 ----
mean loss: 873.07
 ---- batch: 080 ----
mean loss: 863.56
 ---- batch: 090 ----
mean loss: 851.68
 ---- batch: 100 ----
mean loss: 852.42
 ---- batch: 110 ----
mean loss: 856.28
train mean loss: 863.14
epoch train time: 0:00:01.874345
elapsed time: 0:01:04.720776
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 23:22:38.495867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.90
 ---- batch: 020 ----
mean loss: 861.21
 ---- batch: 030 ----
mean loss: 846.38
 ---- batch: 040 ----
mean loss: 857.00
 ---- batch: 050 ----
mean loss: 872.83
 ---- batch: 060 ----
mean loss: 839.59
 ---- batch: 070 ----
mean loss: 876.57
 ---- batch: 080 ----
mean loss: 852.04
 ---- batch: 090 ----
mean loss: 857.61
 ---- batch: 100 ----
mean loss: 870.15
 ---- batch: 110 ----
mean loss: 866.91
train mean loss: 857.77
epoch train time: 0:00:01.866309
elapsed time: 0:01:06.587631
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 23:22:40.362780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.43
 ---- batch: 020 ----
mean loss: 866.78
 ---- batch: 030 ----
mean loss: 853.11
 ---- batch: 040 ----
mean loss: 835.26
 ---- batch: 050 ----
mean loss: 845.95
 ---- batch: 060 ----
mean loss: 858.58
 ---- batch: 070 ----
mean loss: 853.75
 ---- batch: 080 ----
mean loss: 852.65
 ---- batch: 090 ----
mean loss: 860.83
 ---- batch: 100 ----
mean loss: 846.90
 ---- batch: 110 ----
mean loss: 877.11
train mean loss: 854.67
epoch train time: 0:00:01.905587
elapsed time: 0:01:08.493836
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 23:22:42.268954
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.18
 ---- batch: 020 ----
mean loss: 862.82
 ---- batch: 030 ----
mean loss: 865.37
 ---- batch: 040 ----
mean loss: 846.90
 ---- batch: 050 ----
mean loss: 838.23
 ---- batch: 060 ----
mean loss: 861.20
 ---- batch: 070 ----
mean loss: 850.23
 ---- batch: 080 ----
mean loss: 853.64
 ---- batch: 090 ----
mean loss: 845.00
 ---- batch: 100 ----
mean loss: 852.87
 ---- batch: 110 ----
mean loss: 859.63
train mean loss: 852.46
epoch train time: 0:00:01.901602
elapsed time: 0:01:10.396000
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 23:22:44.171079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.84
 ---- batch: 020 ----
mean loss: 886.62
 ---- batch: 030 ----
mean loss: 852.53
 ---- batch: 040 ----
mean loss: 877.01
 ---- batch: 050 ----
mean loss: 854.84
 ---- batch: 060 ----
mean loss: 866.21
 ---- batch: 070 ----
mean loss: 844.27
 ---- batch: 080 ----
mean loss: 862.38
 ---- batch: 090 ----
mean loss: 836.67
 ---- batch: 100 ----
mean loss: 832.12
 ---- batch: 110 ----
mean loss: 837.79
train mean loss: 851.06
epoch train time: 0:00:01.854577
elapsed time: 0:01:12.251124
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 23:22:46.026044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.17
 ---- batch: 020 ----
mean loss: 864.75
 ---- batch: 030 ----
mean loss: 831.62
 ---- batch: 040 ----
mean loss: 863.79
 ---- batch: 050 ----
mean loss: 855.24
 ---- batch: 060 ----
mean loss: 847.26
 ---- batch: 070 ----
mean loss: 860.03
 ---- batch: 080 ----
mean loss: 844.13
 ---- batch: 090 ----
mean loss: 840.35
 ---- batch: 100 ----
mean loss: 837.31
 ---- batch: 110 ----
mean loss: 856.36
train mean loss: 849.64
epoch train time: 0:00:01.877737
elapsed time: 0:01:14.129258
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 23:22:47.904377
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.48
 ---- batch: 020 ----
mean loss: 842.22
 ---- batch: 030 ----
mean loss: 825.85
 ---- batch: 040 ----
mean loss: 849.40
 ---- batch: 050 ----
mean loss: 876.74
 ---- batch: 060 ----
mean loss: 841.27
 ---- batch: 070 ----
mean loss: 870.65
 ---- batch: 080 ----
mean loss: 837.62
 ---- batch: 090 ----
mean loss: 851.62
 ---- batch: 100 ----
mean loss: 866.27
 ---- batch: 110 ----
mean loss: 849.05
train mean loss: 849.80
epoch train time: 0:00:01.884505
elapsed time: 0:01:16.014349
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 23:22:49.789476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.34
 ---- batch: 020 ----
mean loss: 846.11
 ---- batch: 030 ----
mean loss: 864.69
 ---- batch: 040 ----
mean loss: 845.12
 ---- batch: 050 ----
mean loss: 867.71
 ---- batch: 060 ----
mean loss: 839.62
 ---- batch: 070 ----
mean loss: 841.19
 ---- batch: 080 ----
mean loss: 863.17
 ---- batch: 090 ----
mean loss: 847.83
 ---- batch: 100 ----
mean loss: 858.79
 ---- batch: 110 ----
mean loss: 831.38
train mean loss: 849.47
epoch train time: 0:00:01.886239
elapsed time: 0:01:17.901208
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 23:22:51.676322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.46
 ---- batch: 020 ----
mean loss: 845.27
 ---- batch: 030 ----
mean loss: 831.42
 ---- batch: 040 ----
mean loss: 855.42
 ---- batch: 050 ----
mean loss: 853.47
 ---- batch: 060 ----
mean loss: 865.11
 ---- batch: 070 ----
mean loss: 822.09
 ---- batch: 080 ----
mean loss: 849.96
 ---- batch: 090 ----
mean loss: 860.55
 ---- batch: 100 ----
mean loss: 839.12
 ---- batch: 110 ----
mean loss: 861.03
train mean loss: 848.77
epoch train time: 0:00:01.862247
elapsed time: 0:01:19.764024
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 23:22:53.539109
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.28
 ---- batch: 020 ----
mean loss: 850.82
 ---- batch: 030 ----
mean loss: 850.73
 ---- batch: 040 ----
mean loss: 845.90
 ---- batch: 050 ----
mean loss: 837.62
 ---- batch: 060 ----
mean loss: 858.59
 ---- batch: 070 ----
mean loss: 869.60
 ---- batch: 080 ----
mean loss: 833.06
 ---- batch: 090 ----
mean loss: 852.37
 ---- batch: 100 ----
mean loss: 842.53
 ---- batch: 110 ----
mean loss: 843.54
train mean loss: 848.52
epoch train time: 0:00:01.911621
elapsed time: 0:01:21.676181
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 23:22:55.451289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.48
 ---- batch: 020 ----
mean loss: 855.51
 ---- batch: 030 ----
mean loss: 864.61
 ---- batch: 040 ----
mean loss: 850.14
 ---- batch: 050 ----
mean loss: 842.98
 ---- batch: 060 ----
mean loss: 825.48
 ---- batch: 070 ----
mean loss: 831.98
 ---- batch: 080 ----
mean loss: 864.61
 ---- batch: 090 ----
mean loss: 866.64
 ---- batch: 100 ----
mean loss: 839.15
 ---- batch: 110 ----
mean loss: 842.26
train mean loss: 847.89
epoch train time: 0:00:01.858073
elapsed time: 0:01:23.534809
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 23:22:57.309958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 847.54
 ---- batch: 020 ----
mean loss: 837.83
 ---- batch: 030 ----
mean loss: 859.07
 ---- batch: 040 ----
mean loss: 869.98
 ---- batch: 050 ----
mean loss: 839.64
 ---- batch: 060 ----
mean loss: 841.38
 ---- batch: 070 ----
mean loss: 833.64
 ---- batch: 080 ----
mean loss: 851.75
 ---- batch: 090 ----
mean loss: 856.49
 ---- batch: 100 ----
mean loss: 839.90
 ---- batch: 110 ----
mean loss: 850.76
train mean loss: 847.29
epoch train time: 0:00:01.862472
elapsed time: 0:01:25.397876
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 23:22:59.172980
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 813.68
 ---- batch: 020 ----
mean loss: 845.83
 ---- batch: 030 ----
mean loss: 862.90
 ---- batch: 040 ----
mean loss: 863.59
 ---- batch: 050 ----
mean loss: 862.78
 ---- batch: 060 ----
mean loss: 842.88
 ---- batch: 070 ----
mean loss: 856.48
 ---- batch: 080 ----
mean loss: 848.51
 ---- batch: 090 ----
mean loss: 826.35
 ---- batch: 100 ----
mean loss: 860.36
 ---- batch: 110 ----
mean loss: 840.08
train mean loss: 848.19
epoch train time: 0:00:01.919225
elapsed time: 0:01:27.317667
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 23:23:01.092786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.67
 ---- batch: 020 ----
mean loss: 825.13
 ---- batch: 030 ----
mean loss: 839.48
 ---- batch: 040 ----
mean loss: 845.35
 ---- batch: 050 ----
mean loss: 849.42
 ---- batch: 060 ----
mean loss: 845.96
 ---- batch: 070 ----
mean loss: 846.88
 ---- batch: 080 ----
mean loss: 861.82
 ---- batch: 090 ----
mean loss: 844.51
 ---- batch: 100 ----
mean loss: 854.17
 ---- batch: 110 ----
mean loss: 845.54
train mean loss: 846.65
epoch train time: 0:00:01.924246
elapsed time: 0:01:29.242522
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 23:23:03.017650
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.08
 ---- batch: 020 ----
mean loss: 843.58
 ---- batch: 030 ----
mean loss: 853.30
 ---- batch: 040 ----
mean loss: 863.02
 ---- batch: 050 ----
mean loss: 848.27
 ---- batch: 060 ----
mean loss: 844.88
 ---- batch: 070 ----
mean loss: 807.95
 ---- batch: 080 ----
mean loss: 854.85
 ---- batch: 090 ----
mean loss: 848.34
 ---- batch: 100 ----
mean loss: 860.97
 ---- batch: 110 ----
mean loss: 860.09
train mean loss: 847.93
epoch train time: 0:00:01.920257
elapsed time: 0:01:31.163362
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 23:23:04.938442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 852.17
 ---- batch: 020 ----
mean loss: 836.57
 ---- batch: 030 ----
mean loss: 866.55
 ---- batch: 040 ----
mean loss: 871.78
 ---- batch: 050 ----
mean loss: 822.86
 ---- batch: 060 ----
mean loss: 836.22
 ---- batch: 070 ----
mean loss: 860.33
 ---- batch: 080 ----
mean loss: 850.87
 ---- batch: 090 ----
mean loss: 846.20
 ---- batch: 100 ----
mean loss: 849.36
 ---- batch: 110 ----
mean loss: 834.32
train mean loss: 847.49
epoch train time: 0:00:01.869820
elapsed time: 0:01:33.033768
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 23:23:06.808892
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 832.39
 ---- batch: 020 ----
mean loss: 836.06
 ---- batch: 030 ----
mean loss: 858.21
 ---- batch: 040 ----
mean loss: 868.82
 ---- batch: 050 ----
mean loss: 852.26
 ---- batch: 060 ----
mean loss: 859.17
 ---- batch: 070 ----
mean loss: 827.83
 ---- batch: 080 ----
mean loss: 853.26
 ---- batch: 090 ----
mean loss: 841.54
 ---- batch: 100 ----
mean loss: 854.24
 ---- batch: 110 ----
mean loss: 833.59
train mean loss: 846.50
epoch train time: 0:00:01.861755
elapsed time: 0:01:34.896120
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 23:23:08.671281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 838.39
 ---- batch: 020 ----
mean loss: 851.81
 ---- batch: 030 ----
mean loss: 844.13
 ---- batch: 040 ----
mean loss: 845.56
 ---- batch: 050 ----
mean loss: 827.76
 ---- batch: 060 ----
mean loss: 839.36
 ---- batch: 070 ----
mean loss: 846.03
 ---- batch: 080 ----
mean loss: 862.14
 ---- batch: 090 ----
mean loss: 856.53
 ---- batch: 100 ----
mean loss: 851.53
 ---- batch: 110 ----
mean loss: 856.15
train mean loss: 846.39
epoch train time: 0:00:01.904024
elapsed time: 0:01:36.800748
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 23:23:10.575659
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 845.48
 ---- batch: 020 ----
mean loss: 819.11
 ---- batch: 030 ----
mean loss: 841.43
 ---- batch: 040 ----
mean loss: 851.91
 ---- batch: 050 ----
mean loss: 851.96
 ---- batch: 060 ----
mean loss: 854.71
 ---- batch: 070 ----
mean loss: 837.09
 ---- batch: 080 ----
mean loss: 854.01
 ---- batch: 090 ----
mean loss: 858.95
 ---- batch: 100 ----
mean loss: 855.24
 ---- batch: 110 ----
mean loss: 843.13
train mean loss: 846.49
epoch train time: 0:00:01.888101
elapsed time: 0:01:38.689232
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 23:23:12.464421
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 867.41
 ---- batch: 020 ----
mean loss: 863.96
 ---- batch: 030 ----
mean loss: 840.83
 ---- batch: 040 ----
mean loss: 845.89
 ---- batch: 050 ----
mean loss: 836.75
 ---- batch: 060 ----
mean loss: 838.78
 ---- batch: 070 ----
mean loss: 844.68
 ---- batch: 080 ----
mean loss: 853.72
 ---- batch: 090 ----
mean loss: 836.65
 ---- batch: 100 ----
mean loss: 838.37
 ---- batch: 110 ----
mean loss: 837.61
train mean loss: 846.47
epoch train time: 0:00:01.919469
elapsed time: 0:01:40.609365
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 23:23:14.384522
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 857.79
 ---- batch: 020 ----
mean loss: 845.07
 ---- batch: 030 ----
mean loss: 855.28
 ---- batch: 040 ----
mean loss: 847.08
 ---- batch: 050 ----
mean loss: 827.31
 ---- batch: 060 ----
mean loss: 853.46
 ---- batch: 070 ----
mean loss: 838.05
 ---- batch: 080 ----
mean loss: 829.87
 ---- batch: 090 ----
mean loss: 868.31
 ---- batch: 100 ----
mean loss: 854.06
 ---- batch: 110 ----
mean loss: 832.06
train mean loss: 846.25
epoch train time: 0:00:01.868930
elapsed time: 0:01:42.478916
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 23:23:16.254021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 858.53
 ---- batch: 020 ----
mean loss: 859.13
 ---- batch: 030 ----
mean loss: 835.31
 ---- batch: 040 ----
mean loss: 841.68
 ---- batch: 050 ----
mean loss: 870.23
 ---- batch: 060 ----
mean loss: 836.42
 ---- batch: 070 ----
mean loss: 840.80
 ---- batch: 080 ----
mean loss: 829.67
 ---- batch: 090 ----
mean loss: 828.12
 ---- batch: 100 ----
mean loss: 856.91
 ---- batch: 110 ----
mean loss: 858.19
train mean loss: 846.42
epoch train time: 0:00:01.887943
elapsed time: 0:01:44.367431
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 23:23:18.142549
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 855.56
 ---- batch: 020 ----
mean loss: 836.73
 ---- batch: 030 ----
mean loss: 857.09
 ---- batch: 040 ----
mean loss: 863.18
 ---- batch: 050 ----
mean loss: 821.05
 ---- batch: 060 ----
mean loss: 834.15
 ---- batch: 070 ----
mean loss: 859.30
 ---- batch: 080 ----
mean loss: 856.86
 ---- batch: 090 ----
mean loss: 826.91
 ---- batch: 100 ----
mean loss: 843.77
 ---- batch: 110 ----
mean loss: 847.00
train mean loss: 846.15
epoch train time: 0:00:01.887148
elapsed time: 0:01:46.255148
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 23:23:20.030260
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 849.25
 ---- batch: 020 ----
mean loss: 845.36
 ---- batch: 030 ----
mean loss: 845.73
 ---- batch: 040 ----
mean loss: 849.00
 ---- batch: 050 ----
mean loss: 835.59
 ---- batch: 060 ----
mean loss: 837.15
 ---- batch: 070 ----
mean loss: 842.58
 ---- batch: 080 ----
mean loss: 853.92
 ---- batch: 090 ----
mean loss: 846.36
 ---- batch: 100 ----
mean loss: 851.93
 ---- batch: 110 ----
mean loss: 851.85
train mean loss: 845.95
epoch train time: 0:00:01.862149
elapsed time: 0:01:48.117870
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 23:23:21.892969
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 843.56
 ---- batch: 020 ----
mean loss: 857.16
 ---- batch: 030 ----
mean loss: 837.27
 ---- batch: 040 ----
mean loss: 861.28
 ---- batch: 050 ----
mean loss: 858.80
 ---- batch: 060 ----
mean loss: 846.59
 ---- batch: 070 ----
mean loss: 836.58
 ---- batch: 080 ----
mean loss: 849.37
 ---- batch: 090 ----
mean loss: 837.11
 ---- batch: 100 ----
mean loss: 842.17
 ---- batch: 110 ----
mean loss: 847.15
train mean loss: 846.41
epoch train time: 0:00:01.895933
elapsed time: 0:01:50.014370
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 23:23:23.789477
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 856.59
 ---- batch: 020 ----
mean loss: 827.70
 ---- batch: 030 ----
mean loss: 841.50
 ---- batch: 040 ----
mean loss: 851.81
 ---- batch: 050 ----
mean loss: 837.73
 ---- batch: 060 ----
mean loss: 853.05
 ---- batch: 070 ----
mean loss: 847.65
 ---- batch: 080 ----
mean loss: 868.60
 ---- batch: 090 ----
mean loss: 856.06
 ---- batch: 100 ----
mean loss: 846.61
 ---- batch: 110 ----
mean loss: 823.75
train mean loss: 846.08
epoch train time: 0:00:01.909147
elapsed time: 0:01:51.924096
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 23:23:25.699267
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 842.51
 ---- batch: 020 ----
mean loss: 838.56
 ---- batch: 030 ----
mean loss: 848.64
 ---- batch: 040 ----
mean loss: 835.87
 ---- batch: 050 ----
mean loss: 869.79
 ---- batch: 060 ----
mean loss: 833.05
 ---- batch: 070 ----
mean loss: 846.39
 ---- batch: 080 ----
mean loss: 859.20
 ---- batch: 090 ----
mean loss: 843.96
 ---- batch: 100 ----
mean loss: 844.21
 ---- batch: 110 ----
mean loss: 843.50
train mean loss: 846.18
epoch train time: 0:00:01.886646
elapsed time: 0:01:53.811381
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 23:23:27.586484
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 831.54
 ---- batch: 020 ----
mean loss: 850.81
 ---- batch: 030 ----
mean loss: 855.90
 ---- batch: 040 ----
mean loss: 838.76
 ---- batch: 050 ----
mean loss: 847.73
 ---- batch: 060 ----
mean loss: 842.58
 ---- batch: 070 ----
mean loss: 810.47
 ---- batch: 080 ----
mean loss: 857.83
 ---- batch: 090 ----
mean loss: 847.39
 ---- batch: 100 ----
mean loss: 850.38
 ---- batch: 110 ----
mean loss: 855.04
train mean loss: 845.94
epoch train time: 0:00:01.890002
elapsed time: 0:01:55.701962
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 23:23:29.477122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 848.51
 ---- batch: 020 ----
mean loss: 838.61
 ---- batch: 030 ----
mean loss: 852.71
 ---- batch: 040 ----
mean loss: 860.63
 ---- batch: 050 ----
mean loss: 834.75
 ---- batch: 060 ----
mean loss: 866.74
 ---- batch: 070 ----
mean loss: 855.91
 ---- batch: 080 ----
mean loss: 835.35
 ---- batch: 090 ----
mean loss: 824.60
 ---- batch: 100 ----
mean loss: 833.76
 ---- batch: 110 ----
mean loss: 845.33
train mean loss: 846.23
epoch train time: 0:00:01.878662
elapsed time: 0:01:57.581237
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 23:23:31.356356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 818.83
 ---- batch: 020 ----
mean loss: 870.32
 ---- batch: 030 ----
mean loss: 860.38
 ---- batch: 040 ----
mean loss: 852.26
 ---- batch: 050 ----
mean loss: 832.36
 ---- batch: 060 ----
mean loss: 844.02
 ---- batch: 070 ----
mean loss: 838.26
 ---- batch: 080 ----
mean loss: 846.42
 ---- batch: 090 ----
mean loss: 843.77
 ---- batch: 100 ----
mean loss: 844.53
 ---- batch: 110 ----
mean loss: 856.11
train mean loss: 845.59
epoch train time: 0:00:01.886019
elapsed time: 0:01:59.467826
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 23:23:33.242988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 846.22
 ---- batch: 020 ----
mean loss: 827.58
 ---- batch: 030 ----
mean loss: 859.30
 ---- batch: 040 ----
mean loss: 857.48
 ---- batch: 050 ----
mean loss: 831.02
 ---- batch: 060 ----
mean loss: 826.52
 ---- batch: 070 ----
mean loss: 862.23
 ---- batch: 080 ----
mean loss: 822.90
 ---- batch: 090 ----
mean loss: 870.31
 ---- batch: 100 ----
mean loss: 841.53
 ---- batch: 110 ----
mean loss: 855.94
train mean loss: 845.50
epoch train time: 0:00:01.894343
elapsed time: 0:02:01.362834
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 23:23:35.137964
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 837.71
 ---- batch: 020 ----
mean loss: 842.03
 ---- batch: 030 ----
mean loss: 833.13
 ---- batch: 040 ----
mean loss: 834.03
 ---- batch: 050 ----
mean loss: 846.05
 ---- batch: 060 ----
mean loss: 841.42
 ---- batch: 070 ----
mean loss: 865.63
 ---- batch: 080 ----
mean loss: 853.13
 ---- batch: 090 ----
mean loss: 864.07
 ---- batch: 100 ----
mean loss: 830.64
 ---- batch: 110 ----
mean loss: 841.61
train mean loss: 845.19
epoch train time: 0:00:01.891463
elapsed time: 0:02:03.254916
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 23:23:37.030122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 836.35
 ---- batch: 020 ----
mean loss: 847.50
 ---- batch: 030 ----
mean loss: 852.13
 ---- batch: 040 ----
mean loss: 857.69
 ---- batch: 050 ----
mean loss: 833.55
 ---- batch: 060 ----
mean loss: 853.29
 ---- batch: 070 ----
mean loss: 866.64
 ---- batch: 080 ----
mean loss: 836.24
 ---- batch: 090 ----
mean loss: 833.88
 ---- batch: 100 ----
mean loss: 852.89
 ---- batch: 110 ----
mean loss: 826.85
train mean loss: 845.56
epoch train time: 0:00:01.872036
elapsed time: 0:02:05.127636
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 23:23:38.902753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 853.82
 ---- batch: 020 ----
mean loss: 849.65
 ---- batch: 030 ----
mean loss: 855.43
 ---- batch: 040 ----
mean loss: 835.68
 ---- batch: 050 ----
mean loss: 836.37
 ---- batch: 060 ----
mean loss: 848.34
 ---- batch: 070 ----
mean loss: 829.16
 ---- batch: 080 ----
mean loss: 847.55
 ---- batch: 090 ----
mean loss: 848.68
 ---- batch: 100 ----
mean loss: 841.51
 ---- batch: 110 ----
mean loss: 848.89
train mean loss: 845.30
epoch train time: 0:00:01.904040
elapsed time: 0:02:07.032293
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 23:23:40.807397
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 840.80
 ---- batch: 020 ----
mean loss: 848.06
 ---- batch: 030 ----
mean loss: 863.11
 ---- batch: 040 ----
mean loss: 853.93
 ---- batch: 050 ----
mean loss: 838.65
 ---- batch: 060 ----
mean loss: 840.38
 ---- batch: 070 ----
mean loss: 861.52
 ---- batch: 080 ----
mean loss: 856.40
 ---- batch: 090 ----
mean loss: 834.70
 ---- batch: 100 ----
mean loss: 818.49
 ---- batch: 110 ----
mean loss: 839.50
train mean loss: 844.90
epoch train time: 0:00:01.895764
elapsed time: 0:02:08.928650
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 23:23:42.703803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 854.27
 ---- batch: 020 ----
mean loss: 844.25
 ---- batch: 030 ----
mean loss: 842.45
 ---- batch: 040 ----
mean loss: 840.17
 ---- batch: 050 ----
mean loss: 845.43
 ---- batch: 060 ----
mean loss: 864.74
 ---- batch: 070 ----
mean loss: 844.19
 ---- batch: 080 ----
mean loss: 809.95
 ---- batch: 090 ----
mean loss: 833.12
 ---- batch: 100 ----
mean loss: 859.68
 ---- batch: 110 ----
mean loss: 849.42
train mean loss: 844.99
epoch train time: 0:00:01.890610
elapsed time: 0:02:10.819855
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 23:23:44.594998
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 863.08
 ---- batch: 020 ----
mean loss: 836.24
 ---- batch: 030 ----
mean loss: 838.95
 ---- batch: 040 ----
mean loss: 825.06
 ---- batch: 050 ----
mean loss: 841.66
 ---- batch: 060 ----
mean loss: 858.99
 ---- batch: 070 ----
mean loss: 824.82
 ---- batch: 080 ----
mean loss: 858.98
 ---- batch: 090 ----
mean loss: 837.62
 ---- batch: 100 ----
mean loss: 850.96
 ---- batch: 110 ----
mean loss: 863.84
train mean loss: 845.26
epoch train time: 0:00:01.886193
elapsed time: 0:02:12.706632
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 23:23:46.481775
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 850.92
 ---- batch: 020 ----
mean loss: 858.93
 ---- batch: 030 ----
mean loss: 853.45
 ---- batch: 040 ----
mean loss: 839.50
 ---- batch: 050 ----
mean loss: 830.54
 ---- batch: 060 ----
mean loss: 840.98
 ---- batch: 070 ----
mean loss: 857.22
 ---- batch: 080 ----
mean loss: 851.04
 ---- batch: 090 ----
mean loss: 833.81
 ---- batch: 100 ----
mean loss: 836.55
 ---- batch: 110 ----
mean loss: 841.91
train mean loss: 844.47
epoch train time: 0:00:01.872487
elapsed time: 0:02:14.579722
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 23:23:48.354831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 859.45
 ---- batch: 020 ----
mean loss: 855.61
 ---- batch: 030 ----
mean loss: 847.23
 ---- batch: 040 ----
mean loss: 842.03
 ---- batch: 050 ----
mean loss: 854.92
 ---- batch: 060 ----
mean loss: 810.64
 ---- batch: 070 ----
mean loss: 850.09
 ---- batch: 080 ----
mean loss: 800.62
 ---- batch: 090 ----
mean loss: 813.57
 ---- batch: 100 ----
mean loss: 800.47
 ---- batch: 110 ----
mean loss: 784.50
train mean loss: 826.38
epoch train time: 0:00:01.877858
elapsed time: 0:02:16.458132
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 23:23:50.233353
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 714.30
 ---- batch: 020 ----
mean loss: 643.02
 ---- batch: 030 ----
mean loss: 595.12
 ---- batch: 040 ----
mean loss: 537.98
 ---- batch: 050 ----
mean loss: 510.14
 ---- batch: 060 ----
mean loss: 476.29
 ---- batch: 070 ----
mean loss: 450.09
 ---- batch: 080 ----
mean loss: 426.12
 ---- batch: 090 ----
mean loss: 433.40
 ---- batch: 100 ----
mean loss: 410.32
 ---- batch: 110 ----
mean loss: 407.93
train mean loss: 506.45
epoch train time: 0:00:01.860029
elapsed time: 0:02:18.318907
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 23:23:52.093770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.78
 ---- batch: 020 ----
mean loss: 391.53
 ---- batch: 030 ----
mean loss: 404.91
 ---- batch: 040 ----
mean loss: 382.32
 ---- batch: 050 ----
mean loss: 383.43
 ---- batch: 060 ----
mean loss: 376.26
 ---- batch: 070 ----
mean loss: 373.91
 ---- batch: 080 ----
mean loss: 359.14
 ---- batch: 090 ----
mean loss: 365.69
 ---- batch: 100 ----
mean loss: 339.53
 ---- batch: 110 ----
mean loss: 342.20
train mean loss: 374.19
epoch train time: 0:00:01.901204
elapsed time: 0:02:20.220472
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 23:23:53.995590
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 348.84
 ---- batch: 020 ----
mean loss: 347.07
 ---- batch: 030 ----
mean loss: 325.00
 ---- batch: 040 ----
mean loss: 328.04
 ---- batch: 050 ----
mean loss: 343.03
 ---- batch: 060 ----
mean loss: 331.17
 ---- batch: 070 ----
mean loss: 329.66
 ---- batch: 080 ----
mean loss: 314.62
 ---- batch: 090 ----
mean loss: 329.91
 ---- batch: 100 ----
mean loss: 328.49
 ---- batch: 110 ----
mean loss: 327.94
train mean loss: 332.07
epoch train time: 0:00:01.908182
elapsed time: 0:02:22.129328
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 23:23:55.904526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 309.72
 ---- batch: 020 ----
mean loss: 321.56
 ---- batch: 030 ----
mean loss: 320.02
 ---- batch: 040 ----
mean loss: 312.09
 ---- batch: 050 ----
mean loss: 315.73
 ---- batch: 060 ----
mean loss: 299.97
 ---- batch: 070 ----
mean loss: 313.66
 ---- batch: 080 ----
mean loss: 317.10
 ---- batch: 090 ----
mean loss: 311.88
 ---- batch: 100 ----
mean loss: 318.00
 ---- batch: 110 ----
mean loss: 297.01
train mean loss: 312.54
epoch train time: 0:00:01.877948
elapsed time: 0:02:24.007973
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 23:23:57.783078
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 290.24
 ---- batch: 020 ----
mean loss: 301.62
 ---- batch: 030 ----
mean loss: 303.06
 ---- batch: 040 ----
mean loss: 297.34
 ---- batch: 050 ----
mean loss: 289.03
 ---- batch: 060 ----
mean loss: 296.25
 ---- batch: 070 ----
mean loss: 286.51
 ---- batch: 080 ----
mean loss: 301.95
 ---- batch: 090 ----
mean loss: 305.77
 ---- batch: 100 ----
mean loss: 296.39
 ---- batch: 110 ----
mean loss: 304.01
train mean loss: 297.70
epoch train time: 0:00:01.889857
elapsed time: 0:02:25.898391
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 23:23:59.673533
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.82
 ---- batch: 020 ----
mean loss: 285.73
 ---- batch: 030 ----
mean loss: 294.93
 ---- batch: 040 ----
mean loss: 288.17
 ---- batch: 050 ----
mean loss: 289.40
 ---- batch: 060 ----
mean loss: 281.64
 ---- batch: 070 ----
mean loss: 283.47
 ---- batch: 080 ----
mean loss: 284.01
 ---- batch: 090 ----
mean loss: 282.56
 ---- batch: 100 ----
mean loss: 284.22
 ---- batch: 110 ----
mean loss: 282.57
train mean loss: 285.80
epoch train time: 0:00:01.850434
elapsed time: 0:02:27.749447
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 23:24:01.524579
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 271.68
 ---- batch: 020 ----
mean loss: 282.81
 ---- batch: 030 ----
mean loss: 277.70
 ---- batch: 040 ----
mean loss: 265.13
 ---- batch: 050 ----
mean loss: 278.10
 ---- batch: 060 ----
mean loss: 271.11
 ---- batch: 070 ----
mean loss: 280.62
 ---- batch: 080 ----
mean loss: 267.12
 ---- batch: 090 ----
mean loss: 279.79
 ---- batch: 100 ----
mean loss: 273.46
 ---- batch: 110 ----
mean loss: 283.80
train mean loss: 275.12
epoch train time: 0:00:01.884447
elapsed time: 0:02:29.634483
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 23:24:03.409593
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 275.52
 ---- batch: 020 ----
mean loss: 257.77
 ---- batch: 030 ----
mean loss: 264.47
 ---- batch: 040 ----
mean loss: 272.74
 ---- batch: 050 ----
mean loss: 265.38
 ---- batch: 060 ----
mean loss: 273.35
 ---- batch: 070 ----
mean loss: 255.54
 ---- batch: 080 ----
mean loss: 278.77
 ---- batch: 090 ----
mean loss: 270.15
 ---- batch: 100 ----
mean loss: 264.66
 ---- batch: 110 ----
mean loss: 265.97
train mean loss: 267.69
epoch train time: 0:00:01.924155
elapsed time: 0:02:31.559209
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 23:24:05.334333
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 268.24
 ---- batch: 020 ----
mean loss: 259.21
 ---- batch: 030 ----
mean loss: 261.08
 ---- batch: 040 ----
mean loss: 258.38
 ---- batch: 050 ----
mean loss: 252.21
 ---- batch: 060 ----
mean loss: 258.10
 ---- batch: 070 ----
mean loss: 268.37
 ---- batch: 080 ----
mean loss: 255.30
 ---- batch: 090 ----
mean loss: 253.46
 ---- batch: 100 ----
mean loss: 262.41
 ---- batch: 110 ----
mean loss: 271.15
train mean loss: 260.42
epoch train time: 0:00:01.884261
elapsed time: 0:02:33.444048
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 23:24:07.219158
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.11
 ---- batch: 020 ----
mean loss: 253.01
 ---- batch: 030 ----
mean loss: 251.93
 ---- batch: 040 ----
mean loss: 241.46
 ---- batch: 050 ----
mean loss: 253.60
 ---- batch: 060 ----
mean loss: 260.86
 ---- batch: 070 ----
mean loss: 257.01
 ---- batch: 080 ----
mean loss: 256.07
 ---- batch: 090 ----
mean loss: 265.12
 ---- batch: 100 ----
mean loss: 251.61
 ---- batch: 110 ----
mean loss: 243.26
train mean loss: 254.05
epoch train time: 0:00:01.872064
elapsed time: 0:02:35.316676
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 23:24:09.091773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.18
 ---- batch: 020 ----
mean loss: 241.54
 ---- batch: 030 ----
mean loss: 255.73
 ---- batch: 040 ----
mean loss: 250.43
 ---- batch: 050 ----
mean loss: 258.75
 ---- batch: 060 ----
mean loss: 246.27
 ---- batch: 070 ----
mean loss: 257.98
 ---- batch: 080 ----
mean loss: 246.00
 ---- batch: 090 ----
mean loss: 244.28
 ---- batch: 100 ----
mean loss: 239.18
 ---- batch: 110 ----
mean loss: 253.54
train mean loss: 250.07
epoch train time: 0:00:01.863218
elapsed time: 0:02:37.180528
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 23:24:10.955643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.80
 ---- batch: 020 ----
mean loss: 248.97
 ---- batch: 030 ----
mean loss: 244.51
 ---- batch: 040 ----
mean loss: 240.14
 ---- batch: 050 ----
mean loss: 234.92
 ---- batch: 060 ----
mean loss: 254.87
 ---- batch: 070 ----
mean loss: 249.37
 ---- batch: 080 ----
mean loss: 246.78
 ---- batch: 090 ----
mean loss: 241.30
 ---- batch: 100 ----
mean loss: 241.13
 ---- batch: 110 ----
mean loss: 238.46
train mean loss: 244.09
epoch train time: 0:00:01.910921
elapsed time: 0:02:39.092029
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 23:24:12.867157
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.62
 ---- batch: 020 ----
mean loss: 237.24
 ---- batch: 030 ----
mean loss: 230.54
 ---- batch: 040 ----
mean loss: 247.25
 ---- batch: 050 ----
mean loss: 243.52
 ---- batch: 060 ----
mean loss: 238.39
 ---- batch: 070 ----
mean loss: 244.88
 ---- batch: 080 ----
mean loss: 232.01
 ---- batch: 090 ----
mean loss: 245.84
 ---- batch: 100 ----
mean loss: 242.10
 ---- batch: 110 ----
mean loss: 245.37
train mean loss: 241.17
epoch train time: 0:00:01.888074
elapsed time: 0:02:40.980706
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 23:24:14.755860
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 236.44
 ---- batch: 020 ----
mean loss: 239.42
 ---- batch: 030 ----
mean loss: 227.49
 ---- batch: 040 ----
mean loss: 238.13
 ---- batch: 050 ----
mean loss: 241.05
 ---- batch: 060 ----
mean loss: 242.49
 ---- batch: 070 ----
mean loss: 237.83
 ---- batch: 080 ----
mean loss: 234.33
 ---- batch: 090 ----
mean loss: 238.02
 ---- batch: 100 ----
mean loss: 230.68
 ---- batch: 110 ----
mean loss: 242.36
train mean loss: 236.93
epoch train time: 0:00:01.840845
elapsed time: 0:02:42.822179
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 23:24:16.597323
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.65
 ---- batch: 020 ----
mean loss: 231.75
 ---- batch: 030 ----
mean loss: 234.38
 ---- batch: 040 ----
mean loss: 222.44
 ---- batch: 050 ----
mean loss: 228.98
 ---- batch: 060 ----
mean loss: 235.57
 ---- batch: 070 ----
mean loss: 231.21
 ---- batch: 080 ----
mean loss: 228.52
 ---- batch: 090 ----
mean loss: 231.74
 ---- batch: 100 ----
mean loss: 227.87
 ---- batch: 110 ----
mean loss: 235.11
train mean loss: 231.52
epoch train time: 0:00:01.870353
elapsed time: 0:02:44.693133
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 23:24:18.468295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.48
 ---- batch: 020 ----
mean loss: 234.12
 ---- batch: 030 ----
mean loss: 239.62
 ---- batch: 040 ----
mean loss: 224.70
 ---- batch: 050 ----
mean loss: 224.91
 ---- batch: 060 ----
mean loss: 223.55
 ---- batch: 070 ----
mean loss: 231.73
 ---- batch: 080 ----
mean loss: 229.24
 ---- batch: 090 ----
mean loss: 228.00
 ---- batch: 100 ----
mean loss: 225.01
 ---- batch: 110 ----
mean loss: 227.80
train mean loss: 228.20
epoch train time: 0:00:01.883458
elapsed time: 0:02:46.577207
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 23:24:20.352310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.95
 ---- batch: 020 ----
mean loss: 215.46
 ---- batch: 030 ----
mean loss: 225.53
 ---- batch: 040 ----
mean loss: 225.37
 ---- batch: 050 ----
mean loss: 210.99
 ---- batch: 060 ----
mean loss: 223.47
 ---- batch: 070 ----
mean loss: 228.78
 ---- batch: 080 ----
mean loss: 236.18
 ---- batch: 090 ----
mean loss: 222.92
 ---- batch: 100 ----
mean loss: 227.20
 ---- batch: 110 ----
mean loss: 225.47
train mean loss: 224.43
epoch train time: 0:00:01.887876
elapsed time: 0:02:48.465662
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 23:24:22.240786
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.17
 ---- batch: 020 ----
mean loss: 218.65
 ---- batch: 030 ----
mean loss: 219.88
 ---- batch: 040 ----
mean loss: 225.39
 ---- batch: 050 ----
mean loss: 227.57
 ---- batch: 060 ----
mean loss: 222.59
 ---- batch: 070 ----
mean loss: 219.65
 ---- batch: 080 ----
mean loss: 216.44
 ---- batch: 090 ----
mean loss: 225.50
 ---- batch: 100 ----
mean loss: 223.85
 ---- batch: 110 ----
mean loss: 217.86
train mean loss: 221.32
epoch train time: 0:00:01.863377
elapsed time: 0:02:50.329612
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 23:24:24.104725
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.85
 ---- batch: 020 ----
mean loss: 225.02
 ---- batch: 030 ----
mean loss: 216.75
 ---- batch: 040 ----
mean loss: 213.16
 ---- batch: 050 ----
mean loss: 210.53
 ---- batch: 060 ----
mean loss: 224.40
 ---- batch: 070 ----
mean loss: 214.25
 ---- batch: 080 ----
mean loss: 223.37
 ---- batch: 090 ----
mean loss: 218.26
 ---- batch: 100 ----
mean loss: 221.81
 ---- batch: 110 ----
mean loss: 216.15
train mean loss: 218.36
epoch train time: 0:00:01.863769
elapsed time: 0:02:52.194096
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 23:24:25.969250
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.78
 ---- batch: 020 ----
mean loss: 212.08
 ---- batch: 030 ----
mean loss: 210.36
 ---- batch: 040 ----
mean loss: 210.01
 ---- batch: 050 ----
mean loss: 208.31
 ---- batch: 060 ----
mean loss: 221.68
 ---- batch: 070 ----
mean loss: 217.56
 ---- batch: 080 ----
mean loss: 221.59
 ---- batch: 090 ----
mean loss: 216.49
 ---- batch: 100 ----
mean loss: 214.45
 ---- batch: 110 ----
mean loss: 212.40
train mean loss: 215.39
epoch train time: 0:00:01.886488
elapsed time: 0:02:54.081225
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 23:24:27.856356
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.43
 ---- batch: 020 ----
mean loss: 210.67
 ---- batch: 030 ----
mean loss: 223.98
 ---- batch: 040 ----
mean loss: 215.67
 ---- batch: 050 ----
mean loss: 218.54
 ---- batch: 060 ----
mean loss: 202.61
 ---- batch: 070 ----
mean loss: 205.89
 ---- batch: 080 ----
mean loss: 212.48
 ---- batch: 090 ----
mean loss: 207.46
 ---- batch: 100 ----
mean loss: 207.62
 ---- batch: 110 ----
mean loss: 220.42
train mean loss: 211.86
epoch train time: 0:00:01.877193
elapsed time: 0:02:55.959066
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 23:24:29.734168
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.82
 ---- batch: 020 ----
mean loss: 202.64
 ---- batch: 030 ----
mean loss: 211.75
 ---- batch: 040 ----
mean loss: 211.98
 ---- batch: 050 ----
mean loss: 211.85
 ---- batch: 060 ----
mean loss: 210.69
 ---- batch: 070 ----
mean loss: 211.94
 ---- batch: 080 ----
mean loss: 207.56
 ---- batch: 090 ----
mean loss: 209.37
 ---- batch: 100 ----
mean loss: 197.78
 ---- batch: 110 ----
mean loss: 208.25
train mean loss: 208.32
epoch train time: 0:00:01.905400
elapsed time: 0:02:57.865027
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 23:24:31.640164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.50
 ---- batch: 020 ----
mean loss: 209.69
 ---- batch: 030 ----
mean loss: 201.59
 ---- batch: 040 ----
mean loss: 204.70
 ---- batch: 050 ----
mean loss: 216.27
 ---- batch: 060 ----
mean loss: 206.26
 ---- batch: 070 ----
mean loss: 208.34
 ---- batch: 080 ----
mean loss: 205.30
 ---- batch: 090 ----
mean loss: 212.03
 ---- batch: 100 ----
mean loss: 212.25
 ---- batch: 110 ----
mean loss: 197.18
train mean loss: 206.90
epoch train time: 0:00:01.880973
elapsed time: 0:02:59.746610
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 23:24:33.521833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.43
 ---- batch: 020 ----
mean loss: 199.19
 ---- batch: 030 ----
mean loss: 194.07
 ---- batch: 040 ----
mean loss: 203.67
 ---- batch: 050 ----
mean loss: 205.40
 ---- batch: 060 ----
mean loss: 215.74
 ---- batch: 070 ----
mean loss: 204.28
 ---- batch: 080 ----
mean loss: 208.60
 ---- batch: 090 ----
mean loss: 202.27
 ---- batch: 100 ----
mean loss: 202.22
 ---- batch: 110 ----
mean loss: 210.10
train mean loss: 203.86
epoch train time: 0:00:01.875885
elapsed time: 0:03:01.623172
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 23:24:35.398311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.09
 ---- batch: 020 ----
mean loss: 197.15
 ---- batch: 030 ----
mean loss: 196.06
 ---- batch: 040 ----
mean loss: 205.98
 ---- batch: 050 ----
mean loss: 201.08
 ---- batch: 060 ----
mean loss: 205.83
 ---- batch: 070 ----
mean loss: 205.55
 ---- batch: 080 ----
mean loss: 192.26
 ---- batch: 090 ----
mean loss: 205.80
 ---- batch: 100 ----
mean loss: 194.79
 ---- batch: 110 ----
mean loss: 214.62
train mean loss: 201.35
epoch train time: 0:00:01.879171
elapsed time: 0:03:03.502945
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 23:24:37.278081
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.85
 ---- batch: 020 ----
mean loss: 202.73
 ---- batch: 030 ----
mean loss: 193.54
 ---- batch: 040 ----
mean loss: 194.71
 ---- batch: 050 ----
mean loss: 198.23
 ---- batch: 060 ----
mean loss: 204.70
 ---- batch: 070 ----
mean loss: 206.87
 ---- batch: 080 ----
mean loss: 196.03
 ---- batch: 090 ----
mean loss: 199.06
 ---- batch: 100 ----
mean loss: 200.69
 ---- batch: 110 ----
mean loss: 192.16
train mean loss: 198.11
epoch train time: 0:00:01.898006
elapsed time: 0:03:05.401579
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 23:24:39.176711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.09
 ---- batch: 020 ----
mean loss: 193.90
 ---- batch: 030 ----
mean loss: 187.72
 ---- batch: 040 ----
mean loss: 198.02
 ---- batch: 050 ----
mean loss: 196.35
 ---- batch: 060 ----
mean loss: 186.88
 ---- batch: 070 ----
mean loss: 199.33
 ---- batch: 080 ----
mean loss: 214.30
 ---- batch: 090 ----
mean loss: 199.85
 ---- batch: 100 ----
mean loss: 184.11
 ---- batch: 110 ----
mean loss: 206.93
train mean loss: 196.04
epoch train time: 0:00:01.881816
elapsed time: 0:03:07.283994
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 23:24:41.059129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.29
 ---- batch: 020 ----
mean loss: 201.12
 ---- batch: 030 ----
mean loss: 197.22
 ---- batch: 040 ----
mean loss: 200.53
 ---- batch: 050 ----
mean loss: 188.55
 ---- batch: 060 ----
mean loss: 197.99
 ---- batch: 070 ----
mean loss: 200.79
 ---- batch: 080 ----
mean loss: 191.92
 ---- batch: 090 ----
mean loss: 194.22
 ---- batch: 100 ----
mean loss: 197.00
 ---- batch: 110 ----
mean loss: 193.51
train mean loss: 195.14
epoch train time: 0:00:01.907085
elapsed time: 0:03:09.191704
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 23:24:42.966796
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.74
 ---- batch: 020 ----
mean loss: 192.55
 ---- batch: 030 ----
mean loss: 188.41
 ---- batch: 040 ----
mean loss: 186.95
 ---- batch: 050 ----
mean loss: 194.82
 ---- batch: 060 ----
mean loss: 186.07
 ---- batch: 070 ----
mean loss: 189.98
 ---- batch: 080 ----
mean loss: 190.03
 ---- batch: 090 ----
mean loss: 188.03
 ---- batch: 100 ----
mean loss: 197.84
 ---- batch: 110 ----
mean loss: 195.36
train mean loss: 191.06
epoch train time: 0:00:01.889662
elapsed time: 0:03:11.081922
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 23:24:44.857022
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.43
 ---- batch: 020 ----
mean loss: 190.96
 ---- batch: 030 ----
mean loss: 188.58
 ---- batch: 040 ----
mean loss: 187.02
 ---- batch: 050 ----
mean loss: 188.02
 ---- batch: 060 ----
mean loss: 191.23
 ---- batch: 070 ----
mean loss: 198.40
 ---- batch: 080 ----
mean loss: 198.11
 ---- batch: 090 ----
mean loss: 190.20
 ---- batch: 100 ----
mean loss: 197.12
 ---- batch: 110 ----
mean loss: 186.01
train mean loss: 190.83
epoch train time: 0:00:01.866070
elapsed time: 0:03:12.948559
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 23:24:46.723727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.81
 ---- batch: 020 ----
mean loss: 195.31
 ---- batch: 030 ----
mean loss: 175.05
 ---- batch: 040 ----
mean loss: 192.20
 ---- batch: 050 ----
mean loss: 193.47
 ---- batch: 060 ----
mean loss: 190.23
 ---- batch: 070 ----
mean loss: 190.82
 ---- batch: 080 ----
mean loss: 191.87
 ---- batch: 090 ----
mean loss: 193.54
 ---- batch: 100 ----
mean loss: 190.38
 ---- batch: 110 ----
mean loss: 193.60
train mean loss: 189.47
epoch train time: 0:00:01.852843
elapsed time: 0:03:14.802009
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 23:24:48.577153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 181.71
 ---- batch: 020 ----
mean loss: 194.03
 ---- batch: 030 ----
mean loss: 179.00
 ---- batch: 040 ----
mean loss: 181.19
 ---- batch: 050 ----
mean loss: 191.50
 ---- batch: 060 ----
mean loss: 186.79
 ---- batch: 070 ----
mean loss: 184.36
 ---- batch: 080 ----
mean loss: 178.89
 ---- batch: 090 ----
mean loss: 193.41
 ---- batch: 100 ----
mean loss: 189.44
 ---- batch: 110 ----
mean loss: 192.96
train mean loss: 186.60
epoch train time: 0:00:01.862725
elapsed time: 0:03:16.665398
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 23:24:50.440513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 185.88
 ---- batch: 020 ----
mean loss: 188.07
 ---- batch: 030 ----
mean loss: 180.88
 ---- batch: 040 ----
mean loss: 177.40
 ---- batch: 050 ----
mean loss: 184.71
 ---- batch: 060 ----
mean loss: 186.49
 ---- batch: 070 ----
mean loss: 188.44
 ---- batch: 080 ----
mean loss: 190.23
 ---- batch: 090 ----
mean loss: 186.30
 ---- batch: 100 ----
mean loss: 176.32
 ---- batch: 110 ----
mean loss: 189.54
train mean loss: 185.20
epoch train time: 0:00:01.885557
elapsed time: 0:03:18.551535
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 23:24:52.326747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.55
 ---- batch: 020 ----
mean loss: 174.92
 ---- batch: 030 ----
mean loss: 186.63
 ---- batch: 040 ----
mean loss: 180.85
 ---- batch: 050 ----
mean loss: 188.77
 ---- batch: 060 ----
mean loss: 176.67
 ---- batch: 070 ----
mean loss: 184.95
 ---- batch: 080 ----
mean loss: 187.39
 ---- batch: 090 ----
mean loss: 187.75
 ---- batch: 100 ----
mean loss: 189.93
 ---- batch: 110 ----
mean loss: 179.59
train mean loss: 182.41
epoch train time: 0:00:01.886690
elapsed time: 0:03:20.438992
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 23:24:54.214150
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.75
 ---- batch: 020 ----
mean loss: 177.21
 ---- batch: 030 ----
mean loss: 185.66
 ---- batch: 040 ----
mean loss: 185.49
 ---- batch: 050 ----
mean loss: 172.15
 ---- batch: 060 ----
mean loss: 179.75
 ---- batch: 070 ----
mean loss: 185.99
 ---- batch: 080 ----
mean loss: 181.93
 ---- batch: 090 ----
mean loss: 182.19
 ---- batch: 100 ----
mean loss: 177.99
 ---- batch: 110 ----
mean loss: 185.12
train mean loss: 181.36
epoch train time: 0:00:01.882859
elapsed time: 0:03:22.322540
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 23:24:56.097697
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 182.83
 ---- batch: 020 ----
mean loss: 176.30
 ---- batch: 030 ----
mean loss: 175.06
 ---- batch: 040 ----
mean loss: 178.20
 ---- batch: 050 ----
mean loss: 171.00
 ---- batch: 060 ----
mean loss: 187.37
 ---- batch: 070 ----
mean loss: 189.29
 ---- batch: 080 ----
mean loss: 180.41
 ---- batch: 090 ----
mean loss: 176.88
 ---- batch: 100 ----
mean loss: 182.47
 ---- batch: 110 ----
mean loss: 185.14
train mean loss: 180.34
epoch train time: 0:00:01.881239
elapsed time: 0:03:24.204415
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 23:24:57.979603
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.38
 ---- batch: 020 ----
mean loss: 183.12
 ---- batch: 030 ----
mean loss: 183.49
 ---- batch: 040 ----
mean loss: 174.61
 ---- batch: 050 ----
mean loss: 174.71
 ---- batch: 060 ----
mean loss: 183.32
 ---- batch: 070 ----
mean loss: 177.05
 ---- batch: 080 ----
mean loss: 177.89
 ---- batch: 090 ----
mean loss: 174.77
 ---- batch: 100 ----
mean loss: 178.95
 ---- batch: 110 ----
mean loss: 181.81
train mean loss: 178.63
epoch train time: 0:00:01.880289
elapsed time: 0:03:26.085356
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 23:24:59.860623
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 173.50
 ---- batch: 020 ----
mean loss: 181.08
 ---- batch: 030 ----
mean loss: 167.28
 ---- batch: 040 ----
mean loss: 183.02
 ---- batch: 050 ----
mean loss: 167.91
 ---- batch: 060 ----
mean loss: 175.55
 ---- batch: 070 ----
mean loss: 182.23
 ---- batch: 080 ----
mean loss: 179.85
 ---- batch: 090 ----
mean loss: 173.38
 ---- batch: 100 ----
mean loss: 179.23
 ---- batch: 110 ----
mean loss: 181.72
train mean loss: 177.20
epoch train time: 0:00:01.903377
elapsed time: 0:03:27.989450
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 23:25:01.764548
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 176.73
 ---- batch: 020 ----
mean loss: 180.98
 ---- batch: 030 ----
mean loss: 173.15
 ---- batch: 040 ----
mean loss: 178.52
 ---- batch: 050 ----
mean loss: 174.29
 ---- batch: 060 ----
mean loss: 165.26
 ---- batch: 070 ----
mean loss: 181.41
 ---- batch: 080 ----
mean loss: 168.76
 ---- batch: 090 ----
mean loss: 177.68
 ---- batch: 100 ----
mean loss: 176.51
 ---- batch: 110 ----
mean loss: 176.81
train mean loss: 175.49
epoch train time: 0:00:01.866428
elapsed time: 0:03:29.856415
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 23:25:03.631547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.96
 ---- batch: 020 ----
mean loss: 181.74
 ---- batch: 030 ----
mean loss: 172.62
 ---- batch: 040 ----
mean loss: 175.87
 ---- batch: 050 ----
mean loss: 172.91
 ---- batch: 060 ----
mean loss: 176.58
 ---- batch: 070 ----
mean loss: 182.25
 ---- batch: 080 ----
mean loss: 181.11
 ---- batch: 090 ----
mean loss: 166.69
 ---- batch: 100 ----
mean loss: 178.93
 ---- batch: 110 ----
mean loss: 167.11
train mean loss: 174.65
epoch train time: 0:00:01.889270
elapsed time: 0:03:31.746306
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 23:25:05.521429
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 161.16
 ---- batch: 020 ----
mean loss: 171.24
 ---- batch: 030 ----
mean loss: 182.39
 ---- batch: 040 ----
mean loss: 170.22
 ---- batch: 050 ----
mean loss: 174.81
 ---- batch: 060 ----
mean loss: 171.01
 ---- batch: 070 ----
mean loss: 173.64
 ---- batch: 080 ----
mean loss: 176.22
 ---- batch: 090 ----
mean loss: 171.65
 ---- batch: 100 ----
mean loss: 174.73
 ---- batch: 110 ----
mean loss: 174.40
train mean loss: 173.84
epoch train time: 0:00:01.911215
elapsed time: 0:03:33.658150
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 23:25:07.433268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 177.36
 ---- batch: 020 ----
mean loss: 170.79
 ---- batch: 030 ----
mean loss: 169.31
 ---- batch: 040 ----
mean loss: 175.85
 ---- batch: 050 ----
mean loss: 179.47
 ---- batch: 060 ----
mean loss: 167.44
 ---- batch: 070 ----
mean loss: 163.37
 ---- batch: 080 ----
mean loss: 167.78
 ---- batch: 090 ----
mean loss: 169.93
 ---- batch: 100 ----
mean loss: 173.45
 ---- batch: 110 ----
mean loss: 172.18
train mean loss: 171.59
epoch train time: 0:00:01.887108
elapsed time: 0:03:35.545870
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 23:25:09.321048
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 171.95
 ---- batch: 020 ----
mean loss: 161.00
 ---- batch: 030 ----
mean loss: 154.12
 ---- batch: 040 ----
mean loss: 174.65
 ---- batch: 050 ----
mean loss: 179.74
 ---- batch: 060 ----
mean loss: 173.55
 ---- batch: 070 ----
mean loss: 172.48
 ---- batch: 080 ----
mean loss: 170.90
 ---- batch: 090 ----
mean loss: 170.46
 ---- batch: 100 ----
mean loss: 173.00
 ---- batch: 110 ----
mean loss: 174.61
train mean loss: 170.39
epoch train time: 0:00:01.883483
elapsed time: 0:03:37.430007
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 23:25:11.204840
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.34
 ---- batch: 020 ----
mean loss: 167.72
 ---- batch: 030 ----
mean loss: 165.95
 ---- batch: 040 ----
mean loss: 165.18
 ---- batch: 050 ----
mean loss: 181.08
 ---- batch: 060 ----
mean loss: 168.98
 ---- batch: 070 ----
mean loss: 164.55
 ---- batch: 080 ----
mean loss: 174.89
 ---- batch: 090 ----
mean loss: 177.18
 ---- batch: 100 ----
mean loss: 174.24
 ---- batch: 110 ----
mean loss: 165.71
train mean loss: 169.86
epoch train time: 0:00:01.888955
elapsed time: 0:03:39.319251
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 23:25:13.094400
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 164.34
 ---- batch: 020 ----
mean loss: 164.68
 ---- batch: 030 ----
mean loss: 169.40
 ---- batch: 040 ----
mean loss: 164.32
 ---- batch: 050 ----
mean loss: 169.82
 ---- batch: 060 ----
mean loss: 169.80
 ---- batch: 070 ----
mean loss: 171.21
 ---- batch: 080 ----
mean loss: 179.45
 ---- batch: 090 ----
mean loss: 170.55
 ---- batch: 100 ----
mean loss: 168.19
 ---- batch: 110 ----
mean loss: 164.79
train mean loss: 168.54
epoch train time: 0:00:01.891563
elapsed time: 0:03:41.211429
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 23:25:14.986539
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.55
 ---- batch: 020 ----
mean loss: 173.66
 ---- batch: 030 ----
mean loss: 158.82
 ---- batch: 040 ----
mean loss: 170.99
 ---- batch: 050 ----
mean loss: 166.73
 ---- batch: 060 ----
mean loss: 167.68
 ---- batch: 070 ----
mean loss: 162.18
 ---- batch: 080 ----
mean loss: 160.60
 ---- batch: 090 ----
mean loss: 167.00
 ---- batch: 100 ----
mean loss: 172.71
 ---- batch: 110 ----
mean loss: 174.08
train mean loss: 167.91
epoch train time: 0:00:01.889649
elapsed time: 0:03:43.101674
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 23:25:16.876831
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 163.98
 ---- batch: 020 ----
mean loss: 162.23
 ---- batch: 030 ----
mean loss: 168.62
 ---- batch: 040 ----
mean loss: 166.06
 ---- batch: 050 ----
mean loss: 166.74
 ---- batch: 060 ----
mean loss: 170.28
 ---- batch: 070 ----
mean loss: 170.50
 ---- batch: 080 ----
mean loss: 167.13
 ---- batch: 090 ----
mean loss: 166.04
 ---- batch: 100 ----
mean loss: 171.56
 ---- batch: 110 ----
mean loss: 169.71
train mean loss: 167.38
epoch train time: 0:00:01.919936
elapsed time: 0:03:45.022326
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 23:25:18.797463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 170.24
 ---- batch: 020 ----
mean loss: 166.16
 ---- batch: 030 ----
mean loss: 164.04
 ---- batch: 040 ----
mean loss: 163.25
 ---- batch: 050 ----
mean loss: 169.75
 ---- batch: 060 ----
mean loss: 158.00
 ---- batch: 070 ----
mean loss: 166.96
 ---- batch: 080 ----
mean loss: 168.34
 ---- batch: 090 ----
mean loss: 167.05
 ---- batch: 100 ----
mean loss: 159.70
 ---- batch: 110 ----
mean loss: 168.93
train mean loss: 165.63
epoch train time: 0:00:01.842925
elapsed time: 0:03:46.865843
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 23:25:20.640941
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.52
 ---- batch: 020 ----
mean loss: 163.77
 ---- batch: 030 ----
mean loss: 164.76
 ---- batch: 040 ----
mean loss: 168.57
 ---- batch: 050 ----
mean loss: 161.92
 ---- batch: 060 ----
mean loss: 168.55
 ---- batch: 070 ----
mean loss: 160.88
 ---- batch: 080 ----
mean loss: 170.93
 ---- batch: 090 ----
mean loss: 159.74
 ---- batch: 100 ----
mean loss: 164.26
 ---- batch: 110 ----
mean loss: 160.62
train mean loss: 164.78
epoch train time: 0:00:01.904340
elapsed time: 0:03:48.770726
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 23:25:22.545853
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 169.04
 ---- batch: 020 ----
mean loss: 161.20
 ---- batch: 030 ----
mean loss: 164.12
 ---- batch: 040 ----
mean loss: 161.68
 ---- batch: 050 ----
mean loss: 157.01
 ---- batch: 060 ----
mean loss: 172.79
 ---- batch: 070 ----
mean loss: 156.62
 ---- batch: 080 ----
mean loss: 165.37
 ---- batch: 090 ----
mean loss: 163.29
 ---- batch: 100 ----
mean loss: 164.16
 ---- batch: 110 ----
mean loss: 171.14
train mean loss: 164.01
epoch train time: 0:00:01.898431
elapsed time: 0:03:50.669760
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 23:25:24.445000
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.62
 ---- batch: 020 ----
mean loss: 164.94
 ---- batch: 030 ----
mean loss: 166.67
 ---- batch: 040 ----
mean loss: 162.76
 ---- batch: 050 ----
mean loss: 164.41
 ---- batch: 060 ----
mean loss: 161.76
 ---- batch: 070 ----
mean loss: 160.81
 ---- batch: 080 ----
mean loss: 157.55
 ---- batch: 090 ----
mean loss: 162.99
 ---- batch: 100 ----
mean loss: 168.04
 ---- batch: 110 ----
mean loss: 169.48
train mean loss: 163.07
epoch train time: 0:00:01.868354
elapsed time: 0:03:52.538792
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 23:25:26.313914
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 167.92
 ---- batch: 020 ----
mean loss: 156.74
 ---- batch: 030 ----
mean loss: 162.64
 ---- batch: 040 ----
mean loss: 157.54
 ---- batch: 050 ----
mean loss: 156.86
 ---- batch: 060 ----
mean loss: 166.90
 ---- batch: 070 ----
mean loss: 163.68
 ---- batch: 080 ----
mean loss: 162.87
 ---- batch: 090 ----
mean loss: 161.77
 ---- batch: 100 ----
mean loss: 170.67
 ---- batch: 110 ----
mean loss: 161.14
train mean loss: 162.79
epoch train time: 0:00:01.886399
elapsed time: 0:03:54.425992
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 23:25:28.200917
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 156.27
 ---- batch: 020 ----
mean loss: 170.03
 ---- batch: 030 ----
mean loss: 154.68
 ---- batch: 040 ----
mean loss: 155.69
 ---- batch: 050 ----
mean loss: 164.86
 ---- batch: 060 ----
mean loss: 159.05
 ---- batch: 070 ----
mean loss: 156.42
 ---- batch: 080 ----
mean loss: 168.13
 ---- batch: 090 ----
mean loss: 170.23
 ---- batch: 100 ----
mean loss: 157.79
 ---- batch: 110 ----
mean loss: 165.42
train mean loss: 161.80
epoch train time: 0:00:01.881241
elapsed time: 0:03:56.307656
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 23:25:30.082768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 158.25
 ---- batch: 020 ----
mean loss: 165.40
 ---- batch: 030 ----
mean loss: 160.62
 ---- batch: 040 ----
mean loss: 151.74
 ---- batch: 050 ----
mean loss: 164.09
 ---- batch: 060 ----
mean loss: 153.09
 ---- batch: 070 ----
mean loss: 169.34
 ---- batch: 080 ----
mean loss: 165.22
 ---- batch: 090 ----
mean loss: 163.84
 ---- batch: 100 ----
mean loss: 155.48
 ---- batch: 110 ----
mean loss: 164.23
train mean loss: 161.05
epoch train time: 0:00:01.864640
elapsed time: 0:03:58.172888
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 23:25:31.948026
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 160.94
 ---- batch: 020 ----
mean loss: 163.10
 ---- batch: 030 ----
mean loss: 151.33
 ---- batch: 040 ----
mean loss: 154.86
 ---- batch: 050 ----
mean loss: 160.11
 ---- batch: 060 ----
mean loss: 158.09
 ---- batch: 070 ----
mean loss: 164.43
 ---- batch: 080 ----
mean loss: 162.52
 ---- batch: 090 ----
mean loss: 161.41
 ---- batch: 100 ----
mean loss: 165.81
 ---- batch: 110 ----
mean loss: 160.27
train mean loss: 159.85
epoch train time: 0:00:01.873470
elapsed time: 0:04:00.046975
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 23:25:33.822100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.22
 ---- batch: 020 ----
mean loss: 152.02
 ---- batch: 030 ----
mean loss: 161.44
 ---- batch: 040 ----
mean loss: 153.77
 ---- batch: 050 ----
mean loss: 161.22
 ---- batch: 060 ----
mean loss: 159.17
 ---- batch: 070 ----
mean loss: 169.13
 ---- batch: 080 ----
mean loss: 159.96
 ---- batch: 090 ----
mean loss: 155.82
 ---- batch: 100 ----
mean loss: 160.37
 ---- batch: 110 ----
mean loss: 163.91
train mean loss: 158.85
epoch train time: 0:00:01.857338
elapsed time: 0:04:01.904931
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 23:25:35.680069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 162.74
 ---- batch: 020 ----
mean loss: 156.18
 ---- batch: 030 ----
mean loss: 154.82
 ---- batch: 040 ----
mean loss: 147.57
 ---- batch: 050 ----
mean loss: 161.26
 ---- batch: 060 ----
mean loss: 158.02
 ---- batch: 070 ----
mean loss: 158.14
 ---- batch: 080 ----
mean loss: 163.77
 ---- batch: 090 ----
mean loss: 153.98
 ---- batch: 100 ----
mean loss: 153.71
 ---- batch: 110 ----
mean loss: 163.15
train mean loss: 158.06
epoch train time: 0:00:01.861878
elapsed time: 0:04:03.767423
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 23:25:37.542544
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.95
 ---- batch: 020 ----
mean loss: 152.24
 ---- batch: 030 ----
mean loss: 154.83
 ---- batch: 040 ----
mean loss: 154.78
 ---- batch: 050 ----
mean loss: 155.91
 ---- batch: 060 ----
mean loss: 157.68
 ---- batch: 070 ----
mean loss: 161.09
 ---- batch: 080 ----
mean loss: 167.13
 ---- batch: 090 ----
mean loss: 166.57
 ---- batch: 100 ----
mean loss: 151.13
 ---- batch: 110 ----
mean loss: 159.75
train mean loss: 157.93
epoch train time: 0:00:01.882899
elapsed time: 0:04:05.650911
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 23:25:39.426001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.92
 ---- batch: 020 ----
mean loss: 150.77
 ---- batch: 030 ----
mean loss: 154.26
 ---- batch: 040 ----
mean loss: 150.64
 ---- batch: 050 ----
mean loss: 154.98
 ---- batch: 060 ----
mean loss: 157.17
 ---- batch: 070 ----
mean loss: 147.63
 ---- batch: 080 ----
mean loss: 162.50
 ---- batch: 090 ----
mean loss: 159.68
 ---- batch: 100 ----
mean loss: 158.39
 ---- batch: 110 ----
mean loss: 167.53
train mean loss: 156.81
epoch train time: 0:00:01.884759
elapsed time: 0:04:07.536293
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 23:25:41.311418
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.87
 ---- batch: 020 ----
mean loss: 161.81
 ---- batch: 030 ----
mean loss: 152.63
 ---- batch: 040 ----
mean loss: 154.48
 ---- batch: 050 ----
mean loss: 157.59
 ---- batch: 060 ----
mean loss: 169.73
 ---- batch: 070 ----
mean loss: 153.15
 ---- batch: 080 ----
mean loss: 151.91
 ---- batch: 090 ----
mean loss: 154.40
 ---- batch: 100 ----
mean loss: 157.84
 ---- batch: 110 ----
mean loss: 151.04
train mean loss: 156.29
epoch train time: 0:00:01.860606
elapsed time: 0:04:09.397473
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 23:25:43.172592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 157.96
 ---- batch: 020 ----
mean loss: 164.99
 ---- batch: 030 ----
mean loss: 156.73
 ---- batch: 040 ----
mean loss: 154.28
 ---- batch: 050 ----
mean loss: 160.26
 ---- batch: 060 ----
mean loss: 156.10
 ---- batch: 070 ----
mean loss: 157.54
 ---- batch: 080 ----
mean loss: 162.68
 ---- batch: 090 ----
mean loss: 153.02
 ---- batch: 100 ----
mean loss: 148.60
 ---- batch: 110 ----
mean loss: 147.11
train mean loss: 156.25
epoch train time: 0:00:01.877850
elapsed time: 0:04:11.275914
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 23:25:45.051089
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.16
 ---- batch: 020 ----
mean loss: 152.31
 ---- batch: 030 ----
mean loss: 151.29
 ---- batch: 040 ----
mean loss: 157.45
 ---- batch: 050 ----
mean loss: 153.98
 ---- batch: 060 ----
mean loss: 155.55
 ---- batch: 070 ----
mean loss: 160.21
 ---- batch: 080 ----
mean loss: 161.91
 ---- batch: 090 ----
mean loss: 152.97
 ---- batch: 100 ----
mean loss: 161.02
 ---- batch: 110 ----
mean loss: 152.12
train mean loss: 155.38
epoch train time: 0:00:01.879956
elapsed time: 0:04:13.156497
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 23:25:46.931622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.70
 ---- batch: 020 ----
mean loss: 154.58
 ---- batch: 030 ----
mean loss: 154.16
 ---- batch: 040 ----
mean loss: 149.72
 ---- batch: 050 ----
mean loss: 151.94
 ---- batch: 060 ----
mean loss: 153.84
 ---- batch: 070 ----
mean loss: 158.35
 ---- batch: 080 ----
mean loss: 160.79
 ---- batch: 090 ----
mean loss: 158.97
 ---- batch: 100 ----
mean loss: 158.30
 ---- batch: 110 ----
mean loss: 152.70
train mean loss: 155.72
epoch train time: 0:00:01.897572
elapsed time: 0:04:15.054663
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 23:25:48.829886
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.08
 ---- batch: 020 ----
mean loss: 147.35
 ---- batch: 030 ----
mean loss: 148.66
 ---- batch: 040 ----
mean loss: 149.26
 ---- batch: 050 ----
mean loss: 153.03
 ---- batch: 060 ----
mean loss: 155.03
 ---- batch: 070 ----
mean loss: 154.59
 ---- batch: 080 ----
mean loss: 155.73
 ---- batch: 090 ----
mean loss: 162.51
 ---- batch: 100 ----
mean loss: 155.29
 ---- batch: 110 ----
mean loss: 159.37
train mean loss: 153.38
epoch train time: 0:00:01.873853
elapsed time: 0:04:16.929226
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 23:25:50.704346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.06
 ---- batch: 020 ----
mean loss: 147.86
 ---- batch: 030 ----
mean loss: 150.11
 ---- batch: 040 ----
mean loss: 151.85
 ---- batch: 050 ----
mean loss: 158.35
 ---- batch: 060 ----
mean loss: 156.09
 ---- batch: 070 ----
mean loss: 154.70
 ---- batch: 080 ----
mean loss: 154.23
 ---- batch: 090 ----
mean loss: 158.18
 ---- batch: 100 ----
mean loss: 151.73
 ---- batch: 110 ----
mean loss: 156.52
train mean loss: 153.51
epoch train time: 0:00:01.853415
elapsed time: 0:04:18.783235
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 23:25:52.558329
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.03
 ---- batch: 020 ----
mean loss: 142.68
 ---- batch: 030 ----
mean loss: 155.15
 ---- batch: 040 ----
mean loss: 149.22
 ---- batch: 050 ----
mean loss: 149.66
 ---- batch: 060 ----
mean loss: 151.47
 ---- batch: 070 ----
mean loss: 158.09
 ---- batch: 080 ----
mean loss: 145.78
 ---- batch: 090 ----
mean loss: 159.30
 ---- batch: 100 ----
mean loss: 154.82
 ---- batch: 110 ----
mean loss: 156.39
train mean loss: 152.40
epoch train time: 0:00:01.881439
elapsed time: 0:04:20.665255
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 23:25:54.440354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 151.26
 ---- batch: 020 ----
mean loss: 145.39
 ---- batch: 030 ----
mean loss: 153.37
 ---- batch: 040 ----
mean loss: 151.20
 ---- batch: 050 ----
mean loss: 144.72
 ---- batch: 060 ----
mean loss: 152.96
 ---- batch: 070 ----
mean loss: 161.44
 ---- batch: 080 ----
mean loss: 160.10
 ---- batch: 090 ----
mean loss: 150.08
 ---- batch: 100 ----
mean loss: 158.93
 ---- batch: 110 ----
mean loss: 155.02
train mean loss: 153.14
epoch train time: 0:00:01.861746
elapsed time: 0:04:22.527544
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 23:25:56.302727
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.67
 ---- batch: 020 ----
mean loss: 154.42
 ---- batch: 030 ----
mean loss: 146.76
 ---- batch: 040 ----
mean loss: 151.06
 ---- batch: 050 ----
mean loss: 157.58
 ---- batch: 060 ----
mean loss: 152.46
 ---- batch: 070 ----
mean loss: 155.71
 ---- batch: 080 ----
mean loss: 152.54
 ---- batch: 090 ----
mean loss: 150.49
 ---- batch: 100 ----
mean loss: 155.77
 ---- batch: 110 ----
mean loss: 145.29
train mean loss: 152.18
epoch train time: 0:00:01.870509
elapsed time: 0:04:24.398692
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 23:25:58.173783
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.12
 ---- batch: 020 ----
mean loss: 145.03
 ---- batch: 030 ----
mean loss: 148.42
 ---- batch: 040 ----
mean loss: 154.82
 ---- batch: 050 ----
mean loss: 148.10
 ---- batch: 060 ----
mean loss: 152.47
 ---- batch: 070 ----
mean loss: 149.73
 ---- batch: 080 ----
mean loss: 157.74
 ---- batch: 090 ----
mean loss: 157.03
 ---- batch: 100 ----
mean loss: 153.69
 ---- batch: 110 ----
mean loss: 146.71
train mean loss: 151.10
epoch train time: 0:00:01.870319
elapsed time: 0:04:26.269537
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 23:26:00.044625
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 146.85
 ---- batch: 020 ----
mean loss: 139.79
 ---- batch: 030 ----
mean loss: 152.64
 ---- batch: 040 ----
mean loss: 153.62
 ---- batch: 050 ----
mean loss: 157.62
 ---- batch: 060 ----
mean loss: 157.22
 ---- batch: 070 ----
mean loss: 158.35
 ---- batch: 080 ----
mean loss: 150.02
 ---- batch: 090 ----
mean loss: 151.95
 ---- batch: 100 ----
mean loss: 146.84
 ---- batch: 110 ----
mean loss: 148.54
train mean loss: 151.24
epoch train time: 0:00:01.879405
elapsed time: 0:04:28.149551
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 23:26:01.924748
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.24
 ---- batch: 020 ----
mean loss: 142.74
 ---- batch: 030 ----
mean loss: 152.83
 ---- batch: 040 ----
mean loss: 152.63
 ---- batch: 050 ----
mean loss: 142.11
 ---- batch: 060 ----
mean loss: 148.33
 ---- batch: 070 ----
mean loss: 158.92
 ---- batch: 080 ----
mean loss: 156.87
 ---- batch: 090 ----
mean loss: 150.35
 ---- batch: 100 ----
mean loss: 151.54
 ---- batch: 110 ----
mean loss: 147.48
train mean loss: 150.29
epoch train time: 0:00:01.849933
elapsed time: 0:04:30.000109
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 23:26:03.775256
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 150.20
 ---- batch: 020 ----
mean loss: 140.94
 ---- batch: 030 ----
mean loss: 151.06
 ---- batch: 040 ----
mean loss: 148.89
 ---- batch: 050 ----
mean loss: 150.59
 ---- batch: 060 ----
mean loss: 146.03
 ---- batch: 070 ----
mean loss: 149.57
 ---- batch: 080 ----
mean loss: 152.56
 ---- batch: 090 ----
mean loss: 151.12
 ---- batch: 100 ----
mean loss: 149.79
 ---- batch: 110 ----
mean loss: 149.47
train mean loss: 149.53
epoch train time: 0:00:01.917748
elapsed time: 0:04:31.918713
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 23:26:05.693655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.96
 ---- batch: 020 ----
mean loss: 146.07
 ---- batch: 030 ----
mean loss: 146.63
 ---- batch: 040 ----
mean loss: 131.73
 ---- batch: 050 ----
mean loss: 161.79
 ---- batch: 060 ----
mean loss: 154.08
 ---- batch: 070 ----
mean loss: 149.59
 ---- batch: 080 ----
mean loss: 151.48
 ---- batch: 090 ----
mean loss: 152.77
 ---- batch: 100 ----
mean loss: 142.20
 ---- batch: 110 ----
mean loss: 151.20
train mean loss: 149.16
epoch train time: 0:00:01.878410
elapsed time: 0:04:33.797527
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 23:26:07.572622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.75
 ---- batch: 020 ----
mean loss: 136.52
 ---- batch: 030 ----
mean loss: 151.70
 ---- batch: 040 ----
mean loss: 143.41
 ---- batch: 050 ----
mean loss: 153.58
 ---- batch: 060 ----
mean loss: 153.26
 ---- batch: 070 ----
mean loss: 148.75
 ---- batch: 080 ----
mean loss: 151.40
 ---- batch: 090 ----
mean loss: 146.22
 ---- batch: 100 ----
mean loss: 159.07
 ---- batch: 110 ----
mean loss: 140.21
train mean loss: 148.71
epoch train time: 0:00:01.869402
elapsed time: 0:04:35.667456
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 23:26:09.442540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.74
 ---- batch: 020 ----
mean loss: 151.74
 ---- batch: 030 ----
mean loss: 148.51
 ---- batch: 040 ----
mean loss: 146.96
 ---- batch: 050 ----
mean loss: 149.96
 ---- batch: 060 ----
mean loss: 144.23
 ---- batch: 070 ----
mean loss: 140.71
 ---- batch: 080 ----
mean loss: 146.05
 ---- batch: 090 ----
mean loss: 149.44
 ---- batch: 100 ----
mean loss: 150.46
 ---- batch: 110 ----
mean loss: 152.55
train mean loss: 147.92
epoch train time: 0:00:01.877006
elapsed time: 0:04:37.545016
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 23:26:11.320125
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 147.91
 ---- batch: 020 ----
mean loss: 142.24
 ---- batch: 030 ----
mean loss: 148.90
 ---- batch: 040 ----
mean loss: 153.19
 ---- batch: 050 ----
mean loss: 136.46
 ---- batch: 060 ----
mean loss: 147.61
 ---- batch: 070 ----
mean loss: 153.09
 ---- batch: 080 ----
mean loss: 154.19
 ---- batch: 090 ----
mean loss: 152.25
 ---- batch: 100 ----
mean loss: 142.23
 ---- batch: 110 ----
mean loss: 149.13
train mean loss: 147.84
epoch train time: 0:00:01.871828
elapsed time: 0:04:39.417437
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 23:26:13.192565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 145.24
 ---- batch: 020 ----
mean loss: 144.30
 ---- batch: 030 ----
mean loss: 148.40
 ---- batch: 040 ----
mean loss: 151.62
 ---- batch: 050 ----
mean loss: 144.61
 ---- batch: 060 ----
mean loss: 145.76
 ---- batch: 070 ----
mean loss: 149.13
 ---- batch: 080 ----
mean loss: 143.94
 ---- batch: 090 ----
mean loss: 140.21
 ---- batch: 100 ----
mean loss: 151.74
 ---- batch: 110 ----
mean loss: 154.15
train mean loss: 147.38
epoch train time: 0:00:01.887449
elapsed time: 0:04:41.305458
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 23:26:15.080547
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.08
 ---- batch: 020 ----
mean loss: 149.77
 ---- batch: 030 ----
mean loss: 142.77
 ---- batch: 040 ----
mean loss: 149.34
 ---- batch: 050 ----
mean loss: 147.58
 ---- batch: 060 ----
mean loss: 149.20
 ---- batch: 070 ----
mean loss: 153.49
 ---- batch: 080 ----
mean loss: 142.60
 ---- batch: 090 ----
mean loss: 140.61
 ---- batch: 100 ----
mean loss: 145.84
 ---- batch: 110 ----
mean loss: 144.09
train mean loss: 146.81
epoch train time: 0:00:01.848468
elapsed time: 0:04:43.154480
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 23:26:16.929681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 152.86
 ---- batch: 020 ----
mean loss: 139.77
 ---- batch: 030 ----
mean loss: 141.04
 ---- batch: 040 ----
mean loss: 141.16
 ---- batch: 050 ----
mean loss: 146.55
 ---- batch: 060 ----
mean loss: 139.03
 ---- batch: 070 ----
mean loss: 146.32
 ---- batch: 080 ----
mean loss: 143.88
 ---- batch: 090 ----
mean loss: 153.58
 ---- batch: 100 ----
mean loss: 139.66
 ---- batch: 110 ----
mean loss: 154.09
train mean loss: 146.10
epoch train time: 0:00:01.861406
elapsed time: 0:04:45.016535
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 23:26:18.791629
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.63
 ---- batch: 020 ----
mean loss: 148.57
 ---- batch: 030 ----
mean loss: 150.70
 ---- batch: 040 ----
mean loss: 146.21
 ---- batch: 050 ----
mean loss: 144.99
 ---- batch: 060 ----
mean loss: 145.34
 ---- batch: 070 ----
mean loss: 142.62
 ---- batch: 080 ----
mean loss: 143.80
 ---- batch: 090 ----
mean loss: 145.20
 ---- batch: 100 ----
mean loss: 148.15
 ---- batch: 110 ----
mean loss: 144.96
train mean loss: 145.68
epoch train time: 0:00:01.897115
elapsed time: 0:04:46.914253
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 23:26:20.689386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.86
 ---- batch: 020 ----
mean loss: 147.93
 ---- batch: 030 ----
mean loss: 146.52
 ---- batch: 040 ----
mean loss: 148.36
 ---- batch: 050 ----
mean loss: 142.03
 ---- batch: 060 ----
mean loss: 144.86
 ---- batch: 070 ----
mean loss: 145.45
 ---- batch: 080 ----
mean loss: 143.67
 ---- batch: 090 ----
mean loss: 146.34
 ---- batch: 100 ----
mean loss: 140.00
 ---- batch: 110 ----
mean loss: 146.96
train mean loss: 144.96
epoch train time: 0:00:01.901302
elapsed time: 0:04:48.816135
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 23:26:22.591286
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 154.13
 ---- batch: 020 ----
mean loss: 139.28
 ---- batch: 030 ----
mean loss: 145.91
 ---- batch: 040 ----
mean loss: 139.69
 ---- batch: 050 ----
mean loss: 144.37
 ---- batch: 060 ----
mean loss: 144.01
 ---- batch: 070 ----
mean loss: 148.23
 ---- batch: 080 ----
mean loss: 147.25
 ---- batch: 090 ----
mean loss: 147.45
 ---- batch: 100 ----
mean loss: 139.81
 ---- batch: 110 ----
mean loss: 143.98
train mean loss: 145.17
epoch train time: 0:00:01.915606
elapsed time: 0:04:50.732360
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 23:26:24.507450
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.24
 ---- batch: 020 ----
mean loss: 143.73
 ---- batch: 030 ----
mean loss: 139.44
 ---- batch: 040 ----
mean loss: 145.95
 ---- batch: 050 ----
mean loss: 141.93
 ---- batch: 060 ----
mean loss: 145.34
 ---- batch: 070 ----
mean loss: 135.76
 ---- batch: 080 ----
mean loss: 145.61
 ---- batch: 090 ----
mean loss: 137.12
 ---- batch: 100 ----
mean loss: 158.52
 ---- batch: 110 ----
mean loss: 154.98
train mean loss: 144.05
epoch train time: 0:00:01.913265
elapsed time: 0:04:52.646210
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 23:26:26.421386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 153.86
 ---- batch: 020 ----
mean loss: 146.02
 ---- batch: 030 ----
mean loss: 135.22
 ---- batch: 040 ----
mean loss: 148.35
 ---- batch: 050 ----
mean loss: 139.47
 ---- batch: 060 ----
mean loss: 146.93
 ---- batch: 070 ----
mean loss: 138.12
 ---- batch: 080 ----
mean loss: 148.08
 ---- batch: 090 ----
mean loss: 140.16
 ---- batch: 100 ----
mean loss: 149.50
 ---- batch: 110 ----
mean loss: 142.09
train mean loss: 143.92
epoch train time: 0:00:01.929679
elapsed time: 0:04:54.576526
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 23:26:28.351655
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.80
 ---- batch: 020 ----
mean loss: 143.43
 ---- batch: 030 ----
mean loss: 144.78
 ---- batch: 040 ----
mean loss: 153.41
 ---- batch: 050 ----
mean loss: 149.90
 ---- batch: 060 ----
mean loss: 151.28
 ---- batch: 070 ----
mean loss: 143.10
 ---- batch: 080 ----
mean loss: 138.39
 ---- batch: 090 ----
mean loss: 142.06
 ---- batch: 100 ----
mean loss: 132.37
 ---- batch: 110 ----
mean loss: 149.46
train mean loss: 143.95
epoch train time: 0:00:01.954600
elapsed time: 0:04:56.531710
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 23:26:30.306837
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.77
 ---- batch: 020 ----
mean loss: 136.61
 ---- batch: 030 ----
mean loss: 146.21
 ---- batch: 040 ----
mean loss: 146.50
 ---- batch: 050 ----
mean loss: 145.74
 ---- batch: 060 ----
mean loss: 139.13
 ---- batch: 070 ----
mean loss: 138.83
 ---- batch: 080 ----
mean loss: 144.84
 ---- batch: 090 ----
mean loss: 156.39
 ---- batch: 100 ----
mean loss: 143.46
 ---- batch: 110 ----
mean loss: 140.46
train mean loss: 143.32
epoch train time: 0:00:01.926221
elapsed time: 0:04:58.458503
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 23:26:32.233637
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 149.31
 ---- batch: 020 ----
mean loss: 145.37
 ---- batch: 030 ----
mean loss: 138.58
 ---- batch: 040 ----
mean loss: 138.65
 ---- batch: 050 ----
mean loss: 144.47
 ---- batch: 060 ----
mean loss: 144.96
 ---- batch: 070 ----
mean loss: 137.27
 ---- batch: 080 ----
mean loss: 143.67
 ---- batch: 090 ----
mean loss: 144.04
 ---- batch: 100 ----
mean loss: 140.22
 ---- batch: 110 ----
mean loss: 143.61
train mean loss: 142.48
epoch train time: 0:00:01.921026
elapsed time: 0:05:00.380130
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 23:26:34.155313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.70
 ---- batch: 020 ----
mean loss: 143.87
 ---- batch: 030 ----
mean loss: 139.10
 ---- batch: 040 ----
mean loss: 140.27
 ---- batch: 050 ----
mean loss: 137.83
 ---- batch: 060 ----
mean loss: 140.80
 ---- batch: 070 ----
mean loss: 143.65
 ---- batch: 080 ----
mean loss: 143.50
 ---- batch: 090 ----
mean loss: 136.06
 ---- batch: 100 ----
mean loss: 152.85
 ---- batch: 110 ----
mean loss: 151.04
train mean loss: 142.51
epoch train time: 0:00:01.894576
elapsed time: 0:05:02.275379
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 23:26:36.050567
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 143.60
 ---- batch: 020 ----
mean loss: 135.61
 ---- batch: 030 ----
mean loss: 133.46
 ---- batch: 040 ----
mean loss: 144.09
 ---- batch: 050 ----
mean loss: 146.52
 ---- batch: 060 ----
mean loss: 137.54
 ---- batch: 070 ----
mean loss: 146.54
 ---- batch: 080 ----
mean loss: 144.65
 ---- batch: 090 ----
mean loss: 150.34
 ---- batch: 100 ----
mean loss: 143.06
 ---- batch: 110 ----
mean loss: 135.37
train mean loss: 141.96
epoch train time: 0:00:01.959269
elapsed time: 0:05:04.235320
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 23:26:38.010441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.77
 ---- batch: 020 ----
mean loss: 137.17
 ---- batch: 030 ----
mean loss: 146.06
 ---- batch: 040 ----
mean loss: 144.36
 ---- batch: 050 ----
mean loss: 138.08
 ---- batch: 060 ----
mean loss: 144.72
 ---- batch: 070 ----
mean loss: 137.13
 ---- batch: 080 ----
mean loss: 141.73
 ---- batch: 090 ----
mean loss: 140.02
 ---- batch: 100 ----
mean loss: 144.19
 ---- batch: 110 ----
mean loss: 142.61
train mean loss: 141.43
epoch train time: 0:00:01.893786
elapsed time: 0:05:06.129726
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 23:26:39.904866
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.74
 ---- batch: 020 ----
mean loss: 143.95
 ---- batch: 030 ----
mean loss: 141.20
 ---- batch: 040 ----
mean loss: 139.50
 ---- batch: 050 ----
mean loss: 145.99
 ---- batch: 060 ----
mean loss: 143.65
 ---- batch: 070 ----
mean loss: 144.08
 ---- batch: 080 ----
mean loss: 143.26
 ---- batch: 090 ----
mean loss: 141.16
 ---- batch: 100 ----
mean loss: 140.80
 ---- batch: 110 ----
mean loss: 139.02
train mean loss: 141.09
epoch train time: 0:00:01.878827
elapsed time: 0:05:08.009172
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 23:26:41.784283
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.27
 ---- batch: 020 ----
mean loss: 138.41
 ---- batch: 030 ----
mean loss: 132.92
 ---- batch: 040 ----
mean loss: 138.34
 ---- batch: 050 ----
mean loss: 141.39
 ---- batch: 060 ----
mean loss: 143.39
 ---- batch: 070 ----
mean loss: 142.33
 ---- batch: 080 ----
mean loss: 139.47
 ---- batch: 090 ----
mean loss: 140.45
 ---- batch: 100 ----
mean loss: 138.57
 ---- batch: 110 ----
mean loss: 142.48
train mean loss: 140.03
epoch train time: 0:00:01.864278
elapsed time: 0:05:09.874001
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 23:26:43.649178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.11
 ---- batch: 020 ----
mean loss: 142.10
 ---- batch: 030 ----
mean loss: 135.89
 ---- batch: 040 ----
mean loss: 137.68
 ---- batch: 050 ----
mean loss: 140.31
 ---- batch: 060 ----
mean loss: 137.48
 ---- batch: 070 ----
mean loss: 140.87
 ---- batch: 080 ----
mean loss: 139.31
 ---- batch: 090 ----
mean loss: 149.14
 ---- batch: 100 ----
mean loss: 147.01
 ---- batch: 110 ----
mean loss: 134.82
train mean loss: 140.30
epoch train time: 0:00:01.886990
elapsed time: 0:05:11.761637
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 23:26:45.536740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.35
 ---- batch: 020 ----
mean loss: 140.19
 ---- batch: 030 ----
mean loss: 139.20
 ---- batch: 040 ----
mean loss: 134.17
 ---- batch: 050 ----
mean loss: 137.07
 ---- batch: 060 ----
mean loss: 134.07
 ---- batch: 070 ----
mean loss: 144.10
 ---- batch: 080 ----
mean loss: 149.12
 ---- batch: 090 ----
mean loss: 140.79
 ---- batch: 100 ----
mean loss: 141.52
 ---- batch: 110 ----
mean loss: 139.06
train mean loss: 139.41
epoch train time: 0:00:01.880449
elapsed time: 0:05:13.643035
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 23:26:47.417826
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 144.67
 ---- batch: 020 ----
mean loss: 132.99
 ---- batch: 030 ----
mean loss: 127.97
 ---- batch: 040 ----
mean loss: 128.25
 ---- batch: 050 ----
mean loss: 135.92
 ---- batch: 060 ----
mean loss: 140.89
 ---- batch: 070 ----
mean loss: 144.23
 ---- batch: 080 ----
mean loss: 145.42
 ---- batch: 090 ----
mean loss: 145.33
 ---- batch: 100 ----
mean loss: 144.18
 ---- batch: 110 ----
mean loss: 141.80
train mean loss: 139.41
epoch train time: 0:00:01.855389
elapsed time: 0:05:15.498653
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 23:26:49.273760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.37
 ---- batch: 020 ----
mean loss: 130.49
 ---- batch: 030 ----
mean loss: 139.16
 ---- batch: 040 ----
mean loss: 135.91
 ---- batch: 050 ----
mean loss: 143.16
 ---- batch: 060 ----
mean loss: 143.25
 ---- batch: 070 ----
mean loss: 135.61
 ---- batch: 080 ----
mean loss: 144.85
 ---- batch: 090 ----
mean loss: 144.23
 ---- batch: 100 ----
mean loss: 139.32
 ---- batch: 110 ----
mean loss: 136.78
train mean loss: 139.30
epoch train time: 0:00:01.891944
elapsed time: 0:05:17.391149
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 23:26:51.165976
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.72
 ---- batch: 020 ----
mean loss: 133.20
 ---- batch: 030 ----
mean loss: 140.43
 ---- batch: 040 ----
mean loss: 137.15
 ---- batch: 050 ----
mean loss: 132.23
 ---- batch: 060 ----
mean loss: 141.37
 ---- batch: 070 ----
mean loss: 142.74
 ---- batch: 080 ----
mean loss: 141.11
 ---- batch: 090 ----
mean loss: 143.07
 ---- batch: 100 ----
mean loss: 145.60
 ---- batch: 110 ----
mean loss: 137.70
train mean loss: 139.03
epoch train time: 0:00:01.894240
elapsed time: 0:05:19.285677
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 23:26:53.060784
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.81
 ---- batch: 020 ----
mean loss: 132.85
 ---- batch: 030 ----
mean loss: 131.65
 ---- batch: 040 ----
mean loss: 132.08
 ---- batch: 050 ----
mean loss: 138.46
 ---- batch: 060 ----
mean loss: 138.01
 ---- batch: 070 ----
mean loss: 134.57
 ---- batch: 080 ----
mean loss: 141.81
 ---- batch: 090 ----
mean loss: 142.42
 ---- batch: 100 ----
mean loss: 144.10
 ---- batch: 110 ----
mean loss: 133.79
train mean loss: 137.48
epoch train time: 0:00:01.863337
elapsed time: 0:05:21.149582
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 23:26:54.924874
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.74
 ---- batch: 020 ----
mean loss: 128.46
 ---- batch: 030 ----
mean loss: 136.67
 ---- batch: 040 ----
mean loss: 135.96
 ---- batch: 050 ----
mean loss: 143.38
 ---- batch: 060 ----
mean loss: 145.29
 ---- batch: 070 ----
mean loss: 147.54
 ---- batch: 080 ----
mean loss: 138.23
 ---- batch: 090 ----
mean loss: 138.08
 ---- batch: 100 ----
mean loss: 133.96
 ---- batch: 110 ----
mean loss: 135.52
train mean loss: 138.46
epoch train time: 0:00:01.902960
elapsed time: 0:05:23.053323
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 23:26:56.828524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.09
 ---- batch: 020 ----
mean loss: 134.71
 ---- batch: 030 ----
mean loss: 134.68
 ---- batch: 040 ----
mean loss: 134.67
 ---- batch: 050 ----
mean loss: 137.45
 ---- batch: 060 ----
mean loss: 138.52
 ---- batch: 070 ----
mean loss: 138.48
 ---- batch: 080 ----
mean loss: 133.91
 ---- batch: 090 ----
mean loss: 144.04
 ---- batch: 100 ----
mean loss: 133.24
 ---- batch: 110 ----
mean loss: 144.31
train mean loss: 137.01
epoch train time: 0:00:01.887193
elapsed time: 0:05:24.941206
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 23:26:58.716311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 136.79
 ---- batch: 020 ----
mean loss: 131.78
 ---- batch: 030 ----
mean loss: 139.73
 ---- batch: 040 ----
mean loss: 139.93
 ---- batch: 050 ----
mean loss: 134.48
 ---- batch: 060 ----
mean loss: 131.94
 ---- batch: 070 ----
mean loss: 144.33
 ---- batch: 080 ----
mean loss: 136.89
 ---- batch: 090 ----
mean loss: 140.08
 ---- batch: 100 ----
mean loss: 137.60
 ---- batch: 110 ----
mean loss: 141.92
train mean loss: 137.82
epoch train time: 0:00:01.858168
elapsed time: 0:05:26.799939
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 23:27:00.575082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 133.45
 ---- batch: 020 ----
mean loss: 129.59
 ---- batch: 030 ----
mean loss: 144.98
 ---- batch: 040 ----
mean loss: 143.77
 ---- batch: 050 ----
mean loss: 138.76
 ---- batch: 060 ----
mean loss: 134.78
 ---- batch: 070 ----
mean loss: 135.86
 ---- batch: 080 ----
mean loss: 138.78
 ---- batch: 090 ----
mean loss: 134.14
 ---- batch: 100 ----
mean loss: 140.08
 ---- batch: 110 ----
mean loss: 137.46
train mean loss: 137.30
epoch train time: 0:00:01.895617
elapsed time: 0:05:28.696155
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 23:27:02.471265
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.85
 ---- batch: 020 ----
mean loss: 130.74
 ---- batch: 030 ----
mean loss: 138.21
 ---- batch: 040 ----
mean loss: 131.96
 ---- batch: 050 ----
mean loss: 129.70
 ---- batch: 060 ----
mean loss: 143.83
 ---- batch: 070 ----
mean loss: 128.00
 ---- batch: 080 ----
mean loss: 136.57
 ---- batch: 090 ----
mean loss: 147.77
 ---- batch: 100 ----
mean loss: 134.50
 ---- batch: 110 ----
mean loss: 145.79
train mean loss: 136.07
epoch train time: 0:00:01.887573
elapsed time: 0:05:30.584309
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 23:27:04.359430
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 138.44
 ---- batch: 020 ----
mean loss: 131.90
 ---- batch: 030 ----
mean loss: 136.57
 ---- batch: 040 ----
mean loss: 131.73
 ---- batch: 050 ----
mean loss: 134.05
 ---- batch: 060 ----
mean loss: 132.91
 ---- batch: 070 ----
mean loss: 140.01
 ---- batch: 080 ----
mean loss: 136.31
 ---- batch: 090 ----
mean loss: 129.11
 ---- batch: 100 ----
mean loss: 140.18
 ---- batch: 110 ----
mean loss: 141.36
train mean loss: 135.85
epoch train time: 0:00:01.888577
elapsed time: 0:05:32.473449
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 23:27:06.248610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 137.26
 ---- batch: 020 ----
mean loss: 131.95
 ---- batch: 030 ----
mean loss: 133.83
 ---- batch: 040 ----
mean loss: 142.16
 ---- batch: 050 ----
mean loss: 135.48
 ---- batch: 060 ----
mean loss: 127.33
 ---- batch: 070 ----
mean loss: 137.13
 ---- batch: 080 ----
mean loss: 134.93
 ---- batch: 090 ----
mean loss: 135.66
 ---- batch: 100 ----
mean loss: 142.47
 ---- batch: 110 ----
mean loss: 132.79
train mean loss: 135.90
epoch train time: 0:00:01.912796
elapsed time: 0:05:34.386850
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 23:27:08.161989
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 131.15
 ---- batch: 020 ----
mean loss: 134.24
 ---- batch: 030 ----
mean loss: 132.09
 ---- batch: 040 ----
mean loss: 138.92
 ---- batch: 050 ----
mean loss: 145.72
 ---- batch: 060 ----
mean loss: 134.27
 ---- batch: 070 ----
mean loss: 128.90
 ---- batch: 080 ----
mean loss: 143.19
 ---- batch: 090 ----
mean loss: 144.49
 ---- batch: 100 ----
mean loss: 130.21
 ---- batch: 110 ----
mean loss: 132.19
train mean loss: 135.91
epoch train time: 0:00:01.872249
elapsed time: 0:05:36.259702
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 23:27:10.034815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.74
 ---- batch: 020 ----
mean loss: 132.76
 ---- batch: 030 ----
mean loss: 132.77
 ---- batch: 040 ----
mean loss: 137.12
 ---- batch: 050 ----
mean loss: 144.24
 ---- batch: 060 ----
mean loss: 137.14
 ---- batch: 070 ----
mean loss: 133.26
 ---- batch: 080 ----
mean loss: 132.91
 ---- batch: 090 ----
mean loss: 134.97
 ---- batch: 100 ----
mean loss: 140.70
 ---- batch: 110 ----
mean loss: 139.12
train mean loss: 135.10
epoch train time: 0:00:01.878138
elapsed time: 0:05:38.138445
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 23:27:11.913584
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.53
 ---- batch: 020 ----
mean loss: 137.47
 ---- batch: 030 ----
mean loss: 135.14
 ---- batch: 040 ----
mean loss: 133.29
 ---- batch: 050 ----
mean loss: 141.57
 ---- batch: 060 ----
mean loss: 139.09
 ---- batch: 070 ----
mean loss: 131.92
 ---- batch: 080 ----
mean loss: 121.64
 ---- batch: 090 ----
mean loss: 131.67
 ---- batch: 100 ----
mean loss: 140.56
 ---- batch: 110 ----
mean loss: 142.02
train mean loss: 135.21
epoch train time: 0:00:01.878111
elapsed time: 0:05:40.017244
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 23:27:13.792435
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 135.84
 ---- batch: 020 ----
mean loss: 134.66
 ---- batch: 030 ----
mean loss: 138.02
 ---- batch: 040 ----
mean loss: 137.00
 ---- batch: 050 ----
mean loss: 134.79
 ---- batch: 060 ----
mean loss: 130.20
 ---- batch: 070 ----
mean loss: 130.42
 ---- batch: 080 ----
mean loss: 133.40
 ---- batch: 090 ----
mean loss: 133.37
 ---- batch: 100 ----
mean loss: 128.70
 ---- batch: 110 ----
mean loss: 136.86
train mean loss: 134.41
epoch train time: 0:00:01.889305
elapsed time: 0:05:41.907253
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 23:27:15.682378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.34
 ---- batch: 020 ----
mean loss: 136.67
 ---- batch: 030 ----
mean loss: 136.50
 ---- batch: 040 ----
mean loss: 133.54
 ---- batch: 050 ----
mean loss: 133.99
 ---- batch: 060 ----
mean loss: 135.96
 ---- batch: 070 ----
mean loss: 147.06
 ---- batch: 080 ----
mean loss: 134.39
 ---- batch: 090 ----
mean loss: 127.36
 ---- batch: 100 ----
mean loss: 135.86
 ---- batch: 110 ----
mean loss: 133.29
train mean loss: 135.03
epoch train time: 0:00:01.884862
elapsed time: 0:05:43.792717
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 23:27:17.567824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 134.56
 ---- batch: 020 ----
mean loss: 130.54
 ---- batch: 030 ----
mean loss: 134.54
 ---- batch: 040 ----
mean loss: 136.85
 ---- batch: 050 ----
mean loss: 138.37
 ---- batch: 060 ----
mean loss: 129.67
 ---- batch: 070 ----
mean loss: 131.28
 ---- batch: 080 ----
mean loss: 138.52
 ---- batch: 090 ----
mean loss: 139.40
 ---- batch: 100 ----
mean loss: 132.51
 ---- batch: 110 ----
mean loss: 122.78
train mean loss: 133.85
epoch train time: 0:00:01.855538
elapsed time: 0:05:45.648819
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 23:27:19.423990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.57
 ---- batch: 020 ----
mean loss: 132.41
 ---- batch: 030 ----
mean loss: 134.51
 ---- batch: 040 ----
mean loss: 132.14
 ---- batch: 050 ----
mean loss: 127.70
 ---- batch: 060 ----
mean loss: 134.39
 ---- batch: 070 ----
mean loss: 126.84
 ---- batch: 080 ----
mean loss: 142.61
 ---- batch: 090 ----
mean loss: 136.49
 ---- batch: 100 ----
mean loss: 146.61
 ---- batch: 110 ----
mean loss: 130.35
train mean loss: 133.69
epoch train time: 0:00:01.861254
elapsed time: 0:05:47.510686
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 23:27:21.285833
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 139.87
 ---- batch: 020 ----
mean loss: 131.60
 ---- batch: 030 ----
mean loss: 126.85
 ---- batch: 040 ----
mean loss: 128.09
 ---- batch: 050 ----
mean loss: 130.44
 ---- batch: 060 ----
mean loss: 129.08
 ---- batch: 070 ----
mean loss: 133.72
 ---- batch: 080 ----
mean loss: 128.04
 ---- batch: 090 ----
mean loss: 139.64
 ---- batch: 100 ----
mean loss: 139.80
 ---- batch: 110 ----
mean loss: 136.31
train mean loss: 132.70
epoch train time: 0:00:01.864899
elapsed time: 0:05:49.376296
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 23:27:23.151420
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.80
 ---- batch: 020 ----
mean loss: 134.41
 ---- batch: 030 ----
mean loss: 126.56
 ---- batch: 040 ----
mean loss: 137.61
 ---- batch: 050 ----
mean loss: 141.51
 ---- batch: 060 ----
mean loss: 126.07
 ---- batch: 070 ----
mean loss: 126.11
 ---- batch: 080 ----
mean loss: 134.57
 ---- batch: 090 ----
mean loss: 136.35
 ---- batch: 100 ----
mean loss: 134.25
 ---- batch: 110 ----
mean loss: 133.90
train mean loss: 132.36
epoch train time: 0:00:01.892153
elapsed time: 0:05:51.269025
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 23:27:25.044186
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.74
 ---- batch: 020 ----
mean loss: 134.24
 ---- batch: 030 ----
mean loss: 119.41
 ---- batch: 040 ----
mean loss: 135.30
 ---- batch: 050 ----
mean loss: 134.76
 ---- batch: 060 ----
mean loss: 131.20
 ---- batch: 070 ----
mean loss: 135.90
 ---- batch: 080 ----
mean loss: 139.90
 ---- batch: 090 ----
mean loss: 133.94
 ---- batch: 100 ----
mean loss: 135.27
 ---- batch: 110 ----
mean loss: 131.44
train mean loss: 132.67
epoch train time: 0:00:01.910297
elapsed time: 0:05:53.179968
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 23:27:26.955090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.61
 ---- batch: 020 ----
mean loss: 133.50
 ---- batch: 030 ----
mean loss: 132.03
 ---- batch: 040 ----
mean loss: 131.13
 ---- batch: 050 ----
mean loss: 131.83
 ---- batch: 060 ----
mean loss: 137.98
 ---- batch: 070 ----
mean loss: 133.18
 ---- batch: 080 ----
mean loss: 134.26
 ---- batch: 090 ----
mean loss: 137.18
 ---- batch: 100 ----
mean loss: 128.26
 ---- batch: 110 ----
mean loss: 126.31
train mean loss: 132.60
epoch train time: 0:00:01.878140
elapsed time: 0:05:55.058712
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 23:27:28.833869
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.01
 ---- batch: 020 ----
mean loss: 138.48
 ---- batch: 030 ----
mean loss: 136.64
 ---- batch: 040 ----
mean loss: 128.89
 ---- batch: 050 ----
mean loss: 129.31
 ---- batch: 060 ----
mean loss: 127.91
 ---- batch: 070 ----
mean loss: 142.32
 ---- batch: 080 ----
mean loss: 130.21
 ---- batch: 090 ----
mean loss: 133.44
 ---- batch: 100 ----
mean loss: 135.50
 ---- batch: 110 ----
mean loss: 130.09
train mean loss: 132.20
epoch train time: 0:00:01.873466
elapsed time: 0:05:56.932878
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 23:27:30.708004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.36
 ---- batch: 020 ----
mean loss: 134.18
 ---- batch: 030 ----
mean loss: 129.64
 ---- batch: 040 ----
mean loss: 127.37
 ---- batch: 050 ----
mean loss: 132.50
 ---- batch: 060 ----
mean loss: 130.75
 ---- batch: 070 ----
mean loss: 126.00
 ---- batch: 080 ----
mean loss: 131.05
 ---- batch: 090 ----
mean loss: 137.74
 ---- batch: 100 ----
mean loss: 132.28
 ---- batch: 110 ----
mean loss: 138.63
train mean loss: 131.29
epoch train time: 0:00:01.866386
elapsed time: 0:05:58.799865
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 23:27:32.574997
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.90
 ---- batch: 020 ----
mean loss: 122.76
 ---- batch: 030 ----
mean loss: 134.26
 ---- batch: 040 ----
mean loss: 130.34
 ---- batch: 050 ----
mean loss: 125.87
 ---- batch: 060 ----
mean loss: 129.19
 ---- batch: 070 ----
mean loss: 133.28
 ---- batch: 080 ----
mean loss: 129.46
 ---- batch: 090 ----
mean loss: 138.43
 ---- batch: 100 ----
mean loss: 124.69
 ---- batch: 110 ----
mean loss: 141.05
train mean loss: 131.36
epoch train time: 0:00:01.870893
elapsed time: 0:06:00.671696
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 23:27:34.446561
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.72
 ---- batch: 020 ----
mean loss: 129.07
 ---- batch: 030 ----
mean loss: 128.19
 ---- batch: 040 ----
mean loss: 134.30
 ---- batch: 050 ----
mean loss: 134.14
 ---- batch: 060 ----
mean loss: 127.73
 ---- batch: 070 ----
mean loss: 134.88
 ---- batch: 080 ----
mean loss: 131.98
 ---- batch: 090 ----
mean loss: 133.30
 ---- batch: 100 ----
mean loss: 134.21
 ---- batch: 110 ----
mean loss: 128.69
train mean loss: 130.76
epoch train time: 0:00:01.868630
elapsed time: 0:06:02.540633
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 23:27:36.315781
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.02
 ---- batch: 020 ----
mean loss: 126.68
 ---- batch: 030 ----
mean loss: 130.38
 ---- batch: 040 ----
mean loss: 129.59
 ---- batch: 050 ----
mean loss: 125.51
 ---- batch: 060 ----
mean loss: 131.97
 ---- batch: 070 ----
mean loss: 135.44
 ---- batch: 080 ----
mean loss: 134.60
 ---- batch: 090 ----
mean loss: 131.82
 ---- batch: 100 ----
mean loss: 132.72
 ---- batch: 110 ----
mean loss: 127.08
train mean loss: 130.76
epoch train time: 0:00:01.879235
elapsed time: 0:06:04.420489
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 23:27:38.195620
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.71
 ---- batch: 020 ----
mean loss: 130.79
 ---- batch: 030 ----
mean loss: 121.27
 ---- batch: 040 ----
mean loss: 136.12
 ---- batch: 050 ----
mean loss: 131.28
 ---- batch: 060 ----
mean loss: 131.63
 ---- batch: 070 ----
mean loss: 129.37
 ---- batch: 080 ----
mean loss: 136.23
 ---- batch: 090 ----
mean loss: 125.52
 ---- batch: 100 ----
mean loss: 132.17
 ---- batch: 110 ----
mean loss: 131.32
train mean loss: 130.06
epoch train time: 0:00:01.899269
elapsed time: 0:06:06.320350
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 23:27:40.095470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.49
 ---- batch: 020 ----
mean loss: 130.81
 ---- batch: 030 ----
mean loss: 134.84
 ---- batch: 040 ----
mean loss: 123.69
 ---- batch: 050 ----
mean loss: 129.07
 ---- batch: 060 ----
mean loss: 132.18
 ---- batch: 070 ----
mean loss: 125.48
 ---- batch: 080 ----
mean loss: 128.66
 ---- batch: 090 ----
mean loss: 133.32
 ---- batch: 100 ----
mean loss: 137.57
 ---- batch: 110 ----
mean loss: 126.47
train mean loss: 130.30
epoch train time: 0:00:01.889813
elapsed time: 0:06:08.210798
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 23:27:41.986001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.29
 ---- batch: 020 ----
mean loss: 135.05
 ---- batch: 030 ----
mean loss: 133.99
 ---- batch: 040 ----
mean loss: 123.91
 ---- batch: 050 ----
mean loss: 125.42
 ---- batch: 060 ----
mean loss: 133.85
 ---- batch: 070 ----
mean loss: 126.70
 ---- batch: 080 ----
mean loss: 128.37
 ---- batch: 090 ----
mean loss: 130.92
 ---- batch: 100 ----
mean loss: 126.48
 ---- batch: 110 ----
mean loss: 134.11
train mean loss: 129.67
epoch train time: 0:00:01.872043
elapsed time: 0:06:10.083501
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 23:27:43.858608
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.94
 ---- batch: 020 ----
mean loss: 126.58
 ---- batch: 030 ----
mean loss: 127.46
 ---- batch: 040 ----
mean loss: 131.04
 ---- batch: 050 ----
mean loss: 135.00
 ---- batch: 060 ----
mean loss: 127.05
 ---- batch: 070 ----
mean loss: 130.02
 ---- batch: 080 ----
mean loss: 127.74
 ---- batch: 090 ----
mean loss: 128.84
 ---- batch: 100 ----
mean loss: 127.39
 ---- batch: 110 ----
mean loss: 135.37
train mean loss: 129.19
epoch train time: 0:00:01.886842
elapsed time: 0:06:11.970926
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 23:27:45.746096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 122.93
 ---- batch: 020 ----
mean loss: 132.76
 ---- batch: 030 ----
mean loss: 126.80
 ---- batch: 040 ----
mean loss: 121.67
 ---- batch: 050 ----
mean loss: 134.86
 ---- batch: 060 ----
mean loss: 132.93
 ---- batch: 070 ----
mean loss: 133.74
 ---- batch: 080 ----
mean loss: 131.81
 ---- batch: 090 ----
mean loss: 125.68
 ---- batch: 100 ----
mean loss: 130.44
 ---- batch: 110 ----
mean loss: 131.12
train mean loss: 129.41
epoch train time: 0:00:01.886016
elapsed time: 0:06:13.857566
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 23:27:47.632668
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 117.48
 ---- batch: 020 ----
mean loss: 127.37
 ---- batch: 030 ----
mean loss: 139.50
 ---- batch: 040 ----
mean loss: 118.36
 ---- batch: 050 ----
mean loss: 129.25
 ---- batch: 060 ----
mean loss: 120.72
 ---- batch: 070 ----
mean loss: 124.46
 ---- batch: 080 ----
mean loss: 134.80
 ---- batch: 090 ----
mean loss: 137.58
 ---- batch: 100 ----
mean loss: 137.63
 ---- batch: 110 ----
mean loss: 133.08
train mean loss: 129.19
epoch train time: 0:00:01.895423
elapsed time: 0:06:15.753592
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 23:27:49.528666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 129.41
 ---- batch: 020 ----
mean loss: 129.01
 ---- batch: 030 ----
mean loss: 127.49
 ---- batch: 040 ----
mean loss: 133.07
 ---- batch: 050 ----
mean loss: 128.62
 ---- batch: 060 ----
mean loss: 133.27
 ---- batch: 070 ----
mean loss: 129.07
 ---- batch: 080 ----
mean loss: 124.27
 ---- batch: 090 ----
mean loss: 127.01
 ---- batch: 100 ----
mean loss: 134.19
 ---- batch: 110 ----
mean loss: 126.82
train mean loss: 129.51
epoch train time: 0:00:01.890501
elapsed time: 0:06:17.644652
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 23:27:51.419792
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 128.78
 ---- batch: 020 ----
mean loss: 128.59
 ---- batch: 030 ----
mean loss: 120.75
 ---- batch: 040 ----
mean loss: 123.88
 ---- batch: 050 ----
mean loss: 122.21
 ---- batch: 060 ----
mean loss: 122.53
 ---- batch: 070 ----
mean loss: 132.64
 ---- batch: 080 ----
mean loss: 127.59
 ---- batch: 090 ----
mean loss: 138.64
 ---- batch: 100 ----
mean loss: 130.68
 ---- batch: 110 ----
mean loss: 130.49
train mean loss: 128.10
epoch train time: 0:00:01.878529
elapsed time: 0:06:19.523807
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 23:27:53.298982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.47
 ---- batch: 020 ----
mean loss: 125.57
 ---- batch: 030 ----
mean loss: 128.15
 ---- batch: 040 ----
mean loss: 122.93
 ---- batch: 050 ----
mean loss: 129.35
 ---- batch: 060 ----
mean loss: 129.92
 ---- batch: 070 ----
mean loss: 128.35
 ---- batch: 080 ----
mean loss: 128.67
 ---- batch: 090 ----
mean loss: 121.96
 ---- batch: 100 ----
mean loss: 132.76
 ---- batch: 110 ----
mean loss: 137.91
train mean loss: 128.03
epoch train time: 0:00:01.867590
elapsed time: 0:06:21.392025
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 23:27:55.167144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.72
 ---- batch: 020 ----
mean loss: 121.43
 ---- batch: 030 ----
mean loss: 128.82
 ---- batch: 040 ----
mean loss: 123.99
 ---- batch: 050 ----
mean loss: 128.57
 ---- batch: 060 ----
mean loss: 126.22
 ---- batch: 070 ----
mean loss: 130.97
 ---- batch: 080 ----
mean loss: 122.52
 ---- batch: 090 ----
mean loss: 127.04
 ---- batch: 100 ----
mean loss: 133.36
 ---- batch: 110 ----
mean loss: 137.45
train mean loss: 127.87
epoch train time: 0:00:01.870811
elapsed time: 0:06:23.263408
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 23:27:57.038555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 130.03
 ---- batch: 020 ----
mean loss: 126.70
 ---- batch: 030 ----
mean loss: 131.57
 ---- batch: 040 ----
mean loss: 127.43
 ---- batch: 050 ----
mean loss: 126.31
 ---- batch: 060 ----
mean loss: 128.59
 ---- batch: 070 ----
mean loss: 127.20
 ---- batch: 080 ----
mean loss: 128.49
 ---- batch: 090 ----
mean loss: 126.78
 ---- batch: 100 ----
mean loss: 126.91
 ---- batch: 110 ----
mean loss: 129.60
train mean loss: 127.68
epoch train time: 0:00:01.884224
elapsed time: 0:06:25.148248
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 23:27:58.923378
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.65
 ---- batch: 020 ----
mean loss: 120.58
 ---- batch: 030 ----
mean loss: 123.16
 ---- batch: 040 ----
mean loss: 130.86
 ---- batch: 050 ----
mean loss: 132.48
 ---- batch: 060 ----
mean loss: 135.28
 ---- batch: 070 ----
mean loss: 119.09
 ---- batch: 080 ----
mean loss: 124.96
 ---- batch: 090 ----
mean loss: 131.08
 ---- batch: 100 ----
mean loss: 130.72
 ---- batch: 110 ----
mean loss: 125.72
train mean loss: 127.07
epoch train time: 0:00:01.862262
elapsed time: 0:06:27.011115
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 23:28:00.786246
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.37
 ---- batch: 020 ----
mean loss: 131.90
 ---- batch: 030 ----
mean loss: 135.14
 ---- batch: 040 ----
mean loss: 119.91
 ---- batch: 050 ----
mean loss: 127.29
 ---- batch: 060 ----
mean loss: 123.47
 ---- batch: 070 ----
mean loss: 130.49
 ---- batch: 080 ----
mean loss: 127.26
 ---- batch: 090 ----
mean loss: 127.29
 ---- batch: 100 ----
mean loss: 131.10
 ---- batch: 110 ----
mean loss: 119.69
train mean loss: 126.94
epoch train time: 0:00:01.877897
elapsed time: 0:06:28.889589
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 23:28:02.664685
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.79
 ---- batch: 020 ----
mean loss: 121.82
 ---- batch: 030 ----
mean loss: 133.88
 ---- batch: 040 ----
mean loss: 123.75
 ---- batch: 050 ----
mean loss: 122.73
 ---- batch: 060 ----
mean loss: 123.63
 ---- batch: 070 ----
mean loss: 124.38
 ---- batch: 080 ----
mean loss: 133.31
 ---- batch: 090 ----
mean loss: 131.06
 ---- batch: 100 ----
mean loss: 121.05
 ---- batch: 110 ----
mean loss: 129.13
train mean loss: 126.62
epoch train time: 0:00:01.879518
elapsed time: 0:06:30.769663
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 23:28:04.544787
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 113.44
 ---- batch: 020 ----
mean loss: 118.48
 ---- batch: 030 ----
mean loss: 129.68
 ---- batch: 040 ----
mean loss: 121.36
 ---- batch: 050 ----
mean loss: 125.99
 ---- batch: 060 ----
mean loss: 132.57
 ---- batch: 070 ----
mean loss: 139.44
 ---- batch: 080 ----
mean loss: 127.01
 ---- batch: 090 ----
mean loss: 125.14
 ---- batch: 100 ----
mean loss: 127.51
 ---- batch: 110 ----
mean loss: 130.00
train mean loss: 126.75
epoch train time: 0:00:01.907450
elapsed time: 0:06:32.677708
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 23:28:06.452815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 121.81
 ---- batch: 020 ----
mean loss: 116.99
 ---- batch: 030 ----
mean loss: 125.08
 ---- batch: 040 ----
mean loss: 128.23
 ---- batch: 050 ----
mean loss: 128.74
 ---- batch: 060 ----
mean loss: 127.44
 ---- batch: 070 ----
mean loss: 123.34
 ---- batch: 080 ----
mean loss: 122.82
 ---- batch: 090 ----
mean loss: 132.72
 ---- batch: 100 ----
mean loss: 130.41
 ---- batch: 110 ----
mean loss: 125.51
train mean loss: 125.89
epoch train time: 0:00:01.902874
elapsed time: 0:06:34.581133
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 23:28:08.356259
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 132.74
 ---- batch: 020 ----
mean loss: 125.69
 ---- batch: 030 ----
mean loss: 125.44
 ---- batch: 040 ----
mean loss: 122.69
 ---- batch: 050 ----
mean loss: 130.28
 ---- batch: 060 ----
mean loss: 131.28
 ---- batch: 070 ----
mean loss: 119.08
 ---- batch: 080 ----
mean loss: 123.60
 ---- batch: 090 ----
mean loss: 122.69
 ---- batch: 100 ----
mean loss: 124.01
 ---- batch: 110 ----
mean loss: 126.87
train mean loss: 125.83
epoch train time: 0:00:01.901011
elapsed time: 0:06:36.482714
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 23:28:10.257875
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 124.47
 ---- batch: 020 ----
mean loss: 125.48
 ---- batch: 030 ----
mean loss: 122.36
 ---- batch: 040 ----
mean loss: 129.90
 ---- batch: 050 ----
mean loss: 126.67
 ---- batch: 060 ----
mean loss: 121.12
 ---- batch: 070 ----
mean loss: 123.16
 ---- batch: 080 ----
mean loss: 120.44
 ---- batch: 090 ----
mean loss: 121.71
 ---- batch: 100 ----
mean loss: 134.07
 ---- batch: 110 ----
mean loss: 131.47
train mean loss: 125.66
epoch train time: 0:00:01.905343
elapsed time: 0:06:38.388672
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 23:28:12.163825
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 119.72
 ---- batch: 020 ----
mean loss: 131.70
 ---- batch: 030 ----
mean loss: 126.17
 ---- batch: 040 ----
mean loss: 127.01
 ---- batch: 050 ----
mean loss: 118.53
 ---- batch: 060 ----
mean loss: 118.38
 ---- batch: 070 ----
mean loss: 130.48
 ---- batch: 080 ----
mean loss: 126.54
 ---- batch: 090 ----
mean loss: 124.34
 ---- batch: 100 ----
mean loss: 129.60
 ---- batch: 110 ----
mean loss: 123.56
train mean loss: 125.09
epoch train time: 0:00:01.876768
elapsed time: 0:06:40.266059
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 23:28:14.041176
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 118.50
 ---- batch: 020 ----
mean loss: 127.12
 ---- batch: 030 ----
mean loss: 130.94
 ---- batch: 040 ----
mean loss: 121.37
 ---- batch: 050 ----
mean loss: 119.02
 ---- batch: 060 ----
mean loss: 124.30
 ---- batch: 070 ----
mean loss: 130.21
 ---- batch: 080 ----
mean loss: 123.43
 ---- batch: 090 ----
mean loss: 132.43
 ---- batch: 100 ----
mean loss: 124.08
 ---- batch: 110 ----
mean loss: 126.71
train mean loss: 125.61
epoch train time: 0:00:01.914536
elapsed time: 0:06:42.181181
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 23:28:15.956324
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.39
 ---- batch: 020 ----
mean loss: 120.15
 ---- batch: 030 ----
mean loss: 121.50
 ---- batch: 040 ----
mean loss: 130.25
 ---- batch: 050 ----
mean loss: 122.36
 ---- batch: 060 ----
mean loss: 125.43
 ---- batch: 070 ----
mean loss: 131.82
 ---- batch: 080 ----
mean loss: 124.89
 ---- batch: 090 ----
mean loss: 125.12
 ---- batch: 100 ----
mean loss: 116.27
 ---- batch: 110 ----
mean loss: 132.98
train mean loss: 125.35
epoch train time: 0:00:01.883643
elapsed time: 0:06:44.065426
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 23:28:17.840610
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.11
 ---- batch: 020 ----
mean loss: 118.75
 ---- batch: 030 ----
mean loss: 125.68
 ---- batch: 040 ----
mean loss: 116.31
 ---- batch: 050 ----
mean loss: 125.79
 ---- batch: 060 ----
mean loss: 116.89
 ---- batch: 070 ----
mean loss: 133.71
 ---- batch: 080 ----
mean loss: 131.55
 ---- batch: 090 ----
mean loss: 126.26
 ---- batch: 100 ----
mean loss: 126.13
 ---- batch: 110 ----
mean loss: 122.76
train mean loss: 124.19
epoch train time: 0:00:01.891910
elapsed time: 0:06:45.958037
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 23:28:19.733141
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 126.76
 ---- batch: 020 ----
mean loss: 120.15
 ---- batch: 030 ----
mean loss: 126.79
 ---- batch: 040 ----
mean loss: 119.47
 ---- batch: 050 ----
mean loss: 123.47
 ---- batch: 060 ----
mean loss: 130.25
 ---- batch: 070 ----
mean loss: 116.43
 ---- batch: 080 ----
mean loss: 127.07
 ---- batch: 090 ----
mean loss: 123.25
 ---- batch: 100 ----
mean loss: 125.27
 ---- batch: 110 ----
mean loss: 126.74
train mean loss: 124.12
epoch train time: 0:00:01.877170
elapsed time: 0:06:47.835779
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 23:28:21.610921
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 123.16
 ---- batch: 020 ----
mean loss: 119.31
 ---- batch: 030 ----
mean loss: 131.42
 ---- batch: 040 ----
mean loss: 115.16
 ---- batch: 050 ----
mean loss: 118.38
 ---- batch: 060 ----
mean loss: 137.05
 ---- batch: 070 ----
mean loss: 122.60
 ---- batch: 080 ----
mean loss: 124.99
 ---- batch: 090 ----
mean loss: 120.02
 ---- batch: 100 ----
mean loss: 119.68
 ---- batch: 110 ----
mean loss: 129.88
train mean loss: 124.16
epoch train time: 0:00:01.911810
elapsed time: 0:06:49.748200
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 23:28:23.523326
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 125.45
 ---- batch: 020 ----
mean loss: 117.97
 ---- batch: 030 ----
mean loss: 121.39
 ---- batch: 040 ----
mean loss: 121.67
 ---- batch: 050 ----
mean loss: 124.50
 ---- batch: 060 ----
mean loss: 125.05
 ---- batch: 070 ----
mean loss: 123.90
 ---- batch: 080 ----
mean loss: 118.41
 ---- batch: 090 ----
mean loss: 129.86
 ---- batch: 100 ----
mean loss: 127.10
 ---- batch: 110 ----
mean loss: 131.52
train mean loss: 124.05
epoch train time: 0:00:01.873320
elapsed time: 0:06:51.622109
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 23:28:25.397216
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.53
 ---- batch: 020 ----
mean loss: 119.86
 ---- batch: 030 ----
mean loss: 125.67
 ---- batch: 040 ----
mean loss: 115.93
 ---- batch: 050 ----
mean loss: 113.46
 ---- batch: 060 ----
mean loss: 116.56
 ---- batch: 070 ----
mean loss: 112.81
 ---- batch: 080 ----
mean loss: 120.07
 ---- batch: 090 ----
mean loss: 125.18
 ---- batch: 100 ----
mean loss: 116.95
 ---- batch: 110 ----
mean loss: 119.69
train mean loss: 118.38
epoch train time: 0:00:01.869565
elapsed time: 0:06:53.492540
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 23:28:27.267331
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.92
 ---- batch: 020 ----
mean loss: 116.02
 ---- batch: 030 ----
mean loss: 110.10
 ---- batch: 040 ----
mean loss: 121.98
 ---- batch: 050 ----
mean loss: 116.72
 ---- batch: 060 ----
mean loss: 119.53
 ---- batch: 070 ----
mean loss: 110.52
 ---- batch: 080 ----
mean loss: 125.11
 ---- batch: 090 ----
mean loss: 122.50
 ---- batch: 100 ----
mean loss: 116.33
 ---- batch: 110 ----
mean loss: 115.51
train mean loss: 117.84
epoch train time: 0:00:01.908324
elapsed time: 0:06:55.401175
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 23:28:29.176296
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.02
 ---- batch: 020 ----
mean loss: 118.70
 ---- batch: 030 ----
mean loss: 123.26
 ---- batch: 040 ----
mean loss: 114.76
 ---- batch: 050 ----
mean loss: 114.91
 ---- batch: 060 ----
mean loss: 118.21
 ---- batch: 070 ----
mean loss: 113.48
 ---- batch: 080 ----
mean loss: 124.31
 ---- batch: 090 ----
mean loss: 118.61
 ---- batch: 100 ----
mean loss: 118.40
 ---- batch: 110 ----
mean loss: 117.78
train mean loss: 117.65
epoch train time: 0:00:01.903085
elapsed time: 0:06:57.304816
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 23:28:31.079644
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.57
 ---- batch: 020 ----
mean loss: 113.28
 ---- batch: 030 ----
mean loss: 115.04
 ---- batch: 040 ----
mean loss: 114.62
 ---- batch: 050 ----
mean loss: 122.38
 ---- batch: 060 ----
mean loss: 116.43
 ---- batch: 070 ----
mean loss: 123.25
 ---- batch: 080 ----
mean loss: 118.21
 ---- batch: 090 ----
mean loss: 122.93
 ---- batch: 100 ----
mean loss: 111.10
 ---- batch: 110 ----
mean loss: 116.68
train mean loss: 117.47
epoch train time: 0:00:01.870480
elapsed time: 0:06:59.175581
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 23:28:32.950734
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.72
 ---- batch: 020 ----
mean loss: 116.31
 ---- batch: 030 ----
mean loss: 116.89
 ---- batch: 040 ----
mean loss: 115.18
 ---- batch: 050 ----
mean loss: 114.24
 ---- batch: 060 ----
mean loss: 120.01
 ---- batch: 070 ----
mean loss: 118.73
 ---- batch: 080 ----
mean loss: 125.20
 ---- batch: 090 ----
mean loss: 108.03
 ---- batch: 100 ----
mean loss: 112.17
 ---- batch: 110 ----
mean loss: 125.57
train mean loss: 117.52
epoch train time: 0:00:01.917839
elapsed time: 0:07:01.094043
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 23:28:34.869169
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.58
 ---- batch: 020 ----
mean loss: 111.87
 ---- batch: 030 ----
mean loss: 116.96
 ---- batch: 040 ----
mean loss: 120.40
 ---- batch: 050 ----
mean loss: 112.38
 ---- batch: 060 ----
mean loss: 120.66
 ---- batch: 070 ----
mean loss: 118.79
 ---- batch: 080 ----
mean loss: 119.38
 ---- batch: 090 ----
mean loss: 116.36
 ---- batch: 100 ----
mean loss: 111.52
 ---- batch: 110 ----
mean loss: 120.56
train mean loss: 117.45
epoch train time: 0:00:01.867174
elapsed time: 0:07:02.961819
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 23:28:36.736934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.69
 ---- batch: 020 ----
mean loss: 115.93
 ---- batch: 030 ----
mean loss: 116.74
 ---- batch: 040 ----
mean loss: 124.71
 ---- batch: 050 ----
mean loss: 114.49
 ---- batch: 060 ----
mean loss: 121.39
 ---- batch: 070 ----
mean loss: 109.15
 ---- batch: 080 ----
mean loss: 120.24
 ---- batch: 090 ----
mean loss: 113.58
 ---- batch: 100 ----
mean loss: 124.09
 ---- batch: 110 ----
mean loss: 114.58
train mean loss: 117.34
epoch train time: 0:00:01.857266
elapsed time: 0:07:04.819654
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 23:28:38.594776
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.05
 ---- batch: 020 ----
mean loss: 116.35
 ---- batch: 030 ----
mean loss: 112.33
 ---- batch: 040 ----
mean loss: 116.62
 ---- batch: 050 ----
mean loss: 122.73
 ---- batch: 060 ----
mean loss: 120.33
 ---- batch: 070 ----
mean loss: 118.95
 ---- batch: 080 ----
mean loss: 118.58
 ---- batch: 090 ----
mean loss: 120.08
 ---- batch: 100 ----
mean loss: 111.94
 ---- batch: 110 ----
mean loss: 117.14
train mean loss: 117.33
epoch train time: 0:00:01.896588
elapsed time: 0:07:06.716838
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 23:28:40.491935
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.64
 ---- batch: 020 ----
mean loss: 107.38
 ---- batch: 030 ----
mean loss: 122.77
 ---- batch: 040 ----
mean loss: 116.63
 ---- batch: 050 ----
mean loss: 113.00
 ---- batch: 060 ----
mean loss: 120.11
 ---- batch: 070 ----
mean loss: 121.26
 ---- batch: 080 ----
mean loss: 112.34
 ---- batch: 090 ----
mean loss: 120.85
 ---- batch: 100 ----
mean loss: 114.26
 ---- batch: 110 ----
mean loss: 123.84
train mean loss: 117.24
epoch train time: 0:00:01.896781
elapsed time: 0:07:08.614200
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 23:28:42.389365
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 124.73
 ---- batch: 020 ----
mean loss: 115.78
 ---- batch: 030 ----
mean loss: 115.97
 ---- batch: 040 ----
mean loss: 121.15
 ---- batch: 050 ----
mean loss: 116.13
 ---- batch: 060 ----
mean loss: 116.61
 ---- batch: 070 ----
mean loss: 112.51
 ---- batch: 080 ----
mean loss: 117.34
 ---- batch: 090 ----
mean loss: 114.24
 ---- batch: 100 ----
mean loss: 115.08
 ---- batch: 110 ----
mean loss: 113.83
train mean loss: 117.26
epoch train time: 0:00:01.878160
elapsed time: 0:07:10.492968
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 23:28:44.268102
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.31
 ---- batch: 020 ----
mean loss: 118.87
 ---- batch: 030 ----
mean loss: 118.58
 ---- batch: 040 ----
mean loss: 114.75
 ---- batch: 050 ----
mean loss: 120.62
 ---- batch: 060 ----
mean loss: 114.91
 ---- batch: 070 ----
mean loss: 112.38
 ---- batch: 080 ----
mean loss: 114.85
 ---- batch: 090 ----
mean loss: 122.05
 ---- batch: 100 ----
mean loss: 113.21
 ---- batch: 110 ----
mean loss: 120.80
train mean loss: 117.19
epoch train time: 0:00:01.902329
elapsed time: 0:07:12.395881
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 23:28:46.170979
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.66
 ---- batch: 020 ----
mean loss: 112.33
 ---- batch: 030 ----
mean loss: 123.48
 ---- batch: 040 ----
mean loss: 119.85
 ---- batch: 050 ----
mean loss: 116.45
 ---- batch: 060 ----
mean loss: 116.15
 ---- batch: 070 ----
mean loss: 118.66
 ---- batch: 080 ----
mean loss: 116.66
 ---- batch: 090 ----
mean loss: 117.09
 ---- batch: 100 ----
mean loss: 122.02
 ---- batch: 110 ----
mean loss: 111.87
train mean loss: 117.18
epoch train time: 0:00:01.844179
elapsed time: 0:07:14.240601
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 23:28:48.015714
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.13
 ---- batch: 020 ----
mean loss: 114.98
 ---- batch: 030 ----
mean loss: 107.01
 ---- batch: 040 ----
mean loss: 126.69
 ---- batch: 050 ----
mean loss: 118.65
 ---- batch: 060 ----
mean loss: 121.99
 ---- batch: 070 ----
mean loss: 114.74
 ---- batch: 080 ----
mean loss: 116.47
 ---- batch: 090 ----
mean loss: 109.85
 ---- batch: 100 ----
mean loss: 120.49
 ---- batch: 110 ----
mean loss: 117.94
train mean loss: 117.02
epoch train time: 0:00:01.860552
elapsed time: 0:07:16.101812
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 23:28:49.876928
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.87
 ---- batch: 020 ----
mean loss: 109.15
 ---- batch: 030 ----
mean loss: 115.70
 ---- batch: 040 ----
mean loss: 117.27
 ---- batch: 050 ----
mean loss: 114.75
 ---- batch: 060 ----
mean loss: 114.20
 ---- batch: 070 ----
mean loss: 121.98
 ---- batch: 080 ----
mean loss: 118.44
 ---- batch: 090 ----
mean loss: 119.69
 ---- batch: 100 ----
mean loss: 119.46
 ---- batch: 110 ----
mean loss: 121.94
train mean loss: 117.06
epoch train time: 0:00:01.861464
elapsed time: 0:07:17.963851
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 23:28:51.739000
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.30
 ---- batch: 020 ----
mean loss: 114.93
 ---- batch: 030 ----
mean loss: 122.63
 ---- batch: 040 ----
mean loss: 122.48
 ---- batch: 050 ----
mean loss: 110.66
 ---- batch: 060 ----
mean loss: 114.46
 ---- batch: 070 ----
mean loss: 120.25
 ---- batch: 080 ----
mean loss: 113.22
 ---- batch: 090 ----
mean loss: 107.60
 ---- batch: 100 ----
mean loss: 117.21
 ---- batch: 110 ----
mean loss: 120.37
train mean loss: 117.11
epoch train time: 0:00:01.890235
elapsed time: 0:07:19.854759
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 23:28:53.629898
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.25
 ---- batch: 020 ----
mean loss: 118.18
 ---- batch: 030 ----
mean loss: 118.56
 ---- batch: 040 ----
mean loss: 119.87
 ---- batch: 050 ----
mean loss: 113.55
 ---- batch: 060 ----
mean loss: 120.07
 ---- batch: 070 ----
mean loss: 116.79
 ---- batch: 080 ----
mean loss: 118.51
 ---- batch: 090 ----
mean loss: 117.70
 ---- batch: 100 ----
mean loss: 113.58
 ---- batch: 110 ----
mean loss: 110.32
train mean loss: 117.03
epoch train time: 0:00:01.875120
elapsed time: 0:07:21.730526
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 23:28:55.505671
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.85
 ---- batch: 020 ----
mean loss: 113.60
 ---- batch: 030 ----
mean loss: 120.05
 ---- batch: 040 ----
mean loss: 115.68
 ---- batch: 050 ----
mean loss: 117.16
 ---- batch: 060 ----
mean loss: 117.16
 ---- batch: 070 ----
mean loss: 116.39
 ---- batch: 080 ----
mean loss: 115.47
 ---- batch: 090 ----
mean loss: 121.11
 ---- batch: 100 ----
mean loss: 119.67
 ---- batch: 110 ----
mean loss: 114.80
train mean loss: 117.04
epoch train time: 0:00:01.887640
elapsed time: 0:07:23.618753
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 23:28:57.393861
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 118.70
 ---- batch: 020 ----
mean loss: 114.32
 ---- batch: 030 ----
mean loss: 116.12
 ---- batch: 040 ----
mean loss: 121.70
 ---- batch: 050 ----
mean loss: 113.57
 ---- batch: 060 ----
mean loss: 115.09
 ---- batch: 070 ----
mean loss: 116.24
 ---- batch: 080 ----
mean loss: 113.67
 ---- batch: 090 ----
mean loss: 121.22
 ---- batch: 100 ----
mean loss: 123.46
 ---- batch: 110 ----
mean loss: 113.82
train mean loss: 117.01
epoch train time: 0:00:01.870440
elapsed time: 0:07:25.489759
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 23:28:59.264904
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.29
 ---- batch: 020 ----
mean loss: 117.54
 ---- batch: 030 ----
mean loss: 116.14
 ---- batch: 040 ----
mean loss: 114.33
 ---- batch: 050 ----
mean loss: 119.16
 ---- batch: 060 ----
mean loss: 110.31
 ---- batch: 070 ----
mean loss: 116.44
 ---- batch: 080 ----
mean loss: 123.29
 ---- batch: 090 ----
mean loss: 116.59
 ---- batch: 100 ----
mean loss: 118.41
 ---- batch: 110 ----
mean loss: 120.97
train mean loss: 116.92
epoch train time: 0:00:01.871432
elapsed time: 0:07:27.361813
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 23:29:01.136945
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.53
 ---- batch: 020 ----
mean loss: 116.82
 ---- batch: 030 ----
mean loss: 119.05
 ---- batch: 040 ----
mean loss: 118.80
 ---- batch: 050 ----
mean loss: 121.66
 ---- batch: 060 ----
mean loss: 111.34
 ---- batch: 070 ----
mean loss: 114.84
 ---- batch: 080 ----
mean loss: 118.08
 ---- batch: 090 ----
mean loss: 114.80
 ---- batch: 100 ----
mean loss: 123.89
 ---- batch: 110 ----
mean loss: 113.99
train mean loss: 116.84
epoch train time: 0:00:01.870391
elapsed time: 0:07:29.232783
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 23:29:03.007920
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.94
 ---- batch: 020 ----
mean loss: 109.27
 ---- batch: 030 ----
mean loss: 119.30
 ---- batch: 040 ----
mean loss: 117.08
 ---- batch: 050 ----
mean loss: 115.63
 ---- batch: 060 ----
mean loss: 115.00
 ---- batch: 070 ----
mean loss: 123.54
 ---- batch: 080 ----
mean loss: 125.64
 ---- batch: 090 ----
mean loss: 112.70
 ---- batch: 100 ----
mean loss: 113.19
 ---- batch: 110 ----
mean loss: 120.98
train mean loss: 116.76
epoch train time: 0:00:01.885972
elapsed time: 0:07:31.119370
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 23:29:04.894458
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 126.40
 ---- batch: 020 ----
mean loss: 121.63
 ---- batch: 030 ----
mean loss: 122.52
 ---- batch: 040 ----
mean loss: 110.56
 ---- batch: 050 ----
mean loss: 113.23
 ---- batch: 060 ----
mean loss: 116.67
 ---- batch: 070 ----
mean loss: 116.52
 ---- batch: 080 ----
mean loss: 110.88
 ---- batch: 090 ----
mean loss: 112.56
 ---- batch: 100 ----
mean loss: 119.60
 ---- batch: 110 ----
mean loss: 113.06
train mean loss: 116.91
epoch train time: 0:00:01.858874
elapsed time: 0:07:32.978815
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 23:29:06.753974
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.37
 ---- batch: 020 ----
mean loss: 119.83
 ---- batch: 030 ----
mean loss: 115.86
 ---- batch: 040 ----
mean loss: 113.41
 ---- batch: 050 ----
mean loss: 118.45
 ---- batch: 060 ----
mean loss: 119.85
 ---- batch: 070 ----
mean loss: 120.00
 ---- batch: 080 ----
mean loss: 120.34
 ---- batch: 090 ----
mean loss: 121.53
 ---- batch: 100 ----
mean loss: 110.61
 ---- batch: 110 ----
mean loss: 113.02
train mean loss: 116.87
epoch train time: 0:00:01.896439
elapsed time: 0:07:34.875889
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 23:29:08.651004
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.79
 ---- batch: 020 ----
mean loss: 118.68
 ---- batch: 030 ----
mean loss: 114.69
 ---- batch: 040 ----
mean loss: 113.18
 ---- batch: 050 ----
mean loss: 118.59
 ---- batch: 060 ----
mean loss: 120.92
 ---- batch: 070 ----
mean loss: 117.92
 ---- batch: 080 ----
mean loss: 123.41
 ---- batch: 090 ----
mean loss: 119.69
 ---- batch: 100 ----
mean loss: 108.01
 ---- batch: 110 ----
mean loss: 115.55
train mean loss: 116.68
epoch train time: 0:00:01.891493
elapsed time: 0:07:36.767946
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 23:29:10.543116
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.88
 ---- batch: 020 ----
mean loss: 115.48
 ---- batch: 030 ----
mean loss: 119.60
 ---- batch: 040 ----
mean loss: 120.38
 ---- batch: 050 ----
mean loss: 109.01
 ---- batch: 060 ----
mean loss: 120.82
 ---- batch: 070 ----
mean loss: 115.64
 ---- batch: 080 ----
mean loss: 113.50
 ---- batch: 090 ----
mean loss: 117.47
 ---- batch: 100 ----
mean loss: 120.82
 ---- batch: 110 ----
mean loss: 117.90
train mean loss: 116.74
epoch train time: 0:00:01.871028
elapsed time: 0:07:38.639603
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 23:29:12.414767
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.41
 ---- batch: 020 ----
mean loss: 128.08
 ---- batch: 030 ----
mean loss: 119.69
 ---- batch: 040 ----
mean loss: 113.82
 ---- batch: 050 ----
mean loss: 110.72
 ---- batch: 060 ----
mean loss: 107.42
 ---- batch: 070 ----
mean loss: 125.97
 ---- batch: 080 ----
mean loss: 111.79
 ---- batch: 090 ----
mean loss: 118.41
 ---- batch: 100 ----
mean loss: 119.78
 ---- batch: 110 ----
mean loss: 115.10
train mean loss: 116.53
epoch train time: 0:00:01.896097
elapsed time: 0:07:40.536351
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 23:29:14.311426
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.84
 ---- batch: 020 ----
mean loss: 115.78
 ---- batch: 030 ----
mean loss: 114.96
 ---- batch: 040 ----
mean loss: 110.29
 ---- batch: 050 ----
mean loss: 113.49
 ---- batch: 060 ----
mean loss: 122.33
 ---- batch: 070 ----
mean loss: 122.69
 ---- batch: 080 ----
mean loss: 126.48
 ---- batch: 090 ----
mean loss: 113.68
 ---- batch: 100 ----
mean loss: 118.83
 ---- batch: 110 ----
mean loss: 117.97
train mean loss: 116.69
epoch train time: 0:00:01.864373
elapsed time: 0:07:42.401272
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 23:29:16.176382
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.12
 ---- batch: 020 ----
mean loss: 115.27
 ---- batch: 030 ----
mean loss: 113.20
 ---- batch: 040 ----
mean loss: 119.98
 ---- batch: 050 ----
mean loss: 113.91
 ---- batch: 060 ----
mean loss: 114.98
 ---- batch: 070 ----
mean loss: 121.04
 ---- batch: 080 ----
mean loss: 124.04
 ---- batch: 090 ----
mean loss: 114.55
 ---- batch: 100 ----
mean loss: 113.15
 ---- batch: 110 ----
mean loss: 116.13
train mean loss: 116.57
epoch train time: 0:00:01.891505
elapsed time: 0:07:44.293352
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 23:29:18.068464
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 107.05
 ---- batch: 020 ----
mean loss: 118.58
 ---- batch: 030 ----
mean loss: 126.22
 ---- batch: 040 ----
mean loss: 125.10
 ---- batch: 050 ----
mean loss: 117.76
 ---- batch: 060 ----
mean loss: 118.56
 ---- batch: 070 ----
mean loss: 114.17
 ---- batch: 080 ----
mean loss: 106.00
 ---- batch: 090 ----
mean loss: 116.99
 ---- batch: 100 ----
mean loss: 114.45
 ---- batch: 110 ----
mean loss: 114.74
train mean loss: 116.56
epoch train time: 0:00:01.886165
elapsed time: 0:07:46.180081
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 23:29:19.955202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 119.63
 ---- batch: 020 ----
mean loss: 112.78
 ---- batch: 030 ----
mean loss: 114.54
 ---- batch: 040 ----
mean loss: 121.44
 ---- batch: 050 ----
mean loss: 116.28
 ---- batch: 060 ----
mean loss: 116.39
 ---- batch: 070 ----
mean loss: 114.62
 ---- batch: 080 ----
mean loss: 116.59
 ---- batch: 090 ----
mean loss: 114.32
 ---- batch: 100 ----
mean loss: 115.64
 ---- batch: 110 ----
mean loss: 119.30
train mean loss: 116.70
epoch train time: 0:00:01.901212
elapsed time: 0:07:48.081863
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 23:29:21.856969
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.12
 ---- batch: 020 ----
mean loss: 109.08
 ---- batch: 030 ----
mean loss: 111.93
 ---- batch: 040 ----
mean loss: 124.67
 ---- batch: 050 ----
mean loss: 116.52
 ---- batch: 060 ----
mean loss: 109.86
 ---- batch: 070 ----
mean loss: 116.60
 ---- batch: 080 ----
mean loss: 124.22
 ---- batch: 090 ----
mean loss: 117.16
 ---- batch: 100 ----
mean loss: 123.44
 ---- batch: 110 ----
mean loss: 121.48
train mean loss: 116.53
epoch train time: 0:00:01.887783
elapsed time: 0:07:49.970227
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 23:29:23.745352
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.79
 ---- batch: 020 ----
mean loss: 114.88
 ---- batch: 030 ----
mean loss: 119.30
 ---- batch: 040 ----
mean loss: 115.78
 ---- batch: 050 ----
mean loss: 106.83
 ---- batch: 060 ----
mean loss: 119.06
 ---- batch: 070 ----
mean loss: 118.66
 ---- batch: 080 ----
mean loss: 119.53
 ---- batch: 090 ----
mean loss: 118.59
 ---- batch: 100 ----
mean loss: 113.96
 ---- batch: 110 ----
mean loss: 114.27
train mean loss: 116.49
epoch train time: 0:00:01.903911
elapsed time: 0:07:51.874724
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 23:29:25.649916
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.22
 ---- batch: 020 ----
mean loss: 110.00
 ---- batch: 030 ----
mean loss: 115.74
 ---- batch: 040 ----
mean loss: 121.44
 ---- batch: 050 ----
mean loss: 109.79
 ---- batch: 060 ----
mean loss: 119.73
 ---- batch: 070 ----
mean loss: 122.34
 ---- batch: 080 ----
mean loss: 120.41
 ---- batch: 090 ----
mean loss: 115.81
 ---- batch: 100 ----
mean loss: 119.65
 ---- batch: 110 ----
mean loss: 114.31
train mean loss: 116.46
epoch train time: 0:00:01.908853
elapsed time: 0:07:53.784584
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 23:29:27.559484
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.94
 ---- batch: 020 ----
mean loss: 116.36
 ---- batch: 030 ----
mean loss: 111.76
 ---- batch: 040 ----
mean loss: 118.79
 ---- batch: 050 ----
mean loss: 113.28
 ---- batch: 060 ----
mean loss: 117.80
 ---- batch: 070 ----
mean loss: 115.73
 ---- batch: 080 ----
mean loss: 119.71
 ---- batch: 090 ----
mean loss: 116.11
 ---- batch: 100 ----
mean loss: 119.47
 ---- batch: 110 ----
mean loss: 115.51
train mean loss: 116.34
epoch train time: 0:00:01.890813
elapsed time: 0:07:55.675767
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 23:29:29.450869
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 122.92
 ---- batch: 020 ----
mean loss: 110.81
 ---- batch: 030 ----
mean loss: 114.80
 ---- batch: 040 ----
mean loss: 125.89
 ---- batch: 050 ----
mean loss: 109.90
 ---- batch: 060 ----
mean loss: 120.03
 ---- batch: 070 ----
mean loss: 115.94
 ---- batch: 080 ----
mean loss: 112.77
 ---- batch: 090 ----
mean loss: 117.41
 ---- batch: 100 ----
mean loss: 115.86
 ---- batch: 110 ----
mean loss: 112.97
train mean loss: 116.42
epoch train time: 0:00:01.903037
elapsed time: 0:07:57.579388
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 23:29:31.354509
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.89
 ---- batch: 020 ----
mean loss: 115.54
 ---- batch: 030 ----
mean loss: 116.15
 ---- batch: 040 ----
mean loss: 121.92
 ---- batch: 050 ----
mean loss: 112.75
 ---- batch: 060 ----
mean loss: 124.95
 ---- batch: 070 ----
mean loss: 119.41
 ---- batch: 080 ----
mean loss: 116.93
 ---- batch: 090 ----
mean loss: 115.70
 ---- batch: 100 ----
mean loss: 113.69
 ---- batch: 110 ----
mean loss: 110.18
train mean loss: 116.38
epoch train time: 0:00:01.902262
elapsed time: 0:07:59.482324
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 23:29:33.257545
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.69
 ---- batch: 020 ----
mean loss: 121.70
 ---- batch: 030 ----
mean loss: 114.29
 ---- batch: 040 ----
mean loss: 119.79
 ---- batch: 050 ----
mean loss: 116.09
 ---- batch: 060 ----
mean loss: 115.17
 ---- batch: 070 ----
mean loss: 107.13
 ---- batch: 080 ----
mean loss: 117.56
 ---- batch: 090 ----
mean loss: 119.76
 ---- batch: 100 ----
mean loss: 116.32
 ---- batch: 110 ----
mean loss: 115.57
train mean loss: 116.32
epoch train time: 0:00:01.867187
elapsed time: 0:08:01.350181
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 23:29:35.125271
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 120.05
 ---- batch: 020 ----
mean loss: 117.08
 ---- batch: 030 ----
mean loss: 111.24
 ---- batch: 040 ----
mean loss: 117.41
 ---- batch: 050 ----
mean loss: 118.73
 ---- batch: 060 ----
mean loss: 117.22
 ---- batch: 070 ----
mean loss: 126.71
 ---- batch: 080 ----
mean loss: 108.74
 ---- batch: 090 ----
mean loss: 110.67
 ---- batch: 100 ----
mean loss: 118.37
 ---- batch: 110 ----
mean loss: 114.11
train mean loss: 116.27
epoch train time: 0:00:01.865502
elapsed time: 0:08:03.216271
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 23:29:36.991461
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.18
 ---- batch: 020 ----
mean loss: 120.25
 ---- batch: 030 ----
mean loss: 115.86
 ---- batch: 040 ----
mean loss: 117.82
 ---- batch: 050 ----
mean loss: 113.84
 ---- batch: 060 ----
mean loss: 123.70
 ---- batch: 070 ----
mean loss: 121.85
 ---- batch: 080 ----
mean loss: 119.34
 ---- batch: 090 ----
mean loss: 111.07
 ---- batch: 100 ----
mean loss: 114.77
 ---- batch: 110 ----
mean loss: 109.65
train mean loss: 116.22
epoch train time: 0:00:01.872312
elapsed time: 0:08:05.089225
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 23:29:38.864344
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 110.09
 ---- batch: 020 ----
mean loss: 115.44
 ---- batch: 030 ----
mean loss: 118.09
 ---- batch: 040 ----
mean loss: 111.66
 ---- batch: 050 ----
mean loss: 114.80
 ---- batch: 060 ----
mean loss: 113.43
 ---- batch: 070 ----
mean loss: 117.09
 ---- batch: 080 ----
mean loss: 118.58
 ---- batch: 090 ----
mean loss: 129.75
 ---- batch: 100 ----
mean loss: 108.97
 ---- batch: 110 ----
mean loss: 120.64
train mean loss: 116.19
epoch train time: 0:00:01.888349
elapsed time: 0:08:06.978189
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 23:29:40.753301
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 114.67
 ---- batch: 020 ----
mean loss: 111.66
 ---- batch: 030 ----
mean loss: 115.30
 ---- batch: 040 ----
mean loss: 114.71
 ---- batch: 050 ----
mean loss: 121.40
 ---- batch: 060 ----
mean loss: 115.89
 ---- batch: 070 ----
mean loss: 118.20
 ---- batch: 080 ----
mean loss: 114.70
 ---- batch: 090 ----
mean loss: 115.35
 ---- batch: 100 ----
mean loss: 122.85
 ---- batch: 110 ----
mean loss: 115.07
train mean loss: 116.09
epoch train time: 0:00:01.866353
elapsed time: 0:08:08.845094
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 23:29:42.620223
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 121.19
 ---- batch: 020 ----
mean loss: 123.84
 ---- batch: 030 ----
mean loss: 118.84
 ---- batch: 040 ----
mean loss: 107.83
 ---- batch: 050 ----
mean loss: 109.08
 ---- batch: 060 ----
mean loss: 120.03
 ---- batch: 070 ----
mean loss: 119.09
 ---- batch: 080 ----
mean loss: 108.76
 ---- batch: 090 ----
mean loss: 110.10
 ---- batch: 100 ----
mean loss: 119.53
 ---- batch: 110 ----
mean loss: 120.87
train mean loss: 116.26
epoch train time: 0:00:01.923811
elapsed time: 0:08:10.769596
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 23:29:44.544739
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 112.70
 ---- batch: 020 ----
mean loss: 121.27
 ---- batch: 030 ----
mean loss: 110.02
 ---- batch: 040 ----
mean loss: 122.56
 ---- batch: 050 ----
mean loss: 118.22
 ---- batch: 060 ----
mean loss: 113.79
 ---- batch: 070 ----
mean loss: 121.83
 ---- batch: 080 ----
mean loss: 108.88
 ---- batch: 090 ----
mean loss: 114.10
 ---- batch: 100 ----
mean loss: 121.53
 ---- batch: 110 ----
mean loss: 112.21
train mean loss: 115.96
epoch train time: 0:00:01.872335
elapsed time: 0:08:12.642516
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 23:29:46.417627
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 113.13
 ---- batch: 020 ----
mean loss: 109.33
 ---- batch: 030 ----
mean loss: 112.48
 ---- batch: 040 ----
mean loss: 118.77
 ---- batch: 050 ----
mean loss: 115.76
 ---- batch: 060 ----
mean loss: 115.61
 ---- batch: 070 ----
mean loss: 120.14
 ---- batch: 080 ----
mean loss: 120.31
 ---- batch: 090 ----
mean loss: 115.43
 ---- batch: 100 ----
mean loss: 121.27
 ---- batch: 110 ----
mean loss: 115.76
train mean loss: 116.07
epoch train time: 0:00:01.858610
elapsed time: 0:08:14.501691
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 23:29:48.276862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.21
 ---- batch: 020 ----
mean loss: 113.54
 ---- batch: 030 ----
mean loss: 117.54
 ---- batch: 040 ----
mean loss: 114.88
 ---- batch: 050 ----
mean loss: 111.40
 ---- batch: 060 ----
mean loss: 119.10
 ---- batch: 070 ----
mean loss: 119.09
 ---- batch: 080 ----
mean loss: 110.69
 ---- batch: 090 ----
mean loss: 115.35
 ---- batch: 100 ----
mean loss: 117.41
 ---- batch: 110 ----
mean loss: 123.61
train mean loss: 116.03
epoch train time: 0:00:01.871082
elapsed time: 0:08:16.373415
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 23:29:50.148529
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 116.55
 ---- batch: 020 ----
mean loss: 117.41
 ---- batch: 030 ----
mean loss: 114.86
 ---- batch: 040 ----
mean loss: 112.46
 ---- batch: 050 ----
mean loss: 117.67
 ---- batch: 060 ----
mean loss: 116.31
 ---- batch: 070 ----
mean loss: 120.51
 ---- batch: 080 ----
mean loss: 112.43
 ---- batch: 090 ----
mean loss: 114.95
 ---- batch: 100 ----
mean loss: 118.80
 ---- batch: 110 ----
mean loss: 112.32
train mean loss: 116.03
epoch train time: 0:00:01.891371
elapsed time: 0:08:18.265342
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 23:29:52.040451
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 111.09
 ---- batch: 020 ----
mean loss: 117.08
 ---- batch: 030 ----
mean loss: 116.18
 ---- batch: 040 ----
mean loss: 112.32
 ---- batch: 050 ----
mean loss: 111.76
 ---- batch: 060 ----
mean loss: 115.63
 ---- batch: 070 ----
mean loss: 120.54
 ---- batch: 080 ----
mean loss: 111.99
 ---- batch: 090 ----
mean loss: 118.74
 ---- batch: 100 ----
mean loss: 117.61
 ---- batch: 110 ----
mean loss: 123.46
train mean loss: 115.86
epoch train time: 0:00:01.878720
elapsed time: 0:08:20.144670
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 23:29:53.919848
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 115.99
 ---- batch: 020 ----
mean loss: 113.66
 ---- batch: 030 ----
mean loss: 112.67
 ---- batch: 040 ----
mean loss: 113.97
 ---- batch: 050 ----
mean loss: 111.92
 ---- batch: 060 ----
mean loss: 116.88
 ---- batch: 070 ----
mean loss: 110.83
 ---- batch: 080 ----
mean loss: 117.97
 ---- batch: 090 ----
mean loss: 118.08
 ---- batch: 100 ----
mean loss: 120.35
 ---- batch: 110 ----
mean loss: 122.74
train mean loss: 115.93
epoch train time: 0:00:01.876928
elapsed time: 0:08:22.022252
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 23:29:55.797354
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 117.94
 ---- batch: 020 ----
mean loss: 112.80
 ---- batch: 030 ----
mean loss: 111.89
 ---- batch: 040 ----
mean loss: 117.79
 ---- batch: 050 ----
mean loss: 116.52
 ---- batch: 060 ----
mean loss: 121.87
 ---- batch: 070 ----
mean loss: 112.18
 ---- batch: 080 ----
mean loss: 115.72
 ---- batch: 090 ----
mean loss: 114.87
 ---- batch: 100 ----
mean loss: 118.66
 ---- batch: 110 ----
mean loss: 115.67
train mean loss: 115.88
epoch train time: 0:00:01.872488
elapsed time: 0:08:23.903117
checkpoint saved in file: log/CMAPSS/FD004/min-max/bayesian_dense3/bayesian_dense3_7/checkpoint.pth.tar
**** end time: 2019-09-26 23:29:57.677876 ****
