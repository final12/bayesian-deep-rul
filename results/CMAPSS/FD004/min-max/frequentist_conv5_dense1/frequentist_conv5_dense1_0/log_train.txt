Namespace(batch_size=512, dataset='CMAPSS/FD004', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_0', max_epoch=250, max_rul=125, metric='rmse', model='frequentist_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 15442
use_cuda: True
Dataset: CMAPSS/FD004
Building FrequentistConv5Dense1...
Done.
**** start time: 2019-09-26 23:47:43.083529 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 10, 16, 24]             100
              Tanh-2           [-1, 10, 16, 24]               0
            Conv2d-3           [-1, 10, 15, 24]           1,000
              Tanh-4           [-1, 10, 15, 24]               0
            Conv2d-5           [-1, 10, 16, 24]           1,000
              Tanh-6           [-1, 10, 16, 24]               0
            Conv2d-7           [-1, 10, 15, 24]           1,000
              Tanh-8           [-1, 10, 15, 24]               0
            Conv2d-9            [-1, 1, 15, 24]              30
             Tanh-10            [-1, 1, 15, 24]               0
          Flatten-11                  [-1, 360]               0
          Dropout-12                  [-1, 360]               0
           Linear-13                  [-1, 100]          36,000
           Linear-14                    [-1, 1]             100
================================================================
Total params: 39,230
Trainable params: 39,230
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-26 23:47:43.093124
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 4719.77
 ---- batch: 020 ----
mean loss: 3230.34
 ---- batch: 030 ----
mean loss: 1762.59
 ---- batch: 040 ----
mean loss: 1371.22
 ---- batch: 050 ----
mean loss: 1261.20
 ---- batch: 060 ----
mean loss: 1139.34
 ---- batch: 070 ----
mean loss: 1098.46
 ---- batch: 080 ----
mean loss: 1067.73
 ---- batch: 090 ----
mean loss: 1014.28
 ---- batch: 100 ----
mean loss: 987.49
 ---- batch: 110 ----
mean loss: 958.80
train mean loss: 1672.34
epoch train time: 0:00:34.174472
elapsed time: 0:00:34.186598
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-26 23:48:17.270170
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.49
 ---- batch: 020 ----
mean loss: 917.80
 ---- batch: 030 ----
mean loss: 876.81
 ---- batch: 040 ----
mean loss: 874.15
 ---- batch: 050 ----
mean loss: 826.18
 ---- batch: 060 ----
mean loss: 804.61
 ---- batch: 070 ----
mean loss: 819.94
 ---- batch: 080 ----
mean loss: 794.01
 ---- batch: 090 ----
mean loss: 797.01
 ---- batch: 100 ----
mean loss: 771.39
 ---- batch: 110 ----
mean loss: 795.01
train mean loss: 834.95
epoch train time: 0:00:02.215630
elapsed time: 0:00:36.402374
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-26 23:48:19.485977
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.83
 ---- batch: 020 ----
mean loss: 755.95
 ---- batch: 030 ----
mean loss: 733.53
 ---- batch: 040 ----
mean loss: 733.28
 ---- batch: 050 ----
mean loss: 714.45
 ---- batch: 060 ----
mean loss: 699.69
 ---- batch: 070 ----
mean loss: 726.32
 ---- batch: 080 ----
mean loss: 717.71
 ---- batch: 090 ----
mean loss: 704.48
 ---- batch: 100 ----
mean loss: 686.75
 ---- batch: 110 ----
mean loss: 690.39
train mean loss: 717.00
epoch train time: 0:00:02.140654
elapsed time: 0:00:38.543200
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-26 23:48:21.626780
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 690.65
 ---- batch: 020 ----
mean loss: 679.27
 ---- batch: 030 ----
mean loss: 674.48
 ---- batch: 040 ----
mean loss: 650.19
 ---- batch: 050 ----
mean loss: 662.10
 ---- batch: 060 ----
mean loss: 662.37
 ---- batch: 070 ----
mean loss: 668.81
 ---- batch: 080 ----
mean loss: 648.36
 ---- batch: 090 ----
mean loss: 651.27
 ---- batch: 100 ----
mean loss: 652.79
 ---- batch: 110 ----
mean loss: 645.99
train mean loss: 661.65
epoch train time: 0:00:02.137025
elapsed time: 0:00:40.680372
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-26 23:48:23.763981
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 632.69
 ---- batch: 020 ----
mean loss: 651.94
 ---- batch: 030 ----
mean loss: 633.78
 ---- batch: 040 ----
mean loss: 664.10
 ---- batch: 050 ----
mean loss: 637.71
 ---- batch: 060 ----
mean loss: 641.01
 ---- batch: 070 ----
mean loss: 620.51
 ---- batch: 080 ----
mean loss: 624.98
 ---- batch: 090 ----
mean loss: 625.43
 ---- batch: 100 ----
mean loss: 614.88
 ---- batch: 110 ----
mean loss: 623.53
train mean loss: 633.27
epoch train time: 0:00:02.132102
elapsed time: 0:00:42.812657
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-26 23:48:25.896254
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 618.32
 ---- batch: 020 ----
mean loss: 619.86
 ---- batch: 030 ----
mean loss: 607.41
 ---- batch: 040 ----
mean loss: 599.90
 ---- batch: 050 ----
mean loss: 597.46
 ---- batch: 060 ----
mean loss: 590.87
 ---- batch: 070 ----
mean loss: 600.32
 ---- batch: 080 ----
mean loss: 618.90
 ---- batch: 090 ----
mean loss: 602.93
 ---- batch: 100 ----
mean loss: 601.05
 ---- batch: 110 ----
mean loss: 595.71
train mean loss: 604.60
epoch train time: 0:00:02.139197
elapsed time: 0:00:44.952019
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-26 23:48:28.035599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 582.55
 ---- batch: 020 ----
mean loss: 610.84
 ---- batch: 030 ----
mean loss: 594.15
 ---- batch: 040 ----
mean loss: 573.84
 ---- batch: 050 ----
mean loss: 581.16
 ---- batch: 060 ----
mean loss: 580.03
 ---- batch: 070 ----
mean loss: 576.45
 ---- batch: 080 ----
mean loss: 588.20
 ---- batch: 090 ----
mean loss: 592.04
 ---- batch: 100 ----
mean loss: 607.31
 ---- batch: 110 ----
mean loss: 550.44
train mean loss: 585.62
epoch train time: 0:00:02.128792
elapsed time: 0:00:47.080961
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-26 23:48:30.164543
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 583.73
 ---- batch: 020 ----
mean loss: 579.32
 ---- batch: 030 ----
mean loss: 600.08
 ---- batch: 040 ----
mean loss: 576.45
 ---- batch: 050 ----
mean loss: 582.98
 ---- batch: 060 ----
mean loss: 575.13
 ---- batch: 070 ----
mean loss: 572.01
 ---- batch: 080 ----
mean loss: 564.31
 ---- batch: 090 ----
mean loss: 570.46
 ---- batch: 100 ----
mean loss: 585.70
 ---- batch: 110 ----
mean loss: 562.52
train mean loss: 578.03
epoch train time: 0:00:02.128366
elapsed time: 0:00:49.209485
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-26 23:48:32.293065
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 566.67
 ---- batch: 020 ----
mean loss: 560.17
 ---- batch: 030 ----
mean loss: 567.54
 ---- batch: 040 ----
mean loss: 557.54
 ---- batch: 050 ----
mean loss: 574.65
 ---- batch: 060 ----
mean loss: 578.08
 ---- batch: 070 ----
mean loss: 571.61
 ---- batch: 080 ----
mean loss: 558.72
 ---- batch: 090 ----
mean loss: 545.31
 ---- batch: 100 ----
mean loss: 568.44
 ---- batch: 110 ----
mean loss: 542.66
train mean loss: 562.39
epoch train time: 0:00:02.133314
elapsed time: 0:00:51.343009
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-26 23:48:34.426599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 556.05
 ---- batch: 020 ----
mean loss: 560.34
 ---- batch: 030 ----
mean loss: 532.45
 ---- batch: 040 ----
mean loss: 551.61
 ---- batch: 050 ----
mean loss: 559.03
 ---- batch: 060 ----
mean loss: 550.81
 ---- batch: 070 ----
mean loss: 530.11
 ---- batch: 080 ----
mean loss: 549.56
 ---- batch: 090 ----
mean loss: 532.01
 ---- batch: 100 ----
mean loss: 525.83
 ---- batch: 110 ----
mean loss: 553.20
train mean loss: 544.69
epoch train time: 0:00:02.124497
elapsed time: 0:00:53.467675
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-26 23:48:36.551281
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.02
 ---- batch: 020 ----
mean loss: 527.49
 ---- batch: 030 ----
mean loss: 530.26
 ---- batch: 040 ----
mean loss: 548.41
 ---- batch: 050 ----
mean loss: 538.63
 ---- batch: 060 ----
mean loss: 528.08
 ---- batch: 070 ----
mean loss: 543.12
 ---- batch: 080 ----
mean loss: 529.72
 ---- batch: 090 ----
mean loss: 542.14
 ---- batch: 100 ----
mean loss: 531.96
 ---- batch: 110 ----
mean loss: 520.98
train mean loss: 535.18
epoch train time: 0:00:02.132978
elapsed time: 0:00:55.600849
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-26 23:48:38.684465
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 536.14
 ---- batch: 020 ----
mean loss: 530.59
 ---- batch: 030 ----
mean loss: 540.96
 ---- batch: 040 ----
mean loss: 542.35
 ---- batch: 050 ----
mean loss: 533.55
 ---- batch: 060 ----
mean loss: 522.17
 ---- batch: 070 ----
mean loss: 518.40
 ---- batch: 080 ----
mean loss: 520.61
 ---- batch: 090 ----
mean loss: 524.23
 ---- batch: 100 ----
mean loss: 520.45
 ---- batch: 110 ----
mean loss: 512.45
train mean loss: 527.76
epoch train time: 0:00:02.123496
elapsed time: 0:00:57.724561
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-26 23:48:40.808145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 530.51
 ---- batch: 020 ----
mean loss: 537.81
 ---- batch: 030 ----
mean loss: 518.13
 ---- batch: 040 ----
mean loss: 520.26
 ---- batch: 050 ----
mean loss: 512.98
 ---- batch: 060 ----
mean loss: 532.02
 ---- batch: 070 ----
mean loss: 531.61
 ---- batch: 080 ----
mean loss: 525.59
 ---- batch: 090 ----
mean loss: 519.62
 ---- batch: 100 ----
mean loss: 530.76
 ---- batch: 110 ----
mean loss: 523.55
train mean loss: 525.85
epoch train time: 0:00:02.135053
elapsed time: 0:00:59.859769
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-26 23:48:42.943367
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.08
 ---- batch: 020 ----
mean loss: 527.08
 ---- batch: 030 ----
mean loss: 532.45
 ---- batch: 040 ----
mean loss: 517.14
 ---- batch: 050 ----
mean loss: 502.90
 ---- batch: 060 ----
mean loss: 522.68
 ---- batch: 070 ----
mean loss: 526.23
 ---- batch: 080 ----
mean loss: 515.03
 ---- batch: 090 ----
mean loss: 535.98
 ---- batch: 100 ----
mean loss: 537.30
 ---- batch: 110 ----
mean loss: 514.82
train mean loss: 522.90
epoch train time: 0:00:02.128653
elapsed time: 0:01:01.988590
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-26 23:48:45.072171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 524.28
 ---- batch: 020 ----
mean loss: 521.76
 ---- batch: 030 ----
mean loss: 515.84
 ---- batch: 040 ----
mean loss: 524.83
 ---- batch: 050 ----
mean loss: 522.32
 ---- batch: 060 ----
mean loss: 521.63
 ---- batch: 070 ----
mean loss: 520.50
 ---- batch: 080 ----
mean loss: 528.60
 ---- batch: 090 ----
mean loss: 537.70
 ---- batch: 100 ----
mean loss: 513.79
 ---- batch: 110 ----
mean loss: 539.37
train mean loss: 524.64
epoch train time: 0:00:02.131437
elapsed time: 0:01:04.120181
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-26 23:48:47.203762
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.75
 ---- batch: 020 ----
mean loss: 505.31
 ---- batch: 030 ----
mean loss: 519.30
 ---- batch: 040 ----
mean loss: 509.85
 ---- batch: 050 ----
mean loss: 526.65
 ---- batch: 060 ----
mean loss: 556.98
 ---- batch: 070 ----
mean loss: 527.40
 ---- batch: 080 ----
mean loss: 526.81
 ---- batch: 090 ----
mean loss: 533.37
 ---- batch: 100 ----
mean loss: 518.17
 ---- batch: 110 ----
mean loss: 521.36
train mean loss: 523.91
epoch train time: 0:00:02.133624
elapsed time: 0:01:06.253978
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-26 23:48:49.337658
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.75
 ---- batch: 020 ----
mean loss: 497.46
 ---- batch: 030 ----
mean loss: 498.82
 ---- batch: 040 ----
mean loss: 512.12
 ---- batch: 050 ----
mean loss: 518.40
 ---- batch: 060 ----
mean loss: 523.07
 ---- batch: 070 ----
mean loss: 527.82
 ---- batch: 080 ----
mean loss: 525.10
 ---- batch: 090 ----
mean loss: 510.23
 ---- batch: 100 ----
mean loss: 537.81
 ---- batch: 110 ----
mean loss: 519.03
train mean loss: 517.90
epoch train time: 0:00:02.121583
elapsed time: 0:01:08.375814
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-26 23:48:51.459395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 514.25
 ---- batch: 020 ----
mean loss: 524.18
 ---- batch: 030 ----
mean loss: 511.09
 ---- batch: 040 ----
mean loss: 521.52
 ---- batch: 050 ----
mean loss: 513.04
 ---- batch: 060 ----
mean loss: 517.20
 ---- batch: 070 ----
mean loss: 505.14
 ---- batch: 080 ----
mean loss: 508.26
 ---- batch: 090 ----
mean loss: 503.78
 ---- batch: 100 ----
mean loss: 530.79
 ---- batch: 110 ----
mean loss: 523.69
train mean loss: 515.04
epoch train time: 0:00:02.118692
elapsed time: 0:01:10.494670
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-26 23:48:53.578311
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.88
 ---- batch: 020 ----
mean loss: 525.70
 ---- batch: 030 ----
mean loss: 508.01
 ---- batch: 040 ----
mean loss: 502.32
 ---- batch: 050 ----
mean loss: 505.65
 ---- batch: 060 ----
mean loss: 530.78
 ---- batch: 070 ----
mean loss: 506.16
 ---- batch: 080 ----
mean loss: 529.45
 ---- batch: 090 ----
mean loss: 499.57
 ---- batch: 100 ----
mean loss: 511.20
 ---- batch: 110 ----
mean loss: 532.15
train mean loss: 513.48
epoch train time: 0:00:02.129669
elapsed time: 0:01:12.624566
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-26 23:48:55.708173
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 527.62
 ---- batch: 020 ----
mean loss: 518.79
 ---- batch: 030 ----
mean loss: 528.89
 ---- batch: 040 ----
mean loss: 527.26
 ---- batch: 050 ----
mean loss: 501.82
 ---- batch: 060 ----
mean loss: 512.34
 ---- batch: 070 ----
mean loss: 524.11
 ---- batch: 080 ----
mean loss: 528.84
 ---- batch: 090 ----
mean loss: 535.84
 ---- batch: 100 ----
mean loss: 505.22
 ---- batch: 110 ----
mean loss: 504.17
train mean loss: 519.04
epoch train time: 0:00:02.131762
elapsed time: 0:01:14.756510
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-26 23:48:57.840092
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.79
 ---- batch: 020 ----
mean loss: 528.83
 ---- batch: 030 ----
mean loss: 517.48
 ---- batch: 040 ----
mean loss: 520.90
 ---- batch: 050 ----
mean loss: 515.73
 ---- batch: 060 ----
mean loss: 503.18
 ---- batch: 070 ----
mean loss: 518.37
 ---- batch: 080 ----
mean loss: 512.16
 ---- batch: 090 ----
mean loss: 501.27
 ---- batch: 100 ----
mean loss: 507.04
 ---- batch: 110 ----
mean loss: 511.16
train mean loss: 511.53
epoch train time: 0:00:02.126671
elapsed time: 0:01:16.883328
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-26 23:48:59.966909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.26
 ---- batch: 020 ----
mean loss: 529.85
 ---- batch: 030 ----
mean loss: 519.11
 ---- batch: 040 ----
mean loss: 520.29
 ---- batch: 050 ----
mean loss: 514.31
 ---- batch: 060 ----
mean loss: 505.77
 ---- batch: 070 ----
mean loss: 522.30
 ---- batch: 080 ----
mean loss: 502.48
 ---- batch: 090 ----
mean loss: 519.71
 ---- batch: 100 ----
mean loss: 523.15
 ---- batch: 110 ----
mean loss: 518.19
train mean loss: 518.15
epoch train time: 0:00:02.125880
elapsed time: 0:01:19.009375
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-26 23:49:02.092955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.82
 ---- batch: 020 ----
mean loss: 514.25
 ---- batch: 030 ----
mean loss: 510.25
 ---- batch: 040 ----
mean loss: 517.70
 ---- batch: 050 ----
mean loss: 530.98
 ---- batch: 060 ----
mean loss: 533.76
 ---- batch: 070 ----
mean loss: 523.44
 ---- batch: 080 ----
mean loss: 507.23
 ---- batch: 090 ----
mean loss: 502.79
 ---- batch: 100 ----
mean loss: 505.00
 ---- batch: 110 ----
mean loss: 509.51
train mean loss: 514.85
epoch train time: 0:00:02.124541
elapsed time: 0:01:21.134065
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-26 23:49:04.217669
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.73
 ---- batch: 020 ----
mean loss: 512.42
 ---- batch: 030 ----
mean loss: 514.87
 ---- batch: 040 ----
mean loss: 511.39
 ---- batch: 050 ----
mean loss: 523.98
 ---- batch: 060 ----
mean loss: 504.94
 ---- batch: 070 ----
mean loss: 504.33
 ---- batch: 080 ----
mean loss: 511.29
 ---- batch: 090 ----
mean loss: 498.05
 ---- batch: 100 ----
mean loss: 516.17
 ---- batch: 110 ----
mean loss: 506.29
train mean loss: 511.32
epoch train time: 0:00:02.121933
elapsed time: 0:01:23.256187
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-26 23:49:06.339803
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.23
 ---- batch: 020 ----
mean loss: 512.83
 ---- batch: 030 ----
mean loss: 507.84
 ---- batch: 040 ----
mean loss: 508.13
 ---- batch: 050 ----
mean loss: 504.91
 ---- batch: 060 ----
mean loss: 516.47
 ---- batch: 070 ----
mean loss: 518.46
 ---- batch: 080 ----
mean loss: 498.80
 ---- batch: 090 ----
mean loss: 512.47
 ---- batch: 100 ----
mean loss: 508.30
 ---- batch: 110 ----
mean loss: 524.46
train mean loss: 510.04
epoch train time: 0:00:02.126856
elapsed time: 0:01:25.383231
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-26 23:49:08.466813
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 516.63
 ---- batch: 020 ----
mean loss: 515.58
 ---- batch: 030 ----
mean loss: 511.55
 ---- batch: 040 ----
mean loss: 520.13
 ---- batch: 050 ----
mean loss: 522.21
 ---- batch: 060 ----
mean loss: 512.90
 ---- batch: 070 ----
mean loss: 534.23
 ---- batch: 080 ----
mean loss: 524.86
 ---- batch: 090 ----
mean loss: 498.74
 ---- batch: 100 ----
mean loss: 496.47
 ---- batch: 110 ----
mean loss: 504.31
train mean loss: 513.58
epoch train time: 0:00:02.126030
elapsed time: 0:01:27.509506
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-26 23:49:10.593107
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.51
 ---- batch: 020 ----
mean loss: 538.18
 ---- batch: 030 ----
mean loss: 509.84
 ---- batch: 040 ----
mean loss: 519.66
 ---- batch: 050 ----
mean loss: 514.50
 ---- batch: 060 ----
mean loss: 499.41
 ---- batch: 070 ----
mean loss: 497.83
 ---- batch: 080 ----
mean loss: 520.20
 ---- batch: 090 ----
mean loss: 513.33
 ---- batch: 100 ----
mean loss: 502.87
 ---- batch: 110 ----
mean loss: 504.44
train mean loss: 512.80
epoch train time: 0:00:02.123940
elapsed time: 0:01:29.633629
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-26 23:49:12.717212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 520.93
 ---- batch: 020 ----
mean loss: 513.16
 ---- batch: 030 ----
mean loss: 513.58
 ---- batch: 040 ----
mean loss: 516.35
 ---- batch: 050 ----
mean loss: 507.10
 ---- batch: 060 ----
mean loss: 504.34
 ---- batch: 070 ----
mean loss: 498.39
 ---- batch: 080 ----
mean loss: 509.57
 ---- batch: 090 ----
mean loss: 512.38
 ---- batch: 100 ----
mean loss: 496.80
 ---- batch: 110 ----
mean loss: 512.23
train mean loss: 509.46
epoch train time: 0:00:02.125636
elapsed time: 0:01:31.759427
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-26 23:49:14.843012
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.10
 ---- batch: 020 ----
mean loss: 520.97
 ---- batch: 030 ----
mean loss: 504.47
 ---- batch: 040 ----
mean loss: 511.97
 ---- batch: 050 ----
mean loss: 523.61
 ---- batch: 060 ----
mean loss: 507.78
 ---- batch: 070 ----
mean loss: 508.23
 ---- batch: 080 ----
mean loss: 502.19
 ---- batch: 090 ----
mean loss: 492.39
 ---- batch: 100 ----
mean loss: 506.48
 ---- batch: 110 ----
mean loss: 513.99
train mean loss: 507.71
epoch train time: 0:00:02.118517
elapsed time: 0:01:33.878115
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-26 23:49:16.961698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.08
 ---- batch: 020 ----
mean loss: 505.68
 ---- batch: 030 ----
mean loss: 491.47
 ---- batch: 040 ----
mean loss: 504.55
 ---- batch: 050 ----
mean loss: 512.06
 ---- batch: 060 ----
mean loss: 508.26
 ---- batch: 070 ----
mean loss: 518.48
 ---- batch: 080 ----
mean loss: 500.27
 ---- batch: 090 ----
mean loss: 494.95
 ---- batch: 100 ----
mean loss: 514.33
 ---- batch: 110 ----
mean loss: 515.42
train mean loss: 506.69
epoch train time: 0:00:02.126619
elapsed time: 0:01:36.004884
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-26 23:49:19.088464
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.34
 ---- batch: 020 ----
mean loss: 499.28
 ---- batch: 030 ----
mean loss: 502.58
 ---- batch: 040 ----
mean loss: 507.20
 ---- batch: 050 ----
mean loss: 499.10
 ---- batch: 060 ----
mean loss: 522.14
 ---- batch: 070 ----
mean loss: 492.24
 ---- batch: 080 ----
mean loss: 546.89
 ---- batch: 090 ----
mean loss: 512.58
 ---- batch: 100 ----
mean loss: 514.19
 ---- batch: 110 ----
mean loss: 527.36
train mean loss: 511.62
epoch train time: 0:00:02.122680
elapsed time: 0:01:38.127712
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-26 23:49:21.211313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 505.99
 ---- batch: 020 ----
mean loss: 526.17
 ---- batch: 030 ----
mean loss: 518.89
 ---- batch: 040 ----
mean loss: 515.25
 ---- batch: 050 ----
mean loss: 521.72
 ---- batch: 060 ----
mean loss: 519.92
 ---- batch: 070 ----
mean loss: 494.22
 ---- batch: 080 ----
mean loss: 495.71
 ---- batch: 090 ----
mean loss: 524.06
 ---- batch: 100 ----
mean loss: 500.68
 ---- batch: 110 ----
mean loss: 512.32
train mean loss: 512.22
epoch train time: 0:00:02.120475
elapsed time: 0:01:40.248355
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-26 23:49:23.331943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.54
 ---- batch: 020 ----
mean loss: 519.62
 ---- batch: 030 ----
mean loss: 516.56
 ---- batch: 040 ----
mean loss: 504.34
 ---- batch: 050 ----
mean loss: 507.68
 ---- batch: 060 ----
mean loss: 525.54
 ---- batch: 070 ----
mean loss: 501.16
 ---- batch: 080 ----
mean loss: 532.86
 ---- batch: 090 ----
mean loss: 525.34
 ---- batch: 100 ----
mean loss: 511.48
 ---- batch: 110 ----
mean loss: 504.35
train mean loss: 513.91
epoch train time: 0:00:02.124141
elapsed time: 0:01:42.372649
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-26 23:49:25.456230
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 519.24
 ---- batch: 020 ----
mean loss: 519.26
 ---- batch: 030 ----
mean loss: 499.35
 ---- batch: 040 ----
mean loss: 506.39
 ---- batch: 050 ----
mean loss: 509.54
 ---- batch: 060 ----
mean loss: 509.77
 ---- batch: 070 ----
mean loss: 533.42
 ---- batch: 080 ----
mean loss: 520.42
 ---- batch: 090 ----
mean loss: 512.02
 ---- batch: 100 ----
mean loss: 503.88
 ---- batch: 110 ----
mean loss: 514.42
train mean loss: 513.54
epoch train time: 0:00:02.123220
elapsed time: 0:01:44.496021
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-26 23:49:27.579602
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 526.44
 ---- batch: 020 ----
mean loss: 506.31
 ---- batch: 030 ----
mean loss: 498.12
 ---- batch: 040 ----
mean loss: 512.96
 ---- batch: 050 ----
mean loss: 506.47
 ---- batch: 060 ----
mean loss: 517.92
 ---- batch: 070 ----
mean loss: 495.70
 ---- batch: 080 ----
mean loss: 524.81
 ---- batch: 090 ----
mean loss: 523.03
 ---- batch: 100 ----
mean loss: 522.65
 ---- batch: 110 ----
mean loss: 512.11
train mean loss: 513.04
epoch train time: 0:00:02.125637
elapsed time: 0:01:46.621809
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-26 23:49:29.705395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 522.28
 ---- batch: 020 ----
mean loss: 524.70
 ---- batch: 030 ----
mean loss: 511.02
 ---- batch: 040 ----
mean loss: 505.17
 ---- batch: 050 ----
mean loss: 502.18
 ---- batch: 060 ----
mean loss: 512.76
 ---- batch: 070 ----
mean loss: 524.45
 ---- batch: 080 ----
mean loss: 512.03
 ---- batch: 090 ----
mean loss: 499.88
 ---- batch: 100 ----
mean loss: 529.88
 ---- batch: 110 ----
mean loss: 501.78
train mean loss: 512.87
epoch train time: 0:00:02.117468
elapsed time: 0:01:48.739438
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-26 23:49:31.823021
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 502.37
 ---- batch: 020 ----
mean loss: 506.23
 ---- batch: 030 ----
mean loss: 518.18
 ---- batch: 040 ----
mean loss: 506.39
 ---- batch: 050 ----
mean loss: 501.70
 ---- batch: 060 ----
mean loss: 515.50
 ---- batch: 070 ----
mean loss: 499.22
 ---- batch: 080 ----
mean loss: 509.49
 ---- batch: 090 ----
mean loss: 504.10
 ---- batch: 100 ----
mean loss: 501.02
 ---- batch: 110 ----
mean loss: 502.75
train mean loss: 506.39
epoch train time: 0:00:02.126380
elapsed time: 0:01:50.865973
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-26 23:49:33.949577
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 513.82
 ---- batch: 020 ----
mean loss: 495.70
 ---- batch: 030 ----
mean loss: 514.71
 ---- batch: 040 ----
mean loss: 502.28
 ---- batch: 050 ----
mean loss: 514.26
 ---- batch: 060 ----
mean loss: 506.96
 ---- batch: 070 ----
mean loss: 508.78
 ---- batch: 080 ----
mean loss: 497.07
 ---- batch: 090 ----
mean loss: 497.94
 ---- batch: 100 ----
mean loss: 509.00
 ---- batch: 110 ----
mean loss: 525.28
train mean loss: 508.28
epoch train time: 0:00:02.122277
elapsed time: 0:01:52.988425
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-26 23:49:36.072011
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 506.62
 ---- batch: 020 ----
mean loss: 514.46
 ---- batch: 030 ----
mean loss: 510.70
 ---- batch: 040 ----
mean loss: 503.67
 ---- batch: 050 ----
mean loss: 502.72
 ---- batch: 060 ----
mean loss: 505.02
 ---- batch: 070 ----
mean loss: 496.21
 ---- batch: 080 ----
mean loss: 518.45
 ---- batch: 090 ----
mean loss: 496.39
 ---- batch: 100 ----
mean loss: 523.51
 ---- batch: 110 ----
mean loss: 520.97
train mean loss: 509.44
epoch train time: 0:00:02.122808
elapsed time: 0:01:55.111389
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-26 23:49:38.195008
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.37
 ---- batch: 020 ----
mean loss: 502.54
 ---- batch: 030 ----
mean loss: 501.95
 ---- batch: 040 ----
mean loss: 514.42
 ---- batch: 050 ----
mean loss: 508.85
 ---- batch: 060 ----
mean loss: 500.91
 ---- batch: 070 ----
mean loss: 503.62
 ---- batch: 080 ----
mean loss: 520.44
 ---- batch: 090 ----
mean loss: 522.39
 ---- batch: 100 ----
mean loss: 523.79
 ---- batch: 110 ----
mean loss: 521.17
train mean loss: 509.27
epoch train time: 0:00:02.122647
elapsed time: 0:01:57.234226
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-26 23:49:40.317811
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.25
 ---- batch: 020 ----
mean loss: 513.48
 ---- batch: 030 ----
mean loss: 501.31
 ---- batch: 040 ----
mean loss: 525.11
 ---- batch: 050 ----
mean loss: 500.66
 ---- batch: 060 ----
mean loss: 506.47
 ---- batch: 070 ----
mean loss: 506.95
 ---- batch: 080 ----
mean loss: 499.53
 ---- batch: 090 ----
mean loss: 492.22
 ---- batch: 100 ----
mean loss: 505.06
 ---- batch: 110 ----
mean loss: 507.22
train mean loss: 505.98
epoch train time: 0:00:02.116669
elapsed time: 0:01:59.351077
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-26 23:49:42.434656
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.33
 ---- batch: 020 ----
mean loss: 496.60
 ---- batch: 030 ----
mean loss: 524.59
 ---- batch: 040 ----
mean loss: 508.11
 ---- batch: 050 ----
mean loss: 506.87
 ---- batch: 060 ----
mean loss: 505.42
 ---- batch: 070 ----
mean loss: 520.26
 ---- batch: 080 ----
mean loss: 508.24
 ---- batch: 090 ----
mean loss: 499.89
 ---- batch: 100 ----
mean loss: 508.14
 ---- batch: 110 ----
mean loss: 498.99
train mean loss: 506.70
epoch train time: 0:00:02.125490
elapsed time: 0:02:01.476716
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-26 23:49:44.560297
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.54
 ---- batch: 020 ----
mean loss: 489.09
 ---- batch: 030 ----
mean loss: 503.10
 ---- batch: 040 ----
mean loss: 512.46
 ---- batch: 050 ----
mean loss: 524.84
 ---- batch: 060 ----
mean loss: 507.64
 ---- batch: 070 ----
mean loss: 508.73
 ---- batch: 080 ----
mean loss: 495.67
 ---- batch: 090 ----
mean loss: 501.06
 ---- batch: 100 ----
mean loss: 493.87
 ---- batch: 110 ----
mean loss: 514.63
train mean loss: 505.49
epoch train time: 0:00:02.127969
elapsed time: 0:02:03.604847
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-26 23:49:46.688476
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 510.38
 ---- batch: 020 ----
mean loss: 505.72
 ---- batch: 030 ----
mean loss: 513.12
 ---- batch: 040 ----
mean loss: 506.34
 ---- batch: 050 ----
mean loss: 510.69
 ---- batch: 060 ----
mean loss: 506.36
 ---- batch: 070 ----
mean loss: 505.18
 ---- batch: 080 ----
mean loss: 522.91
 ---- batch: 090 ----
mean loss: 497.99
 ---- batch: 100 ----
mean loss: 503.24
 ---- batch: 110 ----
mean loss: 512.97
train mean loss: 508.45
epoch train time: 0:00:02.121241
elapsed time: 0:02:05.726321
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-26 23:49:48.809940
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.90
 ---- batch: 020 ----
mean loss: 510.75
 ---- batch: 030 ----
mean loss: 502.41
 ---- batch: 040 ----
mean loss: 507.56
 ---- batch: 050 ----
mean loss: 508.57
 ---- batch: 060 ----
mean loss: 531.96
 ---- batch: 070 ----
mean loss: 502.95
 ---- batch: 080 ----
mean loss: 503.89
 ---- batch: 090 ----
mean loss: 505.19
 ---- batch: 100 ----
mean loss: 493.31
 ---- batch: 110 ----
mean loss: 500.05
train mean loss: 506.80
epoch train time: 0:00:02.123778
elapsed time: 0:02:07.850322
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-26 23:49:50.933913
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 508.98
 ---- batch: 020 ----
mean loss: 522.52
 ---- batch: 030 ----
mean loss: 506.25
 ---- batch: 040 ----
mean loss: 511.42
 ---- batch: 050 ----
mean loss: 494.26
 ---- batch: 060 ----
mean loss: 497.90
 ---- batch: 070 ----
mean loss: 494.73
 ---- batch: 080 ----
mean loss: 501.14
 ---- batch: 090 ----
mean loss: 504.26
 ---- batch: 100 ----
mean loss: 502.82
 ---- batch: 110 ----
mean loss: 492.28
train mean loss: 503.22
epoch train time: 0:00:02.129929
elapsed time: 0:02:09.980410
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-26 23:49:53.063990
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.69
 ---- batch: 020 ----
mean loss: 507.58
 ---- batch: 030 ----
mean loss: 502.70
 ---- batch: 040 ----
mean loss: 510.01
 ---- batch: 050 ----
mean loss: 509.10
 ---- batch: 060 ----
mean loss: 499.35
 ---- batch: 070 ----
mean loss: 516.84
 ---- batch: 080 ----
mean loss: 494.89
 ---- batch: 090 ----
mean loss: 492.97
 ---- batch: 100 ----
mean loss: 512.13
 ---- batch: 110 ----
mean loss: 499.85
train mean loss: 504.41
epoch train time: 0:00:02.123392
elapsed time: 0:02:12.104025
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-26 23:49:55.187619
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.23
 ---- batch: 020 ----
mean loss: 511.71
 ---- batch: 030 ----
mean loss: 507.79
 ---- batch: 040 ----
mean loss: 510.36
 ---- batch: 050 ----
mean loss: 512.88
 ---- batch: 060 ----
mean loss: 498.05
 ---- batch: 070 ----
mean loss: 498.66
 ---- batch: 080 ----
mean loss: 508.22
 ---- batch: 090 ----
mean loss: 516.60
 ---- batch: 100 ----
mean loss: 483.67
 ---- batch: 110 ----
mean loss: 498.28
train mean loss: 504.66
epoch train time: 0:00:02.123529
elapsed time: 0:02:14.227712
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-26 23:49:57.311289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.85
 ---- batch: 020 ----
mean loss: 506.00
 ---- batch: 030 ----
mean loss: 503.32
 ---- batch: 040 ----
mean loss: 512.03
 ---- batch: 050 ----
mean loss: 499.02
 ---- batch: 060 ----
mean loss: 506.13
 ---- batch: 070 ----
mean loss: 500.68
 ---- batch: 080 ----
mean loss: 527.98
 ---- batch: 090 ----
mean loss: 503.16
 ---- batch: 100 ----
mean loss: 494.83
 ---- batch: 110 ----
mean loss: 487.08
train mean loss: 504.76
epoch train time: 0:00:02.130783
elapsed time: 0:02:16.358640
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-26 23:49:59.442218
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.07
 ---- batch: 020 ----
mean loss: 503.05
 ---- batch: 030 ----
mean loss: 502.97
 ---- batch: 040 ----
mean loss: 527.34
 ---- batch: 050 ----
mean loss: 513.22
 ---- batch: 060 ----
mean loss: 488.07
 ---- batch: 070 ----
mean loss: 497.78
 ---- batch: 080 ----
mean loss: 502.80
 ---- batch: 090 ----
mean loss: 518.25
 ---- batch: 100 ----
mean loss: 509.24
 ---- batch: 110 ----
mean loss: 503.35
train mean loss: 506.54
epoch train time: 0:00:02.128059
elapsed time: 0:02:18.486846
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-26 23:50:01.570475
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.62
 ---- batch: 020 ----
mean loss: 491.32
 ---- batch: 030 ----
mean loss: 501.04
 ---- batch: 040 ----
mean loss: 507.62
 ---- batch: 050 ----
mean loss: 497.41
 ---- batch: 060 ----
mean loss: 493.79
 ---- batch: 070 ----
mean loss: 507.64
 ---- batch: 080 ----
mean loss: 489.48
 ---- batch: 090 ----
mean loss: 503.33
 ---- batch: 100 ----
mean loss: 502.79
 ---- batch: 110 ----
mean loss: 518.70
train mean loss: 500.51
epoch train time: 0:00:02.122686
elapsed time: 0:02:20.609788
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-26 23:50:03.693386
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 503.34
 ---- batch: 020 ----
mean loss: 509.15
 ---- batch: 030 ----
mean loss: 494.41
 ---- batch: 040 ----
mean loss: 494.28
 ---- batch: 050 ----
mean loss: 501.08
 ---- batch: 060 ----
mean loss: 516.83
 ---- batch: 070 ----
mean loss: 501.12
 ---- batch: 080 ----
mean loss: 490.51
 ---- batch: 090 ----
mean loss: 504.61
 ---- batch: 100 ----
mean loss: 517.84
 ---- batch: 110 ----
mean loss: 501.06
train mean loss: 503.02
epoch train time: 0:00:02.123930
elapsed time: 0:02:22.733886
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-26 23:50:05.817470
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 518.63
 ---- batch: 020 ----
mean loss: 497.00
 ---- batch: 030 ----
mean loss: 515.93
 ---- batch: 040 ----
mean loss: 501.86
 ---- batch: 050 ----
mean loss: 499.75
 ---- batch: 060 ----
mean loss: 510.31
 ---- batch: 070 ----
mean loss: 490.70
 ---- batch: 080 ----
mean loss: 499.75
 ---- batch: 090 ----
mean loss: 493.72
 ---- batch: 100 ----
mean loss: 507.74
 ---- batch: 110 ----
mean loss: 511.77
train mean loss: 504.76
epoch train time: 0:00:02.123122
elapsed time: 0:02:24.857165
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-26 23:50:07.940749
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 512.24
 ---- batch: 020 ----
mean loss: 512.58
 ---- batch: 030 ----
mean loss: 509.89
 ---- batch: 040 ----
mean loss: 493.83
 ---- batch: 050 ----
mean loss: 500.32
 ---- batch: 060 ----
mean loss: 505.33
 ---- batch: 070 ----
mean loss: 506.51
 ---- batch: 080 ----
mean loss: 502.33
 ---- batch: 090 ----
mean loss: 491.39
 ---- batch: 100 ----
mean loss: 500.82
 ---- batch: 110 ----
mean loss: 499.18
train mean loss: 502.40
epoch train time: 0:00:02.120836
elapsed time: 0:02:26.978171
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-26 23:50:10.061753
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 507.46
 ---- batch: 020 ----
mean loss: 506.32
 ---- batch: 030 ----
mean loss: 498.11
 ---- batch: 040 ----
mean loss: 506.49
 ---- batch: 050 ----
mean loss: 502.49
 ---- batch: 060 ----
mean loss: 489.84
 ---- batch: 070 ----
mean loss: 495.75
 ---- batch: 080 ----
mean loss: 496.90
 ---- batch: 090 ----
mean loss: 494.03
 ---- batch: 100 ----
mean loss: 503.03
 ---- batch: 110 ----
mean loss: 509.19
train mean loss: 501.35
epoch train time: 0:00:02.122252
elapsed time: 0:02:29.100574
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-26 23:50:12.184159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.17
 ---- batch: 020 ----
mean loss: 498.13
 ---- batch: 030 ----
mean loss: 517.35
 ---- batch: 040 ----
mean loss: 487.43
 ---- batch: 050 ----
mean loss: 491.70
 ---- batch: 060 ----
mean loss: 515.81
 ---- batch: 070 ----
mean loss: 500.32
 ---- batch: 080 ----
mean loss: 481.79
 ---- batch: 090 ----
mean loss: 508.98
 ---- batch: 100 ----
mean loss: 505.79
 ---- batch: 110 ----
mean loss: 508.22
train mean loss: 502.56
epoch train time: 0:00:02.123145
elapsed time: 0:02:31.223871
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-26 23:50:14.307482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.52
 ---- batch: 020 ----
mean loss: 517.51
 ---- batch: 030 ----
mean loss: 530.65
 ---- batch: 040 ----
mean loss: 499.67
 ---- batch: 050 ----
mean loss: 510.53
 ---- batch: 060 ----
mean loss: 493.09
 ---- batch: 070 ----
mean loss: 510.73
 ---- batch: 080 ----
mean loss: 498.26
 ---- batch: 090 ----
mean loss: 488.71
 ---- batch: 100 ----
mean loss: 493.71
 ---- batch: 110 ----
mean loss: 505.06
train mean loss: 505.66
epoch train time: 0:00:02.127013
elapsed time: 0:02:33.351065
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-26 23:50:16.434666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.81
 ---- batch: 020 ----
mean loss: 497.42
 ---- batch: 030 ----
mean loss: 499.97
 ---- batch: 040 ----
mean loss: 493.25
 ---- batch: 050 ----
mean loss: 500.58
 ---- batch: 060 ----
mean loss: 508.99
 ---- batch: 070 ----
mean loss: 496.98
 ---- batch: 080 ----
mean loss: 492.92
 ---- batch: 090 ----
mean loss: 522.18
 ---- batch: 100 ----
mean loss: 508.31
 ---- batch: 110 ----
mean loss: 505.64
train mean loss: 501.70
epoch train time: 0:00:02.126138
elapsed time: 0:02:35.477423
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-26 23:50:18.561004
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.16
 ---- batch: 020 ----
mean loss: 516.44
 ---- batch: 030 ----
mean loss: 502.06
 ---- batch: 040 ----
mean loss: 499.75
 ---- batch: 050 ----
mean loss: 489.69
 ---- batch: 060 ----
mean loss: 484.07
 ---- batch: 070 ----
mean loss: 513.11
 ---- batch: 080 ----
mean loss: 493.88
 ---- batch: 090 ----
mean loss: 498.25
 ---- batch: 100 ----
mean loss: 515.07
 ---- batch: 110 ----
mean loss: 510.41
train mean loss: 501.41
epoch train time: 0:00:02.124102
elapsed time: 0:02:37.601710
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-26 23:50:20.685306
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 491.27
 ---- batch: 020 ----
mean loss: 492.06
 ---- batch: 030 ----
mean loss: 503.76
 ---- batch: 040 ----
mean loss: 492.29
 ---- batch: 050 ----
mean loss: 496.42
 ---- batch: 060 ----
mean loss: 512.30
 ---- batch: 070 ----
mean loss: 492.12
 ---- batch: 080 ----
mean loss: 519.23
 ---- batch: 090 ----
mean loss: 507.55
 ---- batch: 100 ----
mean loss: 490.42
 ---- batch: 110 ----
mean loss: 508.07
train mean loss: 500.74
epoch train time: 0:00:02.126049
elapsed time: 0:02:39.727960
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-26 23:50:22.811542
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.27
 ---- batch: 020 ----
mean loss: 515.21
 ---- batch: 030 ----
mean loss: 509.85
 ---- batch: 040 ----
mean loss: 494.66
 ---- batch: 050 ----
mean loss: 506.84
 ---- batch: 060 ----
mean loss: 502.12
 ---- batch: 070 ----
mean loss: 499.52
 ---- batch: 080 ----
mean loss: 504.93
 ---- batch: 090 ----
mean loss: 484.68
 ---- batch: 100 ----
mean loss: 502.31
 ---- batch: 110 ----
mean loss: 482.74
train mean loss: 500.36
epoch train time: 0:00:02.125441
elapsed time: 0:02:41.853562
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-26 23:50:24.937144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 500.08
 ---- batch: 020 ----
mean loss: 490.97
 ---- batch: 030 ----
mean loss: 511.64
 ---- batch: 040 ----
mean loss: 495.46
 ---- batch: 050 ----
mean loss: 511.55
 ---- batch: 060 ----
mean loss: 481.23
 ---- batch: 070 ----
mean loss: 502.52
 ---- batch: 080 ----
mean loss: 496.06
 ---- batch: 090 ----
mean loss: 494.99
 ---- batch: 100 ----
mean loss: 485.80
 ---- batch: 110 ----
mean loss: 507.20
train mean loss: 496.82
epoch train time: 0:00:02.121095
elapsed time: 0:02:43.974859
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-26 23:50:27.058449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.17
 ---- batch: 020 ----
mean loss: 487.19
 ---- batch: 030 ----
mean loss: 492.09
 ---- batch: 040 ----
mean loss: 497.84
 ---- batch: 050 ----
mean loss: 501.41
 ---- batch: 060 ----
mean loss: 503.38
 ---- batch: 070 ----
mean loss: 485.32
 ---- batch: 080 ----
mean loss: 504.86
 ---- batch: 090 ----
mean loss: 496.55
 ---- batch: 100 ----
mean loss: 505.40
 ---- batch: 110 ----
mean loss: 507.55
train mean loss: 497.83
epoch train time: 0:00:02.121362
elapsed time: 0:02:46.096387
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-26 23:50:29.179974
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 511.79
 ---- batch: 020 ----
mean loss: 480.59
 ---- batch: 030 ----
mean loss: 482.05
 ---- batch: 040 ----
mean loss: 491.19
 ---- batch: 050 ----
mean loss: 495.75
 ---- batch: 060 ----
mean loss: 500.05
 ---- batch: 070 ----
mean loss: 493.16
 ---- batch: 080 ----
mean loss: 506.81
 ---- batch: 090 ----
mean loss: 491.40
 ---- batch: 100 ----
mean loss: 510.83
 ---- batch: 110 ----
mean loss: 502.25
train mean loss: 495.84
epoch train time: 0:00:02.125104
elapsed time: 0:02:48.221647
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-26 23:50:31.305248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.74
 ---- batch: 020 ----
mean loss: 490.59
 ---- batch: 030 ----
mean loss: 492.90
 ---- batch: 040 ----
mean loss: 497.25
 ---- batch: 050 ----
mean loss: 492.10
 ---- batch: 060 ----
mean loss: 498.65
 ---- batch: 070 ----
mean loss: 482.43
 ---- batch: 080 ----
mean loss: 491.69
 ---- batch: 090 ----
mean loss: 511.38
 ---- batch: 100 ----
mean loss: 505.80
 ---- batch: 110 ----
mean loss: 486.52
train mean loss: 495.73
epoch train time: 0:00:02.126440
elapsed time: 0:02:50.348259
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-26 23:50:33.431873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.25
 ---- batch: 020 ----
mean loss: 479.91
 ---- batch: 030 ----
mean loss: 484.74
 ---- batch: 040 ----
mean loss: 497.82
 ---- batch: 050 ----
mean loss: 502.49
 ---- batch: 060 ----
mean loss: 491.11
 ---- batch: 070 ----
mean loss: 479.62
 ---- batch: 080 ----
mean loss: 493.46
 ---- batch: 090 ----
mean loss: 490.35
 ---- batch: 100 ----
mean loss: 483.28
 ---- batch: 110 ----
mean loss: 500.98
train mean loss: 492.63
epoch train time: 0:00:02.125254
elapsed time: 0:02:52.473732
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-26 23:50:35.557312
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.28
 ---- batch: 020 ----
mean loss: 499.37
 ---- batch: 030 ----
mean loss: 501.79
 ---- batch: 040 ----
mean loss: 498.46
 ---- batch: 050 ----
mean loss: 485.06
 ---- batch: 060 ----
mean loss: 506.14
 ---- batch: 070 ----
mean loss: 488.60
 ---- batch: 080 ----
mean loss: 505.92
 ---- batch: 090 ----
mean loss: 496.92
 ---- batch: 100 ----
mean loss: 486.19
 ---- batch: 110 ----
mean loss: 494.76
train mean loss: 495.38
epoch train time: 0:00:02.121566
elapsed time: 0:02:54.595471
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-26 23:50:37.679056
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 489.89
 ---- batch: 020 ----
mean loss: 489.75
 ---- batch: 030 ----
mean loss: 484.16
 ---- batch: 040 ----
mean loss: 480.38
 ---- batch: 050 ----
mean loss: 504.28
 ---- batch: 060 ----
mean loss: 494.50
 ---- batch: 070 ----
mean loss: 503.80
 ---- batch: 080 ----
mean loss: 506.32
 ---- batch: 090 ----
mean loss: 490.99
 ---- batch: 100 ----
mean loss: 501.71
 ---- batch: 110 ----
mean loss: 492.31
train mean loss: 494.61
epoch train time: 0:00:02.121426
elapsed time: 0:02:56.717089
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-26 23:50:39.800673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.96
 ---- batch: 020 ----
mean loss: 481.74
 ---- batch: 030 ----
mean loss: 502.73
 ---- batch: 040 ----
mean loss: 497.07
 ---- batch: 050 ----
mean loss: 521.51
 ---- batch: 060 ----
mean loss: 519.73
 ---- batch: 070 ----
mean loss: 503.87
 ---- batch: 080 ----
mean loss: 494.64
 ---- batch: 090 ----
mean loss: 484.55
 ---- batch: 100 ----
mean loss: 480.58
 ---- batch: 110 ----
mean loss: 513.40
train mean loss: 499.28
epoch train time: 0:00:02.121254
elapsed time: 0:02:58.838505
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-26 23:50:41.922088
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 499.85
 ---- batch: 020 ----
mean loss: 495.10
 ---- batch: 030 ----
mean loss: 505.96
 ---- batch: 040 ----
mean loss: 492.95
 ---- batch: 050 ----
mean loss: 503.46
 ---- batch: 060 ----
mean loss: 509.93
 ---- batch: 070 ----
mean loss: 487.34
 ---- batch: 080 ----
mean loss: 482.56
 ---- batch: 090 ----
mean loss: 504.13
 ---- batch: 100 ----
mean loss: 493.58
 ---- batch: 110 ----
mean loss: 504.74
train mean loss: 498.72
epoch train time: 0:00:02.133310
elapsed time: 0:03:00.971989
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-26 23:50:44.055588
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 501.04
 ---- batch: 020 ----
mean loss: 491.97
 ---- batch: 030 ----
mean loss: 494.29
 ---- batch: 040 ----
mean loss: 507.48
 ---- batch: 050 ----
mean loss: 487.14
 ---- batch: 060 ----
mean loss: 479.90
 ---- batch: 070 ----
mean loss: 497.83
 ---- batch: 080 ----
mean loss: 505.66
 ---- batch: 090 ----
mean loss: 492.11
 ---- batch: 100 ----
mean loss: 499.32
 ---- batch: 110 ----
mean loss: 503.88
train mean loss: 496.73
epoch train time: 0:00:02.134178
elapsed time: 0:03:03.106344
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-26 23:50:46.189937
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 495.10
 ---- batch: 020 ----
mean loss: 502.28
 ---- batch: 030 ----
mean loss: 505.77
 ---- batch: 040 ----
mean loss: 481.08
 ---- batch: 050 ----
mean loss: 476.10
 ---- batch: 060 ----
mean loss: 493.19
 ---- batch: 070 ----
mean loss: 483.76
 ---- batch: 080 ----
mean loss: 507.74
 ---- batch: 090 ----
mean loss: 489.56
 ---- batch: 100 ----
mean loss: 493.53
 ---- batch: 110 ----
mean loss: 506.15
train mean loss: 494.55
epoch train time: 0:00:02.127575
elapsed time: 0:03:05.234106
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-26 23:50:48.317698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.97
 ---- batch: 020 ----
mean loss: 478.62
 ---- batch: 030 ----
mean loss: 498.71
 ---- batch: 040 ----
mean loss: 494.85
 ---- batch: 050 ----
mean loss: 486.41
 ---- batch: 060 ----
mean loss: 494.74
 ---- batch: 070 ----
mean loss: 479.66
 ---- batch: 080 ----
mean loss: 509.20
 ---- batch: 090 ----
mean loss: 494.13
 ---- batch: 100 ----
mean loss: 496.98
 ---- batch: 110 ----
mean loss: 468.70
train mean loss: 488.79
epoch train time: 0:00:02.130346
elapsed time: 0:03:07.364618
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-26 23:50:50.448201
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 504.12
 ---- batch: 020 ----
mean loss: 518.85
 ---- batch: 030 ----
mean loss: 472.27
 ---- batch: 040 ----
mean loss: 482.27
 ---- batch: 050 ----
mean loss: 494.25
 ---- batch: 060 ----
mean loss: 497.90
 ---- batch: 070 ----
mean loss: 487.16
 ---- batch: 080 ----
mean loss: 500.95
 ---- batch: 090 ----
mean loss: 500.61
 ---- batch: 100 ----
mean loss: 505.87
 ---- batch: 110 ----
mean loss: 498.02
train mean loss: 496.21
epoch train time: 0:00:02.133301
elapsed time: 0:03:09.498098
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-26 23:50:52.581690
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 497.06
 ---- batch: 020 ----
mean loss: 495.03
 ---- batch: 030 ----
mean loss: 488.35
 ---- batch: 040 ----
mean loss: 481.35
 ---- batch: 050 ----
mean loss: 481.74
 ---- batch: 060 ----
mean loss: 493.26
 ---- batch: 070 ----
mean loss: 497.49
 ---- batch: 080 ----
mean loss: 503.10
 ---- batch: 090 ----
mean loss: 491.38
 ---- batch: 100 ----
mean loss: 502.58
 ---- batch: 110 ----
mean loss: 488.08
train mean loss: 492.94
epoch train time: 0:00:02.121924
elapsed time: 0:03:11.620181
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-26 23:50:54.703760
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.82
 ---- batch: 020 ----
mean loss: 501.57
 ---- batch: 030 ----
mean loss: 488.16
 ---- batch: 040 ----
mean loss: 494.65
 ---- batch: 050 ----
mean loss: 509.47
 ---- batch: 060 ----
mean loss: 482.70
 ---- batch: 070 ----
mean loss: 478.01
 ---- batch: 080 ----
mean loss: 481.19
 ---- batch: 090 ----
mean loss: 500.43
 ---- batch: 100 ----
mean loss: 488.86
 ---- batch: 110 ----
mean loss: 470.94
train mean loss: 489.97
epoch train time: 0:00:02.125591
elapsed time: 0:03:13.745930
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-26 23:50:56.829516
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 483.10
 ---- batch: 020 ----
mean loss: 468.53
 ---- batch: 030 ----
mean loss: 483.39
 ---- batch: 040 ----
mean loss: 485.13
 ---- batch: 050 ----
mean loss: 485.07
 ---- batch: 060 ----
mean loss: 481.54
 ---- batch: 070 ----
mean loss: 494.17
 ---- batch: 080 ----
mean loss: 490.53
 ---- batch: 090 ----
mean loss: 482.35
 ---- batch: 100 ----
mean loss: 486.88
 ---- batch: 110 ----
mean loss: 482.64
train mean loss: 483.98
epoch train time: 0:00:02.122701
elapsed time: 0:03:15.868788
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-26 23:50:58.952396
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 493.11
 ---- batch: 020 ----
mean loss: 481.46
 ---- batch: 030 ----
mean loss: 484.38
 ---- batch: 040 ----
mean loss: 500.77
 ---- batch: 050 ----
mean loss: 504.48
 ---- batch: 060 ----
mean loss: 514.51
 ---- batch: 070 ----
mean loss: 493.09
 ---- batch: 080 ----
mean loss: 485.07
 ---- batch: 090 ----
mean loss: 488.54
 ---- batch: 100 ----
mean loss: 496.85
 ---- batch: 110 ----
mean loss: 502.08
train mean loss: 494.77
epoch train time: 0:00:02.125762
elapsed time: 0:03:17.994732
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-26 23:51:01.078313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 484.24
 ---- batch: 020 ----
mean loss: 483.94
 ---- batch: 030 ----
mean loss: 478.42
 ---- batch: 040 ----
mean loss: 490.44
 ---- batch: 050 ----
mean loss: 503.95
 ---- batch: 060 ----
mean loss: 473.91
 ---- batch: 070 ----
mean loss: 493.63
 ---- batch: 080 ----
mean loss: 490.67
 ---- batch: 090 ----
mean loss: 484.15
 ---- batch: 100 ----
mean loss: 477.38
 ---- batch: 110 ----
mean loss: 478.34
train mean loss: 485.75
epoch train time: 0:00:02.124050
elapsed time: 0:03:20.118947
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-26 23:51:03.202532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 492.99
 ---- batch: 020 ----
mean loss: 487.40
 ---- batch: 030 ----
mean loss: 482.54
 ---- batch: 040 ----
mean loss: 481.31
 ---- batch: 050 ----
mean loss: 479.36
 ---- batch: 060 ----
mean loss: 481.31
 ---- batch: 070 ----
mean loss: 493.51
 ---- batch: 080 ----
mean loss: 504.18
 ---- batch: 090 ----
mean loss: 509.50
 ---- batch: 100 ----
mean loss: 477.27
 ---- batch: 110 ----
mean loss: 505.94
train mean loss: 490.62
epoch train time: 0:00:02.127442
elapsed time: 0:03:22.246547
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-26 23:51:05.330128
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.75
 ---- batch: 020 ----
mean loss: 495.28
 ---- batch: 030 ----
mean loss: 477.53
 ---- batch: 040 ----
mean loss: 476.62
 ---- batch: 050 ----
mean loss: 490.91
 ---- batch: 060 ----
mean loss: 483.11
 ---- batch: 070 ----
mean loss: 495.25
 ---- batch: 080 ----
mean loss: 488.72
 ---- batch: 090 ----
mean loss: 497.62
 ---- batch: 100 ----
mean loss: 497.68
 ---- batch: 110 ----
mean loss: 471.67
train mean loss: 486.20
epoch train time: 0:00:02.122110
elapsed time: 0:03:24.368811
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-26 23:51:07.452395
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.64
 ---- batch: 020 ----
mean loss: 484.02
 ---- batch: 030 ----
mean loss: 476.73
 ---- batch: 040 ----
mean loss: 484.23
 ---- batch: 050 ----
mean loss: 485.66
 ---- batch: 060 ----
mean loss: 486.72
 ---- batch: 070 ----
mean loss: 483.01
 ---- batch: 080 ----
mean loss: 514.47
 ---- batch: 090 ----
mean loss: 502.68
 ---- batch: 100 ----
mean loss: 476.10
 ---- batch: 110 ----
mean loss: 485.60
train mean loss: 486.31
epoch train time: 0:00:02.123289
elapsed time: 0:03:26.492257
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-26 23:51:09.575839
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.65
 ---- batch: 020 ----
mean loss: 497.18
 ---- batch: 030 ----
mean loss: 489.66
 ---- batch: 040 ----
mean loss: 476.92
 ---- batch: 050 ----
mean loss: 488.21
 ---- batch: 060 ----
mean loss: 491.68
 ---- batch: 070 ----
mean loss: 501.67
 ---- batch: 080 ----
mean loss: 476.11
 ---- batch: 090 ----
mean loss: 477.75
 ---- batch: 100 ----
mean loss: 492.05
 ---- batch: 110 ----
mean loss: 497.15
train mean loss: 486.66
epoch train time: 0:00:02.130990
elapsed time: 0:03:28.623418
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-26 23:51:11.707032
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 482.40
 ---- batch: 020 ----
mean loss: 491.28
 ---- batch: 030 ----
mean loss: 480.36
 ---- batch: 040 ----
mean loss: 480.35
 ---- batch: 050 ----
mean loss: 485.68
 ---- batch: 060 ----
mean loss: 496.60
 ---- batch: 070 ----
mean loss: 489.76
 ---- batch: 080 ----
mean loss: 495.15
 ---- batch: 090 ----
mean loss: 502.34
 ---- batch: 100 ----
mean loss: 492.66
 ---- batch: 110 ----
mean loss: 488.71
train mean loss: 490.23
epoch train time: 0:00:02.128998
elapsed time: 0:03:30.752618
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-26 23:51:13.836229
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 481.96
 ---- batch: 020 ----
mean loss: 479.50
 ---- batch: 030 ----
mean loss: 473.26
 ---- batch: 040 ----
mean loss: 479.76
 ---- batch: 050 ----
mean loss: 472.51
 ---- batch: 060 ----
mean loss: 490.41
 ---- batch: 070 ----
mean loss: 486.50
 ---- batch: 080 ----
mean loss: 487.11
 ---- batch: 090 ----
mean loss: 481.31
 ---- batch: 100 ----
mean loss: 484.41
 ---- batch: 110 ----
mean loss: 476.15
train mean loss: 481.36
epoch train time: 0:00:02.127839
elapsed time: 0:03:32.880650
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-26 23:51:15.964248
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.03
 ---- batch: 020 ----
mean loss: 502.08
 ---- batch: 030 ----
mean loss: 464.83
 ---- batch: 040 ----
mean loss: 492.78
 ---- batch: 050 ----
mean loss: 472.50
 ---- batch: 060 ----
mean loss: 481.08
 ---- batch: 070 ----
mean loss: 479.87
 ---- batch: 080 ----
mean loss: 482.18
 ---- batch: 090 ----
mean loss: 481.21
 ---- batch: 100 ----
mean loss: 467.00
 ---- batch: 110 ----
mean loss: 473.48
train mean loss: 477.62
epoch train time: 0:00:02.129347
elapsed time: 0:03:35.010180
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-26 23:51:18.093763
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.21
 ---- batch: 020 ----
mean loss: 503.57
 ---- batch: 030 ----
mean loss: 498.88
 ---- batch: 040 ----
mean loss: 486.99
 ---- batch: 050 ----
mean loss: 485.11
 ---- batch: 060 ----
mean loss: 473.66
 ---- batch: 070 ----
mean loss: 479.94
 ---- batch: 080 ----
mean loss: 466.15
 ---- batch: 090 ----
mean loss: 494.79
 ---- batch: 100 ----
mean loss: 484.11
 ---- batch: 110 ----
mean loss: 476.86
train mean loss: 484.13
epoch train time: 0:00:02.124563
elapsed time: 0:03:37.134894
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-26 23:51:20.218482
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 496.15
 ---- batch: 020 ----
mean loss: 496.27
 ---- batch: 030 ----
mean loss: 481.67
 ---- batch: 040 ----
mean loss: 480.09
 ---- batch: 050 ----
mean loss: 492.87
 ---- batch: 060 ----
mean loss: 488.02
 ---- batch: 070 ----
mean loss: 475.62
 ---- batch: 080 ----
mean loss: 486.77
 ---- batch: 090 ----
mean loss: 464.01
 ---- batch: 100 ----
mean loss: 469.17
 ---- batch: 110 ----
mean loss: 491.27
train mean loss: 483.58
epoch train time: 0:00:02.122046
elapsed time: 0:03:39.257100
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-26 23:51:22.340681
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 468.70
 ---- batch: 020 ----
mean loss: 490.65
 ---- batch: 030 ----
mean loss: 466.54
 ---- batch: 040 ----
mean loss: 493.67
 ---- batch: 050 ----
mean loss: 480.15
 ---- batch: 060 ----
mean loss: 475.92
 ---- batch: 070 ----
mean loss: 480.22
 ---- batch: 080 ----
mean loss: 484.54
 ---- batch: 090 ----
mean loss: 501.66
 ---- batch: 100 ----
mean loss: 499.49
 ---- batch: 110 ----
mean loss: 492.12
train mean loss: 484.45
epoch train time: 0:00:02.120951
elapsed time: 0:03:41.378206
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-26 23:51:24.461807
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 469.70
 ---- batch: 020 ----
mean loss: 496.15
 ---- batch: 030 ----
mean loss: 486.77
 ---- batch: 040 ----
mean loss: 488.85
 ---- batch: 050 ----
mean loss: 491.03
 ---- batch: 060 ----
mean loss: 480.71
 ---- batch: 070 ----
mean loss: 479.33
 ---- batch: 080 ----
mean loss: 483.60
 ---- batch: 090 ----
mean loss: 481.04
 ---- batch: 100 ----
mean loss: 472.99
 ---- batch: 110 ----
mean loss: 476.02
train mean loss: 482.87
epoch train time: 0:00:02.127884
elapsed time: 0:03:43.506260
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-26 23:51:26.589868
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.37
 ---- batch: 020 ----
mean loss: 476.10
 ---- batch: 030 ----
mean loss: 479.76
 ---- batch: 040 ----
mean loss: 491.44
 ---- batch: 050 ----
mean loss: 492.44
 ---- batch: 060 ----
mean loss: 479.09
 ---- batch: 070 ----
mean loss: 476.93
 ---- batch: 080 ----
mean loss: 476.37
 ---- batch: 090 ----
mean loss: 492.87
 ---- batch: 100 ----
mean loss: 485.49
 ---- batch: 110 ----
mean loss: 480.16
train mean loss: 481.11
epoch train time: 0:00:02.126400
elapsed time: 0:03:45.632845
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-26 23:51:28.716428
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.11
 ---- batch: 020 ----
mean loss: 482.98
 ---- batch: 030 ----
mean loss: 494.25
 ---- batch: 040 ----
mean loss: 460.25
 ---- batch: 050 ----
mean loss: 489.95
 ---- batch: 060 ----
mean loss: 486.27
 ---- batch: 070 ----
mean loss: 475.82
 ---- batch: 080 ----
mean loss: 482.53
 ---- batch: 090 ----
mean loss: 474.37
 ---- batch: 100 ----
mean loss: 477.55
 ---- batch: 110 ----
mean loss: 485.85
train mean loss: 480.49
epoch train time: 0:00:02.124370
elapsed time: 0:03:47.757373
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-26 23:51:30.840955
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 475.27
 ---- batch: 020 ----
mean loss: 494.41
 ---- batch: 030 ----
mean loss: 480.66
 ---- batch: 040 ----
mean loss: 476.45
 ---- batch: 050 ----
mean loss: 474.06
 ---- batch: 060 ----
mean loss: 487.05
 ---- batch: 070 ----
mean loss: 479.74
 ---- batch: 080 ----
mean loss: 457.60
 ---- batch: 090 ----
mean loss: 477.96
 ---- batch: 100 ----
mean loss: 475.14
 ---- batch: 110 ----
mean loss: 497.45
train mean loss: 480.48
epoch train time: 0:00:02.129283
elapsed time: 0:03:49.886840
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-26 23:51:32.970440
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.79
 ---- batch: 020 ----
mean loss: 504.96
 ---- batch: 030 ----
mean loss: 485.98
 ---- batch: 040 ----
mean loss: 481.52
 ---- batch: 050 ----
mean loss: 490.65
 ---- batch: 060 ----
mean loss: 469.91
 ---- batch: 070 ----
mean loss: 488.39
 ---- batch: 080 ----
mean loss: 482.15
 ---- batch: 090 ----
mean loss: 476.23
 ---- batch: 100 ----
mean loss: 474.56
 ---- batch: 110 ----
mean loss: 473.12
train mean loss: 481.45
epoch train time: 0:00:02.126603
elapsed time: 0:03:52.013613
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-26 23:51:35.097207
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 487.47
 ---- batch: 020 ----
mean loss: 474.57
 ---- batch: 030 ----
mean loss: 480.42
 ---- batch: 040 ----
mean loss: 475.69
 ---- batch: 050 ----
mean loss: 488.29
 ---- batch: 060 ----
mean loss: 487.76
 ---- batch: 070 ----
mean loss: 493.61
 ---- batch: 080 ----
mean loss: 477.34
 ---- batch: 090 ----
mean loss: 476.73
 ---- batch: 100 ----
mean loss: 479.43
 ---- batch: 110 ----
mean loss: 490.48
train mean loss: 483.29
epoch train time: 0:00:02.120088
elapsed time: 0:03:54.133874
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-26 23:51:37.217461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.71
 ---- batch: 020 ----
mean loss: 477.75
 ---- batch: 030 ----
mean loss: 482.99
 ---- batch: 040 ----
mean loss: 479.52
 ---- batch: 050 ----
mean loss: 481.89
 ---- batch: 060 ----
mean loss: 480.75
 ---- batch: 070 ----
mean loss: 482.50
 ---- batch: 080 ----
mean loss: 491.42
 ---- batch: 090 ----
mean loss: 487.47
 ---- batch: 100 ----
mean loss: 475.25
 ---- batch: 110 ----
mean loss: 476.44
train mean loss: 480.89
epoch train time: 0:00:02.129243
elapsed time: 0:03:56.263272
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-26 23:51:39.346854
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 474.19
 ---- batch: 020 ----
mean loss: 490.51
 ---- batch: 030 ----
mean loss: 493.20
 ---- batch: 040 ----
mean loss: 484.04
 ---- batch: 050 ----
mean loss: 492.83
 ---- batch: 060 ----
mean loss: 478.55
 ---- batch: 070 ----
mean loss: 479.18
 ---- batch: 080 ----
mean loss: 479.87
 ---- batch: 090 ----
mean loss: 485.39
 ---- batch: 100 ----
mean loss: 483.20
 ---- batch: 110 ----
mean loss: 477.63
train mean loss: 483.56
epoch train time: 0:00:02.128881
elapsed time: 0:03:58.392323
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-26 23:51:41.475934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.71
 ---- batch: 020 ----
mean loss: 465.28
 ---- batch: 030 ----
mean loss: 459.52
 ---- batch: 040 ----
mean loss: 480.90
 ---- batch: 050 ----
mean loss: 487.24
 ---- batch: 060 ----
mean loss: 509.33
 ---- batch: 070 ----
mean loss: 490.92
 ---- batch: 080 ----
mean loss: 471.65
 ---- batch: 090 ----
mean loss: 479.07
 ---- batch: 100 ----
mean loss: 466.67
 ---- batch: 110 ----
mean loss: 478.30
train mean loss: 478.73
epoch train time: 0:00:02.126191
elapsed time: 0:04:00.518709
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-26 23:51:43.602310
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 477.92
 ---- batch: 020 ----
mean loss: 485.63
 ---- batch: 030 ----
mean loss: 495.48
 ---- batch: 040 ----
mean loss: 465.95
 ---- batch: 050 ----
mean loss: 485.19
 ---- batch: 060 ----
mean loss: 479.36
 ---- batch: 070 ----
mean loss: 481.31
 ---- batch: 080 ----
mean loss: 478.63
 ---- batch: 090 ----
mean loss: 482.05
 ---- batch: 100 ----
mean loss: 482.64
 ---- batch: 110 ----
mean loss: 482.83
train mean loss: 481.47
epoch train time: 0:00:02.120820
elapsed time: 0:04:02.639703
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-26 23:51:45.723285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 479.09
 ---- batch: 020 ----
mean loss: 477.41
 ---- batch: 030 ----
mean loss: 487.38
 ---- batch: 040 ----
mean loss: 465.53
 ---- batch: 050 ----
mean loss: 485.06
 ---- batch: 060 ----
mean loss: 482.71
 ---- batch: 070 ----
mean loss: 487.55
 ---- batch: 080 ----
mean loss: 476.98
 ---- batch: 090 ----
mean loss: 485.12
 ---- batch: 100 ----
mean loss: 468.89
 ---- batch: 110 ----
mean loss: 457.37
train mean loss: 477.21
epoch train time: 0:00:02.127060
elapsed time: 0:04:04.766918
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-26 23:51:47.850503
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.53
 ---- batch: 020 ----
mean loss: 475.95
 ---- batch: 030 ----
mean loss: 462.97
 ---- batch: 040 ----
mean loss: 490.78
 ---- batch: 050 ----
mean loss: 466.81
 ---- batch: 060 ----
mean loss: 468.16
 ---- batch: 070 ----
mean loss: 469.40
 ---- batch: 080 ----
mean loss: 471.20
 ---- batch: 090 ----
mean loss: 494.36
 ---- batch: 100 ----
mean loss: 475.98
 ---- batch: 110 ----
mean loss: 477.41
train mean loss: 477.14
epoch train time: 0:00:02.124057
elapsed time: 0:04:06.891145
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-26 23:51:49.974737
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.82
 ---- batch: 020 ----
mean loss: 467.15
 ---- batch: 030 ----
mean loss: 471.74
 ---- batch: 040 ----
mean loss: 472.62
 ---- batch: 050 ----
mean loss: 485.10
 ---- batch: 060 ----
mean loss: 469.87
 ---- batch: 070 ----
mean loss: 472.88
 ---- batch: 080 ----
mean loss: 485.87
 ---- batch: 090 ----
mean loss: 457.90
 ---- batch: 100 ----
mean loss: 488.14
 ---- batch: 110 ----
mean loss: 466.59
train mean loss: 474.53
epoch train time: 0:00:02.130512
elapsed time: 0:04:09.021820
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-26 23:51:52.105404
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 485.30
 ---- batch: 020 ----
mean loss: 474.66
 ---- batch: 030 ----
mean loss: 474.12
 ---- batch: 040 ----
mean loss: 478.36
 ---- batch: 050 ----
mean loss: 483.39
 ---- batch: 060 ----
mean loss: 463.19
 ---- batch: 070 ----
mean loss: 468.33
 ---- batch: 080 ----
mean loss: 489.95
 ---- batch: 090 ----
mean loss: 475.61
 ---- batch: 100 ----
mean loss: 482.16
 ---- batch: 110 ----
mean loss: 460.64
train mean loss: 475.56
epoch train time: 0:00:02.125418
elapsed time: 0:04:11.147408
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-26 23:51:54.231025
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.54
 ---- batch: 020 ----
mean loss: 459.12
 ---- batch: 030 ----
mean loss: 488.06
 ---- batch: 040 ----
mean loss: 471.02
 ---- batch: 050 ----
mean loss: 472.48
 ---- batch: 060 ----
mean loss: 470.78
 ---- batch: 070 ----
mean loss: 472.20
 ---- batch: 080 ----
mean loss: 473.21
 ---- batch: 090 ----
mean loss: 474.65
 ---- batch: 100 ----
mean loss: 472.95
 ---- batch: 110 ----
mean loss: 467.12
train mean loss: 471.46
epoch train time: 0:00:02.128727
elapsed time: 0:04:13.276343
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-26 23:51:56.359943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 478.43
 ---- batch: 020 ----
mean loss: 469.54
 ---- batch: 030 ----
mean loss: 470.93
 ---- batch: 040 ----
mean loss: 480.27
 ---- batch: 050 ----
mean loss: 468.89
 ---- batch: 060 ----
mean loss: 494.59
 ---- batch: 070 ----
mean loss: 468.01
 ---- batch: 080 ----
mean loss: 478.89
 ---- batch: 090 ----
mean loss: 484.56
 ---- batch: 100 ----
mean loss: 485.78
 ---- batch: 110 ----
mean loss: 479.63
train mean loss: 478.08
epoch train time: 0:00:02.130668
elapsed time: 0:04:15.407187
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-26 23:51:58.490772
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.44
 ---- batch: 020 ----
mean loss: 482.51
 ---- batch: 030 ----
mean loss: 479.42
 ---- batch: 040 ----
mean loss: 481.16
 ---- batch: 050 ----
mean loss: 475.24
 ---- batch: 060 ----
mean loss: 463.33
 ---- batch: 070 ----
mean loss: 497.21
 ---- batch: 080 ----
mean loss: 486.33
 ---- batch: 090 ----
mean loss: 467.05
 ---- batch: 100 ----
mean loss: 484.90
 ---- batch: 110 ----
mean loss: 466.96
train mean loss: 477.58
epoch train time: 0:00:02.132318
elapsed time: 0:04:17.539665
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-26 23:52:00.623247
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.84
 ---- batch: 020 ----
mean loss: 471.78
 ---- batch: 030 ----
mean loss: 474.61
 ---- batch: 040 ----
mean loss: 465.78
 ---- batch: 050 ----
mean loss: 474.61
 ---- batch: 060 ----
mean loss: 487.44
 ---- batch: 070 ----
mean loss: 479.82
 ---- batch: 080 ----
mean loss: 483.42
 ---- batch: 090 ----
mean loss: 461.28
 ---- batch: 100 ----
mean loss: 473.40
 ---- batch: 110 ----
mean loss: 476.14
train mean loss: 474.55
epoch train time: 0:00:02.128183
elapsed time: 0:04:19.668015
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-26 23:52:02.751586
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.56
 ---- batch: 020 ----
mean loss: 479.81
 ---- batch: 030 ----
mean loss: 466.14
 ---- batch: 040 ----
mean loss: 481.38
 ---- batch: 050 ----
mean loss: 480.19
 ---- batch: 060 ----
mean loss: 472.71
 ---- batch: 070 ----
mean loss: 471.08
 ---- batch: 080 ----
mean loss: 470.72
 ---- batch: 090 ----
mean loss: 475.00
 ---- batch: 100 ----
mean loss: 444.41
 ---- batch: 110 ----
mean loss: 478.71
train mean loss: 471.95
epoch train time: 0:00:02.139267
elapsed time: 0:04:21.807451
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-26 23:52:04.891033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 494.96
 ---- batch: 020 ----
mean loss: 469.06
 ---- batch: 030 ----
mean loss: 489.88
 ---- batch: 040 ----
mean loss: 480.80
 ---- batch: 050 ----
mean loss: 476.22
 ---- batch: 060 ----
mean loss: 488.47
 ---- batch: 070 ----
mean loss: 469.19
 ---- batch: 080 ----
mean loss: 487.17
 ---- batch: 090 ----
mean loss: 473.21
 ---- batch: 100 ----
mean loss: 474.55
 ---- batch: 110 ----
mean loss: 462.61
train mean loss: 477.63
epoch train time: 0:00:02.138367
elapsed time: 0:04:23.945976
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-26 23:52:07.029557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 471.16
 ---- batch: 020 ----
mean loss: 467.26
 ---- batch: 030 ----
mean loss: 459.74
 ---- batch: 040 ----
mean loss: 472.17
 ---- batch: 050 ----
mean loss: 486.42
 ---- batch: 060 ----
mean loss: 470.02
 ---- batch: 070 ----
mean loss: 478.05
 ---- batch: 080 ----
mean loss: 470.58
 ---- batch: 090 ----
mean loss: 464.07
 ---- batch: 100 ----
mean loss: 471.50
 ---- batch: 110 ----
mean loss: 466.99
train mean loss: 470.49
epoch train time: 0:00:02.135708
elapsed time: 0:04:26.081870
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-26 23:52:09.165478
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 472.70
 ---- batch: 020 ----
mean loss: 462.33
 ---- batch: 030 ----
mean loss: 473.53
 ---- batch: 040 ----
mean loss: 472.36
 ---- batch: 050 ----
mean loss: 475.35
 ---- batch: 060 ----
mean loss: 458.72
 ---- batch: 070 ----
mean loss: 483.10
 ---- batch: 080 ----
mean loss: 476.29
 ---- batch: 090 ----
mean loss: 470.58
 ---- batch: 100 ----
mean loss: 483.19
 ---- batch: 110 ----
mean loss: 481.79
train mean loss: 473.52
epoch train time: 0:00:02.123550
elapsed time: 0:04:28.205614
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-26 23:52:11.289213
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 464.39
 ---- batch: 020 ----
mean loss: 473.17
 ---- batch: 030 ----
mean loss: 466.44
 ---- batch: 040 ----
mean loss: 459.37
 ---- batch: 050 ----
mean loss: 478.37
 ---- batch: 060 ----
mean loss: 462.29
 ---- batch: 070 ----
mean loss: 462.18
 ---- batch: 080 ----
mean loss: 472.16
 ---- batch: 090 ----
mean loss: 460.89
 ---- batch: 100 ----
mean loss: 476.61
 ---- batch: 110 ----
mean loss: 477.15
train mean loss: 468.21
epoch train time: 0:00:02.123970
elapsed time: 0:04:30.329753
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-26 23:52:13.413336
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 470.97
 ---- batch: 020 ----
mean loss: 474.99
 ---- batch: 030 ----
mean loss: 468.65
 ---- batch: 040 ----
mean loss: 483.57
 ---- batch: 050 ----
mean loss: 456.11
 ---- batch: 060 ----
mean loss: 466.27
 ---- batch: 070 ----
mean loss: 472.09
 ---- batch: 080 ----
mean loss: 469.46
 ---- batch: 090 ----
mean loss: 477.98
 ---- batch: 100 ----
mean loss: 467.09
 ---- batch: 110 ----
mean loss: 450.32
train mean loss: 468.68
epoch train time: 0:00:02.121341
elapsed time: 0:04:32.451249
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-26 23:52:15.534829
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.76
 ---- batch: 020 ----
mean loss: 461.87
 ---- batch: 030 ----
mean loss: 485.24
 ---- batch: 040 ----
mean loss: 469.75
 ---- batch: 050 ----
mean loss: 470.13
 ---- batch: 060 ----
mean loss: 461.48
 ---- batch: 070 ----
mean loss: 456.78
 ---- batch: 080 ----
mean loss: 477.25
 ---- batch: 090 ----
mean loss: 468.00
 ---- batch: 100 ----
mean loss: 459.24
 ---- batch: 110 ----
mean loss: 459.09
train mean loss: 467.30
epoch train time: 0:00:02.124562
elapsed time: 0:04:34.575966
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-26 23:52:17.659550
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 462.40
 ---- batch: 020 ----
mean loss: 472.18
 ---- batch: 030 ----
mean loss: 460.28
 ---- batch: 040 ----
mean loss: 481.58
 ---- batch: 050 ----
mean loss: 482.19
 ---- batch: 060 ----
mean loss: 478.69
 ---- batch: 070 ----
mean loss: 479.82
 ---- batch: 080 ----
mean loss: 465.97
 ---- batch: 090 ----
mean loss: 461.65
 ---- batch: 100 ----
mean loss: 468.51
 ---- batch: 110 ----
mean loss: 464.20
train mean loss: 470.50
epoch train time: 0:00:02.134845
elapsed time: 0:04:36.710996
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-26 23:52:19.794624
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.18
 ---- batch: 020 ----
mean loss: 466.43
 ---- batch: 030 ----
mean loss: 468.69
 ---- batch: 040 ----
mean loss: 453.95
 ---- batch: 050 ----
mean loss: 454.08
 ---- batch: 060 ----
mean loss: 470.76
 ---- batch: 070 ----
mean loss: 455.98
 ---- batch: 080 ----
mean loss: 468.59
 ---- batch: 090 ----
mean loss: 472.72
 ---- batch: 100 ----
mean loss: 465.94
 ---- batch: 110 ----
mean loss: 458.90
train mean loss: 463.58
epoch train time: 0:00:02.137488
elapsed time: 0:04:38.848687
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-26 23:52:21.932285
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 467.41
 ---- batch: 020 ----
mean loss: 483.11
 ---- batch: 030 ----
mean loss: 468.04
 ---- batch: 040 ----
mean loss: 466.36
 ---- batch: 050 ----
mean loss: 457.54
 ---- batch: 060 ----
mean loss: 481.04
 ---- batch: 070 ----
mean loss: 464.43
 ---- batch: 080 ----
mean loss: 464.31
 ---- batch: 090 ----
mean loss: 464.92
 ---- batch: 100 ----
mean loss: 460.49
 ---- batch: 110 ----
mean loss: 461.64
train mean loss: 466.61
epoch train time: 0:00:02.135276
elapsed time: 0:04:40.984137
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-26 23:52:24.067736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 465.23
 ---- batch: 020 ----
mean loss: 466.73
 ---- batch: 030 ----
mean loss: 477.97
 ---- batch: 040 ----
mean loss: 456.18
 ---- batch: 050 ----
mean loss: 445.40
 ---- batch: 060 ----
mean loss: 467.14
 ---- batch: 070 ----
mean loss: 468.97
 ---- batch: 080 ----
mean loss: 474.44
 ---- batch: 090 ----
mean loss: 450.76
 ---- batch: 100 ----
mean loss: 459.30
 ---- batch: 110 ----
mean loss: 465.36
train mean loss: 464.28
epoch train time: 0:00:02.138676
elapsed time: 0:04:43.123016
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-26 23:52:26.206598
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 452.44
 ---- batch: 020 ----
mean loss: 470.85
 ---- batch: 030 ----
mean loss: 464.84
 ---- batch: 040 ----
mean loss: 480.05
 ---- batch: 050 ----
mean loss: 467.00
 ---- batch: 060 ----
mean loss: 455.63
 ---- batch: 070 ----
mean loss: 470.25
 ---- batch: 080 ----
mean loss: 484.59
 ---- batch: 090 ----
mean loss: 470.29
 ---- batch: 100 ----
mean loss: 470.00
 ---- batch: 110 ----
mean loss: 454.46
train mean loss: 467.53
epoch train time: 0:00:02.138515
elapsed time: 0:04:45.261687
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-26 23:52:28.345298
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 473.59
 ---- batch: 020 ----
mean loss: 454.35
 ---- batch: 030 ----
mean loss: 451.38
 ---- batch: 040 ----
mean loss: 457.64
 ---- batch: 050 ----
mean loss: 486.51
 ---- batch: 060 ----
mean loss: 473.61
 ---- batch: 070 ----
mean loss: 486.41
 ---- batch: 080 ----
mean loss: 466.68
 ---- batch: 090 ----
mean loss: 477.46
 ---- batch: 100 ----
mean loss: 458.59
 ---- batch: 110 ----
mean loss: 469.06
train mean loss: 468.12
epoch train time: 0:00:02.124163
elapsed time: 0:04:47.386031
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-26 23:52:30.469636
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.48
 ---- batch: 020 ----
mean loss: 460.13
 ---- batch: 030 ----
mean loss: 463.63
 ---- batch: 040 ----
mean loss: 472.42
 ---- batch: 050 ----
mean loss: 465.41
 ---- batch: 060 ----
mean loss: 459.93
 ---- batch: 070 ----
mean loss: 462.73
 ---- batch: 080 ----
mean loss: 465.77
 ---- batch: 090 ----
mean loss: 468.18
 ---- batch: 100 ----
mean loss: 465.22
 ---- batch: 110 ----
mean loss: 471.09
train mean loss: 463.85
epoch train time: 0:00:02.128003
elapsed time: 0:04:49.514208
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-26 23:52:32.597788
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 453.39
 ---- batch: 020 ----
mean loss: 460.39
 ---- batch: 030 ----
mean loss: 458.53
 ---- batch: 040 ----
mean loss: 477.57
 ---- batch: 050 ----
mean loss: 451.14
 ---- batch: 060 ----
mean loss: 475.15
 ---- batch: 070 ----
mean loss: 464.14
 ---- batch: 080 ----
mean loss: 452.72
 ---- batch: 090 ----
mean loss: 460.67
 ---- batch: 100 ----
mean loss: 463.36
 ---- batch: 110 ----
mean loss: 457.14
train mean loss: 461.06
epoch train time: 0:00:02.123635
elapsed time: 0:04:51.637990
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-26 23:52:34.721592
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 476.73
 ---- batch: 020 ----
mean loss: 462.79
 ---- batch: 030 ----
mean loss: 448.82
 ---- batch: 040 ----
mean loss: 460.42
 ---- batch: 050 ----
mean loss: 461.91
 ---- batch: 060 ----
mean loss: 455.22
 ---- batch: 070 ----
mean loss: 465.34
 ---- batch: 080 ----
mean loss: 471.24
 ---- batch: 090 ----
mean loss: 441.81
 ---- batch: 100 ----
mean loss: 462.83
 ---- batch: 110 ----
mean loss: 454.60
train mean loss: 459.44
epoch train time: 0:00:02.131633
elapsed time: 0:04:53.769801
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-26 23:52:36.853406
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 458.15
 ---- batch: 020 ----
mean loss: 446.29
 ---- batch: 030 ----
mean loss: 458.14
 ---- batch: 040 ----
mean loss: 457.73
 ---- batch: 050 ----
mean loss: 445.88
 ---- batch: 060 ----
mean loss: 456.43
 ---- batch: 070 ----
mean loss: 461.00
 ---- batch: 080 ----
mean loss: 455.30
 ---- batch: 090 ----
mean loss: 472.56
 ---- batch: 100 ----
mean loss: 446.64
 ---- batch: 110 ----
mean loss: 469.50
train mean loss: 457.04
epoch train time: 0:00:02.124723
elapsed time: 0:04:55.894699
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-26 23:52:38.978282
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 459.36
 ---- batch: 020 ----
mean loss: 454.79
 ---- batch: 030 ----
mean loss: 457.49
 ---- batch: 040 ----
mean loss: 453.69
 ---- batch: 050 ----
mean loss: 462.62
 ---- batch: 060 ----
mean loss: 463.07
 ---- batch: 070 ----
mean loss: 468.33
 ---- batch: 080 ----
mean loss: 454.61
 ---- batch: 090 ----
mean loss: 454.82
 ---- batch: 100 ----
mean loss: 457.24
 ---- batch: 110 ----
mean loss: 450.52
train mean loss: 457.87
epoch train time: 0:00:02.121955
elapsed time: 0:04:58.016833
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-26 23:52:41.100449
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 454.10
 ---- batch: 020 ----
mean loss: 450.57
 ---- batch: 030 ----
mean loss: 456.48
 ---- batch: 040 ----
mean loss: 465.64
 ---- batch: 050 ----
mean loss: 463.21
 ---- batch: 060 ----
mean loss: 445.08
 ---- batch: 070 ----
mean loss: 445.87
 ---- batch: 080 ----
mean loss: 465.83
 ---- batch: 090 ----
mean loss: 453.61
 ---- batch: 100 ----
mean loss: 449.11
 ---- batch: 110 ----
mean loss: 461.89
train mean loss: 455.11
epoch train time: 0:00:02.127203
elapsed time: 0:05:00.144223
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-26 23:52:43.227808
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 448.70
 ---- batch: 020 ----
mean loss: 452.62
 ---- batch: 030 ----
mean loss: 432.00
 ---- batch: 040 ----
mean loss: 442.39
 ---- batch: 050 ----
mean loss: 452.06
 ---- batch: 060 ----
mean loss: 430.38
 ---- batch: 070 ----
mean loss: 448.82
 ---- batch: 080 ----
mean loss: 447.43
 ---- batch: 090 ----
mean loss: 445.15
 ---- batch: 100 ----
mean loss: 455.66
 ---- batch: 110 ----
mean loss: 456.46
train mean loss: 447.34
epoch train time: 0:00:02.128660
elapsed time: 0:05:02.273058
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-26 23:52:45.356632
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 451.80
 ---- batch: 020 ----
mean loss: 445.68
 ---- batch: 030 ----
mean loss: 436.38
 ---- batch: 040 ----
mean loss: 442.68
 ---- batch: 050 ----
mean loss: 457.35
 ---- batch: 060 ----
mean loss: 453.00
 ---- batch: 070 ----
mean loss: 443.41
 ---- batch: 080 ----
mean loss: 458.14
 ---- batch: 090 ----
mean loss: 452.24
 ---- batch: 100 ----
mean loss: 441.72
 ---- batch: 110 ----
mean loss: 449.10
train mean loss: 447.78
epoch train time: 0:00:02.133863
elapsed time: 0:05:04.407069
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-26 23:52:47.490651
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 447.32
 ---- batch: 020 ----
mean loss: 461.44
 ---- batch: 030 ----
mean loss: 452.46
 ---- batch: 040 ----
mean loss: 445.02
 ---- batch: 050 ----
mean loss: 444.28
 ---- batch: 060 ----
mean loss: 457.63
 ---- batch: 070 ----
mean loss: 445.69
 ---- batch: 080 ----
mean loss: 433.17
 ---- batch: 090 ----
mean loss: 439.18
 ---- batch: 100 ----
mean loss: 444.08
 ---- batch: 110 ----
mean loss: 432.81
train mean loss: 445.87
epoch train time: 0:00:02.127909
elapsed time: 0:05:06.535131
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-26 23:52:49.618735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 434.63
 ---- batch: 020 ----
mean loss: 455.69
 ---- batch: 030 ----
mean loss: 450.96
 ---- batch: 040 ----
mean loss: 446.75
 ---- batch: 050 ----
mean loss: 449.32
 ---- batch: 060 ----
mean loss: 440.69
 ---- batch: 070 ----
mean loss: 425.91
 ---- batch: 080 ----
mean loss: 437.07
 ---- batch: 090 ----
mean loss: 440.76
 ---- batch: 100 ----
mean loss: 443.10
 ---- batch: 110 ----
mean loss: 453.98
train mean loss: 443.59
epoch train time: 0:00:02.124846
elapsed time: 0:05:08.660150
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-26 23:52:51.743731
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 430.46
 ---- batch: 020 ----
mean loss: 440.30
 ---- batch: 030 ----
mean loss: 435.54
 ---- batch: 040 ----
mean loss: 442.35
 ---- batch: 050 ----
mean loss: 441.52
 ---- batch: 060 ----
mean loss: 438.48
 ---- batch: 070 ----
mean loss: 441.32
 ---- batch: 080 ----
mean loss: 431.77
 ---- batch: 090 ----
mean loss: 437.06
 ---- batch: 100 ----
mean loss: 440.49
 ---- batch: 110 ----
mean loss: 422.14
train mean loss: 436.09
epoch train time: 0:00:02.122112
elapsed time: 0:05:10.782413
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-26 23:52:53.865996
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 423.78
 ---- batch: 020 ----
mean loss: 430.63
 ---- batch: 030 ----
mean loss: 429.96
 ---- batch: 040 ----
mean loss: 431.09
 ---- batch: 050 ----
mean loss: 434.32
 ---- batch: 060 ----
mean loss: 434.99
 ---- batch: 070 ----
mean loss: 437.43
 ---- batch: 080 ----
mean loss: 430.34
 ---- batch: 090 ----
mean loss: 435.13
 ---- batch: 100 ----
mean loss: 444.79
 ---- batch: 110 ----
mean loss: 435.71
train mean loss: 433.19
epoch train time: 0:00:02.124810
elapsed time: 0:05:12.907423
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-26 23:52:55.991003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 431.62
 ---- batch: 020 ----
mean loss: 433.39
 ---- batch: 030 ----
mean loss: 434.33
 ---- batch: 040 ----
mean loss: 417.26
 ---- batch: 050 ----
mean loss: 420.97
 ---- batch: 060 ----
mean loss: 434.75
 ---- batch: 070 ----
mean loss: 429.11
 ---- batch: 080 ----
mean loss: 428.67
 ---- batch: 090 ----
mean loss: 415.47
 ---- batch: 100 ----
mean loss: 425.34
 ---- batch: 110 ----
mean loss: 428.09
train mean loss: 427.01
epoch train time: 0:00:02.124900
elapsed time: 0:05:15.032475
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-26 23:52:58.116054
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.57
 ---- batch: 020 ----
mean loss: 425.45
 ---- batch: 030 ----
mean loss: 425.85
 ---- batch: 040 ----
mean loss: 433.85
 ---- batch: 050 ----
mean loss: 414.44
 ---- batch: 060 ----
mean loss: 422.80
 ---- batch: 070 ----
mean loss: 425.87
 ---- batch: 080 ----
mean loss: 429.86
 ---- batch: 090 ----
mean loss: 425.16
 ---- batch: 100 ----
mean loss: 416.97
 ---- batch: 110 ----
mean loss: 421.46
train mean loss: 424.63
epoch train time: 0:00:02.130923
elapsed time: 0:05:17.163548
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-26 23:53:00.247144
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 422.11
 ---- batch: 020 ----
mean loss: 430.88
 ---- batch: 030 ----
mean loss: 427.10
 ---- batch: 040 ----
mean loss: 425.52
 ---- batch: 050 ----
mean loss: 434.91
 ---- batch: 060 ----
mean loss: 438.29
 ---- batch: 070 ----
mean loss: 414.50
 ---- batch: 080 ----
mean loss: 436.07
 ---- batch: 090 ----
mean loss: 425.03
 ---- batch: 100 ----
mean loss: 416.41
 ---- batch: 110 ----
mean loss: 417.09
train mean loss: 425.78
epoch train time: 0:00:02.121355
elapsed time: 0:05:19.285101
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-26 23:53:02.368698
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.52
 ---- batch: 020 ----
mean loss: 422.46
 ---- batch: 030 ----
mean loss: 423.34
 ---- batch: 040 ----
mean loss: 428.41
 ---- batch: 050 ----
mean loss: 410.41
 ---- batch: 060 ----
mean loss: 425.65
 ---- batch: 070 ----
mean loss: 417.40
 ---- batch: 080 ----
mean loss: 409.51
 ---- batch: 090 ----
mean loss: 417.58
 ---- batch: 100 ----
mean loss: 419.15
 ---- batch: 110 ----
mean loss: 441.43
train mean loss: 420.97
epoch train time: 0:00:02.125659
elapsed time: 0:05:21.410943
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-26 23:53:04.494545
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 428.21
 ---- batch: 020 ----
mean loss: 422.59
 ---- batch: 030 ----
mean loss: 425.38
 ---- batch: 040 ----
mean loss: 421.03
 ---- batch: 050 ----
mean loss: 404.80
 ---- batch: 060 ----
mean loss: 425.10
 ---- batch: 070 ----
mean loss: 420.18
 ---- batch: 080 ----
mean loss: 431.42
 ---- batch: 090 ----
mean loss: 409.61
 ---- batch: 100 ----
mean loss: 408.77
 ---- batch: 110 ----
mean loss: 422.82
train mean loss: 419.80
epoch train time: 0:00:02.128282
elapsed time: 0:05:23.539430
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-26 23:53:06.623079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 412.36
 ---- batch: 020 ----
mean loss: 413.47
 ---- batch: 030 ----
mean loss: 416.53
 ---- batch: 040 ----
mean loss: 415.33
 ---- batch: 050 ----
mean loss: 409.21
 ---- batch: 060 ----
mean loss: 411.06
 ---- batch: 070 ----
mean loss: 404.81
 ---- batch: 080 ----
mean loss: 422.30
 ---- batch: 090 ----
mean loss: 410.93
 ---- batch: 100 ----
mean loss: 408.64
 ---- batch: 110 ----
mean loss: 417.65
train mean loss: 412.56
epoch train time: 0:00:02.121570
elapsed time: 0:05:25.661226
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-26 23:53:08.744812
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 421.10
 ---- batch: 020 ----
mean loss: 433.21
 ---- batch: 030 ----
mean loss: 405.59
 ---- batch: 040 ----
mean loss: 407.31
 ---- batch: 050 ----
mean loss: 408.14
 ---- batch: 060 ----
mean loss: 408.30
 ---- batch: 070 ----
mean loss: 404.46
 ---- batch: 080 ----
mean loss: 423.66
 ---- batch: 090 ----
mean loss: 411.22
 ---- batch: 100 ----
mean loss: 410.01
 ---- batch: 110 ----
mean loss: 415.41
train mean loss: 413.53
epoch train time: 0:00:02.126588
elapsed time: 0:05:27.787975
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-26 23:53:10.871555
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 420.63
 ---- batch: 020 ----
mean loss: 409.89
 ---- batch: 030 ----
mean loss: 406.87
 ---- batch: 040 ----
mean loss: 408.60
 ---- batch: 050 ----
mean loss: 409.83
 ---- batch: 060 ----
mean loss: 414.49
 ---- batch: 070 ----
mean loss: 399.77
 ---- batch: 080 ----
mean loss: 409.15
 ---- batch: 090 ----
mean loss: 408.37
 ---- batch: 100 ----
mean loss: 394.20
 ---- batch: 110 ----
mean loss: 412.01
train mean loss: 408.48
epoch train time: 0:00:02.123668
elapsed time: 0:05:29.911793
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-26 23:53:12.995372
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 394.45
 ---- batch: 020 ----
mean loss: 422.55
 ---- batch: 030 ----
mean loss: 422.08
 ---- batch: 040 ----
mean loss: 411.72
 ---- batch: 050 ----
mean loss: 409.47
 ---- batch: 060 ----
mean loss: 400.70
 ---- batch: 070 ----
mean loss: 400.29
 ---- batch: 080 ----
mean loss: 404.85
 ---- batch: 090 ----
mean loss: 404.93
 ---- batch: 100 ----
mean loss: 422.26
 ---- batch: 110 ----
mean loss: 401.68
train mean loss: 408.72
epoch train time: 0:00:02.122470
elapsed time: 0:05:32.034459
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-26 23:53:15.118069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 410.36
 ---- batch: 020 ----
mean loss: 409.86
 ---- batch: 030 ----
mean loss: 414.69
 ---- batch: 040 ----
mean loss: 412.61
 ---- batch: 050 ----
mean loss: 401.08
 ---- batch: 060 ----
mean loss: 397.17
 ---- batch: 070 ----
mean loss: 411.39
 ---- batch: 080 ----
mean loss: 413.29
 ---- batch: 090 ----
mean loss: 410.21
 ---- batch: 100 ----
mean loss: 404.13
 ---- batch: 110 ----
mean loss: 398.40
train mean loss: 407.17
epoch train time: 0:00:02.125076
elapsed time: 0:05:34.159726
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-26 23:53:17.243307
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 407.67
 ---- batch: 020 ----
mean loss: 418.52
 ---- batch: 030 ----
mean loss: 400.48
 ---- batch: 040 ----
mean loss: 392.01
 ---- batch: 050 ----
mean loss: 400.15
 ---- batch: 060 ----
mean loss: 424.36
 ---- batch: 070 ----
mean loss: 404.67
 ---- batch: 080 ----
mean loss: 401.63
 ---- batch: 090 ----
mean loss: 403.25
 ---- batch: 100 ----
mean loss: 409.93
 ---- batch: 110 ----
mean loss: 406.93
train mean loss: 406.11
epoch train time: 0:00:02.126898
elapsed time: 0:05:36.286789
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-26 23:53:19.370452
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 411.01
 ---- batch: 020 ----
mean loss: 405.74
 ---- batch: 030 ----
mean loss: 399.63
 ---- batch: 040 ----
mean loss: 414.16
 ---- batch: 050 ----
mean loss: 406.77
 ---- batch: 060 ----
mean loss: 398.52
 ---- batch: 070 ----
mean loss: 394.34
 ---- batch: 080 ----
mean loss: 418.38
 ---- batch: 090 ----
mean loss: 400.00
 ---- batch: 100 ----
mean loss: 404.65
 ---- batch: 110 ----
mean loss: 402.48
train mean loss: 404.78
epoch train time: 0:00:02.134477
elapsed time: 0:05:38.421498
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-26 23:53:21.505079
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 399.82
 ---- batch: 020 ----
mean loss: 401.18
 ---- batch: 030 ----
mean loss: 395.60
 ---- batch: 040 ----
mean loss: 403.81
 ---- batch: 050 ----
mean loss: 399.85
 ---- batch: 060 ----
mean loss: 400.23
 ---- batch: 070 ----
mean loss: 408.38
 ---- batch: 080 ----
mean loss: 405.90
 ---- batch: 090 ----
mean loss: 391.19
 ---- batch: 100 ----
mean loss: 399.99
 ---- batch: 110 ----
mean loss: 397.35
train mean loss: 400.44
epoch train time: 0:00:02.127944
elapsed time: 0:05:40.549601
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-26 23:53:23.633182
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 386.21
 ---- batch: 020 ----
mean loss: 404.31
 ---- batch: 030 ----
mean loss: 405.52
 ---- batch: 040 ----
mean loss: 390.48
 ---- batch: 050 ----
mean loss: 413.44
 ---- batch: 060 ----
mean loss: 400.17
 ---- batch: 070 ----
mean loss: 404.95
 ---- batch: 080 ----
mean loss: 400.39
 ---- batch: 090 ----
mean loss: 405.77
 ---- batch: 100 ----
mean loss: 396.98
 ---- batch: 110 ----
mean loss: 393.12
train mean loss: 399.67
epoch train time: 0:00:02.128237
elapsed time: 0:05:42.677990
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-26 23:53:25.761574
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 413.08
 ---- batch: 020 ----
mean loss: 414.27
 ---- batch: 030 ----
mean loss: 405.54
 ---- batch: 040 ----
mean loss: 403.84
 ---- batch: 050 ----
mean loss: 388.64
 ---- batch: 060 ----
mean loss: 393.84
 ---- batch: 070 ----
mean loss: 396.36
 ---- batch: 080 ----
mean loss: 397.63
 ---- batch: 090 ----
mean loss: 394.47
 ---- batch: 100 ----
mean loss: 389.96
 ---- batch: 110 ----
mean loss: 397.35
train mean loss: 400.27
epoch train time: 0:00:02.128380
elapsed time: 0:05:44.806530
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-26 23:53:27.890111
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 391.88
 ---- batch: 020 ----
mean loss: 401.36
 ---- batch: 030 ----
mean loss: 397.52
 ---- batch: 040 ----
mean loss: 392.29
 ---- batch: 050 ----
mean loss: 406.93
 ---- batch: 060 ----
mean loss: 388.76
 ---- batch: 070 ----
mean loss: 390.07
 ---- batch: 080 ----
mean loss: 403.95
 ---- batch: 090 ----
mean loss: 398.50
 ---- batch: 100 ----
mean loss: 389.93
 ---- batch: 110 ----
mean loss: 386.22
train mean loss: 394.51
epoch train time: 0:00:02.123411
elapsed time: 0:05:46.930106
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-26 23:53:30.013693
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.38
 ---- batch: 020 ----
mean loss: 404.99
 ---- batch: 030 ----
mean loss: 385.91
 ---- batch: 040 ----
mean loss: 394.62
 ---- batch: 050 ----
mean loss: 392.40
 ---- batch: 060 ----
mean loss: 384.95
 ---- batch: 070 ----
mean loss: 391.19
 ---- batch: 080 ----
mean loss: 404.13
 ---- batch: 090 ----
mean loss: 387.80
 ---- batch: 100 ----
mean loss: 387.40
 ---- batch: 110 ----
mean loss: 379.08
train mean loss: 389.92
epoch train time: 0:00:02.126999
elapsed time: 0:05:49.057282
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-26 23:53:32.140871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.96
 ---- batch: 020 ----
mean loss: 373.64
 ---- batch: 030 ----
mean loss: 388.36
 ---- batch: 040 ----
mean loss: 384.18
 ---- batch: 050 ----
mean loss: 397.40
 ---- batch: 060 ----
mean loss: 389.82
 ---- batch: 070 ----
mean loss: 401.82
 ---- batch: 080 ----
mean loss: 386.25
 ---- batch: 090 ----
mean loss: 390.50
 ---- batch: 100 ----
mean loss: 409.39
 ---- batch: 110 ----
mean loss: 391.54
train mean loss: 390.41
epoch train time: 0:00:02.129834
elapsed time: 0:05:51.187278
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-26 23:53:34.270877
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 384.75
 ---- batch: 020 ----
mean loss: 383.04
 ---- batch: 030 ----
mean loss: 379.72
 ---- batch: 040 ----
mean loss: 396.78
 ---- batch: 050 ----
mean loss: 384.22
 ---- batch: 060 ----
mean loss: 387.33
 ---- batch: 070 ----
mean loss: 380.10
 ---- batch: 080 ----
mean loss: 388.80
 ---- batch: 090 ----
mean loss: 376.39
 ---- batch: 100 ----
mean loss: 386.61
 ---- batch: 110 ----
mean loss: 379.55
train mean loss: 384.76
epoch train time: 0:00:02.123100
elapsed time: 0:05:53.310582
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-26 23:53:36.394164
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.37
 ---- batch: 020 ----
mean loss: 385.07
 ---- batch: 030 ----
mean loss: 391.82
 ---- batch: 040 ----
mean loss: 392.25
 ---- batch: 050 ----
mean loss: 395.61
 ---- batch: 060 ----
mean loss: 384.47
 ---- batch: 070 ----
mean loss: 397.88
 ---- batch: 080 ----
mean loss: 376.67
 ---- batch: 090 ----
mean loss: 379.78
 ---- batch: 100 ----
mean loss: 390.58
 ---- batch: 110 ----
mean loss: 377.19
train mean loss: 385.86
epoch train time: 0:00:02.121545
elapsed time: 0:05:55.432300
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-26 23:53:38.515882
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 385.04
 ---- batch: 020 ----
mean loss: 387.58
 ---- batch: 030 ----
mean loss: 372.95
 ---- batch: 040 ----
mean loss: 381.80
 ---- batch: 050 ----
mean loss: 388.87
 ---- batch: 060 ----
mean loss: 384.69
 ---- batch: 070 ----
mean loss: 385.67
 ---- batch: 080 ----
mean loss: 390.29
 ---- batch: 090 ----
mean loss: 383.19
 ---- batch: 100 ----
mean loss: 375.26
 ---- batch: 110 ----
mean loss: 382.62
train mean loss: 383.94
epoch train time: 0:00:02.125970
elapsed time: 0:05:57.558438
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-26 23:53:40.642023
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 374.83
 ---- batch: 020 ----
mean loss: 370.08
 ---- batch: 030 ----
mean loss: 378.50
 ---- batch: 040 ----
mean loss: 378.50
 ---- batch: 050 ----
mean loss: 394.61
 ---- batch: 060 ----
mean loss: 380.49
 ---- batch: 070 ----
mean loss: 392.35
 ---- batch: 080 ----
mean loss: 382.33
 ---- batch: 090 ----
mean loss: 381.27
 ---- batch: 100 ----
mean loss: 382.53
 ---- batch: 110 ----
mean loss: 386.30
train mean loss: 382.41
epoch train time: 0:00:02.122479
elapsed time: 0:05:59.681099
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-26 23:53:42.764699
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 381.61
 ---- batch: 020 ----
mean loss: 377.62
 ---- batch: 030 ----
mean loss: 373.91
 ---- batch: 040 ----
mean loss: 374.82
 ---- batch: 050 ----
mean loss: 380.79
 ---- batch: 060 ----
mean loss: 377.37
 ---- batch: 070 ----
mean loss: 383.64
 ---- batch: 080 ----
mean loss: 376.01
 ---- batch: 090 ----
mean loss: 379.25
 ---- batch: 100 ----
mean loss: 369.86
 ---- batch: 110 ----
mean loss: 381.38
train mean loss: 377.41
epoch train time: 0:00:02.133318
elapsed time: 0:06:01.814591
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-26 23:53:44.898175
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.49
 ---- batch: 020 ----
mean loss: 367.19
 ---- batch: 030 ----
mean loss: 374.78
 ---- batch: 040 ----
mean loss: 381.27
 ---- batch: 050 ----
mean loss: 377.90
 ---- batch: 060 ----
mean loss: 373.27
 ---- batch: 070 ----
mean loss: 395.32
 ---- batch: 080 ----
mean loss: 377.37
 ---- batch: 090 ----
mean loss: 377.49
 ---- batch: 100 ----
mean loss: 375.24
 ---- batch: 110 ----
mean loss: 383.08
train mean loss: 378.10
epoch train time: 0:00:02.122200
elapsed time: 0:06:03.936982
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-26 23:53:47.020621
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 383.72
 ---- batch: 020 ----
mean loss: 368.26
 ---- batch: 030 ----
mean loss: 384.74
 ---- batch: 040 ----
mean loss: 378.78
 ---- batch: 050 ----
mean loss: 368.42
 ---- batch: 060 ----
mean loss: 370.64
 ---- batch: 070 ----
mean loss: 380.41
 ---- batch: 080 ----
mean loss: 369.83
 ---- batch: 090 ----
mean loss: 380.51
 ---- batch: 100 ----
mean loss: 372.15
 ---- batch: 110 ----
mean loss: 367.36
train mean loss: 375.06
epoch train time: 0:00:02.125423
elapsed time: 0:06:06.062624
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-26 23:53:49.146205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 377.77
 ---- batch: 020 ----
mean loss: 365.62
 ---- batch: 030 ----
mean loss: 365.87
 ---- batch: 040 ----
mean loss: 368.78
 ---- batch: 050 ----
mean loss: 375.33
 ---- batch: 060 ----
mean loss: 382.60
 ---- batch: 070 ----
mean loss: 366.38
 ---- batch: 080 ----
mean loss: 368.85
 ---- batch: 090 ----
mean loss: 374.09
 ---- batch: 100 ----
mean loss: 386.01
 ---- batch: 110 ----
mean loss: 364.28
train mean loss: 372.08
epoch train time: 0:00:02.124483
elapsed time: 0:06:08.187266
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-26 23:53:51.270850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 367.77
 ---- batch: 020 ----
mean loss: 372.21
 ---- batch: 030 ----
mean loss: 369.92
 ---- batch: 040 ----
mean loss: 362.40
 ---- batch: 050 ----
mean loss: 386.53
 ---- batch: 060 ----
mean loss: 375.66
 ---- batch: 070 ----
mean loss: 372.23
 ---- batch: 080 ----
mean loss: 354.59
 ---- batch: 090 ----
mean loss: 357.31
 ---- batch: 100 ----
mean loss: 380.41
 ---- batch: 110 ----
mean loss: 381.66
train mean loss: 371.28
epoch train time: 0:00:02.122558
elapsed time: 0:06:10.309981
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-26 23:53:53.393562
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.92
 ---- batch: 020 ----
mean loss: 379.27
 ---- batch: 030 ----
mean loss: 375.22
 ---- batch: 040 ----
mean loss: 377.62
 ---- batch: 050 ----
mean loss: 375.30
 ---- batch: 060 ----
mean loss: 361.71
 ---- batch: 070 ----
mean loss: 379.66
 ---- batch: 080 ----
mean loss: 367.07
 ---- batch: 090 ----
mean loss: 345.53
 ---- batch: 100 ----
mean loss: 373.24
 ---- batch: 110 ----
mean loss: 376.23
train mean loss: 370.97
epoch train time: 0:00:02.122289
elapsed time: 0:06:12.432449
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-26 23:53:55.516047
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 378.30
 ---- batch: 020 ----
mean loss: 358.94
 ---- batch: 030 ----
mean loss: 375.23
 ---- batch: 040 ----
mean loss: 376.17
 ---- batch: 050 ----
mean loss: 381.56
 ---- batch: 060 ----
mean loss: 367.87
 ---- batch: 070 ----
mean loss: 355.63
 ---- batch: 080 ----
mean loss: 381.18
 ---- batch: 090 ----
mean loss: 383.83
 ---- batch: 100 ----
mean loss: 372.56
 ---- batch: 110 ----
mean loss: 348.85
train mean loss: 370.92
epoch train time: 0:00:02.125972
elapsed time: 0:06:14.558590
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-26 23:53:57.642171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 363.08
 ---- batch: 020 ----
mean loss: 368.56
 ---- batch: 030 ----
mean loss: 370.21
 ---- batch: 040 ----
mean loss: 371.86
 ---- batch: 050 ----
mean loss: 387.87
 ---- batch: 060 ----
mean loss: 367.60
 ---- batch: 070 ----
mean loss: 357.36
 ---- batch: 080 ----
mean loss: 358.91
 ---- batch: 090 ----
mean loss: 360.40
 ---- batch: 100 ----
mean loss: 377.50
 ---- batch: 110 ----
mean loss: 367.65
train mean loss: 367.83
epoch train time: 0:00:02.128139
elapsed time: 0:06:16.686881
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-26 23:53:59.770488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.94
 ---- batch: 020 ----
mean loss: 363.26
 ---- batch: 030 ----
mean loss: 374.77
 ---- batch: 040 ----
mean loss: 350.96
 ---- batch: 050 ----
mean loss: 368.05
 ---- batch: 060 ----
mean loss: 366.82
 ---- batch: 070 ----
mean loss: 367.42
 ---- batch: 080 ----
mean loss: 340.58
 ---- batch: 090 ----
mean loss: 360.44
 ---- batch: 100 ----
mean loss: 365.91
 ---- batch: 110 ----
mean loss: 371.76
train mean loss: 363.22
epoch train time: 0:00:02.124783
elapsed time: 0:06:18.811854
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-26 23:54:01.895433
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.14
 ---- batch: 020 ----
mean loss: 357.49
 ---- batch: 030 ----
mean loss: 369.28
 ---- batch: 040 ----
mean loss: 363.59
 ---- batch: 050 ----
mean loss: 365.32
 ---- batch: 060 ----
mean loss: 367.11
 ---- batch: 070 ----
mean loss: 358.94
 ---- batch: 080 ----
mean loss: 366.41
 ---- batch: 090 ----
mean loss: 357.15
 ---- batch: 100 ----
mean loss: 360.28
 ---- batch: 110 ----
mean loss: 368.48
train mean loss: 362.78
epoch train time: 0:00:02.122211
elapsed time: 0:06:20.934214
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-26 23:54:04.017795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.52
 ---- batch: 020 ----
mean loss: 362.64
 ---- batch: 030 ----
mean loss: 359.52
 ---- batch: 040 ----
mean loss: 359.61
 ---- batch: 050 ----
mean loss: 366.10
 ---- batch: 060 ----
mean loss: 354.57
 ---- batch: 070 ----
mean loss: 366.27
 ---- batch: 080 ----
mean loss: 355.57
 ---- batch: 090 ----
mean loss: 352.41
 ---- batch: 100 ----
mean loss: 362.85
 ---- batch: 110 ----
mean loss: 362.95
train mean loss: 358.97
epoch train time: 0:00:02.124020
elapsed time: 0:06:23.058401
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-26 23:54:06.141991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.88
 ---- batch: 020 ----
mean loss: 352.04
 ---- batch: 030 ----
mean loss: 358.81
 ---- batch: 040 ----
mean loss: 348.93
 ---- batch: 050 ----
mean loss: 363.94
 ---- batch: 060 ----
mean loss: 354.02
 ---- batch: 070 ----
mean loss: 365.27
 ---- batch: 080 ----
mean loss: 352.79
 ---- batch: 090 ----
mean loss: 365.63
 ---- batch: 100 ----
mean loss: 348.37
 ---- batch: 110 ----
mean loss: 357.42
train mean loss: 357.92
epoch train time: 0:00:02.129191
elapsed time: 0:06:25.187749
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-26 23:54:08.271327
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.95
 ---- batch: 020 ----
mean loss: 349.34
 ---- batch: 030 ----
mean loss: 351.16
 ---- batch: 040 ----
mean loss: 351.30
 ---- batch: 050 ----
mean loss: 355.35
 ---- batch: 060 ----
mean loss: 358.84
 ---- batch: 070 ----
mean loss: 345.46
 ---- batch: 080 ----
mean loss: 368.09
 ---- batch: 090 ----
mean loss: 366.89
 ---- batch: 100 ----
mean loss: 367.75
 ---- batch: 110 ----
mean loss: 352.95
train mean loss: 357.12
epoch train time: 0:00:02.123553
elapsed time: 0:06:27.311481
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-26 23:54:10.395069
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 361.44
 ---- batch: 020 ----
mean loss: 350.25
 ---- batch: 030 ----
mean loss: 341.45
 ---- batch: 040 ----
mean loss: 337.75
 ---- batch: 050 ----
mean loss: 349.25
 ---- batch: 060 ----
mean loss: 339.96
 ---- batch: 070 ----
mean loss: 367.18
 ---- batch: 080 ----
mean loss: 354.06
 ---- batch: 090 ----
mean loss: 369.19
 ---- batch: 100 ----
mean loss: 361.77
 ---- batch: 110 ----
mean loss: 351.84
train mean loss: 352.42
epoch train time: 0:00:02.123222
elapsed time: 0:06:29.434875
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-26 23:54:12.518459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 353.52
 ---- batch: 020 ----
mean loss: 361.58
 ---- batch: 030 ----
mean loss: 340.75
 ---- batch: 040 ----
mean loss: 349.03
 ---- batch: 050 ----
mean loss: 356.57
 ---- batch: 060 ----
mean loss: 347.68
 ---- batch: 070 ----
mean loss: 358.93
 ---- batch: 080 ----
mean loss: 356.28
 ---- batch: 090 ----
mean loss: 346.20
 ---- batch: 100 ----
mean loss: 360.43
 ---- batch: 110 ----
mean loss: 359.88
train mean loss: 353.47
epoch train time: 0:00:02.125192
elapsed time: 0:06:31.560223
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-26 23:54:14.643823
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.00
 ---- batch: 020 ----
mean loss: 355.44
 ---- batch: 030 ----
mean loss: 339.39
 ---- batch: 040 ----
mean loss: 359.06
 ---- batch: 050 ----
mean loss: 352.35
 ---- batch: 060 ----
mean loss: 358.80
 ---- batch: 070 ----
mean loss: 357.77
 ---- batch: 080 ----
mean loss: 353.31
 ---- batch: 090 ----
mean loss: 348.50
 ---- batch: 100 ----
mean loss: 361.57
 ---- batch: 110 ----
mean loss: 341.72
train mean loss: 352.10
epoch train time: 0:00:02.125226
elapsed time: 0:06:33.685666
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-26 23:54:16.769266
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.44
 ---- batch: 020 ----
mean loss: 356.58
 ---- batch: 030 ----
mean loss: 348.26
 ---- batch: 040 ----
mean loss: 341.10
 ---- batch: 050 ----
mean loss: 343.93
 ---- batch: 060 ----
mean loss: 360.84
 ---- batch: 070 ----
mean loss: 339.00
 ---- batch: 080 ----
mean loss: 355.37
 ---- batch: 090 ----
mean loss: 354.66
 ---- batch: 100 ----
mean loss: 346.89
 ---- batch: 110 ----
mean loss: 343.18
train mean loss: 349.17
epoch train time: 0:00:02.131885
elapsed time: 0:06:35.817736
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-26 23:54:18.901319
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.85
 ---- batch: 020 ----
mean loss: 358.66
 ---- batch: 030 ----
mean loss: 352.61
 ---- batch: 040 ----
mean loss: 346.68
 ---- batch: 050 ----
mean loss: 351.39
 ---- batch: 060 ----
mean loss: 342.95
 ---- batch: 070 ----
mean loss: 353.22
 ---- batch: 080 ----
mean loss: 350.42
 ---- batch: 090 ----
mean loss: 357.24
 ---- batch: 100 ----
mean loss: 343.37
 ---- batch: 110 ----
mean loss: 338.02
train mean loss: 348.49
epoch train time: 0:00:02.122662
elapsed time: 0:06:37.940565
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-26 23:54:21.024146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.26
 ---- batch: 020 ----
mean loss: 355.59
 ---- batch: 030 ----
mean loss: 353.03
 ---- batch: 040 ----
mean loss: 338.96
 ---- batch: 050 ----
mean loss: 348.17
 ---- batch: 060 ----
mean loss: 360.65
 ---- batch: 070 ----
mean loss: 352.82
 ---- batch: 080 ----
mean loss: 353.70
 ---- batch: 090 ----
mean loss: 351.15
 ---- batch: 100 ----
mean loss: 346.08
 ---- batch: 110 ----
mean loss: 342.94
train mean loss: 350.30
epoch train time: 0:00:02.121678
elapsed time: 0:06:40.062409
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-26 23:54:23.145993
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.38
 ---- batch: 020 ----
mean loss: 344.15
 ---- batch: 030 ----
mean loss: 362.05
 ---- batch: 040 ----
mean loss: 348.30
 ---- batch: 050 ----
mean loss: 337.41
 ---- batch: 060 ----
mean loss: 337.94
 ---- batch: 070 ----
mean loss: 346.53
 ---- batch: 080 ----
mean loss: 350.86
 ---- batch: 090 ----
mean loss: 363.96
 ---- batch: 100 ----
mean loss: 342.50
 ---- batch: 110 ----
mean loss: 350.14
train mean loss: 347.76
epoch train time: 0:00:02.125683
elapsed time: 0:06:42.188267
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-26 23:54:25.271836
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.53
 ---- batch: 020 ----
mean loss: 348.16
 ---- batch: 030 ----
mean loss: 367.09
 ---- batch: 040 ----
mean loss: 346.76
 ---- batch: 050 ----
mean loss: 345.13
 ---- batch: 060 ----
mean loss: 352.93
 ---- batch: 070 ----
mean loss: 347.05
 ---- batch: 080 ----
mean loss: 338.51
 ---- batch: 090 ----
mean loss: 347.95
 ---- batch: 100 ----
mean loss: 359.57
 ---- batch: 110 ----
mean loss: 348.27
train mean loss: 348.54
epoch train time: 0:00:02.123970
elapsed time: 0:06:44.312375
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-26 23:54:27.395975
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.71
 ---- batch: 020 ----
mean loss: 343.89
 ---- batch: 030 ----
mean loss: 344.27
 ---- batch: 040 ----
mean loss: 338.63
 ---- batch: 050 ----
mean loss: 346.71
 ---- batch: 060 ----
mean loss: 341.89
 ---- batch: 070 ----
mean loss: 356.15
 ---- batch: 080 ----
mean loss: 332.67
 ---- batch: 090 ----
mean loss: 353.85
 ---- batch: 100 ----
mean loss: 346.45
 ---- batch: 110 ----
mean loss: 330.00
train mean loss: 344.10
epoch train time: 0:00:02.122008
elapsed time: 0:06:46.434563
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-26 23:54:29.518145
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.86
 ---- batch: 020 ----
mean loss: 342.93
 ---- batch: 030 ----
mean loss: 334.16
 ---- batch: 040 ----
mean loss: 365.23
 ---- batch: 050 ----
mean loss: 338.65
 ---- batch: 060 ----
mean loss: 340.39
 ---- batch: 070 ----
mean loss: 342.47
 ---- batch: 080 ----
mean loss: 340.80
 ---- batch: 090 ----
mean loss: 332.42
 ---- batch: 100 ----
mean loss: 350.45
 ---- batch: 110 ----
mean loss: 338.32
train mean loss: 343.04
epoch train time: 0:00:02.124994
elapsed time: 0:06:48.559709
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-26 23:54:31.643292
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 352.34
 ---- batch: 020 ----
mean loss: 340.82
 ---- batch: 030 ----
mean loss: 338.68
 ---- batch: 040 ----
mean loss: 331.70
 ---- batch: 050 ----
mean loss: 340.61
 ---- batch: 060 ----
mean loss: 347.28
 ---- batch: 070 ----
mean loss: 332.49
 ---- batch: 080 ----
mean loss: 342.89
 ---- batch: 090 ----
mean loss: 348.07
 ---- batch: 100 ----
mean loss: 345.93
 ---- batch: 110 ----
mean loss: 329.99
train mean loss: 340.96
epoch train time: 0:00:02.124436
elapsed time: 0:06:50.684331
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-26 23:54:33.767943
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 360.41
 ---- batch: 020 ----
mean loss: 344.89
 ---- batch: 030 ----
mean loss: 345.72
 ---- batch: 040 ----
mean loss: 332.84
 ---- batch: 050 ----
mean loss: 329.57
 ---- batch: 060 ----
mean loss: 339.35
 ---- batch: 070 ----
mean loss: 339.23
 ---- batch: 080 ----
mean loss: 339.63
 ---- batch: 090 ----
mean loss: 346.28
 ---- batch: 100 ----
mean loss: 341.04
 ---- batch: 110 ----
mean loss: 340.75
train mean loss: 341.56
epoch train time: 0:00:02.122734
elapsed time: 0:06:52.807255
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-26 23:54:35.890841
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 335.44
 ---- batch: 020 ----
mean loss: 341.24
 ---- batch: 030 ----
mean loss: 344.76
 ---- batch: 040 ----
mean loss: 342.60
 ---- batch: 050 ----
mean loss: 343.01
 ---- batch: 060 ----
mean loss: 337.45
 ---- batch: 070 ----
mean loss: 350.92
 ---- batch: 080 ----
mean loss: 343.78
 ---- batch: 090 ----
mean loss: 345.69
 ---- batch: 100 ----
mean loss: 336.42
 ---- batch: 110 ----
mean loss: 344.26
train mean loss: 342.15
epoch train time: 0:00:02.123350
elapsed time: 0:06:54.930796
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-26 23:54:38.014375
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 332.97
 ---- batch: 020 ----
mean loss: 348.82
 ---- batch: 030 ----
mean loss: 331.41
 ---- batch: 040 ----
mean loss: 331.17
 ---- batch: 050 ----
mean loss: 344.93
 ---- batch: 060 ----
mean loss: 334.85
 ---- batch: 070 ----
mean loss: 338.33
 ---- batch: 080 ----
mean loss: 347.61
 ---- batch: 090 ----
mean loss: 343.49
 ---- batch: 100 ----
mean loss: 351.76
 ---- batch: 110 ----
mean loss: 356.26
train mean loss: 341.93
epoch train time: 0:00:02.129722
elapsed time: 0:06:57.060669
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-26 23:54:40.144278
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 339.13
 ---- batch: 020 ----
mean loss: 347.43
 ---- batch: 030 ----
mean loss: 329.99
 ---- batch: 040 ----
mean loss: 328.39
 ---- batch: 050 ----
mean loss: 338.72
 ---- batch: 060 ----
mean loss: 331.42
 ---- batch: 070 ----
mean loss: 330.45
 ---- batch: 080 ----
mean loss: 345.63
 ---- batch: 090 ----
mean loss: 337.63
 ---- batch: 100 ----
mean loss: 336.91
 ---- batch: 110 ----
mean loss: 334.80
train mean loss: 336.72
epoch train time: 0:00:02.128317
elapsed time: 0:06:59.189174
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-26 23:54:42.272770
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.57
 ---- batch: 020 ----
mean loss: 340.25
 ---- batch: 030 ----
mean loss: 346.79
 ---- batch: 040 ----
mean loss: 336.07
 ---- batch: 050 ----
mean loss: 338.71
 ---- batch: 060 ----
mean loss: 338.09
 ---- batch: 070 ----
mean loss: 334.67
 ---- batch: 080 ----
mean loss: 333.74
 ---- batch: 090 ----
mean loss: 333.79
 ---- batch: 100 ----
mean loss: 327.80
 ---- batch: 110 ----
mean loss: 325.25
train mean loss: 336.02
epoch train time: 0:00:02.125101
elapsed time: 0:07:01.314449
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-26 23:54:44.398031
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.85
 ---- batch: 020 ----
mean loss: 333.73
 ---- batch: 030 ----
mean loss: 332.37
 ---- batch: 040 ----
mean loss: 333.97
 ---- batch: 050 ----
mean loss: 328.05
 ---- batch: 060 ----
mean loss: 346.10
 ---- batch: 070 ----
mean loss: 335.32
 ---- batch: 080 ----
mean loss: 332.58
 ---- batch: 090 ----
mean loss: 351.85
 ---- batch: 100 ----
mean loss: 349.04
 ---- batch: 110 ----
mean loss: 335.38
train mean loss: 337.00
epoch train time: 0:00:02.134855
elapsed time: 0:07:03.449507
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-26 23:54:46.533090
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.11
 ---- batch: 020 ----
mean loss: 345.66
 ---- batch: 030 ----
mean loss: 343.57
 ---- batch: 040 ----
mean loss: 336.12
 ---- batch: 050 ----
mean loss: 345.59
 ---- batch: 060 ----
mean loss: 348.13
 ---- batch: 070 ----
mean loss: 336.77
 ---- batch: 080 ----
mean loss: 328.69
 ---- batch: 090 ----
mean loss: 327.46
 ---- batch: 100 ----
mean loss: 329.82
 ---- batch: 110 ----
mean loss: 331.13
train mean loss: 335.90
epoch train time: 0:00:02.123820
elapsed time: 0:07:05.573482
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-26 23:54:48.657066
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 347.19
 ---- batch: 020 ----
mean loss: 337.64
 ---- batch: 030 ----
mean loss: 349.61
 ---- batch: 040 ----
mean loss: 340.83
 ---- batch: 050 ----
mean loss: 322.75
 ---- batch: 060 ----
mean loss: 333.22
 ---- batch: 070 ----
mean loss: 336.50
 ---- batch: 080 ----
mean loss: 333.93
 ---- batch: 090 ----
mean loss: 328.17
 ---- batch: 100 ----
mean loss: 332.63
 ---- batch: 110 ----
mean loss: 327.46
train mean loss: 335.58
epoch train time: 0:00:02.132941
elapsed time: 0:07:07.706591
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-26 23:54:50.790197
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 344.25
 ---- batch: 020 ----
mean loss: 333.54
 ---- batch: 030 ----
mean loss: 341.84
 ---- batch: 040 ----
mean loss: 325.33
 ---- batch: 050 ----
mean loss: 346.78
 ---- batch: 060 ----
mean loss: 330.38
 ---- batch: 070 ----
mean loss: 330.84
 ---- batch: 080 ----
mean loss: 317.85
 ---- batch: 090 ----
mean loss: 325.50
 ---- batch: 100 ----
mean loss: 338.82
 ---- batch: 110 ----
mean loss: 328.83
train mean loss: 333.03
epoch train time: 0:00:02.125192
elapsed time: 0:07:09.831999
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-26 23:54:52.915591
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.44
 ---- batch: 020 ----
mean loss: 324.12
 ---- batch: 030 ----
mean loss: 333.79
 ---- batch: 040 ----
mean loss: 330.65
 ---- batch: 050 ----
mean loss: 343.86
 ---- batch: 060 ----
mean loss: 336.33
 ---- batch: 070 ----
mean loss: 331.73
 ---- batch: 080 ----
mean loss: 331.61
 ---- batch: 090 ----
mean loss: 339.22
 ---- batch: 100 ----
mean loss: 329.01
 ---- batch: 110 ----
mean loss: 324.43
train mean loss: 332.19
epoch train time: 0:00:02.130078
elapsed time: 0:07:11.962252
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-26 23:54:55.045851
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.92
 ---- batch: 020 ----
mean loss: 347.90
 ---- batch: 030 ----
mean loss: 338.71
 ---- batch: 040 ----
mean loss: 329.92
 ---- batch: 050 ----
mean loss: 330.75
 ---- batch: 060 ----
mean loss: 326.64
 ---- batch: 070 ----
mean loss: 330.66
 ---- batch: 080 ----
mean loss: 334.64
 ---- batch: 090 ----
mean loss: 337.13
 ---- batch: 100 ----
mean loss: 335.63
 ---- batch: 110 ----
mean loss: 336.51
train mean loss: 335.34
epoch train time: 0:00:02.123980
elapsed time: 0:07:14.086405
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-26 23:54:57.170007
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 321.74
 ---- batch: 020 ----
mean loss: 332.31
 ---- batch: 030 ----
mean loss: 347.69
 ---- batch: 040 ----
mean loss: 333.65
 ---- batch: 050 ----
mean loss: 336.31
 ---- batch: 060 ----
mean loss: 334.79
 ---- batch: 070 ----
mean loss: 330.03
 ---- batch: 080 ----
mean loss: 333.15
 ---- batch: 090 ----
mean loss: 330.12
 ---- batch: 100 ----
mean loss: 318.80
 ---- batch: 110 ----
mean loss: 327.61
train mean loss: 331.32
epoch train time: 0:00:02.127388
elapsed time: 0:07:16.213961
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-26 23:54:59.297540
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.27
 ---- batch: 020 ----
mean loss: 327.63
 ---- batch: 030 ----
mean loss: 344.79
 ---- batch: 040 ----
mean loss: 328.15
 ---- batch: 050 ----
mean loss: 327.86
 ---- batch: 060 ----
mean loss: 325.42
 ---- batch: 070 ----
mean loss: 336.32
 ---- batch: 080 ----
mean loss: 336.81
 ---- batch: 090 ----
mean loss: 318.21
 ---- batch: 100 ----
mean loss: 332.00
 ---- batch: 110 ----
mean loss: 342.05
train mean loss: 330.79
epoch train time: 0:00:02.126082
elapsed time: 0:07:18.340194
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-26 23:55:01.423777
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 336.22
 ---- batch: 020 ----
mean loss: 320.12
 ---- batch: 030 ----
mean loss: 328.11
 ---- batch: 040 ----
mean loss: 335.46
 ---- batch: 050 ----
mean loss: 342.47
 ---- batch: 060 ----
mean loss: 331.22
 ---- batch: 070 ----
mean loss: 325.75
 ---- batch: 080 ----
mean loss: 326.60
 ---- batch: 090 ----
mean loss: 333.41
 ---- batch: 100 ----
mean loss: 334.75
 ---- batch: 110 ----
mean loss: 329.37
train mean loss: 331.03
epoch train time: 0:00:02.120245
elapsed time: 0:07:20.460598
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-26 23:55:03.544210
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.78
 ---- batch: 020 ----
mean loss: 332.35
 ---- batch: 030 ----
mean loss: 329.52
 ---- batch: 040 ----
mean loss: 329.10
 ---- batch: 050 ----
mean loss: 337.23
 ---- batch: 060 ----
mean loss: 348.70
 ---- batch: 070 ----
mean loss: 323.59
 ---- batch: 080 ----
mean loss: 334.53
 ---- batch: 090 ----
mean loss: 322.95
 ---- batch: 100 ----
mean loss: 323.83
 ---- batch: 110 ----
mean loss: 328.11
train mean loss: 331.46
epoch train time: 0:00:02.126985
elapsed time: 0:07:22.587772
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-26 23:55:05.671354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 342.55
 ---- batch: 020 ----
mean loss: 336.03
 ---- batch: 030 ----
mean loss: 337.28
 ---- batch: 040 ----
mean loss: 333.79
 ---- batch: 050 ----
mean loss: 336.64
 ---- batch: 060 ----
mean loss: 315.03
 ---- batch: 070 ----
mean loss: 318.46
 ---- batch: 080 ----
mean loss: 334.91
 ---- batch: 090 ----
mean loss: 324.88
 ---- batch: 100 ----
mean loss: 342.44
 ---- batch: 110 ----
mean loss: 345.17
train mean loss: 333.62
epoch train time: 0:00:02.130165
elapsed time: 0:07:24.718124
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-26 23:55:07.801711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 337.53
 ---- batch: 020 ----
mean loss: 355.40
 ---- batch: 030 ----
mean loss: 331.72
 ---- batch: 040 ----
mean loss: 326.37
 ---- batch: 050 ----
mean loss: 323.90
 ---- batch: 060 ----
mean loss: 331.47
 ---- batch: 070 ----
mean loss: 336.88
 ---- batch: 080 ----
mean loss: 317.06
 ---- batch: 090 ----
mean loss: 331.68
 ---- batch: 100 ----
mean loss: 332.27
 ---- batch: 110 ----
mean loss: 327.47
train mean loss: 331.43
epoch train time: 0:00:02.128625
elapsed time: 0:07:26.846910
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-26 23:55:09.930510
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 319.46
 ---- batch: 020 ----
mean loss: 328.82
 ---- batch: 030 ----
mean loss: 330.90
 ---- batch: 040 ----
mean loss: 324.97
 ---- batch: 050 ----
mean loss: 320.99
 ---- batch: 060 ----
mean loss: 323.99
 ---- batch: 070 ----
mean loss: 324.70
 ---- batch: 080 ----
mean loss: 333.12
 ---- batch: 090 ----
mean loss: 336.81
 ---- batch: 100 ----
mean loss: 320.07
 ---- batch: 110 ----
mean loss: 332.46
train mean loss: 327.21
epoch train time: 0:00:02.127671
elapsed time: 0:07:28.974766
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-26 23:55:12.058346
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 338.61
 ---- batch: 020 ----
mean loss: 333.74
 ---- batch: 030 ----
mean loss: 331.66
 ---- batch: 040 ----
mean loss: 351.45
 ---- batch: 050 ----
mean loss: 324.87
 ---- batch: 060 ----
mean loss: 325.73
 ---- batch: 070 ----
mean loss: 333.73
 ---- batch: 080 ----
mean loss: 328.26
 ---- batch: 090 ----
mean loss: 333.05
 ---- batch: 100 ----
mean loss: 326.23
 ---- batch: 110 ----
mean loss: 329.07
train mean loss: 332.33
epoch train time: 0:00:02.126168
elapsed time: 0:07:31.101085
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-26 23:55:14.184666
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 325.71
 ---- batch: 020 ----
mean loss: 323.51
 ---- batch: 030 ----
mean loss: 336.39
 ---- batch: 040 ----
mean loss: 329.09
 ---- batch: 050 ----
mean loss: 339.55
 ---- batch: 060 ----
mean loss: 321.45
 ---- batch: 070 ----
mean loss: 335.71
 ---- batch: 080 ----
mean loss: 329.96
 ---- batch: 090 ----
mean loss: 320.54
 ---- batch: 100 ----
mean loss: 329.61
 ---- batch: 110 ----
mean loss: 322.32
train mean loss: 328.40
epoch train time: 0:00:02.127237
elapsed time: 0:07:33.228472
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-26 23:55:16.312053
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 334.16
 ---- batch: 020 ----
mean loss: 320.70
 ---- batch: 030 ----
mean loss: 335.83
 ---- batch: 040 ----
mean loss: 327.89
 ---- batch: 050 ----
mean loss: 330.03
 ---- batch: 060 ----
mean loss: 325.78
 ---- batch: 070 ----
mean loss: 320.64
 ---- batch: 080 ----
mean loss: 324.26
 ---- batch: 090 ----
mean loss: 322.27
 ---- batch: 100 ----
mean loss: 323.70
 ---- batch: 110 ----
mean loss: 326.59
train mean loss: 326.39
epoch train time: 0:00:02.125793
elapsed time: 0:07:35.354418
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-26 23:55:18.438003
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 329.77
 ---- batch: 020 ----
mean loss: 312.00
 ---- batch: 030 ----
mean loss: 328.40
 ---- batch: 040 ----
mean loss: 313.99
 ---- batch: 050 ----
mean loss: 320.83
 ---- batch: 060 ----
mean loss: 340.33
 ---- batch: 070 ----
mean loss: 334.51
 ---- batch: 080 ----
mean loss: 331.56
 ---- batch: 090 ----
mean loss: 319.78
 ---- batch: 100 ----
mean loss: 315.08
 ---- batch: 110 ----
mean loss: 333.75
train mean loss: 325.69
epoch train time: 0:00:02.124129
elapsed time: 0:07:37.478712
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-26 23:55:20.562300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 318.18
 ---- batch: 020 ----
mean loss: 317.05
 ---- batch: 030 ----
mean loss: 322.22
 ---- batch: 040 ----
mean loss: 331.02
 ---- batch: 050 ----
mean loss: 329.59
 ---- batch: 060 ----
mean loss: 325.03
 ---- batch: 070 ----
mean loss: 328.95
 ---- batch: 080 ----
mean loss: 317.91
 ---- batch: 090 ----
mean loss: 324.75
 ---- batch: 100 ----
mean loss: 319.64
 ---- batch: 110 ----
mean loss: 345.55
train mean loss: 325.95
epoch train time: 0:00:02.123744
elapsed time: 0:07:39.602617
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-26 23:55:22.686202
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.28
 ---- batch: 020 ----
mean loss: 314.35
 ---- batch: 030 ----
mean loss: 319.80
 ---- batch: 040 ----
mean loss: 319.07
 ---- batch: 050 ----
mean loss: 319.45
 ---- batch: 060 ----
mean loss: 332.34
 ---- batch: 070 ----
mean loss: 306.83
 ---- batch: 080 ----
mean loss: 318.95
 ---- batch: 090 ----
mean loss: 327.03
 ---- batch: 100 ----
mean loss: 311.74
 ---- batch: 110 ----
mean loss: 310.80
train mean loss: 317.29
epoch train time: 0:00:02.140474
elapsed time: 0:07:41.743281
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-26 23:55:24.826880
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.56
 ---- batch: 020 ----
mean loss: 317.07
 ---- batch: 030 ----
mean loss: 312.11
 ---- batch: 040 ----
mean loss: 323.75
 ---- batch: 050 ----
mean loss: 318.64
 ---- batch: 060 ----
mean loss: 319.88
 ---- batch: 070 ----
mean loss: 307.98
 ---- batch: 080 ----
mean loss: 325.89
 ---- batch: 090 ----
mean loss: 329.35
 ---- batch: 100 ----
mean loss: 314.92
 ---- batch: 110 ----
mean loss: 304.69
train mean loss: 317.86
epoch train time: 0:00:02.124535
elapsed time: 0:07:43.867983
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-26 23:55:26.951566
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.17
 ---- batch: 020 ----
mean loss: 322.02
 ---- batch: 030 ----
mean loss: 320.01
 ---- batch: 040 ----
mean loss: 314.17
 ---- batch: 050 ----
mean loss: 328.77
 ---- batch: 060 ----
mean loss: 312.37
 ---- batch: 070 ----
mean loss: 313.91
 ---- batch: 080 ----
mean loss: 325.91
 ---- batch: 090 ----
mean loss: 317.08
 ---- batch: 100 ----
mean loss: 307.59
 ---- batch: 110 ----
mean loss: 321.21
train mean loss: 317.67
epoch train time: 0:00:02.124794
elapsed time: 0:07:45.992935
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-26 23:55:29.076518
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.30
 ---- batch: 020 ----
mean loss: 315.52
 ---- batch: 030 ----
mean loss: 322.18
 ---- batch: 040 ----
mean loss: 321.54
 ---- batch: 050 ----
mean loss: 320.48
 ---- batch: 060 ----
mean loss: 315.58
 ---- batch: 070 ----
mean loss: 320.44
 ---- batch: 080 ----
mean loss: 322.92
 ---- batch: 090 ----
mean loss: 315.71
 ---- batch: 100 ----
mean loss: 312.17
 ---- batch: 110 ----
mean loss: 310.27
train mean loss: 317.38
epoch train time: 0:00:02.125354
elapsed time: 0:07:48.118439
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-26 23:55:31.202018
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.43
 ---- batch: 020 ----
mean loss: 307.67
 ---- batch: 030 ----
mean loss: 314.85
 ---- batch: 040 ----
mean loss: 315.90
 ---- batch: 050 ----
mean loss: 323.32
 ---- batch: 060 ----
mean loss: 317.15
 ---- batch: 070 ----
mean loss: 325.77
 ---- batch: 080 ----
mean loss: 317.00
 ---- batch: 090 ----
mean loss: 318.31
 ---- batch: 100 ----
mean loss: 308.98
 ---- batch: 110 ----
mean loss: 318.17
train mean loss: 316.66
epoch train time: 0:00:02.128156
elapsed time: 0:07:50.246738
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-26 23:55:33.330316
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.56
 ---- batch: 020 ----
mean loss: 315.39
 ---- batch: 030 ----
mean loss: 308.61
 ---- batch: 040 ----
mean loss: 316.49
 ---- batch: 050 ----
mean loss: 314.04
 ---- batch: 060 ----
mean loss: 317.24
 ---- batch: 070 ----
mean loss: 326.30
 ---- batch: 080 ----
mean loss: 320.35
 ---- batch: 090 ----
mean loss: 302.05
 ---- batch: 100 ----
mean loss: 307.92
 ---- batch: 110 ----
mean loss: 326.37
train mean loss: 315.91
epoch train time: 0:00:02.118680
elapsed time: 0:07:52.365569
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-26 23:55:35.449194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.24
 ---- batch: 020 ----
mean loss: 321.17
 ---- batch: 030 ----
mean loss: 317.27
 ---- batch: 040 ----
mean loss: 320.22
 ---- batch: 050 ----
mean loss: 309.09
 ---- batch: 060 ----
mean loss: 314.14
 ---- batch: 070 ----
mean loss: 312.42
 ---- batch: 080 ----
mean loss: 322.03
 ---- batch: 090 ----
mean loss: 313.09
 ---- batch: 100 ----
mean loss: 336.35
 ---- batch: 110 ----
mean loss: 318.86
train mean loss: 317.66
epoch train time: 0:00:02.120887
elapsed time: 0:07:54.486651
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-26 23:55:37.570253
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.23
 ---- batch: 020 ----
mean loss: 319.76
 ---- batch: 030 ----
mean loss: 309.43
 ---- batch: 040 ----
mean loss: 316.27
 ---- batch: 050 ----
mean loss: 311.96
 ---- batch: 060 ----
mean loss: 326.91
 ---- batch: 070 ----
mean loss: 324.64
 ---- batch: 080 ----
mean loss: 316.55
 ---- batch: 090 ----
mean loss: 313.40
 ---- batch: 100 ----
mean loss: 315.67
 ---- batch: 110 ----
mean loss: 311.70
train mean loss: 316.00
epoch train time: 0:00:02.118211
elapsed time: 0:07:56.605032
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-26 23:55:39.688614
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.41
 ---- batch: 020 ----
mean loss: 301.18
 ---- batch: 030 ----
mean loss: 321.54
 ---- batch: 040 ----
mean loss: 319.00
 ---- batch: 050 ----
mean loss: 307.23
 ---- batch: 060 ----
mean loss: 313.63
 ---- batch: 070 ----
mean loss: 308.65
 ---- batch: 080 ----
mean loss: 309.73
 ---- batch: 090 ----
mean loss: 325.15
 ---- batch: 100 ----
mean loss: 318.24
 ---- batch: 110 ----
mean loss: 328.10
train mean loss: 314.86
epoch train time: 0:00:02.119119
elapsed time: 0:07:58.724304
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-26 23:55:41.807889
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 319.44
 ---- batch: 020 ----
mean loss: 316.95
 ---- batch: 030 ----
mean loss: 322.79
 ---- batch: 040 ----
mean loss: 318.20
 ---- batch: 050 ----
mean loss: 321.47
 ---- batch: 060 ----
mean loss: 315.78
 ---- batch: 070 ----
mean loss: 308.08
 ---- batch: 080 ----
mean loss: 318.37
 ---- batch: 090 ----
mean loss: 320.73
 ---- batch: 100 ----
mean loss: 307.32
 ---- batch: 110 ----
mean loss: 312.26
train mean loss: 317.36
epoch train time: 0:00:02.119909
elapsed time: 0:08:00.844378
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-26 23:55:43.927960
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.43
 ---- batch: 020 ----
mean loss: 319.82
 ---- batch: 030 ----
mean loss: 325.40
 ---- batch: 040 ----
mean loss: 319.00
 ---- batch: 050 ----
mean loss: 321.97
 ---- batch: 060 ----
mean loss: 322.90
 ---- batch: 070 ----
mean loss: 306.77
 ---- batch: 080 ----
mean loss: 313.62
 ---- batch: 090 ----
mean loss: 318.87
 ---- batch: 100 ----
mean loss: 313.01
 ---- batch: 110 ----
mean loss: 314.69
train mean loss: 317.78
epoch train time: 0:00:02.119095
elapsed time: 0:08:02.963655
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-26 23:55:46.047237
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.87
 ---- batch: 020 ----
mean loss: 316.97
 ---- batch: 030 ----
mean loss: 319.34
 ---- batch: 040 ----
mean loss: 312.98
 ---- batch: 050 ----
mean loss: 322.04
 ---- batch: 060 ----
mean loss: 318.96
 ---- batch: 070 ----
mean loss: 320.86
 ---- batch: 080 ----
mean loss: 319.20
 ---- batch: 090 ----
mean loss: 311.93
 ---- batch: 100 ----
mean loss: 312.29
 ---- batch: 110 ----
mean loss: 315.26
train mean loss: 316.66
epoch train time: 0:00:02.125264
elapsed time: 0:08:05.089070
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-26 23:55:48.172653
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 316.93
 ---- batch: 020 ----
mean loss: 317.58
 ---- batch: 030 ----
mean loss: 312.72
 ---- batch: 040 ----
mean loss: 323.74
 ---- batch: 050 ----
mean loss: 312.24
 ---- batch: 060 ----
mean loss: 320.76
 ---- batch: 070 ----
mean loss: 311.54
 ---- batch: 080 ----
mean loss: 312.84
 ---- batch: 090 ----
mean loss: 312.79
 ---- batch: 100 ----
mean loss: 328.87
 ---- batch: 110 ----
mean loss: 326.99
train mean loss: 318.44
epoch train time: 0:00:02.129412
elapsed time: 0:08:07.218647
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-26 23:55:50.302228
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.96
 ---- batch: 020 ----
mean loss: 317.64
 ---- batch: 030 ----
mean loss: 319.43
 ---- batch: 040 ----
mean loss: 305.64
 ---- batch: 050 ----
mean loss: 318.45
 ---- batch: 060 ----
mean loss: 309.26
 ---- batch: 070 ----
mean loss: 323.40
 ---- batch: 080 ----
mean loss: 321.60
 ---- batch: 090 ----
mean loss: 311.25
 ---- batch: 100 ----
mean loss: 310.90
 ---- batch: 110 ----
mean loss: 319.28
train mean loss: 315.12
epoch train time: 0:00:02.127026
elapsed time: 0:08:09.345827
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-26 23:55:52.429441
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.48
 ---- batch: 020 ----
mean loss: 322.22
 ---- batch: 030 ----
mean loss: 318.46
 ---- batch: 040 ----
mean loss: 319.12
 ---- batch: 050 ----
mean loss: 303.73
 ---- batch: 060 ----
mean loss: 312.66
 ---- batch: 070 ----
mean loss: 318.83
 ---- batch: 080 ----
mean loss: 307.40
 ---- batch: 090 ----
mean loss: 303.61
 ---- batch: 100 ----
mean loss: 318.42
 ---- batch: 110 ----
mean loss: 317.85
train mean loss: 314.92
epoch train time: 0:00:02.129317
elapsed time: 0:08:11.475341
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-26 23:55:54.558934
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.50
 ---- batch: 020 ----
mean loss: 319.14
 ---- batch: 030 ----
mean loss: 322.21
 ---- batch: 040 ----
mean loss: 312.30
 ---- batch: 050 ----
mean loss: 318.01
 ---- batch: 060 ----
mean loss: 314.31
 ---- batch: 070 ----
mean loss: 315.06
 ---- batch: 080 ----
mean loss: 316.26
 ---- batch: 090 ----
mean loss: 314.82
 ---- batch: 100 ----
mean loss: 320.73
 ---- batch: 110 ----
mean loss: 311.58
train mean loss: 316.32
epoch train time: 0:00:02.117963
elapsed time: 0:08:13.593467
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-26 23:55:56.677052
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 312.68
 ---- batch: 020 ----
mean loss: 307.09
 ---- batch: 030 ----
mean loss: 319.10
 ---- batch: 040 ----
mean loss: 315.33
 ---- batch: 050 ----
mean loss: 321.66
 ---- batch: 060 ----
mean loss: 326.17
 ---- batch: 070 ----
mean loss: 318.27
 ---- batch: 080 ----
mean loss: 311.77
 ---- batch: 090 ----
mean loss: 309.31
 ---- batch: 100 ----
mean loss: 325.53
 ---- batch: 110 ----
mean loss: 321.89
train mean loss: 316.72
epoch train time: 0:00:02.121152
elapsed time: 0:08:15.714776
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-26 23:55:58.798361
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.84
 ---- batch: 020 ----
mean loss: 313.71
 ---- batch: 030 ----
mean loss: 313.75
 ---- batch: 040 ----
mean loss: 330.54
 ---- batch: 050 ----
mean loss: 320.24
 ---- batch: 060 ----
mean loss: 324.41
 ---- batch: 070 ----
mean loss: 311.34
 ---- batch: 080 ----
mean loss: 312.27
 ---- batch: 090 ----
mean loss: 310.96
 ---- batch: 100 ----
mean loss: 310.74
 ---- batch: 110 ----
mean loss: 309.46
train mean loss: 314.76
epoch train time: 0:00:02.130119
elapsed time: 0:08:17.845053
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-26 23:56:00.928642
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.66
 ---- batch: 020 ----
mean loss: 310.94
 ---- batch: 030 ----
mean loss: 310.78
 ---- batch: 040 ----
mean loss: 316.84
 ---- batch: 050 ----
mean loss: 320.48
 ---- batch: 060 ----
mean loss: 309.78
 ---- batch: 070 ----
mean loss: 308.15
 ---- batch: 080 ----
mean loss: 323.12
 ---- batch: 090 ----
mean loss: 314.92
 ---- batch: 100 ----
mean loss: 311.47
 ---- batch: 110 ----
mean loss: 314.68
train mean loss: 314.35
epoch train time: 0:00:02.123731
elapsed time: 0:08:19.968948
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-26 23:56:03.052536
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 311.16
 ---- batch: 020 ----
mean loss: 322.84
 ---- batch: 030 ----
mean loss: 313.69
 ---- batch: 040 ----
mean loss: 317.32
 ---- batch: 050 ----
mean loss: 313.19
 ---- batch: 060 ----
mean loss: 308.59
 ---- batch: 070 ----
mean loss: 322.53
 ---- batch: 080 ----
mean loss: 315.06
 ---- batch: 090 ----
mean loss: 320.13
 ---- batch: 100 ----
mean loss: 310.37
 ---- batch: 110 ----
mean loss: 312.08
train mean loss: 314.93
epoch train time: 0:00:02.122105
elapsed time: 0:08:22.091218
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-26 23:56:05.174824
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.23
 ---- batch: 020 ----
mean loss: 314.21
 ---- batch: 030 ----
mean loss: 323.16
 ---- batch: 040 ----
mean loss: 314.50
 ---- batch: 050 ----
mean loss: 307.93
 ---- batch: 060 ----
mean loss: 306.83
 ---- batch: 070 ----
mean loss: 312.90
 ---- batch: 080 ----
mean loss: 315.10
 ---- batch: 090 ----
mean loss: 318.02
 ---- batch: 100 ----
mean loss: 313.02
 ---- batch: 110 ----
mean loss: 313.96
train mean loss: 313.78
epoch train time: 0:00:02.119756
elapsed time: 0:08:24.211163
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-26 23:56:07.294758
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.32
 ---- batch: 020 ----
mean loss: 317.45
 ---- batch: 030 ----
mean loss: 325.55
 ---- batch: 040 ----
mean loss: 314.23
 ---- batch: 050 ----
mean loss: 317.07
 ---- batch: 060 ----
mean loss: 315.43
 ---- batch: 070 ----
mean loss: 307.04
 ---- batch: 080 ----
mean loss: 310.44
 ---- batch: 090 ----
mean loss: 313.84
 ---- batch: 100 ----
mean loss: 309.16
 ---- batch: 110 ----
mean loss: 311.57
train mean loss: 313.94
epoch train time: 0:00:02.121468
elapsed time: 0:08:26.332797
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-26 23:56:09.416379
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 321.60
 ---- batch: 020 ----
mean loss: 318.07
 ---- batch: 030 ----
mean loss: 310.64
 ---- batch: 040 ----
mean loss: 315.15
 ---- batch: 050 ----
mean loss: 322.25
 ---- batch: 060 ----
mean loss: 314.82
 ---- batch: 070 ----
mean loss: 313.55
 ---- batch: 080 ----
mean loss: 307.92
 ---- batch: 090 ----
mean loss: 311.80
 ---- batch: 100 ----
mean loss: 320.40
 ---- batch: 110 ----
mean loss: 314.41
train mean loss: 315.76
epoch train time: 0:00:02.122972
elapsed time: 0:08:28.455919
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-26 23:56:11.539500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.68
 ---- batch: 020 ----
mean loss: 321.24
 ---- batch: 030 ----
mean loss: 311.72
 ---- batch: 040 ----
mean loss: 315.72
 ---- batch: 050 ----
mean loss: 325.44
 ---- batch: 060 ----
mean loss: 316.50
 ---- batch: 070 ----
mean loss: 310.75
 ---- batch: 080 ----
mean loss: 307.41
 ---- batch: 090 ----
mean loss: 320.21
 ---- batch: 100 ----
mean loss: 299.32
 ---- batch: 110 ----
mean loss: 311.26
train mean loss: 313.91
epoch train time: 0:00:02.117275
elapsed time: 0:08:30.573359
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-26 23:56:13.656945
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.17
 ---- batch: 020 ----
mean loss: 306.89
 ---- batch: 030 ----
mean loss: 315.99
 ---- batch: 040 ----
mean loss: 324.36
 ---- batch: 050 ----
mean loss: 311.27
 ---- batch: 060 ----
mean loss: 317.76
 ---- batch: 070 ----
mean loss: 309.71
 ---- batch: 080 ----
mean loss: 303.23
 ---- batch: 090 ----
mean loss: 314.33
 ---- batch: 100 ----
mean loss: 310.67
 ---- batch: 110 ----
mean loss: 307.68
train mean loss: 313.13
epoch train time: 0:00:02.117577
elapsed time: 0:08:32.691097
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-26 23:56:15.774679
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.31
 ---- batch: 020 ----
mean loss: 320.00
 ---- batch: 030 ----
mean loss: 314.61
 ---- batch: 040 ----
mean loss: 314.97
 ---- batch: 050 ----
mean loss: 319.53
 ---- batch: 060 ----
mean loss: 302.30
 ---- batch: 070 ----
mean loss: 321.38
 ---- batch: 080 ----
mean loss: 321.33
 ---- batch: 090 ----
mean loss: 320.80
 ---- batch: 100 ----
mean loss: 323.91
 ---- batch: 110 ----
mean loss: 321.88
train mean loss: 317.28
epoch train time: 0:00:02.131150
elapsed time: 0:08:34.822400
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-26 23:56:17.906008
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.02
 ---- batch: 020 ----
mean loss: 311.23
 ---- batch: 030 ----
mean loss: 307.00
 ---- batch: 040 ----
mean loss: 319.25
 ---- batch: 050 ----
mean loss: 310.00
 ---- batch: 060 ----
mean loss: 313.08
 ---- batch: 070 ----
mean loss: 329.45
 ---- batch: 080 ----
mean loss: 320.71
 ---- batch: 090 ----
mean loss: 317.19
 ---- batch: 100 ----
mean loss: 318.16
 ---- batch: 110 ----
mean loss: 305.76
train mean loss: 314.33
epoch train time: 0:00:02.125887
elapsed time: 0:08:36.948468
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-26 23:56:20.032073
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.06
 ---- batch: 020 ----
mean loss: 314.48
 ---- batch: 030 ----
mean loss: 309.52
 ---- batch: 040 ----
mean loss: 314.62
 ---- batch: 050 ----
mean loss: 319.96
 ---- batch: 060 ----
mean loss: 318.98
 ---- batch: 070 ----
mean loss: 323.81
 ---- batch: 080 ----
mean loss: 315.87
 ---- batch: 090 ----
mean loss: 314.90
 ---- batch: 100 ----
mean loss: 306.43
 ---- batch: 110 ----
mean loss: 309.53
train mean loss: 314.79
epoch train time: 0:00:02.119644
elapsed time: 0:08:39.068286
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-26 23:56:22.151867
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 314.49
 ---- batch: 020 ----
mean loss: 320.69
 ---- batch: 030 ----
mean loss: 326.58
 ---- batch: 040 ----
mean loss: 313.74
 ---- batch: 050 ----
mean loss: 313.82
 ---- batch: 060 ----
mean loss: 317.06
 ---- batch: 070 ----
mean loss: 318.05
 ---- batch: 080 ----
mean loss: 304.23
 ---- batch: 090 ----
mean loss: 311.75
 ---- batch: 100 ----
mean loss: 318.65
 ---- batch: 110 ----
mean loss: 315.06
train mean loss: 315.55
epoch train time: 0:00:02.122594
elapsed time: 0:08:41.191031
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-26 23:56:24.274613
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.12
 ---- batch: 020 ----
mean loss: 310.91
 ---- batch: 030 ----
mean loss: 320.22
 ---- batch: 040 ----
mean loss: 328.88
 ---- batch: 050 ----
mean loss: 313.17
 ---- batch: 060 ----
mean loss: 316.58
 ---- batch: 070 ----
mean loss: 319.58
 ---- batch: 080 ----
mean loss: 309.12
 ---- batch: 090 ----
mean loss: 309.26
 ---- batch: 100 ----
mean loss: 302.61
 ---- batch: 110 ----
mean loss: 317.27
train mean loss: 315.66
epoch train time: 0:00:02.118096
elapsed time: 0:08:43.309280
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-26 23:56:26.392863
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 308.97
 ---- batch: 020 ----
mean loss: 307.86
 ---- batch: 030 ----
mean loss: 311.43
 ---- batch: 040 ----
mean loss: 317.41
 ---- batch: 050 ----
mean loss: 325.58
 ---- batch: 060 ----
mean loss: 318.52
 ---- batch: 070 ----
mean loss: 314.61
 ---- batch: 080 ----
mean loss: 325.94
 ---- batch: 090 ----
mean loss: 309.40
 ---- batch: 100 ----
mean loss: 317.81
 ---- batch: 110 ----
mean loss: 305.14
train mean loss: 314.74
epoch train time: 0:00:02.122506
elapsed time: 0:08:45.431960
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-26 23:56:28.515542
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.31
 ---- batch: 020 ----
mean loss: 309.41
 ---- batch: 030 ----
mean loss: 320.14
 ---- batch: 040 ----
mean loss: 320.30
 ---- batch: 050 ----
mean loss: 301.92
 ---- batch: 060 ----
mean loss: 312.60
 ---- batch: 070 ----
mean loss: 323.13
 ---- batch: 080 ----
mean loss: 312.70
 ---- batch: 090 ----
mean loss: 310.90
 ---- batch: 100 ----
mean loss: 307.55
 ---- batch: 110 ----
mean loss: 307.90
train mean loss: 313.10
epoch train time: 0:00:02.118771
elapsed time: 0:08:47.550892
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-26 23:56:30.634476
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 318.39
 ---- batch: 020 ----
mean loss: 306.24
 ---- batch: 030 ----
mean loss: 314.05
 ---- batch: 040 ----
mean loss: 314.16
 ---- batch: 050 ----
mean loss: 309.01
 ---- batch: 060 ----
mean loss: 313.35
 ---- batch: 070 ----
mean loss: 326.03
 ---- batch: 080 ----
mean loss: 317.69
 ---- batch: 090 ----
mean loss: 314.50
 ---- batch: 100 ----
mean loss: 323.57
 ---- batch: 110 ----
mean loss: 310.72
train mean loss: 314.70
epoch train time: 0:00:02.120319
elapsed time: 0:08:49.671398
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-26 23:56:32.754979
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 315.89
 ---- batch: 020 ----
mean loss: 299.61
 ---- batch: 030 ----
mean loss: 303.72
 ---- batch: 040 ----
mean loss: 313.52
 ---- batch: 050 ----
mean loss: 301.93
 ---- batch: 060 ----
mean loss: 318.05
 ---- batch: 070 ----
mean loss: 319.57
 ---- batch: 080 ----
mean loss: 316.37
 ---- batch: 090 ----
mean loss: 314.73
 ---- batch: 100 ----
mean loss: 311.37
 ---- batch: 110 ----
mean loss: 313.19
train mean loss: 311.75
epoch train time: 0:00:02.129054
elapsed time: 0:08:51.800601
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-26 23:56:34.884195
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.07
 ---- batch: 020 ----
mean loss: 305.18
 ---- batch: 030 ----
mean loss: 317.14
 ---- batch: 040 ----
mean loss: 318.13
 ---- batch: 050 ----
mean loss: 314.55
 ---- batch: 060 ----
mean loss: 314.25
 ---- batch: 070 ----
mean loss: 308.32
 ---- batch: 080 ----
mean loss: 306.47
 ---- batch: 090 ----
mean loss: 312.31
 ---- batch: 100 ----
mean loss: 310.13
 ---- batch: 110 ----
mean loss: 312.53
train mean loss: 312.15
epoch train time: 0:00:02.119691
elapsed time: 0:08:53.920454
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-26 23:56:37.004032
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.65
 ---- batch: 020 ----
mean loss: 319.88
 ---- batch: 030 ----
mean loss: 309.33
 ---- batch: 040 ----
mean loss: 316.93
 ---- batch: 050 ----
mean loss: 311.79
 ---- batch: 060 ----
mean loss: 323.38
 ---- batch: 070 ----
mean loss: 322.95
 ---- batch: 080 ----
mean loss: 303.49
 ---- batch: 090 ----
mean loss: 303.90
 ---- batch: 100 ----
mean loss: 300.22
 ---- batch: 110 ----
mean loss: 313.03
train mean loss: 312.77
epoch train time: 0:00:02.126652
elapsed time: 0:08:56.047253
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-26 23:56:39.130835
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 320.29
 ---- batch: 020 ----
mean loss: 321.17
 ---- batch: 030 ----
mean loss: 309.66
 ---- batch: 040 ----
mean loss: 318.68
 ---- batch: 050 ----
mean loss: 315.31
 ---- batch: 060 ----
mean loss: 311.50
 ---- batch: 070 ----
mean loss: 297.57
 ---- batch: 080 ----
mean loss: 304.30
 ---- batch: 090 ----
mean loss: 321.89
 ---- batch: 100 ----
mean loss: 320.24
 ---- batch: 110 ----
mean loss: 311.62
train mean loss: 314.14
epoch train time: 0:00:02.117159
elapsed time: 0:08:58.164562
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-26 23:56:41.248141
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 311.96
 ---- batch: 020 ----
mean loss: 320.10
 ---- batch: 030 ----
mean loss: 323.60
 ---- batch: 040 ----
mean loss: 314.73
 ---- batch: 050 ----
mean loss: 312.31
 ---- batch: 060 ----
mean loss: 320.56
 ---- batch: 070 ----
mean loss: 333.29
 ---- batch: 080 ----
mean loss: 300.01
 ---- batch: 090 ----
mean loss: 299.38
 ---- batch: 100 ----
mean loss: 319.42
 ---- batch: 110 ----
mean loss: 308.64
train mean loss: 314.63
epoch train time: 0:00:02.117889
elapsed time: 0:09:00.282597
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-26 23:56:43.366194
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 307.83
 ---- batch: 020 ----
mean loss: 308.63
 ---- batch: 030 ----
mean loss: 318.88
 ---- batch: 040 ----
mean loss: 313.83
 ---- batch: 050 ----
mean loss: 313.80
 ---- batch: 060 ----
mean loss: 331.20
 ---- batch: 070 ----
mean loss: 307.88
 ---- batch: 080 ----
mean loss: 322.00
 ---- batch: 090 ----
mean loss: 302.24
 ---- batch: 100 ----
mean loss: 303.27
 ---- batch: 110 ----
mean loss: 307.01
train mean loss: 312.25
epoch train time: 0:00:02.121738
elapsed time: 0:09:02.404501
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-26 23:56:45.488082
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.40
 ---- batch: 020 ----
mean loss: 320.14
 ---- batch: 030 ----
mean loss: 319.52
 ---- batch: 040 ----
mean loss: 308.63
 ---- batch: 050 ----
mean loss: 312.93
 ---- batch: 060 ----
mean loss: 316.48
 ---- batch: 070 ----
mean loss: 318.84
 ---- batch: 080 ----
mean loss: 313.86
 ---- batch: 090 ----
mean loss: 320.29
 ---- batch: 100 ----
mean loss: 314.31
 ---- batch: 110 ----
mean loss: 314.21
train mean loss: 315.44
epoch train time: 0:00:02.121863
elapsed time: 0:09:04.526511
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-26 23:56:47.610109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 305.67
 ---- batch: 020 ----
mean loss: 312.77
 ---- batch: 030 ----
mean loss: 314.52
 ---- batch: 040 ----
mean loss: 303.74
 ---- batch: 050 ----
mean loss: 318.32
 ---- batch: 060 ----
mean loss: 311.32
 ---- batch: 070 ----
mean loss: 310.94
 ---- batch: 080 ----
mean loss: 310.82
 ---- batch: 090 ----
mean loss: 317.20
 ---- batch: 100 ----
mean loss: 322.48
 ---- batch: 110 ----
mean loss: 304.63
train mean loss: 311.92
epoch train time: 0:00:02.120231
elapsed time: 0:09:06.646913
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-26 23:56:49.730494
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 323.69
 ---- batch: 020 ----
mean loss: 331.63
 ---- batch: 030 ----
mean loss: 323.25
 ---- batch: 040 ----
mean loss: 303.70
 ---- batch: 050 ----
mean loss: 306.33
 ---- batch: 060 ----
mean loss: 307.46
 ---- batch: 070 ----
mean loss: 316.98
 ---- batch: 080 ----
mean loss: 302.52
 ---- batch: 090 ----
mean loss: 313.70
 ---- batch: 100 ----
mean loss: 308.04
 ---- batch: 110 ----
mean loss: 303.32
train mean loss: 312.32
epoch train time: 0:00:02.120780
elapsed time: 0:09:08.767846
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-26 23:56:51.851444
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 306.98
 ---- batch: 020 ----
mean loss: 319.31
 ---- batch: 030 ----
mean loss: 309.79
 ---- batch: 040 ----
mean loss: 319.99
 ---- batch: 050 ----
mean loss: 310.31
 ---- batch: 060 ----
mean loss: 314.56
 ---- batch: 070 ----
mean loss: 318.41
 ---- batch: 080 ----
mean loss: 308.34
 ---- batch: 090 ----
mean loss: 311.81
 ---- batch: 100 ----
mean loss: 314.11
 ---- batch: 110 ----
mean loss: 311.81
train mean loss: 312.95
epoch train time: 0:00:02.126349
elapsed time: 0:09:10.894400
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-26 23:56:53.977983
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 310.39
 ---- batch: 020 ----
mean loss: 311.75
 ---- batch: 030 ----
mean loss: 311.07
 ---- batch: 040 ----
mean loss: 313.42
 ---- batch: 050 ----
mean loss: 313.07
 ---- batch: 060 ----
mean loss: 316.52
 ---- batch: 070 ----
mean loss: 321.23
 ---- batch: 080 ----
mean loss: 324.92
 ---- batch: 090 ----
mean loss: 305.66
 ---- batch: 100 ----
mean loss: 320.93
 ---- batch: 110 ----
mean loss: 315.91
train mean loss: 315.30
epoch train time: 0:00:02.120077
elapsed time: 0:09:13.014629
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-26 23:56:56.098209
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 309.90
 ---- batch: 020 ----
mean loss: 309.57
 ---- batch: 030 ----
mean loss: 308.34
 ---- batch: 040 ----
mean loss: 309.72
 ---- batch: 050 ----
mean loss: 304.31
 ---- batch: 060 ----
mean loss: 317.44
 ---- batch: 070 ----
mean loss: 310.36
 ---- batch: 080 ----
mean loss: 309.01
 ---- batch: 090 ----
mean loss: 313.40
 ---- batch: 100 ----
mean loss: 311.69
 ---- batch: 110 ----
mean loss: 313.27
train mean loss: 310.18
epoch train time: 0:00:02.131485
elapsed time: 0:09:15.146266
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-26 23:56:58.229847
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 317.39
 ---- batch: 020 ----
mean loss: 308.29
 ---- batch: 030 ----
mean loss: 308.30
 ---- batch: 040 ----
mean loss: 312.02
 ---- batch: 050 ----
mean loss: 311.14
 ---- batch: 060 ----
mean loss: 313.53
 ---- batch: 070 ----
mean loss: 312.04
 ---- batch: 080 ----
mean loss: 312.62
 ---- batch: 090 ----
mean loss: 311.49
 ---- batch: 100 ----
mean loss: 305.68
 ---- batch: 110 ----
mean loss: 317.08
train mean loss: 312.02
epoch train time: 0:00:02.117961
elapsed time: 0:09:17.264373
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-26 23:57:00.347951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 304.98
 ---- batch: 020 ----
mean loss: 318.73
 ---- batch: 030 ----
mean loss: 310.79
 ---- batch: 040 ----
mean loss: 320.76
 ---- batch: 050 ----
mean loss: 306.85
 ---- batch: 060 ----
mean loss: 321.72
 ---- batch: 070 ----
mean loss: 318.53
 ---- batch: 080 ----
mean loss: 312.86
 ---- batch: 090 ----
mean loss: 306.43
 ---- batch: 100 ----
mean loss: 313.26
 ---- batch: 110 ----
mean loss: 314.82
train mean loss: 313.10
epoch train time: 0:00:02.120498
elapsed time: 0:09:19.385013
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-26 23:57:02.468610
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 313.15
 ---- batch: 020 ----
mean loss: 313.20
 ---- batch: 030 ----
mean loss: 304.55
 ---- batch: 040 ----
mean loss: 316.97
 ---- batch: 050 ----
mean loss: 315.29
 ---- batch: 060 ----
mean loss: 315.23
 ---- batch: 070 ----
mean loss: 316.66
 ---- batch: 080 ----
mean loss: 308.80
 ---- batch: 090 ----
mean loss: 312.21
 ---- batch: 100 ----
mean loss: 323.36
 ---- batch: 110 ----
mean loss: 318.67
train mean loss: 314.09
epoch train time: 0:00:02.118989
elapsed time: 0:09:21.504166
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-26 23:57:04.587746
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 300.13
 ---- batch: 020 ----
mean loss: 308.23
 ---- batch: 030 ----
mean loss: 300.45
 ---- batch: 040 ----
mean loss: 312.92
 ---- batch: 050 ----
mean loss: 305.58
 ---- batch: 060 ----
mean loss: 318.67
 ---- batch: 070 ----
mean loss: 309.48
 ---- batch: 080 ----
mean loss: 319.83
 ---- batch: 090 ----
mean loss: 310.07
 ---- batch: 100 ----
mean loss: 316.32
 ---- batch: 110 ----
mean loss: 307.94
train mean loss: 310.09
epoch train time: 0:00:02.126711
elapsed time: 0:09:23.634385
checkpoint saved in file: log/CMAPSS/FD004/min-max/frequentist_conv5_dense1/frequentist_conv5_dense1_0/checkpoint.pth.tar
**** end time: 2019-09-26 23:57:06.717931 ****
