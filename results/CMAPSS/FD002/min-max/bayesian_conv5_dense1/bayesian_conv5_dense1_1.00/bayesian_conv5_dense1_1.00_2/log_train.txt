Namespace(batch_size=512, dataset='CMAPSS/FD002', gamma=0.1, learning_rate=0.001, log_dir='log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_2', max_epoch=250, max_rul=125, metric='rmse', model='bayesian_conv5_dense1', momentum=0.9, normalization='min-max', num_mc=1, optimizer='adam', quantity=1.0, resume=False, step_size=200, visualize_step=50)
pid: 3072
use_cuda: True
Dataset: CMAPSS/FD002
Building BayesianConv5Dense1...
Done.
**** start time: 2019-09-24 21:19:48.915817 ****
________________________________________________________________
        Layer (type)               Output Shape         Param #
================================================================
    BayesianConv2d-1           [-1, 10, 21, 24]             200
           Sigmoid-2           [-1, 10, 21, 24]               0
    BayesianConv2d-3           [-1, 10, 20, 24]           2,000
           Sigmoid-4           [-1, 10, 20, 24]               0
    BayesianConv2d-5           [-1, 10, 21, 24]           2,000
           Sigmoid-6           [-1, 10, 21, 24]               0
    BayesianConv2d-7           [-1, 10, 20, 24]           2,000
           Sigmoid-8           [-1, 10, 20, 24]               0
    BayesianConv2d-9            [-1, 1, 20, 24]              60
         Softplus-10            [-1, 1, 20, 24]               0
          Flatten-11                  [-1, 480]               0
   BayesianLinear-12                  [-1, 100]          96,000
         Softplus-13                  [-1, 100]               0
   BayesianLinear-14                    [-1, 1]             200
         Softplus-15                    [-1, 1]               0
================================================================
Total params: 102,460
Trainable params: 102,460
Non-trainable params: 0
________________________________________________________________
**** EPOCH 000 ****
---- EPOCH 000 TRAINING ----
2019-09-24 21:19:48.933706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 2107.85
 ---- batch: 020 ----
mean loss: 1461.03
 ---- batch: 030 ----
mean loss: 1325.95
 ---- batch: 040 ----
mean loss: 1245.86
 ---- batch: 050 ----
mean loss: 1167.70
 ---- batch: 060 ----
mean loss: 1124.91
 ---- batch: 070 ----
mean loss: 1118.49
 ---- batch: 080 ----
mean loss: 1087.65
 ---- batch: 090 ----
mean loss: 1079.47
train mean loss: 1285.98
epoch train time: 0:00:46.466877
elapsed time: 0:00:46.493594
**** EPOCH 001 ****
---- EPOCH 001 TRAINING ----
2019-09-24 21:20:35.409453
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1054.94
 ---- batch: 020 ----
mean loss: 1045.82
 ---- batch: 030 ----
mean loss: 1051.59
 ---- batch: 040 ----
mean loss: 1027.88
 ---- batch: 050 ----
mean loss: 1013.09
 ---- batch: 060 ----
mean loss: 1027.18
 ---- batch: 070 ----
mean loss: 1037.02
 ---- batch: 080 ----
mean loss: 1030.63
 ---- batch: 090 ----
mean loss: 1069.61
train mean loss: 1037.66
epoch train time: 0:00:16.462478
elapsed time: 0:01:02.956682
**** EPOCH 002 ****
---- EPOCH 002 TRAINING ----
2019-09-24 21:20:51.873001
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1035.61
 ---- batch: 020 ----
mean loss: 1003.98
 ---- batch: 030 ----
mean loss: 996.64
 ---- batch: 040 ----
mean loss: 1020.89
 ---- batch: 050 ----
mean loss: 1009.24
 ---- batch: 060 ----
mean loss: 975.99
 ---- batch: 070 ----
mean loss: 1007.69
 ---- batch: 080 ----
mean loss: 975.89
 ---- batch: 090 ----
mean loss: 997.68
train mean loss: 1001.20
epoch train time: 0:00:16.512763
elapsed time: 0:01:19.470518
**** EPOCH 003 ****
---- EPOCH 003 TRAINING ----
2019-09-24 21:21:08.386932
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 1001.34
 ---- batch: 020 ----
mean loss: 969.05
 ---- batch: 030 ----
mean loss: 984.93
 ---- batch: 040 ----
mean loss: 980.99
 ---- batch: 050 ----
mean loss: 982.14
 ---- batch: 060 ----
mean loss: 973.25
 ---- batch: 070 ----
mean loss: 995.75
 ---- batch: 080 ----
mean loss: 994.04
 ---- batch: 090 ----
mean loss: 975.79
train mean loss: 984.41
epoch train time: 0:00:16.509292
elapsed time: 0:01:35.981050
**** EPOCH 004 ****
---- EPOCH 004 TRAINING ----
2019-09-24 21:21:24.897458
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 983.52
 ---- batch: 020 ----
mean loss: 971.22
 ---- batch: 030 ----
mean loss: 992.83
 ---- batch: 040 ----
mean loss: 978.45
 ---- batch: 050 ----
mean loss: 953.30
 ---- batch: 060 ----
mean loss: 970.93
 ---- batch: 070 ----
mean loss: 985.58
 ---- batch: 080 ----
mean loss: 981.20
 ---- batch: 090 ----
mean loss: 960.94
train mean loss: 973.09
epoch train time: 0:00:16.384893
elapsed time: 0:01:52.367132
**** EPOCH 005 ****
---- EPOCH 005 TRAINING ----
2019-09-24 21:21:41.283526
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 969.34
 ---- batch: 020 ----
mean loss: 958.00
 ---- batch: 030 ----
mean loss: 982.78
 ---- batch: 040 ----
mean loss: 969.80
 ---- batch: 050 ----
mean loss: 986.04
 ---- batch: 060 ----
mean loss: 956.30
 ---- batch: 070 ----
mean loss: 976.23
 ---- batch: 080 ----
mean loss: 971.93
 ---- batch: 090 ----
mean loss: 959.97
train mean loss: 969.28
epoch train time: 0:00:16.478888
elapsed time: 0:02:08.847252
**** EPOCH 006 ****
---- EPOCH 006 TRAINING ----
2019-09-24 21:21:57.763706
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 944.21
 ---- batch: 020 ----
mean loss: 965.66
 ---- batch: 030 ----
mean loss: 974.30
 ---- batch: 040 ----
mean loss: 971.79
 ---- batch: 050 ----
mean loss: 972.54
 ---- batch: 060 ----
mean loss: 954.92
 ---- batch: 070 ----
mean loss: 939.82
 ---- batch: 080 ----
mean loss: 942.58
 ---- batch: 090 ----
mean loss: 942.06
train mean loss: 956.71
epoch train time: 0:00:16.492462
elapsed time: 0:02:25.340934
**** EPOCH 007 ****
---- EPOCH 007 TRAINING ----
2019-09-24 21:22:14.257295
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 949.16
 ---- batch: 020 ----
mean loss: 955.40
 ---- batch: 030 ----
mean loss: 997.65
 ---- batch: 040 ----
mean loss: 930.41
 ---- batch: 050 ----
mean loss: 949.88
 ---- batch: 060 ----
mean loss: 937.91
 ---- batch: 070 ----
mean loss: 959.55
 ---- batch: 080 ----
mean loss: 947.14
 ---- batch: 090 ----
mean loss: 922.20
train mean loss: 947.51
epoch train time: 0:00:16.496831
elapsed time: 0:02:41.838978
**** EPOCH 008 ****
---- EPOCH 008 TRAINING ----
2019-09-24 21:22:30.755403
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 928.87
 ---- batch: 020 ----
mean loss: 935.58
 ---- batch: 030 ----
mean loss: 941.02
 ---- batch: 040 ----
mean loss: 958.47
 ---- batch: 050 ----
mean loss: 924.92
 ---- batch: 060 ----
mean loss: 928.99
 ---- batch: 070 ----
mean loss: 924.31
 ---- batch: 080 ----
mean loss: 913.55
 ---- batch: 090 ----
mean loss: 950.25
train mean loss: 934.41
epoch train time: 0:00:16.515284
elapsed time: 0:02:58.355505
**** EPOCH 009 ****
---- EPOCH 009 TRAINING ----
2019-09-24 21:22:47.271871
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 941.12
 ---- batch: 020 ----
mean loss: 952.89
 ---- batch: 030 ----
mean loss: 909.82
 ---- batch: 040 ----
mean loss: 918.71
 ---- batch: 050 ----
mean loss: 943.48
 ---- batch: 060 ----
mean loss: 937.31
 ---- batch: 070 ----
mean loss: 918.50
 ---- batch: 080 ----
mean loss: 907.43
 ---- batch: 090 ----
mean loss: 918.44
train mean loss: 926.35
epoch train time: 0:00:16.541458
elapsed time: 0:03:14.898123
**** EPOCH 010 ****
---- EPOCH 010 TRAINING ----
2019-09-24 21:23:03.814488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 909.24
 ---- batch: 020 ----
mean loss: 920.77
 ---- batch: 030 ----
mean loss: 920.44
 ---- batch: 040 ----
mean loss: 917.58
 ---- batch: 050 ----
mean loss: 897.14
 ---- batch: 060 ----
mean loss: 912.86
 ---- batch: 070 ----
mean loss: 907.22
 ---- batch: 080 ----
mean loss: 882.89
 ---- batch: 090 ----
mean loss: 886.05
train mean loss: 903.01
epoch train time: 0:00:16.522474
elapsed time: 0:03:31.421724
**** EPOCH 011 ****
---- EPOCH 011 TRAINING ----
2019-09-24 21:23:20.338100
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 830.60
 ---- batch: 020 ----
mean loss: 867.69
 ---- batch: 030 ----
mean loss: 861.76
 ---- batch: 040 ----
mean loss: 863.83
 ---- batch: 050 ----
mean loss: 844.63
 ---- batch: 060 ----
mean loss: 832.20
 ---- batch: 070 ----
mean loss: 810.38
 ---- batch: 080 ----
mean loss: 779.66
 ---- batch: 090 ----
mean loss: 776.16
train mean loss: 824.02
epoch train time: 0:00:16.494924
elapsed time: 0:03:47.918089
**** EPOCH 012 ****
---- EPOCH 012 TRAINING ----
2019-09-24 21:23:36.834461
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 733.64
 ---- batch: 020 ----
mean loss: 752.42
 ---- batch: 030 ----
mean loss: 750.95
 ---- batch: 040 ----
mean loss: 726.65
 ---- batch: 050 ----
mean loss: 725.42
 ---- batch: 060 ----
mean loss: 719.69
 ---- batch: 070 ----
mean loss: 688.02
 ---- batch: 080 ----
mean loss: 698.37
 ---- batch: 090 ----
mean loss: 724.69
train mean loss: 720.59
epoch train time: 0:00:16.534583
elapsed time: 0:04:04.453778
**** EPOCH 013 ****
---- EPOCH 013 TRAINING ----
2019-09-24 21:23:53.370212
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 684.61
 ---- batch: 020 ----
mean loss: 697.17
 ---- batch: 030 ----
mean loss: 672.50
 ---- batch: 040 ----
mean loss: 657.49
 ---- batch: 050 ----
mean loss: 656.77
 ---- batch: 060 ----
mean loss: 643.15
 ---- batch: 070 ----
mean loss: 648.00
 ---- batch: 080 ----
mean loss: 641.82
 ---- batch: 090 ----
mean loss: 640.59
train mean loss: 659.02
epoch train time: 0:00:16.411971
elapsed time: 0:04:20.867072
**** EPOCH 014 ****
---- EPOCH 014 TRAINING ----
2019-09-24 21:24:09.783443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 624.85
 ---- batch: 020 ----
mean loss: 619.27
 ---- batch: 030 ----
mean loss: 632.43
 ---- batch: 040 ----
mean loss: 613.13
 ---- batch: 050 ----
mean loss: 606.64
 ---- batch: 060 ----
mean loss: 599.56
 ---- batch: 070 ----
mean loss: 606.36
 ---- batch: 080 ----
mean loss: 599.73
 ---- batch: 090 ----
mean loss: 589.20
train mean loss: 608.00
epoch train time: 0:00:16.445572
elapsed time: 0:04:37.313774
**** EPOCH 015 ****
---- EPOCH 015 TRAINING ----
2019-09-24 21:24:26.230159
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 578.62
 ---- batch: 020 ----
mean loss: 590.04
 ---- batch: 030 ----
mean loss: 556.26
 ---- batch: 040 ----
mean loss: 561.21
 ---- batch: 050 ----
mean loss: 569.88
 ---- batch: 060 ----
mean loss: 547.06
 ---- batch: 070 ----
mean loss: 555.55
 ---- batch: 080 ----
mean loss: 533.46
 ---- batch: 090 ----
mean loss: 544.75
train mean loss: 559.87
epoch train time: 0:00:16.444124
elapsed time: 0:04:53.759164
**** EPOCH 016 ****
---- EPOCH 016 TRAINING ----
2019-09-24 21:24:42.675582
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 539.29
 ---- batch: 020 ----
mean loss: 531.31
 ---- batch: 030 ----
mean loss: 527.00
 ---- batch: 040 ----
mean loss: 521.40
 ---- batch: 050 ----
mean loss: 519.42
 ---- batch: 060 ----
mean loss: 514.32
 ---- batch: 070 ----
mean loss: 531.59
 ---- batch: 080 ----
mean loss: 519.78
 ---- batch: 090 ----
mean loss: 509.37
train mean loss: 522.18
epoch train time: 0:00:16.444004
elapsed time: 0:05:10.204362
**** EPOCH 017 ****
---- EPOCH 017 TRAINING ----
2019-09-24 21:24:59.120720
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 488.21
 ---- batch: 020 ----
mean loss: 495.63
 ---- batch: 030 ----
mean loss: 491.90
 ---- batch: 040 ----
mean loss: 493.91
 ---- batch: 050 ----
mean loss: 476.78
 ---- batch: 060 ----
mean loss: 488.48
 ---- batch: 070 ----
mean loss: 475.43
 ---- batch: 080 ----
mean loss: 488.61
 ---- batch: 090 ----
mean loss: 482.66
train mean loss: 485.35
epoch train time: 0:00:16.468828
elapsed time: 0:05:26.674425
**** EPOCH 018 ****
---- EPOCH 018 TRAINING ----
2019-09-24 21:25:15.590902
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 490.19
 ---- batch: 020 ----
mean loss: 470.14
 ---- batch: 030 ----
mean loss: 464.53
 ---- batch: 040 ----
mean loss: 456.76
 ---- batch: 050 ----
mean loss: 462.81
 ---- batch: 060 ----
mean loss: 446.53
 ---- batch: 070 ----
mean loss: 456.72
 ---- batch: 080 ----
mean loss: 450.60
 ---- batch: 090 ----
mean loss: 452.93
train mean loss: 460.17
epoch train time: 0:00:16.620125
elapsed time: 0:05:43.295816
**** EPOCH 019 ****
---- EPOCH 019 TRAINING ----
2019-09-24 21:25:32.212178
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 442.95
 ---- batch: 020 ----
mean loss: 443.20
 ---- batch: 030 ----
mean loss: 460.74
 ---- batch: 040 ----
mean loss: 448.34
 ---- batch: 050 ----
mean loss: 440.56
 ---- batch: 060 ----
mean loss: 431.28
 ---- batch: 070 ----
mean loss: 437.94
 ---- batch: 080 ----
mean loss: 439.76
 ---- batch: 090 ----
mean loss: 433.03
train mean loss: 442.56
epoch train time: 0:00:16.500464
elapsed time: 0:05:59.797406
**** EPOCH 020 ****
---- EPOCH 020 TRAINING ----
2019-09-24 21:25:48.713806
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 441.96
 ---- batch: 020 ----
mean loss: 438.49
 ---- batch: 030 ----
mean loss: 430.00
 ---- batch: 040 ----
mean loss: 428.24
 ---- batch: 050 ----
mean loss: 412.36
 ---- batch: 060 ----
mean loss: 429.83
 ---- batch: 070 ----
mean loss: 419.19
 ---- batch: 080 ----
mean loss: 426.84
 ---- batch: 090 ----
mean loss: 420.28
train mean loss: 426.60
epoch train time: 0:00:16.511964
elapsed time: 0:06:16.310525
**** EPOCH 021 ****
---- EPOCH 021 TRAINING ----
2019-09-24 21:26:05.226934
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 416.49
 ---- batch: 020 ----
mean loss: 418.09
 ---- batch: 030 ----
mean loss: 420.27
 ---- batch: 040 ----
mean loss: 422.98
 ---- batch: 050 ----
mean loss: 407.71
 ---- batch: 060 ----
mean loss: 414.21
 ---- batch: 070 ----
mean loss: 399.32
 ---- batch: 080 ----
mean loss: 407.82
 ---- batch: 090 ----
mean loss: 425.36
train mean loss: 413.95
epoch train time: 0:00:16.451750
elapsed time: 0:06:32.763449
**** EPOCH 022 ****
---- EPOCH 022 TRAINING ----
2019-09-24 21:26:21.679736
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 414.54
 ---- batch: 020 ----
mean loss: 392.12
 ---- batch: 030 ----
mean loss: 407.84
 ---- batch: 040 ----
mean loss: 397.57
 ---- batch: 050 ----
mean loss: 390.24
 ---- batch: 060 ----
mean loss: 406.47
 ---- batch: 070 ----
mean loss: 402.09
 ---- batch: 080 ----
mean loss: 400.29
 ---- batch: 090 ----
mean loss: 392.90
train mean loss: 400.03
epoch train time: 0:00:16.786351
elapsed time: 0:06:49.550990
**** EPOCH 023 ****
---- EPOCH 023 TRAINING ----
2019-09-24 21:26:38.467382
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 403.82
 ---- batch: 020 ----
mean loss: 396.27
 ---- batch: 030 ----
mean loss: 393.39
 ---- batch: 040 ----
mean loss: 386.77
 ---- batch: 050 ----
mean loss: 395.64
 ---- batch: 060 ----
mean loss: 401.87
 ---- batch: 070 ----
mean loss: 377.87
 ---- batch: 080 ----
mean loss: 385.07
 ---- batch: 090 ----
mean loss: 381.10
train mean loss: 390.69
epoch train time: 0:00:16.873837
elapsed time: 0:07:06.426048
**** EPOCH 024 ****
---- EPOCH 024 TRAINING ----
2019-09-24 21:26:55.342390
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 382.74
 ---- batch: 020 ----
mean loss: 388.63
 ---- batch: 030 ----
mean loss: 389.80
 ---- batch: 040 ----
mean loss: 386.13
 ---- batch: 050 ----
mean loss: 373.22
 ---- batch: 060 ----
mean loss: 388.08
 ---- batch: 070 ----
mean loss: 364.42
 ---- batch: 080 ----
mean loss: 367.01
 ---- batch: 090 ----
mean loss: 383.57
train mean loss: 380.27
epoch train time: 0:00:16.919528
elapsed time: 0:07:23.346700
**** EPOCH 025 ****
---- EPOCH 025 TRAINING ----
2019-09-24 21:27:12.263114
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 370.97
 ---- batch: 020 ----
mean loss: 366.30
 ---- batch: 030 ----
mean loss: 370.00
 ---- batch: 040 ----
mean loss: 370.39
 ---- batch: 050 ----
mean loss: 375.98
 ---- batch: 060 ----
mean loss: 372.87
 ---- batch: 070 ----
mean loss: 364.28
 ---- batch: 080 ----
mean loss: 370.34
 ---- batch: 090 ----
mean loss: 365.46
train mean loss: 369.07
epoch train time: 0:00:16.884844
elapsed time: 0:07:40.232741
**** EPOCH 026 ****
---- EPOCH 026 TRAINING ----
2019-09-24 21:27:29.149096
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 371.95
 ---- batch: 020 ----
mean loss: 369.19
 ---- batch: 030 ----
mean loss: 361.77
 ---- batch: 040 ----
mean loss: 379.43
 ---- batch: 050 ----
mean loss: 348.24
 ---- batch: 060 ----
mean loss: 364.48
 ---- batch: 070 ----
mean loss: 353.39
 ---- batch: 080 ----
mean loss: 355.16
 ---- batch: 090 ----
mean loss: 355.60
train mean loss: 361.90
epoch train time: 0:00:16.926381
elapsed time: 0:07:57.160370
**** EPOCH 027 ****
---- EPOCH 027 TRAINING ----
2019-09-24 21:27:46.076740
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 366.52
 ---- batch: 020 ----
mean loss: 353.50
 ---- batch: 030 ----
mean loss: 354.28
 ---- batch: 040 ----
mean loss: 346.06
 ---- batch: 050 ----
mean loss: 355.23
 ---- batch: 060 ----
mean loss: 353.25
 ---- batch: 070 ----
mean loss: 341.36
 ---- batch: 080 ----
mean loss: 348.41
 ---- batch: 090 ----
mean loss: 334.85
train mean loss: 349.36
epoch train time: 0:00:16.852597
elapsed time: 0:08:14.014148
**** EPOCH 028 ****
---- EPOCH 028 TRAINING ----
2019-09-24 21:28:02.930575
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 345.69
 ---- batch: 020 ----
mean loss: 339.11
 ---- batch: 030 ----
mean loss: 339.20
 ---- batch: 040 ----
mean loss: 354.96
 ---- batch: 050 ----
mean loss: 334.18
 ---- batch: 060 ----
mean loss: 337.04
 ---- batch: 070 ----
mean loss: 352.41
 ---- batch: 080 ----
mean loss: 340.40
 ---- batch: 090 ----
mean loss: 335.56
train mean loss: 340.83
epoch train time: 0:00:16.939392
elapsed time: 0:08:30.954848
**** EPOCH 029 ****
---- EPOCH 029 TRAINING ----
2019-09-24 21:28:19.871287
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 350.14
 ---- batch: 020 ----
mean loss: 339.50
 ---- batch: 030 ----
mean loss: 336.80
 ---- batch: 040 ----
mean loss: 324.72
 ---- batch: 050 ----
mean loss: 344.26
 ---- batch: 060 ----
mean loss: 348.34
 ---- batch: 070 ----
mean loss: 336.44
 ---- batch: 080 ----
mean loss: 334.36
 ---- batch: 090 ----
mean loss: 329.43
train mean loss: 337.76
epoch train time: 0:00:16.832367
elapsed time: 0:08:47.788621
**** EPOCH 030 ****
---- EPOCH 030 TRAINING ----
2019-09-24 21:28:36.705018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 324.53
 ---- batch: 020 ----
mean loss: 336.12
 ---- batch: 030 ----
mean loss: 330.17
 ---- batch: 040 ----
mean loss: 334.63
 ---- batch: 050 ----
mean loss: 335.62
 ---- batch: 060 ----
mean loss: 333.15
 ---- batch: 070 ----
mean loss: 326.67
 ---- batch: 080 ----
mean loss: 328.61
 ---- batch: 090 ----
mean loss: 321.23
train mean loss: 330.49
epoch train time: 0:00:16.899278
elapsed time: 0:09:04.689103
**** EPOCH 031 ****
---- EPOCH 031 TRAINING ----
2019-09-24 21:28:53.605471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 326.66
 ---- batch: 020 ----
mean loss: 334.31
 ---- batch: 030 ----
mean loss: 322.17
 ---- batch: 040 ----
mean loss: 309.63
 ---- batch: 050 ----
mean loss: 310.06
 ---- batch: 060 ----
mean loss: 329.28
 ---- batch: 070 ----
mean loss: 323.76
 ---- batch: 080 ----
mean loss: 326.57
 ---- batch: 090 ----
mean loss: 320.40
train mean loss: 322.85
epoch train time: 0:00:16.840804
elapsed time: 0:09:21.531056
**** EPOCH 032 ****
---- EPOCH 032 TRAINING ----
2019-09-24 21:29:10.447409
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 317.80
 ---- batch: 020 ----
mean loss: 333.67
 ---- batch: 030 ----
mean loss: 326.04
 ---- batch: 040 ----
mean loss: 314.80
 ---- batch: 050 ----
mean loss: 325.74
 ---- batch: 060 ----
mean loss: 318.79
 ---- batch: 070 ----
mean loss: 320.84
 ---- batch: 080 ----
mean loss: 315.19
 ---- batch: 090 ----
mean loss: 308.38
train mean loss: 319.91
epoch train time: 0:00:16.879493
elapsed time: 0:09:38.411755
**** EPOCH 033 ****
---- EPOCH 033 TRAINING ----
2019-09-24 21:29:27.328148
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 316.36
 ---- batch: 020 ----
mean loss: 321.81
 ---- batch: 030 ----
mean loss: 322.95
 ---- batch: 040 ----
mean loss: 307.42
 ---- batch: 050 ----
mean loss: 307.47
 ---- batch: 060 ----
mean loss: 312.68
 ---- batch: 070 ----
mean loss: 308.58
 ---- batch: 080 ----
mean loss: 311.96
 ---- batch: 090 ----
mean loss: 309.25
train mean loss: 312.09
epoch train time: 0:00:17.007502
elapsed time: 0:09:55.420449
**** EPOCH 034 ****
---- EPOCH 034 TRAINING ----
2019-09-24 21:29:44.336857
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 314.10
 ---- batch: 020 ----
mean loss: 312.40
 ---- batch: 030 ----
mean loss: 308.58
 ---- batch: 040 ----
mean loss: 299.22
 ---- batch: 050 ----
mean loss: 307.83
 ---- batch: 060 ----
mean loss: 308.61
 ---- batch: 070 ----
mean loss: 316.44
 ---- batch: 080 ----
mean loss: 296.91
 ---- batch: 090 ----
mean loss: 306.52
train mean loss: 307.10
epoch train time: 0:00:16.948431
elapsed time: 0:10:12.370157
**** EPOCH 035 ****
---- EPOCH 035 TRAINING ----
2019-09-24 21:30:01.286525
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 327.03
 ---- batch: 020 ----
mean loss: 306.22
 ---- batch: 030 ----
mean loss: 300.21
 ---- batch: 040 ----
mean loss: 307.91
 ---- batch: 050 ----
mean loss: 299.16
 ---- batch: 060 ----
mean loss: 296.49
 ---- batch: 070 ----
mean loss: 307.13
 ---- batch: 080 ----
mean loss: 297.76
 ---- batch: 090 ----
mean loss: 290.21
train mean loss: 303.85
epoch train time: 0:00:16.961145
elapsed time: 0:10:29.332558
**** EPOCH 036 ****
---- EPOCH 036 TRAINING ----
2019-09-24 21:30:18.248942
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 304.80
 ---- batch: 020 ----
mean loss: 301.33
 ---- batch: 030 ----
mean loss: 291.73
 ---- batch: 040 ----
mean loss: 304.53
 ---- batch: 050 ----
mean loss: 300.65
 ---- batch: 060 ----
mean loss: 289.91
 ---- batch: 070 ----
mean loss: 295.23
 ---- batch: 080 ----
mean loss: 292.47
 ---- batch: 090 ----
mean loss: 291.47
train mean loss: 297.82
epoch train time: 0:00:16.946177
elapsed time: 0:10:46.279873
**** EPOCH 037 ****
---- EPOCH 037 TRAINING ----
2019-09-24 21:30:35.196293
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 300.52
 ---- batch: 020 ----
mean loss: 295.04
 ---- batch: 030 ----
mean loss: 291.12
 ---- batch: 040 ----
mean loss: 310.78
 ---- batch: 050 ----
mean loss: 297.52
 ---- batch: 060 ----
mean loss: 293.22
 ---- batch: 070 ----
mean loss: 301.21
 ---- batch: 080 ----
mean loss: 290.82
 ---- batch: 090 ----
mean loss: 293.53
train mean loss: 296.48
epoch train time: 0:00:16.972443
elapsed time: 0:11:03.253514
**** EPOCH 038 ****
---- EPOCH 038 TRAINING ----
2019-09-24 21:30:52.169968
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 283.17
 ---- batch: 020 ----
mean loss: 297.84
 ---- batch: 030 ----
mean loss: 296.95
 ---- batch: 040 ----
mean loss: 290.96
 ---- batch: 050 ----
mean loss: 296.11
 ---- batch: 060 ----
mean loss: 294.08
 ---- batch: 070 ----
mean loss: 295.30
 ---- batch: 080 ----
mean loss: 295.54
 ---- batch: 090 ----
mean loss: 290.64
train mean loss: 292.40
epoch train time: 0:00:16.935481
elapsed time: 0:11:20.190225
**** EPOCH 039 ****
---- EPOCH 039 TRAINING ----
2019-09-24 21:31:09.106673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 286.81
 ---- batch: 020 ----
mean loss: 280.22
 ---- batch: 030 ----
mean loss: 286.73
 ---- batch: 040 ----
mean loss: 288.75
 ---- batch: 050 ----
mean loss: 288.55
 ---- batch: 060 ----
mean loss: 286.16
 ---- batch: 070 ----
mean loss: 283.61
 ---- batch: 080 ----
mean loss: 283.02
 ---- batch: 090 ----
mean loss: 283.43
train mean loss: 284.83
epoch train time: 0:00:16.946036
elapsed time: 0:11:37.137537
**** EPOCH 040 ****
---- EPOCH 040 TRAINING ----
2019-09-24 21:31:26.053947
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.53
 ---- batch: 020 ----
mean loss: 287.34
 ---- batch: 030 ----
mean loss: 285.18
 ---- batch: 040 ----
mean loss: 291.40
 ---- batch: 050 ----
mean loss: 280.54
 ---- batch: 060 ----
mean loss: 285.99
 ---- batch: 070 ----
mean loss: 287.64
 ---- batch: 080 ----
mean loss: 292.06
 ---- batch: 090 ----
mean loss: 278.25
train mean loss: 285.10
epoch train time: 0:00:16.929170
elapsed time: 0:11:54.067923
**** EPOCH 041 ****
---- EPOCH 041 TRAINING ----
2019-09-24 21:31:42.984262
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 277.46
 ---- batch: 020 ----
mean loss: 271.43
 ---- batch: 030 ----
mean loss: 284.60
 ---- batch: 040 ----
mean loss: 289.16
 ---- batch: 050 ----
mean loss: 280.68
 ---- batch: 060 ----
mean loss: 281.54
 ---- batch: 070 ----
mean loss: 275.46
 ---- batch: 080 ----
mean loss: 285.67
 ---- batch: 090 ----
mean loss: 279.88
train mean loss: 280.23
epoch train time: 0:00:16.984521
elapsed time: 0:12:11.053591
**** EPOCH 042 ****
---- EPOCH 042 TRAINING ----
2019-09-24 21:31:59.970010
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.11
 ---- batch: 020 ----
mean loss: 275.01
 ---- batch: 030 ----
mean loss: 279.99
 ---- batch: 040 ----
mean loss: 280.77
 ---- batch: 050 ----
mean loss: 282.61
 ---- batch: 060 ----
mean loss: 279.66
 ---- batch: 070 ----
mean loss: 278.33
 ---- batch: 080 ----
mean loss: 286.10
 ---- batch: 090 ----
mean loss: 281.27
train mean loss: 280.21
epoch train time: 0:00:16.937527
elapsed time: 0:12:27.992341
**** EPOCH 043 ****
---- EPOCH 043 TRAINING ----
2019-09-24 21:32:16.908673
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 274.64
 ---- batch: 020 ----
mean loss: 286.02
 ---- batch: 030 ----
mean loss: 276.34
 ---- batch: 040 ----
mean loss: 280.35
 ---- batch: 050 ----
mean loss: 270.01
 ---- batch: 060 ----
mean loss: 271.45
 ---- batch: 070 ----
mean loss: 276.57
 ---- batch: 080 ----
mean loss: 279.35
 ---- batch: 090 ----
mean loss: 277.21
train mean loss: 276.57
epoch train time: 0:00:16.914160
elapsed time: 0:12:44.907634
**** EPOCH 044 ****
---- EPOCH 044 TRAINING ----
2019-09-24 21:32:33.823879
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 276.21
 ---- batch: 020 ----
mean loss: 260.29
 ---- batch: 030 ----
mean loss: 283.13
 ---- batch: 040 ----
mean loss: 268.17
 ---- batch: 050 ----
mean loss: 269.47
 ---- batch: 060 ----
mean loss: 276.41
 ---- batch: 070 ----
mean loss: 276.03
 ---- batch: 080 ----
mean loss: 274.34
 ---- batch: 090 ----
mean loss: 270.09
train mean loss: 272.50
epoch train time: 0:00:16.921962
elapsed time: 0:13:01.830676
**** EPOCH 045 ****
---- EPOCH 045 TRAINING ----
2019-09-24 21:32:50.747110
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 272.28
 ---- batch: 020 ----
mean loss: 277.67
 ---- batch: 030 ----
mean loss: 263.52
 ---- batch: 040 ----
mean loss: 267.69
 ---- batch: 050 ----
mean loss: 266.70
 ---- batch: 060 ----
mean loss: 260.59
 ---- batch: 070 ----
mean loss: 273.68
 ---- batch: 080 ----
mean loss: 266.69
 ---- batch: 090 ----
mean loss: 271.52
train mean loss: 268.56
epoch train time: 0:00:16.881997
elapsed time: 0:13:18.713977
**** EPOCH 046 ****
---- EPOCH 046 TRAINING ----
2019-09-24 21:33:07.630322
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.70
 ---- batch: 020 ----
mean loss: 273.86
 ---- batch: 030 ----
mean loss: 274.88
 ---- batch: 040 ----
mean loss: 266.52
 ---- batch: 050 ----
mean loss: 262.36
 ---- batch: 060 ----
mean loss: 269.98
 ---- batch: 070 ----
mean loss: 265.65
 ---- batch: 080 ----
mean loss: 263.20
 ---- batch: 090 ----
mean loss: 260.73
train mean loss: 267.99
epoch train time: 0:00:16.926492
elapsed time: 0:13:35.641598
**** EPOCH 047 ****
---- EPOCH 047 TRAINING ----
2019-09-24 21:33:24.557982
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 261.55
 ---- batch: 020 ----
mean loss: 267.50
 ---- batch: 030 ----
mean loss: 266.79
 ---- batch: 040 ----
mean loss: 256.46
 ---- batch: 050 ----
mean loss: 267.80
 ---- batch: 060 ----
mean loss: 263.10
 ---- batch: 070 ----
mean loss: 265.48
 ---- batch: 080 ----
mean loss: 257.87
 ---- batch: 090 ----
mean loss: 262.16
train mean loss: 263.87
epoch train time: 0:00:17.005042
elapsed time: 0:13:52.647965
**** EPOCH 048 ****
---- EPOCH 048 TRAINING ----
2019-09-24 21:33:41.564369
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 259.11
 ---- batch: 020 ----
mean loss: 256.49
 ---- batch: 030 ----
mean loss: 266.79
 ---- batch: 040 ----
mean loss: 257.87
 ---- batch: 050 ----
mean loss: 260.62
 ---- batch: 060 ----
mean loss: 261.09
 ---- batch: 070 ----
mean loss: 268.80
 ---- batch: 080 ----
mean loss: 269.79
 ---- batch: 090 ----
mean loss: 263.95
train mean loss: 263.33
epoch train time: 0:00:16.977049
elapsed time: 0:14:09.626266
**** EPOCH 049 ****
---- EPOCH 049 TRAINING ----
2019-09-24 21:33:58.542680
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 266.12
 ---- batch: 020 ----
mean loss: 250.60
 ---- batch: 030 ----
mean loss: 269.07
 ---- batch: 040 ----
mean loss: 259.23
 ---- batch: 050 ----
mean loss: 266.38
 ---- batch: 060 ----
mean loss: 269.96
 ---- batch: 070 ----
mean loss: 255.07
 ---- batch: 080 ----
mean loss: 262.13
 ---- batch: 090 ----
mean loss: 258.12
train mean loss: 261.48
epoch train time: 0:00:16.949418
elapsed time: 0:14:26.576864
**** EPOCH 050 ****
---- EPOCH 050 TRAINING ----
2019-09-24 21:34:15.493224
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 256.48
 ---- batch: 020 ----
mean loss: 254.06
 ---- batch: 030 ----
mean loss: 262.49
 ---- batch: 040 ----
mean loss: 255.33
 ---- batch: 050 ----
mean loss: 266.10
 ---- batch: 060 ----
mean loss: 269.48
 ---- batch: 070 ----
mean loss: 262.23
 ---- batch: 080 ----
mean loss: 254.34
 ---- batch: 090 ----
mean loss: 255.92
train mean loss: 259.49
epoch train time: 0:00:16.969305
elapsed time: 0:14:43.547336
**** EPOCH 051 ****
---- EPOCH 051 TRAINING ----
2019-09-24 21:34:32.463741
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 258.35
 ---- batch: 020 ----
mean loss: 250.40
 ---- batch: 030 ----
mean loss: 254.83
 ---- batch: 040 ----
mean loss: 262.75
 ---- batch: 050 ----
mean loss: 251.45
 ---- batch: 060 ----
mean loss: 253.92
 ---- batch: 070 ----
mean loss: 246.60
 ---- batch: 080 ----
mean loss: 253.19
 ---- batch: 090 ----
mean loss: 258.33
train mean loss: 254.77
epoch train time: 0:00:16.961681
elapsed time: 0:15:00.510283
**** EPOCH 052 ****
---- EPOCH 052 TRAINING ----
2019-09-24 21:34:49.426715
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 253.27
 ---- batch: 020 ----
mean loss: 255.38
 ---- batch: 030 ----
mean loss: 257.10
 ---- batch: 040 ----
mean loss: 252.51
 ---- batch: 050 ----
mean loss: 254.28
 ---- batch: 060 ----
mean loss: 260.10
 ---- batch: 070 ----
mean loss: 248.32
 ---- batch: 080 ----
mean loss: 247.63
 ---- batch: 090 ----
mean loss: 256.38
train mean loss: 254.32
epoch train time: 0:00:16.983185
elapsed time: 0:15:17.494729
**** EPOCH 053 ****
---- EPOCH 053 TRAINING ----
2019-09-24 21:35:06.411072
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 242.42
 ---- batch: 020 ----
mean loss: 249.25
 ---- batch: 030 ----
mean loss: 260.37
 ---- batch: 040 ----
mean loss: 249.90
 ---- batch: 050 ----
mean loss: 245.71
 ---- batch: 060 ----
mean loss: 249.82
 ---- batch: 070 ----
mean loss: 249.02
 ---- batch: 080 ----
mean loss: 263.86
 ---- batch: 090 ----
mean loss: 248.36
train mean loss: 251.55
epoch train time: 0:00:16.989941
elapsed time: 0:15:34.485895
**** EPOCH 054 ****
---- EPOCH 054 TRAINING ----
2019-09-24 21:35:23.402341
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.99
 ---- batch: 020 ----
mean loss: 250.50
 ---- batch: 030 ----
mean loss: 248.92
 ---- batch: 040 ----
mean loss: 249.98
 ---- batch: 050 ----
mean loss: 250.68
 ---- batch: 060 ----
mean loss: 252.36
 ---- batch: 070 ----
mean loss: 254.76
 ---- batch: 080 ----
mean loss: 236.74
 ---- batch: 090 ----
mean loss: 252.67
train mean loss: 250.00
epoch train time: 0:00:16.986300
elapsed time: 0:15:51.473486
**** EPOCH 055 ****
---- EPOCH 055 TRAINING ----
2019-09-24 21:35:40.389936
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 250.70
 ---- batch: 020 ----
mean loss: 249.15
 ---- batch: 030 ----
mean loss: 246.32
 ---- batch: 040 ----
mean loss: 252.41
 ---- batch: 050 ----
mean loss: 243.12
 ---- batch: 060 ----
mean loss: 251.92
 ---- batch: 070 ----
mean loss: 252.37
 ---- batch: 080 ----
mean loss: 242.69
 ---- batch: 090 ----
mean loss: 240.44
train mean loss: 247.86
epoch train time: 0:00:16.982161
elapsed time: 0:16:08.456897
**** EPOCH 056 ****
---- EPOCH 056 TRAINING ----
2019-09-24 21:35:57.373361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 246.63
 ---- batch: 020 ----
mean loss: 244.22
 ---- batch: 030 ----
mean loss: 249.87
 ---- batch: 040 ----
mean loss: 245.10
 ---- batch: 050 ----
mean loss: 241.50
 ---- batch: 060 ----
mean loss: 242.92
 ---- batch: 070 ----
mean loss: 251.42
 ---- batch: 080 ----
mean loss: 244.12
 ---- batch: 090 ----
mean loss: 249.20
train mean loss: 245.75
epoch train time: 0:00:16.962063
elapsed time: 0:16:25.420299
**** EPOCH 057 ****
---- EPOCH 057 TRAINING ----
2019-09-24 21:36:14.336721
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.10
 ---- batch: 020 ----
mean loss: 250.82
 ---- batch: 030 ----
mean loss: 246.42
 ---- batch: 040 ----
mean loss: 241.84
 ---- batch: 050 ----
mean loss: 246.48
 ---- batch: 060 ----
mean loss: 252.53
 ---- batch: 070 ----
mean loss: 239.16
 ---- batch: 080 ----
mean loss: 242.79
 ---- batch: 090 ----
mean loss: 247.82
train mean loss: 245.03
epoch train time: 0:00:16.984694
elapsed time: 0:16:42.406215
**** EPOCH 058 ****
---- EPOCH 058 TRAINING ----
2019-09-24 21:36:31.322601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 248.19
 ---- batch: 020 ----
mean loss: 248.47
 ---- batch: 030 ----
mean loss: 245.21
 ---- batch: 040 ----
mean loss: 244.25
 ---- batch: 050 ----
mean loss: 240.41
 ---- batch: 060 ----
mean loss: 239.51
 ---- batch: 070 ----
mean loss: 242.01
 ---- batch: 080 ----
mean loss: 244.48
 ---- batch: 090 ----
mean loss: 243.64
train mean loss: 243.98
epoch train time: 0:00:16.953112
elapsed time: 0:16:59.360553
**** EPOCH 059 ****
---- EPOCH 059 TRAINING ----
2019-09-24 21:36:48.276909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.44
 ---- batch: 020 ----
mean loss: 244.27
 ---- batch: 030 ----
mean loss: 238.67
 ---- batch: 040 ----
mean loss: 244.34
 ---- batch: 050 ----
mean loss: 247.42
 ---- batch: 060 ----
mean loss: 243.59
 ---- batch: 070 ----
mean loss: 238.89
 ---- batch: 080 ----
mean loss: 240.14
 ---- batch: 090 ----
mean loss: 240.42
train mean loss: 242.17
epoch train time: 0:00:16.976510
elapsed time: 0:17:16.338314
**** EPOCH 060 ****
---- EPOCH 060 TRAINING ----
2019-09-24 21:37:05.254771
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 241.49
 ---- batch: 020 ----
mean loss: 243.61
 ---- batch: 030 ----
mean loss: 236.31
 ---- batch: 040 ----
mean loss: 240.70
 ---- batch: 050 ----
mean loss: 236.77
 ---- batch: 060 ----
mean loss: 236.81
 ---- batch: 070 ----
mean loss: 243.96
 ---- batch: 080 ----
mean loss: 245.84
 ---- batch: 090 ----
mean loss: 242.28
train mean loss: 240.82
epoch train time: 0:00:17.006672
elapsed time: 0:17:33.346391
**** EPOCH 061 ****
---- EPOCH 061 TRAINING ----
2019-09-24 21:37:22.262795
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 234.93
 ---- batch: 020 ----
mean loss: 237.23
 ---- batch: 030 ----
mean loss: 234.70
 ---- batch: 040 ----
mean loss: 230.50
 ---- batch: 050 ----
mean loss: 236.39
 ---- batch: 060 ----
mean loss: 247.96
 ---- batch: 070 ----
mean loss: 233.54
 ---- batch: 080 ----
mean loss: 234.11
 ---- batch: 090 ----
mean loss: 252.61
train mean loss: 237.88
epoch train time: 0:00:17.025696
elapsed time: 0:17:50.373282
**** EPOCH 062 ****
---- EPOCH 062 TRAINING ----
2019-09-24 21:37:39.289802
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 240.81
 ---- batch: 020 ----
mean loss: 236.82
 ---- batch: 030 ----
mean loss: 239.03
 ---- batch: 040 ----
mean loss: 234.75
 ---- batch: 050 ----
mean loss: 245.66
 ---- batch: 060 ----
mean loss: 238.84
 ---- batch: 070 ----
mean loss: 242.09
 ---- batch: 080 ----
mean loss: 241.21
 ---- batch: 090 ----
mean loss: 233.03
train mean loss: 238.84
epoch train time: 0:00:16.965619
elapsed time: 0:18:07.340205
**** EPOCH 063 ****
---- EPOCH 063 TRAINING ----
2019-09-24 21:37:56.256612
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.64
 ---- batch: 020 ----
mean loss: 239.29
 ---- batch: 030 ----
mean loss: 235.24
 ---- batch: 040 ----
mean loss: 235.61
 ---- batch: 050 ----
mean loss: 246.10
 ---- batch: 060 ----
mean loss: 235.96
 ---- batch: 070 ----
mean loss: 235.64
 ---- batch: 080 ----
mean loss: 230.23
 ---- batch: 090 ----
mean loss: 228.97
train mean loss: 236.33
epoch train time: 0:00:16.955657
elapsed time: 0:18:24.297001
**** EPOCH 064 ****
---- EPOCH 064 TRAINING ----
2019-09-24 21:38:13.213370
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 239.47
 ---- batch: 020 ----
mean loss: 226.88
 ---- batch: 030 ----
mean loss: 241.35
 ---- batch: 040 ----
mean loss: 239.09
 ---- batch: 050 ----
mean loss: 239.93
 ---- batch: 060 ----
mean loss: 237.47
 ---- batch: 070 ----
mean loss: 228.07
 ---- batch: 080 ----
mean loss: 233.23
 ---- batch: 090 ----
mean loss: 234.66
train mean loss: 236.20
epoch train time: 0:00:16.928846
elapsed time: 0:18:41.227025
**** EPOCH 065 ****
---- EPOCH 065 TRAINING ----
2019-09-24 21:38:30.143413
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.65
 ---- batch: 020 ----
mean loss: 240.14
 ---- batch: 030 ----
mean loss: 240.73
 ---- batch: 040 ----
mean loss: 235.59
 ---- batch: 050 ----
mean loss: 234.59
 ---- batch: 060 ----
mean loss: 238.18
 ---- batch: 070 ----
mean loss: 230.47
 ---- batch: 080 ----
mean loss: 236.84
 ---- batch: 090 ----
mean loss: 232.47
train mean loss: 235.13
epoch train time: 0:00:16.991545
elapsed time: 0:18:58.219746
**** EPOCH 066 ****
---- EPOCH 066 TRAINING ----
2019-09-24 21:38:47.136153
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 228.37
 ---- batch: 020 ----
mean loss: 232.66
 ---- batch: 030 ----
mean loss: 239.67
 ---- batch: 040 ----
mean loss: 240.10
 ---- batch: 050 ----
mean loss: 239.81
 ---- batch: 060 ----
mean loss: 239.82
 ---- batch: 070 ----
mean loss: 225.65
 ---- batch: 080 ----
mean loss: 229.18
 ---- batch: 090 ----
mean loss: 228.00
train mean loss: 233.91
epoch train time: 0:00:16.785601
elapsed time: 0:19:15.006693
**** EPOCH 067 ****
---- EPOCH 067 TRAINING ----
2019-09-24 21:39:03.923129
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 233.14
 ---- batch: 020 ----
mean loss: 231.42
 ---- batch: 030 ----
mean loss: 223.13
 ---- batch: 040 ----
mean loss: 229.88
 ---- batch: 050 ----
mean loss: 231.13
 ---- batch: 060 ----
mean loss: 232.61
 ---- batch: 070 ----
mean loss: 235.09
 ---- batch: 080 ----
mean loss: 234.90
 ---- batch: 090 ----
mean loss: 228.56
train mean loss: 231.14
epoch train time: 0:00:16.723782
elapsed time: 0:19:31.731664
**** EPOCH 068 ****
---- EPOCH 068 TRAINING ----
2019-09-24 21:39:20.648057
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.97
 ---- batch: 020 ----
mean loss: 232.71
 ---- batch: 030 ----
mean loss: 221.61
 ---- batch: 040 ----
mean loss: 229.63
 ---- batch: 050 ----
mean loss: 230.27
 ---- batch: 060 ----
mean loss: 227.34
 ---- batch: 070 ----
mean loss: 237.97
 ---- batch: 080 ----
mean loss: 229.07
 ---- batch: 090 ----
mean loss: 235.45
train mean loss: 230.97
epoch train time: 0:00:16.826609
elapsed time: 0:19:48.559476
**** EPOCH 069 ****
---- EPOCH 069 TRAINING ----
2019-09-24 21:39:37.475994
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 237.16
 ---- batch: 020 ----
mean loss: 228.92
 ---- batch: 030 ----
mean loss: 226.26
 ---- batch: 040 ----
mean loss: 227.40
 ---- batch: 050 ----
mean loss: 232.20
 ---- batch: 060 ----
mean loss: 223.67
 ---- batch: 070 ----
mean loss: 218.80
 ---- batch: 080 ----
mean loss: 238.96
 ---- batch: 090 ----
mean loss: 232.16
train mean loss: 229.78
epoch train time: 0:00:16.725940
elapsed time: 0:20:05.286692
**** EPOCH 070 ****
---- EPOCH 070 TRAINING ----
2019-09-24 21:39:54.203061
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 220.24
 ---- batch: 020 ----
mean loss: 223.24
 ---- batch: 030 ----
mean loss: 228.07
 ---- batch: 040 ----
mean loss: 236.23
 ---- batch: 050 ----
mean loss: 239.47
 ---- batch: 060 ----
mean loss: 226.85
 ---- batch: 070 ----
mean loss: 230.86
 ---- batch: 080 ----
mean loss: 228.83
 ---- batch: 090 ----
mean loss: 228.85
train mean loss: 229.19
epoch train time: 0:00:16.673285
elapsed time: 0:20:21.961242
**** EPOCH 071 ****
---- EPOCH 071 TRAINING ----
2019-09-24 21:40:10.877601
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.80
 ---- batch: 020 ----
mean loss: 225.13
 ---- batch: 030 ----
mean loss: 233.74
 ---- batch: 040 ----
mean loss: 225.81
 ---- batch: 050 ----
mean loss: 226.62
 ---- batch: 060 ----
mean loss: 234.62
 ---- batch: 070 ----
mean loss: 219.28
 ---- batch: 080 ----
mean loss: 227.11
 ---- batch: 090 ----
mean loss: 229.33
train mean loss: 227.52
epoch train time: 0:00:16.691066
elapsed time: 0:20:38.653508
**** EPOCH 072 ****
---- EPOCH 072 TRAINING ----
2019-09-24 21:40:27.569905
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.69
 ---- batch: 020 ----
mean loss: 214.30
 ---- batch: 030 ----
mean loss: 227.26
 ---- batch: 040 ----
mean loss: 226.16
 ---- batch: 050 ----
mean loss: 234.01
 ---- batch: 060 ----
mean loss: 224.56
 ---- batch: 070 ----
mean loss: 229.08
 ---- batch: 080 ----
mean loss: 229.85
 ---- batch: 090 ----
mean loss: 229.45
train mean loss: 225.77
epoch train time: 0:00:16.680840
elapsed time: 0:20:55.335555
**** EPOCH 073 ****
---- EPOCH 073 TRAINING ----
2019-09-24 21:40:44.251957
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 225.85
 ---- batch: 020 ----
mean loss: 232.88
 ---- batch: 030 ----
mean loss: 234.58
 ---- batch: 040 ----
mean loss: 222.29
 ---- batch: 050 ----
mean loss: 217.43
 ---- batch: 060 ----
mean loss: 227.25
 ---- batch: 070 ----
mean loss: 224.33
 ---- batch: 080 ----
mean loss: 228.10
 ---- batch: 090 ----
mean loss: 226.06
train mean loss: 226.05
epoch train time: 0:00:16.663894
elapsed time: 0:21:12.000664
**** EPOCH 074 ****
---- EPOCH 074 TRAINING ----
2019-09-24 21:41:00.917082
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 226.54
 ---- batch: 020 ----
mean loss: 228.76
 ---- batch: 030 ----
mean loss: 231.06
 ---- batch: 040 ----
mean loss: 231.08
 ---- batch: 050 ----
mean loss: 224.88
 ---- batch: 060 ----
mean loss: 221.38
 ---- batch: 070 ----
mean loss: 220.97
 ---- batch: 080 ----
mean loss: 219.61
 ---- batch: 090 ----
mean loss: 225.30
train mean loss: 225.60
epoch train time: 0:00:16.696668
elapsed time: 0:21:28.698537
**** EPOCH 075 ****
---- EPOCH 075 TRAINING ----
2019-09-24 21:41:17.614958
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 223.69
 ---- batch: 020 ----
mean loss: 222.83
 ---- batch: 030 ----
mean loss: 229.12
 ---- batch: 040 ----
mean loss: 230.38
 ---- batch: 050 ----
mean loss: 218.52
 ---- batch: 060 ----
mean loss: 218.37
 ---- batch: 070 ----
mean loss: 225.22
 ---- batch: 080 ----
mean loss: 226.51
 ---- batch: 090 ----
mean loss: 223.96
train mean loss: 223.86
epoch train time: 0:00:16.669585
elapsed time: 0:21:45.369381
**** EPOCH 076 ****
---- EPOCH 076 TRAINING ----
2019-09-24 21:41:34.285747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 227.33
 ---- batch: 020 ----
mean loss: 222.91
 ---- batch: 030 ----
mean loss: 221.72
 ---- batch: 040 ----
mean loss: 228.87
 ---- batch: 050 ----
mean loss: 235.86
 ---- batch: 060 ----
mean loss: 222.10
 ---- batch: 070 ----
mean loss: 223.95
 ---- batch: 080 ----
mean loss: 225.24
 ---- batch: 090 ----
mean loss: 228.83
train mean loss: 226.14
epoch train time: 0:00:16.664418
elapsed time: 0:22:02.035039
**** EPOCH 077 ****
---- EPOCH 077 TRAINING ----
2019-09-24 21:41:50.951414
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 218.84
 ---- batch: 020 ----
mean loss: 225.04
 ---- batch: 030 ----
mean loss: 223.67
 ---- batch: 040 ----
mean loss: 238.41
 ---- batch: 050 ----
mean loss: 218.99
 ---- batch: 060 ----
mean loss: 215.58
 ---- batch: 070 ----
mean loss: 222.83
 ---- batch: 080 ----
mean loss: 222.94
 ---- batch: 090 ----
mean loss: 219.08
train mean loss: 223.06
epoch train time: 0:00:16.676969
elapsed time: 0:22:18.713174
**** EPOCH 078 ****
---- EPOCH 078 TRAINING ----
2019-09-24 21:42:07.629523
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.75
 ---- batch: 020 ----
mean loss: 219.24
 ---- batch: 030 ----
mean loss: 220.78
 ---- batch: 040 ----
mean loss: 232.65
 ---- batch: 050 ----
mean loss: 220.56
 ---- batch: 060 ----
mean loss: 215.92
 ---- batch: 070 ----
mean loss: 233.93
 ---- batch: 080 ----
mean loss: 217.95
 ---- batch: 090 ----
mean loss: 221.86
train mean loss: 221.66
epoch train time: 0:00:16.657080
elapsed time: 0:22:35.371393
**** EPOCH 079 ****
---- EPOCH 079 TRAINING ----
2019-09-24 21:42:24.287622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.07
 ---- batch: 020 ----
mean loss: 217.65
 ---- batch: 030 ----
mean loss: 220.74
 ---- batch: 040 ----
mean loss: 228.67
 ---- batch: 050 ----
mean loss: 219.81
 ---- batch: 060 ----
mean loss: 227.10
 ---- batch: 070 ----
mean loss: 214.69
 ---- batch: 080 ----
mean loss: 220.15
 ---- batch: 090 ----
mean loss: 221.95
train mean loss: 220.09
epoch train time: 0:00:16.653366
elapsed time: 0:22:52.025768
**** EPOCH 080 ****
---- EPOCH 080 TRAINING ----
2019-09-24 21:42:40.942214
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.82
 ---- batch: 020 ----
mean loss: 214.12
 ---- batch: 030 ----
mean loss: 218.68
 ---- batch: 040 ----
mean loss: 218.39
 ---- batch: 050 ----
mean loss: 218.84
 ---- batch: 060 ----
mean loss: 218.63
 ---- batch: 070 ----
mean loss: 218.87
 ---- batch: 080 ----
mean loss: 223.72
 ---- batch: 090 ----
mean loss: 226.70
train mean loss: 219.96
epoch train time: 0:00:16.645708
elapsed time: 0:23:08.672963
**** EPOCH 081 ****
---- EPOCH 081 TRAINING ----
2019-09-24 21:42:57.589401
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.83
 ---- batch: 020 ----
mean loss: 223.32
 ---- batch: 030 ----
mean loss: 226.40
 ---- batch: 040 ----
mean loss: 218.84
 ---- batch: 050 ----
mean loss: 226.18
 ---- batch: 060 ----
mean loss: 216.62
 ---- batch: 070 ----
mean loss: 222.92
 ---- batch: 080 ----
mean loss: 214.99
 ---- batch: 090 ----
mean loss: 220.51
train mean loss: 220.42
epoch train time: 0:00:16.667644
elapsed time: 0:23:25.341893
**** EPOCH 082 ****
---- EPOCH 082 TRAINING ----
2019-09-24 21:43:14.258289
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.92
 ---- batch: 020 ----
mean loss: 216.59
 ---- batch: 030 ----
mean loss: 217.72
 ---- batch: 040 ----
mean loss: 215.65
 ---- batch: 050 ----
mean loss: 221.97
 ---- batch: 060 ----
mean loss: 225.95
 ---- batch: 070 ----
mean loss: 213.38
 ---- batch: 080 ----
mean loss: 227.74
 ---- batch: 090 ----
mean loss: 214.69
train mean loss: 218.65
epoch train time: 0:00:16.712817
elapsed time: 0:23:42.055895
**** EPOCH 083 ****
---- EPOCH 083 TRAINING ----
2019-09-24 21:43:30.972249
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 219.22
 ---- batch: 020 ----
mean loss: 212.81
 ---- batch: 030 ----
mean loss: 223.85
 ---- batch: 040 ----
mean loss: 215.23
 ---- batch: 050 ----
mean loss: 217.36
 ---- batch: 060 ----
mean loss: 216.56
 ---- batch: 070 ----
mean loss: 217.73
 ---- batch: 080 ----
mean loss: 216.40
 ---- batch: 090 ----
mean loss: 220.38
train mean loss: 217.98
epoch train time: 0:00:16.680145
elapsed time: 0:23:58.737215
**** EPOCH 084 ****
---- EPOCH 084 TRAINING ----
2019-09-24 21:43:47.653557
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.08
 ---- batch: 020 ----
mean loss: 213.46
 ---- batch: 030 ----
mean loss: 218.00
 ---- batch: 040 ----
mean loss: 212.12
 ---- batch: 050 ----
mean loss: 222.92
 ---- batch: 060 ----
mean loss: 216.31
 ---- batch: 070 ----
mean loss: 210.01
 ---- batch: 080 ----
mean loss: 218.26
 ---- batch: 090 ----
mean loss: 225.15
train mean loss: 216.92
epoch train time: 0:00:16.742844
elapsed time: 0:24:15.481233
**** EPOCH 085 ****
---- EPOCH 085 TRAINING ----
2019-09-24 21:44:04.397647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 221.68
 ---- batch: 020 ----
mean loss: 228.18
 ---- batch: 030 ----
mean loss: 223.57
 ---- batch: 040 ----
mean loss: 227.47
 ---- batch: 050 ----
mean loss: 216.31
 ---- batch: 060 ----
mean loss: 205.53
 ---- batch: 070 ----
mean loss: 204.01
 ---- batch: 080 ----
mean loss: 214.97
 ---- batch: 090 ----
mean loss: 225.10
train mean loss: 218.88
epoch train time: 0:00:16.602473
elapsed time: 0:24:32.084907
**** EPOCH 086 ****
---- EPOCH 086 TRAINING ----
2019-09-24 21:44:21.001258
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.77
 ---- batch: 020 ----
mean loss: 213.02
 ---- batch: 030 ----
mean loss: 221.59
 ---- batch: 040 ----
mean loss: 221.14
 ---- batch: 050 ----
mean loss: 213.26
 ---- batch: 060 ----
mean loss: 222.74
 ---- batch: 070 ----
mean loss: 212.17
 ---- batch: 080 ----
mean loss: 213.47
 ---- batch: 090 ----
mean loss: 210.86
train mean loss: 215.89
epoch train time: 0:00:16.641926
elapsed time: 0:24:48.727995
**** EPOCH 087 ****
---- EPOCH 087 TRAINING ----
2019-09-24 21:44:37.644365
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.23
 ---- batch: 020 ----
mean loss: 215.37
 ---- batch: 030 ----
mean loss: 218.30
 ---- batch: 040 ----
mean loss: 215.60
 ---- batch: 050 ----
mean loss: 201.49
 ---- batch: 060 ----
mean loss: 217.00
 ---- batch: 070 ----
mean loss: 217.72
 ---- batch: 080 ----
mean loss: 216.19
 ---- batch: 090 ----
mean loss: 217.82
train mean loss: 215.10
epoch train time: 0:00:16.615781
elapsed time: 0:25:05.344914
**** EPOCH 088 ****
---- EPOCH 088 TRAINING ----
2019-09-24 21:44:54.261757
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 224.23
 ---- batch: 020 ----
mean loss: 216.12
 ---- batch: 030 ----
mean loss: 214.27
 ---- batch: 040 ----
mean loss: 217.01
 ---- batch: 050 ----
mean loss: 210.29
 ---- batch: 060 ----
mean loss: 222.31
 ---- batch: 070 ----
mean loss: 212.98
 ---- batch: 080 ----
mean loss: 212.60
 ---- batch: 090 ----
mean loss: 209.06
train mean loss: 214.95
epoch train time: 0:00:16.665313
elapsed time: 0:25:22.011918
**** EPOCH 089 ****
---- EPOCH 089 TRAINING ----
2019-09-24 21:45:10.928313
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.46
 ---- batch: 020 ----
mean loss: 219.99
 ---- batch: 030 ----
mean loss: 217.19
 ---- batch: 040 ----
mean loss: 213.73
 ---- batch: 050 ----
mean loss: 217.43
 ---- batch: 060 ----
mean loss: 213.77
 ---- batch: 070 ----
mean loss: 211.30
 ---- batch: 080 ----
mean loss: 215.14
 ---- batch: 090 ----
mean loss: 215.86
train mean loss: 214.97
epoch train time: 0:00:16.661565
elapsed time: 0:25:38.674630
**** EPOCH 090 ****
---- EPOCH 090 TRAINING ----
2019-09-24 21:45:27.591030
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.66
 ---- batch: 020 ----
mean loss: 211.50
 ---- batch: 030 ----
mean loss: 205.07
 ---- batch: 040 ----
mean loss: 216.58
 ---- batch: 050 ----
mean loss: 220.79
 ---- batch: 060 ----
mean loss: 221.45
 ---- batch: 070 ----
mean loss: 209.98
 ---- batch: 080 ----
mean loss: 224.01
 ---- batch: 090 ----
mean loss: 213.72
train mean loss: 215.00
epoch train time: 0:00:16.607094
elapsed time: 0:25:55.282874
**** EPOCH 091 ****
---- EPOCH 091 TRAINING ----
2019-09-24 21:45:44.199188
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.53
 ---- batch: 020 ----
mean loss: 222.10
 ---- batch: 030 ----
mean loss: 208.73
 ---- batch: 040 ----
mean loss: 214.58
 ---- batch: 050 ----
mean loss: 222.01
 ---- batch: 060 ----
mean loss: 223.78
 ---- batch: 070 ----
mean loss: 218.82
 ---- batch: 080 ----
mean loss: 215.75
 ---- batch: 090 ----
mean loss: 210.45
train mean loss: 216.30
epoch train time: 0:00:16.579949
elapsed time: 0:26:11.864041
**** EPOCH 092 ****
---- EPOCH 092 TRAINING ----
2019-09-24 21:46:00.780441
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.18
 ---- batch: 020 ----
mean loss: 220.36
 ---- batch: 030 ----
mean loss: 214.45
 ---- batch: 040 ----
mean loss: 212.49
 ---- batch: 050 ----
mean loss: 207.99
 ---- batch: 060 ----
mean loss: 215.95
 ---- batch: 070 ----
mean loss: 212.97
 ---- batch: 080 ----
mean loss: 220.63
 ---- batch: 090 ----
mean loss: 208.28
train mean loss: 213.77
epoch train time: 0:00:16.529722
elapsed time: 0:26:28.394942
**** EPOCH 093 ****
---- EPOCH 093 TRAINING ----
2019-09-24 21:46:17.311354
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.27
 ---- batch: 020 ----
mean loss: 215.54
 ---- batch: 030 ----
mean loss: 213.48
 ---- batch: 040 ----
mean loss: 215.66
 ---- batch: 050 ----
mean loss: 212.06
 ---- batch: 060 ----
mean loss: 219.59
 ---- batch: 070 ----
mean loss: 221.97
 ---- batch: 080 ----
mean loss: 212.65
 ---- batch: 090 ----
mean loss: 205.70
train mean loss: 214.24
epoch train time: 0:00:16.751675
elapsed time: 0:26:45.147800
**** EPOCH 094 ****
---- EPOCH 094 TRAINING ----
2019-09-24 21:46:34.064179
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.36
 ---- batch: 020 ----
mean loss: 207.07
 ---- batch: 030 ----
mean loss: 220.57
 ---- batch: 040 ----
mean loss: 202.98
 ---- batch: 050 ----
mean loss: 209.43
 ---- batch: 060 ----
mean loss: 221.36
 ---- batch: 070 ----
mean loss: 215.72
 ---- batch: 080 ----
mean loss: 211.87
 ---- batch: 090 ----
mean loss: 212.31
train mean loss: 212.10
epoch train time: 0:00:16.890309
elapsed time: 0:27:02.039554
**** EPOCH 095 ****
---- EPOCH 095 TRAINING ----
2019-09-24 21:46:50.955938
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 215.38
 ---- batch: 020 ----
mean loss: 215.83
 ---- batch: 030 ----
mean loss: 217.53
 ---- batch: 040 ----
mean loss: 207.24
 ---- batch: 050 ----
mean loss: 215.55
 ---- batch: 060 ----
mean loss: 209.55
 ---- batch: 070 ----
mean loss: 205.78
 ---- batch: 080 ----
mean loss: 212.33
 ---- batch: 090 ----
mean loss: 211.14
train mean loss: 212.08
epoch train time: 0:00:16.947583
elapsed time: 0:27:18.988344
**** EPOCH 096 ****
---- EPOCH 096 TRAINING ----
2019-09-24 21:47:07.904711
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 214.34
 ---- batch: 020 ----
mean loss: 206.15
 ---- batch: 030 ----
mean loss: 209.26
 ---- batch: 040 ----
mean loss: 218.06
 ---- batch: 050 ----
mean loss: 213.26
 ---- batch: 060 ----
mean loss: 204.75
 ---- batch: 070 ----
mean loss: 217.99
 ---- batch: 080 ----
mean loss: 206.87
 ---- batch: 090 ----
mean loss: 213.39
train mean loss: 211.92
epoch train time: 0:00:16.900790
elapsed time: 0:27:35.890368
**** EPOCH 097 ****
---- EPOCH 097 TRAINING ----
2019-09-24 21:47:24.806815
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.69
 ---- batch: 020 ----
mean loss: 208.60
 ---- batch: 030 ----
mean loss: 209.03
 ---- batch: 040 ----
mean loss: 211.00
 ---- batch: 050 ----
mean loss: 209.23
 ---- batch: 060 ----
mean loss: 209.82
 ---- batch: 070 ----
mean loss: 217.52
 ---- batch: 080 ----
mean loss: 217.42
 ---- batch: 090 ----
mean loss: 217.87
train mean loss: 212.27
epoch train time: 0:00:16.802245
elapsed time: 0:27:52.693868
**** EPOCH 098 ****
---- EPOCH 098 TRAINING ----
2019-09-24 21:47:41.610242
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.58
 ---- batch: 020 ----
mean loss: 218.24
 ---- batch: 030 ----
mean loss: 204.27
 ---- batch: 040 ----
mean loss: 208.29
 ---- batch: 050 ----
mean loss: 204.25
 ---- batch: 060 ----
mean loss: 213.87
 ---- batch: 070 ----
mean loss: 211.26
 ---- batch: 080 ----
mean loss: 219.97
 ---- batch: 090 ----
mean loss: 209.66
train mean loss: 211.33
epoch train time: 0:00:16.706816
elapsed time: 0:28:09.401815
**** EPOCH 099 ****
---- EPOCH 099 TRAINING ----
2019-09-24 21:47:58.318245
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.68
 ---- batch: 020 ----
mean loss: 203.85
 ---- batch: 030 ----
mean loss: 204.89
 ---- batch: 040 ----
mean loss: 205.34
 ---- batch: 050 ----
mean loss: 213.87
 ---- batch: 060 ----
mean loss: 211.91
 ---- batch: 070 ----
mean loss: 207.07
 ---- batch: 080 ----
mean loss: 211.40
 ---- batch: 090 ----
mean loss: 209.91
train mean loss: 209.71
epoch train time: 0:00:16.689067
elapsed time: 0:28:26.092051
**** EPOCH 100 ****
---- EPOCH 100 TRAINING ----
2019-09-24 21:48:15.008408
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.22
 ---- batch: 020 ----
mean loss: 205.37
 ---- batch: 030 ----
mean loss: 206.38
 ---- batch: 040 ----
mean loss: 208.17
 ---- batch: 050 ----
mean loss: 217.98
 ---- batch: 060 ----
mean loss: 201.74
 ---- batch: 070 ----
mean loss: 201.05
 ---- batch: 080 ----
mean loss: 210.54
 ---- batch: 090 ----
mean loss: 210.70
train mean loss: 208.71
epoch train time: 0:00:16.759059
elapsed time: 0:28:42.852283
**** EPOCH 101 ****
---- EPOCH 101 TRAINING ----
2019-09-24 21:48:31.768639
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.63
 ---- batch: 020 ----
mean loss: 208.20
 ---- batch: 030 ----
mean loss: 211.07
 ---- batch: 040 ----
mean loss: 206.99
 ---- batch: 050 ----
mean loss: 212.94
 ---- batch: 060 ----
mean loss: 210.88
 ---- batch: 070 ----
mean loss: 204.74
 ---- batch: 080 ----
mean loss: 203.52
 ---- batch: 090 ----
mean loss: 209.28
train mean loss: 208.72
epoch train time: 0:00:16.811089
elapsed time: 0:28:59.664488
**** EPOCH 102 ****
---- EPOCH 102 TRAINING ----
2019-09-24 21:48:48.580867
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 213.13
 ---- batch: 020 ----
mean loss: 202.61
 ---- batch: 030 ----
mean loss: 204.75
 ---- batch: 040 ----
mean loss: 205.90
 ---- batch: 050 ----
mean loss: 204.39
 ---- batch: 060 ----
mean loss: 209.16
 ---- batch: 070 ----
mean loss: 207.38
 ---- batch: 080 ----
mean loss: 204.42
 ---- batch: 090 ----
mean loss: 213.13
train mean loss: 207.95
epoch train time: 0:00:16.782617
elapsed time: 0:29:16.448260
**** EPOCH 103 ****
---- EPOCH 103 TRAINING ----
2019-09-24 21:49:05.364643
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.30
 ---- batch: 020 ----
mean loss: 207.60
 ---- batch: 030 ----
mean loss: 206.03
 ---- batch: 040 ----
mean loss: 204.79
 ---- batch: 050 ----
mean loss: 208.44
 ---- batch: 060 ----
mean loss: 207.11
 ---- batch: 070 ----
mean loss: 212.73
 ---- batch: 080 ----
mean loss: 201.72
 ---- batch: 090 ----
mean loss: 209.23
train mean loss: 208.13
epoch train time: 0:00:16.697503
elapsed time: 0:29:33.146931
**** EPOCH 104 ****
---- EPOCH 104 TRAINING ----
2019-09-24 21:49:22.063387
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.49
 ---- batch: 020 ----
mean loss: 218.48
 ---- batch: 030 ----
mean loss: 208.20
 ---- batch: 040 ----
mean loss: 211.29
 ---- batch: 050 ----
mean loss: 211.60
 ---- batch: 060 ----
mean loss: 211.12
 ---- batch: 070 ----
mean loss: 198.04
 ---- batch: 080 ----
mean loss: 204.39
 ---- batch: 090 ----
mean loss: 201.53
train mean loss: 208.72
epoch train time: 0:00:16.695074
elapsed time: 0:29:49.843229
**** EPOCH 105 ****
---- EPOCH 105 TRAINING ----
2019-09-24 21:49:38.759647
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.47
 ---- batch: 020 ----
mean loss: 204.12
 ---- batch: 030 ----
mean loss: 216.73
 ---- batch: 040 ----
mean loss: 211.18
 ---- batch: 050 ----
mean loss: 200.92
 ---- batch: 060 ----
mean loss: 210.73
 ---- batch: 070 ----
mean loss: 206.26
 ---- batch: 080 ----
mean loss: 211.32
 ---- batch: 090 ----
mean loss: 213.93
train mean loss: 208.75
epoch train time: 0:00:16.581201
elapsed time: 0:30:06.425620
**** EPOCH 106 ****
---- EPOCH 106 TRAINING ----
2019-09-24 21:49:55.342049
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.02
 ---- batch: 020 ----
mean loss: 209.94
 ---- batch: 030 ----
mean loss: 204.90
 ---- batch: 040 ----
mean loss: 200.36
 ---- batch: 050 ----
mean loss: 211.68
 ---- batch: 060 ----
mean loss: 211.57
 ---- batch: 070 ----
mean loss: 204.33
 ---- batch: 080 ----
mean loss: 207.64
 ---- batch: 090 ----
mean loss: 205.47
train mean loss: 206.76
epoch train time: 0:00:16.560229
elapsed time: 0:30:22.987298
**** EPOCH 107 ****
---- EPOCH 107 TRAINING ----
2019-09-24 21:50:11.903553
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 203.20
 ---- batch: 020 ----
mean loss: 211.14
 ---- batch: 030 ----
mean loss: 202.65
 ---- batch: 040 ----
mean loss: 208.45
 ---- batch: 050 ----
mean loss: 201.11
 ---- batch: 060 ----
mean loss: 207.13
 ---- batch: 070 ----
mean loss: 205.40
 ---- batch: 080 ----
mean loss: 216.11
 ---- batch: 090 ----
mean loss: 211.32
train mean loss: 207.09
epoch train time: 0:00:16.587927
elapsed time: 0:30:39.576227
**** EPOCH 108 ****
---- EPOCH 108 TRAINING ----
2019-09-24 21:50:28.492580
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 216.93
 ---- batch: 020 ----
mean loss: 208.98
 ---- batch: 030 ----
mean loss: 201.19
 ---- batch: 040 ----
mean loss: 205.33
 ---- batch: 050 ----
mean loss: 200.60
 ---- batch: 060 ----
mean loss: 211.24
 ---- batch: 070 ----
mean loss: 208.40
 ---- batch: 080 ----
mean loss: 209.70
 ---- batch: 090 ----
mean loss: 206.03
train mean loss: 207.89
epoch train time: 0:00:16.630656
elapsed time: 0:30:56.208021
**** EPOCH 109 ****
---- EPOCH 109 TRAINING ----
2019-09-24 21:50:45.124443
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.23
 ---- batch: 020 ----
mean loss: 207.95
 ---- batch: 030 ----
mean loss: 209.63
 ---- batch: 040 ----
mean loss: 199.90
 ---- batch: 050 ----
mean loss: 202.92
 ---- batch: 060 ----
mean loss: 209.11
 ---- batch: 070 ----
mean loss: 208.40
 ---- batch: 080 ----
mean loss: 209.13
 ---- batch: 090 ----
mean loss: 207.05
train mean loss: 207.56
epoch train time: 0:00:16.660321
elapsed time: 0:31:12.869553
**** EPOCH 110 ****
---- EPOCH 110 TRAINING ----
2019-09-24 21:51:01.785988
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 217.03
 ---- batch: 020 ----
mean loss: 205.75
 ---- batch: 030 ----
mean loss: 209.74
 ---- batch: 040 ----
mean loss: 206.75
 ---- batch: 050 ----
mean loss: 205.37
 ---- batch: 060 ----
mean loss: 209.88
 ---- batch: 070 ----
mean loss: 200.11
 ---- batch: 080 ----
mean loss: 200.54
 ---- batch: 090 ----
mean loss: 205.10
train mean loss: 206.72
epoch train time: 0:00:16.692069
elapsed time: 0:31:29.562817
**** EPOCH 111 ****
---- EPOCH 111 TRAINING ----
2019-09-24 21:51:18.479146
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.10
 ---- batch: 020 ----
mean loss: 204.38
 ---- batch: 030 ----
mean loss: 212.77
 ---- batch: 040 ----
mean loss: 212.23
 ---- batch: 050 ----
mean loss: 209.34
 ---- batch: 060 ----
mean loss: 205.44
 ---- batch: 070 ----
mean loss: 209.30
 ---- batch: 080 ----
mean loss: 199.51
 ---- batch: 090 ----
mean loss: 201.91
train mean loss: 205.32
epoch train time: 0:00:16.674389
elapsed time: 0:31:46.238374
**** EPOCH 112 ****
---- EPOCH 112 TRAINING ----
2019-09-24 21:51:35.154758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.63
 ---- batch: 020 ----
mean loss: 207.20
 ---- batch: 030 ----
mean loss: 203.32
 ---- batch: 040 ----
mean loss: 202.04
 ---- batch: 050 ----
mean loss: 201.17
 ---- batch: 060 ----
mean loss: 209.82
 ---- batch: 070 ----
mean loss: 214.42
 ---- batch: 080 ----
mean loss: 206.40
 ---- batch: 090 ----
mean loss: 209.50
train mean loss: 205.91
epoch train time: 0:00:16.704228
elapsed time: 0:32:02.943774
**** EPOCH 113 ****
---- EPOCH 113 TRAINING ----
2019-09-24 21:51:51.860135
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.13
 ---- batch: 020 ----
mean loss: 209.97
 ---- batch: 030 ----
mean loss: 203.86
 ---- batch: 040 ----
mean loss: 210.92
 ---- batch: 050 ----
mean loss: 206.93
 ---- batch: 060 ----
mean loss: 211.58
 ---- batch: 070 ----
mean loss: 196.74
 ---- batch: 080 ----
mean loss: 205.39
 ---- batch: 090 ----
mean loss: 203.43
train mean loss: 206.88
epoch train time: 0:00:16.679378
elapsed time: 0:32:19.624301
**** EPOCH 114 ****
---- EPOCH 114 TRAINING ----
2019-09-24 21:52:08.540653
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.35
 ---- batch: 020 ----
mean loss: 208.19
 ---- batch: 030 ----
mean loss: 200.64
 ---- batch: 040 ----
mean loss: 204.21
 ---- batch: 050 ----
mean loss: 205.25
 ---- batch: 060 ----
mean loss: 206.80
 ---- batch: 070 ----
mean loss: 208.05
 ---- batch: 080 ----
mean loss: 192.49
 ---- batch: 090 ----
mean loss: 206.76
train mean loss: 204.47
epoch train time: 0:00:16.649950
elapsed time: 0:32:36.275442
**** EPOCH 115 ****
---- EPOCH 115 TRAINING ----
2019-09-24 21:52:25.191778
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.02
 ---- batch: 020 ----
mean loss: 218.53
 ---- batch: 030 ----
mean loss: 217.31
 ---- batch: 040 ----
mean loss: 206.26
 ---- batch: 050 ----
mean loss: 216.43
 ---- batch: 060 ----
mean loss: 204.25
 ---- batch: 070 ----
mean loss: 207.99
 ---- batch: 080 ----
mean loss: 201.25
 ---- batch: 090 ----
mean loss: 203.00
train mean loss: 208.85
epoch train time: 0:00:16.622528
elapsed time: 0:32:52.899111
**** EPOCH 116 ****
---- EPOCH 116 TRAINING ----
2019-09-24 21:52:41.815513
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.91
 ---- batch: 020 ----
mean loss: 198.13
 ---- batch: 030 ----
mean loss: 203.60
 ---- batch: 040 ----
mean loss: 199.98
 ---- batch: 050 ----
mean loss: 206.25
 ---- batch: 060 ----
mean loss: 209.17
 ---- batch: 070 ----
mean loss: 212.70
 ---- batch: 080 ----
mean loss: 210.77
 ---- batch: 090 ----
mean loss: 207.90
train mean loss: 204.39
epoch train time: 0:00:16.656811
elapsed time: 0:33:09.557222
**** EPOCH 117 ****
---- EPOCH 117 TRAINING ----
2019-09-24 21:52:58.473662
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.42
 ---- batch: 020 ----
mean loss: 198.10
 ---- batch: 030 ----
mean loss: 204.96
 ---- batch: 040 ----
mean loss: 203.79
 ---- batch: 050 ----
mean loss: 204.31
 ---- batch: 060 ----
mean loss: 203.87
 ---- batch: 070 ----
mean loss: 223.57
 ---- batch: 080 ----
mean loss: 216.16
 ---- batch: 090 ----
mean loss: 218.10
train mean loss: 208.37
epoch train time: 0:00:16.573948
elapsed time: 0:33:26.132394
**** EPOCH 118 ****
---- EPOCH 118 TRAINING ----
2019-09-24 21:53:15.048773
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.88
 ---- batch: 020 ----
mean loss: 210.63
 ---- batch: 030 ----
mean loss: 210.20
 ---- batch: 040 ----
mean loss: 211.08
 ---- batch: 050 ----
mean loss: 208.75
 ---- batch: 060 ----
mean loss: 204.80
 ---- batch: 070 ----
mean loss: 203.13
 ---- batch: 080 ----
mean loss: 207.74
 ---- batch: 090 ----
mean loss: 195.60
train mean loss: 205.45
epoch train time: 0:00:16.620106
elapsed time: 0:33:42.753772
**** EPOCH 119 ****
---- EPOCH 119 TRAINING ----
2019-09-24 21:53:31.670195
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 208.21
 ---- batch: 020 ----
mean loss: 202.48
 ---- batch: 030 ----
mean loss: 200.91
 ---- batch: 040 ----
mean loss: 207.43
 ---- batch: 050 ----
mean loss: 199.49
 ---- batch: 060 ----
mean loss: 209.42
 ---- batch: 070 ----
mean loss: 201.56
 ---- batch: 080 ----
mean loss: 207.51
 ---- batch: 090 ----
mean loss: 203.31
train mean loss: 204.48
epoch train time: 0:00:16.728857
elapsed time: 0:33:59.483947
**** EPOCH 120 ****
---- EPOCH 120 TRAINING ----
2019-09-24 21:53:48.400339
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 209.32
 ---- batch: 020 ----
mean loss: 205.04
 ---- batch: 030 ----
mean loss: 201.78
 ---- batch: 040 ----
mean loss: 211.86
 ---- batch: 050 ----
mean loss: 206.57
 ---- batch: 060 ----
mean loss: 203.61
 ---- batch: 070 ----
mean loss: 196.13
 ---- batch: 080 ----
mean loss: 208.82
 ---- batch: 090 ----
mean loss: 199.08
train mean loss: 204.47
epoch train time: 0:00:16.882860
elapsed time: 0:34:16.367989
**** EPOCH 121 ****
---- EPOCH 121 TRAINING ----
2019-09-24 21:54:05.284221
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.22
 ---- batch: 020 ----
mean loss: 203.23
 ---- batch: 030 ----
mean loss: 211.50
 ---- batch: 040 ----
mean loss: 206.87
 ---- batch: 050 ----
mean loss: 215.04
 ---- batch: 060 ----
mean loss: 205.10
 ---- batch: 070 ----
mean loss: 207.37
 ---- batch: 080 ----
mean loss: 209.52
 ---- batch: 090 ----
mean loss: 201.48
train mean loss: 205.60
epoch train time: 0:00:16.854268
elapsed time: 0:34:33.223271
**** EPOCH 122 ****
---- EPOCH 122 TRAINING ----
2019-09-24 21:54:22.139670
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.04
 ---- batch: 020 ----
mean loss: 205.75
 ---- batch: 030 ----
mean loss: 201.23
 ---- batch: 040 ----
mean loss: 207.71
 ---- batch: 050 ----
mean loss: 201.25
 ---- batch: 060 ----
mean loss: 202.01
 ---- batch: 070 ----
mean loss: 197.02
 ---- batch: 080 ----
mean loss: 199.88
 ---- batch: 090 ----
mean loss: 196.89
train mean loss: 202.20
epoch train time: 0:00:16.623558
elapsed time: 0:34:49.848023
**** EPOCH 123 ****
---- EPOCH 123 TRAINING ----
2019-09-24 21:54:38.764402
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.37
 ---- batch: 020 ----
mean loss: 194.63
 ---- batch: 030 ----
mean loss: 196.26
 ---- batch: 040 ----
mean loss: 206.90
 ---- batch: 050 ----
mean loss: 206.22
 ---- batch: 060 ----
mean loss: 203.10
 ---- batch: 070 ----
mean loss: 204.13
 ---- batch: 080 ----
mean loss: 203.89
 ---- batch: 090 ----
mean loss: 200.79
train mean loss: 202.11
epoch train time: 0:00:16.652920
elapsed time: 0:35:06.502168
**** EPOCH 124 ****
---- EPOCH 124 TRAINING ----
2019-09-24 21:54:55.418599
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.94
 ---- batch: 020 ----
mean loss: 200.22
 ---- batch: 030 ----
mean loss: 196.17
 ---- batch: 040 ----
mean loss: 201.57
 ---- batch: 050 ----
mean loss: 200.97
 ---- batch: 060 ----
mean loss: 204.31
 ---- batch: 070 ----
mean loss: 203.66
 ---- batch: 080 ----
mean loss: 203.14
 ---- batch: 090 ----
mean loss: 204.04
train mean loss: 201.07
epoch train time: 0:00:16.565477
elapsed time: 0:35:23.068906
**** EPOCH 125 ****
---- EPOCH 125 TRAINING ----
2019-09-24 21:55:11.985233
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.50
 ---- batch: 020 ----
mean loss: 203.53
 ---- batch: 030 ----
mean loss: 197.32
 ---- batch: 040 ----
mean loss: 198.41
 ---- batch: 050 ----
mean loss: 201.27
 ---- batch: 060 ----
mean loss: 199.63
 ---- batch: 070 ----
mean loss: 203.32
 ---- batch: 080 ----
mean loss: 206.52
 ---- batch: 090 ----
mean loss: 209.04
train mean loss: 201.87
epoch train time: 0:00:16.585300
elapsed time: 0:35:39.655478
**** EPOCH 126 ****
---- EPOCH 126 TRAINING ----
2019-09-24 21:55:28.571827
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.48
 ---- batch: 020 ----
mean loss: 203.30
 ---- batch: 030 ----
mean loss: 208.97
 ---- batch: 040 ----
mean loss: 207.95
 ---- batch: 050 ----
mean loss: 201.22
 ---- batch: 060 ----
mean loss: 200.69
 ---- batch: 070 ----
mean loss: 198.31
 ---- batch: 080 ----
mean loss: 202.98
 ---- batch: 090 ----
mean loss: 204.25
train mean loss: 202.38
epoch train time: 0:00:16.534798
elapsed time: 0:35:56.191683
**** EPOCH 127 ****
---- EPOCH 127 TRAINING ----
2019-09-24 21:55:45.107959
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.72
 ---- batch: 020 ----
mean loss: 201.97
 ---- batch: 030 ----
mean loss: 199.17
 ---- batch: 040 ----
mean loss: 204.32
 ---- batch: 050 ----
mean loss: 202.67
 ---- batch: 060 ----
mean loss: 200.85
 ---- batch: 070 ----
mean loss: 195.71
 ---- batch: 080 ----
mean loss: 198.46
 ---- batch: 090 ----
mean loss: 207.44
train mean loss: 201.21
epoch train time: 0:00:16.551776
elapsed time: 0:36:12.744473
**** EPOCH 128 ****
---- EPOCH 128 TRAINING ----
2019-09-24 21:56:01.660918
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 212.86
 ---- batch: 020 ----
mean loss: 201.43
 ---- batch: 030 ----
mean loss: 204.99
 ---- batch: 040 ----
mean loss: 207.22
 ---- batch: 050 ----
mean loss: 202.77
 ---- batch: 060 ----
mean loss: 197.32
 ---- batch: 070 ----
mean loss: 202.86
 ---- batch: 080 ----
mean loss: 204.58
 ---- batch: 090 ----
mean loss: 208.30
train mean loss: 204.40
epoch train time: 0:00:16.551132
elapsed time: 0:36:29.296829
**** EPOCH 129 ****
---- EPOCH 129 TRAINING ----
2019-09-24 21:56:18.213205
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.88
 ---- batch: 020 ----
mean loss: 199.40
 ---- batch: 030 ----
mean loss: 198.44
 ---- batch: 040 ----
mean loss: 205.19
 ---- batch: 050 ----
mean loss: 200.07
 ---- batch: 060 ----
mean loss: 195.90
 ---- batch: 070 ----
mean loss: 201.96
 ---- batch: 080 ----
mean loss: 206.10
 ---- batch: 090 ----
mean loss: 202.52
train mean loss: 201.26
epoch train time: 0:00:16.638249
elapsed time: 0:36:45.936251
**** EPOCH 130 ****
---- EPOCH 130 TRAINING ----
2019-09-24 21:56:34.852616
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.35
 ---- batch: 020 ----
mean loss: 198.91
 ---- batch: 030 ----
mean loss: 200.83
 ---- batch: 040 ----
mean loss: 205.39
 ---- batch: 050 ----
mean loss: 200.79
 ---- batch: 060 ----
mean loss: 197.79
 ---- batch: 070 ----
mean loss: 199.66
 ---- batch: 080 ----
mean loss: 199.18
 ---- batch: 090 ----
mean loss: 207.87
train mean loss: 200.85
epoch train time: 0:00:16.618775
elapsed time: 0:37:02.556131
**** EPOCH 131 ****
---- EPOCH 131 TRAINING ----
2019-09-24 21:56:51.472497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 205.21
 ---- batch: 020 ----
mean loss: 200.49
 ---- batch: 030 ----
mean loss: 207.62
 ---- batch: 040 ----
mean loss: 208.96
 ---- batch: 050 ----
mean loss: 203.12
 ---- batch: 060 ----
mean loss: 193.51
 ---- batch: 070 ----
mean loss: 204.02
 ---- batch: 080 ----
mean loss: 209.74
 ---- batch: 090 ----
mean loss: 202.04
train mean loss: 204.07
epoch train time: 0:00:16.540771
elapsed time: 0:37:19.098121
**** EPOCH 132 ****
---- EPOCH 132 TRAINING ----
2019-09-24 21:57:08.014532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.53
 ---- batch: 020 ----
mean loss: 199.86
 ---- batch: 030 ----
mean loss: 196.51
 ---- batch: 040 ----
mean loss: 197.18
 ---- batch: 050 ----
mean loss: 203.40
 ---- batch: 060 ----
mean loss: 201.45
 ---- batch: 070 ----
mean loss: 197.59
 ---- batch: 080 ----
mean loss: 206.91
 ---- batch: 090 ----
mean loss: 197.15
train mean loss: 200.34
epoch train time: 0:00:16.576167
elapsed time: 0:37:35.675455
**** EPOCH 133 ****
---- EPOCH 133 TRAINING ----
2019-09-24 21:57:24.591828
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.19
 ---- batch: 020 ----
mean loss: 192.54
 ---- batch: 030 ----
mean loss: 195.95
 ---- batch: 040 ----
mean loss: 202.40
 ---- batch: 050 ----
mean loss: 201.17
 ---- batch: 060 ----
mean loss: 195.92
 ---- batch: 070 ----
mean loss: 198.99
 ---- batch: 080 ----
mean loss: 204.00
 ---- batch: 090 ----
mean loss: 203.40
train mean loss: 200.63
epoch train time: 0:00:16.573880
elapsed time: 0:37:52.250477
**** EPOCH 134 ****
---- EPOCH 134 TRAINING ----
2019-09-24 21:57:41.166859
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.39
 ---- batch: 020 ----
mean loss: 202.61
 ---- batch: 030 ----
mean loss: 198.46
 ---- batch: 040 ----
mean loss: 201.08
 ---- batch: 050 ----
mean loss: 202.00
 ---- batch: 060 ----
mean loss: 211.33
 ---- batch: 070 ----
mean loss: 200.21
 ---- batch: 080 ----
mean loss: 196.87
 ---- batch: 090 ----
mean loss: 195.80
train mean loss: 200.55
epoch train time: 0:00:16.549976
elapsed time: 0:38:08.801621
**** EPOCH 135 ****
---- EPOCH 135 TRAINING ----
2019-09-24 21:57:57.718033
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 207.90
 ---- batch: 020 ----
mean loss: 204.15
 ---- batch: 030 ----
mean loss: 199.83
 ---- batch: 040 ----
mean loss: 205.52
 ---- batch: 050 ----
mean loss: 196.19
 ---- batch: 060 ----
mean loss: 197.28
 ---- batch: 070 ----
mean loss: 195.97
 ---- batch: 080 ----
mean loss: 200.66
 ---- batch: 090 ----
mean loss: 205.90
train mean loss: 201.88
epoch train time: 0:00:16.546913
elapsed time: 0:38:25.349742
**** EPOCH 136 ****
---- EPOCH 136 TRAINING ----
2019-09-24 21:58:14.266255
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 202.60
 ---- batch: 020 ----
mean loss: 199.08
 ---- batch: 030 ----
mean loss: 205.45
 ---- batch: 040 ----
mean loss: 199.88
 ---- batch: 050 ----
mean loss: 197.56
 ---- batch: 060 ----
mean loss: 200.94
 ---- batch: 070 ----
mean loss: 198.37
 ---- batch: 080 ----
mean loss: 197.42
 ---- batch: 090 ----
mean loss: 200.97
train mean loss: 200.13
epoch train time: 0:00:16.578836
elapsed time: 0:38:41.929839
**** EPOCH 137 ****
---- EPOCH 137 TRAINING ----
2019-09-24 21:58:30.846171
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.95
 ---- batch: 020 ----
mean loss: 199.92
 ---- batch: 030 ----
mean loss: 198.36
 ---- batch: 040 ----
mean loss: 202.47
 ---- batch: 050 ----
mean loss: 196.76
 ---- batch: 060 ----
mean loss: 201.03
 ---- batch: 070 ----
mean loss: 206.83
 ---- batch: 080 ----
mean loss: 203.18
 ---- batch: 090 ----
mean loss: 195.29
train mean loss: 200.44
epoch train time: 0:00:16.566376
elapsed time: 0:38:58.497335
**** EPOCH 138 ****
---- EPOCH 138 TRAINING ----
2019-09-24 21:58:47.413764
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 206.03
 ---- batch: 020 ----
mean loss: 208.66
 ---- batch: 030 ----
mean loss: 207.72
 ---- batch: 040 ----
mean loss: 197.08
 ---- batch: 050 ----
mean loss: 202.51
 ---- batch: 060 ----
mean loss: 196.92
 ---- batch: 070 ----
mean loss: 197.25
 ---- batch: 080 ----
mean loss: 200.49
 ---- batch: 090 ----
mean loss: 206.25
train mean loss: 202.18
epoch train time: 0:00:16.601585
elapsed time: 0:39:15.100133
**** EPOCH 139 ****
---- EPOCH 139 TRAINING ----
2019-09-24 21:59:04.016497
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.18
 ---- batch: 020 ----
mean loss: 195.28
 ---- batch: 030 ----
mean loss: 201.98
 ---- batch: 040 ----
mean loss: 197.20
 ---- batch: 050 ----
mean loss: 193.50
 ---- batch: 060 ----
mean loss: 201.17
 ---- batch: 070 ----
mean loss: 201.04
 ---- batch: 080 ----
mean loss: 205.20
 ---- batch: 090 ----
mean loss: 193.01
train mean loss: 197.75
epoch train time: 0:00:16.671074
elapsed time: 0:39:31.772354
**** EPOCH 140 ****
---- EPOCH 140 TRAINING ----
2019-09-24 21:59:20.688735
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.56
 ---- batch: 020 ----
mean loss: 198.50
 ---- batch: 030 ----
mean loss: 196.75
 ---- batch: 040 ----
mean loss: 200.91
 ---- batch: 050 ----
mean loss: 200.70
 ---- batch: 060 ----
mean loss: 199.39
 ---- batch: 070 ----
mean loss: 197.46
 ---- batch: 080 ----
mean loss: 199.35
 ---- batch: 090 ----
mean loss: 202.74
train mean loss: 198.68
epoch train time: 0:00:16.660576
elapsed time: 0:39:48.434106
**** EPOCH 141 ****
---- EPOCH 141 TRAINING ----
2019-09-24 21:59:37.350488
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.47
 ---- batch: 020 ----
mean loss: 200.15
 ---- batch: 030 ----
mean loss: 208.01
 ---- batch: 040 ----
mean loss: 186.36
 ---- batch: 050 ----
mean loss: 206.40
 ---- batch: 060 ----
mean loss: 198.37
 ---- batch: 070 ----
mean loss: 200.87
 ---- batch: 080 ----
mean loss: 196.35
 ---- batch: 090 ----
mean loss: 202.90
train mean loss: 199.29
epoch train time: 0:00:16.624237
elapsed time: 0:40:05.059862
**** EPOCH 142 ****
---- EPOCH 142 TRAINING ----
2019-09-24 21:59:53.976226
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 198.51
 ---- batch: 020 ----
mean loss: 194.55
 ---- batch: 030 ----
mean loss: 202.48
 ---- batch: 040 ----
mean loss: 203.67
 ---- batch: 050 ----
mean loss: 207.87
 ---- batch: 060 ----
mean loss: 204.20
 ---- batch: 070 ----
mean loss: 204.04
 ---- batch: 080 ----
mean loss: 208.76
 ---- batch: 090 ----
mean loss: 201.13
train mean loss: 203.29
epoch train time: 0:00:16.611687
elapsed time: 0:40:21.672654
**** EPOCH 143 ****
---- EPOCH 143 TRAINING ----
2019-09-24 22:00:10.589041
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 211.23
 ---- batch: 020 ----
mean loss: 204.78
 ---- batch: 030 ----
mean loss: 191.19
 ---- batch: 040 ----
mean loss: 201.96
 ---- batch: 050 ----
mean loss: 193.50
 ---- batch: 060 ----
mean loss: 191.02
 ---- batch: 070 ----
mean loss: 199.79
 ---- batch: 080 ----
mean loss: 191.72
 ---- batch: 090 ----
mean loss: 194.57
train mean loss: 197.96
epoch train time: 0:00:16.551250
elapsed time: 0:40:38.225162
**** EPOCH 144 ****
---- EPOCH 144 TRAINING ----
2019-09-24 22:00:27.141532
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.20
 ---- batch: 020 ----
mean loss: 206.13
 ---- batch: 030 ----
mean loss: 197.85
 ---- batch: 040 ----
mean loss: 197.59
 ---- batch: 050 ----
mean loss: 189.21
 ---- batch: 060 ----
mean loss: 204.06
 ---- batch: 070 ----
mean loss: 198.62
 ---- batch: 080 ----
mean loss: 193.99
 ---- batch: 090 ----
mean loss: 199.19
train mean loss: 197.90
epoch train time: 0:00:16.613124
elapsed time: 0:40:54.839446
**** EPOCH 145 ****
---- EPOCH 145 TRAINING ----
2019-09-24 22:00:43.755896
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.07
 ---- batch: 020 ----
mean loss: 201.83
 ---- batch: 030 ----
mean loss: 194.15
 ---- batch: 040 ----
mean loss: 202.83
 ---- batch: 050 ----
mean loss: 202.33
 ---- batch: 060 ----
mean loss: 201.93
 ---- batch: 070 ----
mean loss: 195.48
 ---- batch: 080 ----
mean loss: 201.22
 ---- batch: 090 ----
mean loss: 197.14
train mean loss: 198.18
epoch train time: 0:00:16.592246
elapsed time: 0:41:11.432975
**** EPOCH 146 ****
---- EPOCH 146 TRAINING ----
2019-09-24 22:01:00.349325
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.19
 ---- batch: 020 ----
mean loss: 200.59
 ---- batch: 030 ----
mean loss: 203.55
 ---- batch: 040 ----
mean loss: 203.41
 ---- batch: 050 ----
mean loss: 196.55
 ---- batch: 060 ----
mean loss: 198.59
 ---- batch: 070 ----
mean loss: 196.03
 ---- batch: 080 ----
mean loss: 191.58
 ---- batch: 090 ----
mean loss: 195.03
train mean loss: 197.94
epoch train time: 0:00:16.568851
elapsed time: 0:41:28.003038
**** EPOCH 147 ****
---- EPOCH 147 TRAINING ----
2019-09-24 22:01:16.919459
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.98
 ---- batch: 020 ----
mean loss: 201.38
 ---- batch: 030 ----
mean loss: 199.55
 ---- batch: 040 ----
mean loss: 195.97
 ---- batch: 050 ----
mean loss: 196.36
 ---- batch: 060 ----
mean loss: 200.91
 ---- batch: 070 ----
mean loss: 192.43
 ---- batch: 080 ----
mean loss: 201.22
 ---- batch: 090 ----
mean loss: 201.26
train mean loss: 197.92
epoch train time: 0:00:16.644784
elapsed time: 0:41:44.648994
**** EPOCH 148 ****
---- EPOCH 148 TRAINING ----
2019-09-24 22:01:33.565467
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 210.65
 ---- batch: 020 ----
mean loss: 200.91
 ---- batch: 030 ----
mean loss: 198.96
 ---- batch: 040 ----
mean loss: 198.75
 ---- batch: 050 ----
mean loss: 189.16
 ---- batch: 060 ----
mean loss: 192.81
 ---- batch: 070 ----
mean loss: 198.49
 ---- batch: 080 ----
mean loss: 199.83
 ---- batch: 090 ----
mean loss: 198.53
train mean loss: 198.02
epoch train time: 0:00:16.567346
elapsed time: 0:42:01.218251
**** EPOCH 149 ****
---- EPOCH 149 TRAINING ----
2019-09-24 22:01:50.134268
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 199.35
 ---- batch: 020 ----
mean loss: 200.70
 ---- batch: 030 ----
mean loss: 194.13
 ---- batch: 040 ----
mean loss: 198.29
 ---- batch: 050 ----
mean loss: 198.01
 ---- batch: 060 ----
mean loss: 202.82
 ---- batch: 070 ----
mean loss: 198.12
 ---- batch: 080 ----
mean loss: 194.14
 ---- batch: 090 ----
mean loss: 196.94
train mean loss: 197.38
epoch train time: 0:00:16.595315
elapsed time: 0:42:17.814345
**** EPOCH 150 ****
---- EPOCH 150 TRAINING ----
2019-09-24 22:02:06.730713
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 204.99
 ---- batch: 020 ----
mean loss: 202.05
 ---- batch: 030 ----
mean loss: 191.97
 ---- batch: 040 ----
mean loss: 203.26
 ---- batch: 050 ----
mean loss: 200.33
 ---- batch: 060 ----
mean loss: 204.34
 ---- batch: 070 ----
mean loss: 189.93
 ---- batch: 080 ----
mean loss: 203.75
 ---- batch: 090 ----
mean loss: 196.61
train mean loss: 199.44
epoch train time: 0:00:16.577324
elapsed time: 0:42:34.392842
**** EPOCH 151 ****
---- EPOCH 151 TRAINING ----
2019-09-24 22:02:23.309275
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.96
 ---- batch: 020 ----
mean loss: 196.58
 ---- batch: 030 ----
mean loss: 195.62
 ---- batch: 040 ----
mean loss: 195.78
 ---- batch: 050 ----
mean loss: 195.36
 ---- batch: 060 ----
mean loss: 212.38
 ---- batch: 070 ----
mean loss: 205.03
 ---- batch: 080 ----
mean loss: 201.28
 ---- batch: 090 ----
mean loss: 204.96
train mean loss: 199.98
epoch train time: 0:00:16.645157
elapsed time: 0:42:51.039192
**** EPOCH 152 ****
---- EPOCH 152 TRAINING ----
2019-09-24 22:02:39.955581
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 200.95
 ---- batch: 020 ----
mean loss: 200.53
 ---- batch: 030 ----
mean loss: 201.13
 ---- batch: 040 ----
mean loss: 191.94
 ---- batch: 050 ----
mean loss: 196.49
 ---- batch: 060 ----
mean loss: 197.82
 ---- batch: 070 ----
mean loss: 199.06
 ---- batch: 080 ----
mean loss: 193.82
 ---- batch: 090 ----
mean loss: 197.00
train mean loss: 197.16
epoch train time: 0:00:16.633574
elapsed time: 0:43:07.674008
**** EPOCH 153 ****
---- EPOCH 153 TRAINING ----
2019-09-24 22:02:56.590457
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.02
 ---- batch: 020 ----
mean loss: 191.04
 ---- batch: 030 ----
mean loss: 202.26
 ---- batch: 040 ----
mean loss: 201.99
 ---- batch: 050 ----
mean loss: 195.01
 ---- batch: 060 ----
mean loss: 191.87
 ---- batch: 070 ----
mean loss: 192.47
 ---- batch: 080 ----
mean loss: 192.47
 ---- batch: 090 ----
mean loss: 194.46
train mean loss: 195.96
epoch train time: 0:00:16.668248
elapsed time: 0:43:24.343500
**** EPOCH 154 ****
---- EPOCH 154 TRAINING ----
2019-09-24 22:03:13.259894
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.32
 ---- batch: 020 ----
mean loss: 196.34
 ---- batch: 030 ----
mean loss: 192.25
 ---- batch: 040 ----
mean loss: 197.56
 ---- batch: 050 ----
mean loss: 197.71
 ---- batch: 060 ----
mean loss: 207.45
 ---- batch: 070 ----
mean loss: 202.26
 ---- batch: 080 ----
mean loss: 197.62
 ---- batch: 090 ----
mean loss: 196.76
train mean loss: 197.82
epoch train time: 0:00:16.733643
elapsed time: 0:43:41.078317
**** EPOCH 155 ****
---- EPOCH 155 TRAINING ----
2019-09-24 22:03:29.994689
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.56
 ---- batch: 020 ----
mean loss: 198.63
 ---- batch: 030 ----
mean loss: 199.43
 ---- batch: 040 ----
mean loss: 192.04
 ---- batch: 050 ----
mean loss: 190.22
 ---- batch: 060 ----
mean loss: 202.48
 ---- batch: 070 ----
mean loss: 196.02
 ---- batch: 080 ----
mean loss: 192.47
 ---- batch: 090 ----
mean loss: 192.73
train mean loss: 195.70
epoch train time: 0:00:16.755886
elapsed time: 0:43:57.835367
**** EPOCH 156 ****
---- EPOCH 156 TRAINING ----
2019-09-24 22:03:46.751758
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.56
 ---- batch: 020 ----
mean loss: 191.33
 ---- batch: 030 ----
mean loss: 200.76
 ---- batch: 040 ----
mean loss: 204.79
 ---- batch: 050 ----
mean loss: 187.57
 ---- batch: 060 ----
mean loss: 197.80
 ---- batch: 070 ----
mean loss: 202.12
 ---- batch: 080 ----
mean loss: 212.02
 ---- batch: 090 ----
mean loss: 197.37
train mean loss: 198.41
epoch train time: 0:00:16.795261
elapsed time: 0:44:14.631808
**** EPOCH 157 ****
---- EPOCH 157 TRAINING ----
2019-09-24 22:04:03.548296
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.21
 ---- batch: 020 ----
mean loss: 205.84
 ---- batch: 030 ----
mean loss: 196.97
 ---- batch: 040 ----
mean loss: 199.45
 ---- batch: 050 ----
mean loss: 201.06
 ---- batch: 060 ----
mean loss: 200.08
 ---- batch: 070 ----
mean loss: 192.22
 ---- batch: 080 ----
mean loss: 190.44
 ---- batch: 090 ----
mean loss: 192.73
train mean loss: 197.56
epoch train time: 0:00:16.753111
elapsed time: 0:44:31.386194
**** EPOCH 158 ****
---- EPOCH 158 TRAINING ----
2019-09-24 22:04:20.302566
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.39
 ---- batch: 020 ----
mean loss: 194.72
 ---- batch: 030 ----
mean loss: 193.71
 ---- batch: 040 ----
mean loss: 189.78
 ---- batch: 050 ----
mean loss: 190.56
 ---- batch: 060 ----
mean loss: 192.16
 ---- batch: 070 ----
mean loss: 197.19
 ---- batch: 080 ----
mean loss: 200.46
 ---- batch: 090 ----
mean loss: 194.99
train mean loss: 195.48
epoch train time: 0:00:16.875524
elapsed time: 0:44:48.263005
**** EPOCH 159 ****
---- EPOCH 159 TRAINING ----
2019-09-24 22:04:37.179399
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.66
 ---- batch: 020 ----
mean loss: 186.23
 ---- batch: 030 ----
mean loss: 199.52
 ---- batch: 040 ----
mean loss: 198.13
 ---- batch: 050 ----
mean loss: 201.11
 ---- batch: 060 ----
mean loss: 196.60
 ---- batch: 070 ----
mean loss: 197.95
 ---- batch: 080 ----
mean loss: 195.59
 ---- batch: 090 ----
mean loss: 194.48
train mean loss: 195.69
epoch train time: 0:00:16.789405
elapsed time: 0:45:05.053636
**** EPOCH 160 ****
---- EPOCH 160 TRAINING ----
2019-09-24 22:04:53.970028
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.80
 ---- batch: 020 ----
mean loss: 196.72
 ---- batch: 030 ----
mean loss: 190.92
 ---- batch: 040 ----
mean loss: 198.38
 ---- batch: 050 ----
mean loss: 195.20
 ---- batch: 060 ----
mean loss: 194.10
 ---- batch: 070 ----
mean loss: 206.21
 ---- batch: 080 ----
mean loss: 193.59
 ---- batch: 090 ----
mean loss: 192.76
train mean loss: 195.55
epoch train time: 0:00:16.819807
elapsed time: 0:45:21.874625
**** EPOCH 161 ****
---- EPOCH 161 TRAINING ----
2019-09-24 22:05:10.790984
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.17
 ---- batch: 020 ----
mean loss: 189.90
 ---- batch: 030 ----
mean loss: 199.23
 ---- batch: 040 ----
mean loss: 193.72
 ---- batch: 050 ----
mean loss: 195.47
 ---- batch: 060 ----
mean loss: 201.10
 ---- batch: 070 ----
mean loss: 198.73
 ---- batch: 080 ----
mean loss: 195.70
 ---- batch: 090 ----
mean loss: 193.84
train mean loss: 195.52
epoch train time: 0:00:16.880820
elapsed time: 0:45:38.756680
**** EPOCH 162 ****
---- EPOCH 162 TRAINING ----
2019-09-24 22:05:27.673029
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.64
 ---- batch: 020 ----
mean loss: 201.56
 ---- batch: 030 ----
mean loss: 203.98
 ---- batch: 040 ----
mean loss: 204.23
 ---- batch: 050 ----
mean loss: 191.26
 ---- batch: 060 ----
mean loss: 198.64
 ---- batch: 070 ----
mean loss: 195.00
 ---- batch: 080 ----
mean loss: 197.78
 ---- batch: 090 ----
mean loss: 189.71
train mean loss: 196.48
epoch train time: 0:00:16.840562
elapsed time: 0:45:55.598391
**** EPOCH 163 ****
---- EPOCH 163 TRAINING ----
2019-09-24 22:05:44.514747
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.51
 ---- batch: 020 ----
mean loss: 186.21
 ---- batch: 030 ----
mean loss: 192.65
 ---- batch: 040 ----
mean loss: 197.26
 ---- batch: 050 ----
mean loss: 198.73
 ---- batch: 060 ----
mean loss: 197.28
 ---- batch: 070 ----
mean loss: 194.93
 ---- batch: 080 ----
mean loss: 188.98
 ---- batch: 090 ----
mean loss: 196.43
train mean loss: 194.78
epoch train time: 0:00:16.914931
elapsed time: 0:46:12.514444
**** EPOCH 164 ****
---- EPOCH 164 TRAINING ----
2019-09-24 22:06:01.430793
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.09
 ---- batch: 020 ----
mean loss: 192.46
 ---- batch: 030 ----
mean loss: 192.97
 ---- batch: 040 ----
mean loss: 189.47
 ---- batch: 050 ----
mean loss: 200.56
 ---- batch: 060 ----
mean loss: 196.24
 ---- batch: 070 ----
mean loss: 196.65
 ---- batch: 080 ----
mean loss: 194.27
 ---- batch: 090 ----
mean loss: 202.71
train mean loss: 195.49
epoch train time: 0:00:16.867106
elapsed time: 0:46:29.382690
**** EPOCH 165 ****
---- EPOCH 165 TRAINING ----
2019-09-24 22:06:18.299149
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.77
 ---- batch: 020 ----
mean loss: 196.17
 ---- batch: 030 ----
mean loss: 189.33
 ---- batch: 040 ----
mean loss: 204.73
 ---- batch: 050 ----
mean loss: 193.31
 ---- batch: 060 ----
mean loss: 203.47
 ---- batch: 070 ----
mean loss: 190.92
 ---- batch: 080 ----
mean loss: 191.29
 ---- batch: 090 ----
mean loss: 199.85
train mean loss: 195.71
epoch train time: 0:00:16.856491
elapsed time: 0:46:46.240400
**** EPOCH 166 ****
---- EPOCH 166 TRAINING ----
2019-09-24 22:06:35.156850
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.26
 ---- batch: 020 ----
mean loss: 194.45
 ---- batch: 030 ----
mean loss: 198.36
 ---- batch: 040 ----
mean loss: 195.03
 ---- batch: 050 ----
mean loss: 194.11
 ---- batch: 060 ----
mean loss: 193.10
 ---- batch: 070 ----
mean loss: 185.86
 ---- batch: 080 ----
mean loss: 203.60
 ---- batch: 090 ----
mean loss: 206.60
train mean loss: 196.85
epoch train time: 0:00:16.839260
elapsed time: 0:47:03.080928
**** EPOCH 167 ****
---- EPOCH 167 TRAINING ----
2019-09-24 22:06:51.997361
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.78
 ---- batch: 020 ----
mean loss: 193.61
 ---- batch: 030 ----
mean loss: 197.55
 ---- batch: 040 ----
mean loss: 190.10
 ---- batch: 050 ----
mean loss: 196.65
 ---- batch: 060 ----
mean loss: 191.21
 ---- batch: 070 ----
mean loss: 196.85
 ---- batch: 080 ----
mean loss: 201.48
 ---- batch: 090 ----
mean loss: 196.64
train mean loss: 195.84
epoch train time: 0:00:16.864942
elapsed time: 0:47:19.947164
**** EPOCH 168 ****
---- EPOCH 168 TRAINING ----
2019-09-24 22:07:08.863515
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.01
 ---- batch: 020 ----
mean loss: 197.12
 ---- batch: 030 ----
mean loss: 190.52
 ---- batch: 040 ----
mean loss: 197.84
 ---- batch: 050 ----
mean loss: 198.40
 ---- batch: 060 ----
mean loss: 208.39
 ---- batch: 070 ----
mean loss: 188.75
 ---- batch: 080 ----
mean loss: 197.45
 ---- batch: 090 ----
mean loss: 194.82
train mean loss: 197.34
epoch train time: 0:00:16.895608
elapsed time: 0:47:36.843945
**** EPOCH 169 ****
---- EPOCH 169 TRAINING ----
2019-09-24 22:07:25.760300
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 201.01
 ---- batch: 020 ----
mean loss: 192.96
 ---- batch: 030 ----
mean loss: 190.41
 ---- batch: 040 ----
mean loss: 193.98
 ---- batch: 050 ----
mean loss: 194.08
 ---- batch: 060 ----
mean loss: 200.62
 ---- batch: 070 ----
mean loss: 197.21
 ---- batch: 080 ----
mean loss: 196.36
 ---- batch: 090 ----
mean loss: 190.39
train mean loss: 194.76
epoch train time: 0:00:16.867449
elapsed time: 0:47:53.712540
**** EPOCH 170 ****
---- EPOCH 170 TRAINING ----
2019-09-24 22:07:42.628907
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 194.57
 ---- batch: 020 ----
mean loss: 196.69
 ---- batch: 030 ----
mean loss: 193.11
 ---- batch: 040 ----
mean loss: 193.85
 ---- batch: 050 ----
mean loss: 205.33
 ---- batch: 060 ----
mean loss: 193.54
 ---- batch: 070 ----
mean loss: 193.67
 ---- batch: 080 ----
mean loss: 184.46
 ---- batch: 090 ----
mean loss: 197.48
train mean loss: 196.39
epoch train time: 0:00:16.777112
elapsed time: 0:48:10.490780
**** EPOCH 171 ****
---- EPOCH 171 TRAINING ----
2019-09-24 22:07:59.407122
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.77
 ---- batch: 020 ----
mean loss: 193.85
 ---- batch: 030 ----
mean loss: 189.97
 ---- batch: 040 ----
mean loss: 195.66
 ---- batch: 050 ----
mean loss: 192.72
 ---- batch: 060 ----
mean loss: 197.53
 ---- batch: 070 ----
mean loss: 201.77
 ---- batch: 080 ----
mean loss: 191.49
 ---- batch: 090 ----
mean loss: 200.09
train mean loss: 194.94
epoch train time: 0:00:16.826539
elapsed time: 0:48:27.318656
**** EPOCH 172 ****
---- EPOCH 172 TRAINING ----
2019-09-24 22:08:16.235880
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.70
 ---- batch: 020 ----
mean loss: 192.67
 ---- batch: 030 ----
mean loss: 194.04
 ---- batch: 040 ----
mean loss: 198.64
 ---- batch: 050 ----
mean loss: 191.73
 ---- batch: 060 ----
mean loss: 193.87
 ---- batch: 070 ----
mean loss: 197.87
 ---- batch: 080 ----
mean loss: 193.25
 ---- batch: 090 ----
mean loss: 189.78
train mean loss: 195.02
epoch train time: 0:00:16.937612
elapsed time: 0:48:44.258333
**** EPOCH 173 ****
---- EPOCH 173 TRAINING ----
2019-09-24 22:08:33.174873
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.48
 ---- batch: 020 ----
mean loss: 188.54
 ---- batch: 030 ----
mean loss: 197.73
 ---- batch: 040 ----
mean loss: 187.61
 ---- batch: 050 ----
mean loss: 191.43
 ---- batch: 060 ----
mean loss: 198.81
 ---- batch: 070 ----
mean loss: 198.18
 ---- batch: 080 ----
mean loss: 205.92
 ---- batch: 090 ----
mean loss: 190.96
train mean loss: 194.20
epoch train time: 0:00:16.983466
elapsed time: 0:49:01.243382
**** EPOCH 174 ****
---- EPOCH 174 TRAINING ----
2019-09-24 22:08:50.159472
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.95
 ---- batch: 020 ----
mean loss: 201.82
 ---- batch: 030 ----
mean loss: 202.05
 ---- batch: 040 ----
mean loss: 187.37
 ---- batch: 050 ----
mean loss: 191.51
 ---- batch: 060 ----
mean loss: 199.98
 ---- batch: 070 ----
mean loss: 191.58
 ---- batch: 080 ----
mean loss: 195.12
 ---- batch: 090 ----
mean loss: 194.55
train mean loss: 195.27
epoch train time: 0:00:17.045848
elapsed time: 0:49:18.290163
**** EPOCH 175 ****
---- EPOCH 175 TRAINING ----
2019-09-24 22:09:07.206524
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.41
 ---- batch: 020 ----
mean loss: 190.18
 ---- batch: 030 ----
mean loss: 192.29
 ---- batch: 040 ----
mean loss: 194.00
 ---- batch: 050 ----
mean loss: 195.41
 ---- batch: 060 ----
mean loss: 196.21
 ---- batch: 070 ----
mean loss: 199.84
 ---- batch: 080 ----
mean loss: 187.91
 ---- batch: 090 ----
mean loss: 195.17
train mean loss: 194.32
epoch train time: 0:00:16.993868
elapsed time: 0:49:35.285205
**** EPOCH 176 ****
---- EPOCH 176 TRAINING ----
2019-09-24 22:09:24.201565
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.74
 ---- batch: 020 ----
mean loss: 192.95
 ---- batch: 030 ----
mean loss: 191.67
 ---- batch: 040 ----
mean loss: 194.89
 ---- batch: 050 ----
mean loss: 198.33
 ---- batch: 060 ----
mean loss: 192.67
 ---- batch: 070 ----
mean loss: 185.32
 ---- batch: 080 ----
mean loss: 187.23
 ---- batch: 090 ----
mean loss: 200.06
train mean loss: 192.38
epoch train time: 0:00:16.947079
elapsed time: 0:49:52.233558
**** EPOCH 177 ****
---- EPOCH 177 TRAINING ----
2019-09-24 22:09:41.150044
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.46
 ---- batch: 020 ----
mean loss: 195.63
 ---- batch: 030 ----
mean loss: 195.06
 ---- batch: 040 ----
mean loss: 187.41
 ---- batch: 050 ----
mean loss: 188.72
 ---- batch: 060 ----
mean loss: 193.52
 ---- batch: 070 ----
mean loss: 197.44
 ---- batch: 080 ----
mean loss: 188.82
 ---- batch: 090 ----
mean loss: 188.89
train mean loss: 192.24
epoch train time: 0:00:16.965728
elapsed time: 0:50:09.200685
**** EPOCH 178 ****
---- EPOCH 178 TRAINING ----
2019-09-24 22:09:58.117116
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 191.73
 ---- batch: 020 ----
mean loss: 194.16
 ---- batch: 030 ----
mean loss: 195.87
 ---- batch: 040 ----
mean loss: 193.41
 ---- batch: 050 ----
mean loss: 190.12
 ---- batch: 060 ----
mean loss: 193.78
 ---- batch: 070 ----
mean loss: 193.63
 ---- batch: 080 ----
mean loss: 183.05
 ---- batch: 090 ----
mean loss: 192.13
train mean loss: 192.24
epoch train time: 0:00:16.876717
elapsed time: 0:50:26.078632
**** EPOCH 179 ****
---- EPOCH 179 TRAINING ----
2019-09-24 22:10:14.994991
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.86
 ---- batch: 020 ----
mean loss: 192.05
 ---- batch: 030 ----
mean loss: 194.15
 ---- batch: 040 ----
mean loss: 188.48
 ---- batch: 050 ----
mean loss: 196.42
 ---- batch: 060 ----
mean loss: 193.50
 ---- batch: 070 ----
mean loss: 189.80
 ---- batch: 080 ----
mean loss: 193.58
 ---- batch: 090 ----
mean loss: 196.67
train mean loss: 193.10
epoch train time: 0:00:16.851230
elapsed time: 0:50:42.931017
**** EPOCH 180 ****
---- EPOCH 180 TRAINING ----
2019-09-24 22:10:31.847491
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.15
 ---- batch: 020 ----
mean loss: 192.18
 ---- batch: 030 ----
mean loss: 193.14
 ---- batch: 040 ----
mean loss: 196.14
 ---- batch: 050 ----
mean loss: 192.46
 ---- batch: 060 ----
mean loss: 185.76
 ---- batch: 070 ----
mean loss: 189.47
 ---- batch: 080 ----
mean loss: 191.30
 ---- batch: 090 ----
mean loss: 190.58
train mean loss: 192.28
epoch train time: 0:00:16.885932
elapsed time: 0:50:59.818245
**** EPOCH 181 ****
---- EPOCH 181 TRAINING ----
2019-09-24 22:10:48.734622
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.45
 ---- batch: 020 ----
mean loss: 195.95
 ---- batch: 030 ----
mean loss: 191.58
 ---- batch: 040 ----
mean loss: 193.60
 ---- batch: 050 ----
mean loss: 188.51
 ---- batch: 060 ----
mean loss: 194.13
 ---- batch: 070 ----
mean loss: 190.20
 ---- batch: 080 ----
mean loss: 191.91
 ---- batch: 090 ----
mean loss: 189.17
train mean loss: 192.11
epoch train time: 0:00:16.893948
elapsed time: 0:51:16.713352
**** EPOCH 182 ****
---- EPOCH 182 TRAINING ----
2019-09-24 22:11:05.629768
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 197.27
 ---- batch: 020 ----
mean loss: 192.53
 ---- batch: 030 ----
mean loss: 201.07
 ---- batch: 040 ----
mean loss: 197.78
 ---- batch: 050 ----
mean loss: 187.50
 ---- batch: 060 ----
mean loss: 199.01
 ---- batch: 070 ----
mean loss: 191.42
 ---- batch: 080 ----
mean loss: 194.13
 ---- batch: 090 ----
mean loss: 185.03
train mean loss: 193.72
epoch train time: 0:00:16.889254
elapsed time: 0:51:33.603803
**** EPOCH 183 ****
---- EPOCH 183 TRAINING ----
2019-09-24 22:11:22.520154
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.67
 ---- batch: 020 ----
mean loss: 193.95
 ---- batch: 030 ----
mean loss: 187.69
 ---- batch: 040 ----
mean loss: 197.58
 ---- batch: 050 ----
mean loss: 194.51
 ---- batch: 060 ----
mean loss: 192.73
 ---- batch: 070 ----
mean loss: 190.98
 ---- batch: 080 ----
mean loss: 195.77
 ---- batch: 090 ----
mean loss: 191.82
train mean loss: 193.18
epoch train time: 0:00:16.906309
elapsed time: 0:51:50.511239
**** EPOCH 184 ****
---- EPOCH 184 TRAINING ----
2019-09-24 22:11:39.427595
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.10
 ---- batch: 020 ----
mean loss: 193.93
 ---- batch: 030 ----
mean loss: 189.24
 ---- batch: 040 ----
mean loss: 195.40
 ---- batch: 050 ----
mean loss: 192.63
 ---- batch: 060 ----
mean loss: 196.79
 ---- batch: 070 ----
mean loss: 190.45
 ---- batch: 080 ----
mean loss: 187.90
 ---- batch: 090 ----
mean loss: 194.16
train mean loss: 192.73
epoch train time: 0:00:16.866455
elapsed time: 0:52:07.378866
**** EPOCH 185 ****
---- EPOCH 185 TRAINING ----
2019-09-24 22:11:56.295252
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 196.80
 ---- batch: 020 ----
mean loss: 186.97
 ---- batch: 030 ----
mean loss: 187.77
 ---- batch: 040 ----
mean loss: 199.86
 ---- batch: 050 ----
mean loss: 205.64
 ---- batch: 060 ----
mean loss: 196.47
 ---- batch: 070 ----
mean loss: 192.97
 ---- batch: 080 ----
mean loss: 188.33
 ---- batch: 090 ----
mean loss: 193.06
train mean loss: 193.78
epoch train time: 0:00:16.838881
elapsed time: 0:52:24.218949
**** EPOCH 186 ****
---- EPOCH 186 TRAINING ----
2019-09-24 22:12:13.135337
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.42
 ---- batch: 020 ----
mean loss: 185.87
 ---- batch: 030 ----
mean loss: 195.47
 ---- batch: 040 ----
mean loss: 191.26
 ---- batch: 050 ----
mean loss: 189.09
 ---- batch: 060 ----
mean loss: 195.84
 ---- batch: 070 ----
mean loss: 197.68
 ---- batch: 080 ----
mean loss: 187.92
 ---- batch: 090 ----
mean loss: 199.40
train mean loss: 192.40
epoch train time: 0:00:16.841118
elapsed time: 0:52:41.061280
**** EPOCH 187 ****
---- EPOCH 187 TRAINING ----
2019-09-24 22:12:29.977745
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 186.98
 ---- batch: 020 ----
mean loss: 198.35
 ---- batch: 030 ----
mean loss: 189.94
 ---- batch: 040 ----
mean loss: 189.62
 ---- batch: 050 ----
mean loss: 191.84
 ---- batch: 060 ----
mean loss: 191.13
 ---- batch: 070 ----
mean loss: 193.51
 ---- batch: 080 ----
mean loss: 195.57
 ---- batch: 090 ----
mean loss: 190.55
train mean loss: 192.23
epoch train time: 0:00:16.863475
elapsed time: 0:52:57.926100
**** EPOCH 188 ****
---- EPOCH 188 TRAINING ----
2019-09-24 22:12:46.842520
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 188.65
 ---- batch: 020 ----
mean loss: 196.99
 ---- batch: 030 ----
mean loss: 187.35
 ---- batch: 040 ----
mean loss: 185.94
 ---- batch: 050 ----
mean loss: 190.77
 ---- batch: 060 ----
mean loss: 191.81
 ---- batch: 070 ----
mean loss: 195.17
 ---- batch: 080 ----
mean loss: 193.73
 ---- batch: 090 ----
mean loss: 190.46
train mean loss: 191.55
epoch train time: 0:00:16.902293
elapsed time: 0:53:14.829660
**** EPOCH 189 ****
---- EPOCH 189 TRAINING ----
2019-09-24 22:13:03.746039
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 189.95
 ---- batch: 020 ----
mean loss: 192.51
 ---- batch: 030 ----
mean loss: 211.54
 ---- batch: 040 ----
mean loss: 196.22
 ---- batch: 050 ----
mean loss: 196.75
 ---- batch: 060 ----
mean loss: 186.47
 ---- batch: 070 ----
mean loss: 189.44
 ---- batch: 080 ----
mean loss: 198.62
 ---- batch: 090 ----
mean loss: 198.15
train mean loss: 195.31
epoch train time: 0:00:16.888260
elapsed time: 0:53:31.719087
**** EPOCH 190 ****
---- EPOCH 190 TRAINING ----
2019-09-24 22:13:20.635463
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.64
 ---- batch: 020 ----
mean loss: 188.60
 ---- batch: 030 ----
mean loss: 195.03
 ---- batch: 040 ----
mean loss: 201.63
 ---- batch: 050 ----
mean loss: 200.33
 ---- batch: 060 ----
mean loss: 191.80
 ---- batch: 070 ----
mean loss: 192.42
 ---- batch: 080 ----
mean loss: 195.90
 ---- batch: 090 ----
mean loss: 183.79
train mean loss: 193.26
epoch train time: 0:00:16.886454
elapsed time: 0:53:48.606674
**** EPOCH 191 ****
---- EPOCH 191 TRAINING ----
2019-09-24 22:13:37.523050
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.17
 ---- batch: 020 ----
mean loss: 195.42
 ---- batch: 030 ----
mean loss: 195.11
 ---- batch: 040 ----
mean loss: 186.23
 ---- batch: 050 ----
mean loss: 189.95
 ---- batch: 060 ----
mean loss: 198.99
 ---- batch: 070 ----
mean loss: 192.51
 ---- batch: 080 ----
mean loss: 189.31
 ---- batch: 090 ----
mean loss: 191.41
train mean loss: 192.16
epoch train time: 0:00:16.834597
elapsed time: 0:54:05.442419
**** EPOCH 192 ****
---- EPOCH 192 TRAINING ----
2019-09-24 22:13:54.358824
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.62
 ---- batch: 020 ----
mean loss: 195.17
 ---- batch: 030 ----
mean loss: 187.94
 ---- batch: 040 ----
mean loss: 190.33
 ---- batch: 050 ----
mean loss: 193.91
 ---- batch: 060 ----
mean loss: 196.12
 ---- batch: 070 ----
mean loss: 185.73
 ---- batch: 080 ----
mean loss: 186.63
 ---- batch: 090 ----
mean loss: 189.95
train mean loss: 190.78
epoch train time: 0:00:16.824478
elapsed time: 0:54:22.268111
**** EPOCH 193 ****
---- EPOCH 193 TRAINING ----
2019-09-24 22:14:11.184507
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 195.21
 ---- batch: 020 ----
mean loss: 185.60
 ---- batch: 030 ----
mean loss: 191.37
 ---- batch: 040 ----
mean loss: 190.53
 ---- batch: 050 ----
mean loss: 186.09
 ---- batch: 060 ----
mean loss: 181.73
 ---- batch: 070 ----
mean loss: 196.60
 ---- batch: 080 ----
mean loss: 195.88
 ---- batch: 090 ----
mean loss: 198.41
train mean loss: 190.97
epoch train time: 0:00:16.817707
elapsed time: 0:54:39.087063
**** EPOCH 194 ****
---- EPOCH 194 TRAINING ----
2019-09-24 22:14:28.003442
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.82
 ---- batch: 020 ----
mean loss: 191.70
 ---- batch: 030 ----
mean loss: 193.05
 ---- batch: 040 ----
mean loss: 193.06
 ---- batch: 050 ----
mean loss: 184.94
 ---- batch: 060 ----
mean loss: 193.75
 ---- batch: 070 ----
mean loss: 192.72
 ---- batch: 080 ----
mean loss: 194.71
 ---- batch: 090 ----
mean loss: 189.18
train mean loss: 191.68
epoch train time: 0:00:16.867828
elapsed time: 0:54:55.956060
**** EPOCH 195 ****
---- EPOCH 195 TRAINING ----
2019-09-24 22:14:44.872471
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 184.78
 ---- batch: 020 ----
mean loss: 191.75
 ---- batch: 030 ----
mean loss: 197.13
 ---- batch: 040 ----
mean loss: 187.13
 ---- batch: 050 ----
mean loss: 189.54
 ---- batch: 060 ----
mean loss: 193.93
 ---- batch: 070 ----
mean loss: 192.93
 ---- batch: 080 ----
mean loss: 190.81
 ---- batch: 090 ----
mean loss: 188.24
train mean loss: 190.79
epoch train time: 0:00:16.921231
elapsed time: 0:55:12.878493
**** EPOCH 196 ****
---- EPOCH 196 TRAINING ----
2019-09-24 22:15:01.794909
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 193.95
 ---- batch: 020 ----
mean loss: 198.81
 ---- batch: 030 ----
mean loss: 193.82
 ---- batch: 040 ----
mean loss: 192.60
 ---- batch: 050 ----
mean loss: 190.25
 ---- batch: 060 ----
mean loss: 191.12
 ---- batch: 070 ----
mean loss: 190.06
 ---- batch: 080 ----
mean loss: 184.69
 ---- batch: 090 ----
mean loss: 193.53
train mean loss: 192.01
epoch train time: 0:00:16.858716
elapsed time: 0:55:29.738415
**** EPOCH 197 ****
---- EPOCH 197 TRAINING ----
2019-09-24 22:15:18.654843
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.39
 ---- batch: 020 ----
mean loss: 191.68
 ---- batch: 030 ----
mean loss: 190.76
 ---- batch: 040 ----
mean loss: 187.14
 ---- batch: 050 ----
mean loss: 188.71
 ---- batch: 060 ----
mean loss: 199.70
 ---- batch: 070 ----
mean loss: 188.86
 ---- batch: 080 ----
mean loss: 190.78
 ---- batch: 090 ----
mean loss: 198.03
train mean loss: 191.33
epoch train time: 0:00:16.880209
elapsed time: 0:55:46.619839
**** EPOCH 198 ****
---- EPOCH 198 TRAINING ----
2019-09-24 22:15:35.536244
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 192.11
 ---- batch: 020 ----
mean loss: 187.40
 ---- batch: 030 ----
mean loss: 190.47
 ---- batch: 040 ----
mean loss: 198.18
 ---- batch: 050 ----
mean loss: 198.81
 ---- batch: 060 ----
mean loss: 192.85
 ---- batch: 070 ----
mean loss: 191.31
 ---- batch: 080 ----
mean loss: 196.18
 ---- batch: 090 ----
mean loss: 186.73
train mean loss: 191.81
epoch train time: 0:00:16.867583
elapsed time: 0:56:03.488646
**** EPOCH 199 ****
---- EPOCH 199 TRAINING ----
2019-09-24 22:15:52.405018
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 187.98
 ---- batch: 020 ----
mean loss: 183.39
 ---- batch: 030 ----
mean loss: 192.77
 ---- batch: 040 ----
mean loss: 188.98
 ---- batch: 050 ----
mean loss: 189.92
 ---- batch: 060 ----
mean loss: 193.24
 ---- batch: 070 ----
mean loss: 189.48
 ---- batch: 080 ----
mean loss: 190.68
 ---- batch: 090 ----
mean loss: 189.05
train mean loss: 189.61
epoch train time: 0:00:16.943162
elapsed time: 0:56:20.433039
**** EPOCH 200 ****
---- EPOCH 200 TRAINING ----
2019-09-24 22:16:09.349412
learning rate: 0.001
 ---- batch: 010 ----
mean loss: 190.90
 ---- batch: 020 ----
mean loss: 190.66
 ---- batch: 030 ----
mean loss: 182.29
 ---- batch: 040 ----
mean loss: 190.22
 ---- batch: 050 ----
mean loss: 193.60
 ---- batch: 060 ----
mean loss: 187.83
 ---- batch: 070 ----
mean loss: 194.79
 ---- batch: 080 ----
mean loss: 195.87
 ---- batch: 090 ----
mean loss: 198.15
train mean loss: 191.62
epoch train time: 0:00:16.847284
elapsed time: 0:56:37.281486
**** EPOCH 201 ****
---- EPOCH 201 TRAINING ----
2019-09-24 22:16:26.197975
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 198.54
 ---- batch: 020 ----
mean loss: 187.29
 ---- batch: 030 ----
mean loss: 186.81
 ---- batch: 040 ----
mean loss: 194.57
 ---- batch: 050 ----
mean loss: 186.84
 ---- batch: 060 ----
mean loss: 181.69
 ---- batch: 070 ----
mean loss: 188.02
 ---- batch: 080 ----
mean loss: 190.62
 ---- batch: 090 ----
mean loss: 192.28
train mean loss: 189.67
epoch train time: 0:00:16.759905
elapsed time: 0:56:54.043477
**** EPOCH 202 ****
---- EPOCH 202 TRAINING ----
2019-09-24 22:16:42.959517
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 191.21
 ---- batch: 020 ----
mean loss: 184.41
 ---- batch: 030 ----
mean loss: 188.11
 ---- batch: 040 ----
mean loss: 181.02
 ---- batch: 050 ----
mean loss: 190.13
 ---- batch: 060 ----
mean loss: 187.74
 ---- batch: 070 ----
mean loss: 184.67
 ---- batch: 080 ----
mean loss: 190.27
 ---- batch: 090 ----
mean loss: 186.29
train mean loss: 187.64
epoch train time: 0:00:16.926830
elapsed time: 0:57:10.971145
**** EPOCH 203 ****
---- EPOCH 203 TRAINING ----
2019-09-24 22:16:59.887573
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.61
 ---- batch: 020 ----
mean loss: 191.27
 ---- batch: 030 ----
mean loss: 183.19
 ---- batch: 040 ----
mean loss: 189.83
 ---- batch: 050 ----
mean loss: 191.59
 ---- batch: 060 ----
mean loss: 180.11
 ---- batch: 070 ----
mean loss: 192.10
 ---- batch: 080 ----
mean loss: 187.69
 ---- batch: 090 ----
mean loss: 186.63
train mean loss: 187.49
epoch train time: 0:00:16.930499
elapsed time: 0:57:27.903027
**** EPOCH 204 ****
---- EPOCH 204 TRAINING ----
2019-09-24 22:17:16.819393
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.00
 ---- batch: 020 ----
mean loss: 188.75
 ---- batch: 030 ----
mean loss: 193.14
 ---- batch: 040 ----
mean loss: 184.57
 ---- batch: 050 ----
mean loss: 183.78
 ---- batch: 060 ----
mean loss: 185.66
 ---- batch: 070 ----
mean loss: 185.59
 ---- batch: 080 ----
mean loss: 197.16
 ---- batch: 090 ----
mean loss: 182.91
train mean loss: 187.08
epoch train time: 0:00:16.945492
elapsed time: 0:57:44.849654
**** EPOCH 205 ****
---- EPOCH 205 TRAINING ----
2019-09-24 22:17:33.766104
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.29
 ---- batch: 020 ----
mean loss: 188.67
 ---- batch: 030 ----
mean loss: 195.75
 ---- batch: 040 ----
mean loss: 176.73
 ---- batch: 050 ----
mean loss: 189.39
 ---- batch: 060 ----
mean loss: 189.02
 ---- batch: 070 ----
mean loss: 183.34
 ---- batch: 080 ----
mean loss: 196.37
 ---- batch: 090 ----
mean loss: 183.19
train mean loss: 187.37
epoch train time: 0:00:16.915377
elapsed time: 0:58:01.766472
**** EPOCH 206 ****
---- EPOCH 206 TRAINING ----
2019-09-24 22:17:50.682902
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.90
 ---- batch: 020 ----
mean loss: 183.18
 ---- batch: 030 ----
mean loss: 184.26
 ---- batch: 040 ----
mean loss: 194.38
 ---- batch: 050 ----
mean loss: 185.48
 ---- batch: 060 ----
mean loss: 187.29
 ---- batch: 070 ----
mean loss: 194.45
 ---- batch: 080 ----
mean loss: 186.91
 ---- batch: 090 ----
mean loss: 188.20
train mean loss: 187.41
epoch train time: 0:00:16.972939
elapsed time: 0:58:18.740668
**** EPOCH 207 ****
---- EPOCH 207 TRAINING ----
2019-09-24 22:18:07.657072
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.60
 ---- batch: 020 ----
mean loss: 180.69
 ---- batch: 030 ----
mean loss: 189.43
 ---- batch: 040 ----
mean loss: 190.91
 ---- batch: 050 ----
mean loss: 189.78
 ---- batch: 060 ----
mean loss: 184.50
 ---- batch: 070 ----
mean loss: 187.25
 ---- batch: 080 ----
mean loss: 183.46
 ---- batch: 090 ----
mean loss: 188.00
train mean loss: 187.52
epoch train time: 0:00:16.900237
elapsed time: 0:58:35.642157
**** EPOCH 208 ****
---- EPOCH 208 TRAINING ----
2019-09-24 22:18:24.558559
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.06
 ---- batch: 020 ----
mean loss: 187.78
 ---- batch: 030 ----
mean loss: 188.09
 ---- batch: 040 ----
mean loss: 196.68
 ---- batch: 050 ----
mean loss: 188.25
 ---- batch: 060 ----
mean loss: 185.30
 ---- batch: 070 ----
mean loss: 185.72
 ---- batch: 080 ----
mean loss: 189.56
 ---- batch: 090 ----
mean loss: 181.57
train mean loss: 187.49
epoch train time: 0:00:16.956161
elapsed time: 0:58:52.599559
**** EPOCH 209 ****
---- EPOCH 209 TRAINING ----
2019-09-24 22:18:41.515951
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.16
 ---- batch: 020 ----
mean loss: 191.72
 ---- batch: 030 ----
mean loss: 189.70
 ---- batch: 040 ----
mean loss: 177.98
 ---- batch: 050 ----
mean loss: 194.26
 ---- batch: 060 ----
mean loss: 194.35
 ---- batch: 070 ----
mean loss: 185.30
 ---- batch: 080 ----
mean loss: 184.67
 ---- batch: 090 ----
mean loss: 184.27
train mean loss: 187.40
epoch train time: 0:00:17.022083
elapsed time: 0:59:09.622838
**** EPOCH 210 ****
---- EPOCH 210 TRAINING ----
2019-09-24 22:18:58.539214
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.64
 ---- batch: 020 ----
mean loss: 189.03
 ---- batch: 030 ----
mean loss: 189.41
 ---- batch: 040 ----
mean loss: 185.70
 ---- batch: 050 ----
mean loss: 182.19
 ---- batch: 060 ----
mean loss: 193.16
 ---- batch: 070 ----
mean loss: 186.26
 ---- batch: 080 ----
mean loss: 186.33
 ---- batch: 090 ----
mean loss: 185.05
train mean loss: 187.02
epoch train time: 0:00:17.017175
elapsed time: 0:59:26.641148
**** EPOCH 211 ****
---- EPOCH 211 TRAINING ----
2019-09-24 22:19:15.557500
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.46
 ---- batch: 020 ----
mean loss: 182.32
 ---- batch: 030 ----
mean loss: 189.54
 ---- batch: 040 ----
mean loss: 185.95
 ---- batch: 050 ----
mean loss: 184.66
 ---- batch: 060 ----
mean loss: 184.56
 ---- batch: 070 ----
mean loss: 191.26
 ---- batch: 080 ----
mean loss: 189.10
 ---- batch: 090 ----
mean loss: 190.01
train mean loss: 187.10
epoch train time: 0:00:17.028160
elapsed time: 0:59:43.670493
**** EPOCH 212 ****
---- EPOCH 212 TRAINING ----
2019-09-24 22:19:32.586908
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.88
 ---- batch: 020 ----
mean loss: 186.47
 ---- batch: 030 ----
mean loss: 186.55
 ---- batch: 040 ----
mean loss: 193.60
 ---- batch: 050 ----
mean loss: 185.05
 ---- batch: 060 ----
mean loss: 180.85
 ---- batch: 070 ----
mean loss: 189.31
 ---- batch: 080 ----
mean loss: 189.97
 ---- batch: 090 ----
mean loss: 185.54
train mean loss: 187.32
epoch train time: 0:00:16.989162
elapsed time: 1:00:00.660867
**** EPOCH 213 ****
---- EPOCH 213 TRAINING ----
2019-09-24 22:19:49.577313
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.86
 ---- batch: 020 ----
mean loss: 190.46
 ---- batch: 030 ----
mean loss: 183.67
 ---- batch: 040 ----
mean loss: 190.69
 ---- batch: 050 ----
mean loss: 186.75
 ---- batch: 060 ----
mean loss: 190.36
 ---- batch: 070 ----
mean loss: 183.54
 ---- batch: 080 ----
mean loss: 186.43
 ---- batch: 090 ----
mean loss: 187.99
train mean loss: 187.00
epoch train time: 0:00:17.009131
elapsed time: 1:00:17.671318
**** EPOCH 214 ****
---- EPOCH 214 TRAINING ----
2019-09-24 22:20:06.587834
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.80
 ---- batch: 020 ----
mean loss: 176.15
 ---- batch: 030 ----
mean loss: 183.46
 ---- batch: 040 ----
mean loss: 186.81
 ---- batch: 050 ----
mean loss: 186.30
 ---- batch: 060 ----
mean loss: 190.72
 ---- batch: 070 ----
mean loss: 194.64
 ---- batch: 080 ----
mean loss: 195.92
 ---- batch: 090 ----
mean loss: 188.01
train mean loss: 187.43
epoch train time: 0:00:17.109750
elapsed time: 1:00:34.782535
**** EPOCH 215 ****
---- EPOCH 215 TRAINING ----
2019-09-24 22:20:23.698942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.07
 ---- batch: 020 ----
mean loss: 189.87
 ---- batch: 030 ----
mean loss: 184.35
 ---- batch: 040 ----
mean loss: 186.30
 ---- batch: 050 ----
mean loss: 184.91
 ---- batch: 060 ----
mean loss: 188.65
 ---- batch: 070 ----
mean loss: 186.57
 ---- batch: 080 ----
mean loss: 188.80
 ---- batch: 090 ----
mean loss: 186.18
train mean loss: 187.61
epoch train time: 0:00:17.212559
elapsed time: 1:00:51.996292
**** EPOCH 216 ****
---- EPOCH 216 TRAINING ----
2019-09-24 22:20:40.912658
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.50
 ---- batch: 020 ----
mean loss: 183.48
 ---- batch: 030 ----
mean loss: 192.58
 ---- batch: 040 ----
mean loss: 187.65
 ---- batch: 050 ----
mean loss: 187.17
 ---- batch: 060 ----
mean loss: 188.21
 ---- batch: 070 ----
mean loss: 184.74
 ---- batch: 080 ----
mean loss: 190.18
 ---- batch: 090 ----
mean loss: 184.26
train mean loss: 187.41
epoch train time: 0:00:16.926624
elapsed time: 1:01:08.924167
**** EPOCH 217 ****
---- EPOCH 217 TRAINING ----
2019-09-24 22:20:57.840552
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.62
 ---- batch: 020 ----
mean loss: 185.46
 ---- batch: 030 ----
mean loss: 188.98
 ---- batch: 040 ----
mean loss: 187.02
 ---- batch: 050 ----
mean loss: 193.58
 ---- batch: 060 ----
mean loss: 183.83
 ---- batch: 070 ----
mean loss: 182.82
 ---- batch: 080 ----
mean loss: 191.43
 ---- batch: 090 ----
mean loss: 187.72
train mean loss: 187.33
epoch train time: 0:00:16.974176
elapsed time: 1:01:25.899603
**** EPOCH 218 ****
---- EPOCH 218 TRAINING ----
2019-09-24 22:21:14.815998
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.47
 ---- batch: 020 ----
mean loss: 187.67
 ---- batch: 030 ----
mean loss: 191.58
 ---- batch: 040 ----
mean loss: 188.18
 ---- batch: 050 ----
mean loss: 185.47
 ---- batch: 060 ----
mean loss: 190.10
 ---- batch: 070 ----
mean loss: 182.02
 ---- batch: 080 ----
mean loss: 190.61
 ---- batch: 090 ----
mean loss: 189.61
train mean loss: 187.36
epoch train time: 0:00:17.056319
elapsed time: 1:01:42.957237
**** EPOCH 219 ****
---- EPOCH 219 TRAINING ----
2019-09-24 22:21:31.873704
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.50
 ---- batch: 020 ----
mean loss: 186.71
 ---- batch: 030 ----
mean loss: 190.39
 ---- batch: 040 ----
mean loss: 187.61
 ---- batch: 050 ----
mean loss: 193.05
 ---- batch: 060 ----
mean loss: 182.06
 ---- batch: 070 ----
mean loss: 181.29
 ---- batch: 080 ----
mean loss: 192.30
 ---- batch: 090 ----
mean loss: 183.14
train mean loss: 186.68
epoch train time: 0:00:16.992949
elapsed time: 1:01:59.951492
**** EPOCH 220 ****
---- EPOCH 220 TRAINING ----
2019-09-24 22:21:48.867995
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.23
 ---- batch: 020 ----
mean loss: 182.38
 ---- batch: 030 ----
mean loss: 185.87
 ---- batch: 040 ----
mean loss: 189.34
 ---- batch: 050 ----
mean loss: 191.75
 ---- batch: 060 ----
mean loss: 184.69
 ---- batch: 070 ----
mean loss: 185.89
 ---- batch: 080 ----
mean loss: 189.66
 ---- batch: 090 ----
mean loss: 186.53
train mean loss: 187.02
epoch train time: 0:00:17.023056
elapsed time: 1:02:16.975876
**** EPOCH 221 ****
---- EPOCH 221 TRAINING ----
2019-09-24 22:22:05.892262
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.35
 ---- batch: 020 ----
mean loss: 185.77
 ---- batch: 030 ----
mean loss: 184.03
 ---- batch: 040 ----
mean loss: 187.83
 ---- batch: 050 ----
mean loss: 185.05
 ---- batch: 060 ----
mean loss: 195.99
 ---- batch: 070 ----
mean loss: 186.32
 ---- batch: 080 ----
mean loss: 191.76
 ---- batch: 090 ----
mean loss: 181.33
train mean loss: 186.91
epoch train time: 0:00:17.156263
elapsed time: 1:02:34.133439
**** EPOCH 222 ****
---- EPOCH 222 TRAINING ----
2019-09-24 22:22:23.050040
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.01
 ---- batch: 020 ----
mean loss: 193.24
 ---- batch: 030 ----
mean loss: 186.00
 ---- batch: 040 ----
mean loss: 183.82
 ---- batch: 050 ----
mean loss: 182.13
 ---- batch: 060 ----
mean loss: 188.06
 ---- batch: 070 ----
mean loss: 185.57
 ---- batch: 080 ----
mean loss: 191.88
 ---- batch: 090 ----
mean loss: 189.33
train mean loss: 187.00
epoch train time: 0:00:17.012514
elapsed time: 1:02:51.147377
**** EPOCH 223 ****
---- EPOCH 223 TRAINING ----
2019-09-24 22:22:40.063768
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.67
 ---- batch: 020 ----
mean loss: 186.48
 ---- batch: 030 ----
mean loss: 185.80
 ---- batch: 040 ----
mean loss: 185.67
 ---- batch: 050 ----
mean loss: 188.95
 ---- batch: 060 ----
mean loss: 188.46
 ---- batch: 070 ----
mean loss: 187.60
 ---- batch: 080 ----
mean loss: 180.11
 ---- batch: 090 ----
mean loss: 193.24
train mean loss: 187.04
epoch train time: 0:00:16.992405
elapsed time: 1:03:08.140952
**** EPOCH 224 ****
---- EPOCH 224 TRAINING ----
2019-09-24 22:22:57.057329
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 181.40
 ---- batch: 020 ----
mean loss: 183.24
 ---- batch: 030 ----
mean loss: 184.95
 ---- batch: 040 ----
mean loss: 184.59
 ---- batch: 050 ----
mean loss: 185.34
 ---- batch: 060 ----
mean loss: 183.09
 ---- batch: 070 ----
mean loss: 188.26
 ---- batch: 080 ----
mean loss: 201.43
 ---- batch: 090 ----
mean loss: 190.14
train mean loss: 187.47
epoch train time: 0:00:17.194503
elapsed time: 1:03:25.336711
**** EPOCH 225 ****
---- EPOCH 225 TRAINING ----
2019-09-24 22:23:14.253109
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.42
 ---- batch: 020 ----
mean loss: 189.03
 ---- batch: 030 ----
mean loss: 184.99
 ---- batch: 040 ----
mean loss: 189.49
 ---- batch: 050 ----
mean loss: 186.66
 ---- batch: 060 ----
mean loss: 186.28
 ---- batch: 070 ----
mean loss: 182.93
 ---- batch: 080 ----
mean loss: 184.50
 ---- batch: 090 ----
mean loss: 189.47
train mean loss: 187.09
epoch train time: 0:00:17.103207
elapsed time: 1:03:42.441145
**** EPOCH 226 ****
---- EPOCH 226 TRAINING ----
2019-09-24 22:23:31.357595
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.61
 ---- batch: 020 ----
mean loss: 192.18
 ---- batch: 030 ----
mean loss: 188.09
 ---- batch: 040 ----
mean loss: 181.60
 ---- batch: 050 ----
mean loss: 183.43
 ---- batch: 060 ----
mean loss: 187.53
 ---- batch: 070 ----
mean loss: 182.07
 ---- batch: 080 ----
mean loss: 189.05
 ---- batch: 090 ----
mean loss: 194.72
train mean loss: 186.94
epoch train time: 0:00:17.070688
elapsed time: 1:03:59.513177
**** EPOCH 227 ****
---- EPOCH 227 TRAINING ----
2019-09-24 22:23:48.429399
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.61
 ---- batch: 020 ----
mean loss: 188.37
 ---- batch: 030 ----
mean loss: 185.01
 ---- batch: 040 ----
mean loss: 192.30
 ---- batch: 050 ----
mean loss: 194.60
 ---- batch: 060 ----
mean loss: 184.26
 ---- batch: 070 ----
mean loss: 186.10
 ---- batch: 080 ----
mean loss: 186.15
 ---- batch: 090 ----
mean loss: 184.16
train mean loss: 186.94
epoch train time: 0:00:17.144560
elapsed time: 1:04:16.658864
**** EPOCH 228 ****
---- EPOCH 228 TRAINING ----
2019-09-24 22:24:05.575257
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 195.63
 ---- batch: 020 ----
mean loss: 180.22
 ---- batch: 030 ----
mean loss: 190.06
 ---- batch: 040 ----
mean loss: 190.80
 ---- batch: 050 ----
mean loss: 181.97
 ---- batch: 060 ----
mean loss: 186.66
 ---- batch: 070 ----
mean loss: 190.46
 ---- batch: 080 ----
mean loss: 182.57
 ---- batch: 090 ----
mean loss: 189.28
train mean loss: 186.80
epoch train time: 0:00:16.976393
elapsed time: 1:04:33.636462
**** EPOCH 229 ****
---- EPOCH 229 TRAINING ----
2019-09-24 22:24:22.552799
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.08
 ---- batch: 020 ----
mean loss: 185.90
 ---- batch: 030 ----
mean loss: 187.39
 ---- batch: 040 ----
mean loss: 194.04
 ---- batch: 050 ----
mean loss: 191.66
 ---- batch: 060 ----
mean loss: 183.50
 ---- batch: 070 ----
mean loss: 187.39
 ---- batch: 080 ----
mean loss: 181.11
 ---- batch: 090 ----
mean loss: 190.21
train mean loss: 187.20
epoch train time: 0:00:17.013072
elapsed time: 1:04:50.650719
**** EPOCH 230 ****
---- EPOCH 230 TRAINING ----
2019-09-24 22:24:39.567121
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.21
 ---- batch: 020 ----
mean loss: 190.41
 ---- batch: 030 ----
mean loss: 189.21
 ---- batch: 040 ----
mean loss: 184.59
 ---- batch: 050 ----
mean loss: 184.37
 ---- batch: 060 ----
mean loss: 185.45
 ---- batch: 070 ----
mean loss: 188.14
 ---- batch: 080 ----
mean loss: 185.88
 ---- batch: 090 ----
mean loss: 186.30
train mean loss: 187.12
epoch train time: 0:00:17.184092
elapsed time: 1:05:07.836230
**** EPOCH 231 ****
---- EPOCH 231 TRAINING ----
2019-09-24 22:24:56.752656
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.83
 ---- batch: 020 ----
mean loss: 186.49
 ---- batch: 030 ----
mean loss: 186.93
 ---- batch: 040 ----
mean loss: 188.71
 ---- batch: 050 ----
mean loss: 184.56
 ---- batch: 060 ----
mean loss: 180.66
 ---- batch: 070 ----
mean loss: 184.68
 ---- batch: 080 ----
mean loss: 189.76
 ---- batch: 090 ----
mean loss: 189.69
train mean loss: 186.88
epoch train time: 0:00:17.182644
elapsed time: 1:05:25.020230
**** EPOCH 232 ****
---- EPOCH 232 TRAINING ----
2019-09-24 22:25:13.936734
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 193.83
 ---- batch: 020 ----
mean loss: 185.35
 ---- batch: 030 ----
mean loss: 188.29
 ---- batch: 040 ----
mean loss: 183.67
 ---- batch: 050 ----
mean loss: 183.03
 ---- batch: 060 ----
mean loss: 190.33
 ---- batch: 070 ----
mean loss: 188.84
 ---- batch: 080 ----
mean loss: 183.77
 ---- batch: 090 ----
mean loss: 188.82
train mean loss: 186.67
epoch train time: 0:00:17.273040
elapsed time: 1:05:42.294951
**** EPOCH 233 ****
---- EPOCH 233 TRAINING ----
2019-09-24 22:25:31.211505
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.29
 ---- batch: 020 ----
mean loss: 185.11
 ---- batch: 030 ----
mean loss: 183.72
 ---- batch: 040 ----
mean loss: 181.78
 ---- batch: 050 ----
mean loss: 189.13
 ---- batch: 060 ----
mean loss: 187.52
 ---- batch: 070 ----
mean loss: 180.65
 ---- batch: 080 ----
mean loss: 189.59
 ---- batch: 090 ----
mean loss: 188.14
train mean loss: 187.08
epoch train time: 0:00:17.262144
elapsed time: 1:05:59.559214
**** EPOCH 234 ****
---- EPOCH 234 TRAINING ----
2019-09-24 22:25:48.475120
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.99
 ---- batch: 020 ----
mean loss: 188.63
 ---- batch: 030 ----
mean loss: 183.65
 ---- batch: 040 ----
mean loss: 191.00
 ---- batch: 050 ----
mean loss: 188.54
 ---- batch: 060 ----
mean loss: 186.66
 ---- batch: 070 ----
mean loss: 190.20
 ---- batch: 080 ----
mean loss: 186.77
 ---- batch: 090 ----
mean loss: 183.13
train mean loss: 186.59
epoch train time: 0:00:17.303653
elapsed time: 1:06:16.863695
**** EPOCH 235 ****
---- EPOCH 235 TRAINING ----
2019-09-24 22:26:05.780148
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.38
 ---- batch: 020 ----
mean loss: 187.66
 ---- batch: 030 ----
mean loss: 182.64
 ---- batch: 040 ----
mean loss: 183.75
 ---- batch: 050 ----
mean loss: 190.15
 ---- batch: 060 ----
mean loss: 190.09
 ---- batch: 070 ----
mean loss: 186.05
 ---- batch: 080 ----
mean loss: 187.78
 ---- batch: 090 ----
mean loss: 187.17
train mean loss: 186.87
epoch train time: 0:00:17.199312
elapsed time: 1:06:34.064503
**** EPOCH 236 ****
---- EPOCH 236 TRAINING ----
2019-09-24 22:26:22.980942
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 185.73
 ---- batch: 020 ----
mean loss: 186.64
 ---- batch: 030 ----
mean loss: 187.32
 ---- batch: 040 ----
mean loss: 190.27
 ---- batch: 050 ----
mean loss: 182.17
 ---- batch: 060 ----
mean loss: 181.21
 ---- batch: 070 ----
mean loss: 191.52
 ---- batch: 080 ----
mean loss: 188.23
 ---- batch: 090 ----
mean loss: 187.84
train mean loss: 186.59
epoch train time: 0:00:17.176766
elapsed time: 1:06:51.242521
**** EPOCH 237 ****
---- EPOCH 237 TRAINING ----
2019-09-24 22:26:40.158862
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.52
 ---- batch: 020 ----
mean loss: 182.62
 ---- batch: 030 ----
mean loss: 196.02
 ---- batch: 040 ----
mean loss: 187.59
 ---- batch: 050 ----
mean loss: 185.75
 ---- batch: 060 ----
mean loss: 180.05
 ---- batch: 070 ----
mean loss: 187.76
 ---- batch: 080 ----
mean loss: 191.96
 ---- batch: 090 ----
mean loss: 186.90
train mean loss: 187.17
epoch train time: 0:00:16.971761
elapsed time: 1:07:08.215414
**** EPOCH 238 ****
---- EPOCH 238 TRAINING ----
2019-09-24 22:26:57.131909
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 190.88
 ---- batch: 020 ----
mean loss: 186.73
 ---- batch: 030 ----
mean loss: 187.56
 ---- batch: 040 ----
mean loss: 186.40
 ---- batch: 050 ----
mean loss: 189.02
 ---- batch: 060 ----
mean loss: 182.62
 ---- batch: 070 ----
mean loss: 185.91
 ---- batch: 080 ----
mean loss: 184.59
 ---- batch: 090 ----
mean loss: 187.38
train mean loss: 186.89
epoch train time: 0:00:17.245116
elapsed time: 1:07:25.461858
**** EPOCH 239 ****
---- EPOCH 239 TRAINING ----
2019-09-24 22:27:14.378402
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 188.41
 ---- batch: 020 ----
mean loss: 190.10
 ---- batch: 030 ----
mean loss: 185.70
 ---- batch: 040 ----
mean loss: 179.18
 ---- batch: 050 ----
mean loss: 188.07
 ---- batch: 060 ----
mean loss: 188.58
 ---- batch: 070 ----
mean loss: 184.15
 ---- batch: 080 ----
mean loss: 184.01
 ---- batch: 090 ----
mean loss: 190.83
train mean loss: 186.92
epoch train time: 0:00:17.257586
elapsed time: 1:07:42.720863
**** EPOCH 240 ****
---- EPOCH 240 TRAINING ----
2019-09-24 22:27:31.637241
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.25
 ---- batch: 020 ----
mean loss: 189.85
 ---- batch: 030 ----
mean loss: 188.05
 ---- batch: 040 ----
mean loss: 178.98
 ---- batch: 050 ----
mean loss: 186.65
 ---- batch: 060 ----
mean loss: 184.26
 ---- batch: 070 ----
mean loss: 190.74
 ---- batch: 080 ----
mean loss: 187.17
 ---- batch: 090 ----
mean loss: 181.44
train mean loss: 186.56
epoch train time: 0:00:17.173807
elapsed time: 1:07:59.895883
**** EPOCH 241 ****
---- EPOCH 241 TRAINING ----
2019-09-24 22:27:48.812328
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 189.85
 ---- batch: 020 ----
mean loss: 182.15
 ---- batch: 030 ----
mean loss: 188.69
 ---- batch: 040 ----
mean loss: 183.75
 ---- batch: 050 ----
mean loss: 187.14
 ---- batch: 060 ----
mean loss: 180.00
 ---- batch: 070 ----
mean loss: 188.52
 ---- batch: 080 ----
mean loss: 181.73
 ---- batch: 090 ----
mean loss: 192.39
train mean loss: 186.74
epoch train time: 0:00:17.242020
elapsed time: 1:08:17.139160
**** EPOCH 242 ****
---- EPOCH 242 TRAINING ----
2019-09-24 22:28:06.055551
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 192.39
 ---- batch: 020 ----
mean loss: 184.15
 ---- batch: 030 ----
mean loss: 189.64
 ---- batch: 040 ----
mean loss: 187.63
 ---- batch: 050 ----
mean loss: 182.32
 ---- batch: 060 ----
mean loss: 184.87
 ---- batch: 070 ----
mean loss: 183.08
 ---- batch: 080 ----
mean loss: 184.47
 ---- batch: 090 ----
mean loss: 192.27
train mean loss: 186.53
epoch train time: 0:00:17.179666
elapsed time: 1:08:34.320082
**** EPOCH 243 ****
---- EPOCH 243 TRAINING ----
2019-09-24 22:28:23.236457
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 186.18
 ---- batch: 020 ----
mean loss: 187.87
 ---- batch: 030 ----
mean loss: 188.89
 ---- batch: 040 ----
mean loss: 183.12
 ---- batch: 050 ----
mean loss: 182.26
 ---- batch: 060 ----
mean loss: 188.65
 ---- batch: 070 ----
mean loss: 192.89
 ---- batch: 080 ----
mean loss: 180.44
 ---- batch: 090 ----
mean loss: 190.64
train mean loss: 186.82
epoch train time: 0:00:17.271258
elapsed time: 1:08:51.592661
**** EPOCH 244 ****
---- EPOCH 244 TRAINING ----
2019-09-24 22:28:40.509011
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 187.53
 ---- batch: 020 ----
mean loss: 195.59
 ---- batch: 030 ----
mean loss: 186.33
 ---- batch: 040 ----
mean loss: 179.31
 ---- batch: 050 ----
mean loss: 187.42
 ---- batch: 060 ----
mean loss: 185.30
 ---- batch: 070 ----
mean loss: 187.19
 ---- batch: 080 ----
mean loss: 181.68
 ---- batch: 090 ----
mean loss: 189.09
train mean loss: 186.87
epoch train time: 0:00:17.260765
elapsed time: 1:09:08.854729
**** EPOCH 245 ****
---- EPOCH 245 TRAINING ----
2019-09-24 22:28:57.771084
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 180.32
 ---- batch: 020 ----
mean loss: 187.78
 ---- batch: 030 ----
mean loss: 182.80
 ---- batch: 040 ----
mean loss: 185.80
 ---- batch: 050 ----
mean loss: 192.16
 ---- batch: 060 ----
mean loss: 188.55
 ---- batch: 070 ----
mean loss: 183.68
 ---- batch: 080 ----
mean loss: 189.22
 ---- batch: 090 ----
mean loss: 190.63
train mean loss: 186.18
epoch train time: 0:00:17.268878
elapsed time: 1:09:26.124871
**** EPOCH 246 ****
---- EPOCH 246 TRAINING ----
2019-09-24 22:29:15.041348
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 196.06
 ---- batch: 020 ----
mean loss: 186.15
 ---- batch: 030 ----
mean loss: 182.64
 ---- batch: 040 ----
mean loss: 191.35
 ---- batch: 050 ----
mean loss: 185.64
 ---- batch: 060 ----
mean loss: 182.46
 ---- batch: 070 ----
mean loss: 186.69
 ---- batch: 080 ----
mean loss: 182.19
 ---- batch: 090 ----
mean loss: 187.81
train mean loss: 186.79
epoch train time: 0:00:17.184365
elapsed time: 1:09:43.310579
**** EPOCH 247 ****
---- EPOCH 247 TRAINING ----
2019-09-24 22:29:32.226941
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 184.94
 ---- batch: 020 ----
mean loss: 185.99
 ---- batch: 030 ----
mean loss: 184.54
 ---- batch: 040 ----
mean loss: 191.44
 ---- batch: 050 ----
mean loss: 190.35
 ---- batch: 060 ----
mean loss: 189.96
 ---- batch: 070 ----
mean loss: 180.98
 ---- batch: 080 ----
mean loss: 180.16
 ---- batch: 090 ----
mean loss: 189.61
train mean loss: 186.30
epoch train time: 0:00:17.406980
elapsed time: 1:10:00.718806
**** EPOCH 248 ****
---- EPOCH 248 TRAINING ----
2019-09-24 22:29:49.635224
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 182.30
 ---- batch: 020 ----
mean loss: 185.38
 ---- batch: 030 ----
mean loss: 184.10
 ---- batch: 040 ----
mean loss: 190.24
 ---- batch: 050 ----
mean loss: 181.65
 ---- batch: 060 ----
mean loss: 190.57
 ---- batch: 070 ----
mean loss: 198.58
 ---- batch: 080 ----
mean loss: 180.33
 ---- batch: 090 ----
mean loss: 184.24
train mean loss: 186.35
epoch train time: 0:00:17.263530
elapsed time: 1:10:17.983554
**** EPOCH 249 ****
---- EPOCH 249 TRAINING ----
2019-09-24 22:30:06.899926
learning rate: 0.0001
 ---- batch: 010 ----
mean loss: 183.32
 ---- batch: 020 ----
mean loss: 187.71
 ---- batch: 030 ----
mean loss: 186.57
 ---- batch: 040 ----
mean loss: 178.22
 ---- batch: 050 ----
mean loss: 192.41
 ---- batch: 060 ----
mean loss: 189.27
 ---- batch: 070 ----
mean loss: 184.49
 ---- batch: 080 ----
mean loss: 188.39
 ---- batch: 090 ----
mean loss: 189.13
train mean loss: 186.35
epoch train time: 0:00:17.225139
elapsed time: 1:10:35.219487
checkpoint saved in file: log/CMAPSS/FD002/min-max/bayesian_conv5_dense1/bayesian_conv5_dense1_1.00/bayesian_conv5_dense1_1.00_2/checkpoint.pth.tar
**** end time: 2019-09-24 22:30:24.135322 ****
